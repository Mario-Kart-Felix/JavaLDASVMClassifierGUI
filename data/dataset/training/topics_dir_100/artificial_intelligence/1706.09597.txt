Path Integral Networks:
End-to-End Differentiable Optimal Control
Masashi Okada, Takenobu Aoshima
AI Solutions Center, Panasonic Corporation.
{okada.masashi001,aoshima.takenobu}@jp.panasonic.com
Luca Rigazio
Panasonic Silicon Valley Laboratory,
Panasonic R&D Company of America
luca.rigazio@us.panasonic.com
Abstract: In this paper, we introduce Path Integral Networks (PI-Net), a recur-
rent network representation of the Path Integral optimal control algorithm. The
network includes both system dynamics and cost models, used for optimal control
based planning. PI-Net is fully differentiable, learning both dynamics and cost
models end-to-end by back-propagation and stochastic gradient descent. Because
of this, PI-Net can learn to plan. PI-Net has several advantages: it can generalize
to unseen states thanks to planning, it can be applied to continuous control tasks,
and it allows for a wide variety learning schemes, including imitation and rein-
forcement learning. Preliminary experiment results show that PI-Net, trained by
imitation learning, can mimic control demonstrations for two simulated problems;
a linear system and a pendulum swing-up problem. We also show that PI-Net is
able to learn dynamics and cost models latent in the demonstrations.
Keywords: Path Integral Optimal Control, Imitation Learning
1 Introduction
Recently, deep architectures such as convolutional neural networks have been successfully applied to
difficult control tasks such as autonomous driving [2], robotic manipulation [13] and playing games
[15]. In these settings, a deep neural network is typically trained with reinforcement learning or imi-
tation learning to represent a control policy which maps input states to control sequences. However,
as already discussed in [5, 21], the resulting networks and encoded policies are inherently reactive,
thus unable to execute planning to decide following actions, which may explain poor generalization
to new or unseen environments. Conversely, optimal control algorithms utilize specified models of
system dynamics and a cost function to predict future states and future cost values. This allows to
compute control sequences that minimize expected cost. Stated differently, optimal control executes
planning for decision making to provide better generalization.
The main practical challenge of optimal control is specifying system dynamics and cost models.
Model-based reinforcement learning [19, 7] can be used to estimate system dynamics by interact-
ing with the environment. However in many robotic applications, accurate system identification is
difficult. Furthermore, predefined cost models accurately describing controller goals are required.
Inverse optimal control or inverse reinforcement learning estimates cost models from human demon-
strations [17, 1, 30], but require perfect knowledge of system dynamics. Other inverse reinforcement
learning methods such as [3, 11, 8] do not require system dynamics perfect knowledge, however,
they limit the policy or cost model to the class of time-varying linear functions.
In this paper, we propose a new approach to deal with these limitations. The key observation is
that control sequences resulting from a specific optimal control algorithm, the path integral control
algorithm [27, 26], are differentiable with respect to all of the controller internal parameters. The
controller itself can thus be represented by a special kind recurrent network, which we call path
integral network (PI-Net). The entire network, which includes dynamics and cost models, can then
ar
X
iv
:1
70
6.
09
59
7v
1 
 [
cs
.A
I]
  2
9 
Ju
n 
20
17
be trained end-to-end using standard back-propagation and stochastic gradient descent with fully
specified or approximated cost models and system dynamics. After training, the network will then
execute planning by effectively running path integral control utilizing the learned system dynamics
and cost model. Furthermore, the effect of modeling errors in learned dynamics can be mitigated by
end-to-end training because cost model could be trained to compensate the errors.
We demonstrate the effectiveness of PI-Net by training the network to imitate optimal controllers
of two control tasks: linear system control and pendulum swing-up task. We also demonstrate that
dynamics and cost models, latent in demonstrations, can be adequately extracted through imitation
learning.
2 Path Integral Optimal Control
Path integral control provides a framework for stochastic optimal control based on Monte-Carlo
simulation of multiple trajectories [12]. This framework has generally been applied to policy im-
provements for parameterized policies such as dynamic movement primitives [23, 11]. Meanwhile
in this paper, we focus on a state-of-the-art path-integral optimal control algorithm [27, 26] devel-
oped for model predictive control (MPC; a.k.a. receding horizon control). In the rest of this section,
we briefly review this path integral optimal algorithm.
Let xti ‚àà Rn denote the state of a dynamical system at discrete time ti, uti ‚àà Rm denotes a control
input for the system. This paper supposes that the dynamics take the following form:
xti+1 = f(xti ,uti + Œ¥uti), (1)
where f : Rn ‚Üí Rn is a dynamics function and Œ¥uti ‚àà Rm is a Gaussian noise vector with
deviation œÉ. The stochastic optimal control problem here is to find the optimal control sequence
{u‚àóti}
N‚àí1
i=0 which minimizes the expected value of trajectory cost function S(œÑt0):
J = E [S(œÑt0)] = E
[
œÜ(xtN ) +
N‚àí1‚àë
i=0
(
q(xti) +
1
2
uTtiRuti
)]
, (2)
where E[¬∑] denotes an expectation values with respect to trajectories œÑt0 =
{xt0 ,ut0 ,xt1 , ¬∑ ¬∑ ¬∑ ,xtN‚àí1} by Eq. (1). œÜ : Rn ‚Üí R and q : Rn ‚Üí R are respectively
terminal- and running-cost; they are arbitrary state-dependent functions. R ‚àà Rm√óm is a positive
definite weight matrix of the quadratic control cost. In [27], a path integral algorithm has been
derived to solve this optimization problem, which iteratively improves {uti}1 to {u‚àóti} by the
following update law:
u‚àóti ‚Üê uti + E
[
exp(‚àíSÃÉ(œÑti)/Œª) ¬∑ Œ¥uti
]/
E
[
exp(‚àíSÃÉ(œÑti)/Œª)
]
, (3)
where SÃÉ(œÑti) is a modified trajectory cost function:
SÃÉ(œÑti) = œÜ(xtN ) +
N‚àí1‚àë
j=i
qÃÉ(xtj ,utj , Œ¥utj ), (4)
qÃÉ(x,u, Œ¥u) = q(x) +
1
2
uTRu +
1‚àí ŒΩ‚àí1
2
Œ¥uTRŒ¥u + uTRŒ¥u. (5)
Eq. (3) can be implemented on digital computers by approximating the expectation value with the
Monte Carlo method as shown in Alg. 1.
Different from other general optimal control algorithms, such as iterative linear quadratic regulator
(iLQR) [24], path integral optimal control does not require first or second-order approximation of
the dynamics and a quadratic approximation of the cost model, naturally allowing for non-linear
system dynamics and cost models. This flexibility allows us to use general function approximators,
such as neural networks, to represent dynamics and cost models in the most general possible form.
2
Noise 
Generator
ùíñùë°ùëñ
ùíôùë°0 ùëûùë°ùëñ
(ùëò)
Control Sequence 
Updater
ùõøùíñùë°ùëñ
(ùëò)
Monte Carlo 
Simulator
ùíñùë°ùëñ
PI-Net Kernel
ùíñùë°ùëñ
‚àó
ùíôùë°0
ùíñùë°ùëñ
PI-Net Kernel
Recurrent Network  (Repeat ùëà times)
Initial 
Control Seq.
State Improved
Control  Seq.
(b)
(a)
ùíôùë°0
ùíñùë°ùëñ
ùëì ùíôùë°ùëñ
(ùëò)
, ùíñùë°ùëñ + ùõøùíñùë°ùëñ
(ùëò)
; ùú∂
‡∑§ùëû ùíôùë°ùëñ
ùëò
, ùíñùë°ùëñ , ùõøùíñùë°ùëñ
(ùëò)
; ùú∑, ùëÖ
ùúô ùíôùë°ùëÅ
(ùëò)
; ùú∏
ùíôùë°ùëñ+1
(ùëò)
ùíôùë°ùëñ
(ùëò)
ùõøùíñùë°ùëñ
(ùëò)
ùëûùë°ùëñ
(ùëò)
ùëûùë°ùëÅ
(ùëò)
Monte Carlo SimulatorÔºö Recurrent Network (Repeat ùëÅ times)
ùõøùíñùë°ùëñ
(ùëò)
ùëûùë°ùëñ
(ùëò)
ùëûùë°ùëñ
(ùëò) ùëÜùë°ùëñ
(ùëò)
‡∑ç
ùëó=0
ùëÅ‚àíùëñ
ùëûùë°ùëÅ‚àíùëó
(ùëò)
ùíñùë°ùëñ +
œÉùëò=0
ùêæ‚àí1 exp ‚àí Œ§ùëÜùë°ùëñ
ùëò
ùúÜ ùõøùíñùë°ùëñ
(ùëò)
œÉùëò=0
ùêæ‚àí1 exp ‚àí Œ§ùëÜùë°ùëñ
ùëò
ùúÜ
ùëûùë°ùëñ
(ùëò)
ùíñùë°ùëñ
ùõøùíñùë°ùëñ
(ùëò)
ùíñùë°ùëñ
‚àó
ùõøùíñùë°ùëñ
(ùëò)
ùíñùë°ùëñ
Control Sequence Updater
‚Ñì5 ‚Ñì7
‚Ñì15
‚Ñì11
‚Ñì4
ùíôùë°0
ùíñùë°ùëñ
Path Integral Network
(c)
(d)
‚Ñì1-8
‚Ñì9-16
‚Ñì1-16
ùëì ùíô, ùíñ; ùú∂ ‡∑§ùëû ùíô, ùíñ, ùõøùíñ;ùú∑, ùëÖ
ùúô ùíô; ùú∏
Dynamics model Cost models
ùíñùë°ùëñ
‚àó
Figure 1: Architecture of PI-Net. Labels with ‚Äò`‚Äô indicate corresponding line numbers in Alg. 1.
Block arrows in (c,d) indicate multiple signal flow with respect to K trajectories.
3 Path Integral Networks
3.1 Architecture
Algorithm 1 Path Integral Optimal Control
input K, N : Num. of trajectories & timesteps
xt0 : State
{uti}: Initial control sequence
{Œ¥u(k)ti }: Gaussian noise
f, q, œÜ,R: Dynamics and cost models
Œª, ŒΩ: Hyper-parameters
output {u‚àóti}: Improved control sequence
1: for k ‚Üê 0 to K ‚àí 1 do
2: x(k)t0 ‚Üê xt0
3: for i‚Üê 0 to N ‚àí 1 do
4: q(k)ti ‚Üê qÃÉ(x
(k)
ti ,uti , Œ¥u
(k)
ti )
5: x(k)ti+1 ‚Üê f
(
x
(k)
ti ,uti + Œ¥u
(k)
ti
)
6: end for
7: q(k)tN ‚Üê œÜ(x
(k)
tN )
8: end for
9: for k ‚Üê 0 to K ‚àí 1 do
10: for i‚Üê 0 to N do
11: SÃÉ(k)œÑti ‚Üê
‚àëN
j=i q
(k)
tN
12: end for
13: end for
14: for i‚Üê 0 to N ‚àí 1 do
15: u‚àóti ‚Üê uti +
‚àëK‚àí1
k=0
[
exp
(
‚àíSÃÉ(k)œÑti
/Œª
)
¬∑Œ¥u(k)t+œÑ
]
‚àëK‚àí1
k=0
[
exp
(
‚àíSÃÉ(k)œÑti /Œª
)]
16: end for
We illustrate the architecture of PI-Net in
Figs. 1 (a)-(d). The architecture encodes Alg. 1
as a fully differentiable recurrent network rep-
resentation. Namely, the forward pass of this
network completely imitates the iterative exe-
cution of Alg. 1.
Top-level architecture of PI-Net is illustrated
in Fig. 1 (a). This network processes input cur-
rent state xt0 and initial control sequence {uti}
to output a control sequence {u‚àóti} improved by
the path integral algorithm. In order to execute
the algorithm, dynamics model f and cost mod-
els qÃÉ, œÜ are also given and embedded into the
network. We suppose f , qÃÉ, œÜ are respectively
parameterized by Œ±, (Œ≤, R)2 and Œ≥, which are
what we intend to train. We remark that PI-Net
architecture allows to use both approximated
parameterized models (e.g., neural networks)
or explicit models for both system dynamics
and cost models. In the network, PI-Net Ker-
nel module is recurrently connected, represent-
ing iterative execution Alg. 1. The number of
recurrence U is set to be sufficiently large for
convergence.
PI-Net Kernel in Fig. 1 (b) contains three mod-
ules: Noise Generator, Monte-Carlo Simulator
and Control Sequence Updater. First, the Noise
Generator procuresK√óN Gaussian noise vec-
tors Œ¥u(k)ti sampled fromN (0, œÉ). Then the noise vectors are input to Monte-Carlo Simulator along
1In the rest of this paper, {uti} represents a sequence {uti}
N‚àí1
i=0 .
2Œ≤ is a parameter for a state-dependent cost model q; see Eqs. (2, 5)
3
with xt0 and {uti}, which estimates running- and terminal-cost values (denoted as q
(k)
ti ) of differ-
ent K trajectories. Finally, the estimated cost values are fed into the Control Sequence Updater to
improve the initial control sequence.
Monte Carlo Simulator in Fig. 1 (c) contains the system dynamics f and cost models qÃÉ, œÜ, which
are responsible to predict future states and costs over K trajectories. The simulations of K trajec-
tories are conducted in parallel. The prediction sequence over time horizon is realized by network
recurrence. In the i-th iteration (i ‚àà {0, 1, ¬∑ ¬∑ ¬∑ , N ‚àí 1}), current K states x(k)ti and perturbated
controls uti + Œ¥u
(k)
ti are input to the dynamics model f to predicted next K states x
(k)
ti+1 . The
predicted states x(k)ti+1 are feedbacked for next iteration. While in the i-th iteration, the above in-
puts are also fed into the cost model qÃÉ to compute running-cost values. Only in the last iteration
(i = N ‚àí 1), predicted terminal-states x(k)tN are input to the cost model œÜ to compute terminal-cost
values. This module is a recurrent network in a recurrent network, making entire PI-Net a nested or
double-looped recurrent network.
Control Sequence Updater in Fig. 1 (d) update input control sequence based on the equations
appeared in `9‚Äì`16 in Alg. 1. Since all equations in the loops can be computed in parallel, no
recurrence is needed for this module.
3.2 Learning schemes
We remark that all the nodes in the computational graph of PI-Net are differentiable. We can there-
fore employ the chain rule to differentiate the network end-to-end, concluding that PI-Net is fully
differentiable. If an objective function with respect to the network control output, denoted asLctrl, is
defined, then we can differentiate the function with the internal parameters (Œ±,Œ≤, R,Œ≥). Therefore,
we can tune the parameters by optimizing the objective function with gradient descent methods. In
other words, we can train internal dynamics f and/or cost models q, œÜ,R end-to-end through the op-
timization. For the optimization, we can re-use all the standard Deep Learning machinery, including
back-propagation and stochastic gradient descent, and a variety of Deep Learning frameworks. We
implemented PI-Net with TensorFlow [6]. Interestingly, all elemental operations of PI-Net can be
described as TensorFlow nodes, allowing to utilize automatic differentiation.
A general use case of PI-Net is imitation learning to learn dynamics and cost models latent in ex-
perts‚Äô demonstrations. Let us consider an open loop control setting and suppose that a dataset
D 3 (x?t0 , {u
?
ti}) is available; x
?
t0 is a state observation and {u
?
ti} is a corresponding control se-
quence generated by an expert. In this case, we can supervisedly train the network by optimizing
Lctrl, i.e., the errors between the expert demonstration {u?ti} and the network output {u
‚àó
ti}. For
closed loop infinite time horizon control setting, the network can be trained as an MPC controller.
If we have a trajectory by an expert {x?t0 ,u
?
t0 ,x
?
t1 , ¬∑ ¬∑ ¬∑ , }, we can construct a dataset D 3 (x
?
ti ,u
?
ti)
and then optimize the estimation errors between the expert control u?ti and the first value of output
control sequence output. If sparse reward function is available, reinforcement learning could be
introduced to train PI-Net. The objective function here is expected return which can be optimized
by policy gradient methods such as REINFORCE [28].
Loss functions In addition to Lctrl, we can append other loss functions to make training faster and
more stable. In an MPC scenario, we can construct a dataset in another form D 3 (x?ti ,u
?
ti ,x
?
ti+1).
In this case, a loss function Ldyn with respect to internal dynamics output can be introduced;
i.e., state prediction errors between f(x?ti ,u
?
ti) and x
?
ti+1 . Furthermore, we can employ loss func-
tions Lcost regarding cost models. In many cases on control domains, we know goal states xg in
prior and we can assume cost models q, œÜ have optimum points at xg . Therefore, loss functions,
which penalize conditions of q(xg) > q(x), can be employed to help the cost models have such
property. This is a useful approach when we utilize highly expressive approximators (e.g., neu-
ral networks) to cost models. In the later experiments, mean squared error (MSE) was used for
Lctrl,dyn. Lcost was defined as œï(q(xg) ‚àí q(x)), where œï is the ramp function. The sum of these
losses can be jointly optimized in a single optimization loop. Of course, dynamics model can be
pre-trained independently by optimizing Ldyn.
4
3.3 Discussion of computational complexity
In order to conduct back-propagation, we must store all values fed into computational graph nodes
during preceding forward pass. Let B be a mini-batch size, then internal dynamics f and running-
cost model qÃÉ in PI-Net are evaluated U √óN √óK √óB times during the forward pass; this value can
grow very fast, making optimization memory hungry. For instance, the experiments of Sect. 5 used
over 100GB of RAM, forcing us to train on CPU instead of GPU.
The complexity can be alleviated by data parallel approach, in which a mini-batch is divided and
processed in parallel with distributed computers. Therefore, we can reduce the batch size B pro-
cessed on a single computer. Another possible approach is to reduce U ; the recurrence number of
the PI-Net Kernel module. In the experiment, initial control sequence is filled with a constant value
(i.e., zero) and U is set to be large enough (e.g., U = 200). In our preliminary experiment, we
found that inputting desired output (i.e., demonstrations) as initial sequences and training PI-Net
with small U did not work; the trained PI-Net just passed through the initial sequence, resulting in
poor generalization performance. In the future, a scheme to determine good initial sequences, which
reduces U while achieving good generalization, must be established.
Note that the memory problem is valid only during training phase because the network does not need
to store input values during control phase. In addition, the mini-batch size is obviously B = 1 in
that phase. Further in MPC scenarios, we can employ warm start settings to reduce U , under which
output control sequences are re-used as initial sequences at next timestep. For instance in [26, 27],
real-time path integral control has been realized by utilizing GPU parallelization.
4 Related Work
Conceptually PI-Net is inspired by the value iteration network (VIN) [21], a differentiable network
representation of the value iteration algorithm designed to train internal state-transition and reward
models end-to-end. The main difference between VIN and PI-Net lies in the underlying algorithms:
the value iteration, generally used for discrete Markov Decision Process (MDP), or path integral op-
timal control, which allows for continuous control. In [21], VIN was applied to 2D navigation task,
where 2D space was discretized to grid map and a reward function was defined on the discretized
map. In addition, action space was defined as eight directions to forward. The experiment showed
that adequately estimated reward map can be utilized to navigate an agent to goal states by not only
discrete control but also continuous control. Let us consider a more complex 2D navigation task
on continuous control, in which velocity must be taken into reward function3. In order to design
such the reward function with VIN, 4D state space (position and velocity) and 2D control space
(vertical and horizontal accelerations) must be discretized. This kind of discretization could cause
combinatorial explosion especially for higher dimensional tasks.
Generally used optimal controller, linear quadratic regulator (LQR), is also differentiable and
Ref. [22] employs this insight to re-shape original cost models to improve short-term MPC per-
formance. The main advantage of the path integral control over (iterative-)LQR is that we do not
require a linear and quadratic approximation of non-linear dynamics and cost model. In order to
differentiate iLQR with non-linear models by back-propagation, we must iteratively differentiate
the functions during preceding forward pass, making the backward pass very complicated.
Policy Improvement with Path Integrals [23] and Inverse Path Integral Inverse Reinforcement Learn-
ing [11] are policy search approaches based on the path integral control framework, which train a
parameterized control policy via reinforcement learning and imitation learning, respectively. These
methods have been succeeded to train policies for complex robotics tasks, however, they assume
trajectory-centric policy representation such as dynamic movement primitives [9]; such the policy is
less generalizable for unseen settings (e.g., different initial states).
Since PI-Net is a policy representation of optimal control, trainable end-to-end by standard back-
propagation, a wide variety of learning to control approaches may be applied, including:
Imitation learning DAGGER [18] and PLATO [10],
3Such as a task to control a mass point to trace a fixed track while forwarding it as fast as possible.
5
0 20 40 60
Number¬†of¬†Epochs
10 2
10 1
M
S
E
(a)¬†Loss¬†convergence
Train
Test
2
0
2
S
ta
te
(b)¬†Pred.¬†by¬†trained¬†model
0 50 100 150 200
Discrete¬†Time
2
4
C
os
t √ó10 2
2
0
2
S
ta
te
(c)¬†Pred.¬†by¬†teacher¬†model
0 50 100 150 200
Discrete¬†Time
2
4
C
os
t √ó10 2
Figure 2: Results in linear system experiment.
Reinforcement learning Deep Deterministic Policy Gradient [14], A3C [16], Trust Region Opti-
mization [20], Guided Policy Search [13] and Path Integral Guided Policy Search [4].
5 Experiments
We conducted experiments to validate the viability of PI-Net. These experiments are meant to test if
the network can effectively learn policies in an imitation learning setting. We did this by supplying
demonstrations generated by general optimal control algorithms, with known dynamics and cost
models (termed teacher models in the rest of this paper). Ultimately, in real application scenarios,
demonstrations may be provided by human experts.
5.1 Linear system
The objective of this experiment is to validate that PI-Net is trainable and it can jointly learn dynam-
ics and cost models latent in demonstrations.
Demonstrations Teacher models in this experiment were linear dynamics, f?(x,u) = F ?x +
G?u, and quadratic cost model, q?(x) = œÜ?(x) = xTQ?x/2, where x ‚àà R4, u ‚àà R2, F ? ‚àà R4√ó4,
G? ‚àà R4√ó2, Q? ‚àà R4√ó4, R? ‚àà R2√ó2. Starting from the fixed set of parameters, we produced
training and test dataset Dtrain, Dtest, which take the from: D 3 (x?t0 , {u
?
ti}). LQR was used to
obtain references {u?ti} from randomly generated state vectors x
?
t0 . The size of each dataset was
|Dtrain| = 950 and |Dtest| = 50.
PI-Net settings Internal dynamics and cost models were also linear and quadratic form whose
initial parameters were different from the teacher models‚Äô. PI-Net was supervisedly trained by
optimizing Lctrl. We did not use Ldyn and Lcost in this experiment.
Results Fig. 2 shows the results of this experiments. Fig. 2 (a) illustrates loss during training
epochs, showing good convergence to a lower fixed point. This validates that PI-Net was indeed
training well and the trained policy generalized well to test samples. Fig. 2 (b, c) exemplifies state
and cost trajectories predicted by trained dynamics and cost models, which were generated by feed-
ing certain initial state and corresponding optimal control sequence into the models. Fig. 2 (c) are
trajectories by the teacher models. State trajectories in Fig. 2(b, c) approximate each other, indicat-
ing the internal dynamics model learned to approximate the teacher dynamics. It is well-known that
different cost functions could result in same controls [30], and indeed cost trajectories in Fig. 2(b,
c) seem not similar. However, this would not be a problem as long as a learned controller is well
generalized to unseen state inputs.
5.2 Pendulum swing up
Next, we tried to imitate demonstrations generated from non-linear dynamics and non-quadratic cost
models while validating the PI-Net applicability to MPC tasks. We also compared PI-Net with VIN.
Demonstrations The experiment focuses on the classical inverted pendulum swing up task [25].
Teacher cost models were q?(Œ∏, Œ∏Ãá) = œÜ?(Œ∏, Œ∏Ãá) = (1 + cos Œ∏)2 + Œ∏Ãá2, R? = 5, where Œ∏ is pendulum
angle and Œ∏Ãá is angular velocity. Under this model assumptions, we firstly conducted 40s MPC sim-
ulations with iLQR to generate trajectories. We totally generated 50 trajectories and then produced
6
Table 1: Training and simulation results on pendulum swing-up task. Trajectory cost shown here is
the average of 10 trajectories.
MSE for MSE for Success Traj. Cost # trainable
Controllers Dtrain Dtest Rate S(œÑ ) params
Expert N/A N/A 100% 404.63 N/A
Trained PI-Net 2.22√ó 10‚àí3 1.65√ó 10‚àí3 100% 429.69 242
Freezed PI-Net 1.91√ó 10‚àí3 5.73√ó 10‚àí3 100% 982.22 49
VIN (LCN) 6.44√ó 10‚àí3 6.89√ó 10‚àí3 0% 2409.29 330,768
VIN (CNN) 4.45√ó 10‚àí3 4.72√ó 10‚àí3 0% 1280.62 1,488
Dtrain 3 (x?ti , u
?
ti ,x
?
ti+1), where control u is torque to actuate a pendulum. We also generated 10
trajectories for Dtest.
PI-Net settings We used a more general modeling scheme where internal dynamics and cost mod-
els were represented by neural networks, both of which had one hidden layer. The number of hidden
nodes was 12 for the dynamics model, and 24 for the cost model. First, we pre-trained the internal
dynamics independently by optimizing Ldyn and then the entire network was trained by optimizing
Lctrl + 10‚àí3 ¬∑ Lcost. In this final optimization, the dynamics model was freezed to focus on cost
learning. Goal states used to define Lcost were xg = (Œ∏, Œ∏Ãá) = (¬±œÄ, 0). We prepared a model vari-
ant, termed freezed PI-Net, whose internal dynamics was the above-mentioned pre-trained one and
cost model was teacher model as is. The freezed PI-Net was not trained end-to-end.
VIN settings VIN was designed to have 2D inputs for continuous states and 1D output for con-
tinuous control. In order to define a reward map embedded in VIN, we discretized 2D continuous
state to 31√ó 31 grid map. This map is cylindrical because angle-axis is cyclic. We also discretized
1D control space to 7 actions, each of which corresponds to different torque. We denote the re-
ward map as R(s, a) ‚àà R31√ó31√ó7, where s and a respectively denote discrete state and action.
The reward map can be decomposed as the sum of R1(s) ‚àà R31√ó31√ó1 and R2(a) ‚àà R1√ó1√ó7.
Original VIN employed convolutional neural networks (CNNs) to represent state transition kernels.
However, this representation implies that state transition probability P(s‚Ä≤|s, a) can be simplified to
P(s‚Ä≤‚àís|a)4. Since this supposition is invalid for the pendulum system5 , we alternatively employed
locally connected neural networks (LCNs; i.e., CNNs without weight sharing) [29]. We also pre-
pared a CNN-based VIN for comparison. The embedded reward maps and transition kernels were
trained end-to-end by optimizing Lctrl.
Results The results of training and MPC simulations with trained controllers are summarized in
Table 1. In the simulations, we observed success rates of task completion (defined as keeping the
pendulum standing up more than 5s) and trajectory cost S(œÑ) calculated by the teacher cost. For
each network, ten 60-second simulations were conducted starting from different initial states. In
the table, freezed PI-Net showed less generalization performance although it was equipped with the
teacher cost. This degradation might result from the modeling errors of the learned dynamics. On
the other hand, trained PI-Net achieved the best performance both on generalization and control,
suggesting that adequate cost model was trained to imitate demonstrations while compensating the
dynamics errors. Fig. 3 illustrates visualized cost models where the cost map of the trained PI-Net
resembles the teacher model well. The reason of VIN failures must result from modeling difficulty
on continuous control tasks. Fine discretization of state and action space would be necessary for
good approximation; however, this results in the explosion of parameters to be trained, making
optimizations difficult. CNN-representation of transition kernels would not work because this is
very rough approximation for the most of control systems. Therefore, one can conclude that the use
of PI-Net would be more reasonable on continuous control because of the VIN modeling difficulty.
4 Under this supposition, probability of relative state transition, with respect to a certain action, is invariable.
5 See the time evolution equation of this system, Œ∏Ãà = ‚àí sin Œ∏ + k ¬∑ u (k: model paramter); relative time-
variation of angular velocity Œ∏Ãá, with respect to certain torque u, varies with pendulum angle Œ∏.
7
0
Angle [rad]
2
0
-2An
gu
la
r V
el
. [
ra
d/
s] Teacher Model
0
Angle [rad]
PI-Net
0
Angle [rad]
VIN (LCN)
0
Angle [rad]
VIN (CNN)
Figure 3: Cost/Reward maps. From left to right, the teacher cost q?, neural cost q trained by PI-Net,
and reward mapsR1 trained by the LCN- and CNN-based VINs. Dense color indicates low cost (or
high reward).
6 Conclusion
In this paper, we introduced path integral networks, a fully differentiable end-to-end trainable repre-
sentation of the path integral optimal control algorithm, which allows for optimal continuous control.
Because PI-Net is fully differentiable, it can rely on powerful Deep Learning machinery to efficiently
estimate in high dimensional parameters spaces from large data-sets. To the best of our knowledge,
PI-Net is the first end-to-end trainable differentiable optimal controller directly applicable to contin-
uous control domains.
PI-Net architecture is highly flexible, allowing to specify system dynamics and cost models in an
explicit way, by analytic models, or in an approximate way by deep neural networks. Parameters
are then jointly estimated end-to-end, in a variety of settings, including imitation learning and re-
inforcement learning. This may be very useful for non-linear continuous control scenarios, such as
the ‚Äúpixel to torques‚Äù scenario, and in situations where it‚Äôs difficult to fully specify system dynamics
or cost models. We postulate this architecture may allow to train approximate system dynamics and
cost models in such complex scenarios while still carrying over the advantages of optimal control
from the underlying path integral optimal control. We show promising initial results in an imitation
learning setting, comparing against optimal control algorithms with linear and non-linear system
dynamics. Future works include a detailed comparison to other baselines, including other learn-to-
control methods as well as experiments in high-dimensional settings. To tackle high-dimensional
problems and to accelerate convergence we plan to combine PI-Net with other powerful methods,
such as guided cost learning [8] and trust region policy optimization [20].
8
References
[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In ICML,
2004.
[2] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, et al. End to end learning for
self-driving cars. arXiv:1604.07316, 2016.
[3] A. Boularias, J. Kober, and J. Peters. Relative entropy inverse reinforcement learning. In
AISTATS, 2011.
[4] Y. Chebotar, M. Kalakrishnan, A. Yahya, A. Li, S. Schaal, and S. Levine. Path integral guided
policy search. arXiv:1610.00529, 2016.
[5] C. Chen, A. Seff, A. Kornhauser, and J. Xiao. DeepDriving: Learning affordance for direct
perception in autonomous driving. In ICCV, 2015.
[6] J. Dean and R. Monga. TensorFlow: Large-scale machine learn-
ing on heterogeneous distributed systems. Google Research Whitepaper,
http://research.google.com/pubs/archive/45166.pdf, 2015.
[7] M. Deisenroth and C. E. Rasmussen. PILCO: A model-based and data-efficient approach to
policy search. In ICML, 2011.
[8] C. Finn, S. Levine, and P. Abbeel. Guided cost learning: Deep inverse optimal control via
policy optimization. In ICML, volume 48, 2016.
[9] A. J. Ijspeert, J. Nakanishi, and S. Schaal. Movement imitation with nonlinear dynamical
systems in humanoid robots. In ICRA, volume 2, 2002.
[10] G. Kahn, T. Zhang, S. Levine, and P. Abbeel. PLATO: Policy learning using adaptive trajectory
optimization. In ICRA, 2017.
[11] M. Kalakrishnan, P. Pastor, L. Righetti, and S. Schaal. Learning objective functions for ma-
nipulation. In ICRA, 2013.
[12] H. J. Kappen. Linear theory for control of nonlinear stochastic systems. Phys. Rev. Lett., 95
(20), 2005.
[13] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies.
JMLR, 17(39), 2016.
[14] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra.
Continuous control with deep reinforcement learning. arXiv:1509.02971, 2015.
[15] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540), 2015.
[16] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, et al. Asynchronous methods for
deep reinforcement learning. In ICML, 2016.
[17] A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory
and application to reward shaping. In ICML, volume 99, 1999.
[18] S. Ross, G. J. Gordon, and D. Bagnell. A reduction of imitation learning and structured pre-
diction to no-regret online learning. In AISTATS, volume 1, 2011.
[19] J. Schmidhuber. An on-line algorithm for dynamic reinforcement learning and planning in
reactive environments. In IJCNN, 1990.
[20] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz. Trust region policy optimiza-
tion. In ICML, 2015.
[21] A. Tamar, S. Levine, P. Abbeel, Y. Wu, and G. Thomas. Value iteration networks. In NIPS,
2016.
9
[22] A. Tamar, G. Thomas, T. Zhang, S. Levine, and P. Abbeel. Learning from the hindsight plan‚Äì
episodic MPC improvement. arXiv:1609.09001, 2016.
[23] E. Theodorou, J. Buchli, and S. Schaal. A generalized path integral control approach to rein-
forcement learning. JMLR, 11, 2010.
[24] E. Todorov and W. Li. A generalized iterative LQG method for locally-optimal feedback
control of constrained nonlinear stochastic systems. In ACC, 2005.
[25] H. O. Wang, K. Tanaka, and M. F. Griffin. An approach to fuzzy control of nonlinear systems:
Stability and design issues. IEEE Trans. Fuzzy Syst., 4(1), 1996.
[26] G. Williams, P. Drews, B. Goldfain, J. M. Rehg, and E. A. Theodorou. Aggressive driving with
model predictive path integral control. In ICRA, 2016.
[27] G. Williams, A. Aldrich, and E. A. Theodorou. Model predictive path integral control: From
theory to parallel computation. J. Guid. Control Dyn., 2017.
[28] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4), 1992.
[29] R. Yentis and M. Zaghloul. Vlsi implementation of locally connected neural network for
solving partial differential equations. IEEE Trans. Circuits Syst. I, Fundam. Theory and Appl.,
43(8), 1996.
[30] B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforce-
ment learning. In AAAI Conference on Artificial Intelligence, volume 8, 2008.
10
A Supplements of Experiments
A.1 Common settings
We used RMSProp for optimization. Initial learning rate was set to be 10‚àí3 and the rate was decayed
by a factor of 2 when no improvement of loss function was observed for five epochs. We set the
hyper-parameters appeared in the path integral algorithm [27] as Œª = 0.01, ŒΩ = 1500.
A.2 Linear System
Demonstrations Dynamics parameters F ?, G? were randomly determined by following equa-
tions;
F ? = exp
[
‚àÜt(A‚àíAT )
]
, ‚àÄa ‚àà A, a ‚àº N (0, 1), (6)
G? =
(
Gc
O2,2
)
, Gc ‚àà R2√ó2,‚àÄg ‚àà Gc, g ‚àº N (0,‚àÜt), (7)
where exp[¬∑] indicates the matrix exponential and the time step size ‚àÜt is 0.01. Cost parameters Q?,
R? were deterministically given by;
Q? = I4,4 ¬∑‚àÜt, R? = I2,2 ¬∑‚àÜt. (8)
Elements of a state vector x?t0 were sampled from N (0, 1) and then the vector was input to LQR to
generate a control sequence {u?ti} whose length was N = 200.
PI-Net settings The internal dynamics parameters were initialized in the same manner as Eqs. (6,
7). According to the internal cost parameters, all values were sampled from N (0,‚àÜt). The number
of trajectories was K = 100 and the number of PI-Net Kernel recurrence was U = 200. We used
the standard deviation œÉ = 0.2 to generate Gaussian noise Œ¥u.
Results Fig. 4 exemplifies control sequences for an unseen state input estimated by trained PI-Net
and LQR.
u 1
Path¬†Integral
LQR
0 25 50 75 100 125 150 175 200
Discrete¬†Time
u 2
Figure 4: Control sequences
A.3 Pendulum Swing-Up
Demonstrations We supposed that the system is dominated by the time-evolution: Œ∏Ãà = ‚àí sin(Œ∏)+
k ¬∑ u, where k was set to be 0.5. We discretized this continuous dynamics by using the forth-order
Runge-Kutta method, where the time step size was ‚àÜt = 0.1.
MPC simulations were conducted under the following conditions. Initial pendulum-angles Œ∏ in tra-
jectories were uniformly sampled from [‚àíœÄ, œÄ] at random. Initial angular velocities were uniformly
sampled from [‚àí1, 1]. iLQR output a control sequence whose length was N = 30, and only the first
value in the sequence was utilized for actuation at each time step.
11
PI-Net settings A neural dynamics model with one hidden layer (12 hidden nodes) and one output
node was used to approximate Œ∏Ãà. Œ∏Ãá and Œ∏ were estimated by the Euler method utilizing their time
derivatives. The cost model is represented as q(Œ∏, Œ∏Ãá) = ||q(Œ∏, Œ∏Ãá)||2, where q is a neural network with
one hidden layer (12 hidden nodes) and 12 output nodes. In the both neural networks, hyperbolic
tangent is used as activation functions for hidden nodes. Other PI-Net parameters were: œÉ = 0.005,
K = 100, U = 200 and N = 30.
VIN settings The VIN design for the pendulum task is described here. In order to know
the value iteration algorithm and the design methodology of VINs, the authors recommend to
read Ref. [21]. The equation of reward propagation appeared in the algorithm, i.e., Q(s, a) ‚Üê‚àë
s‚Ä≤ V(s‚Ä≤)P(s‚Ä≤|s, a)+R(s, a), was represented by LCN or CNN layer, whereQ(s, a), V(s),R(s, a)
were action-value map, value-map, and reward map, respectively. State-transition kernels P(s‚Ä≤|s, a)
were represented by 7 √ó 7 kernels of LCN or CNN. This layer processes the input value map and
creates a matrix (‚àà R31√ó31√ó7), then this matrix is summed up with the reward map to compute
action-state map. The attention module output selected action-state value, i.e.,Q(s, ¬∑) ‚àà R7 where s
corresponds input continuous states. The reactive policy was a neural network with one hidden layer
with 16 hidden nodes. This neural network has 9D inputs (2D for states and 7D for output from the
attention module) and 1D output for control. The activation function is rectified linear function. The
number of recurrence of VIN module is 50.
12

