LIBSVM: A Library for Support Vector Machines
Chih-Chung Chang and Chih-Jen Lin
Department of Computer Science
National Taiwan University, Taipei, Taiwan
Email: cjlin@csie.ntu.edu.tw
Initial version: 2001 Last updated: March 4, 2013
Abstract
LIBSVM is a library for Support Vector Machines (SVMs). We have been
actively developing this package since the year 2000. The goal is to help users
to easily apply SVM to their applications. LIBSVM has gained wide popu-
larity in machine learning and many other areas. In this article, we present
all implementation details of LIBSVM. Issues such as solving SVM optimiza-
tion problems, theoretical convergence, multi-class classification, probability
estimates, and parameter selection are discussed in detail.
Keywords: Classification, LIBSVM, optimization, regression, support vector ma-
chines, SVM
1 Introduction
Support Vector Machines (SVMs) are a popular machine learning method for classifi-
cation, regression, and other learning tasks. Since the year 2000, we have been devel-
oping the package LIBSVM as a library for support vector machines. The Web address
of the package is at http://www.csie.ntu.edu.tw/~cjlin/libsvm. LIBSVM is cur-
rently one of the most widely used SVM software. In this article,1 we present all
implementation details of LIBSVM. However, this article does not intend to teach
the practical use of LIBSVM. For instructions of using LIBSVM, see the README file
included in the package, the LIBSVM FAQ,2 and the practical guide by Hsu et al.
(2003). An earlier version of this article was published in Chang and Lin (2011).
LIBSVM supports the following learning tasks.
1This LIBSVM implementation document was created in 2001 and has been maintained at http:
//www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf.
2LIBSVM FAQ: http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html.
1
Table 1: Representative works in some domains that have successfully used LIBSVM.
Domain Representative works
Computer vision LIBPMK (Grauman and Darrell, 2005)
Natural language processing Maltparser (Nivre et al., 2007)
Neuroimaging PyMVPA (Hanke et al., 2009)
Bioinformatics BDVal (Dorff et al., 2010)
1. SVC: support vector classification (two-class and multi-class).
2. SVR: support vector regression.
3. One-class SVM.
A typical use of LIBSVM involves two steps: first, training a data set to obtain a
model and second, using the model to predict information of a testing data set. For
SVC and SVR, LIBSVM can also output probability estimates. Many extensions of
LIBSVM are available at libsvmtools.3
The LIBSVM package is structured as follows.
1. Main directory: core C/C++ programs and sample data. In particular, the file
svm.cpp implements training and testing algorithms, where details are described
in this article.
2. The tool sub-directory: this sub-directory includes tools for checking data
format and for selecting SVM parameters.
3. Other sub-directories contain pre-built binary files and interfaces to other lan-
guages/software.
LIBSVM has been widely used in many areas. From 2000 to 2010, there were
more than 250,000 downloads of the package. In this period, we answered more than
10,000 emails from users. Table 1 lists representative works in some domains that
have successfully used LIBSVM.
This article is organized as follows. In Section 2, we describe SVM formulations
supported in LIBSVM: C-support vector classification (C-SVC), Î½-support vector
classification (Î½-SVC), distribution estimation (one-class SVM), -support vector re-
gression (-SVR), and Î½-support vector regression (Î½-SVR). Section 3 then discusses
performance measures, basic usage, and code organization. All SVM formulations
3 LIBSVM Tools: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools.
2
supported in LIBSVM are quadratic minimization problems. We discuss the opti-
mization algorithm in Section 4. Section 5 describes two implementation techniques
to reduce the running time for minimizing SVM quadratic problems: shrinking and
caching. LIBSVM provides some special settings for unbalanced data; details are
in Section 6. Section 7 discusses our implementation for multi-class classification.
Section 8 presents how to transform SVM decision values into probability values. Pa-
rameter selection is important for obtaining good SVM models. Section 9 presents a
simple and useful parameter selection tool in LIBSVM. Finally, Section 10 concludes
this work.
2 SVM Formulations
LIBSVM supports various SVM formulations for classification, regression, and distri-
bution estimation. In this section, we present these formulations and give correspond-
ing references. We also show performance measures used in LIBSVM.
2.1 C-Support Vector Classification
Given training vectors xi âˆˆ Rn, i = 1, . . . , l, in two classes, and an indicator vector
y âˆˆ Rl such that yi âˆˆ {1,âˆ’1}, C-SVC (Boser et al., 1992; Cortes and Vapnik, 1995)
solves the following primal optimization problem.
min
w,b,Î¾
1
2
wTw + C
lâˆ‘
i=1
Î¾i (1)
subject to yi(w
TÏ†(xi) + b) â‰¥ 1âˆ’ Î¾i,
Î¾i â‰¥ 0, i = 1, . . . , l,
where Ï†(xi) maps xi into a higher-dimensional space and C > 0 is the regularization
parameter. Due to the possible high dimensionality of the vector variable w, usually
we solve the following dual problem.
min
Î±
1
2
Î±TQÎ±âˆ’ eTÎ±
subject to yTÎ± = 0, (2)
0 â‰¤ Î±i â‰¤ C, i = 1, . . . , l,
where e = [1, . . . , 1]T is the vector of all ones, Q is an l by l positive semidefinite
matrix, Qij â‰¡ yiyjK(xi,xj), and K(xi,xj) â‰¡ Ï†(xi)TÏ†(xj) is the kernel function.
3
After problem (2) is solved, using the primal-dual relationship, the optimal w
satisfies
w =
lâˆ‘
i=1
yiÎ±iÏ†(xi) (3)
and the decision function is
sgn
(
wTÏ†(x) + b
)
= sgn
(
lâˆ‘
i=1
yiÎ±iK(xi,x) + b
)
.
We store yiÎ±i âˆ€i, b, label names,4 support vectors, and other information such as
kernel parameters in the model for prediction.
2.2 Î½-Support Vector Classification
The Î½-support vector classification (SchoÌˆlkopf et al., 2000) introduces a new parameter
Î½ âˆˆ (0, 1]. It is proved that Î½ an upper bound on the fraction of training errors and
a lower bound of the fraction of support vectors.
Given training vectors xi âˆˆ Rn, i = 1, . . . , l, in two classes, and a vector y âˆˆ Rl
such that yi âˆˆ {1,âˆ’1}, the primal optimization problem is
min
w,b,Î¾,Ï
1
2
wTw âˆ’ Î½Ï+ 1
l
lâˆ‘
i=1
Î¾i
subject to yi(w
TÏ†(xi) + b) â‰¥ Ïâˆ’ Î¾i, (4)
Î¾i â‰¥ 0, i = 1, . . . , l, Ï â‰¥ 0.
The dual problem is
min
Î±
1
2
Î±TQÎ±
subject to 0 â‰¤ Î±i â‰¤ 1/l, i = 1, . . . , l, (5)
eTÎ± â‰¥ Î½, yTÎ± = 0,
where Qij = yiyjK(xi,xj). Chang and Lin (2001) show that problem (5) is feasible
if and only if
Î½ â‰¤ 2 min(#yi = +1,#yi = âˆ’1)
l
â‰¤ 1,
so the usable range of Î½ is smaller than (0, 1].
4In LIBSVM, any integer can be a label name, so we map label names to Â±1 by assigning the first
training instance to have y1 = +1.
4
The decision function is
sgn
(
lâˆ‘
i=1
yiÎ±iK(xi,x) + b
)
.
It is shown that eTÎ± â‰¥ Î½ can be replaced by eTÎ± = Î½ (Crisp and Burges, 2000;
Chang and Lin, 2001). In LIBSVM, we solve a scaled version of problem (5) because
numerically Î±i may be too small due to the constraint Î±i â‰¤ 1/l.
min
Î±Ì„
1
2
Î±Ì„TQÎ±Ì„
subject to 0 â‰¤ Î±Ì„i â‰¤ 1, i = 1, . . . , l, (6)
eT Î±Ì„ = Î½l, yT Î±Ì„ = 0.
If Î± is optimal for the dual problem (5) and Ï is optimal for the primal problem
(4), Chang and Lin (2001) show that Î±/Ï is an optimal solution of C-SVM with
C = 1/(Ïl). Thus, in LIBSVM, we output (Î±/Ï, b/Ï) in the model.5
2.3 Distribution Estimation (One-class SVM)
One-class SVM was proposed by SchoÌˆlkopf et al. (2001) for estimating the support of
a high-dimensional distribution. Given training vectors xi âˆˆ Rn, i = 1, . . . , l without
any class information, the primal problem of one-class SVM is
min
w,Î¾,Ï
1
2
wTw âˆ’ Ï+ 1
Î½l
lâˆ‘
i=1
Î¾i
subject to wTÏ†(xi) â‰¥ Ïâˆ’ Î¾i,
Î¾i â‰¥ 0, i = 1, . . . , l.
The dual problem is
min
Î±
1
2
Î±TQÎ±
subject to 0 â‰¤ Î±i â‰¤ 1/(Î½l), i = 1, . . . , l, (7)
eTÎ± = 1,
where Qij = K(xi,xj) = Ï†(xi)
TÏ†(xj). The decision function is
sgn
(
lâˆ‘
i=1
Î±iK(xi,x)âˆ’ Ï
)
.
5More precisely, solving (6) obtains ÏÌ„ = Ïl. Because Î±Ì„ = lÎ±, we have Î±/Ï = Î±Ì„/ÏÌ„. Hence, in
LIBSVM, we calculate Î±Ì„/ÏÌ„.
5
Similar to the case of Î½-SVC, in LIBSVM, we solve a scaled version of (7).
min
Î±
1
2
Î±TQÎ±
subject to 0 â‰¤ Î±i â‰¤ 1, i = 1, . . . , l, (8)
eTÎ± = Î½l.
2.4 -Support Vector Regression (-SVR)
Consider a set of training points, {(x1, z1), . . . , (xl, zl)}, where xi âˆˆ Rn is a feature
vector and zi âˆˆ R1 is the target output. Under given parameters C > 0 and  > 0,
the standard form of support vector regression (Vapnik, 1998) is
min
w,b,Î¾,Î¾âˆ—
1
2
wTw + C
lâˆ‘
i=1
Î¾i + C
lâˆ‘
i=1
Î¾âˆ—i
subject to wTÏ†(xi) + bâˆ’ zi â‰¤ + Î¾i,
zi âˆ’wTÏ†(xi)âˆ’ b â‰¤ + Î¾âˆ—i ,
Î¾i, Î¾
âˆ—
i â‰¥ 0, i = 1, . . . , l.
The dual problem is
min
Î±,Î±âˆ—
1
2
(Î±âˆ’Î±âˆ—)TQ(Î±âˆ’Î±âˆ—) + 
lâˆ‘
i=1
(Î±i + Î±
âˆ—
i ) +
lâˆ‘
i=1
zi(Î±i âˆ’ Î±âˆ—i )
subject to eT (Î±âˆ’Î±âˆ—) = 0, (9)
0 â‰¤ Î±i, Î±âˆ—i â‰¤ C, i = 1, . . . , l,
where Qij = K(xi,xj) â‰¡ Ï†(xi)TÏ†(xj).
After solving problem (9), the approximate function is
lâˆ‘
i=1
(âˆ’Î±i + Î±âˆ—i )K(xi,x) + b.
In LIBSVM, we output Î±âˆ— âˆ’Î± in the model.
2.5 Î½-Support Vector Regression (Î½-SVR)
Similar to Î½-SVC, for regression, SchoÌˆlkopf et al. (2000) use a parameter Î½ âˆˆ (0, 1]
to control the number of support vectors. The parameter  in -SVR becomes a
6
parameter here. With (C, Î½) as parameters, Î½-SVR solves
min
w,b,Î¾,Î¾âˆ—,
1
2
wTw + C(Î½+
1
l
lâˆ‘
i=1
(Î¾i + Î¾
âˆ—
i ))
subject to (wTÏ†(xi) + b)âˆ’ zi â‰¤ + Î¾i,
zi âˆ’ (wTÏ†(xi) + b) â‰¤ + Î¾âˆ—i ,
Î¾i, Î¾
âˆ—
i â‰¥ 0, i = 1, . . . , l,  â‰¥ 0.
The dual problem is
min
Î±,Î±âˆ—
1
2
(Î±âˆ’Î±âˆ—)TQ(Î±âˆ’Î±âˆ—) + zT (Î±âˆ’Î±âˆ—)
subject to eT (Î±âˆ’Î±âˆ—) = 0, eT (Î±+Î±âˆ—) â‰¤ CÎ½, (10)
0 â‰¤ Î±i, Î±âˆ—i â‰¤ C/l, i = 1, . . . , l.
The approximate function is
lâˆ‘
i=1
(âˆ’Î±i + Î±âˆ—i )K(xi,x) + b.
Similar to Î½-SVC, Chang and Lin (2002) show that the inequality eT (Î±+Î±âˆ—) â‰¤ CÎ½
can be replaced by an equality. Moreover, C/l may be too small because users often
choose C to be a small constant like one. Thus, in LIBSVM, we treat the user-
specified regularization parameter as C/l. That is, CÌ„ = C/l is what users specified
and LIBSVM solves the following problem.
min
Î±,Î±âˆ—
1
2
(Î±âˆ’Î±âˆ—)TQ(Î±âˆ’Î±âˆ—) + zT (Î±âˆ’Î±âˆ—)
subject to eT (Î±âˆ’Î±âˆ—) = 0, eT (Î±+Î±âˆ—) = CÌ„lÎ½,
0 â‰¤ Î±i, Î±âˆ—i â‰¤ CÌ„, i = 1, . . . , l.
Chang and Lin (2002) prove that -SVR with parameters (CÌ„, ) has the same solution
as Î½-SVR with parameters (lCÌ„, Î½).
3 Performance Measures, Basic Usage, and Code
Organization
This section describes LIBSVMâ€™s evaluation measures, shows some simple examples
of running LIBSVM, and presents the code structure.
7
3.1 Performance Measures
After solving optimization problems listed in previous sections, users can apply de-
cision functions to predict labels (target values) of testing data. Let x1, . . . ,xlÌ„ be
the testing data and f(x1), . . . , f(xlÌ„) be decision values (target values for regression)
predicted by LIBSVM. If the true labels (true target values) of testing data are known
and denoted as yi, . . . , ylÌ„, we evaluate the prediction results by the following measures.
3.1.1 Classification
Accuracy
=
# correctly predicted data
# total testing data
Ã— 100%
3.1.2 Regression
LIBSVM outputs MSE (mean squared error) and r2 (squared correlation coefficient).
MSE =
1
lÌ„
lÌ„âˆ‘
i=1
(f(xi)âˆ’ yi)2 ,
r2 =
(
lÌ„
âˆ‘lÌ„
i=1 f(xi)yi âˆ’
âˆ‘lÌ„
i=1 f(xi)
âˆ‘lÌ„
i=1 yi
)2(
lÌ„
âˆ‘lÌ„
i=1 f(xi)
2 âˆ’
(âˆ‘lÌ„
i=1 f(xi)
)2)(
lÌ„
âˆ‘lÌ„
i=1 y
2
i âˆ’
(âˆ‘lÌ„
i=1 yi
)2) .
3.2 A Simple Example of Running LIBSVM
While detailed instructions of using LIBSVM are available in the README file of the
package and the practical guide by Hsu et al. (2003), here we give a simple example.
LIBSVM includes a sample data set heart scale of 270 instances. We split the
data to a training set heart scale.tr (170 instances) and a testing set heart scale.te.
$ python tools/subset.py heart_scale 170 heart_scale.tr heart_scale.te
The command svm-train solves an SVM optimization problem to produce a model.6
$ ./svm-train heart_scale.tr
*
optimization finished, #iter = 87
nu = 0.471645
6The default solver is C-SVC using the RBF kernel (48) with C = 1 and Î³ = 1/n.
8
obj = -67.299458, rho = 0.203495
nSV = 88, nBSV = 72
Total nSV = 88
Next, the command svm-predict uses the obtained model to classify the testing set.
$ ./svm-predict heart_scale.te heart_scale.tr.model output
Accuracy = 83% (83/100) (classification)
The file output contains predicted class labels.
3.3 Code Organization
All LIBSVMâ€™s training and testing algorithms are implemented in the file svm.cpp.
The two main sub-routines are svm train and svm predict. The training procedure
is more sophisticated, so we give the code organization in Figure 1.
From Figure 1, for classification, svm train decouples a multi-class problem to
two-class problems (see Section 7) and calls svm train one several times. For re-
gression and one-class SVM, it directly calls svm train one. The probability outputs
for classification and regression are also handled in svm train. Then, according
to the SVM formulation, svm train one calls a corresponding sub-routine such as
solve c svc for C-SVC and solve nu svc for Î½-SVC. All solve * sub-routines call
the solver Solve after preparing suitable input values. The sub-routine Solve min-
imizes a general form of SVM optimization problems; see (11) and (22). Details of
the sub-routine Solve are described in Sections 4-6.
4 Solving the Quadratic Problems
This section discusses algorithms used in LIBSVM to solve dual quadratic problems
listed in Section 2. We split the discussion to two parts. The first part considers
optimization problems with one linear constraint, while the second part checks those
with two linear constraints.
9
svm train
svm train one
solve c svc solve nu svc
Solve
. . . Various SVM formulations
Two-class SVC, SVR, one-class SVM
Main training sub-routine
Solving problems (11) and (22)
Figure 1: LIBSVMâ€™s code organization for training. All sub-routines are in svm.cpp.
4.1 Quadratic Problems with One Linear Constraint: C-SVC,
-SVR, and One-class SVM
We consider the following general form of C-SVC, -SVR, and one-class SVM.
min
Î±
f(Î±)
subject to yTÎ± = âˆ†, (11)
0 â‰¤ Î±t â‰¤ C, t = 1, . . . , l,
where
f(Î±) â‰¡ 1
2
Î±TQÎ±+ pTÎ±
and yt = Â±1, t = 1, . . . , l. The constraint yTÎ± = 0 is called a linear constraint. It can
be clearly seen that C-SVC and one-class SVM are already in the form of problem
(11). For -SVR, we use the following reformulation of Eq. (9).
min
Î±,Î±âˆ—
1
2
[
(Î±âˆ—)T ,Î±T
] [ Q âˆ’Q
âˆ’Q Q
] [
Î±âˆ—
Î±
]
+
[
eT âˆ’ zT , eT + zT
] [Î±âˆ—
Î±
]
subject to yT
[
Î±âˆ—
Î±
]
= 0, 0 â‰¤ Î±t, Î±âˆ—t â‰¤ C, t = 1, . . . , l,
where
y = [1, . . . , 1ï¸¸ ï¸·ï¸· ï¸¸
l
,âˆ’1, . . . ,âˆ’1ï¸¸ ï¸·ï¸· ï¸¸
l
]T .
We do not assume that Q is positive semi-definite (PSD) because sometimes non-PSD
kernel matrices are used.
10
4.1.1 Decomposition Method for Dual Problems
The main difficulty for solving problem (11) is that Q is a dense matrix and may be
too large to be stored. In LIBSVM, we consider a decomposition method to conquer
this difficulty. Some earlier works on decomposition methods for SVM include, for
example, Osuna et al. (1997a); Joachims (1998); Platt (1998); Keerthi et al. (2001);
Hsu and Lin (2002b). Subsequent developments include, for example, Fan et al.
(2005); Palagi and Sciandrone (2005); Glasmachers and Igel (2006). A decomposition
method modifies only a subset of Î± per iteration, so only some columns of Q are
needed. This subset of variables, denoted as the working set B, leads to a smaller
optimization sub-problem. An extreme case of the decomposition methods is the
Sequential Minimal Optimization (SMO) (Platt, 1998), which restricts B to have only
two elements. Then, at each iteration, we solve a simple two-variable problem without
needing any optimization software. LIBSVM considers an SMO-type decomposition
method proposed in Fan et al. (2005).
Algorithm 1 (An SMO-type decomposition method in Fan et al., 2005)
1. Find Î±1 as the initial feasible solution. Set k = 1.
2. If Î±k is a stationary point of problem (2), stop. Otherwise, find a two-element
working set B = {i, j} by WSS 1 (described in Section 4.1.2). Define N â‰¡
{1, . . . , l}\B. Let Î±kB and Î±kN be sub-vectors of Î±k corresponding to B and N ,
respectively.
3. If aij â‰¡ Kii +Kjj âˆ’ 2Kij > 0,7
Solve the following sub-problem with the variable Î±B = [Î±i Î±j]
T .
min
Î±i,Î±j
1
2
[
Î±i Î±j
] [Qii Qij
Qij Qjj
] [
Î±i
Î±j
]
+ (pB +QBNÎ±
k
N)
T
[
Î±i
Î±j
]
subject to 0 â‰¤ Î±i, Î±j â‰¤ C, (12)
yiÎ±i + yjÎ±j = âˆ†âˆ’ yTNÎ±kN ,
else
7We abbreviate K(xi,xj) to Kij .
11
Let Ï„ be a small positive constant and solve
min
Î±i,Î±j
1
2
[
Î±i Î±j
] [Qii Qij
Qij Qjj
] [
Î±i
Î±j
]
+ (pB +QBNÎ±
k
N)
T
[
Î±i
Î±j
]
+
Ï„ âˆ’ aij
4
((Î±i âˆ’ Î±ki )2 + (Î±j âˆ’ Î±kj )2) (13)
subject to constraints of problem (12).
4. SetÎ±k+1B to be the optimal solution of sub-problem (12) or (13), andÎ±
k+1
N â‰¡ Î±kN .
Set k â† k + 1 and go to Step 2.
Note that B is updated at each iteration, but for simplicity, we use B instead of
Bk. If Q is PSD, then aij > 0. Thus sub-problem (13) is used only to handle the
situation where Q is non-PSD.
4.1.2 Stopping Criteria and Working Set Selection
The Karush-Kuhn-Tucker (KKT) optimality condition of problem (11) implies that
a feasible Î± is a stationary point of (11) if and only if there exists a number b and
two nonnegative vectors Î» and Î¾ such that
âˆ‡f(Î±) + by = Î»âˆ’ Î¾,
Î»iÎ±i = 0, Î¾i(C âˆ’ Î±i) = 0, Î»i â‰¥ 0, Î¾i â‰¥ 0, i = 1, . . . , l, (14)
where âˆ‡f(Î±) â‰¡ QÎ± + p is the gradient of f(Î±). Note that if Q is PSD, from the
primal-dual relationship, Î¾, b, and w generated by Eq. (3) form an optimal solution
of the primal problem. The condition (14) can be rewritten as
âˆ‡if(Î±) + byi
{
â‰¥ 0 if Î±i < C,
â‰¤ 0 if Î±i > 0.
(15)
Since yi = Â±1, condition (15) is equivalent to that there exists b such that
m(Î±) â‰¤ b â‰¤M(Î±),
where
m(Î±) â‰¡ max
iâˆˆIup(Î±)
âˆ’yiâˆ‡if(Î±) and M(Î±) â‰¡ min
iâˆˆIlow(Î±)
âˆ’yiâˆ‡if(Î±),
and
Iup(Î±) â‰¡ {t | Î±t < C, yt = 1 or Î±t > 0, yt = âˆ’1}, and
Ilow(Î±) â‰¡ {t | Î±t < C, yt = âˆ’1 or Î±t > 0, yt = 1}.
12
That is, a feasible Î± is a stationary point of problem (11) if and only if
m(Î±) â‰¤M(Î±). (16)
From (16), a suitable stopping condition is
m(Î±k)âˆ’M(Î±k) â‰¤ , (17)
where  is the tolerance.
For the selection of the working set B, we use the following procedure from Section
II of Fan et al. (2005).
WSS 1
1. For all t, s, define
ats â‰¡ Ktt +Kss âˆ’ 2Kts, bts â‰¡ âˆ’ytâˆ‡tf(Î±k) + ysâˆ‡sf(Î±k) > 0, (18)
and
aÌ„ts â‰¡
{
ats if ats > 0,
Ï„ otherwise.
Select
i âˆˆ arg max
t
{âˆ’ytâˆ‡tf(Î±k) | t âˆˆ Iup(Î±k)},
j âˆˆ arg min
t
{
âˆ’ b
2
it
aÌ„it
| t âˆˆ Ilow(Î±k),âˆ’ytâˆ‡tf(Î±k) < âˆ’yiâˆ‡if(Î±k)
}
. (19)
2. Return B = {i, j}.
The procedure selects a pair {i, j} approximately minimizing the function value; see
the term âˆ’b2it/aÌ„it in Eq. (19).
4.1.3 Solving the Two-variable Sub-problem
Details of solving the two-variable sub-problem in Eqs. (12) and (13) are deferred to
Section 6, where a more general sub-problem is discussed.
4.1.4 Maintaining the Gradient
From the discussion in Sections 4.1.1 and 4.1.2, the main operations per iteration
are on finding QBNÎ±
k
N + pB for constructing the sub-problem (12), and calculating
13
âˆ‡f(Î±k) for the working set selection and the stopping condition. These two opera-
tions can be considered together because
QBNÎ±
k
N + pB = âˆ‡Bf(Î±k)âˆ’QBBÎ±kB (20)
and
âˆ‡f(Î±k+1) = âˆ‡f(Î±k) +Q:,B(Î±k+1B âˆ’Î±
k
B), (21)
where |B|  |N | and Q:,B is the sub-matrix of Q including columns in B. If at the
kth iteration we already have âˆ‡f(Î±k), then Eq. (20) can be used to construct the
sub-problem. After the sub-problem is solved, Eq. (21) is employed to have the next
âˆ‡f(Î±k+1). Therefore, LIBSVM maintains the gradient throughout the decomposition
method.
4.1.5 The Calculation of b or Ï
After the solution Î± of the dual optimization problem is obtained, the variables b or
Ï must be calculated as they are used in the decision function.
Note that b of C-SVC and -SVR plays the same role as âˆ’Ï in one-class SVM, so
we define Ï = âˆ’b and discuss how to find Ï. If there exists Î±i such that 0 < Î±i < C,
then from the KKT condition (16), Ï = yiâˆ‡if(Î±). In LIBSVM, for numerical stability,
we average all these values.
Ï =
âˆ‘
i:0<Î±i<C
yiâˆ‡if(Î±)
|{i | 0 < Î±i < C}|
.
For the situation that no Î±i satisfying 0 < Î±i < C, the KKT condition (16) becomes
âˆ’M(Î±) = max{yiâˆ‡if(Î±) | Î±i = 0, yi = âˆ’1 or Î±i = C, yi = 1}
â‰¤ Ï
â‰¤ âˆ’m(Î±) = min{yiâˆ‡if(Î±) | Î±i = 0, yi = 1 or Î±i = C, yi = âˆ’1}.
We take Ï the midpoint of the preceding range.
4.1.6 Initial Values
Algorithm 1 requires an initial feasible Î±. For C-SVC and -SVR, because the zero
vector is feasible, we select it as the initial Î±.
For one-class SVM, the scaled form (8) requires that
0 â‰¤ Î±i â‰¤ 1, and
lâˆ‘
i=1
Î±i = Î½l.
14
We let the first bÎ½lc elements have Î±i = 1 and the (bÎ½lc + 1)st element have Î±i =
Î½l âˆ’ bÎ½lc.
4.1.7 Convergence of the Decomposition Method
Fan et al. (2005, Section III) and Chen et al. (2006) discuss the convergence of Algo-
rithm 1 in detail. For the rate of linear convergence, List and Simon (2009) prove a
result without making the assumption used in Chen et al. (2006).
4.2 Quadratic Problems with Two Linear Constraints: Î½-
SVC and Î½-SVR
From problems (6) and (10), both Î½-SVC and Î½-SVR can be written as the following
general form.
min
Î±
1
2
Î±TQÎ±+ pTÎ±
subject to yTÎ± = âˆ†1, (22)
eTÎ± = âˆ†2,
0 â‰¤ Î±t â‰¤ C, t = 1, . . . , l.
The main difference between problems (11) and (22) is that (22) has two linear
constraints yTÎ± = âˆ†1 and e
TÎ± = âˆ†2. The optimization algorithm is very similar to
that for (11), so we describe only differences.
4.2.1 Stopping Criteria and Working Set Selection
Let f(Î±) be the objective function of problem (22). By the same derivation in Sec-
tion 4.1.2, The KKT condition of problem (22) implies that there exist b and Ï such
that
âˆ‡if(Î±)âˆ’ Ï+ byi
{
â‰¥ 0 if Î±i < C,
â‰¤ 0 if Î±i > 0.
(23)
Define
r1 â‰¡ Ïâˆ’ b and r2 â‰¡ Ï+ b. (24)
If yi = 1, (23) becomes
âˆ‡if(Î±)âˆ’ r1
{
â‰¥ 0 if Î±i < C,
â‰¤ 0 if Î±i > 0.
(25)
15
if yi = âˆ’1, (23) becomes
âˆ‡if(Î±)âˆ’ r2
{
â‰¥ 0 if Î±i < C,
â‰¤ 0 if Î±i > 0.
(26)
Hence, given a tolerance  > 0, the stopping condition is
max (mp(Î±)âˆ’Mp(Î±),mn(Î±)âˆ’Mn(Î±)) < , (27)
where
mp(Î±) â‰¡ max
iâˆˆIup(Î±),yi=1
âˆ’yiâˆ‡if(Î±), Mp(Î±) â‰¡ min
iâˆˆIlow(Î±),yi=1
âˆ’yiâˆ‡if(Î±), and
mn(Î±) â‰¡ max
iâˆˆIup(Î±),yi=âˆ’1
âˆ’yiâˆ‡if(Î±), Mn(Î±) â‰¡ min
iâˆˆIlow(Î±),yi=âˆ’1
âˆ’yiâˆ‡if(Î±).
The following working set selection is extended from WSS 1.
WSS 2 (Extension of WSS 1 for Î½-SVM)
1. Find
ip âˆˆ argmp(Î±k),
jp âˆˆ arg min
t
{
âˆ’
b2ipt
aÌ„ipt
| yt = 1,Î±t âˆˆ Ilow(Î±k),âˆ’ytâˆ‡tf(Î±k) < âˆ’yipâˆ‡ipf(Î±k)
}
.
2. Find
in âˆˆ argmn(Î±k),
jn âˆˆ arg min
t
{
âˆ’
b2int
aÌ„int
| yt = âˆ’1,Î±t âˆˆ Ilow(Î±k),âˆ’ytâˆ‡tf(Î±k) < âˆ’yinâˆ‡inf(Î±k)
}
.
3. Return {ip, jp} or {in, jn} depending on which one gives smaller âˆ’b2ij/aÌ„ij.
4.2.2 The Calculation of b and Ï
We have shown that the KKT condition of problem (22) implies Eqs. (25) and (26)
according to yi = 1 and âˆ’1, respectively. Now we consider the case of yi = 1. If
there exists Î±i such that 0 < Î±i < C, then we obtain r1 = âˆ‡if(Î±). In LIBSVM, for
numerical stability, we average these values.
r1 =
âˆ‘
i:0<Î±i<C,yi=1
âˆ‡if(Î±)
|{i | 0 < Î±i < C, yi = 1}|
.
If there is no Î±i such that 0 < Î±i < C, then r1 satisfies
max
Î±i=C,yi=1
âˆ‡if(Î±) â‰¤ r1 â‰¤ min
Î±i=0,yi=1
âˆ‡if(Î±).
16
We take r1 the midpoint of the previous range.
For the case of yi = âˆ’1, we can calculate r2 in a similar way.
After r1 and r2 are obtained, from Eq. (24),
Ï =
r1 + r2
2
and âˆ’ b = r1 âˆ’ r2
2
.
4.2.3 Initial Values
For Î½-SVC, the scaled form (6) requires that
0 â‰¤ Î±i â‰¤ 1,
âˆ‘
i:yi=1
Î±i =
Î½l
2
, and
âˆ‘
i:yi=âˆ’1
Î±i =
Î½l
2
.
We let the first Î½l/2 elements of Î±i with yi = 1 to have the value one.
8 The situation
for yi = âˆ’1 is similar. The same setting is applied to Î½-SVR.
5 Shrinking and Caching
This section discusses two implementation tricks (shrinking and caching) for the de-
composition method and investigates the computational complexity of Algorithm 1.
5.1 Shrinking
An optimal solution Î± of the SVM dual problem may contain some bounded elements
(i.e., Î±i = 0 or C). These elements may have already been bounded in the middle of
the decomposition iterations. To save the training time, the shrinking technique tries
to identify and remove some bounded elements, so a smaller optimization problem is
solved (Joachims, 1998). The following theorem theoretically supports the shrinking
technique by showing that at the final iterations of Algorithm 1 in Section 4.1.2, only
a small set of variables is still changed.
Theorem 5.1 (Theorem IV in Fan et al., 2005) Consider problem (11) and as-
sume Q is positive semi-definite.
1. The following set is independent of any optimal solution Î±Ì„.
I â‰¡ {i | âˆ’yiâˆ‡if(Î±Ì„) > M(Î±Ì„) or âˆ’ yiâˆ‡if(Î±Ì„) < m(Î±Ì„)}.
Further, for every i âˆˆ I, problem (11) has a unique and bounded optimal solution
at Î±i.
8Special care must be made as Î½l/2 may not be an integer. See also Section 4.1.6.
17
2. Assume Algorithm 1 generates an infinite sequence {Î±k}. There exists kÌ„ such
that after k â‰¥ kÌ„, every Î±ki , i âˆˆ I has reached the unique and bounded optimal
solution. That is, Î±ki remains the same in all subsequent iterations. In addition,
âˆ€k â‰¥ kÌ„:
i 6âˆˆ {t |M(Î±k) â‰¤ âˆ’ytâˆ‡tf(Î±k) â‰¤ m(Î±k)}.
If we denote A as the set containing elements not shrunk at the kth iteration,
then instead of solving problem (11), the decomposition method works on a smaller
problem.
min
Î±A
1
2
Î±TAQAAÎ±A + (pA +QANÎ±
k
N)
TÎ±A
subject to 0 â‰¤ Î±i â‰¤ C, âˆ€i âˆˆ A, (28)
yTAÎ±A = âˆ†âˆ’ yTNÎ±kN ,
where N = {1, . . . , l}\A is the set of shrunk variables. Note that in LIBSVM, we
always rearrange elements of Î±,y, and p to maintain that A = {1, . . . , |A|}. Details
of the index rearrangement are in Section 5.4.
After solving problem (28), we may find that some elements are wrongly shrunk.
When that happens, the original problem (11) is reoptimized from a starting point
Î± = [ Î±AÎ±N ], where Î±A is optimal for problem (28) and Î±N corresponds to shrunk
bounded variables.
In LIBSVM, we start the shrinking procedure in an early stage. The procedure is
as follows.
1. After every min(l, 1000) iterations, we try to shrink some variables. Note that
throughout the iterative process, we have
m(Î±k) > M(Î±k) (29)
because the condition (17) is not satisfied yet. Following Theorem 5.1, we
conjecture that variables in the following set can be shrunk.
{t | âˆ’ytâˆ‡tf(Î±k) > m(Î±k), t âˆˆ Ilow(Î±k), Î±kt is bounded}âˆª
{t | âˆ’ytâˆ‡tf(Î±k) < M(Î±k), t âˆˆ Iup(Î±k), Î±kt is bounded}
= {t | âˆ’ytâˆ‡tf(Î±k) > m(Î±k), Î±kt = C, yt = 1 or Î±kt = 0, yt = âˆ’1}âˆª
{t | âˆ’ytâˆ‡tf(Î±k) < M(Î±k), Î±kt = 0, yt = 1 or Î±kt = C, yt = âˆ’1}.
(30)
Thus, the size of the set A is gradually reduced in every min(l, 1000) iterations.
The problem (28), and the way of calculating m(Î±k) and M(Î±k) are adjusted
accordingly.
18
2. The preceding shrinking strategy is sometimes too aggressive. Hence, when the
decomposition method achieves the following condition for the first time.
m(Î±k) â‰¤M(Î±k) + 10, (31)
where  is the specified stopping tolerance, we reconstruct the gradient (details
in Section 5.3). Then, the shrinking procedure can be performed based on more
accurate information.
3. Once the stopping condition
m(Î±k) â‰¤M(Î±k) +  (32)
of the smaller problem (28) is reached, we must check if the stopping condition
of the original problem (11) has been satisfied. If not, then we reactivate all
variables by setting A = {1, . . . , l} and start the same shrinking procedure on
the problem (28).
Note that in solving the shrunk problem (28), we only maintain its gradient
QAAÎ±A + QANÎ±N + pA (see also Section 4.1.4). Hence, when we reactivate
all variables to reoptimize the problem (11), we must reconstruct the whole
gradient âˆ‡f(Î±). Details are discussed in Section 5.3.
For Î½-SVC and Î½-SVR, because the stopping condition (27) is different from (17),
variables being shrunk are different from those in (30). For yt = 1, we shrink elements
in the following set
{t | âˆ’ytâˆ‡tf(Î±k) > mp(Î±k), Î±t = C, yt = 1}âˆª
{t | âˆ’ytâˆ‡tf(Î±k) < Mp(Î±k), Î±t = 0, yt = 1}.
For yt = âˆ’1, we consider the following set.
{t | âˆ’ytâˆ‡tf(Î±k) > mn(Î±k), Î±t = 0, yt = âˆ’1}âˆª
{t | âˆ’ytâˆ‡tf(Î±k) < Mn(Î±k), Î±t = C, yt = âˆ’1}.
5.2 Caching
Caching is an effective technique for reducing the computational time of the decompo-
sition method. Because Q may be too large to be stored in the computer memory, Qij
elements are calculated as needed. We can use available memory (called kernel cache)
to store some recently used Qij (Joachims, 1998). Then, some kernel elements may
19
not need to be recalculated. Theorem 5.1 also supports the use of caching because
in final iterations, only certain columns of the matrix Q are still needed. If the cache
already contains these columns, we can save kernel evaluations in final iterations.
In LIBSVM, we consider a simple least-recent-use caching strategy. We use a
circular list of structures, where each structure is defined as follows.
struct head_t
{
head_t *prev, *next; // a circular list
Qfloat *data;
int len; // data[0,len) is cached in this entry
};
A structure stores the first len elements of a kernel column. Using pointers prev and
next, it is easy to insert or delete a column. The circular list is maintained so that
structures are ordered from the least-recent-used one to the most-recent-used one.
Because of shrinking, columns cached in the computer memory may be in different
length. Assume the ith column is needed and Q1:t,i have been cached. If t â‰¤ |A|, we
calculate Qt+1:|A|,i and store Q1:|A|,i in the cache. If t > |A|, the desired Q1:|A|,i are
already in the cache. In this situation, we do not change the cached contents of the
ith column.
5.3 Reconstructing the Gradient
If condition (31) or (32) is satisfied, LIBSVM reconstructs the gradient. Because
âˆ‡if(Î±), i = 1, . . . , |A| have been maintained in solving the smaller problem (28),
what we need is to calculate âˆ‡if(Î±), i = |A| + 1, . . . , l. To decrease the cost of this
reconstruction, throughout iterations we maintain a vector GÌ„ âˆˆ Rl.
GÌ„i = C
âˆ‘
j:Î±j=C
Qij, i = 1, . . . , l. (33)
Then, for i /âˆˆ A,
âˆ‡if(Î±) =
lâˆ‘
j=1
QijÎ±j + pi = GÌ„i + pi +
âˆ‘
j:jâˆˆA
0<Î±j<C
QijÎ±j. (34)
Note that we use the fact that if j /âˆˆ A, then Î±j = 0 or C.
20
The calculation of âˆ‡f(Î±) via Eq. (34) involves a two-level loop over i and j.
Using i or j first may result in a very different number of Qij evaluations. We discuss
the differences next.
1. i first: for |A| + 1 â‰¤ i â‰¤ l, calculate Qi,1:|A|. Although from Eq. (34), only
{Qij | 0 < Î±j < C, j âˆˆ A} are needed, our implementation obtains all Qi,1:|A|
(i.e., {Qij | j âˆˆ A}). Hence, this case needs at most
(l âˆ’ |A|) Â· |A| (35)
kernel evaluations. Note that LIBSVM uses a column-based caching implemen-
tation. Due to the symmetry of Q, Qi,1:|A| is part of Qâ€™s ith column and may
have been cached. Thus, Eq. (35) is only an upper bound.
2. j first: let
F â‰¡ {j | 1 â‰¤ j â‰¤ |A| and 0 < Î±j < C}.
For each j âˆˆ F , calculate Q1:l,j. Though only Q|A|+1:l,j is needed in calculating
âˆ‡if(Î±), i = |A|+ 1, . . . , l, we must get the whole column because of our cache
implementation.9 Thus, this strategy needs no more than
l Â· |F | (36)
kernel evaluations. This is an upper bound because certain kernel columns (e.g.,
Q1:|A|,j, j âˆˆ A) may be already in the cache and do not need to be recalculated.
We may choose a method by comparing (35) and (36). However, the decision depends
on whether Qâ€™s elements have been cached. If the cache is large enough, then elements
of Qâ€™s first |A| columns tend to be in the cache because they have been used recently.
In contrast, Qi,1:|A|, i /âˆˆ A needed by method 1 may be less likely in the cache because
columns not in A are not used to solve problem (28). In such a situation, method 1
may require almost (lâˆ’|A|) Â· |A| kernel valuations, while method 2 needs much fewer
evaluations than l Â· |F |.
Because method 2 takes an advantage of the cache implementation, we slightly
lower the estimate in Eq. (36) and use the following rule to decide the method of
calculating Eq. (34):
If (l/2) Â· |F | > (l âˆ’ |A|) Â· |A|
use method 1
Else
use method 2
9We always store the first |A| elements of a column.
21
This rule may not give the optimal choice because we do not take the cache contents
into account. However, we argue that in the worst scenario, the selected method by
the preceding rule is only slightly slower than the other method. This result can be
proved by making the following assumptions.
â€¢ A LIBSVM training procedure involves only two gradient reconstructions. The
first is performed when the 10 tolerance is achieved; see Eq. (31). The second
is in the end of the training procedure.
â€¢ Our rule assigns the same method to perform the two gradient reconstructions.
Moreover, these two reconstructions cost a similar amount of time.
We refer to â€œtotal training time of method xâ€ as the whole LIBSVM training time
(where method x is used for reconstructing gradients), and â€œreconstruction time of
method xâ€ as the time of one single gradient reconstruction via method x. We then
consider two situations.
1. Method 1 is chosen, but method 2 is better.
We have
Total time of method 1
â‰¤ (Total time of method 2) + 2 Â· (Reconstruction time of method 1)
â‰¤ 2 Â· (Total time of method 2). (37)
We explain the second inequality in detail. Method 2 for gradient reconstruction
requires l Â· |F | kernel elements; however, the number of kernel evaluations may
be smaller because some elements have been cached. Therefore,
l Â· |F | â‰¤ Total time of method 2. (38)
Because method 1 is chosen and Eq. (35) is an upper bound,
2 Â· (Reconstruction time of method 1) â‰¤ 2 Â· (l âˆ’ |A|) Â· |A| < l Â· |F |. (39)
Combining inequalities (38) and (39) leads to (37).
2. Method 2 is chosen, but method 1 is better.
We consider the worst situation where Qâ€™s first |A| columns are not in the cache.
As |A|+1, . . . , l are indices of shrunk variables, most likely the remaining lâˆ’|A|
22
Table 2: A comparison between two gradient reconstruction methods. The decom-
position method reconstructs the gradient twice after satisfying conditions (31) and
(32). We show in each row the number of kernel evaluations of a reconstruction. We
check two cache sizes to reflect the situations with/without enough cache. The last
two rows give the total training time (gradient reconstructions and other operations)
in seconds. We use the RBF kernel K(xi,xj) = exp(âˆ’Î³â€–xi âˆ’ xjâ€–2).
(a) a7a: C = 1, Î³ = 4,  = 0.001.
l = 16, 100 Cache = 1,000 MB Cache = 10 MB
Reconstruction |F | |A| Method 1 Method 2 Method 1 Method 2
First 10,597 12,476 0 21,470,526 45,213,024 170,574,272
Second 10,630 12,476 0 0 45,213,024 171,118,048
Training time â‡’ 102s 108s 341s 422s
No shrinking: 111s No shrinking: 381s
(b) ijcnn1: C = 16, Î³ = 4,  = 0.5.
l = 49, 900 Cache = 1,000 MB Cache = 10 MB
Reconstruction |F | |A| Method 1 Method 2 Method 1 Method 2
First 1,767 43,678 274,297,840 5,403,072 275,695,536 88,332,330
Second 2,308 6,023 263,843,538 28,274,195 264,813,241 115,346,805
Training time â‡’ 189s 46s 203s 116s
No shrinking: 42s No shrinking: 87s
columns of Q are not in the cache either and (l âˆ’ |A|) Â· |A| kernel evaluations
are needed for method 1. Because l Â· |F | â‰¤ 2 Â· (l âˆ’ |A|) Â· |A|,
(Reconstruction time of method 2) â‰¤ 2 Â· (Reconstruction time of method 1).
Therefore,
Total time of method 2
â‰¤ (Total time of method 1) + 2 Â· (Reconstruction time of method 1)
â‰¤ 2 Â· (Total time of method 1).
Table 2 compares the number of kernel evaluations in reconstructing the gradient.
We consider problems a7a and ijcnn1.10 Clearly, the proposed rule selects the better
method for both problems. We implement this technique after version 2.88 of LIBSVM.
10Available at http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets.
23
5.4 Index Rearrangement
In solving the smaller problem (28), we need only indices in A (e.g., Î±i, yi, and xi,
where i âˆˆ A). Thus, a naive implementation does not access array contents in a
continuous manner. Alternatively, we can maintain A = {1, . . . , |A|} by rearranging
array contents. This approach allows a continuous access of array contents, but
requires costs for the rearrangement. We decide to rearrange elements in arrays
because throughout the discussion in Sections 5.2-5.3, we assume that a cached ith
kernel column contains elements from the first to the tth (i.e., Q1:t,i), where t â‰¤ l. If
we do not rearrange indices so that A = {1, . . . , |A|}, then the whole column Q1:l,i
must be cached because l may be an element in A.
We rearrange indices by sequentially swapping pairs of indices. If t1 is going to
be shrunk, we find an index t2 that should stay and then swap them. Swapping two
elements in a vector Î± or y is easy, but swapping kernel elements in the cache is more
expensive. That is, we must swap (Qt1,i, Qt2,i) for every cached kernel column i. To
make the number of swapping operations small, we use the following implementation.
Starting from the first and the last indices, we identify the smallest t1 that should
leave the largest t2 that should stay. Then, (t1, t2) are swapped and we continue the
same procedure to identify the next pair.
5.5 A Summary of the Shrinking Procedure
We summarize the shrinking procedure in Algorithm 2.
Algorithm 2 (Extending Algorithm 1 to include the shrinking procedure)
Initialization
1. Let Î±1 be an initial feasible solution.
2. Calculate the initial âˆ‡f(Î±1) and GÌ„ in Eq. (33).
3. Initialize a counter so shrinking is conducted every min(l, 1000) iterations
4. Let A = {1, . . . , l}
For k = 1, 2, . . .
1. Decrease the shrinking counter
2. If the counter is zero, then shrinking is conducted.
(a) If condition (31) is satisfied for the first time, reconstruct the gradient
24
(b) Shrink A by removing elements in the set (30). The implementation
described in Section 5.4 ensures that A = {1, . . . , |A|}.
(c) Reset the shrinking counter
3. If Î±kA satisfies the stopping condition (32)
(a) Reconstruct the gradient
(b) If Î±k satisfies the stopping condition (32)
Return Î±k
Else
Reset A = {1, . . . , l} and set the counter to one11
4. Find a two-element working set B = {i, j} by WSS 1
5. Obtain Q1:|A|,i and Q1:|A|,j from cache or by calculation
6. Solve sub-problem (12) or (13) by procedures in Section 6. Update Î±k to
Î±k+1
7. Update the gradient by Eq. (21) and update the vector GÌ„
5.6 Is Shrinking Always Better?
We found that if the number of iterations is large, then shrinking can shorten the
training time. However, if we loosely solve the optimization problem (e.g., by using
a large stopping tolerance ), the code without using shrinking may be much faster.
In this situation, because of the small number of iterations, the time spent on all
decomposition iterations can be even less than one single gradient reconstruction.
Table 2 compares the total training time with/without shrinking. For a7a, we
use the default  = 0.001. Under the parameters C = 1 and Î³ = 4, the number
of iterations is more than 30,000. Then shrinking is useful. However, for ijcnn1, we
deliberately use a loose tolerance  = 0.5, so the number of iterations is only around
4,000. Because our shrinking strategy is quite aggressive, before the first gradient
reconstruction, only QA,A is in the cache. Then, we need many kernel evaluations for
reconstructing the gradient, so the implementation with shrinking is slower.
If enough iterations have been run, most elements in A correspond to free Î±i
(0 < Î±i < C); i.e., A â‰ˆ F . In contrast, if the number of iterations is small (e.g.,
ijcnn1 in Table 2), many bounded elements have not been shrunk and |F |  |A|.
Therefore, we can check the relation between |F | and |A| to conjecture if shrinking
11That is, shrinking is performed at the next iteration.
25
is useful. In LIBSVM, if shrinking is enabled and 2 Â· |F | < |A| in reconstructing the
gradient, we issue a warning message to indicate that the code may be faster without
shrinking.
5.7 Computational Complexity
While Section 4.1.7 has discussed the asymptotic convergence and the local conver-
gence rate of the decomposition method, in this section, we investigate the computa-
tional complexity.
From Section 4, two places consume most operations at each iteration: finding the
working set B by WSS 1 and calculating Q:,B(Î±
k+1
B âˆ’Î±kB) in Eq. (21).12 Each place
requires O(l) operations. However, if Q:,B is not available in the cache and assume
each kernel evaluation costs O(n), the cost becomes O(ln) for calculating a column
of kernel elements. Therefore, the complexity of Algorithm 1 is
1. #IterationsÃ—O(l) if most columns of Q are cached throughout iterations.
2. #IterationsÃ—O(nl) if columns of Q are not cached and each kernel evaluation
costs O(n).
Several works have studied the number of iterations of decomposition methods; see,
for example, List and Simon (2007). However, algorithms studied in these works
are slightly different from LIBSVM, so there is no theoretical result yet on LIBSVMâ€™s
number of iterations. Empirically, it is known that the number of iterations may
be higher than linear to the number of training data. Thus, LIBSVM may take
considerable training time for huge data sets. Many techniques, for example, Fine
and Scheinberg (2001); Lee and Mangasarian (2001); Keerthi et al. (2006); Segata and
Blanzieri (2010), have been developed to obtain an approximate model, but these are
beyond the scope of our discussion. In LIBSVM, we provide a simple sub-sampling
tool, so users can quickly train a small subset.
6 Unbalanced Data and Solving the Two-variable
Sub-problem
For some classification problems, numbers of data in different classes are unbalanced.
Some researchers (e.g., Osuna et al., 1997b, Section 2.5; Vapnik, 1998, Chapter 10.9)
12Note that because |B| = 2, once the sub-problem has been constructed, solving it takes only a
constant number of operations (see details in Section 6).
26
have proposed using different penalty parameters in the SVM formulation. For ex-
ample, the C-SVM problem becomes
min
w,b,Î¾
1
2
wTw + C+
âˆ‘
yi=1
Î¾i + C
âˆ’
âˆ‘
yi=âˆ’1
Î¾i
subject to yi(w
TÏ†(xi) + b) â‰¥ 1âˆ’ Î¾i, (40)
Î¾i â‰¥ 0, i = 1, . . . , l,
where C+ and Câˆ’ are regularization parameters for positive and negative classes,
respectively. LIBSVM supports this setting, so users can choose weights for classes.
The dual problem of problem (40) is
min
Î±
1
2
Î±TQÎ±âˆ’ eTÎ±
subject to 0 â‰¤ Î±i â‰¤ C+, if yi = 1,
0 â‰¤ Î±i â‰¤ Câˆ’, if yi = âˆ’1,
yTÎ± = 0.
A more general setting is to assign each instance xi a regularization parameter Ci.
If C is replaced by Ci, i = 1, . . . , l in problem (11), most results discussed in earlier
sections can be extended without problems.13 The major change of Algorithm 1 is
on solving the sub-problem (12), which now becomes
min
Î±i,Î±j
1
2
[
Î±i Î±j
] [Qii Qij
Qji Qjj
] [
Î±i
Î±j
]
+ (Qi,NÎ±N + pi)Î±i + (Qj,NÎ±N + pj)Î±j
subject to yiÎ±i + yjÎ±j = âˆ†âˆ’ yTNÎ±kN , (41)
0 â‰¤ Î±i â‰¤ Ci, 0 â‰¤ Î±j â‰¤ Cj.
Let Î±i = Î±
k
i + di and Î±j = Î±
k
j + dj. The sub-problem (41) can be written as
min
di,dj
1
2
[
di dj
] [Qii Qij
Qij Qjj
] [
di
dj
]
+
[
âˆ‡if(Î±k) âˆ‡jf(Î±k)
] [di
dj
]
subject to yidi + yjdj = 0,
âˆ’Î±ki â‰¤ di â‰¤ Ci âˆ’ Î±ki ,âˆ’Î±kj â‰¤ dj â‰¤ Cj âˆ’ Î±kj .
Define aij and bij as in Eq. (18), and dÌ‚i â‰¡ yidi, dÌ‚j â‰¡ yjdj. Using dÌ‚i = âˆ’dÌ‚j, the
objective function can be written as
1
2
aÌ„ij dÌ‚
2
j + bij dÌ‚j.
13This feature of using Ci,âˆ€i is not included in LIBSVM, but is available as an extension at
libsvmtools.
27
Minimizing the previous quadratic function leads to
Î±newi = Î±
k
i + yibij/aÌ„ij,
Î±newj = Î±
k
j âˆ’ yjbij/aÌ„ij. (42)
These two values may need to be modified because of bound constraints. We first
consider the case of yi 6= yj and re-write Eq. (42) as
Î±newi = Î±
k
i + (âˆ’âˆ‡if(Î±k)âˆ’âˆ‡jf(Î±k))/aÌ„ij,
Î±newj = Î±
k
j + (âˆ’âˆ‡if(Î±k)âˆ’âˆ‡jf(Î±k))/aÌ„ij.
In the following figure, a box is generated according to bound constraints. An infea-
sible (Î±newi , Î±
new
j ) must be in one of the four regions outside the following box.
Î±i
Î±j Î±i âˆ’ Î±j = Ci âˆ’ Cj
Î±i âˆ’ Î±j = 0
Ci
Cj
region Iregion IV
region II
region III NA
NA
Note that (Î±newi , Î±
new
j ) does not appear in the â€œNAâ€ regions because (Î±
k
i , Î±
k
j ) is in the
box and
Î±newi âˆ’ Î±newj = Î±ki âˆ’ Î±kj .
If (Î±newi , Î±
new
j ) is in region I, we set
Î±k+1i = Ci and Î±
k+1
j = Ci âˆ’ (Î±ki âˆ’ Î±kj ).
Of course, we must identify the region that (Î±newi , Î±
new
j ) resides. For region I, we have
Î±ki âˆ’ Î±kj > Ci âˆ’ Cj and Î±newi â‰¥ Ci.
Other cases are similar. We have the following pseudo code to identify which region
(Î±newi , Î±
new
j ) is in and modify (Î±
new
i , Î±
new
j ) to satisfy bound constraints.
if(y[i]!=y[j])
{
double quad_coef = Q_i[i]+Q_j[j]+2*Q_i[j];
if (quad_coef <= 0)
28
quad_coef = TAU;
double delta = (-G[i]-G[j])/quad_coef;
double diff = alpha[i] - alpha[j];
alpha[i] += delta;
alpha[j] += delta;
if(diff > 0)
{
if(alpha[j] < 0) // in region III
{
alpha[j] = 0;
alpha[i] = diff;
}
}
else
{
if(alpha[i] < 0) // in region IV
{
alpha[i] = 0;
alpha[j] = -diff;
}
}
if(diff > C_i - C_j)
{
if(alpha[i] > C_i) // in region I
{
alpha[i] = C_i;
alpha[j] = C_i - diff;
}
}
else
{
if(alpha[j] > C_j) // in region II
{
alpha[j] = C_j;
alpha[i] = C_j + diff;
}
}
}
If yi = yj, the derivation is the same.
7 Multi-class classification
LIBSVM implements the â€œone-against-oneâ€ approach (Knerr et al., 1990) for multi-
class classification. Some early works of applying this strategy to SVM include, for
example, Kressel (1998). If k is the number of classes, then k(k âˆ’ 1)/2 classifiers are
constructed and each one trains data from two classes. For training data from the
29
ith and the jth classes, we solve the following two-class classification problem.
min
wij ,bij ,Î¾ij
1
2
(wij)Twij + C
âˆ‘
t
(Î¾ij)t
subject to (wij)TÏ†(xt) + b
ij â‰¥ 1âˆ’ Î¾ijt , if xt in the ith class,
(wij)TÏ†(xt) + b
ij â‰¤ âˆ’1 + Î¾ijt , if xt in the jth class,
Î¾ijt â‰¥ 0.
In classification we use a voting strategy: each binary classification is considered to
be a voting where votes can be cast for all data points x - in the end a point is
designated to be in a class with the maximum number of votes.
In case that two classes have identical votes, though it may not be a good strategy,
now we simply choose the class appearing first in the array of storing class names.
Many other methods are available for multi-class SVM classification. Hsu and
Lin (2002a) give a detailed comparison and conclude that â€œone-against-oneâ€ is a
competitive approach.
8 Probability Estimates
SVM predicts only class label (target value for regression) without probability infor-
mation. This section discusses the LIBSVM implementation for extending SVM to
give probability estimates. More details are in Wu et al. (2004) for classification and
in Lin and Weng (2004) for regression.
Given k classes of data, for any x, the goal is to estimate
pi = P (y = i | x), i = 1, . . . , k.
Following the setting of the one-against-one (i.e., pairwise) approach for multi-class
classification, we first estimate pairwise class probabilities
rij â‰ˆ P (y = i | y = i or j,x)
using an improved implementation (Lin et al., 2007) of Platt (2000). If fÌ‚ is the
decision value at x, then we assume
rij â‰ˆ
1
1 + eAfÌ‚+B
, (43)
where A and B are estimated by minimizing the negative log likelihood of training
data (using their labels and decision values). It has been observed that decision values
30
from training may overfit the model (43), so we conduct five-fold cross-validation to
obtain decision values before minimizing the negative log likelihood. Note that if
some classes contain five or even fewer data points, the resulting model may not be
good. You can duplicate the data set so that each fold of cross-validation gets more
data.
After collecting all rij values, Wu et al. (2004) propose several approaches to
obtain pi,âˆ€i. In LIBSVM, we consider their second approach and solve the following
optimization problem.
min
p
1
2
kâˆ‘
i=1
âˆ‘
j:j 6=i
(rjipi âˆ’ rijpj)2
subject to pi â‰¥ 0,âˆ€i,
kâˆ‘
i=1
pi = 1. (44)
The objective function in problem (44) comes from the equality
P (y = j | y = i or j,x) Â· P (y = i | x) = P (y = i | y = i or j,x) Â· P (y = j | x)
and can be reformulated as
min
p
1
2
pTQp,
where
Qij =
{âˆ‘
s:s 6=i r
2
si if i = j,
âˆ’rjirij if i 6= j.
Wu et al. (2004) prove that the non-negativity constraints pi â‰¥ 0,âˆ€i in problem (44)
are redundant. After removing these constraints, the optimality condition implies that
there exists a scalar b (the Lagrange multiplier of the equality constraint
âˆ‘k
i=1 pi = 1)
such that [
Q e
eT 0
] [
p
b
]
=
[
0
1
]
, (45)
where e is the k Ã— 1 vector of all ones and 0 is the k Ã— 1 vector of all zeros.
Instead of solving the linear system (45) by a direct method such as Gaussian
elimination, Wu et al. (2004) derive a simple iterative method. Because
âˆ’pTQp = âˆ’pTQ(âˆ’be) = bpTe = b,
the optimal solution p satisfies
(Qp)t âˆ’ pTQp = Qttpt +
âˆ‘
j:j 6=t
Qtjpj âˆ’ pTQp = 0, âˆ€t. (46)
Using Eq. (46), we consider Algorithm 3.
31
Algorithm 3
1. Start with an initial p satisfying pi â‰¥ 0,âˆ€i and
âˆ‘k
i=1 pi = 1.
2. Repeat (t = 1, . . . , k, 1, . . .)
pt â†
1
Qtt
[âˆ’
âˆ‘
j:j 6=t
Qtjpj + p
TQp] (47)
normalize p
until Eq. (45) is satisfied.
Eq. (47) can be simplified to
pt â† pt +
1
Qtt
[âˆ’(Qp)t + pTQp].
Algorithm 3 guarantees to converge globally to the unique optimum of problem (44).
Using some tricks, we do not need to recalculate pTQp at each iteration. More
implementation details are in Appendix C of Wu et al. (2004). We consider a relative
stopping condition for Algorithm 3.
â€–Qpâˆ’ pTQpeâ€–âˆ = max
t
|(Qp)t âˆ’ pTQp| < 0.005/k.
When k (the number of classes) is large, some elements of p may be very close to
zero. Thus, we use a more strict stopping condition by decreasing the tolerance by a
factor of k.
Next, we discuss SVR probability inference. For a given set of training data
D = {(xi, yi) | xi âˆˆ Rn, yi âˆˆ R, i = 1, . . . , l}, we assume that the data are collected
from the model
yi = f(xi) + Î´i,
where f(x) is the underlying function and Î´iâ€™s are independent and identically dis-
tributed random noises. Given a test data x, the distribution of y given x and D,
P (y | x,D), allows us to draw probabilistic inferences about y; for example, we can
estimate the probability that y is in an interval such as [f(x)âˆ’âˆ†, f(x)+âˆ†]. Denoting
fÌ‚ as the estimated function based on D using SVR, then Î¶ = Î¶(x) â‰¡ y âˆ’ fÌ‚(x) is the
out-of-sample residual (or prediction error). We propose modeling the distribution of
Î¶ based on cross-validation residuals {Î¶i}li=1. The Î¶iâ€™s are generated by first conduct-
ing a five-fold cross-validation to get fÌ‚j, j = 1, . . . , 5, and then setting Î¶i â‰¡ yiâˆ’ fÌ‚j(xi)
for (xi, yi) in the jth fold. It is conceptually clear that the distribution of Î¶iâ€™s may
resemble that of the prediction error Î¶.
32
Figure 2 illustrates Î¶iâ€™s from a data set. Basically, a discretized distribution like
histogram can be used to model the data; however, it is complex because all Î¶iâ€™s must
be retained. On the contrary, distributions like Gaussian and Laplace, commonly
used as noise models, require only location and scale parameters. In Figure 2, we
plot the fitted curves using these two families and the histogram of Î¶iâ€™s. The figure
shows that the distribution of Î¶iâ€™s seems symmetric about zero and that both Gaussian
and Laplace reasonably capture the shape of Î¶iâ€™s. Thus, we propose to model Î¶i by
zero-mean Gaussian and Laplace, or equivalently, model the conditional distribution
of y given fÌ‚(x) by Gaussian and Laplace with mean fÌ‚(x).
Lin and Weng (2004) discuss a method to judge whether a Laplace and Gaussian
distribution should be used. Moreover, they experimentally show that in all cases
they have tried, Laplace is better. Thus, in LIBSVM, we consider the zero-mean
Laplace with a density function.
p(z) =
1
2Ïƒ
eâˆ’
|z|
Ïƒ .
Assuming that Î¶iâ€™s are independent, we can estimate the scale parameter Ïƒ by maxi-
mizing the likelihood. For Laplace, the maximum likelihood estimate is
Ïƒ =
âˆ‘l
i=1 |Î¶i|
l
.
Lin and Weng (2004) point out that some â€œvery extremeâ€ Î¶iâ€™s may cause inaccurate
estimation of Ïƒ. Thus, they propose estimating the scale parameter by discarding
Î¶iâ€™s which exceed Â±5 Â· (standard deviation of the Laplace distribution). For any new
data x, we consider that
y = fÌ‚(x) + z,
where z is a random variable following the Laplace distribution with parameter Ïƒ.
In theory, the distribution of Î¶ may depend on the input x, but here we assume
that it is free of x. Such an assumption works well in practice and leads to a simple
model.
9 Parameter Selection
To train SVM problems, users must specify some parameters. LIBSVM provides a
simple tool to check a grid of parameters. For each parameter setting, LIBSVM obtains
cross-validation (CV) accuracy. Finally, the parameters with the highest CV accuracy
33
Figure 2: Histogram of Î¶iâ€™s and the models via Laplace and Gaussian distributions.
The x-axis is Î¶i using five-fold cross-validation and the y-axis is the normalized number
of data in each bin of width 1.
are returned. The parameter selection tool assumes that the RBF (Gaussian) kernel
is used although extensions to other kernels and SVR can be easily made. The RBF
kernel takes the form
K(xi,xj) = e
âˆ’Î³â€–xiâˆ’xjâ€–2 , (48)
so (C, Î³) are parameters to be decided. Users can provide a possible interval of C
(or Î³) with the grid space. Then, all grid points of (C, Î³) are tried to find the one
giving the highest CV accuracy. Users then use the best parameters to train the
whole training set and generate the final model.
We do not consider more advanced parameter selection methods because for only
two parameters (C and Î³), the number of grid points is not too large. Further, because
SVM problems under different (C, Î³) parameters are independent, LIBSVM provides
a simple tool so that jobs can be run in a parallel (multi-core, shared memory, or
distributed) environment.
For multi-class classification, under a given (C, Î³), LIBSVM uses the one-against-
one method to obtain the CV accuracy. Hence, the parameter selection tool suggests
the same (C, Î³) for all k(k âˆ’ 1)/2 decision functions. Chen et al. (2005, Section 8)
discuss issues of using the same or different parameters for the k(k âˆ’ 1)/2 two-class
problems.
LIBSVM outputs the contour plot of cross-validation accuracy. An example is in
34
Figure 3: Contour plot of running the parameter selection tool in LIBSVM. The data
set heart scale (included in the package) is used. The x-axis is log2C and the y-axis
is log2 Î³.
Figure 3.
10 Conclusions
When we released the first version of LIBSVM in 2000, only two-class C-SVC was
supported. Gradually, we added other SVM variants, and supported functions such
as multi-class classification and probability estimates. Then, LIBSVM becomes a
complete SVM package. We add a function only if it is needed by enough users. By
keeping the system simple, we strive to ensure good system reliability.
In summary, this article gives implementation details of LIBSVM. We are still
actively updating and maintaining this package. We hope the community will benefit
more from our continuing development of LIBSVM.
35
Acknowledgments
This work was supported in part by the National Science Council of Taiwan via the
grants NSC 89-2213-E-002-013 and NSC 89-2213-E-002-106. The authors thank their
group members and users for many helpful comments. A list of acknowledgments is
at http://www.csie.ntu.edu.tw/~cjlin/libsvm/acknowledgements.
References
B. E. Boser, I. Guyon, and V. Vapnik. A training algorithm for optimal margin
classifiers. In Proceedings of the Fifth Annual Workshop on Computational Learning
Theory, pages 144â€“152. ACM Press, 1992.
C.-C. Chang and C.-J. Lin. Training Î½-support vector classifiers: Theory and algo-
rithms. Neural Computation, 13(9):2119â€“2147, 2001.
C.-C. Chang and C.-J. Lin. Training Î½-support vector regression: Theory and algo-
rithms. Neural Computation, 14(8):1959â€“1977, 2002.
C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology, 2:27:1â€“27:27, 2011. Software
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.
P.-H. Chen, C.-J. Lin, and B. SchoÌˆlkopf. A tutorial on Î½-support vector machines.
Applied Stochastic Models in Business and Industry, 21:111â€“136, 2005. URL http:
//www.csie.ntu.edu.tw/~cjlin/papers/nusvmtoturial.pdf.
P.-H. Chen, R.-E. Fan, and C.-J. Lin. A study on SMO-type decomposition methods
for support vector machines. IEEE Transactions on Neural Networks, 17:893â€“908,
July 2006. URL http://www.csie.ntu.edu.tw/~cjlin/papers/generalSMO.
pdf.
C. Cortes and V. Vapnik. Support-vector network. Machine Learning, 20:273â€“297,
1995.
D. J. Crisp and C. J. C. Burges. A geometric interpretation of Î½-SVM classifiers.
In S. Solla, T. Leen, and K.-R. MuÌˆller, editors, Advances in Neural Information
Processing Systems, volume 12, Cambridge, MA, 2000. MIT Press.
36
K. C. Dorff, N. Chambwe, M. Srdanovic, and F. Campagne. BDVal: repro-
ducible large-scale predictive model development and validation in high-throughput
datasets. Bioinformatics, 26(19):2472â€“2473, 2010.
R.-E. Fan, P.-H. Chen, and C.-J. Lin. Working set selection using second order
information for training SVM. Journal of Machine Learning Research, 6:1889â€“1918,
2005. URL http://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf.
S. Fine and K. Scheinberg. Efficient svm training using low-rank kernel representa-
tions. Journal of Machine Learning Research, 2:243â€“264, 2001.
T. Glasmachers and C. Igel. Maximum-gain working set selection for support vector
machines. Journal of Machine Learning Research, 7:1437â€“1466, 2006.
K. Grauman and T. Darrell. The pyramid match kernel: Discriminative classification
with sets of image features. In Proceedings of IEEE International Conference on
Computer Vision, 2005.
M. Hanke, Y. O. Halchenko, P. B. Sederberg, S. J. Hanson, J. V. Haxby, and S. Poll-
mann. PyMVPA: A Python toolbox for multivariate pattern analysis of fMRI data.
Neuroinformatics, 7(1):37â€“53, 2009. ISSN 1539-2791.
C.-W. Hsu and C.-J. Lin. A comparison of methods for multi-class support vector
machines. IEEE Transactions on Neural Networks, 13(2):415â€“425, 2002a.
C.-W. Hsu and C.-J. Lin. A simple decomposition method for support vector ma-
chines. Machine Learning, 46:291â€“314, 2002b.
C.-W. Hsu, C.-C. Chang, and C.-J. Lin. A practical guide to support vector classifica-
tion. Technical report, Department of Computer Science, National Taiwan Univer-
sity, 2003. URL http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.
pdf.
T. Joachims. Making large-scale SVM learning practical. In B. SchoÌˆlkopf, C. J. C.
Burges, and A. J. Smola, editors, Advances in Kernel Methods â€“ Support Vector
Learning, pages 169â€“184, Cambridge, MA, 1998. MIT Press.
S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and K. R. K. Murthy. Improvements
to Plattâ€™s SMO algorithm for SVM classifier design. Neural Computation, 13:637â€“
649, 2001.
37
S. S. Keerthi, O. Chapelle, and D. DeCoste. Building support vector machines with
reduced classifier complexity. Journal of Machine Learning Research, 7:1493â€“1515,
2006.
S. Knerr, L. Personnaz, and G. Dreyfus. Single-layer learning revisited: a stepwise
procedure for building and training a neural network. In J. Fogelman, editor, Neu-
rocomputing: Algorithms, Architectures and Applications. Springer-Verlag, 1990.
U. H.-G. Kressel. Pairwise classification and support vector machines. In B. SchoÌˆlkopf,
C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods â€“ Support
Vector Learning, pages 255â€“268, Cambridge, MA, 1998. MIT Press.
Y.-J. Lee and O. L. Mangasarian. RSVM: Reduced support vector machines. In
Proceedings of the First SIAM International Conference on Data Mining, 2001.
C.-J. Lin and R. C. Weng. Simple probabilistic predictions for support vector regres-
sion. Technical report, Department of Computer Science, National Taiwan Univer-
sity, 2004. URL http://www.csie.ntu.edu.tw/~cjlin/papers/svrprob.pdf.
H.-T. Lin, C.-J. Lin, and R. C. Weng. A note on Plattâ€™s probabilistic outputs for
support vector machines. Machine Learning, 68:267â€“276, 2007. URL http://www.
csie.ntu.edu.tw/~cjlin/papers/plattprob.pdf.
N. List and H. U. Simon. General polynomial time decomposition algorithms. Journal
of Machine Learning Research, 8:303â€“321, 2007.
N. List and H. U. Simon. SVM-optimization and steepest-descent line search. In
Proceedings of the 22nd Annual Conference on Computational Learning Theory,
2009.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. Kubler, S. Marinov, and
E. Marsi. MaltParser: A language-independent system for data-driven dependency
parsing. Natural Language Engineering, 13(2):95â€“135, 2007.
E. Osuna, R. Freund, and F. Girosi. Training support vector machines: An appli-
cation to face detection. In Proceedings of IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR), pages 130â€“136, 1997a.
E. Osuna, R. Freund, and F. Girosi. Support vector machines: Training and appli-
cations. AI Memo 1602, Massachusetts Institute of Technology, 1997b.
38
L. Palagi and M. Sciandrone. On the convergence of a modified version of SVMlight
algorithm. Optimization Methods and Software, 20(2â€“3):315â€“332, 2005.
J. C. Platt. Fast training of support vector machines using sequential minimal opti-
mization. In B. SchoÌˆlkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in
Kernel Methods - Support Vector Learning, Cambridge, MA, 1998. MIT Press.
J. C. Platt. Probabilistic outputs for support vector machines and comparison to reg-
ularized likelihood methods. In A. Smola, P. Bartlett, B. SchoÌˆlkopf, and D. Schuur-
mans, editors, Advances in Large Margin Classifiers, Cambridge, MA, 2000. MIT
Press.
B. SchoÌˆlkopf, A. Smola, R. C. Williamson, and P. L. Bartlett. New support vector
algorithms. Neural Computation, 12:1207â€“1245, 2000.
B. SchoÌˆlkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson.
Estimating the support of a high-dimensional distribution. Neural Computation,
13(7):1443â€“1471, 2001.
N. Segata and E. Blanzieri. Fast and scalable local kernel machines. Journal of
Machine Learning Research, 11:1883â€“1926, 2010.
V. Vapnik. Statistical Learning Theory. Wiley, New York, NY, 1998.
T.-F. Wu, C.-J. Lin, and R. C. Weng. Probability estimates for multi-class classifica-
tion by pairwise coupling. Journal of Machine Learning Research, 5:975â€“1005, 2004.
URL http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf.
39

