DynASP2.5: Dynamic Programming on
Tree Decompositions in Action∗
Johannes K. Fichte, Markus Hecher, Michael Morak, Stefan Woltran
TU Wien, Vienna, Austria
lastname@dbai.tuwien.ac.at
June 29, 2017
Abstract
A vibrant theoretical research area are efficient exact parameterized algorithms. Very recent
solving competitions such as the PACE challenge show that there is also increasing practical
interest in the parameterized algorithms community. An important research question is whether
dedicated parameterized exact algorithms exhibit certain practical relevance and one can even beat
well-established problem solvers. We consider the logic-based declarative modeling language and
problem solving framework Answer Set Programming (ASP). State-of-the-art ASP solvers rely
considerably on Sat-based algorithms. An ASP solver (DynASP2), which is based on a classical
dynamic programming on tree decompositions, has been published very recently. Unfortunately,
DynASP2 can outperform modern ASP solvers on programs of small treewidth only if the question
of interest is to count the number of solutions. In this paper, we describe underlying concepts of our
new implementation (DynASP2.5) that shows competitive behavior to state-of-the-art ASP solvers
even for finding just one solution when solving problems as the Steiner tree problem that have been
modeled in ASP on graphs with low treewidth. Our implementation is based on a novel approach
that we call multi-pass dynamic programming (M-DPSINC).
1 Introduction
Answer set programming (ASP) is a logic-based declarative modelling language and problem solving
framework [18], where a program consists of sets of rules over propositional atoms and is interpreted
under an extended stable model semantics [23]. Problems are usually modelled in ASP in such a way that
the stable models (answer sets) of a program directly form a solution to the considered problem instance.
Computational problems for disjunctive, propositional ASP such as deciding whether a program has an
answer set are complete for the second level of the Polynomial Hierarchy [9]. In consequence, finding
answer sets usually involves a Sat part (finding a model of the program) and an Unsat part (minimality
check). A variety of CDCL-based ASP solvers have been implemented [19, 4] and proven to be very
successful in solving competitions [14]. Very recently, a dynamic programming based solver (DynASP2 )
that builds upon ideas from parameterized algorithmics was proposed [11]. For disjunctive input
programs, the runtime of the underlying algorithms is double exponential in the incidence treewidth
and linear in the input size (so-called fixed-parameter linear algorithms). DynASP2 (i) takes a tree
decomposition of a certain graph representation (incidence graph) of a given input program and (ii) solves
the program via dynamic programming (DP) on the tree decomposition by traversing the tree exactly
once. Both finding a model and checking minimality are considered at the same time. Once the root
node has been reached, complete solutions (if exist) for the input program can be constructed. This
approach pays off for counting answer sets, but is not competitive for outputting just one answer set.
∗This is the authors self-archived copy including detailed proofs. Research was supported by the Austrian Science Fund
(FWF), Grant Y698.
1
ar
X
iv
:1
70
6.
09
37
0v
1 
 [
cs
.L
O
] 
 2
8 
Ju
n 
20
17
The reason for that lies in the exhaustive nature of dynamic programming as all potential values are
computed locally for each node of the tree decomposition. In consequence, space requirements can
be quite extensive resulting in long running times. Moreover, dynamic programming algorithms on
tree decompositions may yield extremely diverging run-times on tree decompositions of the exact same
width [1]. In this paper, we propose a multi-pass approach (M-DPSINC) for dynamic programming on
tree decompositions as well as a new implementation (DynASP2.5). In contrast to classical dynamic
programming algorithms for problems on the second level of the Polynomial Hierarchy, M-DPSINC traverses
the given tree decomposition multiple times. Starting from the leaves, we compute and store (i) sets of
atoms that are relevant for the Sat part (finding a model of the program) up to the root. Then we
go back again to the leaves and compute and store (ii) sets of atoms that are relevant for the Unsat
part (checking for minimality). Finally, we go once again back to the leaves and (iii) link sets from past
Passes (i) and (ii) that might lead to an answer set in the future. As a result, we allow for early cleanup
of candidates that do not lead to answer sets.
Further, we present technical improvements (including working on non-normalized tree decom-
positions) and employ dedicated customization techniques for selecting tree decompositions. Our
improvements are main ingredients to speedup the solving process for DP algorithms. Experiments
indicate that DynASP2.5 is competitive even for finding one answer set using the Steiner tree problem
on graphs with low treewidth. In particular, we are able to solve instances that have an upper bound on
the incidence treewidth of 14 (whereas DynASP2 solved instances of treewidth at most 9).
Our main contributions can be summarized as follows:
1. We establish a novel fixed-parameter linear algorithm (M-DPSINC), which works in multiple passes
and computes Sat and Unsat parts separately.
2. We present an implementation (DynASP2.5)1 and an experimental evaluation.
Related Work. Jakl, Pichler, and Woltran [17] have considered ASP solving when parameterized by the
treewidth of a graph representation and suggested fixed-parameter linear algorithms. Fichte et al. [11]
have established additional algorithms and presented empirical results on an implementation that is
dedicated to counting answer sets for the full ground ASP language. The present paper extends their
work by a multi-pass dynamic programming algorithm. Bliem et al. [5] have introduced a general multi-
pass approach and an implementation (D-FLATˆ2) for dynamic programming on tree decompositions
solving subset minimization tasks. Their approach allows to specify dynamic programming algorithms
by means of ASP. In a way, one can see ASP in their approach as a meta-language to describe table
algorithms2, whereas our work presents a dedicated algorithm to find an answer set of a program. In
fact, our implementation extends their general ideas for subset minimization (disjunctive rules) to also
support weight rules. However, due to space constraints we do not report on weight rules in this paper.
Beyond that, we require specialized adaptions to the ASP problem semantics, including three valued
evaluation of atoms, handling of non-normalized tree decompositions, and optimizations in join nodes
to be competitive. Abseher, Musliu, and Woltran [2] have presented a framework that computes tree
decompositions via heuristics, which is also used in our solver. Other tree decomposition systems can be
found on the PACE challenge website [7]. Note that improved heuristics for finding a tree decomposition
of smaller width (if possible) directly yields faster results for our solver.
2 Formal Background
2.1 Tree Decompositions
Let G = (V,E) be a graph, T = (N,F, n) a rooted tree, and χ : N → 2V a function that maps
each node t ∈ N to a set of vertices. We call the sets χ(·) bags and N the set of nodes. Then,
the pair T = (T, χ) is a tree decomposition (TD) of G if the following conditions hold: (i) for every
1The source code of our solver is available at https://github.com/daajoe/dynasp/releases/tag/v2.5.0.
2See Algorithm 1 for the concept of table algorithms.
2
vertex v ∈ V there is a node t ∈ N with v ∈ χ(t); (ii) for every edge e ∈ E there is a node t ∈ N with
e ⊆ χ(t); and (iii) for any three nodes t1, t2, t3 ∈ N , if t2 lies on the unique path from t1 to t3, then
χ(t1) ∩ χ(t3) ⊆ χ(t2). We call max{|χ(t)| − 1 | t ∈ N} the width of the TD. The treewidth tw(G) of a
graph G is the minimum width over all possible TDs of G.
Note that each graph has a trivial TD (T, χ) consisting of the tree ({n}, ∅, n) and the mapping
χ(n) = V . It is well known that the treewidth of a tree is 1, and a graph containing a clique of size k
has at least treewidth k − 1. For some arbitrary but fixed integer k and a graph of treewidth at most k,
we can compute a TD of width 6 k in time 2O(k
3) · |V | [6]. Given a TD (T, χ) with T = (N, ·, ·), for a
node t ∈ N we say that type(t) is leaf if t has no children; join if t has children t′ and t′′ with t′ 6= t′′
and χ(t) = χ(t′) = χ(t′′); int (“introduce”) if t has a single child t′, χ(t′) ⊆ χ(t) and |χ(t)| = |χ(t′)|+ 1;
rem (“removal”) if t has a single child t′, χ(t′) ⊇ χ(t) and |χ(t′)| = |χ(t)|+ 1. If every node t ∈ N has
at most two children, type(t) ∈ {leaf, join, int, rem}, and bags of leaf nodes and the root are empty, then
the TD is called nice. For every TD, we can compute a nice TD in linear time without increasing the
width [6]. Later, we traverse a TD bottom up, therefore, let post-order(T, t) be the sequence of nodes in
post-order of the induced subtree T ′ = (N ′, ·, t) of T rooted at t.
2.2 Answer Set programming (ASP)
ASP is a declarative modelling and problem solving framework that combines techniques of knowledge
representation and database theory. A main advantage of ASP is its expressiveness and when using
non-ground programs the advanced declarative problem modelling capability. Prior to solving, non-
ground programs are usually compiled into grounded by a grounder. In this paper, we restrict ourselves
to ground ASP programs. For a comprehensive introduction, see, e.g., [18]. Let `, m, n be non-negative
integers such that ` ≤ m ≤ n, a1, . . ., an distinct propositional atoms and l ∈ {a1,¬a1}. A choice
rule is an expression of the form {a1; . . . ; a`} ← a`+1, . . . , am,¬am+1, . . . ,¬an, intuitively some subset
of {a1, . . . , a`} is true if all atoms a`+1, . . . , am are true and there is no evidence that any atom of
am+1, . . . , an is true. A disjunctive rule is of the form a1 ∨ · · · ∨ a` ← a`+1, . . . , am,¬ am+1, . . ., ¬an,
intuitively at least one atom of a1, . . . , a` must be true if all atoms a`+1, . . . , am are true and there is no
evidence that any atom of am+1, . . . , an is true. An optimization rule is an expression of the form  l.
with the intuitive meaning that when literal l is true, this incurs a penalty of weight w. A rule is either
a disjunctive, a choice, or an optimization rule. For a choice or disjunctive rule r, let Hr := {a1, . . . , a`},
B+r := {a`+1, . . . , am}, and B−r := {am+1, . . . , an}. Usually, if B−r ∪ B+r = ∅ we write for a rule r
simply Hr instead of Hr ← . For an optimization rule r, if l = a1, let B+r := {a1} and B−r := ∅; and
if l = ¬a1, let B−r := {a1} and B+r := ∅. For a rule r, let at(r) := Hr ∪ B+r ∪ B−r denote its atoms
and Br := B
+
r ∪ {¬b | b ∈ B−r } its body. Let a program P be a set of rules, and at(P ) :=
⋃
r∈P at(r)
denote its atoms. and let CH(P ), DISJ(P ), and OPT(P ) denote the set of all choice, disjunctive, and
optimization rules in P , respectively. A set M ⊆ at(P ) satisfies a rule r if (i) (Hr ∪ B−r ) ∩M 6= ∅
or B+r 6⊆M for r ∈ DISJ(P ) or (ii) r is a choice or optimization rule. M is a model of P , denoted by
M  P , if M satisfies every rule r ∈ P .
The reduct rM (i) of a choice rule r is the set {a← B+r | a ∈ Hr ∩M,B−r ∩M = ∅} of rules, and
(ii) of a disjunctive rule r is the singleton {Hr ← B+r | B−r ∩M = ∅}. PM :=
⋃
r∈P r
M is called GL
reduct of P with respect to M . A set M ⊆ at(P ) is an answer set of P if (i) M  P and (ii) there is no
M ′ (M such that M ′  PM , that is, M is subset minimal with respect to PM .
We call cst(P,M) := |{r | r ∈ P, r is an optimization rule, (B+r ∩M) ∪ (B−r \M) 6= ∅}| the cost of
answer set M for P . An answer set M of P is optimal if its cost is minimal over all answer sets.
Example 1. Consider program
P = {
rab︷ ︸︸ ︷
{eab};
rbc︷ ︸︸ ︷
{ebc};
rcd︷ ︸︸ ︷
{ecd};
rad︷ ︸︸ ︷
{ead};
rb︷ ︸︸ ︷
ab ← eab;
rd︷ ︸︸ ︷
ad ← ead;
rc1︷ ︸︸ ︷
ac ← ab, ebc;
rc2︷ ︸︸ ︷
ac ← ad, ecd;
r¬c︷ ︸︸ ︷
← ¬ac}.
The set A = {eab, ebc, ab, ac} is an answer set of P , since {eab, ebc, ab, ac} is the only minimal model
of PA = {eab ←; ebc ←; ab ← eab; ad ← ead; ac ← ab, ebc; ac ← ad, ecd}. Then, consider program
R = {a ∨ c ← b; b ← c,¬g; c ← a; b ∨ c ← e; h ∨ i ← g,¬c; a ∨ b; g ← ¬i; c; {d} ← g}. The
set B = {b, c, d, g} is an answer set of R since {b, c, d, g} and {a, c, d, g} are the minimal models of
3
Solve local
probl. A(t, . . . )
Store results
in A-Tabs[t] 1. Construct graph G
Store witnesses
in W-Tabs[t]
Compute wit-
nesses of W(t, . . . )
Visit next node
t in post-order Done?
no
yes
2. Comp. TD T of G 3.I done?
no
yes
Visit next node
t in post-order
Purge non-witnesses
Store counter-wit-
nesses in C-Tabs[t]
Compute counter-
wits. of C(t, . . . )
3.II done?
no
yes
Visit next node
t in post-order
Purge non-
counter-witnesses
Store result in
W,C-Tabs[t]
Link counter-wits.
to witnesses
4. Print solution 3.III done?
no
yes
Visit next node
t in post-order
3.I. DPW(T )
3.II. DPC(T )
3.III. DPLW,C(T ,W-Tabs,C-Tabs)
3. DPA(T )
←−DynASP2 DynASP2.5−→
Figure 1 Control flow for DP-based ASP solver (DynASP2, left) and for DynASP2.5 (right).
RB = {a ∨ c← b; c← a; b ∨ c← e; a ∨ b; g; c; d← g}.
Given a program P , we consider the problems of computing an answer set (called AS), outputting
the number of optimal answer sets (called #AspO), and listing all optimal answer sets of P (called
EnumAsp). Further, given a propositional formula F and an atom sol, we use the entailment problem
of listing every subset-minimal model M of F with sol ∈M (called EnumMinSAT1).
2.3 Graph Representations of Programs
In order to use TDs for ASP solving, we need dedicated graph representations of programs. The incidence
graph I(P ) of P is the bipartite graph that has the atoms and rules of P as vertices and an edge a r if
a ∈ at(r) for some rule r ∈ P [11]. The semi-incidence graph S(P ) of P is a graph that has the atoms
and rules of P as vertices and (i) an edge a r if a ∈ at(r) for some rule r ∈ P as well as (ii) an edge a b
for disjoint atoms a, b ∈ Hr where r ∈ P is a choice rule. Since for every program P the incidence
graph I(P ) is a subgraph of the semi-incidence graph, we have that tw(I(P )) ≤ tw(S(P )). Further,
by definition of a TD and the construction of a semi-incidence graph that head atoms of choice rules,
respectively, occur in at least one common bag of the TD.
2.4 Sub-Programs
Let T = (T, χ) be a nice TD of graph representation S(P ) of a program P . Further, let T = (N, ·, n) and
t ∈ N . The bag-program is defined as Pt := P ∩χ(t). Further, the set at≤t := {a | a ∈ at(P )∩χ(t′), t′ ∈
post-order(T, t)} is called atoms below t, the program below t is defined as P≤t := {r | r ∈ Pt′ , t′ ∈
post-order(T, t)}, and the program strictly below t is P<t := P≤t \ Pt. It holds that P≤n = P<n = P and
at≤n = at(P ).
3 A Single Pass DP Algorithm
A dynamic programming based ASP solver, such as DynASP2 [11], splits the input program P into
“bag-programs” based on the structure of a given nice tree decomposition for P and evaluates P in parts,
thereby storing the results in tables for each TD node. More precisely, the algorithm works as outlined
on the left and middle of Figure 1 and encompasses the following steps:
1. Construct a graph representation G(P ) of the given input program P .
2. Compute a TD T of the graph G(P ) by means of some heuristic, thereby decomposing G(P ) into
several smaller parts and fixing an ordering in which P will be evaluated.
4
Algorithm 1: Algorithm DPA(T ) for Dynamic Programming on TD T for ASP [11].
In: Table algorithm A, nice TD T = (T, χ) with T = (N, ·, n) of G(P ) according to A.
Out: A-Tabs: maps each TD node t ∈ T to some computed table τt.
1 Child-Tabst := {A-Tabs[t′] | t′ is a child of t in T}
2 for iterate t in post-order(T,n) do
3 A-Tabs[t]← A(t, χ(t), Pt, at≤t,Child-Tabst)
3. Algorithm 1 – DPA(T ) – sketches the general scheme for this step, assuming that an algorithm A,
which highly depends on the graph representation, is given. We usually call A the table algorithm3.
For every node t ∈ T in the tree decomposition T = ((T,E, n), χ) (in a bottom-up traversal),
run A and compute A-Tabs[t], which are sets of tuples (or rows for short). Intuitively, algorithm A
transforms tables of child nodes of t to the current node, and solves a “local problem” using
bag-program Pt. The algorithm thereby computes (i) sets of atoms called (local) witness sets and
(ii) for each local witness set M subsets of M called counter-witness sets [11], and directly follows
the definition of answer sets being (i) models of P and (ii) subset minimal with respect to PM .
4. For root n interpret the table A-Tabs[n] (and tables of children, if necessary) and print the solution
to the considered ASP problem.
Next, we propose a new table algorithm (SINC) for programs without optimization rules. Since our
algorithm trivially extends to counting and optimization rules by earlier work [11], we omit such rules.
The table algorithm SINC employs the semi-incidence graph and is depicted in Algorithm 2. DPSINC
merges two earlier algorithms for the primal and incidence graph [11] resulting in slightly different worst
case runtime bounds (c.f., Theorem 1).
Our table algorithm SINC computes and stores (i) sets of atoms (witnesses) that are relevant for the
Sat part (finding a model of the program) and (ii) sets of atoms (counter-witnesses) that are relevant
for the Unsat part (checking for minimality). In addition, we need to store for each set of witnesses as
well as its set of counter-witnesses satisfiability states (sat-states for short). For the following reason:
By Definition of TDs and the semi-incidence graph, it is true for every atom a and every rule r of a
program that if atom a occurs in rule r, then a and r occur together in at least one bag of the TD.
In consequence, the table algorithm encounters every occurrence of an atom in any rule. In the end,
on removal of r, we have to ensure that r is among the rules that are already satisfied. However, we
need to keep track whether a witness satisfies a rule, because not all atoms that occur in a rule occur
together in exactly one bag. Hence, when our algorithm traverses the TD and an atom is forgotten we
still need to store this sat-state, as setting the forgotten atom to a certain truth value influences the
satisfiability of the rule. Since the semi-incidence graph contains a clique on every set A of atoms that
occur together in choice rule head, those atoms A occur together in a common bag of any TD of the
semi-incidence graph. For that reason, we do not need to incorporate choice rules into the satisfiability
state, in contrast to the algorithm for the incidence graph [11]. We can see witness sets together with
its sat-state as witness. Then, in Algorithm 2 (SINC) a row in the table τt is a triple 〈M,σ, C〉. The
set M ⊆ at(P )∩χ(t) represents a witness set. The family C of sets concerns counter-witnesses, which we
will discuss in more detail below. The sat-state σ for M represents rules of χ(t) satisfied by a superset
of M . Hence, M witnesses a model M ′ ⊇M where M ′  P<t ∪ σ. We use binary operator ∪ to combine
sat-states, which ensures that rules satisfied in at least one operand remain satisfied. We compute a new
sat-state σ from a sat-state and satisfied rules, formally, SatPr(Ṙ,M) := {r | (r,R) ∈ Ṙ,M  R} for
M ⊆ χ(t) \ Pt and program Ṙ(r) constructed by Ṙ, mapping rules to local-programs (Definition 1).
Definition 1. Let P be a program, T = (·, χ) be a TD of S(P ), t be a node of T and R ⊆ Pt. The
local-program R(t) is obtained from R ∪ {← Br | r ∈ R is a choice rule, Hr ( at≤t}4 by removing from
every rule all literals a,¬a with a 6∈ χ(t). We define Ṙ(t) : R→ 2R(t) by Ṙ(t)(r) := {r}(t) for r ∈ R.
3The table algorithm SINC for example is given in Algorithm 2.
4We require to add {← Br | r ∈ R is a choice rule, Hr ( at≤t} in order to decide satisfiability for corner cases of choice
rules involving counter-witnesses of Line 3 in Algorithm 2.
5For set S and element s, we denote S+s :=S ∪ {s} and S−s :=S \ {s}.
5
Algorithm 2: Table algorithm SINC(t, χt, Pt, at≤t,Child-Tabst).
In: Bag χt, bag-program Pt, atoms-below at≤t, child tables Child-Tabst of t. Out: Tab. τt.
1 if type(t) = leaf then τt ← {〈∅, ∅, ∅〉} /* Abbreviations see Footnote 5. */
2 else if type(t) = int, a ∈ χt \ Pt is introduced and τ ′ ∈ Child-Tabst then
3 τt ← {〈M+a , σ ∪ SatPr(Ṗ
(t)
t ,M
+
a ), {〈C+a , ρ ∪ SatPr(Ṗ
(t,M+a )
t , C
+
a )〉 | 〈C, ρ〉 ∈ C} ∪
4 {〈C, ρ ∪ SatPr(Ṗ (t,M
+
a )
t , C)〉 | 〈C, ρ〉 ∈ C} ∪ {〈M,σ ∪ SatPr(Ṗ
(t,M+a )
t ,M)〉}〉 | 〈M,σ, C〉 ∈ τ ′}
5 ∪ {〈M,σ ∪ SatPr(Ṗ (t)t ,M), {〈C, ρ ∪ SatPr(Ṗ
(t,M)
t , C)〉 | 〈C, ρ〉 ∈ C}〉 | 〈M,σ, C〉 ∈ τ ′}
6 else if type(t) = int, r ∈ χt ∩ Pt is introduced and τ ′ ∈ Child-Tabst then
7 τt ← {〈M,σ ∪ SatPr({ṙ}(t),M), {〈C, ρ ∪ SatPr({ṙ}(t,M), C)〉 | 〈C, ρ〉 ∈ C}〉 | 〈M,σ, C〉 ∈ τ ′}
8 else if type(t) = rem, a 6∈ χt is removed atom and τ ′ ∈ Child-Tabst then
9 τt ← {〈M−a , σ, {〈C−a , ρ〉 | 〈C, ρ〉 ∈ C}〉 | 〈M,σ, C〉 ∈ τ ′}
10 else if type(t) = rem, r 6∈ χt is removed rule and τ ′ ∈ Child-Tabst then
11 τt ← {〈M,σ−r ,
{
〈C, ρ−r 〉 | 〈C, ρ〉 ∈ C, r ∈ ρ
}
〉 | 〈M,σ, C〉 ∈ τ ′, r ∈ σ}
12 else if type(t) = join and τ ′, τ ′′ ∈ Child-Tabst with τ ′ 6= τ ′′ then
13 τt ← {〈M,σ′ ∪ σ′′, {〈C, ρ′ ∪ ρ′′〉 | 〈C, ρ′〉 ∈ C′, 〈C, ρ′′〉 ∈ C′′} ∪ {〈M,ρ ∪ σ′′〉 | 〈M,ρ〉 ∈ C′} ∪
14 {〈M,σ′ ∪ ρ〉 | 〈M,ρ〉 ∈ C′′}〉 | 〈M,σ′, C′〉 ∈ τ ′, 〈M,σ′′, C′′〉 ∈ τ ′′}
Example 2. Observe P
(t4)
t4 = {← ebc, rb} and P
(t5)
t5 = {c←} for Pt4 and Pt5 of Figure 2.
In Example 3 we give an idea how we compute models of a given program using the semi-incidence
graph. The resulting algorithm MOD is obtained from SINC, by taking only the first two row positions
(red and green parts). The remaining position (blue part), can be seen as an algorithm (CMOD) that
computes counter-witnesses (see Example 5). Note that we discuss selected cases, and we assume row
numbers in each table τt, i.e., the i
th-row corresponds to ut.i = 〈Mt.i, σt.i〉.
Example 3. Consider program P from Example 1, TD T = (·, χ) in Figure 2, and the tables τ1,. . .,
τ34, which illustrate computation results obtained during post-order traversal of T by DPMOD. Note that
Figure 2 (left) does not show every intermediate node of TD T . Table τ1 = {〈∅, ∅〉} as type(t1) =
leaf (see Algorithm 2 L1). Table τ3 is obtained via introducing rule rab, after introducing atom eab
(type(t2) = type(t3) = int). It contains two rows due to two possible truth assignments using atom eab
(L3–5). Observe that rule rab is satisfied in both rows M3.1 and M3.2, since the head of choice rule rab is
in at≤t3 (see L7 and Definition 1). Intuitively, whenever a rule r is proven to be satisfiable, sat-state σt.i
marks r satisfiable since an atom of a rule of S(P ) might only occur in one TD bag. Consider table τ4
with type(t4) = rem and rab ∈ χ(t3) \ χ(t4). By definition (TDs and semi-incidence graph), we have
encountered every occurrence of any atom in rab. In consequence, MOD enforces that only rows where
rab is marked satisfiable in τ3, are considered for table τ4. The resulting table τ4 consists of rows of τ3
with σ4.i = ∅, where rule rab is proven satisfied (rab ∈ σ3.1, σ3.2, see L 11). Note that between nodes t6
and t10, an atom and rule remove as well as an atom and rule introduce node is placed. Observe that the
second row u6.2 = 〈M6.2, σ6.2〉 ∈ τ6 does not have a “successor row” in τ10, since rb 6∈ σ6.2. Intuitively,
join node t34 joins only common witness sets in τ17 and τ33 with χ(t17) = χ(t33) = χ(t34). In general, a
join node marks rules satisfied, which are marked satisfied in at least one child (see L13–14).
Since we already explained how to obtain models, we only briefly describe how to compute counter-
witnesses. Family C consists of rows (C, ρ) where C ⊆ at(P ) ∩ χ(t) is a counter-witness set in t to M .
Similar to the sat-state σ, the sat-state ρ for C under M represents whether rules of the GL reduct PMt
are satisfied by a superset of C. We can see counter-witness sets together with its sat-state as counter-
witnesses. Thus, C witnesses the existence of C ′ (M ′ satisfying C ′  (P<t ∪ ρ)M
′
since M witnesses
a model M ′ ⊇ M where M ′  P<t. In consequence, there exists an answer set of P if the root table
contains 〈∅, ∅, ∅〉. We require local-reducts for deciding satisfiability of counter-witness sets.
Definition 2. Let P be a program, T = (·, χ) be a TD of S(P ), t be a node of T , R ⊆ Pt and M ⊆ at(P ).
We define local-reduct R(t,M) by [R(t)]
M
and Ṙ(t,M) : R→ 2R(t,M) by Ṙ(t,M)(r) :={r}(t,M), r ∈ R.
Proposition 1 (c.f. [11]). Let P be a program and k :=tw(S(P )). Then, the algorithm DPSINC is correct
and runs in time O(22k+2 · ‖S(P )‖).
6
∅ t1
eab t2
rab, eab t3
eab t4
rb, eab t5
rb, eab, ab
t6
rbc, ebc, ab
t10
rc1, ebc, ab
t12
rc1, act15
∅ t18
rad, ead t20
rd, ead, ad t23
rcd, ecd, ad t27
rc2, ecd, ad t29
rc2, ac t32
r¬c, ac
t34
∅ t36T:
〈M3.i, σ3.i〉 τ3
〈{eab}, {rab}〉
〈∅, {rab}〉
〈M32.i, σ32.i〉
〈{ac}, {rc2}〉
〈∅, {rc2}〉
〈∅, ∅〉
τ32
〈M6.i, σ6.i〉 τ6
〈{eab, ab}, {rb}〉
〈{eab}, ∅〉
〈{ab}, {rb}〉
〈∅, {rb}〉
〈M12.i, σ12.i〉
〈{ebc, ab}, ∅〉
〈{ebc}, {rc1}〉
〈{ab}, {rc1}〉
〈∅, {rc1}〉
τ12
〈M1.i, σ1.i〉 τ1
〈∅, ∅〉
〈M3.i, σ3.i, C3.i〉 τ3
〈{eab}, {rab}, {〈∅, ∅〉}〉
〈∅, {rab}, ∅〉
〈M1.i, σ1.i, C1.i〉 τ1
〈∅, ∅, ∅〉
〈M6.i, σ6.i, C6.i〉 τ6
〈{eab, ab}, {rb}, {
〈{eab}, ∅〉}〉
〈{eab}, ∅, ∅〉
〈{ab}, {rb}, {〈∅, {rb}〉}〉
〈∅, {rb}, ∅〉
〈M12.i, σ12.i, C12.i〉 τ12
〈{ebc, ab}, ∅, {〈{ebc}, {rc1}〉}〉
〈{ebc, ab}, ∅, ∅〉
〈{ebc}, {rc1}, ∅〉
〈{ab}, {rc1}, {〈∅, {rc1}〉}〉
〈{ab}, {rc1}, ∅〉
〈∅, {rc1}, ∅〉
〈M32.i, σ32.i, C32.i〉
〈{ac}, {rc2}, {
〈{ac}, {rc2}〉,
〈∅, {rc2}〉}〉
〈{ac}, {rc2}, {
〈∅, {rc2}〉}〉
〈∅, {rc2}, {
〈∅, {rc2}〉}〉
〈∅, {rc2}, ∅〉
〈∅, ∅, {〈∅, {rc2}〉}〉
〈∅, ∅, ∅〉
τ32
Figure 2 A TD T of the semi-incidence graph S(P ) for program P from Example 1 (center). Selected
DP tables after DPMOD (left) and after DPSINC (right) for nice TD T .
4 DynASP2.5: Implementing a III Pass DP Algorithm
The classical DP algorithm DPSINC (Step 3 of Figure 1) follows a single pass approach. It computes
both witnesses and counter-witnesses by traversing the given TD exactly once. In particular, it stores
exhaustively all potential counter-witnesses, even those counter-witnesses where the witnesses in the
table of a node cannot be extended in the parent node. In addition, there can be a high number of
duplicates among the counter-witnesses, which are stored repeatedly. In this section, we propose a
multi-pass approach (M-DPSINC) for DP on TDs and a new implementation (DynASP2.5), which fruitfully
adapts and extends ideas from a different domain [5]. Our novel algorithm allows for an early cleanup
(purging) of witnesses that do not lead to answer sets, which in consequence (i) avoids to construct
expendable counter-witnesses. Moreover, multiple passes enable us to store witnesses and counter-
witnesses separately, which in turn (ii) avoids storing counter-witnesses duplicately and (iii) allows for
highly space efficient data structures (pointers) in practice when linking witnesses and counter-witnesses
together. Figure 1 (right, middle) presents the control flow of the new multi-pass approach DynASP2.5,
where M-DPSINC introduces a much more elaborate computation in Step 3.
4.1 The Algorithm
Our algorithm (M-DPSINC) executed as Step 3 runs DPMOD, DPCMOD and DPLMOD,CMOD in three passes (3.I,
3.II, and 3.III) as follows:
3.I. First, we run the algorithm DPMOD, which computes in a bottom-up traversal for every node t
in the tree decomposition a table MOD-Tabs[t] of witnesses for t. Then, in a top-down traversal
for every node t in the TD remove from tables MOD-Tabs[t] witnesses, which do not extend to a
witness in the table for the parent node (“Purge non-witnesses”); these witnesses can never be
used to construct a model (nor answer set) of the program.
3.II. For this step, let CMOD be a table algorithm computing only counter-witnesses of SINC (blue
parts of Algorithm 2). We execute DPCMOD, compute for all witnesses counter-witnesses at once
and store the resulting tables in CMOD-Tabs[·]. For every node t, table CMOD-Tabs[t] contains
counter-witnesses to witness being ⊂-minimal. Again, irrelevant rows are removed (“Purge
non-counter-witnesses”).
7
Algorithm 3: Algorithm DPLW,C(T ,W-Tabs,C-Tabs) for linking counter-witnesses to witnesses.
In: Nice TD T = (T, χ) with T = (N, ·, n) of a graph S(P ), and mappings W-Tabs[·], C-Tabs[·].
Out: W,C-Tabs: maps node t ∈ T to some pair (τWt , τCt ) with τWt ∈W-Tabs[t], τCt ∈ C-Tabs[t].
1 Child-Tabst :={W,C-Tabs[t′] | t′ is a child of t in T}
/* Get for a node t tables of (preceeding) combined child rows (CCR) */
2 CCRt :=Π̂τ ′∈Child-Tabstτ
′ /* For Abbreviations see Footnote 6. */
/* Get for a row ~u its combined child rows (origins) */
3 origt(~u) :={S | S ∈ CCRt, ~u ∈ τ, τ = W(t, χ(t), Pt, at≤t, fw(S))}
/* Get for a table S of combined child rows its successors (evolution) */
4 evolt(S) :={~u | ~u ∈ τ, τ = C(t, χ(t), Pt, at≤t, τ ′), τ ′ ∈ S}
5 for iterate t in post-order(T,n) do
6 /* Compute counter-witnesses (≺-smaller rows) for a witness set M */
7 subs≺(f,M, S) :={~u | ~u ∈ C-Tabs[t], ~u ∈ evolt(f(S)), ~u = 〈C, · · · 〉, C ≺M}
8 /* Link each witness ~u to its counter-witnesses and store the results */
9 W,C-Tabs[t]← {(~u, subs((fw,M, S) ∪ subs⊆(fcw,M, S)) | ~u∈W-Tabs[t], ~u= 〈M, · · · 〉, S ∈ origt(u)}
〈M3.i, σ3.i, C3.i〉 τ3
〈{a1, a2, f}, ∅, ∅〉
〈{a2, f}, ∅, ∅〉
〈M2.i, σ2.i, C2.i〉 τ2
〈{a1, a2, f}, {rf , r2}, ∅〉
〈{a1, a2}, {r2}, {
〈{a1}, ∅〉,
〈{a2}, {r2}〉, 〈∅, ∅〉}〉
〈{a1, f}, {rf}, ∅〉
〈{a1}, ∅, {〈∅, ∅〉}〉
〈{a2, f}, {rf , r2}, ∅〉
〈{a2}, {r2}, {〈∅, ∅〉}〉
〈{f}, {rf}, ∅〉
〈∅, ∅, ∅〉
〈M1.i, σ1.i, C1.i〉 τ1
〈{a1, a2, f}, {rc, rcf}, {
〈{a1, f}, {rcf}〉, 〈{a2, f}, {rcf}〉,
〈{f}, {rcf}〉, 〈{a1, a2}, {rc}〉,
〈{a1}, {rc}〉, 〈{a2}, {rc}〉, 〈∅, {rc}〉}〉
〈{a1, a2}, {rc, rcf}, {〈{a1}, {rc, rcf}〉,
〈{a2}, {rc, rcf}〉, 〈∅, {rc, rcf}〉}〉
〈{a1, f}, {rc, rcf}, {〈{f}, {rcf}〉,
〈{a1}, {rc}〉, 〈∅, {rc}〉}〉
〈{a1}, {rc, rcf}, {〈∅, {rc, rf}〉}〉
〈{a2, f}, {rc, rcf}, {〈{a2}, {rc}〉,
〈{f}, {rcf}〉}〉
〈{a2}, {rc, rcf}, {〈∅, {rc, rcf}〉}〉
〈{f}, {rc, rcf}, {〈∅, {rc}〉}〉
〈∅, {rc, rcf}, ∅〉
〈C1.i, ρ1.i〉 τCMOD1
〈{a1, f}, {rcf}〉, 〈{a2, f}, {rcf}〉,
〈{f}, {rcf}〉, 〈{a1, a2}, {rc}〉, 〈{
a1}, {rc}〉, 〈{a2}, {rc}〉, 〈∅, {rc}〉
〈M1.i, σ1.i, C1.i〉 τ1
〈{a1, a2, f}, {rc, rcf}, ∅
〈{a2, f}, {rc, rcf}, ∅〉
〈M2.i, σ2.i, C2.i〉 τ2
〈{a1, a2, f}, {rf , r2}, ∅〉
〈{a2, f}, {rf , r2}, ∅〉
〈M3.i, σ3.i, C3.i〉 τ3
〈{a1, a2, f}, ∅, ∅〉
〈{a2, f}, ∅, ∅〉
Figure 3 Selected DP tables after DPSINC (left) and after M-DPSINC (right) for TD T .
3.III. Finally, in a bottom-up traversal for every node t in the TD, witnesses and counter-witnesses are
linked using algorithm DPLMOD,CMOD (see Algorithm 3). DPLMOD,CMOD takes previous results and
maps rows in MOD-Tabs[t] to a table (set) of rows in CMOD-Tabs[t].
We already explained the table algorithms DPMOD and DPCMOD in the previous section. The main
part of our multi-pass algorithm is the algorithm DPLMOD,CMOD based on the general algorithm DPLW,C
(Algorithm 3) with W = MOD, C = CMOD, which links those separate tables together. Before we
quickly discuss the core of DPLW,C in Lines 5–9, note that Lines 2–4 introduce auxiliary definitions. Line 2
combines rows of the child nodes of given node t, which is achieved by a product over sets6, where we
drop the order and keep sets only. Line 3 concerns determining for a row ~u its origins (finding preceding
combined rows that lead to ~u using table algorithm W). Line 4 covers deriving succeeding rows for a
certain child row combination its evolution rows via algorithm C. In an implementation, origin as well
as evolution are not computed, but represented via pointer data structures directly linking to W-Tabs[·]
or C-Tabs[·], respectively. Then, the table algorithm DPLW,C applies a post-order traversal and links
witnesses to counter-witnesses in Line 9. DPLW,C searches for origins (orig) of a certain witness ~u, uses
the counter-witnesses (fcw) linked to these origins, and then determines the evolution (evol) in order to
derive counter-witnesses (using subs) of ~u.
6For set I = {1, . . . , n} and sets Si, we define
∏
i∈I Si :=S1 × · · · × Sn = {(s1, . . . , sn) : si ∈ Si}. Moreover, for
∏
i∈I Si,
let
∏̂
i∈ISi :={{{s1}, . . . , {sn}} | (s1, . . . , sn) ∈
∏
i∈I Si}. If for each S ∈
∏̂
i∈ISi and {si} ∈ S, si is a pair with a witness
and a counter-witness part, let fw(S) :=
⋃
{(Wi,Ci)}∈S{{Wi}} and fcw(S) :=
⋃
{(Wi,Ci)}∈S{{Ci}} restrict S to the resp.
(counter-)witness parts.
8
Theorem 1. For a program P of semi-incidence treewidth k := tw(S(P )), the algorithm M-DPSINC is
correct and runs in time O(22k+2 · ‖P‖).
Proof (Sketch). Due to space constraints, we only sketch the proof idea for enumerating answer sets of
disjunctive ASP programs by means of M-DPSINC. Let P be a disjunctive program and k :=tw(S(P )).
We establish a reduction R(P, k) of EnumAsp to EnumMinSAT1, such that there is a one-to-one
correspondence between answer sets and models of the formula, more precisely, for every answer set M
of P and for the resulting instance (F, k′) = R(P, k) the set M ∪ {sol} is a subset-minimal model of F
and k′ = tw(I(F )) with k′ ≤ 7k + 2. We compute in time 2O(k′3) · ‖F‖ a TD of width at most k′ [6]
and add sol to every bag. Using a table algorithm designed for SAT [22] we compute witnesses and
counter-witnesses. Conceptually, one could also modify MOD for this task. To finally show correctness
of linking counter-witnesses to witnesses as presented in DPLMOD,MOD, we have to extend earlier work [5,
Theorem 3.25 and 3.26]. Therefore, we enumerate subset-minimal models of F by following each
witness set containing sol at the root having counter-witnesses ∅ back to the leaves. This runs in
time O(22(7k+2)+2 · ‖P‖), c.f., [5, 11]. A more involved (direct) proof, allows to decrease the runtime
to O(22k+2 · ‖P‖) (even for choice rules).
Example 4. Let k be some integer and Pk be some program that contains the following rules rc :={a1, · · · , ak} ←
f , r2 := ← ¬a2, . . ., rk := ← ¬ak, and rf := ← ¬f and rcf :={f} ← . The rules r1, . . ., rk simulate
that only certain subsets of {a1, · · · , ak} are allowed. Rules rf and rcf enforce that f is set to true. Let
T = (T, χ, t3) be a TD of the semi-incidence graph S(Pk) of program Pk where T = (V,E) with V =
{t1, t2, t3}, E = {(t1, t2), (t2, t3)}, χ(t1) = {a1, · · · , ak, f, rc, rcf}, χ(t2) = {a1, · · · , ak, r2, · · · , rk, rf},
and χ(t3) = ∅. Figure 3 (left) illustrates the tables for program P2 after DPSINC, whereas Figure 3 (right)
presents tables using M-DPSINC, which are exponentially smaller in k, mainly due to cleanup. Observe that
Pass 3.II M-DPSINC, “temporarily” materializes counter-witnesses only for τ1, presented in table τ
CMOD
1 .
Hence, using multi-pass algorithm M-DPSINC results in an exponential speedup. Note that we can trivially
extend the program such that we have the same effect for a TD of minimum width and even if we take
the incidence graph. In practice, programs containing the rules above frequently occur when encoding
by means of saturation [9]. The program Pk and the TD T also reveal that a different TD of the same
width, where f occurs already very early in the bottom-up traversal, would result in a smaller table τ1
even when running DPSINC.
4.2 Implementation Details
Efficient implementations of dynamic programming algorithms on TDs are not a by-product of computa-
tional complexity theory and involve tuning and sophisticated algorithm engineering. Therefore, we
present additional implementation details of algorithm M-DPSINC into our prototypical multi-pass solver
DynASP2.5, including two variations (depgraph, joinsize TDs).
Even though normalizing a TD can be achieved without increasing its width, a normalization may
artificially introduce additional atoms. Resulting in several additional intermediate join nodes among
such artificially introduced atoms requiring a significant amount of total unnecessary computation in
practice. On that account, we use non-normalized tree decompositions. In order to still obtain a fixed-
parameter linear algorithm, we limit the number of children per node to a constant. Moreover, linking
counter-witnesses to witnesses efficiently is crucial. The main challenge is to deal with situations where
a row (witness) might be linked to different set of counter-witnesses depending on different predecessors
of the row (hidden in set notation of the last line in Algorithm 3). In these cases, DynASP2.5 eagerly
creates a “clone” in form of a very light-weighted proxy to the original row and ensures that only the
original row (if at all required) serves as counter-witness during pass three. Together with efficient caches
of counter-witnesses, DynASP2.5 reduces overhead due to clones in practice.
Dedicated data structures are vital. Sets of Witnesses and satisfied rules are represented in the
DynASP2.5 system via constant-size bit vectors. 32-bit integers are used to represent by value 1 whether
an atom is set to true or a rule is satisfied in the respective bit positions according to the bag. A
restriction to 32-bit integers seems reasonable as we assume for now (practical memory limitations)
9
1250
0 20 40 60
# instances
[s
]
C
P
U
 
C
P
U
 ti
m
e St (best TD)
250
500
750
1000
Clasp 3.3.0
DynASP2.5 
DynASP2.5 depgraph
DynASP2.5 joinsize
250
500
750
1000
1250
0 20 40 60
[s
]
Clasp 3.3.0
DynASP2.5 
DynASP2.5 depgraph
DynASP2.5 joinsize# instances
C
P
U
 
C
P
U
 ti
m
e St (avg TD)
solver\time
# Avg solved runtime
TO TD 3.I 3.II 3.III Σ
Clasp 3.3.0 34 - - - - 334.5
DynASP2 68 - - - - -
DynASP2.5 28 0.5 3.5 16.6 264.4 285.0
...depgraph 24 10.9 3.4 15.3 251.3 280.9
...joinsize 32 0.7 3.5 16.5 278.3 299.0
Figure 4 Cactus plots showing best and average runtime among five TDs (left). Number of Timeouts
(TO) and average runtime among solved instances (right).
that our approach works well on TDs of width ≤ 20. Since state-of-the-art computers handle such
constant-sized integers extremely efficient, DynASP2.5 allows for efficient projections and joins of rows,
and subset checks in general. In order to not recompute counter-witnesses (in Pass 3.II) for different
witnesses, we use a three-valued notation of counter-witness sets consisting of atoms set to true (T) or
false (F) or false but true in the witness set (TW) used to build the reduct. Note that the algorithm
enforces that only (TW)-atoms are relevant, i.e., an atom has to occur in a default negation or choice
rule.
Minimum width is not the only optimization goal when computing TDs by means of heuristics.
Instead, using TDs where a certain feature value has been maximized in addition (customized TDs)
works seemingly well in practice [1, 21]. While DynASP2.5 (M-DPSINC) does not take additional TD
features into account, we also implemented a variant (DynASP2.5 depgraph), which prefers one out of
ten TDs that intuitively speaking avoids to introduce head atoms of some rule r in node t, without
having encountered every body atom of r below t, similar to atom dependencies in the program [15].
The variant DynASP2.5 joinsize minimizes bag sizes of child nodes of join nodes, c.f. [2].
4.3 Experimental Evaluation
We performed experiments to investigate the runtime behavior of DynASP2.5 and its variants, in order
to evaluate whether our multi-pass approach can be beneficial and has practical advantages over the
classical single pass approach (DynASP2). Further, we considered the dedicated ASP solver Clasp 3.3.07.
Clearly, we cannot hope to solve programs with graph representations of high treewidth. However,
programs involving real-world graphs such as graph problems on transit graphs admit TDs of acceptable
width to perform DP on TDs. To get a first intuition, we focused on the Steiner tree problem (St) for
our benchmarks. Note that we support the most frequently used SModels input format [24] for our
implementation.
We mainly inspected the CPU time using the average over five runs per instance (five fixed
seeds allow certain variance for heuristic TD computation). For each run, we limited the environ-
ment to 16 GB RAM and 1200 seconds CPU time. We used Clasp with options “--stats=2 --opt-
strategy=usc,pmres,disjoint,stratify --opt-usc-shrink=min -q”, which enable very recent improvements
for unsatisfiable cores [3], and disabled solution printing/recording. We also benchmarked Clasp with
branch-and-bound, which was, however, outperformed by the unsat. core options on all our instances.
Note that without the very recent unsatisfiable core advances Clasp timed out on almost every instance.
We refer to an extended version [12] for more details on the benchmark instances, encodings, and
benchmark environment. The left plot in Figure 4 shows the result of always selecting the best among
five TDs, whereas the right plot concerns average runtime. The table in Figure 4 reports on average
running times (TD computation and Passes 3.I, 3.II, 3.III) among the solved instances and the total
number of timeouts (TO). We consider an instance to time out, when all five TDs exceeded the limit. For
the variants depgraph and joinsize, runtimes for computing and selecting among ten TDs are included.
Our empirical benchmark results confirm that DynASP2.5 exhibits competitive runtime behavior even
for TDs of treewidth around 14. Compared to state-of-the-art ASP solver Clasp, DynASP2.5 is capable
7Clasp is available at https://github.com/potassco/clasp/releases/tag/v3.3.0.
10
of additionally delivering the number of optimal solutions. In particular, variant “depgraph” shows
promising runtimes.
5 Conclusion
In this paper, we presented a novel approach for ASP solving based on ideas from parameterized
complexity. Our algorithms runs in linear time assuming bounded treewidth of the input program. Our
solver applies DP in three passes, thereby avoiding redundancies. Experimental results indicate that our
ASP solver is competitive for certain classes of instances with small treewidth, where the latest version
of the well-known solver Clasp hardly keeps up. An interesting question for future research is whether a
linear amount of passes (incremental DP) can improve the runtime behavior.
References
[1] M. Abseher, F. Dusberger, N. Musliu, and S. Woltran. Improving the efficiency of dynamic
programming on tree decompositions via machine learning. JAIR, 58:829–858, 2017.
[2] M. Abseher, N. Musliu, and S. Woltran. htd – a free, open-source framework for (customized) tree
decompositions and beyond. In CPAIOR’17, 2017. To appear.
[3] M. Alviano and C. Dodaro. Anytime answer set optimization via unsatisfiable core shrinking.
TPLP, 16(5-6):533—551, 2016.
[4] M. Alviano, C. Dodaro, W. Faber, N. Leone, and F. Ricca. WASP: A native ASP solver based on
constraint learning. In LPNMR’13, volume 8148 of LNCS, pages 54–66. Springer, 2013.
[5] B. Bliem, G. Charwat, M. Hecher, and S. Woltran. D-FLATˆ2: Subset minimization in dynamic
programming on tree decompositions made easy. Fund. Inform., 147:27–34, 2016.
[6] H. Bodlaender and A. M. C. A. Koster. Combinatorial optimization on graphs of bounded treewidth.
The Computer J., 51(3):255–269, 2008.
[7] H. Dell. The 2st parameterized algorithms and computational experiments challenge – Track A:
Treewidth. Technical report, 2017.
[8] A. Durand, M. Hermann, and P. G. Kolaitis. Subtractive reductions and complete problems for
counting complexity classes. Th. Comput. Sc., 340(3), 2005.
[9] T. Eiter and G. Gottlob. On the computational cost of disjunctive logic programming: Propositional
case. Ann. Math. Artif. Intell., 15(3–4):289–323, 1995.
[10] J. K. Fichte, M. Hecher, M. Morak, and S. Woltran. Answer Set Solving using Tree Decompositions
and Dynamic Programming - The DynASP2 System -. Technical Report DBAI-TR-2016-101, TU
Wien, 2016.
[11] J. K. Fichte, M. Hecher, M. Morak, and S. Woltran. Answer set solving with bounded treewidth
revisited. In LPNMR’17, 2017. To appear.
[12] J. K. Fichte, M. Hecher, and S. Woltran. DynASP2.5: Dynamic Programming on Tree Decomposi-
tions in Action. CoRR, abs/cs/arXiv:1702.02890, 2017.
[13] M. Gebser, J. Bomanson, and T. Janhunen. Rewriting optimization statements in answer-set
programs. Technical Communications of ICLP 2016, 2016.
[14] M. Gebser, M. Maratea, and F. Ricca. What’s hot in the answer set programming competition. In
AAAI’16, pages 4327–4329. The AAAI Press, 2016.
11
[15] Georg Gottlob, Francesco Scarcello, and Martha Sideri. Fixed-parameter complexity in AI and
nonmonotonic reasoning. AIJ, 138(1-2):55–86, 2002.
[16] M. Hermann and R. Pichler. Complexity of counting the optimal solutions. Th. Comput. Sc.,
410(38–40), 2009. URL: http://dx.doi.org/10.1016/j.tcs.2009.05.025.
[17] M. Jakl, R. Pichler, and S. Woltran. Answer-set programming with bounded treewidth. In IJCAI’09,
2009.
[18] T. Janhunen and I. Niemelä. The answer set programming paradigm. 2016.
[19] B. Kaufmann, M. Gebser, R. Kaminski, and T. Schaub. clasp – a conflict-driven nogood learning
answer set solver, 2015.
[20] C. Koch and N. Leone. Stable model checking made easy. In IJCAI’99, 1999.
[21] M. Morak, N. Musliu, R. Pichler, S. Rümmele, and S. Woltran. Evaluating tree-decomposition
based algorithms for answer set programming. In LION’12, LNCS, pages 130–144. Springer, 2012.
[22] M. Samer and S. Szeider. Algorithms for propositional model counting. J. Desc. Alg., 8(1), 2010.
[23] P. Simons, I. Niemelä, and T. Soininen. Extending and implementing the stable model semantics.
AIJ, 138(1-2):181–234, 2002.
[24] T. Syrjänen. Lparse 1.0 user’s manual. tcs.hut.fi/Software/smodels/lparse.ps, 2002.
12
A Additional Example
We assume again row numbers per table τt, i.e., ut.i = 〈Mt.i, σt.i, Ct.i〉 is the ith-row. Further, for each
counter-witness 〈Ct.i.j , ρt.i.j〉 ∈ Ct.i, j marks its “order” (as depicted in Figure 2 (right)) in set Ct.i.
Example 5. Again, we consider P of Example 1 and T = (·, χ) of Figure 2 as well as tables τ1, . . ., τ34
of Figure 2 (right) using DPSINC. We only discuss certain tables. Table τ1 = {〈∅, ∅, ∅〉} as type(t1) = leaf.
Node t2 introduces atom eab, resulting in table {〈{eab}, ∅, {(∅, ∅)}〉, 〈∅, ∅, ∅〉} (compare to Algorithm 2
L3–5). Then, node t3 introduces rule rab, which is removed in node t4. Note that C3.1.1 = 〈∅, ∅〉 ∈ C3.1.1
does not have a “successor row” in table τ4 since rab is not satisfied (see L11 and Definition 2). Table τ6
is then the result of a chain of introduce nodes, and contains for each witness set M6.i every possible
counter-witness set C6.i.j with C6.i.j ( M6.i. We now discuss table τ12, intuitively containing (a
projection of) (counter-)witnesses of τ10, which satisfy rule rbc after introducing rule rc1. Observe that
there is no succeeding witness set for M6.2 = {eab} in τ10 (nor τ12), since eab ∈ M6.2, but ab 6∈ M6.2
(required to satisfy rb). Rows u12.1, u12.4 form successors of u6.3, while rows u12.2, u12.5 succeed u6.1,
since counter-witness set C6.1.1 has no succeeding row in τ10 because it does not satisfy rb. Remaining
rows u12.3, u12.6 have “origin” u6.4 in τ6.
B Omitted Proofs
B.1 Correctness of DynASP2.5
Bliem et al. [5] have shown that augmentable W-Tabs can be transformed into W,W-Tabs, which easily
allows reading off subset-minimal solutions starting at the table W,W-Tabs[n] for TD root n. We follow
their concepts and define a slightly extended variant of augmentable tables. Therefore, we reduce the
problem of enumerating disjunctive programs to EnumMinSAT1 and show that the resulting tables
of algorithm MOD (see Algorithm 2) are augmentable. In the end, we apply an earlier theorem [5]
transforming MOD-Tabs obtained by DPMOD into MOD,MOD-Tabs via the augmenting function aug(·)
proposed in their work. To this extent, we use auxiliary definitions Child-Tabst, origt(·) and evolt(·)
specified in Algorithm 3.
Definition 3. Let T = (T, χ) be a TD where T = (N, ·, ·), W be a table algorithm, t ∈ N , and
τ ∈W-Tabs[t] be the table for node t. For tuple ~u = 〈M,σ, · · · 〉 ∈ τ, we define α(~u) :=M , β(~u) :=σ. We
inductively define
α∗(τ) :=
⋃
~u∈τ
α(~u) ∪
⋃
τ ′∈Child-Tabst
α∗(τ ′), and
β∗(τ) :=
⋃
~u∈τ
β(~u) ∪
⋃
τ ′∈Child-Tabst
β∗(τ ′).
Moreover, we inductively define the extensions of a row ~u ∈ τ as
E(~u) :=
{
{~u} ∪ U | U ∈
⋃
{{~u′1},...,{~u′k}}∈origt(~u)
{τ1 ∪ · · · ∪ τk | τi ∈ E(~u′i) for all 1 ≤ i ≤ k}
}
.
Remark 1. Any extension U ∈ E(~u) contains ~u and exactly one row from each table that is a descendant
of τ. If ~u is a row of a leaf table, E(~u) = {{~u}} since origt(~u) = {∅} assuming
∏
i∈∅ Si = {()}.
Definition 4. Let τn be the table in W-Tabs for TD root n. We define the set sol(W-Tabs) of solutions
of W-Tabs as sol(W-Tabs) :={α∗(U) | ~u ∈ τn, U ∈ E(~u)}
Definition 5. Let τ be a table in W-Tabs such that τ ′1, . . . , τ ′k are the child tables Child-Tabst and let
~u,~v ∈ τ. We say that x ∈ X(~u) has been X−illegally introduced at ~u if there are {{~u′1}, . . . , {~u′k}} ∈
origt(~u) such that for some 1 ≤ i ≤ k it holds that x /∈ X(~u′i) while x ∈ X∗(τ ′i). Moreover, we say that
x ∈ X(~v) \X(~u) has been X−illegally removed at ~u if there is some U ∈ E(~u) such that x ∈ X(U).
13
Definition 6. We call a table τ augmentable if the following conditions hold:
1. For all rows of the form 〈M, · · · , C〉, we have C = ∅.
2. For all ~u,~v ∈ τ with ~u 6= ~v it holds that α(~u) ∪ β(~u) 6= α(~v) ∪ β(v).
3. For all ~u = 〈M,σ, · · · 〉 ∈ τ, {{~u′1}, . . . , {~u′k}} ∈ origt(~u), 1 ≤ i < j ≤ k, I ∈ E(~u′i) and J ∈ E(~u′j)
it holds that α∗(I) ∩ α∗(J) ⊆M and β∗(I) ∩ β∗(J) ⊆ σ.
4. No element of α∗(τ) has been α-illegally introduced and no element of β∗(τ) has been β-illegally
introduced.
5. No element of α∗(τ) has been α-illegally removed and no element of β∗(τ) has been β-illegally
removed.
We call W-Tabs augmentable if all its tables are augmentable.
It is easy to see that MOD-Tabs are augmentable, that is, Algorithm 2 (DPMOD(·)) computes only
augmentable tables.
Observation 1. MOD-Tabs are augmentable, since DPMOD(·) computes augmentable tables. CMOD-Tabs
are augmentable, since DPCMOD(·) computes augmentable tables.
The following theorem establishes that we can reduce an instance of EnumAsp (restricted to disjunc-
tive input programs) when parameterized by semi-incidence treewidth to an instance of EnumMinSAT1
when parameterized by the treewidth of its incidence graph.
Lemma 1. Given a disjunctive program P of semi-incidence treewidth k = tw(S(P )). We can produce
in time O(‖P‖) a propositional formula F such that the treewidth k′ of the incidence graph I(F )8 is
k′ ≤ 7k + 2 and the answer sets of P and subset-minimal models of F * are in a particular one-to-
one correspondence. More precisely, M is an answer set of P if and only if M ∪Maux ∪ {sol} is a
subset-minimal model of F where Maux is a set of additional variables occurring in F , but not in P and
variables introduced by Tseitin normalization.
Proof. Let P be a disjunctive program of semi-incidence treewidth k = tw(S(P )). First, we construct
a formula F consisting of a conjunction over formulas F r, F impl, F sol, Fmin followed by Tseitin
normalization of F to obtain F *. Among the atoms9 of our formulas will the atoms at(P ) of the program.
Further, for each atom a such that a ∈ B−(r) for some rule r ∈ P , we introduce a fresh atom a′. In
the following, we denote by Z ′ the set {z′ : z ∈ Z} for any set Z and by B−P :=
⋃
r∈P B
−
r . Hence, (B
−
P )
′
denotes a set of fresh atoms for atoms occurring in any negative body. Then, we construct the following
formulas:
F r(r) :=Hr ∨ ¬B+r ∨ (B−r )′ for r ∈ P (1)
F impl(a) :=a→ a′ for a ∈ B−P (2)
F sol(a) :=sol→ (a′ → a) for a ∈ B−P (3)
Fmin :=¬sol→
∨
a′∈(B−P )′
(a′ ∧ ¬a) (4)
F :=
∧
r∈P
F r(r) ∧
∧
a∈B−P
F impl(a) ∧
∧
a∈B−P
F sol(a) ∧ Fmin (5)
Next, we show that M is an answer set of P if and only if M ∪ ([M ∩B−P ])′∪{sol} is a subset-minimal
model of F .
8The incidence graph I(F ) of a propositional formula F in CNF is the bipartite graph that has the variables and
clauses of F as vertices and an edge v c if v is a variable that occurs in c for some clause c ∈ F [22].
9Note that we do not distinguish between atoms and propositional variables in terminology here.
14
(⇒): Let M be an answer set of P . We transform M into Y :=M ∪ ([M ∩B−P ])′ ∪{sol}. Observe that Y
satisfies all subformulas of F and therefore Y  F . It remains to show that Y is a minimal model of F .
Assume towards a contradiction that Y is not a minimal model. Hence, there exists X with X ( Y . We
distinguish the following cases:
1. sol ∈ X: By construction of F we have X  a′ ↔ a for any a′ ∈ (B−P )′, which implies that
X ∩ at(P )  PM . However, this contradicts our assumption that M is an answer set of P .
2. sol 6∈ X: By construction of F there is at least one atom a ∈ B−P with a′ ∈ X, but a 6∈ X.
Consequently, X ∩ at(P )  PM . This contradicts again that M is an answer set of P .
(⇐): Given a formula F that has been constructed from a program P as given above. Then, let Y be a
subset-minimal model of F such that sol ∈ Y . By construction we have for every a′ ∈ Y ∩ (B−P )′ that
a ∈ Y . Hence, we let M = at(P ) ∩ Y . Observe that M satisfies every rule r ∈ P according to (A1) and
is in consequence a model of P . It remains to show that M is indeed an answer set. Assume towards a
contradiction that M is not an answer set. Then there exists a model N (M of the reduct PM . We
distinguish the following cases:
1. N is not a model of P : We construct X :=N ∪ [Y ∩ (B−P )′] and show that X is indeed a model
of F . For this, for every r ∈ P where B−(r) ∩M 6= ∅ we have X  F r(r), since (Y ∩ (B−P )′) ⊆ X
by definition of X. For formulas (A1) constructed by F r(r) using remaining rules r, we also have
X  F r(r), since N  {r}M . In conclusion, X  F and X ( Y , and therefore X contradicts Y is
a subset-minimal model of F .
2. N is also a model of P : Observe that then X :=N ∪ [N ∩B−P ]′ ∪ {sol} is also a model of F , which
contradicts optimality of Y since X ( Y .
By Tseitin normalization, we obtain F ∗, thereby introducing fresh atoms la′ for each a′ ∈ (B−P )′:
F r*(r) :=Hr ∨ ¬B+r ∨ (B−r )′ for r ∈ P (A1*)
F impl*(a) :=¬a ∨ a′ for a ∈ B−P (A2*)
F sol*(a) :=¬sol ∨ (¬a′ ∨ a) for a ∈ B−P (A3*)
Fmin1 :=sol ∨
∨
a′∈B−
P ′
(la′) (A4.1*)
Fmin2 (a) :=¬a′ ∨ a ∨ la′ for a ∈ B−P (A4.2*)
Fmin3 (a) :=¬la′ ∨ a′ for a ∈ B−P (A4.3*)
Fmin4 (a) :=¬la′ ∨ ¬a for a ∈ B−P (A4.4*)
Observe that the Tseitin normalization is correct and that there is a bijection between models of F ∗
and F .
Observe that our transformations runs in linear time and that the size of F * is linear in ‖P‖. It
remains to argue that tw(I(F *)) ≤ 7k + 2. For this, assume that T = (T, χ, n) is an arbitrary but fixed
TD of S(P ) of width w. We construct a new TD T ′ :=(T, χ′, n) where χ′ is defined as follows. For each
TD node t,
χ′(t) :=
⋃
a∈B−P ∩χ(t)
{a′, la′} ∪ [χ(t) ∩ at(P )] ∪ {sol} ∪ cl(t)
where
cl(t) :=
⋃
a∈B−P ∩χ(t)
[F impl*(a), F sol*(a), Fmin2 (a), F
min
3 (a), F
min
4 (a)] ∪ {Fmin1 } ∪
⋃
r∈P∩χ(t)
{F r*(r)}.
It is easy to see that T ′ is indeed a TD for I(F ∗) and that the width of T ′ is at most 7w + 2.
15
Definition 7. We inductively define an augmenting function aug(W-Tabs) that maps each table τ ∈
W-Tabs[t] for node t from an augmentable table to a table in W,W-Tabs[t]. Let the child tables of τ be
called τ ′1, . . . , τ
′
k. For any 1 ≤ i ≤ k and ~u ∈ τi, we write res(~u) to denote {~v ∈ aug(τ ′i) | α(~u) = α(~v)}.
We define aug(τ) as the smallest table that satisfies the following conditions:
1. For any ~u ∈ τ, {{~u′1}, . . . , {~u′k}} ∈ origt(~u) and {{~v′1}, . . . , {~v′k}} ∈
∏̂
1≤i≤k res(~u
′
i), there is a row
~v ∈ aug(τ) with α(~u) = α(~v) and {{~v′1}, . . . , {~v′k}} ∈ origt(~v).
2. For any ~u,~v ∈ aug(τ) with ~u = 〈· · · , C〉 such that α(~v) ⊆ α(~u), {{~u′1}, . . . , {~u′k}} ∈ origt(~u) and
{{~v′1}, . . . , {~v′k}} ∈ origt(~v) the following holds: Let 1 ≤ i ≤ k with ~u′i = 〈· · · , Ci〉, ~ci = 〈Ci, · · · 〉 ∈
(Ci ∪ {~u′i}) with Ci ⊆ α(~v′i), and 1 ≤ j ≤ k with ~cj 6= ~u′j or α(~v) ( α(~u). Then, there is a row
~c ∈ C with α(~c) ⊆ α(~v) if and only if ~c ∈ τ and {{~c1}, . . . , {~ck}} ∈ origt(~c).
For W-Tabs, we write aug(W-Tabs) to denote the result isomorphic to W,W-Tabs where each table τ in
W-Tabs corresponds to aug(τ).
Proposition 2. Let W-Tabs be augmentable. Then,
sol(aug(W-Tabs)) = {M ∈ sol(W-Tabs) | @M ′ ∈ sol(W-Tabs) : M ′ (M}.
Proof (Sketch). The proof follows previous work [5]. We sketch only differences from their work. Any
row ~u ∈ τ of any table τ not only consists of set α(~u) being subject to subset-minimization and relevant
to solving EnumAsp. In addition, our definitions presented above also allow “auxiliary” sets β(~u) per
row ~u, which are not subject to the minimization. Moreover, by the correctness of the table algorithm
INC by Fichte and Szeider [11], we only require to store a set C of counter-witnesses 〈C, · · · 〉 ∈ C per
witness set M , where each C forms a strictly ⊂-smaller model of M . As a consequence, there is no need
to differ between sets of counter-witnesses, which are strictly included or not, see [5]. Finally, we do not
need to care about duplicate rows (solved via compression function compr(·) in [5]) in τ, since τ is a set.
Theorem 2. EnumAsp when the input is restricted to disjunctive programs can be solved in time 22
(7k+4) ·
‖P‖ computing aug(DPMOD(·)), where k refers to the treewidth of S(P ).
Proof (Sketch). First, we use reduction R(P, k) = (F ∗, k′) defined in Lemma 1 to construct an instance
of SAT given our disjunctive ASP program P . Note that k′ = tw(I(F ∗)) ≤ 7k + 2. Then, we can
compute in time 2O(k
′3) · |I(F ∗)| a tree decomposition of width at most k′ [6]. Note that since we require
to look for solutions containing sol at the root, we modify each bag of T such that it contains sol.
We call the resulting tree decomposition T ′. We compute aug(DPMOD(T ′)) using formula F ∗ (see
Algorithm 2). Finally, by Theorem 2 and Lemma 1, we conclude that answer sets of P correspond to
{M ∈ sol(aug(DPMOD(T ′))) | sol ∈M, @M ′ ∈ sol(DPMOD(T ′)) : M ′ (M}.
The complexity proof sketched in [5] only cares about the runtime being polynomial. In fact, the
algorithm can be carried out in linear time, following complexity proofs presented by Fichte et al. [11],
which leads to a worst-case runtime of 22
(7k+4) · ‖P‖.
We can now even provide a “constructive definition” of the augmenting function aug(·).
Proposition 3. The resulting table aug(W-Tabs) obtained via DPW(T ) for any TD T is equivalent to
the table DPLW,W(T ) as given in Algorithm 3.
Proof (Idea). Intuitively, (1.) of Definition 7 concerns completeness, i.e., ensures that no row is left out
during the augmentation, and is ensured by row 7 of Algorithm 3 since each ~u ∈W-Tabs is preserved.
The second condition (2.) enforces that there is no missing counter-witness for any witness, and the idea
is that whenever two witnesses ~u,~v ∈ τ are in a subset relation (α(~v) ⊆ α(~u)) and their corresponding
linked counter-witnesses (fcw) of the corresponding origins (orig) are in a strict subset relation, then
there is some counter-witness c for u if and only if ~c ∈ τ is the successor (evol) of these corresponding
linked counter-witnesses. Intuitively, we can not miss any counter-witnesses in DPLW,W(T ) required by
16
(2.), since this required that there are two rows ~u′, ~v′ ∈ τ ′ with α(~v) = α(~u) for one table τ ′. Now let
the corresponding succeeding rows ~u,~v ∈ τ (i.e., ~u ∈ evolt({{~u′}}), ~v ∈ evolt({{~v′}}), respectively) with
α(~v) ( α(~u), β(~v) 6⊆ β(~u) and β(~v) 6⊇ β(~u), mark the first encounter of a missing counter-witness. Since
β(~v) is incomparable to β(~u), we conclude that the first encounter has to be in a table preceeding τ. To
conclude, one can show that DPLW,W(T ) does not contain “too many” rows, which do not fall under
conditions (1.) and (2.).
Theorem 2 works not only for disjunctive ASP via reduction to EnumMinSAT1, where witnesses
and counter-witnesses are derived with the same table algorithm MOD. In fact, one can also link
counter-witnesses to witnesses by means of DPLW,C(·), thereby using table algorithms W,C for computing
witnesses and counter-witnesses, respectively. In order to show correctness of algorithm DPLMOD,CMOD(·)
(Theorem 1) working for any ASP program, it is required to extend the definition of the augmenting
function aug(·) such that it is capable of using two different tables.
Proposition 4. Problem EnumAsp can be solved in time f(k)·‖P‖ computing DPLMOD,CMOD(·), where k
refers to the treewidth of S(P ) and f is a computable function.
C Additional Information on the Benchmarks
C.1 Benchmark Sets
In this paper, we mainly presented10 the Steiner tree problem using public transport networks. We
also considered benchmarks for counting answer sets as carried out in earlier work [11]. DynASP2.5
performs slightly better than DynASP2 on those instances. Hence, we do not report them here.
C.1.1 Transit Graphs
The instance graphs have been extracted from publicly available mass transit data feeds and splited by
transportation type, e.g., train, metro, tram, combinations. We heuristically computed tree decomposi-
tions [2] and obtained relatively fast decompositions of small width unless detailed bus networks were
present. Among the graphs considered were public transit networks of the cities London, Bangladesh,
Timisoara, and Paris.
C.1.2 Steiner Tree Problem
Picture yourself as the head of a famous internet service provider, which is about to provide high-speed
internet for their most-prestigious customers in public administration in order to increase productivity
levels beyond usual standards. However, these well-paying customers have to be connected via expensive
fibre cables. The good news is that the city council already confirmed that you are allowed to use
existing cable ducts, which basically adhere to the city’s transit network. We assumed for simplicity,
that edges have unit costs, and randomly generated a set of terminal stations — which are compliant
with the customers — among our transit stations (vertices). The goal is to search for a set of transit
connections of minimal cardinality such that the customers can be connected for example when putting
fibre cables along the transit network.
An encoding for this problem is depicted in Listing 1 and assumes a specification of the graph
(via edge) and the facilities (terminalVertex ) of our customers in public administration as well as the
number (numVertices) of vertices. The encoding is based on the saturation technique [9] and in fact
outperformed a different encoding presented in Listing 2 on all our instances using both solvers, Clasp
and DynASP2.5. At first sight, this observation seems quite surprising, however, we benchmarked on
more than 60 graphs with 10 varying decompositions for each solver variant and additional configurations
and different encodings for Clasp.
10Benchmarks, encodings, and results are available at https://github.com/daajoe/dynasp experiments/tree/ipec2017.
17
%
%
%
vertex(X) ← edge(X,_).
vertex(Y) ← edge(_,Y).
edge(X,Y) ← edge(Y,X).
%
0 { selectedEdge(X,Y) } 1 ← edge(X,Y), X < Y.
%
s1(X) ∨ s2(X) ← vertex(X).
%
saturate ← selectedEdge(X,Y), s1(X), s2(Y), X < Y.
saturate ← selectedEdge(X,Y), s2(X), s1(Y), X < Y.
%
%
saturate ← N #count{ X : s1(X), terminalVertex(X) }, numVertices(N).
saturate ← N #count{ X : s2(X), terminalVertex(X) }, numVertices(N).
s1(X) ← saturate , vertex(X).
s2(X) ← saturate , vertex(X).
← not saturate.
%
#minimize{ 1,X,Y : selectedEdge(X,Y) }.
Listing 1 Encoding for St
%
%
edge(X,Y) ← edge(Y,X).
{ selectedEdge(X,Y) : edge(X,Y), X < Y }.
%
reached(Y) ← Y = #min{ X : terminalVertex(X) }.
reached(Y) ← reached(X), selectedEdge(X,Y).
reached(Y) ← reached(X), selectedEdge(Y,X).
← terminalVertex(X), not reached(X).
%
#minimize{ 1,X,Y : selectedEdge(X,Y) }.
Listing 2 Alternative encoding for St
Example 6 (Steiner Tree). Let G = (V,E) be a graph and VT ⊆ V . A uniform Steiner tree on G is a
subgraph SG = (VS , ES) of G such that VT ⊆ VS and for each distinct pair v, w ∈ VT there is a path
from v to w. The Steiner tree problem Enum-St asks to output all uniform Steiner trees. We encode
Enum-St into an ASP program as follows: Among the atoms of our program will be an atom av for each
vertex v ∈ VT , and an atom evw for each edge vw ∈ E assuming v < w for an arbitrary, but fixed total
ordering < among V . Let s be an arbitrary vertex s ∈ VT . We generate program P (G,VT ) :={{evw} ←
;  evw | vw ∈ E} ∪ {av ← aw, evw; aw ← av, evw | vw ∈ E, v < w} ∪ {← ¬av | v ∈ VT } ∪ {as ←}.
It is easy to see that the answer sets of the program and the uniform Steiner trees are in a one-to-one
correspondence.
C.1.3 Other Graph Problems
We refer to the technical report [10] for comprehensive benchmark results using common graph problems
and comparing different table algorithms of DynASP2 and DynASP2.5. There we investigated various
18
graph problems including, but not limited to, variants of graph coloring, dominating set and vertex
cover. Note that, however, the cluster setup was slightly different.
C.2 Benchmark Environment
The experiments presented ran on an Ubuntu 16.04.1 LTS Linux cluster of 3 nodes with two Intel Xeon
E5-2650 CPUs of 12 physical cores each at 2.2 Ghz clock speed and 256 GB RAM. All solvers have been
compiled with gcc version 4.9.3 and executed in single core mode.
19

