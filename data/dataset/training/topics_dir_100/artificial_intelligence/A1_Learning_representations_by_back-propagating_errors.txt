(V)
LEARNING INTERNAL REPRESENTATIONS
BERROR PROPAGATION
two
David E. Ruineihart, Geoffrey E. Hint..,
and Ronald J. Williams
0 September 1985
4 ICS Report 8506
COGNITIVE IaQ I
SCIENCE
INSTITUTE FOR COGNITIVE SCIENCE
UNIVERSITY OF CALIFORNIA, SAN DIEGO LA JOLLA, CALIFORNIA 92093
862 18 120,
4-
U-
LEARNING INTERNAL REPRESENTATIONS
BY ERROR PROPAGATION
David E. Rumelhart, Geoffrey E. Hinton,
and Ronald J. Williams DTIC
September 1985 AL"LECT
FEB 20 M
ICS Report M50 SLD
David E. Rumelhart Geoffrey E. Hinton
Institute for Cognitive Science Department of Computer Science
University of California, San Diego Carnegie-Mellon University
Ronald J. Williams
Institute for Cognitive Science Di' AL
University of California, San Diego _
xow"d io n publ@
DkltAtfikuim fi1Id
To be published in D. E. Rumelhart & i. L. McClelland (Eds.), Parallel Distributed
Processing: Explorations in the Microstructure of Cognition. Vol. 1: Foundations.
Cambridge, MA: Bradford Books/MIT Press.
This research wa supported by Contract NOD14.45-K0450, NR 667-58 with the Personnel Lad Training Research Pro-
gams of the Office of Naval Research and by grants from the System Development Foundation. Requests for rm-
prints siould be seat to David E. Ramdhut, Institute for Cognitive Science, C4)13; University of California, San
Diego; La Jolla, CA 92093.
Copyright 0 1985 by David B. Rumeliat, Geoffrey E. finton. and Ronald i. Williams
U P.
Unclassified
SECURITY CLASSIFICATION OF THIS PAGE
REPORT DOCUMENTATION PAGE
la. REPORT SECURITY CLASSIFICATION lb RESTRICTIVE MARKINGS
Unclassified ,,
2a. SECURITY CLASSIFICATION AUTHORITY 3. DISTRIBUTION/AVAILABILITY OF REPORT
Approved for public release;
2b. DECLASSIFICATION I DOWNGRADING SCHEDULE distribution unlimited.
4. PERFORMING ORGANIZATION REPORT NUMBER(S) S. MONITORING ORGANIZATION REPORT NUMBER(S)
ICS 8506
6a. NAME OF PERFORMING ORGANIZATION 6b. OFFICE SYMBOL 7a NAME OF MONITORING ORGANIZATION
Institute for Cognitive (If applicable)
Science I
6c. ADDRESS (City, State, and ZIP Code) 7b. ADDRESS (City, State, and ZIP Code)
C-015
University of California, San Diego
La Jolla, CA 92093
Ba. NAME OF FUNDING/SPONSORING 8b. OFFICE SYMBOL 9. PROCUREMENT INSTRUMENT IDENTIFICATION NUMBER
ORGANIZATION (If applicable)
Personnel & Training Researc N00014-85-K-0450
8c. ADDRESS (City, State, and ZIP Code) 10. SOURCE OF FUNDING NUMBERS
Code 1142 PT PROGRAM PROJECT TASK WORK UNiT
Office of Naval Research ELEMENT NO. NO. NO. ACCESSION NO
800 N. Quincy St., Arlington, VA 22217-5000 NR 667-548
11. TITLE (Include Security Classification)
Learning Internal Representations by Error Propagation
12. PERSONAL AUTHOR(S)
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams
13a. TYPE OF REPORT 13b. TIME COVERED 14. DATE OF REPORT (ear, Month, Day) S. PAGE COUNT
Technical FROMMar 85 TO Sept 8 September 1985 34
16 SUPPLEMFNTARY NOTATION To be published in J. L. McClelland, D. E. Rumelhart, & the PDP Research Group,
Parallel Dislribuled Processing: Exploratiois in the Microstrucure of Cognkion: Vol 1. Founsdations. Cambridge, MA:
Bradford nlooks/Mnr Press.
17. COSATI CODES 18. SUBJECT TERMS (Continue on reverse if necessary and identify by block number)
FIELD GROUP SUB-GROUP -learning; networks; perceptrons; adaptive systems;
learning machines; back propagation
19. ABSTRACT (Continue on reverse if necessary and identify by block number)
This paper presents a generalization of the perceptron learning procedure for learning the correct
sets of connections for arbitrary networks. The rule, called the generalized delta rule, is a simple
scheme for implementing a gradient descent method for finding weights that minimize the sum squared
error of the system's performance. The major theoretical contribution of the work is the procedure
called error propagation, whereby the gradient can be determined by individual units of the network
based only on locally available information. The major empirical contribution of the work is to show
that thc problem of local minima is not serious in this application of gradient descent. i
20. DISTRIBUTION/AVAILABILITY OF ABSTRACT 21. ABSTRACT SECURITY CLASSIFICATION
QUNCLASSIFIED/UNLIMITED 0 SAME AS RPT. 0 DTIC USERS Unclassified
22a NAME OF RESPONSIBLE INDIVIDUAL 22b. TELEPHONE (Include Area Code) 22c. OFFICE SYMBOL
DO FORM 1473, 84 MAR 83 APR edition may be used until exhausted. SECURITY CLASSIFICATION OF THIS PAGE
All other editions are obsolete. Unclassified
Contents
THE PROBLEM .................................................................................
THE GEN4ERALIZED DELTA RULE........................................................ 4
The delta rule and gradient descent................................................ 4
The delta rule for semilinear activation functions in feedforward networks. 5
S[MULATION RESULTS ..................................................................... 8
A useful activation function .......................................................... 8
The learning rate..................................................................... 9
Symmetry breaking................................................................... 10
The XOR Problem........................................................................ 10
Parity ................................................................................... 12
The Encoding Problem................................................................... 14
Symmetry ............................................................................... 17
Addition ................................................................................ 18
*The Negation Problem.................................................................... 21
The T-C Problem .......................................................................... 22
More Simulation Results................................................................. 26
SOME FURTHER GENERALIZATIONS ................................................... 26
The Generalized Delta Rule and Sigma-Pi Units .................................... 26
Recurrent Nets......................................................................... 2
Leff nng to be a shift register ...................................................... 29
Learning to complete sequences ..................................................... 30
CONCLUSION................................................................................... 31
REFERENCES................................................................................ 33
Uiannoun'ced 13
Learning Internal Representations
by Error Propagation
DAVID E. RUMELHART, GEOFFREY E. HINTON, and RONALD J. WILLIAMS
THE PROBLEM
We now have a rather good understanding of simple two-layer associative networks in which
a set of input patterns arriving at an input layer are mapped directly to a set of output patterns
at an output layer. Such networks have no hidden units. They involve only input and owput
units. In these cases there is no internal represenatmon. The coding provided by the external
world must suffice. These networks have proved useful in a wide variety of applications (cf.
Chapters 2, 17, and 18). Perhaps the essential character of such networks is that they map simi-
lar input patterns to similar output patterns. This is what allows these networks to make rea-
sonable generalizations and perform reasonably on patterns that have never before been
presented. The similarity of patterns in a PDP system is determined by their overlap. The
overlap in such networks is determined outside the learning system itself-by whatever pro-
duces the patterns.
The constraint that similar input patterns lead to similar outputs can lead to an inability of
the system to learn certain mappings from input to output. Whenever the representation pro-
vided by the outside world is such that the similarity structure of the input and output pat-
tcrns are very different, a network without internal representations (i.e., a network without
hidden units) will be unable to perform the necessary mappings. A classic example of this case
is the exclusive-or (XOR) problem illustrated in Table 1. Here we see that those patterns
which overlap least arc supposed to generate identical output values. This problem and many
others like it cannot be performed by networks without hidden units with which to create
TABLE I
Input Patterns Output oPatterns
01 -0
2 RUMELHART. HrwrON. and WILLIAMS
TABLE 2
Input Patterns Output Patterns
00 0
010 1.
11 - 0
their own internal representations of the input patterns. It is interesting to note that had the
input patterns contained a third input taking the value 1 whenever the first two have value 1 as
shown in Table 2, a two-layer system would be able to solve the problem.
Minsky and Papert (1969) have provided a very careful analysis of conditions under which
such systems are capable of carrying out the required mappings. They show that in a large
number of interesting cases, networks of this kind are incapable of solving the problems. On
the other hand, as Minsky and Papert also pointed out, if there is a layer of simple perceptron-
like hidden units, as shown in Figure 1, with which the original input pattern can be aug-
mented, there is always a recoding (i.e., an internal representation) of the input patterns in the
hidden units in which the similarity of the patterns among the hidden units can support any
required mapping from the input to the output units. Thus, if we have the right connections
from the input units to a large enough set of hidden units, we can always find a representation
Output Patterns
Internal
O" *Representation
Units
Input Patterns
FIGURE 1. A multilayer network. In this case the information coming to the input units is receded into an inter-
nal representation and the outputs are generated by the internal representation rather than by the original pattern.
Input patterns can always be encoded, if there are enough hidden units, in a form so that the appropriate output pat-
tern can be generated from any input pattern.
6 M
LEARNING miNLRNAL REPRE ENTATIONS 3
that will perform any mapping from input to output through these hidden units. In the case
of the XOR problem, the addition of a feature that detects the conjunction of the input units
changes the similarity structure of the patterns sufficiently to allow the solution to be learned.
As illustrated in Figure 2, this can be done with a single hidden unit. The numbers on the
arrows represent the strengths of the connections among the units. The numbers written in
the circles represent the thresholds of the units. The value of +1.5 for the threshold of the
hidden unit insures that it will be turned on only when both input units are on. The value 0.5
for the output unit insures that it will turn on only when it receives a net positive input
greater than 0.5. The weight of -2 from the hidden uait to the output unit insures that the
output unit will not come on when both input units arc on. Note that from the point of view
of the output unit, the hidden unit is treated as simply another input unit. It is as if the input
patterns consisted of three rather than two units.
The existence of networks such as this illustrates the potential power of hidden units and
internal representations. The problem, as noted by Minsky and Papert, is that whereas there is
a very simple guaranteed learning rule for all problems that can be solved without hidden units,
namely, the perceptron convergence procedure (or the variation due originally to Widrow and
Hoff, 1960, which we call the delta rule; see Chapter 11), there is no equally powerful rule for
learning in networks with hidden units. There have been three basic responses to this lack.
One response is represented by competitive learning (Chapter 5) in which simple unsupervised
learning rules are employed so that useful hidden units develop. Although these approaches are
promising, there is no external force to insure that hidden units appropriate for the required
mapping are developed. The second response is to simply asswne an internal representation
that, on some a priori grounds, seems reasonable. This is the tack taken in the chapter on verb
learning (Chapter 18) and in theinteractive activation model of word perception (McClelland
& Rumelhart, 1981; Rumelhart & McClelland, 1982). The third approach is to attempt to
develop a learning procedure capable of learning an internal representation adequate for per-
forming the task at hand. One such development is presented in the discussion of Boltzmann
machines in Chapter 7. As we have seen, this procedure involves the use of stochastic units,
requires the network to reach equilibrium in two different phases, and is limited to symmetric
networks. Another recent approach, also employing stochastic units, has been developed by
Barto (1985) and various of his colleagues (cf. Barto & Anandan, 1985). In this chapter we
.5 Output Unit
+1 -21 +1
Hidden Unit
S +1 +1
Input Units
FIGURE 2. A simple XOR network with one hidden unit. See text for explanation.
d '% .. - P I .
4 RUMELHAxr, HmroN, and WILLIAMS
present another alternative that works with deterministic units, that involves only local compu-
tations, and that is a clear generalization of the delta rule. We call this the generalized delta
rule. From other considerations, Parker (1985) has independently derived a similar generaliza-
tion, which he calls learning-logic. Le Cun (1985) has also studied a roughly similar learning
scheme. In the remainder of this chapter we first derive the generalized delta rule, then we
illustrate its use by providing some results of our simulations, and finally we indicate some
further generalizations of the basic idea.
THE GENERALIZED DELTA RULE
The learning pro cedure we propose involves the presentation of a set of pairs of input and
output patterns. The system first uses the input vector to produce its own output vector and
then compares this with the desired output, or target vector. If there is no difference, no learn-
ing takes place. Otherwise the weights are changed to reduce the difference. In this case, with
no hidden units, this generates the standard delta rule as described in Chapters 2 and 11. The
rule for changing weights following presentation of input/output pair p is given by
AP-J, = *I(tpj - Op,) ip, = napjipt (1)
*- where tj is the target input for jth component of the output pattern for pattern p, o, is the
jth element of the actual output pattern produced by the presentation of input pattern p, ipi
is the value of the ith element of the input pattern 8,j = tj - opj, and Awj, is the change to
be made to the weight from the i th to the j th unit following presentation of pattern p.
The delta rule and gradient descent. There are many ways of deriving this rule. For
present purposes, it is useful to see that for linear units it minimizes the squares of the
differences between the actual and the desired output values summed over the output units
and all pairs of input/output vectors. One way to show this is to show that the derivative of
the error measure with respect to each weight is proportional to the weight change dictated by
the delta rule, with negative constant of proportionality. This corresponds to performing
steepest descent on a surface in weight space whose height at any point in weight space is equal
to the error measure. (Note that some of the following sections are written in italics. These
sections constitute informal derivations of the claims made in the surrounding text and can be
omitted by the reader who finds such derivations tedious.)
To be more specific, then, let
E 1 , t-opj ) (2)
be our measure of the error on input/output pa. err p and let E = bEp e our overall measure of the
error. We wish to show that the delta rule implements a gradient descent in E when the units are linear. We
will proceed by simply showing that
a• .. . 8pjipi,
* which is proportional to A. w,, as prescribed by the delta rule. When there are no hidden units it is straight-
forward to compute the relevant derivative. For this purpose we use the chain rule to write the derivative as
the product of two parts. the derivative of the error with respect to the output of the unit times the derivative
:(k L 1
LEARNING IRnA 11±PREMSEWATIONS
of the outpit with respect to the weight.
BEp = 2E, Bop (3)
1Rwj CIopj awj,
The first part tells how the error changes with the output of the jth unit and the second part tells how much
changing wj1 changes that outWt. Now, the derivatives are easy to compute. First,from Equation 2
. (t - o,)- 8 (4)
aopj
Not surprisingly, the contribution of unit uj to the error is simply proportional to &. Moreover. sinLe we
have linear units.
from which we conclude that
Thus, substituting back into Equation 3. we see that
* BEp (6)
Bw
as desired. Now, combining this with the observation that
' BE BE,
should lead us to conclude that the net change in wj, after one complete cycle of pattern presentations is pro-
portional to this derivative and hence that the delta rule implements a gradient descent in E. In fact, this is
strictly true only if the values of the weights are not changed during this cycle. By changing the weights after
each pattern is presented we depart to some extent from a true gradient descent in E. Nevertheless, pro-
vided the learning rate (i.e.. the constant of proportionality) is safficiently small, this departure will be negli-
gible and the delta rule will implement a very close approximation to gradient descent in sum-squared error.
In particular, with small enough learning rate. the delta ride will find a set of weights minimizing this error
function.
The delta rule for semilinear activation functions in feedforward networks. We have
shown how the standard delta rule essentially implements gradient descent in sum-squared
error for linear activation functions. In this case, without hidden units, the error surface is
shaped like a bowl with only one minimum, so gradient descent is guaranteed to find the best
set of weights. With hidden units, however, it is not so obvious how to compute the deriva-
tives, and the error surface is not concave upwards, so there is the danger of getting stuck in
local minima. The main theoretical contribution of this chapter is to show that there is an
efficient way of computing the derivatives. The main empirical contribution is to show that
the apparently fatal problem of local minima is irrelevant in a wide variety of learning tasks.
At the end of the chapter we show how the generalized delta rule can be applied to arbitrary
networks, but, to begin with, we confine ourselves to layered feedforward networks. In these
networks, the input units are the bottom layer and the output units are the top layer. There
can be many layers of hidden units in between, but every unit must send its output to higher
layers than its own and must receive its input from lower layers than its own. Given an input
vector, the output vector is computed by a forward pass which computes the activity levels of
each layer in turn using the already computed activity levels in the earlier layers.
Since we are primarily interested in extending this result to the case with hidden units and
since, for reasons outlined in Chapter 2, hidden units with linear activation functions provide
6 RUMELHART, HIjTON, and WILUAMIS
no advantage, we begin by generalizing our analysis to the set of nonlinear activation functions
which wz call semilinear (see Chapter 2). A semilinear activation function is one in which the
output of a unit is a differentiable function of the net total input,
,etpj = wjto,,, (7)
where oj = it if unit i is an input unit. Thus, a semilinear activation function is one in which
o, j = f j (net,,) (8)
and f is differentiable. The generalized delta rule works if the network consists of units hav-
ing semilinear activation functions. Notice that linear threshold units do not satisfy the
,' requirement because their derivative is infinite at the threshold and zero elsewhere.
To get the correct generalization of the delta rule, we must set
A, wo,, - "-_
aw1,
where E is the same sum-squared error function defined earlier. As in the standard delta rule it is again
useful to see this derivative as resulting from the product of two parts: one part reflecting the change in
error as a function of the change in the net input to the unit and one part representing the effect of changing
* a particular weight on the net input. Thus we can write
aOE' 8 E, 8 netp (9)
a 8Wj 8 netj 8w,
By Equation 7 we see that the second factor is
anet~j 1 (10)
= - = Op,.
Now let us define
aEP
bpi anetp,
(By comparing this to Equation 4. note that this is consistent with the definition of 8p used in the original
delta rule for linear units since Oj = netj when unit Uj is linear.) Equation 9 thus has the equivalent form
8E P p iO P
This says that to implement gradient descent in E we should make our weight changes according to
just as in the standard delta rule. The trick is to figure out what 8P, should be for each unit ua in the net-
work. The interesting result, which we now derive, is that there is a simple recursive computation of these 8's
which can be implemented bZ propagating error signals backward through the network.
To compute 8, , we apply the chain rule to write this partial derivative as the product of two2";a netvj
factors, one factor reflecting the change in error as a function of the output of the unit and one reflecting the
* N
LEARNING ITRNAl- a1PRESEPrAIONS 7
change in the output as a function of changes in the input. Thus, we have
8Ep = 8Ep Bop (12)
Bne,, )op0 anefpj
Let us compute the second factor. By Equation 8 we see that
a _One~p =f "1(netpl )"
which is simply the derivative of the squashing function f j for the j th unit, evaluated at the net input netp to
that unit. To compute the first factor, we consider two cases. First, assume that unit uj is an output unit of
the network. In this case, it follows from the definition of Ep that
±_P (t -
which is the same result as we obtained with the standard delta rule. Substituting for the two factors in Equa-
tion 12. we get
5,, = (t,, - opj )f 'j (ne t, (3
for any output unit Uj. If Uj is not an output unit we use the chain rule to write
* P An, ._ BE, I- I 111Wkj ~8,,&WIk1
c.ntp f'ap kc1netk 80,1 k C1ne PA k
In this case, substituting for the two factors in Equation 12 yields
whenever u1 is not an output unit. Equations 13 and 14 give a recursive procedure for computing the 8's for
all units in the network, which are then used to compute the weight changes in the network according to Equa-
tion 11. This procedure constitutes the generalized delta rule for a feedforward network of semilinear units.
These results can be summarized in three equations. First, the generalized delta rule has
exactly the same form as the standard delta rule of Equation 1. The weight on each line
should be changed by an amount proportional to the product of an error signal, 8, available to
the unit receiving input along that line and the output of the unit sending activation along
that line. In symbols,
Ap wj = 1OpjOpt.
The other two equations specify the error signal. Essentially, the determination of the error
signal is a recursive process which starts with the output units. If a unit is an output unit, its
error signal is very similar to the standard delta rule. It is given by
'.'."" 8p, = (toj - op/)f " (net,,)
where f "j(netij) is the derivative of the semilinear activation function which maps the total
* input to the unit to an output value. Finally, the error signal for hidden units for which there
is no specified target is determined recursively in terms of the error signals of the units to
i%A, . "'s% .-, .' .- ' ... " . , . ,' . , ' . : . , ' , , .. "", , % ", , ., , ..' ' , . -, . ' ',,  " ,. , ,, - , , , #" ,,
8 RUIGELHART. HwqroN, and wiaLuAws
which it directly connects and the weights of those connections. That is,
bj, =f "j (nej),) w Wkj
k
whenever the unit is not an output unit.
The application of the generalized delta rule, thus, involves two phases: During the first
phase the input is presented and propagated forward through the network to compute the out-
put value oj for each unit. This output is then compared with the targets, resulting in an
error signal 6,j for each output unit. The second phase involves a backward pass through the
network (analogous to the initial forward pass) during which the error signal is passed to each
unit in the network and the appropriate weight changes are made. This second, backward pass
allows the recursive computation of 8 as indicated above. The first step is to compute 8 for
each of the output units. This is simply the difference between the actual and desired output
values times the derivative of the squashing function. We can then compute weight changes
for all connections that feed into the final layer. After this is done, then compute a's for all
units in the penultimate layer. This propagates the errors back one layer, and the same process
can be repeated for every layer. The backward pass has the same computational complexity as
the forward pass, and so it is not unduly expensive.
We have now generated a gradient descent method for finding weights in any feedforward
network with semilinear units. Before reporting our results with these networks, it is useful to
note some further observations. It is interesting that not all weights need be variable. Any
number of weights in the network can be fixed. In this case, error is still propagated as before;
the fixed weights are simply not modified. It should also be noted that there is no reason why
some output units might not receive inputs from other output units in earlier layers. In this
case, those units receive two different kinds of error: that from the direct comparison with the
target and that passed through the other output units whose activation it affects. In this case,
the correct procedure is to simply add the weight changes dictated by the direct comparison to
that propagated back from the other output units.
SIMULATION RESULTS
We now have a learning procedure which could, in principle, evolve a set of weights to pro-
duce an arbitrary mapping from input to output. However, the procedure we have produced is
a gradient descent procedure and, as such, is bound by all of the problems of any hill climbing
procedure--namcly, the problem of local maxima or (in our case) minima. Moreover, there is a
question of how long it might take a system to learn. Even if we could guarantee that it
would eventually find a solution, there is the question of whether our procedure could learn in
a reasonable period of time. It is interesting to ask what hidden units the system actually
develops in the solution of particular problems. This is the question of what kinds of internal
representations the system actually creates. We do not yet have definitive answers to these
questions. However, we have carried out many simulations which lead us to be optimistic
about the local minima and time questions and to be surprised by the kinds of representations
our learning mechanism discovers. Before proceeding with our results, we must describe our
- simulation system in more detail. In particular, we must specify an activation function and
show how the system can compute the derivative of this function.
A useful activation function. In our above derivations the derivative of the activation func-
tion of unit u, f "J(netj), always played a role. This implies that we need an activation func-
tion for which a derivative exists. It is interesting to note that the linear threshold function,
on which the perceptron is based, is discontinuous and hence will not suffice for the general-
ized delta rule. Similarly, since a linear system achieves no advantage from hidden units, a
ono
LIEARMW;~ i0flhINAL REMISLFGA11ONS '
linear activation function will not suffice either. Thus, we need a continuous, nonlinear activa-
tion function. In most of our experiments we have used the logistic activation function in
which
1 (15)o = - ( °,
1+e
where Oj is a bias similar in function to a threshold. I In order to apply our learning rule, we
need to know the derivative of this function with respect to its total input, Mtj, where
netpj = Ywjjoj + Oj. It is easy to show that this derivative is given by
¢= o,,(I - opj).
Thus, for the logistic activation function, the error signal, 8,j, for an output unit is given by
_aj = (tpj - os)o,,(I - opj).
and the error for an arbitrary hidden Nj is given by
-= opj)( - Wkj
It should be noted that the derivative, opj(1 - oj), reaches its maximum for opj = 0.5 and,
since 0,, o,, 1, approaches its minimum as opj approaches zero or one. Since the amount of
change in a given weight is proportional to this derivative, weights will be changed most for
those units that are near their midrange and, in some sense, not yet committed to being either
on or off. This feature, we believe, contributes to the stability of the learning of the system.
One other feature of this activation function should be noted. The system can not actually
reach its extreme values of 1 or 0 without infinitely large weights. Therefore, in a practical
learning situation in which the desired outputs are binary {0,1), the system can never actually
achieve these values. Therefore, we typically use the values of 0.1 and 0.9 as the targets, even
though we will talk as if values of (0,1) are sought.
The learndng rate. Our learning procedure requires only that the change in weight be pro-
portional to E,/aw. True gradient descent requires that infinitesimal steps be taken. The
constant of proportionality is the learning rate in our procedure. The larger this constant, the
larger the changes in the weights. For practical purposes we choose a learning rate that is as
large as possible without leading to oscillation. This offers the most rapid learning. One way
to increase the learning rate without leading to oscillation is to modify the generalized delta
rule to include a momentum term. This can be accomplished by the following rule:
Aw1,(n+1) = qb(pjo,,) + aAwjj(n) (16)
where the subscript n indexes the presentation number, q) is the learning rate, and a is a con-
stant which determines the effect of past weight changes on the current direction of movement
in weight space. This provides a kind of momentum in weight space that effectively filters out
high-frequency variations of the error-surface in the weight space. This is useful in spaces con-
taining long ravines that are characterized by sharp curvature across the ravine and a gently
sloping floor. The sharp curvature tends to cause divergent oscillations across the ravine. To
prevent these it is necessary to take very small steps, but this causes very slow progress along
the ravine. The momentum filters out the high curvature and thus allows the effective weight
Note that the values of the bias. 0,, ca be learned just like any other weights. We simply imagine that 9, is the
weight from a unit that is always on
10 RUMELHART. IUTON. and WILLIAMS
steps to be bigger. In most of our simulations a was about 0.9. Our experience has been that
we get the same solutions by setting a = 0 and reducing the size of -q, but the system learns
much faster overall with larger values of a and -q.
Symmetry breaking. Our learning procedure has one more problem that can be readily
overcome and this is the problem of symmetry breaking. If all weights start out with equal
values and if the solution requires that unequal weights be developed, the system can never
learn. This is because error is propagated back through the weights in proportion to the values
of the weights. This means that all hidden units connected directly to the output inputs will
get identical error signals, and, since the weight changes depend on the error signals, the
weights from those units to the output units must always be the same. The system is starting
out at a kind of local maximum, which keeps the weights equal, but it is a maximum of the
error function, so once it escapes it will never return. We counteract this problem by starting
the system with small random weights. Under these conditions symmetry problems of this kind
do not arise.
The XOR Problem
It is useful to begin with the exclusive-or problem since it is the classic problem requiring
hidden units and since many other difficult problems involve an XOR as a subproblem. We
* have run the XOR problem many times and with a couple of exceptions discussed below, the
system has always solved the problem. Figure 3 shows one of the solutions to the problem.
6.3 Output Unit
-4.2 / V42
/ -9.41
/ A Hidden Unit
/\
. .2
-6.4 -6.4
Input Units
FIGURE 3. Observed XOR network. The connection weights are written on the arrows and the biases are written
in the circles. Note a postive bias means that the unit is on unless turned off.
i% V
LEARN & iB AL 1EIReSIATiONS 1t
Iis solution was reached after 558 sweeps through the four stimulus patterns with a learning
rate of Yj = 0.5. In this case, both the hidden unit and the output unit have positive biases so
they are on unless turned off. The hidden unit turns on if neither input unit is on. When it is
on, it turns off the output unit. The connections from input to output units arranged them-
selves so that they turn off the output unit whenever both inputs are on. In this case, the net-
work has settled to a solution which is a sort of mirror image of the one illustrated in Figure 2.
We have taught the system to solve the XOR problem hundreds of times. Sometimes we
have used a single hidden unit and direct connections to the output unit as illustrated here,
and other times we have allowed two hidden units and set the connections from the input
units to the outputs to be zero, as shown in Figure 4. In only two cases has the system
encountered a local minimum and thus been unable to solve the problem. Both cases involved
the two hidden units version of the problem and both ended up in the same local minimum.
Figure 5 shows the weights for the local minimum. In this case, the system correctly responds
to two of the patterns-namely, the patterns 00 and 10. In the cases of the other two patterns
11 and 01, the output unit gets a net input of zero. This leads to an output value of 0.5 for
both of these patterns. This state was reached after 6,587 presentations of each pattern with
q=0.25.2 Although many problems require more presentations for learning to occur, further tri-
als on this problem merely increase the magnitude of the weights but do not lead to any
improvement in performance. We do not know the frequency of such local minima, but our
experience with this and other problems is that they are quite rare. We have found only one
other situation in which a local minimum has occurred in many hundreds of problems of vari-
ous sorts. We will discuss this case below.
The XOR problem has proved a useful test case for a number of other studies. Using the
architecture illustrated in Figure 4, a student in our laboratory, Yves Chauvin, has studied the
effect of varying the number of hidden units and varying the learning rate on time to solve the
problem. Using as a learning criterion an error of 0.01 per pattern, Yves found that the average
FIGURE 4. A simple architecture for solving XOR with two hidden units and no direct connections from input to
output.
2 if we set vi 0.5 or above, the system escapes this minimum. In general, however, the best way to avoid local
minima is probably to use very small values of I1.
&XI.
12 RUMELHART. HMNTON, and WILLIAMS
4
---. 8
-4.5 5.3
/
2.0 -.1
-2.0 8
1 4.3 9.2
FIGURE 5. A network at a local minimum for the exclusive-or problem. The dated lines indicate negative weights.
Note that whenever the right-most input unit is on it turns on both bidden units. The weights connecting the bidden
units to the output are arranged so that when both hidden units are on. the output unit gets a net input of zero.
This leads to an output value of 0.5. In the other cases the network provides the correct answer.
number of presentations to solve the problem with -q = 0.25 varied from about 245 for the case
with two hidden units to about 12D presentations for 32 hidden units. The results can be sum-
marized by P = 280 - 331og 2H, where P is the required number of presentations and H is the
number of hidden units employed. Thus, the time to solve XOR is reduced linearly with the
logarithm of the number of hidden units. This result holds for values of H up to about 40 in
the case of XOR. The general result that the time to solution is reduced by increasing the
number of bidden units has been observed in virtually all of our simulations. Yves also studied
the time to solution as a function of learning rate for the case of eight hidden units. He found
an average of about 450 presentations with il = 0.1 to about 68 presentations with 71 = 0.75.
He also found that learning rates larger than this led to unstable behavior. However, within
this range larger learning rates speeded the learning substantially. In most of our problems we
have employed learning rates of -q = 0.25 or smaller and have had no difficulty.
Parity
One of the problems given a good deal of discussion by Minsky and Papert (1969) is the par-
ity problem, in which the output required is 1 if the input pattern contains an odd number of
Is and 0 otherwise. This is a very difficult problem because the most similar patterns (those
which differ by a single bit) require different answers. The XOR problem is a parity problem
with input patterns of size two. We have tried a number of parity problems with patterns
ranging from size two to eight. Generally we have employed layered networks in which direct
*. connections from the input to the output units are not allowed, but must be mediated
through a set of hidden units. In this architecture, it requires at least N hidden units to solve
parity with patterns of length N. Figure 6 illustrates the basic paradigm for the solutions
%P
LEARNIG MIRENAL REPRESENT AMNOS 13
-.5 . .5 -. 5 .-
FIGURE 6. A paradigm for the solutions to the parity problem discovered by the tearming system. See tent for cz-
planation.
discovered by the system. The solid lines in the figure indicate weights of +1 and the dotted
lines indicate weights of -1. Ile numbers in the circles represent the biases of the units. Basi-~cally, the hidden units arranged themselves so that they count the number of inputs. In the
diagram, the one at the far left comes on if one or more input units are on, the next comes on
if two or more are on, etc. All of the hidden units come on if all of the input lines arc on.
Ile first m hidden units come on whenever nt bits are on in the input pattern. Ile hidden
units then connect with alternately positive an~d negative weights. In this way the net input
from the hidden units is zero for even numbers and +1 for odd numbers. Table 3 shows the
actual solution attained for one of our simulations with four input lines and four hidden
units. This solution was reached after 2,825 preentations of each of the sixteen patterns with
q= 0.5. Note that the solution is roughly a mirror image of that shown in Figure 6 in that
the number of hidden units turned on is equal to the number of zero input values rather than
the number of ones. Beyond that the principle is that shown above. It should be noted that
the internal representation created by the learning rule is to arrange that the number of hidden
units that come on is equal to the number of zeros in the input and that the particular hidden~units that come on depend only on the number, not on which input units are on. This is
exactly the sort of recoding required by parity. It is not the kind of representation readily
discovered by unsupervised tearming schemes such as competitive learning.
TABLE 3
Input Units Patterns Value
0 0
1 1011 1
2 -. 1010 0
3 OD010 1
4 .. 0000 -. 0
14 RUMELHART. HINTON, sad WILLIAMS
log 2 Hidden Units
) • a •N Input Units
FIGURE 7. A network for solving the encoder problem. In this problem there axe N orthogonal input patterns each
paired with one of N orthogonal output patterns. There are only IogN"2 hidden units. Thus. if the hidden units take
on binary values, the hidden units must form a binary number to encode each of the input patterns. This is exactly
what the system learns to do.
The Encoding Problem
Ackley, Hinton, and Sejnowski (1985) have posed a problem in which a set of orthogonal
input patterns are mapped to a set of orthogonal output patterns through a small set of hidden
units. In such cases the internal representations of the patterns on the hidden units must be
rather efficient. Suppose that we attempt to map N input patterns onto N output patterns.
Suppose further that log2N hidden units are provided. In this case, we expect that the system
will learn to use the hidden units to form a binary code with a distinct binary pattern for each
of the N input patterns. Figure 7 illustrates the basic architecture for the encoder problem.
Essentially, the problem is to learn an encoding of an N bit pattern into a log9N bit pattern
and then learn to decode this representation into the output pattern. We have presented the
system with a number of these problems. Here we present a problem with eight input pat-
terns, eight output patterns, and three hidden units. In this case the required mapping is the
identity mapping illustrated in Table 4. The problem is simply to turn on the same bit in the
output as in the input. Table 5 shows the mapping generated by our learning system on this
example. It is of some interest that the system employed its ability to use intermediate values
in solving this problem. It could, of course, have found a solution in which the hidden units
TABLE 4
Input Patterns Output Patterns
Q-"m
01O000O0 - 020000
00100000 O0100000
o00000 - 00010000
00001000 - 0000100
0000D100 - O00010
O0O010 - 00000010
000001 - 00000001
LEARNWD4 OJ'RNA. PACMESE1? AfONS ,
TABLE S
Input Hidden Unit Output
Patterns Patterns Patterns
1O0000"0 - .5 0 0 10000000
01000W0 - 0 1 0 - 01000000
0010000 - I 1 0 - 001000
00010000 - I I 1 00010000
0ooloo0 0 1 1 - ooo10oo
00000100 - .5 0 1 - 00000100
O00000 - 1 0 .S -. 00000010
00000001 - 0 0 .5 00000001
took on only the values of zero and one. Often it does just that, but in this instance, and
many others, there are solutions that use the intermediate values, and the learning system finds
them even though it has a bias toward extreme values. It is possible to set up problems that
require the system to make use of intermediate values in order to solve a problem. We now
turn to such a case.
Table 6 shows a very simple problem in which we have to convert from a distributed represen-
tation over two units into a local representation over four units. The similarity structure of the
distributed input patterns is simply not preserved in the local output representation.
We presented this problem to our learning system with a number of constraints which made
it especially difficult. The two input units were only allowed to connect to a single hidden
unit which, in turn, was allowed to connect to four more hidden units. Only these four hidden
units were allowed to connect to the four output units. To solve this problem, then, the sys-
tem must first convert the distributed representation of the input patterns into various inter-
mediate values of the singleton hidden unit in which different activation values correspond to
the different input patterns. These continuous values must then be converted back through
the next layer of hidden units-first to another distributed representation and then, finally, to
a local representation. This problem was presented to the system and it reached a solution
after 5,226 presentations with -n = 0.05. 3 Table 7 shows the sequence of representations the
TABLE 6
Input Patterns Output Patterns
00 1000
01 - 0100
10 OD010
11 0001
TABLE 7
Input Singietoo Remaining Output
Patterns Hidden Unit Hidden Units Patterns
10 0 1 1 1 0 - 0010
11 - .2 - 1 00 0001
00 .6 - .5 0 0 .3- 1000
01 1 0 0 0 1-. 0100
3 Relatively small learning rates make units employing intermediate values easier to obtain.
16 RUMELLART, HX[DMN. and WILLIAMS
system actually developed in order to transform the patterns and solve the problem. Note each
of the four input patterns was mapped onto a particular activation value of the singleton hid-
den unit. These values were then mapped onto distributed patterns at the next layer of hidden
units which were finally mapped into the required local representation at the output level. In
principle, this trick of mapping patterns into activation values and then converting those
activation values back into patterns could be done for any number of patterns, but it becomes
increasingly difficult for the system to make the necessary distinctions as ever smaller
differences among activation values must be distinguished. Figure 8 shows the network the sys-
tem developed to do this job. The connection weights from the hidden units to the output
units have been suppressed for clarity. (The sign of the connection, however, is indicated by
the form of the connection-e.g., dashed lines mean inhibitory connections). The four
different activation values were generated by having relatively large weights of opposite sign.
One input line turns the hidden unit full on, one turns it full off. The two differ by a rela-
tively small amount so that when both turn on, the unit attains a value intermediate between 0
and 0.5. When neither turns on, the near zero bias causes the unit to attain a value slightly
over 0.5. The connections to the second layer of hidden units is likewise interesting. When
the hidden unit is full on, the right-most of these hidden units is turned on and all others
turned off. When the hidden unit is turned off, the other three of these hidden units are on
and the left-most unit off. The other connections from the singleton hidden unit to the other
hidden units are graded so that a distinct pattern is turned on for its other two values. Here
we have an example of the flexibility of the learning system.
Our experience is that there is a propensity for the hidden units to take on extreme values,
but, whenever the learning problem calls for it, they can learn to take on graded values. It is
likely that the propensity to take on extreme values follows from the fact that the logistic is a
sigmoid so that increasing magnitudes of its inputs push it toward zero or one. This means
that in a problem in which intermediate values are required, the incoming weights must remain
of moderate size. It is interesting that the derivation of the generalized delta rule does not
depend on all of the units having identical activation functions. Thus, it would be possible for
some units, those required to encode information in a graded fashion, to be linear while others
might be logistic. The linear unit would have a much wider dynamic range and could encode
more different values. This would be a useful role for a linear unit in a network with hidden
units.
Output
+3 4Units
Ile , \
+2 +2 + . HiddenUnits
~ ~ 6 +9
-/ +4
Ax Input
Units
FIGURE 3. 11c network illustrating the use of intermediate values in solving a problem. See text for explanation.
LEARNDA1G D4'IIRNAL EPbIBSENTATXONS 17
Symmetry
Another interesting problem we studied is that of classifying input strings as to whether or
not they are symmetric about their center. We used patterns of various lengths with various
numbers of hidden units. To our surprise, we discovered that the problem can always be
solved with only two hidden units. To understand the derived representation, consider one of
the solutions generated by our system for strings of length six. This solution was arrived at
after 1,208 presentations of each six-bit pattern with -q = 0.1. The final network is shown in
Figure 9. For simplicity we have shown the six input units in the center of the diagram with
one hidden unit above and one below. The output unit, which signals whether or not the
string is symmetric about its center, is shown at the far right. The key point to see about this
solution is that for a given hidden unit, weights that are symmetric about the middle are equal
in magnitude and opposite in sign. That means that if a symmetric pattern is on, both hidden
units will receive a net input of zero from the input units, and, since the hidden units have a
negative bias, both will be off. In this case, the output unit, having a positive bias, will be on.
The next most important thing to note about the solution is that the weights on each side of
the midpoint of the string are in the ratio of 1:2.4. This insures that each of the eight patterns
that can occur on each side of the midpoint sends a unique activation sum to the hidden unit.
This assures that there is no pattern on the left that will exactly balance a non-mirror-image
pattern on the right. Finally, the two hidden units have identical patterns of weights from the
input units except for sign. This insures that for every nonsymmetric pattern, at least one of
the two hidden units will come on and turn on the output unit. To summarize, the network is
arranged so that both hidden units will receive exactly zero activation from the input units
when the pattern is symmetric, and at least one of them will receive positive input for every
nonsymmetric pattern.
' Hidden Unit
-3.18 + 6a2 -1236/ +12.56 \.623 +3.17 N-
Output
6 3\ l, I / /
10 -9.4
. Hidden Unit
FIGURE 9. Network for solving the symmetry problem. The six open circles represent the input units. There are
two hidden units, one shown above and one below the input units. The output unit is shown to the far left. See
text for explanation.
18 RUMELHART. HiDroN, and WMLLA.MS
This problem was interesting to us because the learning system developed a much more
elegant solution to the problem than we had previously considered. This problem was not the
only one in which this happened. The parity solution discovered by the learning procedure
was also one that we had not "'scovered prior to testing the problem with our learning pro-
cedure. Indeed, we frequently discover these more elegant solutions by giving the system more
hidden units than it needs and observing that it does not make use of some of those provided.
Some analysis of the actual solutions discovered often leads us to the discovery of a better
solution involving fewer hidden units.
Addition
Another interesting problem on which we have tested our learning algorithm is the simple
binary addition problem. This problem is intcresting because there is a very elegant solution to
it, because it is the one problem we have found where we can reliably find local minima and
because the way of avoiding these local minima gives us some insight into the conditions under
which local minima may be found and avoided. Figure 10 illustrates the basic problem and a
minimal solution to it. There are four input units, three output units, and two hidden units.
The output patterns can be viewed as the binary representation of the sum of two two-bit
binary numbers represented by the input patterns. The second and fourth input units in the
* Odiagram correspond to the low-order bits of the two binary numbers and the first and third
units correspond to the two higher order bits. The hidden units correspond to the carry bits
Output Units
. "\\-2 Hidden
Units
Input Units
FIGURE 10. Minimal network for adding two two-bit binary numbers. There are four input units, three output
units, and two hidden units. The output patterns can be viewed as the binary representation of the sum of two two-
bit binary numbers represented by the input patterns. The second and fourth input units in the diagram correspond
to the low-order bits of the two binary numbers, and the firm and third units correspond to the two higher order
bits. The hidden units correspond to the carry bits in the summation. The hidden unit on the far right comes on
when both of the lower order bits in the input pattern are turned on. and the one on the left comes on when both
higher order bits are turned on or when one of the higher order bits and the other hidden unit is turned on. The
weights on all lines are assumed to be +1 except where noted. Negative connections are indicated by dashed lhoes.
As usual, the biases are indicated by the numbers in the circles.
wegt*nalfie r s1e t e+ et hr oe.Ngaie€netou r niae b ahdfns
LEARNl' W'IEMRNAk kVLPk .sVNtA 'lNOS 19
in the summation. Thus the hidden unit on the far right comes on when both of the lower
order bits in the input pattern are turned on, and the one on the left comes on when both
higher order bits are turned on or when one of the higher order bits and the other hidden unit
is turned on. In the diagram, the weights on all lines are assumed to be +1 except where
noted. Inhibitory connections are indicated by dashed lines. As usual, the biases are indicated
by the numbers in the circles. To understand how this network works, it is useful to note that
the lowest order output bit is determined by an exclusive-or among the two low-order input
bits. One way to solve this XOR problem is to have a hidden unit come on when both low-
order input bits are on and then have it inhibit the output unit. Otherwise either of the low-
order input units can turn on the low-order output bit. The middle bit is somewhat more
difficult. Note that the middle bit should come on whenever an odd number of the set con-
taining the two higher order input bits and the lower order carry bit is turned on. Observation
will confirm that the network shown performs that task. The left-most hidden unit receives
inputs from the two higher order bits and from the carry bit. Its bias is such that it will come
on whenever two or more of its inputs are turned on. The middle output unit receives positive
inputs from the same three units and a negative input of -2 from the second hidden unit.
This insures that whenever just one of the three are turned on, the second hidden unit will
remain off and the output bit will come on. Whenever exactly two of the three are on, the
hidden unit will turn on and counteract the two units exciting the output bit, so it will stay
off. Finally, when all three are turned on, the output bit will receive -2 from its carry bit and
+3 from its other three inputs. The tiet is positive, so the middle unit will be on. Finally, the
* -third output bit should turn on whenever the second hidden unit is on-that is, whenever
there is a carry from the second bit. Here then we have a minimal network to carry out the
job at hand. Moreover, it should be noted that the concept behind this network is generaliz-
able to an arbitrary number of input and output bits. In general, for adding two m bit binary
numbers we will require 2m input units, m hidden units, and m +1 output units.
Unfortunately, this is the one problem we have found that reliably leads the system into
local minima. At the start in our learning trials on this problem we allow any input unit to
connect to any output unit and to any hidden unit. We allow any hidden unit to connect to
any output unit, and we allow one of the hidden units to connect to the other hidden unit,
but, since we cart have no loops, the connection in the opposite direction is disallowed. Some-
times the system will discover essentially the same network shown in the figure. Often, how-
ever, the system ends up in a local minimum. The problem arises when the XOR problem on
the low-order bits is not solved in the way shown in the diagram. One way it can fail is when
the "higher" of the two hidden units is "selected" to solve the XOR problem. This is a problem
because then the other hidden unit cannot "see" the carry bit and therefore cannot finally solve
the problem. This problem seems to stem from the fact that the learning of the second output
bit is always dependent on learning the first (because information about the carry is necessary
to learn the second bit) and therefore lags behind the learning of the first bit and has no
influence on the selection of a hidden unit to solve the first XOR problem. Thus, about half
of the time (in this problem) the wrong unit is chosen and the problem cannot be solved. In
this case, the system finds a solution for all of the sums except the 11+11 - 110 (3+3 = 6) case
in which it misses the carry into the middle bit and gets 11+11 - 100 instead. This problem
differs from others we have solved in as much as the hidden units arc not "equipotential" here.
In most of our other problems the hidden units have been equipotential, and this problem has
not arisen.
It should be noted, however, that there is a relatively simple way out of the problem-
namely, add some extra hidden units. In this case we can afford to make a mistake on one or
more selections and the system can still solve the problems. For the problem of adding two-bit
4 The network is the same except for the highet order bit. The highest order bit is always on whenever three or
more of the input units are on. This is always learned first and always learned with direct connections to the input
units.
- -t/
,4.-
20 RUMELHART, HDITON, and WILLIAMs
numbers we have found that the system always solves the problem with one extra hidden unit.
With larger numbers it may require two or three more. For purposes of illustration, we show
the results of one of our runs with three rather than the minimum two hidden units. Figure 11
shows the state reached by the network after 3,020 presentations of each input pattern and
with a learning rate of 71 = 0.5. For convenience, we show the network in four parts. In Fig-
ure 11A we show the connections to and among the hidden units. This figure shows the inter-
nal representation generated for this problem. The "lowest' hidden unit turns off whenever
-, either of the low-order bits are on. In other words it detects the case in which no low-order
bit is turn on. The 'highest' hidden unit is arranged so that it comes on whenever the sum is
less than two. The conditions under which the middle hidden unit comes on are more com-
plex. Table 8 shows the patterns of hidden units which occur to each of the sixteen input pat-
terns. Figure 1IB shows the connections to the lowest order output unit. Noting that the
relevant hidden unit comes on when neither low-order input unit is on, it is clear how the sys-
tem computes XOR. When both low-order inputs are off, the output unit is turned off by the
A B
Output Units Output Units
00000o.o,., , 009
"EHidden 
Hidden' 0 - I--.Units Hidden'
x ,( . , ,0 U n i t s
y 4..x \5 //
Input Units 
Input Units
Output Units Output Units.7
Hidden + \ 4 Hidden
,14 Units 0nUnits
Unit
+S+ 6-2 2 +2\
ddbb
%Input Units Input Units
, "FIGURE 11. Network found for the mmmation problem. A: The connections from the input units to the three
hidde units and the connections among the hidden units. f: The connections from the input and hidden units to
the lowest order output unit. C: The connections from the input and hidden units to the middle output unit. D:
The connections from the input and hidden units to the highest order output unit.
4
- -
' "%,3' ,''. , "I".","., ,.'. L g ' -." , ""''%.," ,-'%''"' . -.-''''''''''''', '_',.">-,"'"''''''.-
LEARNIWP (MkNhA~L Rt , !bBSI ^I ONS L
TABLE 8
Input Ifidden Unit Output
Patterns Patterns Patterns
00 + O0 . II - 000
00+01 - 110 - 001
00+ 10 011 010
00+ 11 010 - 011
01 +00 + 110 - 001
01 +01 010 - 010
01 +10 - 010 - 011
01 + 11 000 - 100
" "1 0 + ODIi 010 01
10+01 011 01Oi
10+ 10 - 001 - 100
10+ 11 - 000 - 101
11 + O0 - 010 - 011
11 +01 000 - 100
'- 11 +10 000 101
11 + 1I 000 - 110
hidden unit. When both low-order input units are on, the output is turned off directly by the
* @two input units. If just one is on, the positive bias on the output unit keeps it on. Figure
lIC gives the connections to the middle output unit, and in Figure IID we show those connec-
tions to the left-most, highest order output unit. It is somewhat difficult to see how these
connections always lead to the correct output answer, but, as can be verified from the figures,
the network is balanced so that this works.
It should be pointed out that most of the problems described thus far have involved hidden
units with quite simple interpretations. It is much more often the case, especially when the
number of hiddcn units exceeds the minimum number required for the task, that the hidden
units are not readily interpreted. This follows from the fact that there is very little tendency
for localist representations to develop. Typically the internal representations are distributed
and it is the pattern of activity over the hidden units, not the meaning of any particular hidden
unit tha is important.
The Negation Problem
Consider a situation in -which the input to a system consists of patterns of n +1 binary values
and an output of n values. Suppose further that the general rule is that n of the input units
should be mapped directly to the output patterns. One of the input bits, however, is special.
It is a negation bit. When that bit is off, the rest of the pattern is supposed to map straight
through, but when it is on, the complement of the pattern is to be mapped to the output.
Table 9 shows the appropriate mapping. In this case the left element of the input pattern is
the negation bit, but the system has no way of knowing this and must learn which bit is the
negation bit. In this case, weights were allowed from any input unit to any hidden or output
unit and from any hidden unit to any output unit. The system learned to set all of the weights
to zero except those shown in Figure 12. The basic structure of the problem and of the solu-
... tion is evident in the figure. Clearly the problem was reduced to a set of three XORs between
the negation bit and each input. In the case of the two right-most input units, the XOR prob-
lems were solved by recruiting a hidden unit to detect the case in which neither the negation
unit nor the corresponding input unit was on. In the third case, the hidden unit detects the
case in which both the negation unit and relevant input were on. In this case the problem was
solved in less than 5.000 passes through the stimulus set with = 0.25.
,1% .,.. .. , ,-e ", , <, ,. .2 c.r . " , ".,.
.
, , ,,,., . , ", , + ,, -. . . . .. . ., . ,. x .r .,,, .. + '- . . - '' ' '',
22 RUMELHART. KinNo, and WULIAMS
TABLE 9
Input Patterns Output Patterns
000 - 001
0010 - 010
0011 - Oil
0100 - 100
0101 - 101
0110 110
0111 - 111
1000 - 111
1001 -. 110
1010 - 101
1011 - 100
1100 i 011
1101 - 010
1110 001
1111 000
0 The T-C Problem
Most of the problems discussed so far (except the symmetry problem) are rather abstract
mathematical problems. We now turn to a more geometric problem-that of discriminating
between a T and a C-independent of translation and rotation. Figure 13 shows the stimulus
patterns used in these experiments. Note, these patterns are each made of five squares and
differ from one another by a single square. Moreover, as Minsky and Papert (1969) point out,
when considering the set of patterns over all possible translations and rotations (of 90", 180",
and 270*), the patterns do not differ in the set of distances among their pairs of squares. To
see a difference between the sets of patterns one must look, at least, at configurations of tri-
.1 .4 +
+4 
.
./ I t'- .41
+4 I
+
/ - 6.I. Io
FIGURE 12. The solution discovered for the negation problan. le rigbt-most unit is the negation unit. The prob-
lem has been reduced and solved as three exclusive-on between the negation unit and each of the other three units.
..t''N?
LEARNDCG NTERNAL REPRESENTATIONS 23
('I
FIGURE 13. The stimulus set for the T-C problem. The set consists of a block T and a block C in each of four
orientations. One of the eight patterns is presented on each trial.
plets of squares. Thus Minsky and Pattern call this a problem of order three. 5 In order to
facilitate the learning, a rather different architecture was employed for this problem. Figure 14
shows the basic structure of the network we employed. Input patterns were now conceptual-
ized as two-dimensional patterns superimposed on a rectangular grid. Rather than allowing
each input unit to connect to each hidden unit, the hidden units themselves were organized
into a two-dimensional grid with each unit receiving input from a square 3x3 region of the
input space. In this sense, the overlapping square regions constitute the predefined receptive
field of the hidden units. Each of the hidden units, over the entire field, feeds into a single
output unit which is to take on the value 1 if the input is a T (at any location or orientation)
and 0 if the input is a C. Further, in order that the learning that occurred be independent of
where on the field the pattern appeared, we constrained all of the units to earn exactly the
same pattern of weights. In this way each unit was constrained to compute exactly the same
function over its receptive field-the receptive fields were constrained to all have the same
shape. This guarantees translation independence and avoids any possible 'edge effects' in the
learning. The learning can readily be extended to arbitrarily large fields of input units. This
constraint was accomplished by simply adding together the weight changes dictated by the delta
rule for each unit and then changing all weights exactly the same amount. In this way, the
whole field of hiddcn units consists simply of replications of a single feature detector centered
on different regions of the input space, and the learning that occurs in one part of the field is
automatically generalized to the rcst of the field.
6
We have run this problem in this way a number of times. As a result, we have found a
)number of solutions. Perhaps the simplest way to understand the system is by looking at the
form of the receptive field for the hidden units. Figure 15 shows several of the receptive fields
we have seen. 7 Figure 15A shows the most local representation developed. This on-center-off-
'Terry Sejoowski pointed out to us that the T-C problem was difficult for models of this sort to learn and there-
fore worthy of study.
S6 A similar procedure hu been employed by Fukushima (1980) in his naoogitrom and by Kienker. Sejnowski. lio-
ton. and Schumacher (1965).
7 The ratios of the weights are about right. The actual values can be larger or smaller than the values given in the
figure.
.~~ ~ ~ %- -. N-4*~.. ~** --
24 RLUMELHART. INUTON, add WILLIAMS
Output
Unit
o 0Hidden
03 0 0 0 Units
O DO
O 0 0 1
I aInput
Units
FIGURE 14. The network for solving Che T-C problem. Sac tet for explanation.
surround detector turns out to be an excellent T detector. Since, as illustrated, a T can extend
into the on-center and achieve a net input of +1, this detector will be turned on for a T at any
orientation. On the other hand, any C extending into the center must cover at least two inhibi-
tory cells. With this detector the bias can be set so that only one of the whole field of inhibi-
tory units will come on whenever a T is presented and none of the hidden units will be turned
on by any C. This is a kind of protrusion detector which differentiates between a T and C by
detecting the protrusion of the T.
The receptive field shown in Figure 15B is again a kind of T detector. Every T activates one
of the hidden units by an amount +2 and none of the hidden units receives more than +1 from
any of the C's. As shown in the figure, T's at 90* and 27(r send a total of +2 to the hidden
units on which the crossbar lines up. The T's at the other two orientations receive +2 from
the way it detects the vertical protrusions of those two characters. Figure 15C shows a more
distributed representation. As illustrated in the figure, each T activates five different hidden
units whereas each C excites only three hidden units. In this case the system again is
differentiating between the characters on the basis of the protruding end of the T which is not
shared by the C.
Finally, the receptive field shown in Figure 15D is even more interesting. In this case every
hidden unit has a positive bias so that it is on unless turned off. The strength of the inhibi-
tory weights are such that if a character overlaps the receptive field of a hidden unit, that unit
turns off. The system works because a C is more compact than a T and therefore the T turns
off more units that the C. The T turns off 21 hidden units, and the C turns off only 20. This
is a truly distributed representation. In each case, the solution was reached in from about
LEARNMIG TMri.NAL REPRESENrArIONS 25
A B
........ ............. ......
....... .......
1 2-1 -1 1-1
1 -1 1
C ....... .......
1 -+2+-1
D -2 -2 -2
~-2-2 -2
-2 -2 -2
FIGURE 15. Receptive fields found in different runs of the T-C problem. A: An on-center-off-surround receptive
field for detecting T's. 8: A vertical bar detector which responds to T's more strongly than C's. C: A diagonal bar~detector. A T activates five such detectors whereas a C activates only thr e such detectors. 1):  compactness
detector. This inhibitory receptive field turns off whenever an input coven any region of its receptive field. Since
the C is more compact than the T it turns off 20 such detectors whereas the T turns off 21 of them.
5,000 to 10,000 presentations of tfie set of eight patterns.'
It is interesting that the inhibitory type of receptive field shown in Figure 15D was the most
common and that there is a predominance of inhibitory connections in this and indeed all of
our simulations. This can be understood by considering the trajectory through which the learn-
ing typically moves. At first, when the system is presented with a difficult problem, the initial
random connections are as likely to mislead as to give the correct answer. In this case, it is
best for the output units to take on a value of 0.5 than to take on a more extreme value. This
follows from the form of the error function given in Equation 2. The output unit can achieve
a constant output of 0.5 by turning off those units feeding into it. Thus, the first thing that
happens in virtually every difficult problem is that the hidden units are turned off. One way
I Since translation independence was built into the learning procedure, it makes no difference where the input oc-
curs; the same thing will be learned whcever the pattern is presented. Thus, there are only eight distinct patterns to
be presented to the system.
-!- . . - - -- - - '' . A7
26 RUMEUIART, IURTON, and WILLIAMS
to achieve this is to have the input units inhibit the hidden units. As the system begins to sort
things out and to learn the appropriate function some of the connections will typically go posi-
tive, but the majority of the connections will remain negative. This bias for solutions involving
inhibitory inputs can often lead to nonintuitive results in which hidden units are often on
unless turned off by the input.
More Simulation Results
We have offered a sample of our results in this section. In addition to having studied our
learning system on the problems discussed here, we have employed back propagation for learn-
ing to multiply binary digits, to play tic-tac-toe, to distinguish between vertical and horizontal
lines, to perform sequences of actions, to recognize characters, to associate random vectors,
and a host of other applications. In all of these applications we have found that the general-
izcd delta rule was capable of generating the kinds of internal representations required for the
problems in question. We have found local minima to be very rare and that the system learns
in a reasonable period of time. Still more studies of this type will be required to understand
precisely the conditions under which the system will be plagued by local minima. Suffice it to
say that the problem has not been serious to date. We now turn to a pointer to some future
0 developments.
SOME FURTHER GENERALIZATIONS
We have intensively studied the learning characteristics of the generalized delta rule on feed-
forward networks and semilinear activations functions. Interestingly these are not the most
general cases to which the learning procedure is applicable. As yet we have only studied a few
examples of the more fully generalized system, but it is relatively easy to apply the same learn-
ing rule to sigma-pi units and to recurrent networks. We will simply sketch the basic ideas
here.
The Generalized Delta Rule and Sigma-Pi Units
It will be rccallcd from Chapter 2 that in the case of sigma-pi units we have
0i ~f J(wit Ioi) (17)
where i varies over the set of conjuncts feeding into unit j and k varies over the elements of
the conjuncts. For simplicity of exposition, we restrict ourselves to the case in which no con-
juncts involve more than two elements. In this case we can notate the weight from the con-
junction of units i and j to unit k by wiJk. The weight on the direct connection from unit i
to unit j would, thus, be wj1 , and since the relation is multiplicative, wk, = wkji. We can now
S.,
LEARNCM WERMAL KEPESIMATIONS 27
rewrite Equation 17 as
We now set
Taking the derivative and simplifying, we get a rule for sigma-pi units strictly analogous to the
rule for semilinear activation functions:
AP wii = , 01 o.
We can see the correct form of the error signal, 8, for this case by inspecting Figure 16. Con-
sider the appropriate value of 81 for unit sh in the figure. As before, the correct value of 81 is
given by the sum of the B's for all of the units into which u, feeds, weighted by the amount of
effect due to the activation of u, times the derivative of the activation function. In the case of
semilinear functions, the measure of a unit's effect on another unit is given simply by the
weight w connecting the first unit to the second. In this case, the %'s effect on uk depends
not only on wtij, but also on the value of uj. Thus, we have
8, f ',(net, )Y, 8 ,,0
Uk U
'". wk .. -W'h
U k
Wi Wj5.g
II
UU IUh
FIGURE 16. The generalized delta rule for sigma-pi units. The products of activation values of individual units c-
tivate output units. See text for explanation of how the & values am computed in this can.
28 RUMELHART, Hm1TON, and WILLIAMS
if a, is not an output unit and, as before,
8, = f I (ne,)(1 -o,)
if it is an output unit.
Recurrent Nets
We have thus far restricted ourselves to feedforward nets. This may seem like a substantial
restriction, but as Minsky and Papert point out, there is, for every recurrent network, a feed-
forward network with identical behavior (over a finite period of time). We will now indicate
how this construction can proceed and thereby show the correct form of the learning rule for
the recurrent network. Consider the simple recurrent network shown in Figure 17A. The
same network in a feedforward architecture is shown in Figure 17B. The behavior of a
B Time
U I'M U 2
W 2 W2
A
Ut U2
I
IN 12 W2 2 .
222
12
FIGURE 17. A comparison of a recurrent network and a feedforward network with identical behavior. A: A com-
pletely connected recurrent network with two units. 8: A feedforward network which behaves the ame as the re-
current network. In this case. we have a separate unit for each timee step and we require that the weights connecting
each layer of units to the net be the same for all layers. Moreover, they must be the same as the analogous weights
ia the recurrent case.
LEARN0JC4G On-'PRiAL ic.Pkkit ATIONS 29
recurrent network can be achieved in a feedforward network at the cost of duplicating the
hardware many times over for the feedforward version of the network. 9 We have distinct units
and distinct weights for each point in time. For naming convenience, we subscript each unit
with its unit number in the corresponding recurrent network and the time it represents. As
long as we constrain the weights at each level of the feedforward network to be the same, we
have a feedforward network which performs identically with the recurrent network of Figure
17A. The appropriate method for maintaining the constraint that all weights be equal is simply
to keep track of the changes dictated for each weight at each level and then change each of the
weights according to the son of these individually prescribed changes. Now, the general rule
for determining the change prescribed for a weight in the system for a particular time is simply
to take the product of an appropriate error measure 5 and the input along the relevant line
both for the appropriate times. Thus, the problem of specifying the correct learning rule for
recurrent networks is simply one of determining the appropriate value of 8 for each time. In a
feedforward network we determine 8 by multiplying the derivative of the activation function
by the sum of the 8's for those units it feeds into weighted by the connection strengths. The
same process works for the recurrent network-except in this case, the value of 8 associated
with a particular unit changes in time as a unit passes error back, sometimes to itself. After
each iteration, as error is being passed back through the network, the change in weight for that
iteration must be added to the weight changes specified by the preceding iterations and the
sum stored. This process of passing error through the network should continue for a number
of iterations equal to the number of iterations through which the activation was originally
passed. At this point, the appropriate changes to all of the weights can be made.
In general, the procedure for a recurrent network is that an input (generally a sequence) is
presented to the system while it runs for some number of iterations. At certain specified times
during the operation of the system, the output of certain units are compared to the target for
* that unit at that time and error signals are generated. Each such error signal is then passed
back through the network for a number of iterations equal to the number of iterations used in
the forward pass. Weight changes are computed at each iteration and a sum of all the weight
changes dictated for a particular weight is saved. Finally, after all such error signals have been
propagated through the system, the weights are changed. The major problem with this pro-
cedure is the memory required. Not only does the system have to hold its summed weight
changes while the error is being propagated, but each unit must somehow record the sequence
of activation values through which it was driven during the original processing. This follows
from the fact that during each iteration while the error is passed back through the system, the
current 8 is relevant to a point earlier in time and the required weight changes depend on the
activation levels of the units at that time. It is not entirely clear how such a mechanism could
be implemented in the brain. Nevertheless, it is tantalizing to realize that such a procedure is
potentially very powerful, since the problem it is attempting to solve amounts to that of
finding a sequential program (like that for a digital computer) that produces specified input-
sequence/output-sequence pairs. Furthermore, the interaction of the teacher with the system
can be quite flexible, so that, for example, should the system get stuck in a local minimum, the
teacher could introduce "hints" in the form of desired output values for intermediate stages of
processing. Our experience with recurrent networks is limited, but we have carried out some
experiments. We turn first to a very simple problem in which the system is induced to invent a
shift register to solve the problem.
.. Learning to be a shift register. Perhaps the simplest class of recurrcnt problems we have
studied is one in which the input and output units are one and the same and there are no hid-
den units. We simply present a pattern and let the system process it for a period of time. The
state of the system is then compared to some target state. If it hasn't reached the target state
9 Note that in this discussion, and indeed in out entire development here, we have masumed a discrete time system
with synchronous update and with each connection involving a unit delay.
Vz x- T ~It~. % 1_1 m'4 F.1KT13*W wn J1 A' 1~~- 1 1. VW7XZ 7 W J A- vl w-j~;r_-lw-1 'JW ? 4C.- '
30 RUMELHART, HITON, and WILLIAMS
at the designated time, error is injected into the system and it modifies its weights. Then it is
shown a new input pattern and restarted. In these cases, there is no constraint on the connec-
tions in the system. Any unit can connect to any other unit. The simplest such problem we
have studied is what we call the shift register problem. In this problem, the units are concep-
tualized as a circular shift register. An arbitrary bit pattern is first established on the units.
They are then allowed to process for two time-steps. The target state, after those two time-
steps, is the original pattern shifted two spaces to the left. The interesting question here con-
cerns the state of the units between the presentation of the start state and the time at which
the target state is presented. One solution to the problem is for the system to become a shift
register and shift the pattern exactly one unit to the left during each time period. If the system
did this then it would surely be shifted two places to the left after two time units. We have
tried this problem with groups of three or five units and, if we constrain the biases on all of
the units to be negative (so the units are off unless turned on), the system always learns to be a
shift register of this sort.1° Thus, even though in principle any unit can connect to any other
unit, the system actually learns to set all weights to zero except the ones connecting a unit to
its left neighbor. Since the target states were determined on the assumption of a circular regis-
tcr, the left-most unit developed a strong connection to the right-most unit. The system learns
this relatively quickly. With 71 = 0.25 it learns perfectly in fewer than 200 sweeps through the
set of possible patterns with either three- or five-unit systems.
The tasks we have described so far are exceptionally simple, but they do illustrate how the
*- algorithm works with unrestricted networks. We have attempted a few more difficult prob-
lems with recurrent networks. One of the more interesting involves learning to complete
sequences of patterns. Our final example comes from this domain.
Learning to complete sequences. Table 10 shows a set of 25 sequences which were chosen so
that the first two items of a sequence uniquely determine the remaining four. We used this set
of sequences to test out the learning abilities of a recurrent network. The network consisted
of five input units (A, B, C, D, E), 30 hidden units, and three output units (1, 2, 3). At Time
1, the input unit corresponding to the first item of the sequence is turned on and the other
input units are turned off. At Time 2, the input unit for the second item in the sequence is
turned on and the others are all turned off. Then all the input units are turned off and kept
off for the remaining four steps of the forward iteration. The network must learn to make the
output units adopt states that represent the rest of the sequence. Unlike simple feedforward
networks (or their iterative equivalents), the errors are not only assessed at the final layer or
time. The output units must adopt the appropriate states during the forward iteration, and so
during the back-propagation phase, errors are injected at each time-step by comparing the
remembered actual states of the output units with their desired states.
TABLE 10
25 SEQUENCES TO BE LEARNED
AA1212 AB1223 AC1231 AD1221 AE1213
BA2312 BB2323 SC2331 BD2321 BE2313
CA3112 CB3123 CC3131 CD3121 CE3113
DA2112 DB2123 DC2131 DD2121 DE2113
EA1312 EB1323 EC3331 ED1321 EE1313
t0 if the constraint that bims be negative is not imposed, othe solutions are possible. These solutions can involve
the units paining through the complements of the shifted pattern or even through more complicated intermediate
sidae. These trajectories are interesting in that they match a simple shift register on all even numbers of shifts, but
do not match following an odd number of shifts.
LEARNI, 0"1'13NA; mpSkLimm-ATiONs 31
The learning procedure for recurrent nets places no constraints on the allowable connectivity
structure' For the sequence completion problem, we used one-way connections from the input
units to the hidden units and from the hidden units to the output units. Every hidden unit
had a one-way connection to every other hidden unit and to itself, and every output unit was
also connected to every other output unit and to itself. All the connections started with small
random weights uniformly distributed between -0.3 and +0.3. All the hidden and output
units started with an activity level of 0.2 at the beginning of each sequence.
We used a version of the learning procedure in which the gradient of the error with respect
to each weight is computed for a whole st of examples before the weights are changed. This
means that each connection must accumulate the sum of the gradients for all the examples and
for all the time steps involved in each example. During training, we used a particular set of 20
examples, and after these were learned almost perfectly we tested the network on the remaining
examples to see if it had picked up on the obvious regularity that relates the first two items of
a sequence to the subsequent four. The results are shown in Table 11. For four out of the five
test sequences, the output units all have the correct values at all times (assuming we treat
values above 0.5 as 1 and values below 0.5 as 0). The network has clearly captured the rule that
the first item of a sequence determines the third and fourth, and the second determines the
fifth and sixth. We repeated the simulation with a different set of random initial weights, and
it got all five test sequences correct.
The learning required 260 sweeps through all 20 training sequences. The errors in the output
units were computed as follows: For a unit that should be on, there was no error if its activity
level was above 0.8, otherwise the derivative of the error was the amount below 0.8. Similarly,
for output units that should be off, the derivative of the error was the amount above 0.2.
After each sweep, each weight was decremented by .02 times the total gradient accumulated on
that sweep plus 0.9 times the previous weight change.
We have shown that the learning procedure can be used to create a network with interesting
sequential behavior, but the particular problem we used can be solved by simply using the hid-
den units to create "delay lines" which hold information for a fixed length of time before allow-
ing it to influence the output. A harder problem that cannot be solved with delay lines of
fixed duration is shown in Table 12. The output is the same as before, but the two input items
can arrive at variable times so that the item arriving at time 2, for example, could be either the
first or the second item and could therefore determine the states of the output units at either
the fifth and sixth or the seventh and eighth times. The new task is equivalent to requiring a
buffer that receives two input "words" at variable times and outputs their "phonemic realiza-
tions" one after the other. This problem was solved successfully by a network similar to the
one above except that it had 60 hidden units and half of their possible interconnections were
omitted at random. The learning was much slower, requiring thousands of sweeps through all
136 training examples. There were also a few more errors on the 14 test examples, but the gen-
eralization was still good with most of the test sequences being completed perfectly.
CONCLUSION
Minsky and Papert (1969) in their pessimistic discussion of pcrceptrons finally, near the end
of their book, discuss multilayer machines. They state:
The pcrceptron has shown itself worthy of study despite (and even because of!) its
severe limitations. It has many features that attract attention: its linearity; its
tt The constraint in feedforward networks is that it must be possible to arrange the units into layers such that units
do not influence units in the same or lower layers. In recurrent nctworks this amounts to the constraint that during
the forward iteration, future states must not affect past ones.
, % % %
32 RUMELHART, HWNTON. and WILLIAMS
TABLE 11
PERFORMANCE OF TILE NETWORK ON FIVE NOVEL TEST SEQUENCES
Input Sequence A D - - -
Desired Outputs - - 1 2 2 1
Actual States of:
Output Unit 1 0.2 0.12 0.90 0.22 0.11 0.83
Output Unit 2 0.2 0.16 0.13 0.82 0.88 0.03
Output Unit 3 0.2 0.07 0.08 0.03 0.01 0.22
Input Sequence B E - - - -
Desired Outputs - - 2 3 1 3
Actual States of:
Output Unit 1 0.2 0.12 0.20 0.25 0.48 0.26
Output Unit 2 0.2 0.16 0.80 0.05 0.04 0.09
Output Unit 3 0.2 0.07 0.02 0.79 0.48 0.53
Input Sequence C A - - - -
Desired Outputs - - 3 1 1 2
Actual States of:
Output Unit 1 0.2 0.12 0.19 0.80 0.87 0.11
Output Unit 2 0.2 0.16 0.19 0.00 0.13 0.70
Output Unit 3 0.2 0.07 0.80 0.13 0.01 0.25
Input Sequence D B - - - -
Desired Outputs - - 2 1 2 3
Actual States of:
Output Unit 1 0.2 0.12 0.16 0.79 0.07 0.11
Output Unit 2 0.2 0.16 0.80 0.15 0.87 0.05
Output Unit 3 0.2 0.07 0.20 0.01 0.13 0.96
Input Sequence E C - - - -
Desired Outputs - - 1 3 3 1
Actual States of:
% a Output Unit 1 0.2 0.12 0.80 0.09 0.27 0.78
Output Unit 2 0.2 0.16 0.20 0.13 0.01 0.02
Output Unit 3 0.2 0.07 0.07 0.94 0.76 0.13
TABLE 12
SIX VARIATIONS OF TIlE SEQUENCE EA1312 PRODUCED BY
PRESENTING THE FIRST TWO ITEMS AT VARIABLE TIMES
EA--1312 E-A-1312 E--A1312
-EA-1312 -E-A1312 -- EA1312
Note: With thes temporal variations, the 25 sequences shown in
Table 10 can be used to generate 150 different sequences.
intriguing learning theorem; its clear paradigmatic simplicity as a kind of parallel com-
putation. There is no reason to suppose that any of these virtues caery over to the
4, , many-layered version. Nevertheless. we consider it to be an important research problem
to elucidate (or reject) our intuitive judgement that the extension is sterile. Perhaps
.N'
LEARNIDG NrEQA. TPkR-SENTATIUN, 33
some powerful convergence theorem will be discovered, or some profound reason for
the failure to produce an interesting *learning theorem for the multilayered machine
will be found. (pp. 231-232)
Although our learning results do not guaantee that we can find a solution for all solvable
problems, our analyses and results have shown that as a practical matter, the error propagation
scheme leads to solutions in virtually every case. In short, we believe that we have answered
Minsky and Papert's challenge and have found a learning result sufficiently powerful to demon-
strate that their pessimism about learning in multilayer machines was misplaced.
One way to view the procedure we have been describing is as a parallel computer that, having
been shown the appropriate input/output exemplars specifying some function, programs itself
to compute that function in general. Parallel computers are notoriously difficult to program.
Here we have a mechanism whereby we do not actually have to know how to write the pro-
gram in order to get the system to do it. Parker (1985) has emphasized this point.
On many occasions we have been surprised to learn of new methods of computing interest-
ing functions by observing the behavior of our learning algorithm. This also raised the ques-
tion of generalization. In most of the cases presented above, we have presented the system
with the entire set of exemplars. It is interesting to ask what would happen if we presented
only a subset of the exemplars at training time and then watched the system generalize to
remaining exemplars. In small problems such as those presented here, the system sometimes
finds solutions to the problems which do not properly generalize. However, preliminary results
on larger problems are very encouraging in this regard. This research is still in progress and
cannot be reported here. This is currently a very active interest of ours.
Finally, we should say that this work is not yet in a finished form. We have only begun our
study of recurrent networks and sigma-pi units. We have not yet applied our learning pro-
cedure to many very complex problems. However, the results to date are encouraging and we
are continuing our work.
REFERENCES
Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for
Boltzmann machines. Cognitive Science, 9, 147-169.
Barto, A. G.. Learning by statistical cooperation of self-interested neuron-like computing elements
(COINS Tech. Rep. 85-11). Amherst: University of Massachusetts, Department of Com-
puter and Information Science.
Barto, A. G., & Anandan, P. (1985). Pattern recognizing stochastic learning automata. IEEE
Transactions on Systems, Man, and Cybernetics.
Fukushima, K. (1980). Ncocognitron: A self-organizing neural network model for a mechan-
ism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36, 193-
202.
Kienker, P. K., Scjnowski, T. J., Hinton, G. E., & Schumacher, L. E. (1985). Separating
figure from ground with a parallel network. Unpublished manuscript.
Le Cun, Y. (1985, June). Une procedure d'apprentissage pour reseau a scuil assymetrique [A
learning procedure for assymetric threshold network]. Proceedings of Cognitiva 85, 599-604.
Paris.
McClelland, J. L., & Rumelhart, D. E. (1981). An interactive activation model of context
effects in letter perception: Part 1. An account of basic findings. Psychological Review, 88,
375-407.
Minsky, M. L., & Papert, S. (1969). Perceptrons. Cambridge, MA: MIT Press.
Parker, D. B. (1985). Learning-4ogic (TR-47). Cambridge, MA: Massachusetts Institute of Tech-
nology, Center for Computational Research in Economics and Management Science.
V -"A
34 RUMELRART, HITON, and WILL"IS
Rumelhart, D. E., & McClelland, J. L. (1982). An interactive activation model of context
effects in letter perception: Part 2. The contextual enhancement effect and some tests and
extensions of the model. Psychological Review, 89, 60-94.
Widrow, G., & Hoff, M. E. (1960). Adaptive switching circuits. Institute of Radio Engineers,
Western Electronic Show and Convention, Convention Record, Part 4, 96-104.
4'O
A.V
ICS Technical Report List
The following is a list of publications by people in the Institute for Cognitive Science. For
reprints, write or call:
Institute for Cognitive Science, C-015
University of California, San Diego
*La Jolla, CA 92093
(619) 452-6771
%. 8301. David Zipser. The Representation of Location. May 1983.
8302. Jeffrey Elman and Jay McClelland. Speech Perception as a Cognitive Process: The
* Interactive Activation Model. April 1983. Also published in N. Lass (Ed.),.Speech and
language: Volume 10, New York: Academic Press, 1983.
8303. Ron Williams. Unit Activation Rules for Cognitive Networks. November 1983.
8304. David Zipser. The Representation of Maps. November 1983.
8305. The HMI Project. User Centered System Design: Part 1, Papers for the CHI '83 Confer-
ence on Human Factors in Computer Systems. November 1983. Also published in A. Janda
(Ed.), Proceedings of the CHI '83 Conference on Human Factors in Computing Systems.
New York: ACM, 1983.
8306. Paul Smolcnsky. Harmony Theory: A Mathematical Framework for Stochastic Parallel
Processing. December 1983. Also published in Proceedings of the National Conference on
vArtificial Intelligence, AAAI-83, Washington DC, 1983.
8401. Stephen W. Draper and Donald A. Norman. Software Engineering for User Interfaces.
0January 1984. Also published in Proceedings of the Seventh International Conference on
*- Software Engineering, Orlando, FL, 1984.
8402. The UCSD HMI Project. User Centered System Design: Part 11, Collected Papers.
-C: March 1984. Also published individually as follows: Norman, D.A. (in press), Stages and
* levels in human-machine interaction, International Journal of Man-Machine Studi,,s;
Draper, S.W., The nature of expertise in UNIX; Owen, D., Users in the real world;
O'Malley, C., Draper, S.W., & Riley, M., Constructive interaction: A method for study-
ing user-computer-user interaction; Smolensky, P., Monty, M.L., & Conway, E., For-
malizing task descriptions for command specification and documentation; Bannon, LJ.,
& O'Malley, C., Problems in evaluation of human-computer interfaces: A case study;
Riley, M., & O'Malley, C., Planning nets: A framework for analyzing user-computer
interactions; all published in B. Shackel (Ed.), INTERACT '84, First Conference on
%IW4f %
Human-Computer Interaction, Amsterdam: North-Holland, 1984; Norman, D.A., & Draper,
S.W., Software engineering for user interfaces, Proceedings of the Seventh International
Conference on Software Engineering, Orlando, FL, 1984.
8403. Steven L. Greenspan and Eric M. Segal. Reference Comprehension: A Topic-Comment
Analysis of Sentence-Picture Verification. April 1984. Also published in Cognitive Psychol-
ogy, 16, 556-606, 1984.
8404. Paul Smolcnsky and Mary S. Riley. Harmony Theory: Problem Solving, Parallel Cognitive
Models, and Thermal Physics. April 1984. The first two papers are published in Proceed-
ings of the Sixth Annual Meeting of the Cognitive Science Society, Boulder, CO, 1984.
8405. David Zipser. A Computational Model of Hippocampus Place-Fields. April 1984.
8406. Michael C. Mozer. Inductive Information Retrieval Using Parallel Distributed Computation.
May 1984.
8407. David E. Rumelhart and David Zipser. Feature Discovery by Competitive Learning. July
1984. Also published in Cognitive Science, 9, 75-112, 1985.
8408. David Zipser. A Theoretical Model of Hippocampal Learning During Classical Conditioning.
December 1984.
8501. Ronald J. Williams. Feature Discovery Through Error-Correction Learning. May 1985.
8502. Ronald J. Williams. Inference of Spatial Relations by Self-Organizing Networks. May 1985.
8503. Edwin L. Hutchins, James D. Hollan, and Donald A. Norman. Direct Manipulation
Interfaces. May 1985. To be published in D. A. Norman & S. W. Draper (Eds.), User
Centered System Design: New Perspectives in Human-Computer Interaction. Hillsdale, NJ:
* Erlbaum.
8504. Mary S. Rilcy. User Understanding. May 1985. To be published in D. A. Norman & S.
W. Draper (Eds.), User Centered System Design: New Perspectives in Human-Computer
Interaction. Hillsdale, NJ: Erlbaum.
8505. Liam J. Bannon. Extending the Design Boundaries of Human-Computer Interaction. May
1985.
8506. David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning Internal
i Representations by Error Propagation. September 1985. To be published in D. E.
Rumelhart & J. L. McClelland (Eds.), Parallel Distributed Processing: Explorations in the
Microstructure of Cognition. Vol. ): Foundations. Cambridge, MA: Bradford Books/MIT
Press.
1n-
Earlier Reports by People in the Cognitive Science Lab
The following is a list of publications by people in the Cognitive Science Lab and the Institute
for Cognitive Science. For reprints, write or call:
Institute for Cognitive Science, C-015
University of California, San Diego
La Jolla, CA 9293
(619) 452-6771
ONR-8001. Donald R. Gentner, Jonathan Grudin, and Eileen Conway. Finger Movements in
Transcription Typing. May 1980.
ONR-8002. James L. McClelland and David E. Rumelhart. An Interactive Activation Model of
the Effect of Context in Perception: Part I. May 1980. Also published in
* Psychological Review, 88.5, pp. 375-401, 1981.
ONR-8003. David E. Rumelhart and James L. McClelland. An Interactive Activation Model of
the Eff ect of Context in Perception: Part 11. July 1980. Also published in
Psychological Review, 89, 1, pp. 60-94, 1982.
ONR-8004. Donald A. Norman. Errors in Human Performance. August 1980.
ONR-8005. David E. Rumelhart and Donald A. Norman. Analogical Processes in Learning.
September 1980. Also published in J. R. Anderson (Ed.), Cognitive skills and their
acquisition. Hillsdale, NJ: Erlbaum, 1981.
ONR-8006. Donald A. Norman and Tim Shallice. Attention to Action: Willed and Automatic
Control of Behavior. December 1980.
ONR-8101. David E. Rumelhart. Understanding Understanding. January 1981.
ONR-8102. David E. Rumelhart and Donald A. Norman. Simulating a Skilled Typist: A
Study of Skilled Cognitive-Motor Performance. May 1981. Also published in
Cognitive Science, 6, pp. 1-36, 1982.
ONR-8103. Donald R. Gentner. Skilled Finger Movements in Typing. July 1981.
ONR-8104. Michael I. Jordan. The Timing of Endpoints in Movement. November 1981.
ONR-8105. Gary Perlman. Two Papers in Cognitive Engineering: The Design of an Interface to
a Programming System and MENUNIX: A Menu-Based Interface to UNIX (User
Manual). November 1981. Also published in Proceedings of the 1982 USENIX
Conference, San Diego, CA, 1982.
ONR-8106. Donald A. Norman and Diane Fisher. Why Alphabetic Keyboards Are Not Easy to
Use: Keyboard Layout Doesnt Much Matter. November 1981. Also published in
Human Factors, 24, pp. 509-515, 1982.
ONR-8107. Donald R. Gentner. Evidence Against a Central Control Model of Timing in Typing.
December 1981. Also published in Journal of Experimental Psychology: Human
Perception and Performance. 8. pp. 793-810, 1982.
ow
ONR-8201. Jonathan T. Grudin and Serge Larochelle. Digraph Frequency Effects in Skilled
Typing. February 1982.
ONR-8202. Jonathan T. Grudin. Central Control of Timing in Skilled Typing. February 1982.
ONR-8203. Amy Geoffroy and Donald A. Norman. Ease of Tapping the Fingers in a Sequence
Depends on the Mental Encoding. March 1982.
ONR-8204. LNR Research Group. Studies of Typing from the LNR Research Group: The role of
context, differences in skill level, errors, hand movements, and a computer simulation.
May 1982. Also published in W. E. Cooper (Ed.), Cognitive aspects of skilled
typewriting. New York: Springer-Verlag, 1983.
ONR-8205. Donald A. Norman. Five Papers on Human-Machine Interaction. May 1982. Also
published individually as follows: Some observations on mental models, in D.
Gentner and A. Stevens (Eds.), Mental models, Hillsdale, NJ: Erlbaum, 1983; A
psychologist views human processing- Human errors and other phenomena
suggest processing mechanisms, in Proceedings of the International Joint Conference
on Artificial Intelligence, Vancouver, 1981; Steps toward a cognitive engineering:
Design rules based on analyses of human error, in Proceedings of the Conference
on Human Factors in Computer Systems, Gaithersburg, MD, 1982; The trouble with
UNIX, in Datwnation, 27,12, November 1981, pp. 139-150; The trouble with
networks, in Datamation, January 1982, pp. 188-192.
ONR-8206. Naomi Miyake. Constructive Interaction. June 1982.
ONR-8207. Donald R. Gentner. The Development of Typewriting Skill. September 1982. Also
published as Acquisition of typewriting skill, in Acta Psychologica, 54, pp. 233-248,
1983.
ONR-8208. Gary Perlman. Natural Artificial Languages: Low-Level Processes. December 1982.
Also published in The International Journal of Man-Machine Studies, 20, pp. 373-419,
1984.
ONR-8301. Michael C. Mozer. Letter Migration In Word Perception. April 1983. Also
published in Journal of Experimental Psychology: Human Perception and
Performance. 9, 4, pp. 531-546, 1983.
ONR-8302. David E. Rumelhart and Donald A. Norman. Representation in Memory. June
1983. To appear in R. C. Atkinson, G. Lindzey, & R. D. Luce (Eds.), Handbook
of experimental psychology. New York: Wiley (in press).
ONR DISTREBUTION LIST
10 m C
a r 0 L
c z0 '4 .-
a. v -C
CU . 0 %,a - ... 5116
Le. L. ; *1 '0 L.'a0 & 0 " cac a1 L;L -- c) 0 0 us~ c S1 1--C 4
61no 2 6 0- L a 6- 93 a. a-02a
SI) a- u1 0, C21- L.45to
06 6 04 . @ 0 044. 1- 064 - .500 0 2
660- : .h4 0' a~ 0.0 a 04 a.L C ~10 044 c .9 0U 0 ' -
-0~4. I 0 v r 4.00; 0 we-4 066 0 LO 0 C 4 L C4 o-0052. 0 04,60 toS Z O 0 6 0 4 £ -CcI -ft"1C 6I .060 - C a *C IV 46 4 C mo 400 0 0C1. O4
04 IsID L. 62 00 V ,01C mw 06. M ,.1 61C - L C
404 cc 1-0 . .0 6 a0600 a . 4. 00 a w24U. in. -3100 - * .14 1. 0 .00S * 0.4 .20 m-0 UN~ .0. )460
£~~~ LOj -, 0'. .- 0 - O L C W ) 1:5.1 0 61. 14 CL -0-4- 0 . 0 c va .00 L2. 0 0 .6 C4 2 5- L.6 1, L50 00 10
,...2L 6 .C-C9 1 AU
CO 0
tP C .0 L
v u- 0
0 T2 00. 0 U% 0
0 - 44 '0 Cm 0- CL.. PI
0 6 1- 61- 0 2z C . - 621 . 0 0 ~ O 60mr? *-L 4 4.
£ 0 N V 0 000 1-.40..4V L1 0 .4 o- 04 0411 -
7 0 a 3 00O 200. 0 00 Vi 2a0 0 60 CL 0 *, 020 20
.- 04OO 9 111 c10 0 000 *2 600 . -0 ; go 1 000 2
13 n-to L II U OM4 C' a -3 M, .C C - ' . .. 6 X 4
L50 N. a5 s-I 4 - C 0 a144 to4 04.4 U. WL 1.4O
a.CO C0 C C! 00 LC00- C N 0- 2,- 0- W 2.4 0-u6.- a b 1.0 low 6 I4. v60 v > C.C
1-L.1 1- C-L 0 006- 53~~
06
U, i C L -
U N
Cw44
04 E
60-0 t .1 - c 00'
-'6 0 000. 0 0. 44 C4 2
0 4 .. 01 aO . 0 2. g1 a CL
6 4 0 .04 L 5 COO 44 0-
1 4 0 CO 0 a6 0 a-4610
6 ti. C- C0 a, .2 =0 &060.0V210 a0 .4 .- 00 20 0' t. ON - .0 0
o a 0 ft .4U 6 0 N:44 ) 0 L 100 0 ON 0 Nl 0 0 v
a 0 .1 600 6>.C- 4S0 1.0 0 V CN0 E' "0oa, CU4.4 0 s 0. a ION 0 C-6 *L .
S44 0 O cO 0 0U 00 Q .4 .10 0 004 > v-2 0 U 0.
1.2102. 0. 00 C 52 2 V - x5 0 0 -. >0 :20
1-44. 0 0.24 t: a 4 0 K5 . C .2 0 £44 w. .0 i L
4..0 6 41 6 -5.L (06 0.0 * 1.0 ' )10 -~.L 0-C"- " CW -. of62. 02 V * I 02-CO 104 44 L 1- I4
V.6 6t 41-44- m 4 1.. .41 >0 0.0 0140 * 00- 2 S) .
v 1 . 31 L. 0. .0.l- c C 4.14 O'S II 2 UMll1 C NW 00C -6
.4 0041 0= L. 002 O>' 2 1E 0 2.0 N 0 0
,u 0 toc2 II.Lle'
f" ~ ~ ~ go " 4 M -C- om4
CL . 0 0 4 0
- CS 0 o- 00 1-4 1 60
n0 a01- 0 u- 44C .. N 00 06 NC
U. C444 20 . 1 2 .s1 1 0 .4. N 021 4 0
4144 A11 C0 * - N * 10 00 -l.0 c 4 N Cn0. C.0 to N 0 0N 0001. c4 N O 60.N0
00 , C4. 06 60 C 0 ' 0 6 40 4s
00a £0 . 6 L c6 2. 0a0402 .4 1
00.I' r4 r4 0 .-. -0= 44005. 1.4 - 046 6 005
Za,21 0 a..1 6 . .4 *40 1)0-. call4 .4 1- a-. CM 0 Ca
C. 64OC 0 0 6
:.S ~ C ;1 -60 L..00.4 1-0 6 02 C 6.30 CO 20 m1CO L0. . Ico 06 -4 .4 600. A.40 .4 62441 024. s6 0,24060 aI
ONR OITFRIBUTION LIST
0%0
0 1. 0 wN-
tA 0. 0.. a4
m 1 =m' u u p.. 4- .0C 'a cu 0 00 0 840 v-~ O
e. o m 0 ~04 0 0.D4 0Q c .40>4 t
-. 4. 00 vc C4 1 0 -A c o> N .oi wS o- 4
aC C4 4.4 'o1 04. 4..- a 4>2
.- 4 4.r 4 c-4 oO. a ~ m ~ . M08 . a-4.4 CO C ' o
t .~ a 0 C a4 > a8 . . -) 4 ~ - 8 4 8 4 0 4 0 . 0 .
F.F
S 0O .0 ;W r . r 
01N
2 ; 0L 
. 4
L-. 0 C E
L C 64 6 .0M U O
0 ca z u>4 03 p >,U4,4-5 2 ,u Q0um 
C A
0~4 V. 0> 44 to 4. 44L L 0> 0-..c 0. w4 0W In4 0 0 8- .w
CIO0 n C,4c =1 to 4 48 4U > 4 14- C .4 a-4 a 0> 00
0 0 m0. 04 4. XC C % - r)O 84 84. L-c L400
L. 0 (, 40 .0 .84 84 CL0
am- Mc. 08 > 4.- 40 0 . . ON c4 - C 0, -C a. 40a,- 0 4.40
.8 ~ 4 >. o.'8 a.-4 4 0440 C0' 4.0.. 4l08
4k a~L0 C4 U 404 .4 4 0 4 840 L0 to4 a 0 a4 '8
4'8 1n~-. r 004 a.84. g.448 o 4 34 .uJF o1 8to4 10
.40 0 .L0a0o21 08. .48 .4 C8. c.a.-.. C m484. W0. I I
.L.. aL0.0. 4L.. 84 W-. L4 0... a04 8.4
448 04 0 84. 0O~'W .0 080 84 11. IEO a840
00
Cm1 >. Cn
10 84 4. 0 e - O
0 -4. 0 "1 0
-. 0~ o.4w 00 3- 4 48 > 14 0 4 .4 t-
4.0 u. L 4. cc C1. aa
m* 54.844
84 0 020CCC 0~ 0~4 40 840--C
084~1 0 .448 US O z4 E 4 'Q-4~~> 8.
Z8S0 E.4 'mt u20 t-.4. 4I4 04.. .484 loW04.T41 0-4.1' cw L C44
a444>.0 m a L8 a0- @1 00.. 10C. . .. O =-
u4.0~ D 444a 0. 081c 4. 08. > 40- C-40 044 c.444 .0c 4
A=A Go Q0Q 3a 08M m4u8a> 00 n4> 84 a888 c 4 V) 0480
S080.0 0-04 0. 00. 'A.84 a14! 02 08 C( 0~ ~ 1 0 02
L C0 Cm40 L4L 4. 44 -C48 4.L.8 ~ ..
.4 0 4
04. a 
4.LU cjC t.00-
0 -0 . 8 0 .4 >4 L r
8444044%~~ L8 0. L 4 0. O N41 48
4 s,'8 to8 .4 4 4 8 C 4UY 8
> .0 844 8 4 cv ~ ~ .L U- C-l . . 4
CL4.. 0C .1 E4-4.~ .8 0 >48 0 .~ a448 CC2 0
4..4C~ ~~~~ @1.8 4.0 L4 4 .444 14 0 = 448 4 2 .. o.
.......... AL.4 0 . 0 . 0 0..4 .. 4 4 . 48
ONR DISTRIUTION LIST
00
@0 CUi . 1 0 r l
-. C,40 11 4a0
u 0 L6 ~ 0 C 4 C
2% in 0 - - .0 0.
a ~ ~~~ cc -C-.-
I~~~e -.. a-.L0 a 2 OC
.0 -U O .O 83 USN On -UCc40 I 1 -. L. -K0' U0 01 - . 000-0 -4- 10 r. 14 0 MS .4 w05O~ Lm 40
A'sL C; C.0 ; SE N - N0"
0 01 .M .0 22, CP 0L).co0 0 xS!t-.- .
o00L CEOO 1C *C u. zO 1U0Oj
0
~~SC W) 0 :-4 v. . U 0 8 -*. Mc C C
Z.0.. i L 6 L COC -? .0. SACQ .0a L A0 0 A
.SS 6 A. I- A 0. M CS."O A , Mcu W, 0 =cA -2c A m 0 (00
L.4f 00 1. 05. . 0- 00 S L c
IS.0 :6:0 C 0 6L, b.
*0 :9. .0 50 A Lt. U0 a 0. 0S *C MM0~f *4V0 *O 0 t
cu~ 0. SO 0.I I. 05 L 5 Lovu L ~11 9 L f 4 DO W -. 0
L* c A a 0 s
1.- 0 g . . 0aZ. -% Lt;Cc
V3 000 a40 D. 0. 0o -y 3M 0td 0 0x 0 b4 0 0 0 r 0 L= .*US C
CL- c. a0 c- a. .0 00 COE-MN S
M0 40: On o-~ 0 I. 55 c 0 LI a.-0 L
0.. =0 " 0 Lo LO, or to _0,00 cC ;0. 055 c 0.
A 0N -LC . S - ..g. 0 :0 r 0 0,.0 -6LO
UN q40 at 'a40 60 L0 q- .0 4 a0 4 I. 0, c.-0.a60NC a .w M M Z0 . u a - a C N 40. 0 0.U.0 40C * 0.0 L0
S.. .00.. C A 4X( = it.0 0.40 V CtL C S.0@
a*-.0 &- .0.C LA I 0 I ~.'o 0. C 0 00i
F0 12-0 chO do ALU A .- =41 . LS 0mc "M400 ms *O 0.0.O5 I&AN 40=4I
00.4 c0 2Sm~ 104 N 03-.. 0C.
w 0 .0 . C 10 CO .. O4
c 0 L to Z00
0, 00 C2 SO cA M0 .00 0 40 6. 00.c..N
a- *LO.0 M .60. *0 _1 *0C*..-
L.0I
.0 c C0Ma.
0m c -0 .0
0.. 0. x It O L C',
2% 4 0 c N M c 0 r . 40 o
SC c O 0 0 M.c00- 0 0L 0 40.0
mMO E00 =.C 00 3M3 (.7 .-c cc;MC 0' MS 0 0. r0 06 SO0
AL C0 40% A 0 1.0 U C "C' U40 auO0 L. 0 660-
uS4 m0 U N . 0 c 96-4 0d w0 L 0 -K .0 .C L 0 IN 0 (..00
L ~ 0 L. C0 60 M0 jr. a c c CC v(0*a . L4 ' L -M
.c" -a ; CC ito.. * 00 C -. 4,- 300 . 0 0
.00 cc~"n c 0..0
UP0
C.A 0 ' 0 . C 0 O .0 ;0(0
C MC 0 40 CS
S Mm 0. L. 0.0 v .0..
0 V0 C.0 R- ON 00 .0 V L0 a.
I A"- 0O 06 r4 0.04 .A500 6 0 640
O MOO ;; 5 a "0 0. u a v 0 C I
00. LCC0 4 4 0 .00.00 Ca r .0 6 C' "5  10 0. V0r.0
a C 46004. . :1 M 10. N 3 . u00 L - A 65a - 0CS
'. L00C .0 LO 0..0. MO 0 0
40 A c006 * 0 0 w- 0 0
. 400. V., 00000 &C .0 xq M r
0 0 00. L ~ S.
0  
0. PCC0o .4 . 4 - ua 00 .a MM 0 *O O 0
-C C3 A 4 0 A I 5 a -4 a- .-a-W a-
.
:
.
a. 'S-&~ W3 c ua
Ot4R DESTRIUYION LIST
00 L 0
t 0 1 x
0 . L Z co 1- 0 C -
c N4 .4 40.
MW 44. 0 NI N -.. j c 0 *I a c
04 >0 .U V Q : a uL.0c
0 NN 00 C CO 4 Wu-4Z
X4. , 0 4J 0 0 4 C.0 0 04 4 -- 41464 - 01
*r 01 a4 ' .4 6 64 0
0 46 * 4. 0 .0 0U 2 0 00 U . U 4 .0 0 V '
,4 14 0 aO ,- 0 a4 00 a4 O O I44 Ica 04N 1.4 LW 0 10 W
0 ,-.24.4. L4-- Z44. -400 4.0. 6... A".. 0440 W.4 >C 44
4-C O -L4 4c.0 0 .0 0 Q0 440. a IQ a4 a . 0 0* W,4 C 040 0
a 04. c. w2 44 44.4 04 44 L .s >0 X L
0 I040. a W- r 0 Z 0 U0 v 44 -C O O on00 OM 4-4
- 000 .1 ... In C. 00 a4 0 0 4 0~
40
41 L 0
0 8. .X0* i .'
NL - 6 m v0U 0.. 00.- o. 0
ol 0- -l. 0 14 14, 6o
.4 6" 0 00 . C, r-4) 00 .I
r N44 V L N 0 a4V4 >0
3C W04 0- r Uk N r. a >4CQ)0
0 N9 M44 L -. c. 0- > P-9 0 . 0 a4 04.4 .4 N 0.4 c~ -0k 
C.6 C1 . L. V r 1.N . >4 C 0 6 - Zo. 0 0 1,O
-a t a -* 0 V0 2 0 - 0 aa-
V CIII 0 0 2. 0 be .. 4- 0 2 4.0 a a 0 24 0 046. ~ V .
00... 6444 0 L - ~4~* 44 46 04 0 A~-0 44 .0 44L-
ca4.40.4 444 2 0L O I's0.- 0 *. 0 4 NJ6 0 0 0
44~; 4>.0 .. 45~2.0 04 M 04 - c40 a4.400 10 .40 - *
4-6r. LII no cL CL6 W-.0 L0 0.SL 4- M C L. - -4 44 ~
020 I 0.A4 3t 0 'r- S 2000'44 A 00'0 0It A 2 A0v* 2 ' 440 .1 62 .42 9o 1 6
444,
0 0
00
C .L 0L to- .04.. III. a4 0 4 L 4.
o c0 M -0 0 >.4f 644a
,a-4 .064 0-. A4 .. No. .. 0.4 L4.-0  0
>400 004 o 04 0>0 00 004 0 
6. 0 4 - 0.-k * 400
044 04 >4 0 - 2 0...C42 C 0 V 40 0 43 . N4 4 . v
4- 40 > 0 J 4 C N. a4- I2 r- III. a. > 4- 44 .-92 41 4. 2. aLa II.4V
20O LW M .-0 c L0 04 M a4W. 0 04 S a. 4 4. 4W
'n .m Mo 1... 6.4. 0. SO- U 0 44. 64- IN a"- za
444u4 4) r 6L o .
D% *0..- 0 k u. 44. "I. I- -0.0 *0-40 .0.4 .C 4)r0 O 4 o a. >O
K00
00. c 6 .L:,-
u 0 aQ ac oa0t 0 l
6 m X
V- W3 00 u 4 . 0 o M aI
0.>u0 404640
W44 4. 4444 00 6 4.
6 6 4-0 4. 4- 4c V 64. 
4
0.1 44 .-. * .42 0.-0 4-04 4 V
04-0 ~ ~ ~ ~ C 0) v- .0 S 0 04.40 A 4 0 a-4 0 0''o-04 W
x 04 A: SO 00 ,~ 0 4 - 24 N 4 - - - - -
c244 COO v -. 4 NN 0 WO. 5 0 .N 4444 M.0 0 4C 4- .0 c4 244 V 4444 - 40 :4CO O 1 04 Sc 5 0
u~ m, 00 001 0 C.44 0c 0I, O 0 0
c - 14 204.44 -, -.a 0 O l, 4 ; 1 ,I c-c. , .
44.. M- .4.. .44 2- I.JO 44 Z44. a- 04
>444. U ' 40: .-.4- i M0 3~4 .. 6 .40t ; ., IS.4.4a
m4- 4.f 04444 0. m .- >..O .- 4 64 - 4.4)-4 N.
0.4 46 CLl. I 44 L .N .4 644 -
-u kca A-iE I AU~4 ~-4* Al.Z 11. . 0 -W 2 a c a a
%404
ONR DISTRIBUTON LEST
.0
a 0
0 0
0 -., - . .0 10o . f .! 0 60E  c0 0 cmm a 0 a6 0
40 9. 0M 0 cA.0 1. 0
4.. 00 a4 4. 1. .0 IO . 0
cc c &3 a 0 1 n 40 ' 044 vi. a 1 0.coo I 0
-0 41m 00 1- a Z;C v4C .3C n ., - 0 c c.4I
.4 . 40 4 6 C . 4 0 L, . . 4 4 2 1 0 40 4 I 0 . 4 I.. 0 0 0
c0L 1 ". 6. U 13, 0 0 a 0 m40 00 a-
C4..- C4'. 6.".01 L 6 02 0 C.40 o, L WC 0.1
:D a0M0.aI =w ca.. 44064 .9 .. I ,-3U m m A. Q 0- 0"
00
Ur
,%,44
'a~6 'a U
.1- 44 3 0 v
0. 0 0 0 0 >4 01 a 04 6I.
0 6 60 z 0 4*4~ . . 0 0... - 0 -
.0 to I0 00 41.0 .44. . v , 6.- 0 X4 1 04 00o F o. 00 60m u4~1 v41. U% aM 44.044 .41 210 ~ ~ ~ ~ ~ > .. 0 N410 0L1 C41.. I W6S 1 14 6104 ) .0 0 001
U 00 0 QUNo U o. M U - 664 - 46 0 64. .CC 1. 0.4 10 0 0L. .) a 4 0-1 x.. O O vCN 01 >1 26 14
.- 10 0 1 C, 0c 2 1. 4 0 00 0 6 . 0 420 4. 24.4 - -4..4 04..1 .0A414.I U lot>o N4.I~C 4.o4 *0 a W2-4a; 1.
A.OCI 000 "0 00 G, .I 660 1 0.- O 0 0 Z . .0 0 0C * 0 r. 40-
0~~~ ~~ ~ 0 %. 0 .C. ,0 . 6 1 4J3.4~ U416
's404 :: 0: L *c x 0- c I1IU1 00411 0 0.0 104102 a,LU 01 W43. 16- L1.UC2L0 : 60 0 L 240 U 00 000 -I .
0.E401. 0: It.44 -1 40 60. 0 0 L 0.1 CL,4#0-v.c 0. 4 0 0 .0401 U.M- I-CW44
21.29- 0. S. .01. L0.1 .4 I~ 0z EJ.0 M0.4 0. 6 ~
L00-0. 6.. 416 u60 'a.- 60 4 41 1. 0 t. V 04.0
.0.0-2 v.02 .4.0 *6 . 6 .- 0 .. 04 -* 0 .D 0.0 1. 
c00
W' I0  ( a~~4 tol C 1 .C4 CM -. SI4 0 a)0 w m
00
04 0% 01
W0 .U 0. I
0 2 21, U &0 0-4mc 0
a. m1 t. EL m 4
W6; c 0 >.4. 0 44 -. a 1 14 0 41.0-C 0m wU0 0. a0 444. 6 0.4 CY4 4
Ot 1.6 a4 00 60 C 0 O 0 0a. z 1 66)6 M2 044 A to .1 U. 9-- Ul x0 - 0
Z434 010. n0 a 414 1.0)4 0 44 0
4.2 > 4. 61. 0- 4 -0 - -: r!1 wo4. .o C V 0, -C
44 ~ ~ ~ . n SCM 610 U C4 4LM . - 6 0.-4; NAM 6 a0C c0 . 1
A64C 0, 60 41 606C a40C c CL. 6 C 0C 04 0 U
IL211C r4 U U0- 1 a.6 00 00.W" 1.4 40
a4016 4M0 A.41. 4412. 0606 w0IS " a 6- 104. N1 2 4.4 .40 .60 lb
'a. A~ .4-4 000 2 0 0 1 Nf M01.1 M M 01-1 O L660 V 0 0 2
000 0 4 6 .04. 6.4 cc - .1 - 0
0.4 2 0 4.1. .060 &6 444 c c 0 . - 40 A140 . 6L0 44. .
60 0. 60 C01 0..11 C;
w0 44 -o CO.' 4100 0 0 62 *0 U4 0 v104 000 00
I,~I 6& 0 0 1 4 6 * 0 4 6 0 a4 C L0 4 0 0 u4 40 0. 0 0 04 4.0 ~ k 0. 01 . .I 1044 0 6 441. 10 0 .0 . I 1 .4cm~4 (I 46 0O.60 ~ 0I44 0-4-0 02 2 .600 00 641 0 0A* I-0- 09 U
0
1. 00 0 4,
06 0 0 -0 a . 0 .0 0 . 0 .0 0 .0 0. 00u10 L4 . 04( a v4 & 0 0 ci 0 a4 c N
0 0 0 0 6 00 1. 0 0 . 0' 1. 00 1. w a00a1 0 1-1 0 614 a 41 a; 44 6. 114 60 41 T 62.0 -4LI6 16C 4 0 0 4 0 r I 0 -C 1 0 44o
.43. 6 4. 6 0- U 0 - U 4- >
0~ 2. .. . . . . . . . . . . . . .
444044 0C L o
ONR DISTRIBUTION LIST
00
03 0
o r3 . 0 0
0 0 3 4 0 0 0
-3 0 . 0 .
-x C 0 0 >4 33 3
a0 6.4 33 w( 3
31 0 . 0 5 0 0 S 03 .a 3
: >- 'o .' = u 0- 300 c 0
I- cc 3 0.3. xU t 0 03 .3 0 W" 0 0 0
4, 0. %.043 W .. 1034.
33 0 r4 -
0430 = 0 -
* * VO .4 0 ) 0 * . 30 0- 5.
>33-3 E 0) sa> t. 3 o s, . o4333..~ ~~~~~~~~~~ 0304.3J -03.. 34.* 0 0 033.
a343 IQ04 3. m.0~ w3 9m1 x3 W *J 0 04
.03. 3 0 3. 3C L3 I . 3 30 3 3.3 043 3.'2.O3 0m oLo, CI &3.. 00333 r-303 OZ w 3 00.43. 0 c 4 vm 0 3Z
A 8 0c0 cm3 :!w.4
- 0 00 0
.0 > 34o 0. :a . .
V)33 .3. 6 a 0 4 a 43 L34 43 3) *C300 . 43 u cn >0 wa 0. 5 0 3.
m4 0 &.3 3.. 3 0( 3000 3.0 4
043 0- 3 .3 c sa- 0 0 3g c,--a 333 -'a0 53 0 03 C0 a-
430 4, 30 o30 w o .Lo 00, 033 W33 C. 12.31 0.3 >33 .4.8( 34
2. 33 o4 W - ).4 43 5, .43 .. 3> .- 30 .0i0 4 - 30 m-30 34 >3N n3.0 u4 3.4 LA a-34003.3 4.0 N .4043 .. )0 U,- CC C~ 'a3 >504>v a 3 m343 v a.43.3 v'. 34> c 43 .
I a- .4 =43 0. 0 !03 4 5 z *3 ,-3 43. 3 . 3
C,4 43> . a4434 a> .30 34 4334 a~.3. A~.u A34 m 3 N.3 .0 -
0 ~ ~ 0 0m3.31.3
30*3 * a M0. 936 .0. a . >.C.. .. >. ~ 0 m C
3.43.0. A3 * m3 . M.4 053 344. 3. 33.V 3- 3 0 .*
4300~~~~~~t a...0 334 O).s 'a3 33334 303 .>s. .30 .3..-.34>* mO..3 .0 00 -- 04 3.4 3-s 2 .53 3933 3-
N 4' L x =
0 c u L
in 34 3-- 4.
-0 (3- c- 53 W)-0aaLWI
3 c 3 Lu 0 0 0 L4
O- 00 go! 0 333 -0. 00 e. 34 ? ,34 3. 0
1.3 - >353 o0 o o, M034 043:43 0 >> 5 N
F443 a ON a..5 .03 M033. I3 > C3. 33 >40 404 ~
4.343 .30 .43400 > --. z ar~ 0 -0 u-0.3 m 3433 V.40 N 3.
043 03 4. 3O.4 43 33 430 V 3 . 34
0.3 0) no 0)O Q34 a43.0 u53 I- t- a .0 nV nU - V um ao
In4 3 1330 3 3> . 30 3...- .33 3.34 3 13 .
0 . 3300. .034 33.40 3 0. 0 3 4 > 03.3 344 433 43
,. A-3- £3 03 m3 03 . \, 00
*~ 0 uc
30 30 r
V* mc .34 '
03 0 - L o
r *3 . . w
34 a3 0
*S 0 L0 0 *3 I4 w. m 0 C
L -3 *3 L 0 0 34 3.
w 43 30 43 v0 e3 030 A0 00:1 - 3
4305',~ ~~~~~ A00 . 3 003 30 3 - .V333 3 3  43 N C'S 333 3433 N 31*331 3)3 >3330 0..* 34 3343Z a3.* 03 . 3..0 033 .3434 44
3403 a3 0 4 0.3. 3 >.0 0 33 034 033303)3.~ ~~~~~~~ 3033 .0 0 4. 33. .0 034. 0

