C
lo
ud
 C
om
pu
ti
ng
14  Published by the IEEE Computer Society 1089-7801/09/$26.00 © 2009 IEEE IEEE INTERNET COMPUTING
C loud computing is, to use a cloud-inspired pun, a nebulously defined term. However, it was arguably 
first popularized in 2006 by Amazon’s 
Elastic Compute Cloud (EC2; www. 
amazon.com/ec2/), which started offer-
ing virtual machines (VMs) for US$0.10 
an hour using both a simple Web inter-
face and a programmer-friendly API. 
Although not the first to propose a 
utility computing model, Amazon EC2 
contributed to popularizing the infra-
structure-as-a-service (IaaS) para digm, 
which became closely tied to the no-
tion of cloud computing. An IaaS cloud 
enables on-demand provisioning of 
computational resources in the form 
of VMs deployed in a cloud provider’s 
data center (such as Amazon’s), mini-
mizing or even eliminating associated 
capital costs for cloud consumers and 
letting those consumers add or remove 
capacity from their IT infrastructure 
to meet peak or fluctuating service de-
mands while paying only for the actual 
capacity used.
Over time, an ecosystem of provid-
ers, users, and technologies has co-
alesced around this IaaS cloud model. 
More IaaS cloud providers, such as 
GoGrid, FlexiScale, and ElasticHosts, 
have emerged, and a growing number 
of companies base their IT strategy on 
cloud-based resources, spending little 
or no capital to manage their own IT 
infrastructures (see http://aws.amazon.
com/solutions/case-studies/ for  several 
examples). Some providers, such as 
One of the many definitions of “cloud” is that of an infrastructure-as-a-service 
(IaaS) system, in which IT infrastructure is deployed in a provider’s data center 
as virtual machines. With IaaS clouds’ growing popularity, tools and technologies 
are emerging that can transform an organization’s existing infrastructure into a 
private or hybrid cloud. OpenNebula is an open source, virtual infrastructure 
manager that deploys virtualized services on both a local pool of resources and 
external IaaS clouds. Haizea, a resource lease manager, can act as a scheduling 
back end for OpenNebula, providing features not found in other cloud software 
or virtualization-based data center management software. 
Borja Sotomayor
University of Chicago 
Rubén S. Montero  
and Ignacio M. Llorente
Universidad Complutense  
de Madrid
Ian Foster
Argonne National Laboratory, 
University of Chicago
Virtual Infrastructure 
Management in Private  
and Hybrid Clouds
SEPTEMBER/OCTOBER 2009 15
Virtual Infrastructure Management
Elastra and Rightscale, focus on deploying and 
managing services on top of IaaS clouds, in-
cluding Web and database servers that benefit 
from such clouds’ elastic capacity, and let their 
clients provision services directly instead of 
having to provision and set up the infrastruc-
ture themselves. Other providers offer products 
that facilitate working with IaaS clouds, such as 
rPath’s rBuilder (www.rpath.org), which enables 
users to dynamically create software environ-
ments to run on a cloud.
Although this ecosystem has evolved around 
public clouds — commercial cloud providers 
that offer a publicly accessible remote inter-
face for creating and managing VM instances 
within their proprietary infrastructure — inter-
est is growing in open source cloud comput-
ing tools that let organizations build their own 
IaaS clouds using their internal infrastructures. 
These private cloud deployments’ primary aim 
isn’t to sell capacity over the Internet through 
publicly accessible interfaces but to give local 
users a flexible and agile private infrastructure 
to run service workloads within their adminis-
trative domains. Private clouds can also support 
a hybrid cloud model by supplementing local in-
frastructure with computing capacity from an 
external public cloud. Private and hybrid clouds 
aren’t exclusive with being public clouds; a pri-
vate/hybrid cloud can allow remote access to its 
resources over the Internet using remote inter-
faces, such as the Web services interfaces that 
Amazon EC2 uses. Here, we look at two open 
source projects that facilitate the management 
of such private/hybrid cloud models.
Virtual Infrastructure Management
To provide users with the same features found 
in commercial public clouds, private/hybrid 
cloud software must
•	 provide a uniform and homogeneous view 
of virtualized resources, regardless of the 
underlying virtualization platform (such as 
Xen, Kernel-based Virtual Machine (KVM), 
or VMware);
•	 manage a VM’s full life cycle, including set-
ting up networks dynamically for groups of 
VMs and managing their storage require-
ments, such as VM disk image deployment or 
on-the-fly software environment creation;
•	 support configurable resource allocation 
policies to meet the organization’s specific 
goals (high availability, server consolidation 
to minimize power usage, and so on); and
•	 adapt to an organization’s changing re-
source needs, including peaks in which lo-
cal resources are insufficient, and changing 
resources, including addition or failure of 
physical resources.
Thus, a key component in private/hybrid 
clouds will be virtual infrastructure (VI) man-
agement, the dynamic orchestration of VMs 
that meets the requirements we’ve just outlined. 
Here, we discuss VI management’s relevance 
not just for creating private/hybrid clouds but 
also within the emerging cloud ecosystem. Our 
two open source projects, OpenNebula (www.
opennebula.org/) and Haizea1 (http://haizea.
cs.uchicago.edu/), are complementary and can 
be used to manage VIs in private/hybrid clouds. 
OpenNebula is a VI manager that organizations 
can use to deploy and manage VMs, either indi-
vidually or in groups that must be coscheduled 
on local resources or external public clouds. It 
automates VM setup (preparing disk images, 
setting up networking, and so on) regardless of 
the underlying virtualization layer (Xen, KVM, 
or VMware are currently supported) or external 
cloud (EC2 or ElasticHosts are currently sup-
ported). Haizea is a resource lease manager that 
can act as a scheduling back end for OpenNeb-
ula, providing leasing capabilities not found in 
other cloud systems, such as advance reserva-
tions (ARs) and resource preemption, which are 
particularly relevant for private clouds.
The Cloud Ecosystem
VI management tools for data centers have 
been around since before cloud computing be-
came the industry’s new buzzword. Several 
of these, such as the Platform VM Orchestra-
tor (www.platform.com/Products/platform-vm 
-orchestrator), VMware vSphere (www.vmware.
com/products/vsphere/), and Ovirt (http://ovirt.
org), meet many of the VI management require-
ments we outlined earlier, providing features 
such as dynamic placement and VM manage-
ment on a pool of physical resources, auto-
matic load balancing, server consolidation, and 
 dynamic infrastructure resizing and partition-
ing. Although creating what we now call a pri-
vate cloud was already possible with existing 
tools, these tools lack other features that are 
relevant for building IaaS clouds, such as pub-
Cloud Computing
16   www.computer.org/internet/ IEEE INTERNET COMPUTING
lic cloud-like interfaces, mechanisms for adding 
such interfaces easily, and the ability to deploy 
VMs on external clouds.
On the other hand, projects such as Globus 
Nimbus2 (http://workspace.globus.org) and Eu-
calyptus3 (www.eucalyptus.com), which we 
term cloud toolkits, can help transform existing 
infrastructure into an IaaS cloud with cloud-
like interfaces. Eucalyptus is compatible with 
the Amazon EC2 interface and is designed to 
support additional client-side interfaces. Glo-
bus Nimbus exposes EC2 and Web Services Re-
source Framework (WSRF) interfaces and offers 
self-configuring virtual cluster support. How-
ever, although these tools are fully functional 
with respect to providing cloud-like interfaces 
and higher-level functionality for security, 
contextualization, and VM disk image man-
agement, their VI management capabilities are 
limited and lack the features of solutions that 
specialize in VI management.
Thus, an ecosystem of cloud tools is start-
ing to form (see Figure 1) in which cloud tool-
kits attempt to span both cloud management 
and VI management but, by focusing on the 
former, don’t deliver the same functionality as 
software written specifically for VI manage-
ment. Although integrating cloud management 
solutions with existing VI managers would 
seem like the obvious solution, this is compli-
cated by the lack of open and standard inter-
faces between the two layers, and the lack of 
certain key features in existing VI managers. 
Our aim, therefore, is to produce a VI manage-
ment solution with a flexible and open archi-
tecture that organizations can employ to build 
private/hybrid clouds. 
With this goal in mind, we started devel-
oping OpenNebula and continue to enhance 
it as part of the EU’s Reservoir project (www.
reservoir-fp7.eu), which aims to develop open 
source technologies to enable the deployment 
and management of complex IT services across 
different administrative domains. OpenNebula 
provides similar functionality to that found in 
existing VI managers but also aims to over-
come those solutions’ shortcomings — namely,
•	 the inability to scale to external clouds;
•	 monolithic and closed architectures that 
are hard to extend or interface with other 
software, not allowing seamless integration 
with existing storage and network manage-
ment solutions deployed in data centers;
•	 a limited choice of preconfigured placement 
policies (first fit, round robin, and so on); 
and
•	 a lack of support for scheduling, deploying, 
and configuring groups of VMs (for example, 
a group of VMs representing a cluster, where 
Amazon EC2
and other
public clouds
Eucalyptus Globus
Nimbus
Need raw
infrastructure
Need to outsource
excess workloads
Need resources on which to instantiate
services (Web, databases, and so on) for their users
Cloud consumers
Cloud management
VI management
VM managersXen KVM VMware. . .
Individual users Other clouds
Cloud interfaces (Amazon EC2 WS, 
Nimbus WSRF, ElasticHosts REST)
Platform-as-a-service
OpenNebula
VMware vSphere
and others
(a)
(b)
(c)
(d)
Cloud toolkits currently do not use
virtual infrastructure managers and,
instead, manage VMs themselves
directly, without providing the full set
of features of VI managers
Figure 1. The cloud ecosystem for building private clouds. (a) Cloud consumers need flexible infrastructure on demand. 
(b) Cloud management provides remote and secure interfaces for creating, controlling, and monitoring virtualized 
resources on an infrastructure-as-a-service cloud. (c) Virtual infrastructure (VI) management provides primitives to 
schedule and manage VMs across multiple physical hosts. (d) VM managers provide simple primitives (start, stop, 
suspend) to manage VMs on a single host.
SEPTEMBER/OCTOBER 2009 17
Virtual Infrastructure Management
all the nodes either deploy entirely or don’t 
deploy at all, and where some VMs’ configu-
ration depends on others’, such as the head-
worker relationship in compute clusters).
Table 1 provides a more detailed comparison be-
tween OpenNebula and several well-known VI 
managers, including cloud toolkits that perform 
VI management. 
Table 1. Comparison of tools providing virtual infrastructure management capabilities.
Tool Provisioning 
model
Default placement policies Configurable 
placement policies
Support for hybrid cloud Remote 
interfaces
Amazon EC2 Best-effort Proprietary Proprietary No EC2 Web 
services API 
VMware 
vSphere
Immediate Initial placement on 
CPU load and dynamic 
placement to balance 
average CPU or memory 
load and consolidate 
servers
No Only when both the local 
and external cloud use 
vSphere
vCloud API
Platform 
Orchestrator
Immediate Initial placement on CPU 
load and migration policies 
based on policy thresholds 
on CPU utilization level
No No No
Nimbus Immediate Static-greedy and round-
robin resource selection
No Includes an “EC2 back 
end” that can forward 
requests to EC2, 
but local and remote 
resources must be 
managed separately
EC2 Web 
services API 
and Nimbus 
Web Services 
Resource 
Framework
Eucalyptus Immediate Static-greedy and round-
robin resource selection
No No EC2 Web 
services API
oVirt Immediate Manual mode No No No
OpenNebula 
1.2
Best-effort Initial placement based 
on requirement/rank 
policies to prioritize those 
resources more suitable 
for the virtual machine 
(VM) using dynamic 
information and dynamic 
placement to consolidate 
servers
Support for any 
static/dynamic 
placement policy
Driver-based 
architecture allows 
interfacing with multiple 
external clouds; supports 
EC2-compatible clouds 
and ElasticHosts
No
OpenNebula 
1.2/ Haizea
Immediate, 
best-effort, 
and advance 
reservation 
(AR)
Dynamic placement to 
implement AR leases
VM placement 
strategies supporting 
queues and priorities
Driver-based 
architecture allows 
interfacing with multiple 
external clouds; supports 
EC2-compatible clouds 
and ElasticHosts
No
OpenNebula 
1.2/ Reservoir
Immediate 
and best-
effort
Load balancing and power-
saving policies
Support for policy-
driven probabilistic 
admission control 
and dynamic 
placement 
optimization to 
satisfy site-level 
management policies
Driver-based 
architecture allows 
interfacing with multiple 
external clouds; supports 
EC2-compatible clouds 
and ElasticHosts
Reservoir 
VEE (virtual 
execution 
environment) 
manager 
interface
Cloud Computing
18   www.computer.org/internet/ IEEE INTERNET COMPUTING
A key feature of OpenNebula’s architecture, 
which we describe more in the next section, is its 
highly modular design, which facilitates integra-
tion with any virtualization platform and third-
party component in the cloud ecosystem, such as 
cloud toolkits, virtual image managers, service 
managers, and VM schedulers. For example, it 
specifies all actions pertaining to setting up a 
VM disk image (transferring the image, install-
ing software on it, and so on) in terms of well-
defined hooks. Although OpenNebula includes a 
default “transfer manager” that uses these hooks, 
it can also leverage existing transfer managers 
or VM image contextualizers just by writing 
code that interfaces between the hooks and the 
third-party software.
The Haizea project — which we developed 
independently from OpenNebula — was the first 
to leverage such an architecture in a way that 
was beneficial to both projects. Haizea origi-
nally could simulate VM scheduling only for 
research purposes, but we modified it to act as 
a drop-in replacement for OpenNebula’s default 
scheduler, with few changes required in the 
Haizea code and none in the OpenNebula code. 
By working together, OpenNebula could offer 
resource leases as a fundamental provisioning 
abstraction, and Haizea could operate with real 
hardware through OpenNebula.
In fact, integrating OpenNebula and Haizea 
provides the only VI management solution of-
fering advance reservation of capacity. As Ta-
ble 1 shows, other VI managers use immediate 
provisioning or best-effort provisioning, which 
we discuss in more detail later. However, pri-
vate clouds — specifically those with limited 
resources in which not all requests are satisfi-
able immediately owing to lack of resources — 
stand to benefit from more sophisticated VM 
placement strategies supporting queues, priori-
ties, and ARs. Additionally, service provision-
ing clouds, such as the one being developed in 
the Reservoir project, have requirements that 
are insupportable with only an immediate pro-
visioning model — for example, they need ca-
pacity reservations at specific times to meet 
service-level agreements (SLAs) or peak capac-
ity requirements. 
The OpenNebula Architecture
The OpenNebula architecture (see Figure 2) en-
compasses several components specialized in 
different aspects of VI management. 
To control a VM’s life cycle, the Open-
Nebula core orchestrates three different man-
agement areas: image and storage technologies 
(that is, virtual appliance tools or distributed 
file systems) for preparing disk images for 
Local infrastructure
SchedulerOpenNebula Core
OpenNebula
libvirt interface Cloud interfaceCommand-line interface
External cloud
• EC2
• ElasticHosts
Storage
• NFS
• CSP
• . . .
Network
• Fixed IPs
• Range of IPs
Virtualization
• Xen
• KVM
• VMWare
Drivers
External cloud
Figure 2. OpenNebula virtual infrastructure engine components. By using a driver-based architecture, OpenNebula can 
be integrated with multiple virtual machine managers, transfer managers, and external cloud providers.
SEPTEMBER/OCTOBER 2009 19
Virtual Infrastructure Management
VMs, the network fabric (such as Dynamic 
Host Configuration Protocol [DHCP] servers, 
firewalls, or switches) for providing VMs with 
a virtual network environment, and the un-
derlying hypervisors for creating and control-
ling VMs. The core performs specific storage, 
network, or virtualization operations through 
pluggable drivers. Thus, OpenNebula isn’t tied 
to any specific environment, providing a uni-
form management layer regardless of the un-
derlying infrastructure.
Besides managing individual VMs’ life cy-
cle, we also designed the core to support servic-
es deployment; such services typically include 
a set of interrelated components (for example, 
a Web server and database back end) requiring 
several VMs. Thus, we can treat a group of re-
lated VMs as a first-class entity in OpenNebula. 
Besides managing the VMs as a unit, the core 
also handles the delivery of context informa-
tion (such as the Web server’s IP address, digital 
certificates, and software licenses) to the VMs.
A separate scheduler component makes VM 
placement decisions. More specifically, the 
scheduler has access to information on all re-
quests OpenNebula receives and, based on these 
requests, keeps track of current and future al-
locations, creating and updating a resource 
schedule and sending the appropriate deploy-
ment commands to the OpenNebula core. The 
OpenNebula default scheduler provides a rank 
scheduling policy that places VMs on physical 
resources according to a ranking algorithm that 
the administrator can configure. It relies on 
real- time data from both the running VMs and 
available physical resources.
OpenNebula offers management  interfaces to 
integrate the core’s functionality within other 
data center management tools, such as account-
ing or monitoring frameworks. To this end, 
OpenNebula implements the libvirt API (http://
libvirt.org), an open interface for VM man-
agement, as well as a command line interface 
(CLI). Additionally, a subset of this functional-
ity is exposed to external users through a cloud 
interface.
Finally, OpenNebula can support a hybrid 
cloud model by using cloud drivers to inter-
face with external clouds. This lets organiza-
tions supplement the local infrastructure with 
computing capacity from a public cloud to 
meet peak demands, better serve their access 
requests (for example, by moving the service 
closer to the user), or implement high availabil-
ity strategies. OpenNebula currently includes 
an EC2 driver, which can submit requests to 
Amazon EC2 and Eucalyptus, as well as an 
ElasticHosts driver.
The Haizea Lease Manager
Haizea is an open source resource lease manager 
and can act as a VM scheduler for OpenNebula 
or be used on its own as a simulator to evalu-
ate different scheduling strategies’ performance 
over time. The fundamental resource provision-
ing abstraction in Haizea is the lease. Intuitive-
ly, a lease is a form of contract in which one 
party agrees to provide a set of resources (an 
apartment, a car, and so on) to another. When 
a user wants to request computational resources 
from Haizea, it does so in the form of a lease; 
the leases are then implemented as VMs man-
aged by OpenNebula. The lease terms Haizea 
supports include hardware resources, software 
environments, and the period during which the 
hardware and software resources must be avail-
able. Currently, Haizea supports AR leases in 
www.cisco.com
Cisco Systems, Inc. is accepting resumes 
for the following position in:
Boxborough, MA
Customer Support Engineer  
(Ref#: BOX7)    
Responsible for providing technical support 
regarding the company’s proprietary systems and 
software to field engineers, technicians, product 
support and company customers who are diagnosing, 
troubleshooting, repairing and debugging complex 
electro/mechanical equipment, computer systems 
and/or complex software.
  
Please mail resumes with reference number to Cisco 
Systems, Inc., Attn: J51W, 170 W. Tasman Drive, Mail 
Stop: SJC 5/1/4, San Jose, CA 95134.  No phone calls 
please.  Must be legally authorized to work in the 
U.S. without sponsorship.  EOE.  
Cloud Computing
20   www.computer.org/internet/ IEEE INTERNET COMPUTING
which resources must be available at a specific 
time; best-effort leases, in which resources are 
provisioned as soon as possible, and requests 
are placed in a queue, if necessary; and immedi-
ate leases, in which resources are provisioned 
when requested or not at all.
To satisfy use cases where resources must 
be guaranteed to be available at certain times, 
various researchers have studied advance res-
ervation of computational resources in the con-
text of parallel computing.4–6 In the absence 
of suspension/resumption capabilities, this ap-
proach produces resource underutilization due 
to the need to vacate resources before an AR 
starts. By using VMs to implement leases, an 
organization can support ARs more efficient-
ly1,7 through resource preemption, suspending 
the VMs of lower-priority leases before a res-
ervation starts, resuming them after the reser-
vation ends, and potentially migrating them to 
other available nodes or even other clouds. We 
can do this without having to make the applica-
tions inside the VM aware that they’re going to 
be suspended, resumed, or even migrated. How-
ever, using VMs introduces runtime overhead, 
which poses additional scheduling challenges, 
as does the preparation overhead of deploying 
the VM disk images that the lease needs. Such 
overheads can noticeably affect performance if 
they aren’t adequately managed.8 Haizea’s ap-
proach is to separately schedule preparation 
overhead rather than assuming it should just 
be deducted from a user’s allocation. However, 
this is complicated when Haizea must support 
multiple lease types with conflicting require-
ments that it has to reconcile — for example, the 
transfers for a lease starting at 2 p.m. could re-
quire delaying transfers for best-effort leases, 
resulting in longer wait times. Haizea uses sev-
eral optimizations, such as reusing disk images 
across leases, to minimize preparation over-
head’s impact. Similarly, Haizea also schedules 
the runtime overhead of suspending, resuming, 
and migrating VMs.
Haizea bases its scheduling on a resource 
slot table that represents all the physical nodes 
it manages over time. It schedules best-effort 
leases using a first-come-first-serve queue 
with backfilling (a common optimization in 
queue-based systems), whereas AR leases use 
a greedy algorithm to select physical resources 
that minimize the number of preemptions. Al-
though the resource selection algorithm is cur-
rently hardcoded, future versions will include 
a policy decision module to let developers 
specify their own resource selection policies 
(for instance, policies to prioritize leases based 
on user, group, project, and so on). This policy 
decision module will also specify the condi-
tions under which Haizea should accept or re-
ject a lease.
Experiences with  
OpenNebula and Haizea
Although OpenNebula and Haizea both origi-
nated in research projects, one of our goals is 
to produce production-quality releases that 
meet other communities’ needs. In fact, we feel 
strongly about using a development model that, 
first and foremost, produces stable software 
suitable for production environments, which we 
can also use for our own research, incorporat-
ing the results into the next stable version. This 
lets us support VI users’ requirements while in-
corporating novel techniques and solutions into 
our releases. OpenNebula has already seen sev-
eral stable releases and has a growing user base 
through its inclusion in the popular  Ubuntu 
GNU/Linux distribution (www.ubuntu.com), 
starting with Ubuntu 9.04 (“Jaunty Jackalope”). 
Our first-hand experiences with OpenNebula 
have mostly occurred in the EU Reservoir proj-
ect, where we are enhancing OpenNebula to 
meet the requirements of several business use 
cases.9 In recent work, we’ve shown  OpenNebula 
to be effective in managing clustered services, 
using it to deploy and manage the back-end 
nodes of a Sun Grid Engine compute cluster and 
an nginx Web server10 on both local resources 
and an external cloud.
Other integration efforts with OpenNebula 
are currently under way (see http://opennebula.
org/doku.php?id=ecosystem), including an im-
plementation of the libvirt interface and a VM 
consolidation scheduler designed to minimize 
energy consumption. The Reservoir project 
is also developing other tools around Open-
Nebula for service elasticity management, VM 
placement to meet SLA commitments, support 
for public cloud interfaces, and a VM sched-
uler (termed policy engine within the project) 
that adds support for policy-driven probabilis-
tic admission control and dynamic placement 
optimization to satisfy site-level management 
policies. We’ve also experimented with inte-
grating OpenNebula with Globus Nimbus.
SEPTEMBER/OCTOBER 2009 21
Virtual Infrastructure Management
Haizea is still in a “technology preview” 
stage, although we plan a first stable release 
later in 2009. In previous joint work with Kate 
Keahey (Argonne National Laboratory),1 we 
used Haizea to simulate 72 30-day workloads 
in six different configurations, or 36 years 
of lease scheduling, producing experimental 
results showing that, when using workloads 
that combine best-effort and AR requests, a 
VM-based approach with suspend/resume can 
overcome the utilization problems typical-
ly associated with AR use. More specifically, 
when measuring the total time required to pro-
cess all the requests in the workload, we found 
that a VM-based approach performed consis-
tently better (up to 32.97 percent), despite the 
overhead required to use VMs. Our results also 
showed that we can minimize VMs’ prepara-
tion overhead — in the form of transferring 
VM disk images from a repository — using 
image transfer scheduling and caching strat-
egies. In more recent work,11,12 we’ve used 
OpenNebula and Haizea together to perform 
experiments on a physical testbed and develop 
a resource model for predicting the runtime 
overhead of suspending/resuming VMs under 
various configurations. We found that, similar 
to scheduling preparation overhead, explicitly 
and separately scheduling suspensions and re-
sumptions is necessary to avoid unnecessary 
delays in leases (for example, if a lease must be 
suspended to make way for a higher-priority 
one, such as an AR). Furthermore, we found 
that accurately estimating the time to sus-
pend and resume leases depends on a variety 
of factors that we must take into account when 
scheduling them.
A s interest in private and hybrid IaaS clouds grows, so will the need for a diverse eco-
system of tools and technologies that can be 
used as building blocks to create and manage 
them. Although some solutions have emerged 
across three broad categories — cloud, VI, and 
VM management — the challenge ahead will be 
integrating multiple components to create com-
plete IaaS cloud-building solutions. 
Private and hybrid clouds will also face the 
challenge of efficiently managing finite re-
sources. However, existing VI managers rely 
on an immediate resource provisioning that 
implicitly assumes that capacity is practically 
infinite. Although this is a fair assumption for 
large cloud providers, such as Amazon EC2, it’s 
not applicable to smaller providers where the 
likelihood of being overloaded is greater. To sat-
isfy SLAs, requests for resources will inevitably 
have to be prioritized, queued, pre-reserved, de-
ployed on external clouds, or even rejected, and 
VI management solutions with these capabili-
ties will be required.
OpenNebula and Haizea address these two 
challenges. By relying on a flexible, open, and 
loosely coupled architecture, OpenNebula is de-
signed from the outset to be easy to integrate 
with other components, such as the Haizea lease 
manager. When used together, OpenNebula and 
Haizea are the only VI management solution 
that provides leasing capabilities beyond imme-
diate provisioning, including best-effort leases 
and advance reservation of capacity. 
Acknowledgments
We gratefully acknowledge the hard work of the Open-
Nebula developers, Javier Fontán and Tino Vázquez. We 
also thank our anonymous reviewers for their insightful 
www.cisco.com
Cisco Systems, Inc. is accepting resumes 
for the following position in:
Herndon, VA
Network Consulting 
Engineer 
(Ref#: HER4)    
Provide consultative, proactive and/or 
reactive support to Company accounts.
Please mail resumes with reference number to Cisco 
Systems, Inc., Attn: J51W, 170 W. Tasman Drive, Mail 
Stop: SJC 5/1/4, San Jose, CA 95134.  No phone calls 
please.  Must be legally authorized to work in the 
U.S. without sponsorship.  EOE.  
Cloud Computing
22   www.computer.org/internet/ IEEE INTERNET COMPUTING
and detailed comments. Development of OpenNebula is 
supported by Consejería de Educación de la Comunidad de 
Madrid, Fondo Europeo de Desarrollo Regional (FEDER), 
and Fondo Social Europeo (FSE), through  BIOGRIDNET Re-
search Program S-0505/TIC/000101; the Ministerio de Edu-
cación y Ciencia through research grant TIN2006-02806; 
and the EU through Reservoir research grant number 
215605. Haizea development is supported by Reservoir, the 
University of Chicago, and the US  Department of Energy 
under contract DE-AC02-06CH11357. Early work on Haizea 
was done in collaboration with Kate Keahey (Argonne 
National Laboratory) and funded by US National Science 
Foundation grant 509408, “Virtual Playgrounds.”
References
1. B. Sotomayor, K. Keahey, and I. Foster, “Combin-
ing Batch Execution and Leasing using Virtual Ma-
chines,” Proc. 17th Int’l Symp. High Performance 
Distributed Computing (HPDC 08), ACM Press, 2008, 
pp. 87–96.
2. K. Keahey et al., “Virtual Workspaces: Achieving Qual-
ity of Service and Quality of Life on the Grid,” Scien-
tific Programming, vol. 13, no. 4, 2005, pp. 265–276.
3. D. Nurmi et al., “The Eucalyptus Open-Source Cloud-
Computing System,” Cloud Computing and Applica-
tions 2008 (CCA 08), 2008; www.cca08.org/papers.php.
4. I. Foster et al., “A Distributed Resource Management 
Architecture that Supports Advance Reservations and 
Co-Allocation,” Proc. Int’l Workshop on Quality of Ser-
vice, IEEE Press, 1999, pp. 27–36.
5. W. Smith, I. Foster, and V. Taylor, “Scheduling with 
Advanced Reservations,” Proc. 14th Int’l Symp. Parallel 
and Distributed Processing (IPDPS 00), IEEE CS Press, 
2000, p. 127.
6. Q. Snell et al., “The Performance Impact of Advance 
Reservation Meta-Scheduling,” Proc. Workshop Job 
Scheduling Strategies for Parallel Processing (IPDPS 00/
JSSPP 00), Springer-Verlag, 2000, pp. 137–153.
7. B. Sotomayor et al., “Enabling Cost-Effective Resource 
Leases with Virtual Machines,” Hot Topics Session in 
ACM/IEEE Int’l Symp. High-Performance Distributed 
Computing 2007 (HPDC 07), 2007; http://workspace.
globus.org/papers/HPDC2007_hottopic_Sotomayor 
KeaheyFosterFreeman.pdf.
8. B. Sotomayor, K. Keahey, and I. Foster, “Overhead 
Matters: A Model for Virtual Resource Management,” 
Proc. 1st Int’l Workshop on Virtualization Technology 
in Distributed Computing (VTDC 06), IEEE CS Press, 
2006, p. 5.
9. B. Rochwerger et al., “The Reservoir Model and Ar-
chitecture for Open Federated Cloud Computing,” IBM 
Systems J., Oct. 2008.
10. R. Moreno-Vozmediano, R. Montero, and I. Llorente, 
“Elastic Management of Cluster-Based Services in the 
Cloud,”  Proc. 1st Workshop on Automated Control for 
Datacenters and Clouds (ACDC 09), ACM Press, 2009, 
pp. 19–24.
11. B. Sotomayor et al., “Capacity Leasing in Cloud Sys-
tems using the OpenNebula Engine,” Proc. Cloud Com-
puting and Applications 2008 (CCA 08), 2008; www.
cca08.org/papers.php.
12. B. Sotomayor et al., “Resource Leasing and the Art of 
Suspending Virtual Machines,”  Proc. 11th IEEE Int’l 
Conf. High-Performance Computing and Communica-
tions (HPCC 09), IEEE Press, 2009, pp. 59–68.
Borja Sotomayor is a PhD candidate in the Department of 
Computer Science at the University of Chicago. His 
research interests include resource provisioning and 
scheduling, distributed systems, and virtualization. 
Sotomayor has an MSc in computer science from the 
University of Chicago, and a computer engineering de-
gree from the University of Deusto, Bilbao, Spain. Con-
tact him at borja@cs.uchicago.edu.
Rubén S. Montero is an associate professor in the Depart-
ment of Computer Architecture at the Complutense 
University of Madrid. His research interests lie mainly 
in resource-provisioning models for distributed sys-
tems, in particular, grid resource management and 
scheduling, distributed management of virtual ma-
chines, and cloud computing. Montero has a PhD in 
physics from Complutense University. Contact him at 
rubensm@dacya.ucm.es.
Ignacio M. Llorente is a full professor and the head of the 
Distributed Systems Architecture Research group at 
the Complutense University of Madrid. His research 
interests include advanced distributed computing and 
virtualization technologies, architecture of large-scale 
distributed infrastructures, and resource-provisioning 
platforms. Llorente has a PhD in computer science from 
the Complutense University of Madrid and an Execu-
tive Master in business administration from the In-
stituto de Empresa Business School. Contact him at 
llorente@dacya.ucm.es. 
Ian Foster is director of the Computation Institute at the 
University of Chicago and Argonne National Labo-
ratory and the Arthur Holly Compton Distinguished 
Service Professor of computer science at the Uni-
versity of Chicago. His research interests include 
distributed computing, parallel computing, and com-
putational science. Foster has a PhD in computer sci-
ence from Imperial College, London. Contact him at 
foster@anl.gov.
For access to more content from the IEEE Computer Society, 
see computingnow.computer.org.
This article was featured in
Top articles, podcasts, and more.
computingnow.computer.org

