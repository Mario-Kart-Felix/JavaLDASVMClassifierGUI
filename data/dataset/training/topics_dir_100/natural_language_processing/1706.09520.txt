Neural SLAM
Jingwei Zhang
University of Freiburg,Germany
zhang@informatik.uni-freiburg.de
Lei Tai
City University of Hong Kong, HK
lei.tai@my.cityu.edu.hk
Joschka Boedecker Wolfram Burgard
University of Freiburg, Germany
{jboedeck,burgard}@informatik.uni-freiburg.de
Ming Liu
HKUST, HK
eelium@ust.hk
Abstract: We present an approach for agents to learn representations of a global
map from sensor data, to aid their exploration in new environments. To achieve
this, we embed procedures mimicking that of traditional Simultaneous Localiza-
tion and Mapping (SLAM) into the soft attention based addressing of external
memory architectures, in which the external memory acts as an internal represen-
tation of the environment. This structure encourages the evolution of SLAM-like
behaviors inside a completely differentiable deep neural network. We show that
this approach can help reinforcement learning agents to successfully explore new
environments where long-term memory is essential. We validate our approach in
both challenging grid-world environments and preliminary Gazebo experiments.
Keywords: External Memory, Exploration, SLAM, Deep Reinforcement Learn-
ing
1 Introduction
1.1 Cognitive Mapping
Studies of animal navigation have shown that the hippocampus plays an important part [16] [12] [3],
as it performs cognitive mapping that combines path integration and visual landmarks, so as to give
the animals sophisticated navigation capabilities instead of just reflexive behaviors based only upon
the immediate information they perceive.
Similarly, to successfully navigate and explore new environments in a timely fashion, intelligent
agents would benefit from having their own internal representation of the environment whilst their
traversal, so as to go beyond the scope of performing reactive actions based on its most recent
sensory input. Traditional methods in robotics thus develop a series of methods like Simultaneous
Localization and Mapping (SLAM), localization in a given map, path planning and motion control,
to enable robots to complete such challenging tasks [21] [11] [10]. Those individual components are
well studied and understood as separate parts, but here we view them as a unified system and attempt
to embed SLAM-like procedures into a neural network, such that SLAM-like behaviors could evolve
out of the course of reinforcement learning agents exploring new environments. This guided learned
system could then benefit from its each individual component (localization, mapping and planning)
adapting in the awareness of each other’s existence, instead of being rigidly combined together as
in traditional methods. Also in this paper we represent this system using a completely differentiable
deep neural network, ensuring the learned representation is distributed and feature-rich, a property
that rarely comes with traditional methods but key to robust and adaptive systems [1].
1.2 External Memory
The memory structure in traditional Recurrent Neural Networks (RNNs) like Long Short Term
Memory Networks (LSTMs) are ultimately short-term. In the context of robots exploring new envi-
ronments, this could refer to the agent remembering that it has been traveling through a long corridor,
but forgetting what it has perceived before entering this corridor, which would not be sufficient for
1st Conference on Robot Learning (CoRL 2017), Mountain View, United States.
ar
X
iv
:1
70
6.
09
52
0v
1 
 [
cs
.L
G
] 
 2
9 
Ju
n 
20
17
developing informative navigation or exploration strategies. For the network to have an internal
representation of the environment, i.e., its own cognitive map, an external memory as proposed in
[5] [6] is required. Having an external memory besides a deep network separates the learning of
computation algorithms from the storage of data over long time-scales. This is essential for learn-
ing successful exploration strategies, since if the computation and the data are mixed together in
the weights of the network, then with the memory demands increasing over time, the expressive
capacity of the network would be very likely to decrease [6].
Besides [5] [6], there is another branch of work on external memory architectures for deep networks
called the Memory Networks. But the Memory Networks as in [15] [18] do not learn what to write
to the memory, which is not sufficient for our task since the network is expected to learn to map onto
its external memory to aid planning.
The Neural Map as proposed in [17] adapts the 1D external memory in [5] to 2D as a structured
map storage for an agent to learn to navigate. However, they are not utilizing the 2D structure of
this memory as all their operations can be conducted as if the memory address were a 1D vector.
Furthermore, they assume the location of the agent is always known so as to write exactly to the
corresponding location in memory while the agent travels through the maze, a prerequisite that can
rarely be met in real-world scenarios.
1.3 Embedding Classic Models into Deep Neural Networks
Embedding domain-specific structures into neural networks can be found in many works. This
line of formulation does not treat the networks completely as black-box approximators that do not
benefiting from the valuable prior knowledge we accumulate over the years (it is like forcing a boy
to deduct all the laws of physics from his observations by himself but not giving him the physics
textbook to learn from), but bias it towards learning representations containing the structures that
we already know it would benefit from having for specific domains.
Tamar et al. [20] embeds the value iteration procedures into a single network, forcing the network to
learn representations following the well-defined policy-evaluation, policy-improvement loop, while
benefiting from the feature-rich representation from deep architectures. Gupta et al. [7] goes one
step further by using the Value Iteration Network as the planning module inside a visual navigation
system. They treat an internal part of the network as an egocentric map and applies motion on it.
The FlowNet [4] adds a cross-correlation layer to compute correlations of features of corresponding
neighboring cells between subsequent frames, which explicitly provides the network with matching
capabilities. This greatly helps the learning of optical flow since the optical flow is computed based
on local pixel dynamics. Zhang et al. [22] forces the network to learn representative features across
tasks by explicitly embedding structures mimicking the computation procedures of successor feature
reinforcement learning into the network, their resulting architecture is able to transfer navigation
policies across similar environments.
Traditionally, when using well-established models in a combined system with some other mod-
ules, they typically do not benefit from the other components, since their behaviors can not adapt
accordingly, as those models come out of deduction but are not evolved out of learning (directly
applying those well established traditional models is like to directly give the boy all the answers
to his physics questions instead of giving him the physics textbook for him to learn to solve those
questions). While if those functionalities are learned along with other components, their behav-
iors can influence each other and the system could potentially obtain performance beyond directly
combining well-established models.
Let’s take SLAM as an example. SLAM is used as a building block for complicated autonomous
systems to aid navigation and exploration, yet the SLAM model and the path planning algorithms are
individually developed, not taking each other into account. Bhatti et al. [2] augment the state space
of their reinforcement learning agent with the output of a traditional SLAM algorithm. Although this
improves the navigation performance of the agent, it still falls into the issues discussed above since
SLAM is rigidly combined into their architecture. While if SLAM-like behaviors can be encouraged
to evolve out of the process of agents learning to navigate or to explore, then the resulting system
would be much more deeply integrated as a whole, with each individual component influencing each
other while benefiting from learning alongside each other. The SLAM model from the resulting
system would be evolved out of the need for exploration or navigation, not purely just for doing
2
SLAM. Also if learning with deep neural nets, the resulting models will be naturally feature-rich,
which is rarely a property of traditional well-established models.
Although an increasing amount of works have been done on utilizing deep reinforcement learning
algorithms for autonomous navigation [13] [23] [22] [7] [19], none of them have an explicit external
memory architecture to equip the agent with capabilities of making long-term decisions based on an
internal representation of a global map. Also, those work mainly focus on learning to navigate to a
target location, while in this paper we attempt to solve a more challenging task of learning to explore
new environments under a time constraint, in which effective memory mechanism is essential.
Following these observations, we attempt to embed the motion prediction step and the measurement
update step of SLAM into our network architecture, by utilizing the soft attention based addressing
mechanism in [5], biasing the write/read operations towards traditional SLAM procedures and treat
the external memory as an internal representation of the map of the environment, and train this model
using deep reinforcement learning algorithms, to encourage the evolve of SLAM-like behaviors
during the course of exploration.
2 Methods
2.1 Background
We formulate the exploration task as a Markov Decision Process (MDP) in which the agent interacts
with the environment through a sequence of observations, actions and rewards. At each time step
t ∈ [0, T ] the agent receives an observation zt (in this paper the agent receives a vector of laser
ranges) of the true state st of the environment. The agent then selects an action at based on a policy
π(at|st), which corresponds to a motion command ut for the agent to execute. The agent then
receives a reward signal Rt and transits to the next state st+1. The goal for the agent is to maximize
the cumulative expected future reward (γ ∈ (0, 1] is the discount factor):
V π(s) = Eπ
[ ∞∑
t=0
γtRt|s0 = s
]
(1)
Recent success in deep reinforcement learning represents the value functions or the policies with
deep neural networks. In this paper we utilize the Asynchronous Advantage Actor-Critic (A3C)
algorithm [14], in which both the policy and value functions are represented by deep neural function
approximators, parameterized by θπ and θV respectively (we note that θπ and θV share parameters
except for their output layers, parameterized by Wπ and WV (Sec. 2.4)). Those parameters are
updated using the following gradients (Gt = γKV (sK) +
∑K
τ=0 γ
τRτ , with K being the rollout
step) (H is the entropy of the policy, λ is the coefficient of the entropy regularization term):
dθπ = ∇θπ log π(at|st; θπ)(Gt − V (st; θV )) + λ∇θπH(π(at|st; θπ)) (2)
dθV = ∂(G
t − V (st; θV ))2/∂θV (3)
2.2 Neural SLAM Architecture
As discussed before, we require our model to have an external memory structure for the agent to
utilize as an internal representation of the environment. Thus we added an external memory chunk
M of sizeH×W ×C (containingH×W memory slots, with C channels or features for each slot),
which can be accessed by the network via a write head and a read head (we note that our work can
be easily extended to multiple heads for write/read, but in this paper we only investigate with one
write/read head) (we also observe that the number of heads can be viewed as the number of particles
as in particle filter [21]).
At each time step, we feed our input directly to an LSTM cell, which gives out a hidden state ht.
This hidden state ht is then used in each head to emit a set of control variables {kt, βt, gt, ρt, ζt}
(each write head additionally emits {et, at}) through a set of linear layers. The write head and the
read head would then each compute their access weight (wtw and w
t
r, both of size H ×W ) based on
3
st LSTM ht
Data
Association
kt
 t
Measurement
Update
gt
⇢t
⇣t
Mapping
et
wtr
wtw
⇡t V t at
rt-1
M
Planning
rt
Motion
Prediction
Localization
&
Figure 1: Visualization of the Neural-SLAM model architecture (we intentionally use blue for the
components in charge of computation, green for memory, and cyan for a mixture of both.)
those control variables. Then the write head would use its access weight wtw along with e
t and at
to write to the memoryMt−1, while the read head would access the updated memoryMt with its
access weight wtr to output a read vector r
t. ht and rt are then concatenated together to compute the
final output: a policy distribution πt, and an estimated value V t, which are then used to calculate
gradients to update the whole model according to Equ. 2 and Equ. 3.
The Neural-SLAM Model Architecture is shown in Fig. 1, and we will describe the operations in
each component in detail in the following section.
2.3 Embedded SLAM Structure
We use the same addressing mechanism for computing the access weights of the write head wtw
and the read head wtr, except that the read head addressing happens after the write head updates the
external memory, thus would access the memory of the current time step. Below we describe the
computations in detail where we refer to both access weights at time step t as wt.
2.3.1 Prior Belief
We view the access weights of the heads as their current beliefs. We make the assumption that the
initial pose of the agent is known at the beginning of each episode. Also, the sensing range of its
onboard sensor is known a priori. Then we initialize the access weight w0 with a Gaussian kernel
centering around the initial pose, filling up the whole sensing area and summing up to 1; all other
areas are assigned with weight 0 (Fig. 4). The external memory is initialized with 0: M0 = 0 (we
discuss in more detail of this choice of initialization value in Sec. 2.5).
2.3.2 Localization & Motion Prediction
At each time step, we first do a motion prediction, by applying the motion command the agent
receives from the last time step ut−1 onto its access weight from the last time step (M here can be
any motion model):
w̄t = M(wt−1, ut−1) (4)
Note that since we view our external memory not as an egocentric map but as a global map, we need
to first localize on the access weight before we can apply this motion model. We thus localize by
first identifying the center of mass in the current access weight matrix as the position of the agent,
then choose the direction with the largest sum of weights within the corresponding sensing area as
its orientation.
2.3.3 Data Association
Each head emits a key vector kt of length C, which is used to compare with each slot (x, y) in the
external memoryMt(x,y) under a similarity measure S, to compute a content-based access weight
4
wtc based on the data association score (each head also outputs a key strength scalar β
t to increase
or decrease the focus of attention):
wtc(x, y) =
exp(βtS(kt,Mt(x,y)))∑
(i,j) exp (β
tS(kt,Mt(i,j)))
(5)
where in this paper we use the cosine similarity for S:
S[u,v] = u · v‖u‖ · ‖v‖ (6)
2.3.4 Measurement Update
We then do a measurement update by the following steps.
First, the content-based access weight from this time step wtc and the last access weight after motion
prediction w̄t are interpolated together using a interpolation gate scalar gt generated by each head:
wtg = g
twtc + (1− gt)w̄t (7)
Then a shift operation is applied based on the shift kernel ρt emitted by each head (in this paper ρ
defines a normalized distribution over a 3× 3 area), to account for the noise in motion and measure-
ment; this shift operation can be viewed as a convolution over the access weight matrices, with ρt
being the convolution kernel:
wtρ(x, y) =
H−1∑
i=0
W−1∑
j=0
wtg(i, j)ρ
t(x− i, y − j) (8)
Finally, the smoothing effect of the shift operation is compensated with a sharpen scalar ζt >= 1:
wt(x, y) =
wtρ(x, y)
ζt∑
(i,j) w
t
ρ(i, j)
ζt
(9)
2.3.5 Mapping
The write head each generates two more vectors additionally (both contain C elements): an erase
vector et and an add vector at. Along with its access weight wtw, the write head access and update
the memory with the following operations:
M̃t(x,y) =Mt−1(x,y)(1− wtwet) (10)
Mt(x,y) = M̃t(x,y) + wtwat (11)
2.4 Planning
After the memory has been updated toMt, the read head access it by its access weight wtr to output
a read vector rt (which can be seen as a summary of the current internal map):
rt =
∑
(i,j)
wtr(i, j)Mt(i,j) (12)
This read vector rt is then concatenated with the hidden state ht and fed into two linear layers (pa-
rameterized byWπ andWV respectively) to give out the policy distribution and the value estimate:
πt(at|st) = Softmax(Wπ[ht, rt]) (13)
V t =Wv[ht, rt] (14)
5
πt and V t are then used in for calculating losses for on-policy deep reinforcement learning as dis-
cussed above in Section 2.1. An action at is then drawn from a multinomial distribution defined by
πt during training, while a greedy action is taken during evaluation and testing.
2.5 Read-out Map from External Memory
As mentioned before, we see the external memoryM as an internal representation of the environ-
ment for the agent. More specifically, we treat the values stored onM as the log odds representation
of occupancy in occupancy grid mapping techniques [21], then from this representation we can re-
cover the occupancy probability of all the grids (i.e., the slots onM) following the equation below:
p(Mt(x,y)) = 1−
1
1 + exp {Mt(x,y)}
(15)
At the beginning of each episode, we set all the values inM0 to 0, corresponding to an occupancy
probability of 0.5, to represent maximum uncertainty. We identify that Equ. 15 is identical to a
Sigmoid operation, thus Sigmoid is used in our implementation for this map read-out operation.
Following this formulation, one possible extension for our method would be to useM to compute
the exploration reward Rexp as an internal reward signal for the agent, to eliminate the need of
receiving Rexp from the ground truth map (for example, use the information gain fromMt−1 toMt
as Rexp) (we refer to Sec. 3.1 for a detailed description of our reward structure).
3 Experiments
3.1 Experimental Setup
We first test our algorithm in simulated grid world environments. We use a curriculum learning
strategy to train the agent to explore randomly generated environments ranging from sizes of 8× 8
to 12 × 12 (we ensure all the free grids are connected together when generating environments). At
the beginning of each episode, the agent is randomly placed in a randomly generated grid world
during both training and evaluation. It has a sensing area of size 3 × 5 (we note that this simulated
laser sensor cannot see through walls nor across sharp angles) Fig. 2.
Figure 2: Visualization of a sample trajectory of a trained Neural-SLAM agent successfully complet-
ing exploration in a new environment. The agent is visualized as a grey grid with a black rectangle
at its center pointing at its current orientation. The obstacles are shown as black grids, free space as
white grids, and grey grids indicate unexplored areas. The world clears up as the agent explores with
its sensor, with its sensing area shown in red bounding boxes (the information in the red bounding
box is the input to the network). An exploration is completed when the agent has cleared up all pos-
sible grids, in which case the current episode is considered as terminated and solved. The episode
would also be terminated (but not considered as solved) when a maximum step of 750 is reached.
The agent can take an action out of {0 : Stand still, 1 : Turn Left, 2 : Turn Right, 3 : Go Straight}.
It receives a reward of −0.04 for each step it takes before completing the exploration task, −0.96
for colliding with obstacles, and 10. for the completion of an exploration. Also during the course of
its exploration, it receives a reward of 1./(3× 5) for each new grid it clears up.
In each time step, the agent receives a sensor reading of size 3×5, which is then fed into the network.
We train the network the same way as in A3C [14] and deploy 16 training processes, optimized with
the ADAM optimizer [8] with a shared momentum across all training processes, with a learning rate
of 1e− 4. We also use a weight decay of 1e− 4 since we find this to be essential for the stability of
6
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Training Steps 1e6
−5.0
−2.5
0.0
2.5
5.0
7.5
10.0
12.5
15.0
A
ve
ra
ge
 R
ew
ar
d
8 × 8 1 0 × 1 0 1 2 × 1 2
A3C
Neural-SLAM without Motion
Neural-SLAM
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Training Steps 1e6
0
50
100
150
200
250
300
350
400
450
N
um
be
r o
f E
pi
so
de
s S
ol
ve
d 
in
 3
00
0 
St
ep
s
8 × 8 1 0 × 1 0 1 2 × 1 2
A3C
Neural-SLAM without Motion
Neural-SLAM
Figure 3: Comparison between the average reward obtained and number of episodes solved in 3000
steps during evaluation by an A3C agent, an A3C agent with external memory (Neural-SLAM with-
out the Localization & Motion Prediction step as discussed in Sec. 2.3.2), and our Neural-SLAM
agent. We train continuously for 3 courses transitioning from world sizes of 8× 8 to 12× 12.
training when combining external memory architectures with A3C. We set the rollout step K to be
20 and the maximum number of steps for each episode to be 750.
We experimented with two baseline setups as comparisons for our Neural-SLAM agent. One is an
A3C agent with 128 LSTM units without external memory architectures. Another one uses the same
setting, except that it interacts with a 2D addressed external memory (12 × 12 × 32), and access
it using the same approach as we described in Sec. 2.3. But unlike our Neural-SLAM agent, no
motion prediction step is executed on the access weights as described in Sec. 2.3.2.
3.2 Grid World Experiments
We conducted experiments in the simulated grid world environment, training the two baseline agents
and our Neural-SLAM agent continuously over a curriculum of 3 courses. We observe that our
Neural-SLAM agent shows a relatively consistent performance across all 3 courses (Fig. 3). Specif-
ically, our agent can still successfully and reliably explore in the 3rd course where the environments
contain more complex structures that effective long-term memory is essential. We visualize the
memory addressing in Fig. 4, and observe that ww converges to a more focused attention to center
around the current pose of the agent, while wr learns to spread out over the entire world area, so
that the resulting read vector r could summarize the current memory for the agent to make planning
decisions. We note that the memory and the weights for write/read heads are all initialized to the
size of the last course which is 12 × 12, yet the agent is able to constrain its writing and reading to
the correct map size that it is currently traveling on, which is 8× 8 (within red bounding boxes).
7
3.3 Gazebo Experiments
We also experimented with a simple 3D world built in Gazebo [9]. We used a slightly different
reward structure: −0.005 as a step cost, −0.05 for collision, 1 for the completion of an exploration,
and the exploration reward is scaled down by 0.1 compared to the grid world experiments. We deploy
24 learners using docker for training our Neural-SLAM agent. From the experimental results shown
in Fig. 5, we can see that our Neural-SLAM agent is able to solve the task effectively.
(a) World views
(b) Write Head Weights ww
(c) Memory M (we note that this visualization is the output of a map read-out operation (Sec. 2.5))
(d) Read Head Weights wr
Figure 4: Visualization of the world view, write head weights ww, memory M, and read head
weights wr during one exploration of our trained Neural-SLAM agent on a simplest environment.
0.0 0.5 1.0 1.5 2.0 2.5
Training Steps 1e5
−15
−10
−5
0
A
ve
ra
ge
 R
ew
ar
d
Neural-SLAM
Figure 5: Gazebo Experiments (rollout step K: 50; maximum steps per episode: 2500).
8
4 Conclusions and Future Work
We propose an approach to provide deep reinforcement learning agents with long-term memory ca-
pabilities by utilizing external memory access mechanisms. We embed SLAM-like procedures into
the soft attention based addressing to bias the write/read operations towards SLAM-like behaviors.
Our method provides the agent with an internal representation of the environment, so as to guide it
to make informative planning decisions to effectively explore new environments. Several interesting
extensions could emerge from our work, including the internal reward as discussed in Sec. 2.5, to
evaluate our approach on more challenging environments, to conduct real-world experiments, and
to experiment with higher dimensional inputs.
References
[1] Y. Bengio. Deep learning of representations: Looking forward. In International Conference
on Statistical Language and Speech Processing, pages 1–37. Springer, 2013.
[2] S. Bhatti, A. Desmaison, O. Miksik, N. Nardelli, N. Siddharth, and P. H. Torr. Playing doom
with slam-augmented deep reinforcement learning. arXiv preprint arXiv:1612.00380, 2016.
[3] T. S. Collett and P. Graham. Animal navigation: path integration, visual landmarks and cogni-
tive maps. Current Biology, 14(12):R475–R477, 2004.
[4] P. Fischer, A. Dosovitskiy, E. Ilg, P. Häusser, C. Hazırbaş, V. Golkov, P. van der Smagt, D. Cre-
mers, and T. Brox. Flownet: Learning optical flow with convolutional networks. arXiv preprint
arXiv:1504.06852, 2015.
[5] A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv preprint
arXiv:1410.5401, 2014.
[6] A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwińska, S. G.
Colmenarejo, E. Grefenstette, T. Ramalho, J. Agapiou, et al. Hybrid computing using a neural
network with dynamic external memory. Nature, 538(7626):471–476, 2016.
[7] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik. Cognitive mapping and planning
for visual navigation. arXiv preprint arXiv:1702.03920, 2017.
[8] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proc. of the Interna-
tional Conference on Learning Representations (ICLR), 2015.
[9] N. Koenig and A. Howard. Design and use paradigms for gazebo, an open-source multi-robot
simulator. In Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ
International Conference on, volume 3, pages 2149–2154. IEEE.
[10] J. Latombe. Robot Motion Planning. Kluwer, 1991.
[11] S. LaValle. Planning Algorithms. Cambridge University Press, 2006.
[12] B. L. McNaughton, F. P. Battaglia, O. Jensen, E. I. Moser, and M.-B. Moser. Path integration
and the neural basis of the’cognitive map’. Nature Reviews Neuroscience, 7(8):663–678, 2006.
[13] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. Ballard, A. Banino, M. Denil, R. Goroshin,
L. Sifre, K. Kavukcuoglu, et al. Learning to navigate in complex environments. arXiv preprint
arXiv:1611.03673, 2016.
[14] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and
K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International
Conference on Machine Learning, pages 1928–1937, 2016.
[15] J. Oh, V. Chockalingam, S. Singh, and H. Lee. Control of memory, active perception, and
action in minecraft. arXiv preprint arXiv:1605.09128, 2016.
[16] J. O’keefe and L. Nadel. The hippocampus as a cognitive map. Oxford: Clarendon Press,
1978.
9
[17] E. Parisotto and R. Salakhutdinov. Neural map: Structured memory for deep reinforcement
learning. arXiv preprint arXiv:1702.08360, 2017.
[18] S. Sukhbaatar, J. Weston, R. Fergus, et al. End-to-end memory networks. In Advances in
neural information processing systems, pages 2440–2448, 2015.
[19] L. Tai, G. Paolo, and M. Liu. Virtual-to-real deep reinforcement learning: Continuous control
of mobile robots for mapless navigation. arXiv preprint arXiv:1703.00420, 2017.
[20] A. Tamar, Y. Wu, G. Thomas, S. Levine, and P. Abbeel. Value iteration networks. In Advances
in Neural Information Processing Systems, pages 2154–2162, 2016.
[21] S. Thrun, W. Burgard, and D. Fox. Probabilistic Robotics. MIT Press, 2005.
[22] J. Zhang, J. T. Springenberg, J. Boedecker, and W. Burgard. Deep reinforcement learn-
ing with successor features for navigation across similar environments. arXiv preprint
arXiv:1612.05533, 2016.
[23] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi. Target-
driven visual navigation in indoor scenes using deep reinforcement learning. arXiv preprint
arXiv:1609.05143, 2016.
10
Supplemental Materials: Neural SLAM
Figure S1: Visualization of randomly generated environments used in our experiments, with sizes
ranging from 8× 8 to 12× 12.
(a) World views
(b) Write Head Weights ww
(c) Memory M (we note that this visualization is the output of a map read-out operation (Sec. 2.5))
(d) Read Head Weights wr
Figure S2: Visualization of the world view, write head weights ww, memory M, and read head
weights wr during the course of one exploration of a trained Neural-SLAM agent without the Lo-
calization & Motion Prediction step as discussed in Sec. 2.3.2 on a simplest environment. Unlike
the addressing mechanism learned by the full Neural-SLAM model as shown in Fig. 4 (with a more
focused write for mapping and a more diffused read to collect information over the entire map), the
agent does not learn a good strategy to utilize its external memory structure. We suspect this could
be due to that, for solving the smaller sized worlds it encountered during its 1st course which con-
tain very few complex structures for navigation and exploration, memory is not essential; while for
the more complex worlds it encountered during the 2nd and 3rd courses, which contain many dead
corners and long corridors, memory on a longer time scale is essential to enable the agent to deploy
complicated exploration strategies as shown in Fig. 2. This could explain its corresponding green
curve in Fig. 3: it learns to solve the 1st course faster than the A3C agent but it does not learn to
use the external memory to guide its exploration properly since no motion model is imposed to bias
its addressing behaviors to that of the SLAM procedures. So in the 2nd and 3rd courses, it does not
adapt as quickly as the full Neural-SLAM agent.
1

