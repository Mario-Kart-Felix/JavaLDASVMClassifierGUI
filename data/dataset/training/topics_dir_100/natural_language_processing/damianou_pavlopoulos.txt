Probabilistic Latent 
Semantic Analysis
Thomas Hofmann
Presentation by
Ioannis Pavlopoulos & Andreas Damianou
for the course of
Data Mining & Exploration
1
Outline
• Latent Semantic Analysis
o Need
o Overview
o Drawbacks
• Probabilistic Latent Semantic Analysis
o Solution to drawbacks of LSA
o Comparison with LSA and document clustering
o Model Construction
• Evaluation of PLSA
2
Need for Latent Semantic Analysis
  
• Applications
o Compare documents in the semantic (concept) space
o Relations between terms
o Compare documents across languages
o Given: Bag of words  Find: matching documents in 
the semantic space
• Problems addressing
o Synonymy
         ex: buy - purchase
o Polysemy
         ex: book (verb) - book (noun) 3
LSA Overview
• Capturing the meaning among words
• Addressing polysemy and synonymy
• Key Idea
o Dimensionality reduction of word-document co-occurence 
matrix
o Construction of  Latent Semantic space
   LSA may classify documents together even if they don’t 
have common words!
Documents Words
Documents Concepts Words
From:
To:
4
LSA Concept
 Singular Value Decomposition (SVD)
 Given N which is the word-document co-occurence 
matrix, compute:
 N = UΣVt where:
− Σ is the diagonal matrix with the singular values 
of N
− U, V two orthogonal matrices
5
LSA
SVD
6
LSA Concept
 Dimensionality Reduction
− Keep the K – largest singular values which show the 
dimensions with the greatest variance between words and 
documents
− Discarding the lowest dimensions is supposed to be 
equivalent to reducing the "noise"
− Terms and documents are converted to points in a K-
Dimensional latent space
 Results do not introduce well defined probabilities 
and thus, are difficult to interpret
7
Probabilistic LSA
Overview
• Implemented to address:
• Automated Document Indexing
• Same  concept to LSA
o Dimensionality Reduction
o Construction of a latent space
BUT…..
• Sound Statistical foundations
o Well defined probabilities
o Explicable results
8
Probabilistic LSA
Aspect Model
• Generative model based on the Aspect model
o Latent variables z are introduced and relate to 
documents d.
o |z| << |d|, as the same zi may be associated with 
more than one documents
o z performs as a bottleneck and results in 
dimensionality reduction
9
Probabilistic LSA
Model
• Joint probability shows the probability of a word w to 
be inside a document d
• Word distributions are combinations of the 
    factors P(w|z) and the mixing weights P(z|d)
Mixing 
weightsMultinomials
Multinomial 
Mixtures
10
Probabilistic LSA
Model
• Conditional Independence assumption
o Documents and Words are independent given z
• Thus, equivalently:
11
Probabilistic LSA
Model fitting
• Expectation Maximization
• Standard procedure for latent variable models
• E-step: Compute the posteriors for the latent 
variables z
• M-step: Update the parameters 
12
Probabilistic LSA
Space
•  
 
Sub-simplex dimensionality ≤ K-1 << D-1
13
Tempered EM
 Avoid overfitting training data
 Introduce a regularization term β
14
Tempered EM  - Concept
 Add a term β < 1 in the E step.
 Used to dampen probabilities in M step.
 Accelerate model fitting procedure compared to 
other methods (ex. annealing)
 Perform EM iterations and then decrease β until 
performance on held-out data deteriorates.
15
PLSA vs LSA
• Great PLSA advantages on the modeling side
o Well defined probabilities
o Interpretable directions in the Probabilistic Latent Semantic 
space as multinomial word distributions
o Better model selection and complexity control (TEM)
• Important LSA drawbacks in the same side
o Not defined properly normalized probabilities
o No obvious interpretations of LS space directions
o Selection of dimensions based on ad-hoc heuristics
• Potential computational advantage of LSA over PLSA
    (SVD vs EM which is an iterative method) 16
Aspect Model vs Clusters
Document Clustering       Aspect Model
PLSA: Documents are not related to a single cluster
         flexibility, effective modeling
Cluster aspect
Documents
17
Evaluation
perplexity
 Perplexity: Measures how “well” a prob. distribution 
can make predictions.
 Low perplexity  more certain predictions, better 
model
 PLSA evaluation method:
− Extract probabilities from LSA
− Unigram model as baseline
 PLSA evaluation results
− PLSA better than LSA
− TEM better than EM
− PLSA allows |Z| > rank(N)
    (N is the co-oc. Matrix)
18
Evaluation
Automatic Indexing
• Given a short document (query q) find the most relevant 
documents
• Baseline term matching s(d,q): cosine scoring method 
combined with term frequencies
• LSA: Linear combination of s(d,q) and the one derived 
from the latent space
• PLSA: Evaluation of similarities of P(z|d) & P(z|q)
19
Evaluation
Precision & Recall
 Precision & Recall:
    Popular measures in
    Information Retrieval.
20
Evaluation
Precision & Recall
For intermediate values of
recall, the precision of 
PLSA is almost 100%
better than the baseline
method!!! 
21
Evaluation 
Polysemy
• Results show advantage of PLSA over polysemy
22
Conclusion
• Documents are represented as vectors of word 
frequencies
• There is no syntactic relation or word­ordering but 
co­occurences still provide useful semantic insights 
about the document topics
• PLSA is a generative model based on this idea.
• It can be used to extract topics from a collection of 
documents
• PLSA significantly outperforms LSA thanks to its 
probabilistic basis.
23
References
 D.M. Blei, A.Y. Ng, and M.I. Jordan, “Latent dirichlet allocation,” J. Mach. 
Learn. Res., vol. 3, 2003, pp. 993-1022.   
 T. Hofmann, “Unsupervised Learning by Probabilistic Latent Semantic 
Analysis,” Machine Learning, vol. 42, Jan. 2001, pp. 177-196. 
 T. Hofmann, “Probabilistic latent semantic analysis,” In Proc. of 
Uncertainty in Artificial Intelligence, UAI’99, 1999, pp. 289--296.  
 DEERWESTER, S., DUMAIS, S., LANDAUER, T., FURNAS, G., AND 
HARSHMAN, R. 1990. Indexing by latent semantic analysis. J. Amer. 
Soc. Info. Sci. 41, 391-407.
   
24

