ar
X
iv
:1
70
6.
08
43
9v
1 
 [
cs
.A
I]
  2
6 
Ju
n 
20
17
Optimal choice problem and its solutions
Marina Sapir
m@sapir.us
http://sapir.us
Abstract
The article introduces the Optimal Choice problem. The problem emerges quite often in various practical
applications, but it was not formalized as a machine learning problem to the best of author’s knowledge.
The problem may seem perplexing because the most common assumptions of statistical learning theory
are violated here, as we discuss. Yet, the Optimal Choice problem may be solved successfully, as we
demonstrate on an example of a real life dataset from a signal processing application. We propose two
approaches to solve the problem. Both of them reach good solutions, at least on the data at hand.
1 Optimal choice problem
Let us introduce a Optimal Choice (OC) problem on an example of horse races.
Given is the history of horse races, including the information about the participating horses and the
winner of each race. Each competing horse is characterized by certain features. The goal is, knowing the
horses running in the next race, predict the winner.
We will discuss important examples of OC after the formal definition.
1.1 Formal Statement
A choice is vector of values of n features, x ∈ D ⊂ ℜn. A lot is a finite set of choices. Denote Ω set of
all possible lots.
Each lot has not more than one choice called prime. Conveniently denote X ′ the prime in the lot X .
If the lot X does not have a prime, it will be denoted as X ′ = ∅.
Then, training set can be presented as a finite set of pairs
Z = {〈X1, I(X
′
1
, X1)〉, ..., 〈Xm, I(X
′
m
, Xm)〉},
where all Xi ∈ Ω and I(x,X) is identity function on X :
1. I(x,X) ∈ {0, 1}
2. I(x,X) = 1 ⇔ x = X ′
3. if X ′ = ∅, I(x,X) ≡ 0.
For example, in horse races, choices are the horses, a lot is a race, and the prime is the winner of the
race.
The goal is to build a labeling function f(x,X) → {0, 1} such that, for every X ∈ Ω, x ∈ X, f(x,X) =
I(x,X).
1
If the condition f(x,X) = I(x,X) holds for a labeling function f and lot X , we say that a lot X is a
success of a labeling function f. Success rate of a labeling function is the probability of its successes on
Ω.
Let us consider a numeric function g(x,X) on pairs X ∈ Ω, x ∈ X. The function g correspond to a
single labeling function
f(x,X) = 1 ⇔ arg max
x∈X
g(x,X) = x
if g(x,X) has a single maximum on X ; otherwise, f(x,X) ≡ ∅ on X.
The function g(x,X) which identifies the primes by its maxima in the lots will be called scoring
function. The success rate of the corresponding labeling function will be associated with the scoring
function.
1.2 Practical examples of the optimal choice problem
OC problem occurs in many practical applications, even though it was not formalized yet, to the best of
author’s knowledge.
1. Competitions
Not only horse races, but most of competitions predictions problems look like OC. Even when the
same teams (participants) compete each year, they change with time and their features change as
well. It means, the lots and choices will be different each year, which makes prediction difficult.
2. Finding cycle in continuous noisy signal
The OC type problem was discovered in real life data analysis as part of the large signal processing
research. The company Predictive Fleet Technology, for which the author did consulting, analyzes
signal from piezoelectric sensors, installed in vehicles. The engine’s ‘signature’ (recorded signal)
continuously reflects changes in pressure in exhaust pipe and crankcase, which occur when engine
works. The cylinders in an engine fire consequently, so it should be possible to identify intervals of
work of each cylinder within the signature. The goal is to evaluate the regularity of the engine and
identify possible issues.
The most important part of the signature interpretation is to find the cycle: interval of time, when
every cylinder works once. The problem is difficult when the signature is irregular and curves
of consecutive cycles do not look the same, and when the signature is very regular and all the
cylinders look identical. Some preliminary work allowed us to find the potential cycles (choices)
for each signature, and each choice is characterized by four “quality criteria” (features). There is a
training set, where an expert identified true cycle for each signature.
The features are obtained by aggregating several signal characteristics: two features evaluate ir-
regularity of each of the curves (from exhaust pipe and crankcase) would have if the given interval
is selected as the cycle. And two other features characterize some measures of complexity of the
interval. All the features correlate negatively with the likelihood of the choice being the prime.
Three features are continuous, and the fourth feature is binary. They are scaled from 0 to 1.
The goal is to develop the rule which identifies the true cycle (the prime) among the chosen intervals
for each signature. The main property of this problem is that there is no “second best”: only one
cycle is correct, the rest are equally wrong. This problem lands naturally into the general OC
problem.
All the solutions proposed here are applied on the dataset of this problem. The data contain 2453
choices in 114 lots, on average 21 choice per lot, no lot contains less than 2 choices, and every lot
has a prime.
2
3. Multiple classes classification with binary classifiers
In many cases, the classification with k > 2 classes is done with a binary classifier. The task is split
on classifications for each one class versus all others. But what if some instances do not fit nicely
in any of the classes, or found similar to more than one class? One still needs to find the “prime”
class. In these types of problems, each lot will have k choices, and each choice will be characterized
by some criteria of fit between the classes and the instance. One needs to find an optimal rule
which will aggregate these single class criteria into a rule for all the classes together. This is an OC
problem.
4. Recommender system Suppose, a recommendation system presents a customer with sets of
choices each time, and lets him to choose one option he likes the best. The choices may be movies,
books, real estate and so on. The goal is to learn from the customer’s past choices and recommend
him new choices in the order of his preferences. Here, we are in situation of the OC problem again.
The training set contains past lots, where each choice is characterized by several features, and the
customer’s choice (prime) is known. The system needs to develop a personalized scoring function
on choices to present them to the customer in order of his preferences.
5. Rating system Let us consider a rating problem. There are two types of such problems, depending
on the feedback. The feedback in the training set may be binary, or it may represent ranks. For
example, the trainer marks each link as “relevant” to a query or not. Another option is to have the
trainer to assign a rank (or rating) to each object in the training set. In both cases, the goal is to
learn to rank the new objects.
Both approaches are hard on the trainer. If actual ratings have many values (various degrees of
relevancy), it may be difficult to assign just two values correctly. Assigning multiple rating in the
training set may be even more difficult. For a normal size training set, it would be too much work
for a trainer to check all the comparisons his ratings imply. Besides, some of the objects he rates
may not be comparable. It means, the trainer can not guaranty that all the relationships implied
by his ranking are true. It leads to inevitable errors, contradictions in labels in training data.
The solution may be to ask the trainer to select groups of comparable objects (lots), and identify
the best choice (prime) in each group. This will lead one to OC problem.
1.3 OC and statistical learning theory
In the widest possible definition, machine learning is ‘automated detection of meaningful patterns in data’
[1]. OC, certainly, fits in this category.
The problem has some similarity with two popular types of machine learning problems: classification
and ranking.
As in binary classification, the goal in OC is to learn a rule, which can be applied on the new data to
classify each choice in the lot as its prime or not.
To see similarity with the ranking problem, let us notice that selecting the prime in each lot establishes
partial order on the set of choices of this lot: X ′ ≻ q for a q ∈ X, q 6= X ′. So, the OC problem can be
considered as a problem of learning the partial order as a function of the features.
Despite the similarities, the main assumptions of statistical learning theory developed for classical
machine learning problems do not hold for OC.
1. Labels are not the function of the features
Statistical learning assumes that the values of the features determine the probability of a label.
Essentially, each classification method approximates a random function on D ⊂ R given on the
training set.
3
In OC, the labels are not a function of the features alone, since they depend both on the choice and
its lot. A choice may be the prime in one lot and not in another. For example, the prime horse in
a small stable is expected to be a poor competitor in some famous derby. It means that close (or
even identical) choices in different lots shall be assigned different labels by the learning algorithm
to have satisfactory success rate.
It implies that the labeling function can not be learned as a function on choices only. Yet, classical
machine learning does not deal with the functions on arbitrary sets of vectors.
2. The fit can not be evaluated point-wise
In classical machine learning, the point-wise fit between the true labels and assigned labels is
estimated as a measure of success, accuracy. For example, in classification, the probability of
the correct labels is evaluated. In ranking, some measure of the correlation (agreement in order)
between the known ranks and predicted scores is estimated.
Counting correct labels on choices or measuring correlation between the assigned and true labels in
the training set can not be used to evaluate the learning success in OC.
Suppose, for example, there are 10 choices in every lot. From classification point of view, if the
decision rule assigns zero to every choice, the rule is 90% correct. From optimal choice point of
view, the success rate is 0, because it did not identify any of the primes.
If a scoring function scores a prime in every lot as the “second-best”, many correlation measures
used in supervised ranking will be rather high. In this case, on each lot, 8 out of every 9 not-prime
choices are below the only prime choice, so AUC = 8/9 [2]. Yet, the scoring function fails to find
the prime everywhere, and, accordingly, the success rate of this scoring function is 0.
3. Independence
In machine learning, both feature vectors and feedback are supposed to be taken independently
from the same distribution. This is, obviously, not the case here.
As for labels, in each lot, only one label is 1, the rest are 0.
The distribution of the choice features is expected to depend on the lot. For example, more pres-
tigious races will include better overall participants and have necessarily different from other races
distribution of features.
However, we can expect that the lots appear independently, according with some probability dis-
tribution Pr(X) on Ω. Also, we can assume that there is probability distribution of a choice to be
the prime in a given lot: Q(x,X) = Pr(x = X ′). The lots and their primes in the training set are
generated in accordance with these two distributions.
2 Solving the problem
The statistical learning assumptions make classic machine learning problems tractable, allow some efficient
solutions. We showed that the assumptions are not satisfied in OC type of problems. So, one may wonder,
if OC has a decent solution.
In fact, identifying these issues helps us to find the ways the problem can be solved. We explore two
paths to the solution here.
First, we consider the problem in an extended set of features, where the added features characterize
lots. If the features are selected successfully, it can make the labels dependent on the choices only. Then,
the point-wise fitting machine learning methods can be applied, provided that, in the end, the fit is still
evaluated with the success rate.
4
As another possible solution, we explore general optimization methods which can optimize the OC
success rate directly in original feature space.
We will show here on the example of the real life and difficult problem of finding cycle in continuous
noisy signal that a good solution can be found efficiently both ways.
2.1 Expanding feature space to apply popular machine learning methods
As we mentioned above, the main issue with OC problem is that the labels depend both on the choice
and its lot. To make the labels less dependent of lots, we add new features about lot as a whole. Denote
D the extended feature space. In D, each choice still has its own specific features as well as new features,
characterizing its lot, and common for every choice in its lot.
The goal of extending the feature space is to have similar in D choices across all lots to have identical
labels with high likelihood.
Selection of the lot features, usually, requires some domain knowledge. However, there may be some
empiric considerations which simplify the selection.
Let us consider a simple case, when there are features F which correlate with the likelihood of a
choice to be prime and mutually correlate (if the features F are developed to predict primes, it is the
case, usually). Denote mX vector of maximal values of the features F in lot X , and suppose values of
mX are used as new features to characterize the lot. Then, a lot with higher values of mX will, likely,
have a prime with higher values of the features F . It is likely as well that lots with similar values of mX
will have similar primes. Or, at the very least, their primes will be less dissimilar, than primes of the lots
with very different features mX .
In finding the cycle problem, two features, cc.d and CrankRegul, have very different distributions
from one engine to another because the regularity of engines varies widely. The features have negative
correlation with the likelihood of the choice to be prime. So we added two new features: min.cc.d, and
min.CrankRegul, which equal minimal in the lot values of the features cc.d and CrankRegul respectively.
For good engines, the features correlate strongly. They both achieve minima on the intervals multiple
of the true cycle. For bad engines, the features do not correlate. It means, for a bad engine, there may
be potential cycles with low value of one feature and high value of another. Then, the bad engine has
low values of both additional features, as do good engines. In this case, additional features do not help
distinguishing the engines and predicting the prime. Fortunately, this does not happen often. The bad
engines have, usually, higher values of these features than good engines.
With all the applied methods, the output was interpreted as a scoring function: primes were identified
by maximal values of the function in each lot.
We used the R implementation of the most popular regression and classification methods: function
SVM with linear kernel from e1071 R package, the function neuralnet from the R package with the same
name, function boosting from the R package adabag, function glm from R base to build logistic regression.
In all the functions, we used predicted continuous output as a scoring function.
Testing was done with “leave one lot out” procedure: each lot, consequently, was removed from
training and used for test. Percentages of the test lots, where the prime was correctly found by each
method, are in the table 1.
Selection of parameters of the neural net is a challenge. We used 1 hidden layer with 3 and 4 neurons.
Adding more layers or neurons does not help in our experiments. ADAboost strongly depends on the
selected method’s parameters as well. In our experiments, ADAboost builds 30 decision trees with the
depth not more than 5. Changing these parameters did not improve the solution.
5
Table 1. Success Rate, Machine Learning Methods
Method Orig. data Extended data
SVM, linear kernel 0.80 0.86
ADAboost 0.74 0.83
Logistic regression 0.82 0.87
Neural net, 3 neurons 0.79 0.79
Neural net, 4 neurons 0.80 0.81
3 Optimization of the success rate criterion
We were looking for a linear function of the original features which maximizes the success rate. The
dataset at hand is relatively small, so we could use the most general (slow) optimization methods, which
do not use derivatives.
3.1 Using standard optimization methods
We applied the standard Nelder - Mead derivative-free optimization method (implemented in the function
optim of the R package stats) to find the a linear function of the features optimizing the success rate
criterion. Depending on the starting point, the success rate of the found functions on the whole dataset
was from 0.82 to 0.86. Other optimization methods implemented in the same R function produced worse
results. It is interesting to notice that the the method works amazingly fast, much faster than neural
network or ADAboost on the same data.
3.2 Brute Force Optimization
The data for the cycle problem are rather small, so we applied the exhaustive search to find true optimal
hypothesis in a narrow class of hypotheses defined by the next rules:
1. The hypotheses are linear functions of the features f(x) = a1 · x1 + . . .+ a4 · x4.
2. All coefficients a1, . . . , a4 have integer values from 0 to n,
where n is a parameter of the algorithm. Out all the possible rules with the same threshold n and identical
(1% tolerance) performance, the algorithms picks the rule with minimal sum of coefficients.
For n = 15, the optimal linear function has the maximal value of coefficients equal 5. The rule has
success rate 88%. The algorithm with the parameter n = 5 was applied in “leave one lot out” experiment
with the same success rate, 88%.
4 Conclusion
Here we formulated the new machine learning problem, Optimal Choice, and proposed two paths to solve
it. The problem emerges in various practical applications quite often, but it has undesirable properties
from statistical learning theory point of view. Formalization of the problem and close look at its statistical
properties helped to find ways to solve it. The solutions were applied to a real life problem of signal
processing with rather satisfactory results.
The proposed solutions have some shortcomings. The first approach is based on extending feature
space, which would require some domain knowledge in each case. The direct optimization of the success
rate criterion worked very well on relatively small data, but it may not be scalable.
The goal of this article will be achieved, if the OC problem attracts some attention and further
research.
6
References
1. Shalev S, Ben-David S (2014) Understanding Machine Learning. Cambridge University Press, NY.
2. Ertekin S, Rudin C (2011) On equivalence relationships between classification and ranking algo-
rithms. Journal of Machine Learning Research 12: 2905 - 2929.
7

