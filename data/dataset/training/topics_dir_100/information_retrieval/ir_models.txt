Information Retrieval Models
Foundations and Relationships

Synthesis Lectures on
Information Concepts,
Retrieval, and Services
Editor
Gary Machionini,University of North Carolina, Chapel Hill
Synthesis Lectures on Information Concepts, Retrieval, and Services is edited by Gary Marchionini
of the University of North Carolina. e series will publish 50- to 100-page publications on topics
pertaining to information science and applications of technology to information discovery,
production, distribution, and management. e scope will largely follow the purview of premier
information and computer science conferences, such as ASIST, ACM SIGIR, ACM/IEEE JCDL,
and ACM CIKM. Potential topics include, but are not limited to: data models, indexing theory and
algorithms, classification, information architecture, information economics, privacy and identity,
scholarly communication, bibliometrics and webometrics, personal information management,
human information behavior, digital libraries, archives and preservation, cultural informatics,
information retrieval evaluation, data fusion, relevance feedback, recommendation systems, question
answering, natural language processing for retrieval, text summarization, multimedia retrieval,
multilingual retrieval, and exploratory search.
Information Retrieval Models: Foundations and Relationships
omas Roelleke
2013
Key Issues Regarding Digital Libraries: Evaluation and Integration
Rao Shen, Marcos André Gonçalves, and Edward A. Fox
2013
Visual Information Retrieval using Java and LIRE
Mathias Lux and Oge Marques
2013
On the Efficient Determination of Most Near Neighbors: Horseshoes, Hand Grenades,
Web Search and Other Situations When Close is Close Enough
Mark S. Manasse
2012
iv
e Answer Machine
Susan E. Feldman
2012
eoretical Foundations for Digital Libraries: e 5S (Societies, Scenarios, Spaces,
Structures, Streams) Approach
Edward A. Fox, Marcos André Gonçalves, and Rao Shen
2012
e Future of Personal Information Management, Part I: Our Information, Always and
Forever
William Jones
2012
Search User Interface Design
Max L. Wilson
2011
Information Retrieval Evaluation
Donna Harman
2011
Knowledge Management (KM) Processes in Organizations: eoretical Foundations and
Practice
Claire R. McInerney and Michael E. D. Koenig
2011
Search-Based Applications: At the Confluence of Search and Database Technologies
Gregory Grefenstette and Laura Wilber
2010
Information Concepts: From Books to Cyberspace Identities
Gary Marchionini
2010
Estimating the Query Difficulty for Information Retrieval
David Carmel and Elad Yom-Tov
2010
iRODS Primer: Integrated Rule-Oriented Data System
Arcot Rajasekar, Reagan Moore, Chien-Yi Hou, Christopher A. Lee, Richard Marciano, Antoine de
Torcy, Michael Wan, Wayne Schroeder, Sheau-Yen Chen, Lucas Gilbert, Paul Tooby, and Bing Zhu
2010
Collaborative Web Search: Who, What, Where, When, and Why
Meredith Ringel Morris and Jaime Teevan
2009
v
Multimedia Information Retrieval
Stefan Rüger
2009
Online Multiplayer Games
William Sims Bainbridge
2009
Information Architecture: e Design and Integration of Information Spaces
Wei Ding and Xia Lin
2009
Reading and Writing the Electronic Book
Catherine C. Marshall
2009
Hypermedia Genes: An Evolutionary Perspective on Concepts, Models, and Architectures
Nuno M. Guimarães and Luís M. Carrico
2009
Understanding User-Web Interactions via Web Analytics
Bernard J. ( Jim) Jansen
2009
XML Retrieval
Mounia Lalmas
2009
Faceted Search
Daniel Tunkelang
2009
Introduction to Webometrics: Quantitative Web Research for the Social Sciences
Michael elwall
2009
Exploratory Search: Beyond the Query-Response Paradigm
Ryen W. White and Resa A. Roth
2009
New Concepts in Digital Reference
R. David Lankes
2009
Automated Metadata in Multimedia Information Systems: Creation, Refinement, Use in
Surrogates, and Evaluation
Michael G. Christel
2009
Copyright © 2013 by Morgan & Claypool
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in
any form or by any means—electronic, mechanical, photocopy, recording, or any other except for brief quotations
in printed reviews, without the prior permission of the publisher.
Information Retrieval Models: Foundations and Relationships
omas Roelleke
www.morganclaypool.com
ISBN: 9781627050784 paperback
ISBN: 9781627050791 ebook
DOI 10.2200/S00494ED1V01Y201304ICR027
A Publication in the Morgan & Claypool Publishers series
SYNTHESIS LECTURES ON INFORMATION CONCEPTS, RETRIEVAL, AND SERVICES
Lecture #27
Series Editor: Gary Machionini, University of North Carolina, Chapel Hill
Series ISSN
Synthesis Lectures on Information Concepts, Retrieval, and Services
Print 1947-945X Electronic 1947-9468
Information Retrieval Models
Foundations and Relationships
omas Roelleke
Queen Mary University of London
SYNTHESIS LECTURES ON INFORMATION CONCEPTS, RETRIEVAL,
AND SERVICES #27
C
M
&
cLaypoolMorgan publishers&
ABSTRACT
Information Retrieval (IR) models are a core component of IR research and IR systems. e
past decade brought a consolidation of the family of IR models, which by 2000 consisted of
relatively isolated views on TF-IDF (Term-Frequency times Inverse-Document-Frequency) as
the weighting scheme in the vector-space model (VSM), the probabilistic relevance framework
(PRF), the binary independence retrieval (BIR) model, BM25 (Best-Match Version 25, the main
instantiation of the PRF/BIR), and language modelling (LM). Also, the early 2000s saw the
arrival of divergence from randomness (DFR).
Regarding intuition and simplicity, though LM is clear from a probabilistic point of view,
several people stated: “It is easy to understand TF-IDF and BM25. For LM, however, we un-
derstand the math, but we do not fully understand why it works.”
is book takes a horizontal approach gathering the foundations of TF-IDF, PRF, BIR,
Poisson, BM25, LM, probabilistic inference networks (PIN’s), and divergence-based models. e
aim is to create a consolidated and balanced view on the main models.
A particular focus of this book is on the “relationships between models.” is includes an
overview over the main frameworks (PRF, logical IR, VSM, generalized VSM) and a pairing
of TF-IDF with other models. It becomes evident that TF-IDF and LM measure the same,
namely the dependence (overlap) between document and query. e Poisson probability helps to
establish probabilistic, non-heuristic roots for TF-IDF, and the Poisson parameter, average term
frequency, is a binding link between several retrieval models and model parameters.
KEYWORDS
Information Retrieval (IR) Models, Foundations & Relationships, TF-IDF, prob-
ability of relevance framework (PRF), Poisson, BM25, language modelling (LM),
divergence from randomness (DFR), probabilistic roots of IR models
ix
To my father, Anton Roelleke.
“Knowing some basics of math is useful in life.”

xi
Contents
List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xix
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxi
1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1 Structure and Contribution of this Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Background: A Timeline of IR Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.3 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3.1 e Notation Issue “Term Frequency” . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.3.2 Notation: Zhai’s Book and this Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2 Foundations of IR Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.1 TF-IDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.1.1 TF Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.1.2 TFlog: Logarithmic TF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.1.3 TFfrac: Fractional (Ratio-based) TF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1.4 IDF Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.1.5 Term Weight and RSV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.1.6 Other TF Variants: Lifted TF and Pivoted TF . . . . . . . . . . . . . . . . . . . . 16
2.1.7 Semi-subsumed Event Occurrences: A Semantics of the BM25-TF . . . 17
2.1.8 Probabilistic IDF: e Probability of Being Informative . . . . . . . . . . . . . 19
2.1.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.2 PRF: e Probability of Relevance Framework . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.2.1 Feature Independence Assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.2.2 Non-Query Term Assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.2.3 Term Frequency Split . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.2.4 Probability Ranking Principle (PRP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.3 BIR: Binary Independence Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.3.1 Term Weight and RSV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
xii
2.3.2 Missing Relevance Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
2.3.3 Variants of the BIR Term Weight . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.3.4 Smooth Variants of the BIR Term Weight . . . . . . . . . . . . . . . . . . . . . . . 33
2.3.5 RSJ Term Weight . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.3.6 On eoretical Arguments for 0:5 in the RSJ Term Weight . . . . . . . . . . 33
2.3.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
2.4 Poisson and 2-Poisson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
2.4.1 Poisson Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
2.4.2 Poisson Analogy: Sunny Days and Term Occurrences . . . . . . . . . . . . . . 36
2.4.3 Poisson Example: Toy Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
2.4.4 Poisson Example: TREC-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
2.4.5 Binomial Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
2.4.6 Relationship between Poisson and Binomial Probability . . . . . . . . . . . . . 40
2.4.7 Poisson PRF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
2.4.8 Term Weight and RSV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
2.4.9 2-Poisson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
2.4.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
2.5 BM25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
2.5.1 BM25-TF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
2.5.2 BM25-TF and Pivoted TF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
2.5.3 BM25: Literature and Wikipedia End 2012 . . . . . . . . . . . . . . . . . . . . . . 46
2.5.4 Term Weight and RSV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
2.5.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
2.6 LM: Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
2.6.1 Probability Mixtures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
2.6.2 Term Weight and RSV: LM1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
2.6.3 Term Weight and RSV: LM (normalized) . . . . . . . . . . . . . . . . . . . . . . . . 52
2.6.4 Term Weight and RSV: JM-LM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.6.5 Term Weight and RSV: Dirich-LM . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.6.6 Term Weight and RSV: LM2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
2.6.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
2.7 PIN’s: Probabilistic Inference Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
2.7.1 e Turtle/Croft Link Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
2.7.2 Term Weight and RSV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
2.7.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
2.8 Divergence-based Models and DFR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
2.8.1 DFR: Divergence from Randomness . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
xiii
2.8.2 DFR: Sampling over Documents and Locations . . . . . . . . . . . . . . . . . . . 65
2.8.3 DFR: Binomial Transformation Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
2.8.4 DFR and KL-Divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
2.8.5 Poisson as a Model of Randomness: P.kt > 0jd; c/: DFR-1 . . . . . . . . . 68
2.8.6 Poisson as a Model of Randomness: P.kt D tfd jd; c/: DFR-2 . . . . . . . . 68
2.8.7 DFR: Elite Documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
2.8.8 DFR: Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
2.8.9 Term Weights and RSV’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
2.8.10 KL-Divergence Retrieval Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
2.8.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
2.9 Relevance-based Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
2.9.1 Rocchio’s Relevance Feedback Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
2.9.2 e PRF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
2.9.3 Lavrenko’s Relevance-based Language Models . . . . . . . . . . . . . . . . . . . . 75
2.10 Precision and Recall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
2.10.1 Precision and Recall: Conditional Probabilities . . . . . . . . . . . . . . . . . . . . 76
2.10.2 Averages: Total Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
2.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
3 Relationships Between IR Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
3.1 PRF: e Probability of Relevance Framework . . . . . . . . . . . . . . . . . . . . . . . . . . 80
3.1.1 Estimation of Term Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
3.2 P.d ! q/: e Probability that d Implies q . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
3.3 e Vector-Space “Model” (VSM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
3.3.1 VSM and Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
3.4 e Generalised Vector-Space Model (GVSM) . . . . . . . . . . . . . . . . . . . . . . . . . 85
3.4.1 GVSM and Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
3.5 A General Matrix Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
3.5.1 Term-Document Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
3.5.2 On the Notation Issue “Term Frequency” . . . . . . . . . . . . . . . . . . . . . . . . 90
3.5.3 Document-Document Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
3.5.4 Co-Occurrence Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
3.6 A Parallel Derivation of Probabilistic Retrieval Models . . . . . . . . . . . . . . . . . . . 92
3.7 e Poisson Bridge: PD.t ju/  avgtf.t; u/ D PL.t ju/  avgdl.u/ . . . . . . . . . . . . . 93
3.8 Query Term Probability Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
3.8.1 Query term mixture assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
3.8.2 Query term burstiness assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
xiv
3.8.3 Query term BIR assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
3.9 TF-IDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
3.9.1 TF-IDF and BIR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
3.9.2 TF-IDF and Poisson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
3.9.3 TF-IDF and BM25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
3.9.4 TF-IDF and LM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
3.9.5 TF-IDF and LM: Side-by-Side . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
3.9.6 TF-IDF and PIN’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
3.9.7 TF-IDF and Divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
3.9.8 TF-IDF and DFR: Risk times Gain . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
3.9.9 TF-IDF and DFR: Gaps between Term Occurrences . . . . . . . . . . . . . . 106
3.10 More Relationships: BM25 and LM, LM and PIN’s . . . . . . . . . . . . . . . . . . . . 108
3.11 Information eory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
3.11.1 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
3.11.2 Joint Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
3.11.3 Conditional Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
3.11.4 Mutual Information (MI) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
3.11.5 Cross Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
3.11.6 KL-Divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
3.11.7 Query Clarity: Divergence(query jj collection) . . . . . . . . . . . . . . . . . . . 111
3.11.8 LM = Clarity(query) – Divergence(query jj doc) . . . . . . . . . . . . . . . . . . 112
3.11.9 TF-IDF = Clarity(doc) – Divergence(doc jj query) . . . . . . . . . . . . . . . . 112
3.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
4 Summary & Research Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
4.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
4.2 Research Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
4.2.1 Retrieval Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
4.2.2 Evaluation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.2.3 A Unified Framework for Retrieval AND Evaluation . . . . . . . . . . . . . . 121
4.2.4 Model Combinations and “New” Models . . . . . . . . . . . . . . . . . . . . . . . 122
4.2.5 Dependence-aware Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
4.2.6 “Query-Log” and other More-Evidence Models . . . . . . . . . . . . . . . . . . 124
4.2.7 Phase-2 Models: Retrieval Result Condensation Models . . . . . . . . . . . 124
4.2.8 A eoretical Framework to Predict Ranking Quality . . . . . . . . . . . . . 124
4.2.9 MIR: Math for IR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
4.2.10 AIR: Abstraction for IR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
xv
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Author’s Biography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137

xvii
List of Figures
1.1 Timeline of research on IR models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Notation: sets, symbols, and probabilities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.3 Notation: Zhai’s book on LM and this book on IR models. . . . . . . . . . . . . . . . . . 8
2.1 TF Variants: TFsum, TFmax, and TFfrac. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2 IDF and Burstiness. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.3 Other TF Variants: Lifted TF and Pivoted TF. . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.4 Independent, Semi-subsumed, and Subsumed Event Occurrences. . . . . . . . . . . 18
2.5 IST: Independence-Subsumption-Triangle. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.6 BIR: Conventional and Event-based Notation. . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.7 “Sunny Days in a Holiday” and “Term Locations in a Document”. . . . . . . . . . . 37
2.8 BM25 Notation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
2.9 Two Probabilistic Inference Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
2.10 Retrieval Models: Overview. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
3.1 Term-Document Matrix TDc : Transpose of DTc . . . . . . . . . . . . . . . . . . . . . . . . 88
3.2 Dual Notation based on Term-Document Matrix TDc . . . . . . . . . . . . . . . . . . . 89
3.3 Child-Parent Matrix CPc : Transpose of PCc . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
3.4 Parallel Derivation of Probabilistic Retrieval Models: Event Spaces. . . . . . . . . . 92
3.5 Poisson Parameter  for all and elite Documents. . . . . . . . . . . . . . . . . . . . . . . . . 99
3.6 TF-IDF and LM: Side-by-Side. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
3.7 Relationships between Retrieval Models: Overview. . . . . . . . . . . . . . . . . . . . . . 114
4.1 Model Lattice. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.2 Unified Model Lattice: Retrieval and Evaluation. . . . . . . . . . . . . . . . . . . . . . . . 121

xix
Preface
“Why did you never write a book?” “I started one, and even a second one; but I never finished
them, because I found it impossible to keep the notation consistent.” is reply by Stephen
Robertson—in a discussion about why writing books takes long and finishing feels impossible
when aiming for perfection—was a main motivation to work on a book about IR models and
theory. e material went into this M&C title on “IR Models: Foundations & Relationships.”
Over the years, I have witnessed several statements on retrieval models. “TF-IDF is purely
heuristic;” “TF-IDF is not a model, it is a weighting scheme in the vector-space model;” “LM is
a clean probabilistic approach;” “LM is full of hacks and holes;” “TF-IDF is intuitive; we know
why it works; it is still not clear why LM works.”
TF-IDF is still the best known model, at least outside of the IR community. Inside of the
IR research community, BM25 and LM are gaining ground. BM25 is viewed as being a heuristic
composition of PRF-based, and therefore well-defined, parameters. BM25 replaced TF-IDF in
the mid 90s, though there is a close connection between the two, and in current literature we
often find the BM25 formulation involving the IDF. LM is a clean probabilistic approach, and
while surprisingly simple it is a very effective approach to IR. ere is an interesting perception
of LM. Whereas the mathematical formulation has been described as clean and even “simple,”
there is the view “it is not clear why LM works.” Also, there is the issue regarding the relationship
between the PRF/BM25 and LM.
While TF-IDF and BM25 are intuitive but there is the view that they are “heuristic,” LM
is a clean and probabilistic approach, but there are issues regarding intuition. ere is a similar
aspect regarding BIR and Poisson. Whereas the BIR model is widely known and understood, its
generalization, the Poisson model, and its role in showing the probabilistic roots of TF-IDF may
be less known.
Last but not least, there is DFR (divergence from randomness). Several researchers told
me: “We will never understand it.” My confession is that I thought I had understood DFR, but
when consolidating material for this book, I had to revisit several foundations to arrive at an
explanatory formulation.
is book brings together on around 100 pages an account of the main formulae that I
believe form the mathematical core to describe the foundations of and relationships between
models. ere are many formulae, but I hope that this account helps to consolidate the state-of-
the-art of retrieval models.
e Morgan & Claypool lecture Zhai [2009] on “Statistical Language Models for Infor-
mation Retrieval,” by ChengXiang Zhai, is the M&C title most closely related to this book on
xx PREFACE
IR Models. With regard to LM, Zhai’s book is much more comprehensive than this book, in
particular with regard to smoothing techniques and application-domains of LM.
is lecture book takes a horizontal approach pulling together the foundations for the main
strands of IR models. Care has been taken to achieve a notation that is consistent across different
retrieval models. A particular emphasis of this book is on the relationships between retrieval
models.
Special effort went into the index and into hyper referencing in this book. Regarding the
paper version, the hyper referencing shows the page number to facilitate navigation. e hyper-
links in the electronic version make it quick (and hopefully easy) to tack between models, and to
explore equations.
omas Roelleke
May 2013
xxi
Acknowledgments
I express my thanks to the colleagues who showed tome the ways into IRmodels and theory: Nor-
bert Fuhr, Gianni Amati, Keith van Rijsbergen, Stephen Robertson, Hugo Zaragoza, Arjen de-
Vries, Djoerd Hiemstra, Fabio Crestani, Mounia Lalmas, Fabrizio Sebastiani, Ingo Frommholz,
ChengXiang Zhai, and Riccardo Baeza-Yates. It is thanks to their knowledge, comments, and
questions, that I kept being encouraged to refine and rewrite formulae again, and all over again.
Also, my students Jun Wang, Hengzhi Wu, Fred Forst, Hany Azzam, Miguel Martinez-Alvarez,
and Marco Bonzanini have greatly contributed to shape our view on what constitutes and relates
IR models.
is lecture book on “IR Models” overlaps somewhat with another book in preparation:
“Probabilistic logical models for DB+IR.” While in this lecture, the focus is on the foundations
and relationships of IR models, the DB+IR book takes these as prerequisite knowledge to repre-
sent knowledge, and to implement retrieval models and retrieval tasks.
Special thanks go to Riccardo Baeza-Yates. While visiting Yahoo Labs at Barcelona Media,
I had the peace and inspiration to consolidate research results regarding abstraction and proba-
bility theory in IR models. is led to the groundwork for this book. I hope that the collection of
material and formulae will help to establish common foundations, including a general notation,
for IR.
Many thanks go to Gianni Amati and Stephen Robertson for explaining on several occa-
sions, with their knowledge and views, the hidden but so important details. Also, the comments
by Andreas Kaltenbrunner and Norman Fenton on the notation proposed in this book were very
helpful when consolidating the final formulation. anks to my colleagues Tassos Tombros and
Fabrizio Smeraldi for proofreading the final version.
I am indebted to Djoerd Hiemstra, Gianni Amati, Miguel Martinez-Alvarez, Marco Bon-
zanini, and Ingo Frommholz, who were willing to explore some of the details and formulae of
this book. I am also grateful for the suggestions and comments by Norbert Fuhr and Keith van
Rijsbergen. Finally, I would like to thank Gary Marchionini and Diane Cerra from Morgan &
Claypool for supporting a book on models and theory.
When trying to finish a book, work becomes part of private life. My family has been patient
on many occasions, asking only sometimes: “Are you in your math world again? Can you come to
our world?”
omas Roelleke
May 2013

1
C H A P T E R 1
Introduction
1.1 STRUCTURE AND CONTRIBUTION OF THIS BOOK
is book is structured into two main chapters: Chapter 2, Foundations of IR Models, and Chap-
ter 3, Relationships between IR Models. Many parts of this book are about widely known models.
e contribution of this book is to consolidate foundations and to highlight relationships. We
view models as members of a family, having one ancestor: probability theory. ere is no quest
to show that one model is superior to another.
Most of the content of this book is based on published material, however, some insights
have not been published previously or are not widely known. ese are listed in the summary,
4.1. e outlook, 4.2, arranges retrieval models in a lattice to position TF-IDF, BM25, and LM
with respect to each other. is extends toward a lattice positioning retrieval and evaluation. We
also look at potential roads ahead that could lead to extensions of existing models or even to new
models for IR.
1.2 BACKGROUND: A TIMELINE OF IR MODELS
Figure 1.1 shows a timeline of IR models.
Maron and Kuhns [1960] is viewed as the inspiration for probabilistic IR. Earlier research
includes the work of Zipf and Luhn in the 50s, and these studies into the distribution of document
frequencies inspired the area of “IR models.”
e 70s were dominated by the vector-space model (VSM), Salton [1971], in combina-
tion with TF-IDF, Salton et al. [1975], and the relevance feedback model, Rocchio [1971]. e
probabilistic model matured, Robertson and Sparck-Jones [1976], and Croft and Harper [1979]
showed that the model works for missing relevance information.
In the 80s, Fuzzy retrieval, Bookstein [1980], and extended Boolean, Salton et al. [1983],
were proposed, but the models did not surpass the VSM and TF-IDF.
In the late 80s, logical IR, van Rijsbergen [1986], entered the stage, leading to various
research branches, one being the endeavour to relate P.d ! q/ to the VSM Wong and Yao
[1995] and to TF-IDF.
e mid 90s saw BM25 taking the lead, Robertson and Walker [1994], Robertson et al.
[1995], where the pivoted document length, Singhal et al. [1996], became known to be a crucial
component.
e late 90s saw language modeling (LM) emerging, Ponte and Croft [1998]. Challenges
included to relate LM to the probability of relevance framework (PRF), Lafferty and Zhai [2003],
2 1. INTRODUCTION
ICTIR 2009 and ICTIR 2011
Roelleke and Wang [2008]: TF-IDF Uncovered
Luk [2008]: Event Spaces
Roelleke and Wang [2006]: Parallel Derivation of Models
Fang and Zhai [2005]: Axiomatic approach
He and Ounis [2005]: TF in BM25 and DFR
Metzler and Croft [2004]: LM and PIN’s
Robertson [2005]: Event Spaces
Robertson [2004]: Understanding IDF
Sparck-Jones et al. [2003]: LM and Relevance
Croft and La erty [2003], La erty and Zhai [2003]: LM book
Zaragoza et al. [2003]: Bayesian extension to LM
Bruza and Song [2003]: probabilistic dependencies in LM
Amati and van Rijsbergen [2002]: DFR
Lavrenko and Croft [2001]: Relevance-based LM
Hiemstra [2000]: TF-IDF and LM
Sparck-Jones et al. [2000]: probabilistic model: status
Ponte and Croft [1998]: LM
Brin and Page [1998a], Kleinberg [1999]: Pagerank and Hits
Robertson et al. [1994], Singhal et al. [1996]:
Pivoted Document Length Normalization
Wong and Yao [1995]: P (d → q) 
P (d → q) 
Robertson and Walker [1994], Robertson et al. [1995]: 2-Poisson, BM25
Church and Gale [1995], Margulis [1992]: Poisson
Fuhr [1992b]: Probabilistic Models in IR
Turtle and Croft [1991, 1990]: PIN’s
Fuhr [1989]: Models for Probabilistic Indexing
Cooper [1991, 1988, 1994]: Beyond Boole, Probability Theory in IR: An Encumbrance
Deerwester et al. [1990], Dumais et al. [1988]: Latent semantic indexing
van Rijsbergen [1986, 1989]:
Bookstein [1980], Salton et al. [1983]: Fuzzy, extended Boolean
Croft and Harper [1979]: BIR without relevance
Robertson and Sparck-Jones [1976]: BIR
Salton [1971], Salton et al. [1975]: VSM, TF-IDF
Rocchio [1971]: Relevance feedback
Maron and Kuhns [1960]: On Relevance, Probabilistic Indexing, and IR
Figure 1.1: Timeline of research on IR models.
Lavrenko and Croft [2001], Sparck-Jones et al. [2003]. Moreover, LM was thought to poten-
tially bring a probabilistic justification to heuristic TF-IDF, Hiemstra [2000]. Pathbreaking “ap-
proaches for using probabilistic dependencies,” Bruza and Song [2003], point at capturing de-
pendencies, and this is of growing importance, Hou et al. [2011].
en, Amati and van Rijsbergen [2002], “Divergence from Randomness (DFR),” created
a new branch of retrieval models, or, as some will say, a new root of models. is was followed by
more meta-work on models, such as Fang and Zhai [2005], “Axiomatic Approach,” and Roelleke
and Wang [2006], “Parallel Derivation.” Similar to the early work in Hiemstra [2000], the moti-
1.3. NOTATION 3
vation was to relate IR models with each other. Robertson [2004] discussed theoretical arguments
for IDF, revisiting the BIR-based justification of IDF. Robertson [2005] pointed at the issue
regarding “event spaces.” Luk [2008] picked this up and questioned the assumptions made in
Lafferty and Zhai [2003] when relating LM to the probability of relevance. Roelleke and Wang
[2008] presented a study where TF-IDF was derived from P.d jq/, and this is related to the
way LM is based on P.qjd/. Robertson and Zaragoza [2009], “e PRF: BM25 and Beyond,”
gathered the state-of-the-art foundations and formulation of PRF and BM25.
While the theory underlying TF-IDF, LM and PRF/BM25 evolved, van Rijsbergen
[2004] promoted “e Geometry of IR.” e approach calls for “back-to-the-roots,” and ad-
vocates the well-defined combination of concepts of geometry (vectors, matrices) and concepts
of probability theory.
e conferences ICTIR 2009 and ICTIR 2011 brought several new insights into the world
of IR models. Among others, the topics “relationships,” Aly and Demeester [2011], and “depen-
dencies,” Hou et al. [2011], point at promising research directions. e recent research is more
specific and adventurous than this book about foundations and relationships.
IR models are introduced in several text books such as Baeza-Yates and Ribeiro-Neto
[2011]: 2nd edition of “Modern IR;” Zhai [2009]: “Statistical LM for IR;” Manning et al.
[2008]: “Introduction to Information Retrieval;” Belew [2000]: “Finding out about;” Grossman
and Frieder [2004]: 2nd edition of “IR: Algorithms and Heuristics;” van Rijsbergen [1979]: “In-
formation Retrieval.”
1.3 NOTATION
Notation ... it is a tedious start, however, it is a must-have. Whereas the “document frequency of
a term” is an intuitive and consistent notion, the “within-document term frequency (of a term)” is
familiar, but causes inconsistency in the mathematical world. I was confronted with this issue in
the mid 90s when Peter Schaeuble pointed out that the “inverse term frequency of a document”
is a useful parameter, however, when defining “itf ” in a dual way to the commonly used “inverse
document frequency,” then there is a clash with the notion “term frequency.” In our work over
the past decade where we tried to achieve the probabilistic logical modeling of all IR models,
the notation issue “term frequency” turned out to be a hurdle important to be overcome; see also
Section 1.3.1 (p. 6), TF Notation Issue; Section 3.5.2 (p. 90), General Matrix Framework: TF
Notation Issue.
For achieving a consistent notation which is applicable across different models, we apply
a notation that makes explicit the “event space.” For example, we apply nD.t; c/ and nL.t; c/ to
refer to Document-based and Location-based counts (frequencies), i.e., the subscript in nD and nL
is the event space. In traditional IR notation, the symbol nt is applied, where nt WDnD.t; c/ is
the number of Documents in which term t occurs. is is also denoted as df.t; c/ WDnD.t; c/, the
document frequency of term t in collection c. Regarding the Location-based frequency, some
4 1. INTRODUCTION
authors employed capital TF, i.e., TF WDnL.t; c/, to refer to the number of times term t occurs
in the collection c.
Figure 1.2 shows step-by-step the basic sets, frequencies, and probabilities.
Special care has been taken to achieve a notation that fits all models, and that underlines
the dualities between probabilities. e first block shows the basic variables and sets. e main
variables are: term t , document d , query q, collection c, and relevance r . e main types of sets are
Documents, Locations, and Terms. For example, Dc is the set of documents in collection c, and
so forth. e lines separating groups of rows elicit that for the conditionals, namely document d ,
query q, collection c, and relevance r , frequencies are denoted in a systematic way.
Below the first block of rows, there is a block regarding the document-related and query-
related frequencies (Location-based). For example, tfd WDnL.t; d/ is the number of Locations at
which term t occurs in document d .
is is followed by two blocks regarding collection-oriented and relevance-oriented
frequencies: Location-based and Document-based frequencies. For example, tfc WD lf.t; c/ WD
nL.t; c/ is the number of Locations at which term t occurs in collection c, and NL.c/ is the to-
tal number of Locations in collection c. df.t; c/ WDnD.t; c/ is the number of Documents in which
term t occurs in collection c, and ND.c/ is the total number of Documents in collection c. e
same formalism applies to the relevant documents, the “collection” r .
To be precise, the counts are defined as follows: nL.t; c/ WDnLc .t/, and NL.c/ WDNLc . e
notation where we avoid the double subscript is preferable for readability.
e next block shows Term-based frequencies: nT .d; c/ is the number of Terms in docu-
ment d in collection c, and NT .c/ is the number of Terms in collection c. We will make little use
of Term-based frequencies, however, they are included here to emphasize the duality between
Documents and Terms, i.e., df.t; c/ WDnD.t; c/, the document frequency of term t , is dual to
tf.d; c/ WDnT .d; c/, the term frequency of document d .
en, there are averages (for parameter u): uDc is for the set of all documents in the col-
lection; uDr is for the set of relevant documents; uD Nr is for the set of non-relevant documents.
ese are the most common sets, but the averages can be denoted for any set of documents (e.g.,
elite sets).
e expression avgdl.u/ denotes the average document length over the documents in
set Du. en, pivdl.d; u/ WDdl=avgdl.u/ is the pivoted document length of document d ; it is
greater than 1:0 for long documents, and smaller for short documents.
Moreover, there are several “average term frequencies.” Let .t; u/ denote the average
frequency over all documents in the set Du. Let avgtf.t; u/ denote the average frequency over
term-elite documents, i.e., the average value of tfd over the documents in which the term occurs.
Finally, let .t; uq/ denote the average frequency over all the documents in the query-elite set.
e query-elite set is the set of documents that contain at least one query term (disjunctive in-
terpretation of query). Note that in the term-elite set, for all documents, tfd  1, whereas in the
1.3. NOTATION 5
Book’s Description of events, sets, and frequencies Traditional
notation notation
t , d , q, c, r term t , document d , query q, collection c, relevant r
D c , D r D c = {d  ,...}1
1
set of
= {t  ,...} set of
1= {l  ,...} set of
Documents in collection c ; D r : relevant documents
Tc , Tr Tc Terms in collection c; Tr : terms that occur in relevant doc-
uments
L c , L r L c Locations in collection c; L r : locations in relevant docu-
ments
nL (t, d ) number of Locations at which term t occurs in document d tf, tfd
NL (d )
(q )
(t , q )
(t, c )
(c )
(r )
(t , r )
(t, c )
(c )
(r )
(t , r )
(c )
(d ,c )
number of Locations in document d (document length) dl
nL number of Locations at which term t occurs in query q qtf, tfq
NL number of Locations in query q (query length) ql
nL number of Locations at which term t occurs in collectionc TF, cf(t)
NL number of Locations in collection c
nL number of Locations at which term t occurs in the set L r
N
L
number of Locations in the set L r
nD number of Documents in which term t occurs in the set D c of collection c n t , df(t)
ND number of Documents in the set D c of collection c N
nD number of Documents in which term t occurs in the set D r of relevant documents rt
ND number of Documents in the set D r of relevant documents R
nT number of Terms in document d in collection c
NT number of Terms in collection c
Let u denote a collection associated with a set of documents. For example: u = c =, or u r = ȓ, or u .
avgdl (u) (u) (u)/ (u)
(d , u) (d , u)
average document length: avgdl =
:= = = =
= =
NL ND
/
/
(u)ND
ND
avgdl
where the collection is implicit, we use avgdl
pivdl pivoted document length: pivdl NL /avgdl (u) /avgdl (u)
(d)
(d) dl pivdl
where the collection is implicit, we use pivdl
average term frequency overall documents in D u : nL (t,u)
(t,u)n DnL (t,u)
(t,u  )
avgtf(t,u) average term frequency overterm-elitedocuments:
q ) average term frequency overquery-elite documents in D u q: nL q (u   )q/
D u q : set of documents that contain at least one query term
Book’s Description of probabilities Traditional
notation notation
PL (t|d) (t|d)
:=(t|q)
nL (t,d)
N L (d)
nL (t,q)
N L (q)
:=(t|c) nL (t,c)
N L (c)
:=(t|r) nL (t,r)
N L (r)
:=(t|c) nD (t,c)
N D (c)
:=(t|r) nD (t,r)
N D (r)
:=(d|c) nT (d,c)
N T (c)
:=(t|c)
(t|c)
avgtf
avgdl
(t,c)
(c)
(c)
Location-based within-document term proba-
bility
P
tfd
|d|
, |d| dl NL (d)
= = =(t|q)P
tfq
|q|
, |q| ql NL (q)
= =(t|c)P
tfc
|c|
, |c| NL (c)
= =(t)P
n t
N
,N ND (c)
= =(t|)P
r t
R
,R ND (r)
PL Location-based within-query term probability
PL Location-based within-collection term proba-
bility
PL Location-based within-relevance term proba-
bility
PD Document-based within-collection term prob-
ability
PD Document-based within-relevance term prob-
ability
PT Term-based document probability
Pavg probability that t occurs in document with av-
erage length; avgtf avgdl
Figure 1.2: Notation: sets, symbols, and probabilities.
6 1. INTRODUCTION
query-elite set, tfd D0 may occur for some terms. is aspect is important when estimating the
probability P.tfd D 0jset of documents/.
In the lower part of Figure 1.2, the main probabilities are listed. For example, PL.t jd/ is the
Location-based term probability in document d , and PD.t jc/ is the Document-based term prob-
ability in collection c. e subscripts D and L indicate the respective event space, namely Docu-
ments and Locations. e duality between probabilities is in particular evident when considering
the Term-based document probability PT .d jc/, a probability we show to underline the duality,
but we will not utilize this probability in this book on foundations. Finally, there is the probabil-
ity Pavg.t jc/ D avgtf.t; c/=avgdl.c/. is is the probability to observe a term in a document with
average length. We will refer to this probability frequently when discussing relationships between
models.
e notation meets the requirement to capture all parameters of the main IR models. e
type of the event space (D, T , or L) is specified as a subscript. e instantiation of a space (c, r , Nr)
is specified as a subscript of the event space or as a parameter in frequency counts. For example,
Dr is the set of relevant documents, and nD.t; r/ WDnDr .t/ is the number of documents with
term t .
e notation allows us to refer in a non-ambiguous way to document-related and to
collection-related parameters, and to document-based and location-based frequencies. is
book’s notation maintains the traditional notation. For example: tfd WD lf.t; d/ WDnL.t; d/ is the
“within-document term frequency,” and tfc WD lf.t; d/ WDnL.t; c/ is the “within-collection term
frequency,” and df.t; c/ WDnD.t; c/ is the “document frequency.” e notation was developed
as part of the general matrix framework for IR, Roelleke et al. [2006]. A summary of the
framework is in Section 3.5 (p. 88), General Matrix Framework. Systematically, if we refer
to df.t; c/ WDnD.t; c/ as the document frequency of term t , then tf.d; c/ WDnT .d; c/ is the term fre-
quency of document d . We look closer at this notation issue.
1.3.1 THE NOTATION ISSUE “TERM FREQUENCY”
In traditional IR terminology, the notion “term frequency” refers to the within-document location
frequency of a term, that is: tfd WD lf.t; d/ WDnL.t; d/ is the within-document term frequency.
Analogously, tfc WD lf.t; c/ WDnL.t; c/ is the collection-wide term frequency.
e notion “document frequency” refers to the number of documents that contain a term,
that is: df.t; c/ WDnD.t; c/ is the within-document term frequency. When trying to create the
duality between “df ” and “tf,” then there is an inconsistency, as the following formulation shows.
df.t; c/ WDnD.t; c/: “document frequency of term t in collection c:”
number of Documents in which term t occurs in collection c
In the dual formulation, exchange the roles of document and term:
tf.d; c/ WDnT .d; c/: “term frequency of document d in collection c:”
number of Terms in document d in collection c
1.3. NOTATION 7
is mathematical duality between document and term frequency follows from the Term-
Document matrix; see Section 3.5 (p. 88), General-Matrix-Framework.
To overcome this notation issue, we define tfd and tfc as follows:
tfd WD lf.t; d/ WDnL.t; d/: “(term) location frequency of term t in document d :”
number of Locations at which term t occurs in document d
tfc WD lf.t; c/ WDnL.t; c/: “(term) location frequency of term t in collection c:”
number of Locations at which term t occurs in collection c
We avoid expressions such as tf.t; d/ or c.t; d/ (“c” for “count”) for within-document counts.
Also, we employ tfc instead of the sometimes used capital TF for the collection-wide term fre-
quency, since TF is the place holder for any TF quantification. e collection-wide term fre-
quency, tfc , has also been denoted in the literature as “collection frequency cf.t/,” however, in
a dual notation, the collection frequency is the number of collections in which the term occurs.
is parameter is for example useful in large-scale distributed IR, for data source selection.
e tedious details of notation are “heavy going,” however, a consistent notation will ad-
vance IR research. When discussing TF-IDF with a mathematical researcher, he pointed at the
TF-issue within minutes, and by resolving it, it became much easier to express IR in matlab.
Also, an early review of this section by an expert in Bayesian reasoning immediately asked for
clarification on the TF notation.
Dual definitions of basic notions such as IDF (inverse document frequency) and ITF (in-
verse term frequency) are difficult to achieve in a less formal notation. e event-space-based
notation with sets (subscripts) T for Terms, D for Documents, and L for Locations, allows for
non-ambiguous definitions. is helps to work out the foundations of and relationships between
IR models.
1.3.2 NOTATION: ZHAI’S BOOK AND THIS BOOK
Figure 1.3 shows the notation used in Zhai [2009], “Statistical LM for IR.” e figure relates the
notations to improve the readability of both books.
8 1. INTRODUCTION
p.wjC /
p.wjC / D
P
D2C c.w; D/
P
D2C jDj
PL.t jc/
PL.t jc/ D
P
d2c d
P
d2c
D
nL.t; c/
P
d2c NL.d/
D
c
NL.c/
w C
D c.w; D/
jDj
t c d d
p.wjC / D
1
jC j
X
D2C
c.w; D/
jDj
jC j
P.t jc/ D
X
d2Dc
P.t jd; c/  P.d jc/
P.d jc/
V
Q D q1; : : : ; qm qi
q D t1; : : : ; tm ti
p.QjD/ D
m
Y
iD1
X
w2V
p.qi jw/p.wj‚D/
p.wj‚D/ D
X
w02V
p.wjw0/p.w0jD/
P.qjd/ D
m
Y
iD1
X
t2V
P.ti jt/  P.t jd/
P.t jd/ D
X
t 02V
P.t jt 0/  P.t 0jd/
.D; Q/ D  D.‚Qjj‚D/
P.qjd/
.d; q/ D  D
 
Pq jjPd

.d; q/ D D
 
Pq jjPc

 D
 
Pq jjPd

.d; q/ D D .Pd jjPc/  D
 
Pd jjPq

O.RjD; Q/ /
p.DjQ; R D r/
p.DjQ; R D Nr/
R
D
Q
O.r jd; q/ /
P.d jq; r/
P.d jq; Nr/
r
R D fr; Nrg
d
q
D D fd1; : : : ; dng
Q D fq1; : : : ; qmg
Figure 1.3: Notation: Zhai’s book on LM and this book on IR models.
9
C H A P T E R 2
Foundations of IR Models
e following list shows the structure of this chapter (the electronic version provides hyperlinks):
Section 2.1: TF-IDF
Section 2.2: PRF: e Probabilistic Relevance Framework
Section 2.3: BIR: Binary Independence Retrieval
Section 2.4: Poisson and 2-Poisson
Section 2.5: BM25: Best-Match Version 25
Section 2.6: LM: Language Modeling
Section 2.7: PIN’s: Probabilistic Inference Networks
Section 2.8: Divergence-based Models (including DFR and KL-Divergence Model)
Section 2.9: Relevance-based Models (Rocchio, PRF and Lavrenko’s relevance-based LM)
Section 2.10: Precision and Recall
Section 2.11: Summary (Model Overview in Figure 2.10)
is chapter presents and discusses the foundations of each model. We take a relatively isolated
view on each model, indicating some relationships. Chapter 3 discusses the relationships in more
detail.
2.1 TF-IDF
It may feel strange to get started with the “heuristic” model TF-IDF. e reason for having TF-
IDF first is that TF-IDF is still a very popular model, and the best known IR model outside of
IR research. is is probably because TF-IDF is very intuitive, and it has been around since the
60s, as a weighting scheme in the vector-space model (VSM).
Another reason to put TF-IDF first is that while being the most popular model, it is also
the most criticized one. “TF-IDF is not a model; it is just a weighting scheme in the vector-
space model.” “TF-IDF is purely heuristic; it has no probabilistic roots.” is book emphasizes
that TF-IDF and LM are dual models that can be shown to be derived from the same root.
10 2. FOUNDATIONS OF IR MODELS
Moreover, TF-IDF is commonly accepted to be a simplified version of BM25. Also, the vector-
space “model” is actually a framework in which TF-IDF, LM, and BM25 can be expressed, which
is why the VSM is in Chapter 3.
We capture the main aspects of TF-IDF in the following definitions:
1. TF variants: Definition 2.1 and Definition 2.2;
2. DF and IDF variants: Definition 2.3 and Definition 2.4;
3. TF-IDF term weight: Definition 2.5;
4. TF-IDF RSV: Definition 2.6.
2.1.1 TF VARIANTS
Definition 2.1 TF Variants: TF.t; d/. TF.t; d/ is a quantification of the within-document
term frequency, tfd . e main variants are:
tfd WD TFtotal.t; d/ WD lftotal.t; d/ WD nL.t; d/ (2.1)
TFsum.t; d/ WD lfsum.t; d/ WD
nL.t; d/
NL.d/

D
tfd
dl

(2.2)
TFmax.t; d/ WD lfmax.t; d/ WD
nL.t; d/
nL.tmax; d /
(2.3)
TFlog.t; d/ WD lflog.t; d/ WD log.1 C nL.t; d//
 
D log.1 C tfd /

(2.4)
TFfrac;K.t; d/ WD lffrac;K.t; d/ WD
nL.t; d/
nL.t; d/ C Kd

D
tfd
tfd C Kd

(2.5)
TFBM25;k1;b.t; d/ WD lfBM25;k1;b.t; d/ WD
nL.t; d/
nL.t; d/ C k1  .b  pivdl.d; c/ C .1   b//
(2.6)
Here, tfd WD nL.t; d/ captures the traditional notation, the “term frequency of term t in docu-
ment d .”
We include here the BM25-TF, although Section 2.5 is dedicated to BM25. is is to
highlight that the BM25-TF is a special form of TFfrac;K . e BM25-TF chooses a parameter
function K proportional to the pivoted documents length; the parameters k1 and b adjust the
impact of the pivotization. Strictly speaking, the parameter list should include the collection. For
example: TFBM25;k1;b.t; d; c/. is is because the pivoted document length, pivdl.d; c/, depends
on the collection. en, for the parameter function K, the notation is K.d; c/ or Kd .c/. For
simplicity, we often drop the collection where it is implicit, i.e., we use pivdl.d/ and Kd . Overall,
the BM25-TF can be applied for TF-IDF, making the TFBM25-IDF variant.
2.1. TF-IDF 11
Analogously to TF.t; d/, the next definition defines the variants of TF.t; c/, the collection-
wide term frequency.
Definition 2.2 TFVariants:TF.t; c/. TF.t; c/ is a quantification of the within-collection term
frequency, tfc . In analogy to TF.t; d/, for TF.t; c/, the main variants are:
tfc WD TFtotal.t; c/ WD lftotal.t; c/ WD nL.t; c/ (2.7)
TFsum.t; c/ WD lfsum.t; c/ WD
nL.t; c/
NL.c/
(2.8)
D
tfcP
d dl
D
tfc
collection_length

(2.9)
TFmax.t; c/ WD lfmax.t; c/ WD
nL.t; c/
nL.tmax; c/
(2.10)
TFlog.t; c/ WD lflog.t; c/ WD log.1 C nL.t; c//
 
D log.1 C tfc/

(2.11)
TFfrac;K.t; c/ WD lffrac;K.t; c/ WD
nL.t; c/
nL.t; c/ C Kc

D
tfc
tfc C Kc

(2.12)
TFBM25;k1;b.t; c/ WD lfBM25;k1;b.t; c/ WD
nL.t; c/
nL.t; c/Ck1 .b pivcl.c; collections/C.1 b//
(2.13)
TF.t; c/ would be used to retrieve a “collection.” is form of retrieval is required for distributed
IR (database selection). Everything said about TF.t; d/ applies to TF.t; c/. Within the context
of this book, however, we will not extend on TF.t; c/. We apply only the total TF, tfc WD nL.t; c/,
the “term frequency in the collection.”
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5  10  15  20
nL(t,d)
tfmax: nL(tmax,d)=20
tfmax: nL(tmax,d)=40
tfsum: NL(d)=100
tfsum: NL(d)=1000
(a) TFsum and TFmax
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5  10  15  20
nL(t,d)
nL(t,d) / (nL(t,d) + K)
K=1
K=2
K=5
K=10
(b) TFfrac
Figure 2.1: TF Variants: TFsum, TFmax, and TFfrac.
Figure 2.1 shows graphs for some of the main TF variants. ese illustrate that TFmax yields
higher TF-values than TFsum does. e linear TF variants are not really important anymore, since
12 2. FOUNDATIONS OF IR MODELS
TFfrac (TFBM25) delivers better and more stable quality. e latter yields relatively high TF-values
already for small frequencies, and the curve saturates for large frequencies. e good and stable
performance of BM25 indicates that this non-linear nature is key for achieving good retrieval
quality. Small values of K lead to a steep rise and early saturation of the TF value, whereas large
values of K lead to a smooth rise and delayed saturation.
TFtotal corresponds to assuming “independence,” and TFsum does the same, but normalizes
by the document length. TFtotal and TFsum are two extremes: TFtotal is steep, too steep. It leads to
a bias toward documents with many term occurrences. erefore, the idea is to lower the curve.
TFsum is relatively flat, too flat. For large documents, TFsum values tend to be small, and this
disadvantages long documents, or, in other words, leads to a bias toward small documents. e
variants TFmax and TFfrac (TFBM25) mitigate the bias of TFtotal and TFsum.
e variant TFfrac is the general form of the BM25-TF. Whereas in TFfrac, the parameter
function K can have any form, in TFBM25, K is proportional to the pivoted document length
(pivdl.d; c/ D dl=avgdl.c/) and involves adjustment parameters (k1, b). e common definition
is:
KBM25;k1;b.d; c/ WD k1 
 
b  pivdl.d; c/ C .1   b/

(2.14)
For b D 1, K is equal to k1 for average documents, less than k1 for short documents, and greater
than k1 for long documents. Large b and large k1 lead to a strong variation of K with respect to
the document length. A strong variation of K means that the document length has a high impact
on the retrieval score, whereby documents shorter than the average documents have an advantage
over documents longer than the average.
e good retrieval quality of BM25 indicates that a non-linear form of TF, where TF grows
faster for small occurrences than it grows in the linear case, but saturates for large occurrences, is
conducive for retrieval quality. erefore, we discuss in the following two non-linear TF’s: TFlog
and TFfrac.
2.1.2 TFlog: LOGARITHMIC TF
e logarithmic TF is defined as follows:
TFlog.t; d/ WD log.1 C tfd /
e logarithmic TF assigns less impact to subsequent occurrences than the total TF does. is
aspect becomes clear when reconsidering that the logarithm is an approximation of the harmonic
sum:
ln.1 C n/  1 C 1
2
C : : : C
1
n
n > 0 (2.15)
To understand this approximation, we recall the following integral:Z nC1
1
1
z
dz D Œln.z/nC11 D ln.n C 1/
2.1. TF-IDF 13
With respect to TFlog, the base of the logarithm is ranking invariant, since it is a constant:
TFlog;base.t; d/ WD
ln.1 C tfd /
ln.base/
Expressing the logarithm as the harmonic sum shows the type of dependence assumption under-
lying TFlog. e first occurrence of a term counts in full, the second counts 1=2, the third counts
1=3, and so forth. is gives a particular insight into the type of dependence that is reflected by
“bending” the total TF into a saturating curve.
Given this consideration of the logarithmic TF, the question is which dependence assump-
tions explain TFfrac and TFBM25?
2.1.3 TFfrac: FRACTIONAL (RATIO-BASED) TF
What we refer to as “fractional TF” is a ratio:
ratio.x; y/ D x
x C y
D
tfd
tfd C Kd
D TFfrac;K.t; d/
Viewing TF as a ratio connects the TF and the probability mixtures commonly used in LM and
Bayesian estimates, but this aspect is beyond the scope of this book.
Moreover, the ratio is related to the harmonic sum of squares.
n
n C 1
 1 C
1
22
C : : : C
1
n2
n > 0 (2.16)
is approximation is based on the following integral:Z nC1
1
1
z2
dz D

 
1
z
nC1
1
D 1  
1
n C 1
D
n
n C 1
e integral is smaller than the sum, since the curve 1=z2 cuts through the rectangles.
e main difference between the logarithmic TF and the fractional TF (BM25-TF) is that
the fractional TF corresponds to a stronger dependence assumption than the logarithmic TF. Ex-
pressing it via the harmonic sum nicely illustrates the type and strength of the assumption. e
fractional TF gives the k-th occurrence of a term an impact of 1=k2, whereas for the logarith-
mic TF, it is 1=k. Also interesting from a theoretical point of view is that the basic harmonic
sum is divergent, whereas the harmonic sum of squares is convergent. We will discuss in Sec-
tion 2.1.7 (p. 17), Semi-subsumed Event Occurrences, more aspects regarding the probabilistic
interpretation of TF quantifications.
Overall, this section showed different TF quantifications. e quantification is crucial for
the performance of TF-IDF. Bending the TF like the logarithmic TF does or shaping it into a
saturating curve like the BM25-TF, means to model the dependence between subsequent occur-
rences of an event, giving more importance to the first than to the subsequent occurrences.
14 2. FOUNDATIONS OF IR MODELS
We finalize the discussion of TF with a useful rewriting. e fractional (ratio-based) TF
can be expressed based on the within-document term probability P.t jd/ D tfd =dl:
TFfrac;K.t; d/ D
tfd
tfd C Kd
D
P.t jd/
P.t jd/ C Kd =dl

D
P.t jd/
P.t jd/ C 1=avgdl
; if Kd Ddl=avgdl

(2.17)
is formulation of the fractional TF is of advantage in probabilistic frameworks, since all com-
ponents, including 1=avgdl, are probabilities.
2.1.4 IDF VARIANTS
e IDF (inverse document frequency) is the negative logarithm of the DF (document fre-
quency). e two definitions to follow capture DF and IDF variants.
Definition 2.3 DF Variants. DF.t; c/ is a quantification of the document frequency, df.t; c/.
e main variants are:
df.t; c/ WD dftotal.t; c/ WD nD.t; c/ (2.18)
dfsum.t; c/ WD
nD.t; c/
ND.c/

D
df.t; c/
ND.c/

(2.19)
dfsum,smooth.t; c/ WD
nD.t; c/ C 0:5
ND.c/ C 1
(2.20)
dfBIR.t; c/ WD
nD.t; c/
ND.c/   nD.t; c/
(2.21)
dfBIR,smooth.t; c/ WD
nD.t; c/ C 0:5
ND.c/   nD.t; c/ C 0:5
(2.22)
Definition 2.4 IDFVariants. IDF.t; c/ is the negative logarithm of a DF quantification. e
main variants are:
idftotal.t; c/ WD   log dftotal.t; c/ (2.23)
idf.t; c/ WD idfsum.t; c/ WD   log dfsum.t; c/ (2.24)
idfsum,smooth.t; c/ WD   log dfsum,smooth.t; c/ (2.25)
idfBIR.t; c/ WD   log dfBIR.t; c/ (2.26)
idfBIR,smooth.t; c/ WD   log dfBIR,smooth.t; c/ (2.27)
Note the defaults: df.t; c/ WD dftotal.t; c/, and idf.t; c/ WD   log dfsum.t; c/. Whereas
df.t; c/ is usually the total count, idf.t; c/ is based on the normalized DF, dfsum. e latter is
equal to the Document-based term probability: PD.t jc/ D dfsum.t; c/.
Figure 2.2 shows the nature of the IDF. e figure also illustrates the notion of burstiness
(graphics from Roelleke and Wang [2006]).
2.1. TF-IDF 15
Figure 2.2: IDF and Burstiness.
IDF.t; c/ D  log PD.t jc/, is high for rare terms, and low for frequent terms.Here, PD.t jc/
is the Document-based term probability. IDF is based on rareness among documents, indepen-
dent of the distribution of term occurrences.
A term is “bursty” if it occurs often in the documents in which it occurs. Burstiness isburstiness
measured by the average term frequency in the elite set, i.e., avgtf.t; c/ D tfc=df.t; c/. Here, tfc D
lf.t; c/ D nL.t; c/ is the number of Locations at which t occurs, and df.t; c/ D nD.t; c/ is the
number of Documents in which t occurs.
Interestingly, burstiness is not an explicit component in neither TF-IDF nor LM. e
product of burstiness and term probability is equal to the average term frequency over all docu-
ments:
.t; c/ D avgtf.t; c/  PD.t jc/ D
nL.t; c/
ND.c/

D
tfc
ND.c/

e negative logarithm yields:
  log .t; c/ D   log
 
avgtf.t; c/  PD.t jc/

D IDF.t; c/   log avgtf.t; c/
We take a closer look at this equation in Section 2.4 (p. 35), Poisson. e intuition is that a good
term is rare (not frequent) and solitude (not bursty) in all documents (in all of the non-relevant
documents). For the set of relevant documents, the sign changes:
log .t; r/ D log
 
avgtf.t; r/  PD.t jr/

D  IDF.t; r/ C log avgtf.t; r/
Among relevant documents, a good term is frequent (low IDF) and bursty (high avgtf). e IDF
in TF-IDF can be interpreted as assuming IDF.t; r/D0 (term occurs in every relevant document,
which is also true for the empty set), and avgtf.t; r/ D avgtf.t; Nr/. en, the approximation is as
follows:
IDF.t; c/  IDF.t; Nr/   IDF.t; r/ C
 
log.avgtf.t; r//   log.avgtf.t; Nr//

16 2. FOUNDATIONS OF IR MODELS
see also Section 2.4 (p. 35), Poisson.
Generalizations and issues regarding IDF are manifested in the literature. For example:
“Generalised IDF,” Metzler [2008] and “Inverse Document Frequency: A Measure of Deviations
from Poisson,” Church and Gale [1995].
2.1.5 TERM WEIGHT AND RSV
e TF-IDF term weight can be formally defined as follows.
Definition 2.5 TF-IDF term weight wTF-IDF. Let t be a term, d a document, q a query, and
c a collection.
wTF-IDF.t; d; q; c/ WD TF.t; d/  TF.t; q/  IDF.t; c/ (2.28)
Here, TF and IDF are placeholders for the different estimates listed in the definitions for
TF and IDF. e TF-IDF RSV is defined as the sum of TF-IDF term weights.
Definition 2.6 TF-IDF retrieval status value RSVTF-IDF.
RSVTF-IDF.d; q; c/ WD
X
t
wTF-IDF.t; d; q; c/ (2.29)
Inserting wTF-IDF yields the RSV in decomposed form.
RSVTF-IDF.d; q; c/ D
X
t
TF.t; d/  TF.t; q/  IDF.t; c/ (2.30)
e sum can be
P
t or
P
t2d\q ; this is because TF.t; d/D0 for t 62d , and TF.t; q/D0 for t 62q.
In the remainder of this section on TF-IDF, we reconsider “other TF variants,” “semi-
subsumed event occurrences: a semantics of the BM25-TF,” and “the probabilistic interpretation
of idf.t; c/=maxidf.c/.”
2.1.6 OTHER TF VARIANTS: LIFTED TF AND PIVOTED TF
In addition to the previously discussed TF variants, even more TF variants have been proposed.
Two representatives are the lifted TF and the pivoted TF.
TFlifted;a.t; d/ WD a C .1   a/  TF.t; d/ (2.31)
TFpiv;K.t; d/ WD
TFtotal.t; d/
Kd
(2.32)
For TFlifted, let 0  a  1 be the parameter. For TFpiv, let K be the parameter function, where
Kd .c/ D pivdl.d; c/ D dl=avgdl.c/ is a special setting.
2.1. TF-IDF 17
e lifted variant yields TF values greater than the parameter “a;” this guarantees high
values already for small frequencies.
e pivoted variant normalizes the total count by a pivotization parameter. e pivoted
TF value of large documents is smaller than the non-pivoted value (non-pivoted value divided
by denominator greater than 1), whereas the TF value of small documents is greater than the
non-pivoted value (denominator less than 1).
e graphs in Figure 2.3 illustrate the nature of lifted and pivoted TF variants.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5  10  15  20
nL(t,d)
tfmax: nL(tmax,d)=20
tfmax: nL(tmax,d)=40
tfsum: NL(d)=100
tfsum: NL(d)=1000
(a) TFlifted: a D 0:5
 0
 5
 10
 15
 20
 0  5  10  15  20
nL(t,d)
nL(t,d) / (b * NL(d)/avgdl + (1-b))
NL(d)=100
NL(d)=200
NL(d)=1000
(b) TFpiv: b D 0:7, avgdl D 200
Figure 2.3: Other TF Variants: Lifted TF and Pivoted TF.
Lifted TF yields larger TF values than the respective non-lifted TF; TFlifted can be viewed
as the linear approximation of TFfrac.
Pivoted TF lowers the respective non-pivoted TF value for large documents, and increases
the value for small documents. TFpiv;K.t; d/ D tfd =Kd can be used conveniently for expressing
TFfrac;K :
TFfrac;K.t; d/ D
tfd
tfd C Kd
D
tfd =Kd
tfd =Kd C 1
D
TFpiv;K.t; d/
TFpiv;K.t; d/ C 1
(2.33)
See also equation 2.17 where we used P.t jd/ to express TFfrac. e formulation as a ratio is
convenient and useful in several contexts, for example when discussing a probabilistic semantics
of the BM25-TF.
2.1.7 SEMI-SUBSUMED EVENT OCCURRENCES: A SEMANTICS OF THE
BM25-TF
Figure 2.4 shows the overall idea of semi-subsumed event occurrences, Wu and Roelleke [2009]:semi-
subsumed
event
occurrences
in the set-based illustration, the overlap of semi-subsumed event occurrences is greater than for
independent event occurrences, and smaller than for subsumed event occurrences.
18 2. FOUNDATIONS OF IR MODELS
Figure 2.4: Independent, Semi-subsumed, and Subsumed Event Occurrences.
e notion “semi-subsumed” was created to establish a relationship between the BM25-TF
quantification and probability theory.
Example2.7 Semi-subsumedeventoccurrences. For example, consider an event e that occurs
twice.e two events e1 and e2 reflect the two occurrences.e single event probability is P.e1/D
P.e2/D0:3. e conjunctive probability is:
P.e1 ^ e2/ D
8̂<̂
:
0:32 D 0:09 independent: pn
0:3

2 2
2C1

 0:2008 semi-subsumed: p2
n
nC1
0:31 subsumed: p1
Definition 2.8 Semi-subsumedEvent (Term)Occurrences. Let L1; : : : ; Ln be random vari-
ables over the same event space. For IR, let the set of terms be the event space. With respect to
IR, a variable Li corresponds to the i-th Location (position) in a document.
Given the term probability P.Li D t/ D P.t/, the conjunctive probability of n independent
occurrences of t is:
P.L1 D t ^ L2 D t ^ : : : ^ Ln D t/ D P.t/
n
In contrast, the conjunctive probability of n semi-subsumed occurrences of t is:
P.L1 D t ^ L2 D t ^ : : : ^ Ln D t/ D P.t/

2 n
nC1

When interpreting a document d as a conjunction of term events at locations, for indepen-
dent event occurrences, the document probability is:
P.d/ D
Y
t IN d
P.t/ D
Y
t2d
P.t/nL.t;d/
 
D
Y
t2d
P.t/tfd
!
2.1. TF-IDF 19
e notation “t IN d ” views d as a sequence, whereas “t 2d ” views d as a set; nL.t; d/D tfd is
the number of occurrences of event t in sequence d . e subscript L denotes the type of random
variable.
For semi-subsumed event occurrences, the document probability is:
P.d/ D
Y
t2d
P.t/

2
nL.t;d/
nL.t;d/C1

D
Y
t2d
P.t/2TFfrac;KD1.t;d/
e equation underlines that the notion of semi-subsumed explains the meaning of the fractional
TF.
Figure 2.5 shows a graphical illustration, the independence-subsumption triangle (IST).independence-
subsumption
triangle
e IST illustrates that semi-subsumed, i.e., the exponent 2  n=.n C 1/, is the mid-point
between independent (exponent n) and subsumed (exponent 1). is mid-point is the har-
monic mean of 1 and n, which is h.f1; ng/ D 2  .1  n/=.n C 1/, and equivalently, h.f1; ng/ D
2  1=.1=1 C 1=n/.
e left slope of the triangle shows the case for independent event occurrences: the expo-
nent is n, the number of times the event occurs. e right slope shows the case for subsumed event
occurrences: the exponent is 1, since the multiple occurrences of the same event coincide. For the
altitude, i.e., the middle between independence and subsumption, the exponent is 2  n=.n C 1/.
Similarly, there is semi-disjoint between independent and disjoint, a notion not further discussed
here.
e final example underlines that different terms are independent, whereas the occurrences
of the same term are semi-subsumed.
Example 2.9 Independent terms, semi-subsumed term occurrences. Given a term sequence
where t1 occurs twice, t2 occurs three times, and t3 occurs once. e probability of the sequence
is:
P.t1; t1; t2; t2; t2; t3/ D P.t1/
4
3  P.t2/
6
4  P.t3/
2
2
We have discussed that the notion “semi-subsumed event occurrences” assigns a probabilis-
tic semantics to the fractional (BM25) TF.e next section looks at IDF: amax-based normaliza-
tion is sometimes convenient, and does not affect the ranking. From a probabilistic point of view,
the question is: which probabilistic semantics can be assigned to the value idf.t; c/=maxidf.c/?
2.1.8 PROBABILISTIC IDF: THEPROBABILITYOFBEING INFORMATIVE
In probabilistic scenarios, a normalized IDF value such as 0  idf.t; c/=maxidf.c/  1 can be use-
ful, Here, maxidf.c/ WD   log 1
ND.c/
is the maximal value of idf.t; c/. We refer to the normalized
20 2. FOUNDATIONS OF IR MODELS
independent semi-subsumed subsumed
1 1
2/2
2 2
1
2
3/2
2
2
3 3
1
3
4/2
3
3
4 4
1
4
2
4
5/2
4
3
4
4
5 5
1
5
2
5
6/2
5
4
5
5
... ... ... ...
n n
1
n
2
n
3
n
(n+1)/2
n
n 2
n
n 1
n
n
angle
independent subsumed
altitude
-6 -5 -4 -3 -2 -1 0 1
1
2
3
4
5
6
7 7.00 4.67 4.67 2.80 2.33 2.00 1.75
...   2n / (n + angle + 1)   ...
1.56 1.40 1.27 1.17 1.08 1.00
6.00 4.00 3.00 2.40 2.00 1.71 1.50 1.33 1.20 1.09 1.00
5.00 3.33 2.50 2.00 1.67 1.43 1.25 1.11 1.00
4.00 2.67 2.00 1.60 1.33 1.14 1.00
3.00 2.00 1.50 1.20 1.00
2.00 1.33 1.00
1.00
n
2 3 4 5 6
Figure 2.5: IST: Independence-Subsumption-Triangle.
IDF as “probabilistic IDF” (PIDF), and we discuss in this section the relationship between the
normalized IDF and probability theory.
A probabilistic semantics of a max-normalized IDF can be achieved by introducing
an “informativeness-based” probability, Roelleke [2003], as opposed to the normal notion of
“occurrence-based” probability, and we denote the probability as P.t informsjc/, to contrast it
from the usual P.t jc/ WD P.t occursjc/. e next definition captures this approach formally.
Definition 2.10 Probability of being informative (probabilistic idf ).
maxidf.c/ WD   log 1
ND.c/
D log ND.c/ (2.34)
2.1. TF-IDF 21
P.t informsjc/ WD pidf.t; c/ WD idf.t; c/
maxidf.c/
(2.35)
e normalization corresponds to setting the base of the logarithm, as the following def-
initions and equations recall. e logarithm to base b is defined as the fraction of the logarithm
of the argument x and the logarithm of the base b.
logb.x/ WD
log.x/
log.b/
D
ln.x/
ln.b/
erefore, the PIDF can be formulated as follows:
P.t informsjc/ D idf.t; c/
maxidf.c/
D
  log P.t occursjc/
log ND.c/
D   logND.c/
nD.t; c/
ND.c/
(2.36)
Continuing with the equation from above leads to the complement probability:
P.t informsjc/ D 1   logND.c/ nD.t; c/ D 1  
log nD.t; c/
log ND.c/
D 1   P.:t informsjc/ (2.37)
In an alternative formulation, we start from the complement probability:
P.:t informsjc/ D maxidf.c/   idf.t; c/
maxidf.c/
D
log nD.t; c/
log ND.c/
(2.38)
For the occurrence probability PD.t occursjc/ D nD.t;c/ND.c/ , if we apply the logarithm to numera-
tor and denominator, then we obtain the complement of the probability of being informative:
PD.:t informsjc/ D log nD.t;c/log ND.c/ .
ough the above shows how we can interpret the value pidf.t; c/ D idf.t; c/=maxidf.c/,
as a probability of being informative, the theoretically inclined researcher will still ask: Is there a
better way?
e convergence equation (limit definition) for the exponent function, see Bronstein [1987]
or other math text books, leads to a probabilistic semantics. e convergence equation is:
lim
N !1

1  

N
N
D e  (2.39)
Here, =N is the probability that an event occurs, and the convergence shows that e  is the
probability that the event does not occur in N trials.
e Euler convergence can be related to idf, and this establishes a probabilistic semantics
of P.t informsjc/.
Set .t; c/ WD idf.t; c/. en, e idf.t;c/ is the probability that the event t which occurs in
average idf.t; c/ times, does not occur in N D maxidf.c/ trials.
22 2. FOUNDATIONS OF IR MODELS
is assigns a semantics to idf.t; c/, and to P.t informsjc/. Because of idf.t; c/ WD
 log P.t occursjc/, the occurrence probability is equal to the probability that t is not informative
in maxidf.c/ trials. e following theorem formalizes this relationship.
eorem 2.11 Occurrence-Informativeness-eorem.. e probability that a term t occurs is
equal to the probability that the term is not informative in maxidf trials.
P.t occursjc/ D
 
1   P.t informsjc/
maxidf.c/ (2.40)
Proof. Step 1: Insert the definition of P.t informsjc/ (Definition 2.10 (p. 20), pidf ).
P.t occursjc/ D

1  
idf.t; c/
maxidf.c/
maxidf.c/
(2.41)
Step 2: Approximate expression by exponent function (Euler convergence).
P.t occursjc/ D e idf.t;c/ (2.42)
Step 3: Insert definition of idf.
P.t occursjc/ D elog P.t occursjc/ D P.t occursjc/ (2.43)

After this excursus regarding the probabilistic interpretation of pidf, we show how to for-
mally prove the ranking equivalence between RSVTF-IDF and RSVTF-PIDF. For this proof, we
define the notion rank-equivalent and score-equivalent.
Definition 2.12 Rank-equivalent and Score-equivalent. Two scoring functions A and B are
rank-equivalent iff for all retrieved objects x and y: if RSVA.x/ > RSVA.y/, then RSVB.x/ >
RSVB.y/.
Two scoring functions A and B are score-equivalent iff for all retrieved objects: RSVA.x/ D
  RSVB.x/, where  is a constant.
eorem 2.13 TF-IDF Ranking Equivalence.. RSVTF-IDF and RSVTF-PIDF are score-
equivalent.
RSVTF-IDF.d; q; c/
score
D
X
t
TF.t; d/  TF.t; q/  P.t informsjc/ (2.44)
2.2. PRF: THE PROBABILITY OF RELEVANCE FRAMEWORK 23
Proof. Insert definition of RSVTF-IDF and definition of P.t informsjc/:X
t
TF.t; d/  TF.t; q/  idf.t; c/ scoreD
X
t
TF.t; d/  TF.t; q/  idf.t; c/
maxidf.c/
(2.45)
Since maxidf.c/ is a constant, the scoring equivalence holds. 
2.1.9 SUMMARY
We have discussed the foundations of TF-IDF. We have considered the different variants of TF
and IDF. For TF, we have elaborated that the TF quantifications correspond to dependence as-
sumptions. In particular, the logarithmic TF and fractional TF (BM25-TF) can be expressed as
harmonic sums, and this shows the type of dependence assumption that is inherent to TF quantifi-
cations. Also, a TF of the form tfdtfd C1 corresponds to a particular assumption referred to as “semi-
subsumption,” an assumption that forms the altitude in the independence-subsumption-triangle.
For IDF, we have discussed what a max-based normalization of the form idf.t; c/=maxidf.c/
means. Overall, we have looked at foundations that assign a meaning to TF and IDF.
2.2 PRF: THE PROBABILITY OF RELEVANCE
FRAMEWORK
e PRF is based on the probability of relevance:
P.r jd; q/
Here, “r” denotes relevant, d stands for a document, and q stands for a query. When I started
studying probabilistic IR and first encountered this expression, this probability seemed rather
abstract to me. Also, in the literature, it is often denoted as P.Rjd; q/, and since R is also viewed
as a random variable or the set of relevant documents, understanding the probability of relevance
does not become easier. In this book, we use lower case letters for events, and view R D fr; Nrg as
the random variable (set of events). To make the probability less abstract, consider an example.
Example 2.14 Probability of relevance. Let three users u1; u2; u3 have judged document-
query pairs. e result is recorded as follows:
User Doc Query J udgement R
u1 d1 q1 r
u1 d2 q1 r
u1 d3 q1 Nr
u2 d1 q1 r
u2 d2 q1 Nr
u2 d3 q1 Nr
u3 d1 q1 r
u3 d2 q1 Nr
u4 d1 q1 Nr
u4 d2 q1 Nr
24 2. FOUNDATIONS OF IR MODELS
en, for example, PU .r jd1; q1/ D 3=4, and PU .r jd2; q1/ D 1=4, and PU .r jd3; q1/ D
0=2. e subscript in PU denotes the “event space,” a set of users. With respect to the theorem
of total probability, we can notate:
PU .r jd; q/ D
X
u2U
P.r jd; q; u/  P.u/
e example indicates that judgments can be incomplete, and the different ways to fill-up
the incomplete judgements affects the probability that can be observed.
In any case, for a new query, we do not have judgements. (We leave out approaches that
exploit similarities between new and past queries to deduce judgements for a new query.) e
probability is estimated via Bayes’s theorem:
P.r jd; q/ D
P.d; qjr/  P.r/
P.d; q/
D
P.d; q; r/
P.d; q/
(2.46)
e decision whether or not to retrieve a document is based on the so-called “Bayesian decision
rule:” retrieve document d , if the probability of relevance is greater than the probability of non-
relevance:
retrieve d for q if P.r jd; q/ > P. Nr jd; q/
is can be expressed via probabilistic odds of relevance.
O.r jd; q/ WD
P.r jd; q/
P. Nr jd; q/
D
P.r jd; q/
1   P.r jd; q/
(2.47)
en, the decision rule is:
retrieve d for q if O.r jd; q/ > 1 (2.48)
To compute the value of the probabilistic odds, the application of Bayes’s theorem yields:
O.r jd; q/ D
P.r jd; q/
P. Nr jd; q/
D
P.d; q; r/
P.d; q; Nr/
D
P.d; qjr/
P.d; qj Nr/

P.r/
P. Nr/
(2.49)
Since P.r/=P. Nr/ is a constant, the following rank equivalence holds:
O.r jd; q/
rank
D
P.d; qjr/
P.d; qj Nr/
(2.50)
e document-pair probabilities can be decomposed in two ways:
P.d; qjr/
P.d; qj Nr/
D
P.d jq; r/  P.qjr/
P.d jq; Nr/  P.qj Nr/
.) BM25/ (2.51)
D
P.qjd; r/  P.d jr/
P.qjd; Nr/  P.d j Nr/
.) LM?/ (2.52)
2.2. PRF: THE PROBABILITY OF RELEVANCE FRAMEWORK 25
Equation 2.51, where d depends on q, is the basis of BIR, Poisson and BM25. ose mod-
els view d as a vector in the term space of all terms, and decompose the document likelihood
P.d jq; r/ accordingly.
Equation 2.52 has been related to LM, P.qjd/, Lafferty and Zhai [2003], “Probabilis-
tic Relevance Models Based on Document and Query Generation.” is relationship and the
assumptions required to establish it, are controversial, Luk [2008], since at least one of the as-
sumptions used to establish that P.qjd/ is proportional to the odds of relevance, O.r jd; q/, is
questionable. Azzopardi and Roelleke [2007], “Relevance in LM,” discusses several approaches
to add relevance into LM. One of the milestones regarding the combination of relevance and
LM, Lavrenko and Croft [2001], “Relevance-based Language Models,” proposed a relevance-
based LM, but this model is actually a query expansion technique, as the authors describe it
themselves.
For the purpose of this section, we continue with the decomposition of the document event.
In equation 2.51, the factor P.qjr/=P.qj Nr/ is constant for a set of documents. is justifies the
following ranking equivalence:
O.r jd; q/
rank
D
P.d jq; r/
P.d jq; Nr/
(2.53)
e next step represents document d as a vector Ed D .f1; : : : ; fn/ in a space of features Efi :
P.d jq; r/ D P. Ed jq; r/ (2.54)
A feature could be, for example, the frequency of a word (term), the document length, document
creation time, time of last update, document owner, number of in-links, or number of out-links.
Often, the PRF model is formulated for a vector of term frequencies only; Fuhr [1992a] extended
the model using other features as well.
2.2.1 FEATURE INDEPENDENCE ASSUMPTION
Assumption 1 (PRF feature independence assumption.) e features are independent events.
PRF independence assumption: P. Ed jq; r/ 
Y
i
P.fi jq; r/ (2.55)
Often, the term features (term frequencies) constitute the feature space.
Note that this formulation of the independence assumption is stronger than the formulation
for the fraction of feature probabilities:
P.d jq; r/
P.d jq; Nr/

Y
i
P.fi jq; r/
P.fi jq; Nr/
(2.56)
For the purpose of this book, it is not required to distinguish between these two assumptions.
26 2. FOUNDATIONS OF IR MODELS
e next assumption reduces the number of elements in the product. e document vector
has a component for each term of the term space. e idea is that the non-query terms can be
ignored for retrieval (ranking).
2.2.2 NON-QUERY TERM ASSUMPTION
Assumption 2 (PRF non-query term assumption.) For non-query terms, the feature probability is
the same in relevant documents and non-relevant documents.
PRF non-query term assumption: for all non-query terms: P.fi jq; r/ D P.fi jq; Nr/ (2.57)
is reduces the product over all the features to the product over the query features, i.e., the features of
the query terms:
P.d jq; r/
P.d jq; Nr/

Y
i jti 2q
P.fi jq; r/
P.fi jq; Nr/
(2.58)
A less strict assumption could be based on viewing the product of P.fi jq; r/=P.fi jq; Nr/ of
the non-query terms as a constant, but it is sufficient to apply 1 D P.fi jq; r/=P.fi jq; Nr/.
2.2.3 TERM FREQUENCY SPLIT
Finally, the product over query terms is split into two parts. e first part captures the fi > 0
features, i.e., the document terms. e second part captures the fi D 0 features, i.e., the non-
document terms.
PRF term frequency split:
Y
i jti 2q
P.fi jq; r/
P.fi jq; Nr/
D
24 Y
i jti 2d\q
P.fi jq; r/
P.fi jq; Nr/
35  24 Y
i jti 2qnd
P.0jq; r/
P.0jq; Nr/
35
(2.59)
e first product is over the terms that occur in both, document and query; the second product
is over the terms that occur in the query only.
2.2.4 PROBABILITY RANKING PRINCIPLE (PRP)
Robertson [1977], “e Probability Ranking Principle (PRP) in IR,” describes the PRP as a
framework to discuss formally “what is a good ranking?” Robertson [1977] quotes Cooper’s for-
mal statement of the PRP:
“If a reference retrieval system’s response to each request is a ranking of the documents
in the collections in order of decreasing probability of usefulness to the user who
submitted the request, ..., then the overall effectiveness of the system ... will be the
best that is obtainable on the basis of that data.”
en, the paper devises an informal definition of an alternative principle. e key sentences char-
acterizing this alternative principle are:
2.2. PRF: THE PROBABILITY OF RELEVANCE FRAMEWORK 27
“Documents should be ranked in such a way that the probability of the user being
satisfied by any given rank position is a maximum. is alternative principle deals
successfully with some of the situations in which the [original] PRP fails, but there
are many problems with it.”
Formally, we can capture the principle as follows. Let A and B be rankings. en, a ranking A is
better than a ranking B if at every rank, the probability of satisfaction in A is higher than for B ,
i.e.:
8rank W P.satisfactoryjrank; A/ > P.satisfactoryjrank; B/
Obviously, the notion “at every rank” needs to be softer to compare rankings. For example, one
could base the comparison on the area under the probability curve. Also, being satisfactory at
early ranks might need to be reflected.
e discussion shows that the implementation of the PRP is difficult.
“Unfortunately, the dilemma is not resolvable: in some circumstances an optimal rank-
ing under one criterion cannot be optimal under another criterion.”
For a discussion of the optimality, see Gordon and Lenk [1992], “When is the Probability Rank-
ing Principle Suboptimal?”.
To illustrate the role of the PRP, we look next at an example from Fuhr [1989], “Models
for Retrieval with Probabilistic Indexing.”
e idea is to express the PRP via the expectation value of gain (or costs). Let g.d/ be a gain
function. With respect to the probability of relevance framework (PRF), let the gain function be
defined as the logarithm of the odds of relevance:
g.d/ WD log O.r jd/ D log P.r jd/
P. Nr jd/
e function can be interpreted as “gain,” i.e., the gain in relevant information when reading
document d . We could view the negative value of gain as costs, i.e., costs.d/ D  gain.d/. is
leads to the dual formulation of the PRP based on minimizing the costs. For the purpose of this
section, we continue with the gain function.
Let D be a random variable (set of documents). e expected gain is as follows:
Eg ŒD WD
X
d2D
P.d/  g.d/
0@D X
g.d/2fg.d1/;:::g
P.g.d//  g.d/
1A
e equation makes explicit the transformation step between the random variable of gains and
the random variable of documents.
Now, the idea is that with respect to the PRP, for two sets D1 and D2, the expected gain
is greater for the set in which the probability of relevance is greater:
P.r jD1/ > P.r jD2/ H) Eg ŒD1 > Eg ŒD2
28 2. FOUNDATIONS OF IR MODELS
One of the main messages in Fuhr [1989] is that this implication does not hold, and we review
in the following the example demonstrating this aspect.
Let D1 and D2 be two disjoint sets of documents. D1 contains 80 documents, 40 of which
are relevant. D2 contains 79 documents, 39 of which are relevant. en, the probability of rele-
vance is greater in D1 than in D2.
8d 2 D1 W PD1.r jd/ D 40=80 D 0:5
8d 2 D2 W PD2.r jd/ D 39=79<0:5
erefore, we expect Eg ŒD1 > Eg ŒD2.
Split set D1 into two subsets. One subset contains 39 relevant documents, the other con-
tains one relevant document. In a similar way, split set D2. One subset contains 30 relevant doc-
uments, the other contains 10 relevant documents.
is leads to the following subsets:
D1
D11 D12
P
relevant 39 1 40
not relevant 31 9 40P
70 10 80
D2
D21 D22
P
relevant 30 9 39
not relevant 39 1 40P
69 10 79
e motivation underlying the splits is to create extreme cases, where in D1, the set with
more relevant documents, we make the second subset contain mainly non-relevant documents,
whereas in D2, the less good set, we make the second subset contain mainly relevant documents.
e expectation value of D1 is as follows:
Eg ŒD1 D PD11.d/  log
PD11.r jd/
PD11. Nr jd/
C PD12.d/  log
PD12.r jd/
PD12. Nr jd/
When inserting the statistics of the example¹, we obtain for the two sets D1 and D2 the following
expectation values:
Eg ŒD1 D 70=80  log
39=70
31=70
C 10=80  log 1=10
9=10
  0:07
Eg ŒD2 D 69=79  log
30=69
39=69
C 10=79  log 9=10
1=10
 0:05
e expectation value of D2 is greater than the one of D1. erefore, Fuhr [1989] states: “e
ranking function Eg ŒD does not yield a ranking according to the PRP.”
¹ I am grateful to Norbert who cross-checked this example, and this clarified a typo in his original paper; replace
.39=79/=.31=80/ by .39=70/=.31=70/, etc.
2.3. BIR: BINARY INDEPENDENCE RETRIEVAL 29
2.2.5 SUMMARY
We have discussed the foundations of the PRF. e PRF is based on the probabilistic odds of
relevance, O.r jd; q/ D P.r jd; q/=P. Nr jd; q/. It basically corresponds to the Bayesian decision rule
to retrieve a document if its probability of relevance is greater than its probability of non-relevance.
e PRF is usually expanded for the document event, P.d jr; q/, leading to the BIR and Poisson
model.
2.3 BIR: BINARY INDEPENDENCE RETRIEVAL
e BIR instantiation of the PRF assumes the vector components to be binary term features, i.e.,
Ed D .x1; x2; : : : ; xn/, where xi 2 f0; 1g.
Starting with a conjunction of terms in document d , the occurrences of terms is repre-
sented in a binary feature vector Ed in the term space. For example, for a vocabulary (space) of five
terms, the term-based probability P.t2; t3; t2/ corresponds to the binary feature-based probability
P.0; 1; 1; 0; 0/.
e product over all features can be reduced to query features, t 2 q, because for non-query
terms, P.xt jq; r/ D P.xt jq; Nr/ is assumed. More generally, the assumption is that the product of
non-query term feature probabilities does not affect the ranking.
Moreover, instead of the conjunct “q; r ,” just “r” is used, which is justified by the implica-
tion r ! q, the set of relevant documents implies the query. Also, the event xt D 1 is expressed
as t , and xt D 0 as Nt .
en, q D .d \ q/ [ .q n d/ is applied to split the product (see Equation 2.59 (p. 26), Term
Frequency Split).
O.r jd; q/ /
Y
t2q
P.xt jr/
P.xt j Nr/
D
Y
t2q
P.t jr/
P.t j Nr/
D
24 Y
t2d\q
P.t jr/
P.t j Nr/
35  24 Y
t2qnd
P.Nt jr/
P.Nt j Nr/
35 (2.60)
Next, we apply a transformation to make the second product (the product over non-document
terms) to be independent of the document. Essentially, we multiply the equation by 1:0:
1:0 D
Y
t2d\q

P.Nt j Nr/
P.Nt jr/

P.Nt jr/
P.Nt j Nr/

en, we obtain for the probabilistic odds:
O.r jd; q/ /
0@ Y
t2d\q

P.t jr/
P.t j Nr/

P.Nt j Nr/
P.Nt jr/
1A   Y
t2q
P.Nt jr/
P.Nt j Nr/
!
(2.61)
e second product is document-independent, which means it is ranking invariant, and therefore
can be dropped.
30 2. FOUNDATIONS OF IR MODELS
Alternatively, the BIR weight can be derived using the binomial probability. For this for-
mulation, let at WD P.t jr/ and bt WD P.t j Nr/ be abbreviations of the respective probabilities.Y
t
Pbinomial;1;at .xt /
Pbinomial;1;bt .xt /
D
Y
t
a
xt
t
b
xt
t

.1   at /
.1 xt /
.1   bt /.1 xt /
In the literature, Robertson and Sparck-Jones [1976], van Rijsbergen [1979], the symbols pi and
qi are used, whereas this book employs at and bt . is is to avoid confusion between pi and
probabilities, and qi and queries.
e next step is based on inserting the xt ’s. 8t 2 d W xt D 1 and 8t 62 d W xt D 0.
O.r jd; q/ /
Y
t2d
at
bt

Y
t 62d
1   at
1   bt
By applying the non-query term assumption, Equation 2.57 (p. 26), non-query term assumption,
the products reduce to terms that are in the query:
O.r jd; q/ /
Y
t2d\q
at
bt

Y
t2qnd
1   at
1   bt
(2.62)
Finally, after multiplying with 1:0 (see above), we obtain the expression equivalent to equa-
tion 2.61.
O.r jd; q/ /
Y
t2d\q
at  .1   bt /
bt  .1   at /

Y
t2q
1   at
1   bt
(2.63)
2.3.1 TERM WEIGHT AND RSV
e BIR term weight can be formally defined as follows.
Definition 2.15 BIR term weight wBIR.
wBIR.t; r; Nr/ WD log

PD.t jr/
PD.t j Nr/

PD.Nt j Nr/
PD.Nt jr/

(2.64)
A simplified form, referred to as F1, considers term presence only and uses the collection to
approximate term frequencies and probabilities in the set of non-relevant documents.
wBIR;F1.t; r; c/ WD log
PD.t jr/
PD.t jc/
(2.65)
e term weight reflects the discriminative power of the term to distinguish between rele-
vant and non-relevant documents.
2.3. BIR: BINARY INDEPENDENCE RETRIEVAL 31
e BIR RSV is defined as the sum of BIR term weights.
Definition 2.16 BIR retrieval status value RSVBIR.
RSVBIR.d; q; r; Nr/ WD
X
t2d\q
wBIR.t; r; Nr/ (2.66)
Inserting wBIR yields the RSV in decomposed form.
RSVBIR.d; q; r; Nr/ D
X
t2d\q
log P.t jr/  P.
Nt j Nr/
P.t j Nr/  P.Nt jr/
(2.67)
ere are problems for missing relevance information (empty sets). For example, if
ND.r/ D 0 or ND. Nr/ D 0, then the probabilities are not defined (division by zero). Moreover,
for P.t j Nr/ D 0 leads to division-by-zero, and for P.t jr/ D 0 or P.Nt j Nr/ D 0, the logarithm is not
defined. ese problems can be solved conceptually by viewing the query as a relevant and a non-
relevant document, i.e., by adding two documents that contain all query terms.
2.3.2 MISSING RELEVANCE INFORMATION
Usually, non-relevance is not explicit, i.e., the set of non-relevant documents is derived from
positive relevance judgements. is is in contrast to approaches where we would consider non-
relevance to be specified explicitly, for example, by an explicit, negative judgement. For the esti-
mation of non-relevant documents, there are two main approaches:
1. P.t j Nr/  P.t jc/, where P.t jc/ D nD.t;c/
ND.c/
: Assume that the collection-wide term probabil-
ity is a good approximation of the term probability in non-relevant documents. We will use
Nr  c for referring to this approach.
2. P.t j Nr/  nD.t;c/ nD.t;r/
ND.c/ ND.r/
: Use the set of all documents minus the set of relevant documents
to estimate the term probability in non-relevant documents. We will use Nr  c n r for re-
ferring to this approach.
e second problem is the “empty” set problem. For ND.r/ D 0, the probability P.t jr/ is
not defined. e same mathematical issue occurs for ND.c/ D 0, though an empty set of docu-
ments is somewhat artificial. Smoothing deals with the empty set problem:
1. Add the query virtually to the set of relevant documents, and to the set of non-relevant
documents (to the collection, respectively). is guarantees that the term probabilities are
defined for each query term. is smoothing means, for example, P.t jr/ D nD.t;r/C1
ND.r/C1
.
2. Other forms of smoothing. For example, P.t jr/ D nD.t;r/C0:5
ND.r/C1
.
32 2. FOUNDATIONS OF IR MODELS
Going back to the BIR term weight, for missing relevance information, P.t jr/ is not de-
fined, and P.t j Nr/ D P.t jc/ is assumed. Croft and Harper [1979], Using Probabilistic Models of
Document Retrieval without Relevance Information, proposed to define a constant Cr if P.t jr/
is not defined:
Cr D
P.t jr/
1   P.t jr/
(2.68)
en, the BIR term weight is:
wBIR.t; r; c/ D Cr 
1   P.t jc/
P.t jc/
; if Dr D fg (2.69)
e assumption Cr D 1 is a special assumption, and in general, the constant Cr should be consid-
ered when formulating the BIR for missing relevance information; see also Section 3.9.1 (p. 96),
TF-IDF and BIR.
2.3.3 VARIANTS OF THE BIR TERM WEIGHT
is section uses the event-space-based and the conventional notation. Figure 2.6 shows the no-
tations side-by-side.
conventional event-based
PD.t jr/ rt =R nD.t; r/=ND.r/
Nr WD c PD.t j Nr/ nt =N nD.t; c/=ND.c/
Nr WD c n r PD.t j Nr/ .nt   rt /=.N   R/ .nD.t; c/   nD.t; r//=.ND.c/   ND.r//
Figure 2.6: BIR: Conventional and Event-based Notation.
Definition 2.17 Variants of the BIR term weight: estimation of Nr . e four main variants of
the BIR term weight reflect the combinations of whether only term presence or presence and
absence, and which approximation is applied for the set of non-relevant documents.
e following lattice displays the four variants (before smoothing):
Nr  c Nr  c n r
Presence
only
rt =R
nt =N
rt =R
.nt  rt /=.N  R/
Presence
and
absence
rt  .N   nt /
nt  .R   rt /
rt  ..N  R/   .nt  rt //
.nt  rt /  .R   rt /
2.3. BIR: BINARY INDEPENDENCE RETRIEVAL 33
2.3.4 SMOOTH VARIANTS OF THE BIR TERM WEIGHT
Definition 2.18 Smooth variants of the BIR term weight. e smooth variants are safe for
the case where the set of relevant documents is empty. e smoothing shown here can be viewed
as assuming two virtual documents, therefore N C 2. One of the virtual documents is relevant,
therefore R C 1.
Nr  c Nr  c n r
Presence
only
.rt C 0:5/=.R C 1/
.nt C 1/=.N C 2/
.rt C 0:5/=.R C 1/
.nt  rt C 0:5/=.N  R C 1/
Presence
and
absence
.rt C 0:5/  .N   nt C 1/
.nt C 1/  .R   rt C 0:5/
.rt C 0:5/  ..N  R/   .nt  rt / C 0:5/
.nt  rt C 0:5/  .R   rt C 0:5/
2.3.5 RSJ TERM WEIGHT
We refer to the smooth variant of the BIR term weight as RSJ (Robertson-SparckJones) weight.
Definition 2.19 RSJ term weight wRSJ. e RSJ term weight is a smooth BIR term weight.
e probability estimation is as follows:
P.t jr/ WD .df.t; r/ C 0:5/=.ND.r/ C 1/;
P.t j Nr/ WD ..df.t; c/ C 1/   .df.t; r/ C 0:5//=..ND.c/ C 2/   .ND.r/ C 1//.
Inserting these estimates into the BIR term weight yields the RSJ term weight:
wRSJ;F4.t; r; Nr; c/ WD
.df.t; r/ C 0:5/=.ND.r/   df.t; r/ C 0:5/
.df.t; c/ df.t; r/C0:5/=..ND.c/ ND.r//   .df.t; c/ df.t; r// C 0:5/
e traditional notation is: rt WD df.t; r/; nt WD df.t; c/; N WD ND.c/; R WD ND.r/.
2.3.6 ON THEORETICAL ARGUMENTS FOR 0:5 IN THE RSJ TERM
WEIGHT
Norbert Fuhr mentioned on several occasions that the 0:5 in the smooth variant of the BIR term
weight requires a justification. When discussing this with Stephen Robertson, he replied: “ere
is a justification; how was it again?”
Laplace’s law of succession offers a justification. Given an event that occurred mt times inLaplace law
of succession M past trials, then the probability is P.t/ D mt=M . After kt occurrences in K new trials, the
probability is:
P.t/ WD
mt C kt
M C K
34 2. FOUNDATIONS OF IR MODELS
To improve readability of formulae, we use mt and M rather than the common notation nt and
N , since for the BIR, nt D df.t; c/ is the document frequency, and N D ND.c/ is the number of
documents.
e frequency-based aggregation of evidence is obviously different from combining the
probabilities, where we have lost the information about the cardinalities of the sets involved. For
example, combine P.t/ D mt=M D 5; 000=10; 000 D 0:5 with P.t/ D kt=K D 1=2 D 0:5.
e law of succession can be expressed as a mixture. For demonstrating this, we form an
equation to search for the mixture parameter .
mt C kt
M C K
D  
mt
M
C .1   / 
kt
K
D   PM .t/ C .1   /  PK.t/
Set  D M
MCK
. is leads to:
mt C kt
M C K
D
M
M C K

mt
M
C
K
K C M

kt
K
D
mt
M C K
C
kt
M C K
It is evident that the Laplace smoothing is equivalent to a probability mixture where the mixture
parameter is the ratio  D M
M CK
, i.e., the number of past trials divided by the sum of past trials
and new trials; see also Section 2.6.1 (p. 49), Probability Mixtures.
For the smoothing of the probabilities in the RSJ weight, this means:
P.t jr/ D
0:5 C rt
1 C R
D
1
1 C R

0:5
1
C
R
R C 1

rt
R
D
0:5
1 C R
C
rt
1 C R
P.t jc/ D
1 C nt
2 C N
D
2
2 C N

1
2
C
N
N C 2

nt
N
D
1
2 C N
C
nt
2 C N
Whereas the smoothing of P.t jc/ is based on full integer counts, mt D 1 and M D 2, that are
intuitive, the smoothing for P.t jr/ is based on mt D 0:5, and this lacks intuition. e following
transformation helps to discuss the inside of the smoothing.
P.t jr/ D
0:5 C rt
1 C R
D
1=2 C 2  rt=2
2=2 C 2  R=2
D
1 C 2  rt
2 C 2  R
(2.70)
Now it is evident that the smoothing of P.t jr/ means to double the impact of counts in relevant
documents. is means we can view fractional values of past occurrences as stretching out the
new occurrences. An alternative smoothing based on four virtual documents, two of which are
relevant and two are non-relevant, has been discussed in Amati [2009], de Vries and Roelleke
[2005]. Each term occurs in one relevant and one non-relevant document. en, the smoothing
is as follows:
P.t jr/ D
1 C rt
2 C R
P.t jc/ D
2 C nt
4 C N
Whatever the smoothing, P.t jr/ and P.t jc/ need to be well-defined for missing evidence. e
0:5 in the RSJ weight makes the weight safe for missing relevance information (empty sets). We
have reviewed options to explain where the 0:5 comes from.ereby, we discussed the relationship
between the Laplace law of succession and probability mixtures.
2.4. POISSON AND 2-POISSON 35
2.3.7 SUMMARY
We have discussed the foundations of BIR. We have made explicit the assumptions that make
BIR an instance of the PRF. We defined the term weight wBIR and the score RSVBIR.
en, we reviewed the variants of the BIR term weight, the variants reflecting missing
relevance information, and term presence only. We considered the smooth variants of the BIR
term weights, and we discussed theoretical arguments to explain the “0:5-smoothing” applied in
the RSJ weight.
e BIR can be viewed as a special case of the next model, the Poisson model.
2.4 POISSON AND 2-POISSON
e Poisson instantiation of the PRF assumes the vector components to be term frequencies, i.e.,
Ed D .f1; : : : ; fn/, where fi 2 f0; 1; 2; : : :g.
e Poisson model seems to be less known and popular than the BIR and the BM25 model,
though the scientific literature offers deep insights into using Poison for IR: e.g., Margulis [1992],
N-Poisson Document Modeling, Church and Gale [1995], Inverse Document Frequency: A
Measure of Deviations from Poisson.
e main purpose for dedicating a section to the Poisson model is to consolidate the role of
Poisson for IR. Also, there is the aim to demystify the Poisson probability; some research students
resign when hearing “Poisson.” Engaging with the foundations of the Poisson probability is useful:
1. e Poisson model is next to the BIR model the natural instantiation of a PRF model; the
BIR model is a special case of the Poisson model.
2. e 2-Poisson probability is arguably the foundation of the BM25-TF quantification
Robertson and Walker [1994], “Some Simple Effective Approximations to the 2-Poisson
Model.”
3. e Poisson probability is a model of randomness. Divergence from randomness
(DFR), Section 2.8.1 (p. 63), is based on the probability P.t 2 d jcollection/ D P.tfd >
0jcollection/. e probability P.tfd jcollection/ can be estimated by a Poisson probability.
4. e Poisson parameter .t; c/ D tfc=df.t; c/, i.e., the average number of term occurrences,
relates Document-based and Location-based probabilities.
avgtf.t; c/  PD.t jc/ D .t; c/ D avgdl.c/  PL.t jc/
We refer to this relationship as Poisson bridge since the average term frequency is the pa-
rameter of the Poisson probability; Section 3.7 (p. 93), Poisson Bridge.
5. e Poisson model yields a foundation of TF-IDF; Section 3.9.2 (p. 98), TF-IDF and
Poisson.
36 2. FOUNDATIONS OF IR MODELS
6. e Poisson bridge helps to relate TF-IDF and LM; Section 3.9.4 (p. 101), TF-IDF and
LM.
2.4.1 POISSON PROBABILITY
Definition 2.20 PoissonProbability. Let t an event that occurs in average t times. e Pois-
son probability is:
PPoisson;t .k/ WD
kt
kŠ
 e t (2.71)
Example 2.21 Poisson Probability. For example, the probability that k D 4 sunny days occur
in a week, given the average  D p  n D 180=360  7 D 3:5 sunny days per week, is:
PPoisson;D3:5.k D 4/ D
.3:5/4
4Š
 e 3:5  0:1888 (2.72)
For small n, the Poisson probability is quite different from the binomial probability (ex-
ample p. 39). For n >> kt , and t D n  pt , the Poisson probability approximates the binomial
probability:
PPoisson;t .kt /  Pbinomial;n;pt .kt /
Details regarding this approximation are discussed in Section 2.4.6 (p. 40), Relationship between
Poisson and Binomial Probability.
2.4.2 POISSON ANALOGY: SUNNY DAYS AND TERM OCCURRENCES
To further make Poisson familiar, we underline the parallel between sunny days and term occur-
rences.
• ksunny days correspond to kt locations.
• A holiday h of length NDays.h/ corresponds to a document d of length NLocations.d/.
• e single event probability psunny corresponds to the single event probability pt .
Figure 2.7 shows a table to illustrate the analogy between the scenario “sunny days in a
holiday,” and the scenario “term occurrences in a document.” e analogy helps to relate to the
single event probabilities and averages involved. e last row indicates other event spaces.
2.4. POISSON AND 2-POISSON 37
Figure 2.7: “Sunny Days in a Holiday” and “Term Locations in a Document”.
2.4.3 POISSON EXAMPLE: TOY DATA
Example 2.22 Poisson: SyntheticData. Let u be a collection, and let Du be the respective set
of documents. Let t be a term, and let it occur in nD.t; u/ D1,000 documents. Let nD.t; u; kt /
denote the number of documents in which t occurs kt times. Let the within-document term
frequencies be distributed as follows:
kt 0 1 2 3 4 5 6 7 8 9 10
P
nD.t; u; kt / 4; 000 600 200 100 50 30 10 7 1 1 1 5; 000
nL D kt  nD 0 600 400 300 200 150 60 49 8 9 10 1; 786
e term occurs once in 600 documents, twice in 200 documents, etc. e total oc-
currence is tfu D nL.t; u/ D 1; 786. e average frequency over t-documents is avgtf.t; u/ D
nL.t; u/=nD.t; u/ D1,786/1,000 D 1.786.
e average frequency over all documents is .t; u/ D nL.t; u/=ND.u/ D 1; 786=5; 000 D
0:3572.
38 2. FOUNDATIONS OF IR MODELS
e following table shows the observed probabilities and the Poisson probabilities.
kt 0 1 2 3 4 5 6 7 8 9 10
elite documents
nD.t; u; kt / 600 200 100 50 30 10 7 1 1 1 1; 000
Pobs.kt / 0 0:600 0:200 0:100 0:050 0:030 0:010 0:007
nL 600 400 300 200 150 60 49 8 9 10 1; 786
PPoisson.kt / 0:168 0:299 0:267 0:159 0:071 0:025 0:008 0:002 1:786
all documents
nD.t; u; kt / 4; 000 600 200 100 50 30 10 7 1 1 1 5; 000
Pobs.kt / 0:800 0:120 0:040 0:020 0:010 0:006 0:002 0:0014
nL 0 600 400 300 200 150 60 49 8 9 10 1; 786
PPoisson.kt / 0:700 0:250 0:045 5e   03 5e   04 3e   05 2e   06 1e   07 0:3572
e first Poisson probability is based on avgtf.t; u/ D1,786/1,000, i.e., this estimate ex-
cludes the documents in which the term does not occur. e second Poisson probability is based
on .t; u/ D1,786/5,000=0.3572, the average over all documents.
For the second Poisson probability, the main characteristics is that PPoisson;.kt D 1/ is
greater than the observed probability, whereas for kt > 1, the Poisson probability decreases faster
than the observed probability. e tail of the Poisson probability is “too thin.”
2.4.4 POISSON EXAMPLE: TREC-2
To showcase observed versus Poisson probabilities for real data, consider the statistics for “spy,”
“africa,” and “control” taken from collection TREC-2:
Example 2.23 Poisson: TREC-2.
k t 0 1 2 3 4 5 6 7 8 9 10
nD
n L
spy avgtf = 3, 551/1, 997 = 1.79
nD 740, 614 1, 309 320 137 107 54 32 17 12 4 5 1, 997
n L 0 1, 309 640 411 428 270 192 119 96 36 50 3, 551
P obs(k) 0.9973 0.0018 0.0004 0.0002 0.0001 0.0001 :::
P Poisson .k/ 0.9952 0.0048 0.0000 0.0000 0.0000 0.0000 :::
africa avgtf = 19, 681/8, 533 = 2.31
nD 734, 078 4, 584 1, 462 809 550 345 271 182 137 105 88 8, 533
n L 0 4, 584 2, 924 2, 427 2, 200 1, 725 1, 626 1, 274 1, 096 945 880 19, 681
P obs(k) 0.9885 0.0062 0.0020 0.0011 0.0007 0.0005 :::
P Poisson(k) 0.9738 0.0258 0.0003 0.0000 :::
control avgtf = 204, 888/111, 830 = 1.82
nD 630, 781 69, 990 21, 259 8, 836 4, 444 2, 747 1, 625 1, 113 795 550 471 111, 830
n L 0 69, 990 42, 518 26, 508 17, 776 13, 735 9, 750 7, 791 6, 360 4, 950 4, 710 204, 088
P obs(k) 0.8494 0.0942 0.0286 0.0119 0.0060 0.0037 0.0022 0.0015 :::
P Poisson(k) 0.7597 0.2088 0.0287 0.0026 0.0002 0.0000 :::
e statistics for the three terms illustrate the same effect demonstrated for toy data. e
Poisson probability spends its mass on small k, whereas the observed probability assigns more
mass to larger k. e effect is observable for rare (“spy” is a rare term), medium frequent (“africa”),
and relatively frequent (“control”) terms.e numerical example underlines that the Poisson prob-
ability fails to reflect the burstiness of terms, i.e., there are more documents in which term occur-
rences co-occur, than the Poisson probability tells.
2.4. POISSON AND 2-POISSON 39
e Poisson model could be used to estimate the document probability P.d jq; r/ as
the product of the probabilities of the within-document term frequencies kt . Let kt WD tfd WD
nL.t; d/ be the total count of locations at which term t occurs. en, the starting point of the
Poisson model is:
P.d jq; r/ D P. Ed jq; r/ D
Y
t
P.kt jq; r/ (2.73)
Note that the product is over all terms, not just over the document terms. For non-document
terms, kt D 0.
e Poisson probability is an approximation of the binomial probability. We review the
justification of this approximation in the following section. Readers familiar with Poisson may
want to continue at Definition 2.20 (p. 36), Poisson Probability.
2.4.5 BINOMIAL PROBABILITY
Assume we draw balls (events) from an urn u. e set of events is non-distinct, i.e., some events
occur several times. e probability to draw the event t is P.t ju/ D n.t;u/
N.u/
, where n.t; u/ is the
number of occurrences of event t in urn u, and N.u/ is the number of elements (balls).
For example, the probability of a sunny day at the east coast of England is psunny WD
P.sunnyjeast-coast/ D 0:5. We keep in mind that this probability could be based on the statis-
tics over the past 10 years, i.e., psunny D1,800/3,600, or on the statistics over just a fortnight,
psunny D 7=14. In a more general fashion, we may write P.sunnyjM/ or PM .sunny/, where M
indicates the model (and space) used to estimate the probability.
Given the single event probability pt , what is the probability that in N trials the
event t occurs kt times? is probability is denoted as PN;pt .kt /, and the binomial probability
Pbinomial;N;pt .kt / is the standard estimate.
Definition2.24 BinomialProbability. Let t be an event that occurs with probability pt . Let N
be the number of trials. e binomial probability is:
Pbinomial;N;pt .kt / WD
 
N
kt
!
 p
kt
t  .1   pt /
.N  kt / (2.74)
Example 2.25 BinomialProbability. For example, the probability that k D 4 sunny days occur
in N D 7 days, given the single event probability p D 180
360
D 0:5, is:
Pbinomial;N D7;pD0:5.k D 4/ D
 
7
4
!
 0:54  .1   0:5/.7 4/  0:2734 (2.75)
40 2. FOUNDATIONS OF IR MODELS
2.4.6 RELATIONSHIP BETWEEN POISSON AND BINOMIAL
PROBABILITY
e Poisson probability is an approximation of the binomial probability.
After setting the single event probability as p D =n, the binomial probability becomes:
Pbinomial;n;p.k/ D
 
n
k
!



n
k


1  

n
.n k/

k
kŠ
 e 
e next expression illustrates details of the approximation:
n  .n 1/  : : :  .n  kC 1/
kŠ

 
k
nk
!


n   
n
 k


1  

n
n
e approximation involves the Euler convergence.
lim
n!1

1  

n
n
D e  (2.76)
Moreover, the approximation assumes the following:
n  .n   1/  : : :  .n   k C 1/
.n   /k
 1
e relationship between binomial and Poisson probability can be viewed differently. In-
stead of decomposing .1   =n/.n k/, we could approximate the whole expression to be e .
1  

n
.n k/
 e  n >> k
en, the second approximation is as follows:
n  .n   1/  :::  .n   k C 1/
nk
 1
e main fact is that Poisson approximates the binomial probability. Which inner approximation
is employed to explain the overall approximation is a secondary issue, but it is useful to reflect
the details, in particular when deriving new probability functions that capture the dependence of
events.
2.4.7 POISSON PRF
Back to the PRF, we review the derivation steps leading from the odds of relevance, O.r jd; q/,
toward the Poisson term weight, wPoisson, and the retrieval status value, RSVPoisson.
After assuming that the product over non-query feature probabilities does not affect the
ranking, see Equation 2.57 (p. 26), non-query term assumption, the probabilistic odds of relevance
2.4. POISSON AND 2-POISSON 41
are rank-equivalent (proportional) to the product of the fractions of the frequency probabilities
of the query terms:
O.r jd; q/
rank
D
Y
t2q
P.kt jr/
P.kt j Nr/
; kt WD tfd WD nL.t; d/ (2.77)
Splitting the product into document and non-document terms (Equation 2.59 (p. 26), PRF term
frequency split) yields:
O.r jd; q/
rank
D
Y
t2d\q
P.kt jr/
P.kt j Nr/

Y
t2qnd
P.0jr/
P.0j Nr/
(2.78)
Compare this equation to Equation 2.60 (p. 29), BIR Term Frequency Split.
Next, we insert the Poisson probability, PPoisson;.kju/ D 
k
kŠ
 e .
O.r jd; q/
rank
D
Y
t2d\q
"
.t; d; r/kt
.t; d; Nr/kt

e .t;d;r/
e .t;d; Nr/
#

Y
t2qnd
e .t;d;r/
e .t;d; Nr/
(2.79)
e meaning of the Poisson probability is as follows:
PPoisson;.t;d;u/.kt /: probability to observe kt occurrences of term t in dl trials.
.t; d; u/ D dl  PL.t ju/ is the number of occurrences expected in a document with
length dl.
From the relationship between the binomial and Poisson probability, n  pt D t is the
Poisson parameter. erefore, for the PRF, the Poisson parameter is dl  PL.t ju/ D .t; d; u/.
It remains to rewrite equation 2.79 to group the expressions to obtain a product over query
terms.
O.r jd; q/
rank
D
Y
t2d\q
"
.t; d; r/kt
.t; d; Nr/kt
#

Y
t2q
e .t;d;r/
e .t;d; Nr/
(2.80)
is transformation is reminiscent of the derivation of BIR, Equation 2.61 (p. 29), BIR O.r jd; q/.
e main difference is that for BIR, the product over query terms was independent of the
document, whereas for Poisson, the document length is in a parameter in the product over
query terms; for example, for relevant documents, .t; d; r/ D dl  PL.t jr/. Only if we assume
.t; d; r/ D .t; d; Nr/, then the second product drops out (as it did for the BIR case).
Finally, we apply the natural logarithm.
log O.r jd; q/ rankD
24 X
t2d\q
kt  log

.t; d; r/
.t; d; Nr/
35CX
t2q
..t; d; Nr/   .t; d; r// (2.81)
42 2. FOUNDATIONS OF IR MODELS
Inserting the Poisson parameters, .t; d; r/Ddl  PL.t jr/, and .t; d; Nr/Ddl  PL.t j Nr/, yields:
log O.r jd; q/ rankD
24 X
t2d\q
kt  log

PL.t jr/
PL.t j Nr/
35C dl X
t2q
.PL.t j Nr/   PL.t jr// (2.82)
By inserting the Poisson bridge, Section 3.7 (p. 93), the Document-based term probability be-
comes explicit:
PL.t ju/ D
avgtf.t; u/
avgdl.u/
 PD.t ju/ where uDr or uD Nr (2.83)
is will be exploited in Section 3.9.2 (p. 98), TF-IDF and Poisson.
2.4.8 TERM WEIGHT AND RSV
e Poisson term weight can be formally defined as follows.
Definition 2.26 Poisson term weight wPoisson.
wPoisson.t; d; q; r; Nr/ WD TF.t; d/  log
.t; d; r/
.t; d; Nr/

D TF.t; d/  log PL.t jr/
PL.t j Nr/

(2.84)
eweight is based on the averages in relevant and non-relevant documents; the statistics in
non-relevant documents are often approximated by the average in all documents (the collection).
Also, the weight is shown using TF.t; d/; this is to emphasize that we allow for different
quantifications of the event occurrences. For the Poisson model which assumes independence, the
setting is TF.t; d/ D tfd , i.e., TF is the total term frequency. However, fromTF-IDF and BM25,
we know that this independence assumption leads to relatively poor retrieval quality. erefore,
we make the Poisson model less naive by allowing for a smarter TF quantification, smarter in the
sense that it reflects the dependence between multiple term occurrences.
e Poisson RSV is defined as the sum of Poisson term weights plus the length normaliza-
tion.
Definition 2.27 Poisson retrieval status value RSVPoisson.
RSVPoisson.d; q; r; Nr/ WD
24 X
t2d\q
wPoisson.t; d; q; r; Nr/
35C len_normPoisson (2.85)
2.4. POISSON AND 2-POISSON 43
Inserting wPoisson yields the RSV in decomposed form.
RSVPoisson.d; q; r; Nr/ D
24 X
t2d\q
TF.t; d/  log PL.t jr/
PL.t j Nr/
35C dl X
t
.PL.t j Nr/   PL.t jr// (2.86)
e Poisson bridge can be used to replace the Location-based term probability PL.t ju/ byPoisson
bridge an expression that involves the Document-based term probability PD.t ju/.
avgtf.t; u/  PD.t ju/ D .t; u/ D avgdl.u/  PL.t ju/ (2.87)
is leads to:
RSVPoisson.d; q; r; Nr/ D
24 X
t2d\q
TF.t; d/  log avgtf.t; r/  avgdl. Nr/  PD.t jr/
avgtf.t; Nr/  avgdl.r/  PD.t j Nr/
35C len_normPoisson
(2.88)
Because of avgtf.t; u/  PD.t ju/ D .t; u/, the RSV can be formulated as follows:
RSVPoisson.d; q; r; Nr/ D
24 X
t2d\q
TF.t; d/  log avgdl. Nr/  .t; r/
avgdl.r/  .t; Nr/
35C len_normPoisson (2.89)
is equation concludes the discussion of the Poisson model. We focused in this section mainly
on Poisson as an instance of the PRF, however, it is evident that the Poisson model has TF-IDF
ingredients; see also Section 3.9.2 (p. 98), TF-IDF and Poisson.
2.4.9 2-POISSON
e main motivation for a multi-dimensional Poisson probability is to mix the probabilities for
different averages. e following example illustrates this.
Example 2.28 2-Poisson: How many cars to expect?
How many cars are expected on a given commuter car park?
Approach 1: In average, there are 700 cars per week. e daily average is:  D 700=7 D 100
cars/day.
en, PD100.k/ is the probability that there are k cars wanting to park on a given day.
is estimation is less accurate than an estimation based on a 2-dimensional model. is
is because Mo-Fr are the busy days, and on weekends, the car park is nearly empty. is means
that a distribution such as (130, 130, 130, 130, 130, 25, 25) is more likely than 100 each day.
Approach 2: In a more detailed analysis, we observe 650 cars Mon–Fri (work days) and 50
cars Sat–Sun (weekend days). e averages are: 1 D 650=5 D 130 cars/work-day, 2 D 50=2 D
25 cars/we-day.
44 2. FOUNDATIONS OF IR MODELS
en, P1D5=7;1D130;2D2=7;2D25.k/ is the 2-dimensional Poisson probability that there
are k cars looking for a car park.
e main reason to include in this book the 2-Poisson probability is that it is viewed as a
motivation for the BM25-TF quantification, Robertson and Walker [1994], “Simple Approxi-
mations to the 2-Poisson Model.” In an exchange with Stephen Robertson, he explained:
“e investigation into the 2-Poisson probability motivated the BM25-TF quantifi-
cation. Regarding the combination of TF and RSJ weight in BM25, TF can be viewed
as a factor to reflect the uncertainty about whether the RSJ weight wRSJ is correct; for
terms with a relatively high within-document TF, the weight is correct; for terms with
a relatively low within-document TF, there is uncertainty about the correctness. In
other words, the TF factor can be viewed as a weight to adjust the impact of the RSJ
weight.”
Regarding the interpretation of TF quantification ranging between 1 and tfd , the setting
TF.t; d/ D 1 reflects subsumption of event occurrences, and TF.t; d/ D tfd reflects independence.
e BM25-TF setting, TFBM25.t; d/ D tfd =.tfd C Kd / is related to assuming semi-subsumption;
see also Section 2.1.7 (p. 17).
Given the effect of 2-Poisson probabilities and the notion of semi-subsumed events, there
is an interesting relationship to explore as part of future research. For the scope of this book, we
briefly look at the 2-Poisson probability.
e main idea of the 2-Poisson probability is to combine (interpolate) two Poisson proba-
bilities. e following equation shows the combination:
P2-Poisson;1;2;.kt / WD  

kt
1
kt Š
 e 1 C .1   / 

kt
2
kt Š
 e 2 (2.90)
e 2-Poisson probability function peaks at 1 and 2. erefore, the distribution function has
a plateau. For values k < 1, the probability is small. en, near k  1, the distribution func-
tion steps up to the plateau. e next step is at 2, from where the plateau is approximately
  P1.1/ C .1   /  P2.2/.
Regarding the averages 1 and 2, for example, one average could be over all documents,
and the other average could be over an elite set of documents (e.g., the documents that contain
at least one query term).
2.4.10 SUMMARY
We have discussed the foundations of the Poisson Model. e main aspect is that the Pois-
son model interprets the document event as a sequence of term frequencies: P.d jq; c/ DQ
t PPoisson;.t;d;c/.kt jq; c/. Here, kt D tfd is the term frequency of term t in document d , and
.t; d; c/ is the average term frequency, the term frequency expected for the document length dl.
2.5. BM25 45
Stepwise, we derived the RSVPoisson, and made explicit the components that relate to IDF, bursti-
ness (avgtf.t; c/), and document length normalization.e Poissonmodel and its main parameter,
.t; d; c/ D dl  PL.t jc/, is a binding link between IR models.
We also briefly touched on N-Poisson and 2-Poisson, and the latter is viewed as a motiva-
tion for the BM25-TF quantification we meet again in the next section.
2.5 BM25
e ingredients of BM25 have been prepared in Section 2.1 (p. 9), TF-IDF, and in Section 2.3
(p. 29), BIR. Central to BM25 (see Robertson et al. [1994], Robertson and Walker [1994],
Robertson et al. [1995, 1998], Robertson and Zaragoza [2009] and related publications) is the
BM25-TF quantification.
2.5.1 BM25-TF
Definition 2.29 BM25-TF.
TFBM25;K.t; d/ WD
tfd
tfd C Kd
(2.91)
Kd adjusts the shape of the quantification. It is proportional to the pivoted document length:
pivdl.d/ WD dl=avgdl.
Kd WD k1 
 
b  pivdl.d/ C .1   b/

(2.92)
e parameters k1 and b adjust the effect of the pivotization.
More generally, we require to make explicit the collection (set of documents) for which
the pivotization and parametrization shall apply. Let u be a collection. en, the BM25-TF is as
follows;
TFBM25;K.t; d; u/ WD
tfd
tfd C Kd .u/
(2.93)
en, the parameters are collection-specific, i.e., avgdl.u/, pivdl.d; u/, k1.u/ and b.u/.
2.5.2 BM25-TF AND PIVOTED TF
In Section 2.1.6 (p. 16), Other TF Variants, we related the fractional TF, TFfrac, to the pivoted
TF, TFpiv. We formalize the relationship in the next definition.
Definition 2.30 TF pivoted.
Pivotization with respect to parameter Kd :
TFpiv;K.t; d/ WD
tfd
Kd
(2.94)
46 2. FOUNDATIONS OF IR MODELS
Pivotization with respect to the pivoted document length, i.e., Kd D pivdl:
TFpiv.t; d/ WD TFpiv;pivdl.t; d/ D
tfd
pivdl
(2.95)
Relationships between TFfrac;K and TFpiv;K :
TFfrac;K.t; d/ D
TFpiv;K.t; d/
TFpiv;K.t; d/ C 1
(2.96)
Relationship between TFfrac;K and TFpiv:
TFfrac;K.t; d/ D
TFpiv.t; d/
TFpiv.t; d/ C Kd =pivdl
(2.97)
(In BM25 SIGIR tutorials, pivoted TF’s were denoted as tf0d ).
It is evident that the BM25-TF is an instantiation of the fractional TF.
TFBM25;k1;b.t; d/ D TFfrac;K.t; d/ D
tfd
tfd C Kd
D
TFpiv;K.t; d/
TFpiv;K.t; d/ C 1
(2.98)
is formulation is helpful to relate the BM25-TF to probability theory; see also Section 2.1.7
(p. 17), Semi-subsumed Events Occurrences.
2.5.3 BM25: LITERATURE AND WIKIPEDIA END 2012
Since there are many formulations and interpretations of what is BM25, it is worthwhile to briefly
document here what we find at the end of 2012 on Wikipedia, http://en.wikipedia.org/
wiki/Okapi_BM25.
Given a query Q, containing keywords q1; : : : ; qn, the BM25 score of a document
Dis:
score.D; Q/ D
nX
iD1
IDF.qi / 
f .qi ; D/  .k1 C 1/
f .qi ; D/ C k1  .1   b C b 
jDj
avgdl /
;
where f .qi ; D/ is qi ’s term frequency in the document D, jDj is the length of the
document D in words, and avgdl is the average document length in the text collection
from which documents are drawn. k1 and b are free parameters, usually chosen, in
absence of an advanced optimisation, as k1 2 Œ1:2; 2:0 and b D 0:75. IDF.qi / is the
IDF (inverse document frequency) weight of the query term qi . It is usually computed
as:
IDF.qi / D log
N   n.qi / C 0:5
n.qi / C 0:5
;
2.5. BM25 47
where N is the total number of documents in the collection, and n.qi / is the number
of documents containing qi .
ere are several interpretations for IDF and slight variations on its formula. In the
original BM25 derivation, the IDF component is derived from the Binary Indepen-
dence Model.
is formulation of BM25 is also given in Li [2011], page 16, as a foundation of learning
to rank.
ere are two differences between this formulation and the formulation given in this book.
1. Instead of IDF, we employ the RSJ weight, wRSJ. For missing relevance information,
wRSV.t/  IDF.t/.
2. e factor .k1 C 1/ is omitted (mostly). As long as k1 is a constant, independent of term
and document, the factor in the numerator is ranking invariant.
2.5.4 TERM WEIGHT AND RSV
e BM25 term weight can be formally defined as follows.
Definition 2.31 BM25 term weight wBM25.
wBM25;k1;b;k3.t; d; q; r; Nr/ WD
X
t2d\q
TFBM25;k1;b.t; d/  TFBM25;k3.t; q/  wRSJ.t; r; Nr/ (2.99)
e parameters k1, b, and k3 allow for adjusting the effect of the pivotization.
For the RSJ term weight, see Definition 2.19 (p. 33), wRSJ.
e BM25RSV is defined as the sum of BM25 termweights plus the length normalization.
Definition 2.32 BM25 retrieval status value RSVBM25.
RSVBM25;k1;b;k2;k3.d; q; r; Nr; c/ WD
24 X
t2d\q
wBM25;k1;b;k3.t; d; q; r; Nr/
35C len_normBM25;k2
(2.100)
ough the BM25-TF normalizes with respect to the document length, BM25 has in
addition a length normalization component separate of the term-match component; Robertson
et al. [1998]. is is:
len_normBM25;k2.d; q; c/ WD k2  ql 
avgdl.c/   dl
avgdl.c/ C dl
(2.101)
48 2. FOUNDATIONS OF IR MODELS
Long documents are suppressed (value of length norm is negative, this decreases the score),
whereas short documents are lifted (value is positive, this increases the score). is separate nor-
malization component reinforces the effect of the normalization as in the BM25-TF, where the
TF component is smaller for long and greater for short documents.
Figure 2.8 shows an overview over the symbols used.
traditional book description
notation notation
tf or tfd tfd or nL.t; d/ within-document term frequency, i.e., number of Locations at which t occurs
in d
K Kd parameter to adjust impact of tfd : Kd D k1  .b  pivdl.d/ C .1   b//
b b parameter to adjust impact of pivoted document length normalization
k1 k1 parameter to adjust impact of tfd
tf0d TFpiv;K.t; d/ normalized (pivoted) within-document term frequency
qtf or tfq tfq or nL.t; q/ within-query term frequency
k3 k3 parameter to adjust impact of tfq
w
.1/
t wRSJ.t; r; Nr/ RSJ term weight (Section 2.3.5 (p. 33), RSJ term weight)
ql ql or NL.q/ query length: number of locations in query q
dl dl or NL.d/ document length: number of locations in document d
avgdl avgdl.c/ average document length
k2 k2 parameter to adjust impact of document length normalization
Figure 2.8: BM25 Notation.
ere were variations of BM25 that show a constant used as an exponent: tf
a
d
tfad CK
a
d
. Accord-
ing to Stephen Robertson, the parameter did not stand the test of time.
2.5.5 SUMMARY
We have discussed the foundations of BM25, an instance of the PRF. e main ingredients are:
1. e TF quantification, Definition 2.29 (p. 45), TFBM25: contains the pivoted document
length.
see also Definition 2.1 (p. 10), TF_d Variants.
2. e RSJ term weight, Definition 2.19 (p. 33), wRSJ: smooth variant of the BIR term weight.
3. A document length normalization separate from the term-based match of document and
query.
e BM25-TF quantification is a fractional TF, and the latter can be expressed using the pivoted
TF. e RSJ term weight is a smooth variant of the BIR term weight, where the “0:5-smoothing”
can be explained through Laplace’s law of succession. e document length normalization, can be
2.6. LM: LANGUAGE MODELING 49
related to the derivation of the Poisson RSV, Definition 2.27 (p. 42), RSVPoisson. A long document
receives a penalty, whereas a short document receives a reward.
We are now leaving the world of document-likelihood models and move toward LM, the
query-likelihood model.
2.6 LM: LANGUAGE MODELING
2.6.1 PROBABILITY MIXTURES
Since the late 90s, language modeling (LM), Croft and Lafferty [2003], Hiemstra [2000], Ponte
and Croft [1998], is a popular retrieval model. e starting point of LM is to mix the within-
document term probability P.t jd/ and the collection-wide term probability P.t jc/. is mixture
resolves the so-called zero-probability problem when computing the probability P.qjd/ where q
is a conjunction of term events, and there exists a term t where P.t jd/ D 0. A probabilistic mix-
ture is not specific to LM; it is a general concept of probability theory.We give in the following the
general definition and an example where we compute the probability P.sunny; warm; :::jglasgow/.
Definition 2.33 Probability Mixture (General). Let three events “x; y; z,” and two condi-
tional probabilities P.zjx/ and P.zjy/ be given.
en, P.zjx; y/ can be estimated as a linear combination/mixture of P.zjx/ and P.zjy/.
P.zjx; y/  ıx  P.zjx/ C .1   ıx/  P.zjy/ (2.102)
Here, 0 < ıx < 1 is the mixture parameter.
e mixture parameters can be constant ( Jelinek-Mercer mixture), or can be set propor-
tional to the total probabilities. is weighs the conditional probabilities according to the to-
tal probabilities. Assume that the norm jxj tells how often x occurs, among N trials in total.
en: P.x/ D jxj=N and P.y/ D jyj=N . is estimation transforms the mixture parameter into
a norm-based expression: ıx WD jxjjxjCjyj .
For a conditional with n evidence events, the mixture is:
P.zjx1; x2; : : : ; xn/ D
X
i
ıi  P.zjxi /
e mixture parameter is:
ıj WD
P.xj /P
i P.xi /
Example 2.34 Probability Mixture (General). For example, let
P.sunny; warm; rainy; dry; windyjglasgow/ describe the probability that a day in Glasgow
is sunny, the next day is warm, the next rainy, and so forth. If for one event (e.g., sunny), the
50 2. FOUNDATIONS OF IR MODELS
probability were zero, then the probability of the conjunction is zero. A mixture solves the
problem. For example, mix P.xjglasgow/ with P.xjuk/ where P.xjuk/ > 0 for each event x.
en, in a week in winter, when P.sunnyjglasgow/ D 0, and for the whole of the UK, the
weather office reports 2 of 7 days as sunny, the mixed probability is:
P.sunnyjglasgow; uk/ D ı  0
7
C .1   ı/ 
2
7
e LM term weight is a mixture (interpolation) of the term probabilities P.t jd/ and
P.t jc/.
Definition 2.35 ProbabilityMixture (LM). Let P.t jd/ and P.t jc/ be two term probabilities;
P.t jd/ is the within-document probability (foreground model), and P.t jc/ is the collection-wide
probability (background model). en, P.t jd; c/ can be estimated as follows:
P.t jd; c/ D ı  P.t jd/ C .1   ı/  P.t jc/
Example 2.36 Probability Mixture (LM). For example, let P.sailingjd/ D 2=100 and
P.sailingjc/ D 100=106 be given.
P.sailingjd; c/ D ı  2
100
C .1   ı/ 
100
106
One of the beauty of LM is that the probability mixture can be easily formulated for several
models. Given a set M of models, the term (event) probability is estimated as follows:
P.t jm1; m2; : : : ; mn/ D
X
i
ıi  P.t jmi / (2.103)
We could call this a “weighted sum” approach where the weighted sum is based on the model
weights ıi and the term probabilities P.t jmi /. Xue et al. [2008], “Retrieval models for question
and answer archives,” showed how an approach with three models (sentence model, document
model, collection model) helps solving question-answering tasks (sentence retrieval).
For the purpose of this book, the formulation of LM term weights and RSV’s is for the
special case where we mix one foreground model with one background model. We introduce in
the following sections several variants of LM-based term weights and RSV’s. In one overview,
the respective term weights are as follows:
2.6. LM: LANGUAGE MODELING 51
Section 2.6.2: LM1: Basic mixture:
wLM1;ıd .t; d; q; c/ WD TF.t; q/  log .ıd  P.t jd/ C .1   ıd /  P.t jc//
Section 2.6.3: LM: LM1 normalized: Mixture divided by background probability P.t jc/:
wLM;ıd .t; d; q; c/ WD TF.t; q/  log

ıd  P.t jd/ C .1   ıd /  P.t jc/
P.t jc/

Section 2.6.4: JM-LM: LM where ı WD ıd is a constant, independent of a document.
wJM-LM;ı.t; d; q; c/ WD TF.t; q/  log

1 C
ı
1   ı

P.t jd/
P.t jc/

Section 2.6.5: Dirich-LM: LM where ıd is proportional to the document.
ıd D
jd j
jd j C 
Section 2.6.6 LM2: A formulation equivalent to wLM:
wLM2;ıd .t; d; q; c/ WD TF.t; q/    log

P.t jc/
.1   ıd /  P.t jc/ C ıd  P.t jd/

2.6.2 TERM WEIGHT AND RSV: LM1
e LM1 term weight can be formally defined as follows.
Definition 2.37 LM1 term weight wLM1. Let P.t jd/ be the within-document term proba-
bility (foreground probability). Let P.t jc/ is the within-collection term probability (background
probability). e parameter ıd is the mixture parameter; this can be proportional to the fore-
ground model (Dirichlet mixture) or a constant ( JM mixture).
e probability P.t jd; c/ is estimated via a mixture:
P.t jd; c/ WD ıd  P.t jd/ C .1   ıd /  P.t jc/ (2.104)
en, the term weight is:
wLM1;ıd .t; d; q; c/ WD TF.t; q/  log .ıd  P.t jd/ C .1   ıd /  P.t jc// (2.105)
Regarding the notation, we employ ı rather than  as the mixture parameter. is is be-
cause  is the parameter reserved for the Poisson probability ( is the average occurrence of the
event).
52 2. FOUNDATIONS OF IR MODELS
e LM1 term weight is based on
wLM1.t; d; q; c/ D TF.t; q/  log P.t jd; c/
P.t jd; c/ is greater for document terms than for non-document terms, since P.t jd/ > 0 for docu-
ment terms, and P.t jd/ D 0 for non-document terms.us, the product over themixed probabil-
ities of the query terms is greater for documents that contain the query terms than for documents
that do not contain the query terms. Assuming independence of term events, the product over
the term probabilities P.t jd/ is equal to P.qjd/. Formally, the term independence assumption
of LM is:
Assumption 3 Language Modeling Independence (Dependence) Assumption:
P.qjd; c/ 
Y
t IN q
P.t jd; c/ 
Y
t2q
P.t jd; c/TF.t;q/ (2.106)
e notation t IN q views q is a sequence of terms (multiple occurrences of the same
term), whereas “t 2 q” views q as a set, and the multiple occurrences are reflected in the ex-
ponent TF.t; q/. Similarly to the discussion regarding the meaning of TF.t; d/, Section 2.1.1
(p. 10), TF-Variants, namely that certain TF variants reflect a dependence assumption, TF.t; q/
can be interpreted as the dependence between the term occurrences in a document.
Documents that do not contain the rare query terms are penalized strongly, since for rare
terms, P.t jc/ is small, and thus, P.qjd; c/ is small. RSVLM1 is the logarithm of P.qjd; c/.
e LM1 RSV is defined as the sum of LM1 term weights.
Definition 2.38 LM1 retrieval status value RSVLM1.
RSVLM1;ıd .d; q; c/ WD
X
t2q
wLM1;ıd .t; d; q; c/ (2.107)
Inserting wLM1 yields the RSV in decomposed form.
RSVLM1;ıd .d; q; c/ D
X
t2q
TF.t; q/  log .ıd  P.t jd/ C .1   ıd /  P.t jc// (2.108)
For the set-based decomposition, TF.t; q/ reflects the multiple occurrences of t in q.
2.6.3 TERM WEIGHT AND RSV: LM (NORMALIZED)
e subscript LM1 is used to refer to the non-normalized term weight and RSV. RSVLM is the
normalized RSV, where RSVJM-LM is the normalization of the JM mixture, and RSVDirich-LM is
the normalization of the Dirichlet mixture.
2.6. LM: LANGUAGE MODELING 53
e normalization of LM1 is based on dividing P.qjd; c/ by the document-independent
constant P.qjc/, where P.qjc/ D
Q
t IN q P.t jc/ is the query probability.
e LM term weight can be formally defined as follows.
Definition 2.39 LM term weight wLM.
wLM;ıd .t; d; q; c/ WD TF.t; q/  log

.1   ıd /  P.t jc/ C ıd  P.t jd/
P.t jc/

(2.109)
e LM RSV is defined as the sum of LM term weights.
Definition 2.40 LM retrieval status value RSVLM.
RSVLM;ıd .d; q; c/ WD
X
t2q
wLM;ıd .t; d; q; c/ (2.110)
Inserting wLM yields the RSV in decomposed form.
RSVLM;ıd .d; q; c/ D
X
t2q
TF.t; q/  log

.1   ıd /  P.t jc/ C ıd  P.t jd/
P.t jc/

(2.111)
e relationship between LM and LM1 is based on dividing P.qjd; c/ by P.qjc/, a con-
stant, document-independent factor.
log P.qjd; c/
P.qjc/
D log P.qjd; c/   log P.qjc/
is leads to the following equation:
RSVLM.d; q; c/ D RSVLM1.d; q; c/  
X
t2q
TF.t; q/  log P.t jc/ (2.112)
RSVLM is equal to the difference between RSVLM1 and the document-independent factorP
t2q log P.t jc/.
54 2. FOUNDATIONS OF IR MODELS
2.6.4 TERM WEIGHT AND RSV: JM-LM
In addition, for constant ı, the score can be divided by
Q
t IN q.1   ı/. is leads to the following
equation, Hiemstra [2000]:
P.qjd; c/
P.qjc/ 
Q
t IN q.1   ı/
D
Q
t IN q P.t jd; c/Q
t IN q Œ.1   ı/  P.t jc/
D
Y
t2d\q

1 C
ı
1   ı

P.t jd/
P.t jc/
TF.t;q/
(2.113)
is transformation is helpful since the product over query terms is reduced to the product over
terms that occur in document and query.
e JM-LM term weight can be formally defined as follows.
Definition 2.41 JM-LM term weight wJM-LM.
wJM-LM;ı.t; d; q; c/ WD TF.t; q/  log

1 C
ı
1   ı

P.t jd/
P.t jc/

(2.114)
e JM-LM RSV is defined as the sum of JM-LM term weights.
Definition 2.42 JM-LM retrieval status value RSVJM-LM.
RSVJM-LM;ı.d; q; c/ WD
X
t2d\q
wJM-LM;ı.t; d; q; c/ (2.115)
Inserting wJM-LM yields the RSV in decomposed form.
RSVJM-LM;ı.d; q; c/ WD
X
t2q
TF.t; q/  log

1 C
ı
1   ı

P.t jd/
P.t jc/

(2.116)
2.6.5 TERM WEIGHT AND RSV: DIRICH-LM
eDirichlet-based LMapplies a document-dependent (foreground-dependent)mixture param-
eter such as ıd D dl=.dl C /. e RSV is based on P.qjd/=P.q/.
e Dirich-LM term weight can be formally defined as follows.
Definition 2.43 Dirich-LM term weight wDirich-LM.
wDirich-LM;.t; d; q; c/ WD TF.t; q/  log


Cjd j
C
jd j
jd jC

P.t jd/
P.t jc/

(2.117)
e usage of jd j underlines that any normalization of the document d could be used to
2.6. LM: LANGUAGE MODELING 55
parametrize the mixture. Often, the normalization is jd j D dl D NL.d/, i.e., the normalization
is with respect to the document length.
e Dirich-LM RSV is defined as the sum of Dirich-LM term weights.
Definition 2.44 Dirich-LM retrieval status value RSVDirich-LM.
RSVDirich-LM;.d; q; c/ WD
X
t2q
wDirich-LM;.t; d; q; c/ (2.118)
Inserting wDirich-LM yields the RSV in decomposed form.
RSVDirich-LM.d; q; c; / WD
X
t2q
TF.t; q/  log


 C jd j
C
jd j
jd j C 

P.t jd/
P.t jc/

(2.119)
Finally, we show some transformations that make the length component explicit.
For TF.t; q/ D tfq , and by isolating =. C jd j/, for the RSV, we obtain:
RSVDirch-LM;.d; q; c/ D
X
t2q
tfq  log


 C jd j


1 C
jd j


P.t jd/
P.t jc/

(2.120)
Next, we make use of ql D
P
t tfq . Also, we choose jd j D dl. Moreover, P.t jd/ D tfd =dl
is the common estimate for the within-document term probability.
RSVDirch-LM;.d; q; c/ D
24 X
t2d\q
tfq  log

1 C
tfd


1
P.t jc/
35C ql  log 
 C dl
(2.121)
What is the setting of ? One option is:  D avgdl, i.e., the Dirichlet parameter is equal to the
average document length. en RSVDirich-LM becomes:
RSVDirch-LM;Davgdl.d; q; c/ D
24 X
t2d\q
tfq  log

1 C
tfd
tfc
 ND.c/
35C ql  log avgdl.c/
avgdl.c/ C dl
In the term matching component, avgdl  P.t jc/ D tfc=ND.c/ D .t; c/ is the average term fre-
quency (the expected term frequency in a document of average length), the parameter we know
from Section 2.4 (p. 35), Poisson. In the length normalization component, avgdl=.avgdlCdl/ is
reminiscent of the document length normalization component of BM25; Equation 2.101 (p. 47),
len_normBM25.
56 2. FOUNDATIONS OF IR MODELS
2.6.6 TERM WEIGHT AND RSV: LM2
In Section 3.9.4 (p. 101), TF-IDF and LM, we discuss that TF-IDF is based on the negative
logarithm of PD.t jc/TF.t;d/, and that this is an approximation of an expression dual to the LM
term weight.
  log PD.t jc/TF.t;d/    log

PL.t jc/
PL.t jq; c/
TF.t;d/
D   log

PL.t jc/
PL.t jc/=PD.t jc/
TF.t;d/
is relationship between TF-IDF and LM, and also the modeling of LM in a probabilistic
logical framework, motivated the definition of an LM variant referred to as LM2. Essentially,
LM2 is the same as LM, we just swap numerator and denominator in the normalization, i.e., we
apply
log P.t jd; c/
P.t jc/
D   log P.t jc/
P.t jd; c/
e LM2 term weight can be formally defined as follows.
Definition 2.45 LM2 term weight wLM2.
wLM2;ıd .t; d; q; c/ WD TF.t; q/    log

P.t jc/
.1   ıd /  P.t jc/ C ıd  P.t jd/

(2.122)
e LM2 RSV is defined as the sum of LM2 term weights.
Definition 2.46 LM2 retrieval status value RSVLM2.
RSVLM2;ıd .d; q; c/ WD
X
t2q
wLM2;ıd .t; d; q; c/ (2.123)
Inserting wLM2 yields the RSV in decomposed form.
RSVLM2;ıd .d; q; c/ D
X
t2q
TF.t; q/   log

P.t jc/
.1   ıd /  P.t jc/ C ıd  P.t jd/

(2.124)
e relationship between LM2 and LM1 is expressed similarly to Equation 2.112 (p. 53),
LM and LM1.
RSVLM2.d; q; c/ D
"X
t2q
TF.t; q/    log P.t jc/
#
C RSVLM1.d; q; c/ (2.125)
Next, we consider the JM-LM2 term weight, and this is followed by the Dirich-LM2 term
weight.
2.6. LM: LANGUAGE MODELING 57
e JM-LM2 term weight can be formally defined as follows.
Definition 2.47 JM-LM2 term weight wJM-LM2.
wJM-LM2;ı.t; d; q; c/ WD TF.t; q/    log

.1   ı/  P.t jc/
.1   ı/  P.t jc/ C ı  P.t jd/

(2.126)
e Dirichlet-based term weight can be obtained by inserting ıd D dldlC into the general
LM term weight, equation 2.122.
wDirich-LM2;.t; d; q; c/ D TF.t; q/    log
. C dl/  P.t jc/
  P.t jc/ C dl  P.t jd/
e Dirich-LM2 term weight can be formally defined as follows.
Definition 2.48 Dirich-LM2 term weight wDirich-LM2.
wDirich-LM2;.t; d; q; c/ WD TF.t; q/    log

  P.t jc/ C dl  P.t jc/
  P.t jc/ C dl  P.t jd/

(2.127)
In this formulation, in the numerator, the expression dl  P.t jc/ D EŒtfd , is the expected
within-document term frequency; see also Section 2.4 (p. 35), Poisson. In the denominator, the
expression dl  P.t jd/ D tfd , is the actual within-document term frequency. is shows that
Dirichlet-based LM measures the difference between the expected term frequency, dl  PL.t jc/,
and the actual term frequency, tfd .
2.6.7 SUMMARY
We have discussed the foundations of LM. We referred to the basic mixture as LM1, to distin-
guish it from three normalizations: the general normalization P.qjd/=P.q/, and the specific nor-
malizations JM-LM and Dirich-LM. JM-LM is based on P.qjd/=P.q/=
Q
t IN q.1   ı/, where
ı is a constant. Dirich-LM is based on P.qjd/=P.q/, where ıd D jd jjd jC is the mixture parameter
proportional to the document length.
ere are several research issues regarding LM:
1. Estimation of P.t jd/ and P.t jc/:
e intuitive estimates are: PL.t jd/ WDnL.t; d/=NL.d/ D tfd =dl and PL.t jc/ WD
nL.t; c/=NL.c/ D tfc=.
P
d dl/.
For P.t jc/, another estimate is the Document-based estimate: PD.t jc/ WD
nD.t; c/=ND.c/ D df.t; c/=ND.c/. is needs careful consideration since then, P.t jd/ and
P.t jc/ are based on different event spaces.
58 2. FOUNDATIONS OF IR MODELS
Since the Document-based estimate is the one used in IDF,  log PD.t jc/, this estimate
motivates one to view LM as a probabilistic interpretation of TF-IDF; see also Section 3.9.4
(p. 101), TF-IDF and LM.
2. Setting of the mixture parameter ı:
In JM-LM, ı is constant. For example ı WD 0:8, means to assign a higher impact to the
document-based probability P.t jd/ than to the collection-based probability P.t jc/.
Alternatively, and conducive for retrieval performance, is the proportional setting in Dirich-
LM. is is ıd WD jd jjd jC , where a norm (e.g., jd j could be the document length) is applied
to balance P.t jd/ and P.t jc/. is reflects that there is more trust in probabilities estimated
from large (long) documents than small (short) documents. e choices for jd j and  open
up several pathways.
3. Relationship between LM and the probability of relevance?
Azzopardi and Roelleke [2007], Lafferty and Zhai [2003], Lavrenko and Croft [2001],
Sparck-Jones et al. [2003] discuss the relationship between the odds of relevance, O.r jd; q/,
and LM, P.qjd/.
In addition to the common formulations here denoted LM1, LM, JM-LM and Dirich-LM,
this book introduces a formulation referred to as LM2. LM2 is based on the negative logarithm,
namely log P.t jd;c/
P.t jc/
D   log P.t jc/
P.t jd;c/
. is formulation is of advantage in the probabilistic logical
modeling of LM, and can be also used to describe relationships between LM and TF-IDF.
LM is the model based on the conjunctive decomposition of P.qjd/. Twenty years before
LM became popular, Turtle and Croft [1990] proposed a PIN-based modeling of retrieval, which
explains IR as being based on the disjunctive decomposition of P.qjd/.
2.7 PIN’S: PROBABILISTIC INFERENCE NETWORKS
Figure 2.9 shows two PIN’s, one for modeling document retrieval, and another one for modeling
“Find Mr. X.” e side-by-side arrangement underlines the duality between document retrieval
and “Find Mr. X.” For document retrieval, the PIN models the relationships (inference) between
documents, terms, and queries. e purpose is to estimate the probability that document d is
relevant to query q, i.e., that query q is inferred from document d , or in other words, that d
implies q. For “Find Mr. X,” the PIN models the inference between cities, activities, and Mr. X.
e purpose is to estimate the probability that Mr. X is inferred from a given city. e parallels
between the two scenarios helps to compare PIN’s in IR against the general role of PIN’s for
modeling knowledge and reasoning.
e documents (cities) are the source events, and the query (Mr. X) is the target event.
Note that the detective wants P.londonjmrx/ to make a decision about the city to travel to.
Transferring this to the IR case means that a search engine wants P.d jq/ to make a decision
2.7. PIN’S: PROBABILISTIC INFERENCE NETWORKS 59
t1 t2
d1 d2
t4t3
q
(a) Document Retrieval
Football fanUnderground user
Mr. X
London Dortmund
(b) Find Mr. X
Figure 2.9: Two Probabilistic Inference Networks.
about the rank of a document. e conditional probabilities may feel a bit counter-intuitive since
the PIN is directed from the source to the target, whereas the conditional probabilities are of
the form P.sourcejtarget/. is analogy points us at the issue to decide whether the inference is
d ! q or q ! d .
For a document, the terms are target events, and for the query, the terms are source events.
Document d1 implies/contains three term events: t1; t2; t3. is leads to 23 D 8 min-terms, where
a min-term is a conjunction of positive and negative term events (e.g., t1 ^ t2 ^ Nt2 is a min-term).
erefore, the computation of a PIN is of exponential complexity (O.2n/, where n is the number
of source events for a given node).
e overall idea of a PIN is to compute the conditional probability of any selected target
node. e computation is based on the decomposition for a set of disjoint and exhaustive events.
Let X be an exhaustive set of disjoint events.
P.qjd/ D
X
x2X
P.qjx/  P.xjd/ (2.128)
e min-terms are the disjoint events.
X D f.t1; t2; t3/; .t1; t2; Nt3/; : : :g (2.129)
e min-terms correspond to the Boolean conjunctions of the source events that point to the
target.
e probability flow in a PIN can be described via a matrix; this matrix is referredlink matrix
to as the link or transition matrix. e link matrix L contains the transition probabilities
P.targetjx; source/, where x is a min-term of the nodes that connect the target node to the source
node. Usually, P.targetjx; source/ D P.targetjx/ is assumed, and this assumption is referred to
as the linked independence assumption. Using the link matrix, we obtain the following expressions:
60 2. FOUNDATIONS OF IR MODELS
L WD

P.qjx1/ : : : P.qjxn/
P. Nqjx1/ : : : P. Nqjxn/

(2.130)

P.qjd/
P. Nqjd/

D L 
0B@ P.x1jd/:::
P.xnjd/
1CA (2.131)
For illustrating the link matrix, a matrix for three terms is shown next.
LTransposed D
2666666666664
P.qjt1; t2; t3/ P. Nqjt1; t2; t3/
P.qjt1; t2; Nt3/ P. Nqjt1; t2; Nt3/
P.qjt1; Nt2; t3/ P. Nqjt1; Nt2; t3/
P.qjt1; Nt2; Nt3/ P. Nqjt1; Nt2; Nt3/
P.qj Nt1; t2; t3/ P. Nqj Nt1; t2; t3/
P.qj Nt1; t2; Nt3/ P. Nqj Nt1; t2; Nt3/
P.qj Nt1; Nt2; t3/ P. Nqj Nt1; Nt2; t3/
P.qj Nt1; Nt2; Nt3/ P. Nqj Nt1; Nt2; Nt3/
3777777777775
(2.132)
ere are some special link matrices. e matrices Lor and Land reflect the Boolean com-
bination of the linkage between a source and a target.
Lor D

1 1 1 1 1 1 1 0
0 0 0 0 0 0 0 1

(2.133)
Land D

1 0 0 0 0 0 0 0
0 1 1 1 1 1 1 1

(2.134)
It is evident that the link matrices for the Boolean cases correspond to a particular distribu-
tion of the probabilities P.targetjx/. e multiplication of the link matrix with the probabilities
of the sources yields the target probabilities. is is illustrated in the next equation of the “3-term”
example.

P.qjd/
P. Nqjd/

D L 
0BBBBBBBBBBB@
P.t1; t2; t3jd/
P.t1; t2; Nt3jd/
P.t1; Nt2; t3jd/
P.t1; Nt2; Nt3jd/
P. Nt1; t2; t3jd/
P. Nt1; t2; Nt3jd/
P. Nt1; Nt2; t3jd/
P. Nt1; Nt2; Nt3jd/
1CCCCCCCCCCCA
(2.135)
Often, a PINwill be under-specified, i.e., not all min-term probabilities but only the single-
event probabilities, e.g., P.t jd/ and P.qjt/ will be available.
2.7. PIN’S: PROBABILISTIC INFERENCE NETWORKS 61
2.7.1 THE TURTLE/CROFT LINK MATRIX
Croft and Turtle [1992], Turtle and Croft [1992] proposes a special setting of the link matrix. Let
wt WD P.qjt/ be the query term probabilities. en, estimate the link matrix elements P.qjx/,
where x is a Boolean combination of terms, as follows.
LTurtle/Croft D
24 1 w1Cw2w0 w1Cw3w0 w1w0 w2Cw3w0 w2w0 w3w0 0
0 w3
w0
w2
w0
w2Cw3
w0
w1
w0
w1Cw3
w0
w1Cw2
w0
1
35 (2.136)
e multiplication of the first row with the min-term probabilities yields the following:
P.qjd/ D
w1 C w2 C w3
w0
 P.t1; t2; t3jd/ C
w1 C w2
w0
 P.t1; t2; Nt3jd/ C
w1 C w3
w0
 P.t1; Nt2; t3jd/ C
w2 C w3
w0
 P. Nt1; t2; t3jd/ C
w1
w0
 P.t1; Nt2; Nt3jd/ C
w2
w0
 P. Nt1; t2; Nt3jd/ C
w3
w0
 P. Nt1; Nt2; t3jd/
e special setting of L leads to a formation of weights and probabilities that reduces to
a closed form of linear complexity. e intermediate step is illustrated in the following equation
(we use ti D P.ti jd/ for shortening the expressions).
w1t1 C w2t2 C w3t3 D
w1t1t2t3 C w2t1t2t3 C w3t1t2t3C #w1 C w2 C w3
w1t1t2   w1t1t2t3 C w2t1t2   w2t1t2t3C #w1 C w2
w1t1t3   w1t1t2t3 C w3t1t3   w3t1t2t3C #w1 C w3
w2t2t3   w2t1t2t3 C w3t2t3   w3t1t2t3C #w2 C w3
w1t1   w1t1t2   w1t1t3 C w1t1t2t3C #w1
w2t2   w2t1t2   w2t2t3 C w2t1t2t3C #w2
w3t3   w3t1t3   w3t2t3 C w3t1t2t3 #w3
Due to the Boolean combination of weights, all expressions with more than one term true
cancel out. us, the probability P.qjd/ becomes:
P.qjd/ D
w1
w0
 P.t1jd/ C
w2
w0
 P.t2jd/ C
w3
w0
 P.t3jd/ (2.137)
e term weight wi corresponds to the conditional query probability P.qjti /.
wi WD P.qjti / (2.138)
62 2. FOUNDATIONS OF IR MODELS
2.7.2 TERM WEIGHT AND RSV
e PIN-based term weight can be formally defined as follows.
Definition 2.49 PIN-based term weight wPIN.
wPIN.t; d; q; c/ WD
1P
t 0 P.qjt
0; c/
 P.qjt; c/  P.t jd; c/ (2.139)
e PIN-based RSV is defined as the sum of PIN-based term weights.
Definition 2.50 PIN-based retrieval status value RSVPIN.
RSVPIN.d; q; c/ WD
X
t
wPIN.t; d; q; c/ (2.140)
Inserting wPIN yields the RSV in decomposed form.
RSVPIN.d; q; c/ D
1P
t P.qjt; c/

X
t2d\q
P.qjt; c/  P.t jd; c/ (2.141)
It is evident that P.qjt; c/ could be replaced by pidf.t; c/ because both values are high for
rare terms. We look at this relationship between PIN’s and TF-IDF in Section 3.9.6 (p. 104),
TF-IDF and PIN’s.
Example 2.51 PIN RSV computation Let the following term probabilities be given:
ti P.ti jd/ P.qjti /
sailing 2=3 1=10; 000
boats 1=2 1=1; 000
Terms are independent events. P.t jd/ is proportional to the within-document term frequency,
and P.qjt/ is proportional to the IDF. e RSV is:
RSVPIN.d; q; c/ D
1
1=10; 000 C 1=1; 000
 .1=10; 000  2=3 C 1=1; 000  1=2/ D
D
1
11=10; 000
 .2=30; 000 C 1=2; 000/ D
10; 000
11
 .2 C 15/=30; 000 D
D
1
11
 .2 C 15/=3 D 17=33  0:51
2.8. DIVERGENCE-BASED MODELS AND DFR 63
2.7.3 SUMMARY
We have discussed the foundations of PIN’s. We have reconsidered the special link matrix,
LTurtle/Croft, described in Croft and Turtle [1992], Turtle and Croft [1992], which leads to an RSV
with linear complexity. Essentially, the RSV is the sum of the term weights wPIN.t; d; q; c/ D
P.qjt/P
t0 P.qjt
0/
 P.t jd/. Herein, the probability P.qjt / is proportional to the IDF, and the probabil-
ity P.t jd/ is proportional to the within-document term frequency. is indicates the relationship
between TF-IDF and PIN’s; see also Section 3.9.6 (p. 104), TF-IDF and PIN’s.
2.8 DIVERGENCE-BASED MODELS AND DFR
To approach the role of divergence-based models, we consider first a general example, and relate
this to IR (terms and documents).
Example 2.52 Divergence. Let Puk be the probability function for the weather in the
UK, and let Plondon be the probability function for the weather in London. Let X D
fsunny; cloudy; rainy; : : :g be the set of events.
e fraction Puk.sunny/=Plondon.sunny/ measures the difference between the two proba-
bilities.
e weighted sum of the logarithm of the fractions of probabilities is known as KL-
divergence:
DKL .PukjjPlondon/ D
X
x2X
Puk.x/  log
Puk.x/
Plondon.x/
It can be viewed as the expectation value of the random variable for the logarithm of the frac-
tions of the two probabilities. Also, KL-divergence is the difference between cross entropy,
Hcross.Puk; Plondon/ and entropy, H.Puk/; see also Section 3.11.6 (p. 111), Information eory:
KL-Divergence.
e probabilities can be estimated in different ways. One possibility is to observe the events
over days. Let nDays.sunny; uk/ be the number of sunny days in the UK, and let NDays.uk/ be
the total number of days. en, PDays.sunnyjuk/ WD nDays.sunny; uk/=NDays.uk/ is the respective
probability.
Another option could be to count the trips to the UK where one experienced at least one
sunny day: PTrips.sunnyjuk/ WD nTrips.sunny; uk/=NTrips.uk/.
Regarding the analogy to IR, counting sunny days corresponds to counting term occurrences,
and counting trips corresponds to counting documents; see also Section 2.4.2 (p. 36), Poisson Anal-
ogy: Sunny Days and Term Occurrences.
2.8.1 DFR: DIVERGENCE FROM RANDOMNESS
e underlying idea of DFR is as follows:²
²Quote from http://ir.dcs.gla.ac.uk/terrier/doc/dfr_description.html, 2006.
64 2. FOUNDATIONS OF IR MODELS
e more the divergence of the within-document term frequency (tfd ) from its fre-
quency within the collection (tfc), meaning the more divergent from randomness the
term is, the more the information carried by the term in the document.
wDFR-1;M .t; d; collection/ /   log PM .t 2 d jcollection/
M stands for the type of model of the DFR employed to compute the probability.
We can translate this as follows: for a term occurring in a random pattern, PM .t 2 d jc/ is maxi-
mal, i.e., the DFR weight is minimal. On the other hand, PM .t 2 d jc/ is smaller than the maxi-
mal probability, if term t occurs in a non-random pattern. Whereas a randomly distributed term
occurs in very few documents more than once, we find for terms in IR test collections that they
occur in more documents more frequently than a model of randomness tells; see also Section 2.4
(p. 35), Poisson Model.
In addition to the probability P.t 2 d jc/, there is the probability P.tfd jc/, the probability
of the term frequency. Gianni Amati views the latter, P.tfd jc/, as the new DFR—the second-
generation DFR.
wDFR-2;M .t; d; collection/ /   log PM .tfd jcollection/
e original formulation, i.e., the first-generation DFR with P.t 2 d jc/, can be expressed as the
cumulative of the probabilities of the term frequency:
P.t 2 d jcollection/ D P.tfd > 0jcollection/ D 1   P.tfd D 0jcollection/
It is also helpful to apply the following notation:
P.t 2 d jcollection/ D P.kt > 0jd; collection/
In this formulation, kt is the frequency of term t , and it becomes explicit that the document
event d is part of the conditional. us DFR-1 corresponds to the probability of the condition
kt > 0, and DFR-2 corresponds to the condition kt D tfd .
Amati and van Rijsbergen [2002] introduced DFR, and important is that in addition to
the two different conditions on tfd there are two ways of computing the informative content of a
term. e first is based on the negative logarithm of the frequency probability, and the second is
the complement (Popper’s formulation of informative content).
inf1.t; d; c/ WD   log P1.condition on kt jd; c/ (2.142)
inf2.t; d; c/ WD 1   P2.condition on kt jd; c/ (2.143)
Both measures express that the informative content is high if the frequency probability is small
(the term is rare).
e combination of the measures is at the core of DFR. Equation 1 from Amati and van
Rijsbergen [2002] is:
wDFR.t; d; c/ D inf2.t; d; c/  inf1.t; d; c/ (2.144)
2.8. DIVERGENCE-BASED MODELS AND DFR 65
is equation is open for the two cases, kt > 0 or kt D tfd , and two different models of random-
ness can be chosen for the two information measures. In addition, the condition can be adapted to
kt D tfnd , where tfnd is a normalized value. e normalization is a function of tfd , the document
length dl, and any other parameter such as the average document length avgdl.
In summary, DFR models are “made up” of three components (see Table I in Amati and
van Rijsbergen [2002]):
1. Basic Models: Poisson (P); Approximation of the binomial models with the divergence (D);
Geometric (G); Bose-Einstein; Mixture of Poisson and IDF; and others
2. First normalization: Laplace (L), Bernoulli (B)
3. Second (length) normalization:
Hypothesis 1: Uniform term frequency distribution: tfd =dl. en, the normalized term
frequency is tfnd D tfd =.dl=avgdl/. is is because of the integral over the density function:Z dlCavgdl
dl
tfd =dl dx D tfd =dl  avgdl
Hypothesis 2: Term frequency density is a decreasing function: tfd =length. en, the fol-
lowing integral gives the normalized term frequency:Z dlCavgdl
dl
tfd =x dx D tfd  ln

1 C
avgdl
dl

Given this systematics, for example, the DFR model PL1 is a combination of Poisson (basic
model), Laplace (first normalization), and hypothesis 1 (normalization with respect to the docu-
ment length).
2.8.2 DFR: SAMPLING OVER DOCUMENTS AND LOCATIONS
ough the underlying idea of DFR seems intuitive, it is difficult to fully understand the effect
of the formulae related to DFR³. To highlight some of the issues, we review the example given
in Baeza-Yates and Ribeiro-Neto [2011], page 114.
Example 2.53 DFR, binomial probability.
“To illustrate, consider a collection with ND D1,000 documents and a term kt [Baeza-
Yates and Ribeiro-Neto [2011] use kt to refer to a term; we use in this book t for the
³I would like to thank Gianni Amati, Ricardo Baeza-Yates, and Berthier Ribeiro-Neto for discussing the formulation of DFR.
66 2. FOUNDATIONS OF IR MODELS
term, and kt for the frequency] that occurs nt D10 times in the collection. en, the
probability of observing kt D4 occurrences of term kt [t] in a document is given by
P.kt jc/ D
 
nt
kt
!
 pkt  .1   p/.nt  kt / D
 
10
4
!


1
1; 000
4


1  

1
1; 000
.10 4/
which is a standard binomial distribution. In general, let p D 1=N be the probability
of observing a term in a document, where N is the total number of documents.”
We first reconsider the binomial probability. e single event probability, pd DP.d jc/D
1=ND.c/D1=1,000, is the probability of document d . e number of trials, nt D tfc D10, is the
number of times term t occurs in collection c.
Pbinomial;tfc ;pd .kd / is the probability of a sequence
with kd D4 occurrences of document d in nD tfc D 10 trials;
a trial corresponds to drawing a ball (document) from the urn (Documents in collec-
tion c).
Expressed as a conditional probability, we may write P.kd jt; c/DPbinomial;tfc ;P.d jc/.kd /,
and this underlines that the probability of the frequency kd of the document d depends on tfc ,
the number of occurrences of term t .
However, when estimating the probability of observing kt occurrences of term t in a par-
ticular document, then we need to sample over term occurrences (locations at which terms occur).
erefore, let pt DPL.t jc/D1/1,000 be the single event probability, i.e., term t occurs in
0:1% of the Locations. For example, PL.t jc/D tfc=NL.c/D100=105, where tfc DnL.t; c/D100 is
the number of Locations at which term t occurs in collection c, and NL.c/ D 105 is the number of
Locations (length of the collection). e number of trials is equal to the document length: nd Ddl.
Pbinomial;dl;PL.t jc/.kt / is the probability of a sequence
with kt D4 occurrences of term t in nD dlD 10 trials;
a trial corresponds to drawing a ball (location) from the urn (Locations in collection c).
We can imagine the urn to contain a ball per location, and each ball is labelled with the term that
occurs at the location. In this formulation of the binomial, the number of trials is the document
length, n D dl, rather than the collection-wide term frequency, n D tfc .
2.8.3 DFR: BINOMIAL TRANSFORMATION STEP
Finally, we investigate how to relate the two binomial probabilities. For the first formulation, the
average  D n  p is as follows:
.t; c/ D nt  P.d jc/ D tfc 
1
ND.c/
(2.145)
2.8. DIVERGENCE-BASED MODELS AND DFR 67
is is the average occurrence of a term among all documents.
On the other hand, the expected frequency for document d is:
.t; d; c/ D dl  PL.t jc/ (2.146)
How are the averages .t; c/ and .t; d; c/ related?
e following equation highlights that the averages are equal for the case dl D avgdl.
.t; d; c/ D dl  PL.t jc/ D dl 
tfc=ND.c/
NL.c/=ND.c/
D dl  .t; c/
avgdl.c/
(2.147)
e equation is based on dividing the numerator and denominator of PL.t jc/ by ND.c/. is
leads to the right expression showing .t; c/ and avgdl.c/.
For dl D avgdl.c/, i.e., for a document of average length, the averages are equal, i.e.,
.t; d; c/ D .t; c/. erefore, the binomial and Poisson probabilities are equal. e following
two equations, one for the binomial probability, and one for the Poisson probability, formalize
this equality.
Pbinomial;nDtfc ;pd D1=ND.c/.kd / D Pbinomial;nDavgdl;pt Dtfc=NL.c/.kt / (2.148)
PPoisson;Dtfc=ND.c/.kd / D PPoisson;Davgdltfc=NL.c/.kt / (2.149)
e probability that the document d occurs kd times in tfc trials is equal to the probability that the
term t occurs kt times in avgdl trials. e formulation of the Poisson probability underlines that
for the special case of a document of average length, the Poisson parameter is .t; c/ D tfc=ND.c/.
is excursus regarding the binomial probability Pn;p.kt jc/ helps to understand the DFR
RSV and term weight.
2.8.4 DFR AND KL-DIVERGENCE
Taking DFR word-by-word, DFR is based on measuring the divergence between observed prob-
ability and model (randomness) probability. For DFR first-generation, the term probability is
P.kt > 0jd; c/. Accordingly, the KL divergence is:
DKL .PobsjjPrandom/ D
X
t
Pobs.kt > 0jd; c/  log
Pobs.kt > 0jd; c/
Prandom.kt > 0jd; c/
(2.150)
Here, the condition “kt > 0” is equivalent to the event “t 2 d .”
For DFR second-generation, the term probability is P.kt jd; c/. Here, the condition “kt D
tfd ” is equivalent to the event “tfd .”
DKL .PobsjjPrandom/ D
X
t
Pobs.kt jd; c/  log
Pobs.kt jd; c/
Prandom.kt jd; c/
(2.151)
68 2. FOUNDATIONS OF IR MODELS
KL-divergence is the difference between cross entropy and entropy; see alsoSection 3.11.6 (p. 111),
Information eory: KL Divergence. is can be applied to motivate a divergence-based score;
Section 2.8.10 (p. 73), KL-Divergence RetrievalModel. Also, the difference between divergences,
can be applied to explain LM, Section 3.11.8 (p. 112), and TF-IDF, Section 3.11.9 (p. 112).
2.8.5 POISSON AS A MODEL OF RANDOMNESS: P.kt > 0jd; c/: DFR-1
If we employed the Poisson probability as a model of randomness, then, for P.kt > 0jd; c/, we
obtain:
wDFR-1.t; d; c/ /   log .1   P.kt D 0jd; c// D   log

1   e .t;d;c/

(2.152)
e next transformation step is based on the Euler convergence (limit definition): see also Equa-
tion 2.39 (p. 21):
e  D lim
n!1

1  

n
n
Using the convergence, we obtain:
wDFR-1.t; d; c/ /   log

1  

1  
.t; d; c/
n
n
(2.153)
e next transformation inserts n D dl and, respectively, .t; d; c/ D dl  PL.t jc/. is is because
for a document, the sample size is n D dl.
wDFR-1.t; d; c/ /   log

1   .1   PL.t jc//
dl

(2.154)
is equation shows that the DFR term weight is inverse proportional to the probability that
term t occurs at least once in n D dl trials.
2.8.6 POISSON AS A MODEL OF RANDOMNESS: P.kt D tfd jd; c/: DFR-2
Alternatively, we apply the logarithm to the Poisson probability P.tfd jc/ rather than to P.t 2
d jc/.
wDFR-2.t; d; c/ /   log P.tfd jd; c/ D   log
 
.t; d; c/tfd
.tfd /Š
 e .t;d;c/
!
(2.155)
In the next step, we apply the natural logarithm to decompose the expression.
  ln P.tfd jd; c/ D ln..tfd /Š/   tfd  ln..t; d; c// C .t; d; c/ (2.156)
To facilitate the expression, we use from now k WD tfd and  WD .t; d; c/ (saving subscripts and
parameters).
2.8. DIVERGENCE-BASED MODELS AND DFR 69
Next, Stirling’s formula is applied for ln.kŠ/:
ln.kŠ/ D k  ln.k/   k C 1=2  ln.2    k/ (2.157)
(We apply the basic formula; the more exact formula involves 1=.12  k/). In the next equation,
we insert Stirling’s formula.
  ln P.kjd; c/ D k  ln.k/   k C 1=2  ln.2    k/   k  ln./ C  (2.158)
Some transformation steps group the expression as follows:
  ln P.kjd; c/ D k  ln k

C .   k/ C 1=2  ln.2    k/ (2.159)
Next, we insert the average term frequency, .t; d; c/ D dl  PL.t jc/, to be expected in a document
of length dl; see also Section 2.4 (p. 35), Poisson Model.
  ln P.tfd jd; c/ D tfd  ln
tfd
dl  PL.t jc/
C .dl  PL.t jc/   tfd / C 1=2  ln.2    tfd / (2.160)
Finally, we apply the Poisson bridge (Section 3.7 (p. 93), Poisson Bridge) to replace the Location-
based term probability PL.t jc/ by an expression containing the Document-based term probability
PD.t jc/. is shows that a part of the DFR term weight contains an expression related to TF-
IDF:
tfd    ln.dl  PL.t jc// D tfd    ln

dl  avgtf.t; c/
avgdl.c/
 PD.t jc/

(2.161)
Herein, not only the traditional IDF, IDF.t; c/ D  ln PD.t jc/, is explicit. Similar to the Pois-
son model, the burstiness of a term and a document length pivotization are explicit as well:
burstiness.t; c//avgtf.t; c/ and pivdl.c/Ddl=avgdl.c/; see also Section 3.9.2 (p. 98), TF-IDF
and Poisson.
2.8.7 DFR: ELITE DOCUMENTS
e probabilities can be computed for a subset of the collection, for example, the elite doc-
uments selected for ranking the retrieved documents: P.tfd > 0jset of elite documents/ and
P.tfd jset of elite documents/. For ranking different documents with respect to one query, the
set of documents retrieved for the query (in a disjunctive interpretation) is a reasonable elite set.
For a probabilistic ranking model, care has to be taken that the combination of probabilities es-
timated from different elite sets does not lead to a heuristic model.
2.8.8 DFR: EXAMPLE
To illustrate DFR further, we look at a synthetic example; see also Section 2.4.3 (p. 37), Poisson
Example: Toy Data.
Example 2.54 DFR. Let t be a term, and let c be a collection. Let the term occur in tfc D
nL.t; c/D200 locations, and in df.t; c/DnD.t; c/D100 documents. e average (expected) term
70 2. FOUNDATIONS OF IR MODELS
frequency is avgtf.t; c/D200=100D2; this is the average over the documents in which the term
occurs.
Let ND.c/D1,000 be the total number of documents. e term occurs in 10% of the docu-
ments: PD.t jc/D100/1,000. e average (expected) term frequency is .t; c/D200/1,000D2/10;
this is the average over all documents.
e following table shows a row per term frequency kt D0; : : : ; 6. e column headed nD
is the number ofDocuments that contain kt occurrences of t , i.e., nD.t; c; kt /. e column headed
nL is the number of Locations at which the term occurs: nL Dkt  nD . e columns to the right
show the observed and Poisson probabilities.
kt nD nL Pobs;all.kt / PPoisson;all;D0:02.kt / Pobs;elite.kt / PPoisson;elite;D2.kt /
0 900 0 0:900 0:819 0:00 0:135
1 58 58 0:058 0:164 0:58 0:271
2 19 38 0:019 0:016 0:19 0:271
3 0 0 0:000 0:001 0:00 0:180
4 12 48 0:012 0:000 0:12 0:090
5 10 50 0:010 0:000 0:10 0:036
6 1 6 0:001 0:000 0:01 0:012
1,000 200 D0:20  D avgtfD2
Pobs;all.kt / is the observed probability over all documents. PPoisson;all;.kt / is the Poisson
probability, where .t; c/ D nL.t; c/=ND.c/ D 0:20 is the Poisson parameter. e table illustrates
how the observed probability is different from the Poisson probability. PPoisson.1/ is greater than
Pobs.1/, whereas for kt > 1, the observed probabilities are greater than the Poisson probabilities.
ere is more mass in the tail of the observed distribution than the Poisson distribution assumes
(thin-tail distribution).
Moreover, the columns to the right illustrate the usage of the elite documents instead of all
documents. Here, the single event probability is based on the locations of elite documents only.
2.8.9 TERM WEIGHTS AND RSV’S
We show the definitions of various DFR term weights. e general definition is as follows.
Definition 2.55 DFR term weight wDFR.
wDFR;M .t; d; c/ WD   log PM .condition on kt jc/ (2.162)
Next, we show some definitions for DFR-1, where the condition is kt > 0, which is equiv-
alent to t 2 d . is is followed by definitions for DFR-2, where the condition is kt D tfd .
Definition 2.56 DFR-1 term weight wDFR-1.
wDFR-1;M .t; d; c/ WD  log PM .t 2 d jc/ (2.163)
2.8. DIVERGENCE-BASED MODELS AND DFR 71
Definition 2.57 DFR-1 term weight wDFR-1;binomial.
wDFR-1;binomial.t; d; c/ WD  log
h
1   .1   1=ND.c//
tfc
i
(2.164)
As discussed in Section 2.8.3 (p. 66), Transformation Step, the genuine formulation should
be based on the term probability PL.t jc/ D tfc=NL.c/ and on n D dl trials. For a document of
average document length, dl D avgdl, we obtain:
.1   1=ND.c//
tfc D .1   tfc=NL.c//avgdl (2.165)
e truth of this equation is evident in the following rewriting:
.1   1=ND.c//
tfc ND.c/=ND.c/ D .1   tfc=NL.c//NL.c/=ND.c/ (2.166)
Because of the Euler convergence (limit definition of the exponent function), e  D
limN !1.1   =N /N , see also Equation 2.39 (p. 21), we obtain: 
e 1
tfc=ND.c/
D

e tfc
1=ND.c/
D e tfc=ND.c/ (2.167)
is transformations coincides with the definition of the Poisson-based DFR-1 term
weight following next.
Definition 2.58 DFR-1 term weight wDFR-1;Poisson. Let .t; d; c/ D dl  tfc=NL.c/ be the av-
erage occurrence of term t in document d .
wDFR-1;Poisson.t; d; c/ WD  log
h
1   e .t;d;c/
i
(2.168)
For a document of average length, this is:
e avgdltfc=NL.c/ D e tfc=ND.c/ (2.169)
is is the same expression as discussed in equation 2.167 for the binomial case.
Next, we consider the DFR-2 weights.
Definition 2.59 DFR-2 term weight wDFR-2.
wDFR-2;M .t; d; c/ WD  log PM .tfd jc/ (2.170)
72 2. FOUNDATIONS OF IR MODELS
Definition 2.60 DFR-2 term weight wDFR-2;binomial.
wDFR-2;binomial.t; d; c/ WD  log
" 
tfc
tfd
!


1
ND.c/
tfd


1  

1
ND.c/
.tfc tfd /#
(2.171)
For the transformation step between Pbinomial;nDtfc ;pD1=ND.c/.kd / and
Pbinomial;nDdl;pDtfc=NL.c/.kt /, see Section 2.8.3 (p. 66), Transformation Step.
Definition 2.61 DFR-2 term weight wDFR-2;Poisson.
wDFR-2;Poisson.t; d; c/ WD  log
"
.t; d; c/tfd
tfd Š
 e .t;d;c/
#
(2.172)
e DFR RSV is defined as the sum of DFR term weights.
Definition 2.62 DFR retrieval status value RSVDFR.
RSVDFR;M .d; q; c/ WD
X
t2d\q
wDFR;M .t; d; c/ (2.173)
Inserting wDFR yields the RSV in decomposed form. For the binomial probability in DFR-
2, it is:
RSVDFR-2;binomial.d; q; c/ D
 
X
t2d\q
"
log
 
tfc
tfd
!
C tfd  log
1
ND.c/
C .tfc   tfd /  log

1  
1
ND.c/
#
(2.174)
For the Poisson probability, it is:
RSVDFR-2;Poisson.d; q; c/ D  
X
t2d\q

tfd  ln
tfc
ND.c/
  ln..tfd /Š/  
tfc
ND.c/

(2.175)
see also Section 2.8.6 (p. 68), Poisson as Model of Randomness.
2.9. RELEVANCE-BASED MODELS 73
2.8.10 KL-DIVERGENCE RETRIEVAL MODEL
In Zhai [2009], page 55, the KL-divergence and a ranking score are related as follows.
score.d; q/ D  DKL
 
PqjjPd

(2.176)
e motivation is that negative divergence is a similarity measure. KL-divergence is the difference
between cross entropy and entropy; see also Section 3.11 (p. 108), Information eory.
score.d; q/ D  

Hcross.Pq; Pd /   H.Pq/

(2.177)
en, the score isis ranking-equivalent to the negative cross entropy, since the entropy of the
query does not affect the ranking. e next steps relate cross entropy to the query likelihood:
score.d; q/ rankD
X
t
P.t jq/  log P.t jd/
D
X
t
tfq
ql
 log P.t jd/
rank
D
X
t
tfq  log P.t jd/
D log P.qjd/
is justifies the view that LM is a divergence-based model. We extend this discussion in chap-
ter 3, where we show that LM can be seen to be proportional to the difference between query clar-
ity and the divergence between document and query. In a similar way, TF-IDF can be seen to be
proportional to the difference between “document clarity” and the divergence between query and
document; see also Section 3.11.8 (p. 112), Difference between Divergences: LM; Section 3.11.9
(p. 112), Difference between Divergences: TF-IDF.
2.8.11 SUMMARY
We have discussed the foundations of divergence-based models and DFR.
Divergence between random variables (probability distributions) is a concept thatmany stu-
dents perceive as relatively complex. I heard students speaking of “mathematical sophistication.”
is section on divergence aimed at demystifying divergence and DFR. In particular, we dis-
cussed the transformation step that relates the DFR model and the binomial probability, namely
the probability that a term t occurs kt in a document of length dl, given its single event probabil-
ity PL.t jc/ D tfc=NL.c/. We discussed that for a document of average length, the DFR model
corresponds to this binomial probability.
2.9 RELEVANCE-BASED MODELS
In this section we look at the main models to incorporate relevance information. ereby we
discuss the resemblance between the models.
74 2. FOUNDATIONS OF IR MODELS
2.9.1 ROCCHIO’S RELEVANCE FEEDBACK MODEL
Rocchio [1971], “Relevance Feedback in Information Retrieval,” is the must-have reference and
background for what a relevance feedback model aims at. ere are two formulations that aggre-
gate term weights:
weight.t; q/ D weight0.t; q/ C 1
jRj

X
d2R
Ed  
1
j NRj

X
d2 NR
Ed (2.178)
weight.t; q/ D ˛  weight0.t; q/ C ˇ 
X
d2R
Ed    
X
d2 NR
Ed (2.179)
In the second formulation, the parameters ˛, ˇ, and  allow for adjusting the impact of genuine
query, relevant documents, and non-relevant documents, whereas in the first formulation, the
impact is proportional to the cardinality of the sets of relevant and non-relevant documents.
2.9.2 THE PRF
e log-based formulation of the probabilistic odds of relevance is:
log P.r jd; q/
P. Nr jd; q/
D log .P.r jd; q//   log .P. Nr jd; q// (2.180)
After the transformation steps we considered in Section 2.2 (p. 23), PRF, term weight is:
wPRF.t; r; Nr/ D log
P.t jr/  P.Nt j Nr/
P.t j Nr/  P.Nt jr/
D log .P.t jr/=P.Nt jr//   log .P.t j Nr/=P.Nt j Nr// (2.181)
e term probabilities are estimated in the usual way. For example: P.t jr/ D nD.t; r/=ND.r/.
is estimate can be expressed more general, namely as a sampling over the documents in the set
Dr of relevant documents:
P.t jr/ D
X
d2Dr
P.t jd/  P.d jr/ (2.182)
is is the total probability for the set Dr where the documents are considered as disjoint events.
With the prior P.d jr/ D 1=ND.r/, and P.t jd/ D 1 if t 2 d , and P.t jd/ D 0 otherwise, the total
probability is equal to the estimate nD.t; r/=ND.r/.
e formulation of the term weight underlines the resemblance between the Rocchio for-
mula and the PRF. Both formulae rely on the idea to gather evidence from relevant documents
and add this to the term weight, while subtracting the evidence regarding term occurrences in
non-relevant documents. Relevance-based language models elaborate on the approach to sample
over documents.
2.9. RELEVANCE-BASED MODELS 75
2.9.3 LAVRENKO’S RELEVANCE-BASED LANGUAGE MODELS
Lavrenko and Croft [2001], “Relevance-based Language Models,” proposes an LM-based ap-
proach to estimate P.t jr/. e idea is that P.t jr/ D P.t jq/ for missing relevance information.
ere are two main methods for estimating P.t jq/: method 1 is based on “sampling (total
probability) followed by a conditional independence assumption,” and method 2, referred to as
“conditional sampling”, is based on “a conditional independence assumption followed by sampling
(total probability).”
e paper views q as a vector, whereas in this book, we denote it as a sequence. e paper
precisely positions that this “Relevance-based LM” is a “query expansion technique.”
We argue that P.t jEq/ is a good approximation in the absence of training data:
P.t jr/  P.t jEq/. [e original papers uses Eq D q1; : : : ; qn.]
e main contribution of this work is a formal probabilistic approach to estimate
P.t jr/, which has been done in a heuristic fashion by previous researchers. ...
From a traditional IR perspective, our method is a massive query expansion technique.
...
From an LM point of view, our method provides an elegant way of estimating a LM
whenwe have no training examples except a very short (two to three word) description.
Because of P.t; q/ D P.t jq/  P.q/, and since P.q/ is a constant, the model is formulated
for P.t; q/. For the formal description of the models let M be a sample of documents (this could
be the top-k documents initially retrieved).
Method 1: Step 1a: Estimate P.t; q/ via the total probability over sample M :
P.t; q/ D
X
m2M
P.t; qjm/  P.m/
Each m is a “language model,” and in this context, documents could be viewed as models. Note
the resemblance between this sampling and the sampling shown for PRF, in equation 2.182.
For example, given 10 documents, and the prior P.m/ D 1=10 for each model (document).
is means that each occurrence of t in document m will increase the probability P.t; q/.
Step 1b: Estimate P.t; qjm/ as the product of term and query probability. Make a condi-
tional independence assumption, namely P.qjt; m/ D P.qjm/.
P.t; qjm/ D P.t jm/  P.qjt; m/ D P.t jm/  P.qjm/
is assumptionmeans that the query is viewed to be independent of the term under investigation.
Moreover, apply a conditional independence assumption for the query terms:
P.t; qjm/ D P.t jm/ 
Y
t 02q
P.t 0jm/
is means that for P.t; qjm/ > 0, there must be P.t jm/ > 0 and 8t 0 W P.t 0jm/ > 0.
76 2. FOUNDATIONS OF IR MODELS
Method 2: Step 2a: Estimate P.qjt / as the product of query term probabilities.
P.t; q/ D P.t/  P.qjt/ D P.t/ 
Y
t 02 q
P.t 0jt /
Step 2b: Estimate P.t 0jt/ via the total probability over sample M :
P.t 0jt / D
X
m2M
P.t 0jm/  P.mjt/
In this estimate, the so-called linked-independence assumption P.t 0jt; m/ D P.t 0jm/ has been
applied.
Both methods are elegant ways to estimate P.t jr/ for the case of missing relevance, and, if
relevance data is available, then the relevant documents enhance the sample set M:
2.10 PRECISION AND RECALL
2.10.1 PRECISION AND RECALL: CONDITIONAL PROBABILITIES
Precision and recall are not what one would refer to as a “retrieval model.” We include the evalua-
tion measures into this chapter on retrieval models since both, retrieval and evaluation, are based
on conditional probabilities.
Given a set of retrieved documents, and a set of relevant documents, precision is the portion
of retrieved documents that are relevant, and recall is the portion of relevant documents that were
retrieved.
Definition 2.63 Precision and recall: Conditional probabilities. Precision is the conditional
probability of the event “relevant” given the event “retrieved”. Analogously, recall is the conditional
probability of the event “retrieved” given the event “relevant”. e definition and application of
Bayes’s theorem yields:
precision.q/ WD P.relevantjretrieved; q/ D P.retrieved; relevantjq/
P.retrievedjq/
(2.183)
recall.q/ WD P.retrievedjrelevant; q/ D P.retrieved; relevantjq/
P.relevantjq/
(2.184)
2.10.2 AVERAGES: TOTAL PROBABILITIES
e total probability theorem can be used to express average precision and other measures such as
discounted cumulative gain (DCG). ere are two averages: 1. e average precision per query,
which is the average over recall points. DCG can be viewed as a weighted average with a decaying
weight for lower ranks. 2. e mean of the average precisions, which is the average over queries.
2.11. SUMMARY 77
For the purpose of making the case, we look at the mean of the average precisions. Let Q
be a set of queries.
mean-avg-precision.Q/ WDPQ.relevantjretrieved/D
X
q2Q
P.relevantjretrieved; q/P.q/ (2.185)
e query probability is usually the uniform prior, i.e., P.q/ D 1=jQj, but it could be used to
reflect query aspects (e.g., difficulty, usefulness).
We have discussed the foundations of precision and recall. One of the main aspects was
to revisit that precision and recall share the roots of IR models: probabilities. From this point of
view, methods from IR models can be applied to evaluation, and the other way round, methods
from evaluation may impact on IR models. is aspect will be discussed in the research outlook,
end of this book.
2.11 SUMMARY
We have discussed the main strands of IR models: TF-IDF, PRF, BIR, Poisson, BM25, LM,
PIN’s and DFR. Figure 2.10 shows a concise overview summarizing the RSV’s and term weights.
e joint display of the RSV’s and term weights aims at indicating some of the symmetries
and relationships between models. We have already outlined in this chapter on “Foundations”
that all the models have probabilistic roots. e VSM (vector-space “model”) has not been listed
as a “model,” since it is a framework in which any model can be expressed (see Section 3.3 (p. 83),
VSM). Also, PIN’s and DFR can be viewed as frameworks, i.e., models of a higher level than
TF-IDF and LM.
e pure forms of TF-IDF and LM do not involve statistics over relevant documents. TF-
IDF is based on P.d jq/=P.d/, and LM is based on P.qjd/=P.q/, which indicates that both
models measure the same, namely P.d; q/=.P.d/  P.q//, the dependence between d and q.
e PRF is the conceptual framework to incorporate relevance. BIR, Poisson, and BM25
are considered as the main instantiations of the PRF, whereby the Poisson model is less known
than BIR and BM25. ese models are based on the “document likelihood,” P.d jq/. Naturally,
the question is: what about the “query likelihood,” P.qjd/? Is LM also an instantiation of the
PRF? is question was one of the motivations to investigate and formalize the relationships
between IR models.
78 2. FOUNDATIONS OF IR MODELS
.d; q; c/ WD
X
t
w .t; d; q; c/
w .t; d; q; c/ WD .t; d/  .t; q/  .t; c/
.d; q; r; Nr/ WD
X
t2d\q
w .t; r; Nr/
w .t; r; Nr/ WD

PD.t jr/
PD.t j Nr/

PD.Nt j Nr/
PD.Nt jr/

; w ; .t; r; c/ WD
PD.t jr/
PD.t jc/
.d; q; r; Nr/ WD
2
4
X
t2d\q
w .t; d; q; r; Nr/
3
5C
w .t; d; q; r; Nr/ WD .t; d/ 
.t;d;r/
.t;d; Nr/

D .t; d/ 
PL.t jr/
PL.t j Nr/

;k1;b;k2;k3.d; q; r; Nr; c/ WD
2
4
X
t2d\q
w ;k1;b;k3.t; d; q; r; Nr/
3
5C
w ;k1;b;k3.t; d; q; r; Nr/ WD
P
t2d\q ;k1;b
.t; d/  ;k3.t; q/  w .t; r;
w w
;ı .d; q; c/ WD
X
t2d\q
w ;ı .t; d; q; c/
w ;ı .t; d; q; c/ WD .t; q/ 

1C ı
1 ı

P.t jd/
P.t jc/

;.d; q; c/ WD
X
t2q
w ;.t; d; q; c/
w ;.t; d; q; c/ WD .t; q/ 


Cjd j
C
jd j
jd jC

P.t jd/
P.t jc/

.d; q; c/ WD
X
t
w .t; d; q; c/
w .t; d; q; c/ WD 1P
t
0 P.qjt 0;c/
 P.qjt; c/  P.t jd; c/
;M .d; q; c/ WD
X
t2d\q
w ;M .t; d; c/
w ;M .t; d; c/ WD  PM .t 2 d jc/
w ; .t; d; c/ WD  
h
1  .1  1=ND.c//
c
i
w ; .t; d; c/ WD  
h
1  e .t;d;c/
i
w ;M .t; d; c/ WD  PM . d jc/
w ; .t; d; c/ WD  

 
c
d



1
ND.c/

d


1  

1
ND.c/
. c d /

w ; .t; d; c/ WD  
h
.t;d;c/ d
d Š
 e .t;d;c/
i
Figure 2.10: Retrieval Models: Overview.
79
C H A P T E R 3
Relationships Between IR
Models
e following list shows the structure of this chapter (the electronic version provides hyperlinks):
Section 3.1: PRF: Probabilistic Relevance Framework: Some of this section repeats what has
been introduced in Section 2.2 (p. 23), PRF.
Section 3.2: Logical IR: P.d ! q/
Section 3.3: VSM: Vector-Space Model
Section 3.4: GVSM: Generalised Vector-Space Model
Section 3.5: A General Matrix Framework
Section 3.6: A Parallel Derivation of IR Models
Section 3.7: e Poisson Bridge
Section 3.8: Query Term Probability Assumptions
Section 3.9: TF-IDF and other models
Section 3.10: More Relationships: BM25 and LM, LM and PIN’s
Section 3.11: Information eory (with a focus on divergence)
Section 3.12: Summary (Relationship Overview in Figure 3.7)
e first eight sections discuss frameworks and formulae that help to relate IR models. en,
Section 3.9 pairs TF-IDF with most of the models discussed in chapter 2. Section 3.10 points at
more relationships: BM25 and LM, LM and PIN’s. is is finally followed by Section 3.11 which
briefly reviews the main concepts of information theory. We include the concepts for two reasons.
Firstly, they form the foundation of divergence-based models. Secondly, there is a divergence-
based formulation of both, TF-IDF and LM.
80 3. RELATIONSHIPS BETWEEN IR MODELS
3.1 PRF: THE PROBABILITY OF RELEVANCE
FRAMEWORK
e probability of relevance framework has been introduced in Section 2.2 (p. 23), PRF. Some
of the aspects will be repeated in the following. In the context of relationships, we briefly review
the PRF, while highlighting the issues arising from viewing the document and query events as
either a sequence of terms, a binary vector, or a frequency vector. is prepares the case to relate
the PRF to TF-IDF and LM.
e starting point of the PRF is Bayes’s theorem to estimate the relevance probabil-
ity P.r jd; q/.
P.r jd; q/ D
P.r/  P.d; qjr/
P.d; q/
(3.1)
e joint probability P.d; qjr/ can be decomposed in two ways:
P.d; qjr/ D P.qjr/  P.d jq; r/ (3.2)
D P.d jr/  P.qjd; r/ (3.3)
In equation 3.2, d depends on q, whereas in equation 3.3, q depends on d . P.d jq/ can be viewed
as a foundation of TF-IDF, and P.qjd/ is the foundation of LM. erefore, for LM, Lafferty
and Zhai [2003] investigated how to relate P.qjd; r/ to P.qjd/; see also Equation 2.51 (p. 24),
P.d jq; r/; Equation 2.52 (p. 24), P.qjd; r/.
e probabilities P.d jq; r/ and P.qjd; r/ are estimated as products of term probabilities.
For an event h (hypothesis h) and event e (evidence e), the decomposition is:
P.hje/ D
Y
t IN h
P.t je/ D
Y
t2h
P.t je/n.t;h/ (3.4)
In this decomposition, the notation “t IN h” views the event h as a sequence of event occurrences;
each t may occur multiple times. e notation “t 2 h” views the event h as a set, and for each t ,
n.t; h/ is the number of occurrences of t in h.
Accordingly, the probabilities P.d jq; r/ and P.qjd; r/ can be decomposed as follows:
P.d jq; r/ D
Y
t IN d
P.t jq; r/ (3.5)
P.qjd; r/ D
Y
t IN q
P.t jd; r/ (3.6)
Note the usage of t IN d , i.e., we have not yet aggregated the multiple occurrences the events.
In fact, we have not specified yet what type of event t is.
For example, d could be a sequence of terms (i.e., the event space is ft1; : : : ; tng), or d could
be a sequence of 0/1 events (i.e., the event space is f0; 1g), or d could be a sequence of integers
(i.e., the event space is f0; 1; 2; : : :g).
3.1. PRF: THE PROBABILITY OF RELEVANCE FRAMEWORK 81
For a sequence of terms, let n.t; q/ denote the number of times t occurs in query q. en
the independence assumption can be re-written as follows:
P.qjd; r/ D P.t1; : : : ; tmjd; r/ D
Y
t2q
P.t jd; r/n.t;q/ (3.7)
On the other hand, for the document, it is common to use a binary feature vector or a frequency
vector:
P.d jq; r/ D P.x1; : : : ; xnjq; r/ D
Y
t
P.xt jq; r/ (3.8)
P.d jq; r/ D P.f1; : : : ; fnjq; r/ D
Y
t
P.ft jq; r/ (3.9)
Retrieval (ranking) is usually based on the probabilistic odds P.r jd; q/=P. Nr jd; q/.
probabilistic odds: O.r jd; q/ D P.r jd; q/
P. Nr jd; q/
(3.10)
For documents that are more likely to be relevant than not relevant, P.r jd; q/ > P. Nr jd; q/, i.e.,
O.r jd; q/ > 1.
3.1.1 ESTIMATION OF TERM PROBABILITIES
Central to the PRF is the estimation of term probabilities. We outline in this section the issues
regarding the event space used to estimate the probabilities. A more detailed discussion is in
Section 3.6 (p. 92), Parallel Derivation.
e estimation of term probabilities can be based on
1. document frequencies,
2. location (term) frequencies, or
3. average frequencies.
Let u be a collection, and let Du be the set of Documents associated with the collection (for
relevant documents, u D r , and for non-relevant documents, u D Nr).
Let nD.t; u/ be the number of Documents in which term t occurs in u, and let nL.t; u/
be the number of Locations at which term t occurs in u. en, the document-frequency-based
estimation is:
P.t ju/ D PD.t ju/ D
nD.t; u/
ND.u/
(3.11)
is Document-based term probability is typically applied in the BIR model.
82 3. RELATIONSHIPS BETWEEN IR MODELS
Alternatively, the location-frequency-based estimation is:
P.t ju/ D PL.t ju/ D
nL.t; u/
NL.u/
(3.12)
is Location-based term probability is typically applied in LM.
e third estimation is based on the average term frequency. ere are two averages:
1. elite.t; u/ WD nL.t; u/=nD.t; u/ D tfu=df.t; u/: the average frequency over the documents
in which term t occurs; we denote the elite-based average as avgtf.t; u/ WD elite.t; u/;
2. all.t; u/ WD nL.t; u/=ND.u/ D tfu=ND.u/: the average frequency over all the documents;
.t; u/ WD all.t; u/.
is leads to the third alternative for associating a probability with a term:
P.kt ju/ D PPoisson;.kt ju/ D
.t; u/kt
kt Š
 e .t;u/ (3.13)
e three alternatives can be summarized as follows:
P.d/ WD
8̂̂<̂
:̂
probability event space
P.1; 0; 0; 1; 0; : : :/ f0; 1g binary
P.t1; t4; t1; : : :/ ft1; t2; : : : g terms
P.2; 0; 0; 1; 0; : : :/ f0; 1; 2; : : : g frequencies
(3.14)
e first representation views the event d as a binary feature vector, the second as a sequence
of terms, and the third as a frequency vector; see also Section 3.6 (p. 92), Parallel Derivation of
Retrieval Models.
3.2 P.d ! q/: THE PROBABILITY THAT d IMPLIES q
van Rijsbergen [1986] proposed the framework of logical retrieval. e overall idea of P.d ! q/
is to define a logical implication that mirrors the notion of relevance. en, the relevance event is
replaced by the implication event. Mathematically, this is:
P.r jd; q/ / P.d ! q/ (3.15)
e probability of relevance is viewed to be proportional to the probability of the implication.
en, the task reduces (or expands, that depends on the view point and research) to esti-
mating P.d ! q/. One approach is as follows:
P.d ! q/ / P.qjd/ (3.16)
e probability of the implication is viewed to be proportional to the conditional probability.
3.3. THE VECTOR-SPACE “MODEL” (VSM) 83
When considering d and q as conjunctions of terms, then d ! q measures exhaustiveness
of d . For example, the implication t1 ^ t2 ! t1 is true, that is for each exhaustive document (docu-
ment that contains ALL query terms), P.qjd/ D P.d ! q/ D 1. e less exhaustive a document
is, the less P.d ! q/. On the contrary, P.q ! d/ measures the specificity of d .
Logical IR inspired various lines of research. By interpreting the implication in differ-
ent ways, various retrieval models can be explained; Wong and Yao [1995]. e possible world
semantics underlying probabilistic logics is a powerful framework to describe the “kinematics”
of probabilities, i.e., the transfer of probability mass from some possible worlds to other possi-
ble worlds; Crestani and van Rijsbergen [1995], “Probability Kinematics in IR;” Crestani and
Van Rijsbergen [1995], “IR by Logical Imaging.” is endeavour is closely related to the discus-
sion regarding “thin-tail” distributions in Section 2.4 (p. 35), Poisson and Section 2.8.1 (p. 63),
DFR. Moreover, in addition to using logical IR as a framework for IR models, and as a means
to adapt probabilistic reasoning, the logical implication d ! q also inspired work on semantic
retrieval, Meghini et al. [1993], Nie [1992], Roelleke and Fuhr [1996], since logic is an ideal
candidate to model semantics and relationships.
3.3 THE VECTOR-SPACE “MODEL” (VSM)
is book does not list the VSM as a retrieval “model.” Since retrieval models can be expressed
in vector/matrix algebra, we view the VSM as a framework to describe, relate, and implement
retrieval models.
Regarding the VSM being a model, the VSM views documents and queries as vectors, and
it defines an RSV based on the cosine of the angle between the vectors. e cosine of the angle
†. Ed; Eq/ is as follows:
cos.†. Ed; Eq// WD
Ed  Eqp
Ed 2 
p
Eq2
(3.17)
e query vector Eq is constant for a set of documents. erefore, the query length
p
Eq2 does not
affect the ranking. en, RSVVSM.d; q/ of the document-query pair .d; q/ is defined as follows:
Definition 3.1 VSM retrieval status value RSVVSM.
RSVVSM.d; q/ WD cos.†. Ed; Eq// 
q
Eq2 D
Ed  Eqp
Ed 2
(3.18)
e vector-space framework can be viewed as an alternative to the probabilistic relational algebra.
An engine that provides vector algebra operations can be used to implementmany retrieval models
(Cornacchia and de Vries [2007], Roelleke et al. [2006]).
Critical for the performance of the VSM is the choice of the vector-space and the setting
of the vector components. A common approach is to consider a space of term vectors Eti . is is
84 3. RELATIONSHIPS BETWEEN IR MODELS
reflected in the following example where we use a space of three terms of which the query contains
t1 and t2:
Eq D
0@ wq;1wq;2
wq;3
1A D 0@ 11
0
1A D 1  Et1 C 1  Et2 C 0  Et3 (3.19)
We use wq;i for referring to the i-th component of vector Eq. Further, let two documents d1 and
d2 be given:
Ed1 D
0@ wd1;1wd1;2
wd1;3
1A D 0@ 04
1
1A
Ed2 D
0@ wd2;1wd2;2
wd2;3
1A D 0@ 10
2
1A
Here, the vector components show that t2 and t3 represent d1 where t2 is a better representative
than t3. For d2, t3 is the best representative.
In the VSM, the normalization is with respect to the length of the vector.q
Ed 2 D
q
w2
d;1
C : : : C w2
d;n
(3.20)
is normalization leads to vectors of length 1:0.
1:0 D
 
Edp
Ed 2
!2
(3.21)
is is the so-called Euclidean norm, also referred to as the L2 norm, j Ed jL2 D
qP
i w
2
d;i
, as
opposed to the L1 norm, which is j Ed jL1 D
P
i wd;i .
e scalar product of document and query is:
Ed T  Eq D
X
i
wd;i  wq;i (3.22)
A common approach is to set the document and query components as TF-IDF weights. In the
VSM, TF-IDF weights are used for both, document and query. e following RSV uses within-
document and within-query frequencies, and multiplies them with idf. e normalization is wrt
the document. Let tfd  idf.t; c/ be the document vector components, and let tfq  idf.t; c/ be the
query vector components.
RSVVSM.d; q/ D
1p
Ed 2

X
t
tfd  tfq  .idf.t; c//2 (3.23)
3.4. THE GENERALISED VECTOR-SPACE MODEL (GVSM) 85
3.3.1 VSM AND PROBABILITIES
e VSM can be used in a convenient way to model probabilities; this approach has been utilized
in Wong and Yao [1995] in the context of modeling P.d ! q/. Let t1; : : : ; tn be disjoint and
exhaustive events, i.e., P.ti ^ tj / D 0 and
P
t P.t/ D 1.en, the theorem of the total probability
yields for the probabilities P.qjd/ and P.d jq/:
P.qjd/ D
X
t
P.qjt/  P.t jd/ (3.24)
P.d jq/ D
X
t
P.d jt /  P.t jq/ (3.25)
For P.qjd/, the interpretation of the query vector and the document vector is:
Eq D
0B@ P.qjt1/:::
P.qjtn/
1CA Ed D
0B@ P.t1jd/:::
P.tnjd/
1CA (3.26)
en, the vector product is equal to a probability, i.e., P.qjd/ D Eq  Ed .
3.4 THE GENERALISED VECTOR-SPACE MODEL (GVSM)
One of the main motivations to include the GVSM into this book is the following relationship
between the total probability and the GVSM:
P.qjd/ D
X
t
P.qjt /  P.t jd/ D
1
P.d/

X
t
P.qjt /  P.d jt /  P.t/ D Ed T  G  Eq (3.27)
e vector Ed reflects P.d jt/, and the matrix G contains the term probabilities P.t/ on the main
diagonal, and the vector Eq reflects P.qjt /.
Before we continue the discussion of this relationship, we define RSVGVSM and review
the main motivation for the GVSM. e matrix components gij associate the component of
dimension Eti with the component of dimension Etj . e RSVGVSM is defined as follows:
Definition 3.2 GVSM retrieval status value RSVGVSM.
RSVGVSM.d; q; G/ WD Ed T  G  Eq (3.28)
For keeping the formalism light, we omit here the normalization. For G D I (I is the identity
matrix, i.e., gi i D 1, upper and lower triangle is zero), the generalized product is equal to the
scalar product.
Ed T  I  Eq D Ed T  Eq D wd;1  wq;1 C : : : C wd;n  wq;n (3.29)
86 3. RELATIONSHIPS BETWEEN IR MODELS
Next, consider an example where g21 D 1, i.e., the product wd;2  wq;1 affects the RSV.
G D
24 1 0 01 1 0
0 0 1
35
is matrix G leads to the following RSV:
RSVGSVM.d; q; G/ D .wd;1Cwd;2/  wq;1 C wd;2  wq;2 C wd;3  wq;3 (3.30)
e GVSM is useful for matching semantically related terms. For example, let t1 D
“classification” and t2 D “categorization” be two dimensions of the vector-space. en, for the
example matrix G above, a query for “classification” (wq;1 D 1) retrieves a document contain-
ing “categorization” (wd;2 D 1), even though wq;2 D 0, i.e., “categorization” does not occur in
the query, and wd;1 D 0, i.e., “classification” does not occur in the document. is facility of the
GVSM makes it the foundation of “LSA: latent semantic analysis,” Deerwester et al. [1990], Du-
mais et al. [1988], Dupret [2003], also referred to as LSI, “latent semantic indexing.” Hofmann
[1999] describes the probabilistic variant, probabilistic latent semantic indexing (PLSI).
3.4.1 GVSM AND PROBABILITIES
e GVSM has an interesting relationship with the total probability. With Ed and Eq having com-
ponents P.d jt/ and P.qjt /, we can define the matrix G to contain P.t/ on the main diagonal,
with upper and lower triangle containing zeros. is leads to:
P.d; q/ D
 
P.d jt1/ : : : P.d jtn/


0B@ P.t1/ 0: : :
0 P.tn/
1CA 
0B@ P.qjt1/:::
P.d jtn/
1CA (3.31)
is representation raises the question of what is the meaning of the triangle elements. e tri-
angle elements can be interpreted as conditional probabilities such as P.ti jtj /. us, the GVSM
points a way to relate terms that, in the basic formulation of the total probability theorem, are
disjoint events. e values in the upper and lower triangle reflect the dependence between terms,
i.e., the terms overlap. e overlap is zero for disjoint terms.
e next equation shows the effect for a matrix where t1 and t2 are not disjoint, i.e., the
terms “interfere.” For non-disjoint terms, the matrix elements are: g1;2 DP.t2jt1/¤0 and g2;1 D
P.t1jt2/¤0.
RSVGSVM.d; q; G/ D (3.32)
P.d jt1/  P.t1/  P.qjt1/ C P.d jt2/  P.t2jt1/  P.qjt1/ C
P.d jt2/  P.t2/  P.qjt2/ C P.d jt1/  P.t1jt2/  P.qjt2/ C
P.d jt3/  P.t3/  P.qjt3/
3.4. THE GENERALISED VECTOR-SPACE MODEL (GVSM) 87
With g1;2 and g2;1 set in this way, the query term probability P.qjt1/ is multiplied by P.t1/ 
P.d jt1/ and P.t2jt1/  P.d jt2/. With respect to the “classification=categorization” example, this
is:
P.d jt2/  P.t2jt1/  P.qjt1/
P.d jcategorization/  P.categorizationjclassification/  P.qjclassification/
is explains the role of G as a matrix of probabilities.
Next, we look at the GVSM slightly differently. We view G as the product of two matrices:
GT
d
 Gq . With the usual rules of vector-matrix algebra, we obtain the following scalar product:
Ed T  G  Eq D Ed T  .GTd  Gq/  Eq D .Gd 
Ed/T  .Gq  Eq/ (3.33)
is equation illustrates that the GVSM leads to a formulation where the matrix-times-vector
products yield a transformation of the vectors Ed and Eq into another space in which the scalar
product Eds  Eqs is equal to the generalized product Ed  G  Eq.
Eds
T
 Eqs D .Gd  Ed/
T
 .Gq  Eq/ (3.34)
With respect to the matrix G for modeling the total probability, this leads to the following ma-
trices:
Gd D
264
p
P.t1/ 0 0
0
p
P.t2/ 0
0 0
p
P.t3/
375 Gq D
264
p
P.t1/ 0 0
0
p
P.t2/ 0
0 0
p
P.t3/
375
e product of the matrices is:
G D GTd  Gq D
24P.t1/ 0 00 P.t2/ 0
0 0 P.t3/
35 (3.35)
Particular to this formalization are the square roots of the probabilities. is relates the GVSM
to the more general approaches that combine geometry and probabilities: Melucci [2008], Pi-
wowarski et al. [2010], van Rijsbergen [2004].
In summary, the discussion of the VSM/GVSM framework pointed out that any retrieval
model (e.g., TF-IDF, BM25, LM) can be expressed in a vector-like way. Geometric and prob-
abilistic concepts can be combined, and the combination is based on choosing probabilities as
vector/matrix components. e geometric operations (products of matrices and vectors) yield
probabilities.
88 3. RELATIONSHIPS BETWEEN IR MODELS
3.5 A GENERAL MATRIX FRAMEWORK
e original motivation of the general matrix framework, Roelleke et al. [2006], was to express
several retrieval models and other concepts (e.g., structured document retrieval, evaluation mea-
sures) in a concise mathematical framework. Initially a secondary result was to establish a notation
strictly based on the mathematical notion of matrices. From today’s point of view, the notation
gained a significant role. e matrix framework is the foundation of the notation, Section 1.3
(p. 3), employed for describing several retrieval models.
We revisit in this section the basics of the general matrix framework.
3.5.1 TERM-DOCUMENT MATRIX
Figure 3.1 shows a term-document matrix, a representation commonly used in IR.
TDc Dc
doc1 doc2 doc3 doc4 doc5 nD.t; c/ nL.t; c/
Tc
sailing 1 2 1 1 0 4 5
boats 1 1 0 0 1 3 3
east 0 0 1 0 0 1 1
coast 0 0 1 0 0 1 1
nT .d; c/ 2 2 3 1 1
nL.d; c/ 2 3 3 1 1
Figure 3.1: Term-Document Matrix TDc : Transpose of DTc .
We refer to this matrix as TDc since
1. the horizontal dimension corresponds to Terms,
2. the vertical dimension corresponds to Documents,
3. the dimensions span the space c (c is the collection).
From a formal point of view, the matrix represents the function c W Tc  Dc ! F , where Tc is
the set of Terms and Dc is the set of Documents in collection c. F D f0; 1; 2; : : :g is the set of
frequencies.e function c.t; d/ corresponds to thematrix element, which is thewithin-document
term frequency, and this leads to the following definition: tfd WD c.t; d/.
In a similar way, there are matrices per document and per query. For example, TLd is the
Term-Location matrix of document d . e function d W Td  Ld ! f0; 1g returns the matrix
element, where Td is the set of Terms and Ld is the set of Locations in document d .
e notation nL.t; d/ WD c.t; d/ makes explicit that the elements c.t; d/ of matrix TDc
correspond to the number of Location at which term t occurs in document d .
en, for the matrices TDc and TLd , the dual notation to refer sums of matrix elements,
is as follows. For the collection matrix TDc , we obtain:
3.5. A GENERAL MATRIX FRAMEWORK 89
1. nT .d; c/: number of Terms that occur in d of c
2. nD.t; c/: number of Documents in which t occurs in c
In a dual way, for the document matrix TLd , we obtain:
1. nT .l; d/: number of Terms that occur at l in d
2. nL.t; d/: number of Locations at which t occurs in d
e notation shown in figure 3.2 underlines how to derive a consistent notation based on
the duality between TDc and TLd ; see also Figure 1.2 (p. 5), Notation. According to this formal,
nD.t; c/ number of Documents in which term t occurs in collection c jfd jd 2 Dc ^ c.t; d/ > 0gj
nL.t; c/ number of Locations at which term t occurs in collection c
P
d2Dc
c.t; d/
nT .d; c/ number of Terms in document d in collection c jft jt 2 Tc ^ c.t; d/ > 0gj
nL.d; c/ number of Locations in document d in collection c
P
t2Tc
c.t; d/
ND.c/ number of Documents in collection c jDc j
NT .c/ number of Terms in collection c jTc j
NL.c/ number of Locations in collection c
P
t nL.t; c/.D
P
d nL.d; c//
Frequencies
df.t; c/ document frequency of term t df.t; c/ WD nD.t; c/
tf.d; c/ term frequency of document d tf.d; c/ WD nT .d; c/
lf.t; c/ location frequency of term t lf.t; c/ WD nL.t; c/
lf.d; c/ location frequency of document d lf.d; c/ WD nL.d; c/
Figure 3.2: Dual Notation based on Term-Document Matrix TDc .
matrix-based approach to define counts, tf.d; c/ WD nT .d; c/ is the term frequency, i.e., the num-
ber of distinct terms in a document. is is consistent and dual to the document frequency, i.e.,
the number of distinct documents that contain a term. For maintaining a clear link to the tradi-
tional notation, we define tfd WD nL.t; d/ and tfc WD nL.t; c/. Also, dl WD NL.d/ WD nL.d; c/ is
a consistent definition to refer to the document length of document d in collection c.
Retrieval parameters can be conveniently expressed in a mathematically consistent way. For
example:
PL.t jc/ WD
nL.t; c/
NL.c/
D
tfc
NL.c/
PL.d jc/ WD
nL.d; c/
NL.c/
D
dl
NL.c/
PL.t jd/ WD
nL.t; d/
NL.d/
D
tfd
dl
PD.t jc/ WD
nD.t; c/
ND.c/
D
df.t; c/
ND.c/
PT .d jc/ WD
nT .d; c/
NT .c/
D
tf.d; c/
NT .c/
90 3. RELATIONSHIPS BETWEEN IR MODELS
e notation supports the duality between inverse document frequency (high for rare/discrimi-
native terms) and inverse term frequency (high for short/concise documents):
idf.t; c/ WD   log PD.t jc/ itf.d; c/ WD   log PT .d jc/
Similar to the way a document-term matrix represents a collection, there are location-term ma-
trices that represent the documents. Moreover, matrices are convenient for modeling links and
structure. is leads to the following types of matrices:
DTc Document-Term matrix of collection c
STd Section-Term matrix of document d
LTd Location-Term matrix of document d
PCc Parent-Child matrix of collection c
SDc Source-Destination (link) matrix of collection c
In a dual way to the counts for the DTc , the counts for the other matrices follow. For example:
nD.t; c/ number of Documents with term t in document d
nS .t; d/ number of Sections with term t in document d
nL.t; d/ number of Locations with term t in document d
nP .x; c/ number of Parents with child x in collection c
nS .d; c/ number of Sources with destination d in collection c
e matrix approach then leads to dual notions such as “inverse section frequency of a term”
and “inverse source frequency of a destination.” ese are well-defined parameters that can be
exploited for ranking retrieved items.
3.5.2 ON THE NOTATION ISSUE “TERM FREQUENCY”
is mathematical, matrix-based approach to IR makes explicit that for the two dimensions T
and D, the term frequency is a feature of a document, and the document frequency is a feature
of a term.
term frequency document frequency
tf.d; c/ WD nT .d; c/ df.t; c/ WD nD.t; c/
e notion of “term frequency” in traditional IR leads to ambiguities. e “term frequency” used
in IR, is actually the “location frequency of a term.” is is the number of locations at which the
term occurs. e following table underlines this notation issue.
traditional notation
term frequency of d in c tf.d; c/ WD nT .d; c/
document frequency of t in c df.t; c/ WD nD.t; c/
location frequency of t in d lf.t; d/ WD nL.t; d/ tfd WD nL.t; d/
location frequency of t in c lf.t; c/ WD nL.t; c/ tfc WD nL.t; c/
3.5. A GENERAL MATRIX FRAMEWORK 91
e subscripts D, T , and L indicate the space; see also Section 1.3 (p. 3), Notation; Section 1.3.1
(p. 6), TF Notation Issue.
3.5.3 DOCUMENT-DOCUMENT MATRIX
Finally, we consider in this section matrices for modeling structure (link) information. For exam-
ple, a relation covering the structure of the collection, as shown in the matrix CPc (child-parent
in collection c) in figure 3.3.
Figure 3.3: Child-Parent Matrix CPc : Transpose of PCc .
Parameters (frequencies) such as nC .parent; c/ and nP .child; c/ are consistently defined
based on the dimensions Child and Parent. For a tree structure, a child has one parent, and for
a graph, we generalize toward source (parent) and destination (child), and a destination can be
reached from several sources.
3.5.4 CO-OCCURRENCE MATRICES
For all matrices, similar operations can be defined. For example, the self-multiplications to model
co-occurrence, co-references, and co-citations.
document similarity (co-occurrence for terms): DDc D DTc  TDc (3.36)
term similarity (co-occurrence in documents): TTc D TDc  DTc (3.37)
parent similarity (co-reference): PPc D PCc  CPc (3.38)
child similarity (co-citation): CCc D CPc  PCc (3.39)
e vision is that given a system that can handle large-scale matrix operations, techniques
of IR can be implemented on such a common and unifying framework in a flexible and re-usable
way, Cornacchia and de Vries [2007].
92 3. RELATIONSHIPS BETWEEN IR MODELS
3.6 A PARALLEL DERIVATION OF PROBABILISTIC
RETRIEVAL MODELS
Figure 3.4 shows the term probabilities and related parameters as they occur in BIR, Poisson, and
LM.
Figure 3.4: Parallel Derivation of Probabilistic Retrieval Models: Event Spaces.
is parallel consideration of event probabilities makes explicit the event spaces underlying
the models. An event is either a binary feature, a frequency, or a term.
For BIR, the subscript D in PD.xt jc/ shows that the probability is based on counting
“Documents,” and the event notation xt shows that the random variable is over binary term
features, i.e., xt 2 f0; 1g. However, we will often use PD.t jc/ and PD.Nt jc/ instead. Formally, the
relationship between these two different expressions can be captured as follows:
PD.t jc/ WD PD.xt D 1jc/
PD.Nt jc/ WD PD.xt D 0jc/
Also, it is sometimes easy to employ the following relationship between the binary event proba-
bility PD.xt jc/ and the term probability PD.t jc/:
PD.xt jc/ D PD.t jc/
xt  PD.Nt jc/
1 xt
3.7. THE POISSON BRIDGE: PD.t ju/  avgtf.t; u/ D PL.t ju/  avgdl.u/ 93
is equation can be used in the context of the binomial probability. If xt D 1, then PD.t jc/ is
the term probability; if xt D 0, then it is PD.Nt jc/.
For Poisson, the subscript  in P.kt jc/ shows that the probability is based on the average
occurrence , and the event notation kt shows that the random variable is over frequencies, i.e.,
kt 2 f0; 1; 2; : : :g. e probability of a term frequency is the basis of DFR, Section 2.8.1 (p. 63),
Divergence from Randomness.
For LM, the subscript L in PL.t jc/ shows that the probability is based on counting “Loca-
tions,” and the event notation t shows that the random variable is over terms, i.e., t 2 ft1; : : : ; tng.
is discussion shows that LM is based on “proper” term probabilities, whereas BIR and
Poisson rely on probabilities of term features. Moreover, the parallel derivation of the models
elicits the relationships between models given the relationships between event spaces.
3.7 THE POISSON BRIDGE:
PD.t ju/  avgtf.t; u/ D PL.t ju/  avgdl.u/
In TF-IDF, the Document-based term probability PD.t jc/ occurs, and in LM, the Location-
based term probability PL.t jc/ is the background probability. For investigating the relationships
between models, the Poisson bridge is a helper to relate those two probabilities.
e bridge is based on the average frequency of the term, and since the average is the
Poisson parameter, we refer to the relationship as “Poisson Bridge.” Let .t; u/ be the average
term frequency of term t over all documents in set u. Let avgtf.t; u/ be the average term frequency
over elite documents (documents in which term t occurs). Let avgdl.u/ be the average document
length. e Poisson bridge, Roelleke and Wang [2006], is defined as follows:
Definition 3.3 Poisson Bridge. Let u be the background (universal collection). For example,
u D c for the collection (set of all documents), or u D r for the set of relevant documents, or
u D Nr for the set of non-relevant documents.
avgtf.t; u/  PD.t ju/ D .t; u/ D avgdl.u/  PL.t ju/ (3.40)
To verify the equation, the decomposed form of the Poisson bridge is given next.
nL.t; u/
nD.t; u/

nD.t; u/
ND.u/
D
nL.t; u/
ND.u/
D
NL.u/
ND.u/

nL.t; u/
NL.u/
(3.41)
To illustrate the Poisson bridge, we look at an example.
94 3. RELATIONSHIPS BETWEEN IR MODELS
For the term “africa” in TREC-2, the Poisson bridge is as follows.
Example 3.4 Poisson Bridge: “africa” in TREC-2 e basic statistics regarding number of
documents, averages, and term occurrence are:
ND.trec2/ D 742; 611
avgdl.trec2/  350
NL.trec2/  25m
nD.africa; trec2/ D 8; 533
nL.africa; trec2/ D 19; 681
e Poisson bridge is:
19; 681
8; 533

8; 533
742; 611
D
19;681
742;611
D
25m
742; 611

19; 681
25m
avgtf.t; c/  PD.t jc/ D .t; c/ D avgdl.c/  PL.t jc/
e Poisson bridge helps to insert the Document-based term probability, PD.t jc/, into
LM-based expressions, where the Location-based term probability, PL.t jc/, is applied. Also, vice
versa, the Location-based term probability can be inserted into TF-IDF, BIR, BM25, etc. ese
transformations isolate or make explicit parameters in retrieval models. Overall, this supports a
theoretical/mathematical approach to relate IR models.
3.8 QUERY TERM PROBABILITY ASSUMPTIONS
3.8.1 QUERY TERMMIXTURE ASSUMPTION
is assumption is an extreme mixture assumption, assigning either the foreground (within-
query) or the background (within-collection) probability.
Definition 3.5 Query termmixture assumption.
P.t jq; c/ D

P.t jq/ if t 2 q
P.t jc/ if t 62 q (3.42)
see also Section 3.9.4 (p. 101), TF-IDF and LM.
3.8. QUERY TERM PROBABILITY ASSUMPTIONS 95
3.8.2 QUERY TERM BURSTINESSASSUMPTION
In the probabilistic derivation of IR models, we find expressions such as PL.t jq/=PL.t jc/. To
relate such an expression to the document-based term probability, PD.t jc/, we define the query
term burstiness assumption.
Definition 3.6 Query term burstiness assumption.
P.t jq; c/ D
avgtf.t; c/
avgdl.c/
(3.43)
Because of the Poisson bridge, Section 3.7 (p. 93), the following equation holds:
P.t jq; c/ D
avgtf.t; c/
avgdl.c/
D
PL.t jc/
PD.t jc/
(3.44)
Example 3.7 Query term burstiness assumption. Let the term t occur in 0:1 percent of
the Documents, i.e., PD.t jc/ D1/1,000. For ND.c/ D 106 documents, this means df.t; c/ D
nD.t; c/ D1,000.
ere are at least 1,000 term occurrences. Let the term occur in tfc D nL.t; c/ D2,000
Locations, i.e., the average term occurrence is avgtf.t; c/ D2,000/1,000=2.
en, for avgdl.c/ D1,000, the Location-based collection-wide term probability is
PL.t jc/ D2,000/109 D 2=106.
For the within-query term probability, the assumption means:
PL.t jq/ D
PL.t jc/
PD.t jc/
D
2=106
1=1,000
D
2
1,000
D
avgtf.t; c/
avgdl.c/
e within-query term probability, P.t jq/, is large if the term is bursty (high avgtf.t; c/).
e rationale of the assumption is that the “virtual” query length is equal to the average document
length, and the query term occurs with frequency tfq D avgtf.t; c/ in the query. For real-world
collections and the informative terms, the average term frequency is less than the average docu-
ment length, avgtf.t; c/ < avgdl.c/, and therefore, PL.t jc/ < PD.t jc/.
is assumption means we can transform log PL.t jq/
PL.t jc/
into log 1
PD.t jc/
. is can be used to
relate the expression P.d jq/=P.d/ to TF-IDF; see also Section 3.9.4 (p. 101), TF-IDF and LM.
96 3. RELATIONSHIPS BETWEEN IR MODELS
3.8.3 QUERY TERM BIRASSUMPTION
Similar to the motivation for the query term burstiness assumption, we are looking for ways to
relate P.t jq/=P.t jc/ to .1   P.t jc//=P.t jc/, the latter being the expression familiar from Sec-
tion 2.3 (p. 29), BIR.
Definition 3.8 Query term BIR assumption.
P.t jq; c/ D

1   P.t jc/ if t 2 q
P.t jc/ if t 62 q (3.45)
is assumptionmeans we can transform P.t jq/
P.t jc/
into 1 P.t jc/
P.t jc/
. For a rare term, 1   P.t jc/ 
1, which is applied to establish the relationships between BIR and IDF.
Interesting in this context is the discussion whether P.t jc/ is Document-based or
Location-based. For the Location-based estimate, we can apply the Poisson bridge: PL.t jc/ D
avgtf.t;c/
avgdl.c/  PD.t jc/. is leads to
avgdl.c/=avgtf.t;c/ PD.t jc/
PD.t jc/
. is expression explicitly considers the
burstiness of the terms.
3.9 TF-IDF
3.9.1 TF-IDF AND BIR
TF-IDF and BIR can be related by viewing the probability PD.t jc/ as an approximation of the
probability PD.t j Nr/, which occurs in the BIR term weight. For TF-IDF, Definition 2.6 (p. 16),
RSVTF-IDF, and BIR, Definition 2.16 (p. 31), RSVBIR, we have:
RSVTF-IDF.d; q; c/ D
X
t2d\q
wTF-IDF.t; d; q; c/ (3.46)
RSVBIR.d; q; r; Nr/ D
X
t2d\q
wBIR.t; r; Nr/ (3.47)
e respective term weights are:
wTF-IDF.t; d; q; c/ D TF.t; d/  TF.t; q/  IDF.t; c/ D
tfd
tfd C Kd

tfq
tfq C Kq
   log PD.t jc/
(3.48)
wBIR.t; r; Nr/ WD log

PD.t jr/
PD.t j Nr/

PD.Nt j Nr/
PD.Nt jr/

(3.49)
Robertson [2004] views BIR as a theoretical justification of IDF, and in the following, we review
this relationship.
3.9. TF-IDF 97
Croft and Harper [1979] proposed for missing relevance, to assume that P.t jr/=P.Nt jr/ is
a constant.
Cr D
P.t jr/
1   P.t jr/
(3.50)
When inserting this into the BIR term weight, we obtain:
wBIR;Cr .t; r; Nr/ D log.Cr/ C log
1   P.t j Nr/
P.t j Nr/
(3.51)
is equation underlines that in general the factor log Cr is part of the BIR term weight for
missing relevance. For establishing the justification of IDF, let us assume Cr D 1.en, we obtain:
wBIR;Cr D1.t; r; Nr/ D log
1   P.t j Nr/
P.t j Nr/
(3.52)
e next step is to assume that the statistics in non-relevant, Nr , can be approximated by the
statistics in the collection, c. en, we obtain the collection-based BIR term weight for missing
relevance information (Dr D fg):
IDFBIR.t; c/ D log
1   PD.t jc/
PD.t jc/
D wBIR.t; r; Nr  c/ ŒDr D fg (3.53)
For P.t jc/ WD PD.t jc/ D nD.t; c/=ND.c/, this becomes:
IDFBIR.t; c/ D log
ND.c/   nD.t; c/
nD.t; c/

D log N   nt
nt

(3.54)
For N >> nt , the following approximation can be applied:
log N   nt
nt
 log N
nt
(3.55)
is establishes the ground to view IDF as an approximation of the BIR term weight.
wBIR.t; r; Nr  c/  IDF.t; c/ (3.56)
Another formulation of the relationship between TF-IDF and BIR is reported in de Vries
and Roelleke [2005], where IDF is used to rewrite the BIR term weight. For the simplified (term
presence-only) BIR term weight, this is:
wBIR;presence-only.t; r; Nr/ D IDF.t; Nr/   IDF.t; r/ (3.57)
e assumption IDF.t; r/ D 0, i.e., a query term is assumed to occur in all relevant documents,
leads to wBIR.t; c/ D IDF.t; c/.
98 3. RELATIONSHIPS BETWEEN IR MODELS
When considering presence and absence, the relationship is:
wBIR.t; r; Nr/ D IDF.t; Nr/   IDF.t; r/ C IDF.Nt ; r/   IDF.Nt ; Nr/ (3.58)
is equation makes explicit that BIR can be viewed as a linear combination of IDF values.
In summary, the relationship between TF-IDF and BIR may be utilized to replace the IDF
component in TF-IDF by an BIR-motivated weight:
RSVTF-BIR.d; q; r; Nr; c/ WD
X
t2d\q
TF.t; d/  TF.t; q/  wBIR.t; r; Nr/ (3.59)
When using the RSJ term weight (smooth variant of the BIR term weight), then this becomes:
RSVTF-RSJ.d; q; r; Nr; c/ WD
X
t2d\q
TF.t; d/  TF.t; q/  wRSJ.t; r; Nr/ (3.60)
is can be viewed as the foundation of BM25.
3.9.2 TF-IDF AND POISSON
e frequency-based formulation of the PRF applied the Poisson probability; see also Section 2.4
(p. 35), Poisson. is leads to an expression that explains TF-IDF.
e probability can be based on all of the documents, or on a subset (an elite set) of the
documents. For example, the documents that contain at least one query term could be considered
as the elite documents. Figure 3.5 shows the formulation of the parameters involved. Let u denote
a set of documents. For example: u D c for all documents or u D r for relevant documents. e
notation uq refers to the elite documents (that contain at least one query term). Morever, avgdl.u/
is the average documents over all documents in u, and avgdl.uq/ is the average over the elite
documents.
e logarithm of the expected frequency .t; d; u/ leads to an expression where the IDF
becomes explicit.
log .t; d; u/ D log .dl  PL.t ju// D log
 dl
avgdl.u/
 avgtf.t; u/  PD.t ju/

D
D log
 dl
avgdl.u/

C log.avgtf.t; u//   IDF.t; u/
In addition to the IDF, the expression contains a component reflecting length normaliza-
tion, and a component reflecting burstiness.
e factor dl=avgdl equals one for a document of average length. Moreover, with respect tolength nor-
malization the PRF, the fraction of relevant and non-relevant documentsmeans that dl cancels out. However,
the factor avgdl.r/=avgdl. Nr/ becomes explicit through this formulation.
3.9. TF-IDF 99
Figure 3.5: Poisson Parameter  for all and elite Documents.
e logarithm of the average term frequency in elite documents measures burstiness:burstiness
burstiness.t; u/ WD log avgtf.t; u/ (3.61)
A term is bursty if it occurs with a high term frequency in the documents in which it occurs.
Using burstiness, IDF, and average document length, the Poisson term weight can be formulated
as follows:
wPoisson.t; d; q; r; Nr/ D (3.62)
TF.t; d/  .IDF.t; Nr/   IDF.t; r// C
TF.t; d/  .burstiness.t; r/   burstiness.t; Nr// C
TF.t; d/  log avgdl. Nr/
avgdl.r/
When grouping the expression by relevant and non-relevant, we obtain:
wPoisson.t; d; q; r; Nr/ D (3.63)
TF.t; d/ 
 
IDF.t; Nr/   burstiness.t; Nr/ C log avgdl. Nr/

 
TF.t; d/ 
 
IDF.t; r/   burstiness.t; r/ C log avgdl.r/

e expressions underline the interplay between burstiness and IDF and average document
length, between relevant and non-relevant. e term weight is high if the term is bursty and
frequent in relevant documents, and solitude (not bursty) and rare in non-relevant documents.
With respect to the Poisson term weight, TF-IDF can be viewed as assuming IDF.t; c/ 
IDF.t; Nr/   IDF.t; r/, i.e., the collection-based IDF is proportional to the difference between
100 3. RELATIONSHIPS BETWEEN IR MODELS
the IDF over non-relevant and over relevant documents. For good terms, the approximation
IDF.t; r/  0 feels reasonable, if we were to assume that good terms are frequent in relevant
documents. Moreover, TF-IDF can be viewed as assuming that burstiness is the same in relevant
and non-relevant documents. More precisely, TF-IDF can be viewed as a model that assumes
that the combination of burstiness and average document length components does not affect the
ranking.
3.9.3 TF-IDF AND BM25
When comparing the RSV’s of TF-IDF and BM25, then it becomes evident that where TF-IDF
uses IDF.t; c/, BM25 uses wRSJ.t; d; q; r; Nr/. We find in the literature BM25 formulations that
use IDF rather than the RSJ weight; see also Section 2.5 (p. 45), BM25. Also, the naive form
of TF-IDF is based on TFtotal, whereas BM25 uses TFBM25. Changing the TF quantification
leads to TFBM25-IDF, which can be viewed as an approximation of BM25 for the case of missing
relevance information.
e Laplace-like correction (smoothing) of the BIR term weight is the step to transform
the BIR weight into the RSJ weight:
wRSJ.t; r; c/ / IDFBIR.t; c/ D log
ND.c/   nD.t; c/ C 0:5
nD.t; c/ C 0:5
(3.64)
see also Section 2.3.5 (p. 33), RSJ Term Weight.
e following table shows a side-by-side of a naive and a BM25-motivated TF-IDF.
In summary, this section draws a relationship between TF-IDF and BM25, this relationship
being based on what has been discussed in Section 3.9.1 (p. 96), TF-IDF and BIR. For the case
of missing relevance, IDF can be viewed as an approximation of the RSJ term weight. On the
other hand, the RSJ term weight can be used instead of IDF. Regarding the TF, we discussed
that TFtotal corresponds to making an independence assumption, whereas TFBM25 captures the
3.9. TF-IDF 101
dependence of the multiple occurrences of the term; see also Section 2.1.7 (p. 17), Semi-subsumed
Events Occurrences. e relationship between TF-IDF and BM25 gives ground to the notion
“TF-RSJ,” where the TF component is BM25-TF, and the IDF term weight is replaced by the
RSJ term weight.
3.9.4 TF-IDF AND LM
Hiemstra [2000] showed a potential way to view LM as a probabilistic interpretation of TF-IDF.
Essentially, the relationship is based on the observation that there is an inverse term probability
involved in both TF-IDF and LM.
Moreover, Hiemstra [2000] and Kamps et al. [2004], “Length Normalization in XML
Retrieval,” show the following estimates for the term probabilities used in LM:
P.t jd/ D
tf.t; d/P
t 0 tf.t 0; d /
D
tfd
dl
(3.65)
P.t jc/ D
df.t; c/P
t 0 df.t 0; c/
D
df.t; c/
ND.c/
(3.66)
e Location-based within-document term probability, P.t jd/ WD PL.t jd/, is the foreground
probability, and the Document-based collection-wide term probability, P.t jc/ WD PD.t jc/, is
the background probability. e subscripts L and D indicate that the foreground is “Location-
based” (the Locations at which terms occur are the balls in the urn) whereas the background is
“Document-based” (the Documents in which terms occur are the balls in the urn). From an event
space of view, the background probability should be “Location-based” as well.
PL.t jc/ D P.t jc/ D
tf.t; c/P
t 0 tf.t 0; c/
D
tfc
NL.c/
(3.67)
e “Location-based” probability is more correct in the sense that then both, the within-
document and the collection-wide probability are based on “Locations.”
When following the argument in Section 1.3 (p. 3), Notation, regarding “term frequency”
versus “location” frequency, the duality between the two term probabilities is evident.
PL.t jd/ D
lf.t; d/P
t 0 lf.t 0; d /
D
nL.t; d/
NL.d/
D
tfd
dl
(3.68)
PL.t jc/ D
lf.t; c/P
t 0 lf.t 0; c/
D
nL.t; c/
NL.c/
D
tfc
collection_length
(3.69)
By assuming a document-based background model for LM, Hiemstra [2000], the side-by-side
of simplified TF-IDF and LM term weights is as follows:
TF-IDF
TF.t; d/  log 1
PD.t jc/
LM
TF.t; q/  log

1 C
ı
1   ı

PL.t jd/
PD.t jc/

102 3. RELATIONSHIPS BETWEEN IR MODELS
Here, the inverse of PD.t jc/ is present in the TF-IDF and LM term weights. Given that
from a probabilistic, event-space-based, point of view, PL.t jc/ should be used in the LM formu-
lation, the question is how this affects the relationship between TF-IDF and LM. For discussing
this question, we apply the Poisson bridge to replace PL.t jc/ in the genuine LM formulation.
TF.t; q/  log

1 C
ı
1   ı

PL.t jd/
PL.t jc/

D TF.t; q/  log
0@1 C ı
1   ı

PL.t jd/
avgtf.t;c/
avgdl.c/  PD.t jc/
1A
Eventually we could construct from this expression a transformation to show a relationship be-
tween TF-IDF and LM.
When looking at this issue, it turned out that a more immediate relationship follows from
the fact that TF-IDF can be derived from P.d jq; c/ and LM from P.qjd; c/. Both, TF-IDF
and LM are proportional to the document-query independence measure.
RSVTF-IDF.d; q; c/ /
P.d; qjc/
P.d jc/  P.qjc/
(3.70)
RSVLM.d; q; c/ /
P.d; qjc/
P.d jc/  P.qjc/
(3.71)
Based on this view, TF-IDF and LM measure the same. Of course, they are not ranking equiva-
lent; the different decompositions into term probabilities, and the assumptions applied, break the
ranking equivalence.
TF-IDF estimates P.d jq; c/ from the product of term probabilities, P.t jq; c/, whereas
LM estimates P.qjd; c/ from P.t jd; c/. For the term probabilities, different mixtures are ap-
plied. TF-IDF is based on the “extreme mixture,” where P.t jq; c/ D P.t jq/ for query terms
and P.t jq; c/ D P.t jc/ for non-query terms. LM applies the linear mixture P.t jd; c/ D ıd 
P.t jd/ C .1   ıd /  P.t jc/ for all terms.
3.9.5 TF-IDF AND LM: SIDE-BY-SIDE
Figure 3.6 shows a side-by-side derivation of TF-IDF and LM.
TF-IDF, based on P.d jq; c/=P.d jc/, decomposes the document event, whereas LM,
based on P.qjd; c/=P.qjc/, decomposes the query event. is shows that the two models share
the same root:
TF-IDF  P.d jq; c/
P.d jc/
D
P.d; qjc/
P.d jc/  P.qjc/
D
P.qjd; c/
P.qjc/
 LM (3.72)
Starting out with the same root, TF-IDF decomposes the document event, and LM decomposes
the query event. TF-IDF can be viewed as applying an extreme mixture, whereas LM applies the
linear mixture. TF-IDF normalizes by P.d jc/, and LM by P.qjc/.
3.9. TF-IDF 103
Figure 3.6: TF-IDF and LM: Side-by-Side.
104 3. RELATIONSHIPS BETWEEN IR MODELS
Step 1 in Figure 3.6 highlights the type of mixture assumption: extrememixture assumption
on the TF-IDF side, linear mixture assumption on the LM side (Dirichlet mixture, parameter
ıd D dl=.dl C /).
Step 2 applies the logarithm. Also, it replaces the total term occurrence counts, n.t; d/
and n.t; q/, by TF quantifications. e TF quantification reflects the dependence of the multiple
occurrences of a term.
Step 3 on the TF-IDF side deals with the transformation from PL.t jq/=PL.t jc/ to
1=PD.t jc/. For this transformation, we apply the query term burstiness assumption:
P.t jq; c/ D
avgtf.t; c/
avgdl.c/
see also Section 3.8 (p. 94), Query Term Probability Assumptions. e assumption means that
with respect to this mixture-based derivation based on the event space “set of terms,” TF-IDF
assumes that the query term probability is equal to the fraction of the elite-average term fre-
quency, avgtf.t; c/, and the average document length, avgdl.c/. e elite-average term frequency
is proportional to burstiness.
Step 3 on the LM side inserts the mixture parameter and the likelihood estimate P.t jd/ D
tfd =dl.
Step 4 on the TF-IDF side inserts IDF. On the LM side, the final step combines the
expressions.
Overall, this side-by-side derivation of TF-IDF and LM, based on the event space of
“terms,” underlines that TF-IDF and LM can be viewed as measuring the same, namely the
document-query (in)dependence, P.d; qjc/=.P.d jc/  P.qjc//.
3.9.6 TF-IDF AND PIN’S
For independent terms, in the PIN d ! ft1; t2; t3g ! q, we derived the following equation:
P.qjd/ D
X
t
P.qjt /P
t 0 P.qjt
0/
 P.t jd/ (3.73)
see also Section 2.7 (p. 58), PIN’s; Equation 2.137 (p. 61), P.qjd/.
For the computation of P.qjt /, imagine to add virtually a tuple “.t; q/” for each query term,
to the collection. en, for each term, P.qjt/ > 0. For terms that do not occur in the collection,
i.e., only do occur in the query, P.qjt / D 1.
P.qjt / is greater for rare terms than for frequent terms, i.e., P.qjt / is proportional to
IDF.t/. Whereas P.qjt/ is based on all the term occurrences, IDF.t/ D  log PD.t/ is based
on the space of Documents.
For the computation of P.t jd/, if we choose the usual estimate, then this is P.t jd/ D
tfd =dl D TFsum.t; d/. We know for TF-IDF, that TFBM25.t; d/ performs better. e estimate
P.t jd/ D TFBM25.t; d/ D tfd =.tfd C K/ can be interpreted in the Laplace sense (see Amati and
van Rijsbergen [2002], Section 3.9.8 (p. 105), TF-IDF and DFR).
3.9. TF-IDF 105
is justifies the following relationships:
P.t jd/ / TF.t; d/ (3.74)
P.qjt/ / IDF.t/ (3.75)X
t
P.qjt /P
t 0 P.qjt
0/
 P.t jd/ /
X
t
IDF.t/P
t 0 IDF.t 0/
 TF.t; d/ (3.76)
e PIN RSV is proportional to TF-IDF. See also Equation 2.141 (p. 62), RSVPIN, Equa-
tion 2.30 (p. 16), RSVTF-IDF.
3.9.7 TF-IDF AND DIVERGENCE
Section 2.8 (p. 63), Divergence-based Models, has already pointed at relationships between TF-
IDF and divergence. In the following sections we look at two approaches: “TF-IDF and DFR:
Risk-times-Gain,” and “TF-IDF and DFR: A Gap-based Model.” Moreover, a relationship
between TF-IDF and divergence (in general) is captured under information theory, in Sec-
tion 3.11.9 (p. 112), Difference between Divergences: TF-IDF.
3.9.8 TF-IDF AND DFR: RISK TIMES GAIN
Amati and van Rijsbergen [2002] introduces a particular usage of DFR to interpret TF-IDF. To
make the step toward TF-IDF, there is the following definition of P1. “P1 is the probability that
a given document contains tfd tokens of the given term:”
P1.tfd jc/ WD
df.t; c/
ND.c/
tfd
(3.77)
is can be enhanced with any smoothing, for example:
P1.tfd jc/ WD
df.t; c/ C 0:5
ND.c/ C 1
tfd
(3.78)
en, we obtain an explanation of TF-IDF:
inf1.t; d; c/ D   log P1.tfd jc/ D tfd  idf.t; c/ (3.79)
Looking carefully at this formulation discloses that P1 is the probability of a particular sequence
of documents that all contain the respective term. e sequence contains tfd documents.
e reason why this setting of P1 is chosen is to establish a relationship between DFR and
TF-IDF. e missing link, namely the normalization of the TF component, can be achieved by
choosing inf2 accordingly.
inf2.t; d; c/ D 1   P2.tfd jc/ D 1  
tfd
tfd C 1
D
1
tfd C 1
(3.80)
106 3. RELATIONSHIPS BETWEEN IR MODELS
Here, P2.tfd jc/ models the aftereffect (Laplace law of succession).
With terminology regarding inf1, inf2, gain, risk, and loss, the formulation is as follows:
inf1 D gain C loss
wDFR D gain
wDFR D inf1   loss
loss D P2  inf1
risk D inf2 D 1   P2 D
1
tfd C K
wDFR D risk  inf1
wDFR.t; d; c/ D
tfd
tfd C 1
 idf.t; c/
is leads to a DFR-based formulation of TF-IDF with the BM25-TF quantification.
3.9.9 TF-IDF AND DFR: GAPS BETWEEN TERM OCCURRENCES
Clarifying the relationship between TF-IDF and DFR took more effort than expected, and I
would like to thank Gianni Amati for the discussions and hours we have spent together on the
what and how of DFR and TF-IDF.
I had been struggling with the risk-times-gain approach to arrive at the TF quantification
tfd =.tfd C K/. erefore, Gianni and I discussed repeatedly the roots of DFR. Currently, I pre-
fer an interpretation that rests on the combination of two models: a model for computing the
probability of a document that contains the term, and a model for computing the probability of
the gap between two term occurrences.
For the first model, the probability to find kt > 0 occurrences in a particular document d
is proportional to finding a document that contains t .
P.kt > 0jd; c/ / P.kt > 0jc/ (3.81)
In a more verbose way, the probabilities can be expressed as follows:
P.kt > 0 in document d jc/ / P.exists a document that contains t jc/ (3.82)
en, themaximum-likelihood estimate is the usual document-frequency-based term probability:
P.kt > 0jc/ D PD.t jc/ D
nD.t; c/
ND.c/

D
df.t; c/
ND.c/

(3.83)
We refer to this randomness model as Mdf, since it is based on the document frequency of the
term.
For the second randomness model, let term t occur tfd times in document d . en,
gap.t; d/ WD dl=.tfd C 1/ is the average gap length. For example, for a document where dl D 100,
3.9. TF-IDF 107
and for a term with tfd D 3, the average gap length is 25. is is independent of the posi-
tions of the term. Let the term occur at positions 10, 50, and 80. e average gap length is
.10 C 40 C 30 C 20/=.3 C 1/. Assuming a Poisson distribution, Pgap.0jt; d / D e gap.t;d/ is the
probability that a gap of length zero occurs.
e greater the probability of t 2 d , the greater the probability to find a gap of length zero,
i.e., the greater the probability that two term occurrences are adjacent to each other. erefore,
P.kt > 0/ is proportional to P.gap-length D 0/.
P.kt > 0jd; c/ / P.gap-length D 0jt; d / (3.84)
When choosing the gap probability as a randomness model Mgap, then we obtain:
 ln PMgap.t 2 d jc/ D   ln e dl=.tfd C1/ D
dl
tfd C 1
(3.85)
e combination of the two randomness models Mgap and Mdf is as follows:
wDFR;df+gap.t; d; c/ D wDFR;Mgap.t; d; c/  wDFR;Mdf.t; d; c/ D
 dl
tfd C 1



  log nD.t; c/
ND.c/

e next steps applies the cross entropy, Section 3.11.5 (p. 110):
Hcross.Pobs.kt > 0jd; c/; PDFR;df+gap.t jd; c// D
X
t2ft1;t2;:::g
Pobs.kt > 0jd; c/  wDFR;df+gap.t; d; c/
e observed probability is proportional to the within-document term probability.
Pobs.kt > 0jd; c/ / P.t jd/ D
tfd
dl
Inserting this into the cross entropy yields:
Hcross.P.t jd/; PDFR;df+gap.t jd; c// /
X
t2ft1;t2;:::g
tfd =dl  wDFR;df+gap.t; d; c/ (3.86)
Finally, inserting the DFR term weight and reducing the expression leads to:
Hcross.P.t jd/; PDFR;df+gap.t jd; c// /
X
t2ft1;t2;:::g
tfd
tfd C 1
 idf.t; c/ (3.87)
us, TF-IDF is shown to be proportional to the cross entropy between the within-
document term probability P.t jd/ D tfd =dl and the term probability PDFR;df+gap.t jd; c/ D
PD.t jc/
dl=.tfd C1/.
108 3. RELATIONSHIPS BETWEEN IR MODELS
3.10 MORE RELATIONSHIPS: BM25 AND LM, LM AND PIN’S
Regarding BM25 and LM, Lavrenko and Croft [2001], “Relevance-based Language Models,”
provide the starting point, and Lafferty and Zhai [2003] proposes a relationship between the odds
of relevance, O.r jd; q/ and P.qjd/. is relationship and the assumptions required to establish
it, are controversial, Luk [2008]; see also Section 2.2 (p. 23), PRF. Given the complexity of the
relationship between BM25 and LM, this relationship is not yet covered in this lecture series.
e research outlook positions the models and indicates the type of relationship to be explored.
Regarding LM and PIN’s, Metzler and Croft [2004], “Combining the language model and
inference network approaches to retrieval” describes the relationship, following up on Croft and
Turtle [1992], Turtle and Croft [1992] who laid out earlier the PIN case for IR. e case “LM
and PIN’s” is interesting because on one hand, there is the conjunctive decomposition of P.qjd/
as known for LM, and on the other hand, there is the disjunctive (weighted sum) decomposition
of P.qjd/ that is related to TF-IDF; Section 3.9.6 (p. 104), TF-IDF and PIN’s.
3.11 INFORMATION THEORY
We revisit in the following selected concepts of information theory that are relevant to IR.
ereby, we point at the relationships between divergence and IR models. In one overview, the
concepts are:
Section 3.11.1 Entropy H.X/
Section 3.11.2 Joint Entropy H.X; Y / WD Hjoint.X; Y /
Section 3.11.3 Conditional Entropy H.X jjY / D H.X; Y /   H.Y /
Section 3.11.4 Mutual Information MI.X; Y / D H.X/ C H.Y /   H.X; Y /
Section 3.11.5 Cross Entropy H.X I Y / WD Hcross.X; Y /
Section 3.11.6 KL-Divergence DKL .X jjY / D Hcross.X; Y /   H.Y /
Section 3.11.7 Query Clarity: DKL
 
PqjjPc

Section 3.11.8 LM = clarity(query) – divergence(query jj doc)
Section 3.11.9 TF-IDF = clarity(doc) – divergence(doc jj query)
3.11. INFORMATION THEORY 109
3.11.1 ENTROPY
When discussing relationships between IR models, then information theory and its most pop-
ular concept, entropy, have to be included, although the relationship between IR models and
information theory is controversial; Robertson [2004].
Definition 3.9 Entropy. Let XP D ft1; : : : ; tng be a set of events, where P.t/ is the probability
of event t . With respect to IR, the set could be a vocabulary, i.e., a set of terms (words). We can
also denote X as a probability distribution: X D .P.t1/; : : : ; P.tn//.
H.X/ denotes the entropy of X .
H.X/ WD
X
t2X
P.t/    log P.t/ (3.88)
More precisely, the notation is H.PX / or H.P.X//, but often the probability function P is omit-
ted.
is definition of entropy is sometimes referred to as “Shannon’s theorem,” though the
theorem is that entropy is equal to the expected code length.
e resemblance between entropy and TF-IDF has inspired researchers to establish a re-
lationship between TF-IDF and information theory, Aizawa [2003]. When discussing this with
StephenRobertson, he pointed out: “ere is no connection betweenTF-IDF and entropy (Shan-
non). e primary goal of the entropy formula is to estimate the bandwidth required to transfer
signals.” ough there is no direct relationship immediately evident, entropy-related concepts
(cross entropy, divergence) can be used to explain IR models; Zhai [2009]; see also Section 2.8.10
(p. 73), KL-Divergence Retrieval Model.
Next, we briefly review the interpretation of entropy, and introduce conditional entropy
and KL-divergence. Entropy is used to determine the bandwidth required to transfer a stream of
signals, where a signal corresponds to the occurrence of a token, and P.t/ is the probability of the
token.
Example 3.10 Entropy. Given two streams sa D 1; 1; 1; 0 and sb D 1; 1; 0; 0. e entropy’s are:
Hsa D 3=4    log2.3=4/ C 1=4    log2.1=4/ D 3=4  .2   log2 3/ C 1=4  .2   log2 1/ < 1
Hsb D 2=4    log2.2=4/ C 2=4    log2.2=4/ D .2=4  .2   log2 2//  2 D 1
Hsb > Hsa : in stream b the distribution of tokens is uniform; the entropy is maximal if the prob-
abilities are uniform (equi-probable).
Entropy is the expectation value of the code length. e bandwidth required to transfer
a stream is equal to the number of signals (length of stream) times the entropy (expected code
length).
110 3. RELATIONSHIPS BETWEEN IR MODELS
3.11.2 JOINT ENTROPY
Joint entropy is the entropy of the joint events of two random variables.
H.X; Y / WD Hjoint.X; Y / WD
X
x;y
P.x; y/    log P.x; y/ (3.89)
3.11.3 CONDITIONAL ENTROPY
Conditional entropy is the difference between joint entropy and entropy.
H.X jjY / WD H.X; Y /   H.Y / (3.90)
Conditional entropy, H.X jjY /, is not to be confused with KL-divergence, DKL .X jjY /.
3.11.4 MUTUAL INFORMATION (MI)
MI measures the (in)dependence between two random variables.
MI.X; Y / WD H.X/ C H.Y /   H.X; Y / (3.91)
For independence, MI.X; Y / D 0; however, MI.X; Y / D 0 does not necessarily mean indepen-
dence.
3.11.5 CROSS ENTROPY
Cross entropy is a generalization of entropy:
H.X I Y / WD Hcross.X; Y / WD
X
t
PX .t/    log PY .t/ (3.92)
For PX D PY , cross entropy is entropy. Cross entropy can be interpreted as the expected code
length when transferring a stream of signals, where the tokens are distributed according to
PX , and the encoding is based on PY . Regarding the notation, Hcross.PX ; PY /, is equivalent to
Hcross.X; Y /.
e following equation shows the relationship between cross entropy and joint entropy.
Hcross.P.X; Y /; P.X; Y // D Hjoint.X; Y / (3.93)
e cross entropy between X and Y is greater than (or equal to) the entropy of Y :
Hcross.X; Y /  H.Y / (3.94)
Intuitively, this is the case since an encoding, based on PY , of a stream of signals in which tokens
are distributed according to PX , requires more bandwidth than an encoding based on PY .
3.11. INFORMATION THEORY 111
Whereas the relationship between basic entropy and IR is not clear, cross entropy and IR
immediately connect for the case of LM; see also Section 2.8.10 (p. 73), KL-Divergence Retrieval
Model.
Hcross.Pq; Pd / D
X
t
Pq.t/   log Pd .t/ (3.95)
see also Section 3.11.8 (p. 112), Difference between Divergences: LM.
3.11.6 KL-DIVERGENCE
KL-divergence is the difference between cross entropy and entropy.
DKL.X jjY / WD Hcross.X; Y /   H.X/ (3.96)
KL-divergence is positive, since cross entropy is greater than entropy.
e KL-divergence can be applied to express mutual information (MI):
MI.X; Y / D DKL.P.X; Y /jjP.X/  P.Y // (3.97)
MI is the KL-divergence between the joint probability P.X; Y / and the product P.X/  P.Y /,
i.e., the divergence between the joint probability and “independence.”
e relationship between KL-divergence and conditional entropy is illustrated in the fol-
lowing sequence of equations:
H.X jjY / D H.X; Y /   H.Y /
D H.X; Y /   H.Y / C H.X/   H.X/
D H.X/   ŒH.X/ C H.Y /   H.X; Y /
D H.X/   MI.X; Y /
D H.X/   DKL .P.X; Y /jjP.X/  P.Y //
Conditional entropy is the difference between joint entropy and entropy. By adding 0 D H.X/  
H.X/ to the equation, one can rewrite the right side using MI and KL-divergence, respectively.
Retrieval models can be shown to be related to the KL-divergence. See also Section 2.8.10
(p. 73), KL-Divergence Retrieval Model.
3.11.7 QUERY CLARITY: DIVERGENCE(QUERY jj COLLECTION)
Query clarity is the divergence between the within-query term probability, Pq.t/ D P.t jq/, and
the collection-wide term probability, Pc.t/ D P.t jc/.
DKL
 
PqjjPc

D
X
t
Pq.t/  log
Pq.t/
Pc.t/
(3.98)
A query is clear if the term probabilities, P.t jq/ and P.t jc/, are different, i.e., if the term distri-
bution in the query is different from the term distribution in the collection.
112 3. RELATIONSHIPS BETWEEN IR MODELS
e concept of query clarity is related to “query selectivity.” ere are different ways to
define query selectivity, and one of the common approaches is the sum of IDF values.
In a dual way to query clarity, one can define document clarity.
e difference between divergences leads to expressions that can be used to explain TF-IDF
and LM.
3.11.8 LM = CLARITY(QUERY) – DIVERGENCE(QUERY jj DOC)
Consider the difference between two query “clarity’s,” DKL.PqjjPc/ and DKL.PqjjPd /.
DKL
 
PqjjPc

  DKL
 
PqjjPd

D
D
X
t
Pq.t/  log
Pq.t/
Pc.t/
 
X
t
Pq.t/  log
Pq.t/
Pd .t/
D
X
t
Pq.t/  log
Pd .t/
Pc.t/
(3.99)
e first divergence measures the query “clarity:” a query is clear if it is divergent from the col-
lection. e second divergence measures how divergent q is from d : a document is not relevant if
the query is divergent from the document, or, in other words, a document is relevant if the query
is not divergent from the document. e next equation inserts Pq.t/ D P.t jq/, etc.X
t
Pq.t/  log
Pd .t/
Pc.t/
D
X
t
P.t jq/  log P.t jd/
P.t jc/
Finally, by inserting the usual mixture for P.t jd/, the difference between divergences can imme-
diately be seen to be related to RSVLM.X
t
P.t jq/  log P.t jd/
P.t jc/
D
X
t2q
tfq
ql
 log d  P.t jd/ C .1 d /  P.t jc/
P.t jc/
/
X
t2q
TF.t; q/  log d  P.t jd/ C .1 d /  P.t jc/
P.t jc/
(3.100)
For P.t jq/, we applied P.t jq/ / TF.t; q/; this is justified since ql does not affect the ranking.
For P.t jd/, we used the linear mixture of foreground and background model.
3.11.9 TF-IDF = CLARITY(DOC) – DIVERGENCE(DOC jj QUERY)
In a dual way, by changing the roles of d and q, the difference between divergences can be related
to RSVTF-IDF. Consider the difference between DKL.Pd jjPc/ and DKL.Pd jjPq/.
DKL .Pd jjPc/   DKL
 
Pd jjPq

D
D
X
t
Pd .t/  log
Pd .t/
Pc.t/
 
X
t
Pd .t/  log
Pd .t/
Pq.t/
D
X
t
Pd .t/  log
Pq.t/
Pc.t/
(3.101)
e next equation inserts Pq.t/ D P.t jq/, etc.X
t
Pd .t/  log
Pq.t/
Pc.t/
D
X
t
P.t jd/  log P.t jq/
P.t jc/
3.12. SUMMARY 113
In Section 3.8 (p. 94), Query Term Probability Assumptions, we discussed the following bursti-
ness assumption for the query term probability P.t jq/:
P.t jq; c/ D
avgtf.t; c/
avgdl.c/
By inserting this assumption, and by using the Poisson bridge, PL.t jc/ D avgtf.t; c/=avgdl.c/ 
PD.t jc/, the difference between divergences can be seen to be related to RSVTF-IDF.X
t
P.t jd/  log P.t jq/
P.t jc/
D
X
t2q
tfd
dl
 log 1
PD.t jc/
/
X
t2q
TF.t; d/  log 1
PD.t jc/
(3.102)
Alternatively, one could apply an extreme mixture: P.t jq/ D 1   P.t jc/ for query terms, and
P.t jq/ D P.t jc/ for non-query terms. is is followed by the transformation from P.t jc/ to
the Document-based probability PD.t jc/; see also Section 3.7 (p. 93), Poisson Bridge. e
interesting aspect of this alternative is that we obtain the expression .avgdl.c/=avgtf.t; c/  
PD.t jc//=PD.t jc/, where avgtf and avgdl are explicit; see also Section 3.8.3 (p. 96), Query term
BIR assumption.
Overall, the difference between divergences is a concise framework to explain both, LM
and TF-IDF.
3.12 SUMMARY
ere are several frameworks that can be viewed as meta-models that house the more concrete
models such as TF-IDF, BM25, and LM. We have looked at the main frameworks: probabilistic
relevance framework (PRF), logical IR, VSMand its generalization, theGVSM.Model instances
can be derived and/or expressed in these frameworks. e general matrix framework underlines
this aspect.
Moreover, the parallel derivation of models clarifies the different event spaces of a “term
probability” used in BIR, Poisson, and LM. Figure 3.7 shows an overview over the main relation-
ships.
Firstly, there are the different event spaces: for BIR, f0; 1g, for Poisson, the frequencies,
f0; 1; 2; : : :g, and for LM, the terms themselves, i.e., ft1; t2; : : :g. We discussed how the event
spaces can be related to each other.
Second, there are the BIR-based justification of IDF, and the IDF-based formulation of
the BIR term weight.
ird, the Poisson termweight elicits the usual TF-IDF component, and in addition, shows
how the parameters burstiness and average document length theoretically should complement
the IDF component. Also, the version of the Poisson model for missing relevance is related to
114 3. RELATIONSHIPS BETWEEN IR MODELS
Figure 3.7: Relationships between Retrieval Models: Overview.
divergence-based retrieval, if we were to choose the Poisson probability as the model of random-
ness.
Fourth, there is the relationship between TF-IDF and LM: it can be shown that both
models measure the document-query-(in)dependence (DQI). e DQI is a factor that occurs in
mutual information (MI), i.e., the DQI is proportional to MI.d; q/.
3.12. SUMMARY 115
Fifth, the relationship between TF-IDF and DFR can be based on a randomness model
that computes the probability of the gap-length between two term occurrences.
Sixth, both, LM and TF-IDF can be shown to be related to the differences between di-
vergences. is extends the view that LM is divergence-based; see also Section 2.8.10 (p. 73),
KL-Divergence-Retrieval-Model.

117
C H A P T E R 4
Summary & Research Outlook
4.1 SUMMARY
is book provides the mathematical core of the main strands of IR models: TF-IDF, PRF,
BIR, BM25, LM, and DFR. Most of what is presented in this book is common knowledge to
researchers familiar with IR models. Some of the more recent or less known insights shown in
this book, are:
1. Section 2.1 (p. 9), TF-IDF:
We structured the TF and IDF variants. We emphasized that the TF variants reflect a de-
pendence assumption. For example, the logarithmic TF, Section 2.1.2 (p. 12), corresponds
to assuming that the second term occurrence has an impact of 1=2, the third occurrence
1=3, and so forth (harmonic sum). e BM25-TF can be related to the harmonic sum of
squares, i.e., the second term occurrence has an impact of 1=22, the third occurrence 1=32,
and so forth.
Section 2.1.7 (p. 17), Semi-subsumed Event Occurrences:
We discussed that n=..n C 1/=2/, the mid-point between independence and subsumption,
assigns a probabilistic semantics to the BM25-TF quantification, tfdtfd CKd .
2. Section 2.3 (p. 29), BIR:
Section 2.3.6 (p. 33), 0:5 Smoothing in the RSJ Term Weight:
We looked at Laplace-based and mixture-based arguments regarding the 0:5 in wRSJ, the
term weight employed in BM25.
3. Section 2.4 (p. 35), Poisson Model:
e Poisson term weight, wPoisson, contains the common IDF component,  log PD.t jc/,
and makes explicit components such as “burstiness,” avgtf.t; c/, and “document length nor-
malization,” dl=avgdl.
4. Section 2.6 (p. 49), LM:
ere are various forms to denote the LM-based term weight. One form, referred to as
LM2, section 2.6.6, is currently not common, but is useful for implementing LM, and also
for relating LM to other retrieval models. For example, both the BM25-TF and LM2 are
essentially ratios of probabilities, and this motivates the idea to explore ratio-based formu-
lations of retrieval models.
118 4. SUMMARY & RESEARCH OUTLOOK
5. Section 2.8 (p. 63), Divergence-based Models:
Section 2.8.1 (p. 63), DFR:
We discussed the DFR first generation, P.t 2 d jc/, and the DFR second generation,
P.tfd jc/. Also, we investigated the meaning of the DFR RSV, which is based on a bino-
mial probability of kd occurrences of document d , and how this is related to kt occurrences
of term t . e Poisson model and DFR are related for the case where the Poisson model
is formulated for missing relevance, and the Poisson probability is chosen as a model of
randomness.
6. Section 2.10 (p. 76), Precision and Recall:
Precision and Recall, and IR models share the same root: probabilities. is leads to some
interesting dualities, pointing at pathways to transfer methods from IR models to evalua-
tion, and vice versa.
7. Section 3.3 (p. 83), VSM:
Section 3.4 (p. 85), GVSM:
e vector-space “model” is a framework in which TF-IDF, BM25, and LM can be ex-
pressed. Already the VSM, but in particular the GVSM, are frameworks that can be used
to combine geometric measures with concepts of probability theory.
8. Section 3.7 (p. 93), Poisson Bridge:
is equation helps to relate Document-based and Location-based term probabilities. e
Document-based term probability PD.t jc/ occurs in TF-IDF, and the Location-based term
probability PL.t jc/ occurs in LM.
9. Section 3.9 (p. 96), Relationships: TF-IDF:
Section 3.9.5 (p. 102), TF-IDF and LM: Side-by-Side:
is derivation shows that TF-IDF and LM measure the same, namely the document-
query-independence:DQI WD P.d; q/=.P.d/  P.q//. Besides establishing the relationship
between TF-IDF and LM, the derivation also establishes probabilistic roots for TF-IDF.
Section 3.9.9 (p. 106), TF-IDF and DFR: Gaps between Term Occurrences:
In addition to the traditional explanation based on the combination of risk and gain, we
have considered an explanation that is based on combining a document-frequency-based
DFR model with a gap-based DFR model, where the latter is based on the probability of
the gap size between two term occurrences.
10. Section 3.11.8 (p. 112), Divergence and LM:
Section 3.11.9 (p. 112), Divergence and TF-IDF:
TF-IDF and LM are shown to be proportional to differences between divergences. So far,
only LM is viewed to be based on the KL-divergence (see Zhai [2009], page 55). We have
looked at this aspect in Section 2.8.10 (p. 73), KL-Divergence Retrieval Model.
4.2. RESEARCH OUTLOOK 119
4.2 RESEARCH OUTLOOK
is book contributes a consolidation of IR models, and this consolidation discloses research
issues and opportunities. Forecasting the roads ahead is speculative, but there is evidence that
retrieval models and evaluation models have to evolve, and that model combinations improve
performance. Also, dependence of events (term occurrences and user interactions), and the se-
quence of events, need to be reflected in a more sophisticated way. Moreover, the candidate result,
retrieved by the basic models, needs to be post-processed for obtaining a satisfying result. Finally,
similar to physics, a discipline that greatly advanced because of its close interplay with math, I
believe that IR has to improve its interplay with math. is leads to the following items in the
research outlook:
Section 4.2.1: Retrieval Models
Section 4.2.2: Evaluation Models
Section 4.2.3: A Unified Framework for Retrieval AND Evaluation
Section 4.2.4: Model Combinations and New Models (query performance prediction)
Section 4.2.5: Dependence-aware Models, Sequence-based Models, Order-aware Models
Section 4.2.6: Query-Log Models and other More-Evidence Models
Section 4.2.7: Phase-2 Models: Retrieval Result Condensation Models (diversity, novelty)
Section 4.2.8: eoretical Framework to Predict Ranking Quality
Section 4.2.9: Math for IR
Section 4.2.10: Abstraction for IR
4.2.1 RETRIEVAL MODELS
Figure 4.1 shows a lattice of relevance and likelihood.
Relevance is “present” or “missing,” and the likelihood is “document” likelihood or “query”
likelihood. In this lattice, we can position the models TF-IDF, LM, and BM25.
is lattice emphasizes that TF-IDF and LM measure the same, namely the document-
query (in)dependence (DQI).
TF-IDF  P.d jq/
P.d/
D
P.d; q/
P.d/  P.q/
D
P.qjd/
P.q/
 LM
e discussion that aims to classify the models to be either discriminative (TF-IDF, which doc-
ument is implied by the query, q ! d ) or generative (LM, which document implies/generates
the query, d ! q), is then led by the decomposition of either document or query event. TF-IDF
120 4. SUMMARY & RESEARCH OUTLOOK
Figure 4.1: Model Lattice.
decomposes the document event, whereas LM decomposes the query event. is complements
the view of logical IR, Nie [1992], van Rijsbergen [1986], Wong and Yao [1995], where retrieval
scores are viewed to be a semantics of P.d ! q/.
Moreover, the lattice underlines that PRF/BM25, traditionally based on the document
likelihood, P.d jq; r/, can be viewed as the general form of TF-IDF, or, in other words, TF-IDF
is a special form of BM25. When using IDF in place of the RSJ term weight, then BM25 can be
formulated as the difference betweenRSVTF-IDF.d; q; Nr/ andRSVTF-IDF.d; q; r/. In a similar way,
an LM-based “BM25” is the difference between the respective LM-based RSV’s. LM and PRF
are on different levels, and regarding the discussion that LM is proportional to the probabilistic
odds, the lattice emphasizes that, similar to the way TF-IDF and traditional BM25 are related,
there is an LM-based “PRF/BM25,” i.e., a consequent decomposition of the odds of relevance
based on P.qjd; r/, where the query event is decomposed. In traditional BM25, the RSJ term
weight is based on a binary event space. For the LM-based BM25, the term weight is based on
the space of terms.
4.2.2 EVALUATION MODELS
is book is about retrieval models, and Section 2.10 (p. 76), Precision & Recall, established
that probability theory is the common ancestor of both, retrieval models and evaluation models.
Precision and recall are conditional probabilities, and the mean average precision can be expressed
via the total probability theorem.
Taking this probabilistic view on evaluation, the idea is to devise a unified model that is an
abstraction of retrieval AND evaluation.
4.2. RESEARCH OUTLOOK 121
4.2.3 A UNIFIED FRAMEWORK FOR RETRIEVAL AND EVALUATION
e idea to devise unified models is a continuous research topic, and for IR, its foundation may
be Robertson et al. [1982], “e Unified Probabilistic Model for IR.” Also, Stephen Robert-
son presented in his keynote at the SIGIR 2004 eory Workshop “A new unified probabilistic
model”.
e unified model we discuss here in the research outlook rests on the idea of exploring a
duality between retrieval and evaluation. e duality is based on the following two probabilities:
P.relevantjd; q/ and P.usefuljretrieved; relevant/. Figure 4.2 shows the way the duality between
retrieval and evaluation unfolds.
Figure 4.2: Unified Model Lattice: Retrieval and Evaluation.
e unified model has five parameters, as the following pseudo-code illustrates:
define UnifiedModel(Task, Hypothesis, Model1, Event1, Model2, Event2) {
# $Task & $Model1 & $Model2
122 4. SUMMARY & RESEARCH OUTLOOK
# & P($Event1 | $Event2, $Hypothesis) & P($Event2 | $Event1, $Hypothesis)
}
main () {
UnifiedModel("Retrieval", rel, "TF-IDF", d, "LM", q);
UnifiedModel("Evaluation", useful, "Precision", rel, "Recall", retr);
}
Note that this arrangement is not supposed to indicate that TF-IDF is precision-oriented
and LM is recall-oriented. e message of the arrangement is that in this template we realize that
what is familiar for IR models can be transferred to evaluation, and the other way round, what is
familiar for evaluation, can be transferred to IR models.
We know for retrieval models that the coverage of the dependencies of the multiple oc-
currences of a term, expressed as the TF quantification, is important. We know that a concept
of “discriminativeness” is essential. How do event frequencies and discriminativeness transfer to
evaluation models?
Whereas on the retrieval side, the probabilistic odds are the most recognized approach to
IR, on the evaluation side, the basic implications dominate. is naturally leads to the question
of how to evolve evaluation measures, taking advantage of what is known for retrieval models.
e F1-measure combines the two sides of evaluation, where F1 is the harmonic mean of
precision and recall. How does this combination transfer to IR models?
In a direct comparison, retrieval and evaluation read as follows:
e document is relevant for the query if
the probability P.relevantjd; q/ is greater than P.:relevantjd; q/.
e retrieved documents are useful for the relevant documents if
the probability P.usefulj retr; rel/ is greater than P.:usefulj retr; rel/.
In summary, the relationships between retrieval and evaluation can be exploited to transfer
concept from evaluation to retrieval, and vice versa.
4.2.4 MODEL COMBINATIONS AND “NEW” MODELS
Investigations into model combinations show pathways to improving retrieval quality. One
promising approach is query performance prediction (QPP, select model and parameters based
on query features), Hauff et al. [2010], Shtok et al. [2010]. is will lead to “new” models, but
it is currently an open question how predictors will find their way into widely accepted standards
of retrieval models. What will become the standard TF-IDF-QPP and LM-QPP?
Other candidates for pathways to new models are divergence-based models, or, in the wider
context, moving from “probabilistic models” toward “information-theoretic” models. DFR is an
information-theoretic model, however, there is the view that the other models embody compo-
nents of DFR, and can be explained as being divergence-based.
4.2. RESEARCH OUTLOOK 123
Whether QPP or DFR or other, one of the overall questions is where to search for new
models. Chengxiang Zhai described the question at ICTIR 2011 like this:
We can either search near to the models we know, or we search in far away land, search
in completely new areas.
“Near to the models we know” means to search in the quadrangle TF-IDF, LM, TF-IDF-BM25,
and LM-BM25. Considering relevance, user profiles (personalization) and query features (per-
formance prediction) is still near to what we know.
“Search far away” means to go beyond what we usually do. Maybe this means to go back
to the basics, back to the good old vector-space model, where the Euclidean norm and other
normalization in combination with the state-of-the-art knowledge about event dependencies,
likelihoods, and event spaces may lead to improvements. Already van Rijsbergen [2004] points
at this combination of probabilistic and geometric methods that may lead to “new” models.
ough around since the early 00s, divergence-based models still feel at the beginning in
the sense that they are still perceived as being complex. is book has highlighted some of the
issues regarding DFR, discussed the relationships between DFR and the binomial probability,
and between divergence and the more intuitive models such as TF-IDF, BM25, and LM. It is
interesting that currently we work in a vicinity where IR models are closer to “probability theory”
than to “information theory.” I believe that the road to new models means first to establish new
math, for example, new mathematical concepts to capture aspects such as the dependence of event
occurrences, or new math that merges probability theory and information theory.
4.2.5 DEPENDENCE-AWARE MODELS
A simple test such as a web search with query “information retrieval” versus “retrieval information”
shows: the order (sequence) of words does matter. e algorithm (let the query term probability
depend on the position, or apply a phrase-based matching to complement the single-term match)
is the secret of the engineers. From an IR model perspective, the standard formulations of “bag-
of-word” IR models do not consider the order of terms in the sequences that represent queries
and documents.
Even when agreeing that the TF component models a form of dependence assumption,
the order of terms is not reflected. Dependence comes in different contexts and forms. Hou et al.
[2011], “High-Order Word Dependence Mining,” showed that the modeling of triplets, that is
dependencies of order 3, is conducive for IR. Zhai et al. [2003], “Beyond Independent Rele-
vance,” pointed at the dependence in subtopic retrieval, and the dependence between the hits in a
retrieval result. Wu and Roelleke [2009] proposed semi-subsumption as a notion to describe the
dependence inherently modeled by the BM25-TF.
Overall, the standard “bag-of-word” formulations need to evolve to standards that capture
dependence, sequences (order) and proximity (back to the roots of IR). e problem here is not
so much to find “a model,” the problem seems to be to find a formulation, that is to find concepts
that are easy to grasp, and lead to a widely known standard.
124 4. SUMMARY & RESEARCH OUTLOOK
4.2.6 “QUERY-LOG” AND OTHER MORE-EVIDENCE MODELS
e field “learning to rank,” Li [2011], shows that given more evidence than the similarity
of terms, retrieval quality can be improved. Mixing topical scores with other scores such as
popularity-based scores (web pages, page-rank, Brin and Page [1998b]) is well known to enhance
retrieval performance. In addition to exploiting evidence about the collection of documents, it
is conducive to exploit evidence about collections of queries, as, for example, demonstrated by
Deng et al. [2009], where a notion “IQF: inverse query frequency” has been proposed, and this
confirms the benefits of taking advantage of dualities and relationships between models.
is book focused on the consolidation of topical models. Future research into foundations
of IR models could lead to standard formulations that mix the main types and sources of evidence.
Regarding types of evidence, state-of-the-art search systems mix topical evidence and structural
evidence (bibliographic coupling, co-citations, in-links and out-links, partitioning of documents).
Regarding source of evidence, the collection of documents, the collection of queries, the user
profile, and domain-specific background knowledge (e.g., a list of synonyms or a more refined
taxonomy) are common sources when devising a ranking function. e research challenge is to
shape standards of formal models that capture the different types and sources of evidence.
4.2.7 PHASE-2 MODELS: RETRIEVAL RESULT CONDENSATION MODELS
Today’s retrieval systems are based on several retrieval phases. e main two phases are:
1. Retrieve the result candidates where the ranking is based on content, structure, and relation-
ships. Also, parameters such as user profile (location, age, gender, history) can be included
for a customer-specific ranking.
2. Condense the result. is phase takes a holistic view at the retrieval result, exploiting the
relationships between the retrieved items.
Phase-2, the condensation of the result, aims at making the result satisfactory, diverse,
personal, profitable, attractive, etc. is phase means to promote some of the result candidates
and demote others. Whereas TF-IDF, BM25, and LM are widely known “standards” for the first
phase, the models used for the second phase are specific.
Topics such as diversification and result clustering (aggregation) are ubiquitous and it can
be expected that similar to the way TF-IDF and friends are established standards, we will see
standards for result post-processing becoming as widely known as the basic IR models. Kurland
et al. [2011], “A Unified Framework for Post-Retrieval Query-Performance Prediction,” is one
of the attempts pointing the way toward the unification and consolidation of this field.
4.2.8 A THEORETICAL FRAMEWORK TO PREDICT RANKING QUALITY
Before the laws of force and gravity were formulated, researchers lifted and bounced many objects
of different kinds and measured speed and effect. Again and again.
4.2. RESEARCH OUTLOOK 125
Whereas in the physical world, there are models to compute expected speed and impact
of an accelerating object, it is not clear whether we can find a formal framework that tells for
a retrieval model the expected quality. I appreciate the comment of IR experts and colleagues
saying: “is is nonsense.” Definitely, it is challenging. e axiomatic framework, Fang and Zhai
[2005], is a milestone in devising a theoretical framework in which to express what must hold
for a retrieval function for it to be reasonable. Relatively remote to IR but relevant in this context
is work on formalizing properties of knowledge and belief; Fagin and Halpern [1994]. is type
of theoretical work tells us how to formalize properties and concepts that are difficult to capture
formally.
4.2.9 MIR: MATH FOR IR
Physics always has inspired and rewritten math, and many mathematical concepts only came into
existence because physicians required an abstraction to describe problems and phenomenas.
One of themain endeavours of this book is to base the consolidation of IRmodels on a non-
ambiguous and concise mathematical world. e journey was more complicated than expected,
and crystalising and stabilising the mathematical ingredients have required in-depth studies of
mathematical groundwork. One of the conclusions is: we need more math for IR.
In probability theory, we usually meet the traditional assumptions: independent, disjoint,
and subsumed events. To move some of the achievements of IR into other application domains,
research is required that generates and establishes new, IR-drivenmath, to transfer what is learned
from IR models into the mathematical world of probability theory.
TF-IDF, a compact, mathematical expression, has been a popular retrieval model for more
than 40 years, and BM25 and LM are catching up. Is there a new intuitive formula, a new math-
ematical concept, that will lead to a new retrieval model?
4.2.10 AIR: ABSTRACTION FOR IR
Math for IR is only one form of abstraction. Other, more concrete forms of abstraction are pro-
gramming languages, software environments, data models, and knowledge representation. Cor-
nacchia et al. [2008], Fuhr [1995], Meghini et al. [1993], Roelleke et al. [2008] and related
publications point at possibilities to employ mathematical frameworks and probabilistic variants
of logic’s (descriptive languages) for modeling IR models and tasks. Whereas Lucene-based im-
plementations of search systems become popular, and the traditional term-document (key-value)
index is still the dominating “data model” for IR, and the gap between database solutions and IR
systems is still wide, there is a demand for technology that on one hand is lighter weight than
traditional “elephant” DB technology, and on the other hand reliably delivers levels of abstraction
that make building and maintaining application-specific IR systems more efficient.
is book provides a mathematical basis and a detailed consolidation of models, and this
is the footwork required for establishing higher abstraction for IR.

127
Bibliography
Aizawa, A. (2003). An information-theoretic perspective of tf-idf measures. Information Pro-
cessing and Management, 39:45–65. DOI: 10.1016/S0306-4573(02)00021-3. 109
Aly, R. and Demeester, T. (2011). Towards a better understanding of the relationship between
probabilistic models in IR. In ICTIR, Bertinoro, Italy, volume 6931, pages 164–175. DOI:
10.1007/978-3-642-23318-0_16. 3
Amati, G. (2009). BM25. In Liu, L. and Özsu, M. T., editors, Encyclopedia of Database Systems,
pages 257–260. Springer US. 34
Amati, G. and van Rijsbergen, C. J. (2002). Probabilistic models of information retrieval based on
measuring the divergence from randomness. ACMTransaction on Information Systems (TOIS),
20(4):357–389. DOI: 10.1145/582415.582416. 2, 64, 65, 104, 105
Azzopardi, L. and Roelleke, T. (2007). Explicitly considering relevance within the language
modelling framework. In Proceedings of the 1st International Conference oneory of Information
Retrieval (ICTIR 07) - Studies in eory of Information Retrieval. 25, 58
Baeza-Yates, R. A. and Ribeiro-Neto, B. A. (2011). Modern Information Retrieval - the concepts
and technology behind search, Second edition. Pearson Education Ltd., Harlow, England. 3, 65
Belew, R. K. (2000). Finding out about. Cambridge University Press. 3
Bookstein, A. (1980). Fuzzy requests: An approach to weighted Boolean searches. Journal of the
American Society for Information Science, 31:240–247. DOI: 10.1002/asi.4630310403. 1
Brin, S. and Page, L. (1998a). e anatomy of a large-scale hypertextual web search engine.
Computer Networks, 30(1-7):107–117. DOI: 10.1016/j.comnet.2012.10.007.
Brin, S. and Page, L. (1998b). e anatomy of a large-scale hypertextual web search engine. In
7th International WWW Conference, Brisbane, Australia. DOI: 10.1016/j.comnet.2012.10.007.
124
Bronstein, I. N. (1987). Taschenbuch der Mathematik. Harri Deutsch, un, Frankfurt am Main.
21
Bruza, P. and Song, D. (2003). A comparison of various approaches for using probabilistic de-
pendencies in language modeling. In DBLP:conf/sigir/2003 [2003], pages 419–420. 2
128 BIBLIOGRAPHY
Church, K. and Gale, W. (1995). Inverse document frequency (idf ): A measure of deviation from
Poisson. In Proceedings of the ird Workshop on Very Large Corpora, pages 121–130. 16, 35
Cooper, W. (1991). Some inconsistencies and misnomers in probabilistic IR. In Bookstein, A.,
Chiaramella, Y., Salton, G., and Raghavan, V., editors, Proceedings of the Fourteenth Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval,
pages 57–61, New York.
Cooper, W. S. (1988). Getting beyond Boole. Information Processing and Management,
24(3):243–248. DOI: 10.1016/0306-4573(88)90091-X.
Cooper, W. S. (1994). Triennial ACM SIGIR award presentation and paper: e formalism
of probability theory in IR: A foundation for an encumbrance. In Croft and van Rijsbergen
[1994], pages 242–248.
Cornacchia, R. and de Vries, A. P. (2007). A parameterised search system. In ECIR, pages 4–15.
DOI: 10.1007/978-3-540-71496-5_4. 83, 91
Cornacchia, R., Héman, S., Zukowski, M., de Vries, A. P., and Boncz, P. A. (2008). Flexible and
efficient IR using array databases. VLDB J., 17(1). DOI: 10.1007/s00778-007-0071-0. 125
Crestani, F. and Van Rijsbergen, C. J. (1995). Information retrieval by logical imaging. Journal
of Documentation, 51(1):3–17. DOI: 10.1108/eb026939. 83
Crestani, F. and van Rijsbergen, C. J. (1995). Probability kinematics in information retrieval. In
Fox et al. [1995], pages 291–299. 83
Croft, B. and Lafferty, J., editors (2003). Language Modeling for Information Retrieval. Kluwer.
DOI: 10.1007/978-94-017-0171-6. 49, 130
Croft, W. and Harper, D. (1979). Using probabilistic models of document retrieval without
relevance information. Journal of Documentation, 35:285–295. DOI: 10.1108/eb026683. 1,
32, 96
Croft, W. and Turtle, H. (1992). Retrieval of complex objects. In Pirotte, A., Delobel, C., and
Gottlob, G., editors, Advances in Database Technology — EDBT’92, pages 217–229, Berlin et
al. Springer. 61, 63, 108
Croft, W. B. and van Rijsbergen, C. J., editors (1994). Proceedings of the Seventeenth Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval,
London, et al. Springer-Verlag. 128, 132
DBLP:conf/sigir/2003 (2003). SIGIR 2003: Proceedings of the 26th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval, July 28 - August 1,
2003, Toronto, Canada. ACM. 127
BIBLIOGRAPHY 129
de Vries, A. and Roelleke, T. (2005). Relevance information: A loss of entropy but a gain for
IDF? In ACM SIGIR, pages 282–289, Salvador, Brazil. DOI: 10.1145/1076034.1076084. 34,
97
Deerwester, S., Dumais, S., Furnas, G., Landauer, T., and Harshman, R. (1990). Indexing by
latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.
DOI: 10.1002/(SICI)1097-4571(199009)41:6%3C391::AID-ASI1%3E3.0.CO;2-9. 86
Deng, H., King, I., and Lyu, M. R. (2009). Entropy-biased models for query representation on
the click graph. In Proceedings of the 32nd international ACM SIGIR conference on Research and
development in information retrieval, SIGIR ’09, pages 339–346, New York, NY, USA. ACM.
DOI: 10.1145/1571941.1572001. 124
Dumais, S. T., Furnas, G. W.and Landauer, T. K., and Deerwester, S. (1988). Using latent
semantic analysis to improve information retrieval. pages 281–285. 86
Dupret, G. (2003). Latent concepts and the number orthogonal factors in latent semantic anal-
ysis. In SIGIR, pages 221–226. DOI: 10.1145/860435.860477. 86
Fagin, R. and Halpern, J. (1994). Reasoning about knowledge and probability. Journal of the
ACM, 41(2):340–367. DOI: 10.1145/174652.174658. 125
Fang, H. and Zhai, C. (2005). An exploration of axiomatic approaches to information retrieval.
In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research
and development in information retrieval, pages 480–487, New York, NY, USA. ACM. DOI:
10.1145/1076034.1076116. 2, 125
Fox, E., Ingwersen, P., and Fidel, R., editors (1995). Proceedings of the 18th Annual Interna-
tional ACMSIGIRConference onResearch andDevelopment in InformationRetrieval, NewYork.
ACM. 128, 129
Fuhr, N. (1989). Models for retrieval with probabilistic indexing. Information Processing and
Management, 25(1):55–72. DOI: 10.1016/0306-4573(89)90091-5. 27, 28
Fuhr, N. (1992a). Integration of probabilistic fact and text retrieval. In Belkin, N., Ingwersen,
P., and Pejtersen, M., editors, Proceedings of the Fifteenth Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval, pages 211–222, New York.
25
Fuhr, N. (1992b). Probabilistic models in information retrieval. eComputer Journal, 35(3):243–
255. DOI: 10.1093/comjnl/35.3.243.
Fuhr, N. (1995). Probabilistic datalog - a logic for powerful retrieval methods. In Fox et al.
[1995], pages 282–290. 125
130 BIBLIOGRAPHY
Gordon, M. and Lenk, P. (1992). When is the probability ranking principle suboptimal? Jour-
nal of the American Society for Information Science, 43(1):1–14. DOI: 10.1002/(SICI)1097-
4571(199201)43:1%3C1::AID-ASI1%3E3.0.CO;2-5. 27
Grossman, D. A. and Frieder, O. (2004). Information Retrieval. Algorithms and Heuristics, 2nd
ed., volume 15 of e Information Retrieval Series. Springer. 3
Hauff, C., Azzopardi, L., Hiemstra, D., and de Jong, F. (2010). Query performance prediction:
Evaluation contrasted with effectiveness. In Gurrin, C., He, Y., Kazai, G., Kruschwitz, U.,
Little, S., Roelleke, T., Rüger, S. M., and van Rijsbergen, K., editors, ECIR, volume 5993 of
Lecture Notes in Computer Science, pages 204–216. Springer. 122
He, B. and Ounis, I. (2005). Term frequency normalisation tuning for BM25 and DFR models.
In ECIR, pages 200–214. DOI: 10.1007/978-3-540-31865-1_15.
Hiemstra, D. (2000). A probabilistic justification for using tf.idf term weighting in
information retrieval. International Journal on Digital Libraries, 3(2):131–139. DOI:
10.1007/s007999900025. 2, 49, 54, 101
Hofmann, T. (1999). Probabilistic latent semantic indexing. In SIGIR, pages 50–57. ACM.
DOI: 10.1145/312624.312649. 86
Hou, Y., He, L., Zhao, X., and Song, D. (2011). Pure high-order word dependence mining
via information geometry. In Advances in Information Retrieval eory: ird International
Conference, ICTIR, 2011, Bertinoro, Italy, September 12-14, 2011, Proceedings, volume 6931,
pages 64–76. Springer-Verlag New York Inc. DOI: 10.1007/978-3-642-23318-0_8. 2, 3, 123
Kamps, J., de Rijke, M., and Sigurbj&#246;rnsson, B. (2004). Length normalization in XML
retrieval. In Proceedings of the 27th annual international ACM SIGIR conference on research and
development in information retrieval, pages 80–87, New York, NY, USA. ACM Press. DOI:
10.1145/1008992.1009009. 101
Kleinberg, J. (1999). Authoritative sources in a hyperlinked environment. Journal of ACM, 46.
DOI: 10.1145/324133.324140.
Kurland, O., Shtok, A., Carmel, D., and Hummel, S. (2011). A unified framework for post-
retrieval query-performance prediction. In ICTIR, pages 15–26. DOI: 10.1007/978-3-642-
23318-0_4. 124
Lafferty, J. and Zhai, C. (2003). Probabilistic Relevance Models Based on Document and Query
Generation, chapter 1. In Croft and Lafferty [2003]. 1, 3, 25, 58, 80, 108
Lavrenko, V. and Croft, W. B. (2001). Relevance-based language models. In SIGIR, pages
120–127. ACM. 2, 25, 58, 75, 108
BIBLIOGRAPHY 131
Li, H. (2011). Learning to Rank for Information Retrieval and Natural Language Processing. Syn-
thesis Lectures on Human Language Technologies. Morgan & Claypool Publishers. DOI:
10.2200/S00348ED1V01Y201104HLT012. 47, 124
Luk, R.W. P. (2008). On event space and rank equivalence between probabilistic retrieval models.
Inf. Retr., 11(6):539–561. DOI: 10.1007/s10791-008-9062-z. 3, 25, 108
Manning, C. D., Raghavan, P., and Schuetze, H., editors (2008). Introduction to Information
Retrieval. Cambridge University Press. DOI: 10.1017/CBO9780511809071. 3
Margulis, E. (1992). N-poisson documentmodelling. In Belkin, N., Ingwersen, P., and Pejtersen,
M., editors,Proceedings of the Fifteenth Annual International ACMSIGIRConference onResearch
and Development in Information Retrieval, pages 177–189, New York. 35
Maron, M. and Kuhns, J. (1960). On relevance, probabilistic indexing, and information retrieval.
Journal of the ACM, 7:216–244. DOI: 10.1145/321033.321035. 1
Meghini, C., Sebastiani, F., Straccia, U., and anos, C. (1993). A model of information re-
trieval based on a terminological logic. In Korfhage, R., Rasmussen, E., and Willett, P., edi-
tors, Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 298–308, New York. ACM. 83, 125
Melucci, M. (2008). A basis for information retrieval in context. ACM Transactions on Informa-
tion Systems (TOIS), 26(3). DOI: 10.1145/1361684.1361687. 87
Metzler, D. (2008). Generalized inverse document frequency. In Shanahan, J. G., Amer-Yahia,
S.,Manolescu, I., Zhang, Y., Evans, D.A., Kolcz, A., Choi, K.-S., andChowdhury, A., editors,
CIKM, pages 399–408. ACM. 16
Metzler, D. and Croft, W. B. (2004). Combining the language model and inference net-
work approaches to retrieval. Information Processing & Management, 40(5):735–750. DOI:
10.1016/j.ipm.2004.05.001. 108
Nie, J. (1992). Towards a probabilistic modal logic for semantic-based information retrieval.
In Belkin, N., Ingwersen, P., and Pejtersen, M., editors, Proceedings of the Fifteenth Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval,
pages 140–151, New York. 83, 120
Piwowarski, B., Frommholz, I., Lalmas, M., and Van Rijsbergen, K. (2010). What can Quantum
eory Bring to Information Retrieval? In Proc. 19th International Conference on Information
and Knowledge Management, pages 59–68. DOI: 10.1145/1871437.1871450. 87
Ponte, J. and Croft, W. (1998). A language modeling approach to information retrieval. In Croft,
W. B., Moffat, A., van Rijsbergen, C. J., Wilkinson, R., and Zobel, J., editors, Proceedings of the
132 BIBLIOGRAPHY
21st Annual International ACM SIGIR Conference on Research and Development in Information
Retrieval, pages 275–281, New York. ACM. 1, 49
Robertson, S. (1977). e probability ranking principle in IR. Journal of Documentation, 33:294–
304. DOI: 10.1108/eb026647. 26
Robertson, S. (2004). Understanding inverse document frequency: On theoretical arguments for
idf. Journal of Documentation, 60:503–520. DOI: 10.1108/00220410410560582. 3, 96, 109
Robertson, S. (2005). On event spaces and probabilistic models in information retrieval. Infor-
mation Retrieval Journal, 8(2):319–329. DOI: 10.1007/s10791-005-5665-9. 3
Robertson, S., Maron, M., and Cooper, W. (1982). e unified probabilistic model for ir. In
Salton, G. and Schneider, H.-J., editors, Research and Development in Information Retrieval,
pages 108–117, Berlin et al. Springer. DOI: 10.1007/BFb0036332. 121
Robertson, S., S.Walker, S. J., Hancock-Beaulieu,M., andGatford,M. (1994). Okapi at TREC-
3. In Text REtrieval Conference. 45
Robertson, S. and Sparck-Jones, K. (1976). Relevance weighting of search terms. Journal of the
American Society for Information Science, 27:129–146. DOI: 10.1002/asi.4630270302. 1, 30
Robertson, S. E. and Walker, S. (1994). Some simple effective approximations to the 2-Poisson
model for probabilistic weighted retrieval. In Croft and van Rijsbergen [1994], pages 232–241.
1, 35, 44, 45
Robertson, S. E., Walker, S., and Hancock-Beaulieu, M. (1995). Large test collection experi-
ments on an operational interactive system: Okapi at TREC. Information Processing andMan-
agement, 31:345–360. DOI: 10.1016/0306-4573(94)00051-4. 1, 45
Robertson, S. E., Walker, S., and Hancock-Beaulieu, M. (1998). Okapi at trec-7: Automatic ad
hoc, filtering, vlc and interactive. In TREC, pages 199–210. 45, 47
Robertson, S. E. and Zaragoza, H. (2009). e probabilistic relevance framework: Bm25
and beyond. Foundations and Trends in Information Retrieval, 3(4):333–389. DOI:
10.1561/1500000019. 3, 45
Rocchio, J. (1971). Relevance feedback in information retrieval. In Salton [1971]. 1, 74
Roelleke, T. (2003). A frequency-based and a Poisson-based probability of being informative. In
ACM SIGIR, pages 227–234, Toronto, Canada. DOI: 10.1145/860435.860478. 20
Roelleke, T. and Fuhr, N. (1996). Retrieval of complex objects using a four-valued logic. In
Frei, H.-P., Harmann, D., Schäuble, P., and Wilkinson, R., editors, Proceedings of the 19th
International ACM SIGIR Conference on Research and Development in Information Retrieval,
pages 206–214, New York. ACM. 83
BIBLIOGRAPHY 133
Roelleke, T., Tsikrika, T., and Kazai, G. (2006). A general matrix framework for modelling
information retrieval. Journal on Information Processing & Management (IP&M), Special Issue
on eory in Information Retrieval, 42(1). DOI: 10.1016/j.ipm.2004.11.006. 6, 83, 88
Roelleke, T. and Wang, J. (2006). A parallel derivation of probabilistic information retrieval
models. In ACM SIGIR, pages 107–114, Seattle, USA. DOI: 10.1145/1148170.1148192. 2,
14, 93
Roelleke, T. and Wang, J. (2008). TF-IDF uncovered: A study of theories and probabilities. In
ACM SIGIR, pages 435–442, Singapore. DOI: 10.1145/1390334.1390409. 3
Roelleke, T., Wu, H., Wang, J., and Azzam, H. (2008). Modelling retrieval models in a proba-
bilistic relational algebra with a new operator: e relational Bayes. VLDB Journal, 17(1):5–37.
DOI: 10.1007/s00778-007-0073-y. 125
Salton, G., editor (1971). e SMART Retrieval System - Experiments in Automatic Document
Processing. Prentice Hall, Englewood, Cliffs, New Jersey. 1, 132
Salton,G., Fox, E., andWu,H. (1983). ExtendedBoolean information retrieval. Communications
of the ACM, 26:1022–1036. DOI: 10.1145/182.358466. 1
Salton, G., Wong, A., and Yang, C. (1975). A vector space model for automatic indexing. Com-
munications of the ACM, 18:613–620. DOI: 10.1145/361219.361220. 1
Shtok, A., Kurland, O., and Carmel, D. (2010). Using statistical decision theory and relevance
models for query-performance prediction. In Crestani, F., Marchand-Maillet, S., Chen, H.-
H., Efthimiadis, E. N., and Savoy, J., editors, SIGIR, pages 259–266. ACM. 122
Singhal, A., Buckley, C., and Mitra, M. (1996). Pivoted document length normalisation. In Frei,
H., Harmann, D., Schäuble, P., and Wilkinson, R., editors, Proceedings of the 19th Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval,
pages 21–39, New York. ACM. 1
Sparck-Jones, K., Robertson, S., Hiemstra, D., and Zaragoza, H. (2003). Language modelling
and relevance. LanguageModelling for Information Retrieval, pages 57–70. DOI: 10.1007/978-
94-017-0171-6_3. 2, 58
Sparck-Jones, K., Walker, S., and Robertson, S. E. (2000). A probabilistic model of informa-
tion retrieval: development and comparative experiments: Part 1. Information Processing and
Management, 26:779–808. DOI: 10.1016/S0306-4573(00)00015-7.
Turtle, H. and Croft, W. (1991). Efficient probabilistic inference for text retrieval. In Proceedings
RIAO 91, pages 644–661, Paris, France.
134 BIBLIOGRAPHY
Turtle, H. and Croft, W. (1992). A comparison of text retrieval models. e Computer Journal,
35. DOI: 10.1093/comjnl/35.3.279. 61, 63, 108
Turtle, H. and Croft, W. B. (1990). Inference networks for document retrieval. In Vidick,
J.-L., editor, Proceedings of the 13th International Conference on Research and Development in
Information Retrieval, pages 1–24, New York. ACM. 58
van Rijsbergen, C. J. (1979). Information Retrieval. Butterworths, London, 2. edition.
http://www.dcs.glasgow.ac.uk/Keith/Preface.html. 3, 30
van Rijsbergen, C. J. (1986). A non-classical logic for information retrieval. eComputer Journal,
29(6):481–485. DOI: 10.1093/comjnl/29.6.481. 1, 82, 120
van Rijsbergen, C. J. (1989). Towards an information logic. In Belkin, N. and van Rijsbergen,
C. J., editors,Proceedings of theTwelfth Annual International ACMSIGIRConference onResearch
and Development in Information Retrieval, pages 77–86, New York.
van Rijsbergen, C. J. (2004). eGeometry of Information Retrieval. Cambridge University Press.
DOI: 10.1017/CBO9780511543333. 3, 87, 123
Wong, S. and Yao, Y. (1995). On modeling information retrieval with probabilistic inference.
ACMTransactions on Information Systems, 13(1):38–68. DOI: 10.1145/195705.195713. 1, 83,
85, 120
Wu, H. and Roelleke, T. (2009). Semi-subsumed events: A probabilistic semantics for the BM25
term frequency quantification. In ICTIR (International Conference on eory in Information
Retrieval). Springer. DOI: 10.1007/978-3-642-04417-5_43. 17, 123
Xue, X., Jeon, J., and Croft, W. B. (2008). Retrieval models for question and answer archives. In
Myaeng, S.-H., Oard, D. W., Sebastiani, F., Chua, T.-S., and Leong, M.-K., editors, SIGIR,
pages 475–482. ACM. 50
Zaragoza, H., Hiemstra, D., Tipping, M. E., and Robertson, S. E. (2003). Bayesian extension
to the language model for ad hoc information retrieval. In ACM SIGIR, pages 4–9, Toronto,
Canada. DOI: 10.1145/860435.860439.
Zhai, C. (2009). Statistical Language Models for Information Retrieval. Morgan & Claypool
Publishers. DOI: 10.2200/S00158ED1V01Y200811HLT001. xix, 3, 7, 73, 109, 118
Zhai, C. X., Cohen, W. W., and Lafferty, J. (2003). Beyond independent relevance: methods and
evaluation metrics for subtopic retrieval. In Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in informaion retrieval, SIGIR ’03, pages 10–17,
New York, NY, USA. ACM. DOI: 10.1145/860435.860440. 123
135
Author’s Biography
THOMAS ROELLEKE
omas Roelleke holds a Dr rer nat (Ph.D.) and a Diplom der Ingenieur-Informatik (MSc in
Engineering & Computer Science) of the University of Dortmund.
After school education in Meschede, Germany, he attended the b.i.b., the Nixdorf Com-
puter school for professions in informatics, in Paderborn. Nixdorf Computer awarded him a sales
and management trainee program, after which he was appointed as product consultant in the
Unix/DB/4GL marketing of Nixdorf Computer. He studied Diplom-Ingenieur-Informatik at
the University of Dortmund (UniDo), and was later a lecturer/researcher at UniDo. His research
focused on probabilistic reasoning and knowledge representations, hypermedia retrieval, and the
integration of retrieval and database technologies. His lecturing included information/database
systems, object-oriented design and programming, and software engineering. He obtained his
Ph.D. in 1999 for the thesis titled ”POOL: A probabilistic object-oriented logic for the repre-
sentation and retrieval of complex objects—a model for hypermedia retrieval”. Since 1999, he
has been working as a strategic IT consultant, founder and director of small businesses, research
fellow, and lecturer at the Queen Mary University of London (QMUL).
Research contributions include a probabilistic relational algebra (PRA), a probabilistic
object-oriented logic (POOL), the relational Bayes, a matrix-based framework for IR, a par-
allel derivation of IR models, a probabilistic interpretation of the BM25-TF based on “semi-
subsumed” event occurrences, and theoretical studies of retrieval models.
omas Roelleke lives in England, in a village in the middle between buzzy London and
beautiful East Anglia.

137
Index
P.d ! q/: logical IR, 82
RSVBIR: decomposed, 31
RSVBIR: definition, 31
RSVBM25: definition, 47
RSVDFR: decomposed: Binomial, 72
RSVDFR: decomposed: Poisson, 72
RSVDFR: definition, 72
RSVDirich-LM: decomposed, 55
RSVDirich-LM: definition, 55
RSVJM-LM: decomposed, 54
RSVJM-LM: definition, 54
RSVLM1: decomposed, 52
RSVLM1: definition, 52
RSVLM2: decomposed, 56
RSVLM2: definition, 56
RSVLM: decomposed, 53
RSVLM: definition, 53
RSVPIN: decomposed, 62
RSVPIN: definition, 62
RSVPoisson: decomposed, 43
RSVPoisson: definition, 42
RSVTF-IDF: decomposed, 16
RSVTF-IDF: definition, 16
TFBM25: definition, 45
TFBM25: foundations TF-IDF, 10
TFfrac: foundations TF-IDF, 10
TFfrac: ratio, 13
TFlifted: foundations TF-IDF, 16
TFlog, 12
TFlog: foundations TF-IDF, 10
TFmax: foundations TF-IDF, 10
TFpiv: definition, 45
TFpiv: foundations TF-IDF, 16
TFsum: foundations TF-IDF, 10
TFtotal: foundations TF-IDF, 10
df.t; c/, 14
dfBIR: foundations TF-IDF, 14
dfsum: foundations TF-IDF, 14
dftotal: foundations TF-IDF, 14
idf.t; c/, 14
tfd , 10
wBIR: definition, 30
wBM25: definition, 47
wDFR-1: definition, 70
wDFR-2: definition, 71
wDFR: definition, 70
wDirich-LM2: definition, 57
wDirich-LM: definition, 54
wJM-LM2: definition, 57
wJM-LM: definition, 54
wLM1: definition, 51
wLM2: definition, 56
wLM: definition, 53
wPIN: definition, 62
wPoisson: definition, 42
wRSJ: definition, 33
wTF-IDF: definition, 16
wDFR-1;Poisson: definition, 71
wDFR-1;binomial: definition, 71
wDFR-2;Poisson: definition, 72
138 INDEX
wDFR-2;binomial: definition, 72
assumption
feature independence: PRF, 25
independence: PRF, 25
non-query term, 26
binomial probability
definition, 39
example: sunny days, 39
BIR and TF-IDF, 96
BIR term weight
derivation via binomial probability, 30
BIR: foundations, 29
BM25 and TF-IDF, 100
BM25: foundations, 45
BM25: notation, 48
burstiness, 15, 98
conditional entropy, 110
cross entropy, 110
definition
RSVBIR, 31
RSVBM25, 47
RSVDFR, 72
RSVDirich-LM, 55
RSVJM-LM, 54
RSVLM1, 52
RSVLM2, 56
RSVLM, 53
RSVPIN, 62
RSVPoisson, 42
RSVTF-IDF, 16
TFBM25, 45
TFpiv, 45
wBIR, 30
wBM25, 47
wDFR-1, 70
wDFR-2, 71
wDFR, 70
wDirich-LM2, 57
wDirich-LM, 54
wJM-LM2, 57
wJM-LM, 54
wLM1, 51
wLM2, 56
wLM, 53
wPIN, 62
wPoisson, 42
wRSJ, 33
wTF-IDF, 16
wDFR-1;Poisson, 71
wDFR-1;binomial, 71
wDFR-2;Poisson, 72
wDFR-2;binomial, 72
binomial probability, 39
DF variants, 14
entropy, 109
IDF variants, 14
Poisson bridge, 93
Poisson probability, 36
probability mixture (general), 49
probability mixture (LM), 50
probability of being informative, 20
query term BIR assumption, 96
query term burstiness assumption, 95
query term mixture assumption, 94
semi-subsumed event occurrences, 18
TF variants, 10
DF variants, 14
DFR and KL-divergence, 67
DFR and TF-IDF: gaps, 106
DFR and TF-IDF: risk times gain, 105
DFR: divergence from randomness, 63
DFR: elite documents, 69
DFR: example, 69
DFR: sampling, 65
DFR: term weights and RSV’s, 70
INDEX 139
DFR: transformation step, 66
Dirichlet-LM, 54
Divergence and DFR: foundations, 63
divergence from randomness (DFR), 63
divergence(London, UK): example, 63
entropy, 109
entropy: definition, 109
event spaces, 92
example
RSVPIN, 62
2-Poisson, 43
binomial probability: sunny days, 39
DFR: binomial-probability, 65
DFR: toy data, 69
divergence(London, UK), 63
independent terms and semi-subsumed
term occurrences, 19
Poisson probability: sunny days, 36
Poisson TREC-2, 38
Poisson: synthetic data, 37
probability mixture (general), 49
probability mixture (LM), 50
probability of relevance, 23
query term burstiness assumption, 95
semi-subsumed event occurrences, 18
exponent function: convergence equation
(limit definition), 21
extreme mixture assumption, 94
feature independence assumption: PRF, 25
foundations
BIR, 29
BM25, 45
Divergence and DFR, 63
LM, 49
PIN’s, 58
Poisson, 35
precision and recall, 76
PRF, 23
relevance-based models, 73
TF-IDF, 9
fractional TF, 13
gain + loss: DFR, 106
general matrix framework, 88
generalized vector-space model (GVSM), 85
GVSM and probabilities, 86
GVSM: generalized vector-space model, 85
IDF variants, 14
independence assumption: PRF, 25
independence-subsumption triangle, 19
independent terms and semi-subsumed term
occurrences: example, 19
inf1, 106
inf2, 106
information theory, 108
IQF: inverse query frequency, 124
IST: independence-subsumption triangle, 19
JM-LM, 54
joint entropy, 110
KL-divergence and DFR, 67
KL-divergence retrieval model, 73
KL-divergence: information theory, 111
Laplace law of succession, 33
length normalization, 98
link matrix, 59
LM and TF-IDF, 101
LM normalized, 52
LM1: LM mixture, 51
LM2, 56
LM: Dirichlet-LM, 54
LM: foundations, 49
LM: JM-LM, 54
logarithmic TF, 12
140 INDEX
logical IR: P.d ! q/, 82
matrix framework, 88
MI: mutual information, 110
mutual information (MI), 110
non-query term assumption, 26
notation, 3
notation: BM25, 48
notation: sets, symbols, probabilities, 5
parallel derivation, 92
pidf: probabilistic idf, 19
PIN’s and TF-IDF, 104
PIN’s: foundations, 58
Poisson
example TREC-2, 38
example: synthetic data, 37
Poisson and TF-IDF, 98
Poisson bridge, 43, 93
Poisson bridge: definition, 93
Poisson PRF, 40
Poisson probability
definition, 36
example: sunny days, 36
Poisson: example TREC-2, 38
Poisson: foundations, 35
precision and recall: foundations, 76
PRF: foundations, 23
PRF: Poisson, 40
PRF: relationships, 80
probabilistic idf, 19
probabilistic retrieval models: parallel
derivation, 92
probability P.d ! q/, 82
probability mixture (general): definition, 49
probability mixture (general): example, 49
probability mixture (LM): definition, 50
probability mixture (LM): example, 50
probability of being informative, 19
probability of being informative: definition,
20
probability of relevance framework, 80
probability of relevance: example, 23
Probability Ranking Principle (PRP), 26
PRP: Probability Ranking Principle, 26
query clarity, 111
query selectivity, 112
query term BIR assumption, 96
query term burstiness assumption, 95
query term mixture assumption, 94
query term probability assumptions, 94
rank-equivalent, 22
ratio-based TF, 13
relationships
divergence and LM, 112
divergence and TF-IDF, 112
GVSM, 85
GVSM and probabilities, 86
logical IR: P.d ! q/, 82
TF-IDF, 96
TF-IDF and BIR, 96
TF-IDF and BM25, 100
TF-IDF and DFR: gaps, 106
TF-IDF and DFR: risk times gain, 105
TF-IDF and LM, 101
TF-IDF and PIN’s, 104
TF-IDF and Poisson, 98
VSM, 83
VSM and probabilities, 85
relationships: overview, 114
relevance-based models: foundations, 73
retrieval models: overview, 78
retrieval models: timeline, 2
RSJ term weight, 33
score-equivalent, 22
semi-subsumed event occurrences, 17
INDEX 141
semi-subsumed event occurrences: definition,
18
semi-subsumed event occurrences: example,
18
Stirling’s formula: Poisson, DFR-2, 69
term statistics: Poisson example, 38
term weight and RSV
BIR, 30
BM25, 47
DFR, 70
Dirich-LM, 54
JM-LM, 54
LM-normalized, 52
LM1, 51
LM2, 56
PIN’s, 62
Poisson, 42
TF-IDF, 16
TF variants, 10
TF-IDF and BIR, 96
TF-IDF and BM25, 100
TF-IDF and DFR: gaps, 106
TF-IDF and DFR: risk times gain, 105
TF-IDF and LM, 101
TF-IDF and LM: side-by-side, 102
TF-IDF and PIN’s, 104
TF-IDF and Poisson, 98
TF-IDF: foundations, 9
TREC-2: Poisson example, 38
unified model lattice, 121
vector-space model (VSM), 83
VSM and probabilities, 85
VSM: vector-space model, 83

