Energy-Based Sequence GANs for Recommendation and Their
Connection to Imitation Learning
Jaeyoon Yoo
ECE, Seoul National University
yjy765@snu.ac.kr
Heonseok Ha
ECE, Seoul National University
heonseok.ha@gmail.com
Jihun Yi
ECE, Seoul National University
t080205@snu.ac.kr
Jongha Ryu
ECE, UC San Diego
jongha.ryu@gmail.com
Chanju Kim
NAVER Corp.
chanju.kim@navercorp.com
Jung-Woo Ha
NAVER Corp.
jungwoo.ha@navercorp.com
Young-Han Kim
ECE, UC San Diego
yhk@ucsd.edu
Sungroh Yoon
ECE, Seoul National University
sryoon@snu.ac.kr
ABSTRACT
Recommender systems aim to find an accurate and efficient map-
ping from historic data of user-preferred items to a new item that
is to be liked by a user. Towards this goal, energy-based sequence
generative adversarial nets (EB-SeqGANs) are adopted for recom-
mendation by learning a generative model for the time series of
user-preferred items. By recasting the energy function as the feature
function, the proposed EB-SeqGANs is interpreted as an instance
of maximum-entropy imitation learning.
KEYWORDS
Generative adversarial network, Recommendation system, Energy-
based model, Imitation learning
ACM Reference format:
Jaeyoon Yoo, HeonseokHa, Jihun Yi, Jongha Ryu, Chanju Kim, Jung-WooHa,
Young-Han Kim, and Sungroh Yoon. 2017. Energy-Based Sequence GANs
for Recommendation and Their Connection to Imitation Learning. In Pro-
ceedings of ACM Conference, Washington, DC, USA, July 2017 (Conference’17),
6 pages.
https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
Over the past decades, numerous attempts have been made to build
efficient mechanisms to recommend items (such as products, songs,
videos, news articles, and so on) to users based on the preference
of individual users, with the ultimate goal of increasing the level
of satisfaction for users and the corresponding level of revenue for
providers. Due to their high computational cost, traditional rec-
ommendation systems (see, for example, [17]) have typically dealt
with relatively small datasets. Availability of large-scale datasets as
well as advances in hardware for computation and storage, how-
ever, are changing the landscape for development of more powerful
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
Conference’17, July 2017, Washington, DC, USA
© 2017 Copyright held by the owner/author(s).
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM.
https://doi.org/10.1145/nnnnnnn.nnnnnnn
recommendation systems that have both high accuracy and scala-
bility [5, 11].
In this context, recommendation algorithms based on deep learn-
ing have recently received much attention. A recommendation
system can be viewed as a mapping from a sequence of items
a user preferred to a new item the user is likely to prefer. As a
universal approximator[15], a deep neural network is expected to
learn this mapping efficiently and accurately in a scalable man-
ner. Moreover, deep learning often finds proper representations
of items implicitly, thus allowing transformative recommendation
system designs across multiple domains. As a consequence, several
variations of deep neural networks, including autoencoders [27],
convolutional neural networks(CNNs) [18], and recurrent neural
networks(RNNs) [11], have been proposed for recommendation.
In order to emulate typical behaviors of a user more explicitly, a
generative model can be utilized to reproduce a sequence of items
preferred by the user. For example, a hiddenMarkovmodel has been
trained to generate a next item from previous items and recommend
it to the user [26].
In this paper, we proposes to use state-of-the-art generative
adversarial networks (GANs) [9] as a deep learning based generative
model for recommendation. More concretely, we combine energy-
based GANs(EBGANs) [35] and sequence GANs(SeqGANs) [34] to
develop energy-based sequence GANs (EB-SeqGANs) to learn the
sequential preference of a user and use the resulting generative
deep network to produce a recommended next item to the user. The
use of GANs in general and EB-SeqGANs in particular (which, to
the best of our knowledge, has not been explored before) provides
a promising framework for the task of recommendation for the
following two reasons.
First, GANs harness both the descriptive and discriminative
power of deep learning effectively to train a generative model
with high accuracy and scalability. Since the seminar paper by
Goodfellow et al. [9], GANs have proved to outperform existing
algorithms in many tasks ranging from face image generation [25]
to robust control [13]. GANs, properly trained, are expected to
predict a precise pattern of each user’s preference.
Second, EB-SeqGANs, with a proper choice of the potential func-
tion, can be interpreted as an instance of imitation learning based
on the maximum entropy principle. Imitation learning aims to
ar
X
iv
:1
70
6.
09
20
0v
1 
 [
cs
.I
R
] 
 2
8 
Ju
n 
20
17
Conference’17, July 2017, Washington, DC, USAJaeyoon Yoo, Heonseok Ha, Jihun Yi, Jongha Ryu, Chanju Kim, Jung-Woo Ha, Young-Han Kim, and Sungroh Yoon
find a suitable decision by mimicking the experts’ demonstration.
Consequently, the proposed EB-SeqGANs are expected to be well
trained from expert recommendations (in this case, the actual items
preferred by the users themselves).
The rest of the paper is organized as follows. Section 2 provides a
brief introduction on GANs and imitation learning. In Section 3, we
develop our recommendation system using EB-SeqGANs. Section 4
is devoted to elucidate the connection between EB-SeqGANs and
imitation learning. Section 5 introduces related work. In section 6,
we discuss the ideas presented and conclude.
Throughout the paper, music recommendation will be used to
illustrate in concrete terms how EB-SeqGANs can be used for rec-
ommendation. The proposed approach, however, can be applied
more broadly and easily transformed to other domains.
2 BACKGROUND
2.1 Generative Adversarial Networks
Generative adversarial networks [9] is a class of generative models
that learns by game theoretic competition between a generator
G and a discriminator D. The generator G learns the underlying
distribution pdata based on data, so thatG can generate an artificial
data that resembles the real data. On the other hand, the discrimi-
nator D learns to distinguish between the true data and the fake
one generated by generator.G and D are usually implemented with
deep learning models. Once the learning procedure is done, one
would expect that G may have enough generative power to fool
discriminator, which implies thatG generates a data looks like the
real one. The following is the objective for GANs:
max
D
ES∼pdata logD(S) + ES∼pG log(1 − D(S)) (1)
min
pG
ES∼pG log(1 − D(S)) (2)
S is data on the domain of interest. It may be a two dimensional
matrix in the image domain and a sequence of items in recom-
mender system domain. D(S) represents the likelihood for discrimi-
natorD to predict S to be real data.WhilepG is the distribution from
which generatorG generates a fake data, pdata is the distribution of
the real data. ES∼q [f (S)] represents an expectation of f (S) when S
follows the distribution q. (1) means discriminator D must increase
the likelihood of real data and decrease the likelihood of the one
that generated by generator. At the end, this leads D to distinguish
the real data from the fake one. (2) means that the generator G
should be optimized to generate the data that looks real so that the
discriminator D gives high likelihood to the generated data. Then
optimizing generator G leads pG to be optimized to pdata and vice
versa. Thus we write pG as an optimization variable in (2). In the
rest of the paper, a generator refers to either G or pG with abuse
of terminology. We define S as a data but we would like to remark
that S can be used to mean a state in the section 2.4 and 4.
2.2 Energy-Based Generative Adversarial
Networks
Energy-based GANs [35] is one of many variants of GANs. It
changes the meaning of discriminator’s output D from the likeli-
hood in (1), (2) to energy function, and D learns to make the energy
function low for data from pdata and high for those from pG . More
precisely, the objective is given as follows:
min
D
ES∼pdataD(S) + ES∼pG [m − D(S)]+ (3)
min
pG
ES∼pGD(S) +ψ (pG ) (4)
D(S) means the energy for data S rather than likelihood as in
previous section. This difference attributes to the change from (1),(2)
to (3),(4). Discriminator discriminates real data from fake data by
minimizing the energy of data from pdata and maximizing that of
data from pG . This is described in (3). Generator is optimized to
generate samples with low energy, in other words, realistic data in
(4). [x]+ = max{x , 0} denotes a margin function, and this term in
updating D cuts off the fake data from G when it is too unrealistic
to D, i.e. has energy larger than m. Also ψG (·) is a functional of
distribution and plays a role of preventing mode collapsing of pG .
Without this term, pG would collapse down to only one datum that
minimizes D.
2.3 Sequence generative adversarial networks
Sequence GANs is proposed to deal with sequential data in GANs
framework [34]. It still uses the same objective in (1) for D, but
considers a temporal structure of data when optimizing pG . We use
the following gradient of the objective for pG when the data is of
the form S = s1s2 . . . sT :
T∑
t=1
Es ′1s
′
2 ...s
′
t∼pG (S )∇pG (s
′
t |s ′1 . . . s ′t−1)Q(s ′1 . . . s ′t ). (5)
In (5), Q(s1 . . . st ) denotes the expected log-likelihood of the dis-
criminator given s1 . . . st . The detail to derive (5) is close to (9)-(14)
in the next section, so we omit the derivation of (5). For the inter-
ested reader, please refer to [34]. While the learning procedure, we
sample s1 . . . st and then estimate Q(s1 . . . st ) from rollout samples.
Then, we optimize pG using the estimated Q from (5). Updating D
is to optimize (1).
2.4 Maximum Entropy-based Imitation
Learning
Imitation learning is an algorithm to solve decision making pro-
cess [1]. The set of disciplined actions at each state is called policy.
The basic idea of imitation learning is to solve the problem by
mimicking the expert’s policy based on given demonstrations.
Ziebart et al. introduced Maximum entropy(MaxEnt) imitation
learningwhich is based on themaximum entropy principle [16] [36].
In [36], Ziebart et al. regarded an imitation learning as finding a
policy P which satisfies
EP [f (S,A)] = EPE [f (S,A)], (6)
where f (S,A) is a feature vector for a state S and an action A,
and PE denotes the expert’s policy which is approximated by the
given demonstration. However, this is still an ill-posed problem in
a sense that we cannot pick one policy among many candidates
satisfying above. To give the guideline of the choice, [36] adopts the
maximum entropy principle which states that the most reasonable
probability distribution with constraints is the one that maximizes
Energy-Based Sequence GANs for Recommendation and Their Connection to Imitation LearningConference’17, July 2017, Washington, DC, USA
its entropy [16]. More precisely, the problem now can be formulated
as
min
P
− H (P)
s.t. EP [f (S,A)] = EPE [f (S,A)]
(7)
where H (P) denotes the entropy of P .
Due to Slater’s condition strong duality holds, and one can obtain
primal and dual solutions from KKT condition [4]. The solution has
the form
P(S,A) = exp(−c
T f (S,A))
Z
, (8)
where c is a vector that maximizes the log-likelihood of expert’s
demonstration, and Z is a partition function. There is freedom to
choose a feature vector f (S,A), and domain knowledge may be
exploited depending on applications. For example, for a sequen-
tial recommender system, one possible option is using a feature
from word2vec [22] learning inspired by natural language process-
ing [21]. In this paper, for simplicity we choose a state-action occur-
rence as our feature. A state-action occurrence feature has the same
dimension with the number of state-action pairs. Provided the order
of the state-action pairs, the ith element of feature vector, fi (S,A)
is 1 when (S,A) is a ith state-action pair and 0 otherwise. In this
case the solution can be represented as P(S,A) = exp(−c(S,A))/Z .
3 ENERGY-BASED SEQGANS FOR
RECOMMENDATION
In this section, we would propose generative adversarial networks
adopted recommender system specified in Algorithm. 1. We apply
EBGANs [35] to recommender system which gives the form of line
6-11 in Algorithm. 1. EBGANs is known to be robust to hyperpa-
rameters and has more stable training process [2] compared to the
original GANs in (1) and (2). Considering the temporal structure, we
extend EBGANs to EB-SeqGANs for recommender systems. This
extension adds line 1-4 in Algorithm. 1.
To apply EBGANs to a recommender system, we need to specify
the data and the models for the generator and the discriminator.
First, in music recommendation system, the data S in (3) and (4)
may correspond to a played history. We write S = s1s2s3 . . . sT
where st represents the song played at timestep t . Secondly, for the
generatorG , as a music recommender system exploiting a listening
history must be causal, we use recurrent neural networks (RNNs),
which has showed an outstanding performance for temporal data
processing [21] [29]. For the discriminator D, any deep learning
model such as 1D convolutional neural networks or autoencoder
can be used [35].
We use a negative entropy −H (pG ) for ψ in (4) instead of the
heuristic repelling regularization term in [35]. As the negative
entropy becomes smaller, pG is less likely to be sharp, and mode
collapsing is avoided. The validity of using the negative entropy will
be corroborated by a theoretical interpretation in the next section.
Considering the temporal structure of pG , we can convert the
update process of pG in (4) into SeqGANs framework [34]. Substi-
tuting negative entropy in (4), the gradient of the objective of pG is
given as follows.
Algorithm 1 EB-SeqGANs(pG ,D)
1: procedure SeqGANs(pG ,D)
2: д← ∑t ∑s1 ...st pG (s1 . . . st )∇ logpG (st |s1 . . . st−1)
3:
∑
st+1 ...sT pG (st+1 . . . sT |s1 . . . st )[D(S) + logpG (S)]
4: optimize pG with д as the gradient
5:
6: Require : 1) Choosem, the margin value. 2) Choose the regu-
larizer termψ (negative entropy in this paper), 3) Obtain data
sampled from pdata
7: Initialize pG and D
8: repeat
9: update D
maximize ES∼pdataD(S) + ES∼pG [m − D(S)]+ w.r.t D
10: update pG
minimize ES∼pGD(S) +ψ (pG ) w.r.t. pG
by SeqGANs(pG ,D)
11: until convergence
∇EpG (S )[D(S) + logpG (S)] (9)
=
∑
∇pG (S)[D(S) + logpG (S)] +
∑
pG (S)∇ logpG (S) (10)
=
∑
∇pG (S)[D(S) + logpG (S)] +
∑
∇pG (S) (11)
=
∑
pG (S)∇ logpG (S)[D(S) + logpG (S)] (12)
=
∑
pG (S)
∑
t
∇ logpG (st |s1 . . . st−1)[D(S) + logpG (S)] (13)
=
∑
t
∑
s1 ...st
pG (s1 . . . st )∇ logpG (st |s1 . . . st−1)
∑
st+1 ...sT
pG (st+1 . . . sT |s1 . . . st )[D(S) + logpG (S)]
(14)
Note that While obtaining (12), we change the order of summa-
tion and gradient and use the fact that the sum of probability is
1. In (14),
∑
st+1 ...sT pG (st+1 . . . sT |s1 . . . st )[D(S)+ logpG (S)] is ex-
pected value of D(S) + logpG (S) given s1 . . . st and plays the same
role withQ in (5). Thus, for the pG with temporal structure, (4) can
be optimized as in SeqGANs. Consequently, each generator and
discriminator optimization would follow (14) and (3), respectively.
We would like to call this update process by energy-based sequence
generative adversarial networks(EB-SeqGANs).
Once EB-SeqGANs in a recommender system is trained, we can
exploit the learned model to recommend items by providing users
a sequence that pG generates. The overall process contains two
phase as illustrated in Figure 1. In the training phase, we train gen-
erator and discriminator with EB-SeqGANs framework given users’
histories. In the recommendation phase, the learned generator is
exploited to generate the recommending list given the songs that
user listened before.
4 CONNECTION BETWEEN EBGAN AND
IMITATION LEARNING
In this section, we show that how to perform imitation learning
in recommender system, and as a result the energy-based GANs
Conference’17, July 2017, Washington, DC, USAJaeyoon Yoo, Heonseok Ha, Jihun Yi, Jongha Ryu, Chanju Kim, Jung-Woo Ha, Young-Han Kim, and Sungroh Yoon
EB-SeqGANs framework
Training phase
RNN-based 
Generator
Neural Network-based 
Discriminator
s2s1 st-1…
s2s1 st-1…
s2s1 st-1…
s2s1 sT…
Recommendation phase
Auto play list
Recommendation
EB-SeqGANs framework
st+1st …
st+1st …
…
st+1st …
Top ks2s1 st-1…
RNN-based 
Generator
Neural Network-based 
Discriminator
Figure 1: Overall process of EB-SeqGANs in recommender system
can be interpreted as imitation learning. The overall flow of this
section is illustrated in Figure 2.
Music recommendation can be viewed as a series of decision
making process in that it determines which item to suggest. That is,
it is to determine which song to recommend at timestep t , provided
that the user listened s1, s2, s3, . . . , st−1 before.
The common framework for the decision making process is re-
inforcement learning [30]. Reinforcement learning finds a policy
that maximizes the reward the agent gets by performing an action
a at each state s . There have been several papers that proposed
an application of reinforcement learning in recommendation sys-
tem [8] [31]. Music recommendation problem can be formulated
as a reinforcement learning problem by considering a history of
played songs as a state and recommending a song to listen as an
action. Assuming the recommended song must be played at each
time, the transition is deterministic. In addition, a reward would
be a numeric value that indicates how much the user likes the
recommended song a.
A reward-maximizing policy found by reinforcement learning
can be regarded as a music recommender system. However, defining
the reward practically is hardly possible. Thus it is not straightfor-
ward to apply reinforcement learning algorithms to music recom-
mender system.
Instead, recommender system stores the users’ histories. Thus,
imitation learning is a reasonable approach to construct the recom-
mender system.
As mentioned above, the solution of MaxEnt imitation learning is
the form of P(S,A) = exp(−c(S,A))/Z , and c is found bymaximizing
the log–likelihood of the experts’ demonstration. Several attempts
show how to implement the optimization of c [36] [24] [33] [7].
While most of them are not scalable, the method in [7] shows
its scalability. However, since [7] used the gradient which has a
fraction form, it causes overflow and high variance issues in that
denominator part of the gradient is a probability density lower than
1. However, as we will show, our method described in (20), (21) does
not have such form, thus may not suffer from those issues.
To optimize c to maximize the log–likelihood of the experts’
demonstration, we will use gradient-based optimization and the
gradient of the log–likelihood is given as follows.
∇ log(likelihood) = −E(S,A)∼demo∇c(S,A) + E(S,A)∼P (S,A)∇c(S,A)
(15)
In the first term, demo stands for the expert’s demonstrations.
The first term can be calculated easily by substituting given demon-
strations. Second term is, however, intractable for the system with
large state and action space as well as long timestep. It is equivalent
to the negative term in Restricted Boltzmann Machine (RBM) [12],
and hence this term can be estimated by the sampling method used
in Restricted Boltzmann Machine. One can use importance sam-
pling, but it raises high variance. Using metropolis-hasting (MH)
or Gibbs sampling may be another option, however, these methods
have high computational complexity as they require a large number
of inferences.
To circumvent those disadvantages, we introduce an optimiza-
tion variable q(S,A) in place of the sampling distribution P(S,A)
in (15). q(S,A) can be of any family of distribution from which it
is easy to sample. Using q(S,A) and approximating the gradient of
log–likelihood brings out the following equation.
∇ log(likelihood) ≈ −E(S,A)∼demo∇c(S,A) + Eq(S,A)∇c(S,A) (16)
Hence, by integrating the above equation, we can find an approxi-
mate log–likelihood(LL) up to constants.
Approximate LL = −E(S,A)∼democ(S,A) + E(S,A)∼q(S,A)c(S,A)
(17)
As we introduce q(S,A) instead of P(S,A) to ease the sampling
procedure, we need to impose the constraint that q(S,A) is close
to P(S,A). Hence we require that q(S,A) and P(S,A) are close in
KL divergence. It can be seen as approximate inference [3]. Still,
however, there is freedom to choose among KL(q | |P) and KL(P | |q)
since KL divergence is asymmetric. We choose KL(q | |P) to be mini-
mized for the following reason. First, KL(P | |q) requires sampling
from P(S,A), which is desired to be avoided. Second, when P(S,A)
is multimodal, which is a common case, KL(P | |q) would average
out the modes and result in a poor approximation [3].
Summing up, we can modify the original problem that optimizes
c with the gradient (15) by the following two-step problem.
min
q
KL(q | |P) for fixed c (18)
max
c
− E(S,A)∼democ(S,A) + E(S,A)∼q(S,A)c(S,A) for fixed q (19)
Ideally, if q minimizes KL divergence, q(S,A) matches up P(S,A)
exactly and the gradient of approximate LL becomes the exact one.
Energy-Based Sequence GANs for Recommendation and Their Connection to Imitation LearningConference’17, July 2017, Washington, DC, USA
Recommender 
System
Imitation 
Learning
Exponential Dist.
Optimization
EBGAN
Decision making process solution
MaxEnt principle
EM style formulation
Energy-based 
SeqGAN
Temporal structure
Figure 2: Flowchart of section 4
These steps are very close to expectation maximization algorithm
(EM) and approximate inference [3]. Now, imitation learning can
be considered as repeating the above process using gradient based
optimization.
In (18), KL(q | |P) = Eq(S,A)c(S,A) − H (q) + logZ gives a new
objective Eq(S,A)c(S,A) −H (q) for q because logZ is constant with
respect to q. In addition, as mentioned above, recommender system
can be assumed to have a deterministic transition at = st+1, thenA
can be omitted. Finally, imitation learning on recommender system
is formulated as follows.
repeat
min
q
ES∼q(S )c(S) − H (q) for fixed c (20)
max
c
− ES∼democ(S) + ES∼q(S )c(S) for fixed q (21)
One may observe the similarities between (20), (21) and (3), (4).
c ,q in (20), (21) correspond to D and pG in (3),(4) respectively. Sub-
stituting negative entropy for regularizer term in (4) makes the
objective for q and pG equivalent, and the objective for c and D
are the same except for the minor difference, the margin function
[·]+ = max(·, 0).
To exploit the learned imitation learning system for the rec-
ommendation, the learned distribution of the sequence P(S) =
exp(−c(S))/Z may be used. However, it is intractable to get P(S)
when state space is large. Instead, q(S) can be used to recommend
items because it becomes close to P(S) provided that the learning
process is successful. It matches to using pG for recommendation.
Applying imitation learning to music recommendation system
is reasonable in that constructing the recommender system can be
regarded as solving decision making processes. We have showed
that our method can be considered as imitation learning by approx-
imating the gradient with approximate distribution. Also it can be
interpreted that while imitation learning uses the given feature
such as state occurrence, EBGANs determines the energy with im-
plicit feature. i.e EBGANs determines cT f in (8) without definite
feature f . Finally, this theoretical connection between EBGANs and
MaxEnt imitation learning supports the application of EB-SeqGANs
to recommender systems.
5 RELATEDWORK
The idea of exploiting deep learning and generative modeling for
recommender systems has been widely explored. Salakhutdinov
et al. [27] developed a recommendation system that can predict
movie ratings by using the restricted Boltzmann machine (RBM).
Ouyang et al. [23] used autoencoders to predict movie ratings.
Van den Oord et al. [32] combined convolutional neural networks
(CNNs) and a matrix factorization technique to learn the represen-
tation of music data for recommendation. Sahoo et al. [26] utilized
the hidden Markov model (HMM) while Hidasi et al. [11] used
recurrent neural networks (RNNs) for generative modeling.
There exists prior work that attempted to reveal connections be-
tween GANs and other types of learning techniques closely related
to imitation learning, although none of these previous attempts
were in the context of recommender system design. Ho and Er-
mon [13] studied the relationship between inverse reinforcement
learning (IRL) and the original GANs mentioned in Section 2. Their
idea was based on that IRL works by giving rewards to expert
policies while penalizing fake policies, which resembles the main
idea of GANs. Finn et al. [6] connected the original GANs and
the guided cost learning [7], which is to solve the imitation learn-
ing problem by using importance sampling and the guided policy
search [19]. The key difference between their approach and ours is
as follows: Their approach directly approximates the training objec-
tive in imitation learning, whereas our method first approximates
the gradient of the objective and then approximates the objective
using the approximated gradient, as detailed in Section 4. The di-
rect approximation of the objective often tends to cause numerical
issues such as overflows and high variances. This difference also
allows us to link imitation learning to energy-based GANs rather
than the original GANs.
Our idea of replacing the sampling distribution in the imitation
learning by an optimization variable was inspired by [10] and has
some connection to the actor-critic reinforcement learning [30]. In
this paper, we optimize the sampling distribution with sequence
generative adversarial network, whereas in [10], the authors used
the Stein-variational gradient descent (SVGD) [20]. Applying SVGD
to categorical data in recommender systems is difficult, as the kernel
functions and the notion of distance in categorical space are often
not clearly defined.
6 DISCUSSION
As the need for generating sequences of recommendation items
emerges in recent recommender systems, adopting GANs (one of
the most popular deep generative models) may be a logical step.
Conference’17, July 2017, Washington, DC, USAJaeyoon Yoo, Heonseok Ha, Jihun Yi, Jongha Ryu, Chanju Kim, Jung-Woo Ha, Young-Han Kim, and Sungroh Yoon
In this paper, we have described how to adopt EBGANs to con-
struct such a recommender system and also showed how EBGANs
and imitation learning can be related. Applying EBGANs to a rec-
ommender system can be interpreted as an instance of applying
maximum-entropy imitation learning to the recommender system.
In this paper, we used an RNNs for recommender systems in a
GANs framework. Another option to use an RNNs in the context
of recommender system design would be to use an RNNs in a su-
pervised learning setting [11]. This can be regarded as a behavior
cloning that learns the action at each timestep separately [14]. How-
ever, it is known that the behavior cloning suffers from cascading
errors [14], which can cause a recommender system to return an
unlikely sequence of items as the error accumulates. In the case of a
music recommender system, avoiding such a problem is particularly
critical because many users often turn on the so-called auto-play
functionality. Our proposed method can be regarded as imitation
learning and can learn an entire trajectory of recommendations,
rather than individual ones. We thus expect that our approach will
suffer less from the cascading error issue.
Our next step will be testing the proposed method with various
recommendation data for thorough performance evaluation. As
mentioned previously, our methodology is not limited to music
recommender systems but is applicable to other types of sequential
recommender systems (e.g., click stream analysis and recommenda-
tion). Of note is that the popular top@k recall metric [11] will not
be ideal for assessing the performance of sequential recommender
systems. This metric is not related to cascading errors as the previ-
ous history is not generated by the recommender system itself but
always given by data. Consequently, we will need an alternative
metric to better assess the performance of a recommender system
that returns a sequence of recommendations.
We may encounter other challenges when testing the proposed
approach with real data. Examples include the strong dependence
of the sequence GAN components on pretraining and slow con-
vergence to equilibria [25]. We expect that employing appropriate
techniques such as trust region policy optimization (TRPO) will be
helpful for addressing such challenges [28] [13].
REFERENCES
[1] Tamim Asfour, Pedram Azad, Florian Gyarfas, and Rüdiger Dillmann. 2008. Imi-
tation learning of dual-arm manipulation tasks in humanoid robots. International
Journal of Humanoid Robotics 5, 02 (2008), 183–202.
[2] David Berthelot, Tom Schumm, and Luke Metz. 2017. BEGAN: Boundary Equilib-
rium Generative Adversarial Networks. arXiv preprint arXiv:1703.10717 (2017).
[3] Christopher M Bishop. 2006. Pattern recognition. Machine Learning 128 (2006),
1–58.
[4] Stephen Boyd and Lieven Vandenberghe. 2004. Convex optimization. Cambridge
university press.
[5] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks
for youtube recommendations. In Proceedings of the 10th ACM Conference on
Recommender Systems. ACM, 191–198.
[6] Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. 2016. A Connec-
tion between Generative Adversarial Networks, Inverse Reinforcement Learning,
and Energy-Based Models. arXiv preprint arXiv:1611.03852 (2016).
[7] Chelsea Finn, Sergey Levine, and Pieter Abbeel. 2016. Guided cost learning:
Deep inverse optimal control via policy optimization. In Proceedings of the 33rd
International Conference on Machine Learning, Vol. 48.
[8] Nick Golovin and Erhard Rahm. 2004. Reinforcement learning architecture for
web recommendations. In Information Technology: Coding and Computing, 2004.
Proceedings. ITCC 2004. International Conference on, Vol. 1. IEEE, 398–402.
[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In Advances in neural information processing systems. 2672–2680.
[10] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. 2017.
Reinforcement Learning with Deep Energy-Based Policies. arXiv preprint
arXiv:1702.08165 (2017).
[11] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.
2015. Session-based recommendations with recurrent neural networks. arXiv
preprint arXiv:1511.06939 (2015).
[12] Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensional-
ity of data with neural networks. science 313, 5786 (2006), 504–507.
[13] Jonathan Ho and Stefano Ermon. 2016. Generative adversarial imitation learning.
In Advances in Neural Information Processing Systems. 4565–4573.
[14] Jonathan Ho, Jayesh Gupta, and Stefano Ermon. 2016. Model-free imitation learn-
ing with policy optimization. In International Conference on Machine Learning.
2760–2769.
[15] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989. Multilayer feed-
forward networks are universal approximators. Neural networks 2, 5 (1989),
359–366.
[16] Edwin T Jaynes. 1957. Information theory and statistical mechanics. Physical
review 106, 4 (1957), 620.
[17] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-
niques for recommender systems. Computer 42, 8 (2009).
[18] Chenyi Lei, Dong Liu, Weiping Li, Zheng-Jun Zha, and Houqiang Li. 2016. Com-
parative Deep Learning of Hybrid Representations for Image Recommendations.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
2545–2553.
[19] Sergey Levine and Vladlen Koltun. 2013. Guided Policy Search.. In ICML (3). 1–9.
[20] Qiang Liu and Dilin Wang. 2016. Stein variational gradient descent: A gen-
eral purpose Bayesian inference algorithm. In Advances In Neural Information
Processing Systems. 2370–2378.
[21] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khu-
danpur. 2010. Recurrent neural network based language model.. In Interspeech,
Vol. 2. 3.
[22] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Advances in neural information processing systems. 3111–3119.
[23] Yuanxin Ouyang, Wenqi Liu, Wenge Rong, and Zhang Xiong. 2014. Autoencoder-
based collaborative filtering. In International Conference on Neural Information
Processing. Springer, 284–291.
[24] Jan Peters, Katharina Mulling, and Yasemin Altun. 2010. Relative entropy policy
search. In Twenty-Fourth AAAI Conference on Artificial Intelligence.
[25] Alec Radford, Luke Metz, and Soumith Chintala. 2015. Unsupervised representa-
tion learning with deep convolutional generative adversarial networks. arXiv
preprint arXiv:1511.06434 (2015).
[26] Nachiketa Sahoo, Param Vir Singh, and Tridas Mukhopadhyay. 2010. A hidden
Markov model for collaborative filtering. (2010).
[27] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. 2007. Restricted Boltz-
mann machines for collaborative filtering. In Proceedings of the 24th international
conference on Machine learning. ACM, 791–798.
[28] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz.
2015. Trust region policy optimization. In Proceedings of the 32nd International
Conference on Machine Learning (ICML-15). 1889–1897.
[29] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning
with neural networks. In Advances in neural information processing systems. 3104–
3112.
[30] Richard S Sutton and Andrew G Barto. 1998. Reinforcement learning: An intro-
duction. Vol. 1. MIT press Cambridge.
[31] Nima Taghipour and Ahmad Kardan. 2008. A hybrid web recommender system
based on q-learning. In Proceedings of the 2008 ACM symposium on Applied
computing. ACM, 1164–1168.
[32] Aaron Van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep
content-based music recommendation. In Advances in neural information process-
ing systems. 2643–2651.
[33] Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. 2015. Deep inverse
reinforcement learning. CoRR (2015).
[34] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2016. Seqgan: sequence
generative adversarial nets with policy gradient. arXiv preprint arXiv:1609.05473
(2016).
[35] Junbo Zhao, Michael Mathieu, and Yann LeCun. 2016. Energy-based generative
adversarial network. arXiv preprint arXiv:1609.03126 (2016).
[36] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. 2008.
Maximum Entropy Inverse Reinforcement Learning.. In AAAI, Vol. 8. Chicago,
IL, USA, 1433–1438.

