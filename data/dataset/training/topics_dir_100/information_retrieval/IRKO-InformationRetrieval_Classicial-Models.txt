Prof. Dr. Knut Hinkelmann 41Information Retrieval and Knowledge Organisation - 2 Information Retrieval
2.2 Classical Information Retrieval Models
Boolean Model
Vectorspace Model
Prof. Dr. Knut Hinkelmann 42Information Retrieval and Knowledge Organisation - 2 Information Retrieval
2.2.1 The Boolean Model Retrieval Model
Binary index: Terms are either present or absent. Thus, 
wij ∈ {0,1}
Queries are specified as Boolean expressions in which 
terms are combined with the operators AND, OR, and 
NOT
q = ta  AND  (tb  AND  NOT tc)
Simple model based on set theory with precise semantics
The model views each document as just a set of words
vehicle OR  car AND accident
Prof. Dr. Knut Hinkelmann 43Information Retrieval and Knowledge Organisation - 2 Information Retrieval
Boolean Retrieval Function
The retrieval function can be defined recursivley
R(ti,di) = TRUE, if wij = 1 (i.e. ti is in dj )
= FALSE, if wij = 0 (i.e. ti is not in dj )
R(q1 AND q2,di) = R(q1,di)  AND  R(q2,di)
R(q1 OR q2,di) = R(q1,di)  OR  R(q2,di)
R(NOT q,di) = NOT R(q,di)
The Boolean functions computes only values 0 or 1, i.e. Boolean retrieval
classifies documents into two categories
relevant (R = 1) 
irrelevant (R = 0)
Prof. Dr. Knut Hinkelmann 44Information Retrieval and Knowledge Organisation - 2 Information Retrieval
Example für Boolesches Retrieval
(vehicle OR  car) AND  accident
Query:
R(vehicle OR  car AND  accident, d1) =
R(vehicle OR  car AND  accident, d2) =
R(vehicle OR  car AND  accident, d3) =
TRUE
FALSE
FALSE
d1 d2 d3
accident 1 0 1
car 1 1 0
cause 0 0 1
crowd 0 0 1
die 1 0 0
drive 0 0 1
four 0 0 1
heavy 1 0 0
injur 0 0 1
more 0 1 0
morning 1 0 0
people 1 0 1
quarter 0 1 0
register 0 1 0
truck 0 0 1
trucker 0 0 1
vehicle 0 1 0
vienna 1 1 1
yesterday 1 0 0
(vehicle AND  car) OR  accident
Query:
R(vehicle AND  car OR  accident, d1) =
R(vehicle AND car OR accident, d2) =
R(vehicle AND car OR accident, d3) =
TRUE
TRUE
TRUE
Prof. Dr. Knut Hinkelmann 45Information Retrieval and Knowledge Organisation - 2 Information Retrieval
Processing Boolean Queries
Conjunctive queries are most widely
used
Example: Processing simple conjunctive
queries:
Locate "car" in the dictionary
Retrieve its postings
Locate "accident" in the dictionary
Retrieve its postings
Intersect the two posting lists
Query Optimization: For more than two
terms in a conjunctive query, start with
two shortest posting lists
car AND  accident
Algorithm for the intersection of 
two posting list p1 und p2:
Prof. Dr. Knut Hinkelmann 46Information Retrieval and Knowledge Organisation - 2 Information Retrieval
Drawbacks of the Boolean Model
Retrieval based on binary decision criteria 
no notion of partial matching
No ranking of the documents is provided (absence of a 
grading scale)
The query q = t1 OR t2 OR t3 is satisfied by document
containing one, two or three of the terms t1, t2, t3
No weighting of terms, wij ∈ {0,1}
Information need has to be translated into a Boolean 
expression which most users find awkward
The Boolean queries formulated by the users are most 
often too simplistic
As a consequence, the Boolean model frequently returns 
either too few or too many documents in response to a 
user query
Prof. Dr. Knut Hinkelmann 47Information Retrieval and Knowledge Organisation - 2 Information Retrieval
2.2.2 Vector Space Model
Index can be regarded as an n-
dimensional space
wij > 0  whenever  ti ∈ dj
Each term corresponds to a 
dimension
To each term  ti is associated a 
unitary vector vec(i)
The unitary vectors vec(i)  and
vec(j) are assumed to be 
orthonormal  (i.e., index terms are 
assumed to occur independently 
within the documents)
document can be regarded as
vector started from (0,0,0)
point in space
(4,3,1)
vehicle
accident
car
(3,2,3)
d1 d2
accident 4 3
car 3 2
vehicle 1 3
Example:
Prof. Dr. Knut Hinkelmann 48Information Retrieval and Knowledge Organisation - 2 Information Retrieval
2.2.2.1 Coordinate Matching
Documents and query are represented as 
document vectors vec(dj) = (w1j, w2j, …, wkj) 
query vector vec(q) = (w1q,...,wkq)
Vectors have binary values
wij = 1 if term ti occurs in Dokument dj
wij = 0 else
Ranking:
Return the documents containing at least one query term
rank by number of occuring query terms
Ranking function: scalar product
R(q,d) = q * d
=       qi * diΣ
i=1
n
Multiply
components and 
summarize
Prof. Dr. Knut Hinkelmann 49Information Retrieval and Knowledge Organisation - 2 Information Retrieval
Coordinate Matching: Example
Resultat:
q * d1   =
q * d2   =
q * d3   =
3
2
2
query vector represents
terms of the query
accident heavy vehicles vienna
d1 d2 d3 q
accident 1 0 1 1
car 1 1 0 0
cause 0 0 1 0
crowd 0 0 1 0
die 1 0 0 0
drive 0 0 1 0
four 0 0 1 0
heavy 1 0 0 1
injur 0 0 1 0
more 0 1 0 0
morning 1 0 0 0
people 1 0 1 0
quarter 0 1 0 0
register 0 1 0 0
truck 0 0 1 0
trucker 0 0 1 0
vehicle 0 1 0 1
vienna 1 1 1 1
yesterday 1 0 0 0
Prof. Dr. Knut Hinkelmann 50Information Retrieval and Knowledge Organisation - 2 Information Retrieval
Assessment of Coordinate Matching
Advantage compared to Boolean Model: Ranking
Three main drawbacks
frequency of terms in documents in not considered
no weighting of terms
privilege for larger documents
Prof. Dr. Knut Hinkelmann 51Information Retrieval and Knowledge Organisation - 2 Information Retrieval
2.2.2.2 Term Weighting
Use of binary weights is too limiting
Non-binary weights provide consideration for partial 
matches
These term weights are used to compute a degree of 
similarity between a query and each document
How to compute the weights  wij and  wiq ?
A good weight must take into account two effects:
quantification of intra-document contents (similarity)
tf  factor, the term frequency within a document
quantification of inter-documents separation (dissi-milarity)
idf  factor, the inverse document frequency
wij = tf(i,j) * idf(i) (Baeza-Yates & Ribeirp-Neto 1999)
Prof. Dr. Knut Hinkelmann 52Information Retrieval and Knowledge Organisation - 2 Information Retrieval
TF - Term Frequency
Let freq(i,j) be the raw frequency of term 
ti within document  dj (i.e. number of 
occurrences of term ti in document dj)
A simple tf factor can be computed as
f(i,j) = freq(i,j)
A normalized  tf  factor is given by
f(i,j)  =  freq(i,j) /  max(freq(l,j))
where the maximum is computed over 
all terms which occur within the 
document  dj
d1 d2 d3 q
accident 2 0 1 1
car 1 1 0 0
cause 0 0 1 0
crowd 0 0 1 0
die 1 0 0 0
drive 0 0 1 0
four 0 0 1 0
heavy 2 0 0 1
injur 0 0 1 0
more 0 2 0 0
morning 1 0 0 0
people 1 0 2 0
quarter 0 1 0 0
register 0 1 0 0
truck 0 0 1 0
trucker 0 0 1 0
vehicle 0 1 0 1
vienna 1 1 1 1
yesterday 1 0 0 0
(Baeza-Yates & Ribeiro-Neto 1999)
For reasons of simplicity, in this example f(i,j) = freq(i,j)
Prof. Dr. Knut Hinkelmann 53Information Retrieval and Knowledge Organisation - 2 Information Retrieval
IDF – Inverse Document Frequency
IDF can also be interpreted as the amount of information  
associated with the term ti . A term occurring in few
documents is more useful as an index term than a term
occurring in nearly every document
Let ni be the number of documents containing term ti
(document frequency)
N be the total number of documents
A simple idf factor can be computed as
idf(i) =  1/ni
A normalized  idf  factor is given by
idf(i) =  log (N/ni)
the log is used to make the values of  tf and  idf comparable. 
Prof. Dr. Knut Hinkelmann 54Information Retrieval and Knowledge Organisation - 2 Information Retrieval
Example with TF and IDF
In this examle a  simple tf factor 
f(i,j) = freq(i,j)
and a simple idf factor
idf(i) =  1/ni
are used
It is of advantage to store IDF and 
TF separately
IDF d1 d2 d3
accident 0.5 2 0 1
car 0.5 1 1 0
cause 1 0 0 1
crowd 1 0 0 1
die 1 1 0 0
drive 1 0 0 1
four 1 0 0 1
heavy 1 2 0 0
injur 1 0 0 1
more 1 0 2 0
morning 1 1 0 0
people 0.5 1 0 2
quarter 1 0 1 0
register 1 0 1 0
truck 1 0 0 1
trucker 1 0 0 1
vehicle 1 0 1 0
vienna 0.33 1 1 1
yesterday 1 1 0 0
Prof. Dr. Knut Hinkelmann 55Information Retrieval and Knowledge Organisation - 2 Information Retrieval
Indexing a new Document
Changes of the indexes when adding a new document d
a new document vector with tf factors for d is created
idf factors for terms occuring in d are adapted
All other document vectors remain unchanged
Prof. Dr. Knut Hinkelmann 56Information Retrieval and Knowledge Organisation - 2 Information Retrieval
Ranking Scalar product computes co-occurrences
of term in document and query
Drawback: Scalar product privileges large 
documents over small ones
Euclidian distance between endpoint of 
vectors
Drawback: euclidian distance privileges small
documents over large ones
Angle between vectors
the smaller the angle beween query and 
document vector the more similar they are
the angle is independent of the size of the
document
the cosine is a good measure of the angle 
t1
t2
q d
Prof. Dr. Knut Hinkelmann 57Information Retrieval and Knowledge Organisation - 2 Information Retrieval
Cosine Ranking Formula
the more the directions of query a and 
document dj coincide the more relevant is
dj
the cosine formula takes into account the
ratio of the terms not their concrete
number
Let θ be the angle between q and dj
Because all values wij >= 0 the angle θ is
between 0° und 90°
the larger θ the less is cos θ
the less θ the larger is cos θ
cos 0 = 1
cos 90° = 0
t1
t2
q dj
cos(q,dj) = 
q ° dj
|q| ° |dj|
Prof. Dr. Knut Hinkelmann 58Information Retrieval and Knowledge Organisation - 2 Information Retrieval
The Vector Model
The best term-weighting schemes use weights which are 
given by 
wij =  f(i,j) *  log(N/ni)
the strategy is called a tf-idf  weighting scheme
For the query term weights, a suggestion is
wiq =  (0.5  +  [0.5 * freq(i,q) / max(freq(l,q)]) *  log(N/ni)
(Baeza-Yates & Ribeirp-Neto 1999)
Prof. Dr. Knut Hinkelmann 59Information Retrieval and Knowledge Organisation - 2 Information Retrieval
The Vector Model
The vector model with  tf-idf  weights is a good ranking 
strategy with general collections
The vector model is usually as good as the known ranking 
alternatives. It is also simple and fast to compute.
Advantages:
term-weighting improves quality of the answer set
partial matching allows retrieval of docs that approximate 
the query conditions
cosine ranking formula sorts documents according to 
degree of similarity to the query
Disadvantages:
assumes independence of index terms (??); not clear that 
this is bad though (Baeza-Yates & Ribeiro-Neto 1999)

