Classical Music Clustering Based on Acoustic Features
Xindi Wang
Northeastern University
360 Huntington Ave.
Boston, Massachusetts 02115
wang.xind@husky.neu.edu
Syed Arefinul Haque
Northeastern University
360 Huntington Ave.
Boston, Massachusetts 02115
haque.s@husky.neu.edu
ABSTRACT
In this paper we cluster 330 classical music pieces collected from
MusicNet database based on their musical note sequence. We use
shingling and chord trajectory matrices to create signature for each
music piece and performed spectral clustering to find the clusters.
Based on different resolution, the output clusters distinctively indi-
cate composition from different classical music era and different
composing style of the musicians.
KEYWORDS
clustering, classical music, shingles, spectral clustering
1 INTRODUCTION
For centuries, people have been intrigued by the depth and richness of
classical music. This leads to questions like what characterize Bach’s music
and what makes them so different from others compositions. In this project,
we build a clustering algorithm for classical music pieces from the music
itself using MusicNet dataset [12]. This clustering algorithm would help
music recommendation, understand patterns in music and relationships
between music pieces.
1.1 Motivation
1.1.1 Genres we get from human cooperative classification are fuzzy. Cat-
egorizing musics based on genre is a difficult problem. Currently, people rely
on methods involving humans, such as social tags and collaborative filtering.
However, social tags are subjective decisions which could be influenced by
cultural and perceptual differences [3]. Collaborative filtering [4] might not
work well for music pieces that have not appeared in anyone’s playlist or
review.
1.1.2 Acoustic feature to understand music compositions. One way to
address this problem is to rely on acoustic features. This prevents the in-
coherency related to human labeling and this is the only kind of feature
that is presented in any music. Also, during the process, we would have the
opportunity to gain patterns in music compositions.
1.2 Contribution
• Built a system that could take any given set of music with their
note sequence, find clusters based on two extracted features.
• Proposed two ways of extracting features from music’s note se-
quence, shingles and chord trajectory matrix. Among them, chord
trajectory matrix is novel and offers us a new way to see music
structure.
• Found clusters that are related to composers, and could reflect
music era.
2 RELATEDWORK
In music feature extraction, spectrogram [6] is a very widely used feature.
A spectrogram is a visual representation of the spectrum of frequencies in
a sound or other signal as they vary with time or some other variable. The
calculation of spectrogram is related to Fourier transformation. However,
spectrogram has limited potential to help us understand composer’s com-
posing styles and patterns. Spectrogram only takes care of spectrum of a
given music piece, not focusing on how music progresses, how different
parts of music cooperate together, etc., and is highly related to instruments.
In [8], an information theoretic approach was applied to find the sim-
ilarity between musics. They used a dataset of MIDI files of 60 classical
music pieces, 12 jazz pieces and 12 rock pieces and clustered those musics
based on normalized compression distance (NCD). They mentioned that
an ideal candidate for the information theoretic distance would have been
Kolmogorov complexity which is not computable in practice. Instead they
rely on the compressibility of music as a proxy of information theoretic
distance. Genres they found using this method seems to conform to the spe-
cific artist and pre-labeled genres. The quartet method they used produces
slightly different similarity score even for the same pair of music because
of the randomness in the similarity calculation process.
Since the purpose of our clustering is not only finding similar pieces,
but also discovering composing style for different composers, we choose to
utilize information directly related to composition - the music note sequence,
and propose method that could compare different note sequences.
3 BACKGROUND
3.1 Basic Music Concept
3.1.1 Measure. In musical notation, a measure (or bar) is a segment
of time corresponding to a specific number of beats in which each beat is
represented by a particular note value and the boundaries of the bar are
indicated by vertical bar lines [9].
3.1.2 Beats. In music theory, the beat is the basic unit of time, the pulse
(regularly repeating event), of the mensural level [9]. In MusicNet data,
each note has a record of measure and beat. Here the beat denotes the time
when this note happens in this specific beat, where each measure has time
interval "1". For example, in the score below, the first note has beat 0, the
second note has beat 1/4 (since time signature is 4/4, so each quarter note
has a length of 0.25).
Figure 1: Concepts of classical music
3.2 Music Coding
3.2.1 Instrument Code. MusicNet records the instrument of each note
as well. The coding is based on the MIDI instrument code and the decoding
information for different instruments can be found in [1].
3.2.2 MIDI Codes for notes. To record the note value, MusicNet use
MIDI encoding for notes, which ranges from 0-127. It expands 11 octaves
ar
X
iv
:1
70
6.
08
92
8v
1 
 [
cs
.I
R
] 
 2
7 
Ju
n 
20
17
for 12 pitches (C, C#, etc.). Information for decoding the midi code for notes
can be found in [2].
3.2.3 Interval Tree. MusicNet data use Interval Tree [5] to store infor-
mation of notes. An interval tree is a tree data structure to hold intervals.
Specifically, it allows one to efficiently find all intervals that overlap with
any given interval or point. It is often used for windowing queries.
4 PROPOSED APPROACH
The question we want to solve is, given a set of music, how we can get indi-
vidual signature of each music by utilizing only musical notation features,
and then how we can find clusters and validate them.
4.1 Feature Extraction
4.1.1 Shingles. A natural way of extracting musical features is by re-
garding each music piece as a document, where the "alphabets" are the notes.
Therefore, we could use shingling method to build the signature-music ma-
trix and use MinHash and LSH to estimate similarity. Through MinHash
and LSH we can query pieces having similarity larger than a threshold we
want or get top k similar pieces for each piece. Therefore, we could produce
a ϵ -affinity matrix or a k-nearest neighbor affinity matrix.
4.1.2 Chord Trajectory Matrix. Another way of extracting music fea-
tures is through building a Chord Trajectory Matrix. In music, chord transi-
tion is very important and varies based on different composers, instruments
and themes. Since we have 128 MIDI notes (coded 0-127) and pause (we
coded it as 128), we would have a 129 × 129 matrix that could represent
one music piece. After building Chord Trajectory Matrices, we could use
different norms to calculate similarity between them and obtain similarity
score for each pair of music pieces.
4.2 Spectral Clustering
Both shingles and chord trajectory matrix would produce an affinity matrix
for music pieces, where an entry close to 0 means the pair is dissimilar and
close to 1 means they are similar. Spectral clustering [11] is a clustering
method focusing on the connectivity of data, which directly utilize similarity
between data points. To perform spectral clustering we need constructs the
Laplacian matrix from the affinity matrix and use the eigenvectors to obtain
balanced cut of the graph.
4.3 Validation
Validation of clusters is a difficult problem. In this project, we use both
internal and external validation. For internal validation, we use silhouette
coefficient, which compare the distances within cluster and between clusters.
For external validation, we utilize the meta-data we have, which includes
composers, movements and ensembles. We put all the meta-data together
and form a "document" for each cluster. Then we calculate TF-IDF [7] for
each term to help us understand the cluster as well as validating them. For
example, to get an idea of which cluster is mostly about which musician we
get the composer metadata for each music piece in the cluster and find the
TF-IDF value for all possible composers in that cluster. Then we choose the
top k keywords(composer) for the cluster to be its sense making keyword.
4.4 Flow Chart
The flow chart of our approach is shown in Fig. 2 and Fig. 3
5 EXPERIMENTS
5.1 Data and Data Processing
The dataset we use is MusicNet [12], which is a collection of 330 free-
licensed classical music recordings, in total of 34 hours of chamber music
performances. It consists of almost 1.3 million labels indicating the precise
time of each note every recording, the instrument that plays each note and
the notes position in the metrical structure of the composition. Besides
important Acoustic Features, MusicNet also includes composer and title.
These meta-data label for clustering are used during validation of results.
The data structure of MusicNet is interval tree (See 3.2.3), which allows
users to query what notes are playing at a given time or between a time
interval. However, querying by time would produce duplicate note record
if the length of the note is longer than the query interval and this would
influence the quality of shingles and chord trajectory matrix. Therefore we
develop an algorithm to extract note sequence using the information of
start-time, end-time, measure, beat for each individual note.
The key of this algorithm is to add pauses between notes because we
would lost information of pauses if we use all the labels in the interval tree
directly. Adding pauses are done in two parts: adding pauses between notes
and adding pauses at beginning of a measure and at end of a measure. For
adding pauses between notes, we compare the start time and end time for
sequential notes and if the difference is larger than a threshold, we add a
pause between them. For adding pauses at the beginning and the end, we
obtain the start time and end time for each measure across all instruments,
naming global start time and global end time. Then for each measure for
each instrument, we compare its start and end time with the global start
and end time. If the difference is larger than a threshold, we add a pause.
The limitation of this process is that, we only add one pause regardless
of how long the pause is. This may influence the result of the shingles and
the chord trajectory matrix.
Another problem in the dataset is that the note are only distinguished
by instrument. However, when we have two violins in one piece, we can
not distinguish them. This problem could potentially be solved using fac-
torization. For simplicity, we ignore this issue in the project. Therefore the
note sequence we get is merged as one instrument for each unique type of
instrument.
One final problem is how to create shingles out of note sequence with
notes being played together. There are multiple ways to deal with this
problem. For the sake of simplicity we take the numerically highest note if
there are multiple notes happening at the same time. This is based on the
assumption that notes appearing together usually constitutes a harmonic
chord, and the highest note usually could represent the dominant note in
this chord.
5.1.1 Coding Variation. The original data coding for note is the 128
MIDI note coding. We have different variation of coding to capture different
aspect of the note.
Pitch Coding. Ignoring the octaves, we could convert the 128MIDI coding
to 12 pitch coding. The 12 pitches are C, C#, D, D#, E, F, F#, G, G#, A, A#, B.
In this way, we only focus on what chord is now presenting.
Relative Coding. Recoding the first note of the piece as 0, we could recode
every note in this piece based on this first note baseline. In this way, we
would have similarity 1 if two pieces are just a simple modulation shift.
Change Coding. We could calculate the change between two sequential
notes by simply subtracting the MIDI code of the later note from the MIDI
code of previous note. Then we can use the sequence of differences to
represent the music. In this way, we are focusing on the change of chord of
the music piece.
5.2 Results
5.2.1 Shingles. We did shingling on different shingle size (k ∈ [2, 7])
and different data coding (default MIDI coding, pitch coding, relative coding,
change coding). To get affinity matrix, we use MinHashing and LSH to
efficiently calculate similarity and obtain two types of graph, ϵ -graph and k -
nearest neighbor graph. For ϵ -graph, we use different threshold for different
size of shingle (0.1 for k = 2, 0.05 for k = 3, 0.01 for k = 4 and k = 5, 0.005
Classical Music Clustering Based on Acoustic Features
Figure 2: Flow chart for shingles
Figure 3: Flow chart for chord trajectory matrix
for k = 6 and k = 7). For k -nearest neighbor graph, we choose the number
of neighbors as 10. When doing spectral clustering, we could use the whole
corpus or select specific instrument we are interested in - we select piano
(MIDI code 1), violin (MIDI code 41), viola (MIDI code 42) and cello (MIDI
code 43) in our experiments.
The obtained similarity distribution for different shingle size is shown
in Fig. 4. We could see that the similarity score goes down very quickly
when we have a larger shingle size. For k = 2, the average similarity score
is about 0.5, but for k = 7, most of the pieces has similarity close to 0.
Figure 4: Similarity distribution for different shingle size
Here is one example clustering result on shingle size 4, using pitch coding
and ϵ -graph, clustered on the whole corpus and setting number of clusters
to be 22 (Fig. 5). Using TF-IDF [7] to find the characteristic keywords of each
cluster, we could see that on the left of the graph, there is a big cluster of
Bachmusic. Whenwe zoom into this cluster, there are actually three clusters
all containing Bach musics. The keyword of the first one is "fugue-Bach",
the second one is "cello-suite-Bach" and the third one is "harpsichord-Bach".
This cluster separation corresponds to different types of Bach music we
have in our corpus. Therefore, we could conclude that clusters we found
could reflect certain work of a composer.
To further validate the performance of clustering, we use the internal
method for cluster validation - silhouette coefficient [10]. Silhouette coeffi-
cient compare the within cluster distance and between cluster distance. The
closer to 1, the better the performance. Fig. 7 shows the silhouette coefficient
distribution for different shingle size fixing the number of clusters to be 10
(here we all use pitch coding). We could see that as the shingle size increases,
the silhouette coefficient decreases. Fig. 6 shows the silhouette coefficient
distribution for different number of clusters setting shingle size to be 3.
With more clusters, the silhouette coefficient score decreases. However we
could see that the silhouette coefficient score is low in general, far from the
ideal value of 1. This may be because the similarity score between pieces are
Figure 5: Cluster visualization for shingle method (22 clus-
ters)
very low in general (shown in Fig. 4), therefore the within cluster distance
and between cluster distance are not that different from each other.
We also conduct K-medoids clustering on shingles. When we use the
Manhattan distance, the output is always a huge cluster even when we
increase the number of cluster. When we use Jaccard distance, we have
similar result as spectral clustering and the silhouette coefficient remains to
be low. For k = 3 and pitch coding, the average silhouette coefficients are
0.0176,0.0126, 0.0063, for the number of clusters 2, 5 and 10 respectively.
From silhouette coefficient point of view, spectral clustering is better than
K-medoids.
Figure 6: Silhouette Coefficient for different shingle size (10
clusters)
Figure 7: Silhouette Coefficient for different number of clus-
ters (shingle size 3)
5.2.2 Chord Trajectory Matrix. The chord trajectory matrix is another
way to create a signature out of each music piece. In this matrix we create
a relationship between each MIDI code (ranging from 0-127) and pauses
(considered as the 128th note) based on which note follows which note. In
other words we can think of the sequence of musical notes as a directed
path and chord trajectory matrix is the adjacency matrix of that path. To
clarify the concept lets imagine we have the following sequence of mu-
sical notes: {A, C, C, A, C, pause, C, C, pause, pause }. Then without
considering the MIDI encoding we can think of the sequence as a set of
the edges {(A, C), (C, C), (C, A), ..., (pause, pause)} and convert them
to the chord trajectory matrix,C3×3 which looks like the following:
C =
A C pause( )0 2 0 A
1 2 2 C
0 1 1 pause
In the above chord trajectory matrix, we make an increment to an entry
of C, cii by 1 when a note i is followed by the note itself and we make an
increment to the entry of C, ci j by 1 when note i is followed by note j . But
there are some cases where a collection of notes are played together and
followed by another note/s. For example lets imagine the notes’ sequence is
{{A, B }, {B, C }, D }. In that case the edges would be cross product of each
pair of sets in the sequence and the edges for the chord trajectory matrix
would look like {(A, B), (A, C), (B, B), (B, C), (B, D), (C, D)}.
Combining chord trajectory matrices in an ensemble piece: If a
music piece is an ensemble of different instruments then we create the
chord trajectory matrix of each instrument separately and finally add them
together. All of the chord trajectory matrix is 128 × 128. As a result, simple
addition between the matrices are well defined. In Fig. 8 we show one
such example where we create a chord trajectory matrix of music no. 2365
from the MusicNet dataset which is titled String Quartet No 12 in E-flat,
composed by Beethoven.
Visual similarity in chord trajectorymatrix:By inspecting the chord
trajectory matrix, we can see some interesting visual similarity between
the music pieces. We know that Bach is celebrated for his unique style of
composing fugue and cello compositions. By inspecting the visual repre-
sentation of the chord trajectory matrices In Fig. 9 we can see that Bach’s
Fugues have an unique shape which looks like Phoenix bird and his cello
compositions almost has a beetle like shape. The most intriguing outcome
from the visual inspection is that each of the Bach fugue has this unique
phoenix bird shape.
Finding difference between two chord trajectorymatrices: To find
the distance between two chord trajectory matricesC1, C2, at first we create
a difference matrix, D = C1−C2. Then we calculate the norm of D . We have
tried two different norms. The first norm is | |D | |F =
√∑
i
∑
j d2i j , which is
known as the Frobenius norm and second norm is , | |D | |F =
∑
i
∑
j |di j |
which we call as absolute value norm. Upon inspection we have found
that absolute value norm penalizes small differences in the entries of two
matrices which is not desired for our distance calculation. So we finally
used Frobenius norm to calculate distances between two matrices.
Creating affinity matrix: After finding pairwise distances between all
the 330 music we create an affinity matrix. But before that we normalize
all the distances by doing a feature scaling using the following formula:
x ′ = x−min(X )max (X )−min(X ) . After normalizing we convert the distance into
affinity by subtracting the distance from 1.
Spectral Clustering: Finally from the affinity matrix we perform spec-
tral clustering in the some method as described above. To validate the
clustering we calculated the silhouette co-efficient of the whole cluster.
The co-efficient is very low for all the cases. We tried different number
of clusters. the average silhouette coefficients are 0.232, 0.106 and 0.021
respectively when the number of clusters are 2, 5 and 24.
Sense Making of the Clusters: When we find two clusters in all the
330 music pieces (Fig. 11), the first cluster’s keyword is "Bach-Haydn" and
the other cluster’s keyword is "Ravel-Faure". Which makes sense based
on the division of epochs in classical music. Based on the era of different
classical musicians in Fig. 10 we can say that these two clusters indicate the
two extremes of the timeline. Again, when we break them into five clusters
(Fig. 12) we can see that each of the clusters represent a sequential mix of
different musical era demonstrated in Fig. 10.
Most interestingly, we see all the Bach musics are contained in a single
large cluster along with other pieces while we keep increasing the number
of clusters; although we saw the visual dissimilarity between Bach’s fugues
and cello compositions. When the number of clusters is increased into 24,
suddenly Bach musics are nicely separated into two clusters, where one
cluster is all the fugues along with some compositions of Beethoven and in
the other cluster we find all the cello compositions by Bach along with a
single piano composition by him.
6 CONCLUSIONS AND FUTUREWORK
6.1 Conclusions
In this project, we develop a process for music clustering using two set of
feature extraction methods - shingles and chord trajectory matrix. We find
that the clusters for both features are related to composers style of compos-
ing music. For chord trajectory matrix approach, the clusters we found also
relate to different eras of music. From chord trajectory matrix itself, we also
discover certain patterns for musical ensembles and composers.
Classical Music Clustering Based on Acoustic Features
Figure 8: Combining different instruments in an ensemble into single chord trajectory matrix
Figure 9: Patterns and similarity in Bach music
Figure 10: Era of classical music
6.2 Limitations and Future Work
• Improvements on shingles From our investigation, clustering
results for shingles are not ideal based on our meta-data. The
clusters are more mixed than the chord trajectory approach. This
could relate to the choice of shingle size and data coding.
• Improvements on chord trajectory matrix The chord trajec-
tory matrix we proposed are very basic. We omit the information
of instrument and ensembles. Also the norm we used for similarity
calculation are basic Frobenius norm and Absolute value norm. In
Figure 11: Cluster visualization for chord trajectory matrix
(2 clusters)
Figure 12: Cluster visualization for chord trajectory matrix
(5 clusters)
our future work we would like to put weights on instrument in-
formation while constructing chord trajectory matrix and we will
also explore different norms to find the similarity score between
matrices.
• Investigation on silhouette coefficientWe use silhouette coef-
ficient as the internal validation for clusters. However the result is
not ideal. We would like to investigate more on whether silhouette
coefficient is a good way for our clusters and method and whether
there are better metric for cluster validation.
• Analysis on harmony In our proposed feature extraction meth-
ods (shingles and chord trajectory matrix), we emphasize on how
music progress, i.e notes appearing sequentially. However, another
very important aspect of music is about what notes are appear-
ing together to produce harmony. We would like to include this
feature into our future analysis and clustering.
• Temporal information The features we use all lose the temporal
information. We would like to include the time information in our
features by using methods such as temporal motifs [12].
• Data limitWe only have 330 pieces of classical music data, and
the data is biased. We have about 40% percent of Beethoven music
and the remaining 60% percent belongs to 9 different composers.
This leads to the fact that Beethoven appears in almost every
cluster we have. We would like to experiment our method on a
larger corpus.
REFERENCES
[1] Accessed April 25, 2016. General MIDI instrument codes. http://www.ccarh.org/
courses/253/handout/gminstruments
[2] Accessed April 25, 2016. MIDI Note Numbers for Different Octaves.
http://www.electronics.dit.ie/staff/tscarff/Music_technology/midi/midi_
note_numbers_for_octaves.htm
[3] Luis Barreira, Sofia Cavaco, and Joaquim Ferreira. 2011. Unsupervised Music
Genre Classification with a Model-Based Approach. (2011), 268–281.
[4] Pedro Cano, Oscar Celma, Markus Koppenberger, and Javier M. Buld. 2006.
Topology of music recommendation networks. Chaos 16, 1 (2006). https://doi.
org/10.1063/1.2137622 arXiv:arXiv:physics/0512266v1
[5] Mark De Berg, Marc Van Kreveld, Mark Overmars, and Otfried Cheong
Schwarzkopf. 2000. Computational geometry. In Computational geometry.
Springer, 1–17.
[6] Eric J Isaacson. 2005. What You See Is What You Get: on Visualizing Music.. In
ISMIR. Citeseer, 389–395.
[7] Jure Leskovec, Anand Rajaraman, and Jeffrey David Ullman. 2014. Mining of
massive datasets. Cambridge University Press.
[8] R de Wolf R. Cilibrasi P.M.B. Vitanyi. 2004. Algorithmic clustering of music
based on string compression. Computer Music Journal 28, 4 (2004), 49–67. https:
//doi.org/10.1162/0148926042728449
[9] Gardner Read. 1972. Music notation: a manual of modern practice. Rodale Press.
[10] Peter J Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and
validation of cluster analysis. Journal of computational and applied mathematics
20 (1987), 53–65.
[11] Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and image segmentation.
IEEE Transactions on pattern analysis and machine intelligence 22, 8 (2000), 888–
905.
[12] John Thickstun, Zaid Harchaoui, and Sham Kakade. 2017. Learning Features
of Music from Scratch. In International Conference on Learning Representations
(ICLR).

