A preference elicitation interface for collecting dense
recommender datasets with rich user information
Demo
Pantelis P. Analytis
Cornell University
pp464@cornell.edu
Tobias Schnabel
Cornell University
tbs49@cornell.edu
Stefan Herzog
MPI for Human Development
herzog@mpib-berlin.mpg.de
Daniel Barkoczi
MPI for Human Development
barkoczi@mpib-berlin.mpg.de
Thorsten Joachims
Cornell University
tj@cs.cornell.edu
ABSTRACT
We present an interface that can be leveraged to quickly and ef-
fortlessly elicit people’s preferences for visual stimuli, such as
photographs, visual art and screensavers, along with rich side-
information about its users. We plan to employ the new interface to
collect dense recommender datasets that will complement existing
sparse industry-scale datasets. The new interface and the collected
datasets are intended to foster integration of research in recom-
mender systems with research in social and behavioral sciences. For
instance, we will use the datasets to assess the diversity of human
preferences in different domains of visual experience. Further, using
the datasets we will be able to measure crucial psychological effects,
such as preference consistency, scale acuity and anchoring biases.
Last, we the datasets will facilitate evaluation in counterfactual
learning experiments.
CCS CONCEPTS
•Human-centered computing→ Collaborative filtering; So-
cial media; Collaborative and social computing devices;
KEYWORDS
preference elicitation, recommender system datasets, visual art
1 INTRODUCTION
Over the last three decades the recommender systems community
has made immense progress in the way we represent, understand
and learn people’s preferences as a function of previously collected
explicit or implicit evaluations. Research in recommender systems
has by all means increased the quality of the curated and recom-
mended content in the online world. Several large datasets have
been a crucial component of this success, as they have commonly
functioned as test-beds on which new theories and algorithms have
been compared (Movielens, LastFM and Netflix to name just a few).
Most of these datasets, however, are very sparse. They contain
thousands items and even the most popular among the items have
been evaluated only by a small subset of their users. Given the large
fraction of missing ratings, it is challenging to accurately estimate
even simple quantities like the average quality of an item, especially
since the patterns of missing data are subject to strong selection
Recsys’17, August 2017, Como, Italy
2017. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn
biases [6]. This presents fundamental challenges when evaluating
recommendation algorithms on sparse datasets. Further, it becomes
an obstacle for scholars in the social and behavioral sciences as
workarounds have to be developed for dealing with missing values.
To the best of our knowledge, the only dense collaborative filter-
ing dataset was the outcome of the Jester Interface [3]. The interface
curated 100 jokes of various styles and topics. People utilized a slider
to evaluate 5 jokes that were presented to them sequentially. The
first evaluations were used to estimate people’s preferences and to
recommend them the remaining jokes. The users continued to read
and evaluate jokes until the pool of 100 items was exhausted. In
total, more than 70.000 people have evaluated at least some of the
jokes, and more than 14.000 have evaluated all the jokes, resulting
to a fully evaluated subset of the dataset.
Figure 1: The design of the preference elicitation interface.
We replicate the design of the Jester interface, using a contin-
uous bar that people can use to express howmuch they liked
or disliked an item. Participants have to wait for at least 5
seconds before they can proceed to the next item.
ar
X
iv
:1
70
6.
08
18
4v
2 
 [
cs
.S
I]
  2
7 
Ju
n 
20
17
2 THE INTERFACE AND DATA COLLECTION
We plan to collect new datasets in different domains of people’s vi-
sual experience, ranging from photographs and paintings to designs
for screensavers. Our interface replicates the design of the Jester
interface, adding new elements that can counteract its limitations.
At the outset, people are provided with instructions about how to
use the interface. Then, before the presentation of the stimuli we
collect demographic information about the users. To reduce possi-
ble order effects, the visual stimuli are presented in random order.
As in Jester, users are asked to evaluate items using a slider bar;
they can move the marker of the slide bar to the left to indicate that
they did not like the item, or to the right to indicate that they liked
it. We implement a continuous scale, which allows a fine-grained
evaluation of the presented items. Finally, to limit anchoring bias,
the slide bar is initially semi-transparent and the colors become
vivid only when the user has clicked on it.1
Once all the items have been evaluated, we collect further psy-
chologically relevant information about the users. Numerous stud-
ies have shown that side information can substantially improve
estimates of people’s preference and it complements first hand eval-
uations [5]. In the first experiments we will deploy the visual-art
expertise questionnaire developed by Chatterjee et al. [2] to gauge
people’s familiarity with the visual arts and a succinct version of the
big-five questionnaire to quickly assess the people’s personalities
[7] (see Figure 2). It takes about 20 minutes to complete the current
version of the interface, including the instructions, questionnaires
and evaluation phase.
We intend to conduct the first experiments at Amazon’s Mechani-
cal Turk labor market. Several studies have shown that for effortless
tasks the results produced on mTurk are comparable to laboratory
studies [4]. The visual stimuli used in this interface evoke imme-
diate aesthetic judgments, and thus can quickly be transformed
to evaluations. Eventually, we intend to develop a data visualiza-
tion tool that will reward people who complete the study with
information about their preference profiles and how they relate to
those of other individuals. Thus, we intend to create an inherently
motivating interface using as a reward the informational value gen-
erated by the collected data. In this way, we will reduce the cost of
data collection, but also introduce basic ideas behind collaborative
filtering and recommender systems to the wider public.
3 POTENTIAL APPLICATIONS
We envisage several new applications for the developed datasets.
Here we foreshadow a few of these potential uses, keeping in mind
that the community that will have access to the produced datasets
will certainly come up with more. First, they will facilitate cross-
fertilizationwith the cognitive and behavioral sciences. For instance,
social and cognitive psychologists have extensively studied simple
strategies for inference and estimation where different features are
used to predict an objective truth. The new datasets will open the
way to study strategies for social preference learning in domains
where no objective truth exists [1]. Also, we can manipulate the
design of the interface to study relevant behavioral effects, such as
to study the consistency of evaluations or to investigate the effect
1The interface can be accessed at http://abc-webstudy.mpib-berlin.mpg.de/recstrgs/
study_simulator.php. Both the code for the interface and the collected data will be
publicly available.
Figure 2: At the end of the evaluation phase we collect addi-
tional information about the users. We invited the users to
complete a questionnaire about their expertist in the visual
arts and a 10-question version of the big-five questionnaire.
of the granularity of the evaluation scale on the predictions. To
sum up, the datasets will allow us to better understand preference
diversity and its implications for different recommender systems
algorithms as well as for psychological social learning strategies.
Moreover, we believe that the new datasets can fuel existing
streams of research in recommender systems and machine learning.
For instance, dealing with selection-biases and with data missing
not at random is a growing research stream in recommender sys-
tems and machine learning [9]. To evaluate algorithms tuned to
deal with such problems, we can impose selection biases ex-ante
and remove data from the dense dataset accordingly. This set-up
could complement existing sparse datasets for learning, with the
difference that selection biases can be controlled and varied in order
to test robustness. Moving on to the broader class of counterfactual
simulations, dense datasets greatly simplify evaluation since they
can serve as ground-truth when conducting simulations [8].
REFERENCES
[1] Pantelis P Analytis, Daniel Barkoczi, and Stefan M Herzog. You’re special, but
it doesn’t matter if you’re a greenhorn: Social recommender strategies for mere
mortals. In Cognitive Science Society, pages 1799–1804. Cognitive Science Society,
2015.
[2] Anjan Chatterjee, Page Widick, Rebecca Sternschein, William B Smith, and Bianca
Bromberger. The assessment of art attributes. Empirical Studies of the Arts,
28(2):207–222, 2010.
[3] Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Eigentaste: A
constant time collaborative filtering algorithm. Information Retrieval, 4(2):133–151,
2001.
[4] Gabriele Paolacci, Jesse Chandler, and Panagiotis G Ipeirotis. Running experiments
on Amazon Mechanical Turk. Judgment and Decision Making, 5(5), 2010.
[5] Seung-Taek Park and Wei Chu. Pairwise preference regression for cold-start
recommendation. In Proceedings of the third ACM conference on Recommender
systems, pages 21–28. ACM, 2009.
[6] B. Pradel, N. Usunier, and P. Gallinari. Ranking with non-random missing ratings:
influence of popularity and positivity on evaluation metrics. In RecSys, pages
147–154, 2012.
[7] Beatrice Rammstedt and Oliver P John. Measuring personality in one minute
or less: A 10-item short version of the big five inventory in english and german.
Journal of Research in Personality, 41(1):203–212, 2007.
[8] Matthew J Salganik, Peter Sheridan Dodds, and Duncan J Watts. Experimental
study of inequality and unpredictability in an artificial cultural market. Science,
311(5762):854–856, 2006.
[9] Tobias Schnabel, Adith Swaminathan, Peter I Frazier, and Thorsten Joachims.
Unbiased comparative evaluation of ranking functions. In ICTIR, pages 109–118,
2016.
2

