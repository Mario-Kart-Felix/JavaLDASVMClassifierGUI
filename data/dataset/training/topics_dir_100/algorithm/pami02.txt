An Efficient k-Means Clustering Algorithm:
Analysis and Implementation
Tapas Kanungo, Senior Member, IEEE, David M. Mount, Member, IEEE,
Nathan S. Netanyahu, Member, IEEE, Christine D. Piatko, Ruth Silverman, and
Angela Y. Wu, Senior Member, IEEE
AbstractÃIn k-means clustering, we are given a set of n data points in d-dimensional space Rd and an integer k and the problem is to
determine a set of k points in Rd, called centers, so as to minimize the mean squared distance from each data point to its nearest center.
A popular heuristic for k-means clustering is Lloyd's algorithm. In this paper, we present a simple and efficient implementation of Lloyd's
k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only
major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis
of the algorithm's running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we
present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization,
data compression, and image segmentation.
Index TermsÃPattern recognition, machine learning, data mining, k-means clustering, nearest-neighbor searching, k-d tree,
computational geometry, knowledge discovery.
Ã¦
1 INTRODUCTION
CLUSTERING problems arise in many different applica-tions, such as data mining and knowledge discovery
[19], data compression and vector quantization [24], and
pattern recognition and pattern classification [16]. The
notion of what constitutes a good cluster depends on the
application and there are many methods for finding clusters
subject to various criteria, both ad hoc and systematic.
These include approaches based on splitting and merging
such as ISODATA [6], [28], randomized approaches such as
CLARA [34], CLARANS [44], methods based on neural nets
[35], and methods designed to scale to large databases,
including DBSCAN [17], BIRCH [50], and ScaleKM [10]. For
further information on clustering and clustering algorithms,
see [34], [11], [28], [30], [29].
Among clustering formulations that are based on
minimizing a formal objective function, perhaps the most
widely used and studied is k-means clustering. Given a set
of n data points in real d-dimensional space, Rd, and an
integer k, the problem is to determine a set of k points in Rd,
called centers, so as to minimize the mean squared distance
from each data point to its nearest center. This measure is
often called the squared-error distortion [28], [24] and this
type of clustering falls into the general category of variance-
based clustering [27], [26].
Clustering based on k-means is closely related to a
number of other clustering and location problems. These
include the Euclidean k-medians (or the multisource Weber
problem) [3], [36] in which the objective is to minimize the
sum of distances to the nearest center and the geometric
k-center problem [1] in which the objective is to minimize
the maximum distance from every point to its closest center.
There are no efficient solutions known to any of these
problems and some formulations are NP-hard [23]. An
asymptotically efficient approximation for the k-means
clustering problem has been presented by Matousek [41],
but the large constant factors suggest that it is not a good
candidate for practical implementation.
One of the most popular heuristics for solving the k-means
problem is based on a simple iterative scheme for finding a
locally minimal solution. This algorithm is often called the
k-means algorithm [21], [38]. There are a number of variants
to this algorithm, so, to clarify which version we are using, we
will refer to it as Lloyd's algorithm. (More accurately, it should
be called the generalized Lloyd's algorithm since Lloyd's
original result was for scalar data [37].)
Lloyd's algorithm is based on the simple observation that
the optimal placement of a center is at the centroid of the
associated cluster (see [18], [15]). Given any set of k centersZ,
for each center z 2 Z, let V Â…zÂ† denote its neighborhood, that is,
the set of data points for which z is the nearest neighbor. In
geometric terminology, V Â…zÂ† is the set of data points lying in
the Voronoi cell of z [48]. Each stage of Lloyd's algorithm
moves every center point z to the centroid of V Â…zÂ† and then
updates V Â…zÂ† by recomputing the distance from each point to
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 24, NO. 7, JULY 2002 881
. T. Kanungo is with the IBM Almaden Research Center, 650 Harry Road,
San Jose, CA 95120. E-mail: kanungo@almaden.ibm.com.
. D.M. Mount is with the Department of Computer Science, University of
Maryland, College Park, MD 20742. E-mail: mount@cs.umd.edu.
. N.S. Netanyahu is with the Department of Mathematics and Computer
Science, Bar-Ilan University, Ramat-Gan, Israel.
E-mail: nathan@cs.biu.ac.il.
. C.D. Piatko is with the Applied Physics Laboratory, The John Hopkins
University, Laurel, MD 20723. E-mail: christine.piatko@jhuapl.edu.
. R. Silverman is with the Center for Automation Research, University of
Maryland, College Park, MD 20742. E-mail: ruth@cfar.umd.edu.
. A.Y. Wu is with the Department of Computer Science and Information
Systems, American University, Washington, DC 20016.
E-mail: awu@american.edu.
Manuscript received 1 Mar. 2000; revised 6 Mar. 2001; accepted 24 Oct.
2001.
Recommended for acceptance by C. Brodley.
For information on obtaining reprints of this article, please send e-mail to:
tpami@computer.org, and reference IEEECS Log Number 111599.
0162-8828/02/$17.00 ÃŸ 2002 IEEE
its nearest center. These steps are repeated until some
convergence condition is met. See Faber [18] for descriptions
of other variants of this algorithm. For points in general
position (in particular, if no data point is equidistant from two
centers), the algorithm will eventually converge to a point
that is a local minimum for the distortion. However, the result
is not necessarily a global minimum. See [8], [40], [47], [49] for
further discussion of its statistical and convergence proper-
ties. Lloyd's algorithm assumes that the data are memory
resident. Bradley et al. [10] have shown how to scale k-means
clustering to very large data sets through sampling and
pruning. Note that Lloyd's algorithm does not specify the
initial placement of centers. See Bradley and Fayyad [9], for
example, for further discussion of this issue.
Because of its simplicity and flexibility, Lloyd's algorithm
is very popular in statistical analysis. In particular, given any
other clustering algorithm, Lloyd's algorithm can be applied
as a postprocessing stage to improve the final distortion. As
we shall see in our experiments, this can result in significant
improvements. However, a straightforward implementation
of Lloyd's algorithm can be quite slow. This is principally due
to the cost of computing nearest neighbors.
In this paper, we present a simple and efficient
implementation of Lloyd's algorithm, which we call the
filtering algorithm. This algorithm begins by storing the data
points in a kd-tree [7]. Recall that, in each stage of Lloyd's
algorithm, the nearest center to each data point is computed
and each center is moved to the centroid of the associated
neighbors. The idea is to maintain, for each node of the tree,
a subset of candidate centers. The candidates for each node
are pruned, or Âªfiltered,Âº as they are propagated to the
node's children. Since the kd-tree is computed for the data
points rather than for the centers, there is no need to update
this structure with each stage of Lloyd's algorithm. Also,
since there are typically many more data points than
centers, there are greater economies of scale to be realized.
Note that this is not a new clustering method, but simply an
efficient implementation of Lloyd's k-means algorithm.
The idea of storing the data points in a kd-tree in clustering
was considered by Moore [42] in the context of estimating the
parameters of a mixture of Gaussian clusters. He gave an
efficient implementation of the well-known EM algorithm.
The application of this idea to k-means was discovered
independently by Alsabti et al. [2], Pelleg and Moore [45], [46]
(who called their version the blacklisting algorithm), and
Kanungo et al. [31]. The purpose of this paper is to present a
more detailed analysis of this algorithm. In particular, we
present a theorem that quantifies the algorithm's efficiency
when the data are naturally clustered and we present a
detailed series of experiments designed to advance the
understanding of the algorithm's performance.
In Section 3, we present a data-sensitive analysis which
shows that, as the separation between clusters increases, the
algorithm runs more efficiently. We have also performed a
number of empirical studies, both on synthetically generated
data and on real data used in applications ranging from color
quantization to data compression to image segmentation.
These studies, as well as a comparison we ran against the
popular clustering scheme, BIRCH1 [50], are reported in
Section 4. Our experiments show that the filtering algorithm is
quite efficient even when the clusters are not well-separated.
2 THE FILTERING ALGORITHM
In this section, we describe the filtering algorithm. As
mentioned earlier, the algorithm is based on storing the
multidimensional data points in a kd-tree [7]. For complete-
ness, we summarize the basic elements of this data structure.
Define a box to be an axis-aligned hyper-rectangle. The
bounding box of a point set is the smallest box containing all the
points. A kd-tree is a binary tree, which represents a
hierarchical subdivision of the point set's bounding box
using axis aligned splitting hyperplanes. Each node of the
kd-tree is associated with a closed box, called a cell. The root's
cell is the bounding box of the point set. If the cell contains at
most one point (or, more generally, fewer than some small
constant), then it is declared to be a leaf. Otherwise, it is split
into two hyperrectangles by an axis-orthogonal hyperplane.
The points of the cell are then partitioned to one side or the
other of this hyperplane. (Points lying on the hyperplane can
be placed on either side.) The resulting subcells are the
children of the original cell, thus leading to a binary tree
structure. There are a number of ways to select the splitting
hyperplane. One simple way is to split orthogonally to the
longest side of the cell through the median coordinate of the
associated points [7]. Givennpoints, this produces a tree with
OÂ…nÂ† nodes and OÂ…lognÂ† depth.
We begin by computing a kd-tree for the given data
points. For each internal node u in the tree, we compute the
number of associated data points u:count and weighted
centroid u:wgtCent, which is defined to be the vector sum of
all the associated points. The actual centroid is just
u:wgtCent=u:count. It is easy to modify the kd-tree con-
struction to compute this additional information in the
same space and time bounds given above. The initial
centers can be chosen by any method desired. (Lloyd's
algorithm does not specify how they are to be selected. A
common method is to sample the centers at random from
the data points.) Recall that, for each stage of Lloyd's
algorithm, for each of the k centers, we need to compute the
centroid of the set of data points for which this center is
closest. We then move this center to the computed centroid
and proceed to the next stage.
For each node of the kd-tree, we maintain a set of candidate
centers. This is defined to be a subset of center points that
might serve as the nearest neighbor for some point lying
within the associated cell. The candidate centers for the root
consist of all k centers. We then propagate candidates down
the tree as follows: For each nodeu, letC denote its cell and let
Z denote its candidate set. First, compute the candidate z 2 Z
that is closest to the midpoint of C. Then, for each of the
remaining candidates z 2 Znfzg, if no part of C is closer to z
than it is to z, we can infer that z is not the nearest center to
any data point associated with u and, hence, we can prune, or
Âªfilter,Âº z from the list of candidates. If u is associated with a
single candidate (which must be z) then z is the nearest
neighbor of all its data points. We can assign them to z by
adding the associated weighted centroid and counts to z.
Otherwise, if u is an internal node, we recurse on its children.
If u is a leaf node, we compute the distances from its
associated data point to all the candidates inZ and assign the
data point to its nearest center. (See Fig. 1.)
It remains to describe how to determine whether there is
any part of cell C that is closer to candidate z than to z. Let
H be the hyperplane bisecting the line segment zz. (See
Fig. 2.) H defines two halfspaces; one that is closer to z and
882 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 24, NO. 7, JULY 2002
1. Balanced Iterative Reducing and Clustering using Hierarchies.
the other to z. If C lies entirely to one side of H, then it
must lie on the side that is closer to z (since C's midpoint is
closer to z) and so z may be pruned. To determine which is
the case, consider the vector ~u Âˆ zÃ¿ z, directed from z to
z. Let vÂ…HÂ† denote the vertex of C that is extreme in this
direction, that is, the vertex of C that maximizes the dot
product Â…vÂ…HÂ† ~uÂ†. Thus, z is pruned if and only if
distÂ…z; vÂ…HÂ†Â†  distÂ…z; vÂ…HÂ†Â†. (Squared distances may be
used to avoid taking square roots.) To compute vÂ…HÂ†, let
Â‰Cmini ; Cmaxi ÂŠ denote the projection of C onto the ith
coordinate axis. We take the ith coordinate of vÂ…HÂ† to be
Cmini if the ith coordinate of ~u is negative and C
max
i
otherwise. This computation is implemented by a proce-
dure z:isFartherÂ…z; CÂ†, which returns true if every part of
C is farther from z than to z.
The initial call is to FilterÂ…r; Z0Â†, where r is the root of the
tree and Z0 is the current set of centers. On termination,
center z is moved to the centroid of its associated points,
that is, z z:wgtCent=z:count.
Our implementation differs somewhat from those of
Alsabti et al. [2] and Pelleg and Moore [45]. Alsabti et al.'s
implementation of the filtering algorithm uses a less effective
pruning method based on computing the minimum and
maximum distances to each cell, as opposed to the bisecting
hyperplane criterion. Pelleg and Moore's implementation
uses the bisecting hyperplane, but they define z (called the
owner) to be the candidate that minimizes the distance to the
cell rather than the midpoint of the cell. Our approach has the
advantage that if two candidates lie within the cell, it will
select the candidate that is closer to the cell's midpoint.
3 DATA SENSITIVE ANALYSIS
In this section, we present an analysis of the time spent in each
stage of the filtering algorithm. Traditional worst-case
analysis is not really appropriate here since, in principle,
the algorithm might encounter scenarios in which it
degenerates to brute-force search. This happens, for example,
if the center points are all located on a unit sphere centered at
the origin and the data points are clustered tightly around the
origin. Because the centers are nearly equidistant to any
subset of data points, very little pruning takes place and the
algorithm degenerates to a brute force OÂ…knÂ† search. Of
course, this is an absurdly contrived scenario. In this section,
we will analyze the algorithm's running time not only as a
function ofkandn, but as a function of the degree to which the
data set consists of well-separated clusters. This sort of
approach has been applied recently by Dasgupta [13] and
Dasgupta and Shulman [14] in the context of clustering data
sets that are generated by mixtures of Gaussians. In contrast,
our analysis does not rely on any assumptions regarding
Gaussian distributions.
For the purposes of our theoretical results, we will need a
data structure with stronger geometric properties than the
simple kd-tree. Consider the basic kd-tree data structure
described in the previous section. We define the aspect ratio of
a cell to be the ratio of the length of its longest side to its
shortest side. The size of a cell is the length of its longest side.
The cells of a kd-tree may generally have arbitrarily high
aspect ratio. For our analysis, we will assume that, rather than
a kd-tree, the data points are stored in a structure called a
balanced box-decomposition tree (or BBD-tree) for the point set
[5]. The following two lemmas summarize the relevant
properties of the BBD-tree. (See [5] for proofs.)
Lemma 1. Given a set of n data points P in Rd and a bounding
hypercube C for the points, in OÂ…dn lognÂ† time it is possible to
construct a BBD-tree representing a hierarchical decomposi-
tion of C into cells of complexity OÂ…dÂ† such that:
1. The tree has OÂ…nÂ† nodes and depth OÂ…lognÂ†.
2. The cells have bounded aspect ratio and, with every
2d levels of descent in the tree, the sizes of the
associated cells decrease by at least a factor of 1=2.
Lemma 2 (Packing Constraint). Consider any set C of cells of
the BBD-tree with pairwise disjoint interiors, each of size at
least s, that intersect a ball of radius r. The size of such a set is,
at most, 1Â‡ d4rs e
Ã¿ d
.
Our analysis is motivated by the observation that a great
deal of the running time of Lloyd's algorithm is spent in the
later stages of the algorithm when the center points are close
to their final locations but the algorithm has not yet converged
[25]. The analysis is based on the assumption that the data set
KANUNGO ET AL.: AN EFFICIENT K-MEANS CLUSTERING ALGORITHM: ANALYSIS AND IMPLEMENTATION 883
Fig. 1. The filtering algorithm.
Fig. 2. Candidate z is pruned because C lies entirely on one side of the
bisecting hyperplane H.
can indeed be clustered into k natural clusters and that the
current centers are located close to the true cluster centers.
These are admittedly strong assumptions, but our experi-
mental results will bear out the algorithm's efficiency even
when these assumptions are not met.
We say that a node is visited if the Filter procedure is
invoked on it. An internal node is expanded if its children are
visited. A nonexpanded internal node or a leaf node is a
terminal node. A nonleaf node is terminal if there is no center
z that is closer to any part of the associated cell than the
closest center z. Note that, if a nonleaf cell intersects the
Voronoi diagram (that is, there is no center that is closest to
every part of the cell), then it cannot be a terminal node. For
example, in Fig. 3, the node with cell a is terminal because it
lies entirely within the Voronoi cell of its nearest center.
Cell b is terminal because it is a leaf. However, cell c is not
terminal because it is not a leaf and it intersects the Voronoi
diagram. Observe that, in this rather typical example, if the
clusters are well-separated and the center points are close to
the true cluster centers, then a relatively small fraction of
the data set lies near the edges of the Voronoi diagram of
the centers. The more well-separated the clusters, the
smaller this fraction will be and, hence, fewer cells of the
search structure will need to be visited by the algorithm.
Our analysis formalizes this intuition.
We assume that the data points are generated indepen-
dently from k different multivariate distributions in Rd.
Consider any one such distribution. Let X Âˆ Â…x1; x2; . . . ; xdÂ†
denote a random vector from this distribution. Let  2 Rd
denote the mean point of this distribution and let  denote
the d d covariance matrix for the distribution [22]
 Âˆ EÂ…Â…XÃ¿ Â†Â…XÃ¿ Â†T Â†:
Observe that the diagonal elements of  are the variances of
the random variables that are associated, respectively, with
the individual coordinates. Let trÂ…Â† denote the trace of ,
that is, the sum of its diagonal elements. We will measure
the dispersion of the distribution by the variable  Âˆ ÂÂÂÂÂÂÂÂÂÂÂtrÂ…Â†p .
This is a natural generalization of the notion of standard
deviation for a univariate distribution.
We will characterize the degree of separation of the
clusters by two parameters. Let Â…iÂ† and Â…iÂ† denote,
respectively, the mean and dispersion of the ith cluster
distribution. Let
rmin Âˆ 1
2
min
i6Âˆj
jÂ…iÂ† Ã¿ Â…jÂ†j and max Âˆ max
i
Â…iÂ†;
where jq Ã¿ pj denotes the Euclidean distance between
points q and p. The former quantity is half the minimum
distance between any two cluster centers and the latter is
the maximum dispersion. Intuitively, in well-separated
clusters, rmin is large relative to max. We define the cluster
separation of the point distribution to be
 Âˆ rmin
max
:
This is similar to Dasgupta's notion of pairwise c-separated
clusters, where c Âˆ 2 [13]. It is also similar to the separation
measure for cluster i of Coggins and Jain [12], defined to be
minj 6Âˆi jÂ…iÂ† Ã¿ Â…jÂ†j=Â…iÂ†.
We will show that, assuming that the candidate centers
are relatively close to the cluster means, Â…iÂ†, then, as 
increases (i.e., as clusters are more well-separated) the
algorithm's running time improves. Our proof makes use of
the following straightforward generalization of Cheby-
shev's inequality (see [20]) to multivariate distributions.
The proof proceeds by applying Chebyshev's inequality
to each coordinate and summing the results over all
d coordinates.
Lemma 3. Let X be a random vector in Rd drawn from a
distribution with mean  and dispersion  (the square root of
the trace of the covariance matrix). Then, for all positive t,
Pr jXÃ¿ j > tÂ… Â†  d
t2
:
For   0, we say that a set of candidates is -close with
respect to a given set of clusters if, for each center cÂ…iÂ†, there is
an associated cluster mean Â…iÂ† within distance at most rmin
and vice versa. Here is the main result of this section. Recall
that a node is visited if the Filter procedure is invoked on it.
Theorem 1. Consider a set of n points in Rd drawn from a
collection of cluster distributions with cluster separation
 Âˆ rmin=max, and consider a set of k candidate centers that
are -close to the cluster means, for some  < 1. Then, for any
, 0 <  < 1Ã¿ , and for some constant c, the expected number
of nodes visited by the filtering algorithm is
O k
c
ÂÂÂ
d
p

 !d
Â‡2dk lognÂ‡ dn
2Â…1Ã¿ Â†2
0@ 1A:
Before giving the proof, let us make a few observations
about this result. The result provides an upper bound the
number of nodes visited. The time needed to process each
node is proportional to the number of candidates for the
node, which is at most k and is typically much smaller.
Thus, the total running time of the algorithm is larger by at
most a factor of k. Also, the total running time includes an
additive contribution of OÂ…dn lognÂ† time needed to build
the initial BBD-tree.
884 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 24, NO. 7, JULY 2002
Fig. 3. Filtering algorithm analysis. Note that the density of points near
the edges of the Voronoi diagram is relatively low.
We note that, for fixed d, , and  (bounded away from 0
and 1Ã¿ ), the number of nodes visited as a function of n is
OÂ…nÂ† (since the last term in the analysis dominates). Thus, the
overall running time is OÂ…knÂ†, which seems to be no better
than the brute-force algorithm. The important observation,
however, is that, as cluster separation  increases, the
OÂ…kn=2Â† running time decreases and the algorithm's
efficiency increases rapidly. The actual expected number of
cells visited may be much smaller for particular distributions.
Proof. Consider the ith cluster. Let cÂ…iÂ† denote a candidate
center that is within distance rmin from its mean 
Â…iÂ†. Let
Â…iÂ† denote its dispersion. Let BÂ…iÂ† denote a ball of radius
rmin centered at 
Â…iÂ†. Observe that, because no cluster
centers are closer than 2rmin, these balls have pairwise
disjoint interiors. Let bÂ…iÂ† denote a ball of radius rminÂ…1Ã¿
Ã¿ Â† centered at cÂ…iÂ†. Because cÂ…iÂ† is -close to i, bÂ…iÂ† is
contained within a ball of radius rminÂ…1Ã¿ Â† centered at Â…iÂ†
and, hence, is contained withinBÂ…iÂ†. Moreover, the distance
between the boundaries of BÂ…iÂ† and bÂ…iÂ† is at least rmin.
Consider a visited node and let C denote its
associated cell in the BBD-tree. We consider two cases.
First, suppose that, for some cluster i, C \ bÂ…iÂ† 6Âˆ ;. We
call C a close node. Otherwise, if the cell does not intersect
any of the bÂ…iÂ† balls, it is a distant node. (See Fig. 4.) The
intuition behind the proof is as follows: If clustering is
strong, then we would expect only a few points to be far
from the cluster centers and, hence, there are few distant
nodes. If a node is close to its center and it is not too
large, then there is only one candidate for the nearest
neighbor and, hence, the node will be terminal. Because
cells of the BBD tree have bounded aspect ratio, the
number of large cells that can overlap a ball is bounded.
First, we bound the number of distant visited nodes.
Consider the subtree of the BBD-tree induced by these
nodes. If a node is distant, then its children are both distant,
implying that this induced subtree is a full binary tree (that
is, each nonleaf node has exactly two children). It is well-
known that the total number of nonleaf nodes in a full
binary tree is not greater than the number of leaves and,
hence, it suffices to bound the number of leaves.
The data points associated with all the leaves of the
induced subtree cannot exceed the number of data points
that lie outside of bÂ…iÂ†. As observed above, for any cluster i, a
data point of this cluster that lies outside of bÂ…iÂ† is at distance
from Â…iÂ† of at least
rminÂ…1Ã¿ Â† Âˆ rmin
Â…iÂ†
Â…1Ã¿ Â†Â…iÂ†  Â…1Ã¿ Â†Â…iÂ†:
By Lemma 3, the probability of this occurring is at most
d=Â…Â…1Ã¿ Â†Â†2. Thus, the expected number of such data
points is at most dn=Â…Â…1Ã¿ Â†Â†2. It follows from standard
results on binary tree that the number of distant of nodes
is at most twice as large.
Next, we bound the number of close visited nodes.
Recall a visited node is said to be expanded if the algorithm
visits its children. Clearly, the number of close visited
nodes is proportional to the number of close expanded
nodes. For each cluster i, consider the leaves of the induced
subtree consisting of close expanded nodes that intersect
bÂ…iÂ†. (The total number of close expanded nodes will be
larger by a factor ofk.) To bound the number of such nodes,
we further classify them by the size of the associated cell.
An expanded node vwhose size is at least 4rmin is large and,
otherwise, it is small. We will show that the number of large
expanded nodes is bounded byOÂ…2d lognÂ†and the number
of small expanded nodes is bounded by OÂ…Â…c ÂÂÂdp =Â†dÂ†, for
some constant c.
We first show the bound on the number of large
expanded nodes. In the descent through the BBD-tree, the
sizes of the nodes decrease monotonically. Consider the set
of all expanded nodes of size greater than 4rmin. These
nodes induce a subtree in the BBD-tree. Let L denote the
leaves of this tree. The cells associated with the elements of
Lhavepairwisedisjoint interiorsandtheyintersectbÂ…iÂ† and,
hence, they intersectBÂ…iÂ†. It follows from Lemma 2 (applied
toBÂ…iÂ† and the cells associated withL) that there are at most
Â…1Â‡ d4rmin=Â…4rminÂ†eÂ†d Âˆ 2d such cells. By Lemma 1, the
depth of the tree isOÂ…lognÂ† and, hence, the total number of
expanded large nodes isOÂ…2d lognÂ†, as desired.
Finally, we consider the small expanded nodes. We
assert that the diameter of the cell corresponding to any
such node is at least rmin. If not, then this cell would lie
entirely within a ball of radius rminÂ…1Ã¿ Â† of cÂ…iÂ†. Since the
cell was expanded, we know that there must be a point
in this cell that is closer to some other center cj. This
implies that the distance between cÂ…iÂ† and cÂ…jÂ† is less than
2rminÂ…1Ã¿ Â†. Since the candidates are -close, it follows
that there are two cluster means Â…iÂ† and Â…jÂ† that are
closer than 2rminÂ…1Ã¿ Â† Â‡ 2rmin Âˆ 2rmin. However, this
would contradict the definition of rmin. A cell in
dimension d of diameter x has longest side length of at
least x=
ÂÂÂ
d
p
. Thus, the size of each such cell is at least
rmin=
ÂÂÂ
d
p
. By Lemma 2, it follows that the number of such
cells that overlap BÂ…iÂ† is at most
c
ÂÂÂ
d
p

 !d
:
Applying a similar analysisused byArya andMount [4] for
approximate range searching, the number of expanded
nodes is asymptotically the same. This completes the
proof. tu
4 EMPIRICAL ANALYSIS
To establish the practical efficiency of the filtering algorithm,
we implemented it and tested its performance on a number of
KANUNGO ET AL.: AN EFFICIENT K-MEANS CLUSTERING ALGORITHM: ANALYSIS AND IMPLEMENTATION 885
Fig. 4. Distant and close nodes.
data sets. These included both synthetically generated data
and data used in real applications. The algorithm was
implemented in C++ using the g++ compiler and was run
on a Sun Ultra 5 running Solaris 2.6. The data structure
implemented was a kd-tree that was constructed from the
ANN library [43] using the sliding midpoint rule [39]. This
decomposition method was chosen because our studies have
shown that it performs better than the standard kd-tree
decomposition rule for clustered data sets. Essentially, we
performed two different types of experiments. The first type
compared running times of the filtering algorithm against
two different implementations of Lloyd's algorithm. The
second type compared cluster distortions obtained for the
filtering algorithm with those obtained for the BIRCH
clustering scheme [50].
For the running time experiments, we considered two
other algorithms. Recall that the essential task is to compute
the closest center to each data point. The first algorithm
compared against was a simple brute-force algorithm, which
computes the distance from every data point to every center.
The second algorithm, called kd-center, operates by building a
kd-tree with respect to the center points and then uses the
kd-tree to compute the nearest neighbor for each data point.
The kd-tree is rebuilt at the start of each stage of Lloyd's
algorithm. We compared these two methods against the
filtering algorithm by performing two sets of experiments,
one involving synthetic data and the other using data derived
from applications in image segmentation and compression.
We used the following experimental structure for the
above experiments: For consistency, we ran all three algo-
rithms on the same data set and the same initial placement of
centers. Because the running time of the algorithm depends
heavily on the number of stages, we report the average
running time per stage by computing the total running time
and then dividing by the number of stages. In the case of the
filtering algorithm, we distinguished between two cases
according to whether or not the preprocessing time was taken
into consideration. The reason for excluding preprocessing
time is that, when the number of stages of Lloyd's algorithm is
very small, the effect of the preprocessing time is amortized
over a smaller number of stages and this introduces a bias.
We measured running time in two different ways. We
measured both the CPU time (using the standard clock()
function) and a second quantity called the number of node-
candidate pairs. The latter quantity is a machine-independent
statistic of the algorithm's complexity. Intuitively, it
measures the number of interactions between a node of
the kd-tree (or data point in the case of brute force) and a
candidate center. For the brute-force algorithm, this
quantity is always kn. For the kd-center algorithm, for each
data point, we count the number of nodes that were
accessed in the kd-tree of the centers for computing its
nearest center and sum this over all data points. For the
filtering algorithm, we computed a sum of the number of
candidates associated with every visited node of the tree. In
particular, for each call to FilterÂ…u; ZÂ†, the cardinality of the
set Z of candidate centers is accumulated. The one-time cost
of building the kd-tree is not included in this measure.
Note that the three algorithms are functionally equiva-
lent if points are in general position. Thus, they all produce
the same final result and, so, there is no issue regarding the
quality of the final output. The primary issue of interest is
the efficiency of the computation.
4.1 Synthetic Data
We ran three experiments to determine the variations in
running time as a function of cluster separation, data set size,
and dimension. The first experiment tested the validity of
Theorem 1. We generatedn Âˆ 10; 000 data points in R3. These
points were distributed evenly among 50 clusters as follows:
The 50 cluster centers were sampled from a uniform
distribution over the hypercube Â‰Ã¿1; 1ÂŠd. A Gaussian dis-
tribution was then generated around each center, where each
coordinate was generated independently from a univariate
Gaussian with a given standard deviation. The standard
deviation varied from 0.01 (very well-separated) up to 0.7
(virtually unclustered). Because the same distribution was
used for cluster centers throughout, the expected distances
between cluster centers remained constant. Thus, the
expected value of the cluster separation parameter  varied
inversely with the standard deviation. The initial centers
were chosen by taking a random sample of data points.
For each standard deviation, we ran each of the
algorithms (brute-force, kd-center, and filter) three times.
For each of the three runs, a new set of initial center points
was generated and all three algorithms were run using the
same data and initial center points. The algorithm ran for a
maximum of 30 stages or until convergence.
The average CPU times per stage, for all three methods,
are shown for k Âˆ 50 in Fig. 5a and for k Âˆ 20 in Fig. 6a. The
results of these experiments show that the filtering
algorithm runs significantly faster than the other two
algorithms. Ignoring the bias introduced by preprocessing,
its running time improves when the clusters are more well-
separated (for smaller standard deviations). The improve-
ment in CPU time predicted by Theorem 1 is not really
evident for very small standard deviations because initi-
alization and preprocessing costs dominate. However, this
improvement is indicated in Figs. 5b and 6b, which plot the
(initialization independent) numbers of node-candidate
pairs. The numbers of node candidate pairs for the other
two methods are not shown, but varied from 4 to 10 times
greater than those of the filtering algorithm.
Our theoretical analysis does not predict that the filtering
algorithm will be particularly efficient for unclustered data
sets, but it does not preclude this possibility. Nonetheless,
we observe in this experiment that the filtering algorithm
ran significantly faster than the brute-force and kd-center
algorithms, even for large standard deviations. This can be
attributed, in part, to the fact that the filtering algorithm
simply does a better job in exploiting economies of scale by
storing the much larger set of data points in a kd-tree
(rather than center points as kd-center does).
The objective of the second experiment was to study the
effects of data size on the running time. We generated data
sets of points whose size varied fromn Âˆ 1; 000 ton Âˆ 20; 000
and where each data set consisted of 50 Gaussian clusters. The
standard deviation was fixed at 0.10 and the algorithms were
run with k Âˆ 50. Again, the searches were terminated when
stability was achieved or after 30 stages. As before, we ran
each case three times with different starting centers and
averaged the results. The CPU times per stage and number of
886 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 24, NO. 7, JULY 2002
node-candidate pairs for all three methods are shown in Fig. 7.
The results of this experiment show that, for fixed k and large
n, all three algorithms have running times that vary linearly
with n, with the filtering algorithm doing the best.
The third experiment was designed to test the effect of the
dimension on running times of the three k-means algorithms.
Theorem 1 predicts that the filtering algorithm's running time
increases exponentially with dimension, whereas that of the
brute-force algorithm (whose running time is OÂ…dknÂ†)
increases only linearly with dimension d. Thus, as the
dimension increases, we expect that the filtering algorithm's
speed advantage would tend to diminish. This exponential
dependence on dimension seems to be characteristic of many
algorithms that are based on kd-trees and many variants, such
as R-trees.
We repeated an experimental design similar to the one
used by Pelleg and Moore [45]. For each dimension d, we
created a random data set with 20,000 points. The points were
sampled from a clustered Gaussian distribution with
72 clusters and a standard deviation of 0:05 along each
coordinate. (The standard deviation is twice that used by
Pelleg and Moore because they used a unit hypercube and our
hypercube has side length 2.) The three k-means algorithms
were run with k Âˆ 40 centers. In Fig. 8, we plot the average
times taken per stage for the three algorithms. We found that,
for this setup, the filtering algorithm outperforms the brute-
force and kd-center algorithms for dimensions ranging up to
the mid 20s. These results confirm the general trend reported
in [45], but the filtering algorithm's performance in moderate
dimensions 10 to 20 is considerably better.
KANUNGO ET AL.: AN EFFICIENT K-MEANS CLUSTERING ALGORITHM: ANALYSIS AND IMPLEMENTATION 887
Fig. 6. Average CPU times and node-candidate pairs per stage versus cluster standard deviation for n Âˆ 10; 000, k Âˆ 20.
Fig. 5. Average CPU times and node-candidate pairs per stage versus cluster standard deviation for n Âˆ 10; 000, k Âˆ 50.
4.2 Real Data
To understand the relative efficiency of this algorithm
under more practical circumstances, we ran a number of
experiments on data sets derived from actual applications
in image processing, compression, and segmentation. Each
experiment involved solving a k-means problem on a set of
points in Rd for various ds. In each case, we applied all
three algorithms: brute force (Brute), kd-center (KDcenter),
and filtering (Filter). In the filtering algorithm, we included
the preprocessing time in the average. In each case, we
computed the average CPU time per stage in seconds (T-)
and the average number of node-candidate pairs (NC-).
This was repeated for values of k chosen from f8; 64; 256g.
The results are shown in Table 1.
The experiments are grouped into three general cate-
gories. The first involves a color quantization application. A
color image is given and a number of pixels are sampled
from the image (10,000 for this experiment). Each pixel is
represented as a triple consisting of red, green, and blue
components, each in the range Â‰0; 255ÂŠ and, hence, is a point
in R3. The input images were chosen from a number of
standard images for this application. The images are shown
in Fig. 9 and the running time results are given in the upper
half of Table 1, opposite the corresponding image names
Âªballs,Âº Âªkiss,Âº . . . , Âªball1_s.Âº
The next experiment involved a vector quantization
application for data compression. Here, the images are
gray-scale images with pixel values in the range Â‰0; 255ÂŠ.
Each 2 2 subarray of pixels is selected and mapped to a
4-element vector and the k-means algorithm is run on the
resulting set of vectors. The images are shown in Fig. 10 and
the results are given in Table 1, opposite the names
Âªcouple,Âº Âªcrowd,Âº . . . ; Âªwoman2.Âº
The final experiment involved image segmentation. A
512 512 Landsat image of Israel consisting of four spectral
bands was used. The resulting 4-element vectors in the
range Â‰0; 255ÂŠ were presented to the algorithms. One of the
image bands is shown in Fig. 11 and the results are
provided in Table 1, opposite the name ÂªIsrael.Âº
An inspection of the results reveals that the filtering
algorithm significantly outperformed the other two algo-
rithms in all cases. Plots of the underlying point distributions
showed that most of these data sets were really not well-
clustered. Thus, the filtering algorithm is quite efficient, even
when the conditions of Theorem 1 are not satisfied.
4.3 Comparison with BIRCH
A comparison between our filtering algorithm and the
BIRCH clustering scheme [50] is presented in Table 2. The
table shows the distortions produced by both algorithms.
The column ÂªBIRCHÂº reports the distortions obtained with
the BIRCH software. The column labeled ÂªFilterÂº provides
the distortions obtained due to the k-means filtering
algorithm (with centers initially sampled at random from
the data points). Alternatively, one can use the centers
obtained by BIRCH as initial centers for our filtering
algorithm. This is shown in the column labeled ÂªCom-
binedÂº of the table. As can be seen, the distortions are
always better and considerably better in some of the cases.
(This is because k-means cannot increase distortions.)
Indeed, as was mentioned in Section 1, k-means may be
run to improve the distortion of any clustering algorithm.
888 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 24, NO. 7, JULY 2002
Fig. 7. Running times and node-candidate pairs versus data size for k Âˆ 50,  Âˆ 0:10.
Fig. 8. Average CPU times per stage versus dimension for n Âˆ 20; 000,
 Âˆ 0:05, and k Âˆ 40.
5 CONCLUDING REMARKS
We have presented an efficient implementation of Lloyd's
k-means clustering algorithm, called the filtering algorithm.
The algorithm is easy to implement and only requires that a
kd-tree be built once for the given data points. Efficiency is
achieved because the data points do not vary throughout the
computation and, hence, this data structure does not need to
be recomputed at each stage. Since there are typically many
more data points than ÂªqueryÂº points (i.e., centers), the
KANUNGO ET AL.: AN EFFICIENT K-MEANS CLUSTERING ALGORITHM: ANALYSIS AND IMPLEMENTATION 889
TABLE 1
Running Times for Various Test Inputs
relative advantage provided by preprocessing in the above
manner is greater. Our algorithm differs from existing
approaches only in how nearest centers are computed, so it
could be applied to the many variants of Lloyd's algorithm.
The algorithm has been implemented and the source
code is available on request. We have demonstrated the
practical efficiency of this algorithm both theoretically,
through a data sensitive analysis, and empirically, through
experiments on both synthetically generated and real data
sets. The data sensitive analysis shows that the more well-
separated the clusters, the faster the algorithm runs. This is
subject to the assumption that the center points are indeed
close to the cluster centers. Although this assumption is
rather strong, our empirical analysis on synthetic data
indicates that the algorithm's running time does improve
dramatically as cluster separation increases. These results
for both synthetic and real data sets indicate that the
filtering algorithm is significantly more efficient than the
other two methods that were tested. Further, they show that
the algorithm scales well up to moderately high dimensions
(from 10 to 20). Its performance in terms of distortions is
competitive with that of the well-known BIRCH software
and, by running k-means as a postprocessing step to BIRCH,
it is possible to improve distortions significantly.
A natural question is whether the filtering algorithm can
be improved. The most obvious source of inefficiency in the
algorithm is that it passes no information from one stage to
the next. Presumably, in the later stages of Lloyd's algorithm,
as the centers are converging to their final positions, one
would expect that the vast majority of the data points have the
same closest center from one stage to the next. A good
algorithm should exploit this coherence to improve the
running time. A kinetic method along these lines was
proposed in [31], but this algorithm is quite complex and
does not provide significantly faster running time in practice.
The development of a simple and practical algorithm which
combines the best elements of the kinetic and filtering
approaches would make a significant contribution.
ACKNOWLEDGMENTS
The authors would like to thank Azriel Rosenfeld for his
comments and Hao Li for his help in running the BIRCH
experiments. They also are grateful to the anonymous
referees for their many valuable suggestions. This research
was funded in part by the US National Science Foundation
under Grant CCR-0098151 and by the US Army Research
Laboratory and the US Department of Defense under
890 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 24, NO. 7, JULY 2002
Fig. 9. Sample of images in the color quantization data set. Each pixel in the RGB color images is represented by a triple with values in
the range [0, 255].
Fig. 10. Sample of images in the image compression dataset. Each pixel in these gray-scale images is represented by 8 bits. 2 2 nonoverlapping
blocks were used to create 4-element vectors.
Fig. 11. Images in satellite image data set. One of four spectral bands of
a Landsat image over Israel is shown. The image is 512 512 and each
pixel is represented by 8 bits.
Contract MDA9049-6C-1250. A preliminary version of this
paper appeared in the Proceedings of the 16th Annual ACM
Symposium on Computational Geometry, pp. 100-109, June 2000.
REFERENCES
[1] P.K. Agarwal and C.M. Procopiuc, ÂªExact and Approximation
Algorithms for Clustering,Âº Proc. Ninth Ann. ACM-SIAM Symp.
Discrete Algorithms, pp. 658-667, Jan. 1998.
[2] K. Alsabti, S. Ranka, and V. Singh, ÂªAn Efficient k-means
Clustering Algorithm,Âº Proc. First Workshop High Performance Data
Mining, Mar. 1998.
[3] S. Arora, P. Raghavan, and S. Rao, ÂªApproximation Schemes for
Euclidean k-median and Related Problems,Âº Proc. 30th Ann. ACM
Symp. Theory of Computing, pp. 106-113, May 1998.
[4] S. Arya and D. M. Mount, ÂªApproximate Range Searching,Âº
Computational Geometry: Theory and Applications, vol. 17, pp. 135-
163, 2000.
[5] S. Arya, D.M. Mount, N.S. Netanyahu, R. Silverman, and A.Y. Wu,
ÂªAn Optimal Algorithm for Approximate Nearest Neighbor
Searching,Âº J. ACM, vol. 45, pp. 891-923, 1998.
[6] G.H. Ball and D.J. Hall, ÂªSome Fundamental Concepts and
Synthesis Procedures for Pattern Recognition Preprocessors,Âº
Proc. Int'l Conf. Microwaves, Circuit Theory, and Information Theory,
Sept. 1964.
[7] J.L. Bentley, ÂªMultidimensional Binary Search Trees Used for
Associative Searching,Âº Comm. ACM, vol. 18, pp. 509-517, 1975.
[8] L. Bottou and Y. Bengio, ÂªConvergence Properties of the k-means
Algorithms,Âº Advances in Neural Information Processing Systems 7,
G. Tesauro and D. Touretzky, eds., pp. 585-592. MIT Press, 1995.
[9] P.S. Bradley and U. Fayyad, ÂªRefining Initial Points for K-means
Clustering,Âº Proc. 15th Int'l Conf. Machine Learning, pp. 91-99, 1998.
[10] P.S. Bradley, U. Fayyad, and C. Reina, ÂªScaling Clustering
Algorithms to Large Databases,Âº Proc. Fourth Int'l Conf. Knowledge
Discovery and Data Mining, pp. 9-15, 1998.
[11] V. Capoyleas, G. Rote, and G. Woeginger, ÂªGeometric Cluster-
ings,Âº J. Algorithms, vol. 12, pp. 341-356, 1991.
[12] J.M. Coggins and A.K. Jain, ÂªA Spatial Filtering Approach to
Texture Analysis,Âº Pattern Recognition Letters, vol. 3, pp. 195-203,
1985.
[13] S. Dasgupta, ÂªLearning Mixtures of Gaussians,Âº Proc. 40th IEEE
Symp. Foundations of Computer Science, pp. 634-644, Oct. 1999.
[14] S. Dasgupta and L.J. Shulman, ÂªA Two-Round Variant of EM for
Gaussian Mixtures,Âº Proc. 16th Conf. Uncertainty in Artificial
Intelligence (UAI-2000), pp. 152-159, June 2000.
[15] Q. Du, V. Faber, and M. Gunzburger, ÂªCentroidal Voronoi
Tesselations: Applications and Algorithms,Âº SIAM Rev., vol. 41,
pp. 637-676, 1999.
[16] R.O. Duda and P.E. Hart, Pattern Classification and Scene Analysis.
New York: John Wiley & Sons, 1973.
[17] M. Ester, H. Kriegel, and X. Xu, ÂªA Database Interface for
Clustering in Large Spatial Databases,Âº Proc. First Int'l Conf.
Knowledge Discovery and Data Mining (KDD-95), pp. 94-99, 1995.
[18] V. Faber, ÂªClustering and the Continuous k-means Algorithm,Âº Los
Alamos Science, vol. 22, pp. 138-144, 1994.
[19] U.M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy,
Advances in Knowledge Discovery and Data Mining. AAAI/MIT
Press, 1996.
[20] W. Feller, An Introduction to Probability Theory and Its Applications,
third ed. New York: John Wiley & Sons, 1968.
[21] E. Forgey, ÂªCluster Analysis of Multivariate Data: Efficiency vs.
Interpretability of Classification,Âº Biometrics, vol. 21, p. 768, 1965.
[22] K. Fukunaga, Introduction to Statistical Pattern Recognition. Boston:
Academic Press, 1990.
[23] M.R. Garey and D.S. Johnson, Computers and Intractability: A Guide
to the Theory of NP-Completeness. New York: W.H. Freeman, 1979.
[24] A. Gersho and R.M. Gray, Vector Quantization and Signal
Compression. Boston: Kluwer Academic, 1992.
[25] M. Inaba, private communication, 1997.
[26] M. Inaba, H. Imai, and N. Katoh, ÂªExperimental Results of a
Randomized Clustering Algorithm,Âº Proc. 12th Ann. ACM Symp.
Computational Geometry, pp. C1-C2, May 1996.
KANUNGO ET AL.: AN EFFICIENT K-MEANS CLUSTERING ALGORITHM: ANALYSIS AND IMPLEMENTATION 891
TABLE 2
Average Distortions for the Filtering Algorithm, BIRCH, and the Two Combined
[27] M. Inaba, N. Katoh, and H. Imai, ÂªApplications of Weighted
Voronoi Diagrams and Randomization to Variance-Based
k-clustering,Âº Proc. 10th Ann. ACM Symp. Computational Geometry,
pp. 332-339, June 1994.
[28] A.K. Jain and R.C. Dubes, Algorithms for Clustering Data. Engle-
wood Cliffs, N.J.: Prentice Hall, 1988.
[29] A.K. Jain, P.W. Duin, and J. Mao, ÂªStatistical Pattern Recognition:
A Review,Âº IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 22, no. 1, pp. 4-37, Jan. 2000.
[30] A.K. Jain, M.N. Murty, and P.J. Flynn, ÂªData Clustering: A
Review,Âº ACM Computing Surveys, vol. 31, no. 3, pp. 264-323, 1999.
[31] T. Kanungo, D.M. Mount, N.S. Netanyahu, C. Piatko, R. Silver-
man, and A.Y. Wu, ÂªComputing Nearest Neighbors for Moving
Points and Applications to Clustering,Âº Proc. 10th Ann. ACM-
SIAM Symp. Discrete Algorithms, pp. S931-S932, Jan. 1999.
[32] T. Kanungo, D.M. Mount, N.S. Netanyahu, C. Piatko, R. Silver-
man, and A.Y. Wu, ÂªThe Analysis of a Simple k-means Clustering
Algorithm,Âº Technical Report CAR-TR-937, Center for Automa-
tion Research, Univ. of Maryland, College Park, Jan. 2000.
[33] T. Kanungo, D.M. Mount, N.S. Netanyahu, C. Piatko, R. Silver-
man, and A.Y. Wu, ÂªThe Analysis of a Simple k-means Clustering
Algorithm,Âº Proc. 16th Ann. ACM Symp. Computational Geometry,
pp. 100-109, June 2000.
[34] L. Kaufman and P.J. Rousseeuw, Finding Groups in Data: An
Introduction to Cluster Analysis. New York: John Wiley & Sons,
1990.
[35] T. Kohonen, Self-Organization and Associative Memory, third ed.
New York: Springer-Verlag, 1989.
[36] S. Kolliopoulos and S. Rao, ÂªA Nearly Linear-Time Approxima-
tion Scheme for the Euclidean k-median Problem,Âº Proc. Seventh
Ann. European Symp. Algorithms, J. Nesetril, ed., pp. 362-371, July
1999.
[37] S. P. Lloyd, ÂªLeast Squares Quantization in PCM,Âº IEEE Trans.
Information Theory, vol. 28, 129-137, 1982.
[38] J. MacQueen, ÂªSome Methods for Classification and Analysis of
Multivariate Observations,Âº Proc. Fifth Berkeley Symp. Math.
Statistics and Probability, vol. 1, pp. 281-296, 1967.
[39] S. Maneewongvatana and D.M. Mount, ÂªAnalysis of Approximate
Nearest Neighbor Searching with Clustered Point Sets,Âº Proc.
Workshop Algorithm Eng. and Experiments (ALENEX '99), Jan. 1999.
Available from: http://ftp.cs.umd.edu-pub/faculty/mount/
Papers/dimacs99.ps.gz.
[40] O.L. Mangasarian, ÂªMathematical Programming in Data Mining,Âº
Data Mining and Knowledge Discovery, vol. 1, pp. 183-201, 1997.
[41] J. Matousek, ÂªOn Approximate Geometric k-clustering,Âº Discrete
and Computational Geometry, vol. 24, pp. 61-84, 2000.
[42] A. Moore, ÂªVery Fast EM-Based Mixture Model Clustering Using
Multiresolution kd-Trees,Âº Proc. Conf. Neural Information Processing
Systems, 1998.
[43] D.M. Mount and S. Arya, ÂªANN: A Library for Approximate
Nearest Neighbor Searching,Âº Proc. Center for Geometric Computing
Second Ann. Fall Workshop Computational Geometry, 1997. (available
from http://www.cs.umd.edu/~mount/ANN.)
[44] R.T. Ng and J. Han, ÂªEfficient and Effective Clustering Methods
for Spatial Data Mining,Âº Proc. 20th Int'l Conf. Very Large Databases,
pp. 144-155, Sept. 1994.
[45] D. Pelleg and A. Moore, ÂªAccelerating Exact k-means Algorithms
with Geometric Reasoning,Âº Proc. ACM SIGKDD Int'l Conf.
Knowledge Discovery and Data Mining, pp. 277-281, Aug. 1999.
[46] D. Pelleg and A. Moore, Âªx-means: Extending k-means with
Efficient Estimation of the Number of Clusters,Âº Proc. 17th Int'l
Conf. Machine Learning, July 2000.
[47] D. Pollard, ÂªA Centeral Limit Theorem for k-means Clustering,Âº
Annals of Probability, vol. 10, pp. 919-926, 1982.
[48] F.P. Preparata, M.I. Shamos, Computational Geometry: An Introduc-
tion, third ed. Springer-Verlag, 1990.
[49] S.Z. Selim and M.A. Ismail, ÂªK-means-type Algorithms: A
Generalized Convergence Theorem and Characterization of Local
Optimality,Âº IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 6, pp. 81-87, 1984.
[50] T. Zhang, R. Ramakrishnan, and M. Livny, ÂªBIRCH: A New Data
Clustering Algorithm and Its Applications,Âº Data Mining and
Knowledge Discovery, vol. 1, no. 2, pp. 141-182, 1997. (software
available from www.cs.wisc.edu/~vganti/birchcode).
Tapas Kanungo received the PhD degree from
the University of Washington, Seattle. Currently,
he is a research staff member at the IBM Almaden
Research Center, San Jose, California. Prior to
working at IBM, he served as a codirector of the
Language and Media Processing Lab at the
University of Maryland, College Park. His current
interests are in information extraction and retrie-
val and document analysis. He is senior member
of the IEEE and the IEEE Computer Society.
David M. Mount received the PhD degree from
Purdue University in computer science in 1983.
He is a professor in the Department of Computer
Science at the University of Maryland with a joint
appointment at the Institute for Advanced
Computer Studies. His primary research inter-
ests include the design, analysis, and imple-
mentation of data structures and algorithms for
geometric problems, particularly problems with
applications in image processing, pattern recog-
nition, information retrieval, and computer graphics. He is an associate
editor for Pattern Recognition. He is a member of the IEEE.
Nathan S. Netanyahu received BSc and MSc
degrees in electrical engineering from the
Technion, Israel Institute of Technology, and
the MSc and PhD degrees in computer science
from the University of Maryland, College Park.
He is currently a senior lecturer in the Depart-
ment of Mathematics and Computer Science at
Bar-Ilan University and is also affiliated with the
Center for Automation Research, University of
Maryland, College Park. Dr. Netanyahu's main
research interests are in the areas of algorithm design and analysis,
computational geometry, image processing, pattern recognition, remote
sensing, and robust statistical estimation. He currently serves as an
associate editor for Pattern Recognition. He is a member of the IEEE.
Christine D. Piatko received the BA degree in
computer science and mathematics from New
York University in 1986 and the MS and PhD
degrees from Cornell University in 1989 and
1993, respectively. She is currently a computer
science researcher at The Johns Hopkins
University Applied Physics Laboratory. Her
research interests include computational geo-
metry and information retrieval.
Ruth Silverman received the PhD degree in
mathematics from the University of Washington in
1970. She is a retired professor from the Depart-
ment of Computer Science at the University of the
District of Columbia and she is a visiting professor
at the Center for Automation Research at the
University of Maryland, College Park.
Angela Y. Wu received the PhD degree in
computer science from the University of Mary-
land at College Park in 1978. She is a professor
in the Department of Computer Science and
Information Systems, American University, Wa-
shington D.C., where she has taught since 1980.
She is a senior member of the IEEE.
. For more information on this or any other computing topic,
please visit our Digital Library at http://computer.org/publications/dlib.
892 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 24, NO. 7, JULY 2002

