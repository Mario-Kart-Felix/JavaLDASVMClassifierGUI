IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
1 
  
Abstract—Context-based Adaptive Binary Arithmetic Cod-
ing (CABAC) as a normative part of the new ITU-T | ISO/IEC 
standard H.264/AVC for video compression is presented. By 
combining an adaptive binary arithmetic coding technique 
with context modeling, a high degree of adaptation and redun-
dancy reduction is achieved. The CABAC framework also in-
cludes a novel low-complexity method for binary arithmetic 
coding and probability estimation that is well suited for effi-
cient hardware and software implementations. CABAC sig-
nificantly outperforms the baseline entropy coding method of 
H.264/AVC for the typical area of envisaged target applica-
tions. For a set of test sequences representing typical material 
used in broadcast applications and for a range of acceptable 
video quality of about 30 to 38 dB, average bit-rate savings of 9 
to 14% are achieved. 
 
Index Terms—CABAC, entropy coding, context modeling, 
binary arithmetic coding, H.264, MPEG-4 AVC. 
 
I. INTRODUCTION 
ATURAL camera-view video signals show non-
stationary statistical behavior. The statistics of these 
signals largely depend on the video content and the acquisi-
tion process. Traditional concepts of video coding that rely 
on a mapping from the video signal to a bitstream of vari-
able length-coded syntax elements exploit some of the non-
stationary characteristics but certainly not all of it. More-
over, higher-order statistical dependencies on a syntax ele-
ment level are mostly neglected in existing video coding 
schemes. Designing an entropy coding scheme for a video 
coder by taking into consideration these typically observed 
statistical properties, however, offers room for significant 
improvements in coding efficiency. 
Context-based Adaptive Binary Arithmetic Coding 
(CABAC) is one of the two entropy coding methods of the 
new ITU-T | ISO/IEC standard for video coding, 
H.264/AVC [1],[2]. The algorithm was first introduced in a 
rudimentary form in [7] and evolved over a period of suc-
cessive refinements [8]−[17]. In this paper, we present a de-
scription of the main elements of the CABAC algorithm in 
its final, standardized form as specified in [1]. Unlike the 
specification in [1], the presentation in this paper is in-
 
Manuscript received May 21, 2003.  
The authors are with the Fraunhofer Institute for Communications – 
Heinrich Hertz Institute, Berlin, Germany. 
 
tended to provide also some information on the underlying 
conceptual ideas as well as the theoretical and historical 
background of CABAC.  
Entropy coding in today’s hybrid block-based video cod-
ing standards such as MPEG-2 [3], H.263 [4], and MPEG-4 
[5] is generally based on fixed tables of variable length 
codes (VLC). For coding the residual data in these video 
coding standards, a block of transform coefficient levels is 
first mapped onto a one-dimensional list using an inverse 
scanning pattern. This list of transform coefficient levels is 
then coded using a combination of run-length and variable 
length coding. Due to the usage of variable length codes, 
coding events with a probability greater than 0.5 cannot be 
efficiently represented and hence, a so-called alphabet ex-
tension of “run” symbols representing successive levels 
with value zero is used in the entropy coding schemes of 
MPEG-2, H.263, and MPEG-4. Moreover, the usage of 
fixed VLC tables does not allow an adaptation to the actual 
symbol statistics, which may vary over space and time as 
well as for different source material and coding conditions. 
Finally, since there is a fixed assignment of VLC tables and 
syntax elements, existing inter-symbol redundancies cannot 
be exploited within these coding schemes. 
Although, from a conceptual point-of-view, it is well 
known for a long time that all these deficiencies can be most 
easily resolved by arithmetic codes [23], little of this 
knowledge was actually translated into practical entropy 
coding schemes specifically designed for block-based hy-
brid video coding. One of the first hybrid block-based video 
coding schemes that incorporate an adaptive binary arithme-
tic coder capable of adapting the model probabilities to the 
existing symbol statistics was presented in [6]. The core of 
that entropy coding scheme was inherited from the JPEG 
standard (at least for coding of DCT coefficients) [25], and 
an adjustment of its modeling part to the specific statistical 
characteristics of typically observed residual data in a hy-
brid video coder was not carried out. As a result, the per-
formance of this JPEG-like arithmetic entropy coder in the 
hybrid block-based video coding scheme of [6] was not 
substantially better for inter-coded pictures than that of its 
VLC-based counterpart.  
The first and – until H.264/AVC was officially released – 
the only standardized arithmetic entropy coder within a hy-
brid block-based video coder is given by Annex E of H.263 
[4]. Three major drawbacks in the design of that optional 
Context-Based Adaptive Binary Arithmetic Coding 
in the H.264/AVC Video Compression Standard 
Detlev Marpe, Member, IEEE, Heiko Schwarz, and Thomas Wiegand 
N 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
2 
arithmetic coding scheme can be identified. First, Annex E 
is applied to the same syntax elements as the VLC method 
of H.263 including the combined symbols for coding of 
transform coefficient levels. Thus, one of the fundamental 
advantages of arithmetic coding that a non-integer code 
length can be assigned to coding events is unlikely to be ex-
ploited. Second, all probability models in Annex E of H.263 
are non-adaptive in the sense that their underlying probabil-
ity distributions are assumed to be static. Although, multiple 
probability distribution models are defined and chosen in a 
frequency-dependent way for the combined symbols of run, 
level and “last” information, this conditioning does not re-
sult in a significant gain in coding efficiency, since an adap-
tation to the actual symbol statistics is not possible. Finally, 
the generic m-ary arithmetic coder used in Annex E in-
volves a considerable amount of computational complexity, 
which may not be justified in most application scenarios, 
especially in view of the typically observed, small margins 
of coding gains. 
Entropy coding schemes based on arithmetic coding are 
quite frequently involved in the field of non block-based 
video coding. Most of these alternative approaches to video 
coding are based on the discrete wavelet transform (DWT) 
in combination with disparate methods of temporal predic-
tion, such as overlapped block motion compensation, grid-
based warping or motion-compensated temporal filtering 
[18],[19],[20]. The corresponding entropy coding schemes 
are often derived from DWT-based still image coding 
schemes like SPIHT [21] or other predecessors of 
JPEG2000 [35].  
In our prior work on wavelet-based hybrid video coding, 
which led to one of the proposals for the H.26L standardiza-
tion [19], the entropy coding method of partitioning, ag-
gregation and conditional coding (PACC) was developed 
[22]. One of its main distinguishing features is related to the 
partitioning strategy: Given a source with a specific alpha-
bet size, for instance, quantized transform coefficients, it 
was found to be useful to first reduce the alphabet size by 
partitioning the range according to a binary selector, which 
e.g. in the case of transform coefficients would be typically 
given by the decision whether the coefficient is quantized to 
zero or not. In fact, range partitioning using binary selectors 
can be viewed as a special case of a binarization scheme, 
where a symbol of a non-binary alphabet is uniquely 
mapped to a sequence of binary decisions prior to further 
processing.  
This (somehow) dual operation to the aforementioned al-
phabet extension, which in the sequel we will therefore refer 
to as alphabet reduction, is mainly motivated by the fact 
that it allows the subsequent modeling stage to operate more 
efficiently on this maximally reduced (binary) alphabet. In 
this way, the design and application of higher-order condi-
tioning models is greatly simplified and, moreover, the risk 
of “overfitting” the model is reduced. As a positive side ef-
fect, a fast table-driven binary arithmetic coder can be util-
ized for the final arithmetic coding stage.  
The design of CABAC is in the spirit of our prior work. 
To circumvent the drawbacks of the known entropy coding 
schemes for hybrid block-based video coding such as An-
nex E of H.263, we combine an adaptive binary arithmetic 
coding technique with a well-designed set of context mod-
els. Guided by the principle of alphabet reduction, an addi-
tional binarization stage is employed for all non-binary val-
ued symbols. Since the increased computational complexity 
of arithmetic coding in comparison to variable length cod-
ing is generally considered as its main disadvantage, great 
importance has been devoted to the development of an algo-
rithmic design that allows efficient hardware and software 
implementations. 
For some applications, however, the computational re-
quirements of CABAC may be still too high given today’s 
silicon technology. Therefore, the baseline entropy coding 
method of H.264/AVC [1] offers a different compression-
complexity trade-off operating at reduced coding efficiency 
and complexity level compared to CABAC. It mostly relies 
on a single infinite-extended codeword set consisting of 
zero-order Exp-Golomb codes, which are used for all syntax 
elements except for the residual data. For coding the resid-
ual data, a more sophisticated method called Context-
Adaptive Variable Length Coding (CAVLC) is employed. 
In this scheme, inter-symbol redundancies are exploited by 
switching VLC tables for various syntax elements depend-
ing on already transmitted coding symbols [1],[2]. The 
CAVLC method cannot provide an adaptation to the actu-
ally given conditional symbol statistics. Furthermore, cod-
ing events with symbol probabilities greater than 0.5 cannot 
be efficiently coded due to the fundamental lower limit of 1 
bit/symbol imposed on variable length codes. This restric-
tion prevents the usage of coding symbols with a smaller al-
phabet size for coding the residual data, which could allow 
a more suitable construction of contexts for switching be-
tween the model probability distributions.  
The remainder of the paper is organized as follows. In 
Section II, we present an overview of the CABAC frame-
work including a high-level description of its three basic 
building blocks of binarization, context modeling and bi-
nary arithmetic coding. We also briefly discuss the motiva-
tion and the principles behind the algorithmic design of 
CABAC. A more detailed description of CABAC is given 
in Section III, where the individual steps of the algorithm 
are presented in depth. Finally, in Section IV we provide 
experimental results to demonstrate the performance gains 
of CABAC relative to the baseline entropy coding mode of 
H.264/AVC for a set of interlaced video test sequences. 
II. THE CABAC FRAMEWORK 
Fig. 1 shows the generic block diagram for encoding a 
single syntax element in CABAC.1 The encoding process 
 
1 For simplicity and for clarity of presentation we restrict our exposition of 
CABAC to an encoder only view. In the text of the H.264/AVC standard 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
3 
consists of at most three elementary steps:  
1) binarization  
2) context modeling 
3) binary arithmetic coding 
In the first step, a given non-binary valued syntax element 
is uniquely mapped to a binary sequence, a so-called bin 
string. When a binary valued syntax element is given, this 
initial step is bypassed, as shown in Fig. 1. For each element 
of the bin string or for each binary valued syntax element, 
one or two subsequent steps may follow depending on the 
coding mode. 
In the so-called regular coding mode, prior to the actual 
arithmetic coding process the given binary decision, which, 
in the sequel, we will refer to as a bin, enters the context 
modeling stage, where a probability model is selected such 
that the corresponding choice may depend on previously 
encoded syntax elements or bins. Then, after the assignment 
of a context model the bin value along with its associated 
model is passed to the regular coding engine, where the fi-
nal stage of arithmetic encoding together with a subsequent 
model updating takes place (see Fig. 1).  
Alternatively, the bypass coding mode is chosen for se-
lected bins in order to allow a speedup of the whole encod-
ing (and decoding) process by means of a simplified coding 
engine without the usage of an explicitly assigned model, as 
illustrated by the lower right branch of the switch in Fig. 1.  
In the following, the three main functional building 
blocks, which are binarization, context modeling, and bi-
nary arithmetic coding, along with their interdependencies 
are discussed in more detail. 
A. Binarization 
1) General Approach 
For a successful application of context modeling and 
adaptive arithmetic coding in video coding we found that 
the following two requirements should be fulfilled:  
                                                                                                 
[1] itself, the converse perspective dominates – the standard normatively 
specifies only how to decode the video content without specifying how to 
encode it. 
i)  a fast and accurate estimation of conditional probabili-
ties must be achieved in the relatively short time inter-
val of a slice coding unit 
ii)  the computational complexity involved in performing 
each elementary operation of probability estimation 
and subsequent arithmetic coding must be kept at a 
minimum to facilitate a sufficiently high throughput of 
these inherently sequentially organized processes.  
To fulfill both requirements we introduce the important 
“pre-processing” step of first reducing the alphabet size of 
the syntax elements to encode. Alphabet reduction in 
CABAC is performed by the application of a binarization 
scheme to each non-binary syntax element resulting in a 
unique intermediate binary codeword for a given syntax 
element, called a bin string. The advantages of this ap-
proach are both in terms of modeling and implementation.  
First, it is important to note that nothing is lost in terms of 
modeling, since the individual (non-binary) symbol prob-
abilities can be recovered by using the probabilities of the 
individual bins of the bin string. For illustrating this aspect, 
let us consider the binarization for the syntax element 
mb_type of a P/SP slice.  
As depicted in Fig. 2 (left), the terminal nodes of the bi-
nary tree correspond to the symbol values of the syntax 
element such that the concatenation of the binary decisions 
for traversing the tree from the root node to the correspond-
ing terminal node represents the bin string of the corre-
sponding symbol value. For instance, consider the value “3” 
of mb_type, which signals the macroblock type “P_8x8”, 
i.e., the partition of the macroblock into four 8×8 sub-
macroblocks in a P/SP slice. In this case the corresponding 
bin string is given by “001”. As an obvious consequence, 
the symbol probability p(“3”) is equal to the product of the 
probabilities p(C0)(“0”), p(C1)(“0”) and p(C2)(“1”), where C0, 
C1 and C2 denote the (binary) probability models of the 
corresponding internal nodes, as shown in Fig. 2. This rela-
tion is true for any symbol represented by any such binary 
tree, which can be deduced by the iterated application of the 
Total Probability Theorem [26]. 
Fig. 1. CABAC encoder block diagram. 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
4 
 
Fig. 2. Illustration of the binarization for mb_type (left) and sub_mb_type 
(right) both for P/SP slices. 
 
Although at this stage nothing seems to be gained, there 
is already the advantage of using a binary arithmetic coding 
engine on the bin string instead of an m-ary arithmetic coder 
operating on the original m-ary source alphabet. Adaptive 
m-ary arithmetic coding (for m > 2) is in general a computa-
tionally complex operation requiring at least two multiplica-
tions for each symbol to encode as well as a number of 
fairly complex operations to perform the update of the 
probability estimation [36]. In contrast to that, there are fast, 
multiplication-free variants of binary arithmetic coding, one 
of which was specifically developed for the CABAC 
framework, as further described below. Since the probabil-
ity of symbols with larger bin strings is typically very low, 
the computational overhead of coding all bins of that bin 
string instead of using only one pass in an m-ary arithmetic 
coder is fairly small and can be easily compensated by using 
a fast binary coding engine.  
Finally, as the most important advantage, binarization en-
ables context modeling on a sub-symbol level. For specific 
bins, which, in general, are represented by the most fre-
quently observed bins, conditional probabilities can be 
used, whereas other, usually less frequently observed bins 
can be treated using a joint, typically zero-order probability 
model. Compared to the conventional approach of using 
context models in the original domain of the source with 
typically large alphabet size (like e.g. components of motion 
vector differences or transform coefficient levels) this addi-
tional freedom in the design offers a flexible instrument for 
using higher-order conditional probabilities without suffer-
ing from context “dilution” effects. These effects are often 
observed in cases, where a large number of conditional 
probabilities have to be adaptively estimated on a relatively 
small (coding) time interval, such that there are not enough 
samples to reach a reliable estimate for each model.2  
For instance, when operating in the original alphabet do-
main, a quite moderately chosen 2nd order model for a given 
syntax element alphabet of size m = 256 will result in the in-
tractably large number of 2562 ⋅ (256 − 1) ≈ 224 symbol 
probabilities to be estimated for that particular syntax ele-
ment only. Even for a zero-order model, the task of tracking 
255 individual probability estimates according to the previ-
 
2 A more rigorous treatment of that problem can be found in [23][24].  
ous example is quite demanding. However, typically meas-
ured probability density functions (pdf) of prediction re-
siduals or transformed prediction errors can be modeled by 
highly peaked Laplacian, generalized Gaussian or geometric 
distributions [28], where it is reasonable to restrict the esti-
mation of individual symbol statistics to the area of the 
largest statistical variations at the peak of the pdf. Thus, if, 
for instance, a binary tree resulting from a Huffman code 
design would be chosen as a binarization for such a source 
and its related pdf, only the nodes located in the vicinity of 
the root node would be natural candidates for being mod-
eled individually, whereas a joint model would be assigned 
to all nodes on deeper tree levels corresponding to the “tail” 
of the pdf. Note that this design is different from the exam-
ple given in Fig. 2, where each (internal) node has its own 
model.  
In the CABAC framework, typically, only the root node 
would be modeled using higher-order conditional probabili-
ties. In the above example this would result for a 2nd order 
model in only 4 different binary probability models instead 
of m2 different m-ary probability models with m = 256. 
2) Design of CABAC Binarization Schemes 
As already indicated above, a binary representation for a 
given non-binary valued syntax element provided by the bi-
narization process should be close to a minimum-
redundancy code. On the one hand, this allows to easily ac-
cessing the most probable symbols by means of the binary 
decisions located at or close to the root node for the subse-
quent modeling stage. On the other hand, such a code tree 
minimizes the number of binary symbols to encode on the 
average, hence minimizing the computational workload in-
duced by the binary arithmetic coding stage.  
However, instead of choosing a Huffman tree for a given 
training sequence, the design of binarization schemes in 
CABAC (mostly) relies on a few basic code trees, whose 
structure enables a simple on-line computation of all code 
words without the need for storing any tables. There are 
four such basic types: the unary code, the truncated unary 
code, the kth order Exp-Golomb code and the fixed-length 
code. In addition, there are binarization schemes based on a 
concatenation of these elementary types. As an exception of 
these structured types, there are five specific, mostly un-
structured binary trees that have been manually chosen for 
the coding of macroblock types and sub-macroblock types. 
Two examples of such trees are shown in Fig. 2.  
In the remaining part of this section, we explain in more 
detail the construction of the four basic types of binarization 
and its derivatives. 
Unary and Truncated Unary Binarization Scheme: For 
each unsigned integer valued symbol x ≥ 0 the unary code 
word in CABAC consists of x “1” bits plus a terminating 
“0” bit. The truncated unary (TU) code is only defined for x 
with 0 ≤ x ≤ S, where for x < S the code is given by the 
unary code, whereas for x = S the terminating “0” bit is ne-
glected such that the TU code of x = S is given by codeword 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
5 
consisting of x “1” bits only. 
kth order Exp-Golomb Binarization Scheme: Exponen-
tial Golomb codes were first proposed by Teuhola [29] in 
the context of run-length coding schemes. This parameter-
ized family of codes is a derivative of Golomb codes, which 
have been proven to be optimal prefix-free codes for geo-
metrically distributed sources [30]. Exp-Golomb codes are 
constructed by a concatenation of a prefix and a suffix code 
word. Fig. 3 shows the construction of the kth order Exp-
Golomb (EGk) code word for a given unsigned integer sym-
bol x. The prefix part of the EGk code word consists of a 
unary code corresponding to the value  .)12/(log)( 2 += kxxl   
The EGk suffix part is computed as the binary representa-
tion of x + 2k ( 1 − 2l(x) ) using k + l( x ) significant bits, as 
can be seen from the pseudo-C code in Fig. 3. 
Consequently, for the EGk binarization the number of 
symbols having the same code length of k + 2 · l( x ) + 1 is 
geometrically growing. By inverting Shannon’s relationship 
between ideal code length and symbol probability, we can 
e.g. easily deduce that EG0 is the optimal code for a pdf 
p(x) = ½ · (x + 1)−2 with x ≥ 0. This implies that for an ap-
propriately chosen parameter k the EGk code represents a 
fairly good first-order approximation of the ideal prefix-free 
code for tails of typically observed pdfs, at least for syntax 
elements that are representing prediction residuals. 
Fixed-Length Binarization Scheme: For the applica-
tion of fixed-length (FL) binarization, a finite alphabet of 
values of the corresponding syntax element is assumed. Let 
x denote a given value of such a syntax element, where 
0 ≤ x < S. Then, the FL code word of x is simply given by 
the binary representation of x with a fixed (minimum) num-
ber  .log2 SlFL =  of bits. Typically, FL binarization is ap-
plied to syntax elements with a nearly uniform distribution 
or to syntax elements, where each bit in the FL binary rep-
resentation represents a specific coding decision as e.g. in 
the part of the coded block pattern symbol related to the lu-
minance residual data.  
Concatenation of Basic Binarization Schemes: From 
the basic binarization schemes as described above three 
more binarization schemes are derived. The first one is a 
concatenation of a 4-bit FL prefix as a representation of the 
luminance related part of the coded block pattern and a TU 
suffix with S = 2 representing the chrominance related part 
of coded_block_pattern. 
Both the second and third concatenated scheme are de-
rived from the TU and the EGk binarization. These 
schemes, which are referred as Unary / kth order Exp-
Golomb (UEGk) binarizations, are applied to motion vector 
differences and absolute values of transform coefficient lev-
els. The design of these concatenated binarization schemes 
is motivated by the following observations. First, the unary 
code is the simplest prefix-free code in terms of implemen-
tation cost. Secondly, it permits a fast adaptation of the in-
dividual symbol probabilities in the subsequent context 
modeling stage, since the arrangement of the nodes in the 
corresponding tree is typically such that with increasing dis-
tance of the internal nodes from the root node the corre-
sponding binary probabilities are less skewed.3 These ob-
servations are only accurate for small values of the absolute 
motion vector differences and transform coefficient levels. 
For larger values, there is not much use of an adaptive mod-
eling leading to the idea of concatenating an adapted trun-
cated unary tree as a prefix and a static Exp-Golomb code 
tree as a suffix. Typically, for larger values, the EGk suffix 
part represents already a fairly good fit to the observed 
probability distribution, as already mentioned above. Thus, 
it is reasonable to speedup the encoding of the bins related 
to the EGk suffix part in CABAC by using the fast bypass 
coding engine for uniformly distributed bins, as further de-
scribed in Section III.D.  
For motion vector differences UEGk binarization is con-
structed as follows. Let us assume the value mvd of a mo-
tion vector component is given. For the prefix part of the 
UEGk bin string, a TU binarization is invoked using the ab-
solute value of mvd with a cut-off value of S = 9. If mvd is 
equal to zero, the bin string consists only of the prefix code 
word “0”. If the condition |mvd| ≥ 9 holds, the suffix is con-
structed as an EG3 codeword for the value of |mvd| − 9, to 
which the sign of mvd is appended using the sign bit “1” for 
a negative mvd and the sign bit “0” otherwise. For mvd val-
ues with 0 < |mvd| < 9, the suffix consists only of the sign 
bit. Noting that the component of a motion vector difference 
represents the prediction error at quarter-sample accuracy, 
the prefix part always corresponds to a maximum error 
component of ±2 samples. With the choice of the Exp-
Golomb parameter k = 3, the suffix code words are given 
 
3 The aspect of a suitable ordering of nodes in binary trees for optimal 
modeling and fast adaptation has been addressed in [31], although in a 
slightly different context. 
while(1) { 
   // unary prefix part of EGk 
   if (x >= (1<<k) ) { 
      put( 1 ) 
      x = x – (1<<k) 
      k++ 
   } else { 
      put( 0 ) // terminating “0” of prefix part 
      while(k--) // binary suffix part of EGk 
        put( (x>>k) & 0x01 ) 
      break 
   } 
} 
 
Fig. 3.  Construction of kth order Exp-Golomb (EGk) code for a given un-
signed integer symbol x.  
TABLE I 
UEG0 BINARIZATION FOR ENCODING OF ABSOLUTE VALUES OF 
TRANSFORM COEFFICIENT LEVELS  
 
Bin string Abs. 
value TU prefix  EG0 suffix  
1 0                    
2 1 0                   
3 1 1 0                  
4 1 1 1 0                 
5 1 1 1 1 0                
... . . . . . .               
... . . . . . . . . . . . .         
13 1 1 1 1 1 1 1 1 1 1 1 1 0        
14 1 1 1 1 1 1 1 1 1 1 1 1 1 0       
15 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0      
16 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0    
17 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1    
18 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0  
19 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1  
20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0  
... . . . . . . . . . . . . . . . . . . . . 
bin 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ... 
 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
6 
such that a geometrical increase of the prediction error in 
units of 2 samples is captured by a linear increase in the 
corresponding suffix code word length. 
UEGk binarization of absolute values of transform coef-
ficient levels (abs_level) is specified by the cut-off value 
S = 14 for the TU prefix part and the order k = 0 for the 
EGk suffix part. Note that the binarization and subsequent 
coding process is applied to the syntax element co-
eff_abs_value_minus1 = abs_level − 1, since zero valued 
transform coefficient levels are encoded using a significance 
map, as described in more detail in Section III.B below. The 
construction of a bin string for a given value of co-
eff_abs_value_minus1 is similar to the construction of 
UEGk bin strings for the motion vector difference compo-
nents except that no sign bit is appended to the suffix. Ta-
ble I shows the corresponding bin strings for values of 
abs_level from 1 to 20, where the prefix parts are high-
lighted in gray shaded columns. 
B. Context Modeling 
One of the most important properties of arithmetic coding 
is the possibility to utilize a clean interface between model-
ing and coding such that in the modeling stage, a model 
probability distribution is assigned to the given symbols, 
which then, in the subsequent coding stage, drives the actual 
coding engine to generate a sequence of bits as a coded rep-
resentation of the symbols according to the model distribu-
tion. Since it is the model that determines the code and its 
efficiency in the first place, it is of paramount importance to 
design an adequate model that explores the statistical de-
pendencies to a large degree and that this model is kept “up 
to date” during encoding. However, there are significant 
model costs involved by adaptively estimating higher-order 
conditional probabilities.  
Suppose a pre-defined set T of past symbols, a so-called 
context template, and a related set C = {0,…, C−1} of con-
texts is given, where the contexts are specified by a model-
ing function F: T→ C operating on the template T. For each 
symbol x to be coded, a conditional probability p( x|F( z ) ) 
is estimated by switching between different probability 
models according to the already coded neighboring symbols 
z ∈ T. After encoding x using the estimated conditional 
probability p( x|F( z ) ), the probability model is updated 
with the value of the encoded symbol x. Thus, p( x|F( z ) ) is 
estimated on the fly by tracking the actual source statistics. 
Since the number τ of different conditional probabilities to 
be estimated for an alphabet size of m is equal to 
τ = C ⋅ (m − 1), it is intuitively clear that the model cost, 
which represents the cost of “learning” the model distribu-
tion, is proportional to τ.4 This implies that by increasing 
the number C of different context models, there is a point, 
where overfitting of the model may occur such that inaccu-
 
4 Rissanen derived a refinement of that model cost measure by also tak-
ing into account that the precision of estimating the probabilities increases 
with the number of observations [24]. 
rate estimates of p( x|F( z ) ) will be the result. 
In CABAC, this problem is solved by imposing two se-
vere restrictions on the choice of the context models. First, 
very limited context templates T consisting of a few 
neighbors of the current symbol to encode are employed 
such that only a small number of different context models C 
is effectively used. Second, as already motivated in the last 
section, context modeling is restricted to selected bins of the 
binarized symbols. As a result, the model cost is drastically 
reduced, even though the ad-hoc design of context models 
under these restrictions may not result in the optimal choice 
with respect to coding efficiency. In fact, in a recently con-
ducted research, it has been shown that additional gains can 
be obtained by applying the novel GRASP algorithm for an 
optimized selection of context models using larger context 
templates within the CABAC framework [31]. However, the 
improvements are quite moderate compared to the drastic 
increase in complexity required for performing the two-pass 
GRASP algorithm. 
  
       
    B   
   A C   
       
  
Fig. 4.  Illustration of a context template consisting of two neighboring 
syntax elements A and B to the left and on top of the current syntax ele-
ment C. 
 
Four basic design types of context models can be distin-
guished in CABAC. The first type involves a context tem-
plate with up to two neighboring syntax elements in the past 
of the current syntax element to encode, where the specific 
definition of the kind of neighborhood depends on the syn-
tax element. Usually, the specification of this kind of con-
text model for a specific bin is based on a modeling func-
tion of the related bin values for the neighboring element to 
the left and on top of the current syntax element, as shown 
in Fig. 4.  
The second type of context models is only defined for the 
syntax elements of mb_type and sub_mb_type. For this kind 
of context models, the values of prior coded bins 
(b0, b1, b2,…, bi−1) are used for the choice of a model for a 
given bin with index i. Note that in CABAC these context 
models are only used to select different models for different 
internal nodes of the corresponding binary trees, as already 
discussed in Section II.A. 
Both the third and fourth type of context models is ap-
plied to residual data only. In contrast to all other types of 
context models, both types depend on the context categories 
of different block types, as given in Tables IV and V below. 
Moreover, the third type does not rely on past coded data, 
but on the position in the scanning path. For the fourth type, 
modeling functions are specified that involve the evaluation 
of the accumulated number of encoded (decoded) levels 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
7 
with a specific value prior to the current level bin to encode 
(decode). 
Besides these context models based on conditional prob-
abilities, there are fixed assignments of probability models 
to bin indices for all those bins that have to be encoded in 
regular mode and to which no context model of the previous 
specified category can be applied.  
The entity of probability models used in CABAC can be 
arranged in a linear fashion such that each model can be 
identified by a unique so-called context index γ. Table II 
contains an overview of the syntax elements in H.264/AVC 
and its related context indices. The numbering of indices 
was arranged in such a way that the models related to 
mb_type, sub_mb_type and mb_skip_flag for different slice 
types are distinguished by their corresponding indices. Al-
though the ordering of models in Table II is clearly not the 
most economical way of housekeeping the models in 
CABAC, it serves the purpose of demonstrating the concep-
tual idea.  
Each probability model related to a given context index γ 
is determined by a pair of two values, a 6-bit probability 
state index σγ and the (binary) value ϖγ of the most prob-
able symbol (MPS), as will be further described in Section 
III.C below. Thus, the pairs ( σγ, ϖγ  ) for 0 ≤ γ ≤ 398 and 
hence the models themselves can be efficiently represented 
by 7-bit unsigned integer values. 
The context indices in the range from 0 to 72 are related 
to syntax elements of macroblock type, sub-macroblock 
type, and prediction modes of spatial and of temporal type 
as well as slice-based and macroblock-based control infor-
mation. For this type of syntax elements, a corresponding 
context index γ can be calculated as 
 
,SS χγ +Γ=   (1) 
 
where ΓS denotes the so-called context index offset, which is 
defined as the lower value of the range given in Table II, 
and χS denotes the context index increment of a given syn-
tax element S. Note that the variable χS may depend only on 
the bin index, in which case a fixed assignment of probabil-
ity model is given, or alternatively, it may specify one of the 
first or second type of context models, as given above. The 
notation introduced in (1) will be used in Section III.A for a 
more detailed description of the context models for syntax 
elements of the above given type.  
Context indices in the range from 73 to 398 are related to 
the coding of residual data.5 Two sets of context models are 
specified for the syntax elements significant_coeff_flag and 
last_significant_coeff_flag, where the coding of both syntax 
elements is conditioned on the scanning position as further 
described in Section III.B. Since in macroblock adaptive 
frame/field coded frames, the scanning pattern depends on 
the mode decision of a frame/field coded macroblock, sepa-
 
5 As an exception, context index γ = 276 is related to the end of slice 
flag. 
rate sets of models have been defined for both modes. The 
range values in the lower row of the corresponding syntax 
elements in Table II specify the context indices for field-
based coding mode. Note that in pure frame or field coded 
pictures only 277 out of the total number of 399 probability 
models are actually used. 
The context index for coded_block_pattern is specified 
by the relation of (1), whereas for all other syntax elements 
of residual data, a context index γ is given by  
 
,)_( SSS catctx χγ +∆+Γ=   (2) 
 
where in addition to the context index offset ΓS the context 
category (ctx_cat) dependent offset ∆S is employed. The 
specification of the context categories and the related values 
of ∆S are given in Tables IV and V below. By using the no-
tation of (2), a more detailed description of the context 
models for syntax elements of residual data will be given in 
Section III.B. 
Note that for the context-modeling process only past 
coded values of syntax elements are evaluated that belong 
to the same slice, where the current coding process takes 
place. It is also worth noting that regardless of the type of 
context model, conditioning is always confined to syntax 
element values such that the entropy encoding/decoding 
process can be completely decoupled from the rest of the 
encoding/decoding operations in a H.264/AVC en-
coder/decoder. 
C. Binary Arithmetic Coding 
Binary arithmetic coding is based on the principle of re-
cursive interval subdivision that involves the following ele-
mentary multiplication operation. Suppose that an estimate 
of the probability pLPS ∈ (0, 0.5] of the least probable sym-
bol (LPS) is given and that the given interval is represented 
TABLE II 
SYNTAX ELEMENTS AND ASSOCIATED RANGE OF CONTEXT INDICES 
 
Slice type 
Syntax element 
SI/I P/SP B 
mb_type 0/3-10 14-20 27-35 
mb_skip_flag  11-13 24-26 
sub_mb_type  21-23 36-39 
mvd (horizontal)  40-46 40-46 
mvd (vertical)  47-53 47-53 
ref_idx  54-59 54-59 
mb_qp_delta 60-63 60-63 60-63 
intra_chroma_pred_mode 64-67 64-67 64-67 
prev_intra4x4_pred_mode_flag 68 68 68 
rem_intra4x4_pred_mode 69 69 69 
mb_field_decoding_flag 70-72 70-72 70-72 
coded_block_pattern 73-84 73-84 73-84 
coded_block_flag 85-104 85-104 85-104 
significant_coeff_flag 
105-165, 
277-337 
105-165, 
277-337 
105-165, 
277-337 
last_significant_coeff_flag 
166-226, 
338-398 
166-226, 
338-398 
166-226, 
338-398 
coeff_abs_level_minus1 227-275 227-275 227-275 
end_of_slice_flag 276 276 276 
 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
8 
by its lower bound L and its width (range) R. Based on that 
settings, the given interval is subdivided into two sub-
intervals: one interval of width 
 
RLPS = R × pLPS, (3) 
 
which is associated with the LPS, and the dual interval of 
width RMPS = R − RLPS, which is assigned to the most prob-
able symbol (MPS) having a probability estimate of 
1 − pLPS. Depending on the observed binary decision, either 
identified as the LPS or the MPS, the corresponding sub-
interval is then chosen as the new current interval. A binary 
value pointing into that interval represents the sequence of 
binary decisions processed so far, whereas the range of that 
interval corresponds to the product of the probabilities of 
those binary symbols. Thus, to unambiguously identify that 
interval and hence the coded sequence of binary decisions, 
the Shannon lower bound on the entropy of the sequence is 
asymptotically approximated by using the minimum preci-
sion of bits specifying the lower bound of the final interval. 
In a practical implementation of binary arithmetic coding 
the main bottleneck in terms of throughput is the multiplica-
tion operation in (3) required to perform the interval subdi-
vision. A significant amount of work has been published in 
literature aimed at speeding up the required calculation in 
(3) by introducing some approximations of either the range 
R or of the probability pLPS such that the multiplication can 
be avoided [32],[33],[34]. Among these low-complexity bi-
nary arithmetic coding methods, the Q coder [32] and its de-
rivatives QM and MQ coder [35] have attracted great atten-
tion, especially in the context of the still image coding stan-
dardization groups JPEG and JBIG. 
Although the MQ coder represents the state-of-the-art in 
fast binary arithmetic coding, we found that it considerably 
degrades coding efficiency, at least in the application sce-
nario of H.264/AVC video coding [36]. Motivated by this 
observation, we have designed an alternative multiplication-
free binary arithmetic coding scheme, the so-called modulo 
coder (M coder), which can be shown to have negligible 
performance degradation in comparison to a conventional 
binary arithmetic coder, as e.g. proposed in [37]. At the 
same time, our novel design of the M coder has been shown 
to provide a higher throughput rate than the MQ coder, 
when compared in a software-based implementation [36]. 
The basic idea of our multiplication-free approach to bi-
nary arithmetic coding is to project both the legal range 
[Rmin, Rmax) of interval width R and the probability range as-
sociated with the LPS onto a small set of representative val-
ues Q = {Q0, …, QK−1} and P = {p0, …, pN−1}, respectively. 
By doing so, the multiplication on the right hand side of (3) 
can be approximated by using a table of K × N pre-
computed product values Qρ × pσ for {0 ≤ ρ ≤ K−1} and 
{0 ≤ σ ≤ N−1}. For the regular arithmetic core engine in 
H.264/AVC, a good trade-off between a reasonable size of 
the corresponding table and a sufficiently good approxima-
tion of the “exact” interval subdivision of (3) was found by 
using a set Q of K = 4 quantized range values together with 
set P of N = 64 LPS related probability values, as described 
in more details in Section III.C and III.D below. 
Another distinct feature of the binary arithmetic coding 
engine in H.264/AVC, as already mentioned above, is its 
simplified bypass coding mode. This mode is established 
for specific syntax elements or parts thereof, which are as-
sumed to be nearly uniformly distributed. For coding this 
kind of symbols in bypass mode, the computationally ex-
pensive probability estimation will be completely omitted. 
 
III. DETAILED DESCRIPTION OF CABAC 
This section provides detailed information for each syn-
tax element regarding the specific choice of a binarization 
scheme and the associated context models for each bin of 
the corresponding bin string. For that purpose, the syntax 
elements are divided into two categories. The first category, 
which is described in Section III.A, contains the elements 
related to macroblock type, sub-macroblock type, and in-
formation of prediction modes both of spatial and of tempo-
ral type as well as slice-based and macroblock-based con-
trol information. In the second category, which is described 
in Section III.B, all residual data elements, i.e., all syntax 
elements related to the coding of transform coefficients are 
combined. 
In addition, a more detailed explanation of the probability 
estimation process and the table-based binary arithmetic 
coding engine of CABAC is given in Sections III.C and 
III.D, respectively. 
 
A. Coding of Macroblock Type, Prediction Mode and 
Control Information 
1) Coding of Macroblock and Sub-Macroblock Type 
At the top level of the macroblock layer syntax the sig-
naling of mb_skip_flag and mb_type is performed. The bi-
nary-valued mb_skip_flag indicates whether the current 
macroblock in a P/SP or B slice is skipped, and if it is not 
(i.e., mb_skip_flag = 0) further signaling of mb_type speci-
fies the chosen macroblock type. For each 8×8 sub-
macroblock of a macroblock coded in “P_8x8” or “B_8x8” 
mode, an additional syntax element (sub_mb_type) is pre-
sent that specifies the type of the corresponding sub-
macroblock. In this section, we will restrict our presentation 
to the coding of mb_type, mb_skip_flag and sub_mb_type 
in P/SP slices only; for more information the reader is re-
ferred to [1]. 
Macroblock Skip Flag: For coding of the 
mb_skip_flag statistical dependencies between neighboring 
values of the syntax element mb_skip_flag are exploited by 
means of a simple but effective context design. For a given 
macroblock C, the related context models involve the 
mb_skip_flag values of the neighboring macroblocks to the 
left (denoted by A) and on top of C (denoted by B). More 
specifically, the corresponding context index increment 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
9 
χMbSkip is defined by 
 
χMbSkip ( C ) = (mb_skip_flag( A ) != 0) : 0 ? 1 
                       + (mb_skip_flag( B ) != 0) : 0 ? 1. (4) 
 
If one or both of the neighboring macroblocks X (A or B) 
are not available (e.g. because they are outside of the cur-
rent slice), the corresponding mb_skip_flag( X ) value in (4) 
is set to 0.6  
Macroblock Type: As already stated above, for the bi-
narization of mb_type and sub_mb_type specifically de-
signed binarization schemes are used. Fig. 2 shows the cor-
responding binarization trees for mb_type (left) and 
sub_mb_type (right) that are used in P or SP slices, where 
the terminal nodes of the trees correspond to the symbol 
values of the syntax element and the concatenation of the 
binary decisions on the way from the root node to the corre-
sponding terminal node represents the bin string of the cor-
responding symbol value. Note that the mb_type value of 
“4” for P slices is not used in CABAC entropy coding 
mode. For the values “5” – “30” of mb_type, which repre-
sent the intra macroblock types in a P slice, the correspond-
ing bin strings consist of a concatenation of the prefix bin 
string “1” as shown in Fig. 2 and a suffix bin string, which 
is further specified in [1]. 
For coding a bin value corresponding to the binary deci-
sion at an internal node as shown in Fig. 2 separate context 
models denoted by C0, ..., C3 for mb_type and 
C’0, C’1, C’2 for sub_mb_type are employed. 
 
2) Coding of Prediction Modes 
Since all samples of a macroblock are predicted, the cor-
responding prediction modes have to be transmitted. For a 
macroblock coded in intra mode, these syntax elements are 
given by the intra prediction modes for both luminance and 
chrominance, whereas for an inter coded macroblock the 
reference picture index/indices together with their related 
motion vector component differences have to be signaled.  
Intra Prediction Modes for Luma 4×4: The luminance 
intra prediction modes for 4×4 blocks are itself predicted 
resulting in the syntax elements of the binary-valued 
prev_intra4x4_pred_mode_flag and the mode indicator 
rem_intra4x4_pred_mode, where the latter is only present if 
the former takes a value of 0. For coding these syntax ele-
ments two separate probability models are utilized: one for 
coding of the flag and another for coding each bin value of 
the 3-bit FL binarized value of rem_intra4x4_pred_mode. 
Intra Prediction Modes for Chroma: Spatially 
neighboring intra prediction modes for the chrominance 
typically exhibits some correlation, which are exploited by a 
simple context design that relies on the related modes of the 
neighboring macroblocks A to the left and B on top of the 
current macroblock. However, not the modes of the 
neighbors itself are utilized for specifying the context 
model, but rather the binary-valued information ChPred-
InDcMode, which signals whether the corresponding mode 
takes the typically observed most probable mode given by 
the value “0” (DC prediction). Thus, the corresponding con-
text index increment χChPred (C) for a given MB C is defined 
by 
 
χChPred ( C ) = (ChPredInDcMode( A ) != 0) : 0 ? 1 
                           + (ChPredInDcMode( B ) != 0) : 0 ? 1. (5) 
 
This context design results in 3 models, which are only 
applied to the coding of the value representing the first bin 
of the TU binarization of the current value of the syntax 
element intra_chroma_pred_mode to encode. Since the 
value of the first bin carries the ChPredInDcMode( C ) in-
formation of the current macroblock C, it is reasonable to 
restrict the application of the 3 models defined by (5) to this 
bin only. For the two remaining bins an additional (fixed) 
probability model is applied. 
Reference Picture Index: The same design principle as 
before was applied to the construction of the context models 
for the reference picture index ref_idx7. First, the relevant 
information for conditioning the first bin value of ref_idx is 
extracted from the reference picture indices of the neighbor-
ing macroblock or sub-macroblock partitions A to the left 
and B on top of the current partition C. This information is 
appropriately condensed in the binary flag RefIdxZeroFlag, 
which indicates whether ref_idx with value 0 is chosen for 
the corresponding partition. As a slight variation, the related 
context index increment was chosen to represent 4 instead 
of 3 models as in the previously discussed context designs: 
 
χRefIdx ( C ) = (RefIdxZeroFlag ( A ) != 0) : 0 ? 1 
                     + 2 · ((RefIdxZeroFlag ( B ) != 0) : 0 ? 1). 
 
Application of these context models to the first bin of the 
unary binarized reference picture index is complemented by 
the usage of two additional probability models for encoding 
of the values related to the second and all remaining bins. 
Components of Motion Vector Differences: Motion 
vector differences are prediction residuals, for which a con-
text model is established in CABAC that is based on the lo-
cal prediction error. Let mvd( X, cmp ) denote the value of a 
motion vector difference component of direction 
cmp ∈ {horizontal, vertical} related to a macroblock or sub-
macroblock partition X. Then, the related context index in-
crement χMvd ( C,cmp ) for a given macroblock or sub-
macroblock partition C and component cmp is determined 
by 
                                                                                                 
6 In the following, the information about the “exception handling” in 
the definition of the context index increments will be (mostly) neglected. 
For filling that information gap, the interesting reader is referred to [1]. 
7 For clarity of presentation, the reference picture list suffices l0 and l1 
are suppressed in the following exposition, both for the reference picture 
index and the motion vector difference. 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
10 
( )
( )
( )
( )
( ) ( ) ( ) ,, ,  ,,with 
32,, if,2
32 ,,3 if,1
3,, if ,0
,
cmpBmvdcmpAmvdcmpBAe
cmpBAe
cmpBAe
cmpBAe
cmpCMvd
+=





>
≤≤
<
=χ
  (6) 
 
where A and B represent the corresponding macroblock or 
sub-macroblock partitions to the left and on the top of the 
regarded macroblock or sub-macroblock partition C, re-
spectively.8 χMvd ( C, cmp ) in (6) is only applied for the se-
lection of the probability model that is used to code the 
value of the first bin of the binarized value of 
mvd( C, cmp ). Binarization of motion vector differences is 
achieved by applying the UEG3 binarization scheme with a 
cut-off value of 9. That implies in particular that only the 
unary prefix part is encoded in regular coding mode, where 
4 additional context models are employed for coding the 
values related to the second, third, fourth, and fifth to ninth 
bin of the prefix part. The values of the bins related to the 
Exp-Golomb suffix part including the sign bit are encoded 
using the bypass coding mode. 
 
3) Coding of Control Information 
Three additional syntax elements are signaled at the mac-
roblock or macroblock pair level, which we refer to as con-
trol information. These elements are given by mb_qp_delta, 
end_of_slice_flag, and mb_field_decoding_flag. 
Macroblock-based Quantization Parameter Change: 
For updating the quantization parameter on a macroblock 
level, mb_qp_delta is present for each non-skipped macrob-
lock with a value of coded_block_pattern unequal to 0.9 For 
coding the signed value δ( C ) of this syntax element for a 
given macroblock C in CABAC, δ( C ) is first mapped onto 
a positive value δ+( C ) by using the relation 
 
δ+( C ) = 2| δ( C ) | − ( ( δ( C ) > 0 ) : 1 ? 0 ).  
 
Then, δ+( C ) is binarized using the unary binarization 
scheme. For encoding the value of the corresponding first 
bin, a context model is selected based on the binary decision 
(δ( P ) != 0) for the preceding macroblock P of C in decod-
ing order. This results in two probability models for the first 
bin, whereas for the second and all remaining bins two addi-
tional probability models are utilized. 
End of Slice Flag: For signaling the last macroblock 
(or macroblock pair) in a slice, the end_of_slice_flag is pre-
sent for each macroblock (pair). It is encoded using a spe-
cifically designed non-adaptive probability model such that 
the event of a non-terminating macroblock (pair) is related 
to the highest possible MPS probability (see Section III.C 
for more details on the related probability model). 
Macroblock Pair Field Flag: In macroblock adaptive 
 
8 The precise definition of a neighboring partition used for context 
modeling of both the reference picture index and the motion vector differ-
ence is given in [1]. 
9 For macroblocks coded in an “Intra_16x16” prediction mode, the syn-
tax element mb_qp_delta is always present. 
frame/field coded frames, the mb_field_decoding_flag sig-
nals for each macroblock pair whether it is coded in frame 
or field coding mode. For coding this flag in CABAC, spa-
tial correlations between the coding mode decisions of 
neighboring macroblock pairs are exploited by choosing be-
tween 3 probability models. For a given macroblock pair C 
the selection of the corresponding model is performed by 
means of the related context index increment χMbField ( C ), 
which is defined as 
 
χMbField ( C ) = mb_field_decoding_flag( A ) 
                         + mb_field_decoding_flag( B ). 
 
Here, A and B represent the corresponding macroblock pairs 
to the left and on the top of the current macroblock pair C. 
 
Fig. 5.  Flow diagram of the CABAC encoding scheme for a block of 
transform coefficients. 
 
B. Coding of Residual Data 
1) Characteristic Features 
For the coding of residual data within the H.264/AVC 
standard specifically designed syntax elements are used in 
CABAC entropy coding mode. These elements and their re-
lated coding scheme are characterized by the following dis-
tinct features: 
• A one-bit symbol coded_block_flag and a binary-
valued significance map are used to indicate the occur-
rence and the location of non-zero transform coeffi-
cients in a given block. 
• Non-zero levels are encoded in reverse scanning order. 
• Context models for coding of non-zero transform coef-
ficients are chosen based on the number of previously 
transmitted non-zero levels within the reverse scanning 
path. 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
11 
2) Encoding Process for Residual Data 
Fig. 5 illustrates the CABAC encoding scheme for a sin-
gle block of transform coefficients.  
First, the coded block flag is transmitted for the given 
block of transform coefficients unless the coded block pat-
tern or the macroblock mode indicates that the regarded 
block has no non-zero coefficients. If the coded block flag 
is zero, no further information is transmitted for the block; 
otherwise, a significance map specifying the positions of 
significant coefficients is encoded. Finally, the absolute 
value of the level as well as the sign is encoded for each 
significant transform coefficient. These values are transmit-
ted in reverse scanning order.  
In the following, a more detailed description of each of 
the major building blocks of Fig. 5 is given together with a 
brief specification of the CABAC encoding procedure for 
the coded_block_pattern symbol. 
Coded Block Pattern: For each non-skipped macro-
block with prediction mode not equal to Intra_16x16, the 
coded_block_pattern symbol indicates which of the six 8×8 
blocks – 4 for luminance and 2 for chrominance – contain 
non-zero transform coefficients. A given value of the syntax 
element coded_block_pattern is binarized using the con-
catenation of a 4-bit FL and a TU binarization with cut-off 
value S = 2, as already noted in Section II.A. 
Coded Block Flag: coded_block_flag is a one-bit sym-
bol, which indicates if there are significant, i.e., non-zero 
coefficients inside a single block of transform coefficients, 
for which the coded block pattern indicates non-zero 
entries. If coded_block_flag is zero, no further information 
is transmitted for the related block. 
Scanning of Transform Coefficients: The 2-D arrays of 
transform coefficient levels of those sub-blocks for which 
the coded_block_flag indicates non-zero entries are first 
mapped onto a one-dimensional list using a given zig-zag 
scanning pattern. 
 
TABLE III 
EXAMPLE FOR ENCODING THE SIGNIFICANCE MAP 
 
Scanning position 1 2 3 4 5 6 7 8 9 
Transf. coefficient levels 9 0 -5 3 0 0 -1 0 1 
significant_coeff_flag 1 0 1 1 0 0 1 0 1 
last_significant_coeff_flag 0  0 0   0  1 
 
Significance Map: If the coded_block_flag indicates 
that a block has significant coefficients, a binary-valued 
significance map is encoded. For each coefficient in scan-
ning order, a one-bit symbol significant_coeff_flag is 
transmitted. If the significant_coeff_flag symbol is one, i.e., 
if a non-zero coefficient exists at this scanning position, a 
further one-bit symbol last_significant_coeff_flag is sent. 
This symbol indicates if the current significant coefficient is 
the last one inside the block or if further significant coeffi-
cients follow. Table III shows an example for the signifi-
cance map encoding procedure.  
Note that the flags (significant_coeff_flag, 
last_significant_coeff_flag) for the last scanning position of 
a block are never transmitted. If the last scanning position is 
reached and the significance map encoding was not already 
terminated by a last_significant_coeff_flag with value one, 
it is obvious that the last coefficient has to be significant. 
Level Information: The encoded significance map de-
termines the locations of all significant coefficients inside a 
block of quantized transform coefficients. The values of the 
significant coefficients (levels) are encoded by using two 
coding symbols: coeff_abs_level_minus1 (representing the 
absolute value of the level minus 1), and coeff_sign_flag 
(representing the sign of levels). While coeff_sign_flag is a 
one-bit symbol (with values of 1 for negative coefficients), 
the UEG0 binarization scheme, as depicted in Table I is 
used for encoding the values of coeff_abs_level_minus1. 
The levels are transmitted in reverse scanning order (begin-
ning with the last significant coefficient of the block) allow-
ing the usage of reasonably adjusted context models, as de-
scribed in the next paragraph. 
 
TABLE IV 
VALUES OF CONTEXT INDEX OFFSET ∆ DEPENDING ON CONTEXT CATEGORY 
(AS SPECIFIED IN TABLE V) AND SYNTAX ELEMENT 
 
Context category (ctx_cat) 
Syntax element 
0 1 2 3 4 
coded_block_flag 0 4 8 12 16 
significant_coeff_flag 0 15 29 44 47 
last_significant_coeff_flag 0 15 29 44 47 
coeff_abs_level_minus1 0 10 20 30 39 
 
3) Context Models for Residual Data 
In H.264/AVC residual data coding, there are 12 differ-
ent types of transform coefficient blocks (denoted by 
BlockType in left column of Table V), which typically have 
different kinds of statistics.  
However, for most sequences and coding conditions 
some of the statistics are very similar. To keep the number 
of different context models used for coefficient coding rea-
sonably small, the block types are classified into 5 catego-
ries as specified in the right column of Table V. For each of 
these categories, a special set of context models is used for 
all syntax elements related to residual data with the excep-
tion of coded_block_pattern, as further described below. 
Coded Block Pattern: Since each of the bits in the bin 
string of coded_block_pattern represents a coding decision 
of a corresponding block of transform coefficients, the cho-
sen probability models for that syntax element depend on 
the bin index. For bin indices from 0 to 3 corresponding to 
the four 8×8 luminance blocks, the context index increment 
χCBP for a given 8×8 block C related to bin index bin_idx is 
given by 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
12 
 
χCBP ( C, bin_idx ) = ((CBP_Bit( A ) != 0) : 0? 1)  
              + 2 · ((CBP_Bit( B ) != 0) : 0? 1), 
 
where CBP_Bit( A ) and CBP_Bit( B ) represent the bit of 
the coded block pattern corresponding to the 8×8 blocks A 
to the left and B on the top of the regarded block C, respec-
tively.  
 
TABLE V 
BASIC BLOCK TYPES WITH NUMBER OF COEFFICIENTS AND ASSOCIATED 
CONTEXT CATEGORIES 
 
BlockType MaxNumCoeff 
Context  
category (ctx_cat) 
Luma DC block for  
Intra16x16 
16 Luma-Intra16-DC: 0 
Luma AC block for  
Intra16x16 
15 Luma-Intra16-AC: 1 
Luma block for Intra 4x4 16 
Luma block for Inter 16 
Luma-4x4: 2 
U-Chroma DC block for Intra 4 
V-Chroma DC block for Intra 4 
U-Chroma DC block for Inter 4 
V-Chroma DC block for Inter 4 
Chroma-DC: 3 
U-Chroma AC block for Intra 15 
V-Chroma AC block for Intra 15 
U-Chroma AC block for Inter 15 
V-Chroma AC block for Inter 15 
Chroma-AC: 4 
 
For each of the bin indices 4 and 5, which are related to 
the 2 “chrominance” bins in the binarized value of 
coded_block_pattern, a similar context assignment rule as 
for the luminance bins is defined such that for both bin indi-
ces together 8 additional probability models are specified 
[1].  
Coded Block Flag: Coding of the coded_block_flag 
utilizes four different probability models for each of the five 
categories as specified in Table V. The context index in-
crement χCBFlag ( C ) for a given block C is determined by 
 
χCBFlag ( C ) = coded_block_flag( A )  
   + 2 · coded_block_flag( B ), (7) 
 
where A and B represent the corresponding blocks of the 
same type to the left and on the top of the regarded block C, 
respectively. Only blocks of the same type are used for con-
text determination. The following block types are differenti-
ated: Luma-DC, Luma-AC, Chroma-U-DC, Chroma-U-AC, 
Chroma-V-DC, and Chroma-V-AC. If no neighboring block 
X (A or B) of the same type exists (e.g. because the current 
block is intra coded and the neighboring block X is inter 
coded), the corresponding coded_block_flag(X) value in (7) 
is set to 0. If a neighboring block X (A or B) is outside the 
picture area or positioned in a different slice, the corre-
sponding coded_block_flag(X) value is replaced by a de-
fault value. If the current block C is coded using an intra 
prediction mode, a default value of 1 is used; otherwise, a 
default value of 0 is used. Thus, while six block types are 
distinguished for determining the context increment, five 
different sets of models (each for one category specified in 
the right column of Table V) are used for encoding the 
coded block flag. This results in a total number of 20 differ-
ent probability models for the coded_block_flag bit. 
Significance Map: For encoding the significance map, 
up to 15 different probability models are used for both the 
significant_coeff_flag and the last_significant_coeff_flag. 
The choice of the models and thus the corresponding con-
text index increments χSIG and χLAST depend on the scanning 
position, i.e., for a coefficient coeff[ i ], which is scanned at 
the ith position, the context index increments are determined 
as follows:  
 
χSIG ( coeff[ i ] ) = χLAST ( coeff[ i ] ) = i. 
 
Depending on the maximum number of coefficients 
(MaxNumCoeff) for each context category as given in Table 
V, this results in MaxNumCoeff − 1 different contexts. 
Thus, a total number of 61 different models for both the sig-
nificant_coeff_flag and the last_significant_coeff_flag is 
reserved.  
Level Information: Reverse scanning of the level in-
formation allows a more reliable estimation of the statistics, 
because at the end of the scanning path it is very likely to 
observe the occurrence of successive so-called trailing 1’s, 
i.e., transform coefficient levels with absolute value equal 
to 1. Consequently, for encoding coeff_abs_level_minus1, 
two adequately designed sets of context models are used: 
one for the first bin (with bin index 0) and another one for 
the bins with indices 1 to 13.  
Let NumT1( i ) denote the accumulated number of al-
ready encoded/decoded trailing 1’s, and let NumLgt1( i ) 
denote the accumulated number of encoded/decoded levels 
with absolute value greater than 1, where both counters are 
related to the current scanning position i within the proc-
essed transform coefficient block. Note that both counters 
are initialized with the value of 0 at the beginning of the re-
verse scanning of levels and that the numbers of both count-
ers are monotonically increasing with decreasing scanning 
index i along the backward scanning path. Then, the context 
for the first bin of coeff_abs_level_minus1 is determined by 
the current value NumT1, where the following additional 
rules apply. If more than three past coded coefficients have 
an absolute value of 1, the context index increment of 3 is 
always chosen. When a level with an absolute value greater 
than 1 has been encoded, i.e., when NumLgt1 is greater than 
0, a context index increment of 4 is used for all remaining 
levels of the regarded block. Thus, for encoding the first 
bin, as shown in the second (light gray-shaded) column of 
Table I, the corresponding context index increment 
χAbsLFirstB ( i ) at the scanning position i is given as  
 


 >
=
otherwise  ))(3max(
0)( if 4
)(
,iNumT1,
iNumLgt1,
iAbsLFirstBχ . 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
13 
For encoding the bins with indices 1 to 13 (as shown in 
the dark gray-shaded columns in Table I), the context index 
increment χAbsLRemB ( i ) is determined by NumLgt1 with a 
maximum context index increment of 4, i.e., 
 
χAbsLRemB ( i ) = max ( 4, NumLgt1( i ) ). 
 
For the bins of coeff_abs_level_minus1 with index 
greater than 13 as well as for the sign information co-
eff_sign_flag, the bypass coding mode is used for all block 
types. Thus, the total number of different probability models 
for encoding the level information is 49.10 
Table VI shows an example for the determination of the 
context index increments used for encoding the absolute 
value of levels of significant transform coefficients. Note 
that the transform coefficient levels are processed in reverse 
scanning order, i.e., from the 9th position to the first position 
in scanning order. 
 
TABLE VI 
EXAMPLE FOR DETERMINATION OF CONTEXT INDEX INCREMENTS FOR 
ENCODING THE ABSOLUTE VALUES OF TRANSFORM COEFFICIENT LEVELS 
 
Scanning position 1 2 3 4 5 6 7 8 9 
Transf. coefficient levels 9 0 -5 3 0 0 -1 0 1 
χAbsLFirstB 4  4 2   1  0 
χAbsLRemB 2  1 0      
 
C. Probability Estimation 
As outlined in Section II.C, the basic idea of the new 
multiplication-free binary arithmetic coding scheme for 
H.264/AVC relies on the assumption that the estimated 
probabilities of each context model can be represented by a 
sufficiently limited set of representative values. For 
CABAC, 64 representative probability values 
pσ ∈ [0.01875, 0.5] were derived for the LPS by the follow-
ing recursive equation 
 
.5.0 and 
5.0
01875.0
with        
 63,,1 allfor   
0
63
1
1
=




=
=⋅= −
p
pp
α
σα σσ K
 (8) 
 
Here, both the chosen scaling factor α ≈ 0.95 and the 
cardinality N = 64 of the set of probabilities represent a 
good compromise between the desire for fast adaptation 
(α → 0; small N), on one hand, and the need for a suffi-
ciently stable and accurate estimate (α → 1; larger N), on 
the other hand. Note that unlike e.g. in the MQ coder, there 
is no need to tabulate the representative LPS probability 
values {pσ | 0 ≤ σ ≤ 63} in the CABAC approach. As fur-
 
10 Note that for the chrominance DC blocks, there are only 4 different 
models for the bins of coeff_abs_level_minus1 with indices 1 to 13, since 
at maximum 4 non-zero levels are transmitted.  
ther described below, each probability value pσ is only im-
plicitly addressed in the arithmetic coding engine by its cor-
responding index σ. 
As a result of this design, each context model in CABAC 
can be completely determined by two parameters: its current 
estimate of the LPS probability, which in turn is character-
ized by an index σ between 0 and 63, and its value of MPS 
ϖ being either 0 or 1. Thus, probability estimation in 
CABAC is performed by using a total number of 128 differ-
ent probability states, each of them efficiently represented 
by a 7-bit integer value. In fact, one of the state indices 
(σ = 63) is related to an autonomous, non-adaptive state 
with a fixed value of MPS, which is only used for encoding 
of binary decisions before termination of the arithmetic 
codeword, as further explained below. Therefore, only 126 
probability states are effectively used for the representation 
and adaptation of all (adaptive) context models. 
0.0
0.1
0.2
0.3
0.4
0.5
0 5 10 15 20 25 30 35 40 45 50 55 60
MPS
LPS
state σ
state σ+1
L
P
S 
pr
ob
ab
il
it
y
probability state index σ  
Fig. 6.  LPS probability values and transition rules for updating the prob-
ability estimation of each state after observing a LPS (dashed lines in left 
direction) and a MPS (solid lines in right direction). 
 
1) Update of Probability States 
As already stated above, all probability models in 
CABAC with one exception are (backward) adaptive mod-
els, where an update of the probability estimation is per-
formed after each symbol has been encoded. Actually, for a 
given probability state, the update depends on the state in-
dex and the value of the encoded symbol identified either as 
a LPS or a MPS. As a result of the updating process a new 
probability state is derived, which consists of a potentially 
modified LPS probability estimate and, if necessary, a 
modified MPS value.  
Fig. 6 illustrates the probability values {pσ | 0 ≤ σ ≤ 62} 
for the LPS estimates together with their corresponding 
transition rules for updating the state indices. In the event of 
an MPS, a given state index is simply incremented by 1, 
unless a MPS occurs at state index 62, where the LPS prob-
ability is already at its minimum, or equivalently, the maxi-
mum MPS probability is reached. In the latter case, the state 
index 62 remains fixed until a LPS is seen, in which case the 
state index is changed by decrementing the state index by an 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
14 
amount illustrated by the dashed line in Fig. 6. This rule ap-
plies in general to each occurrence of an LPS with the fol-
lowing exception. Assuming an LPS has been encoded at 
the state with index σ = 0, which corresponds to the equi-
probable case, the state index remains fixed, but the MPS 
value ϖ will be toggled such that the value of the LPS and 
MPS will be interchanged. In all other cases, no matter, 
which symbol has been encoded, the MPS value will not be 
altered. The derivation of the transition rules for the LPS 
probability is based on the following relation between a 
given LPS probability pold and its updated counterpart pnew: 
 
  
occurs  a if    ),1(
occurs  a if  ),,max(
old
62old
new



−+⋅
⋅
=
LPSp
MPSpp
p
αα
α  
 
where the value of α is given as in (8). 
With regard to a practical implementation of the prob-
ability estimation process in CABAC, it is important to note 
that all transition rules can be realized by at most two tables 
each having 63 entries of 6-bit unsigned integer values. Ac-
tually, it is sufficient to provide a single table TransIdxLPS, 
which determines for a given state index σ the new updated 
state index TransIdxLPS[ σ ] in case an LPS has been ob-
served.11 The MPS driven transitions can be obtained by a 
simple (saturated) increment of the state index σ by the 
fixed value of 1 resulting in an updated state index 
min ( σ + 1, 62 ). 
 
2) Initialization and Reset of Probability States 
The basic self-contained unit in H.264/AVC video coding 
is a slice. This fact implies in particular certain restrictions 
on the backward adaptation process of probability models 
as described in the previous paragraph. Since the lifetime of 
the backward adaptation cannot exceed the duration of the 
whole slice encoding process, which in turn may represent a 
substantial amount of the whole adaptation process, all 
models have to be re-initialized at the slice boundaries using 
some pre-defined probability states. In the absence of any 
prior knowledge about the source, one possible choice 
would be to initialize each model with the equi-probable 
state. However, CABAC provides a built-in mechanism for 
incorporating some a priori knowledge about the source sta-
tistics in the form of appropriate initialization values for 
each of the probability models. This so-called initialization 
process for context models allows an adjustment of the ini-
tial probability states in CABAC on two levels. 
Quantization Parameter Dependent Initialization: On 
the lower level of adjustment there is a default set of ini-
tialization values, which are derived from the initially given 
slice quantization parameter SliceQP, thus providing some 
kind of pre-adaptation of the initial probability states to the 
different coding conditions represented by the current value 
of the SliceQP parameter.  
 
11 The specific values of this state transition table can be found in [1]. 
1. σpre =  
max(1, min(126,(( µγ ∗ SliceQP) >> 4) + νγ)) 
2. if( σpre <= 63 ) { 
σ = 63 - σpre 
ϖ = 0  
} else { 
σ = σpre - 64 
ϖ = 1 
} 
 
Fig. 7. SliceQP dependent initialization procedure. First, one of the 
admissible probability states σpre (numbered between 1 and 126) is de-
rived from the given parameters (µγ, νγ) and SliceQP. Then, in a second 
step σpre is translated into the probability state index σ and the value of 
the MPS (ϖ). 
 
Training sequences have been used to fit the initial prob-
ability state of each model to the quantization parameter. By 
using a linear regression, a pair of parameters (µγ, νγ) was 
obtained for each probability model with context index γ 
(0 ≤ γ ≤ 398, γ ≠ 276), from which the corresponding 
SliceQP dependent initial probability state is derived during 
the initialization process by applying the procedure shown 
in Fig. 7. 
Slice Dependent Initialization: The concept of a low-
level pre-adaptation of the probability models was general-
ized by defining two additional sets of context initialization 
parameters for those probability models specifically used in 
P and B slices. In this way, the encoder is enabled to choose 
for these slice types between three initialization tables such 
that a better fit to different coding scenarios and/or different 
types of video content can be achieved.12 This forward ad-
aptation process requires the signaling of the chosen initiali-
zation table, which is done by specifying the corresponding 
table index (0–2) in the slice header. In addition, an in-
creased amount of memory for the storage of the initializa-
tion tables is required. However, access to this memory of 
approx. 3 KB is needed only once per slice. Note that a 
chosen initialization table triggers for each probability 
model the same low-level, SliceQP dependent initialization 
procedure as described in the previous paragraph. Depend-
ing on the slice size and the bit-rate, which, in turn, depends 
on the amount of data that can be used for the backward ad-
aptation process of the symbol statistics, bit-rate savings of 
up to 3% have been obtained by using the instrument of 
slice dependent context initialization [13]. 
 
D. Table-Based Binary Arithmetic Coding 
In this section, we present some more detailed informa-
tion about the binary arithmetic coding engine of 
H.264/AVC. Actually, the CABAC coding engine consists 
of two sub-engines, one for the regular coding mode, which 
includes the utilization of adaptive probability models, and 
another so-called “bypass” coding engine for a fast encod-
 
12 For a description of an example of the non-normative table selection 
process, please refer to [13]. 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
15 
ing of symbols, for which an approximately uniform prob-
ability is assumed to be given. The following presentation 
of the basic features of the CABAC coding engine also in-
volves aspects of renormalization, carry-over control and 
termination.  
 
 
Fig. 8. Flow diagram of the binary arithmetic encoding process including 
the updating process of the probability estimation (in gray shaded boxes) 
for a single bin value (binVal) using the regular coding mode. 
 
1) Interval Subdivision in Regular Coding Mode 
Fig. 8 illustrates the binary arithmetic encoding process 
for a given bin value binVal using the regular coding mode. 
The internal state of the arithmetic encoding engine is as 
usual characterized by two quantities: the current interval 
range R and the base (lower endpoint) L of the current code 
interval. Note, however, that the precision needed to store 
these registers in the CABAC engine (both in regular and 
bypass mode) can be reduced up to 9 and 10 bits, respec-
tively. Encoding of the given binary value binVal observed 
in a context with probability state index σ and value of MPS 
ϖ is performed in a sequence of 4 elementary steps as fol-
lows. 
In the first and major step, the current interval is subdi-
vided according to the given probability estimates. This in-
terval subdivision process involves three elementary opera-
tions as shown in the topmost box of the flow diagram in 
Fig. 8. First, the current interval range R is approximated by 
a quantized value Q( R ) using an equi-partition of the 
whole range 28 ≤ R < 29 into four cells. But instead of using 
the corresponding representative quantized range values Q0, 
Q1, Q2, and Q3 explicitly in the CABAC engine, Q( R ) is 
only addressed by its quantizer index ρ, which can be effi-
ciently computed by a combination of a shift and bit mask-
ing operation, i.e., 
ρ = (R >> 6) & 3.  
 
Then, this index ρ and the probability state index σ are 
used as entries in a 2-D table TabRangeLPS to determine 
the (approximate) LPS related sub-interval range RLPS, as 
shown in Fig. 8. Here the table TabRangeLPS contains all 
64 × 4 pre-computed product values pσ · Qρ for 0 ≤ σ ≤ 63 
and 0 ≤ ρ ≤ 3 in 8-bit precision.13  
Given the dual sub-interval range R − RLPS for the MPS, 
the sub-interval corresponding to the given bin value binVal 
is chosen in the second step of the encoding process. If bin-
Val is equal to the MPS value ϖ, the lower sub-interval is 
chosen so that L is unchanged (right path of the branch in 
Fig. 8); otherwise, the upper sub-interval with range equal 
to RLPS is selected (left branch in Fig. 8). 
In the third step of the regular arithmetic encoding proc-
ess the update of the probability states is performed as de-
scribed in Section III.C (gray shaded boxes in Fig. 8), and 
finally, the fourth step consists of the renormalization of the 
registers L and R (“RenormE” box in Fig. 8) as further de-
scribed below. 
 
 
Fig. 9.  Flow diagram of the binary arithmetic encoding process for a sin-
gle bin value (binVal) using the bypass coding mode. 
 
2) Bypass Coding Mode 
To speedup the encoding (and decoding) of symbols, for 
which R − RLPS ≈ RLPS ≈ R/2 is assumed to hold, the regular 
arithmetic encoding process as described in the previous 
paragraph is simplified to a large extent. First, a “bypass” of 
the probability estimation and update process is established, 
and second, the interval subdivision is performed such that 
two equi-sized sub-intervals are provided in the interval 
subdivision stage. But instead of explicitly halving the cur-
rent interval range R, the variable L is doubled before 
choosing the lower or upper sub-interval depending on the 
value of the symbol to encode (0 or 1, respectively). In this 
way, doubling of L and R in the subsequent renormalization 
is no longer required provided that the renormalization in 
the bypass is operated with doubled decision thresholds (see 
 
13 For the specific values of TabRangeLPS, the reader is referred to [1]. 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
16 
Fig. 9).  
At this point, however, one might argue that ideally no 
arithmetic operation would be required, if the binary sym-
bols to encode would be directly written to the bitstream, 
i.e., if the whole arithmetic coding engine would be by-
passed. Since it is by no means a trivial task to multiplex 
raw bits with an arithmetic codeword without performing 
some kind of termination of the codeword, and since the 
bypass coding mode is intended to be used for symbols, 
which, in general, are not grouped together, this kind of 
“lazy coding mode” as e.g. established in JPEG2000 [35] is 
ruled out in the present context. 
 
3) Renormalization and Carry-Over Control 
 A renormalization operation after interval subdivision is 
required whenever the new interval range R does no longer 
stay within its legal range of [28, 29). Each time renormali-
zation must be carried out one or more bits can be output. 
However, in certain cases, the polarity of the output bits will 
be resolved in subsequent output steps, i.e., carry propaga-
tion might occur in the arithmetic encoder. For the CABAC 
engine the renormalization process and carry-over control 
of [37] was adopted. This implies in particular, that the en-
coder has to resolve any carry propagation by monitoring 
the bits that are outstanding for being emitted. More details 
can be found in [1]. 
 
4) Termination of Arithmetic Code Word 
A special fixed, i.e., non-adapting probability state with 
index σ = 63 was designed such that the associated table en-
tries TabRangeLPS[ 63, ρ ] and hence RLPS are determined 
by a fixed value of 2 regardless of the given quantized range 
index ρ. This guarantees that for the terminating syntax 
element in a slice, which, in general, is given by the LPS 
value of the end of slice flag,14 7 bits of output are produced 
in the related renormalization step. Two more disambiguat-
ing bits are needed to terminate the arithmetic codeword for 
a given slice. By preventing the decoder from performing a 
renormalization after recovering the terminating syntax 
element, the decoder never reads more bits than were actu-
ally produced by the encoder for that given slice. 
 
TABLE VII 
INTERLACED TEST SEQUENCES 
 
Name Resolution Frame rate Duration 
Canoe 720 × 576 25 Hz 6 sec. 
Formula 1 720 × 576 25 Hz 6 sec. 
Rugby 720 × 576 25 Hz 6 sec. 
Mobile & Calendar 720 × 480 30 Hz 6 sec. 
Football 720 × 480 30 Hz 6 sec. 
 
14 Another terminating symbol is given by the LPS value of the bin of 
the macroblock type indicating the PCM mode as further specified in [1]. 
IV. EXPERIMENTAL RESULTS 
In our experiments for evaluating the coding efficiency of 
CABAC, we addressed the coding of television sequences 
for broadcast, a typical target application for the Main pro-
file of H.264/AVC. The set of interlaced standard definition 
sequences used for testing CABAC is listed in Table VII.  
All simulations were performed using the main profile of 
H.264/AVC. An IDR-picture was inserted every 500 milli-
seconds and 2 non-reference B-frames were inserted be-
tween each pair of anchor frames. The motion search was 
conducted in a range of [-32…32] × [-32…32] samples for 
3 reference frames. All encoding mode decisions including 
the motion search, the macroblock mode decision, and the 
macroblock and picture-based frame/field decision were 
performed using the simple and effective Lagrangian coder 
control presented in [38]. Bit-rates were adjusted by using 
fixed values of the quantization parameter (QP) for an entire 
sequence. The value of QP for B pictures was set to 
QP( B ) = QP( I/P ) + 2, where QP( I/P ) is the quantization 
parameter for I and P pictures. 
In our experiments, we compare the coding efficiency of 
CABAC to the coding efficiency of the baseline entropy 
coding method of H.264/AVC. The baseline entropy coding 
method uses the zero-order Exp-Golomb code for all syntax 
elements with the exception of the residual data, which are 
coded using the coding method of Context-Adaptive Vari-
able Length Coding (CAVLC) [1],[2].  
In Fig. 10, the bit-rate savings of CABAC relative to the 
default entropy coding method of H.264/AVC are shown 
against the average PSNR of the luminance component for 
the five interlaced sequences of the test set. It can be seen 
that CABAC significantly outperforms the baseline entropy 
coding method of H.264/AVC for the typical area of target 
applications. For the range of acceptable video quality for 
broadcast application of about 30 to 38 dB and averaged 
over all tested sequences, bit-rate savings of 9 to 14% are 
achieved, where higher gains are obtained at lower rates. 
24 26 28 30 32 34 36 38 40 42
0
2
4
6
8
10
12
14
16
18
20
22
PSNR [dB]
B
it-
ra
te
 s
av
in
gs
 [%
]
Canoe            
Formula 1        
Rugby            
Mobile & Calendar
Football         
Average 
 
Fig. 10. Bit-rate savings provided by CABAC relative to the baseline en-
tropy coding method CAVLC of H.264/AVC. 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
17 
V. CONCLUSION 
The CABAC design is based on the key elements of bi-
naization, context modeling, and binary arithmetic coding. 
Binarization enables efficient binary arithmetic coding via a 
unique mapping of non-binary syntax elements to a se-
quence of bits, which are called bins. Each bin can either be 
processed in the regular coding mode or the bypass mode. 
The latter is chosen for selected bins in order to allow a 
speedup of the whole encoding (and decoding) process by 
means of a simplified non-adaptive coding engine without 
the usage of probability estimation. The regular coding 
mode provides the actual coding benefit, where a bin may 
be context modeled and subsequently arithmetic encoded. 
As a design decision, in general only the most probable bin 
of a syntax element is context modeled using previously 
coded/decoded bins. Moreover, all regular coded bins are 
adapted by estimating their actual pdf. 
The estimation of the pdf and the actual binary arithmetic 
coding/decoding is conducted using an efficient table-based 
approach. This multiplication-free method enables efficient 
implementations in hardware and software.  
The entropy coding method of Context-based Adaptive 
Binary Arithmetic Coding (CABAC) is part of the Main 
profile of H.264/AVC [1] and may find its way into video 
streaming, broadcast, or storage applications within this 
profile. Experimental results have shown the superior per-
formance of CABAC in comparison to the baseline entropy 
coding method of VLC/CAVLC. For typical test sequences 
in broadcast applications, averaged bit-rate savings of 9 to 
14% corresponding to a range of acceptable video quality of 
about 30 to 38 dB were obtained. 
 
REFERENCES 
[1] T. Wiegand (Ed.), “Draft ITU-T Recommendation H.264 and Draft 
ISO/IEC 14496-10 AVC,” Joint Video Team of ISO/IEC 
JTC1/SC29/WG11 & ITU-T SG16/Q.6 Doc. JVT-G050, Pattaya, 
Thailand, March 2003. 
[2] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra, “Over-
view of the H.264/AVC video coding standard,” this issue. 
[3] ITU-T and ISO/IEC JTC1, “Generic coding of moving pictures and 
associated audio information – Part 2: Video”, ITU-T Recommenda-
tion H.262 – ISO/IEC 13818-2 (MPEG-2), Nov. 1994. 
[4] ITU-T, “Video coding for low bitrate communications”, ITU-T Rec-
ommendation H.263; version 1, Nov. 1995; version 2, Jan. 1998. 
[5] ISO/IEC JTC1, “Coding of audio-visual objects – Part 2: Visual”, 
ISO/IEC 14496-2 (MPEG-4 Visual version 1), Apr. 1999; Amend-
ment 1 (version 2), Feb. 2000; Amendment 4 (streaming profile), 
Jan. 2001. 
[6] C. A. Gonzales, “DCT coding of motion sequences including arith-
metic coder”, ISO/IEC JCT1/SC2/WP8, MPEG 89/187, Aug. 1989. 
[7] D. Marpe, G. Blättermann, and T. Wiegand, “Adaptive codes for 
H.26L,” ITU-T SG16/Q.6 Doc. VCEG-L13, Eibsee, Germany, Jan. 
2001. 
[8] D. Marpe, G. Blättermann, G. Heising, and T. Wiegand, “Further re-
sults for CABAC entropy coding scheme,” ITU-T SG16/Q.6 Doc. 
VCEG-M59, Austin, USA, March 2001. 
[9] D. Marpe, G. Blättermann, and T. Wiegand, “Improved CABAC,” 
ITU-T SG16/Q.6 Doc. VCEG-O18, Pattaya, Thailand, Dec. 2001. 
[10] D. Marpe, G. Blättermann, T. Wiegand, R. Kurceren, M. Karcze-
wicz, and J. Lainema, “New results on improved CABAC,” Joint 
Video Team of ISO/IEC JTC1/SC29/WG11 & ITU-T SG16/Q.6 
Doc. JVT-B101, Geneva, Switzerland, Feb. 2002. 
[11] H. Schwarz, D. Marpe, G. Blättermann, and T. Wiegand, “Improved 
CABAC,” Joint Video Team of ISO/IEC JTC1/SC29/WG11 & ITU-
T SG16/Q.6 Doc. JVT-C060, Fairfax, USA, March 2002. 
[12] D. Marpe, G. Heising, G. Blättermann, and T. Wiegand, “Fast arith-
metic coding for CABAC,” Joint Video Team of ISO/IEC 
JTC1/SC29/WG11 & ITU-T SG16/Q.6 Doc. JVT-C061, Fairfax, 
USA, March 2002. 
[13] H. Schwarz, D. Marpe, and T. Wiegand, “CABAC and slices,” Joint 
Video Team of ISO/IEC JTC1/SC29/WG11 & ITU-T SG16/Q.6 
Doc. JVT-D020, Klagenfurt, Austria, July 2002. 
[14] M. Karczewicz, “Analysis and simplification of intra prediction,” 
Joint Video Team of ISO/IEC JTC1/SC29/WG11 & ITU-T 
SG16/Q.6 Doc. JVT-D025, Klagenfurt, Austria, July 2002. 
[15] D. Marpe, G. Blättermann, G. Heising, and T. Wiegand, “Proposed 
cleanup changes for CABAC,” Joint Video Team of ISO/IEC 
JTC1/SC29/WG11 & ITU-T SG16/Q.6 Doc. JVT-E059, Geneva, 
Switzerland, Oct. 2002. 
[16] F. Bossen, “CABAC cleanup and complexity reduction”, Joint Video 
Team of ISO/IEC JTC1/SC29/WG11 & ITU-T SG16/Q.6 Doc. JVT-
E086, Geneva, Switzerland, Oct. 2002. 
[17] D. Marpe, H. Schwarz, G. Blättermann, and T. Wiegand, “Final 
CABAC cleanup,” Joint Video Team of ISO/IEC JTC1/SC29/WG11 
& ITU-T SG16/Q.6 Doc. JVT-F039, Awaji, Japan, Dec. 2002. 
[18] D. Marpe and H. L. Cycon: “Very Low Bit-Rate Video Coding Us-
ing Wavelet-Based Techniques,” IEEE Trans. on Circ. and Systems 
for Video Technology, vol. 9, no. 1, pp. 85-94, April 1999. 
[19] G. Heising, D. Marpe, H. L. Cycon and A. P. Petukhov, “Wavelet-
Based Very Low Bit-Rate Video Coding Using Image Warping and 
Overlapped Block Motion Compensation,” IEE Proceedings - Vi-
sion, Image and Signal Proc., vol. 148, no. 2, pp. 93-101, April 
2001. 
[20] S. -J. Choi and J. W. Woods, “Motion-compensated 3-D subband 
coding of video,” IEEE Trans. on Image Proc., vol. 8, no. 2, pp. 
155-167, Feb. 1999. 
[21] A. Said and W. Pearlman, “A new, fast, and efficient image codec 
based on set partitioning in hierarchical trees,” IEEE Trans. on Circ. 
and Systems for Video Technology, vol. 6, no. 3, pp. 243-250, June 
1996. 
[22] D. Marpe and H. L. Cycon, “Efficient pre-coding techniques for 
wavelet-based image compression,” Proc. Picture Coding Sympo-
sium 1997, pp. 45-50, 1997. 
[23] J. Rissanen and G. G. Langdon, Jr., “Universal modeling and cod-
ing,” IEEE Trans. on Inf. Theory, vol. 27, no. 1, pp. 12-23, Jan. 
1981. 
[24] J. Rissanen, “Universal coding, information, prediction, and estima-
tion,” IEEE Trans. on Inf. Theory, vol. 30, no. 4, pp. 629-636, July 
1984.  
[25] W. B. Pennebaker and J. L. Mitchell, JPEG: Still Image Data Com-
pression Standard, Van Nostrand Reinhold, New York, USA, 1993. 
[26] A. Papoulis, Probability, Random Variables, and Stochastic Proc-
esses, McGraw-Hill, New York, USA, pp. 37-38, 1984. 
[27] M. J. Weinberger, J. Rissanen, and R. B. Arps; “Application of uni-
versal context modeling to lossless compression of gray-scale im-
ages,” IEEE Trans. on Image Processing, vol. 5, no. 4, pp. 575-586, 
April 1996. 
[28]  A. N. Netravali and B. G. Haskell, Digital Pictures, Representation 
and Compression, New York, USA, Plenum Press, 1988. 
[29] J. Teuhola, “A compression method for clustered bit-vectors,” In-
formation Processing Letters, vol. 7, pp. 308-311, Oct. 1978. 
[30] R. Gallager and D. Van Voorhis, “Optimal source codes for geomet-
rically distributed integer alphabets,” IEEE Trans. Inf. Theory, vol. 
21, no. 3, pp. 228-230, March 1975. 
[31] M. Mrak, D. Marpe, and T. Wiegand, “A context modeling algo-
rithm and its application in video compression,” to be presented at 
IEEE Int. Conf. Image Proc. (ICIP), Barcelona, Spain, Sept. 2003. 
[32] W. B. Pennebaker, J. L. Mitchell, G. G. Langdon, and R. B. Arps, 
“An overview of the basic principles of the Q-Coder adaptive binary 
arithmetic coder”, IBM J. Res. Dev., vol. 32, pp. 717-726, 1988. 
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. X, NO. Y, MONTH 2003 
 
18 
[33] J. Rissanen and K. M. Mohiuddin, “A multiplication-free multial-
phabet arithmetic code,” IEEE Trans. Commun., vol. 37, pp. 93-98, 
Feb. 1989. 
[34] P. G. Howard and J. S. Vitter, “Practical implementations of arith-
metic coding,” in Image and Text Compression, J. A. Storer (Ed.), 
Kluwer, 1992, pp. 85-112. 
[35] D. Taubman and M. W. Marcellin, JPEG2000 Image Compression: 
Fundamentals, Standards and Practice, Kluwer Academic Publish-
ers, 2002. 
[36] D. Marpe and T. Wiegand, “A highly efficient multiplication-free 
binary arithmetic coder and its application in video coding,” to be 
presented at IEEE int. Conf. Image Proc. (ICIP), Barcelona, Spain, 
Sept. 2003. 
[37]  A. Moffat, R. M. Neal, and I. H. Witten, “Arithmetic coding revis-
ited”, Proc. IEEE Data Compression Conference, Snowbird (USA), 
pp. 202-211, 1996. 
[38] T. Wiegand, H. Schwarz, A. Joch, F. Kossentini, and G. J. Sullivan, 
“Rate-constrained coder control and comparison of video coding 
standards,” this issue. 
 
 
Detlev Marpe (M’00) received the Dipl.-Math. 
degree with highest honors from the Technical 
University Berlin (TUB), Germany, in 1990. 
From 1991 to 1993, he was a Research and 
Teaching Assistant in the Department of Mathe-
matics at TUB. Since 1994, he has been involved 
in several industrial and research projects in the 
area of still image coding, image processing, 
video coding and video streaming. In 1999, he 
joined the Fraunhofer-Institute for Communica-
tions HHI, Berlin, Germany, where as a Project 
Leader in the Image Processing Department, he is currently responsible for 
projects focused on the development of advanced video coding and video 
transmission technologies. 
He has published more than 30 journal and conference articles in the 
area of image and video processing, and he holds several international pat-
ents. He has been involved in the ITU-T and ISO/IEC standardization ac-
tivities for still image and video coding, to which he contributed more 
than 40 input documents. From 2001 to 2003, as an Ad-hoc Group Chair-
man in the Joint Video Team of ITU-T VCEG and ISO/IEC MPEG, he 
was responsible for the development of the CABAC entropy coding 
scheme within the H.264/AVC standardization project. His research inter-
ests include still image and video coding, image and video communication 
as well as computer vision and information theory. 
As a Cofounder of daViKo GmbH, a Berlin-based start-up company 
focused on developing server-less multipoint videoconferencing products 
for Intranet or Internet collaboration, Mr. Marpe received the Prime Prize 
of the 2001 Multimedia Start-up Competition from the German Federal 
Ministry of Economics and Technology.  
 
Heiko Schwarz received the Dipl.-Ing. degree in 
electrical engineering from the University of 
Rostock, Germany, in 1996 and the Dr.-Ing. de-
gree from the University of Rostock in 2000. In 
1999, he joined the Fraunhofer-Institute for 
Communications HHI, Berlin, Germany. His re-
search interests include image and video com-
pression, video communication as well as signal 
processing. 
 
 
Thomas Wiegand is the head of the Image 
Communication Group in the Image Processing 
Department of the Heinrich Hertz Institute Berlin, 
Germany. He received the Dr.-Ing. degree from 
the University of Erlangen-Nuremberg, Germany, 
in 2000 and the Dipl.-Ing. degree in Electrical 
Engineering from the Technical University of 
Hamburg-Harburg, Germany, in 1995.  
From 1993 to 1994, he was a Visiting Re-
searcher at Kobe University, Japan. In 1995, he 
was a Visiting Scholar at the University of California at Santa Barbara, 
USA, where he started his research on video compression and transmis-
sion. Since then he has published several conference and journal papers on 
the subject and has contributed successfully to the ITU-T Video Coding 
Experts Group (ITU-T SG16 Q.6 - VCEG) / ISO/IEC Moving Pictures 
Experts Group (ISO/IEC JTC1/SC29/WG11 - MPEG) / Joint Video Team 
(JVT) standardization efforts and holds various international patents in 
this field. From 1997 to 1998, he has been a Visiting Researcher at Stan-
ford University, USA, and served as a consultant to 8x8, Inc., Santa Clara, 
CA, USA.  
In October 2000, he has been appointed as the Associated Rapporteur 
of the ITU-T VCEG. In December 2001, he has been appointed as the 
Associated Rapporteur / Co-Chair of the JVT that has been created by 
ITU-T VCEG and ISO/IEC MPEG for finalization of the H.264/AVC 
video coding standard. In February 2002, he has been appointed as the 
Editor of the H.264/AVC video coding standard. 

