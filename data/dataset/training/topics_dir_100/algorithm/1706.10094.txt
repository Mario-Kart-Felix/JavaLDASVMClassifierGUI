ar
X
iv
:1
70
6.
10
09
4v
1 
 [
cs
.D
S]
  3
0 
Ju
n 
20
17
Time-Space Trade-Offs for Lempel-Ziv Compressed Indexing∗
Philip Bille†
phbi@dtu.dk
Mikko Berggren Ettienne‡
miet@dtu.dk
Inge Li Gørtz†
inge@dtu.dk
Hjalte Wedel Vildhøj
hwvi@dtu.dk
Abstract
Given a string S, the compressed indexing problem is to preprocess S into a compressed
representation that supports fast substring queries. The goal is to use little space relative to
the compressed size of S while supporting fast queries. We present a compressed index based
on the Lempel-Ziv 1977 compression scheme. Let n, and z denote the size of the input string,
and the compressed LZ77 string, respectively. We obtain the following time-space trade-offs.
Given a pattern string P of length m, we can solve the problem in
(i) O(m+ occ lg lg n) time using O(z lg(n/z) lg lg z) space, or
(ii) O(m(1 + lg
ǫ z
lg(n/z)
) + occ(lg lgn+ lgǫ z)) time using O(z lg(n/z)) space, for any 0 < ǫ < 1
In particular, (i) improves the leading term in the query time of the previous best solution
from O(m lgm) to O(m) at the cost of increasing the space by a factor lg lg z. Alternatively,
(ii) matches the previous best space bound, but has a leading term in the query time of
O(m(1 + lg
ǫ z
lg(n/z)
)). However, for any polynomial compression ratio, i.e., z = O(n1−δ), for
constant δ > 0, this becomes O(m). Our index also supports extraction of any substring of
length ℓ in O(ℓ + lg(n/z)) time. Technically, our results are obtained by novel extensions
and combinations of existing data structures of independent interest, including a new batched
variant of weak prefix search.
1 Introduction
Given a string S, the compressed indexing problem is to preprocess S into a compressed repre-
sentation that supports fast substring queries, that is, given a string P , report all occurrences
of substrings in S that match P . Here the compressed representation can be any compression
scheme or measure (kth order entropy, smallest grammar, Lempel-Ziv, etc.). The goal is to use
little space relative to the compressed size of S while supporting fast queries. Compressed in-
dexing is a key computational primitive for querying massive data sets and the area has received
significant attention over the last decades with numerous theoretical and practical solutions, see
e.g. [26, 14, 30, 24, 15, 16, 22, 23, 17, 35, 31, 11, 28, 20, 25, 5] and the surveys [35, 33, 34, 21].
The Lempel-Ziv 1977 compression scheme (LZ77) [39] is a classic compression scheme based
on replacing repetitions by references in a greedy left-to-right order. Numerous variants of LZ77
have been developed and several widely used implementations are available (such as gzip [1]).
Recently, LZ77 has been shown to be particularly effective at handling highly-repetitive data
sets [31, 33, 28, 10, 4] and LZ77 compression is always at least as powerful as any grammar
representation [38, 9].
In this paper, we consider compressed indexing based on LZ77 compression. Relatively few
results are known for this version of the problem. Let n, z, and m denote the size of the input
string, the compressed LZ77 string, and the pattern string, respectively. Kärkkäinen and Ukkonen
introduced the problem in 1996 [26] and gave an initial solution that required read-only access to
∗A preliminary version of this paper appeared in the Proceedings of the 28th Annual symposium on Combinatorial
Pattern Matching, 2017
†Supported by the Danish Research Council (DFF – 4005-00267, DFF – 1323-00178)
‡Supported by the Danish Research Council (DFF – 4005-00267)
1
the uncompressed text. Interestingly, this work is among the first results in compressed index-
ing [35]. More recently, Gagie et al. [19, 20] revisited the problem and gave a solution using space
O(z lg(n/z)) and query time O(m lgm + occ lg lg n), where occ is the number of occurrences of P
in S. Note that these bounds assume a constant sized alphabet.
1.1 Our Results
We show the following main result.
Theorem 1. Given a string S of length n from a constant sized alphabet compressed using LZ77
into a string of length z we can build a compressed-index supporting substring queries in:
(i) O(m + occ lg lgn) time using O(z lg(n/z) lg lg z) space, or
(ii) O(m(1 + lg
ǫ z
lg(n/z) ) + occ(lg lgn + lg
ǫ z)) time using O(z lg(n/z)) space, for any 0 < ǫ < 1
Compared to the previous bounds Thm. 1 obtains new interesting trade-offs. In particular,
Thm. 1 (i) improves the leading term in the query time of the previous best solution from O(m lg m)
to O(m) at the cost of increasing the space by only a factor lg lg z. Alternatively, Thm. 1 (ii)
matches the previous best space bound, but has a leading term in the query time of O(m(1 +
lgǫ z
lg(n/z) )). However, for any polynomial compression ratio, i.e., z = O(n
1−δ), for constant δ > 0,
this becomes O(m).
Gagie et al. [20] also showed how to extract an arbitrary substring of S of length ℓ in time
O(ℓ+ lgn). We show how to support the same extraction operation and slightly improve the time
to O(ℓ + lg(n/z)).
Technically, our results are obtained by new variants and extensions of existing data structures
in novel combinations. In particular, we consider a batched variant of the weak prefix search problem
and give the first non-trivial solution to it. We also generalize the well-known bidirectional compact
trie search technique [29] to reduce the number of queries at the cost of increasing space. Finally,
we show how to combine this efficiently with range reporting and fast random-access in a balanced
grammar leading to the result.
As mentioned all of the above bounds hold for a constant size alphabet. However, Thm. 1 is an
instance of full time-space trade-off that also supports general alphabets. We discuss the details
in Sec. 8.
2 Preliminaries
We assume a standard unit-cost RAM model with word size w = Θ(lg n) and that the input is from
an integer alphabet Σ = {1, 2, . . . , nO(1)} and measure space complexity in words unless otherwise
specified.
A string S of length n = |S| is a sequence S[1] . . . S[n] of n characters drawn from Σ. The
string S[i] . . . S[j] denoted S[i, j] is called a substring of S. ǫ is the empty string and S[i, i] = S[i]
while S[i, j] = ǫ when i > j. The substrings S[1, i] and S[j, n] are the ith prefix and the jth suffix
of S respectively. The reverse of the string S is denoted rev(S) = S[n]S[n− 1] . . . S[1]. We use the
results from Fredman et al. [18] when referring to perfect hashing allowing us to build a dictionary
on O(k) constant sized keys in O(k) expected time supporting constant time lookups.
2.1 Compact Tries
A trie for a set D of k strings is a rooted tree where the vertices corresponds to the prefixes of the
strings in D. str(v) denotes the prefix corresponding to the vertex v. str(v) = ǫ if v is the root
while v is the parent of u if str(v) is equal to str(u) without the last character. This character is
then the label of the edge from u to v. The depth of vertex v is the number of edges on the path
from v to the root.
We assume each string in D is terminated by a special character $ /∈ Σ such that each string
in D corresponds to a leaf. The children of each vertex are sorted from left to right in increasing
2
lexicographical order, and therefore the left to right order of the leaves corresponds to the lexi-
cographical order of the strings in D. Let rank(s) denote the rank of the string s ∈ D in this
order.
A compact trie for D denoted TD is obtained from the trie by removing all vertices v with
exactly one child excluding the root and replacing the two edges incident to v with a single edge
from its parent to its child. This edge is then labeled with the concatenation of the edge labels it
replaces, thus the edges of a compact trie may be labeled by strings. The skip interval of a vertex
v ∈ TD with parent u is (|str(u)|, |str(v)|] denoted skip(v) and skip(v) = ∅ if v is the root. The
locus of a string s in TD, denoted locus(s), is the minimum depth vertex v such that s is a prefix
of str(v). If there is no such vertex, then locus(s) = ⊥.
In order to reduce the space used by TD we only store the first character of every edge and
in every vertex v we store |str(v)| (This variation is also known as a PATRICIA tree [32]). We
navigate TD by storing a dictionary in every internal vertex mapping the first character of the
label of an edge to the respective child. The size of TD is O(k).
2.2 Karp-Rabin Fingerprints
A Karp-Rabin fingerprinting function [27] is a randomized hash function for strings. We use a
variation of the original definition appearing in Porat and Porat [37]. The fingerprint for a string
S of length n is defined as:
φ(S) =
n∑
i=1
S[i] · ri−1 mod p
where p is a prime and r is a random integer in Zp (the field of integers modulo p). Storing the
values n, rn mod p and r−n mod p along with a fingerprint allows for efficient composition an
subtraction of fingerprints:
Lemma 1 (Porat and Porat [37], Breslauer and Galil [7]). Let x, y, z be strings such that x = yz.
Given two of the three fingerprints φ(x), φ(y), φ(z), the third can be computed in constant time.
It follows that we can compute and store the fingerprints of each of the prefixes of a string S
of length n in O(n) time and space such that we afterwards can compute the fingerprint of any
substring S[i, j] in constant time. We say that the fingerprints of the strings x and y collide when
φ(x) = φ(y) and x 6= y. A fingerprinting function φ is collision-free for a set of strings if there are
no fingerprint collisions between any of the strings.
Lemma 2 (Porat and Porat [37]). Let x and y be different strings of length at most n and let
p = Θ(n2+α) for some α > 0. The probability that φ(x) = φ(y) is less than 1/n1+α.
2.3 Range Reporting
Let X ⊆ {0, . . . , u}d be a set of points in a d-dimensional grid. The orthogonal range reporting
problem in d-dimensions is to compactly represent X while supporting range reporting queries,
that is, given a rectangle R = [a1, b2]× · · · × [ad, bd] report all points in the set R∩X . We use the
following results for 2-dimensional range reporting:
Lemma 3 (Chan et al. [8]). For any set of n points in [0, u]×[0, u] and 2 ≤ B ≤ lgǫ n, 0 < ǫ < 1 we
can solve 2-d orthogonal range reporting with O(n lg n) expected preprocessing time, O(n lgB lg n)
space and (1+k) ·O(B lg lg u) query time where k is the number of occurrences inside the rectangle.
2.4 LZ77
The Ziv-Lempel algorithm from 1977 [39] provides a simple and natural way to compress strings.
The LZ77 parse of a string S of length n is a sequence Z of z subsequent substrings of S called
phrases such that S = Z[1]Z[2], . . . , Z[z]. Z is constructed in a left to right pass of S: Assume that
we have found the sequence Z[1, i] producing the string S[1, j−1] and let S[j, j′−1] be the longest
prefix of S[j, n− 1] that is also a substring of S[1, j′ − 2]. Then Z[i+ 1] = S[j, j′]. The occurrence
of S[j, j′ − 1] in S[1, j′ − 2] is called the source of the phrase Z[i]. Thus a phrase is composed by
3
the contents of its possibly empty source and a trailing character which we call the phrase border
and is typically represented as a triple Z[i] = (start, len, c) where start is the starting position of
the source, len is the length of the source and c ∈ Σ is the border. For a phrase Z[i] = S[j, j′] we
denote the position of its border by border(Z[i]) = j′ and its source by source(Z[i]) = S[j, j′ − 1].
For example, the string abcabcabc . . . abc of length n has the LZ77 parse |a|b|c|abcabcabc . . . abc| of
length 4 which is represented as Z = (0, 0, a)(0, 0, b)(0, 0, c)(0, n− 4, c).
3 Prefix Search
The prefix search problem is to preprocess a set of strings such that later, we can find all the
strings in the set that are prefixed by some query string. Belazzougui et al. [3] consider the weak
prefix search problem, a relaxation of the prefix search problem where we are only requested to
output the ranks (in lexicographic order) of the strings that are prefixed by the query pattern and
we only require no false negatives. Thus we may answer arbitrarily when no strings are prefixed
by the query pattern.
Lemma 4 (Belazzougui et al. [3], appendix H.3). Given a set D of k strings with average length
l, from an alphabet of size σ, we can build a data structure using O(k(lg l + lg lg σ)) bits of space
supporting weak prefix search for a pattern P of length m in O(m lg σ/w + lgm) time where w is
the word size.
The term m lg σ/w stems from preprocessing P with an incremental hash function such that
the hash of any substring P [i, j] can be obtained in constant time afterwards. Therefore we can
do weak prefix search for h substrings of P in O(m lg σ/w + h lgm) time. We now describe a data
structure that builds on the ideas from Lemma 4 but obtains the following:
Lemma 5. Given a set D of k strings, we can build a data structure taking O(k) space supporting
weak prefix search for h substrings of a pattern P of length m in time O(m+h(m/x+ lg x)) where
x is a positive integer.
If we know h when building our data structure, we set x to h and obtain a query time of
O(m + h lg h) with Lemma 5.
Before describing our data structure we need the following definition: The 2-fattest number
in a nonempty interval of strictly positive integers is the number in the interval whose binary
representation has the highest number of trailing zeroes.
3.1 Data Structure
Let TD be the compact trie representing the set D of k strings and let x be a positive integer.
Denote by fat(v) the 2-fattest number in the skip interval of a vertex v ∈ TD. The fat prefix of v
is the length fat(v) prefix of str(v). Denote by Dfat the set of fat prefixes induced by the vertices
of TD. The x-prefix of v is the shortest prefix of str(v) whose length is a multiple of x and is in
the interval skip(v). If v’s skip interval does not span a multiple of x, then v has no x-prefix. Let
Dx be the set of x-prefixes induced by the vertices of TD. The data structure is the compact trie
TD augmented with:
• A fingerprinting function φ.
• A dictionary G mapping the fingerprints of the strings in Dfat to their associated vertex.
• A dictionary H mapping the fingerprints of the strings in Dx to their associated vertex.
• For every vertex v ∈ TD we store the rank in D of the string represented by the leftmost
and rightmost leaf in the subtree of v, denoted lv and rv respectively.
The data structure is similar to the one by Belazzougui et al. [3] except for the dictionary H, which
we use in the first step of our search.
There are at most k strings in each of Dfat and Dx thus the total space of the data structure
is O(k).
4
Let i be the start of the skip interval of some vertex v ∈ TD and define the pseudo-fat numbers
of v to be the set of 2-fattest numbers in the intervals [i, p] where i ≤ p < fat(v). We use Lemma 2
to find a fingerprinting function φ that is collision-free for the strings in Dfat, the strings in Dx
and all the length l-prefixes of the strings in D where l is a pseudo-fat number in the skip interval
of some vertex v ∈ TD.
Observe that the range of strings in D that are prefixed by some pattern P of length m is
exactly [lv, rv] where v = locus(P ). Answering a weak prefix search query for P is comprised by
two independent steps. First step is to find a vertex v ∈ TD such that str(v) is a prefix of P and
m − |str(v)| ≤ x. We say that v is in x-range of P . Next step is to apply a slightly modified
version of the search technique from Belazzougui et al. [3] to find the exit vertex for P , that is,
the deepest vertex v′ ∈ TD such that str(v′) is a prefix of P . Having found the exit vertex we can
find the locus in constant time as it is either the exit vertex itself or one of its children.
3.2 Finding an x-range Vertex
We now describe how to find a vertex in x-range of P . If m < x we simply report that the root
of TD is in x-range of P . Otherwise, let v be the root of TD and for i = 1, 2, . . . ⌊m/x⌋ we check
if ix > |str(v)| and φ(P [1, ix]) is in H in which case we update v to be the corresponding vertex.
Finally, if |str(v)| ≥ m we report that v is locus(P ) and otherwise we report that v is in x-range
of P . In the former case, we report [lv, rv] as the range of strings in D prefixed by P . In the latter
case we pass on v to the next step of the algorithm.
We now show that the algorithm is correct when P prefixes a string in D. It is easy to verify
that the x-prefix of v prefixes P at all time during the execution of the algorithm. Assume that
|str(v)| ≥ m by the end of the algorithm. We will show that in that case v = locus(P ), i.e., that v
is the highest node prefixed by P . Since P prefixes a string in D, the x-prefix of v prefixes P , and
|str(v)| ≥ m, then P prefixes v. Since the x-prefix of v prefixes P , P does not prefix the parent of
v and thus v is the highest node prefixed by P .
Assume now that |str(v)| < m. We will show that v is in x-range of P . Since P prefixes a string
in D and the x-prefix of v prefixes P , then str(v) prefixes P . Let P [1, ix] be the x-prefix of v. Since
v is returned, either φ(P [1, jx]) 6∈ H or jx ≤ |str(v)| for all i < j ≤ ⌊m/x⌋. If φ(P [1, jx]) 6∈ H then
P [1, jx] is not a x-prefix of any node in TD. Since P prefixes a string in D this implies that jx is
in the skip interval of v, i.e., jx ≤ |str(v)|. This means that jx ≤ |str(v)| for all i < j ≤ ⌊m/x⌋.
Therefore ⌊m/x⌋x ≤ |str(v)| < m and it follows that m − |str(v)| < x. We already proved that
str(v) prefixes P and therefore v is in x-range of P .
In case P does not prefix any string in D we either report that v = locus(P ) even though
locus(P ) = ⊥ or report that v is in x-range of P because m − |str(v)| ≤ x even though str(v) is
not a prefix of P due to fingerprint collisions. This may lead to a false positive. However, false
positives are allowed in the weak prefix search problem.
Given that we can compute the fingerprint of substrings of P in constant time the algorithm
uses O(m/x) time.
3.3 From x-range to Exit Vertex
We now consider how to find the exit vertex of P hereafter denoted ve. The algorithm is similar
to the one presented in Belazzougui et al. [3] except that we support starting the search from not
only the root, but from any ancestor of ve.
Let v be any ancestor of ve, let y be the smallest power of two greater than m−|str(v)| and let
z be the largest multiple of y no greater than |str(v)|. The search progresses by iteratively halving
the search interval while using G to maintain a candidate for the exit vertex and to decide in which
of the two halves to continue the search.
Let vc be the candidate for the exit vertex and let l and r be the left and right boundary for
our search interval. Initially vc = v, l = z and r = z + 2y. When r − l = 1, the search terminates
and reports vc. In each iteration, we consider the mid b = (l+ r)/2 of the interval [l, r] and update
the interval to either [b, r] or [l, b]. There are three cases:
1. b is out of bounds
5
(a) If b > m set r to b.
(b) If b ≤ |str(vc)| set l to b.
2. P [1, b] ∈ Dfat, let u be the corresponding vertex, i.e. G(φ(P [1, b])) = u.
(a) If |str(u)| < m, set vc to u and l to b.
(b) If |str(u)| ≥ m, report u = locus(P ) and terminate.
3. P [1, b] /∈ Dfat and thus φ(P [1, b]) is not in G, set r to b.
Observe that we are guaranteed that all fingerprint comparisons are collision-free in case P
prefixes a string in D. This is because the length of the prefix fingerprints we consider are all
either 2-fattest or pseudo-fat in the skip interval of locus(P ) or one of its ancestors and we use a
fingerprinting function that is collision-free for these strings.
3.3.1 Correctness
We now show that the invariant l ≤ |str(vc)| ≤ |str(ve)| < r is satisfied and that str(vc) is a prefix
of P before and after each iteration. After O(lg x) iterations r − l = 1 and thus l = |str(ve)| =
|str(vc)| and therefore vc = ve. Initially vc is an ancestor of ve and thus str(vc) is a prefix of P ,
l = z ≤ |str(vc)| and r = z + 2y > m > |str(ve)| so the invariant is true. Now assume that the
invariant is true at the beginning of some iteration and consider the possible cases:
1. b is out of bounds
(a) b > m then because |str(ve)| ≤ m, setting r to b preserves the invariant.
(b) b ≤ |str(vc)| then setting l to b preserves the invariant.
2. P [1, b] ∈ Dfat, let u = G(φ(P [1, b])).
(a) |str(u)| ≤ m then str(u) is a prefix of P and thus b = fat(u) ≤ |str(u)| ≤ |str(ve)| so
setting l to b and vc to u preserves the invariant.
(b) |str(u)| ≥ m yet u = G(φ(P [1, b])). Then u is the locus of P .
3. P [1, b] /∈ Dfat, and thus φ(P [1, b]) is not in G. As we are not in any of the out of bounds
cases we have |str(vc)| < b < m. Thus, either b > |str(ve)| and setting r to b preserves the
invariant. Otherwise b ≤ |str(ve)| and thus b must be in the skip interval of some vertex u on
the path from vc to ve excluding vc. But skip(u) is entirely included in (l, r) and because b is
2-fattest in (l, r)1 it is also 2-fattest in skip(u). It follows that fat(u) = b which contradicts
P [1, b] /∈ Dfat and thus the invariant is preserved.
Thus if P prefixes a string in D we find either the exit vertex ve or the locus of P . In the
former case the locus of P is the child of ve identified by the character P [|str(v′)| + 1]. Having
found the vertex u = locus(P ) we report [lu, ru] as the range of strings in D prefixed by P . In
case P does not prefix any strings in D, the fact that the fingerprint of a prefix of P match the
fingerprint of some fat prefix in Dx does not guarantee equality of the strings. There are two
possible consequences of this. Either the search successfully finds what it believes to be the locus
of P even though locus(P ) = ⊥ in which case we report a false positive. Otherwise, there is no
child identified by P [|str(v′)| + 1] in which case we can correctly report that no strings in D are
prefixed by S, a true negative. Recall that false positives are allowed as we are considering the
weak prefix search problem.
3.3.2 Complexity
The size of the interval [l, r] is halved in each iteration, thus we do at most O(lg(m − |str(v)|))
iterations, where v is the vertex from which we start the search. If we use the technique from the
previous section to find a starting vertex in x-range of P , we do O(lg x) iterations. Each iteration
takes constant time. Note that if P does not prefix a string in D we may have fingerprint collisions
and we may be given a starting vertex v such that str(v) does not prefix P . This can lead to a
false positive, but we still have m−|str(v)| ≤ x and therefore the time complexity remains O(lg x).
1If b− a = 2i, i > 0 and a is a multiple of 2i−1 then the mid of the interval (a+ b)/2 is 2-fattest in (a, b).
6
3.4 Multiple Substrings
In order to answer weak prefix search queries for h substrings of a pattern P of length m, we first
preprocess P in O(m) time such that we can compute the fingerprint of any substring of P in
constant time using Lemma 1. We can then answer a weak prefix search query for any substring
of P in total time O(m/x+lg x) using the techniques described in the previous sections. The total
time is therefore O(m + h(m/x + lg x)).
4 Distinguishing Occurrences
The following sections describe our compressed-index consisting of three independent data struc-
tures. One that finds long primary occurrences, one that finds short primary occurrences and one
that finds secondary occurrences.
Let Z be the LZ77 parse of length z representing the string S of length n. If S[i, j] is a phrase
of Z then any substring of S[i, j − 1] is a secondary substring of S. These are the substrings of
S that do not contain any phrase borders. On the other hand, a substring S[i, j] is a primary
substring of S when there is some phrase S[i′, j′] where i′ ≤ i ≤ j′ ≤ j, these are the substrings
that contain one or more phrase borders. Any substring of S is either primary or secondary. A
primary substring that match a query pattern P is a primary occurrence of P while a secondary
substring that match P is a secondary occurrence [26].
5 Long Primary Occurrences
For simplicity, we assume that the data structure given in Lemma 5 not only solves the weak prefix
problem, but also answers correctly when the query pattern does not prefix any of the indexed
strings. Later in Section 5.3 we will see how to lift this assumption. The following data structure
and search algorithm is a variation of the classical bidirectional search technique for finding primary
occurrences [26].
5.1 Data Structure
For every phrase S[i, j] the strings S[i, j + k], 0 ≤ k < τ are relevant substrings unless there is
some longer relevant substring ending at position j + k. If S[i′, j′] is a relevant substring then the
string S[j′+1, n] is the associated suffix. There are at most zτ relevant substrings of S and equally
many associated suffixes. The primary index is comprised by the following:
• A prefix search data structure TD on the set of reversed relevant substrings.
• A prefix search data structure TD′ on the set of associated suffixes.
• An orthogonal range reporting data structure R on the zτ × zτ grid. Consider a relevant
substring S[i, j]. Let x denote the rank of rev(S[i, j]) in the lexicographical order of the
reversed relevant substrings, let y denote the rank of its associated suffix S[j + 1, n] in the
lexicographical order of the associated suffixes. Then (x, y) is a point in R and along with it
we store the pair (j, b), where b is the position of the rightmost phrase border contained in
S[i, j].
Note that every point (x, y) in R is induced by some relevant substring S[i, j] and its associated
suffix S[j + 1, n]. If some prefix P [1, k] is a suffix of S[i, j] and the suffix P [k + 1,m] is a prefix of
S[j + 1, n] then S[j−k+ 1, j−k+m] is an occurrence of P and we can compute its exact location
from k and j.
5.2 Searching
The data structure can be used to find the primary occurrences of a pattern P of length m when
m > τ . Consider the O(m/τ) prefix-suffix pairs (P [1, iτ ], P [iτ + 1,m]) for i = 1, . . . , ⌊m/τ⌋ and
the pair (P [1,m], ǫ) in case m is not a multiple of τ . For each such pair, we do a prefix search for
7
rev(P [1, iτ ]) and P [iτ + 1,m] in TD and TD′ , respectively. If either of these two searches report
no matches, we move on to the next pair. Otherwise, let [l, r], [l′, r′] be the ranges reported from
the search in TD and TD′ respectively. Now we do a range reporting query on R for the rectangle
[l, r] × [l′, r′]. For each point reported, let (j, b) be the pair stored with the point. We report
j − iτ + 1 as the starting position of a primary occurrence of P in S.
Finally, in case m is not a multiple of τ , we need to also check the pair (P [1,m], ǫ). We search
for rev(P [1,m]) in in TD and ǫ in TD′ . If the search for rev(P [1,m]) reports no match we stop.
Otherwise, we do a range reporting query as before. For each point reported, let (j, b) be the pair
stored with the point. To check that the occurrence has not been reported before we do as follows.
Let k be the smallest positive integer such that j −m+ kτ > b. If kτ > m we report j −m+ 1 as
the starting position of a primary occurrence.
5.2.1 Correctness
We claim that the reported occurrences are exactly the primary occurrences of P . We first prove
that all primary occurrences are reported correctly. Let P = S[i′, j′] be a primary occurrence. As
it is a primary occurrence, there must be some phrase S[i∗, j∗] such that i∗ ≤ i′ ≤ j∗ ≤ j′. Let
k be the smallest positive integer such that i′ + kτ − 1 ≥ j∗. There are two cases: kτ ≤ m and
kτ > m. If kτ ≤ m then P [1, kτ ] is a suffix of the relevant substring ending at i′ + kτ − 1. Such
a relevant substring exists since i′ + kτ − 1 < j∗ + τ . Thus its reverse rev(P [1, kτ ]) prefixes a
string s in D, while P [kτ + 1,m] is a prefix of the associated suffix S[i′ + kτ, n] ∈ D′. Therefore,
the respective ranks of s and S[i′ + kτ, n] in D and D′ are plotted as a point in R which stores
the pair (i′ + kτ − 1, b). We will find this point when considering the prefix-suffix pair (P [1, kτ ],
P [kτ + 1,m]), and correctly report (i′ + kτ − 1)− kτ + 1 = i′ as the starting position of a primary
occurrence. If kτ > m then P [1,m] is a suffix of the relevant substring ending in i′ +m−1. Such a
relevant substring exists since i′ +m−1 < i′ +kτ −1 < j∗ + τ . Thus its reverse prefixes a string in
D and trivially ǫ is a prefix of the associated suffix. It follows as before that the ranks are plotted
as a point in R storing the pair (i′ + m − 1, b) and that we find this point when considering the
pair (P [1,m], ǫ). When considering (P [1,m], ǫ) we report (i′ +m− 1)−m+ 1 = i′ as the starting
position of a primary occurrence if kτ > m, and thus i′ is correctly reported.
We now prove that all reported occurrences are in fact primary occurrences. Assume that we
report j−iτ+1 for some i and j as the starting position of a primary occurrence in the first part of
the procedure. Then there exist strings rev(S[i′, j]) and S[j + 1, n] in D and D′ respectively such
that S[i′, j] is suffixed by P [1, iτ ] and S[j + 1, n] is prefixed by P [iτ + 1,m]. Therefore j − iτ + 1
is the starting position of an occurrence of P . The string S[i′, j] is a relevant suffix and therefore
there exists a border b in the interval [j−τ +1, j]. Since i ≥ 1 the occurrence contains the border b
and it is therefore a primary occurrence. If we report j−m+1 for some j as the starting position of
a primary occurrence in the second part of the procedure, then rev(P [1,m]) is a prefix of a string
rev(S[i′, j]) in D. It follows immediately that j − m + 1 is the starting point of an occurrence.
Since m > τ we have j −m + 1 < j − τ + 1, and by the definition of relevant substring there is a
border in the interval [j − τ + 1, j]. Therefore the occurrence contains the border and is primary.
5.2.2 Complexity
We now consider the time complexity of the algorithm described. First we will argue that any
primary occurrence is reported at most once and that the search finds at most two points in R
identifying it. Let S[i′, j′] be a primary occurrence reported when we considered the prefix-suffix
pair (P [1, kτ ], P [kτ +1,m]) as in the proof of correctness. None of the pairs (P [1, iτ ], P [iτ +1,m]),
where i < k will identify this occurrence as i′+iτ−1 < j. None of the pairs (P [1, hτ ], P [hτ+1,m]),
where h > k, will identify this occurrence. This is the case since i′+hτ−1 > j+τ−1, and from the
definition of relevant substrings it follows that if S[i, j] is a phrase, S[a, b] is a relevant substring
and a < i, then b < i + τ − 1. Thus there are no relevant substrings that end after j + τ − 1 and
start before i′ < j. Therefore, only one of the pairs (P [1, iτ ], P [iτ + 1,m]) for i = 1, . . . , ⌊m/x⌋
identifies the occurrence. If (k+ 1)τ > m then we might also find the occurrence when considering
the pair (P [1,m], ǫ), but we do not report i′ as kτ ≤ m.
After preprocessing P in O(m) time, we can do the O(m/τ) prefix searches in total time
O(m + m/τ(m/x + lg x)) where x is a positive integer by Lemma 5. Using the range reporting
8
data structure by Chan et al. [8] each range reporting query takes (1+k)·O(B lg lg(zτ)) time where
2 ≤ B ≤ lgǫ(zτ) and k is the number of points reported. As each such point in one range reporting
query corresponds to the identification of a unique primary occurrence of P , which happens at
most twice for every occurrence we charge O(kB lg lg(zτ)) to reporting the occurrences. The total
time to find all primary occurrences is thus O(m + mτ (
m
x + lg x + B lg lg(zτ)) + occ B lg lg(zτ))
where occ is the number of primary and secondary occurrences of P .
5.3 Prefix Search Verification
The prefix data structure from Lemma 5 gives no guarantees of correct answers when the query
pattern does not prefix any of the indexed strings. If the prefix search gives false-positives, we may
end up reporting occurrences of P that are not actually there. We show how to solve this problem
after introducing a series of tools that we will need.
5.3.1 Straight Line Programs
A straight line program (SLP) for a string S is a context-free grammar generating the single string
S.
Lemma 6 (Rytter [38], Charikar et al. [9]). Given an LZ77 parse Z of length z producing a string
S of length n we can construct a SLP for S of size O(z lg(n/z)) in time O(z lg(n/z)).
The construction from Rytter [38] produces a balanced grammar for every consecutive substring
of length n/z of S after a preprocessing step transforms Z such that no compression element is
longer than n/z. These grammars are then connected to form a single balanced grammar of height
O(lg n) which immediately yields extraction of any substring S[i, j] in time O(lg(n) + j − i). We
give a simple solution to reduce this to O(lg(n/z) + j − i), that also supports computation of the
fingerprint of a substring in O(lg(n/z)) time.
Lemma 7. Given an LZ77 parse Z of length z producing a string S of length n we can build
a data structure that for any substring S[i, j] can extract S[i, j] in O(lg(n/z) + j − i) time and
compute the fingerprint φ(S[i, j]) in O(lg(n/z)) time. The data structure uses O(z lg(n/z)) space
and O(n) construction time.
Proof. Assume for simplicity that n is a multiple of z. We construct the SLP producing S from Z.
Along with every non-terminal of the SLP we store the size and fingerprint of its expansion. Let
s1, s2, . . . sz be consecutive length n/z substrings of S. We store the balanced grammar producing
si along with the fingerprint φ(S[1, (i − 1)n/z]) at index i in a table A.
Now we can extract si in O(n/z) time and any substring si[j, k] in time O(lg(n/z) + k − j).
Also, we can compute the fingerprint φ(si[j, k]) in O(lg(n/z)) time. We can easily do a constant
time mapping from a position in S to the grammar in A producing the substring covering that
position and the corresponding position inside the substring. But then any fingerprint φ(S[1, j])
can be computed in time O(lg(n/z)). Now consider a substring S[i, j] that starts in sk and ends
in sl, k < l. We extract S[i, j] in O(lg(n/z) + j − i) time by extracting the appropriate suffix of
sk, all of sm for k < m < l and the appropriate prefix of sl. Each of the fingerprints stored by the
data structure can be computed in O(1) time after preprocessing S in O(n) time. Thus table A is
filled in O(z) time and by Lemma 6 the SLPs stored in A uses a total of O(z lg(n/z)) space and
construction time.
5.3.2 Verification of Fingerprints
We need the following lemma for the verification.
Lemma 8 (Bille et al. [6]). Given a string S of length n, we can find a fingerprinting function φ
that is collision-free for all length l substrings of S where l is a power of two in O(n lg n) expected
time.
9
5.3.3 Verification Technique
Our verification technique is identical to the one given by Gagie et al. [20] and involves a simple
modification of the search for long primary occurrences. By using Lemma 7 instead of bookmark-
ing [20] for extraction and fingerprinting and because we only need to verify O(m/τ) strings, the
verification procedure takes O(m + m/τ lg(n/z)) time and uses O(z lg(n/z)) space.
Consider the string S of length n that we wish to index and let Z be the LZ77 parse of S.
The verification data structure is given by Lemma 7. Consider the prefix search data structure
TD′ as given in Section 5.1 and let φ be the fingerprinting function used by the prefix search, the
case for TD is symmetric. We alter the search for primary occurrences such that it first does the
O(m/τ) prefix searches, then verifies the results and discards false-positives before moving on to
do the O(m/τ) range reporting queries on the verified results. We also modify φ using Lemma 8
to be collision-free for all substrings of the indexed strings which length is a power of two.
Let Q1, Q2, . . .Qj be the all the suffixes of P for which the prefix search found a locus candidate,
let the candidates be v1, v2, . . . vj ∈ TD′ and let pi be str(vi)[1, |Qi|]. Assume that |Qi| < |Qi+1|,
and let 2-suf(Q) and 2-pre(Q) denote the fingerprints using φ of the suffix and prefix respectively
of length 2⌊lg |Q|⌋ of some string Q. The verification progresses in iterations. Initially, let a = 1,
b = 2 and for each iteration do as follows:
1. 2-suf(Qa) 6= 2-suf(pa) or 2-pre(Qa) 6= 2-pre(pa): Discard va and set a = a + 1 and b = b + 1.
2. 2-suf(Qa) = 2-suf(pa) and 2-pre(Qa) = 2-pre(pa), let R = pb[|pa| − |pb| + 1, |pa|].
(a) 2-suf(R) = 2-suf(Qa) and 2-pre(R) = 2-pre(Qa): set a = a + 1 and b = b + 1.
(b) 2-suf(R) 6= 2-suf(Qa) or 2-pre(R) 6= 2-pre(Qa): discard vb and set b = b + 1.
3. b = j + 1: If all vertices have been discarded, report no matches. Otherwise, let vf be the
last vertex considered, that was not discarded. Report all non-discarded vertices vi where
|pi| is no longer than the longest common suffix of pf and Qf as verified and discard the rest.
Consider the correctness and complexity of the algorithm. In case 1, clearly, pa does not match
Qa and thus va must be a false-positive. Now observe that because Qi is a suffix of P , it is also
a suffix of Qi′ for any i < i
′. Thus in case 2 (b), if R does not match Qa then vb must be a
false-positive. In case 2 (a), both va and vb may still be false-positives, yet by Lemma 8, pa is a
suffix of pb because 2-suf(pa) = 2-suf(R) and 2-pre(pa) = 2-pre(R). Finally, in case 3, vf is a true
positive if and only if pf = Qf . But any other non-discarded vertex vi 6= vf is also only a true
positive if pf and Qf share a length |pi| suffix because pi is a suffix of pf and Qi is a suffix of Qp.
The algorithm does j iterations and fingerprints of substrings of P can be computed in constant
time after O(m) preprocessing. Every vertex v ∈ TD′ represents one or more substrings of S. If we
store the starting index in S of one of these substrings in v when constructing TD′ we can compute
the fingerprint of any substring str(v)[i, j] by computing the fingerprint of S[i′ + i − 1, i′ + j − 1]
where i′ is the starting index of one of the substring of S that v represents. By Lemma 7, the
fingerprint computations take O(lg(n/z)) time and because j ≤ m/τ the total time complexity of
the algorithm is O(m + m/τ lg(n/z)).
6 Short Primary Occurrences
We now describe a simple data structure that can find primary occurrences of P in time O(m+occ)
using space O(zτ) whenever m ≤ τ where τ is a positive integer.
Let Z be the LZ77 parse of the string S of length n. Let Z[i] = S[si, ei] and define F to be
the union of the strings S[k,min{ei + τ, n}] where max{1, si, ei − τ} ≤ k ≤ ei for i = 1, 2, . . . z.
There are at most zτ such strings, each of length O(τ) and they are all suffixes of the z length 2τ
substrings of S starting τ positions before each border position. We store these substrings along
with the compact trie TF over the strings in F . The edge labels of TF are compactly represented
by storing references into one of the substrings. Every leaf stores the starting position in S of the
string it represents and the position of the leftmost border it contains.
The combined size of TF and the substrings we store is O(zτ) and we simply search for P
by navigating vertices using perfect hashing [18] and matching edge labels character by character.
10
Now either locus(P ) = ⊥ in which case there are no primary occurrences of P in S; otherwise,
locus(P ) = v for some vertex v ∈ TF and thus every leaf in the subtree of v represents a substring
of S that is prefixed by P . By using the indices stored with the leaves, we can determine the
starting position for each occurrence and if it is primary or secondary. Because each of the strings
in F start at different positions in S, we will only find an occurrence once. Also, it is easy to see
that we will find all primary occurrences because of how the strings in F are chosen. It follows that
the time complexity is O(m+occ) where occ is the number of primary and secondary occurrences.
7 The Secondary Index
Let Z be the LZ77 parse of length z representing the string S of length n. We find the secondary
occurrences by applying the most recent range reporting data structure by Chan et al. [8] to the
technique described by Kärkkäinen and Ukkonen [26] which is inspired by the ideas of Farach and
Thorup [13].
Let o1, . . . oocc be the starting positions of the occurrences of P in S ordered increasingly.
Assume that oh is a secondary occurrence such that P = S[oh, oh + m − 1]. Then by definition,
S[oh, oh + m − 1] is a substring the prefix S[i, j − 1] of some phrase S[i, j] and there must be an
occurrence of P in the source of that phrase. More precise, let S[k, l] = S[i, j − 1] be the source
of the phrase S[i, j] then oh′ = k + oh − i is an occurrence of P for some h′ < h. We say that
oh′ , which may be primary or secondary, is the source occurrence of the secondary occurrence oh
given the LZ77 parse of S. Thus every secondary occurrence has a source occurrence. Note that
it follows from the definition that no primary occurrence has a source occurrence.
We find the secondary occurrences as follows: Build a range reporting data structure Q on the
n× n grid and if S[i, j] is a phrase with source S[i′, j′] we plot a point (i′, j′) and along with it we
store the phrase start i.
Now for each primary occurrence o found by the primary index, we query Q for the rectangle
[0, o]× [o+m−1, n]. The points returned are exactly the occurrences having o as source. For each
point (x, y) and phrase start i reported, we report an occurrence o′ = i + o− x and recurse on o′
to find all the occurrences having o′ as source.
Because no primary occurrence have a source, while all secondary occurrences have a source,
we will find exactly the secondary occurrences.
The range reporting structure Q is built using Lemma 3 with B = 2 and uses space O(z lg lg z).
Exactly one range reporting query is done for each primary and secondary occurrence each taking
O((1 + k) lg lg n) where k is the number of points reported. Each reported point identifies a
secondary occurrence, so the total time is O(occ lg lgn).
8 The Compressed Index
We obtain our final index by combining the primary index, the verification data structure and the
secondary index. We use the transformed LZ77 parse generated by Lemma 6 when building our
primary index. Therefore no phrase will be longer than n/z and therefore any primary occurrence
of P will have a prefix P [1, k] where k ≤ n/z that is a suffix of some phrase. It then follows that
we need only consider the multiples (P [1, iτ ], P [iτ + 1,m]) for i < ⌊n/zτ ⌋ when searching for long
primary occurrences. This yields the following complexities:
• O(m+ min{m,n/z}τ (
m
x + lg x+ B lg lg(zτ)) + occ B lg lg(zτ)) time and O(zτ lgB lg(zτ)) space
for the index finding long primary occurrences where x and τ are positive integers and
2 ≤ B ≤ lgǫ(zτ).
• O(m + occ) time and O(z lg(n/z)) space for the index finding short primary occurrences.
• O(m + m/τ lg(n/z)) time and O(z lg(n/z)) space for the verification data structure.
• O(occ lg lg n) time and O(z lg lg z) space for the secondary index.
If we fix x at n/z we have min{m,n/z}τ
m
x ≤ m in which case we obtain the following trade-off simply
by combining the above complexities.
11
Theorem 2. Given a string S of length n from an alphabet of size σ compressed using LZ77
to a string of length z we can build a compressed-index supporting substring queries in O(m +
m
τ (lg(n/z)+B lg lg(zτ))+occ(B lg lg(zτ)+lg lg n)) time using O(z(lg(n/z)+τ lgB lg(zτ)+lg lg z))
space for any query pattern P of length m where 2 ≤ B ≤ lgǫ(zτ), 0 < ǫ < 1 and τ is a positive
integer.
We note that none of our data structures assume constant sized alphabet and thus Thm. 2
holds for any alphabet size.
8.1 Trade-offs
Thm. 2 gives rise to a series of interesting time-space trade-offs.
Corollary 1. Given a string S of length n from an alphabet of size σ compressed using LZ77 into
a string of length z we can build a compressed-index supporting substring queries in
(i) O(m(1 + lg lg zlg(n/z) ) + occ lg lg n) time using O(z lg(n/z) lg lg z) space, or
(ii) O(m(1 + lg
ǫ z
lg(n/z) ) + occ(lg lgn + lg
ǫ z)) time using O(z lg(n/z)) space, or
(iii) O(m lgǫ(n/z) + occ lg lg n) time using O(z lg(n/z)) space, or
(iv) O(m + occ lg lgn) time using O(z(lg(n/z) lg lg z + lg lg2 z)) space, or
(v) O(m + occ(lg lg n + lgǫ z)) time using O(z(lg(n/z) + lgǫ
′
z)) space.
for any 0 < ǫ < 1 and 0 < ǫ′ < 1.
Proof. For (i) set B = 2 and τ = lg(n/z), for (ii) set B = lgǫ z and τ = lg(n/z), for (iii) set
B = 2 and τ = lgǫ
′
n/z for some 0 < ǫ′ < 1, for (iv) set B = 2 and τ = lg(n/z) + lg lg z, for (v) set
B = lgǫ
′
(z) and τ = lg(n/z) + lgǫ z.
The leading term in the time complexity of Cor. 1 (i) is O(m) whenever lg lg(z) = O(lg(n/z))
which is true when z = O(n/ lgn), i.e. for all strings that are compressible by at least a logarithmic
fraction. For σ = O(1) we have z = O(n/ lg n) all strings [36] and thus Thm. 1 (i) follows
immediately. Cor. 1 (ii) matches previous best space bounds but obtains a leading term of O(m)
for any polynomial compression rate. Thm. 1 (ii) is a weaker version of this because it assumes
constant sized alphabet and therefore follows immediately. Cor. 1 (iii) matches the space and
time for reporting occurrences of previous best bounds by Gagie et al. [20] but with a leading
term of O(m lgǫ(n/z)) compared to a leading term of O(m lgm). Cor. 1 (iv) and (v) show how to
guarantee the fast query times with leading term O(m) without the assumptions on compression
ratio that (i) and (ii) require to match this, but at the cost of increased space.
8.2 Preprocessing
We now consider the preprocessing time of the data structure. Let Z be the LZ77 parse of the
string S of length n let TD and TD′ be the compact tries used in the index for long primary
occurrences. The compact trie TD index O(zτ) substrings of S with overall length O(nτ). Thus
we can construct the trie in O(nτ) time by sorting the strings and successively inserting them in
their sorted order [2]. The compact tries TD′ index zτ < n suffixes of S and can be built in O(n)
time using O(n) space [12]. The index for short primary occurrences is a generalized suffix tree
over z strings of length O(τ) with total length zτ < n and is therefore also built in O(n) time.
The dictionaries used by the prefix search data structures and for trie navigation contain O(zτ)
keys and are built in expected linear time using perfect hashing [18]. The range reporting data
structures used by the primary and secondary index over O(zτ) points are built in O(zτ lg(zτ))
expected time using Lemma 3.
Building the SLP for our verification data structure takes O(z lg(n/z)) time using Lemma 6
and finding an appropriate fingerprinting function φ takes O(n lg n) expected time using Lemma 8.
The prefix search data structures TD and TD′ also require that φ is collision-free for the x-prefixes,
12
fat prefixes and the prefixes with pseudo fat lengths. There are at most O(zτ lg n) such prefixes [3].
If we compute these fingerprints incrementally while doing a traversal of the tries, we expect all
the fingerprints to be unique. We simply check this by sorting the fingerprints in linear time and
checking for duplicates by doing a linear scan. If we choose a prime p = Θ(n5) with Lemma 2 then
the probability of a collision between any two strings is O(1/n4) and by a union bound over the
O((n lg n)
2
) possible collisions the probability that φ is collision-free is at least 1 − 1/n. Thus the
expected time to find our required fingerprinting function is O(n + n lgn).
All in all, the preprocessing time for our combined index is therefore expected O(n lg n + nτ).
References
[1] www.gzip.org.
[2] Arne Andersson and Stefan Nilsson. A new efficient radix sort, 1994.
[3] Djamal Belazzougui, Paolo Boldi, Rasmus Pagh, and Sebastiano Vigna. Fast prefix search in
little space, with applications. In Proc. 18th ESA, pages 427–438, 2010.
[4] Djamal Belazzougui, Fabio Cunial, Travis Gagie, Nicola Prezza, and Mathieu Raffinot. Com-
posite repetition-aware data structures. In Proc. 26st CPM, pages 26–39, 2015.
[5] Djamal Belazzougui, Travis Gagie, Pawel Gawrychowski, Juha Kärkkäinen, Alberto Ordóñez
Pereira, Simon J. Puglisi, and Yasuo Tabei. Queries on lz-bounded encodings. In Ali Bilgin,
Michael W. Marcellin, Joan Serra-Sagristà, and James A. Storer, editors, 2015 Data Com-
pression Conference, DCC 2015, Snowbird, UT, USA, April 7-9, 2015, pages 83–92. IEEE,
2015.
[6] Philip Bille, Inge Li Gørtz, Benjamin Sach, and Hjalte Wedel Vildhøj. Time-space trade-offs
for longest common extensions. In Proc. 23rd CPM, 2012.
[7] Dany Breslauer and Zvi Galil. Real-time streaming string-matching. ACM Trans. Algorithms,
10(4):22:1–22:12, 2014.
[8] Timothy M. Chan, Kasper Green Larsen, and Mihai Patrascu. Orthogonal range searching
on the ram, revisited. In Proc. 27th SOCG, pages 1–10, 2011.
[9] Moses Charikar, Eric Lehman, Ding Liu, Rina Panigrahy, Manoj Prabhakaran, Amit Sa-
hai, and Abhi Shelat. The smallest grammar problem. IEEE Trans. Information Theory,
51(7):2554–2576, 2005.
[10] Francisco Claude, Antonio Fariña, Miguel A. Mart́ınez-Prieto, and Gonzalo Navarro. Univer-
sal indexes for highly repetitive document collections. Inf. Syst., 61:1–23, 2016.
[11] Francisco Claude and Gonzalo Navarro. Improved grammar-based compressed indexes. In
Proc. 19th SPIRE, pages 180–192, 2012.
[12] M. Farach. Optimal suffix tree construction with large alphabets. In Proceedings of the 38th
Annual Symposium on Foundations of Computer Science, FOCS ’97, pages 137–, Washington,
DC, USA, 1997. IEEE Computer Society.
[13] Martin Farach and Mikkel Thorup. String matching in lempel-ziv compressed strings. Algo-
rithmica, 20(4):388–404, 1998.
[14] P. Ferragina and G. Manzini. Opportunistic data structures with applications. In Proc. 41st
FOCS, pages 390–398, 2000.
[15] Paolo Ferragina and Giovanni Manzini. An experimental study of an opportunistic index. In
Proc. 12th SODA, pages 269–278, 2001.
[16] Paolo Ferragina and Giovanni Manzini. Indexing compressed text. J. ACM, 52(4):552–581,
2005.
13
[17] Paolo Ferragina, Giovanni Manzini, Veli Mäkinen, and Gonzalo Navarro. Compressed repre-
sentations of sequences and full-text indexes. ACM Trans. Algorithms, 3(2), 2007.
[18] Michael L. Fredman, János Komlós, and Endre Szemerédi. Storing a sparse table with 0(1)
worst case access time. J. ACM, 31(3):538–544, 1984.
[19] Travis Gagie, Pawe l Gawrychowski, Juha Kärkkäinen, Yakov Nekrich, and Simon J. Puglisi.
A faster grammar-based self-index. In Proc. 6th LATA, pages 240–251, 2012.
[20] Travis Gagie, Pawe l Gawrychowski, Juha Kärkkäinen, Yakov Nekrich, and Simon J Puglisi.
LZ77-based self-indexing with faster pattern matching. In Proc. 11th LATIN, pages 731–742,
2014.
[21] Travis Gagie and Simon J. Puglisi. Searching and indexing genomic databases via kerneliza-
tion. Frontiers in Bioengineering and Biotechnology, 3:12, 2015.
[22] Roberto Grossi, Ankur Gupta, and Jeffrey Scott Vitter. High-order entropy-compressed text
indexes. In Proc. 14th SODA, pages 841–850, 2003.
[23] Roberto Grossi, Ankur Gupta, and Jeffrey Scott Vitter. When indexing equals compression:
Experiments with compressing suffix arrays and applications. In Proc. 15th SODA, pages
636–645, 2004.
[24] Roberto Grossi and Jeffrey Scott Vitter. Compressed suffix arrays and suffix trees with
applications to text indexing and string matching. In Proc. 32nd STOC, pages 397–406, 2000.
[25] Juha Kärkkäinen and Erkki Sutinen. Lempel-Ziv index for q-grams. Algorithmica, 21(1):137–
154, 1998.
[26] Juha Kärkkäinen and Esko Ukkonen. Lempel-Ziv parsing and sublinear-size index structures
for string matching. In Proc. 3rd WSP, pages 141–155, 1996.
[27] Richard M. Karp and Michael O. Rabin. Efficient randomized pattern-matching algorithms.
IBM J. Res. Dev., 31(2):249–260, 1987.
[28] Sebastian Kreft and Gonzalo Navarro. On compressing and indexing repetitive sequences.
Theoret. Comp. Sci., 483:115 – 133, 2013.
[29] Moshe Lewenstein. Orthogonal range searching for text indexing. In Space-Efficient Data
Structures, Streams, and Algorithms - Papers in Honor of J. Ian Munro on the Occasion of
His 66th Birthday, pages 267–302, 2013.
[30] Veli Mäkinen. Compact suffix array. In Proc. 11th CPM, pages 305–319, 2000.
[31] Veli Mäkinen, Gonzalo Navarro, Jouni Sirén, and Niko Välimäki. Storage and retrieval of
highly repetitive sequence collections. J. Comput. Bio., 17(3):281–308, 2010.
[32] Donald R. Morrison. Patricia&mdash;practical algorithm to retrieve information coded in
alphanumeric. J. ACM, 15(4):514–534, October 1968.
[33] Gonzalo Navarro. Indexing highly repetitive collections. In Proc. 23rd IWOCA, pages 274–
279, 2012.
[34] Gonzalo Navarro. Compact Data Structures - A Practical Approach. Cambridge University
Press, 2016.
[35] Gonzalo Navarro and Veli Mäkinen. Compressed full-text indexes. ACM Comput. Surv.,
39(1), 2007.
[36] Gonzalo Navarro and Veli Mäkinen. Compressed full-text indexes. ACM Comput. Surv.,
39(1), April 2007.
14
[37] Benny Porat and Ely Porat. Exact and approximate pattern matching in the streaming model.
In Proc. 50th FOCS, pages 315–323, 2009.
[38] Wojciech Rytter. Application of Lempel–Ziv factorization to the approximation of grammar-
based compression. Theoret. Comp. Sci., 302(1–3):211 – 222, 2003.
[39] Jacob Ziv and Abraham Lempel. A universal algorithm for sequential data compression. IEEE
Trans. Information Theory, 23(3):337–343, 1977.
15

