Performance Assessment of
Multiobjective Optimizers:
An Analysis and Review
Eckart Zitzler1, Lothar Thiele1, Marco Laumanns1,
Carlos M. Fonseca2, and Viviane Grunert da Fonseca2
1 Computer Engineering and Networks Laboratory (TIK)
Department of Information Technology and Electrical Engineering
Swiss Federal Institute of Technology (ETH) Zurich, Switzerland
Email: {zitzler, thiele, laumanns}@tik.ee.ethz.ch
2ADEEC and ISR (Coimbra)
Faculty of Sciences and Technology
University of Algarve, Portugal
Email: cmfonsec@ualg.pt, vgrunert@csi.fct.ualg.pt
TIK-Report No. 139
Institut für Technische Informatik und Kommunikationsnetze, ETH Zürich
Gloriastrasse 35, ETH-Zentrum, CH–8092 Zürich, Switzerland
June 26, 2002
Abstract
An important issue in multiobjective optimization is the quantitative comparison of the perfor-
mance of different algorithms. In the case of multiobjective evolutionary algorithms, the outcome
is usually an approximation of the Pareto-optimal front, which is denoted as an approximation
set, and therefore the question arises of how to evaluate the quality of approximation sets. Most
popular are methods that assign each approximation set a vector of real numbers that reflect dif-
ferent aspects of the quality. Sometimes, pairs of approximation sets are considered too. In this
study, we provide a rigorous analysis of the limitations underlying this type of quality assessment.
To this end, a mathematical framework is developed which allows to classify and discuss existing
techniques.
1 Introduction
With many multiobjective optimization problems,
knowledge about the Pareto-optimal front helps the
decision maker in choosing the best compromise solu-
tion. For instance, when designing computer systems,
engineers often perform a so-called design space ex-
ploration to learn more about the trade-off surface.
Thereby, the design space is reduced to the set of
optimal trade-offs: a first step in selecting an appro-
priate implementation.
However, generating the Pareto-optimal front can
be computationally expensive and is often infeasible,
because the complexity of the underlying application
prevents exact methods from being applicable. Evo-
lutionary algorithms (EAs) are an alternative: they
usually do not guarantee to identify optimal trade-
offs but try to find a good approximation, i.e., a
set of solutions that are (hopefully) not too far away
from the optimal front. Various multiobjective EAs
are available, and certainly we are interested in the
technique that provides the best approximation for a
given problem. For this reason, comparative studies
are conducted, e.g., [26][22][19]; they aim at revealing
strengths and weaknesses of certain approaches and
at identifying the most promising algorithms. This,
in turn, leads to the question of how to compare the
performance of multiobjective optimizers.
The notion of performance includes both the qual-
ity of the outcome as well as the computational re-
sources needed to generate this outcome. Concern-
ing the latter aspect, it is common practice to keep
the number of fitness evaluations or the overall run-
time constant—in this sense, there is no difference
between single and multiobjective optimization. As
to the quality aspect, however, there is a difference.
In single-objective optimization, we can define qual-
ity by means of the objective function: the smaller (or
larger) the value, the better the solution. In contrast,
it is not clear what quality means in the presence of
several optimization criteria: closeness to the optimal
front, coverage of a wide range of diverse solutions,
or other properties? Therefore, it is difficult to de-
fine appropriate quality measures for approximations
of the Pareto-optimal front, and as a consequence
graphical plots have been used to compare the out-
comes of multiobjective EAs until recently, as Van
Veldhuizen points out [21].
Progress, though, has been made and meanwhile
several studies can be found in the literature that
address the problem of comparing approximations of
the trade-off surface in a quantitative manner. Most
popular are unary quality measures, i.e., the mea-
sure assigns each approximation set a number that
reflects a certain quality aspect, and usually a com-
bination of them is used, e.g., [22][4]. Other methods
are based on binary quality measures, which assign
numbers to pairs of approximation sets, e.g., [26][9].
A third, and conceptually different approach, is the
attainment function approach [8], which consists of
estimating the probability of attaining arbitrary goals
in objective space from multiple approximation sets.
Despite of this variety, it has remained unclear up to
now how the different measures are related to each
other and what their advantages and disadvantages
are. Accordingly, there is no common agreement on
which measure(s) should be used.
Recently, a few studies have been carried out to
clarify this situation. Hansen and Jaszkiewicz [9]
studied and proposed some quality measures that
induce a linear ordering on the space of possible
approximations—on the basis of assumptions about
the decision maker’s preferences. They first in-
troduced three different “outperformance” relations
for multiobjective optimizers and then investigated
whether the measures under consideration are com-
pliant with these relations. The basic question they
considered was: whenever an approximation is bet-
ter than another according to an “outperformance”
relation, does the comparison method also evaluate
the former as being better (or at least not worse)
than the latter? More from a practical point of view,
Knowles, Corne, and Oates [12] compared the infor-
mation provided by different assessment techniques
on two database management applications. Later,
Knowles [14] and Knowles and Corne [13] discussed
and contrasted several commonly used quality mea-
sures in the light of Hansen and Jaszkiewicz’s ap-
proach as well as according to other criteria such as,
e.g., sensitivity to scaling. They showed that about
1
one third of the investigated quality measures are not
compliant with any of the ”outperformance” relations
introduced by Hansen and Jaszkiewicz.
This paper takes a different perspective that allows
a more rigorous analysis and classification of compar-
ison methods. In contrast to [9], [14], and [13], we fo-
cus on the statements that can be made on the basis
of the information provided by quality measures. Is
it, for instance, possible to conclude from the qual-
ity “measurements” that an approximation A is un-
doubtedly better than approximation B in the sense
that A, loosely speaking, entirely dominates B? This
is a crucial issue in any comparative study, and im-
plicitly most papers in this area rely on the assump-
tion that this property is satisfied for the measures
used. To investigate quality measures from this per-
spective, a formal framework will be introduced that
substantially goes beyond Hansen and Jaszkiewicz’s
approach as well as that of Knowles and Corne; e.g.,
it will enable us to consider combinations of qual-
ity measures and to prove theoretical limitations of
unary quality measures, both issues not addressed in
[9], [14], and [13]. In detail, we will show that
• there exists no unary quality measure that is able
to indicate whether an approximation A is better
than an approximation B;
• the above statement even holds if we consider a
finite combination of unary measures;
• most existing quality measures that have been
proposed to indicate that A is better than B at
best allow to infer that A is not worse than B,
i.e., A is better than or incomparable to B;
• unary measures being able to detect that A is
better than B exist, but their use is in general
restricted;
• binary quality measures overcome the limita-
tions of unary measures and, if properly de-
signed, are capable of indicating whether A is
better than B.
Furthermore, we will review existing quality mea-
sures in the light of this framework and discuss them
also from a practical point of view. Note that we
focus on the comparison of approximations of the
Pareto-optimal front rather than on algorithms, i.e.,
we assume that for each multiobjective EA only one
run is performed. In the case of multiple runs, the
distribution of the indicator values would have to be
considered instead of the values themselves; this im-
portant issue will not be addressed in the present
paper.
2 Theoretical Framework
Before analyzing and classifying quality measures, we
must clarify the concepts we will be dealing with:
what is the outcome of a multiobjective optimizer,
when is an outcome considered to be better than an-
other, what is a quality measure, what is a compar-
ison method, etc.? These terms will be formally de-
fined in this section.
2.1 Approximation Sets
The scenario considered in this paper involves an
arbitrary optimization problem with n objectives,
which are, without loss of generality, all to be mini-
mized. We will use the symbol Z to denote the space
of all possible solutions to the problem with respect
to the objective values; Z is also called the objective
space and each element of Z is referred to as objective
vector. Here, we will use the terms objective vector
and solution interchangeably.
We consider the most general case, in which all ob-
jectives are considered to be equally important—no
additional knowledge about the problem is available.
The only assumption we make is that a solution z1
is preferable to another solution z2 if z1 is at least as
good as z2 in all objectives and better with respect
to at least one objective. This is commonly known
as the concept of Pareto dominance, and we also say
z1 dominates z2. The dominance relation induces
a partial order on the search space, so that we can
define an optimal solution to be one that is not dom-
inated by any other solution. However, several such
solutions, which are denoted as Pareto optimal , may
2
2
f
10
f
15
a
b
c
d
Figure 1: Examples of dominance relations on objec-
tive vectors. Assuming that two objectives are to be
minimized, it holds that a  b, a  c, a  d, b  d,
c  d, a  d, a  a, a  b, a  c, a  d, b  b,
b  d, c  c, c  d, d  d, and b ‖ c.
exist as two objective vectors can be incomparable
to each other: each is superior to the other in some
objectives and inferior in other objectives. Fig. 1 vi-
sualizes these concepts and also gives some examples
for other common relations on pairs of objective vec-
tors. Table 2 comprises a summary of the relations
used in this paper.
The vast majority of papers in the area of evolu-
tionary multiobjective optimization is concerned with
the problem of how to identify the Pareto-optimal so-
lutions or, if this is infeasible, to generate good ap-
proximations of them. Taking this as the basis of our
study, we here consider the outcome of a multiobjec-
tive EA (or other heuristic) as a set of incomparable
solutions, or formally [9]:
Definition 1 (Approximation set) Let A ⊆ Z be
a set of objective vectors. A is called an approxima-
tion set if any element of A does not dominate or is
not equal to any other objective vector in A. The set
of all approximation sets is denoted as Ω.
The motivation behind this definition is that all
solutions dominated by any other solution outputted
by the optimization algorithm are of no interest and
therefore can be discarded. This will simplify the
considerations in the following sections.
Note that the above definition does not comprise
any notion of quality. We are certainly not inter-
ested in any approximation set, but we want the EA
to generate a good approximation set. The ultimate
5 10
5
2
f
f
1
P
A1
A2
A3
Figure 2: Outcomes of three hypothetical algorithms
for a two-dimensional minimization problem. The
corresponding approximation sets are denoted as A1,
A2, and A3; the Pareto-optimal front P consist of
three objective vectors. Between A1, A2, and A3,
the following dominance relations hold: A1  A3,
A2  A3, A1  A3, A1  A1, A1  A2, A1  A3,
A2  A2, A2  A3, A3  A3, A1  A2, A1  A3,
and A2  A3.
goal is to identify the so-called Pareto-optimal front,
that is the set of all Pareto-optimal solutions. This
aim, however, is usually not achievable. Moreover,
it is impossible to exactly describe what a good ap-
proximation is in terms of a number of criteria such as
closeness to the Pareto-optimal front, diversity, etc.—
this will be shown in Section 3.1. However, we can
make statements about the quality of approximation
sets in comparison to other approximation sets.
Consider, e.g., the outcomes of three hypotheti-
cal algorithms as depicted in Fig. 2. Solely on the
basis of Pareto dominance, one can state that A1
and A2 are both superior to A3 as any solution in
A3 is dominated by at least one solution in A1 and
A2. Furthermore, A1 can be considered superior to
A2 as it contains all solutions in A2 and another so-
lution not included in A2, although this statement
is weaker than the previous one. Accordingly, we
will distinguish four levels of superiority in this pa-
per as defined in Table 1: A strictly dominates B
(A  B), A dominates B (A  B), A is better than
B (A  B), and A weakly dominates B (A  B),
where A  B ⇒ A  B ⇒ A  B ⇒ A  B.
3
relation objective vectors approximation sets
strictly dominates z1  z2 z1 is better than z2 in all objectives A  B every z2 ∈ B is strictly dominated
by at least one z1 ∈ A
dominates z1  z2 z1 is not worse than z2 in all objectives A  B every z2 ∈ B is dominated by
and better in at least one objective at least one z1 ∈ A
better A  B every z2 ∈ B is weakly dominated by
at least one z1 ∈ A and A = B
weakly dominates z1  z2 z1 is not worse than z2 in all objectives A  B every z2 ∈ B is weakly dominated by
at least one z1 ∈ A
incomparable z1 ‖ z2 neither z1 weakly dominates z2 nor A ‖ B neither A weakly dominates B nor
z2 weakly dominates z1 B weakly dominates A
Table 1: Relations on objective vectors and approximation sets considered in this paper. The relations ≺,
≺≺, , and  are defined accordingly, e.g., z1 ≺ z2 is equivalent to z2  z1 and A  B is defined as B  A.
Weak dominance (A  B) means that any solution
in B is weakly dominated by a solution in A. How-
ever, this does not rule out equality, because A  A
for all approximation sets A ∈ Ω. In this case, one
cannot say that A is better than B. Instead, the
relation  can be used as it represents the most gen-
eral and weakest form of superiority. It requires that
an approximation set is at least as good as another
approximation set (A  B), while the latter is not
as good as the former (B 	 A), roughly speaking.
In the example, A1 is better than A2 and A3, and
A2 is better than A3. This definition of superior-
ity is the one implicitly used in most papers in the
field. The next level of superiority, the  relation, is
a straight-forward extension of Pareto dominance to
approximation sets. It does not allow that two solu-
tions in A and B are equal and therefore is stricter
than what we usually require. As mentioned above,
A1 and A2 dominate A3, but A1 does not dominate
A2. Strict dominance stands for the highest level of
superiority and means an approximation set is supe-
rior to another approximation set in the sense that
for any solution in the latter there exists a solution in
the former that is better in all objectives. In Fig. 2,
A1 strictly dominates A3, but A2 does not as the ob-
jective vector (10, 4) is not strictly dominated by any
objective vector in A2.
2.2 Comparison Methods
Quality measures have been introduced to compare
the outcomes of multiobjective optimizers in a quan-
titative manner. Certainly, the simplest comparison
method would be to check whether an outcome is bet-
ter than another with respect to the three dominance
relations ,, and . We have demonstrated this
in the context of the discussion of Fig. 2. The reason,
however, why quality measures have been used is to
be able to make more precise statements:
• If one algorithm is better than another, can we
express how much better it is?
• If no algorithm can be said to be better than the
other, are there certain aspects in which respect
we can say the former is better than the latter?
Hence, the key question when designing quality
measures is how to best summarize approximation
sets by means of a few characteristic numbers—
similarly to statistics where the mean, the standard
deviation, etc. are used to describe a probability dis-
tribution in a compact way. It is unavoidable to lose
information by such a reduction, and the crucial point
is not to lose the information one is interested in.
There are many examples of quality measures in
the literature. Some aim at measuring the distance
of an approximation set to the Pareto-optimal front:
Van Veldhuizen [21], e.g., calculated for each solution
4
in the approximation set under consideration the Eu-
clidean distance to the closest Pareto-optimal objec-
tive vector and then took the average over all of these
distances. Other measures try to capture the diver-
sity of an approximation set, e.g., the chi-square-like
deviation measure used by Srinivas and Deb [18]. A
further example is the hypervolume measure which
considers the volume of the objective space domi-
nated by an approximation set [26]. In these three
cases, an approximation set is assigned a real num-
ber which is meant to reflect (certain aspects of) the
quality of an approximation set. Alternatively, one
can assign numbers to pairs of approximation sets.
Zitzler and Thiele [26], e.g., introduced the coverage
function which gives for a pair (A,B) of approxima-
tion sets the fraction of solutions in B that are weakly
dominated by one or more solutions in A.
In summary, we can state that quality measures
map approximation sets to the set of real numbers.
The underlying idea is to quantify quality differ-
ences between approximation sets by applying com-
mon metrics (in the mathematical sense) to the re-
sulting real numbers. This observation enables us
to formally define what a quality measure is; how-
ever, we will use the term “quality indicator” in the
following as “measure” is often used with different
meanings.
Definition 2 (Quality indicator) An m-ary qual-
ity indicator I is a function I : Ωm → IR, which
assigns each vector (A1, A2, . . . , Am) of m approxi-
mation sets a real value I(A1, . . . , Am).
The measures discussed above are examples for
unary and binary quality indicators; however, in prin-
ciple a quality indicator can take an arbitrary num-
ber of arguments. Thereby, also other comparison
methods that explicitly account for multiple runs
and involve statistical testing procedures [7][11][8]
can be expressed within this framework. Further-
more, often not a single indicator but rather a com-
bination of different quality indicators is used in or-
der to assess approximation sets. Van Veldhuizen
and Lamont [22], for instance, applied a combina-
tion I = (IGD , IS , IONVG) of three indicators where
IGD(A) denotes the average distance of solutions in
A to the Pareto-optimal front, IS(A) measures the
variance of distances between neighboring solutions
in A, and IONVG(A) gives the number of elements in
A. Accordingly, the combination (or quality indicator
vector) I can be regarded as a function that assigns
each approximation set a triple of real numbers.
Quality indicators, though, need interpretation. In
particular, we would like to formally describe state-
ments such as “if and only if IGD(A) = 0, then all so-
lutions in A have zero distance to the Pareto-optimal
front P and therefore A ⊆ P and also B 	 A for
any B ∈ Ω”. To this end, we introduce two con-
cepts. A pseudo-Boolean function E maps vectors
of real numbers to Booleans. In the above exam-
ple, we would define E(IGD(A)) := (IGD(A) = 0),
i.e., E is true if and only if IGD(A) = 0. Such
a combination of one or more quality indicators I
and a Boolean function E is also called a compar-
ison method CI,E . In the example, the comparison
method CIGD ,E based on IGD and E would be defined
as CIGD ,E(A,B) = E(IGD(A)), and the conclusion is
that CIGD ,E(A,B) ⇔ A ⊆ P ∧ B 	 A. In the fol-
lowing, we will focus on comparison methods that i)
consider two approximation sets only and ii) use ei-
ther only unary or only binary indicators (cf. Fig. 3).
Definition 3 (Comparison method) Let A,B ∈
Ω be two approximation sets, I = (I1, I2, . . . , Ik) a
combination of quality indicators, and E : IRk×IRk →
{false, true} a Boolean function which takes 2 real
vectors of length k as arguments. If all indicators in
I are unary, the comparison method CI,E defined by
I and E is a Boolean function of the form
CI,E(A,B) = E(I(A), I(B))
where I(A′) = (I1(A′), I2(A′), . . . , Ik(A′)) for A′ ∈
Ω. If I contains only binary indicators, the compar-
ison method CI,E is defined as
CI,E(A,B) = E(I(A,B), I(B,A))
where I(A′, B′) = (I1(A′, B′), I2(A′, B′), . . . , Ik(A′, B′))
for A′, B′ ∈ Ω.
Whenever we will specify a particular com-
parison method CI,E , we will write E :=
5
b) (A, B)
I(B, A)
E(I(A, B), I(B, A))
I(A, B) true
false
a)
B
true
false
A
E(I(A), I(B))
I(A)
I(B)
(A)I1
(B)I
(A)I
true
false
E(
A
(A)2I
B
(B)2I
(B)I1
(A),I (B))Ic)
Figure 3: Illustration of the concept of a compari-
son method for a single unary quality indicator (a),
a single binary quality indicator (b), and a combina-
tion of two unary quality indicators (c). In cases (a)
and (b), first the indicator I is applied to the two ap-
proximation sets A,B. The resulting two real values
are passed to the Boolean function E, which defines
the outcome of the comparison. In case (c), each of
the two indicators is applied to A and B, and the re-
sulting two indicator values are combined in a vector
I(A) and I(B) respectively. Afterwards, the Boolean
function E decides the outcome of the comparison on
the basis of these two real vectors.
<expression> instead of E(. . .) ⇔ <expression>
in order to improve readability. For in-
stance, E := (I1(A) > I1(B)) means that
E((I1(A), I2(A), . . . , Ik(A)), (I1(B), I2(B), . . . , Ik(B)))
is
true if and only if I1(A) > I1(B), given a combination
of k unary indicators.
Definition 3 may appear overly formal for describ-
ing what a comparison method basically is, and fur-
thermore it does not specify the actual conclusion
(what does it mean if CI,E(A,B) is true?). As we
will see in the following, however, it provides a sound
basis for studying the power of quality indicators—
the power of indicating relationships (better, incom-
parable, etc.) between approximation sets.
2.3 Linking Comparison Methods and
Dominance Relations
The goal of a comparative study is to reveal differ-
ences in performance between multiobjective opti-
mizers, and the strongest statement we can make in
this context is that an algorithm outperforms another
one. Independently of what definition of “outperfor-
mance” we use, it always should be compliant with
the most general notion in terms of the -relation,
i.e., the statement “algorithm a outperforms algo-
rithm b” should also imply that the outcome A of
the first method is better than the outcome B of
the second method (A  B).1 More accurate as-
sessments may be possible if preference information
is given [9], however, most studies assume that addi-
tional knowledge is not available, i.e., all objectives
are to be considered equally important.
In this paper, we are interested in the question
what conclusions can be drawn with respect to the
dominance relations listed in Table 1 on the basis of
a comparison method CI,E . If CI,E(A,B) is a suffi-
cient condition for, e.g., A  B, then this compari-
son method is capable of indicating that A is better
than B, i.e., CI,E(A,B) ⇒ A  B. If CI,E(A,B)
is in addition a necessary condition for A  B, then
the comparison method even indicates whether A is
better than B, i.e., CI,E(A,B) ⇔ A  B. In the
following, we will use the terms compatibility and
completeness in order to characterize a comparison
method in the above manner.
Definition 4 (Compatibility and completeness)
Let  be an arbitrary binary relation on approxima-
tion sets. The comparison method CI,E is denoted
as -compatible if either for any A,B ∈ Ω
CI,E(A,B) ⇒ A  B
or for any A,B ∈ Ω
CI,E(A,B) ⇒ B  A
The comparison method CI,E is denoted as -
1Recall that we assume that only a single optimization run
is performed per algorithm.
6
complete if either for any A,B ∈ Ω
A  B ⇒ CI,E(A,B)
or for any A,B ∈ Ω
B  A ⇒ CI,E(A,B)
To illustrate this terminology, let us go back to the
example depicted in Fig. 2 and consider the follow-
ing binary indicator I, which is inspired by concepts
presented in [15]:
Definition 5 (Binary -indicator) Suppose with-
out loss of generality a minimization problem with
n positive objectives, i.e., Z ⊆ IR+n. An objective
vector z1 = (z11 , z
1
2 , . . . , z
1
n) ∈ Z is said to -dominate
another objective vector z2 = (z21 , z
2
2 , . . . , z
2
n) ∈ Z,
written as z1  z2, if and only if
∀1 ≤ i ≤ n : z1i ≤  · z2i
for a given  > 0. We define the binary -indicator
I as
I(A,B) = inf
∈IR
{∀z2 ∈ B ∃z1 ∈ A : z1  z2}
for any two approximation sets A,B ∈ Ω.
The -indicator gives the factor by which an ap-
proximation set is worse than another with respect to
all objectives, or to be more precise: I(A,B) equals
the minimum factor  such that for any solution in B
there is at least one solution in A that is not worse
by a factor of  in all objectives.2 In practice, the 
value can be calculated as
I(A,B) = max
z2∈B
min
z1∈A
max
1≤i≤n
z1i
z2i
2In the same manner, an additive -indicator I+ can be
defined:
I+(A, B) = inf
∈IR
{∀z2 ∈ B ∃z1 ∈ A : z1 + z2}
where z1 + z2 if and only if
∀1 ≤ i ≤ n : z1i ≤  + z2i
5 10
5
2
f
f
1
P
A1
A2
A3
ε < 1
ε = 1
ε > 1
Figure 4: The dark-shaded area depicts the subspace
that is -dominated by the solutions in A1 for  =
9
10 ; the medium-shaded area represents the subspace
weakly dominated by A1 (equivalent to  = 1); the
light-shaded area refers to the subspace -dominated
by the solutions in A1 for  = 4. Note that the ar-
eas are overlapping, i.e., the medium-shaded area in-
cludes the dark-shaded one, and the light-shaded area
includes both of the other areas.
For instance, I(A1, A2) = 1, I(A1, A3) = 910 , and
I(A1, P ) = 4 in our previous example (cf. Fig. 4). In
the single-objective case, I(A,B) simply is the ratio
between the two objective values represented by A
and B.
Now, what comparison methods can be con-
structed using the -indicator? Consider, e.g., the
Boolean function E := (I(B,A) > 1). The corre-
sponding comparison method CI,E is -complete as
A  B implies that that I(B,A) > 1. On the other
hand, CI,E is not -compatible as A ‖ B also implies
that I(B,A) > 1. If we choose a slightly modified
Boolean function F := (I(A,B) ≤ 1 ∧ I(B,A) >
1), then we obtain a comparison method CI,F that is
both -compatible and -complete. The differences
between the two comparison methods are graphically
depicted in Fig. 5.
In the remainder of this paper, we will theoretically
study and classify quality indicators using the above
framework. Given a particular quality indicator (or
a combination of several indicators), we will inves-
tigate whether there exists a Boolean function such
that the resulting comparison method is compatible
and in addition complete with respect to the vari-
7
Figure 5: Top: Partitioning of the set of ordered
pairs (A,B) ∈ Ω2 of approximation sets into (over-
lapping) subsets induced by the different dominance
relations; each subset labeled with a certain rela-
tion  contains those pairs (A,B) for which A  B.
Note that this is only a schematic representation, e.g.,
there are no pairs (A,B) with A  B, A 	 B, and
A 	= B. Bottom: The black area stands for those or-
dered pairs (A,B) for which I(B,A) > 1 (left) resp.
I(A,B) ≤ 1 ∧ I(B,A) > 1 (right).
ous dominance relations. That is we determine how
powerful existing quality indicators are in terms of
their capability of indicating that or whether A  B,
A  B, A ‖ B, etc. The next section is devoted to
unary quality indicators, while binary indicators will
be discussed in Section 4.
3 Comparison Methods Based
on Unary Quality Indicators
Unary quality indicators are most commonly used in
the literature; what makes them attractive is their
capability of assigning quality values to an approxi-
mation set independent of other sets under consider-
ation. They have limitations, though, and there are
differences in the power of existing indicators as will
be shown in the following.
3.1 Limitations
Naturally, many studies have attempted to capture
the multiobjective nature of approximation sets by
deriving distinct indicators for the distance to the
Pareto-optimal front and the diversity within the ap-
proximation front. Therefore, the question arises
whether in principle there exists such a combina-
tion of, e.g., two indicators—one for distance, one
for diversity—such that we can detect whether an
approximation set is better than another. Such a
combination of indicators, applicable to any type of
problem, would be ideal because then any approxima-
tion set could be characterized by two real numbers
that reflect the different aspects of the overall quality.
The variety among the indicators proposed suggests
that this goal is, at least, difficult to achieve. The
following theorem shows that in general it cannot be
achieved.
Theorem 1 Suppose an optimization problem with
n ≥ 2 objectives where the objective space is Z = IRn.
Then, there exists no comparison method CI,E based
on a finite combination I of unary quality indica-
tors that is -compatible and -complete at the same
time, i.e,
CI,E(A,B) ⇔ A  B
for any approximation sets A,B ∈ Ω.
That is for any combination I of a finite number
of unary quality indicators we cannot find a Boolean
function E such that the corresponding comparison
method is -compatible and -complete. Or in other
words: the number of criteria, that determine what
a good approximation set is, is infinite.
We only sketch the proof here, the details can be
found in the appendix. First, we need the following
fundamental results from set theory [10]:
• IR, IRk, and any open interval (a, b) in IR resp.
hypercube (a, b)k in IRk have the same cardinal-
ity, denoted as 2ℵ0 , i.e., there is a bijection from
any of these sets to any other;
• If a set S has cardinality 2ℵ0 , then the cardinality
of the power set P(S) of S is 22ℵ0 , i.e., there is
8
2
z
z
1
b
a
a b
(a,a)
(b,b)
S
Figure 6: Illustration of the construction used in The-
orem 1 for a two-dimensional minimization problem.
We consider an open rectangle (a, b)2 and define an
open line S within. For S holds that any two ob-
jective vectors contained are incomparable to each
other, and therefore any subset A ⊆ S is an approxi-
mation set.
no injection from P(S) to any set of cardinality
2ℵ0 .
As we consider the most general case where Z = IRn,
we can construct a set S (cf. Fig. 6) such that any
two points contained are incomparable to each other.
Accordingly, any subset A of S is an approximation
set and the power set of S, the cardinality of which is
22
ℵ0 , is exactly the set of all approximation sets A ⊆
S. We will then show that any two approximation
sets A,B ⊆ S with A 	= B must differ in at least
one of the k indicator values. Therefore, an injection
from a set of cardinality 22
ℵ0 to IRk is required, which
finally leads to a contradiction.
Note that Theorem 1 also holds (i) if we only as-
sume that Z contains an open hypercube in IRn for
which CI,E has the desired property, and (ii) if we
consider any other relation from Table 1 (for ‖ and
 it follows directly from Theorem 1, for  and 
the proof has to be slightly modified).
Given this result, one may ask under which condi-
tions the construction of such a comparison method
is possible. For instance, such a comparison method
exists if we allow an infinite number of indicators.
The empirical attainment function [8], when applied
to single approximation sets, can be understood as
a combination of |Z| unary indicators, where |Z| de-
notes the cardinality of Z. If Z = IRn, then this com-
bination comprises an infinite number of unary indi-
cators. On its basis, a -compatible and -complete
comparison method can be constructed.
The situation also changes, if we require that each
approximation set contains at maximum l objective
vectors.
Corollary 1 Let Z = IRn. It exists a unary indica-
tor I and a Boolean function E such that
CI,E(A,B) ⇔ A  B
for any A,B ∈ Ω with |A|, |B| ≤ l.
Proof. Without loss of generality we restrict our-
selves to Z = (0, 1)n in the proof. The indicator I is
constructed as follows:
I(A) = 0.d11d
1
2 . . . d
1
l d
2
1d
2
2 . . . d
2
l d
3
1 . . .
where dij denotes the ith digit after the decimal
point of the jth element in A. If A contains less
than l elements, the first element is duplicated as
many times as necessary. Accordingly, there is an
injective function R that maps each real number in
(0, 1) to an approximation set. If we define E as
E := (R(I(A))  R(I(B))), the corresponding com-
parison method CI,E has the desired properties. 
The theorem, however, is rather of theoretical than
of practical use. The indicator constructed in the
proof is able to indicate whether A is better than
B, but it does not express how much better it is—
this is one of the motives for using quality indicators.
What we actually want is to apply a metric to the
indicator values. Therefore, a reasonable requirement
for a useful combination of indicators may be that if
A is better than or equal to B, then A is at least as
good as B with respect to all k indicators, i.e.:
A  B ⇒ (∀ 1 ≤ i ≤ k : Ii(A) ≥ Ii(B)
)
That this condition holds is an implicit assumption
made in many studies. If we now restrict the size of
9
the approximation sets to l and assume an indicator
combination with the above property, can we then
detect whether A is better than B? To answer this
question, we will investigate a slightly reformulated
statement, namely
A  B ⇔ (∀ 1 ≤ i ≤ k : Ii(A) ≥ Ii(B)
)
as this is equivalent to
A  B ⇔ (∀ 1 ≤ i ≤ k : Ii(A) ≥ Ii(B)
) ∧(∃ 1 ≤ j ≤ k : Ij(A) > Ij(B)
)
Furthermore, we will only consider the simplest case
where l = 1, i.e., each approximation set consists of
a single objective vector.
Theorem 2 Suppose an optimization problem with
n ≥ 2 objectives where the objective space is Z =
IRn. Let I = (I1, I2, . . . , Ik) be a combination of k
unary quality indicators and E :=
(∀ 1 ≤ i ≤ k :
Ii({z1}) ≥ Ii({z2})
)
a Boolean function such that
CI,E({z1}, {z2}) ⇔ z1  z2
for any pair of objective vectors z1,z2 ∈ Z. Then,
the number of indicators is greater than or equal to
the number of objectives, i.e., k ≥ n.
Proof. See appendix.
This theorem is a formalization of what is intu-
itively clear: we cannot reduce the dimensionality of
the objective space without losing information. We
need at least as many indicators as objectives to be
able to detect whether an objective vector weakly
dominates or dominates another objective vector. As
a consequence, a fixed number of unary indicators is
not sufficient for problems of arbitrary dimensionality
even if we consider sets containing a single objective
vector only.
In summary, we can state that the power of unary
quality indicators is restricted. Theorem 1 proves
that there does not exist any comparison meth-
ods based on unary indicators that is -compatible
and -complete at the same time. This rules
out also other combinations, Table 2 shows which.
It reveals that the best we can achieve is either
compatibility completeness
none      
 + - - - - - -
 + ? - - - - -
 + ? ? - - - -
 + + + + - ? ?
 + + + + - - ?
 + + + + - - -
Table 2: Overview of possible compatibil-
ity/completeness combinations with unary quality
indicators. A minus means there is no comparison
method CI,E that is compatible regarding the
row-relation and complete regarding the column-
relation. A plus indicates that such a comparison
method is known, while a question mark stands
for a combination for which it is unclear whether a
corresponding comparison method exists.
-compatibility without any completeness, or 	-
compatibility in combination with -completeness.
That means we either can make strong statements
(“A strongly dominates B”) for only a few pairs
A  B; or we can make weaker statements (“A is
not worse than B”, i.e., A  B or A ‖ B) for all
pairs A  B.
3.2 Classification
We now will review existing unary quality indicators
according to the inferential power of the compari-
son methods that can be constructed on their ba-
sis: -compatible, 	-compatible, and not compati-
ble with any relation listed in Table 2. Table 3 pro-
vides an overview of the various indicators discussed
here. In this context, we would also like to point
out the relationships between the dominance rela-
tions, e.g., -compatibility implies -compatibility,
	-compatibility implies 	-compatibility, and -
completeness implies -completeness.
3.2.1 -Compatibility
The use of -compatible comparison methods based
on unary indicators is restricted according to Theo-
rem 2: in order to detect dominance between objec-
10
tive vectors at least as many indicators as objectives
are required. Hence, it is not surprising that, to our
best knowledge, no -compatible comparison meth-
ods have been proposed in the literature; their design,
though, is possible:
• Suppose a minimization problem and let
IHC1 (A) = supa∈IR {{(a, a, . . . , a)}  A}
IHC2 (A) = infb∈IR {{(b, b, . . . , b)}  A}
We assume that Z is bounded, i.e.,IHC1 (A) and
IHC2 (A) always exists. As illustrated in Fig. 7,
the two indicator values characterize a hyper-
cube that contains all objective vectors in A. If
we define the indicator IHC = (IHC1 , I
HC
2 ) and
the Boolean function E as E := (IHC2 (A) <
IHC1 (B)), then the comparison method CIHC,E
is -compatible.
• Suppose a minimization problem and let
IOi (A) = inf
a∈IR
{∀(z1, . . . , zn) ∈ A : zi ≤ a}
for 1 ≤ i ≤ n and
IOn+1(A) =


0 if A contains two or
more elements
1 else
The idea behind these indicators is similar to
above. We consider the smallest hyperrectan-
gle that entirely encloses A. This hyperrectan-
gle comprises exactly one point O that is weakly
dominated by all members in A; in the case of a
two-dimensional minimization problem, it is the
upper right corner of the enclosing rectangle (cf.
Fig. 7). We see that IO1 , . . . , I
O
n are the coordi-
nates of this point O. IOn+1 serves to distinguish
between single objective vectors and larger ap-
proximation sets. Let IO = (IO1 , . . . , I
O
n+1) and
define the Boolean function E as E := (∀1 ≤ i ≤
n + 1 : IOi (A) < I
O
i (B)). Then, the comparison
method CIO,E is -compatible; it detects dom-
inance between an approximation set and those
objective vectors that are dominated by all mem-
bers of this approximation set.
























2
z
z
1
2
z
z
1
(a, a)
a b
b
a
A
A
(b, b)
O = (c, d)
Figure 7: Two indicators capable of indicating that
A  B for some A,B ∈ Ω. On the left hand side, it is
depicted how the IHC indicator defines a hypercube
around an approximation set A, where IHC1 (A) = a
and IHC2 (A) = b. The right picture is related to the
IO indicator: for any objective vector in the shaded
area we can detect that it is dominated by the ap-
proximation set A. Here, IO1 (A) = c, I
O
2 (A) = d, and
IO3 (A) = 0.
Note that both comparison methods are even -
compatible, but neither is complete with regard to
any dominance relation.
Moreover, some unary indicators can also be used
to design a -compatible comparison method if the
Pareto-optimal front P is known. Consider, e.g., the
following unary -indicator I1 that is based on the
binary -indicator from Definition 5:
I1(A) = I(A,P )
Obviously, I1(A) = 1 implies A = P . Thus, in com-
bination with the Boolean function E := (I1(A) =
1 ∧ I1(B) > 1) a comparison method can be defined
that is -compatible and detects that A is better than
B for all pairs A,B ∈ Ω with A = P and B 	= P .
The same construction can be made for some other
indicators, e.g., the hypervolume indicator, as well.
Nevertheless, these comparison methods are only ap-
plicable if some of the algorithms under consideration
can actually generate the Pareto-optimal front.
11
3.2.2 	-Compatibility
Consider the above unary -indicator I1. For any
pair A,B ∈ Ω it holds
A  B ⇒ I1(A) < I1(B)
and (which follows from this)
I1(A) < I1(B) ⇒ A 	≺	≺ B ⇒ A 	 B
Therefore, the comparison method CI1,E with E :=
(I1(A) < I1(B)) is 	-compatible and -complete,
but neither - nor -complete. That is whenever
A  B, we will be able to state that A is not worse
than B. On the other hand, there are cases A  B
for which this conclusion cannot be drawn, although
A is actually not worse than B. The same holds
for the two indicators proposed by [6] and [1]. We
will not discuss these in detail and only remark that
the following example can be used to show that both
indicators in combination with the Boolean function
E := (I(A) < I(B)) are not -complete (and -
complete): the Pareto-optimal front is P = {(1, 1)},
and A = {(4, 2)} and B = {(4, 3)}.
The hypervolume indicator IH [26][24] is the only
unary indicator we are aware of that is capable of
detecting that A is not worse than B for all pairs
A  B. It gives the hypervolume of that portion of
the objective space that is weakly dominated by an
approximation set A.3 We notice that from A  B
follows that IH(A) > IH(B); the reason is that A
must contain at least one objective vector that is not
weakly dominated by B, thus, a certain portion of
the objective space is dominated by A but not by B.
This observation implies both 	-compatibility and
-completeness.
Van Veldhuizen [21] suggested an indicator, the er-
ror ratio IER, on the basis of which a 	-compatible
(but not 	-compatible) comparison method can be
defined. IER(A) gives the ratio of Pareto-optimal ob-
jective vectors to all objective vectors in the approx-
imation set A. Obviously, if IER(A) > 0, i.e., A con-
tains at least one Pareto-optimal solution, then there
3Note that Z has to be bounded, i.e., there must exist a
hypercube in IRn that encloses Z. If this requirement is not
fulfilled, it can be easily achieved by an appropriate transfor-
mation.
exists no B ∈ Ω with B  A. On the other hand, if
A consist of only a single Pareto-optimal point, then
IER(A) ≥ IER(B) for all B  A; if B contains not
only Pareto-optimal points, then IER(A) > IER(B).
Therefore, C(IER,E) with E := (IER(A) > IER(B))
is not 	-compatible. However, if we consider just
the total number (rather than the ratio) of Pareto-
optimal points in the approximation set, we obtain
	-compatibility. This also holds for the indicator
used in [20], which gives the ratio of the number of
Pareto-optimal solutions in A to the cardinality of
the Pareto-optimal front. Nevertheless, the power of
these comparison methods is limited because none of
them is complete with respect to any dominance re-
lation.
3.2.3 Incompatibility
Section 3.1 has revealed the difficulties when trying to
separate the overall quality of approximation sets into
distinct aspects. Nevertheless, it would be desirable
if we could look at certain criteria such as diversity
separately, and accordingly several authors suggested
formalizations of specific aspects by means of unary
indicators. However, we have to be aware that often
these indicators do in general neither indicate that
A  B nor A 	 B.
One class of indicators that do not allow any
conclusions to be drawn regarding the domi-
nance relationship between approximation sets is
represented by the various diversity indicators
[18][17][24][16][3][23]. If we consider a pair A,B ∈ Ω
with A  B, in general the indicator value of A can
be less or greater than or even equal to the value
assigned to B (for the diversity indicators referenced
above). Therefore, the comparison methods based on
these indicators are neither compatible nor complete
with respect to any dominance relation or comple-
ment of it. For a more detailed discussion of some of
the above indicators, the interested reader is referred
to [14].
The same holds for the three indicators proposed in
[21]: overall nondominated vector generation IONVG,
generational distance IGD, and maximum Pareto
front error IME. The first just gives the number of
12
indicator name / reference Boolean function compatibility completeness
IHC enclosing hypercube indicator / Section 3.2.1 I
HC
2 (A) < I
HC
1 (B)  -
IO objective vector indicator / Section 3.2.1 I
O
i (A) > I
O
i (B)  -
IH hypervolume indicator / [26] IH(A) > IH(B)  
IW average best weight combination / [6] IW (A) < IW (B)  
ID distance from reference set / [1] ID(A) < ID(B)  
I1 unary -indicator / Section 3.2.2 I1(A) < I1(B)  
IPF fraction of Pareto-optimal front covered / [20] IPF(A) > IPF(B)  -
IP number of Pareto points contained / Section 3.2.2 IP (A) > IP (B)  -
IER error ratio / [21] IER(A) > 0  -
ICD chi-square-like deviation indicator / [18] ICD(A) < ICD(B) - -
IS spacing / [17] IS(A) < IS(B) - -
IONVG overall nondominated vector generation / [21] IONVG(A) > IONVG(B) - -
IGD generational distance / [21] IGD(A) < IGD(B) - -
IME maximum Pareto front error / [21] IME(A) < IME(B) - -
IMS maximum spread / [24] IMS(A) > IMS(B) - -
IMD minimum distance between two solutions / [16] IMD(A) > IMD(B) - -
ICE coverage error / [16] ICE(A) < ICE(B) - -
IDU deviation from uniform distribution / [3] IDU(A) < IDU(B) - -
IOS Pareto spread / [23] IOS(A) > IOS(B) - -
IA accuracy / [23] IA(A) > IA(B) - -
INDC number of distinct choices / [23] INDC(A) > INDC(B) - -
ICL cluster / [23] ICL(A) < ICL(B) - -
Table 3: Overview of unary indicators. Each entry corresponds to a specific comparison method defined by
the indicator and the Boolean function in that row. With respect to compatibility and completeness, not all
relations are listed but only the strongest as, e.g., -compatibility implies -compatibility (cf. Section 3.2).
elements in the approximation set, and it is obvi-
ous that it does not provide sufficient information to
conclude A  B, A 	 B, etc. Why this also ap-
plies to the other two, both distance indicators, will
only be sketched here. Assume a two-dimensional
minimization problem for which the Pareto-optimal
front P consists of the two objective vectors (1, 0) and
(0, 10). Now, consider the three sets A = {(2, 5)},
B = {(3, 9)}, and C = {(10, 10)}. For both dis-
tance indicators holds I(B) < I(A) < I(C), but
A  B  C, provided that Euclidean distance is
considered. Thus, we cannot conclude whether one
set is better or worse than another by just looking
at the order of the indicator values. A similar argu-
ment as for the generational distance applies to the
coverage error indicator presented in [16]; the only
difference is that the coverage error denotes the min-
imum distance to the Pareto-optimal front instead of
the average distance.
Finally, one can ask whether it is possible to com-
bine several indicators for which no 	-compatible
comparison method exists in such a way that the re-
sulting indicator vector allows to detect that A is not
worse than B. Van Veldhuizen and Lamont [22], for
instance, used generational distance and overall non-
dominated vector generation in conjunction with the
diversity indicator of [17], while Deb et al. [4] applied
a similar combination of diversity and distance indi-
cators. Other examples can be found in, e.g., [2] and
[16]. As in all of these cases counterexamples can be
constructed that show the corresponding comparison
methods to be not 	-compatible, the above question
remains open and is not investigated in more depth
here.
13
4 Comparison Methods Based
on Binary Quality Indicators
Binary quality indicators can be used to overcome
the difficulties with unary indicators. However, they
also have a drawback: when we compare t algorithms
using a single binary indicator, we obtain t(t−1) dis-
tinct indicator values—in contrast to the t values in
the case of a unary indicator. This renders the anal-
ysis and the presentation of the results more difficult.
Nevertheless, Theorem 1 suggests that this is in the
nature of multiobjective optimization problems.
4.1 Limitations
In principle, there are no such theoretical limitations
of binary indicators as for unary indicators. For in-
stance, the indicator
I(A,B) =


4 A  B
3 A  B
2 A  B
1 A = B
0 else
allows to construct comparison methods compatible
and complete with regard to any of the dominance re-
lations. However, this usually does not hold for exist-
ing, practically useful binary indicators, in particular
for those indicators that are, as Knowles and Corne
[13] denote it, symmetric, i.e., I(A,B) = c− I(B,A)
for a constant c. Although, symmetric indicators are
attractive as only half the number of indicator values
has to be considered in comparison to a general bi-
nary indicator, their inferential power is restricted as
we will show in the following.
Without loss of generality, suppose that c = 0, i.e.,
I(A,B) = −I(B,A); otherwise consider the trans-
formation I ′(A,B) = c/2 − I(A,B). The question
is whether we can construct a -compatible and -
complete comparison method based on this indicator;
according to the discussion in Section 3.1, we assume
that E =: (I(A,B) > I(B,A)).
Theorem 3 Let I be a binary indicator with
I(A,B) = −I(B,A) for A,B ∈ Ω and E a Boolean
function with E =: I(A,B) > I(B,A). If the cor-
responding comparison method CI,E is -compatible
and -complete, then I(A,B) = 0 for all A,B ∈ Ω
with A = B or A ‖ B.
Proof. Let A,B ∈ Ω. From A  B ⇔ I(A,B) >
I(B,A) follows that A 	 B ⇔ I(A,B) ≤ I(B,A)
and therefore A ‖ B ∨ A = B ⇔ A 	 B ∧ B 	 A ⇔
I(A,B) = I(B,A). From the symmetry I(A,B) =
−I(B,A) then follows that A ‖ B ∨ A = B is equiv-
alent to I(A,B) = 0. 
A consequence of this theorem is that a symmet-
ric, binary indicator, for which A  B ⇔ I(A,B) >
I(B,A), can detect whether A is better than B, but
not whether A  B, A ‖ B, or A = B. On the
other hand, it follows from I(A,B) 	= 0 for a pair
A ‖ B that CI,E cannot be -compatible, if it is
-complete. We will use this result in the following
discussion of existing binary indicators.
4.2 Classification
In contrast to unary indicators, only a few binary
indicators can be found in the literature. We will
classify them according to the criterion whether a
corresponding comparison method exists that is -
compatible and -complete with regard to a specific
relation .
As mentioned in Section 2.2, Zitzler and Thiele [26]
suggested the coverage indicator IC where IC(A,B)
gives the fraction of solutions in B that are weakly
dominated by at least one solution in A. IC(A,B) =
1 is equivalent to A  B (A weakly dominates B)
and therefore comparison methods CIC ,E compati-
ble and complete with regard to the , , ‖, and
= relations can be constructed. Furthermore, with
E := (IC(A,B) = 1 ∧ IC(B,A) = 0) we obtain a
comparison method CIC ,E that is -compatible and
-complete.
Hansen and Jaszkiewicz [9] proposed three sym-
metric, binary indicators IR1 , IR2 , and IR3 that are
based on a set of utility functions. The utility func-
tions can be used to formalize and incorporate pref-
14
erence information; however, if no additional knowl-
edge is available, Hansen and Jaszkiewicz suggest to
use a set of weighted Tchebycheff utility functions.
In this case, the resulting comparison methods are in
general -complete but not -compatible as Theo-
rem 3 applies (I(A,B) can be greater or less than 0
if A ‖ B). Accordingly, these indicators in general
do not allow to construct a comparison method that
is both compatible and complete with respect to any
of the relations in Table 1. However, it has to be
emphasized here that these indicators have been de-
signed with regard to the incorporation of preference
information.
In [24], a binary version IH2 of the hypervolume
indicator IH [26] was proposed; the same indicator
was used in [12]. IH2(A,B) is defined as the hyper-
volume of the subspace that is weakly dominated by
A but not by B. From IH2(A,B) = 0 follows that
B  A and therefore, as with the coverage indicator,
comparison methods CIH ,E compatible and complete
regarding the , , ‖, and = relations are possi-
ble. However, there exists no -compatible and -
complete or -compatible and -complete com-
parison method solely based on the binary hypervol-
ume indicator.
Knowles and Corne [11] presented a comparison
method based on the study by Fonseca and Flem-
ing [7]. Although designed for the statistical anal-
ysis of multiple optimization runs, the method can
be formulated in terms of an m-ary indicator ILI if
only one run is performed per algorithm or the algo-
rithms are deterministic. We here restrict ourselves
to the case m = 2 as all of the following statements
also hold for m > 2. A user-defined set of lines
in the objective space, all of them passing the ori-
gin and none of them perpendicular to any of the
axes, forms the scaffolding of Knowles and Corne’s
approach. First, for each line the intersections with
the attainment surfaces [7] defined by the approxi-
mation sets under consideration are calculated. The
intersections are then sorted according to their dis-
tance to the origin, and the resulting order defines
a ranking of the approximation sets with respect to
this line. If only two approximation sets are consid-
ered, then ILI(A,B) gives the fraction of the lines
for which A is ranked higher than B. Accordingly,
the most significant outcome would be ILI(A,B) = 1
and ILI(B,A) = 0. However, this method strongly
depends on the choice of the lines, and certain parts
of the attainment surface are not sampled. There-
fore, in the above case either A is better than B or
both approximation are incomparable to each other.
As a consequence, the comparison method CILI,E
with E := (ILI(A,B) = 1 ∧ ILI(B,A) = 0) is in
the general case not -compatible; however, it is 	-
compatible and -complete.
Finally, we have shown already in Section 2.3 that
a -compatible and -complete comparison method
exists for the -indicator. The case I(A,B) ≤ 1 is
equivalent to A  B and the same statements as
for the coverage and the binary hypervolume indi-
cators hold. Furthermore, the comparison method
CI,E with E := (I(A,B) < 1) is -compatible
and -complete.
Table 4 summarizes the results of this section.
Note that it only contains information about compar-
ison methods that are both compatible and complete
with respect to the different dominance relations.
5 Discussion
5.1 Summary of Results
We have proposed a mathematical framework to
study quality assessment methods for multiobjective
optimizers. Starting with the assumption that the
outcome of a multiobjective EA is a set of incom-
parable solutions, a so-called approximation set, we
have introduced several dominance relations on ap-
proximation sets. These relations represent a for-
mal description of what we intuitively understand
by one approximation set being better than another.
The term quality indicator has been used to capture
the notion of a quality measure, and a comparison
method has been defined as a combination of quality
indicators and a pseudo-Boolean function that evalu-
ates the indicator values. Furthermore, we have dis-
cussed two properties of comparison methods, namely
compatibility and completeness, which characterize
15
ind. name / reference relation
    = ‖
I epsilon indicator / I(A, B) < 1 - I(A, B) ≤ 1 I(A, B) ≤ 1 I(A, B) = 1 I(A, B) > 1
Section 2.2 I(B, A) > 1 I(B, A) = 1 I(B, A) > 1
I+ additive epsilon I+(A, B) < 0 - I+(A, B) ≤ 0 I+(A, B) ≤ 0 I+(A, B) = 0 I+(A, B) > 0
indicator / Section 2.2 I+(B, A) > 0 I+(B, A) = 0 I+(B, A) > 0
IC coverage / [26] - IC(A, B) = 1 IC(A, B) = 1 IC(A, B) = 1 IC(A, B) = 1 0 < IC(A, B) < 1
IC(B, A) = 0 IC(B, A) < 1 IC(B, A) = 1 0 < IC(B, A) < 1
IH2 binary hypervolume - - IH2(A, B) > 0 IH2(A, B) ≥ 0 IH2(A, B) = 0 IH2(A, B) > 0
indicator / [24] IH2(B, A) = 0 IH2(B, A) = 0 IH2(B, A) = 0 IH2(B, A) > 0
IR1 utility function - - - - - -
indicator R1 / [9]
IR2 utility function - - - - - -
indicator R2 / [9]
IR3 utility function - - - - - -
indicator R3 / [9]
ILI lines of intersection / - - - - - -
[11]
Table 4: Overview of binary indicators. A minus means that in general there is no comparison method CI,E
based on the indicator I in the corresponding row that is compatible and complete regarding the relation in
the corresponding column. Otherwise, an expression is given that describes an appropriate Boolean function
E.
the relationship between comparison methods and
dominance relations. On the basis of this framework,
existing comparison methods have been analyzed and
discussed. The key results are:
• Unary quality indicators, i.e., quality measures
that summarize an approximation set in terms of
a real number, are in general not capable of in-
dicating whether an approximation set is better
than another—also if several of them are used.
This even holds if we consider approximation
sets containing a single objective vector only.
• Existing unary indicators at best allow to infer
that an approximation set is not worse than an-
other, e.g., the distance indicator by Czyzak and
Jaszkiewicz [1], the hypervolume indicator by
Zitzler and Thiele [26], or the unary -indicator
presented in this paper. However, with many
unary indicators and also combinations of unary
indicators no statement about the relation be-
tween the corresponding approximation sets can
be made. That is, although an approximation
set A may be evaluated better than an approxi-
mation set B with respect to all of the indicators,
B can actually be superior to A with respect to
the dominance relations. This holds especially
for the various diversity measures and also for
some of the distance indicators proposed in the
literature.
• We have given two examples demonstrating that
comparison methods based on unary indicators
can be constructed such that A can be recog-
nized as being better than B for some approxi-
mation sets A,B. It has also been shown that
the practical use of this type of indicator is nat-
urally restricted.
• Binary indicators, which assign real numbers to
ordered pairs of approximation sets, in princi-
ple do not possess the theoretical limitations of
unary indicators. The binary -indicator pro-
posed in this paper, e.g., is capable of detecting
whether an approximation set is better than an-
other. However, not all existing binary indica-
tors have this property. Furthermore, it has to
be mentioned that the greater inferential power
16
comes along with additional complexity: in con-
trast to unary indicators, the number of indi-
cator values to be considered is not linear but
quadratic in the number of approximation sets.
5.2 Conclusions
This study has shown that in general the quality of
an approximation set cannot be completely described
by a (finite) set of distinct criteria such as diversity
and distance. Hence, binary quality indicators rep-
resent the lowest level of representation on which it
is still possible to detect whether an algorithm per-
forms better than another in terms of the quality of
the outcomes. On the other hand, this does not mean
that unary quality indicators are generally useless.
In conjunction with a -compatible and -complete
comparison method, they can be used to further dif-
ferentiate between incomparable approximation sets
and to focus on specific, usually problem-dependent
aspects. However, we have to be aware that they
often represent preference information and therefore
for each problem the assumptions and knowledge ex-
ploited should be clearly specified. A more detailed
discussion of this issue can be found in [9].
Moreover, we have studied quality indicators only
for one, but essential criterion: the inferential power.
Certainly, there are many other aspects according
to which comparison methods can be investigated,
e.g., the computational effort, the sensitivity to scal-
ing, the requirement to have knowledge about the
Pareto-optimal front, etc. Several such aspects are
studied in [14] and [13]. The coverage indicator [25]
represents an example where these additional con-
siderations come into play. Although being capable
of detecting dominance between approximation sets,
it does not provide additional information if, e.g., A
dominates B and B dominates C (“how much bet-
ter is A than B with respect to C?”); furthermore,
the indicator values are often difficult to interpret if
the two approximation sets under consideration are
incomparable. In the light of this discussion, the bi-
nary -indicator defined in Section 2.2 possesses sev-
eral desirable features. It represents a natural ex-
tension to the evaluation of approximation schemes
in theoretical computer science [5] and gives the fac-
tor by which an outcome is worse than another. In
addition to that, it is cheap to compute.
Finally, the stochasticity of multiobjective EAs is
another issue that has to be addressed. Multiple op-
timization runs require the application of statistical
tests, and in principle there are two ways to incorpo-
rate these tests in a comparison method: the statisti-
cal testing procedure can be included in the indicator
functions or in the Boolean function. Knowles and
Corne’s approach [11] belongs to the first category,
while Van Veldhuizen and Lamont’s study [22] is an
example for the second category. The attainment
function method proposed by Grunert da Fonseca,
Fonseca, and Hall [8] can be expressed in terms of
an infinite number of indicators and therefore falls in
the second category. However, in contrast to [22] and
[11] this method is able to detect whether an approx-
imation set is better than another. To investigate in
more depth how all these approaches are related to
each other is the subject of ongoing research.
Appendix
Proof of Theorem 1. Let us suppose that
such a comparison method CI,E exists where I =
(I1, I2, . . . , Ik) is a combination of k unary quality
indicators and E a corresponding Boolean function
IR2k → {false, true}. Furthermore, assume, without
loss of generality, that the first two objectives are to
be minimized (otherwise the definition of the follow-
ing set S has to be modified accordingly).
Choose a, b ∈ IR with a < b, and consider S =
{(z1, z2, . . . , zn) ∈ Z ; a < zi < b, 1 ≤ i ≤ n ∧ z2 =
b + a − z1}; obviously, for any z1,z2 ∈ Z either
z1 = z2 or z1 ‖ z2, because z11 > z21 implies z12 < z22 .
Furthermore, let ΩS ⊆ Ω denote the set of approxi-
mations sets A ∈ Ω with A ⊆ S.
As S ∈ Ω and any subset of an approximation set
is again an approximation set, ΩS is identical to the
power set P(S) of S. In addition, there is an injection
f from the open interval (a, b) to S with f(r) = (r, b+
a−r, (b+a)/2, (b+a)/2, . . . , (b+a)/2), it follows that
the cardinality of S is at least 2ℵ0 . As a consequence,
17
the cardinality of ΩS is at least 22
ℵ0 .
Now, we will use Lemma 1 (see below): it shows
that for any A,B ∈ ΩS with A 	= B the quality
indicator values differ, i.e., Ii(A) 	= Ii(B) for at least
one indicator Ii, 1 ≤ i ≤ k. Therefore, there must be
an injection from ΩS to IRk, the codomain of I. This
means there is an injection from a set of cardinality
22
ℵ0 (or greater) to a set of cardinality 2ℵ0 . From this
absurdity, it follows that such a comparison method
CI,E cannot exist. 
Lemma 1 Let Z = {(z1, z2, . . . , zn) ∈ IRn; a < zi <
b, 1 ≤ i ≤ n} be an open hypercube in IRn with n ≥ 2,
a, b ∈ IR, and a < b. Furthermore, assume there
exists a combination of unary quality indicators I =
(I1, I2, . . . , Ik) and a Boolean function E such that
for any approximation sets A,B ∈ Ω:
CI,E(A,B) ⇔ A  B
Then, for all A,B ∈ Ω with A 	= B there is at least
one quality indicator Ii with 1 ≤ i ≤ k such that
Ii(A) 	= Ii(B).
Proof. Let A,B ∈ Ω be two arbitrary approxima-
tion sets with A 	= B. First note that CI,E(A,B)
implies CI,E(B,A) is false (and vice versa) as A  B
implies B 	 A. If A  B or B  A, then
Ii(A) 	= Ii(B) for at least one 1 ≤ i ≤ k because oth-
erwise CI,E(A,B) = CI,E(B,A) = CI,E(A,A) would
be false. If A ‖ B, there are two cases: (1) both A
and B contain only a single objective vector, or (2)
either set consists of more than one element.
Case 1: Choose z ∈ Z with A ‖ {z} and B ‖ {z}
(such an objective vector exists as Z is an open
hypercube in IRn). Then A ∪ {z}  A and
A ∪ {z} ‖ B, and from the former follows
CI,E(A ∪ {z}, A) is true. Accordingly, Ii(A) 	=
Ii(B) for at least one 1 ≤ i ≤ k because other-
wise CI,E(A∪{z}, B) = CI,E(A∪{z}, A) would
be true which contradicts A ∪ {z} ‖ B.
Case 2: Assume, without loss of generality, that A
contains more than one objective vector, and
choose z ∈ A with {z} ‖ B (such an ele-
ment must exist as A ‖ B). Then, A  {z},
which implies that CI,E(A, {z}). Now suppose
Ii(A) = Ii(B) for all 1 ≤ i ≤ k; it follows that
CI,E(B, {z}) = CI,E(A, {z}) is true which is a
contradiction to B ‖ {z}.
In summary, all cases (A  B, B  A, and A ‖ B)
imply that Ii(A) 	= Ii(B) for at least one 1 ≤ i ≤ k.

Proof of Theorem 2. We will exploit the fact
that in IR the number of disjoint open intervals
(a, b) = {z ∈ IR ; a < z < b} with a < b is count-
able [10]; in general, this means that IRk contains
only countably many disjoint open hyperrectangles
(a1, b1)× (a2, b2)× · · · × (ak, bk) = {(z1, z2, . . . , zk) ∈
IRk ; ai < zi < bi, 1 ≤ i ≤ k} with ai < bi. The basic
idea is that whenever fewer indicators than objectives
are available, uncountably many disjoint open hyper-
rectangles arise—a contradiction. Furthermore, we
will show a slightly modified statement, which is more
general: if Z contains an open hypercube (u, v)n with
u < v such that for any z1,z2 ∈ (u, v)n:
(∀ 1 ≤ i ≤ k : Ii({z1}) ≥ Ii({z2})
) ⇔ z1  z2
then k ≥ n.
Without loss of generality assume a minimization
problem in the following. We will argue by induction.
n = 2: Let a, b ∈ (u, v) with a < b and consider the
incomparable objective vectors (a, b) and (b, a).
If k = 1, then either I1({(a, b)}) ≥ I1({(b, a)})
or vice versa; this leads to a contradiction to
(a, b) 	 (b, a) and (b, a) 	 (a, b).
n − 1 → n: Suppose n > 2, k < n and that the
statement holds for n − 1. Choose a, b ∈ (u, v)
with a < b, and consider the n − 1 dimensional
open hypercube Sc = {(z1, z2, . . . , zn−1, c) ∈
(u, v)n ; a < zi < b, 1 ≤ i ≤ n − 1} for an
arbitrary c ∈ (u, v).
First, we will show that Ii({(b, . . . , b, c)}) <
Ii({(a, . . . , a, c)}) for all 1 ≤ i ≤ k. As-
sume Ii({(b, . . . , b, c)}) ≥ Ii({(a, . . . , a, c)})
18
for any i. If Ii({(b, . . . , b, c)}) >
Ii({(a, . . . , a, c)}), then (a, . . . , a, c) 	
(b, . . . , b, c), which yields a contradiction.
If Ii({(b, . . . , b, c)}) = Ii({(a, . . . , a, c)}), then
Ii({z}) = Ii({(b, . . . , b, c)}) for all z ∈ Sc,
because (a, . . . , a, c)  z if z ∈ Sc. Then for any
z1,z2 ∈ Sc it holds
∀1 ≤ j ≤ k, j 	= i :
Ij({z1}) ≥ Ij({z2}) ⇔ z1  z2
which contradicts the assumption that for any
n − 1 dimensional open hypercube in IRn−1 at
least n − 1 indicators are necessary. Therefore,
Ii({(b, . . . , b, c)}) < Ii({(a, . . . , a, c)}).
Now, we consider the image of Sc in indi-
cator space. The vectors I({(b, . . . , b, c)})
and I({(a, . . . , a, c)}) determine an open
hyperrectangle Hc = {(y1, y2, . . . , yk) ∈
IRk ; Ii({(b, . . . , b, c)}) < yi <
Ii({(a, . . . , a, c)}), 1 ≤ i ≤ k} where
I(z) = (I1(z), I2(z), . . . , Ik(z)). Hc has
the following properties:
1. Hc is open in all k dimensions as for all
1 ≤ i ≤ k: inf{yi ; (y1, y2, . . . , yk) ∈ Hc} =
Ii({(b, . . . , b, c)}) < Ii({(a, . . . , a, c)}) =
sup{yi ; (y1, y2, . . . , yk) ∈ Hc}.
2. Hc contains an infinite number of elements.
3. Hc ∩ Hd = ∅ for any d ∈ (u, v), d > c: as-
sume y ∈ Hc∩Hd; then I({(a, . . . , a, c)}) ≥
y ≥ I({(b, . . . , b, d)}), which yields a con-
tradiction as (a, . . . , a, c) 	 (b, . . . , b, d).
Since c was arbitrarily chosen within (u, v), there
are uncountably many disjoint open hyperrect-
angles of dimensionality k in the k dimensional
indicator space. This contradiction implies that
k ≥ n. 
Acknowledgments
The authors would like to thank Wade Ramey for
the tip about the proof of Theorem 2. The re-
search has been supported by the Swiss National Sci-
ence Foundation (SNF) under the ArOMA project
2100-057156.99/1 and by the Portuguese Founda-
tion for Science and Technology under the POCTI
programme (Project POCTI/MAT/10135/98), co-
financed by the European Regional Development
Fund.
References
[1] P. Czyzak and A. Jaszkiewicz. Pareto simu-
lated annealing—a metaheuristic for multiobjec-
tive combinatorial optimization. Multi-Criteria
Decision Analysis, 7:34–47, 1998.
[2] Prabuddha De, Jay B. Ghosh, and Charles E.
Wells. Heuristic estimation of the efficient fron-
tier for a bi-criteria scheduling problem. Deci-
sion Sciences, 23:596–609, 1992.
[3] Kalyanmoy Deb. Multi-objective optimization
using evolutionary algorithms. Wiley, Chich-
ester, UK, 2001.
[4] Kalyanmoy Deb, S. Agrawal, A. Pratap, and
T. Meyarivan. A fast elitist non-dominated sort-
ing genetic algorithm for multi-objective opti-
mization: NSGA-II. In M. Schoenauer et al.,
editors, Parallel Problem Solving from Nature –
PPSN VI, pages 849–858, Berlin, 2000. Springer.
[5] Thomas Erlebach, Hans Kellerer, and Ulrich
Pferschy. Approximating multi-objective knap-
sack problems. In Frank K. H. A. Dehne, Jörg-
Rüdiger Sack, and Roberto Tamassia, editors,
Proceedings of the Seventh International Work-
shop on Algorithms and Data Structures (WADS
2001), pages 210–221, Berlin, Germany, 2001.
Springer.
[6] Henrik Esbensen and Ernest S. Kuh. Design
space exploration using the genetic algorithm. In
IEEE International Symposium on Circuits and
Systems (ISCAS’96), volume 4, pages 500–503,
Piscataway, NJ, 1996. IEEE Press.
19
[7] Carlos M. Fonseca and Peter J. Fleming. On
the performance assessment and comparison of
stochastic multiobjective optimizers. In Hans-
Michael Voigt, Werner Ebeling, Ingo Rechen-
berg, and Hans-Paul Schwefel, editors, Fourth
International Conference on Parallel Problem
Solving from Nature (PPSN-IV), pages 584–593,
Berlin, Germany, 1996. Springer.
[8] Viviane Grunert da Fonseca, Carlos M. Fon-
seca, and Andreia O. Hall. Inferential perfor-
mance assessment of stochastic optimisers and
the attainment function. In E. Zitzler, K. Deb,
L. Thiele, C. A. Coello Coello, and D. Corne,
editors, Proceedings of the First International
Conference on Evolutionary Multi-Criterion Op-
timization (EMO 2001), volume 1993 of Lec-
ture Notes in Computer Science, pages 213–225,
Berlin, 2001. Springer-Verlag.
[9] Michael P. Hansen and Andrzej Jaszkiewicz.
Evaluating the quality of approximations of the
non-dominated set. Technical report, Institute
of Mathematical Modeling, Technical University
of Denmark, 1998. IMM Technical Report IMM-
REP-1998-7.
[10] Karel Hrbacek and Thomas Jech. Introduction
to Set Theory. Marcel Dekker, Inc., New York,
1999.
[11] J. D. Knowles and D. W. Corne. Approximat-
ing the nondominated front using the pareto
archived evolution strategy. Evolutionary Com-
putation, 8(2):149–172, 2000.
[12] J. D. Knowles, D. W. Corne, and M. J. Oates.
On the assessment of multiobjective approaches
to the adaptive distributed database manage-
ment problem. In M. Schoenauer et al., editors,
Parallel Problem Solving from Nature – PPSN
VI, pages 869–878, Berlin, 2000. Springer.
[13] Joshua Knowles and David Corne. On met-
rics for comparing non-dominated sets. In
Congress on Evolutionary Computation (CEC
2002), pages 711–716, Piscataway, NJ, 2002.
IEEE Press.
[14] Joshua D. Knowles. Local-Search and Hybrid
Evolutionary Algorithms for Pareto Optimiza-
tion. PhD thesis, Department of Computer Sci-
ence, University of Reading, UK, 2002.
[15] Marco Laumanns, Lothar Thiele, Kalyanmoy
Deb, and Eckart Zitzler. Combining convergence
and diversity in evolutionary multi-objective op-
timization. Evolutionary Computation, 10(3),
2002.
[16] Serpil Sayin. Measuring the quality of discrete
representations of efficient sets in multiple ob-
jective mathematical programming. Math. Pro-
gram., Ser. A 87, pages 543–560, 2000.
[17] J. Schott. Fault tolerant design using single
and multicriteria genetic algorithm optimiza-
tion. Master’s thesis, Department of Aeronau-
tics and Astronautics, Massachusetts Institute of
Technology, 1995.
[18] N. Srinivas and Kalyanmoy Deb. Multiobjec-
tive optimization using nondominated sorting in
genetic algorithms. Evolutionary Computation,
2(3):221–248, 1994.
[19] K. C. Tan, T. H. Lee, and E. F. Khor. Evolu-
tionary algorithms for multi-objective optimiza-
ton: Performance assessments and comparisons.
In Proceedings of the 2001 Congress on Evolu-
tionary Computation CEC2001, pages 979–986,
COEX, World Trade Center, 159 Samseong-
dong, Gangnam-gu, Seoul, Korea, 27-30 May
2001. IEEE Press.
[20] E. L. Ulungu, J. Teghem, Ph. Fortemps, and
D. Tuyttens. Mosa method: a tool for solving
multiobjective combinatorial optimization prob-
lems. Journal of Multi-Criteria Decision Analy-
sis, 8:221–236, 1999.
[21] David A. Van Veldhuizen. Multiobjective Evo-
lutionary Algorithms: Classifications, Analyses,
and New Innovations. PhD thesis, Graduate
School of Engineering of the Air Force Institute
of Technology, Air University, June 1999.
20
[22] David A. Van Veldhuizen and Gary B. Lamont.
On measuring multiobjective evolutionary algo-
rithm performance. In A. Zalzala and R. Eber-
hart, editors, Congress on Evolutionary Compu-
tation (CEC 2000), volume 1, pages 204–211,
Piscataway, NJ, 2000. IEEE Press.
[23] Jin Wu and Shapour Azarm. Metrics for quality
assessment of a multiobjective design optimiza-
tion solution set. Transactions of the ASME,
Journal of Mechanical Design, 123:18–25, March
2001.
[24] Eckart Zitzler. Evolutionary Algorithms for Mul-
tiobjective Optimization: Methods and Applica-
tions. PhD thesis, Swiss Federal Institute of
Technology (ETH) Zurich, Switzerland, 1999.
TIK-Schriftenreihe Nr. 30, Diss ETH No. 13398,
Shaker Verlag, Aachen, Germany.
[25] Eckart Zitzler and Lothar Thiele. An evolu-
tionary algorithm for multiobjective optimiza-
tion: The strength pareto approach. Techni-
cal Report 43, Computer Engineering and Net-
works Laboratory (TIK), Swiss Federal Institute
of Technology (ETH) Zurich, Gloriastrasse 35,
CH-8092 Zurich, Switzerland, May 1998.
[26] Eckart Zitzler and Lothar Thiele. Multiobjec-
tive optimization using evolutionary algorithms
— a comparative case study. In Agoston E.
Eiben, Thomas Bäck, Marc Schoenauer, and
Hans-Paul Schwefel, editors, Fifth International
Conference on Parallel Problem Solving from
Nature (PPSN-V), pages 292–301, Berlin, Ger-
many, 1998. Springer.
Addendum (July 3, 2002)
Section 3.2.2, 1st paragraph: The statement
I1(A) < I1(B) ⇒ A 	≺	≺ B ⇒ A 	 B
is wrong. The 	-compatibility follows from
A  B ⇒ I1(A) ≤ I1(B)
which implies
I1(A) < I1(B) ⇒ A 	 B
Section 3.2.3 Independently of this study, Knowles
and Corne [14][13] have also shown the incom-
patibility of the following indicators: IS , IME,
IDU, and IONVG.
21

