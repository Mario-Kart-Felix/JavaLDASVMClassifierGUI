ar
X
iv
:1
70
6.
08
67
2v
1 
 [
cs
.L
G
] 
 2
7 
Ju
n 
20
17
Fast and robust tensor decomposition with
applications to dictionary learning
Tselil Schramm∗ David Steurer†
June 28, 2017
Abstract
We develop fast spectral algorithms for tensor decomposition that match the ro-
bustness guarantees of the best known polynomial-time algorithms for this problem
based on the sum-of-squares (SOS) semidefinite programming hierarchy.
Our algorithms can decompose a 4-tensor with n-dimensional orthonormal com-
ponents in the presence of error with constant spectral norm (when viewed as an
n2-by-n2 matrix). The running time is n5 which is close to linear in the input size n4.
We also obtain algorithms with similar running time to learn sparsely-used orthog-
onal dictionaries even when feature representations have constant relative sparsity
and non-independent coordinates.
The only previous polynomial-time algorithms to solve these problem are based on
solving large semidefinite programs. In contrast, our algorithms are easy to implement
directly and are based on spectral projections and tensor-mode rearrangements.
Or work is inspired by recent of Hopkins, Schramm, Shi, and Steurer (STOC’16) that
shows how fast spectral algorithms can achieve the guarantees of SOS for average-case
problems. In this work, we introduce general techniques to capture the guarantees of
SOS for worst-case problems.
∗UC Berkeley, tschramm@cs.berkeley.edu. T. S. is supported by an NSF Graduate Research Fellowship
(NSF award no. 1106400).
†Cornell University, dsteurer@cs.cornell.edu. D. S. is supported by a Microsoft Research Fellow-
ship, a Alfred P. Sloan Fellowship, NSF awards (CCF-1408673,CCF-1412958,CCF-1350196), and the Simons
Collaboration for Algorithms and Geometry.
Contents
1 Introduction 1
1.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Preliminaries 5
3 Techniques 6
4 Decomposing orthogonal 4-tensors 13
4.1 Postprocessing for closer vectors . . . . . . . . . . . . . . . . . . . . . . . . . 17
4.2 Near-orthonormal components . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4.3 Full Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4.4 Supporting Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
5 Learning Orthonormal Dictionaries 24
5.1 Postprocessing to refine approximation . . . . . . . . . . . . . . . . . . . . . 27
5.2 From independent columns to orthonormal columns . . . . . . . . . . . . . 29
5.3 Sample complexity bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
References 34
A Useful Tools 34
1 Introduction
Tensor decomposition is the following basic inverse problem: Given a k-th order tensor
T ∈ (d)⊗k of the form
T 
n∑
i1
a⊗k
i
+ E, (1.1)
we aim to approximately recover one or all of the unknown components a1, . . . , an ∈ d .
The goal is to develop algorithms that can solve this problem efficiently under the weakest
possible assumptions on the order k, the components a1, . . . , an, and the error E.
Tensor decomposition is studied extensively across many disciplines including ma-
chine learning and signal processing. It is a powerful primitive for solving a wide range
of other inverse / learning problems, for example: blind source separation / indepen-
dent component analysis [LCC07], learning phylogenetic trees and hidden Markov mod-
els [MR05], mixtures of Gaussians [HK13], topic models [AFH+12], dictionary learning
[BKS15, MSS16], and noisy-or Bayes nets [AGMR16].
A classical algorithm based on simultaneous diagonalization [Har70, LMV96] (often
attributed to R. Jennrich) can decompose the input tensor (1.1) when the components are
linearly independent, there is no error, and the order of the tensor is at least 3. Current
research on algorithms for tensor decomposition aims to improve over the guarantees of
this classical algorithm in two important ways:
Overcomplete tensors: What conditions allow us to decompose tensors when compo-
nents are linearly dependent?
Robust decomposition: What kind of errors can efficient decomposition algorithms tol-
erate? Can we tolerate errors E with “magnitude” comparable to the low-rank part∑n
i1 a
⊗k
i
?
The focus of this work is on robustness. There are two ways in which errors arise in
applications of tensor decomposition. The first is due to finite samples. For example, in
some applications T is the empirical k-th moment of some distribution and the error E
accounts for the difference between the empirical moment and actual moment (“popula-
tion moment”). Errors of this kind can be made smaller at the expense of requiring a
larger number of samples from the distribution. Therefore, robustness of decomposition
algorithms helps with reducing sample complexity.
Another way in which errors arise is from modeling errors (“systematic errors”). These
kinds of errors are more severe because they cannot be reduced by taking larger samples.
Two important applications of tensor decomposition with such errors are learning Noisy-or
Bayes networks [AGMR16] and sparsely-used dictionaries [BKS15]. For noisy-or networks,
the errors arise due to non-linearities in the model. For sparsely-used dictionaries, the
errors arise due to unknown correlations in the distribution of sparse feature represen-
tations. These examples show that robust tensor decomposition allows us to capture a
wider range of models.
1
Robustness guarantees for tensor decomposition algorithms have been studied exten-
sively (e.g., the work on tensor power iteration [AGH+14]). The polynomial-time algorithm
with the best known robustness guarantees for tensor decomposition [BKS15, MSS16] are
based on the sum-of-squares (SOS) method, a powerful meta-algorithm for polynomial
optimization problems based on semidefinite programming relaxations. Unfortunately,
these algorithms are far from practical and have polynomial running times with large
exponents. The goal of this work is to develop practical tensor decomposition algorithms
with robustness guarantees close to those of SOS-based algorithms.
For the sake of exposition, we consider the case that the components a1, . . . , an ∈ d
of the input tensor T are orthonormal. (Through standard reductions which we explain
later, most of our results also apply to components that are spectrally close to orthonormal
or at least linearly independent.) It turns out that the robustness guarantees that SOS
achieves for the case that T is a 4-tensor are significantly stronger than its guarantees for
3-tensors. These stronger guarantees are crucial for applications like dictionary learning.
(It also turns out that for 3-tensors, an analysis of Jennrich’s aforementioned algorithm
using matrix concentration inequalities gives robustness guarantees that are similar to
those of SOS [MSS16, AGMR16].)
In this work, we develop an easy-to-implement, randomized spectral algorithm to
decompose 4-tensors with orthonormal components even when the error tensor E has
small but constant spectral norm as a d2-by-d2 matrix. This robustness guarantee is
qualitatively optimal with respect to this norm in the sense that an error tensor E with
constant spectral norm (as a d2-by-d2 matrix) could change each component by a constant
proportion of its norm. To the best of our knowledge, the only previous algorithms
with this kind of robustness guarantee are based on SOS.1 Our algorithm runs in time
d2+ω 6 d4.373 using fast matrix multiplication. Even without fast matrix multiplication, our
running time of d5 is close to linear in the size of the input d4 and significantly faster than
the running time of SOS. As we will discuss later, an extension of this algorithm allows us
to solve instances of dictionary learning that previously could provably be solved only by
SOS.
A related previous work [HSSS16] also studied the question how to achieve similar
guarantees for tensor decomposition as SOS using just spectral algorithms. Our algorithms
follow the same general strategy as the algorithms in this prior work: In an algorithm
based on the SOS semidefinite program, one solves a convex programming relaxation to
obtain a “proxy” for a true, integral solution (such as a component of the tensor). Because
the program is a relaxation, one must then process or “round” the relaxation or proxy
into a true solution. In our algorithms, as in [HSSS16], instead of finding solutions to
SOS semidefinite programs, the algorithms find “proxy objects” that behave in similar
ways with respect to the rounding procedures used by SOS-based algorithms. Since
these rounding procedures tend to be quite simple, there is hope that generating proxy
1We remark that the aforementioned analysis [MSS16] of Jennrich’s algorithm can tolerate errors E if its
spectral norm as a non-square d3-by-d matrix is constant. However, this norm of E can be larger by a
√
d
factor than its spectral norm as a square matrix.
2
objects that “fool” these procedures is computationally more efficient than solving general
semidefinite programs.
However, our algorithmic techniques for finding these “proxy objects” differ signifi-
cantly from those in prior work. The reason is that many of the techniques in [HSSS16],
e.g., concentration inequalities for matrix-valued polynomials, are tailored to average-case
problems and therefore do not apply in our setting because we do not make distributional
assumption about the errors E.
The basic version of our algorithm is specified by a sequence of convex setsX1, . . . ,Xr ⊆
(d)⊗4 of 4-tensors and proceeds as follows:
Given a 4-tensor T ∈ (d)⊗4, compute iterative projections T(1) , . . . , T(r) to the
convex sets X1 , . . . ,Xr (with respect to euclidean norm) and apply Jennrich’s
algorithm on T(r).
It turns out that the SOS-based algorithm correspond to the case that r  1 and X1  XSOS
is the feasible region of a large semidefinite program. For our fast algorithm, X1, . . . ,Xr
are simpler sets defined in terms of singular values or eigenvalues of matrix reshapings of
tensors. Therefore, projections boil down to fast eigenvector computations. We choose the
sets X1, . . . ,Xr such that they contain XSOS and show that the iterative projection behaves
in a similar way as the projection to XSOS. In this sense our algorithm is similar in spirit
to iterated projective methods like the Bregman method (e.g., [GO09]).
Dictionary learning. In this basic unsupervised learning problem, the goal is to learn
an unknown matrix A ∈ d×n from i.i.d. samples y(1)  Ax(1), . . . , y(m)  Ax(m), where
x(1), . . . , x(m) are i.i.d. samples from a distribution {x} over sparse vectors in n. (Here,
the algorithm has access only to the vectors y(1), . . . , y(m) but not to x(1), . . . , x(m).)
Dictionary learning, also known as sparse coding, is studied extensively in neuro-
science [OF97], machine learning [EP07, MRBL07], and computer vision [EA06, MLB+08,
YWHM08]. Most algorithms for this problem used in practice do not come with strong
provable guarantees. In recent years, several algorithms with provable guarantees have
been developed for this problem [AAJ+14, ABGM14, AGMM15, BKS15, MSS16, HM16].
For the case that the coordinates of the distribution {x} are independent (and non-
Gaussian) there is a well-known reduction2 of this problem to tensor decomposition
where the components are the columns of A and the error E can be made inverse polyno-
mially small by taking sufficiently many samples. (In the case of independent coordinates,
dictionary learning becomes a special case of independent component analysis / blind
source separation, where this reduction originated.) Variants of Jennrich’s spectral tensor
decomposition algorithm (e.g., [BCMV14, MSS16]) imply strong provable guarantees for
dictionary learning in the case that {x} has independent coordinates even in the “over-
complete” regime when n ≫ d (using a polynomial number of samples).
More challenging is the case that {x} has non-independent coordinates, especially if
those correlations are unknown. We consider a model proposed in [BKS15] (similar to a
2The reduction only requires k-wise independence where k is the order to tensor the reduction produces.
3
model in [ABGM14]): We say that the distribution {x} is τ-nice if
1.  x4
i
 1 for all i ∈ [n],
2.  x2
i
x2
j
6 τ for all i , j ∈ [n],
3.  xix jxk xℓ  0 unless xi x jxkxℓ is a square.
The conditions allow for significant correlations in the support set of the vector x. For
example, we can obtain a τ-nice distribution {x} by starting from any distribution over
subsets S ⊆ [n] such that {i ∈ S}  p for all i ∈ [n] and { j ∈ S | i ∈ S} 6 τ for all i , j
and choosing x of the form xi  p
−1/4 · σi if i ∈ S and xi  0 if i < S, where σ1, . . . , σn are
independent random signs.
An extension of our aforementioned algorithm for orthogonal tensor decomposition
with spectral norm error allows us to learn orthonormal dictionaries from τ-nice distribu-
tions. To the best of our knowledge, the only previous algorithms to provably solve this
problem use sum-of-squares relaxations [BKS15, MSS16], which have large polynomial
running time. Our algorithm recovers a 0.99 fraction of the columns of A up to error τ from
Õ(n3) samples, and runs in time n3+O(τ)d4. By a standard reduction, our algorithm also
works for non-singular dictionaries and the running time increases by a factor polynomial
in the condition number of A.
1.1 Results
Tensor decomposition. For tensor decomposition, we give an algorithm with close to
linear running time that recovers the rank-1 components of a tensor with orthonormal
components, so long as the spectral norm of the square unfoldings of the error tensor is
small.
Theorem 1.1 (Tensor decomposition with spectral norm error). There exists a randomized
spectral algorithm with the following guarantees: Given a tensor T ∈ (d)⊗4 of the form T ∑n
i1 a
⊗4
i
+ E such that a1, . . . , an ∈ d are orthonormal and E has spectral norm at most ε as
a d2-by-d2 matrix, the algorithm can recover one of the components with ℓ2-error O(ε) in time
Õ(d2+ω+O(ε)) with high probability, and recover 0.99n of the components with ℓ2-error O(ε) in
time Õ(d2+ω+O(ε)) with high probability.
Furthermore, if ε 6 O(log log n/log3 n), the algorithm can recover all components up to error
O(ε) in time Õ(d2+ω + nd4) with high probability. Here, ω 6 2.373 is the matrix multiplication
exponent.
The orthogonality condition may at first seem restrictive, but for most applications
it is possible to take a tensor with linearly independent components and transform it to
a tensor with orthogonal components, as we will do for our dictionary learning result
below. Furthermore, our algorithm works as-is if the components are sufficiently close to
orthonormal:
Corollary 1.2. If we have that a1, . . . , an are only approximately orthonormal in the sense that the
ai are independent and
∑
i aia
⊤
i
− IdS
 6 η, where IdS is the identity in the subspace spanned by
the ai , then we can recover bi so that 〈ai , bi〉2 > 1 − O(
√
η) with the same algorithm and runtime
guarantees.
4
These robustness guarantees are comparable to those of the sum-of-squares-based
algorithms in [BKS15, MSS16] for the undercomplete case, which are the best known.
Meanwhile, the sum-of-squares based algorithms require solving large semidefinite pro-
grams, while the running time of our algorithms is close to linear in the size of the input,
and our algorithms are composed of simple matrix-vector multiplications.
On the other hand, our algorithms fail to work in the overcomplete case, when the
rank grows above n, and the components are no longer linearly independent. One inter-
esting open question is whether the techniques used in this paper can be extended to the
overcomplete case.
Dictionary learning. Using our tensor decomposition algorithm as a primitive, we give
an algorithm for dictionary learning when the sample distribution is τ-nice.
Theorem 1.3 (Dictionary learning). Suppose that A ∈ d×n is a dictionary with orthonormal
columns, and that we are given random independent samples of the form y  Ax for x ∼ D.
Suppose furthermore that D is τ-nice, as defined above, for τ < c∗ for some universal constant c∗.
Then there is a randomized spectral algorithm that recovers orthonormal vectors b1, . . . , bk ∈ d
for k > 0.99n with 〈bi , ai〉2 > (1− O(τ)), and with high probability requires m  Õ(n3) samples
and time Õ(d2+ω + n1+O(τ)d4 + md4).
The total runtime is thus Õ(n3d4)—in the theorem statement, we write it in terms of
the number of samples m in order to separate the time spent processing the samples from
the learning phase. We note that the sample complexity bound that we have, m  Õ(n3),
may very well be sub-optimal; we suspect that m  Õ(n2) is closer to the truth, which
would yield a better runtime.
We are also able to apply standard whitening operations (as in e.g. [AGHK13]) to
extend our algorithm to dictionaries with linearly-independent, but non-orthonormal,
columns, at the cost of polynomially many additional samples.
Corollary 1.4. If A ∈ d×n is a dictionary with linearly independent columns, then there is a ran-
domized spectral algorithm that recovers the columns of A with guarantees similar to Theorem 1.3
given Õ(n2 · f (µ)) additional samples, where µ  λmax(AA⊤)/λmin(AA⊤) is the condition
number of the covariance matrix, and f is a polynomial function.
To our knowledge, our algorithms are the only remotely efficient dictionary learn-
ing algorithms with provable guarantees that permit τ-nice distributions in which the
coordinates of x may be correlated by constant factors, the only other ones being the
sum-of-squares semidefinite programming based algorithms of [BKS15, MSS16].
2 Preliminaries
Throughout the rest of this paper, we will denote tensors by boldface letters such as T,
matrices by capital letters M, and vectors by lowercase letters v, when the distinction is
helpful. We will use A⊗k/u⊗k to denote the kth Kronecker power of a matrix/vector with
5
itself. To enhance legibility, for u ∈ d we will at times abuse notation and use u⊗4 to
denote the order-4 tensor u ⊗ u ⊗ u ⊗ u, the d2 × d2 matrix (u⊗2)(u⊗2)⊤, and the dimension
d4 vector u⊗4—we hope the meaning will be clear from context.
For a tensor T ∈ (d)⊗4 and a partition of the modes {1, 2, 3, 4} into two ordered sets
A and B, we let TA,B denote the reshaping of T as d
|A|-by-d |B | matrix, where the modes
in A are used to index rows and the modes in B are used to index columns. For example,
T{1,2},{3,4} is a d2-by-d2 matrix such that the entry at row (i , j) and column (k , ℓ) contains
the entry Ti, j,k ,ℓ of T. We remark that the order used to specify the modes matters—for
example, for the rank-1 tensor T  a ⊗ b ⊗ a ⊗ b, we have that T{1,2}{3,4}  (a ⊗ b)(a ⊗ b)⊤ is
a symmetric matrix, while T{2,1}{3,4}  (b ⊗ a)(a ⊗ b)⊤ is not. We use ‖TA,B ‖ to denote the
spectral norm (largest singular value) of the matrix TA,B .
We will also make frequent use of the following lemma, which states that the distance
between two points cannot increase when both are projected onto a closed, convex set.
Lemma. Let C ⊂ n be a closed convex set, and let Π : n → C be the projection operator onto
C in terms of norm ‖ · ‖2, i.e. Π(x)
def
 argminc∈C ‖x − c‖2. Then for any x , y ∈ n,
‖x − y‖2 > ‖Π(x) −Π(y)‖2.
This lemma is well-known (see e.g. [Roc76]), but we will prove it for completeness in
Appendix A.
3 Techniques
In this section we give a high-level overview of the algorithms in our paper, and of their
analyses. We begin with the tensor decomposition algorithm, after which we’ll explain
the (non-trivial) extension to the dictionary learning application. At the very end, we will
discuss the relationship between our algorithms and sum-of-squares relaxations.
Suppose have a tensor T ∈ (d)⊗4, and that T  S + E where the signal S is a low-rank
tensor with orthonormal components, S 
∑
i∈[n] a
⊗4
i
, and the noise E is an arbitrary tensor
of noise with the restriction that for any reshaping of E into a square matrix E, ‖E‖ 6 ε.
Our goal is to (approximately) recover the rank-1 components, a1, . . . , an ∈ d, up to signs.
Failure of Jennrich’s algorithm. To motivate the algorithm and analysis, it first makes
sense to consider the case when the noise component E  0. In this case, we can run
Jennrich’s algorithm: if we choose a d2-dimensional random vector 1 ∼ N(0, Id), we can
compute the contraction
M1
def

n∑
i, j1
1i jTi j 
n∑
i1
〈1 , a⊗2
i
〉 · aia⊤i ,
where Ti j is the i , jth d × d matrix slice of the tensor T. Since the ai are orthogonal, the
coefficients 〈1 , a⊗2
i
〉 are independent, and so we find ourselves in an ideal situation—M1
6
is a sum of the orthogonal components we want to recover with independent Gaussian
coefficients. A simple eigendecomposition will recover all of the ai .
On the other hand, when we have a nonzero noise tensor E, a random contraction
along modes {1, 2} results in the matrix
M1 
n∑
i1
〈1 , a⊗2i 〉aia
⊤
i +
n∑
i, j1
1i j · Ei j ,
where the Ei j are d × d slices of the tensor E. The last term, composed of the error,
complicates things. Standard facts about Gaussian matrix series assert that the spectral
norm of the error term behaves like ‖E{1,2,3}{4}‖, the spectral norm of a d3 × d reshaping
of E, whereas we only have control over square reshapings such as ‖E{1,2}{3,4}‖.3 These
can be off by polynomial factors. If the Frobenius norm of E is ‖E‖2
F
≈ ε2d2, which is
the magnitude one would expect from a tensor whose square reshapings are full-rank
matrices with spectral norm ε, then we have that necessarily
‖E{1,2,3}{4}‖2 >
‖E‖2
F
rank
(
E{1,2,3}{4}
) > ε2d ,
since there are at most d nonzero singular values of rectangular reshapings of E. In
this case, unless ε ≪ 1/
√
d, the components aia
⊤
i
are completely drowned out by the
contribution of the noise, and so the robustness guarantees leave something to be desired.
Basic idea. The above suggests that, as long as we allow the error E to have large
Frobenius norm, an approach based on random contraction will not succeed. Our basic
idea is to take T, whose error has small spectral norm, and transform it into a tensor T′
whose error has small Frobenius norm.
Because we do not know the decomposition of T, we cannot access the error E directly.
However, we do know that for any d2 × d2 reshaping T of T,
T  S + E,
where S 
∑n
i1 a
⊗4
i
, and ‖E‖ 6 ε. The rank of S is n 6 d, and all eigenvalues of S are 1.
Thus, if we perform the operation
T>ε  (T − ε Id)+,
where (·)+ denotes projection to the cone of positive semidefinite matrices, we expect that
the signal term S will survive, while the noise term E will be dampened. More formally,
we know that T has n eigenvalues of magnitude 1±ε, and d2−n eigenvalues of magnitude
at most ε, and therefore rank(T>ε) 6 n. Also by definition, ‖T − T>ε‖ 6 ε. Therefore, we
have that
S + E  T  T>ε + E′
3In fact, this observation was crucial in the analysis of [MSS16]—in that work, semidefinite programming
constraints are used to control the spectral norm of the rectangular reshapings.
7
with ‖E′‖ 6 ε, and thus
‖T>ε − S‖  ‖E − E′‖.
Since S, T>ε are both of rank at most n, and E′, E have spectral norm bounded by ε, we
have that
‖T>ε − S‖2F 6
(
rank(S) + rank(T>ε)
)
· (‖E‖ + ‖E′‖)2 6 2n · 4ε2.
So, the Frobenius norm is no longer an impassable obstacle to the random contraction
approach—using our upper bound on the Frobenius norm of our new error Ẽ
def
 T>ε − S,
we have that the average squared singular value of Ẽ{1,2,3}{4} will be
σ2av1(Ẽ{1,2,3}{4}) 
‖Ẽ‖2
F
d
 O
(n
d
ε2
)
.
So while Ẽ{1,2,3}{4} may have large singular values, by Markov’s inequality it cannot have
too many singular values larger than O(ε).
Finally, to eliminate these large singular values, we will project T>ε into the set of
matrices whose rectangular reshapings have singular values at most 1—because S is a
member of this convex set, the projection can only decrease the Frobenius norm. After
this, we will apply the random contraction algorithm, as originally suggested.
Variance of Gaussian matrix series. Recall that we wanted to sample a random d2-
dimensional Gaussian vector 1, and the perform the contraction
M1
def

d∑
i, j1
1i jT
>ε
i j 
d∑
i1
〈1 , a⊗2
i
〉 · aia⊤i +
∑
i j
1i j · Ẽi j .
The error term on the right is a matrix Gaussian series. The following lemma describes
the behavior of the spectra of matrix Gaussian series:
Lemma (See e.g. [Tro12]). Let 1 ∼ N(0, Id), and let A1, . . . ,Ak be n × m real matrices. Define
σ2  max
{∑
i AiA
⊤
i
 , ∑i A⊤i Ai}. Then

(
k∑
i1
1iAi
 > t
)
6 (n + m) · exp
(
− t
2
2σ2
)
.
For us, this means that we must have bounds on the spectral norm of both
∑
i j Ẽi j Ẽ
⊤
i j
and
∑
i j Ẽ
⊤
i j
Ẽi j . This means that if we have performed the contraction along modes 1 and
2, so that the index i comes from mode 1 and the index j comes from mode 2, then it is
not hard to verify that ‖∑i j Ẽi j Ẽ⊤i j ‖  ‖Ẽ{1,2,3}{4}‖2, and ‖∑i j Ẽ⊤i j Ẽi j ‖  ‖Ẽ{1,2,4}{3}‖2. So,
we must control the maximum singular values of two different rectangular reshapings of
Ẽ simultaneously.
It turns out that for us it suffices to perform two projections in sequence—we first
reshape T>ε to the matrix T>ε{123}{4},project it to the set of matrices with singular values
8
at most 1, and then reshape the result along modes {124}{3}, and project to the same
set again. As mentioned before, because projection to a convex set containing S cannot
increase the distance from S, the Frobenius norm of the new error can only decrease. What
is less obvious is that performing the second projection will not destroy the property that
the reshaping along modes {123}{4} has spectral norm at most 1. By showing that each
projection corresponds to either left- or right- multiplication of T>ε{123}{4} and T
>ε
{124}{3} by
matrices of spectral norm at most 1, we are able to show that the second projection does
not create large singular values for the first flattening, and so two projections are indeed
enough. Call the resulting tensor (T>ε)61.
Now, if we perform a random contraction in the modes 1, 2, we will have
M1 
∑
i j
1i j(T>ε)61i j 
∑
i
〈a⊗2i , 1〉aia
⊤
i + Ê1 ,
where the spectral norm of ‖Ê1 ‖ 6
√
log n with good probability. So, ignoring for the mo-
ment dependencies between 〈a⊗2
i
, 1〉 and E1 , maxi |〈1 , a⊗2i 〉| > 1.1 · ‖E1 ‖ with probability
at least n−.2, which will give ai correlation 0.9 with the top eigenvector of M1 with good
probability.
Improving accuracy of components. The algorithm described thus far will recover com-
ponents bi that are 0.9-correlated with the ai , in the sense that 〈bi , ai〉2 > 0.9. To boost the
accuracy of the recovered components, we use a simple method which resembles a single
step of tensor power iteration.
We’ll use the closeness of our original tensor T to
∑
i a
⊗4
i
in spectral norm. We let T be
a d2 × d2 flattening of T, and compute the vector
v  T(bi ⊗ bi)  0.9 · ai ⊗ ai +
∑
j,i
〈bi , a j〉2a j ⊗ a j + E(bi ⊗ bi).
Now, when the vector v is reshaped to a d × d matrix V , the term E(bi ⊗ bi) is a matrix
of Frobenius norm (and thus spectral norm) at most ε. By the orthonormality of the a j ,
the sum of the coefficients in the second term is at most 0.1, and so ai is ε-close to the top
eigenvector of V .
Recovering every component. Because the Frobenius norm of the error in (T>ε)61 is
O(ε
√
n), there may be a small fraction of the components a⊗4
i
that are “canceled out” by
the error—for instance, we can imagine that the error term is Ê  −∑ε2ni1 a⊗4i . So while only
a constant fraction of the components a⊗4
i
can be more than ε-correlated with the error,
we may still be unable to recover some fixed ε2-fraction of the ai via random contractions.
To recover all components, we must subtract the components that we have found
already and run the algorithm iteratively—if we have found m  0.99n components, then
if we could perfectly subtract them from T, we would end up with an even lower-rank
signal tensor, and thus be able to make progress by truncating all but 0.1n eigenvalues in
the first step.
9
The challenge is that we have recovered b1, . . . , bm that are only (1− ε)-correlated with
the ai , and so naively subtracting T−
∑m
i1 b
⊗4
i
can result in a Frobenius and spectral norm
error of magnitude ε
√
m—thus the total error is still proportional to
√
n rather than
√
0.1n.
In order to apply our algorithm recursively, we first orthogonalize the components we
have found b1, . . . , bm to obtain new components b̃1, . . . , b̃m. Because the bi are close to the
truly orthonormal ai , the orthogonalization step cannot push too many of the b̃i more than
O(ε)-far from the bi—in fact, letting B be the matrix whose columns are the bi , and letting
A be the matrix whose columns are the corresponding ai , we use that ‖A−B‖F 6 O(ε)
√
m,
and that the matrix B̃ with columns b̃i is closer to B than A. We keep only the b̃i for which
〈b̃⊗4
i
,T〉 > 1− O(ε), and we argue that there must be at least 0.9m such b̃i . Let K ⊂ [m] be
the set of indices for which this occurred.
Now, given that the two sets of orthogonal vectors {b̃i}i∈K and {ai}i∈K are all O(ε) close,
we are able to prove that 
∑
i∈K
b̃⊗4i − a
⊗4
i
 6 O(√ε).
So subtracting the b̃⊗4
i
will not introduce a large spectral norm! Since we will only need
to perform this recursion O(log n) times, allowing for some leeway in ε (by requiring√
ε log n  o(1)), we are able to recover all of the components.
Dictionary learning. In the dictionary learning problem, there is an unknown dictionary,
A ∈ d×n, and we receive independent samples of the form y  Ax for x ∼ D for some
distribution D over n. The goal is, given access only to the samples y, recover A.
We can use our tensor decomposition algorithm to learn the dictionary A, as long as
the columns of A are linearly independent. For the sake of this overview, assume instead
that the columns of A are orthonormal. Then given samples y(1), . . . , y(m) for m  poly(n),
we can compute the 4th moment tensor to accuracy ε in the spectral norm,
1
m
m∑
j1
(y( j))⊗4 ≈ 
x∼D
[
(Ax)⊗4
]
.
If the right-hand side were close to
∑
i a
⊗4
i
in spectral norm, we would be done. However,
for almost any distribution D which is supported on x with more than one nonzero
coordinate, this is not the case. If we assume that [xix jxk xℓ]  0 unless xix jxk xℓ is a
square, then we can calculate that any square reshaping of this tensor will have the form

x∼D
[
(Ax)⊗4
]

∑
i
[x4i ] · a⊗4i +
∑
i, j
[x2i x2j ] · (ai a⊤j ⊗ ai a⊤j + aia⊤i ⊗ a j a⊤j + aia⊤j ⊗ a j a⊤i ) .
If [x4
i
]  [x4
j
] for all i , j, then the first term on the right is exactly the 4th order tensor
that we want. The second term on the right can be further split into three distinct matrices,
one for each configuration of the ai , a j . The ai a
⊤
i
⊗ a ja⊤j term and the aia⊤j ⊗ a j a⊤i terms
can be shown to have spectral norm at most maxi, j [x2i x2j ], and so as long as we require
10
that maxi, j [x2i x
2
j
] ≪ ε[x4
k
], these terms have spectral norm within the allowance of
our tensor decomposition algorithm.
The issue is with the ai a
⊤
j
⊗ ai a⊤j term. This term factors into (ai ⊗ ai)(a j ⊗ a j)⊤, and
because of this the entire sum
∑
i, j [x2i x
2
j
] · ai a⊤j ⊗ aia⊤j has rank at most n ≪ d2, but
Frobenius norm as large as maxi, j [x2i x
2
j
] · n. If we require that the coordinates of x ∼ D
are independent, then we can see that this is actually close to a spurious rank-1 component,
which can be easily removed without altering the signal term too much.4 However, if we
wish to let the coordinates of x exhibit correlations, we have very little information about
the spectrum of this term.
In the sum-of-squares relaxation, this issue is overcome easily: because of the symme-
tries required of the SDP solution matrix X, 〈X, aia⊤j ⊗ ai a⊤j 〉  〈X, aia⊤i ⊗ a ja⊤j 〉, so by
linearity this low-rank error term cannot influence the objective function any more than
the aia
⊤
i
⊗ a j a⊤j term.
Inspired by this sum-of-squares analysis, we remove these unwanted directions as
follows. Given the scaled moment matrix M  1
[x4
1
] x∼D
[
(Ax)⊗4
]
(where the scaling
serves to make the coefficients of the signal 1), we truncate the small eigenvalues:
M>ε  (M{1,2}{3,4} − ε Id)+.
This removes the spectrum in the direction of the “nice” error terms, corresponding to
ai a
⊤
i
⊗ a j a⊤j and ai a⊤j ⊗ a ja⊤i . The fact that the rest of the matrix is low-rank means that we
can apply an analysis similar to the analysis in the first step of our tensor decomposition
to argue that ‖M>ε −∑i a⊗4i ‖F 6 ‖M −∑i a⊗4i ‖F.
Now, we re-shape M>ε, so that if initially we had the flattening M → M{1,2}{3,4}, we
look at the flattening M>ε{1,3}{2,4}. In this flattening, the term aia
⊤
j
⊗ ai a⊤j from M>ε is
transformed to aia
⊤
i
⊗ a ja⊤j , and so the problematic error term from the original flattening
has spectral norm ε in this flattening! Applying the projection
(M>ε{1,3}{2,4} − ε Id)+
eliminates the problematic term, and brings us again closer to the target matrix
∑
i a
⊗4
i
.
Thus, we end up with a tensor that is close to
∑
i a
⊗4
i
in Frobenius norm, and we can apply
our tensor decomposition algorithm.
Connection to sum-of-squares algorithms. We take a moment to draw parallels between
our algorithm and tensor decomposition algorithms in the sum-of-squares hierarchy. In
noisy orthogonal tensor decomposition, we want to solve the non-convex program
argmax〈X,T〉 s.t.
{
X ∈ d2×d2 , X  0, ‖X‖  1, rank(X)  n , X ∈ Span{u⊗4 : u ∈ d}
}
.
The intended solution of this program is X  S (after which we can run Jennrich’s
algorithm to recover individual components). At first it may not be obvious that the
4As was done in [HSSS15], for example, albeit in a slightly different context.
11
maximizer of the above program is close to S, but for any unit vector x ∈ d,
〈x⊗4,T〉 
n∑
i1
〈x , ai〉4 + (x ⊗ x)⊤E(x ⊗ x).
The error term is at most ε by our bound on ‖E‖, and the first term is ‖x⊤A‖4
4
, where A is the
matrix whose columns are the ai . Since by the orthonormality of the ai , ‖x⊤A‖2 6 1, and
the ℓ4 norm is maximized relative to the ℓ2 for vectors supported on a single coordinate,
the x that maximize this must be ε-close to one of the ai . In conjunction with the ‖X‖ 6 1
constraint and the rank(X)  n constraint, we have that X : S is the maximizer.
The (somewhat simplified) corresponding sum-of-squares relaxation is the semidefi-
nite program
max〈X,T〉 s.t.
{
X ∈ d2×d2 , X  0, ‖X‖ 6 1, ‖X‖2F  n , Xi jkℓ  Xπ(i jkℓ) ∀π ∈ S4
}
, 5
The constraints ‖X‖2
F
 n and Xi jkℓ  Xπ(i jkℓ) together are a relaxation of the constraint
that X be a rank-n matrix in the symmetric subspace Span{u⊗4}. Further, the constraint
‖X‖ 6 1 is enforced in every rectangular d × d3 reshaping of X (this consequence of the
SOS constraints is crucially used in [MSS16]).
To solve this semidefinite program, one should project T into the intersection of all of
the convex feasible regions of the constraints. However, projecting to the intersection is an
expensive operation in terms of runtime. Instead, we choose a subset of these constraints,
and project T into the set of points satisfying each constraint sequentially, rather than
simultaneously. These are not equivalent projection operations, but because we select our
operations carefully, we are able to show that our T is close to the SDP optimum in a sense
that is sufficient for successfully running Jennrich’s algorithm.
In the first step of our algorithm, we change the objective function from 〈X,T〉 to 〈X,T−
ε Id〉. In the sum-of-squares SDP, this does not change the objective value dramatically—
because the original objective value is at least n, and because the Frobenius norm constraint
‖X‖2F  n constraint in conjunction with the sum-of-squares constraints implies that
〈X, Id〉  εn. Therefore this perturbation cannot decrease the objective by more than a
multiplicative factor of ε. Then, we project a square reshaping of the objective to the PSD
cone, (T − ε Id)+—this corresponds to the constraint that X  0.6 Finally, we project first
to the set of matrices that have spectral norm at most 1 for one rectangular reshaping, then
repeat for another rectangular reshaping. So after perturbing the objective very slightly,
then choosing three of the convex constraints to project to in sequence, we end up with
an object that approximates the maximizer of the SDP in a sense that is sufficient for our
purposes.
5 The program is actually over matrices indexed by all subsets of d of size at most 2,
( d
62
)
, but for simplicity
in this description we ignore this (and the interaction with the SOS variables corresponding to lower-degree
monomials or lower-order moments).
6For a proof that truncating the negative eigenvalues of a matrix is equivalent to projection to the PSD
cone in Frobenius norm, see Fact A.4.
12
Our dictionary learning pre-processing can be interpreted similarly. We first perturb
the objective function by ε Id, and project to the PSD cone. Then, in reshaping the tensor
again, we choose another point that has the same projection onto any point in the feasible
region (by moving along an equivalence class in the symmetry constraint). Finally, we
perturb the objective by ε Id again, and again project to the PSD cone.
4 Decomposing orthogonal 4-tensors
Recall our setting: we are given a 4-tensor T ∈ (d)⊗4 of the form T  S + E where E is a
noise tensor and S 
∑n
i1 a
⊗4
i
for orthonormal vectors a1, . . . , an. (We address the more
general case of nearly orthonormal vectors in Section 4.2.).
First, we have a pre-processing step, in which we go from a tensor with low spectral
norm error to a tensor with low Frobenius norm error.7
Algorithm 4.1 (Preprocessing: spectral-to-Frobenius norm). .
Input: A tensor T ∈ (d)⊗4, and an error parameter ε.
1. Reshape T to the d2 × d2 matrix T def T{1,2}{3,4}.
2. Truncate to 0 all eigenvalues of that have magnitude less than ε:
T>ε
def
 (T − ε Id)+,
where we have used (M)+ to denote projection to the PSD cone.
Output: The tensor T>ε.
Lemma 4.2. Suppose that for some square reshaping E of E (without loss of generality along
modes {1, 2}, {3, 4}), ‖E‖ 6 ε. Say we are given access to T  S + E, and we produce the matrix
T>ε  (T − ε · Id)+ as described in Algorithm 4.1. Then ‖T′ − S‖F 6 2ε
√
2n. This operation
requires time Õ(min{nd4 , d2+ω}).
Proof. Because ‖E‖ 6 ε, T  S + E has only n eigenvalues of magnitude more than ε. So
rank(T>ε) 6 n, and therefore rank(T>ε − S) 6 2n. Furthermore, S + E  T  T>ε + Ẽ for a
matrix Ẽ of spectral norm at most ε. Therefore ‖T>ε − S‖ 6 2ε, and ‖T>ε − S‖F 6 2ε
√
2n.
To compute T>ε, we can compute the top n eigenvectors of T. Since O(ε) · λn >
λn+1, where λn , λn+1 are the nth and (n + 1)st eigenvalues, we can compute this in time
Õ(min{nd4 , d2+ω}) via subspace power iteration (see for example [HP14]). 
Now, we can run our main algorithm:
Algorithm 4.3. .
Input: A tensor T ∈ (d)⊗4.
1. Project T to the set of tensors whose rectangular reshapings along modes {1, 2, 3}, {4}
have spectral norm at most 1 (obtaining a new tensor T̂ with ‖T̂{1,2,3}{4}‖ 6 1).
7We are approximating T  S + E with rank(S)  n ≪ d2, and ‖E‖ 6 ε, so for example if E  ε Id then
we may have ‖T − S‖2
F
 ε2d2, which is too large for us.
13
2. Project T̂ to the set of tensors whose rectangular reshapings along modes {1, 2, 4}, {3}
have spectral norm at most 1, obtaining a new tensor T61  S + E61.
3. Sample 1 ∼ N(0, Idd2), and compute the random flattening M1
def

∑d2
j1 1 jT
61
j
.
Output: uL and uR, the top left- and right- unit singular vectors of M1 .
Under the appropriate conditions on T, with probability Õ(n−ε), Algorithm 4.3 will
output a vector that is 0.9-correlated with a⊗2
j
for some j ∈ [n].
Theorem 4.4. Suppose we are given a 4-tensor T ∈ (d)⊗4, and T  ∑i a⊗4i + E, where
a1, . . . , an ∈ d are orthonormal vectors and ‖E‖F 6 η
√
n.
Then running Algorithm 4.3 Õ(n1+O(η)) times allows us to recover m > 0.99n unit vectors
u1, . . . , um ∈ d such that for each i ∈ [m], there exists j ∈ [n] such that
〈ui , a j〉2 > 0.99.
Recovering one component requires time Õ(d2+ωnO(η)), and recovering m components requires
time Õ(max{mnO(η)d4, d2+ω}).
We can then post-process the vectors to obtain a vector that has correlation 1 − ε with
a j—the details are given in Section 4.3 below.
We will prove Theorem 4.4 momentarily, but first, we bring the reader’s attention to
a nontrivial technical issue left unanswered by Theorem 4.4. The issue is that we can
only guarantee that Algorithm 4.3 recovers 0.99n of the vectors, and the set of recoverable
vectors is invariant under the randomness of the algorithm. That is, as a side effect of
the error-reducing step 2, the S part of T may also be adversely affected. For that reason,
in Õ(n) runs of Algorithm 4.3, we can only guarantee that recover a constant fraction of
the components, and we must iteratively remove the components we recover in order to
continue to make progress. This removal must be handled delicately to ensure that the
Frobenius norm of the error shrinks at each step, so the conditions of Theorem 4.4 continue
to be met (for shrinking values of n). The overall algorithm, which uses Algorithm 4.3 as
a subroutine, will be given in Section 4.3.
We will now prove the correctness of Algorithm 4.3 step-by-step, tying details together
at the end of this subsection. First, we argue that in step 1, the truncation of the large
eigenvalues cannot increase the Frobenius norm of the error.
Lemma 4.5. Suppose that we define T61 to be the result of projecting T  S + E to the set of
tensors whose rectangular reshapings along modes {1, 2, 3}, {4} have spectral norm at most 1, then
projecting the result to the set of tensors whose rectangular reshapings along modes {1, 2, 3}, {4}
have spectral norm at most 1. Then ‖T61 − S‖F 6 ‖E‖F, and
‖T61{1,2,3}{4}‖ 6 1, and ‖T
61
{1,2,4}{3}‖ 6 1 .
This operation requires time Õ(d2+ω).
14
Proof. To establish the first claim, we note that the tensor T61 was obtained by two pro-
jections of different rectangular reshapings of the matrix S + E to the set of rectangular
matrices with singular value at most 1. This set is closed, convex, and contains S, and
so the error can only decrease in Frobenius norm (see Lemma A.2 in Appendix A for a
proof),
‖T61 − S‖F 6 ‖T − S‖F  ‖E‖F .
It is not hard to see that each projection step can be accomplished by reshaping the
tensor to the appropriate rectangular matrix, then truncating all singular values larger than
1 to 1. Now, we establish the remaining claims. For convenience, define T̂61 to be the matrix
(S+E){123}{4} after restricting singular values of magnitude > 1 to 1. Now, we reshape T̂61
to a new matrix B
def
 T̂61{1,2,4}{3}, which has the d
2 blocks B1  (T̂61)⊤1 , . . . , Bd2  (T̂61)⊤d2 .
Say that the singular value decomposition of B is B  UΣV⊤. Define Σ̂−1 to be the diagonal
matrix with entries equal to those of Σ−1 when the value is < 1 and with ones elsewhere.
When we truncate the large singular values of B this is equivalent to multiplying by
P  UΣ̂−1U⊤. The result is the matrix PB, with blocks PB1  P(T̂61)⊤1 , . . . , PBd2 
P(T̂61)⊤
d2
. By definition, the singular values of PB  T61{3}{1,2,4} are at most 1. Also,
T61{4}{123}  T̂
61(P ⊗ Idd2), and by the submultiplicativity of the norm, ‖T̂61(P ⊗ Idd2)‖ 6
‖T̂61‖ · ‖P ⊗ Idd2 ‖ 6 1, and the first reshaping still has spectral norm at most 1.
Finally, each reshaping step takes O(d4) time. Since we are only interested in truncating
large singular values, it suffices for us to compute the SVD corresponding to singular values
between
√
n and 1. This can be done via subspace power iteration, which here involves the
multiplication of a d3 × d matrix and a d × d matrix (with intermediate orthogonalization
steps for the d × d matrix, see [HP14]), which requires time Õ(d2+ω), where ω is the matrix
multiplication constant. Going forward the representation of the matrix will be as the
original matrix, with the subtracted SVD corresponding to large singular values. 
We will need to argue that if the Frobenius norm of the error matrix is small, this is
a sufficient condition under which we succeed. For this, we will use the following two
lemmas. The first tells us that with probability Ω̃(n−O(ε)) over the choice of 1, we will have
for some i ∈ [n] that
M1  c · ai a⊤i + N,
where |c | > ‖N ‖ and furthermore ‖Nai‖ and ‖N⊤ai ‖ are small:
Lemma 4.6. Let 1 ∼ N(0, Idd2). Suppose that ‖T{1,2,3}{4}‖ , ‖T{1,2,3}{4}‖ 6 1, and that ‖T −∑
i a
⊗4
i
‖F 6 ε
√
n. Define the matrix M1 to be the flattening of T 
∑
i a
⊗4
i
+ E along 1 in the
modes {1} and {2}, and let |c | be the magnitude of a j ’s projection onto M1 , i.e.
M1
def

d2∑
j1
〈e j , 1〉 · T j  c · a j a⊤j + N .
Then for a 1 − 3δ fraction of j ∈ [n],

1
[
|c | > (1 + β)‖N ‖ , ‖N⊤a j ‖ , ‖Na j‖ 6 (ε/δ)(c +
√
2 + o(1))
]
 Ω̃
(
n
−
(
1+β
1−(1+β)ε/δ
)2)
.
15
In particular, if δ  Ω(1), β  O(ε), β < 1, then this probability is Ω̃(n−(1+O(ε))).
The proof consists primarily of the application of concentration inequalities, and we
provide it below in Section 4.4.
The second lemma states that if M1 indeed has the form above, the top singular vectors
of M1 must be close to the component ai .
Lemma 4.7. Let M1 be an n × n matrix, and a1 ∈ n, and suppose that
M1  c · a1a⊤1 + N
with |c | > (1+β)‖N ‖ for β > 0, and ‖Na1‖ , ‖N⊤a1‖ 6 ε |c | so that the relationship 2ε(1+β)β < 0.01
holds. Then letting u be a top singular vector of M1 , it follows that
〈u , a1〉2 > 0.99.
The proof requires some careful calculations, but is not complicated, and we will prove
it below in Section 4.4.
Finally, we are ready to stitch these arguments together and prove that Algorithm 4.3
works.
Proof of Theorem 4.4. After reshaping and truncating T in step 1 of the algorithm, by
Lemma 4.5 the matrix T61 
∑
i a
⊗4
i
+ E has the properties that
‖T61{1,2,3}{4}‖ , ‖T
61
{1,2,4}{3}‖ 6 1,
and also that still ‖E‖F 6 η
√
n.
We can now apply Lemma 4.6 with δ  1300 and β  400η/δ  O(η) to conclude that
for at least a 0.99-fraction of the i ∈ [n], with probability at least Õ(n−1−O(η)), we will have
M1  c · ai a⊤i + N,
where ‖N ‖ 6 (1 + β)c and ‖Nai ‖ , ‖N⊤ai ‖ 6 48η · |c |. Applying Lemma 4.7, we have that
either the left- or right- top unit singular vector u of M1 has correlation at least
〈u , ai〉2 > 0.99,
as desired.
For runtime, by our arguments in Lemma 4.5 step 1 takes time Õ(d2+ω). After this, with
either representation of our matrix T61 (whether we compute the full truncated SVD or
have the original matrix minus the subtracted SVD), performing power iteration to find the
top eigenvector with the flattening M1 takes time Õ(d4), and finding a single component
takes Õ(n−O(η)) samples of random contractions. Since we can reuse T61 with new random
contractions, the total runtime for recovering one component is Õ(d2+ωn−O(η)), and by the
independence of the runs recovering m components requires Õ(d2+ω) + mn−O(η) · Õ(d4)
time. 
With the core of our algorithm in place, we now take care of the remaining technical
issues: recovery precision, working with near-orthonormal vectors, and recovering the
full set of component vectors.
16
4.1 Postprocessing for closer vectors
Because the precision of recovery will be important in not amplifying the error, we begin
with our precision-amplifying postprocessing algorithm.
Algorithm 4.8 (Postprocessing for error reduction). .
Input: A tensor T ∈ (d)⊗4, a vector u ∈ d2 , and an error parameter ε > ‖E{12}{34}‖.
1. Compute the matrix-vector product a
def
 T{1,2}{3,4}(u ⊗ u).
2. Reshape a ∈ d2 to a d × d matrix A, and compute the top left- and right- singular
vectors vL and vR of A.
Output: If for one of v ∈ {vL , vR}, (v⊗2)⊤Tv⊗2 > (1 − 3ε)2 − ε, output v.
Lemma 4.9. Suppose that v is a unit vector with 〈v , ai〉2 > 0.99, and T 
∑
i a
⊗4
i
+E for ‖E‖ 6 ε
and a1, . . . , an orthonormal. Then if we let A be the reshaping of T(v ⊗ v) to a d × d matrix, and
if we let uL , uR be the top left- and right- unit singular vectors of M, then
〈uL , ai〉2 > 1 − 3ε or 〈uR , ai〉2 > 1 − 3ε.
In other words, Algorithm 4.8 succeeds. Further, the time required is Õ(d4).
Proof of Lemma 4.9. For convenience and without loss of generality, let i : 1, and let
α
def
 1 − 〈a1, v〉2 6 0.01. Because a1, . . . , an are orthonormal, we can write v 
∑
j 〈a j , v〉 ·
a j + w, where w ⊥ a j for all j ∈ [n]. By assumption, 〈a1, v〉2 > 1 − α, and therefore∑
j>1〈v , a j〉2 + ‖w‖2 6 α. Now,
(T − E)(v ⊗ v) 
∑
j
a⊗2j (a
⊗2
j )
⊤ ©­«
∑
j
〈a j , v〉a j ⊗
∑
j
〈a j , v〉a jª®¬

∑
j,k ,ℓ
a⊗2
j
〈a j , ak〉〈a j , aℓ〉〈ak , v〉〈aℓ , v〉
by the orthonormality of the ai ,

∑
j
a⊗2
j
〈a j , v〉2 .
Therefore, defining M to be the n × n reshaping of T(v ⊗ v) and defining N to be the n × n
reshaping of E(v ⊗ v),
M  (1 − α)a1a⊤1 +
∑
j>1
〈a j , v〉2a ja⊤j + N,
where ‖N ‖F 6 ε, since ‖E‖ 6 ε.
Now, we have that
‖M‖ > 1 − α − ε,
and that if we choose η so that 1 − η > 2α > 1/50,
‖M − η · a1a⊤1 ‖ 6 1 − α − η + ε.
17
Thus, ‖M − ηa1a⊤1 ‖ 6 ‖M‖ − η+ 2ε, and we have by Fact A.3 (see Appendix A for a proof)
that the top unit eigenvector u of M is such that
〈u , a1〉2 >
η − 2ε
η
> 1 − 2ε
η
.
Choosing η  49/50, the result follows. 
4.2 Near-orthonormal components
We’ll now dispense with the discrepancy between the orthonormal and near-orthonormal
cases.
Fact 4.10. If S 
∑
i aia
⊤
i
 Id+E for ‖E‖ 6 ε, then ã1  S−1/2a1, . . . , ãn  S−1/2an are
orthonormal, 〈ãi , ai〉2 > (1 − ε)‖ai ‖2, and
∑
i
ã⊗4i − a
⊗4
i
 6 4√ε.
Proof. The fact that the ãi are orthonormal follows because they are independent and have
Gram matrix Id. Using the fact that the eigenvalues of S are between (1 − ε)−1/2 and
(1 + ε)−1/2, quantity 〈ai , ãi〉2 
(
a⊤
i
S−1/2ai
)2
>
‖ai ‖2
1+ε > ‖ai ‖2(1 − ε). Finally,
∑
i
ã⊗4
i
− a⊗4
i
 (S−1/2)⊗2
(∑
i
a⊗4
i
)
(S−1/2)⊗2 −
∑
i
a⊗4
i
and because ‖(S−1/2)⊗2 − Id ‖ 6
√
ε, and
∑
i a
⊗4
i
 ∑i j ai a⊤i ⊗ a ja⊤j , this difference has
spectral norm at most 3ε‖∑i a⊗4i ‖ 6 3√ε(1 + ε)2 6 4√ε. 
4.3 Full Recovery
Now we give the full algorithm, which will remove the components we find in each step
from the tensor without amplifying the spectral norm of the error too much.
Algorithm 4.11 (Full tensor decomposition). .
Input: A tensor T ∈ (d)⊗4, and the error parameter ε > ‖E{1,2}{3,4}‖.
1. Initialize the set of known components K  ∅, and the set of components under
inspection B  ∅.
2. Initialize a working copy of T, T
(0)
work
, and keep a clean copy of T called Tclean .
3. For t  0, . . . , 100 log n,
(a) Preprocess T
(t)
work
with Algorithm 4.1, then run Algorithm 4.3 with T
(t)
work
Õ(n)
times, then postprocess with Algorithm 4.8 using Tclean and error parameter
ε. If this produces an output vector v, add v to B (unless K already contains a
vector that is 1 − ε correlated with v).
18
(b) Let B  b1, . . . , bm , and abuse notation by letting B also be the matrix whose ith
row is bi . Compute the singular value decomposition B  UΣV
⊤, and compute
the orthonormalized set B̃  {b̃i}  {UΣ−1U⊤bi}mi1.
(c) Remove from B̃ any b̃i for which 〈Tclean , b̃i
⊗4〉 < (1 − 6ε)2 − ε.
(d) Update the known components: set K : K ∪ B̃, and set B, B̃ : ∅.
(e) Update the working tensor by removing known components: set T
(t+1)
work
:
T
(t)
work
−∑b̃∈B̃ b̃⊗4.
Output: The set of known components K.
Theorem 4.12. Given T 
∑n
i1 a
⊗4
i
+ E where the ai are orthonormal and ‖E{1,2}{3,4}‖ 6 ε,
then if ε < O(η2/log2 n), with probability 1 − o(1), Algorithm 4.11 recovers orthonormal vectors
b1, . . . , bn so that there exists a permutation π : [n] → [n] such that for each i ∈ [n],
〈ai , bπ(i)〉2 > 1 − 3ε.
Furthermore, this requires runtime Õ(n1+O(η)d2+ω).
First, we prove that if we have an orthonormal basis that approximates a1, . . . , ak , we can
subtract it without introducing a large spectral norm error—this motivates and justifies
steps 3(b)–3(e).
Lemma 4.13. Let a1, . . . , ak ∈ d and b1, . . . , bk ∈ d be two sets of orthonormal vectors, such
that 〈ai , bi〉2 > 1 − ε. Then 
∑
i
a⊗4
i
− b⊗4
i

2
6 4
√
ε
Proof. Define the matrices U,V ∈ d2×k so that the ith column of U (or V) is equal to a⊗2
i
(b⊗2
i
respectively). We have that∑
i
a⊗4
i
− b⊗4
i
 UU⊤ − VV⊤  (U − V)(U + V)⊤.
So it suffices for us to bound ‖U − V ‖ · ‖U + V ‖.
By the subadditivity of the norm, ‖U +V ‖ 6 ‖U‖ + ‖V ‖  2. Meanwhile, the singular
values of U − V are the square roots of the eigenvalues of ‖(U − V)⊤(U − V)‖, and so we
bound
(U − V)⊤(U − V)  UU⊤ + VV⊤ − U⊤V − V⊤U
 2 Idk −U⊤V − V⊤U , (4.1)
where the second line follows because U and V have orthonormal columns. Now, by
assumption we know that
U⊤V  (1 − ε) · Idk +E,
where for i , j, Ei j  〈bi , a j〉2 and Eii  〈bi , ai〉2 − (1 − ε), and by the orthonormality of
the a j , ∑
j
|Ei j | 
∑
j
〈bi , a j〉2  ε.
19
So the 1-norm of the rows of E is at most ε. By the orthonormality of the b j , the same
holds for the 1-norm of the columns,
∑
i |Ei j |. It follows that ‖E‖ 6 ε. Therefore,
U⊤V + V⊤U  2(1 − ε) Idk +Ê,
where ‖Ê‖ 6 2ε. Returning to (4.1), we can conclude that ‖(U − V)‖ 6 2
√
ε, and we have
our result. 
Now, we will prove that by orthogonalizing, we do not harm too many components b̃i .
Lemma 4.14. Suppose a1, . . . , ak ∈ d are orthonormal vectors, and u1, . . . , uk ∈ d are unit
vectors such that ‖ui − ai ‖22 6 ε. Let U be the d × k matrix whose ith column is ui , U  XΣY
be the singular value decomposition of U , and let ũi  XΣ
−1X⊤ui . Then for a 1 − δ fraction of
i ∈ [k],
〈ui , ũi〉 > 1 − ε/2δ.
Proof. For convenience, let A be the d × k matrix whose ith column is ai , and let Ũ 
XΣ−1X⊤U , and ũi  XΣ−1X⊤ui . Let  be the space of all real d × k real matrices with
orthonormal columns, and notice that Ũ ,A ∈  and that Ũ is closer to U than A. Indeed,
for any matrix X with orthonormal columns,
‖X−U‖2F  k+ ‖U‖2F −2〈U,X〉 > k+ ‖U‖2F−2‖X‖‖U‖∗  k+ ‖U‖2F−2‖U‖∗  ‖Ũ−U‖2F ,
Where we have used that the spectral norm and nuclear norm are dual. Therefore,
‖Ũ − U‖2F 6 ‖A − U‖2F 
∑
i
‖ai − ui ‖22  ε · k
And on average, ε > ‖ũi − ui ‖22 , so by Markov’s inequality, for at least (1 − δ)k of the ui ,
〈ui , ũi〉 > 1 − ε/2δ. 
Finally, we are ready to prove that Algorithm 4.11 works.
Proof of Theorem 4.12. We claim that in the tth iteration of step 3, with high probability we
have at most 0.45tn components remaining to be found, and that T
(t)
work
 T + F(t) where
‖F(t)‖ 6 8t
√
ε. For t  0, this is easily true.
Now assume this holds for t, and we will prove it for t + 1. Since by assumption
t
√
ε log n 6 100
√
ε log n 6 O(η), applying Lemma 4.2 and Theorem 4.4 to the running of
preprocessing Algorithm 4.1 and the main step Algorithm 4.3 with T
(t)
work
and Lemma 4.9
to the running of the postprocessing Algorithm 4.8 with Tclean , in step 3(a) with high
probability we will find m > 0.9nt vectors b1, . . . , bm so that 〈bi , ai〉2 > 1−3ε. Furthermore,
this takes a total of Õ(mnO(η)d2+ω) time.
By Lemma 4.14, in step 3(c) we will remove no more than a half of the b̃i , while
maintaining 〈b̃i , ai〉2 > 1−3ε (where we are abusing notation by re-indexing conveniently),
so that nt+1 > 0.45nt . Finally, by Lemma 4.13, we have that
|B̃ |∑
i1
a⊗4
i
− b̃i
⊗4

2
6 4
√
3ε,
20
So that in step 3(e),
T
(t+1)
work

©­«
T
(t)
work
−
|B̃ |∑
i1
a⊗4i
ª®¬
+
©­«
|B̃ |∑
i1
a⊗4i −
∑
b̃i∈B̃
b̃⊗4i
ª®¬
 T
(t−1)
work
+ F,
for a matrix F with ‖F‖ 6 8
√
ε. By induction, this implies that T
(t+1)
work
 T + F(t+1) where
‖F(t+1)‖ 6 ‖F‖ + ‖F(t)‖ 6 (t + 1)8
√
ε.
Taking a union bound over the high-probability success of Algorithm 4.3, we have that
after t  O(log n) steps we have found all of the components. We have spent a total of
Õ(n1+O(η)d2+ω) time in step 3(a). Finally, steps 3(b)-3(e) of Algorithm 4.11 require no more
than Õ(d3) time, and since the entire loop runs Õ(1) times, we have our result. 
4.4 Supporting Lemmas
Now we circle back and prove the omitted supporting lemmas.
Proof of Lemma 4.6. For convenience, fix j : 1. Let 1(1) be the component of 1 in the
direction a⊗2
1
, and let 1(>1) be the component of 1 orthogonal to a⊗2
1
. Notice that 1(1), 1(>1)
are independent.
By the orthogonality of the ai , our matrix M1 can be written as
M1  〈1(1) , a⊗21 〉 · a1a
⊤
1 +
d2∑
j1
(1(1)
j
+ 1
(>1)
j
) · (S − a⊗4i + E) j
 〈1(1) , a⊗21 〉 · a1a
⊤
1 +
©­«
d2∑
j1
1
(1)
j
· E jª®¬
+
©­«
d2∑
j1
1
(>1)
j
· T jª®¬
,
where T j is the jth matrix slice of T. For convenience, we can refer to the two sums on the
right as
N 
©­«
d2∑
j1
1
(1)
j
· E j
ª®¬
+
©­«
d2∑
j1
1
(>1)
j
· T j
ª®¬
.
First, we get a lower bound on the probability that the coefficient of a1a
⊤
1
is large. Let
G1(α) be the event that |〈1(1) , a⊗21 〉|  ‖1(1)‖ >
√
2α log n. By standard tail estimates on
univariate Gaussians, we have that
[G1(α)] > Õ(n−α).
Now, we bound ‖N ‖. Define the event E>1(ρ) to be the even that
E>1(ρ)
def




d2∑
j1
1
(>1)
j
· T j
 6
√
2(1 + ρ) log d


21
By Lemma A.1, we can conclude that

[
E>1(ρ)
]
> 1 − d−ρ .
To bound ‖N ‖, it thus remains to understand the term∑
j
1
(1)
j
E j  〈1 , a⊗21 〉 ·
∑
j
a⊗2
i
( j) · E j  〈1 , a⊗21 〉 · (aia
⊤
i ⊗ Idd2)E, (4.2)
where the quantity (ai a⊤i ⊗ Idd2)E corresponds to the contraction of E along two modes by
the vector a⊗2
i
. We make the following observation:
Observation 4.15. If P1, . . . , Pn are orthogonal projections from n
4 → K for some convex
set K, then for a 1 − δ fraction of i ∈ [n],
‖PiE‖F 6 ε/δ.
Proof. This follows from the fact that
ε2n > ‖E‖2F >
∑
i
‖PiE‖2F ,
and then by an application of Markov’s inequality. 
Now, note that
∑
j ai a
⊤
i
⊗ Idd2 for i ∈ [n] are orthogonal projectors from n
4
to n
2
.
Thus it follows that for a 1 − δ fraction of i ∈ [n], and without loss of generality assuming
that i  1 is among them, ‖(a1a⊤1 ⊗ Idd2)E‖F 6 ε/δ. Therefore for any unit vectors u , v ∈ d ,
returning to (4.2),u⊤
©­«
d2∑
j1
1
(1)
j
· E jª®¬
v
 
〈1 , a⊗21 〉 · 〈uv⊤, (a1a⊤1 ⊗ Idd2)E〉
6 ‖1(1)‖ ·
(a1a⊤1 ⊗ Idd2)EF · ‖uv⊤‖F 6 εδ ‖1(1)‖ .
Thus, combining the above we have a two-part upper bound on ‖N ‖.
Finally, define the event Ea1 ,E(θ) to be the event that
Ea1 ,E(θ)
def




©­«
d2∑
j1
1
(>1)
j
· T jª®¬
a1

2
,

©­«
d2∑
j1
1
(>1)
j
· T jª®¬
⊤
a1

2
6
ε
δ
·
√
2(1 + θ)


Examining this form, we can split
©­«
d2∑
j1
1
(>1)
j
· T jª®¬
a1 
d2∑
j1
1
(>1)
j
· (S − a⊗41 ) j a1 +
d2∑
j1
1
(>1)
j
· E j a1

d2∑
j1
1
(>1)
j
· E j a1
22
where the last line follows because the ai are orthogonal. We note that
∑
j 1
(>1)
j
E j a1 is a
Gaussian contraction of the form (a1 ⊗ Idd3)E. Again appealing to Observation 4.15 and to
the fact that the ai ⊗ Idd3 are orthogonal projections, we conclude that for a 1 − δ fraction
of i ∈ [n], ‖(a1 ⊗ Idd3)E‖F 6 ε/δ. Without loss of generality we assume that this is true for
i  1, from which it follows by Lemma A.1 that



d2∑
j1
1
(>1)
j
· E j a1

2
6
ε
δ
√
2(1 + θ)

> 1 − d−θ .
We can apply the same arguments to
(∑
j 1
(>1)
j
E j
)⊤
a1, and we conclude that

[
Ea1 ,E(θ)
]
> 1 − 2d−θ .
Now, by the union bound E>1(ρ) and Ea1 ,E both occur with probability at least 1 −
d−ρ − 2d−θ. Also, we notice that E>1 ∪ Ea1 ,E and G1 are independent. Therefore, for
ρ, θ > log log n/log n,
[G1(α), E>1(ρ), Ea1 ,E(θ)] > Õ(n−α) .
Conditioning on E>1 and G1,
M1  c · a1a⊤1 + N,
where |c | >
√
2α log n, and N is a matrix of norm at most (ε/δ)c +
√
2(1 + ρ) log d, such
that ‖Na1‖ , ‖N⊤a1‖ 6 (ε/δ)(c +
√
2(1 + θ)).
We now set α so that |c | > β‖N ‖. This occurs when
α >
(
β
1
1 − β(ε/δ)
)2
(1 + ρ).
Choosing β  1 + β′, ρ, θ  log log n/log n, we have our conclusion for a1, and by
symmetry for all other ai in the 1− 3δ fraction of i ∈ [n] for which the Frobenius norms of
the contractions are small. 
Proof of Lemma 4.7. Assume without loss of generality that c > 0. Choose κ  2ε |c |δ 6
|c |
(
1 − 11+β
)
. When κ · a1a⊤1 is subtracted from M1 , then given any unit vector v ∈ d with
|〈v , a1〉|  α, we can write v  αa1 + w where 〈w , a1〉  0 and ‖w‖ 
√
1 − α2. Examining
the action of M1 − κa1a⊤1 on v,
‖(M1 − κ · a1a⊤1 )v‖22  ‖(c − κ)αa1 + Nv‖22
6 (c − κ)2α2 + (c − κ)αa⊤1 Nv + (c − κ)αv⊤Na1 + v⊤N⊤Nv
applying the Cauchy-Schwarz inequality and our bounds on ‖N⊤a1‖ , ‖Na1‖,
6 (c − κ)2α2 + 2(c − κ)αεc + v⊤N⊤Nv .
23
Now expanding the v⊤N⊤Nv term along the components of v,
v⊤N⊤Nv  (αa1 + w)⊤N⊤N(αa1 + w)
6 (α‖Na1‖ + ‖w‖‖N ‖)2
and since ‖w‖ 
√
1 − α2, ‖Na1‖ 6 εc, and ‖N ‖ 6 c/(1 + β) 6 c(1 − β + 2β2),
6
(
αεc +
√
1 − α2(1 − β + 2β2)c
)2
,
and putting these together,
‖(M1 − κ · a1a⊤1 )v‖22 6 (c − κ)2α2 + 2(c − κ)αεc +
(
αεc +
√
1 − α2
1 + β
c
)2
It is easy to see that when c/(1+ β) < c − κ, this quantity is maximized at α  1, and so by
our choice of κ we have that
‖(M1 − κ · a1a⊤1 )v‖22 6 (c − κ)2 + 2εc(c − κ) + ε2c2  (c(1 + ε) − κ)2
and thus ‖M1 − κa1a⊤1 ‖ 6 (1 + ε)c − κ.
Now we will lower bound ‖M1 ‖.
‖M1 ‖ > a⊤1 M1a1  c + a⊤1 Na1
> c − ‖a1‖‖Na1‖
> c(1 − ε) .
Where we have applied the Cauchy-Schwarz inequality, and the assumption that ‖Na1‖ 6
εc. It follows that
‖M1 − κa1a⊤1 ‖ 6 ‖M1 ‖ + 2εc − κ.
Finally applying Fact A.3, we can conclude that for either the left- or right-singular unit
vector u of M1 ,
〈ai , u〉2 >
κ − 2εc
κ
> 1 − δ .
Choosing δ 
2ε(1+β)
β as small as possible, we have our result. 
5 Learning Orthonormal Dictionaries
Here, we show how to use our tensor decomposition algorithm to learn dictionaries with
orthonormal basis vectors.
Problem 5.1. Given access to a dictionary A ∈ d×d with independent columns a1, . . . , ad ,
in the form of samples y(1)  Ax(1), . . . , y(m)  Ax(m) for independent x(i), recover A.
24
Below, in Section 5.3, we will prove that Õ(n3) samples suffice to estimate the 4th
moment tensor within o(1) spectral norm error. Computing this matrix from Õ(n3)
samples takes Õ(n3d4) time. Thus we can equivalently formulate the problem as follows:
Problem 5.2. Given access to A ∈ d×n with independent columns a1, . . . , an, via a noisy
copy of the 4th moment tensor T  [(Ax)⊗4] + E, recover the columns a1, . . . , an .
Finally, given access to a sufficiently large number of samples, we can reduce to the
case where the columns of A are orthogonal:
Lemma 5.3. Suppose that the samples y  Ax are generated from a distribution over x for which
[x2
i
]  [x2
j
] for all i , j ∈ [n], and that the columns of A are independent. Then there exists
an efficient reduction from the case when A has independent columns to the case when A has
orthogonal columns, with sample complexity growing polynomially with the condition number of
Σ  AA⊤.
The proof is straightforward, involving a transformation by the empirical covariance
matrix, and we give it below in Section 5.2.
Now, we reduce the dictionary learning problem to tensor decomposition. In Section 3,
we explained that the 4th moment tensor itself may be far from our target tensor
∑
i a
⊗4
i
,
due to the presence of a low-rank, high-Frobenius norm component. The sum-of-squares
algorithm for this problem can overcome this difficulty by exploiting the SDP’s symme-
try constraints. In our algorithm, we will exploit this symmetry manually to go from
[(Ax)⊗4] to a tensor that approximates ∑i a⊗4i well in Frobenius norm.
Algorithm 5.4. Input: A noisy copy of the fourth moment tensor T  [(Ax)⊗4] + E,
truncation parameter ε.
1. Reshape T to T{1,2}{3,4}, and perform the eigenvalue truncation
T>ε
def
 (T − ε Id)+,
where + denotes projection to the PSD cone.
2. Compute the truncation of a different reshaping of T>ε,
T̃  (σ(T>ε{1,3}{2,4}) − ε Id)+
Output: The tensor T̃ as an approximation of
∑
i [x4i ] · a
⊗4
i
.
Our claim is that this produces a tensor that is close to
∑
i [x4i ] · a
⊗4
i
in Frobenius norm.
Theorem 5.5. If the x are independent and distributed so that [xix jxkxℓ]  0 unless xix jxkxℓ
is a square, and so that for all i , j ∈ [n], [x2
i
x2
j
] 6 α[x4
1
] for α < 1, then given access to
T  [(Ax)⊗4] + E where ‖E‖ 6 α, Algorithm 5.4 with ε  3α returns a tensor T̃ such that T̃ −
∑
i
[x4i ] · a
⊗4
i

F
6 9α
√
n .
25
Proof. For convenience denote by T
def
 ([(Ax)⊗4] + E){1,2}{3,4}, and define S 
∑
i [x4i ] ·
a⊗4
i
. We have that
T − E  [(Ax)⊗4] 
d∑
i, j,k ,ℓ1
[xix j xkxℓ] · (ai ⊗ a j)(ak ⊗ aℓ)⊤

∑
i
[x4i ] · a⊗4i +
∑
i, j
[x2i x2j ] ·
(
(a⊗2
i
)(a⊗2
j
)⊤ + ai a⊤i ⊗ a j a⊤j + a ja⊤i ⊗ ai a⊤j
)
The latter term can be split into three distinct matrices—the first is a potentially low-rank
matrix, and may have large eigenvectors.8 The latter two terms have small spectral norm.
Claim 5.6.
∑
i, j
[x2i x2j ] · ai a⊤i ⊗ a ja⊤j
 6 α[x41], and

∑
i, j
[x2i x2j ] · a ja⊤i ⊗ ai a⊤j
 6 α[x41] .
We’ll prove this claim below. Now, again for convenience define the matrix N to be the
remaining term, N 
∑
i, j [x2i x
2
j
](a⊗2
i
)(a⊗2
j
)⊤. From Claim 5.6 and by our assumption on
‖E‖,
T  S + N + Ê,
for ‖Ê‖ 6 3α. On the other hand, if we let B be the d × n matrix whose ith column is a⊗2
i
,
and we let X be the n × n matrix whose i , jth entry is [x2
i
x2
j
], then S + N  BXB⊤, and
so rank(S + N) 6 n.
It follows that when we perform the eigenvalue truncation in step 1 of Algorithm 5.4,
T<ε  (T − 3α · Id)+,
then we have that rank(T<ε) 6 n as well. Also by definition of truncation, T  T<ε + Ẽ,
and because to begin with we had T  0, ‖Ẽ‖ 6 3α. Putting the above together, it follows
that
‖T<ε − (S + N)‖F  ‖Ẽ − Ê‖F 6 6α
√
2n ,
where we have used that rank(T<ε − (S + N)) 6 2n and ‖Ẽ − E‖ 6 6α. Now, we recall the
reshaping operation on tensors from step 2 of Algorithm 5.4—in going from the reshaping
{1, 2}{3, 4} to {1, 3}{2, 4}, the rank-1 tensor (a ⊗ b)(c ⊗ d)⊤) is reshaped to (a ⊗ c)(b ⊗ d)⊤.
Let σ(·) denote this reshaping operation. Reshaping does not change the Frobenius norm.
So by linearity, and since σ fixes S,
‖σ(T<ε) − S − σ(N)‖F  ‖T<ε − (S + N)‖F 6 6α
√
2n.
We now remark that σ maps N to one of the bounded-norm matrices from Claim 5.6,
σ(N) 
∑
i, j
[x2i x
2
j ] · aia
⊤
i ⊗ a j a⊤j  α · Id .
8For instance, in the case when [x2
i
x2
j
]  p2, this term is rank-1 and has spectral norm pn.
26
Furthermore, because the positive semidefinite cone is a closed convex set, and because
projection to closed convex sets can only decrease distances (see Lemma A.2),σ(T<ε) − S − σ(N)
F

σ(T<ε) − S − α · Id−(σ(N) − α · Id)
F
>
(σ(T<ε) − S − α · Id)
+
− (σ(N) − α · Id)+

F
>
(σ(T<ε) − S − α · Id)
+

F
>
(σ(T<ε) − α · Id)
+
− S

F
,
where to obtain the last inequality we used that S is positive semidefinite. Therefore, step
3 of the algorithm ensures that T̃ is close to S in Frobenius norm, as desired. 
Now we prove that the spectral norms of the symmetrizations of the tensor have small
spectral norm.
Proof of Claim 5.6. The first matrix that we are interested in is PSD, and can dominated by
a tensor power of the identity:
0 
∑
i, j
[x2
i
x2
j
]
[x4
1
]
· aia⊤i ⊗ a j a⊤j  α
∑
i, j
ai a
⊤
i ⊗ a ja⊤j  α ·
(∑
i
aia
⊤
i
)
⊗ ©­«
∑
j
a j a
⊤
j
ª®
¬
 α · Id .
For the second matrix, if we let A be the d2 × n2 matrix whose i , jth column is ai ⊗ a j , and
let M be the n2 × n2 matrix whose i , jth diagonal entry is [x2
i
x2
j
], then∑
i, j
[x2i x
2
j ] · a ja
⊤
i ⊗ ai a⊤j  AMΠA⊤,
whereΠ is the permutation matrix that takes the i , jth row to the j, ith row. By assumption,
‖M‖ 6 maxi, j [x2i x2j ] 6 α[x21], and the columns of A are orthonormal, so ‖A‖  1. It
follows by the submultiplicativity of the spectral norm that
∑
i, j
[x2i x2j ] · a j a⊤i ⊗ ai a⊤j
 6 α[x41] .
This gives us the claim. 
When [x4
i
]  p for all i ∈ [n], applying Algorithm 4.3 with T̃ a total of Õ(n) times
will allow us to recover m > n/2 vectors b1, . . . , bm with 〈ai , bi〉2 > 0.99. The following
subsections contain the details regarding the refinement of the approximation, and the
sample complexity bounds for estimating the 4th moment tensor.
5.1 Postprocessing to refine approximation
We now analyze the postprocessing algorithm Algorithm 4.8 for the context of dictionary
learning, in which our tensor has the form T  [(Ax)⊗4]. We claim that, despite not hav-
ing bounded spectral norm error away from
∑
i [x4i ] · a
⊗4
i
, the postprocessing algorithm
still succeeds.
27
Lemma 5.7. Suppose that we are given T  [(Ax)⊗4], where x is distributed so that
[xix jxkxℓ]  0 unless xix jxkxℓ is a square, [x4i ]  p for all i ∈ [n], and [x2i x2j ] 6 αp.
Suppose furthermore that we have a unit vector u such that 〈b , ai〉2 > 0.99 for some i ∈ [n]. Then
applying Algorithm 4.8 to u and T with error parameter 1/2 returns a vector v such that
〈v , ai〉2 > 1 − 16α.
Proof. Without loss of generality, let i : 1 so that 〈b , ai〉2  1 − η > 0.99 (henceforth, we
use i as an ordinary index). We have that
[(Ax)⊗4](u ⊗ u)  p
∑
i
〈u , ai〉2a⊗2i
+
∑
i, j
[x2i x
2
j ]
(
〈u , a j〉2 · a⊗2i + 〈u , ai〉〈u , a j〉(ai ⊗ a j + a j ⊗ ai)
)
Define Mu to be the reshaping of[(Ax)⊗4](u⊗u) to a d2×d2 matrix. We must understand
the spectrum of Mu , and for now we turn our attention to the second sum. Splitting the
second sum into distinct parts, we have by the orthonormality of the ai that∑
i, j
[x2i x2j ] · 〈u , a j〉2 · ai a⊤i  maxi, j [x
2
i x
2
j ] ·
∑
i, j
ai a
⊤
i  αp · Id,
where the last line is by our assumption on [x2
i
x2
j
]. Finally, for any w ∈ d,
w⊤
©­«
∑
i, j
[x2i x
2
j ] · 〈u , ai〉〈u , a j〉(ai a
⊤
j + a j a
⊤
i )
ª®¬
w

∑
i, j
[x2i x2j ] · 2〈u , ai〉〈u , a j〉〈w , ai〉〈w , a j〉
Applying Cauchy-Schwarz and pulling out the maximum multiplier,
6 2 max
i, j
(E[x2i x2j ]) ·
©­«
∑
i, j
〈u , ai〉2〈w , a j〉2ª®¬
1/2 ©­«
∑
i, j
〈u , a j〉2〈w , ai〉2ª®¬
1/2
Now noticing that the two parenthesized terms are actually identical, then adding a
positive quantity and factoring,
6 2 max
i, j
(E[x2i x
2
j ]) ·
©­«
(∑
i
〈u , ai〉2
) ©­«
∑
j
〈w , a j〉2ª®¬
ª®¬
 2 max
i, j
[x2i x
2
j ] 6 2pα.
An identical proof, up to signs, gives us a lower bound of 2pα.
Therefore,
1
p
Mu 
∑
i
〈u , ai〉2aia⊤i + E,
28
For a matrix E with ‖E‖ 6 4α.
It remains to argue that the top eigenvector of Mu is a1. We have that
‖p−1Mu‖ > a⊤1 Mua1
a⊤1
∑
i
〈u , ai〉2aia⊤i a1 + a⊤1 Ea1
> 1 − η − 4α,
where the last line follows from the orthonormality of the ai and our bound on ‖E‖.
Meanwhile, for any unit vector w ∈ d and any ε < 1 − 2η,
w⊤
(
p−1M − ε · a1a⊤1
)
w  (1 − η − ε)〈a1 , w〉2 +
∑
i>1
〈u , ai〉2〈ai , w〉2 + w⊤Ew
and since maxi>1〈ai , u〉2 6 η,
6 (1 − η − ε)〈a1, w〉2 + η · (1 − 〈a1, w〉2) + 4α
6 1 − η − ε + 4α,
where the last line follows because w is a unit vector, and we chose ε so that 1− η − ε > η.
Therefore
‖p−1Mu − εa1a⊤1 ‖ 6 ‖p−1Mu ‖ − ε + 8α.
Applying Fact A.3, we conclude that if v is the top eigenvector of Mu, then 〈v , a1〉2 >
ε−8α
ε > 1 − 8αε . Since we can choose ε  12 and still have that ε < 0.98 < 1 − 2η, the
conclusion follows. 
5.2 From independent columns to orthonormal columns
We now use standard techniques to prove that one can reduce from a dictionary with
independent columns to a dictionary with orthogonal columns, given sufficiently many
samples.
Proof of Lemma 5.3. Note that the expected covariance matrix of the samples is equal to a
scaled version of the covariance matrix,
[(Ax)(Ax)⊤]  [x1]2 · Σ.
Since we have assumed that ‖x‖2
2
6 n, the Frobenius norm of (Ax)(Ax)⊤ is bounded by n
for every sample, and [((Ax)(Ax)⊤)2] 6 n2. By applying a matrix Bernstein inequality
(see e.g. [Tro12]), we have that so long as we have m > Õ((n/β)2) samples, the empirical
covariance matrix Σ̂  1m
∑m
i1 y
(i)(y(i))⊤ will approximate Σwithin β in spectral norm.
So given sufficiently many samples, we can compute a good spectral approximation of
Σ−1/2, Σ̂−1/2 with Σ̂−1/2 − Σ−1/2 6 ε.
29
Assuming access to such a Σ̂−1/2, we can transform A to Ã  Σ̂−1/2A. Now, for matrices
X, Y, Z of suitable dimensions,
YXY − ZXZ  1
2
((Y − Z)X(Y + Z) + (Y + Z)X(Y − Z)) .
Applying this to ÃÃ⊤ − Id  Σ̂−1/2ΣΣ̂−1/2 − Σ−1/2ΣΣ−1/2,ÃÃ⊤ − Id 6 ‖Σ̂−1/2 − Σ−1/2‖ · ‖Σ‖ · (‖Σ̂−1/2‖ + ‖Σ−1/2‖)
6 ε · ‖Σ‖ · (2 + ε)‖Σ−1/2‖
6 O
(
ε
λmax(Σ)
λmin(Σ)1/2
)
.
Defining η
def
 ‖ÃÃ⊤ − Id ‖, we have that ÃÃ⊤  (1 ± η) Id, and the columns of Ã are
near-orthonormal. Similarly, we can transform our samples
y(i)  Ax(i) → ỹ(i)  Σ̂−1/2Ax(i) .
Now, define Sdi f f 
(
(Σ̂−1/2)⊗2 − (Σ−1/2)⊗2
)
and Ssum 
(
(Σ̂−1/2)⊗2 + (Σ−1/2)⊗2
)
. We can
factor the difference
[(Ãx)⊗4 − (Σ−1/2Ax)⊗4] 
12Sdi f f [(Ax)⊗4]Ssum + 12Ssum [(Ax)⊗4]Sdi f f

6 ‖Ssum ‖ · ‖Sdi f f ‖ ·
[(Ax)⊗4]
6
(
‖Σ̂−1/2‖2 + ‖Σ−1/2‖2
)
· ‖Sdi f f ‖ ·
[(Ax)⊗4]
6 (2 + ε)‖Σ−1‖ · ‖Sdi f f ‖ ·
[(Ax)⊗4]
Applying the identity
A⊗2 − B⊗2  1
2
(A − B) ⊗ (A + B) + 1
2
(A − B) ⊗ (A + B),
to Sdi f f , we can get that
[(Ãx)⊗4 − (Σ−1/2Ax)⊗4] 6 (2 + ε)‖Σ−1‖ · ‖Σ̂−1/2 − Σ−1/2‖ (‖Σ̂−1/2‖ + ‖Σ−1/2‖) [(Ax)⊗4] ,
6 9 · ‖Σ−3/2‖ · ε
[(Ax)⊗4] .
Since we can choose the number of samples so as to make this last quantity as small as
we would like, as a function of the condition number, and then appealing to Fact 4.10, the
reduction is complete. 
5.3 Sample complexity bounds
Below is our bound on the sample complexity of approximating the 4th moment tensor,
which we believe may be loose.
30
Proposition 5.8. Given samples of the form y(i)  Ax(i) for x(i) ∼ D, as long as β > [x8
i
]
dominates the expectation of any other order-8 monomial in x, and any monomial with odd
multiplicity has expectation 0, and the entries of x are bounded by κ, then with high probability
given m > Õ(max{βn3 , (κn)2}) samples, 1m
m∑
i1
(y(i))⊗4 − 
x∼D
[(Ax)⊗4]
 6 o(1).
Proof. Our matrix has the form
M 
∑
i jkℓ
xix jxkxℓ · (ai ⊗ a j)(ak ⊗ aℓ)⊤,
And the ai are orthonormal, so
[MM⊤] 
∑
i, j,i′ , j′
k ,ℓ
[xix jxi′x j′x2kx
2
ℓ] · (ai ⊗ a j)(ai′ ⊗ a j′)⊤.
This is because of the orthonormality of the ai , which guarantees that terms in the product
MM⊤ in which we have an inner product between two non-identical vectors drop out to
0.
If we define A to be the d2 × n2 matrix whose columns are the Kronecker products
ai ⊗ a j for all i , j ∈ [n], and if for each pair k , ℓ ∈ [n] we define X(k ,ℓ) be the n2 × n2 matrix
whose (i , j), (i′, j′)th entry is [xix j xix j′x2k x
2
ℓ
], we can realize E[MM⊤] as
[MM⊤]  A
(∑
k ,ℓ
X(k ,ℓ)
)
A⊤.
Because we assumed that E[xi x jxi′x j′x2k x
2
ℓ
]  0 unless every index appears with even
multiplicity, the entry of X(k ,ℓ) will be 0. This happens only on the diagonal, unless i′  j′
and i  j, or for X(k ,ℓ) in the intersection of the (k , ℓ)th row and the (ℓ, k)th column and
the (ℓ, k)th row and the (k , ℓ)th column. So we split each X(k ,ℓ) into a diagonal part D(k ,ℓ),
an intersection part corresponding to the (k , ℓ) and (ℓ, k) intersections C(k ,ℓ) , and the rest
of the off-diagonal part R(k ,ℓ) . and it follows that
‖[MM⊤]‖ 6 n2‖A‖2 max
k ,ℓ
(D(k ,ℓ) + Ck ,ℓ + R(k ,ℓ)) .
Because every entry is bounded by [x8
i
] 6 β, and the D(k ,ℓ) are diagonal, the ‖D‖ term
contributes β. The C matrices have Frobenius norm 2β, and the R matrices have only n2
nonzero entries, so ‖R‖F 6 βn. Therefore,
‖[MM⊤]‖ 6 3βn3.
For each sample x(i), we have that ‖ 1m (Ax(i))⊗4‖F 6 κ
2n2
m , and we have by the above
reasoning that ‖[ 1
m2
(Ax(i))⊗4(Ax(i))⊗4]‖ 6 3[x8
i
] n3
m2
. So the variance of the empiri-
cal 4-tensor is
√
n3/m, and the absolute bound on the norm of any summand is n2/m.
Applying a matrix Bernstein inequality (see e.g. [Tro12]), we have that as long as we
have m ≫ max{κ2n2 log n , βn3 log n} samples, with high probability we approximate
[(Ax)⊗4] within spectral norm o(1). 
31
References
[AAJ+14] Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netra-
palli, and Rashish Tandon, Learning sparsely used overcomplete dictionaries,
COLT, JMLR Workshop and Conference Proceedings, vol. 35, JMLR.org, 2014,
pp. 123–137. 3
[ABGM14] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma, More algorithms
for provable dictionary learning, CoRR abs/1401.0579 (2014). 3, 4
[AFH+12] Anima Anandkumar, Dean P. Foster, Daniel J. Hsu, Sham Kakade, and Yi-Kai
Liu, A spectral algorithm for latent dirichlet allocation, NIPS, 2012, pp. 926–934.
1
[AGH+14] Animashree Anandkumar, Rong Ge, Daniel J. Hsu, Sham M. Kakade, and Ma-
tus Telgarsky, Tensor decompositions for learning latent variable models, Journal
of Machine Learning Research 15 (2014), no. 1, 2773–2832. 2
[AGHK13] Animashree Anandkumar, Rong Ge, Daniel J. Hsu, and Sham Kakade, A ten-
sor spectral approach to learning mixed membership community models, COLT,
JMLR Workshop and Conference Proceedings, vol. 30, JMLR.org, 2013,
pp. 867–881. 5
[AGMM15] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra, Simple, efficient, and
neural algorithms for sparse coding, COLT, JMLR Workshop and Conference
Proceedings, vol. 40, JMLR.org, 2015, pp. 113–149. 3
[AGMR16] Sanjeev Arora, Rong Ge, Tengyu Ma, and Andrej Risteski, Provable learning of
noisy-or networks, CoRR abs/1612.08795 (2016). 1, 2
[BCMV14] Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vĳayaragha-
van, Smoothed analysis of tensor decompositions, STOC, ACM, 2014, pp. 594–603.
3
[BKS15] Boaz Barak, Jonathan A. Kelner, and David Steurer, Dictionary learning and
tensor decomposition via the sum-of-squares method, STOC, ACM, 2015, pp. 143–
151. 1, 2, 3, 4, 5
[EA06] Michael Elad and Michal Aharon, Image denoising via sparse and redundant
representations over learned dictionaries, Image Processing, IEEE Transactions
on 15 (2006), no. 12, 3736–3745. 3
[EP07] Andreas Argyriou Theodoros Evgeniou and Massimiliano Pontil, Multi-task
feature learning, Advances in Neural Information Processing Systems 19: Pro-
ceedings of the 2006 Conference, vol. 19, MIT Press, 2007, pp. 41–48. 3
32
[GO09] Tom Goldstein and Stanley Osher, The split bregman method for l1-regularized
problems, SIAM journal on imaging sciences 2 (2009), no. 2, 323–343. 3
[Har70] Richard A Harshman, Foundations of the parafac procedure: Models and conditions
for an" explanatory" multi-modal factor analysis. 1
[HK13] Daniel Hsu and Sham M. Kakade, Learning mixtures of spherical Gaussians:
moment methods and spectral decompositions, ITCS’13—Proceedings of the 2013
ACM Conference on Innovations in Theoretical Computer Science, ACM,
New York, 2013, pp. 11–19. MR 3385380 1
[HM16] Elad Hazan and Tengyu Ma, A non-generative framework and convex relaxations
for unsupervised learning, NIPS, 2016, pp. 3306–3314. 3
[HP14] Moritz Hardt and Eric Price, The noisy power method: A meta algorithm with
applications, NIPS, 2014, pp. 2861–2869. 13, 15
[HSSS15] Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer, Speed-
ing up sum-of-squares for tensor decomposition and planted sparse vectors, CoRR
abs/1512.02337 (2015). 11
[HSSS16] , Fast spectral algorithms from sum-of-squares proofs: tensor decomposition
and planted sparse vectors, STOC, ACM, 2016, pp. 178–191. 2, 3
[LCC07] Lieven De Lathauwer, Joséphine Castaing, and Jean-François Cardoso, Fourth-
order cumulant-based blind identification of underdetermined mixtures, IEEE Trans.
Signal Processing 55 (2007), no. 6-2, 2965–2973. 1
[LMV96] Lieven De Lathauwer, Bart De Moor, and Joos Vandewalle, Blind source sepa-
ration by simultaneous third-order tensor diagonalization, EUSIPCO, IEEE, 1996,
pp. 1–4. 1
[MLB+08] Julien Mairal, Marius Leordeanu, Francis Bach, Martial Hebert, and Jean
Ponce, Discriminative sparse image models for class-specific edge detection and
image interpretation, Computer Vision–ECCV 2008, Springer, 2008, pp. 43–56.
3
[MR05] Elchanan Mossel and Sébastien Roch, Learning nonsingular phylogenies and
hidden markov models, STOC, ACM, 2005, pp. 366–375. 1
[MRBL07] Y Marc’Aurelio Ranzato, Lan Boureau, and Yann LeCun, Sparse feature learning
for deep belief networks, Advances in neural information processing systems 20
(2007), 1185–1192. 3
[MSS16] Tengyu Ma, Jonathan Shi, and David Steurer, Polynomial-time tensor decompo-
sitions with sum-of-squares, FOCS, IEEE Computer Society, 2016, pp. 438–446.
1, 2, 3, 4, 5, 7, 12, 34
33
[OF97] Bruno A Olshausen and David J Field, Sparse coding with an overcomplete basis
set: A strategy employed by v1?, Vision research 37 (1997), no. 23, 3311–3325. 3
[Oli10] Roberto I. Oliveira, Sums of random Hermitian matrices and an inequality by
Rudelson., Electron. Commun. Probab. 15 (2010), 203–212 (English). 34
[Roc76] R. Tyrrell Rockafellar, Monotone operators and the proximal point algorithm, SIAM
Journal on Control and Optimization 14 (1976), no. 5, 877–898. 6, 34
[Tro12] Joel A. Tropp, User-friendly tail bounds for sums of random matrices, Foundations
of Computational Mathematics 12 (2012), no. 4, 389–434. 8, 29, 31
[YWHM08] Jianchao Yang, John Wright, Thomas Huang, and Yi Ma, Image super-resolution
as sparse representation of raw image patches, Computer Vision and Pattern
Recognition, 2008. CVPR 2008. IEEE Conference on, IEEE, 2008, pp. 1–8. 3
A Useful Tools
Lemma A.1 (Concentration of random tensor contractions [MSS16]). Let 1 be a standard
Gaussian vector ink , 1 ∼ N(0, Idk). Let A be a tensor in (k) ⊗ (ℓ) ⊗ (m), and call the three
modes of A α, β, γ respectively. Let Ai be a ℓ × m slice of A along mode α. Then,

[
k∑
i1
1iAi
 > t · max
{A{αβ}{γ} , A{αγ}{β}}
]
6 (m + ℓ) exp
(
− t
2
2
)
.
Proof. We compute the expectation and variance of our matrix,

1
[
k∑
i1
1iAi
]
 0, and
1
[
k∑
i1
1iAi
]  max
{
k∑
i1
AiA
⊤
i
 ,

k∑
i1
A⊤i Ai

}
,
The two variance terms correspond to ‖A{αβ}{γ}‖2 and ‖A{αγ}{β}‖2 respectively. We can
now apply concentration results for matrix Gaussian series to conclude the proof [Oli10].

The following lemma states that distances can only decrease under projections to a
convex set, and is well-known (see e.g. [Roc76]).
Lemma A.2. Let C ⊂ n be a closed convex set, and let Π : n → C be the projection operator
onto C in terms of norm ‖ · ‖2, i.e. Π(x)
def
 argminc∈C ‖x − c‖2. Then for any x , y ∈ n,
‖x − y‖2 > ‖Π(x) −Π(y)‖2.
34
Proof. If we let Dx  x −Π(x), Dy  y −Π(y),
‖x − y‖2  ‖Dx − Dy +Π(x) −Π(y)‖2
 ‖Dx − Dy‖2 + ‖Π(x) −Π(y)‖2 + 2〈Dx − Dy ,Π(x) −Π(y)〉
Now the conclusion will follow from the fact that
〈Dx − Dy ,Π(x) −Π(y)〉 > 0.
This is because, by definition of Π,
Π(x)  argminc∈C ‖x − c‖22  argminp∈n
1
2
‖x − p‖22 + C(p) ,
where C(·) is the convex function defined to be ∞ on elements not in C and 0 otherwise.
From the strong convexity of the last expression the projection is unique. From the
optimality conditions, it follows that Π(x) is the unique point p ∈ n such that x − p ∈
∂ C(p), where ∂ C(p) is the set of subgradients of C at p.
By definition of the subgradient and by the convexity of C, for any p , q ∈ n and for
1p ∈ ∂ C(p),1q ∈ ∂ C(q),
C(p) + 〈1p , q − p〉 6 C(x)
〈1p , q − p〉 6 C(q) − C(p)
−〈1q , q − p〉 6 − C(q) + C(p)
〈1p − 1q , p − q〉 > 0 .
Now, taking p  Π(x) and q  Π(y), we have that Dx  x − Π(x) ∈ ∂ C(Π(x)), and
Dy  y −Π(y) ∈ ∂ C(Π(y)), so from the above,
〈Dx − Dy ,Π(x) −Π(y)〉 > 0,
as desired. 
Fact A.3. Let v ∈ n, and suppose that ‖M − vv⊤‖ 6 ‖M‖ − ε‖v‖2. Then if u , w are the top
unit left- and right-singular vectors of M,
〈u , v〉2 > ε · ‖v‖2 or 〈w , v〉2 > ε · ‖v‖2
Proof. We have that
‖M‖ − ε‖v‖2 > |u⊤(M − vv⊤)w | > |u⊤Mw | − |u⊤vv⊤w |  ‖M‖ − |〈u , v〉〈w , v〉|,
where the second inequality is the triangle inequality. Rearranging, the conclusion follows.

Fact A.4. If A is an n × n symmetric matrix with eigendecomposition ∑i∈[n] λiui u⊤i for orthonor-
mal u1, . . . , un ∈ n and eigenvalues λ1 > · · · > λn, then the projection of A to the PSD cone is
equal to
∑
i∈[n] [λi > 0] · λiuiu⊤i .
35
Proof. Let Â  A + B be the projection (in Frobenius norm) of A to the PSD cone. Because
u1, . . . , un are orthonormal, we may choose an orthonormal basis V  {vi}n
2
i1
forn
2
that
includes v1  u1 ⊗ u1, v2  u2 ⊗ u2, . . . , vn  un ⊗ un , as the first n basis vectors. Now,
viewing A, B as vectors inn
2
, we can write A 
∑n
i1 λivi for λi the eigenvalues of A, and
write B 
∑n2
i1 βi vi for some scalars β1, . . . , βn2 .
For any eigenvector ui of A, we have that u
⊤
i
Âui  〈vi ,A + B〉  λi + βi by the
orthonormality of the vi . Therefore, if λi < 0 we must have βi > |λi |, since Â is PSD. We
also have that ‖A − Â‖2F  ‖B‖2F 
∑n2
i1 β
2
i
, so B 
∑n
i1 [λi < 0] · |λi | · ui u⊤i minimizes
the Frobenius norm of the difference, which (after checking to see that A + B has all
non-negative eigenvalues) concludes the proof.

36

