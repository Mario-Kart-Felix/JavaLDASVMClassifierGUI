IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 6, NO. 3, JUNE 1996 243 
A New, Fast, and Efficient Image Codec Based 
on Set Partitioning in Hierarchical Trees 
Amir Said, Member, IEEE, and William A. Pearlman, Senior Member, IEEE 
Albstruct- Embedded zerotree wavelet (EZW) coding, intro- 
duced by J. M. Shapiro, is a very effective and computationally 
simple technique for image compression. Here we offer an alter- 
native explanation of the principles of its operation, so that the 
reasons for its excellent performance can be better understood. 
Theise principle9 are partial ordering by magnitude with a set 
parlitioning sorting algorithm, ordered bit plane transmission, 
and exploitation of self-similarity across different scales of an 
image wavelet transform. Moreover, we present a new and 
different implementation based on set partitioning in hierarchical 
trees (SPIHT), which provides even better performance than 
our previously reported extension of EZW that surpassed the 
performance of the original EZW. The image coding results, 
calculated from actual file sizes and images reconstructed by 
the decoding algorithm, are either comparable to or surpass 
previous results obtained through much more sophisticated and 
computationally complex methods. In addition, the new coding 
and decoding procedures are extremely fast, and they can be 
made even faster, with only small loss in performance, by omitting 
entropy coding of the bit stream by arithmetic code. 
I. INTRODUCTION 
IMAGE compression techniques, especially nonreversible I or lossy ones, have been known to grow computationally 
more complex as they grow more efficient, confirming the 
tenets of source coding theorems in information theory that 
a code for a (stationary) source approaches optimality in the 
limit of infinite computation (source length). Notwithstand- 
ing, the image coding technique called embedded zerotree 
wavelet (EZW), introduced by Shapiro [ 11, interrupted the 
simultaneous progression of efficiency and complexity. This 
technique not only was competitive in performance with the 
most complex techniques, but was extremely fast in execution 
and produced an embedded bit stream. With an embedded 
bit stream, the reception of code bits can be stopped at any 
poinit and the image can be decompressed and reconstructed. 
Following that significant work, we developed an alternative 
exposition of the underlying principles of the EZW technique 
and presented an extension that achieved even better results 
[61. 
In this article, we again explain that the EZW technique is 
based on three concepts: 1) partial ordering of the transformed 
image elements by magnitude, with transmission of order by a 
Manuscript received April 18, 1995. This paper was recommended by 
Associate Editor C. Gonzales. This paper was presented in part at the IEEE 
lnternational Symposium on Clrcuits and Systems, Chcago, IL, May 1993. 
A. Said is with the Faculty of Electrical Engineering, State University of 
Campinas (UNICAMP), Campinas, SP, 1308 1, Brazil. 
W. A. Pearlman is with the Department of Electrical, Computer, and 
Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180 USA. 
Publisher Item Identifier S 1051-8215(96)04105-5. 
subset partitioning algorithm that is duplicated at the decoder, 
2) ordered bit plane transmission of refinement bits, and 3) ex- 
ploitation of the self-similarity of the image wavelet transform 
across different scales. As to be explained, the partial ordering 
is a result of comparison of transform element (coefficient) 
magnitudes to a set of octavely decreasing thresholds. We say 
that an element is significant or insignificant with respect to a 
given threshold, depending on whether or not it exceeds that 
threshold. 
In this work, crucial parts of the coding process-the way 
subsets of coefficients are partitioned and how the significance 
information is conveyed-are fundamentally different from the 
aforementioned works. In the previous works, arithmetic cod- 
ing of the bit streams was essential to compress the ordering 
information as conveyed by the results of the significance 
tests. Here, the subset partitioning is so effective and the 
significance information so compact that even binary uncoded 
transmission achieves about the same or better performance 
than in these previous works. Moreover, the utilization of 
arithmetic coding can reduce the mean squared error or in- 
crease the peak signal-to-noise ratio (PSNR) by 0.3-0.6 dB 
for the same rate or compressed file size and achieve results 
which are equal to or superior to any previously reported, 
regardless of complexity. Execution times are also reported 
to indicate the rapid speed of the encoding and decoding 
algorithms. The transmitted code or compressed image file 
is completely embedded, so that a single file for an image 
at a given code rate can be truncated at various points and 
decoded to give a series of reconstructed images at lower 
rates. Previous versions [l], [6] could not give their best 
performance with a single embedded file and required, for each 
rate, the optimization of a certain parameter. The new method 
solves this problem by changing the transmission priority and 
yields, with one embedded file, its top performance for all 
rates. 
The encoding algorithms can be stopped at any compressed 
file size or let run until the compressed file is a representation 
of a nearly lossless image. We say nearly lossless because the 
compression may not be reversible, as the wavelet transform 
filters, chosen for lossy coding, have noninteger tap weights 
and produce noninteger transform coefficients, which are trun- 
cated to finite precision. For perfectly reversible compression, 
one must use an integer multiresolution transform, such as the 
S+P transform introduced in [14], which yields excellent re- 
versible compression results when used with the new extended 
EZW techniques. 
1051-8215/96$05.00 0 1996 IEEE 
I 
244 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL 6, NO 3, JUNE 1996 
This paper is organized as follows. The next section, 
Section 11, describes an embedded coding or progressive 
transmission scheme that prioritizes the code bits according 
to their reduction in distortion. Section 111 explains the 
principles of partial ordering by coefficient magnitude and 
ordered bit plane transmission, which suggest a basis for 
an efficient coding method. The set partitioning sorting 
procedure and spatial orientation trees (called zerotrees 
previously) are detailed in Sections IV and V, respectively. 
Using the principles set forth in the previous sections, 
the coding and decoding algorithms are fully described in 
Section VI. In Section VII, rate, distortion, and execution 
time results are reported on the operation of the coding 
algorithm on test images and the decoding algorithm on 
the resultant compressed files. The figures on rate are 
calculated from actual compressed file sizes and on mean 
squared error or PSNR from the reconstructed images given 
by the decoding algorithm. Some reconstmcted images are 
also displayed. These results are put into perspective by 
comparison to previous work. The conclusion of the paper 
is in Section VIII. 
11. PROGRESSIVE IMAGE TRANSMISSION 
We assume that the original image is defined by a set of pixel 
values P ~ , ~ ,  where (2, j )  is the pixel coordinate. To simplify 
the notation we represent two-dimensional (2-D) arrays with 
bold letters. The coding is actually done to the array 
c = R(P) (1) 
where R( .) represents a unitary hierarchical subband transfor- 
mation (e.g., [4]). The 2-D array c has the same dimensions 
of p, and each element c , , ~  is called transform coefjcient 
at coordinate (2, j ) .  For the purpose of coding, we assume 
that each c , , ~  is represented with a fixed-point binary format, 
with a small number of bits-typically 16 or less-and c m  
be treated as an integer. 
In a progressive transmission scheme, the decoder initially 
sets the reconstruction vector E to zero and updates its com- 
ponents according to the coded message. After receiving the 
value (approximate or exact) of some coefficients, the decoder 
can obtain a reconstructed image 
p = n-’(e). (2) 
A major objective in a progressive transmission scheme is 
to select the most important information-which yields the 
largest distortion reduction-to be transmitted first. For this 
selection, we use the mean squared-error (MSE) distortion 
measure 
(3) 
where N is the number of image pixels. Furthermore, we use 
the fact that the Euclidean norm is invariant to the unitary 
BIT ROW 
sim I ~ I ~ I ~ I ~ I ~ I ~ I ~ I ~ I ~ I ~ I ~ I ~ I ~ I ~ I s ~  
msb 
isb 
Fig 1. Binary representation of the magnitude-ordered coefficients 
transformation 0, i.e., 
(4) 
From (4) it is clear that if the exact value of the transform 
coefficient ~ , j  is sent to the decoder, then the MSE decreases 
by I c ~ , ~ ~ ~ / N .  This means that the coefficients with larger 
magnitude should be transmitted first because they have a 
larger content of information.’ This corresponds to the pro-’ 
gressive transmission method proposed by DeVore et al. [3] .  
Extending their approach, we can see that the information 
in the value of I C , , ~ ~  can also be ranked according to its 
binary representation, and the most significant bits should be 
transmitted first. This idea is used, for example, in the bit-plane 
method for progressive transmission [2]. 
Following, we present a progressive transmission scheme 
that incorporates these two concepts: ordering the coefficients 
by magnitude and transmitting the most significant bits first. 
To simplify the exposition, we first assume that the ordering 
information is explicitly transmitted to the decoder. Later, we 
show a much more efficient method to code the ordering 
information. 
DI. TRANSMISSION OF THE COEFFICIENT VALUES 
Let us assume that the coefficients are ordered according 
to the minimum number of bits required for its magnitude 
binary representation, that is, ordered according to a one-to- 
one mapping rj : I H 12, such that 
Fig. 1 shows the schematic binary representation of a list 
of magnitude-ordered coefficients. Each column k in Fig. 1 
contains the bits of The bits in the top row indicate the 
sign of the coefficient. The rows are numbered from the bottom 
up, and the bits in the lowest row are the least significant. 
Now, let us assume that, besides the ordering information, 
the decoder also receives the numbers pn corresponding to 
the number of coefficients such that 2n 5 IC%, 1 < 2n+1. In 
the example of Fig. 1 we have p5 = 2, p4 = 2, p3 = 4, etc. 
Since the transformation R is unitary, all bits in a row have 
the same content of information, and the most effective order 
for progressive transmission is to sequentially send the bits 
in each row, as indicated by the arrows in Fig. 1. Note that, 
‘Here the term infonation is used to indicate how much the distortion :an 
decrease after receiving that p“ of the coded message 
SAID AND PEARLMAN: IMAGE CODEC BASED ON SET PARTITIONING IN HIERARCHICAL TREES 245 
because the coefficients are in decreasing order of magnitude, 
the Ileading “0” bits and the first “1” of any column do not 
need to be transmitted, since they can be inferred from pn  
and the ordering. 
The progressive transmission method outlined above can be 
implemented with the following algorithm to be used by the 
encoder. 
Al‘gorithm I: 
output n = Llog,(max(;, j , {  le;, I})] to the decoder; 
output pn,  followed by the pixel coordinates ~ ( k )  and 
sign of each of the pn coefficients such that 2” 5 
output the nth most significant bit of all the coefficients 
with I 2 2n+1 (i.e., those that had their coordinates 
transmitted in previous sorting passes), in the same order 
used to send the coordinates (refinement pass); 
decrement n by one, and go to Step 2). 
< zn+l (sorting pass); 
The algorithm stops at the desired rate or distortion. Nor- 
mally, good quality images can be recovered after a relatively 
small fraction of the pixel coordinates are transmitted. 
The h c t  that this coding algorithm uses uniform scalar 
quantization may give the impression that it must be much 
inferior to other methods that use nonuniform and/or vector 
quantization. However, this is not the case: the ordering infor- 
mation makes this simple quantization method very efficient. 
On the other hand, a large fraction of the “bit-budget’’ is spent 
in the sorting pass, and it is there that the sophisticated coding 
methods are needed. 
IV. SET PARTITIONING SORTING ALGORITHM 
One of the main features of the proposed coding method is 
that the ordering data is not explicitly transmitted. Instead, it 
is balsed on the fact that the execution path of any algorithm 
is defined by the results of the comparisons on its branching 
points. So, if the encoder and decoder have the same sort- 
ing algorithm, then the decoder can duplicate the encoder’s 
execution path if it receives the results of the magnitude 
comparisons, and the ordering information can be recovered 
from the execution path. 
One important fact used in the design of the sorting algo- 
rithm is that we do not need to sort all coefficients. Actually, 
we need an algorithm that simply selects the coefficients such 
that 2n 5 I C , , ~ ~  < 2n+1, with n decremented in each pass. 
Given n, if I C , , ~ I  2 2” then we say that a coefficient is 
significant; otherwise it is called insigniJicant. 
The sorting algorithm flivides the set of pixels into parti- 
tioning subsets 7, and performs the magnitude test 
If thle decoder receives a “no’’ to that answer (the subset is 
insignificant), then it knows that all coefficients in 7, are 
insignificant. If the answer is “yes” (the subset is significant), 
then a certain rule shared by the encoder and the decoder 
is used to partition 7, into new subsets 7,,1, and the 
significance test is then applied to the new subsets. This set 
division process continues until the magnitude test is done to 
all single coordinate significant subsets in order to identify 
each significant coefficient. 
To reduce the number of magnitude comparisons (message 
bits) we define a set partitioning rule that uses an expected 
ordering in the hierarchy defined by the subband pyramid. The 
objective is to create new partitions such that subsets expected 
to be insignificant contain a large number of elements, and 
subsets expected to be significant contain only one element. 
To make clear the relationship between magnitude compar- 
isons and message bits, we use the function 
to indicate the significance of a set of coordinates 7.  To 
simplify the notation of single pixel sets, we write Sn({ ( i ,  j ) } )  
as Sn(z, j). 
V. SPATIAL ORIENTATION TREES 
Normally, most of an image’s energy is concentrated in 
the low frequency components. Consequently, the variance 
decreases as we move from the highest to the lowest levels 
of the subband pyramid. Furthermore, it has been observed 
that there is a spatial self-similarity between subbands, and 
the coefficients are expected to be better magnitude-ordered 
if we move downward in the pyramid following the same 
spatial orientation. [Note the mild requirements for ordering 
in (5). ]  For instance, large low-activity areas are expected 
to be identified in the highest levels of the pyramid, and 
they are replicated in the lower levels at the same spatial 
locations. 
A tree structure, called spatial orientation tree, naturally 
defines the spatial relationship on the hierarchical pyramid. 
Fig. 2 shows how our spatial orientation tree is defined in 
a pyramid constructed with recursive four-subband splitting. 
Each node of the tree corresponds to a pixel and is identified 
by the pixel coordinate. Its direct descendants (offspring) 
correspond to the pixels of the same spatial orientation in the 
next finer level of the pyramid. The tree is defined in such a 
way that each node has either no offspring (the leaves) or four 
offspring, which always form a group of 2 x 2 adjacent pixels. 
In Fig. 2, the arrows are oriented from the parent node to its 
four offspring. The pixels in the highest level of the pyramid 
are the tree roots and are also grouped in 2 x 2 adjacent pixels. 
However, their offspring branching rule is different, and in 
each group, one of them (indicated by the star in Fig. 2) has 
no descendants. 
The following sets of coordinates are used to present the 
new coding method: 
O(i,  j ) :  set of coordinates of all offspring of node ( 2 ,  j); 
D(i ,  j ) :  set of coordinates of all descendants of the node 
3-1: set of coordinates of all spatial orientation tree roots 
(i, d; 
(nodes in the highest pyramid level); 
C ( i ,  j )  = D(i ,  j )  - O(i ,  j ) .  
246 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL 6, NO 3, JUNE 1996 
Fig. 2. 
tree 
Examples of parent-offspring dependencies in the spatial-orientation 
For instance, except at the highest and lowest pyramid 
levels, we have 
O(2, j )  ={(ai, 2 j ) ,  (a i ,  2 j  + 1); (ai + 1, 2 j ) ,  
(a i  + 1, z j  + I)}. (8) 
We use parts of the spatial orientation trees as the partition- 
ing subsets in the sorting algorithm. The set partitionmg rules 
are simply the following. 
I)  The initial partition is formed with the sets ((2, 3 ) )  and 
D(z, J ) ,  for all ( 2 ,  J )  E X. 
2) If D(z ,  J )  is significant, then it i s  partitioned into C(z, J )  
plus the four single-element sets with ( k ,  I )  E O(z, 2 ) .  
3) If C(z, J )  is significant, then it is partitioned into the 
four sets D ( k ,  I ) ,  with ( k ,  I )  E O(z, J ) .  
VI. CODING ALGORITHM 
Since the order in which the subsets are tested for signif- 
icance is important, in a practical implementation the signif- 
icance information is stored in three ordered lists, called list 
of insignificant sets (LIS), list of insignificant pixels (LIP), 
and list of significant pixels (LSP). In all lists each entry is 
identified by a coordinate (2, j ) ,  which in the LIP and LSP 
represents individual pixels, and in the LIS represents either 
the set D(z, J )  or C ( i ,  j ) .  To differentiate between them, we 
say that a LIS entry is of type A if it represents D(z; j), and 
of type B if it represents L(i, j ) .  
During the sorting pass (see Algorithm I), the pixels in 
the LIP-which were insignificant in the previous pass-are 
tested, and those that become significant are moved to the 
LSP. Similarly, sets are sequentially evaluated following the 
LIS order, and when a set is found to be significant it is 
removed from the list and partitioned. The new subsets with 
more than one element are added back to the LIS, while the 
single-coordinate sets are added to the end of the LIP or the 
LSP, depending whether they are insignificant or significant, 
respectively. The LSP contains the coordinates of the pixels 
that are visited in the refinement pass. 
Below we present the new encoding algorithm in its entirety. 
It is essentially equal to Algorithm I, but uses the set- 
partitioning approach in its sorting pass. 
Algorithm 11: 
1) Initialization: output n = llog, (max(,, 3 ) {  I C , , ~  I } ) ] ;  
set the LSP as an empty list, and add the coordinates 
(i, j )  E X to the LIP, and only those with descendants 
also to the LIS, as type A entries. 
2.1) for each entry (2, J )  in the LIP do: 
2) Sorting Pass: 
‘2.1.1) output Sn(2, J ) ;  
2.1.2) if S,(z, j )  = 1 then move (2, j )  to the LSP 
and output the sign of et, ; 
2.2) for each entry ( i ,  j) in the LIS do: 
2.2.1) if the entry is of type A then 
if Sn(D(z,  1)) = 1 then 
output Sn(D(2, J ) ) ;  
* for each ( k ,  I )  E O(z, J )  do: 
output S n ( k ,  1); 
if Sn(k, I )  = 1 then add ( k ,  E )  to the 
if S,(k, 1 )  = 0 then add ( I C ,  I )  to the 
* if C( i ,  j )  # 0 then move (i, j )  to the 
end of the LIS, as an entry of type B, 
and go to Step 2.2.2); otherwise, remove 
entry ( i ,  j )  from the LIS; 
LSP and output the sign of c k , ~ ;  
end of the LIP; 
2.2.2) if the entry is of type B then 
output Sn(L(2, J ) ) ;  
if Sn(L(z, j ) )  = 1 then 
* add each ( k ,  I )  E O(z, j )  to the end of 
* remove (i, j) from the LIS. 
the LIS as an entry of type A; 
3) Refinement Pass: for each entry (2, j )  in the LSP, 
except those included in the last sorting pass (i.e., with 
same n), output the nth most significant bit of le,, 1 ;  
4) Quantization-Step Update: decrement n by 1 and go 
to Step 2. 
One important characteristic of the algorithm is that the 
entries added to the end of the LIS in Step 2.2) are evaluated 
before that same sorting pass ends. So, when we say “for each 
entry in the LIS” we also mean those that are being added to its 
end. With Algorithm 11, the rate can be precisely controlled be- 
cause the transmitted information is formed of single bits. The 
encoder can also use the property in (4) to estimate the progres- 
sive distortion reduction and stop at a desired distortion value. 
Note that in Algorithm 11, all branching conditions based on 
the significance data Sn-which can only be calculated with 
the knowledge of  are output by the encoder. Thus, to 
obtain the desired decoder’s algorithm, which duplicates the 
encoder’s execution path as it sorts the significant coefficients, 
we simply have to replace the words output by input in 
Algorithm 11. Comparing the algorithm above to Algorithm 
I, we can see that the ordering information ~ ( k )  is recovered 
when the coordinates of the significant coefficients are added 
to the end of the LSP; that is, the coefficients pointed by 
the coordinates in the LSP are sorted as in (5). But note that 
SAID AND PEARLMAN: IMAGE CODEC BASED ON SET PARTITIONING IN HIERARCHICAL TREES 
0.2 0.3 0.4 0.5 0.6 
Fig. 3. Comparative evaluation of the new coding method. 
whenever the decoder inputs data, its three control lists (LIS, 
LIP, and LSP) are identical to the ones used by the encoder at 
the moment it outputs that data, which means that the decoder 
indeed recovers the ordering from the execution path. It is 
easy to see that with this scheme, coding and decoding have 
the isme computational complexity. 
An additional task done by decoder is to update the re- 
constructed image. For the value of n when a coordinate is 
moved to the LSP, it is known that 2n 5 I c , , ~ ~  < 2n+1. 
So, the decoder uses that information, plus the sign bit that 
is input just after the insertion in the LSP, to set = 
f1.5 x 2n. Similarly, during the refinement pass, the decoder 
adds or subtracts Zn- l  to & j  when it inputs the bits of the 
binary representation of let, I. In this manner, the distortion 
gradually decreases during both the sorting and refinement 
passles. 
As with any other coding method, the efficiency of 
Algorithm I1 can be improved by entropy-coding its output, 
but at the expense of a larger coding/decoding time. Practical 
experiments have shown that normally there is little to 
be gained by entropy-coding the coefficient signs or the 
bits put out during the refinement pass. On the other 
handl, the significance values are not equally probable, 
and there is a statistical dependence between Sn(i ,  j )  and 
241 
Sn[D(i,  j ) ]  and also between the significance of adjacent 
pixels. 
We exploited this dependence using the adaptive arithmetic 
coding algorithm of Witten et aZ. [7]. To increase the coding 
efficiency, groups of 2 x 2 coordinates were kept together in 
the lists, and their significance values were coded as a single 
symbol by the arithmetic coding algorithm. Since the decoder 
only needs to know the transition from insignificant to signif- 
icant (the inverse is impossible), the amount of information 
that needs to be coded changes according to the number m of 
insignificant pixels in that group, and in each case it can be 
conveyed by an entropy-coding alphabet with 2m symbols. 
With arithmetic coding it is straightforward to use several 
adaptive models [7] ,  each with 2m symbols, m E (1, 2, 3, 4}, 
to code the information in a group of four pixels. 
By coding the significance information together, the average 
bit rate corresponds to an mth order entropy. At the same 
time, by using different models for the different number of 
insignificant pixels, each adaptive model contains probabilities 
conditioned to the fact that a certain number of adjacent 
pixels are significant or insignificant. This way the dependence 
between magnitudes of adjacent pixels is fully exploited. The 
scheme above was also used to code the significance of trees 
rooted in groups of 2 x 2 pixels. 
248 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL 6, NO 3, JUNE 1996 
rate 
(bpp) 
(c)  ( 4  
Fig. 4. 
rate = 0.25 bpp, PSNR = 34.1 dB, (d) rate = 0.15 bpp, PSNR = 31.9 dB. 
Images obtamed with the arithmetic code version of the new coding method. (a) Original LENA, (b) rate = 0.5 bpp, PSNR = 37.2 dB, (c)  
TABLE I edges. It is important to observe that the bit rates are not 
binary arithmetic 
uncoded coded 
I code 1 decode 1 code 1 decode I 
entropy estimates-they were calculated from the actual size 
of the compressed files. Furthermore, by using the progressive 
transmission ability, the sets of distortions are obtained from- 
the same $le, that is, the decoder read the first bytes of the 
file (up to the desired rate), calculated the inverse subband 
transformation, and then compared the recovered image with 
the original. The distortion is measured by the peak signal to 
noise ratio 
With arithmetic entropy-coding it is still possible to produce 
a coded file with the exact code rate and possibly a few unused 
bits to pad the file to the desired size. 
VII. NUMERICAL RESULTS 
The following results were obtained with monochrome, 8 
bpp, 512 x 512 images. Practical tests have shown that the 
pyramid transformation does not have to be exactly unitary, 
so we used five-level pyramids constructed with the 9/7-tap 
filters of [5] ,  and using a “reflection” extension at the image 
(9) 
where MSE denotes the mean squared-error between the 
original and reconstructed images. 
Results are obtained both with and without entropy-coding 
the bits put out with Algorithm 11. We call the version without 
entropy coding binary-uncoded. In Fig. 3 are plotted the PSNR 
versus rate obtained for the luminance (Y) component of 
LENA both for binary uncoded and entropy-coded using 
arithmetic code. Also in Fig. 3, the same is plotted for the 
luminance image GOLDHILL. The numerical results with 
SAID AND PEARLMAN: IMAGE CODEC BASED ON SET PARTITIONING IN HIERARCHICAL TREES 249 
arithmetic coding surpass in almost all respects the best 
efforts previously reported, despite their sophisticated and 
comlputationally complex algorithms (e.g., 151, [81-[101, [131, 
[151). Even the numbers obtained with the binary uncoded 
versions are superior to those in all these schemes, except 
possibly the arithmetic and entropy constrained trellis quan- 
tizal.ion (ACTCQ) method in [l l] .  PSNR versus rate points 
for competitive schemes, including the latter one, are also 
plotted in Fig. 3. The new results also surpass those in the 
original EZW [l]  and are comparable to those for extended 
EZW in [6], which along with ACTCQ rely on arithmetic 
codmg. The binary uncoded figures are only 0.3-0.6 dB lower 
in PSNR than the corresponding ones of the arithmetic coded 
versions, showing the efficiency of the partial ordering and 
set partitioning procedures. If one does not have access to 
the best CPU’s and wishes to achieve the fastest execution, 
one could opt to omit arithmetic coding and suffer little 
consequence in PSNR degradation. Intermediary results can 
be obtained with, for example, Huffman entropy-coding. A 
recent work [12], which reports similar performance to our 
arithmetic coded ones at higher rates, uses arithmetic and 
trellis coded quantization (ACTCQ) with classification in 
wavelet subbands. However, at rates below about 0.5 bpp, 
ACTCQ is not as efficient and classification overhead is not 
insignificant. 
Note in Fig. 3 that in both PSNR curves for the image 
LENA there is an almost imperceptible “dip” near 0.7 bpp. 
It occurs when a sorting pass begins, or eqaivalently, a new 
bit-plane begins to be coded, and is due to a discontinuity 
in the slope of the rate x distortion curve. In previous EZW 
versions [l], [6], this “dip” is much more pronounced, of up 
to 1 dB PSNR, meaning that their embedded files did not yield 
their best results for all rates. Fig. 3 shows that the new version 
does not present the same problem. 
In Fig. 4, the original images are shown along with their cor- 
responding reconstructions by our method (arithmetic coded 
only) at 0.5,0.25, and 0.15 bpp. There are no objectionable ar- 
tifacts, such as the blocking prevalent in JPEG-coded images, 
and even the lowest rate images show good visual quality. 
Table I shows the corresponding CPU times, excluding the 
time spent in the image transformation, for coding and decod- 
ing LENA. The pyramid transformation time was 0.2 s in an 
IBM RS/6000 workstation (model 590, which is particularly 
efficient for floating-point operations). The programs were not 
optimized to a commercial application level, and these times 
are shown just to give an indication of the method’s speed. 
The ratio between the coding/decoding times of the different 
versions can change for other CPU’s, with a larger speed 
advantage for the binary-uncoded version. 
VIII. SUMMARY AND CONCLUSIONS 
We have presented an algorithm that operates through set 
partitioning in hierarchical trees (SPIHT) and accomplishes 
completely embedded coding. This SPIHT algorithm uses the 
principles of partial ordering by magnitude, set partitioning 
by significance of magnitudes with respect to a sequence of 
octavely decreasing thresholds, brdered bit plane transmission, 
and self-similarity across scale in an image wavelet trans- 
form. The realization of these principles in matched coding 
and decoding algorithms is a new one and is shown to be 
more effective than in previous implementations of EZW 
coding. The image coding results in most cases surpass those 
reported previously on the same images, which use much 
more complex algorithms and do not possess the embedded 
coding property and precise rate control. The software and 
documentation, which are copyrighted and under patent ap- 
plications, may be accessed in the Internet site with URL 
h t t p :  / / i p l  . r p i  .edu/SPIHTorobtainedbyanonymous 
ftp to i p l  . r p i  . edu with the path pub/EW-Code in the 
compressed archive file code t ree .  t a r .  gz. (The file must 
be decompressed with the command gunzip and exploded 
with the command “ ta r  xvf”; the instructions are in the 
file code t ree .  doc.) We feel that the results of this coding 
algorithm with its embedded code and fast execution are so 
impressive that it is a serious candidate for standardization in 
future image compression systems. 
REFERENCES 
J. M. Shapiro, “Embedded image coding using zerotrees of wavelets 
coefficients,” IEEE Trans. Signal Processing, vol. 41, pp. 3445-3462, 
Dec. 1993. 
M. Rabbani and P. W. Jones, Digital Image Compression Techniques. 
Bellingham, WA: SPIE Opt. Eng. Press, 1991. 
R. A. DeVore, B. Jawerth, and B. J. Lucier, “Image compression through 
wavelet transform coding,” IEEE Trans. Inform. Theory, vol. 38, pp, 
719-746, Mar. 1992. 
E. H. Adelson, E. Simoncelli, and R. Hingorani, “Orthogonal pyramid 
transforms for image coding,” in Proc. SPIE, vol. 845, Visual Commun. 
and Image Proc. 11, Cambridge, MA, Oct. 1987, pp. 50-58 
M. Antonini, M. Barlaud, P. Mathieu, and I. Daubechies, “Image coding 
using wavelet transform,” IEEE Trans. Imam Processing, vol. 1, pp. _ _  
205-220, Apr. 1992. 
A. Said and W. A. Pearlman, “Image compression using the spatial- 
orientation tree,” in IEEE lnt. Symp. Circuits-and Systems,-Chicago, IL, 
May 1993, pp. 279-282. 
I. H. Witten, R. M. Neal, and J. G. Cleary, “Arithmetic coding for data 
compression,” Commun. ACM, vol. 30, pp. 520-540, June 1987. 
P. Sriram and M. W. Marcellin, “Wavelet coding of images using 
trellis coded quantization,” in SPIE Con$ Visual lnform. Process., 
Orlando, FL, Apr. 1992, pp. 238-247; also “Image coding using wavelet 
transforms and entropy-constrained trellis quantization,” IEEE Trans. 
Image Processing, vol. 4, pp. 725-733, June 1995. 
Y. H. Kim and J. W. Modestino, “Adaptive entropy coded subband 
coding of images,” IEEE Trans. Image Processing, vol. 1, pp. 3148,  
Jan. 1992. 
N. Tanabe and N. Farvardin, “Subband image coding using entropy- 
constrained quantization over noisy channels,” IEEE J. Select. Areas 
Commun., vol. 10, pp. 926-943, June 1992. 
R. L. Joshi, V. J. Crump, and T. R. Fischer, “Image subband coding 
using arithmetic and trellis coded quantization,” IEEE Trans. Circuits 
Syst. Video Technol., vol. 5, pp. 515-523, Dec. 1995. 
R. L. Joshi, T. R. Fischer, and R. H. Bamberger, “Optimum classification 
in subband coding of images,” in Proc. 1994 IEEE Int. Con$ Image 
Processing, Austin, TX, Nov. 1994, vol. 11, pp. 883-887. 
J. H. Kasner and M. W. Marcellin, “Adaptive wavelet coding of images,” 
in Proc. I994 IEEE Con$ Image Processing, Austin, TX, Nov. 1994, 
vol. 3, pp. 358-362. 
A. Said and W. A. Pearlman, “Reversible image compression via 
multiresolution representation and predictive coding,” in Proc. SPIE 
Con$ Visual Communications and Image Processing ’93, Cambridge, 
MA, Nov. 1993, Proc. SPIE 2094, pp. 664-674. 
D. P. de Garrido, W. A. Pearlman, and W. A. Finamore, “A clustering 
algorithm for entropy-constrained vector quantizer design with appli- 
cations in coding image pyramids,” IEEE Trans. Circuits Syst. Video 
Technol., vol. 5 ,  pp. 83-95, Apr. 1995. 
250 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL 6, NO 3, JUNE 1996 
Amir Said (S’90-M’93) received the B.S. and 
M S degrees in electncal engineenng from the 
State University of Campinas, Brazil, in 1985 and 
1988, respectlvely In 1994, he received the Ph D 
degree in computer and systems engineenng from 
Rensselaer Polytechnic Insktute, Troy, NY 
After a year at IBM Brazil, in 1988 he joined 
the School of Electncal Engineenng at the State 
University of Campinas, Brazil, as a Lecturer, and 
in 1994, he rejoined as an Assistant Professor. 
HLs research interests include image compression, 
William A. Pearlman (S’61-M’64-SM’84) 
received the S.B and SM.  degrees in electrical 
engineenng from the Massachusetts Institute of 
Technology, Cambridge, MA, in 1963 and the 
Ph.D degree in electrical engineenng from Stanford 
Umversity, Stanford, CA, in 1974 
Between 1963 and 1974 he was employed as 
an engineer and consultant by Lockheed Missiles 
and Space Company in Sunnyvale, CA, and as an 
engineer by GTE-Sylvania in Mountan View, CA. 
He left mdustry in 1974 to become an Assistant 
channel coding and modulation, signal processing, and optimization. 
from Rensselaer Polytechnic Institute 
Professor in the Department of Electrical and Computer Engineering at 
the University of Wisconsin-Madison In 1979 he joined the Electrical, 
Computer, and Systems Engineering Department at Rensselaer Polytechnic 
Instltute, where he is currently Professor During the 1985-1986 academic 
year, and agan for the spnng semester in 1993, he was on sabbatlcal leave 
as Visiting Professor and Lady Davis Scholar in the Department of Electrical 
Engineenng at the Techmon-Israel Institute of Technology, Hafa, Israel He 
also held a Visiting Professor Char at Delft University of Technology in 
Delft, The Netherlands in January and February 1993 and has been an IBM 
Visiting Scientist in the IBM-ko Scientific Center in Rio de Janeiro, Brazil in 
1988 ETIS research interests are in informabon theory, source coding theory, 
image, video, and audio coding, and image processing He has authored or 
co-authored about 100 publications in these fields 
Dr. Pearlman has served on the steering c o m t t e e  of SPJE’s Visual 
Communications and Image Processing Conference since its inception in 1986 
and in 1989 was the conference charman he has also served on technical 
program c o m t t e e s  of several IEEE conferences He is currently Associate 
Ebtor of Coding for the E E E  TRANSACTIONS ON IMAGE PROCESSING He is 
a Fellow of SPE. 
Dr S a d  was awarded the Allen B DuMont pnze for academc achievement 
I 

