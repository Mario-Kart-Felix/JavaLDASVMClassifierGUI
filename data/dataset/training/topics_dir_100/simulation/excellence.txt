MARCH/APRIL 2004 Copublished by the IEEE CS and the AIP        1521-9615/04/$20.00 © 2004 IEEE 57
PERSPECTIVESP E R S P E C T I V E S  I N  CO M P U TAT I O N A L  SC I E N C E
Stand-alone modeling can serve the
first purpose; the other two goals
need a full integration of the model-
ing effort into a scientific or engi-
neering program. 
Some excellent work, much of it re-
lated to the US Department of En-
ergy’s laboratories, is reviewed. Some
less happy stories are recounted. 
In the past, some of the most im-
pressive work has involved complexity
and chaos. Prediction in a complex
world therefore requires a first princi-
ples understanding based on the inter-
section of theory, experiment, and sim-
ulation.
I work within a Department of
Energy-supported research pro-
gram at the University of Chicago
called the ASCI/Alliances Center
for Astrophysical Thermonuclear
Flashes. The main goal of ASCI, the
alliance for scientific computing, is
to gain knowledge and experience
relevant for the construction of
large-scale computer simulations,
thus supporting computer research
on complex systems and thereby
helping the DOE maintain a stock-
pile of nuclear weapons. My inter-
est here is to provide an overview of
the art and science of scientific
computer simulation.
The Best: Great Examples 
of Scientific Computing 
in the Heroic Period
To begin: I am happy to tell you that
the Department of Energy’s scientific
laboratories have supported some of
the very best scientific computing ever
done. Indeed, in some sense they in-
vented scientific computing. In this
section, I describe some of the best ex-
amples of scientific computing, and
you will see a preponderance of De-
partment of Energy-supported work. 
In the Monte Carlo method, a ran-
dom-number generator is used to per-
form deterministic calculations.1 The
Rosenbluths, the Tellers, Ulam, and
Metropolis played major roles in
putting the method together and ap-
plying it to the “Metropolis algorithm”
for calculating the properties of systems
in thermodynamic equilibrium. This
paper was cited in Computing in Science
& Engineering (January/February 2000)
as being among the top 10 algorithms
having the “greatest influence on the
development and practice of science
and engineering in the 20th century.”
When this calculation was first per-
formed, the idea that a numerical
method could be built on the fake, con-
structed, randomness of a typical com-
puter’s random-number generator
seemed almost magical. Indeed, in a fa-
mous incident, Alston Householder said
he would stop the computational work at
Oak Ridge National Laboratory while
he waited to be convinced that random-
number generators could work.2
Today, we have a fairly clear explana-
tion of why the Monte Carlo method
works—namely, that for expectation
values, the rapid increase in the number
of configurations with energy is com-
pensated for by the rapid decrease of the
Boltzmann factor, leaving a narrow win-
dow of states that actually have to be
explored. Computers’ random-number
generators are now fully accepted and
reasonably well understood. The most
powerful and surprising feature that re-
mains from this early calculation is the
insight that using an inherently proba-
bilistic algorithm can lead to an enor-
mous compression in the number of
computational steps required. 
Monte Carlo calculations use a ran-
dom-number generator to simulate
something else, often a system in
thermodynamic equilibrium. By con-
trast, the molecular dynamics approach
uses the computer to solve Newton’s
equations of motion and follow the tra-
jectories of all the particles in the system.
Fermi, Pasta, and Ulam performed one
of the first calculations of this kind.
They studied a linear chain of atoms
coupled with anharmomic forces and
driven by an initial large tweaking in one
mode.3 They then used a simulation to
follow the system as the 128 atoms jig-
gled about and transferred energy from
one mode to another. After a time, the
energy became satisfactorily mixed (as
EXCELLENCE IN COMPUTER SIMULATION
By Leo P. Kadanoff
E XCELLENT COMPUTER SIMULATIONS ARE DONE FOR A PUR-POSE. THE MOST VALID PURPOSES ARE TO EXPLORE UN-
CHARTED TERRITORY, TO RESOLVE A WELL-POSED SCIENTIFIC OR
TECHNICAL QUESTION, OR TO MAKE A GOOD DESIGN CHOICE.
58 COMPUTING IN SCIENCE & ENGINEERING
expected), but they kept the computer
going. At one magic moment, the sys-
tem returned very nearly to its original
starting point, once again having almost
all its energy in one mode. These scien-
tists had discovered an almost integrable
system—experimentally! By doing ex-
ploratory calculations in heretofore un-
explored territory, they stumbled across
some wonderful new science.
Berni Alder and his collaborator
Tom Wainright, working at Livermore
National Laboratory, displayed an
amazing mastery of the molecular dy-
namics method. They were involved in
not one but two great discoveries.
They started by looking at the motion
of hard spheres bouncing off one an-
other. To everyone’s amazement, in
spite of the purely repulsive interac-
tions, they saw a phase transition from
a fluid state into a solid one.4 Surprise
number two was that the motion of
these hard spheres—indeed, of any col-
liding fluid particles—engenders per-
sisting correlations.5 These “long time
tails” remained a perplexing mystery
for a long time, but now they are pretty
well understood to be a consequence of
the hydrodynamic motion of the fluid
as it flows past its own molecules.
Chaotic behavior, now characterized as
sensitive dependence on initial conditions
(or the “butterfly effect”) was discovered
“accidentally” by Edward Lorenz6 while
working with an early and very primitive
program for solving linked sets of ordi-
nary differential equations. It is said that
one day the computer prematurely
aborted a half-completed and recorded
run. Lorenz went back and punched in
the numbers corresponding (at least in
their first few digits) to the results the
computer used on the previous day. To
his amazement and our subsequent edifi-
cation, the resulting run was entirely dif-
ferent. The high-order digits mattered—
a lot—and chaos was discovered.
My last “great” example comes from
Los Alamos National Laboratory.7
Mitchell Feigenbaum was using a not-
very-fancy desk calculator to study the
properties of models previously investi-
gated by Ulam. The calculator took a
number, squared it, formed a linear
combination from the square and some
fixed coefficients, and generated a new
number. Feigenbaum and his trusty
computer carried on that simple process
through many steps, and patterns
emerged—wonderful, unexpected pat-
terns—that showed how systems even
simpler than Lorenz’s could become
“just a little bit” chaotic. An exciting lit-
tle world unexpectedly opened up. 
In all these examples, the scientists in-
volved discovered and explored entirely
new pieces of science. In the Monte Carlo
case, the novelty was in using a new kind
of computer algorithm with new concep-
tualizations of possible computations. In
the other cases, a highly simplified model
combined with a new kind of hardware or
calculational technique permitted the first
scientific investigation of new domains of
physics. These researchers discovered
new scales of length or time: long-term
recurrence or long-range order. They
found new scientific ideas that were gen-
eral and applicable to a broad range of
systems. Subsequent experiment, theory,
and simulation has delved into each of
these ideas in great detail and taken them
much further.
Good Recent Examples
We may suspect that the heroic age has
passed. The nature of discoveries is
somewhat different now. Recent exam-
ples illustrate some of the best things
computational people are now doing. 
One of the best pieces of science
done in recent years is the discovery of
neutrino mass and neutrino oscillations.
The first hint of this major discovery
came from a discrepancy between the
flux of neutrinos from the Sun (mea-
sured by Ray Davis and others) and the
flux of solar reactions and activity pre-
dicted by computer models.8 For the
discrepancy to be taken seriously, one
had to believe in the accuracy and reli-
ability of the solar models. It was per-
suasive that extremely competent peo-
ple had conducted the experiments, and
believed in their results. Another per-
suasive factor was an observational pro-
gram seeking additional tests for the
models. These models were verified by
comparing them with seismographic
data recording wave activity within the
sun. The seismographic predictions of
the models fit the observations. With
this increased credibility of the models,
the original discrepancy in neutrino
fluxes was seen to be a serious problem.
Something had to give. Eventually, the
part of the picture concerned with ele-
mentary particle physics had to be
modified. The then-accepted theory as-
sumed that neutrinos have zero mass,
but this idea was abandoned to fit the
solar data. Later observations have sup-
ported this change.
This big discovery was made by the
experimentalists who observed and
counted neutrinos. Major credit also
went to the theorists who held the sci-
entific program together, particularly
John Bahcall. The computer-model
builders were a third, and quite essen-
tial, part of the enterprise. All together,
these investigations produced a major
unexpected advance in our understand-
ing of the fundamentals of the universe.
Another interesting recent example
involves a problem called single-bubble
sonoluminescence. This phrase describes
a situation in which a resonant acoustic
field forms a bubble and excites it
strongly enough so that the bubble
emits light and becomes visible.9 Here,
once again, experimentalists made the
major discoveries and started a pro-
P E R S P E C T I V E S  I N  C O M P U T A T I O N A L  S C I E N C E
MARCH/APRIL 2004 59
gram of work in the area. By 1997,
they set out the values of some of the
most important parameters describing
the system. As this understanding of
the subject developed, simulations
were performed and closely tied to
theoretical and experimental work.
Thanks to these simulations, very
many of the proposed steps and ad-
vances were looked at, checked, and
more deeply understood. 
In this example, simulations played a
major integrating role but never led the
advances in the field. I would judge that
this is the common role for simulation
in “tabletop” science. I expect this ex-
ample will serve as a paradigm for fu-
ture investigations: a scientific or tech-
nical problem involving many branches
of science will be largely understood
through the interlinked efforts of many
investigators who often use simulations
to check their arguments.
My third example is drawn from
computer simulations cosmology of the
early universe.10 Our colleagues work-
ing in this area have a hard task. Exper-
iments are impossible. They can only
observe—never manipulate—their sys-
tem. They are much further away from
their system, in both space and time,
than are the solar physicists, so even
their observational data are quite lim-
ited in scope. For this reason, they must
be more dependent on simulations than
researchers in most other branches of
science. Cosmologists construct entire
universes—intended to be reasonably
realistic—within their computers. They
start from dark matter and baryons and
then observe the model bringing to-
gether clusters. Step by step, the com-
puters make objects on many scales,
down to the size of galaxies. But can
these constructed universes give real in-
sight into the processes involved? 
One might worry that the model-mak-
ing gives too much freedom, that simula-
tors will always be able to fit the known
facts with a wide variety of schemes. The
simulators disagree. Bertschinger states
that the “main use [of the calculations]
has been and continues to be the testing
of the viability of cosmological models of
structure formation.”10 The work takes
the theoretical conceptions of the field,
casts them into the form of specific mod-
els, and then runs them. Many models
simply blow up, yielding nothing sensi-
ble. The remaining ones give well-de-
fined results that can be analyzed to see
whether they agree with observations.
Many models fail at this stage.
In the long run, the simulators hope
to strain out all but the one correct phys-
ical model for the development process.
This filtering is a brave goal that the par-
ticipants believe they can achieve. I can-
not tell whether they’re being too opti-
mistic. At the moment, several different
models apparently work quite well: “Re-
cent high-resolutions simulations com-
pare remarkably well with many aspects
of the observed galaxy distribution.”10
In all three of these recent examples,
simulation’s role was to work with the-
ory, observation, and experiment—es-
sentially, to serve as a cross-check on the
other models and thereby increase the
investigators’ confidence that they un-
derstood phenomena that were not oth-
erwise open to observation. In the solar
neutrino example, simulations made the
investigators confident that they under-
stood solar behavior and thus could lo-
cate an error in previous assumptions
about neutrinos. In the sonolumines-
cence case, simulations were a necessary
part of putting together an intricate
puzzle. The early universe investigators
hope and expect to weed out incorrect
mechanisms and theories by carefully
testing their consequences and compar-
ing them with current observations
about the universe. In each case, simu-
lation works by being part of a carefully
constructed program of activity.
Not So Good: Optimization
of Enthusiasm and
Misjudgment
Recently, a provocative and controver-
sial experiment conducted at Oak Ridge
suggested that fusion was occurring in
deuterated acetone via a process involv-
ing resonance excitation of bubbles. The
reporting paper involved both experi-
mental work and computer simula-
tions.11 “[A] roughly ten-fold increase in
the external driving pressure was used in
the calculations” beyond the pressure di-
rectly produced by the experimental sit-
uation “to approximately account for the
effect of pressure intensification within
the imploding bubble clusters.” As a re-
sult, their “[h]ydrodynamic shock code
simulation supported the observed
New Feature Section: Perspectives in Computational Science
Computational science is beginning to play an important role in scientific research and development. Modern scientific re-search—originally grounded in experiment and its theoretical interpretation—is increasingly moving toward becoming a
triad of experiment, theory, and computer simulation. This is because the analysis of simulations of complicated systems can
be a portal to discovery of important but hitherto unnoticed simplifications and regularities. Computational tools play an im-
portant role in the design and testing of new engineering products, and simulation results form part of the basis for many pol-
icy decisions. Yet, computational science is not nearly as mature as experimental, theoretical, or engineering science. 
The editors of Computing in Science & Engineering have established a new feature section we call Perspectives in Compu-
tational Science to encourage consideration and discussion of the issues that our field must address on the road to matu-
rity. We will feature invited papers by senior scientists and engineers to share their perspectives on our field. The very posi-
tive response to the May/June 2002 article by Robert Laughlin, “The Physical Basis of Computability,” helped us realize the
potential utility of this new feature section. —Douglass Post, Associate Editor in Chief
60 COMPUTING IN SCIENCE & ENGINEERING
P E R S P E C T I V E S  I N  C O M P U T A T I O N A L  S C I E N C E
data.” It is remarkable that the referee
process for such a high-visibility paper
allowed an apparently uncontrolled ap-
proximation in a key step in the com-
puter calculation. Subsequent work12
seemed to disprove Oak Ridge’s experi-
mental result, but that is not the point.
Because of the “roughly ten-fold in-
crease,” the simulation was sufficiently
uncontrolled so that it neither supported
nor could refute the experiment. Nei-
ther the authors nor the editors should
have permitted it to be published.
This example makes one ask what
kind of quality control is appropriate for
a computer calculation used to check a
controversial experimental result. This
problem is broader than just one paper.
The whole early history of single-bub-
ble sonoluminescence required step-by-
step work to eliminate provocative but
incorrect mechanisms. For example, a
set of early experiments by Barber and
his colleagues13 reported very short
widths for the emitted pulse of light.
This short width opened the door to
novel mechanisms for explaining the
emitted light’s total intensity. Later de-
velopments suggested that the short
pulse width was a misstep by the exper-
imentalists. In contrast to the excellent
work in sonoluminescence in the post-
1997 period, this misstep led simulators
and theorists quite astray. A host of in-
correct speculations and mechanisms
ran through the field, intended to ex-
plain the “observed” behavior. 
Despite one essentially correct simu-
lation,14 the pre-1997 simulations15 did
almost nothing to weed out these incor-
rect discussions, undercutting the hope
that simulations might provide a good
tool for such a purpose. (Note that this
weeding was one of the main goals of the
astrophysical modeling.) Instead, the
speculations continued unhindered un-
til an experiment by Gomph and his col-
leagues16 showed that the pulse width
was much longer than previously be-
lieved, which implied a lower tempera-
ture for the emitting drop. After this, at-
tention turned away from the incorrect
mechanisms so that theory, experiment,
and simulation could produce a consen-
sus about what was actually happening.
(There were technical reasons for the
failures of the simulations. For example,
the earliest simulations17 used a zero-
viscosity code in which heat conduction
was also neglected. These codes under-
estimated the damping mechanisms
and hence produced a very strong
shock, which would, in the approxima-
tions used by the investigators, produce
an infinitely high temperature.18 Later
simulations by Vuong and Szeri14 cast
doubt on the relevance of shocks to the
observed behavior of sonolumines-
cence. However, the field did not turn
around until new experimental results
caught people’s attention.)
The examples of the Oak Ridge paper
and some of the earlier sonolumines-
cence simulations suggest that the mod-
els might have been directed toward the
wrong goals. Rather than being used in
the process of checking, critiquing, and
eliminating incorrect possibilities, they
were apparently used to support and ex-
emplify the presumptions of the scien-
tists involved. A program of modeling
should either elucidate new processes or
identify wrong directions.  Otherwise,
there is no point in performing it.
Another example, which might be
entirely mythical, involves a trans-
portation investment model reportedly
put together in Britain with the goal of
getting the best transportation system
while minimizing public spending.
The model involved a broad mix of
roads, rail, and public improvements;
the aim was an overall maximization of
benefits, taking into account public
spending and the value of time saved.
All costs and benefits were converted
into pounds, and an overall optimiza-
tion was sought and achieved.
The next step was to bring in an out-
side group of experts to study the
model’s recommendations and build a
plan for implementing them. This
group noticed several apparent anom-
alies. The strangest, according to the
story, was the elimination of all spend-
ing for improving pedestrian crossings.
This result was considered peculiar, es-
pecially since the value of pedestrian
time saved was included in the model. A
careful look explained how the conclu-
sion was reached: the decreased spend-
ing had the effect of increasing accidents
at the crossings. According to experi-
ence, and also the model, the major re-
sult would be increased deaths among
older pedestrians, thus spending on
pensions would be reduced. The model
counted this outcome as a benefit.
The government that paid for the
modeling was not amused. (An appar-
ently less mythical recent example con-
cerned an American cigarette company
and the Czech government. The gov-
ernment was advised to support ciga-
rette advertising since the early deaths
thereby caused would have a beneficial
effect on pension spending.  Apparently
no computer model was needed to
reach this conclusion.)
This outcome brings us to a moral:
The transportation study failed because
the modeling was done too mechani-
cally, without enough thinking about
either the actual processes going on or
the actual goals of the sponsors. The
modelers did not realize that the design
goals were actually multidimensional.
Modeling efforts should include theory
and common sense. The two examples
relating to bubbles have a different
moral: In those cases, the simulations,
both the several wrong ones and even
the essentially correct simulation of
Vuong and Szeri,14 did not effectively
MARCH/APRIL 2004 61
refute the incorrect experiments. In-
stead, the simulations were effectively
trumped by experiments, which the
community judged to be decisive.
Present Challenges
In this section, I shall describe some
work involving simulations in which I
have played some role. 
Convective Turbulence
In Rayleigh-Bénard flow, a fluid is
placed in a box, heated from below and
cooled from above. A parameter, called
the Rayleigh number gives a dimen-
sionless measure of the heating’s
strength: the higher the Rayleigh num-
ber, the more turbulent the system. To
compare with other turbulent systems,
one might say that the Rayleigh num-
ber is roughly the square of the
Reynolds number or the fourth power
of the Taylor-Reynolds number.
A little heating of the system from be-
low causes no motion of fluid. However,
with increased heating and increased
Rayleigh numbers, we see first motion
and then chaos. At Rayleigh numbers
above roughly 108, turbulent flows and
structures form (see Figure 1). As the
cartoon in Figure 2 shows, the heated
box contains many structures, including
plumes, waves, and jets.19 How far are
we from examining this experimental
behavior in computer simulations?
Good simulations exist, in both two
and three dimensions, but the three-di-
mensional simulations do not resolve
the structures seen in experiments,
which reliably reach Rayleigh numbers
as high as 1014. Simulations hardly go
beyond 1012 because of limitations in
resolution and computer time. Theory
suggests phase transitions—qualitative
changes in behavior—at roughly 108,
1011, and 1019. Theorists are unsure of
what will happen, and consider a large
range of possibilities. Simulations can-
not hope to reach directly into the do-
mains touched by theory and experi-
ment. Nonetheless, we are beginning to
learn how to use theoretical ideas to ex-
trapolate simulation results from lower
Rayleigh numbers to higher ones. The
simulations provide detailed informa-
tion to help us see what is really hap-
pening in much more detail than ex-
periments can currently provide. The
high Rayleigh number experiments’
data generation is limited by the design,
manufacture, and placement of delicate
and tiny temperature-measuring de-
vices and by the difficulty of assuring
uniform heat transfer into the cell.
One recent example is a simulation by
Marcus Brüggen and Christian Kalser
describing a hot bubble toward the cen-
ter of a galaxy (see Figure 3).20 Because
we cannot see into the galactic center,
this bubble can only be “observed” via
computer simulation. Nevertheless, the
authors appear confident that they have
caught some of the essential features of
heat transfer in this region.
Table 1 compares what we might gain
from experiment and what we might
gain from simulation. Clearly, both are
necessary. In the first four rows, experi-
ment does better because it runs longer
and with more extreme flows, more rep-
etitions, and hence more flexibility. Ex-
perimentalists can measure few things,
relatively imprecisely, in hard-to-con-
trol situations, but they cannot change
the initial data just a little and run again. 
Theory is also required to extrapolate
a simulation’s result into a physically in-
teresting situation. More broadly, the-
ory is required for simulators to
• assess algorithm reliability,
• make better algorithms, and
• help define what is worth “measuring.”
Theorists also help bring it all to-
gether—recall the work of Oppen-
heimer, Teller, and Bahcall. Ideally, sci-
entists would do it all, much like
Leonardo da Vinci or Enrico Fermi.
But usually, different people have dif-
ferent specialized skills. To solve hard
problems, all the various kinds of sci-
entific skills must work together and, in
the end, pull in the same direction.
Figure 1. A shadowgraph showing the
spatial distribution of thermal plumes in
a Rayleigh-Bénard cell. The fluid is
dipropylene glycol, which has a rather
high viscosity (Prandtl number = 596), so
the pattern of plumes appears in a
simplified form. The Rayleigh number is
6.8 × 108. (The picture was taken by S.
Lam in Keqing Xia’s laboratory and is
part of a joint project with Penger Tong.)
Figure 2.Cartoon view of the central
region of a box, a mixing zone 
containing plumes, and very thin
boundary layers at top and bottom. 
The plumes are believed to arise from
the spray thrown up by waves traveling
across the boundary layer.19
62 COMPUTING IN SCIENCE & ENGINEERING
Jets and Sprays
We shall look at dielectric and con-
ducting fluids moved by an electric
field, based on an experiment. 
Lene Oddershede and Sidney Nagel’s
experimental work starts with oil float-
ing on water (see Figure 4).21 They ap-
ply a strong electric field, with the re-
gions of strongest field strength near the
curved electrode sitting in the oil. The
lower fluid, the one with the higher di-
electric constant, is pulled upward to-
ward the stronger electric field. Thus, in
Figure 4’s first few panels, we see that
the water forms itself into a bump.
Here is a nice, simple problem that
we could use as an exercise in a simula-
tional partial differential equations
course. The flow looks simple and easy
to understand, but in the real world,
surprises are possible, even likely (see
the last few panels in Figure 4). After a
time, the water bump forms itself into
a sharp point. Then, starting from the
point, something starts moving through
the oil. In the next to last frame, that
motion resolves itself into a jet of
charged fluid. In the final frame, the
fluid breaks up into many tiny droplets.
Complex systems sometimes show
qualitative changes in their behavior—
for instance, if our bump has turned into
lightning and rain. Now our simple
problem has developed new phenomena
and new scales. Experiment is very good
at finding unexpected behavior and de-
scribing its overall characteristics; the-
ory often can explain what’s going on.
After an appropriate pause for algo-
rithm development, simulations then
can test the ideas and fill in the details.
Recently, my student Moses Hohman
established the basic mechanism for rain
production by doing a simulation inves-
tigating the linear stability (in this case,
instability) analysis of a charged jet.
Working with Michael Brenner, M.
Shin, and G.C. Rutledge, he looked for
and saw a whipping instability in the
motion. This instability produces a
turning motion, much like that of a
corkscrew; the drops are presumed to be
thrown off by the spinning.
In a parallel effort, my student Cheng
Yang has looked at the process of singu-
larity formation in the interface between
two unlike dielectric fluids in motion in
an electric field. He was looking for the
structure formed very near the singu-
larity, but he found a surprise—a result
P E R S P E C T I V E S  I N  C O M P U T A T I O N A L  S C I E N C E
Table 1. How experiment and simulation complement one another. 
Quantity Simulation Experiment
Turnovers Five or 10 Thousands
Ra Up to 1011 Up to 1014
Runs Few and costly Many
Flexibility Low High
Measure Anything Very few things
Precision Often very high Variable
Equations Well known Often unknown
Small variation Easy Impossible
in initial data
50
40
30
20
10
0
Log10 density
20
Figure 3. A simulation describing a hot bubble rising from the center of a galaxy. In these pictures, gravity points toward the left.  The
simulations describe two-dimensional flow with zero viscosity and zero thermal conductivity. The color-coding describes density. 
40 60 80 100
r
–3.0 –2.5 –2.0 –1.5 –1.0 –0.5 0.0
MARCH/APRIL 2004 63
contrary to our initial presupposition.
From the previous literature, especially
the work of G.I. Taylor, I expected to
see the formation of a static cone-like
structure that could have a persistent ex-
istence in the electric field. Yang actu-
ally found a transient dynamical conical
structure, which formed for an instant
and then broke up (see Figure 5).22 As
his thesis adviser, I’m more than slightly
proud that his simulation found some-
thing unexpected, and that he stuck to
his guns long enough to convince his
thesis committee that this result was
both surprising and correct. So often,
simulations only yield what was desired
from the beginning.
The Rayleigh-Taylor Instability
The Rayleigh-Taylor instability has
been an important focus of recent work,
especially within the ASCI program.
The instability can arise whenever a
heavier fluid sits on top of a lighter one.
If the interface between the two re-
mains horizontal, nothing happens, but
a wrinkling of the surface can produce
a cascade of changes in which jets and
plumes of the heavier fluid penetrate
into the lighter one and vice versa.
Some experimental studies of this
situation have been performed. For ad-
ministrative reasons, a decision was
made early in the ASCI programs to
concentrate on simulations—with only
minor input from experiment. Re-
cently, this unbalanced approach’s
weakness was recognized, resulting in
plans for an increased emphasis on ex-
periment. Unfortunately, the earlier,
unbalanced style has affected some of
the Rayleigh-Taylor work.
Many important simulations of the
Rayleigh-Taylor system have been per-
formed. To see the fully developed in-
stability, some major simplifications of
the physical model are required. Be-
cause ASCI is interested in large
Reynolds numbers, viscosity is usually
neglected in the simulations. Further-
more, to maximize the effect of the in-
stability, you have to neglect the surface
tension in the interface between the flu-
ids. These choices have been made to
speed up the simulation, and they do so.
However, the problem that remains is
technically “ill posed:” we cannot prove
that it is mathematically meaningful.
The practical meaning is that we can-
not promise different approximation
approaches will converge to the same
answer, and that any one of those will
correspond to the experimental system.
The outcome has been, to say the
least, quite interesting. A group of
studies has been put together—all aim-
ing to measure the degree of penetra-
tion of one fluid into another.23 The
Figure 4. The experimental21 production of a singularity at the onset of an 
electrohydrodynamic spout. Two fluids, oil (above) and water (below), are
separated by an interface. A strong electric field points into the electrode shown at
the top of each frame. This field carries the water, with its higher dielectric constant,
upward into the region of a strong electric field. Eventually, the interface comes to a
point and breaks down. A discharge is produced and generates many small droplets
of water in the oil.
64 COMPUTING IN SCIENCE & ENGINEERING
penetration is determined in terms of a
coefficient called α, which measures
the extent of the mixing zone relative
to a purely ballistic motion of the flu-
ids. An experiment measuring this
quantity has been compared to half a
dozen different groups’ simulations, all
starting from identical initial condi-
tions. The results fall into two groups.
The experiment,24 the theory,25 and
one of the simulations26 show an α-
value of roughly 0.06; the other simu-
lations give α of the order of 0.03 or
less (see Figure 6). Another study23
takes a different tack by looking at a
single penetrating region and using pe-
riodic boundary conditions (see Figure
7). Note that the flow is extremely
complex and quite sensitively depen-
dent on the computational resolution.
If we take the generated pictures at
their face value, we would conclude
that the shape of the interface given by
the simulation would never converge.
On the other hand, there is some indi-
cation of convergence of the value of α.
We still don’t know if the approxima-
tion of zero surface tension and viscos-
ity make any sense, or if the value of α
obtained in this way is meaningful.
To drive this point home, let’s look
at one more example. Figure 8 shows
four calculations of the mixing of the
spray produced by a breaking wave.27
All four describe the same, ill-posed
problem: wave motion without surface
tension or viscosity. All four start from
the same initial data, and all four have
the same value of the “wind” driving
the wave. The only differences are in
calculational resolution—and in the
answers. The patterns of spray look
quite different. The graph on the
lower right shows not only that the
measured amount of mixing depends
on resolution but also that it’s a non-
monotonic function of the resolution.
In short, much more work will be re-
quired before we can, with full relia-
bility, estimate the mixing from this
calculational method.
The problems with these calcula-
tions illustrate the well-known fact that
finding a valid answer from a computer
simulation can be a matter of some
subtlety. For example, the calculation
in Figure 3 has a range of validity that
must be regarded as unknown because
the numerical method must still be re-
garded as unproven. The calculation
describes events at the center of a
galaxy. We are interested in having an
accurate picture of what goes on there,
but we can afford to wait for the fur-
ther scientific developments that
should tell us more about the accuracy
of the calculational method. In other
cases, however, we may need accurate
answers to questions involving highly
turbulent flows. Unfortunately, we
have no proven way of getting them.
T o maintain a national capacityfor understanding the develop-
ment of complexity and multiscale
phenomena, we should support first
principles studies of a variety of dif-
ferent complex systems. Each such
study requires a balanced and inter-
disciplinary program of research in
which theory, simulation, and experi-
ment work together to ask and answer
incisively posed questions.
The goal of my group’s research at
Chicago is to ask important questions
about the world. We solve simple
model problems, like those I’ve dis-
cussed here, and then ask questions like
• How does complexity arise? Why is
chaos often observed?
• What dramatic events occur in the
fluid? Are they commonplace?
• Why do fluids naturally form
structures?
A parallel goal is to teach students to
ask incisive questions. These are good
problems for students because they are
small enough to be solved quickly.
They are also down-to-earth enough
P E R S P E C T I V E S  I N  C O M P U T A T I O N A L  S C I E N C E
–4 2 0 2 4
z
1.0
0.5
0.0
0.5
1.0
t = 0.0
t = 25.86
–3.16 –3.14 –3.12
z
–0.02
0.00
0.02
h(
z)
h(
z)
h(
z)
t = 22.0
t = 24.0
t = 25.0
t = 25.5
t = 25.75
t = 25.86
0 5 10 15 20 25 30
t
10–1
10–2
10–3
V t
ip
Figure 5. Computer simulation22 of a singularity in a situation in which two fluids with
different dielectric constants are separated by an interface. The electric field generates
polarization, producing forces on the drop’s surface. Surface tension provides additional
forces. The first frame shows the drop’s initial and final shapes. Originally, it had an
ellipsoidal shape; after a time, the drop develops cone-like points on the ends. The
second frame shows how the cone gradually sharpens. The final frame shows that there
is indeed a singularity—the tip’s velocity diverges at a critical time.
MARCH/APRIL 2004 65
so that each student can appreciate
what they’re about.
In the world outside of schools, we
simulators have an important role to
play as part of the teams within scien-
tific and engineering groups devoted to
understanding design and develop-
ment. In the past, we have sometimes
appeared in a supporting role, filling in
the details in understandings con-
structed by others. We may wish to be
more incisive, pointing out where the
design won’t work, how the theory
won’t hold water, or why the experi-
ment is wrongly interpreted. We also
may wish to be more creative, using
our simulations to point the way to
overall understanding or good design.
Only if we are both more creative and
more critical can we expect our work to
be evaluated and tested by the hands-
on and the pencil-and-paper people
who also form a part of our scientific
and engineering world. Such a give-
and-take approach forms the basis of
good design and understanding.
Conversely, if our work only justifies
and explains the work done by design-
ers and experimentalists, if we simula-
tors never say that the other guys are
dead wrong, then we deserve a situa-
tion in which simulation is relegated to
the position of a third and lesser
branch of science, considerably behind
experiment and theory.
Acknowledgments
This work was supported in part by the
US Department of Energy through the
ASCI/FLASH program and by the
University of Chicago MRSEC and via
a NSF-DMR grant. I thank Alexandros
Alexakis, Marko Kleine Berkenbusch,
Michael Brenner, Alan Calder, Sascha
Hilgenfeldt, Robert Laughlin, Steve
Libby, Detlef Lohse, Sidney Nagel,
Robert Rosner, Andrew J. Szeri,
Penger Tong, Keqing Xia, and Yuan-
Nan Young for helpful comments and
for help in getting together the figures.
References
1. N. Metropolis et al., “Equation of State Calcu-
lations by Fast Computing Machines,” J.
Chemical Physics, vol. 21, no. 6, 1953, pp.
1087–1092. 
2. P. Galison, Image and Logic, Univ. of Chicago
Press, 1997, p. 702.
3. E. Fermi, J. Pasta, and S. Ulam, “Studies in
Nonlinear Problems, I.,” Nonlinear Wave Mo-
tion, A.C. Newell, ed., Am. Mathematical
Soc., 1974. (Article is reproduced in A.C.
Newell, ed., Nonlinear Wave Motion, Am.
Mathematical Soc., 1974 and in M. Tabor,
“The FUP Experiment,” Chaos and Integrabil-
ity in Nonlinear Dynamics: An Introduction,
John Wiley & Sons, 1989, p. 280).
4. B.J. Alder and T.E. Wainwright, “Phase Transi-
tion for Hard Sphere System,” J. Chemical
Physics, vol. 27, no. 5, 1957, pp. 1208–1209. 
5. B. Alder and T. Wainright, “Decay of the Ve-
locity Autocorrelation Function,” Physical Rev.
A, vol. 1, no. 1, 1970, pp. 18–21.
6. E. Lorenz, The Essence of Chaos, Univ. of
Washington Press, 1993.
7. M. Feigenbaum, “Universal Behavior in Non-
Linear Systems,” Los Alamos Science, vol. 1,
no. 1, 1981, pp. 4–27.
8. J.H. Bahcall, M.H. Pinsonneault, and S. Basu,
“Solar Models: Current Epoch and Time De-
pendence, Neutrinos, and Helioseismological
Properties,” Astrophysical J., vol. 555, no, 2,
2001, pp. 900–1012.
9. M.P. Brenner, S. Hilgenfeldt, and D. Lohse,
“Single Bubble Sonoluminescence,” Rev.
Modern Physics, vol. 74, 2002, pp. 425–484.
10. E. Bertschinger, “Simulations of Structure For-
mation in the Universe,” Ann. Rev. Astronomy
and Astrophysics, vol. 36, 1998, pp. 599–654. 
11. R.P. Taleyarkhan et al., “Evidence for Nuclear
Emissions During Acoustic Cavitation,” Science,
vol. 295, no. 5561, 2002, pp. 1868–1873.
12. D. Shapira and M. Saltmarsh, “Nuclear Fusion
in Collapsing Bubbles—Is It There? An At-
tempt to Repeat the Observation of Nuclear
Emissions from Sonoluminescence,” Physical
Rev. Letters, vol. 89, no. 10, 2002, p. 104302. 
13. B.P. Barber et al., “Resolving the Picosecond
Characteristics of Synchronous Sonolumines-
cence,” J. Acoustical Soc. Am., vol. 91, no. 5,
1992, pp. 3061–3063.
14. V.Q. Vuong and A.J. Szeri, “Sonolumines-
cence and Diffusive Transport,” Physics of Flu-
ids, vol. 8, no. 8, 1996, pp. 2354–2364.
15. C.C. Wu and P.H. Roberts, “Shock Wave
Propagation in a Sonoluminescing Gas Bub-
ble,” Physical Rev. Letters, vol. 70, no. 22,
1993, pp. 3424–3427.
Figure 6. The Rayleigh-Taylor instability. The initial state was a gently wavy interface
separating a high-density fluid from a low-density one. Gravity (pointing up!) then
destabilizes the interface, producing the mixed regions shown. Unmixed regions are
transparent. Red, yellow, and green show successively higher densities. This
simulation assumes that both viscosity and surface tension are negligibly small.23
66 COMPUTING IN SCIENCE & ENGINEERING
16. B. Gomph et al., “Resolving Sonoluminescence
Pulse Width with Single Photon Counting,”
Physical Rev. Letters, vol. 79, no. 7, 1997, pp.
1405–1408.
17. C.C. Wu and P.H. Roberts, “Shock Wave
Propagation in a Sonoluminescing Gas Bub-
ble,” Physical Rev. Letters, vol. 70, 1993, pp.
3424–3427.
18. W.C. Moss et al., “Hydrodynamic Simula-
tions of Bubble Collapse and Picosecond
Sonoluminescence,” Physics of Fluids, vol. 6,
1994, pp. 2979–2985.
19. X.-L. Qiu and P. Tong, “Temperature Oscilla-
tions in Turbulent Rayleigh-Benard Convec-
tion,” Physical Rev. E, vol. 66, 2002, p. 026308.
20. M. Brüggen and C. Kalser, “Hot Bubbles
from Active Galactic Nuclei as a Heat Source
in Cooling-Flow Clusters,” Nature, vol. 318,
no. 6895, 2002, pp. 301–303.
21. L. Oddershede and S.R. Nagel, “Singularity
During the Onset of an Electrohydrodynamic
Spout,” Physical Rev. Letters, vol. 85, no. 6,
2000, pp. 1234–1237.
22. C. Yang, Viscous Flow: Approach to Singularity
in an Electric Field, PhD thesis, Dept. of
Physics, Univ. of Chicago, 2003.
23. A.C. Calder et al., “On Validating an Astro-
physical Simulation Code,” Astrophysical J. Sup-
plement, vol. 143, no. 1, 2002, pp. 201–230.
24. G. Dimonte and M. Schneider, “Density Ra-
tio Dependence of Rayleigh-Taylor Mixing for
Sustained and Impulsive Acceleration Histo-
ries,” Physics of Fluids A, vol. 12, no. 2, 2000,
pp. 304–321.
25. B. Cheng, J. Glimm, and D.H. Sharp, “A 3D RNG
Bubble Merger Model of Rayleigh Taylor Mix-
ing,” Chaos, vol. 12, no. 2, 2002, pp. 267–274.
26. J. Glimm et al., “A Critical Analysis of Rayleigh-
Taylor Growth Rates,” J. Computational
Physics, vol. 169, no. 2, 2001, pp. 652–677.
27. A.C. Calder et al., “Mixing by Non-Linear
Gravity Wave Breaking on a White Dwarf Sur-
face,” Proc. Int’l Conf. Classical Nova Explosions,
Am. Inst. of Physics, 2003, pp. 134–138.
Leo P. Kadanoff is the John D. and Catherine
T. MacArthur Distinguished Service Professor of
Physics and Mathematics, Emeritus, at the Uni-
versity of Chicago. He is also a member of the
Committee on the History and Philosophy of
Science at this same institution. He has made
fundamental contributions to the fields of
chaos, statistical, solid-state, and nonlinear the-
oretical and computational physics. His research
has emphasized scaling and the universality par-
ticularly in dynamical problems and phase tran-
sitions. Contact him at LeoP@uchicago.edu.
P E R S P E C T I V E S  I N  C O M P U T A T I O N A L  S C I E N C E
Figure 7. The Rayleigh-Taylor instability, once more. This calculation is done with a simple “one bump” initial state. The effect of
resolution is studied by using resolutions differing by a factor of two in successive panels. Note how the results change
considerably with resolution. The highest resolution picture is qualitatively different from the others in that the left–right
symmetry is broken. (Figure courtesy of Alan Calder, and rather similar to ones appearing in reference 23.)
MARCH/APRIL 2004 67
0.0
2.0 × 106
1.5 × 106
1.0 × 106
0.5 × 106
0.0
0.5 × 106 1.0 × 106
x (cm)
1.5 × 106 2.0 × 106
y 
(c
m
)
0.0
2.0 × 106
1.5 × 106
1.0 × 106
0.5 × 106
0.0
0.5 × 106 1.0 × 106
x (cm)
1.5 × 106 2.0 × 106
y 
(c
m
)
0.0
2.0 × 106
1.5 × 106
1.0 × 106
0.5 × 106
0.0
0.5 × 106 1.0 × 106
x (cm)
1.5 × 106 2.0 × 106
y 
(c
m
)
0.0
2.0 × 106
1.5 × 106
1.0 × 106
0.5 × 106
0.0
0.5 × 106 1.0 × 106
x (cm)
1.5 × 106 2.0 × 106
y 
(c
m
)
0.0
2.0 × 106
1.5 × 106
1.0 × 106
0.5 × 106
0.0
0.5 × 106 1.0 × 106
x (cm)
1.5 × 106 2.0 × 106
y 
(c
m
)
0.00
8 × 108
6 × 108
4 × 108
2 × 108
0
00.5 0.10
Time (sec.)
0.15 0.20 0.25
M
ix
ed
 C
/O
 m
as
s 
pe
r 
ar
ea
 (
gr
/c
m
2 )
Figure 8. Wave breaking at a white dwarf surface. This figure shows the result of wind-driven instability on the surface of a star.
Surface tension and viscosity are assumed to be negligibly small. The different panels show the same initial condition at the
same time but the resolutions differ by a factor of two in neighboring panels. On the right, we see plots of mixing versus time
for these different resolutions. The take-home message is that resolution matters both in the profile and also in the mixing.
(Figure courtesy of Alan Calder; simulations by Alexandros Alexakis.)

