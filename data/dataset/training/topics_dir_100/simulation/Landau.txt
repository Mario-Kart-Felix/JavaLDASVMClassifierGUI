c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Python Multimodal eBook Version
A Survey of
Computational Physics
Introductory Computational Science
RUBIN H. LANDAU
Oregon State University
MANUEL JOSÉ PÁEZ
University of Antioquia
CRISTIAN C. BORDEIANU
University of Bucharest
with Video Production by Sally Haerer
PRINCETON UNIVERSITY PRESS
PRINCETON AND OXFORD
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Contents
Preface iii
1. Computational Science Basics 1
1.1 Software Needed to Use This eBook 1
1.2 Using The Features of This eBook 2
1.3 Computational Physics and Computational Science 5
1.4 Viewing the Subjects to be Covered 6
1.5 Making Computers Obey; Languages (Theory) 9
1.6 Programming Warmup 11
1.6.1 Structured Program Design 12
1.6.2 Shells, Editors and Execution 13
1.6.3 Formatted I/O in Python 14
1.6.4 I/O Redirection 15
1.6.5 Command-Line Input 16
1.6.6 I/O Exceptions: FileCatchThrow.py 17
1.6.7 Automatic Code Documentation  17
1.7 Computer Number Representations (Theory) 18
1.7.1 IEEE Floating-Point Numbers 19
1.7.2 Python and the IEEE 754 Standard 24
1.7.3 Over/Underflows Exercises 24
1.7.4 Machine Precision (Model) 25
1.7.5 Determine Your Machine Precision 26
1.8 Problem: Summing Series 26
1.8.1 Numerical Summation (Method) 27
1.8.2 Implementation and Assessment 27
2. Errors & Uncertainties in Computations 29
2.1 Types of Errors (Theory) 29
2.1.1 Model for Disaster: Subtractive Cancellation 31
2.1.2 Subtractive Cancellation Exercises 31
2.1.3 Round-off Error in a Single Step 33
2.1.4 Round-off Error Accumulation After Many Steps 33
2.2 Errors in Spherical Bessel Functions (Problem) 34
2.2.1 Numerical Recursion Relations (Method) 35
2.2.2 Implementation and Assessment 36
2.3 Experimental Error Investigation (Problem) 37
2.3.1 Error Assessment 39
3. Visualization Tools 42
3.1 Installing Graphics Packages in Python 43
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
2 CONTENTS
3.2 Data Visualization 43
3.3 Importing Packages/Modules into Python Programs 44
3.4 Matplotlib: 2-D Graphs Within Python 44
3.4.1 Matplotlib 3-D 48
3.4.2 Matplotlib Exercises 49
3.5 2-D Plots with Visual (VPython) 49
3.6 Animations with Visual (VPython) 51
3.7 Gnuplot: Reliable 2-D and 3-D Plots 52
3.7.1 Gnuplot Input Data Format  53
3.7.2 Printing Plots 54
3.7.3 Gnuplot Surface (3-D) Plots 54
3.7.4 Gnuplot Vector Fields 56
3.7.5 Animations from Gnuplot 58
3.8 OpenDX for Dicing and Slicing 59
3.9 Texturing and 3-D Imaging 59
3.10 Grace/ACE: Superb 2-D Graphs for Unix/Linux 60
3.10.1 Grace Basics 60
4. Python Object-Oriented Programs: Impedance & Batons 65
4.1 Unit I. Basic Objects: Complex Impedance 65
4.2 Complex Numbers (Math) 66
4.3 Resistance Becomes Impedance (Theory) 67
4.4 Abstract Data Structures, Objects (CS) 68
4.4.1 Object Declaration and Construction 69
4.4.2 Implementation in Python 69
4.4.3 Without Dummies and z’s 72
4.4.4 The Elegance of Overload 73
4.4.5 Python OOP Differs from Java and C++* 73
4.5 Python’s Built-in Complex Number Type 74
4.6 Complex Currents (Solution) 74
4.7 OOP Worked Examples 75
4.7.1 OOP Beats 76
4.7.2 OOP Planet 77
4.8 Subclasses and Class Inheritance 79
4.9 Unit II. Advanced Objects: Baton Projectiles 80
4.10 Trajectory of a Thrown Baton (Problem) 80
4.10.1 Combined Translation + Rotation ( Theory) 81
4.11 OOP Design Concepts (CS) 83
4.12 Multiple Classes and Multiple Inheritances 83
4.13 Multiple Inheritances, Classes in the Same File 87
4.14 Multiple Inheritance, Separate Files 88
4.14.1 Composition Exercise 88
4.14.2 Calculating the Baton’s Energy (Extension) 89
4.14.3 Examples of Inheritance and Hierarchies 90
4.14.4 Baton with a Lead Weight (Application) 91
4.14.5 Encapsulation to Protect Classes 92
4.14.6 Encapsulation Exercise 93
4.14.7 Complex Object Interface (Extension) 94
4.15 OOP Example: Superposition of Motions 96
4.16 Newton’s Laws of Motion (Theory) 96
4.17 OOP Class Structure (Method) 97
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
CONTENTS 3
4.18 Python Implementation 97
5. Monte Carlo Simulations (Nonthermal) 99
5.1 Unit I. Deterministic Randomness 99
5.2 Random Sequences (Theory) 99
5.2.1 Random-Number Generation (Algorithm) 100
5.2.2 Implementation: Random Sequence 102
5.2.3 Assessing Randomness and Uniformity 103
5.3 Unit II. Monte Carlo Applications 104
5.4 A Random Walk (Problem) 104
5.4.1 Random-Walk Simulation 105
5.4.2 Implementation: Random Walk 106
5.5 Radioactive Decay (Problem) 107
5.5.1 Discrete Decay (Model) 107
5.5.2 Continuous Decay (Model) 108
5.5.3 Decay Simulation 109
5.6 Decay Implementation and Visualization 110
6. Integration 111
6.1 Integrating a Spectrum (Problem) 111
6.2 Quadrature as Box Counting (Math) 111
6.2.1 Algorithm: Trapezoid Rule 113
6.2.2 Algorithm: Simpson’s Rule 114
6.2.3 Integration Error (Analytic Assessment) 115
6.2.4 Algorithm: Gaussian Quadrature 117
6.2.5 Implementation and Error Assessment 118
6.3 Experimentation 120
6.4 Higher-Order Rules (Algorithm) 120
6.5 Monte Carlo Integration by Stone Throwing 121
6.5.1 Stone Throwing Implementation 121
6.5.2 Integration by Mean Value (Math) 122
6.6 High-Dimensional Integration (Problem) 123
6.6.1 Multidimensional Monte Carlo 123
6.6.2 Error Assessment 124
6.6.3 Implementation: 10-D Monte Carlo Integration 124
6.7 Integrating Rapidly Varying Functions (Problem) 124
6.7.1 Variance Reduction (Method) 124
6.7.2 Importance Sampling (Method) 125
6.7.3 Von Neumann Rejection (Method) 125
6.7.4 Simple Gaussian Distribution 126
6.8 Nonuniform Assessment  126
6.8.1 Implementation 126
7. Differentiation & Searching 129
7.1 Unit I. Numerical Differentiation 129
7.2 Forward Difference (Algorithm) 130
7.3 Central Difference (Algorithm) 130
7.4 Extrapolated Difference (Method) 131
7.5 Error Analysis (Assessment) 132
7.6 Second Derivatives (Problem) 133
7.6.1 Second-Derivative Assessment 133
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
4 CONTENTS
7.7 Unit II. Trial-and-Error Searching 134
7.8 Quantum States in a Square Well (Problem) 134
7.9 Trial-and-Error Roots via Bisection Algorithm 134
7.9.1 Bisection Algorithm Implementation 135
7.10 Newton–Raphson Searching (Improved Algorithm) 136
7.10.1 Newton–Raphson with Backtracking 138
7.10.2 Newton–Raphson Implementation 138
8. Solving Systems of Equations with Matrices;
Data Fitting 139
8.1 Unit I. Systems of Equations and Matrix Computing 139
8.2 Two Masses on a String 140
8.2.1 Statics (Theory) 141
8.2.2 Multidimensional Searching 141
8.3 Classes of Matrix Problems (Maths) 143
8.3.1 Practical Matrix Computing 145
8.3.2 Implementation: Scientific Libraries, WWW 147
8.4 Numerical Python and Python Matrix Library 148
8.4.1 Arrays in Python: 149
8.4.2 LinearAlgebra.py Package 150
8.4.3 Exercises for Testing Matrix Calls 152
8.4.4 Matrix Solution of the String Problem 154
8.4.5 Explorations 155
8.5 Unit II. Data Fitting 155
8.6 Fitting an Experimental Spectrum (Problem) 156
8.6.1 Lagrange Interpolation (Method) 156
8.6.2 Lagrange Implementation, Assessment 157
8.6.3 Explore Extrapolation 158
8.6.4 Cubic Splines (Method) 159
8.6.5 Spline Fit of Cross Section (Implementation) 161
8.7 Fitting Exponential Decay (Problem) 161
8.7.1 Theory to Fit 161
8.8 Least-Squares Fitting (Method) 162
8.8.1 Theory and Implementation 164
8.8.2 Exponential Decay Fit Assessment 166
8.8.3 Exercise: Fitting Heat Flow 166
8.8.4 Linear Quadratic Fit (Extension) 167
8.8.5 Linear Quadratic Fit Assessment 169
8.8.6 Nonlinear Fit to a Cross Section 169
9. Differential Equation Applications 172
9.1 Unit I. Free Nonlinear Oscillations 172
9.2 Nonlinear Oscillators (Models) 173
9.3 Types of Differential Equations (Math) 174
9.4 Dynamic Form for ODEs (Theory) 175
9.5 ODE Algorithms 177
9.5.1 Euler’s Rule 178
9.5.2 Runge–Kutta Algorithm 178
9.5.3 ABM Predictor-Corrector 181
9.5.4 Assessment: rk2 versus rk4 versus rk45 182
9.6 Solution for Nonlinear Oscillations (Assessment) 183
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
CONTENTS 5
9.6.1 Precision Assessment: E Conservation 184
9.7 Extensions: Nonlinear Resonances, Beats, Friction 184
9.7.1 Friction (Model) 184
9.7.2 Resonances & Beats: Model, Implementation 185
9.8 Extension: Time-Dependent Forces 186
9.9 Unit II. Binding A Quantum Particle 186
9.10 Theory: Quantum Eigenvalue Problem 187
9.10.1 Model: Nucleon in a Box 188
9.11 Algorithm: Eigenvalues via ODE Solver + Search 188
9.11.1 Numerov Algorithm for Schrödinger ODE  189
9.11.2 Implementation 192
9.12 Explorations 194
9.13 Unit III. Scattering, Projectiles and Planetary Orbits 195
9.14 Problem 1: Classical Chaotic Scattering 195
9.14.1 Model and Theory 195
9.14.2 Implementation 197
9.14.3 Assessment 198
9.15 Problem 2: Balls Falling Out of the Sky 198
9.16 Theory: Projectile Motion with Drag 198
9.16.1 Simultaneous Second-Order ODEs 200
9.16.2 Assessment 200
9.17 Problem 3: Planetary Motion 201
9.17.1 Implementation: Planetary Motion 201
10.Fourier Analysis: Signals and Filters 203
10.1 Unit I. Fourier Analysis of Nonlinear Oscillations 203
10.2 Fourier Series (Math) 204
10.2.1 Example 1: Sawtooth Function 206
10.2.2 Example 2: Half-wave Function 206
10.3 Exercise: Summation of Fourier Series 206
10.4 Fourier Transforms (Theory) 207
10.4.1 Discrete Fourier Transform Algorithm 208
10.4.2 Aliasing 211
10.4.3 Fourier Series DFT (Algorithm) 213
10.4.4 Assessments 213
10.4.5 Nonperiodic Function DFT (Exploration) 215
10.5 Unit II. Filtering Noisy Signals 215
10.6 Noise Reduction via Autocorrelation (Theory) 215
10.6.1 Exercises 218
10.7 Filtering with Transforms (Theory) 218
10.7.1 Digital Filters: Windowed Sinc Filters 221
10.8 Unit III. Fast Fourier Transform Algorithm (FFT)  222
10.8.1 Bit Reversal 225
10.9 FFT Implementation 226
10.10 FFT Assessment 228
11.Wavelet Analysis & Data Compression 230
11.1 Unit I. Wavelet Basics 230
11.2 Wave Packets and Uncertainty Principle (Theory) 232
11.2.1 Assessment 233
11.3 Short-Time Fourier Transforms (Math) 233
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
6 CONTENTS
11.4 The Wavelet Transform 234
11.4.1 Wavelet Basis Functions 235
11.4.2 Continuous Wavelet Transform 238
11.5 Unit II. Discrete Wavelet Transforms, MRA 238
11.5.1 Pyramid Scheme Implementation  243
11.5.2 Daubechies Wavelets via Filtering 246
11.5.3 DWT Implementation and Exercise 248
12.Discrete & Continuous Nonlinear Dynamics 251
12.1 Unit I. Bug Population Dynamics (Discrete) 251
12.2 The Logistic Map (Model) 252
12.3 Properties of Nonlinear Maps (Theory) 253
12.3.1 Fixed Points 253
12.3.2 Period Doubling, Attractors 254
12.4 Mapping Implementation 254
12.5 Bifurcation Diagram 255
12.5.1 Implementation 256
12.5.2 Visualization Algorithm: Binning 257
12.5.3 Feigenbaum Constants (Exploration) 257
12.6 Logistic Map Random Numbers (Exploration)· 258
12.7 Other Maps (Exploration) 258
12.8 Signals of Chaos: Lyapunov Coefficients  258
12.8.1 Shannon Entropy  259
12.9 Unit I Quiz 260
12.10 Unit II. Pendulums Become Chaotic 262
12.11 Chaotic Pendulum ODE 262
12.11.1 Free Pendulum Oscillations 263
12.11.2 Solution as Elliptic Integrals 263
12.11.3 Implementation and Test: Free Pendulum 264
12.12 Visualization: Phase Space Orbits 265
12.12.1 Chaos in Phase Space 267
12.12.2 Assessment in Phase Space 269
12.13 Exploration: Bifurcations of Chaotic Pendulums 271
12.14 Alternative Problem: The Double Pendulum 272
12.15 Assessment: Fourier/Wavelet Analysis of Chaos 274
12.16 Exploration: Another Type of Phase Space Plot 275
12.17 Further Explorations 275
12.18 Unit III. Coupled Predator–Prey Models  276
12.19 Lotka–Volterra Model 276
12.19.1 LVM with Prey Limit 278
12.19.2 LVM with Predation Efficiency 278
12.19.3 LVM Implementation and Assessment 279
12.19.4 Two Predators, One Prey (Exploration) 280
13.Fractals & Statistical Growth 282
13.1 Fractional Dimension (Math) 282
13.2 The Sierpiński Gasket (Problem 1) 283
13.2.1 Sierpiński Implementation 284
13.2.2 Assessing Fractal Dimension 284
13.3 Beautiful Plants (Problem 2) 285
13.3.1 Self-Affine Connection (Theory) 285
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
CONTENTS 7
13.3.2 Barnsley’s Fern Implementation 286
13.3.3 Self-Affinity in Trees Implementation 287
13.4 Ballistic Deposition (Problem 3) 287
13.4.1 Random Deposition Algorithm 288
13.5 Length of British Coastline (Problem 4) 289
13.5.1 Coastlines as Fractals (Model) 289
13.5.2 Box Counting Algorithm 290
13.5.3 Coastline Implementation and Exercise 291
13.6 Correlated Growth, Forests, Films (Problem 5) 292
13.6.1 Correlated Ballistic Deposition Algorithm 292
13.7 Globular Cluster (Problem 6) 293
13.7.1 Diffusion-Limited Aggregation Algorithm 293
13.7.2 Fractal Assessment of DLA/Pollock 295
13.8 Fractals in Bifurcation Plot (Problem 7) 296
13.9 Fractals from Cellular Automata 296
13.10 Perlin Noise Adds Realism  298
13.10.1 Including Ray Tracing 301
13.11 Quiz 303
14.HPC Hardware, Tuning, Parallel Computing 304
14.1 Unit I. High-Performance Computers (CS) 304
14.2 Memory Hierarchy 305
14.3 The Central Processing Unit 308
14.4 CPU Design: Reduced Instruction Set Computer 308
14.5 CPU Design: Multiple-Core Processors 309
14.6 CPU Design: Vector Processor 310
14.7 Unit II. Parallel Computing 310
14.8 Parallel Semantics (Theory) 311
14.9 Distributed Memory Programming 313
14.10 Parallel Performance 314
14.10.1 Communication Overhead 315
14.11 Parallelization Strategy 316
14.12 Practical Aspects of MIMD Message Passing 317
14.12.1 High-Level View of Message Passing 318
14.13 Example of a Supercomputer: IBM Blue Gene/L 320
14.14 Unit III. HPC Program Optimization 322
14.14.1 Programming for Virtual Memory 324
14.14.2 Optimizing Programs; Python versus Fortran/C 324
14.14.3 Empirical Performance of Hardware 326
14.14.4 Python versus Fortran/C 327
14.15 Programming for the Data Cache (Method) 331
14.15.1 Exercise 1: Cache Misses 332
14.15.2 Exercise 2: Cache Flow 332
14.15.3 Exercise 3: Large-Matrix Multiplication 333
15.Thermodynamic Simulations, Quantum Path Integration 335
15.1 Unit I. Magnets via the Metropolis Algorithm 335
15.2 An Ising Chain (Model) 335
15.3 Statistical Mechanics (Theory) 337
15.3.1 Analytic Solutions 338
15.4 Metropolis Algorithm 338
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
8 CONTENTS
15.4.1 Implementation 340
15.4.2 Equilibration, Thermodynamic Properties 341
15.4.3 Beyond Nearest Neighbors, 1-D (Exploration) 343
15.5 Unit II. Magnets via Wang–Landau Sampling  344
15.6 Wang–Landau Sampling 346
15.6.1 Ising Model Implementation 348
15.6.2 Assessment 351
15.7 Unit III. Feynman Path Integrals  351
15.8 Feynman’s Space-Time Propagation (Theory) 351
15.8.1 Bound-State Wave Function (Theory) 354
15.8.2 Lattice Path Integration (Algorithm) 354
15.8.3 Lattice Implementation 359
15.8.4 Assessment and Exploration 361
15.9 Exploration: Quantum Bouncer’s Paths  361
16.Simulating Matter with Molecular Dynamics 364
16.1 Molecular Dynamics (Theory) 364
16.1.1 Connection to Thermodynamic Variables 367
16.1.2 Setting Initial Velocities 368
16.1.3 PBC and V (r) Cutoff 368
16.2 Verlet and Velocity-Verlet Algorithms 370
16.3 1-D Implementation and Exercise 371
16.4 Trajectory Analysis 374
16.5 Quiz 374
17.PDEs for Electrostatics & Heat Flow 376
17.1 PDE Generalities 376
17.2 Unit I. Electrostatic Potentials 377
17.2.1 Laplace’s Elliptic PDE (Theory) 378
17.3 Fourier Series Solution of a PDE 378
17.3.1 Polynomial Expansion As an Algorithm 380
17.4 Solution: Finite-Difference Method 381
17.4.1 Relaxation and Overrelaxation 383
17.4.2 Lattice PDE Implementation 384
17.5 Assessment via Surface Plot 384
17.6 Alternate Capacitor Problems 385
17.7 Implementation and Assessment 387
17.8 Electric Field Visualization (Exploration) 388
17.9 Laplace Quiz 388
17.10 Unit II. Finite-Element Method  389
17.11 Electric Field from Charge Density (Problem) 389
17.12 Analytic Solution 390
17.13 Finite-Element (Not Difference) Methods 390
17.13.1 Weak Form of PDE 390
17.13.2 Galerkin Spectral Decomposition 391
17.14 FEM Implementation and Exercises 394
17.15 Exploration 396
17.16 Unit III. Heat Flow via Time-Steps (Leapfrogs) 396
17.17 The Parabolic Heat Equation (Theory) 396
17.17.1 Solution: Analytic Expansion 398
17.17.2 Solution: Time-Stepping 398
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
CONTENTS 9
17.17.3 Von Neumann Stability Assessment 400
17.17.4 Heat Equation Implementation 401
17.18 Assessment and Visualization 402
17.19 Improved Heat Flow: Crank–Nicolson Method 402
17.19.1 Solution of Tridiagonal Matrix Equations  405
17.19.2 Implementation, Assessment 407
18.PDE Waves: String, Quantum Packet, and E&M 408
18.1 Unit I. Vibrating String 408
18.2 The Hyperbolic Wave Equation (Theory) 409
18.2.1 Solution via Normal-Mode Expansion 410
18.2.2 Algorithm: Time-Stepping 411
18.2.3 Implementation 413
18.2.4 Assessment, Exploration 414
18.3 Waves with Friction (Extension) 415
18.4 Waves for Variable Tension and Density (Extension) 416
18.4.1 Waves on Catenary 417
18.4.2 Derivation of Catenary Shape 417
18.4.3 Catenary and Frictional Wave Exercises 418
18.5 Unit II. Quantum Wave Packets 419
18.6 Time-Dependent Schrödinger Equation (Theory) 420
18.6.1 Finite-Difference Algorithm 421
18.6.2 Wave Packet Implementation, Animation 422
18.7 Wave Packets in Other Wells (Exploration) 423
18.8 Algorithm for the 2-D Schrödinger Equation 424
18.9 Unit III. E&M Waves, Finite-Difference t Domain  426
18.10 Maxwell’s Equations 426
18.11 FDTD Algorithm 427
18.11.1 Implementation 430
18.11.2 Assessment 431
18.11.3 Extension: Circularly Polarized Waves 432
19.Solitons & Computational Fluid Dynamics 435
19.1 Unit I. Advection, Shocks, Russell’s Soliton 435
19.2 Theory: Continuity and Advection Equations 436
19.2.1 Advection Implementation 437
19.3 Theory: Shock Waves via Burgers’ Equation 437
19.3.1 Lax–Wendroff Algorithm for Burgers’ Equation 438
19.3.2 Implementation and Assessment 440
19.4 Including Dispersion 440
19.5 Shallow-Water Solitons; the KdeV Equation 441
19.5.1 Analytic Soliton Solution 442
19.5.2 Algorithm for KdeV Solitons 443
19.5.3 Implementation: KdeV Solitons 444
19.5.4 Exploration: Solitons in Phase Space, Crossing 445
19.6 Unit II. River Hydrodynamics 446
19.7 Hydrodynamics, the Navier–Stokes Equation (Theory) 446
19.7.1 Boundary Conditions for Parallel Plates 448
19.7.2 Analytic Solution for Parallel Plates 450
19.7.3 Finite-Difference Algorithm and Overrelaxation 451
19.7.4 Successive Overrelaxation Implementation 452
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
10 CONTENTS
19.8 2-D Flow over a Beam 452
19.9 Theory: Vorticity Form of Navier–Stokes Equation 452
19.9.1 Finite Differences and the SOR Algorithm 454
19.9.2 Boundary Conditions for a Beam 455
19.9.3 SOR on a Grid Implementation 457
19.9.4 Assessment 457
19.9.5 Exploration 459
20. Integral Equations in Quantum Mechanics 461
20.1 Unit I. Bound States of Nonlocal Potentials 461
20.2 Momentum-Space Schrödinger Equation (Theory) 462
20.2.1 Integral to Linear Equations (Method) 462
20.2.2 Delta-Shell Potential (Model) 464
20.2.3 Binding Energies Implementation 464
20.2.4 Wave Function (Exploration) 466
20.3 Unit II. Nonlocal Potential Scattering  466
20.4 Lippmann–Schwinger Equation (Theory) 466
20.4.1 Singular Integrals (Math) 467
20.4.2 Numerical Principal Values 468
20.4.3 Reducing Integral to Matrix Equations 468
20.4.4 Solution via Inversion, Elimination 469
20.4.5 Scattering Implementation 470
20.4.6 Scattering Wave Function (Exploration) 471
A. Glossary 473
B. Installing Python, VPython,Matplotlib,NumPy 479
C. OpenDX, Industrial-Strength Data Visualization 480
C.1 Getting DX and Unix Running (for Windows) 481
C.2 Test Drive of DX Visual Programming 481
C.3 DX Tools Summary 486
C.4 DX Data Structure and Storage 487
C.5 Sample Visual Programs 488
C.5.1 Sample 1: Linear Plot 489
C.5.2 Sample 2: Fourier Transform 489
C.5.3 Sample 3: Potential of a 2-D Capacitor 490
C.5.4 Sample 4: Vector Field Plots 491
C.5.5 Sample 5: 3-D Scalar Potentials 491
C.5.6 Sample 6: 3-D Functions, the Hydrogen Atom 492
C.6 Animations with OpenDX 495
C.6.1 Scripted Animation with OpenDX 496
C.6.2 Wave Packet and Slit Animation 498
D. An MPI Tutorial 500
D.1 Running on a Beowulf 500
D.2 Running MPI 503
D.2.1 MPI under the SGE Queueing System 504
D.2.2 MPI Under the Torque/PBS Queueing System 506
D.2.3 Running Parallel Jobs with Torque 508
D.3 Your First MPI Program: MPIhello.c 509
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
CONTENTS 11
D.3.1 MPIhello.c Explained 510
D.3.2 Send/Receive Messages: MPImessage2.c 511
D.3.3 Receive More Messages: MPImessage3.c 512
D.3.4 Broadcast Messages 513
D.3.5 Exercise 513
D.4 Parallel Tuning 514
D.5 A String Vibrating in Parallel 517
D.5.1 MPIstring.c Exercise 519
D.6 Deadlock 520
D.6.1 Nonblocking Communication 521
D.6.2 Collective Communication 521
D.7 Bootable Cluster CD  522
D.8 Parallel Computing Exercises 522
D.9 List of MPI Commands 522
E. Software on the CD 525
F. Compression via DWT with Thresholding 528
F.1 More on Thresholding 530
F.2 Wavelet Implementation and Assessment 531
Bibliography 533
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Preface
Constructing this eBook in a time of rapidly-changing technology has been like playing
basketball in a rotating reference frame; regardless of how carefully we aim at the basket we
really do not know where the ball will end up. Starting in 1995, while we were writing our
first Computational Physics (CP) text in LATEX, we envisioned the nascent World Wide Web
and its technologies as being able to enhance a text book by permitting interactions with the
materials via a variety of senses, modes of learning and technologies. Just as during our careers
we have seen computing change the way science is done, we believed that new technologies
would eventually change what readers view as a textbook. During the ensuing years, we have
continued to develop CP courses, record video lectures, develop Web enhancement, and watch
the widening acceptance of eBooks and on-line education. Our aim here is to increase the
educational value of textbooks and courses , and to help promote the developing field of CP,
where combining a textbook with computation seems so natural.
While we have peacefully continued our quests during the emeritus years of the original
two authors, technology and the views of those around us regarding reading and their personal
realities have marched ahead. At the moment we see a generation of students who view video
and on-line experiences as much closer to reality than their elders, and who do much of their
reading and class work online. And all this is viewed as quite ordinary. In addition, the trade
journals and technology pages of our beloved New York Times indicate that the publishing
and technology sectors are taking an increasing interest in eBooks and in new devices that can
present eBooks with more advanced Web technologies. Finally the world seems to be providing
the means to make our vision concrete.
Even in 2006 while completing the Java version of our Survey text, we realized that for
a number of reasons it made good sense to also have a Python version of our text. First,
Python does seem to be the easiest and quickest language for beginning programming. Second,
Chabay and Sherwood have also developed a series of Vpython labs to supplement their series
of introductory physics texts, and the increasing acceptance of those texts provide an excellent
segue for a CP course using Python. Finally, we have always believed that computational
modeling for physics transcends any particular language, and indeed have supplied alternate
versions of our codes in a multiple of languages, and so Python is yet another language.
All these pieces came together when, upon completing the year-long editing of the Java-
based paper text, we spoke with our editor Vickie Kearn at Princeton University Press and
explained the value of having another version of the Survey based on Python. After economic
realities were explained to us, we hit upon the idea of creating the Python version of the Survey
as a multimodal eBook. This would let us explore some of the frontiers of digital text books,
and would encourage the advancement of CP by having the book with its video lectures, codes,
animations and text on line in the National Science Digital Library (Compadre). And all this
without entailing immediate expense on the publisher’s part. We had already made some video
lectures with support from the NSF’s EPIC project, Oregon State University and Microsoft
Corporation, and after three years of trying, were able to get some support from the NSF’s
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
iv PREFACE
CCLI program, which has permitted us to move to this stage. Accordingly, there is a free
version of the eBook in Compadre, as well as DVDs available from Princeton Press.
While the eBook we present here is certainly a prototype, it is a complete and serious text
meant for immediate classroom or online use. Its features, and especially the new ones for
an eBook, are explained in Chapter 1. Also given in Chapter 1 are several graphical concept
maps with active links to chapters and sections, the different maps appropriate for different
level readers. Although these maps are a step towards a personalizable text, this book does not
advance as far as we would prefer in that direction. We have reorganized chapters and units,
but have not yet laid out paths with prerequisites to individual sections. However, because this
is an Ebook, changes will probably be made continually and we encourage your opinions and
suggestions.
On the technical side, the codes in this text were developed with Python 2.5 and the packages
matplotlib, numpy, and Visual 3. There is a new Python 2.6 out, but as of this moment it is not
stable and does not work right with the present versions of matplotlib and numpy and so we
do not recommend it yet. However, there are plans to make matplotlib and numpy compatible
after Python 2.6 stabilizes, and at that point we will change our codes as needed.
Multimodal eBook Acknowledgments
Manuel J Paez, University of Medellin, Colombia, SA, CoAuthor C. E. Yaguna, J. Zuluaga,
Oscar A. Restrepo, Guillermo Avendano-Franco Cristian Bordeianu, University of Bucharest,
Romania, CoAuthor Paul Fink, Robyn Wangberg, CoAuthors Justin Elser, Chris Sullivan (sys-
tem support) Sally Haerer, Saturo S. Kano (consultants, producers) Melanie Johnson (Unix Tu-
torials) Hans Kowallik (Computational Physics text, sounds, codes, LAPACK, PVM) Matthew
Ervin Des Voigne (tutorials) Bertrand Laubsch (Java sound, decay simulation) Jon J Maestri
(vizualizations, animations, quantum wave packets) Guenter Schneider, Al Stetz, David McIn-
tyre (First Course) Juan Vanegas (OpenDX) Connelly Barnes (OOP, PtPlot) Phil Carter, Donna
Hertel (MPI) Zlatko Dimcovic (Wavelets, Java I/O) Joel Wetzel (figures) Pat Cannan, Don
Corliss, Corvallis High School (N-D Newton Raphson) Brian Schlatter Daniel Moore, (REU,
Summer 98; Whitman College, WA) Justin Murray, (REU, Summer 98; Weber State Univer-
sity, Ogden, Utah Brandon Smith, (REU, Summer 97; Chico State/SDSC, CA) Paul D. Hillard,
III (REU, Summer 96; Southern Univ, LA) Kevin Wolver, (REU, Summer 96; St Ambrose, IA)
Preface to the Paper Java Edition
In the decade since two of us wrote the book Computational Physics (CP), we have seen a
good number of computational physics texts and courses come into existence. This is good.
Multiple texts help define a still-developing field and provide a choice of viewpoints and focus.
After perusing the existing texts, we decided that the most worthwhile contribution we could
make was to extend our CP text so that it surveys many of the topics we hear about at computa-
tional science conferences. Doing that, while still keeping the level of presentation appropriate
for upper-division undergraduates or beginning graduate students, was a challenge.
As we look at what we have assembled here, we see more than enough material for a full
year’s course (details in Chapter 1 “Computational Science Basics”). When overlapped with
our new lower-division text, A First Course in Scientific Computing, and when combined with
studies in applied mathematics and computer science, we hope to have created a path for un-
dergraduate computational physics/science education to follow.
The ensuing decade has also strengthened our view that the physics community is well
served by having CP as a prominent member of the broader computational science and
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PREFACE v
engineering (CSE) community. This view affects our book in two ways. First, we present
CP as a multidisciplinary field of study that contains elements from applied mathematics and
computer science, as well as physics. Accordingly, we do not view the pages we spend on
these subjects as space wasted not studying physics but rather as essential components of a
multidisciplinary education. Second, we try to organize and present our materials according to
the steps in the scientific problem-solving paradigm that lie at the core of CSE:
Problem↔ theory↔ model↔ method↔ implementation↔ assessment.
This format places the subject matter in its broader context and indicates how the steps are
applicable to a wider class of problems. Most importantly, educational assessments and surveys
have indicated that some students learn science, mathematics, and technology better when they
are presented together in context rather than as separate subjects. (To some extent, the loss of
“physics time” learning math and CS is made up for by this more efficient learning approach.)
Likewise, some students who may not profess interest in math or CS are motivated to learn
these subjects by experiencing their practical value in physics problem solving.
Though often elegant, we view some of the new CP texts as providing more of the theory
behind CP than its full and practical multidisciplinary scope. While this may be appropriate
for graduate study, when we teach from our texts we advocate a learn-by-doing approach that
requires students to undertake a large number of projects in which they are encouraged to make
discoveries on their own. We attempt to convey that it is the students’ job to solve each problem
professionally, which includes understanding the computed results. We believe that this “blue-
collar” approach engages and motivates the students, encompasses the fun and excitement of
CP, and stimulates the students to take pride in their work.
As computers have become more powerful, it has become easier to use complete problem-
solving environments like Mathematica, Maple, Matlab, and Femlab to solve scientific prob-
lems. Although these environments are often used for serious work, the algorithms and numer-
ics are kept hidden from the user, as if in a black box. Although this may be a good environment
for an experienced computational scientist, we think that if you are trying to learn scientific
computation, then you need to look inside the black box and get your hands dirty. This is
probably best done through the use of a compiled language that forces you to deal directly
with the algorithm and requires you to understand the computer’s storage of numbers and inner
workings.
Notwithstanding our viewpoint that being able to write your own codes is important for
CP, we also know how time-consuming and frustrating debugging programs can be, especially
for beginners. Accordingly, rather than make the reader write all their codes from scratch,
we include basic programs to modify and extend. This not only leaves time for exploration
and analysis but also more realistically resembles a working environment in which one must
incorporate new developments with preexisting developments of others.
The choice of Java as our prime programming language may surprise some readers who
know it mainly for its prowess in Web computing (we do provide Fortran77, Fortran95, and
C versions of the programs on the CD). Actually, Java is quite good for CP education since it
demands proper syntax, produces useful error messages, and is consistent and intelligent in its
handling of precision (which C is not). And when used as we use it, without a strong emphasis
on object orientation, the syntax is not overly heavy. Furthermore, Java now runs on essentially
all computer systems in an identical manner, is the most used of all programming languages,
and has a universal program development environment available free from Sun [SunJ], where
the square brackets refer to references in the bibliography. (Although we recommend using
shells and jEdit [jEdit] for developing Java programs, many serious programmers prefer a de-
velopment platform such as Eclipse [Eclipse].) This means that practitioners can work just as
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
vi PREFACE
well at home or in the developing world. Finally, Java’s speed does not appear to be an issue for
educational projects with present-day fast computers. If more speed is needed, then conversion
to C is straightforward, as is using the C and Fortran programs on the CD.
In addition to multilanguage codes, the CD also contains animations, visualizations, color
figures, interactive Java applets, MPI and PVM codes and tutorials, and OpenDX codes. More
complete versions of the programs, as well as programs left for exercises, are available to
instructors from RHL. There is also a digital library version of the text containing streamingCD
video lectures and interactive equations under development.
Specific additions to this book, not found in our earlier CP text, include chapters and ap-
pendixes on visualization tools, wavelet analysis, molecular dynamics, computational fluid
dynamics, MPI, and PVM. Specific subjects added to this text include shock waves, soli-
tons, IEEE floating-point arithmetic, trial-and-error searching, matrix computing with libraries,
object-oriented programming, chaotic scattering, Lyapunov coefficients, Shannon entropy,
coupled predator–prey systems, advanced PDE techniques (successive overrelaxation, finite
elements, Crank–Nicholson and Lax–Wendroff methods), adaptive-step size integrators, pro-
jectile motion with drag, short-time Fourier transforms, FFT, filtering, Wang–Landau simula-
tions of thermal systems, Perlin noise, cellular automata, and waves on catenaries.
Acknowledgments
This book and the courses it is based upon could not have been created without financial
support from the National Science Foundation’s CCLI, EPIC, and NPACI programs, as well as
from the Oregon State University Physics Department and the College of Science. Thank you
all and we hope you are proud.
Immature poets imitate;
mature poets steal.
— T. S. Elliot
Our CP developments have followed the pioneering path paved with the books of Thompson,
Koonin, Gould and Tobochnik, and Press et al.; indubitably, we have borrowed material from
them and made it our own. We wish to acknowledge the many contributions provided by Hans
Kowallik, who started as a student in our CP course, continued as a researcher in early Web
tutorials, and has continued as an international computer journeyman. Other people have con-
tributed in various places: Henri Jansen (early class notes), Juan Vanegas (OpenDX), Connelly
Barnes (OOP and PtPlot), Phil Carter and Donna Hertel (MPI), Zlatko Dimcovic (improved
codes and I/O), Joel Wetzel (improved figures and visualizations), Oscar A. Restrepo (QM-
Cbouncer), and Justin Elser (system and software support).
It is our pleasure to acknowledge the invaluable friendship, encouragement, helpful discus-
sions, and experiences we have had with our colleagues and students over the years. We are
particularly indebted to Guillermo Avendaño-Franco, Saturo S. Kano, Bob Panoff, Guenter
Schneider, Paul Fink, Melanie Johnson, Al Stetz, Jon Maestri, David McIntyre, Shashikant
Phatak, Viktor Podolskiy, and Cherri Pancake. Our gratitude also goes to the reviewers Ali
Eskanarian, Franz J. Vesely, and John Mintmire for their thoughtful and valuable suggestions,
and to Ellen Foos of Princeton University Press for her excellent and understanding production
work. Heartfelt thanks goes to Vickie Kearn, our editor at Princeton University Press, for her
encouragement, insight, and efforts to keep this project alive in various ways.
In spite of everyone’s best efforts, there are still errors and confusing statements for which
we are responsible.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PREFACE vii
Finally, we extend our gratitude to the wives, Jan and Lucia, whose reliable support and
encouragement are lovingly accepted, as always.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter One
Computational Science Basics
Some people spend their entire lives reading but never get beyond reading the words on the page;
they don’t understand that the words are merely stepping stones placed across a fast-flowing river,
and the reason they’re there is so that we can reach the farther shore; it’s the other side that matters.
—José Saramago
As an introduction to the eBook to follow, we start this chapter with a discussion of the soft-
ware needed to utilize the various multimedia materials it contains, and the symbols used in the
text to indicate and activate these features. Then we give a description of how computational
physics (CP) fits into the broader field of computational science, and what topics constitue the
contents of CP. We get down to basics by examining computing languages, number represen-
tations, and programming. Related topics dealing with hardware basics are found in Chapter
14, “High-Performance Computing Hardware, Tuning, and Parallel Computing.”
1.1 SOFTWARE NEEDED TO USE THIS EBOOK
This eBook is designed to be read as a hyperlinked portable document format (pdf) file. So
the first thing you need is a pdf reader. The default reader is Adobe Reader 9 or later, which
is free, but we recommend Adobe Reader Pro 9 or latter so that you will be able to write
comments in the book and customize it in other ways. We have developed the eBook on
Windows operating systems, but have tested it with Macs and Linux. (However, some of the
pdf links to executeable programs may not work with Linux and Macs, in which case you can
just copy, paste and run the codes in IDLE.)
Because this document links to HTML pages, you will also need a Web browser such
as Firefox, Chrome, Internet Explore, or Safari. In fact, you may want to have two, since if
something like an applet does not work well in one, you can try another. If you want to look
at the xml version of equations, then you need to have your operating system set up so that
files ending in .xml are opened by Mozilla. Some of the linked Web pages load other media
in turn, and some require Adobe Flash Player (free), Java Run Time Environment or JRE (also
free), and a sound player. There are also a good number of mpg/mpeg animations included in
the text, and so you will need an mpeg player. Mpeg and sound players are packaged with
most operating systems, with free ones available from Real and Apple (Quicktime). The movie
players usually play sound as well.
The codes in the text employ Python 2.5 and the Python packages matplotlib, numpy,
and Visual 3 (the latter including the Python editor IDLE). These are all free, but they have
to be downloaded and then install in a particular order (instructions in Appendix??). As we
write, there is a new Python 2.6 out, but the libraries do not run well with it yet, and so we
recommend waiting.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
2 CHAPTER 1
1.2 USING THE FEATURES OF THIS EBOOK
You can read this book just as you might a paper one. However, we recommend that you take
advantage of its multimedia features as an assist to your learning. Although studies show
that different people learn in different ways, many learners benefit from experiencing multiple
approaches to a subject.
As in Web documents, this eBook contains links to objects signified by words in blue.
For example, clicking on 1.1 in Figure 1.1, will jump you to the specified figure (actually to
the caption, which is above the figure). Equations, tables, listings, pages and chapter sections
have similar links throughout the text and in the table of Contents and Index. To get back to the
page from whence you came, the easiest thing is to have Acrobat’s Previous View (backarrow)
button activated (View/Toolbars/More Tools/Previous View or Page Navigation Toolbar), and then to
use it. Alternatively, on Windows you can Alt plus←, or right click on the page you are viewing
with your mouse and select Previous View. In either case, you should be duly transported1. If
you are using Acrobat Pro, an additional two useful options when you right click your mouse
is Add Sticky Notes and Add Bookmark, both useful for personalizing the text. Although links
to other parts of this document should not illicit any complaints from Acrobat, if a link takes
you outside of pdf pages, say to a Web page or to a movie file, then Acrobat may ask your
permission before proceeding. Furthermore, you may need to modify some of the Preferences
in Acrobat relating to Trust so that it will be easier to open external links.
At the beginning of each chapter there is a table indicating which video lectures, applets
and animations are available for that chapter (we have delayed that table in this chapter so
we can explain it first). The names of the video lectures are links, for example, Introduction
to Computational Physics, where the small image of the lecturer in the margin indicates a
lecture. These links open a Web page within a browser with all the lecture components. There
is a window showing the lecturer sitting for an office hour, another window with annotated
slides synchronized to the lecture, a linked table of contents to that lecture, and video controls
that let you stop and jump around. Go ahead and try it! We suggest that you first read the text
before attending lecture, but feel free to find whatever combination works best for you. At the
moment, lectures are available for more than half of the text and we are working at finishing
the rest (see RHL’s Web pages for latest lectures).
Applets are small application programs written in Java that run through a Java-enabled
Web browser. The user does not deal with the code directly, but rather interacts with it via
buttons and sliders on the screen. This means that the reader does not have to know anything
at all about Java to run the applet (in fact, visual programming of applets is so complicated
that we do not recommend looking at the source unless you want to learn how to write applets.
We use the applets to illustrate the results to be expected for projects in the book, or to help
in understanding some concepts. Usually we just give the name of the Applet as a link, such
as Chaotic Scattering, although sometimes we place the link in a marginal icon, such as here.
Click on the link or the “Applet” icon to initiate the applet, noting that it may take some time
to load a browser and start the applet.
Code listings are presented with the codes formatted within a shaded box. Key words are
in italics and comments on the right, for example, Listing 1.1 (where 1.1 is a link). Note that
we have structured the codes so that a line is skipped before major elements like functions, and
that indentations indicate structures essential in Python. However, in order to conserve space,
sometimes we do not insert as many blank lines as we should, and sometimes we place several
1On a Mac right clicking is accomplished by Control + click.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
COMPUTATIONAL SCIENCE BASICS 3
commands on one line.
Listing 1.1 A sample code, LaplaceLine.py. 
""" LaplaceLine.py: Solution of Laplace’s eqtn with 3D matplot """
from numpy i m p o r t ∗ ; i m p o r t p y l a b as p ; i m p o r t m a t p l o t l i b . axes3d as p3
p r i n t ("Initializing" )
Nmax = 100 ; N i t e r = 7 0 ; V = z e r o s ( ( Nmax , Nmax) , f l o a t ) # f l o a t maybe F l o a t
p r i n t "Working hard, wait for the figure while I count to 60"
f o r k i n r a n g e ( 0 , Nmax−1) : V[ k , 0 ] = 100 .0 # l i n e a t 100V
f o r i t e r i n r a n g e ( N i t e r ) : # i t e r a t i o n s ove r a l g o r i t h m
i f i t e r %10 == 0 : p r i n t i t e r
f o r i i n r a n g e ( 1 , Nmax−2) :
f o r j i n r a n g e ( 1 , Nmax−2) : V[ i , j ] = 0 . 2 5∗ (V[ i +1 , j ]+V[ i−1, j ]+V[ i , j +1]+V[ i , j −1])
x = r a n g e ( 0 , Nmax−1, 2 ) ; y = r a n g e ( 0 , 50 , 2 ) # p l o t e v e r y o t h e r p o i n t
X, Y = p . meshgr id ( x , y )
d e f f u n c t z (V) : # F u n c t i o n r e t u r n s V( x , y )
z = V[X,Y]
r e t u r n z
Z = f u n c t z (V)
f i g = p . f i g u r e ( ) # C r e a t e f i g u r e
ax = p3 . Axes3D ( f i g ) # p l o t axes
ax . p l o t w i r e f r a m e (X, Y, Z , c o l o r = ’r’ ) # r e d w i r e f r a m e
ax . s e t x l a b e l (’X’ ) # l a b e l axes
ax . s e t y l a b e l (’Y’ )
ax . s e t z l a b e l (’Potential’ )
p . show ( ) # d i s p l a y f i g , c l o s e s h e l l t o q u i t
While these listings may look great, their formatting makes them inappropriate for cutting and
pasting. If you want to cut and paste a code, you can go to the Codes directory and copy it from
there, or you can take note of the icon in the margin next to the code. If you click on this
icon, you will open up an HTML (Web) page in a browser containing the code in a form that
you can copy and paste. You can then run or modify the code.
If you go back to this same code listing, you will notice an image of a python in the
margin. On Windows computers, and if you have Python installed, clicking on the python icon
will execute the Python source code and present the output on your screen. (Before trying it,
please note that this may take some time the first time you try it as Python gets loaded and
linked in, however, it will be faster after that.) Why not try it now? Doing this on Macs and
Linux machines may load the code but may not execute it, in which case you can do that with
IDLE. For the LaplaceLine.py code given here, a surface plot of the electric potential V (x, y)
will appear. Grabbing this plot with your left mouse button will rotate it in 3-D space. Grabbing
this plot with your right mouse button will zoom it in or out. The buttons on the bottom of the
window present further options for viewing and saving the plot. As is true for the listing, the
equations in this document may look great, but the pdf formatting interferes with the ability
to communicate their content to software and people. For instance, it may be very helpful to
be able to take an equation from the text and process it in a symbolic manipulation program
such as Maple, Mathematica or Sage, or feed it to a reader that can speak the equation for
the visually impaired. Having a MathML or xml version of the entire text (our original plan)
would permit this, but very few people would have the software set up to read it. So our present
compromise is to link in xml versions of many key equations to the equations presented in this
pdf document. For example, clicking on the xml icon to the right of the equation below opens
up a browser (which should be Mozilla Firefox for a proper view) which displays the same
equation based on an xml source file. (On some Acrobat readers, you may need to left-click on
the icon and tell Acrobat to open a browser rather than just try to read the xml directly.) Try it.
xmll
Nfix = sign× (αn2n + αn−12n−1 + · · ·+ α020 + · · ·+ α−m2−m).
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
4 CHAPTER 1
Figure 1.1 Simulation has been added to experiment and theory as a basic approach of science and its search for
underlying truths. This is the modern problem-solving paradigm followed in this book.
Experiment
data
Theory
Model
Evaluate & Explore
Method
analytic
numeric
Implementation
program
tool set
Assess
visualize
think
Truth
Simulation
data
Once you have the equation in the browser window, you can view the xml source and port the
xml to other programs.
Even though we try to be careful to define each term the first time it is used, we
also have included a Glossary in Appendix A for reference. You will find various specialized
words within this eBook linked to the glossary. These links are denoted by the word in blue
and a speaker icon that looks like: “An algorithm .” Clicking on the word will take you to
the Glossary, while clicking on the speaker will read the definition to you (just close your eyes
and enjoy the mellifluous tones of a vintage Bronx accent).
The text also uses various symbols and fonts to help clarify the type of material being
dealt with. These include:
in the margin Material on the CD
 Optional material
at line’s end End of exercise or problem
Monospace font Words as they would appear on a computer screen
Italic font Note to reader at beginning of chapter
what’s to follow
Sans serif font Program commands from drop-down menus
We also indicate a user–computer dialog via three different fonts on a line:
Monospace computer’s output > Bold monospace user’s command Comments
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
COMPUTATIONAL SCIENCE BASICS 5
Figure 1.2 A representation of the multidisciplinary nature of computational physics both as an overlap of physics,
applied mathematics, and computer science and as a bridge among them.
C
P
Physics
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures (Click on links to see video lectures.)
Name Sections Name Sections
Introduction to Computational Physics 1.2 Computing Basics 1.3-1.4
Number Representations 1.5 IEEE Floating Point 1.5–1.6
Machine Precision (IEEE) 1.53–1.6
1.3 COMPUTATIONAL PHYSICS AND COMPUTATIONAL SCIENCE
This book adopts the view that CP is a subfield of computational science. This means that CP is
a multidisciplinary subject combining aspects of physics, applied mathematics, and computer
science (CS) (Figure 1.2), with the aim of solving realistic physics problems. Other com-
putational sciences replace the physics with biology, chemistry, engineering, and so on, and
together face grand challenge problems such as
Climate prediction Materials science Structural biology
Superconductivity Semiconductor design Drug design
Human genome Quantum chromodynamics Turbulence
Speech and vision Relativistic astrophysics Vehicle dynamics
Nuclear fusion Combustion systems Oil and gas recovery
Ocean science Vehicle signature Undersea surveillance
Whereas related, computational science is not computer science. Computer science studies
computing for its own intrinsic interest and develops the hardware and software tools that com-
putational scientists use. Likewise, applied mathematics develops and studies the algorithms
that computational scientists use. As much as we too find math and computer science inter-
esting for their own sakes, our focus is on solving physical problems; we need to understand
the CS and math tools well enough to be able to solve our problems correctly. As CP has
matured, we have come to realize that it is more than the overlap of physics, computer science,
and mathematics (Figure 1.2). It is also a bridge among them (the central region in Figure 1.2)
containing core elements of it own, such as computational tools and methods. To us, CP’s
commonality of tools and a problem-solving mindset draws it toward the other computational
sciences and away from the subspecialization found in so much of physics.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
6 CHAPTER 1
Figure 1.3 A concept map of the Basic Computational Science Topics covered in this book (click on figure for
larger and linked view). The green hexagons indicate major subjects, the pink ellipses indicate subareas,
and the blue triangles indicate subjects contained on another map. The round bubbles are used to group
general areas. In the stand alone “content” map linked to this concept map, each concept is linked to
corresponding content within the text.
CP
Computational
Science Intro Programming
Representing
Numbers
Errors
Monte Carlo
Visualization Tools
Object Oriented
IntegrationDifferentiationSearching
Hardware
Parallel
Tune CP
CP
Adv
CP
ODE
CP
Adv Viz
Thermal
CP
Optional
Basic Computational Topics
A Survey of Computational Physics
eBook
In order to emphasize our computational science focus, to the extent possible, we present
the subjects in this book in the form of a problem to solve, with the components that consti-
tute the solution separated according to the scientific problem-solving paradigm (Figure 1.1
left). Traditionally, physics employs both experimental and theoretical approaches to discover
scientific truth (Figure 1.1 right). Being able to transform a theory into an algorithm requires
significant theoretical insight, detailed physical and mathematical understanding, and a mas-
tery of the art of programming. The actual debugging, testing, and organization of scientific
programs is analogous to experimentation, with the numerical simulations of nature being es-
sentially virtual experiments. The synthesis of numbers into generalizations, predictions, and
conclusions requires the insight and intuition common to both experimental and theoretical sci-
ence. In fact, the use of computation and simulation has now become so prevalent and essential
a part of the scientific process that many people believe that the scientific paradigm has been
extended to include simulation as an additional dimension (Figure 1.1).
1.4 VIEWING THE SUBJECTS TO BE COVERED
As we indicated in the preface, we believe that the ideal eBook text should let the reader
customize the selection and order of topics to best meet the reader’s background and needs.
Although this book is not quite there yet, we take a step in that direction with the concept
maps [VUE] shown in Figures 1.3 and 1.4, with an overview of the entire text presented as a
concept map in Figure 1.5. These maps display the contents of a document as a knowledge
field, roughly analogous to an electric field, with field lines connecting the various areas and
subareas.
Each of these concept map figures is linked to a stand alone map, which is not only
easier to read, but also has its concepts linked to sections of the text. Technically, this means
that these stand alone maps are content maps. This in turn gives the reader the option to use
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
COMPUTATIONAL SCIENCE BASICS 7
Figure 1.4 A concept map of the Computational Physics Topics covered in this book (click on it for larger and
linked view). The green hexagons indicate major subjects, the pink ellipses indicate subareas, and
the blue triangles indicate subjects contained on another map. The round bubbles are used to group
general areas. In the stand alone “content” map linked to this concept map, each concept is linked to
corresponding content within the text.
Shocks
Solitons
Differential Eqtns
Linear Alg
Ordinary
Partial
df/dx
Basics
After ODE
Fractals
Nonlinear
Systems
Chaos
Discrete
Continuous
MonteC
Thermal
 Simulations
Basics
Quantum Path
Integrals
Metropolis
Classes
Libes
N-D Search
Search
Basics
Oscilator
 EM  
elliptic
Heat  
parabolic
Waves
hyperbolic
Fluids
nonlinear finite
element
Quantum 
Eigen
Molecular
Dynamics
Verlet
Integral
Eqtns
Integrate
Basics
Eigen
Bound States
Singular
Scattering
High Perform
Computing
Hdware
Basics
Hdware
Parallel
Tuning
Adv
Libes
Data Anal
Fitting
Fourier
Wavelets
Basics
Adv Viz
Viz
CP Topics
A Survey of Computational Physics
eBook
Matrices
Matrices
the content map and its links as a guide to read the text2.
You will find in the Basic Topics concept map in Figure 1.3, the elementary software
tools needed for computational science. Notice how the general areas (bubbles) are linked
together with arrows indicating the recommended order in which the subjects should be fol-
lowed. However, the Hardware bubble’s link is labeled “Optional” because although it fits in
logically at the beginning of a course, some readers or instructors may prefer to cover it later
with High Performance Computing as part of the CP topics. Likewise, the “Searching” bubble
sits like an island unconnected to the mainland. It is an essential topic that fits in well to an in-
troductory course, but does not require a specific logical placement. You as a reader, of course,
are free to construct your own path through the materials. In contrast to the Basic Topics Map,
the CP Topics map in Figure 1.4 has many more topics, but fewer directed field lines among
them. This displays the greater freedom in which these topics may be approached, although
the essential prerequisites from the basic topics are indicated. Notice also in Figure 1.4 that
much of the CP we present appears to involve differential equations, both ordinary and par-
tial. In practise, many of those applications employ linear algebra libraries, for which links
are indicated, as well as scientific libraries, also include under linear algebra. In contrast, this
text is rather unique in also covering some topics in integral equations and their applications
in physics. While not seen in many physics courses, it is an important topic in research and
applications and we suggest that at least the Eigen Bound State problem be approached.
A more traditional way to view the materials in this text is in terms of its use in courses.
In our classes [CPUG] we use approximately the first third of the text, with its emphasis on
computing tools, for a course in scientific computing (after students have acquired familiarity
2Although many people find much value in these maps, others find them confusing, and especially at first. In any case, it may
take some study before the reader “sees” the concept connections.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
8 CHAPTER 1
Figure 1.5 A concept map overview of this entire eBook (click on it for larger and linked view). Each green angular
box is linked to an appropriate concept map.
How to Read this eBook
Required Computer
Tools
Basic Scientific
Computing  Topics
Computational Physics
Topics
Preq
Basic CourseCP Course
Table 1.1 Topics for one quarter (10 Weeks) of a scientific computing course. Units are indicated by I, II, and III,
and the visualization, here spread out into several laboratory periods, can be completed in one. Options:
week 3 on visualization; postpone matrix computing; postpone hardware basics; devote a week to OOP;
include hardware basics in week 2.
Week Topics Chapter Week Topics Chapter
1 OS tools, limits 1, (4) 6 Matrices, N-D search 8I
2 Errors, visualization 2, 3 7 Data fitting 8II
3 Monte Carlo, visualization 5, 3 8 ODE oscillations 9I
4 Integration, visualization 6, (3) 9 ODE eigenvalues 9II
5 Derivatives,searching 7I, II 10 Hardware basics 14I, III
with a compiled language). Typical topics covered in the 10 weeks of such a course are given
in Table 1.1. Some options are indicated in the caption, and, depending upon the background
of the students, other topics may be included or substituted. The latter two-thirds of the text
includes more physics, and, indeed, we use it for a two-quarter (20-week) course in computa-
tional physics. Typical topics covered for each term are given in Table 1.4. What with many
of the topics being research level, we suspect that these materials can easily be used for a full
year’s course as well.
For these materials to contribute to a successful learning experience, we assume that the
reader will work through the problem at the beginning of each chapter or unit. This entails
studying the text, writing, debugging and running programs, visualizing the results, and then
expressing in words what has been done and what can be concluded. Further exploring is
encouraged. Although we recognize that programming is a valuable skill for scientists, we
also know that it is incredibly exacting and time-consuming. In order to lighten the workload
somewhat, we provide “bare bones” programs in the text and on the CD. We recommend that
these be used as guides for the reader’s own programs or tested and extended to solve the
problem at hand. As part of this approach we suggest that the learner write up a mini lab report
for each problem containing
Equations solved Numerical method Code listing
Visualization Discussion Critique
The report should be an executive summary of the type given to a boss or manager; make it
clear that you understand the materials but do not waste everyone’s time.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
COMPUTATIONAL SCIENCE BASICS 9
Table 1.2 Topics for two quarters (20 Weeks) of a computational physics course. Units are indicated by I, II, and
III. Options: include OpenDX visualization (§3.5, Appendix C); include multiresolution analysis (11II);
include FFT (10III) in place of wavelets; include FFT (10III) in place of parallel computing; substitute
Feynman path integrals (15III) for integral equations (20); add several weeks on CFD (hard); substitute
coupled predator-prey (12III) for heat PDE (17III); include quantum wave packets (18II) in place of CFD;
include finite element method (17II) in place of heat PDE.
Computational Physics I Computational Physics II
Week Topics Chapter Week Topics Chapter
1 Nonlinear ODEs 9I, II 1 Ising model, Metropolis 15I
algorithm
2 Chaotic scattering 9III 2 Molecular dynamics 16
3 Fourier analysis, filters 10I, II 3 Project completions —
4 Wavelet analysis 11I 4 Laplace and Poisson PDEs 17I
5 Nonlinear maps 12I 5 Heat PDE 17III
6 Chaotic/double pendulum 12II 6 Waves, catenary, friction 18I
7 Project completion 12I, II 7 Shocks and solitons 19I
8 Fractals, growth 13 8 Fluid dynamics 19 II
9 Parallel computing, MPI 14II 9 Quantum integral equations 20I (II)
10 More parallel computing 14III 10 Feynman path integration 15III
One of the most rewarding uses of computers is visualizing and analyzing the results of
calculations with 2-D and 3-D plots, with color, and with animation. This assists in the debug-
ging process, hastens the development of physical and mathematical intuition, and increases
the enjoyment of the work. It is essential that you learn to use visualization tools as soon as
possible, and so in Chapter 3, “Visualization Tools,” and C we describe a number of free visu-
alization tools that we use and recommend. We include many figures showing visualizations
(unfortunately just in gray scale), with color versions on the CD.
1.5 MAKING COMPUTERS OBEY; LANGUAGES (THEORY)
Computers are incredibly fast, accurate, and stupid; humans are incredibly slow,
inaccurate, and brilliant; together they are powerful beyond imagination.
— Albert Einstein
As anthropomorphic as your view of your computer may be, keep in mind that computers
always do exactly as they are told. This means that you must tell them exactly everything they
have to do. Of course the programs you run may have such convoluted logic that you may not
have the endurance to figure out the details of what you have told the computer to do, but it is
always possible in principle. So your first problem is to obtain enough understanding so that
you feel well enough in control, no matter how illusionary, to figure out what the computer is
doing.
Before you tell the computer to obey your orders, you need to understand that life is
not simple for computers. The instructions they understand are in a basic machine language
3 that tells the hardware to do things like move a number stored in one memory location to
3The Beginner’s All-Purpose Symbolic Instruction Code (BASIC) programming language of the original PCs should not be
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
10 CHAPTER 1
Figure 1.6 A schematic view of a computer’s kernel and shells.
Program development
GUI
Shell
Utilities
Kernel
cp rm
del
windows
appletviewer
Hardware
another location or to do some simple binary arithmetic. Very few computational scientists talk
to computers in a language computers can understand. When writing and running programs,
we usually communicate to the computer through shells, in high-level languages ( Python,
Java, Fortran, C), or through problem-solving environments (Maple, Mathematica, and Matlab).
Eventually these commands or programs are translated into the basic machine language that
the hardware understands.
A shell is a command-line interpreter, that is, a set of small programs run by a
computer that respond to the commands (the names of the programs) that you key in. Usually
you open a special window to access the shell, and this window is called a shell as well.
It is helpful to think of these shells as the outer layers of the computer’s operating system
(OS) (Figure 1.5), within which lies a kernel of elementary operations. (The user seldom
interacts directly with the kernel, except possibly when installing programs or when building
an operating system from scratch.) It is the job of the shell to run programs, compilers, and
utilities that do things like copying files. There can be different types of shells on a single
computer or multiple copies of the same shell running at the same time.
Operating systems have names such as Unix, Linux, DOS, MacOS, and MS Windows.
The operating system is a group of programs used by the computer to communicate with
users and devices, to store and read data, and to execute programs. Under Unix and Linux, the
OS tells the computer what to do in an elementary way, while Windows includes various graph-
ical elements as part of the operating system (this increases speed at the cost of complexity).
The OS views you, other devices, and programs as input data for it to process; in many ways,
it is the indispensable office manager. While all this may seem complicated, the purpose of the
OS is to let the computer do the nitty-gritty work so that you can think higher-level thoughts
and communicate with the computer in something closer to your normal everyday language.
When you submit a program to your computer in a high-level language , the com-
puter uses a compiler to process it. The compiler is another program that treats your
program as a foreign language and uses a built-in dictionary and set of rules to translate it into
basic machine language. As you can probably imagine, the final set of instructions is quite
detailed and long and the compiler may make several passes through your program to decipher
your logic and translate it into a fast code. The translated statements form an object or
confused with basic machine language.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
COMPUTATIONAL SCIENCE BASICS 11
compiled code, and when linked together with other needed subprograms, form a load
module. A load module is a complete set of machine language instructions that can be
loaded into the computer’s memory and read, understood, and followed by the computer.
Languages such as Fortran and C use compilers to read your entire program and
then translate it into basic machine instructions. Languages such as BASIC and Maple translate
each line of your program as it is entered. Compiled languages usually lead to more efficient
programs and permit the use of vast subprogram libraries. Interpreted languages give a more
immediate response to the user and thereby appear “friendlier.” The Python and Java languages
are a mix of the two. When you first compile your program, Python interprets it into an in-
termediate, universal byte code which gets stored as a PYC (or PYO) file. This file can
be transported to and used on other computers, although not with different versions of Python.
Then, when you run your program, Python recompiles the byte code into a machine-specific
compiled code.
1.6 PROGRAMMING WARMUP
Before we go on to serious work, we want to ensure that your local computer is working right
for you. Assume that calculators have not yet been invented and that you need a program to
calculate the area of a circle. Rather than use any specific language, write that program in
pseudocode that can be converted to your favorite language later. The first program tells the
computer:4 
C a l c u l a t e a r e a o f c i r c l e # Do t h i s compute r !
This program cannot really work because it does not tell the computer which circle to consider
and what to do with the area. A better program would be 
r e a d r a d i u s # I n p u t
c a l c u l a t e a r e a o f c i r c l e # Numerics
p r i n t a r e a # Outpu t
The instruction calculate area of circle has no meaning in most computer languages, so
we need to specify an algorithm, that is, a set of rules for the computer to follow: 
r e a d r a d i u s # I n p u t
PI = 3 .141593 # S e t c o n s t a n t
a r e a = PI ∗ r ∗ r # Algo r i t hm
p r i n t a r e a # Outpu t
This is a better program, and so let’s see how to implement it in Python (other language
versions are on the CD). In Listing 1.2 we give a Python version of our Area program. This is
a simple program that outputs to the screen, with its input built into the program.
Listing 1.2 The program Area.py outputs to the screen, with its input built into the program. 
# Area . py : Area o f a c i r c l e , s i m p l e program
from v i s u a l i m p o r t ∗
modelN = 1
r a d i u s = 1 .
4Comments placed in the field to the right are for your information and not for the computer to act upon.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
12 CHAPTER 1
c i rcum = 2 . ∗ math . p i ∗ r a d i u s
a r e a = r a d i u s ∗ r a d i u s ∗ math . p i
p r i n t ’Program number = ’ , modelN
p r i n t ’Radius = ’ , r a d i u s
p r i n t ’Circumference = ’ , c i r cum
p r i n t ’Area = ’ , a r e a
#OUTPUT:
’’’
Program number = 1
Radius = 1.0
Circumference = 6.28318530718
Area = 3.14159265359
’’’
p r i n t "Press a character to finish"
s= r a w i n p u t ( )
1.6.1 Structured Program Design
Programming is a written art that blends elements of science, mathematics, and computer sci-
ence into a set of instructions that permit a computer to accomplish a desired task. Now that
we are getting into the program-writing business, you will benefit from understanding the over-
all structures that you should be building into your programs, in addition to the grammar of a
computer language. As with other arts, we suggest that until you know better, you follow some
simple rules. A good program should
• Give the correct answers.
• Be clear and easy to read, with the action of each part easy to analyze.
• Document itself for the sake of readers and the programmer.
• Be easy to use.
• Be easy to modify and robust enough to keep giving correct answers after modification.
• Be passed on to others to use and develop further.
One attraction of object-oriented programming (Chapter 4; “Object-Oriented Programs:
Impedance & Batons”) is that it enforces these rules automatically. An elementary way to
make any program clearer is to structure it with indentation, skipped lines, and braces
placed strategically. This is done to provide visual clues to the function of the different pro-
gram parts (the “structures” in structured programming). Regardless of the fact that compilers
ignore these visual clues, human readers are aided by having a program that not only looks
good but also has its different logical parts visually evident. Even though the space limita-
tions of a printed page keep us from inserting as many blank lines as we would prefer, we
recommend that you do as we say and not as we do!
In Figure 1.7 we present basic and detailed flowcharts that illustrate a possible program
for computing projectile motion. A flowchart is not meant to be a detailed description of a
program but instead is a graphical aid to help visualize its logical flow. As such, it is inde-
pendent of a specific computer language and is useful for developing and understanding the
basic structure of a program. We recommend that you draw a flowchart or (second best) write
a pseudocode before you write a program. Pseudocode is like a text version of a flowchart
that leaves out details and instead focuses on the logic and structures: 
S t o r e g , Vo , and t h e t a
C a l c u l a t e R and T
Begin t ime loop
P r i n t o u t "not yet fired" i f t < 0
P r i n t o u t "grounded" i f t > T
C a l c u l a t e , p r i n t x ( t ) and y ( t )
P r i n t o u t e r r o r message i f x > R , y > H
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
COMPUTATIONAL SCIENCE BASICS 13
Figure 1.7 A flowchart illustrating a program to compute projectile motion. On the left are the basic components
of the program, and on the right are some of its details. When writing a program, first map out the
basic components, then decide upon the structures, and finally fill in the details. This is called top-down
programming.
Initialize Constants
Basic Calculations
Loop over time
End
Store g, V0, θ
Calculate R, T
Loop over time
Calculate x(t), y(t)
Print x, y “Not Yet Fired”
End
“Grounded”
0 < t < T ?
t < 0 ?
NY
NY
End t ime loop End program
1.6.2 Shells, Editors, and Execution
1. To gain some experience with your computer system, use an editor to enter the program
Area.py that computes the area of a circle (yes, we know you can copy it from the CD,
but you may need some exercise before getting to work). Then write your file to disk by
saving it in your home (personal) directory (we advise having a separate subdirectory for
each week). Note: For those who are familiar with Python, you may want to enter the
program AreaFormatted.py instead (described in a later section) that uses commands
that produce formatted output.
2. Compile and execute the appropriate version of Area.py.
3. Change the program so that it computes the volume 43πr
3 of a sphere. Then write
your file to disk by saving it in your home (personal) directory and giving it the name
AreaMod.py.
4. Compile and execute AreaMod (remember that the file name and class name must agree).
5. Check that your changes are correct by running a number of trial cases. A good input
datum is r = 1 because then A = π. Then try r = 10.
6. Experiment with your program. For example, see what happens if you leave out decimal
points in the assignment statement for r, if you assign r equal to a blank, or if you
assign a letter to r. Remember, it is unlikely that you will “break” anything by making a
mistake, and it is good to see how the computer responds when under stress.
7. Revise Area.py so that it takes input from a file name that you have made up, then writes
in a different format to another file you have created, and then reads from the latter file.
8. See what happens when the data type used for output does not match the type of data in
the file (e.g., data are doubles, but read in as ints).
9. Revise AreaMod so that it uses a main method (which does the input and output) and a
separate method for the calculation. Check that you obtain the same answers as before.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
14 CHAPTER 1
Listing 1.3 AreaFormatted.py uses output statements to control the format of the outputted numbers. 
# AreaScanner : examples o f use o f f o r m a t e d o u t p u t
# and r e a d i n g i n p u t from t h e keyboard . Also : I n p u t / Outpu t wi th f i l e s
from v i s u a l i m p o r t ∗
name= r a w i n p u t ( ’Key in your name: ’ ) # r a w i n p u t i s good f o r s t r i n g s
p r i n t "Hi " , name
r a d i u s = i n p u t (’Enter a radius: ’ ) # t h i s i n p u t works wi th n u m e r i c a l v a l u e s
p r i n t ’you entered radius= %8.5f’%r a d i u s # f o r m a t t e d o u t p u t
name= r a w i n p u t (’Key in another name: ’ ) # r a w i n p u t i s good f o r s t r i n g s
r a d i u s = i n p u t (’Enter a radius: ’ )
p r i n t ’Enter new name and r in file Name.dat’
i n p f i l e =open (’Name.dat’ ,’r’ ) # t o r e a d from f i l e Name . d a t
f o r l i n e i n i n p f i l e :
l i n e = l i n e . s p l i t ( ) # s p l i t s components o f l i n e
name= l i n e [ 0 ] # f i r s t e n t r y i n t h e l i s t
p r i n t " Hi %10s" %(name ) # p r i n t Hi p l u s f i r s t e n t r y
r = f l o a t ( l i n e [ 1 ] ) # second e n t r y c o n v e r t t o f l o a t ( i t i s a s t r i n g )
p r i n t " r = %13.5f" %( r ) # c o n v e r t s x t o f l o a t and p r i n t i t
i n p f i l e . c l o s e ( )
A=math . p i∗ r∗∗2 # use r a d i u s t o f i n d c i r c l e s ’s area
print "Done, look in A.dat\n"
outfile=open(’A. d a t’,’w+ t’)
outfile.write( ’ r = %13.5 f\n’%(r))
outfile.write(’A = %13.5 f\n’%(A))
outfile.close()
print ’ r = %13.5 f’%(r) #screen output
print ’A = %13.5 f’%(A)
print ’Now example o f i n t e g e r i n p u t ’
age=int(input (’Now key i n your age as an i n t e g e r : ’))
print "age: %4d years old, you don’ t l ook i t !\n"%(age)
print "P r e s s a c h a r a c t e r t o f i n i s h "
s=raw_input()
1.6.3 Formatted I/O in Python
The simplest I/O with Python is outputting to the screen with the print command (as seen in
Area.py, Listing 1.2), and inputting from the keyboard with the input command (as seen in
AreaFormatted.py, Listing 1.3). We also see in AreaFormatted.py that we can input strings
(literal numbers and letters) by either enclosing the string in quotes (single or double), or using
the raw input command without quotes. To print a string with print, place the string in
quotes.
The simplest output prints the value of a float just by giving its name:
print ’eps = ’, eps Output float in Python’s default format
This uses Python’s default format, which tends to vary depending on the precision of the num-
ber being printed. As an alternative, you can control the format of your output. For floats you
need to specify two things. First, how many digits (places) after the decimal point is desired,
and second, how many spaces overall should be used for the number:
print("x = %6.3f, Pi = %9.6f, Age = %d \n") % (x, math.pi, age)
print "x = %6.3f, %(x), "Pi = %9.6f," %(math.pi), "Age = %d "%(age)," \n"
x = 12.345, Pi = 3.141593, Age = 39 Output from either
Here the %6.3f formats a float (which is a double in Python) to be printed in fixed-point no-
tation (the f) with 3 places after the decimal point and with 6 places overall (1 place for the
decimal point, 1 for the sign, 1 for the digit before the decimal point, and 3 for the decimal).
The directive %9.6f has 6 digits after the decimal place and 9 overall.
To print an integer we need specify only the total number of digits (there is no decimal
part), and we do that with the %d (d for digits) format. The % symbol in these output formats
indicates a conversion from the computer’s internal format to that used for output. Notice in
Listing 1.3 how we read from the keyboard, as well as from a file, and output to both screen
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
COMPUTATIONAL SCIENCE BASICS 15
and file. Beware that if you do not create the file Name.dat, the program will issue (“throw”)
an error message of the sort:
IOError: [Errno 2] No such file or directory: ’Name.dat’
Note that we have also use a %n directive here to indicate a new line. Other directives, some of
which are demonstrated in Directives.py in Listing 1.4 (and some of which like backspace
may not yet work right) are:
\" double quote \0NNN octal value NNN \\ backslash
\a alert (bell) \b backspace \c no further output
\f form feed \n new line \r carriage return
\t horizontal tab \v vertical tab %% a single %
Listing 1.4 The program Directives.py illustrates formatting via directives and escape characters. 
p r i n t "as hexadecimal b = %x "%(b ) # works h e x a d e c i m a l
p r i n t "learn \"Python\" " # use o f do ub l e q u o t e symbol
p r i n t "shows a backslash \\" # use o f \\
p r i n t ’use of single \’ quotes \’ ’ # p r i n t s i n g l e q u o t e s
1.6.4 I/O Redirection
Most programming environments assume that the standard (default) input is from the keyboard
and the standard output is to the computer screen. But you can change that. A simple way
to read from or write to a file is via command-line redirection from within the shell in which
you are running your program. (This must be a Unix or Windows shell and not the Python
shell since the former interpret system commands, while the latter interprets individual Python
statements.) For example, here we run the program Directives.py from a system shell and
redirect the output from the default screen to the file outfile.dat:
C:\PythonCodes> AreaFormatted.py > outfile.dat Redirect standard output
Likewise, we can redirect input from the default keyboard to the file infile.dat:
C:PythonCodes> AreaFormatted.py < infile.dat Redirect standard input
Or you can put them both together for complete redirection:
C:\PythonCodes> AreaFormatted.py < infile.dat > outfile.dat Redirect standard I/O
In Listing 1.4 we show a clever way to redirect the standard output by temporarily redefining
the operating system’s standard output to a file. Run it and see!
Listing 1.5 The program DirectivesOut.py redirects output to the file OutFile2.dat by temporarily renaming
the system’s standard output. 
p r i n t "as hexadecimal b = %x "%(b ) # works h e x a d e c i m a l
p r i n t "learn \"Python\" " # use o f do ub l e q u o t e symbol
p r i n t "shows a backslash \\" # use o f \\
p r i n t ’use of single \’ quotes \’ ’ # p r i n t s i n g l e q u o t e s
Listing 1.6 The program CommandLineArgs.py demonstrates how arguments can be transferred to a main pro-
gram via the command line. 
# CommandLineArgs . j a v a : Accep t s 3 o r 4 a rgumen t s from command l i n e , e . g . :
# j a v a CommandLineArgs a n I n t aDouble [ a S t r i n g ] .
# [ a S t r i n g ] i s o p t i o n a l f i l e n a m e . See CmdLineArgsDemo on CD f o r f u l l d o c u m e n t a t i o n
# W r i t t e n by Z l a t k o Dimcovic i n Java ∗ /
i m p o r t s y s
i n t P a r a m = 0 # Othe r v a l u e s OK
doubleParam = 0 . 0 ; # D e f a u l t s , a r g s o p t i o n a l
f i l e n a m e = "baseName"
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
16 CHAPTER 1
p r i n t ’there are %d arguments ’ % l e n ( s y s . a rgv )
p r i n t s y s . a rgv
i f l e n ( s y s . a rgv ) == 3 or l e n ( s y s . a rgv ) == 4 : # Demand 2 or 3 a r g s
i n t P a r a m = i n t ( s y s . a rgv [ 1 ] )
doubleParam = f l o a t ( s y s . a rgv [ 2 ] )
i f l e n ( s y s . a rgv ) == 4 :
f i l e n a m e = s y s . a rgv [ 3 ] # 4 t h argument f i l e n a m e
e l s e :
f i l e n a m e ="_i"+ s y s . a rgv [ 1 ] +"_d" + s y s . a rgv [ 2 ] +".dat"
# p r i n t in tPa ram , doubleParam , f i l e n a m e
e l s e :
p r i n t "\n\t Usage: java CmdLineArgs intParam doubleParam [file]"
p r i n t "\t 1st arg must be int, 2nd double (or int),"
p r i n t ’Input arguments: intParam (1st) = ’ , i n tPa ram ,’doubleParam (2nd) = ’ , doubleParam
i f l e n ( s y s . a rgv ) == 4 :
p r i n t "String input: " , f i l e n a m e
e l i f l e n ( s y s . a rgv ) == 3 :
p r i n t "No file, use" , f i l e n a m e
e l s e :
p r i n t "\n\t ERROR ! len(sys.argv) must be 3 or 4 \n"
1.6.5 Command-Line Input
Although we tend not to in our sample programs, you can also input data to your programs
from the command line or shell by specifying values for the argument of your main program
(or method). (Again we remind you that this would be the Unix or Windows shell which
accepts systems commands, and not the Python shell, which accepts only individual Python
statements.) Since main methods are still methods, they can take arguments (a parameter list)
and return values. Normally one would execute a Python program from a shell just by entering
the name of the file, for example, $ CommandLineArgs.py, or by double clicking on the file
name. However you may add arguments that get sent to the main program by including them
after the file name. As an example, the program CommandLineArgs.py in Listing ?? accepts
and then uses arguments from the command line
D:\PythonCodes> CommandLineArgs.py 2 1.0 TempFile Args: int, float, string
Here the main method is given an integer 2, a float 1.0, and a string TempFile, with the latter
to be used as a file name. Note that this program is not shy about telling you what you should
have done if you have forgotten to give it the arguments it expects. Further details are given
within the program.
Listing 1.7 FileCatchThrow.py reads from the file and handles the I/O exception. 
# F i l eCa tchThrow . py : throw , c a t c h IO e x c e p t i o n
# program wi th m i s t a k e t o s e e a c t i o n o f e x c e p t i o n
i m p o r t s y s
i m p o r t math
r = 2
c i rcum = 2.∗ math . p i∗ r # C a l c u l a t e c i rcum
A = math . p i∗ r∗∗2 # C a l c u l a t e A
t r y :
q = open ("ThrowCatch.dat" ,’w’ ) # i n t e n t i o n a l m i s t a k e ’r’ , b u t run wi th i t
# change ’r’ by ’w’ and run a g a i n
e x c e p t I O E r r o r :
p r i n t ’Cannot open file’
e l s e :
q . w r i t e ("r = %9.6f, length = %9.6f, A= %9.6f "%(r , c i rcum , A) )
q . c l o s e ( )
p r i n t ’output in ThrowCatch.out’
# c a t c h ( IOExcep t ion ex ){ex . p r i n t S t a c k T r a c e ( ) ; } # Catch
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
COMPUTATIONAL SCIENCE BASICS 17
Figure 1.8 A part of the automatic code documentation produced by the Python utility epydoc when used on the
program OOPbeats.py.
HTML
1.6.6 I/O Exceptions: FileCatchThrow.py
Exceptions occur when something goes wrong during the I/O process, such as not finding a
file, trying to read past the end of a file, or interruptions of the I/O. Dealing with exceptions is
important because it prevents the computer from freezing up and lets you know that there may
be a problem that needs attention. If, for example, a file is not found, then Python will create
an Exception object and pass it along (“throw exception”) to the program that called the main
method, and send the error message to you via the Python Shell. You gain control over these
exceptions by including the phrase:
except IOError
In Listing 1.7 we give an example of the use of the except IOError construct and of how a
program can deal with (catches) a thrown exception object. We see that the program has a
try structure, within which a file is opened, and then a catch structure, where the program
proceeds if an exception is thrown. The error is thus caught by the catch statement which
prints a statement to that effect.
1.6.7 Automatic Code Documentation 
There is a freely available package epydoc that automatically generates fancy-looking html or
pdf documentation of your Python program (have we mentioned that documentation is very
important for scientific computing and very ignored?). You can find epydoc at
http://epydoc.sourceforge.net/
where you need to pick the version appropriate for your operating system, download it, and
then install it. To use epydoc you include comments in your code containing the necessary
commands, and then process the comments with the separate epydoc application.
As an example, we use epydoc with the program OOPbeats.py, which you can find on
the CD. For organizations’s sake, we create a new subdirectory Docs in the directory containing
our Python codes, and place the output there. In this way we do not clutter our code directory
with html files and such. To run epydoc, you must go to the directory on your computer where
Python is stored and start up the graphical user interface (GUI) for epydoc. On our Windows
machine this is
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
18 CHAPTER 1
C:\Python25\Lib\site-packages\epydoc\gui.py
Go to the equivalent directory on your computer and execute gui.py by double clicking on it.
Now that you have a nice window, select File/New Project and then use the Browse menu to select
the file you wish to document (OOPbeats.py). A typical output is shown in Figure 1.8.
1.7 COMPUTER NUMBER REPRESENTATIONS (THEORY)
Computers may be powerful, but they are finite. A problem in computer design is how to
represent an arbitrary number using a finite amount of memory space and then how to deal
with the limitations arising from this representation. As a consequence of computer memories
being based on the magnetic or electronic realization of a spin pointing up or down, the most
elementary units of computer memory are the two binary integers (bits) 0 and 1. This means
that all numbers are stored in memory in binary form, that is, as long strings of zeros and
ones. As a consequence, N bits can store integers in the range [0, 2N ], yet because the sign
of the integer is represented by the first bit (a zero bit for positive numbers), the actual range
decreases to [0, 2N−1].
Long strings of zeros and ones are fine for computers but are awkward for users. Conse-
quently, binary strings are converted to octal, decimal, or hexadecimal numbers before
the results are communicated to people. Octal and hexadecimal numbers are nice because the
conversion loses no precision, but not all that nice because our decimal rules of arithmetic do
not work for them. Converting to decimal numbers makes the numbers easier for us to work
with, but unless the number is a power of 2, the process leads to a decrease in precision.
A description of a particular computer system normally states the word length, that
is, the number of bits used to store a number. The length is often expressed in bytes, with
1 byte ≡ 1 B def= 8 bits.
Memory and storage sizes are measured in bytes, kilobytes, megabytes, gigabytes, terabytes,
and petabytes (1015). Some care should be taken here by those who chose to compute sizes in
detail because K does not always mean 1000:
1 K def= 1 kB = 210 bytes = 1024 bytes.
This is often (and confusingly) compensated for when memory size is stated in K, for example,
512 K = 29 bytes = 524, 288 bytes× 1 K
1024 bytes
.
Conveniently, 1 byte is also the amount of memory needed to store a single letter like “a”,
which adds up to a typical printed page requiring ∼3 kB.
The memory chips in some older personal computers used 8-bit words. This meant that
the maximum integer was 27 = 128 (7 because 1 bit is used for the sign). Trying to store a
number larger than the hardware or software was designed for (overflow) was common on
these machines; it was sometimes accompanied by an informative error message and some-
times not. Using 64 bits permits integers in the range 1–263 ' 1019. While at first this may
seem like a large range, it really is not when compared to the range of sizes encountered in the
physical world. As a case in point, the ratio of the size of the universe to the size of a proton is
approximately 1041.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
COMPUTATIONAL SCIENCE BASICS 19
Figure 1.9 The limits of single-precision floating-point numbers and the consequences of exceeding these limits.
The hash marks represent the values of numbers that can be stored; storing a number in between these
values leads to truncation error. The shaded areas correspond to over- and underflow.
Underflow T
ru
n
c
a
ti
o
n
O
v
e
rf
lo
w
O
v
e
rf
lo
w
0
-10 10-10 +10+38 +38-45-45
1.7.1 IEEE Floating-Point Numbers
Real numbers are represented on computers in either fixed-point or floating-point notation.
Fixed-point notation can be used for numbers with a fixed number of places beyond the decimal
point (radix) or for integers. It has the advantages of being able to use two’s complement
arithmetic and being able to store integers exactly.5 In the fixed-point representation with N
bits and with a two’s complement format, a number is represented as
xmllNfix = sign× (αn2
n + αn−12n−1 + · · ·+ α020 + · · ·+ α−m2−m), (1.1)
where n+m = N − 2. That is, 1 bit is used to store the sign, with the remaining (N − 1) bits
used to store the αi values (the powers of 2 are understood). The particular values for N,m,
and n are machine-dependent. Integers are typically 4 bytes (32 bits) in length and in the range
−2147483648 ≤ 4-B integer ≤ 2147483647.
An advantage of the representation (1.1) is that you can count on all fixed-point numbers to
have the same absolute error of 2−m−1 [the term left off the right-hand end of (1.1)]. The
corresponding disadvantage is that small numbers (those for which the first string of α values
are zeros) have large relative errors. Because in the real world relative errors tend to be more
important than absolute ones, integers are used mainly for counting purposes and in special
applications (like banking).
Most scientific computations use double-precision floating-point numbers (64 b = 8 B).
The floating-point representation of numbers on computers is a binary version of what is
commonly known as scientific or engineering notation. For example, the speed of light
c = +2.99792458×10+8 m/s in scientific notation and +0.299792458×10+9 or 0.299795498
E09 m/s in engineering notation. In each of these cases, the number in front is called the man-
tissa and contains nine significant figures. The power to which 10 is raised is called the
exponent, with the plus sign included as a reminder that these numbers may be negative.
Floating-point numbers are stored on the computer as a concatenation (juxtaposition) of
the sign bit, the exponent, and the mantissa. Because only a finite number of bits are stored,
the set of floating-point numbers that the computer can store exactly, machine numbers (the
hash marks in Figure 1.9), is much smaller than the set of real numbers. In particular, machine
numbers have a maximum and a minimum (the shading in Figure 1.9). If you exceed the max-
5The two’s complement of a binary number is the value obtained by subtracting the number from 2N for an N -bit represen-
tation. Because this system represents negative numbers by the two’s complement of the absolute value of the number, additions
and subtractions can be made without the need to work with the sign of the number.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
20 CHAPTER 1
Table 1.3 The IEEE 754 Standard for Primitive Data Types.
Name Type Bits Bytes Range
boolean Logical 1 18 true or false
char String 16 2 ’\u0000’↔ ’\uFFFF’ (ISO Unicode characters)
byte Integer 8 1 −128↔ +127
short Integer 16 2 −32, 768↔ +32, 767
int Integer 32 4 −2, 147, 483, 648↔ +2, 147, 483, 647
long Integer 64 8 −9, 223, 372, 036, 854, 775, 808↔ 9, 223, 372, 036,
854, 775, 807
float Floating 32 4 ±1.401298× 10−45 ↔ ±3.402923× 10+38
double Floating 64 8 ±4.94065645841246544× 10−324 ↔
±1.7976931348623157× 10+308
imum, an error condition known as overflow occurs; if you fall below the minimum, an
error condition known as underflow occurs. In the latter case, the software and hardware
may be set up so that underflows are set to zero without your even being told. In contrast,
overflows usually halt execution.
The actual relation between what is stored in memory and the value of a floating-point
number is somewhat indirect, with there being a number of special cases and relations used over
the years. In fact, in the past each computer operating system and each computer language
contained its own standards for floating-point numbers. Different standards meant that the
same program running correctly on different computers could give different results. Even
though the results usually were only slightly different, the user could never be sure if the lack
of reproducibility of a test case was due to the particular computer being used or to an error in
the program’s implementation.
In 1987, the Institute of Electrical and Electronics Engineers (IEEE) and the American
National Standards Institute (ANSI) adopted the IEEE 754 standard for floating-point arith-
metic. When the standard is followed, you can expect the primitive data types to have the
precision and ranges given in Table 1.3. In addition, when computers and software adhere to
this standard, and most do now, you are guaranteed that your program will produce identical
results on different computers. However, because the IEEE standard may not produce the most
efficient code or the highest accuracy for a particular computer, sometimes you may have to
invoke compiler options to demand that the IEEE standard be strictly followed for your test
cases. After you know that the code is okay, you may want to run with whatever gives the
greatest speed and precision.
There are actually a number of components in the IEEE standard, and different computer
or chip manufacturers may adhere to only some of them. Furthermore, Python may not follow
all as it develops, but probably will in time. Normally a floating-point number x is stored as
xmll xfloat = (−1)s × 1.f × 2e−bias, (1.2)
that is, with separate entities for the sign s, the fractional part of the mantissa f , and the
exponential field e. All parts are stored in binary form and occupy adjacent segments of a
single 32-bit word for singles or two adjacent 32-bit words for doubles. The sign s is stored
as a single bit, with s = 0 or 1 for a positive or a negative sign. Eight bits are used to stored the
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
COMPUTATIONAL SCIENCE BASICS 21
Table 1.4 Representation Scheme for Normal and Abnormal IEEE Singles.
Number Name Values of s, e, and f Value of Single
Normal 0 < e < 255 (−1)s × 2e−127 × 1.f
Subnormal e = 0, f 6= 0 (−1)s × 2−126 × 0.f
Signed zero (±0) e = 0, f = 0 (−1)s × 0.0
+∞ s = 0, e = 255, f = 0 +INF
−∞ s = 1, e = 255, f = 0 --INF
Not a number s = u, e = 255, f 6= 0 NaN
exponent e, which means that e can be in the range 0 ≤ e ≤ 255. The endpoints, e = 0 and
e = 255, are special cases (Table 1.4). Normal numbers have 0 < e < 255, and with them the
convention is to assume that the mantissa’s first bit is a 1, so only the fractional part f after the
binary point is stored. The representations for subnormal numbers and the special cases are
given in Table 1.4.
Note that the values ±INF and NaN are not numbers in the mathematical sense, that is,
objects that can be manipulated or used in calculations to take limits and such. Rather, they
are signals to the computer and to you that something has gone awry and that the calculation
should probably stop until you straighten things out. In contrast, the value −0 can be used in
a calculation with no harm. Some languages may set unassigned variables to −0 as a hint that
they have yet to be assigned, though it is best not to count on that!
The IEEE representations ensure that all normal floating-point numbers have the same
relative precision. Because the first bit is assumed to be 1, it does not have to be stored, and
computer designers need only recall that there is a phantom bit there to obtain an extra bit of
precision. During the processing of numbers in a calculation, the first bit of an intermediate
result may become zero, but this is changed before the final number is stored. To repeat, for
normal cases, the actual mantissa (1.f in binary notation) contains an implied 1 preceding the
binary point.
Finally, in order to guarantee that the stored biased exponent e is always positive, a
fixed number called the bias is added to the actual exponent p before it is stored as the biased
exponent e. The actual exponent, which may be negative, is
p = e− bias. (1.3)
1.7.1.1 Examples of IEEE Representations
There are two basic, IEEE floating-point formats, singles and doubles. Singles or floats is
shorthand for single- precision floating-point numbers, and doubles is shorthand for
double-precision floating-point numbers. Singles occupy 32 bits overall, with 1 bit for the sign,
8 bits for the exponent, and 23 bits for the fractional mantissa (which gives 24-bit precision
when the phantom bit is included). Doubles occupy 64 bits overall, with 1 bit for the sign, 10
bits for the exponent, and 53 bits for the fractional mantissa (for 54-bit precision). This means
that the exponents and mantissas for doubles are not simply double those of floats, as we see
in Table 1.3. (In addition, the IEEE standard also permits extended precision that goes beyond
doubles, but this is all complicated enough without going into that right now.)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
22 CHAPTER 1
s e f
Bit position 31 30 23 22 0
To see this scheme in action, look at the 32-bit float representing (1.2):
The sign bit s is in bit position 31, the biased exponent e is in bits 30–23, and the fractional
part of the mantissa f is in bits 22–0. Since 8 bits are used to store the exponent e and since
28 = 256, e has the range
0 ≤ e ≤ 255.
The values e = 0 and 255 are special cases. With bias = 12710, the full exponent
p = e10 − 127,
and, as indicated in Table 1.3, for singles has the range
−126 ≤ p ≤ 127.
The mantissa f for singles is stored as the 23 bits in positions 22–0. For normal numbers, that
is, numbers with 0 < e < 255, f is the fractional part of the mantissa, and therefore the actual
number represented by the 32 bits is
xmll Normal floating-point number = (−1)
s × 1.f × 2e−127.
Subnormal numbers have e = 0, f 6= 0. For these, f is the entire mantissa, so the actual
number represented by these 32 bit is
Subnormal numbers = (−1)s × 0.f × 2e−126. (1.4)
The 23 bits m22–m0, which are used to store the mantissa of normal singles, correspond to the
representation
xmll Mantissa = 1.f = 1 +m22 × 2
−1 +m21 × 2−2 + · · ·+m0 × 2−23, (1.5)
with 0.f used for subnormal numbers. The special e = 0 representations used to store ±0 and
±∞ are given in Table 1.4.
To see how this works in practice (Figure 1.9), the largest positive normal floating-point
number possible for a 32-bit machine has the maximum value for e (254) and the maximum
value for f :
Xmax = 01111 1111 1111 1111 1111 1111 1111 111
= (0)(1111 1111)(1111 1111 1111 1111 1111 111), (1.6)
where we have grouped the bits for clarity. After putting all the pieces together, we obtain the
value shown in Table 1.3:
s = 0, e = 1111 1110 = 254, p = e− 127 = 127,
f = 1.1111 1111 1111 1111 1111 111 = 1 + 0.5 + 0.25 + · · · ' 2,
⇒ (−1)s × 1.f × 2p=e−127 ' 2× 2127 ' 3.4× 1038. (1.7)
Likewise, the smallest positive floating-point number possible is subnormal (e = 0) with a
single significant bit in the mantissa:
0 0000 0000 0000 0000 0000 0000 0000 001.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
COMPUTATIONAL SCIENCE BASICS 23
This corresponds to
s = 0, e = 0, p = e− 126 = −126
f = 0.0000 0000 0000 0000 0000 001 = 2−23
⇒ (−1)s × 0.f × 2p=e−126 = 2−149 ' 1.4× 10−45 (1.8)
In summary, single-precision (32-bit or 4-byte) numbers have six or seven decimal places of
significance and magnitudes in the range
1.4× 10−45 ≤ single precision ≤ 3.4× 1038
Doubles are stored as two 32-bit words, for a total of 64 bits (8 B). The sign occupies 1 bit, the
exponent e, 11 bits, and the fractional mantissa, 52 bits:
s e f f (cont.)
Bit position 63 62 52 51 32 31 0
As we see here, the fields are stored contiguously, with part of the mantissa f stored in separate
32-bit words. The order of these words, and whether the second word with f is the most or
least significant part of the mantissa, is machine- dependent. For doubles, the bias is quite a bit
larger than for singles,
Bias = 11111111112 = 102310,
so the actual exponent p = e− 1023.
The bit patterns for doubles are given in Table 1.5, with the range and precision given in
Table 1.3. To repeat, if you write a program with doubles, then 64 bits (8 bytes) will be used to
store your floating-point numbers. Doubles have approximately 16 decimal places of precision
(1 part in 252) and magnitudes in the range
4.9× 10−324 ≤ double precision ≤ 1.8× 10308. (1.9)
If a single-precision number x is larger than 2128, a fault condition known as an overflow
occurs (Figure 1.9). If x is smaller than 2−128, an underflow occurs. For overflows,
the resulting number xc may end up being a machine-dependent pattern, not a number (NAN),
or unpredictable. For underflows, the resulting number xc is usually set to zero, although this
can usually be changed via a compiler option. (Having the computer automatically convert un-
derflows to zero is usually a good path to follow; converting overflows to zero may be the path
to disaster.) Because the only difference between the representations of positive and negative
numbers on the computer is the sign bit of one for negative numbers, the same considerations
hold for negative numbers.
Table 1.5 Representation Scheme for IEEE Doubles
Number Name Values of s, e, and f Value of Double
Normal 0 < e < 2047 (−1)s × 2e−1023 × 1.f
Subnormal e = 0, f 6= 0 (−1)s × 2−1022 × 0.f
Signed zero e = 0, f = 0 (−1)s × 0.0
+∞ s = 0, e = 2047, f = 0 +INF
−∞ s = 1, e = 2047, f = 0 −INF
Not a number s = u, e = 2047, f 6= 0 NaN
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
24 CHAPTER 1
In our experience, serious scientific calculations almost always require at least 64-bit
(double-precision) floats. And if you need double precision in one part of your calculation,
you probably need it all over, which means double-precision library routines for methods and
functions.
1.7.2 Python and the IEEE 754 Standard
Python is a relatively recent language with changes and extensions occurring as its use spreads
and as its features mature. It should be no surprise then that Python does not at present adhere
to all aspects, and especially the special cases, of the IEEE 754 standard. Probably the most
relevant difference for us is that Python does not support single (32 bit) precision floating point
numbers. So when we deal with a data type called a float in Python, it is the equivalent of a
double in the IEEE standard. Since singles are inadequate for most scientific computing, this is
not a loss. However be wary, if you switch over to Java or C you should declare your variables
as doubles and not as floats. While Python eliminates single-precision floats, it adds a new
data type complex for dealing with complex numbers. Complex numbers are stored as pairs of
doubles and are quite useful in science. We discuss complex numbers in Chapter 4, “Python
Object-Oriented Programs”, and Python’s native implementation in § ??.
The details of how closely Python adheres to the IEEE 754 standard depend upon the
details of Python’s use of the C or Java language to power the Python interpreter. In particular,
with the recent 64 bit architectures for CPUs, the range may even be greater than the IEEE
standard, and the abnormal numbers (±INF, NaN) may differ. Likewise, the exact conditions
for overflows and underflows may also differ. That being the case, the exploratory exercises
to follow become all that more interesting since we cannot say that we know what results you
should obtain!
1.7.3 Over/Underflows Exercises
1. Consider the 32-bit single-precision floating-point number
s e f
Bit position 31 30 23 22 0
Value 0 0000 1110 1010 0000 0000 0000 0000 000
a. What are the (binary) values for the sign s, the exponent e, and the fractional mantissa
f . (Hint: e10 = 14.)
b. Determine decimal values for the biased exponent e and the true exponent p.
c. Show that the mantissa of A equals 1.625000.
d. Determine the full value of A.
2. Write a program to test for the underflow and overflow limits (within a factor of 2) of
your computer system and of your computer language. A sample pseudocode is 
unde r = 1 .
ove r = 1 .
b e g i n do N t i m e s
under = under / 2 .
ove r = ove r ∗ 2 .
w r i t e o u t : l oop number , under , ove r
end do
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
COMPUTATIONAL SCIENCE BASICS 25
You may need to increase N if your initial choice does not lead to underflow and overflow.
(Notice that if you want to be more precise regarding the limits of your computer, you
may want to multiply and divide by a number smaller than 2.)
a. Check where under- and overflow occur for single-precision floating-point numbers
(floats). Give your answer in decimal.
b. Check where under- and overflow occur for double-precision floating-point numbers
(doubles).
c. Check where under- and overflow occur for integers. Note: There is no exponent
stored for integers, so the smallest integer corresponds to the most negative one. To
determine the largest and smallest integers, you must observe your program’s output
as you explicitly pass through the limits. You accomplish this by continually adding
and subtracting 1. (Because integer arithmetic uses two’s complement arithmetic, you
should expect some surprises.)
1.7.4 Machine Precision (Model)
A major concern of computational scientists is that the floating-point representation used to
store numbers is of limited precision. In general for a 32-bit-word machine, single-precision
numbers are good to 6–7 decimal places, while doubles are good to 15–16 places. To see how
limited precision affects calculations, consider the simple computer addition of two single-
precision words:
7 + 1.0× 10−7 = ?
The computer fetches these numbers from memory and stores the bit patterns
7 = 0 10000010 1110 0000 0000 0000 0000 000, (1.10)
10−7 = 0 01100000 1101 0110 1011 1111 1001 010, (1.11)
in working registers (pieces of fast-responding memory). Because the exponents are different,
it would be incorrect to add the mantissas, and so the exponent of the smaller number is made
larger while progressively decreasing the mantissa by shifting bits to the right (inserting zeros)
until both numbers have the same exponent:
10−7 = 0 01100001 0110 1011 0101 1111 1100101 (0)
= 0 01100010 0011 0101 1010 1111 1110010 (10) (1.12)
· · ·
= 0 10000010 0000 0000 0000 0000 0000 000 (0001101 · · · 0
⇒ 7 + 1.0× 10−7 = 7. (1.13)
Because there is no room left to store the last digits, they are lost, and after all this hard work
the addition just gives 7 as the answer (truncation error in Figure 1.9). In other words, because
a 32-bit computer stores only 6 or 7 decimal places, it effectively ignores any changes beyond
the sixth decimal place.
The preceding loss of precision is categorized by defining the machine precision m
as the maximum positive number that, on the computer, can be added to the number stored as
1 without changing that stored 1:
1c + m
def= 1c, (1.14)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
26 CHAPTER 1
where the subscript c is a reminder that this is a computer representation of 1. Consequently,
an arbitrary number x can be thought of as related to its floating-point representation xc by
xc = x(1± ), || ≤ m,
where the actual value for  is not known. In other words, except for powers of 2 that are
represented exactly, we should assume that all single-precision numbers contain an error in the
sixth decimal place and that all doubles have an error in the fifteenth place. And as is always
the case with errors, we must assume that we do not know what the error is, for if we knew,
then we would eliminate it! Consequently, the arguments we put forth regarding errors are
always approximate, and that is the best we can do.
1.7.5 Determine Your Machine Precision
Write a program to determine the machine precision m of your computer system (within a
factor of 2 or better). A sample pseudocode is 
eps = 1 .
b e g i n do N t i m e s
eps = eps / 2 . # Make s m a l l e r
one = 1 . + eps # Wr i t e loop number , one , eps
end do
A Python implementation is given in Listing 1.8, while a more precise one is ByteLimit.py
on the instructor’s CD.
Listing 1.8 The code Limits.py determines machine precision within a factor of 2. Note how we skip a line at the
beginning of each class or method and how we align the closing brace vertically with its appropriate
key word (in italics). 
# L i m i t s . py : d e t e r m i n e s machine p r e c i s i o n r o u g h l y
from v i s u a l i m p o r t ∗
N = 10
eps = 1 . 0
f o r i i n x r an ge (N) :
eps = eps / 2
o n e P l u s e p s = 1 . 0 + eps
p r i n t ’one + eps = ’ , o n e P l u s e p s
p r i n t ’eps = ’ , eps
1. Determine experimentally the precision of single-precision floats.
2. Determine experimentally the precision of double-precision floats.
To print out a number in decimal format, the computer must convert from its internal binary
representation. This not only takes time, but unless the number is a power of 2, leads to a loss
of precision. So if you want a truly precise indication of the stored numbers, you should avoid
conversion to decimals and instead print them out in octal or hexadecimal format (\0NNN).
1.8 PROBLEM: SUMMING SERIES
A classic numerical problem is the summation of a series to evaluate a function. As an example,
consider the infinite series for sinx:
sinx = x− x
3
3!
+
x5
5!
− x
7
7!
+ · · · (exact).
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
COMPUTATIONAL SCIENCE BASICS 27
Your problem is to use this series to calculate sinx for x < 2π and x > 2π, with an absolute
error in each case of less than 1 part in 108. While an infinite series is exact in a mathematical
sense, it is not a good algorithm because we must stop summing at some point. An algorithm
would be the finite sum
xmllsinx '
N∑
n=1
(−1)n−1x2n−1
(2n− 1)!
(algorithm). (1.15)
But how do we decide when to stop summing? (Do not even think of saying, “When the answer
agrees with a table or with the built-in library function.”)
1.8.1 Numerical Summation (Method)
Never mind that the algorithm (1.15) indicates that we should calculate (−1)n−1x2n−1 and
then divide it by (2n− 1)! This is not a good way to compute. On the one hand, both (2n− 1)!
and x2n−1 can get very large and cause overflows, even though their quotient may not. On
the other hand, powers and factorials are very expensive (time-consuming) to evaluate on the
computer. Consequently, a better approach is to use a single multiplication to relate the next
term in the series to the previous one:
xmll(−1)
n−1x2n−1
(2n− 1)!
=
−x2
(2n− 1)(2n− 2)
(−1)n−2x2n−3
(2n− 3)!
⇒ nth term = −x
2
(2n− 1)(2n− 2)
× (n− 1)th term. (1.16)
While we want to ensure definite accuracy for sinx, that is not so easy to do. What is easy to
do is to assume that the error in the summation is approximately the last term summed (this
assumes no round-off error, a subject we talk about in Chapter 2, “Errors & Uncertainties in
Computations”). To obtain an absolute error of 1 part in 108, we then stop the calculation when∣∣∣∣nth termsum
∣∣∣∣ < 10−8, (1.17)
where “term” is the last term kept in the series (1.15) and “sum” is the accumulated sum of all
the terms. In general, you are free to pick any tolerance level you desire, although if it is too
close to, or smaller than, machine precision, your calculation may not be able to attain it. A
pseudocode for performing the summation is 
t e rm = x , sum = x , eps = 10ˆ(−8) # I n i t i a l i z e do
do t e rm = −t e rm∗x∗x / ( 2 n +1) / ( 2∗ n−2) ; # New wr t o l d
sum = sum + term # Add te rm
w h i l e abs ( te rm / sum ) > eps # Break i t e r a t i o n
end do
1.8.2 Implementation and Assessment
1. Write a program that implements this pseudocode for the indicated x values. Present
the results as a table with the headings
x imax sum |sum− sin(x)|/sin(x)
where sin(x) is the value obtained from the built-in function. The last column here is
the relative error in your computation. Modify the code that sums the series in a “good
way” (no factorials) to one that calculates the sum in a “bad way” (explicit factorials).
2. Produce a table as above.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
28 CHAPTER 1
3. Start with a tolerance of 10−8 as in (1.17).
4. Show that for sufficiently small values of x, your algorithm converges (the changes are
smaller than your tolerance level) and that it converges to the correct answer.
5. Compare the number of decimal places of precision obtained with that expected from
(1.17).
6. Without using the identity sin(x+ 2nπ) = sin(x), show that there is a range of some-
what large values of x for which the algorithm converges, but that it converges to the
wrong answer.
7. Show that as you keep increasing x, you will reach a regime where the algorithm does
not even converge.
8. Now make use of the identity sin(x + 2nπ) = sin(x) to compute sinx for large x
values where the series otherwise would diverge.
9. Repeat the calculation using the “bad” version of the algorithm (the one that calculates
factorials) and compare the answers.
10. Set your tolerance level to a number smaller than machine precision and see how this
affects your conclusions.
Beginnings are hard.
—Chaim Potok
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Two
Errors & Uncertainties in Computations
To err is human, to forgive divine.
— Alexander Pope
Whether you are careful or not, errors and uncertainties are a part of computation. Some
errors are the ones that humans inevitably make, but some are introduced by the computer.
Computer errors arise because of the limited precision with which computers store numbers or
because algorithms or models can fail. Although it stifles creativity to keep thinking “error”
when approaching a computation, it certainly is a waste of time, and may lead to harm, to work
with results that are meaningless (“garbage”) because of errors. In this chapter we examine
some of the errors and uncertainties that may occur in computations. Even though we do not
dwell on it, the lessons of this chapter apply to all other chapters as well.
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
Errors 2.1, 2.3 * *
2.1 TYPES OF ERRORS (THEORY)
Let us say that you have a program of high complexity. To gauge why errors should be of
concern, imagine a program with the logical flow
start→ U1 → U2 → · · · → Un → end, (2.1)
where each unit U might be a statement or a step. If each unit has probability p of being
correct, then the joint probability P of the whole program being correct is P = pn. Let us say
we have a medium-sized program with n = 1000 steps and that the probability of each step
being correct is almost one, p ' 0.9993. This means that you end up with P ' 12 , that is, a
final answer that is as likely wrong as right (not a good way to build a bridge). The problem
is that, as a scientist, you want a result that is correct—or at least in which the uncertainty is
small and of known size.
Four general types of errors exist to plague your computations:
Blunders or bad theory: typographical errors entered with your program or data, running
the wrong program or having a fault in your reasoning (theory), using the wrong data file,
and so on. (If your blunder count starts increasing, it may be time to go home or take a
break.)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
30 CHAPTER 2
Random errors: imprecision caused by events such as fluctuations in electronics, cosmic
rays, or someone pulling a plug. These may be rare, but you have no control over them
and their likelihood increases with running time; while you may have confidence in a 20-s
calculation, a week-long calculation may have to be run several times to check repro-
ducibility.
Approximation errors: imprecision arising from simplifying the mathematics so that a
problem can be solved on the computer. They include the replacement of infinite series
by finite sums, infinitesimal intervals by finite ones, and variable functions by constants.
For example,
sin(x) =
∞∑
n=1
(−1)n−1x2n−1
(2n− 1)!
(exact)
'
N∑
n=1
(−1)n−1x2n−1
(2n− 1)!
= sin(x) + E(x,N), (algorithm) (2.2)
where E(x,N) is the approximation error and where in this case E is the series fromN+1
to∞. Because approximation error arises from the algorithm we use to approximate the
mathematics, it is also called algorithmic error. For every reasonable approximation, the
approximation error should decrease as N increases and vanish in the N → ∞ limit.
Specifically for (2.2), because the scale for N is set by the value of x, a small approxima-
tion error requires N  x. So if x and N are close in value, the approximation error will
be large.
Round-off errors: imprecision arising from the finite number of digits used to store
floating-point numbers. These “errors” are analogous to the uncertainty in the mea-
surement of a physical quantity encountered in an elementary physics laboratory. The
overall round-off error accumulates as the computer handles more numbers, that is, as the
number of steps in a computation increases, and may cause some algorithms to become
unstable with a rapid increase in error. In some cases, round-off error may become the
major component in your answer, leading to what computer experts call garbage.
For example, if your computer kept four decimal places, then it will store 13 as 0.3333 and
2
3 as 0.6667, where the computer has “rounded off” the last digit in
2
3 . Accordingly, if we
ask the computer to do as simple a calculation as 2(13)−
2
3 , it produces
2
(
1
3
)
− 2
3
= 0.6666− 0.6667 = −0.0001 6= 0. (2.3)
So even though the result is small, it is not 0, and if we repeat this type of calculation millions
of times, the final answer might not even be small (garbage begets garbage).
When considering the precision of calculations, it is good to recall our discussion in
Chapter 1, “Computational Science Basics,” of significant figures and of scientific notation
given in your early physics or engineering classes. For computational purposes, let us consider
how the computer may store the floating-point number
a = 11223344556677889900 = 1.12233445566778899× 1019. (2.4)
Because the exponent is stored separately and is a small number, we can assume that it will
be stored in full precision. In contrast, some of the digits of the mantissa may be truncated.
In double precision the mantissa of a will be stored in two words, the most significant part
representing the decimal 1.12233, and the least significant part 44556677. The digits beyond
7 are lost. As we see below, when we perform calculations with words of fixed length, it is
inevitable that errors will be introduced (at least) into the least significant parts of the words.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
ERRORS & UNCERTAINTIES IN COMPUTATIONS 31
2.1.1 Model for Disaster: Subtractive Cancellation
A calculation employing numbers that are stored only approximately on the computer can
be expected to yield only an approximate answer. To demonstrate the effect of this type of
uncertainty, we model the computer representation xc of the exact number x as
xc ' x(1 + x). (2.5)
Here x is the relative error in xc, which we expect to be of a similar magnitude to the machine
precision m. If we apply this notation to the simple subtraction a = b− c, we obtain
a = b− c ⇒ ac ' bc − cc ' b(1 + b)− c(1 + c)
⇒ ac
a
' 1 + b
b
a
− c
a
c. (2.6)
We see from (2.6) that the resulting error in a is essentially a weighted average of the errors in
b and c, with no assurance that the last two terms will cancel. Of special importance here is to
observe that the error in the answer ac increases when we subtract two nearly equal numbers
(b ' c) because then we are subtracting off the most significant parts of both numbers and
leaving the error-prone least-significant parts:
ac
a
def= 1 + a ' 1 +
b
a
(b − c) ' 1 +
b
a
max(|b|, |c|). (2.7)
This shows that even if the relative errors in b and c may cancel somewhat, they are multiplied
by the large number b/a, which can significantly magnify the error. Because we cannot assume
any sign for the errors, we must assume the worst [the “max” in (2.7)].
Theorem 2.1.1. If you subtract two large numbers and end up with a small one, there
will be less significance, and possibly a lot less significance, in the small one.
We have already seen an example of subtractive cancellation in the power series sum-
mation for sinx ' x − x3/3! + · · · for large x. A similar effect occurs for e−x '
1− x+ x2/2!− x3/3! + · · · for large x, where the first few terms are large but of alternating
sign, leading to an almost total cancellation in order to yield the final small result. (Subtractive
cancellation can be eliminated by using the identity e−x = 1/ex, although round-off error will
still remain.)
2.1.2 Subtractive Cancellation Exercises
1. Remember back in high school when you learned that the quadratic equation
ax2 + bx+ c = 0 (2.8)
has an analytic solution that can be written as either
xmllx1,2 = −b±
√
b2 − 4ac
2a
or x′1,2 =
−2c
b±
√
b2 − 4ac
. (2.9)
Inspection of (2.9) indicates that subtractive cancellation (and consequently an increase
in error) arises when b2  4ac because then the square root and its preceding term
nearly cancel for one of the roots.
a. Write a program that calculates all four solutions for arbitrary values of a, b, and c.
b. Investigate how errors in your computed answers become large as the subtractive
cancellation increases and relate this to the known machine precision. (Hint: A good
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
32 CHAPTER 2
test case employs a = 1, b = 1, c = 10−n, n = 1, 2, 3, . . ..)
c. Extend your program so that it indicates the most precise solutions.
2. As we have seen, subtractive cancellation occurs when summing a series with alternating
signs. As another example, consider the finite sum
xmll
S
(1)
N =
2N∑
n=1
(−1)n n
n+ 1
. (2.10)
If you sum the even and odd values of n separately, you get two sums:
xmll
S
(2)
N = −
N∑
n=1
2n− 1
2n
+
N∑
n=1
2n
2n+ 1
. (2.11)
All terms are positive in this form with just a single subtraction at the end of the cal-
culation. Yet even this one subtraction and its resulting cancellation can be avoided by
combining the series analytically to obtain
xmll
S
(3)
N =
N∑
n=1
1
2n(2n+ 1)
. (2.12)
Even though all three summations S(1), S(2), and S(3) are mathematically equal, they
may give different numerical results.
a. Write a single-precision program that calculates S(1), S(2), and S(3).
b. Assume S(3) to be the exact answer. Make a log-log plot of the relative error versus
the number of terms, that is, of log10 |(S
(1)
N −S
(3)
N )/S
(3)
N | versus log10(N). Start with
N = 1 and work up to N = 1, 000, 000. (Recollect that log10 x = lnx/ ln 10.) The
negative of the ordinate in this plot gives an approximate value for the number of
significant figures.
c. See whether straight-line behavior for the error occurs in some region of your plot.
This indicates that the error is proportional to a power of N .
3. In spite of the power of your trusty computer, calculating the sum of even a simple series
may require some thought and care. Consider the two series
S(up) =
N∑
n=1
1
n
, S(down) =
1∑
n=N
1
n
.
Both series are finite as long as N is finite, and when summed analytically both give the
same answer. Nonetheless, because of round-off error, the numerical value of S(up) will
not be precisely that of S(down).
a. Write a program to calculate S(up) and S(down) as functions of N .
b. Make a log-log plot of (S(up) − S(down))/(|S(up)|+ |S(down)|) versus N .
c. Observe the linear regime on your graph and explain why the downward sum is gen-
erally more precise.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
ERRORS & UNCERTAINTIES IN COMPUTATIONS 33
2.1.3 Round-off Error in a Single Step
Let’s start by seeing how error arises from a single division of the computer representations of
two numbers:
a =
b
c
⇒ ac =
bc
cc
=
b(1 + b)
c(1 + c)
,
⇒ ac
a
=
1 + b
1 + c
' (1 + b)(1− c) ' 1 + b − c,
⇒ ac
a
' 1 + |b|+ |c|. (2.13)
Here we ignore the very small 2 terms and add errors in absolute value since we cannot assume
that we are fortunate enough to have unknown errors cancel each other. Because we add the
errors in absolute value, this same rule holds for multiplication. Equation (2.13) is just the
basic rule of error propagation from elementary laboratory work: You add the uncertainties in
each quantity involved in an analysis to arrive at the overall uncertainty.
We can even generalize this model to estimate the error in the evaluation of a general
function f(x), that is, the difference in the value of the function evaluated at x and at xc:
E = f(x)− f(xc)
f(x)
' df(x)/dx
f(x)
(x− xc). (2.14)
So, for
f(x) =
√
1 + x,
df
dx
=
1
2
1√
1 + x
(2.15)
⇒ E ' 1
2
1√
1 + x
(x− xc). (2.16)
If we evaluate this expression for x = π/4 and assume an error in the fourth place of x, we
obtain a similar relative error of 1.5× 10−4 in
√
1 + x.
2.1.4 Round-off Error Accumulation After Many Steps
There is a useful model for approximating how round-off error accumulates in a calculation
involving a large number of steps. We view the error in each step as a literal “step” in a random
walk, that is, a walk for which each step is in a random direction. As we derive and simulate in
Chapter 5, “Monte Carlo Simulations,” the total distance covered in N steps of length r, is, on
the average,
R '
√
N r. (2.17)
By analogy, the total relative error ro arising after N calculational steps each with machine
precision error m is, on the average,
ro '
√
N m. (2.18)
If the round-off errors in a particular algorithm do not accumulate in a random manner, then a
detailed analysis is needed to predict the dependence of the error on the number of steps N . In
some cases there may be no cancellation, and the error may increase as Nm. Even worse, in
some recursive algorithms, where the error generation is coherent, such as the upward recursion
for Bessel functions, the error increases as N !.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
34 CHAPTER 2
Figure 2.1 The first four spherical Bessel functions jl (x) as functions of x. Notice that for small x, the values for
increasing l become progressively smaller.
0.0 2.0 4.0 6.0 8.0 10.0 12.0
x
0.0
0.2
0.4
0.6
0.8
1.0
j
l
(x)
l = 0
l = 1
l = 3
Our discussion of errors has an important implication for a student to keep in mind before
being impressed by a calculation requiring hours of supercomputer time. A fast computer may
complete 1010 floating-point operations per second. This means that a program running for 3 h
performs approximately 1014 operations. Therefore, if round-off error accumulates randomly,
after 3 h we expect a relative error of 107m. For the error to be smaller than the answer,
we need m < 10−7, which requires double precision and a good algorithm. If we want a
higher-precision answer, then we will need a very good algorithm.
2.2 ERRORS IN SPHERICAL BESSEL FUNCTIONS (PROBLEM)
Accumulating round-off errors often limits the ability of a program to calculate accurately.
Your problem is to compute the spherical Bessel and Neumann functions jl(x) and nl(x).
These function are, respectively, the regular/irregular (nonsingular/singular at the origin) solu-
tions of the differential equation
x2f ′′(x) + 2xf ′(x) +
[
x2 − l(l + 1)
]
f(x) = 0. (2.19)
The spherical Bessel functions are related to the Bessel function of the first kind by jl(x) =√
π/2xJn+1/2(x). They occur in many physical problems, such as the expansion of a plane
wave into spherical partial waves,
eik·r =
∞∑
l=0
il (2l + 1)jl(kr)Pl(cos θ). (2.20)
Figure 2.1 shows what the first few jl look like, and Table 2.1 gives some explicit values. For
the first two l values, explicit forms are
j0(x) = +
sin x
x
, j1(x) = +
sin x
x2
− cos x
x
(2.21)
n0(x) =−
cos x
x
. n1(x) = −
cos x
x2
− sin x
x
. (2.22)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
ERRORS & UNCERTAINTIES IN COMPUTATIONS 35
Table 2.1 Approximate Values for Spherical Bessel Functions of Orders 3, 5, and 8 (from Maple)
x j3(x) j5(x) j8(x)
0.1 +9.518519719 10−6 +9.616310231 10−10 +2.901200102 10−16
1 +9.006581118 10−3 +9.256115862 10−05 +2.826498802 10−08
10 −3.949584498 10−2 −5.553451162 10−01 +1.255780236 10+00
2.2.1 Numerical Recursion Relations (Method)
The classic way to calculate jl(x) would be by summing its power series for small values of
x/l and summing its asymptotic expansion for large x values. The approach we adopt is based
on the recursion relations
xmlljl+1(x) =
2l + 1
x
jl(x)− jl−1(x), (up), (2.23)
jl−1(x) =
2l + 1
x
jl(x)− jl+1(x), (down). (2.24)
Equations (2.23) and (2.24) are the same relation, one written for upward recurrence from
small to large l values, and the other for downward recurrence to small l values. With just a
few additions and multiplications, recurrence relations permit rapid, simple computation of the
entire set of jl values for fixed x and all l.
To recur upward in l for fixed x, we start with the known forms for j0 and j1 (2.21) and
use (2.23). As you will prove for yourself, this upward recurrence usually seems to work at
first but then fails. The reason for the failure can be seen from the plots of jl(x) and nl(x)
versus x (Figure 2.1). If we start at x ' 2 and l = 0, we will see that as we recur jl up to
larger l values with (2.23), we are essentially taking the difference of two “large” functions to
produce a “small” value for jl. This process suffers from subtractive cancellation and always
reduces the precision. As we continue recurring, we take the difference of two small functions
with large errors and produce a smaller function with yet a larger error. After a while, we are
left with only round-off error (garbage).
To be more specific, let us call j(c)l the numerical value we compute as an approximation
for jl(x). Even if we start with pure jl, after a short while the computer’s lack of precision
effectively mixes in a bit of nl(x):
j
(c)
l = jl(x) + nl(x). (2.25)
This is inevitable because both jl and nl satisfy the same differential equation and, on that
account, the same recurrence relation. The admixture of nl becomes a problem when the
numerical value of nl(x) is much larger than that of jl(x) because even a minuscule amount
of a very large number may be large. In contrast, if we use the upward recurrence relation
(2.23) to produce the spherical Neumann function nl, there will be no problem because we are
combining small functions to produce larger ones (Figure 2.1), a process that does not contain
subtractive cancellation.
The simple solution to this problem (Miller’s device) is to use (2.24) for downward re-
cursion of the jl values starting at a large value l = L. This avoids subtractive cancellation by
taking small values of jl+1(x) and jl(x) and producing a larger jl−1(x) by addition. While the
error may still behave like a Neumann function, the actual magnitude of the error will decrease
quickly as we move downward to smaller l values. In fact, if we start iterating downward with
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
36 CHAPTER 2
arbitrary values for j(c)L+1 and j
(c)
L , after a short while we will arrive at the correct l dependence
for this value of x. Although the numerical value of j(c)0 so obtained will not be correct because
it depends upon the arbitrary values assumed for j(c)L+1 and j
(c)
L , the relative values will be ac-
curate. The absolute values are fixed from the know value (2.21), j0(x) = sinx/x. Because
the recurrence relation is a linear relation between the jl values, we need only normalize all the
computed values via
jnormalizedl (x) = j
compute
l (x)×
janalytic0 (x)
jcompute0 (x)
. (2.26)
Accordingly, after you have finished the downward recurrence, you obtain the final answer by
normalizing all j(c)l values based on the known value for j0.
2.2.2 Implementation and Assessment: Recursion Relations
A program implementing recurrence relations is most easily written using subscripts. If you
need to polish up on your skills with subscripts, you may want to study our program Bessel.py
in Listing ?? before writing your own.
1. Write a program that uses both upward and downward recursion to calculate jl(x) for
the first 25 l values for x = 0.1, 1, 10.
2. Tune your program so that at least one method gives “good” values (meaning a relative
error '10−10). See Table 2.1 for some sample values.
3. Show the convergence and stability of your results.
4. Compare the upward and downward recursion methods, printing out l, j(up)l , j
(down)
l , and
the relative difference |j(up)l − j
(down)
l |/|j
(up)
l |+ |j
(down)
l |.
5. The errors in computation depend on x, and for certain values of x, both up and down
recursions give similar answers. Explain the reason for this.
Listing 2.1 Bessel.pydetermines spherical Bessel functions by downward recursion (you should modify this to also
work by upward recursion). 
# B e s s e l . py
from v i s u a l i m p o r t ∗
from v i s u a l . g raph i m p o r t ∗
xmax = 4 0 . ; xmin = 0 . 2 5 ; s t e p = 0 . 1 # G lob a l c l a s s v a r i a b l e s
o r d e r = 1 0 ; s t a r t = 50 # P l o t j o r d e r
g raph1 = g d i s p l a y ( wid th = 500 , h e i g h t = 500 , t i t l e = ’Sperical Bessel, L = 1 (red), 10’ ,
x t i t l e = ’x’ , y t i t l e = ’j(x)’ )
p t s = g d o t s ( shape = ’square’ , s i z e = 1 , c o l o r = c o l o r . g r e e n )
d e f down ( x , n , m) : # Method down , r e c u r s downward
j = z e r o s ( ( s t a r t + 2 ) , F l o a t )
j [m + 1] = j [m] = 1 . # S t a r t w i th a n y t h i n g
f o r k i n r a n g e (m, 0 , − 1) :
j [ k − 1] = ( ( 2 .∗ k + 1 . ) / x )∗ j [ k ] − j [ k + 1]
s c a l e = ( s i n ( x ) / x ) / j [ 0 ] # S c a l e s o l u t i o n t o known j [ 0 ]
r e t u r n j [ n ] ∗ s c a l e
f o r x i n a r a n g e ( xmin , xmax , s t e p ) :
p t s . p l o t ( pos = ( x , down ( x , o r d e r , s t a r t ) ) )
p t s = g d o t s ( shape = ’square’ , s i z e = 1 , c o l o r = c o l o r . r e d )
f o r x i n a r a n g e ( xmin , xmax , s t e p ) :
p t s . p l o t ( pos = ( x , down ( x , 1 , s t a r t ) ) )
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
ERRORS & UNCERTAINTIES IN COMPUTATIONS 37
2.3 EXPERIMENTAL ERROR INVESTIGATION (PROBLEM)
Numerical algorithms play a vital role in computational physics. Your problem is to take a
general algorithm and decide
1. Does it converge?
2. How precise are the converged results?
3. How expensive (time-consuming) is it?
On first thought you may think, “What a dumb problem! All algorithms converge if enough
terms are used, and if you want more precision, then use more terms.” Well, some algorithms
may be asymptotic expansions that just approximate a function in certain regions of parameter
space and converge only up to a point. Yet even if a uniformly convergent power series is
used as the algorithm, including more terms will decrease the algorithmic error but increase
the round-off errors. And because round-off errors eventually diverge to infinity, the best we
can hope for is a “best” approximation. Good algorithms are good not only because they are
fast but also because being fast means that round-off error does not have much time to grow.
Let us assume that an algorithm takes N steps to find a good answer. As a rule of
thumb, the approximation (algorithmic) error decreases rapidly, often as the inverse power of
the number of terms used:
approx '
α
Nβ
. (2.27)
Here α and β are empirical constants that change for different algorithms and may be only
approximately constant, and even then only as N → ∞. The fact that the error must fall off
for large N is just a statement that the algorithm converges.
In contrast to this algorithmic error, round-off error tends to grow slowly and somewhat
randomly with N . If the round-off errors in each step of the algorithm are not correlated, then
we know from previous discussion that we can model the accumulation of error as a random
walk with step size equal to the machine precision m:
ro '
√
Nm. (2.28)
This is the slow growth with N that we expect from round-off error. The total error in a
computation is the sum of the two types of errors:
tot = approx + ro (2.29)
tot '
α
Nβ
+
√
Nm. (2.30)
For small N we expect the first term to be the larger of the two but ultimately to be overcome
by the slowly growing round-off error.
As an example, in Figure 2.2 we present a log-log plot of the relative error in numerical
integration using the Simpson integration rule (Chapter 6, “Integration”). We use the log10
of the relative error because its negative tells us the number of decimal places of precision
obtained.1 As a case in point, let us assume A is the exact answer and A(N) the computed
answer. If
A−A(N)
A
' 10−9, then log10
∣∣∣∣A−A(N)A
∣∣∣∣ ' −9. (2.31)
1Most computer languages use lnx = loge x. Yet since x = a
logax, we have log10 x = lnx/ ln 10.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
38 CHAPTER 2
Figure 2.2 A log-log plot of relative error versus the number of points used for a numerical integration. The ordinate
value of '10−14 at the minimum indicates that ∼14 decimal places of precision are obtained before
round-off error begins to build up. Notice that while the round-off error does fluctuate, on the average it
increases slowly.
10 100
10-13
10-9
N
|e
rr
or
|
Approximation Error
Round Off Error
We see in Figure 2.2 that the error does show a rapid decrease for small N , consistent with an
inverse power law (2.27). In this region the algorithm is converging. As N is increased, the
error starts to look somewhat erratic, with a slow increase on the average. In accordance with
(2.29), in this region round-off error has grown larger than the approximation error and will
continue to grow for increasing N . Clearly then, the smallest total error will be obtained if we
can stop the calculation at the minimum near 10−14, that is, when approx ' ro.
In realistic calculations you would not know the exact answer; after all, if you did, then
why would you bother with the computation? However, you may know the exact answer
for a similar calculation, and you can use that similar calculation to perfect your numerical
technique. Alternatively, now that you understand how the total error in a computation behaves,
you should be able to look at a table or, better yet, a graph (Figure 2.2) of your answer and
deduce the manner in which your algorithm is converging. Specifically, at some point you
should see that the mantissa of the answer changes only in the less significant digits, with that
place moving further to the right of the decimal point as the calculation executes more steps.
Eventually, however, as the number of steps becomes even larger, round-off error leads to a
fluctuation in the less significant digits, with a gradual increase on the average. It is best to quit
the calculation before this occurs.
Based upon this understanding, an approach to obtaining the best approximation is to
deduce when your answer behaves like (2.29). To do that, we call A the exact answer and
A(N) the computed answer after N steps. We assume that for large enough values of N , the
approximation converges as
A(N) ' A+ α
Nβ
, (2.32)
that is, that the round-off error term in (2.29) is still small. We then run our computer program
with 2N steps, which should give a better answer, and use that answer to eliminate the unknown
A:
A(N)−A(2N) ' α
Nβ
. (2.33)
To see if these assumptions are correct and determine what level of precision is possible for
the best choice of N , plot log10 |[A(N)−A(2N)]/A(2N)| versus log10N , similar to what we
have done in Figure 2.2. If you obtain a rapid straight-line drop off, then you know you are in
the region of convergence and can deduce a value for β from the slope. As N gets larger, you
should see the graph change from a straight-line decrease to a slow increase as round-off error
begins to dominate. A good place to quit is before this. In any case, now you understand the
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
ERRORS & UNCERTAINTIES IN COMPUTATIONS 39
error in your computation and therefore have a chance to control it.
As an example of how different kinds of errors enter into a computation, we assume we
know the analytic form for the approximation and round-off errors:
approx '
1
N2
, ro '
√
Nm, (2.34)
⇒ tot = approx + ro '
1
N2
+
√
Nm. (2.35)
The total error is then a minimum when
dtot
dN
=
−2
N3
+
1
2
m√
N
= 0, (2.36)
⇒ N5/2 = 4
m
. (2.37)
For a single-precision calculation (m ' 10−7), the minimum total error occurs when
N5/2 ' 4
10−7
⇒ N ' 1099, ⇒ tot ' 4× 10−6. (2.38)
In this case most of the error is due to round-off and is not approximation error. Observe, too,
that even though this is the minimum total error, the best we can do is about 40 times machine
precision (in double precision the results are better).
Seeing that the total error is mainly round-off error ∝
√
N , an obvious way to decrease
the error is to use a smaller number of steps N . Let us assume we do this by finding another
algorithm that converges more rapidly with N , for example, one with approximation error
behaving like
approx '
2
N4
. (2.39)
The total error is now
tot = ro + approx '
2
N4
+
√
Nm. (2.40)
The number of points for minimum error is found as before:
dtot
dN
= 0 ⇒ N9/2 ⇒ N ' 67 ⇒ tot ' 9× 10−7. (2.41)
The error is now smaller by a factor of 4, with only 116 as many steps needed. Subtle are the
ways of the computer. In this case the better algorithm is quicker and, by using fewer steps,
produces less round-off error.
Exercise: Repeat the error estimates for a double-precision calculation.
2.3.1 Error Assessment
We have already discussed the Taylor expansion of sinx:
sin(x) = x− x
3
3!
+
x5
5!
− x
7
7!
+ · · · =
∞∑
n=1
(−1)n−1x2n−1
(2n− 1)!
. (2.42)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
40 CHAPTER 2
Figure 2.3 The error in the summation of the series for e−x versus N. The values of x increase vertically for each
curve. Note that a negative initial slope corresponds to decreasing error with N, and that the dip corre-
sponds to a rapid convergence followed by a rapid increase in error. (Courtesy of J. Wiren.)
This series converges in the mathematical sense for all values of x. Accordingly, a reasonable
algorithm to compute the sin(x) might be
sin(x) '
N∑
n=1
(−1)n−1x2n−1
(2n− 1)!
. (2.43)
While in principle it should be faster to see the effects of error accumulation in this algorithm
by using single-precision numbers, C and Python tend to use double-precision mathematical
libraries, and so it is hard to do a pure single-precision computation. Accordingly, do these
exercises in double precision, as you should for all scientific calculations involving floating-
point numbers.
1. Write a program that calculates sin(x) as the finite sum (2.43). (If you already did this
in Chapter 2, “Computational Science Basics,” then you may reuse that program and its
results here.)
2. Calculate your series for x ≤ 1 and compare it to the built-in function Math.sin(x) (you
may assume that the built-in function is exact). Stop your summation at an N value for
which the next term in the series will be no more than 10−7 of the sum up to that point,
xmll |(−1)Nx2N+1|
(2N − 1)!
≤ 10−7
∣∣∣∣∣
N∑
n=1
(−1)n−1x2n−1
(2n− 1)!
∣∣∣∣∣ . (2.44)
3. Examine the terms in the series for x ' 3π and observe the significant subtractive can-
cellations that occur when large terms add together to give small answers. [Do not use
the identity sin(x + 2π) = sinx to reduce the value of x in the series.] In particular,
print out the near-perfect cancellation around n ' x/2.
4. See if better precision is obtained by using trigonometric identities to keep 0 ≤ x ≤ π.
5. By progressively increasing x from 1 to 10, and then from 10 to 100, use your program
to determine experimentally when the series starts to lose accuracy and when it no longer
converges.
6. Make a series of graphs of the error versus N for different values of x. (See Chapter 3,
“Visualization Tools.”) You should get curves similar to those in Figure 2.3.
Because this series summation is such a simple, correlated process, the round-off error does not
accumulate randomly as it might for a more complicated computation, and we do not obtain the
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
ERRORS & UNCERTAINTIES IN COMPUTATIONS 41
error behavior (2.32). We will see the predicted error behavior when we examine integration
rules in Chapter 6, “Integration.”
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Three
Visualization Tools
If I can’t picture it, I can’t understand it.
— Albert Einstein
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
- - - -
In this chapter we discuss the tools needed to visualize data produced by simulations and
measurements. Whereas other books may choose to relegate this discussion to an appendix, or
not to include it at all, we believe that visualization is such an integral part of computational
science, and so useful for your work in the rest of this book, that we have placed it right here,
up front. (We do, however, place our OpenDx tutorial in Appendix C since it may be a bit much
for beginners.)
ALL THE VISUALIZATION TOOLS WE discuss are powerful enough for professional
scientific work and are free or open source. Commercial packages such as Matlab, AVS,
Amira, and Noesys produce excellent scientific visualization but are less widely avail-
able. Mathematica and Maple have excellent visualization packages as well, but we have not
found them convenient when dealing with large numerical data sets.1 The tools we discuss,
and have used in preparing the visualizations for the text, are
Matplotlib: Matplotlib is a very powerful library of plotting functions callable from within
Python that is capable of producing publication quality figures in a number of output
formats. It is, by design, similar to the plotting packages with MATLAB, and is made
more powerful by its use of the numpy package for numerical work. In addition to 2-D
plots, Matplotlib can also create interactive, 3-D visualizations of data.
Visual (VPython): The programming language “Python” is so often employed with the Vi-
sual graphics module and the IDLE interface that the combination is often referred to as
Vpython. Much of the use of the Visual extension has been to create 3-D demonstrations
and animations for education, which are surprisingly easy to make and useful. We will
use Visual just occasionally for 2-D plots of numerical data and animations.
Tkinter: Although beyond the level of this text, Python contains a graphical user interface
(GUI) programming module called Tkinter or Tk. It is employed by the packages we use
and you may see them refer to it.
1Visualization with Maple and Mathematica is discussed in [L 05].
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
VISUALIZATION TOOLS 43
Gnuplot: 2-D and 3-D plotting, predominantly stand-alone (after you have finished running
your program). Originally for Unix operating systems, with an excellent windows port
also available.
Ace/gr (Grace): Stand-alone, menu-driven, publication-quality 2-D plotting for Unix sys-
tems; can run under MS Windows with Cygwin.
OPenDX: Formerly IBM DataExplorer. A stand-alone, multidimensional data tool for Unix
or for Windows under Cygwin (tutorial in C).
3.1 INSTALLING GRAPHICS PACKAGES IN PYTHON
1. We recommend that you go to the VPython site to get the free Python language, Visual
package and IDLE.
vpython.org/
They have packages there for Windows, Linux, and Macintosh.
2. Download and install Python first: presently python-2.5.4.msi. (Note: you may have to
load Python 4, which is not the most current version, in order to use the Matplot plotting
library.)
3. Next download and install Visual; presently (for Windows)
VPython-Win-Py2.5-5.03 candidate.exe.
4. Now download and install Numpy. This is a library for doing numerical work in Pythyon
that is especially useful for matrix operations, and is much like MATLAB (both are based
on the LAPACK) . Numpy is useful in its own right as well as being used by the
plotting package Matplotlib. We obtained Numpy from
sourceforge.net/projects/numpy
and used the file numpy-1.2.1-win32-superpack-python2.5.exe
5. Download and install Matplotlib, we got it from
sourceforge.net/projects/matplotlib/
and used matplotlib-0.98.53.win32-py2.5.exe.
6. Download and install the extensions for Matplotlib 3-D graphs: this was called
matplotlib-0.99.3, and we got it from
sourceforge.net/projects/matplotlib/
On these sites you will find free and useful examples and documentation, which we recom-
mend. In particular, there is Python documentation in the Doc directory of your Python instal-
lation, while the Matplotlib User’s Guide is at
matplotlib.sourceforge.net/contents.html
3.2 DATA VISUALIZATION
One of the most rewarding uses of computers is visualizing the results of calculations. While
in the past this was done with 2-D plots, in modern times it is regular practice to use 3-D
(surface) plots, volume rendering (dicing and slicing), and animation, as well as virtual reality
(gaming) tools. These types of visualizations are often breathtakingly beautiful and may pro- CD
vide deep insights into problems by letting us see and “handle” the functions with which we
are working. Visualization also assists in the debugging process, the development of physical
and mathematical intuition, and the all-around enjoyment of your work. One of the reasons for
visualization’s effectiveness may arise from the large fraction (∼10% direct and ∼50% with
coupling) of our brain involved in visual processing and the advantage gained by being able to
use this brainpower as a supplement to our logical powers.
In thinking about ways to present your results, keep in mind that the point of visualization
is to make the science clearer and to communicate your work to others. It follows then that
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
44 CHAPTER 3
you should make all figures as clear, informative, and self-explanatory as possible, especially if
you will be using them in presentations without captions. This means labels for all curves and
data points, a title, and labels on the axes.2 After this, you should look at your visualization
and ask whether there are better choices of units, ranges of axes, colors, style, and so on, that
might get the message across better and provide better insight. Considering the complexity of
human perception and cognition, there may not be a single best way to visualize a particular
data set, and so some trial and error may be necessary to see what looks best. Although all this
may seem like a lot of work, the more often you do it the quicker and better you get at it and
the better you will be able to communicate your work to others.
3.3 IMPORTING PACKAGES/MODULES INTO PYTHON PROGRAMS
A package or module is a collection of related methods or classes of methods that are assembled
together into a library. Incorporate these packages without having to know details about the
methods permits you to vastly extend the power of your Python programs3. You use a package
by placing an appropriate import or from statement at the beginning of your program. These
statements find, compile, and run the methods contained in the package. The import statement
loads the entire module package, which is efficient, but may require you to include the package
name as a prefix to the method you want. For example,
>>> import visual.graph
>>> y1 = visual.graph.gcurve(color=visual.graph.color.blue) Lots of prefixes!
where the >>> indicate that each command can be executed from a Python shell. Some of the
typing can be avoided by assigning a symbol to the package name:
>>> import visual.graph as vg Import method graph from visual package
>>> y1 = vg.gcurve(color=vg.color.blue, delta=3) Note prefix vg
In contrast to loading an entire package, the from statement can be used to load a specific
method, which means that no prefix is required when it is used:
>>> from pylab import plot Import method plot from pylab package
>>> plot(x, y, ’-’, lw=2) No prefix on plot
Technically, plot works here without a prefix because the imported name is copied into the
scope from where the from statement appear, and because the individual method has been
copied (import loads the package, but individual methods still need to be copied). There
is starred version of from that copies all of the methods of a package and thus extends the
namespace to include all of the methods:
>>> from pylab import * Import all methods from pylab package
>>> plot(x, y, ’-’, lw=2) A pylab command without prefix
>>> polar(...) A different pylab command
3.4 MATPLOTLIB: 2-D GRAPHS WITHIN PYTHON
Matplotlib is a powerful plotting package that lets you make 2-D and 3-D graphs, histograms,
power spectra, bar charts, errorcharts, scatterplots, and so forth, directly from your Python pro-
gram. It is free, uses the sophisticated numerics of numpy and LAPACK, and is easy to use.
Matplot commands were designed to be similar to the plotting commands of MATLAB, a com-
mercial problem-solving environment that is very popular with engineers, and like MATLAB,
2Although this may not need saying, place the independent variable x along the abscissa (horizontal), and the dependent
variable y = f(x) along the ordinate.
3The Python Package Index at pypi.python.org/pypi is a repository of 5835 software packages.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
VISUALIZATION TOOLS 45
Figure 3.1 Left: Output from EasyMatPlot.py of a simple, x-y plot using the matplotlib package. Right:
Output from GradesMatPlot.py that places two sets of data points, two curves, and unequal upper
and lower error bars, all on one plot.
-6 -4 -2 0 2 4 6
x
-1.0
-0.5
0.0
0.5
1.0
f(
x
)
MatPlotLib Example
f(x) vs x
-1 0 1 2 3 4 5
Years in College
-6
-4
-2
0
2
4
6
G
P
A
Grade Inflation
assumes that you are plotting arrays. Although Matplotlib is written in Python, it is not built
into Python and so you have to import Matplotlib into your program as the pylab library:
from pylab import * # load matplotlib commands
We see this command on line 3 of EasyMatPlot.py in Listing 3.1. Note that Matplotlib requires
you to calculate the x and y values of the points you want plotted, which usually means that x
and y should be arrays. We do this on lines 9 and 10 with
x = arange(Xmin, Xmax, DelX) # x range with increment
y = -sin(x)*cos(x) # function to plot
As you can see, creating arrays for plotting is easy with the numpy command arange that
constructs an array of floating point numbers between Xmax and Xmin, in steps of DelX (range
is for integers). And since x is an array, y is automatically one too! Note next how the plot
command places the points on the plot, with the ‘-’ indicating a line, and with line width (lw)
= 2 points. The xlabel and ylabel commands label the axis, and we place a title on top of
the plot and one along with x axis with the text and title commands. The actual graph
you see on your desktop, a graphical application, is produced with the show command. Other
commands are given in the table below.
Matplotlib Command Effect
plot(x, y, ’-’, lw=2) x-y curve, line width 2pts myPlot.setYRange(-8., 8.) Set y range
show() Show output graph myPlot.setSize(500, 400) Plot size in pixels
xlabel( ’x’ ) x axis label pyplot.semilogx Semilog x plot
ylabel( ’f(x)’ ) y axis label pyplot.semilogy Semilog y plot
title(’f vs x’) Add title grid(True) Draw grid
text(x, y, s) Add text s at (x, y) myPlot.setColor(false) Black & White
myPlot.addPoint(0, x, y, true) Add (x, y) to set 0, connect myPlot.setButtons(true) Display zoom button
myPlot.addPoint(1,x,y, false) Add (x, y) to set 1, no connect myPlot.fillPlot( ) Adjust ranges to data
pyplot.errorbar Point + error bar myPlot.setImpulses(true, 0) Vert lines to x axis, set 0
pyplot.clf() Clear current figure pyplot.contour Contour lines
pyplot.scatter Scatter plot pyplot.bar Bar charts
pyplot.polar Polar plot pyplot.gca Current axis instance
myPlot.setXRange(-1., 1.) Set x range
Listing 3.1 The program‘ EasyMatPlot.py produces a simple, 2-D x-y plot using the matplotlib package
(which includes the numpy package). 
# EasyMatP lo t . py : Simple p l o t u s i n g m a t p l o t l i b
2
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
46 CHAPTER 3
from p y l a b i m p o r t ∗ # Load m a t p l o t l i b
4
Xmin = −5.0; Xmax = + 5 . 0 ; N p o i n t s = 500
6DelX= ( Xmax−Xmin ) / N p o i n t s # D e l t a x
x = a r a n g e ( Xmin , Xmax , DelX ) # x r a n g e wi th i n c r e m e n t
8y = s i n ( x )∗ s i n ( x∗x ) # F u n c t i o n t o p l o t
10p r i n t ’arange => x[0], x[1], x[499] = %8.2f %8.2f %8.2f’ %(x [ 0 ] , x [ 1 ] , x [ 4 9 9 ] )
p r i n t ’arange => y[0], y[1], y[499] = %8.2f %8.2f %8.2f’ %(y [ 0 ] , y [ 1 ] , y [ 4 9 9 ] )
12p r i n t "\n Now doing the plotting thing, look for Figure 1 on desktop"
14x l a b e l (’x’ ) ; y l a b e l (’f(x)’ ) ; t i t l e (’ f(x) vs x’ ) # l a b e l s
t e x t (−1.5 , 0 . 1 , ’MatPlotLib Example’ ) # Text on p l o t
16p l o t ( x , y , ’-’ , lw =2)
g r i d ( True ) # Form g r i d
18
show ( ) # C r e a t e g raph a p p l i c a t i o n
Listing 3.2 The program GradesMatplot.py produces a simple, 2-D x-y plot using the matplotlib and
Visual packages. 
# Grade . py : P l o t t i n g o f g r a d e s t o i l l u s t r a t e m u l t i s e t s and v a r i o u s c u r v e s
2
from v i s u a l i m p o r t ∗ # C o n t a i n a r r a y command
4i m p o r t p y l a b # M a t p l o t l i b
6p y l a b . t i t l e (’Grade Inflation’ ) # T i t l e and l a b e l s
p y l a b . x l a b e l (’Years in College’ )
8p y l a b . y l a b e l (’GPA’ )
10xa = a r r a y ([−1 , 5 ] ) # For h o r i z o n t a l l i n e
ya = a r r a y ( [ 0 , 0 ] ) # Array o f z e r o s
12p y l a b . p l o t ( xa , ya ) # Draw h o r i z o n t a l l i n e
14x0 = a r r a y ( [ 0 , 1 , 2 , 3 , 4 ] ) # Data s e t 0 p o i n t s
y0 = a r r a y ( [−1 .4 , + 1 . 1 , 2 . 2 , 3 . 3 , 4 . 0 ] )
16p y l a b . p l o t ( x0 , y0 , ’bo’ ) # Data s e t 0 = b l u e c i r c l e s
p y l a b . p l o t ( x0 , y0 , ’g’ ) # Data s e t 0 = l i n e
18
y1 = a r r a y ( [ 4 . 0 , 2 . 7 , −1.8 , −0.9 , 2 . 6 ] ) # Data s e t 1 p o i n t s
20t = a r a n g e ( 0 , 5 , 1 )
p y l a b . p l o t ( t , y1 , ’r’ )
22
e r r 1 s u p = a r r a y ( [ 1 . 0 , 0 . 3 , 1 . 2 , 0 . 4 , 0 . 1 ] ) # Asymmetric e r r o r b a r s
24e r r 1 i n f = a r r a y ( [ 2 . 0 , 0 . 6 , 2 . 3 , 1 . 8 , 0 . 4 ] )
p y l a b . e r r o r b a r ( t , y1 , [ e r r 1 i n f , e r r 1 s u p ] , fmt = ’o’ ) # P l o t e r r o r b a r s
26
p y l a b . g r i d ( True ) # Gr id l i n e
28p y l a b . show ( ) # C r e a t e p l o t on s c r e e n
The plot command is actually quite versatile and can take an arbitrary number of sets of x and
y values as arguments to plot, as we see in GradesMatplot.py Listing 3.2 and Figure 3.1 Right.
We start on line 3 of by importing the Visual package so we can use the array command, while
on line 4 we import Matplotlib (pylab) for our plotting routines. What with two packages, we
will add the pylab prefix to the plot commands so that Python knows which package to use.
In order to place a horizontal line along y = 0, on line 10 and 11 we use the array command
to set up an array of x values with −1 ≤ x ≤ 5, a corresponding array of y values y ≡ 0, and
then plot these arrays (line 12). Next we go about placing four more plots in this one figure.
First on lines 14–16 we create a data set 0 as blue points and then connect them with a green
line. Second on lines 19–21 we plot up a different data set as a red line. Finally, on lines 23–25
we define unequal lower and upper error bars and have the plot command add them to data set
1 and plot them. We then add grid lines and show the plot on the screen.
Often, and especially when doing exploratory work and trying to preserve trees, it is
useful both to place several plots in one figure and also to have several different figures as
output. Matplotlib lets you do this with the plot and the subplot commands. For example,
in MatPlot2figs.py in Listing 3.3 and Figure 3.2 we have placed two plots in one figure, and
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
VISUALIZATION TOOLS 47
Figure 3.2 Left and Right columns show two separate outputs, each of two figures, produced by
MatPlot2figs.py. (We used the slider button to add some space between the red and blue plots.)
-6 -4 -2 0 2 4 6
x
-1.0
-0.5
0.0
0.5
1.0
f(
x
)
-sin(x)*cos(x^2)
-6 -4 -2 0 2 4 6
x
-2
-1
0
1
2
3
4
f(
x
)
exp(-x/4)*sin(x)
-6 -4 -2 0 2 4 6
x
0.0
0.2
0.4
0.6
0.8
1.0
f(
x
)
sin^2(x)*cos^2(x^2)
-6 -4 -2 0 2 4 6
x
0
2
4
6
8
10
12
f(
x
)
exp(-x/2)*sin^2(x)
then output two different figures, each containing two plots. The key here is repetition of the
commands:
figure(1) The 1st figure
subplot(2,1,1) 2 rows, 1 column, 1st subplot
subplot(2,1,2) 2 rows, 1 column, 2nd subplot
Listing 3.3 The program MatPlot2figs.py produces the two figures shown in Figure 3.2, with each having two plots
created by matplotlib. 
# M a t P l o t 2 f i g s . py : m a t p l o t l i b p l o t o f 2 s u b p l o t s on 1 f i g u r e & 2 s e p a r a t e f i g u r e s
2
from p y l a b i m p o r t ∗ # Load m a t p l o t l i b
4
Xmin = −5.0; Xmax = 5 . 0 ; N p o i n t s = 500
6DelX= ( Xmax−Xmin ) / N p o i n t s # D e l t a x
x1 = a r a n g e ( Xmin , Xmax , DelX ) # x1 r a n g e
8x2 = a r a n g e ( Xmin , Xmax , DelX / 2 0 ) # D i f f e r e n t x2 r a n g e
y1 = −s i n ( x1 )∗cos ( x1∗x1 ) # F u n c t i o n 1
10y2 = exp(−x2 / 4 . ) ∗ s i n ( x2 ) # F u n c t i o n 2
p r i n t "\n Now doing the plotting thing, look for Figures 1 & 2 on desktop"
12
# F i g u r e 1
14f i g u r e ( 1 )
s u b p l o t ( 2 , 1 , 1 ) # 1 s t s u b p l o t i n f i r s t f i g u r e
16p l o t ( x1 , y1 , ’r’ , lw =2)
x l a b e l (’x’ ) ; y l a b e l ( ’f(x)’ ) ; t i t l e ( ’-sin(x)*cos(xˆ2)’ ) # Words
18g r i d ( True ) # Form g r i d
s u b p l o t ( 2 , 1 , 2 ) # 2nd s u b p l o t i n f i r s t f i g u r e
20p l o t ( x2 , y2 , ’-’ , lw =2)
x l a b e l (’x’ ) # Axes l a b e l s
22y l a b e l ( ’f(x)’ )
t i t l e ( ’exp(-x/4)*sin(x)’ )
24
# F i g u r e 2
26f i g u r e ( 2 )
s u b p l o t ( 2 , 1 , 1 ) # 1 s t s u b p l o t i n 2nd f i g u r e
28p l o t ( x1 , y1∗y1 , ’r’ , lw =2)
x l a b e l (’x’ ) ; y l a b e l ( ’f(x)’ ) ; t i t l e ( ’sinˆ2(x)*cosˆ2(xˆ2)’ )
# form g r i d
30s u b p l o t ( 2 , 1 , 2 ) # 2nd s u b p l o t i n 2nd f i g u r e
p l o t ( x2 , y2∗y2 , ’-’ , lw =2)
32x l a b e l (’x’ ) ; y l a b e l ( ’f(x)’ ) ; t i t l e ( ’exp(-x/2)*sinˆ2(x)’ )
g r i d ( True )
34
show ( ) # Show g r a p h s
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
48 CHAPTER 3
Figure 3.3 A 3-D wire frame Left and a surface plot plus wireframe Right produced by SurfacePlot.py and
Matplotlib.
X
-3
-2
-1
0
1
2
3
Y
-3
-2
-1
0
1
2
3
Z
-1.0
-0.5
0.0
0.5
1.0
X
-3
-2
-1
0
1
2
3
Y
-3
-2
-1
0
1
2
3
Z
-1.0
-0.5
0.0
0.5
1.0
3.4.1 Matplotlib 3-D
A 2-D plot of V (r) = 1/r versus r is fine for visualizing the potential field surrounding a single
charge. However, when the same potential is viewed as a function of Cartesian coordinates,
V (x, y) = 1/
√
x2 + y2, we need a 3-D visualization. We get that by creating a world in which
the z dimension (mountain height) is the value of the potential, and x and y lie on a flat plane
below the mountain. Because the surface we are creating is a 3-D object, it is not possible to
draw it on a flat screen, and so different techniques are used to give the impression of three
dimensions to our brains. We do that by rotating the object, shading it, employing parallax,
and other tricks.
Listing 3.4 The program SurfacePlot.py produces surface (3-D) plots (Figure 3.3) using Matplotlib commands.
label=SurfacePlot.py 
# S imple3Dplo t . py : Simple m a t p l o t l i b 3D p l o t you can r o t a t e and s c a l e v i a mouse
2
i m p o r t p y l a b as p # i n c l u d e s numpy ( a r a n g e )
4from v i s u a l i m p o r t ∗ # f o r a r a n g e
i m p o r t m a t p l o t l i b . axes3d as p3
6p r i n t "Please be patient, I have packages to import and lots of points to plot"
d e l t a = 0 . 1
8x = a r a n g e ( −3. , 3 . , d e l t a )
y = a r a n g e ( −3. , 3 . , d e l t a )
10X, Y = p . meshgr id ( x , y )
Z = s i n (X)∗cos (Y) # s u r f a c e h e i g h t
12
f i g = p . f i g u r e ( ) # c r e a t e f i g u r e
14ax = p3 . Axes3D ( f i g ) # p l o t s axes
# ax . p l o t s u r f a c e (X, Y, Z ) # s u r f a c e
16ax . p l o t w i r e f r a m e (X, Y, Z , c o l o r = ’r’ ) # w i r e f r a m e
ax . s e t x l a b e l (’X’ )
18ax . s e t y l a b e l (’Y’ )
ax . s e t z l a b e l (’Z’ )
20
p . show ( ) # show f i g u r e
Although surface (3-D) plotting within Python is not currently maintained or supported by
the Matplotlib group, as we shall see in this section, the commands still work well. As an
alternative, or if you prefer to do your plotting after the data are computed, Gnuplot does very
nice 3-D plotting, and in §3.7.3 we discuss how to do that. Out approaches to Matplotlib and
Gnuplot for 3-D plotting have some differences; Matplotlib requires a function z(x, y), while
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
VISUALIZATION TOOLS 49
Gnuplot works with just the stored values of z.
In Figure 3.3 we show a wire-frame plot (left) and a surface-plus-wire-frame plot (right),
as obtained from the program SurfacePlot.py in Listing ??. Lines 8 and 9 are the standard
assignments of ranges for floats using arange, while line 10 uses Matplotlib’s meshgrid com-
mand to set up the grid of x and y values used for the surface plot. The rest of the program is
rather self-explanatory; fig is the plot object, ax is the 3-D axes object (the library axes3d is
part of the matplotlib extension for 3-D), and plot wireframe and plot surface create wire
frame and surface plots respectively.
3.4.2 Matplotlib Exercises
We encourage you to make your own plots and personalize them to your own likings by try-
ing out other commands and by including further options in the commands. The matplotlib
documentation is extensive and available on the Web. Explore:
1. how to zoom in and zoom out on sections of a plot,
2. how to save your plots in various formats,
3. how to print up your graphs,
4. how to pan through your graphs,
5. the effects obtained by adjusting the plot parameters available from the pull-down menu,
6. how to increase the space between subplots,
7. and for surfaces, explore how to rotate and scale the surfaces.
Listing 3.5 The program EasyVisual.py produces a 2-D x-y plot using Visualpackage. 
# E a s y V i s u a l . py : Simple g raph o b j e c t u s i n g V i s u a l
2
from v i s u a l . g raph i m p o r t ∗ # I mp or t V i s u a l
4
f u n c t 1 = g c u r ve ( c o l o r = c o l o r . cyan ) # Connec ted c u r v e o b j e c t
6
f o r x i n a r a n g e ( 0 . , 8 . 1 , 0 . 1 ) : # x r a n g e
8f u n c t 1 . p l o t ( pos = ( x , 5 .∗ cos ( 2 .∗ x )∗exp (−0.4∗x ) ) ) # P l o t p o i n t s
10graph1 = g d i s p l a y ( x = 0 , y = 0 , wid th = 600 , h e i g h t = 450 , # Second p l o t
t i t l e = ’Visual 2-D Plot’ , x t i t l e = ’x’ , y t i t l e = ’ f(x)’ ,
12xmax = 5 . , xmin = − 6 . , ymax = 1 , ymin = − 1 . 5 ,
f o r e g r o u n d = c o l o r . b l ack , background = c o l o r . w h i t e )
14
p l o t O b j = g d o t s ( c o l o r = c o l o r . b l a c k ) # Dots
16
f o r x i n a r a n g e ( −5. , +5 , 0 . 1 ) :
18p l o t O b j . p l o t ( pos = ( x , cos ( x ) ) )
3.5 2-D PLOTS WITH VISUAL (VPYTHON)
As we indicated in the introduction to this chapter, the Python Visual package is often used to
create 3-D demonstrations and animations for education, and, as we see now, it also can make
some nice 2-D graphs and animations. In Figure 3.4 we see the two separate plots produced by
the program EasyVisual.py in Listing 3.5. As you peruse this program you will notice that
the technique for plotting with Visual is quite different from that used by Matplotlib. While the
latter first creates arrays of x and y values and then does all the plotting in one fell swoop, this
Visual program creates plot objects and then add the points to them, one-by-one. What we also
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
50 CHAPTER 3
Figure 3.4 Screen dumps of x-y plots produced by the Visual package in Listing EasyVisual.py using default
(Left) and user-supplied (Right) options.
see in Listing 3.5 is how to create the two separate plots in Figure 3.4, the first using default
plotting parameters to produce a connected curve (gcurve), while the second produces just the
points (gdots) as well as axes labels and a title.
Listing 3.6 The program 3GraphVisual.py produces a simple, 2-D x-y plot using the matplotlib and
numpypackages. 
# 3 GraphVisua l . py : 3 p l o t s i n t h e same f i g u r e , w i th ba r s , d o t s and c u r v e
2
i m p o r t v i s u a l . g raph as vg
4
s t r i n g = "blue: sin(xˆ2), red: cos(x), black: sin(x)*cos(x)"
6graph1 = vg . g d i s p l a y ( x =0 , y =0 , wid th =600 , h e i g h t =350 , t i t l e = s t r i n g , x t i t l e =’x’ ,
y t i t l e =’y’ , xmax = 5 . , xmin =−5. , ymax = 1 . 1 , ymin =−0.6)
8# curve , v e r t i c a l ba r s , d o t s
y1 = vg . g cu rv e ( c o l o r =vg . c o l o r . b lue , d e l t a =3)
10y2 = vg . g v b a r s ( c o l o r =vg . c o l o r . r e d )
y3 = vg . g d o t s ( c o l o r =vg . c o l o r . whi te , d e l t a =3)
12
f o r x i n vg . a r a n g e (−5 , 5 , 0 . 1 ) : # a r a n g e f o r f l o a t s
14y1 . p l o t ( pos =( x , vg . s i n ( x )∗vg . s i n ( x ) ) )
y2 . p l o t ( pos =( x , vg . cos ( x )∗vg . cos ( x ) ) )
16y3 . p l o t ( pos =( x , vg . s i n ( x )∗vg . cos ( x ) ) )
As we already said for Matplotlib, it is often a good idea to place several plots in the
same figure. We give an example of this in 3GraphVisual.py (Listing 3.6) which produced
Figure 3.5 left. We see here red vertical bars (gvbars), dots (gdots), and a curve (gcurve).
Also note in 3GraphVisual.py that we avoid some very long plotting commands by starting
the program with
import visual.graph as vg
This both imports Visual’s graphing package, and permits us to use the symbol vg in place of
visual.graph in the commands.
Listing 3.7 The program VharmosAnimate.py solves the time-dependent Schrödinger equation for a probability
wave packet and creates an animation with Visual. 
# VHarmos animate . py : S o l v e s & a n i m a t e Schroed . Eqtn f o r wavepacke t i n HO
2
from v i s u a l i m p o r t ∗
4
# i n i t i a l i z e wave f u n c t i o n , p r o b a b i l i t y , p o t e n t i a l
6xmax =300; dx = 0 . 0 4 ; dx2=dx∗dx ; k0 = 5.5∗math . p i ; d t = dx2 / 4 . 0 ;
xp = −6; prob = z e r o s ( ( xmax +1) , F l o a t ) ; v= z e r o s ( ( xmax +1) , F l o a t ) ;
8p s r = z e r o s ( ( xmax +1 ,2 ) , F l o a t ) ; p s i = z e r o s ( ( xmax +1 ,2 ) , F l o a t )
10g= d i s p l a y ( wid th =500 , h e i g h t =250 , t i t l e =’Wave packet in harmonic well’ )
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
VISUALIZATION TOOLS 51
Figure 3.5 Left: Output graph from the program 3GraphVisual.py that produces a simple, 2-D x-y plot
using the matplotlib package. Right Three frames from the animation of a quantum mechan-
ical wave packet bound within a harmonic oscillator potential that was produced with the program
VharmosAnimate.py.
P l o t O b j = c u r v e ( x = r a n g e ( 0 , 3 0 0 + 1 ) , c o l o r = c o l o r . ye l low , r a d i u s = 4 . 0 )
12
# i n i t i a l c o n d i t i o n
14f o r i i n r a n g e ( 0 , xmax ) :
p s r [ i , 0 ] = math . exp (−0.5∗( xp / 0 . 5 ) ∗∗2) ∗ math . cos ( k0∗xp ) ; # Re wave P s i
16p s i [ i , 0 ] = math . exp (−0.5∗( xp / 0 . 5 ) ∗∗2) ∗ math . s i n ( k0∗xp ) ; # Im P s i
v [ i ] = 15.0∗ xp∗xp # P o t e n t i a l
18xp += dx
c o u n t = 0
20w h i l e 1 : # Time p r o p a g a t i o n
f o r i i n r a n g e ( 1 , xmax−1) : # Re P s i
22p s r [ i , 1 ] = p s r [ i , 0 ] − d t ∗( p s i [ i +1 ,0]+ p s i [ i −1 ,0]
−2.∗ p s i [ i , 0 ] ) / ( dx∗dx ) + d t∗v [ i ]∗ p s i [ i , 0 ]
24prob [ i ] = p s r [ i , 0 ]∗ p s r [ i , 1 ] + p s i [ i , 0 ]∗ p s i [ i , 0 ]
i f c o u n t %10==0: # Add p o i n t s t o p l o t
26j =0
f o r i i n r a n g e ( 1 , xmax−1 ,3) :
28P l o t O b j . x [ j ] = 2∗ i−xmax
P l o t O b j . y [ j ] = 130∗ prob [ i ]
30j = j +1
P l o t O b j . pos
32f o r i i n r a n g e ( 1 , xmax−1) :
p s i [ i , 1 ] = p s i [ i , 0 ] + d t ∗( p s r [ i +1 ,1]+ p s r [ i −1 ,1]−2.∗ p s r [ i , 1 ] ) / ( dx∗dx )−d t∗v [ i ]∗ p s r [ i , 1 ]
34
f o r i i n r a n g e ( 0 , xmax ) :
36p s i [ i , 0 ] = p s i [ i , 1 ]
p s r [ i , 0 ] = p s r [ i ] [ 1 ]
38c o u n t = c o u n t +1
3.6 ANIMATIONS WITH VISUAL (VPYTHON)
Animating with Visual is essentially making the same 2-D plot over and over again at slightly
different times, and plotting them on top of each other so as to give the impression of motion.
We give a number of sample codes that produce animation in the PythonCodes directory and
list one in Listing 3.7. You will need to run the code to experience the animation, although you
can get the idea from the three frames shown in Figure 3.5 right.
The code VharmosAnimate.py in Listing 3.7 comes from Chapter 18 in which we de-
scribe methods for solving the time-dependent Schrödinger equation with an arbitrary potential
for the complex wave function. Here we solve for a Gaussian wave packet bound within a har-
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
52 CHAPTER 3
Figure 3.6 A Gnuplot graph for three data sets with impulses and lines.
–6 –4 –2 0
30
20
10
0
–10
–20
–30
2 4 6
‘dat a. graph’
‘2. dat’
‘3. dat’
monic oscillator potential. Although we do not discuss the algorithm here, you should notice
how the animation is made. Lines 10 and 11 set up the plot, with PlotObj representing the plot
object. Lines 29 and 30 are within a loop that updates the points, with line 33 (still within the
loop) plotting the points. Since this is done continually (every 10 counts), the replotting ap-
pears as an animation. Note that being able to plot points individually without having to store
them all in an array for all times keep the memory demand of the program quite reasonable.
3.7 GNUPLOT: RELIABLE 2-D AND 3-D PLOTS
Gnuplot is a versatile 2-D and 3-D graphing package that makes Cartesian, polar, surface, and
contour plots. Although PtPlot is good for 2-D plotting with Java, only Gnuplot can create
surface plots of numerical data. Gnuplot is classic open software, available free on the Web,
and supports many output formats.
Begin Gnuplot with a file of (x, y) data points, say, graph.dat. Next issue the gnuplot
command from a shell or from the Start menu. A new window with the Gnuplot prompt
gnuplot> should appear. Construct your graph by entering Gnuplot commands at the Gnuplot
prompt or by using the pull-down menus in the Gnuplot window:
> gnuplot Start Gnuplot program
Terminal type set to ‘x11’ Type of terminal for Unix
gnuplot> The Gnuplot prompt
gnuplot> plot "graph.dat" Plot data file graph.dat
Plot a number of graphs on the same plot using several data files (Figure 3.6):
gnuplot> plot ‘graph.dat’ with impulses, ‘2.dat’, ‘3.dat’ with lines
The general form of the 2-D plotting command and its options are
plot {ranges} function {title} {style} {, function ... } Command
with points Default. Plot a symbol at each point.
with lines Plot lines connecting the points.
with linespoint Plot lines and symbols.
with impulses Plot vertical lines from the x axis to points.
with dots Plot a small dot at each point (scatterplots).
For Gnuplot to accept the name of an external file, that name must be placed in ‘single’ or
“double” quotes. If there are multiple file names, the names must be separated by commas.
Explicit values for the x and y ranges are set with options:
gnuplot> plot [xmin:xmax] [ymin:ymax] "file" Generic
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
VISUALIZATION TOOLS 53
gnuplot> plot [--10:10] [--5:30] "graph.dat" Explicit
3.7.1 Gnuplot Input Data Format 
The format of the data file that Gnuplot can read is not confined to (x, y) values. You may also
read in a data file as a C language scanf format string xy by invoking the using option in the
plot command. (Seeing that it is common for Linux/ Unix programs to use this format for
reading files, you may want to read more about it.)
plot ‘datafile’ { using { xy | yx | y } {"scanf string"} }
This format explicitly reads selected rows into x or y values while skipping past text or un-
wanted numbers:
gnuplot> plot "data" using "%f%f" Default, 1st x, 2nd y.
gnuplot> plot "data" using yx "%f %f" Reverse, 1st y, 2nd x.
gnuplot> plot "data" xy using "%*f %f %*f %f" Use row 2,4 for x, y.
gnuplot> plot "data" using xy "%*6c %f%*7c%f"
This last command skips past the first six characters, reads one x, skips the next seven charac-
ters, and then reads one y. It works for reading in x and y from files such as 
t h e t a : −20.000000 Energy : −3.041676 t h e t a : −19.000000 Energy :
−3.036427 t h e t a : −18.000000 Energy : −3.030596 t h e t a : −17.000000
Energy : −3.024081 t h e t a : −16.000000 Energy : −3.016755
Observe that because the data read by Gnuplot are converted to floating-point numbers, you
use %f to read in the values you want.
Besides reading data from files, Gnuplot can also generate data from user-defined and
library functions. In these cases the default independent variable is x for 2-D plots and (x, y)
for 3-D ones. Here we plot the acceleration of a nonharmonic oscillator:
gnuplot> k = 10 Set value for k
gnuplot> a(x) = .5*k*x**2 Analytic expression
gnuplot> plot [--10:10] a(x) Plot analytic function
A useful feature of Gnuplot is its ability to plot analytic functions along with numerical
data. For example, Figure 3.7 compares the theoretical expression for the period of a simple
pendulum to experimental data of the form 
# l e n g t h ( cm ) p e r i o d ( s e c )
10 0 . 8
20 0 . 9
30 1 . 2
40 1 . 3
50 1 . 5
60 1 . 6
70 1 . 7
80 1 . 8
90 1 . 9
100 2 . 0
Note that the first line of text is ignored since it begins with a # . We plot with
gnuplot> g = 980 Set value for g
gnuplot> y(x) = 2*3.1416*sqrt(x/g) Period T = y, length L = x
gnuplot> plot "exp.data", y(x) Plot both data and function
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
54 CHAPTER 3
Figure 3.7 A Gnuplot plot of data from a file plus an analytic function.
0.6
0.8
1
1.2
1.4
1.6
1.8
2
2.2
10 20 30 40 50 60 70 80 90 100
’exp.dat’
y(x)
3.7.2 Printing Plots
Gnuplot supports a number of printer types including PostScript. The safest way to print a plot
is to save it to a file and then print the file:
1. Set the “terminal type” for your printer.
2. Send the plot output to a file.
3. Replot the figure for a new output device.
4. Quit Gnuplot (or get out of the Gnuplot window).
5. Print the file with a standard print command.
For a more finished product, you can import Gnuplot’s output .ps file into a drawing program,
such as CorelDraw or Illustrator, and fix it up just right. To see what types of printers and
other output devices are supported by Gnuplot, enter the set terminal command without any
options in a gnuplot shell. Here is an example of creating and printing a PostScript figure:
gnuplot> set terminal postscript Choose local printer type
Terminal type set to ‘postscript’ Gnuplot response
gnuplot> set term postscript eps Another option
gnuplot> set output "plt.ps" Send figure to file
gnuplot> replot Plot again so file is sent
gnuplot> quit Or get out of gnu window
% lp plt.ps Unix print command
3.7.3 Gnuplot Surface (3-D) Plots
A 2-D plot is fine for visualizing the potential field V (r) = 1/r surrounding a single
charge. However, when the same potential is expressed as a function of Cartesian coordi-
nates, V (x, y) = 1/
√
x2 + y2, we need a 3-D visualization. We get that by creating a world
in which the z dimension (mountain height) is the value of the potential and x and y lie on a
flat plane below the mountain. Because the surface we are creating is a 3-D object, it is not
possible to draw it on a flat screen, and so different techniques are used to give the impression
of three dimensions to our brains. We do that by rotating the object, shading it, employing
parallax, and other tricks.
In Gnuplot, the surface (3-D) plot command is splot, and it is used in the same manner
as plot—with the obvious extension to (x, y, z). A surface (Figure 3.8) is specified by placing
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
VISUALIZATION TOOLS 55
Figure 3.8 A surface plot z(x, y) = z (row, column) showing the input data format used for creating it. Only z values
are stored, with successive rows separated by blank lines and the column values repeating for each row.
Z(x,y)
y
0
0
10
40
20
x
row
 1 (
z11
, z12
, ...)
row
 2 (
z21
, z22
, ...)
row
 3 (
z31
, z32
, ...)
row
 4 (
z41
, z42
, ...)
the z(x, y) values in a matrix but without ever giving the x and y values explicitly. The x values
are the row numbers in the matrix and the y values are the column values (Figure 3.8). This
means that only the z values are read in and that they are read in row by row, with different
rows separated by blank lines:
row 1 (blank line) row 2 (blank line) row 3 . . . row N.
Here each row is input as a column of numbers, with just one number per line. For example,
13 columns each with 25 z values would be input as a sequence of 25 data elements, followed
by a blank line, and then another sequence followed by a blank line, and so on: 
0 . 0
0 .695084369397148
1.355305208363503
1.9461146066793003
. . .
−1.0605832625347442
−0.380140746321537
[ b l a n k l i n e ]
0 . 0
0 .6403868757235301
1.2556172093991282
. . .
2 .3059977070286473
2.685151549102467
[ b l a n k l i n e ]
2 .9987593603912095
. . .
Although there are no explicit x and y values given, Gnuplot plots the data with the x and y
assumed to be the row and column numbers.
Versions 4.0 and above of Gnuplot have the ability to rotate 3-D plots interactively. You
may also adjust your plot with the command
gnuplot> set view rotx, rotz, scale, scalez
where 0 ≤ rotx ≤ 180◦ and 0 ≤ rotz ≤ 360◦ are angles in degrees and the scale factors
control the size. Any changes made to a plot are made when you redraw the plot using the
replot command.
To see how this all works, here we give a sample Gnuplot session that we will use in
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
56 CHAPTER 3
Chapter 17, “PDEs for Electrostatics & Heat Flow,” to plot a 3-D surface from numerical data.
The program Laplace.java contains the actual code used to output data in the form for a
Gnuplot surface plot.4
> gnuplot Start Gnuplot system from a shell
gnuplot> set hidden3d Hide surface whose view is blocked
gnuplot> set nohidden3d Show surface though hidden from view
gnuplot> splot ‘Laplace.dat’ with lines Surface plot of Laplace.dat with lines
gnuplot> set view 65,45 Set x and y rotation viewing angles
gnuplot> replot See effect of your change
gnuplot> set contour Project contours onto xy plane
gnuplot> set cntrparam levels 10 10 contour levels
gnuplot> set terminal epslatex Output in Encapsulated PostScript for LaTeX
gnuplot> set terminal PostScript Output in PostScript format for printing
gnuplot> set output "Laplace.ps" Plot output to be sent to file Laplace.ps
gnuplot> splot ‘Laplace.dat’ w l Plot again, output to file
gnuplot> set terminal x11 To see output on screen again
gnuplot> set title ‘Potential V(x,y) vs x,y’ Title graph
gnuplot> set xlabel ‘x Position’ Label x axis
gnuplot> set ylabel ‘y Position’ Label y axis
gnuplot> set zlabel ‘V(x,y)’; replot Label z axis and replot
gnuplot> help Tell me more
gnuplot> set nosurface Do not draw surface; leave contours
gnuplot> set view 0, 0, 1 Look down directly onto base
gnuplot> replot Draw plot again; may want to write to file
gnuplot> quit Get out of Gnuplot
3.7.4 Gnuplot Vector Fields
Even though it is simpler to compute a scalar potential than a vector field, vector fields often
occur in nature. In Chapter 17, “PDEs for Electrostatics & Heat Flow,” we show how to
compute the electrostatic potential U(x, y) on an x + y grid of spacing ∆. Since the field is
the negative gradient of the potential, E = −~∇U(x, y), and since we solve for the potential
on a grid, it is simple to use the central-difference approximation for the derivative (Chapter 7
“Differentiation & Searching”) to determine E:
xmll Ex'
U(x+ ∆, y)− U(x−∆, y)
2∆
=
Ui+1,j − Ui−1,j
2∆
, (3.1)
Ey '
U(x, y + ∆)− U(x, y −∆)
2∆
=
Ui,j+1 − Ui,j−1
2∆
. (3.2)
Gnuplot contains the vectors style for plotting vector fields as arrows of varying lengths and
directions (Figure 3.9).
> plot ‘Laplace field.dat’ using 1:2:3:4 with vectors Vector plot
Here Laplace field.data is the data file of (x, y, Ex, Ey) values, the explicit columns to
plot are indicated, and additional information can be provided to control arrow types. What
Gnuplot actually plots are vectors from (x, y) to (x + ∆x, y + ∆y), where you input a data
file with each line containing the (x, y,∆x,∆y) values. Thousands of tiny arrows are not very
illuminating (Figure 3.9 left), nor are overlapping arrows. The solution is to plot fewer points
4Under Windows, there is a graphical interface that is friendlier than the Gnuplot subcommands. The subcommand approach
we indicate here is reliable and universal.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
VISUALIZATION TOOLS 57
Figure 3.9 Two visualizations created by Gnuplot of the same electric field within and around a parallel plate ca-
pacitor. The figure on the right includes equipotential surfaces and uses one-fifth as many field points,
and longer vectors, but of constant length. The x and y values are the column and row indices.
y
x
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100
y
x
0
5
10
15
20
0 5 10 15 20
and larger arrows. On the right in Figure 3.9 we plot every fifth point normalized to unit length
via
xmll∆x =
Ex
N
, ∆y =
Ey
N
, N =
√
E2x + E2y . (3.3)
The data file was produced with our Laplace.java program with the added lines 
ex = −( U[ i + 1 ] [ j ] − U[ i −1][ j ] ) ; / / Compute f i e l d
components
ey = −( U[ i ] [ j +1] − U[ i ] [ j−1] ) ;
enorm = Math . s q r t ( ex∗ex + ey∗ey ) ; / /
Normal izat ion f a c t o r
w. p r i n t l n (" "+ i /5+" "+ j /5+" "+ex / enorm +" "+ey / enorm +" " ) ; / / Output
And here are some commands that add contour lines to the field plots:
gnuplot> unset key
gnuplot> set nosurface
gnuplot> set contour base
gnuplot> set cntrparam levels 10
gnuplot> set view 0,0,1,1
gnuplot> splot ‘Laplace pot1.dat’ with lines
gnuplot> set terminal push
gnuplot> set terminal table
gnuplot> set out ‘equipot.dat’
gnuplot> replot
gnuplot> set out
gnuplot> set terminal pop
gnuplot> reset
gnuplot> plot ‘equipot.dat’ with lines
gnuplot> unset key
gnuplot> plot ‘equipot.dat’ with lines, ‘Laplace field1.dat’ with vectors
By setting terminal to table and setting out to equipot.dat, the numerical data for equipo-
tential lines are saved in the file equipot.dat. This file can then be plotted together with the
vector field lines.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
58 CHAPTER 3
3.7.5 Animations from Gnuplot
An animation is a collection of images called frames that when viewed in sequence con-
vey the sensation of continuous motion. It is an excellent way to visualize the time behavior
of a simulation or of a function f(x, t), as may occur in wave motion or heat flow. In the
Codes section of the CD, we give several sample animations of the figures in this book and we
recommend that you view them.
Gnuplot itself does not create animations. However, you can use it to create a sequence
of frames and then concatenate the frames into a movie. Here we create an animated gif that,
when opened with a Web browser, automatically displays the frames in a rapid enough se-
quence for your mind’s eye to see them as a continuous event. Although Gnuplot does not
output .gif files, we outputted pixmap files and converted them to gif’s. Because a num-
ber of commands are needed for each frame and because hundreds or thousands of frames
may be needed for a single movie, we wrote the script MakeGifs.script to automate the
process. (A script is a file containing a series of commands that might normally be entered
by hand to execute within a shell. When placed in a script, they are executed as a single
command.) The script in Listing 3.8, along with the file samp color (both on the CD under
Codes/Animations ColorImages/Utilities), should be placed in the directory containing
the data files (run.lmn in this case).
The #! line at the beginning tells the computer that the subsequent commands are in the
korn shell. The symbol $i indicates that i is an argument to the script. In the present case, the
script is run by giving the name of the script with three arguments, (1) the beginning time, (2)
the maximum number of times, and (3) the name of the file where you wish your gifs to be
stored:
% MakeGifs.script 1 101 OutFile Make gif from times 1 to 101 in OutFile
The > symbol in the script indicates that the output is directed to the file following the
>. The ppmquant command takes a pixmap and maps an existing set of colors to new ones. For
this to work, you must have the map file samp colormap in the working directory. Note that
the increments for the counter, such as i=i+99, should be adjusted by the user to coincide with
the number of files to be read in, as well as their names. Depending on the number of frames,
it may take some time to run this script. Upon completion, there should be a new set of files of
the form OutfileTime.gif, where Outfile is your chosen name and Time is a multiple of the
time step used. Note that you can examine any or all of these gif files with a Web browser.
The final act of movie making is to merge the individual gif files into an animated gif
with a program such as gifmerge:
> gifmerge --10 *.gif > movie Merge all .gif files into movie
Listing 3.8 MakeGifs.script, a script for creating animated gifs. 
#! / b i n / ksh
u n a l i a s rm
i n t e g e r i =$1
w h i l e t e s t i − l t $2
do
i f t e s t i − l t 10
t h e n
p r i n t "set terminal pbm small color; set output\"$3t=0000$i.ppm\"; set noxtics; set noytics;
set size 1.0, 1.0; set yrange [0:.1];
plot ’run.0000$i’ using 1:2 w lines, ’run.0000$i’ using 1:3 w lines;
" >d a t a 0 0 0 0 $ i . gnu
g n u p l o t d a t a 0 0 0 0 $ i . gnu
ppmquant −map samp colormap $ 3 t =0000 $ i . ppm>$ 3 a t =0000 $ i . ppm
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
VISUALIZATION TOOLS 59
p p m t og i f −map samp colormap $ 3 a t =0000 $ i . ppm > $30000$i . g i f
rm $ 3 t =0000 $ i . ppm $ 3 a t =0000 $ i . ppm d a t a 0 0 0 0 $ i . gnu
i = i +99
f i
i f t e s t i −g t 9 −a i − l t 1000
t h e n
p r i n t "set terminal pbm small color; set output\"$3t=00$i.ppm\"; set noxtics; set noytics;
set size 1.0, 1.0; set yrange [0:.1];
plot ’run.00$i’ using 1:2 w lines, ’run.00$i’ using 1:3 w lines;
" >d a t a 0 0 $ i . gnu
g n u p l o t d a t a 0 0 $ i . gnu
ppmquant −map samp colormap $ 3 t =00 $ i . ppm>$ 3 a t =00 $ i . ppm
p p m t og i f −map samp colormap $ 3 a t =00 $ i . ppm > $300$ i . g i f
rm $ 3 t =00 $ i . ppm $ 3 a t =00 $ i . ppm d a t a 0 0 $ i . gnu
i = i +100
f i
i f t e s t i −g t 999 −a i − l t 10000
t h e n
p r i n t "set terminal pbm small color; set output\"$3t=0$i.ppm\"; set noxtics; set noytics;
set size 1.0, 1.0; set yrange [0:.1];
plot ’run.0$i’ using 1:2 w lines, ’run.0$i’ using 1:3 w lines;
" >d a t a 0 $ i . gnu
g n u p l o t d a t a 0 $ i . gnu
ppmquant −map samp colormap $ 3 t =0 $ i . ppm>$ 3 a t =0 $ i . ppm
p p m t og i f −map samp colormap $ 3 a t =0 $ i . ppm > $30$ i . g i f
rm $ 3 t =0 $ i . ppm $ 3 a t =0 $ i . ppm d a t a 0 $ i . gnu
i = i +100
f i
done
Here the --10 separates the frames by 0.1 s in real time, and the * is a wildcard that will in-
clude all .gif files in the present directory. Because we constructed the .gif files with sequential
numbering, gifmerge pastes them together in the proper sequence and places them in the file
movie, which can be viewed with a browser.
3.8 OPENDX FOR DICING AND SLICING
See Appendix C and the CD.
3.9 TEXTURING AND 3-D IMAGING
In §13.10 we give a brief explanation of how the inclusion of textures (Perlin noise) in a visu-
alization can add an enhanced degree of realism. While it is a useful graphical technique, it
incorporates the type of correlations and coherence also present in fractals and thus is in Chap-
ter 13, “Fractals & Statistical Growth.” In a related vein, in §13.10.1 we discuss the graphical
technique of ray tracing and how it, especially when combined with Perlin noise, can produce
strikingly realistic visualizations.
Stereographic imaging creates a virtual reality in which your brain and eye see objects
as if they actually existed in our 3-D world. There are a number of techniques for doing this,
such as virtual reality caves in which the viewer is immersed in an environment with images
all around, and projection systems that project multiple images, slightly displaced, such that
the binocular vision system in your brain (possibly aided by appropriate glasses) creates a 3-D
image in your mind’s eye.
Stereographics is often an effective way to let the viewer see structures that might oth-
erwise be lost in the visualization of complex geometries, such as in molecular studies or star
creation. But as effective as it may be, stereo viewing is not widely used in visualization be-
cause of the difficulty and expense of creating and viewing images. Here we indicate how the
low-end, inexpensive viewing technique known as ChromaDepth [Chrom] can produce many
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
60 CHAPTER 3
of the same effects as high-end stereo vision without the use of special equipment. Not only is
the technique easy to view and easy to publish, it is also easy to create [Bai 05]. Indeed, the
OpenDX color images (visible on the CD) work well with ChromaDepth.CD
ChromaDepth consists of two pieces: a simple pair of glasses and a display methodology.
The glasses contain very thin, diffractive gratings. One grating is blazed so that it shifts colors
on the red end of the spectrum more than on the blue end, and this makes the red elements
in the 3-D scene appear to be closer to the viewer. This often works fine with the same color
scheme used for coding topographic maps or for scientific visualizations and so requires little
or no extra work. You just write your computer program so that it color-codes the output in
a linear rainbow spectrum based on depth. If you do not wear the glasses, you still see the
visualization, yet with the glasses on, the image appears to jump out at you.
3.10 GRACE/ACE: SUPERB 2-D GRAPHS FOR UNIX/LINUX
Our favorite package for producing publication-quality 2-D graphs from numerical data is
Grace/xmgrace. It is free, easy to work with, incredibly powerful, and has been used for many
of the (nicer) figures in our research papers and books. Grace is a WYSIWYG tool that also
contains powerful scripting languages and data analysis tools, although we will not get into
that. We will illustrate multiple-line plots, color plots, placing error bars on data, multiple-axis
options and such. Grace is derived from Xmgr, aka ACE/gr, originally developed by the Ore-
gon Graduate Institute and now developed and maintained by the Weizmann Institute [Grace].
Grace is designed to run under the Unix/Linux operating systems, although we have had suc-
cess using in on an MS Windows system from within the Cygwin [CYG] Linux emulation.5
3.10.1 Grace Basics
To learn about Grace properly, we recommend that you work though some of the tutorials
available under its Help menu, study the user’s guide, and use the Web tutorial [Grace]. We
present enough here to get you started and to provide a quick reference. The first step in
creating a graph is to have the data you wish to plot in a text (ASCII) file. The data should be
broken up into columns, with the first column the abscissa (x values) and the second column
the ordinate (y values). The columns may be separated by spaces or tabs but not by commas.
For example, the file Grace.dat on the CD and in the left column of Table 3.1 contains one
abscissa and one ordinate per line, while the file Grace2.dat in the right column of Table 3.1
contains one abscissa and two ordinates (y values) per line.
1. Open Grace by issuing the grace or xmgrace command from a Unix/ Linux/Cygwin
command line (prompt):
> grace Start Grace from Unix shell
In any case, make sure that your command brings up the user-friendly graphical inter-
face shown on the left in Figure 3.10 and not the pure-text command one.
2. Plot a single data set by starting from the menu bar on top. Then
5If you do this, make sure to have the Cygwin download include the xorg-x11-base package in the X11 category (or a
later version), as well as xmgrace.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
VISUALIZATION TOOLS 61
Table 3.1 ∗The text file Grace.dat (on the CD under Codes/JavaCodes/Data) contains one x value and one y
value per line. The file Grace2.dat contains one x value and two y values per line.
Text Files Grace.dat and Grace2.dat∗
Grace.dat Grace2.dat
x y x y z
1 2 1 2 50
2 4 2 4 29
3 5 3 5 23
4 7 4 7 20
5 10 5 10 11
6 11 6 11 10
7 20 7 20 7
8 23 8 23 5
9 29 9 29 4
10 50 10 50 2
a. Select progressively from the submenus that appear, Data/Import/ASCII.
b. The Read sets window shown on the right in Figure 3.10 appears.
c. Select the directory (folder) and file in which you have the data; in the present case
select Grace.dat.
d. Select Load as/Single set, Set type/XY and Autoscale on read/XY.
e. Select OK (to create the plot) and then Cancel to close the window.
3. Plot multiple data sets (Figure 3.10) by following similar procedure as in step 2, only
now
a. Select Grace2.dat, which contains two y values as shown in Table 3.1.
b. Change Load as to NXY to indicate multiple data sets and then plot.
Note: We suggest that you start your graphs off with Autoscale on read in order to see
all the data sets plotted. You may then change the scale if you want, or eliminate some
points and replot.
4. Label and modify the axis properties by going back to the main window. Most of the
basic utilities are under the Plot menu.
a. Select Plot/Axis properties.
b. Within the Axis Property window that appears, select Edit/X axis or Edit/Y axis as ap-
propriate.
c. Select Scale/Linear for a linear plot, or Scale/Logarithmic for a logarithmic or semilog
plot.
d. Enter your choice for Axis label in the window.
e. Customize the ticking and the numbers’ format to your heart’s desire.
f. Choose Apply to see your changes and then Accept to close the window.
5. Title the graph, as shown on the left in Figure 3.10, by starting from the main window
and again going to the Plot menu.
a. Select Plot/Graph appearance.
b. From the Graph Appearance window that appears, select the Main tab and from there
enter the title and subtitle.
6. Label the data sets, as shown by the box on the left in Figure 3.10, by starting from
the main window and again going to the Plot menu.
a. Select Plot/Set Appearance.
b. From the Set Appearance window that appears, highlight each set from the Select set
window.
c. Enter the desired text in the Legend box.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
62 CHAPTER 3
Figure 3.10 Left: The main Grace window, with the file Grace2.dat plotted with the title, subtitle, and labels.
Right: The data input window.
Figure 3.11 Left: A plot of points as symbols with no line. Right: A plot of points as symbols with lines connecting
the points and with error bars read from the file.
d. Choose Apply for each set and then Accept.
e. You can adjust the location, and other properties, of the legend box from Plot/Graph
Appearance/Leg. box.
7. Plotting points as symbols (Figure 3.11 left) is accomplished by starting at the main
menu. Then
a. Select Plot/Set Appearance.
b. Select one of the data sets being plotted.
c. Under the Main/Symbol properties, select the symbol Type and Color.
d. Choose Apply, and if you are done with all the data sets, Accept.
8. Including error bars (Figure 3.11 left) is accomplished by placing
them in the data file read into Grace along with the data points. Un-
der Data/Read Sets are, among others, these possible formats for Set type:
( X Y DX ), ( X Y DY ), ( X Y DX DX ), ( X Y DY DY ), ( X Y DX DX DY DY )
Here DX is the error in the x value, DY is the error in the y value, and repeated values
for DX or DY are used when the upper and lower error bars differ; for instance, if there
is only one DY, then the data point is Y± DY, but if there are two error bars given, then
the data point is Y + DY1, −DY2. As a case in point, here is the data file for ( X Y DY ):
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
VISUALIZATION TOOLS 63
Figure 3.12 Left: Four graphs placed in a 2× 2 matrix. Right: The window that opens under Edit/Arrange Graphs
and is used to set up the matrix into which multiple graphs are placed.
0 2 4 6 8 10
X values
0
10
20
30
40
50
Y
 v
al
ue
s
Points as symbols with 
error bars (X Y DY)
Line properties = dashed
0 2 4 6 8 10
0
10
20
30
40
50
0
Graph 2 
0 2 4 6 8 10
0
10
20
30
40
50
0 2 4 6
10
20
30
40
50
X 1 2 3 4 5 6 7 8 9 10
Y 2 4 5 7 10 11 20 23 29 50
DY 3 2 3 3.6 2.6 5.3 3.1 3.9 7 8
9. Multiple plots on one page (Figure 3.12 left) are created by starting at the main
window. Then
a. Select Edit/Arrange Graphs.
b. An Arrange Graphs window (Figure 3.12 right) opens and provides the options for
setting up a matrix into which your graphs are placed.
c. Once the matrix is set up (Figure 3.12 left), select each space in sequence and then
create the graph in the usual way.
d. To prevent the graphs from getting too close to each other, go back to the Arrange
Graphs window and adjust the spacing between graphs.
10. Printing and saving plots
a. To save your plot as a complete Grace project that can be opened again and edited,
from the main menu select File/Save As and enter a filename.agr as the file name. It
is a good idea to do this as a backup before printing your plot (communication with
a piece of external hardware is subject to a number of difficulties, some of which
may cause a program to “freeze up” and for you to lose your work).
b. To print the plot, select File/Print Setup from the main window
c. If you want to save your plot to a file, select Print to file and then enter the file name.
If you want to print directly to a printer, make sure that Print to file is not selected
and that the selected printer is the one to which you want your output to go (some
people may not take kindly to your stuff appearing on their desk, and you may not
want some of your stuff to appear on someone else’s desk).
d. From Device, select the file format that will be sent to the printer or saved.
e. Apply your settings when done and then close the Print/Device Setup window by se-
lecting Accept.
f. If now, from the main window, you select File/Print, the plot will be sent to the printer
or to the file. Yes, this means that you must “Print” the plot in order to send it to a
file.
If you have worked through the steps above, you should have a good idea of how Grace works.
Basically, you just need to find the command you desire under a menu item. To help you in
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
64 CHAPTER 3
Grace Menu and Submenu Items
Edit Data
Data sets Data set operations
Set operations sort, reverse, join, split, drop points
copy, move, swap Transformations
Arrange graphs expressions, histograms, transforms,
matrix, offset, spacing convolutions, statistical ops,
interpolations
Overlay graphs Feature extraction
Autoscale graphs min/max, average, deviations,
frequency,
Regions COM, rise/fall time, zeros
Hot links Import
Set/Clear local/fixed point Export
Preferences
Plot View
Plot appearance Show locator bar (default)
background, time stamp, font, color Show status bar (default)
Graph appearance Show tool bar (default)
style, title, labels, frame, legends Page setup
Set appearance Redraw
style, symbol properties, error bars Update all
Axis properties
labels, ticks, placement
Load/Save parameters
Window
Command Font tool
Point explorer Console
Drawing objects
your search, in Table 3.10.1 we list the Grace menu and submenu items.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Four
Python Object-Oriented Programs:
Impedance & Batons
This chapter contains two units dealing with object-oriented programming (OOP) at
increasing levels of sophistication. In most of the codes in this book we try to keep our
programming transparent to a wide class of users and to keep our Python examples similar
to those in C and Fortran. Accordingly, we have deliberately shied away from the use of ad-
vanced OOP techniques. Nevertheless, OOP is a key element in modern programming and so
it is essential that all readers have some understanding of it. We recommend that you read
Unit I (the example is easy) so that you are comfortable declaring, creating, and manipulating
both static and dynamic objects. Unit II deals with more advanced aspects of OOP and, while
recommended, may be put off for later reading, especially for those who are object-challenged
at this stage in their computing careers.
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
Object Oriented Programming I 4.1–4.5 Object Oriented Programming II N A
Applets
Name Sections Name Sections
Starbrite (H-R diagram) - Baton 4.9
4.1 UNIT I. BASIC OBJECTS; COMPLEX IMPEDANCE
Problem: We are given a circuit containing a resistor of resistance R, an inductor of induc-
tance L, and a capacitor of capacitance C (Figure 4.1 left). All three elements are connected in
series to an alternating voltage source V (t) = V0 cos ωt. Determine the magnitude and time
dependence of the current in this circuit as a function of the frequency ω.
We solve this RLC circuit for you and assign as your particular problem that you repeat
the calculation for a circuit in which there are two RLC circuits in parallel (Figure 4.1 right).
Assume a single value for inductance and capacitance, and three values for resistance:
L = 1000 H, C =
1
1000
F, R =
1000
1.5
,
1000
2.1
,
1000
5.2
Ω. (4.1)
Consider frequencies of applied voltage in the range 0 < ω < 2/
√
LC = 2/s.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
66 CHAPTER 4
Figure 4.1 Left: An RLC circuit connected to an alternating voltage source. Right: Two RLC circuits connected in
parallel to an alternating voltage. Observe that one of the parallel circuits has double the values of R, L,
and C as does the other.
R R 2R
L L 2L
C C 2C
4.2 COMPLEX NUMBERS (MATH)
Complex numbers are useful because they let us double our work output with only the slightest
increase in effort. This is accomplished by manipulating them as if they were real numbers and
then separating the real and imaginary parts at the end of the calculation. We define the symbol
z to represent a number with both real and imaginary parts (Figure 4.2 left):
z = x+ iy, Re z = x, Im z = y. (4.2)
Here i def=
√
−1 is the imaginary number, and the combination of real plus imaginary numbers
is called a complex number. In analogy to a vector in an imaginary 2-D space, we also use polar
coordinates to represent the same complex number:
xmll
r =
√
x2 + y2, θ = tan−1(y/x), (4.3)
x = r cos θ, y = r sin θ. (4.4)
The essence of the computing aspect of our problem is the programming of the rules of arith-
metic for complex numbers. This is an interesting chore because while most computer lan-
guages contain all the rules for real numbers, you must educate them as to the rules for com-
plex numbers (Fortran and Python being the well-educated exceptions). So although complex
numbers are a primitive data type in Python, for pedagogic purposes, and to match the Java
version of this text, we here construct complex numbers as objects. We start with two complex
numbers, which we distinguish with subscripts:
z1 = x1 + i y1, z2 = x2 + i y2. (4.5)
Complex arithmetic rules derive from applying algebra to z’s Re and Im parts:
Addition: z1 + z2 = (x1 + x2) + i(y1 + y2), (4.6)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 67
Figure 4.2 Left: Representation of a complex number as a vector in space. Right: An abstract drawing, or what?
Im
 z
 =
 y
y
z(x,y)
Re z = x
r
x
θ
Subtraction: z1 − z2 = (x1 − x2) + i(y1 − y2), (4.7)
Multiplication: z1 × z2 = (x1 + iy1)× (x2 + iy2) (4.8)
= (x1x2 − y1y2) + i(x1y2 + x2y1)
Division:
z1
z2
=
x1 + iy1
x2 + iy2
× x2 − iy2
x2 − iy2
(4.9)
=
(x1x2 + y1y2) + i(y1x2 − x1y2)
x22 + y
2
2
.
An amazing theorem by Euler relates the base of the natural logarithm system, complex
numbers, and trigonometry:
eiθ = cos θ + i sin θ (Euler’s theorem). (4.10)
This leads to the polar representation of complex numbers (Figure 4.2 left),
z ≡ x+ iy = reiθ = r cos θ + ir sin θ. (4.11)
Likewise, Euler’s theorem can be applied with a complex argument to obtain
ez = ex+iy = exeiy = ex(cos y + i sin y). (4.12)
4.3 RESISTANCE BECOMES IMPEDANCE (THEORY)
We apply Kirchhoff’s laws to the RLC circuit in Figure 4.1 left by summing voltage drops as
we work our way around the circuit. This gives the differential equation for the current I(t) in
the circuit:
dV (t)
dt
= R
dI
dt
+ L
d2I
dt2
+
I
C
, (4.13)
where we have taken an extra time derivative to eliminate an integral over the current. The
analytic solution follows by assuming that the voltage has the form V (t) = V0 cos ωt and by
guessing that the resulting current I(t) = I0e−iωt will also be complex, with its real part the
physical current. Because (4.13) is linear in I , the law of linear superposition holds, and so we
can solve for the complex I and then extract its real and imaginary parts:
xmll
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
68 CHAPTER 4
I(t) =
1
Z
V0e
−iωt, Z = R+ i
(
1
ωC
− ωL
)
, (4.14)
⇒ I(t) = V0
|Z|
e−i(ωt+θ) =
V0
|Z|
[cos(ωt+ θ)− i sin(ωt+ θ)] , (4.15)
|Z|=
√
R2 +
(
1
ωC
− ωL
)2
, θ = tan−1
(
1/ωC − ωL
R
)
.
We see that the amplitude of the current equals the amplitude of the voltage divided by the
magnitude of the complex impedance, and that the phase of the current relative to that of the
voltage is given by θ.
The solution for the two RLC circuits in parallel (Figure 4.1 right) is analogous to that
with ordinary resistors. Two impedances in series have the same current passing through them,
and so we add voltages. Two impedances in parallel have the same voltage across them, and so
we add currents:
Zser = Z1 + Z2,
1
Zpar
=
1
Z1
+
1
Z2
. (4.16)
4.4 ABSTRACT DATA STRUCTURES, OBJECTS (CS)
What do you see when you look at the abstract object on the right of Figure 4.2? Some readers
may see a face in profile, others may see some parts of human anatomy, and others may see a
total absence of artistic ability. This figure is abstract in the sense that it does not try to present
a true or realistic picture of the object but rather uses a symbol to suggest more than meets the
eye. Abstract or formal concepts pervade mathematics and science because they make it easier
to describe nature. For example, we may define v(t) as the velocity of an object as a function
of time. This is an abstract concept in the sense that we cannot see v(t) but rather just infer it
from the changes in the observable position. In computer science we create an abstract object
by using a symbol to describe a collection of items. In Python we have built- in or primitive
data types such as integers, floating-point numbers, Booleans, and strings. In addition, we
may define abstract data structures of our own creation by combining primitive data types into
more complicated structures called objects. These objects are abstract in the sense that they are
named with a single symbol yet they contain multiple parts.
To distinguish between the general structure of objects we create and the set of data that
its parts contain, the general object is called a class, while the object with specific values
for its parts is called an instance of the class, or just an object . In this unit our objects will
be complex numbers, while at other times they may be plots, vectors, or matrices. The classes
that we form will not only contain objects (data structures) but also the associated methods for
modifying the objects, with the entire class thought of as an object.
In computer science, abstract data structures must possess three properties:
1. Typename: Procedure to construct new data types from elementary pieces.
2. Set values: Mechanism for assigning values to the defined data type.
3. Set operations: Rules that permit operations on the new data type (you would not have
gone to all the trouble of declaring a new data type unless you were interested in doing
something with it).
In terms of these properties, when we declare a variable to have two (real and imaginary) parts,
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 69
we satisfy property 1. When we assign doubles to the parts, we satisfy property 2. And
when we define addition and subtraction, we satisfy property 3. The actual process of creating
objects in your Python program uses nonstatic, dynamic or active variables and methods that
modify or interact with objects. It is these dynamic (nonstatic) method that utilize the power
of object-oriented programming.
4.4.1 Object Declaration and Construction
Though we may assign a name like x to an object, because objects have multiple components
you cannot assign one explicit value to an object. It follows then, that when Python deals
with objects, it does so by reference; that is, the name of the variable refers to the location
in memory where the object is stored and not to the explicit values of the object’s parts. To
see what this means in practice, the class file Complex.py in Listing 4.1 adds and multiplies
complex numbers, with the complex numbers represented as objects.
Listing 4.1 ComplexDummy.py defines the Complex class that permits the use of complex data objects. The
constructor method init uses z as an intermediary or dummy variable. 
# ComplexDummy . py : c l a s s Complex p e r f o r m s complex a l g e b r a wi th dummy i n t e r m e d i a r y
2
c l a s s Complex :
4d e f i n i t ( z , x , y ) : # c l a s s c o n s t r u c t o r
z . r e = x # a s s i g n r e a l p a r t o f complex
6z . im = y # a s s i g n imag p a r t o f complex
8d e f a d d t ( z1 , z2 ) : # adds z1 + z2
r e t u r n Complex ( z1 . r e + z2 . re , z1 . im + z2 . im )
10
d e f s u b t ( z1 , z2 ) : # s u b t r a c t s z1−z2
12r e t u r n Complex ( z1 . r e − z2 . re , z1 . im− z2 . im )
14d e f mul t ( z1 , z2 ) : # m u l t i p l i e s z1∗z2
r e t u r n Complex ( z1 . r e∗z2 . r e −z1 . im∗z2 . im ,
16z1 . r e∗z2 . im+z1 . im∗z2 . r e )
18d e f s t r ( z ) : # c o n v e r t z t o s t r i n g f o r p r i n t i n g
r e t u r n ’(%f, %f) ’ %(z . re , z . im )
20
p r i n t ’Operations with two complex numbers\n’
22
z1 =Complex ( 2 . 0 , 3 . 0 ) # t h e f i r s t complex number
24p r i n t ’z1 =’ , z1
z2= Complex ( 4 . 0 , 6 . 0 ) # o t h e r complex one
26p r i n t "z2 =" , z2
z3=Complex . a d d t ( z1 , z2 ) # add o p e r a t i o n
28p r i n t "z1 + z2= " , z3
p r i n t ’z1 - z2=’ , Complex . s u b t ( z1 , z2 ) # p r i n t s bo th r e and im
30p r i n t ’z1 * z2=’ , Complex . mul t ( z1 , z2 ) # m u l t i p l i c a t i o n
p r i n t "Press a character to finish"
32s= r a w i n p u t ( )
4.4.2 Implementation in Python
In the program ComplexDummy.py we define and test the class Complex that adds, subtracts and
multiplies complex numbers (division is left as an exercise).
1. Enter the program ComplexDummy.py by hand, trying to understand it as best you are
able. (Yes, we know that you can just copy it, but then you do not become familiar with
the constructs.)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
70 CHAPTER 4
2. Compile and execute this program and check that the output agrees with the results you
obtain via a hand calculation. The results we obtained were: 
O p e r a t i o n s wi th two complex numbers
z1 = ( 2 . 0 0 0 0 0 0 , 3 . 0 0 0 0 0 0 )
z2 = ( 4 . 0 0 0 0 0 0 , 6 . 0 0 0 0 0 0 )
z1 + z2= ( 6 . 0 0 0 0 0 0 , 9 . 0 0 0 0 0 0 )
z1 − z2= (−2.000000 , −3.000000)
z1 ∗ z2= (−10.000000 , 2 4 . 0 0 0 0 0 0 )
Observe the following in ComplexDummy.py:
• The class Complex is declared on line 3 with the statement
class Complex
• The class methods are defined with the def statement.
• The first method in the program has the funny-looking name init . The double under-
scores on each side of a method indicate a reserved name in Python, in this case, for the
class constructor. Here init is special because it constructs the initial object that will
be given the same name as the class. Yes, this means that even though this methods’s
name is init , it produces a Complex object because Complex is the name of the class.
• The word Complex is a number of things in this program. It is the name of the class on line
3, as well as the name of the method called to create complex objects on lines 6 and 10
(we remind the reader that calling Complex really calls init ). Although neophytes may
view this multiple and indirect uses of the name Complex as confusing, more experienced
users may view it as elegant and efficient.
• The constructor has three arguments, z, x and y. By convention, the first argument of a
method always refers to the object on which the method acts. Since in this case the first
argument is z, it represents the Complex object and so can be used as a Complex object
without being further assigned.
• The second and third arguments of the constructor init are x and y, and represent
the real and the imaginary parts of the complex object to be constructed. Since only the
first argument is special and assumed to be a complex number, x and y are just ordinary
doubles.
• The actual object returned by the constructor is a tuple containing two doubles (real and
imaginary parts). A tuple is like a list, but is enclosed in round brackets (...) rather than
in the square brackets [...] of a list. Operationally, a tuple differs from a list in being
immutable (elements can’t be changed) and this makes them safer and more efficient to
use.
• The dot operator is used to refer to the attributes of an object (as in Java). In this case we
extract the re and im parts of a complex object much like we would extract the components
of a physical vector, that is, by taking dot products of it with unit vectors along each axis:
z1.re real part of object z1 z1.im imaginary part of object z1
z2.re real part of object z2 z2.im imaginary part of object z2
The dot notation is also used to apply methods to objects, as we shall see.
• The constructor init is seen to use an intermediary or dummy variable z to construct
the complex object. It sets self equal to z and returns self as the complex object. A
more elegant method will follow soon.
• The variable self is always used to name the object associated with the class (also true in
Java), in this case Complex.
• Our version of complex arithmetic uses the methods addt, subt and mult, each of which
returns a Complex object consisting of a tuple of real and imaginary parts. (Names like
“add” and “sum” might seem more natural, yet they are have already been reserved.)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 71
• Notice that ComplexDummy.py also contains the method str , with the underscores again
indicating a method native to Python. We use it here to redefine for this class the built-in
Python method for converting an argument into a string. Specifically, when called by the
print command it permits both parts of a complex number to be printed in one fell swoop.
• As is standard in Python, indentation is used to define program structure.
• There is no explicit main method in this program where execution begins, but instead it
just begins on line 22 with the first statement that is not contained in a method.
Let us now stock of what we have up to this point. On line 4 we declare that the variables
z.re and z.im will be the two separate parts of the created object. As each instance of each
object created will have different values for the object’s parts, these variables are referred to as
instance variables. Because the name of the class and the names of the objects it creates are
all the same, it is sometimes useful to use yet another word to distinguish one from the other.
Hence the phrase instance of a class is used to refer to the created objects (in our example, z1,
z2, and z3). This distinguishes them from the abstract data type (Complex). Look now at the
part of the program that calls the methods and the statement:
z1 = Complex(2.0, 3.0) 24
Although the class file does not contain a method Complex, because this is the name of the
class, Python substitutes the default constructor init for Complex and creates the object z1
with two parts to it. Likewise for object z2.
Exercise: Change line 24 so that z1 has real and imaginary parts of 0. Then, set the real and
imaginary parts to 2.0 and 3.0 explicitly with statements of the form z1.re = 2.0, and check
that you get the same result as before.
Once our complex-number objects have been created, it is easy to do arithmetic with them.
We know the rules of complex arithmetic and complex trigonometry and so can write Python
methods to implement them. It makes sense to place these methods in the same class file that
defines the data type since these associated methods are needed to manipulate objects of that
data type. In contrast to z1 and z2, the complex object z3 is not created with the constructor
directly, but instead is created by being set equal to the sum of two objects:
z3 = Complex.addt(z1, z2) 30
This says to add the complex number z1 to the complex number z2, and then store the result
“as” (in the memory location reserved for) the complex number z3.
Lines 9–10 contain the method addt. As indicated before, the dot operator convention
means that z1.re will contain the re part of z1 and that z1.im will contain the imaginary part.
Thus the operation on line 10 extracts the real parts of z1 and z2, adds them together, and then
does the same for the imaginary parts. We also notice on line 10 that the method addt returns
an object of the type Complex with two parts, and so z3 becomes a Complex object. Also notice
on that line how the return statement does not directly return a Complex object, such as z1, but
rather seems to return a call to the method Complex; the method call returns a Complex object
that then gets returned via the method addt.
Notice how the methods defined in this class have the class name preceding them and
separated from the class name by a dot (for example, Complex.addt). One way of understand-
ing line 30 above is that we are performing the operation addt on the object Complex, with z1
and z2 being parameter objects (arguments for the methods). The general form is
<object>.<method>(<arg1>, <arg2>, ...)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
72 CHAPTER 4
where it is not necessary for a method to take arguments. Another way of viewing this statement
is that Complex is the name of the class in which the method is to be found. Go ahead, try
running the program without the Complex. in front of a method name and see if Python will
accept it!
4.4.3 Without Dummies and z’s
The program ComplexSelf.py in Listing 4.2 also adds and multiplies complex numbers as
objects. However, it avoids the use of the dummy variable z in the constructor and z1 and
z2 in the other methods by, instead, using the special names self and other as variables. The
reserved word self refers to the object associated with the class, which in this case is Complex,
while the reserved word other refers to another object of the same class, but different from
self. This is similar to Java. Other than eliminating one line in the addt method, the program
is the same as before but with standard names in all the methods.
Listing 4.2 ComplexSelf.py defines the class Complex using the special names self and other. 
# ComplexSel f . py : c r e a t e s Complex c l a s s u s i n g s e l f & o t h e r , n o t dummy and z’s
2
class Complex:
4
def __init__(self, x, y): # class constructor
6self.re = x # assign real part
self.im = y # assign imag part
8
def addt(self, other): # adds self to other
10return Complex(self.re + other.re, self.im + other.im)
12def subt(self, other): # subtract self-other
return Complex(self.re - other.re, self.im- other.im)
14
def mult(self, other): # multiplies self * other
16return Complex(self.re*other.re -self.im*other.im,
self.re*other.im+self.im*other.re)
18def __str__ (self): # convert z to string for print
return ’(%f , %f ) ’ %(self.re, self.im)
20
print ’O p e r a t i o n s wi th two complex numbers\n’
22
z1 =Complex(2.0, 3.0) # first complex number
24print ’z1 =’, z1
z2= Complex(4.0, 6.0) # other complex one
26print "z2 =",z2
z3=Complex.addt(z1,z2) # add z1 + z2
28print "z1 + z2= ",z3
print ’z1 − z2=’, Complex.subt(z1,z2)
30print ’z1 ∗ z2=’, Complex.mult(z1,z2)
print "Press a character to finish"
32s=raw_input()
Exercise
1. Enter the ComplexSelf.py class file by hand, trying to understand it in the process. If
you have entered ComplexDummy.py by hand, you may modify that program to save some
time (but be careful!).
2. Compile and execute ComplexSelf.py and check that the output agrees with the results
you obtained previously.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 73
4.4.4 The Elegance of Overload
A more elegant way of writing our class for complex arithmetic is to use a technique known as
operator overloading in which we extend Python’s built-in definitions of addition, subtraction
and multiplication so that they also apply to complex numbers. In this way we can use our
usual symbols and let Python decide which method to use based on the arguments given to the
methods. Some of the special methods defined in Python for the overloading are:
Symbol Method Symbol Method
+ add - sub
* mul / div
with more methods defined for overloading other symbols.
Listing 4.3 ComplexOverLoad.py defines a Complex class that overloads the +, - and / operators. 
# ComplexOverload . py : c l a s s Complex p e r f o r m s complex a r i t h m e t i c v i a o p e r a t o r o v e r l o a d
2
c l a s s Complex1 :
4
"Another class to add subtract and multiply complex numbers"
6"Uses overload of operators + - and * "
8d e f i n i t ( s e l f , x , y ) : # c l a s s c o n s t r u c t o r
s e l f . r e =x # a s s i n g r e a l p a r t
10s e l f . im=y # a s s i n g imag p a r t
12d e f a d d ( s e l f , o t h e r ) : # e x t e n d p r e d e f i n e d add
r e t u r n Complex1 ( s e l f . r e + o t h e r . re , s e l f . im + o t h e r . im )
14
d e f s u b ( s e l f , o t h e r ) : # e x t e n d p r e d e f i n e d s u b t r a c t
16r e t u r n Complex1 ( s e l f . r e − o t h e r . re , s e l f . im − o t h e r . im )
18d e f m u l ( s e l f , o t h e r ) : # e x t e n d p r e d e f i n e d mul t
r e t u r n Complex1 ( s e l f . r e∗ o t h e r . r e −s e l f . im∗ o t h e r . im ,
20s e l f . r e∗ o t h e r . im + s e l f . im∗ o t h e r . r e )
22d e f s t r ( s e l f ) :
r e t u r n ’(%f , %f) ’ %( s e l f . re , s e l f . im )
24
p r i n t ’\n Operations with two complex numbers via operator overload\n’
26z1 =Complex1 ( 2 . 0 , 3 . 0 ) # f i r s t complex number
p r i n t ’z1=’ , z1
28z2= Complex1 ( 4 . 0 , 6 . 0 ) # o t h e r complex number
p r i n t "z2=" , z2 , "\n"
30p r i n t "z1+z2=" , z1+z2 # use a d d f o r +
p r i n t "z1*z2=" , z1∗z2 # use m u l f o r ∗
32p r i n t ’z1-z2=’ , z1−z2 # use s u b f o r −
p r i n t ’z1*z2=’ , z1∗z2
34p r i n t "Press a character to finish"
s= r a w i n p u t ( )
The program ComplexOverLoad.py in Listing 4.3 does the same calculation as the previous
programs but with overloading. Its executable part is seen to use the Complex constructor to
create the complex numbers, but other than that, the arithmetic operations look like regular
arithmetic operations. However, the definitions of add , sub and mul for complex num-
bers in the top part of the program are new, and will be used only when acting on Complex
objects. Otherwise, the regular definitions are used.
4.4.5 Python OOP Differs from Java and C++*
(If you are not a Java or C++ programmer, it makes sense to skip this section.)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
74 CHAPTER 4
1. Python classes are by default public and so do not have to be declared as such.
2. There are no static methods for objects in Python, that is, they are all dynamic, active or
nonstatic.
3. For advanced programming that integrates Python with C, there are procedures to deal
with C’s static methods in Python.
4. Python does not require the user to declare a variable’s type, for example, as we might in
Java with public static double x;.
5. Python does not require the name of a file to match the name of the class it contains, and
so we have the class complex in the file named ComplexDummy.py.
4.5 PYTHON’S BUILT-IN COMPLEX NUMBER TYPE
Finally, we should indicate that in addition to built-in numeric data types of integers, long in-
tegers and floating point numbers (doubles), Python also has built-in complex objects, which
are composed of doubles. Similar to what we have created here, Python’s complex numbers
are declared as complex(real, imag), with z.real and z.imag the real and imaginary parts
respectively, and with the parentheses mandatory. Note that the built-in type name uses low-
ercase complex and z.real and z.imag instead of our z.re and z.im. The first argument in
the complex declaration can also be a complex number represented via a string, in which case
the second argument should not be given. To see how this is done, from within an interactive
Python shell we have:
>>> z1 = complex(2.0, 3.0) Define complex z1
>>> z2 = complex(4.0, 6.0) Define complex z2
>>> print z1, z2
(2+3j) (4+6j) The returned output
Note here that the suffix of j or J is used for the imaginary number (normally called i in math
and physics, but j in engineering). For this to work, there cannot be any spaces before the j,
and the pure imaginary number i needs to have a 1 before the j, that is 1j. This also means
that we can write complex numbers as (real + imagj) and even use that form with methods:
>>> z1 = 2.0 + 3.0j Define complex z1
>>> z2 = 4.0 + 6.0J Define complex z2
>>> print z1+z2
>>> print z1 + z2
(6+9j)
>>> abs(1+1j)
1.4142135623730951 The answer
>>> z1.conjugate() Note object.method
(2-3j) The answer
4.6 COMPLEX CURRENTS (SOLUTION)
1. Extend the class Complex by adding new methods to take the modulus, take the complex
conjugate, and determine the phase of complex numbers.
2. Test your methods by checking that the following identities hold for a variety of complex
numbers:
z + z = 2z, z + z∗ = 2 Re z
z − z = 0, z − z∗ = 2 Im z (4.17)
zz∗ = |z|2, zz∗ = r2 (which is real)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 75
Figure 4.3 Left: A plot of 1/|Z| versus resistance R and frequency ω showing that the magnitude of the current has
a maximum at ω = 1. Right: A plot of the current’s phase versus resistance R and frequency ω showing
that below resonance, ω < 1, the current lags the voltage, while above resonance the current leads the
voltage.
0
400
800
2
R
1 /  Z
1
0
400
R
800
2
0.5
–1
1
Hint: Compare your output to some cases of pure real, pure imaginary, and simple
complex numbers that you are able to evaluate by hand.
3. Equation (4.14) gives the magnitude and phase of the current in a single RLC circuit.
Modify the given complex arithmetic program so that it performs the required complex
arithmetic.
4. Compute and then make a plot of the magnitude and phase of the current in the circuit as
a function of frequency 0 ≤ ω ≤ 2.
5. Construct a z(x, y) surface plot of the magnitude and phase of the current as functions
of both the frequency of the external voltage ω and of the resistance R. Observe how
the magnitude has a maximum when the external frequency ω = 1/
√
LC . This is the
resonance frequency.
6. Another approach is to make a 3-D visualization of the complex Z as a function of a
complex argument (Figure 4.3). Do this by treating the frequency ω = x + iy as a
complex number. You should find a sharp peak at x = Re(ω) = 1. Adjust the plot so
that you tell where Im(1/Z) changes sign. If you look closely at the graph, you should
also see that there is a maximum for a negative imaginary value of ω. This is related to
the length of the lifetime of the resonance.
7. Assessment: You should notice a resonance peak in the magnitude at the same frequency
for which the phase vanishes. The smaller the resistance R, the more sharply the circuit
should pass through resonance. These types of circuits were used in the early days of
radio to tune to a specific frequency. The sharper the peak, the better the quality of
reception.
8. The second part of the problem dealing with the two circuits in parallel is very similar to
the first part. You need to change only the value of the impedance Z used. To do that,
explicitly perform the complex arithmetic implied by (4.16), deduce a new value for the
impedance, and then repeat the calculation of the current.
4.7 OOP WORKED EXAMPLES
Creating object-oriented programs requires a transition from a procedural programming mind-
set, in which functions take arguments as input and produce answers as output, to one in which
objects are created, probed, transferred, and modified. To assist you in the transition, we present
here two sample procedural programs and their OOP counterparts. In both cases the OOP ex-
amples are longer but presumably easier to modify and extend.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
76 CHAPTER 4
Figure 4.4 Superposition of two waves with similar wave numbers.
Superposition of Two Sine Waves
4.7.1 OOP Beats
You obtain beats if you add together two sine functions y1 and y2 with nearly identical frequen-
cies,
y3(t) = y1(t) + y2(t) = A sin(30 t) +A sin(33 t). (4.18)
Beats look like a single sine wave with a slowly varying amplitude (Figure 4.4). In Listing 4.4
we give Beats.py, a simple program that plots beats. You see here that all the computation
is done in the main program, with the only method called is that for plotting. The two sine
functions are added together on line 15, which is within the for loop that runs over time. Con-
trast this with the object-oriented program OOPBeats.py in Listing 4.5 that produces the same
graph.
Listing 4.4 Beats.py plots beats using procedural programming. Contrast this with the object-oriented program
OOPBeats.py in Listing 4.5. 
# B e a t s : p l o t s s i n (30∗x ) + s i n (33∗x ) , 0<=x <=5.0
2
from v i s u a l . g raph i m p o r t ∗ # g r a p h i c s and math c l a s s e s
4# P l o t s e t u p ( i n i t x , window width , h e i g h t , t i t l e , axe s names , f o n t & window c o l o r ) i n i t i a l
p o s i t i o n , g r a p h i c s window wid th and h e i g h t ,
g raph = g d i s p l a y ( x = 0 , y = 0 , wid th = 500 , h e i g h t = 300 , # s e t up p l o t
6t i t l e = ’Beats: f(x)=sin(30*x)+sin(33*x)’ ,
x t i t l e = ’x’ , y t i t l e = ’f(x)’ , xmax = 5 . 0 , xmin = 0 . 0 , ymax = 2 ,
8ymin=−2, f o r e g r o u n d = c o l o r . b l a c k , background = c o l o r . w h i t e )
f u n c t i o n = gc u r v e ( c o l o r = c o l o r . r e d )
10f o r x i n a r a n g e ( 0 . , 5 . 0 , 0 . 0 1 ) : # min , max , s t e p
r a t e ( 4 0 )
12y = math . s i n (30∗x ) + math . s i n (33∗x ) # f u n c t i o n t o p l o t
f u n c t i o n . p l o t ( pos =( x , y ) ) # p l o t f u n c t i o n
In the OOP version, the main program is the two lines at the end. It is short because all it does is
create an OOPbeats object named sumsines, and then sums the two waves by having sumwaves
modify the object. The constructor for an OOPbeats object is given on line 6, followed by the
sumwaves method. The sumwaves method takes the arguments needed for plotting and does the
plotting.
Listing 4.5 OOPBeats.py plots beats using OOP, in contrast to the procedural program Beats.py in Listing 4.4. 
# OOPbeats . py : OOP s u p e r p o s i t i o n o f two s i n e waves
2
from v i s u a l . g raph i m p o r t ∗ # g r a p h i c s and math c l a s s e s
4
c l a s s OOPbeats :
6
d e f i n i t ( s e l f , Ampl , f r e q 1 , f r e q 2 ) : # c l a s s c o n s t r u c t o r
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 77
8s e l f .A = Ampl # Ampl i tude
s e l f . k1 = f r e q 1 # f r e q u e n c i e s
10s e l f . k2 = f r e q 2
12d e f sumwaves ( s e l f , m y t i t l e , m y x t i t l e , m y y t i t l e , xma , xmi , yma , ymi ) :
g raph = g d i s p l a y ( x = 0 , y = 0 , wid th = 500 , h e i g h t = 300 ,
14t i t l e = m y t i t l e , x t i t l e = m y x t i t l e , y t i t l e = m y y t i t l e , xmax = xma ,
xmin = xmi , ymax = yma , ymin = ymi , f o r e g r o u n d = c o l o r . b l ack ,
16background = c o l o r . w h i t e )
f u n c t i o n = gc u r v e ( c o l o r = c o l o r . r e d )
18f o r x i n a r a n g e ( 0 . , + 5 . 0 , 0 . 0 1 ) : # min , max , s t e p
r a t e ( 1 0 )
20y = s e l f .A∗math . s i n ( s e l f . k1∗x ) + s e l f .A∗math . s i n ( s e l f . k2∗x ) # p l o t f u n c t i o n
f u n c t i o n . p l o t ( pos =( x , y ) )
22b e a t s =OOPbeats ( 1 . 0 , 3 0 . 0 , 3 3 . 0 ) # i n s t a n c e o f c l a s s
b e a t s . sumwaves (’Superposition of 2 sine waves’ ,’x’ ,’f(x)’ ,5 ,0 ,2 ,−2)
4.7.2 OOP Planet
In our second example we add together two periodic functions representing positions versus
time. One set describes the position of the moon as it revolves around a planet, and the other
the position of the planet as it revolves about the sun. The position of the planet at time t
relative to the sun is described by
xp = R cos(ωp t), yp = R sin(ωp t). (4.19)
The position of the satellite, relative to the sun, is given by the sum of its position relative to
the planet and the position of the planet relative to the sun:
xmllxs =xp + r cos(ωs t) = R cos(ωp t) + r cos(ωs t),
ys = yp + r sin(ωs t) = R sin(ωp t) + r sin(ωs t).
If ωs ' ωp this looks like beating, yet we will make a parametric plot of x(t) versus y(t) to
visualize the orbit. The procedural program Moon.py in Listing 4.6 produces Figure 4.5.
Listing 4.6 The procedural program Moon.py computes trajectory of a satellite seen from a planet. 
#Moon . py : moon o r b i t i n g a p l a n e t
from v i s u a l . g raph i m p o r t ∗ # g r a p h i c s and math c l a s s e s
g raph = g d i s p l a y ( x = 0 , y = 0 , wid th = 500 , h e i g h t = 500 ,
t i t l e = ’Motion of a satellite around a planet’ ,
x t i t l e = ’x’ , y t i t l e = ’y’ , xmax = 5 . 0 , xmin = − 5 . 0 , ymax = 5 , ymin = − 5 ,
f o r e g r o u n d = c o l o r . b l ack , background = c o l o r . w h i t e )
moonfun = g cu rv e ( c o l o r = c o l o r . r e d ) # f o r a 2D graph , c u r v e i n r e d
Rad ius = 4 . 0 # P l a n e t O r b i t r a d i u s
w p l a n e t = 2 . 0 # p l a n e t a n g u l a r v e l o c i t y
r a d i u s = 1 . 0 # moon O r b i t r a d i u s a round p l a n e t
wmoon = 1 4 . 0 # moon ang . v e l . a round p l a n e t
f o r t ime i n a r a n g e ( 0 . , 3 . 2 , 0 . 0 2 ) : # t ime min , max , s t e p
r a t e ( 2 0 )
x = Rad ius∗math . cos ( w p l a n e t∗ t ime ) + r a d i u s∗math . cos ( wmoon∗ t ime ) # moon x
y = Radius∗math . s i n ( w p l a n e t∗ t ime ) + r a d i u s∗math . s i n ( wmoon∗ t ime ) # moon y
moonfun . p l o t ( pos = ( x , y ) ) # p l o t s moon p o s i t i o n
Exercise: Rewrite the program using OOP:
1. Define a mother class OOPPlanet containing:
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
78 CHAPTER 4
Figure 4.5 The trajectory of a satellite as seen from a planet.
Radius Planet’s orbit radius
wplanet Planet’s orbit ω
(xp, yp) Planet’s coordinates
getX(double t), getY(double t) Planet coordinates methods
trajectory() Method for planet’s orbit
2. Define a daughter class OOPMoon containing:
radius Radius of moon’s orbit
wmoon Frequency of moon in orbit
(xm, ym) Moon’s coordinates
trajectory() Method for moon’s orbit relative to sun
3. The main program must contain one instance of the class planet and another instance
of the class Moon, that is, one planet object and one Moon object.
4. Have each instance call its own trajectory method to plot the appropriate orbit. For the
planet this should be a circle, while for the moon it should be a circle with retrogrades
(Figure 4.5).
One solution, which produces the same results as the previous program, is the program
OOPPlanet.py in Listing 4.7. As with OOPbeats.py, the main program for OOPPlanet.py
is at the end and is short; it creates an OOPMoon object and then plots the moon’s orbit by
applying the trajectory method to the object. The class contains:
1. A constructor init used to initialize the values of the planet radius and its angular
velocity.
2. Methods getX and getY that obtain the position of the planet at a given time from a planet
object. Methods such as these that extract information from objects are called accessors.
3. A method scenario that prepares a window in which a planet’s or moon’s orbit (or both)
will be plotted.
4. A method called position that plots the planet’s orbit around the sun.
Listing 4.7 OOPPlanet.py creates an OOPMoon object and then plots the moon’s orbit by applying the trajectory
method to the object. 
# OOPPlanet . py : P l a n e t o r b i t i n g Sun , o r moon o r b i t i n g p l a n e t
2
from v i s u a l . g raph i m p o r t ∗ # g r a p h i c s and math c l a s s e s
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 79
4
c l a s s OOPPlanet :
6d e f i n i t ( s e l f , Rad , pomg ) : # P l a n e t c l a s s c o n s t r u c t o r
s e l f . Rad ius = Rad # P l a n e t Rad ius
8s e l f . w p l a n e t = pomg # P l a n e t a n g u l a r v e l o c i t y
10d e f getX ( s e l f , t ime ) :
r e t u r n s e l f . Rad ius∗math . cos ( s e l f . w p l a n e t∗ t ime )
12
d e f getY ( s e l f , t ime ) :
14r e t u r n s e l f . Rad ius∗math . s i n ( s e l f . w p l a n e t∗ t ime )
# D e f i n e s window parmas , can be used by p l a n e t o r moon
16
d e f s c e n a r i o ( s e l f , m y t i t l e , m y x t i t l e , m y y t i t l e , xma , xmi , yma , ymi ) : # S e t window
18graph = g d i s p l a y ( x = 0 , y = 0 , wid th = 500 , h e i g h t = 500 ,# s e t s a s p e c t r a t i o
t i t l e = m y t i t l e , x t i t l e = m y x t i t l e , y t i t l e = m y y t i t l e , xmax = xma ,
20xmin = xmi , ymax = yma , ymin = ymi , f o r e g r o u n d = c o l o r . b l ack ,
background = c o l o r . w h i t e )
22
d e f p o s i t i o n ( s e l f ) : # f o r p l a n e o r b i t
24p l a n e t f u n = g cu r ve ( c o l o r = c o l o r . r e d )
f o r t ime i n a r a n g e ( 0 . , + 3 . 3 , 0 . 0 2 ) : # min , max , s t e p
26xp = s e l f . getX ( t ime )
yp = s e l f . getY ( t ime )
28p l a n e t f u n . p l o t ( pos = ( xp , yp ) ) # p l o t f u n c t i o n
30c l a s s OOPMoon( OOPPlanet ) : # OOPMoon i s s u b c l a s s o f OOPPlanet
32d e f i n i t ( s e l f , Rad , pomg , rad , momg) : # c l a s s c o n s t r u c t o r p l a n e t&Moon params
34OOPPlanet . i n i t ( s e l f , Rad , pomg )
s e l f . r a d i u s = r a d # Moon r a d i u s
36s e l f . wmoon = momg # Moon a n g u l a r v e l o c i t y
38d e f p o s i t i o n ( s e l f ) : # moon o r b i t a round p l a n e t
moonfun = g cu rv e ( c o l o r = c o l o r . b l u e ) # t r a c e o r b i t i n b l u e
40f o r t ime i n a r a n g e ( 0 . , + 3 . 3 , 0 . 0 2 ) : # min , max , s t e p
xm = s e l f . getX ( t ime ) + s e l f . r a d i u s∗math . cos ( s e l f . wmoon∗ t ime ) # x moon
42ym = s e l f . getY ( t ime ) + s e l f . r a d i u s∗math . s i n ( s e l f . wmoon∗ t ime ) # y moon
moonfun . p l o t ( pos = (xm , ym) ) # p l o t s moon’s position
44rate(10)
moon = OOPMoon(4.0, 2.0, 1.0, 14.0) # init planet & moon
46moon.scenario(’ S a t e l i t e o r b i t a round p l a n e t’, ’x’, ’y’, 5, - 5, 5, - 5) # title
moon.position() # overrrides planet position
48planet = OOPPlanet(4.0, 2.0) # initializes planet
planet.position()
50#uncomment next two lines for window with the motion of the planet
# planet.scenario(’P l a n e t o r b i t a round Sun’, ’x’, ’y’, 5, - 5, 5, - 5)
52# planet.position()
4.8 SUBCLASSES AND CLASS INHERITANCE
What is new about the program in Listing 4.7 is that it contains two classes, OOPPlanet on line
6 and OOPMoon on line 30, with OOPMoon within OOPPlanet. That being the case, OOPMoon is
a subclass or daughter class of the mother class OOPPlanet. The daughter class inherits the
properties of the mother class as well as having properties of its own. Thus, on lines 40 and
41, OOPMoon uses the getX(time) and getY(time) methods from the OOPPlanet class without
having to say OOPPlanet.getX(time) to specify the class name.
Specifically, note on line 30 how the class OOPMoon is defined a as a child of the class
OOPPlanet with the statement containing OOPPlanet as an argument:
class OOPMoon(OOPPlanet): Defines subclass OOPMoon
Because OOPMoon is a subclass of OOPPlanet, it inherits the scenario, getX and getY from
OOPPlanet. But since the OOPMoon class defines its own position method, this overrides the
one in the mother class.
To plot the motion of the moon around a planet as seen from the sun, it is necessary to
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
80 CHAPTER 4
specify the radius of the planet’s orbit around the sun as well as its angular velocity. This is the
reason why the parameter list of the constructor for OOPMoon,
def init ( self, Rad, pomg, rad, momg ): 32
not only contains self, which passes the properties of OOPMoon, but also contains properties of
the planet and the angular velocity of the moon around the planet. In turn, we see that on line
33,
OOPPlanet. init (self, Rad, pomg) 33
two of these arguments are transmitted to the OOPPlanet class through its member init. Be-
cause this initialization does not actually create any instances of the two classes, it is called an
abstract initialization. As a consequence of inheritance, the methods getX and getY defined in
the OOPPlanet, are also owned by the OOPMoon and are used there to add the coordinates of the
moon and planet together:
xm=self.getX(time) + self.radius*math.cos(self.wmoon*time) # x moon 40
ym=self.getY(time) + self.radius*math.sin(self.wmoon*time) # y moon 41
4.9 UNIT II. ADVANCED OBJECTS; BATON PROJECTILES 
In this unit we look at more advanced aspects of OOP. These aspects are designed to help
make programming more efficient by making the reuse of already written components easier
and more reliable. The ideal is to permit this even for entirely different future projects for
which you will have no memory or knowledge of the internal workings of the already written
components that you want to reuse. OOP concepts can be particularly helpful in complicated
projects in which you need to add new features without “breaking” the old ones and in which
you may be modifying code that you did not write.
4.10 TRAJECTORY OF A THROWN BATON (PROBLEM)
We wish to describe the trajectory of a baton that spins as it travels through the air. On the left
in Figure 4.6 the baton is shown as as two identical spheres joined by a massless bar. Each
sphere has mass m and radius r, with the centers of the spheres separated by a distance L. The
baton is thrown with the initial velocity (Figure 4.6 center) corresponding to a rotation about
the center of the lower sphere.
Problem: Write an OOP program that computes the position and velocity of the baton as a
function of time. The program should
1. plot the position of each end of the baton as a function of time;
2. plot the translational kinetic energy, the rotational kinetic energy, and the potential en-
ergy of the baton, all as functions of time;
3. use several classes as building blocks so that you may change one building block without
affecting the rest of the program;
4. (optional) then be extended to solve for the motion of a baton with an additional lead
weight at its center.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 81
Figure 4.6 Left: The baton before it is thrown. “X” marks the COM. Center: The initial conditions for the baton
as it is thrown. Right: The baton spinning in the air under the action of gravity.
rr
L
X
a b
m ma
b
X
X



2v
v
0
0
mg
4.10.1 Combined Translation and Rotation (Theory)
Classical dynamics describes the motion of the baton as the motion of its center of mass (COM)
(marked with an “X” in Figure 4.6), plus a rotation about the COM (Figure 4.6 right). Because
the translational and rotational motions are independent, each may be determined separately,
and because we ignore air resistance, the angular velocity ω about the COM is constant.
The baton is thrown with an initial velocity (Figure 4.6 center). The simplest way to
view this is as a translation of the entire baton with a velocity V0 and a rotation of angular
velocity ω about the COM. To determine ω, we note that the tangential velocity due to rotation
is
vt =
1
2
ωL. (4.20)
For the direction of rotation as indicated in Figure 4.6, this tangential velocity is added to the
COM velocity at the top of the baton and is subtracted from the COM velocity at the bottom.
Because the total velocity equals 0 at the bottom and 2V0 at the top, we are able to solve for ω:
1
2
ωL− V0 = 0 ⇒ V0 =
1
2
ωL, ⇒ ω = 2V0
L
. (4.21)
If we ignore air resistance, the only force acing on the baton is gravity, and it acts at the COM
of the baton (Figure 4.6 right). Figure 4.7 shows a plot of the trajectory [x(t), y(t)] of the
COM:
(xcm, ycm) =
(
V0xt, V0yt−
1
2
gt2
)
, (vx,cm, vy,cm) = (V0x, V0y − gt) ,
where the horizontal and vertical components of the initial velocity are
V0x = V0 cos θ, V0y = V0 sin θ.
Even though ω = constant, it is a constant about the COM, which itself travels along a
parabolic trajectory. Consequently, the motion of the baton’s ends may appear complicated to
an observer on the ground (Figure 4.7 right). To describe the motion of the ends, we label one
end of the baton a and the other end b (Figure 4.6 left). Then, for an angular orientation φ of
the baton,
φ(t) = ωt+ φ0 = ωt, (4.22)
where we have taken the initial φ = φ0 = 0. Relative to the COM, the ends of the baton are
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
82 CHAPTER 4
Figure 4.7 Left: The trajectory (x(t), y(t)) followed by the baton’s COM. Right: The applet JParabola.java showing
the entire baton as its COM follows a parabola.
q
described by the polar coordinates
(ra, φa) =
(
L
2
, φ(t)
)
, (rb, φb) =
(
L
2
, φ(t) + π
)
. (4.23)
The ends of the baton are also described by the Cartesian coordinates
(x′a, y
′
a) =
L
2
[cosωt, sinωt] , (x′b, y
′
b) =
L
2
[cos(ωt+ π), sin(ωt+ π)] .
The baton’s ends, as seen by a stationary observer, have the vector sum of the position of the
COM plus the position relative to the COM:
xmll (xa, ya) =
[
V0xt+
L
2
cos(ωt), V0yt−
1
2
gt2 +
L
2
sin(ωt)
]
, (4.24)
(xb, yb) =
[
V0xt+
L
2
cos(ωt+ π), V0yt−
1
2
gt2 +
L
2
sin(ωt+ π)
]
.
If La and Lb are the distances of ma and mb from COM, then
La =
mb
ma +mb
, Lb =
ma
ma +mb
, ⇒ maLa = mbLb. (4.25)
The moment of inertia of the masses (ignoring the bar connecting them) is
Imasses = maL2a +mbL
2
b . (4.26)
If the bar connecting the masses is uniform with mass m and length L, then it has a moment of
inertia about its COM of
Ibar =
1
12
mL2. (4.27)
Because the COM of the bar is at the same location as the COM of the masses, the total moment
of inertia for the system is just the sum of the two:
Itot = Imasses + Ibar. (4.28)
The potential energy of the masses is
PEmasses = (ma +mb)gh = (ma +mb)g
(
V0 t sin θ −
1
2
gt2
)
, (4.29)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 83
while the potential energy of the bar just hasma+mb replaced bym since both share the same
COM location. The rotational kinetic energy of rotation is
KErot =
1
2
Iω2, (4.30)
with ω the angular velocity and I the moment of inertia for either the masses or the bar (or the
sum). The translational kinetic energy of the masses is
KEtrans =
1
2
m
[
(V0 sin θ − g t)2 + (V0 cos θ)2
]
, (4.31)
with ma +mb replaced by m for the bar’s translational KE.
To get a feel for the interestingly complicated motion of a baton, we recommend that the
reader try out the applet JParabola on the CD (Fig. 4.7 right).
% appletviewer jcenterofmass.html
4.11 OOP DESIGN CONCEPTS (CS)
In accord with our belief that much of education is just being an understanding of what the
words mean, we start by defining OOP as programming containing component objects with
four characteristics [Smi 91]:
Encapsulation: The data and the methods used to produce or access data are encap-
sulated into entities called objects. For our problem, the data are initial positions,
velocities, and properties of a baton, and the objects are the baton and its path. As part of
the OOP philosophy, data are manipulated only via distinct methods.
Abstraction: Operations applied to objects are assumed to yield standard results according
to the nature of the objects. To illustrate, summing two matrices always gives another
matrix. By incorporating abstraction into programming, we concentrate more on solving
the problem and less on the details of the implementation.
Inheritance: Objects inherit characteristics (including code) from their ancestors yet may
be different from their ancestors. A baton inherits the motion of a point particle, which in
this case describes the motion of the COM, and extends that by permitting rotations about
the COM. In addition, if we form a red baton, it inherits the characteristics of a colorless
baton but with the property of color added to it.
Polymorphism: Methods with the same name may affect different objects differently. Child
objects may have member functions with the same name but with properties differing from
those of their ancestors (analogous to method overload, where the method used depends
upon the method’s arguments).
4.12 MULTIPLE CLASSES AND MULTIPLE INHERITANCES
We now solve our baton problem using OOP techniques. Although we can also solve it with
the traditional techniques of procedural programming, this problem contains the successive
layers of complexity that make it appropriate for OOP. We will use several source (.py) files
for this problem, each yielding a different class corresponding to a different aspect of the
baton. There is a class Ball.py (Listing 4.8) representing a ball on the end of the baton,
a class Path.py (Listing 4.9) representing the trajectory of the COM, and a class Baton.py
(Listing 4.10) assembling the other classes into an object representing the baton.
Listing 4.8 The class Ball representing the ball on the end of the baton. 
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
84 CHAPTER 4
# B a l l . py : I s o l a t e d b a l l w i th Mass and Radius
2
c l a s s B a l l :
4d e f i n i t ( s e l f , mass , r a d i u s ) : # b a l l c l a s s c o n s t r u c t o r
s e l f .m = mass # i n i t i a l i z e mass
6s e l f . r = r a d i u s # i n i t i a l i z e r a d i u s
8d e f getM1 ( s e l f ) : # g e t b a l l mass
r e t u r n s e l f .m
10
d e f getR ( s e l f ) : # g e t r a d i u s
12r e t u r n s e l f . r # g e t moment o f i n e r t i a
14d e f g e t I 1 ( s e l f ) :
r e t u r n ( 2 . 0 / 5 . 0 ) ∗ s e l f .m∗( s e l f . r )∗∗2
The Ball class (Listing 4.8) creates a ball object of mass m, radius r, and moment of
inertia I . It contains the three dynamic and short methods, getM, getR, and getI for extracting
the mass, radius, and moment of inertia of a ball. Inasmuch as we use Ball as a building
block, it is a good idea to keep the methods simple, and just add more methods to create more
complicated objects. Take stock of how similar Ball is to the Complex class. In the present
case, m and r behave like the re and im in the Complex class in that they too act by being
attached to the end of an object’s name.
Dynamic methods are like dynamic variables in that they behave differently depending
on the object they modify. To cite an instance, ball1.getR() and ball2.getR() return differ-
ent values if the balls have different radii. The methods getM and getR are template methods;
that is, they do not compute anything now but are included to facilitate future extensions. To
name an instance, if the Ball class becomes more complex, you may need to sum the masses
of its constituent parts in order to return the ball’s total mass. With the template in place, you
do that without having to reacquaint yourself with the rest of the code first. This testing code
creates a Ball object and prints out its properties by affixing get methods to the object.
Exercises: Compile and run the modified Ball.py to ensure that the Ball class still works
properly. Test the class Path. Create a Path object and find its properties at several different
times. Because this a test code, it does not need to do much; being able to make the object with
expected properties is enough.
Listing 4.9 The class Path creating an object that represents the trajectory of the center of mass. 
# Pa th . py : P a r a b o l i c T r a j e c t o r y o f COM
2
i m p o r t math
4c l a s s Pa th : # Pa th o f COM mass
6d e f i n i t ( s e l f , v0 , t h e t a ) : # c o n s t r u c t o r f o r p a r a b o l a
s e l f . g = 9 . 8 # g r a v i t y
8s e l f . v0 = v0 # i n i t i a l speed
s e l f . t h e t a = t h e t a # i n i t i a l a n g l e
10s e l f . v0x = s e l f . v0∗math . cos ( s e l f . t h e t a ∗math . p i / 1 8 0 . 0 ) # i n i t i a l Vx
s e l f . v0y = s e l f . v0∗math . s i n ( s e l f . t h e t a ∗math . p i / 1 8 0 . 0 ) # i n i t i a l Vy
12
d e f getX ( s e l f , t ) : # g e t Xcom
14s e l f . t = t
r e t u r n s e l f . v0x∗ s e l f . t
16
d e f getY ( s e l f , t ) : # g e t Ycom
18s e l f . t = t
r e t u r n s e l f . v0y∗ s e l f . t − 0 .5∗ s e l f . g∗ t ∗∗2
The class Path (Listing 4.9) creates an object that represents the trajectory [(x(t), y(t)]
of the COM. This class computes the initial velocity components V0x and V0y, and is another
building block that we will use to construct the baton’s trajectory. The acceleration due to
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 85
gravity g, which is the same for all objects, is associated with Path. The constructor method for
the class Path takes the polar coordinates (V0, θ) as arguments and computes the components
of the initial velocity, (v0x, v0y).
Listing 4.10 Baton.py combines ball and path classes to form a baton. 
# Baton . py : Combine B a l l and Pa th c l a s s e s t o form Baton
2
from v i s u a l . g raph i m p o r t ∗ # g r a p h i c s and math c l a s s e s
4i m p o r t Bal l , Pa th # i m p o r t o t h e r c l a s s e s
6c l a s s Baton ( B a l l . Ba l l , Pa th . Pa th ) : # Baton i n h e r i t s B a l l and Pa th p r o p s
d e f i n i t ( s e l f , mass , r a d i u s , v0 , t h e t a , L1 , w1 ) : # c o n s t r u c t Baton
8B a l l . B a l l . i n i t ( s e l f , mass , r a d i u s ) # c o n s t r u c t B a l l
Pa th . Pa th . i n i t ( s e l f , v0 , t h e t a ) # c o n s t r u c t Pa th
10s e l f . L = L1 # Baton l e n g t h
s e l f .w = w1 # Baton ang v e l o c i t y
12
d e f getM ( s e l f ) :
14r e t u r n 2 .0∗ s e l f . getM1 ( )
16d e f g e t I ( s e l f ) :
r e t u r n (2∗ s e l f . g e t I 1 ( ) + 0 .5∗ s e l f . getM ( ) ∗ s e l f . L∗∗2)
18
d e f getXa ( s e l f , t ) :
20xa = s e l f . getX ( t ) + 0 .5∗ s e l f . L∗math . cos ( s e l f .w∗ t )
r e t u r n xa
22
d e f getYa ( s e l f , t ) :
24r e t u r n s e l f . getY ( t ) + 0 .5∗ s e l f . L∗math . s i n ( s e l f .w∗ t )
26d e f getXb ( s e l f , t ) :
r e t u r n s e l f . getX ( t ) − 0 .5∗ s e l f . L∗math . cos ( s e l f .w∗ t )
28
d e f getYb ( s e l f , t ) :
30r e t u r n s e l f . getY ( t ) − 0 .5∗ s e l f . L∗math . s i n ( s e l f .w∗ t )
32d e f s c e n a r i o ( s e l f , m y t i t l e , m y x t i t l e , m y y t i t l e , xma , xmi , yma , ymi ) :
g raph = g d i s p l a y ( x = 0 , y = 0 , wid th = 500 , h e i g h t = 500 ,
34t i t l e = m y t i t l e , x t i t l e = m y x t i t l e , y t i t l e = m y y t i t l e , xmax = xma ,
xmin = xmi , ymax = yma , ymin = ymi , f o r e g r o u n d = c o l o r . b l ack ,
36background = c o l o r . w h i t e )
d e f p o s i t i o n ( s e l f ) :
38ba tonmassa = g c u r ve ( c o l o r = c o l o r . b l u e ) # b l u e t r a j e c t o r y a
ba tonmassb = gc u r ve ( c o l o r = c o l o r . r e d ) # r e d t r a j e c t o r y b
40batoncm = g cu rv e ( c o l o r = c o l o r . magenta )
42t = 0 . 0 # s t a r t mot ion a t t ime 0
c o u n t = 4
44yy = s e l f . getYa ( t ) # i n i t i a l y a
w h i l e ( s e l f . getYa ( t )>= 0 . 0 ) : # do t i l l y Yb <0
46xa = s e l f . getXa ( t ) # Xa
ya = s e l f . getYa ( t ) # Yb
48# p l o t a
xb = s e l f . getXb ( t )
50yb = s e l f . getYb ( t )
p o i n t s = [ ( xa , ya ) , ( xb , yb ) ]
52g cu rv e ( c o l o r = ( 0 . 8 , 0 . 8 , 0 . 8 ) , pos= p o i n t s )
ba tonmassa . p l o t ( pos = ( xa , ya ) )
54ba tonmassb . p l o t ( pos = ( xb , yb ) ) # p l o t s b
56# i f c o u n t%4 = = 0 : # p l o t s b a t o n i f uncommented 2 n e x t two l i n e s
# gc u r v e ( pos = [ ( xa , ya ) , ( xb , yb ) ] , c o l o r = c o l o r . cyan )
58xcm = s e l f . getX ( t ) # Xcom
ycm = s e l f . getY ( t ) # Ycom
60r a t e ( 5 )
batoncm . p l o t ( pos = ( xcm , ycm ) ) # p l o t COM
62t += 0 . 0 2 # i n c r e m e n t t ime
c o u n t += 1
64
mybaton = Baton ( 0 . 5 , 0 . 4 , 1 5 . 0 , 3 4 . 0 , 2 . 5 , 1 5 . 0 ) # mass r a d i u s v0 t h e t a L w
66# n e x t t i t l e and geomet ry f o r t h e g raph
mybaton . s c e n a r i o (’Positions of mass a(blue), b(red) and center of mass (magenta)’ ,
68’x’ , ’y’ , 20 , 0 , 5 , − 1) # xmx = 20 , xmin = 0
mybaton . p o s i t i o n ( ) # ymax = 10 , ymin = − 1
Now that we have created the building-block classes, we combine them to create the baton’s
trajectory in Baton.py (Listing 4.10). The class Baton inherits the properties of mass, radius
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
86 CHAPTER 4
and moment of inertia from the Ball class, as well as the methods getR, getM and getI. In
addition to its inherited properties, the Baton class adds properties describing the length L and
the angular velocity w of the bar, and methods getXa, getYa, getXb and getYb that extract
the positions of the bar ends. The method scenario set up the plot, using position to locate
the balls. Notice how the constructor Baton(Path p, Ball b, ...) takes the Path and Ball
objects as arguments and constructs the Baton object from them. This is interesting because the
Ball and Path belong to classes not included here. For this to work, we must place the Ball
and Path class files in the directory in which we are creating a Baton. The Python compiler
will then find them.
Next we define the get methods for manipulating baton objects. This is the standard way
of indicating that a method will retrieve or extract some property from the object to which the
method is appended. For instance, Baton.getM returns 2m, that is, the sum of the masses of the
two spheres, while getI returns the moment of inertia. The methods getXa, getYa, getXb, and
getYb take time t as an argument and return the coordinates of the baton’s ends at that time.
These methods first determine the position of the COM by calling path.getX or path.getY,
and then add on the relative coordinates of the ends.
Listing 4.11 Dumbbell.py combines ball and path classes to form a dumbbell. 
# Dumbbell . py : Combines c l a s s e s t o form a Baton wi th Mass and Radius
2
from v i s u a l . g raph i m p o r t ∗ # g r a p h i c s and math c l a s s e s
4c l a s s B a l l :
d e f i n i t ( s e l f , mass , r a d i u s ) : # B a l l c o n s t r u c t o r
6s e l f .m = mass # i n i t i a l i z e mass
s e l f . r = r a d i u s # i n i t i a l i z e r a d i u s
8
d e f getM1 ( s e l f ) : # g e t B a l l mass
10r e t u r n s e l f .m
12d e f getR ( s e l f ) : # g e t B a l l r a d i u s
r e t u r n s e l f . r
14
d e f g e t I 1 ( s e l f ) :
16r e t u r n ( 2 . 0 / 5 . 0 ) ∗ s e l f .m∗( s e l f . r )∗∗2
18c l a s s Pa th : # p a r a b o l i c p a t h o f COM
d e f i n i t ( s e l f , v0 , t h e t a ) : # Pa th c o n s t r u c t o r
20s e l f . g = 9 . 8 # g r a v i t y
s e l f . v0 = v0 # i n i t i a l speed
22s e l f . t h e t a = t h e t a # i n i t i a l a n g l e
s e l f . v0x = s e l f . v0∗math . cos ( s e l f . t h e t a ∗math . p i / 1 8 0 . 0 ) # i n i t i a l Vx
24s e l f . v0y = s e l f . v0∗math . s i n ( s e l f . t h e t a ∗math . p i / 1 8 0 . 0 ) # i n i t i a l Vy
26d e f getX ( s e l f , t ) : # g e t Xcom
s e l f . t = t
28r e t u r n s e l f . v0x∗ s e l f . t
30d e f getY ( s e l f , t ) : # g e t Ycom
s e l f . t = t
32r e t u r n s e l f . v0y∗ s e l f . t − 0 .5∗ s e l f . g∗ t ∗∗2
34c l a s s Baton ( Ba l l , Pa th ) : # Baton i n h e r i n t s B a l l & Pa th p r o p e r t i e s
d e f i n i t ( s e l f , mass , r a d i u s , v0 , t h e t a , L1 , w1 ) :
36B a l l . i n i t ( s e l f , mass , r a d i u s ) # c o n t r u c t s B a l l
Pa th . i n i t ( s e l f , v0 , t h e t a ) # c o n s t r u c t s c l a s s P a t h
38s e l f . L = L1 # Lenght o f Baton
s e l f .w = w1 # ang v e l o c i t y Baton
40
d e f getM ( s e l f ) :
42r e t u r n 2 .0∗ s e l f . getM1 ( )
44d e f g e t I ( s e l f ) :
r e t u r n (2∗ s e l f . g e t I 1 ( ) + 0 .5∗ s e l f . getM ( ) ∗ s e l f . L∗∗2)
46
d e f getXa ( s e l f , t ) :
48xa = s e l f . getX ( t ) + 0 .5∗ s e l f . L∗math . cos ( s e l f .w∗ t )
r e t u r n xa
50
d e f getYa ( s e l f , t ) :
52r e t u r n s e l f . getY ( t ) + 0 .5∗ s e l f . L∗math . s i n ( s e l f .w∗ t )
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 87
54d e f getXb ( s e l f , t ) :
r e t u r n s e l f . getX ( t ) − 0 .5∗ s e l f . L∗math . cos ( s e l f .w∗ t )
56
d e f getYb ( s e l f , t ) :
58r e t u r n s e l f . getY ( t ) − 0 .5∗ s e l f . L∗math . s i n ( s e l f .w∗ t )
60d e f s c e n a r i o ( s e l f , m y t i t l e , m y x t i t l e , m y y t i t l e , xma , xmi , yma , ymi ) :
g raph = g d i s p l a y ( x = 0 , y = 0 , wid th = 500 , h e i g h t = 500 , # f o r a s p e c t r a t i o
62t i t l e = m y t i t l e , x t i t l e = m y x t i t l e , y t i t l e = m y y t i t l e , xmax = xma ,
xmin = xmi , ymax = yma , ymin = ymi , f o r e g r o u n d = c o l o r . b l ack ,
64background = c o l o r . w h i t e )
66d e f p o s i t i o n ( s e l f ) :
ba tonmassa = g c u r ve ( c o l o r = c o l o r . b l u e ) # b l u e a t r a j e c t o r y
68ba tonmassb = gc u r ve ( c o l o r = c o l o r . r e d ) # r e d b t r a j e c t o r y
batoncm = g cu rv e ( c o l o r = c o l o r . magenta )
70t = 0 . 0 # s t a r t mot ion a t t =0
c o u n t = 4
72yy = s e l f . getYa ( t ) # i n i t i a l Ya
w h i l e ( s e l f . getYa ( t )>= 0 . 0 ) : # do t i l l Yb <0
74xa = s e l f . getXa ( t ) # Xa
ya = s e l f . getYa ( t ) # Yb
76ba tonmassa . p l o t ( pos = ( xa , ya ) ) # p l o t a
xb = s e l f . getXb ( t )
78yb = s e l f . getYb ( t )
ba tonmassb . p l o t ( pos = ( xb , yb ) ) # p l o t b
80# i f c o u n t%4 == 0 :
# uncommented 2 two l i n e s t o p l o t b a t o n
82# gc u r v e ( pos = [ ( xa , ya ) , ( xb , yb ) ] , c o l o r = c o l o r . cyan )
xcm = s e l f . getX ( t ) # Xcom
84ycm = s e l f . getY ( t ) # Ycom
batoncm . p l o t ( pos = ( xcm , ycm ) ) # p l o t COM
86r a t e ( 1 0 )
t += 0 . 0 2 # i n c r e m e n t s t ime i n 0 . 0 2
88c o u n t += 1
90mybaton = Baton ( 0 . 5 , 0 . 4 , 1 5 . 0 , 3 4 . 0 , 2 . 5 , 1 5 . 0 ) # m r a d i u s v0 t h e t a L w
mybaton . s c e n a r i o (’Positions of mass a(blue), b(red) and center of mass (magenta)’ ,
92’x’ , ’y’ , 20 , 0 , 5 , − 1) # xmx = 20 , xmin = 0
mybaton . p o s i t i o n ( ) # ymax = 10 , ymin = − 1
4.13 MULTIPLE INHERITANCES, CLASSES IN THE SAME FILE
When dealing with multiple classes, you have the choice of placing all the classes in the same
file, or having each class in a different file. While the former is more self-contained, the
latter is more flexible since then the classes can then be used by a variety of programs. The
process of inheritance from multiple classes differs for each point of view. For example, the
program Dumbbell.py in Listing 4.11 has all the Ball, Path and Baton classes in the same file.
Individual files with each class are also given in the same directory. The inheritance of the two
classes begins with the statement
class Baton(Ball, Path): 37
in which the Ball and Path class names are passed as arguments to Baton. Note next that the
constructor of Baton has six arguments, besides self,
def init (self, mass, radius, v0, theta, L1, w1): 39
The arguments mass and radius are used to initialize Ball, the arguments v0 and theta are
used to initialize Path, while L1 and w1 are used to initialize Baton:
Ball. init (self,mass,radius) Construct Ball
Path. init (self,v0,theta) Construct Path
self.L = L1 Length of Baton
self.w = w1 Ang velocity of Baton
Now Baton can use the methods of Ball and Path, as if they were its own:
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
88 CHAPTER 4
def getXa(self,t): 52
xa = self.getX(t) + 0.5 * self.L * math.cos(self.w*t) 53
Here the x position is obtained via the method getX defined in Path. The last lines (92–95)
in the program define an instance myBaton of the class Baton and initializes the three classes
using Baton’s methods scenario and position.
4.14 MULTIPLE INHERITANCE, SEPARATE FILES
The same OOP directory we have been using also contains the separate class files Ball.py,
Path.py and Baton.py. When used as separate files, the classes Ball and Path have to be
compiled to produce the class (byte) files Ball.pyc and Path.pyc. This is done automatically
by IDLE when the Run/Run Module item is selected for Baton.py, which contains the critical
statement
import Ball, Path # import other classes
Then Baton will look for these .pyc files when it imports Ball and Path.
Note next the unusual way in which the Ball and Path classes are initialized with the use of
double names: 
c l a s s Baton ( B a l l . Ba l l , Pa th . Pa th ) : # Baton i n h e r i t s B a l l and Pa th p r o p e r t i e s
d e f i n i t ( s e l f , mass , r a d i u s , v0 , t h e t a , L1 , w1 ) :
B a l l . B a l l . i n i t ( s e l f , mass , r a d i u s ) # c o n s t r u c t B a l l
Pa th . Pa th . i n i t ( s e l f , v0 , t h e t a ) # c o n s t r u c t Pa th
s e l f . L=L1 # l e n g t h o f Baton
s e l f .w=w1
The first of the double names refers to the class, the second to the object. The rest of the
program is the same as before and the results are identical.
The technique of constructing complex objects from simpler ones is called composition.
As a consequence of the simple objects being contained within the more complex ones, the
former are described as active class variables. This means that their properties change depend-
ing upon which object they are within. When you use composition to create more complex
objects, you are working at a higher level of abstraction. Ideally, composition hides the dis-
tracting details of the simpler objects from your view so that you focus on the major task to be
accomplished. This is analogous to first designing the general form of a bridge before worrying
about the colors of the cables to be used.
4.14.1 Composition Exercise
1. Compile and run the latest version of Baton.py. The program should run and plot the
trajectory of one end of the baton as it travels through the air (Figure 4.7).
2. Plot the trajectory of end b on the same graph that shows the trajectory of end a, but in
a different color. You may do this by copying and pasting the for loop for a and then
modifying it for b.
3. Change the mass of the ball in the Baton constructor method to some large number, for
instance, 50 kg. Add print statements in the constructor and the main program to show
how the ball class variable and the myBall object are affected by the new mass. You
should find that ball and myBall both reference the same object since they both refer to
the same memory location. In this way changes to one object are reflected in the other
object.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 89
In Python, an object is passed between methods and manipulated by reference. This
means that its address in memory is passed and not the actual values of all the component parts
of it. On the other hand, primitive data types like int and double are manipulated by value.
At times we may actually say that objects are references. This means that when one object is
set equal to another, both objects point to the same location in memory (the start location of
the first component of the object). Therefore all three variables myBall, p, and q refer to the
same object in memory. If we change the mass of the ball, all three variables will reflect the
new mass value. This also works for object arguments: If you pass an object as an argument to
a method and the method modifies the object, then the object in the calling program will also
be modified.
4.14.2 Calculating the Baton’s Energy (Extension)
Extend your classes so that they plot the energy of the baton as a function of time. Plot the ki-
netic energy of translation, the kinetic energy of rotation, and the potential energy as functions
of time:
1. The translational kinetic energy of the baton is the energy associated with the motion of
the center of mass. Write a getKEcm method in the Baton class that returns the kinetic
energy of translation KEcm(t) = mvcm(t)2/2. In terms of pseudocode the method is 
Get p r e s e n t v a l u e o f Vx .
Get p r e s e n t v a l u e o f Vy .
Compute Vˆ2 = Vxˆ2 + Vy ˆ 2 .
Re tu rn mVˆ 2 / 2 .
Before you program this method, write getVx and getVy methods that extract the COM
velocity from a baton. Seeing as how the Path class already computes xcm(t) and ycm(t),
it is the logical place for the velocity methods. As a guide, we suggest consulting the getX
and getY methods.
2. Next we need the method getKEcm in the Baton class to compute KEcm(t). Inasmuch as
the method will be in the Baton class, we may call any of the methods in Baton, as well
as access the path and ball subobjects there (subobjects because they reside inside the
Baton object or class). We obtain the velocity components by applying the getV methods
to the path subobject within the Baton object: 
d e f getKEcm ( s e l f , t ) : # COM t r a n s l a t i o n a l KE
r e t u r n s e l f . getM ( ) ∗ ( ( s e l f . getVx ( t ) )∗∗2 +( s e l f . getVy ( t ) ) ∗∗2) / 2 .
Even though the method is in a different class than the object, Python handles this. Study
how getM(), being within getKEcm, acts on the same object as does getKEcm without
explicitly specifying the object.
3. Compile the modified Baton and Path classes.
4. Modify Baton.py to plot the translational kinetic energy of the center of mass as a func-
tion of time. Comment out the old for loops used for plotting the positions of the baton’s
ends and add the code 
d e f e n e r g i e s ( s e l f ) :
batonKEr = g cu r ve ( c o l o r = c o l o r . b l u e ) # b l u e : f o r r o t a t i o n a l KE
batonPE = g cu r ve ( c o l o r = c o l o r . g r e e n ) # g r e e n : f o r PE
batonKEcm = g c u r ve ( c o l o r = c o l o r . magenta ) # magenta : f o r KEcm
b a t o n E t o t a l = g cu rv e ( c o l o r = c o l o r . r e d ) # r e d : f o r t o t a l E
t = 0 . 0 # s t a r t mot ion a t t ime 0
yy = s e l f . getYa ( t ) # i n i t i a l Ya ( t =0)
w h i l e ( s e l f . getYa ( t ) >=0.0) : # do t i l l Ya < 0
KEcm = s e l f . getKEcm ( t ) # KEcom
PE = s e l f . getPE ( t ) # PE
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
90 CHAPTER 4
KEr = s e l f . getKEr ( t ) # r o t a t i o n a l KE
xcm= s e l f . getX ( t ) # Xcom
E t o t = KEcm + PE + KEr # t o t a l E
batonKEcm . p l o t ( pos =(xcm , KEcm) ) # p l o t KEcom
batonKEr . p l o t ( pos =(xcm , KEr ) ) # p l o t KErot
b a t o n E t o t a l . p l o t ( pos =(xcm , E t o t ) ) # p l o t t o t a l E
batonPE . p l o t ( pos =(xcm , PE ) )
t += 0 . 0 2 # i n c r e m e n t t
Compile the modified Baton.py and check that your plot is physically reasonable. The
translational kinetic energy should decrease and then increase as the baton goes up and
comes down.
5. Write a method in the Baton class that computes the kinetic energy of rotation about the
COM, KEro = 12Iω
2. Call getI to extract the moment of inertia of the baton and check
that all classes still compile properly.
6. The potential energy of the baton PE(t) = mgycm(t) is that of a point particle with
the total mass of the baton located at the COM. Write a method in the Baton class
that computes PE. Use getM to extract the mass of the baton and use path.g to ex-
tract the acceleration due to gravity g. To determine the height as a function of time,
write a method path.getY(t) that accesses the path object. Make sure that the methods
getKEcm, getKEr, and getPE are in the Baton class.
7. Plot on one graph the translational kinetic energy, the kinetic energy of rotation, the
potential energy, and the total energy.
Check that the total energy and rotational energies remain constant in time. However, the
gravitational potential energy should fluctuate.
4.14.3 Examples of Inheritance and Object Hierarchies
Up until this point we have built up our classes via composition, that is, by placing objects
inside other objects (using objects as arguments). As powerful as composition is, it is not
appropriate in all circumstances. As a case in point, you may want to modify the Baton class
to create similar, but not identical, objects such as 3-D batons. A direct approach to extending
the program would be to copy and paste parts of the original code into a new class and then
modify the new class. However, this is error-prone and leads to long, complicated, repetitive
code. The OOP approach applies the concept of inheritance to allow us to create new objects
that inherit the properties of old objects but have additional properties as well. This is how we
create entire hierarchies of objects.
As an example, let us say we want to place red balls on the ends of the baton. We make
a new class RedBall that inherits properties from Ball: 
c l a s s RedBal l ( B a l l ) : # D e f i n e s RedBal l a s s u b c l a s s o f B a l l
. . .
As written, this code creates a RedBall class that is identical to the original Ball class. The
key word extends tells Java to copy all the methods and variables from Ball into RedBall. In
OOP terminology, the Ball class is the parent class or superclass, and the RedBall class is the
child class or subclass. It follows then that a class hierarchy is a sort of family tree for classes,
with the parent classes at the top of the tree. As things go, children beget children of their own
and trees often grow high.
To make RedBall different from Ball, we add the property of color: 
c l a s s RedBal l ( B a l l ) : # D e f i n e s RedBal l a s s u b c l a s s o f B a l l
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 91
d e f i n i t ( s e l f , mass , r a d i u s ) : # RedBal l C o n s t r u c t o r
B a l l . i n i t ( s e l f , mass , r a d i u s ) # B a l l C o n s t r u c t o r
d e f g e t C o l o r ( s e l f ) : # New p r o p e r t y o f s u b c l a s s
r e t u r n ’Red’
d e f getR ( s e l f ) : # O v e r r i d e s B a l l’s getR method
return 1.0
We see that the child class receives mass and radius as arguments in addition to self, and
transfers them to Ball via its constructor. In addition there are the two methods getColor and
getR that overrides the Ball methods with the same name. This means that RedBall inherits
mass and radius from Ball, and that the new getR method overrides the original method.
4.14.4 Baton with a Lead Weight (Application)
As a second example, we employ inheritance to create a class of objects representing a baton
with a weight at its center. We call this new class LeadBaton.py and make it a child of the
parent class Baton.py. Consequently, the LeadBaton class inherits all the methods from the
parent Baton class, in addition to having new ones of its own:
Listing 4.12 LeadBaton.py inherits ball and path properties from Baton. 
# LeadBaton . py i n h e r i t s methods from b a t o n
2
from v i s u a l . g raph i m p o r t ∗ # g r a p h i c s and math c l a s s e s
4i m p o r t Bal l , Pa th
’’’LeadBaton class inheritance of methods from Baton’’’
6
c l a s s Baton ( B a l l . Ba l l , Pa th . Pa th ) : # Baton i n h e r i t s B a l l and Pa th p r o p e r t i e s
8
d e f i n i t ( s e l f , mass , r a d i u s , v0 , t h e t a , L1 , w1 ) :
10B a l l . B a l l . i n i t ( s e l f , mass , r a d i u s ) # c o n t r u c t B a l l
Pa th . Pa th . i n i t ( s e l f , v0 , t h e t a ) # c o n s t r u c t Pa th
12s e l f . L = L1 # Lenght o f Baton
s e l f .w = w1 # ang v e l o c i t y
14
d e f getM ( s e l f ) :
16r e t u r n 2 .0∗ s e l f . getM1 ( )
18d e f g e t I ( s e l f ) :
r e t u r n (2∗ s e l f . g e t I 1 ( ) + 0 .5∗ s e l f . getM ( ) ∗ s e l f . L∗∗2)
20
d e f getXa ( s e l f , t ) :
22xa = s e l f . getX ( t ) + 0 .5∗ s e l f . L∗math . cos ( s e l f .w∗ t )
r e t u r n xa
24
d e f getYa ( s e l f , t ) :
26r e t u r n s e l f . getY ( t ) + 0 .5∗ s e l f . L∗math . s i n ( s e l f .w∗ t )
28d e f getXb ( s e l f , t ) :
r e t u r n s e l f . getX ( t ) − 0 .5∗ s e l f . L∗math . cos ( s e l f .w∗ t )
30
d e f getYb ( s e l f , t ) :
32r e t u r n s e l f . getY ( t ) − 0 .5∗ s e l f . L∗math . s i n ( s e l f .w∗ t )
34d e f s c e n a r i o ( s e l f , m y t i t l e , m y x t i t l e , m y y t i t l e , xma , xmi , yma , ymi ) :
g raph = g d i s p l a y ( x = 0 , y = 0 , wid th = 500 , h e i g h t = 500 , # f o r same a s p e c t r a t i o i n a x i s
36t i t l e = m y t i t l e , x t i t l e = m y x t i t l e , y t i t l e = m y y t i t l e , xmax = xma ,
xmin = xmi , ymax = yma , ymin = ymi , f o r e g r o u n d = c o l o r . b l ack ,
38background = c o l o r . w h i t e )
40d e f p o s i t i o n ( s e l f ) :
ba tonmassa = g c u r ve ( c o l o r = c o l o r . b l u e ) # b l u e t r a j e c t o r y mass a
42ba tonmassb = gc u r ve ( c o l o r = c o l o r . r e d ) # r e d t r a j e c t o r y mass b
batoncm = g cu rv e ( c o l o r = c o l o r . magenta )
44t = 0 . 0 # s t a r t mot ion a t t ime 0
c o u n t = 4
46yy = s e l f . getYa ( t ) # i n i t i a l y p o s i t i o n mass a
w h i l e ( s e l f . getYa ( t )>= 0 . 0 ) : # do t i l l Yb <0
48xa = s e l f . getXa ( t ) # Xa
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
92 CHAPTER 4
ya = s e l f . getYa ( t ) # Yb
50ba tonmassa . p l o t ( pos = ( xa , ya ) ) # p l o t a
xb = s e l f . getXb ( t )
52yb = s e l f . getYb ( t )
ba tonmassb . p l o t ( pos = ( xb , yb ) ) # p l o t b
54# i f c o u n t%4 == 0 : # p l o t s b a t o n i f uncommented 2 n e x t two l i n e s
# gc u r ve ( pos = [ ( xa , ya ) , ( xb , yb ) ] , c o l o r = c o l o r . cyan )
56xcm = s e l f . getX ( t ) # Xcom
ycm = s e l f . getY ( t ) # Ycom
58batoncm . p l o t ( pos = ( xcm , ycm ) ) # p l o t s COM
t += 0 . 0 2 # i n c r e m e n t s t
60c o u n t += 1
62c l a s s LeadBaton ( Baton ) :
64d e f i n i t ( s e l f , mass , r a d i u s , v0 , t h e t a , L1 , w1 , M1) : # C o n s t r u c t o r
"To initialize Baton"
66Baton . i n i t ( s e l f , mass , r a d i u s , v0 , t h e t a , L1 , w1 ) # i n i t Baton
s e l f .MM = M1 + s e l f . getM1 ( ) # l e a d mass + Baton mass
68d e f getM ( s e l f ) :
r e t u r n s e l f .MM
70
myLeadBaton = LeadBaton ( 0 . 5 , 0 . 4 , 1 5 . 0 , 3 4 . 0 , 2 . 5 , 1 5 . 0 , 3 5 . 0 )
72p r i n t "myLeadBaton mass = " , myLeadBaton . getM ( )
p r i n t "Enter any character to finish"
74s= r a w i n p u t ( )
The first line declares LeadBaton to be a subclass of Baton. The second line contains the
constructor for a LeadBaton object, with the first six arguments (not counting self) needed
to initialize the Baton, and the last argument being the lead mass. Note next that the getM()
method for LeadBaton overrides the method with the same name in Baton, and returns the
LeadBaton mass. Finally, the instance of myLeadBaton is built with all seven parameters (be-
sides self) and with the mass property of LeadBaton.
Exercise:
1. Run and create plots from the LeadBaton class. Instead of creating Baton, now create a
LeadBaton with
LeadBaton myBaton = new LeadBaton(myPath, myBall, 2.5, 15., 10.);
Here the argument “10.” describes a 10-kg mass at the center of the baton.
2. Run the program and check that you get a plot of the energies of the lead baton versus
time. Compare its energy to that of an ordinary baton and comment on the differences.
3. You should see now how OOP permits us to create many types of batons with only slight
modifications of the code. You can switch between a Baton and a LeadBaton object
with only a single change, a modification that would be significantly more difficult with
procedural programming.
4.14.5 Encapsulation to Protect Classes
In the previous section we created the classes for Ball, Path, and Baton objects. In all cases the
Python source code for each class had the same basic structure: class variables, constructors,
and get methods. Yet classes do different things, and it is common to categorize the functions
of classes as either of the following.
Interface: How the outside world manipulates an object; all methods that are applied to that
object; or
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 93
Implementation: The actual internal workings of an object; how the methods make their
changes.
As applied to our program, the interface for the Ball class includes a constructor Ball and the
getM, getR, and getI methods. The interface for the Path class includes a constructor Path
and the getX and getY methods.
Pure OOP strives to keep objects abstract and to manipulate them only through methods.
This makes it easy to follow and to control where variables are changed and thereby makes
modifying an existing program easier and less error-prone. With this purpose in mind, we
separate methods into those that perform calculations and those that cause the object to do
things.
As programmers, we may rewrite an object’s code as we want and still have the same
working object with a fixed interface for the rest of the world. Furthermore, since the object’s
interface is constant, even though we may change the object, there is no need to modify any
code that uses the object. This is a great advance in the ability to reuse code and to use other
people’s codes properly.
This two-step process of creating and protecting abstract objects is known as encapsu-
lation. An encapsulated object may be manipulated only in a general manner that keeps the
irrelevant details of its internal workings safely hidden within. Just what constitutes an “irrel-
evant detail” is in the eye of the programmer. In general, you should place the private key
word before every nonstatic class variable and then write the appropriate methods for accessing
the relevant variables. This OOP process of hiding the object’s variables is called data hiding.
4.14.6 Encapsulation Exercise
1. Place the private key word before all the class variables in Ball.py. This accomplishes
the first step in encapsulating an object. Print out myBall.m and myBall.r.
2. Create methods that allow you to manipulate the object in an abstract way; for example,
to modify the mass of a Ball object and assign it to the private class variable m, include
the command myBall.setM(5.0). This is the second step in encapsulation. We already
have the methods getM, getR, and getI, and the object constructor Ball, but they do not
assign a mass to the ball. Insofar as we have used a method to change the private variable
m, we have kept our code as general as possible and still have our objects encapsulated.
3. When we write getM(), we are saying that M is the property to be retrieved from a Ball
object. Inversely, the method setM sets the property M of an object equal to the argument
that is given. This is part of encapsulation because with both get and set methods on
hand, you do not need to access the class variables from outside the class. The use of get
and set methods is standard practice in Python. You do not have to write get and set
methods for every class you create, but you should create these methods for any class you
want encapsulated. If you look back at Chapter 3, “Visualization Tools,” you will see that
the classes in the PtPlot library have many get and set methods, for example, getTitle,
setTitle, getXLabel, and setXLabel.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
94 CHAPTER 4
4.14.7 Complex Object Interface (Extension)
In Listing ?? we display KomplexPolar.py, our design for an interface for complex numbers.
In Listing 4.13 we give KomplexPolarTest.py, a program that tests our interface. To avoid
confusion with Complex objects, we call the new object Komplex. We include methods for
addition, subtraction, multiplication, division, negation, and conjugation, as well as get and
set methods for the real, imaginary, modulus, and phase. Remember, an interface must give
the arguments and return type for each method.
Listing 4.13 KomplexPolar.py is an interface for complex numbers and is used in KomplexPolarTest.py in List-
ing 4.13. 
"""
2
#KomplexPolar: complex numbes via modulus
4
import math
6
’’’Cartesian/polar complex via package
8" t y p e c = 0" - >polar representation, else: rectangular’’’
class Komplex:
10
def __init__(self, x, y, typec): # Kopmplex constructor
12if typec == 0: # 0: for polar rep
self.mod = x # magnitud
14self.theta = y # polar angle
else: # rectangular representation
16self.re = x
self.im = y
18
def getRe(self): # return rectangular real part
20return self.mod*math.cos(self.theta)
22def getIm(self): # return rectangular imag part
return self.mod*math.sin(self.theta)
24
def setRe(self): # returns rectangular real part
26re = self.mod*math.cos(self.theta)
return re
28
def setIm(self): # return rectangular imag part
30im = self.mod*math.sin(self.theta)
return im
32
def add(self, other, typec): # add two complex numbers
34if typec == 0: # polar representation
tempMod = math.sqrt(self.mod*self.mod + other.mod*other.mod
36+ 2*self.mod*other.mod*math.cos(self.theta - other.theta) )
angtheta = math.atan2(self.mod*math.sin(self.theta) # notice atan2
38+ other.mod*math.sin(other.theta), self.mod*math.cos(self.theta)
+ other.mod*math.cos(other.theta) )
40return Komplex(tempMod, angtheta, 0) # returns sum in polar coord.
else: # returns sum in rectangular coord.
42return Komplex(self.re + other.re, self.im + other.im, 1)
44def sub(self, other, typec): # now for subtraction
if typec == 0:
46tempmod = math.sqrt(self.mod*self.mod + other.mod*other.mod -
2*self.mod*other.mod*(math.cos(self.theta)*math.cos(other.theta)
48+ math.sin(self.theta)*math.sin(other.theta) ))
y = self.mod*math.sin(self.theta) - other.mod*math.sin(other.theta)
50x = self.mod*math.cos(self.theta) - other.mod*math.cos(other.theta)
angtheta = math.atan2(y, x)
52return Komplex(tempmod, angtheta, 0)
else:
54return Komplex(self.re - other.re, self.im - other.im, 1)
56def div(self, other, typec): # complex division z1/z2
if typec == 0:
58return Komplex(self.mod/other.mod, self.theta - other.theta, 0)
else:
60numre = self.re*other.re + self.im*other.im
deno = other.re*other.re + other.im*other.im
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 95
62numim = self.im*other.re - self.re*other.im
return Komplex(numre/deno, numim/deno, 1)
64
def mult(self, other, typec): # complex multiplication
66if typec == 0:
return Komplex(self.mod*other.mod, self.theta + other.theta, 0)
68else:
return Komplex(self.re*other.re - self.im*other.im,
70self.re*other.im + self.im*other.re, 1)
72def conj(self, typec): # complex conj.
if typec == 0:
74self.mod = self.mod
self.theta = - self.theta
76return(self.mod, self.theta, 0)
else:
78return Komplex(self.re, - self.im, 1)
We still represent complex numbers in Cartesian or polar coordinates:
z = x+ iy = reiθ. (4.32)
Insofar as the complex number itself is independent of representation, we must be able to switch
between the rectangular or polar representation. This is useful because certain manipulations
are simpler in one representation than in the other; for example, division is easier in polar
representation:
z1
z2
=
a+ ib
c+ id
=
ac+ bd+ i(bc− ad)
c2 + d2
=
r1e
iθ1
r2eiθ2
=
r1
r2
ei(θ1−θ2). (4.33)
Listings ?? and 4.12 are our implementation of an interface that permits us to use ei-
ther representation when manipulating complex numbers. There are three files, Komplex,
KomplexInterface, and KomplexTest, all given in the listings. Because these classes call
each other, each must be in a class by itself. However, for the compiler to find all the classes,
they need to be in the same directory.
Listing 4.14 KomplexPolarTest.py manipulates complex numbers using the interface KomplexInterface in List-
ing ??. 
# KomplexPo la rTes t . py : t e s t s Komplex package
2
from KomplexPolar i m p o r t Komplex # b r i n g Komplex from KomplexPolar
4
i m p o r t math
6
a = Komplex ( 1 . 0 , 1 . 0 , 1 ) # i n i t i a l i z e Komplex a : c a r t e s i a n
8b = Komplex ( 1 . 0 , 2 . 0 , 1 ) # i n i t i a l i z e Komplex b : c a r t e s i a n
10p r i n t "Cartesian: Re a = " , a . re , " Im a = " , a . im # r e a l and im p a r t s
p r i n t "Cartesian: Re b = " , b . re , " Im b = " , b . im
12
c = Komplex . add ( a , b , 1 ) # a + b = c
14
p r i n t "Cartesian: c = a + b = (" , c . re , ", " , c . im , ")" # r e & im p a r t s
16e = b
18p r i n t "Cartesian: e = b = (" , e . re , ", " , e . im , ")" # Komplex c a r t e s i a n
"Polar version, uses get and set methods" # now p o l a r examples
20
a = Komplex ( math . s q r t ( 2 . 0 ) , math . p i / 4 . 0 , 0 ) # use R and t h e t a
22b = Komplex ( math . s q r t ( 5 . ) , math . a t a n 2 ( 2 . 0 , 1 . 0 ) , 0 ) # b = (R , t h e t a , 0 )
24p r i n t "Polar: Re a = " , a . ge tRe ( ) , ", Im a = " , a . ge t Im ( ) # c a r t e s i a n r e
p r i n t "Polar: Re b = " , b . ge tRe ( ) , ", Im b = " , b . ge t Im ( ) # c a r t e s i a n im
26
c = Komplex . add ( a , b , 0 ) # a + b = c
28e = c # d e f i n e s e
30p r i n t "Polar: Re e = " , e . ge tRe ( ) , ", Im e = " , e . ge t Im ( ) # c a r t e s i a n r e & im
p r i n t "Enter any character to finish"
32s= r a w i n p u t ( )
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
96 CHAPTER 4
You should observe how KomplexInterface requires us to have methods for getting and
setting the real and imaginary parts of Komplex objects, as well as adding, subtracting, mul-
tiplying, dividing, and conjugating complex objects. (In the comments we see the suggestion
that there should also be methods for getting and setting the modulus and phase.)
The class Komplex contains the constructors for Komplex objects. This differs from our
previous implementation Complex by having the additional integer variable type. If type = 0,
then the complex numbers are in polar representation, else they are in Cartesian representation.
So, for example, the method for arithmetic, such as the add method on line 22, is actually two
different methods depending upon the value of type. In contrast, the get and set methods for
real and imaginary parts are needed only for the polar representation, and so the value of type
is not needed.
4.15 OOP EXAMPLE: SUPERPOSITION OF MOTIONS
The isotropy of space implies that motion in one direction is independent of motion in other
directions. So, when a soccer ball is kicked, we have acceleration in the vertical direction
and simultaneous, yet independent, uniform motion in the horizontal direction. In addition,
Galilean invariance (velocity independence of Newton’s laws of motion) tells us that when an
acceleration is added to uniform motion, the distance covered due to the acceleration adds to
the distance covered due to uniform velocity.
Your problem is to describe motion in such a way that velocities and accelerations in
each direction are treated as separate entities or objects independent of motion in other direc-
tions. In this way the problem is viewed consistently from both the programming philosophy
and the basic physics.
4.16 NEWTON’S LAWS OF MOTION (THEORY)
Newton’s second law of motion relates the force vector F acting on a massm to the acceleration
vector a of the mass:
F = ma, Fi = m
d2xi
dt2
, (i = 1, 2, 3). (4.34)
If the force in the x direction vanishes, Fx = 0, the equation of motion (4.34) has a solution
corresponding to uniform motion in the x direction with a constant velocity v0x:
x = x0 + v0xt. (4.35)
Equation (4.35) is the base or parent object in our example. If the force in the y direction also
vanishes, then there will also be uniform y motion:
y = y0 + v0yt. (4.36)
We consider uniform x motion as a parent and view uniform y motion as a child.
Equation (4.34) tells us that a constant force in the x direction causes a constant acceler-
ation ax in that direction. The solution of the x equation of motion with uniform acceleration
is
x = x0 + v0xt+ 12axt
2. (4.37)
For projectile motion without air resistance, we usually have ax = 0 and ay = −g =
−9.8 m/s2:
y = y0 + v0yt− 12gt
2. (4.38)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PYTHON OBJECT-ORIENTED PROGRAMS: IMPEDANCE & BATONS 97
This y motion is a child of the parent x motion.
4.17 OOP CLASS STRUCTURE (METHOD)
The class structure we use to solve our problem contains the objects
Parent class Um1D: 1-D uniform motion for given initial conditions,
Child class Um2D: uniform 2-D motion; child class of Um1D,
Child class Am2d: 2-D accelerated motion; child class of Um2D.
The member functions include
x: position after time t,
archive: creator of a file of position versus time.
For our projectile motion, encapsulation is a combination of the initial conditions (x0, vx0)
with the member functions used to compute x(t). Our member functions are the creator of the
class of uniform 1-D motion Um1D and the creator x(t) of a file of x as a function of time t.
Inheritance is the child class Um2D for uniform motion in both the x and y directions, it being
created from the parent class Um1D of 1-D uniform motion. Abstraction is present (although
not used powerfully) by the simple addition of motion in the x and y directions. Polymorphism
is present by having the member function that creates the output file different for 1-D and 2-
D motions. In this implementation of OOP, the class Accm2D for accelerated motion in two
dimensions inherits uniform motion in two dimensions (which in turn inherits uniform 1-D
motion) and adds to it the attribute of acceleration.
4.18 PYTHON IMPLEMENTATION
Listing 4.15 Accm2D.py is an OOP program for accelerated motion in two dimensions. 
# Accmd . Py : Python a c c e l e r a t e d mot ion i n 2D
2
from v i s u a l . g raph i m p o r t ∗
4
c l a s s Um1D:
6
d e f i n i t ( s e l f , x0 , d t , vx0 , t t o t ) : # c l a s s c o n s t r u c t o r
8s e l f . x00 = 0 # i n i t i a l x p o s i t i o n
s e l f . d e l t = d t # t ime i n c r e m e n t
10s e l f . vx = vx0 # x v e l o c i t y
s e l f . t ime = t t o t # t o t a l t i e m e
12s e l f . s t e p s = i n t ( t t o t / s e l f . d e l t ) # t o t a l number s t e p s ( c o n v e r t t o i n t )
14d e f x ( s e l f , t t ) : # x p o s i t i o n a t t ime t t
r e t u r n s e l f . x00 + t t ∗ s e l f . vx
16’’’to be used in graphics’’’
18d e f s c e n a r i o ( s e l f , mxx , myy , m y t i t l e , m y x t i t l e , m y y t i t l e , xma , xmi , yma , ymi ) : # g e n e r a l
g raph = g d i s p l a y ( x = mxx , y = myy , wid th = 500 , h e i g h t = 200 , # f o r same a s p e c t r a t i o i n
a x i s
20t i t l e = m y t i t l e , x t i t l e = m y x t i t l e , y t i t l e = m y y t i t l e , xmax = xma ,
xmin = xmi , ymax = yma , ymin = ymi , f o r e g r o u n d = c o l o r . b l ack ,
22background = c o l o r . w h i t e )
24d e f a r c h i v e ( s e l f ) : # p roduce f i l e , p l o t 1D x mot ion
unimot ion1D = g cu r ve ( c o l o r = c o l o r . b l u e )
26t t = 0 . 0
f = open (’unimot1D.dat’ , ’w’ ) # Disk f i l e p roduced f o r 1D mot ion
28f o r i i n r a n g e ( s e l f . s t e p s ) :
xx = s e l f . x ( t t )
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
98 CHAPTER 4
30unimot ion1D . p l o t ( pos = ( t t , xx ) ) # P l o t s x vs t ime
f . w r i t e (" %f %f\n"%( t t , xx ) ) # x vs t f o r f i l e
32t t += s e l f . d e l t # i n c r e a s e t ime
f . c l o s e d # c l o s e d i s k f i l e
34’’’Uniform motion in 2D’’’
36c l a s s Um2D(Um1D) : # Um2D s u b c l a s s o f Um1D
38d e f i n i t ( s e l f , x0 , d t , vx0 , t t o t , y0 , vy0 ) : # C o n s t r u c t o r Um2D
Um1D. i n i t ( s e l f , x0 , d t , vx0 , t t o t ) # t o c o n s t r u c t Um1D
40s e l f . y00 = y0 # i n i t i a l i z e s y p o s i t i o n
s e l f . vy = vy0 # i n i t i a l i z e s y v e l o c i t y
42
d e f y ( s e l f , t t ) : # p r o d u c e s y a t t ime t t
44r e t u r n s e l f . y00 + t t ∗ s e l f . vy
46d e f a r c h i v e ( s e l f ) : # o v e r r i d e s a r c h i v e f o r 1D
unimot2d = g cu r ve ( c o l o r = c o l o r . magenta )
48t t = 0 . 0
f = open (’Um2D.dat’ , ’w’ ) # Opens new Um2D f i l e
50f o r i i n r a n g e ( s e l f . s t e p s ) :
xx = s e l f . x ( t t )
52yy = s e l f . y ( t t )
unimot2d . p l o t ( pos = ( xx , yy ) ) # p l o t s y vs x p o s i t i o n
54f . w r i t e (" %f %f\n"%(xx , yy ) ) # w r i t e s x y i n a r c h i v e
t t += s e l f . d e l t
56f . c l o s e d # c l o s e s open Um2D f i l e
58’’’Accelerated motion in 2D’’’
c l a s s Accm2D (Um2D) : # Daug the r o f U21D
60d e f i n i t ( s e l f , x0 , d t , vx0 , t t o t , y0 , vy0 , accx , accy ) : # Accm2D c o n s t r u c t o r
Um2D. i n i t ( s e l f , x0 , d t , vx0 , t t o t , y0 , vy0 ) # Um2D c o n s t r u c t o r
62s e l f . ax = accx # adds a c c e l e r e t i o n s
s e l f . ay = accy # t o t h i s c l a s s
64
d e f xy ( s e l f , t t , i ) :
66s e l f . xxac = s e l f . x ( t t ) + s e l f . ax∗ t t ∗∗2
s e l f . yyac = s e l f . y ( t t ) + s e l f . ay∗ t t ∗∗2
68i f ( i == 1) : # i f a c c e l e r a t i o n i n x
r e t u r n s e l f . xxac
70e l s e :
r e t u r n s e l f . yyac # i f a c c e l e t a t i o n i n y
72
d e f a r c h i v e ( s e l f ) :
74acmot ion = gc u rv e ( c o l o r = c o l o r . r e d )
t t = 0 . 0
76f = open (’Accm2D.dat’ , ’w’ )
f o r i i n r a n g e ( s e l f . s t e p s ) :
78s e l f . xxac = s e l f . xy ( t t , 1 )
s e l f . yyac = s e l f . xy ( t t , 2 )
80f . w r i t e (" %f %f\n"%( s e l f . xxac , s e l f . yyac ) ) # t o d i s k f i l e
acmot ion . p l o t ( pos = ( s e l f . xxac , s e l f . yyac ) ) # p l o t acc . mot ion
82t t += s e l f . d e l t
f . c l o s e d
84
#comment unmd um2d or myAcc t o change p l o t
86unmd = Um1D( 0 . 0 , 0 . 1 , 2 . 0 , 4 . 0 ) # x0 , dt , vx0 , t t o t
unmd . s c e n a r i o ( 0 , 0 , ’Uniform motion in 1D ’ , # f o r 1D un i fo rm mot ion
88’time’ , ’x’ , 4 . 0 , 0 , 1 0 . 0 , 0 ) # For ID tmax tmin xmax xmin
unmd . a r c h i v e ( ) # a r c h i v e 1D
90um2d = Um2D( 0 . 0 , 0 . 1 , 2 . 0 , 4 . 0 , 0 . 0 , 5 . 0 ) # x0 , dt , vx0 , t t o t , y0 , vy0
um2d . s c e n a r i o ( 0 , 200 , ’Uniform motion in 2D ’ , # f o r 2D un i fo rm mot ion
92’x’ , ’y’ , 1 0 . 0 , 0 , 2 5 . 0 , 0 ) # xmx xmin ymax ymin
um2d . a r c h i v e ( ) # a r c h i v e i n two dim . mot ion
94myAcc = Accm2D ( 0 . 0 , 0 . 1 , 1 4 . 0 , 4 . 0 , 0 . 0 , 1 4 . 0 , 0 . 0 , − 9 . 8 )
myAcc . s c e n a r i o ( 0 , 400 , ’Accelerated motion ’ , ’x’ , ’y’ , 55 , 0 , 5 , − 1 0 0 . 0 ) # f o r ac . mot ion 2D
96myAcc . a r c h i v e ( ) # a r c h i v e i n a c c e l e r a t e d mot ion
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Five
Monte Carlo Simulations (Nonthermal)
Unit I of this chapter addresses the problem of how computers generate numbers that appear
random and how we can determine how random they are. Unit II shows how to use these
random numbers to simulate physical processes. In Chapter 6, “Integration,” we see show
how to use these random numbers to evaluate integrals, and in Chapter 15, “Thermodynamic
Simulations & Feynman Quantum Path Integration,” we investigate the use of random numbers
to simulate thermal processes and the fluctuations in quantum systems.
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
Random Numbers for Monte Carlo 5.1–5.2 Monte Carlo Simulations 5.3–5.6
Applets
Name Sections Name Sections
Radio Decay 5.5–5.62
5.1 UNIT I. DETERMINISTIC RANDOMNESS
Some people are attracted to computing because of its deterministic nature; it’s nice to have a
place in one’s life where nothing is left to chance. Barring machine errors or undefined vari-
ables, you get the same output every time you feed your program the same input. Nevertheless,
many computer cycles are used for Monte Carlo calculations that at their very core include el-
ements of chance. These are calculations in which random numbers generated by the computer
are used to simulate naturally random processes, such as thermal motion or radioactive decay,
or to solve equations on the average. Indeed, much of computational physics’ recognition has
come about from the ability of computers to solve previously intractable problems using Monte
Carlo techniques.
5.2 RANDOM SEQUENCES (THEORY)
We define a sequence of numbers r1, r2, . . . as random if there are no correlations among the
numbers. Yet being random does not mean that all the numbers in the sequence are equally
likely to occur. If all the numbers in a sequence are equally likely to occur, then the sequence
is said to be uniform, and the numbers can be random as well. To illustrate, 1, 2, 3, 4, . . . is
uniform but probably not random. Further, it is possible to have a sequence of numbers that, in
some sense, are random but have very short-range correlations among themselves, for example,
r1, (1− r1), r2, (1− r2), r3, (1− r3), . . .
have short-range but not long-range correlations.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
100 CHAPTER 5
Mathematically, the likelihood of a number occurring is described by a distribution func-
tion P (r), where P (r) dr is the probability of finding r in the interval [r, r + dr]. A uniform
distribution means that P (r) = a constant. The standard random-number generator on com-
puters generates uniform distributions between 0 and 1. In other random words, the standard
random-number generator outputs numbers in this interval, each with an equal probability yet
each independent of the previous number. As we shall see, numbers can also be generated
nonuniformly and still be random.
By the nature of their construction, computers are deterministic and so cannot create a
random sequence. By the nature of their creation, computed random number sequences must
contain correlations and in this way are not truly random. Although it may be a bit of work,
if we know rm and its preceding elements, it is always possible to figure out rm+1. For this
reason, computers are said to generate pseudorandom numbers (yet with our incurable laziness
we won’t bother saying “pseudo” all the time). While more sophisticated generators do a
better job at hiding the correlations, experience shows that if you look hard enough or use
these numbers long enough, you will notice correlations. A primitive alternative to generating
random numbers is to read in a table of true random numbers determined by naturally random
processes such as radioactive decay or to connect the computer to an experimental device that
measures random events. This alternative is not good for production work but may be a useful
check in times of doubt.
5.2.1 Random-Number Generation (Algorithm)
The linear congruent or power residue method is the common way of generating a pseudoran-
dom sequence of numbers 0 ≤ ri ≤ M − 1 over the interval [0,M − 1]. You multiply the
previous random number ri−1 by the constant a, add another constant c, take the modulus
by M , and then keep just the fractional part (remainder)1 as the next random number ri+1:
xmll ri+1 def= (a ri + c) modM = remainder
(
a ri + c
M
)
. (5.1)
The value for r1 (the seed) is frequently supplied by the user, and mod is a built-in function on
your computer for remaindering (it may be called amod or dmod). This is essentially a bit-shift
operation that ends up with the least significant part of the input number and thus counts on the
randomness of round-off errors to generate a random sequence.
As an example, if c = 1, a = 4,M = 9, and you supply r1 = 3, then you obtain the
sequence
r1 = 3, (5.2)
r2 = (4× 3 + 1)mod 9 = 13 mod 9 = rem
13
9
= 4, (5.3)
r3 = (4× 4 + 1)mod 9 = 17 mod 9 = rem
17
9
= 8, (5.4)
r4 = (4× 8 + 1)mod 9 = 33 mod 9 = rem
33
9
= 6, (5.5)
r5−10 = 7, 2, 0, 1, 5, 3. (5.6)
1You may obtain the same result for the modulus operation by subtracting M until any further subtractions would leave a
negative number; what remains is the remainder.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
MONTE CARLO SIMULATIONS (NONTHERMAL) 101
Figure 5.1 Left: A plot of successive random numbers (x, y) = (ri, ri + 1) generated with a deliberately “bad”
generator. Right: A plot generated with the library routine drand48.
0 50 100 150 200 250
x
0
50
100
150
200
250
y
0 50 100 150 200 250
x
We get a sequence of length M = 9, after which the entire sequence repeats. If we want
numbers in the range [0, 1], we divide the r’s by M = 9:
0.333, 0.444, 0.889, 0.667, 0.778, 0.222, 0.000, 0.111, 0.555, 0.333.
This is still a sequence of length 9 but is no longer a sequence of integers. If random numbers
in the range [A,B] are needed, you only need to scale:
xmllxi = A+ (B −A)ri, 0 ≤ ri ≤ 1, ⇒ A ≤ xi ≤ B. (5.7)
As a rule of thumb: Before using a random-number generator in your programs, you should
check its range and that it produces numbers that “look” random.
Although not a mathematical test, you should always make a graphical display of your
random numbers. Your visual cortex is quite refined at recognizing patterns and will tell you
immediately if there is one in your random numbers. For instance, Figure 5.1 shows generated
sequences from “good” and “bad” generators, and it is clear which is not random (although if
you look hard enough at the random points, your mind may well pick out patterns there too).
The linear congruent method (5.1) produces integers in the range [0,M−1] and therefore
becomes completely correlated if a particular integer comes up a second time (the whole cycle
then repeats). In order to obtain a longer sequence, a and M should be large numbers but
not so large that ari−1 overflows. On a computer using 48-bit integer arithmetic, the built-in
random-number generator may use M values as large as 248 ' 3 × 1014. A 32-bit generator
may use M = 231 ' 2×109. If your program uses approximately this many random numbers,
you may need to reseed the sequence during intermediate steps to avoid the cycle repeating.
Your computer probably has random-number generators that are better than the one you
will compute with the power residue method. You may check this out in the manual or the help
pages (try the man command in Unix) and then test the generated sequence. These routines
may have names like rand, rn, random, srand, erand, drand, or drand48.
We recommend a version of drand48 as a random-number generator. It generates random
numbers in the range [0, 1] with good spectral properties by using 48-bit integer arithmetic with
the parameters2
xmllM = 2
48, c=B (base 16) = 13 (base 8), (5.8)
a = 5DEECE66D (base 16) = 273673163155 (base 8). (5.9)
2Unless you know how to do 48-bit arithmetic and how to input numbers in different bases, it may be better to enter large
numbers like M = 112233 and a = 9999.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
102 CHAPTER 5
Figure 5.2 A plot of a uniform pseudorandom sequence ri versus i. The points are connected to make it easier to
follow the order.
0
0.2
0.4
0.6
0.8
1
0 20 40 60 80 100
R
a
n
d
o
m
N
u
m
b
e
r
r
Sequence Number
To initialize the random sequence, you need to plant a seed in it. In Fortran you call
the subroutine srand48 to plant your seed, while in Python the statement random.seed(None)
seeds the generator with the system time (see Walk.py in Listing 5.1 for details).
5.2.2 Implementation: Random Sequence
1. Write a simple program to generate random numbers using the linear congruent method
(5.1).
2. For pedagogical purposes, try the unwise choice: (a, c,M, r1) = (57, 1, 256, 10). Deter-
mine the period, that is, how many numbers are generated before the sequence repeats.
3. Take the pedagogical sequence of random numbers and look for correlations by observ-
ing clustering on a plot of successive pairs (xi, yi) = (r2i−1, r2i), i = 1, 2, . . .. (Do not
connect the points with lines.) You may “see” correlations (Figure 5.1), which means
that you should not use this sequence for serious work.
4. Make your own version of Figure 5.2; that is, plot ri versus i.
5. Test the built-in random-number generator on your computer for correlations by plotting
the same pairs as above. (This should be good for serious work.)
6. Test the linear congruent method again with reasonable constants like those in (5.8)
and (5.9). Compare the scatterplot you obtain with that of the built-in random-number
generator. (This, too, should be good for serious work.)
For scientific work we recommend using an industrial-strength random-number generator. To
see why, here we assess how bad a careless application of the power residue method can be.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
MONTE CARLO SIMULATIONS (NONTHERMAL) 103
Table 5.1 A table of a uniform, pseudo-random sequence ri generated by RandNum.py.
0.04689502438508175 0.20458779675039795 0.5571907470797255 0.05634336673593088
0.9360668645897467 0.7399399139194867 0.6504153029899553 0.8096333704183057
0.3251217462543319 0.49447037101884717 0.14307712613141128 0.32858127644188206
0.5351001685588616 0.9880354395691023 0.9518097953073953 0.36810077925659423
0.6572443815038911 0.7090768515455671 0.5636787474592884 0.3586277378006649
0.38336910654033807 0.7400223756022649 0.4162083381184535 0.3658031553038087
0.7484798900468111 0.522694331447043 0.14865628292663913 0.1741881539527136
0.41872631012020123 0.9410026890120488 0.1167044926271289 0.8759009012786472
0.5962535409033703 0.4382385414974941 0.166837081276193 0.27572940246034305
0.832243048236776 0.45757242791790875 0.7520281492540815 0.8861881031774513
0.04040867417284555 0.14690149294881334 0.2869627609844023 0.27915054491588953
0.7854419848382436 0.502978394047627 0.688866810791863 0.08510414855949322
0.48437643825285326 0.19479360033700366 0.3791230234714642 0.9867371389465821
5.2.3 Assessing Randomness and Uniformity
Because the computer’s random numbers are generated according to a definite rule, the num-
bers in the sequence must be correlated with each other. This can affect a simulation that
assumes random events. Therefore it is wise for you to test a random-number generator to
obtain a numerical measure of its uniformity and randomness before you stake your scientific
reputation on it. In fact, some tests are simple enough for you to make it a habit to run them
simultaneously with your simulation. In the examples to follow, we test for either randomness
or uniformity.
1. Probably the most obvious, but often neglected, test for randomness and uniformity is
to look at the numbers generated. For example, Table 5.1 presents some output from
RandNum.py. If you just look at these numbers, you will know immediately that they
all lie between 0 and 1, that they appear to differ from each other, and that there is no
obvious pattern (like 0.3333).
2. As we have seen, a quick visual test (Figure 5.2) involves taking this same list and plot-
ting it with ri as ordinate and i as abscissa. Observe how there appears to be a uniform
distribution between 0 and 1 and no particular correlation between points (although your
eye and brain will try to recognize some kind of pattern).
3. A simple test of uniformity evaluates the kth moment of a distribution:
xmll〈xk〉 = 1
N
N∑
i=1
xki . (5.10)
If the numbers are distributed uniformly, then (5.10) is approximately the moment of the
distribution function P (x):
1
N
N∑
i=1
xki '
∫ 1
0
dx xkP (x) ' 1
k + 1
+O
(
1√
N
)
. (5.11)
If (5.11) holds for your generator, then you know that the distribution is uniform. If
the deviation from (5.11) varies as 1/
√
N , then you also know that the distribution is
random.
4. Another simple test determines the near-neighbor correlation in your random sequence
by taking sums of products for small k:
xmllC(k) = 1
N
N∑
i=1
xi xi+k, (k = 1, 2, . . .). (5.12)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
104 CHAPTER 5
If your random numbers xi and xi+k are distributed with the joint probability distribution
P (xi, xi+k) and are independent and uniform, then (5.12) can be approximated as an
integral:
1
N
N∑
i=1
xi xi+k '
∫ 1
0
dx
∫ 1
0
dy xy P (x, y) =
1
4
. (5.13)
If (5.13) holds for your random numbers, then you know that they are uniform and in-
dependent. If the deviation from (5.13) varies as 1/
√
N , then you also know that the
distribution is random.
5. As we have seen, an effective test for randomness is performed by making a scatterplot
of (xi = r2i, yi = r2i+1) for many i values. If your points have noticeable regularity,
the sequence is not random. If the points are random, they should uniformly fill a square
with no discernible pattern (a cloud) (Figure 5.1).
6. Test your random-number generator with (5.11) for k = 1, 3, 7 and N =
100, 10, 000, 100, 000. In each case print out
√
N
∣∣∣∣∣ 1N
N∑
i=1
xki −
1
k + 1
∣∣∣∣∣ (5.14)
to check that it is of order 1.
5.3 UNIT II. MONTE CARLO APPLICATIONS
Now that we have an idea of how to use the computer to generate pseudorandom numbers,
we build some confidence that we can use these numbers to incorporate the element of chance
into a simulation. We do this first by simulating a random walk and then by simulating an atom
decaying spontaneously. After that, we show how knowing the statistics of random numbers
leads to the best way to evaluate multidimensional integrals.
5.4 A RANDOM WALK (PROBLEM)
Consider a perfume molecule released in the front of a classroom. It collides randomly with
other molecules in the air and eventually reaches your nose even though you are hidden in
the last row. The problem is to determine how many collisions, on the average, a perfume
molecule makes in traveling a distance R. You are given the fact that a molecule travels an
average (root-mean-square) distance rrms between collisions.
Listing 5.1 Walk.py calls the random-number generator from the random package. Note that a different seed is
needed for a different sequence. 
# Walk . py Random walk wi th g raph
from v i s u a l i m p o r t ∗
from v i s u a l . g raph i m p o r t ∗
i m p o r t random
random . seed ( None ) # Seed g e n e r a t o r , None => sys tem c l o c k
jmax = 1000
x = 0 . ; y = 0 . # S t a r t a t o r i g i n
graph1 = g d i s p l a y ( wid th = 500 , h e i g h t = 500 , t i t l e = ’Random Walk’ , x t i t l e = ’x’ ,
y t i t l e = ’y’ )
p t s = g c u r ve ( c o l o r = c o l o r . y e l l o w )
f o r i i n r a n g e ( 0 , jmax + 1) :
p t s . p l o t ( pos = ( x , y ) ) # P l o t p o i n t s
x += ( random . random ( ) − 0 . 5 ) ∗2 . # −1 =< x =< 1
y += ( random . random ( ) − 0 . 5 ) ∗2 . # −1 =< y =< 1
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
MONTE CARLO SIMULATIONS (NONTHERMAL) 105
p t s . p l o t ( pos = ( x , y ) )
r a t e ( 1 0 0 )
p r i n t "Walk Program Complete, \n Data stored in Walk.dat\n"
p r i n t "Distance from origin R = " , s q r t ( x∗x + y∗y )
5.4.1 Random-Walk Simulation
There are a number of ways to simulate a random walk with (surprise, surprise) different as-
sumptions yielding different physics. We will present the simplest approach for a 2-D walk,
with a minimum of theory, and end up with a model for normal diffusion. The research litera-
ture is full of discussions of various versions of this problem. For example, Brownian motion
corresponds to the limit in which the individual step lengths approach zero with no time delay
between steps. Additional refinements include collisions within a moving medium (abnormal
diffusion), including the velocities of the particles, or even pausing between steps. Models
such as these are discussed in Chapter 13, “Fractals & Statistical Growth,” and demonstrated
by some of the corresponding applets on the CD.
In our random-walk simulation (Figure 5.3) an artificial walker takes sequential steps
with the direction of each step independent of the direction of the previous step. For our model
we start at the origin and take N steps in the xy plane of lengths (not coordinates)
(∆x1,∆y1), (∆x2,∆y2), (∆x3,∆y3), . . . , (∆xN ,∆yN ). (5.15)
Even though each step may be in a different direction, the distances along each Cartesian
axis just add algebraically (aren’t vectors great?). Accordingly, the radial distance R from the
starting point after N steps is
R2 = (∆x1 + ∆x2 + · · ·+ ∆xN )2 + (∆y1 + ∆y2 + · · ·+ ∆yN )2
= ∆x21 + ∆x
2
2 + · · ·+ ∆x2N + 2∆x1∆x2 + 2∆x1∆x3 + 2∆x2∆x1 + · · ·
+ (x→ y). (5.16)
If the walk is random, the particle is equally likely to travel in any direction at each step. If
we take the average of a large number of such random steps, all the cross terms in (5.16) will
vanish and we will be left with
R2rms ' 〈∆x21 + ∆x22 + · · ·+ ∆x2N + ∆y21 + ∆y22 + · · ·+ ∆y2N 〉
= 〈∆x21 + ∆y21〉+ 〈∆x22 + ∆y22〉+ · · ·
= N〈r2〉 = Nr2rms,
⇒ Rrms '
√
Nrrms , (5.17)
where rrms =
√
〈r2〉 is the root-mean-square step size.
To summarize, if the walk is random, then we expect that after a large number of steps
the average vector distance from the origin will vanish:
〈~R〉 = 〈x〉~i+ 〈y〉~j ' 0. (5.18)
However, (5.17) indicates that the average scalar distance from the origin is
√
Nrrms, where
each step is of average length rrms. In other words, the vector endpoint will be distributed
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
106 CHAPTER 5
Figure 5.3 Some of the N steps in a random walk that end up a distance R from the origin. Notice how the ∆x’s for
each step add algebraically.
x1
y1
R
N
y2
v
v
v
Figure 5.4 Left: A simulation of a random walk. Right: The distance covered in two walks of N steps using different
schemes for including randomness. The theoretical prediction (5.17) is the straight line.
7 Random Walks
Distance vs Steps
0
100
200
300
0 100 200 300
sqrt(N)
R
uniformly in all quadrants, and so the displacement vector averages to zero, but the length
of that vector does not. For large N values,
√
Nrrms  Nrrms but does not vanish. In our
experience, practical simulations agree with this theory, but rarely perfectly, with the level of
agreement depending upon the details of how the averages are taken and how the randomness
is built into each step.
5.4.2 Implementation: Random Walk
The program Walk.py in Listing 5.1 is a sample random-walk simulation. It’s key element is
random values for the x and y components of each step, 
x += ( random . random ( ) − 0 . 5 ) ∗2 . # −1 =< x =< 1
y += ( random . random ( ) − 0 . 5 ) ∗2 . # −1 =< y =< 1
Here we omit the scaling factor that normalizes each step to length 1. When using your com-
puter to simulate a random walk, you should expect to obtain (5.17) only as the average dis-
placement after many trials, not necessarily as the answer for each trial. You may get different
answers depending on how you take your random steps (Figure 5.4 right).
Start at the origin and take a 2-D random walk with your computer.
1. To increase the amount of randomness, independently choose random values for ∆x′ and
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
MONTE CARLO SIMULATIONS (NONTHERMAL) 107
∆y′ in the range [−1, 1]. Then normalize them so that each step is of unit length
∆x =
1
L
∆x′, ∆y =
1
L
∆y′, L =
√
∆x′2 + ∆y′2.
2. Use a plotting program to draw maps of several independent random walks, each of 1000
steps. Comment on whether these look like what you would expect of a random walk.
3. If you have your walker taking N steps in a single trial, then conduct a total number
K '
√
N of trials. Each trial should have N steps and start with a different seed.
4. Calculate the mean square distance R2 for each trial and then take the average of R2 for
all your K trials:
〈R2(N) 〉 = 1
K
K∑
k=1
R2(k)(N).
5. Check the validity of the assumptions made in deriving the theoretical result (5.17) by
checking how well
〈∆xi∆xj 6=i〉
R2
' 〈∆xi∆yj〉
R2
' 0.
Do your checking for both a single (long) run and for the average over trials.
6. Plot the root-mean-square distance Rrms =
√
〈R2(N)〉 as a function of
√
N . Values of
N should start with a small number, where R '
√
N is not expected to be accurate, and
end at a quite large value, where two or three places of accuracy should be expected on
the average.
5.5 RADIOACTIVE DECAY (PROBLEM)
Your problem is to simulate how a small number N of radioactive particles decay.3 In par-
ticular, you are to determine when radioactive decay looks like exponential decay and when it
looks stochastic (containing elements of chance). Because the exponential decay law is
a large-number approximation to a natural process that always ends with small numbers, our CD
simulation should be closer to nature than is the exponential decay law (Figure 5.5). In fact,
if you go to the CD and “listen” to the output of the decay simulation code, what you will
hear sounds very much like a Geiger counter, a convincing demonstration of the realism of the
simulation.
Spontaneous decay is a natural process in which a particle, with no external stimulation,
decays into other particles. Even though the probability of decay of any one particle in any time
interval is constant, just when it decays is a random event. Because the exact moment when
any one particle decays is random, it does not matter how long the particle has been around
or whether some other particles have decayed. In other words, the probability P of any one
particle decaying per unit time interval is a constant, and when that particle decays, it is gone
forever. Of course, as the total number of particles decreases with time, so will the number
of decays, but the probability of any one particle decaying in some time interval is always the
same constant as long as that particle exists.
5.5.1 Discrete Decay (Model)
Imagine having a sample of N(t) radioactive nuclei at time t (Figure 5.5 inset). Let ∆N be the
number of particles that decay in some small time interval ∆t. We convert the statement “the
3Spontaneous decay is also discussed in Chapter 8, “Solving Systems of Equations with Matrices; Data Fitting,” where we fit
an exponential function to a decay spectrum.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
108 CHAPTER 5
Figure 5.5 A sample containing N nuclei, each one of which has the same probability of decaying per unit time
(circle). Semilog plots of the results from several decay simulations. Notice how the decay appears
exponential (like a straight line) when the number of nuclei is large, but stochastic for log N ≤ 2.0.
0 400 800 1200
t
0
2
4
100,00010,000
1,000
100
10
log[N(t)]
probability P of any one particle decaying per unit time is a constant” into the equation
P = ∆N(t)/N(t)
∆t
= −λ, (5.19)
⇒ ∆N(t)
∆t
= −λN(t), (5.20)
where the constant λ is called the decay rate. Of course the real decay rate or activity is
∆N(t)/∆t, which varies with time. In fact, because the total activity is proportional to the
total number of particles still present, it too is stochastic with an exponential-like decay in
time. [Actually, because the number of decays ∆N(t) is proportional to the difference in
random numbers, its stochastic nature becomes evident before that of N(t).]
Equation (5.20) is a finite-difference equation in terms of the experimental measurables
N(t), ∆N(t), and ∆t. Although it cannot be integrated the way a differential equation can, it
can be solved numerically when we include the fact that the decay process is random. Because
the process is random, we cannot predict a single value for ∆N(t), although we can predict
the average number of decays when observations are made of many identical systems of N
decaying particles.
5.5.2 Continuous Decay (Model)
When the number of particles N →∞ and the observation time interval ∆t→ 0, an approxi-
mate form of the radioactive decay law (5.20) results:
∆N(t)
∆t
−→ dN(t)
dt
= −λN(t). (5.21)
This can be integrated to obtain the time dependence of the total number of particles and of the
total activity:
xmll N(t) =N(0)e
−λt = N(0)e−t/τ , (5.22)
dN(t)
dt
=−λN(0)e−λt = dN
dt
e−λt(0). (5.23)
We see that in this limit we obtain exponential decay, which leads to the identification of the
decay rate λ with the inverse lifetime:
λ =
1
τ
. (5.24)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
MONTE CARLO SIMULATIONS (NONTHERMAL) 109
So we see from its derivation that exponential decay is a good description of nature for a large
number of particles where ∆N/N ' 0. The basic law of nature (5.19) is always valid, but as
we will see in the simulation, exponential decay (5.23) becomes less and less accurate as the
number of particles becomes smaller and smaller.
5.5.3 Decay Simulation
A program for simulating radioactive decay is surprisingly simple but not without its subtleties.
We increase time in discrete steps of ∆t, and for each time interval we count the number of
nuclei that have decayed during that ∆t. The simulation quits when there are no nuclei left to
decay. Such being the case, we have an outer loop over the time steps ∆t and an inner loop
over the remaining nuclei for each time step. The pseudocode is simple: 
i n p u t N, lambda
t =0
w h i l e N > 0
DeltaN = 0
f o r i = 1 . . N
i f ( r i < lambda ) Del taN = DeltaN + 1
end f o r
t = t +1
N = N − DeltaN
Outpu t t , DeltaN , N
end w h i l e
When we pick a value for the decay rate λ = 1/τ to use in our simulation, we are setting
the scale for times. If the actual decay rate is λ = 0.3 × 106 s−1 and if we decide to measure
times in units of 10−6 s, then we will choose random numbers 0 ≤ ri ≤ 1, which leads to λ
values lying someplace near the middle of the range (e.g., λ ' 0.3). Alternatively, we can use
a value of λ = 0.3× 106 s−1 in our simulation and then scale the random numbers to the range
0 ≤ ri ≤ 106. However, unless you plan to compare your simulation to experimental data, you
do not have to worry about the scale for time but instead should focus on the physics behind
the slopes and relative magnitudes of the graphs.
Listing 5.2 Decay.py simulates spontaneous decay in which a decay occurs if a random number is smaller than the
decay parameter. 
# Decay . py s p o n t a n e o u s decay s i m u l a t i o n
from v i s u a l i m p o r t ∗
from v i s u a l . g raph i m p o r t ∗
i m p o r t random
lambda1 = 0 . 0 1 # Decay c o n s t a n t
max = 1 0 0 0 . ; t ime max = 500 ; seed = 68111 # Params
number = n loop = max # I n i t i a l v a l u e
# random . seed ( seed ) # Seed number g e n e r a t o r
g raph1 = g d i s p l a y ( wid th =500 , h e i g h t =500 , t i t l e =’Spontaneous Decay’ , x t i t l e =’Time’ ,
y t i t l e = ’Number left’ , xmax =500 , xmin =0 , ymax =1000 , ymin =0)
d e c a y f u n c = gc u r v e ( c o l o r = c o l o r . g r e e n )
f o r t ime i n a r a n g e ( 0 , t ime max + 1) : # Time loop
f o r atom i n a r a n g e ( 1 , number + 1 ) : # Decay loop
decay = random . random ( )
i f ( decay < lambda1 ) :
n loop = n loop − 1 # A decay
number = n loop
d e c a y f u n c . p l o t ( pos = ( t ime , number ) )
r a t e ( 3 0 )
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
110 CHAPTER 5
5.6 DECAY IMPLEMENTATION AND VISUALIZATION
Write a program to simulate radioactive decay using the simple program in Listing 5.2 as a
guide. You should obtain results like those in Figure 5.5.
1. Plot the logarithm of the number left lnN(t) and the logarithm of the decay rate
ln ∆N(t) versus time. Note that the simulation measures time in steps of ∆t (gener-
ation number).
2. Check that you obtain what looks like exponential decay when you start with large values
for N(0), but that the decay displays its stochastic nature for small N(0) [large N(0)
values are also stochastic; they just don’t look like it].
3. Create two plots, one showing that the slopes of N(t) versus t are independent of N(0)
and another showing that the slopes are proportional to λ.
4. Create a plot showing that within the expected statistical variations, lnN(t) and
ln ∆N(t) are proportional.
5. Explain in your own words how a process that is spontaneous and random at its very
heart can lead to exponential decay.
6. How does your simulation show that the decay is exponential-like and not a power law
such as N = βt−α?
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Six
Integration
In this chapter we discuss numerical integration, a basic tool of scientific computation. We
derive the Simpson and trapezoid rules but just sketch the basis of Gaussian quadrature, which,
though our standard workhorse, is long in derivation. We do discuss Gaussian quadrature in its
various forms and indicate how to transform the Gauss points to a wide range of intervals. We
end the chapter with a discussion of Monte Carlo integration, which is fundamentally different
from other integration techniques.
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
Numerical Integration 6.1–6.3 -
Applets
Name Sections Name Sections
Area with MC 6.5 -
6.1 INTEGRATING A SPECTRUM (PROBLEM)
Problem: An experiment has measured dN(t)/dt, the number of particles per unit time
entering a counter. Your problem is to integrate this spectrum to obtain the number of particles
N(1) that entered the counter in the first second for an arbitrary decay rate
N(1) =
∫ 1
0
dN(t)
dt
dt. (6.1)
6.2 QUADRATURE AS BOX COUNTING (MATH)
The integration of a function may require some cleverness to do analytically but is relatively
straightforward on a computer. A traditional way to perform numerical integration by hand is to
take a piece of graph paper and count the number of boxes or quadrilaterals lying below a curve
of the integrand. For this reason numerical integration is also called numerical quadrature even
when it becomes more sophisticated than simple box counting.
The Riemann definition of an integral is the limit of the sum over boxes as the width h
of the box approaches zero (Figure 6.1):
xmll∫ b
a
f(x) dx = lim
h→0
h (b−a)/h∑
i=1
f(xi)
 . (6.2)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
112 CHAPTER 6
Figure 6.1 The integral
∫ b
a f(x) dx is the area under the graph of f(x) from a to b. Here we break up the area into four
regions of equal widths h.
a x
i xi+1 xi+2 b
x
f(x)
The numerical integral of a function f(x) is approximated as the equivalent of a finite sum over
boxes of height f(x) and width wi:∫ b
a
f(x) dx '
N∑
i=1
f(xi)wi, (6.3)
which is similar to the Riemann definition (6.2) except that there is no limit to an infinitesimal
box size. Equation (6.3) is the standard form for all integration algorithms; the function f(x)
is evaluated at N points in the interval [a, b], and the function values fi ≡ f(xi) are summed
with each term in the sum weighted by wi. While in general the sum in (6.3) gives the exact
integral only when N →∞, it may be exact for finite N if the integrand is a polynomial. The
different integration algorithms amount to different ways of choosing the points and weights.
Generally, the precision increases as N gets larger, with round-off error eventually limiting the
increase. Because the “best” approximation depends on the specific behavior of f(x), there is
no universally best approximation. In fact, some of the automated integration schemes found
in subroutine libraries switch from one method to another and change the methods for different
intervals until they find ones that work well for each interval.
In general you should not attempt a numerical integration of an integrand that contains
a singularity without first removing the singularity by hand.1 You may be able to do this
very simply by breaking the interval down into several subintervals so the singularity is at an
endpoint where an integration point is not placed or by a change of variable:∫ 1
−1
|x|f(x) dx=
∫ 0
−1
f(−x) dx+
∫ 1
0
f(x) dx, (6.4)
∫ 1
0
x1/3 dx=
∫ 1
0
3y3 dy, (y = x1/3), (6.5)
∫ 1
0
f(x) dx√
1− x2
= 2
∫ 1
0
f(1− y2) dy√
2− y2
, (y2 = 1− x). (6.6)
Likewise, if your integrand has a very slow variation in some region, you can speed up the
integration by changing to a variable that compresses that region and places few points there.
Conversely, if your integrand has a very rapid variation in some region, you may want to change
to variables that expand that region to ensure that no oscillations are missed.
1In Chapter 20, “Integral Equations in Quantum Mechanics,” we show how to remove such a singularity even when the inte-
grand is unknown.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
INTEGRATION 113
Elementary Weights for Uniform-Step Integration Rules
Name Degree Elementary Weights
Trapezoid 1 (1, 1)h
2
Simpson’s 2 (1, 4, 1)h
3
3
8 3 (1, 3, 3, 1)
3
8h
Milne 4 (14, 64, 24, 64, 14) h
45
Listing 6.1 TrapMethods.py integrates the arbitrary function f(y) via the trapezoid rule. Note how the step size h
depends on the interval and how the weights at the ends and middle differ. 
# TrapMethods T r a p e z o i d i n t e g r a t i o n u s i n g a d e f i n e d f u n c t i o n = t ˆ2
from v i s u a l i m p o r t ∗
A = 0 . 0
B = 3 . 0
N = 4
d e f f ( y ) : # The f u n c t i o n b e i n g i n t e g r a t e d
p r i n t ’\n y = ’ , y
p r i n t ’ f(y) = ’ , y∗y
r e t u r n y∗y
d e f wTrap ( i , h ) : # F u n c t i o n d e t e r m i n e s w e i gh t
i f ( ( i == 1) o r ( i == N) ) :
wLocal = h / 2 . 0
e l s e :
wLocal = h
r e t u r n wLocal
h = (B − A) / ( N − 1)
suma = 0 . 0
f o r i i n r a n g e ( 1 , N + 1) :
t = A + ( i − 1)∗h
w = wTrap ( i , h )
suma = suma + w ∗ f ( t )
p r i n t ’\n Final sum = ’ , suma
p r i n t "Press a character to finish"
s= r a w i n p u t ( )
6.2.1 Algorithm: Trapezoid Rule
The trapezoid and Simpson integration rules use values of f(x) at evenly spaced values of x.
They use N points xi(i = 1, N) evenly spaced at a distance h apart throughout the integration
region [a, b] and include the endpoints. This means that there are (N −1) intervals of length h:
h =
b− a
N − 1
, xi = a+ (i− 1)h, i = 1, N, (6.7)
where we start our counting at i = 1. The trapezoid rule takes each integration interval i and
constructs a trapezoid of width h in it (Figure 6.2). This approximates f(x) by a straight line
in each interval i and uses the average height (fi+fi+1)/2 as the value for f . The area of each
such trapezoid is ∫ xi+h
xi
f(x) dx ' h(fi + fi+1)
2
=
1
2
hfi +
1
2
hfi+1. (6.8)
In terms of our standard integration formula (6.3), the “rule” in (6.8) is for N = 2 points with
weights wi ≡ 12 (Table 6.2.1).
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
114 CHAPTER 6
Figure 6.2 Right: Straight-line sections used for the trapezoid rule. Left: Two parabolas used in Simpson’s rule.
a
x
b
f(x)
tr
a
p
 1
tr
a
p
 2
tr
a
p
 3
tr
a
p
 4
a
x
b
f(x)
parabola 1
parabola 2
In order to apply the trapezoid rule to the entire region [a, b], we add the contributions
from each subinterval:∫ b
a
f(x) dx ' h
2
f1 + hf2 + hf3 + · · ·+ hfN−1 +
h
2
fN . (6.9)
You will notice that because the internal points are counted twice (as the end of one interval
and as the beginning of the next), they have weights of h/2 + h/2 = h, whereas the endpoints
are counted just once and on that account have weights of only h/2. In terms of our standard
integration rule (6.32), we have
xmll
wi =
{
h
2
, h, . . . , h,
h
2
}
(trapezoid rule). (6.10)
In Listing 6.1 we provide a simple implementation of the trapezoid rule.
6.2.2 Algorithm: Simpson’s Rule
For each interval, Simpson’s rule approximates the integrand f(x) by a parabola (Figure 6.2
right):
f(x) ' αx2 + βx+ γ, (6.11)
with all intervals equally spaced. The area under the parabola for each interval is∫ xi+h
xi
(αx2 + βx+ γ) dx =
αx3
3
+
βx2
2
+ γx
∣∣∣∣xi+h
xi
. (6.12)
In order to relate the parameters α, β, and γ to the function, we consider an interval from −1
to +1, in which case ∫ 1
−1
(αx2 + βx+ γ) dx =
2α
3
+ 2γ. (6.13)
But we notice that
f(−1) = α− β + γ, f(0) = γ, f(1) = α+ β + γ,
⇒ α = f(1) + f(−1)
2
− f(0), β = f(1)− f(−1)
2
, γ = f(0).
(6.14)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
INTEGRATION 115
In this way we can express the integral as the weighted sum over the values of the function at
three points: ∫ 1
−1
(αx2 + βx+ γ) dx =
f(−1)
3
+
4f(0)
3
+
f(1)
3
. (6.15)
Because three values of the function are needed, we generalize this result to our problem by
evaluating the integral over two adjacent intervals, in which case we evaluate the function at
the two endpoints and in the middle (Table 6.2.1):∫ xi+h
xi−h
f(x) dx=
∫ xi+h
xi
f(x) dx+
∫ xi
xi−h
f(x) dx
' h
3
fi−1 +
4h
3
fi +
h
3
fi+1. (6.16)
Simpson’s rule requires the elementary integration to be over pairs of intervals, which in turn
requires that the total number of intervals be even or that the number of points N be odd. In
order to apply Simpson’s rule to the entire interval, we add up the contributions from each pair
of subintervals, counting all but the first and last endpoints twice:∫ b
a
f(x)dx ' h
3
f1 +
4h
3
f2 +
2h
3
f3 +
4h
3
f4 + · · ·+
4h
3
fN−1 +
h
3
fN . (6.17)
In terms of our standard integration rule (6.3), we have
xmll
wi =
{
h
3
,
4h
3
,
2h
3
,
4h
3
, . . . ,
4h
3
,
h
3
}
(Simpson’s rule). (6.18)
The sum of these weights provides a useful check on your integration:
N∑
i=1
wi = (N − 1)h. (6.19)
Remember, the number of points N must be odd for Simpson’s rule.
6.2.3 Integration Error (Analytic Assessment)
In general, you should choose an integration rule that gives an accurate answer using the least
number of integration points. We obtain a crude estimate of the approximation or algorithmic
error E and the relative error  by expanding f(x) in a Taylor series around the midpoint of the
integration interval. We then multiply that error by the number of intervals N to estimate the
error for the entire region [a, b]. For the trapezoid and Simpson rules this yields
Et = O
(
[b− a]3
N2
)
f (2), Es = O
(
[b− a]5
N4
)
f (4), t,s =
Et,s
f
. (6.20)
We see that the third-derivative term in Simpson’s rule cancels (much like the central-difference
method does in differentiation). Equations (6.20) are illuminating in showing how increasing
the sophistication of an integration rule leads to an error that decreases with a higher inverse
power of N yet is also proportional to higher derivatives of f . Consequently, for small inter-
vals and functions f(x) with well-behaved derivatives, Simpson’s rule should converge more
rapidly than the trapezoid rule.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
116 CHAPTER 6
To model the error in integration, we assume that after N steps the relative round-off
error is random and of the form
ro '
√
Nm, (6.21)
where m is the machine precision,  ∼ 10−7 for single precision and  ∼ 10−15 for double
precision (the standard for science). Because most scientific computations are done with dou-
bles, we will assume double precision. We want to determine an N that minimizes the total
error, that is, the sum of the approximation and round-off errors:
tot ' ro + approx. (6.22)
This occurs, approximately, when the two errors are of equal magnitude, which we approximate
even further by assuming that the two errors are equal:
ro = approx =
Etrap,simp
f
. (6.23)
To continue the search for optimum N for a general function f , we set the scale of function
size and the lengths by assuming
f (n)
f
' 1, b− a = 1 ⇒ h = 1
N
. (6.24)
The estimate (6.23), when applied to the trapezoid rule, yields
√
Nm'
f (2)(b− a)3
fN2
=
1
N2
, (6.25)
⇒ N ' 1
(m)2/5
=
(
1
10−15
)2/5
= 106, (6.26)
⇒ ro'
√
Nm = 10−12. (6.27)
The estimate (6.23), when applied to Simpson’s rule, yields
√
Nm =
f (4)(b− a)5
fN4
=
1
N4
, (6.28)
⇒ N = 1
(m)2/9
=
(
1
10−15
)2/9
= 2154, (6.29)
⇒ ro'
√
Nm = 5× 10−14. (6.30)
These results are illuminating in that they show how
• Simpson’s rule is an improvement over the trapezoid rule.
• It is possible to obtain an error close to machine precision with Simpson’s rule (and with
other higher-order integration algorithms).
• Obtaining the best numerical approximation to an integral is not achieved by letting N →
∞ but with a relatively small N ≤ 1000.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
INTEGRATION 117
6.2.4 Algorithm: Gaussian Quadrature
It is often useful to rewrite the basic integration formula (6.3) such that we separate a weighting
function W (x) from the integrand:∫ b
a
f(x) dx ≡
∫ b
a
W (x)g(x) dx '
N∑
i=1
wig(xi). (6.31)
In the Gaussian quadrature approach to integration, the N points and weights are chosen to
make the approximation error vanish if g(x) were a (2N − 1)-degree polynomial. To obtain
this incredible optimization, the points xi end up having a specific distribution over [a, b]. In
general, if g(x) is smooth or can be made smooth by factoring out some W (x) (Table 6.2.4),
Gaussian algorithms will produce higher accuracy than the trapezoid and Simpson rules for the
same number of points. Sometimes the integrand may not be smooth because it has different
behaviors in different regions. In these cases it makes sense to integrate each region separately
and then add the answers together. In fact, some “smart” integration subroutines decide for
themselves how many intervals to use and what rule to use in each.
All the rules indicated in Table 6.2.4 are Gaussian with the general form (6.31). We
can see that in one case the weighting function is an exponential, in another a Gaussian, and
in several an integrable singularity. In contrast to the equally spaced rules, there is never an
integration point at the extremes of the intervals, yet the values of the points and weights change
as the number of points N changes. Although we will leave the derivation of the Gaussian
points and weights to the references on numerical methods, we note here that for ordinary
Gaussian (Gauss–Legendre) integration, the points yi turn out to be theN zeros of the Legendre
polynomials, with the weights related to the derivatives, PN (yi) = 0, and wi = 2/([(1 −
y2i )[P
′
N (yi)]
2]. Subroutines to generate these points and weights are standard in mathematical
function libraries, are found in tables such as those in [A&S 72], or can be computed. The
gauss subroutines we provide on the CD also scale the points to span specified regions. As
a check that your points are correct, you may want to compare them to the four-point set in
Table 6.1.
6.2.4.1 Mapping Integration Points
Our standard convention (6.3) for the general interval [a, b] is∫ b
a
f(x) dx '
N∑
i=1
f(xi)wi. (6.32)
With Gaussian points and weights, the y interval −1 < yi ≤ 1 must be mapped onto the x
interval a ≤ x ≤ b. Here are some mappings we have found useful in our work. In all cases
Types of Gaussian Integration Rules
Integral Name Integral Name∫ 1
−1 f(y) dy Gauss
∫ 1
−1
F (y)√
1−y2 dy Gauss–Chebyshev∫∞
−∞ e
−y2F (y) dy Gauss–Hermite
∫∞
0 e
−yF (y) dy Gauss–Laguerre∫∞
0
e−y√
y F (y) dy Associated Gauss–Laguerre
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
118 CHAPTER 6
Table 6.1 Points and Weights for four-point Gaussian Quadrature
±yi wi
0.33998 10435 84856 0.65214 51548 62546
0.86113 63115 94053 0.34785 48451 37454
(yi, w
′
i) are the elementary Gaussian points and weights for the interval [−1, 1], and we want
to scale to x with various ranges.
1. [−1, 1]→ [a, b] uniformly, (a+ b)/2 = midpoint:
xmll xi =
b+ a
2
+
b− a
2
yi, wi =
b− a
2
w
′
i, (6.33)
⇒
∫ b
a
f(x) dx=
b− a
2
∫ 1
−1
f [x(y)] dy. (6.34)
2. [0→∞], a = midpoint:
xi = a
1 + yi
1− yi
, wi =
2a
(1− yi)2
w
′
i. (6.35)
3. [−∞→∞], scale set by a:
xi = a
yi
1− y2i
, wi =
a(1 + y2i )
(1− y2i )2
w
′
i. (6.36)
4. [b→∞], a+ 2b = midpoint:
xi =
a+ 2b+ ayi
1− yi
, wi =
2(b+ a)
(1− yi)2
w
′
i. (6.37)
5. [0→ b], ab/(b+ a) = midpoint:
xi =
ab(1 + yi)
b+ a− (b− a)yi
, wi =
2ab2
(b+ a− (b− a)yi)2
w
′
i. (6.38)
As you can see, even if your integration range extends out to infinity, there will be points
at large but not infinite x. As you keep increasing the number of grid points N , the last
xi gets larger but always remains finite.
6.2.5 Implementation and Error Assessment
1. Write a double-precision program to integrate an arbitrary function numerically using the
trapezoid rule, the Simpson rule, and Gaussian quadrature. For our assumed problem
there is an analytic answer:
dN(t)
dt
= e−t ⇒ N(1) =
∫ 1
0
e−t dt = 1− e−1.
2. Compute the relative error  = |(numerical-exact)/exact| in each case. Present your data
in the tabular form
N T S G
2 · · · · · · · · ·
10 · · · · · · · · ·
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
INTEGRATION 119
Figure 6.3 Log-log plot of the error in the integration of exponential decay using the trapezoid rule, Simpson’s rule,
and Gaussian quadrature versus the number of integration points N. Approximately 15 decimal places
of precision are attainable with double precision (left), and 7 places with single precision (right).
10 100
10
-13
10
-9
10
-5
N
|e
rr
o
r|
trapezoid
Simpson
Gaussian
10 100
N
10
-9
10
-7
10
-5
10
-3
10
-1
|e
rr
o
r|
trapezoid
Simpson
Gaussian
with spaces or tabs separating the fields. Try N values of 2, 10, 20, 40, 80, 160, . . . .
(Hint: Even numbers may not be the assumption of every rule.)
3. Make a log-log plot of relative error versus N (Figure 6.3). You should observe that
 ' CNα ⇒ log  = α logN + constant.
This means that a power-law dependence appears as a straight line on a log-log plot, and
that if you use log10, then the ordinate on your log-log plot will be the negative of the
number of decimal places of precision in your calculation.
4. Use your plot or table to estimate the power-law dependence of the error  on the number
of points N and to determine the number of decimal places of precision in your calcula-
tion. Do this for both the trapezoid and Simpson rules and for both the algorithmic and
round-off error regimes. (Note that it may be hard to reach the round-off error regime
for the trapezoid rule because the approximation error is so large.)
In Listing 6.2 we give a sample program that performs an integration with Gaussian points.
The method gauss generates the points and weights and may be useful in other applications as
well.
Listing 6.2 IntegGauss.py integrates the function f(x) via Gaussian quadrature. The points and weights are gener-
ated in the method gauss, which remains fixed for all applications. Note that the parameter eps, which
controls the level of precision desired, should be set by the user, as should the value for job, which con-
trols the mapping of the Gaussian points onto arbitrary intervals (they are generated for −1 ≤ x ≤ 1). 
# I n t e g G a u s s . py G a u s s i a n q u a d r a t u r e g e n e r a t e s own p t s and wts
from v i s u a l i m p o r t ∗
from v i s u a l . g raph i m p o r t ∗
max in = 101 # Numb i n t e r v a l s
vmin = 0 . ; vmax = 1 . # I n t r a n g e s
ME = 2.7182818284590452354 E0 # E u l e r’s const
w = zeros( (2001), Float)
x = zeros( (2001), Float)
def f(x):
return (exp( - x) ) # f(x)
def gauss(npts, job, a, b, x, w):
m = 0; i = 0; j = 0
t = 0.; t1 = 0.; pp = 0.; p1 = 0.; p2 = 0.; p3 = 0.
eps = 3.E-14 # Accuracy: ********ADJUST THIS*********!
m = (npts + 1)/2
for i in xrange(1, m + 1):
t = cos(math.pi*(float(i) - 0.25)/(float(npts) + 0.5) )
t1 = 1
while( (abs(t - t1) ) >= eps):
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
120 CHAPTER 6
p1 = 1. ; p2 = 0.
for j in xrange(1, npts + 1):
p3 = p2; p2 = p1
p1 = ( (2.*float(j) - 1)*t*p2 - (float(j) - 1.)*p3)/(float(j) )
pp = npts*(t*p1 - p2)/(t*t - 1.)
t1 = t; t = t1 - p1/pp
x[i - 1] = - t; x[npts - i] = t
w[i - 1] = 2./( (1. - t*t)*pp*pp)
w[npts - i] = w[i - 1]
# print" x[i - 1]", x[i - 1] , " w " , w[npts - i]
if (job == 0):
for i in xrange(0, npts):
x[i] = x[i]*(b - a)/2. + (b + a)/2.
w[i] = w[i]*(b - a)/2.
if (job == 1):
for i in xrange(0, npts):
xi = x[i]
x[i] = a*b*(1. + xi) / (b + a - (b - a)*xi)
w[i] = w[i]*2.*a*b*b/( (b + a - (b - a)*xi)*(b + a - (b - a)*xi) )
if (job == 2):
for i in xrange(0, npts):
xi = x[i]
x[i] = (b*xi + b + a + a) / (1. - xi)
w[i] = w[i]*2.*(a + b)/( (1. - xi)*(1. - xi) )
def gaussint (no, min, max):
quadra = 0.
gauss (no, 0, min, max, x, w) # Returns pts & wts
for n in xrange(0, no):
quadra += f(x[n]) * w[n] # Calculate integral
return (quadra)
for i in xrange(3, max_in + 1, 2):
result = gaussint(i, vmin, vmax)
print" ", i, " ", abs(result - 1 + 1/ME)
print "Press a character to finish"
s=raw_input()
6.3 EXPERIMENTATION
Try two integrals for which the answers are less obvious:
F1 =
∫ 2π
0
sin(100x) dx, F2 =
∫ 2π
0
sinx(100x) dx. (6.39)
Explain why the computer may have trouble with these integrals.
6.4 HIGHER-ORDER RULES (ALGORITHM)
As in numerical differentiation, we can use the known functional dependence of the error on
interval size h to reduce the integration error. For simple rules like the trapezoid and Simpson
rules, we have the analytic estimates (6.23), while for others you may have to experiment to
determine the h dependence. To illustrate, ifA(h) andA(h/2) are the values of the integral de-
termined for intervals h and h/2, respectively, and we know that the integrals have expansions
with a leading error term proportional to h2,
A(h)'
∫ b
a
f(x) dx+ αh2 + βh4 + · · · , (6.40)
A
(
h
2
)
'
∫ b
a
f(x) dx+
αh2
4
+
βh4
16
+ · · · . (6.41)
Consequently, we make the h2 term vanish by computing the combination
4
3
A
(
h
2
)
− 1
3
A(h) '
∫ b
a
f(x) dx− βh
4
4
+ · · · . (6.42)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
INTEGRATION 121
Clearly this particular trick (Romberg’s extrapolation) works only if the h2 term dominates
the error and then only if the derivatives of the function are well behaved. An analogous
extrapolation can also be made for other algorithms.
In Table 6.2.1 we gave the weights for several equal-interval rules. Whereas the Simpson
rule used two intervals, the three-eighths rule uses three, and the Milne2 rule four. (These
are single-interval rules and must be strung together to obtain a rule extended over the entire
integration range. This means that the points that end one interval and begin the next are
weighted twice.) You can easily determine the number of elementary intervals integrated over,
and check whether you and we have written the weights right, by summing the weights for
any rule. The sum is the integral of f(x) = 1 and must equal h times the number of intervals
(which in turn equals b− a):
N∑
i=1
wi = h×Nintervals = b− a. (6.43)
6.5 PROBLEM: MONTE CARLO INTEGRATION BY STONE THROWING
Imagine yourself as a farmer walking to your furthermost field to add algae-eating fish to a
pond having an algae explosion. You get there only to read the instructions and discover that
you need to know the area of the pond in order to determine the correct number of the fish to
add. Your problem is to measure the area of this irregularly shaped pond with just the materials
at hand [G,T&C 06].
It is hard to believe that Monte Carlo techniques can be used to evaluate integrals. After
all, we do not want to gamble on the values! While it is true that other methods are preferable
for single and double integrals, Monte Carlo techniques are best when the dimensionality of
integrations gets large! For our pond problem, we will use a sampling technique (Figure 6.4):
1. Walk off a box that completely encloses the pond and remove any pebbles lying on the
ground within the box.
2. Measure the lengths of the sides in natural units like feet. This tells you the area of the
enclosing box Abox.
3. Grab a bunch of pebbles, count their number, and then throw them up in the air in random
directions.
4. Count the number of splashes in the pond Npond and the number of pebbles lying on the
ground within your box Nbox.
5. Assuming that you threw the pebbles uniformly and randomly, the number of pebbles
falling into the pond should be proportional to the area of the pond Apond. You determine
that area from the simple ratio
Npond
Npond +Nbox
=
Apond
Abox
⇒ Apond =
Npond
Npond +Nbox
Abox. (6.44)
6.5.1 Stone Throwing Implementation
Use sampling (Figure 6.4) to perform a 2-D integration and thereby determine π:
2There is, not coincidentally, a Milne Computer Center at Oregon State University, although there no longer is a central
computer there.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
122 CHAPTER 6
Figure 6.4 Throwing stones in a pond as a technique for measuring its area. There is a tutorial on this on the CD
where you can see the actual “splashes” (the dark spots) used in an integration.
Pond
1. Imagine a circular pond enclosed in a square of side 2 (r = 1).
2. We know the analytic area of a circle
∮
dA = π.
3. Generate a sequence of random numbers −1 ≤ ri ≤ +1.
4. For i = 1 to N , pick (xi, yi) = (r2i−1, r2i).
5. If x2i + y
2
i < 1, let Npond = Npond + 1; otherwise let Nbox = Nbox + 1.
6. Use (6.44) to calculate the area, and in this way π.
7. Increase N until you get π to three significant figures (we don’t ask much — that’s only
slide-rule accuracy).
6.5.2 Integration by Mean Value (Math)
The standard Monte Carlo technique for integration is based on the mean value theorem (pre-
sumably familiar from elementary calculus):
I =
∫ b
a
dx f(x) = (b− a)〈f〉. (6.45)
The theorem states the obvious if you think of integrals as areas: The value of the integral of
some function f(x) between a and b equals the length of the interval (b − a) times the mean
value of the function over that interval 〈f〉 (Figure 6.5). The integration algorithm uses Monte
Carlo techniques to evaluate the mean in (6.45). With a sequence a ≤ xi ≤ b of N uniform
random numbers, we want to determine the sample mean by sampling the function f(x) at
these points:
〈f〉 ' 1
N
N∑
i=1
f(xi). (6.46)
This gives us the very simple integration rule:
xmll ∫ b
a
dx f(x) ' (b− a) 1
N
N∑
i=1
f(xi) = (b− a)〈f〉. (6.47)
Equation (6.47) looks much like our standard algorithm for integration (6.3) with the “points”
xi chosen randomly and with uniform weights wi = (b− a)/N . Because no attempt has been
made to obtain the best answer for a given value of N , this is by no means an optimized way
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
INTEGRATION 123
Figure 6.5 The area under the curve f(x) is the same as that under the dashed line y = 〈f 〉.
< f(x) >
f(x)
x
to evaluate integrals; but you will admit it is simple. If we let the number of samples of f(x)
approach infinity N → ∞ or if we keep the number of samples finite and take the average
of infinitely many runs, the laws of statistics assure us that (6.47) will approach the correct
answer, at least if there were no round-off errors.
For readers who are familiar with statistics, we remind you that the uncertainty in the
value obtained for the integral I after N samples of f(x) is measured by the standard devi-
ation σI . If σf is the standard deviation of the integrand f in the sampling, then for normal
distributions we have
σI '
1√
N
σf . (6.48)
So for large N , the error in the value obtained for the integral decreases as 1/
√
N .
6.6 HIGH-DIMENSIONAL INTEGRATION (PROBLEM)
Let’s say that we want to calculate some properties of a small atom such as magnesium with
12 electrons. To do that we need to integrate atomic wave functions over the three coordinates
of each of 12 electrons. This amounts to a 3× 12 = 36-D integral. If we use 64 points for each
integration, this requires about 6436 ' 1065 evaluations of the integrand. If the computer were
fast and could evaluate the integrand a million times per second, this would take about 1059 s,
which is significantly longer than the age of the universe (∼1017 s).
Your problem is to find a way to perform multidimensional integrations so that you are
still alive to savor the answers. Specifically, evaluate the 10-D integral
I =
∫ 1
0
dx1
∫ 1
0
dx2 · · ·
∫ 1
0
dx10 (x1 + x2 + · · ·+ x10)2 . (6.49)
Check your numerical answer against the analytic one, 1556 .
6.6.1 Multidimensional Monte Carlo
It is easy to generalize mean value integration to many dimensions by picking random points
in a multidimensional space. For example,∫ b
a
dx
∫ d
c
dy f(x, y) ' (b− a)(d− c) 1
N
N∑
i
f(xi) = (b− a)(d− c)〈f〉. (6.50)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
124 CHAPTER 6
6.6.2 Error in Multidimensional Integration (Assessment)
When we perform a multidimensional integration, the error in the Monte Carlo technique, be-
ing statistical, decreases as 1/
√
N . This is valid even if the N points are distributed over
D dimensions. In contrast, when we use these same N points to perform a D-dimensional
integration as D 1-D integrals using a rule such as Simpson’s, we use N/D points for each in-
tegration. For fixedN , this means that the number of points used for each integration decreases
as the number of dimensions D increases, and so the error in each integration increases with
D. Furthermore, the total error will be approximately N times the error in each integral. If we
put these trends together and look at a particular integration rule, we will find that at a value of
D ' 3–4 the error in Monte Carlo integration is similar to that of conventional schemes. For
larger values of D, the Monte Carlo method is always more accurate!
6.6.3 Implementation: 10-D Monte Carlo Integration
Use a built-in random-number generator to perform the 10-D Monte Carlo integration in (6.49).
1. Conduct 16 trials and take the average as your answer.
2. Try sample sizes of N = 2, 4, 8, . . . , 8192.
3. Plot the error versus 1/
√
N and see if linear behavior occurs.
6.7 INTEGRATING RAPIDLY VARYING FUNCTIONS (PROBLEM)
It is common in many physical applications to integrate a function with an approximately
Gaussian dependence on x. The rapid falloff of the integrand means that our Monte Carlo
integration technique would require an incredibly large number of points to obtain even modest
accuracy. Your problem is to make Monte Carlo integration more efficient for rapidly varying
integrands.
6.7.1 Variance Reduction (Method)
If the function being integrated never differs much from its average value, then the standard
Monte Carlo mean value method (6.47) should work well with a large, but manageable, number
of points. Yet for a function with a large variance (i.e., one that is not “flat”), many of the
random evaluations of the function may occur where the function makes a slight contribution
to the integral; this is, basically, a waste of time. The method can be improved by mapping
the function f into a function g that has a smaller variance over the interval. We indicate two
methods here and refer you to [Pres 00] and [Koon 86] for more details.
The first method is a variance reduction or subtraction technique in which we devise a
flatter function on which to apply the Monte Carlo technique. Suppose we construct a function
g(x) with the following properties on [a, b]:
|f(x)− g(x)| ≤ ,
∫ b
a
dx g(x) = J. (6.51)
We now evaluate the integral of f(x) − g(x) and add the result to J to obtain the required
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
INTEGRATION 125
Figure 6.6 The Von Neumann rejection technique for generating random points with weight w(x).
accept
reject
integral ∫ b
a
dx f(x) =
∫ b
a
dx [f(x)− g(x)] + J. (6.52)
If we are clever enough to find a simple g(x) that makes the variance of f(x)− g(x) less than
that of f(x) and that we can integrate analytically, we obtain more accurate answers in less
time.
6.7.2 Importance Sampling (Method)
A second method for improving Monte Carlo integration is called importance sampling be-
cause it lets us sample the integrand in the most important regions. It derives from expressing
the integral in the form
I =
∫ b
a
dx f(x) =
∫ b
a
dxw(x)
f(x)
w(x)
. (6.53)
If we now use w(x) as the weighting function or probability distribution for our random num-
bers, the integral can be approximated as
I =
〈
f
w
〉
' 1
N
N∑
i=1
f(xi)
w(xi)
. (6.54)
The improvement from (6.54) is that a judicious choice of weighting function w(x) ∝ f(x)
makes f(x)/w(x) more constant and thus easier to integrate.
6.7.3 Von Neumann Rejection (Method)
A simple, ingenious method for generating random points with a probability distribution w(x)
was deduced by von Neumann. This method is essentially the same as the rejection or sam-
pling method used to guess the area of a pond, only now the pond has been replaced by the
weighting function w(x), and the arbitrary box around the lake by the arbitrary constant W0.
Imagine a graph of w(x) versus x (Figure 6.6). Walk off your box by placing the lineW = W0
on the graph, with the only condition being W0 ≥ w(x). We next “throw stones” at this graph
and count only those that fall into the w(x) pond. That is, we generate uniform distributions in
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
126 CHAPTER 6
x and y ≡W with the maximum y value equal to the width of the box W0:
(xi,Wi) = (r2i−1,W0r2i). (6.55)
We then reject all xi that do not fall into the pond:
If Wi < w(xi), accept, If Wi > w(xi), reject. (6.56)
The xi values so accepted will have the weighting w(x) (Figure 6.6). The largest acceptance
occurs where w(x) is large, in this case for midrange x. In Chapter 15, “Thermodynamic
Simulations & Feynman Quantum Path Integration,” we apply a variation of the rejection tech-
nique known as the Metropolis algorithm. This algorithm has now become the cornerstone of
computation thermodynamics.
6.7.4 Simple Gaussian Distribution
The central limit theorem can be used to deduce a Gaussian distribution via a simple summa-
tion. The theorem states, under rather general conditions, that if {ri} is a sequence of mutually
independent random numbers, then the sum
xN =
N∑
i=1
ri (6.57)
is distributed normally. This means that the generated x values have the distribution
PN (x) =
exp
[
− (x−µ)
2
2σ2
]
√
2πσ2
, µ = N〈r〉, σ2 = N(〈r2〉 − 〈r〉2). (6.58)
6.8 NONUNIFORM ASSESSMENT 
Use the von Neumann rejection technique to generate a normal distribution of standard devia-
tion 1 and compare to the simple Gaussian method.
6.8.1 Implementation
In order for w(x) to be the weighting function for random numbers over [a, b], we want it to
have the properties∫ b
a
dxw(x) = 1, [w(x) > 0], dP(x→ x+ dx) = w(x) dx, (6.59)
where dP is the probability of obtaining an x in the range x → x + dx. For the uniform
distribution over [a, b], w(x) = 1/(b− a).
Inverse transform/change of variable method : Let us consider a change of variables
that takes our original integral I (6.53) to the form
I =
∫ b
a
dx f(x) =
∫ 1
0
dW
f [x(W )]
w[x(W )]
. (6.60)
Our aim is to make this transformation such that there are equal contributions from all
parts of the range in W ; that is, we want to use a uniform sequence of random numbers
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
INTEGRATION 127
for W . To determine the new variable, we start with u(r), the uniform distribution over
[0, 1],
u(r) =
{
1, for 0 ≤ r ≤ 1,
0, otherwise.
(6.61)
We want to find a mapping r ↔ x or probability function w(x) for which probability is
conserved:
w(x) dx = u(r) dr, ⇒ w(x) =
∣∣∣∣drdx
∣∣∣∣u(r). (6.62)
This means that even though x and r are related by some (possibly) complicated mapping,
x is also random with the probability of x lying in x→ x+ dx equal to that of r lying in
r → r + dr.
To find the mapping between x and r (the tricky part), we change variables to W (x)
defined by the integral
W (x) =
∫ x
−∞
dx′w(x′). (6.63)
We recognizeW (x) as the (incomplete) integral of the probability density u(r) up to some
point x. It is another type of distribution function, the integrated probability of finding a
random number less than the value x. The function W (x) is on that account called a
cumulative distribution function and can also be thought of as the area to the left of r = x
on the plot of u(r) versus r. It follows immediately from the definition (6.63) that W (x)
has the properties
W (−∞) = 0; W (∞) = 1, (6.64)
dW (x)
dx
=w(x), dW (x) = w(x) dx = u(r) dr. (6.65)
Consequently, Wi = {ri} is a uniform sequence of random numbers, and we just need to
invert (6.63) to obtain x values distributed with probability w(x).
The crux of this technique is being able to invert (6.63) to obtain x = W−1(r). Let
us look at some analytic examples to get a feel for these steps (numerical inversion is
possible and frequent in realistic cases).
Uniform weight function w: We start with the familiar uniform distribution
w(x) =
{
1
b−a , if a ≤ x ≤ b,
0, otherwise.
(6.66)
After following the rules, this leads to
W (x) =
∫ x
a
dx′
1
b− a
=
x− a
b− a
(6.67)
⇒ x= a+ (b− a)W ⇒ W−1(r) = a+ (b− a)r, (6.68)
where W (x) is always taken as uniform. In this way we generate uniform random 0 ≤
r ≤ 1 and uniform random a ≤ x ≤ b.
Exponential weight: We want random points with an exponential distribution:
w(x) =
{
1
λe
−x/λ, for x > 0,
0, for x < 0,
W (x) =
∫ x
0
dx′
1
λ
e−x
′/λ = 1− e−x/λ,
⇒ x=−λ ln(1−W ) ≡ −λ ln(1− r). (6.69)
In this way we generate uniform random r : [0, 1] and obtain x = −λ ln(1−r) distributed
with an exponential probability distribution for x > 0. Notice that our prescription (6.53)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
128 CHAPTER 6
and (6.54) tells us to use w(x) = e−x/λ/λ to remove the exponential-like behavior from
an integrand and place it in the weights and scaled points (0 ≤ xi ≤ ∞). Because the
resulting integrand will vary less, it may be approximated better as a polynomial:∫ ∞
0
dx e−x/λf(x) ' λ
N
N∑
i=1
f(xi), xi = −λ ln(1− ri). (6.70)
Gaussian (normal) distribution: We want to generate points with a normal distribution:
w(x′) =
1√
2πσ
e−(x
′−x)2/2σ2 . (6.71)
This by itself is rather hard but is made easier by generating uniform distributions in
angles and then using trigonometric relations to convert them to a Gaussian distribution.
But before doing that, we keep things simple by realizing that we can obtain (6.71) with
mean x and standard deviation σ by scaling and a translation of a simpler w(x):
w(x) =
1√
2π
e−x
2/2, x′ = σx+ x. (6.72)
We start by generalizing the statement of probability conservation for two different distri-
butions (6.62) to two dimensions [Pres 94]:
p(x, y) dx dy = u(r1, r2) dr1 dr2 ⇒ p(x, y) = u(r1, r2)
∣∣∣∣∂(r1, r2)∂(x, y)
∣∣∣∣ .
We recognize the term in vertical bars as the Jacobian determinant:
J =
∣∣∣∣∂(r1, r2)∂(x, y)
∣∣∣∣ def= ∂r1∂x ∂r2∂y − ∂r2∂x ∂r1∂y . (6.73)
To specialize to a Gaussian distribution, we consider 2πr as angles obtained from a uni-
form random distribution r, and x and y as Cartesian coordinates that will have a Gaussian
distribution. The two are related by
x =
√
−2 ln r1 cos 2πr2, y =
√
−2 ln r1 sin 2πr2. (6.74)
The inversion of this mapping produces the Gaussian distribution
r1 = e−(x
2+y2)/2, r2 =
1
2π
tan−1
y
x
, J = −e
−(x2+y2)/2
2π
. (6.75)
The solution to our problem is at hand. We use (6.74) with r1 and r2 uniform random
distributions, and x and y are then Gaussian random distributions centered around x = 0.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Seven
Differentiation & Searching
In this chapter we add two more tools to our computational toolbox: numerical differentiation
and trial-and-error searching. In Unit I we derive the forward-difference, central-difference,
and extrapolated-difference methods for differentiation. They will be used throughout the book,
especially for partial differential equations. In Unit II we devise ways to search for solutions
to nonlinear equations by trial and error and apply our new-found numerical differentiation
tools there. Although trial-and-error searching may not sound very precise, it is in fact widely
used to solve problems where analytic solutions do not exist or are not practical. In Chapter 8,
“Solving Systems of Equations with Matrices; Data Fitting,” we extend these search and dif-
ferentiation techniques to the solution of simultaneous equations using matrix techniques. In
Chapter 9, “Differential Equation Applications,” we combine trial-and-error searching with
the solution of ordinary differential equations to solve the quantum eigenvalue problem.
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
Numerical Differentiation 7.1–7.6 Trial and Error Searching 7.7–7.10
N-Dimensional Searching 8.2 -
7.1 UNIT I. NUMERICAL DIFFERENTIATION
Problem: Figure 7.1 shows the trajectory of a projectile with air resistance. The dots indicate
the times t at which measurements were made and tabulated. Your problem is to determine
the velocity dy/dt ≡ y′ as a function of time. Note that since there is realistic air resistance
present, there is no analytic function to differentiate, only this table of numbers.
You probably did rather well in your first calculus course and feel competent at taking
derivatives. However, you may not ever have taken derivatives of a table of numbers using the
elementary definition
dy(t)
dt
def= lim
h→0
y(t+ h)− y(t)
h
. (7.1)
In fact, even a computer runs into errors with this kind of limit because it is wrought with
subtractive cancellation; the computer’s finite word length causes the numerator to fluctuate
between 0 and the machine precision m as the denominator approaches zero.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
130 CHAPTER 7
Figure 7.1 Forward-difference approximation (slanted dashed line) and central-difference approximation (horizon-
tal line) for the numerical first derivative at point t. The central difference is seen to be more accurate.
( The trajectory is that of a projectile with air resistance.)
Centraly(t)
0
t
t +
h
/2
t - h
/2
Forward
y(t)
0
t
t +
 h
7.2 FORWARD DIFFERENCE (ALGORITHM)
The most direct method for numerical differentiation starts by expanding a function in a Taylor
series to obtain its value a small step h away:
y(t+ h) = y(t) + h
dy(t)
dt
+
h2
2!
d2y(t)
dt2
+
h3
3!
dy3(t)
dt3
+ · · · . (7.2)
We obtain the forward-difference derivative algorithm by solving (7.2) for y′(t):
xmll dy(t)
dt
∣∣∣∣
fd
def=
y(t+ h)− y(t)
h
. (7.3)
An approximation for the error follows from substituting the Taylor series:
dy(t)
dt
∣∣∣∣
fd
' dy(t)
dt
+
h
2
dy2(t)
dt2
+ · · · . (7.4)
You can think of this approximation as using two points to represent the function by a straight
line in the interval from x to x+ h (Figure 7.1 left).
The approximation (7.3) has an error proportional to h (unless the heavens look down
upon you kindly and make y′′ vanish). We can make the approximation error smaller by making
h smaller, yet precision will be lost through the subtractive cancellation on the left-hand side
(LHS) of (7.3) for too small an h. To see how the forward-difference algorithm works, let
y(t) = a+ bt2. The exact derivative is y′ = 2bt, while the computed derivative is
dy(t)
dt
∣∣∣∣
fd
' y(t+ h)− y(t)
h
= 2bt+ bh. (7.5)
This clearly becomes a good approximation only for small h(h 2t).
7.3 CENTRAL DIFFERENCE (ALGORITHM)
An improved approximation to the derivative starts with the basic definition (7.1) or geometri-
cally as shown in Figure 7.1 on the right. Now, rather than making a single step of h forward,
we form a central difference by stepping forward half a step and backward half a step:
xmll
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIATION & SEARCHING 131
dy(t)
dt
∣∣∣∣
cd
def=
y(t+ h/2)− y(t− h/2)
h
. (7.6)
We estimate the error in the central-difference algorithm by substituting the Taylor series for
y(t± h/2) into (7.6):
y
(
t+
h
2
)
− y
(
t− h
2
)
'
[
y(t) +
h
2
y′(t) +
h2
8
y′′(t) +
h3
48
y′′′(t) +O(h4)
]
−
[
y(t)− h
2
y′(t) +
h2
8
y′′(t)− h
3
48
y′′′(t) +O(h4)
]
=hy′(t) +
h3
24
y′′′(t) +O(h5),
⇒ dy(t)
dt
∣∣∣∣
cd
' y′(t) + 1
24
h2y′′′(t) +O(h4). (7.7)
The important difference between this algorithm and the forward difference from (7.3) is that
when y(t− h/2) is subtracted from y(t+ h/2), all terms containing an even power of h in the
two Taylor series cancel. This make the central-difference algorithm accurate to order h2 (h3
before division by h), while the forward difference is accurate only to order h. If the y(t) is
smooth, that is, if y′′′h2/24  y′′h/2, then you can expect the central-difference error to be
smaller. If we now return to our parabola example (7.5), we will see that the central difference
gives the exact derivative independent of h:
dy(t)
dt
∣∣∣∣
cd
' y(t+ h/2)− y(t− h/2)
h
= 2bt. (7.8)
7.4 EXTRAPOLATED DIFFERENCE (METHOD)
Because a differentiation rule based on keeping a certain number of terms in a Taylor series
also provides an expression for the error (the terms not included), we can reduce the theoretical
error further by forming a combination of algorithms whose summed errors extrapolate to zero.
One algorithm is the central-difference algorithm (7.6) using a half-step back and a half-step
forward. The second algorithm is another central-difference approximation using quarter-steps:
dy(t, h/2)
dt
∣∣∣∣
cd
def=
y(t+ h/4)− y(t− h/4)
h/2
' y′(t) + h
2
96
d3y(t)
dt3
+ · · · . (7.9)
A combination of the two eliminates both the quadratic and linear error terms:
dy(t)
dt
∣∣∣∣
ed
def=
4Dcdy(t, h/2)−Dcdy(t, h)
3
(7.10)
' dy(t)
dt
− h
4y(5)(t)
4× 16× 120
+ · · · . (7.11)
Here (7.10) is the extended-difference algorithm, (7.11) gives its error, and Dcd represents the
central-difference algorithm. If h = 0.4 and y(5) ' 1, then there will be only one place of
round-off error and the truncation error will be approximately machine precision m; this is the
best you can hope for.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
132 CHAPTER 7
When working with these and similar higher-order methods, it is important to remem-
ber that while they may work as designed for well-behaved functions, they may fail badly for
functions containing noise, as may data from computations or measurements. If noise is evi-
dent, it may be better to first smooth the data or fit them with some analytic function using the
techniques of Chapter 8, “Solving Systems of Equations with Matrices; Data Fitting,” and then
differentiate.
7.5 ERROR ANALYSIS (ASSESSMENT)
The approximation errors in numerical differentiation decrease with decreasing step size h,
while round-off errors increase with decreasing step size (you have to take more steps and do
more calculations). Remember from our discussion in Chapter 2, “Errors & Uncertainties in
Computations,” that the best approximation occurs for an h that makes the total error approx+ro
a minimum, and that as a rough guide this occurs when ro ' approx.
We have already estimated the approximation error in numerical differentiation rules by
making a Taylor series expansion of y(x + h). The approximation error with the forward-
difference algorithm (7.3) is O(h), while that with the central-difference algorithm (7.7) is
O(h2):
fdapprox '
y′′h
2
, cdapprox '
y′′′h2
24
. (7.12)
To obtain a rough estimate of the round-off error, we observe that differentiation essentially
subtracts the value of a function at argument x from that of the same function at argument
x + h and then divides by h : y′ ' [y(t + h) − y(t)]/h. As h is made continually smaller,
we eventually reach the round-off error limit where y(t + h) and y(t) differ by just machine
precision m:
ro '
m
h
. (7.13)
Consequently, round-off and approximation errors become equal when
ro ' approx,
m
h
' fdapprox =
y(2)h
2
,
m
h
' cdapprox =
y(3)h2
24
,
⇒ h2fd =
2m
y(2)
, ⇒ h3cd =
24m
y(3)
.
(7.14)
We take y′ ' y(2) ' y(3) (which may be crude in general, though not bad for et or cos t) and
assume double precision, m ' 10−15:
hfd ' 4× 10−8, hcd ' 3× 10−5,
⇒ fd '
m
hfd
' 3× 10−8, ⇒ cd '
m
hcd
' 3× 10−11.
(7.15)
This may seem backward because the better algorithm leads to a larger h value. It is not. The
ability to use a larger h means that the error in the central-difference method is about 1000
times smaller than the error in the forward-difference method.
We give a full program Diff.py, yet the programming for numerical differentiation is so
simple that we need give only the lines here 
FD = ( y ( t +h ) − y ( t ) ) / h ; / / forward d i f f
CD = ( y ( t +h / 2 ) − y ( t−h / 2 ) ) / h ; / / c e n t r a l d i f f
ED = ( 8∗ ( y ( t +h / 4 ) − y ( t−h / 4 ) ) − ( y ( t +h / 2 )−y ( t−h / 2 ) ) ) / ( 3∗ h ) ; / / ex trap d i f f
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIATION & SEARCHING 133
1. Use forward-, central-, and extrapolated-difference algorithms to differentiate the func-
tions cos t and et at t = 0.1, 1., and 100.
a. Print out the derivative and its relative error E as functions of h. Reduce the step size
h until it equals machine precision h ' m.
b. Plot log10 |E| versus log10 h and check whether the number of decimal places ob-
tained agrees with the estimates in the text.
c. See if you can identify regions where truncation error dominates at large h and round-
off error at small h in your plot. Do the slopes agree with our model’s predictions?
7.6 SECOND DERIVATIVES (PROBLEM)
Let’s say that you have measured the position y(t) versus time for a particle (Figure 7.1). Your
problem now is to determine the force on the particle. Newton’s second law tells us that force
and acceleration are linearly related:
F = ma = m
d2y
dt2
, (7.16)
where F is the force, m is the particle’s mass, and a is the acceleration. So if we can determine
the acceleration a(t) = d2y/dt2 from the y(t) values, we can determine the force.
The concerns we expressed about errors in first derivatives are even more valid for second
derivatives where additional subtractions may lead to additional cancellations. Let’s look again
at the central-difference method:
dy(t)
dt
' y(t+ h/2)− y(t− h/2)
h
. (7.17)
This algorithm gives the derivative at t by moving forward and backward from t by h/2. We
take the second derivative d2y/dt2 to be the central difference of the first derivative:
xmlld2y(t)
dt2
' y
′(t+ h/2)− y′(t− h/2)
h
' [y(t+ h)− y(t)]− [y(t)− y(t− h)]
h2
(7.18)
=
y(t+ h) + y(t− h)− 2y(t)
h2
. (7.19)
As we did for first derivatives, we determine the second derivative at t by evaluating the func-
tion in the region surrounding t. Although the form (7.19) is more compact and requires fewer
steps than (7.18), it may increase subtractive cancellation by first storing the “large” number
y(t + h) + y(t − h) and then subtracting another large number 2y(t) from it. We ask you to
explore this difference as an exercise.
7.6.1 Second-Derivative Assessment
Write a program to calculate the second derivative of cos t using the central-difference algo-
rithms (7.18) and (7.19). Test it over four cycles. Start with h ' π/10 and keep reducing h
until you reach machine precision.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
134 CHAPTER 7
7.7 UNIT II. TRIAL-AND-ERROR SEARCHING
Many computer techniques are well-defined sets of procedures leading to definite outcomes.
In contrast, some computational techniques are trial-and-error algorithms in which decisions
on what steps to follow are made based on the current values of variables, and the program
quits only when it thinks it has solved the problem. (We already did some of this when we
summed a power series until the terms became small.) Writing this type of program is usually
interesting because we must foresee how to have the computer act intelligently in all possible
situations, and running them is very much like an experiment in which it is hard to predict what
the computer will come up with.
7.8 QUANTUM STATES IN A SQUARE WELL (PROBLEM)
Probably the most standard problem in quantum mechanics1 is to solve for the energies of a
particle of mass m bound within a 1-D square well of radius a:
V (x) =
{
−V0, for |x| ≤ a,
0, for |x| ≥ a.
(7.20)
As shown in quantum mechanics texts [Gott 66], the energies of the bound states E = −EB <
0 within this well are solutions of the transcendental equations√
10− EB tan
(√
10− EB
)
=
√
EB (even), (7.21)√
10− EB cotan
(√
10− EB
)
=
√
EB (odd), (7.22)
where even and odd refer to the symmetry of the wave function. Here we have chosen units
such that h̄ = 1, 2m = 1, a = 1, and V0 = 10. Your problem is to
1. Find several bound-state energies EB for even wave functions, that is, the solution of
(7.21).
2. See if making the potential deeper, say, by changing the 10 to a 20 or a 30, produces a
larger number of, or deeper bound states.
7.9 TRIAL-AND-ERROR ROOTS VIA BISECTION ALGORITHM
Trial-and-error root finding looks for a value of x at which
f(x) = 0,
where the 0 on the right-hand side (RHS) is conventional (an equation such as 10 sinx = 3x3
can easily be written as 10 sinx− 3x3 = 0). The search procedure starts with a guessed value
for x, substitutes that guess into f(x) (the “trial”), and then sees how far the LHS is from zero
(the “error”). The program then changes x based on the error and tries out the new guess in
f(x). The procedure continues until f(x) ' 0 to some desired level of precision, until the
changes in x are insignificant, or until it appears that progress is not being made.
The most elementary trial-and-error technique is the bisection algorithm. It is reliable but
slow. If you know some interval in which f(x) changes sign, then the bisection algorithm will
always converge to the root by finding progressively smaller and smaller intervals in which the
1We solve this same problem in §9.9 using an approach that is applicable to almost any potential and which also provides the
wave functions. The approach of this section works only for the eigenenergies of a square well.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIATION & SEARCHING 135
zero occurs. Other techniques, such as the Newton–Raphson method we describe next, may
converge more quickly, but if the initial guess is not close, it may become unstable and fail
completely.
The basis of the bisection algorithm is shown on the left in Figure 7.2. We start with
two values of x between which we know a zero occurs. (You can determine these by making a
graph or by stepping through different x values and looking for a sign change.) To be specific,
let us say that f(x) is negative at x− and positive at x+:
f(x−) < 0, f(x+) > 0. (7.23)
(Note that it may well be that x− > x+ if the function changes from positive to negative as x
increases.) Thus we start with the interval x+ ≤ x ≤ x− within which we know a zero occurs.
The algorithm (given on the CD as Bisection.py) then picks the new x as the bisection of the
interval and selects as its new interval the half in which the sign change occurs: 
x = ( x P l u s + xMinus ) / 2
i f ( f ( x ) f ( x P l u s ) > 0 ) x P l u s = x
e l s e xMinus = x
This process continues until the value of f(x) is less than a predefined level of precision or
until a predefined (large) number of subdivisions occurs.
The example in Figure 7.2 on the left shows the first interval extending from x− = x+1
to x+ = x−1. We then bisect that interval at x, and since f(x) < 0 at the midpoint, we set
x− ≡ x−2 = x and label it x−2 to indicate the second step. We then use x+2 ≡ x+1 and x−2
as the next interval and continue the process. We see that only x− changes for the first three
steps in this example, but that for the fourth step x+ finally changes. The changes then become
too small for us to show.
7.9.1 Bisection Algorithm Implementation
1. The first step in implementing any search algorithm is to get an idea of what your func-
tion looks like. For the present problem you do this by making a plot of f(E) =√
10− EB tan(
√
10− EB) −
√
EB versus EB . Note from your plot some approxi-
mate values at which f(EB) = 0. Your program should be able to find more exact
values for these zeros.
2. Write a program that implements the bisection algorithm and use it to find some solutions
of (7.21).
3. Warning: Because the tan function has singularities, you have to be careful. In fact,
your graphics program (or Maple) may not function accurately near these singularities.
One cure is to use a different but equivalent form of the equation. Show that an equivalent
form of (7.21) is √
E cot(
√
10− E)−
√
10− E = 0. (7.24)
4. Make a second plot of (7.24), which also has singularities but at different places. Choose
some approximate locations for zeros from this plot.
5. Compare the roots you find with those given by Maple or Mathematica.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
136 CHAPTER 7
7.10 NEWTON–RAPHSON SEARCHING (IMPROVED ALGORITHM)
The Newton–Raphson algorithm finds approximate roots of the equation
f(x) = 0
more quickly than the bisection method. As we see graphically in Figure 7.2 on the right, this
algorithm is the equivalent of drawing a straight line f(x) ' mx + b tangent to the curve
at an x value for which f(x) ' 0 and then using the intercept of the line with the x axis at
x = −b/m as an improved guess for the root. If the “curve” were a straight line, the answer
would be exact; otherwise, it is a good approximation if the guess is close enough to the root
for f(x) to be nearly linear. The process continues until some set level of precision is reached.
If a guess is in a region where f(x) is nearly linear (Figure 7.2), then the convergence is much
more rapid than for the bisection algorithm.
The analytic formulation of the Newton–Raphson algorithm starts with an old guess x0
and expresses a new guess x as a correction ∆x to the old guess:
x0 = old guess, ∆x = unknown correction (7.25)
⇒ x=x0 + ∆x = (unknown) new guess. (7.26)
We next expand the known function f(x) in a Taylor series around x0 and keep only the linear
terms:
xmll f(x = x0 + ∆x) ' f(x0) +
df
dx
∣∣∣∣
x0
∆x. (7.27)
We then determine the correction ∆x by calculating the point at which this linear approxima-
tion to f(x) crosses the x axis:
f(x0) +
df
dx
∣∣∣∣
x0
∆x = 0, (7.28)
⇒ ∆x = − f(x0)
df/dx|x0
. (7.29)
The procedure is repeated starting at the improved x until some set level of precision is ob-
tained.
The Newton–Raphson algorithm (7.29) requires evaluation of the derivative df/dx at
each value of x0. In many cases you may have an analytic expression for the derivative and can
build it into the algorithm. However, especially for more complicated problems, it is simpler
and less error-prone to use a numerical forward-difference approximation to the derivative:2
df
dx
' f(x+ δx)− f(x)
δx
, (7.30)
where δx is some small change in x that you just chose [different from the ∆ used for searching
in (7.29)]. While a central-difference approximation for the derivative would be more accurate,
it would require additional evaluations of the f ’s, and once you find a zero, it does not matter
how you got there. On the CD we give the programs NewtonCD.py (also Listing 7.1) and
NewtonFD.py, which implement the derivative both ways.
2We discuss numerical differentiation in Chapter 7, “Differentiation & Searching.”
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIATION & SEARCHING 137
Figure 7.2 A graphical representation of the steps involved in solving for a zero of f (x) using the bisection algorithm
(left) and the Newton–Raphson method (right). The bisection algorithm takes the midpoint of the inter-
val as the new guess for x, and so each step reduces the interval size by one-half. The Newton–Raphson
method takes the new guess as the zero of the line tangent to f (x) at the old guess. Four steps are shown
for the bisection algorithm, but only two for the more rapidly convergent Newton–Raphson method.
f(x)
x
x+1
x-2
x-1
x+4
x-3
+0 -
Figure 7.3 Two examples of how the Newton–Raphson algorithm may fail if the initial guess is not in the region
where f (x) can be approximated by a straight line. Left: A guess lands at a local minimum/maximum,
that is, a place where the derivative vanishes, and so the next guess ends up at x =∞. Right: The search
has fallen into an infinite loop. Backtracking would help here.
Listing 7.1 NewtonCD.py uses the Newton–Raphson method to search for a zero of the function f (x). A central-
difference approximation is used to determine df /dx. 
# NewtonCD . py Newton Se a r c h wi th c e n t r a l d i f f e r e n c e
from v i s u a l i m p o r t ∗
from v i s u a l . g raph i m p o r t ∗
x = 2 . ; dx = 1e−2; eps = 1e−6; # P a r a m e t e r s
imax = 100 ; # Max no of i t e r a t i o n s
d e f f ( x ) : # f u n c t i o n d e f
r e t u r n 2∗ cos ( x ) − x
f o r i t i n r a n g e ( 0 , imax + 1) :
F = f ( x )
i f ( abs ( F ) <= eps ) : # Check f o r c o n v e r g e n c e
p r i n t "Root found, tolerance eps = " , eps
break
p r i n t "Iteration # = " , i t , " x = " , x , " f(x) = " , F
d f = ( f ( x + dx / 2 ) − f ( x − dx / 2 ) ) / dx # C e n t r a l d i f f d e r i v
dx = − F / d f
x += dx # New g u e s s
p r i n t "Press a character to finish"
s= r a w i n p u t ( )
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
138 CHAPTER 7
7.10.1 Newton–Raphson with Backtracking
Two examples of possible problems with the Newton–Raphson algorithm are shown in Fig-
ure 7.3. On the left we see a case where the search takes us to an x value where the function
has a local minimum or maximum, that is, where df/dx = 0. Because ∆x = −f/f ′, this leads
to a horizontal tangent (division by zero), and so the next guess is x =∞, from where it is hard
to return. When this happens, you need to start your search with a different guess and pray that
you do not fall into this trap again. In cases where the correction is very large but maybe not
infinite, you may want to try backtracking (described below) and hope that by taking a smaller
step you will not get into as much trouble.
In Figure 7.3 on the right we see a case where a search falls into an infinite loop sur-
rounding the zero without ever getting there. A solution to this problem is called backtracking.
As the name implies, in cases where the new guess x0 + ∆x leads to an increase in the mag-
nitude of the function, |f(x0 + ∆x)|2 > |f(x0)|2, you should backtrack somewhat and try a
smaller guess, say, x0 + ∆x/2. If the magnitude of f still increases, then you just need to
backtrack some more, say, by trying x0 + ∆x/4 as your next guess, and so forth. Because
you know that the tangent line leads to a local decrease in |f |, eventually an acceptable small
enough step should be found.
The problem in both these cases is that the initial guesses were not close enough to the
regions where f(x) is approximately linear. So again, a good plot may help produce a good
first guess. Alternatively, you may want to start your search with the bisection algorithm and
then switch to the faster Newton–Raphson algorithm when you get closer to the zero.
7.10.2 Newton–Raphson Implementation
1. Use the Newton–Raphson algorithm to find some energies EB that are solutions of
(7.21). Compare this solution with the one found with the bisection algorithm.
2. Again, notice that the 10 in this equation is proportional to the strength of the potential
that causes the binding. See if making the potential deeper, say, by changing the 10
to a 20 or a 30, produces more or deeper bound states. (Note that in contrast to the
bisection algorithm, your initial guess must be closer to the answer for the Newton–
Raphson algorithm to work.)
3. Modify your algorithm to include backtracking and then try it out on some problem
cases.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Eight
Solving Systems of Equations with Matrices;
Data Fitting
Unit I applies the trial-and-error techniques developed in Chapter 7, “Differentiation &
Searching,” to solve a set of simultaneous nonlinear equations. This leads us into general
matrix computing using scientific libraries. In Unit II we look at several ways in which theo-
retical formulas are fit to data and see that these often require the matrix techniques of Unit
I.
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
Matrix Computing 8.1–8.3 Trial and Error Searching 7.7–7.10
N-Dimensional Searching 8.2.2 Interpolation 8.4–8.5
Least Square Fitting 8.7 -
Applets
Name Sections Name Sections
Lagrange Interpolation 8.5 Spline Interpolation 8.5
8.1 UNIT I. SYSTEMS OF EQUATIONS AND MATRIX COMPUTING
Physical systems are often modeled by systems of simultaneous equations written in matrix
form. As the models are made more realistic, the matrices often become large, and comput-
ers become an excellent tool for solving such problems. What makes computers so good is
that matrix manipulations intrinsically involve the continued repetition of a small number of
simple instructions, and algorithms exist to do this quite efficiently. Further speedup may be
achieved by tuning the codes to the computer’s architecture, as discussed in Chapter 14, “High-
Performance Computing Hardware, Tuning, & Parallel Computing.”
Industrial-strength subroutines for matrix computing are found in well-established
scientific libraries. These subroutines are usually an order of magnitude or more faster than the
elementary methods found in linear algebra texts,1 are usually designed to minimize round-off
error, and are often “robust,” that is, have a high chance of being successful for a broad class of
problems. For these reasons we recommend that you do not write your own matrix subroutines
but instead get them from a library. An additional value of library routines is that you can often
run the same program either on a desktop machine or on a parallel supercomputer, with matrix
routines automatically adapting to the local architecture.
1Although we prize the book [Pres 94] and what it has accomplished, we cannot recommend taking subroutines from it. They
are neither optimized nor documented for easy, stand-alone use, whereas the subroutine libraries recommended in this chapter are.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
140 CHAPTER 8
Figure 8.1 Left: Two weights connected by three pieces of string and suspended from a horizontal bar of length L.
The angles and the tensions in the strings are unknown. Right: A free body diagram for one weight in
equilibrium.
T1
L1
T2
L2
T3
L3
W1
W2
θ1 θ3
θ2
θ3
Ti
Ti+1
Wi
θi
θi+1
i
The thoughtful reader may be wondering when a matrix is “large” enough to require the
use of a library routine. While in the past large may have meant a good fraction of your com-
puter’s random-access memory (RAM), we now advise that a library routine be used whenever
the matrix computations are so numerically intensive that you must wait for results. In fact,
even if the sizes of your matrices are small, as may occur in graphical processing, there may
be library routines designed just for that which speed up your computation.
Now that you have heard the sales pitch, you may be asking, “What’s the cost?” In the
later part of this chapter we pay the costs of having to find what libraries are available, of having
to find the name of the routine in that library, of having to find the names of the subroutines
your routine calls, and then of having to figure out how to call all these routines properly. And
because some of the libraries are in Fortran, if you are a C programmer you may also be taxed
by having to call a Fortran routine from your C program. However, there are now libraries
available in most languages.
8.2 TWO MASSES ON A STRING
Two weights (W1,W2) = (10, 20) are hung from three pieces of string with lengths
(L1, L2, L3) = (3, 4, 4) and a horizontal bar of length L = 8 (Figure 8.1). The problem
here is to find the angles assumed by the strings and the tensions exerted by the strings.
In spite of the fact that this is a simple problem requiring no more than first-year physics
to formulate, the coupled transcendental equations that result are inhumanely painful to solve
analytically. However, we will show you how the computer can solve this problem, but even
then only by a trial-and-error technique with no guarantee of success. Your problem is to
test this solution for a variety of weights and lengths and then to extend it to the three-weight
problem (not as easy as it may seem). In either case check the physical reasonableness of your
solution; the deduced tensions should be positive and of similar magnitude to the weights of
the spheres, and the deduced angles should correspond to a physically realizable geometry, as
confirmed with a sketch. Some of the exploration you should do is to see at what point your
initial guess gets so bad that the computer is unable to find a physical solution.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 141
8.2.1 Statics (Theory)
We start with the geometric constraints that the horizontal length of the structure is L and that
the strings begin and end at the same height (Figure 8.1 left):
L1 cos θ1 + L2 cos θ2 + L3 cos θ3 =L, (8.1)
L1 sin θ1 + L2 sin θ2 − L3 sin θ3 = 0, (8.2)
sin2 θ1 + cos2 θ1 = 1, (8.3)
sin2 θ2 + cos2 θ2 = 1, (8.4)
sin2 θ3 + cos2 θ3 = 1. (8.5)
Observe that the last three equations include trigonometric identities as independent equations
xmll
because we are treating sin θ and cos θ as independent variables; this makes the search proce-
dure easier to implement. The basics physics says that since there are no accelerations, the sum
of the forces in the horizontal and vertical directions must equal zero (Figure 8.1 right):
T1 sin θ1 − T2 sin θ2 −W1 = 0, (8.6)
T1 cos θ1 − T2 cos θ2 = 0, (8.7)
T2 sin θ2 + T3 sin θ3 −W2 = 0, (8.8)
T2 cos θ2 − T3 cos θ3 = 0. (8.9)
Here Wi is the weight of mass i and Ti is the tension in string i. Note that since we do not have
a rigid structure, we cannot assume the equilibrium of torques.
8.2.2 Multidimensional Searching
Equations (8.1)–(8.9) are nine simultaneous nonlinear equations. While linear equations can
be solved directly, nonlinear equations cannot [Pres 00]. You can use the computer to search for
a solution by guessing, but there is no guarantee of finding one. We apply to our set the same
Newton–Raphson algorithm as used to solve a single equation by renaming the nine unknown
angles and tensions as the subscripted variable yi and placing the variables together as a vector:
~y =

x1
x2
x3
x4
x5
x6
x7
x8
x9

=

sin θ1
sin θ2
sin θ3
cos θ1
cos θ2
cos θ3
T1
T2
T3

. (8.10)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
142 CHAPTER 8
The nine equations to be solved are written in a general form with zeros on the right-hand sides
and placed in a vector:
fi(x1, x2, . . . , xN ) = 0, i = 1, N, (8.11)
~f(~y) =

f1(~y)
f2(~y)
f3(~y)
f4(~y)
f5(~y)
f6(~y)
f7(~y)
f8(~y)
f9(~y)

=

3x4 + 4x5 + 4x6 − 8
3x1 + 4x2 − 4x3
x7x1 − x8x2 − 10
x7x4 − x8x5
x8x2 + x9x3 − 20
x8x5 − x9x6
x21 + x
2
4 − 1
x22 + x
2
5 − 1
x23 + x
2
6 − 1

= ~0. (8.12)
The solution to these equations requires a set of nine xi values that make all nine fi’s vanish
xmll
simultaneously. Although these equations are not very complicated (the physics after all is
elementary), the terms quadratic in xmake them nonlinear, and this makes it hard or impossible
to find an analytic solution. The search algorithm is to guess a solution, expand the nonlinear
equations into linear form, solve the resulting linear equations, and continue to improve the
guesses based on how close the previous one was to making ~f = 0.
Explicitly, let the approximate solution at any one stage be the set {xi} and let us assume
that there is an (unknown) set of corrections {∆xi} for which
fi(x1 + ∆x1, x2 + ∆x2, . . . , x9 + ∆x9) = 0, i = 1, 9. (8.13)
We solve for the approximate ∆xi’s by assuming that our previous solution is close enough to
the actual one for two terms in the Taylor series to be accurate:
fi(x1 + ∆x1, . . . , x9 + ∆x9) ' fi(x1, . . . , x9) +
9∑
j=1
∂fi
∂xj
∆xj = 0. i = 1, 9. (8.14)
We now have a solvable set of nine linear equations in the nine unknowns ∆xi, which we
express as a single matrix equation
f1 + ∂f1/∂x1 ∆x1 + ∂f1/∂x2 ∆x2 + · · ·+ ∂f1/∂x9 ∆x9 = 0,
f2 + ∂f2/∂x1 ∆x1 + ∂f2/∂x2 ∆x2 + · · ·+ ∂f2/∂x9 ∆x9 = 0,
. . .
f9 + ∂f9/∂x1 ∆x1 + ∂f9/∂x2 ∆x2 + · · ·+ ∂f9/∂x9 ∆x9 = 0,
f1
f2
. . .
f9
+

∂f1/∂x1 ∂f1/∂x2 · · · ∂f1/∂x9
∂f2/∂x1 ∂f2/∂x2 · · · ∂f2/∂x9
. . .
∂f9/∂x1 ∂f9/∂x2 · · · ∂f9/∂x9


∆x1
∆x2
. . .
∆x9
= 0. (8.15)
Note now that the derivatives and the f ’s are all evaluated at known values of the xi’s, so only
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 143
the vector of the ∆xi values is unknown. We write this equation in matrix notation as
~f + F′ ~∆x = 0, ⇒ F′ ~∆x = −~f, (8.16)
~∆x =

∆x1
∆x2
. . .
∆x9
 , ~f =

f1
f2
. . .
f9
 , F′ =

∂f1/∂x1 · · · ∂f1/∂x9
∂f2/∂x1 · · · ∂f2/∂x9
. . .
∂f9/∂x1 · · · ∂f9/∂x9
 .
Here we use bold to emphasize the vector nature of the columns of fi and ∆xi values and
xmll
call the matrix of the derivatives F′ (it is also sometimes called J because it is the Jacobian
matrix).
The equation F′ ~∆x = −~f is in the standard form for the solution of a linear equation
(often written A~x = ~b), where ~∆x is the vector of unknowns and ~b = −~f . Matrix equations
are solved using the techniques of linear algebra, and in the sections to follow we shall show
how to do that on a computer. In a formal (and sometimes practical) sense, the solution of
(8.16) is obtained by multiplying both sides of the equation by the inverse of the F′ matrix:
~∆x = −F′−1 ~f, (8.17)
where the inverse must exist if there is to be a unique solution. Although we are dealing with
matrices now, this solution is identical in form to that of the 1-D problem, ∆x = −(1/f ′)f . In
fact, one of the reasons we use formal or abstract notation for matrices is to reveal the simplicity
that lies within.
As we indicated for the single-equation Newton–Raphson method, while for our two-
mass problem we can derive analytic expressions for the derivatives ∂fi/∂xj , there are
9 × 9 = 81 such derivatives for this (small) problem, and entering them all would be both
time-consuming and error-prone. In contrast, especially for more complicated problems, it is
straightforward to program a forward-difference approximation for the derivatives,
∂fi
∂xj
' fi(xj + ∆xj)− fi(xj)
δxj
, (8.18)
where each individual xj is varied independently since these are partial derivatives and δxj are
some arbitrary changes you input. While a central-difference approximation for the derivative
would be more accurate, it would also require more evaluations of the f ’s, and once we find a
solution it does not matter how accurate our algorithm for the derivative was.
As also discussed for the 1-D Newton–Raphson method (§7.10.1), the method can fail
if the initial guess is not close enough to the zero of f (here all N of them) for the f ’s to be
approximated as linear. The backtracking technique may be applied here as well, in the present
case, progressively decreasing the corrections ∆xi until |f |2 = |f1|2 + |f2|2 + · · · + |fN |2
decreases.
8.3 CLASSES OF MATRIX PROBLEMS (MATH)
It helps to remember that the rules of mathematics apply even to the world’s most powerful
computers. For example, you should have problems solving equations if you have more un-
knowns than equations or if your equations are not linearly independent. But do not fret. While
you cannot obtain a unique solution when there are not enough equations, you may still be able
to map out a space of allowable solutions. At the other extreme, if you have more equations
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
144 CHAPTER 8
than unknowns, you have an overdetermined problem, which may not have a unique solution.
An overdetermined problem is sometimes treated using data fitting in which a solution to a suf-
ficient set of equations is found, tested on the unused equations, and then improved if needed.
Not surprisingly, this latter technique is known as the linear least-squares method because it
finds the best solution “on the average.”
The most basic matrix problem is the system of linear equations you have to solve for
the two-mass problem:
A~x = ~b, AN×N ~xN×1 = ~bN×1, (8.19)
where A is a knownN×N matrix, ~x is an unknown vector of lengthN , and~b is a known vector
of length N . The best way to solve this equation is by Gaussian elimination or lower-upper
(LU) decomposition. This yields the vector ~x without explicitly calculating A−1. Another,
albeit slower and less robust, method is to determine the inverse of A and then form the solution
by multiplying both sides of (8.19) by A−1:
~x = A−1~b. (8.20)
Both the direct solution of (8.19) and the determination of a matrix’s inverse are standards in a
matrix subroutine library.
If you have to solve the matrix equation
A~x = λ~x, (8.21)
with ~x an unknown vector and λ an unknown parameter, then the direct solution (8.20) will not
be of much help because the matrix~b = λ~x contains the unknowns λ and ~x. Equation (8.21) is
the eigenvalue problem. It is harder to solve than (8.19) because solutions exist for only certain
λ values (or possibly none depending on A). We use the identity matrix to rewrite (8.21) as
[A− λI]~x = 0, (8.22)
and we see that multiplication by [A− λI]−1 yields the trivial solution
~x = 0 (trivial solution). (8.23)
While the trivial solution is a bona fide solution, it is trivial. A more interesting solution
requires the existence of a condition that forbids us from multiplying both sides of (8.22) by
[A − λI]−1. That condition is the nonexistence of the inverse, and if you recall that Cramer’s
rule for the inverse requires division by det[A − λI], it is clear that the inverse fails to exist
(and in this way eigenvalues do exist) when
det[A− λI] = 0. (8.24)
The λ values that satisfy this secular equation are the eigenvalues of (8.21).
If you are interested in only the eigenvalues, you should look for a matrix routine that
solves (8.24). To do that, first you need a subroutine to calculate the determinant of a matrix,
and then a search routine to zero in on the solution of (8.24). Such routines are available in
libraries. The traditional way to solve the eigenvalue problem (8.21) for both eigenvalues and
eigenvectors is by diagonalization. This is equivalent to successive changes of basis vectors,
each change leaving the eigenvalues unchanged while continually decreasing the values of
the off-diagonal elements of A. The sequence of transformations is equivalent to continually
operating on the original equation with a matrix U:
UA(U−1U)~x=λU~x, (8.25)
(UAU−1)(U~x) =λU~x, (8.26)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 145
until one is found for which UAU−1 is diagonal:
UAU−1 =

λ
′
1 · · · 0
0 λ
′
2 · · · 0
0 0 λ
′
3 · · ·
0 · · · λ′N
 . (8.27)
The diagonal values of UAU−1 are the eigenvalues with eigenvectors
~xi = U−1êi; (8.28)
that is, the eigenvectors are the columns of the matrix U−1. A number of routines of this type
are found in subroutine libraries.
8.3.1 Practical Matrix Computing
Many scientific programming bugs arise from the improper use of arrays.2 This may be due to
the extensive use of matrices in scientific computing or to the complexity of keeping track of
indices and dimensions. In any case, here are some rules of thumb to observe.
Computers are finite: Unless you are careful, your matrices will use so much memory that
your computation will slow down significantly, especially if it starts to use virtual memory.
As a case in point, let’s say that you store data in a 4-D array with each index having a
physical dimension of 100: A[100] [100] [100] [100]. This array of (100)4 64-byte words
occupies '1 GB of memory.
Processing time: Matrix operations such as inversion require on the order of N3 steps for
a square matrix of dimension N . Therefore, doubling the dimensions of a 2-D square
matrix (as happens when the number of integration steps is doubled) leads to an eightfold
increase in processing time.
Paging: Many operating systems have virtual memory in which disk space is used when
a program runs out of RAM (see Chapter 14, “High-Performance Computing Hardware,
Tuning, and Parallel Computing,” for a discussion of how computers arrange memory).
This is a slow process that requires writing a full page of words to the disk. If your
program is near the memory limit at which paging occurs, even a slight increase in a
matrix’s dimension may lead to an order-of-magnitude increase in execution time.
Matrix storage: While we think of matrices as multidimensional blocks of stored numbers,
the computer stores them as linear strings. For instance, a matrix a[3,3] in Python, a[3][3]
in Java is stored in row-major order (Figure 8.2 left):
a1,1 a1,2 a1,3 a2,1 a2,2 a2,3 a3,1 a3,2 a3,3 . . . ,
while in Fortran it is stored in column-major order (Figure 8.2 right):
a1,1 a2,1 a3,1 a1,2 a2,2 a3,2 a1,3 a2,3 a3,3 . . . .
It is important to keep this linear storage scheme in mind in order to write proper code
and to permit the mixing of Python and Fortran programs.
When dealing with matrices, you have to balance the clarity of the operations being per-
formed against the efficiency with which the computer performs them. For example,
having one matrix with many indices such as V[L,Nre,Nspin,k,kp,Z,A] may be neat pack-
aging, but it may require the computer to jump through large blocks of memory to get
to the particular values needed (large strides) as you vary k, kp, and Nre. The solution
2Even a vector V (N) is called an array, albeit a 1-D one.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
146 CHAPTER 8
Figure 8.2 Left: Row-major order used for matrix storage in Python and Java. Right: Column-major order used
for matrix storage in Fortran. How successive matrix elements are stored in a linear fashion in memory
is shown at the bottom.
a
a a
ab
b b
bc
c c
c
d
d d
de
e e
ef
f f
f
g
g g
gh
h h
hi
i i
i
Row Major Column Major
would be to have several matrices such as V1[Nre,Nspin,k,kp,Z,A], V2[Nre,Nspin,k,kp,Z,A],
and V3[Nre,Nspin,k,kp,Z,A].
Subscript 0: It is standard in Python, C and Java to have array indices begin with the value 0.
While this is now permitted in Fortran, the standard has been to start indices at 1. On that
account, in addition to the different locations in memory due to row-major and column-
major ordering, the same matrix element may be referenced differently in the different
languages:
Location Java/C Element Fortran Element
Lowest a[0][0] a(1,1)
a[0][1] a(2,1)
a[1][0] a(3,1)
a[1][1] a(1,2)
a[2][0] a(2,2)
Highest a[2][1] a(3,2)
Physical and logical dimensions: When you run a program, you issue commands such
as zeros((3,3), Float), double a[3][3]) or Dimension a(3,3) that tell the compiler how much
memory it needs to set aside for the array a. This is called physical memory. Some-
times you may use arrays without the full complement of values declared in the declaration
statements, for example, as a test case. The amount of memory you actually use to store
numbers is the matrix’s logical size.
Modern programming techniques permit dynamic memory allocation; that is, you may
use variables as the dimension of your arrays and read in the values of the variables at
run time. With these languages you should read in the sizes of your arrays at run time
and thus give them the same physical and logical sizes. However, Fortran77, which is
the language used for many library routines, requires the dimensions to be specified at
compile time, and so the physical and logical sizes may well differ. To see why care is
needed if the physical and logical sizes of the arrays differ, imagine that you declared
a[3][3] but defined elements only up to a[2][2]. Then the a in storage would look like
a[1][1]’ a[1][2]’ a[1][3] a[2][1]’ a[2][2]’ a[2][3] a[3][1] a[3][2] a[3][3],
where only the elements with primes have values assigned to them. Clearly, the defined a
values do not occupy sequential locations in memory, and so an algorithm processing this
matrix cannot assume that the next element in memory is the next element in your array.
This is the reason why subroutines from a library often need to know both the physical
and logical sizes of your arrays.
Passing sizes to subprograms : This is needed when the logical and physical dimensions
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 147
of arrays differ, as is true with some library routines but probably not with the programs
you write. In cases such as those using external libraries, you must also watch that the
sizes of your matrices do not exceed the bounds that have been declared in the subpro-
grams. This may occur without an error message and probably will give you the wrong
answers. In addition, if you are running a Python program that calls a Fortran subroutine,
you will need to pass pointers to variables and not the actual values of the variables to the
Fortran subprograms (Fortran makes reference calls, which means it deals with pointers
only as subprogram arguments). Here we have a program possibly running some data
stored nearby: 
main / / In main program
d imens ion a ( 1 0 0 ) , b ( 4 0 0 )
f u n c t i o n Sample ( a ) / / In s u b r o u t i n e
d imens ion a ( 1 0 ) / / Smal ler dimension
a ( 3 0 0 ) = 12 / / Out o f bounds , but no message
One way to ensure size compatibility among main programs and subroutines is to declare
array sizes only in your main program and then pass those sizes along to your subprograms
as arguments.
Equivalence, pointers, references manipulations : Once upon a time computers had
such limited memories that programmers conserved memory by having different variables
occupy the same memory location, the theory being that this would cause no harm as long
as these variables were not being used at the same time. This was done by the use of
Common and Equivalence statements in Fortran and by manipulations using pointers and
references in other languages. These types of manipulations are now obsolete (the bane
of object-oriented programming) and can cause endless grief; do not use them unless it is
a matter of “life or death”!
Say what’s happening: You decrease programming errors by using self-explanatory labels
for your indices (subscripts), stating what your variables mean, and describing your stor-
age schemes.
Tests: Always test a library routine on a small problem whose answer you know (such as
the exercises in §8.4.3). Then you’ll know if you are supplying it with the right arguments
and if you have all the links working.
8.3.2 Implementation: Scientific Libraries, WORLD WIDE WEB
Some major scientific and mathematical libraries available include the following:
NETLIB A WWW metalib of free ScaLAPACK Distributed memory
math libraries LAPACK
LAPACK Linear Algebra Pack JLAPACK LAPACK library in Java
SLATEC Comprehensive math and ESSL Engineering and Science
statistical pack Subroutine Library (IBM)
IMSL International Math and CERNLIB European Centre for
Statistical Libraries Nuclear Research Library
BLAS Basic Linear Algebra JAMA Java Matrix Library
NAG Numerical Algorithms LAPACK ++ Linear algebra in C++
Group (UK Labs)
TNT C++ Template Numerical GNU Scientific Full scientific libraries
Toolkit GSL in C and C++
Except for ESSL, IMSL, and NAG, all these libraries are in the public domain. However, even
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
148 CHAPTER 8
the proprietary ones are frequently available on a central computer or via an institutionwide
site license. General subroutine libraries are treasures to possess because they typically contain
optimized routines for almost everything you might want to do, such as
Linear algebra manipulations Matrix operations Interpolation, fitting
Eigensystem analysis Signal processing Sorting and searching
Solutions of linear equations Differential equations Roots, zeros, and extrema
Random-number operations Statistical functions Numerical quadrature
You can search the Web to find out about these libraries or to download one if it is not already
on your computer. Alternatively, an excellent place to start looking for a library is Netlib, a
repository of free software, documents, and databases of interest to computational scientists.
Linear Algebra Package (LAPACK) is a free, portable, modern (1990) library of
Fortran77 routines for solving the most common problems in numerical linear algebra. It is
designed to be efficient on a wide range of high-performance computers under the proviso that
the hardware vendor has implemented an efficient set of Basic Linear Algebra Subroutines
(BLAS). In contrast to LAPACK, the Sandia, Los Alamos, Air Force Weapons Laboratory
Technical Exchange Committee (SLATEC) library contains general-purpose mathematical and
statistical Fortran routines and is consequently more general. Nonetheless, it is not as tuned to
the architecture of a particular machine as is LAPACK.
Sometimes a subroutine library supplies only Fortran routines, and this requires a C pro-
grammer to call a Fortran routine (we describe how to do that in ??). In some cases, C-language
routines may also be available, but they may not be optimized for a particular machine.
As an example of what may be involved in using a scientific library, consider the
SLATEC library, which we recommend. The full library contains a guide, a table of con-
tents, and documentation via comments in the source code. The subroutines are classified by
the Guide to Available Mathematical Software (GAMS) system. For our masses-on-strings
problem we have found the needed routines:
snsq-s, dnsq-d Find zero of n-variable, nonlinear function
snsqe-s, dnsqe-d Easy-to-use snsq
If you extract these routines, you will find that they need the following:
enorm.f j4save.f r1mach.f xerprn.f fdjac1.f r1mpyq.f
xercnt.f xersve.f fdump.f qform.f r1updt.f xerhlt.f
xgetua.f dogleg.f i1mach.f qrfac.f snsq.f xermsg.f
Of particular interest in these “helper” routines, are i1mach.f, r1mach.f, and d1mach.f. They
tell LAPACK the characteristic of your particular machine when the library is first installed.
Without that knowledge, LAPACK does not know when convergence is obtained or what step
sizes to use.
8.4 NUMERICAL PYTHON AND PYTHON MATRIX LIBRARY
Numerical Python (numpy) (numpy.scipy.org) provides a fast, compact, multidimensional
array facility to Python. It is the successor to both Numeric and Numarray. In turn, SciPy
(www.scipy.org/) is an open source library of scientific tools for Python that supplements
NumPy. SciPy includes modules for linear algebra, optimization, integration, special functions,
signal and image processing, statistics, genetic algorithms, ODE solvers, and others. In this
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 149
section we will review some use of the array data type present in the Visual package, and in
§ 8.4.2 give some examples of the use of the LinearAlgebra package.
8.4.1 Arrays in Python:
Python interprets a sequence of ordered items, L = l0, l1, . . . , lN−1, as a list and represents it
with a single symbol L:
>>> L = [1, 2, 3, 4] Create list
>>> print L[0] Print first element
1
>>> print L Print the entire object
[1, 2, 3, 4] Python’s output
Square brackets [...] are used for lists, while round parenthesis (...) are used for sequences.
Lists are mutable or changeable, which make them more flexible; sequences cannot have their
elements changed. As we see in the print L[0] command, individual elements in a list are
referenced by indices, while in the print L command we see that the whole list can be refer-
enced as an object. While most languages use arrays to store sequences and require you
to specify the dimension of the array, Python lists are dynamic, which means they adjust their
sizes as needed. Python can perform a large number of operations on lists, for example,
Operation Effect Operation Effect
L = [1, 2, 3, 4] Form list L1 + L2 Concatenate lists
L[i] ith element len(L) Length of list L
i in L True if i in L L[i:j] Slice from i to j
for i in L Iteration index L.append(x) Append x to end of L
L.count(x) Number of x’s in L L.index(x) Location of 1st x in L
L.remove(x) Remove 1st x in L L.reverse() Reverse elements in L
L.sort() Order elements in L
Although present in Java, C and Fortran, Python itself does not have array as a data
type. It is, however, contained in the Visual package where it converts a sequence into a 1-D
array. We shall use Visual’s array command to form computer representations of vectors and
matrices (which means that you must import Visual in your programs). For example, here we
show the results of running our program Matrix.py from a shell:
>>> from visual import * Need Visual for arrays
>>> vector1 = array([1, 2, 3, 4, 5]) Fill 1-D array 1
>>> print ’vector1 = ’, vector1 Print entire array object
vector1 = [1 2 3 4 5] Output 3
>>> vector2 = vector1 + vector1 Add 2 vectors
>>> print ’vector1 + vector1 =’, vector2 Print array 5
vector1 + vector1 = [ 2 4 6 8 10] Output
>>> vector2 = 3 * vector1 Mult array by scalar 7
>>> print ’3 * vector1 = ’, vector2 Print 1-D array
3 * vector1 = [ 3 6 9 12 15] Output 9
>>> matrix1 = array(([0,1],[1,3])) An array of arrays
>>> print ’matrix1 = ’, matrix1 Print 2-D array 11
matrix1 = [[0 1] 13
[1 3]]
>>> print ’vector1.shape, matrix1.shape = ’, vector1.shape, matrix1.shape
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
150 CHAPTER 8
vector1.shape, matrix1.shape = (5,) (2, 2) Output of dimensions 16
>>> print ’matrix1 * matrix1 = ’, matrix1 * matrix1 Matrix multiply
matrix1 * matrix1 = [[0 1] Output 18
[1 9]] 20
On line 4 we add two 1-D array (vector) objects together and print out the answer to see that it
works as expected. Likewise we see on lines 7–9 that multiplication of an array by a constant
does in fact multiply each element. On line 10 we construct a “matrix” as a 1-D array of two
1-D arrays, and when we print it out we note that it does indeed look like a matrix. However,
on lines 16–20 we multiply two of these matrices together only to discover that the result is not
what one normally would expect from matrix multiplication.
8.4.2 LinearAlgebra.py Package
We have just seen that while Python is pretty good at treating an array object in an abstract
sense when we use 1-D arrays for vectors, Python does not create what we normally think of
as abstract matrices when we use 2-D arrays. Fortunately, there is the LinearAlgebra package
that treats 2-D arrays (a 1-D array of 1-D arrays) as abstract matrices, and also provides a
simple interface to the powerful LAPACK linear algebra library. We give some examples in
next, and recommend that you use these libraries rather than try to write your own matrix
routines.
Our first example of linear algebra is the standard matrix equation
A~x = ~b. (8.29)
This describes a set of linear equations with ~x an unknown vector and A a known matrix. We
take A to be 3 × 3, ~b to be 3 × 1, and let the program figure out that ~x is a 3 × 1 vector3. We
start by importing all the packages, by inputting a matrix and a vector, and then by printing out
A and ~x:
>>> from visual import *; from Numeric import*; from LinearAlgebra import*
>>> A = array( [ [1,2,3], [22,32,42]], [55,66,100]] ) An array of arrays
>>> print ’A =’,A
A = [[ 1 2 3]
[ 22 32 42]
[ 55 66 100]]
>>> b = array([1,2,3])
>>> print ’b =’,b
b = [1 2 3]
We now solve A~x = ~b using the solve linear equations command, and test how close
A~x−~b comes to a zero vector:
>>> x = solve linear equations(A, b) Does solution
>>> print ’x =’, x
x = [ -1.4057971 -0.1884058 0.92753623]
>>> print ’Residual =’, matrixmultiply(A, x) - b LHS-RHS
Residual = [2.22044605e-016 1.77635684e-015 1.19904087e-014] Tiny
This is really quite impressive. We have solved the entire set of linear equations (by elim-
ination) with just the single command solve linear equations, performed a matrix multi-
3Don’t be bothered by the fact that although we think as these vectors as 3× 1, they get printed out as 1× 3.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 151
plication with the single command matrixmultiply, did a matrix subtraction with the usual
subtraction command, and obtained close to machine precision from the numerics.
Although not as efficient numerically, a direct way of solving A~x = ~b is by calculating
the inverse A−1 and then multiplying by the inverse, ~x = A−1~b:
>>> print matrixmultiply(inverse(A), A) Test out inverse
[[1.00000000e+000 1.13797860e-015 1.47624968e-015] Identity matrix?
[-1.38777878e-017 1.00000000e+000 -8.88178420e-016]
[-6.24500451e-016 -4.16333634e-016 1.00000000e+000]]
>>> print ’x =’, matrixmultiply(inverse(A), b)
x = [-1.4057971 -0.1884058 0.92753623] Solution
Here we first tested that inverse(A) finds the inverse of matrix A by seeing if A times
inverse(A) equals the identity matrix. Then we used the inverse to solve the matrix equa-
tion directly, and, within precision, got the same answer as before.
Our second example arises in the solution for the principal-axes system for a cube and
requires us to find a coordinate system in which the inertia tensor is diagonal. This entails
solving the eigenvalue problem:
I ~ω = λ~ω, (8.30)
where I is the inertia matrix, ~ω is an eigenvector, and λ is an eigenvalue. The program Eigen.py
in Listing 8.1 solves for the eigenvalues and vectors and produces output of the form
I = [[ 0.66666667 -0.25 -0.25 ]
[-0.25 0.66666667 -0.25 ]
[-0.25 -0.25 0.66666667]]
Eigenvalues = [ 0.91666667 0.16666667 0.91666667]
Matrix of Eigenvectors = [[ 0.81649658 -0.40824829 -0.40824829]
[-0.57735027 -0.57735027 -0.57735027]
[ 0.43514263 -0.81589244 0.38074981]]
LHS - RHS = [ 1.11022302e-016 -5.55111512e-017 -1.11022302e-016]
Look at Eigen.py and notice how on line 5 we set up the array I with all the elements of
the inertia tensor and on line 8 we solve for its eigenvalues and eigenvectors. On line 12 we
extract the first eigenvector, and use it with the first eigenvalue to check if we have a solution
by comparing the right and left sides of (8.30).
Listing 8.1 Eigen.py uses the LinearAlgebra package and Visual’s array to solve the eigenvalue problem. Note
the matrix operations eigenvectors and matrixmultiply. 
# Eigen . py S o l u t i o n o f m a t r i x e i g e n v a l u e problem
2
from v i s u a l i m p o r t ∗ ; from Numeric i m p o r t ∗ ; from L i n e a r A l g e b r a i m p o r t∗
4
I = a r r a y ( [ [ 2 . / 3 , −1 . / 4 , −1 . / 4 ] , [ −1 . / 4 , 2 . / 3 , −1 . / 4 ] , [ −1 . / 4 , −1 . / 4 , 2 . / 3 ] ] )
6p r i n t ’\n I =\n’ , I
8Es , e v e c t o r s = e i g e n v e c t o r s ( I ) # S o l v e s e i g e n v a l u e problem
p r i n t ’\n Eigenvalues = \n’ , Es
10p r i n t ’\n Matrix of Eigenvectors =\n’ , e v e c t o r s
12Vec = a r r a y ( [ e v e c t o r s [ 0 , 0 ] , e v e c t o r s [ 0 , 1 ] , e v e c t o r s [ 0 , 2 ] ] )
p r i n t ’\n A single eigenvector to test RHS vs LHS =’ , Vec , ’\n’
14
LHS = m a t r i x m u l t i p l y ( I , Vec )
16RHS = m a t r i x m u l t i p l y ( Vec , Es [ 0 ] )
p r i n t ’LHS - RHS =\n’ , LHS−RHS
18p r i n t "Press a character to finish"
s= r a w i n p u t ( )
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
152 CHAPTER 8
8.4.3 Exercises for Testing Matrix Calls
Before you direct the computer to go off crunching numbers on a million elements of some
xmll
matrix, it’s a good idea for you to try out your procedures on a small matrix, especially one for
which you know the right answer. In this way it will take you only a short time to realize how
hard it is to get the calling procedure perfectly right! Here are some exercises.
1. Find the inverse of A =
+4 −2 +1+3 +6 −4
+2 +1 +8
.
a. As a general procedure, applicable even if you do not know the analytic answer, check
your inverse in both directions; that is, check that AA−1 = A−1A = I.
b. Verify that A−1 =
1
263
+52 +17 +2−32 +30 +19
−9 −8 +30
.
2. Consider the same matrix A as before, now used to describe three simultaneous linear
equations, A~x = ~b, or explicitly,a11 a12 a13a21 a22 a23
a31 a32 a33
x1x2
x3
 =
b1b2
b3
.
Here the vector~b on the RHS is assumed known, and the problem is to solve for the
vector ~x. Use an appropriate subroutine to solve these equations for the three different ~x
vectors appropriate to these three different~b values on the RHS:
~b1 =
+12−25
+32
, ~b2 =
 +4−10
+22
, ~b3 =
+20−30
+40
.
The solutions should be
~x1 =
+1−2
+4
, ~x2 =
+0.312−0.038
+2.677
, ~x3 =
+2.319−2.965
+4.790
.
3. Consider the matrix A =
(
α β
−β α
)
, where you are free to use any values you want
for α and β. Use a numerical eigenproblem solver to show that the eigenvalues and
eigenvectors are the complex conjugates
~x1,2 =
(
+1
∓i
)
, λ1,2 = α∓ iβ.
4. Use your eigenproblem solver to find the eigenvalues of the matrix
A =
−2 +2 −3+2 +1 −6
−1 −2 +0
 .
a. Verify that you obtain the eigenvalues λ1 = 5, λ2 = λ3 = −3. Notice that double
roots can cause problems. In particular, there is a uniqueness problem with their
eigenvectors because any combination of these eigenvectors is also an eigenvector.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 153
b. Verify that the eigenvector for λ1 = 5 is proportional to
~x1 =
1√
6
−1−2
+1
 .
c. The eigenvalue −3 corresponds to a double root. This means that the correspond-
ing eigenvectors are degenerate, which in turn means that they are not unique. Two
linearly independent ones are
~x2 =
1√
5
−2+1
+0
 , ~x3 = 1√
10
30
1
 .
In this case it’s not clear what your eigenproblem solver will give for the eigenvectors.
Try to find a relationship between your computed eigenvectors with the eigenvalue−3
and these two linearly independent ones.
5. Your model of some physical system results in N = 100 coupled linear equations in N
unknowns:
a11y1 + a12y2 + · · ·+ a1NyN = b1,
a21y1 + a22y2 + · · ·+ a2NyN = b2,
· · ·
aN1y1 + aN2y2 + · · ·+ aNNyN = bN .
In many cases, the a and b values are known, so your exercise is to solve for all the x
values, taking a as the Hilbert matrix and~b as its first row:
[aij ] = a =
[
1
i+ j − 1
]
=

1 12
1
3
1
4 · · ·
1
100
1
2
1
3
1
4
1
5 · · ·
1
101
. . .
1
100
1
101 · · · · · ·
1
199
 ,
[bi] = ~b =
[
1
i
]
=

1
1
2
1
3
. . .
1
100

.
Compare to the analytic solution 
y1
y2
. . .
yN
 =

1
0
. . .
0
 .
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
154 CHAPTER 8
8.4.4 Matrix Solution of the String Problem
We have now set up the solution to our problem of two masses on a string and have the matrix
tools needed to solve it. Your problem is to check out the physical reasonableness of the
solution for a variety of weights and lengths. You should check that the deduced tensions are
positive and that the deduced angles correspond to a physical geometry (e.g., with a sketch).
Since this is a physics-based problem, we know that the sine and cosine functions must be
less than 1 in magnitude and that the tensions should be similar in magnitude to the weights
of the spheres. Our solution is given in NewtonNDanimate.py, which shows graphically the
step-by-step search for solution.
Listing 8.2 The code NewtonNDanimate.py that shows the step-by-step search for solution of the two-mass-on-a-
string problem via a Newton-Raphson search. 
# NewtonND . py Mul t iD imens ion Newton S ea rc h
from v i s u a l i m p o r t ∗
from Numeric i m p o r t∗
from L i n e a r A l g e b r a i m p o r t∗
s c e n e = d i s p l a y ( x =0 , y =0 , wid th =500 , h e i g h t =500 ,
t i t l e =’String and masses configuration’ )
tempe = c u r v e ( x= r a n g e ( 0 , 5 0 0 ) , c o l o r = c o l o r . b l a c k )
n = 9
eps = 1e−6
d e r i v = z e r o s ( ( n , n ) , F l o a t )
# dx = z e r o s ( ( n ) , F l o a t )
f = z e r o s ( ( n ) , F l o a t )
x = a r r a y ( [ 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 , 1 . , 1 . , 1 . ] )
d e f p l o t c o n f i g ( ) :
f o r o b j i n s c e n e . o b j e c t s :
o b j . v i s i b l e =0 # t o e r a s e t h e p r e v i o u s c o n f i g u r a t i o n
L1 =3 .0
L2 =4 .0
L3 =4 .0
xa=L1∗x [ 3 ] #L1∗cos ( t h 1 )
ya=L1∗x [ 0 ] #L1 s i n ( t h 1 )
xb=xa+L2∗x [ 4 ] #L1∗cos ( t h 1 ) +L2∗cos ( t h 2 )
yb=ya+L2∗x [ 1 ] #L1∗ s i n ( t h 1 ) +L2∗ sen ( t h 2 )
xc=xb+L3∗x [ 5 ] #L1∗cos ( t h 1 ) +L2∗cos ( t h 2 ) +L3∗cos ( t h 3 )
yc=yb−L3∗x [ 2 ] #L1∗ s i n ( t h 1 ) +L2∗ sen ( t h 2 )−L3∗ s i n ( t h 3 )
mx=100.0 # f o r l i n e a r c o o r d i n a t e t r a n s f o r m a t i o n
bx=−500.0 # from 0=< x =<10
my=−100.0 # t o −500 =<x window=>500
by =500.0 #same t r a n s f o r m a t i o n f o r y
xap=mx∗xa+bx # t o keep a s p e c t r a t i o
yap=my∗ya+by
b a l l 1 = s p h e r e ( pos =( xap , yap ) , c o l o r = c o l o r . cyan , r a d i u s =15)
xbp=mx∗xb+bx
ybp=my∗yb+by
b a l l 2 = s p h e r e ( pos =( xbp , ybp ) , c o l o r = c o l o r . cyan , r a d i u s =25)
xcp=mx∗xc+bx
ycp=my∗yc+by
x0=mx∗0+bx
y0=my∗0+by
l i n e 1 = c u r v e ( pos = [ ( x0 , y0 ) , ( xap , yap ) ] , c o l o r = c o l o r . ye l low , r a d i u s =4)
l i n e 2 = c u r v e ( pos = [ ( xap , yap ) , ( xbp , ybp ) ] , c o l o r = c o l o r . ye l low , r a d i u s =4)
l i n e 3 = c u r v e ( pos = [ ( xbp , ybp ) , ( xcp , ycp ) ] , c o l o r = c o l o r . ye l low , r a d i u s =4)
t o p l i n e = c u r v e ( pos = [ ( x0 , y0 ) , ( xcp , ycp ) ] , c o l o r = c o l o r . red , r a d i u s =4)
d e f F ( x , f ) : # D ef in e F f u n c t i o n
f [ 0 ] = 3∗x [ 3 ] + 4∗x [ 4 ] + 4∗x [ 5 ] − 8 . 0
f [ 1 ] = 3∗x [ 0 ] + 4∗x [ 1 ] − 4∗x [ 2 ]
f [ 2 ] = x [ 6 ]∗ x [ 0 ] − x [ 7 ]∗ x [ 1 ] − 1 0 . 0
f [ 3 ] = x [ 6 ]∗ x [ 3 ] − x [ 7 ]∗ x [ 4 ]
f [ 4 ] = x [ 7 ]∗ x [ 1 ] + x [ 8 ]∗ x [ 2 ] − 2 0 . 0
f [ 5 ] = x [ 7 ]∗ x [ 4 ] − x [ 8 ]∗ x [ 5 ]
f [ 6 ] = math . pow ( x [ 0 ] , 2 ) + math . pow ( x [ 3 ] , 2 ) − 1 . 0
f [ 7 ] = math . pow ( x [ 1 ] , 2 ) + math . pow ( x [ 4 ] , 2 ) − 1 . 0
f [ 8 ] = math . pow ( x [ 2 ] , 2 ) + math . pow ( x [ 5 ] , 2 ) − 1 . 0
d e f d F i d X j ( x , d e r i v , n ) : # D ef in e d e r i v a t i v e f u n c t i o n
h = 1e−4
f o r j i n r a n g e ( 0 , n ) :
temp = x [ j ]
x [ j ] = x [ j ] + h / 2 .
F ( x , f )
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 155
f o r i i n r a n g e ( 0 , n ) : d e r i v [ i , j ] = f [ i ]
x [ j ] = temp
f o r j i n r a n g e ( 0 , n ) :
temp = x [ j ]
x [ j ] = x [ j ] − h / 2 .
F ( x , f )
f o r i i n r a n g e ( 0 , n ) : d e r i v [ i , j ] = ( d e r i v [ i , j ] − f [ i ] ) / h
x [ j ] = temp
f o r i t i n r a n g e ( 1 , 100) :
r a t e ( 1 )
F ( x , f )
d F i d Xj ( x , d e r i v , n )
B = a r r a y ( [ [ − f [ 0 ] ] , [ − f [ 1 ] ] , [ − f [ 2 ] ] , [ − f [ 3 ] ] , [ − f [ 4 ] ] , [ − f [ 5 ] ] , [ − f [ 6 ] ] , [ −
f [ 7 ] ] , [ − f [ 8 ] ] ] )
s o l = s o l v e l i n e a r e q u a t i o n s ( d e r i v , B)
dx = t a k e ( s o l , ( 0 , ) , 1 ) # t a k e t h e f i r s t column of m a t r i x s o l
f o r i i n r a n g e ( 0 , n ) :
x [ i ] = x [ i ] + dx [ i ]
p l o t c o n f i g ( )
e r rX = e r r F = e r r X i = 0 . 0
f o r i i n r a n g e ( 0 , n ) :
i f ( x [ i ] != 0 . ) : e r r X i = abs ( dx [ i ] / x [ i ] )
e l s e : e r r X i = abs ( dx [ i ] )
i f ( e r r X i > er rX ) : e r rX = e r r X i
i f ( abs ( f [ i ] ) > e r r F ) : e r r F = abs ( f [ i ] )
i f ( ( e r rX <= eps ) and ( e r r F <= eps ) ) : break
p r i n t ’Number of iterations = ’ , i t
p r i n t ’Solution:’
f o r i i n r a n g e ( 0 , n ) :
p r i n t ’x[’ , i , ’] = ’ , x [ i ]
8.4.5 Explorations
1. See at what point your initial guess gets so bad that the computer is unable to find a
physical solution.
2. A possible problem with the formalism we have just laid out is that by incorporating the
identity sin2 θi + cos2 θi = 1 into the equations we may be discarding some information
about the sign of sin θ or cos θ. If you look at Figure 8.1, you can observe that for some
values of the weights and lengths, θ2 may turn out to be negative, yet cos θ should remain
positive. We can build this condition into our equations by replacing f7 − f9 with f ’s
based on the form
f7 = x4 −
√
1− x21, f8 = x5 −
√
1− x22, f9 = x6 −
√
1− x23. (8.31)
See if this makes any difference in the solutions obtained.
2. Solve the similar three-mass problem. The approach is the same, but the number of
equations is larger.
8.5 UNIT II. DATA FITTING
Data fitting is an art worthy of serious study by all scientists. In this unit we just scratch
the surface by examining how to interpolate within a table of numbers and how to do a least-
squares fit to data. We also show how to go about making a least-squares fit to nonlinear
functions using some of the search techniques and subroutine libraries we have already dis-
cussed.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
156 CHAPTER 8
8.6 FITTING AN EXPERIMENTAL SPECTRUM (PROBLEM)
Problem: The cross sections measured for the resonant scattering of a neutron from a nucleus
are given in Table 8.1 along with the measurement number (index), the energy, and the experi-
mental error. Your problem is to determine values for the cross sections at energy values lying
between those in the table.
You can solve this problem in a number of ways. The simplest is to numerically inter-
polate between the values of the experimental f(Ei) given in Table 8.1. This is direct and
easy but does not account for there being experimental noise in the data. A more appropriate
way to solve this problem (discussed in §8.8) is to find the best fit of a theoretical function to
the data. We start with what we believe to be the “correct” theoretical description of the data,
f(E) =
fr
(E − Er)2 + Γ2/4
, (8.32)
where fr, Er, and Γ are unknown parameters. We then adjust the parameters to obtain the best
fit. This is a best fit in a statistical sense but in fact may not pass through all (or any) of the
data points. For an easy, yet effective, introduction to statistical data analysis, we recommend
[B&R 02].
These two techniques of interpolation and least-squares fitting are powerful tools that let
you treat tables of numbers as if they were analytic functions and sometimes let you deduce
statistically meaningful constants or conclusions from measurements. In general, you can view
data fitting as global or local. In global fits, a single function in x is used to represent the entire
set of numbers in a table like Table 8.1. While it may be spiritually satisfying to find a single
function that passes through all the data points, if that function is not the correct function for
describing the data, the fit may show nonphysical behavior (such as large oscillations) between
the data points. The rule of thumb is that if you must interpolate, keep it local and view global
interpolations with a critical eye.
8.6.1 Lagrange Interpolation (Method)
Consider Table 8.1 as ordered data that we wish to interpolate. We call the independent vari-
able x and its tabulated values xi(i = 1, 2, . . .), and we assume that the dependent variable is
the function g(x), with tabulated values gi = g(xi). We assume that g(x) can be approximated
as a (n− 1)-degree polynomial in each interval i:
gi(x) ' a0 + a1x+ a2x2 + · · ·+ an−1xn−1, (x ' xi). (8.33)
Because our fit is local, we do not assume that one g(x) can fit all the data in the table but
instead use a different polynomial, that is, a different set of ai values, for each region of the
table. While each polynomial is of low degree, multiple polynomials are used to span the entire
table. If some care is taken, the set of polynomials so obtained will behave well enough to be
used in further calculations without introducing much unwanted noise or discontinuities.
The classic interpolation formula was created by Lagrange. He figured out a closed-
form one that directly fits the (n− 1)-order polynomial (8.33) to n values of the function g(x)
evaluated at the points xi. The formula is written as the sum of polynomials:
xmll
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 157
Table 8.1 Experimental values for a scattering cross section f(E) as a function of energy.
i 1 2 3 4 5 6 7 8 9
Ei (MeV) [≡ xi] 0 25 50 75 100 125 150 175 200
g(Ei) (mb) 10.6 16.0 45.0 83.5 52.8 19.9 10.8 8.25 4.7
Error = ±σi (mb) 9.34 17.9 41.5 85.5 51.5 21.5 10.8 6.29 4.14
g(x)' g1λ1(x) + g2λ2(x) + · · ·+ gnλn(x), (8.34)
λi(x) =
n∏
j(6=i)=1
x− xj
xi − xj
=
x− x1
xi − x1
x− x2
xi − x2
· · · x− xn
xi − xn
. (8.35)
For three points, (8.34) provides a second-degree polynomial, while for eight points it gives a
seventh-degree polynomial. For example, here we use a four-point Lagrange interpolation to
determine a third-order polynomial that reproduces each of the tabulated values:
x1−4 = (0, 1, 2, 4) g1−4 = (−12,−12,−24,−60), (8.36)
g(x) =
(x− 1)(x− 2)(x− 4)
(0− 1)(0− 2)(0− 4)
(−12) + x(x− 2)(x− 4)
(1− 0)(1− 2)(1− 4)
(−12)
+
x(x− 1)(x− 4)
(2− 0)(2− 1)(2− 4)
(−24) + x(x− 1)(x− 2)
(4− 0)(4− 1)(4− 2)
(−60),
⇒ g(x) =x3 − 9x2 + 8x− 12. (8.37)
As a check we see that
g(4) = 43 − 9(42) + 32− 12 = −60, g(0.5) = −10.125. (8.38)
If the data contain little noise, this polynomial can be used with some confidence within the
range of the data, but with risk beyond the range of the data.
Notice that Lagrange interpolation makes no restriction that the points in the table be
evenly spaced. As a check, it is also worth noting that the sum of the Lagrange multipliers
equals one,
∑n
i=1 λi = 1. Usually the Lagrange fit is made to only a small region of the table
with a small value of n, even though the formula works perfectly well for fitting a high-degree
polynomial to the entire table. The difference between the value of the polynomial evaluated
at some x and that of the actual function is equal to the remainder
Rn '
(x− x1)(x− x2) · · · (x− xn)
n!
g(n)(ζ), (8.39)
where ζ lies somewhere in the interpolation interval but is otherwise undetermined. This shows
that if significant high derivatives exist in g(x), then it cannot be approximated well by a
polynomial. In particular, if g(x) is a table of experimental data, it is likely to contain noise,
and then it is a bad idea to fit a curve through all the data points.
8.6.2 Lagrange Implementation, Assessment
Consider the experimental neutron scattering data in Table 8.1. The expected theoretical func-
tional form that describes these data is (8.32), and our empirical fits to these data are shown in
Figure 8.3.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
158 CHAPTER 8
Figure 8.3 Three fits to cross-section data. Short dashed line: Lagrange interpolation using an eight-degree polyno-
mial that passes through all the data points but has nonphysical oscillations between points; solid line:
cubic splines (smooth but not accurate); dashed line: Least-squares parabola fit (a best fit with a bad the-
ory). The best approach is to do a least-squares fit of the correct theoretical function, the Breit–Wigner
method (8.32).
0 50 100 150 200
E (MeV)
0
20
40
60
80
C
ro
s
s
 S
e
c
ti
o
n
Data
Lagrange
Cubic Splines
Parabola (lst sq)
1. Write a subroutine to perform an n-point Lagrange interpolation using (8.34). Treat n as
an arbitrary input parameter. (You can also do this exercise with the spline fits discussed
in § 8.6.4.)
2. Use the Lagrange interpolation formula to fit the entire experimental spectrum with one
polynomial. (This means that you must fit all nine data points with an eight-degree
polynomial.) Then use this fit to plot the cross section in steps of 5 MeV.
3. Use your graph to deduce the resonance energy Er (your peak position) and Γ (the full
width at half-maximum). Compare your results with those predicted by our theorist
friend, (Er,Γ) = (78, 55) MeV.
4. A more realistic use of Lagrange interpolation is for local interpolation with a small
number of points, such as three. Interpolate the preceding cross-sectional data in 5-
MeV steps using three-point Lagrange interpolation. (Note that the end intervals may be
special cases.)
This example shows how easy it is to go wrong with a high-degree-polynomial fit. Although the
polynomial is guaranteed to pass through all the data points, the representation of the function
away from these points can be quite unrealistic. Using a low-order interpolation formula, say,
n = 2 or 3, in each interval usually eliminates the wild oscillations. If these local fits are
then matched together, as we discuss in the next section, a rather continuous curve results.
Nonetheless, you must recall that if the data contain errors, a curve that actually passes through
them may lead you astray. We discuss how to do this properly in §8.8.
8.6.3 Explore Extrapolation
We deliberately have not discussed extrapolation of data because it can lead to serious system-
atic errors; the answer you get may well depend more on the function you assume than on the
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 159
data you input. Add some adventure to your life and use the programs you have written to
extrapolate to values outside Table 8.1. Compare your results to the theoretical Breit–Wigner
shape (8.32).
8.6.4 Cubic Splines (Method)
If you tried to interpolate the resonant cross section with Lagrange interpolation, then you saw
that fitting parabolas (three-point interpolation) within a table may avoid the erroneous and
possibly catastrophic deviations of a high-order formula. (A two-point interpolation, which
connects the points with straight lines, may not lead you far astray, but it is rarely pleasing to
the eye or precise.) A sophisticated variation of an n = 4 interpolation, known as cubic splines,
often leads to surprisingly eye-pleasing fits. In this approach (Figure 8.3), cubic polynomials
are fit to the function in each interval, with the additional constraint that the first and second
derivatives of the polynomials be continuous from one interval to the next. This continuity of
slope and curvature is what makes the spline fit particularly eye-pleasing. It is analogous to
what happens when you use the flexible spline drafting tool (a lead wire within a rubber sheath)
from which the method draws its name.
The series of cubic polynomials obtained by spline-fitting a table of data can be inte-
grated and differentiated and is guaranteed to have well-behaved derivatives. The existence
of meaningful derivatives is an important consideration. As a case in point, if the interpo-
lated function is a potential, you can take the derivative to obtain the force. The complexity
of simultaneously matching polynomials and their derivatives over all the interpolation points
leads to many simultaneous linear equations to be solved. This makes splines unattractive for
hand calculation, yet easy for computers and, not surprisingly, popular in both calculations
and graphics. To illustrate, the smooth curves connecting points in most “draw” programs are
usually splines, as is the solid curve in Figure 8.3.
The basic approximation of splines is the representation of the function g(x) in the subin-
terval [xi, xi+1] with a cubic polynomial:
g(x)' gi(x), for xi ≤ x ≤ xi+1, (8.40)
gi(x) = gi + g′i(x− xi) +
1
2
g′′i (x− xi)2 +
1
6
g′′′i (x− xi)3. (8.41)
This representation makes it clear that the coefficients in the polynomial equal the values of
g(x) and its first, second, and third derivatives at the tabulated points xi. Derivatives beyond
the third vanish for a cubic. The computational chore is to determine these derivatives in terms
of the N tabulated gi values. The matching of gi at the nodes that connect one interval to the
next provides the equations
gi(xi+1) = gi+1(xi+1), i = 1, N − 1. (8.42)
The matching of the first and second derivatives at each interval’s boundaries provides the
equations
g′i−1(xi) = g
′
i(xi), g
′′
i−1(xi) = g
′′
i (xi). (8.43)
The additional equations needed to determine all constants is obtained by matching the third
derivatives at adjacent nodes. Values for the third derivatives are found by approximating them
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
160 CHAPTER 8
in terms of the second derivatives:
g′′′i '
g′′i+1 − g′′i
xi+1 − xi
. (8.44)
As discussed in Chapter 7, “Differentiation & Searching,” a central-difference approximation
would be better than a forward-difference approximation, yet (8.44) keeps the equations sim-
pler.
It is straightforward though complicated to solve for all the parameters in (8.41). We
leave that to other reference sources [Thom 92, Pres 94]. We can see, however, that matching
at the boundaries of the intervals results in only (N − 2) linear equations for N unknowns.
Further input is required. It usually is taken to be the boundary conditions at the endpoints
a = x1 and b = xN , specifically, the second derivatives g′′(a) and g′′(b). There are several
ways to determine these second derivatives:
Natural spline: Set g′′(a) = g′′(b) = 0; that is, permit the function to have a slope at
the endpoints but no curvature. This is “natural” because the derivative vanishes for the
flexible spline drafting tool (its ends being free).
Input values for g′ at the boundaries: The computer uses g′(a) to approximate g′′(a). If
you do not know the first derivatives, you can calculate them numerically from the table
of gi values.
Input values for g′′ at the boundaries: Knowing values is of course better than approxi-
mating values, but it requires the user to input information. If the values of g′′ are not
known, they can be approximated by applying a forward-difference approximation to the
tabulated values:
g′′(x) ' [g(x3)− g(x2)]/[x3 − x2]− [g(x2)− g(x1)]/[x2 − x1]
[x3 − x1]/2
. (8.45)
8.6.4.1 Cubic Spline Quadrature (Exploration)
A powerful integration scheme is to fit an integrand with splines and then integrate the cubic
polynomials analytically. If the integrand g(x) is known only at its tabulated values, then this
is about as good an integration scheme as is possible; if you have the ability to calculate the
function directly for arbitrary x, Gaussian quadrature may be preferable. We know that the
spline fit to g in each interval is the cubic (8.41)
g(x) ' gi + g′i(x− xi) +
1
2
g′′i (x− xi)2 +
1
6
g′′′i (x− xi)3. (8.46)
It is easy to integrate this to obtain the integral of g for this interval and then to sum over all
intervals: ∫ xi+1
xi
g(x) dx'
(
gix+
1
2
g′ix
2
i +
1
6
g′′i x
3 +
1
24
g′′′i x
4
)∣∣∣∣xi+1
xi
, (8.47)
∫ xk
xj
g(x) dx=
k∑
i=j
(
gix+
1
2
g′ix
2
i +
1
6
g′′i x
3 +
1
24
g′′′i x
4
)∣∣∣∣xi+1
xi
. (8.48)
Making the intervals smaller does not necessarily increase precision, as subtractive cancella-
tions in (8.47) may get large.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 161
8.6.5 Spline Fit of Cross Section (Implementation)
Fitting a series of cubics to data is a little complicated to program yourself, so we recom-
mend using a library routine. While we have found quite a few Java-based spline appli-
cations available on the internet, none seemed appropriate for interpreting a simple set of
numbers. That being the case, we have adapted the splint.c and the spline.c functions
fromSplineInteract.py program shown in Listing 8.3 (there is also an applet version on the
CD). Your problem now is to carry out the assessment in § 8.6.2 using cubic spline interpola-
tion rather than Lagrange interpolation.
8.7 FITTING EXPONENTIAL DECAY (PROBLEM)
Figure 8.4 presents actual experimental data on the number of decays ∆N of the π meson as
a function of time [Stez 73]. Notice that the time has been “binned” into ∆t = 10-ns intervals
and that the smooth curve is the theoretical exponential decay expected for very large numbers.
Your problem is to deduce the lifetime τ of the π meson from these data (the tabulated lifetime
of the pion is 2.6× 10−8 s).
8.7.1 Theory to Fit
Assume that we start with N0 particles at time t = 0 that can decay to other particles.4 If we
wait a short time ∆t, then a small number ∆N of the particles will decay spontaneously, that
is, with no external influences. This decay is a stochastic process, which means that there is
an element of chance involved in just when a decay will occur, and so no two experiments are
expected to give exactly the same results. The basic law of nature for spontaneous decay is that
the number of decays ∆N in a time interval ∆t is proportional to the number of particles N(t)
present at that time and to the time interval
∆N(t) = −1
τ
N(t)∆t ⇒ ∆N(t)
∆t
= −λN(t). (8.49)
Listing 8.3 SplineInteract.py performs a cubic spline fit to data and permits interactive control. The arrays x[]
and y[] are the data to fit, and the values of the fit at Nfit points are output. 
# S p l i n e I n t e r a c t . py S p l i n e f i t w i th s l i d e t o c o n t r o l number o f p o i n t s
from v i s u a l i m p o r t ∗ ; from v i s u a l . g raph i m p o r t ∗ ;
from v i s u a l . g raph i m p o r t g d i s p l a y , gc u r ve
from v i s u a l . c o n t r o l s i m p o r t s l i d e r , c o n t r o l s , t o g g l e
x = a r r a y ( [ 0 . , 0 . 1 2 , 0 . 2 5 , 0 . 3 7 , 0 . 5 , 0 . 6 2 , 0 . 7 5 , 0 . 8 7 , 0 . 9 9 ] ) # i n p u t
y = a r r a y ( [ 1 0 . 6 , 1 6 . 0 , 4 5 . 0 , 8 3 . 5 , 5 2 . 8 , 1 9 . 9 , 1 0 . 8 , 8 . 2 5 , 4 . 7 ] )
n = 9 ; np = 15
# I n i t i a l i z e
y2 = z e r o s ( ( n ) , F l o a t ) ; u = z e r o s ( ( n ) , F l o a t )
g raph1 = g d i s p l a y ( x =0 , y =0 , wid th =500 , h e i g h t =500 ,
t i t l e =’Spline Fit’ , x t i t l e =’x’ , y t i t l e =’y’ )
f u n c t 1 = g d o t s ( c o l o r = c o l o r . y e l l o w )
f u n c t 2 = g d o t s ( c o l o r = c o l o r . r e d )
g raph1 . v i s i b l e = 0
d e f u p d a t e ( ) : # N f i t = 30 # N o u t p u t p t s
N f i t = c o n t r o l . v a l u e
4Spontaneous decay is discussed further and simulated in § 5.5.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
162 CHAPTER 8
f o r i i n r a n g e ( 0 , n ) : # Spread o u t p o i n t s t o make them s t a n d o u t
f u n c t 1 . p l o t ( pos = ( x [ i ] , y [ i ] ) )
f u n c t 1 . p l o t ( pos = ( 1 . 0 1∗ x [ i ] , 1 .01∗ y [ i ] ) )
f u n c t 1 . p l o t ( pos = ( . 9 9∗ x [ i ] , .99∗ y [ i ] ) )
yp1 = ( y[1]−y [ 0 ] ) / ( x[1]−x [ 0 ] )−(y[2]−y [ 1 ] ) / ( x[2]−x [ 1 ] ) +( y[2]−y [ 0 ] ) / ( x[2]−x [ 0 ] )
ypn = ( y [ n−1] − y [ n−2]) / ( x [ n−1] − x [ n−2]) − ( y [ n−2] − y [ n−3]) / ( x [ n−2] − x [ n−3]) + ( y [ n−1] −
y [ n−3]) / ( x [ n − 1] − x [ n − 3 ] )
i f ( yp1 > 0 . 9 9 e30 ) : y2 [ 0 ] = 0 . ; u [ 0 ] = 0 .
e l s e :
y2 [ 0 ] = − 0 . 5
u [ 0 ] = ( 3 . / ( x [ 1 ] − x [ 0 ] ) ) ∗( ( y [ 1 ] − y [ 0 ] ) / ( x [ 1 ] − x [ 0 ] ) − yp1 )
f o r i i n r a n g e ( 1 , n − 1) : # Decompos i t i on loop
s i g = ( x [ i ] − x [ i − 1 ] ) / ( x [ i + 1 ] − x [ i − 1 ] )
p = s i g∗y2 [ i − 1] + 2 .
y2 [ i ] = ( s i g − 1 . ) / p
u [ i ] = ( y [ i +1] − y [ i ] ) / ( x [ i +1] − x [ i ] ) − ( y [ i ] − y [ i −1]) / ( x [ i ] − x [ i −1])
u [ i ] = ( 6 .∗ u [ i ] / ( x [ i + 1 ] − x [ i − 1 ] ) − s i g∗u [ i − 1 ] ) / p
i f ( ypn > 0 . 9 9 e30 ) : qn = un = 0 . # T e s t f o r n a t u r a l
e l s e :
qn = 0 . 5 ;
un = ( 3 / ( x [ n − 1] − x [ n − 2 ] ) ) ∗( ypn − ( y [ n − 1] − y [ n − 2 ] ) / ( x [ n − 1] − x [ n − 2 ] ) )
y2 [ n − 1] = ( un − qn∗u [ n − 2 ] ) / ( qn∗y2 [ n − 2] + 1 . )
f o r k i n r a n g e ( n − 2 , 1 , − 1) :
y2 [ k ] = y2 [ k ]∗ y2 [ k + 1] + u [ k ]
f o r i i n r a n g e ( 1 , N f i t + 2 ) : # i n i t i a l i z a t i o n ends , b e g i n f i t
xou t = x [ 0 ] + ( x [ n − 1] − x [ 0 ] ) ∗( i − 1) / ( N f i t )
k l o = 0 ; k h i = n − 1 # B i s e c t i o n a l g o r
w h i l e ( k h i − k l o >1) :
k = ( k h i + k l o ) >> 1
i f ( x [ k ] > xou t ) : k h i = k
e l s e : k l o = k
h = x [ k h i ] − x [ k l o ]
i f ( x [ k ] > xou t ) : k h i = k
e l s e : k l o = k
h = x [ k h i ] − x [ k l o ]
a = ( x [ k h i ] − xou t ) / h
b = ( xou t − x [ k l o ] ) / h
you t = ( a∗y [ k l o ]+ b∗y [ k h i ] + ( ( a∗a∗a−a )∗y2 [ k l o ] + ( b∗b∗b−b )∗y2 [ k h i ] ) ∗( h∗h ) / 6 . )
f u n c t 2 . p l o t ( pos = ( xout , you t ) )
c = c o n t r o l s ( x =300 , y =0 , wid th =200 , h e i g h t =200) #
C o n t r o l v i a s l i d e r
# c o n t r o l = s l i d e r ( pos = ( − 50 , 50 , 0 ) , min = 2 , max = 100 , a c t i o n = u p d a t e )
c o n t r o l = s l i d e r ( pos = ( −50, 50 , 0 ) , min = 2 , max = 100 , a c t i o n = u p d a t e )
t o g g l e ( pos = ( 0 , 35 , − 5) , t e x t 1 = "Number of points" , h e i g h t = 0)
c o n t r o l . v a l u e = 2
u p d a t e ( )
w h i l e 1 :
c . i n t e r a c t ( )
r a t e ( 5 0 ) # u p d a t e up t o 10 t i m e s p e r second
f u n c t 2 . v i s i b l e = 0
Here τ = 1/λ is the lifetime of the particle, with λ the rate parameter. The actual decay rate is
given by the second equation in (8.49). If the number of decays ∆N is very small compared
to the number of particles N , and if we look at vanishingly small time intervals, then the
difference equation (8.49) becomes the differential equation
dN(t)
dt
' −λN(t) = 1
τ
N(t). (8.50)
This differential equation has an exponential solution for the number as well as for the decay
rate:
N(t) = N0e−t/τ ,
dN(t)
dt
= −N0
τ
e−t/τ =
dN(0)
dt
e−t/τ . (8.51)
Equation (8.51) is the theoretical formula we wish to “fit” to the data in Figure 8.4. The output
of such a fit is a “best value” for the lifetime τ .
8.8 LEAST-SQUARES FITTING (METHOD)
Books have been written and careers have been spent discussing what is meant by a “good
fit” to experimental data. We cannot do justice to the subject here and refer the reader to
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 163
Figure 8.4 A reproduction of the experimental measurement in [Stez 73] of the number of decays of a π meson
as a function of time. Measurements are made during time intervals of 10-ns length. Each “event”
corresponds to a single decay.
0 40 80 120
t  [ns]
0
20
40
N
u
m
b
e
r N(t)
data
fit
[B&R 02, Pres 94, M&W 65, Thom 92]. However, we will emphasize three points:
1. If the data being fit contain errors, then the “best fit” in a statistical sense should not pass
through all the data points.
2. If the theory is not an appropriate one for the data (e.g., the parabola in Figure 8.3), then
its best fit to the data may not be a good fit at all. This is good, for it indicates that this is
not the right theory.
3. Only for the simplest case of a linear least-squares fit can we write down a closed-form
solution to evaluate and obtain the fit. More realistic problems are usually solved by
trial-and-error search procedures, sometimes using sophisticated subroutine libraries.
However, in §8.8.6 we show how to conduct such a nonlinear search using familiar tools.
Imagine that you have measured ND data values of the independent variable y as a function of
the dependent variable x:
(xi, yi ± σi), i = 1, ND, (8.52)
where ±σi is the uncertainty in the ith value of y. (For simplicity we assume that all the
errors σi occur in the dependent variable, although this is hardly ever true [Thom 92]). For
our problem, y is the number of decays as a function of time, and xi are the times. Our goal
is to determine how well a mathematical function y = g(x) (also called a theory or a model)
can describe these data. Alternatively, if the theory contains some parameters or constants,
our goal can be viewed as determining the best values for these parameters. We assume that
the model function g(x) contains, in addition to the functional dependence on x, an additional
dependence upon MP parameters {a1, a2, . . . , aMP }. Notice that the parameters {am} are not
variables, in the sense of numbers read from a meter, but rather are parts of the theoretical
model, such as the size of a box, the mass of a particle, or the depth of a potential well. For the
exponential decay function (8.51), the parameters are the lifetime τ and the initial decay rate
dN(0)/dt. We indicate this as
g(x) = g(x; {a1, a2, . . . , aMP }) = g(x; {am}). (8.53)
We use the chi-square (χ2) measure as a gauge of how well a theoretical function g reproduces
data:
xmll
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
164 CHAPTER 8
Figure 8.5 Left: A linear least-squares best fit of data to a straight line. Here the deviation of theory from experiment
is greater than would be expected from statistics, or in other words, a straight line is not a good theory
for these data. Right: A linear least-squares best fit of different data to a parabola. Here we see that the
fit misses approximately one-third of the points, as expected from the statistics for a good fit.
0
100
200
300
400
0 400 800 1200 1600 2000
x
y(x)
0.4
0.8
1.2
1.6
1 1.2 1.4 1.6 1.8 2
x
y(x)
χ2
def=
ND∑
i=1
(
yi − g(xi; {am})
σi
)2
, (8.54)
where the sum is over the ND experimental points (xi, yi ± σi). The definition (8.54) is such
that smaller values of χ2 are better fits, with χ2 = 0 occurring if the theoretical curve went
through the center of every data point. Notice also that the 1/σ2i weighting means that mea-
surements with larger errors5 contribute less to χ2.
Least-squares fitting refers to adjusting the parameters in the theory until a minimum in
χ2 is found, that is, finding a curve that produces the least value for the summed squares of the
deviations of the data from the function g(x). In general, this is the best fit possible or the best
way to determine the parameters in a theory. TheMP parameters {am,m = 1,MP } that make
χ2 an extremum are found by solving the MP equations:
∂χ2
∂am
= 0, ⇒
ND∑
i=1
[yi − g(xi)]
σ2i
∂g(xi)
∂am
= 0, (m = 1,MP ). (8.55)
More usually, the function g(x; {am}) has a sufficiently complicated dependence on the am
xmll
values for (8.55) to produce MP simultaneous nonlinear equations in the am values. In these
cases, solutions are found by a trial-and-error search through the MP -dimensional parameter
space, as we do in §8.8.6. To be safe, when such a search is completed, you need to check that
the minimum χ2 you found is global and not local. One way to do that is to repeat the search
for a whole grid of starting values, and if different minima are found, to pick the one with the
lowest χ2.
8.8.1 Least-Squares Fitting: Theory and Implementation
When the deviations from theory are due to random errors and when these errors are described
by a Gaussian distribution, there are some useful rules of thumb to remember [B&R 02]. You
know that your fit is good if the value of χ2 calculated via the definition (8.54) is approximately
equal to the number of degrees of freedom χ2 ' ND −MP , where ND is the number of data
5If you are not given the errors, you can guess them on the basis of the apparent deviation of the data from a smooth curve, or
you can weigh all points equally by setting σi ≡ 1 and continue with the fitting.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 165
points and MP is the number of parameters in the theoretical function. If your χ2 is much less
thanND−MP , it doesn’t mean that you have a “great” theory or a really precise measurement;
instead, you probably have too many parameters or have assigned errors (σi values) that are
too large. In fact, too small a χ2 may indicate that you are fitting the random scatter in the
data rather than missing approximately one-third of the error bars, as expected for a normal
distribution. If your χ2 is significantly greater than ND −MP , the theory may not be good,
you may have significantly underestimated your errors, or you may have errors that are not
random.
The MP simultaneous equations (8.55) can be simplified considerably if the functions
g(x; {am}) depend linearly on the parameter values ai, e.g.,
g (x; {a1, a2}) = a1 + a2x. (8.56)
In this case (also known as linear regression and shown on the left in Figure 8.5) there are
MP = 2 parameters, the slope a2, and the y intercept a1. Notice that while there are only
two parameters to determine, there still may be an arbitrary number ND of data points to fit.
Remember, a unique solution is not possible unless the number of data points is equal to or
greater than the number of parameters. For this linear case, there are just two derivatives,
∂g(xi)
∂a1
= 1,
∂g(xi)
∂a2
= xi, (8.57)
and after substitution, the χ2 minimization equations (8.55) can be solved [Pres 94]:
xmlla1 = SxxSy − SxSxy∆ , a2 =
SSxy − SxSy
∆
, (8.58)
S =
ND∑
i=1
1
σ2i
, Sx =
ND∑
i=1
xi
σ2i
, Sy =
ND∑
i=1
yi
σ2i
, (8.59)
Sxx =
ND∑
i=1
x2i
σ2i
, Sxy =
ND∑
i=1
xiyi
σ2i
, ∆ = SSxx − S2x. (8.60)
Statistics also gives you an expression for the variance or uncertainty in the deduced parame-
xmll
ters:
σ2a1 =
Sxx
∆
, σ2a2 =
S
∆
. (8.61)
This is a measure of the uncertainties in the values of the fitted parameters arising from the
uncertainties σi in the measured yi values. A measure of the dependence of the parameters on
each other is given by the correlation coefficient:
ρ(a1, a2) =
cov(a1, a2)
σa1σa2
, cov(a1, a2) =
−Sx
∆
. (8.62)
Here cov(a1, a2) is the covariance of a1 and a2 and vanishes if a1 and a2 are independent. The
correlation coefficient ρ(a1, a2) lies in the range −1 ≤ ρ ≤ 1, with a positive ρ indicating that
the errors in a1 and a2 are likely to have the same sign, and a negative ρ indicating opposite
signs.
The preceding analytic solutions for the parameters are of the form found in statistics
books but are not optimal for numerical calculations because subtractive cancellation can make
the answers unstable. As discussed in Chapter 2, “Errors & Uncertainties in Computations,” a
rearrangement of the equations can decrease this type of error. For example, [Thom 92] gives
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
166 CHAPTER 8
improved expressions that measure the data relative to their averages:
a1 = y − a2x, a2 =
Sxy
Sxx
, x =
1
N
Nd∑
i=1
xi, y =
1
N
Nd∑
i=1
yi
Sxy =
Nd∑
i=1
(xi − x)(yi − y)
σ2i
, Sxx =
Nd∑
i=1
(xi − x)2
σ2i
. (8.63)
In Fit.py in Listing 8.4 we give a program that fits a parabola to some data. You can use
it as a model for fitting a line to data,can use our closed-form expressions for a straight-line fit.
In Fit.py on the instructor’s CD we give a program for fitting to the decay data.
8.8.2 Exponential Decay Fit Assessment
Fit the exponential decay law (8.51) to the data in Figure 8.4. This means finding values for τ
and ∆N(0)/∆t that provide a best fit to the data and then judging how good the fit is.
1. Construct a table (∆N/∆ti, ti), for i = 1, ND from Figure 8.4. Because time was
measured in bins, ti should correspond to the middle of a bin.
2. Add an estimate of the error σi to obtain a table of the form (∆N/∆ti± σi, ti). You can
estimate the errors by eye, say, by estimating how much the histogram values appear to
fluctuate about a smooth curve, or you can take σi '
√
events. (This last approximation
is reasonable for large numbers, which this is not.)
3. In the limit of very large numbers, we would expect a plot of ln |dN/dt| versus t to be a
straight line:
ln
∣∣∣∣∆N(t)∆t
∣∣∣∣ ' ln ∣∣∣∣∆N0∆t
∣∣∣∣− 1τ∆t.
This means that if we treat ln |∆N(t)/∆t| as the dependent variable and time ∆t as the
independent variable, we can use our linear fit results. Plot ln |∆N/∆t| versus ∆t.
4. Make a least-squares fit of a straight line to your data and use it to determine the lifetime
τ of the π meson. Compare your deduction to the tabulated lifetime of 2.6× 10−8 s and
comment on the difference.
5. Plot your best fit on the same graph as the data and comment on the agreement.
6. Deduce the goodness of fit of your straight line and the approximate error in your de-
duced lifetime. Do these agree with what your “eye” tells you?
8.8.3 Exercise: Fitting Heat Flow
The table below gives the temperature T along a metal rod whose ends are kept at a fixed
constant temperature. The temperature is a function of the distance x along the rod.
xi (cm) 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0
Ti (C) 14.6 18.5 36.6 30.8 59.2 60.1 62.2 79.4 99.9
1. Plot the data to verify the appropriateness of a linear relation
T (x) ' a+ bx. (8.64)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 167
2. Because you are not given the errors for each measurement, assume that the least signif-
icant figure has been rounded off and so σ ≥ 0.05. Use that to compute a least-squares
straight-line fit to these data.
3. Plot your best a+ bx on the curve with the data.
4. After fitting the data, compute the variance and compare it to the deviation of your fit
from the data. Verify that about one-third of the points miss the σ error band (that’s what
is expected for a normal distribution of errors).
5. Use your computed variance to determine the χ2 of the fit. Comment on the value
obtained.
6. Determine the variances σa and σb and check whether it makes sense to use them as the
errors in the deduced values for a and b.
8.8.4 Linear Quadratic Fit (Extension)
As indicated earlier, as long as the function being fitted depends linearly on the unknown
parameters ai, the condition of minimum χ2 leads to a set of simultaneous linear equations for
the a’s that can be solved on the computer using matrix techniques. To illustrate, suppose we
want to fit the quadratic polynomial
g(x) = a1 + a2x+ a3x2 (8.65)
to the experimental measurements (xi, yi, i = 1, ND) (Figure 8.5 right). Because this g(x) is
linear in all the parameters ai, we can still make a linear fit even though x is raised to the second
power. [However, if we tried to a fit a function of the form g(x) = (a1 + a2x) exp(−a3 x) to
the data, then we would not be able to make a linear fit because one of the a’s appears in the
exponent.]
The best fit of this quadratic to the data is obtained by applying the minimum χ2 condi-
tion (8.55) for Mp = 3 parameters and ND (still arbitrary) data points. A solution represents
the maximum likelihood that the deduced parameters provide a correct description of the data
for the theoretical function g(x). Equation (8.55) leads to the three simultaneous equations for
a1, a2, and a3:
ND∑
i=1
[yi − g(xi)]
σ2i
∂g(xi)
∂a1
= 0,
∂g
∂a1
= 1, (8.66)
ND∑
i=1
[yi − g(xi)]
σ2i
∂g(xi)
∂a2
= 0,
∂g
∂a2
= x, (8.67)
ND∑
i=1
[yi − g(xi)]
σ2i
∂g(xi)
∂a3
= 0,
∂g
∂a3
= x2. (8.68)
Note: Because the derivatives are independent of the parameters (the a’s), the a dependence
arises only from the term in square brackets in the sums, and because that term has only a linear
dependence on the a’s, these equations are linear equations in the a’s.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
168 CHAPTER 8
Exercise: Show that after some rearrangement, (8.66)–(8.68) can be written as
Sa1 + Sxa2 + Sxxa3 =Sy, (8.69)
Sxa1 + Sxxa2 + Sxxxa3 =Sxy,
Sxxa1 + Sxxxa2 + Sxxxxa3 =Sxxy.
Here the definitions of the S’s are simple extensions of those used in (8.58)–(8.60) and are
programmed in Fit.py shown in Listing 8.4. After placing the three unknown parameters into
a vector x and the known three RHS terms in (8.69) into a vector~b, these equations assume
the matrix form:
A~x = ~b, (8.70)
A =
 S Sx SxxSx Sxx Sxxx
Sxx Sxxx Sxxxx
, ~x =
 a1a2
a3
, ~b =
 SySxy
Sxxy
.
This is the exactly the matrix problem we solved in § 8.4 with the code Fit.py given in List-
ing 8.4. The solution for the parameter vector ~a is obtained by solving the matrix equations.
Although for 3 × 3 matrices we can write out the solution in closed form, for larger problems
the numerical solution requires matrix methods.
Listing 8.4 Fit.py performs a least-squares fit of a parabola to data using the LinearAlgebra package to solve the
set of linear equations S~a = ~s. 
# F i t . py L i n e a r l e a s t s q u a r e f i t ; e . g . o f m a t r i x c o m p u t a t i o n a r r a y s
i m p o r t p y l a b as p ;
from v i s u a l i m p o r t ∗ ; from Numeric i m p o r t ∗ ; from L i n e a r A l g e b r a i m p o r t∗
t = a r a n g e ( 1 . 0 , 2 . 0 , 0 . 1 ) # x r a n g e c u r v e
x = a r r a y ( [ 1 . , 1 . 0 5 , 1 . 1 5 , 1 . 3 2 , 1 . 5 1 , 1 . 6 8 , 1 . 9 2 ] ) # Given x v a l u e s
y = a r r a y ( [ 0 . 5 2 , 0 . 7 3 , 1 . 0 8 , 1 . 4 4 , 1 . 3 9 , 1 . 4 6 , 1 . 5 8 ] ) # Given y v a l u e s
p . p l o t ( x , y , ’bo’ ) # P l o t d a t a i n b l u e
s i g = a r r a y ( [ 0 . 1 , 0 . 1 , 0 . 2 , 0 . 3 , 0 . 2 , 0 . 1 , 0 . 1 ] ) # e r r o r b a r l e n g h t s
p . e r r o r b a r ( x , y , s i g ) # P l o t e r r o r b a r s
p . t i t l e (’Linear least square fit’ ) # P l o t f i g u r e
p . x l a b e l ( ’x’ ) # Labe l axes
p . y l a b e l ( ’y’ )
p . g r i d ( True ) # p l o t g r i d
Nd = 7
A = z e r o s ( ( 3 , 3 ) , F l o a t ) # I n i t i a l i z e
bvec = z e r o s ( ( 3 , 1 ) , F l o a t )
s s = sx = sxx = sy = sxxx = sxxxx = sxy = sxy = sxxy = 0 .
f o r i i n r a n g e ( 0 , Nd ) :
s i g 2 = s i g [ i ] ∗ s i g [ i ]
s s += 1 . / s i g 2 ; sx += x [ i ] / s i g 2 ; sy += y [ i ] / s i g 2
r h l = x [ i ] ∗ x [ i ] ; sxx += r h l / s i g 2 ; sxxy += r h l ∗ y [ i ] / s i g 2
sxy += x [ i ]∗y [ i ] / s i g 2 ; sxxx += r h l ∗x [ i ] / s i g 2 ; sxxxx += r h l ∗ r h l / s i g 2
A = a r r a y ( [ [ ss , sx , sxx ] , [ sx , sxx , sxxx ] , [ sxx , sxxx , sxxxx ] ] )
bvec = a r r a y ( [ sy , sxy , sxxy ] )
xvec = m a t r i x m u l t i p l y ( i n v e r s e (A) , bvec ) # I n v e r t m a t r i x
I t e s t = m a t r i x m u l t i p l y (A, i n v e r s e (A) ) # M a t r i x m u l t i p l y
p r i n t ’\n x vector via inverse’
p r i n t xvec , ’\n’
p r i n t ’A*inverse(A)’
p r i n t I t e s t , ’\n’
xvec = s o l v e l i n e a r e q u a t i o n s (A, bvec ) # S o l u t i o n v i a e l i m i n a t i o n
p r i n t ’x Matrix via direct’
p r i n t xvec ,
p r i n t ’FitParabola Final Results\n’
p r i n t ’y(x) = a0 + a1 x + a2 xˆ2’ # The d e s i r e d f i t
p r i n t ’a0 = ’ , x [ 0 ]
p r i n t ’a1 = ’ , x [ 1 ]
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 169
p r i n t ’a2 = ’ , x [ 2 ] , ’\n’
p r i n t ’ i xi yi yfit ’
f o r i i n r a n g e ( 0 , Nd ) :
s = xvec [ 0 ] + xvec [ 1 ]∗ x [ i ] + xvec [ 2 ]∗ x [ i ]∗x [ i ]
p r i n t " %d %5.3f %5.3f %8.7f \n" %(i , x [ i ] , y [ i ] , s )
# r e d l i n e i s t h e f i t , r e d d o t s t h e f i t s a t y [ i ]
c u r v e = xvec [ 0 ] + xvec [ 1 ]∗ t + xvec [ 2 ]∗ t ∗∗2
p o i n t s = xvec [ 0 ] + xvec [ 1 ]∗ x + xvec [ 2 ]∗ x∗∗2
p . p l o t ( t , curve ,’r’ , x , p o i n t s , ’ro’ )
p . show ( )
8.8.5 Linear Quadratic Fit Assessment
1. Fit the quadratic (8.65) to the following data sets [given as (x1, y1), (x2, y2), . . .]. In
each case indicate the values found for the a’s, the number of degrees of freedom, and
the value of χ2.
a. (0, 1)
b. (0, 1), (1, 3)
c. (0, 1), (1, 3), (2, 7)
d. (0, 1), (1, 3), (2, 7), (3, 15)
2. Find a fit to the last set of data to the function
y = Ae−bx
2
. (8.71)
Hint: A judicious change of variables will permit you to convert this to a linear fit. Does
a minimum χ2 still have meaning here?
8.8.6 Nonlinear Fit of the Breit–Wigner Formula
to a Cross Section
Problem: Remember how we started Unit II of this chapter by interpolating the values in
Table 8.1, which gave the experimental cross section Σ as a function of energy. Although
we did not use it, we also gave the theory describing these data, namely, the Breit–Wigner
resonance formula (8.32):
f(E) =
fr
(E − Er)2 + Γ2/4
. (8.72)
Your problem is to determine what values for the parameters Er, fr, and Γ in (8.72) provide
the best fit to the data in Table 8.1.
Because (8.72) is not a linear function of the parameters (Er,Σ0,Γ), the three equations
that result from minimizing χ2 are not linear equations and so cannot be solved by the tech-
niques of linear algebra (matrix methods). However, in our study of the masses on a string
problem in Unit I, we showed how to use the Newton–Raphson algorithm to search for solu-
tions of simultaneous nonlinear equations. That technique involved expansion of the equations
about the previous guess to obtain a set of linear equations and then solving the linear equa-
tions with the matrix libraries. We now use this same combination of fitting, trial-and-error
searching, and matrix algebra to conduct a nonlinear least-squares fit of (8.72) to the data in
Table 8.1.
Recollect that the condition for a best fit is to find values of the MP parameters am in
the theory g(x, am) that minimize χ2 =
∑
i[(yi − gi)/σi]2. This leads to the MP equations
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
170 CHAPTER 8
(8.55) to solve
ND∑
i=1
[yi − g(xi)]
σ2i
∂g(xi)
∂am
= 0, (m = 1,MP ). (8.73)
To find the form of these equations appropriate to our problem, we rewrite our theory function
(8.72) in the notation of (8.73):
a1 = fr, a2 =ER, a3 = Γ2/4, x = E, (8.74)
⇒ g(x) = a1
(x− a2)2 + a3
. (8.75)
The three derivatives required in (8.73) are then
∂g
∂a1
=
1
(x− a2)2 + a3
,
∂g
∂a2
=
−2a1(x− a2)
[(x− a2)2 + a3]2
,
∂g
∂a3
=
−a1
[(x− a2)2 + a3]2
.
Substitution of these derivatives into the best-fit condition (8.73) yields three simultaneous
equations in a1, a2, and a3 that we need to solve in order to fit the ND = 9 data points (xi, yi)
in Table 8.1:
9∑
i=1
yi − g(xi, a)
(xi − a2)2 + a3
= 0,
9∑
i=1
yi − g(xi, a)
[(xi − a2)2 + a3]2
= 0,
9∑
i=1
{yi − g(xi, a)} (xi − a2)
[(xi − a2)2 + a3]2
= 0. (8.76)
Even without the substitution of (8.72) for g(x, a), it is clear that these three equations depend
on the a’s in a nonlinear fashion. That’s okay because in §8.2.2 we derived the N -dimensional
Newton–Raphson search for the roots of
fi(a1, a2, . . . , aN ) = 0, i = 1, N, (8.77)
where we have made the change of variable yi → ai for the present problem. We use that same
formalism here for the N = 3 equations (8.76) by writing them as
f1(a1, a2, a3) =
9∑
i=1
yi − g(xi, a)
(xi − a2)2 + a3
= 0, (8.78)
f2(a1, a2, a3) =
9∑
i=1
{yi − g(xi, a)} (xi − a2)
[(xi − a2)2 + a3]2
= 0, (8.79)
f3(a1, a2, a3) =
9∑
i=1
yi − g(xi, a)
[(xi − a2)2 + a3]2
= 0. (8.80)
Because fr ≡ a1 is the peak value of the cross section, ER ≡ a2 is the energy at which the
peak occurs, and Γ = 2
√
a3 is the full width of the peak at half-maximum, good guesses for
the a’s can be extracted from a graph of the data. To obtain the nine derivatives of the three
f ’s with respect to the three unknown a’s, we use two nested loops over i and j, along with the
forward-difference approximation for the derivative
∂fi
∂aj
' fi(aj + ∆aj)− fi(aj)
∆aj
, (8.81)
where ∆aj corresponds to a small, say ≤1%, change in the parameter value.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLVING SYSTEMS OF EQUATIONS WITH MATRICES; DATA FITTING 171
8.8.6.1 Nonlinear Fit Implementation
Use the Newton–Raphson algorithm as outlined in §8.8.6 to conduct a nonlinear search for the
best-fit parameters of the Breit–Wigner theory (8.72) to the data in Table 8.1. Compare the
deduced values of (fr, ER,Γ) to that obtained by inspection of the graph.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Nine
Differential Equation Applications
Part of the attraction of computational problem solving is that it is easy to solve almost every
differential equation. Consequently, while most traditional (read “analytic”) treatments of
oscillations are limited to the small displacements about equilibrium where the restoring forces
are linear, we eliminate those restrictions here and reveal some interesting nonlinear physics.
In Unit I we look at oscillators that may be harmonic for certain parameter values but then
become anharmonic. We start with simple systems that have analytic solutions, use them to test
various differential-equation solvers, and then include time-dependent forces and investigate
nonlinear resonances and beating.1 In Unit II we examine how a differential-equation solver
may be combined with a search algorithm to solve the eigenvalue problem. In Unit III we
investigate how to solve the simultaneous ordinary differential equations (ODEs) that
arise in scattering, projectile motion, and planetary orbits.
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
Ordinary Diff Eqs (ODEs) 9.1–9.4 ODE Algorithms 9.5
ODE Lab 9.5.4–9.8 -
Applets
Name Sections Name Sections
The Chaotic Pendulum 12.11–12.13 Hypersensitive Pendulums 12.11–12.13
Planetary Orbits 9.17 HearData: Sound Converter for Data 9.7
Chaotic Scattering 9.14 Relativistic Scattering 9.14
ABM predictor corrector 9.5.3 Visualizing with Sound 9.7
Photoelectric Effect 9.10-9.12
-
9.1 UNIT I. FREE NONLINEAR OSCILLATIONS
Problem: In Figure 9.1 we show a mass m attached to a spring that exerts a restoring force
toward the origin, as well as a hand that exerts a time-dependent external force on the mass.
We are told that the restoring force exerted by the spring is nonharmonic, that is, not simply
proportional to displacement from equilibrium, but we are not given details as to how this is
nonharmonic. Your problem is to solve for the motion of the mass as a function of time. You
may assume the motion is constrained to one dimension.
1In Chapter 12, “Discrete & Continuous Nonlinear Dynamics,” we make a related study of the realistic pendulum and its
chaotic behavior. Some special properties of nonlinear equations are discussed in Chapter 19, “Solitons & Computational Fluid
Dynamics.”
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 173
Figure 9.1 A mass m attached to a spring with restoring force Fk(x) and with an external agency (a hand) subjecting
the mass to a time-dependent driving force as well.
F (x,t)ext
F (x)k
9.2 NONLINEAR OSCILLATORS (MODELS)
This is a problem in classical mechanics for which Newton’s second law provides us with the
equation of motion
Fk(x) + Fext(x, t) = m
d2x
dt2
, (9.1)
where Fk(x) is the restoring force exerted by the spring and Fext(x, t) is the external force.
Equation (9.1) is the differential equation we must solve for arbitrary forces. Because we are
not told just how the spring departs from being linear, we are free to try out some different
models. As our first model, we try a potential that is a harmonic oscillator for small displace-
ments x and also contains a perturbation that introduces a nonlinear term to the force for large
x values:
V (x)' 1
2
kx2
(
1− 2
3
αx
)
, (9.2)
⇒ Fk(x) =−
dV (x)
dx
= −kx(1− αx) = md
2x
dt2
, (9.3)
where we have omitted the time-dependent external force. Equation (9.3) is the second-order
ODE we need to solve. If αx 1, we should have essentially harmonic motion.
We can understand the basic physics of this model by looking at the curves on the left
in Figure 9.2. As long as x < 1/α, there will be a restoring force and the motion will be
periodic (repeated exactly and indefinitely in time), even though it is harmonic (linear) only for
small-amplitude oscillations. Yet, as the amplitude of oscillation gets larger, there will be an
asymmetry in the motion to the right and left of the equilibrium position. And if x > 1/α, the
force will become repulsive and the mass will “roll” down the potential hill.
As a second model of a nonlinear oscillator, we assume that the spring’s potential func-
tion is proportional to some arbitrary even power p of the displacement x from equilibrium:
V (x) =
1
p
kxp, ( p even). (9.4)
We require an even p to ensure that the force,
Fk(x) = −
dV (x)
dx
= −kxp−1, (9.5)
contains an odd power of p, which guarantees that it is a restoring force for positive or negative
x values. We display some characteristics of this potential on the right in Figure 9.2. We
see that p = 2 is the harmonic oscillator and that p = 6 is nearly a square well with the mass
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
174 CHAPTER 9
Figure 9.2 Left: The potential of an harmonic oscillator (solid curve) and of an oscillator with an anharmonic
correction (dashed curve). Right: The shape of the potential energy function V(x) ∝ |x|p for different p
values. The linear and nonlinear labels refer to restoring force derived from these potentials.
Harmonic
Anharmonic
V(x)
x
1/Linear
Nonlinear
Unbound
V
p = 2
xx
V
p = 6
Linear Nonlinear
Harmonic Anharmonic
moving almost freely until it hits the wall at x ' ±1. Regardless of the p value, the motion will
be periodic, but it will be harmonic only for p = 2. Newton’s law (9.1) gives the second-order
ODE we need to solve:
Fext(x, t)− kxp−1 = m
d2x
dt2
. (9.6)
9.3 TYPES OF DIFFERENTIAL EQUATIONS (MATH)
The background material in this section is presented to avoid confusion about semantics. The
well-versed reader may want to skim or skip it.
Order: A general form for a first-order differential equation is
dy
dt
= f(t, y), (9.7)
where the “order” refers to the degree of the derivative on the LHS. The derivative or force
function f(t, y) on the RHS, is arbitrary. For instance, even if f(t, y) is a nasty function of y
and t such as
dy
dt
= −3t2y + t9 + y7, (9.8)
this is still first order in the derivative. A general form for a second-order differential equation
is
d2y
dt2
+ λ
dy
dt
= f
(
t,
dy
dt
, y
)
. (9.9)
The derivative function f on the RHS is arbitrary and may involve any power of the first
derivative as well. To illustrate,
d2y
dt2
+ λ
dy
dt
= −3t2
(
dy
dt
)4
+ t9y(t) (9.10)
is a second-order differential equation, as is Newton’s law (9.1).
In the differential equations (9.7) and (9.9), the time t is the independent variable and
the position y is the dependent variable. This means that we are free to vary the time at which
we want a solution, but not the value of the solution y at that time. Note that we often use the
symbol y or Y for the dependent variable but that this is just a symbol. In some applications
we use y to describe a position that is an independent variable instead of t.
Ordinary and partial: Differential equations such as (9.1) and (9.7) are ordinary dif-
ferential equations because they contain only one independent variable, in these cases t. In
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 175
contrast, an equation such as the Schrödinger equation
i
∂ψ(x,t)
∂t
= − 1
2m
[
∂2ψ
∂x2
+
∂2ψ
∂y2
+
∂2ψ
∂z2
]
+ V (x)ψ(x,t) (9.11)
(where we have set h̄ = 1) contains several independent variables, and this makes it a partial
differential equation (PDE). The partial derivative symbol ∂ is used to indicate that the depen-
dent variable ψ depends simultaneously on several independent variables. In the early parts of
this book we limit ourselves to ordinary differential equations. In Chapters 17–19 we examine
a variety of partial differential equations.
Linear and nonlinear: Part of the liberation of computational science is that we are no longer
limited to solving linear equations. A linear equation is one in which only the first power of y
or dny/dnt appears; a nonlinear equation may contain higher powers. For example,
dy
dt
= g3(t)y(t) (linear),
dy
dt
= λy(t)− λ2y2(t) (nonlinear). (9.12)
An important property of linear equations is the law of linear superposition that lets us add
solutions together to form new ones. As a case in point, if A(t) and B(t) are solutions of the
linear equation in (9.12), then
y(t) = αA(t) + βB(t) (9.13)
is also a solution for arbitrary values of the constants α and β. In contrast, even if we were
clever enough to guess that the solution of the nonlinear equation in (9.12) is
y(t) =
a
1 + be−λt
(9.14)
(which you can verify by substitution), things would be amiss if we tried to obtain a more
general solution by adding together two such solutions:
y1(t) =
a
1 + be−λt
+
a′
1 + b′e−λt
(9.15)
(which you can verify by substitution).
Initial and boundary conditions: The general solution of a first-order differential equation
always contains one arbitrary constant. The general solution of a second-order differential
equation contains two such constants, and so forth. For any specific problem, these constants
are fixed by the initial conditions. For a first-order equation, the sole initial condition may be
the position y(t) at some time. For a second-order equation, the two initial conditions may be
the position and velocity at some time. Regardless of how powerful the hardware and software
that you employ, the mathematics remains valid, and so you must know the initial conditions
in order to solve the problem uniquely.
In addition to the initial conditions, it is possible to further restrict the solutions of differ-
ential equations. One such way is by boundary conditions that constrain the solution to have
fixed values at the boundaries of the solution space. Problems of this sort are called eigenvalue
problems, and they are so demanding that solutions do not always exist, and even when they
do exist, a trial-and-error search may be required to find them. In Unit II we discuss how to
extend the techniques of the present unit to boundary-value problems.
9.4 DYNAMIC FORM FOR ODES (THEORY)
A standard form for ODEs, which has found use both in numerical analysis [Pres 94, Pres 00]
and in classical dynamics [Schk 94, Tab 89, J&S 98], is to express ODEs of any order as N
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
176 CHAPTER 9
simultaneous first-order ODEs:
dy(0)
dt
= f (0)(t, y(i)),
dy(1)
dt
= f (1)(t, y(i)), (9.16)
. . . (9.17)
dy(N−1)
dt
= f (N−1)(t, y(i)), (9.18)
where y(i) dependence in f is allowed but not any dependence on derivatives dy(i)/dt. These
equations can be expressed more compactly by use of the N -dimensional vectors (indicated
here in boldface) y and f :
xmll dy(t)/dt = f(t,y), (9.19)
y =

y(0)(t)
y(1)(t)
. . .
y(N−1)(t)
 , f =

f (0)(t,y)
f (1)(t,y)
. . .
f (N−1)(t,y)
 .
The utility of such compact notation is that we can study the properties of the ODEs, as well
as develop algorithms to solve them, by dealing with the single equation (9.19) without having
to worry about the individual components. To see how this works, let us convert Newton’s law
d2x
dt2
=
1
m
F
(
t, x,
dx
dt
)
(9.20)
to standard dynamic form. The rule is that the RHS may not contain any explicit derivatives,
although individual components of y(i) may represent derivatives. To pull this off, we define
the position x as the dependent variable y(0) and the velocity dx/dt as the dependent variable
y(1):
y(0)(t) def= x(t), y(1)(t) def=
dx
dt
=
dy(0)
dt
. (9.21)
The second-order ODE (9.20) now becomes two simultaneous first-order ODEs:
dy(0)
dt
= y(1)(t),
dy(1)
dt
=
1
m
F (t, y(0), y(1)). (9.22)
This expresses the acceleration [the second derivative in (9.20)] as the first derivative of the
velocity [y(1)]. These equations are now in the standard form (9.19), with the derivative or
force function f having the two components
f (0) = y(1)(t), f (1) =
1
m
F (t, y(0), y(1)), (9.23)
where F may be an explicit function of time as well as of position and velocity.
To be even more specific, applying these definitions to our spring problem (9.6), we
obtain the coupled first-order equations
dy(0)
dt
= y(1)(t),
dy(1)
dt
=
1
m
[
Fext(x, t)− ky(0)(t)p−1
]
, (9.24)
where y(0)(t) is the position of the mass at time t and y(1)(t) is its velocity. In the standard
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 177
Figure 9.3 The steps of length h taken in solving a differential equation. The solution starts at time t = a and is
integrated to t = b.
N
t=0 t=T
form, the components of the force function and the initial conditions are
f (0)(t,y) = y(1)(t), f (1)(t,y) =
1
m
[
Fext(x, t)− k(y(0))p−1
]
,
y(0)(0) = x0, y(1)(0) = v0. (9.25)
Breaking a second-order differential equation into two first-order ones is not just an
arcane mathematical maneuver. In classical dynamics it occurs when transforming the single
Newtonian equation of motion involving position and acceleration (9.1), into two Hamiltonian
equations involving position and momentum:
dpi
dt
= Fi, m
dyi
dt
= pi. (9.26)
9.5 ODE ALGORITHMS
The classic way to solve a differential equation is to start with the known initial value of the
dependent variable, y0 ≡ y(t = 0), and then use the derivative function f(t, y) to find an
approximate value for y at a small step ∆t = h forward in time; that is, y(t = h) ≡ y1.
Once you can do that, you can solve the ODE for all t values by just continuing stepping to
larger times one small h at a time (Figure 9.3).2 Error is always a concern when integrating
differential equations because derivatives require small differences, and small differences are
prone to subtractive cancellations and round-off error accumulation. In addition, because our
stepping procedure for solving the differential equation is a continuous extrapolation of the
initial conditions, with each step building on a previous extrapolation, this is somewhat like
a castle built on sand; in contrast to interpolation, there are no tabulated values on which to
anchor your solution.
It is simplest if the time steps used throughout the integration remain constant in size,
and that is mostly what we shall do. Industrial-strength algorithms, such as the one we discuss
in §9.5.2, adapt the step size by making h larger in regions where y varies slowly (this speeds
up the integration and cuts down on round-off error) and making h smaller in regions where y
varies rapidly.
2To avoid confusion, notice that y(n) is the nth component of the y vector, while yn is the value of y after n time steps. (Yes,
there is a price to pay for elegance in notation.)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
178 CHAPTER 9
Figure 9.4 Left: Euler’s algorithm for the forward integration of a differential equation for one time step. The
linear extrapolation with the initial slope is seen to cause the error ∆. Right: The rk2 algorithm is also
a linear extrapolation of the solution yn to yn+1, but with the slope (bold line segment) at the interval’s
midpoint. The error is seen to be much smaller.
Euler’s
Rule

y(t)
tn tn+1
h
rk2
y(t)
tn tn+1
slope
tn+1/2
9.5.1 Euler’s Rule
Euler’s rule (Figure 9.4 left) is a simple algorithm for integrating the differential equation (9.7)
by one step and is just the forward-difference algorithm for the derivative:
dy(t)
dt
' y(tn+1)− y(tn)
h
= f(t,y), (9.27)
⇒ yn+1'yn + hf(tn,yn), (9.28)
where yn
def= y(tn) is the value of y at time tn. We know from our discussion of differentiation
xmll
that the error in the forward-difference algorithm isO(h2), and so this too is the error in Euler’s
rule.
To indicate the simplicity of this algorithm, we apply it to our oscillator problem for the
first time step:
y
(0)
1 = x0 + v0h, y
(1)
1 = v0 + h
1
m
[Fext(t = 0) + Fk(t = 0)] . (9.29)
Compare these to the projectile equations familiar from first-year physics,
x = x0 + v0h+
1
2
ah2, v = v0 + ah, (9.30)
and we see that the acceleration does not contribute to the distance covered (no h2 term), yet it
does contribute to the velocity (and so will contribute belatedly to the distance in the next time
step). This is clearly a simple algorithm that requires very small h values to obtain precision.
Yet using small values for h increases the number of steps and the accumulation of round-off
error, which may lead to instability.3 Whereas we do not recommend Euler’s algorithm for
general use, it is commonly used to start some of the more precise algorithms.
9.5.2 Runge–Kutta Algorithm
Even though no one algorithm will be good for solving all ODEs, the fourth-order Runge–
Kutta algorithm rk4, or its extension with adaptive step size, rk45, has proven to be robust
3Instability is often a problem when you integrate a y(t) that decreases as the integration proceeds, analogous to upward
recursion of spherical Bessel functions. In that case, and if you have a linear problem, you are best off integrating inward from
large times to small times and then scaling the answer to agree with the initial conditions.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 179
and capable of industrial-strength work. In spite of rk4 being our recommended standard
method, we derive the simpler rk2 here and just give the results for rk4. The Runge–Kutta
algorithms for integrating a differential equation are based upon the formal (exact) integral of
our differential equation:
dy
dt
= f(t, y) ⇒ y(t) =
∫
f(t, y) dt (9.31)
⇒ yn+1 = yn +
∫ tn+1
tn
f(t, y) dt. (9.32)
To derive the second-order Runge–Kutta algorithm rk2 (Figure 9.4 right, rk2.py on the CD),
we expand f(t, y) in a Taylor series about the midpoint of the integration interval and retain
two terms:
f(t, y) ' f(tn+1/2, yn+1/2) + (t− tn+1/2)
df
dt
(tn+1/2) +O(h2). (9.33)
Because (t− tn+1/2) to any odd power is equally positive and negative over the interval tn ≤
t ≤ tn+1, the integral of (t− tn+1/2) in (9.32) vanishes and we obtain our algorithm:∫ tn+1
tn
f(t, y) dt' f(tn+1/2, yn+1/2)h+O(h3), (9.34)
⇒ yn+1' yn + hf(tn+1/2, yn+1/2) +O(h3) (rk2). (9.35)
We see that while rk2 contains the same number of terms as Euler’s rule, it obtains a
higher level of precision by taking advantage of the cancellation of the O(h) terms [likewise,
rk4 has the integral of the t−tn+1/2 and (t−tn+1/2)3 terms vanish]. Yet the price for improved
precision is having to evaluate the derivative function and y at the middle of the time interval,
t = tn + h/2. And there’s the rub, for we do not know the value of yn+1/2 and cannot use this
algorithm to determine it. The way out of this quandary is to use Euler’s algorithm for yn+1/2:
yn+1/2 ' yn +
1
2
h
dy
dt
= yn +
1
2
hf(tn, yn). (9.36)
Putting the pieces all together gives the complete rk2 algorithm:
xmllyn+1 ' yn + k2, (rk2) (9.37)
k2 = hf
(
tn +
h
2
, yn +
k1
2
)
, k1 = h f(tn, yn), (9.38)
where we use boldface to indicate the vector nature of y and f . We see that the known derivative
function f is evaluated at the ends and the midpoint of the interval, but that only the (known)
initial value of the dependent variable y is required. This makes the algorithm self-starting.
As an example of the use of rk2, we apply it to our spring problem:
y
(0)
1 = y
(0)
0 + hf
(0)
(
h
2
, y
(0)
0 + k1
)
' x0 + h
[
v0 +
h
2
Fk(0)
]
,
y
(1)
1 = y
(1)
0 + hf
(1)
[
h
2
, y0 +
h
2
f(0, y0)
]
' v0 +
h
m
[
Fext
(
h
2
)
+ Fk
(
y
(1)
0 +
k1
2
)]
.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
180 CHAPTER 9
These equations say that the position y(0) changes because of the initial velocity and force,
while the velocity changes because of the external force at t = h/2 and the internal force at
two intermediate positions. We see that the position now has an h2 time dependence, which at
last brings us up to the level of first-year physics.
rk4: The fourth-order Runge–Kutta method rk4 (Listing 9.1) obtains O(h4) precision by ap-
proximating y as a Taylor series up to h2 (a parabola) at the midpoint of the interval. This
approximation provides an excellent balance of power, precision, and programming simplicity.
There are now four gradient (k) terms to evaluate with four subroutine calls needed to provide
a better approximation to f(t, y) near the midpoint. This is computationally more expensive
than the Euler method, but its precision is much better, and the step size h can be made larger.
Explicitly, rk4 requires the evaluation of four intermediate slopes, and these are approximated
with the Euler algorithm:
xmll yn+1 = yn + 16(k1 + 2k2 + 2k3 + k4), (9.39)
k1 = hf(tn,yn), k2 = hf
(
tn +
h
2
,yn +
k1
2
)
,
k3 = hf
(
tn +
h
2
,yn +
k2
2
)
, k4 = hf(tn + h,yn + k3).
Listing 9.1 rk4.py solves an ODE with the RHS given by the method f() using a fourth-order Runge–Kutta
algorithm. Note that the method f(), which you will need to change for each problem, is kept separate
from the algorithm, which it is best not to change. 
# rk4 . py 2nd o r d e r Runge K u t t a
from v i s u a l i m p o r t ∗ ; from v i s u a l . g raph i m p o r t ∗
# I n i t i a l i z a t i o n
a = 0 . ; b = 1 0 . ; n = 100
ydumb = z e r o s ( ( 2 ) , F l o a t ) ; y = z e r o s ( ( 2 ) , F l o a t ) ; f R e t u r n = z e r o s ( ( 2 ) , F l o a t )
k1 = z e r o s ( ( 2 ) , F l o a t ) ; k2 = z e r o s ( ( 2 ) , F l o a t ) ; k3 = z e r o s ( ( 2 ) , F l o a t ) ;
k4 = z e r o s ( ( 2 ) , F l o a t )
y [ 0 ] = 3 . ; y [ 1 ] = −5.; t = a ; h = ( b−a ) / n ;
d e f f ( t , y , f R e t u r n ) : # f u n c t i o n t o r e t u r n RHS ( f o r c e f u n c t i o n )
f R e t u r n [ 0 ] = y [ 1 ] # RHS 1 s t eq
f R e t u r n [ 1 ] = −100.∗y [0]−2.∗y [ 1 ] + 10 .∗ s i n ( 3 .∗ t ) # RHS 2nd
graph1 = g d i s p l a y ( x =0 , y =0 , wid th = 400 , h e i g h t = 400 , t i t l e = ’RK4’ ,
x t i t l e = ’t’ , y t i t l e = ’Y[0]’ , xmin =0 , xmax =10 , ymin=−2,ymax =3)
f u n c t 1 = g cu r ve ( c o l o r = c o l o r . b l u e )
g raph2 = g d i s p l a y ( x =400 , y =0 , wid th = 400 , h e i g h t = 400 , t i t l e = ’RK4’ ,
x t i t l e = ’t’ , y t i t l e = ’Y[1]’ , xmin =0 , xmax =10 , ymin=−25,ymax =18)
f u n c t 2 = g cu r ve ( c o l o r = c o l o r . r e d )
f u n c t 1 . p l o t ( pos = ( t , y [ 0 ] ) )
f u n c t 2 . p l o t ( pos = ( t , y [ 1 ] ) )
w h i l e ( t < b ) : # Time loop
i f ( ( t + h ) > b ) : h = b − t # L a s t s t e p
f ( t , y , f R e t u r n ) # E v a l u a t e RHS’s, return in fReturn
k1[0] = h*fReturn[0]; k1[1] = h*fReturn[1] # Compute function values
for i in range(0, 2): ydumb[i] = y[i] + k1[i]/2.
f(t + h/2., ydumb, fReturn)
k2[0] = h*fReturn[0]; k2[1] = h*fReturn[1]
for i in range(0, 2): ydumb[i] = y[i] + k2[i]/2.
f(t + h/2., ydumb, fReturn)
k3[0] = h*fReturn[0]; k3[1] = h*fReturn[1]
for i in range(0, 2): ydumb[i] = y[i] + k3[i]
f(t + h, ydumb, fReturn)
k4[0] = h*fReturn[0]; k4[1] = h*fReturn[1]
for i in range(0, 2): y[i] = y[i] + (k1[i] + 2.*(k2[i] + k3[i]) + k4[i])/6.
t = t + h
rate(30)
funct1.plot(pos = (t, y[0]) )
funct2.plot(pos = (t, y[1]) ) # End while loop
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 181
Figure 9.5 The log of the relative error (number of places of precision) obtained with rk4 using a differing number
N of steps over the same time interval.
-15
-7
-9
-13
lo
g
 |
R
e
l 
E
rr
o
r|
time
Error in rk4
N = 5000
N = 1000
N = 500
rk45: (rk45.py on the CD) A variation of rk4, known as the Runge–Kutta–Fehlberg method
or rk45 [Math 02], automatically doubles the step size and tests to see how an estimate of the
error changes. If the error is still within acceptable bounds, the algorithm will continue to use
the larger step size and thus speed up the computation; if the error is too large, the algorithm
will decrease the step size until an acceptable error is found. As a consequence of the extra
information obtained in the testing, the algorithm obtains O(h5) precision but often at the
expense of extra computing time. Whether that extra time is recovered by being able to use a
larger step size depends upon the application.
9.5.3 Adams–Bashforth–Moulton Predictor-Corrector
Another approach for obtaining high precision in an ODE algorithm uss the solution from
previous steps, say, yn−2 and yn−1, in addition to yn, to predict yn+1. (The Euler and rk
methods use just the previous step.) Many of these methods tend to be like a Newton’s search
method; we start with a guess or prediction for the next step and then use an algorithm such
as rk4 to check on the prediction. This yields a correction. As with rk45, one can use the
difference between prediction and correction as a measure of the error and then adjust the
step size to obtain improved precision [Math 92, Pres 00]. For those readers who may want to
explore such methods, ABM.py on the CD gives our implementation of the Adams–Bashforth–
Moulton predictor-corrector scheme.
yn+1 = yn + 16(k1 + 2k2 + 2k3 + k4), (9.40)
k1 =hf(tn, yn), k2 = hf(tn +
h
2
, yn +
k1
2
),
k3 =hf(tn +
h
2
, yn +
k2
2
), k4 = hf(tn + h, yn + k3).
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
182 CHAPTER 9
Table 9.1 Comparison of ODE Solvers for Different Equations
Equation No. Method Initial h No. of Flops Time (ms) Relative Error
(9.41) rk4 0.01 1000 5.2 2.2× 10−8
rk45 1.00 72 1.5 1.8× 10−8
(9.42) rk4 0.01 227 8.9 1.8× 10−8
rk45 0.1 3143 36.7 5.7× 10−11
9.5.4 Assessment: rk2 versus rk4 versus rk45
While you are free to do as you please, we do not recommend that you write your own rk4
method unless you are very careful. We will be using rk4 for some high-precision work, and
unless you get every fraction and method call just right, your rk4 may appear to work well but
still not give all the precision that you shouldRegardless, we recommend that you write your
own rk2, as doing so will make it clearer as to how the Runge–Kutta methods work, but without
all the pain. We give rk2, rk4, and rk45 codes on the CD and list rk4.py in Listing 9.1.
1. Write your own rk2 method. Design your method for a general ODE; this means making
the derivative function f(t, x) a separate method.
2. Use your rk2 solver in a program that solves the equation of motion (9.6) or (9.24). Use
double precision to help control subtractive cancellation and plot both the position x(t)
and velocity dx/dt as functions of time.
3. Once your ODE solver compiles and executes, do a number of things to check that it is
working well and that you know what h values to use.
a. Adjust the parameters in your potential so that it corresponds to a pure harmonic
oscillator (set p = 2 or α = 0). For this case we have an analytic result with which to
compare:
x(t) = A sin(ω0t+ φ), v(t) = ω0A cos(ω0t+ φ), ω0 =
√
k/m.
b. Pick values of k and m such that the period T = 2π/ω is a nice number with which
to work (something like T = 1).
c. Start with a step size h ' T/5 and make h smaller until the solution looks smooth,
has a period that remains constant for a large number of cycles, and agrees with the
analytic result. As a general rule of thumb, we suggest that you start with h ' T/100,
where T is a characteristic time for the problem at hand. Here we want you to start
with a large h so that you can see a bad solution turn good.
d. Make sure that you have exactly the same initial conditions for the analytic and nu-
merical solutions (zero displacement, nonzero velocity) and then plot the two to-
gether. It is good if you cannot tell them apart, yet that only ensures that there are
approximately two places of agreement.
e. Try different initial velocities and verify that a harmonic oscillator is isochronous,
that is, that its period does not change as the amplitude varies.
4. Now that you know you can get a good solution of an ODE with rk2, compare the
solutions obtained with the rk2, rk4, and rk45 solvers.
5. Make a table of comparisons similar to Table 9.1. There we compare rk4 and rk45 for
the two equations
2yy′′ + y2 − y′2 = 0, (9.41)
y′′ + 6y5 = 0, (9.42)
with initial conditions ([y(0), y′(0)] = (1, 1). Although nonlinear, (9.41) does have the
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 183
Figure 9.6 Different periods for oscillations within the restoring potential V ∝ x7 depending upon the amplitude.
0
-4
0
4
Amplitude Dependence, p = 7
time
x(t)
analytic solution y(t) = 1 + sin t. But be warned, the rk procedures may be inaccurate
for this equation if integrated through y(t) = 0; the two terms in the equation propor-
tional to y vanish there, leaving y′2 = 0, which is problematic. A different methods,
such as predictor-corrector, might then be better.4 Equation (9.42) corresponds to our
standard potential (9.4), with p = 6. Although we have not tuned rk45, the table shows
that by setting its tolerance parameter to a small enough number, rk45 will obtain bet-
ter precision than rk4 (Figure 9.5) but that it requires ∼10 times more floating-point
operations and takes ∼5 times longer.
9.6 SOLUTION FOR NONLINEAR OSCILLATIONS (ASSESSMENT)
Use your rk4 program to study anharmonic oscillations by trying powers in the range p = 2–12
or anharmonic strengths in the range 0 ≤ αx ≤ 2. Do not include any explicit time-dependent
forces yet. Note that for large values of p you may need to decrease the step size h from the
value used for the harmonic oscillator because the forces and accelerations get large near the
turning points.
1. Check that the solution remains periodic with constant amplitude and period for a given
initial condition and value of p or α regardless of how nonlinear you make the force. In
particular, check that the maximum speed occurs at x = 0 and that the minimum speed
occurs at maximum x. The latter is a consequence of energy conservation.
2. Verify that nonharmonic oscillators are nonisochronous, that is, that different initial con-
ditions (amplitudes) lead to different periods (Figure 9.6).
3. Explain why the shapes of the oscillations change for different p’s or α’s.
4. Devise an algorithm to determine the period T of the oscillation by recording times
at which the mass passes through the origin. Note that because the motion may be
asymmetric, you must record at least three times.
5. Construct a graph of the deduced period as a function of initial amplitude.
6. Verify that the motion is oscillatory but not harmonic as the initial energy approaches
k/6α2 or for p > 6.
7. Verify that for the anharmonic oscillator with E = k/6α2, the motion changes from
4We thank Guenter Schneider for pointing this out to us.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
184 CHAPTER 9
oscillatory to translational. See how close you can get to the separatrix where a single
oscillation takes an infinite time. (There is no separatrix for the power-law potential.)
9.6.1 Precision Assessment: Energy Conservation
We have not explicitly built energy conservation into our ODE solvers. Nonetheless, unless you
have explicitly included a frictional force, it follows from the form of the equations of motion
that energy must be a constant for all values of p or α. That being the case, the constancy of
energy is a demanding test of the numerics.
1. Plot the potential energy PE(t) = V [x(t)], the kinetic energy KE(t) = mv2(t)/2, and
the total energy E(t) = KE(t) + PE(t), for 50 periods. Comment on the correlation
between PE(t) and KE(t) and how it depends on the potential’s parameters.
2. Check the long-term stability of your solution by plotting
− log10
∣∣∣∣E(t)− E(t = 0)E(t = 0)
∣∣∣∣ ' number of places of precision
for a large number of periods (Figure 9.5). Because E(t) should be independent of time,
the numerator is the absolute error in your solution and when divided by E(0), becomes
the relative error (say 10−11). If you cannot achieve 11 or more places, then you need to
decrease the value of h or debug.
3. Because a particle bound by a large-p oscillator is essentially “free” most of the time,
you should observe that the average of its kinetic energy over time exceeds its average
potential energy. This is actually the physics behind the Virial theorem for a power-law
potential:
〈KE〉 = p
2
〈PE〉. (9.43)
Verify that your solution satisfies the Virial theorem. (Those readers who have worked
the perturbed oscillator problem can use this relation to deduce an effective p value,
which should be between 2 and 3.)
9.7 EXTENSIONS: NONLINEAR RESONANCES,
BEATS, FRICTION
Problem: So far our oscillations have been rather simple. We have ignored friction and as-
sumed that there are no external forces (hands) influencing the system’s natural oscillations.
Determine
1. How the oscillations change when friction is included.
2. How the resonances and beats of nonlinear oscillators differ from those of linear
oscillators.
3. How introducing friction affects resonances.
9.7.1 Friction (Model)
The world is full of friction, and it is not all bad. For while friction may make it harder to pedal
a bike through the wind, it also tends to stabilize motion. The simplest models for frictional
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 185
force are called static, kinetic, and viscous friction:
F
(static)
f ≤ −µsN, F
(kinetic)
f = −µkN
v
|v|
, F
(viscous)
f = −bv. (9.44)
Here N is the normal force on the object under consideration, µ and b are parameters, and v is
the velocity. This model for static friction is appropriate for objects at rest, while the model for
kinetic friction is appropriate for an object sliding on a dry surface. If the surface is lubricated,
or if the object is moving through a viscous medium, then a frictional force proportional to
velocity is a better model.5
1. Extend your harmonic oscillator code to include the three types of friction in (9.44) and
observe how the motion differs for each.
2. Hint: For the simulation with static plus kinetic friction, each time the oscillator has
v = 0 you need to check that the restoring force exceeds the static force of friction. If
not, the oscillation must end at that instant. Check that your simulation terminates at
nonzero x values.
3. For your simulations with viscous friction, investigate the qualitative changes that occur
for increasing b values:
Underdamped: b < 2mω0 Oscillation within a decaying envelope
Critically damped: b = 2mω0 Nonoscillatory, finite time decay
Over damped: b > 2mω0 Nonoscillatory, infinite time decay
9.7.2 Resonances & Beats: Model, Implementation
All stable physical systems will oscillate if displaced slightly from their rest positions. The
frequency ω0 with which such a system executes small oscillations about its rest positions is
called its natural frequency. If an external sinusoidal force is applied to this system, and if
the frequency of the external force equals the natural frequency ω0, then a resonance may
occur in which the oscillator absorbs energy from the external force and the amplitude of
oscillation increases with time. If the oscillator and the driving force remain in phase over time,
the amplitude will increase continuously unless there is some mechanism, such as friction or
nonlinearities, to limit the growth.
If the frequency of the driving force is close to the natural frequency of the oscillator,
then a related phenomena, known as beating, may occur. In this situation there is interference
between the natural amplitude that is independent of the driving force plus an amplitude due to
the external force. If the frequency of the driver is very close to the natural frequency, then the
resulting motion,
x ' x0 sinωt+ x0 sinω0t =
(
2x0 cos
ω − ω0
2
t
)
sin
ω + ω0
2
t, (9.45)
looks like the natural vibration of the oscillator at the average frequency ω+ω02 , yet with an
amplitude 2x0 cos ω−ω02 t that varies with the slow beat frequency
ω−ω0
2 .
5The effect of air resistance on projectile motion is studied in Unit III of this chapter.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
186 CHAPTER 9
9.8 EXTENSION: TIME-DEPENDENT FORCES
To extend our simulation to include an external force,
Fext(t) = F0 sinωt, (9.46)
we need to include some time dependence in the force function f(t,y) that occurs in our ODE
solver.
1. Add the sinusoidal time-dependent external force (9.46) to the space-dependent restoring
force in your program (do not include friction yet).
2. Start by using a very large value for the magnitude of the driving force F0. This should
lead to mode locking (the 500-pound-gorilla effect), where the system is overwhelmed
by the driving force and, after the transients die out, the system oscillates in phase with
the driver regardless of its frequency.
3. Now lower F0 until it is close to the magnitude of the natural restoring force of the
system. You need to have this near equality for beating to occur.
4. Verify that for the harmonic oscillator, the beat frequency, that is, the number of varia-
tions in intensity per unit time, equals the frequency difference (ω − ω0)/2π in cycles
per second, where ω ' ω0.
5. Once you have a value for F0 matched well with your system, make a series of runs
in which you progressively increase the frequency of the driving force for the range
ω0/10 ≤ ω ≤ 10ω0.
6. Make of plot of the maximum amplitude of oscillation that occurs as a function of the
frequency of the driving force.
7. Explore what happens when you make nonlinear systems resonate. If the nonlinear sys-
tem is close to being harmonic, you should get beating in place of the blowup that occurs
for the linear system. Beating occurs because the natural frequency changes as the am-
plitude increases, and thus the natural and forced oscillations fall out of phase. Yet once
out of phase, the external force stops feeding energy into the system, and the amplitude
decreases; with the decrease in amplitude, the frequency of the oscillator returns to its
natural frequency, the driver and oscillator get back in phase, and the entire cycle repeats.
8. Investigate now how the inclusion of viscous friction modifies the curve of amplitude
versus driver frequency. You should find that friction broadens the curve.
9. Explain how the character of the resonance changes as the exponent p in the potential
V (x) = k|x|p/p is made larger and larger. At large p, the mass effectively “hits” the
wall and falls out of phase with the driver, and so the driver is less effective.
9.9 UNIT II. BINDING A QUANTUM PARTICLE
Problem: In this unit is we want to determine whether the rules of quantum mechanics are
applicable inside a nucleus. More specifically, you are told that nuclei contain neutrons and
protons (nucleons) with mass mc2 ' 940 MeV and that a nucleus has a size of about 2 fm.6
Your explicit problem is to see if these experimental facts are compatible, first, with quantum
mechanics and, second, with the observation that there is a typical spacing of several million
electron volts (MeV) between the ground and excited states in nuclei.
This problem requires us to solve the bound-state eigenvalue problem for the 1-D, time-
dependent Schrödinger equation. Even though this equation is an ODE, which we know how to
solve quite well by now, the extra requirement that we need to solve for bound states makes this
6A fermi (fm) equals 10−13 cm = 10−15 m, and h̄c ' 197.32 MeV fm.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 187
an eigenvalue problem. Specifically, the bound-state requirement imposes boundary conditions
on the form of the solution, which in turn means that a solution exists only for certain energies,
the eigenenergies or eigenvalues.
If this all sounds a bit much for you now, rest assured that you do not need to un-
derstand all the physics behind these statements. What we want is for you to gain experi-
ence with the technique of conducting a numerical search for the eigenvalue in conjunction
with solving an ODE numerically. This is how one solves the numerical ODE eigenvalue
problem. In §20.2.1, we discuss how to solve the equivalent, but more advanced, momentum-
space eigenvalue problem as a matrix problem. In Chapter 18, PDE Waves: String, Quantum
Packet, and we study the related problem of the motion of a quantum wave packet confined
to a potential well. Further discussions of the numerical bound-state problem are found in
[Schd 00, Koon 86].
9.10 THEORY: QUANTUM EIGENVALUE PROBLEM
Quantum mechanics describes phenomena that occur on atomic or subatomic scales (an ele-
mentary particle is subatomic). It is a statistical theory in which the probability that a particle
is located in a region dx around the point x is P = |ψ(x)|2 dx, where ψ(x) is called the wave
function. If a particle of energy E moving in one dimension experiences a potential V (x), its
wave function is determined by an ODE (a PDE if greater than 1-D) known as the
time-independent Schrödinger equation7:
xmll−h̄
2
2m
d2ψ(x)
dx2
+ V (x)ψ(x) = Eψ(x). (9.47)
Although we say we are solving for the energy E, in practice we solve for the wave vector κ.
The energy is negative for bound states, and so we relate the two by
κ2 = −2m
h̄2
E =
2m
h̄2
|E|. (9.48)
The Schrödinger equation then takes the form
d2ψ(x)
dx2
− 2m
h̄2
V (x)ψ(x) = κ2ψ(x). (9.49)
When our problem tells us that the particle is bound, we are being told that it is confined to
some finite region of space. The only way to have a ψ(x) with a finite integral is to have it
decay exponentially as x→ ±∞ (where the potential vanishes):
ψ(x)→
{
e−κx, for x→ +∞,
e+κx, for x→ −∞.
(9.50)
In summary, although it is straightforward to solve the ODE (9.47) with the techniques
we have learned so far, we must also require that the solution ψ(x) simultaneously satisfies the
boundary conditions (9.50). This extra condition turns the ODE problem into an eigenvalue
problem that has solutions (eigenvalues) for only certain values of the energy E. The ground-
state energy corresponds to the smallest (most negative) eigenvalue. The ground-state wave
function (eigenfunction), which we must determine in order to find its energy, must be nodeless
and even (symmetric) about x = 0. The excited states have higher (less negative) energies and
wave functions that may be odd (antisymmetric).
7The time-dependent equation requires the solution of a partial differential equation, as discussed in Chapter 18, “PDE Waves:
String, Quantum Packet, and E&M.”
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
188 CHAPTER 9
9.10.1 Model: Nucleon in a Box
The numerical methods we describe are capable of handling the most realistic potential shapes.
Yet to make a connection with the standard textbook case and to permit some analytic check-
ing, we will use a simple model in which the potential V (x) in (9.47) is a finite square well
(Figure 9.7):
V (x) =
{
−V0 = −83 MeV, for |x| ≤ a = 2 fm,
0, for |x| > a = 2 fm,
(9.51)
where values of 83 MeV for the depth and 2 fm for the radius are typical for nuclei (these are
the units in which we solve the problem). With this potential the Schrödinger equation (??)
becomes
d2ψ(x)
dx2
+
(
2m
h̄2
V0 − κ2
)
ψ(x) = 0, for |x| ≤ a, (9.52)
d2ψ(x)
dx2
− κ2ψ(x) = 0, for |x| > a. (9.53)
To evaluate the ratio of constants here, we insert c2, the speed of light squared, into both the
numerator and the denominator [L 96, Appendix A.1]:
2m
h̄2
=
2mc2
(h̄c)2
' 2× 940 MeV
(197.32 MeV fm)2
= 0.0483 MeV−1 fm−2. (9.54)
9.11 COMBINED ALGORITHMS: EIGENVALUES VIA ODE SOLVER + SEARCH
The solution of the eigenvalue problem combines the numerical solution of the ordinary differ-
ential equation (??) with a trial-and-error search for a wave function that satisfies the boundary
conditions (9.50). This is done in several steps:8
1. Start on the very far left at x = −Xmax ' −∞, where Xmax  a. Assume that the wave
function there satisfies the left-hand boundary condition:
ψL(x = −Xmax) = e+κx = e−κXmax .
2. Use your favorite ODE solver to step ψL(x) in toward the origin (to the right) from
x = −Xmax until you reach the matching radius xmatch. The exact value of this matching
radius is not important, and our final solution should be independent of it. On the left in
Figure 9.7, we show a sample solution with xmatch = −a; that is, we match at the left edge
of the potential well. In the middle and on the right in Figure 9.7 we see some guesses
that do not match.
3. Start on the very far right, that is, at x = +Xmax ' +∞, with a wave function that
satisfies the right-hand boundary condition:
ψR(x = +κXmax) = e−κx = e−κXmax .
4. Use your favorite ODE solver (e.g., rk4) to step ψR(x) in toward the origin (to the left)
from x = +Xmax until you reach the matching radius xmatch. This means that we have
stepped through the potential well (Figure 9.7).
5. In order for probability and current to be continuous at x = xmatch, ψ(x) and ψ′(x) must
be continuous there. Requiring the ratio ψ′(x)/ψ(x), called the logarithmic derivative,
8The procedure outlined here is for a general potential that falls off gradually. For a square well with sharp cutoffs the analytic
solution is valid right up unto the walls, whereas for a Coulomb potential with very slow falloff a modified asymptotic form is
required.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 189
Figure 9.7 Left: Computed wave function and the square-well potential (bold lines). The wave function computed
by integration in from the left is matched to the one computed by integration in from the right (dashed
curve) at a point near the left edge of the well. Note how the wave function decays rapidly outside the
well. Right: A first guess at a wave function with energy E that is 0.5% too low (dotted curve). We see
that the left wave function does not vary rapidly enough to match the right one at x = 500. The solid
curve shows a second guess at a wave function with energy E that is 0.5% too high. We see that now the
left wave function varies too rapidly.
xmatch
0x
V(x)
-V0
-a a
0
0x
Low EHigh E
to be continuous encapsulates both continuity conditions into a single condition and is
independent of ψ’s normalization.
6. Even though we do not know ahead of time which energiesE or κ values are eigenvalues,
we still need a starting value for the energy in order to use our ODE solver. Such being
the case, we start the solution with a guess for the energy. A good guess for ground-state
energy would be a value somewhat up from that at the bottom of the well, E > −V0.
7. Because it is unlikely that any guess will be correct, the left- and right-wave functions
will not quite match at x = xmatch (Figure 9.7). This is okay because we can use the
amount of mismatch to improve the next guess. We measure how well the right and left
wave functions match by calculating the difference
xmll∆(E, x) = ψ
′
L(x)/ψL(x)− ψ′R(x)/ψR(x)
ψ′L(x)/ψL(x) + ψ
′
R(x)/ψR(x)
∣∣∣∣
x=xmatch
, (9.55)
where the denominator is used to avoid overly large or small numbers. Next we try a
different energy, note how much ∆(E) has changed, and use this to deduce an intelligent
guess at the next energy. The search continues until the left- and right-wave ψ′/ψ match
within some tolerance.
9.11.1 Numerov Algorithm for Schrödinger ODE 
We generally recommend the fourth-order Runge–Kutta method for solving ODEs, and its com-
bination with a search routine for solving the eigenvalue problem. In this section we present
the Numerov method, an algorithm that is specialized for ODEs not containing any first deriva-
tives (such as our Schrödinger equation). While this algorithm is not as general as rk4, it is
of O(h6) and thus speeds up the calculation by providing additional precision.
We start by rewriting the Schrödinger equation (9.49) in the compact form,
d2ψ
dx2
+ k2(x)ψ = 0, k2(x) def=
2m
h̄2
E + V0, for |x| < a,E, for |x| > a, (9.56)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
190 CHAPTER 9
where k2 = −κ2 in potential-free regions. Observe that although (9.56) is specialized to a
square well, other potentials would have V (x) in place of −V0. The trick in the Numerov
method is to get extra precision in the second derivative by taking advantage of there being no
first derivative dψ/dx in (9.56). We start with the Taylor expansions of the wave functions,
ψ(x+ h) ' ψ(x) + hψ(1)(x) + h
2
2
ψ(2)(x) +
h3
3!
ψ(3)(x) +
h4
4!
ψ(4)(x) + · · ·
ψ(x− h) ' ψ(x)− hψ(1)(x) + h
2
2
ψ(2)(x)− h
3
3!
ψ(3)(x) +
h4
4!
ψ(4)(x) + · · · ,
where ψ(n) signifies the nth derivative dnψ/dxn. Because the expansion of ψ(x− h) has odd
powers of h appearing with negative signs, all odd powers cancel when we add ψ(x + h) and
ψ(x− h) together:
ψ(x+ h) + ψ(x− h) ' 2ψ(x) + h2ψ(2)(x) + h
4
12
ψ(4)(x) +O(h6),
⇒ ψ(2)(x) ' ψ(x+ h) + ψ(x− h)− 2ψ(x)
h2
− h
2
12
ψ(4)(x) +O(h4).
To obtain an algorithm for the second derivative we eliminate the fourth-derivative term by
applying the operator 1 + h2
12
d2
dx2
to the Schrödinger equation (9.56):
ψ(2)(x) +
h2
12
ψ(4)(x) + k2(x)ψ +
h2
12
d2
dx2
[k2(x)ψ(4)(x)] = 0.
We eliminate the ψ(4) terms by substituting the derived expression for the ψ(2):
ψ(x+ h) + ψ(x− h)− 2ψ(x)
h2
+ k2(x)ψ(x) +
h2
12
d2
dx2
[k2(x)ψ(x)] ' 0.
Now we use a central-difference approximation for the second derivative of k2(x)ψ(x)
h2
d2[k2(x)ψ(x)]
dx2
' [(k2ψ)x+h − (k2ψ)x] + [(k2ψ)x−h − (k2ψ)x].
After this substitution we obtain the Numerov algorithm:
xmll
ψ(x+ h) '
2[1− 512h
2k2(x)]ψ(x)− [1 + h212k
2(x− h)]ψ(x− h)
1 + h2k2(x+ h)/12
. (9.57)
We see that the Numerov algorithm uses the values of ψ at the two previous steps x and x− h
to move ψ forward to x+ h. To step backward in x, we need only to reverse the sign of h. Our
implementation of this algorithm, Numerov.py, is given in Listing 9.2.
Listing 9.2 QuantumNumerov.py solves the 1-D time-independent Scrödinger equation for bound-state energies
using a Numerov method (rk4 also works, as we show in Listing 9.3). 
’’’
VQuantumNumerov.py illustrates the use of bisection to find
the energy and the wavefunction for a finite square well
’’’
from v i s u a l i m p o r t ∗
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 191
from v i s u a l . g raph i m p o r t ∗
p s i g r = d i s p l a y ( x =0 , y =0 , wid th =600 , h e i g h t =300 ,
t i t l e =’Wave function, red line: match of right and left wavefunctions’ )
p s i = c u r v e ( x= r a n g e ( 0 , 1 0 0 0 ) , d i s p l a y = p s i g r , c o l o r = c o l o r . ye l l o w )
p s i 2 g r = d i s p l a y ( x =0 , y =300 , wid th =600 , h e i g h t =200 , t i t l e =’Wave function squared’ )
p s i o = c u r v e ( x= r a n g e ( 0 , 1 0 0 0 ) , c o l o r = c o l o r . magenta , d i s p l a y = p s i 2 g r )
e n e r g r = d i s p l a y ( x =0 , y =500 , wid th =600 , h e i g h t =200 , t i t l e =’Potential and autoenergy’ )
p o t e n = c u r v e ( x= r a n g e ( 0 , 1 0 0 0 ) , c o l o r = c o l o r . cyan , d i s p l a y = e n e r g r ) # p l o t p o t e n t i a l
a u t o e n = c u r v e ( x= r a n g e ( 0 , 1 0 0 0 ) , d i s p l a y = e n e r g r )
d l =1e−6 # ve ry s m a l l i n t e r v a l t o s t o p b i s e c t i o n
u l = z e r o s ( [ 1 5 0 1 ] , F l o a t )
u r = z e r o s ( [ 1 5 0 1 ] , F l o a t )
k 2 l = z e r o s ( [ 1 5 0 1 ] , F l o a t ) #k∗∗2 i n S c h r o e d i n g e r eq . l e f t wavefunc .
k2r = z e r o s ( [ 1 5 0 1 ] , F l o a t ) #k∗∗2 i n S c h r o e d i n g e r eq . r i g h t wavefunc .
n = 1501
m = 5 # t o p l o t e v e r y 5 p o i n t s
imax = 100
x l 0 = −1000 # l e f t m o s t x p o i n t wave f u n c t i o n
xr0 = 1000 # r i g h t m o s t x p o i n t wave f u n c t i o n
h = ( 1 . 0∗ ( xr0−x l 0 ) / ( n−1) ) # 1 . 0 t o make i t a f l o a t number and a v o i d
# i n t e g e r d i v i s i o n ( h = 1 . 3 3 3 3 . . and n o t h =1)
amin= −0.001 # r o o t be tween amin and amax
amax = −0.00085
e = amin # t a k e t h i s as t h e i n i t i a l g u e s s f o r e ne rg y
de = 0 . 0 1
u l [ 0 ] = 0 . 0
u l [ 1 ] = 0 .00001
ur [ 0 ] = 0 . 0
u r [ 1 ] = 0 .00001
im = 500 # match p o i n t l e f t and r i g h t wv
n l = im+2 # f o r l e f t wv
nr = n−im+1 # f o r r i g h t wave f u n c t i o n
i s t e p =0
d e f V( x ) : # P o t e n t i a l f i n i t e s q u a r e w e l l
i f ( abs ( x ) <=500) :
v=−0.001 # w e l l d e p t h
e l s e :
v=0
r e t u r n v
d e f s e t k 2 ( ) : # s e t s k 2 l =( s q r t ( e−V) ) ˆ2 and k2r
f o r i i n r a n g e ( 0 , n ) :
x l = x l 0 + i∗h
xr = xr0−i∗h
k 2 l [ i ] = e−V( x l )
k2r [ i ] = e−V( xr )
d e f numerov ( n , h , k2 , u ) : #Numerov a l g o r i t h m can be used f o r
b =( h∗∗2) / 1 2 . 0 # l e f t and r i g h t wave f u n c t i o n s
f o r i i n r a n g e ( 1 , n−1) :
u [ i +1]=(2∗ u [ i ]∗ (1 .0−5.∗ b∗k2 [ i ] ) −(1.+b∗k2 [ i −1])∗u [ i −1]) / ( 1 . + b∗k2 [ i + 1 ] )
s e t k 2 ( )
numerov ( nl , h , k2l , u l ) # f i n d s l e f t wave f u n c t i o n
numerov ( nr , h , k2r , u r ) # f i n d s r i g h t wave f u n c t i o n
f a c t = u r [ nr−2]/ u l [ im ] # t o R e s c a l e s o l u t i o n
f o r i i n r a n g e ( 0 , n l ) :
u l [ i ] = f a c t ∗u l [ i ]
f0 = ( u r [ nr−1]+ u l [ nl−1]−ur [ nr−3]−u l [ nl −3]) / ( 2∗ h∗ur [ nr−2]) # Log d e r i v
d e f n o r m a l i z e ( ) :
asum = 0
f o r i i n r a n g e ( 0 , n ) : # t o n o r m a l i z e wave f u n c t i o n
i f i > im :
u l [ i ] = u r [ n−i−1]
asum = asum+ u l [ i ]∗ u l [ i ]
asum = s q r t ( h∗asum ) ;
# p r i n t "istep=" , i s t e p
# p r i n t "e= " , e ," de= " , de
e l a b e l = l a b e l ( pos =(700 , 500) , t e x t =’e=’ , box =0 , d i s p l a y = p s i g r )
e l a b e l . t e x t = ’e=%10.8f’ %e
i l a b e l = l a b e l ( pos =(700 , 400) , t e x t =’istep=’ , box =0 , d i s p l a y = p s i g r )
i l a b e l . t e x t = ’istep=%4s’ %i s t e p
p o t e n . pos =[(−1500 ,200) , (−1000 ,200) ,(−1000 ,−200) , (0 ,−200) , ( 0 , 2 0 0 ) , ( 1 0 0 0 , 2 0 0 ) ]
a u t o e n . pos =[(−1000 , e ∗400000.0+200) , ( 0 , e ∗400000.0+200) ]
l a b e l ( pos =(−1150 ,−240) , t e x t =’0.001’ , box =0 , d i s p l a y = e n e r g r )
l a b e l ( pos =(−1000 ,300) , t e x t =’0’ , box =0 , d i s p l a y = e n e r g r )
l a b e l ( pos =(−900 ,180) , t e x t =’-500’ , box =0 , d i s p l a y = e n e r g r )
l a b e l ( pos =(−100 ,180) , t e x t =’500’ , box =0 , d i s p l a y = e n e r g r )
l a b e l ( pos =(−500 ,180) , t e x t =’0’ , box =0 , d i s p l a y = e n e r g r )
l a b e l ( pos = ( 9 0 0 , 1 2 0 ) , t e x t =’r’ , box =0 , d i s p l a y = e n e r g r )
j =0
f o r i i n r a n g e ( 0 , n ,m) :
x l = x l 0 + i∗h
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
192 CHAPTER 9
u l [ i ] = u l [ i ] / asum #wave f u n c t i o n n o r m a l i z e d
p s i . x [ j ]= xl−500 # p l o t p s i
p s i . y [ j ]=10000 .0∗ u l [ i ] # n e x t v e r t i c a l l i n e i n d i c a t e s match o f wvfs .
l i n e = c u r v e ( pos =[(−830 ,−500) , (−830 ,500) ] , c o l o r = c o l o r . red , d i s p l a y = p s i g r )
p s i o . x [ j ]= xl−500 # p l o t p s i
p s i o . y [ j ] = 1 . 0 e5∗u l [ i ]∗∗2
j +=1
w h i l e abs ( de ) > d l and i s t e p < imax : # b i s e c t i o n a l g o r i t h m b e g i n s
r a t e ( 2 ) # t o slowdown a n i m a t i o n
e1 = e # g u e s s e d r o o t
e =( amin+amax ) / 2 # h a l f i n t e r v a l
f o r i i n r a n g e ( 0 , n ) :
k 2 l [ i ]= k 2 l [ i ]+ e−e1
k2r [ i ]= k2r [ i ]+ e−e1
im =500;
n l = im+2
nr = n−im +1;
numerov ( nl , h , k2l , u l ) # Find w a v e f u n t i o n s f o r new k2l , k2r
numerov ( nr , h , k2r , u r )
f a c t = u r [ nr−2]/ u l [ im ]
f o r i i n r a n g e ( 0 , n l ) :
u l [ i ] = f a c t ∗u l [ i ]
f1 = ( u r [ nr−1]+ u l [ nl−1]−ur [ nr−3]−u l [ nl −3]) / ( 2∗ h∗ur [ nr−2]) # Log d e r i v .
r a t e ( 2 )
i f f0∗ f1 < 0 : # b i s e c t i o n l o c a l i z e r o o t
amax = e
de = amax−amin
e l s e :
amin = e
de = amax−amin
f0 = f1
n o r m a l i z e ( )
i s t e p = i s t e p +1
9.11.2 Implementation: Eigenvalues via an ODE Solver Plus
Bisection Algorithm
1. Combine your bisection algorithm search program with your rk4 or Numerov ODE
solver program to create an eigenvalue solver. Start with a step size h = 0.04.
2. Write a subroutine that calculates the matching function ∆(E, x) as a function of en-
ergy and matching radius. This subroutine will be called by the bisection algorithm
program to search for the energy at which ∆(E, x = 2) vanishes.
3. As a first guess, take E ' 65 MeV.
4. Search until ∆(E, x) changes in only the fourth decimal place. We do this in the code
QuantumEigen.py shown in Listing 9.3.
5. Print out the value of the energy for each iteration. This will give you a feel as to
how well the procedure converges, as well as a measure of the precision obtained. Try
different values for the tolerance until you are confident that you are obtaining three
good decimal places in the energy.
6. Build in a limit to the number of energy iterations you permit and print out when the
iteration scheme fails.
7. As we have done, plot the wave function and potential on the same graph (you will have
to scale the potential to get them both to fit).
8. Deduce, by counting the number of nodes in the wave function, whether the solution
found is a ground state (no nodes) or an excited state (with nodes) and whether the
solution is even or odd (the ground state must be even).
9. Include in your version of Figure 9.7 a horizontal line within the potential indicating
the energy of the ground state relative to the potential’s depth.
10. Increase the value of the initial energy guess and search for excited states. Make sure
to examine the wave function for each state found to ensure that it is continuous and to
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 193
count the number of nodes.
11. Add each new state found as another horizontal bar within the potential.
12. Verify that you have solved the problem, that is, that the spacing between levels is on
the order of MeV for a nucleon bound in a several-fermi well.
Listing 9.3 QuantumEigen.py solves the 1-D time-independent Schrödinger equation for bound-state energies
using the rk4 algorithm. 
# QuantumEigen . py : F i n d s a u t o e n e r g y and w a v e f u n c t i o n u s i n g rk4 and b i s e c t i o n
# I n i t i a l g u e s s e d en e r g y : −17.0
# mass / ( ( hba r∗c ) ∗∗2)= 940MeV/ ( 1 9 7 . 3 3MeV−fm )∗∗2 =0 .4829 , w e l l wid th =20 .0 fm
# w e l l d e p t h 10 MeV
#Wave f u n c t i o n n o t n o r m a l i z e d . P o t e n t i a l n o t t o s c a l e .
from v i s u a l i m p o r t ∗
from v i s u a l . g raph i m p o r t ∗
p s i g r = d i s p l a y ( x =0 , y =0 , wid th =600 , h e i g h t =300 ,
t i t l e =’Right and left wavefunctions’ )
# graph1 = g d i s p l a y ( wid th =600 , h e i g h t =500 , t i t l e =’Schrodinger Eqn RK4: Eigenfunction (red and
yellow), Potential(blue)’ ,
# x t i t l e =’x’ , y t i t l e =’Y(x)’ , xmax = 3 0 . 0 , xmin=−30,ymax = 0 . 0 1 , ymin =−0.3)
Lwf = c u r v e ( x= r a n g e ( 5 0 2 ) , c o l o r = c o l o r . r e d )
Rwf = c u r v e ( x= r a n g e ( 9 9 7 ) , c o l o r = c o l o r . y e l l ow )
eps = 1E−3 # v a r i a b l e s , p r e c i s i o n
n s t e p s = 501
E = −17.0 # t r y t h i s e ne rg y as a g u e s s
h = 0 . 0 4
count max = 100
Emax = 1.1∗E # between t h e s e Emax and Emin
Emin = E / 1 . 1 # i s t h e e i g e n e n e r g
d e f f ( x , y , F , E ) :
F [ 0 ] = y [ 1 ]
F [ 1 ] = −(0.4829) ∗(E−V( x ) )∗y [ 0 ]
d e f V( x ) :
i f ( abs ( x ) < 1 0 . ) : # P o t e n t i a l w e l l d e p t h
r e t u r n ( −16.0) # i t was 16
e l s e :
r e t u r n ( 0 . )
d e f rk4 ( t , y , h , Neqs , E ) :
F = z e r o s ( ( Neqs ) , F l o a t )
ydumb = z e r o s ( ( Neqs ) , F l o a t )
k1 = z e r o s ( ( Neqs ) , F l o a t )
k2 = z e r o s ( ( Neqs ) , F l o a t )
k3 = z e r o s ( ( Neqs ) , F l o a t )
k4 = z e r o s ( ( Neqs ) , F l o a t )
f ( t , y , F , E )
f o r i i n r a n g e ( 0 , Neqs ) :
k1 [ i ] = h∗F [ i ]
ydumb [ i ] = y [ i ] + k1 [ i ] / 2 .
f ( t + h / 2 . , ydumb , F , E )
f o r i i n r a n g e ( 0 , Neqs ) :
k2 [ i ] = h∗F [ i ]
ydumb [ i ] = y [ i ] + k2 [ i ] / 2 .
f ( t + h / 2 . , ydumb , F , E )
f o r i i n r a n g e ( 0 , Neqs ) :
k3 [ i ]= h∗F [ i ]
ydumb [ i ] = y [ i ] + k3 [ i ]
f ( t + h , ydumb , F , E ) ;
f o r i i n r a n g e ( 0 , Neqs ) :
k4 [ i ]= h∗F [ i ]
y [ i ]= y [ i ] + ( k1 [ i ]+2∗ ( k2 [ i ]+ k3 [ i ] ) +k4 [ i ] ) / 6 . 0
d e f d i f f ( E , h ) :
y = z e r o s ( ( 2 ) , F l o a t )
i m a t c h = n s t e p s / 3 # Matching r a d i u s
nL = i m a t c h + 1
y [ 0 ] = 1 . E−15; # I n i t i a l wf on l e f t
y [ 1 ] = y [ 0 ]∗ s q r t (−E∗0 .4 8 2 9 )
f o r i x i n r a n g e ( 0 , nL + 1) :
x = h ∗ ( i x −n s t e p s / 2 )
rk4 ( x , y , h , 2 , E )
l e f t = y [ 1 ] / y [ 0 ] # Log d e r i v a t i v e
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
194 CHAPTER 9
y [ 0 ] = 1 . E−15; # s l o p e f o r even ; r e v e r s e f o r odd
y [ 1 ] = −y [ 0 ]∗ s q r t (−E∗0 .4 8 2 9 ) # I n i t i a l i z e R wf
f o r i x i n r a n g e ( n s t e p s , nL+1 ,−1) :
x = h∗( i x+1−n s t e p s / 2 )
rk4 ( x , y , −h , 2 , E )
r i g h t = y [ 1 ] / y [ 0 ] # Log d e r i v a t i v e
r e t u r n ( ( l e f t − r i g h t ) / ( l e f t + r i g h t ) )
d e f p l o t ( E , h ) : # Repea t i n t e g r a t i o n s f o r p l o t
x = 0 .
n s t e p s = 1501 # T o t a l no i n t e g r a t i o n s t e p s
y = z e r o s ( ( 2 ) , F l o a t )
yL = z e r o s ( ( 2 , 5 0 5 ) , F l o a t )
i m a t c h = 500 # Matching p o i n t
nL = i m a t c h + 1 ;
y [ 0 ] = 1 . E−40 # I n i t i a l wf on t h e l e f t
y [ 1 ] = −s q r t (−E∗0 .4 8 2 9 ) ∗y [ 0 ]
f o r i x i n r a n g e ( 0 , nL +1) : # l e f t wave f u n c t i o n
yL [ 0 ] [ i x ] = y [ 0 ]
yL [ 1 ] [ i x ] = y [ 1 ]
x = h ∗ ( i x −n s t e p s / 2 )
rk4 ( x , y , h , 2 , E )
y [ 0 ] = −1.E−15 # − s l o p e : even ; r e v e r s e f o r odd
y [ 1 ] = −s q r t (−E∗0 .4 8 2 9 )∗y [ 0 ]
j =0
f o r i x i n r a n g e ( n s t e p s −1,nL + 2 ,−1) : # r i g h t wave f u n c t i o n
x = h ∗ ( i x + 1 −n s t e p s / 2 ) # I n t e g r a t e i n
rk4 ( x , y , −h , 2 , E )
Rwf . x [ j ] = 2 .∗ ( i x + 1 −n s t e p s / 2 ) −500.0
Rwf . y [ j ]= y [0 ]∗35 e−9 +200
j +=1
x=x−h
normL = y [ 0 ] / yL [ 0 ] [ nL ]
j =0
# Renorma l i ze L wf & d e r i v a t i v e
f o r i x i n r a n g e ( 0 , nL +1) :
x = h ∗ ( ix−n s t e p s / 2 + 1)
y [ 0 ] = yL [ 0 ] [ i x ]∗normL
y [ 1 ] = yL [ 1 ] [ i x ]∗normL
Lwf . x [ j ] = 2 .∗ ( i x −n s t e p s / 2 + 1 )−500.0
Lwf . y [ j ]= y [0 ]∗35 e−9+200 # f a c t o r t o r e d u c e s c a l e
j +=1
f o r c o u n t i n r a n g e ( 0 , count max +1) :
r a t e ( 1 ) # s l o w e s t r a t e t o show changes
# I t e r a t i o n loop
E = ( Emax + Emin ) / 2 . # D iv id e E r a n g e
D i f f = d i f f ( E , h )
i f ( d i f f ( Emax , h )∗D i f f > 0) :
Emax = E # B i s e c t i o n a l g o r i t h m
e l s e :
Emin = E
i f ( abs ( D i f f ) < eps ) :
break
i f c o u n t >3: # f i r s t i t e r a t e s a r e ve ry i r r e g u l a r
r a t e ( 4 )
p l o t ( E , h )
e l a b e l = l a b e l ( pos =(700 , 400) , t e x t =’E=’ , box =0)
e l a b e l . t e x t = ’E=%13.10f’ %E
i l a b e l = l a b e l ( pos =(700 , 600) , t e x t =’istep=’ , box =0)
i l a b e l . t e x t = ’istep=%4s’ %c o u n t
e l a b e l = l a b e l ( pos =(700 , 400) , t e x t =’E=’ , box =0) # r e c o r d l a s t i t e r a t i o n
e l a b e l . t e x t = ’E=%13.10f’ %E
i l a b e l = l a b e l ( pos =(700 , 600) , t e x t =’istep=’ , box =0)
i l a b e l . t e x t = ’istep=%4s’ %c o u n t
p r i n t "Final eigenvalue E = " ,E
p r i n t "iterations, max = " , c o u n t
9.12 EXPLORATIONS
1. Check to see how well your search procedure works by using arbitrary values for the
starting energy. For example, because no bound-state energies can lie below the bottom
of the well, try E ≥ −V0, as well as some arbitrary fractions of V0. In every case
examine the resulting wave function and check that it is both symmetric and continuous.
2. Increase the depth of your potential progressively until you find several bound states.
Look at the wave function in each case and correlate the number of nodes in the wave
function and the position of the bound state in the well.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 195
3. Explore how a bound-state energy changes as you change the depth V0 of the well. In
particular, as you keep decreasing the depth, watch the eigenenergy move closer to E =
0 and see if you can find the potential depth at which the bound state has E ' 0.
4. For a fixed well depth V0, explore how the energy of a bound state changes as the well
radius a is varied.
5.  Conduct some explorations in which you discover different combinations of (V0, a)
that give the same ground-state energies (discrete ambiguities). The existence of several
different combinations means that a knowledge of ground-state energy is not enough to
determine a unique depth of the well.
6. Modify the procedures to solve for the eigenvalue and eigenfunction for odd wave func-
tions.
7. Solve for the wave function of a linear potential:
V (x) = −V0
{
|x|, for |x| < a,
0, for |x| > a.
There is less potential here than for a square well, so you may expect smaller binding
energies and a less confined wave function. (For this potential, there are no analytic
results with which to compare.)
8. Compare the results obtained, and the time the computer took to get them, using both the
Numerov and rk4 methods.
9. Newton–Raphson extension: Extend the eigenvalue search by using the Newton–
Raphson method in place of the bisection algorithm. Determine how much faster and
more precise it is.
9.13 UNIT III. SCATTERING, PROJECTILES AND PLANETARY ORBITS
9.14 PROBLEM 1: CLASSICAL CHAOTIC SCATTERING
Problem: One expects the classical scattering of a projectile from a barrier to be a continuous
process. Yet it has been observed in experiments conducted on pinball machines (Figure 9.8
left) that for certain conditions, the projectile undergoes multiple internal scatterings and ends
up with a final trajectory that is apparently unrelated to the initial one. Your problem is to
determine if this process can be modeled as scattering from a static potential or if there must
be active mechanisms built into the pinball machines that cause chaotic scattering.
Although this problem is easy to solve on the computer, the results have some chaotic
features that are surprising (chaos is discussed further in Chapter 12, “Discrete & Continuous
Nonlinear Dynamics”). In fact, the applet Disper2e.html on the CD (created by Jaime Zulu-
aga) that simulates this problem continues to be a source of wonderment for readers as well as
authors.
9.14.1 Model and Theory
Our model for balls bouncing off the electrically activated bumpers in pinball machines is a
point particle scattering from the stationary 2-D potential [Bleh 90]
V (x, y) = ±x2y2e−(x2+y2). (9.58)
This potential has four circularly symmetric peaks in the xy plane (Figure 9.8 right). The
two signs correspond to repulsive and attractive potentials, respectively (the pinball machine
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
196 CHAPTER 9
Figure 9.8 Left: A classic pinball machine in which multiple scatterings occur from the round structures on the
top. Right: Scattering from the potential V(x,y) = x2y2e−(x
2+y2). The incident velocity is v in the y
direction, and the impact parameter (x value) is b. The velocity of the scattered particle is v′, and its
scattering angle is θ.
v
V(x,y)
x
y
b
v'

contains only repulsive interactions). Because there are four peaks in this potential, we suspect
that it may be possible to have multiple scatterings in which the projectile bounces back and
forth among the peaks, somewhat as in a pinball machine.
The theory for this problem is classical dynamics. Visualize a scattering experiment in
which a projectile starting out at an infinite distance from a target with a definite velocity v and
an impact parameter b (Figure 9.8 right) is incident on a target. After interacting with the target
and moving a nearly infinite distance from it, the scattered particle is observed at the scattering
angle θ. Because the potential cannot recoil, the speed of the projectile does not change, but
its direction does. An experiment typically measures the number of particles scattered and
then converts this to a function, the differential cross section σ(θ), which is independent of the
details of the experimental apparatus:
σ(θ) = lim
Nscatt(θ)/∆Ω
Nin/∆Ain
. (9.59)
Here Nscatt(θ) is the number of particles per unit time scattered into the detector at angle θ
subtending a solid angle ∆Ω, Nin is the number of particle per unit time incident on the target
of cross-sectional area ∆Ain, and the limit in (9.59) is for infinitesimal detector and area sizes.
The definition (9.59) for the cross section is the one that experimentalists use to convert
their measurements to a function that can be calculated by theory. We as theorists solve for the
trajectories of particles scattered from the potential (9.58) and from them deduce the scattering
angle θ. Once we have the scattering angle, we predict the differential cross section from the
dependence of the scattering angle upon the classical impact parameter b [M&T 03]:
σ(θ) =
b∣∣dθ
db
∣∣ sin θ(b) . (9.60)
The surprise you should find in the simulation is that for certain parameters, dθ(b)/db has zeros
and discontinuities, and this leads to a highly variable, large cross section.
The dynamical equations to solve are just Newton’s law for the x and y motions with the
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 197
potential (9.58):
F =ma
−∂V
∂x
î− ∂V
∂y
ĵ=m
d2x
dt2
, (9.61)
∓ 2xye−(x2+y2)
[
y(1− x2)̂i+ x(1− y2)ĵ
]
=m
d2x
dt2
î+m
d2y
dt2
ĵ. (9.62)
The equations for the x and y motions are simultaneous second-order ODEs:
m
d2x
dt2
=∓ 2y2x(1− x2)e−(x2+y2), (9.63)
m
d2y
dt2
=∓ 2x2y(1− y2)e−(x2+y2). (9.64)
Because the force vanishes at the peaks in Figure 9.8, these equations tell us that the peaks are
xmll
at x = ±1 and y = ±1. Substituting these values into the potential (9.58) yields Vmax = ±e−2,
which sets the energy scale for the problem.
9.14.2 Implementation
In §9.16.1 we will also describe how to express simultaneous ODEs such as (9.63) and (9.64)
in the standard rk4 form. Even though both equations are independent, we solve them simul-
taneously to determine the scattering trajectory [x(t), y(t)]. We use the same rk4 algorithm we
used for a single second-order ODE, only now the arrays will be 4-D rather than 2-D:
dy(t)
dt
= f(t,y), (9.65)
y(0)
def= x(t), y(1) def= y(t), (9.66)
y(2)
def=
dx
dt
, y(3)
def=
dy
dt
, (9.67)
where the order in which the y(i)s are assigned is arbitrary. With these definitions and equations
(9.63) and (9.64), we can assign values for the force function:
f (0) = y(2), f (1) = y(3), (9.68)
f (2) =
∓1
m
2y2x(1− x2)e−(x2+y2) = ∓1
m
2y(1)
2
y(0)(1− y(0)2)e−(y(0)
2
+y(1)
2
),
f (3) =
∓1
m
2x2y(1− y2)e−(x2+y2) = ∓1
m
2y(0)
2
y(1)(1− y(1)2)e−(y(0)
2
+y(1)
2
).
To deduce the scattering angle from our simulation, we need to examine the trajectory of
the scattered particle at an “infinite” separation from the target. To approximate that, we wait
until the scattered particle no longer feels the potential (say |PE|/KE ≤ 10−10) and call this
infinity. The scattering angle is then deduced from the components of velocity,
θ = tan−1
(
vy
vx
)
= atan2(Vx, Vy). (9.69)
Here atan2 is a function in most computer languages that computes the arctangent in the
correct quadrant without requiring any explicit divisions (that can blow up).
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
198 CHAPTER 9
9.14.3 Assessment
1. Apply the rk4 method to solve the simultaneous second-order ODEs (9.63) and (9.64)
with a 4-D force function.
2. The initial conditions are (a) an incident particle with only a y component of velocity
and (b) an impact parameter b (the initial x value). You do not need to vary the initial y,
but it should be large enough such that PE/KE ≤ 10−10, which means that the KE ' E.
3. Good parameters are m = 0.5, vy(0) = 0.5, vx(0) = 0.0, ∆b = 0.05, −1 ≤ b ≤ 1. You
may want to lower the energy and use a finer step size once you have found regions of
rapid variation.
4. Plot a number of trajectories [x(t), y(t)] that show usual and unusual behaviors. In par-
ticular, plot those for which backward scattering occurs, and consequently for which
there is much multiple scattering.
5. Plot a number of phase space trajectories [x(t), ẋ(t)] and [y(t), ẏ(t)]. How do these differ
from those of bound states?
6. Determine the scattering angle θ = atan2(Vx,Vy) by determining the velocity of the
scattered particle after it has left the interaction region, that is, PE/KE ≤ 10−10.
7. Identify which characteristics of a trajectory lead to discontinuities in dθ/db and thus
σ(θ).
8. Run the simulations for both attractive and repulsive potentials and for a range of ener-
gies less than and greater than Vmax = exp(−2).
9. Time delay: Another way to find unusual behavior in scattering is to compute the time
delay T (b) as a function of the impact parameter b. The time delay is the increase in
the time it takes a particle to travel through the interaction region after the interaction is
turned on. Look for highly oscillatory regions in the semilog plot of T (b), and once you
find some, repeat the simulation at a finer scale by setting b ' b/10 (the structures are
fractals, see Chapter 13, “Fractals & Statistical Growth”).
9.15 PROBLEM 2: BALLS FALLING OUT OF THE SKY
Golf and baseball players claim that hit balls appear to fall straight down out of the sky at the
end of their trajectories (the solid curve in Figure 9.9). Your problem is to determine whether
there is a simple physics explanation for this effect or whether it is “all in the mind’s eye.” And
while you are wondering why things fall out of the sky, see if you can use your new-found
numerical tools to explain why planets do not fall out of the sky.
9.16 THEORY: PROJECTILE MOTION WITH DRAG
Figure 9.9 shows the initial velocity V0 and inclination θ for a projectile launched from the
origin. If we ignore air resistance, the projectile has only the force of gravity acting on it and
therefore has a constant acceleration g = 9.8 m/s2 in the negative y direction. The analytic
solutions to the equations of motion are
x(t) = V0xt, y(t) = V0yt− 12gt
2, (9.70)
vx(t) = V0x, vy(t) = V0y − gt, (9.71)
where (V0x, V0y) = V0(cos θ, sin θ). Solving for t as a function of x and substituting it into the
y(t) equation show that the trajectory is a parabola:
y =
V0y
V0x
x− g
2V 20x
x2. (9.72)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 199
Figure 9.9 The trajectory of a projectile fired with initial velocity V0 in the θ direction. The solid curve includes air
resistance.
V0
y

R
H
x
with drag
0
0
Likewise, it is easy to show (dashed curve in Figure 9.9) that without friction the range R =
2V 20 sin θ cos θ/g and the maximum height H =
1
2V
2
0 sin
2 θ/g.
The parabola of frictionless motion is symmetric about its midpoint and so does not
describe a ball dropping out of the sky. We want to determine if the inclusion of air resistance
leads to trajectories that are much steeper at their ends than at their beginnings (solid curve in
Figure 9.9). The basic physics is Newton’s second law in two dimensions for a frictional force
F(f) opposing motion, and a vertical gravitational force −mgêy:
F(f) −mgêy = m
d2x(t)
dt2
, (9.73)
⇒ F (f)x = m
d2x
dt2
, F (f)y −mg = m
d2y
dt2
, (9.74)
where the bold symbols indicate vector quantities.
The frictional force F(f) is not a basic force of nature but rather a simple model of a
complicated phenomenon. We do know that friction always opposes motion, which means it is
in the direction opposite to velocity. One model assumes that the frictional force is proportional
to a power n of the projectile’s speed [M&T 03]:
F(f) = −km |v|n v
|v|
, (9.75)
where the −v/|v| factor ensures that the frictional force is always in a direction opposite that
of the velocity. Physical measurements indicate that the power n is noninteger and varies
with velocity, and so a more accurate model would be a numerical one that uses the empirical
velocity dependence n(v). With a constant power law for friction, the equations of motion are
xmll
d2x
dt2
= −k vnx
vx
|v|
,
d2y
dt2
= −g − k vny
vy
|v|
, |v| =
√
v2x + v2y . (9.76)
We shall consider three values for n, each of which represents a different model for the air
resistance: (1) n = 1 for low velocities; (2) n = 32 , for medium velocities; and (3) n = 2 for
high velocities.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
200 CHAPTER 9
Figure 9.10 Left: The gravitational force on a planet a distance r from the sun. The x and y components of the
force are indicated. Right: Output from the applet Planet (on the CD) showing the precession of a
planet’s orbit when the gravitational force ∝ 1/r4.
f
9.16.1 Simultaneous Second-Order ODEs
Even though (9.76) are simultaneous second-order ODEs, we can still use our regular ODE
solver on them after expressing them in standard form
dy
dt
= y(t,y) (standard form). (9.77)
We pick y to be the 4-D vector of dependent variables:
y(0) = x(t), y(1) =
dx
dt
, y(2) = y(t), y(3) =
dy
dt
. (9.78)
We express the equations of motion in terms of y to obtain the standard form:
dy(0)
dt
(
≡ dx
dt
)
= y(1),
dy(1)
dt
(
≡ d
2x
dt2
)
=
1
m
F (f)x (y)
dy(2)
dt
(
≡ dy
dt
)
= y(3),
dy(3)
dt
(
≡ d
2y
dt2
)
=
1
m
F (f)y (y)− g.
And now we just read off the components of the force function f(t,y):
f (0) = y(1), f (1) =
1
m
F (f)x , f
(2) = y(3), f (3) =
1
m
F (f)y − g.
9.16.2 Assessment
1. Modify your rk4 program so that it solves the simultaneous ODEs for projectile motion
(9.76) with friction (n = 1).
2. Check that you obtain graphs similar to those in Figure 9.9.
3. The model (9.75) with n = 1 is okay for low velocities. Now modify your program to
handle n = 32 (medium-velocity friction) and n = 2 (high-velocity friction). Adjust the
value of k for the latter two cases such that the initial force of friction k vn0 is the same
for all three cases.
4. What is your conclusion about balls falling out of the sky?
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DIFFERENTIAL EQUATION APPLICATIONS 201
9.17 PROBLEM 3: PLANETARY MOTION
Newton’s explanation of the motion of the planets in terms of a universal law of gravitation is
one of the great achievements of science. He was able to prove that the planets traveled along
elliptical paths with the sun at one vertex and to predict periods of the motion accurately. All
Newton needed to postulate was that the force between a planet of mass m and the sun of mass
M is
F (g) = −GmM
r2
. (9.79)
Here r is the planet-CM distance, G is the universal gravitational constant, and the attractive
force lies along the line connecting the planet and the sun (Figure 9.10 left). The hard part
for Newton was solving the resulting differential equations because he had to invent calculus
to do it and then had go through numerous analytic manipulations. The numerical solution is
straightforward since even for planets the equation of motion is still
f = ma = m
d2x
dt2
, (9.80)
with the force (9.79) having components (Figure 9.10)
fx =F (g) cos θ = F (g)
x
r
, (9.81)
fy =F (g) sin θ = F (g)
y
r
, (9.82)
r=
√
x2 + y2. (9.83)
The equation of motion yields two simultaneous second-order ODEs:
xmlld2x
dt2
= −GM x
r3
,
d2y
dt2
= −GM y
r3
. (9.84)
9.17.1 Implementation: Planetary Motion
1. Assume units such that GM = 1 and use the initial conditions
x(0) = 0.5, y(0) = 0, vx(0) = 0.0, vy(0) = 1.63.
2. Modify your ODE solver program to solve (9.84).
3. You may need to make the time step small enough so that the elliptical orbit closes upon
itself, as it should, and the number of steps large enough such that the orbits just repeat.
4. Experiment with the initial conditions until you find the ones that produce a circular orbit
(a special case of an ellipse).
5. Once you have obtained good precision, note the effect of progressively increasing the
initial velocity until the orbits open up and become hyperbolic.
6. Using the same initial conditions that produced the ellipse, investigate the effect of the
power in (9.79) being 1/r4 rather than 1/r2. You should find that the orbital ellipse now
rotates or precesses (Figure 9.10). In fact, as you should verify, even a slight variation
from an inverse square power law (as arises from general relativity) causes the orbit to
precess.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
202 CHAPTER 9
9.17.1.1 Exploration: Restricted Three-Body Problem
Extend the previous solution for planetary motion to one in which a satellite of tiny mass moves
under the influence of two planets of equal massM = 1. Consider the planets as rotating about
their center of mass in circular orbits and of such large mass that they are uninfluenced by the
satellite. Assume that all motions remain in the xy plane and that the units are such thatG = 1.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Ten
Fourier Analysis; Signals and Filters
In Unit I of this chapter we examine Fourier series and Fourier transforms, two standard tools
for decomposing periodic and nonperiodic motions. We find that as implemented for numerical
computation, both the series and the integral become a discrete Fourier transform (DFT),
which is simple to program. In Unit II we discuss the subject of signal filtering and see that
various Fourier tools can be used to reduce noise in measured or simulated signals. In Unit III
we present a discussion of the fast Fourier transform (FFT), a technique that is so efficient that
it permits evaluations of DFTs in real time.
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
Fourier Analysis (Intro) 10.1–10.3 Discrete Fourier Transform I 10.4
Discrete Fourier Transform II 10.4 Filters for Noise 10.5–10.7
DFT Aliasing 10.4.2 The Fast Fourier Transform 10.8–10.10
10.1 UNIT I. FOURIER ANALYSIS OF NONLINEAR OSCILLATIONS
Consider a particle oscillating in the nonharmonic potential of equation (9.4):
V (x) =
1
p
k|x|p, (10.1)
for p 6= 2, or for the perturbed harmonic oscillator (9.2),
V (x) =
1
2
kx2
(
1− 2
3
αx
)
. (10.2)
While free oscillations in these potentials are always periodic, they are not truly sinusoidal.
Your problem is to take the solution of one of these nonlinear oscillators and relate it to the
solution
x(t) = A0 sin(ωt+ φ0) (10.3)
of the linear harmonic oscillator. If your oscillator is sufficiently nonlinear to behave like the
sawtooth function (Figure 10.1 left), then the Fourier spectrum you obtain should be similar to
that shown on the right in Figure 10.1.
In general, when we undertake such a spectral analysis, we want to analyze the steady-
state behavior of a system. This means that the initial transient behavior has had a chance to
die out. It is easy to identify just what the initial transient is for linear systems but may be less
so for nonlinear systems in which the “steady state” jumps among a number of configurations.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
204 CHAPTER 10
Figure 10.1 Left: A sawtooth function in time. Right: The Fourier spectrum of frequencies in natural units
contained in this sawtooth function.
t
-1
0
1
y(
t)
0 20
-1
0
1
Y
(ω
)

10.2 FOURIER SERIES (MATH)
Part of our interest in nonlinear oscillations arises from their lack of study in traditional physics
courses even though linear oscillations are just the first approximation to a naturally oscillatory
system. If the force on a particle is always toward its equilibrium position (a restoring force),
then the resulting motion will be periodic but not necessarily harmonic. A good example is
the motion in a highly anharmonic well p ' 10 that produces an x(t) looking like a series of
pyramids; this motion is periodic but not harmonic.
In numerical analysis there really is no distinction between a Fourier integral and a
Fourier series because the integral is always approximated as a finite series. We will illus-
trate both methods. In a sense, our approach is the inverse of the traditional one in which
the fundamental oscillation is determined analytically and the higher-frequency overtones are
determined by perturbation theory [L&L,M 76]. We start with the full (numerical) periodic
solution and then decompose it into what may be called harmonics. When we speak of funda-
mentals, overtones, and harmonics, we speak of solutions to the linear boundary-value prob-
lem, for example, of waves on a plucked violin string. In this latter case, and when given the
correct conditions (enough musical skill), it is possible to excite individual harmonics or sums
of them in the series
y(t) = b0 sinω0t+ b1 sin 2ω0t+ · · · . (10.4)
Anharmonic oscillators vibrate at a single frequency (which may vary with amplitude) but not
with a sinusoidal waveform. Expanding the oscillations in a Fourier series does not imply that
the individual harmonics can be excited (played).
You may recall from classical mechanics that the general solution for a vibrating system
can be expressed as the sum of the normal modes of that system. These expansions are pos-
sible because we have linear operators and, subsequently, the principle of superposition: If
y1(t) and y2(t) are solutions of some linear equation, then α1y1(t)+α2y2(t) is also a solution.
The principle of linear superposition does not hold when we solve nonlinear problems. Nev-
ertheless, it is always possible to expand a periodic solution of a nonlinear problem in terms
of trigonometric functions with frequencies that are integer multiples of the true frequency of
the nonlinear oscillator.1 This is a consequence of Fourier’s theorem being applicable to any
single-valued periodic function with only a finite number of discontinuities. We assume we
know the period T , that is, that
y(t+ T ) = y(t). (10.5)
1We remind the reader that every periodic system by definition has a period T and consequently a true frequency ω. Nonethe-
less, this does not imply that the system behaves like sinωt. Only harmonic oscillators do that.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FOURIER ANALYSIS; SIGNALS AND FILTERS 205
This tells us the true frequency ω:
ω ≡ ω1 =
2π
T
. (10.6)
A periodic function (usually called the signal) can be expanded as a series of harmonic func-
tions with frequencies that are multiples of the true frequency:
xmlly(t) = a0
2
+
∞∑
n=1
(an cosnωt+ bn sinnωt) . (10.7)
This equation represents the signal y(t) as the simultaneous sum of pure tones of frequency
nω. The coefficients an and bn measure of the amount of cosnωt and sinnωt present in y(t),
specifically, the intensity or power at each frequency is proportional to a2n + b
2
n.
The Fourier series (10.7) is a best fit in the least-squares sense of Chapter 8, “Solving
Systems of Equations with Matrices; Data Fitting,” because it minimizes
∑
i[y(ti) − yi]2,
where i denotes different measurements of the signal. This means that the series converges to
the average behavior of the function but misses the function at discontinuities (at which points
it converges to the mean) or at sharp corners (where it overshoots). A general function y(t)
may contain an infinite number of Fourier components, although low-accuracy reproduction is
usually possible with a small number of harmonics.
The coefficients an and bn are determined by the standard techniques for function ex-
pansion. To find them, multiply both sides of (10.7) by cosnωt or sinnωt, integrate over one
period, and project a single an or bn:
xmll
(
an
bn
)
=
2
T
∫ T
0
dt
(
cosnωt
sinnωt
)
y(t), ω def=
2π
T
. (10.8)
As seen in the bn coefficients (Figure 10.1 right), these coefficients usually decrease in magni-
tude as the frequency increases and can occur with a negative sign, the negative sign indicating
relative phase.
Awareness of the symmetry of the function y(t) may eliminate the need to evaluate all
the expansion coefficients. For example,
• a0 is twice the average value of y:
a0 = 2 〈y(t)〉 . (10.9)
• For an odd function, that is, one for which y(−t) = −y(t), all of the coefficients an ≡ 0
and only half of the integration range is needed to determine bn:
bn =
4
T
∫ T/2
0
dt y(t) sinnωt. (10.10)
However, if there is no input signal for t < 0, we do not have a truly odd function, and so
small values of an may occur.
• For an even function, that is, one for which y(−t) = y(t), the coefficient bn ≡ 0 and only
half the integration range is needed to determine an:
an =
4
T
∫ T/2
0
dt y(t) cosnωt. (10.11)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
206 CHAPTER 10
10.2.1 Example 1: Sawtooth Function
The sawtooth function (Figure 10.1) is described mathematically as
y(t) =

t
T/2 , for 0 ≤ t ≤
T
2
,
t−T
T/2 , for
T
2
≤ t ≤ T.
(10.12)
It is clearly periodic, nonharmonic, and discontinuous. Yet it is also odd and so can be repre-
sented more simply by shifting the signal to the left:
y(t) =
t
T/2
, −T
2
≤ t ≤ T
2
. (10.13)
Even though the general shape of this function can be reproduced with only a few terms of the
Fourier components, many components are needed to reproduce the sharp corners. Because
the function is odd, the Fourier series is a sine series and (10.8) determines the values
bn =
2
T
∫ +T/2
−T/2
dt sinnωt
t
T/2
=
ω
π
∫ +π/ω
−π/ω
dt sinnωt
ωt
π
=
2
nπ
(−1)n+1,
⇒ y(t) = 2
π
[
sinωt− 1
2
sin 2ωt+
1
3
sin 3ωt− · · ·
]
. (10.14)
10.2.2 Example 2: Half-wave Function
The half-wave function
y(t) =
sinωt, for 0 < t < T/2,0, for T/2 < t < T, (10.15)
is periodic, nonharmonic (the upper half of a sine wave), and continuous, but with discontin-
uous derivatives. Because it lacks the sharp corners of the sawtooth function, it is easier to
reproduce with a finite Fourier series. Equation (10.8) determines
an =

−2
π(n2−1) , n even or 0,
0, n odd,
bn =

1
2
, n = 1,
0, n 6= 1,
⇒ y(t) = 1
2
sinωt+
1
π
− 2
3π
cos 2ωt− 2
15π
cos 4ωt+ · · · . (10.16)
10.3 EXERCISE: SUMMATION OF FOURIER SERIES
1. Sawtooth function: Sum the Fourier series for the sawtooth function up to order n =
2, 4, 10, 20 and plot the results over two periods.
a. Check that in each case the series gives the mean value of the function at the points
of discontinuity.
b. Check that in each case the series overshoots by about 9% the value of the function
on either side of the discontinuity (the Gibbs phenomenon).
2. Half-wave function: Sum the Fourier series for the half-wave function up to order
n = 2, 4, 10 and plot the results over two periods. (The series converges quite well,
doesn’t it?)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FOURIER ANALYSIS; SIGNALS AND FILTERS 207
10.4 FOURIER TRANSFORMS (THEORY)
Although a Fourier series is the right tool for approximating or analyzing periodic functions,
the Fourier transform or integral is the right tool for nonperiodic functions. We convert from
series to transform by imagining a system described by a continuum of “fundamental” frequen-
cies. We thereby deal with wave packets containing continuous rather than discrete frequen-
cies.2 While the difference between series and transform methods may appear clear mathemat-
ically, when we approximate the Fourier integral as a finite sum, the two become equivalent.
By analogy with (10.7), we now imagine our function or signal y(t) expressed in terms
of a continuous series of harmonics (inverse Fourier transform):
xmll
y(t) =
∫ +∞
−∞
dω Y (ω)
eiωt√
2π
, (10.17)
where for compactness we use a complex exponential function.3 The expansion amplitude
Y (ω) is analogous to the Fourier coefficients (an, bn) and is called the Fourier transform of
y(t). The integral (10.17) is the inverse transform since it converts the transform to the signal.
The Fourier transform converts y(t) to its transform Y (ω):
Y (ω) =
∫ +∞
−∞
dt
e−iωt√
2π
y(t). (10.18)
The 1/
√
2π factor in both these integrals is a common normalization in quantum mechanics
but maybe not in engineering where only a single 1/2π factor is used. Likewise, the signs in
the exponents are also conventions that do not matter as long as you maintain consistency.
If y(t) is the measured response of a system (signal) as a function of time, then Y (ω) is
the spectral function that measures the amount of frequency ω present in the signal. [However,
some experiments may measure Y (ω) directly, in which case an inverse transform is needed to
obtain y(t).] In many cases Y (ω) is a complex function with positive and negative values and
with significant variation in magnitude. Accordingly, it is customary to eliminate some of the
complexity of Y (ω) by making a semilog plot of the squared modulus |Y (ω)|2 versus ω. This
is called a power spectrum and provides you with an immediate view of the amount of power
or strength in each component.
If the Fourier transform and its inverse are consistent with each other, we should be able
to substitute (10.17) into (10.18) and obtain an identity:
Y (ω)=
∫ +∞
−∞
dt
e−iωt√
2π
∫ +∞
−∞
dω′
eiω
′t
√
2π
Y (ω′)=
∫ +∞
−∞
dω′
{∫ +∞
−∞
dt
ei(ω
′−ω)t
2π
}
Y (ω′).
For this to be an identity, the term in braces must be the Dirac delta function:∫ +∞
−∞
dt ei(ω
′−ω)t = 2πδ(ω′ − ω). (10.19)
While the delta function is one of the most common and useful functions in theoretical physics,
2We follow convention and consider time t the function’s variable and frequency ω the transform’s variable. Nonetheless, these
can be reversed or other variables such as position x and wave vector k may also be used.
3Recollect the principle of linear superposition and that exp(iωt) = cosωt+ i sinωt. This means that the real part of y gives
the cosine series, and the imaginary part the sine series.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
208 CHAPTER 10
it is not well behaved in a mathematical sense and misbehaves terribly in a computational
sense. While it is possible to create numerical approximations to δ(ω′ − ω), they may well
be borderline pathological. It is certainly better for you to do the delta function part of an
integration analytically and leave the nonsingular leftovers to the computer.
10.4.1 Discrete Fourier Transform Algorithm
If y(t) or Y (ω) is known analytically or numerically, the integral (10.17) or (10.18) can be
evaluated using the integration techniques studied earlier. In practice, the signal y(t) is mea-
sured at just a finite number N of times t, and these are what we must use to approximate the
transform. The resultant discrete Fourier transform is an approximation both because the sig-
nal is not known for all times and because we integrate numerically.4 Once we have a discrete
set of transforms, they can be used to reconstruct the signal for any value of the time. In this
way the DFT can be thought of as a technique for interpolating, compressing, and extrapolating
data.
We assume that the signal y(t) is sampled at (N + 1) discrete times (N time intervals),
with a constant spacing h between times:
yk
def= y(tk), k = 0, 1, 2, . . . , N, (10.20)
tk
def= kh, h = ∆t. (10.21)
In other words, we measure the signal y(t) once every hth of a second for a total time T . This
corresponds to period T and sampling rate s:
T
def= Nh, s =
N
T
=
1
h
. (10.22)
Regardless of the actual periodicity of the signal, when we choose a period T over which to
sample, the mathematics produces a y(t) that is periodic with period T ,
y(t+ T ) = y(t). (10.23)
We recognize this periodicity, and ensure that there are only N independent measurements
used in the transform, by requiring the first and last y’s to be the same:
y0 = yN . (10.24)
If we are analyzing a truly periodic function, then the first N points should all be within one
period to guarantee their independence. Unless we make further assumptions, theseN indepen-
dent input data y(tk) can determine no more than N independent output Fourier components
Y (ωk).
The time interval T (which should be the period for periodic functions) is the largest time
over which we consider the variation of y(t). Consequently, it determines the lowest frequency,
ω1 =
2π
T
, (10.25)
contained in our Fourier representation of y(t). The frequencies ωn are determined by the
number of samples taken and by the total sampling time T = Nh as
ωn = nω1 = n
2π
Nh
, n = 0, 1, . . . , N. (10.26)
4More discussion can be found in [B&H 95], which is devoted to just this topic.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FOURIER ANALYSIS; SIGNALS AND FILTERS 209
Here ω0 = 0 corresponds to the zero-frequency or DC component.
The DFT algorithm results from (1) evaluating the integral in (10.18) not from −∞ to
+∞ but rather from time 0 to time T over which the signal is measured, and from (2) using
the trapezoid rule for the integration5
Y (ωn)
def=
∫ +∞
−∞
dt
e−iωnt√
2π
y(t) '
∫ T
0
dt
e−iωnt√
2π
y(t), (10.27)
'
N∑
k=1
h y(tk)
e−iωntk√
2π
= h
N∑
k=1
yk
e−2πikn/N√
2π
. (10.28)
To keep the final notation more symmetric, the step size h is factored from the transform Y and
a discrete function Yn is defined:
Yn
def=
1
h
Y (ωn) =
N∑
k=1
yk
e−2πikn/N√
2π
. (10.29)
With this same care in accounting, and with dω → 2π/Nh, we invert the Yn’s:
y(t) def=
∫ +∞
−∞
dω
eiωt√
2π
Y (ω) '
N∑
n=1
2π
Nh
eiωnt√
2π
Y (ωn). (10.30)
Once we know the N values of the transform, we can use (10.30) to evaluate y(t) for any time
t. There is nothing illegal about evaluating Yn and yk for arbitrarily large values of n and k, yet
there is also nothing to be gained. Because the trigonometric functions are periodic, we just
get the old answer:
y(tk+N ) = y(tk), Y (ωn+N ) = Y (ωn). (10.31)
Another way of stating this is to observe that none of the equations change if we replace ωnt
by ωnt+ 2πn. There are still just N independent output numbers for N independent inputs.
We see from (10.26) that the larger we make the time T = Nh over which we sample
the function, the smaller will be the frequency steps or resolution.6 Accordingly, if you want
a smooth frequency spectrum, you need to have a small frequency step 2π/T . This means
you need a large value for the total observation time T . While the best approach would be
to measure the input signal for longer times, in practice a measured signal y(t) is often ex-
tended in time (“padded”) by adding zeros for times beyond the last measured signal, which
thereby increases the value of T . Although this does not add new information to the analysis,
it does build in the experimentalist’s belief that the signal has no existence at times after the
measurements are stopped.
While periodicity is expected for Fourier series, it is somewhat surprising for Fourier
integrals, which have been touted as the right tool for nonperiodic functions. Clearly, if we
input values of the signal for longer lengths of time, then the inherent period becomes longer,
and if the repeat period is very long, it may be of little consequence for times short compared
to the period. If y(t) is actually periodic with period Nh, then the DFT is an excellent way
of obtaining Fourier series. If the input function is not periodic, then the DFT can be a bad
approximation near the endpoints of the time interval (after which the function will repeat) or,
correspondingly, for the lowest frequencies.
5The alert reader may be wondering what has happened to the h/2 with which the trapezoid rule weights the initial and final
points. Actually, they are there, but because we have set y0 ≡ yN , two h/2 terms have been added to produce one h term.
6See also §10.4.2 where we discuss the related phenomenon of aliasing.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
210 CHAPTER 10
The discrete Fourier transform and its inverse can be written in a concise and insightful
way, and be evaluated efficiently, by introducing a complex variable Z for the exponential and
then raising Z to various powers:
xmll
yk =
√
2π
N
N∑
n=1
Z−nkYn, Z = e−2πi/N , (10.32)
Yn =
1√
2π
N∑
k=1
Znkyk, n = 0, 1, . . . , N, (10.33)
where Znk ≡ [(Z)n]k. With this formulation, the computer needs to compute only powers of
Z. We give our DFT code in Listing 10.1. If your preference is to avoid complex numbers, we
can rewrite (10.32) in terms of separate real and imaginary parts by applying Euler’s theorem:
Z = e−iθ, ⇒ Z±nk = e∓inkθ = cosnkθ ∓ i sinnkθ, (10.34)
where θ def= 2π/N . In terms of the explicit real and imaginary parts,
Yn =
1√
2π
N∑
k=1
[(cos(nkθ)Re yk + sin(nkθ) Im yk
+ i(cos(nkθ) Im yk − sin(nkθ)Re yk)] , (10.35)
yk =
√
2π
N
N∑
n=1
[(cos(nkθ) ReYn − sin(nkθ)Im Yn
+ i(cos(nkθ)Im Yn + sin(nkθ) ReYn )] . (10.36)
Readers new to DFTs are often surprised when they apply these equations to practical
situations and end up with transforms Y having imaginary parts, even though the signal y is
real. Equation (10.35) shows that a real signal (Im yk ≡ 0) will yield an imaginary transform
unless
∑N
k=1 sin(nkθ) Re yk = 0. This occurs only if y(t) is an even function over −∞ ≤
t ≤ +∞ and we integrate exactly. Because neither condition holds, the DFTs of real, even
functions may have small imaginary parts. This is not due to an error in programming and in
fact is a good measure of the approximation error in the entire procedure.
The computation time for a discrete Fourier transform can be reduced even further by
use of the fast Fourier transform algorithm. An examination of (10.32) shows that the DFT
is evaluated as a matrix multiplication of a vector of length N containing the Z values by a
vector of length N of y value. The time for this DFT scales like N2, while the time for the
FFT algorithm scales as N log2N . Although this may not seem like much of a difference, for
N = 102−3, the difference of 103−5 is the difference between a minute and a week. For this
reason, FFT is often used for on-line analysis of data. We discuss FFT techniques in §10.8.
Listing 10.1 DFTcomplex.py computes the discrete Fourier transform for the signal in method f( ). 
# DFTcomplex . py : D i s c r e t e F o u r i e r Transform , u s i n g b u i l t i n complex f u n c t i o n s
from v i s u a l i m p o r t ∗ ; from v i s u a l . g raph i m p o r t ∗
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FOURIER ANALYSIS; SIGNALS AND FILTERS 211
i m p o r t cmath # f o r complex math f u n c t i o n s
s i g n g r = g d i s p l a y ( x =0 , y =0 , wid th =600 , h e i g h t =250 , t i t l e =’Signal’ , x t i t l e =’x’ , y t i t l e =
’signal’ , xmax = 2 .∗math . pi , xmin = 0 , ymax = 30 , ymin = 0)
s i g f i g = gc u rv e ( c o l o r = c o l o r . ye l low , d i s p l a y = s i g n g r )
imagr = g d i s p l a y ( x =0 , y =250 , wid th =600 , h e i g h t =250 , t i t l e =’Imag Fourier TF’ ,
x t i t l e = ’x’ , y t i t l e =’TF.Imag’ , xmax = 1 0 . , xmin=−1, ymax =1000 , ymin =−0.200)
i m p a r t = g v b a r s ( d e l t a = 0 . 0 5 , c o l o r = c o l o r . red , d i s p l a y = imagr ) # t h i n b a r s
r e a g r = g d i s p l a y ( x =0 , y =500 , wid th =600 , h e i g h t =250 , t i t l e =’Real Fourier TF’ ,
x t i t l e =’x’ , y t i t l e =’TF.Real’ , xmax = 1 0 . , xmin=−1, ymax =2000 , ymin =−0.200)
r e a p a r t = g v b a r s ( d e l t a = 0 . 0 5 , c o l o r = c o l o r . b lue , d i s p l a y = r e a g r ) # t h i n b a r s
N = 1000 ; Np = N # Number p o i n t s
s i g n a l = z e r o s ( (N+1) , F l o a t )
twop i = 2 .∗math . p i ; s q 2 p i = 1 . / math . s q r t ( twop i ) ; h = twop i /N
d f t z = z e r o s ( ( Np ) , Complex ) # s e q u e n c e complex e l e m e n t s
d e f f ( s i g n a l ) : # s i g n a l f u n c t i o n
s t e p = twop i /N; x = 0 .
f o r i i n r a n g e ( 0 , N+1) :
s i g n a l [ i ] = (1+2∗math . s i n ( x +2) +3∗math . s i n (4∗x +2) ) ∗(1+2∗math . s i n ( x +2)
+ 3∗math . s i n (4∗x + 2 . ) )
s i g f i g . p l o t ( pos = ( x , s i g n a l [ i ] ) ) # p l o t s i g n a l
x += s t e p
d e f f o u r i e r ( d f t z ) : # DFT
f o r n i n r a n g e ( 0 , Np ) :
zsum = complex ( 0 . 0 , 0 . 0 ) # r e a l and imag p a r t s = z e r o
f o r k i n r a n g e ( 0 , N) : # loop f o r sums
zexpo = complex ( 0 , twop i∗k∗n /N) # complex e x p o n e n t
zsum += s i g n a l [ k ]∗ exp(−zexpo ) # F o u r i e r t r a n s f o r m c o r e
d f t z [ n ] = zsum ∗ s q 2 p i # f a c t o r
i f d f t z [ n ] . imag != 0 : # p l o t i f n o t t o o s m a l l imag
i m p a r t . p l o t ( pos = ( n , d f t z [ n ] . imag ) ) # p l o t b a r s
i f d f t z [ n ] . r e a l != 0 : # p l o t t h e r e a l p a r t
r e a p a r t . p l o t ( pos = ( n , d f t z [ n ] . r e a l ) )
f ( s i g n a l ) ; f o u r i e r ( d f t z ) # c a l l s i g n a l , t r a n s f o r m
10.4.2 Aliasing(Assessment) 
The sampling of a signal by DFT for only a finite number of times limits the accuracy of the
deduced high-frequency components present in the signal. Clearly, good information about
very high frequencies requires sampling the signal with small time steps so that all the wiggles
can be included. While a poor deduction of the high-frequency components may be tolerable if
all we care about are the low-frequency ones, the high-frequency components remain present
in the signal and may contaminate the low-frequency components that we deduce. This effect
is called aliasing and is the cause of the moir pattern distortion in digital images.
As an example, consider Figure 10.2 showing the two functions sin(πt/2) and sin(2πt)
for 0 ≤ t ≤ 8, with their points of overlap in bold. If we were unfortunate enough to sample
a signal containing these functions at the times t = 0, 2, 4, 6, 8, then we would measure y ≡ 0
and assume that there was no signal at all. However, if we were unfortunate enough to measure
the signal at the filled dots in Figure 10.2 where sin(πt/2) = sin(2πt), specifically, t =
0, 1210,
4
3, . . ., then our Fourier analysis would completely miss the high-frequency component.
In DFT jargon, we would say that the high-frequency component has been aliased by the low-
frequency component. In other cases, some high-frequency values may be included in our
sampling of the signal, but our sampling rate may not be high enough to include enough of
them to separate the high-frequency component properly. In this case some high-frequency
signals would be included spuriously as part of the low-frequency spectrum, and this would
lead to spurious low-frequency oscillations when the signal is synthesized from its Fourier
components.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
212 CHAPTER 10
Figure 10.2 A plot of the functions sin(πt/2) and sin(2πt ). If the sampling rate is not high enough, these signals
will appear indistinguishable. If both are present in a signal (e.g., as a signal that is the sum of the two)
and if the signal is not sampled at a high enough rate, the deduced low-frequency component will be
contaminated by the higher-frequency component.
sin(2 t) sin( t/2)
-1
0
1
2 4 6
More precisely, aliasing occurs when a signal containing frequency f is sampled at a
rate of s = N/T measurements per unit time, with s ≤ f/2. In this case, the frequencies
f and f − 2s yield the same DFT, and we would not be able to determine that there are two
frequencies present. That being the case, to avoid aliasing we want no frequencies f > s/2
to be present in our input signal. This is known as the Nyquist criterion. In practice, some
applications avoid the effects of aliasing by filtering out the high frequencies from the signal
and then analyzing the remaining low-frequency part. (The low-frequency sinc filter discussed
in §10.7.1 is often used for this.) Even though this approach eliminates some high-frequency
information, it lessens the distortion of the low-frequency components and so may lead to
improved reproduction of the signal.
If accurate values for the high frequencies are required, then we will need to increase the
sampling rate s by increasing the number N of samples taken within the fixed sampling time
T = Nh. By keeping the sampling time constant and increasing the number of samples taken,
we make the time step h smaller, and this picks up the higher frequencies. By increasing the
number N of frequencies that you compute, you move the higher-frequency components you
are interested in closer to the middle of the spectrum and thus away from the error-prone ends.
If we vary the the total time sampling time T = Nh but not the sampling rate s =
N/T = 1/h, we make ω1 smaller because the discrete frequencies
ωn = nω1 = n
2π
T
(10.37)
are measured in steps of ω1. This leads to a smoother frequency spectrum. However, to keep
the time step the same and thus not lose high-frequency information, we would also need to
increase the number ofN samples. And as we said, this is often done, after the fact, by padding
the end of the data set with zeros.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FOURIER ANALYSIS; SIGNALS AND FILTERS 213
10.4.3 Fourier Series DFT (Algorithm)
For simplicity let us consider the Fourier cosine series:
y(t) =
∞∑
n=0
an cos(nωt), ak =
2
T
∫ T
0
dt cos(kωt)y(t). (10.38)
Here T def= 2π/ω is the actual period of the system (not necessarily the period of the simple
harmonic motion occurring for a small amplitude). We assume that the function y(t) is sampled
for a discrete set of times
y(t = tk) ≡ yk, k = 0, 1, . . . , N. (10.39)
Because we are analyzing a periodic function, we retain the conventions used in the DFT and
require the function to repeat itself with period T = Nh; that is, we assume that the amplitude
is the same at the first and last points:
y0 = yN . (10.40)
This means that there are only N independent values of y being used as input. For these N
independent yk values, we can determine uniquely only N expansion coefficients ak. If we use
the trapezoid rule to approximate the integration in (10.38), we determine the N independent
Fourier components as
an '
2h
T
N∑
k=1
cos (nωtk) y(tk) =
2
N
N∑
k=1
cos
(
2πnk
N
)
yk, n = 0, . . . , N. (10.41)
Because we can determine only N Fourier components from N independent y(t) values, our
Fourier series for the y(t) must be in terms of only these components:
y(t) '
N∑
n=0
an cos(nωt) =
N∑
n=0
an cos
(
2πnt
Nh
)
. (10.42)
In summary, we sample the function y(t) at N times, t1, . . . , tN . We see that all the values
of y sampled contribute to each ak. Consequently, if we increase N in order to determine
more coefficients, we must recompute all the an values. In the wavelet analysis in Chapter 11,
“Wavelet Analysis & Data Compression,” the theory is reformulated so that additional sam-
plings determine higher Fourier components without affecting lower ones.
10.4.4 Assessments
Simple analytic input: It is always good to do these simple checks before examining more
complex problems. If your system has some Fourier analysis packages (such as the graph-
ing package Ace/gr), you may want to compare your results with those from the packages.
Once you understand how the packages work, it makes sense to use them.
1. Sample the even signal
y(t) = 3 cos(ωt) + 2 cos(3ωt) + cos(5ωt).
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
214 CHAPTER 10
Decompose this into its components; then check that they are essentially real and
in the ratio 3:2:1 (or 9:4:1 for the power spectrum) and that they resum to give the
input signal.
2. Experiment on the separate effects of picking different values of the step size h and
of enlarging the measurement period T = Nh.
3. Sample the odd signal
y(t) = sin(ωt) + 2 sin(3ωt) + 3 sin(5ωt).
Decompose this into its components; then check that they are essentially imaginary
and in the ratio 1:2:3 (or 1:4:9 if a power spectrum is plotted) and that they resum
to give the input signal.
4. Sample the mixed-symmetry signal
y(t) = 5 sin(ωt) + 2 cos(3ωt) + sin(5ωt).
Decompose this into its components; then check that there are three of them in the
ratio 5:2:1 (or 25:4:1 if a power spectrum is plotted) and that they resum to give the
input signal.
5. Sample the signal
y(t) = 5 + 10 sin(t+ 2).
Compare and explain the results obtained by sampling (a) without the 5, (b) as given
but without the 2, and (c) without the 5 and without the 2.
6. In our discussion of aliasing, we examined Figure 10.2 showing the functions
sin(πt/2) and sin(2πt). Sample the function
y(t) = sin
(π
2
t
)
+ sin(2πt)
and explore how aliasing occurs. Explicitly, we know that the true transform con-
tains peaks at ω = π/2 and ω = 2π. Sample the signal at a rate that leads to
aliasing, as well as at a higher sampling rate at which there is no aliasing. Com-
pare the resulting DFTs in each case and check if your conclusions agree with the
Nyquist criterion.
Highly nonlinear oscillator: Recall the numerical solution for oscillations of a spring with
power p = 12 [see (10.1)]. Decompose the solution into a Fourier series and determine
the number of higher harmonics that contribute at least 10%; for example, determine the
n for which |bn/b1| < 0.1. Check that resuming the components reproduces the signal.
Nonlinearly perturbed oscillator: Remember the harmonic oscillator with a nonlinear per-
turbation (9.2):
V (x) =
1
2
kx2
(
1− 2
3
αx
)
, F (x) = −kx(1− αx). (10.43)
For very small amplitudes of oscillation (x  1/α), the solution x(t) will essentially be
only the first term of a Fourier series.
1. We want the signal to contain “approximately 10% nonlinearity.” This being the
case, fix your value of α so that αxmax ' 10%, where xmax is the maximum
amplitude of oscillation. For the rest of the problem, keep the value of α fixed.
2. Decompose your numerical solution into a discrete Fourier spectrum.
3. Plot a graph of the percentage of importance of the first two, non-DC Fourier com-
ponents as a function of the initial displacement for 0 < x0 < 1/2α. You should
find that higher harmonics are more important as the amplitude increases. Because
both even and odd components are present, Yn should be complex. Because a
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FOURIER ANALYSIS; SIGNALS AND FILTERS 215
10% effect in amplitude becomes a 1% effect in power, make sure that you make a
semilog plot of the power spectrum.
4. As always, check that resumations of your transforms reproduce the signal.
(Warning: The ω you use in your series must correspond to the true frequency of the system,
not just the ω of small oscillations.)
10.4.5 Nonperiodic Function DFT (Exploration)
Consider a simple model (a wave packet) of a “localized” electron moving through space and
time. A good model for an electron initially localized around x = 5 is a Gaussian multiplying
a plane wave:
ψ(x, t = 0) = exp
[
−1
2
(
x− 5.0
σ0
)2]
eik0x. (10.44)
This wave packet is not an eigenstate of the momentum operator7 p = id/dx and in fact
contains a spread of momenta. Your problem is to evaluate the Fourier transform,
ψ(p) =
∫ +∞
−∞
dx
eipx√
2π
ψ(x), (10.45)
as a way of determining the momenta components in (10.44).
10.5 UNIT II. FILTERING NOISY SIGNALS
You measure a signal y(t) that obviously contains noise. Your problem is to determine the
frequencies that would be present in the signal if it did not contain noise. Of course, once
you have a Fourier transform from which the noise has been removed, you can transform it to
obtain a signal s(t) with no noise.
In the process of solving this problem we examine two simple approaches: the use of
autocorrelation functions and the use of filters. Both approaches find wide applications in
science, with our discussion not doing the subjects justice. However, we will see filters again
in the discussion of wavelets in Chapter 11, “Wavelet Analysis & Data Compression.”
10.6 NOISE REDUCTION VIA AUTOCORRELATION (THEORY)
We assume that the measured signal is the sum of the true signal s(t), which we wish to
determine, plus the noise n(t):
y(t) = s(t) + n(t). (10.46)
Our first approach to separating the signal from the noise relies on that fact that noise is a
random process and thus should not be correlated with the signal. Yet what do we mean when
we say that two functions are correlated? Well, if the two tend to oscillate with their nodes
and peaks in much the same places, then the two functions are clearly correlated. An analytic
measure of the correlation of two arbitrary functions y(t) and x(t) is the correlation function
c(τ) =
∫ +∞
−∞
dt y∗(t)x(t+ τ) ≡
∫ +∞
−∞
dt y∗(t− τ)x(t), (10.47)
7We use natural units in which h̄ = 1.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
216 CHAPTER 10
where τ , the lag time, is a variable. Even if the two signals have different magnitudes, if they
have similar time dependences except for one lagging or leading the other, then for certain val-
ues of τ the integrand in (10.47) will be positive for all values of t. In this case the two signals
interfere constructively and produce a large value for the correlation function. In contrast, if
both functions oscillate independently, then it is just as likely for the integrand to be positive as
to be negative, in which case the two signals interfere destructively and produce a small value
for the integral.
Before we apply the correlation function to our problem, let us study some of its proper-
ties. We use (10.17) to express c, y∗, and x in terms of their Fourier transforms:
c(τ) =
∫ +∞
−∞
dω′′ C(ω′′)
eiω
′′t
√
2π
, y∗(t) =
∫ +∞
−∞
dω Y ∗(ω)
e−iωt√
2π
,
x(t+ τ) =
∫ +∞
−∞
dω′ X(ω′)
e+iωt√
2π
. (10.48)
Because ω, ω′, and ω′′ are dummy variables, other names may be used for these variables
without changing any results. When we substitute these representations into the definition
(10.47) and assume that the resulting integrals converge well enough to be rearranged, we
obtain ∫ +∞
−∞
dω′′ C(ω′′)eiω
′′t =
1
2π
∫ +∞
−∞
dω
∫ +∞
−∞
dω′ Y ∗(ω)X(ω′)eiωτ2πδ(ω′ − ω)
=
∫ +∞
−∞
dωY ∗(ω)X(ω)eiωτ ,
⇒ C(ω) =
√
2π Y ∗(ω)X(ω), (10.49)
where the last line follows because ω′′ and ω are equivalent dummy variables. Equation (10.49)
says that the Fourier transform of the correlation function between two signals is proportional
to the product of the transform of one signal and the complex conjugate of the transform of the
other. (We shall see a related convolution theorem for filters.)
A special case of the correlation function c(τ) is the autocorrelation function A(τ). It
measures the correlation of a time signal with itself:
xmll
A(τ) def=
∫ +∞
−∞
dt y∗(t) y(t+ τ) ≡
∫ +∞
−∞
dt y(t) y∗(t− τ). (10.50)
This function is computed by taking a signal y(t) that has been measured over some time
period and then averaging it over time using y(t+ τ) as a weighting function. This process is
also called folding a function onto itself (as might be done with dough) or a convolution. To
see how this folding removes noise from a signal, we go back to the measured signal (10.46),
which was the sum of pure signal plus noise s(t) + n(t). As an example, on the upper left in
Figure 10.3 we show a signal that was constructed by adding random noise to a smooth signal.
When we compute the autocorrelation function for this signal, we obtain a function (upper
right in Figure 10.3) that looks like a broadened, smoothed version of the signal y(t). We can
understand how the noise is removed by taking the Fourier transform of s(t) +n(t) to obtain a
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FOURIER ANALYSIS; SIGNALS AND FILTERS 217
Figure 10.3 From bottom left to right: The function plus noise s(t) + n(t), the autocorrelation function versus time,
the power spectrum obtained from autocorrelation function, and the noisy signal after passage through
a lowpass filter.
0
2
4
6
8
10
0 2 4 6 8 10 12
Initial Function y(t) + Noise
t (s)
y
0.4
0.6
0.8
1.0
1.2
1.4
x10
2
0 2 4 6 8 10 12
Autocorrelation Function A(tau)
tau (s)
A
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
x10
3
0 5 10 15 20 25 30 35 40 45
Power Spectrum (with Noise)
Frequency
P
10
y
0
2
4
6
8
0 2 4 6 8 10 12
Function  y(t) + Noise After Low pass Filter
t (s)
simple sum of transforms:
Y (ω) =S(ω) +N(ω), (10.51){
S(ω)
N(ω)
}
=
∫ +∞
−∞
dt
{
s(t)
n(t)
}
e−iωt√
2π
. (10.52)
Because the autocorrelation function (10.50) for y(t) = s(t) + n(t) involves the second
power of y, is not a linear function, that is, Ay 6= As +An, but instead,
Ay(τ) =
∫ +∞
−∞
dt [s(t)s(t+ τ) + s(t)n(t+ τ) + n(t)n(t+ τ)] . (10.53)
If we assume that the noise n(t) in the measured signal is truly random, then it should average
to zero over long times and be uncorrelated at times t and t + τ . This being the case, both
integrals involving the noise vanish, and so
Ay(τ) '
∫ +∞
−∞
dt s(t) s(t+ τ) = As(τ). (10.54)
Thus, the part of the noise that is random tends to be averaged out of the autocorrelation func-
tion, and we are left with the autocorrelation function of approximately the pure signal.
This is all very interesting but is not the transform S(ω) of the pure signal that we need
to solve our problem. However, application of (10.49) with Y (ω) = X(ω) = S(ω) tells us
that the Fourier transform A(ω) of the autocorrelation function is proportional to |S(ω)|2:
A(ω) =
√
2π |S(ω)|2. (10.55)
The function |S(ω)|2 is the power spectrum we discussed in §10.4. For practical purposes,
knowing the power spectrum is often all that is needed and is easier to understand than a
complex S(ω); in any case it is all that we can calculate.
As a procedure for analyzing data, we (1) start with the noisy measured signal and (2)
compute its autocorrelation function A(t) via the integral (10.50). Because this is just folding
the signal onto itself, no additional functions or input is needed. We then (3) perform a DFT on
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
218 CHAPTER 10
Figure 10.4 Input signal f is filtered by h, and the output is g.
the autocorrelation function A(t) to obtain the power spectrum. For example, in Figure 10.3
we see a noisy signal (lower left), the autocorrelation function (lower right), which clearly is
smoother than the signal, and finally, the deduced power spectrum (upper left). Notice that the
broadband high-frequency components characteristic of noise are absent from the power spec-
trum. You can easily modify the sample program DFTcomplex.py in Listing 10.1 to compute
the autocorrelation function and then the power spectrum from A(τ). We present a program
NoiseSincFilter.py on the instructor’s CD that does this.
10.6.1 Autocorrelation Function Exercises
1. Imagine that you have sampled the pure signal
s(t) =
1
1− 0.9 sin t
. (10.56)
Although there is just a single sine function in the denominator, there is an infinite num-
ber of overtones as follows from the expansion
s(t) ' 1 + 0.9 sin t+ (0.9 sin t)2 + (0.9 sin t)3 + · · · . (10.57)
a. Compute the DFT S(ω). Make sure to sample just one period but to cover the entire
period. Make sure to sample at enough times (fine scale) to obtain good sensitivity to
the high-frequency components.
b. Make a semilog plot of the power spectrum |S(ω)|2.
c. Take your input signal s(t) and compute its autocorrelation function A(τ) for a full
range of τ values (an analytic solution is okay too).
d. Compute the power spectrum indirectly by performing a DFT on the autocorre-
lation function. Compare your results to the spectrum obtained by computing |S(ω)|2
directly.
2. Add some random noise to the signal using a random number generator:
y(ti) = s(ti) + α(2ri − 1), 0 ≤ ri ≤ 1, (10.58)
where α is an adjustable parameter. Try several values of α, from small values that just
add some fuzz to the signal to large values that nearly hide the signal.
a. Plot your noisy data, their Fourier transform, and their power spectrum obtained
directly from the transform with noise.
b. Compute the autocorrelation function A(t) and its Fourier transform.
c. Compare the DFT of A(τ) to the power spectrum and comment on the effectiveness
of reducing noise by use of the autocorrelation function.
d. For what value of α do you essentially lose all the information in the input?
10.7 FILTERING WITH TRANSFORMS (THEORY)
A filter (Figure 10.4) is a device that converts an input signal f(t) to an output signal g(t) with
some specific property for the latter. More specifically, an analog filter is defined [Hart 98] as
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FOURIER ANALYSIS; SIGNALS AND FILTERS 219
integration over the input function:
g(t) =
∫ +∞
−∞
dτ f(τ)h(t− τ) def= f(t) ∗ h(t). (10.59)
The operation indicated in (10.59) occurs often enough that it is given the name convolution
and is denoted by an asterisk ∗. The function h(t) is called the response or transfer function of
the filter because it is the response of the filter to a unit impulse:
h(t) =
∫ +∞
−∞
dτ δ(τ)h(t− τ). (10.60)
Such being the case, h(t) is also called the unit impulse response function or Green’s function.
Equation (10.59) states that the output g(t) of a filter equals the input f(t) convoluted with
the transfer function h(t − τ). Because the argument of the response function is delayed by a
time τ relative to that of the signal in the integral (10.59), τ is called the lag time. While the
integration is over all times, the response of a good detector usually peaks around zero time. In
any case, the response must equal zero for τ > t because events in the future cannot affect the
present (causality).
The convolution theorem states that the Fourier transform of the convolution g(t) is pro-
portional to the product of the transforms of f(t) and h(t):
G(ω) =
√
2π F (ω)H(ω). (10.61)
The theorem results from expressing the functions in (10.59) by their transforms and using the
resulting Dirac delta function to evaluate an integral (essentially what we did in our discussion
of correlation functions). This is an example of how some relations are simpler in transform
space than in time space.
Regardless of the domain used, filtering as we have defined it is a linear process involving
just the first powers of f . This means that the output at one frequency is proportional to the
input at that frequency. The constant of proportionality between the two may change with
frequency and thus suppress specific frequencies relative to others, but that constant remains
fixed in time. Because the law of linear superposition is valid for filters, if the input to a
filter is represented as the sum of various functions, then the transform of the output will be
the sum of the functions’ Fourier transforms. Because the transfer function may be complex,
H(ω) = |H(ω)| exp[iφ(ω)], the filter may also shift the phase of the input at frequency ω by
an amount φ.
Filters that remove or decrease high-frequency components more than they do low-
frequency components, are called lowpass filters. Those that filter out the low frequencies
are called highpass filters. A simple lowpass filter is the RC circuit on the left in Figure 10.5,
and it produces the transfer function
H(ω) =
1
1 + iωτ
=
1− iωτ
1 + ω2τ2
, (10.62)
where τ = RC is the time constant. The ω2 in the denominator leads to a decrease in the
response at high frequencies and therefore makes this a lowpass filter (the iω affects only the
phase). A simple highpass filter is the RC circuit on the right in Figure 10.5, and it produces
the transfer function
H(ω) =
iωτ
1 + iωτ
=
iωτ + ω2τ2
1 + ω2τ2
. (10.63)
H = 1 at large ω, yet H vanishes as ω → 0, which makes this a highpass filter.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
220 CHAPTER 10
Figure 10.5 Left: An RC circuit arranged as a lowpass filter. Right: An RC circuit arranged as a highpass filter.
Figure 10.6 A delay-line filter in which the signal at different time translations is scaled by different amounts ci.
Σ
  
Filters composed of resistors and capacitors are fine for analog signal processing. For
digital processing we want a digital filter that has a specific response function for each fre-
quency range. A physical model for a digital filter may be constructed from a delay line with
taps at various spacing along the line (Figure 10.6) [Hart 98]. The signal read from tap n is just
the input signal delayed by time nτ , where the delay time τ is a characteristic of the particular
filter. The output from each tap is described by the transfer function δ(t − nτ), possibly with
scaling factor cn. As represented by the triangle on the right in Figure 10.6, the signals from
all taps are ultimately summed together to form the total response function:
h(t) =
N∑
n=0
cn δ(t− nτ). (10.64)
In the frequency domain, the Fourier transform of a delta function is an exponential, and so
(10.64) results in the transfer function
H(ω) =
N∑
n=0
cn e
−i nωτ , (10.65)
where the exponential indicates the phase shift from each tap.
If a digital filter is given a continuous time signal f(t) as input, its output will be the
discrete sum
g(t) =
∫ +∞
−∞
dt′ f(t′)
N∑
n=0
cn δ(t− t′ − nτ) =
N∑
n=0
cn f(t− nτ). (10.66)
And of course, if the signal’s input is a discrete sum, its output will remain a discrete sum. (We
restrict ourselves to nonrecursive filters [Pres 94].) In either case, we see that knowledge of the
filter coefficients ci provides us with all we need to know about a digital filter. If we look back
at our work on the discrete Fourier transform in §10.4.1, we can view a digital filter (10.66)
as a Fourier transform in which we use an N -point approximation to the Fourier integral. The
cn’s then contain both the integration weights and the values of the response function at the
integration points. The transform itself can be viewed as a filter of the signal into specific
frequencies.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FOURIER ANALYSIS; SIGNALS AND FILTERS 221
Figure 10.7 Lower left: Frequency response for an ideal lowpass filter. Lower right: Truncated-sinc filter kernel
(time domain). Upper left: Windowed-sinc filter kernel. Upper right: windowed-sinc filter frequency
response.
0.0
0.2
0.4
0.6
0.8
1.0
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50
Ideal frequency response
Frequency
A
m
p
l
i
t
u
d
e
2
x10
0.0
0.5
1.0
1.5
x10
3
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
Truncated-sinc filter kernel
Sample number
A
m
p
l
i
t
u
d
e
2
x10
0.0
0.5
1.0
1.5
x10
3
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
Windowed-sinc filter kernel
Sample number
A
m
p
l
i
t
u
d
e
0.0
0.2
0.4
0.6
0.8
1.0
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45
Windowed-sinc frequency response
Frequency
A
m
p
l
i
t
u
d
e
10.7.1 Digital Filters: Windowed Sinc Filters (Exploration) 
Problem: Construct digital versions of highpass and lowpass filters and determine which filter
works better at removing noise from a signal.
A popular way to separate the bands of frequencies in a signal is with a windowed sinc
filter [Smi 99]. This filter is based on the observation that an ideal lowpass filter passes all
frequencies below a cutoff frequency ωc and blocks all frequencies above this frequency. And
because there tends to be more noise at high frequencies than at low frequencies, removing the
high frequencies tends to remove more noise than signal, although some signal is inevitably
lost. One use for windowed sinc filters is in reducing aliasing by removing the high-frequency
component of a signal before determining its Fourier components. The graph on the lower
right in Figure 10.7 was obtained by passing our noisy signal through a sinc filter (using the
program NoiseSincFilter.py).
If both positive and negative frequencies are included, an ideal low-frequency filter will
look like the rectangular pulse in frequency space:
H(ω, ωc) = rect
(
ω
2ωc
)
rect(ω) =
1, if |ω| ≤
1
2 ,
0, otherwise.
(10.67)
Here rect(ω) is the rectangular function (Figure 10.8). Although maybe not obvious, a rect-
angular pulse in the frequency domain has a Fourier transform that is proportional to the sinc
function in the time domain [Smi 91, Wiki]∫ +∞
−∞
dω e−iωtrect(ω) = sinc
(
t
2
)
def=
sin(πt/2)
πt/2
, (10.68)
where the π’s are sometimes omitted. Consequently, we can filter out the high-frequency
components of a signal by convoluting it with sin(ωct)/(ωct), a technique also known as the
Nyquist–Shannon interpolation formula. In terms of discrete transforms, the time-domain rep-
resentation of the sinc filter is
h[i] =
sin(ωci)
iπ
. (10.69)
All frequencies below the cutoff frequency ωc are passed with unit amplitude, while all higher
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
222 CHAPTER 10
Figure 10.8 The rectangle function rect(ω) whose Fourier transform is sinc(t).
frequencies are blocked.
In practice, there are a number of problems in using this function as the filter. First,
as formulated, the filter is noncausal; that is, there are coefficients at negative times, which
is nonphysical because we do not start measuring the signal until t = 0. Second, in order
to produce a perfect rectangular response, we would have to sample the signal at an infinite
number of times. In practice, we sample at (M + 1) points (M even) placed symmetrically
around the main lobe of sin(πt)/πt and then shift times to purely positive values,
h[i] =
sin[2πωc(i−M/2)]
i−M/2
, 0 ≤ t ≤M. (10.70)
As might be expected, a penalty is incurred for making the filter discrete; instead of the ideal
rectangular response, we obtain a Gibbs overshoot, with rounded corners and oscillations be-
yond the corner.
There are two ways to reduce the departures from the ideal filter. The first is to increase
the length of times for which the filter is sampled, which inevitably leads to longer compute
times. The other way is to smooth out the truncation of the sinc function by multiplying it with
a smoothly tapered curve like the Hamming window function:
w[i] = 0.54− 0.46 cos(2πi/M). (10.71)
In this way the filter’s kernel becomes
h[i] =
sin[2πωc(i−M/2)]
i−M/2
[
0.54− 0.46 cos(2πi
M
)
]
. (10.72)
The cutoff frequency ωc should be a fraction of the sampling rate. The time length M deter-
mines the bandwidth over which the filter changes from 1 to 0.
Exercise: Repeat the exercise that added random noise to a known signal, this time using
the sinc filter to reduce the noise. See how small you can make the signal and still be able to
separate it from the noise.
10.8 UNIT II. FAST FOURIER TRANSFORM ALGORITHM 
We have seen in (10.32) that a discrete Fourier transform can be written in the compact form
Yn =
1√
2π
N∑
k=1
Znk yk, Z = e−2πi/N , n = 0, 1, . . . , N − 1. (10.73)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FOURIER ANALYSIS; SIGNALS AND FILTERS 223
Even if the signal elements yk to be transformed are real, Z is always complex, and
therefore we must process both real and imaginary parts when computing transforms. Because
both n and k range over N integer values, the (Zn)k yk multiplications in (10.73) require some
N2 multiplications and additions of complex numbers. As N gets large, as happens in realistic
applications, this geometric increase in the number of steps slows down the algorithm.
In 1965, Cooley and Tukey discovered an algorithm8 that reduces the number of oper-
ations necessary to perform a DFT from N2 to roughly N log2N [Co,65, Donn 05]. Even
though this may not seem like such a big difference, it represents a 100-fold speedup for
1000 data points, which changes a full day of processing into 15 min of work. Because of
its widespread use (including cell phones), the fast Fourier transform algorithm is considered
one of the 10 most important algorithms of all time.
The idea behind the FFT is to utilize the periodicity inherent in the definition of the
DFT (10.73) to reduce the total number of computational steps. Essentially, the algorithm
divides the input data into two equal groups and transforms only one group, which requires
∼ (N/2)2 multiplications. It then divides the remaining (nontransformed) group of data in
half and transforms them, continuing the process until all the data have been transformed. The
total number of multiplications required with this approach is approximately N log2N .
Specifically, the FFT’s time economy arises from the computationally expensive com-
plex factor Znk[= ((Z)n)k] being equal to the same cyclically repeated value as the integers n
and k vary sequentially. For instance, for N = 8,
Y0 =Z0y0 + Z0y1 + Z0 y2 + Z0 y3 + Z0 y4 + Z0 y5 + Z0 y6 + Z0 y7,
Y1 =Z0y0 + Z1y1 + Z2y2 + Z3 y3 + Z4 y4 + Z5 y5 + Z6 y6 + Z7 y7,
Y2 =Z0y0 + Z2y1 + Z4 y2 + Z6 y3 + Z8 y4 + Z10y5 + Z12y6 + Z14y7,
Y3 =Z0y0 + Z3y1 + Z6 y2 + Z9 y3 + Z12y4 + Z15y5 + Z18y6 + Z21y7,
Y4 =Z0y0 + Z4y1 + Z8 y2 + Z12y3 + Z16y4 + Z20y5 + Z24y6 + Z28y7,
Y5 =Z0y0 + Z5y1 + Z10y2 + Z15y3 + Z20y4 + Z25y5 + Z30y6 + Z35y7,
Y6 =Z0y0 + Z6y1 + Z12y2 + Z18y3 + Z24y4 + Z30y5 + Z36y6 + Z42y7,
Y7 =Z0y0 + Z7y1 + Z14y2 + Z21y3 + Z28y4 + Z35y5 + Z42y6 + Z49y7,
where we include Z0(≡1) for clarity. When we actually evaluate these powers of Z, we find
8Actually, this algorithm has been discovered a number of times, for instance, in 1942 by Danielson and Lanczos [Da,42], as
well as much earlier by Gauss.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
224 CHAPTER 10
only four independent values:
Z0 = exp (0) = +1, Z1 = exp(−2π8 i) = +
√
2
2 − i
√
2
2 ,
Z2 = exp(−2π8 2i) = −i, Z
3 = exp(−2π8 3i) = −
√
2
2 − i
√
2
2 ,
Z4 = exp(−2π8 4i) = −Z
0, Z5 = exp(−2π8 5i) = −Z
1,
Z6 = exp(−2π8 6i) = −Z
2, Z7 = exp(−2π8 7i) = −Z
3,
Z8 = exp(−2π8 8i) = +Z
0, Z9 = exp(−2π8 9i) = +Z
1,
Z10 = exp(−2π8 10i) = +Z
2, Z11 = exp(−2π8 11i) = +Z
3,
Z12 = exp(−2π8 11i) = −Z
0, · · · .
(10.74)
When substituted into the definitions of the transforms, we obtain
xmll Y0 =Z
0y0 + Z0y1 + Z0y2 + Z0y3 + Z0y4 + Z0y5 + Z0y6 + Z0y7,
Y1 =Z0y0 + Z1y1 + Z2y2 + Z3y3 − Z0y4 − Z1y5 − Z2y6 − Z3y7,
Y2 =Z0y0 + Z2y1 − Z0y2 − Z2y3 + Z0y4 + Z2y5 − Z0y6 − Z2y7,
Y3 =Z0y0 + Z3y1 − Z2y2 + Z1y3 − Z0y4 − Z3y5 + Z2y6 − Z1y7,
Y4 =Z0y0 − Z0y1 + Z0y2 − Z0y3 + Z0y4 − Z0y5 + Z0y6 − Z0y7,
Y5 =Z0y0 − Z1y1 + Z2y2 − Z3y3 − Z0y4 + Z1y5 − Z2y6 + Z3y7,
Y6 =Z0y0 − Z2y1 − Z0y2 + Z2y3 + Z0y4 − Z2y5 − Z0y6 + Z2y7,
Y7 =Z0y0 − Z3y1 − Z2y2 − Z1y3 − Z0y4 + Z3y5 + Z2y6 + Z1y7,
Y8 =Y0.
We see that these transforms now require 8× 8 = 64 multiplications of complex numbers, in
addition to some less time-consuming additions. We place these equations in an appropriate
form for computing by regrouping the terms into sums and differences of the y’s:
Y0 =Z0(y0 + y4) + Z0(y1 + y5) + Z0(y2 + y6) + Z0(y3 + y7),
Y1 =Z0(y0 − y4) + Z1(y1 − y5) + Z2(y2 − y6) + Z3(y3 − y7),
Y2 =Z0(y0 + y4) + Z2(y1 + y5)− Z0(y2 + y6)− Z2(y3 + y7),
Y3 =Z0(y0 − y4) + Z3(y1 − y5)− Z2(y2 − y6) + Z1(y3 − y7),
Y4 =Z0(y0 + y4)− Z0(y1 + y5) + Z0(y2 + y6)− Z0(y3 + y7),
Y5 =Z0(y0 − y4)− Z1(y1 − y5) + Z2(y2 − y6)− Z3(y3 − y7),
Y6 =Z0(y0 + y4)− Z2(y1 + y5)− Z0(y2 + y6) + Z2(y3 + y7),
Y7 =Z0(y0 − y4)− Z3(y1 − y5)− Z2(y2 − y6)− Z1(y3 − y7),
Y8 =Y0.
Note the repeating factors inside the parentheses, with combinations of the form yp±yq. These
symmetries are systematized by introducing the butterfly operation (Figure 10.9). This oper-
ation takes the yp and yq data elements from the left wing and converts them to the yp + Zyq
elements in the upper- and lower-right wings. In Figure 10.10 we show what happens when
we apply the butterfly operations to an entire FFT process, specifically to the pairs (y0, y4),
(y1, y5), (y2, y6), and (y3, y7). Notice how the number of multiplications of complex numbers
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FOURIER ANALYSIS; SIGNALS AND FILTERS 225
Figure 10.9 The basic butterfly operation in which elements yp and yq are transformed into yp + Z yq and yp − Z yq.
p
q
Z
p + Zq
p - Zq
Figure 10.10 The butterfly operations performing a FFT on four pairs of data.
has been reduced: For the first butterfly operation there are 8 multiplications by Z0; for the
second butterfly operation there are 8 multiplications, and so forth, until a total of 24 multipli-
cations is made in four butterflies. In contrast, 64 multiplications are required in the original
DFT (10.8).
10.8.1 Bit Reversal
The reader may have observed that in Figure 10.10 we started with 8 data elements in the order
0–7 and that after three butterfly operators we obtained transforms in the order 0, 4, 2, 6, 1,
5, 3, 7. The astute reader may may also have observed that these numbers correspond to the
bit-reversed order of 0–7. Let us look into this further. We need 3 bits to give the order of each
of the 8 input data elements (the numbers 0–7). Explicitly, on the left in Table 10.1 we give
the binary representation for decimal numbers 0–7, their bit reversals, and the corresponding
decimal numbers. On the right we give the ordering for 16 input data elements, where we need
4 bits to enumerate their order. Notice that the order of the first 8 elements differs in the two
cases because the number of bits being reversed differs. Notice too that after the reordering,
the first half of the numbers are all even and the second half are all odd.
The fact that the Fourier transforms are produced in an order corresponding to the bit-
reversed order of the numbers 0–7 suggests that if we process the data in the bit-reversed order
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
226 CHAPTER 10
Figure 10.11 Modified FFT.
0, 4, 6, 2, 1, 5, 3, 7, then the output Fourier transforms will be ordered. We demonstrate this
conjecture in Figure 10.11, where we see that to obtain the Fourier transform for the 8 input
data, the butterfly operation had to be applied 3 times. The number 3 occurs here because it
is the power of 2 that gives the number of data; that is, 23 = 8. In general, in order for a
FFT algorithm to produce transforms in the proper order, it must reshuffle the input data into
bit-reversed order. As a case in point, our sample program starts by reordering the 16 (24)
data elements given in Table 10.2. Now the 4 butterfly operations produce sequentially ordered
output.
10.9 FFT IMPLEMENTATION
The first FFT program we are aware of was written in 1967 in Fortran IV by Norman Brenner
at MIT’s Lincoln Laboratory [Hi,76] and was hard for us to follow. Our (easier-to-follow) Java
version of it is in Listing 10.2. Its input isN = 2n data to be transformed (FFTs always require
2N input data). If the number of your input data is not a power of 2, then you can make it so by
concatenating some of the initial data to the end of your input until a power of 2 is obtained;
since a DFT is always periodic, this just starts the period a little earlier. This program assigns
complex numbers at the 16 data points
ym = m+mi, m = 0, . . . , 15, (10.75)
reorders the data via bit reversal, and then makes four butterfly operations. The data are stored
in the array dtr[max][2], with the second subscript denoting real and imaginary parts. We
increase speed further by using the 1-D array data to make memory access more direct:
data[1] = dtr[0][1], data[2] = dtr[1][1], data[3] = dtr[1][0], . . . ,
which also provides storage for the output. The FFT transforms data using the butterfly oper-
ation and stores the results back in dtr[][], where the input data were originally.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FOURIER ANALYSIS; SIGNALS AND FILTERS 227
Table 10.1 Binary-Reversed 0–7.
Decimal
Decimal Binary Reversal Reversal
0 000 000 0
1 001 100 4
2 010 010 2
3 011 110 6
4 100 001 1
5 101 101 5
6 110 011 3
7 111 111 7
Binary-Reversed 0–16
Decimal
Decimal Binary Reversal Reversal
0 0000 0000 0
1 0001 1000 8
2 0010 0100 4
3 0011 1100 12
4 0100 0010 2
5 0101 1010 10
6 0110 0110 6
7 0111 1110 14
8 1000 0001 1
9 1001 1001 9
10 1010 0101 5
11 1011 1101 13
12 1100 0011 3
13 1101 1011 11
14 1110 0111 7
15 1111 1111 15
Table 10.2 Reordering for 16 Data Complex Points
Order Input Data New Order Order Input Data New Order
0 0.0 + 0.0i 0.0 + 0.0i 8 8.0 + 8.0i 1.0 + 1.0i
1 1.0 + 1.0i 8.0 + 8.0i 9 9.0 + 9.0i 9.0 + 9.0i
2 2.0 + 2.0i 4.0 + 4.0i 10 10.0 + 10.i 5.0 + 5.0i
3 3.0 + 3.0i 12.0 + 12.0i 11 11.0 + 11.0i 13.0 + 13.0i
4 4.0 + 4.0i 2.0 + 2.0i 12 12.0 + 12.0i 3.0 + 3.0i
5 5.0 + 5.0i 10.0 + 10.i 13 13.0 + 13.0i 11.0 + 11.0i
6 6.0 + 6.0i 6.0 + 6.0i 14 14.0 + 14.i 7.0 + 7.0i
7 7.0 + 7.0i 14.0 + 14.0i 15 15.0 + 15.0i 15.0 + 15.0i
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
228 CHAPTER 10
10.10 FFT ASSESSMENT
1. Compile and execute FFT.py. Make sure you understand the output.
2. Take the output from FFT.py, inverse-transform it back to signal space, and compare it
to your input. [Checking that the double transform is proportional to itself is adequate,
although the normalization factors in (10.32) should make the two equal.]
3. Compare the transforms obtained with a FFT to those obtained with a DFT (you may
choose any of the functions studied before). Make sure to compare both precision and
execution times.
Listing 10.2 FFT.py computes the FFT or inverse transform depending upon the sign of isign. 
# FFT . py F a s t F o u r i e r Trans fo rm ; d t r [ p o i n t s ] [ 2 ] = Re , Im s i g n a l , max = 2ˆm < 1024
from v i s u a l i m p o r t ∗ ; from v i s u a l . g raph i m p o r t ∗
max = 2100 ; p o i n t s = 1026
d a t a = z e r o s ( ( max ) , F l o a t ) ; d t r = z e r o s ( ( p o i n t s , 2 ) , F l o a t )
d e f f f t ( nn , i s i g n ) : # FFT of d t r [ n , 2 ]
n = 2∗nn
f o r i i n r a n g e ( 0 , nn +1) : # O r i g i n a l d a t a i n d t r t o d a t a
j = 2∗ i + 1
d a t a [ j ] = d t r [ i , 0 ] # Rea l d t r , odd d a t a [ j ]
d a t a [ j +1] = d t r [ i , 1 ] # Imag d t r , even d a t a [ j +1]
j = 1 # P l a c e d a t a i n b i t r e v e r s e o r d e r
f o r i i n r a n g e ( 1 , n +2 , 2 ) :
i f ( i−j ) < 0 : # Reorde r e q u i v a l e n t t o b i t r e v e r s e
tempr = d a t a [ j ]
t empi = d a t a [ j +1]
d a t a [ j ] = d a t a [ i ]
d a t a [ j +1] = d a t a [ i +1]
d a t a [ i ] = tempr
d a t a [ i +1] = tempi
m = n / 2 ;
w h i l e (m − 2 > 0) :
i f ( j − m) <= 0 :
break
j = j−m
m = m/ 2
j = j +m;
p r i n t " Bit - reversed data "
f o r i i n r a n g e ( 1 , n + 1 , 2 ) : p r i n t "%2d data[%2d] %9.5f " %(i , i , d a t a [ i ] ) # P r i n t
r e o r d e r
mmax = 2
w h i l e (mmax − n ) < 0 : # Begin t r a n s f o r m
i s t e p = 2 ∗ mmax
t h e t a = 6 . 2 8 3 1 8 5 3 / ( 1 . 0∗ i s i g n ∗mmax)
s i n t h = math . s i n ( t h e t a / 2 . 0 )
w s t p r = −2.0 ∗ s i n t h ∗∗2
w s t p i = math . s i n ( t h e t a )
wr = 1 . 0 ; wi = 0 . 0
f o r m i n r a n g e ( 1 , mmax+1 , 2 ) :
f o r i i n r a n g e (m, n +1 , i s t e p ) :
j = i + mmax
tempr = wr∗ d a t a [ j ] − wi∗ d a t a [ j +1]
t empi = wr∗ d a t a [ j +1] + wi∗ d a t a [ j ]
d a t a [ j ] = d a t a [ i ] − t empr
d a t a [ j +1] = d a t a [ i +1] − t empi
d a t a [ i ] = d a t a [ i ] + tempr
d a t a [ i +1] = d a t a [ i +1] + tempi
tempr = wr
wr = wr∗w s t p r − wi∗w s t p i + wr
wi = wi∗w s t p r + tempr∗w s t p i + wi ;
mmax = i s t e p
f o r i i n r a n g e ( 0 , nn ) :
j = 2∗ i + 1
d t r [ i , 0 ] = d a t a [ j ]
d t r [ i , 1 ] = d a t a [ j +1]
nn = 16 # Power o f 2
i s i g n = −1 # − 1 t r a n s f o r m , + 1 i n v e r s e t r a n s f o r m
p r i n t ’ INPUT’
p r i n t " i Re part Im part"
f o r i i n r a n g e ( 0 , nn ) : # Form a r r a y
d t r [ i , 0 ] = 1 .0∗ i # Rea l p a r t
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FOURIER ANALYSIS; SIGNALS AND FILTERS 229
d t r [ i , 1 ] = 1 .0∗ i # Im p a r t
p r i n t " %2d %9.5f %9.5f" %(i , d t r [ i , 0 ] , d t r [ i , 1 ] )
f f t ( nn , i s i g n ) # C a l l FFT , use g l o b a l d t r [ ] [ ]
p r i n t ’ Fourier transform’
p r i n t " i Re Im "
f o r i i n r a n g e ( 0 , nn ) : p r i n t " %2d %9.5f %9.5f "%(i , d t r [ i , 0 ] , d t r [ i , 1 ] )
p r i n t "Press a character to finish"
s= r a w i n p u t ( )
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Eleven
Wavelet Analysis & Data Compression
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
Wavelet Basics 11.1–11.3 Continuous Wavelet Transform 11.4
Discrete Wavelet Transform 11.5 -
PROBLEM
Problem: You have sampled the signal in Figure 11.1 that seems to contain an increasing
number of frequencies as time increases. Your problem is to undertake a spectral analysis of
this signal that tells you, in the most compact way possible, how much of each frequency is
present at each instant in time. Hint: Although we want the method to be general enough to
work with numerical data, for pedagogical purposes it is useful to know that the signal is
y(t) =

sin 2πt, for 0 ≤ t ≤ 2,
5 sin 2πt+ 10 sin 4πt, for 2 ≤ t ≤ 8,
2.5 sin 2πt+ 6 sin 4πt+ 10 sin 6πt, for 8 ≤ t ≤ 12.
(11.1)
11.1 UNIT I. WAVELET BASICS
The Fourier analysis we used in §10.4.1 reveals the amount of the harmonic functions sin(ωt)
and cos(ωt) and their overtones that are present in a signal. An expansion in periodic functions
is fine for stationary signals (those whose forms do not change in time) but has shortcomings
for the variable form of our problem signal (11.1). One such problem is that the Fourier
Figure 11.1 The time signal (11.1) containing an increasing number of frequencies as time increases. The boxes
are possible placements of windows for short-time Fourier transforms.
0 1 2 3 4 5 6 7 8 9
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
WAVELET ANALYSIS & DATA COMPRESSION 231
Figure 11.2 Four sample mother wavelets. Clockwise from top: Morlet (real part), Mexican hat, Daub4 e6 (ex-
plained later), and Haar. Wavelet basis functions are generated by scaling and translating these mother
wavelets.
–1.0
–0.5
0.0
0.5
–6 –4 –2 0 2 4 6
t

0.0
–4 0 4
t

–1.0
0.0
–4 0
t
4
ψ
0 1000
0.1
0
–0.1
Daub4 e6
200 400 600 800
reconstruction has all constituent frequencies occurring simultaneously and so does not contain
time resolution information indicating when each frequency occurs. Another shortcoming is
that all the Fourier components are correlated, which results in more information being stored
than may be needed and no convenient way to remove the excess storage.
There are a number of techniques that extend simple Fourier analysis to nonstationary
signals. In this chapter we include an introduction to wavelet analysis, a field that has seen
extensive development and application in the last decade in areas as diverse as brain waves and
gravitational waves. The idea behind wavelet analysis is to expand a signal in a complete set
of functions (wavelets), each of which oscillates for a finite period of time and each of which
is centered at a different time. To give you a preview before we get into the details, we show
four sample wavelets in Figure 11.2. Because each wavelet is local in time, it is a wave packet1
containing a range of frequencies. These wave packets are called wavelets because they are
small and do not extend for long times.
Although wavelets are required to oscillate in time, they are not restricted to a particular
functional form [Add 02]. As a case in point, they may be oscillating Gaussians (Morlet: top
left in Figure 11.2),
Ψ(t) = e2πite−t
2/2σ2 = (cos 2πt+ i sin 2πt)e−t
2/2σ2 (Morlet), (11.2)
the second derivative of a Gaussian (Mexican hat, top right),
Ψ(t) = −σ2 d
2
dt2
e−t
2/2σ2 =
(
1− t
2
σ2
)
e−t
2/2σ2 , (11.3)
an up-and-down step function (lower left), or a fractal shape (bottom right). Such wavelets are
localized in both time and frequency; that is, they are large for just a finite time and contain
a finite range of frequencies. As we shall see, translating and scaling one of these mother
wavelets generates an entire set of child wavelet basis functions, with each individual function
covering a different frequency range at a different time.
1We discuss wave packets further in §11.2.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
232 CHAPTER 11
Figure 11.3 Left: A wave packet in time corresponding to the functional form (11.4) with ω0 = 5 and N = 6.
Right: The Fourier transform in frequency of this same wave packet.
–1.0
0.0
–4 0 4
t
y
0.0
0 10
Y

11.2 WAVE PACKETS AND UNCERTAINTY PRINCIPLE (THEORY)
A wave packet or wave train is a collection of waves added together in such a way as to produce
a pulse of width ∆t. As we shall see, the Fourier transform of a wave packet is a pulse in the
frequency domain of width ∆ω. We will first study such wave packets analytically and then
use others numerically. An example of a simple wave packet is just a sine wave that oscillates
at frequency ω0 for N periods (Figure 11.3 left) [A&W 01]:
xmll
y(t) =
sinω0t, for |t| < N
π
ω0
≡ N T2 ,
0, for |t| > N πω0 ≡ N
T
2 ,
(11.4)
where we relate the frequency to the period via the usual ω0 = 2π/T . In terms of these
parameters, the width of the wave packet is
∆t = NT = N
2π
ω0
. (11.5)
The Fourier transform of the wave packet (11.4) is a straight-forward application of the trans-
form formula (10.18):
Y (ω) =
∫ +∞
−∞
dt
e−iωt√
2π
y(t) =
−i√
2π
∫ Nπ/ω0
0
dt sinω0t sinωt
=
(ω0 + ω) sin
[
(ω0 − ω)Nπω0
]
− (ω0 − ω) sin
[
(ω0 + ω)Nπω0
]
√
2π(ω20 − ω2)
, (11.6)
where we have dropped a factor of −i that affects only the phase. While at first glance (11.6)
appears to be singular at ω = ω0, it just peaks there (Figure 11.3 right), reflecting the predom-
inance of frequency ω0. However, there are sharp corners in the signal y(t) (Figure 11.3 left),
and these give Y (ω) a width ∆ω.
There is a fundamental relation between the widths ∆t and ∆ω of a wave packet. Al-
though we use a specific example to determine that relation, it is true in general. While there
may not be a precise definition of “width” for all functions, one can usually deduce a good
measure of the width (say, within 25%). To illustrate, if we look at the right of Figure 11.3,
it makes sense to use the distance between the first zeros of the transform Y (ω) (11.6) as the
width ∆ω. The zeros occur at
ω − ω0
ω0
= ± 1
N
⇒ ∆ω ' ω − ω0 =
ω0
N
, (11.7)
where N is the number of cycles in our original wave packet. Because the wave packet in time
makesN oscillations each of period T , a reasonable measure of the width ∆t of the signal y(t)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
WAVELET ANALYSIS & DATA COMPRESSION 233
is
∆t = NT = N
2π
ω0
. (11.8)
When the products of the frequency width (11.7) and the time width (11.8) are combined, we
obtain
∆t∆ω ≥ 2π. (11.9)
The greater-than sign is used to indicate that this is a minimum, that is, that y(t) and Y (ω)
extend beyond ∆t and ∆ω, respectively. Nonetheless, most of the signal and transform should
lie within the bound (11.9).
A relation of the form (11.9) also occurs in quantum mechanics, where it is known as
the Heisenberg uncertainty principle, with ∆t and ∆ω being called the uncertainties in t and
ω. It is true for transforms in general and states that as a signal is made more localized in
time (smaller ∆t) the transform becomes less localized (larger ∆ω). Conversely, the signal
y(t) = sinω0t is completely localized in frequency and so has an infinite extent in time,
∆t ' ∞.
11.2.1 Wave Packet Assessment
Consider the following wave packets:
y1(t) = e−t
2/2, y2(t) = sin(8t)e−t
2/2, y3(t) = (1− t2) e−t
2/2.
For each wave packet:
1. Estimate the width ∆t. A good measure might be the full width at half-maxima (FWHM)
of |y(t)|.
2. Evaluate and plot the Fourier transform Y (ω).
3. Estimate the width ∆ω of the transform. A good measure might be the full width at
half-maxima of |Y (ω)|.
4. Determine the constant C for the uncertainty principle
∆t∆ω ≥ 2πC.
11.3 SHORT-TIME FOURIER TRANSFORMS (MATH)
The constant amplitude of the functions sinωt and cosωt for all times can limit the usefulness
of Fourier transforms. Because these functions and their overtones extend over all times with
that constant amplitude, there is considerable overlap among them, and thus the information
present in various Fourier components is correlated. This is undesirable for compressed data
storage, where you want to store a minimum number of data and want to be able to cut out some
of these data with just a minimal effect on the signal reconstruction.2 In lossless compression,
which reproduces the original signal exactly, you save space by storing how many times each
data element is repeated and where each element is located. In lossy compression, in addition to
removing repeated elements, you also eliminate some transform components, consistent with
2Wavelets have also proven to be a highly effective approach to data compression, with the Joint Photographic Experts Group
(JPEG) 2000 standard being based on wavelets. In F we give a full example of image compression with wavelets.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
234 CHAPTER 11
the uncertainty relation (11.9) and with the level of resolution required in the reproduction.
This leads to a greater compression.
In §10.4.1 we defined the Fourier transform Y (ω) of a signal y(t) as
Y (ω) =
∫ +∞
−∞
dt
e−iωt√
2π
y(t) = 〈ω |y 〉 . (11.10)
As is true for simple vectors, you can think of (11.10) as giving the overlap or scalar product of
the basis function exp(iωt)/
√
2π and the signal y(t) [notice that the complex conjugate of the
exponential basis function appears in (11.10)]. Another view of (11.10) is as the mapping or
projection of the signal into ω space. In this case the overlap projects the amount of the periodic
function exp(iωt)/
√
2π in the signal y(t). In other words, the Fourier component Y (ω) is also
the correlation between the signal y(t) and the basis function exp(iωt)/
√
2π, which is what
results from filtering the signal y(t) through a frequency-ω filter. If there is no exp(iωt) in the
signal, then the integral vanishes and there is no output. If y(t) = exp(iωt), the signal is at
only one frequency, and the integral is accordingly singular.
The problem signal in Figure 11.1 clearly has different frequencies present at different
times and for different lengths of time. In the past this signal might have been analyzed with a
precursor of wavelet analysis known as the short-time Fourier transform. With that technique,
the signal y(t) is “chopped up” into different segments along the time axis, with successive
segments centered about successive times τ1, τ2, . . . , τN . For instance, we show three such
segments in Figure 11.1. Once we have the dissected signal, a Fourier analysis is made of each
segment. We are then left with a sequence of transforms
(
Y
(ST)
τ1 , Y
(ST)
τ2 , . . . , Y
(ST)
τN
)
, one for
each short-time interval, where the superscript (ST) indicates short time.
Rather than chopping up a signal, we express short-time Fourier transforming mathe-
matically by imagining translating a window function w(t − τ) by a time τ over the signal in
Figure 11.1:
Y (ST)(ω, τ) =
∫ +∞
−∞
dt
eiωt√
2π
w(t− τ) y(t). (11.11)
Here the values of the translation time τ correspond to different locations of window w over
the signal, and the window function is essentially a transparent box of small size on an opaque
background. Any signal within the width of the window is transformed, while the signal lying
outside the window is not seen. Note that in (11.11) the extra variable τ in the Fourier transform
indicating the location of the time around which the window was placed. Clearly, since the
short-time transform is a function of two variables, a surface or 3-D plot is needed to view the
amplitude as a function of both ω and τ .
11.4 THE WAVELET TRANSFORM
The wavelet transform of a time signal y(t),
xmll
Y (s, τ) =
∫ +∞
−∞
dt ψ∗s,τ (t)y(t) (wavelet transform), (11.12)
is similar in concept and notation to a short-time Fourier transform. Rather than using exp(iωt)
as the basis functions, we use wave packets or wavelets ψs,τ (t) localized in time, such as the
those shown in Figure 11.2. Because each wavelet is localized in time, each acts as its own
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
WAVELET ANALYSIS & DATA COMPRESSION 235
Figure 11.4 Four wavelet basis functions generated by scaling (s) and translating (τ ) the oscillating Gaussian
mother wavelet. Clockwise from top: (s = 1, τ = 0), (s = 1/2, τ = 0), (s = 1, τ = 6), and
(s = 2, τ = 60). Note how s < 1 is a wavelet with higher frequency, while s > 1 has a lower
frequency than the s = 1 mother. Likewise, the τ = 6 wavelet is just a translated version of the τ = 0
one directly above it.
–0.6
0.0
0.6
–6 –4 –2 0 2 4 6
t
s = 2, = 0


–1.0
0.0
1.0
–6 –4 –2 0 2 4 6
t

–1.0
1.0
0.0
–4 –2 0 2 4 6 8 10
t
s = 1, τ = 6

–0.6
0.0
0.6
–6 –4 –2 0 2 4 6
t
s = 2,  = 0

window function. Because each wavelet is oscillatory, each contains its own small range of
frequencies.
Equation (11.12) says that the wavelet transform Y (s, τ) is a measure of the amount of
basis function ψs,τ (t) present in the signal y(t). The τ variable indicates the time portion of
the signal being decomposed, while the s variable is equivalent to the frequency present during
that time:
ω =
2π
s
, s =
2π
ω
(scale–frequency relation). (11.13)
Because it is key to much that follows, it is a good idea to think about (11.13) for a while. If
we are interested in the time details of a signal, then this is another way of saying that we are
interested in what is happening at small values of the scale s. Equation (11.13) indicates that
small values of s correspond to high-frequency components of the signal. That being the case,
the time details of the signal are in the high-frequency, or low-scale, components.
11.4.1 Generating Wavelet Basis Functions
The conceptual discussion of wavelets is over, and it is time to get to work. We first need
a technique for generating wavelet bases, and then we need to discretize this technique. As
is often the case, the final formulation will turn out to be simple and short, but it will be a
while before we get there. Just as the expansion of a function in a complete orthogonal set is
not restricted to any particular set, so the theory of wavelets is not restricted to any particular
wavelet basis, although there is some art involved in choosing the most appropriate wavelets
for a given signal. The standard way to generate a family of wavelet basis functions starts
with Ψ(t), a mother or analyzing function of the real variable t, and then use this to generate
daughter wavelets. As a case in point, let us start with the mother wavelet
Ψ(t) = sin(8t)e−t
2/2. (11.14)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
236 CHAPTER 11
Figure 11.5 Left: Comparison of the input and reconstituted signal using Morlet wavelets. As expected for Fourier
transforms, the reconstruction is least accurate near the endpoints. Right: The continuous wavelet spec-
trum obtained by analyzing the input signal (11.18) with Morelet wavelets. Observe how at small values
of time τ there is predominantly one frequency present, how a second, higher-frequency (smaller-scale)
component enters at intermediate times, and how at larger times a still higher-frequency component
enters. Further observation indicates that the large-s component has an amplitude consistent with the
input. (Figure courtesy of Z. Dimcovic.)
0 2 4 6 8 10 12
–20
–10
0
10
Input Signal
Inverted Transform
S
i
g
n
a
l
Time  t
0
1
2
s
0
4
8
12
–1
0
1

We then generate the four wavelet basis functions displayed in Figure 11.4 by scaling, translat-
ing, and normalizing this mother wavelet:
ψs,τ (t)
def=
1√
s
Ψ
(
t− τ
s
)
=
1√
s
sin
[
8(t− τ)
s
]
e−(t−τ)
2/2s2 . (11.15)
We see that larger or smaller values of s, respectively, expand or contract the mother wavelet,
while different values of τ shift the center of the wavelet. Because the wavelets are inherently
oscillatory, the scaling leads to the same number of oscillations occurring in different time
spans, which is equivalent to having basis states with differing frequencies. We see that s < 1
produces a higher-frequency wavelet, while s > 1 produces a lower-frequency one, both of
the same shape. As we shall see, we do not need to store much information to outline the
large-time-scale s behavior of a signal (its smooth envelope), but we do need more information
to specify its short-time-scale s behavior (details). And if we want to resolve finer features in
the signal, then we will need to have more information on yet finer details. Here the division
by
√
s is made to ensure that there is equal “power” (or energy or intensity) in each region of
s, although other normalizations can also be found in the literature. After substituting in the
daughters, the wavelet transform (11.12) and its inverse [VdB 99] are
xmll Y (s, τ) = 1√s
∫ +∞
−∞
dtΨ∗
(
t− τ
s
)
y(t) (wavelet transform), (11.16)
y(t) =
1
C
∫ +∞
−∞
dτ
∫ +∞
0
ds
ψ∗s,τ (t)
s3/2
Y (s, τ) (inverse transform), (11.17)
where the normalization constant C depends on the wavelet used.
The general requirements for a mother wavelet Ψ are [Add 02, VdB 99]
1. Ψ(t) is real.
2. Ψ(t) oscillates around zero such that its average is zero:∫ +∞
−∞
Ψ(t) dt = 0.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
WAVELET ANALYSIS & DATA COMPRESSION 237
Figure 11.6 A signal is analyzed by starting with a narrow wavelet at the signal’s beginning and producing a coeffi-
cient that measures the similarity of the signal to the wavelet. The wavelet is successively shifted over
the length of the signal. Then the wavelet is expanded and the analysis repeated.
3. Ψ(t) is local, that is, a wave packet, and is square-integrable:
Ψ(|t| → ∞)→ 0 (rapidly),
∫ +∞
−∞
|Ψ(t)|2 dt <∞.
4. The transforms of low powers of t vanish, that is, the first p moments:∫ +∞
−∞
t0 Ψ(t) dt =
∫ +∞
−∞
t1 Ψ(t) dt = · · · =
∫ +∞
−∞
tp−1 Ψ(t) dt = 0.
This makes the transform more sensitive to details than to general shape.
You can think of scale as being like the scale on a map (also discussed in §13.5.2 with
reference to fractal analysis) or in terms of resolution, as might occur in photographic images.
Regardless of the words, we will see in Chapter 12, “Discrete & Continuous Nonlinear Dy-
namics,” that if we have a fractal, then we have a self-similar object that looks the same at all
scales or resolutions. Similarly, each wavelet basis function in a set is self-similar to the others,
but at a different scale or location.
In summary, wavelet bases are functions of the time variable t, as well as of the two
parameters s and τ . The t variable is integrated over to yield a transform that is a function of
the time scale s (frequency 2π/s) and window location τ .
As an example of how we use the two degrees of freedom, consider the analysis of a chirp
signal sin(60t2) (Figure 11.6). We see that a slice at the beginning of the signal is compared
to our first basis function. (The comparison is done via the convolution of the wavelet with the
signal.) This first comparison is with a narrow version of the wavelet, that is, at low scale, and
yields a single coefficient. The comparison at this scale continues with the next signal slice and
ends when the entire signal has been covered (the top row in Figure 11.6). Then the wavelet
is expanded, and comparisons are repeated. Eventually, the data are processed at all scales
and at all time intervals. The narrow signals correspond to a high-resolution analysis, while
the broad signals correspond to low resolution. As the scales get larger (lower frequencies,
lower resolution), fewer details of the time signal remain visible, but the overall shape or gross
features of the signal become clearer.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
238 CHAPTER 11
11.4.2 Continuous Wavelet Transform Implementation
We want to develop some intuition as to what wavelet transforms look like before going on to
apply them in unknown situations. Accordingly, modify the program you have been using for
the discrete Fourier transform so that it now computes the continuous wavelet transform.
1. You will want to see the effect of using different mother wavelets. Accordingly, write a
method that calculates the mother wavelet for
a. a Morlet wavelet (11.2),
b. a Mexican hat wavelet (11.3),
c. a Haar wavelet (the square wave in Figure 11.2).
2. Try out your transform for the following input signals and see if the results make sense:
a. A pure sine wave y(t) = sin 2πt,
b. A sum of sine waves y(t) = 2.5 sin 2πt+ 6 sin 4πt+ 10 sin 6πt,
c. The nonstationary signal for our problem (11.1)
y(t) =

sin 2πt, for 0 ≤ t ≤ 2,
5 sin 2πt+ 10 sin 4πt, for 2 ≤ t ≤ 8,
2.5 sin 2πt+ 6 sin 4πt+ 10 sin 6πt, for 8 ≤ t ≤ 12.
(11.18)
d. The half-wave function
y(t) =
{
sinωt, for 0 < t < T/2,
0, for T/2 < t < T.
3.  Use (11.17) to invert your wavelet transform and compare the reconstructed signal to
the input signal (you can normalize the two to each other). On the right in Figure 11.5
we show our comparison.
In Listing 11.1 we give our continuous wavelet transformation CWT.py [Lang]. Because
wavelets, with their transforms in two variables, are somewhat hard to grasp at first, we suggest
that you write your own code and include a portion that does the inverse transform as a check.
In the next section we will describe the discrete wavelet transformation that makes optimal
discrete choices for the scale and time translation parameters s and τ . Figure 11.5 shows
the spectrum produced for the input signal (11.1) in Figure 11.1. As was our goal, we see
predominantly one frequency at short times, two frequencies at intermediate times, and three
frequencies at longer times.
11.5 UNIT II. DISCRETE WAVELET TRANSFORMS AND
MULTIRESOLUTION ANALYSIS
As was true for DFTs, if a time signal is measured at only N discrete times,
y(tm) ≡ ym, m = 1, . . . , N, (11.19)
then we can determine only N independent components of the transform Y . The trick is to
compute only theN independent components required to reproduce the input signal, consistent
with the uncertainty principle. The discrete wavelet transform (DWT) evaluates the transforms
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
WAVELET ANALYSIS & DATA COMPRESSION 239
Figure 11.7 Time and frequency resolutions. Each box represents an equal portion of the time–frequency plane but
with different proportions of time and frequency.
with discrete values for the scaling parameter s and the time translation parameter τ :
ψj,k(t) =
Ψ
[
(t− k2j)/2j
]
√
2j
≡
Ψ
(
t/2j − k
)
√
2j
(DWT), (11.20)
s = 2j , τ =
k
2j
, k, j = 0, 1, . . . . (11.21)
Here j and k are integers whose maximum values are yet to be determined, and we have
xmll
assumed that the total time interval T = 1, so that time is always measured in integer values.
This choice of s and τ , based on powers of 2, is called a dyadic grid arrangement and is seen
to automatically perform the scalings and translations at the different time scales that are at the
heart of wavelet analysis.3 The discrete wavelet transform now becomes
xmllYj,k =
∫ +∞
−∞
dt ψj,k(t) y(t) '
∑
m
ψj,k(tm)y(tm)h (DWT), (11.22)
where the discreteness here refers to the wavelet basis set and not the time variable. For an
orthonormal wavelet basis, the inverse discrete transform is then
y(t) =
+∞∑
j, k=−∞
Yj,k ψj,k(t) (inverse DWT). (11.23)
This inversion will exactly reproduce the input signal at the N input points if we sum over an
infinite number of terms [Add 02]. Practical calculations will be less exact.
Listing 11.1 CWT.py computes a normalized (< 1) continuous wavelet transform of the signal data in input[
] (here assigned as a sum of sine functions) using Morlet wavelets (courtesy of Z. Dimcovic). The
discrete wavelet transform (DWT) in Listing 11.2 is faster and yields a compressed transform but is
less transparent. 
Shows d i f f e r e n t w a v e l e t s ( midd le f rame ) as used d u r i n g t r a n s f o r m , and
t r a n s f o r m ( t o p f rame ) f o r each w a v e l e t . Then w a v e l e t t r a n s l a t e s and s c a l e s .
See f i g u r e 1 1 . 6
CWT zd . j a v a C o n t i n u o u s Wavele t Trans fo rm . W r i t t e n by Z l a t k o Dimcovic
Wait t o s e e t h e i n v e r s e t r a n s f o r m , t h a t i s t h e o r i g i n a l s i g n a l s e e F ig 1 1 . 5
’’’
3Note that some references scale down with increasing j, in contrast to our scaling up.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
240 CHAPTER 11
from visual import *
from visual.graph import *
N=240
transfgr=display(x=0,y=0,width=600,height=200, title=’Transform , n o t n o r m a l i z e d’)
transf = curve(x=range(0,90),display=transfgr,color=color.cyan)
wavlgr = display(x=0,y=200,width=600,height=200,
title=’Mo r l e t Wavele t a t d i f f e r e n t s c a l e s , up t o s =12 .0’)
wavelet = curve(x=range(0,N),display=wavlgr,color=color.yellow)
invtrgr=display(x=0,y=400,width=600,height=200,
title=’ I n v e r s e Trans fo rm = o r i g i n a l s i g n a l , n o t no rma l i zed , a f t e r t r a n s f o r m i s
e v a l u a t e d’)
invtr= curve(x=range(0,N),display=invtrgr,color=color.green)
iT = 0.0
fT = 12.0
W = fT - iT # i,f times
h = W/N; #Steps
noPtsSig = N
noS = 30
noTau = 90 # of pts
iTau = 0.
iS = 0.1
tau = iTau
s = iS
# Need *very* small s steps for high-frequency, but only if s is small
# Thus increment s by multiplying by number close enough to 1
dTau = W/noTau
dS = (W/iS)**(1./noS)
#print "Using:ttau + dTau, dTau = %3.3f (noTau = %d)"
# + "\n\t s*dS, dS = %3.3f (noS = %d)%n%n", dTau, noTau, dS, noS);
#f = open(’ t r a n s f o r m . d a t’,’w + t’) # Data file
sig = zeros((noPtsSig),Float) # Signal
maxY = 0.001;
def signal(noPtsSig, y): #this is the signal
t = 0.0
hs = W / noPtsSig
t1 = W/6.
t2 = 4.*W/6.
for i in range(0,noPtsSig):
if t>= iT and t<=t1:
y[i] = sin(2*math.pi*t)
elif t>=t1 and t<=t2:
y[i] = 5.*sin(2*math.pi*t) + 10.*sin(4*math.pi*t);
elif t>=t2 and t<=fT:
y[i]=2.5*sin(2*math.pi*t)+ 6.*sin(4*math.pi*t)+10.0*sin(6*math.pi*t)
else:
print "In signal(...) : t out of range."
sys.exit(1)
t +=hs
signal(noPtsSig, sig)
Y = zeros((noS,noTau),Float) # Transform
def morlet(t, s, tau): # Mother see eq. 11.14
T = (t-tau)/s
return sin(8*T) * exp( -T*T/2. )
def transform(s, tau, sig): #see eq. 11.16
integral = 0.
t = iT; # "initial time" = class variable
wvlabel = label(pos=(0, 2.5), text=’s=’, box=0,display=wavlgr)
wvlabel.text = ’s =%5.3 f’ %s
for i in range(0,len(sig)):
t +=h
yy=morlet(t,s,tau)
wavelet.x[i]=2*t-12 #the transform are drawn
wavelet.y[i]=yy
integral += sig[i]*yy*h
output=integral / math.sqrt(s)
return output
def invTransform(t,Y): #given the transform (from previous steps)
s = iS #computes the original signal
tau = iTau
recSig_t = 0
for i in range (0,noS):
s *= dS
tau = iTau
for j in range (0,noTau):
tau += dTau
recSig_t += dTau*dS *(s**(-1.5))* Y[i,j] * morlet(t,s,tau)
return recSig_t
print "working, finding transform"
for i in range( 0, noS):
s *= dS # Scaling s
tau = iT
for j in range(0,noTau):
tau +=dTau # Translation
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
WAVELET ANALYSIS & DATA COMPRESSION 241
Y[i,j] = transform(s, tau, sig)
transf.x[j]=2.*tau-12
transf.y[j]=0.5*Y[i,j]
print "transform found"
print "finding inverse transform" # Find inverse TF
recSigData = "recSig.dat"
recSig = zeros(len(sig)) # Same resolution
t = 0.0;
print "count to 10"
kco=0
j=0
for rs in range(0, len(recSig)): #with inverse transform
recSig[rs] = invTransform(t, Y) #find the original signal
t += h #not normalized
invtr.x[rs]=2*rs-N
invtr.y[rs]=0.8*recSig[rs]
if kco%24==0: #print to see when ends
j +=1
#print j
kco +=1
print "nDone"
Notice in (11.20) and (11.22) that we have kept the time variable t in the wavelet basis
functions continuous, even though s and τ are made discrete. This is useful in establishing the
orthonormality of the basis functions,∫ +∞
−∞
dt ψ∗j,k(t)ψj′,k′(t) = δjj′ δkk′ , (11.24)
where δm,n is the Kronecker delta function. Being normalized to 1 means that each wavelet
basis has “unit energy”; being orthogonal means that each basis function is independent of the
others. And because wavelets are localized in time, the different transform components have
low levels of correlation with each other. Altogether, this leads to efficient and flexible data
storage.
The use of a discrete wavelet basis makes it clear that we sample the input signal at the
discrete values of time determined by the integers j and k. In general, you want time steps
that sample the signal at enough times in each interval to obtain the desired level of precision.
A rule of thumb is to start with 100 steps to cover each major feature. Ideally, the needed
times correspond to the times at which the signal was sampled, although this may require some
forethought.
Consider Figure 11.7. We measure a signal at a number of discrete times within the
intervals (k or τ values) corresponding to the vertical columns of fixed width along the time
axis. For each time interval, we want to sample the signal at a number of scales (frequencies
or j values). However, as discussed in §11.2, the basic mathematics of Fourier transforms
indicates that the width ∆t of a wave packet ψ(t) and the width ∆ω of its Fourier transform
Y (ω) are related by an uncertainty principle
∆ω∆t ≥ 2π.
This relation constrains the number of times we can meaningfully sample a signal in order to
determine a number of Fourier components. So while we may want a high-resolution repro-
duction of our signal, we do not want to store more data than are needed to obtain that repro-
duction. If we sample the signal for times centered about some τ in an interval of width ∆τ
(Figure 11.7) and then compute the transform at a number of scales s or frequencies ω = 2π/s
covering a range of height ∆ω, then the relation between the height and width is restricted by
the uncertainty relation, which means that each of the rectangles in Figure 11.7 has the same
area ∆ω∆t = 2π. The increasing heights of the rectangles at higher frequencies means that
a larger range of frequencies should be sampled as the frequency increases. The premise here
is that the low-frequency components provide the gross or smooth outline of the signal which,
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
242 CHAPTER 11
Figure 11.8 A multifrequency dyadic (power-of-2) filter tree used for discrete wavelet transformation. The L boxes
represent lowpass filters, while the H boxes represent highpass filters, each of which performs a con-
volution (transform). The circles containing ↓ 2 filter out half of the signal that enters them, which is
called subsampling or factor-of-2 decimation.
L
L
H H
LL
LHH
2
2
2
2
Data
Input 2
being smooth, does not require much detail, while the high-frequency components give the de-
tails of the signal over a short time interval and so require many components in order to record
these details with high resolution.
Industrial-strength wavelet analyses do not compute explicit integrals but instead apply
a technique known as multiresolution analysis (MRA). We give an example of this technique
in Figure 11.8 and in the code DWT.py in Listing 11.2. It is based on a pyramid algorithm that
samples the signal at a finite number of times and then passes it successively through a number
of filters, with each filter representing a digital version of a wavelet.
Filters were discussed in §10.7, where in (10.59) we defined the action of a linear filter
as a convolution of the filter response function with the signal. A comparison of the definition
of a filter to the definition of a wavelet transform (11.12) shows that the two are essentially the
same. Such being the case, the result of the transform operation is a weighted sum over the
input signal values, with each weight the product of the integration weight times the value of
the wavelet function at the integration point. Therefore, rather than tabulate explicit wavelet
functions, a set of filter coefficients is all that is needed for discrete wavelet transforms.
Because each filter in Figure 11.8 changes the relative strengths of the different frequency
components, passing the signal through a series of filters is equivalent, in the wavelet sense,
to analyzing the signal at different scales. This is the origin of the name “multiresolution
analysis.” Figure 11.8 shows how the pyramid algorithm passes the signal through a series of
highpass filters (H) and then through a series of lowpass filters (L). Each filter changes the scale
to that of the level below. Notice too, the circles containing ↓2 in Figure 11.8. This operation
filters out half of the signal and so is called subsampling or factor-of-2 decimation. It is the
way we keep the areas of each box in Figure 11.7 constant as we vary the scale and translation
times. We consider subsampling further when we discuss the pyramid algorithm.
In summary, the DWT process decomposes the signal into smooth information stored in
the low-frequency components and detailed information stored in the high-frequency compo-
nents. Because high-resolution reproductions of signals require more information about details
than about gross shape, the pyramid algorithm is an effective way to compress data while still
maintaining high resolution (we implement compression in F). In addition, because compo-
nents of different resolution are independent of each other, it is possible to lower the number
of data stored by systematically eliminating higher-resolution components. The use of wavelet
filters builds in progressive scaling, which is particularly appropriate for fractal-like reproduc-
tions.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
WAVELET ANALYSIS & DATA COMPRESSION 243
Figure 11.9 The original signal is processed by high- and low-band filters, and the outputs are downsampled with
every other point kept. The process continues until there are only two output points of high-band
filtering and two points of low-band filtering. The total number of output data equals the total number
of signal points. It may be easier to understand the output of such an analysis as the signal passed
through various filters rather than as a set of Fourier-like coefficients.
Input
N Samples
N/2
N/4
N/8
2
N/2
N/4
N/8
2
c
c
c
c
d
d
d
d
(1)
(2)
(3)
(n)
(1)
(2)
(3)
(n)
Coefficients
Coefficients
Coefficients
Coefficients
CoefficientsCoefficients
Coefficients
Coefficients
L
L
L
L
H
H
H
H
11.5.1 Pyramid Scheme Implementation 
We now wish to implement the pyramid scheme outlined in Figure 11.8. The filters L and H
will be represented by matrices, which is an approximate way to perform the integrations or
convolutions. Then there is a decimation of the output by one-half, and finally an interleaving
of the output for further filtering. This process simultaneously cuts down on the number of
points in the data set and changes the scale and the resolution. The decimation reduces the
number of values of the remaining signal by one half, with the low-frequency part discarded
because the details are in the high-frequency parts.
As indicated in Figure 11.9, the pyramid algorithm’s DWT successively (1) applies the
(soon-to-be-derived) c matrix (11.35) to the whole N -length vector,

Y0
Y1
Y2
Y3
 =

c0 c1 c2 c3
c3 −c2 c1 −c0
c2 c3 c0 c1
c1 −c0 c3 −c2


y0
y1
y2
y3
 , (11.25)
(2) applies it to the (N/2)-length smooth vector, (3) and then repeats until two smooth com-
ponents remain. (4) After each filtering, the elements are ordered, with the newest two smooth
elements on top, the newest detailed elements below, and the older detailed elements below
that. (5) The process continues until there are just two smooth elements left.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
244 CHAPTER 11
To illustrate, here we filter and reorder an initial vector of length N = 8:

y1
y2
y3
y4
y5
y6
y7
y8

filter−→

s
(1)
1
d
(1)
1
s
(1)
2
d
(1)
2
s
(1)
3
d
(1)
3
s
(1)
4
d
(1)
4

order−→

s
(1)
1
s
(1)
2
s
(1)
3
s
(1)
4
d
(1)
1
d
(1)
2
d
(1)
3
d
(1)
4

filter−→

s
(2)
1
d
(2)
1
s
(2)
2
d
(2)
2
d
(1)
1
d
(1)
2
d
(1)
3
d
(1)
4

order−→

s
(2)
1
s
(2)
2
d
(2)
1
d
(2)
2
d
(1)
1
d
(1)
2
d
(1)
3
d
(1)
4

. (11.26)
The discrete inversion of a transform vector back to a signal vector is made using the transpose
(inverse) of the transfer matrix at each stage. For instance,
y0
y1
y2
y3
 =

c0 c3 c2 c1
c1 −c2 c3 −c0
c2 c1 c0 c3
c3 −c0 c1 −c2


Y0
Y1
Y2
Y3
 . (11.27)
As a more realistic example, imagine that we have sampled the chirp signal y(t) =
sin(60t2) for 1024 times. The filtering process through which we place this signal is illustrated
as a passage from the top to the bottom in Figure 11.9. First the original 1024 samples are
passed through a single low band and a single high band (which is mathematically equivalent
to performing a series of convolutions). As indicated by the down arrows, the output of the first
stage is then downsampled (the number reduced by a factor of 2). This results in 512 points
from the high-band filter as well as 512 points from the low-band filter. This produces the first-
level output. The output coefficients from the high-band filters are called {d(1)i } to indicate
that they show details, and {s(1)i } to indicate that they show smooth features. The superscript
indicates that this is the first level of processing. The detail coefficients {d(1)} are stored to
become part of the final output.
In the next level down, the 512 smooth data {s(1)i } are passed through new low- and
high-band filters using a broader wavelet. The 512 outputs from each are downsampled to
form a smooth sequence {s(2)i } of size 256 and a detailed sequence {d
(2)
i } of size 256. Again
the detail coefficients {d(2)} are stored to become part of the final output. (Note that this is only
half the size of the previously stored details.) The process continues until there are only two
numbers left for the detail coefficients and two numbers left for the smooth coefficients. Since
this last filtering is done with the broadest wavelet, it is of the lowest resolution and therefore
requires the least information.
In Figure 11.10 we show the actual effects on the chirp signal of pyramid filtering for
various levels in the processing. (The processing is done with four-coefficient Daub4 wavelets,
which we will discuss soon.) At the uppermost level, the Daub4 wavelet is narrow, and so
convoluting this wavelet with successive sections of the signal results in smooth components
that still contain many large high-frequency parts. The detail components, in contrast, are
much smaller in magnitude. In the next stage, the wavelet is dilated to a lower frequency and
the analysis is repeated on just the smooth (low-band) part. The resulting output is similar, but
with coarser features for the smooth coefficients and larger values for the details. Note that
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
WAVELET ANALYSIS & DATA COMPRESSION 245
Figure 11.10 The filtering of the original signal at the top goes through the pyramid algorithm and produces the
outputs shown, in successive passes. The sampling is reduced by a factor of 2 in each step. Note that
in the upper graphs we have connected the points to make the output look continuous, while in the
lower graphs, with fewer points, we have plotted the output as histograms to make the points more
evident.
(a)
(b)
c)(
(d)
(e)
(f)
(g)
(h)
(i)
(j)
-4
-2
0
2
4
0
0
10 20
20
30 40
40
50 60
60
70 90 110 130
- 1.0
1.0
0
80 100 120 140 180 220
2
0
-2
4
0
-4
0 15 30 35 45 55 65
0 4 8 12 16
8
4
-4
0
-8
0 1
2
0
2
-2
0
2
3 4
0
2
4
0 2 4
-2
0
2
4
5 6 7 8
10 12 14
2 6 8
0
2
-2
6
4
4
16
4
0
-4
-8
8
4
0
-4
-8
16 20 24 28 32
0.2
0
-0.2
3
1
0
-1
-3
2
0
-2
0 50 100 150 250200
0.04
0.02
-0.02
-0.04
300 350 400 450 500
600 700 800 900 1000
0.06
0.04
0.02
0
1
0
-1
0 100 200 300 400 500
0.8
0.4
0
-0.4
-0.8
0 0.2 0.4 0.6 0.8 1.0
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
246 CHAPTER 11
in the upper graphs we have connected the points to make the output look continuous, while
in the lower graphs, with fewer points, we have plotted the output as histograms to make the
points more evident. Eventually the downsampling leads to just two coefficients output from
each filter, at which point the filtering ends.
To reconstruct the original signal (called synthesis or transformation) a reversed pro-
cess is followed: Begin with the last sequence of four coefficients, upsample them, pass them
through low- and high-band filters to obtain new levels of coefficients, and repeat until all the
N values of the original signal are recovered. The inverse scheme is the same as the processing
scheme (Figure 11.9), only now the direction of all the arrows is reversed.
11.5.2 Daubechies Wavelets via Filtering
We should now be able to understand that digital wavelet analysis has been standardized to
the point where classes of wavelet basis functions are specified not by their analytic forms but
rather by their wavelet filter coefficients. In 1988, the Belgian mathematician Ingrid Daubechies
discovered an important class of such filter coefficients [Daub 95]. We will study just the
Daub4 class containing the four coefficients c0, c1, c2, and c3.
Imagine that our input contains the four elements {y1, y2, y3, y4} corresponding to mea-
surements of a signal at four times. We represent a lowpass filter L and a highpass filter H in
terms of the four filter coefficients as
L=
(
+c0 +c1 +c2 +c3
)
(11.28)
H =
(
+c3 −c2 +c1 −c0
)
. (11.29)
To see how this works, we form an input vector by placing the four signal elements in a column
and then multiply the input by L and H:
L

y0
y1
y2
y3
 = (+c0 +c1 +c2 +c3)

y0
y1
y2
y3
 = c0y0 + c1y1 + c2y2 + c3y3,
H

y0
y1
y2
y3
 = (+c3 −c2 +c1 −c0)

y0
y1
y2
y3
 = c3y0 − c2y1 + c1y2 − c0y3.
We see that if we choose the values of the ci’s carefully, the result of L acting on the signal
vector is a single number that may be viewed as a weighted average of the four input signal
elements. Since an averaging process tends to smooth out data, the lowpass filter may be
thought of as a smoothing filter that outputs the general shape of the signal.
In turn, we see that if we choose the ci values carefully, the result of H acting on the
signal vector is a single number that may be viewed as the weighted differences of the input
signal. Since a differencing process tends to emphasize the variation in the data, the highpass
filter may be thought of as a detail filter that produces a large output when the signal varies
considerably, and a small output when the signal is smooth.
We have just seen how the individual L and H filters, each represented by a single row,
output one number when acting upon an input signal containing four elements in a column. If
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
WAVELET ANALYSIS & DATA COMPRESSION 247
we want the output of the filtering process Y to contain the same number of elements as the
input (four y’s in this case), we just stack the L and H filters together:

Y0
Y1
Y2
Y3
 =

L
H
L
H


y0
y1
y2
y3
 =

c0 c1 c2 c3
c3 −c2 c1 −c0
c2 c3 c0 c1
c1 −c0 c3 −c2


y0
y1
y2
y3
 . (11.30)
Of course the first and third rows of the Y vector will be identical, as will the second and fourth,
but we will get to that soon.
Now we go about determining the values of the filter coefficients ci by placing specific
demands upon the output of the filter. We start by recalling that in our discussion of discrete
Fourier transforms we observed that a transform is equivalent to a rotation from the time do-
main to the frequency domain. Yet we know from our study of linear algebra that rotations are
described by orthogonal matrices, that is, matrices whose inverses are equal to their transposes.
In order for the inverse transform to return us to the input signal, the transfer matrix must be
orthogonal. For our wavelet transformation to be orthogonal, we must have the 4 × 4 filter
matrix times its transpose equal to the identity matrix:

c0 c1 c2 c3
c3 −c2 c1 −c0
c2 c3 c0 c1
c1 −c0 c3 −c2


c0 c3 c2 c1
c1 −c2 c3 −c0
c2 c1 c0 c3
c3 −c0 c1 −c2
=

1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
 ,
⇒ c20 + c21 + c22 + c23 = 1, c2c0 + c3c1 = 0. (11.31)
Two equations in four unknowns are not enough for a unique solution, so we now include the
further requirement that the detail filterH = (c3, −c0, c1, −c2) must output a zero if the input
is smooth. We define “smooth” to mean that the input is constant or linearly increasing:
(
y0 y1 y2 y3
)
=
(
1 1 1 1
)
or
(
0 1 2 3
)
. (11.32)
This is equivalent to demanding that the moments up to order p are zero, that is, that we have
an “approximation of order p.” Explicitly,
H
(
y0 y1 y2 y3
)
=H
(
1 1 1 1
)
= H
(
0 1 2 3
)
= 0,
⇒ c3 − c2 + c1 − c0 = 0, 0× c3 − 1× c2 + 2× c1 − 3× c0 = 0,
⇒ c0 =
1 +
√
3
4
√
2
' 0.483, c1 =
3 +
√
3
4
√
2
' 0.836, (11.33)
c2 =
3−
√
3
4
√
2
' 0.224, c3 =
1−
√
3
4
√
2
' −0.129. (11.34)
We now have our basic Daub4 filter coefficients. They can be used to process signals with
more than four elements by creating a square filter matrix of the needed dimension that repeats
these elements by placing the row versions of L and H along the diagonal, with successive
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
248 CHAPTER 11
Figure 11.11 Left: The Daub4 e6 wavelet constructed by inverse transformation of the wavelet coefficients. Right:
The sum of Daub4 e10 and Daub4 1e58 wavelets of different scale and time displacements.
–0.1
–0.06
–0.02
0.02
0.06
0.1
0 400 800 1200
–0.3
–0.1
0.1
0.3
0 400 800 1200
pairs displaced two columns to the right. For example, for eight elements,
Y0
Y1
Y2
Y3
Y4
Y5
Y6
Y7

=

c0 c1 c2 c3 0 0 0 0
c3 −c2 c1 −c0 0 0 0 0
0 0 c0 c1 c2 c3 0 0
0 0 c3 −c2 c1 −c0 0 0
0 0 0 0 c0 c1 c2 c3
0 0 0 0 c3 −c2 c1 −c0
c2 c3 0 0 0 0 c0 c1
c1 −c0 0 0 0 0 c3 −c2


y0
y1
y2
y3
y4
y5
y6
y7

. (11.35)
Note that in order not to lose any information, the last pair on the bottom two rows is wrapped
xmll
over to the left. If you perform the actual multiplications indicated in (11.35), you will note
that the output has successive smooth and detailed information. The output is processed with
the pyramid scheme.
The time dependences of two Daub4 wavelets is displayed in Figure 11.11. To obtain
these from our filter coefficients, first imagine that an elementary wavelet y1,1(t) ≡ ψ1,1(t) is
input into the filter. This should result in a transform Y1,1 = 1. Inversely, we obtain y1,1(t) by
applying the inverse transform to a Y vector with a 1 in the first position and zeros in all the
other positions. Likewise, the ith member of the Daubechies class is obtained by applying the
inverse transform to a Y vector with a 1 in the ith position and zeros in all the other positions.
On the left in Figure 11.11 is the wavelet for coefficient 6 (thus the e6 notation). On
the right in Figure 11.11 is the sum of two wavelets corresponding to the coefficients 10 and
58. We see that the two wavelets have different levels of scale as well as different time posi-
tions. So even though the time dependence of the wavelets is not evident when wavelet (filter)
coefficients are used, it is there.
11.5.3 DWT Implementation and Exercise
Listing 11.2 gives our program for performing a DWT on the chirp signal y(t) = sin(60t2).
The method pyram calls the daube4 method to perform the DWT or inverse DWT, depending
upon the value of sign.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
WAVELET ANALYSIS & DATA COMPRESSION 249
1. Modify the program so that you output to a file the values for the input signal that your
code has read in. It is always important to check your input.
2. Try to reproduce the left of Figure 11.10 by using various values for the variable nend
that controls when the filtering ends. A value nend=1024 should produce just the first
step in the downsampling (top row in Figure 11.10). Selecting nend=512 should produce
the next row, while nend=4 should output just two smooth and detailed coefficients.
3. Reproduce the scale–time diagram shown on the right in Figure 11.10. This diagram
shows the output at different scales and serves to interpret the main components of the
signal and the time in which they appear. The time line at the bottom of the figure
corresponds to a signal of length 1 over which 256 samples were recorded. The low-
band (smooth) components are shown on the left, and the high-band components on the
right.
a. The bottommost figure results when nend = 256.
b. The figure in the second row up results from nend = 128, and we have the output
from two filterings. The output contains 256 coefficients but divides time into four
intervals and shows the frequency components of the original signal in more detail.
c. Continue with the subdivisions for nend = 64, 32, 16, 8, and 4.
4. For each of these choices except the topmost, divide the time by 2 and separate the
intervals by vertical lines.
5. The topmost spectrum is your final output. Can you see any relation between it and the
chirp signal?
6. Change the sign of sign and check that the inverse DWT reproduces the original signal.
7. Use the code to visualize the time dependence of the Daubechies mother function at
different scales.
a. Start by performing an inverse transformation on the eight-component signal
[0,0,0,0,1,0,0,0]. This should yield a function with a width of about 5 units.
b. Next perform an inverse transformation on a unit vector with N = 32 but with all
components except the fifth equal to zero. The width should now be about 25 units, a
larger scale but still covering the same time interval.
c. Continue this procedure until you obtain wavelets of 800 units.
d. Finally, with N = 1024, select a portion of the mother wavelet with data in the
horizontal interval [590,800]. This should show self-similarity similar to that at the
bottom of Figure 11.11.
Listing 11.2 DWT.py computes the discrete wavelet transform using the pyramid algorithm for the 2n signal values
stored in f[ ] (here assigned as the chirp signal sin 60t2). The Daub4 digital wavelets are the basis
functions, and sign = ±1 for transform/inverse. 
# DWT. py : D i s c r e t e Wavele t Transform , Daubech ie s t y p e
# Uses g l o b a l v a r i a b l e s nedeed t o pe r fo rm w a v e l e t o r i n v e r s e
from v i s u a l i m p o r t ∗ ; from v i s u a l . g raph i m p o r t ∗
sq3 = math . s q r t ( 3 ) ; f s q 2 = 4.0∗math . s q r t ( 2 ) ; N = 1024 # N = 2ˆ n
c0 = (1+ sq3 ) / f s q 2 ; c1 = (3+ sq3 ) / f s q 2 # Daubech ie s 4 c o e f f i c e n t s
c2 = (3− sq3 ) / f s q 2 ; c3 = (1− sq3 ) / f s q 2
d e f c h i r p ( x i ) : # c h i r p s i g n a l
y = math . s i n ( 6 0 . 0∗ x i ∗∗2) ;
r e t u r n y ;
d e f daube4 ( f , n , s i g n ) : # DWT i f s i g n >= 0 , i n v e r s e i f s i g n < 0
t r = z e r o s ( ( n + 1) , F l o a t ) # t e m p o r a r y v a r i a b l e
i f n < 4 : r e t u r n
mp = n / 2 # m i d p o i n t o f a r r a y
mp1 = mp + 1 # m i d p o i n t p l u s one
i f s i g n >= 0 : # DWT
j = 1
i = 1
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
250 CHAPTER 11
maxx = n / 2
i f n > 128 : # a p p r o p i a t e s c a l e s
maxy = 3 . 0
miny = − 3 . 0
Maxy = 0 . 2
Miny = − 0 . 2
speed = 50 # f a s t r a t e
e l s e :
maxy = 1 0 . 0
miny = − 5 . 0
Maxy = 7 . 5
Miny = − 7 . 5
speed = 8 # f o r l ower r a t e
t r a n s f g r 1 = g d i s p l a y ( x =0 , y =0 , wid th =600 , h e i g h t =400 , t i t l e =’Wavelet TF, down sample +
low pass’ , xmax=maxx , xmin =0 , ymax=maxy , ymin=miny )
t r a n s f = g v b a r s ( d e l t a = 2 .0∗ n /N, c o l o r = c o l o r . cyan , d i s p l a y = t r a n s f g r 1 )
t r a n s f g r 2 = g d i s p l a y ( x =0 , y =400 , wid th =600 , h e i g h t =400 , t i t l e =’Wavelet TF, down sample +
high pass’ , xmax=2∗maxx , xmin =0 , ymax=Maxy , ymin=Miny )
t r a n s f 2 = g v b a r s ( d e l t a =2.0∗ n /N, c o l o r = c o l o r . cyan , d i s p l a y = t r a n s f g r 2 )
w h i l e j <= n − 3 :
r a t e ( speed )
t r [ i ] = c0∗ f [ j ] + c1∗ f [ j +1] + c2∗ f [ j +2] + c3∗ f [ j +3] # low−p a s s
t r a n s f . p l o t ( pos = ( i , t r [ i ] ) ) # c c o e f f i c i e n t s
t r [ i +mp] = c3∗ f [ j ] − c2∗ f [ j +1] + c1∗ f [ j +2] − c0∗ f [ j +3] # high−p a s s
t r a n s f 2 . p l o t ( pos = ( i + mp , t r [ i + mp ] ) )
i += 1 # d c o e f f i c e n t s
j += 2 # downsampling
t r [ i ] = c0∗ f [ n−1] + c1∗ f [ n ] + c2∗ f [ 1 ] + c3∗ f [ 2 ] # low−p a s s f i l t e r
t r a n s f . p l o t ( pos = ( i , t r [ i ] ) ) # c c o e f f i c i e n t s
t r [ i +mp] = c3∗ f [ n−1] − c2∗ f [ n ] + c1∗ f [ 1 ] − c0∗ f [ 2 ] # high−p a s s f i l t e r
t r a n s f 2 . p l o t ( pos = ( i +mp , t r [ i +mp ] ) )
e l s e : # i n v e r s e DWT
t r [ 1 ] = c2∗ f [mp] + c1∗ f [ n ] + c0∗ f [ 1 ] + c3∗ f [ mp1 ] # low− p a s s f i l t e r
t r [ 2 ] = c3∗ f [mp] − c0∗ f [ n ] + c1∗ f [ 1 ] − c2∗ f [ mp1 ] # high−p a s s f i l t e r
j = 3
f o r i i n r a n g e ( 1 , mp) :
t r [ j ] = c2∗ f [ i ] + c1∗ f [ i +mp] + c0∗ f [ i +1] + c3∗ f [ i +mp1 ] # low− p a s s
j += 1 # upsample
t r [ j ] = c3∗ f [ i ] − c0∗ f [ i +mp] + c1∗ f [ i +1] − c2∗ f [ i +mp1 ] # high−p a s s
j += 1 ; # upsampl ing
f o r i i n r a n g e ( 1 , n +1) :
f [ i ] = t r [ i ] # copy TF t o a r r a y
d e f pyram ( f , n , s i g n ) : # DWT, r e p l a c e s f by TF
i f ( n < 4) : r e t u r n # t o o few d a t a
nend = 4 # i n d i c a t e s when t o s t o p
i f s i g n >= 0 : # Trans fo rm
nd = n
w h i l e nd >= nend : # Downsample f i l t e r i n g
daube4 ( f , nd , s i g n )
nd /= 2
e l s e : # I n v e r s e TF
f o r nd i n r a n g e ( 4 , n + 1) : # Upsampling
daube4 ( f , nd , s i g n )
nd ∗= 2
f = z e r o s ( (N + 1) , F l o a t ) # d a t a v e c t o r
i n x i = 1 . 0 /N # f o r c h i r p s i g n a l
x i = 0 . 0
f o r i i n r a n g e ( 1 , N + 1) :
f [ i ] = c h i r p ( x i ) # F u n c t i o n t o TF
x i += i n x i ;
n = N # must be 2 ˆm
pyram ( f , n , 1 ) # TF
# pyram ( f , n , − 1) # I n v e r s e TF
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Twelve
Discrete & Continuous Nonlinear Dynamics
Nonlinear dynamics is one of the success stories of computational science. It has been ex-
plored by mathematicians, scientists, and engineers, with computers as an essential tool. The
computations have led to the discovery of new phenomena such as solitons, chaos, and fractals,
as you will discover on your own. In addition, because biological systems often have complex
interactions and may not be in thermodynamic equilibrium states, models of them are often
nonlinear, with properties similar to those of other complex systems.
In Unit I we develop the logistic map as a model for how bug populations achieve dy-
namic equilibrium. It is an example of a very simple but nonlinear equation producing surpris-
ing complex behavior. In Unit II we explore chaos for a continuous system, the driven realistic
pendulum. Our emphasis there is on using phase space as an example of the usefulness of an
abstract space to display the simplicity underlying complex behavior. In Unit III we extend
the discrete logistic map to nonlinear differential models of coupled predator–prey populations
and their corresponding phase space plots.
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
Discrete Chaos 12.1–12.5 Pendulums Become Chaotic 12.10–12.14
Applets and Animations
Name Sections Name Sections
The Chaotic Pendulum 12.11–12.13 Hypersensitive Pendulums 12.11–12.13
Double Pendulum Movie 12.14 Classical Chaotic Scattering 9.14
HearData: Sound Converter for Data 12.13 Lissajous Figures 12.12
Visualizing with Sound 12.13
12.1 UNIT I. BUG POPULATION DYNAMICS (DISCRETE)
Problem: The populations of insects and the patterns of weather do not appear to follow any
simple laws.1 At times they appear stable, at other times they vary periodically, and at other
times they appear chaotic, only to settle down to something simple again. Your problem is to
deduce if a simple, discrete law can produce such complicated behavior.
1Except maybe in Oregon, where storm clouds come to spend their weekends.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
252 CHAPTER 12
12.2 THE LOGISTIC MAP (MODEL)
Imagine a bunch of insects reproducing generation after generation. We start with N0 bugs,
then in the next generation we have to live with N1 of them, and after i generations there are
Ni bugs to bug us. We want to define a model of how Nn varies with the discrete generation
number n. For guidance, we look to the radioactive decay simulation in Chapter 5, “Monte
Carlo Simulations”, where the discrete decay law, ∆N/∆t = −λN , led to exponential-like
decay. Likewise, if we reverse the sign of λ, we should get exponential-like growth, which is a
good place to start our modelling. We assume that the bug-breeding rate is proportional to the
number of bugs:
∆Ni
∆t
= λ Ni. (12.1)
Because we know as an empirical fact that exponential growth usually tapers off, we improve
the model by incorporating the observation that bugs do not live on love alone; they must
also eat. But bugs, not being farmers, must compete for the available food supply, and this
might limit their number to a maximum N∗ (called the carrying capacity). Consequently, we
modify the exponential growth model (12.1) by introducing a growth rate λ′ that decreases as
the population Ni approaches N∗:
λ = λ′(N∗ −Ni) ⇒
∆Ni
∆t
= λ′(N∗ −Ni)Ni. (12.2)
We expect that when Ni is small compared to N∗, the population will grow exponentially, but
that as Ni approaches N∗, the growth rate will decrease, eventually becoming negative if Ni
exceeds N∗, the carrying capacity.
Equation (12.2) is one form of the logistic map. It is usually written as a relation between
the number of bugs in future and present generations:
Ni+1 =Ni + λ′∆t(N∗ −Ni)Ni, (12.3)
=Ni
(
1 + λ′∆tN∗
) [
1− λ
′∆t
1 + λ′∆tN∗
Ni
]
. (12.4)
The map looks simple when expressed in terms of natural variables:
xmll xi+1 = µxi(1− xi), (12.5)
µ
def= 1 + λ′∆tN∗, xi
def=
λ′∆t
µ
Ni '
Ni
N∗
, (12.6)
where µ is a dimensionless growth parameter and xi is a dimensionless population variable.
Observe that the growth rate µ equals 1 when the breeding rate λ′ equals 0, and is otherwise
expected to be larger than 1. If the number of bugs born per generation λ′∆t is large, then
µ ' λ′∆tN∗ and xi ' Ni/N∗. That is, xi is essentially a fraction of the carrying capacity
N∗. Consequently, we consider x values in the range 0 ≤ xi ≤ 1, where x = 0 corresponds
to no bugs and x = 1 to the maximum population. Note that there is clearly a linear, quadratic
dependence of the RHS of (12.5) on xi. In general, a map uses a function f(x) to map one
number in a sequence to another,
xi+1 = f(xi). (12.7)
For the logistic map, f(x) = µx(1− x), with the quadratic dependence of f on x making this
a nonlinear map, while the dependence on only the one variable xi makes it a one-dimensional
map.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 253
Figure 12.1 The insect population xn versus the generation number n for various growth rates. (A) µ = 2.8, a single
attractor. If the fixed point is xn = 0, the system becomes extinct. (B) µ = 3.3, a double attractor. (C)
µ = 3.5, a quadruple attractor. (D) µ = 3.8, a chaotic regime. If µ < 1, the population goes extinct.
01 02 0
0
0.4
0.8
A
01 02 0
B
01 02 0
C
01 02 0
D
xn
n n
12.3 PROPERTIES OF NONLINEAR MAPS (THEORY)
Rather than do some fancy mathematical analysis to determine the properties of the logistic
map [Rash 90], we prefer to have you study it directly on the computer by plotting xi versus
generation number i. Some typical behaviors are shown in Figure 12.1. In 12.1A we see
equilibration into a single population; in 12.1B we see oscillation between two population
levels; in 12.1C we see oscillation among four levels; and in 12.1D we see a chaotic system.
The initial population x0 is known as the seed, and as long as it is not equal to zero, its exact
value usually has little effect on the population dynamics (similar to what we found when
generating pseudorandom numbers). In contrast, the dynamics are unusually sensitive to the
value of the growth parameter µ. For those values of µ at which the dynamics are complex,
there may be extreme sensitivity to the initial condition x0 as well as to the exact value of µ.
12.3.1 Fixed Points
An important property of the map (12.5) is the possibility of the sequence xi reaching a fixed
point at which xi remains or fluctuates about. We denote such fixed points as x∗. At a one-cycle
fixed point, there is no change in the population from generation i to generation i+ 1; that is,
xi+1 = xi = x∗. (12.8)
Using the logistic map (12.5) to relate xi+1 to xi yields the algebraic equation
µx∗(1− x∗) = x∗ ⇒ x∗ = 0 or x∗ =
µ− 1
µ
. (12.9)
The nonzero fixed point x∗ = (µ − 1)/µ corresponds to a stable population with a balance
between birth and death that is reached regardless of the initial population (Figure 12.1A). In
contrast, the x∗ = 0 point is unstable and the population remains static only as long as no bugs
exist; if even a few bugs are introduced, exponential growth occurs. Further analysis (§12.8)
tells us that the stability of a population is determined by the magnitude of the derivative of the
mapping function f(xi) at the fixed point [Rash 90]:∣∣∣∣ dfdx
∣∣∣∣
x∗
< 1 (stable). (12.10)
For the one cycle of the logistic map (12.5), we have
df
dx
∣∣∣∣
x∗
= µ− 2µx∗ =
{
µ, stable at x∗ = 0 if µ < 1,
2− µ, stable at x∗ = µ−1µ if µ < 3.
(12.11)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
254 CHAPTER 12
12.3.2 Period Doubling, Attractors
Equation (12.11) tells us that while the equation for fixed points (12.9) may be satisfied for all
values of µ, the populations will not be stable if µ > 3. For µ > 3, the system’s long-term
population bifurcates into two populations (a two-cycle), an effect known as period doubling
(Figure 12.1B). Because the system now acts as if it were attracted to two populations, these
populations are called attractors or cycle points. We can easily predict the x values for these
two-cycle attractors by demanding that generation i+2 have the same population as generation
i:
xi = xi+2 = µxi+1(1− xi+1) ⇒ x∗ =
1 + µ±
√
µ2 − 2µ− 3
2µ
. (12.12)
We see that as long as µ > 3, the square root produces a real number and thus that physical
solutions exist (complex or negative x∗ values are nonphysical). We leave it to your computer
explorations to discover how the system continues to double periods as µ continues to increase.
In all cases the pattern is the same: One of the populations bifurcates into two.
12.4 MAPPING IMPLEMENTATION
Program the logistic map to produce a sequence of population values xi as a function of the
generation number i. These are called map orbits. The assessment consists of confirmation of
Feigenbaum’s observations [Feig 79] of the different behavior patterns shown in Figure 12.1.
These occur for growth parameter µ = (0.4, 2.4, 3.2, 3.6, 3.8304) and seed population x0 =
0.75. Identify the following on your graphs:
1. Transients: Irregular behaviors before reaching a steady state that differ for different
seeds.
2. Asymptotes: In some cases the steady state is reached after only 20 generations, while for
larger µ values, hundreds of generations may be needed. These steady-state populations
are independent of the seed.
3. Extinction: If the growth rate is too low, µ ≤ 1, the population dies off.
4. Stable states: The stable single-population states attained for µ < 3 should agree with
the prediction (12.9).
5. Multiple cycles: Examine the map orbits for a growth parameter µ increasing continu-
ously through 3. Observe how the system continues to double periods as µ increases. To
illustrate, in Figure 12.1C with µ = 3.5, we notice a steady state in which the population
alternates among four attractors (a four-cycle).
6. Intermittency: Observe simulations for 3.8264 < µ < 3.8304. Here the system appears
stable for a finite number of generations and then jumps all around, only to become stable
again.
7. Chaos: We define chaos as the deterministic behavior of a system displaying no dis-
cernible regularity. This may seem contradictory; if a system is deterministic, it must
have step-to-step correlations (which, when added up, mean long-range correlations); but
if it is chaotic, the complexity of the behavior may hide the simplicity within. In an op-
erational sense, a chaotic system is one with an extremely high sensitivity to parameters
or initial conditions. This sensitivity to even minuscule changes is so high that it is im-
possible to predict the long-range behavior unless the parameters are known to infinite
precision (a physical impossibility).
The system’s behavior in the chaotic region is critically dependent on the exact values
of µ and x0. Systems may start out with nearly identical values for µ and x0 but end up
with quite different ones. In some cases the complicated behaviors of nonlinear systems
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 255
will be chaotic, but unless you have a bug in your program, they will not be random.2
a. Compare the long-term behaviors of starting with the two essentially identical seeds
x0 = 0.75 and x′0 = 0.75(1 + ), where  ' 2× 10−14.
b. Repeat the simulation with x0 = 0.75 and two essentially identical survival param-
eters, µ = 4.0 and µ′ = 4.0(1 − ). Both simulations should start off the same but
eventually diverge.
12.5 BIFURCATION DIAGRAM (ASSESSMENT)
Computing and watching the population change with generation number gives a good idea of
the basic dynamics, at least until it gets too complicated to discern patterns. In particular, as
the number of bifurcations keeps increasing and the system becomes chaotic, it is hard for us
to see a simple underlying structure within the complicated behavior. One way to visualize
what is going on is to concentrate on the attractors, that is, those populations that appear to
attract the solutions and to which the solutions continuously return. A plot of these attractors
(long-term iterates) of the logistic map as a function of the growth parameter µ is an elegant
way to summarize the results of extensive computer simulations.
A bifurcation diagram for the logistic map is given in Figure 12.2, while one for a Gaus-
sian map is given in Figure 12.3. For each value of µ, hundreds of iterations are made to make
sure that all transients essentially die out, and then the values (µ, x∗) are written to a file for
hundreds of iterations after that. If the system falls into an n cycle for this µ value, then there
should predominantly be n different values written to the file. Next, the value of the initial
populations x0 is changed slightly, and the entire procedure is repeated to ensure that no fixed
points are missed. When finished, your program will have stepped through all the values of
growth parameter µ, and for each value of µ it will have stepped through all the values of the
initial population x0. Our sample program Bugs.py is shown in Listing 12.1.
Listing 12.1 Bugs.py produces the bifurcation diagram of the logistic map. A finished program requires finer grids,
a scan over initial values, and removal of duplicates. 
# L o g i s t i c map
from v i s u a l . g raph i m p o r t ∗
m min = 1 . 0 ; m max = 4 . 0 ; s t e p = 0 . 0 1
graph1 = g d i s p l a y ( wid th = 600 , h e i g h t = 400 , t i t l e = ’Logistic map’ , x t i t l e = ’m’ ,
y t i t l e = ’x’ , xmax = 4 . 0 , xmin = 1 . 0 , ymax = 1 . 0 , ymin = 0 . 0 )
p t s = g d o t s ( shape = ’square’ , s i z e = 1 , c o l o r = c o l o r . g r e e n )
l a s t y = i n t (1000 ∗ 0 . 5 ) # t o e l i m i n a t e l a t e r some p o i n t s
c o u n t = 0 # t o p l o t l a t e r e v e r y two i t e r a t i o n s
f o r m i n a r a n g e ( m min , m max , s t e p ) :
y = 0 . 5
f o r i i n r a n g e ( 1 , 2 0 1 , 1 ) : # t o a v o i d t r a n s i e n t s
y = m∗y∗(1−y )
f o r i i n r a n g e ( 2 0 1 , 4 0 2 , 1 ) :
y = m∗y∗( 1 − y )
f o r i i n r a n g e ( 2 0 1 , 402 , 1 ) : # t o a v o i d t r a n s i e n t s
y = m∗y∗(1 − y )
i n t y = i n t (1000 ∗ y )
i f i n t y != l a s t y and c o u n t%2 == 0 : p t s . p l o t ( pos =(m, y ) ) # t o a v o i d r e p e a t s
l a s t y = i n t y
c o u n t += 1
p r i n t "done"
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
256 CHAPTER 12
Figure 12.2 The bifurcation plot, attractor populations versus growth rate, for the logistic map. The inset shows
some details of a three-cycle window.
Figure 12.3 A bifurcation plot for the Gaussian map. (Courtesy of W. Hager.)
12.5.1 Bifurcation Diagram Implementation
The last part of this problem is to reproduce Figure 12.2 at various levels of detail. (You
can listen to a sonification of this diagram on the CD or use one of the applet there to create
your own sonification.) While the best way to make a visualization of this sort would be with
visualization software that permits you to vary the intensity of each individual point on the
screen, we simply plot individual points and have the density in each region determined by the
number of points plotted there. When thinking about plotting many individual points to draw
a figure, it is important to keep in mind that your screen resolution is ∼100 dots per inch and
your laser printer resolution may be 300 dots per inch. This means that if you plot a point at
each pixel, you will be plotting∼3000× 3000'10 million elements. Beware: This can require
some time and may choke a printer. In any case, printing at a finer resolution is a waste of time.
2You may recall from Chapter 5, “Monte Carlo Simulations,” that a random sequence of events does not even have step-by-step
correlations.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 257
12.5.2 Visualization Algorithm: Binning
1. Break up the range 1 ≤ µ ≤ 4 into 1000 steps and loop through them. These are the
“bins” into which we will place the x∗ values.
2. In order not to miss any structures in your bifurcation diagram, loop through a range of
initial x0 values as well.
3. Wait at least 200 generations for the transients to die out and then print the next several
hundred values of (µ, x∗) to a file.
4. Print your x∗ values to no more than three or four decimal places. You will not be able
to resolve more places than this on your plot, and this restriction will keep your output
files smaller by permitting you to remove duplicates. It is hard to control the number
of decimal places in the output with Java’s standard print commands (although printf
and DecimalFormat do permit control). A simple approach is to multiply the xi values by
1000 and then throw away the part to the right of the decimal point. Because 0 ≤ xn ≤ 1,
this means that 0 ≤ 100∗xn ≤ 1000, and you can throw away the decimal part by casting
the resulting numbers as integers:
Ix[i]= (int)(1000*x[i]) Convert to 0 ≤ ints ≤ 1000
You may then divide by 1000 if you want floating-point numbers.
5. You also need to remove duplicate values of (x, µ) from your file (they just take up space
and plot on top of each other). You can do that in Unix/Linux with the sort -u command.
6. Plot your file of x∗ versus µ. Use small symbols for the points and do not connect them.
7. Enlarge sections of your plot and notice that a similar bifurcation diagram tends to be
contained within each magnified portion (this is called self-similarity).
8. Look over the series of bifurcations occurring at
µk ' 3, 3.449, 3.544, 3.5644, 3.5688, 3.569692, 3.56989, . . . . (12.13)
The end of this series is a region of chaotic behavior.
9. Inspect the way this and other sequences begin and then end in chaos. The changes
sometimes occur quickly, and so you may have to make plots over a very small range of
µ values to see the structures.
10. A close examination of Figure 12.2 shows regions where, with a slight increase in µ,
a very large number of populations suddenly change to very few populations. Whereas
these may appear to be artifacts of the video display, this is a real effect and these re-
gions are called windows. Check that at µ = 3.828427, chaos turns into a three-cycle
population.
12.5.3 Feigenbaum Constants (Exploration)
Feigenbaum discovered that the sequence of µk values (12.13) at which bifurcations occur
follows a regular pattern [Feig 79]. Specifically, it converges geometrically when expressed in
terms of the distance between bifurcations δ:
xmllµk → µ∞ −
c
δk
, δ = lim
k→∞
µk − µk−1
µk+1 − µk
. (12.14)
Use your sequence of µk values to determine the constants in (12.14) and compare them to
those found by Feigenbaum:
µ∞ ' 3.56995, c ' 2.637, δ ' 4.6692. (12.15)
Amazingly, the value of the Feigenbaum constant δ is universal for all second-order maps.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
258 CHAPTER 12
Table 12.1 Several Nonlinear Maps to Explore
Name f(x) Name f(x)
Logistic µx(1− x) Tent µ(1− 2 |x− 1/2|)
Ecology xeµ(1−x) Quartic µ[1− (2x− 1)4]
Gaussian ebx
2
+ µ
12.6 LOGISTIC MAP RANDOM NUMBERS (EXPLORATION) 
There are claims that the logistic map in the chaotic region (µ ≥ 4),
xi+1 ' 4xi(1− xi), (12.16)
can be used to generate random numbers [P&R 95]. Although successive xi’s are correlated, if
the population for approximately every sixth generation is examined, the correlation is effec-
tively gone and random numbers result. To make the sequence more uniform, a trigonometric
transformation is used:
yi =
1
π
cos−1(1− 2xi). (12.17)
Use the random-number tests discussed in Chapter 5, “Monte Carlo Simulation,” to test this
claim.
12.7 OTHER MAPS (EXPLORATION)
Bifurcations and chaos are characteristic properties of nonlinear systems. Yet systems can be
nonlinear in a number of ways. Table 12.1 lists four maps that generate xi sequences containing
bifurcations. The tent map derives its nonlinear dependence from the absolute value operator,
while the logistic map is a subclass of the ecology map. Explore the properties of these other
maps and note the similarities and differences.
12.8 SIGNALS OF CHAOS: LYAPUNOV COEFFICIENTS 
The Lyapunov coefficient λi provides an analytic measure of whether a system is chaotic
[Wolf 85, Ram 00, Will 97]. Physically, the coefficient is a measure of the growth rate of the
solution near an attractor. For 1-D maps there is only one such coefficient, whereas in general
there is a coefficient for each direction in space. The essential assumption is that neighboring
paths xn near an attractor have an n (or time) dependence L ∝ exp(λt). Consequently, orbits
that have λ > 0 diverge and are chaotic; orbits that have λ = 0 remain marginally stable,
while orbits with λ < 0 are periodic and stable. Mathematically, the Lyapunov coefficient or
exponent is defined as
λ = lim
t→∞
1
t
log
L(t)
L(t0)
, (12.18)
where L(t) is the distance between neighboring phase space trajectories at time t.
We calculate the Lyapunov exponent for a general 1-D map,
xn+1 = f(xn), (12.19)
and then apply the result to the logistic map. To determine stability, we examine perturbations
about a reference trajectory x0 by adding a small perturbation and iterating once [Mann 90,
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 259
Ram 00]:
x̂0 = x0 + δx0, x̂1 = x1 + δx1. (12.20)
We substitute this into (12.19) and expand f in a Taylor series around x0:
x1 + δx1 = f(x0 + δx0) ' f(x0) +
δf
δx
∣∣∣∣
x0
δx0 = x1 +
δf
δx
∣∣∣∣
x0
δx0,
⇒ δx1'
(
δf
δx
)
x0
δx0. (12.21)
(This is the proof of our earlier statement about the stability of maps.) To deduce the general
result we examine one more iteration:
δx2'
(
δf
δx
)
x1
δx1 =
(
δf
δx
)
x0
(
δf
δx
)
x1
δx0, (12.22)
⇒ δxn =
n−1∏
i=0
(
δf
δx
)
xi
δx0. (12.23)
This last relation tells us how trajectories differ on the average after n steps:
|δxn| = Ln|δx0|, Ln =
n−1∏
i=0
∣∣∣∣∣
(
δf
δx
)
xi
∣∣∣∣∣ . (12.24)
We now solve for the Lyapunov number L and take its logarithm to obtain the Lyapunov expo-
nent:
λ = ln(L) = lim
n→∞
1
n
n−1∑
i=0
ln
∣∣∣∣∣
(
δf
δx
)
xi
∣∣∣∣∣ . (12.25)
For the logistic map we obtain
λ =
1
n
n−1∑
i=0
ln |µ− 2µxi|, (12.26)
where the sum is over iterations.
The code LyapLog.py in Listing 12.2 computes the Lyapunov exponents for the bifur-
cation plot of the logistic map. In Fig. 12.4 left we show its output, and note that the sign
changes in λ where the system becomes chaotic, and abrupt changes in slope at bifurcations.
(A similar curve is obtained for the fractal dimension of the logistic map as, indeed, the two
are proportional.) In Figure 12.4 left we show its output and note the sign changes in λ where
the system becomes chaotic, and abrupt changes in slope at bifurcations. (A similar curve is
obtained for the fractal dimension of the logistic map, as indeed the two are proportional.)
12.8.1 Shannon Entropy 
Shannon entropy, like the Lyapunov exponent, is another analytic measure of chaos. It is a
measure of uncertainty that has proven useful in communication theory[Shannon 48, Ott 02,
G,T&C 06] and led to the concept of information entropy. Imagine that an experiment has N
possible outcomes. If the probability of each is p1, p2, . . . , pN , with normalization such that
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
260 CHAPTER 12
Figure 12.4 Left: Lyapunov exponent and bifurcation values for the logistic map as functions of the growth rate µ.
Right: Shannon entropy (reduced by a factor of 5) and the Lyapunov coefficient for the logistic map.

-0.4
0
0.4
0.8
3.5 3.6 3.7 3.8 3.9 4
Lyapunov exponent
Entropy
∑N
i=1 pi = 1, the Shannon entropy is defined as
SShannon = −
N∑
i=1
pi ln pi. (12.27)
Listing 12.2 LyapLog.py computes Lyapunov exponents for the bifurcation plot of the logistic map as a function
of growth rate. Note the fineness of the µ grid. 
# LyapLog . py : Lyapunov c o e f f o r l o g i s t i c map
from v i s u a l i m p o r t ∗ ; from v i s u a l . g raph i m p o r t ∗
m min = 2 . 1 ; m max = 4 . 0 ; s t e p = 0 . 0 5
graph1 = g d i s p l a y ( t i t l e = ’Lyapunov coef (blue) for logistic map (red)’ ,
x t i t l e = ’m’ , y t i t l e = ’x , Lyap’ , ymax = 1 . 0 , ymin = − 0 . 6 )
f u n c t 1 = g d o t s ( c o l o r = c o l o r . r e d )
f u n c t 2 = g cu r ve ( c o l o r = c o l o r . y e l l o w )
f o r m i n a r a n g e ( m min , m max , s t e p ) : # m loop
y = 0 . 5
suma = 0 . 0
f o r i i n r a n g e ( 1 , 401 , 1 ) : y = m∗y∗(1 − y ) # Skip t r a n s i e n t s
f o r i i n r a n g e ( 4 0 2 , 601 , 1 ) :
y = m∗y∗(1 − y )
f u n c t 1 . p l o t ( pos = (m, y ) )
suma = suma + math . l o g ( abs (m∗ ( 1 . − 2 .∗ y ) ) ) # Lyapunov
f u n c t 2 . p l o t ( pos = (m, suma / 4 0 1 ) ) # Normal i ze
If pi ≡ 0, there is no uncertainty and SShannon = 0, as you might expect. If all N outcomes
have equal probability, pi ≡ 1/N , we obtain SShannon = lnN , which diverges slowly as
N →∞.
The code Entropy.py in Listing 12.3 computes the Shannon entropy for the the logistic
map as a function of the growth parameter µ. The results (Figure 12.4 left) are seen to be quite
similar to the Lyapunov exponent, again with discontinuities occurring at the bifurcations.
12.9 UNIT I QUIZ
1. Consider the logistic map.
a. Make sketches of what a graph of population xi versus generation number i would
look like for extinction and for a period-two cycle.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 261
b. Describe in words and possibly a diagram the relation between the preceding two
sketches and the bifurcation plot of xi versus i.
2. Consider the tent map. Rather than compute this map, study it with just a piece of paper.
a. Make sketches of what a graph of population xi versus generation number i would
look like for extinction, for a period-one cycle, and for a period-two cycle.
b. Show that there is a single fixed point for µ > 1/2 and a period-two cycle for µ > 1.
Listing 12.3 Entropy.py computes the Shannon entropy for the logistic map as a function of growth parameter µ. 
# En t ropy . py Shannon En t ropy wi th L o g i s t i c map u s i n g T k i n t e r
from T k i n t e r i m p o r t ∗ ; i m p o r t math
from v i s u a l . g raph i m p o r t ∗ # f o r prob = z e r o s . . .
g l o b a l Xwidth , Yhe igh t
r o o t = Tk ( ) ; r o o t . t i t l e (’Entropy versus mu ’ )
mumin = 3 . 5 ; mumax = 4 . 0 ; dmu = 0 . 0 0 5 ; nb in = 1000 ; nmax = 100000
prob = z e r o s ( ( 1 0 0 0 ) , F l o a t )
minx = mumin ; maxx = mumax ; miny = 0 ; maxy = 2 . 5 ; Xwidth = 500 ; Yhe igh t = 500
c = Canvas ( r o o t , w id th = Xwidth , h e i g h t = Yhe igh t ) # i n i t i a l i z e c an va s
c . pack ( ) # pack c an va s
Bu t ton ( r o o t , t e x t = ’Quit’ , command = r o o t . q u i t ) . pack ( ) # t o b e g i n q u i t
d e f w o r l d 2 s c ( xl , y t , xr , yb ) : # x − l e f t , y − top , x − r i g h t , y − bot tom
’’’ maxx: window width maxy: window height
rm: right margin bm: bottom margin
lm: left margin tm: right margin
bx, mx, by, my: global constants for linear transformations
xcanvas = mx*xworld + mx ycanvas = my*yworld + my
from world (double) to window (int) coordinates
’’’
maxx = Xwidth # c an va s wid th
maxy = Yhe igh t # c an va s h e i g h t | | | tm |
lm = 0.10∗maxx # l e f t margin | | | |
rm = 0.90∗maxx # r i g h t margin | lm | | | |
bm = 0.85∗maxy # bot tom margin | | | | |
tm = 0.10∗maxy # t o p margin | | | | |
mx = ( lm − rm ) / ( x l − xr ) # | | bm rm | |
bx = ( x l∗rm − xr∗lm ) / ( x l − xr ) # | | | | |
my = ( tm − bm) / ( y t − yb ) # | |
by = ( yb∗tm − y t∗bm) / ( yb − y t ) # | |
l i n e a r T r = [mx , bx , my , by ] # ( maxx , maxy )
r e t u r n l i n e a r T r # r e t u r n s a l i s t w i th 4 e l e m e n t s
# P l o t y , x , axes ; wor ld coord c o n v e r t e d t o c an va s c o o r d i n a t e s
d e f x y a x i s (mx , bx , my , by ) : # t o be c a l l e d a f t e r c a l l workd2sc
x1 = ( i n t ) (mx∗minx + bx ) # minima and maxima c o n v e r t e d t o
x2 = ( i n t ) (mx∗maxx + bx ) # ca nv as c o o r d i n a d e s
y1 = ( i n t ) (my∗maxy + by )
y2 = ( i n t ) (my∗miny + by )
yc = ( i n t ) (my∗0 .0 + by )
c . c r e a t e l i n e ( x1 , yc , x2 , yc , f i l l = "red" ) # p l o t s x a x i s
c . c r e a t e l i n e ( x1 , y1 , x1 , y2 , f i l l = ’red’ ) # p l o t s y − a x i s
f o r i i n r a n g e ( 7 ) : # t o p l o t x t i c s
x = minx + ( i − 1) ∗0 .1 # wor ld c o o r d i n a t e s
x1 = ( i n t ) (mx∗x + bx ) # c o n v e r t t o c an va s coord
x2 = ( i n t ) (mx∗minx + bx )
y = miny + i ∗0 .5 # r e a l c o o r d i n a t e s
y2 = ( i n t ) (my∗y + by ) # c o n v e r t t o c an va s c o o r d s
c . c r e a t e l i n e ( x1 , yc − 4 , x1 , yc + 4 , f i l l = ’red’ ) # t i c s x
c . c r e a t e l i n e ( x2 − 4 , y2 , x2 + 4 , y2 , f i l l = ’red’ ) # t i c s y
c . c r e a t e t e x t ( x1 + 10 , yc + 10 , t e x t = ’%5.2f’% ( x ) , f i l l = ’red’ , a nc h o r = E ) # x a x i s
c . c r e a t e t e x t ( x2 + 30 , y2 , t e x t = ’%5.2f’% ( y ) , f i l l = ’red’ , a nc h o r = E ) # y a x i s
c . c r e a t e t e x t ( 7 0 , 30 , t e x t = ’Entropy’ , f i l l = ’red’ , a nc h o r = E ) # y a x i s name
c . c r e a t e t e x t ( 4 2 0 , yc − 10 , t e x t = ’mu’ , f i l l = ’red’ , a nc h o r = E ) # x a x i s name
mx , bx , my , by = w o r l d 2 s c ( minx , maxy , maxx , miny ) # t h e c a l l r e t u r n s a l i s t
x y a x i s (mx , bx , my , by ) # g i v e v a l u e s t o a x i s
mu0 = mumin∗mx + bx # f o r t h e b e g i n n i n g
e n t r 0 = my∗0 .0 + by # f i r s t coord . mu0 , e n t r 0
f o r mu i n a r a n g e ( mumin , mumax , dmu ) : # mu loop
p r i n t mu
f o r j i n r a n g e ( 1 , nb in ) :
p rob [ j ] = 0
y = 0 . 5
f o r n i n r a n g e ( 1 , nmax + 1) :
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
262 CHAPTER 12
y = mu∗y ∗ ( 1 . 0 − y ) # L o g i s t i c map , Skip t r a n s i e n t s
i f ( n > 30000) :
i b i n = i n t ( y∗nb in ) + 1
prob [ i b i n ] += 1
e n t r o p y = 0 .
f o r i b i n i n r a n g e ( 1 , nb in ) :
i f ( p rob [ i b i n ]>0) :
e n t r o p y = e n t r o p y − ( p rob [ i b i n ] / nmax )∗math . log10 ( prob [ i b i n ] / nmax )
e n t r p c = my∗ e n t r o p y + by # e n t r o p y t o c an va s c o o r d s
muc = mx∗mu + bx # mu t o c an va s c o o r d s
c . c r e a t e l i n e ( mu0 , e n t r 0 , muc , e n t r p c , wid th = 1 , f i l l = ’blue’ ) # s m a l l l i n e segment
mu0 = muc # b e g i n v a l u e s f o r n e x t l i n e
e n t r 0 = e n t r p c
r o o t . main loop ( ) # makes e f f e c t i v e e v e n t s
12.10 UNIT II. PENDULUMS BECOME CHAOTIC
In Unit I on bugs we discovered that a simple nonlinear difference equation yields solutions
that may be simple, complicated, or chaotic. Unit III will extend that model to the differential
form, which also exhibits complex behaviors. Now we search for similar nonlinear, complex
behaviors in the differential equation describing a realistic pendulum. Because chaotic be-
havior may resemble noise, it is important to be confident that the unusual behaviors arise
from physics and not numerics. Before we explore the solutions, we provide some theoretical
background on the use of phase space plots for revealing the beauty and simplicity underlying
complicated behaviors. We also provide two chaotic pendulum applets (Figure 12.5)under-
standing the new concepts. Our study is based on the description in [Rash 90], on the analytic
discussion of the parametric oscillator in [L&L,M 76], and on a similar study of the vibrating
pivot pendulum in [G,T&C 06].
Consider the pendulum on the left in Figure 12.5. We see a pendulum of length l driven by
an external sinusoidal torque f through air with a coefficient of drag α. Because there is no
restriction that the angular displacement θ be small, we call this a realistic pendulum. Your
problem is to describe the motion of this pendulum, first when the driving torque is turned off
but the initial velocity is large enough to send the pendulum over the top, and then when the
driving torque is turned on.
12.11 CHAOTIC PENDULUM ODE
What we call a chaotic pendulum is just a pendulum with friction and a driving torque (Fig-
ure 12.5 left) but with no small-deflection-angle approximation. Newton’s laws of rotational
motion tell us that the sum of the gravitational torque −mgl sin θ, the frictional torque −βθ̇,
and the external torque τ0 cos ωt equals the moment of inertia of the pendulum times its angu-
lar acceleration [Rash 90]:
xmll I d
2θ
dt2
=−mgl sin θ − β dθ
dt
+ τ0 cosωt, (12.28)
⇒ d
2θ
dt2
=−ω20 sin θ − α
dθ
dt
+ f cosωt, (12.29)
where ω0 =
mgl
I
, α =
β
I
, f =
τ0
I
. (12.30)
Equation (12.29) is a second-order time-dependent nonlinear differential equation. Its nonlin-
earity arises from the sin θ, as opposed to the θ, dependence of the gravitational torque. The
parameter ω0 is the natural frequency of the system arising from the restoring torque, α is a
measure of the strength of friction, and f is a measure of the strength of the driving torque.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 263
In our standard ODE form, dy/dt = y (Chapter 9, “Differential Equation Applications”), we
have two simultaneous first-order equations:
dy(0)
dt
= y(1), (12.31)
dy(1)
dt
=−ω20 sin y(0) − αy(1) + f cosωt,
where y(0) = θ(t), y(1) =
dθ(t)
dt
. (12.32)
xmllFigure 12.5 Left: A pendulum of length l driven through air by an external sinusoidal torque. The strength of the
torque is given by f and that of air resistance by α. Right: A double pendulum.
f
m
l


l1
l2
12.11.1 Free Pendulum Oscillations
If we ignore friction and external torques, (12.29) takes the simple form
d2θ
dt2
= −ω20 sin θ (12.33)
If the displacements are small, we can approximate sin θ by θ and obtain the linear equation of
simple harmonic motion with frequency ω0:
d2θ
dt2
' −ω20θ ⇒ θ(t) = θ0 sin(ω0t+ φ). (12.34)
In Chapter 9, “Differential Equation Applications,” we studied how nonlinearities produce
anharmonic oscillations, and indeed (12.33) is another good candidate for such studies. As
before, we expect solutions of (12.33) for the free realistic pendulum to be periodic, but with
a frequency ω ' ω0 only for small oscillations. Furthermore, because the restoring torque,
mgl sin θ ' mgl(θ − θ3/3), is less than the mglθ assumed in a harmonic oscillator, realistic
pendulums swing slower (have longer periods) as their angular displacements are made larger.
12.11.2 Solution as Elliptic Integrals
The analytic solution to the realistic pendulum is a textbook problem [L&L,M 76, M&T 03,
Schk 94], except that it is hardly a solution and hardly analytic. The “solution” is based on
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
264 CHAPTER 12
energy being a constant (integral) of the motion. For simplicity, we start the pendulum off at
rest from its maximum displacement θm. Because the initial energy is all potential, we know
that the total energy of the system equals its initial potential energy (Figure 12.5),
E = PE(0) = mgl −mgl cos θm = 2mgl sin2
(
θm
2
)
. (12.35)
Yet since E = KE + PE is a constant, we can write for any value of θ
xmll 2mgl sin2 θm2 =
1
2
I
(
dθ
dt
)2
+ 2mgl sin2
θ
2
,
⇒ dθ
dt
= 2ω0
[
sin2
θm
2
− sin2 θ
2
]1/2
⇒ dt
dθ
=
T0/π[
sin2(θm/2)− sin2(θ/2)
]1/2 ,
⇒ T
4
=
T0
4π
∫ θm
0
dθ[
sin2(θm/2)− sin2(θ/2)
]1/2 = T04π sin θmF
(
θm
2
θ
2
)
, (12.36)
⇒ T ' T0
[
1 +
1
4
sin2
θm
2
+
9
64
sin4
θm
2
+ · · ·
]
, (12.37)
where we have assumed that it takes T/4 for the pendulum to travel from θ = 0 to θm. The
integral in (12.36) is an elliptic integral of the first kind. If you think of an elliptic integral
as a generalization of a trigonometric function, then this is a closed-form solution; otherwise,
it’s an integral needing computation. The series expansion of the period (12.37) is obtained
by expanding the denominator and integrating it term by term. It tells us, for example, that an
amplitude of 80◦ leads to a 10% slowdown of the pendulum relative to the small θ result. In
contrast, we will determine the period empirically without the need for any expansions.
12.11.3 Implementation and Test: Free Pendulum
As a preliminary to the solution of the full equation (12.29), modify your rk4 program to solve
(12.33) for the free oscillations of a realistic pendulum.
1. Start your pendulum at θ = 0 with θ̇(0) 6= 0. Gradually increase θ̇(0) to increase the
importance of nonlinear effects.
2. Test your program for the linear case (sin θ → θ) and verify that
a. your solution is harmonic with frequency ω0 = 2π/T0, and
b. the frequency of oscillation is independent of the amplitude.
3. Devise an algorithm to determine the period T of the oscillation by counting the time
it takes for three successive passes of the amplitude through θ = 0. (You need three
passes because a general oscillation may not be symmetric about the origin.) Test your
algorithm for simple harmonic motion where you know T0.
4. For the realistic pendulum, observe the change in period as a function of increasing initial
energy or displacement. Plot your observations along with (12.37).
5. Verify that as the initial KE approaches 2mgl, the motion remains oscillatory but not
harmonic (Figure 12.8).
6. At E = 2 mgl (the separatrix), the motion changes from oscillatory to rotational (“over
the top” or “running”). See how close you can get to the separatrix and to its infinite
period.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 265
Figure 12.6 The data screen (left) and output screen (right) of the applet HearData on the CD. Columns of
(ti, x(ti)) data are pasted into the data window, processed into the graph in the output window, and then
converted to sound that is played by Java.
7.  Use the applet HearData (Figure 12.6) to convert your different oscillations to sound
and hear the difference between harmonic motion (boring) and anharmonic motion con-
taining overtones (interesting).
12.12 VISUALIZATION: PHASE SPACE ORBITS
The conventional solution to an equation of motion is the position x(t) and the velocity v(t) as
functions of time. Often behaviors that appear complicated as functions of time appear simpler
when viewed in an abstract space called phase space, where the ordinate is the velocity v(t)
and the abscissa is the position x(t) (Figure 12.7). As we see from the phase space figures, the
solutions form geometric objects that are easy to recognize. (We provide two applets on the
CD, Pend1 and Pend2, to help the reader make the connections between phase space shapes
and the corresponding physical motion.)
The position and velocity of a free harmonic oscillator are given by the trigonometric
functions
x(t) = A sin(ωt), v(t) =
dx
dt
= ωA cos(ωt). (12.38)
When substituted into the total energy, we obtain two important results:
E= KE + PE =
(
1
2
m
)
v2 +
(
1
2
ω2m2
)
x2 (12.39)
=
ω2m2A2
2m
cos2(ωt) +
1
2
ω2m2A2 sin2(ωt) =
1
2
mω2A2. (12.40)
The first equation, being that of an ellipse, proves that the harmonic oscillator follows closed
elliptical orbits in phase space, with the size of the ellipse increasing with the system’s energy.
The second equation proves that the total energy is a constant of the motion. Different initial
conditions having the same energy start at different places on the same ellipse and transverse
the same orbits.
In Figures 12.7–12.10 we show various phase space structures. Study these figures and
their captions and note the following:
• The orbits of anharmonic oscillations will still be ellipselike but with angular corners that
become more distinct with increasing nonlinearity.
• Closed trajectories describe periodic oscillations [the same (x, v) occur again and again],
with clockwise motion.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
266 CHAPTER 12
Figure 12.7 Three potentials and their characteristic behaviors in phase space. The different orbits correspond to
different energies, as indicated by the limits within the potentials (dashed lines). Notice the absence
of trajectories in regions forbidden by energy conservation. Left: A repulsive potential leads to open
orbits in phase space characteristic of nonperiodic motion. The phase space trajectories cross at the
hyperbolic point in the middle, an unstable equilibrium point. Middle: The harmonic oscillator leads
to symmetric ellipses in phase space; the closed orbits indicate periodic behavior, and the symmetric
trajectories indicate a symmetric potential. Right: A nonharmonic oscillator. Notice that the ellipselike
trajectories neither are ellipses nor are symmetric with respect to the v(t) axis.
xv(t)
x(t)
E1 E1
x(t)
v(t)
V(x)
x
E1
E2
E3
xv(t)
x(t)
E1
E2
E3
Figure 12.8 Left: Phase space trajectories for a plane (i.e. 2-D) pendulum including “over the top” or rotating
solutions. The trajectories are symmetric with respect to vertical and horizontal reflections through the
origin. At the bottom of the figure is shown the corresponding θ dependence of the potential. Right:
Position versus time for two initial conditions of a chaotic pendulum that end up with the same limit
cycle, and the corresponding phase space orbits. (Courtesy of W. Hager.)
pendulum
falls
back
pendulum
starts
rotating
–4
V(θ)
–2
0
2
–2 0 2 4 6 8 10
θ
θ
• Open orbits correspond to nonperiodic or “running” motion (a pendulum rotating like a
propeller).
• Regions of space where the potential is repulsive lead to open trajectories in phase space
(Figure12.7).
• As seen in Figure 12.8 left, the separatrix corresponds to the trajectory in phase space
that separates open and closed orbits. Motion on the separatrix is indeterminant, as the
pendulum may balance at the maxima of V (θ).
• Friction may cause the energy to decrease with time and the phase space orbit to spiral
into a fixed point.
• For certain parameters, a closed limit cycle occurs in which the energy pumped in by the
external torque exactly balances that lost by friction (Figure 12.8 right).
• Because solutions for different initial conditions are unique, different orbits do not cross.
Nonetheless, open orbits join at points of unstable equilibrium (hyperbolic points in Fig-
ure12.7 left) where an indeterminacy exists.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 267
Figure 12.9 Position and phase space plots for a chaotic pendulum with ω0 = 1, α = 0.2, f = 0.52, and ω = 0.666.
The rightmost initial condition displays more of the broadband Fourier spectrum characteristic of chaos.
(Examples of chaotic behavior can be seen in Figure 12.10.)
many cycle1 cycle3 cycle
12.12.1 Chaos in Phase Space
It is easy to solve the nonlinear ODE (12.31) on the computer using our usual techniques.
However, it is not so easy to understand the solutions because they are so rich in complexity.
The solutions are easier to understand in phase space, particularly if you learn to recognize
some characteristic structures there. Actually, there are a number of “tools” that can be used to
decide if a system is chaotic in contrast to just complex. Geometric structures in phase space
is one of them, and determination of the Lyupanov coefficient (discussed in §12.8) is another.
Both signal the simplicity lying within the complexity.
What may be surprising is that even though the ellipselike figures in phase space were
observed originally for free systems with no friction and no driving torque, similar structures
continue to exist for driven systems with friction. The actual trajectories may not remain on
a single structure for all times, but they are attracted to them and return to them often. In
contrast to periodic motion, which corresponds to closed figures in phase space, random motion
appears as a diffuse cloud filling an entire energetically accessible region. Complex or chaotic
motion falls someplace in between (Figure 12.9 middle). If viewed for long times and for
many initial conditions, chaotic trajectories (flows) through phase space, while resembling
the familiar geometric figures, they may contain dark or diffuse bands in places rather than
single lines. The continuity of trajectories within bands means that a continuum of solutions
is possible and that the system flows continuously among the different trajectories forming the
band. The transitions among solutions cause the coordinate space solutions to appear chaotic
and are what makes them hypersensitive to the initial conditions (the slightest change in which
causes the system to flow to nearby trajectories).
Pick out the following phase space structures in your simulations.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
268 CHAPTER 12
Figure 12.10 Some examples of complicated behaviors of a realistic pendulum. In the top three rows, sequentially
we see θ(t) behaviors, a phase space diagram containing regular patterns with dark bands, and a broad
Fourier spectrum. These features are characteristic of chaos. In the bottom two rows we see how the
behavior changes abruptly after a slight change in the magnitude of the force and that for f = 0.54
there occur the characteristic broad bands of chaos.
–15 –10
–2
0
2
f= .52
0 200 400
–10
0
–10
–2
0
2
f= .54
0 200 400 600
time
0
20
x.
.
0 200 400 600 800
0
40
θ(0)=0.314, θ(0)=0.8, ω=0.697
02 04 0 60 80
–2
0
2
02 4
ω
6 8 10
20
40
0
θ(t)
θ(t)
θ(θ)
θ(θ)
θ(θ)
.
Y(ω)
time
θ(t)
0 10 20
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 269
Limit cycles: When a chaotic pendulum is driven by a not-too-large driving torque, it is
possible to pick the magnitude for this torque such that after the initial transients die off,
the average energy put into the system during one period exactly balances the average
energy dissipated by friction during that period (Figure 12.8 right):
〈f cosωt〉 =
〈
α
dθ
dt
〉
=
〈
α
dθ(0)
dt
cosωt
〉
⇒ f = αdθ(0)
dt
. (12.41)
This leads to limit cycles that appear as closed ellipselike figures, yet the solution may be
unstable and make sporadic jumps between limit cycles.
Predictable attractors: Well-defined, fairly simple periodic behaviors that are not partic-
ularly sensitive to the initial conditions. These are orbits, such as fixed points and limit
cycles, into which the system settles or returns to often. If your location in phase space is
near a predictable attractor, ensuing times will bring you to it.
Strange attractors: Well-defined, yet complicated, semiperiodic behaviors that appear to
be uncorrelated with the motion at an earlier time. They are distinguished from predica-
ble attractors by being fractal (Chapter 13, “Fractals & Statistical Growth”) chaotic, and
highly sensitive to the initial conditions [J&S 98]. Even after millions of oscillations, the
motion remains attracted to them.
Chaotic paths: Regions of phase space that appear as filled-in bands rather than lines. Con-
tinuity within the bands implies complicated behaviors, yet still with simple underlying
structure.
Mode locking: When the magnitude f of the driving torque is larger than that for a limit
cycle (12.41), the driving torque can overpower the natural oscillations, resulting in a
steady-state motion at the frequency of the driver. This is called mode locking. While
mode locking can occur for linear or nonlinear systems, for nonlinear systems the driving
torque may lock onto the system by exciting its overtones, leading to a rational relation
between the driving frequency and the natural frequency:
ω
ω0
=
n
m
, n,m = integers. (12.42)
Butterfly effects: One of the classic quips about the hypersensitivity of chaotic systems to
the initial conditions is that the weather pattern in North America is hard to predict well
because it is sensitive to the flapping of butterfly wings in South America. Although this
appears to be counterintuitive because we know that systems with essentially identical ini-
tial conditions should behave the same, eventually the systems diverge. The applet Pend2
(Figure12.11 bottom) lets you compare two simulations of nearly identical initial condi-
tions. As seen on the right in Figure 12.11, the initial conditions for both pendulums differ
by only 1 part in 917, and so the initial paths in phase space are the same. Nonetheless,
at just the time shown here, the pendulums balance in the vertical position, and then one
falls before the other, leading to differing oscillations and differing phase space plots from
this time onward.
12.12.2 Assessment in Phase Space
The challenge in understanding simulations of the chaotic pendulum (12.31) is that the 4-D
parameter space (ω0, α, f, ω) is so immense that only sections of it can be studied system-
atically. We expect that sweeping through driving frequency ω should show resonances and
beating; sweeping through the frictional force α should show underdamping, critical damping,
and overdamping; and sweeping through the driving torque f should show mode locking (for
the right values of ω). All these behaviors can be found in the solution of your differential
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
270 CHAPTER 12
Figure 12.11 Top row: Output from the applet Pend1 producing an animation of a chaotic pendulum, along with
the corresponding position versus time and phase space plots. Right: the resulting Fourier spectrum
produced by Pend1. Bottom row: output from the applet Pend2 producing an animation of two chaotic
pendula, along with the corresponding phase space plots, and the final output with limit cycles (dark
bands).
equation, yet they are mixed together in complex ways.
In this assessment you should try to reproduce the behaviors shown in the phase space
diagrams in Figure 12.9 and in the applets in Figure 12.11. Beware: Because the system is
chaotic, you should expect that your results will be sensitive to the exact values of the initial
conditions and to the details of your integration routine. We suggest that you experiment; start
with the parameter values we used to produce our plots and then observe the effects of making
very small changes in parameters until you obtain different modes of behavior. Consequently,
an inability to reproduce our results for the parameter values does not necessarily imply that
something is “wrong.”
1. Take your solution to the realistic pendulum and include friction, making α an input
parameter. Run it for a variety of initial conditions, including over-the-top ones. Since
no energy is fed to the system, you should see spirals.
2. Next, verify that with no friction, but with a very small driving torque, you obtain a
perturbed ellipse in phase space.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 271
3. Set the driving torque’s frequency to be close to the natural frequency ω0 of the pendulum
and search for beats (Figure 12.6 right). Note that you may need to adjust the magnitude
and phase of the driving torque to avoid an “impedance mismatch” between the pendulum
and driver.
4. Finally, scan the frequency ω of the driving torque and search for nonlinear resonance (it
looks like beating).
5. Explore chaos: Start off with the initial conditions we used in Figure 12.9,
(x0, v0) = (−0.0885, 0.8), (−0.0883, 0.8), (−0.0888, 0.8).
To save time and storage, you may want to use a larger time step for plotting than the one
used to solve the differential equations.
6. Indicate which parts of the phase space plots correspond to transients. (The applets on
the CD may help you with this, especially if you watch the phase space features being
built up in time.)
7. Ensure that you have found:
a. a period-3 limit cycle where the pendulum jumps between three major orbits in phase
space,
b. a running solution where the pendulum keeps going over the top,
c. chaotic motion in which some paths in the phase space appear as bands.
8. Look for the “butterfly effect” (Figure 12.11 bottom). Start two pendulums off with
identical positions but with velocities that differ by 1 part in 1000. Notice that the initial
motions are essentially identical but that at some later time the motions diverge.
12.13 EXPLORATION: BIFURCATIONS OF CHAOTIC PENDULUMS
We have seen that a chaotic system contains a number of dominant frequencies and that the
system tends to “jump” from one to another. This means that the dominant frequencies occur
sequentially, in contrast to linear systems where they occur simultaneously. We now want
to explore this jumping as a computer experiment. If we sample the instantaneous angular
velocity θ̇ = dθ/dt of our chaotic simulation at various instances in time, we should obtain a
series of values for the frequency, with the major Fourier components occurring more often than
others.3 These are the frequencies to which the system is attracted. That being the case, if we
make a scatterplot of the sampled θ̇s for many times at one particular value of the driving force
and then change the magnitude of the driving force slightly and sample the frequencies again,
the resulting plot may show distinctive patterns of frequencies. That a bifurcation diagram
similar to the one for bug populations results is one of the mysteries of life.
In the scatterplot in Figure 12.12, we sample θ̇ for the motion of a chaotic pendulum
with a vibrating pivot point (in contrast to our usual vibrating external torque). This pendulum
is similar to our chaotic one (12.29), but with the driving force depending on sin θ:
d2θ
dt2
= −α dθ
dt
−
(
ω20 + f cosωt
)
sin θ. (12.43)
Essentially, the acceleration of the pivot is equivalent to a sinusoidal variation of g or ω20 .
Analytic [L&L,M 76, § 25–30] and numeric [DeJ 92, G,T&C 06] studies of this system exist.
To obtain the bifurcation diagram in Figure 12.12:
1. Use the initial conditions θ(0) = 1 and θ̇(0) = 1.
2. Set α = 0.1, ω0 = 1, and ω = 2, and vary 0 ≤ f ≤ 2.25.
3We refer to this angular velocity as θ̇ since we have already used ω for the frequency of the driver and ω0 for the natural
frequency.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
272 CHAPTER 12
Figure 12.12 A bifurcation diagram for the damped pendulum with a vibrating pivot (see also the similar diagram
for a double pendulum, Figure 12.14). The ordinate is |dθ/dt|, the absolute value of the instantaneous
angular velocity at the beginning of the period of the driver, and the abscissa is the magnitude of the
driving force f. Note that the heavy line results from the overlapping of points, not from connecting
the points (see enlargement in the inset).
0 1 2
0
2
f
|
(t
) 
|

3. For each value of f , wait 150 periods of the driver before sampling to permit transients
to die off. Sample θ̇ for 150 times at the instant the driving force passes through zero.
4. Plot the 150 values of |θ̇| versus f .
12.14 ALTERNATIVE PROBLEM: THE DOUBLE PENDULUM
For those of you who have already studied a chaotic pendulum, an alternative is to study a
double pendulum without any small-angle approximation (Figure 12.5 right and Fig. 12.13, and
animation DoublePend.mpg on the CD). A double pendulum has a second pendulum connected
to the first, and because each pendulum acts as a driving force for the other, we need not include
an external driving torque to produce a chaotic system (there are enough degrees of freedom
without it).
The equations of motions for the double pendulum are derived most directly from the
Lagrangian formulation of mechanics. The Lagrangian is fairly simple but has the θ1 and θ2
motions innately coupled:
xmll L= KE− PE =
1
2
(m1 +m2)l21θ̇1
2
+
1
2
m2l
2
2θ̇2
2
(12.44)
+m2l1l2θ̇1θ̇2 cos(θ1 − θ2) + (m1 +m2)gl1 cos θ1 +m2gl2 cos θ2.
The resulting dynamic equations couple the θ1 and θ2 motions:
(m1 +m2)l1θ̈1 +m2l2θ̈2 cos(θ1 − θ2) +m2l2θ̇2
2
sin(θ1 − θ2) (12.45)
+g(m1 +m2) sin θ1 = 0,
m2l2θ̈2 +m2l1θ̈1 cos(θ1 − θ2)−m2l1θ̇1
2
sin(θ1 − θ2) +mg sin θ2 = 0. (12.46)
Usually textbooks approximate these equations for small oscillations, which diminish the ef-
fects of the coupling. “Slow” and “fast” modes then result for in-phase and antiphase oscilla-
tions, respectively, that look much like regular harmonic motions. What’s more interesting is
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 273
Figure 12.13 Photographs of a double pendulum built by a student in the OSU Physics Department. The longer
pendulum consists of two separated shafts so that the shorter one can rotate through it. Both pendula
can go over the top. We see the pendulum released from rest and then moving quickly. The flash
photography stops the motion in various stages. (Photograph, R. Landau.)
loading DoublePend.mpg
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
274 CHAPTER 12
Figure 12.14 Left: Phase space trajectories for a double pendulum with m1 = 10m2, showing two dominant attrac-
tors. Right: A bifurcation diagram for the double pendulum displaying the instantaneous velocity of
the lower pendulum as a function of the mass of the upper pendulum. (Both plots are courtesy of J.
Danielson.)


.


0
0
4-8
Angular Velocity Massversus
10
0A
n
g
u
la
r 
V
e
lo
c
it
y
 o
f 
L
o
w
e
r 
P
e
n
d
u
lu
m
10 15
Mass of Upper Pendulum
the motion that results without any small-angle restrictions, particularly when the pendulums
have enough initial energy to go over the top (Figure 12.13). On the left in Figure 12.14 we see
several phase space plots for the lower pendulum withm1 = 10m2. When given enough initial
kinetic energy to go over the top, the trajectories are seen to flow between two major attractors
as energy is transferred back and forth to the upper pendulum.
On the right in Figure 12.14 is a bifurcation diagram for the double pendulum. This was
created by sampling and plotting the instantaneous angular velocity θ̇2 of the lower pendulum
at 70 times as the pendulum passed through its equilibrium position. The mass of the upper
pendulum (a convenient parameter) was then changed, and the process repeated. The resulting
structure is fractal and indicates bifurcations in the number of dominant frequencies in the
motion. A plot of the Fourier or wavelet spectrum as a function of mass is expected to show
similar characteristic frequencies.
12.15 ASSESSMENT: FOURIER/WAVELET ANALYSIS OF CHAOS
We have seen that a realistic pendulum experiences a restoring torque, τg ∝ sin θ ' θ −
θ3/3! + θ5/5! + · · · , that contains nonlinear terms that lead to nonharmonic behavior. In
addition, when a realistic pendulum is driven by an external sinusoidal torque, the pendulum
may mode-lock with the driver and so oscillate at a frequency that is rationally related to the
driver’s. Consequently, the behavior of the realistic pendulum is expected to be a combination
of various periodic behaviors, with discrete jumps between modes.
In this assessment you should determine the Fourier components present in the pendu-
lum’s complicated and chaotic behaviors. You should show that a three-cycle structure, for
example, contains three major Fourier components, while a five-cycle structure has five. You
should also notice that when the pendulum goes over the top, its spectrum contains a steady-
state (DC) component.
1. Dust off your program for analyzing a y(t) into Fourier components. Alternatively, you
may use a Fourier analysis tool contained in your graphics program or system library
(e.g., Grace and OpenDX).
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 275
2. Apply your analyzer to the solution of the chaotic pendulum for the cases where there
are one-, three-, and five-cycle structures in phase space. Deduce the major frequencies
contained in these structures.
3. Compare your results with the output of the Pend1 applet (Figure 12.11 top).
4. Try to deduce a relation among the Fourier components, the natural frequency ω0, and
the driving frequency ω.
5. A classic signal of chaos is a broadband, although not necessarily flat, Fourier spectrum.
Examine your system for parameters that give chaotic behavior and verify this statement.
Wavelet Exploration: We saw in Chapter 11, “Wavelet Analysis & Data Compression”, that
a wavelet expansion is more appropriate than a Fourier expansion for a signal containing com-
ponents that occur for finite periods of time. Because chaotic oscillations are just such signals,
repeat the Fourier analysis of this section using wavelets instead of sines and cosines. Can you
discern the temporal sequence of the various components?
12.16 EXPLORATION: ANOTHER TYPE OF PHASE SPACE PLOT
Imagine that you have measured the displacement of some system as a function of time. Your
measurements appear to indicate characteristic nonlinear behaviors, and you would like to
check this by making a phase space plot but without going to the trouble of measuring the
conjugate momenta to plot versus displacement. Amazingly enough, one may also plot x(t+τ)
versus x(t) as a function of time to obtain a phase space plot [Abar 93]. Here τ is a lag time and
should be chosen as some fraction of a characteristic time for the system under study. While
this may not seem like a valid way to make a phase space plot, recall the forward difference
approximation for the derivative,
v(t) =
dx(t)
dt
' x(t+ τ)− x(t)
τ
. (12.47)
We see that plotting x(t+ τ) versus x(t) is equivalent to plotting v(t) versus x(t).
Exercise: Create a phase space plot from the output of your chaotic pendulum by plotting
θ(t+ τ) versus θ(t) for a large range of t values. Explore how the graphs change for different
values of the lag time τ . Compare your results to the conventional phase space plots you
obtained previously.
12.17 FURTHER EXPLORATIONS
1. The nonlinear behavior in once-common objects such as vacuum tubes and metronomes
is described by the van der Pool equation,
indexVan der Pool equation
d2x
dt2
+ µ(x2 − x20)
dx
dt
+ ω20x = 0. (12.48)
The behavior predicted for these systems is self-limiting because the equation contains a
limit cycle that is also a predictable attractor. You can think of (12.48) as describing an
oscillator with x-dependent damping (the µ term). If x > x0, friction slows the system
down; if x < x0, friction speeds the system up. Orbits internal to the limit cycle spiral
out until they reach the limit cycle; orbits external to it spiral in.
2. Duffing oscillator: Another damped, driven nonlinear oscillator is
d2θ
dt2
− 1
2
θ(1− θ2) = −α dθ
dt
+ f cosωt. (12.49)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
276 CHAPTER 12
While similar to the chaotic pendulum, it is easier to find multiple attractors for this
oscillator [M&L 85].
3. Lorenz attractor: In 1962 Lorenz [Tab 89] was looking for a simple model for weather
prediction and simplified the heat transport equations to
dx
dt
= 10(y − x), dy
dt
= −xz + 28x− y, dz
dt
= xy − 8
3
z. (12.50)
The solution of these simultaneous first-order nonlinear equations gave the complicated
behavior that has led to the modern interest in chaos (after considerable doubt regarding
the reliability of the numerical solutions).
4. A 3-D computer fly: Make x+ y, x+ z, and y + z plots of the equations
x = sin ay − z cos bx, y = z sin cx− cos dy, z = e sinx. (12.51)
Here the parameter e controls the degree of apparent randomness.
5. Hénon–Heiles potential: The potential and Hamiltonian
V (x, y) =
1
2
x2 +
1
2
y2 + x2y − 1
3
y3, H =
1
2
p2x +
1
2
p2y + V (x, y), (12.52)
are used to describe three interacting astronomical objects. The potential binds the ob-
jects near the origin but releases them if they move far out. The equations of motion
follow from the Hamiltonian equations:
dpx
dt
= −x− 2xy, dpy
dt
= −y − x2 + y2, dx
dt
= px,
dy
dt
= py.
a. Numerically solve for the position [x(t), y(t)] for a particle in the Hénon–Heiles po-
tential.
b. Plot [x(t), y(t)] for a number of initial conditions. Check that the initial condition
E < 16 leads to a bounded orbit.
c. Produce a Poincaré section in the (y, py) plane by plotting (y, py) each time an orbit
passes through x = 0.
12.18 UNIT III. COUPLED PREDATOR–PREY MODELS 
In Unit I we saw complicated behavior arising from a model of bug population dynamics in
which we imposed a maximum population. We described that system with a discrete logistic
map. In Unit II we saw complex behaviors arising from differential equations and learned
how to use phase space plots to understand them. In this unit we study the differential equa-
tion model describing predator–prey population dynamics proposed by the American physical
chemist Lotka [Lot 25] and the Italian mathematician Volterra [Volt 26]. Differential equa-
tions are easy to solve numerically and should be a good approximation if populations are
large. However, there are equivalent discrete map versions of the model as well. Though
simple, versions of these equations are used to model biological systems and neural networks.
Problem: Is it possible to use a small number of predators to control a population of pests so
that the number of pests remains approximately constant? Include in your considerations the
interaction between the populations as well as the competition for food and predation time.
12.19 LOTKA–VOLTERRA MODEL
We extend the logistic map to the Lotka–Volterra Model (LVM) to describe two populations
coexisting in the same geographical region. Let
p(t) = prey density, P (t) = predator density. (12.53)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 277
Figure 12.15 The populations of prey p and predator P from the Lotka–Volterra model. Left: The time depen-
dences of the prey p(t) (solid) and the predators P(t) (dashed). Right: Prey population p versus
predator population P. The different orbits in this “phase space plot” correspond to different initial
conditions.
T
t
0
2
4
0 200 400
p(t)
P(t)
P
p
0
2
4
0 1 2
In the absence of interactions between the species, we assume that the prey population p breeds
at a per-capita rate of a, which would lead to exponential growth:
dp
dt
= ap, ⇒ p(t) = p(0)eat. (12.54)
Yet exponential growth does not occur because the predators P eat more prey as their numbers
increase. The interaction rate between predator and prey requires both to be present, with the
simplest assumption being that it is proportional to their joint probability:
Interaction rate = bpP.
This leads to a prey growth rate including both predation and breeding:
dp
dt
= a p− b pP, (LVM-I for prey). (12.55)
If left to themselves, predators P will also breed and increase their population. Yet predators
need animals to eat, and if there are no other populations to prey upon, they will eat each other
(or their young) at a per-capita mortality rate m:
dP
dt
∣∣∣∣
competition
= −mP, ⇒ P (t) = P (0)e−mt. (12.56)
However, once there are prey to interact with (read ”eat”) at the rate bpP , the predator popula-
tion will grow at the rate
dP
dt
=  b p P −mP (LVM-I for predators), (12.57)
where  is a constant that measures the efficiency with which predators convert prey interactions
into food.
Equations (12.55) and (12.57) are two simultaneous ODEs and are our first model. We
solve them with the rk4 algorithm of Chapter 9, “Differential Equation Applications”, after
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
278 CHAPTER 12
placing them in the standard dynamic form,
dy/dt = f(y, t),
y0 = p, f0 = a y0 − b y0 y1,
y1 = P, f1 =  b y0 y1 −my1.
(12.58)
A sample code to do this is PredatorPrey.py and is given on the CD. Results from our
solution are shown in Figure 12.15. On the left we see that the two populations oscillate out of
phase with each other in time; when there are many prey, the predator population eats them and
grows; yet then the predators face a decreased food supply and so their population decreases;
that in turn permits the prey population to grow, and so forth. On the right in Figure 12.15
we plot a phase space plot (phase space plots are discussed in Unit II) of P (t) versus p(t).
A closed orbit here indicates a limit cycle that repeats indefinitely. Although increasing the
initial number of predators does decrease the maximum number of pests, it is not a satisfactory
solution to our problem, as the large variation in the number of pests cannot be called control.
12.19.1 LVM with Prey Limit
The initial assumption in the LVM that prey grow without limit in the absence of predators is
clearly unrealistic. As with the logistic map, we include a limit on prey numbers that accounts
for depletion of the food supply as the prey population grows. Accordingly, we modify the
constant growth rate a → a(1 − p/K) so that growth vanishes when the population reaches a
limit K, the carrying capacity,
dp
dt
= a p
(
1− p
K
)
− b pP, dP
dt
=  b p P −mP (LVM-II). (12.59)
The behavior of this model with prey limitations is shown in Figure 12.16. We see that both
populations exhibit damped oscillations as they approach their equilibrium values. In addition,
and as hoped for, the equilibrium populations are independent of the initial conditions. Note
how the phase space plot spirals inward to a single close limit cycle, on which it remains, with
little variation in prey number. This is control, and we may use it to predict the expected pest
population.
12.19.2 LVM with Predation Efficiency
An additional unrealistic assumption in the original LVM is that the predators immediately
eat all the prey with which they interact. As anyone who has watched a cat hunt a mouse
knows, predators spend time finding prey and also chasing, killing, eating, and digesting it (all
together called handling). This extra time decreases the rate bpP at which prey are eliminated.
We define the functional response pa as the probability of one predator finding one prey. If a
single predator spends time tsearch searching for prey, then
pa = b tsearch p ⇒ tsearch =
pa
bp
. (12.60)
If we call th the time a predator spends handling a single prey, then the effective time a predator
spends handling a prey is pa th. Such being the case, the total time T that a predator spends
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 279
Figure 12.16 The Lotka–Volterra model of prey population p and predator population P with a prey population
limit. Left: The time dependences of the prey p(t) (solid) and the predators P(t) (dashed). Right:
Prey population p versus predator population P.
t
0
1
2
3
0 200 400
P
p
p
P
1
2
3
1 2.2
finding and handling a single prey is
T = tsearch + thandling =
pa
bp
+ path ⇒
pa
T
=
bp
1 + bpth
,
where pa/T is the effective rate of eating prey. We see that as the number of prey p→∞, the
efficiency in eating them→ 1. We include this efficiency in (12.59) by modifying the rate b at
which a predator eliminates prey to b/(1 + bpth):
dp
dt
= ap
(
1− p
K
)
− bpP
1 + bpth
, (LVM-III). (12.61)
To be more realistic about the predator growth, we also place a limit on the predator carrying
capacity but make it proportional to the number of prey:
dP
dt
= mP
(
1− P
kp
)
, (LVM-III). (12.62)
Solutions for the extended model (12.61) and (12.62) are shown in Figure 12.17. Observe the
existence of three dynamic regimes as a function of b:
• small b: no oscillations, no overdamping,
• medium b: damped oscillations that converge to a stable equilibrium,
• large b: limit cycle.
The transition from equilibrium to a limit cycle is called a phase transition.
We finally have a satisfactory solution to our problem. Although the prey population is
not eliminated, it can be kept from getting too large and from fluctuating widely. Nonetheless,
changes in the parameters can lead to large fluctuations or to nearly vanishing predators.
12.19.3 LVM Implementation and Assessment
1. Write a program to solve (12.61) and (12.62) using the rk4 algorithm and the following
parameter values.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
280 CHAPTER 12
Figure 12.17 Lotka–Volterra model with predation efficiency and prey limitations. From left to right: overdamping,
b = 0.01; damped oscillations, b = 0.1, and limit cycle, b = 0.3.
t
0
400
0 400
P
p
P
o
p
u
la
ti
o
n
t
0
200
0
t
0 400
P
p
Model a b  m K k
LVM-I 0.2 0.1 1 0.1 0 —
LVM-II 0.2 0.1 1 0.1 20 —
LVM-III 0.2 0.1 — 0.1 500 0.2
2. For each of the three models, construct
a. a time series for prey and predator populations,
b. phase space plots of predator versus prey populations.
3. LVM-I: Compute the equilibrium values for the prey and predator populations. Do you
think that a model in which the cycle amplitude depends on the initial conditions can be
realistic? Explain.
4. LVM-II: Calculate numerical values for the equilibrium values of the prey and predator
populations. Make a series of runs for different values of prey carrying capacity K. Can
you deduce how the equilibrium populations vary with prey carrying capacity?
5. Make a series of runs for different initial conditions for predator and prey populations.
Do the cycle amplitudes depend on the initial conditions?
6. LVM-III: Make a series of runs for different values of b and reproduce the three regimes
present in Figure 12.17.
7. Calculate the critical value for b corresponding to a phase transition between the stable
equilibrium and the limit cycle.
12.19.4 Two Predators, One Prey (Exploration)
1. Another version of the LVM includes the possibility that two populations of predators P1
and P2 may “share” the same prey population p. Investigate the behavior of a system in
which the prey population grows logistically in the absence of predators:
dp
dt
= ap
(
1− p
K
)
− (b1P1 + b2P2) p, (12.63)
dP
dt
= 1b1pP1 −m1P1,
dP2
dt
= 2b2pP2 −m2P2. (12.64)
a. Use the following values for the model parameters and initial conditions:
a = 0.2, K = 1.7, b1 = 0.1, b2 = 0.2, m1 = m2 = 0.1, 1 = 1.0,
2 = 2.0, p(0) = P2(0) = 1.7, and P1(0) = 1.0.
b. Determine the time dependences for each population.
c. Vary the characteristics of the second predator and calculate the equilibrium popula-
tion for the three components.
d. What is your answer to the question, “Can two predators that share the same prey
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
DISCRETE & CONTINUOUS NONLINEAR DYNAMICS 281
coexist?”
2. The nonlinear nature of the Lotka–Volterra model can lead to chaos and fractal behavior.
Search for chaos by varying the growth rates.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Thirteen
Fractals & Statistical Growth
It is common to notice regular and eye-pleasing natural objects, such as plants and sea shells,
that do not have well-defined geometric shapes. When analyzed mathematically, some of these
objects have a dimension that is a fractional number, rather than an integer, and so are called
“fractals”. In this chapter we implement simple, statistical models that generate fractals. To
the extent that these models generate structures that look like those in nature, it is reasonable
to assume that the natural processes must be following similar rules arising from the basic
physics or biology that creates the objects. Detailed applications of fractals can be found in
the literature [Mand 82, Arm 91, E&P 88, Sand 94, PhT 88].
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
In production - - -
Applets and Animations
Name Sections Name Sections Name Sections
Sierpinski 13.2 Fern 13.3 Tree 13.3
Film 13.4 Column 13.6 DLA 13.7
Cellular Automata 13.9 Ballistic Movie 13.4 -
13.1 FRACTIONAL DIMENSION (MATH)
Benoit Mandelbrot, who first studied fractional-dimension figures with supercomputers at IBM
Research, gave them the name fractals [Mand 82]. Some geometric objects, such as Koch
curves, are exact fractals with the same dimension for all their parts. Other objects, such
as bifurcation curves, are statistical fractals in which elements of randomness occur and the
dimension can be defined only locally or on the average.
Consider an abstract object such as the density of charge within an atom. There are an
infinite number of ways to measure the “size” of this object. For example, each moment 〈rn〉 is
a measure of the size, and there is an infinite number of moments. Likewise, when we deal with
complicated objects, there are different definitions of dimension and each may give a somewhat
different value. In addition, the fractal dimension is often defined by using a measuring box
whose size approaches zero, which is not practical for realistic applications.
Our first definition of the fractional dimension df (or Hausdorff–Besicovitch dimension)
is based on our knowledge that a line has dimension 1, a triangle has dimension 2, and a cube
has dimension 3. It seems perfectly reasonable to ask if there is some mathematical formula that
agrees with our experience with regular objects yet can also be used for determining fractional
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FRACTALS & STATISTICAL GROWTH 283
Figure 13.1 Left: A Sierpiński gasket containing 10,000 points constructed as a statistical fractal. Each filled part
of this figure is self-similar. Right: A Sierpiński gasket constructed by successively connecting the
midpoints of the sides of each equilateral triangle. (A–C) The first three steps in the process.
0
100
200
300
0 100 200 300
10,000 points
dimensions. For simplicity, let us consider objects that have the same length L on each side,
as do equilateral triangles and squares, and that have uniform density. We postulate that the
dimension of an object is determined by the dependence of its total mass upon its length:
M(L) ∝ Ldf , (13.1)
where the power df is the fractal dimension. As you may verify, this rule works with the 1-D,
2-D, and 3-D regular figures in our experience, so it is a reasonable definition. When we apply
(13.1) to fractal objects, we end up with fractional values for df . Actually, we will find it easier
to determine the fractal dimension not from an object’s mass, which is extensive (depends on
size), but rather from its density, which is intensive. The density is defined as mass/length for
a linear object, as mass/area for a planar object, and as mass/volume for a solid object. That
being the case, for a planar object we hypothesize that
ρ =
M(L)
area
∝ L
df
L2
∝ Ldf−2. (13.2)
13.2 THE SIERPIŃSKI GASKET (PROBLEM 1)
To generate our first fractal (Figure 13.1), we play a game of chance in which we place dots at
points picked randomly within a triangle. Here are the rules (which you should try out in the
margins now).
1. Draw an equilateral triangle with vertices and coordinates:
vertex 1: (a1, b1); vertex 2: (a2, b2); vertex 3: (a3, b3).
2. Place a dot at an arbitrary point P = (x0, y0) within this triangle.
3. Find the next point by selecting randomly the integer 1, 2, or 3:
a. If 1, place a dot halfway between P and vertex 1.
b. If 2, place a dot halfway between P and vertex 2.
c. If 3, place a dot halfway between P and vertex 3.
4. Repeat the process using the last dot as the new P .
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
284 CHAPTER 13
Mathematically, the coordinates of successive points are given by the formulas
xmll (xk+1, yk+1) = (xk, yk) + (an, bn)2 , n = integer (1 + 3ri), (13.3)
where ri is a random number between 0 and 1 and where the integer function outputs the
closest integer smaller than or equal to the argument. After 15,000 points, you should obtain a
collection of dots like those on the left in Figure 13.1.
13.2.1 Sierpiński Implementation
Write a program to produce a Sierpiński gasket. Determine empirically the fractal dimension of
your figure. Assume that each dot has mass 1 and that ρ = CLα. (You can have the computer
do the counting by defining an array box of all 0 values and then change a 0 to a 1 when a dot
is placed there.)
13.2.2 Assessing Fractal Dimension
The topology in Figure 13.1 was first analyzed by the Polish mathematician Sierpiński. Ob-
serve that there is the same structure in a small region as there is in the entire figure. In other
words, if the figure had infinite resolution, any part of the figure could be scaled up in size and
would be similar to the whole. This property is called self-similarity.
We construct a regular form of the Sierpiński gasket by removing an inverted equilateral
triangle from the center of all filled equilateral triangles to create the next figure (Figure 13.1
right). We then repeat the process ad infinitum, scaling up the triangles so each one has side
r = 1 after each step. To see what is unusual about this type of object, we look at how its
density (mass/area) changes with size and then apply (13.2) to determine its fractal dimension.
Assume that each triangle has mass m and assign unit density to the single triangle:
ρ(L = r) ∝ M
r2
=
m
r2
def= ρ0 (Figure 13.1A)
Next, for the equilateral triangle with side L = 2, the density
ρ(L = 2r) ∝ (M = 3m)
(2r)2
= 34mr2 =
3
4
ρ0 (Figure 13.1B)
We see that the extra white space in Figure 13.1B leads to a density that is 34 that of the previous
stage. For the structure in Figure 13.1C, we obtain
ρ(L = 4r) ∝ (M = 9m)
(4r)2
= (34)2
m
r2
=
(
3
4
)2
ρ0. (Figure 13.1C)
We see that as we continue the construction process, the density of each new structure is 34 that
of the previous one. Interesting. Yet in (13.2) we derived that
ρ ∝ CLdf−2. (13.4)
Equation (13.4) implies that a plot of the logarithm of the density ρ versus the logarithm of the
length L for successive structures yields a straight line of slope
df − 2 =
∆ log ρ
∆ logL
. (13.5)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FRACTALS & STATISTICAL GROWTH 285
As applied to our problem,
df = 2 +
∆ log ρ(L)
∆ logL
= 2 +
log 1− log 34
log1− log 2
' 1.58496. (13.6)
As is evident in Figure 13.1, as the gasket grows larger (and consequently more massive),
it contains more open space. So even though its mass approaches infinity as L → ∞, its
density approaches zero! And since a 2-D figure like a solid triangle has a constant density as
its length increases, a 2-D figure has a slope equal to 0. Since the Sierpiński gasket has a slope
df−2 ' −0.41504, it fills space to a lesser extent than a 2-D object but more than a 1-D object
does; it is a fractal with dimension ≤1.6.
13.3 BEAUTIFUL PLANTS (PROBLEM 2)
It seems paradoxical that natural processes subject to chance can produce objects of high regu-
larity and symmetry. For example, it is hard to believe that something as beautiful and graceful
as a fern (Figure 13.2 left) has random elements in it. Nonetheless, there is a clue here in that
much of the fern’s beauty arises from the similarity of each part to the whole (self-similarity),
with different ferns similar but not identical to each other. These are characteristics of frac-
tals. Your problem is to discover if a simple algorithm including some randomness can draw
regular ferns. If the algorithm produces objects that resemble ferns, then presumably you have
uncovered mathematics similar to that responsible for the shapes of ferns.
13.3.1 Self-Affine Connection ( Theory)
In (13.3), which defines mathematically how a Sierpiński gasket is constructed, a scaling factor
of 12 is part of the relation of one point to the next. A more general transformation of a point
P = (x, y) into another point P ′ = (x′, y′) via scaling is
(x′, y′) = s(x, y) = (sx, sy) (scaling). (13.7)
If the scale factor s > 0, an amplification occurs, whereas if s < 0, a reduction occurs. In our
definition (13.3) of the Sierpiński gasket, we also added in a constant an. This is a translation
operation, which has the general form
(x′, y′) = (x, y) + (ax, ay) (translation). (13.8)
Another operation, not used in the Sierpiński gasket, is a rotation by angle θ:
x′ = x cos θ − y sin θ, y′ = x sin θ + y cos θ (rotation). (13.9)
The entire set of transformations, scalings, rotations, and translations defines an affine transfor-
mation (affine denotes a close relation between successive points). The transformation is still
considered affine even if it is a more general linear transformation with the coefficients not all
related by a single θ (in that case, we can have contractions and reflections). What is important
is that the object created with these rules turns out to be self-similar; each step leads to new
parts of the object that bear the same relation to the ancestor parts as the ancestors did to theirs.
This is what makes the object look similar at all scales.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
286 CHAPTER 13
Figure 13.2 Left: A fern after 30,000 iterations of the algorithm (13.10). If you enlarge this, you will see that each
frond has a similar structure. Right: A fractal tree created with the simple algorithm (13.13).
13.3.2 Barnsley’s Fern Implementation
We obtain a Barnsley’s fern [Barns 93] by extending the dots game to one in which new points
are selected using an affine connection with some elements of chance mixed in:
xmll
(x, y)n+1 =

(0.5, 0.27yn), with 2% probability,
(−0.139xn + 0.263yn + 0.57
0.246xn + 0.224yn − 0.036), with 15% probability,
(0.17xn − 0.215yn + 0.408
0.222xn + 0.176yn + 0.0893), with 13% probability,
(0.781xn + 0.034yn + 0.1075
−0.032xn + 0.739yn + 0.27), with 70% probability.
(13.10)
To select a transformation with probability P , we select a uniform random number 0 ≤
r ≤ 1 and perform the transformation if r is in a range proportional to P:
P =

2%, r < 0.02,
15%, 0.02 ≤ r ≤ 0.17,
13%, 0.17 < r ≤ 0.3,
70%, 0.3 < r < 1.
(13.11)
The rules (13.10) and (13.11) can be combined into one:
(x, y)n+1 =

(0.5, 0.27yn), r < 0.02,
(−0.139xn + 0.263yn + 0.57
0.246xn + 0.224yn − 0.036), 0.02 ≤ r ≤ 0.17,
(0.17xn − 0.215yn + 0.408
0.222xn + 0.176yn + 0.0893), 0.17 < r ≤ 0.3,
(0.781xn + 0.034yn + 0.1075,
−0.032xn + 0.739yn + 0.27), 0.3 < r < 1.
(13.12)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FRACTALS & STATISTICAL GROWTH 287
Although (13.10) makes the basic idea clearer, (13.12) is easier to program.
The starting point in Barnsley’s fern (Figure 13.2) is (x1, y1) = (0.5, 0.0), and the points
are generated by repeated iterations. An important property of this fern is that it is not com-
pletely self-similar, as you can see by noting how different the stems and the fronds are. Never-
theless, the stem can be viewed as a compressed copy of a frond, and the fractal obtained with
(13.10) is still self-affine, yet with a dimension that varies in different parts of the figure.
hyperbaseurl./
loading 2slits.mpg
13.3.3 Self-Affinity in Trees Implementation
Now that you know how to grow ferns, look around and notice the regularity in trees (such as in
Figure 13.2 right). Can it be that this also arises from a self-affine structure? Write a program,
similar to the one for the fern, starting at (x1, y1) = (0.5, 0.0) and iterating the following
self-affine transformation:
(xn+1, yn+1) =

(0.05xn, 0.6yn), 10% probability,
(0.05xn,−0.5yn + 1.0), 10% probability,
(0.46xn − 0.15yn, 0.39xn + 0.38yn + 0.6), 20% probability,
(0.47xn − 0.15yn, 0.17xn + 0.42yn + 1.1), 20% probability,
(0.43xn + 0.28yn,−0.25xn + 0.45yn + 1.0), 20% probability,
(0.42xn + 0.26yn,−0.35xn + 0.31yn + 0.7), 20% probability.
(13.13)
13.4 BALLISTIC DEPOSITION (PROBLEM 3)
There are a number of physical and manufacturing processes in which particles are deposited
on a surface and form a film. Because the particles are evaporated from a hot filament, there is
randomness in the emission process yet the produced films turn out to have well-defined, reg-
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
288 CHAPTER 13
Figure 13.3 A simulation of the ballistic deposition of 20,000 particles on a substrate of length 200. The vertical
height increases with the length of deposition time so that the top is the final surface.
0 100 200
0
100
200
Length
S
u
rf
a
c
e
 H
e
ig
h
t
ular structures. Again we suspect fractals. Your problem is to develop a model that simulates
this growth process and compare your produced structures to those observed.
13.4.1 Random Deposition Algorithm
The idea of simulating random depositions was first reported in [Vold 59], which includes ta-
bles of random numbers used to simulate the sedimentation of moist spheres in hydrocarbons.
We shall examine a method of simulation [Fam 85] that results in the deposition shown in Fig-
ure 13.3. Consider particles falling onto and sticking to a horizontal line of length L composed
of 200 deposition sites. All particles start from the same height, but to simulate their different
velocities, we assume they start at random distances from the left side of the line. The simula-
tion consists of generating uniform random sites between 0 and L and having a particle stick
to the site on which it lands. Because a realistic situation may have columns of aggregates of
different heights, the particle may be stopped before it makes it to the line, or it may bounce
around until it falls into a hole. We therefore assume that if the column height at which the
particle lands is greater than that of both its neighbors, it will add to that height. If the particle
lands in a hole, or if there is an adjacent hole, it will fill up the hole. We speed up the simulation
by setting the height of the hole equal to the maximum of its neighbors:
1. Choose a random site r.
2. Let the array hr be the height of the column at site r.
3. Make the decision:
hr =
{
hr + 1, if hr ≥ hr−1, hr > hr+1,
max[hr−1, hr+1], if hr < hr−1, hr < hr+1.
(13.14)
Our simulation is Fractals/Film.py with the essential loop:
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FRACTALS & STATISTICAL GROWTH 289
 
s p o t = i n t ( random )
i f ( s p o t == 0)
i f ( c o a s t [ s p o t ] < c o a s t [ s p o t +1] )
c o a s t [ s p o t ] = c o a s t [ s p o t + 1 ] ;
e l s e c o a s t [ s p o t ] + + ;
e l s e i f ( s p o t == c o a s t . l e n g t h − 1)
i f ( c o a s t [ s p o t ] < c o a s t [ spo t −1]) c o a s t [ s p o t ] = c o a s t [ spo t −1];
e l s e c o a s t [ s p o t ] + + ;
e l s e i f ( c o a s t [ s p o t ]< c o a s t [ spo t−1] && c o a s t [ s p o t ]< c o a s t [ s p o t +1] )
i f ( c o a s t [ spo t−1] > c o a s t [ s p o t +1] ) c o a s t [ s p o t ] = c o a s t [ spo t −1];
e l s e c o a s t [ s p o t ] = c o a s t [ s p o t + 1 ] ;
e l s e c o a s t [ s p o t ] + + ;
The results of this type of simulation show several empty regions scattered throughout the
line (Figure 13.3), which is an indication of the statistical nature of the process while the
film is growing. Simulations by Fereydoon reproduced the experimental observation that the
average height increases linearly with time and produced fractal surfaces. (You will be asked
to determine the fractal dimension of a similar surface as an exercise.)
13.5 LENGTH OF BRITISH COASTLINE (PROBLEM 4)
In 1967 Mandelbrot [Mand 67] asked a classic question, “How long is the coast of Britain?” If
Britain had the shape of Colorado or Wyoming, both of which have straight-line boundaries,
its perimeter would be a curve of dimension 1 with finite length. However, coastlines are
geographic not geometric curves, with each portion of the coast often statistically self-similar
to the entire coast yet on a reduced scale. In the latter cases the perimeter may be modeled as
a fractal, in which case the length is either infinite or meaningless. Mandelbrot deduced the
dimension of the west coast of Great Britain to be df = 1.25. In your problem we ask you to
determine the dimension of the perimeter of one of our fractal simulations.
13.5.1 Coastlines as Fractals (Model)
The length of the coastline of an island is the perimeter of that island. While the concept of
perimeter is clear for regular geometric figures, some thought is required to give it meaning
for an object that may be infinitely self-similar. Let us assume that a map maker has a ruler
of length r. If she walks along the coastline and counts the number of times N that she must
place the ruler down in order to cover the coastline, she will obtain a value for the length L of
the coast as Nr. Imagine now that the map maker keeps repeating her walk with smaller and
smaller rulers. If the coast was a geometric figure or a rectifiable curve, at some point the length
L would become essentially independent of r and would approach a constant. Nonetheless, as
discovered empirically by Richardson [Rich 61] for natural coastlines, such as those of South
Africa and Britain, the perimeter appears to be a function of r:
L(r) = Mr1−df , (13.15)
where M and df are empirical constants. For a geometric figure or for Colorado, df = 1 and
the length approaches a constant as r → 0. Yet for a fractal with df > 1, the perimeter L→∞
as r → 0. This means that as a consequence of self-similarity, fractals may be of finite size but
have infinite perimeters. Physically, at some point there may be no more details to discern as
r → 0 (say, at the quantum or Compton size limit), and so the limit may not be meaningful.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
290 CHAPTER 13
Figure 13.4 Examples of the use of box counting to determine fractal dimension. On the left the perimeter is being
covered, in the middle an entire figure is being covered, and on the right a “coastline” is being covered
by boxes of two different sizes (scales).
2r
0 40 80
0
40
100
13.5.2 Box Counting Algorithm
Consider a line of length L broken up into segments of length r (Figure 13.4 left). The number
of segments or “boxes” needed to cover the line is related to the size r of the box by
N(r) =
L
r
=
C
r
, (13.16)
where C is a constant. A proposed definition of fractional dimension is the power of r in this
expression as r → 0. In our example, it tells us that the line has dimension df = 1. If we
now ask how many little circles of radius r it would take to cover or fill a circle of area A
(Figure 13.4 middle), we will find that
N(r) = lim
r→0
A
πr2
⇒ df = 2, (13.17)
as expected. Likewise, counting the number of little spheres or cubes that can be packed within
a large sphere tells us that a sphere has dimension df = 3. In general, if it takesN little spheres
or cubes of side r → 0 to cover some object, then the fractal dimension df can be deduced as
N(r) = C
(
1
r
)df
= C ′ sdf (as r → 0), (13.18)
logN(r) = logC − df log(r) (as r → 0), (13.19)
⇒ df = − lim
r→0
∆N(r)
∆r
. (13.20)
Here s ∝ 1/r is called the scale in geography, so r → 0 corresponds to an infinite scale. To
illustrate, you may be familiar with the low scale on a map being 10,000 m to a centimeter,
while the high scale is 100 m to a centimeter. If we want the map to show small details (sizes),
we need a map of high scale.
We will use box counting to determine the dimension of a perimeter, not of an entire
figure. Once we have a value for df , we can determine a value for the length of the perimeter
via (13.15). (If you cannot wait to see box counting in action, on the DVD you will find an
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FRACTALS & STATISTICAL GROWTH 291
Figure 13.5 Dimensions of a line, box, and coastline determined by box counting.
square (m = 2.00)
coastline (m = 1.3)
straight line (m = 1.02)
log(scale)
lo
g
(N
u
m
b
e
r 
b
o
x
e
s
)
applet Jfracdim that goes through all the steps of box counting before your eyes and even plots
the results.)
13.5.3 Coastline Implementation and Exercise
Rather than ruin our eyes using a geographic map, we use a mathematical one. Specifically,
with a little imagination you will see that the top portion of Figure 13.3 looks like a natural
coastline. Determine df by covering this figure, or one you have generated, with a semitrans-
parent piece of graph paper1, and counting the number of boxes containing any part of the
coastline (Figures 13.4 and 13.5).
1. Print your coastline graph with the same physical scale (aspect ratio) for the vertical
and horizontal axes. This is required because the graph paper you will use for box
counting has square boxes and so you want your graph to also have the same vertical
and horizontal scales. Place a piece of graph paper over your printout and look though
the graph paper at your coastline. If you do not have a piece of graph paper available,
or if you are unable to obtain a printout with the same aspect ratio for the horizontal and
vertical axes, add a series of closely spaced horizontal and vertical lines to your coastline
printout and use these lines as your graph paper. (Box counting should still be accurate
if both your coastline and your graph paper are have the same aspect ratios.)
2. The vertical height in our printout was 17 cm, and the largest division on our graph paper
was 1 cm. This sets the scale of the graph as 1:17, or s = 17 for the largest divisions
(lowest scale). Measure the vertical height of your fractal, compare it to the size of the
biggest boxes on your “piece” of graph paper, and thus determine your lowest scale.
1Yes, we are suggesting a painfully analog technique based on the theory that trauma leaves a lasting impression. If you prefer,
you can store your output as a matrix of 1 and 0 values and let the computer do the counting, but this will take more of your time!
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
292 CHAPTER 13
3. With our largest boxes of 1 cm×1 cm, we found that the coastline passed throughN = 24
large boxes, that is, that 24 large boxes covered the coastline at s = 17. Determine how
many of the largest boxes (lowest scale) are needed to cover your coastline.
4. With our next smaller boxes of 0.5 cm × 0.5 cm, we found that 51 boxes covered the
coastline at a scale of s = 34. Determine how many of the midsize boxes (midrange
scale) are needed to cover your coastline.
5. With our smallest boxes of 1 mm×1 mm, we found that 406 boxes covered the coastline
at a scale of s = 170. Determine how many of the smallest boxes (highest scale) are
needed to cover your coastline.
6. Equation (13.20) tells us that as the box sizes get progressively smaller, we have
logN ' logA+ df log s,
⇒ df '
∆ logN
∆ log s
=
logN2 − logN1
log s2 − log s1
=
log(N2/N1)
log(s2/s1)
.
Clearly, only the relative scales matter because the proportionality constants cancel out
in the ratio. A plot of logN versus log s should yield a straight line. In our example we
found a slope of df = 1.23. Determine the slope and thus the fractal dimension for your
coastline. Although only two points are needed to determine the slope, use your lowest
scale point as an important check. (Because the fractal dimension is defined as a limit
for infinitesimal box sizes, the highest scale points are more significant.)
7. As given by (13.15), the perimeter of the coastline
L ∝ s1.23−1 = s0.23. (13.21)
If we keep making the boxes smaller and smaller so that we are looking at the coastline
at higher and higher scale and if the coastline is self-similar at all levels, then the scale
s will keep getting larger and larger with no limits (or at least until we get down to some
quantum limits). This means
L ∝ lim
s→∞
s0.23 =∞. (13.22)
Does your fractal imply an infinite coastline? Does it make sense that a small island like
Britain, which you can walk around, has an infinite perimeter?
13.6 CORRELATED GROWTH, FORESTS, FILMS (PROBLEM 5)
It is an empirical fact that in nature there is increased likelihood that a plant will grow if there
is another one nearby (Figure 13.6 left). This correlation is also valid for the “growing” of
surface films, as in the previous algorithm. Your problem is to include correlations in the
surface simulation.
13.6.1 Correlated Ballistic Deposition Algorithm
A variation of the ballistic deposition algorithm, known as the correlated ballistic deposition
algorithm, simulates mineral deposition onto substrates on which dendrites form [Tait 90]. We
extend the previous algorithm to include the likelihood that a freshly deposited particle will
attract another particle. We assume that the probability of sticking P depends on the distance
d that the added particle is from the last one (Figure 13.6 right):
P = c dη. (13.23)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FRACTALS & STATISTICAL GROWTH 293
Figure 13.6 Left: A view that might be seen in the undergrowth of a forest or a correlated ballistic deposition.
Right: The probability of particle i + 1 sticking in some column depends on the distance d from the
previously deposited particle i.
i
i+1d
Here η is a parameter and c is a constant that sets the probability scale.2 For our implementation
we choose η = −2, which means that there is an inverse square attraction between the particles
(decreased probability as they get farther apart).
As in our study of uncorrelated deposition, a uniform random number in the interval
[0, L] determines the column in which the particle will be deposited. We use the same rules
about the heights as before, but now a second random number is used in conjunction with
(13.23) to decide if the particle will stick. For instance, if the computed probability is 0.6 and
if r < 0.6, the particle will be accepted (sticks); if r > 0.6, the particle will be rejected.
13.7 GLOBULAR CLUSTER (PROBLEM 6)
Consider a bunch of grapes on an overhead vine. Your problem is to determine how its tanta-
lizing shape arises. In a flash of divine insight, you realize that these shapes, as well as others
such as those of dendrites, colloids, and thin-film structure, appear to arise from an aggregation
process that is limited by diffusion.
13.7.1 Diffusion-Limited Aggregation Algorithm
A model of diffusion-limited aggregation (DLA) has successfully explained the relation be-
tween a cluster’s perimeter and mass [W&S 83]. We start with a 2-D lattice containing a seed
particle in the middle, draw a circle around the particle, and place another particle on the cir-
cumference of the circle at some random angle. We then release the second particle and have it
execute a random walk, much like the one we studied in Chapter 5, “Monte Carlo Simulations,”
but restricted to vertical or horizontal jumps between lattice sites. This is a type of Brownian
motion that simulates diffusion. To make the model more realistic, we let the length of each
2The absolute probability, of course, must be less than one, but it is nice to choose c so that the relative probabilities produce a
graph with easily seen variations.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
294 CHAPTER 13
Listing 13.1 Column.py simulates correlated ballistic deposition of minerals onto substrates on which dendrites
form. 
by RH Landau , MJ Paez , and CC BORDEIANU
C o p y r i g h t P r i n c e t o n U n i v e r s i t y P r e s s , P r i n c e t o n , 2008 .
E l e c t r o n i c M a t e r i a l s c o p y r i g h t : R Landau , Oregon S t a t e Univ , 2008 ;
MJ Paez , Univ A n t i o q u i a , 2008 ; and CC BORDEIANU, Univ B u c h a r e s t , 2008 .
S u p p o r t by N a t i o n a l S c i e n c e F o u n d a t i o n """
# Column.py
from visual import *; import random
maxi = 100000; npoints = 200 # Number iterations, spaces
i = 0; dist = 0; r = 0; x = 0; y = 0
oldx = 0; oldy = 0; pp = 0.0; prob = 0.0
hit = zeros( (200), Int)
graph1 = display(width = 500, height = 500,
title = ’Correlated Ballistic Deposition’, range = 250.0,
background=color.white)
for i in range(0, npoints): hit[i] = 0 # Clear array
oldx = 100; oldy = 0
for i in range(1, maxi + 1):
r = int(npoints*random.random() )
x = r - oldx
y = hit[r] - oldy
dist = x*x + y*y
if (dist == 0): prob = 1.0 # Sticking prob depends on last x
else: prob = 9.0/dist
pp = random.random()
if (pp < prob):
if(r>0 and r<(npoints - 1) ):
if( (hit[r] >= hit[r - 1]) and (hit[r] >= hit[r + 1]) ):
hit[r] = hit[r] + 1
else:
if (hit[r - 1] > hit[r + 1]):
hit[r] = hit[r - 1]
else: hit[r] = hit[r + 1]
oldx = r
oldy = hit[r]
olxc = oldx*2 - 200 # linear transform 0<oldx<200 - > - 200<olxc<200
olyc = oldy*4 - 200 # linear transform 0<oldy<100 - > - 200<olxy<200
sphere(pos=(olxc,olyc), radius=1.5, color=color.green)
step vary according to a random Gaussian distribution. If at some point during its random walk,
the particle encounters another particle within one lattice spacing, they stick together and the
walk terminates. If the particle passes outside the circle from which it was released, it is lost
forever. The process is repeated as often as desired and results in clusters (Figure 13.7 and
applet dla).
1. Write a subroutine that generates random numbers with a Gaussian distribution.3
2. Define a 2-D lattice of points represented by the array grid[400][400] with all ele-
ments initially zero.
3. Place the seed at the center of the lattice; that is, set grid[199][199]=1.
4. Imagine a circle of radius 180 lattice spacings centered at grid[199][199]. This is the
circle from which we release particles.
5. Determine the angular position of the new particle on the circle’s circumference by
generating a uniform random angle between 0 and 2π.
6. Compute the x and y positions of the new particle on the circle.
7. Determine whether the particle moves horizontally or vertically by generating a uniform
random number 0 < rxy < 1 and applying the rule
if
{
rxy < 0.5, motion is vertical,
rxy > 0.5, motion is horizontal.
(13.24)
3We indicated how to do this in § 6.8.1.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FRACTALS & STATISTICAL GROWTH 295
Figure 13.7 Left: A globular cluster of particles of the type that might occur in a colloid. Right: The applet
Dla2en.html on the DVD lets you watch these clusters grow. Here the cluster is at the center of the
circle, and random walkers are started at random points around the circle.
8. Generate a Gaussian-weighted random number in the interval [−∞,∞]. This is the
size of the step, with the sign indicating direction.
9. We now know the total distance and direction the particle will move. It jumps one
lattice spacing at a time until this total distance is covered.
10. Before a jump, check whether a nearest-neighbor site is occupied:
a. If occupied, the particle stays at its present position and the walk is over.
b. If unoccupied, the particle jumps one lattice spacing.
c. Continue the checking and jumping until the total distance is covered, until the par-
ticle sticks, or until it leaves the circle.
11. Once one random walk is over, another particle can be released and the process re-
peated. This is how the cluster grows.
Because many particles are lost, you may need to generate hundreds of thousands of particles
to form a cluster of several hundred particles.
13.7.2 Fractal Analysis of DLA or Pollock
A cluster generated with the DLA technique is shown in Figure 13.7. We wish to analyze it
to see if the structure is a fractal and, if so, to determine its dimension. (As an alternative,
you may analyze the fractal nature of the Pollock painting in Figure 13.8, a technique used to
determine the authenticity of this sort of art.) As a control, simultaneously analyze a geometric
figure, such as a square or circle, whose dimension is known. The analysis is a variation of the
one used to determine the length of the coastline of Britain.
1. If you have not already done so, use the box counting method to determine the fractal
dimension of a simple square.
2. Draw a square of length L, small relative to the size of the cluster, around the seed
particle. (Small might be seven lattice spacings to a side.)
3. Count the number of particles within the square.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
296 CHAPTER 13
Figure 13.8 Number 8 by the American painter Jackson Pollock. (Used with permission, Neuberger Museum, State
University of New York.) It has been found that Pollock’s paintings exhibit a characteristic fractal
structure. See if you can determine the fractal dimensions within this painting.
4. Compute the density ρ by dividing the number of particles by the number of sites avail-
able in the box (49 in our example).
5. Repeat the procedure using larger and larger squares.
6. Stop when the cluster is covered.
7. The (box counting) fractal dimension df is estimated from a log-log plot of the density
ρ versus L. If the cluster is a fractal, then (13.2) tells us that ρ ∝ Ldf−2, and the graph
should be a straight line of slope df − 2.
The graph we generated had a slope of −0.36, which corresponds to a fractal dimension of
1.66. Because random numbers are involved, the graph you generate will be different, but
the fractal dimension should be similar. (Actually, the structure is multifractal, and so the
dimension varies with position.)
13.8 FRACTALS IN BIFURCATION PLOT (PROBLEM 7)
Recollect the project involving the logistics map where we plotted the values of the stable
population numbers versus the growth parameter µ. Take one of the bifurcation graphs you
produced and determine the fractal dimension of different parts of the graph by using the same
technique that was applied to the coastline of Britain.
13.9 FRACTALS FROM CELLULAR AUTOMATA
We have already indicated in places how statistical models may lead to fractals. There is a
class of statistical models known as cellular automata that produce complex behaviors from
very simple systems. Here we study some.
Cellular automata were developed by von Neumann and Ulam in the early 1940s (von
Neumann was also working on the theory behind modern computers then). Though very sim-
ple, cellular automata have found applications in many branches of science [Peit 94, Sipp 96].
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FRACTALS & STATISTICAL GROWTH 297
Their classic definition is [Barns 93]:
A cellular automaton is a discrete dynamical system in which space, time, and
the states of the system are discrete. Each point in a regular spatial lattice, called
a cell, can have any one of a finite number of states, and the states of the cells
in the lattice are updated according to a local rule. That is, the state of a cell at a
given time depends only on its won state one time step previously, and the states of
its nearby neighbors at the previous time step. All cells on the lattice are updated
synchronously, and so the state of the entice lattice advances in discrete time steps.
A cellular automaton in two dimensions consists of a number of square cells that grow upon
each other. A famous one, invented by Conway in the 1970s, is Conway’s Game of Life. In
this, cells with value 1 are alive, while cells with value 0 are dead. Cells grow according to the
following rules:
1. If a cell is alive and if two or three of its eight neighbors are alive, then the cell remains
alive.
2. If a cell is alive and if more than three of its eight neighbors are alive, then the cell dies
because of overcrowding.
3. If a cell is alive and only one of its eight neighbors is alive, then the cell dies of loneliness.
4. If a cell is dead and more than three of its neighbors are alive, then the cell revives.
A variation on the Game of Life is to include a “rule one out of eight” that a cell will be alive
if exactly one of its neighbors is alive, otherwise the cell will remain unchanged.
Listing 13.2 Gameoflife.py is an extension of Conway’s Game of Life in which cells always revive if one out of
eight neighbors is alive. 
# G a m e o f l i f e . py : C e l l u l a r a u t o m a t a i n 2 d i m e n s i o n s
’’’* Rules: a cell can be either dead (0) or alive (1)
* If a cell is alive:
* on next step will remain alive if
* 2 or 3 of its closer 8 neighbors are alive.
* If more than 3 of its 8 neighbors are alive the cell dies of overcrowdedness.
* If less than 2 neighbors are alive the cell dies of loneliness
* A dead cell will be alive if 3 of its 8 neighbors are alive’’’
from v i s u a l . g raph i m p o r t ∗ ; i m p o r t random
s c e n e = d i s p l a y ( wid th = 500 , h e i g h t = 500 , t i t l e = ’Game of Life’ ) # 1 s t d i s t r i b u t i o n
c e l l = z e r o s ( ( 5 0 , 5 0 ) ) ; c e l l u = z e r o s ( ( 5 0 , 5 0 ) )
d e f d r a w c e l l s ( ce ) :
f o r o b j i n s c e n e . o b j e c t s :
o b j . v i s i b l e = 0 # e r a s e p r e v i o u s c e l l s
c u r v e ( pos= [(−49 ,−49) , (−49 ,49) , ( 4 9 , 4 9 ) , (49 ,−49) ,(−49 ,−49) ] , c o l o r = c o l o r . w h i t e )
f o r j i n r a n g e ( 0 , 5 0 ) :
f o r i i n r a n g e ( 0 , 5 0 ) :
i f ce [ i , j ]== 1 :
xx= 2∗ i −50
yy= 2∗ j−50
box ( pos =( xx , yy , 0 ) , a x i s = ( 1 , 0 , 0 ) , l e n g t h = 2 , wid th =0 , h e i g h t = 2 , c o l o r = c o l o r . cyan )
d e f i n i t i a l ( ) :
f o r j i n r a n g e ( 2 0 , 2 8 ) : # i n i t i a l s t a t e o f c e l l s
f o r i i n r a n g e ( 2 0 , 28) :
r = i n t ( random . random ( ) ∗2)
c e l l [ j , i ] = r # dead or a l i v e a t random
r e t u r n c e l l
d e f g a m e o f l i f e ( c e l l ) : # r u l e s and a n a l y s i s o f n e i g h b o r s
f o r i i n r a n g e ( 1 , 4 9 ) : # o b s e r v e 8 n e i g h b o r s o f c e l l [ i , j ]
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
298 CHAPTER 13
Figure 13.9 The rules for two versions of the Game of Life, given graphically on the top row, create the gaskets
below. (Output obtained from the applet JCellAut on the DVD.)
f o r j i n r a n g e ( 1 , 4 9 ) :
# a l i v e = 0 # sum 8 n e i g h b o r s ( some a r e z e r o )
sum1= c e l l [ i−1, j−1]+ c e l l [ i , j−1]+ c e l l [ i +1 , j−1]
sum2= c e l l [ i−1, j ]+ c e l l [ i +1 , j ]+ c e l l [ i−1, j +1]+ c e l l [ i , j +1]+ c e l l [ i +1 , j +1]
a l i v e = sum1+sum2
i f c e l l [ i , j ]== 1 :
i f a l i v e == 2 or a l i v e == 3 : # r e m a i n s a l i v e
c e l l u [ i , j ]= 1 # l i v e s
i f a l i v e > 3 or a l i v e <2: # overc rowded or s o l i t u d e
c e l l u [ i , j ]= 0 # d i e s
i f c e l l [ i , j ]== 0 :
i f a l i v e == 3 :
c e l l u [ i , j ]= 1 # r e v i v e s
e l s e :
c e l l u [ i , j ]= 0 # r e m a i n s dead
a l i v e = 0
r e t u r n c e l l u
temp= i n i t i a l ( )
d r a w c e l l s ( temp )
w h i l e 1 :
r a t e ( 6 )
c e l l = temp
temp= g a m e o f l i f e ( c e l l )
d r a w c e l l s ( c e l l )
In 1983 Wolfram developed the statistical mechanics of cellular automata and indicated
how one can be used to generate a Sierpiński gasket [Wolf 83]. Since we have already seen
that a Sierpiński gasket exhibits fractal geometry (§13.2), this represents a microscopic model
of how fractals may occur in nature. This model uses eight rules, given graphically at theop of
t
Figure 13.9, to generate new cells from old. We see all possible configurations for three cells
in the top row, and the begetted next generation in the row below. At the bottom of Figure 13.9
is a Sierpiński gasket of the type created by the applet JCellAut on the DVD (under Applets).
This plays the game and lets you watch and control the growth of the gasket.
13.10 PERLIN NOISE ADDS REALISM 
We have already seen in this chapter how statistical fractals are able to generate objects with a
striking resemblance to those in nature. This appearance of realism may be further enhanced
by including a type of coherent randomness known as Perlin noise. The resulting textures
so resemble those of clouds, smoke, and fire that one cannot help but wonder if a similar
mechanism might also be occurring in nature. The technique we are about to discuss was
developed by Ken Perlin of New York University, who won an Academy Award (an Oscar) in
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FRACTALS & STATISTICAL GROWTH 299
Figure 13.10 Left: The rectangular grid used to locate a square in space and a corresponding point within the
square. As shown with the arrows, unit vectors gi with random orientation are assigned at each grid
point. Right: A point within each square is located by drawing the four pi. The gi vectors are the
same as on the left.
(x  , y  )
(x, y)
(x  , y  )
(x  , y  )
(x  , y  )
0
00
0
1
1
1 1
(x, y)
(x  , y  )00
0
0
(x  , y  )
0 1
(x  , y  )
01
(x  , y  )1 1
g
g
g
g
1
1
2
2
3
3
p p
pp
1997 for it and has continued to improve it [Perlin]. This type of coherent noise has found use
in important physics simulations of stochastic media [Tick 04], as well as in video games and
motion pictures (Tron).
The inclusion of Perlin noise in a simulation adds both randomness and a type of coher-
ence among points in space that tends to make dense regions denser and sparse regions sparser.
This is similar to our correlated ballistic deposition simulations (§13.6.1) and related to chaos
in its long-range randomness and short-range correlations. We start with some known function
of x and y and add noise to it. For this purpose Perlin used the mapping or ease function
(Figure 13.11 right)
f(p) = 3p2 − 2p3. (13.25)
As a consequence of its S shape, this mapping makes regions close to 0 even closer to 0, while
making regions close to 1 even closer (in other words, it increases the tendency to clump,
which shows up as higher contrast). We then break space up into a uniform rectangular grid
of points (Figure 13.10 left) and consider a point (x, y) within a square with vertices (x0, y0),
(x1, y0), (x0, y1), and (x1, y1). We next assign unit gradients vectors g0–g3 with random
orientation at each grid point. A point within each square is located by drawing the four pi
vectors (Figure13.10 right):
p0 = (x− x0)i + (y − y0)j, p1 = (x− x1)i + (y − y0)j, (13.26)
p2 = (x− x1)i + (y − y1)j, p3 = (x− x0)i + (y − y1)j. (13.27)
Next the scalar products of the p′s and the g′s are formed:
s = p0 · g0, t = p1 · g1, v = p2 · g2, u = p3 · g3. (13.28)
As shown on the left in Figure 13.11, the numbers s, t, u, and v are assigned to the four
vertices of the square and represented there by lines perpendicular to the square with lengths
proportional to the values of s, t, u, and v (which can be positive or negative).
The actual mapping proceeds via a number of steps (Figure 13.12):
1. Transform the point (x, y) to (sx, sy),
sx = 3x2 − 2x3, sy = 3y2 − 2y3. (13.29)
2. Assign the lengths s, t, u, and v to the vertices in the mapped square.
3. Obtain the height a (Figure 13.12) via linear interpolation between s and t.
4. Obtain the height b via linear interpolation between u and v.
5. Obtain sy as a linear interpolation between a and b.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
300 CHAPTER 13
Figure 13.11 Left: The numbers s, t, u, and v are represented by perpendiculars to the four vertices, with lengths
proportional to their values. Right: The function 3p2 − 2p3 is used as a map of the noise at a point
like (x,y) to others close by.
(x
,
y
)
(x
, 
y
)
0
0
(x
, 
y
)
0
1
(x
, 
y
)
0
1
(x
, 
y
)
1
1
s
t
u
v
0.2
0.2
0.4
0.4
0.6
0.6
0.8
0.8
1
1
3p 2p2 3
p
Figure 13.12 Left: The point (x, y) is mapped to point (sx, xy). Right: Using (13.29). Then three linear interpolations
are performed to find c, the noise at (x, y).
(x
, y
)
(x
  ,
 y
  )0
0
(x
  ,
 y
  )
0
1 (x
  ,
 y
  )0
1
(x
  ,
 y
  )
1
1
s
t
u v
s
t
u v
s 
 ,
s
( 
   
   
 )
x
y
a
b
noise
cc
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FRACTALS & STATISTICAL GROWTH 301
Figure 13.13 After the addition of Perlin noise, the random scatterplot on the left becomes the clusters on the right.
y
x
6. The vector c so obtained is now the two components of the noise at (x, y).
Perlin’s original C code to accomplish this mapping (along with other goodies) is found
in [Perlin]. It takes as input the plot of random points (r2i, r2i+1) on the left in Figure 13.13
(which is the same as Figure 5.1) and by adding coherent noise produces the image on the
right in Figure 13.13. The changes we made from the original program are (1) including an
int before the variables p[], start, i, and j, and (2) adding # include <time.h> and the line
srand(time(NULL)); at the beginning of method init( ) in order to obtain different random num-
bers each time the program runs. The main method of the C program we used is below. The
program outputs a data file that we visualized with OpenDX to produce the image montania.tiff
on the right in Figure 13.13.
13.10.1 Including Ray Tracing
Ray tracing is a technique that renders an image of a scene by simulating the way rays of
light actually travel [Pov-Ray]. To avoid tracing rays that do not contribute to the final image,
ray-tracing programs start at the viewer, trace rays backward onto the scene, and then back
again onto the light sources. You can vary the location of the viewer and light sources and the
properties of the objects being viewed, as well as atmospheric conditions such as fog, haze,
and fire.
As an example of what can be done, on the left in Figure 13.14 we show the output from
the ray-tracing program Pov-Ray [Pov-Ray], using as input the coherent random noise on the
right in Figure 13.13. The program options we used are given in Listing 13.3 and are seen to
include commands to color the islands, to include waves, and to give textures to the sky and
the sea. Pov-Ray also allows the possibility of using Perlin noise to give textures to the objects
to be created. For example, the stone cup on the right in Figure 13.14 has a marblelike texture
produced by Perlin noise. 
main ( ) {
i n t m, n ;
f l o a t hol , x , ym , ve [ 2 ] ;
FILE ∗pf ;
y = 0 . 2 ;
p f = fopen ("mountain1.dat" , "w" ) ;
f o r ( n =0; n<50; n++ ) {
x = 0 . 1 ;
ve [ 1 ] = y∗n ;
f o r ( m=0; m<50; m++ ) {
ve [ 0 ] = x∗m;
/ / Coordinates p o i n t between 0 1
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
302 CHAPTER 13
Figure 13.14 Left: The output from the Pov-Ray ray-tracing program that took as input the 2-D coherent random
noise plot in Figure 13.13 and added height and fog. Right: An image of a surface of revolution
produced by Pov-Ray in which the marblelike texture is created by Perlin noise.
h o l = n o i s e 2 ( ve ) ;
f p r i n t f ( pf , "%f\n" , h o l ) ;
} }
f c l o s e ( p f ) ;
}
Listing 13.3 Islands.pov in the Codes/Animations/Fractals/ directory gives the Pov-Ray ray-tracing
commands needed to convert the coherent noise random plot of Figure 13.13 into the mountainlike
image on the left in Figure 13.14. 
/ / I s l a n d s . pov Pov−Ray program to c r e a t e I s l a n d s , by Manuel J Paez
p l a n e {
<0, 1 , 0>, 0 / / Sky
pigment { c o l o r rgb <0, 0 , 1> }
s c a l e 1
r o t a t e <0, 0 , 0>
t r a n s l a t e y∗0 .2
}
g l o b a l s e t t i n g s {
a d c b a i l o u t 0 .00392157
assumed gamma 1 . 5
n o i s e g e n e r a t o r 2
}
# d e c l a r e I s l a n d t e x t u r e = t e x t u r e {
pigment {
g r a d i e n t <0, 1 , 0> / / V e r t i c a l d i r e c t i o n
c o l o r m a p { / / Color the i s l a n d s
[ 0 . 1 5 c o l o r rgb <1, 0 . 9 6 8 6 2 7 , 0> ]
[ 0 . 2 c o l o r rgb <0.886275 , 0 . 7 3 3 3 3 3 , 0.180392> ]
[ 0 . 3 c o l o r rgb <0.372549 , 0 . 6 4 3 1 3 7 , 0.0823529> ]
[ 0 . 4 c o l o r rgb <0.101961 , 0 . 5 8 8 2 3 5 , 0.184314> ]
[ 0 . 5 c o l o r rgb <0.223529 , 0 . 6 6 6 6 6 7 , 0.301961> ]
[ 0 . 6 c o l o r rgb <0.611765 , 0 . 8 8 6 2 7 5 , 0.0196078> ]
[ 0 . 6 9 c o l o r rgb <0.678431 , 0 . 9 2 1 5 6 9 , 0.0117647> ]
[ 0 . 7 4 c o l o r rgb <0.886275 , 0 . 8 8 6 2 7 5 , 0.317647> ]
[ 0 . 8 6 c o l o r rgb <0.823529 , 0 . 7 9 6 0 7 8 , 0.0196078> ]
[ 0 . 9 3 c o l o r rgb <0.905882 , 0 . 5 4 5 0 9 8 , 0.00392157 > ]
}
}
f i n i s h {
ambien t r g b f t <0.2 , 0 . 2 , 0 . 2 , 0 . 2 , 0.2>
d i f f u s e 0 . 8
}
}
camera { / / Camera c h a r a c t e r i s t i c s and l o c a t i o n
p e r s p e c t i v e
l o c a t i o n <−15, 6 , −20> / / Located here
sky <0, 1 , 0>
d i r e c t i o n <0, 0 , 1>
r i g h t <1.3333 , 0 , 0>
up <0, 1 , 0>
l o o k a t <−0.5, 0 , 4> / / l o o k i n g at t h a t p o i n t
a n g l e 36
}
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
FRACTALS & STATISTICAL GROWTH 303
l i g h t s o u r c e {<−10, 20 , −25>, rgb <1, 0 . 7 3 3 3 3 3 , 0.00392157>} / / Light
# d e c l a r e I s l a n d s = h e i g h t f i e l d { / / Takes g i f and f i n d s h e i g h t s
g i f "d:\pov\montania.gif" / / Windows d i r e c t o r y naming
s c a l e <50, 2 , 50>
t r a n s l a t e <−25, 0 , −25>
}
o b j e c t { / / I s l a n d s
I s l a n d s
t e x t u r e {
I s l a n d t e x t u r e
s c a l e 2
}
}
box { / / Upper f a c e o f the box i s the sea
<−50, 0 , −50>, <50, 0 . 3 , 50> / / Locat ion of 2 o p p o s i t e v e r t i c e s
t r a n s l a t e <−25, 0 , −25>
t e x t u r e { / / S imulate waves
normal {
s p o t t e d
0 . 4
s c a l e <0.1 , 1 , 0.1>
}
pigment { c o l o r rgb <0.164706 , 0 . 5 5 6 8 6 3 , 0.901961> }
}
}
fog { / / A c o n s t a n t fog i s d e f i n e d
f o g t y p e 1
d i s t a n c e 30
rgb <0.984314 , 1 , 0.964706>
}
13.11 QUIZ
1. Recall how box counting is used to determine the fractal dimension of an object. Imagine
that the result of some experiment or simulation is an interesting geometric figure.
a. What might be the physical/theoretical importance of determining that this object is a
fractal?
b. What might be the importance of determining its fractal dimension?
c. Why is it important to use more than two sizes of boxes?
d. Below is a figure composed of boxes of side 1.
1 1 1
1 1 1
1 1 1
Use box counting to determine the fractal dimension of this figure.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Fourteen
High-Performance Computing Hardware,
Tuning, and Parallel Computing
Hardware—( In this chapter and in D we discuss a number of topics associated with high-
performance computing (HPC). If history can be our guide, today’s HPC hardware and soft-
ware will be on desktop machines a decade from now. If history In Unit I we discuss the
theory of a high-performance computer’s memory and central processor design. In Unit II
we examine parallel computers. In D we extend Unit II by giving a detailed tutorial on
use of the message-passing interface (MPI) package, while a tutorial on an earlier package,
Parallel virtual machine (PVM), is provided on the CD. In Unit III we discuss some tech-
niques for writing programs that are optimized for HPC hardware, such as virtual mem-
ory and cache. By running the short implementations given in Unit III, you will experi-
ment with your computer’s memory and experience some of the concerns, techniques, re-
wards, and shortcomings of HPC. HPC is a broad subject, and our presentation is brief and
given from a practitioner’s point of view. The text [Quinn 04] surveys parallel computing
and MPI from a computer science point of view. References on parallel computing include
[Sterl 99, Quinn 04, Pan 96, VdeV 94, Fox 94]. References on MPI include Web resources
[MPI, MPI2, MPImis] and the texts [Quinn 04, Pach 97, Lusk 99]. More recent developments,
such as programming for multicore computers, cell computers, and field-programmable gate
accelerators, will be discussed in future books.
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
High Performance Computing 14.1–14.4 HPC Hardware II 14.4–14.6, 14.13
HPC Exercises 14.14–14.15 HPC Exercises II 14.14–14.15
14.1 UNIT I. HIGH-PERFORMANCE COMPUTERS (CS)
By definition, supercomputers are the fastest and most powerful computers available, and at
this instant, the term “supercomputers” almost always refers to parallel machines. They are
the superstars of the high-performance class of computers. Unix workstations and modern
personal computers (PCs), which are small enough in size and cost to be used by a small group
or an individual, yet powerful enough for large-scale scientific and engineering applications,
can also be high-performance computers. We define high-performance computers as machines
with a good balance among the following major elements:
• Multistaged (pipelined) functional units.
• Multiple central processing units (CPUs) (parallel machines).
• Multiple cores.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 305
Figure 14.1 The logical arrangement of the CPU and memory showing a Fortran array A(N) and matrix M(N, N)
loaded into memory.
CPU
A(1)
A(2)
A(3)
A(N)
M(1,1)
M(2,1)
M(3,1)
M(N,N)
M(N,1)
M(1,2)
M(2,2)
M(3,2)
Figure 14.2 The elements of a computer’s memory architecture in the process of handling matrix storage.
Swap Space
Page N
RAM
Data Cache
Registers
CPU
A(2048)A(2032)   ...A(16)A(1)       ...
Page 1
Page 2
Page 3
A(1)
A(2)
A(3)
A(N)
M(2,1)
M(1,1)
M(N,1)
M(3,1)
M(1,2)
M(2,2)
M(3,2)
M(N,N)
• Fast central registers.
• Very large, fast memories.
• Very fast communication among functional units.
• Vector, video, or array processors.
• Software that integrates the above effectively.
As a simple example, it makes little sense to have a CPU of incredibly high speed coupled to a
memory system and software that cannot keep up with it (the present state of affairs).
14.2 MEMORY HIERARCHY
An idealized model of computer architecture is a CPU .” sequentially executing a stream
of instructions and reading from a continuous block of memory. To illustrate, in Figure 14.1
we see a vector A[ ] and an array M[ ][ ] loaded in memory and about to be processed. The
real world is more complicated than this. First, matrices are not stored in blocks but rather in
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
306 CHAPTER 14
Figure 14.3 Typical memory hierarchy for a single-processor, high-performance computer (B = bytes, K, M, G, T
= kilo, mega, giga, tera).
Main Store
cache
cache
RAM
CPU
32
T
B
@
11
1M
b/
s
2
M
B
2
G
B
32
K
B
6
G
B
/s
linear order. For instance, in Fortran it is in column-major order:
M(1,1) M(2,1) M(3,1) M(1,2) M(2,2) M(3,2) M(1,3) M(2,3) M(3,3),
while in Python, Java and C it is in row-major order:
M(0,0) M(0,1) M(0,2) M(1,0) M(1,1) M(1,2) M(2,0) M(2,1) M(2,2).
Second, the values for the matrix elements may not even be in the same physical place. Some
may be in RAM, some on the disk, some in cache, and some in the CPU. To give some of these
words more meaning, in Figures 14.2 and 14.3 we show simple models of the memory archi-
tecture of a high-performance computer. This hierarchical arrangement arises from an effort
to balance speed and cost with fast, expensive memory supplemented by slow, less expensive
memory. The memory architecture may include the following elements:
CPU: Central processing unit, the fastest part of the computer. The CPU consists of a num-
ber of very-high-speed memory units called registers containing the instructions
sent to the hardware to do things like fetch, store, and operate on data. There are usually
separate registers for instructions, addresses, and operands (current data). In many cases
the CPU also contains some specialized parts for accelerating the processing of floating-
point numbers.
Cache (high-speed buffer): A small, very fast bit of memory that holds instructions, ad-
dresses, and data in their passage between the very fast CPU registers and the slower
RAM. This is seen in the next level down the pyramid in Figure 14.3. The main memory
is also called dynamic RAM (DRAM), while the cache is called static RAM
(SRAM). If the cache is used properly, it eliminates the need for the CPU to wait for data
to be fetched from memory.
Cache and data lines: The data transferred to and from the cache or CPU are grouped into
cache lines or data lines. The time it takes to bring data from memory into the cache is
called latency.
RAM: Random-access memory or central memory is in the middle memory in the hierarchy
in Figure 14.3. RAM can be accessed directly, that is, in random order, and it can be
accessed quickly, that is, without mechanical devices. It is where your program resides
while it is being processed.
Pages: Central memory is organized into pages, .” which are blocks of memory of fixed
length. The operating system labels and organizes its memory pages much like we do the
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 307
Figure 14.4 Multitasking of four programs in memory at one time in which the programs are executed in round-
robin order.
D
C
A
B
pages of a book; they are numbered and kept track of with a table of contents. Typical
page sizes are from 4 to 16 KB.
Hard disk: Finally, at the bottom of the memory pyramid is permanent storage on magnetic
disks or optical devices. Although disks are very slow compared to RAM, they can store
vast amounts of data and sometimes compensate for their slower speeds by using a cache
of their own, the paging storage controller.
Virtual memory: True to its name, this is a part of memory you will not find in our figures
because it is virtual. It acts like RAM but resides on the disk.
When we speak of fast and slow memory we are using a time scale set by the clock in the CPU.
To be specific, if your computer has a clock speed or cycle time of 1 ns, this means that it could
perform a billion operations per second if it could get its hands on the needed data quickly
enough (typically, more than 10 cycles are needed to execute a single instruction). While it
usually takes 1 cycle to transfer data from the cache to the CPU, the other memories are much
slower, and so you can speed your program up by not having the CPU wait for transfers among
different levels of memory. Compilers try to do this for you, but their success is affected by
your programming style.
As shown in Figure 14.2 for our example, virtual memory permits your program to use
more pages of memory than can physically fit into RAM at one time. A combination of operat-
ing system and hardware maps this virtual memory into pages with typical lengths of 4–16 KB.
Pages not currently in use are stored in the slower memory on the hard disk and brought into
fast memory only when needed. The separate memory location for this switching is known as
swap space (Figure 14.2). Observe that when the application accesses the memory location for
M[i][j], the number of the page of memory holding this address is determined by the com-
puter, and the location of M[i][j] within this page is also determined. A page fault occurs if
the needed page resides on disk rather than in RAM. In this case the entire page must be read
into memory while the least recently used page in RAM is swapped onto the disk. Thanks to
virtual memory, it is possible to run programs on small computers that otherwise would re-
quire larger machines (or extensive reprogramming). The price you pay for virtual memory
is an order-of-magnitude slowdown of your program’s speed when virtual memory is actually
invoked. But this may be cheap compared to the time you would have to spend to rewrite your
program so it fits into RAM or the money you would have to spend to buy a computer with
enough RAM for your problem.
Virtual memory also allows multitasking, .” the simultaneous loading into memory
of more programs than can physically fit into RAM (Figure 14.4). Although the ensuing switch-
ing among applications uses computing cycles, by avoiding long waits while an application is
loaded into memory, multitasking increases the total throughout and permits an improved com-
puting environment for users. For example, it is multitasking that permits a windows system to
provide us with multiple windows. Even though each window application uses a fair amount of
memory, only the single application currently receiving input must actually reside in memory;
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
308 CHAPTER 14
Table 14.1 Computation of c = (a+ b)/(d ∗ f)
Arithmetic Unit Step 1 Step 2 Step 3 Step 4
A1 Fetch a Fetch b Add —
A2 Fetch d Fetch f Multiply —
A3 — — — Divide
the rest are paged out to disk. This explains why you may notice a slight delay when switching
to an idle window; the pages for the now active program are being placed into RAM and the
least used application still in memory is simultaneously being paged out.
14.3 THE CENTRAL PROCESSING UNIT
How does the CPU get to be so fast? Often, it employs prefetching and pipelining; that
is, it has the ability to prepare for the next instruction before the current one has finished.
It is like an assembly line or a bucket brigade in which the person filling the buckets at one
end of the line does not wait for each bucket to arrive at the other end before filling another
bucket. In the same way a processor fetches, reads, and decodes an instruction while another
instruction is executing. Consequently, even though it may take more than one cycle to perform
some operations, it is possible for data to be entering and leaving the CPU on each cycle.
To illustrate, Table 14.1 indicates how the operation c = (a + b)/(d ∗ f) is handled. Here
the pipelined arithmetic units A1 and A2 are simultaneously doing their jobs of fetching and
operating on operands, yet arithmetic unit A3 must wait for the first two units to complete their
tasks before it has something to do (during which time the other two sit idle).
14.4 CPU DESIGN: REDUCED INSTRUCTION SET COMPUTER
Reduced instruction set computer (RISC) architecture (also called superscalar) is a design
philosophy for CPUs developed for high-performance computers and now used broadly. It
increases the arithmetic speed of the CPU by decreasing the number of instructions the CPU
must follow. To understand RISC we contrast it with complex instruction set computer (CISC),
architecture. In the late 1970s, processor designers began to take advantage of very-large-scale
integration (VLSI) which allowed the placement of hundreds of thousands of devices on a
single CPU chip. Much of the space on these early chips was dedicated to microcode programs
written by chip designers and containing machine language instructions that set the operating
characteristics of the computer. There were more than 1000 instructions available, and many
were similar to higher-level programming languages like Pascal and Forth. The price paid
for the large number of complex instructions was slow speed, with a typical instruction taking
more than 10 clock cycles. Furthermore, a 1975 study by Alexander and Wortman of the XLP
compiler of the IBM System/360 showed that about 30 low-level instructions accounted for
99% of the use with only 10 of these instructions accounting for 80% of the use.
The RISC philosophy is to have just a small number of instructions available at the chip
level but to have the regular programmer’s high level-language, such as Fortran or C, translate
them into efficient machine instructions for a particular computer’s architecture. This simpler
scheme is cheaper to design and produce, lets the processor run faster, and uses the space
saved on the chip by cutting down on microcode to increase arithmetic power. Specifically,
RISC increases the number of internal CPU registers, thus making it possible to obtain longer
pipelines (cache) for the data flow, a significantly lower probability of memory conflict, and
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 309
Figure 14.5 Left: A generic view of the Intel core-2 dual-core processor, with CPU-local level-1 caches and a
shared, on-die level-2 cache (courtesy of D. Schmitz). Right: The AMD Athlon 64 X2 3600 dual-core
CPU (Wikimedia Commons).
CPU Core
and
L1 Caches
CPU Core
and
L1 Caches
Bus Interface
and
L2 Caches
Dual CPU Core Chip
some instruction-level parallelism.
The theory behind this philosophy for RISC design is the simple equation describing the
execution time of a program:
CPU time = no.instructions× cycles/instruction× cycle time. (14.1)
Here “CPU time” is the time required by a program, “no. instructions” is the total number
of machine-level instructions the program requires (sometimes called the path length), “cy-
cles/instruction” is the number of CPU clock cycles each instruction requires, and “cycle time”
is the actual time it takes for one CPU cycle. After viewing (14.1) we can understand the CISC
philosophy, which tries to reduce CPU time by reducing no. instructions, as well as the RISC
philosophy, which tries to reduce CPU time by reducing cycles/instruction (preferably to one).
For RISC to achieve an increase in performance requires a greater decrease in cycle time and
cycles/instruction than the increase in the number of instructions.
In summary, the elements of RISC are the following:
Single-cycle execution for most machine-level instructions.
Small instruction set of less than 100 instructions.
Register-based instructions operating on values in registers, with memory access confined
to load and store to and from registers.
Many registers, usually more than 32.
Pipelining, that is, concurrent processing of several instructions.
High-level compilers to improve performance.
14.5 CPU DESIGN: MULTIPLE-CORE PROCESSORS
The year preceding the publication of this book has seen a rapid increase in the inclusion of
dual-core, or even quad-core, chips as the computational engine of computers. As seen in
Figure 14.5, a dual-core chip has two CPUs in one integrated circuit with a shared interconnect
and a shared level-2 cache. This type of configuration with two or more identical processors
connected to a single shared main memory is called symmetric multiprocessing, or SMP.. It is
likely that by the time you read this book, 16-core or greater chips will be available.
Although multicore chips were designed for game playing and single precision, they
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
310 CHAPTER 14
Table 14.2 Computation of Matrix [C] = [A] + [B]
Step 1 Step 2 · · · Step 99
c(1) = a(1) + b(1) c(2) = a(2) + b(2) · · · c(99) = a(99) + b(99)
should also be useful in scientific computing if new tools, algorithms, and programming meth-
ods are employed. These chips attain more speed with less heat and more energy efficiency
than single-core chips, whose heat generation limits them to clock speeds of less than 4 GHz.
In contrast to multiple single-core chips, multicore chips use fewer transistors per CPU and are
thus simpler to make and cooler to run.
Parallelism is built into a multicore chip because each core can run a different task.
However, since the cores usually share the same communication channel and level-2 cache,
there is the possibility of a communication bottleneck if both CPUs use the bus at the same
time. Usually the user need not worry about this, but the writers of compilers and software
must so that your code will run in parallel. As indicated in our MPI tutorial in D, modern Intel
compilers make use of each multiple core and even have MPI treat each core as a separate
processor.
14.6 CPU DESIGN: VECTOR PROCESSOR
Often the most demanding part of a scientific computation involves matrix operations. On a
classic (von Neumann) scalar computer, the addition of two vectors of physical length 99 to
form a third ultimately requires 99 sequential additions (Table 14.2). There is actually much
behind-the-scenes work here. For each element i there is the fetch of a(i) from its location
in memory, the fetch of b(i) from its location in memory, the addition of the numerical values
of these two elements in a CPU register, and the storage in memory of the sum in c(i). This
fetching uses up time and is wasteful in the sense that the computer is being told again and
again to do the same thing.
When we speak of a computer doing vector processing, we mean that there are
hardware components that perform mathematical operations on entire rows or columns of ma-
trices as opposed to individual elements. (This hardware can also handle single-subscripted
matrices, that is, mathematical vectors.) In the vector processing of [A] + [B] = [C], the suc-
cessive fetching of and addition of the elements A and B are grouped together and overlaid,
and Z ' 64–256 elements (the section size) are processed with one command, as seen in Ta-
ble 14.3. Depending on the array size, this method may speed up the processing of vectors by
a factor of about 10. If all Z elements were truly processed in the same step, then the speedup
would be ∼ 64–256.
Vector processing probably had its heyday during the time when computer manufacturers
produced large mainframe computers designed for the scientific and military communities.
These computers had proprietary hardware and software and were often so expensive that only
corporate or military laboratories could afford them. While the Unix and then PC revolutions
have nearly eliminated these large vector machines, some do exist, as well as PCs that use
vector processing in their video cards. Who is to say what the future holds in store?
14.7 UNIT II. PARALLEL COMPUTING
There is little question that advances in the hardware for parallel computing are impressive.
Unfortunately, the software that accompanies the hardware often seems stuck in the 1960s. In
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 311
Table 14.3 Vector Processing of Matrix [A] + [B] = [C]
Step 1 Step 2 Step 3 · · · Step Z
c(1) = a(1) + b(1)
c(2) = a(2) + b(2)
c(3) = a(3) + b(3)
· · ·
c(Z) = a(Z) + b(Z)
our view, message passing has too many details for application scientists to worry about and
requires coding at a much, or more, elementary level than we prefer. However, the increasing
occurrence of clusters in which the nodes are symmetric multiprocessors has led to the de-
velopment of sophisticated compilers that follow simpler programming models; for example,
partitioned global address space compilers such as Co-Array Fortran, Unified Parallel C, and
Titanium. In these approaches the programmer views a global array of data and then manip-
ulates these data as if they were contiguous. Of course the data really are distributed, but the
software takes care of that outside the programmer’s view. Although the program may not
be as efficient a use of the processors as hand coding, it is a lot easier, and as the number of
processors becomes very large, one can live with a greater degree of inefficiency. In any case,
if each node of the computer has a number of processors with a shared memory and there are a
number of nodes, then some type of a hybrid programming model will be needed.
Problem: Start with the program you wrote to generate the bifurcation plot for bug dynamics
in Chapter 12, “Discrete & Continuous Nonlinear Dynamics,” and modify it so that different
ranges for the growth parameter µ are computed simultaneously on multiple CPUs. Although
this small a problem is not worth investing your time in to obtain a shorter turnaround time,
it is worth investing your time into it gain some experience in parallel computing. In general,
parallel computing holds the promise of permitting you to obtain faster results, to solve big-
ger problems, to run simulations at finer resolutions, or to model physical phenomena more
realistically; but it takes some work to accomplish this.
14.8 PARALLEL SEMANTICS (THEORY)
We saw earlier that many of the tasks undertaken by a high-performance computer are run in
parallel by making use of internal structures such as pipelined and segmented CPUs, hierarchi-
cal memory, and separate I/O processors. While these tasks are run “in parallel,” the modern
use of parallel computing or parallelism denotes applying multiple processors to a sin-
gle problem [Quinn 04]. It is a computing environment in which some number of CPUs are
running asynchronously and communicating with each other in order to exchange intermediate
results and coordinate their activities.
For instance, consider matrix multiplication in terms of its elements:
[B] = [A][B] ⇒ Bi,j =
N∑
k=1
Ai,kBk,j . (14.2)
Because the computation of Bi,j for particular values of i and j is independent of the compu-
tation of all the other values, each Bi,j can be computed in parallel, or each row or column
of [B] can be computed in parallel. However, because Bk,j on the RHS of (14.2) must be
the “old” values that existed before the matrix multiplication, some communication among the
parallel processors is required to ensure that they do not store the “new” values of Bk,j before
all the multiplications are complete. This [B] = [A][B] multiplication is an example of data
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
312 CHAPTER 14
dependency, in which the data elements used in the computation depend on the order in which
they are used. In contrast, the matrix multiplication [C] = [A][B] is a data parallel operation
in which the data can be used in any order. So already we see the importance of communica-
tion, synchronization, and understanding of the mathematics behind an algorithm for parallel
computation.
The processors in a parallel computer are placed at the nodes of a communication net-
work. Each node may contain one CPU or a small number of CPUs, and the communication
network may be internal to or external to the computer. One way of categorizing parallel com-
puters is by the approach they employ in handling instructions and data. From this viewpoint
there are three types of machines:
• Single-instruction, single-data (SISD): These are the classic (von Neumann) serial
computers executing a single instruction on a single data stream before the next instruction
and next data stream are encountered.
• Single-instruction, multiple-data (SIMD): Here instructions are processed from a
single stream, but the instructions act concurrently on multiple data elements. Generally
the nodes are simple and relatively slow but are large in number.
• Multiple instructions, multiple data (MIMD): In this category each processor
runs independently of the others with independent instructions and data. These are the
types of machines that employ message-passing packages, such as MPI, to communicate
among processors. They may be a collection of workstations linked via a network, or
more integrated machines with thousands of processors on internal boards, such as the
Blue Gene computer described in §14.13. These computers, which do not have a shared
memory space, are also called multicomputers. Although these types of computers are
some of the most difficult to program, their low cost and effectiveness for certain classes
of problems have led to their being the dominant type of parallel computer at present.
The running of independent programs on a parallel computer is similar to the multi-
tasking feature used by Unix and PCs. In multitasking (Figure 14.6 left) several independent
programs reside in the computer’s memory simultaneously and share the processing time in a
round robin or priority order. On a SISD computer, only one program runs at a single time,
but if other programs are in memory, then it does not take long to switch to them. In multipro-
cessing (Figure 14.6 right) these jobs may all run at the same time, either in different parts of
memory or in the memory of different computers. Clearly, multiprocessing becomes compli-
cated if separate processors are operating on different parts of the same program because then
synchronization and load balance (keeping all the processors equally busy) are concerns.
In addition to instructions and data streams, another way of categorizing parallel compu-
tation is by granularity. A grain is defined as a measure of the computational work to be done,
more specifically, the ratio of computation work to communication work.
• Coarse-grain parallel: Separate programs running on separate computer systems with
the systems coupled via a conventional communication network. An illustration is six
Linux PCs sharing the same files across a network but with a different central memory
system for each PC. Each computer can be operating on a different, independent part of
one problem at the same time.
• Medium-grain parallel: Several processors executing (possibly different) programs si-
multaneously while accessing a common memory. The processors are usually placed on
a common bus (communication channel) and communicate with each other through
the memory system. Medium-grain programs have different, independent, parallel sub-
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 313
Figure 14.6 Left: Multitasking of four programs in memory at one time. On a SISD computer the programs are
executed in round robin order. Right: Four programs in the four separate memories of a MIMD com-
puter.
D
C
A
B
C D
BA
routines running on different processors. Because the compilers are seldom smart enough
to figure out which parts of the program to run where, the user must include the multitask-
ing routines in the program.1
• Fine-grain parallel: As the granularity decreases and the number of nodes increases,
there is an increased requirement for fast communication among the nodes. For this reason
fine-grain systems tend to be custom-designed machines. The communication may be via
a central bus or via shared memory for a small number of nodes, or through some form
of high-speed network for massively parallel machines. In the latter case, the compiler
divides the work among the processing nodes. For example, different for loops of a
program may be run on different nodes.
14.9 DISTRIBUTED MEMORY PROGRAMMING
An approach to concurrent processing that, because it is built from commodity PCs, has gained
dominant acceptance for coarse- and medium-grain systems is distributed memory. In it, each
processor has its own memory and the processors exchange data among themselves through
a high-speed switch and network. The data exchanged or passed among processors have en-
coded to and from addresses and are called messages. The clusters of PCs or workstations
that constitute a Beowulf 2 are examples of distributed memory computers (Figure 14.7). The
unifying characteristic of a cluster is the integration of highly replicated compute and commu-
nication components into a single system, with each node still able to operate independently.
In a Beowulf cluster, the components are commodity ones designed for a general market, as
are the communication network and its high-speed switch (special interconnects are used by
major commercial manufacturers, but they do not come cheaply). Note: A group of computers
connected by a network may also be called a cluster but unless they are designed for parallel
processing, with the same type of processor used repeatedly and with only a limited number of
processors (the front end) onto which users may log in, they are not usually called a Beowulf.
The literature contains frequent arguments concerning the differences among clus-
ters, commodity clusters, Beowulfs, constellations, massively parallel systems, and so forth
[Dong 05]. Even though we recognize that there are major differences between the clusters on
the top 500 list of computers and the ones that a university researcher may set up in his or her
lab, we will not distinguish these fine points in the introductory materials we present here.
For a message-passing program to be successful, the data must be divided among nodes
so that, at least for a while, each node has all the data it needs to run an independent subtask.
1Some experts define our medium grain as coarse grain yet this distinction changes with time.
2Presumably there is an analogy between the heroic exploits of the son of Ecgtheow and the nephew of Hygelac in the 1000 C.E.
poem Beowulf and the adventures of us common folk assembling parallel computers from common elements that have surpassed
the performance of major corporations and their proprietary, multi-million-dollar supercomputers.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
314 CHAPTER 14
Figure 14.7 Two views of modern parallel computing (courtesy of Yuefan Deng).
Values of Parallel Processing
Mainframe
Vector Computer
PC
Work
station
Mini
B
eo
w
u
lf
When a program begins execution, data are sent to all the nodes. When all the nodes have
completed their subtasks, they exchange data again in order for each node to have a complete
new set of data to perform the next subtask. This repeated cycle of data exchange followed
by processing continues until the full task is completed. Message-passing MIMD programs
are also single-program, multiple-data programs, which means that the programmer writes a
single program that is executed on all the nodes. Often a separate host program, which starts
the programs on the nodes, reads the input files and organizes the output.
14.10 PARALLEL PERFORMANCE
Imagine a cafeteria line in which all the servers appear to be working hard and fast yet the
ketchup dispenser has some relish partially blocking its output and so everyone in line must
wait for the ketchup lovers up front to ruin their food before moving on. This is an example
of the slowest step in a complex process determining the overall rate. An analogous situa-
tion holds for parallel processing, where the “relish” may be the issuing and communicating
of instructions. Because the computation cannot advance until all the instructions have been
received, this one step may slow down or stop the entire process.
As we soon will demonstrate, the speedup of a program will not be significant unless
you can get ∼90% of it to run in parallel, and even then most of the speedup will probably
be obtained with only a small number of processors. This means that you need to have a
computationally intense problem to make parallelization worthwhile, and that is one of the
reasons why some proponents of parallel computers with thousands of processors suggest that
you not apply the new machines to old problems but rather look for new problems that are both
big enough and well-suited for massively parallel processing to make the effort worthwhile.
The equation describing the effect on speedup of the balance between serial and parallel
parts of a program is known as Amdahl’s law [Amd 67, Quinn 04]. Let
p = no. of CPUs T1 = 1-CPU time, Tp = p-CPU time. (14.3)
The maximum speedup Sp attainable with parallel processing is thus
Smaxp =
T1
Tp
→ p. (14.4)
This limit is never met for a number of reasons: Some of the program is serial, data and memory
conflicts occur, communication and synchronization of the processors take time, and it is rare
to attain a perfect load balance among all the processors. For the moment we ignore these
complications and concentrate on how the serial part of the code affects the speedup. Let
f be the fraction of the program that potentially may run on multiple processors. The fraction
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 315
Figure 14.8 The theoretical speedup of a program as a function of the fraction of the program that potentially may
be run in parallel. The different curves correspond to different numbers of processors.
Parallel Fraction
S
p
e
e
d
u
p
p = 2
p
 =
 1
6
p
 =
 i
n
fi
n
it
y
Amdahl's Law
0
4
8
0 20% 40% 60% 80%
Percent Parallel
S
p
e
e
d
u
p
1− f of the code that cannot be run in parallel must be run via serial processing and thus takes
time:
Ts = (1− f)T1 (serial time). (14.5)
The time Tp spent on the p parallel processors is related to Ts by
Tp = f
T1
p
. (14.6)
That being so, the speedup Sp as a function of f and the number of processors is
Sp =
T1
Ts + Tp
=
1
1− f + f/p
(Amdahl’s law). (14.7)
Some theoretical speedups are shown in Figure 14.8 for different numbers p of processors.
Clearly the speedup will not be significant enough to be worth the trouble unless most of the
code is run in parallel (this is where the 90% of your in- parallel figure comes from). Even
an infinite number of processors cannot increase the speed of running the serial parts of the
code, and so it runs at one processor speed. In practice this means many problems are limited
to a small number of processors and that often for realistic applications only 10%–20% of the
computer’s peak performance may be obtained.
14.10.1 Communication Overhead
As discouraging as Amdahl’s law may seem, it actually overestimates speedup because it ig-
nores the overhead for parallel computation. Here we look at communication overhead. As-
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
316 CHAPTER 14
sume a completely parallel code so that its speedup is
Sp =
T1
Tp
=
T1
T1/p
= p. (14.8)
The denominator assumes that it takes no time for the processors to communicate. However,
it take a finite time, called latency, to get data out of memory and into the cache or onto the
communication network. When we add in this latency, as well as other times that make up the
communication time Tc, the speedup decreases to
Sp '
T1
T1/p+ Tc
< p (with communication time). (14.9)
For the speedup to be unaffected by communication time, we need to have
T1
p
 Tc ⇒ p
T1
Tc
. (14.10)
This means that as you keep increasing the number of processors p, at some point the time
spent on computation T1/pmust equal the time Tc needed for communication, and adding more
processors leads to greater execution time as the processors wait around more to communicate.
This is another limit, then, on the maximum number of processors that may be used on any one
problem, as well as on the effectiveness of increasing processor speed without a commensurate
increase in communication speed.
The continual and dramatic increases in CPU speed, along with the widespread adop-
tion of computer clusters, is leading to a changing view as to how to judge the speed of an
algorithm. Specifically, the slowest step in a process is usually the rate-determining step, and
the increasing speed of CPUs means that this slowest step is more and more often access to or
communication among processors. Such being the case, while the number of computational
steps is still important for determining an algorithm’s speed, the number and amount of mem-
ory access and interprocessor communication must also be mixed into the formula. This is
currently an active area of research in algorithm development.
14.11 PARALLELIZATION STRATEGY
A typical organization of a program containing both serial and parallel tasks is given in Ta-
ble14.4. The user organizes the work into units called tasks, with each task assigning work
(threads) to a processor. The main task controls the overall execution as well as the subtasks
that run independent parts of the program (called parallel subroutines, slaves, guests, or sub-
tasks). These parallel subroutines can be distinctive subprograms, multiple copies of the same
subprogram, or even for loops.
It is the programmer’s responsibility to ensure that the breakup of a code into parallel
subroutines is mathematically and scientifically valid and is an equivalent formulation of the
original program. As a case in point, if the most intensive part of a program is the evaluation
of a large Hamiltonian matrix, you may want to evaluate each row on a different processor.
Consequently, the key to parallel programming is to identify the parts of the program that may
benefit from parallel execution. To do that the programmer should understand the program’s
data structures (see below), know in what order the steps in the computation must be performed,
and know how to coordinate the results generated by different processors.
The programmer helps speed up the execution by keeping many processors simultane-
ously busy and by avoiding storage conflicts among different parallel subprograms. You do
this load balancing by dividing your program into subtasks of approximately equal numerical
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 317
Table 14.4 A typical organization of a program containing both serial and parallel tasks.
Main task program
Main routine
Serial subroutine a
Parallel sub 1 Parallel sub 2 Parallel sub 3
Summation task
intensity that will run simultaneously on different processors. The rule of thumb is to make
the task with the largest granularity (workload) dominant by forcing it to execute first and to
keep all the processors busy by having the number of tasks an integer multiple of the number
of processors. This is not always possible.
The individual parallel threads can have shared or local data. The shared data may be
used by all the machines, while the local data are private to only one thread. To avoid storage
conflicts, design your program so that parallel subtasks use data that are independent of the data
in the main task and in other parallel tasks. This means that these data should not be modified
or even examined by different tasks simultaneously. In organizing these multiple tasks, reduce
communication overhead costs by limiting communication and synchronization. These costs
tend to be high for fine-grain programming where much coordination is necessary. However,
do not eliminate communications that are necessary to ensure the scientific or mathematical
validity of the results; bad science can do harm!
14.12 PRACTICAL ASPECTS OF MIMD MESSAGE PASSING
It makes sense to run only the most numerically intensive codes on parallel machines. Fre-
quently these are very large programs assembled over a number of years or decades by a num-
ber of people. It should come as no surprise, then, that the programming languages for parallel
machines are primarily Fortran90, which has explicit structures for the compiler to parallelize,
and C. (We have not attained good speedup with Java in parallel and therefore do not recom-
mend it for parallel computing.) Effective parallel programming becomes more challenging as
the number of processors increases. Computer scientists suggest that it is best not to attempt
to modify a serial code but instead to rewrite it from scratch using algorithms and subroutine
libraries best suited to parallel architecture. However, this may involve months or years of
work, and surveys find that ∼70% of computational scientists revise existing codes [Pan 96].
Most parallel computations at present are done on a multiple-instruction, multiple-data
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
318 CHAPTER 14
computers via message passing. In D we give a tutorial on the use of MPI, the most common
message-passing interface. Here we outline some practical concerns based on user experience
[Dong 05, Pan 96].
Parallelism carries a price tag: There is a steep learning curve requiring intensive ef-
fort.Failures may occur for a variety of reasons, especially because parallel environments
tend to change often and get “locked up” by a programming error. In addition, with
multiple computers and multiple operating systems involved, the familiar techniques for
debugging may not be effective.
Preconditions for parallelism: If your program is run thousands of times between changes,
with execution time in days, and you must significantly increase the resolution of the out-
put or study more complex systems, then parallelism is worth considering. Otherwise, and
to the extent of the difference, parallelizing a code may not be worth the time investment.
The problem affects parallelism: You must analyze your problem in terms of how and
when data are used, how much computation is required for each use, and the type of
problem architecture:
• Perfectly parallel: The same application is run simultaneously on different data sets,
with the calculation for each data set independent (e.g., running multiple versions of a
Monte Carlo simulation, each with different seeds, or analyzing data from independent
detectors). In this case it would be straightforward to parallelize with a respectable per-
formance to be expected.
• Fully synchronous: The same operation applied in parallel to multiple parts of the same
data set, with some waiting necessary (e.g., determining positions and velocities of par-
ticles simultaneously in a molecular dynamics simulation). Significant effort is required,
and unless you balance the computational intensity, the speedup may not be worth the
effort.
• Loosely synchronous: Different processors do small pieces of the computation but with
intermittent data sharing (e.g., diffusion of groundwater from one location to another). In
this case it would be difficult to parallelize and probably not worth the effort.
• Pipeline parallel: Data from earlier steps processed by later steps, with some overlap-
ping of processing possible (e.g., processing data into images and then into animations).
Much work may be involved, and unless you balance the computational intensity, the
speedup may not be worth the effort.
14.12.1 High-Level View of Message Passing
Although it is true that parallel computing programs may become very complicated, the basic
ideas are quite simple. All you need is a regular programming language like C or Fortran, plus
four communication statements:3
send: One processor sends a message to the network. It is not necessary to indicate who
will receive the message, but it must have a name.
receive: One processor receives a message from the network. This processor does not have
to know who sent the message, but it has to know the message’s name.
myid: An integer that uniquely identifies each processor.
numnodes: An integer giving the total number of nodes in the system.
3Personal communication, Yuefan Deng.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 319
Figure 14.9 A master process and two slave processes passing messages. Notice how this program has more sends
than receives and consequently may lead to results that depend on order of execution, or may even lock
up.
Compute
Create
Create
Compute
Receive
Receive
Receive
Compute
Receive
Send
Master
compute
send
compute
send
compute
receive
send
compute
send
Slave 1
compute
send
compute
send
compute
receive
send
compute
send
Slave 2
T
im
e
Once you have made the decision to run your program on a computer cluster, you will have
to learn the specifics of a message-passing system such as MPI (D). Here we give a broader
view. When you write a message-passing program, you intersperse calls to the message-passing
library with your regular Fortran or C program. The basic steps are
1. Submit your job from the command line or a job control system.
2. Have your job start additional processes.
3. Have these processes exchange data and coordinate their activities.
4. Collect these data and have the processes stop themselves.
We show this graphically in Figure 14.9 where at the top we see a master process create two
slave processes and then assign work for them to do (arrows). The processes then communicate
with each other via message passing, output their data to files, and finally terminate.
What can go wrong: Figure 14.9 also illustrates some of the difficulties:
• The programmer is responsible for getting the processes to cooperate and for dividing the
work correctly.
• The programmer is responsible for ensuring that the processes have the correct data to
process and that the data are distributed equitably.
• The commands are at a lower level than those of a compiled language, and this introduces
more details for you to worry about.
• Because multiple computers and multiple operating systems are involved, the user may
not receive or understand the error messages produced.
• It is possible for messages to be sent or received not in the planned order.
• A race condition may occur in which the program results depend upon the specific order-
ing of messages. There is no guarantee that slave 1 will get its work done before slave 2,
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
320 CHAPTER 14
Figure 14.10 The building blocks of Blue Gene (adapted from [Gara 05]).
Chip
(2 processors)
Card
(2 chips)
Board
(16 cards)
Cabinet
(32 boards)
2.8/5.6 Gflops
4 MB
11.2 Gflops
1 GB DDR
80 Gflops
16 GB DDR
System
(64 cabinets)
5.7 Tflops
512 GB
360 Tflops
32TB
even though slave 1 may have started working earlier (Figure 14.9).
• Note in Figure 14.9 how different processors must wait for signals from other processors;
this is clearly a waste of time and has potential for deadlock.
• Processes may deadlock, that is, wait for a message that never arrives.
14.13 EXAMPLE OF A SUPERCOMPUTER: IBM BLUE GENE/L
Whatever figures we give to describe the latest supercomputer will be obsolete by the time you
read them. Nevertheless, for the sake of completeness and to set the scale for the present, we do
it anyway. At the time of this writing, the fastest computer, in some aggregate sense, is the IBM
Blue Gene series [Gara 05]. The name reflects its origin in a computer originally intended for
gene research that is now sold as a general-purpose supercomputer (after approximately $600
million in development costs).
A building-block view of Blue Gene is given in Figure 14.10. In many ways this is
a computer built by committee, with compromises made in order to balance cost, cooling,
computing speed, use of existing technologies, communication speed, and so forth. As a case
in point, the CPUs have dual cores, with one for computing and the other for communication.
This reflects the importance of communication for distributed-memory computing (there are
both on- and off-chip distributed memories). And while the CPU is fast at 5.6 GFLOPs,
there are faster ones available, but they would generate so much heat that it would not be
possible to obtain the extreme scalability up to 216 = 65,536 dual-processor nodes. The next
objective is to balance a low cost/performance ratio with a high performance/ watt ratio.
Observe that at the lowest level Blue Gene contains two CPUs (dual cores) on a chip,
with two chips on a card, with 16 cards on a board, with 32 boards in a cabinet, and up to 64
cabinets for a grand total of 65,536 CPUs (Figure 14.10). And if a way can be found to make
all these chips work together for the common good on a single problem, they would turn out
a peak performance of 360 × 1012 floating-point operations per second (360 tFLOPs). Each
processor runs a Linux operating system (imagine what the cost in both time and money would
be for Windows!) and utilizes the hardware by running a distributed memory MPI with C,
C++, and Fortran90 compilers; this is not yet a good place for Java.
Blue Gene has three separate communication networks (Figure 14.11). At the heart of the
memory system is a 64×32×32 3-D torus that connects all the memory blocks; Figure 14.11a
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 321
Figure 14.11 (a) A 3-D torus connecting 2× 2× 2 memory blocks. (b) The global collective memory system. (c)
The control and GB-Ethernet memory system (adapted from [Gara 05]).
Gigabyte Internet
I/O Node
Fast Ethernet
Compute Nodes
FPGA
JTAG
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
322 CHAPTER 14
shows the torus for 2×2×2 memory blocks. The links are made by special link chips that also
compute; they provide both direct neighbor–neighbor communications as well as cut through
communication across the network. The result of this sophisticated network is that there is
approximately the same effective bandwidth and latencies (response times) between all nodes,
yet in order to obtain high speed, it is necessary to keep communication local. For node-to-node
communication a rate of 1.4 Gb/s = 1/10−9 s = 1 ns is obtained.
The latency ranges from 100 ns for 1 hop, to 6.4 µs for 64 hops between processors.
The collective network in Figure 14.11b is used to communicate with all the processors simul-
taneously, what is known as a broadcast, and does so at 4 b/cycle. Finally, the control and
gigabit ethernet network (Figure 14.11c) is used for I/O to communicate with the switch (the
hardware communication center) and with ethernet devices. Its total capacity is greater than 1
tb (= 1012)
¯
/s.
The computing heart of Blue Gene is its integrated circuit and the associated memory
system (Figure 14.12). This is essentially an entire computer system on a chip containing,
among other things,
• two PowerPC 440s with attached floating-point units (for rapid processing of floating-
point numbers); one CPU is for computing, and one is for I/O.
• a RISC architecture CPU with seven stages, three pipelines, and 32-b, 64-way associative
cache lines,
• variable memory page size,
• embedded dynamic memory controllers,
• a gigabit ethernet adapter,
• a total of 512 MB/node (32 tB when summed over nodes),
• level-1 (L1) cache of 32 KB, L2 cache of 2 KB, and L3 cache of 4 MB.
14.14 UNIT III. HPC PROGRAM OPTIMIZATION
The type of optimization often associated with high-performance or numerically intensive
computing is one in which sections of a program are rewritten and reorganized in order
to increase the program’s speed. The overall value of doing this, especially as computers
have become so fast and so available, is often a subject of controversy between computer
scientists and computational scientists. Both camps agree that using the optimization options
of compilers is a good idea. However, computational scientists tend to run large codes with
large amounts of data in order to solve real-world problems and often believe that you cannot
rely on the compiler to do all the optimization, especially when you end up with time on your
hands waiting for the computer to finish executing your program. Here are some entertaining,
yet insightful, views [Har 96] reflecting the conclusions of those who have reflected upon this
issue:
More computing sins are committed in the name of efficiency (without neces-
sarily achieving it) than for any other single reason—including blind stupidity.
— W.A. Wulf
We should forget about small efficiencies, say about 97% of the time: premature
optimization is the root of all evil.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 323
Figure 14.12 The single-node memory system (adapted from [Gara 05]).
Blue Gene/L ASIC
Snoop
Main store
L3 cache (embedded DRAM)
LocksScratch
SRAM
Bus
interface
Bus
interface
L2WL2W L2R L2R
CPU 1 CPU 2
L1
D-cache
L1
D-cache
L1
I-cache
L1
I-cache
440 core440 core
— Donald Knuth
The best is the enemy of the good.
— Voltaire
Rule 1: Do not do it.
Rule 2 (for experts only): Do not do it yet.
Do not optimize as you go: Write your program without regard to possible
optimizations, concentrating instead on making sure that the code is clean,
correct, and understandable. If it’s too big or too slow when you’ve finished, then
you can consider optimizing it.
Remember the 80/20 rule: In many fields you can get 80% of the result with
20% of the effort (also called the 90/10 rule—it depends on who you talk
to). Whenever you’re about to optimize code, use profiling to find out where
that 80% of execution time is going, so you know where to concentrate your effort.
Always run “before” and “after” benchmarks: How else will you know that your
optimizations actually made a difference? If your optimized code turns out to be
only slightly faster or smaller than the original version, undo your changes and go
back to the original, clear code.
Use the right algorithms and data structures: Do not use an O(n2) DFT algorithm
to do a Fourier transform of a thousand elements when there’s an O(n log n) FFT
available. Similarly, do not store a thousand items in an array that requires an
O(n) search when you could use an O(log n) binary tree or an O(1) hash table.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
324 CHAPTER 14
14.14.1 Programming for Virtual Memory (Method)
While paging makes little appear big, you pay a price because your program’s run time in-
creases with each page fault. If your program does not fit into RAM all at once, it will run
significantly slower. If virtual memory is shared among multiple programs that run simultane-
ously, they all can’t have the entire RAM at once, and so there will be memory access conflicts,
in which case the performance of all the programs will suffer. The basic rules for programming
for virtual memory are:
1. Do not waste your time worrying about reducing the amount of memory used (the work-
ing set size) unless your program is large. In that case, take a global view of your entire
program and optimize those parts that contain the largest arrays.
2. Avoid page faults by organizing your programs to successively perform their calculations
on subsets of data, each fitting completely into RAM.
3. Avoid simultaneous calculations in the same program to avoid competition for memory
and consequent page faults. Complete each major calculation before starting another.
4. Group data elements close together in memory blocks if they are going to be used together
in calculations.
14.14.2 Optimizing Programs; Python versus Fortran/C
Many of the optimization techniques developed for Fortran and C are also relevant for Python
applications. Yet while Python is a good language for scientific programming and is as univer-
sal and portable as Java, at present Python code runs slower than Fortran, C or even Java code.
In part, this is a consequence of the Fortran and C compilers having been around longer and
thereby having been better refined to get the most out of a computer’s hardware, and in part this
is also a consequence of Python not being designed speed. Since modern computers are so fast,
whether a program takes 1s or 3s usually does not matter much, especially in comparison to
the hours or days of your time that it might take to modify a program for different computers.
However, you may want to convert the code to C (whose command structure is similar to that
of Python) if you are running a computation that takes hours or days to complete and will be
doing it many times.
Especially when asked to, Fortran and C compilers look at your entire code as a single
entity and rewrite it for you so that it runs faster. (The rewriting is at a fairly basic level,
so there’s not much use in your studying the compiler’s output as a way of improving your
programming skills.) In particular, Fortran and C compilers are very careful in accessing arrays
in memory. They also are careful to keep the cache lines full so as not to keep the CPU
waiting with nothing to do. There is no fundamental reason why a program written in Java
cannot be compiled to produce an highly efficient code, and indeed such compilers are being
developed and becoming available. However, such code is optimized for a particular computer
architecture and so is not portable. In contrast, the byte code (.class file) produced by Java is
designed to be interpreted or recompiled by the Java Virtual Machine (just another program).
When you change from Unix to Windows, for example, the Java Virtual Machine program
changes, but the byte code is the same. This is the essence of Java’s portability.
In order to improve the performance of Java, many computers and browsers run a Just-
in-Time (JIT) Java compiler. If a JIT is present, the Java Virtual Machine feeds your byte
code Prog.class to the JIT so that it can be recompiled into native code explicitly tailored
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 325
to the machine you are using. Although there is an extra step involved here, the total time it
takes to run your program is usually 10–30 times faster with a JIT as compared to line-by-line
interpretation. Because the JIT is an integral part of the Java Virtual Machine on each operating
system, this usually happens automatically.
In the experiments below you will investigate techniques to optimize both Fortran and
Java programs and to compare the speeds of both languages for the same computation. If you
run your Java code on a variety of machines (easy to do with Java), you should also be able to
compare the speed of one computer to that of another. Note that a knowledge of Fortran is not
required for these exercises.
14.14.2.1 Good and Bad Virtual Memory Use (Experiment)
To see the effect of using virtual memory, run these simple pseudocode examples on your
computer (Listings 14.1–14.4). Use a command such as time to measure the time used for
each example. These examples call functions force12 and force21. You should write these
functions and make them have significant memory requirements for both local and global vari-
ables.
Listing 14.1 BAD program, too simultaneous. 
f o r j = 1 , n ; { f o r i = 1 , n ; {
f12 ( i , j ) = f o r c e 1 2 ( p ion ( i ) , p ion ( j ) )
/ / F i l l f12
f21 ( i , j ) = f o r c e 2 1 ( p ion ( i ) , p ion ( j ) )
/ / F i l l f21
f t o t = f12 ( i , j ) + f21 ( i , j ) }}
/ / F i l l
f t o t
You see (Listing 14.1) that each iteration of the for loop requires the data and code for all
the functions as well as access to all the elements of the matrices and arrays. The working set
size of this calculation is the sum of the sizes of the arrays f12(N,N), f21(N,N), and pion(N)
plus the sums of the sizes of the functions force12 and force21.
A better way to perform the same calculation is to break it into separate components
(Listing 14.2):
Listing 14.2 GOOD program, separate loops. 
f o r j = 1 , n ; { f o r i = 1 , n ; f12 ( i , j ) = f o r c e 1 2 ( p ion ( i ) , p ion ( j ) ) }
f o r j = 1 , n ; { f o r i = 1 , n ; f21 ( i , j ) = f o r c e 2 1 ( p ion ( i ) , p ion ( j ) ) }
f o r j = 1 , n ; { f o r i = 1 , n ; f t o t = f12 ( i , j ) + f21 ( i , j ) }
Here the separate calculations are independent and the working set size, that is, the amount of
memory used, is reduced. However, you do pay the additional overhead costs associated with
creating extra for loops. Because the working set size of the first for loop is the sum of the
sizes of the arrays f12(N, N) and pion(N), and of the function force12, we have approxi-
mately half the previous size. The size of the last for loop is the sum of the sizes for the two
arrays. The working set size of the entire program is the larger of the working set sizes for the
different for loops.
As an example of the need to group data elements close together in memory or common
blocks if they are going to be used together in calculations, consider the following code (Listing
14.3):
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
326 CHAPTER 14
Listing 14.3 BAD Program, discontinuous memory. 
Common zed , y l t ( 9 ) , p a r t ( 9 ) , z p a r t 1 ( 5 0 0 0 0 ) , z p a r t 2 ( 5 0 0 0 0 ) , med2 ( 9 )
f o r j = 1 , n ; y l t ( j ) = zed ∗ p a r t ( j ) / med2 ( 9 ) / / D i scont inuous v a r i a b l e s
Here the variables zed, ylt, and part are used in the same calculations and are adjacent in
memory because the programmer grouped them together in Common (global variables). Later,
when the programmer realized that the array med2 was needed, it was tacked onto the end of
Common. All the data comprising the variables zed, ylt, and part fit onto one page, but the
med2 variable is on a different page because the large array zpart2(50000) separates it from
the other variables. In fact, the system may be forced to make the entire 4-KB page available
in order to fetch the 72 B of data in med2. While it is difficult for a Fortran or C programmer to
ensure the placement of variables within page boundaries, you will improve your chances by
grouping data elements together (Listing 14.4):
Listing 14.4 GOOD program, continuous memory. 
Common zed , y l t ( 9 ) , p a r t ( 9 ) , med2 ( 9 ) , z p a r t 1 ( 5 0 0 0 0 ) , z p a r t 2 ( 5 0 0 0 0 )
f o r j = 1 , n ; y l t ( j ) = zed∗ p a r t ( j ) / med2 ( J )
/ / Continuous
14.14.3 Empirical Performance of Hardware
In this section you conduct an experiment in which you run a complete program in several lan-
guages and on as many computers as are available. In this way you explore how a computer’s
architecture and software affect a program’s performance.
Even if you do not know (or care) what is going on inside a program, some optimizing
compilers are smart and caring enough to figure it out for you and then go about rewriting your
program for improved performance. You control how completely the compiler does this when
you add optimization options to the compile command:
> f90 --O tune.f90
Here --O turns on optimization (O is the capital letter “oh,” not zero). The actual optimization
that is turned on differs from compiler to compiler. Fortran and C compilers have a bevy of such
options and directives that let you truly customize the resulting compiled code. Sometimes op-
timization options make the code run faster, sometimes not, and sometimes the faster-running
code gives the wrong answers (but does so quickly).
Because computational scientists may spend a good fraction of their time running com-
piled codes, the compiler options tend to become quite specialized. As a case in point, most
compilers provide a number of levels of optimization for the compiler to attempt (there are no
guarantees with these things). Although the speedup obtained depends upon the details of the
program, higher levels may give greater speedup, as well as a concordant greater risk of being
wrong.
The Forte/Sun Fortran compiler options include
--O Use the default optimization level (--O3)
--O1 Provide minimum statement-level optimizations
--O2 Enable basic block-level optimizations
--O3 Add loop unrolling and global optimizations
--O4 Add automatic inlining of routines from the same source file
--O5 Attempt aggressive optimizations (with profile feedback)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 327
For the Visual Fortran (Compaq, Intel) compiler under windows, options are entered
as /optimize and for optimization are
/optimize:0 Disable most optimizations
/optimize:1 Local optimizations in the source program unit
/optimize:2 Global optimization, including /optimize:1
/optimize:3 Additional global optimizations; speed at cost of code size:
loop unrolling, instruction scheduling, branch code
replication, padding arrays for cache
/optimize:4 Interprocedure analysis, inlining small procedures
/optimize:5 Activate loop transformation optimizations
The gnu compilers gcc, g77, g90 accept --O options as well as
--malign--double Align doubles on 64-bit boundaries
--ffloat--store For codes using IEEE-854 extended
precision
--fforce--mem, --fforce--addr Improves loop optimization
--fno--inline Do not compile statement functions inline
--ffast--math Try non-IEEE handling of floats
--funsafe--math--optimizations Speeds up float operations; incorrect results
possible
--fno--trapping--math Assume no floating-point traps generated
--fstrength--reduce Makes some loops faster
--frerun--cse--after--loop
--fexpensive--optimizations
--fdelayed--branch
--fschedule--insns
--fschedule--insns2
--fcaller--saves
--funroll--loops Unrolls iterative DO loops
--funroll--all--loops Unrolls DO WHILE loops
14.14.4 Python versus Fortran/C
The various versions of the program tune solve the matrix eigenvalue problem
Hc = Ec (14.11)
for the eigenvalues E and eigenvectors c of a Hamiltonian matrix H. Here the individual
Hamiltonian matrix elements are assigned the values
Hi,j =
{
i, for i = j,
0.3|i−j|, for i 6= j,
=

1 0.3 0.14 0.027 . . .
0.3 2 0.3 0.9 . . .
0.14 0.3 3 0.3 . . .
. . .
 . (14.12)
Because the Hamiltonian is almost diagonal, the eigenvalues should be close to the values of
the diagonal elements and the eigenvectors should be close to N -dimensional unit vectors. For
the present problem, the H matrix has dimension N × N ' 2000 × 2000 = 4, 000, 000,
which means that matrix manipulations should take enough time for you to see the effects of
optimization. If your computer has a large supply of central memory, you may need to make
the matrix even larger to see what happens when a matrix does not all fit into RAM.
We find the solution to (14.11) via a variation of the power or Davidson method. We
start with an arbitrary first guess for the eigenvector c and use it to calculate the energy corre-
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
328 CHAPTER 14
sponding to this eigenvector,4
c0 '

1
0
...
0
 , E ' c
†
0Hc0
c†0c0
, (14.13)
where c†0 is the row vector adjoint of c0. Because H is nearly diagonal with diagonal elements
that increases as we move along the diagonal, this guess should be close to the eigenvector with
the smallest eigenvalue. The heart of the algorithm is the guess that an improved eigenvector
has the kth component
c1|k ' c0|k +
[H− EI]c0|k
E −Hk,k
, (14.14)
where k ranges over the length of the eigenvector. If repeated, this method converges to the
eigenvector with the smallest eigenvalue. It will be the smallest eigenvalue since it gets the
largest weight (smallest denominator) in (14.14) each time. For the present case, six places of
precision in the eigenvalue are usually obtained after 11 iterations. Here are the steps to follow:
• Vary the variable err in tune that controls precision and note how it affects the number
of iterations required.
• Try some variations on the initial guess for the eigenvector (14.14) and see if you can get
the algorithm to converge to some of the other eigenvalues.
• Keep a table of your execution times versus technique.
• Compile and execute tune.f90 and record the run time. On Unix systems, the compiled
program will be placed in the file a.out. From a Unix shell, the compilation, timing, and
execution can all be done with the commands
> f90 tune.f90 Fortran compilation
> cc --lm tune.c C compilation, gcc also likely
> time a.out Execution
Here the compiled Fortran program is given the (default) name a.out, and the time com-
mand gives you the execution (user) time and system time in seconds to execute a.out.
• As indicated in §14.14.3, you can ask the compiler to produce a version of your program
optimized for speed by including the appropriate compiler option:
> f90 --O tune.f90
Execute and time the optimized code, checking that it still gives the same answer, and note
any speedup in your journal.
• Try out optimization options up to the highest levels and note the run time and accuracy
obtained. Usually --O3 is pretty good, especially for as simple a program as tune with
only a main method. With only one program unit we would not expect --O4 or --O5 to be
an improvement over --O3. However, we do expect --O3, with its loop unrolling, to be an
improvement over --O2.
• The program tune4 does some loop unrolling (we will explore that soon). To see the best
we can do with Fortran, record the time for the most optimized version of tune4.f90.
• The program Tune.py in Listing 14.5 is the Java equivalent of the Fortran program
tune.f90.
• To get an idea of what Tune.py does (and give you a feel for how hard life is for the poor
computer), assume ldim =2 and work through one iteration of Tune by hand. Assume
4Note that the codes refer to the eigenvector c0 as coef.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 329
that the iteration loop has converged, follow the code to completion, and write down the
values assigned to the variables.
• Compile and execute Tune.py. You do not have to issue the time command since we built
a timer into the Java program (however, there is no harm in trying it). Check that you still
get the same answer as you did with Fortran and note how much longer it takes with Java.
You might be surprised how much slower Python is than Fortran.
• We now want to perform a little experiment in which we see what happens to performance
as we fill up the computer’s memory. In order for this experiment to be reliable, it is best
for you to not to be sharing the computer with any other users. On Unix systems, the
who --a command shows you the other users (we leave it up to you to figure out how to
negotiate with them).
• To get some idea of what aspect of our little program is making it so slow, compile and
run Tune.py for the series of matrix sizes ldim = 10, 100, 250, 500, 750, 1025, 2500, and
3000. You may get an error message that Java is out of memory at 3000. This is because
you have not turned on the use of virtual memory.
Listing 14.5 Tune.py is meant to be numerically intensive enough to show the results of various types of optimiza-
tions. The program solves the eigenvalue problem iteratively for a nearly diagonal Hamiltonian matrix
using a variation of the power method. 
# Tune . py B a s i c t u n i n g program showing memory a l l o c a t i o n
i m p o r t d a t e t i m e ; from v i s u a l i m p o r t ∗ ; from v i s u a l . g raph i m p o r t ∗
Ldim = 251 ; i t e r = 0 ; s t e p = 0 .
d i a g = z e r o s ( ( Ldim , Ldim ) , F l o a t ) ; c o e f = z e r o s ( ( Ldim ) , F l o a t )
s igma = z e r o s ( ( Ldim ) , F l o a t ) ; ham = z e r o s ( ( Ldim , Ldim ) , F l o a t )
t 0 = d a t e t i m e . d a t e t i m e . now ( ) # I n i t i a l i z e t ime
f o r i i n x r an g e ( 1 , Ldim ) : # S e t up H a m i l t o n i a n
f o r j i n x r an g e ( 1 , Ldim ) :
i f ( abs ( j − i ) >10) : ham [ j , i ] = 0 .
e l s e : ham [ j , i ] = math . pow ( 0 . 3 , abs ( j − i ) )
ham [ i , i ] = i ; c o e f [ i ] = 0 . ;
c o e f [ 1 ] = 1 . ; e r r = 1 . ; i t e r = 0 ;
p r i n t "iter ener err "
w h i l e ( i t e r < 15 and e r r > 1 . e−6) : # Compute c u r r e n t e ne rg y & n o r m a l i z e
i t e r = i t e r + 1 ; e n e r = 0 . ; ov lp = 0 . ;
f o r i i n x r an g e ( 1 , Ldim ) :
ov lp = ov lp + c o e f [ i ]∗ c o e f [ i ]
s igma [ i ] = 0 .
f o r j i n x r an g e ( 1 , Ldim ) :
s igma [ i ] = sigma [ i ] + c o e f [ j ]∗ham [ j ] [ i ]
e n e r = e n e r + c o e f [ i ]∗ s igma [ i ]
e n e r = e n e r / ov lp
f o r i i n x r an g e ( 1 , Ldim ) :
c o e f [ i ] = c o e f [ i ] / math . s q r t ( ov lp )
s igma [ i ] = sigma [ i ] / math . s q r t ( ov lp )
e r r = 0 . ;
f o r i i n x r an g e ( 2 , Ldim ) : # Update
s t e p = ( sigma [ i ] − e n e r∗ c o e f [ i ] ) / ( e n e r − ham [ i , i ] )
c o e f [ i ] = c o e f [ i ] + s t e p
e r r = e r r + s t e p∗ s t e p
e r r = math . s q r t ( e r r )
p r i n t " %2d %9.7f %9.7f "%( i t e r , ener , e r r )
d e l t a t = d a t e t i m e . d a t e t i m e . now ( ) − t 0 # E l a p s e d t ime
p r i n t " time = " , d e l t a t
• Make a graph of run time versus matrix size. It should be similar to Figure 14.13, although
if there is more than one user on your computer while you run, you may get erratic results.
Note that as our matrix becomes larger than ∼1000 × 1000 in size, the curve sharply
increases in slope with execution time, in our case increasing like the third power of the
dimension. Since the number of elements to compute increases as the second power of
the dimension, something else is happening here. It is a good guess that the additional
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
330 CHAPTER 14
Figure 14.13 Running time versus dimension for an eigenvalue search using Tune.py and tune.f90.
Ti
m
e 
(s
ec
on
ds
)
Java, Solaris
Execution Time vs Matrix Size
Matrix Dimension
100 100010
Fortran, Solaris
Matrix Dimension
100 1000 10,000
1 1
0.01
10 10
100
1000
10,000
100
0.1
0.1
slowdown is due to page faults in accessing memory. In particular, accessing 2-D arrays,
with their elements scattered all through memory, can be very slow.
• Repeat the previous experiment with tune.f90 that gauges the effect of increasing the
ham matrix size, only now do it for ldim = 10, 100, 250, 500, 1025, 3000, 4000, 6000,. . . .
You should get a graph like ours. Although our implementation of Fortran has automatic
virtual memory, its use will be exceedingly slow, especially for this problem (possibly a
50-fold increase in time!). So if you submit your program and you get nothing on the
screen (though you can hear the disk spin or see it flash busy), then you are probably in
the virtual memory regime. If you can, let the program run for one or two iterations, kill
it, and then scale your run time to the time it would have taken for a full computation.
• To test our hypothesis that the access of the elements in our 2-D array ham [i][j] is
slowing down the program, we have modified Tune.py into Tune4.py in Listing 14.6.
• Look at Tune4.py and note where the nested for loop over i and j now takes step of
∆i = 2 rather the unit steps in Tune.py. If things work as expected, the better memory
access of Tune4.py should cut the run time nearly in half. Compile and execute Tune4.py.
Record the answer in your table.
Listing 14.6 Tune4.py does some loop unrolling by explicitly writing out two steps of a for loop (steps of 2.) This
results in better memory access and faster execution. 
# Tune4 . py Model t u n i n g program
i m p o r t d a t e t i m e ; from v i s u a l i m p o r t ∗ ; from v i s u a l . g raph i m p o r t ∗
Ldim = 200 ; i t e r 1 = 0 ; s t e p = 0 .
ham = z e r o s ( ( Ldim , Ldim ) , F l o a t ) ; d i a g = z e r o s ( ( Ldim ) , F l o a t )
c o e f = z e r o s ( ( Ldim ) , F l o a t ) ; s igma = z e r o s ( ( Ldim ) , F l o a t )
t 0 = d a t e t i m e . d a t e t i m e . now ( ) # I n i t i a l i z e t ime
f o r i i n x r an ge ( 1 , Ldim ) : # S e t up H a m i l t o n i a n
f o r j i n x r an g e ( 1 , Ldim ) :
i f abs ( j − i ) >10: ham [ j , i ] = 0 .
e l s e : ham [ j , i ] = math . pow ( 0 . 3 , abs ( j − i ) )
f o r i i n x r an ge ( 1 , Ldim ) :
ham [ i , i ] = i
c o e f [ i ] = 0 .
d i a g [ i ] = ham [ i , i ]
c o e f [ 1 ] = 1 . ; e r r = 1 . ; i t e r = 0 ;
p r i n t "iter ener err "
w h i l e ( i t e r 1 < 15 and e r r > 1 . e−6) : # Compute c u r r e n t e ne rg y & n o r m a l i z e
i t e r 1 = i t e r 1 + 1
e n e r = 0 .
ov lp1 = 0 .
ov lp2 = 0 .
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 331
f o r i i n x r an g e ( 1 , Ldim − 1 , 2 ) :
ov lp1 = ov lp1 + c o e f [ i ]∗ c o e f [ i ]
ov lp2 = ov lp2 + c o e f [ i + 1]∗ c o e f [ i + 1 ]
t 1 = 0 .
t 2 = 0 .
f o r j i n x r an g e ( 1 , Ldim ) :
t 1 = t 1 + c o e f [ j ]∗ham [ j , i ]
t 2 = t 2 + c o e f [ j ]∗ham [ j , i + 1 ]
s igma [ i ] = t 1
sigma [ i + 1 ] = t 2
e n e r = e n e r + c o e f [ i ]∗ t 1 + c o e f [ i + 1]∗ t 2
ov lp = ov lp1 + ov lp2
e n e r = e n e r / ov lp
f a c t = 1 . / math . s q r t ( ov lp )
c o e f [ 1 ] = f a c t ∗ c o e f [ 1 ]
e r r = 0 . # Update & e r r o r norm
f o r i i n x r an g e ( 2 , Ldim ) :
t = f a c t ∗ c o e f [ i ]
u = f a c t ∗s igma [ i ] − e n e r∗ t
s t e p = u / ( e n e r − d i a g [ i ] )
c o e f [ i ] = t + s t e p
e r r = e r r + s t e p∗ s t e p
e r r = math . s q r t ( e r r )
p r i n t " %2d %15.13f %15.13f "%( i t e r 1 , ener , e r r )
d e l t a t = d a t e t i m e . d a t e t i m e . now ( ) − t 0 # E l a p s e d t ime
p r i n t " time = " , d e l t a t
• In order to cut the number of calls to the 2-D array in half, we employed a technique know
as loop unrolling in which we explicitly wrote out some of the lines of code that otherwise
would be executed implicitly as the for loop went through all the values for its counters.
This is not as clear a piece of code as before, but it evidently, permits the compiler to pro-
duce a faster executable. To check that Tune and Tune4 actually do the same thing, assume
ldim =4 and run through one iteration of Tune4.py by hand. Hand in your manual trial.
14.15 PROGRAMMING FOR THE DATA CACHE (METHOD)
Data caches are small, very fast memory used as temporary storage between the ultrafast
CPU registers and the fast main memory. They have grown in importance as high-performance
computers have become more prevalent. For systems that use a data cache, this may well be
the single most important programming consideration; continually referencing data that are
not in the cache (cache misses) may lead to an order-of-magnitude increase in CPU time. As
indicated in Figures 14.2 and 14.14, the data cache holds a copy of some of the data in memory.
The basics are the same for all caches, but the sizes are manufacturer-dependent. When the
CPU tries to address a memory location, the cache manager checks to see if the data are in the
cache. If they are not, the manager reads the data from memory into the cache, and then the
CPU deals with the data directly in the cache. The cache manager’s view of RAM is shown in
Figure 14.14.
When considering how a matrix operation uses memory, it is important to consider the
stride of that operation, that is, the number of array elements that are stepped through as
the operation repeats. For instance, summing the diagonal elements of a matrix to form the
trace
TrA =
N∑
i=1
a(i, i) (14.15)
involves a large stride because the diagonal elements are stored far apart for large N. However,
the sum
c(i) = x(i) + x(i+ 1) (14.16)
has stride 1 because adjacent elements of x are involved. The basic rule in programming for a
cache is
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
332 CHAPTER 14
Figure 14.14 The cache manager’s view of RAM. Each 128-B cache line is read into one of four lines in cache.
Cache
Virtual Memory
256 lines of 128b (32KB)
• Keep the stride low, preferably at 1, which in practice means:
• Vary the leftmost index first on Fortran arrays.
• Vary the rightmost index first on Java and C arrays.
14.15.1 Exercise 1: Cache Misses
We have said a number of times that your program will be slowed down if the data it needs are
in virtual memory and not in RAM. Likewise, your program will also be slowed down if the
data required by the CPU are not in the cache. For high-performance computing, you should
write programs that keep as much of the data being processed as possible in the cache. To do
this you should recall that Fortran matrices are stored in successive memory locations with the
row index varying most rapidly (column-major order), while Java and C matrices are stored in
successive memory locations with the column index varying most rapidly (row-major order).
While it is difficult to isolate the effects of the cache from other elements of the computer’s
architecture, you should now estimate its importance by comparing the time it takes to step
through the matrix elements row by row to the time it takes to step through the matrix elements
column by column.
By actually running on machines available to you, check that the two simple codes in
Listing 14.7 with the same number of arithmetic operations take significantly different times to
run because one of them must make large jumps through memory with the memory locations
addressed not yet read into the cache: 
x ( j ) = m( 1 , j )
/ / S e q u e n t i a l column r e f e r e n c e
Listing 14.7 Sequential column and row references. 
f o r j = 1 , 9999 ;
x ( j ) = m( j , 1 )
/ /
S e q u e n t i a l row r e f e r e n c e
14.15.2 Exercise 2: Cache Flow
Test the importance of cache flow on your machine by comparing the time it takes to run the
two simple programs in Listings 14.8 and 14.9. Run for increasing column size idim and
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
HIGH-PERFORMANCE COMPUTING HARDWARE, TUNING, AND PARALLEL COMPUTING 333
compare the times for loopA versus those for loopB. A computer with very small caches may
be most sensitive to stride.
Listing 14.8 GOOD f90, BAD Java/C Program; minimum, maximum stride. 
Dimension Vec ( idim , jd im )
/ / S t r i d e 1 f e t c h ( f90 )
f o r j = 1 , jd im ; { f o r i =1 , id im ; Ans = Ans + Vec ( i , j )∗Vec ( i , j ) }
Listing 14.9 BAD f90, GOOD Java/C Program; maximum, minimum stride. 
Dimension Vec ( idim , jd im ) / /
S t r i d e jdim f e t c h ( f90 )
f o r i = 1 , id im ; { f o r j =1 , jd im ; Ans = Ans + Vec ( i , j )∗Vec ( i , j ) }
Loop A steps through the matrix Vec in column order. Loop B steps through in row order. By
changing the size of the columns (the rightmost Fortran index), we change the step size (stride)
taken through memory. Both loops take us through all the elements of the matrix, but the stride
is different. By increasing the stride in any language, we use fewer elements already present
in the cache, require additional swapping and loading of the cache, and thereby slow down the
whole process.
14.15.3 Exercise 3: Large-Matrix Multiplication
As you increase the dimensions of the arrays in your program, memory use increases geometri-
cally, and at some point you should be concerned about efficient memory use. The penultimate
example of memory usage is large-matrix multiplication:
[C] = [A]× [B], (14.17)
cij =
N∑
k=1
aik × bkj . (14.18)
Listing 14.10 BAD f90, GOOD Java/C Program; maximum, minimum stride. 
f o r i = 1 , N; {
/ / Row
f o r j = 1 , N; {
/ / Column
c ( i , j ) = 0 . 0
/ / I n i t i a l i z e
f o r k = 1 , N; {
c ( i , j ) = c ( i , j ) + a ( i , k )∗b ( k , j ) }}} / /
Accumulate sum
This involves all the concerns with different kinds of memory. The natural way to code (14.17)
follows from the definition of matrix multiplication (14.18), that is, as a sum over a row of A
times a column of B. Try out the two code in Listings 14.10 and 14.11 on your computer. In
Fortran, the first code has B with stride 1, but C with stride N . This is corrected in the second
code by performing the initialization in another loop. In Java and C, the problems are reversed.
On one of our machines, we found a factor of 100 difference in CPU times even though the
number of operations is the same!
Listing 14.11 GOOD f90, BAD Java/C Program; minimum, maximum stride. 
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
334 CHAPTER 14
f o r j = 1 , N; {
/ / I n i t i a l i z a t i o n
f o r i = 1 , N; {
c ( i , j ) = 0 . 0 }
f o r k = 1 , N; {
f o r i = 1 , N; {c ( i , j ) = c ( i , j ) + a ( i , k )∗b ( k , j ) }}}
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Fifteen
Thermodynamic Simulations &
Feynman Quantum Path Integration
In Unit I of this chapter we describe how magnetic materials are simulated by using the
Metropolis algorithm to solve the Ising model. This extends the techniques studied in Chap-
ter 5, “Monte Carlo Simulations,” to thermodynamics. Not only do thermodynamic simulations
have important practical applications, but they also give us insight into what is “dynamic” in
thermodynamics. In Unit II we describe a new Monte Carlo algorithm known as Wang–Landau
sampling that in the last few years has shown itself to be far more efficient than the 50-year-old
Metropolis algorithm. Wang–Landau sampling is an active subject in present research, and it
is nice to see it fitting well into an elementary textbook. Unit III applies the Metropolis algo-
rithm to Feynman’s path integral formulation of quantum mechanics [F&H 65]. The theory,
while the most advanced to be found in this book, forms the basis for field- theoretic calcu-
lations of quantum chromodynamics, some of the most fundamental and most time-consuming
computations in existence. Basic discussions can be found in [Mann 83, MacK 85, M&N 87],
with a recent review in [Potv 93].
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
- - - *
Applets
Name Sections Name Sections
Feynman Path Integrals 15.7–15.8
15.1 UNIT I. MAGNETS VIA THE METROPOLIS ALGORITHM
Ferromagnets contain finite-size domains in which the spins of all the atoms point in the same
direction. When an external magnetic field is applied to these materials, the different domains
align and the materials become “magnetized.” Yet as the temperature is raised, the total mag-
netism decreases, and at the Curie temperature the system goes through a phase transition
beyond which all magnetization vanishes. Your problem is to explain the thermal behavior of
ferromagnets.
15.2 AN ISING CHAIN (MODEL)
As our model we consider N magnetic dipoles fixed in place on the links of a linear chain
(Figure 15.1). (It is a straightforward generalization to handle 2-D and 3-D lattices.) Because
the particles are fixed, their positions and momenta are not dynamic variables, and we need
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
336 CHAPTER 15
Figure 15.1 A 1-D lattice of N spins. The interaction energy V = ±J between nearest-neighbor pairs is shown for
aligned and opposing spins.
E = + J
E = – J
worry only about their spins. We assume that the particle at site i has spin si, which is either
up or down:
si ≡ sz,i = ±
1
2
. (15.1)
Each configuration of the N particles is described by a quantum state vector
|αj〉 = |s1, s2, . . . , sN 〉 =
{
±1
2
, ±1
2
, . . .
}
, j = 1, . . . , 2N . (15.2)
Because the spin of each particle can assume any one of two values, there are 2N different
possible states for the N particles in the system. Since fixed particles cannot be interchanged,
we do not need to concern ourselves with the symmetry of the wave function.
The energy of the system arises from the interaction of the spins with each other and
with the external magnetic field B. We know from quantum mechanics that an electron’s spin
and magnetic moment are proportional to each other, so a magnetic dipole–dipole interaction
is equivalent to a spin–spin interaction. We assume that each dipole interacts with the external
magnetic field and with its nearest neighbor through the potential:
Vi = −Jsi · si+1 − gµb si · B. (15.3)
Here the constant J is called the exchange energy and is a measure of the strength of the spin–
spin interaction. The constant g is the gyromagnetic ratio, that is, the proportionality constant
between a particle’s angular momentum and magnetic moment. The constant µb = eh̄/(2mec)
is the Bohr magneton, the basic measure for magnetic moments.
Even for small numbers of particles, the 2N possible spin configurations gets to be very
large (220 > 106), and it is expensive for the computer to examine them all. Realistic samples
with ∼1023 particles are beyond imagination. Consequently, statistical approaches are usually
assumed, even for moderate values of N . Just how large N must be for this to be accurate is
one of the things we want you to explore with your simulations.
The energy of this system in state αk is the expectation value of the sum of the potential
V over the spins of the particles:
Eαk =
〈
αk
∣∣∣∑
i
Vi
∣∣∣αk〉 = −J N−1∑
i=1
sisi+1 −Bµb
N∑
i=1
si. (15.4)
An apparent paradox in the Ising model occurs when we turn off the external magnetic field
and thereby eliminate a preferred direction in space. This means that the average magnetization
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 337
should vanish even though the lowest energy state would have all spins aligned. The answer to
this paradox is that the system with B = 0 is unstable. Even if all the spins are aligned, there
is nothing to stop their spontaneous reversal. Indeed, natural magnetic materials have multiple
finite domains with all the spins aligned, but with the different domains pointing in different
directions. The instabilities in which domains change direction are called For simplicity we
assumeB = 0, which means that the spins interact just with each other. However, be cognizant
of the fact that this means there is no preferred direction in space, and so you may have to be
careful how you calculate observables. For example, you may need to take an absolute value
of the total spin when calculating the magnetization, that is, to calculate 〈|
∑
i si|〉 rather than
〈
∑
i si〉.
The equilibrium alignment of the spins depends critically on the sign of the exchange
energy J . If J > 0, the lowest energy state will tend to have neighboring spins aligned. If the
temperature is low enough, the ground state will be a ferromagnet with all the spins aligned.
If J < 0, the lowest energy state will tend to have neighbors with opposite spins. If the
temperature is low enough, the ground state will be a antiferromagnet with alternating spins.
The simple 1-D Ising model has its limitations. Although the model is accurate in de-
scribing a system in thermal equilibrium, it is not accurate in describing the approach to ther-
mal equilibrium (nonequilibrium thermodynamics is a difficult subject for which the theory is
not complete). Second, as part of our algorithm we postulate that only one spin is flipped at
a time, while real magnetic materials tend to flip many spins at a time. Other limitations are
straightforward to improve, for example, the addition of longer-range interactions, the motion
of the centers, higher-multiplicity spin states, and two and three dimensions.
A fascinating aspect of magnetic materials is the existence of a critical temperature, the
Curie temperature, above which the gross magnetization essentially vanishes. Below the Curie
temperature the quantum state of the material has long-range order extending over macroscopic
dimensions; above the Curie temperature there is only short-range order extending over atomic
dimensions. Even though the 1-D Ising model predicts realistic temperature dependences for
the thermodynamic quantities, the model is too simple to support a phase transition. However,
the 2-D and 3-D Ising models do support the Curie temperature phase transition.
15.3 STATISTICAL MECHANICS (THEORY)
Statistical mechanics starts with the elementary interactions among a system’s particles and
constructs the macroscopic thermodynamic properties such as specific heats. The essential
assumption is that all configurations of the system consistent with the constraints are possible.
In some simulations, such as the molecular dynamics ones in Chapter 16, “Simulating Matter
with Molecular Dynamics,” the problem is set up such that the energy of the system is fixed.
The states of this type of system are described by what is called a microcanonical ensemble. In
contrast, for the thermodynamic simulations we study in this chapter, the temperature, volume,
and number of particles remain fixed, and so we have what is called a canonical ensemble.
When we say that an object is at temperature T , we mean that the object’s atoms are
in thermodynamic equilibrium at temperature T such that each atom has an average energy
proportional to T . Although this may be an equilibrium state, it is a dynamic one in which the
object’s energy fluctuates as it exchanges energy with its environment (it is thermodynamics
after all). Indeed, one of the most illuminating aspects of the simulation we shall develop is its
visualization of the continual and random interchange of energy that occurs at equilibrium.
The energyEαj of state αj in a canonical ensemble is not constant but is distributed with
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
338 CHAPTER 15
probabilities P (αj) given by the Boltzmann distribution:
P(Eαj , T ) =
e−Eαj /kBT
Z(T )
, Z(T ) =
∑
αj
e−Eαj /kBT . (15.5)
Here k is Boltzmann’s constant, T is the temperature, and Z(T ) is the partition function,
a weighted sum over states. Note that the sums in (15.5) are over the individual states or
configurations of the system. Another formulation, such as the Wang–Landau algorithm in
Unit II, sums over the energies of the states of the system and includes a density-of-states
factor g(Ei) to account for degenerate states with the same energy. While the present sum over
states is a simpler way to express the problem (one less function), we shall see that the sum
over energies is more efficient numerically. In fact, in this unit we even ignore the partition
function Z(T ) because it cancels out when dealing with the ratio of probabilities.
15.3.1 Analytic Solutions
For very large numbers of particles, the thermodynamic properties of the 1-D Ising model can
be solved analytically and determine [P&B 94]
xmll U = 〈E〉 (15.6)
U
J
= −N tanh J
kBT
= −N e
J/kBT − e−J/kBT
eJ/kBT + e−J/kBT
=
N, kBT → 0,0, kBT →∞. (15.7)
The analytic results for the specific heat per particle and the magnetization are
C(kBT ) =
1
N
dU
dT
=
(J/kBT )2
cosh2(J/kBT )
(15.8)
M(kBT ) =
NeJ/kBT sinh(B/kBT )√
e2J/kBT sinh2(B/kBT ) + e−2J/kBT
. (15.9)
The 2-D Ising model has an analytic solution, but it was not easy to derive [Yang 52,
Huang 87]. Whereas the internal energy and heat capacity are expressed in terms of elliptic
integrals, the spontaneous magnetization per particle has the rather simple form
M(T ) =
0, T > Tc(1+z2)1/4(1−6z2+z4)1/8√
1−z2 , T < Tc,
(15.10)
kTc ' 2.269185J, z = e−2J/kBT , (15.11)
where the temperature is measured in units of the Curie temperature Tc.
15.4 METROPOLIS ALGORITHM
In trying to devise an algorithm that simulates thermal equilibrium, it is important to understand
that the Boltzmann distribution (15.5) does not require a system to remain in the state of lowest
energy but says that it is less likely for the system to be found in a higher-energy state than
in a lower-energy one. Of course, as T → 0, only the lowest energy state will be populated.
For finite temperatures we expect the energy to fluctuate by approximately kBT about the
equilibrium value.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 339
In their simulation of neutron transmission through matter, Metropolis, Rosenbluth,
Teller, and Teller [Metp 53] invented an algorithm to improve the Monte Carlo calculation
of averages. This Metropolis algorithm is now a cornerstone of computational physics. The
sequence of configurations it produces (a Markov chain) accurately simulates the fluctuations
that occur during thermal equilibrium. The algorithm randomly changes the individual spins
such that, on the average, the probability of a configuration occurring follows a Boltzmann
distribution. (We do not find the proof of this trivial or particularly illuminating.)
The Metropolis algorithm is a combination of the variance reduction technique discussed
in §6.7.1 and the von Neumann rejection technique discussed in §6.7.3. There we showed how
to make Monte Carlo integration more efficient by sampling random points predominantly
where the integrand is large and how to generate random points with an arbitrary probability
distribution. Now we would like to have spins flip randomly, have a system that can reach any
energy in a finite number of steps (ergodic sampling), have a distribution of energies described
by a Boltzmann distribution, yet have systems that equilibrate quickly enough to compute in
reasonable times.
The Metropolis algorithm is implemented via a number of steps. We start with a fixed
temperature and an initial spin configuration and apply the algorithm until a thermal equilib-
rium is reached (equilibration). Continued application of the algorithm generates the statistical
fluctuations about equilibrium from which we deduce the thermodynamic quantities such as
the magnetization M(T ). Then the temperature is changed, and the whole process is repeated
in order to deduce the T dependence of the thermodynamic quantities. The accuracy of the de-
duced temperature dependences provides convincing evidence for the validity of the algorithm.
Because the possible 2N configurations of N particles can be a very large number, the amount
of computer time needed can be very long. Typically, a small number of iterations ' 10N is
adequate for equilibration.
The explicit steps of the Metropolis algorithm are as follows.
1. Start with an arbitrary spin configuration αk = {s1, s2, . . . , sN}.
2. Generate a trial configuration αk+1 by
a. picking a particle i randomly and
b. flipping its spin.1
3. Calculate the energy Eαtr of the trial configuration.
4. If Eαtr ≤ Eαk , accept the trial by setting αk+1 = αtr.
5. If Eαtr > Eαk , accept with relative probabilityR = exp(−∆E/kBT ):
a. Choose a uniform random number 0 ≤ ri ≤ 1.
b. Set αk+1 =
αtr, if R ≥ rj (accept),αk, if R < rj (reject).
The heart of this algorithm is its generation of a random spin configuration αj (15.2) with
probability
P(Eαj , T ) ∝ e−Eαj /kBT . (15.12)
The technique is a variation of von Neumann rejection (stone throwing in § 6.5) in which a
random trial configuration is either accepted or rejected depending upon the value of the Boltz-
1Large-scale, practical computations make a full sweep in which every spin is updated once and then use this as the new trial
configuration. This is found to be more efficient and useful in removing some autocorrelations. (We thank G. Schneider for this
observation.)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
340 CHAPTER 15
Figure 15.2 A 1-D lattice of 100 spins aligned along the ordinate. Up spins are indicated by circles, and down spins
by blank spaces. The iteration number (“time”) dependence of the spins is shown along the abscissa.
Even though the system starts with all up spins (a “cold” start), the system is seen to form domains as
it equilibrates.
mann factor. Explicitly, the ratio of probabilities for a trial configuration of energy Et to that
of an initial configuration of energy Ei is
R = Ptr
Pi
= e−∆E/kBT , ∆E = Eαtr − Eαi . (15.13)
If the trial configuration has a lower energy (∆E ≤ 0), the relative probability will be greater
than 1 and we will accept the trial configuration as the new initial configuration without further
ado. However, if the trial configuration has a higher energy (∆E > 0), we will not reject it out
of hand but instead accept it with relative probability R = exp(−∆E/kBT ) < 1. To accept
a configuration with a probability, we pick a uniform random number between 0 and 1, and if
the probability is greater than this number, we accept the trial configuration; if the probability
is smaller than the chosen random number, we reject it. (You can remember which way this
goes by letting Eαtr → ∞, in which case P → 0 and nothing is accepted.) When the trial
configuration is rejected, the next configuration is identical to the preceding one.
How do you start? One possibility is to start with random values of the spins (a “hot”
start). Another possibility (Figure 15.2) is a “cold” start in which you start with all spins parallel
(J > 0) or antiparallel (J < 0). In general, one tries to remove the importance of the starting
configuration by letting the calculation run a while ('10N rearrangements) before calculating
the equilibrium thermodynamic quantities. You should get similar results for hot, cold, or
arbitrary starts, and by taking their average you remove some of the statistical fluctuations.
15.4.1 Metropolis Algorithm Implementation
1. Write a program that implements the Metropolis algorithm, that is, that produces a new
configuration αk+1 from the present configuration αk. (Alternatively, use the program
IsingViz.py shown in Listing 15.1.)
2. Make the key data structure in your program an array s[N] containing the values of the
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 341
spins si. For debugging, print out + and − to give the spin at each lattice point and
examine the pattern for different trial numbers.
3. The value for the exchange energy J fixes the scale for energy. Keep it fixed at J = 1.
(You may also wish to study antiferromagnets with J = −1, but first examine ferromag-
nets whose domains are easier to understand.)
4. The thermal energy kBT is in units of J and is the independent variable. Use kBT = 1
for debugging.
5. Use periodic boundary conditions on your chain to minimize end effects. This means
that the chain is a circle with the first and last spins adjacent to each other.
6. Try N ' 20 for debugging, and larger values for production runs.
7. Use the printout to check that the system equilibrates for
a. a totally ordered initial configuration (cold start); your simulation should resemble
Figure 15.2.
b. a random initial configuration (hot start).
15.4.2 Equilibration, Thermodynamic Properties (Assessment)
1. Watch a chain of N atoms attain thermal equilibrium when in contact with a heat bath.
At high temperatures, or for small numbers of atoms, you should see large fluctuations,
while at lower temperatures you should see smaller fluctuations.
2. Look for evidence of instabilities in which there is a spontaneous flipping of a large
number of spins. This becomes more likely for larger kBT values.
3. Note how at thermal equilibrium the system is still quite dynamic, with spins flipping all
the time. It is this energy exchange that determines the thermodynamic properties.
4. You may well find that simulations at small kBT (say, kBT ' 0.1 for N = 200) are
slow to equilibrate. Higher kBT values equilibrate faster yet have larger fluctuations.
5. Observe the formation of domains and the effect they have on the total energy. Regard-
less of the direction of spin within a domain, the atom–atom interactions are attractive
and so contribute negative amounts to the energy of the system when aligned. However,
the ↑↓ or ↓↑ interactions between domains contribute positive energy. Therefore you
should expect a more negative energy at lower temperatures where there are larger and
fewer domains.
6. Make a graph of average domain size versus temperature.
Listing 15.1 Ising.py implements the Metropolis algorithm for a 1-D Ising chain. 
# V Is in g . py : I s i n g model
from v i s u a l i m p o r t ∗
i m p o r t random
from v i s u a l . g raph i m p o r t ∗
# D i s p l a y f o r t h e a r r ow s
s c e n e = d i s p l a y ( x =0 , y =0 , wid th =700 , h e i g h t =200 , r a n g e =40 , t i t l e =’Ising spins’ )
# d i s p l a y f o r t h e e ne rg y
engraph = g d i s p l a y ( y =200 , wid th =700 , h e i g h t =300 , t i t l e =’Energy of a spin system’ ,
x t i t l e =’iteration’ , y t i t l e =’Energy ’ , xmax =500 , xmin =0 , ymax =5 , ymin=−5)
e n p l o t = g c u r ve ( c o l o r = c o l o r . ye l l o w ) # f o r t h e e ne rg y p l o t
N=30 # number o f s p i n s
B = 2 . # m a g n e t i c f i e l d
mu = . 3 3 # g mu ( gi romag . t i m e s Bohrs magneton )
J = . 2 0 # Exchange en e r gy
k = 1 . # Boltmann c o n s t a n t
T = 1 0 0 . # Tempera tu r e
s t a t e = z e r o s ( (N) ) # s p i n s s t a t e some up ( 1 ) some down ( 0 )
S= z e r o s ( (N) , F l o a t )
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
342 CHAPTER 15
t e s t = s t a t e # a t e s t s t a t e
random . seed ( ) # Seed random g e n e r a t o r
d e f en e rg y ( S ) : # Method t o c a l c e n e r gy
F i r s t T e r m = 0 .
SecondTerm = 0 . # Sum e ne rg y
f o r i i n r a n g e ( 0 ,N−2) :
F i r s t T e r m += S [ i ]∗S [ i + 1 ]
F i r s t T e r m ∗= −J
f o r i i n r a n g e ( 0 ,N−1) :
SecondTerm += S [ i ]
SecondTerm ∗= −B∗mu ;
r e t u r n ( F i r s t T e r m + SecondTerm ) ;
ES = en e r g y ( s t a t e ) # S t a t e , t e s t ’s energy
def spstate(state):
#plots the spins according to their state
for obj in scene.objects:
obj.visible=0 #to erase the previous arrows
j=0
for i in range(-N,N,2): #30 spins numbered from 0 to 29
if state[j]==-1: #case spin down
ypos=5 #we want spins at same height
else:
ypos=0
if 5*state[j]<0:
arrowcol= (1,1,1) #white arrow if spin down
else:
arrowcol=(0.7,0.8,0)
arrow(pos=(i,ypos,0),axis=(0,5*state[j],0),color=arrowcol) #plot arrow
j +=1
for i in range(0 ,N):
state[i] = -1 # Set initial state, all spins down
for obj in scene.objects:
obj.visible=0
spstate(state) #plots initial state: all spins down
ES=energy(state) #finds the energy of the spin system
#Here is the Metropolis algorithm
for j in range (1,500): # Change state and test
rate(3) #to be able to see the flipping
test = state #test is the previous spin state
r = int(N*random.random()); # Flip spin randomly
test[r] *= -1 #flips temporarily that spin
ET = energy(test) #finds the energy of the test configur.
p = math.exp((ES-ET)/(k*T)) #test with Boltzmann factor
enplot.plot(pos=(j,ES)) #adds a segment to the curve of energy
if p >= random.random(): #to see is the trial config. is accepted
state = test
spstate(state)
ES = ET
Thermodynamic Properties: For a given spin configuration αj , the energy and magnetiza-
tion are given by
Eαj = −J
N−1∑
i=1
sisi+1, Mj =
N∑
i=1
si. (15.14)
The internal energy U(T ) is just the average value of the energy,
U(T ) = 〈E〉, (15.15)
where the average is taken over a system in equilibrium. At high temperatures we expect
a random assortment of spins and so a vanishing magnetization. At low temperatures when
all the spins are aligned, we expect M to approach N/2. Although the specific heat can be
computed from the elementary definition
C =
1
N
dU
dT
, (15.16)
the numerical differentiation may be inaccurate since U has statistical fluctuations. A better
way to calculate the specific heat is to first calculate the fluctuations in energy occurring during
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 343
Figure 15.3 Simulation results for the energy, specific heat, and magnetization of a 1-D lattice of 100 spins as a
function of temperature.
kT kT
–0.8
–0.4
0.0
0 2 4 0 2 4
0.1
0.2
0.3
0
1
EC
M
0.5
M trials and then determine the specific heat from the fluctuations:
xmllU2 = 1
M
M∑
t=1
(Et)2, (15.17)
C =
1
N2
U2 − (U)2
kBT 2
=
1
N2
〈E2〉 − 〈E〉2
kBT 2
. (15.18)
1. Extend your program to calculate the internal energy U and the magnetizationM for the
chain. Do not recalculate entire sums when only one spin changes.
2. Make sure to wait for your system to equilibrate before you calculate thermodynamic
quantities. (You can check that U is fluctuating about its average.) Your results should
resemble Figure 15.3.
3. Reduce statistical fluctuations by running the simulation a number of times with different
seeds and taking the average of the results.
4. The simulations you run for small N may be realistic but may not agree with statistical
mechanics, which assumesN ' ∞ (you may assume thatN ' 2000 is close to infinity).
Check that agreement with the analytic results for the thermodynamic limit is better for
large N than small N .
5. Check that the simulated thermodynamic quantities are independent of initial conditions
(within statistical uncertainties). In practice, your cold and hot start results should agree.
6. Make a plot of the internal energy U as a function of kBT and compare it to the analytic
result (15.7).
7. Make a plot of the magnetizationM as a function of kBT and compare it to the analytic
result. Does this agree with how you expect a heated magnet to behave?
8. Compute the energy fluctuations U2 (15.17) and the specific heat C (15.18). Compare
the simulated specific heat to the analytic result (15.8).
15.4.3 Beyond Nearest Neighbors, 1-D (Exploration)
• Extend the model so that the spin–spin interaction (15.3) extends to next-nearest neigh-
bors as well as nearest neighbors. For the ferromagnetic case, this should lead to more
binding and less fluctuation because we have increased the couplings among spins and
thus increased the thermal inertia.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
344 CHAPTER 15
Figure 15.4 Left: Logarithm of the density of states log g(E) ∝ S versus the energy per particle for a 2-D Ising
model on an 8 × 8 lattice. Right: The histogram H(E) showing the number of states visited as a
function of the energy per particle. The aim of WLS is to make this function flat.
0
10
20
30
40
-2 -1 0 1 2
lo
g
 g
(E
)
E/N
0
4000
8000
–2 –1 0 1 2
H
(E
)
E/N
• Extend the model so that the ferromagnetic spin–spin interaction (15.3) extends to nearest
neighbors in two dimensions, and for the truly ambitious, three dimensions. Continue
using periodic boundary conditions and keep the number of particles small, at least to
start [G,T&C 06].
1. Form a square lattice and place
√
N spins on each side.
2. Examine the mean energy and magnetization as the system equilibrates.
3. Is the temperature dependence of the average energy qualitatively different from that of
the 1-D model?
4. Identify domains in the printout of spin configurations for small N .
5. Once your system appears to be behaving properly, calculate the heat capacity and mag-
netization of the 2-D Ising model with the same technique used for the 1-D model. Use
a total number of particles of 100 ≤ N ≤ 2000.
6. Look for a phase transition from ordered to unordered configurations by examining the
heat capacity and magnetization as functions of temperature. The former should diverge,
while the latter should vanish at the phase transition (Figure 15.5).
Exercise: Three fixed spin-12 particles interact with each other at temperature T = 1/kb such
that the energy of the system is
E = −(s1 s2 + s2 s3).
The system starts in the configuration ↑↓↑. Do a simulation by hand that uses the Metropolis
algorithm and the series of random numbers 0.5, 0.1, 0.9, 0.3 to determine the results of just
two thermal fluctuations of these three spins.
15.5 UNIT II. MAGNETS VIA WANG–LANDAU SAMPLING 
In Unit I we used a Boltzmann distribution to simulate the thermal properties of an Ising model.
There we described the probabilities for explicit spin states α with energy Eα for a system at
temperature T and summed over various configurations. An equivalent formulation describes
the probability that the system will have the explicit energy E at temperature T :
P(Ei, T ) = g(Ei)
e−Ei/kBT
Z(T )
, Z(T ) =
∑
Ei
g(Ei) e−Ei/kBT . (15.19)
Here kB is Boltzmann’s constant, T is the temperature, g(Ei) is the number of states of energy
Ei (i = 1, . . . ,M ), Z(T ) is the partition function, and the sum is still over all M states of the
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 345
Figure 15.5 The energy, specific heat, and magnetization as a function of temperature for a 2-D lattice with 40,000
spins. The values of C and E have been scaled to fit on the same plot as M. (Courtesy of J. Wetzel.)
–80000
–40000
40000
0
0 2 4 6 8 10
E
CV
M
kT
2-D Ising
Model
system but now with states of the same energy entering just once owing to g(Ei) accounting
for their degeneracy. Because we again apply the theory to the Ising model with its discrete
spin states, the energy also assumes only discrete values. If the physical system had an energy
that varied continuously, then the number of states in the interval E → E+dE would be given
by g(E) dE and g(E) would be called the density of states. As a matter of convenience, we
call g(Ei) the density of states even when dealing with discrete systems, although the term
“degeneracy factor” may be more precise.
Even as the Metropolis algorithm has been providing excellent service for more
than 50 years, recent literature shows increasing use of Wang–Landau sampling (WLS)
[WL 04, Clark]. Because WLS determines the density of states and the associated partition
function, it is not a direct substitute for the Metropolis algorithm and its simulation of thermal
fluctuations. However, we will see that WLS provides an equivalent simulation for the Ising
model.2 Nevertheless, there are cases where the Metropolis algorithm can be used but WLS
cannot, computation of the ground-state function of the harmonic oscillator (Unit III) being an
example.
The advantages of WLS is that it requires much shorter simulation times than the
Metropolis algorithm and provides a direct determination of g(Ei). For these reasons it has
shown itself to be particularly useful for first-order phase transitions where systems spend long
times trapped in metastable states, as well as in areas as diverse as spin systems, fluids, liquid
crystals, polymers, and proteins. The time required for a simulation becomes crucial when
large systems are modeled. Even a spin lattice as small as 8 × 8 has 264 ' 1.84 × 1019
configurations, and it would be too expensive to visit them all.
In Unit I we ignored the partition function when employing the Metropolis algorithm.
Now we focus on the partition function Z(T ) and the density-of-states function g(E). Because
g(E) is a function of energy but not temperature, once it has been deduced, Z(T ) and all
thermodynamic quantities can be calculated from it without having to repeat the simulation for
2We thank Oscar A. Restrepo of the Universidad de Antioquia for letting us use some of his material here.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
346 CHAPTER 15
each temperature. For example, the internal energy and the entropy are calculated directly as
U(T ) def= 〈E〉 =
∑
Ei
Ei g(Ei) e−Ei/kBT∑
Ei
g(Ei) e−Ei/kBT
, (15.20)
S = kBT ln g(Ei). (15.21)
The density of states g(Ei) will be determined by taking the equivalent of a random walk in
energy space. We flip a randomly chosen spin, record the energy of the new configuration,
and then keep walking by flipping more spins to change the energy. The table H(Ei) of the
number of times each energy Ei is attained is called the energy histogram (an example of why
it is called a histogram is given in Figure 15.4 on the right). If the walk were continued for
a very long time, the histogram H(Ei) would converge to the density of states g(Ei). Yet
with 1019–1030 steps required even for small systems, this direct approach is unrealistically
inefficient because the walk would rarely ever get away from the most probable energies.
Clever idea number 1 behind the Wang–Landau algorithm is to explore more of the
energy space by increasing the likelihood of walking into less probable configurations. This is
done by increasing the acceptance of less likely configurations while simultaneously decreasing
the acceptance of more likely ones. In other words, we want to accept more states for which the
density g(Ei) is small and reject more states for which g(Ei) is large (fret not these words, the
equations are simple). To accomplish this trick, we accept a new energy Ei with a probability
inversely proportional to the (initially unknown) density of states,
P(Ei) =
1
g(Ei)
, (15.22)
and then build up a histogram of visited states via a random walk.
The problem with clever idea number 1 is that g(Ei) is unknown. WLS’s clever idea 2
is to determine the unknown g(Ei) simultaneously with the construction of the random walk.
This is accomplished by improving the value of g(Ei) via the multiplication g(Ei)→ f g(Ei),
where f > 1 is an empirical factor. When this works, the resulting histogram H(Ei) becomes
“flatter” because making the small g(Ei) values larger makes it more likely to reach states with
small g(Ei) values. As the histogram gets flatter, we keep decreasing the multiplicative factor
f until it is satisfactory close to 1. At that point we have a flat histogram and a determination
of g(Ei).
At this point you may be asking yourself, “Why does a flat histogram mean that we have
determined g(Ei)?” Flat means that all energies are visited equally, in contrast to the peaked
histogram that would be obtained normally without the 1/g(Ei) weighting factor. Thus, if by
including this weighting factor we produce a flat histogram, then we have perfectly counter-
acted the actual peaking in g(Ei), which means that we have arrived at the correct g(Ei).
15.6 WANG–LANDAU SAMPLING
The steps in WLS are similar to those in the Metropolis algorithm, but now with use of the
density-of-states function g(Ei) rather than a Boltzmann factor:
1. Start with an arbitrary spin configuration αk = {s1, s2, . . . , sN} and with arbitrary
values for the density of states g(Ei) = 1, i = 1, . . . ,M , where M = 2N is the
number of states of the system.
2. Generate a trial configuration αk+1 by
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 347
a. picking a particle i randomly and
b. flipping i’s spin.
3. Calculate the energy Eαtr of the trial configuration.
4. If g(Eαtr) ≤ g(Eαk), accept the trial, that is, set αk+1 = αtr.
5. If g(Eαtr) > g(Eαk), accept the trial with probability P = g(Eαk)/(g(Eαtr):
a. choose a uniform random number 0 ≤ ri ≤ 1.
b. set αk+1 =
{
αtr, if P ≥ rj (accept),
αk, if P < rj (reject).
This acceptance rule can be expressed succinctly as
P(Eαk → Eαtr) = min
[
1,
g(Eαk)
g(Eαtr)
]
, (15.23)
which manifestly always accepts low-density (improbable) states.
6. One we have a new state, we modify the current density of states g(Ei) via the multi-
plicative factor f :
g(Eαk+1)→ f g(Eαk+1), (15.24)
and add 1 to the bin in the histogram corresponding to the new energy:
H(Eαk+1)→ H(Eαk+1) + 1. (15.25)
7. The value of the multiplier f is empirical. We start with Euler’s number f = e =
2.71828, which appears to strike a good balance between very large numbers of small
steps (small f ) and too rapid a set of jumps through energy space (large f ). Because the
entropy S = kB ln g(Ei) → kB[ln g(Ei) + ln f ], (15.24) corresponds to a uniform
increase by kB in entropy.
8. Even with reasonable values for f , the repeated multiplications in (15.24) lead to ex-
ponential growth in the magnitude of g. This may cause floating-point overflows and
a concordant loss of information [in the end, the magnitude of g(Ei) does not matter
since the function is normalized]. These overflows are avoided by working with loga-
rithms of the function values, in which case the update of the density of states (15.24)
becomes
ln g(Ei)→ ln g(Ei) + ln f. (15.26)
9. The difficulty with storing ln g(Ei) is that we need the ratio of g(Ei) values to cal-
culate the probability in (15.23). This is circumvented by employing the identity
x = exp(ln x) to express the ratio as
g(Eαk)
g(Eαtr)
= exp
[
ln
g(Eαk)
g(Eαtr)
]
= exp [ln g(Eαk)]− exp [ln g(Eαtr)] . (15.27)
In turn, g(Ek) = f × g(Ek) is modified to ln g(Ek)→ ln g(Ek) + ln f .
10. The random walk in Ei continues until a flat histogram of visited energy values is ob-
tained. The flatness of the histogram is tested regularly (every 10,000 iterations), and
the walk is terminated once the histogram is sufficiently flat. The value of f is then
reduced so that the next walk provides a better approximation to g(Ei). Flatness is
measured by comparing the variance in H(Ei) to its average. Although 90%–95% flat-
ness can be achieved for small problems like ours, we demand only 80% (Figure 15.4):
If
Hmax −Hmin
Hmax +Hmin
< 0.2, stop, let f →
√
f (ln f → ln f/2). (15.28)
11. Then keep the generated g(Ei) and reset the histogram values h(Ei) to zero.
12. The walks are terminated and new ones initiated until no significant correction to the
density of states is obtained. This is measured by requiring the multiplicative factor
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
348 CHAPTER 15
f ' 1 within some level of tolerance; for example, f ≤ 1 + 10−8. If the algorithm is
successful, the histogram should be flat (Figure 15.4) within the bounds set by (15.28).
13. The final step in the simulation is normalization of the deduced density of states g(Ei).
For the Ising model with N up or down spins, a normalization condition follows from
knowledge of the total number of states [Clark]:∑
Ei
g(Ei) = 2N ⇒ g(norm)(Ei) =
2N∑
Ei
g(Ei)
g(Ei). (15.29)
Because the sum in (15.29) is most affected by those values of energy where g(Ei) is
large, it may not be precise for the low-Ei densities that contribute little to the sum.
Accordingly, a more precise normalization, at least if your simulation has done a good
job in occupying all energy states, is to require that there are just two grounds states
with energies E = −2N (one with all spins up and one with all spins down):∑
Ei=−2N
g(Ei) = 2. (15.30)
In either case it is good practice to normalize g(Ei) with one condition and then use the
other as a check.
15.6.1 WLS Ising Model Implementation
We assume an Ising model with spin–spin interactions between nearest neighbors located in an
L× L lattice (Figure 15.6). To keep the notation simple, we set J = 1 so that
E = −
N∑
i↔j
σiσj , (15.31)
where ↔ indicates nearest neighbors. Rather than recalculate the energy each time a spin is
flipped, only the difference in energy is computed. For example, for eight spins in a 1-D array,
− Ek = σ0σ1 + σ1σ2 + σ2σ3 + σ3σ4 + σ4σ5 + σ5σ6 + σ6σ7 + σ7σ0, (15.32)
where the 0–7 interaction arises because we assume periodic boundary conditions. If spin 5 is
flipped, the new energy is
− Ek+1 = σ0σ1 + σ1σ2 + σ2σ3 + σ3σ4 − σ4σ5 − σ5σ6 + σ6σ7 + σ7σ0, (15.33)
and the difference in energy is
∆E = Ek+1 − Ek = 2(σ4 + σ6)σ5. (15.34)
This is cheaper to compute than calculating and then subtracting two energies.
When we advance to two dimensions with the 8× 8 lattice in Figure 15.6, the change in
energy when spin σi,j flips is
∆E = 2σi,j(σi+1,j + σi−1,j + σi,j+1 + σi,j−1), (15.35)
which can assume the values −8,−4, 0, 4, and 8. There are two states of minimum energy
−2N for a 2-D system with N spins, and they correspond to all spins pointing in the same
direction (either up or down). The maximum energy is +2N , and it corresponds to alternating
spin directions on neighboring sites. Each spin flip on the lattice changes the energy by four
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 349
Figure 15.6 The numbering scheme used for our 8× 8 2-D lattice of spins.
y
.....
.....
.....
.....
.....
.....
0 1 2 43 5 6 7
8       9        10      11      12       13     14        15
56      57       58      59      60      61       61      63
x
units between these limits, and so the values of the energies are
Ei = −2N, −2N + 4, −2N + 8, . . . , 2N − 8, 2N − 4, 2N. (15.36)
These energies can be stored in a uniform 1-D array via the simple mapping
E′ = (E + 2N)/4 ⇒ E′ = 0, 1, 2, . . . , N. (15.37)
Listing 15.2 displays our implementation of Wang–Landau sampling to calculate the density of
states and internal energy U(T ) (15.20). We used it to obtain the entropy S(T ) and the energy
histogram H(Ei) illustrated in Figure 15.4. Other thermodynamic functions can be obtained
by replacing the E in (15.20) with the appropriate variable. The results look like those in
Figure 15.5. A problem that may be encountered when calculating these variables is that the
sums in (15.20) can become large enough to cause overflows, even though the ratio would not.
You
Listing 15.2 WangLandau.py simulates the 2-D Ising model using Wang–Landau sampling to compute the density
of states and from that the internal energy. 
# WangLandau . py : Wang Landau a l g o r i t h m f o r 2−D s p i n sys tem
""" Author in Java: Oscar A. Restrepo,
Universidad de Antioquia, Medellin, Colombia
Each time that fac changes, a new histogrm is generated.
Only the first Histogram is plotted to reduce computational time"""
from v i s u a l i m p o r t ∗ ; i m p o r t random ; from v i s u a l . g raph i m p o r t ∗
L = 8 ; N = ( L∗L )
# n e x t l i n e s f o r g r a p h i c s
e n t g r = g d i s p l a y ( x =0 , y =0 , wid th =500 , h e i g h t =250 , t i t l e =’Density of States’ , x t i t l e = ’E/N’ ,
y t i t l e =’log g(E)’ , xmax = 2 . 0 , xmin = −2.0 , ymax =45 , ymin = 0)
e n t r p = g cu r ve ( c o l o r = c o l o r . ye l low , d i s p l a y = e n t g r )
e n e r g y g r = g d i s p l a y ( x = 0 , y = 250 , wid th = 500 , h e i g h t = 250 , t i t l e = ’Energy versus T’ , x t i t l e
= ’T’ , y t i t l e = ’U(T)/N’ , xmax = 8 . 0 , xmin = 0 , ymax = 0 . 0 , ymin = − 2 . 0 )
e n e r g = gc u rv e ( c o l o r = c o l o r . cyan , d i s p l a y = e n e r g y g r )
h i s t o g r = d i s p l a y ( x = 0 , y = 500 , wid th = 500 , h e i g h t = 300 , t i t l e = ’First histogram: H(E) vs.
E/N, corresponds to log(f) = 1, ’ )
h i s t o = c u r v e ( x = r a n g e ( 0 , N + 1) , c o l o r = c o l o r . red , d i s p l a y = h i s t o g r )
x a x i s = c u r v e ( pos = [ ( − N, − 10) , (N, − 10) ] )
minE = l a b e l ( t e x t = ’ - 2’ , pos = ( − N + 3 , − 15) , box = 0)
maxE = l a b e l ( t e x t = ’2’ , pos = (N − 3 , − 15) , box = 0)
zeroE = l a b e l ( t e x t = ’0’ , pos = ( 0 , − 15) , box = 0)
t i cm = c u r v e ( pos = [ ( − N, − 10) , ( − N, − 13) ] )
t i c 0 = c u r v e ( pos = [ ( 0 , − 10) , ( 0 , − 13) ] )
t icM = c u r v e ( pos = [ ( N, − 10) , (N, − 13) ] )
e n r = l a b e l ( t e x t = ’E/N’ , pos = (N/ 2 , − 15) , box = 0)
sp = z e r o s ( ( L , L ) ) # Gr id s i z e , s p i n s
h i s t = z e r o s ( (N + 1) )
p r h i s t = z e r o s ( (N + 1) ) # H i s t o g r a m s
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
350 CHAPTER 15
S = z e r o s ( (N + 1) , F l o a t ) # En t ropy = l o g g ( E )
d e f iE ( e ) : r e t u r n i n t ( ( e + 2∗N) / 4 )
d e f I n t E n e r g y ( ) :
e x p o n e n t = 0 . 0
f o r T i n a r a n g e ( 0 . 2 , 8 . 2 , 0 . 2 ) : # S e l e c t lambda max
Ener = − 2∗N
maxL = 0 . 0 # I n i t i a l i z e
f o r i i n r a n g e ( 0 , N + 1) :
i f S [ i ] ! = 0 and ( S [ i ] − Ener / T )>maxL :
maxL = S [ i ] − Ener / T
Ener = Ener + 4
sumdeno = 0
sumnume = 0
Ener = − 2∗N
f o r i i n r a n g e ( 0 , N) :
i f S [ i ] != 0 :
e x p o n e n t = S [ i ] − Ener / T − maxL
sumnume += Ener∗math . exp ( e x p o n e n t )
sumdeno += math . exp ( e x p o n e n t )
Ener = Ener + 4 . 0
U = sumnume / sumdeno /N # i n t e r n a l e ne r gy U( T ) /N
e n e r g . p l o t ( pos = ( T , U) )
d e f WL( ) : # Wang − Landau s a m p l in g
Hinf = 1 . e10 # i n i t i a l v a l u e s f o r His togram
Hsup = 0 .
t o l = 1 . e−3 # t o l e r a n c e , s t o p s t h e a l g o r i t h m
i p = z e r o s ( L )
im = z e r o s ( L ) # BC R or down , L or up
h e i g h t = abs ( Hsup − Hinf ) / 2 . # I n i t i a l i z e h i s t o g r a m
ave = ( Hsup + Hinf ) / 2 . # a b o u t a v e r a g e o f h i s t o g r a m
p e r c e n t = h e i g h t / ave
f o r i i n r a n g e ( 0 , L ) :
f o r j i n r a n g e ( 0 , L ) : sp [ i , j ] = 1 # I n i t i a l s p i n s
f o r i i n r a n g e ( 0 , L ) :
i p [ i ] = i + 1
im [ i ] = i − 1 # Case p lus , minus
i p [ L − 1] = 0
im [ 0 ] = L − 1 # B o r d e r s
Eold = − 2∗N # I n i t i a l i z e e ne r gy
f o r j i n r a n g e ( 0 , N + 1) : S [ j ] = 0 # En t ropy i n i t i a l i z e d
i t e r = 0
f a c = 1 # I n i t i a l f a c t o r l n e
w h i l e f a c > t o l :
i = i n t (N∗random . random ( ) ) # S e l e c t random s p i n
xg = i%L
yg = i / L # L o c a l i z e x , y , g r i d p o i n t
Enew = Eold + 2∗( sp [ i p [ xg ] , yg ] + sp [ im [ xg ] , yg ] + sp [ xg , i p [ yg ] ]
+ sp [ xg , im [ yg ] ] ) ∗ sp [ xg , yg ] # Change e n e r gy
d e l t a S = S [ iE ( Enew ) ] − S [ iE ( Eold ) ] # Change e n t r o p y
i f d e l t a S <= 0 or random . random ( ) < math . exp ( − d e l t a S ) :
Eold = Enew ;
sp [ xg , yg ] ∗= − 1 # F l i p s p i n
S [ iE ( Eold ) ] += f a c ; # Change e n t r o p y
i f i t e r %10000 == 0 : # Check f l a t n e s s e v e r y 10000 sweeps
f o r j i n r a n g e ( 0 , N + 1) :
i f j == 0 :
Hsup = 0
Hinf = 1 e10 # I n i t i a l i z e new h i s t o g r a m
i f h i s t [ j ] == 0 : c o n t i n u e # E n e r g i e s n e v e r v i s i t e d
i f h i s t [ j ] > Hsup :
Hsup = h i s t [ j ]
i f h i s t [ j ] < Hinf : Hinf = h i s t [ j ]
h e i g h t = Hsup − Hinf
ave = Hsup + Hinf
p e r c e n t = 1 .0∗ h e i g h t / ave # 1 . 0 t o make i t f l o a t number
i f p e r c e n t < 0 . 3 : # His togram f l a t ?
p r i n t " iter " , i t e r , " log(f) " , f a c
f o r j i n r a n g e ( 0 , N + 1) :
p r h i s t [ j ] = h i s t [ j ] # t o p l o t
h i s t [ j ] = 0 # Save h i s t
f a c ∗= 0 . 5 # E q u i v a l e n t t o l o g ( s q r t ( f ) )
i t e r += 1
h i s t [ iE ( Eold ) ] += 1 # Change h i s t o g r a m , add 1 , u p d a t e
i f f a c >= 0 . 5 : # j u s t show t h e f i r s t h i s t o g r a m
f o r j i n r a n g e ( 0 , N + 1) :
o r d e r = j ∗4 − 2∗N
h i s t o . x [ j ] = 2 .0∗ j − N
h i s t o . y [ j ] = 0 .025∗ h i s t [ j ] − 10
d e l t a S = 0 . 0
p r i n t "wait because iter > 13 000 000" # n o t a lways t h e same
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 351
WL( ) # C a l l Wang Landau a l g o r i t h m
d e l t a S = 0 . 0
f o r j i n r a n g e ( 0 , N + 1) :
o r d e r = j ∗4 − 2∗N
d e l t a S = S [ j ] − S [ 0 ] + math . l o g ( 2 )
i f S [ j ] != 0 : e n t r p . p l o t ( pos = ( 1 .∗ o r d e r /N, d e l t a S ) ) # p l o t e n t r o p y
I n t E n e r g y ( ) ;
p r i n t "Done"
work around that by factoring out a common large factor; for example,∑
Ei
X(Ei) g(Ei) e−Ei/kBT = eλ
∑
Ei
X(Ei) eln g(Ei)−Ei/kBT−λ, (15.38)
where λ is the largest value of ln g(Ei)−Ei/kBT at each temperature. The factor eλ does not
actually need to be included in the calculation of the variable because it is present in both the
numerator and denominator and so cancels out.
15.6.2 WLS Ising Model Assessment
Repeat the assessment conducted in §15.4.2 for the thermodynamic properties of the Ising
model but use WLS in place of the Metropolis algorithm.
15.7 UNIT III. FEYNMAN PATH INTEGRALS 
Problem: As is well known, a classical particle attached to a linear spring undergoes simple
harmonic motion with a position in space as a function of time given by x(t) = A sin(ω0t+φ).
Your problem is to take this classical space-time trajectory x(t) and use it to generate the
quantum wave function ψ(x, t) for a particle bound in a harmonic oscillator potential.
15.8 FEYNMAN’S SPACE-TIME PROPAGATION (THEORY)
Feynman was looking for a formulation of quantum mechanics that gave a more direct connec-
tion to classical mechanics than does Schrödinger theory and that made the statistical nature
of quantum mechanics evident from the start. He followed a suggestion by Dirac that Hamil-
ton’s principle of least action, which can be used to derive classical mechanics, may be the
h̄ → 0 limit of a quantum least-action principle. Seeing that Hamilton’s principle deals with
the paths of particles through space-time, Feynman posultated that the quantum wave function
describing the propagation of a free particle from the space-time point a = (xa,ta) to the point
b = (xb, tb) can expressed as [F&H 65]
ψ(xb, tb) =
∫
dxaG(xb, tb;xa, ta)ψ(xa, ta), (15.39)
where G is the Green’s function or propagator
G(xb, tb;xa, ta) ≡ G(b, a) =
√
m
2πi(tb − ta)
exp
[
i
m(xb − xa)2
2(tb − ta)
]
. (15.40)
Equation (15.39) is a form of Huygens’s wavelet principle in which each point on the wavefront
ψ(xa, ta) emits a spherical wavelet G(b; a) that propagates forward in space and time. It states
that a new wavefront ψ(xb, tb) is created by summation over and interference with all the other
wavelets.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
352 CHAPTER 15
Figure 15.7 A collection of paths connecting the initial space-time point A to the final point B. The solid line is
the trajectory followed by a classical particle, while the dashed lines are additional paths sampled by a
quantum particle. A classical particle somehow “knows” ahead of time that travel along the classical
trajectory minimizes the action S.
Time
A
B
t
b
x
b
x
a
t
a
Position
Feynman imagined that another way of interpreting (15.39) is as a form of Hamilton’s
principle in which the probability amplitude (wave function ψ) for a particle to be at B is
equal to the sum over all paths through space-time originating at time A and ending at B
(Figure 15.7). This view incorporates the statistical nature of quantum mechanics by having
different probabilities for travel along the different paths. All paths are possible, but some are
more likely than others. (When you realize that Schrödinger theory solves for wave functions
and considers paths a classical concept, you can appreciate how different it is from Feynman’s
view.) The values for the probabilities of the paths derive from Hamilton’s classical principle
of least action:
The most general motion of a physical particle moving along the classical tra-
jectory x̄(t) from time ta to tb is along a path such that the action S[x̄(t)] is an
extremum:
δS[x̄(t)] = S[x̄(t) + δx(t)]− S[x̄(t)] = 0, (15.41)
with the paths constrained to pass through the endpoints:
δ(xa) = δ(xb) = 0.
This formulation of classical mechanics, which is based on the calculus of variations, is equiv-
alent to Newton’s differential equations if the action S is taken as the line integral of the La-
grangian along the path:
S[x̄(t)] =
∫ tb
ta
dtL [x(t), ẋ(t)] , L = T [x, ẋ]− V [x]. (15.42)
Here T is the kinetic energy, V is the potential energy, ẋ = dx/dt, and square brackets indicate
a functional3 of the function x(t) and ẋ(t).
3A functional is a number whose value depends on the complete behavior of some function and not just on its behavior at one
point. For example, the derivative f ′(x) depends on the value of f at x, yet the integral I[f ] =
∫ b
a dx f(x) depends on the entire
function and is therefore a functional of f .
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 353
pt
Figure 15.8 Left: The probability distribution for the harmonic oscillator ground state as determined with a path-
integral calculation (the classical result has maxima at the two turning points). Right: A space-time
trajectory used as a quantum path.
-40 -20 0 20 40
Position
0
0.05
0.1
0.15
0.2
P
ro
b
a
b
ili
ty
quantum
classical
0 20 40 60 80 100
Time
-2
-1
0
1
2
P
o
s
it
io
n
Feynman observed that the classical action for a free particle (V = 0),
S[b, a] =
m
2
(ẋ)2 (tb − ta) =
m
2
(xb − xa)2
tb − ta
, (15.43)
is related to the free-particle propagator (15.40) by
G(b, a) =
√
m
2πi(tb − ta)
eiS[b,a]/h̄. (15.44)
This is the much sought connection between quantum mechanics and Hamilton’s principle.
Feynman then postulated a reformulation of quantum mechanics that incorporated its statistical
aspects by expressing G(b, a) as the weighted sum over all paths connecting a to b,
G(b, a) =
∑
paths
eiS[b,a]/h̄ (path integral). (15.45)
Here the classical action S (15.42) is evaluated along different paths (Figure 15.7), and the ex-
ponential of the action is summed over paths. The sum (15.45) is called a path integral because
it sums over actions S[b, a], each of which is an integral (on the computer an integral and a sum
are the same anyway). The essential connection between classical and quantum mechanics is
the realization that in units of h̄ ' 10−34 Js, the action is a very large number, S/h̄ ≥ 1020, and
so even though all paths enter into the sum (15.45), the main contributions come from those
paths adjacent to the classical trajectory x̄. In fact, because S is an extremum for the classical
trajectory, it is a constant to first order in the variation of paths, and so nearby paths have phases
that vary smoothly and relatively slowly. In contrast, paths far from the classical trajectory are
weighted by a rapidly oscillating exp(iS/h̄), and when many are included, they tend to cancel
each other out. In the classical limit, h̄→ 0, only the single classical trajectory contributes and
(15.45) becomes Hamilton’s principle of least action! In Figure 15.8 we show an example of a
trajectory used in path-integral calculations.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
354 CHAPTER 15
15.8.1 Bound-State Wave Function (Theory)
Although you may be thinking that you have already seen enough expressions for the Green’s
function, there is yet another one we need for our computation. Let us assume that the Hamil-
tonian H̃ supports a spectrum of eigenfunctions,
H̃ψn = Enψn,
each labeled by the index n. Because H̃ is Hermitian, the solutions form a complete orthonor-
mal set in which we may expand a general solution:
ψ(x, t) =
∞∑
n=0
cn e
−iEnt ψn(x), cn =
∫ +∞
−∞
dxψ∗n(x)ψ(x, t = 0), (15.46)
where the value for the expansion coefficients cn follows from the orthonormality of the ψn’s.
If we substitute this cn back into the wave function expansion (15.46), we obtain the identity
ψ(x, t) =
∫ +∞
−∞
dx0
∑
n
ψ∗n(x0)ψn(x)e
−iEntψ(x0, t = 0). (15.47)
Comparison with (15.39) yields the eigenfunction expansion for G:
G(x, t;x0, t0 = 0) =
∑
n
ψ∗n(x0)ψn(x)e
−iEnt. (15.48)
We relate this to the bound-state wave function (recall that our problem is to calculate that) by
(1) requiring all paths to start and end at the space position x0 = x, (2) by taking t0 = 0, and
(3) by making an analytic continuation of (15.48) to negative imaginary time (permissable for
analytic functions):
G(x,−iτ ;x, 0) =
∑
n
|ψn(x)|2e−Enτ = |ψ0|2e−E0τ + |ψ1|2e−E1τ + · · ·
⇒ |ψ0(x)|2 = lim
τ→∞
eE0τG(x,−iτ ;x, 0) . (15.49)
The limit here corresponds to long imaginary times τ , after which the parts of ψ with higher
energies decay more quickly, leaving only the ground state ψ0.
Equation (15.49) provides a closed-form solution for the ground-state wave function
directly in terms of the propagator G. Although we will soon describe how to compute this
equation, look now at Figure 15.8 showing some results of a computation. Although we start
with a probability distribution that peaks near the classical turning points at the edges of the
well, after a large number of iterations we end up with a distribution that resembles the expected
Gaussian. On the right we see a trajectory that has been generated via statistical variations
about the classical trajectory x(t) = A sin(ω0t+ φ).
15.8.2 Lattice Path Integration (Algorithm)
Because both time and space are integrated over when evaluating a path integral, we set up a
lattice of discrete points in space-time and visualize a particle’s trajectory as a series of straight
lines connecting one time to the next (Figure 15.9). We divide the time between points A and
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 355
Figure 15.9 Left: A path through the space-time lattice that starts and ends at xa = xb. The action is an integral over
this path, while the path integral is a sum of integrals over all paths. The dotted path BD is a transposed
replica of path AC. Right: The dashed path joins the initial and final times in two equal time steps; the
solid curve uses N steps each of size ε. The position of the curve at time tj defines the position xj.

ta
tb
ti
xa
tb
ta
tj
xa
B into N equal steps of size ε and label them with the index j:
ε
def=
tb − ta
N
⇒ tj = ta + jε (j = 0, N). (15.50)
Although it is more precise to use the actual positions x(tj) of the trajectory at the times tj to
determine the xjs (as in Figure 15.9), in practice we discretize space uniformly and have the
links end at the nearest regular points. Once we have a lattice, it is easy to evaluate derivatives
or integrals on a link4::
dxj
dt
' xj − xj−1
tj − tj−1
=
xj − xj−1
ε
, (15.51)
Sj 'Lj ∆t '
1
2
m
(xj − xj−1)2
ε
− V (xj)ε, (15.52)
where we have assumed that the Lagrangian is constant over each link.
Lattice path integration is based on the composition theorem for propagators:
G(b, a) =
∫
dxj G(xb, tb;xj , tj)G(xj , tj ;xa, ta) (ta < tj , tj < tb). (15.53)
For a free particle this yields
G(b, a) =
√
m
2πi(tb − tj)
√
m
2πi(tj − ta)
∫
dxj e
i(S[b,j]+S[j,a])
=
√
m
2πi(tb − ta)
∫
dxj e
iS[b,a], (15.54)
4Even though Euler’s rule has a large error, it is often use in lattice calculations because of its simplicity. However, if the
Lagrangian contains second derivatives, you should use the more precise central-difference method to avoid singularities.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
356 CHAPTER 15
where we have added the actions since line integrals combine as S[b, j]+S[j, a] = S[b, a]. For
the N -linked path in Figure 15.9, equation (15.53) becomes
G(b, a) =
∫
dx1 · · · dxN−1 eiS[b,a], S[b, a] =
N∑
j=1
Sj , (15.55)
where Sj is the value of the action for link j. At this point the integral over the single path
shown in Figure 15.9 has become an N -term sum that becomes an infinite sum as the time step
ε approaches zero.
To summarize, Feynman’s path-integral postulate (15.45) means that we sum over all
paths connecting A to B to obtain Green’s function G(b, a). This means that we must sum
not only over the links in one path but also over all the different paths in order to produce the
variation in paths required by Hamilton’s principle. The sum is constrained such that paths
must pass through A and B and cannot double back on themselves (causality requires that
particles move only forward in time). This is the essence of path integration. Because we are
integrating over functions as well as along paths, the technique is also known as functional
integration.
The propagator (15.45) is the sum over all paths connecting A to B, with each path
weighted by the exponential of the action along that path, explicitly:
G(x, t;x0, t0) =
∑∫
dx1 dx2 · · · dxN−1eiS[x,x0], (15.56)
S[x, x0] =
N−1∑
j=1
S[xj+1, xj ] '
N−1∑
j=1
L (xj , ẋj) ε, (15.57)
where L(xj , ẋj) is the average value of the Lagrangian on link j at time t = jε. The computa-
tion is made simpler by assuming that the potential V (x) is independent of velocity and does
not depend on other x values (local potential). Next we observe thatG is evaluated with a nega-
tive imaginary time in the expression (15.49) for the ground-state wave function. Accordingly,
we evaluate the Lagrangian with t = −iτ :
L (x, ẋ) = T − V (x) = +1
2
m
(
dx
dt
)2
− V (x), (15.58)
⇒ L
(
x,
dx
−idτ
)
= −1
2
m
(
dx
dτ
)2
− V (x). (15.59)
We see that the reversal of the sign of the kinetic energy in L means that L now equals the
negative of the Hamiltonian evaluated at a real positive time t = τ :
H
(
x,
dx
dτ
)
=
1
2
m
(
dx
dτ
)2
+ V (x) = E, (15.60)
⇒ L
(
x,
dx
−idτ
)
= −H
(
x,
dx
dτ
)
. (15.61)
In this way we rewrite the t-path integral of L as a τ -path integral of H and so express the
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 357
action and Green’s function in terms of the Hamiltonian:
S[j + 1, j] =
∫ tj+1
tj
L(x, t) dt=−i
∫ τj+1
τj
H(x, τ) dτ, (15.62)
⇒ G(x,−iτ ;x0, 0) =
∫
dx1 . . . dxN−1 e
−
∫ τ
0 H(τ
′)dτ ′ , (15.63)
where the line integral of H is over an entire trajectory. Next we express the path integral in
terms of the average energy of the particle on each link, Ej = Tj + Vj , and then sum over
links5 to obtain the summed energy E :∫
H(τ) dτ '
∑
j
εEj = εE({xj}), (15.64)
E({xj})
def=
N∑
j=1
[
m
2
(
xj − xj−1
ε
)2
+ V
(
xj + xj−1
2
)]
. (15.65)
In (15.65) we have approximated each path link as a straight line, used Euler’s derivative rule to
obtain the velocity, and evaluated the potential at the midpoint of each link. We now substitute
this G into our solution (15.49) for the ground-state wave function in which the initial and final
points in space are the same:
lim
τ→∞
G(x,−iτ, x0 = x, 0)∫
dxG(x,−iτ, x0 = x, 0)
=
∫
dx1 · · · dxN−1 exp
[
−
∫ τ
0 Hdτ
′]∫
dx dx1 · · · dxN−1 exp
[
−
∫ τ
0 Hdτ
′
]
⇒ |ψ0(x)|2 =
1
Z
lim
τ→∞
∫
dx1 · · · dxN−1 e−εE , (15.66)
Z = lim
τ→∞
∫
dx dx1 · · · dxN−1e−εE . (15.67)
The similarity of these expressions to thermodynamics, even with a partition function Z, is no
accident; by making the time parameter of quantum mechanics imaginary, we have converted
the time-dependent Schrödinger equation to the heat diffusion equation:
i
∂ψ
∂(−iτ)
=
−∇2
2m
ψ ⇒ ∂ψ
∂τ
=
∇2
2m
ψ. (15.68)
It is not surprising then that the sum over paths in Green’s function has each path weighted
by the Boltzmann factor P = e−εE usually associated with thermodynamics. We make the
connection complete by identifying the temperature with the inverse time step:
P = e−εE = e−E/kBT ⇒ kBT =
1
ε
≡ h̄
ε
. (15.69)
Consequently, the ε → 0 limit, which makes time continuous, is a “high-temperature” limit.
The τ → ∞ limit, which is required to project the ground-state wave function, means that we
must integrate over a path that is long in imaginary time, that is, long compared to a typical
time h̄/∆E. Just as our simulation of the Ising model in Unit I required us to wait a long time
while the system equilibrated, so the present simulation requires us to wait a long time so that
all but the ground-state wave function has decayed. Alas, this is the solution to our problem
of finding the ground-state wave function.
To summarize, we have expressed the Green’s function as a path integral that requires
integration of the Hamiltonian along paths and a summation over all the paths (15.66). We
5In some cases, such as for an infinite square well, this can cause problems if the trial link causes the energy to be infinite. In
that case, one can modify the algorithm to use the potential at the beginning of a link
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
358 CHAPTER 15
evaluate this path integral as the sum over all the trajectories in a space-time lattice. Each
trial path occurs with a probability based on its action, and we use the Metropolis algorithm to
include statistical fluctuation in the links, as if they are in thermal equilibrium. This is similar
to our work with the Ising model in Unit I, however now, rather than reject or accept a flip in
spin based on the change in energy, we reject or accept a change in a link based on the change
in energy. The more iterations we let the algorithm run for, the more time the deduced wave
function has to equilibrate to the ground state.
In general, Monte Carlo Green’s function techniques work best if we start with a good
guess at the correct answer and have the algorithm calculate variations on our guess. For the
present problem this means that if we start with a path in space-time close to the classical tra-
jectory, the algorithm may be expected to do a good job at simulating the quantum fluctuations
about the classical trajectory. However, it does not appear to be good at finding the classical
trajectory from arbitrary locations in space-time. We suspect that the latter arises from δS/h̄
being so large that the weighting factor exp(δS/h̄) fluctuates wildly (essentially averaging out
to zero) and so loses its sensitivity.
15.8.2.1 A Time-Saving Trick
As we have formulated the computation, we pick a value of x and perform an expensive com-
putation of line integrals over all space and time to obtain |ψ0(x)|2 at one x. To obtain the
wave function at another x, the entire simulation must be repeated from scratch. Rather than
go through all that trouble again and again, we will compute the entire x dependence of the
wave function in one fell swoop. The trick is to insert a delta function into the probability
integral (15.66), thereby fixing the initial position to be x0, and then to integrate over all values
for x0:
|ψ0(x)|2 =
∫
dx1 · · · dxN e−εE(x,x1,...) (15.70)
=
∫
dx0 · · · dxNδ(x− x0) e−εE(x,x1,...). (15.71)
This equation expresses the wave function as an average of a delta function over all paths, a
procedure that might appear totally inappropriate for numerical computation because there is
tremendous error in representing a singular function on a finite-word-length computer. Yet
when we simulate the sum over all paths with (15.71), there will always be some x value
for which the integral is nonzero, and we need to accumulate only the solution for various
(discrete) x values to determine |ψ0(x)|2 for all x.
To understand how this works in practice, consider path AB in Figure 15.9 for which
we have just calculated the summed energy. We form a new path by having one point on the
chain jump to point C (which changes two links). If we replicate section AC and use it as the
extension AD to form the top path, we see that the path CBD has the same summed energy
(action) as path ACB and in this way can be used to determine |ψ(x′j)|2. That being the case,
once the system is equilibrated, we determine new values of the wave function at new locations
x′j by flipping links to new values and calculating new actions. The more frequently some xj
is accepted, the greater the wave function at that point.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 359
15.8.3 Lattice Implementation
The program QMC.py in Listing 15.3 evaluates the integral (15.45) by finding the aver-
age of the integrand δ(x0 − x) with paths distributed according to the weighting function
exp[−εE(x0, x1, . . . , xN )]. The physics enters via (15.73), the calculation of the summed en-
ergy E(x0, x1, . . . , xN ). We evaluate the action integral for the harmonic oscillator potential
V (x) =
1
2
x2 (15.72)
and for a particle of massm = 1. Using a convenient set of natural units, we measure lengths in√
1/mω ≡
√
h̄/mω = 1 and times in 1/ω = 1. Correspondingly, the oscillator has a period
T = 2π. Figure 15.8 shows results from an application of the Metropolis algorithm. In this
computation we started with an initial path close to the classical trajectory and then examined
1
2 million variations about this path. All paths are constrained to begin and end at x = 1 (which
turns out to be somewhat less than the maximum amplitude of the classical oscillation).
When the time difference tb − ta equals a short time like 2T , the system has not had
enough time to equilibrate to its ground state and the wave function looks like the probability
distribution of an excited state (nearly classical with the probability highest for the particle to
be near its turning points where its velocity vanishes). However, when tb− ta equals the longer
time 20T , the system has had enough time to decay to its ground state and the wave function
looks like the expected Gaussian distribution. In either case (Figure 15.8 right), the trajectory
through space-time fluctuates about the classical trajectory. This fluctuation is a consequence
of the Metropolis algorithm occasionally going uphill in its search; if you modify the program
so that searches go only downhill, the space-time trajectory will be a very smooth trigonometric
function (the classical trajectory), but the wave function, which is a measure of the fluctuations
about the classical trajectory, will vanish! The explicit steps of the calculation are
1. Construct a grid of N time steps of length ε (Figure 15.9). Start at t = 0 and extend to
time τ = Nε [this means N time intervals and (N + 1) lattice points in time]. Note that
time always increases monotonically along a path.
2. Construct a grid of M space points separated by steps of size δ. Use a range of x values
several time larger than the characteristic size or range of the potential being used and
start with M ' N .
3. When calculating the wave function, any x or t value falling between lattice points should
be assigned to the closest lattice point.
4. Associate a position xj with each time τj , subject to the boundary conditions that the
initial and final positions always remain the same, xN = x0 = x.
5. Choose a path of straight-line links connecting the lattice points corresponding to the
classical trajectory. Observe that the x values for the links of the path may have values that
increase, decrease, or remain unchanged (in contrast to time, which always increases).
6. Evaluate the energy E by summing the kinetic and potential energies for each link of the
path starting at j = 0:
E(x0, x1, . . . , xN ) '
N∑
j=1
[
m
2
(
xj − xj−1
ε
)2
+ V
(
xj + xj−1
2
)]
. (15.73)
7. Begin a sequence of repetitive steps in which a random position xj associated with time
tj is changed to the position x′j (point C in Figure 15.9). This changes two links in the
path.
8. For the coordinate that is changed, use the Metropolis algorithm to weigh the change with
the Boltzmann factor.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
360 CHAPTER 15
9. For each lattice point, establish a running sum that represents the value of the wave func-
tion squared at that point.
10. After each single-link change (or decision not to change), increase the running sum for
the new x value by 1. After a sufficiently long running time, the sum divided by the
number of steps is the simulated value for |ψ(xj)|2 at each lattice point xj .
11. Repeat the entire link-changing simulation starting with a different seed. The average
wave function from a number of intermediate-length runs is better than that from one
very long run.
Listing 15.3 QMC.py determines the ground-state probability via a Feynman path integration using the Metropolis
algorithm to simulate variations about the classical trajectory. 
# QMC. j a v a : Quantum MonteCarlo , Feynman p a t h i n t e g r a t i o n from i n i t i a l s t l i n e
from v i s u a l i m p o r t ∗ ; i m p o r t random ; from v i s u a l . g raph i m p o r t ∗
N = 100 ; M = 101 ; x s c a l e = 1 0 .
p a t h = z e r o s ( [M] , F l o a t ) ; p rob = z e r o s ( [M] , F l o a t ) # I n i t i a l i z e
# I n i t i a l i z e
t r a j e c = d i s p l a y ( wid th = 300 , h e i g h t = 500 , t i t l e = ’Spacetime Trajectories’ )
t r p l o t = c u r v e ( y = r a n g e ( 0 , 100) , c o l o r = c o l o r . magenta , d i s p l a y = t r a j e c )
d e f t r j a x s ( ) : # p l o t a x i s f o r t r a j e c t o r i e s
t r a x = c u r v e ( pos = [ ( − 97 , − 100) , ( 1 0 0 , − 100) ] , c o l o r = c o l o r . cyan , d i s p l a y = t r a j e c ) # x
ax
# c u r v e ( pos = [ ( 0 , − 100) , ( 0 , 100) ] , c o l o r = c o l o r . cyan , d i s p l a y = t r a j e c ) # y l a b e l ( pos
= ( 0 , 110) , t e x t = ’t’ , box = 0 , d i s p l a y = t r a j e c )
l a b e l ( pos = ( 0 , − 110) , t e x t = ’0’ , box = 0 , d i s p l a y = t r a j e c )
l a b e l ( pos = ( 6 0 , − 110) , t e x t = ’x’ , box = 0 , d i s p l a y = t r a j e c )
wvgraph = d i s p l a y ( x = 340 , y = 150 , wid th = 500 , h e i g h t = 300 , t i t l e = ’Ground State Probability’ )
wvplo t = c u r v e ( x = r a n g e ( 0 , 100) , d i s p l a y = wvgraph ) # f o r p r o b a b i l i t y
wvfax = c u r v e ( c o l o r = c o l o r . cyan )
d e f wvfaxs ( ) : # a x i s f o r p r o b a b i l i t y
wvfax = c u r v e ( pos = [(−600 ,−155) , (800 ,−155) ] , d i s p l a y =wvgraph , c o l o r = c o l o r . cyan )
c u r v e ( pos = [(0 ,−150) , ( 0 , 4 0 0 ) ] , d i s p l a y =wvgraph , c o l o r = c o l o r . cyan )
l a b e l ( pos = (−80 ,450) , t e x t =’Probability’ , box = 0 , d i s p l a y = wvgraph )
l a b e l ( pos = (600 ,−220) , t e x t =’x’ , box =0 , d i s p l a y =wvgraph )
l a b e l ( pos = (0 ,−220) , t e x t =’0’ , box =0 , d i s p l a y =wvgraph )
t r j a x s ( ) ; wvfaxs ( ) # p l o t axes
d e f en e rg y ( p a t h ) : # HO en e r g y
sums = 0 .
f o r i i n r a n g e ( 0 , N−2) : sums += ( p a t h [ i +1] − p a t h [ i ] ) ∗( p a t h [ i +1] − p a t h [ i ] )
sums += p a t h [ i +1]∗ p a t h [ i + 1 ] ;
r e t u r n sums
d e f p l o t p a t h ( p a t h ) : # p l o t t r a j e c t o r y i n xy s c a l e
f o r j i n r a n g e ( 0 , N) :
t r p l o t . x [ j ] = 20∗ p a t h [ j ]
t r p l o t . y [ j ] = 2∗ j − 100
d e f p l o t w v f ( prob ) : # p l o t prob
f o r i i n r a n g e ( 0 , 100) :
wvplo t . c o l o r = c o l o r . y e l l ow
wvplo t . x [ i ] = 8∗ i − 400 # c o n v e n i e n t c o o r d i n a t e s
wvplo t . y [ i ] = 4 .0∗ prob [ i ] − 150 # f o r c e n t e r e d f i g u r e
oldE = e n e r gy ( p a t h ) # f i n d E of p a t h
w h i l e 1 : # p i c k random e l e m e n t
r a t e ( 1 0 ) # s lows t h e p a i n t i n g s
e l e m e n t = i n t (N∗random . random ( ) ) # M e t r o p o l i s a l g o r i t h m
change = 2 . 0∗ ( random . random ( ) − 0 . 5 )
p a t h [ e l e m e n t ] += change # Change p a t h
newE = e ne rg y ( p a t h ) ; # Find new E
i f newE > oldE and math . exp ( − newE + oldE )<= random . random ( ) :
p a t h [ e l e m e n t ] −= change # R e j e c t
p l o t p a t h ( p a t h ) # p l o t r e s u l t i n g l a s t t r a j e c t o r y
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 361
elem = i n t ( p a t h [ e l e m e n t ]∗16 + 50) # i f p a t h = 0 , elem = 50
# elem = m ∗p a t h [ e l e m e n t ] + b i s t h e l i n e a r t r a n s f o r m a t i o n
# i f p a t h = − 3 . 0 , elem = 2 i f p a t h = 3 . 0 , elem = 98 = > b = 50 , m = 16 l i n e a r t r n s f .
# t h i s way x = 0 c o r r e s p o n d t o prob [ 5 0 ]
i f elem < 0 : elem = 0 , # n e g a t i v e case n o t a l l o w e d
i f elem > 100 : elem = 100 # i f exceed max
prob [ elem ] += 1 # i n c r e a s e p r o b a b i l i t y f o r t h a t x v a l u e
p l o t w v f ( prob ) # p l o t prob
oldE = newE
15.8.4 Assessment and Exploration
1. Plot some of the actual space-time paths used in the simulation along with the classical
trajectory.
2. For a more continuous picture of the wave function, make the x lattice spacing smaller;
for a more precise value of the wave function at any particular lattice site, sample more
points (run longer) and use a smaller time step ε.
3. Because there are no sign changes in a ground-state wave function, you can ignore the
phase, assume ψ(x) =
√
ψ2(x), and then estimate the energy via
E =
〈ψ|H |ψ〉
〈ψ|ψ〉
=
ω
2〈ψ|ψ〉
∫ +∞
−∞
ψ∗(x)
(
− d
2
dx2
+ x2
)
ψ(x) dx,
where the space derivative is evaluated numerically.
4. Explore the effect of making h̄ larger and thus permitting greater fluctuations around the
classical trajectory. Do this by decreasing the value of the exponent in the Boltzmann
factor. Determine if this makes the calculation more or less robust in its ability to find the
classical trajectory.
5. Test your ψ for the gravitational potential (see quantum bouncer below):
V (x) = mg|x|, x(t) = x0 + v0t+ 12gt
2.
15.9 EXPLORATION: QUANTUM BOUNCER’S PATHS 
Another problem for which the classical trajectory is well known is that of a quantum bouncer.
Here we have a particle dropped in a uniform gravitational field, hitting a hard floor, and
then bouncing. When treated quantum mechanically, quantized levels for the particle result
[Gibbs 75, Good 92, Whine 92, Bana 99, Vall 00]. In 2002 an experiment to discern this grav-
itational effect at the quantum level was performed by Nesvizhevsky et al. [Nes 02] and de-
scribed in [Schw 02]. It consisted of dropping ultracold neutrons from a height of 14 µm unto
a neutron mirror and watching them bounce. It found a neutron ground state at 1.4 peV.
We start by determining the analytic solution to this problem for stationary states and
then generalize it to include time dependence.6 The time-independent Schrödinger equation
for a particle in a uniform gravitation potential is
− h̄
2
2m
d2ψ(x)
dx2
+mxg ψ(x) =E ψ(x), (15.74)
ψ(x ≤ 0) = 0, (boundary condition). (15.75)
The boundary condition (15.75) is a consequence of the hard floor at x = 0. A change of
6Oscar A. Restrepo assisted in the preparation of this section.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
362 CHAPTER 15
Figure 15.10 The Airy function squared (continuous line) and the Quantum Monte Carlo solution |ψ0(q)|2 (dashed
line) after a million trajectories.
z
QMC
Analytic
 (z)|
2
0.6
0.4
0.2
0
0 2 4 6
variables converts (15.74) to a dimensionless form,
d2ψ
dz2
− (z − zE)ψ= 0, (15.76)
z = x
(
2gm2
h̄2
)1/3
, zE = E
(
2
h̄2mg2
)1/3
. (15.77)
This equation has an analytic solution in terms of Airy functions Ai(z) [L 96]:
ψ(z) = Nn Ai(z − zE), (15.78)
where Nn is a normalization constant and zE is the scaled value of the energy. The boundary
condition ψ(0) = 0 implies that
ψ(0) = NE Ai(−zE) = 0, (15.79)
which means that the allowed energies of the system are discrete and correspond to the zeros
zn of the Airy functions [Pres 00] at negative argument. To simplify the calculation, we take
h̄ = 1, g = 2, and m = 12 , which leads to z = x and zE = E.
The time-dependent solution for the quantum bouncer is constructed by forming the
infinite sum over all the discrete eigenstates, each with a time dependence appropriate to its
energy:
ψ(z, t) =
∞∑
n=1
CnNn Ai(z − zn)e−iEn t/h̄, (15.80)
where the Cn’s are constants.
Figure 15.10 shows the results of solving for the quantum bouncer’s ground-state prob-
ability |ψ0(z)|2 using Feynman’s path integration. The time increment dt and the total time t
were selected by trial and error in such a way as to make |ψ(0)|2 ' 0 (the boundary condition).
To account for the fact that the potential is infinite for negative x values, we selected trajecto-
ries that have positive x values over all their links. This incorporates the fact that the particle
can never penetrate the floor. Our program is given in Listing 15.4, and it yields the results in
Figure 15.10 after using 106 trajectories and a time step ε = dτ = 0.05. Both results were
normalized via a trapezoid integration. As can be seen, the agreement between the analytic
result and the path integration is satisfactory.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
THERMODYNAMIC SIMULATIONS & FEYNMAN QUANTUM PATH INTEGRATION 363
Listing 15.4 QMCbouncer.py uses Feynman path integration to compute the path of a quantum particle in a grav-
itational field. 
# QMCbouncer . py : g . s . w a v e f u n c t i o n v i a p a t h i n t e g r a t i o n
from v i s u a l i m p o r t ∗ ; i m p o r t random ; from v i s u a l . g raph i m p o r t ∗
# P a r a m e t e r s
N = 100 ; d t = 0 . 0 5 ; g = 2 . 0 ; h = 0 . 0 0 ; maxel = 0
p a t h = z e r o s ( [ 1 0 1 ] , F l o a t ) ; a r r = p a t h ; prob = z e r o s ( [ 2 0 1 ] , F l o a t ) # I n i t i a l i z e
t r a j e c = d i s p l a y ( wid th = 300 , h e i g h t = 500 , t i t l e = ’Spacetime Trajectory’ )
t r p l o t = c u r v e ( y = r a n g e ( 0 , 100) , c o l o r = c o l o r . magenta , d i s p l a y = t r a j e c )
d e f t r j a x s ( ) : # p l o t a x i s f o r t r a j e c t o r i e s
t r a x = c u r v e ( pos = [(−97 ,−100) , (100 ,−100) ] , c o l o r = c o l o r . cyan , d i s p l a y = t r a j e c )
c u r v e ( pos = [(−85 , −100) , ( −85, 100) ] , c o l o r = c o l o r . cyan , d i s p l a y = t r a j e c )
l a b e l ( pos = (−85 ,110) , t e x t = ’t’ , box = 0 , d i s p l a y = t r a j e c )
l a b e l ( pos = (−85 , −110) , t e x t = ’0’ , box = 0 , d i s p l a y = t r a j e c )
l a b e l ( pos = ( 6 0 , −110) , t e x t = ’x’ , box = 0 , d i s p l a y = t r a j e c )
wvgraph = d i s p l a y ( x =350 , y =80 , wid th =500 , h e i g h t =300 , t i t l e = ’Ground State Prob’ )
wvplo t = c u r v e ( x = r a n g e ( 0 , 50) , d i s p l a y = wvgraph ) # wave f u n c t i o n p l o t
wvfax = c u r v e ( c o l o r = c o l o r . cyan )
d e f wvfaxs ( ) : # p l o t a x i s f o r w a v e f u n c t i o n
wvfax = c u r v e ( pos = [(−200 ,−155) , (800 ,−155) ] , d i s p l a y =wvgraph , c o l o r = c o l o r . cyan )
c u r v e ( pos = [(−200 ,−150) , (−200 ,400) ] , d i s p l a y =wvgraph , c o l o r = c o l o r . cyan )
l a b e l ( pos = (−80 , 450) , t e x t = ’Probability’ , box = 0 , d i s p l a y = wvgraph )
l a b e l ( pos = ( 6 0 0 , −220) , t e x t = ’x’ , box = 0 , d i s p l a y = wvgraph )
l a b e l ( pos = (−200 , −220) , t e x t = ’0’ , box = 0 , d i s p l a y = wvgraph )
t r j a x s ( ) ; wvfaxs ( ) # p l o t axes
d e f en e rg y ( a r r ) : # Method f o r Energy of p a t h
esum = 0 .
f o r i i n r a n g e ( 0 ,N) : esum += 0 . 5∗ ( ( a r r [ i +1]− a r r [ i ] ) / d t )∗∗2+g∗( a r r [ i ]+ a r r [ i + 1 ] ) / 2
r e t u r n esum
d e f p l o t p a t h ( p a t h ) : # Method t o p l o t xy t r a j e c t o r y
f o r j i n r a n g e ( 0 , N) :
t r p l o t . x [ j ] = 20∗ p a t h [ j ] − 85
t r p l o t . y [ j ] = 2∗ j − 100
d e f p l o t w v f ( prob ) : # Method t o p l o t wave f u n c t i o n
f o r i i n r a n g e ( 0 , 50) :
wvplo t . c o l o r = c o l o r . y e l l ow
wvplo t . x [ i ] = 20∗ i − 200
wvplo t . y [ i ] = 0 .5∗ prob [ i ] − 150
oldE = e n e r gy ( p a t h ) # I n i t i a l E
c o u n t e r = 1 # f o r ea 100 i t e r a t i o n s
norm = 0 . # w a v e f u n c t i o n i s p l o t t e d
maxx = 0 . 0
w h i l e 1 : # "Infinite" l oop
r a t e ( 1 0 0 )
e l e m e n t = i n t (N∗random . random ( ) )
i f e l e m e n t != 0 and e l e m e n t != N: # Ends n o t a l l o w e d
change = ( ( random . random ( ) − 0 . 5 ) ∗2 0 . ) / 1 0 . # −1 =<random =< 1
i f p a t h [ e l e m e n t ] + change > 0 . : # No n e g a t i v e p a t h s
p a t h [ e l e m e n t ] += change # change t e m p o r a r i l y
newE = e ne rg y ( p a t h ) # New t r a j e c t o r y E
i f newE > oldE and math . exp ( − newE + oldE ) <= random . random ( ) :
p a t h [ e l e m e n t ] −= change # Link r e j e c t e d
p l o t p a t h ( p a t h )
e l e = i n t ( p a t h [ e l e m e n t ]∗ 1 2 5 0 . / 1 0 0 . ) # S c a l e changed
i f e l e >= maxel : maxel = e l e # S c a l e change 0 t o N
i f e l e m e n t != 0 : prob [ e l e ] += 1
oldE = newE ;
i f c o u n t e r %100 == 0 : # p l o t w a v e f u n c t i o n e v e r y 100
f o r i i n r a n g e ( 0 , N) : # max x v a l u e o f p a t h
i f p a t h [ i ] >= maxx : maxx = p a t h [ i ]
h = maxx / maxel # s p a c e s t e p
f i r s t l a s t = h∗0 .5∗ ( p rob [ 0 ] + prob [ maxel ] ) # f o r t r a p . i n t e g r . e x t r e m e s
f o r i i n r a n g e ( 0 , maxel + 1) : norm = norm + prob [ i ] # n o r m a l i z e
norm = norm∗h + f i r s t l a s t # Trap r u l e
p l o t w v f ( prob ) # p l o t p r o b a b i l i t y
c o u n t e r += 1
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Sixteen
Simulating Matter with Molecular Dynamics
Problem: Determine whether a collection of argon molecules placed in a box will form an
ordered structure at low temperature.
You may have seen in your introductory classes that the ideal gas law can be derived from
first principles if gas molecules are treated as billiard balls bouncing off the walls but not
interacting with each other. We want to extend this model so that we can solve for the motion
of every molecule in a box interacting with every other molecule via a potential. We picked
argon because it is an inert element with a closed shell of electrons and so can be modeled as
almost-hard spheres.
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
- - - *
16.1 MOLECULAR DYNAMICS (THEORY)
Molecular dynamics (MD) is a powerful simulation technique for studying the physical and
chemical properties of solids, liquids, amorphous materials, and biological molecules. Even
though we know that quantum mechanics is the proper theory for molecular interactions, MD
uses Newton’s laws as the basis of the technique and focuses on bulk properties, which do
not depend much on small-r behaviors. In 1985 Car and Parrinello showed how MD can be
extended to include quantum mechanics by applying density functional theory to calculate the
force [C&P 85]. This technique, known as quantum MD, is an active area of research but is
beyond the realm of the present chapter.1 For those with more interest in the subject, there
are full texts [A&T 87, Rap 95, Hock 88] on MD and good discussions [G,T&C 06, Thij 99,
Fos 96], as well as primers [Erco] and codes, [NAMD, Mold, ALCMD] available on-line.
MD’s solution of Newton’s laws is conceptually simple, yet when applied to a very large
number of particles becomes the “high school physics problem from hell.” Some approxima-
tions must be made in order not to have to solve the 1023–1025 equations of motion describing
a realistic sample but instead to limit the problem to∼106 particles for protein simulations and
∼108 particles for materials simulations. If we have some success, then it is a good bet that the
model will improve if we incorporate more particles or more quantum mechanics, something
that becomes easier as computing power continues to increase.
In a number of ways, MD simulations are similar to the thermal Monte Carlo simulations
1We thank Satoru S. Kano for pointing this out to us.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SIMULATING MATTER WITH MOLECULAR DYNAMICS 365
Figure 16.1 The molecule–molecule effective interaction arises from the many-body interaction of the electrons
and nucleus in one electronic cloud with the electrons and nucleus in another electron cloud. (The size
of the nucleus at the center is highly exaggerated relative to the size of the molecule, and the electrons
are really just points.)
+ +
we studied in Chapter 15, “Thermodynamic Simulations & Feynman Quantum Path Integra-
tion.” Both typically involve a large number N of interacting particles that start out in some set
configuration and then equilibrate into some dynamic state on the computer. However, in MD
we have what statistical mechanics calls a microcanonical ensemble in which the energyE and
volume V of the N particles are fixed. We then use Newton’s laws to generate the dynamics of
the system. In contrast, Monte Carlo simulations do not start with first principles but instead
incorporate an element of chance and have the system in contact with a heat bath at a fixed
temperature rather than keeping the energy E fixed. This is called a canonical ensemble.
Because a system of molecules is dynamic, the velocities and positions of the molecules
change continuously, and so we will need to follow the motion of each molecule in time to
determine its effect on the other molecules, which are also moving. After the simulation has
run long enough to stabilize, we will compute time averages of the dynamic quantities in order
to deduce the thermodynamic properties. We apply Newton’s laws with the assumption that the
net force on each molecule is the sum of the two-body forces with all other (N −1) molecules:
m
d2ri
dt2
= Fi(r0, . . . , rN−1) (16.1)
m
d2ri
dt2
=
N−1∑
i<j=0
fij , i = 0, . . . , (N − 1). (16.2)
In writing these equations we have ignored the fact that the force between argon atoms really
arises from the particle–particle interactions of the 18 electrons and the nucleus that constitute
each atom (Figure 16.1). Although it may be possible to ignore this internal structure when
deducing the long-range properties of inert elements, it matters for systems such as polyatomic
molecules that display rotational, vibrational, and electronic degrees of freedom as the temper-
ature is raised.2
We assume that the force on molecule i derives from a potential and that the potential is
2We thank Saturo Kano for clarifying this point.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
366 CHAPTER 16
Table 16.1 Parameter Values and Scales for the Lennard-Jones Potential.
Quantity Mass Length Energy Time Temperature
Unit m σ 
√
mσ2/ /kB
Value 6.7× 10−26 kg 3.4× 10−10 m 1.65× 10−21 J 4.5× 10−12 s 119 K
the sum of central molecule–molecule potentials:
Fi(r0, r1, . . . , rN−1) =−∇riU(r0, r1, . . . , rN−1), (16.3)
U(r0, r1, . . . , rN−1) =
∑
i<j
u(rij) =
N−2∑
i=0
N−1∑
j=i+1
u(rij), (16.4)
⇒ fij = −
du(rij)
drij
(
xi − xj
rij
êx +
yi − yj
rij
êy +
zi − zj
rij
êz
)
. (16.5)
Here rij = |ri − rj | = rji is the distance between the centers of molecules i and j, and the
xmll
limits on the sums are such that no interaction is counted twice. Because we have assumed
a conservative potential, the total energy of the system, that is, the potential plus kinetic en-
ergies summed over all particles, should be conserved over time. Nonetheless, in a practical
computation we “cut the potential off” [assume u(rij) = 0] when the molecules are far apart.
Because the derivative of the potential produces an infinite force at this cutoff point, energy will
no longer be precisely conserved. Yet because the cutoff radius is large, the cutoff occurs only
when the forces are minuscule, and so the violation of energy conservation should be small
relative to approximation and round-off errors.
In a first-principles calculation, the potential between any two argon atoms arises from
the sum over approximately 1000 electron–electron and electron–nucleus Coulomb interac-
tions. A more practical calculation would derive an effective potential based on a form of
many-body theory, such as Hartree–Fock or density functional theory. Our approach is simpler
yet. We use the Lennard–Jones potential,
u(r) = 4
[(σ
r
)12
−
(σ
r
)6]
, (16.6)
f(r) = −du
dr
=
48
r2
[(σ
r
)12
− 1
2
(σ
r
)6]
r. (16.7)
Here the parameter  governs the strength of the interaction, and σ determines the length scale.
Both are deduced by fits to data, which is why this is called a “phenomenological” potential.
Some typical values for the parameters, and corresponding scales for the variables, are
given in Table 16.1. In order to make the program simpler and to avoid under- and overflows,
it is helpful to measure all variables in the natural units formed by these constants. The inter-
particle potential and force then take the forms
u(r) = 4
[
1
r12
− 1
r6
]
, f(r) =
48
r
[
1
r12
− 1
2r6
]
. (16.8)
The Lennard-Jones potential is seen in Figure 16.2 to be the sum of a long-range attractive
interaction ∝ 1/r6 and a short-range repulsive one ∝ 1/r12. The change from repulsion to
attraction occurs at r = σ. The minimum of the potential occurs at r = 21/6σ = 1.1225σ,
which would be the atom–atom spacing in a solid bound by this potential. The repulsive 1/r12
term in the Lennard-Jones potential (16.6) arises when the electron clouds from two atoms
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SIMULATING MATTER WITH MOLECULAR DYNAMICS 367
Figure 16.2 The Lennard-Jones potential. Note the sign change at r = 1 and the minimum at r ' 1.1225 (natural
units). Note too that because the r axis does not extend to r = 0, the very high central repulsion is not
shown.
repulsive
attraction
u(r)
r
Lennard-Jones
overlap, in which case the Coulomb interaction and the Pauli exclusion principle keep the
electrons apart. The 1/r12 term dominates at short distances and makes atoms behave like hard
spheres. The precise value of 12 is not of theoretical significance (although it’s being large is)
and was probably chosen because it is 2× 6.
The 1/r6 term dominates at large distances and models the weak van der Waals induced
dipole–dipole attraction between two molecules.3 The attraction arises from fluctuations in
which at some instant in time a molecule on the right tends to be more positive on the left
side, like a dipole⇐. This in turn attracts the negative charge in a molecule on its left, thereby
inducing a dipole ⇐ in it. As long as the molecules stay close to each other, the polarities
continue to fluctuate in synchronization⇐⇐ so that the attraction is maintained. The resultant
dipole–dipole attraction behaves like 1/r6, and although much weaker than a Coulomb force,
it is responsible for the binding of neutral, inert elements, such as argon for which the Coulomb
force vanishes.
16.1.1 Connection to Thermodynamic Variables
We assume that the number of particles is large enough to use statistical mechanics to relate
the results of our simulation to the thermodynamic quantities (the simulation is valid for any
number of particles, but the use of statistics requires large numbers). The equipartition theorem
tells us that for molecules in thermal equilibrium at temperature T , each molecular degree of
freedom has an energy kBT/2 on the average associated with it, where kB = 1.38×10−23 J/K
is Boltzmann’s constant. A simulation provides the kinetic energy of translation4:
KE =
1
2
〈
N−1∑
i=0
v2i
〉
. (16.9)
The time average of KE (three degrees of freedom) is related to temperature by
〈KE〉 = N 3
2
kBT ⇒ T =
2〈KE〉
3kBN
. (16.10)
3There are also van der Waals forces that cause dispersion, but we are not considering those here.
4Unless the temperature is very high, argon atoms, being inert spheres, have no rotational energy.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
368 CHAPTER 16
Figure 16.3 Left: Two frames from the animation of a 1-D simulation that starts with uniformly spaced atoms.
Note how an image atom has moved in from the bottom after an atom leaves from the top. Right: Two
frames from the animation of a 2-D simulation showing the initial and an equilibrated state. Note how
the atoms start off in a simple cubic arrangement but then equilibrate to a face-centered-cubic lattice.
In all cases, it is the interatomic forces that constrain the atoms to a lattice.
The system’s pressure P is determined by a version of the Virial theorem,
PV = NkBT +
w
3
, w =
〈
N−1∑
i<j
rij · fij
〉
, (16.11)
where the Virial w is a weighted average of the forces. Note that because ideal gases have no
interaction forces, their Virial vanishes and we have the ideal gas law. The pressure is thus
P =
ρ
3N
(2〈KE〉+ w) , (16.12)
where ρ = N/V is the density of the particles.
16.1.2 Setting Initial Velocities
Even though we start the system off with a velocity distribution characteristic of some tempera-
ture, since the system is not in equilibrium initially (some of the assigned KE goes into PE), this
is not the true temperature of the system [Thij 99]. Note that this initial randomization is the
only place where chance enters into our MD simulation, and it is there to speed the simulation
along. Once started, the time evolution is determined by Newton’s laws, in contrast to Monte
Carlo simulations which are inherently stochastic. We produce a Gaussian (Maxwellian) ve-
locity distribution with the methods discussed in Chapter 5, “Monte Carlo Simulations.” In
our sample code we take the average 112
∑12
i=1 ri of uniform random numbers 0 ≤ ri ≤ 1 to
produce a Gaussian distribution with mean 〈r〉 = 0.5. We then subtract this mean value to
obtain a distribution about 0.
16.1.3 Periodic Boundary Conditions and Potential Cutoff
It is easy to believe that a simulation of 1023 molecules should predict bulk properties well, but
with typical MD simulations employing only 103–106 particles, one must be clever to make
less seem like more. Furthermore, since computers are finite, the molecules in the simulation
are constrained to lie within a finite box, which inevitably introduces artificial surface effects
from the walls. Surface effects are particularly significant when the number of particles is
small because then a large fraction of the molecules reside near the walls. For example, if 1000
particles are arranged in a 10× 10× 10× 10 cube, there are 103–83 = 488 particles one unit
from the surface, that is, 49% of the molecules, while for 106 particles this fraction falls to 6%.
The imposition of periodic boundary conditions (PBCs) strives to minimize the short-
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SIMULATING MATTER WITH MOLECULAR DYNAMICS 369
Figure 16.4 The infinite space generated by imposing periodic boundary conditions on the particles within the
simulation volume (shaded box). The two-headed arrows indicate how a particle interacts with the
nearest version of another particle, be that within the simulation volume or an image. The vertical
arrows indicate how the image of particle 4 enters when particle 4 exits.
4
5
32
14
5
32
1 4
5
32
1
4
5
32
14
5
32
1 4
5
32
1
4
5
32
14
5
32
1 4
5
32
1
comings of both the small numbers of particles and of artificial boundaries. Even though we
limit our simulation to an Lx × Ly × Lz box, we imagine this box being replicated to infinity
in all directions (Figure 16.4). Accordingly, after each time-integration step we examine the
position of each particle and check if it has left the simulation region. If it has, then we bring
an image of the particle back through the opposite boundary (Figure 16.4):
x ⇒
{
x+ Lx, if x ≤ 0,
x− Lx, if x > Lx.
(16.13)
Consequently, each box looks the same and has continuous properties at the edges. As shown
by the one-headed arrows in Figure 16.4, if a particle exits the simulation volume, its image
enters from the other side, and so balance is maintained.
In principle a molecule interacts with all others molecules and their images, so even
though there is a finite number of atoms in the interaction volume, there is an effective infinite
number of interactions [Erco]. Nonetheless, because the Lennard–Jones potential falls off
so rapidly for large r, V (r = 3σ) ' V (1.13σ)/200, far-off molecules do not contribute
significantly to the motion of a molecule, and we pick a value rcut ' 2.5σ beyond which we
ignore the effect of the potential:
u(r) =
{
4
(
r−12 − r−6
)
, for r < rcut,
0, for r > rcut.
(16.14)
Accordingly, if the simulation region is large enough for u(r > Li/2) ' 0, an atom interacts
with only the nearest image of another atom.
The only problem with the cutoff potential (16.14) is that since the derivative du/dr is
singular at r = rcut, the potential is no longer conservative and thus energy conservation is no
longer ensured. However, since the forces are already very small at rcut, the violation will also
be very small.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
370 CHAPTER 16
16.2 VERLET AND VELOCITY-VERLET ALGORITHMS
A realistic MD simulation may require integration of the 3-D equations of motion for 1010 time
steps for each of 103–106 particles. Although we could use our standard rk4 ODE solver for
this, time is saved by using a simple rule embedded in the program. The Verlet algorithm uses
the central-difference approximation (Chapter 7, “Differentiation & Searching”) for the second
derivative to advance the solutions by a single time step h for all N particles simultaneously:
xmll Fi[r(t), t] = d
2ri
dt2
' ri(t+ h) + ri(t− h)− 2ri(t)
h2
, (16.15)
⇒ ri(t+ h)' 2ri(t)− ri(t− h) + h2Fi(t) + O(h4), (16.16)
where we have set m = 1. (Improved algorithms may vary the time step depending upon the
speed of the particle.) Notice that even though the atom–atom force does not have an explicit
time dependence, we include a t dependence in it as a way of indicating its dependence upon
the atoms’ positions at a particular time. Because this is really an implicit time dependence,
energy remains conserved.
Part of the efficiency of the Verlet algorithm (16.16) is that it solves for the position of
each particle without requiring a separate solution for the particle’s velocity. However, once we
have deduced the position for various times, we can use the central-difference approximation
for the first derivative of ri to obtain the velocity:
vi(t) =
dri
dt
' ri(t+ h)− ri(t− h)
2h
+ O(h2). (16.17)
Note, finally, that because the Verlet algorithm needs r from two previous steps, it is not self-
starting and so we start it with the forward difference,
r(t = −h) ' r(0)− hv(0) + h
2
2
F(0). (16.18)
Velocity-Verlet Algorithm: Another version of the Verlet algorithm, which we recommend
because of its increased stability, uses a forward-difference approximation for the derivative to
advance both the position and velocity simultaneously:
xmll ri(t+ h)' ri(t) + hvi(t) + h
2
2
Fi(t) + O(h3), (16.19)
vi(t+ h)'vi(t) + ha(t) + O(h2) (16.20)
'vi(t) + h
[
Fi(t+ h) + Fi(t)
2
]
+ O(h2). (16.21)
Although this algorithm appears to be of lower order than (16.16), the use of updated positions
when calculating velocities, and the subsequent use of these velocities, make both algorithms
of similar precision.
Of interest is that (16.21) approximates the average force during a time step as [Fi(t +
h) + Fi(t)]/2. Updating the velocity is a little tricky because we need the force at time t+ h,
which depends on the particle positions at t+ h. Consequently, we must update all the particle
positions and forces to t + h before we update any velocities, while saving the forces at the
earlier time for use in (16.21). As soon as the positions are updated, we impose periodic
boundary conditions to ensure that we have not lost any particles, and then we calculate the
forces.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SIMULATING MATTER WITH MOLECULAR DYNAMICS 371
16.3 1-D IMPLEMENTATION AND EXERCISE
On the CD you will find a folder MDanimations that contains a number of 2-D animations
(movies) of solutions to the MD equations. Some frames from theseshown in Figure 16.3. We
recommend that you look at them in order to better visualize what the particles do during an
MD simulation. In particular, these simulations use a potential and temperature that should
lead to a solid or liquid system, and so you should see the particles binding together.
Listing 16.1 MD.py performs a 1-D MD simulation with too small a number of large time steps for just a few
particles. To be realistic the user should change the parameters and the number of random numbers
added to form the Gaussian distribution. 
#MD. py M o l e c u l a r dynamics 2D
from v i s u a l i m p o r t ∗
from v i s u a l . g raph i m p o r t ∗
i m p o r t random
s c e n e = d i s p l a y ( x =0 , y =0 , wid th =350 , h e i g h t =350 , t i t l e =’Molecular Dynamics’ ,
r a n g e =10)
sceneK = g d i s p l a y ( x =0 , y =350 , wid th =600 , h e i g h t =150 , t i t l e =’Average KE’ ,
ymin = 0 . 0 , ymax = 5 . 0 , xmin =0 , xmax =500 , x t i t l e =’time’ , y t i t l e =’KE avg’ )
Kavegraph= g c u r ve ( c o l o r = c o l o r . r e d )
sceneT = g d i s p l a y ( x =0 , y =500 , wid th =600 , h e i g h t =150 , t i t l e =’Average PE’ ,
ymin=−60,ymax = 0 . , xmin =0 , xmax =500 , x t i t l e =’time’ , y t i t l e =’PE avg’ )
Tcurve = g cu rv e ( c o l o r = c o l o r . cyan )
Natom = 25
Nmax = 25
T i n i t =2 .0
dens =1 .0 # d e n s i t y ( 1 . 2 0 f o r f c c )
t 1 = 0
x = z e r o s ( (Nmax) , F l o a t ) # x p o s i t i o n o f atoms
y = z e r o s ( (Nmax) , F l o a t ) # y p o s i t i o n
vx = z e r o s ( (Nmax) , F l o a t ) # v e l . x o f atoms
vy = z e r o s ( (Nmax) , F l o a t ) # y component v e l o c i t y atoms
fx = z e r o s ( ( Nmax , 2 ) , F l o a t ) # x component o f f o r c e
fy = z e r o s ( ( Nmax , 2 ) , F l o a t ) # y component o f f o r c e
L = i n t ( 1 .∗Natom∗∗0 .5 ) # s i d e o f s q u a r e wi th atoms
atoms = [ ]
# graph1 = d i s p l a y ( wid th =500 , h e i g h t =500 ,
# t i t l e =’Molecular Dynamics’ , r a n g e =10)
d e f t w e l v e r a n ( ) : # computes a v e r a g e o f 12 random numbers
s =0 .0
f o r i i n r a n g e ( 1 , 1 3 ) :
s +=random . random ( )
r e t u r n s /12 .0−0.5
d e f i n i t i a l p o s v e l ( ) : # i n i t i a l p o s i t i o n s , v e l o c i t i e s o f atoms
i = −1
f o r i x i n r a n g e ( 0 , L ) : # x−> 0 1 2 3 4
f o r i y i n r a n g e ( 0 , L ) : # y=0 0 5 10 15 20
i = i + 1 # y=1 1 6 11 16 21
x [ i ] = i x # y=2 2 7 12 17 22
y [ i ] = i y # y=3 3 8 13 18 23
vx [ i ] = t w e l v e r a n ( ) # y=4 4 9 14 19 24
vy [ i ] = t w e l v e r a n ( ) # number ing of 25 atoms
vx [ i ] = vx [ i ]∗math . s q r t ( T i n i t )
vy [ i ] = vy [ i ]∗math . s q r t ( T i n i t )
f o r j i n r a n g e ( 0 , Natom ) :
xc = 2∗x [ j ] − 4
yc = 2∗y [ j ] − 4
atoms . append ( s p h e r e ( pos =( xc , yc ) , r a d i u s = 0 . 5 , c o l o r = c o l o r . r e d ) )
d e f s i g n ( a , b ) :
i f ( b >= 0 . 0 ) :
r e t u r n abs ( a )
e l s e :
r e t u r n − abs ( a )
d e f F o r c e s ( t , w, PE , PEorW ) : # s e t s t h e f o r c e s on each of 25 p a r t i c l e s
# i n v r 2 = 0 .
r 2 c u t = 9 . # o f PEorW==1 computes PE e l s e , w
PE = 0 .
f o r i i n r a n g e ( 0 , Natom ) :
fx [ i ] [ t ] = fy [ i ] [ t ] = 0 . 0 # t o make sums , s t a r t i n 0
f o r i i n r a n g e ( 0 , Natom−1 ) :
f o r j i n r a n g e ( i + 1 , Natom ) :
dx = x [ i ] − x [ j ] # atom s e p a r a t i o n x
dy = y [ i ] − y [ j ] # atom s e p a r a t i o n y
i f ( abs ( dx ) > 0.50∗L ) : # s m a l l e s t d i s t a n c e from p a r t . o r image
dx = dx − s i g n ( L , dx ) # i n t e r a c t w i th c l o s e r image
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
372 CHAPTER 16
i f ( abs ( dy ) > 0.50∗L ) : # same f o r y
dy = dy − s i g n ( L , dy )
r2 = dx∗dx + dy∗dy # ( d i s t a n c e )∗∗2
i f ( r2 < r 2 c u t ) : # i f l e s s t h a n r c u t ∗∗2
i f ( r2 == 0 . ) : # t o a v o i d 0 d e n o m i n a t o r
r2 = 0 .0001
i n v r 2 = 1 . / r2 # compute t h i s f a c t o r
w i j = 4 8 .∗ ( i n v r 2 ∗∗3 − 0 . 5 ) ∗ i n v r 2 ∗∗3 # ( 4 8 / r ∗∗6) [ 1 / r ∗∗6−0.5/ r ∗∗6]
f i j x = w i j∗ i n v r 2∗dx # fx Eqs . 1 6 . 7 , 1 6 . 8
f i j y = w i j∗ i n v r 2∗dy # fy
fx [ i ] [ t ] = fx [ i ] [ t ] + f i j x # Eq . 1 6 . 3
fy [ i ] [ t ] = fy [ i ] [ t ] + f i j y
fx [ j ] [ t ] = fx [ j ] [ t ] − f i j x # i n o p p o s i t e s e n s e
fy [ j ] [ t ] = fy [ j ] [ t ] − f i j y # f o r n e x t i t e r a t i o n i n i
PE = PE + 4 .∗ ( i n v r 2 ∗∗3) ∗ ( ( i n v r 2 ∗∗3) − 1 . )
w = w + w i j
i f ( PEorW == 1) :
r e t u r n PE
e l s e :
r e t u r n w
d e f t i m e v o l u t i o n ( ) :
avT =0 .0
avP =0 .0
Pavg =0 .0
avKE =0.0
avPE =0.0
t 1 =0
PE = 0 . 0
h = 0 .031 # s t e p
hover2 = h / 2 . 0
# i n i t i a l KE & PE v i a F o r c e s
KE = 0 . 0
w =0.0
i n i t i a l p o s v e l ( )
f o r i i n r a n g e ( 0 , Natom ) :
KE=KE+( vx [ i ]∗ vx [ i ]+ vy [ i ]∗ vy [ i ] ) / 2 . 0
# System . o u t . p r i n t l n (""+ t +" PE= "+PE+" KE = "+KE+" PE+KE = "+(PE+KE) ) ;
PE = F o r c e s ( t1 , w, PE , 1 )
t ime =1
w h i l e 1 :
# r a t e ( 1 0 0 )
f o r i i n r a n g e ( 0 , Natom ) :
PE = F o r c e s ( t1 , w, PE , 1 )
x [ i ] = x [ i ] + h∗( vx [ i ] + hover2∗ fx [ i ] [ t 1 ] ) # v e l o c i t y V e r l e t a l g o r i t h m
y [ i ] = y [ i ] + h∗( vy [ i ] + hover2∗ fy [ i ] [ t 1 ] ) ;
i f x [ i ] <= 0 . :
x [ i ] = x [ i ] + L # p e r i o d i c boundary c o n d i t i o n s
i f x [ i ] >= L :
x [ i ] = x [ i ] − L
i f y [ i ] <= 0 . :
y [ i ] = y [ i ] + L
i f y [ i ] >= L :
y [ i ] = y [ i ] − L
xc = 2∗x [ i ] − 4
yc = 2∗y [ i ] − 4
atoms [ i ] . pos =( xc , yc )
PE = 0 .
t 2 =1
PE = F o r c e s ( t2 , w, PE , 1 )
KE = 0 .
w = 0 .
f o r i i n r a n g e (0 , Natom ) :
vx [ i ] = vx [ i ] + hover2 ∗( fx [ i ] [ t 1 ] + fx [ i ] [ t 2 ] )
vy [ i ] = vy [ i ] + hover2 ∗( fy [ i ] [ t 1 ] + fy [ i ] [ t 2 ] )
KE = KE + ( vx [ i ]∗ vx [ i ] + vy [ i ]∗ vy [ i ] ) / 2
w = F o r c e s ( t2 , w, PE , 2 )
P= dens ∗(KE+w)
T=KE / ( Natom )
# i n c r e m e n t a v e r a g e s
avT = avT + T # Tempera tu r e
avP = avP + P # P r e s s u r e
avKE = avKE + KE # K i n e t i c e n e r gy
avPE = avPE + PE # P o t e n t i a l e ne r gy
t ime += 1
t = t ime
i f ( t ==0) :
t =1
Pavg = avP / t
eKavg = avKE / t
ePavg = avPE / t
Tavg = avT / t
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SIMULATING MATTER WITH MOLECULAR DYNAMICS 373
Figure 16.5 The kinetic, potential, and total energy for a 2-D MD simulation with 36 particles (left), and 300
particles (right), both with an initial temperature of 150 K. The potential energy is negative, the kinetic
energy is positive, and the total energy is seen to be conserved (flat).
T
E
n
e
rg
ía
 (
J
)
2E–13 4E–13 6E–13 8E–13 1E–12 1.2E–120 1.4E–12
E
n
er
g
y 
(J
)
8.00E–19
6.00E–19
4.00E–19
2.00E–19
0.00E+00
–2.00E–19
–4.00E–19
–6.00E–19
–8.00E–19
–1.00E–18
–1.20E–18
–1.40E–18
Energy vs Time
for 300 particles 2D box, initialy at 150 k
Time (sec)  (568 steps)
p r e =( i n t ) ( Pavg ∗1000)
Pavg= p r e / 1 0 0 0 . 0
k e n e r =( i n t ) ( eKavg∗1000)
eKavg= k e n e r / 1 0 0 0 . 0
Kavegraph . p l o t ( pos =( t , eKavg ) )
p e n e r =( i n t ) ( ePavg ∗1000)
ePavg= p e n e r / 1 0 0 0 . 0
tempe =( i n t ) ( Tavg∗1000000)
Tavg=tempe / 1 0 0 0 0 0 0 . 0
Tcurve . p l o t ( pos =( t , ePavg ) , d i s p l a y = sceneT )
t i m e v o l u t i o n ( )
The program MD.py implements an MD simulation in 1-D using the velocity–Verlet algorithm.
Use it as a model and do the following:
1. Ensure that you can run and visualize the 1-D simulation.
2. Place the particles initially at the sites of a face-centered-cubic (FCC) lattice, the equi-
librium configuration for a Lennard-Jones system at low temperature. The particles will
find their own ways from there. An FCC lattice has four quarters of a particle per unit
cell, so an L3 box with a lattice constant L/N contains (parts of) 4N3 = 32, 108, 256,
. . . particles.
3. To save computing time, assign initial particle velocities corresponding to a fixed-
temperature Maxwellian distribution.
4. Print the code and indicate on it which integration algorithm is used, where the periodic
boundary conditions are imposed, where the nearest image interaction is evaluated, and
where the potential is cut off.
5. A typical time step is ∆t = 10−14 s, which in our natural units equals 0.004. You
probably will need to make 104–105 such steps to equilibrate, which corresponds to a
total time of only 10−9 s (a lot can happen to a speedy molecule in 10−9 s). Choose the
largest time step that provides stability and gives results similar to Figure 16.5.
6. The PE and KE change with time as the system equilibrates. Even after that, there will
be fluctuations since this is a dynamic system. Evaluate the time-averaged energies for
an equilibrated system.
7. Compare the final temperature of your system to the initial temperature. Change the
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
374 CHAPTER 16
Figure 16.6 Left: The temperature after equilibration as a function of initial kinetic energy for a simulation with
36 particles in two dimensions. Right: The pressure versus temperature for a simulation with several
hundred particles. (Courtesy of J. Wetzel.)
0
100
200
300
F
in
a
l
T
e
m
p
e
ra
tu
re
  
(K
)
200 E-19100 E-19
Initial KE (j)
P
1
2
0
0 0.1 0.2 0.3
T
initial temperature and look for a simple relation between it and the final temperature
(Figure 16.6).
16.4 TRAJECTORY ANALYSIS
1. Modify your program so that it outputs the coordinates and velocities of some particles
throughout the simulation. Note that you do not need as many time steps to follow a
trajectory as you do to compute it and so you may want to use the mod operator %100 for
output.
2. Start your assessment with a 1-D simulation at zero temperature. The particles should
remain in place without vibration. Increase the temperature and note how the particles
begin to move about and interact.
3. Try starting off all your particles at the minima in the Lennard-Jones potential. The
particles should remain bound within the potential until you raise the temperature.
4. Repeat the simulations for a 2-D system. The trajectories should resemble billiard ball–
like collisions.
5. Create an animation ofof several particles.
6. Calculate and plot as a function of temperature the root-mean-square displacement of
molecules:
Rrms =
√〈
|r(t+ ∆t)− r(t)|2
〉
, (16.22)
where the average is over all the particles in the box. Verify that for a liquid R2rms grows
linearly with time.
7. Test your system for time-reversal invariance. Stop it at a fixed time, reverse all the
velocities, and see if the system retraces its trajectories back to the initial configuration.
16.5 QUIZ
1. We wish to make an MD simulation by hand of the positions of particles 1 and 2 that are
in a 1-D box of side 8. For an origin located at the center of the box, the particles are
initially at rest and at locations xi(0) = −x2(0) = 1. The particles are subject to the
force
F (x) =

10, for |x1 − x2| ≤ 1,
−1, for 1 ≤ |x1 − x2| ≤ 3,
0, otherwise.
(16.23)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SIMULATING MATTER WITH MOLECULAR DYNAMICS 375
Figure 16.7 A simulation of a projectile shot into a group of particles. (Courtesy of J. Wetzel.)
Use a simple algorithm to determine the positions of the particles up until the time they
leave the box. Make sure to apply periodic boundary conditions. Hint: Since the config-
uration is symmetric, you know the location of particle 2 by symmetry and do not need
to solve for it. We suggest the Verlet algorithm (no velocities) with a forward-difference
algorithm to initialize it. To speed things along, use a time step of h = 1/
√
2.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Seventeen
PDEs for Electrostatics & Heat Flow
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
- - - *
Applets
Name Sections Name Sections
Heat Equation 17.16–17.18 -
17.1 PDE GENERALITIES
Physical quantities such as temperature and pressure vary continuously in both space and time.
Such being our world, the function or field U(x, y, z, t) used to describe these quantities must
contain independent space and time variations. As time evolves, the changes in U(x, y, z, t) at
any one position affect the field at neighboring points. This means that the dynamic equations
describing the dependence of U on four independent variables must be written in terms of
partial derivatives, and therefore the equations must be partial differential equations (PDEs),
in contrast to ordinary differential equations (ODEs).
The most general form for a PDE with two independent variables is
A
∂2U
∂x2
+ 2B
∂2U
∂x∂y
+ C
∂2U
∂y2
+D
∂U
∂x
+ E
∂U
∂y
= F, (17.1)
where A, B, C, and F are arbitrary functions of the variables x and y. In the table below we
define the classes of PDEs by the value of the discriminant d in the second row [A&W 01],
with the next two rows being examples:
We usually think of a parabolic equation as containing a first-order derivative in one variable
and a second-order derivative in the other; a hyperbolic equation as containing second-order
derivatives of all the variables, with opposite signs when placed on the same side of the equal
Elliptic Parabolic Hyperbolic
d = AC −B2 > 0 d = AC −B2 = 0 d = AC −B2 < 0
∇2U(x) = −4πρ(x) ∇2U(x, t) = a ∂U/∂t ∇2U(x, t) = c−2∂2U/∂t2
Poisson’s Heat Wave
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 377
Table 17.1 The Relation Between Boundary Conditions and Uniqueness for PDEs.
Boundary Elliptic Hyperbolic Parabolic
Condition (Poisson Equation) (Wave Equation) (Heat Equation)
Dirichlet open surface Underspecified Underspecified Unique and stable (1-D)
Dirichlet closed surface Unique and stable Overspecified Overspecified
Neumann open surface Underspecified Underspecified Unique and Stable (1-D)
Neumann closed surface Unique and stable Overspecified Overspecified
Cauchy open surface Nonphysical Unique and stable Overspecified
Cauchy closed surface Overspecified Overspecified Overspecified
sign; and an elliptic equation as containing second-order derivatives of all the variables, with
all having the same sign when placed on the same side of the equal sign.
After solving enough problems, one often develops some physical intuition as to whether
one has sufficient boundary conditions for there to exist a unique solution for a given physical
situation (this, of course, is in addition to requisite initial conditions). For instance, a string tied
at both ends and a heated bar placed in an infinite heat bath are physical situations for which the
boundary conditions are adequate. If the boundary condition is the value of the solution on a
surrounding closed surface, we have a Dirichlet boundary condition. If the boundary condition
is the value of the normal derivative on the surrounding surface, we have a Neumann boundary
condition. If the value of both the solution and its derivative are specified on a closed boundary,
we have a Cauchy boundary condition. Although having an adequate boundary condition
is necessary for a unique solution, having too many boundary conditions, for instance, both
Neumann and Dirichlet, may be an overspecification for which no solution exists.1
Solving PDEs numerically differs from solving ODEs in a number of ways. First, be-
cause we are able to write all ODEs in a standard form,
dy(t)
dt
= f(y, t), (17.2)
with t the single independent variable, we are able to use a standard algorithm, rk4 in our case,
to solve all such equations. Yet because PDEs have several independent variables, for example,
ρ(x, y, z, t), we would have to apply (17.2) simultaneously and independently to each variable,
which would be very complicated. Second, since there are more equations to solve with PDEs
than with ODEs, we need more information than just the two initial conditions [x(0), ẋ(0)]. In
addition, because each PDE often has its own particular set of boundary conditions, we have to
develop a special algorithm for each particular problem.
17.2 UNIT I. ELECTROSTATIC POTENTIALS
Your problem is to find the electric potential for all points inside the charge-free square shown
in Figure 17.1. The bottom and sides of the region are made up of wires that are “grounded”
(kept at 0 V). The top wire is connected to a battery that keeps it at a constant 100 V.
1Although conclusions drawn for exact PDEs may differ from those drawn for the finite-difference equations, they are usually
the same; in fact, Morse and Feshbach [M&F 53] use the finite-difference form to derive the relations between boundary conditions
and uniqueness for each type of equation shown in Table 17.1 [Jack 88].
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
378 CHAPTER 17
Figure 17.1 Left: The shaded region of space within a square in which we want to determine the electric potential.
There is a wire at the top kept at a constant 100 V and a grounded wire (dashed) at the sides and bottom.
Right: The electric potential as a function of x and y. The projections onto the shaded xy plane are
equipotential surfaces or lines.
x
y
V(x, y)
x y
v
17.2.1 Laplace’s Elliptic PDE ( Theory)
We consider the entire square in Figure 17.1 as our boundary with the voltages prescribed upon
it. If we imagine infinitesimal insulators placed at the top corners of the box, then we will have
a closed boundary within which we will solve our problem. Since the values of the potential are
given on all sides, we have Neumann conditions on the boundary and, according to Table 17.1,
a unique and stable solution.
It is known from classical electrodynamics that the electric potential U(x) arising from
static charges satisfies Poisson’s PDE [Jack 88]:
∇2U(x) = −4πρ(x), (17.3)
where ρ(x) is the charge density. In charge-free regions of space, that is, regions where ρ(x) =
0, the potential satisfies Laplace’s equation:
∇2U(x) = 0. (17.4)
Both these equations are elliptic PDEs of a form that occurs in various applications. We solve
them in 2-D rectangular coordinates:
xmll ∂2U(x, y)
∂x2
+
∂2U(x, y)
∂y2
=
{
0, Laplace’s equation,
−4πρ(x), Poisson’s equation.
(17.5)
In both cases we see that the potential depends simultaneously on x and y. For Laplace’s
equation, the charges, which are the source of the field, enter indirectly by specifying the
potential values in some region of space; for Poisson’s equation they enter directly.
17.3 FOURIER SERIES SOLUTION OF A PDE
For the simple geometry of Figure 17.1 an analytic solution of Laplace’s equation
∂2U(x, y)
∂x2
+
∂2U(x, y)
∂y2
= 0 (17.6)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 379
exists in the form of an infinite series. If we assume that the solution is the product of indepen-
dent functions of x and y and substitute the product into (17.6), we obtain
U(x, y) = X(x)Y (y) ⇒ d
2X(x)/dx2
X(x)
+
d2Y (y)/dy2
Y (y)
= 0. (17.7)
BecauseX(x) is a function of only x, and Y (y) of only y, the derivatives in (17.7) are ordinary
as opposed to partial derivatives. Since X(x) and Y (y) are assumed to be independent, the
only way (17.7) can be valid for all values of x and y is for each term in (17.7) to be equal to a
constant:
d2Y (y)/dy2
Y (y)
=−d
2X(x)/dx2
X(x)
= k2, (17.8)
⇒ d
2X(x)
dx2
+ k2X(x) = 0,
d2Y (y)
dy2
− k2Y (y) = 0. (17.9)
We shall see that this choice of sign for the constant matches the boundary conditions and gives
us periodic behavior in x. The other choice of sign would give periodic behavior in y, and that
would not work with these boundary conditions.
The solutions for X(x) are periodic, and those for Y (y) are exponential:
X(x) = A sin kx+B cos kx, Y (y) = Ceky +De−ky. (17.10)
The x = 0 boundary condition U(x = 0, y) = 0 can be met only if B = 0. The x = L
boundary condition U(x = L, y) = 0 can be met only for
kL = nπ, n = 1, 2, . . . . (17.11)
Such being the case, for each value of n there is the solution
Xn(x) = An sin
(nπ
L
x
)
. (17.12)
For each value of kn that satisfies the x boundary conditions, Y (y) must satisfy the y boundary
condition U(x, 0) = 0, which requires D = −C:
Yn(y) = C(ekny − e−kny) ≡ 2C sinh
(nπ
L
y
)
. (17.13)
Because we are solving linear equations, the principle of linear superposition holds, which
means that the most general solution is the sum of the products:
U(x, y) =
∞∑
n=1
En sin
(nπ
L
x
)
sinh
(nπ
L
y
)
. (17.14)
The En values are arbitrary constants and are fixed by requiring the solution to satisfy the
remaining boundary condition at y = L, U(x, y = L) = 100 V:
∞∑
n=1
En sin
nπ
L
x sinhnπ = 100 V. (17.15)
We determine the constants En by projection: Multiply both sides of the equation by
sinmπ/Lx, with m an integer, and integrate from 0 to L:
∞∑
n
En sinhnπ
∫ L
0
dx sin
nπ
L
x sin
mπ
L
x =
∫ L
0
dx 100 sin
mπ
L
x. (17.16)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
380 CHAPTER 17
Figure 17.2 The analytic (Fourier series) solution of Laplace’s equation showing Gibbs-overshoot oscillations near
x = 0. The solution shown here uses 21 terms, yet the oscillations remain even if a large number of
terms is summed.
0
0
100
0
20
20
40
x y
V(x,y)
The integral on the LHS is nonzero only for n = m, which yields
En =
{
0, for n even,
4(100)
nπ sinhnπ , for n odd.
(17.17)
Finally, we obtain the potential at any point (x, y) as
xmll U(x, y) =
∞∑
n=1,3,5,...
400
nπ
sin
(nπx
L
) sinh(nπy/L)
sinh(nπ)
. (17.18)
17.3.1 Polynomial Expansion As an Algorithm
It is worth pointing out that even though a product of separate functions of x and y is an
acceptable form for a solution to Laplace’s equation, this does not mean that the solution to
realistic problems will have this form. Indeed, a realistic solution can be expressed as an infinite
sum of such products, but the sum is no longer separable. Worse than that, as an algorithm, we
must stop the sum at some point, yet the series converges so painfully slowly that many terms
are needed, and so round-off error may become a problem. In addition, the sinh functions in
(17.18) overflow for large n, which can be avoided somewhat by expressing the quotient of the
two sinh functions in terms of exponentials and then taking a large n limit:
sinh(nπy/L)
sinh(nπ)
=
enπ(y/L−1) − e−nπ(y/L+1)
1− e−2nπ
→ enπ(y/L−1). (17.19)
A third problem with the “analytic” solution is that a Fourier series converges only in the mean
square (Figure 17.2). This means that it converges to the average of the left- and right-hand
limits in the regions where the solution is discontinuous [Krey 98], such as in the corners
of the box. Explicitly, what you see in Figure 17.2 is a phenomenon known as the Gibbs
overshoot that occurs when a Fourier series with a finite number of terms is used to represent
a discontinuous function. Rather than fall off abruptly, the series develops large oscillations
that tend to overshoot the function at the corner. To obtain a smooth solution, we had to sum
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 381
Figure 17.3 The algorithm for Laplace’s equation in which the potential at the point (x, y) = (i, j)∆ equals the aver-
age of the potential values at the four nearest-neighbor points. The nodes with white centers correspond
to fixed values of the potential along the boundaries.
i, j+1
i-1, j
i, j-1
i, j i+1, j
y
x
40,000 terms, where, in contrast, the numerical solution required only hundreds of iterations.
17.4 SOLUTION: FINITE-DIFFERENCE METHOD
To solve our 2-D PDE numerically, we divide space up into a lattice (Figure 17.3) and solve for
U at each site on the lattice. Since we will express derivatives in terms of the finite differences
in the values of U at the lattice sites, this is called a finite-difference method. A numerically
more efficient, but also more complicated approach, is the finite-element method (Unit II),
which solves the PDE for small geometric elements and then matches the elements.
To derive the finite-difference algorithm for the numeric solution of (17.5), we follow the
same path taken in § 7.1 to derive the forward-difference algorithm for differentiation. We start
by adding the two Taylor expansions of the potential to the right and left of (x, y) and above
and below (x, y):
U(x+ ∆x, y) =U(x, y) +
∂U
∂x
∆x+
1
2
∂2U
∂x2
(∆x)2 + · · · , (17.20)
U(x−∆x, y) =U(x, y)− ∂U
∂x
∆x+
1
2
∂2U
∂x2
(∆x)2 − · · · . (17.21)
All odd terms cancel when we add these equations, and we obtain a central-difference approx-
imation for the second partial derivative good to order ∆4:
∂2U(x, y)
∂x2
' U(x+ ∆x, y) + U(x−∆x, y)− 2U(x, y)
(∆x)2
, (17.22)
∂2U(x, y)
∂y2
' U(x, y + ∆y) + U(x, y −∆y)− 2U(x, y)
(∆y)2
. (17.23)
Substituting both these approximations in Poisson’s equation (17.5) leads to a finite-difference
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
382 CHAPTER 17
form of the PDE:
U(x+ ∆x, y) + U(x−∆x, y)− 2U(x, y)
(∆x)2
+
U(x, y + ∆y) + U(x, y −∆y)− 2U(x, y)
(∆y)2
= −4πρ.
We assume that the x and y grids are of equal spacings ∆x = ∆y = ∆, and so the algorithm
takes the simple form
U(x+ ∆, y) + U(x−∆, y) + U(x, y + ∆) + U(x, y −∆)− 4U(x, y) = −4πρ. (17.24)
The reader will notice that this equation shows a relation among the solutions at five points in
space. When U(x, y) is evaluated for the Nx x values on the lattice and for the Ny y values,
we obtain a set of Nx ×Ny simultaneous linear algebraic equations for U[i][j] to solve. One
approach is to solve these equations explicitly as a (big) matrix problem. This is attractive, as
it is a direct solution, but it requires a great deal of memory and accounting. The approach we
follow here is based on the algebraic solution of (17.24) for U(x, y):
U(x, y)' 1
4
[U(x+ ∆, y) + U(x−∆, y) + U(x, y + ∆) + U(x, y −∆)]
+πρ(x, y)∆2, (17.25)
where we would omit the ρ(x) term for Laplace’s equation. In terms of discrete locations on
our lattice, the x and y variables are
x = x0 + i∆, y = y0 + j∆, i, j = 0, . . . , Nmax-1, (17.26)
where we have placed our lattice in a square of side L. The finite-difference algorithm (17.25)
becomes
Ui,j =
1
4
[Ui+1,j + Ui−1,j + Ui,j+1 + Ui,j−1] + πρ(i∆, j∆)∆2. (17.27)
This equation says that when we have a proper solution, it will be the average of the potential
at the four nearest neighbors (Figure 17.3) plus a contribution from the local charge density. As
an algorithm, (17.27) does not provide a direct solution to Poisson’s equation but rather must
be repeated many times to converge upon the solution. We start with an initial guess for the
potential, improve it by sweeping through all space taking the average over nearest neighbors
at each node, and keep repeating the process until the solution no longer changes to some level
of precision or until failure is evident. When converged, the initial guess is said to have relaxed
into the solution.
A reasonable question with this simple an approach is, “Does it always converge, and if
so, does it converge fast enough to be useful?” In some sense the answer to the first question is
not an issue; if the method does not converge, then we will know it; otherwise we have ended
up with a solution and the path we followed to get there does not matter! The answer to the
question of speed is that relaxation methods may converge slowly (although still faster than a
Fourier series), yet we will show you two clever tricks to accelerate the convergence.
At this point it is important to remember that our algorithm arose from expressing the
Laplacian∇2 in rectangular coordinates. While this does not restrict us from solving problems
with circular symmetry, there may be geometries where it is better to develop an algorithm
based on expressing the Laplacian in cylindrical or spherical coordinates in order to have grids
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 383
that fit the geometry better.
17.4.1 Relaxation and Overrelaxation
There are a number of ways in which the algorithm (17.25) can be iterated so as to convert the
boundary conditions to a solution. Its most basic form is the Jacobi method and is one in which
the potential values are not changed until an entire sweep of applying (17.25) at each point is
completed. This maintains the symmetry of the initial guess and boundary conditions. A rather
obvious improvement on the Jacobi method employs the updated guesses for the potential in
(17.25) as soon as they are available. As a case in point, if the sweep starts in the upper-left-
hand corner of Figure 17.3, then the leftmost (i --1, j) and topmost (i, j--1) values of
the potential used will be from the present generation of guesses, while the other two values of
the potential will be from the previous generation:
xmllU (new)i,j = 14
[
U
(old)
i+1,j + U
(new)
i−1,j + U
(old)
i,j+1 + U
(new)
i,j−1
]
(Gauss–Seidel method) (17.28)
This technique, known as the Gauss–Seidel (GS) method, usually leads to accelerated conver-
gence, which in turn leads to less round-off error. It also uses less memory as there is no need
to store two generations of guesses. However, it does distort the symmetry of the boundary
conditions, which one hopes is insignificant when convergence is reached.
A less obvious improvement in the relaxation technique, known as successive overrelax-
ation (SOR), starts by writing the algorithm (17.25) in a form that determines the new values
of the potential U (new) as the old values U (old) plus a correction or residual r:
U
(new)
i,j = U
(old)
i,j + ri,j . (17.29)
While the Gauss–Seidel technique may still be used to incorporate the updated values in U (old)
to determine r, we rewrite the algorithm here in the general form:
ri,j ≡U (new)i,j − U
(old)
i,j
=
1
4
[
U
(old)
i+1,j + U
(new)
i−1,j + U
(old)
i,j+1 + U
(new)
i,j−1
]
− U (old)i,j . (17.30)
The successive overrelaxation technique [Pres 94, Gar 00] proposes that if convergence is ob-
tained by adding r to U , then even more rapid convergence might be obtained by adding more
or less of r:
xmllU (new)i,j = U (old)i,j + ωri,j , (SOR), (17.31)
where ω is a parameter that amplifies or reduces the residual. The nonaccelerated relaxation
algorithm (17.28) is obtained with ω = 1, accelerated convergence (overrelaxation) is obtained
with ω ≥ 1, and underrelaxation is obtained with ω < 1. Values of 1 ≤ ω ≤ 2 often work well,
yet ω > 2 may lead to numerical instabilities. Although a detailed analysis of the algorithm is
needed to predict the optimal value for ω, we suggest that you explore different values for ω to
see which one works best for your particular problem.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
384 CHAPTER 17
17.4.2 Lattice PDE Implementation
In Listing 17.1 we present the code LaplaceLine.py that solves the square-wire problem (Fig-
ure 17.1). Here we have kept the code simple by setting the length of the box L = Nmax∆ =
100 and by taking ∆ = 1:
U(i,Nmax) = 99 (top), U(1, j) = 0 (left),
U(Nmax, j) = 0 (right), U(i, 1) = 0 (bottom),
(17.32)
We run the algorithm (17.27) for a fixed 1000 iterations. A better code would vary ∆ and
the dimensions and would quit iterating once the solution converges to some tolerance. Study,
compile, and execute the basic code.
Listing 17.1 LaplaceLine.py solves Laplace’s equation via relaxation. The various parameters need to be adjusted
for an accurate solution. 
""" LaplaceLine.py: Solution of Laplace’s eqtn with 3D matplot """
from numpy i m p o r t ∗ ; i m p o r t p y l a b as p ; i m p o r t m a t p l o t l i b . axes3d as p3
p r i n t ("Initializing" )
Nmax = 100 ; N i t e r = 7 0 ; V = z e r o s ( ( Nmax , Nmax) , f l o a t ) # f l o a t maybe F l o a t
p r i n t "Working hard, wait for the figure while I count to 60"
f o r k i n r a n g e ( 0 , Nmax−1) : V[ k , 0 ] = 100 .0 # l i n e a t 100V
f o r i t e r i n r a n g e ( N i t e r ) : # i t e r a t i o n s ove r a l g o r i t h m
i f i t e r %10 == 0 : p r i n t i t e r
f o r i i n r a n g e ( 1 , Nmax−2) :
f o r j i n r a n g e ( 1 , Nmax−2) : V[ i , j ] = 0 . 2 5∗ (V[ i +1 , j ]+V[ i−1, j ]+V[ i , j +1]+V[ i , j −1])
x = r a n g e ( 0 , Nmax−1, 2 ) ; y = r a n g e ( 0 , 50 , 2 ) # p l o t e v e r y o t h e r p o i n t
X, Y = p . meshgr id ( x , y )
d e f f u n c t z (V) : # F u n c t i o n r e t u r n s V( x , y )
z = V[X,Y]
r e t u r n z
Z = f u n c t z (V)
f i g = p . f i g u r e ( ) # C r e a t e f i g u r e
ax = p3 . Axes3D ( f i g ) # p l o t axes
ax . p l o t w i r e f r a m e (X, Y, Z , c o l o r = ’r’ ) # r e d w i r e f r a m e
ax . s e t x l a b e l (’X’ ) # l a b e l axes
ax . s e t y l a b e l (’Y’ )
ax . s e t z l a b e l (’Potential’ )
p . show ( ) # d i s p l a y f i g , c l o s e s h e l l t o q u i t
17.5 ASSESSMENT VIA SURFACE PLOT
After executing LaplaceLine.py, you should see a surface plot like Figure 17.1. Study this
file in order to understand how to make surface plots with Matplotlib in Python. Seeing that it
is important to visualize your output to ensure the reasonableness of the solution, you should
learn how to make such a plot before exploring more interesting problems. The 3-D surface
plots we show in this chapter were made with both gnuplot, and below we show how to do that.
> gnuplot Start Gnuplot system from a shell
gnuplot> set hidden3d Hide surface whose view is blocked
gnuplot> set unhidden3d Show surface though hidden from view
gnuplot> splot ‘Laplace.dat’ with lines Surface plot of Laplace.dat with lines
gnuplot> set view 65,45 Set x and y rotation viewing angles
gnuplot> replot See effect of your change
gnuplot> set contour Project contours onto xy plane
gnuplot> set cntrparam levels 10 10 contour levels
gnuplot> set terminal PostScript Output in PostScript format for printing
gnuplot> set output "Laplace.ps" Output to file Laplace.ps
gnuplot> splot ‘Laplace.dat’ w l Plot again, output to file
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 385
Figure 17.4 Left: A simple model of a parallel-plate capacitor within a box. A realistic model would have the plates
close together, in order to condense the field, and the enclosing grounded box so large that it has no
effect on the field near the capacitor. Right: A numerical solution for the electric potential for this
geometry. The projection on the xy plane gives the equipotential lines.
100 V
–100 V
(X, Y)
d
w
L
L
0
20
40
60
80
100 0
20
40
60
80
100
-100
0
100
100
80
60
40
20
0
-20
-40
-60
-80
V(x, y)
xy
gnuplot> set terminal x11 To see output on screen again
gnuplot> set title ‘Potential V(x,y) vs x,y’ Title graph
gnuplot> set xlabel ‘x Position’ Label x axis
gnuplot> set ylabel ‘y Position’ Label y axis
gnuplot> set zlabel ‘V(x,y)’; replot Label z axis and replot
gnuplot> help Tell me more
gnuplot> set nosurface Do not draw surface; leave contours
gnuplot> replot Draw plot again; may want to write to file
gnuplot> quit Get out of Gnuplot
Because Gnuplot 4 and later versions permit you to rotate surface plots interactively, we rec-
ommend that you do just that to find the best viewing angle. Changes made to a plot are seen
when you redraw the plot using the replot command. For this sample session the default out-
put for your graph is your terminal screen. To print a paper copy of your graph we recommend
first saving it to a file as a PostScript document (suffix .ps) and then printing out that file to a
PostScript printer. You create the PostScript file by changing the terminal type to Postscript,
setting the name of the file, and then issuing the subcommand splot again. This plots the result
out to a file. If you want to see plots on your screen again, set the terminal type back to x11
again (for Unix’s X Windows System) and then plot it again.
17.6 ALTERNATE CAPACITOR PROBLEMS
We give you (or your instructor) a choice now. You can carry out the assessment using our
wire-plus-grounded-box problem, or you can replace that problem with a more interesting
one involving a realistic capacitor or nonplanar capacitors. We now describe the capacitor
problem and then move on to the assessment and exploration.
Elementary textbooks solve the capacitor problem for the uniform field confined between two
infinite plates. The field in a finite capacitor varies near the edges (edge effects) and extends
beyond the edges of the capacitor (fringe fields). We model the realistic capacitor in a grounded
box (Figure 17.4) as two plates (wires) of finite length. Write your simulation such that it
is convenient to vary the grid spacing ∆ and the geometry of the box and plate. We pose
three versions of this problem, each displaying somewhat different physics. In each case the
boundary condition V = 0 on the surrounding box must be imposed for all iterations in order
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
386 CHAPTER 17
Figure 17.5 A guess as to how charge may rearrange itself on finite conducting plates.
+ + + + + +++ +
− − − − − − − − −
+
+ +
− −
+
−
100V
-100V
to obtain a unique solution.
loading laplacew
1. For the simplest version, assume that the plates are very thin sheets of conductors, with
the top plate maintained at 100 V and the bottom at −100 V. Because the plates are con-
ductors, they must be equipotential surfaces, and a battery can maintain them at constant
voltages. Write or modify the given program to solve Laplace’s equation such that the
plates have fixed voltages.
2. For the next version of this problem, assume that the plates are composed of a line
of dielectric material with uniform charge densities ρ on the top and −ρ on the bottom.
Solve Poisson’s equation (17.3) in the region including the plates, and Laplace’s equation
elsewhere. Experiment until you find a numerical value for ρ that gives a potential similar
to that shown in Figure 17.6 for plates with fixed voltages.
3. For the final version of this problem investigate how the charges on a capacitor with
finite-thickness conducting plates (Figure 17.5) distribute themselves. Since the plates
are conductors, they are still equipotential surfaces at 100 and −100 V, only now they
have a thickness of at least 2∆ (so we can see the difference between the potential near
the top and the bottom surfaces of the plates). Such being the case, we solve Laplace’s
equation (17.4) much as before to determine U(x, y). Once we have U(x, y), we sub-
stitute it into Poisson’s equation (17.3) and determine how the charge density distributes
itself along the top and bottom surfaces of the plates. Hint: Since the electric field is no
longer uniform, we know that the charge distribution also will no longer be uniform. In
addition, since the electric field now extends beyond the ends of the capacitor and since
field lines begin and end on charge, some charge may end up on the edges and outer
surfaces of the plates (Figure 17.4).
4. The numerical solution to our PDE can be applied to arbitrary boundary conditions. Two
boundary conditions to explore are triangular and sinusoidal:
U(x) =
200x/w, for x ≤ w/2,100(1− x/w), for x ≥ w/2, U(x) = 100 sin
(
2πx
w
)
.
5. Square conductors: You have designed a piece of equipment consisting of a small
metal box at 100 V within a larger grounded one (Figure 17.8). You find that sparking
occurs between the boxes, which means that the electric field is too large. You need to
determine where the field is greatest so that you can change the geometry and eliminate
the sparking. Modify the program to satisfy these boundary conditions and to determine
the field between the boxes. Gauss’s law tells us that the field vanishes within the inner
box because it contains no charge. Plot the potential and equipotential surfaces and
sketch in the electric field lines. Deduce where the electric field is most intense and try
redesigning the equipment to reduce the field.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 387
Figure 17.6 Left: A Gnuplot visualization of the computed electric potential for a capacitor with finite width plates.
Right: An OpenDX visualization of the charge distribution along one plate determined by evaluating
∇2V(x, y) (courtesy of J. Wetzel). Note the “lightening rod” effect of charge accumulating at corners
and points.
0 10
20 30
40
0
10
20
30
40
–100
0
V(x, y)
x
y
Figure 17.7 Left: Computed equipotential surfaces and electric field lines for a realistic capacitor. Right: Equipo-
tential surfaces and electric field lines mapped onto the surface for a 3-D capacitor constructed from
two tori (see OpenDX in Appendix C).
6. Cracked cylindrical capacitor: You have designed the cylindrical capacitor containing
a long outer cylinder surrounding a thin inner cylinder (Figure 17.8 right). The cylinders
have a small crack in them in order to connect them to the battery that maintains the
inner cylinder at −100 V and outer cylinder at 100 V. Determine how this small crack
affects the field configuration. In order for a unique solution to exist for this problem,
place both cylinders within a large grounded box. Note that since our algorithm is based
on expansion of the Laplacian in rectangular coordinates, you cannot just convert it to a
radial and angle grid.
17.7 IMPLEMENTATION AND ASSESSMENT
1. Write or modify the CD program to find the electric potential for a capacitor within a
grounded box. Use the labeling scheme on the left in Figure 17.4.
2. To start, have your program undertake 1000 iterations and then quit. During debugging,
examine how the potential changes in some key locations as you iterate toward a solution.
3. Repeat the process for different step sizes ∆ and draw conclusions regarding the stability
and accuracy of the solution.
4. Once your program produces reasonable solutions, modify it so that it stops iterating
after convergence is reached, or if the number of iterations becomes too large. Rather
than trying to discern small changes in highly compressed surface plots, use a numerical
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
388 CHAPTER 17
measure of precision, for example,
trace =
∑
i
|V[i][i]|,
which samples the solution along the diagonal. Remember, this is a simple algorithm
and so may require many iterations for high precision. You should be able to obtain
changes in the trace that are less than 1 part in 104. (The break command or a while
loop is useful for this type of test.)
5. Equation (17.31) expresses the successive overrelaxation technique in which conver-
gence is accelerated by using a judicious choice of ω. Determine by trial and error the
approximate best value of ω. You will be able to double the speed.
6. Now that the code is accurate, modify it to simulate a more realistic capacitor in which
the plate separation is approximately 110 of the plate length. You should find the field
more condensed and more uniform between the plates.
7. If you are working with the wire-in-the-box problem, compare your numerical solution
to the analytic one (17.18). Do not be surprised if you need to sum thousands of terms
before the analytic solution converges!
17.8 ELECTRIC FIELD VISUALIZATION (EXPLORATION)
Plot the equipotential surfaces on a separate 2-D plot.electric field by drawing curves orthogo-
nal to the equipotential lines, beginning and ending on the boundaries (where the charges lie).
The regions of high density are regions of high electric field. Physics tells us that the electric
field E is the negative gradient of the potential:
E = −∇U(x, y) = −∂U(x, y)
∂x
̂x −
∂U(x, y)
∂y
̂y, (17.33)
where ̂i is a unit vector in the i direction. While at first it may seem that some work is involved
in determining these derivatives, once you have a solution for U(x, y) on a grid, it is simple to
use the central-difference approximation for the derivative to determine the field, for example:
Ex '
U(x+ ∆, y)− U(x−∆, y)
2∆
=
Ui+1,j − Ui−1,j
2∆
. (17.34)
Once you have a data file representing such a vector field, it can be visualized by plotting
arrows of varying lengths and directions, or with just lines (Figure 17.7). This is possible in
Maple and Mathematica [L 05] or with vectors style in Gnuplot2, where N is a normalization
factor.
17.9 LAPLACE QUIZ
You are given a simple Laplace-type equation
∂u
∂x
+
∂u
∂y
= −ρ(x, y),
where x and y are Cartesian spatial coordinates and ρ(x, y) is the charge density in space.
1. Develop a simple algorithm that will permit you to solve for the potential u between two
square conductors kept at fixed u, with a charge density ρ between them.
2The Gnuplot command plot "Laplace field.dat" using 1:2:3:4 with Vectors plots variable-length ar-
rows at (x, y) with components Dx∝ Ex and Dy∝ Ey. You determine empirically what scale factor gives you the best visualization
(nonoverlapping arrows). Accordingly, you output data lines of the form (x, y, Ex/N, Ey/N)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 389
Figure 17.8 Left: The geometry of a capacitor formed by placing two long, square cylinders within each other.
Right: The geometry of a capacitor formed by placing two long, circular cylinders within each other.
The cylinders are cracked on the side so that wires can enter the region.
2. Make a simple sketch that shows with arrows how your algorithm works.
3. Make sure to specify how you start and terminate the algorithm.
4. Thinking outside the box: Find the electric potential for all points outside the charge-
free square shown in Figure 17.1. Is your solution unique?
17.10 UNIT II. FINITE-ELEMENT METHOD 
In this unit we solve a simpler problem than the one in Unit I (1-D rather than 2-D), but we
do it with a less simple algorithm (finite element). Our usual approach to PDEs in this text
uses finite differences to approximate various derivatives in terms of the finite differences of
a function evaluated upon a fixed grid. The finite-element method (FEM), in contrast, breaks
space up into multiple geometric objects (elements), determines an approximate form for the
solution appropriate to each element, and then matches the solutions up at the domain edges.
The theory and practice of FEM as a numerical method for solving partial differential equations
have been developed over the last 30 years and still provide an active field of research. One of
the theoretical strengths of FEM is that its mathematical foundations allow for elegant proofs of
the convergence of solutions to many delicate problems. One of the practical strengths of FEM
is that it offers great flexibility for problems on irregular domains or highly varying coefficients
or singularities. Although finite differences are simpler to implement than FEM, they are less
robust mathematically and less efficient in terms of computer time for big problems. Finite
elements in turn are more complicated to implement but more appropriate and precise for
complicated equations and complicated geometries. In addition, the same basic finite-element
technique can be applied to many problems with only minor modifications and yields solutions
that may be evaluated for any value of x, not just those on a grid. In fact, the finite-elements
method with various preprogrammed multigrid packages has very much become the standard
for large-scale practical applications. Our discussion is based upon [Shaw 92, Li, Otto].
17.11 ELECTRIC FIELD FROM CHARGE DENSITY (PROBLEM)
You are given two conducting plates a distance b− a apart, with the lower one kept at potential
Ua, the upper plate at potential Ub, and a uniform charge density ρ(x) placed between them
(Figure 17.9). Your problem is to compute the electric potential between the plates.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
390 CHAPTER 17
Figure 17.9 Two metal plates with a charge density between them. The dots are the nodes xi, and the lines connect-
ing the nodes are the finite elements.
node
element
 x
b
x0
xN
17.12 ANALYTIC SOLUTION
The relation between charge density ρ(x) and potential U(x) is given by Poisson’s equation
(17.5). For our problem, the potential U changes only in the x direction, and so the PDE
becomes the ODE:
d2U(x)
dx2
= −4πρ(x) = −1, 0 < x < 1, (17.35)
where we have set ρ(x) = 1/4π to simplify the programming. The solution we want is subject
to the Dirichlet boundary conditions:
U(x = a = 0) = 0, U(x = b = 1) = 1, (17.36)
⇒ U(x) = −x
2
(x− 3). (17.37)
Although we know the analytic solution, we shall develop the finite-element method for solving
the ODE as if it were a PDE (it would be in 2-D) and as if we did not know the solution.
Although we will not demonstrate it, this method works equally well for any charge density
ρ(x).
17.13 FINITE-ELEMENT (NOT DIFFERENCE) METHODS
In a finite-element method, the domain in which the PDE is solved is split into finite subdo-
mains, called elements, and a trial solution to the PDE in each subdomain is hypothesized.
Then the parameters of the trial solution are adjusted to obtain a best fit (in the sense of Chap-
ter 8, “Solving Systems of Equations with Matrices; Data Fitting”) to the exact solution. The
numerically intensive work is in finding the best values for these parameters and in matching
the trial solutions for the different subdomains. A FEM solution follows six basic steps [Li]:
1. Derivation of a weak form of the PDE. This is equivalent to a least-squares minimization
of the integral of the difference between the approximate and exact solutions.
2. Discretization of the computational domains.
3. Generation of interpolating or trial functions.
4. Assembly of the stiffness matrix and load vector.
5. Implementation of the boundary conditions.
6. Solution of the resulting linear system of equations.
17.13.1 Weak Form of PDE
Finite-difference methods look for an approximate solution of an approximate PDE. Finite-
element methods strive to obtain the best possible global agreement of an approximate trial
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 391
solution with the exact solution. We start with the differential equation
−d
2U(x)
dx2
= 4πρ(x). (17.38)
A measure of overall agreement must involve the solution U(x) over some region of space,
such as the integral of the trial solution. We can obtain such a measure by converting the
differential equation (17.38) to its equivalent integral or weak form. We assume that we have
an approximate or trial solution Φ(x) that vanishes at the endpoints, Φ(a) = Φ(b) = 0 (we
satisfy the boundary conditions later). We next multiply both sides of the differential equation
(17.38) by Φ:
− d
2U(x)
dx2
Φ(x) = 4πρ(x)Φ(x). (17.39)
Next we integrate (17.39) by parts from a to b:
−
∫ b
a
dx
d2U(x)
dx2
Φ(x) =
∫ b
a
dx 4πρ(x) Φ(x), (17.40)
−dU(x)
dx
Φ(x) |ba +
∫ b
a
dx
dU(x)
dx
Φ′(x) =
∫ b
a
dx 4πρ(x) Φ(x),
⇒
∫ b
a
dx
dU(x)
dx
Φ′(x) =
∫ b
a
dx 4πρ(x) Φ(x). (17.41)
Equation (17.41) is the weak form of the PDE. The unknown exact solution U(x) and the trial
function Φ are still to be specified. Because the approximate and exact solutions are related by
the integral of their difference over the entire domain, the solution provides a global best fit to
the exact solution.
17.13.2 Galerkin Spectral Decomposition
The approximate solution to a weak PDE is found via a stepwise procedure. We split the full
domain of the PDE into subdomains called elements, find approximate solutions within each
element, and then match the elemental solutions onto each other. For our 1-D problem the sub-
domain elements are straight lines of equal length, while for a 2-D problem, the elements can
be parallelograms or triangles (Figure 17.9). Although life is simpler if all the finite elements
are the same size, this is not necessary. Indeed, higher precision and faster run times may be
obtained by picking small domains in regions where the solution is known to vary rapidly, and
picking large domains in regions of slow variation.
The critical step in the finite-element method is expansion of the trial solution U in terms
of a set of basis functions φi:
U(x) '
N−1∑
j=0
αjφj(x). (17.42)
We chose φi’s that are convenient to compute with and then determine the unknown expansion
coefficients αj . Even if the basis functions are not sines or cosines, this expansion is still called
a spectral decomposition. In order to satisfy the boundary conditions, we will later add another
term to the expansion. Considerable study has gone into the effectiveness of different basis
functions. If the sizes of the finite elements are made sufficiently small, then good accuracy
is obtained with simple piecewise-continuous basis functions, such as the triangles in Figure
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
392 CHAPTER 17
Figure 17.10 Left: A set of overlapping basis functions φi. Each function is a triangle from xi−1 to xi+1. Middle:
A Piecewise-linear function. Right: A piecewise-quadratic function.
x0 x1 xN-1. . .
1
0

N. . .
17.10. Specifically, we use basis functions φi that form a triangle or “hat” between xi−1 and
xi+1 and equal 1 at xi:
φi(x) =

0, for x < xi−1, or x > xi+1,
x−xi−1
hi−1
, for xi−1 ≤ x ≤ xi,
xi+1−x
hi
, for xi ≤ x ≤ xi+1,
(hi = xi+1 − xi). (17.43)
Because we have chosen φi(xi) = 1, the values of the expansion coefficients αi equal the
values of the (still-to-be-determined) solution at the nodes:
U(xi)'
N−1∑
i=0
αiφi(xi) = αiφi(xi) = αi, (17.44)
⇒ U(x)'
N−1∑
j=0
U(xj)φj(x). (17.45)
Consequently, you can think of the hat functions as linear interpolations between the solution
at the nodes.
17.13.2.1 Solution via Linear Equations
Because the basis functions φi in (17.42) are known, solving for U(x) involves determining
the coefficients αj , which are just the unknown values of U(x) on the nodes. We determine
those values by substituting the expansions for U(x) and Φ(x) into the weak form of the PDE
(17.41) and thereby convert them to a set of simultaneous linear equations (in the standard
matrix form):
Ay = b. (17.46)
We substitute the expansion U(x) '
∑N−1
j=0 αjφj(x) into the weak form (17.41):∫ b
a
dx
d
dx
N−1∑
j=0
αjφj(x)
 dΦ
dx
=
∫ b
a
dx4πρ(x)Φ(x).
By successively selecting Φ(x) = φ0, φ1, . . . , φN−1, we obtain N simultaneous linear equa-
tions for the unknown αj’s:∫ b
a
dx
d
dx
N−1∑
j=0
αjφj(x)
 dφi
dx
=
∫ b
a
dx 4πρ(x)φi(x), i = 0, . . . , N − 1. (17.47)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 393
We factor out the unknown αj’s and write out the equations explicitly:
α0
∫ b
a
φ′0φ
′
0 dx+ α1
∫ b
a
φ′0φ
′
1 dx+ · · ·+ αN−1
∫ b
a
φ′0φ
′
N−1 dx=
∫ b
a
4πρφ0 dx,
α0
∫ b
a
φ′1φ
′
0 dx+ α1
∫ b
a
φ′1φ
′
1 dx+ · · ·+ αN−1
∫ b
a
φ′1φ
′
N−1 dx=
∫ b
a
4πρφ1 dx,
. . .
α0
∫ b
a
φ′N−1φ
′
0 dx+ α1
∫
· · ·+ αN−1
∫ b
a
φ′N−1φ
′
N−1 dx=
∫ b
a
4πρφN−1 dx.
Because we have chosen the φi’s to be the simple hat functions, the derivatives are easy to
evaluate analytically (otherwise they can be done numerically):
dφi,i+1
dx
=

0, x < xi−1, or xi+1 < x,
1
hi−1
, xi−1 ≤ x ≤ xi,
−1
hi
, xi ≤ x ≤ xi+1,
0, x < xi, or xi+2 < x
1
hi
, xi ≤ x ≤ xi+1,
−1
hi+1
, xi+1 ≤ x ≤ xi+2.
(17.48)
The integrals to evaluate are∫ xi+1
xi−1
dx(φ
′
i)
2 =
∫ xi
xi−1
dx
1
(hi−1)2
+
∫ xi+1
xi
dx
1
h2i
=
1
hi−1
+
1
hi
,
∫ xi+1
xi−1
dxφ
′
iφ
′
i+1 =
∫ xi+1
xi−1
dxφ
′
i+1φ
′
i =
∫ xi+1
xi
dx
−1
h2i
= − 1
hi
,
∫ xi+1
xi−1
dx (φ
′
i+1)
2 =
∫ xi+1
xi
dx (φ
′
i+1)
2 =
∫ xi+1
xi
dx
+1
h2i
= +
1
hi
.
We rewrite these equations in the standard matrix form (17.46) with y constructed from the
unknown αj’s, and the tridiagonal stiffness matrix A constructed from the integrals over the
derivatives:
y =

α0
α1
. . .
αN−1
 , b =

∫ x1
x0
dx 4πρ(x)φ0(x)∫ x2
x1
dx 4πρ(x)φ1(x)
. . .∫ xN
xN−1
dx4πρ(x)φN−1(x)
 , (17.49)
A =

1
h0
+ 1h1 −
1
h1
− 1h0 0 . . .
− 1h1
1
h1
+ 1h2 −
1
h2
0 . . .
0 − 1h2
1
h2
+ 1h3 −
1
h3
. . .
. . . . . . − 1hN−1 −
1
hN−2
1
hN−2
+ 1hN−1

. (17.50)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
394 CHAPTER 17
The elements in A are just combinations of inverse step sizes and so do not change for different
charge densities ρ(x). The elements in b do change for different ρ’s, but the required integrals
can be performed analytically or with Gaussian quadrature (Chapter 6, “Integration”). Once A
and b are computed, the matrix equations are solved for the expansion coefficients αj contained
in y.
17.13.2.2 Dirichlet Boundary Conditions
Because the basis functions vanish at the endpoints, a solution expanded in them also vanishes
there. This will not do in general, and so we add the particular solutionUaφ0(x), which satisfies
the boundary conditions [Li]:
U(x) =
N−1∑
j=0
αjφj(x) + UaφN (x) (satisfies boundary conditions), (17.51)
where Ua = U(xa). We substitute U(x) − Uaφ0(x) into the weak form to obtain (N + 1)
simultaneous equations, still of the form Ay = b but now with
A =

A0,0 · · · A0,N−1 0
. . .
AN−1,0 · · · AN−1,N−1 0
0 0 · · · 1
 , b′ =

b0 −A0,0Ua
. . .
bN−1 −AN−1,0Ua
Ua
 . (17.52)
This is equivalent to adding a new element and changing the load vector:
b′i = bi −Ai,0Ua, i = 1, . . . , N − 1, b′N = Ua. (17.53)
To impose the boundary condition at x = b, we again add a term and substitute into the weak
form to obtain
b′i = bi −Ai,N−1Ub, i = 1, . . . , N − 1 b′N = Ub. (17.54)
We now solve the linear equations Ay = b′. For 1-D problems, 100–1000 equations are
common, while for 3-D problems there may be millions. Because the number of calculations
varies approximately as N2, it is important to employ an efficient and accurate algorithm be-
cause round-off error can easily accumulate after thousands of steps. We recommend one from
a scientific subroutine library (see Chapter 8, “Solving Systems of Equations with Matrices;
Data Fitting”).
17.14 FEM IMPLEMENTATION AND EXERCISES
In Listing 17.2 we give our program LaplaceFEM.py that determines the FEM solution, and in
Figure 17.11 we show that solution. We see on the left that three elements do not provide good
agreement with the analytic result, whereas N = 11 elements produces excellent agreement.
1. Examine the FEM solution for the choice of parameters
a = 0, b = 1, Ua = 0, Ub = 1.
2. Generate your own triangulation by assigning explicit x values at the nodes over the
interval [0, 1].
3. Start with N = 3 and solve the equations for N values up to 1000.
4. Examine the stiffness matrix A and ensure that it is triangular.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 395
5. Verify that the integrations used to compute the load vector b are accurate.
6. Verify that the solution of the linear equation Ay = b is correct.
7. Plot the numerical solution for U(x) for N = 10, 100, and 1000 and compare with the
analytic solution.
8. The log of the relative global error (number of significant figures) is
E = log10
∣∣∣∣ 1b− a
∫ b
a
dx
UFEM(x)− Uexact(x)
Uexact(x)
∣∣∣∣ .
Plot the global error versus x for N = 10, 100, and 1000.
Listing 17.2 LaplaceFEM.py provides a finite-element method solution of the 1-D Laplace equation via a Galerkin
spectral decomposition. The resulting matrix equations are solved with Matplotlib. Although the
algorithm is more involved than the solution via relaxation (Listing 17.1), it is a direct solution with
no iteration required. 
# LaplaceFEM . py : S o l u t n o f L a p l a c e Eq v i a f i n i t e e l e m e n t s method
from v i s u a l i m p o r t ∗ ; from Numeric i m p o r t ∗ ; from L i n e a r A l g e b r a i m p o r t∗
from v i s u a l i m p o r t ∗ ; from v i s u a l . g raph i m p o r t ∗
N = 1 1 ; h = 1 . / ( N − 1)
u = z e r o s ( (N) , F l o a t ) ; A = z e r o s ( (N, N) , F l o a t ) ; b = z e r o s ( (N, N) , F l o a t )
x2 = z e r o s ( ( 2 1 ) , F l o a t ) ; u fem = z e r o s ( ( 2 1 ) , F l o a t ) ; u e x a c t = z e r o s ( ( 2 1 ) , F l o a t )
e r r o r = z e r o s ( ( 2 1 ) , F l o a t ) ; x = z e r o s ( (N) , F l o a t )
g raph1 = g d i s p l a y ( wid th = 500 , h e i g h t = 500 , t i t l e = ’FEM - Exact (blue line), solution (red
points)’ ,
x t i t l e = ’x’ , y t i t l e = ’U’ , xmax =1 , ymax =1 , xmin =0 , ymin =0)
f u n c t 1 = g c u r ve ( c o l o r = c o l o r . b l u e ) ; f u n c t 2 = g d o t s ( c o l o r = c o l o r . r e d )
f u n c t 3 = g c u r ve ( c o l o r = c o l o r . cyan )
f o r i i n x r an g e ( 0 , N) : x [ i ] = i∗h
f o r i i n x r an g e ( 0 , N) : # I n i t i a l i z e
b [ i , 0 ] = 0 .
f o r j i n x r an g e ( 0 , N) : A[ i ] [ j ] = 0 .
d e f l i n 1 ( x , x1 , x2 ) : r e t u r n ( x − x1 ) / ( x2 − x1 ) # Hat func
d e f l i n 2 ( x , x1 , x2 ) : r e t u r n ( x2 − x ) / ( x2 − x1 )
d e f f ( x ) : r e t u r n 1 . # RHS of e q u a t i o n
d e f i n t 1 ( min , max ) : # Simpson
no = 1000
sum = 0 .
i n t e r v a l = ( ( max − min ) / ( no − 1) )
f o r n i n x r an g e ( 2 , no , 2 ) : # Loop odd p o i n t s
x = i n t e r v a l ∗ ( n − 1)
sum += 4 ∗ f ( x )∗ l i n 1 ( x , min , max )
f o r n i n x r an g e ( 3 , no , 2 ) : # Loop even p o i n t s
x = i n t e r v a l ∗ ( n − 1)
sum += 2 ∗ f ( x )∗ l i n 1 ( x , min , max )
sum += f ( min )∗ l i n 1 ( min , min , max ) + f ( max )∗ l i n 1 ( max , min , max )
sum ∗= i n t e r v a l / 6 .
r e t u r n ( sum )
d e f i n t 2 ( min , max ) : # Simpson
no = 1000
sum = 0 .
i n t e r v a l = ( ( max − min ) / ( no − 1) )
f o r n i n x r an g e ( 2 , no , 2 ) : # Loop odd p o i n t s
x = i n t e r v a l ∗ ( n − 1)
sum += 4 ∗ f ( x )∗ l i n 2 ( x , min , max )
f o r n i n x r an g e ( 3 , no , 2 ) : # Loop even p o i n t s
x = i n t e r v a l ∗ ( n − 1)
sum += 2 ∗ f ( x )∗ l i n 2 ( x , min , max )
sum += f ( min )∗ l i n 2 ( min , min , max ) + f ( max )∗ l i n 2 ( max , min , max )
sum ∗= i n t e r v a l / 6 .
r e t u r n ( sum )
d e f n u m e r i c a l ( x , u , xp ) :
N = 11 # i n t e r p o l a t e n u m e r i c a l s o l u t i o n
y = 0 .
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
396 CHAPTER 17
f o r i i n x r an g e ( 0 , N − 1) :
i f ( xp >= x [ i ] and xp <= x [ i + 1 ] ) :
y = l i n 2 ( xp , x [ i ] , x [ i + 1 ] )∗u [ i ] + l i n 1 ( xp , x [ i ] , x [ i + 1 ] )∗u [ i + 1 ]
r e t u r n y
d e f e x a c t ( x ) : # A n a l y t i c s o l u t i o n
u = − x∗( x − 3 . ) / 2 .
r e t u r n u
f o r i i n x r an ge ( 1 , N) :
A[ i − 1 , i − 1] = A[ i − 1 , i − 1] + 1 . / h
A[ i − 1 , i ] = A[ i − 1 , i ] − 1 . / h
A[ i , i − 1] = A[ i − 1 , i ]
A[ i , i ] = A[ i , i ] + 1 . / h
b [ i − 1 , 0 ] = b [ i − 1 , 0 ] + i n t 2 ( x [ i − 1 ] , x [ i ] )
b [ i , 0 ] = b [ i , 0 ] + i n t 1 ( x [ i − 1 ] , x [ i ] )
f o r i i n x r an ge ( 1 , N) : # D i r i c h l e t BC @ l e f t end
b [ i , 0 ] = b [ i , 0 ] − 0 .∗A[ i , 0 ]
A[ i , 0 ] = 0 .
A[ 0 , i ] = 0 .
A[ 0 , 0 ] = 1 .
b [ 0 , 0 ] = 0 .
f o r i i n x r an ge ( 1 , N) : # D i r i c h l e t bc @ r i g h t end
b [ i , 0 ] = b [ i , 0 ] − 1 .∗A[ i , N − 1]
A[ i , N − 1] = 0 .
A[N − 1 , i ] = 0 .
A[N − 1 , N − 1] = 1 .
b [N − 1 , 0 ] = 1 .
s o l = s o l v e l i n e a r e q u a t i o n s (A, b )
f o r i i n x r an ge ( 0 , N) : u [ i ] = s o l [ i , 0 ]
f o r i i n x r an ge ( 0 , 21 ) : x2 [ i ] = 0 .05∗ i
f o r i i n x r an ge ( 0 , 21) :
r a t e ( 6 )
u fem [ i ] = n u m e r i c a l ( x , u , x2 [ i ] )
u e x a c t [ i ] = e x a c t ( x2 [ i ] )
f u n c t 1 . p l o t ( pos = ( 0 . 0 5∗ i , u e x a c t [ i ] ) )
r a t e ( 6 )
f u n c t 2 . p l o t ( pos = ( 0 . 0 5∗ i , u fem [ i ] ) )
e r r o r [ i ] = u fem [ i ] − u e x a c t [ i ] # Glo ba l e r r o r
17.15 EXPLORATION
1. Modify your program to use piecewise-quadratic functions for interpolation and compare
to the linear function results.
2. Explore the resulting electric field if the charge distribution between the plates has the
explicit x dependences
ρ(x) =
1
4π

1
2 − x,
sinx,
1 at x = 0, −1 at x = 1 (a capacitor).
17.16 UNIT III. HEAT FLOW VIA TIME-STEPS (LEAPFROGS)
Problem: You are given an aluminum bar of length L = 1 m and width w aligned along
the x axis (Figure 17.12). It is insulated along its length but not at its ends. Initially the bar
is at a uniform temperature of 100 K, and then both ends are placed in contact with ice water
at 0 K. Heat flows out of the noninsulated ends only. Your problem is to determine how the
temperature will vary as we move along the length of the bar at later times.
17.17 THE PARABOLIC HEAT EQUATION (THEORY)
A basic fact of nature is that heat flows from hot to cold, that is, from regions of high temper-
ature to regions of low temperature. We give these words mathematical expression by stating
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 397
Figure 17.11 Exact (line) versus FEM solution (points) for the two-plate problem for N = 3 and N = 11 finite
elements.
0
1
0 1
U
x
N
=
3
(s
ca
led
)
N 
= 
11
Figure 17.12 A metallic bar insulated along its length with its ends in contact with ice.
100 K
that the rate of heat flow H through a material is proportional to the gradient of the temperature
T across the material:
H = −K∇T (x, t), (17.55)
where K is the thermal conductivity of the material. The total amount of heat Q(t) in the
material at any one time is proportional to the integral of the temperature over the material’s
volume:
Q(t) =
∫
dxCρ(x)T (x, t), (17.56)
whereC is the specific heat of the material and ρ is its density. Because energy is conserved, the
rate of decrease inQwith time must equal the amount of heat flowing out of the material. After
this energy balance is struck and the divergence theorem applied, the heat equation results:
xmll∂T (x, t)∂t =
K
Cρ
∇2T (x, t). (17.57)
The heat equation (17.57) is a parabolic PDE with space and time as independent vari-
ables. The specification of this problem implies that there is no temperature variation in di-
rections perpendicular to the bar (y and z), and so we have only one spatial coordinate in the
Laplacian:
xmll
∂T (x, t)
∂t
=
K
Cρ
∂2T (x, t)
∂x2
. (17.58)
As given, the initial temperature of the bar and the boundary conditions are
T (x, t = 0) = 100 K, T (x = 0, t) = T (x = L, t) = 0 K. (17.59)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
398 CHAPTER 17
17.17.1 Solution: Analytic Expansion
Analogous to Laplace’s equation, the analytic solution starts with the assumption that the so-
lution separates into the product of functions of space and time:
T (x, t) = X(x)T (t). (17.60)
When (17.60) is substituted into the heat equation (17.58) and the resulting equation is divided
by X(x)T (t), two noncoupled ODEs result:
d2X(x)
dx2
+ k2X(x) = 0,
dT (t)
dt
+ k2
C
Cρ
T (t) = 0, (17.61)
where k is a constant still to be determined. The boundary condition that the temperature equals
zero at x = 0 requires a sine function for X:
X(x) = A sin kx. (17.62)
The boundary condition that the temperature equals zero at x = L requires the sine function to
vanish there:
sin kL = 0 ⇒ k = kn = nπ/L, n = 1, 2, . . . . (17.63)
The time function is a decaying exponential with k in the exponent:
T (t) = e−k2nt/Cρ, ⇒ T (x, t) = An sin knxe−k
2
nt/Cρ, (17.64)
where n can be any integer and An is an arbitrary constant. Since (17.58) is a linear equation,
the most general solution is a linear superposition of all values of n:
T (x, t) =
∞∑
n=1
An sin knx e−k
2
nt/Cρ. (17.65)
The coefficients An are determined by the initial condition that at time t = 0 the entire bar has
temperature T = 100 K:
T (x, t = 0) = 100 ⇒
∞∑
n=1
An sin knx = 100. (17.66)
Projecting the sine functions determines An = 4T0/nπ for n odd, and so
xmll T (x, t) =
∞∑
n=1,3,...
4T0
nπ
sin knxe−k
2
nKt/(Cρ). (17.67)
17.17.2 Solution: Time-Stepping
As we did with Laplace’s equation, the numerical solution is based on converting the differ-
ential equation to a finite-difference (”difference”) equation. We discretize space and time on
a lattice (Figure 17.13) and solve for solutions on the lattice sites. The horizontal nodes with
white centers correspond to the known values of the temperature for the initial time, while the
vertical white nodes correspond to the fixed temperature along the boundaries. If we also knew
the temperature for times along the bottom row, then we could use a relaxation algorithm as
we did for Laplace’s equation. However, with only the top row known, we shall end up with
an algorithm that steps forward in time one row at a time, as in the children’s game leapfrog.
As is often the case with PDEs, the algorithm is customized for the equation being solved
and for the constraints imposed by the particular set of initial and boundary conditions. With
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 399
Figure 17.13 The algorithm for the heat equation in which the temperature at the location x = i∆x and time
t = (j + 1)∆t is computed from the temperature values at three points of an earlier time. The nodes
with white centers correspond to known initial and boundary conditions. (The boundaries are placed
artificially close for illustrative purposes.)
x
t
i-1,j i+1,j
i,j+1
only one row of times to start with, we use a forward-difference approximation for the time
derivative of the temperature:
∂T (x, t)
∂t
' T (x, t+ ∆t)− T (x, t)
∆t
. (17.68)
Because we know the spatial variation of the temperature along the entire top row and the left
and right sides, we are less constrained with the space derivative as with the time derivative.
Consequently, as we did with the Laplace equation, we use the more accurate central-difference
approximation for the space derivative:
∂2T (x, t)
∂x2
' T (x+ ∆x, t) + T (x−∆x, t)− 2T (x, t)
(∆x)2
. (17.69)
Substitution of these approximations into (17.58) yields the heat difference equation
T (x, t+ ∆t)− T (x, t)
∆t
=
K
Cρ
T (x+ ∆x, t) + T (x−∆x, t)− 2T (x, t)
∆x2
. (17.70)
We reorder (17.70) into a form in which T can be stepped forward in t:
Ti,j+1 = Ti,j + η [Ti+1,j + Ti−1,j − 2Ti,j ] , η =
K∆t
Cρ∆x2
, (17.71)
where x = i∆x and t = j∆t. This algorithm is explicit because it provides a solution in terms
of known values of the temperature. If we tried to solve for the temperature at all lattice sites
in Figure. 17.13 simultaneously, then we would have an implicit algorithm that requires us to
solve equations involving unknown values of the temperature. We see that the temperature at
space-time point (i, j+1) is computed from the three temperature values at an earlier time j and
at adjacent space values i± 1, i. We start the solution at the top row, moving it forward in time
for as long as we want and keeping the temperature along the ends fixed at 0 K (Figure 17.14).
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
400 CHAPTER 17
Figure 17.14 A numerical calculation of the temperature versus position and versus time, with isotherm contours
projected onto the horizontal plane on the left and with a red-blue color scale used to indicate temper-
ature on the right (the color is visible on the figures on the CD).
80
80
40
010
0
0
t
T
x
17.17.3 Von Neumann Stability Assessment
When we solve a PDE by converting it to a difference equation, we hope that the solution of the
latter is a good approximation to the solution of the former. If the difference-equation solution
diverges, then we know we have a bad approximation, but if it converges, then we may feel
confident that we have a good approximation to the PDE. The von Neumann stability analysis
is based on the assumption that eigenmodes of the difference equation can be written as
Tm,j = ξ(k)j eikm∆x, (17.72)
where x = m∆x and t = j∆t, but i =
√
−1 is the imaginary number. The constant k
in (17.72) is an unknown wave vector (2π/λ), and ξ(k) is an unknown complex function.
View (17.72) as a basis function that oscillates in space (the exponential) with an amplitude
or amplification factor ξ(k)j that increases by a power of ξ for each time step. If the general
solution to the difference equation can be expanded in terms of these eigenmodes, then the
general solution will be stable if the eigenmodes are stable. Clearly, for an eigenmode to be
stable, the amplitude ξ cannot grow in time j, which means |ξ(k)| < 1 for all values of the
parameter k [Pres 94, Anc 02].
Application of a stability analysis is more straightforward than it might appear. We just
substitute the expression (17.72) into the difference equation (17.71):
ξj+1eikm∆x = ξj+eikm∆x + η
[
ξjeik(m+1)∆x + ξj+eik(m−1)∆x − 2ξj+eikm∆x
]
.
After canceling some common factors, it is easy to solve for ξ:
ξ(k) = 1 + 2η[cos(k∆x)− 1]. (17.73)
In order for |ξ(k)| < 1 for all possible k values, we must have
η =
K ∆t
Cρ∆x2
<
1
2
. (17.74)
This equation tells us that if we make the time step ∆t smaller, we will always improve the
stability, as we would expect. But if we decrease the space step ∆x without a simultaneous
quadratic increase in the time step, we will worsen the stability. The lack of space-time sym-
metry arises from our use of stepping in time but not in space.
In general, you should perform a stability analysis for every PDE you have to solve,
although it can get complicated [Pres 94]. Yet even if you do not, the lesson here is that you
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 401
may have to try different combinations of ∆x and ∆t variations until a stable, reasonable
solution is obtained. You may expect, nonetheless, that there are choices for ∆x and ∆t for
which the numerical solution fails and that simply decreasing an individual ∆x or ∆t, in the
hope that this will increase precision, may not improve the solution.
Listing 17.3 EqHeat.py solves the heat equation for a 1-D space and time by leapfrogging (time-stepping) the
initial conditions forward in time. You will need to adjust the parameters to obtain a solution like those
in the figures. 
# EqHeat . py s o l v e s h e a t e q u a t i o n v i a f i n i t e d i f f e r e n c e s , 3−D p l o t
from numpy i m p o r t ∗ ; i m p o r t p y l a b as p ; i m p o r t m a t p l o t l i b . axes3d as p3
Nx = 101 ; Nt = 9000 ; Dx = 0 . 0 1 ; Dt = 0 . 3
KAPPA = 2 1 0 . ; SPH = 9 0 0 . ; RHO = 2700 . # c o n d u c t i v i t y , s p e c i f i c hea t , d e n s i t y
T = z e r o s ( ( Nx , 2 ) , f l o a t ) ; Tpl = z e r o s ( ( Nx , 31) , f l o a t )
p r i n t "Working, wait for figure after count to 30"
f o r i x i n r a n g e ( 1 , Nx − 1) : T [ ix , 0 ] = 1 0 0 . 0 ; # i n i t i a l t e m p e r a t u r e
T [ 0 , 0 ] = 0 . 0 ; T [ 0 , 1 ] = 0 . # f i r s t and l a s t p o i n t s a t 0
T [ Nx−1 ,0] = 0 . ; T [ Nx−1 ,1] = 0 . 0
cons = KAPPA / ( SPH∗RHO)∗Dt / ( Dx∗Dx ) ; # c o n s t a n t
m = 1 # c o u n t e r f o r rows , one e v e r y 300 t ime s t e p s
f o r t i n r a n g e ( 1 , Nt ) : # t ime i t e r a t i o n
f o r i x i n r a n g e ( 1 , Nx − 1) : # F i n i t e d i f f e r e n c e s
T [ ix , 1 ] = T [ ix , 0 ] + cons ∗(T [ i x + 1 , 1 ] + T [ i x − 1 , 0 ] − 2 .0∗T [ ix , 0 ] )
i f t %300 == 0 or t == 1 : # f o r t = 1 and e v e r y 300 t ime s t e p s
f o r i x i n r a n g e ( 1 , Nx − 1 , 2 ) : Tpl [ ix , m] = T [ ix , 1 ]
p r i n t m
m = m + 1 # i n c r e a s e m e v e r y 300 t ime s t e p s
f o r i x i n r a n g e ( 1 , Nx − 1) : T [ ix , 0 ] = T [ ix , 1 ] # 100 p o s i t o n s a t t =m
x = r a n g e ( 1 , Nx − 1 , 2 ) # p l o t e v e r y o t h e r x p o i n t
y = r a n g e ( 1 , 30) # e v e r y 10 p o i n t s i n y ( t ime )
X, Y = p . meshgr id ( x , y ) # g r i d f o r p o s i t i o n and t ime
d e f f u n c t z ( Tpl ) : # F u n c t i o n r e t u r n s t e m p e r a t u r e
z = Tpl [X, Y]
r e t u r n z
Z = f u n c t z ( Tpl )
f i g = p . f i g u r e ( ) # c r e a t e f i g u r e
ax = p3 . Axes3D ( f i g ) # p l o t s a x i s
ax . p l o t w i r e f r a m e (X, Y, Z , c o l o r = ’r’ ) # r e d w i r e f r a m e
ax . s e t x l a b e l (’Position’ ) # l a b e l axes
ax . s e t y l a b e l (’time’ )
ax . s e t z l a b e l (’Temperature’ )
p . show ( ) # shows f i g u r e , c l o s e Python s h e l l
p r i n t "finished"
17.17.4 Heat Equation Implementation
Recollect that we want to solve for the temperature distribution within an aluminum bar of
length L = 1 m subject to the boundary and initial conditions
T (x = 0, t) = T (x = L, t) = 0 K, T (x, t = 0) = 100 K. (17.75)
The thermal conductivity, specific heat, and density for Al are
K = 237 W/(mK), C = 900 J/(kg K), ρ = 2700 kg/m3. (17.76)
1. Write or modify EqHeat.py in Listing 17.3 to solve the heat equation.
2. Define a 2-D array T[101][2] for the temperature as a function of space and time. The
first index is for the 100 space divisions of the bar, and the second index is for present and
past times (because you may have to make thousands of time steps, you save memory by
saving only two times).
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
402 CHAPTER 17
3. For time t = 0 ( j = 1), initialize T so that all points on the bar except the ends are at
100 K. Set the temperatures of the ends to 0 K.
4. Apply (17.68) to obtain the temperature at the next time step.
5. Assign the present-time values of the temperature to the past values:
T[i][1] = T[i][2], i = 1,... , 101.
6. Start with 50 time steps. Once you are confident the program is running properly, use
thousands of steps to see the bar cool smoothly with time. For approximately every 500
time steps, print the time and temperature along the bar.
17.18 ASSESSMENT AND VISUALIZATION
1. Check that your program gives a temperature distribution that varies smoothly along the
bar and agrees with the boundary conditions, as in Figure 17.14.
2. Check that your program gives a temperature distribution that varies smoothly with time
and attains equilibrium. You may have to vary the time and space steps to obtain well-
behaved solutions.
3. Compare the analytic and numeric solutions (and the wall times needed to compute them).
If the solutions differ, suspect the one that does not appear smooth and continuous.
4. Make surface plots of temperature versus position for several times.
5. Better yet, make a surface plot of temperature versus position versus time.
6. Plot the isotherms (contours of constant temperature).
7. Stability test: Check (17.74) that the temperature diverges in t if η > 14 .
8. Material dependence: Repeat the calculation for iron. Note that the stability condition
requires you to change the size of the time step.
9. Initial sinusoidal distribution sin(πx/L): Compare to the analytic solution,
T (x, t) = sin(πx/L)e−π
2Kt/(L2Cρ).
10. Two bars in contact: Two identical bars 0.25 m long are placed in contact along one
of their ends with their other ends kept at 0 K. One is kept in a heat bath at 100 K, and
the other at 50 K. Determine how the temperature varies with time and location (Fig-
ure 17.15).
11. Radiating bar (Newton’s cooling): Imagine now that instead of being insulated along
its length, a bar is in contact with an environment at a temperature Te. Newton’s law of
cooling (radiation) says that the rate of temperature change due to radiation is
∂T
∂t
= −h(T − Te), (17.77)
where h is a positive constant. This leads to the modified heat equation
∂T (x, t)
∂t
=
K
Cρ
∂2T
∂2x
− hT (x, t). (17.78)
Modify the algorithm to include Newton’s cooling and compare the cooling of this bar
with that of the insulated bar.
17.19 IMPROVED HEAT FLOW: CRANK–NICOLSON METHOD
The Crank–Nicolson method [C&N 47] provides a higher degree of precision for the heat equa-
tion (17.57). This method calculates the time derivative with a central-difference approxima-
tion, in contrast to the forward-difference approximation used previously. In order to avoid
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 403
Figure 17.15 Temperature versus position and time when two bars at differing temperatures are placed in contact at
t = 0. The projected contours show the isotherms.
0
40
80 0
20
40
0
50
100
x
t
T(x, t)
introducing error for the initial time step, where only a single time value is known, the method
uses a split time step,3 so that time is advanced from time t to t+ ∆t/2:
∂T
∂t
(
x, t+
∆t
2
)
' T (x, t+ ∆t)− T (x, t)
∆t
+O(∆t2). (17.79)
Yes, we know that this looks just like the forward-difference approximation for the derivative at
time t+∆t, for which it would be a bad approximation; regardless, it is a better approximation
for the derivative at time t+ ∆t/2, though it makes the computation more complicated. Like-
wise, in (17.68) we gave the central-difference approximation for the second space derivative
for time t. For t = t+ ∆t/2, that becomes
2(∆x)2
∂2T
∂x2
(
x, t+
∆t
2
)
(17.80)
' [T (x−∆x, t+ ∆t)− 2T (x, t+ ∆t) + T (x+ ∆x, t+ ∆t)]
+ [T (x−∆x, t)− 2T (x, t) + T (x+ ∆x, t)] +O(∆x2).
In terms of these expressions, the heat difference equation is
Ti,j+1 − Ti,j =
η
2
[Ti−1,j+1 − 2Ti,j+1 + Ti+1,j+1 + Ti−1,j − 2Ti,j + Ti+1,j ] ,
x = i∆x, t = j∆t, η =
K∆t
Cρ∆x2
. (17.81)
We group together terms involving the same temperature to obtain an equation with future
times on the LHS and present times on the RHS:
−Ti−1, j+1 +
(
2
η
+ 2
)
Ti, j+1 − Ti+1, j+1 = Ti−1, j +
(
2
η
− 2
)
Ti, j + Ti+1, j .
(17.82)
This equation represents an implicit scheme for the temperature Ti,j , where the word “implicit”
means that we must solve simultaneous equations to obtain the full solution for all space. In
contrast, an explicit scheme requires iteration to arrive at the solution. It is possible to solve
3In §18.6.1 we develop another split-time algorithm for solution of the Schrödinger equation, where the real and imaginary
parts of the wave function are computed at times that differ by ∆t/2.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
404 CHAPTER 17
(17.82) simultaneously for all unknown temperatures (1 ≤ i ≤ N ) at times j and j + 1. We
start with the initial temperature distribution throughout all of space, the boundary conditions
at the ends of the bar for all times, and the approximate values from the first derivative:
Ti, 0, known, T0, j , known, TN, j , known,
T0, j+1 = T0, j = 0, TN, j+1 = 0, TN, j = 0.
We rearrange (17.82) so that we can use these known values of T to step the j = 0 solution
forward in time by expressing (17.82) as a set of simultaneous linear equations (in matrix
form):
xmll

(
2
η + 2
)
−1
−1
(
2
η + 2
)
−1
−1
(
2
η + 2
)
−1
. . . . . . . . .
−1
(
2
η + 2
)
−1
−1
(
2
η + 2
)


T1,j+1
T2,j+1
T3,j+1)
...
Tn−2,j+1
Tn−1,j+1

=

T0,j+1 + T0,j +
(
2
η − 2
)
T1,j + T2,j
T1,j +
(
2
η − 2
)
T2,j + T3,j
T2,j +
(
2
η − 2
)
T3,j + T4,j
...
Tn−3,j +
(
2
η − 2
)
Tn−2,j + Tn−1,j
Tn−2,j +
(
2
η − 2
)
Tn−1,j + Tn,j + Tn,j+1

. (17.83)
Observe that the T ’s on the RHS are all at the present time j for various positions, and at future
time j + 1 for the two ends (whose T s are known for all times via the boundary conditions).
We start the algorithm with the Ti,j=0 values of the initial conditions, then solve a matrix
equation to obtain Ti,j=1. With that we know all the terms on the RHS of the equations (j = 1
throughout the bar and j = 2 at the ends) and so can repeat the solution of the matrix equations
to obtain the temperature throughout the bar for j = 2. So again we time-step forward, only
now we solve matrix equations at each step. That gives us the spatial solution directly.
Not only is the Crank–Nicolson method more precise than the low-order time-stepping
method of Unit III, but it also is stable for all values of ∆t and ∆x. To prove that, we apply
the von Neumann stability analysis discussed in §17.17.3 to the Crank–Nicolson algorithm by
substituting (17.71) into (17.82). This determines an amplification factor
ξ(k) =
1− 2η sin2(k∆x/2)
1 + 2η sin2(k∆x/2)
. (17.84)
Because sin2() is positive-definite, this proves that |ξ| ≤ 1 for all ∆t, ∆x, and k.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 405
17.19.1 Solution of Tridiagonal Matrix Equations 
The Crank–Nicolson equations (17.83) are in the standard [A]x = b form for linear equations,
and so we can use our previous methods to solve them. Nonetheless, because the coefficient
matrix [A] is tridiagonal (zero elements except for the main diagonal and two diagonals on
either side of it),

d1 c1 0 0 · · · · · · · · · 0
a2 d2 c2 0 · · · · · · · · · 0
0 a3 d3 c3 · · · · · · · · · 0
· · · · · · · · · · · · · · · · · · · · · · · ·
0 0 0 0 · · · aN−1 dN−1 cN−1
0 0 0 0 · · · 0 aN dN


x1
x2
x3
. . .
xN−1
xN

=

b1
b2
b3
. . .
bN−1
bN

,
a more robust and faster solution exists that makes this implicit method as fast as an explicit
one. Because tridiagonal systems occur frequently, we now outline the specialized technique
for solving them [Pres 94]. If we store the matrix elements ai,j using both subscripts, then
we will need N2 locations for elements and N2 operations to access them. However, for a
tridiagonal matrix, we need to store only the vectors {di}i=1,N , {ci}i=1,N , and {ai}i=1,N ,
along, above, and below the diagonals. The single subscripts on ai, di, and ci reduce the
processing from N2 to (3N − 2) elements.
We solve the matrix equation by manipulating the individual equations until the coeffi-
cient matrix is upper triangular with all the elements of the main diagonal equal to 1. We start
by dividing the first equation by d1, then subtract a2 times the first equation,
1 c1d1 0 0 · · · · · · · · · 0
0 d2 − a2c1d1 c2 0 · · · · · · · · · 0
0 a3 d3 c3 · · · · · · · · · 0
· · · · · · · · · · · · · · · · · · · · · · · ·
0 0 0 0 · · · aN−1 dN−1 cN−1
0 0 0 0 · · · 0 aN dN


x1
x2
x3
. . .
·
xN

=

b1
d1
b2 − a2b1d1
b3
. . .
·
bN

,
and then dividing the second equation by the second diagonal element,

1 c1d1 0 0 · · · · · · · · · 0
0 1 c2
d2−a2 c1a1
0 · · · · · · 0
0 a3 d3 c3 · · · · · · · · · 0
· · · · · · · · · · · · · · · · · · · · · · · ·
0 0 0 0 aN−1 dN−1 cN−1
0 0 0 0 · · · 0 aN dN


x1
x2
x3
. . .
·
xN

=

b1
d1
b2−a2 b1d1
d2−a2 c1d1
b3
. . .
·
bN

.
Assuming that we can repeat these steps without ever dividing by zero, the system of equations
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
406 CHAPTER 17
will be reduced to upper triangular form,
1 h1 0 0 · · · 0
0 1 h2 0 · · · 0
0 0 1 h3 · · · 0
0 · · · · · · . . . . . . · · ·
0 0 0 0 · · · · · ·
0 0 0 · · · 0 1


x1
x2
x3
. . .
·
xN

=

p1
p2
p3
. . .
·
pN

,
where h1 = c1/d1 and p1 = b1/d1. We then recur for the others elements:
hi =
ci
di − aihi−1
, pi =
bi − aipi−1
di − aihi−1
. (17.85)
Finally, back substitution leads to the explicit solution for the unknowns:
xi = pi − hixi−1; i = n− 1, n− 2, . . . , 1, xN = pN . (17.86)
In Listing 17.4 we give the program HeatCNTridiag.py that solves the heat equation using the
Crank–Nicolson algorithm via a triadiagonal reduction.
Listing 17.4 HeatCNTridiag.py is the complete program for solution of the heat equation in one space dimension
and time via the Crank–Nicolson method. The resulting matrix equations are solved via a technique
specialized to tridiagonal matrices. 
# HeatCNTr id iag . py : s o l u t i o n o f h e a t e q t n v i a CN method
from v i s u a l i m p o r t ∗ ; from Numeric i m p o r t ∗ ; from numpy i m p o r t ∗ ; i m p o r t p y l a b as p
i m p o r t m a t p l o t l i b . axes3d as p3
Max = 5 1 ; n = 5 0 ; m = 50
Ta = z e r o s ( ( Max) , F l o a t ) ; Tb = z e r o s ( (Max ) , F l o a t ) ; Tc = z e r o s ( ( Max) , F l o a t )
Td = z e r o s ( (Max) , F l o a t ) ; a = z e r o s ( ( Max) , F l o a t ) ; b = z e r o s ( (Max ) , F l o a t )
c = z e r o s ( (Max ) , F l o a t ) ; d = z e r o s ( (Max) , F l o a t ) ; x = z e r o s ( ( Max) , F l o a t )
t = z e r o s ( ( Max , Max ) , F l o a t )
d e f T r i d i a g ( a , d , c , b , Ta , Td , Tc , Tb , x , n ) : # D e f i ne T r i d i a g method
Max = 51
h = z e r o s ( (Max ) , F l o a t )
p = z e r o s ( (Max ) , F l o a t )
f o r i i n r a n g e ( 1 , n +1) :
a [ i ] = Ta [ i ]
b [ i ] = Tb [ i ]
c [ i ] = Tc [ i ]
d [ i ] = Td [ i ]
h [ 1 ] = c [ 1 ] / d [ 1 ]
p [ 1 ] = b [ 1 ] / d [ 1 ]
f o r i i n r a n g e ( 2 , n +1) :
h [ i ] = c [ i ] / ( d [ i ]−a [ i ]∗h [ i −1])
p [ i ] = ( b [ i ] − a [ i ]∗p [ i −1]) / ( d [ i ]−a [ i ]∗h [ i −1])
x [ n ] = p [ n ]
f o r i i n r a n g e ( n − 1 , 1,−1 ) : x [ i ] = p [ i ] − h [ i ]∗x [ i +1]
wid th = 1 . 0 ; h e i g h t = 0 . 1 ; c t = 1 . 0 # R e c t a n g l e W & H
f o r i i n r a n g e ( 0 , n ) : t [ i , 0 ] = 0 . 0 # I n i t i a l i z e
f o r i i n r a n g e ( 1 , m) : t [ 0 ] [ i ] = 0 . 0
h = wid th / ( n − 1 ) # Compute s t e p s i z e s and c o n s t a n t s
k = h e i g h t / ( m − 1 )
r = c t ∗ c t ∗ k / ( h ∗ h )
f o r j i n r a n g e ( 1 ,m+1) :
t [ 1 , j ] = 0 . 0
t [ n , j ] = 0 . 0 # BCs
f o r i i n r a n g e ( 2 , n ) : t [ i ] [ 1 ] = s i n ( p i ∗ h ∗ i )
# ICs
f o r i i n r a n g e ( 1 , n +1) : Td [ i ] = 2 . + 2 . / r
Td [ 1 ] = 1 . ; Td [ n ] = 1 .
f o r i i n r a n g e ( 1 , n ) : Ta [ i ] = −1.0; Tc [ i ] = −1.0; # Off d i a g o n a l
Ta [ n−1] = 0 . 0 ; Tc [ 1 ] = 0 . 0 ; Tb [ 1 ] = 0 . 0 ; Tb [ n ] = 0 . 0
p r i n t "I’m working hard, wait for the fig while I count to 50"
f o r j i n r a n g e ( 2 ,m+1) :
p r i n t j
f o r i i n r a n g e ( 2 , n ) : Tb [ i ] = t [ i −1][ j−1] + t [ i + 1 ] [ j−1] + ( 2 / r−2) ∗ t [ i ] [ j−1]
T r i d i a g ( a , d , c , b , Ta , Td , Tc , Tb , x , n ) # So lve sys tem
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDES FOR ELECTROSTATICS & HEAT FLOW 407
f o r i i n r a n g e ( 1 , n +1) : t [ i ] [ j ] = x [ i ]
p r i n t ("Finished" )
x = r a n g e ( 1 , m+1) # P l o t e v e r y o t h e r x p o i n t
y = r a n g e ( 1 , n +1) # e v e r y o t h e r y p o i n t
X, Y = p . meshgr id ( x , y )
d e f f u n c t z ( t ) : # F u n c t i o n r e t u r n s p o t e n t i a l
z = t [X, Y]
r e t u r n z
Z = f u n c t z ( t )
f i g = p . f i g u r e ( ) # C r e a t e f i g u r e
ax = p3 . Axes3D ( f i g ) # p l o t s axes
ax . p l o t w i r e f r a m e (X, Y, Z , c o l o r = ’r’ ) # r e d w i r e f r a m e
ax . s e t x l a b e l (’t’ ) # l a b e l axes
ax . s e t y l a b e l (’x’ )
ax . s e t z l a b e l (’T’ )
p . show ( ) # d i s p l a y f i g u r e , c l o s e s h e l l t o q u i t
17.19.2 Crank–Nicolson Implementation, Assessment
Use the Crank–Nicolson method to solve for the heat flow in the metal bar in §17.16.
1. Write a program using the Crank–Nicolson method to solve the heat equation for at least
100 time steps.
2. Solve the linear system of equations (17.83) using either Matplotlib or the special tridi-
agonal algorithm.
3. Check the stability of your solution by choosing different values for the time and space
steps.
4. Construct a contoured surface plot of temperature versus position and versus time.
5. Compare the implicit and explicit algorithms used in this chapter for relative precision
and speed. You may assume that a stable answer that uses very small time steps is
accurate.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Eighteen
PDE Waves: String, Quantum Packet, and E&M
In this chapter we explore the numerical solution of a number of PDEs known as wave equa-
tions. We have two purposes in mind. First, especially if you have skipped the discussion
of the heat equation in Chapter 17, “PDES for Electrostatics & Heat Flow,” we wish to give
another example of how initial conditions in time are treated with a time-stepping or leapfrog
algorithm. Second, we wish to demonstrate that once we have a working algorithm for solv-
ing a wave equation, we can include considerably more physics than is possible with analytic
treatments. Unit I deals with a number of aspects of waves on a string. Unit II deals with
quantum wave packets, which have their real and imaginary parts solved for at different (split)
times. Unit III extends the treatment to electromagnetic waves that have the extra complication
of being vector waves with interconnected E and H fields. Shallow-water waves, dispersion,
and shock waves are studied in Chapter 19, “Solitons and Computational Fluid Dynamics.”
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
- - - *
Applets and Animations
Name Sections Name Sections
Waves on a String 18.1–18.2 String normal mode 18.1–18.2
Two peaks on a String 18.1–18.2 Catenary Wave Movie 18.4
Wavepacket-Wavepacket Scattering 18.5-18.7 Wavepacket Slit Movie 18.5-18.7
Square Well 18.6 Harmonic Oscillator 18.7
Two Slit Interference 18.8
18.1 UNIT I. VIBRATING STRING
Problem: Recall the demonstration from elementary physics in which a string tied down at
both ends is plucked “gently” at one location and a pulse is observed to travel along the string.
Likewise, if the string has one end free and you shake it just right, a standing-wave pattern
is set up in which the nodes remain in place and the antinodes move just up and down. Your
problem is to develop an accurate model for wave propagation on a string and to see if you
can set up traveling- and standing-wave patterns.1
1Some similar but independent studies can also be found in [Raw 96].
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDE WAVES: STRING, QUANTUM PACKET, AND E&M 409
Figure 18.1 Left: A stretched string of length L tied down at both ends and under high enough tension to ignore
gravity. The vertical disturbance of the string from its equilibrium position is y(x, t). Right: A differ-
ential element of the string showing how the string’s displacement leads to the restoring force.
L
y(x,t)
x
θ
Δ
Δ
18.2 THE HYPERBOLIC WAVE EQUATION (THEORY)
Consider a string of lengthL tied down at both ends (Figure 18.1 left). The string has a constant
density ρ per unit length, a constant tension T , no frictional forces acting on it, and a tension
that is so high that we may ignore sagging due to gravity. We assume that displacement of the
string y(x, t) from its rest position is in the vertical direction only and that it is a function of
the horizontal location along the string x and the time t.
To obtain a simple linear equation of motion (nonlinear wave equations are discussed in
Chapter 19, “Solitons & Computational Fluid Dynamics”), we assume that the string’s relative
displacement y(x, t)/L and slope ∂y/∂x are small. We isolate an infinitesimal section ∆x
of the string (Figure 18.1 right) and see that the difference in the vertical components of the
tension at either end of the string produces the restoring force that accelerates this section of
the string in the vertical direction. By applying Newton’s laws to this section, we obtain the
familiar wave equation:
xmll∑Fy = ρ∆x ∂2y
∂t2
, (18.1)
∑
Fy = T sin θ(x+ ∆x)− T sin θ(x) =T
∂y
∂x
∣∣∣∣
x+∆x
− T ∂y
∂x
∣∣∣∣
x
' T ∂
2y
∂x2
,
⇒ ∂
2y(x, t)
∂x2
=
1
c2
∂2y(x, t)
∂t2
, c=
√
T
ρ
, (18.2)
where we have assumed that θ is small enough for sin θ ' tan θ = ∂y/∂x. The existence of
two independent variables x and t makes this a PDE. The constant c is the velocity with which
a disturbance travels along the wave and is seen to decrease for a heavier string and increase for
a tighter one. Note that this signal velocity c is not the same as the velocity of a string element
∂y/∂t.
The initial condition for our problem is that the string is plucked gently and released. We
assume that the “pluck” places the string in a triangular shape with the center of triangle 810 of
the way down the string and with a height of 1:
y(x, t = 0) =
{
1.25x/L, x ≤ 0.8L,
(5− 5x/L), x > 0.8L,
(initial condition 1). (18.3)
Because (18.2) is second-order in time, a second initial condition (beyond initial displacement)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
410 CHAPTER 18
is needed to determine the solution. We interpret the “gentleness” of the pluck to mean the
string is released from rest:
∂y
∂t
(x, t = 0) = 0, (initial condition 2). (18.4)
The boundary conditions have both ends of the string tied down for all times:
y(0, t) ≡ 0, y(L, t) ≡ 0, (boundary conditions). (18.5)
18.2.1 Solution via Normal-Mode Expansion
The analytic solution to (18.2) is obtained via the familiar separation-of-variables technique.
We assume that the solution is the product of a function of space and a function of time:
y(x, t) = X(x)T (t). (18.6)
We substitute (18.6) into (18.2), divide by y(x, t), and are left with an equation that has a
solution only if there are solutions to the two ODEs:
d2T (t)
dt2
+ ω2T (t) = 0,
d2X(x)
dt2
+ k2X(x) = 0, k def=
ω
c
. (18.7)
The angular frequency ω and the wave vector k are determined by demanding that the solutions
satisfy the boundary conditions. Specifically, the string being attached at both ends demands
X(x = 0, t) =X(x = l, t) = 0 (18.8)
⇒ Xn(x) =An sin knx, kn =
π(n+ 1)
L
, n = 0, 1, . . . . (18.9)
The time solution is
Tn(t) = Cn sinωnt+Dn cosωnt, ωn = nck0 = n
2πc
L
, (18.10)
where the frequency of this nth normal mode is also fixed. In fact, it is the single frequency of
oscillation that defines a normal mode. The initial condition (18.3) of zero velocity, ∂y/∂t(t =
0) = 0, requires the Cn values in (18.10) to be zero. Putting the pieces together, the normal-
mode solutions are
yn(x, t) = sin knx cosωnt, n = 0, 1, . . . . (18.11)
Since the wave equation (18.2) is linear in y, the principle of linear superposition holds
and the most general solution for waves on a string with fixed ends can be written as the sum
of normal modes:
xmll y(x, t) =
∞∑
n=0
Bn sin knx cosωnt. (18.12)
(Yet we will lose linear superposition once we include nonlinear terms in the wave equation.)
The Fourier coefficientBn is determined by the second initial condition (18.3), which describes
how the wave is plucked:
y(x, t = 0) =
∞∑
n
Bn sinnk0x. (18.13)
Multiply both sides by sinmk0x, substitute the value of y(x, 0) from (18.3), and integrate from
0 to l to obtain
Bm = 6.25
sin(0.8mπ)
m2π2
. (18.14)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDE WAVES: STRING, QUANTUM PACKET, AND E&M 411
You will be asked to compare the Fourier series (18.12) to our numerical solution. While it
is in the nature of the approximation that the precision of the numerical solution depends on
the choice of step sizes, it is also revealing to realize that the precision of the analytic solution
depends on summing an infinite number of terms, which can be done only approximately.
18.2.2 Algorithm: Time-Stepping
As with Laplace’s equation and the heat equation, we look for a solution y(x, t) only for dis-
crete values of the independent variables x and t on a grid (Figure 18.2):
x = i∆x, i= 1, . . . , Nx, t = j∆t, j = 1, . . . , Nt, (18.15)
y(x, t) = y(i∆x, i∆t) def= yi,j . (18.16)
In contrast to Laplace’s equation where the grid was in two space dimensions, the grid in
Figure 18.2 is in both space and time. That being the case, moving across a row corresponds to
increasing x values along the string for a fixed time, while moving down a column corresponds
to increasing time steps for a fixed position. Even though the grid in Figure 18.2 may be square,
we cannot use a relaxation technique for the solution because we do not know the solution on
all four sides. The boundary conditions determine the solution along the right and left sides,
while the initial time condition determines the solution along the top.
As with the Laplace equation, we use the central-difference approximation to discretize
the wave equation into a difference equation. First we express the second derivatives in terms
of finite differences:
∂2y
∂t2
' yi,j+1 + yi,j−1 − 2yi,j
(∆t)2
,
∂2y
∂x2
' yi+1,j + yi−1,j − 2yi,j
(∆x)2
. (18.17)
Substituting (18.17) in the wave equation (18.2) yields the difference equation
yi,j+1 + yi,j−1 − 2yi,j
c2(∆t)2
=
yi+1,j + yi−1,j − 2yi,j
(∆x)2
. (18.18)
Notice that this equation contains three time values: j+1 = the future, j = the present, and
j−1 = the past. Consequently, we rearrange it into a form that permits us to predict the future
solution from the present and past solutions:
yi,j+1 = 2yi,j − yi,j−1 +
c2
c′2
[yi+1,j + yi−1,j − 2yi,j ] , c′
def=
∆x
∆t
. (18.19)
Here c′ is a combination of numerical parameters with the dimension of velocity whose size
relative to c determines the stability of the algorithm. The algorithm (18.19) propagates the
wave from the two earlier times, j and j − 1, and from three nearby positions, i − 1, i, and
i+ 1, to a later time j + 1 and a single space position i (Figure 18.2).
As you have seen in our discussion of the heat equation, a leapfrog method is quite
different from a relaxation technique. We start with the solution along the topmost row and
then move down one step at a time. If we write the solution for present times to a file, then we
need to store only three time values on the computer, which saves memory. In fact, because
the time steps must be quite small to obtain high precision, you may want to store the solution
only for every fifth or tenth time.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
412 CHAPTER 18
Figure 18.2 The solutions of the wave equation for four earlier space-time points are used to obtain the solution at
the present time. The boundary and initial conditions are indicated by the white-centered dots.
X
i, j-1
i-1, j i, j i+1, j
i, j+1
t
Initializing the recurrence relation is a bit tricky because it requires displacements from
two earlier times, whereas the initial conditions are for only one time. Nonetheless, the rest
condition (18.3) when combined with the central-difference approximation lets us extrapolate
to negative time:
∂y
∂t
(x, 0) ' y(x,∆t)− y(x,−∆t)
2∆t
= 0, ⇒ yi,0 = yi,2. (18.20)
Here we take the initial time as j = 1, and so j = 0 corresponds to t = −∆t. Substituting this
relation into (18.19) yields for the initial step
yi,2 = yi,1 +
c2
2c′2
[yi+1,1 + yi−1,1 − 2yi,1] (for j = 2 only). (18.21)
Equation (18.21) uses the solution throughout all space at the initial time t = 0 to propagate
(leapfrog) it forward to a time ∆t. Subsequent time steps use (18.19) and are continued for as
long as you like.
As is also true with the heat equation, the success of the numerical method depends on
the relative sizes of the time and space steps. If we apply a von Neumann stability analysis
to this problem by substituting ym,j = ξj exp(ikm ∆x), as we did in § 17.17.3, a complicated
equation results. Nonetheless, [Pres 94] shows that the difference-equation solution will be
stable for the general class of transport equations if
c ≤ c′ = ∆x/∆t (Courant condition). (18.22)
Equation (18.22) means that the solution gets better with smaller time steps but gets worse for
smaller space steps (unless you simultaneously make the time step smaller). Having different
sensitivities to the time and space steps may appear surprising because the wave equation
(18.2) is symmetric in x and t, yet the symmetry is broken by the nonsymmetric initial and
boundary conditions.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDE WAVES: STRING, QUANTUM PACKET, AND E&M 413
Exercise: Figure out a procedure for solving for the wave equation for all times in just one
step. Estimate how much memory would be required.
Exercise: Try to figure out a procedure for solving for the wave motion with a relaxation
technique. What would you take as your initial guess, and how would you know when the
procedure has converged?
18.2.3 Wave Equation Implementation
The program EqString.py in Listing 18.1 solves the wave equation for a string of lengthL = 1
m with its ends fixed and with the gently plucked initial conditions. Note that our use of L = 1
violates our assumption that y/L 1 but makes it easy
Listing 18.1 EqString.py solves the wave equation via time stepping for a string of length L = 1 m with its ends
fixed and with the gently plucked initial conditions. You will need to modify this code to include new
physics. 
# E q S t r i n g . py : Animated l e a p f r o g s o l u t i o n o f wave e q u a t i o n ( Sec 1 8 . 2 . 2 )
from v i s u a l i m p o r t ∗
# S e t up c u r v e
g = d i s p l a y ( wid th = 600 , h e i g h t = 300 , t i t l e = ’Vibrating string’ )
v i b s t = c u r v e ( x = r a n g e ( 0 , 100) , c o l o r = c o l o r . y e l l ow )
b a l l 1 = s p h e r e ( pos = ( 1 0 0 , 0 ) , c o l o r = c o l o r . red , r a d i u s = 2)
b a l l 2 = s p h e r e ( pos = ( − 100 , 0 ) , c o l o r = c o l o r . red , r a d i u s = 2)
b a l l 1 . pos
b a l l 2 . pos
v i b s t . r a d i u s = 1 . 0
# P a r a m e t e r s
rho = 0 . 0 1 # s t r i n g d e n s i t y
t e n = 4 0 . # s t r i n g t e n s i o n
c = math . s q r t ( t e n / rho ) # P r o p a g a t i o n speed
c1 = c # CFL c r i t e r i u m
r a t i o = c∗c / ( c1∗c1 )
# I n i t i a l i z a t i o n
x i = z e r o s ( ( 1 0 1 , 3 ) , F l o a t ) # 101 x’s & 3 t’s ( maybe f l o a t )
f o r i i n r a n g e ( 0 , 81) :
x i [ i , 0 ] = 0.00125∗ i ; # I n i t i a l c o n d i t i o n s
f o r i i n r a n g e ( 8 1 , 101) : # f i r s t p a r t o f s t r i n g
x i [ i , 0 ] = 0 . 1 − 0 . 0 0 5∗ ( i − 80) # second p a r t o f s t r i n g
f o r i i n r a n g e ( 0 , 100) : # F i r s t t s t e p
v i b s t . x [ i ] = 2 .0∗ i − 100 .0 # a s s i g n & s c a l e x : 0<i <100 − > − 100< i <100
v i b s t . y [ i ] = 300.∗ x i [ i , 0 ] # a s s i n g & s c a l e y : x i t o 300∗ x i
v i b s t . pos # draw s t r i n g
# L a t e r t ime s t e p s
f o r i i n r a n g e ( 1 , 100) : # use a l g o r i t h m
x i [ i , 1 ] = x i [ i , 0 ] + 0 .5∗ r a t i o ∗( x i [ i + 1 , 0 ] + x i [ i − 1 , 0 ] − 2∗ x i [ i , 0 ] )
w h i l e 1 : # c o n t i n u e p l o t t i n g u n t i l u s e r q u i t s
r a t e ( 5 0 ) # d e l a y s p l o t t i n g , ( b i g g e r = s l o w e r )
f o r i i n r a n g e ( 1 , 100) :
x i [ i , 2 ] = 2 .∗ x i [ i , 1 ] − x i [ i , 0 ] + r a t i o ∗( x i [ i + 1 , 1 ] + x i [ i − 1 , 1 ] − 2∗ x i [ i , 1 ] )
f o r i i n r a n g e ( 1 , 100) :
v i b s t . x [ i ] = 2 .∗ i − 100 .0 # s c a l e d x − p o s i t i o n s
v i b s t . y [ i ] = 300.∗ x i [ i , 2 ] # s c a l e d y − p o s i t i o n s
v i b s t . pos # p l o t s t r i n g
f o r i i n r a n g e ( 0 , 101) :
x i [ i , 0 ] = x i [ i , 1 ] # r e c y c l e a r r a y
x i [ i , 1 ] = x i [ i , 2 ]
p r i n t "finished"
to display the results; you should try L = 1000 to be realistic. The values of density and
tension are entered as constants, ρ = 0.01 kg/m and T = 40 N, with the space grid set at 101
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
414 CHAPTER 18
Figure 18.3 The vertical displacement as a function of position x and time t. Left: A string initially plucked near its
right end forms a pulse that divides into waves traveling to the right and to the left. In both cases y/L
should be small, but that would be harder to show in a small space. Right: A string initially placed in a
standing wave on a string with friction. Notice how the standing wave moves up and down with time.
(Courtesy of J. Wiren.)
points, corresponding to ∆ = 0.01 cm.
18.2.4 Assessment, Exploration
1. Solve the wave equation and make a surface plot of displacement versus time and posi-
tion.
2. Explore a number of space and time step combinations. In particular, try steps that
satisfy and that do not satisfy the Courant condition (18.22). Does your exploration
conform with the stability condition?
3. Compare the analytic and numeric solutions, summing at least 200 terms in the analytic
solution.
4. Use the plotted time dependence to estimate the peak’s propagation velocity c. Compare
the deduced c to (18.2).
5. Our solution of the wave equation for a plucked string leads to the formation of a wave
packet that corresponds to the sum of multiple normal modes of the string. On the right
in Figure 18.3 we show the motion resulting from the string initially placed in a single
normal mode (standing wave),
y(x, t = 0) = 0.001 sin 2πx,
∂y
∂t
(x, t = 0) = 0.
Modify the program to incorporate this initial condition and see if a normal mode results.
6. Observe the motion of the wave for initial conditions corresponding to the sum of two
adjacent normal modes. Does beating occur?
7. When a string is plucked near its end, a pulse reflects off the ends and bounces back
and forth. Change the initial conditions of the model program to one corresponding to a
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDE WAVES: STRING, QUANTUM PACKET, AND E&M 415
Figure 18.4 The vertical displacement as a function of position and time of a string initially plucked simultaneously
at two points, as shown by arrows. Note that each initial peak breaks up into waves traveling to the
right and to the left. The traveling waves invert on reflection from the fixed ends. As a consequence of
these inversions, the t ' 12 wave is an inverted t = 0 wave.
Two Initial Pulses
y(x, t)
t
x
0
string plucked exactly in its middle and see if a traveling or a standing wave results.
8.  Figure 18.4 shows the wave packets that result as a function of time for initial con-
ditions corresponding to the double pluck indicated on the left in the figure. Verify that
initial conditions of the form
y(x, t = 0)
0.005
=

0, 0.0 ≤ x ≤ 0.1,
10x− 1, 0.1 ≤ x ≤ 0.2,
−10x+ 3, 0.2 ≤ x ≤ 0.3,
0, 0.3 ≤ x ≤ 0.7,
10x− 7, 0.7 ≤ x ≤ 0.8,
−10x+ 9, 0.8 ≤ x ≤ 0.9,
0, 0.9 ≤ x ≤ 1.0
lead to this type of a repeating pattern. In particular, observe whether the pulses move or
just oscillate up and down.
18.3 WAVES WITH FRICTION (EXTENSION)
The string problem we have investigated so far can be handled by either a numerical or an
analytic technique. We now wish to extend the theory to include some more realistic physics.
These extensions have only numerical solutions.
Real plucked strings do not vibrate forever because the real world contains friction. Con-
sider again the element of a string between x and x + dx (Figure 18.1 right) but now imagine
that this element is moving in a viscous fluid such as air. An approximate model has the fric-
tional force pointing in a direction opposite the (vertical) velocity of the string and proportional
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
416 CHAPTER 18
to that velocity, as well as proportional to the length of the string element:
Ff ' −2κ ∆x
∂y
∂t
, (18.23)
where κ is a constant that is proportional to the viscosity of the medium in which the string is
vibrating. Including this force in the equation of motion changes the wave equation to
∂2y
∂t2
= c2
∂2y
∂x2
− 2κ
ρ
∂y
∂t
. (18.24)
In Figure 18.3 we show the resulting motion of a string plucked in the middle when friction is
included. Observe how the initial pluck breaks up into waves traveling to the right and to the
left that are reflected and inverted by the fixed ends. Because those parts of the wave with the
higher velocity experience greater friction, the peak tends to be smoothed out the most as time
progresses.
Exercise: Generalize the algorithm used to solve the wave equation to now include friction
and check if the wave’s behavior seems physical (damps in time). Start with T = 40 N and
ρ = 10 kg/m and pick a value of κ large enough to cause a noticeable effect but not so large
as to stop the oscillations. As a check, reverse the sign of κ and see if the wave grows in time
(which would eventually violate our assumption of small oscillations).
18.4 WAVES FOR VARIABLE TENSION AND DENSITY (EXTENSION)
We have derived the propagation velocity for waves on a string as c =
√
T/ρ. This says that
waves move slower in regions of high density and faster in regions of high tension. If the
density of the string varies, for instance, by having the ends thicker in order to support the
weight of the middle, then c will no longer be a constant and our wave equation will need to be
extended. In addition, if the density increases, then so will the tension because it takes greater
tension to accelerate a greater mass. If gravity acts, then we will also expect the tension at
the ends of the string to be higher than in the middle because the ends must support the entire
weight of the string.
To derive the equation for wave motion with variable density and tension, consider again
the element of a string (Figure 18.1 right) used in our derivation of the wave equation. If we
do not assume the tension T is constant, then Newton’s second law gives
xmll F =ma (18.25)
⇒ ∂
∂x
[
T (x)
∂y(x, t)
∂x
]
∆x= ρ(x)∆x
∂2u(x, t)
∂t2
(18.26)
⇒ ∂T (x)
∂x
∂y(x, t)
∂x
+ T (x)
∂2y(x, t)
∂x2
= ρ(x)
∂2y(x, t)
∂t2
. (18.27)
If ρ(x) and T (x) are known functions, then these equations can be solved with just a small
modification of our algorithm.
In §18.4.1 we will solve for the tension in a string due to gravity. Readers interested in
an alternate easier problem that still shows the new physics may assume that the density and
tension are proportional:
ρ(x) = ρ0eαx, T (x) = T0eαx. (18.28)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDE WAVES: STRING, QUANTUM PACKET, AND E&M 417
Figure 18.5 Left: A uniform string suspended from its ends in a gravitational field assumes a catenary shape. Right:
A force diagram of a section of the catenary at its lowest point. The tension now varies along the string.
x
u
D
T
T0
W
dx
ds
ds
While we would expect the tension to be greater in regions of higher density (more mass to
move and support), being proportional is clearly just an approximation. Substitution of these
relations into (18.27) yields the new wave equation:
∂2y(x, t)
∂x2
+ α
∂y(x, t)
∂x
=
1
c2
∂2y(x, t)
∂t2
, c2 =
T0
ρ0
. (18.29)
Here c is a constant that would be the wave velocity if α = 0. This equation is similar to
the wave equation with friction, only now the first derivative is with respect to x and not t.
The corresponding difference equation follows from using central-difference approximations
for the derivatives:
yi,j+1 = 2yi,j − yi,j−1 +
αc2(∆t)2
2∆x
[yi+1,j − yi,j ] +
c2
c′2
[yi+1,j + yi−1,j − 2yi,j ],
yi,2 = yi,1 +
c2
c′2
[yi+1,1 + yi−1,1 − 2yi,1] +
αc2(∆t)2
2∆x
[yi+1,1 − yi,1]. (18.30)
18.4.1 Waves on Catenary
Up until this point we have been ignoring the effect of gravity upon our string’s shape and
tension. This is a good approximation if there is very little sag in the string, as might happen if
the tension is very high and the string is light. Even if there is some sag, our solution for y(x, t)
could be used as the disturbance about the equilibrium shape. However, if the string is massive,
say, like a chain or heavy cable, then the sag in the middle caused by gravity could be quite
large (Figure 18.5), and the resulting variation in shape and tension needs to be incorporated
into the wave equation. Because the tension is no longer uniform, waves travel faster near the
ends of the string, which are under greater tension since they must support the entire weight of
the string.
18.4.2 Derivation of Catenary Shape
Consider a string of uniform density ρ acted upon by gravity. To avoid confusion with our use
of y(x) to describe a disturbance on a string, we call u(x) the equilibrium shape of the string
(Figure 18.5). The statics problem we need to solve is to determine the shape u(x) and the
tension T (x). The inset in Figure 18.5 is a free-body diagram of the midpoint of the string and
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
418 CHAPTER 18
shows that the weight W of this section of arc length s is balanced by the vertical component
of the tension T . The horizonal tension T0 is balanced by the horizontal component of T :
T (x) sin θ=W = ρgs, T (x) cos θ = T0, (18.31)
⇒ tan θ= ρgs/T0. (18.32)
The trick is to convert (18.32) to a differential equation that we can solve. We do that by
replacing the slope tan θ by the derivative du/dx and taking the derivative with respect to x:
du
dx
=
ρg
T0
s, ⇒ d
2u
dx2
=
ρg
T0
ds
dx
. (18.33)
Yet since ds =
√
dx2 + du2, we have our differential equation
d2u
dx2
=
1
D
√
dx2 + du2
dx
=
1
D
√
1 +
(
du
dx
)2
, (18.34)
D=T0/ρg, (18.35)
where D is a combination of constants with the dimension of length. Equation (18.34) is the
equation for the catenary and has the solution [Becker 54]
u(x) = D cosh
x
D
. (18.36)
Here we have chosen the x axis to lie a distance D below the bottom of the catenary (Fig-
ure 18.5) so that x = 0 is at the center of the string where y = D and T = T0. Equation
(18.33) tells us the arc length s = Ddu/dx, so we can solve for s(x) and, via (18.31), for the
tension T (x):
s(x) = D sinh
x
D
, ⇒ T (x) = T0
ds
dx
= ρgu(x) = T0 cosh
x
D
. (18.37)
It is this variation in tension that causes the wave velocity to change for different positions on
the string.
18.4.3 Catenary and Frictional Wave Exercises
We have given you the program EqString.py (Listing 18.1) that solves the wave equation.
Modify it to produce waves on a catenary including friction or for the assumed density and
tension given by (18.28) with α = 0.5, T0 = 40 N, and ρ0 = 0.01 kg/m. (The instructor’s CD
contains the programs CatFriction.py and CatString.py that do this.)
1. Look for some interesting cases and create surface plots of the results.
2. Explain in words how the waves dampen and how a wave’s velocity appears to change.
The behavior you obtain may look something like that shown in Figure 18.6.
3. Normal modes: Search for normal-mode solutions of the variable-tension wave equa-
tion, that is, solutions that vary as
u(x, t) = A cos(ωt) sin(γx).
Try using this form to start your program and see if you can find standing waves. Use
large values for ω.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDE WAVES: STRING, QUANTUM PACKET, AND E&M 419
Figure 18.6 The wave motion of a plucked catenary with friction. (Courtesy of Juan Vanegas.)
u(x,t)
t = 1
2
3
4
5
6
x
loading CatFrictionAnimate
4. When conducting physics demonstrations, we set up standing-wave patterns by driving
one end of the string periodically. Try doing the same with your program; that is, build
into your code the condition that for all times
y(x = 0, t) = A sinωt.
Try to vary A and ω until a normal mode (standing wave) is obtained.
5. (For the exponential density case.) If you were able to find standing waves, then verify
that this string acts like a high-frequency filter, that is, that there is a frequency below
which no waves occur.
6. For the catenary problem, plot your results showing both the disturbance u(x, t) about
the catenary and the actual height y(x, t) above the horizontal for a plucked string initial
condition.
7. Try the first two normal modes for a uniform string as the initial conditions for the cate-
nary. These should be close to, but not exactly, normal modes.
8. We derived the normal modes for a uniform string after assuming that k(x) = ω/c(x) is
a constant. For a catenary without too much x variation in the tension, we should be able
to make the approximation
c(x)2 ' T (x)
ρ
=
T0 cosh(x/d)
ρ
.
See if you get a better representation of the first two normal modes if you include some
x dependence in k.
18.5 UNIT II. QUANTUM WAVE PACKETS
Problem: An experiment places an electron with a definite momentum and position in a 1-D
region of space the size of an atom. It is confined to that region by some kind of attractive
potential. Your problem is to determine the resultant electron behavior in time and space.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
420 CHAPTER 18
Figure 18.7 The position as a function of time of a localized electron confined to a square well (computed with the
code SqWell.py available on the instructor’s CD). The electron is initially on the right with a Gaussian
wave packet. In time, the wave packet spreads out and collides with the walls.
0
20
t
x
18.6 TIME-DEPENDENT SCHRÖDINGER EQUATION (THEORY)
Because the region of confinement is the size of an atom, we must solve this problem quantum
mechanically. Nevertheless, it is different from the problem of a particle confined to a box
considered in Chapter 9, “Differential Equation Applications,” because now we are starting
with a particle of definite momentum and position. In Chapter 9 we had a time-independent
situation in which we had to solve the eigenvalue problem. Now the definite momentum and
position of the electron imply that the solution is a wave packet, which is not an eigenstate
with a uniform time dependence of exp(−iωt). Consequently, we must now solve the time-
dependent Schrödinger equation.
We model an electron initially localized in space at x = 5 with momentum k0 (h̄ = 1
in our units) by a wave function that is a wave packet consisting of a Gaussian multiplying a
plane wave:
xmll
ψ(x, t = 0) = exp
[
−1
2
(
x− 5
σ0
)2]
eik0x. (18.38)
To solve the problem we must determine the wave function for all later times. If (18.38) were
an eigenstate of the Hamiltonian, its exp(−iωt) time dependence can be factored out of the
Schrödinger equation (as is usually done in textbooks). However, H̃ψ 6= Eψ for this ψ, and
so we must solve the full time-dependent Schrödinger equation. To show you where we are
going, the resulting wave packet behavior is shown in Figures 18.7 and 18.8.
The time and space evolution of a quantum particle is described by the 1-D time-
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDE WAVES: STRING, QUANTUM PACKET, AND E&M 421
Figure 18.8 The probability density as a function of time for an electron confined to a 1-D harmonic oscillator
potential well. On the left is a conventional surface plot from Gnuplot, while on the right is a color
visualization from OpenDX.
0
10
x
t
x
t
8
10
0
1
0
2
0
6
0
7
0
10
10
6
4
2
0
T
im
e
8
Posit
ion0
1
0
2
0
3
0
4
0
5
0
6
0
7
0
10
5
080
00
5
dependent Schrödinger equation,
i
∂ψ(x, t)
∂t
= H̃ψ(x, t) (18.39)
i
∂ψ(x, t)
∂t
=− 1
2m
∂2ψ(x, t)
∂x2
+ V (x)ψ(x, t), (18.40)
where we have set 2m = 1 to keep the equations simple. Because the initial wave function
is complex (in order to have a definite momentum associated with it), the wave function will
be complex for all times. Accordingly, we decompose the wave function into its real and
imaginary parts:
xmllψ(x, t) = R(x, t) + i I(x, t), (18.41)
⇒ ∂R(x, t)
∂t
=− 1
2m
∂2I(x, t)
∂x2
+ V (x)I(x, t), (18.42)
∂I(x, t)
∂t
= +
1
2m
∂2R(x, t)
∂x2
− V (x)R(x, t), (18.43)
where V (x) is the potential acting on the particle.
18.6.1 Finite-Difference Algorithm
The time-dependent Schrödinger equation can be solved with both implicit (large-matrix) and
explicit (leapfrog) methods. The extra challenge with the Schrödinger equation is to ensure
that the integral of the probability density
∫ +∞
−∞ dx ρ(x, t) remains constant (conserved) to a
high level of precision for all time. For our project we use an explicit method that improves the
numerical conservation of probability by solving for the real and imaginary parts of the wave
function at slightly different or “staggered” times [Ask 77, Viss 91, MLP 00]. Explicitly, the
real part R is determined at times 0, ∆t, . . ., and the imaginary part I at 12∆t,
3
2∆t, . . . . The
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
422 CHAPTER 18
algorithm is based on (what else?) the Taylor expansions of R and I:
R
(
x, t+
1
2
∆t
)
=R
(
x, t− 1
2
∆t
)
+ [4α+ V (x) ∆t]I(x, t)
−2α[I(x+ ∆x, t) + I(x−∆x, t)], (18.44)
where α = ∆t/2(∆x)2. In discrete form with Rt=n∆tx=i∆x, we have
Rn+1i =R
n
i − 2
{
α
[
Ini+1 + I
n
i−1
]
− 2 [α+ Vi ∆t] Ini
}
, (18.45)
In+1i = I
n
i + 2
{
α
[
Rni+1 +R
n
i−1
]
− 2 [α+ Vi ∆t]Rni
}
, (18.46)
where the superscript n indicates the time and the subscript i the position.
The probability density ρ is defined in terms of the wave function evaluated at three
different times:
ρ(t) =
R
2(t) + I
(
t+ ∆t2
)
I
(
t− ∆t2
)
, for integer t,
I2(t) +R
(
t+ ∆t2
)
R
(
t− ∆t2
)
, for half-integer t.
(18.47)
Although probability is not conserved exactly with this algorithm, the error is two orders higher
than that in the wave function, and this is usually quite satisfactory. If it is not, then we need to
use smaller steps. While this definition of ρ may seem strange, it reduces to the usual one for
∆t→ 0 and so can be viewed as part of the art of numerical analysis. You will investigate just
how well probability is conserved. We refer the reader to [Koon 86, Viss 91] for details on the
stability of the algorithm.
18.6.2 Wave Packet Implementation, Animation
In Listing 18.2 you will find the program Harmos.py that solves for the motion of the wave
packet (18.38) inside a harmonic oscillator potential. The program Slit.py on the instructor’s
CD solves for the motion of a Gaussian wave packet as it passes through a slit (Figure 18.10).
You should solve for a wave packet confined to the square well:
V (x) =
∞, x < 0, or x > 15,0, 0 ≤ x ≤ 15.
1. Define arrays psr[751][2] and psi[751][2] for the real and imaginary parts of ψ, and
Rho[751] for the probability. The first subscript refers to the x position on the grid, and
the second to the present and future times.
2. Use the values σ0 = 0.5, ∆x = 0.02, k0 = 17π, and ∆t = 12∆x
2.
3. Use equation (18.38) for the initial wave packet to define psr[j][1] for all j at t = 0
and to define psi[j][1] at t = 12∆t.
4. Set Rho[1] = Rho[751] = 0.0 because the wave function must vanish at the infinitely
high well walls.
5. Increment time by 12∆t. Use (18.45) to compute psr[j][2] in terms of psr[j][1], and
(18.46) to compute psi[j][2] in terms of psi[j][1].
6. Repeat the steps through all of space, that is, for i = 2–750.
7. Throughout all of space, replace the present wave packet (second index equal to 1) by the
future wave packet (second index 2).
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDE WAVES: STRING, QUANTUM PACKET, AND E&M 423
8. After you are sure that the program is running properly, repeat the time-stepping for
∼5000 steps.
Listing 18.2 Harmos.py solves the time-dependent Schrödinger equation for a particle described by a Gaussian
wave packet moving within a harmonic oscillator potential. 
# VHarmos Animate . py : S o l v e s & a n i m a t e Schroed Eqtn f o r wavepacke t i n HO
from v i s u a l i m p o r t ∗
# i n i t i a l i z e wave f u n c t i o n , p r o b a b i l i t y , p o t e n t i a l
xmax = 300 ; dx = 0 . 0 4 ; dx2 = dx∗dx ; k0 = 5.5∗math . p i ; d t = dx2 / 4 .
xp = −6; prob = z e r o s ( ( xmax +1) , F l o a t ) ; v = z e r o s ( ( xmax +1) , F l o a t )
p s r = z e r o s ( ( xmax +1 ,2 ) , F l o a t ) ; p s i = z e r o s ( ( xmax +1 ,2 ) , F l o a t )
g = d i s p l a y ( wid th = 500 , h e i g h t = 250 , t i t l e = ’Wave Packet in Harmonic Well’ )
P l o t O b j = c u r v e ( x = r a n g e ( 0 , 300+1) , c o l o r = c o l o r . ye l l ow ) # s e t c u r v e
f o r i i n r a n g e ( 0 , xmax ) :
xp2 = xp ∗ xp
p s r [ i , 0 ] = math . exp (−0.5∗xp2 / 0 . 2 5 ) ∗ math . cos ( k0∗xp ) ; # Re wave P s i
p s i [ i , 0 ] = math . exp (−0.5∗xp2 / 0 . 2 5 ) ∗ math . s i n ( k0∗xp ) ; # Im P s i
v [ i ] = 15.0∗ xp2 # P o t e n t i a l
xp += dx
c o u n t = 0
w h i l e 1 : # Time p r o p a g a t i o n
f o r i i n r a n g e ( 1 , xmax − 1) : # Re P s i
p s r [ i , 1 ] = p s r [ i , 0 ] − d t ∗( p s i [ i +1 ,0 ] + p s i [ i −1 ,0]
− 2 .∗ p s i [ i , 0 ] ) / ( dx2 ) + d t∗v [ i ]∗ p s i [ i , 0 ]
prob [ i ] = p s r [ i , 0 ]∗ p s r [ i , 1 ] + p s i [ i , 0 ]∗ p s i [ i , 0 ]
i f c o u n t %10 == 0 : # Add p o i n t s t o p l o t
j = 0
f o r i i n r a n g e ( 1 , xmax−1, 3 ) :
P l o t O b j . x [ j ] = 2∗ i − xmax
P l o t O b j . y [ j ] = 130∗ prob [ i ]
j = j + 1
P l o t O b j . r a d i u s = 4 . 0
P l o t O b j . pos # Add p o i n t s
f o r i i n r a n g e ( 1 , xmax − 1) :
p s i [ i , 1 ] = p s i [ i , 0 ] + d t ∗( p s r [ i +1 ,1 ] + p s r [ i −1 ,1] −2.∗ p s r [ i , 1 ] ) / dx2 − d t∗v [ i ]∗ p s r [ i , 1 ]
f o r i i n r a n g e ( 0 , xmax ) :
p s i [ i , 0 ] = p s i [ i , 1 ]
p s r [ i , 0 ] = p s r [ i ] [ 1 ]
c o u n t = c o u n t + 1
1. Animation: Output the probability density after every 200 steps for use in animation. CD
2. Make a surface plot of probability versus position versus time. This should look like
Figure 18.7 or 18.8.
3. Make an animation showing the wave function as a function of time.
4. Check how well the probability is conserved for early and late times by determining the
integral of the probability over all of space,
∫ +∞
−∞ dx ρ(x), and seeing by how much it
changes in time (its specific value doesn’t matter because that’s just normalization).
5. What might be a good explanation of why collisions with the walls cause the wave packet
to broaden and break up? (Hint: The collisions do not appear so disruptive when a
Gaussian wave packet is confined within a harmonic oscillator potential well.)
18.7 WAVE PACKETS IN OTHER WELLS (EXPLORATION)
1-D Well: Now confine the electron to lie within the harmonic oscillator potential:
V (x) =
1
2
x2 (−∞ ≤ x ≤ ∞).
Take the momentum k0 = 3π, the space step ∆x = 0.02, and the time step ∆t = 14∆x
2.
Note that the wave packet broadens yet returns to its initial shape!
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
424 CHAPTER 18
Figure 18.9 The probability density as a function of x and y of an electron confined to a 2-D parabolic tube. The
electron’s initial localization is described by a Gaussian wave packet in both the x and y directions. The
times are 100, 300, and 500 steps.
x
y
100 300 500
2-D Well : Now confine the electron to lie within a 2-D parabolic tube (Figure 18.9):
V (x, y) = 0.9x2, −9.0 ≤ x ≤ 9.0, 0 ≤ y ≤ 18.0.
The extra degree of freedom means that we must solve the 2-D PDE:
i
∂ψ(x, y, t)
∂t
= −
(
∂2ψ
∂x2
+
∂2ψ
∂y2
)
+ V (x, y)ψ. (18.48)
Assume that the electron’s initial localization is described by the 2-D Gaussian wave
packet:
ψ(x, y, t = 0) = eik0xx eik0yy exp
[
−(x− x0)
2
2σ20
]
exp
[
−(y − y0)
2
2σ20
]
. (18.49)
Note that you can solve the 2-D equation by extending the method we just used in 1-D or
you can look at the next section where we develop a special algorithm.
18.8 ALGORITHM FOR THE 2-D SCHRÖDINGER EQUATION
One way to develop an algorithm for solving the time-dependent Schrödinger equation in 2-D
is to extend the 1-D algorithm to another dimension. Rather than do that, we apply quantum
theory directly to obtain a more powerful algorithm [MLP 00]. First we note that equation
(18.48) can be integrated in a formal sense [L&L,M 76] to obtain the operator solution:
ψ(x, y, t) =U(t)ψ(x, y, t = 0) (18.50)
U(t) = e−iH̃t, H̃ = −
(
∂2
∂x2
+
∂2
∂y2
)
+ V (x, y),
where U(t) is an operator that translates a wave function by an amount of time t and H̃ is
the Hamiltonian operator. From this formal solution we deduce that a wave packet can be
translated ahead by time ∆t via
ψn+1i,j = U(∆t)ψ
n
i,j , (18.51)
where the superscripts denote time t = n∆t and the subscripts denote the two spatial variables
x = i∆x and y = j∆y. Likewise, the inverse of the time evolution operator moves the solution
back one time step:
ψn−1 = U−1(∆t)ψn = e+iH̃∆tψn. (18.52)
While it would be nice to have an algorithm based on a direct application of (18.52), the
references show that the resulting algorithm is not stable. That being so, we base our algorithm
on an indirect application [Ask 77], namely, the relation between the difference in ψn+1 and
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDE WAVES: STRING, QUANTUM PACKET, AND E&M 425
Figure 18.10 The probability density as a function of position and time for an electron incident upon and passing
through a slit.
ψn−1:
ψn+1 = ψn−1 + [e−iH̃∆t − e+iH̃∆t]ψn, (18.53)
where the difference in sign of the exponents is to be noted. The algorithm derives from com-
bining the O(∆x2) expression for the second derivative obtained from the Taylor expansion,
∂2ψ
∂x2
' −1
2
[
ψni+1,j + ψ
n
i−1,j − 2ψni,j
]
, (18.54)
with the corresponding-order expansion of the evolution equation (18.53). Substituting the re-
sulting expression for the second derivative into the 2-D time-dependent Schrödinger equation
results in2
ψn+1i,j = ψ
n−1
i,j − 2i
[(
4α+ 12∆tVi,j
)
ψni,j − α
(
ψni+1,j + ψ
n
i−1,j + ψ
n
i,j+1 + ψ
n
i,j−1
)]
,
where α = ∆t/2(∆x)2. We convert this complex equations to coupled real equations by
substituting in the wave function ψ = R+ iI ,
Rn+1i,j =R
n−1
i,j + 2
[(
4α+ 12∆tVi,j
)
Ini,j − α
(
Ini+1,j + I
n
i−1,j + I
n
i,j+1 + I
n
i,j−1
)]
,
In+1i,j = I
n−1
i,j − 2
[(
4α+ 12∆tVi,j
)
Rni,j + α
(
Rni+1,j +R
n
i−1,j +R
n
i,j+1 +R
n
i,j−1
)]
.
This is the algorithm we use to integrate the 2-D Schrödinger equation. To determine the
probability, we use the same expression (18.47) used in 1-D.
18.8.0.1 Exploration: A Bound and Diffracted 2-D Packet
1. Determine the motion of a 2-D Gaussian wave packet within a 2-D harmonic oscillator
potential:
V (x, y) = 0.3(x2 + y2), −9.0 ≤ x ≤ 9.0, −9.0 ≤ y ≤ 9.0. (18.55)
2. Center the initial wave packet at (x, y) = (3.0,−3) and give it momentum (k0x, k0y) =
(3.0, 1.5).
3. Young’s single-slit experiment has a wave passing through a small slit with the trans-
mitted wave showing interference effects. In quantum mechanics, where we represent a
particle by a wave packet, this means that an interference pattern should be formed when
a particle passes through a small slit. Pass a Gaussian wave packet of width 3 through a
slit of width 5 (Figure 18.10) and look for the resultant quantum interference.
2For reference sake, note that the constants in the equation change as the dimension of the equation changes; that is, there will
be different constants for the 3-D equation, and therefore our constants are different from the references!
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
426 CHAPTER 18
loading 2slits.mpg
18.9 UNIT III. E&M WAVES VIA FINITE-DIFFERENCE TIME DOMAIN 
Problem: You are given a region in space in which the E and H fields are known to have a
sinusoidal spatial variation
Ex(z, t = 0) = 0.1 sin
2πz
100
, (18.56)
Hy(z, t = 0) = 0.1 sin
2πz
100
, 0 ≤ z ≤ 200. (18.57)
Determine the fields for all z values for all subsequent times.
Simulations of electromagnetic waves are of tremendous practical importance. Indeed,
the fields of nanotechnology and spintronics rely heavily upon such simulations. The basic
techniques used to solve for electromagnetic waves are essentially the same as those we used
in Units I and II for string and quantum waves: Set up a grid in space and time and then step
the initial solution forward in time one step at a time. For E&M simulations, this technique is
known as the finite difference time domain (FDTD) method. What is new for E&M waves is that
they are vector fields, with the variations of one generating the other, so that the components of
E and B are coupled to each other. Our treatment of FDTD does not do justice to the wealth
of physics that can occur, and we recommend [Sull 00] for a more complete treatment and
[Ward 04] (and their Web site) for modern applications.
18.10 MAXWELL’S EQUATIONS
The description of electromagnetic (EM) waves via Maxwell’s equations is given in many
textbooks. For propagation in just one dimension (z) and for free space with no sinks or
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDE WAVES: STRING, QUANTUM PACKET, AND E&M 427
Figure 18.11 A single electromagnetic pulse traveling along the z axis. The coupled E and H pulses are indicated
by solid and dashed curves, respectively, and the pulses at different z values correspond to different
times.
x
y
z
Ex Hy
Ex
Hy
t
sources, four coupled PDEs result:
~∇ ·E = 0 ⇒ ∂Ex(z, t)
∂x
= 0 (18.58)
~∇ ·H = 0 ⇒ ∂Hy(z, t)
∂y
= 0, (18.59)
∂E
∂t
= +
1
0
~∇×H ⇒ ∂Ex
∂t
= − 1
0
∂Hy(z, t)
∂z
, (18.60)
∂H
∂t
= − 1
µ0
~∇×E ⇒ ∂Hy
∂t
= − 1
µ0
∂Ex(z, t)
∂z
. (18.61)
As indicated in Figure 18.11, we have chosen the electric field E(z, t) to oscillate (be polarized)
in the x direction and the magnetic field H(z, t) to be polarized in the y direction. As indicated
by the bold arrow in Figure 18.11, the direction of power flow for the assumed transverse
electromagnetic (TEM) wave is given by the right-hand rule for E×H. Note that although we
have set the initial conditions such that the EM wave is traveling in only one dimension (z), its
electric field oscillates in a perpendicular direction (x) and its magnetic field oscillates in yet
a third direction (y); so while some may call this a 1-D wave, the vector nature of the fields
means that the wave occupies all three dimensions.
18.11 FDTD ALGORITHM
We need to solve the two coupled PDEs (18.60) and (18.61) appropriate for our problem. As is
usual for PDEs, we approximate the derivatives via the central-difference approximation, here
in both time and space. For example,
∂E(z, t)
∂t
'
E(z, t+ ∆t2 )− E(z, t−
∆t
2 )
∆t
, (18.62)
∂E(z, t)
∂z
'
E(z + ∆z2 , t)− E(z −
∆z
2 , t)
∆z
. (18.63)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
428 CHAPTER 18
Figure 18.12 The scheme for using the known values of Ex and Hy at three earlier times and three different space
positions to obtain the solution at the present time. Note that the values of Ex are determined on
the lattice of filled circles, corresponding to integer space indices and half-integer time indices. In
contrast, the values of Hy are determined on the lattice of open circles, corresponding to half-integer
space indices and integer time indices.
t
n
n+1
n+1/2
n-1/2
k k+
1
k+
1/
2
k-
1/
2
Hy
Ex
We next substitute the approximations into Maxwell’s equations and rearrange the equations
into the form of an algorithm that advances the solution through time. Because only first
derivatives occur in Maxwell’s equations, the equations are simple, although the electric and
magnetic fields are intermixed.
As introduced by Yee [Yee 66], we set up a space-time lattice (Figure 18.12) in which
there are half-integer time steps as well as half-integer space steps. The magnetic field will be
determined at integer time sites and half-integer space sites (open circles), and the electric field
will be determined at half-integer time sites and integer space sites (filled circles). Because the
fields already have subscripts indicating their vector nature, we indicate the lattice position as
superscripts, for example,
Ex(z, t)→ Ex(k∆z, n∆t)→ Ek,nx . (18.64)
Maxwell’s equations (18.60) and (18.61) now become the discrete equations
E
k,n+1/2
x − Ek,n−1/2x
∆t
=−H
k+1/2,n
y −Hk−1/2,ny
0∆z
,
H
k+1/2,n+1
y −Hk+1/2,ny
∆t
=−E
k+1,n+1/2
x − Ek,n+1/2x
µ0∆z
.
To repeat, this formulation solves for the electric field at integer space steps (k) but half-integer
time steps (n), while the magnetic field is solved for at half-integer space steps but integer time
steps.
We convert these equations into two simultaneous algorithms by solving for Ex at time
n+ 12 , and Hy at time n:
Ek,n+1/2x =E
k,n−1/2
x −
∆t
0 ∆z
(
Hk+1/2,ny −Hk−1/2,ny
)
, (18.65)
Hk+1/2,n+1y =H
k+1/2,n
y −
∆t
µ0∆z
(
Ek+1,n+1/2x − Ek,n+1/2x
)
. (18.66)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDE WAVES: STRING, QUANTUM PACKET, AND E&M 429
The algorithms must be applied simultaneously because the space variation of Hy determines
the time derivative of Ex, while the space variation of Ex determines the time derivative of Hy
(Figure 18.12). This algorithm is more involved than our usual time-stepping ones in that the
electric fields (filled circles in Figure 18.12) at future times t = n + 12 are determined from
the electric fields at one time step earlier t = n − 12 , and the magnetic fields at half a time
step earlier t = n. Likewise, the magnetic fields (open circles in Figure 18.12) at future times
t = n + 1 are determined from the magnetic fields at one time step earlier t = n, and the
electric field at half a time step earlier t = n + 12 . In other words, it is as if we have two
interleaved lattices, with the electric fields determined for half-integer times on lattice 1 and
the magnetic fields at integer times on lattice 2.
Although these half-integer times appear to be the norm for FDTD methods [Taf 89,
Sull 00], it may be easier for some readers to understand the algorithm by doubling the index
values and referring to even and odd times:
Ek,nx = E
k,n−2
x −
∆t
0∆z
(
Hk+1,n−1y −Hk−1,n−1y
)
, k even, n odd, (18.67)
Hk,ny = H
k,n−2
y −
∆t
µ0∆z
(
Ek+1,n−1x − Ek−1,n−1x
)
, k odd, n even. (18.68)
This makes it clear that E is determined for even space indices and odd times, while H is
determined for odd space indices and even times.
We simplify the algorithm and make its stability analysis simpler by normalizing the
electric fields to have the same dimensions as the magnetic fields,
Ẽ =
√
µ0
0
E. (18.69)
The algorithm (18.65) and (18.66) now becomes
Ẽk,n+1/2x = Ẽ
k,n−1/2
x + β
(
Hk−1/2,ny −Hk+1/2,ny
)
, (18.70)
Hk+1/2,n+1y = H
k+1/2,n
y + β
(
Ẽk,n+1/2x − Ẽk+1,n+1/2x
)
, (18.71)
β =
c
∆z/∆t
, c =
1
√
0µ0
. (18.72)
Here c is the speed of light in a vacuum and β is the ratio of the speed of light to grid velocity
∆z/∆t.
The space step ∆z and the time step ∆t must be chosen so that the algorithm is stable.
The scales of the space and time dimensions are set by the wavelength and frequency, respec-
tively, of the propagating wave. As a minimum, we want at least 10 grid points to fall within a
wavelength:
∆z ≤ λ
10
. (18.73)
The time step is then determined by the Courant stability condition [Taf 89, Sull 00] to be
β =
c
∆z/∆t
≤ 1
2
. (18.74)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
430 CHAPTER 18
As we have seen before, (18.74) implies that making the time step smaller improves precision
and maintains stability, but making the space step smaller must be accompanied by a simulta-
neous decrease in the time step in order to maintain stability (you should check this).
Listing 18.3 FDTD.py solves Maxwell’s equations via FDTD time stepping (finite-difference time domain) for
linearly polarized wave propagation in the z direction in free space. 
# FDTD . py FDTD s o l u t i o n o f Maxwell’s equations in 1-D
from visual import *
from visual.graph import *
xmax=201
ymax=100
zmax=100
scene = display(x=0,y=0,width= 800, height= 500,
title= ’E f i e l d : i n cyan , H f i e l d : i n r e d . P r o p a g a t i o n wi th p e r i o d i c boundary
c o n d i t i o n s’,
forward=(-0.6,-0.5,-1))
Efield = curve(x=range(0,xmax),color= color.cyan,radius=1.5,display=scene)
Hfield = curve(x=range(0,xmax),color=color.red,radius=1.5,display=scene)
vplane= curve(pos=[(-xmax,ymax),(xmax,ymax),(xmax,-ymax),(-xmax,-ymax),
(-xmax,ymax)],color=color.cyan)
zaxis=curve(pos=[(-xmax,0),(xmax,0)],color=color.magenta)
hplane=curve(pos=[(-xmax,0,zmax),(xmax,0,zmax),(xmax,0,-zmax),(-xmax,0,-zmax),
(-xmax,0,zmax)],color=color.magenta)
ball1 = sphere(pos = (xmax+30, 0,0), color = color.black, radius = 2)
ts = 2 # for old and new time
beta = 0.05
Ex=zeros((xmax,ts),Float) # init E (gaussian), 201 points, ts=0 old, ts=1 new
Hy=zeros((xmax,ts),Float) # init H (gaussian), 201 points, ts=0 old, ts=1 new
Exlabel1 = label( text = ’Ex’, pos = (-xmax-10, 50), box = 0 )
Exlabel2 = label( text = ’Ex’, pos = (xmax+10, 50), box = 0 )
Hylabel = label( text = ’Hy’, pos = (-xmax-10, 0,50), box = 0 )
zlabel = label( text = ’Z’, pos = (xmax+10, 0), box = 0 ) #to shift figure
#totime=100
ti=0 #t=0: initial position, t=1. next time iteration
def inifields():
for i in range(0,xmax): # in world coordinates
Ex[i,0] = 0.1*sin(2*math.pi*i/100.0)
Hy[i,0] = 0.1*sin(2*math.pi*i/100.0)
def plotfields(ti): # screen coordinates
xx=0
for i in range(0,xmax):
Efield.x[i] = 2*xx-xmax # world to screen coords.
Efield.y[i] = 800*Ex[xx,ti]
Hfield.x[i] = 2*xx-xmax # world to screen coords.
Hfield.z[i] = 800*Hy[xx,ti]
xx+=1
inifields() #initial time
plotfields(ti)
ti=1
count=1
while 1:
for k in range(1,xmax-1):
Ex[k,1]=Ex[k,0] + beta*(Hy[k-1,0]-Hy[k+1,0]) # Maxwell Eq 1
Hy[k,1]=Hy[k,0] + beta*(Ex[k-1,0]-Ex[k+1,0]) # Maxwell Eq 2
Ex[0,1] = Ex[0,0] + beta*(Hy[xmax-2,0]-Hy[1,0]) #periodic boundary
Ex[xmax-1,1]=Ex[xmax-1,0]+ beta*(Hy[xmax-2,0]-Hy[1,0]) #conditions for first
Hy[0,1]=Hy[0,0] + beta*(Ex[xmax-2,0]-Ex[1,0]) #periodic boundary
Hy[xmax-1,1]=Hy[xmax-1,0]+ beta*(Ex[xmax-2,0]-Ex[1,0])
plotfields(ti) #plot sin fields after initial cond.
count+=1
for k in range(0,xmax):
Ex[k,0] = Ex[k,1] # for next iteration in time
Hy[k,0] = Hy[k,1] # old field = new field
18.11.1 Implementation
In Listing 18.3 we provide a simple implementation of the FDTD algorithm for a z lattice of
200 sites. The initial conditions correspond to a sinusoidal variation of the E and H fields for
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDE WAVES: STRING, QUANTUM PACKET, AND E&M 431
Figure 18.13 The E field (cyan) and the H field (red) at the intial time (left) and at a later time (right). Periodic
boundary conditions are used at the ends of the spatial region, which means that the large z wave
continues into the z = 0 wave.
all z values in for 0 ≤ z ≤ 200:
Ex(z, t = 0) = 0.1 sin
2πz
100
, Hy(z, t = 0) = 0.1 sin
2πz
100
, (18.75)
The algorithm then steps out in time for as long you the user desires. The discrete form of
Maxwell equations used are:
Ex[k, 1] = Ex[k, 0] + beta* (Hy[k-1, 0] - Hy[k+1, 0]) (18.76)
Hy[k, 1] = Hy[k, 0] + beta* (Ex[k-1, 0] - Ex[k+1, 0]) (18.77)
where 1 ≤ k ≤ 200, and beta is a constant. The second index takes the values 0 and 1,
with 0 being the old time and 1 the new. At the end of each iteration the new field througout
all of space becomes the old one, and a new, new one is computed. With this algorithm, the
spatial endpoints k=0 and k=xmax-1 remain undefined. We define them by assuming periodic
boundary conditions:
Ex[0, 1] = Ex[0, 0] + beta* (Hy[xmax-2, 0] - Hy[1,0]) (18.78)
Ex[xmax-1, 1] = Ex[xmax-1, 0] + beta* (Hy[xmax-2, 0] - Hy[1,0]) (18.79)
Hy[0, 1] = Hy[0, 0] + beta* (Ex[xmax-2, 0] - Ex[1,0]) (18.80)
Hy[xmax-1, 1] = Hy[xmax-1, 0] + beta* (Ex[xmax-2, 0] - Ex[1,0]) (18.81)
18.11.2 Assessment
1. Impose boundary conditions such that all fields vanish on the boundaries. Compare the
solutions so obtained to those without explicit conditions for times less than and greater
than those at which the pulses hit the walls.
2. Examine the stability of the solution for different values of ∆z and ∆t and thereby test
the Courant condition (18.74).
3. Extend the algorithm to include the effect of entering, propagating through, and exiting a
dielectric material placed within the z integration region.
a. Ensure that you see both transmission and reflection at the boundaries.
b. Investigate the effect of a dielectric with an index of refraction less than 1.
4. The direction of propagation of the pulse is given E×H, which depends on the relative
phase between the E and H fields. (With no initial H field, we obtain pulses both to the
right and the left.)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
432 CHAPTER 18
Figure 18.14 The E and H fields at time 100 for a circularly polarized EM wave in free space.
a. Modify the algorithm such that there is an initial H pulse as well as an initial E pulse,
both with a Gaussian times a sinusoidal shape.
b. Verify that the direction of pulse propagation changes if the E and H fields have
relative phases of 0 or π.
5. Investigate the resonator modes of a wave guide by picking the initial conditions corre-
sponding to plane waves with nodes at the boundaries.
6. Determine what happens when you try to set up standing waves with wavelengths longer
than the size of the integration region.
7. Simulate unbounded propagation by building in periodic boundary conditions into the
algorithm.
8.  Place a medium with periodic permittivity in the integration volume. This should act
as a frequency-dependent filter, which does not propagate certain frequencies at all.
18.11.3 Extension: Circularly Polarized Waves
We now extend our treatment to EM waves in which the E and H fields, while still transverse
and propagating in the z direction, are not restricted to linear polarizations along just one axis.
Accordingly, we add to (18.60) and (18.61):
∂Hx
∂t
=
1
µ0
∂Ey
∂z
, (18.82)
∂Ey
∂t
=
1
0
∂Hx
∂z
. (18.83)
When discretized in the same way as (18.65) and (18.66), we obtain
Hk+1/2,n+1x =H
k+1/2,n
x +
∆t
µ0 ∆z
(
Ek+1,n+1/2y − Ek,n+1/2y
)
, (18.84)
Ek,n+1/2y =E
k,n−1/2
y +
∆t
0 ∆z
(
Hk+1/2,ny −Hk−1/2,ny
)
. (18.85)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
PDE WAVES: STRING, QUANTUM PACKET, AND E&M 433
To produce a circularly polarized traveling wave, we set the initial conditions:
Ex = cos
(
t− z
c
+ φy
)
, Hx =
√
0
µ0
cos
(
t− z
c
+ φy
)
, (18.86)
Ey = cos
(
t− z
c
+ φx
)
, Hy =
√
0
µ0
cos
(
t− z
c
+ φx + π
)
. (18.87)
We take the phases to be φx = π/2 and φy = 0, so that their difference φx − φy = π/2, which
leads to circular polarization. We include the initial conditions in the same manner as we did
the Gaussian pulse, only now with these cosine functions.
Listing 18.4 gives our implementation EMcirc.py for waves with transverse two-
component E and H fields. Some results of the simulation are shown in Figure 18.14, where
you will note the difference in phase between E and H.
Listing 18.4 CircPolartzn.py solves Maxwell’s equations via FDTD time-stepping for circularly polarized wave
propagation in the z direction in free space. 
# C i r c P o l a r z t n . py : s o l v e s Maxwell eqs . u s i n g FDTD method
# g i v e n t h e i n i t i a l E and H f i e l d components a s c o s i n e f u n c t i o n s f o r 200 p o i n t s
#The same s p a c e r e g i o n i s o b s e r v e d as t ime t r a n s c u r s
from v i s u a l i m p o r t ∗
from v i s u a l . g raph i m p o r t ∗ # g r a p h i c s and math c l a s s e s
s c e n e = d i s p l a y ( x = 0 , y = 0 , wid th = 600 , h e i g h t = 400 , r a n g e = 200 ,
t i t l e =’Circular polarization, E field in white, H field in yellow’ )
g l o b a l phy , pyx
max = 201
d e f E x i n i ( t im , z , phx ) : #x component E f i e l d a l l x and t ime t
r e t u r n math . cos ( t im−2.0∗math . p i∗z /200 +phx )
d e f E y i n i ( t im , z , phy ) : #y component E f i e l d a l l x and t ime t
r e t u r n math . cos ( t im−2∗math . p i∗z /200 +phy )
d e f Hxin i ( t im , z , phy ) : #x component H f i e l d a l l x and t ime t
r e t u r n math . cos ( t im−2∗math . p i∗z /200 +phy+math . p i )
d e f Hyin i ( t im , z , phx ) : #y component H f i e l d a l l x and t ime t
r e t u r n math . cos ( t im−2∗math . p i∗z /200 +phx )
# c= ( c0 / dz )∗d t
c = 0 . 1 # Couran t s t a b i l i t y c o n d i t i o n , u n s t a b l e f o r c >0.1
t ime = 100
Ex= z e r o s ( ( max +2 ,2 ) , F l o a t ) # Ex and Hy components
Hy = z e r o s ( ( max +2 ,2 ) , F l o a t )
Ey = z e r o s ( ( max +2 ,2 ) , F l o a t ) # Ey and Hx components
Hx = z e r o s ( ( max +2 ,2 ) , F l o a t )
d e f p l o t f i e l d s ( Ex , Ey , Hx , Hy ) :
r a t e ( 1 0 ) # a v o i d s f l i c k e r i n g
f o r o b j i n s c e n e . o b j e c t s :
o b j . v i s i b l e =0
a r r o w c o l = ( 1 , 1 , 1 ) #E i n w h i t e c o l o r
f o r i i n r a n g e ( 0 , max , 1 0 ) : # xyz r e f e r e n c e sys tem i s d i f f e r e n t f o r p l o t s
ar row ( pos = (0 , i −100 ,0) , a x i s =(35∗Ey [ i , 1 ] , 0 , 3 5∗Ex [ i , 1 ] ) , c o l o r = a r r o w c o l ) # p l o t a r row
arrow ( pos = (0 , i −100 ,0) , a x i s =(35∗Hy [ i , 1 ] , 0 , 3 5∗Hx [ i , 1 ] ) , c o l o r = c o l o r . ye l l ow ) # p l o t a r row
d e f i n i f i e l d s ( ) : # I n i t i a l v a l u e s f o r E and H f i e l d s
phx = 0.5∗math . p i
phy = 0 . 0
f o r k i n r a n g e ( 0 , max ) :
Ex [ k , 0 ] = E x i n i ( 0 , k , phx )
Ey [ k , 0 ] = E y i n i ( 0 , k , phy )
Hx [ k , 0 ] = Hxin i ( 0 , k , phy )
Hy [ k , 0 ] = Hyin i ( 0 , k , phx )
d e f n e w f i e l d s ( ) :
w h i l e 1 : # t ime s t e p s
#
f o r k i n r a n g e ( 1 , max−1) : #New Ex , Ey components
Ex [ k , 1 ] = Ex [ k , 0 ] + c∗(Hy [ k−1,0]−Hy [ k + 1 , 0 ] )
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
434 CHAPTER 18
Ey [ k , 1 ] = Ey [ k , 0 ] + c∗(Hx [ k+1 ,0]−Hx [ k−1 ,0])
Hx [ k , 1 ] = Hx [ k , 0 ] + c∗(Ey [ k+1 ,0]−Ey [ k−1 ,0]) #New Hx , Hy components
Hy [ k , 1 ] = Hy [ k , 0 ] + c∗(Ex [ k−1,0]−Ex [ k + 1 , 0 ] )
Ex [ 0 , 1 ] =Ex [ 0 , 0 ] + c∗(Hy[200−1 ,0]−Hy [ 1 , 0 ] ) # p e r i o d i c boundary
Ex [ 2 0 0 , 1 ] = Ex [ 2 0 0 , 0 ] + c∗(Hy[200−1 ,0]−Hy [ 1 , 0 ] ) # c o n d i t i o n s f o r f i r s t
Ey [ 0 , 1 ] =Ey [ 0 , 0 ] + c∗(Hx[1 ,0]− Hx[200−1 ,0]) # and l a s t p o i n t s
Ey [ 2 0 0 , 1 ] = Ey [ 2 0 0 , 0 ] + c∗(Hx[1 ,0]− Hx[200−1 ,0])
Hx [ 0 , 1 ] =Hx [ 0 , 0 ] + c∗(Ey[1 ,0]− Ey [200−1 ,0])
Hx[ 2 0 0 , 1 ] =Hx[ 2 0 0 , 0 ] + c∗(Ey[1 ,0]− Ey [200−1 ,0])
Hy [ 0 , 1 ] =Hy [ 0 , 0 ] + c∗(Ex[200−1 ,0]−Ex [ 1 , 0 ] )
Hy[ 2 0 0 , 1 ] =Hy[ 2 0 0 , 0 ] + c∗(Ex[200−1 ,0]−Ex [ 1 , 0 ] )
p l o t f i e l d s ( Ex , Ey , Hx , Hy )
f o r k i n r a n g e ( 0 , max ) : # u p d a t e f i e l d s o l d =new
Ex [ k , 0 ] = Ex [ k , 1 ]
Ey [ k , 0 ] = Ey [ k , 1 ]
Hx [ k , 0 ] = Hx [ k , 1 ]
Hy [ k , 0 ] = Hy [ k , 1 ]
i n i f i e l d s ( ) # I n i t i a l f i e l d components a t t =0
n e w f i e l d s ( ) # s u b s e q u e n t e v o l u t i o n o f f i e l d s
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Nineteen
Solitons & Computational Fluid Dynamics
In Unit I of this chapter we discuss shallow-water soliton waves. This extends the discussion of
waves in Chapter 18, “PDE Waves: String, Quantum Packet, and E&M,” by progressively in-
cluding nonlinearities, dispersion, and hydrodynamic effects. In Unit II we confront the more
general equations of computational fluid dynamics (CFD) and their solutions.1 The mathe-
matical description of the motion of fluids, though not a new subject, remains a challenging
one. The equations are complicated and nonlinear, there are many degrees of freedom, the
nonlinearities may lead to instabilities, analytic solutions are rare, and the boundary condi-
tions for realistic geometries (like airplanes) are not intuitive. These difficulties may explain
why fluid dynamics is often absent from undergraduate and even graduate physics curricula.
Nonetheless, as an essential element of the real world that also has tremendous practical im-
portance, we encourage its study. We recommend [F&W 80, L&L,F 87] for those interested in
the derivations, and [Shaw 92] for more details about the computations.
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
- - - *
Applets and Animations
Name Sections Name Sections
Solitons 19.5 2-D Solitons 19.5
Smoothed Particle Hydrodynamics 19.6–19.9 Shock Waves 19.3
19.1 UNIT I. ADVECTION, SHOCKS, RUSSELL’S SOLITON
In 1834, J. Scott Russell observed on the Edinburgh-Glasgow canal [Russ 44]:
I was observing the motion of a boat which was rapidly drawn along a narrow
channel by a pair of horses, when the boat suddenly stopped—not so the mass of
water in the channel which it had put in motion; it accumulated round the prow
of the vessel in a state of violent agitation, then suddenly leaving it behind, rolled
forward with great velocity, assuming the form of a large solitary elevation, a
rounded, smooth and well-defined heap of water, which continued its course along
the channel apparently without change of form or diminution of speed. I followed
it on horseback, and overtook it still rolling on at a rate of some eight or nine miles
an hour, preserving its original figure some thirty feet long and a foot to a foot and
a half in height. Its height gradually diminished, and after a chase of one or two
1We acknowledge some helpful reading of Unit I by Satoru S. Kano.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
436 CHAPTER 19
miles I lost it in the windings of the channel. Such, in the month of August 1834,
was my first chance interview with that singular and beautiful phenomenon. . . .
Russell also noticed that an initial arbitrary waveform set in motion in the channel evolves into
two or more waves that move at different velocities and progressively move apart until they
form individual solitary waves. In Figure 19.2 we see a single steplike wave breaking up into
approximately eight of these solitary waves (also called solitons). These eight solitons occur so
frequently that some consider them the normal modes for this nonlinear system. Russell went
on to produce these solitary waves in a laboratory and empirically deduced that their speed c is
related to the depth h of the water in the canal and to the amplitude A of the wave by
c2 = g(h+A), (19.1)
where g is the acceleration due to gravity. Equation (19.1) implies an effect not found for linear
systems, namely, that waves with greater amplitudes A travel faster than those with smaller
amplitudes. Observe that this is similar to the formation of shock waves but different from
dispersion in which waves of different wavelengths have different velocities. The dependence
of c on amplitude A is illustrated in Figure 19.3, where we see a taller soliton catching up with
and passing through a shorter one.
Problem: Explain Russell’s observations and see if they relate to the formation of tsunamis.
The latter are ocean waves that form from sudden changes in the level of the ocean floor and
then travel over long distances without dispersion or attenuation until they wreak havoc on a
distant shore.
19.2 THEORY: CONTINUITY AND ADVECTION EQUATIONS
The motion of a fluid is described by the continuity equation and the Navier–Stokes equation
[L&L,M 76]. We will discuss the former here and the latter in §19.7 of Unit II. The continuity
equation describes conservation of mass:
xmll ∂ρ(x, t)
∂t
+ ~∇ · j = 0, j def= ρv(x, t). (19.2)
Here ρ(x, t) is the mass density, v(x, t) is the velocity of the fluid, and the product j = ρv
is the mass current. As its name implies, the divergence ~∇ · j describes the spreading of the
current in a region of space, as might occur if there were a current source. Physically, the
continuity equation (19.2) states that changes in the density of the fluid within some region of
space arise from the flow of current in and out of that region.
For 1-D flow in the x direction and for a fluid that is moving with a constant velocity
v = c, the continuity equation (19.2) takes the simple form
xmll ∂ρ∂t +
∂(cρ)
∂x
= 0, (19.3)
∂ρ
∂t
+ c
∂ρ
∂x
= 0. (19.4)
This equation is known as the advection equation, where the term “advection” is used to de-
scribe the horizontal transport of a conserved quantity from one region of space to another due
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLITONS & COMPUTATIONAL FLUID DYNAMICS 437
to a velocity field. For instance, advection describes dissolved salt transported in water.
The advection equation looks like a first-derivative form of the wave equation, and in-
deed, the two are related. A simple substitution proves that any function with the form of a
traveling wave,
u(x, t) = f(x− ct), (19.5)
will be a solution of the advection equation. If we consider a surfer riding along the crest of
a traveling wave, that is, remaining at the same position relative to the wave’s shape as time
changes, then the surfer does not see the shape of the wave change in time, which implies that
x− ct = constant ⇒ x = ct+ constant. (19.6)
The speed of the surfer is therefore dx/dt = c, which is a constant. Any function f(x − ct)
is clearly a traveling wave solution in which an arbitrary pulse is carried along by the fluid at
velocity c without changing shape.
19.2.1 Advection Implementation
Although the advection equation is simple, trying to solve it by a simple differencing scheme
(the leapfrog method) may lead to unstable numerical solutions. As we shall see when we look
at the nonlinear version of this equation, there are better ways to solve it. Listing 19.1 presents
our code for solving the advection equation using the Lax–Wendroff method (a better method).
19.3 THEORY: SHOCK WAVES VIA BURGERS’ EQUATION
In a later section we will examine use of the KdeV equation to describe Russell’s solitary
waves. In order to understand the physics contained in that equation, we study some terms in
it one at a time. To start, consider Burgers’ equation [Burg 74]:
xmll
∂u
∂t
+ u
∂u
∂x
= 0, (19.7)
∂u
∂t
+ 
∂(u2/2)
∂x
= 0, (19.8)
where the second equation is the conservative form. This equation can be viewed as a varia-
tion on the advection equation (19.4) in which the wave speed c = u is proportional to the
amplitude of the wave, as Russell found for his waves. The second nonlinear term in Burgers’
equation leads to some unusual behaviors. Indeed, John von Neumann studied this equation as
a simple model for turbulence [F&S].
Listing 19.1 AdvecLax.py solves the advection equation via the Lax–Wendroff scheme. 
# AdvecLax . py : So lve a d v e c t i o n e q n t v i a Lax − Wendroff scheme
from v i s u a l i m p o r t ∗ ; from v i s u a l . g raph i m p o r t ∗
m = 100 ; c = 1 . ; dx = 1 . /m; b e t a = 0 . 8 # b e t a = c∗d t / dx
u = z e r o s ( (m+1) , F l o a t ) ; u0 = z e r o s ( (m+1) , F l o a t ) ; u f = z e r o s ( (m+1) , F l o a t )
d t = b e t a∗dx / c ; T f i n a l = 0 . 5 ; n = i n t ( T f i n a l / d t )
g raph1 = g d i s p l a y ( wid th =600 , h e i g h t =500 ,
t i t l e =’Advection Eqn: Initial (red), Exact (blue), Numerical Lax-Wendroff
(yellow)’ ,
x t i t l e = ’x’ , y t i t l e = ’u(x), Blue=exact, Yellow=Sim’ ,
xmin =0 , xmax =1 , ymin =0 , ymax =1)
i n i t f n = gc u rv e ( c o l o r = c o l o r . r e d ) # I n i t i a l f u n c t i o n
e x a c t f n = gc u r ve ( c o l o r = c o l o r . b l u e ) # Exac t f u n c t i o n
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
438 CHAPTER 19
Figure 19.1 The formation of a shock wave. Wave height versus position for increasing times as visualized with
Gnuplot (left) and with OpenDX (right).
0
5
10
15
20
0
4
8
12
-4
-2
0
2
4
u(x,t)
x
t
numfn = gc u rv e ( c o l o r = c o l o r . ye l l ow ) # Numer ica l f u n c t i o n
f o r i i n r a n g e ( 0 , m) : # I n i t i a l and e x a c t f u n c t i o n s
x = i∗dx
u0 [ i ] = exp ( − 300.∗ ( x − 0 . 1 2 ) ∗∗2) # G a u s s i a n i n i t i a l d a t a
i n i t f n . p l o t ( pos = ( 0 . 0 1∗ i , u0 [ i ] ) )
u f [ i ] = exp ( − 3 0 0 .∗ ( x − 0 . 1 2 − c∗T f i n a l )∗∗2 ) # Exac t i n b l u e c o l o r
e x a c t f n . p l o t ( pos = ( 0 . 0 1∗ i , u f [ i ] ) )
r a t e ( 2 0 )
f o r j i n r a n g e ( 0 , n +1) :
f o r i i n r a n g e ( 0 , m − 1) :
u [ i + 1 ] = (1.− b e t a∗ b e t a )∗u0 [ i + 1]−(0.5∗ b e t a ) ∗(1.− b e t a )∗u0 [ i + 2 ] + ( 0 . 5∗ b e t a ) ∗ ( 1 . +
b e t a )∗u0 [ i ]
u [ 0 ] = 0 . # Lax − Wendroff scheme
u [m − 1] = 0 .
u0 [ i ] = u [ i ]
f o r j i n r a n g e ( 0 , m−1 ) :
r a t e ( 3 0 )
numfn . p l o t ( pos = ( 0 . 0 1∗ j , u [ j ] ) ) # S o l u t i o n
In the advection equation (19.4), all points on the wave move at the same speed c, and
so the shape of the wave remains unchanged in time. In Burgers’ equation (19.7), the points
on the wave move (“advect”) themselves such that the local speed depends on the local wave’s
amplitude, with the high parts of the wave moving progressively faster than the low parts.
This changes the shape of the wave in time; if we start with a wave packet that has a smooth
variation in height, the high parts will speed up and push their way to the front of the packet,
thereby forming a sharp leading edge known as a shock wave [Tab 89]. A shock wave solution
to Burgers’ equation with  = 1 is shown in Figure 19.1.
19.3.1 Lax–Wendroff Algorithm for Burgers’ Equation
We first solve Burgers’ equation (19.4) via the usual approach in which we express the deriva-
tives as central differences. This leads to a leapfrog scheme for the future solution in terms of
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLITONS & COMPUTATIONAL FLUID DYNAMICS 439
present and past ones:
u(x, t+ ∆t) =u(x, t−∆t)− β
[
u2(x+ ∆x, t)− u2(x−∆x, t)
2
]
,
ui,j+1 =ui,j−1 − β
[
u2i+1,j − u2i−1,j
2
]
, β =

∆x/∆t
. (19.9)
Here u2 is the square of u and is not its second derivative, and β is a ratio of constants known
as the Courant–Friedrichs–Lewy (CFL) number. As you should prove for yourself, β < 1 is
required for stability.
While we have used a leapfrog method with success in the past, its low-order approxi-
mation for the derivative becomes inaccurate when the gradients can get large, as happens with
shock waves, and the algorithm may become unstable [Pres 94]. The Lax–Wendroff method at-
tains better stability and accuracy by retaining second-order differences for the time derivative:
xmllu(x, t+ ∆t) ' u(x, t) +
∂u
∂t
∆t+
1
2
∂2u
∂t2
∆t2. (19.10)
To covert (19.10) to an algorithm, we use Burgers’ equation ∂u/∂t = −∂(u2/2)/∂x for the
first-order time derivative. Likewise, we use this equation to express the second-order time
derivative in terms of space derivatives:
∂2u
∂t2
=
∂
∂t
[
− ∂
∂x
(
u2
2
)]
= − ∂
∂x
∂
∂t
(
u2
2
)
(19.11)
=− ∂
∂x
(
u
∂u
∂t
)
= 2
∂
∂x
[
u
∂
∂x
(
u2
2
)]
.
We next substitute these derivatives into the Taylor expansion (19.10) to obtain
u(x, t+ ∆t) = u(x, t)−∆t ∂
∂x
(
u2
2
)
+
(∆t)2
2
2
∂
∂x
[
u
∂
∂x
(
u2
2
)]
.
We now replace the outer x derivatives by central differences of spacing ∆x/2:
u(x, t+ ∆t) =u(x, t)− ∆t
2
u2(x+ ∆x, t)− u2(x−∆x, t)
2∆x
+
(∆t)2 2
2
× 1
2∆x
[
u
(
x+
∆x
2
, t
)
∂
∂x
u2
(
x+
∆x
2
, t
)
− u
(
x− ∆x
2
, t
)
∂
∂x
u2
(
x− ∆x
2
, t
)]
.
Next we approximate u(x±∆x/2, t) by the average of adjacent grid points,
u(x± ∆x
2
, t) ' u(x, t) + u(x±∆x, t)
2
,
and apply a central-difference approximation to the second derivatives:
∂u2(x±∆x/2, t)
∂x
=
u2(x±∆x, t)− u2(x, t)
±∆x
.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
440 CHAPTER 19
Finally, putting all these derivatives together yields the discrete form
ui,j+1 = ui,j −
β
4
(
u2i+1,j − u2i−1,j
)
+
β2
8
[
(ui+1,j + ui,j)
(
u2i+1,j − u2i,j
)
−(ui,j + ui−1,j)
(
u2i,j − u2i−1,j
)]
, (19.12)
where we have substituted the CFL number β. This Lax–Wendroff scheme is explicit, centered
upon the grid points, and stable for β < 1 (small nonlinearities).
19.3.2 Implementation and Assessment of
Burgers’ Shock Equation
1. Write a program to solve Burgers’ equation via the leapfrog method.
2. Define arrays u0[100] and u[100] for the initial data and the solution.
3. Take the initial wave to be sinusoidal, u0[i]= 3 sin(3.2x), with speed c = 1.
4. Incorporate the boundary conditions u[0]=0 and u[100]=0.
5. Keep the CFL number β < 1 for stability.
6. Now modify your program to solve Burgers’ shock equation (19.8) using the Lax–
Wendroff method (19.12).
7. Save the initial data and the solutions for a number of times in separate files for plotting.
8. Plot the initial wave and the solution for several time values on the same graph in order
to see the formation of a shock wave (like Figure 19.1).
9. Run the code for several increasingly large CFL numbers. Is the stability condition
β < 1 correct for this nonlinear problem?
10. Compare the leapfrog and Lax–Wendroff methods. With the leapfrog method you
should see shock waves forming but breaking up into ripples as the square edge de-
velops. The ripples are numerical artifacts. The Lax–Wendroff method should give a a
better shock wave (square edge), although some ripples may still occur.
Listing 19.1 presents our implementation of the Lax–Wendroff method.
19.4 INCLUDING DISPERSION
We have just seen that Burgers’ equation can turn an initially smooth wave into a square-
edged shock wave. An inverse wave phenomenon is dispersion, in which a waveform disperses
or broadens as it travel through a medium. Dispersion does not cause waves to lose energy
and attenuate but rather to lose information with time. Physically, dispersion may arise when
the propagating medium has structures with a spatial regularity equal to some fraction of a
wavelength. Mathematically, dispersion may arise from terms in the wave equation that contain
higher-order space derivatives. For example, consider the waveform
u(x, t) = e±i(kx−ωt) (19.13)
corresponding to a plane wave traveling to the right (“traveling” because the phase kx − ωt
remains unchanged if you increase x with time). When this u(x, t) is substituted into the
advection equation (19.4), we obtain
ω = ck. (19.14)
This equation is an example of a dispersion relation, that is, a relation between frequency ω
and wave vector k. Because the group velocity of a wave
vg =
∂ω
∂k
, (19.15)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLITONS & COMPUTATIONAL FLUID DYNAMICS 441
Figure 19.2 A single two-level waveform at time zero progressively breaks up into eight solitons (labeled) as time
increases. The tallest soliton (1) is narrower and faster in its motion to the right. You can generate an
animation of this with the program SolitonAnimate.py.
0 20 40 60x
0
4
8
t
0
1
2
12345678
the linear dispersion relation (19.14) leads to all frequencies having the same group velocity c
and thus dispersionless propagation.
Let us now imagine that a wave is propagating with a small amount of dispersion, that
is, with a frequency that has somewhat less than a linear increase with the wave number k:
ω ' ck − βk3. (19.16)
Note that we skip the even powers in (19.16), so that the group velocity,
vg =
dω
dk
' c− 3βk2, (19.17)
is the same for waves traveling to the left the or the right. If plane-wave solutions like (19.13)
were to arise from a wave equation, then (as verified by substitution) the ω term of the disper-
sion relation (19.16) would arise from a first-order time derivative, the ck term from a first-order
space derivative, and the k3 term from a third-order space derivative:
∂u(x, t)
∂t
+ c
∂u(x, t)
∂x
+ β
∂3u(x, t)
∂x3
= 0. (19.18)
We leave it as an exercise to show that solutions to this equation do indeed have waveforms
that disperse in time.
19.5 SHALLOW-WATER SOLITONS; THE KDEV EQUATION
In this section we look at shallow-water soliton waves. Though including some complica-
tions,fascinating and is one for which the computer has been absolutely essential for discovery
and understanding. In addition, we recommend that you look at some of the soliton animation
we have placed in the animations folder on the CD.
We want to understand the unusual water waves that occur in shallow, narrow channels such
as canals [Abar 93, Tab 89]. The analytic description of this “heap of water” was given by
Korteweg and deVries (KdeV) [KdeV 95] with the partial differential equation
xmll
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
442 CHAPTER 19
Figure 19.3 Two shallow-water solitary waves crossing each other computed with the code Soliton.py. The
taller soliton on the left catches up with and overtakes the shorter one at t ' 5.
0
40
80
120
x
0
2
4
6
8
t
∂u(x, t)
∂t
+ εu(x, t)
∂u(x, t)
∂x
+ µ
∂3u(x, t)
∂x3
= 0. (19.19)
As we discussed in §19.1 in our study of Burgers’ equation, the nonlinear term εu ∂u/∂t leads
to a sharpening of the wave and ultimately a shock wave. In contrast, as we discussed in our
study of dispersion, the ∂3u/∂x3 term produces broadening. These together with the ∂u/∂t
term produce traveling waves. For the proper parameters and initial conditions, the dispersive
broadening exactly balances the nonlinear narrowing, and a stable traveling wave is formed.
The KdeV equation solved (19.19) analytically and proved that the speed (19.1) given by
Russell is in fact correct. Seventy years after its discovery, the KdeV equation was rediscovered
by Zabusky and Kruskal [Z&K 65], who solved it numerically and found that a cos(x/L) initial
condition broke up into eight solitary waves (Figure 19.2). They also found that the parts of
the wave with larger amplitudes moved faster than those with smaller amplitudes, which is why
the higher peaks tend to be on the right in Figure 19.2. As if wonders never cease, Zabusky
and Kruskal, who coined the name soliton for the solitary wave, also observed that a faster
peak actually passed through a slower one unscathed (Figure 19.3).
19.5.1 Analytic Soliton Solution
The trick in analytic approaches to these types of nonlinear equations is to substitute a guessed
solution that has the form of a traveling wave,
u(x, t) = u(ξ = x− ct). (19.20)
This form means that if we move with a constant speed c, we will see a constant wave form
(but now the speed will depend on the magnitude of u). There is no guarantee that this form of
a solution exists, but it is a lucky guess because substitution into the KdeV equation produces
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLITONS & COMPUTATIONAL FLUID DYNAMICS 443
a solvable ODE and its solution:
xmll−c ∂u∂ξ +  u
∂u
∂ξ
+ µ
d3u
dξ3
= 0, (19.21)
u(x, t) =
−c
2
sech2
[
1
2
√
c(x− ct− ξ0)
]
, (19.22)
where ξ0 is the initial phase. We see in (19.22) an amplitude that is proportional to the wave
speed c, and a sech2 function that gives a single lumplike wave. This is a typical analytic form
for a soliton.
19.5.2 Algorithm for KdeV Solitons
The KdeV equation is solved numerically using a finite-difference scheme with the time and
space derivatives given by central-difference approximations:
∂u
∂t
' ui,j+1 − ui,j−1
2∆t
,
∂u
∂x
' ui+1,j − ui−1,j
2∆x
. (19.23)
To approximate ∂3u(x, t)/∂x3, we expand u(x, t) to O(∆t)3 about the four points u(x ±
2∆x, t) and u(x±∆x, t),
u(x±∆x, t) ' u(x, t)± (∆x)∂u
∂x
+
(∆x)2
2!
∂2u
∂2x
± (∆x)
3
3!
∂3u
∂x3
, (19.24)
which we solve for ∂3u(x, t)/∂x3. Finally, the factor u(x, t) in the second term of (19.19) is
taken as the average of three x values all with the same t:
u(x, t) ' ui+1,j + ui,j + ui−1,j
3
. (19.25)
These substitutions yield the algorithm for the KdeV equation:
ui,j+1'ui,j−1 −

3
∆t
∆x
[ui+1,j + ui,j + ui−1,j ] [ui+1,j − ui−1,j ]
−µ ∆t
(∆x)3
[ui+2,j + 2ui−1,j − 2ui+1,j − ui−2,j ] . (19.26)
To apply this algorithm to predict future times, we need to know u(x, t) at present and past
times. The initial-time solution ui,1 is known for all positions i via the initial condition. To
find ui,2, we use a forward-difference scheme in which we expand u(x, t), keeping only two
terms for the time derivative:
ui,2'ui,1 −
∆t
6 ∆x
[ui+1,1 + ui,1 + ui−1,1] [ui+1,1 − ui−1,1]
−µ
2
∆t
(∆x)3
[ui+2,1 + 2ui−1,1 − 2ui+1,1 − ui−2,1] . (19.27)
The keen observer will note that there are still some undefined columns of points, namely, u1,j ,
u2,j , uNmax−1,j , and uNmax,j , where Nmax is the total number of grid points. A simple technique
for determining their values is to assume that u1,2 = 1 and uNmax,2 = 0. To obtain u2,2 and
uNmax−1,2, assume that ui+2,2 = ui+1,2 and ui−2,2 = ui−1,2 (avoid ui+2,2 for i = Nmax − 1,
and ui−2,2 for i = 2). To carry out these steps, approximate (19.27) so that
ui+2,2 + 2ui−1,2 − 2ui+1,2 − ui−2,2 → ui−1,2 − ui+1,2.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
444 CHAPTER 19
The truncation error and stability condition for our algorithm are related:
E(u) = O[(∆t)3] +O[∆t(∆x)2] , (19.28)
1
(∆x/∆t)
[
|u|+ 4 µ
(∆x)2
]
≤ 1 . (19.29)
The first equation shows that smaller time and space steps lead to a smaller approximation
error, yet because round-off error increases with the number of steps, the total error does not
necessarily decrease (Chapter 2, “Errors & Uncertainties in Computations”). Yet we are also
limited in how small the steps can be made by the stability condition (19.29), which indicates
that making ∆x too small always leads to instability. Care and experimentation are required.
19.5.3 Implementation: KdeV Solitons
Modify or run the program Soliton.py in Listing 19.2 that solves the KdeV equation (19.19)
for the initial condition
u(x, t = 0) =
1
2
[
1− tanh
(
x− 25
5
)]
,
with parameters  = 0.2 and µ = 0.1. Start with ∆x = 0.4 and ∆t = 0.1. These constants are
chosen to satisfy (19.28) with |u| = 1.
1. Define a 2-D array u[131][3] with the first index corresponding to the position x and
the second to the time t. With our choice of parameters, the maximum value for x is
130× 0.4 = 52.
2. Initialize the time to t = 0 and assign values to u[i][1].
3. Assign values to u[i][2], i = 3, 4, . . . , 129, corresponding to the next time interval.
Use (19.27) to advance the time but note that you cannot start at i = 1 or end at i = 131
because (19.27) would include u[132][2] and u[--1][1], which are beyond the limits
of the array.
4. Increment the time and assume that u[1][2] = 1 and u[131][2] = 0. To obtain u[2][2]
and u[130][2], assume that u[i+2][2] = u[i+1][2] and u[i--2][2] = u[i--1][2].
Avoid u[i+2][2] for i = 130, and u[i--2][2] for i = 2. To do this, approximate
(19.27) so that (19.28) is satisfied.
5. Increment time and compute u[i][j] for j = 3 and for i = 3, 4, . . . , 129,
using equation (19.26). Again follow the same procedures to obtain the missing array
elements u[2][j] and u[130][j] (set u[1][j] = 1. and u[131][j] = 0). As you
print out the numbers during the iterations, you will be convinced that it was a good
choice.
6. Set u[i][1] = u[i][2] and u[i][2] = u[i][3] for all i. In this way you are ready to
find the next u[i][j] in terms of the previous two rows.
7. Repeat the previous two steps about 2000 times. Write your solution to a file after
approximately every 250 iterations.
8. Use your favorite graphics tool (we used Gnuplot) to plot your results as a 3-D graph of
disturbance u versus position and versus time.
9. Observe the wave profile as a function of time and try to confirm Russell’s observation
that a taller soliton travels faster than a smaller one.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLITONS & COMPUTATIONAL FLUID DYNAMICS 445
Listing 19.2 Soliton.py solves the KdeV equation for 1-D solitons corresponding to a “bore” initial conditions. The
code Soliton animate.py produces an animation. 
# S o l i t o n . py : S o l v e s Korteweg de V r i e s e q u a t i o n f o r a s o l i t o n .
from numpy i m p o r t ∗ ; i m p o r t p y l a b as p ; i m p o r t m a t p l o t l i b . axes3d as p3
ds = 0 . 4 ; d t = 0 . 1 ; max = 2000 ; mu = 0 . 1 ; eps = 0 . 2 ; mx = 131
u = z e r o s ( (mx , 3 ) , f l o a t ) ; s p l = z e r o s ( (mx , 21) , f l o a t )
m = 1 # c o u n t s l i n e s t o be p l o t t e d
f o r i i n r a n g e ( 0 , 131) : # i n i t i a l wave
u [ i , 0 ] = 0 . 5∗ ( 1 −(( math . exp ( 2∗ ( 0 . 2∗ ds∗ i −5.) )−1) / ( math . exp ( 2∗ ( 0 . 2∗ ds∗ i −5.) ) +1) ) )
u [ 0 , 1 ] = 1 . ; u [ 0 , 2 ] = 1 . ; u [ 1 3 0 , 1 ] = 0 . ; u [ 1 3 0 , 2 ] = 0 . # End p o i n t s
f o r i i n r a n g e ( 0 , 131 , 2 ) : s p l [ i , 0 ] = u [ i , 0 ] # i n i t i a l wave e v e r y 2x s t e p s
f a c = mu∗d t / ( ds ∗∗3)
p r i n t "Working, please hold breath and wait while I count to 20"
f o r i i n r a n g e ( 1 , mx − 1) : # F i r s t t ime s t e p
a1 = eps∗d t ∗( u [ i + 1 , 0 ] + u [ i , 0 ] + u [ i − 1 , 0 ] ) / ( ds ∗6 . )
i f i > 1 and i < 129 : a2 = u [ i +2 ,0 ] + 2 .∗ u [ i −1 ,0] − 2 .∗ u [ i +1 ,0 ] − u [ i −2 ,0]
e l s e : a2 = u [ i−1, 0 ] − u [ i +1 , 0 ]
a3 = u [ i +1 , 0 ] − u [ i−1, 0 ]
u [ i , 1 ] = u [ i , 0 ] − a1∗a3 − f a c∗a2 / 3 .
f o r j i n r a n g e ( 1 , max + 1) : # n e x t t ime s t e p s
f o r i i n r a n g e ( 1 , mx − 2) :
a1 = eps∗d t ∗( u [ i + 1 , 1 ] + u [ i , 1 ] + u [ i − 1 , 1 ] ) / ( 3 .∗ ds )
i f i >1 and i<mx−2: a2 = u [ i +2 ,1 ] + 2 .∗ u [ i −1 ,1] − 2 .∗ u [ i +1 ,1 ] − u [ i −2 ,1]
e l s e : a2 = u [ i − 1 , 1 ] − u [ i + 1 , 1 ]
a3 = u [ i +1 , 1 ] − u [ i−1, 1 ]
u [ i , 2 ] = u [ i , 0 ] − a1∗a3 − 2 .∗ f a c∗a2 / 3 .
i f j %100 == 0 : # p l o t e v e r y 100 t ime s t e p s
f o r i i n r a n g e ( 1 , mx − 2) : s p l [ i , m] = u [ i , 2 ]
p r i n t m
m = m + 1
f o r k i n r a n g e ( 0 , mx) : # r e c y c l e a r r a y t o save memory
u [ k , 0 ] = u [ k , 1 ]
u [ k , 1 ] = u [ k , 2 ]
x = r a n g e ( 0 , mx , 2 ) # p l o t e v e r y o t h e r p o i n t
y = r a n g e ( 0 , 21) # p l o t 21 l i n e s e v e r y 100 t s t e p s
X, Y = p . meshgr id ( x , y )
f i g = p . f i g u r e ( ) # c r e a t e f i g u r e
ax = p3 . Axes3D ( f i g ) # p l o t axes
ax . p l o t w i r e f r a m e (X, Y, s p l [X, Y] , c o l o r = ’r’ ) # r e d w i r e f r a m e
ax . s e t x l a b e l (’Positon’ ) # l a b e l axes
ax . s e t y l a b e l (’Time’ )
ax . s e t z l a b e l (’Disturbance’ )
p . show ( ) # show f i g u r e , c l o s e Python s h e l l
p r i n t "That’s all folks!"
19.5.4 Exploration: Solitons in Phase Space, Crossing
1. Explore what happens when a tall soliton collides with a short one.
a. Start by placing a tall soliton of height 0.8 at x = 12 and a smaller soliton in front of
it at x = 26:
u(x, t = 0) = 0.8
[
1− tanh2
(
3x
12
− 3
)]
+ 0.3
[
1− tanh2
(
4.5x
26
− 4.5
)]
.
b. Do they reflect from each other? Do they go through each other? Do they interfere?
Does the tall soliton still move faster than the short one after the collision (Figure
19.3)?
2. Construct phase–space plots [u̇(t) versus u(t)] of the KdeV equation for various parame-
ter values. Note that only very specific sets of parameters produce solitons. In particular,
by correlating the behavior of the solutions with your phase–space plots, show that the
soliton solutions correspond to the separatrix solutions to the KdeV equation. In other
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
446 CHAPTER 19
Figure 19.4 Cross-sectional view of the flow of a stream around a submerged beam (left) and two parallel plates
(right). Both beam and plates have length L along the direction of flow. The flow is seen to be sym-
metric about the centerline and to be unaffected at the bottom and surface by the submerged object.
surface
bottom bottom
surface
River River
xx
y
y
L
H
L
words, the stability in time for solitons is analogous to the infinite period for a pendulum
balanced straight upward.
19.6 UNIT II. RIVER HYDRODYNAMICS
Problem: In order to give migrating salmon a place to rest during their arduous upstream
journey, the Oregon Department of Environment is thinking of placing objects in several deep,
wide, fast-flowing streams. One such object is a long beam of rectangular cross section (Fig-
ure 19.4 left), and another is a set of plates (Figure 19.4 right). The objects are far enough
below the surface so as not to disturb the surface flow, and far enough from the bottom of the
stream so as not to disturb the flow there either.
Your problem is to determine the spatial dependence of the stream’s velocity and, specif-
ically, whether the wake of the object will be large enough to provide a resting place for a
meter-long salmon.
19.7 HYDRODYNAMICS, THE NAVIER–STOKES EQUATION (THEORY)
We continue with the assumption made in Unit I that water is incompressible and thus that its
density ρ is constant. We also simplify the theory by looking only at steady-state situations,
that is, ones in which the velocity is not a function of time. However, to understand how water
flows around objects, like our beam, it is essential to include the complication of frictional
forces (viscosity).
For the sake of completeness, we repeat here the first equation of hydrodynamics, the
continuity equation (19.2):
xmll ∂ρ(x, t)
∂t
+ ~∇ · j = 0, j def= ρv(x, t). (19.30)
Before proceeding to the second equation, we introduce a special time derivative, the hydro-
dynamic derivative Dv/Dt, which is appropriate for a quantity contained in a moving fluid
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLITONS & COMPUTATIONAL FLUID DYNAMICS 447
[F&W 80]:
Dv
Dt
def= (v · ~∇)v + ∂v
∂t
. (19.31)
This derivative gives the rate of change, as viewed from a stationary frame, of the velocity
of material in an element of fluid and so incorporates changes due to the motion of the fluid
(first term) as well as any explicit time dependence of the velocity. Of particular interest is that
Dv/Dt is second order in the velocity, and so its occurrence reflects nonlinearities into the
theory. You may think of these nonlinearities as related to the fictitious (inertial) forces that
would occur if we tried to describe the motion in the fluid’s rest frame (an accelerating frame).
The material derivative is the leading term in the Navier–Stokes equation,
Dv
Dt
= ν∇2v − 1
ρ
~∇P (ρ, T, x), (19.32)
∂vx
∂t
+
z∑
j=x
vj
∂vx
∂xj
= ν
z∑
j=x
∂2vx
∂x2j
− 1
ρ
∂P
∂x
,
∂vy
∂t
+
z∑
j=x
vj
∂vy
∂xj
= ν
z∑
j=x
∂2vy
∂x2j
− 1
ρ
∂P
∂y
,
∂vz
∂t
+
z∑
j=x
vj
∂vz
∂xj
= ν
z∑
j=x
∂2vz
∂x2j
− 1
ρ
∂P
∂z
. (19.33)
Here ν is the kinematic viscosity, P is the pressure, and (19.33) shows the derivatives in Carte-
sian coordinates. This equation describes transfer of the momentum of the fluid within some
region of space as a result of forces and flow (think dp/dt = F), there being a simultaneous
equation for each of the three velocity components. The v · ∇v term in Dv/Dt describes
transport of momentum in some region of space resulting from the fluid’s flow and is often
called the convection or advection term.2 The ~∇P term describes the velocity change as a
result of pressure changes, and the ν∇2v term describes the velocity change resulting from
viscous forces (which tend to dampen the flow).
The explicit functional dependence of the pressure on the fluid’s density and temperature
P (ρ, T, x) is known as the equation of state of the fluid and would have to be known before
trying to solve the Navier–Stokes equation. To keep our problem simple we assume that the
pressure is independent of density and temperature, which leaves the four simultaneous partial
differential equations (19.30) and (19.32) to solve. Because we are interested in steady-state
flow around an object, we assume that all time derivatives of the velocity vanish. Because we
assume that the fluid is incompressible, the time derivative of the density also vanishes, and
(19.30) and (19.32) become
~∇ · v≡
∑
i
∂vi
∂xi
= 0, (19.34)
(v · ~∇)v = ν∇2v − 1
ρ
~∇P. (19.35)
The first equation expresses the equality of inflow and outflow and is known as the condition
of incompressibility. In as much as the stream in our problem is much wider than the width (z
2We discuss pure advection in §19.1. In oceanology or meteorology, convection implies the transfer of mass in the vertical
direction where it overcomes gravity, whereas advection refers to transfer in the horizontal direction.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
448 CHAPTER 19
dimension) of the beam and because we are staying away from the banks, we will ignore the z
dependence of the velocity. The explicit PDEs we need to solve are then:
xmll ∂vx
∂x
+
∂vy
∂y
= 0, (19.36)
ν
(
∂2vx
∂x2
+
∂2vx
∂y2
)
= vx
∂vx
∂x
+ vy
∂vx
∂y
+
1
ρ
∂P
∂x
, (19.37)
ν
(
∂2vy
∂x2
+
∂2vy
∂y2
)
= vx
∂vy
∂x
+ vy
∂vy
∂y
+
1
ρ
∂P
∂y
. (19.38)
19.7.1 Boundary Conditions for Parallel Plates
The plate problem is relatively easy to solve analytically, and so we will do it! This will give
us some experience with the equations as well as a check for our numerical solution. To find a
unique solution to the PDEs (19.36)–(19.38), we need to specify boundary conditions. As far as
we can tell, picking boundary conditions is somewhat of an acquired skill, and it becomes easier
with experience (similar to what happens after solving hundreds of electrostatics problems).
Some of the boundary conditions apply to the flow at the surfaces of submerged objects, while
others apply to the “box” or tank that surrounds the fluid. As we shall see, sometimes these
boundary conditions relate to the velocities directly, while at other times they relate to the
derivatives of the velocities.
We assume that the submerged parallel plates are placed in a stream that is flowing with
a constant velocity V0 in the horizontal direction (Figure 19.4 right). If the velocity V0 is not
too high or the kinematic viscosity ν is sufficiently large, then the flow should be smooth and
without turbulence. We call such flow laminar. Typically, a fluid undergoing laminar flow
moves in smooth paths that do not close on themselves, like the smooth flow of water from a
faucet. If we imagine attaching a vector to each element of the fluid, then the path swept out
by that vector is called a streamline or line of motion of the fluid. These streamlines can be
visualized experimentally by adding a colored dye to the stream. We assume that the plates are
so thin that the flow remains laminar as it passes around and through them.
If the plates are thin, then the flow upstream of them is not affected, and we can limit
our solution space to the rectangular region in Figure 19.5. We assume that the length L and
separation H of the plates are small compared to the size of the stream, so the flow returns to
uniform as we get far downstream from the plates. As seen in Figure 19.5, there are boundary
conditions at the inlet where the fluid enters our solution space, at the outlet where it leaves,
and at the stationary plates. In addition, since the plates are far from the stream’s bottom and
surface, we may assume that the dotted-dashed centerline is a plane of symmetry, with identical
flow above and below the plane. We thus have four different types of boundary conditions to
impose on our solution:
Solid plates: Since there is friction (viscosity) between the fluid and the plate surface, the
only way to have laminar flow is to have the fluid’s velocity equal to the plate’s velocity,
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLITONS & COMPUTATIONAL FLUID DYNAMICS 449
Figure 19.5 The boundary conditions for two thin submerged plates. The surrounding box is the integration volume
within which we solve the PDEs and upon whose surface we impose the boundary conditions. In
practice the box is much larger than L and H.
L
H
which means both are zero:
vx = vy = 0.
Such being the case, we have smooth flow in which the negligibly thin plates lie along
streamlines of the fluid (like a “streamlined” vehicle).
Inlet: The fluid enters the integration domain at the inlet with a horizontal velocity V0. Since
the inlet is far upstream from the plates, we assume that the fluid velocity at the inlet is
unchanged:
vx = V0, vy = 0.
Outlet: Fluid leaves the integration domain at the outlet. While it is totally reasonable to
assume that the fluid returns to its unperturbed state there, we are not told what that is. So,
instead, we assume that there is a physical outlet at the end with the water just shooting
out of it. Consequently, we assume that the water pressure equals zero at the outlet (as at
the end of a garden hose) and that the velocity does not change in a direction normal to
the outlet:
P = 0,
∂vx
∂x
=
∂vy
∂x
= 0.
Symmetry plane: If the flow is symmetric about the y = 0 plane, then there cannot be flow
through the plane and the spatial derivatives of the velocity components normal to the
plane must vanish:
vy = 0,
∂vy
∂y
= 0.
This condition follows from the assumption that the plates are along streamlines and that
they are negligibly thin. It means that all the streamlines are parallel to the plates as well
as to the water surface, and so it must be that vy = 0 everywhere. The fluid enters in
the horizontal direction, the plates do not change the vertical y component of the velocity,
and the flow remains symmetric about the centerline. There is a retardation of the flow
around the plates due to the viscous nature of the flow and due to the v = 0 boundary
layers formed on the plates, but there are no actual vy components.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
450 CHAPTER 19
19.7.2 Analytic Solution for Parallel Plates
For steady flow around and through the parallel plates, with the boundary conditions imposed
and vy ≡ 0, the continuity equation (19.36) reduces to
∂vx
∂x
= 0. (19.39)
This tells us that vx does not vary with x. With these conditions, the Navier–Stokes equations
(19.38) in x and y reduce to the linear PDEs
∂P
∂x
= ρν
∂2vx
∂y2
,
∂P
∂y
= 0. (19.40)
(Observe that if the effect of gravity were also included in the problem, then the pressure would
increase with the depth y.) Since the LHS of the first equation describes the x variation,
and the RHS the y variation, the only way for the equation to be satisfied in general is if both
sides are constant:
∂P
∂x
= C, ρν
∂2vx
∂y2
= C. (19.41)
Double integration of the second equation with respect to y and replacement of the constant by
∂P/∂x yields
ρν
∂vx
∂y
=
∂P
∂x
y + C1, ⇒ ρνvx =
∂P
∂x
y2
2
+ C1y + C2,
where C1 and C2 are constants. The values of C1 and C2 are determined by requiring the fluid
to stop at the plate, vx(0) = vx(H) = 0, where H is the distance between plates. This yields
ρν vx(y) =
1
2
∂P
∂x
(
y2 − yH
)
. (19.42)
Because ∂P/∂y = 0, the pressure does not vary with y. The continuity and smoothness of P
over the region,
∂2P
∂x∂y
=
∂2P
∂y∂x
= 0, (19.43)
are a consequence of laminar flow. Such being the case, we may assume that ∂P/∂x has no y
dependence, and so (19.42) describes a velocity profile varying as y2.
A check on our numerical CFD simulation ensures that it also gives a parabolic velocity
profile for two parallel plates. To be even more precise, we can determine ∂P/∂x for this prob-
lem and thereby produce a purely numerical answer for comparison. To do that we examine
a volume of current that at the inlet starts out with 0 ≤ y ≤ H . Because there is no vertical
component to the flow, this volume ends up flowing between the plates. If the volume has a
unit z width, then the mass flow (mass/unit time) at the inlet is
Q(mass/time) = ρ× 1×H × dx
dt
= ρHvx = ρHV0. (19.44)
When the fluid is between the plates, the velocity has a parabolic variation in height y. Con-
sequently, we integrate over the area of the plates without changing the net mass flow between
the plates:
Q =
∫
ρvxdA = ρ
∫ H
0
vx(y) dy =
1
2ν
∂P
∂x
(
H3
3
− H
3
2
)
. (19.45)
Yet we know that Q = ρHV0, and substitution gives us an expression for how the pressure
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLITONS & COMPUTATIONAL FLUID DYNAMICS 451
gradient depends upon the plate separation:
∂P
∂x
= −12ρνV0
H2
. (19.46)
We see that there is a pressure drop as the fluid flows through the plates and that the drop
increases as the plates are brought closer together (the Bernoulli effect). To program the equa-
tions, we assign the values V0 = 1 m/s ('2.24 mi/h), ρ = 1 kg/m3 (≥air), ν = 1 m2/s (some-
what less viscous than glycerin), and H = 1 m (typical boulder size):
∂P
∂x
= −12 ⇒ vx = 6y(1− y). (19.47)
19.7.3 Finite-Difference Algorithm and Overrelaxation
Now we develop an algorithm for solution of the Navier–Stokes and continuity PDEs using
successive overrelaxation. This is a variation of the method used in Chapter 17, “PDEs for
Electrostatics & Heat Flow,” to solve Poisson’s equation. We divide space into a rectangular
grid with the spacing h in both the x and y directions:
x = ih, i = 0, . . . , Nx; y = jh, j = 0, . . . , Ny.
We next express the derivatives in (19.36)–(19.38) as finite differences of the values of the
velocities at the grid points using central-difference approximations. For ν = 1 m2/s and
ρ = 1 kg/m3, this yields
vxi+1,j − vxi−1,j + v
y
i,j+1 − v
y
i,j−1 = 0,
vxi+1,j + v
x
i−1,j + v
x
i,j+1 + v
x
i,j−1 − 4vxi,j
=
h
2
vxi,j
[
vxi+1,j − vxi−1,j
]
+
h
2
vyi,j
[
vxi,j+1 − vxi,j−1
]
+
h
2
[Pi+1,j − Pi−1,j ] ,
vyi+1,j + v
y
i−1,j + v
y
i,j+1 + v
y
i,j−1 − 4v
y
i,j
=
h
2
vxi,j
[
vyi+1,j − v
y
i−1,j
]
+
h
2
vyi,j
[
vyi,j+1 − v
y
i,j−1
]
+
h
2
[Pi,j+1 − Pi,j−1] .
Since vy ≡ 0 for this problem, we rearrange terms to obtain for vx:
4vxi,j = v
x
i+1,j + v
x
i−1,j + v
x
i,j+1 + v
x
i,j−1 −
h
2
vxi,j
[
vxi+1,j − vxi−1,j
]
−h
2
vyi,j
[
vxi,j+1 − vxi,j−1
]
− h
2
[Pi+1,j − Pi−1,j ] . (19.48)
We recognize in (19.48) an algorithm similar to the one we used in solving Laplace’s equa-
tion by relaxation. Indeed, as we did there, we can accelerate the convergence by writing the
algorithm with the new value of vx given as the old value plus a correction (residual):
vxi,j = v
x
i,j + ri,j , r
def= vx(new)i,j − v
x(old)
i,j (19.49)
⇒ r= 1
4
{
vxi+1,j + v
x
i−1,j + v
x
i,j+1 + v
x
i,j−1 −
h
2
vxi,j
[
vxi+1,j − vxi−1,j
]
− h
2
vyi,j
[
vxi,j+1 − vxi,j−1
]
− h
2
[Pi+1,j − Pi−1,j ]
}
− vxi,j . (19.50)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
452 CHAPTER 19
As done with the Poisson equation algorithm, successive iterations sweep the interior of the
grid, continuously adding in the residual (19.49) until the change becomes smaller than some
set level of tolerance, |ri,j | < ε.
A variation of this method, successive overrelaxation, increases the speed at which the
residuals approach zero via an amplifying factor ω:
vxi,j = v
x
i,j + ω ri,j (SOR). (19.51)
The standard relaxation algorithm (19.49) is obtained with ω = 1, an accelerated convergence
(overrelaxation) is obtained with ω ≥ 1, and underrelaxation occurs for ω < 1. Values ω > 2
lead to numerical instabilities and so are not recommended. Although a detailed analysis of
the algorithm is necessary to predict the optimal value for ω, we suggest that you test different
values for ω to see which one provides the fastest convergence for your problem.
19.7.4 Successive Overrelaxation Implementation
1. Modify the program Beam.py, or write your own, to solve the Navier–Stokes equation
for the velocity of a fluid in 2-D flow. Represent the x and y components of the velocity
by the arrays vx[Nx][Ny] and vy[Nx][Ny].
2. Specialize your solution to the rectangular domain and boundary conditions indicated in
Figure 19.5.
3. Use of the following parameter values,
ν= 1 m2/s, ρ = 1 kg/m3, (flow parameters),
Nx = 400, Ny = 40, h = 1, (grid parameters),
leads to the equations
∂P
∂x
= −12, ∂P
∂y
= 0, vx =
3j
20
(
1− j
40
)
, vy = 0.
4. For the relaxation method, output the iteration number and the computed vx and then
compare the analytic and numeric results.
5. Repeat the calculation and see if SOR speeds up the convergence.
19.8 2-D FLOW OVER A BEAM
Now that the comparison with an analytic solution has shown that our CFD simulation works,
we return to determining if the beam in Figure 19.4 might produce a good resting place for
salmon. While we have no analytic solution with which to compare, our canoeing and fishing
adventures have taught us that standing waves with fish in them are often formed behind rocks
in streams, and so we expect there will be a standing wave formed behind the beam.
19.9 THEORY: VORTICITY FORM OF NAVIER–STOKES EQUATION
We have seen how to solve numerically the hydrodynamics equations
~∇ · v = 0, (v · ~∇)v = −1
ρ
~∇P + ν∇2v. (19.52)
These equations determine the components of a fluid’s velocity, pressure, and density as func-
tions of position. In analogy to electrostatics, where one usually solves for the simpler scalar
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLITONS & COMPUTATIONAL FLUID DYNAMICS 453
potential and then takes its gradient to determine the more complicated vector field, we now
recast the hydrodynamic equations into forms that permit us to solve two simpler equations for
simpler functions, from which the velocity is obtained via a gradient operation.3
We introduce the stream function u(x) from which the velocity is determined by the curl
operator:
v def= ~∇× u(x) = ̂x
(
∂uz
∂y
− ∂uy
∂z
)
+ ̂y
(
∂ux
∂z
− ∂uz
∂x
)
. (19.53)
Note the absence of the z component of velocity v for our problem. Since ~∇· (~∇×u) ≡ 0, we
see that any v that can be written as the curl of u automatically satisfies the continuity equation
~∇ · v = 0. Further, since v for our problem has only x and y components, u(x) needs to have
only a z component:
uz ≡ u ⇒ vx =
∂u
∂y
, vy = −
∂u
∂x
. (19.54)
(Even though the vorticity has just one component, it is a pseudoscalar and not a scalar because
it reverses sign upon reflection.) It is worth noting that in 2-D flows, the contour lines u =
constant are the streamlines.
The second simplifying function is the vorticity field w(x), which is related physically
and alphabetically to the angular velocity ω of the fluid. Vorticity is defined as the curl of the
velocity (sometimes with a − sign):
w def= ~∇× v(x). (19.55)
Because the velocity in our problem does not change in the z direction, we have
wz =
(
∂vy
∂x
− ∂vx
∂y
)
. (19.56)
Physically, we see that the vorticity is a measure of how much the fluid’s velocity curls or
rotates, with the direction of the vorticity determined by the right-hand rule for rotations. In
fact, if we could pluck a small element of the fluid into space (so it would not feel the internal
strain of the fluid), we would find that it is rotating like a solid with angular velocity ω ∝ w
[Lamb 93]. That being the case, it is useful to think of the vorticity as giving the local value of
the fluid’s angular velocity vector. If w = 0, we have irrotational flow.
The field lines of w are continuous and move as if attached to the particles of the fluid. A
uniformly flowing fluid has vanishing curl, while a nonzero vorticity indicates that the current
curls back on itself or rotates. From the definition of the stream function (19.53), we see that
the vorticity w is related to it by
w = ~∇× v = ~∇× (~∇× u) = ~∇(~∇ · u)−∇2u, (19.57)
where we have used a vector identity for ~∇ × (~∇ × u). Yet the divergence ~∇ · u = 0 since
u has only a z component that does not vary with z (or because there is no source for u). We
have now obtained the basic relation between the stream function u and the vorticity w:
~∇2u = −w. (19.58)
Equation (19.58) is analogous to Poisson’s equation of electrostatics, ∇2φ = −4πρ, only now
each component of vorticity w is a source for the corresponding component of the stream
3If we had to solve only the simpler problem of irrotational flow (no turbulence), then we would be able to use a scalar velocity
potential, in close analogy to electrostatics [Lamb 93]. For the more general rotational flow, two vector potentials are required.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
454 CHAPTER 19
function u. If the flow is irrotational, that is, if w = 0, then we need only solve Laplace’s
equation for each component of u. Rotational flow, with its coupled nonlinearities equations,
leads to more interesting behavior.
As is to be expected from the definition of w, the vorticity form of the Navier–Stokes
equation is obtained by taking the curl of the velocity form, that is, by operating on both sides
with ~∇×. After significant manipulations we obtain
ν∇2w = [(~∇× u) · ~∇]w. (19.59)
This and (19.58) are the two simultaneous PDEs that we need to solve. In 2-D, with u and w
having only z components, they are
xmll ∂2u
∂x2
+
∂2u
∂y2
= −w, (19.60)
ν
(
∂2w
∂x2
+
∂2w
∂y2
)
=
∂u
∂y
∂w
∂x
− ∂u
∂x
∂w
∂y
. (19.61)
So after all that work, we end up with two simultaneous, nonlinear, elliptic PDEs for the func-
tions w(x, y) and u(x, y) that look like a mixture of Poisson’s equation with the frictional and
variable-density terms of the wave equation. The equation for u is Poisson’s equation with
source w and must be solved simultaneously with the second. It is this second equation that
contains mixed products of the derivatives of u and w and thus introduces nonlinearity.
19.9.1 Finite Differences and the SOR Algorithm
We solve (19.60) and (19.61) on an Nx ×Ny grid of uniform spacing h with
x = i∆x = ih, i = 0, . . . , Nx, y = j∆y = jh, j = 0, . . . , Ny.
Since the beam is symmetric about its centerline (Figure 19.4 left), we need the solution only
in the upper half-plane. We apply the now familiar central-difference approximation to the
Laplacians of u and w to obtain the difference equation
∂2u
∂x2
+
∂2u
∂y2
' ui+1,j + ui−1,j + ui,j+1 + ui,j−1 − 4ui,j
h2
. (19.62)
Likewise, for the first derivatives,
∂u
∂y
∂w
∂x
' ui,j+1 − ui,j−1
2h
wi+1,j − wi−1,j
2h
. (19.63)
The difference form of the vorticity Navier–Stokes equation (19.60) becomes
ui,j =
1
4
(
ui+1,j + ui−1,j + ui,j+1 + ui,j−1 + h2wi,j
)
, (19.64)
wi,j =
1
4
(wi+1,j + wi−1,j + wi,j+1 + wi,j−1)−
R
16
{[ui,j+1 − ui,j−1]
× [wi+1,j − wi−1,j ]− [ui+1,j − ui−1,j ] [wi,j+1 − wi,j−1]} , (19.65)
R=
1
ν
=
V0h
ν
(in normal units). (19.66)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLITONS & COMPUTATIONAL FLUID DYNAMICS 455
Note that we have placed ui,j and wi,j on the LHS of the equations in order to obtain an
algorithm appropriate to solution by relaxation.
The parameter R in (19.66) is related to the Reynolds number. When we solve the
problem in natural units, we measure distances in units of grid spacing h, velocities in units of
initial velocity V0, stream functions in units of V0h, and vorticity in units of V0/h. The second
form is in regular units and is dimensionless. This R is known as the grid Reynolds number
and differs from the physical R, which has a pipe diameter in place of the grid spacing h.
The grid Reynolds number is a measure of the strength of the coupling of the nonlinear
terms in the equation. When the physical R is small, the viscosity acts as a frictional force
that damps out fluctuations and keeps the flow smooth. When R is large (R ' 2000), physical
fluids undergo phase transitions from laminar to turbulent flow in which turbulence occurs
at a cascading set of smaller and smaller space scales [Rey 83]. However, simulations that
produce the onset of turbulence have been a research problem since Reynolds first experiments
in 1883 [Rey 83, F&S], possibly because laminar flow is stable against small perturbations and
some large-scale “kick” appears necessary to change laminar to turbulent flow. Recent research
along these lines have been able to find unstable, traveling-wave solutions to the Navier–Stokes
equations, and the hope is that these may lead to a turbulent transition [Fitz 04].
As discussed in §19.7.3, the finite-difference algorithm can have its convergence accel-
erated by the use of successive overrelaxation (19.64):
ui,j = ui,j + ω r
(1)
i,j , wi,j = wi,j + ω r
(2)
i,j (SOR). (19.67)
Here ω is the overrelaxation parameter and should lie in the range 0 < ω < 2 for stability. The
residuals are just the changes in a single step, r(1) = unew − uold and r(2) = wnew − wold:
r
(1)
i,j =
1
4
(ui+1,j + ui−1,j + ui,j+1 + ui,j−1 + wi,j)− ui,j , (19.68)
r
(2)
i,j =
1
4
(
wi+1,j + wi−1,j + wi,j+1 + wi,j−1 −
R
4
{[ui,j+1 − ui,j−1]
× [wi+1,j − wi−1,j ]− [ui+1,j − ui−1,j ] [wi,j+1 − wi,j−1]}
)
− wi,j .
19.9.2 Boundary Conditions for a Beam
A well-defined solution of these elliptic PDEs requires a combination of (less than obvious)
boundary conditions on u and w. Consider Figure 19.6, based on the analysis of [Koon 86].
We assume that the inlet, outlet, and surface are far from the beam, which may not be evident
from the not-to-scale figure.
Freeflow: If there were no beam in the stream, then we would have free flow with the entire
fluid possessing the inlet velocity:
vx ≡ V0, vy = 0, ⇒ u = V0y, w = 0. (19.69)
(Recollect that we can think of w = 0 as indicating no fluid rotation.) The centerline
divides the system along a symmetry plane with identical flow above and below it. If the
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
456 CHAPTER 19
Figure 19.6 Boundary conditions for flow around the beam in Figure 19.4. The flow is symmetric about the
centerline, and the beam has length L in the x direction (along flow).
O
u
tl
e
t
dw/dx = 0
du/dx = 0vx = du/dy = V0
w = 0
In
le
t
Half
Beam
Surface
vx = du/dy = V0 w = 0
y
x
vy = -du/dx = 0
center line
w = u = 0 w = u = 0
u = 0
u = 0
vy = -du/dx = 0
A
B
C
E
F
G
H
D
velocity is symmetric about the centerline, then its y component must vanish there:
vy = 0, ⇒
∂u
∂x
= 0 (centerline AE). (19.70)
Centerline: The centerline is a streamline with u = constant because there is no velocity
component perpendicular to it. We set u = 0 according to (19.69). Because there cannot
be any fluid flowing into or out of the beam, the normal component of velocity must vanish
along the beam surfaces. Consequently, the streamline u = 0 is the entire lower part of
Figure 19.6, that is, the centerline and the beam surfaces. Likewise, the symmetry of the
problem permits us to set the vorticity w = 0 along the centerline.
Inlet: At the inlet, the fluid flow is horizontal with uniform x component V0 at all heights
and with no rotation:
vy = −
∂u
∂x
= 0, w = 0 (inlet F), vx =
∂u
∂y
= V0. (19.71)
Surface: We are told that the beam is sufficiently submerged so as not to disturb the flow on
the surface of the stream. Accordingly, we have free-flow conditions on the surface:
vx =
∂u
∂y
= V0, w = 0 (surface G). (19.72)
Outlet: Unless something truly drastic is happening, the conditions on the far downstream
outlet have little effect on the far upstream flow. A convenient choice is to require the
stream function and vorticity to be constant:
∂u
∂x
=
∂w
∂x
= 0 (outlet H). (19.73)
Beamsides: We have already noted that the normal component of velocity vx and stream
function u vanish along the beam surfaces. In addition, because the flow is viscous, it is
also true that the fluid “sticks” to the beam somewhat and so the tangential velocity also
vanishes along the beam’s surfaces. While these may all be true conclusions regarding
the flow, specifying them as boundary conditions would overrestrict the solution (see Ta-
ble 17.1 for elliptic equations) to the point where no solution may exist. Accordingly, we
simply impose the no-slip boundary condition on the vorticity w. Consider a grid point
(x, y) on the upper surface of the beam. The stream function u at a point (x, y+h) above
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLITONS & COMPUTATIONAL FLUID DYNAMICS 457
it can be related via a Taylor series in y:
u(x, y + h) = u(x, y) +
∂u
∂y
(x, y)h+
∂2u
∂y2
(x, y)
h2
2
+ · · · . (19.74)
Because w has only a z component, it has a simple relation to ∇× v:
w ≡ wz =
∂vy
∂x
− ∂vx
∂y
. (19.75)
Because of the fluid’s viscosity, the velocity is stationary along the beam top:
vx =
∂u
∂y
= 0 (beam top). (19.76)
Because the current flows smoothly along the top of the beam, vy must also vanish. In
addition, since there is no x variation, we have
∂vy
∂x
= 0 ⇒ w = −∂vx
∂y
= −∂
2u
∂y2
. (19.77)
After substituting these relations into the Taylor series (19.74), we can solve for w and
obtain the finite-difference version of the top boundary condition:
w ' −2u(x, y + h)− u(x, y)
h2
⇒ wi,j = −2
ui,j+1 − ui,j
h2
(top). (19.78)
Similar treatments applied to other surfaces yield the following boundary conditions.
u = 0; w = 0 Centerline EA
u = 0, wi,j = −2(ui+1,j − ui,j)/h2 Beam back B
u = 0, wi,j = −2(ui,j+1 − ui,j)/h2 Beam top C
u = 0, wi,j = −2(ui−1,j − ui,j)/h2 Beam front D
∂u/∂x = 0, w = 0 Inlet F
∂u/∂y = V0, w = 0 Surface G
∂u/∂x = 0, ∂w/∂x = 0 Outlet H
19.9.3 SOR on a Grid Implementation
Beam.py in Listing 19.3 is our solution of the vorticity form of the Navier–Stokes equation.
You will notice that while the relaxation algorithm is rather simple, some care is needed in
implementing the many boundary conditions. Relaxation of the stream function and of the
vorticity is done by separate methods, and the file output format is that for a Gnuplot surface
plot.
19.9.4 Assessment
1. Use Beam.py as a basis for your solution for the stream function u and the vorticity w
using the finite-differences algorithm (19.64).
2. A good place to start your simulation is with a beam of size L = 8h, H = h, Reynolds
number R = 0.1, and intake velocity V0 = 1. Keep your grid small during debugging,
say, Nx = 24 and Ny = 70.
3. Explore the convergence of the algorithm.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
458 CHAPTER 19
Figure 19.7 Left: Gnuplot surface plot with contours of the stream function u for R = 5. Right: Contours of stream
function for R = 1 visualized with colors by OpenDX.
Figure 19.8 Left: The velocity field around the beam as represented by vectors. Right: The vorticity as a function
of x and y. Rotation is seen to be largest behind the beam.
x
y
0
12
6
40 80
w(x,y)
x
y
0
-1
0
50 0
20
a. Print out the iteration number and u values upstream from, above, and downstream
from the beam.
b. Determine the number of iterations necessary to obtain three-place convergence for
successive relaxation (ω = 0).
c. Determine the number of iterations necessary to obtain three-place convergence for
successive overrelaxation (ω ' 0.3). Use this number for future calculations.
4. Change the beam’s horizontal placement so that you can see the undisturbed current en-
tering on the left and then developing into a standing wave. Note that you may need to
increase the size of your simulation volume to see the effect of all the boundary condi-
tions.
5. Make surface plots including contours of the stream function u and the vorticity w.
Explain the behavior seen.
6. Is there a region where a big fish can rest behind the beam?
7. The results of the simulation (Figure 19.7) are for the one-component stream function u.
Make several visualizations showing the fluid velocity throughout the simulation region.
Note that the velocity is a vector with two components (or a magnitude and direction),
and both degrees of freedom are interesting to visualize. A plot of vectors would work
well here (Gnuplot and OpenDX make vector plots for this purpose, Mathematica has
plotfield, and Maple has fieldplot, although the latter two require some work for numeri-
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
SOLITONS & COMPUTATIONAL FLUID DYNAMICS 459
cal data).
8. Explore how increasing the Reynolds numberR changes the flow pattern. Start atR = 0
and gradually increase R while watching for numeric instabilities. To overcome the in-
stabilities, reduce the size of the relaxation parameter ω and continue to larger R values.
9. Verify that the flow around the beam is smooth for small R values, but that it separates
from the back edge for large R, at which point a small vortex develops.
19.9.5 Exploration
1. Determine the flow behind a circular rock in the stream.
2. The boundary condition at an outlet far downstream should not have much effect on the
simulation. Explore the use of other boundary conditions there.
3. Determine the pressure variation around the beam.
Listing 19.3 Beam.py solves the Navier–Stokes equation for the flow over a plate. 
# Beam . py : s o l v e s Na v i e r − S t o k e s e q u a t i o n f o r f low around beam , s e c t i o n s 1 9 . 9
from numpy i m p o r t ∗ ; i m p o r t p y l a b as p ; i m p o r t m a t p l o t l i b . axes3d as p3
p r i n t "Working, wait for the figure after 100 iterations"
Nxmax = 7 0 ; Nymax = 2 0 ; IL = 1 0 ; H = 8 ; T = 8 ; h = 1 .
u = z e r o s ( ( Nxmax + 1 , Nymax + 1) , f l o a t ) # St ream
w = z e r o s ( ( Nxmax + 1 , Nymax + 1) , f l o a t ) # V o r t i c i t y
V0 = 1 . 0 ; omega = 0 . 1 ; nu = 1 . ; i t e r = 0 ; R = V0 ∗ h / nu # Renold #
d e f b o r d e r s ( ) : # Method b o r d e r s : i n i t & B . C .
f o r i i n r a n g e ( 0 , Nxmax + 1) : # I n i t i a l i z e s t r e a m f u n c t i o n
f o r j i n r a n g e ( 0 , Nymax + 1 ) : # And v o r t i c i t y
w[ i , j ] = 0 .
u [ i , j ] = j ∗ V0
f o r i i n r a n g e ( 0 , Nxmax + 1 ) : # F l u i d s u r f a c e
u [ i , Nymax ] = u [ i , Nymax − 1] + V0 ∗ h
w[ i , Nymax − 1] = 0 .
f o r j i n r a n g e ( 0 , Nymax + 1 ) :
u [ 1 , j ] = u [ 0 , j ]
w[ 0 , j ] = 0 . # I n l e t
f o r i i n r a n g e ( 0 , Nxmax + 1 ) : # C e n t e r l i n e
i f i <= IL and i>= IL + T :
u [ i , 0 ] = 0 .
w[ i , 0 ] = 0 .
f o r j i n r a n g e ( 1 , Nymax ) : # O u t l e t
w[ Nxmax , j ] = w[ Nxmax − 1 , j ]
u [ Nxmax , j ] = u [ Nxmax − 1 , j ] # Boundary c o n d i t i o n s
d e f beam ( ) : # Method beam ; BC f o r beam
f o r j i n r a n g e ( 0 , H + 1) : # Beam s i d e s
w[ IL , j ] = − 2 ∗ u [ IL − 1 , j ] / ( h∗h ) # F r o n t s i d e
w[ IL + T , j ] = − 2 ∗ u [ IL + T + 1 , j ] / ( h∗h ) # Back s i d e
f o r i i n r a n g e ( IL , IL + T + 1) : w[ i , H − 1] = − 2 ∗ u [ i , H ] / ( h∗h ) ; # Top
f o r i i n r a n g e ( IL , IL + T + 1 ) :
f o r j i n r a n g e ( 0 , H + 1) :
u [ IL , j ] = 0 . # F r o n t
u [ IL+T , j ] = 0 . # Back
u [ i , H] = 0 ; # t o p
d e f r e l a x ( ) : # Method t o r e l a x s t r e a m
beam ( ) # R e s e t c o n d i t i o n s a t beam
f o r i i n r a n g e ( 1 , Nxmax ) : # Relax s t r e a m f u n c t i o n
f o r j i n r a n g e ( 1 , Nymax ) :
r1 = omega ∗ ( ( u [ i +1 , j ]+ u [ i−1, j ]+ u [ i , j +1]+ u [ i , j−1] + h∗h∗w[ i , j ] ) ∗0.25−u [ i , j ] )
u [ i , j ] += r1
f o r i i n r a n g e ( 1 , Nxmax ) : # Relax v o r t i c i t y
f o r j i n r a n g e ( 1 , Nymax ) :
a1 = w[ i +1 , j ] + w[ i−1, j ] + w[ i , j +1] + w[ i , j−1]
a2 = ( u [ i , j +1] − u [ i , j −1])∗(w[ i +1 , j ] − w[ i − 1 , j ] )
a3 = ( u [ i +1 , j ] − u [ i−1, j ] ) ∗(w[ i , j +1] − w[ i , j − 1 ] )
r2 = omega ∗( ( a1 − (R / 4 . ) ∗( a2 − a3 ) ) / 4 . 0 − w[ i , j ] )
w[ i , j ] += r2
b o r d e r s ( )
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
460 CHAPTER 19
w h i l e ( i t e r <= 100) :
i t e r += 1
i f i t e r %10 == 0 : p r i n t i t e r
r e l a x ( )
f o r i i n r a n g e ( 0 , Nxmax + 1) :
f o r j i n r a n g e ( 0 , Nymax + 1 ) : u [ i , j ] = u [ i , j ] / ( V0∗h ) # V0h u n i t s
x = r a n g e ( 0 , Nxmax − 1) ; y = r a n g e ( 0 , Nymax − 1)
X, Y = p . meshgr id ( x , y )
d e f f u n c t z ( u ) : # r e t u r n s s t r e a m f low t o p l o t
z = u [X, Y] # f o r s e v e r a l i t e r a t i o n s
r e t u r n z
Z = f u n c t z ( u ) # h e r e t h e f u n c t i o n i s c a l l e d
f i g = p . f i g u r e ( ) # c r e a t e s t h e f i g u r e
ax = p3 . Axes3D ( f i g ) # p l o t s t h e a x i s f o r t h e f i g u r e
ax . p l o t w i r e f r a m e (X, Y, Z , c o l o r = ’r’ ) # t o s e e s u r f a c e o f w i r e f r a m e i n r e d
ax . s e t x l a b e l (’X’ ) # l a b e l t h e t h r e e axes
ax . s e t y l a b e l (’Y’ )
ax . s e t z l a b e l (’Stream Function’ )
p . show ( ) # show f i g u r e & c l o s e Python s h e l l
p r i n t "finished"
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Chapter Twenty
Integral Equations in Quantum Mechanics
The power and accessibility of high-speed computers have changed the view as to what kind
of equations are soluble. In Chapter 9, “Differential Equation Applications,” and Chapter
12, “Discrete & Continuous Nonlinear Dynamics,” we saw how even nonlinear differential
equations can be solved easily and can give new insight into the physical world. In this chapter
we examine how the integral equations of quantum mechanics can be solved for both bound
and scattering states. In Unit I we extend our treatment of the eigenvalue problem, solved as a
coordinate-space differential equation in Chapter 9, to the equivalent integral-equation problem
in momentum space. In Unit II we treat the singular integral equations for scattering, a more
difficult problem. After studying this chapter, we hope that the reader will view both integral
and differential equations as soluble.
VIDEO LECTURES, APPLETS AND ANIMATIONS FOR THIS CHAPTER
Lectures
Name Sections Name Sections
- - - *
20.1 UNIT I. BOUND STATES OF NONLOCAL POTENTIALS
Problem: A particle undergoes a many-body interaction with a medium (Figure 20.1) that
results in the particle experiencing an effective potential at r that depends on the wave function
at the r′ values of the other particles [L 96]:
V (r)ψ(r)→
∫
dr′ V (r, r′)ψ(r′). (20.1)
This type of interaction is called nonlocal and leads to a Schrödinger equation that is a com-
bined integral and differential (“integrodifferential”) equation:
− 1
2µ
d2ψ(r)
dr2
+
∫
dr′ V (r, r′)ψ(r′) = Eψ(r). (20.2)
Your problem is to figure out how to find the bound-state energies E and wave functions ψ for
the integral equation in (20.2).1
1We use natural units in which h̄ ≡ 1 and omit the traditional bound-state subscript n onE and ψ in order to keep the notation
simpler.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
462 CHAPTER 20
Figure 20.1 A dark particle moving in a dense multiparticle medium. The nonlocality of the potential felt by the
dark particle at r arises from the particle interactions at all r′.
r r’
20.2 MOMENTUM-SPACE SCHRÖDINGER EQUATION (THEORY)
One way of dealing with equation (20.2) is by going to momentum space where it becomes the
integral equation [L 96]
k2
2µ
ψ(k) +
2
π
∫ ∞
0
dpp2V (k, p)ψ(p) = Eψ(k), (20.3)
where we restrict our solution to l = 0 partial waves. In (20.3), V (k, p) is the momentum-
xmll
space representation (double Fourier transform) of the potential,
V (k, p) =
1
kp
∫ ∞
0
dr sin(kr)V (r) sin(pr), (20.4)
and ψ(k) is the (unnormalized) momentum-space wave function (the probability amplitude for
finding the particle with momentum k),
ψ(k) =
∫ ∞
0
drkrψ(r) sin(kr). (20.5)
Equation (20.3) is an integral equation for ψ(k), in contrast to an integral representation of
ψ(k), because the integral in it cannot be evaluated until ψ(p) is known. Although this may
seem like an insurmountable barrier, we will transform this equation into a matrix equation
that can be solved with the matrix techniques discussed in Chapter 8, “Solving Systems of
Equations with Matrices; Data Fitting.”
20.2.1 Integral to Linear Equations (Method)
We approximate the integral over the potential as a weighted sum over N integration points
(usually Gauss quadrature2) for p = kj , j = 1, N :∫ ∞
0
dpp2V (k, p)ψ(p) '
N∑
j=1
wjk
2
jV (k, kj)ψ(kj). (20.6)
This converts the integral equation (20.3) to the algebraic equation
k2
2µ
ψ(k) +
2
π
N∑
j=1
wjk
2
jV (k, kj)ψ(kj) = E. (20.7)
2See Chapter 6, “Integration,” for a discussion of numerical integration.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
INTEGRAL EQUATIONS IN QUANTUM MECHANICS 463
Figure 20.2 The grid of momentum values on which the integral equation is solved.
Nk3k2k1k
Equation (20.7) contains the N unknowns ψ(kj), the single unknown E, and the unknown
function ψ(k). We eliminate the need to know the entire function ψ(k) by restricting the
solution to the same values of ki as used to approximate the integral. This leads to a set of N
coupled linear equations in (N + 1) unknowns:
k2i
2µ
ψ(ki) +
2
π
N∑
j=1
wjk
2
j V (ki, kj)ψ(kj) = Eψ(ki), i = 1, N. (20.8)
As a case in point, if N = 2, we would have the two simultaneous linear equations
k21
2µ
ψ(k1) +
2
π
w1k
2
1 V (k1, k1)ψ(k1) + w2k
2
2 V (k1, k2) =Eψ(k1),
k22
2µ
ψ(k2) +
2
π
w1k
2
1 V (k2, k1)ψ(k1) + w2k
2
2 V (k2, k2)ψ(k2) =Eψ(k2).
We write our coupled dynamic equations in matrix form as
[H][ψ] = E[ψ] (20.9)
or as explicit matrices
xmll

k21
2µ +
2
πV (k1, k1)k
2
1w1
2
πV (k1, k2)k
2
2w2 · · · 2πV (k1, kN )k
2
NwN
2
πV (k2, k1)k
2
1w1
2
πV (k2, k2)k
2
2w2 +
k22
2µ · · ·
. . .
· · · · · · · · · k
2
N
2µ +
2
πV (kN , kN )k
2
NwN

×

ψ(k1)
ψ(k2)
. . .
ψ(kN )
 = E

ψ(k1)
ψ(k2)
. . .
ψ(kN )
 . (20.10)
Equation (20.9) is the matrix representation of the Schrödinger equation (20.3). The wave
function ψ(k) on the grid is the N × 1 vector
[ψ(ki)] =

ψ(k1)
ψ(k2)
. . .
ψ(kN )
 . (20.11)
The astute reader may be questioning the possibility of solving N equations for (N + 1)
unknowns. That reader is wise; only sometimes, and only for certain values of E (eigenvalues)
will the computer be able to find solutions. To see how this arises, we try to apply the matrix
inversion technique (which we will use successfully for scattering in Unit II). We rewrite (20.9)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
464 CHAPTER 20
as
[H − EI][ψ] = [0] (20.12)
and multiply both sides by the inverse of [H − EI] to obtain the formal solution
[ψ] = [H − EI]−1[0]. (20.13)
This equation tells us that (1) if the inverse exists, then we have the trivial solution ψ ≡ 0,
which is not very interesting, and (2) for a nontrivial solution to exist, our assumption that
the inverse exists must be incorrect. Yet we know from the theory of linear equations that the
inverse fails to exist when the determinant vanishes:
det[H − EI] = 0 (bound-state condition). (20.14)
Equation (20.14) is the additional equation needed to find unique solutions to the eigenvalue
problem. Nevertheless, there is no guarantee that solutions of (20.14) can always be found, but
if they are found, they are the desired eigenvalues of (20.9).
20.2.2 Delta-Shell Potential (Model)
To keep things simple and to have an analytic answer to compare with, we consider the local
delta-shell potential:
V (r) =
λ
2µ
δ(r − b). (20.15)
This would be a good model for an interaction that occurs in 3-D when two particles are pre-
dominantly a fixed distance b apart. We use (20.4) to determine its momentum-space represen-
tation:
V (k′, k) =
1
k′k
∫ ∞
0
sin(k′r′)
λ
2µ
δ(r − b) sin(kr) dr = λ
2µ
sin(k′b) sin(kb)
k′k
. (20.16)
Beware: We have chosen this potential because it is easy to evaluate the momentum-space
matrix element of the potential. However, its singular nature in r space leads to a very slow
falloff in k space, and this causes the integrals to converge so slowly that numerics are not as
precise as we would like.
If the energy is parameterized in terms of a wave vector κ by E = −κ2/2µ, then for this
potential there is, at most, one bound state and it satisfies the transcendental equation [Gott 66]
e−2κb − 1 = 2κ
λ
. (20.17)
Note that bound states occur only for attractive potentials and only if the attraction is strong
enough. For the present case this means that we must have λ < 0.
Exercise: Pick some values of b and λ and use them to verify with a hand calculation that
(20.17) can be solved for κ.
20.2.3 Binding Energies Implementation
An actual computation may follow two paths. The first calls subroutines to evaluate the de-
terminant of the [H − EI] matrix in (20.14) and then to search for those values of energy
for which the computed determinant vanishes. This provides E, but not wave functions. The
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
INTEGRAL EQUATIONS IN QUANTUM MECHANICS 465
other approach calls an eigenproblem solver that may give some or all eigenvalues and eigen-
functions. In both cases the solution is obtained iteratively, and you may be required to guess
starting values for both the eigenvalues and eigenvectors. In Listing 20.1 we present our solu-
tion of the integral equation for bound states of the delta-shell potential using the JAMA matrix
library and the gauss method for Gaussian quadrature points and weights.
Listing 20.1 Bound.py solves the Lippmann–Schwinger integral equation for bound states within a delta-shell po-
tential. The integral equations are converted to matrix equations using Gaussian grid points, and they
are solved with LINEARALGEBRA. 
#Bound . py : Bound s t a t e s o l u t n o f Lippmann−Schwinger e q u a t i o n i n p s p a c e
from v i s u a l i m p o r t ∗ ; from Numeric i m p o r t ∗ ; from L i n e a r A l g e b r a i m p o r t∗
min1 = 0 . ; max1 = 2 0 0 . ; u = 0 . 5 ; b =10 .
d e f g a u s s ( np t s , a , b , x ,w) :
pp = 0 . ; m = ( n p t s + 1) / 2 ; eps = 3 . E−10 # Accuracy : ADJUST!
f o r i i n x r an g e ( 1 ,m+1) :
t = cos ( math . p i ∗( f l o a t ( i ) −0.25) / ( f l o a t ( n p t s ) + 0 . 5 ) )
t 1 = 1
w h i l e ( ( abs ( t−t 1 ) ) >= eps ) :
p1 = 1 . ; p2 = 0 . ;
f o r j i n x r an g e ( 1 , n p t s +1) :
p3 = p2
p2 = p1
p1 =((2∗ j−1)∗ t∗p2−( j−1)∗p3 ) / j
pp = n p t s ∗( t∗p1−p2 ) / ( t∗ t −1.)
t 1 = t ; t = t 1 − p1 / pp
x [ i−1] = −t
x [ np t s−i ] = t
w[ i−1] = 2 . / ( ( 1 . − t∗ t )∗pp∗pp )
w[ np t s−i ] = w[ i−1]
f o r i i n x r an g e ( 0 , n p t s ) :
x [ i ] = x [ i ]∗ ( b−a ) / 2 . + ( b + a ) / 2 .
w[ i ] = w[ i ]∗ ( b−a ) / 2 .
f o r M i n x ra ng e ( 1 6 , 129 , 8 ) :
z =[−1024 , −512, −256, −128, −64, −32, −16, −8, −4, −2]
f o r lmbda i n z :
A = z e r o s ( (M,M) , F l o a t ) # H a m i l t o n i a n
WR = z e r o s ( (M) , F l o a t ) # E i g e n v a l u e s , p o t e n t i a l
k = z e r o s ( (M) , F l o a t ) ; w = z e r o s ( (M) , F l o a t ) ; # P t s & wts
g a u s s (M, min1 , max1 , k , w) # C a l l g a u s s p o i n t s
f o r i i n x r an g e ( 0 ,M) :
# S e t H a m i l t o n i a n
f o r j i n x r an g e ( 0 ,M) :
VR = lmbda / 2 / u∗ s i n ( k [ i ]∗b ) / k [ i ]∗ s i n ( k [ j ]∗b ) / k [ j ]
A[ i , j ] = 2 . / math . p i∗VR∗k [ j ]∗k [ j ]∗w[ j ]
i f ( i == j ) :
A[ i , j ] += k [ i ]∗k [ i ] / 2 / u
ev , e v e c t o r s = e i g e n v e c t o r s (A)
r e a l e v = ev . r e a l # Rea l e i g e n v a l u e s
f o r j i n x r an g e ( 0 ,M) :
i f ( r e a l e v [ j ]<0) :
p r i n t " M (size), lmbda, ReE=" ,M," " , lmbda ," " , r e a l e v [ j ]
break
p r i n t "Press a character to finish"
s= r a w i n p u t ( )
1. Write your own program, or modify the code on the CD, to solve the integral equation
(20.9) for the delta-shell potential (20.16). Either evaluate the determinant of [H − EI]
and then find the E for which the determinant vanishes or find the eigenvalues and
eigenvectors for this H .
2. Set the scale by setting 2µ = 1 and b = 10.
3. Set up the potential and Hamiltonian matrices V (i, j) and H(i, j) for Gaussian quadra-
ture integration with at least N = 16 grid points.
4. Adjust the value and sign of λ for bound states. A good approach is to start with a large
negative value for λ and then make it less negative. You should find that the eigenvalue
moves up in energy.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
466 CHAPTER 20
Figure 20.3 Left: A projectile of momentum k (dark particle at r) scattering from a dense medium. Right: The same
process viewed in the CM system, where the k’s are CM momenta.
2
r r’
k
k’
-k
-k’
m1
m2
5. Try increasing the number of grid points in steps of 8, for example, 16, 24, 32, 64, . . .,
and see how the energy changes.
6. Note: Your eigenenergy solver may return several eigenenergies. The true bound state
will be at negative energy and will be stable as the number of grid points changes. The
others are numerical artifacts.
7. Extract the best value for the bound-state energy and estimate its precision by seeing
how it changes with the number of grid points.
8. Check your solution by comparing the RHS and LHS in the matrix multiplication
[H][ψ] = E[ψ].
9. Verify that, regardless of the potential’s strength, there is only a single bound state and
that it gets deeper as the magnitude of λ increases.
20.2.4 Wave Function (Exploration)
1. Determine the momentum-space wave function ψ(k). Does it fall off at k → ∞? Does
it oscillate? Is it well behaved at the origin?
2. Determine the coordinate-space wave function via the Bessel transform
ψ(r) =
∫ ∞
0
dkψ(k)
sin(kr)
kr
k2. (20.18)
Does ψ0(r) fall off as you would expect for a bound state? Does it oscillate? Is it well
behaved at the origin?
3. Compare the r dependence of this ψ0(r) to the analytic wave function:
ψ0(r) ∝
{
e−κr − eκr, for r < b,
e−κr, for r > b.
(20.19)
20.3 UNIT II. NONLOCAL POTENTIAL SCATTERING 
Problem: Again we have a particle interacting with the nonlocal potential discussed in Unit
I (Figure 20.3 left), only now the particle has sufficiently high energy that it scatters rather
than binds with the medium. Your problem is to determine the scattering cross section for
scattering from a nonlocal potential.
20.4 LIPPMANN–SCHWINGER EQUATION (THEORY)
Because experiments measure scattering amplitudes and not wave functions, it is more direct
to have a theory dealing with amplitudes rather than wave functions. An integral form of
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
INTEGRAL EQUATIONS IN QUANTUM MECHANICS 467
the Schrödinger equation dealing with the reaction amplitude or R matrix is the Lippmann–
Schwinger equation:
R(k′, k) = V (k′, k) +
2
π
P
∫ ∞
0
dp
p2V (k′, p)R(p, k)
(k20 − p2)/2µ
. (20.20)
As in Unit I, the equations are for partial wave l = 0 and h̄ = 1. In (20.20) the momentum k0
xmll
is related to the energy E and the reduced mass µ by
E =
k20
2µ
, µ =
m1m2
m1 +m2
, (20.21)
and the initial and final COM momenta k and k′ are the momentum-space variables. The
experimental observable that results from a solution of (20.20) is the diagonal matrix element
R(k0, k0), which is related to the scattering phase shift δ0 and thus the cross section:
xmllR(k0, k0) = −
tan δl
ρ
, ρ = 2µk0. (20.22)
Note that (20.20) is not just the evaluation of an integral; it is an integral equation in which
R(p, k) is integrated over all p, yet since R(p, k) is unknown, the integral cannot be evaluated
until after the equation is solved! The symbolP in (20.20) indicates the Cauchy principal-value
prescription for avoiding the singularity arising from the zero of the denominator (we discuss
how to do that next).
20.4.1 Singular Integrals (Math)
A singular integral
G =
∫ b
a
g(k) dk, (20.23)
is one in which the integrand g(k) is singular at a point k0 within the interval [a, b] and yet the
integral G is still finite. (If the integral itself were infinite, we could not compute it.) Unfor-
tunately, computers are notoriously bad at dealing with infinite numbers, and if an integration
point gets too near the singularity, overwhelming subtractive cancellation or overflow may oc-
cur. Consequently, we apply some results from complex analysis before evaluating singular
integrals numerically.3
In Figure 20.4 we show three ways in which the singularity of an integrand can be
avoided. The paths in Figures 20.4A and 20.4B move the singularity slightly off the real
k axis by giving the singularity a small imaginary part ±i. The Cauchy principal-value pre-
scription P (Figure 20.4C) is seen to follow a path that “pinches” both sides of the singularity
at k0 but does not to pass through it:
P
∫ +∞
−∞
f(k) dk = lim
→0
[∫ k0−
−∞
f(k) dk +
∫ +∞
k0+
f(k) dk
]
. (20.24)
The preceding three prescriptions are related by the identity∫ +∞
−∞
f(k) dk
k − k0 ± i
= P
∫ +∞
−∞
f(k) dk′
k − k0
∓ iπf(k0), (20.25)
which follows from Cauchy’s residue theorem.
3 [S&T 93] describe a different approach using Maple and Mathematica.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
468 CHAPTER 20
Figure 20.4 Three different paths in the complex k′ plane used to evaluate line integrals when there are singularities.
Here the singularities are at k and −k, and the integration variable is k′.
20.4.2 Numerical Principal Values
A numerical evaluation of the principal value limit (20.24) is awkward because computers have
limited precision. A better algorithm follows from the theorem
P
∫ +∞
−∞
dk
k − k0
= 0. (20.26)
This equation says that the curve of 1/(k− k0) as a function of k has equal and opposite areas
on both sides of the singular point k0. If we break the integral up into one over positive k and
one over negative k, a change of variable k → −k permits us to rewrite (20.26) as
P
∫ +∞
0
dk
k2 − k20
= 0. (20.27)
We observe that the principal-value exclusion of the singular point’s contribution to the integral
is equivalent to a simple subtraction of the zero integral (20.27):
P
∫ +∞
0
f(k) dk
k2 − k20
=
∫ +∞
0
[f(k)− f(k0)] dk
k2 − k20
. (20.28)
Notice that there is no P on the RHS of (20.28) because the integrand is no longer singular at
k = k0 (it is proportional to the df/dk) and can therefore be evaluated numerically using the
usual rules. The integral (20.28) is called the Hilbert transform of f and also arises in inverse
problems.
20.4.3 Reducing Integral Equations to Matrix Equations (Method)
Now that we can handle singular integrals, we can go about reducing the integral equation
(20.20) to a set of linear equations that can be solved with matrix methods. We start by rewriting
the principal-value prescription as a definite integral [H&T 70]:
xmll R(k′, k) = V (k′, k) + 2
π
∫ ∞
0
dp
p2V (k′, p)R(p, k)− k20V (k′, k0)R(k0, k)
(k20 − p2)/2µ
. (20.29)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
INTEGRAL EQUATIONS IN QUANTUM MECHANICS 469
We convert this integral equation to linear equations by approximating the integral as a sum
over N integration points (usually Gaussian) kj with weights wj :
R(k, k0)'V (k, k0) +
2
π
N∑
j=1
k2jV (k, kj)R(kj , k0)wj
(k20 − k2j )/2µ
− 2
π
k20V (k, k0)R(k0, k0)
N∑
m=1
wm
(k20 − k2m)/2µ
. (20.30)
We note that the last term in (20.30) implements the principal-value prescription and cancels
the singular behavior of the previous term. Equation (20.30) contains the (N + 1) unknowns
R(kj , k0) for j = 0, N . We turn it into (N + 1) simultaneous equations by evaluating it for
(N + 1) k values on a grid (Figure 20.2) consisting of the observable momentum k0 and the
integration points:
k = ki =
{
kj , j = 1, N (quadrature points),
k0, i = 0 (observable point).
(20.31)
There are now (N + 1) linear equations for (N + 1) unknowns Ri ≡ R(ki, k0):
Ri = Vi +
2
π
N∑
j=1
k2jVijRjwj
(k20 − k2j )/2µ
− 2
π
k20Vi0R0
N∑
m=1
wm
(k20 − k2m)/2µ
. (20.32)
We express these equations in matrix form by combining the denominators and weights into a
single denominator vector D:
Di =
+
2
π
wik2i
(k20 − k2i )/2µ
, for i = 1, N,
− 2π
∑N
j=1
wjk20
(k20 − k2j )/2µ
, for i = 0.
(20.33)
The linear equations (20.32) now assume that the matrix form
R−DV R = [1−DV ]R = V, (20.34)
where R and V are column vectors of length N + 1:
[R] =

R0,0
R1,0
. . .
RN,0
 , [V ] =

V0,0
V1,0
. . .
VN,0
 . (20.35)
We call the matrix [1 − DV ] in (20.34) the wave matrix F and write the integral equation as
the matrix equation
[F ][R] = [V ], Fij = δij −DjVij . (20.36)
With R the unknown vector, (20.36) is in the standard form AX = B, which can be solved by
the mathematical subroutine libraries discussed in Chapter 8, “Solving Systems of Equations
with Matrices; Data Fitting.”
20.4.4 Solution via Inversion, Elimination
An elegant (but alas not efficient) solution to (20.36) is by matrix inversion:
[R] = [F ]−1[V ]. (20.37)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
470 CHAPTER 20
Listing 20.2 Scatt.py solves the Lippmann–Schwinger integral equation for scattering from a delta-shell potential.
The singular integral equations are regularized by a subtraction, converted to matrix equations using
Gaussian grid points, and then solved with Jama. 
# S c a t t . j a v a : Soln o f Lippmann Schwinger i n p s p a c e f o r s c a t t e r i n g
from v i s u a l . g raph i m p o r t ∗
from g a u s s i m p o r t g a u s s # g a u s s . pyc f o r g a u s s method
i m p o r t numpy . l i n a l g as l i n a # Numpy’s LinearAlgebra will be lina
graphscatt = gdisplay(x=0, y=0, xmin=0, xmax=6,ymin=0, ymax=1, width=600, height=400,
title=’S wave c r o s s s e c t i o n vs E’, xtitle=’kb’, ytitle=’ [ s i n ( d e l t a ) ]∗∗2’)
sin2plot = gcurve(color=color.yellow)
M = 27; b = 10.0; n = 26
k = zeros((M), Float); x = zeros((M), Float); w = zeros((M), Float)
Finv = zeros((M,M),Float); F = zeros((M,M), Float); D = zeros((M), Float)
V = zeros((M), Float); Vvec = zeros((n+1,1),Float)
scale = n/2; lambd = 1.5
gauss(n, 2, 0., scale, k, w) # Set up points & wts
ko = 0.02
for m in range(1,901):
k[n] = ko
for i in range (0, n): D[i]=2/pi*w[i]*k[i]*k[i]/(k[i]*k[i]-ko*ko) # D matrix
D[n] = 0.
for j in range(0,n): D[n]=D[n]+w[j]*ko*ko/(k[j]*k[j]-ko*ko)
D[n] = D[n]*(-2./math.pi)
for i in range(0,n+1): # Set up F matrix and V vector
for j in range(0,n+1):
pot = -b*b * lambd * math.sin(b*k[i])* math.sin(b*k[j])/(k[i]*b*k[j]*b)
F[i][j] = pot*D[j] # Form F
if i==j: F[i][j] = F[i][j] + 1.
V[i] = pot # Define V
for i in range(0,n+1): Vvec[i][0]= V[i]
Finv = lina.inv(F) # Use LinearAlgebra fir inverse
R = dot(Finv, Vvec) # Matrix multiply
RN1 = R[n][0]
shift = math.atan(-RN1*ko)
sin2 = (math.sin(shift))**2
sin2plot.plot(pos = (ko*b,sin2)) # Plot sin**2(delta)
ko = ko + 0.2*math.pi/1000.0
print"Done"
Because the inversion of even complex matrices is a standard routine in mathematical libraries,
(20.37) is a direct solution for the R matrix. Unless you need the inverse for other purposes
(like calculating wave functions), a more efficient approach is to use Gaussian elimination to
find an [R] that solves [F ][R] = [V ] without computing the inverse.
20.4.5 Scattering Implementation
For the scattering problem, we use the same delta-shell potential (20.16) discussed in §20.2.2
for bound states:
V (k′, k) =
−|λ|
2µk′k
sin(k′b) sin(kb). (20.38)
This is one of the few potentials for which the Lippmann–Schwinger equation (20.20) has an
analytic solution [Gott 66] with which to check:
tan δ0 =
λb sin2(kb)
kb− λb sin(kb) cos(kb)
. (20.39)
Our results were obtained with 2µ = 1, λ b = 15, and b = 10, the same as in [Gott 66]. In
Figure 20.5 we give a plot of sin2 δ0 versus kb, which is proportional to the scattering cross
section arising from the l = 0 phase shift. It is seen to reach its maximum values at energies
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
INTEGRAL EQUATIONS IN QUANTUM MECHANICS 471
Figure 20.5 The energy dependence of the cross section for l = 0 scattering from an attractive delta-shell potential
with λ b = 15. The dashed curve is the analytic solution (20.39), and the solid curve results from
numerically solving the integral Schrödinger equation, either via direct matrix inversion or via LU
decomposition.
0 2 4 6
0
1
kb
Analytic
corresponding to resonances. In Listing 20.2 we present our program for solving the scattering
integral equation using the JAMA matrix library and the gauss method for quadrature points.
For your implementation:
1. Set up the matrices V[], D[], and F[][]. Use at least N = 16 Gaussian quadrature
points for your grid.
2. Calculate the matrix F−1 using a library subroutine.
3. Calculate the vector R by matrix multiplication R = F−1V .
4. Deduce the phase shift δ from the i = 0 element of R:
R(k0, k0) = R0,0 = −
tan δ
ρ
, ρ = 2µk0. (20.40)
5. Estimate the precision of your solution by increasing the number of grid point in steps
of two (we found the best answer for N = 26). If your phase shift changes in the second
or third decimal place, you probably have that much precision.
6. Plot sin2 δ versus energyE = k20/2µ starting at zero energy and ending at energies where
the phase shift is again small. Your results should be similar to those in Figure 20.5. Note
that a resonance occurs when δl increases rapidly through π/2, that is, when sin2 δ0 = 1.
7. Check your answer against the analytic results (20.39).
20.4.6 Scattering Wave Function (Exploration)
1. The F−1 matrix that occurred in our solution to the integral equation
R = F−1V = (1− V G)−1V (20.41)
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
472 CHAPTER 20
is actually quite useful. In scattering theory it is known as the wave matrix because it is
used in expansion of the wave function:
u(r) = N0
N∑
i=1
sin(kir)
kir
F (ki, k0)−1. (20.42)
Here N0 is a normalization constant and the R matrix gives standing-wave boundary
conditions. Plot u(r) and compare it to a free wave.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Appendix A
Glossary
absolute value- The value of a quantity expressed as a positive number, for example, |f(x)|.
accuracy- The degree of exactness provided by a description or theory. Accuracy usually
refers to an absolute quality, while precision usually refers to the number of digits used
to represent a number.
address- The numerical designation of a location in memory. An identifier, such as a label,
that points to an address in memory or a data source.
algorithm- A set of rules for solving a problem in a finite number of steps. Usually indepen-
dent of the software or hardware.
allocate- To assign a resource for use, often memory.
alphanumeric- The combination of alphabetic letters, numerical digits, and special charac-
ters, such as %, $, and /.
analog- The mapping of a continuous physical observable to numbers, for example, a car’s
speed to the numbers on its speedometer.
animation- A process in which motion is simulated by presenting a series of slightly different
pictures (frames) in succession.
append- To add on, especially at the end of an object or word.
application- A self-contained, executable program containing tasks to be performed by a
computer, usually for a practical purpose.
architecture- The overall design of a computer in terms of its major components: memory,
processor, I/O, and communication.
archive- To copy programs and data to an auxiliary medium or file system for long-term,
compact storage.
argument- A parameter passed from one program part to another or to a command.
arithmetic unit- The part of the central processing unit that performs arithmetic.
array (matrix)- A group of numbers stored together in rows and columns that may be refer-
enced by one or more subscripts. Each number in an array is an array element.
assignment statement- A command that sets a value to a variable or symbol.
B- The abbreviation for byte (8 bits).
b- The abbreviation for bit (binary digit).
background- (1) A technique of having a programming run at low priority (“in the back-
ground”) while a higher-priority program runs “in the foreground.” (2) The part of a
video display not containing windows.
base- The radix of a number system. (For example, 10 is the radix of the decimal system.)
basic machine language- Instructions telling the hardware to do basic a operation such as
store or add binary numbers.
batch- The running of programs without user interaction, often in the
background.
baud- The number of signal elements per unit time, often 1 bit per second.
binary- Related to the number system with base 2.
BIOS- Basic input/output system.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
474 appenidix a
bit- Contraction of “binary digit”; digit 0 or 1 in binary representation.
Boolean algebra- A branch of symbolic logic dealing with logical relations as opposed to
numerical values.
boot- To “bootstrap”; to start a computer by loading the operating system.
branch- To pick a path within a program based on the values of variables.
bug- A mistake in a computer program or operating system; a malfunction.
bus- A communication channel (a bunch of wires) used for transmitting information quickly
among computer parts.
byte- Eight bits of storage. Java uses two bytes to store a single character in extended uni-
code.
byte code- Compiled code read by all computer systems but still needing to be interpreted
(or recompiled); contained in a class file.
cache- Small, very fast memory used as temporary storage between very fast CPU registers
and main memory or between disk and RAM.
calling sequence- The data and setup needed to call a method or subprogram.
central processing unit (CPU)- The part of a computer that accepts and acts on instructions;
where calculations are done and communications controlled.
checkpoint- A statement within a program that stops normal execution and provides output
to assist in debugging.
checksum- The summation of digits or bits used to check the integrity of data.
child- An object created by a parent object.
class- (1) A group of objects or methods having a common characteristic. (2) A collection
of data types and associated methods. (3) An instance of an object. (4) The byte code
version of a Java program.
clock- Electronics that generate periodic signals to control execution.
code- A program or the writing of a program (often compiled).
column- The vertical line of numbers in an array.
column-major order- The method used by Fortran to store matrices in which the leftmost
subscript attains its maximum value before the subscript to the right is incremented. (Java
and C use row-major order.)
command- A computer instruction; a control signal.
command key- A keyboard key, or combination of keys, that performs a predefined function.
compilation- The translation of a program written in a high-level language to (more) basic
language.
compiler- A program that translates source code from a high-level computer language to
more basic machine language.
concatenate- To join together two or more strings head to tail.
concurrent processing- The same as parallel processing; the simultaneous execution of sev-
eral related instructions.
conditional statement- A statement executed only under certain conditions.
control character- A character that modifies or controls the running of a program (e.g., con-
trol + c).
control statement- A statement within a program that transfers control to
another section of the program.
copy- To transfer data without removing the original.
CPU- See central processing unit.
crash- The abnormal termination of a program or a piece of hardware.
cycle time (clock speed)- The time needed for a CPU to execute a simple
instruction.
data- Information stored in numerical form; plural of datum.
data dependence- Occurs when two statements are addressing identical storage locations.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
GLOSSARY 475
dependence- Relation among program statements in which the results depend on the order
in which the statements are executed.
data type- Definitions that permit proper interpretation of a character string.
debug- To detect, locate, and remove mistakes in software or hardware.
default- The assumption made when no specific directive is given.
delete- To remove and leave no record.
DFT- Discrete Fourier transform.
digital- The representation of quantities in discrete form; contrast analog.
dimension of an array The number of elements that may be referenced by an array index.
The logical dimension is the largest value actually used by the program.
directory- A collection of files given their own name.
discrete- Related to distinct elements.
disk, disc- A circular magnetic medium used for storage.
double precision- The use of two memory words to store a number.
download- To transfer data from a remote computer to a local computer.
DRAM- See dynamic RAM. Contrast with SRAM.
driver- A set of instructions needed to transmit data to or from an external device.
dump- Data resulting from the listing of all information in memory.
dynamic RAM- Computer memory needing frequent refreshment.
E- A symbol for “exponent.” To illustrate, 1.97E2 = 1.97× 102.
element- An item of data within an array; a component of a language.
enable- To make a computer part operative.
ethernet- A high-speed local area network (LAN) composed of specific cable technology
and communication protocols.
executable program- A set of instructions that can be loaded into a computer’s memory and
executed.
executable statement- A statement that causes a certain computational action, such as as-
signing a value to a variable.
fetch- To locate and retrieve information from storage.
FFT- Fast Fourier transform.
flash memory- Memory that does not require power to retain its contents.
floating point- The finite storage of numbers in scientific notation.
FLOP- Floating-point operation.
foreground- Running high-priority programs before lower-priority programs.
Fortran- Acronym for formula translation; a classic computer language.
fragmentation- File storage in many small, dispersed pieces.
garbage- Meaningless numbers, usually the result of error or improper
definition. Obsolete data in memory waiting to be removed (“collected”).
giga, G- Prefix indicating 1 billion, 109, of something (US).
GUI- Graphical user interface; a windows environment.
hard disk- A circular, spinning, storage device using magnetic memory.
hardware- The physical components of a computer system.
hashing- A transformation that converts keystrokes to data values.
heuristic- A trial-and-error approach to problem solving.
hexadecimal- Base 16; {0,1,2,3,4,5,6,7,8,9,A,B,C,D,E,F}.
hidden line surface- The part of a graphics object normally hidden from view.
high-level language- A programming language similar to normal language.
host computer- A central or remote computer serving other computers.
HPC- High-performance computing.
icon- A small on-screen symbol that activates an application.
increment- The amount added to a variable, especially an array index.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
476 appenidix a
index- The symbol used to locate a variable in an array; the subscript.
infinite loop- The endless repetition of a set of instructions.
input- The introduction of data from an external device into main storage.
instructions- Commands to the hardware to do basic things.
instruction stack- The ordered group of instructions currently in use.
interpolation- Finding values between known values.
interpreter- A language translator that sequentially converts each line of source code to
machine code and immediately executes each line.
interrupt- A command that stops the execution of a program when an abnormal condition is
encountered.
iterate- To repeat a series of steps automatically.
jump- A departure from the linear processing of code; branch, transfer.
just-in-time compiler- A program that recompiles a Java class file into more
efficient machine code.
kernel- The inner or central part of a large program or of an operating system that is not
modified (much) when run on different computers.
kill- To delete or stop a process.
kilo, k- Prefix indicating 1 thousand, 103.
LAN- Local area network.
LAPACK- A linear algebra package (a subroutine library).
language- Rules, representations, and conventions used to communicate
information.
LHS- Left-hand side.
library (lib)- A collection of programs or methods usually on a related topic.
linking- Connecting separate pieces of code to form an executable program.
literal- A symbol that defines itself, such as the letter A.
load- To read information into a computer’s memory.
load module- A program that is loaded into memory and run immediately.
log in (on)- To sign onto a computer; to begin a session.
loop- A set of instructions executed repeatedly as long as some condition is met.
low-level language- Machine-related commands not for humans.
machine language- Commands understood by computer hardware.
machine precision- The maximum positive number that, when added to the number stored
as 1, does not change it.
macro- A single higher-level statement resulting in several lower-level ones.
main method- The section of an application program where execution begins.
main storage- The fast electronic memory; physical memory.
mantissa- Significant digits in a floating-point number; for example, 1.2 in 1.2E3.
mega, M- A prefix denoting a million, or 1, 048, 576 = 220.
method- A subroutine used to calculate a function or manipulate data.
MIMD- Multiple-instruction, multiple-data computer.
modular programming- The technique of writing programs with many reusable independent
parts.
modulo (mod)- A function that yields a remainder after the division of numbers.
multiprocessors- Computers with more than one processor.
multitasking- The system by which several jobs reside in a computer’s memory simultane-
ously; they may run in parallel or sequentially.
NAN- Not a number; a computer error message.
nesting- Embedding a group of statements within another group.
object- A software component with multiple parts or properties.
object-oriented programming- A modular programming style focused on classes of data ob-
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
GLOSSARY 477
jects and associated methods to interact with the objects.
object program (code)- A program in basic machine language produced by
compiling a high-level language.
octal- Base 8; easy to convert to or from binary.
ODE- Ordinary differential equation.
1-D- One-dimensional.
operating system (OS)- The program that controls a computer and runs applications, pro-
cesses I/O, and shells.
OOP- Object-oriented programming.
optimization- The modification of a program to make it run more quickly.
overflow- The result of trying to store too large a number.
package- A collection of related programs or classes.
page- A segment of memory that is read as a single block.
parallel (concurrent) processing- Simultaneous or independent processing in
different CPUs.
parallelization- Rewriting an existing program to run in parallel.
partition- The section of memory assigned to a program during its execution.
PC- Personal computer.
PDE- Partial differential equation.
physical memory- The fast electronic memory of a computer; main memory; contrast with
virtual memory.
physical record- The physical unit of data for input or output that may contain a number of
logical records.
pipeline (segmented) arithmetic units- An assembly-line approach to central processing; the
CPU simultaneously gathers, stores, and processes data.
pixel- A picture element; a dot on the screen. See also voxel.
Portable Document Format, .pdf - A document format developed by Adobe that is of high
quality and readable by an extended browser.
PostScript, .ps- A language developed by Adobe for printing high-quality text and graphics.
precision- The degree of exactness with which a quantity is presented. High-precision num-
bers are not necessarily accurate.
program- A set of instructions that a computer interprets and executes.
protocol- A set of rules or conventions.
pseudocode- A mixture of normal language and coding that provides a symbolic guide to a
program.
queue- An ordered group of items waiting to be acted upon in turn.
radix- The base number in a number system that is raised to powers.
RAM- Random-access (central) memory that is reached directly.
random access- Reading or writing memory independent of storage order.
record- A collection of data items treated as a unit.
recurrence/recursion- Repetition producing new values from previous ones.
registers- Very high-speed memory used by the central processing unit.
reserved words- Words that cannot be used in an application program.
RHS- Right-hand side.
RISC- A CPU design for a reduced instruction set computer.
row-major order- The method used by Java to store matrices in which the rightmost subscript
varies most rapidly and attains its maximum value before the left subscript is incremented.
run- To execute a program.
scalar- A data value or number, for example, π.
serial/scalar processing- Calculations in which numbers are processed in
sequence. Contrast with vector processing and parallel processing.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
478 appenidix a
shell- A command-line interpreter; the part of the operating system where the user enters
commands.
SIMD- A single instruction, multiple-data computer.
simulation- The modeling of a real system by a computer program.
single precision- The use of one computer word to store a variable.
SISD- A single-instruction, single-data computer.
software- Programs or instructions.
source code- A program in a high-level language needing compilation to run.
SRAM- See static RAM.
Static RAM.- Memory that retains its contents as long as power is applied.
Contrast with DRAM.
stochastic- A process in which there is an element of chance.
stride- The number of array elements stepped through as an operation repeats.
string- A connected sequence of characters treated as a single object.
structure- The organization or arrangement of a program or a computer.
subprogram- Part of a program invoked by another program unit; a subroutine.
supercomputer- The class of fastest and most powerful computers available.
superscalar- A later-generation RISC computer.
syntax- The rules governing the structure of a language.
TCP/IP- Transmission control protocol/internet protocol.
telnet- Protocols for computer–computer communications.
tera, T- Prefix indicating 1012.
top-down programming- Designing a program from the most general view of
the problem down to the specific subroutines.
unary- An operation that uses only one operand; monadic.
underflow- The result of trying to store too small a number.
unit- A device having a special function.
upload- Data transfer from a local to a remote computer; the opposite of
download.
URL- Universal resource locator; web address.
utility programs- Programs to enhance other programs or do chores.
vector- A group of N numbers in memory arranged in 1-D order.
vector processing- Calculations in which an entire vector of numbers is
processed with one operation.
virtual memory- Memory on the slow, hard disk and not in fast RAM.
visualization- Conversion of numbers to 2-D and 3-D pictures or graphs.
volume- A physical unit of a storage medium, such as a disk.
voxel- A volume element on a regular 3-D grid.
word- A unit of main storage, usually 1, 2, 4, 6, or 8 bytes.
word length- The amount of memory used to store a computer word.
WWW- World wide web.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Appendix B
Installing Python, VPython,Matplotlib,NumPy
The codes in this text were developed with Python 2.5 and the packages matplotlib, NumPy,
and Visual 3. There is a new Python 2.6 out, but as of this moment it is not stable and does
not work right with the present versions of matplotlib and numpy. However, we also read that
matplotlib and numpy will be made compatible after Python 2.6 has been stabilized. So at
present, please stick with Python 2.5 and Visual 3. There are versions for Windows, Linux and
Macs.
1. Go to
http://vpython.org/
Download Python-2.5.4. For Windows, install it by double-clicking on the downloaded file.
Remember, you need to install Python before VPython.
2. Next, from the same site download Vpython (VPython-Win-Py2.5-3.2.11.exe for Win-
dows). Install it.
3. Next, go to
http://sourceforge.net/projects/numpy/files/NumPy/
Download NumPy. We used numpy-1.1.1-win32-superpack-python2.5.exe, but it is not
the latest and that is for Windows. Install it.
4. Finally, go to
http://sourceforge.net/projects/matplotlib/
Download matplotlib. We used matplotlib-0.98.3.win32-py2.5.exe. Install it.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Appendix C
OpenDX, Industrial-Strength Data Visualization
Most of the visualizations we use in this book are 2-D, y(x) plots, or 3-D, z(x, y) (surface)
plots. Some of the applications, especially the applets, use animations, which may also be 2-D
or 3-D. Samples are found on the CD. We use and recommend Grace (2-D) and Gnuplot (2-D,
3-D) for stand-alone visualizations, and PtPlot for visualizations callable from Java programs
[L 05]. All have the power and flexibility for scientific work, as well as being free or open
source.
An industrial-strength tool for data visualization, which we also recommend, is OpenDX,
or DX for short.1 It was originally developed as IBM Data Explorer but has now joined the
ranks of open-source software [DX1, DX2] and is roughly equivalent to the commercial pack-
age AVS. DX works under Linux, Unix, or a Linux emulator (Cygwin) on a PC. The design
goals of DX were to
• Run under many platforms with various data formats.
• Be user-friendly via visual programming, a modern technique in which programs are writ-
ten by connecting lines between graphical objects, as opposed to issuing textual com-
mands (Figure C.3 right).
• Handle large multidimensional data sets via volume rendering of f(x, y, z) as well as
slicing and dicing of higher-dimensional data sets.
• Create attractive graphics without expensive hardware.
• Have a graphical user interface that avoids the necessity of learning many commands.
The price to pay for all this power is the additional time spent learning how to do new things.
Nevertheless, it is important for students of computational physics to get some experience with
state-of-the-art visualization.
This appendix is meant as an introduction to DX. We use it to visualize some commonly
encountered data such as scalar and vector fields and 3-D probability densities. DX can do
much more than that, and, indeed, is a standard tool in visualization laboratories around the
world. However, the visualizations in this text are just in gray, while DX uses color as a key
element. Consequently, we recommend that you examine the DX visualizations on the CD to
appreciate their beauty and effectiveness.
Analogous to the philosophy behind the Unix operating system, DX is a toolbox con-
taining tools that permit you to create visualizations customized to your data. These visual-
izations may be created by command-line programming (for experienced users) or by visual
programming. Typically, six separate steps are involved:
1Juan Manuel Vanegas Moller and Guillermo Avendaño assisted in the preparation of this appendix.
opendx, industrial-strength data visualization 481
1. Store the data in a file using a standard format.
2. Use the Data Prompter to describe the data’s organization and to place that description in
a .general file.
3. Import the data into DX.
4. Chose a visualization method appropriate for the data type.
5. Create a visual program with the Visual Program Editor by networking modules via drawn
lines.
6. Create and manipulate an image.
C.1 GETTING DX AND UNIX RUNNING (FOR WINDOWS)
If you do not have DX running on your computer, you can download it free from the DX home
page [DX1]. Once running, just issue the dx or DX command and the system starts up. DX uses
the Unix X-Windows system. If you are running under MS Windows, then you will first need to
start an X-server program and then start DX at the resulting X-Windows prompt. We discussed
this in Chapter 3, where we recommended the free Unix shell emulator Cygwin [CYG].
In order to run DX with Cygwin, you must start your X server prior to launching DX.
You may also have to tell the X server where to place the graphical display:
> startxwin.sh Starts X server under Cygwin
> set environment DISPLAY localhost:0 Sets X11 display
> dx Starts DX; or start menu
Here we issued the dx command from an X11 window. This works if your PATH variable
includes the location of DX. We have had problems getting the dx command to work on some
of our Cygwin installations, and in those cases we followed the alternative approach of loading
DX as a regular MS Windows application. To do that, we first start the X11 server from a
Cygwin bash shell with the command startxwin.sh and then start DX from the Start menu.
While both approaches work fine, DX run from a Unix shell looks for your files within the Unix
file system at /home/userid, while DX run under Windows looks within the MS Windows file
system at C:\Documents and Settings\userid\My Documents\. Another approach, which
is easier but costs money, is to install a commercial X11 server such as X-Win 32 [XWIN32]
coupled with your DX. Opening DX through MS Windows then automatically opens the X11
server with no effort on your part.
C.2 TEST DRIVE OF DX VISUAL PROGRAMMING
Here we lead you through a test drive that creates a color surface plot with DX. This is delib-
erately cookbook-style in order to get you on the road and running. We then go on to provide
a more systematic discussion and exploration of DX features, but without all the details.
If you want to see some of the capabilities of DX without doing any work,
look at some of the built-in Samples accessible from the main menu (we recommend
ThunderStreamlines.net and RubberTube.net). Once the pretty picture appears, go to Op-
tions/ViewControl/Mode and try out Rotate and Navigate. If you want to see how the graphical
program created the visualization, go to Windows/Open Visual Program Editor and rearrange the
program elements.
1. Prepare input data: Run the program ShockLax.java that describes shock wave for-
mation and produces the file Shock.dat we wish to visualize; alternatively, copy the file
ShockLax.dat from the CD (Codes/JavaCodes). If you are running DX under Cygwin,
482 appendix c
Figure C.1 Left: The main Data Explorer window. Right: The Data Prompter window that opens when you select Import
Data.
you will need to copy this file to the appropriate folder in /home/userid, where userid
is your name on the computer.
2. Examine input data: Take note of the structure of the data in Shock.dat. It is in a file
format designed for creating a surface plot of z(x, y) with Gnuplot. It contains a column
of 25 data values, a blank line, and then another 25 values followed by another blank
lines, and so forth, for a total of 13 columns each with 25 data values:
0.0
0.6950843693971483
1.355305208363503
1.9461146066793003. . .
–1.0605832625347442
–0.380140746321537
(blank line)
0.0
0.6403868757235301
1.2556172093991282. . .
2.3059977070286473
2.685151549102467
(blank line)
2.9987593603912095
· · ·
Recall that the data are the z values, the locations in the column are the x values, and the
column number gives the y values. The blank lines were put in to tell Gnuplot that we
are starting a new column.
3. Edit data: While you can leave the blank lines in the file and still produce a DX
visualization, they are not necessary because we explicitly told DX that we have one
column of 25× 13 data elements. Try both and see!
4. Start DX: Either key in dx at the Unix prompt or start an X-Windows server and then
start DX. A Data Explorer main window (Figure C.1 left) should appear.
5. Import data: Press the Import Data button in the main Data Explorer window. The Data
Prompter window (Figure C.1 right) appears after a flash and a bang. Either key in the
full path name of the data file (/home/mpaez/numerical.dat in our case) or click on
Select Data File from the File menu. We recommend that after entering the file name, you
depress the Browse Data button in the Data Prompter. This gives you a window (Figure C.2
right) showing you what DX thinks is in your data file. If you do not agree with DX,
then you need to find some way to settle the dispute.
6. Describe data: Press the Grid or Scattered file button in the Data Prompter window. The
window now expands (Figure C.2 left) to one that lets you pick out a description of the
data.
opendx, industrial-strength data visualization 483
Figure C.2 Left: The expanded Data Prompter window obtained by selecting Grid or Scattered File. Right: The File
Browser window used to examine a data file.
Figure C.3 The Visual Program Editor with Tools or Categories on the left and the canvas on the right. In the left
window a single module has been selected from Categories, while in the right window several modules
have been selected and networked together.
a. Select the leftmost Grid type, that is, the one with the regular array of small squares.
b. For these data, select Single time step, which is appropriate because our data contain
only a single column with no explicit (x, y) values.
c. Because we have only the one z component to plot (a scalar field), set the slider at 1.
d. Press the Describe Data button. This opens another window that tells DX the structure
of the data file. Enter 25 as the first entry for the Grid size, and 13 as the second entry.
This describes the 13 groups of 25 data elements in a single long column. Press the
button Column.
e. Under the File menu, select Save as and enter a name with a .general extension, for
example, Shock.general. The file will contain
file = /home/mpaez/Shock.dat grid = 25 x 13 format = ascii
interleaving = field majority = column field = field0 structure =
scalar type = float dependency = positions positions = regular,
regular, 0, 1, 0, 1 end
Now close all the Data Prompter windows.
7. Program visualization with a visual editor: Next design the program for your visu-
alization by drawing lines to connect building blocks (modules). This is an example of
visual programming in which drawing a flowchart replaces writing lines of code.
a. Press the button New Visual Program from the Data Explorer main window. The Visual
484 appendix c
Figure C.4 Left: The image resulting from selecting Execute Once in the Visual Program Editor. Right: The same
image after being rotated with the mouse reveals its rubber sheet nature.
Program Editor window appears (Figure C.3 left). There is a blank space to the right
known as the canvas, upon which you will draw, and a set of Tools on the left. (We
list the names of all the available tools in §C.3, with their descriptions available from
DX’s built-in user’s manual.)
b. Click on the + to the left of Import and Export. The box changes to - and
opens up a list of categories. Select Import so that it remains highlighted and then
click near the top of the canvas. This should place the Import module on the canvas
(Figure C.3 left).
c. Close Import and Export and open the Transformation box. Select AutoColor so that it
remains highlighted and then click below the Import module. The AutoColor image
that appears on the canvas will be used to color the surface based on the z values.
d. Connect the output of Import to the left input of Autocolor by dragging your mouse,
keeping the left button pressed until the two are connected.
e. Close the Transformation box and look under Realization. Select RubberSheet (a descrip-
tive name for a surface plot), place it on the canvas below Autocolor, and connect the
output of Autocolor to the left input tag of RubberSheet.
f. Close the Realization box and look under the Rendering tool. Select Image and connect
its input to the output of RubberSheet. You should now have a canvas (Figure C.3
right).
g. A quick double-click on the Import block should bring up the Import window. Push the
Name button so that it turns green and ensure that the .general file that you created
previously appears under Value. If not, enter it by hand. In our case it has the name
"/home/mpaez/numer.general". Next press the Apply button and then OK to close the
window.
h. Return to the Visual Program Editor. On the menu across the top, press Execute/Execute
Once, and an image (Figure C.4 left) should appear.
8. Manipulating the image: Try grabbing and rotating the image with the mouse. If that
does not work, you need to tell DX what you want:
a. From the menu at the top of the Image window select Options; in the drop-down win-
dow select Mode; in the next drop-down window select Rotate. For instance, the rather
flat-looking image on the left in Figure C.4 acquires depth when rotated to the image
on the right.
b. Again select Options from the Image window’s menu (Figure C.5 left), but this time
select AutoAxes from the drop-down window. In the new window key in Position in
the X space and Time in the Y space. Press Apply/OK, and an image (Figure C.5 right)
opendx, industrial-strength data visualization 485
Figure C.5 Left: The AutoAxes Configuration window brought up as an Option from the Visual Program Editor.
Right: The final image with axes and labels.
Figure C.6 On the left the Scale option is selected under Rendering and placed on the canvas. Double-clicking on
the Image icon produces the Scale window on the right. By changing the second line under Value to
[112], the z scale is doubled.
should appear.
c. Now that you have the image you desire, you need to save it. From the File menu
of the Image window, select Save Image. A pop-up window appears, from which you
select Format, then the format you desire for the image, then the Output file name, and
finally Apply.
d. You want to ensure that the visual program is saved (in addition to the data file). In
the Image window select File/Save Program As (we selected Shock2.net).
9. Improved scale: Often the effectiveness of a surface plot like Figure C.5 can be im-
proved by emphasizing the height variation of the surface. The height is changed in DX
by changing the scale:
a. Go to the Visual Program Editor, select Rendering/scale (Figure C.6 right). Place the
Scale icon between the RubberSheet and Image icons and connect.
b. Double-click the Image icon, and a Scale window opens (Figure C.6 right).
c. Change the second line under Value from [111] to [112] to double the z scale. Press
Apply/OK.
d. Select Execute, and then under Option, select Execute once. The improved image in
Figure C.7 appears.
486 appendix c
C.3 DX TOOLS SUMMARY
DX has built-in documentation that includes a QuickStart Guide, a User’s Guide, a User’s
Reference, a Programmer’s Reference, and an Installation and Configuration Guide. Here we
give a short summary of some of the available tools. Although you can find all this information
under the Tools categories in the Visual Program Editor, it helps to know what you are looking
for! The DX on-line help provides more information about each tool. In what follows we list
the tools in each category.
Categories
Annotation DXLink Debugging Flow control Import and export
Interactor Interface control Realization Rendering Special
Structuring Transformation Windows All
Annotation: adds various information to visualization
Annotation DXLink Debugging Flow control Import Export
AutoAxes AutoGlyph Caption ColorBar Format Glyph
Legend Parse Plot Ribbon Text Tube
DXLink: DX control from other programs
DXLInput DXLInputNamed DXLOutput
Debugging: analyzes program’s execution
Message Echo Print Describe System Trace
Usage Verify VisualObject
Flow Control: execution flow in a visual program
Done Execute First ForEachMember ForEachN GetGlobal
GetLocal Route SetGlobal SetLocal Switch
Import and Export: data flow and processing in a visual program
Export Import ImportSpreadsheet Include Partition ReadImage
Reduce Refine Slab Slice Stack Transpose
WriteImage
Interactor: interactive control of input to modules via an interface
FileSelector Integer IntegerList Reset Scalar ScalarList
Selector String SelectorList StringList Toggle Value
ValueList VectorList Vector
Interface Control: control DX tools from within a visual program
ManageColormapEditor ManageControlPanel ManageImageWindow
ManageSequencer
Realization: create structures for rendering and display
AutoGrid ShowConnections Connect Construct Enumerate Grid
Isolate ShowBoundary MapToPlane Regrid RubberSheet Sample
Isosurface ShowPositions Band ShowBox Streakline Streamline
Rendering: create/modify an image
AmbientLight Arrange AutoCamera Camera ClipBox ClipPlane
Display FaceNormals Image Light Normals Overlay
Render Reorient Rotate Scale ScaleScreen Shade
Transform Translate UpdateCamera
opendx, industrial-strength data visualization 487
Figure C.7 The graph with a better scale for the z axis.
Special: miscellaneous
Colormap Input Output Pick Probe
ProbeList Receiver Sequencer Transmitter
Structuring: manipulate DX data structures
Append Attribute ChangeGroupMember CopyContainer Collect Rename
Extract Replace ChangeGroupType CollectNamed Inquire List
Mark Remove CollectMultiGrid CollectSeries Select Unmark
Transformation: modify or add to an input Field
AutoColor AutoGrayScale Categorize CategoryStatistics Color Compute
Compute2 Convert DFT Direction DivCurl Equalize
FFT Filter Gradient Histogram Lookup Map
Measure Morph Post QuantizeImage Sort Statistics
Windows: create or supervise image windows
ReadImageWindow SuperviseState SuperviseWindow
C.4 DX DATA STRUCTURE AND STORAGE
Good organization or structuring of data is important for the efficient extraction (mining) of
signals from data. Data that are collected in fixed steps for the independent variables, for
example, y(x = 1), y(x = 2), . . ., y(x = 100), fit a regular grid. The corresponding data are
called regular data. The coordinates of a single datum within a regular set can be generated
488 appendix c
Figure C.8 Geometric and file representations of 1-D, 2-D, and 3-D data sets. The letters a, b, c, d, ...
represent numerical values that may be scalars or vectors.
from three parameters:
1. The origin of the data, for instance, the first datum at (x = 0, y = 0).
2. The size of the steps in each dimension for the independent variables, for example, (∆x,
∆y).
3. The total number of data in the set, for example, N .
The data structures used by spreadsheets store dependent variables with their associated inde-
pendent variables, for instance, {x, y, z(x, y)}. However, as already discussed for the Gnuplot
surface plot (Figure 3.8 right), if the data are regular, then storing the actual values of x and y is
superfluous for visualization since only the fact that they are evenly spaced matters. That being
the case, regular data need only contain the values of the dependent variable f(x, y) and the
three parameters discussed previously. In addition, in order to ensure the general usefulness of
a data set, a description of the data and its structure should be placed at the beginning of the set
or in a separate file.
Figure C.8 gives a geometric representation of 1-D, 2-D, and 3-D data structures. Here
each row of data is a group of lines, where each line ends with a return character. The columns
correspond to the different positions in the row. The different rows are separated by blank lines.
For the 3-D structure, we separate values for the third dimension by blank spaces from other
elements in the same row.
C.5 SAMPLE VISUAL PROGRAMS
We have included on the CD a number of DX programs we have found useful for some of
the problems in this book and which produce the visualizations we are about to show. Be-
cause color is such an important part of these visualizations, we suggest that you look at these
visualizations on the CD as well. Although you are free to try these programs, please note
that you will first have to edit them so that the file paths point to your directory, for example,
/home/userid/DXdata, rather than the named directories in the files.
opendx, industrial-strength data visualization 489
Figure C.9 The visual program Simple linear plot.net on the left imports a simple linear data file and pro-
duces the simple x + y plot shown on the right.
Figure C.10 The visual program on the left computes the discrete Fourier transform of the function (3.1), read in as
data, resulting in the plot on the right.
C.5.1 Sample 1: Linear Plot
The visual program simple linear plot.net (Figure C.9 left) reads data from the file
simple linear data.dat and produces the simple linear plot shown on the right. These data
are the sums of four sine functions,
f(t) = 1.8 sin(25t) + 0.22 sin(38t) + 0.5 sin(47t) + 0.95 sin(66t), (3.1)
stored at regular values of t.
C.5.2 Sample 2: Fourier Transform
As discussed in Chapter 10,“Fourier Analysis; Signals and Filters,” the discrete Fourier trans-
form (DFT) and the fast Fourier transform (FFT) are standard tools used to analyze oscillatory
signals. DX contains modules for both tools. On the left in Figure C.10 you see how we have
extended our visual program to calculate the DFT. The right side of Figure C.10 shows the
490 appendix c
Figure C.11 The visual program on the left produces visualization of the electric potential on a 2-D plane containing
a capacitor. Colors represent different potential values.
Figure C.12 The visual program on the left produces the surface plot of the electric potential on a 2-D plane con-
taining a capacitor. The surface height and color vary in accord with the values of the potential.
resulting discrete Fourier transform with the expected four peaks at ω = 25, 38, 47, and 66
rad/s, as expected from the function (3.1). Note how in the visual program we passed the data
through a Compute module to convert it from doubles to floats. This is necessary because DX’s
DFT accepts only floats. After computing the DFT, we pass the data through another Compute
module to take the absolute value of the complex transform, and then we plot them.
C.5.3 Sample 3: Potential of a 2-D Capacitor
While a vector quantity such as the electric field may be more directly related to experiment
than a scalar potential, the potential is simpler to visualize. This is what we did in Chap-
ter 17, “PDEs for Electrostatics and Heat Flow,” with the program Lapace.py when we solved
Laplace’s equation numerically. Some output from that simulation is used in the DX visualiza-
tion of the electric potential in a plane containing a finite parallel-plate capacitor (Figure C.11).
The visual program Laplace.net1 shown on the left creates the visualization, with the module
AutoColor coloring each point in the plane according to its potential value.
opendx, industrial-strength data visualization 491
Figure C.13 The visual program on the left produces the electric field visualization on the right.
Another way to visualize the same potential field is to plot the potential as the height
of a 3-D surface, with the addition of the surface’s color being determined by the value of
the potential. The visual program Laplace.net 2 on the left in Figure C.12 produces the
visualization on the right. The module RubberSheet creates the 3-D surface from potential data.
The range of colors can be changed, as well as the opacity of the surface, through the use of
the Colormap and Color modules. The ColorBar module is also included to show how the value
of the potential is mapped to each color in the plot.
C.5.4 Sample 4: Vector Field Plots
Visualization of the scalar field V (x, y) required us to display one number at each point in
a plane. A visualization of the vector electric field E(x, y) = −~∇V requires us to display
three numbers at each point. In addition to it being more work to visualize a vector field, the
increased complexity of the visualization may make the physics less clear even if the mathe-
matics is more interesting. This is for you, and your mind’s eye, to decide.
DX makes the plotting of vector fields easy by providing the module Gradient to compute
the gradient of a scalar field. The visual program Laplace.net 3 on the left in Figure C.13
produces the visualization of the capacitor’s electric field shown on the right. Here the Auto-
Glyph module is used to visualize the vector nature of the field as small vectors (glyphs), and
the Isosurface module is used to plot the equipotential lines. If the 3-D surface of the potential
were used in place of the lines, much of the electric field would be obstructed.
C.5.5 Sample 5: 3-D Scalar Potentials
We leave it as an exercise for you to solve Laplace’s equation for a 3-D capacitor composed of
two concentric tori. Although the extension of the simulation from 2-D to 3-D is straightfor-
ward, the extension of the visualization is not. To illustrate, instead of equipotential lines, there
are now equipotential surfaces V (x, y, z), each of which is a solid figure with other surfaces
hidden within. Likewise, while we can again use the Gradient module to compute the electric
field, a display of arrows at all points in space is messy. Such being the case, one approach is
to map the electric field onto a surface, with a display of only those vectors that are parallel
or perpendicular to the surface. Typically, the surface might be an equipotential surface or the
492 appendix c
Figure C.14 The visual program on the left plots a torus as a surface and maps the electric field onto this surface.
The visualization on the right results.
xy or yz plane. Because of the symmetry of the tori, the planes appear to provide the best
visualization.
The visual program Laplace-3d.net 1 on the left in (Figure C.14) plots an electric field
mapped onto an equipotential surface. The visualization on the right shows that the electric
field is perpendicular to the surface but does not provide information about the behavior of the
field in space. The program Laplace-3d.net 2 (Figure C.15) plots the electric field on the
y + z plane going through the tori. The plane is created by the module Slab. In addition, an
equipotential surface surrounding each torus is plotted, with the surfaces made semitransparent
in order to view their relation to the electric field. The bending of the arrows is evident in the
closeup on the far right.
C.5.6 Sample 6: 3-D Functions, the Hydrogen Atom
The electron probability density ρnlm(r, θ, φ) (calculated by H atom wf.java) is a scalar func-
tion that depends on three spatial variables [Libb 03]. Yet because the physical interpretation
of a density is different from that of a potential, different visualization techniques are required.
One way of visualizing a density is to draw 3-D surfaces of constant density. Because the den-
sity has a single fixed value, the surfaces indicates regions of space that have equal likelihood
of having an electron present. The visual program H atom prob density.net 1 produced the
visualization in Figure C.16. This visualization does an excellent job of conveying the shape
and rotational symmetry of the state but does not provide information about the variation of the
density throughout space.
A modern technique for visualizing densities is volume rendering. This method, which
is DX’s default for 3-D scalar functions, represents the density as a translucent cloud or gel
with light shining through it. The program H atom prob density.net 2 produces the electron
cloud shown in Figure C.17. We see a fuzziness and a color change indicative of the variation
of the density, but no abrupt edges. However, in the process we lose the impression of how the
state fills space and of its rotational symmetry. In addition, our eye equates the fuzziness with
a low-quality image, both because it is less sharp and because the colors are less intense. We
now alleviate that shortcoming.
As we have seen with the electric field, visualizing the variation of a field along two
opendx, industrial-strength data visualization 493
Figure C.15 The visual program at the top plots two equipotential surfaces in yellow and green, as well as the
electric field on a y + z plane going through the tori. The blue and red surfaces are the charged tori.
The image on the far right is a close-up of the middle image.
Figure C.16 The visual program on the left produces the surface plot of constant probability density for the 3-D
state on the right.
494 appendix c
Figure C.17 The visual program on the left produces the 3-D cloud density on the right.
Figure C.18 The visual program on the left produces the 3-D probability density on the right. The density is plotted
only on the surface of the xy and yz planes, with both planes made semitransparent.
opendx, industrial-strength data visualization 495
Figure C.19 Left: The visual DX program used to create an animation of the formation of a soliton wave. The
arrows on the Sequence Control are used to move between the frames. Right: Nine frames from the DX
animation.
planes is a good way to convey its 3-D nature. Because the electron cloud is rotationally
symmetric about the z axis, we examine the variation along the xy and yz planes (the xz
plane is equivalent to the yz one). Figure C.18 shows the visualization obtained with the
visual program H atom prob density.net 3. Here Slab modules are used to create the 2-D
plots of the density on the two planes, and the planes are made semitransparent for ease of
interpretation. This visualization is clearly an improvement over those in Figure C.17, both in
sharpness and in conveying the 3-D nature of the density.
C.6 ANIMATIONS WITH OPENDX
An animation is a collection of images called frames that when viewed in sequence convey
the sensation of continuous motion. It is an excellent way to visualize the behavior in time
of a simulation or of a function f(x, t), as might occur in wave motion or heat flow. In the
Codes section of the CD, we give several sample animations of the figures in this book and we
recommend that you try them out to see how effective animation is as a tool.
The easiest animation format to view is an animated gif, which can be viewed with
any Web browser. (The file ends with the familiar .gif extender but has a series of images CD
that load in sequence.) Otherwise, animations tend to be in multimedia/video file formats
such as MPEG, DivX, mp3, yuv, ogg, and avi. Not all popular multimedia players such as
RealPlayer, Windows Media Player, and QuickTime player play all formats, so you may need
to experiment. We have found that the free multimedia players VLC and ImageMagick work
well for various audio and video formats, as well as for DVDs, VCDs, and streaming protocols
(the player starts before all the media are received).
A simple visual program to create animations of the formation of solitons (Chapter 19,
”Solitons and Computational Fluid Dynamics”) with OpenDX is shown on the left in Fig-
ure C.5.6. The new item here is the Sequencer module, which provides motion control (it is
found under Tools/Special). As its name implies, Sequencer produces a sequence of integers
corresponding to the frame numbers in the animation (the user sets the minimum, maximum,
increment, and starting frame number). It is connected to the Format module, which is used to
create file names given the three input values: (1) soliton, (2) the output of the Sequencer, and
(3) a .general for the suffix. The Import module is used to read in the series of data files as the
frame number increases.
On the right in Figure C.5.6 we show a series of eight frames that are merged to form
496 appendix c
Figure C.20 Left: Use of the Visual Program Editor to produce the image of a wave packet that passes through a
slit. Right: The visual program used to produce frames for an OpenDX animation.
the animation (a rather short animation). They are plots of the data files soliton001.dat,
soliton002.dat, . . . soliton008.dat, with one file for each frame and with a specified Field
in each file imported at each time step. The Sequencer outputs the integer 2, and the output
string from the Format becomes soliton002.general. This string is then input to Import as the
name of the .general file to read in. In this way we import and image a whole series of files.
C.6.1 Scripted Animation with OpenDX
By employing the scripting ability of OpenDX, it is possible to generate multiple images with
the Visual Editor being invoked only for the first image. The multiple images can then be merged
into a movie. Here we describe the steps followed to create an .mpg movie:
1. Assemble a sequence of data files, one for each frame of the movie.
2. Use OpenDX to create an image file from the first data file. Also create .general and
.net files.
3. Employ OpenDX with the --script option invoked to create a script. Read in the data
file and the .net file and create a .jpg image. (Other image types are also possible, but
this format produces good quality without requiring excessive disk space.)
4. Merge the .jpg image files into a movie by using a program such as Mplayer in Linux.
We suggest producing the movie in the .avi or .mpg format.
To illustrate these steps, assume that you have a solution to Laplace’s equation for a two-
plate capacitor, with the voltage on the plates having a sinusoidal time dependence. Data
files lapl000.dat, lapl001.dat, . . . , lapl120.dat are produced in the Gnuplot 3-D format,
opendx, industrial-strength data visualization 497
each corresponding to a different time. To produce the first OpenDX image, lapl000.dat is
copied to data.dat, which is a generic name used for importing data into OpenDX, and the
file lapl.general describing the data is saved: 
f i l e = / home / mpaez / d a t a . d a t g r i d = 50 x 50 f o r m a t = a s c i i
i n t e r l e a v i n g = f i e l d m a j o r i t y = column f i e l d = f i e l d 0 s t r u c t u r e =
s c a l a r t y p e = f l o a t dependency = p o s i t i o n s p o s i t i o n s = r e g u l a r ,
r e g u l a r , 0 , 1 , 0 , 1 end
Next, the Visual Program Editor is used to create the visual program (Figure C.20 right), and the
program is saved as the file lapl.net.
To manipulate the first image so that the rest of them have the same appearance, we
assembled the visual program in Figure C.20.
1. In the Control panel, double-click FileSelector and enter the path and name (lapl.general)
of the .general file saved in the previous step.
2. The images should be plotted to the same scale for the movie to look continuous. This
means you do not want autoscale on. Select AutoColor and expand the box. For min
select --100 and for max select 100. Do the same in RubberSheet. This sets the scale for
all the figures.
3. From the rotate icon, select axis, enter x, and in rotation enter --85. For the next axis
button enter y, and in the next rotation button enter 10.0. These angles ensure a good
view of the figure.
4. To obtain a better figure, on the Scale icon change the default scale from [1 1 1] to [1 4
3].
5. Next the internal DX script dat2image.sh is written by using the Script option.
6. Now we write an image to the disk. Select WriteImage and in the second box enter image
as the name of a generic image. Then for format select tiff to create image.tiff.
7. You now have a script that can be used to convert other data files to images.
loading laplacew
We have scripted the basic DX procedure for producing an image from data. To repeat the
process for all the frames that will constitute a movie, we have written a separate shell script
called dat2image.sh (it is on the CD) that
1. Individually copies each data file to a generic file data.dat, which is what DX uses as
input with lapl.general.
2. Then OpenDX is called via
dx -script lapl.net
to generate images on the disk.
3. The tiff image image.tiff is transformed to the jpg format with names such as
image.001. jpg, image.002. jpg, and so on (on Linux this is done with convert).
4. The Linux program MPlayer comes with an encoder called mencoder that takes the .
jpg files and concatenates them into an .avi movie clip that you can visualize with
Linux programs such as Xine, Kaffeine, and MPlayer. The last-mentioned program can
be downloaded for Windows, but it works only in the System Symbol window or shell:
> mplayer -loop 0 output.avi
You will see the film running continuously. This file is called dat2image.sh and is given
in Listing C1. To run it in Linux,
> sh dat2image.sh
498 appendix c
Listing C.1 The shell script dat2image.sh creates an animation. The script is executed by entering its name at a
shell prompt. 
do
i f t e s t $ i − l t 10
t h e n
cp ${ p r e f i x }00 $ i . d a t d a t a . d a t
dx −s c r i p t l a p l . n e t
c o n v e r t image . t i f f image . 0 0 $ i . j p g
e l i f t e s t $ i − l t 100
t h e n
cp ${ p r e f i x }0 $ i . d a t d a t a . d a t
dx −s c r i p t l a p l . n e t
c o n v e r t image . t i f f image . 0 $ i . j p g
e l i f t e s t $ i − l t 1000
t h e n
cp ${ p r e f i x }$ i . d a t d a t a . d a t
dx −s c r i p t l a p l . n e t
c o n v e r t image . t i f f image . $ i . j p g
f i
i = ‘ exp r $ i + 1 ‘
done
mencoder"mf:// image.*.jpg" −mf f p s =25 −o o u t p u t . a v i −ovc l a v c
−l a v c o p t s vcodec =mpeg4
In each step of this shell script, the script copies a data file, for example, lapl014.dat
in the fifteenth iteration, to the generic file data.dat. It then calls dx in a script option with
the generic file lapl.net. An image image.14.tiff is produced and written to disk. This file
is then transformed to image.14.jpg using the Linux convert utility. Once you have 120
.jpg files, Mplayer is employed to make the .avi animation, which can be visualized with the
same Mplayer software or with any other that accepts .avi format, for example, Kaffeine,
Xine, and MPlayer in Linux. In Windows you have to download MPlayer for Windows, which
works only in the DOS shell (system symbol window). To use our program, you have to edit
lapl.net to use your file names. Ensure that lapl.general and file data.dat are in the same
directory.
C.6.2 Wave Packet and Slit Animation
loading 2slits.mpg
On the left in Figure C.20 we show frames from an OpenDX movie that visualizes the solution
of the Schrödinger equation for a Gaussian wave packet passing through a narrow slit. There
are 140 frames (slit001.dat– slit140.dat), with each generated after five time iterations,
with 45 × 30 data in a column. A separate program generates the data file SlitBarrier.dat
that has only the potential (the barrier with the slit). The images produced by these programs
are combined into one showing the wave packet and the potential. On the left side of the visual
program:
• Double-click FileSelector and enter /home/mpaez/slit.general, the path, and the name
of the file in the Control Panel under Import name.
• Double-click AutoColor and enter 0.0 for min; for max enter 0.8.
• Double-click RubberSheet and repeat the last step for min and max.
• Double-click the left Scale icon and change [111] to [113].
• Double-click WriteImage and write the name of the generic image slitimage and format
= tiff.
On the right side of the visual program:
• Double-click Import and enter the path and the .general file name /home/
opendx, industrial-strength data visualization 499
mpaez/SlitBarrier.general.
• Double-click AutoColor and select 0.0 for min (use expand) and 0.8 for max.
• Double-click the right-hand Scale and enter [1 1 2]. The scale for the barrier is smaller
than the one for the wave packet so that the barrier does not hide too much of the wave
packet.
In this scheme we use the Image icon to manipulate the image to obtain the best view. This
needs to be done just once.
loading 2dsol
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Appendix D
An MPI Tutorial
In this appendix we present a tutorial on the use of MPI on a small Beowulf cluster composed
of Unix or Linux computers.1 This follows our philosophy of “learning while doing.” Our
presentation is meant to help the user from the ground up, something that might not be needed
if you were working at a central computing center with a reasonable level of support. Although
your problem is still to take the program you have written to generate the bifurcation plot for
bug populations and run different ranges of µ values simultaneously on several CPUs, in a
more immediate sense your task is to get the experience of running MPI, to understand some
of the MPI commands within the programs, and then to run a timing experiment. In §D.9 at the
end of the appendix we give a listing and a brief description of the MPI commands and data
types. General information about MPI is given in [MPI], detailed information about the syntax
of MPI commands appears in [MPI2], and other useful material can be found in [MPImis].
The standard reference on the C language is [K&R 88], although we prefer [OR]. MPI is very
much the standard software protocol for parallel computing and is at a higher level than its
predecessor PVM.
WHILE IN THE PAST we have run Java programs with a version of MPI, the difference in
communication protocols used by MPI and Java have led to poor performance or to additional
complications needed to improve performance [Fox 03]. In addition, you usually would not
bother parallelizing a program unless it requires very large amounts of computing time, and
those types of programs are usually written in Fortran or C (both for historical reasons and
because Java is slower). So it makes sense for us to use Fortran or C for our MPI examples.
We will use C because it is similar to Java.
D.1 RUNNING ON A BEOWULF
A Beowulf cluster is a collection of independent computers each with its own memory and
operating system that are connected to each other by a fast communication network over which
messages are exchanged among processors. MPI is a library of commands that make commu-
nication between programs running on the different computers possible. The messages are sent
as data contained in arrays. Because different processors do not directly access the memory on
some other computer, when a variable is changed on one computer, it is not changed automat-
ically in the other copies of the program running on other processors. This is an example of
where MPI comes into play.
In Figure D.1 we show a typical, but not universal, configuration for a Beowulf cluster.
Almost all clusters have the common feature of using MPI for communication among com-
puters and Unix/Linux for the operating system. The cluster in Figure D.1 is shown within a
cloud. The cloud symbolizes the grouping and connection of what are still independent com-
1This material was developed with the help of Kristopher Wieland, Kevin Kyle, Dona Hertel, and Phil Carter. Some of the
other materials derive from class notes from the Ohio Super Computer Center, which were written in part by Steve Gordon.
an mpi tutorial 501
Figure D.1 A schematic view of a cluster (cloud) connected to front-end machines (box).
Front End Machines
SchedSchedSched
2
3
5
4
MPI_COMM_WOLRD1
0
puters communicating via MPI (the lines). The MPI COMM WORLD within the cloud is an MPI
data type containing all the processors that are allowed to communicate with each other (in this
case six). The box in Figure D.1 represents the front end or submit hosts. These are the com-
puters from which users submit their jobs to the Beowulf and later work with the output from
the Beowulf. We have placed the front-end computers outside the Beowulf cloud, although
they could be within. This type of configuration frees the Beowulf cluster from administrative
chores so that it can concentrate on number crunching, and is useful when there are multiple
users on the Beowulf.
Finally, note that we have placed the letters “Sched” within the front-end machines.
This represents a configuration in which these computers are also running some type of a
scheduler, grid engine, or queueing system that oversees the running of jobs submitted to MPI
by a number of users. For instance, if we have a cluster of 20 computers and user A requests
10 machines and user B requests 8 machines, then the grid engine will permit both users to run
simultaneously and assign their jobs to different computers. However, if user A has requested
16 machines and user B 8, then the grid engine will make one of the users wait until the other
finishes their work.
Some setup is required before you can run MPI on several computers at once. If someone
has already done this for you, then you may skip the rest of this section and move on to §D.3.
Our instructions have been run on a cluster of Sun computers running Solaris Unix (in a later
section we discuss how to do this using the Torque scheduler on a Linux system). You will
have to change the computer names and such for your purposes, but the steps should remain
the same.
• First you need to have an active account on each of the computers in the Beowulf cluster.
• Open a shell on one of the machines designated for users to sign onto (the front end).
You do not have to be sitting at the front end but instead can use ssh or telnet. Make the
directory mpi in your home directory:
> cd ˜ Change to home directory
> mkdir output Screen output gets stored here first
> mkdir output/error Place to store error messages
> mkdir mpi A place to store your mpi stuff
• You need to have your Beowulf account configured so that Unix can find the MPI com-
mands that you issue from the command line or from your programs. When you log
onto the computer, the operating system reads a configuration file .cshrc residing in your
home directory. It contains the places where the operating system looks for commands.
(We are assuming here that you are using either cshell or tcshell, if not, then modify
your .login, which should work regardless of the shell.) When a file name begins with
502 appendix d
a dot, it is usually hidden from view when you list the files, but it can be seen with the
command ls -la. The list of places where Unix looks for commands is an environmen-
tal variable called PATH, and it should include the current version of the mpich-n.m/bin
directory where the scripts for MPI reside. For us this is
/usr/local/cluster/mpich-1.2.6/bin This should be in your PATH
Here the directory name cluster and 1.2.6 may need to be replaced by the name and
number on your local system.
• Because the .cshrc file controls your environment, having an error in this file can lead to
a nonfunctional computer. And since the format is rather detailed and unforgiving, it is
easy to make mistakes. So before you work on your existing .cshrc file, make a backup
copy of it:
> cp .cshrc .cshrc bk
You can use this backup file as a reference or copy it back to .cshrc if things get to be
too much of a mess. If you have really messed things up, your system administrator may
have to copy the file back for you.
• Edit your .cshrc file so that it contains a line in which setenv PATH includes
/usr/local/cluster/mpich-1.2.6/bin. If you do not have a .cshrc file, just create
one. Find a line containing setenv PATH and add this in after one of the colons, making
sure to separate the path names with colons. As an example, the .cshrc file for user rubin
is 
# @( # ) c s h r c 1 . 1 1 8 9 / 1 1 / 2 9 SMI umask 022
s e t e n v PATH / u s r / l o c a l / b i n : / o p t / SUNWspro / b i n : / o p t / SUNWrtvc / b i n :
/ o p t / SUNWste / b i n : / u s r / b i n / X11 : / u s r / openwin / b i n : / u s r / d t / b i n : / u s r / ucb / :
/ u s r / c c s / b i n / : / u s r / b i n : / b i n : / u s r / s b i n / : / s b i n :
/ u s r / l o c a l / c l u s t e r / mpich −1 . 2 . 6 / b i n : s e t e n v PAGER l e s s s e t e n v
CLASSPATH / home / r u b i n : / home / r u b i n / dev / j a v a / chapmanjava / c l a s s e s / :
/ home / r u b i n / dev / 5 6 5 / j a v a c o d e / :
/ home / r u b i n / dev / 5 6 5 / c u r r p r o j / : / home / r u b i n : / home / r u b i n / mpiJava :
/ u s r / l o c a l / mpiJava / l i b / c l a s s e s :
s e t prompt="%˜::%m> "
• If you are editing your .login file, enter as the last line in the file:
set path = $path /usr/local/cluster/mpich-1.2.6/bin
• Because dot files are read by the system when you first log on, you will have to log off and
back on for your changes to take effect. (Alternatively, you can use the source command
to avoid logging off and on.) Once you have logged back on, check the values of the PATH
environmental variable:
> echo $PATH
From Unix shell, tells you what Unix thinks
• Let us now take a look at what is done to the computers to have them run as a Beowulf
cluster. On Unix systems the “slash” directory / is the root or top directory. Change the
directory to /
> cd / Change to root directory
You should see files there, such as the kernel and the devices, that are part of the operating
system. You should not modify these files, as that could cause real problems (it is the sort
of thing that hackers and system administrators do).
• MPI is a local addition to the operating system. We have MPI and the Sun Grid
Engine (SGE) in the /usr/local/cluster directory. Here the first / indicates the
root directory and usr is the directory name under the root. Change the directory to
/usr/local/cluster, or wherever MPI is kept on your system, and notice the directories
an mpi tutorial 503
scripts and mpich-1.2.6 (or maybe just a link to mpich). Feel free to explore these di-
rectories. The directory scripts contains various scripts designed to make running your
MPI programs easier. (Scripts are small programs containing shell commands that are
executed in order when the file is run.)
• In the mpich-1.2.6 directory you will notice that there are examples in C, C++, and
Fortran. Feel free to copy these to your home directory and try them:
> cp -r examples /home/userid/mpi
where userid is your name. We encourage you to try out the examples, although some
may need modification to work on your local system.
• Further documentation can be found in
/usr/local/cluster/mpich-1.2.6/doc/mpichman-chp4.pdf MPI documentation
/usr/local/cluster/sge/doc/SGE53AdminUserDoc.pdf SGE documentation
/usr/local/cluster/sge/doc/SGE53Ref.pdf SGE reference
man qstat Manual page on qstat
• Copy the script run mpi.sh from the Codes/MPIcodes directory on the CD to your per-
sonal mpi directory. This script contains the commands needed to run a program on the
cluster.
• Copy the file /usr/local/cluster/mpich/share/machines.solaris to your home di-
rectory and examine it. (The solaris extender is there because we are using the Solaris
version of the Unix operating system on our Beowulf; you may need to change this for
your local system.) This file contains a list of all the computers that are on the Beowulf
cluster and available to MPI for use (though there is no guarantee that all the machines are
operative): 
# Change t h i s f i l e t o c o n t a i n t h e machines t h a t you want t o use
# t o run MPI j o b s on . Format : 1 h o s t p e r l i n e , e i t h e r hos tname
# or hos tname : n , where n i s t h e number o f p r o c e s s o r s .
# hos tname s h o u l d be t h e same as o u t p u t from "hostname" command
p a u l
r o s e
tomek
manuel
D.2 RUNNING MPI
If you are the only one working on a Beowulf cluster, then it may make sense to submit your
jobs directly to MPI. However, if there is the possibility that a number of people may be using
the cluster, or that you may be submitting a number of jobs to the cluster, then it is a good idea
to use some kind of a queue management system to look after your jobs. This can avoid the
inefficiency of having different jobs compete with each other for time and memory or having
the entire cluster “hang” because a job has requested a processor that is not available. In this
section we describe the use of the Sun Grid Engine [SGE]. In a later section we describe the use
of the Torque/Portable Batch System (PBS) scheduler on a Linux system; the two are similar
in purpose and commands, work under many operating systems, and are free.
On the left in Figure D.2 is a schematic view of how a C program containing MPI com-
mands is executed. On the right in this figure is a schematic view of how a scheduling system
takes an executable program and runs it under MPI on several systems. When a program is sub-
mitted to a cluster via a management system, the system installs a copy of the same program
on each computer assigned to run the program.
There are a number of scripts that interpret the MPI commands you give within your
504 appendix d
Figure D.2 Left: A schematic view of the MPI command MPI COMM contained within the C program MPI.c.
On the outer wrapper, the program is compiled with the shell command mpicc, which expands the
MPI commands and invokes the C compiler cc. Right: A schematic view of how a scheduler runs the
executable program MPIpi.o under MPI and on several Linux CPUs.
MPIpi.c
cc MPIpi.c
> mpicc MPIpi.c
MPI_COMM
Linux
Scheduler
Linux
Linux
MPI
MPIpi.o
programs (the commands are not part of the standard Fortran or C language), and then call
the standard compilers. These scripts are called wrappers because they surround the standard
compilers as a way of extending them to include MPI commands:
mpicc C compiler mpicxx C++ compiler
mpif77 Fortran 77 compiler mpif90 Fortran 90 compiler
mpiifort Intel Fortran compilers mpiicc Intel C compiler
Typically you compile your programs on the front end of the Beowulf, or the master machines,
but not on the execution nodes. You use these commands just as you use regular compiler
commands, only now you may include MPI commands in your source program:
> mpicc --o name name.c Compile name.c with MPI wrapper script
> mpif77 --o name name.f Compile name.f with MPI wrapper script
D.2.1 MPI under the SGE Queueing System
Table D.1 lists some of the key number of Sun grid engine commands used to execute compiled
programs. Other queueing systems have similar commands. The usual method of executing
a program at the prompt runs only on the local computer. In order to run on a number of
machines, the program needs to be submitted to the queue management system. We show this
in Listing D.1, which uses the run mpi.sh script and the qsub command to submit jobs to run
in batch mode:
> qsub run mpi.sh name Submit name to run on cluster
This command returns a job ID number, which you should record to keep track of your pro-
gram. Note that in order for this script to work, both runMPI.sh and the program name must
be in the current directory. If you need to pass parameters to your program, place the program
name as well as the parameters in quotes:
an mpi tutorial 505
Table D.1 Some Common SGE Commands
Command Action
qsub myscript Submit batch script or job myscript
qhost Show job/host status
qalter <job id> Change parameters for job in queue
qdel job id Remove job id
qstat Display status of batch jobs
qstat -f Full listing for qstat
qstat -u <username> User only for qstat
qmon X-Window front end (integrated functionality)
> qsub run mpi.sh "name -r 10" Parameters and program name in quotes
Listing D.1 The script runMPI.sh used to run an MPI program. 
# You may want t o modify t h e p a r a m e t e r s f o r
# "-N" ( j o b queue name ) , "-pe" ( queue t y p e and number o f r e q u e s t e d CPUs ) ,
# "myjob" ( your compi l ed e x e c u t a b l e ) .
# You can compi l e you code , f o r example myjob . c (∗ . f ) , w i th GNU mpicc o r
# mpif77 c o m p i l e r s a s f o l l o w s :
# "mpicc -o myjob myjob.c" or "mpif77 -o myjob myjob.f"
# You can m o n i t o r your j o b s wi th command
# "qstat -u your_username" or "qstat -f" t o s e e a l l queues .
# To remove your job , run "qdel job_id"
# To k i l l r u n n i n g job , use "qdel -f job_id"
# −−−−−−A t t e n t i o n : #$ i s a s p e c i a l CODINE symbol , n o t a comment −−−−−
#
# The name , which w i l l i d e n t i f y your j o b i n t h e queue sys tem
#$ −N MPI job
#
# Queue r e q u e s t , mpich . You can s p e c i f y t h e number o f r e q u e s t e d CPUs ,
# f o r example , from 2 t o 3
#$ −pe c l a s s m p i 4−6
#
# −−−−−−−−−−−−−−−−−−−−−−−−−−−
#$ −cwd
#$ −o $HOME/ o u t p u t / $JOB NAME−$JOB ID
#$ −e $HOME/ o u t p u t / e r r o r / $JOB NAME−$JOB ID . e r r o r
#$ −v MPIR HOME=/ u s r / l o c a l / c l u s t e r / mpich−1.2.6
# −−−−−−−−−−−−−−−−−−−−−−−−−−−
echo "Got $NSLOTS slots."
# Don’t modify the line below if you don’ t know what i t i s
$MPIR HOME / b i n / mpirun −np $NSLOTS $1
D.2.1.1 Status of Submitted Programs
After your program is successfully submitted, SGE places it in a queue where it waits for the
requested number of processors to become available. SGE then executes the program on the
cluster and directs the output to a file in the output subdirectory within your home directory.
The program itself uses MPI and C/Fortran commands. In order to check the status of your
submitted program, use qstat along with your job ID number:
> qstat 1263 Tell me the status of Job 1263
job-ID prior name user state submit/start at queue master ja-task-ID
1263 0 Test MPI J dhertel qw 07/20/2005 12:13:51
506 appendix d
This is a typical qstat output. The qw in the state column indicates that the program is in the
queue and waiting to be executed.
> qstat 1263 Same as above, but at later time
job-ID prior name user state submit/start at queue master ja-task-ID
1263 0 Test MPI J dhertel t 07/20/2005 12:14:06 eigen11.q MASTER
1263 0 Test MPI J dhertel t 07/20/2005 12:14:06 eigen11.q SLAVE
1263 0 Test MPI J dhertel t 07/20/2005 12:14:06 eigen12.q SLAVE
1263 0 Test MPI J dhertel t 07/20/2005 12:14:06 eigen3.q SLAVE
1263 0 Test MPI J dhertel t 07/20/2005 12:14:06 eigen5.q SLAVE
1263 0 Test MPI J dhertel t 07/20/2005 12:14:06 eigen8.q SLAVE
Here the program has been assigned a set of nodes (eigenN is the name of the computers), with
the last column indicating whether that node is a master, host, slave, or guest (to be discussed
further in §D.3.1). At this point the state column will have either a t, indicating transfer, or
an r, indicating running.
The output from your run is sent to the file Test MPI.<jobID>.out in the output sub-
directory within your home directory. Error messages are sent to a corresponding file in the
error subdirectory. Of course you can still output to a file in the current working directory, as
well as input from a file.
D.2.2 MPI Under the Torque/PBS Queueing System
Most Beowulf clusters use Linux, a version of the Unix operating system that runs on PC ar-
chitecture. A popular, commercially supported version of Linux that runs well for CPUs using
64-bit words is SUSE [SUSE]. We have used this setup with MPI libraries, Intel compilers, and
the cluster edition of the Math Kernel Library [Intel]. Although we could run the SGE sched-
uler and resource manager on this system, the compilers come with the Torque open source
resource manager [Torque], which works quite well. Torque is based on and uses the same
commands as Portable Batch System [PBS]. In this section we give a tutorial on the use of
Torque for submitting jobs to MPI.2 As we shall see, the steps and commands are very similar
to those for SGE and follow the system outlined in Figure D.2.
D.2.2.1 Running Serial Jobs with Torque
Sometimes you may have a serial job that runs too long for you to wait for its completion,
or maybe you want to submit a number of long jobs and not have them compete with each
other for resources. Either case can be handled by using the queueing system usually used for
parallel processing, only now with multiple jobs on just one computer. In this case there are
no MPI commands to deal with, but just three torque commands and a shell script that initiates
your program:
qsub Submit jobs to queue via Torque
qstat Check status of jobs in queue
qdel Delete a submitted job
script Shell script that initiates program
2We thank Justin Elser for setting up this system and for preparing the original form of this tutorial.
an mpi tutorial 507
Note that you cannot give Torque a complied binary program to run but rather just a shell
script3 that calls your executable. This is probably for security and reliability. For example:
> icc SerProg.c --o SerProg Intel C Compiler, out to SerProg
> qsub SerProg Submit SerProg to queue
qsub: file must be an ascii script Torque’s output
> qsub script1 Submit a script that calls SerProg
JobID = 12; output from SerProg Successful submission
Here is a simple script1 for initiating the serial job in the file SerProg (you should copy this
into a file so you can use it): 
# ! / b i n / bash
cd \$PBS O WORKDIR
. / Se rP rog
Observe the #!/bin/bash statement at the beginning of the script. A statement of this form
is required to tell the operating system which shell (command line interpreter) to use. This
line is for the bash shell, with other choices including tcsh and ksh. The next command
cd $PBS O WORKDIR (which you should not modify) sets a PBS environmental variable so that
the script looks in the current working directory. The last command, ./SerProg, contains the
name of the file you wish to execute and is the only line you should modify. (The ./ means the
current directory, but you can also use ../SerProg for a file one directory up.)
As we indicated before, once you have submitted a job to Torque, you can log off (to get
a mug of your favorite beverage) and the job will remain in the compute queue or continue to
run. Alternatively, you may want to submit the job to a cluster machine different from the one
on which you are working, log off from that one, and then continue working on your present
machine. The job will run remotely with other users still able to run on that remote machine.
(Other than a possible slowdown in speed, they may not even realize that they are sharing it
with you.) Before you log off, it is probably a good idea to determine the status of your jobs in
the queue. This is done with the qstat command:
> qstat What is queue’s status?
JobID Name User Time Use S Queue Reply
170.phy ScriptName Justin 0 R batch
> qdel 170 Delete (kill) my job
This output indicates that Justin’s job has id 170 (the .phy indicates the server phy), that it
was submitted via the script ScriptName, that it is in the state R for running (Q if queued, E if
executing), and that it is in the batch queue (default). You can delete your job by issuing the
qdel command with your JobID.
If your program normally outputs to a file, then it will still output to that file un-
der Torque. However, if your program outputs to stdout (the screen), the output will
be redirected to the file ScriptName.oJobID and any errors will be redirected to the file
ScriptName.eJobID. Here ScriptName is the shell script you used to submit the job, and
JobID is the ID given to it by Torque. For example, here is the output from the long list com-
mand ll:
> ll Unix long list command
total 32
-rwxr-xr-x 1 q users 17702 2006-11-15 16:38 SerProg
3Recall that a shell script is just a file containing commands that would otherwise be entered at a shell’s prompt. When the file
name is given execution permission and entered as a command, all the commands within the file are executed.
508 appendix d
-rw-r--r-- 1 q users 1400 2006-11-15 16:38 SerProg.c
-rw-r--r-- 1 q users 585 2006-11-15 16:39 ScriptName
-rw------- 1 q users 0 2006-11-15 16:39 ScriptName.e164
-rw------- 1 q users 327 2006-11-15 16:39 ScriptName.o164
D.2.3 Running Parallel Jobs with Torque
The basic Torque steps for submitting a script and ascertaining its status are the same for
parallel jobs as for serial jobs, only now the script must have more commands in it and must
call MPI. The first thing that must be done is to create a Multiprocessors Daemon (MPD)
secretword (not your password) so that Torque can keep multiple jobs separate from all the
others:
> cd $HOME Dotfiles stored in your home directory
> echo "MPD SECRETWORD=MySecretword" >> .mpd.conf Replace MySecretword
> chmod 600 .mpd.conf Change permission for file .mpd.conf
Note that you do not really need to remember your secretword because Torque uses it “behind
the scenes”; you just need to have it stored in the file .mpd.conf in your home directory. (This
file is not normally visible because it begins with a dot.)
Listing D.2 The script TorqueScript.sh used to submit an MPI program. 
# ! / b i n / bash
#
# A l l l i n e s s t a r t i n g wi th "#PBS" a r e PBS commands
#
# Reques t 2 nodes wi th 2 p r o c e s s o r p e r node ( ppn ) (= 4 p r o c e s s o r s )
# ppn can e i t h e r be 1 or 2
#
#PBS −l nodes =2: ppn=2
#
# S e t w a l l c l o c k max t ime t o 0 hours , 15 m i n u t e s and 0 s e c o n d s
#PBS −l w a l l t i m e = 0 0 : 1 5 : 0 0
#
# cd t o working d i r e c t o r y
cd $PBS O WORKDIR
# name of e x e c u t a b l e
myprog=MPIpi
#
# Number o f p r o c e s s o r s i s $NP
NP=4
#
# Run MYPROG wi th a p p r o p r i a t e mpirun s c r i p t
mpirun −r s s h −n $NP $myprog
#
# make s u r e t o e x i t t h e s c r i p t , e l s e j o b won’t finish properly
exit 0
In Listing D.2and on the CD we give the script TorqueScript.sh used to submit MPI
jobs to the Torque scheduler. The script is submitted to Torque from a shell via the qsub
command:
> qsub TorqueScript.sh
Observe again that the script must start with the line #!/bin/bash, to indicate which shell it
should run under, and that the lines beginning with #PBS are commands to the Torque sched-
uler, not comments! The lines
myprog=MPIpi
mpirun -r ssh -n $NP $myprog
in the script run the compiled version of the program MPIpi.c, which is also on the CD. Here
NP is the total number of processors you want to run on, and its value is written into the script.
an mpi tutorial 509
You will need to change MPIpi to the name of the compiled program you wish to run. (As
long as the line $PBS O WORKDIR precedes this one, Torque knows that your program is in the
working directory.) The line
#PBS -l nodes=2:ppn=2
tells Torque to reserve two computers (nodes) with two processors per node (ppn) for a total
of four processors. This value of ppn is appropriate for dual-core computers with two CPUs
on their chips. If you have computers with four cores on each chip, then you should set ppn=4.
If you want to use only one processor per machine, for example, to gauge the speedup from
multiple cores, then set ppn=1. Even though we have reserved nodes*ppn processors for Torque
to use, the actual number of processors used by MPI is given by the variable NP in the call
mpirun -r ssh -n NP myprog
Accordingly, we must set the value for NP as ≤nodes*ppn within the script. The maximum
wall clock time that your job can run is set to 15 min via
#PBS -l walltime=00:15:00
This is actual run time, not the total time in the queue, and so this clock does not start ticking
until the job starts executing. In general it is a good idea to use a walltime command with about
twice the time you expect your job to run just in case something goes wrong. (Not only is this
a nice thing to do for others, but it can also keep you from wasting a finite resource or waiting
around forever.) Next observe that the mpirun command, which starts MPI, has the argument
-r ssh. This is required on our installation for the machines to be able to communicate with
each other using ssh and scp rather than the default rsh and rcp. The latter are less secure.
Finally, the script ends with exit 0. This gives the script exit status 0 and thus provides a
graceful ending. Graceless endings may not give the operating system enough time to clear
your output files from buffers, which means that you may not see all your output!
D.3 YOUR FIRST MPI PROGRAM: MPIHELLO.C
Listing D.3 gives the simple MPI program MPIhello.c. It has each of the processors print
Hello World, followed by the processor’s rank. Compile MPIhello.c using:
> mpicc MPIhello.c -o hello Compilation via compiler wrapper
After successful compilation, an executable file hello should be in the directory in which you
did the compilation. The program is executed via the script run mpi.sh either directly or by
use of the management command qsub:
> run mpi.sh hello Run directly under MPI
> qsub run mpi.sh hello Run under management system
This script sets up the running of the program on the 10 processors, with processor 0 the host
and processors 1–9 the guests.
Listing D.3 MPIhello.c gets each processor to say hello via MPI. 
/ / MPIhello . c has each p r o c e s s o r p r i n t s h e l l o to sc re en
# i n c l u d e "mpi.h"
# i n c l u d e <s t d i o . h>
i n t main ( i n t argc , char ∗a rgv [ ] ) {
i n t myrank ;
M P I I n i t ( &argc , &argv ) ; / / I n i t i a l i z e MPI
MPI Comm rank ( MPI COMM WORLD, &myrank ) ; / / Get CPU’ s rank
p r i n t f ( "Hello World from processor %d\n" , myrank ) ;
M P I F i n a l i z e ( ) ; / / F i n a l i z e MPI
r e t u r n 0 ;
}
510 appendix d
D.3.1 MPIhello.c Explained
Here is what is contained in MPIhello.c:
• The inclusion of MPI headers via the #include "mpi.h" statement on lines 2–3. These
are short files that assist the C compiler by telling it the type of arguments that MPI
functions use for input and output without giving any details about the functions. (In
Fortran we used include "/usr/local/cluster/mpich-2.1.6/include/mpif.h" after
the program line.)
• The main method is declared with an int main(int argc, char *argv[]) statement,
where argc is a pointer to the number of arguments and argv is a pointer to the argument
vector passed to main when you run the program from a shell. (Pointers are variable
types that give the locations in memory where the values of the variables reside rather
than the variables’ actual values.) These arguments are passed to MPI to tell it how many
processors you desire.
• The int myrank statement declares the variable myrank, which stands for the rank of the
computer. Each processor running the program is assigned a unique number called its
rank by MPI. This is how you tell the difference among identical programs running on
different CPUs.
• The processor that executes the highest level of the program is called the host or master,
and all other machines are called guests or slaves. The host always has myrank = 0,
while all the other processors, based on who responds first, have their processor numbers
assigned to myrank. This means that myrank = 1 for the first guest to respond, 2 for the
second, and so on. Giving each processor a unique value for myrank is a critical element
in parallel processing.
• The MPI init() and MPI Finalize() commands in MPIhello.c initialize and terminate
MPI, respectively. All MPI programs must have these lines, with the MPI commands
always placed between them. The MPI Init(&argv, &argc) function call takes two ar-
guments, both beginning with a & that indicates a pointer. These arguments are used for
communication between the operating system and MPI.
• The MPI Comm rank(MPI COMM WORLD, &myrank) call returns a different value for rank for
each processor running the program. The first argument is a predefined constant telling
MPI which grouping of processors to communicate with. Unless you have set up groups
of processors, just use the default communicator MPI COMM WORLD. The second argument
is an integer that is returned with the rank of the individual program.
When MPIhello.c is executed, each processor prints its rank to the screen. Notice that
it does not print the ranks in order and that the order will probably be different each time you
run the program. Take a look at the output (in the file output/MPI job-xxxx). It should look
something like this: 
"Hello, world!" from node 3 of 4 on e i g e n 3 . s c i e n c e . o r e g o n s t a t e . l o c a l
"Hello, world!" from node 2 of 4 on e i g e n 2 . s c i e n c e . o r e g o n s t a t e . l o c a l
Node 2 r e p o r t i n g
"Hello, world!" from node 1 of 4 on e i g e n 1 . s c i e n c e . o r e g o n s t a t e . l o c a l
"Hello, world!" from node 0 of 4 on e i g e n 1 1 . s c i e n c e . o r e g o n s t a t e . l o c a l
If the processing order matters for proper execution, call MPI Barrier( MPI COMM WORLD ) to
synchronize the processors. It is similar to inserting a starting line at a relay race; a processor
stops and waits at this line until all the other processors reach it, and then they all set off at the
same time. However, modern programming practice suggests that you try to design programs
so that the processors do not have to synchronize often. Having a processor stop and wait
an mpi tutorial 511
Argument Name Use in MPI Send and MPI Recv
msg Pointer (& in front) to array to be sent/received
msg size Size of array sent; may be bigger than actual size
MPI TYPE Predefined constant indicating variable type within array,
other possible constants: MPI INTEGER, MPI DOUBLE
dest Rank of processor receiving message
tag Number that uniquely identifies a message
comm A communicator, for example, predefined constant
MPI COMM WORLD
source Rank of processor sending message; if receiving messages
from any source, use predefined constant MPI ANY SOURCE
status Pointer to variable type MPI Status containing status info
obviously slows down the number crunching and consequently removes some of the advantage
of parallel computing. However, as a scientist it is more important to have correct results than
fast ones, and so do not hesitate to insert barriers if needed.
Exercise: Modify MPIhello.c so that only the guest processors say hello. Hint: What do the
guest processors all have in common?
D.3.2 Send/Receive Messages: MPImessage2.c
Sending and receiving data constitute the heart of parallel computing. Guest processors need
to transmit the data they have processed back to the host, and the host has to assemble the data
and then assign new work to the guests. An important aspect of MPI communication is that if
one processor sends data, another processor must receive those data. Otherwise, the sending
processor may wait indefinitely for a signal that its data have been received or the receiving
processor may wait indefinitely until it receives the data it is programmed to expect.
Listing D.4 MPImessage2.c uses MPI commands to both send and receive messages. Note the possibility of block-
ing, in which the program waits for a message. 
/ / MPImessage2 . c : source node sends message to d e s t
# i n c l u d e "mpi.h"
# i n c l u d e <s t d i o . h>
i n t main ( i n t argc , char ∗a rgv [ ] ) {
i n t rank , m s g s i z e = 6 , t a g = 10 , s o u r c e = 0 , d e s t = 1 ;
M P I S t a t u s s t a t u s ;
M P I I n i t ( &argc , &argv ) ; / / I n i t i a l i z e MPI
MPI Comm rank ( MPI COMM WORLD, &rank ) ; / / Get CPU’ s rank
i f ( r ank == s o u r c e ) {
char ∗msg = "Hello" ;
p r i n t f ("Host about to send message: %s\n" , msg ) ; / / Send , may block t i l l r e c i e v e d
MPI Send ( msg , msg s i ze , MPI CHAR , d e s t , t ag , MPI COMM WORLD ) ;
}
e l s e i f ( r ank == d e s t ) {
char b u f f e r [ m s g s i z e + 1 ] ; / / Rece ive
MPI Recv ( &b u f f e r , msg s i ze , MPI CHAR , sou rce , t ag , MPI COMM WORLD, &s t a t u s ) ;
p r i n t f ("Message recieved by %d: %s\n" , rank , b u f f e r ) ;
}
p r i n t f ("NODE %d done.\n" , r ank ) ; / / A l l nodes p r i n t
M P I F i n a l i z e ( ) ; / / F i n a l i z e MPI
r e t u r n 0 ;
}
512 appendix d
There is a basic MPI command MPI Send to send a message from a source node, and another
basic command MPI Recv is needed for a destination node to receive it. The message itself
must be an array even if there is only one element in the array. We see these commands in use
in MPImessage2.c in Listing D.4. This program accomplishes the same thing as MPIhello.c
but with send and receive commands. The host sends the message and prints out a message,
while the guests print out when they receive a message. The forms of the commands are
MPI Send(msg, msg size, MPI TYPE, dest, tag, MPI COMM WORLD); Send
MPI Recv(msg, msg size, MPI TYPE, source, tag, comm, status); Receive
The arguments and their descriptions are given in §D.3.2. The criteria for successfully sending
and receiving a message are
1. The sender must specify a valid destination rank, and the processor with that rank must
call MPI recv.
2. The receiver must specify a valid source rank or MPI ANY SOURCE.
3. The send and receive communicators must be the same.
4. The tags must match.
5. The receiver’s message array must be large enough to hold the array.
Exercise: Modify MPImessage2.c so that all processors say hello.
D.3.3 Receive More Messages: MPImessage3.c
Listing D.5 MPImessage3.c contains MPI commands that have each guest processor send a message to the host
processor who then prints out the rank of that guest. 
/ / MPImessage3 . c : g u e s t s send rank to the host , who p r i n t s them
# i n c l u d e "mpi.h"
# i n c l u d e <s t d i o . h>
i n t main ( i n t argc , char ∗a rgv [ ] ) {
i n t rank , s i z e , m s g s i z e = 6 , t a g = 10 , h o s t = 0 , n [ 1 ] , r [ 1 ] , i ; / / 1−D Arrays
M P I S t a t u s s t a t u s ;
M P I I n i t ( &argc , &argv ) ; / / I n i t i a l i z e MPI
MPI Comm rank ( MPI COMM WORLD, &rank ) ; / / Get CPU’ s rank
MPI Comm size ( MPI COMM WORLD, &s i z e ) ; / / Get number of CPUs
i f ( r ank != h o s t ) {
n [ 0 ] = rank ;
p r i n t f ("node %d about to send message\n" , r ank ) ;
MPI Send ( &n , 1 , MPI INTEGER , hos t , t ag , MPI COMM WORLD ) ;
}
e l s e {
f o r ( i = 1 ; i < s i z e ; i ++ ) {
MPI Recv ( &r , 1 , MPI INTEGER , MPI ANY SOURCE , tag , MPI COMM WORLD, &s t a t u s ) ;
p r i n t f ("Message recieved: %d\n" , r [ 0 ] ) ; }
}
M P I F i n a l i z e ( ) ; / / F i n a l i z e MPI
r e t u r n 0 ;
}
A bit more advanced use of message passing is shown by MPImessage3.c in Listing D.5. Here
each guest sends a message to the host who then prints out the rank of the guest that sent the
message. The host loops through all the guests since otherwise it would stop looking for more
messages after the first one arrives. The host calls MPI Comm size to determine the number of
processors.
an mpi tutorial 513
D.3.4 Broadcast Messages
If we used the same technique to send a message from one node to several other nodes, we
would have to loop over calls to MPI Send. In MPIpi.c in Listing D.6, we see an easy way to
send a message to all the other nodes.
Listing D.6 MPIpi.c uses a number of processors to compute π by a Monte Carlo rejection (stone throwing). 
/ / MPIpi . c computes p i in p a r a l l e l by s t o n e throwing
# i n c l u d e "mpi.h" # i n c l u d e <s t d i o . h> # i n c l u d e <math . h>
do ub l e f ( do ub l e ) ;
i n t main ( i n t argc , char ∗a rgv [ ] ) {
i n t n , myid , numprocs , i , namelen ;
do ub l e PI25DT = 3.141592653589793238462643;
do ub l e mypi , p i , h , sum , x , s t a r t w t i m e = 0 . , endwtime ;
char p r o c e s s o r n a m e [ MPI MAX PROCESSOR NAME ] ;
M P I I n i t ( &argc , &argv ) ;
MPI Comm size ( MPI COMM WORLD, &numprocs ) ;
MPI Comm rank ( MPI COMM WORLD, &myid ) ;
M P I G e t p r o c e s s o r n a m e ( p r o c e s s o r n a m e , &namelen ) ;
f p r i n t f ( s t d o u t ,"Process %d of %d is on %s\n" , myid , numprocs , p r o c e s s o r n a m e ) ;
f f l u s h ( s t d o u t ) ;
n = 10000 ; / / D e f a u l t # o f r e c t a n g l e s
i f ( myid == 0 ) s t a r t w t i m e = MPI Wtime ( ) ;
MPI Bcast ( &n , 1 , MPI INT , 0 , MPI COMM WORLD ) ;
h = 1 . / ( do ub l e ) n ;
sum = 0 . ;
f o r ( i = myid + 1 ; i <= n ; i += numprocs ) { / / B e t t e r i f worked back
x = h ∗ ( ( do ub l e ) i − 0 . 5 ) ;
sum += f ( x ) ;
}
mypi = h ∗ sum ;
MPI Reduce ( &mypi , &pi , 1 , MPI DOUBLE , MPI SUM , 0 , MPI COMM WORLD ) ;
i f ( myid == 0) {
endwtime = MPI Wtime ( ) ;
p r i n t f ("pi is approximately %.16f, Error is %.16f\n" , p i , f a b s ( p i − PI25DT ) ) ;
p r i n t f ("wall clock time = %f\n" , endwtime−s t a r t w t i m e ) ;
f f l u s h ( s t d o u t ) ;
}
M P I F i n a l i z e ( ) ;
r e t u r n 0 ;
}
do ub l e f ( do ub l e a ) { r e t u r n ( 4 . / ( 1 . + a∗a ) ) ;} / / Funct ion f ( a )
This simple program computes π in parallel using the Monte Carlo “stone throwing” technique
discussed in Chapter 5, “Monte Carlo Simulation.” Notice the new MPI commands:
• MPI Wtime is used to return the wall time in seconds (the time as given by a clock on the
wall). This is useful when computing speedup curves (Figure D.3).
• MPI Bcast sends out data from one processor to all the others. In our case the host broad-
casts the number of iterations to the guests, which in turn replace their current values of n
with the one received from the host.
• MPI Allreduce is a glorified broadcast command. It collects the values of the variable
mypi from each of the processors, performs an operation on them with MPI SUM, and then
broadcasts the result via the variable pi.
D.3.5 Exercise
On the left in Figure D.3 we show our results for the speedup obtained by calculating π in
parallel with MPIpi.c. This exercise leads you through the steps required to obtain your own
speedup curve:
514 appendix d
Figure D.3 Execution time versus number of processors. Left: For the calculation of π with MPIpi.c. Right: For
the solution of an eigenvalue problem with TuneMPI.c. Note that the single-processor result here does
not include the overhead for running MPI.
0
40
80
120
160
10
Number of Processors
MPIpi.c
E
x
e
c
u
ti
o
n
T
im
e
(s
e
c
)
1 2 3 4 5 6 7 8 9 0 5 10 15 20
Number of Processors
0
50
100
150
200
250
300
E
x
e
c
u
ti
o
n
T
im
e
TuneMPI
1. Two versions of a parallel program are possible. In the active host version the host acts
just like a guest and does some work. In the lazy host version the host does no work but
instead just controls the action. Does MPIpi.c contain an active or a lazy host? Change
MPIpi.c to the other version and record the difference in execution times.
2. Make a plot of the time versus the number of processors required for the calculation of
π.
3. Make a speedup plot, that is, a graph of the computation time divided by the time for one
processor versus the number of processors.
4. Record how long each of your runs takes and how accurate the answers are. Does round-
off error enter in? What could you do to get a more accurate value for π?
D.4 PARALLEL TUNING
Recall the Tune program with which we experimented in Chapter 14, “High-Performance Com-
puting Hardware, Tuning, and Parallel Computing,” to determine how memory access for a
large matrix affects the running time of programs. You may also recall that as the size of the
matrix was made larger, the execution time increased more rapidly than the number of opera-
tions the program had to perform, with the increase coming from the time it took to transfer
the needed matrix elements in and out of central memory.
Because parallel programming on a multiprocessor also involves a good deal of data
transfer, the Tune program is also a good teaching tool for seeing how communication costs
affect parallel computations. Listing D.7 gives the program TuneMPI.c, which is a modified
version of the Tune program in which each row of the large-matrix multiplication is performed
an mpi tutorial 515
on a different processor using MPI:
[H]N×N × [Ψ]N×1 =

⇒ rank 1 ⇒
⇒ rank 2 ⇒
⇒ rank 3 ⇒
⇒ rank 1 ⇒
⇒ rank 2 ⇒
⇒ rank 3 ⇒
⇒ rank 1 ⇒
. . .

N×N
×

ψ1 ⇓
ψ2 ⇓
ψ3 ⇓
ψ4 ⇓
ψ5 ⇓
ψ6 ⇓
ψ7 ⇓
. . .

N×1
. (D.4.1)
Here the arrows indicate how each row of H is multiplied by the single column of Ψ, with
the multiplication of each row performed on a different processor (rank). The assignment of
rows to processors continues until we run out of processors, and then it starts all over again.
Since this multiplication is repeated for a number of iterations, this is the most computationally
intensive part of the program, and so it makes sense to parallelize it.
On the right in Figure D.3 is the speedup curve we obtained by running TuneMPI.c.
However, even if the matrix is large, the Tune program is not computationally intensive enough
to overcome the cost of communication among nodes inherent in parallel computing. Con-
sequently, to increase computing time we have inserted an inner for loop over k that takes
up time but accomplishes nothing (we’ve all had days like that). Slowing down the program
should help make the speedup curve more realistic.
Listing D.7 The C program TuneMPI.c is a parallel version of Tune.py, which we used to test the effects of
various optimization modifications. 
/∗ TuneMPI . c : a matrix a lgebra program to be tuned f o r performace
N X N Matrix speed t e s t s us ing MPI ∗ /
# i n c l u d e "mpi.h"
# i n c l u d e <s t d i o . h>
# i n c l u d e <t ime . h>
# i n c l u d e <math . h>
i n t main ( i n t argc , char ∗a rgv [ ] ) {
M P I S t a t u s s t a t u s ;
t i m e t s y s t i m e i , s y s t i m e f ;
i n t N = 200 , MAX = 15 , h = 1 , myrank , nmach , i , j , k , i t e r = 0 ;
l ong d i f f t i m e = 0 l ;
do ub l e ERR = 1 . 0 e−6, dummy = 2 . , t imempi [ 2 ] , ham [N] [N] , c o e f [N] , s igma [N ] ;
do ub l e e n e r [ 1 ] , e r r [ 1 ] , ov lp [ 1 ] , mycoef [ 1 ] , mysigma [ 1 ] , myener [ 1 ] , myerr [ 1 ] ;
do ub l e myovlp [ 1 ] , s t e p = 0 . 0 ;
/ / MPI I n i t i a l i z a t i o n
M P I I n i t (& argc , &argv ) ;
MPI Comm rank ( MPI COMM WORLD, &myrank ) ;
MPI Comm size ( MPI COMM WORLD, &nmach ) ;
M P I B a r r i e r ( MPI COMM WORLD ) ;
i f ( myrank == 0 ) {
t imempi [ 0 ] = MPI Wtime ( ) ; / / S tore i n i t i a l t ime
s y s t i m e i = t ime (NULL) ;
}
p r i n t f ("\n\t Processor %d checking in...\n" , myrank ) ;
f f l u s h ( s t d o u t ) ;
f o r ( i = 1 ; i < N; i ++ ) { / / Se t up Hamiltonian and s t a r t i n g v e c t o r
f o r ( j = 1 ; j < N; j ++ ) {
i f ( abs ( j−i ) > 10 ) ham [ j ] [ i ] = 0 . 0 ;
e l s e ham [ j ] [ i ] = pow ( 0 . 3 , abs ( j−i ) ) ;
}
ham [ i ] [ i ] = i ;
c o e f [ i ] = 0 . 0 ;
}
c o e f [ 1 ] = 1 . 0 ;
e r r [ 0 ] = 1 . 0 ;
i t e r = 0 ;
i f ( myrank == 0 ) { / / S t a r t i t e r a t i n g towards the s o l u t i o n
516 appendix d
p r i n t f ( "\nIteration #\tEnergy\t\tERR\t\tTotal Time\n" ) ;
f f l u s h ( s t d o u t ) ;
}
w h i l e ( i t e r < MAX && e r r [ 0 ] > ERR ) { / / S t a r t whi l e loop
i t e r = i t e r + 1 ;
mycoef [ 0 ] = 0 . 0 ;
e n e r [ 0 ] = 0 . 0 ; myener [ 0 ] = 0 . 0 ;
ov lp [ 0 ] = 0 . 0 ; myovlp [ 0 ] = 0 . 0 ;
e r r [ 0 ] = 0 . 0 ; myerr [ 0 ] = 0 . 0 ;
f o r ( i = 1 ; i < N; i ++) {
h = ( i n t ) ( i ) %(nmach−1)+1 ;
i f ( myrank == h ) {
myovlp [ 0 ] = myovlp [ 0 ] + c o e f [ i ]∗ c o e f [ i ] ;
mysigma [ 0 ] = 0 . 0 ;
f o r ( j = 1 ; j < N; j ++ ) mysigma [ 0 ] = mysigma [ 0 ] + c o e f [ j ]∗ham [ j ] [ i ] ;
myener [ 0 ] = myener [ 0 ] + c o e f [ i ]∗mysigma [ 0 ] ;
MPI Send ( &mysigma , 1 , MPI DOUBLE , 0 , h , MPI COMM WORLD ) ;
}
i f ( myrank == 0 ) {
MPI Recv ( &mysigma , 1 , MPI DOUBLE , h , h , MPI COMM WORLD, &s t a t u s ) ;
s igma [ i ]= mysigma [ 0 ] ;
}
} / / End of f o r ( i . . .
MPI Al l r educe ( &myener , &ener , 1 , MPI DOUBLE , MPI SUM , MPI COMM WORLD ) ;
MPI Al l r educe ( &myovlp , &ovlp , 1 , MPI DOUBLE , MPI SUM , MPI COMM WORLD ) ;
MPI Bcast ( &sigma , N−1, MPI DOUBLE , 0 , MPI COMM WORLD ) ;
e n e r [ 0 ] = e n e r [ 0 ] / ( ov lp [ 0 ] ) ;
f o r ( i = 1 ; i< N; i ++) {
h = ( i n t ) ( i ) %(nmach−1)+1 ;
i f ( myrank == h ) {
mycoef [ 0 ] = c o e f [ i ] / s q r t ( ov lp [ 0 ] ) ;
mysigma [ 0 ] = sigma [ i ] / s q r t ( ov lp [ 0 ] ) ;
MPI Send ( &mycoef , 1 , MPI DOUBLE , 0 , nmach+h +1 , MPI COMM WORLD ) ;
MPI Send ( &mysigma , 1 , MPI DOUBLE , 0 , 2∗nmach+h +1 , MPI COMM WORLD ) ;
}
i f ( myrank == 0) {
MPI Recv ( &mycoef , 1 , MPI DOUBLE , h , nmach+h +1 , MPI COMM WORLD, &s t a t u s ) ;
MPI Recv ( &mysigma , 1 , MPI DOUBLE , h , 2∗nmach+h +1 , MPI COMM WORLD, &s t a t u s ) ;
c o e f [ i ]= mycoef [ 0 ] ;
s igma [ i ]= mysigma [ 0 ] ;
}
} / / End of f o r ( i . . .
MPI Bcast ( &sigma , N−1, MPI DOUBLE , 0 , MPI COMM WORLD ) ;
MPI Bcast ( &coef , N−1, MPI DOUBLE , 0 , MPI COMM WORLD ) ;
f o r ( i = 2 ; i < N ; i ++ ) {
h = ( i n t ) ( i ) %(nmach−1) +1;
i f ( myrank == h ) {
s t e p = ( sigma [ i ] − e n e r [ 0 ]∗ c o e f [ i ] ) / ( e n e r [0]−ham [ i ] [ i ] ) ;
mycoef [ 0 ] = c o e f [ i ] + s t e p ;
myerr [ 0 ] = myerr [ 0 ] + pow ( s t e p , 2 ) ;
f o r ( k= 0 ; k <= N∗N; k++ ) / / Slowdown loop
{ dummy = pow (dummy , dummy) ; dummy = pow ( dummy , 1 . 0 / dummy) ; }
MPI Send ( &mycoef , 1 , MPI DOUBLE , 0 , 3∗nmach+h +1 , MPI COMM WORLD ) ;
} / / end of i f ( myrank . .
i f ( myrank == 0) {
MPI Recv(&mycoef , 1 , MPI DOUBLE , h ,3∗ nmach+h +1 , MPI COMM WORLD, &s t a t u s ) ;
c o e f [ i ]= mycoef [ 0 ] ;
}
} / / End of f o r ( i . . .
MPI Bcast ( &coef , N−1, MPI DOUBLE , 0 , MPI COMM WORLD ) ;
MPI Al l r educe ( &myerr , &e r r , 1 , MPI DOUBLE , MPI SUM , MPI COMM WORLD ) ;
e r r [ 0 ] = s q r t ( e r r [ 0 ] ) ;
i f ( myrank==0 ) { p r i n t f ("\t#%d\t%g\t%g\n" , i t e r , e n e r [ 0 ] , e r r [ 0 ] ) ;
f f l u s h ( s t d o u t ) ; }
} / / End whi le
i f ( myrank == 0) {
s y s t i m e f = t ime (NULL) ; / / Output e l a p s e d time
d i f f t i m e = ( ( l ong ) s y s t i m e f ) − ( ( l ong ) s y s t i m e i ) ;
p r i n t f ( "\n\tTotal wall time = %d s\n" , d i f f t i m e ) ;
f f l u s h ( s t d o u t ) ;
t imempi [ 1 ] = MPI Wtime ( ) ;
p r i n t f ("\n\tMPItime= %g s\n" , ( t imempi [1]− t imempi [ 0 ] ) ) ;
f f l u s h ( s t d o u t ) ;
}
M P I F i n a l i z e ( ) ;
}
an mpi tutorial 517
D.4.0.1 TuneMPI.c Exercise
1. Compile TuneMPI.c:
> mpicc TuneMPI.c -lm -o TuneMPI Compilation
Here -lm loads the math library and -o places the object in TuneMPI. This is the base
program. It will use one processor as the host and another one to do the work.
2. To determine the speedup with multiple processors, you need to change the run mpi.sh
script. Open it with an editor and find a line of the form
#$ -pe class mpi 1-4 A line in run mpi.sh script
The last number on this line tells the cluster the maximum number of processors to use.
Change this to the number of processors you want to use. Use a number from 2 to 10;
starting with one processor leads to an error message, as that leaves no processor to do
the work. After changing run mpi.sh, run the program on the cluster. With the SEG
management system this is done via
> qsub run mpi.sh TuneMPI Submit to queue via SGE
3. You are already familiar with the scalar version of the Tune program. Find the scalar
version of Tune.c (and add the extra lines to slow the program down) or modify the
present one so that it runs on only one processor. Run the scalar version of TuneMPI
and record the time it takes. Because there is overhead associated with running MPI, we
expect the scalar program to be faster than an MPI program running on a single processor.
4. Open another window and watch the processing of your MPI jobs on the host computer.
Check that all the temporary files are removed.
5. You now need to collect data for a plot of running time versus number of machines. Make
sure your matrix size is large, say, with N=200 and up. Run TuneMPI on a variable number
of machines, starting at 2, until you find no appreciable speedup (or an actual slowdown)
with an increasing number of machines.
6. Warning: While you will do no harm running on the Beowulf when others are also
running on it, in order to get meaningful, repeatable speedup graphs, you should have the
cluster all to yourself. Otherwise, the time it takes to switch jobs around and to set up
and drop communications may slow down your runs significantly. A management system
should help with this. If you are permitted to log in directly to the Beowulf machines,
you can check what is happening via who:
> rsh rose who Who is running on rose?
> rsh rubin who Who is running on rubin?
> rsh emma who Who is running on emma?
7. Increase the matrix size in steps and record how this affects the speedups. Remember,
once the code is communications-bound, distributing it over many processors probably
will make it run slower, not faster!
D.5 A STRING VIBRATING IN PARALLEL
The program MPIstring.c given in Listing D.8 is a parallel version of the solution of the
wave equation (eqstring.c) discussed in Chapter 18, “PDE Waves: String, Wave Packet,
and Electromagnetic.” The algorithm calculates the future ([2]) displacement of a given string
element from the present ([1]) displacements immediately to the left and right of that section,
as well as the present and past ([0]) displacements of that element. The program is parallelized
by assigning different sections of the string to different nodes.
518 appendix d
Listing D.8 MPIstring.c solves the wave equation for a string using several processors via MPI commands. 
/ / Code l i s t i n g f o r MPIstring . c
# i n c l u d e <s t d i o . h>
# i n c l u d e <math . h>
# i n c l u d e "mpi.h"
# d e f i n e maxt 10000 / / Number of t ime s t e p s to take
# d e f i n e L 10000 / / Number of d i v i s i o n s o f the s t r i n g
# d e f i n e rho 0 . 0 1 / / Dens i ty per l e n g t h ( kg /m)
# d e f i n e t e n 4 0 . 0 / / Tension (N)
# d e f i n e d e l t a t 1 . 0 e−4 / / De l ta t ( s )
# d e f i n e d e l t a x . 0 1 / / De l ta x (m)
# d e f i n e s k i p 50 / / Number of t ime s t e p s to sk ip b e f o re p r i n t i n g
/∗ Need s q r t ( ten / rho ) <= d e l t a x / d e l t a t f o r a s t a b l e s o l u t i o n
Decrease d e l t a t f o r more accuracy , c ’ = d e l t a x / d e l t a t ∗ /
main ( i n t argc , char ∗a rgv [ ] ) {
c o n s t d ou b l e s c a l e = pow ( d e l t a t / d e l t a x , 2 ) ∗ t e n / rho ;
i n t i , j , k , myrank , numprocs , s t a r t , s t op , avgwidth , maxwidth , l e n ;
do ub l e l e f t , r i g h t , s t a r t w t i m e , i n i t \ s t r i n g ( i n t i n d e x ) ;
FILE ∗o u t ;
M P I I n i t ( &argc , &argv ) ;
MPI Comm rank ( MPI COMM WORLD, &myrank ) ; / / Get my rank
MPI Comm size ( MPI COMM WORLD, &numprocs ) ; / / Number of p r o c e s s o r s
M P I S t a t u s s t a t u s ;
i f ( myrank == 0) {
s t a r t w t i m e = MPI Wtime ( ) ;
o u t = fopen ("eqstringmpi.dat" ,"w" ) ;
}
/ / a s s i g n s t r i n g to each node 1 s t and l a s t p o i n t s (0 and L−1) −must =0 , e l s e error
/ / Thus L−2 segments f o r numprocs p r o c e s s o r s
avgwid th = ( L−2) / numprocs ;
s t a r t = avgwid th∗myrank +1;
i f ( myrank < numprocs − 1) s t o p = avgwid th ∗( myrank +1) ;
e l s e s t o p = L−2;
i f ( myrank == 0) maxwidth = L−2 − avgwid th ∗( numprocs−1) ;
e l s e maxwidth = 0 ;
do ub l e r e s u l t s [ maxwidth ] ; / / Holds p r i n t f o r master
l e n = s t o p − s t a r t ; / / Length of the array − 1
do ub l e x [ 3 ] [ l e n + 1 ] ;
f o r ( i = s t a r t ; i <= s t o p ; i ++) x [ 0 ] [ i−s t a r t ] = i n i t s t r i n g ( i ) ;
x [ 1 ] [ 0 ] = x [ 0 ] [ 0 ] + 0 . 5∗ s c a l e ∗( x [ 0 ] [ 1 ] + i n i t s t r i n g ( s t a r t −1)−2.∗x [ 0 ] [ 0 ] ) ; / / 1 s t s t e p
x [ 1 ] [ l e n ] = x [ 0 ] [ l e n ] +0.5∗ s c a l e ∗( i n i t s t r i n g ( s t o p +1)+x [ 0 ] [ l en−1]−2.0∗x [ 0 ] [ l e n ] ) ;
f o r ( i = 1 ; i < l e n ; i ++)
{ x [ 1 ] [ i ] = x [ 0 ] [ i ] + 0 .5∗ s c a l e ∗( x [ 0 ] [ i +1] + x [ 0 ] [ i−1] − 2 .0∗ x [ 0 ] [ i ] ) ; }
f o r ( k =1; k<maxt ; k ++) { / / Later t ime s t e p s
i f ( myrank == 0) { MPI Send ( &x [ 1 ] [ l e n ] , 1 , MPI DOUBLE , 1 , 1 , MPI COMM WORLD) ;
l e f t = 0 . 0 ; } / / Send to R, g e t from L
e l s e i f ( myrank < numprocs − 1) MPI Sendrecv (&x [ 1 ] [ l e n ] , 1 ,
MPI DOUBLE , myrank +1 , 1 , &l e f t , 1 , MPI DOUBLE , myrank−1, 1 , MPI COMM WORLD, &s t a t u s ) ;
e l s e MPI Recv ( &l e f t , 1 , MPI DOUBLE , myrank−1, 1 , MPI COMM WORLD, &s t a t u s ) ;
i f ( myrank == numprocs − 1) { / / Send to L & g e t from R
MPI Send ( &x [ 1 ] [ 0 ] , 1 , MPI DOUBLE , myrank−1, 2 , MPI COMM WORLD ) ;
r i g h t = 0 . 0 ;
}
e l s e i f ( myrank > 0) MPI Sendrecv ( &x [ 1 ] [ 0 ] , 1 , MPI DOUBLE , myrank−1, 2 ,
&r i g h t , 1 , MPI DOUBLE , myrank +1 , 2 , MPI COMM WORLD, &s t a t u s ) ;
e l s e MPI Recv ( &r i g h t , 1 , MPI DOUBLE , 1 , 2 , MPI COMM WORLD, &s t a t u s ) ;
x [ 2 ] [ 0 ] = 2 .∗ x [ 1 ] [ 0 ] − x [ 0 ] [ 0 ] + s c a l e ∗ ( x [ 1 ] [ 1 ] + l e f t − 2 .∗ x [ 1 ] [ 0 ] ) ;
f o r ( i = 1 ; i < l e n ; i ++)
{ x [ 2 ] [ i ] = 2 .∗ x [ 1 ] [ i ] − x [ 0 ] [ i ] + s c a l e ∗( x [ 1 ] [ i +1]+ x [ 1 ] [ i−1]−2.∗x [ 1 ] [ i ] ) ; }
x [ 2 ] [ l e n ] = 2 .∗ x [ 1 ] [ l e n ] − x [ 0 ] [ l e n ] + s c a l e ∗( r i g h t +x [ 1 ] [ l en−1] −2.∗x [ 1 ] [ l e n ] ) ;
f o r ( i = 0 ; i <= l e n ; i ++) { x [ 0 ] [ i ] = x [ 1 ] [ i ] ; x [ 1 ] [ i ] = x [ 2 ] [ i ] ; }
i f ( k%s k i p == 0) { / / P r i n t us ing gnuplot 3D gr id format
i f ( myrank != 0) MPI Send(&x [ 2 ] [ 0 ] , l e n +1 , MPI DOUBLE , 0 , 3 , MPI COMM WORLD) ;
e l s e {
f p r i n t f ( out ,"%f\n" , 0 . 0 ) ; / / Le f t edge of ( always 0 )
f o r ( i =0 ; i < avgwid th ; i ++ ) f p r i n t f ( out ,"%f\n" , x [ 2 ] [ i ] ) ;
f o r ( i =1 ; i < numprocs−1; i ++ ) {
MPI Recv ( r e s u l t s , avgwidth , MPI DOUBLE , i , 3 , MPI COMM WORLD, &s t a t u s ) ;
f o r ( j = 0 ; j < avgwid th ; j ++) f p r i n t f ( out , "%f\n" , r e s u l t s [ j ] ) ;
}
MPI Recv ( r e s u l t s , maxwidth , MPI DOUBLE , numprocs−1, 3 , MPI COMM WORLD, &s t a t u s ) ;
f o r ( j =0 ; j < maxwidth ; j ++ ) f p r i n t f ( out ,"%f\n" , r e s u l t s [ j ] ) ;
f p r i n t f ( out ,"%f\n" , 0 . 0 ) ; / / R edge
f p r i n t f ( out ,"\n" ) ; / / Empty l i n e f o r gnuplot
}
}
}
i f ( myrank == 0)
p r i n t f ("Data stored in eqstringmpi.dat\nComputation time: %f s\n" ,
MPI Wtime ( )−s t a r t w t i m e ) ;
M P I F i n a l i z e ( ) ;
an mpi tutorial 519
e x i t ( 0 ) ;
}
do ub l e i n i t s t r i n g ( i n t i n d e x ) {
i f ( i n d e x < ( L−1)∗4 / 5 ) r e t u r n 1 .0∗ i n d e x / ( ( L−1)∗4 / 5 ) ;
r e t u r n 1 . 0∗ ( L−1−i n d e x ) / ( ( L−1)−(L−1)∗4 / 5 ) ;
/ / Half o f a s i n e wave
}
Notice how the MPI Recv() and MPI Send() commands require a pointer as their first ar-
gument, or an array element. When sending more than one element of an array to MPI Send(),
send a pointer to the first element of the array as the first argument, and then the number of el-
ements as the second argument. Observe near the end of the program how the MPI Send() call
is used to send len + 1 elements of the 2-D array x[][], starting with the element x[2][0].
MPI sends these elements in the order in which they are stored in memory. In C, arrays are
stored in row-major order with the first element of the second row immediately following the
last element of the first row, and so on. Accordingly, this MPI Send() call sends len + 1 ele-
ments of row 2, starting with column 0, which means all of row 2. If we had specified len +
5 elements instead, MPI would have sent all of row 2 plus the first four elements of row 3.
In MPIstring.c, the calculated future position of the string is stored in row 2 of
x[3][len + 1], with different sections of the string stored in different columns. Row 1
stores the present positions, and row 0 stores the past positions. This is different from the
column-based algorithm used in the serial program eqstring.c, following the original Fortran
program, which was column-, rather than row-based. This was changed because MPI reads
data by rows. The initial displacement of the string is given by the user-supplied function
init string(). Because the first time step requires only the initial displacement, no message
passing is necessary. For later times, each node sends the displacement of its rightmost point
to the node with the next highest rank. This means that the rightmost node (rank = numprocs
--1) does not send data and that the master (rank = 0) does not receive data. Communication
is established via the MPI Sendrecv() command, with the different sends and receives using
tags to ensure proper delivery of the messages.
Next in the program, the nodes (representing string segments) send to and receive data
from the segment to their right. All these sends and receives have a tag of 2. After every 50
iterations, the master collects the displacement of each segment of the string and outputs it.
Each slave sends the data for the future time with a tag of 3. The master first outputs its own
data and then calls MPI Recv() for each node, one at a time, printing the data it receives.
D.5.1 MPIstring.c Exercise
1. Ensure that the input data (maxt, L, scale, skip) in MPIstring.c are the same as
those in eqstring.c.
2. Ensure that init string() returns the initial configuration used in eqstring.c.
3. Compile and run eqstring.c via
> gcc eqstring.c -o eqstring -lm Compile C code
> ./eqstring Run in present directory
4. Run both programs to ensure that they produce the same output. (In Unix this is easy to
check with the diff command.)
Listing D.9 The MPI program MPIdeadlock.c illustrates deadlock (waiting to receive). The code
MPIdeadlock-fixed.c in Listing D.6 removes the block. 
520 appendix d
/ / Code l i s t i n g f o r MPIdeadlock . c
# i n c l u d e <s t d i o . h>
# i n c l u d e "mpi.h"
# d e f i n e MAXLEN 100
main ( i n t argc , char ∗a rgv [ ] ) {
i n t myrank , numprocs , f romrank , t o r a n k ;
char t o s e n d [MAXLEN] , r e c e i v e d [MAXLEN] ;
M P I S t a t u s s t a t u s ;
M P I I n i t ( &argc , &argv ) ;
MPI Comm rank ( MPI COMM WORLD, &myrank ) ;
MPI Comm size ( MPI COMM WORLD, &numprocs ) ;
i f ( myrank == 0) f romrank = numprocs − 1 ;
e l s e f romrank = myrank − 1 ;
i f ( myrank == numprocs − 1 ) t o r a n k = 0 ;
e l s e t o r a n k = myrank + 1 ; / / Save s t r i n g to send in tosend :
s p r i n t f ( t o send , "Message sent from node %d to node %d\n" , myrank , t o r a n k ) ;
MPI Recv ( r e c e i v e d , MAXLEN, MPI CHAR , fromrank , 0 , MPI COMM WORLD, &s t a t u s ) ;
MPI Send ( to send , MAXLEN, MPI CHAR , t o r a n k , 0 , MPI COMM WORLD ) ;
p r i n t f ( "%s" , r e c e i v e d ) ; / / P r i n t s t r i n g a f t e r s u c c e s s f u l r e c e i v e
M P I F i n a l i z e ( ) ;
e x i t ( 0 ) ;
}
D.6 DEADLOCK
It is important to avoid deadlock when using the MPI Send() and MPI Recv() commands.
Deadlock occurs when one or more nodes wait for a nonoccurring event to take place. This can
arise if each node waits to receive a message that is not sent. Compile and execute deadlock.c:
> mpicc deadlock.c -o deadlock Compile
> qsub run mpi.sh deadlock Execute
Take note of the job ID returned, which we will call xxxx. Wait a few seconds and then look at
the output of the program:
> cat output/MPI job-xxxx Examine output
The output should list how many nodes (slots) were assigned to the job. Because all these
nodes are now deadlocked, we have to cancel this job:
> qdel xxxx Cancel deadlocked job
> qdel all Alternate cancel
There are a number of ways to avoid deadlock. The program MPIstring.c used the function
MPI Sendrecv() to handle much of the message passing, and this does not cause deadlock. It
is possible to use MPI Send() and MPI Recv(), but you should be careful to avoid deadlock, as
we do in MPIdeadlock-fixed.c in Listing D.6. 
/ / deadlock−f i x e d . c : MPI deadlock . c wi thout deadlock by P h i l Carter
# i n c l u d e <s t d i o . h>
# i n c l u d e "mpi.h"
# d e f i n e MAXLEN 100
main ( i n t argc , char ∗a rgv [ ] ) {
i n t myrank , numprocs , t o r a n k , i ;
char t o s e n d [MAXLEN] , r e c e i v e d [MAXLEN] ;
M P I S t a t u s s t a t u s ;
M P I I n i t ( &argc , &argv ) ;
MPI Comm rank ( MPI COMM WORLD, &myrank ) ;
MPI Comm size ( MPI COMM WORLD, &numprocs ) ;
i f ( myrank == numprocs − 1 ) t o r a n k = 0 ;
e l s e t o r a n k = myrank + 1 ; / / Save s t r i n g to send in tosend :
s p r i n t f ( t o send , "Message sent from node %d to node %d\n" , myrank , t o r a n k ) ;
f o r ( i = 0 ; i < numprocs ; i ++ ) {
i f ( myrank == i ) MPI Send ( to send , MAXLEN, MPI CHAR , t o r a n k , i , MPI COMM WORLD) ;
e l s e i f ( myrank == i +1 | | ( i == numprocs − 1 && myrank == 0) )
MPI Recv ( r e c e i v e d , MAXLEN, MPI CHAR , i , i , MPI COMM WORLD, &s t a t u s ) ;
}
p r i n t f ("%s" , r e c e i v e d ) ; / / P r i n t s t r i n g a f t e r s u c c e s s f u l r e c e i v e
M P I F i n a l i z e ( ) ;
e x i t ( 0 ) ;
an mpi tutorial 521
}
D.6.1 Nonblocking Communication
MPI Send() and MPI Recv(), as we have said, are susceptible to deadlock because they block
the program from continuing until the send or receive is finished. This method of message
passing is called blocking communication. One way to avoid deadlock is to use nonblocking
communication such as MPI Isend(), which returns before the send is complete and thus frees
up the node. Likewise, a node can call MPI Irecv(), which does not wait for the receive to
be completed. Note that a node can receive a message with MPI Recv() even if it was sent
using MPI Isend(), and similarly, receive a message using MPI Irecv() even if it was sent
with MPI Send().
There are two ways to determine whether a nonblocking send or receive is finished. One
is to call MPI Test(). It is your choice as to whether you want to wait for the communication
to be completed (e.g., to ensure that all string segments are at current time and not past time).
To wait, call MPI Wait(), which blocks execution until communication is complete. When
you start a nonblocking send or receive, you get a request handle of data type MPI Request to
identify the communication you may need to wait for. A disadvantage of using nonblocking
communication is that you have to ensure that you do not use the data being communicated
until the communication has been completed. You can check for this using MPI Test() or wait
for completion using MPI Wait().
Exercise: Rewrite MPIdeadlock.c so that it avoids deadlock by using nonblocking commu-
nication. Hint: Replace MPI Recv() by MPI Irecv().
D.6.2 Collective Communication
MPI contains several commands that automatically and simultaneously exchange messages
among multiple nodes. This is called collective communication, in contrast to point-to-
point communication between two nodes. The program MPIpi.c has already introduced the
MPI Reduce() command. It receives data from multiple nodes, performs an operation on
the data, and stores the result on one node. The program tuneMPI.c used a similar func-
tion MPI Allreduce() that does the same thing but stores the result on every node. The latter
program also used MPI Bcast(), which allows a node to send the same message to multiple
nodes.
Collective commands communicate among a group of nodes specified by a communi-
cator, such as MPI COMM WORLD. For example, in MPIpi.c we called MPI Reduce() to receive
results from every node, including itself. Other collective communication functions include
MPI Scatter(), which has one node send messages to every other node. This is similar to
MPI Bcast(), but the former sends a different message to each node by breaking up an array
into pieces of specified lengths and sending the pieces to nodes. Likewise, MPI Gather() gath-
ers data from every node (including the root node) and places it in an array, with data from node
0 placed first, followed by node 1, and so on. A similar function, MPI Allgather(), stores the
data on every node rather than just the root node.
522 appendix d
D.7 BOOTABLE CLUSTER CD 
One of the difficulties in learning how to parallel compute is the need for a parallel computer.
Even though there may be many computers around that you may be able to use, knitting them
all together into a parallel machine takes time and effort. However, if your interest is in learning
about and experiencing distributed parallel computing, and not in setting up one of the fastest
research machines in the world, then there is an easy way. It is called a bootable cluster CD
(BCCD) and is a file on a CD. When you start your computer with the CD in place, you are
given the option of having the computer ignore your regular operating system and instead boot
from the CD into a preconfigured distributed computing environment. The new system does
not change your system but rather is a nondestructive overlay on top of the existing hardware
that runs a full-fledged parallel computing environment on just about any workstation-class
system, including Macs. You boot up every machine you wish to have on your cluster this way,
and if needed, set up a domain name system (DNS) and dynamic host configuration protocol
(DHCP) servers, which are also included. Did we mention that the system is free? [BCCD]
D.8 PARALLEL COMPUTING EXERCISES
1. Bifurcation plot: If you have not yet done so, take the program you wrote to generate the
bifurcation plot for bug populations and run different ranges of µ values simultaneously
on several CPUs.
2. Processor ring: Write a program in which
a. a set of processors are arranged in a ring.
b. each processor stores its rank in MPI COMM WORLD in an integer.
c. each processor passes this rank on to its neighbor on the right.
d. each processor keeps passing until it receives its rank back.
3. Ping pong: Write a program in which two processors repeatedly pass a message back
and forth. Insert timing calls to measure the time taken for one message and determine
how the time taken varies with the size of the message.
4. Broadcast: Have processor 1 send the same message to all the other processors and then
receive messages of the same length from all the other processors. How does the time
taken vary with the size of the messages and the number of processors?
D.9 LIST OF MPI COMMANDS
MPI Data Types and Operators
MPI defines some of its own data types. The following are data types used as arguments
to MPI commands.
• MPI Comm: A communicator, used to specify group of nodes, most commonly
MPI COMM WORLD for all the nodes.
• MPI Status: A variable holding status information returned by functions such as
MPI Recv().
• MPI Datatype: A predefined constant indicating the type of data being passed in a func-
tion such as MPI Send() (see below).
• MPI O: A predefined constant indicating the operation to be performed on data in func-
tions such as MPI Reduce() (see below).
• MPI Request: A request handle to identify a nonblocking send or receive, for example,
when using MPI Wait() or MPI Test().
an mpi tutorial 523
MPI OP Description MPI Datatype C Data Type
MPI MAX Maximum MPI CHAR char
MPI MIN Minimum MPI SHORT short
MPI SUM Sum MPI INT int
MPI PROD Product MPI LONG long
MPI LAND Logical and MPI FLOAT float
MPI BAND Bitwise and MPI DOUBLE double
MPI LOR Logical or MPI UNSIGNED CHAR unsigned char
MPI BOR Bitwise or MPI UNSIGNED SHORT unsigned short
MPI LXOR Logical exclusive or MPI UNSIGNED unsigned int
MPI BXOR Bitwise exclusive or MPI UNSIGNED LONG unsigned long
MPI MINLOC Find node’s min and rank
MPI MAXLOC Find node’s max and rank
Predefined Constants: MPI Op and MPI Datatype
For a more complete list of constants used in MPI, see
http://www-unix.mcs.anl.gov/mpi/www/www3/Constants.html.
MPI Commands
Below we list and identify the MPI commands used in this appendix. For
the syntax of each command, along with many more MPI commands, see
http://www-unix.mcs.anl.gov/mpi/www/, where each command is a hyperlink to a
full description.
Basic MPI Commands
• MPI Send: Sends a message.
• MPI Recv: Receives a message.
• MPI Sendrecv: Sends and receives a message.
• MPI Init: Starts MPI at the beginning of the program.
• MPI Finalize: Stops MPI at the end of the program.
• MPI Comm rank: Determines a node’s rank.
• MPI Comm size: Determines the number of nodes in the communicator.
• MPI Get processor name: Determines the name of the processor.
• MPI Wtime: Returns the wall time in seconds since an arbitrary time in the past.
• MPI Barrier: Blocks until all the nodes have called this function.
Collective Communication
• MPI Reduce: Performs an operation on all copies of a variable and stores the result on a
single node.
• MPI Allreduce: Like MPI Reduce, but stores the result on all the nodes.
• MPI Gather: Gathers data from a group of nodes and stores them on one node.
• MPI Allgather: Like MPI Gather but stores the result on all the nodes.
• MPI Scatter: Sends different data to all the other nodes (opposite of MPI Gather).
• MPI Bcast: Sends the same message to all the other processors.
Nonblocking Communication
• MPI Isend: Begins a nonblocking send.
524 appendix d
• MPI Irecv: Begins a nonblocking receive.
• MPI Wait: Waits for an MPI send or receive to be completed.
• MPI Test: Tests for the completion of a send or receive.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Appendix E
Software on the CD
PythonCodes Contents by Section
Section Program Description
1.6 AreaFormatted Java 1.5 scanner
class
1.6.2 Area First program
1.7.5 Limits Machine precision
1.7.5 ByteLimit Precise machine
precision
2.2.2 Bessel Bessel function
recurrence
3.4 EasyPlot Simple use
of MatplotLib
3.4 MatPlot2figs Two plots,
one graph
4.4 Complex Complex number
objects
4.4 ComplexDyn Dynamic complex
objects
4.7.1 Beats Procedural beats
4.7.1 OOPBeats Objective beats
4.7.2 Moon Procedural moon
orbits
4.7.2 OOPMoon Objective moon
orbits
4.7.2 OOPlanet Objective moon
orbits
5.2.2 RandNum Java random
utility
5.4.2 Walk Random-walk
simulation
5.5 Radiactive Decay Decay
5.6 Decay Spontaneous
decay
4.7.1 vonNeuman Integration reject
6.6.3 Int10d 10-D Monte Carlo
integral
6.2.1 Trap Trapezoid rule
Section Program Description
6.2.5 IntegGauss Gaussian quadrature
7.5 Diff Numerical
differentiation
7.9 Bisection Bisection root
finding
7.10 Newton cd Newton–Raphson
roots
7.10 Newton fd Newton–Raphson
roots
8.6.1 Lagrange Lagrange Interpolation
8.6.5 SplineAppl Cubic spline fit
8.8.1 Fit Linear least-squares fit
19 Soliton KdV Equation
9.5.2 rk2, rk4 2nd, 4th O
Runge-Kutta
9.5.2 rk45 Adaptive step
rk solver
9.5.2 ABM Predictor-corrector
solver
9.11 Numerov Quantum bound
states
9.11 QuantumEigen rk quantum bound
states
10.4.3 DFTassesment DFT example
10.8 FFT Fast Fourier Transform
10.8 FFTapplic FFT example
10.7 Noise Filtering
10.4.1 DFT Discrete Fourier
transform
11.4.2 CWT Continuous
wavelet TF
11.5.3 DWT Discrete Wavelet
transform
526 appendix f
PythonCodes Contents by Section Continued
Section Program Description
12.5 Bugs Bifurcation diagram
12.8 LyapLog Lyapunov exponents
12.8.1 Entropy Shannon logistic
entropy
12.19 PredatorPrey Lotka–Volterra model
13.4.1 BallisticDepo Random deposition
13.3.2 Fern Fern
13.3.2 Fern3D Fern 3Dim
13.3.2 Tree Tree
13.3.2 Tree2 Another tree
13.3.2 Column Correlated deposition
13.7.1 DLA Diffusion Limited
Aggregation
13.9 CellAut, Cellular automata
13.9 OutofEight Another Cellular
Automata
14.14.4 Tune, Tune4 Optimization
testing
13.9 LifeGame Game of Life
15.6.1 WangLandau Wang Landau
Algorithm
16.3 MD 2-D MD simulation
Section Program Description
16.3 MD1D 1-D MD simulation
17.4.2 LaplaceLine Finite differential
Laplace equation
17.14 LaplaceFEM Finite element
Laplace equation
17.17.4 EqHeat Heat equation
via leapfrog
17.19.1 HeatCNTridiag Crank–Nicolson
heat equation
18.2.3 EqString Waves on a string
18.11 FDTD FDTD Algorithm
18.11 CircPolarztn Circular Polarization
19.9.3 Beam 2-D Navier–Stokes
fluid
19.1 AdvecLax Advection
19.3.1 Shock Shock waves
20.2.3 Bound Bound integral
equation
20.4.5 Scatt Scattering integral
equation
15.2 Ising Ising model
15.8.3 QMC Feynman path
integration
15.9 QMCbouncer Quantum bouncer
Animations Contents
(requires player VLC or QuickTime for mpeg, avi; browser for gifs)
Directory Chapter Directory Chapter
DoublePendulum (also two pendulums; 12 Fractals (see also applets) 13
see also applets)
MapleWaveMovie (need Maple; 18 Laplace (DX movie) 17
includes source files)
MD 16 TwoSlits (includes DX source files) 18
2-Dsoliton (includes DX source files) 18, 19 Utilities (scripts, colormaps) 3
Waves (animated gifs need browser) 18
Software on the CD 527
Applets Directory Contents
(see index.html; requires browser or Java appletviewer)
Applet Chapter Applet Chapter
Area 1 Parabolic motion 4
The chaotic pendulum 12 2 Hypersensitive Pendula 12
Planetary orbits 9 Normal Mode 18
String motion 18 Normal Mode 18
Cellular automata for Sierpiński 13 Solitons 18
Spline interpolation 8 Relativistic scattering 9
Lagrange interpolation 8 Young’s two slit interference 18
Wavelet compression 11 Starbrite (H-R diagram) 4
HearData: a sound player for data 12 Photoelectric effect 9
Visualizing physics with Sound 9 Create Lissajous figures 12
Radioactive Decay 5 Spline Interpolation 8
Wavepacket-wavepacket collision movies 18 Heat equation 17
ABM predictor corrector 9 Wave function (SqWell ), (HarOs) 18
Wave function (Asym), (PotBarrier) 18 Fractals (Sierpiński), (fern), (tree) 13
Fractals (film) (column) (dla) 13 Feynman path integrals (QMC) 15
Four-centers chaotic scattering 12 Shock wave 19
Schrö. Eq, With Split operator 18
MPIcodes Contents
Program Description Program Description
MPIhello.c First MPI program MPIdeadlock.c Deadlock examples
MPIdeadlock-fixed.c Fixed deadlocks MPImessage2.c Send and receive messages
MPImessage3.c More messages MPIpi.c Parallel computation of π
MPIstring.c Parallel string equation run mpi.sh Template script for grid engine
TuneMPI.c Optimization tuning for MPI
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Appendix F
Compression via DWT with Thresholding
An important application of discrete wavelet transformation is image compression. Anyone
who has stored high-resolution digital images or videos knows that such files can be very large.
DWT can reduce image size significantly with just a minimal loss of quality by storing only
a small number of smooth components and only as many detailed components as needed for
a fixed resolution. To compress an image, we compute and store the DWT of each row of the
image, setting all wavelet coefficients smaller than a certain threshold to zero (thresholding).
When the image is needed, the inverse wavelet transform is used to reconstruct each row of the
original image. For example, Table D.1 shows file sizes resulting from using DWT for image
compression. We note (columns 1 and 2) that there is a factor-of-2 reduction in size arising
from storing the DWT rather than the data themselves, and that this is independent of the
threshold criteria (it still stores the zeros). After the compression program WinZip removes the
zeros (columns 3 and 4), we note that the DWT file is a factor of 3 smaller than the compressed
original, and a factor of 11 smaller than the noncompressed original.
A usual first step in dealing with digital images is to convert from one picture format
to another.1 We started with a 3073 × 2304 pixel (picture element) jpg (Joint Photographic
Experts Group) color image. For our example (Figure D.1 left) we reduced the image size to
512× 512 (a power of 2) and converted it to gray scale. You can do this with the free program
IrfanView [irfanview] (Windows), the GNU image editor The Gimp (Unix/Linux) [Gimp], or
Netpbm [Netpbm]. We used Netpbm’s utility jpegtopnm to convert the .jpg to the portable
gray map image in Netpbm format:
> jpegtopnm marianabw.jpg > mariana.pnm Convert jpeg to pnm
If you open mariana.pnm (available on the CD), you will find
P5
512 512
255
yvttsojgrvv|yngcawbRQ] ...
This format is not easy to work with, so we converted it to mariana,
> pnmtopnm -plain mariana.pnm>mariana Convert pnm format to integers
Except for the first three lines that contain information about the internal code (width, height,
0 for black, and 255 for white), we now have integers: 
P2
512 512
255
143 174 209 235 249 250 250 255 255 255 255 255 255 255 255 255 255
255 255 255 255 254 253 253 254 254 255 255 255 255 255 255 255 255
. . .
1We thank Guillermo Avendanño-Franco for help with image programs.
Software on the CD 529
Table D.1 Compression of a Data File Using DWT with a 20% Cutoff Threshold
Original File DWT Reconstructed WinZipped Original WinZipped DWT
25,749 11,373 7,181 2,447
Figure D.1 Left: The original picture of Mariana. Middle: The reconstituted picture for a compression ratio of 8
( = 9). Right: The reconstituted picture for a compression ratio of 46 ( = 50). The black dots in the
images are called “salt and pepper” and can be eliminated.
Because Java is not very flexible with I/O formats, we wrote a small C program to convert
mariana to the one-column format mariana.dat: 
# i n c l u d e <s t r i n g . h>
# i n c l u d e <s t d l i b . h>
/ / Reads mariana . dat in decimal a s c i i , form 1−column mariana . dat
main ( ) {
i n t i , j , k ;
char cd [ 1 0 ] , c ;
FILE ∗pf , ∗pou t ;
p f = fopen ("mariana" , "r" ) ;
pou t = fopen ("mariana.dat" ,"w" ) ;
f s c a n f ( pf ,"%s" ,& cd ) ; p r i n t f ("%s\n" , cd ) ;
f s c a n f ( pf ,"%s" ,& cd ) ; p r i n t f ("%s\n" , cd ) ;
f s c a n f ( pf ,"%s" ,& cd ) ; p r i n t f ("%s\n" , cd ) ;
f s c a n f ( pf ,"%s" ,& cd ) ; p r i n t f ("%s\n" , cd ) ;
f o r ( k = 0 ; k < 512 ; k++ ) {
f o r ( j = 0 ; j < 512 ; j ++ ) f s c a n f ( pf ,"%d" ,& i ) ; f p r i n t f ( pout ,"%d\n" , i ) ;
}
f c l o s e ( p f ) ; f c l o s e ( pou t ) ;
}
Now that we have an accessible file format, we compress and expand it:
1. Read the one-column file and form the array fg[512][512] containing all the informa-
tion about the image.
2. Use the program DaubMariana.py on the CD to apply the Daubechies 4-wavelet trans-
form to each row of fg. This forms a new 2-D array containing the transformed matrix.
3. Use DaubMariana.py again, now transforming each column of the transformed matrix
and saving the result in a different array.
4. Compress the image by selecting the tolerance level eps=9 and eliminating all signals
that are smaller than eps:
if abs(fg[i][j]) < eps, fg[i][j]=0
5. Apply the inverse wavelet transform to the modified array, column by column and then
row by row.
6. Save the reconstituted image to Marianarec via output redirection:
> java DaubMariana > Marianarec
7. The program also creates the file comp-info.dat containing information about the com-
pression:
530 appendix f
 
Number o f nonze ro c o e f s b e f o r e c o m p r e s s i o n : 262136
I f abs ( c o e f )<9c o e f =0
Number o f nonze ro c o e f f i c i e n t s a f t e r c o m p r e s s i o n : 32753
Compress ion r a t i o :8 .003419534088481
If the program is compiled and run again, the resulting output will contain the compres-
sion image in a file with many zeros: 
P2
512 512
255
143 1543 2296 −405 594 −311 50 68 444 −375 0 20 0 274 −138 423
371 −19 −51 −100 24 0 118 120 62 −59 404 −111 61 −82 306 −255
204 0 81 −60 0 −29 83 −214 55 −12 −34 24 −32 46 −37 29 −14 45 47
. . .
0 0 0 0 0 0 16 −24 0 0 0 −13 0 −32 0 0 0 0 0 9 −29 28 −21 13
0 −11 0 18 9 −18 0 −11 11 0 0 0 0 0 10 −20 0 0 0 0 −11 0 0
0 0 0 0 0 27 −29 0 −18 0 21 24 −54 15 0 0 0 0 0 0 0 0 0 0 0
−12 9 −9 9 0 19 0 0 0 0 0 0 −9 0 0 0 0 0 0 0 72 −53 0 0 0 0
0 0 44 35 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 −12 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 −19 0 11 0 −18
. . .
Although this is an image file, it is not meant for viewing as an image (it will be mainly
black).
F.1 MORE ON THRESHOLDING
Often in practical applications, a good number of the wavelet coefficients are nearly equal to
zero. When these coefficients are set to zero via some thresholding scheme [D&J 94], DWT
files containing long strings of zeros result. Through a type of compression known as entropy
coding, the amount of memory needed to store files of this sort can be greatly reduced.
Before we go on to compression algorithms, it is important to note that there are different
types of thresholding. In hard thresholding, a rather arbitrary tolerance is selected, and any
wavelet coefficient whose absolute value is below this tolerance is set to zero. Presumably,
these small numbers have only small effects on the image. In soft thresholding, a tolerance h is
selected, and, as before, any wavelet coefficient whose absolute value is below this tolerance is
set to zero. However, all other entries d are replaced with sign(d) | |d| − h|. Soft thresholding
can be thought of as a translation of the signal toward zero by the amount h. Finally, in
quantile thresholding, a percentage p of entries are selected, and p percent of those entries with
the smallest absolute values are set to zero.
DWT with thresholding is useful in analyzing signals and for compressing signals so
that less memory is needed to store the transforms than to store the original signals. However,
we have yet to take advantage of the frequent occurrence of zeros as wavelet coefficients.
Huffman entropy coding is well suited for compressing data that contain many zeros. With this
method, an integer sequence q is changed to a shorter sequence e that is stored as 8-bit integers.
Strings of zeros are coded by the numbers 1–100, 105, and 106, while nonzero integers in q
are coded by 101–104 and 107–254. The idea is to use two or three numbers for coding, with
the first being a signal that a large number or a long zero sequence is coming. Entropy coding
is designed so that the numbers that are expected to appear the most often in q need the least
amount of space in e.
A step in compression, known as quantization, converts a sequence w of floating-point
numbers to a sequence q of integers. The simplest technique is to round the floats to the nearest
integer. Another option is to multiply each number in w by a constant k and then round to
the nearest integer. Quantization is called lossy because information is lost when a float is
Software on the CD 531
converted to an integer. In Table D.1 we showed the effect of compression using the WinZip
data compression algorithm. This is a hybrid of LZ77 and Huffman coding also known as
Deflate.
F.2 WAVELET IMPLEMENTATION AND ASSESSMENT
1. Write a program to plot Daub4 wavelets. (Our sample program is Daub4.py.) Observe
the behavior of the wavelet functions for different values of the coefficients. In order to
do this, place a 1 in the coefficient vector for the wavelet structure you want and place
0’s in all other locations. Then perform the inverse transform to produce the physical
domain representation of the wavelet.
2. Run the code Daub4.py for different threshold values.
3. Run the code DaubCompress.py that uses other functions to give input data.
4. Write a Java program or applet that compresses a 512×512 image using Daub4 wavelets.
To do this, extend method wt1 so that it performs a 2-D wavelet transform. Note that you
may need some methods from the java.awt.image package to plot the images. First,
create an Image object using Image img. The MemoryImageSource class is used to create
an image from an array of pixel values using the constructor MemoryImageSource(int
w, int h, int pix[ ], int depls, int scan). Here w and h are the dimensions of
the image, pix[ ] is an array containing the pixels values, depls is the deployment of
data in pix[ ], and scan is the length of a row in pix[ ]. Finally, draw the image using
drawImage(Image img, int x, int y, this), where x and y are the coordinates of
the left corner of the image. Alternatively, you can use a program such as Matlab or
Maple to display images from an array of integers.
5. Modify the program DaubCompress.py so that it outputs the DWT to a file.
6. Pick a different function to analyze, the more complicated the better.
7. Plot the resulting DWT data in a meaningful way.
8. Show in your plots the effects of increasing the threshold parameter in order to cut out
more of the smaller transform values.
9. Examine the reconstituted signal for various threshold values including zero and note the
effect of the cutoff on the image quality.
c©Princeton Univ Press; c©Landau, Paez, Bordeianu, 2010. For personal use only. Supported by the National Science Foundation.
Bibliography
[Abar 93] ABARBANEL, H. D. I., M. I. RABINOVICH, AND M. M. SUSHCHIK (1993),
Introduction to Nonlinear Dynamics for Physicists, World Scientific, Singapore. 275
[A&S 72] ABRAMOWITZ, M., AND I. A. STEGUN (1972), Handbook of Mathematical
Functions, 10th Ed. U.S. Govt. Printing Office, Washington. 117
[Add 02] ADDISON, P. S. (2002), The Illustrated Wavelet Transform Handbook, Institute of
Physics Publishing, Bristol and Philadelphia. 231, 236, 239
[ALCMD] MORRIS, J., D. TURNER, AND K.-M. HO AL CMD, Ames Laboratory Classi-
cal Molecular Dynamics.
http://codeblue.umich.edu/hoomd-blue/.
[A&T 87] ALLAN, M. P. AND J. P. TILDESLEY (1987), Computer Simulations of Liquids,
Oxford Science Publications, Oxford, UK. 364
[Amd 67] AMDAHL, G., Validity of the Single-Processor Approach to Achieving Large-Scale
Computing Capabilities, Proc. AFIPS., 483 (1967). 364
[Anc 02] ANCONA, M. G. (2002), Computational Methods for Applied Science & Engineer-
ing, Rinton Press, Princeton, N.J. 314
[A&W 01] ARFKEN, G. B., AND H. J. WEBER (2001), Mathematical Methods for Physi-
cists, Harcourt/Academic Press, San Diego. 400
[Argy 91] ARGYRIS, J., M. HAASE, AND J. C. HEINRICH (1991), Comput. Meth. Appl.
Mech. Eng., 86, 1. 232, 376
[Arm 91] ARMIN, B., AND H. SHLOMO, EDS. (1991), Fractals and Disordered Systems,
Springer-Verlag, Berlin.
[Ask 77] ASKAR, A., AND A. S. CAKMAK (1977), J. Chem. Phys. 68, 2794. 282
[Bai 05] BAILEY, M. OSU ChromaDepth Scientific Visualization Gallery,
web.engr.oregonstate.edu/˜mjb/chromadepth/. 421, 424
[Bana 99] BANACLOCHE, J. G., (1999), A quantum bouncing ball, Am. J. Phys. 67, 776-
782. 60
[Barns 93] BARNSLEY, M. F. AND L. P. HURD, (1993), Fractal Image Compression, A. K.
Peters, UK. 361
[Becker 54] BECKER, R. A., (1954), Introduction to Theoretical Mechanics, McGraw-Hill,
New York. 286, 297
[Berry] BERRYMAN, A. A., Predator-Prey Dynamics,classes.entom.wsu.edu/543/. 418
534 appendix f
[B&R 02] BEVINGTON, P. R., AND D. K. ROBINSON (2002), Data Reduction and Error
Analysis for the Physical Sciences, 3rd Ed., McGraw-Hill, New York.
[Bin 01] BINDER, K. AND D. W. HEERMANN, (2001) Monte Carlo Methods, Springer-
Verlag, Berlin. 156, 163, 164
[Bleh 90] BLEHER, S., C. GREBOGI, AND E. OTT (1990), Bifurcations in Chaotic Scatter-
ing, Physica D, 46, 87.
[Burg 74] BURGERS, J. M., (1974), The Non-Linear Diffusion Equation; Asymptotic Solu-
tions and Stattistical Problems, Reidel, Boston. 195
[DX2] BRAUN, J. R. FORD, AND D. THOMPSON, OpenDX: Paths to Visualization, (2001),
Visualization and Imagery Solutions, Inc., Missoula. 437
[B&H 95] BRIGGS, W. L., AND V. E. HENSON (1995), The DFT, An Owner’s Manual,
SIAM, Philadelphia. 480
[C&P 85] R. CAR AND M. PARRINELLO (1985), Phys. Rev. Lett. 55, 2471. 208
[C&P 88] CARRIER, G. F., AND C. E. PEARSON (1988), Partial Differential Equations,
Academic Press, San Diego. 364
[C&L 81] CHRISTIANSEN, P. L., AND P. S. LOMDAHL (1981), Physica 2D, 482.
[CPUG] CPUG, Computational Physics degree program for Undergraduates, Oregon State
University, www.physics.oregonstate.edu/ rubin/CPUG.
[C&N 47] CRANK, J., AND P. NICOLSON (1946), Procd. Cambridge Phil. Soc. 43, 50. 7
[C&O 78] CHRISTIANSEN, P. L., AND O. H. OLSEN (1978), Phys. Lett. 68A, 185; (1979),
Physica Scripta 20, 531. 402
[Chrom] ChromaDepth Technologies CHROMADEPTH TECHNOLOGIES www.chromatek.com/.
[Clark] CLARK UNIVERSITY, Statistical & Thermal Physics Curriculum Development
Project, stp.clarku.edu/; Density of States of the 2D Ising Model. 59
[Co,65] COOLEY, J. W. AND J. W. TUKEY, (1965), Math. Comput., 19, 297. 345, 348
[Cour 28] COURANT, R., K. FRIEDRICHS, AND H. LEWY (1928), Mathematische Annalen
100, 32. 223
[Cre 81] CREUTZ, M. AND B. FREEDMAN, A statistical approach to quantum mechanics,
(1981), Ann. Phys. (N.Y.), 132, 427-462.
[CYG] Cygwin, a Linux-like environment for Windows, x.cygwin.com/.
[Da,42] DANIELSON, G. C. AND C. LANCZOS, (1942), J. Franklin Inst., 233, 365. 60, 481
[Daub 95] DAUBECHIES, I. (1995), Wavelets and other phase domain localization methods,
Proc. Int. Cong. Mathematicians, 1, 2, Basel, 56 (1995). Birkhäuser. 223
[DeJ 92] DE JONG, M. L. (1992), Chaos and the simple pendulum, The Physics Teacher 30,
115. 246
[DeV 95] DEVRIES, P. L. (1996), Resource Letter CP-1: Computational Physics, Am. J.
Phys. 64, 364. 271
[Dong 05] DONGARRA, J., T. STERLING, H. SIMON, AND E. STROHMAIER (2005), High-
Performance Computing, IEEE/AIP Comp. in Science & Engr. 7, 51.
Software on the CD 535
[Donn 05] DONNELLY, D. AND B. RUST (2005), The Fast Fourier Transform for Experi-
mentalists, IEEE/AIP Comp. in Science & Engr. 7, 71. 313, 318
[D&J 94] DONOHO, D.L. AND I. M. JOHNSTONE, (1994a), Ideal denoising in an orthonor-
mal basis chosen from a library of bases, Compt. Rend. Acad. Sci. Paris Ser. A, 319,
1317. 223
[DX1] DX, OPEN DX, The open source software project based on IBM’s Visualization Data
Explorer, www.opendx.org/. 530
[Jtut] ECK, D., (2002), Introduction to Programming Using Java, Version 4, (a free text-
book),math.hws.edu/javanotes/. 480, 481
[Eclipse] ECLIPSE, an open development platform, www.eclipse.org/.
[Erco] ERCOLESSI, F., A Molecular Dynamics Primer, www.ud.infn.it/˜ercolessi/md/. v
[E&P 88] EUGENE, S. H., AND M. PAUL (1988), Nature 335, 405. 364, 369
[F&S] FALKOVICH, G., AND K. R. SREENIVASAN (2006), Lesson from Hydrodynamic Tur-
bulence, Physics Today 43. 282
[Fam 85] FAMILY, F., AND T VICSEK (1985), J. Phys. A 18, L75. 437, 455
[Feig 79] FEIGENBAUM, M. J. (1979), J. Stat. Physics 21, 669. 288
[F&W 80] FETTER, A. L., AND J. D. WALECKA (1980), Theoretical Mechanics of Particles
and Continua, McGraw-Hill, New York. 254, 257
[F&H 65] FEYNMAN, R. P., AND A. R. HIBBS (1965), Quantum Mechanics and Path Inte-
grals, McGraw-Hill, New York. 435, 447
[Fitz 04] FITZGERALD, R. (2004), New Experiments Set the Scale for the Onset of Turbu-
lence in Pipe Flow, Phys. Today, 57, 21. 335, 351
[Fos 96] FOSDICK L. D, E. R. JESSUP, C. J. C. SCHAUBLE, AND G. DOMIK (1996), An
Introduction to High Performance Scientific Computing, MIT Press, Cambridge. 455
[Fox 03] FOX, G., HPJava: A Data Parallel Programming Alternative, (2003), IEEE/AIP
Compt. in Sci, & Engr., 5, 60. 364
[Fox 94] FOX, G., Parallel Computing Works!, (1994) Morgan Kaufmann, San Diego. 500
[Gara 05] GARA, A., M. A. BLUMRICH, D. CHEN, G. L.-T. CHIU, P. COTEUS, M. E.
GIAMPAPA, R. A. HARING, P. HEIDELBERGER, D. HOENICKE, G. V. KOPCSAY,
T. A. LIEBSCH, M. OHMACHT, B. D. STEINMACHER-BUROW, T. TAKKEN, AND P.
VRANAS, Overview of the Blue Gene/L system architecure, (2005) IBM J. Res & Dev
49, 195. 304
[Gar 00] GARCIA, A. L. (2000), Numerical Methods for Physics, 2nd ed., Prentice Hall,
Upper Saddle River. 320, 321, 323
[Gibbs 75] GIBBS, R. L. (1975), The quantum bouncer, Am. J. Phys., 43, 25-28. 383
[Good 92] GOODINGS, D. A. AND T. SZEREDI (1992), The quantum bouncer by the path
integral method, Am. J. Phys., 59, 924-930. 361
[Gimp] GIMP, the GNU image manipulation program, www.gimp.org/. 361
536 appendix f
[GNU] Gnuplot, a portable command-line driven interactive data and function plotting utility,
www.gnuplot.info/. 528
[Gold 67] GOLDBERG, A., H. M. SCHEY, AND J. L. SCHWARTZ (1967), Am. J. Phys. 3,
177.
[Gos 99] GOSWANI, J. C., A. K. Chan, (1999) Fundamentals of Wavelets, John Wiley & Sons,
Inc., New York.
[Gott 66] GOTTFRIED, K. (1966), Quantum Mechanics, Benjamin, New York.
[G,T&C 06] GOULD, H., J. TOBOCHNIK AND W. CHRISTIAN (2006), An Introduction to
Computer Simulation Methods, 3rd ed., Addison-Wesley, Reading, MA. 134, 464, 470
[Grace] GRACE, a WYSIWYG 2D plotting tool for the X Window System (descendant of
ACE/gr, Xmgr),plasma-gate.weizmann.ac.il/Grace/. 121, 259, 262, 271, 344, 364
[Graps 95] GRAPS, A. (1995), An Introduction to Wavelets, IEEE/AIP Comp. in Science &
Engr. 2, 50. 60
[BCCD] GRAY, P., AND T. MURPHY (2006), Computing in Science & Engineering 8, 82;
bccd.cs.uni.edu/.
[Gurney] GURNEY, W. S. C. AND R. M. NISBET, (1998), Ecological Dynamics, Oxford
Uni. Press, Oxford, New York. 522
[H&T 70] HAFTEL, M. I., AND F. TABAKIN (1970), Nucl. Phys. 158, 1.
[Har 96] HARDWICH, J., Rules for Optimization, www.cs.cmu.edu/˜jch/java. 468
[Hart 98] HARTMANN, W. M. (1998), Signals, Sound, and Sensation, AIP Press, Springer-
Verlag, New York. 322
[Hi,76] HIGGINS, R. J., (1976), Am. J. Phys., 44, 766. 218, 220
[Hock 88] HOCKNEY, R.W. AND J.W. EASTWOOD (1988), Computer Simulation Using
Particles, Adam Hilger, Bristol, UK. 226
[Huang 87] HUNAG, K. (1987), Statistical Mechanics, Wiley, New York. 364
[Intel] Intel Cluster Tools,
www3.intel.com/cd/software/products/asmo-na/eng/cluster/244171.htm;
Intel Compilers,
www3.intel.com/cd/software/products/asmo-na/eng/compilers/284264.htm. 338
[irfanview] IRFANVIEW, www.irfanview.com/. 506
[Jack 88] JACKSON, J. D. (1988), Classical Electrodynamics, 3rd ed., Wiley, New York.
528
[Jama] JAMA, A Java matrix package; Java Numerics, math.nist.gov/javanumerics/jama/. 377,
378
[J&S 98] JOSÉ, J. V, AND E. J. SALATAN, (1988) Classical Dynamics, Cambridge Univ.
Press, Cambridge, UK. 175, 269
[jEdit] JEDIT, a mature programmer’s text editor, www.jedit.org/. v
Software on the CD 537
[K&R 88] KERNIGHAN, B. AND D. RITCHIE (1988) The C Programming Language, 2nd
ed., Prentice Hall, Englewood Cliffs, NJ. 500
[Koon 86] KOONIN, S. E. (1986), Computational Physics, Benjamin, Menlo Park, CA. 124,
187, 422, 455
[KdeV 95] KORTEWEG, D. J., AND G. DEVRIES (1895), Phil. Mag. 39, 4. 441
[Krey 98] KREYSZIG, E. (1998), Advanced Engineering Mathematics, 8th ed., Wiley, New
York. 380
[Kutz] KUTZ N., Scientific Computing,
www.amath.washington.edu/courses/581-autumn-2003/.
[Lamb 93] LAMB, H., (1993), Hydrodynamics, 6th ed., Cambridge University Press, Cam-
bridge, UK. 453
[L&L,F 87] LANDAU, L. D., AND E. M. LIFSHITZ (1987), Fluid Mechanics, 2nd Ed.,
Butterworth-Heinemann, Oxford, UK. 435
[L&L,M 76] LANDAU, L. D., AND E. M. LIFSHITZ (1976), Quantum Mechanics, Perga-
mon, Oxford, UK. 204, 262, 263, 271, 424, 436
[L&L,M 77] LANDAU, L. D., AND E. M. LIFSHITZ (1976), Mechanics, 3rd ed.,
Butterworth-Heinemann, Oxford, UK.
[L 05] LANDAU, R. H. (2005), A First Course in Scientific Computing, Princeton Univ.
Press, Princeton. 42, 388, 480
[L 96] LANDAU, R. H. (1996), Quantum Mechanics II, A Second Course in Quantum The-
ory, 2nd ed., Wiley, New York. 188, 362, 461, 462
[L&F 93] LANDAU, R. H., AND P. J. FINK (1993), A Scientist’s and Engineer’s Guide to
Workstations and Supercomputers, Wiley, New York.
[Lang] LANG, W. C., K. FORINASH, (1998), Time-frequency analysis with the continuous
wavelet transform, Amer. J. of Phys, 66, 794. 238
[LAP 00] ANDERSON, E., Z. BAI, C. BISCHOF, J. DEMMEL, J. DONGARRA, J. DU
CROZ, A. GREENBAUM, S. HAMMARLING, A. MCKENNEY, S. OSTROUCHOV,
AND D. SORENSEN (2000), LAPACK Users’ Guide, 3rd ed., SIAM, Philadelphia;
www.netlib.org.
[Li] LI, Z., Numerical Methods for Partial Differential Equations - Finite Element Method,
www4.ncsu.edu/˜zhilin/TEACHING/MA587/. 389, 390, 394
[Libb 03] LIBOFF, R. L., (2003), Introductory Quantum Mechanics, Addison Wesley,
Reading, MA. 492
[Lot 25] LOTKA, A. J., (1925) Elements of Physical Biology, Williams & Wilkins, Baltimore.
276
[MacK 85] MACKEOWN, P. K. (1985), Am. J. Phys. 53, 880. 335
[Lusk 99] LUSK, W. E. AND A. SKJELLUM (1999) Using MPI: Portable Parallel
Programming with the Message-Passing Interface, 2nd Ed., MIT Press, Cambridge, MA.
304
[M&N 87] MACKEOWN, P. K., AND D. J. NEWMAN (1987) Computational Techniques in
Physics, Adam Hilger, Bristol, UK. 335
538 appendix f
[MLP 00] MAESTRI, J. J. V., R. H. LANDAU, AND M. J P/’AEZ (2000) Two-Particle
Schrödinger Equation animations of wave packet-wave packet scattering, , Am. J. Phys.
68, 1113-1119. 421, 424
[Mallat 89] MALLAT, P.G. (1982), A Theory for Multiresolution Signal Decomposition: The
Wavelet Representation, IEEE Transaction on Pattern Analysis and Machine Intelligence
Vol.11, No. 7, 674-693.
[Mand 67] MANDELBROT, B. (1967), How long is the coast of Britain?, Science, 156, 638.
289
[Mand 82] MANDELBROT, B. (1982), The Fractal Geometry of Nature, 29, Freeman, San
Francisco. 282
[Mann 90] MANNEVILLE, P., (1990), Dissipative Structures and Weak Turbulence, Aca-
demic Press, San Diego. 259
[Mann 83] MANNHEIM, P. D. (1983), Am. J. Phys. 51, 328. 335
[M&T 03] MARION, J. B. AND S. T. THORNTON (2003), Classical Dynamics of Particles
and Systems, 5th ed., Harcourt Brace Jovanovich, Orlando, FL. 196, 199, 263
[Math 02] MATHEWS, J., (2002), Numerical Methods for Mathematics, Science and Engi-
neering, Prentice Hall, Upper Saddle River. 181
[Math 92] MATHEWS, J. (1992), Numerical Methods for Mathematics, Science and Engi-
neering, Prentice Hall, Englewood Cliff, NJ. 181
[M&W 65] MATHEWS, J. AND R. L. WALKER (1965), Mathematical Methods of Physics,
Benjamin, Reading, MA. 163
[MW] MATHWORKS, Matlab Wavelet Toolbox, www.mathworks.com/.
[Metp 53] METROPOLIS, M., A. W. ROSENBLUTH, M. N. ROSENBLUTH, A. H. TELLER,
AND E. TELLER (1953), J. Chem. Phys. 21, 1087. 339
[Mold] REFSON, K. Moldy, A General-Purpose Molecular Dynamics Simulation Pro-
gram,www.earth.ox.ac.uk/˜keithr/moldy.html. 364
[M&L 85] MOON, F. C. AND G.-X. LI (1985), Phys. Rev Lett. 55, 1439. 276
[M&F 53] MORSE, P. M., AND H. FESHBACH (1953), Methods of Theoretical Physics,
McGraw-Hill, New York. 377
[MPI] MATH. & COMPUTER SCIENCE DIV., ARGONNE NAT. LAB., The Message Passing
Interface (MPI) Standard, (updated May 9, 2006) www-unix.mcs.anl.gov/mpi/. 304, 500
[MPI2] MATHEMATICS AND COMPUTER SCIENCE DIVISION, ARGONNE NATIONAL
LABORATORY, Web Pages for MPI and MPE, (updated August 4, 2004)
www-unix.mcs.anl.gov/mpi/www. 304, 500
[MPImis] ACADEMIC COMPUTING & COMMUNICATIONS CENTER, UNIV. OF ILLINOIS AT
CHICAGO, Argo Beowulf Cluster: MPI Commands and Examples, (Updated 3 December
2004) www.uic.edu/depts/accc/hardware/argo/mpi routines.html. 304, 500
[NAMD] NELSON, M., W. HUMPHREY, A. GURSOY, A. DALKE, L. KALE, R. D. SKEEL,
AND K. SCHULTEN, (1996), NAMD - Scalable Molecular Dynamics, J. of Supercom-
puting Applications and High Performance Computing, www.ks.uiuc.edu/Research/namd/.
364
Software on the CD 539
[NSF] NATION SCIENCE FOUNDATION SUPERCOMPUTER CENTERS: Cornell Theory Cen-
ter, www.tc.cornell.edu; Natl. Cntr. for Supercomp. Appls., www.ncsa.uiuc.edu; Pittsburgh
Supercomputing Center, www.psc.edu; San Diego Supercomputing Center, www.sdsc.edu;
Natl. Center for Atmospheric research, www.ucar.edu.
[Nes 02] NESVIZHEVSKY, V.V., H. G. BORNER, A. K. PETUKHOV, H. ABELE, S.
BAESSLER, F. J. RUESS, T. STOFERLE, A. WESTPHAL, A. M. GAGARSKI, G. A.
PETROV, AND A. V. STRELKOV, Quantum states of neutrons in the Earth’s gravitational
field, (2002), Nature 415, 297. 361
[Netpbm] NETPBM, a package of graphics programs and programming library,
netpbm.sourceforge.net/doc/. 528
[Ott 02] OTT, E., (2002), Chaos in Dynamical Systems, Cambridge University Press, Cam-
bridge. 259
[Otto] OTTO A., Numerical Simulations of Fluids and Plasmas
how.gi.alaska.edu/ao/sim/chapters/chap6.pdf. . 389
[OR] OUALLINE, S., Practical C Programming, (1997) O’Reilly & Assoc, Sebastopol. 500
[Pach 97] PACHECO, P. S., (1997), Parallel Programming with MPI,, Morgan Kaufmann, San
Diego. 304
[Pan 96] PANCAKE, C. M., (1996), Is Parallelism for You?, IEEE Computational Sci. &
Engr, 3, 18. 304, 317, 318
[PBS] Portable Batch System, www.openpbs.org/. 506
[P&D 81] PEDERSEN, N. F., AND A. DAVIDSON (1981), Appl. Phys. Lett. 39, 830.
[Peit 94] PEITGEN, H.-O., H. JÜRGENS, AND D. SAUPE (1992), Chaos and Fractals,
Springer-Verlag, New York. 296
[Penn 94] PENNA, T. J. P. (1994), Comput. in Phys. 9, 341.
[Perlin] Ken Perlin, NYU Media Research Laboratory, mrl.nyu.edu/˜perlin. 299, 301
[P&R 95] PHATAK, S. C., AND S. S. RAO (1995), Logistic map: A possible random-number
generator, Phys. Rev. E 51, 3670. 258
[PhT 88] PHYSICS TODAY, Special issue on chaos, December 1988. 282
[P&W 91] PINSON, L. J., AND R. S. WIENER (1991), Objective-C Object-Oriented
Programming Techniques, Addison-Wesley, Reading, MA.
[P&B 94] PLISCHKE, M., AND B. BERGERSEN (1994), Equilibrium Statistical Physics, 2nd
Ed., World Scientific Pub. Co., Singapore. 338
[Polikar] POLIKAR,R., The Wavelet Tutorial,
users.rowan.edu/˜polikar/WAVELETS/WTtutorial.html.
[Potv 93] POTVIN, J. (1993), Comput. in Phys. 7, 149. 335
[Pov-Ray] Persistence of Vision Raytracer, www.povray.org. 301
[Pres 94] PRESS, W. H., B. P. FLANNERY, S. A. TEUKOLSKY, AND W. T. VETTERLING
(1994), Numerical Recipes, Cambridge University Press, Cambridge, UK. 128, 139,
160, 163, 165, 175, 220, 383, 400, 405, 412, 439
540 appendix f
[Pres 00] PRESS, W. H., B. P. FLANNERY, S. A. TEUKOLSKY, AND W. T. VETTERLING
(2000), Numerical Recipes in C++, 2nd Ed., Cambridge University Press, Cambridge,
UK. 124, 141, 175, 181, 362
[PtPlot] PTPLOT, a 2-D data plotter and histogram tool implemented in
Java,ptolemy.eecs.berkeley.edu/java/ptplot/.
[PVM] A. GEIST, A, A. BEGUELIN, JACK DONGARRA, WEICHENG JIANG, ROBERT
MANCHEK, AND VAIDY SUNDERAM (1994), PVM: Parallel Virtual Machine A User’s
Guide and Tutorial for Networked Parallel Computing, Oak Ridge National Laboratory,
Oak Ridge, TN.
[Quinn 04] QUINN, M. J. (2004), Parallel Programming in C with MPI and OpenMP, Mc-
Graw Hill Higher Education, New York, NY. 304, 311, 314
[Ram 00] RAMASUBRAMANIAN, K. AND M. S. SRIRAM, (2000), A comparative study of
computation of Lyapunov spectra with different algorithms, Physica D, 139 72. 258, 259
[Rap 95] RAPAPORT, D.C (1995), The Art of Molecular Dynamics Simulation, Cambridge
University Press, Cambridge, UK. 364
[Rash 90] RASBAND, S. N. (1990), Chaotic Dynamics of Nonlinear Systems, Wiley, New
York. 253, 262
[Raw 96] RAWITSCHER, G., I. KOLTRACHT, H. DAI, AND C. RIBETTI (1996), Comput. in
Phys., 10, 335. 408
[R&M93] REITZ, J.R., F. J. MILFORD, AND CHRISTY, R. W. (1993), Foundations of Elec-
tromagnetic Theory, Fourth Ed., Addison-Wesley, Reading, PA.
[Rey 83] REYNOLDS, O. (1883), Proc. R. Soc. London, 35, 84. 455
[Rhei 74] RHEINBOLD, W. C. (1974), Methods for Solving Systems of Nonlinear Equations,
SIAM, Philadelphia.
[Rich 61] RICHARDSON. L. F., (1961), Problem of contiguity: an appendix of statistics of
deadly quarrels, General Systems Yearbook, 6, 139. 289
[Riz] RIZNICHENKO G. Y., Mathematical Models in Biophysics.
http://www.biophysics.org/Portals/1/PDFs/Education/galina.pdf.
[Rowe 95] ROWE, A. C. H. AND P. C. ABBOTT (1995), Daubechies Wavelets and Mathe-
matica, Comput. in Phys. 9, 635-548.
[Russ 44] RUSSELL, J. S. (1844), Report of the 14th Meeting of the British Association for
the Advancement of Science, John Murray, London.
[Sand 94] SANDER, E., L. M. SANDER, AND R. M. ZIFF (1994), Comput. in Phys. 8, 420.
435
[Schk 94] SCHECK, F. (1994), Mechanics, from Newton’s Laws to Deterministic Chaos, 2nd
ed., Springer-Verlag, New York. 282
[Schw 02] SCHWARZCHILD, B., (2002), Phys. Today, 55, 20. 175, 263
[Schd 00] SCHMID, E. W., G. SPITZ, AND W. LÖSCH (2000), Theoretical Physics on the
Personal Computer, 2nd Ed., Springer-Verlag, Berlin.
[Shannon 48] SHANNON, C. E., , (1948), The Bell System Technical Journal, 27, 379. 187
Software on the CD 541
[Shar] SHAROV, A., Quantitative Population Ecology.
http://home.comcast.net/ sharov/PopEcol/popecol.html.
[Shaw 92] SHAW C. T. (1992), Using Computational Fluid Dynamics, Prentice Hall, Engle-
wood Cliff, NJ. 259
[S&T 93] SINGH, P. P., AND W. J. THOMPSON (1993), Comput. in Phys. 7, 388.
[Sipp 96] SIPPER., M. (1997), Evolution of Parallel Cellular Machines Springer-Verlag,
Heidelberg; cell-auto.com/. 389, 435
[Smi 91] SMITH, D. N. (1991), Concepts of Object-Oriented Programming, McGraw-Hill,
New York. 467
[Smi 99] SMITH, S. W. (1999), The Scientist and Engineer’s Guide to Digital Signal Process-
ing, California Technical Publishing, San Diego, California. 296
[Sterl 99] STERLING, T., J. SALMON, D. BECKER, AND D. SAVARESE (1999), How to Build
a Beowulf, MIT Press, Cambridge, MA. 83, 221
[Stez 73] STETZ, A., J. CARROLL, N. CHIRAPATPIMOL, M. DIXIT, G. IGO, M. NASSER,
D. ORTENDAHL, AND V. PEREZ-MENDEZ (1973), “Determination of the Axial Vec-
tor Form Factor in the Radiative Decay of the Pion”, LBL 1707, invited paper at the
Symposium of the Division of Nuclear Physics, Washington, DC, April. 221
[Sull 00] SULLIVAN, D. (2000), Electromagnetic Simulations Using the FDTD Methods,
IEEE Press, New York. 304
[SunJ] SUN JAVA DEVELOPER’S SITE, java.sun.com/. 161, 163
[SGE] SUN N1 GRID ENGINE, www.sun.com/software/gridware/. 426, 429
[SUSE] THE OPENSUSE PROJECT, en.opensuse.org/Welcome to openSUSE.org. v
[Tab 89] TABOR, M. (1989), Chaos and Integrability in Nonlinear Dynamics, Wiley, New
York. 503
506
[Taf 89] TAFLOVE, A AND S. HAGNESS. (2000), Computational Electrodynamics: The Fi-
nite Difference Time Domain Method, 2nd Ed., Artech House, Boston. 175, 276, 438
[Tait 90] TAIT, R. N., T. SMY, AND M. J. BRETT (1990), Thin Solid Films 187, 375. 429
[Thij 99] THIJSSEN J. M. (1999), Computational Physics, Cambridge University Press,
Cambridge, UK. 292
[Thom 92] THOMPSON, W. J. (1992), Computing for Scientists and Engineers, Wiley, New
York. 364, 368
[Tick 04] TICKNER, J. (2004), Simulating nuclear particle transport in stochastic media us-
ing Perlin noise functions, Nucl. Instru. & Mtds, B, 203, 124. 160, 163, 165
[Torque] TORQUE Resource Manager,
www.clusterresources.com/pages/products/torque-resource-manager.php. 299
[UCES] UNDERGRADUATE COMPUTATIONAL ENGINEERING AND SCI-
ENCE,www.krellinst.org/UCES/. 506
542 appendix f
[Vall 00] VALLÉE, O., (2000) Comment on a quantum bouncing ball by Julio Gea Bana-
cloche, Am J. Phys., 68, 672-673.
361
[VdeV 94] VAN DE VELDE, E. F. (1994), Concurrent Scientific Computing, Springer-Verlag,
New York. 304
[VdB 99] VAN DEN BERG, J. C. (ED.) (1999), Wavelets in Physics, Cambridge University
Press, Cambridge. 236
[Vida 99] VIDAKOVIC, B. (1999), Statistical Modeling by Wavelets, Wiley .
[Viss 91] VISSCHER, P. B. (1991), Comput. in Phys. 5, 596. 421, 422
[Vold 59] VOLD, M. J. (1959), J. Collod. Sci. 14, 168. 288
[Volt 26] VOLTERRA, V. (1926) Variazioni e fluttuazioni del numero d’individui in specie
animali conviventi, Mem. R. Accad. Naz. dei Lincei. Ser. VI, 2. 276
[VUE] VUE The Visual Understanding Environment, a tool for managing and integrating
digital resources in support of teaching, learning and research, vue.tufts.edu/. 6
[Ward 04] WARD, D. W AND K. A. NELSON (2004), Finite Difference Time Domain
(FDTD) Simulations of Electromagnetic Wave Propagation using a Spreadsheet, ArXiv
Physics 0402091 1-8. 426
[WL 04] LANDAU, D. P, S.-H. TSAI AND M. EXLER, (2004), A new approach to Monte
Carlo simulations in statistical physics: Wang-Landau sampling, Am. J. Phys. 72,
1294;
LANDAU, D. P, AND F. WANG, (2001), Determining the density of states for classical
statistical models: A random walk algorithm to produce a flat histogram, Phys. Rev. E
64, 056101. 345
[WW 04] WARBURTON, R. D. H. AND J. WANG, (2004), Analysis of asymptotic projectile
motion with air resistance using the Lambert W. function, Am. J. Phys. 72, 1404.
[Whine 92] WHINERAY, J., (1992), An energy representation approach to the quantum
bouncer, Am. J. Phys. 60, 948-950. 361
[Wiki] WIKIPEDIA, the free encyclopedia, en.wikipedia.org/. 221
[Will 97] WILLIAMS, G. P., (1997), Chaos Theory Tamed, Joseph Henry Press, Washington,
D.C. 258
[W&S 83] WITTEN, T. A., AND L. M. SANDER (1981), Phys. Rev. Lett. 47, 1400; (1983),
Phys. Rev. B 27, 5686. 293
[Wolf 85] WOLF, A., J. B. SWIFT, H. L. SWINNEY, AND J. A. VASTANO, (1985), Deter-
mining Lyapunov Exponents from a Time Series Physica D, 16, 285. 258
[Wolf 83] WOLFRAM S. (1983), Statistical Mechanics of Cellular Automata, Rev. Mod.
Phys. 55, 601. 298
[XWIN32] X-WIN32, a focused PC X server, www.starnet.com/products/xwin32/. 481
[Yang 52] YANG, C. N. (1952), The Spontaneous Magnetization of a Two-Dimensional Ising
Model Phys. Rev. 85, 809. 338
[Yee 66] YEE, K. (1966), IEEE Transactions on Antennas and Propagation AP-14, 302. 428
Software on the CD 543
[Z&K 65] ZABUSKY, N. J., AND M. D. KRUSKAL (1965), Phys. Rev. Lett. 15, 240. 442
[Zucker] ZUCKER, M. The Perlin noise FAQ; see too JÖNSSON, A., Generating Perlin Noise,
www.angelcode.com/dev/perlin/perlin.asp.

