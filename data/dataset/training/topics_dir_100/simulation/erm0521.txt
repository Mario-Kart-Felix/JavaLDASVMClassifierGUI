T
27March/Apr i l  2005 EDUCAUSE r e v i e w26 EDUCAUSE r e v i e w  March/Apr i l  2005 © 2005  And r i es  van  Dam,  Sascha  Becke r ,  and  Rosemary  Miche l l e  S impson
Andries van Dam is Professor of Computer Science and Vice President for Research at Brown University. He has been working on electronic books
with interactive illustrations for teaching and research since the late 1960s. Sascha Becker is a multidisciplinary software designer, developer, user,
and critic, currently working as a research programmer in the Brown University Department of Computer Science. Rosemary Michelle Simpson is
an information structures designer currently working in the Brown University Department of Computer Science. This article is drawn from a pre-
sentation delivered at the 2004 Aspen Symposium of the Forum for the Future of Higher Education.
NEXT-
GENERATION 
EDUCATIONAL 
SOFTWARE
Why We Need It
& a Research Agenda for Getting It
he dream of universal access to high-quality, personalized educational con-
tent that is available both synchronously and asynchronously remains unrealized. For
more than four decades, it has been said that information technology would be a key
enabling technology for making this dream a reality by providing the ability to produce
compelling and individualized content, the means for delivering it, and effective feed-
back and assessment mechanisms. Although IT has certainly had some impact, it has
become a cliché to note that education is the last field to take systematic advantage of IT.
There have been some notable successes of innovative software (e.g., the graphing
calculator, the Geometer’s Sketchpad, and the World Wide Web as an information-
storage and -delivery vehicle), but we continue to teach—and students continue to
learn—in ways that are virtually unchanged since the invention of the blackboard.
I l l us t ra t ion  by  S teve  McCracken ,  ©  2005
By Andries van Dam, Sascha Becker, and Rosemary Michelle Simpson
There are many widely accepted reasons
for the lack of dramatic improvement:
■ Inadequate investment in appropriate
research and development of authoring
tools and new forms of content 
■ Inadequate investment in the creation
of new dynamic and interactive content
that takes proper advantage of digital
hypermedia and simulation capabili-
ties (as opposed to repurposed print
content) at all educational levels and
across the spectrum of disciplines
■ Inadequate investment in appropriate
IT deployment in schools (e.g., although
PCs are available in K-12, there are too
few of them, they are underpowered,
and they have little content beyond
traditional “drill-and-kill” computer-
aided instruction, or CAI; at the post-
secondary level there is more avail-
ability of computers and software,
plus routine use of the Internet, but
still a dearth of innovative content that
leverages the power of the medium)
■ Inadequate support for teacher educa-
tion in IT tools and techniques and for
the incorporation of IT-based content
into the curriculum
■ The general conservatism of educational
institutions 
Despite this disappointing record, we
remain optimistic. The dramatic advances
in hardware technology, especially during
the last decade, provide extraordinary
new capabilities, and the desire to “do
something” to address the need  for life-
long, on-demand learning is finally being
widely recognized. The ubiquity and ac-
cessibility of the Internet has given rise to
a new kind of learning community and en-
vironment, one that was predicted by Tim
Berners-Lee in his 1995 address to the
MIT/Brown Vannevar Bush Symposium1
and that John Seely Brown elaborated into
the rich notion of a learning ecology in his
seminal article “Growing Up Digital: How
the Web Changes Work, Education, and
the Ways People Learn.”2 There is great
hope that this emergent learning environ-
ment will in time pervade all organiza-
tions, binding learners and teachers to-
gether in informal,  ever-changing,
adaptive learning communities.
Here we will first recapitulate some
well-known technology trends that make
current platforms so exciting, and then
we will briefly discuss leveraging this
technology into highly desirable forms of
learning. Next we will examine an IT-
oriented education research agenda pre-
pared by a consortium called the Learn-
ing Federation and will present some
promising educational software experi-
ments being conducted at Brown Univer-
sity. Finally we will describe an as-yet-
unrealized concept called “clip models”:
simulation-based interoperable families
of components that represent multiple
levels of explanatory power and simula-
tion fidelity designed to be assembled
into systems. We make no attempt here to
present a critical review of the entire field
of educational software or of its impact. A
variety of organizations, journals, and
conferences is addressing the uses and
impact of IT in education; in particular,
EDUCAUSE and its Center for Applied
Research (ECAR) provide a good intro-
duction to resources and studies of IT in
higher education. 
Technology Trends
Exponential advances in computer archi-
tecture in the last two decades have enabled
the creation of far more compelling and en-
gaging educational software than we could
have dreamed of in the Apple II days. Ad-
vances in four areas of IT will continuously
raise the bar on user experiences: platform
power used for computation and graphics/
multimedia; networking; ubiquitous com-
puting; and storage.
The commoditization of the necessary
platforms, a trend described by Moore’s
“law,” is epitomized by supercomputer
power and high-end graphics/multi-
media capabilities in desktop and laptop
computers costing less than $1,000 and
even in specialized game boxes costing
less than $200. Alan Kay’s Dynabook vi-
sion can at long last be realized,3 and even
the personalized and ever-evolving
“young lady’s illustrated primer” from
Neal Stephenson’s Diamond Age will leave
the realm of science fiction.4
Advances in networking enrich user ex-
periences with ubiquitous, always-on,
high-bandwidth connections. Already,
gigabit networking over local area net-
works is a reality, and the Internet2 proj-
ect is creating the core of a massively
broadband global network. Wireless con-
nectivity is widely available in both devel-
oped and underdeveloped countries and
will rapidly increase in bandwidth. The
commoditization of bandwidth elimi-
nates physical distance and carrier costs
as a factor in providing resources to a
worldwide audience.
Ubiquitous computing environments5
have become commonplace; embedded
sensors and microcomputers transform or-
dinary passive objects into intelligent ob-
jects that interact with each other and with
us in a great diversity of form factors. Keep-
ing pace with the hardware is ever-more so-
phisticated software that uses the results of
artificial intelligence research in practical
applications of the “smart objects.” 
Compelling experiences and work
products alike require data storage that is
reliable, fast, and inexpensive. A 1.44-
megabyte floppy disk cost a few dollars in
the early 1990s and couldn’t be relied on
to keep data safe during a bus ride home
from school; today, blank DVDs can be
permanently “burnt” with 4.7 gigabytes of
data for less than a dollar each, and 20-
gigabyte mobile storage devices cost ap-
proximately $200, making the Library of
Congress accessible anywhere. In addi-
tion to raw capacity, however, data needs
that must be addressed include security,
privacy, validity, and format persistence. 
Despite predictions that we will hit a
technological wall in the coming decade,
new advances repeatedly push any such
wall out into the indefinite future. For ex-
ample, developments in nanotechnology
and quantum computing promise new
capabilities in all four areas. Indeed, one
can only wish that the same exponential
improvement curves that apply to
28 EDUCAUSE r e v i e w  March/Apr i l  2005
Despite predictions that we willhit a technological wall in the
coming decade, new advances
repeatedly push any such wall out
into the indefinite future. 
hardware also applied to software and
content creation. Regrettably, both these
hugely important areas have shown, at
most, modest improvements, and there
are no signs of breakthrough technology
on the horizon—only continued slow,
evolutionary progress. But it is precisely
because of the revolutionary improve-
ments in hardware that we can create
breakthrough experiences and content.
Now is the time to mount such an effort.
IT in Education: Appropriate Role?
So, what is the appropriate role for IT in
education, in the broadest sense? As al-
ways, IT’s role is to augment (not to re-
place) the teacher, to provide human-
centered tools that encourage and
support adaptability and flexibility, and
to enable appropriate modes of learning
(e.g., small team interaction and not just
individual task performance).6 Principles
such as situated, active learning (i.e.,
learning by doing rather than just by lis-
tening)—principles that foster interactive
involvement of the learner with the edu-
cational materials—are well supported by
current technology trends. However, one
size does not fit all in educational soft-
ware. Unless new tools allow exploration
at multiple levels of detail and accommo-
date diverse learning styles,7 they will be
just as limited as ordinary textbooks. But
this is easier to say than to do: there is no
collective experience in authoring at
multiple levels of detail and multiple
points of view. Such authoring requires
the development of skills and tools of far
greater power than we have experience
with to date. 
The most important task in the appli-
cation of IT to education is to author
stimulating content that is as compelling
as “twitch games” or even as strategy
ga m e s  a p p e a r  to  b e .  Ne w  c o n te n t
dropped into existing curricula typically
shows no improvement in outcomes; we
must also redefine curricula to support
learner-centered, on-demand explo-
ration and problem-solving, and we must
break down traditional disciplinary
boundaries. We must also train educators
to take advantage of these new capabili-
ties. This will require massive invest-
ment, on a scale we have not encountered
heretofore. This content creation, curric-
ula adaptation, and educator training will
also require a long period of experimen-
tation, as well as tolerance for the false
starts that are an inevitable part of all in-
novation processes. For example, where-
as classical CAI was thought to hold great
promise in the 1960s, its applicability
turned out to be rather limited; the same
held for Keller plan self-paced instruc-
tion and other innovations that are in fact
now reappearing in different guises.
Content and curriculum alone are not
sufficient. We must provide support for
all aspects of learning, in both formal and
informal education, not just in schools
but in all venues, ranging from the home
to the office and the factory floor—any-
place where learners gather, singly or in
groups. In addition, we must provide
support for all aspects of this process, in-
cluding course administration (as
WebCT and its competitors are doing),
continuous assessment (a deep research
problem), and digital rights management
(still a very contentious and difficult soci-
etal, commercial, and research problem).
Returning to the topic of content, we
must develop software that accommo-
dates many different human-computer
interactions, from single-user to mas-
sively collaborative multi-user. Genres
must be equally diverse, from cognitive
tutors such as CMU’s Pump Algebra
Tutor, or PAT (http://act.psy.cmu.edu/
awpt/awpt-home.html), to simulation-
and rule-based interactive models (mi-
croworlds), massive multiplayer games,
and robots constructed and programmed
to carry out specified tasks.
Even before such adaptive, personal-
ized content is widely available, we must
also rethink learning environments—that
is, envision profound structural change at
all levels of education to accommodate
the kind of learning the new content facil-
itates. Early examples of experiments in
structural change include Eric Mazur’s
Peer Instruction Physics (http://mazur-
www.harvard.edu/education/education
menu.php),  the RPI Studio Model
(http://ciue.rpi.edu/), and the Virginia
Tech Math Emporium (http://www.
emporium.vt.edu/). Both the RPI Studio
Model and the Virginia Tech Math Empo-
rium change not just the structure of the
educational process but even the facili-
ties required. This is just the beginning of
rethinking college and university instruc-
tion from the ground up. In addition, dis-
tance learning, as embodied by the Open
University (http://www.open.ac.uk/) and
the University of Phoenix (http://www.
phoenix.edu/), shows that non-campus-
based instruction can work, although the
materials used are not particularly inno-
vative as yet. On a cautionary note, we
should add that there have been many re-
cent failures in commercial distance
learning. Traditional colleges and univer-
sities with classroom/laboratory instruc-
tion will not soon be replaced, although
they will certainly be augmented by
newer, IT-based forms of learning.
The Computing Research Associa-
tion’s “Grand Challenge 3”—“Provide a
Teacher for Every Learner” (http://www.
c r a . o r g / r e p o r t s / g c . s y s t e m s . p d f ) —
describes some of the genres mentioned
above, but the most important conclu-
sion of that report, reflected in its title, is
that by providing powerful tools, we offer
the opportunity to rethink the relation-
ship between teachers and learners. The
appropriate use of IT will empower teach-
ers to enhance their mentoring roles and
can supplement such teacher support
with peer and computer-based mentoring
and tutoring to provide students with es-
sentially full-time, on-demand, context-
specific help. Building domain-specific
mentoring, tutoring, and question-
answering is scarcely a solved problem
and will require a very significant re-
search and development (R&D) effort.
Getting There: Learning 
Federation Research Roadmaps
To better understand the issues involved
and to direct a focused research invest-
30 EDUCAUSE r e v i e w  March/Apr i l  2005
U nless new tools allow explorationat multiple levels of detail and
accommodate diverse learning styles,
they will be just as limited as 
ordinary textbooks. 
ment effort, Andries van Dam helped to
found a small steering group that has
proposed the creation of a nonprofit, 
industry-led foundation, called the Learn-
ing Federation (http://www.thelearning
federation.org/), modeled on the highly
successful Sematech Consortium (http://
www.sematech.org). The Learning Federa-
tion is a partnership joining companies,
colleges and universities, government
agencies, and private foundations whose
purpose is to provide a critical mass of
funding for long-term basic and applied
pre-competitive research in learning sci-
ence and technology. This research, to be
conducted by interdisciplinary teams, is
meant to lead to the development not only
of next-generation authoring tools but
also of exemplary curricula for both syn-
chronous and asynchronous learning.
The Federation’s first task was to pro-
duce a Learning Science and Technology
R&D Roadmap. This roadmap describes
a platform-neutral research plan to
stimulate the development and dissemi-
nation of next-generation learning tools,
with an initial focus on postsecondary
science, technology, engineering, and
mathematics. The component roadmaps,
which address five critical focus areas for
learning science and technology R&D,
were developed using expert input from
companies, colleges and universities,
government research facilities, and oth-
ers with unique expertise during a series
of specialized workshops, consultative
meetings, and interviews. Each roadmap
provides an assessment of the R&D
needs, identifies key research questions
and technical requirements, and specifies
long-term goals and three-, five-, and ten-
year benchmarks—the roadmap to the
long-term goals. The following sections
give the abstracts from the component
roadmaps, along with the URLs where
the full PDF files may be downloaded.
Instructional Design: Using Games 
and Simulations in Learning
“Learning environments that provide
learners opportunities to apply their
knowledge to solve practical problems
and invite exploration can lead to faster
learning, greater retention, and higher
levels of motivation and interest. Unfor-
unately, these learning strategies are
rarely used today because they are diffi-
cult to implement in standard classroom
environments. Expected improvements
in technology have the potential to signif-
icantly reduce the cost and complexity of
implementing learning-by-doing en-
vironments. The combined forces of
high-powered computing, unparalleled
bandwidth, and advances in software ar-
chitecture are poised to make realistic
gaming and simulation environments
more feasible and economical. Because
these tools will be increasingly available,
it is important to understand appropriate
contexts and methods for implementa-
tion. The challenge is to understand how
the tools should be used, with whom and
for what?” See (http://www.thelearning
federation.org/instructional.html).
Question Generation and 
Answering Systems
“Question generation is understood to play
a central role in learning, because it both
reflects and promotes active learning and
31March/Apr i l  2005 EDUCAUSE r e v i e w
construction of knowledge. A key chal-
lenge to researchers and practitioners alike
is to find ways to facilitate inquiry by
taking advantage of the benefits offered
by emerging technologies. Further ex-
ploration is needed in the realm of intu-
itive interfaces that allow the learner to
use spoken language, or coach the learner
on how to ask questions, tools to enable
answers to learners’ questions—includ-
ing linking learners to real people, as well
as the creation of intelligent systems that
ask the learner questions or present
problems that require major attention
and conversation.” See (http://www.
thelearningfederation.org/question.html).
Learner Modeling and Assessment
“Assessment generates data for decisions
such as what learning resources to pro-
vide individual learners and who to se-
lect or promote into particular jobs.
These decisions are only as valid as the
data and interpretations that are avail-
able. Ideally, every educational decision-
maker, from teacher to human resource
director, would have access to real-time
valid data to make a decision about an in-
dividual, group, or program. There is a
critical need to articulate more precisely
and reach consensus on many of the the-
oretical underpinnings and models that
drive our assessment practices.” See
(http://www.thelearningfederation.
org/learner.html).
Building Simulations 
and Exploration Environments
“Research has demonstrated that simula-
tion environments are powerful learning
tools that encourage exploration by al-
lowing learners to manipulate their
learning experience and visualize results.
Simulations used in academic settings
can enhance lectures, supplement labs,
and engage students. In the workplace,
simulations are a cost-effective way to
train personnel. Despite important suc-
cesses in the use of simulation and syn-
thetic environments, there are still a
number of limitations to current applica-
tions and programs. The goal of this R&D
effort is to make simulations and syn-
thetic environments easier to build and
incorporate into learning environments.”
See (http://www.thelearningfederation.
org/building.html).
Integration Tools for Building and
Maintaining Advanced Learning Systems
“As specifications and standards have
been developed to support web-based
system directed learning systems, the
means for creating interoperable and ro-
bust instructional content have emerged.
However, these specifications have de-
fined a technically complex infrastruc-
ture that is unfriendly to instructional de-
signers. Designers should be able to focus
entirely on content, the needs of stu-
dents, and instructional theory and not
on the mechanics of the software. A vari-
ety of authoring and integration tools are
needed to make it easy to identify soft-
ware resources and to combine these re-
sources into a functioning system.” See
(http://www.thelearningfederation.org/
integration.html).
Beginnings: Brown University Projects
Microworlds
At Brown University, partially inspired by
Kay’s powerful Dynabook vision,8 we
have been particularly interested in
building simulation and exploration en-
vironments, often referred to as micro-
worlds. These can be used to teach abstract
concepts, such as Newton’s laws and the
Nyquist limit for signal processing, and
skills, such as spatial visualization and in-
tegration by parts. The combination of
the Web and Java applets has resulted in a
proliferation of applets across a broad
range of subjects. 
For the last decade, inspired by many
applets on the Web, we have been devel-
oping computer graphics teaching ap-
plets called Exploratories (see Figure 1).
These highly interactive, simulation-
based applets are built from reusable
software components and can be embed-
ded in a Web-based hypermedia environ-
ment or used as downloadable compo-
nents in a wide variety of settings. Their
design builds on a geometric structure to
simulate behavior. Users can control the
environment and experiment with differ-
ent variables through interface-exposed
parameters. To date we have over fifty
Exploratories in computer graphics
a l o n e  ( h t t p : / / w w w. c s . b r o w n . e d u /
exploratories/).
Gesture-Based Tablet PC Applications
In an increasingly ubiquitous world of
iPods, digital camera cell phones, and
wireless everything, the WIMP (“Win-
dows, Icons, Menus, and Pointers”) inter-
face has been gradually augmented by
post-WIMP interface techniques as mo-
bile users experience the convergence of
media and communication technologies.
The laptop workhorse has been expand-
ing its capabilities as well with the advent
of the Tablet PC and its pen-based inter-
face. Until the last decade, pen-based
technology was not good or cheap
enough for widespread use. Gesture
recognition, handwriting recognition,
and digitizer technology have signifi-
32 EDUCAUSE r e v i e w  March/Apr i l  2005
Figure 1. Exploratory on color mixing, highlighting the differences between 
mixing light and mixing paint
cantly improved in performance and
availability in the last few years; now ap-
plications can be developed and de-
ployed at retail scope. 
Just as with microworlds, the Brown
computer graphics group has been ex-
perimenting with gesture-based user in-
terfaces and applications for many years9
and is currently developing gestural in-
terfaces for the Tablet PC. Two of these
are MathPad2 and ChemPad.
MathPad2. Mathematical sketching is a
pen-based, modeless gestural interaction
paradigm for mathematics problem-
solving. Whereas it derives from the
familiar pencil-and-paper process of
drawing supporting diagrams to facili-
tate the formulation of mathematical ex-
pressions, users can also leverage their
physical intuition by watching their
hand-drawn diagrams animate in
response to continuous or discrete
parameter changes in their written
formulas. Implicit associations that are
inferred, either automatically or with
gestural guidance, from mathematical
expressions, diagram labels, and drawing
elements drive the diagram animation.
The modeless nature of mathematical
sketching enables users to switch freely
between modifying diagrams or expres-
sions and viewing animations. Mathe-
matical sketching can also support
computational tools for graphing, manip-
ulating, and solving equations.
The MathPad2 mathematical sketching
application currently uses MATLAB as its
underlying math engine and provides a
fully gestural interface for editing. Expres-
sions can be deleted, edited, and rerecog-
nized in a fully modeless operation (see
Figure 2).
ChemPad. Organic chemistry is the study
of the structure and function of carbon-
based molecules. These molecules have
complex, three-dimensional structures
that determine their functions. Ideally,
students would do all their thinking and
drawing in three dimensions (3D), but
whiteboards and paper notebooks support
only 2D structures and projections of 3D
and higher-dimensional structures. To
compensate, organic chemists use a
complicated 2D schematic notation for
indicating the spatial arrangement of
atoms in a molecule. With practice and
insight, beginning chemists can develop
the ability to look at such a 2D schematic
description of a molecule and auto-
matically construct a mental model of the
3D structure of that molecule. 
Teachers of organic chemistry identify
this spatial understanding as a key deter-
minant of whether students will succeed
in organic chemistry. We have been de-
signing and developing a software proj-
ect, ChemPad, whose purpose is to help
organic chemistry students develop an
understanding of the 3D structure of
molecules and the skill of constructing a
3D mental model of a molecule that
matches a 2D diagram (see Figure 3).
ChemPad fosters this understanding by
allowing the student to sketch a 2D dia-
gram and then to see and manipulate the
3D model described by the diagram. 
33March/Apr i l  2005 EDUCAUSE r e v i e w
A pen-based interface is particularly
appropriate for drawing organic chem-
istry molecules because the existing soft-
ware tools in this area are difficult to learn
and use, which places them out of the
reach of most students. Drawing with pen
and paper, though, is not entirely satisfac-
tory; it is difficult to produce neat draw-
ings, and it is difficult to erase and correct
errors neatly. ChemPad addresses both
these issues, with a simple interface that
mimics drawing on paper and with a
“beautify” function that “tidies up” a stu-
dent’s drawing. ChemPad also provides
validity-checking; many of the structures
that beginning students draw do not de-
scribe physically possible molecules. Un-
like paper and pencil, ChemPad can de-
tect and indicate certain kinds of errors.
One possible extension of this approach
would be to add simulation capabilities
so that the static ball-and-link 3D dia-
grams can start to approximate the actual
dynamics of molecular interaction.
Limitations of Our Current Work
Although microworlds have been useful
adjuncts to the undergraduate computer
graphics course, they fall short of the
goals for a far more ambitious vision.
Microworlds and Exploratories are re-
stricted to single concepts with a small set
of parameters. However, because they are
component- and parameter-based, they
illustrate some of the fundamental prin-
ciples that will be essential in fully func-
tioning clip-model environments, and
they open possibilities for evolving even
more flexible structures. The combina-
tion of fluid and multi-POV (point of
view) hypermedia information structures
with component-based software architec-
tures may provide a foundation on which
we can build.
Tablet-PC-based gestural interfaces to
applications are underdeveloped be-
cause the state-of-the-art in robust user-
independent gesture recognition is still
primitive. Furthermore, gesture sets are
anything but self-disclosing, and they
take considerable time to learn. Finally,
our experiments thus far are essentially
single-user in their orientation and don’t
facilitate a collaborative, team-based ap-
proach to learning. The next section ad-
dresses some of the issues involved in de-
signing software that can be adapted to
multiple needs, users, and levels of detail.
Clip Models: A Proposal for 
Next-Generation Educational 
and Research Software
Over forty years ago, Jerome Bruner pro-
posed a radically new theory of educa-
tion: “Any subject can be taught effec-
tively in some intellectually honest form
to any child at any stage of develop-
ment.”10 Although many people have dis-
puted the more extreme claims attached
to that hypothesis, it is an admirable goal.
One way to implement it is through the
“spiral approach to learning,” common to
formal education, in which a learner en-
counters a topic multiple times through-
out his or her education, each time at an
increasing level of sophistication. Fur-
thermore, at any stage, the learner should
be able to mix and match educational
modules at different levels of sophistica-
tion within the same general topic area.
Simpler modules can offer overviews of a
subject for review or provide context
34 EDUCAUSE r e v i e w  March/Apr i l  2005
Figure 3.The ChemPad program displays an organic molecule, 2-chloro-ethanol.
On the right side, the user has sketched the non-hydrogen atoms of the molecule
using standard organic chemistry conventions. On the left side, the program has
generated and rendered a 3D view of the molecule.The user is able to rotate the 
3D representation and examine it from arbitrary points of view.
Figure 2. MathPad2 sketching interface of a mass spring system
when the intent is to go more deeply into
related topics. 
The kinds of modules we are most in-
terested in here are simulation- or rule-
based modules that help create ex-
plorable models of subsystems, which
can be linked into increasingly higher-
level subsystems. Such modules can help
simulate most aspects and components
of the natural and man-made worlds. We
will focus here on simulating subsystems
of the human body at all levels, from the
molecular to the cellular to the gross
anatomical. Each subsystem of the
human body must then be simulated at a
level appropriate to the (educational)
purpose. There is not just a single
model/simulation for each component of
the system (e.g., the heart or lungs) but a
family of models/simulations varying in
explanatory power and simulation fi-
delity—not to mention, ideally, in the
learning style it is to match. Furthermore,
since subsystems interact with each
other, the models and their underlying
simulations must be able to interoperate.
We summarize the properties of these
types of models with the term “clip mod-
els”: simulation-based families of compo-
nents that represent multiple levels of ex-
planatory power and simulation fidelity
designed to interoperate and to be assem-
bled into systems. In particular, unlike
clip art, which represents only images,
clip models emphasize behavior, inter-
action/exploration, and interoperability.
This concept of mix-and-match,
multi-LOD (level of detail) models poses
huge challenges to would-be imple-
menters. The inherent challenges of
building multi-resolution, multi-view,
multi-complexity interoperating simula-
tions have not yet been confronted be-
cause most simulation efforts have been
standalone projects. In the same way,
repositories of learning objects have
stored only objects at a single level of ex-
planatory power, and component frame-
works in use by software developers have
not been designed with the complexity of
interoperation between components at
different levels of detail in mind. 
A Biological Scenario
The concept of clip models can best be
explained with an illustration. The details
don’t really matter; the important thing is
to note the complexity of the relation-
ships between simulated components
and the potential applications of this 
family of simulations for education and
research. 
Figures 4, 5, and 6 are an abstract rep-
resentation of how the heart and vascular
systems interact with other systems used
by the human body to regulate oxygena-
tion—that is, to make sure that we have
enough oxygen in our blood, and not too
much. This homeostasis, crucial for
maintaining life, relies on several inter-
connected mechanisms, including func-
tions of the kidney, the heart, the brain,
the lungs, and chemoreceptors located
throughout the body.
All of these systems are connected by
the blood, and each of them plays a
slightly different role. The blood’s behav-
ior as an oxygen carrier is determined by
macro- and microscopic factors, from the
fraction of blood composed of red blood
cells, visible to the human eye in a test
tube, to the electrostatic attraction be-
tween oxygen and hemoglobin mole-
cules at a scale too fine to be seen with any
microscope. Hormones that regulate the
actions of the kidney, the heart, and the
lungs are generated by the kidney, the
brain, the lungs, and the endocrine sys-
tem, including endocrine glands located
in or near the brain.
The kidneys monitor and correct vari-
ous characteristics of the blood. To un-
derstand their function, we must first
perceive them at a coarse level as organs
that produce urine by filtering the blood.
At this coarse scale, we must understand
only that blood is delivered to and ac-
cepted from the kidneys by large blood
vessels and that the kidneys produce
urine; this level of understanding is 
appropriate for an elementary school 
student. To understand how the kid-
neys perform this function, a more 
advanced learner must examine their
structure at a much finer scale, the scale
of the nephron, of which each kidney has
millions.
The heart rate and the volume of blood
ejected per heartbeat control the rate of
36 EDUCAUSE r e v i e w  March/Apr i l  2005
Figure 5.A more detailed view of the 
kidney, a cross section with some of the 
internal structures
Figure 6.A more detailed view of a
nephron, the microscopic functional unit
of the kidney
Figure 4. Elements in the system for control of oxygenation in the human body 
38 EDUCAUSE r e v i e w  March/Apr i l  2005
distribution of oxygen to the body; these
factors are jointly controlled by the brain
and by the heart itself. The lungs’ respira-
tion rate and inhalation volume are con-
trolled by the brain via nerves, but the
oxygen absorption and the carbon diox-
ide elimination rate are also determined
by the concentration of these gases in the
blood. The carotid bodies, in the neck,
monitor the oxygen concentration in the
blood and inform the brain when more
oxygen is needed. The brain then issues
hormones and neural signals, carried by
the blood and the nervous system to other
organs, which adjust their operation to
correct the problems. 
In our simplified illustration, we show
three levels of detail for examining the
roles of the heart and the kidneys in
homeostatic oxygenation. Figure 5 illus-
trates more detail of the kidney, a cross
section with some of the internal struc-
tures. Figure 6 depicts a single nephron in
the kidney. A learner can dynamically se-
lect which level to examine and may ex-
plore different levels at different times,
depending on need. Clearly, clip-model
exploration by itself may not suffice in an
educational context. We must not only
embed it in explanatory context (e.g., a hy-
permedia document) and organizational
structure (e.g., a self-paced course) but
also enrich it with some type of problem-
solving and/or construction activity and
continuous feedback and assessment.
Clip-Model Implications
The interconnected mechanisms in the
example above, along with the funda-
mental interconnectedness of system
components in all disciplines, cannot all
be studied or understood at once, nor can
they be understood with purely linear
thought . The learner ’s exploratory
process naturally follows the intercon-
nected web of causality, but linear or hier-
archical organizations (such as those in
most software data sources and all text-
books) force the learner into an artifi-
cially linearized exploration. Lineariza-
tion discourages the cross-disciplinary
insights that fuel deep understanding,
since it encourages compartmentalized
rote knowledge. 
As noted above, the varied needs of
audiences at many different levels of so-
phistication preclude a one-to-one map-
ping between a given concept (such as the
circulation of blood through the cardio-
vascular system) and a single clip model.
Thus, instructional designers must think
not in terms of creating a single clip
model for a given topic but in terms of
creating one or more components in a
family of interrelated clip models that
cover a broad range of explanations and
their representations. These models must
correctly reflect the ontology and seman-
tics of the subject matter at each point
along the multiple axes of age, knowledge
level, task, and individual learning. We
must also accommodate the variety of
learning environments in which such
clip models will be presented. These in-
novative and, by their nature, emergent
learning environments must be made
available online and on-site, in synchro-
nous and asynchronous and in virtual
and real classrooms, servicing both single
on-demand learners and collaborative
learners, either in impromptu virtual
study groups or in formats yet to be de-
fined. Another dimension we need to ex-
plore more deeply is team collaboration.
Clearly, these requirements present a
huge challenge for instructional design
and learning technology.
The variety of pedagogical needs that
clip models must satisfy is a complicating
factor that makes their design immensely
harder than that of ordinary components
in standard software engineering. A po-
tential approach to thinking about the
problem may be to use an extension of
the MVC (Model-View-Controller) para-
digm of object-oriented programming to
describe the necessary interrelationships
between these different concept repre-
sentations. Each concept or real-world
object must be represented by a family of
models (e.g., the heart as a pump, the
heart as a muscle, the heart as an organ in
the chest cavity), with widely different de-
grees of sophistication. Each model sup-
ports multiple views (e.g., simplified 3D
models, realistic 3D models, 2D schemat-
ics, the sound heard through a stethoscope)
T he varied needs of audiences atmany different levels of
sophistication preclude a one-to-one
mapping between a given concept 
and a single clip model. 
and, for each view, multiple controllers
that may present a learner-chosen UI
(user interface) style. Multiple models
that must interact, regardless of level of
detail and simulation fidelity, geometri-
cally complicate the single-model para-
digm of classic MVC.
Challenges
This intersection of simulation science,
software engineering, ontology (formal
naming scheme) building, instructional
design, and user interface design forms
the technological aspect of this complex
problem. In addition to the technological
challenges, there are interdisciplinary or-
ganizational challenges: building clip
models is essentially a new design disci-
pline that requires collaborative teams of
experts from cognitive science, social sci-
ences, arts, design, story-telling profes-
sions, information structure design, and
instructional design—working with
teachers, domain experts, simulation sci-
entists, and computer scientists. We can
identify challenges for ontological engi-
neering, simulation science, software en-
gineering, and educational design. 
As a prerequisite to interoperation,
simulation elements must agree on the
ontology of the conceptual realm they
represent. Without a shared ontology or
mappings between related ontologies,
two simulation elements cannot inter-
operate if they disagree on the point in
the nephron at which the “filtrate” be-
comes “urine” or the names for the lobes
of the liver. Furthermore, the ontology
must encompass not just (geometric)
structure (anatomy, in the case of biolo-
gicial systems) but also behavior (bio-
chemical, electrochemical, biomechani-
cal, etc.), a largely untackled problem, at
least for biology. As an additional com-
plication, when you have a single author
or a small team of authors writing a sin-
gle book targeted at a single audience,
the domain specification as seen in the
definition and relationships of concepts
and terms is an important but manage-
able task. When you expand the context
as described above, the situation be-
comes orders of magnitude more com-
plex. Who will define the master ontol-
ogy? How will  other classification
schemes and vocabularies build a corre-
spondence map? Some sort of collabora-
tive approach to ontological engineering
will have to be used in order to build an
ontology that is acceptable to many
members of a given field.
Simulation science does not have a suffi-
ciently flexible framework for wiring to-
gether components of a simulation from
various providers that were not designed
to interoperate from the beginning. How
to connect simulations from different
problem domains for the same subsystem
is still a difficult problem. For example,
simulating the heart’s operation biochem-
ically, electrochemically, and with compu-
tational fluid dynamics, while dealing
with flexible (nonrigid) and time-varying
geometry and both normal and abnormal
behavior, is still a daunting problem. Even
with a standard vocabulary, adaptive
multi-resolution simulations will be even
harder to interoperate; how can they de-
termine at what level of detail to share in-
formation? If we are running interactive
simulations, should we allow algorithms
to run with graceful degradation in order
to meet time requirements? What is the
nature of such approximations? How can
the valid operating ranges of particular
simulations be determined? How can the
simulations report when they venture be-
yond those ranges? If these simulations
are to be used in real science, as we hope,
they must have a mechanism for compar-
ing them with experimental results and
for validating their calculations. How can
a researcher compare predictions made
by a Stanford heart model and by a Har-
vard heart model? How will a kidney
model created by nephrologists at Johns
Hopkins share data with a heart model
from Stanford or a lungs model from Cal-
tech not purposely designed to interoper-
ate? How can a seventh-grade teacher in
Nebraska use a fourth-grade teacher’s set
of human anatomy clip models as the
basis for a more detailed model of the cir-
culatory system?  
The software engineering challenges
range from the commercial and social
difficulties of persuading scientists to
work within a common model to the 
software design characteristics that will
enable flexibility at all levels. Just as 
object-oriented programming is a vast im-
provement in the power of abstraction
compared to assembly language, so must
the clip-model framework design be to
today’s component frameworks; the chal-
lenges are simply too great to be addressed
by incremental advances. A clip-model
framework must address various ques-
tions. How can simulations ensure that
they get the data they need, in the format
they need, regardless of the level of fidelity
at which connected clip models are run-
ning their simulation? For example, how
will a heart model cope with changing
stiffness in the valves if the valve model is
not designed to adjust to stenosis? What
protocols will keep all the simulation
components synchronized in time, even if
one runs in real time and another takes a
day to compute a single time-step? Who
will maintain the repository of code? Who
will control the standards? How can inter-
operability be preserved when some com-
ponents are proprietary?
The educational design challenges of our
vision are the same problems facing
today’s educational software designers.
How can teachers, learners, and scientists
find the components that best meet their
needs? How does a student figure out that
he or she is an auditory learner if the stu-
dent is bombarded with visual materials?
How can users evaluate the reliability,
correctness, bias, and trustworthiness of
authors of the components? 
Progress So Far 
Our field is more prepared to address this
challenge today than we were twenty years
ago. Software engineers used to joke and
complain about rewriting a linked list, a
common data structure, in every new lan-
guage, project, and environment. Since
then, library standardization (especially
the C++ Standard Library, the Standard
40 EDUCAUSE r e v i e w  March/Apr i l  2005
Building clip models is essentially anew design discipline that requires
collaborative teams of experts.
Template Library, Microsoft .NET, and
Java’s extensive built-in libraries) has made
reusable data structures available to almost
any software project. Reusable component
libraries have advanced to include algo-
rithms (generic programming in C++), user
interface elements (Windows Forms, Java
Swing), and structured data (XML). Our
proposal for clip models follows this trend
of abstraction and reuse.  Although none
of the efforts below address the full gener-
ality of our clip-model idea, there are a
number of projects that are important
stepping-stones toward our goals.
Various groups are working to build
learning object repositories (http://
elearning.utsa.edu/guides/LO-repositories.
htm)—for example, the ARIADNE Foun-
dation (http://www.ariadne-eu.org/) and
M E R LOT  ( h t t p : / / w w w. m e r l o t . o rg /
Home.po)—and to develop standards and
reference models for learning objects—
for example, SCORM (http://www.adlnet.
org/index.cfm?fuseaction=scormabt)
and IEEE’s WG12: Learning Object Meta-
data (http://ltsc.ieee.org/wg12/). Several
efforts have begun to address some of the
simulation science challenges identified
above. The Center for Component Tech-
nology for Terascale Simulation Software
is designing a Common Component Ar-
chitecture (http://www.cca-forum.org/)
as part of a program of the U.S. Depart-
ment of Energy (DOE) Office of Science
(SC). The Knowledge Web community is
now starting to tackle the problem of
identifying and encoding domain-
specific ontologies for the Web. Clyde W.
Holsapple and K. D. Joshi describe a col-
laborative approach to designing an on-
tology; the approach begins with inde-
pendent ontological proposals from
several authors and incorporates input
from many contributors.11 At the Univer-
sity of Washington, the Structural Infor-
matics Group has been working on the
Digital Anatomist Foundational Model of
Anatomy (FMA), an ambitious anatomi-
cal ontology: “The FMA is a domain on-
tology that represents a coherent body of
explicit declarative knowledge about
human anatomy ” (http://sig.biostr.
washington.edu/projects/fm/AboutFM.
html). The ambitious Digital Human Proj-
ect (http://www.fas.org/dh/index.html),
which uses the FMA ontologies, is in-
tended to incorporate all biologically
relevant systems at all time and scale di-
mensions. They range from 10-14 sec
chemical reactions to 109 sec lifetimes
and perhaps 1012 for ecosystems, and
from 10-9 meter chemical structures to
meter-scale humans. The work thus far
has focused on a series of scattered proj-
ects around the world, including the
CellML (http://www.cellml.org/public/
about/what_is_cellml.html) and other
work at the University of Aukland. In the
United States, the NIH (National Insti-
tutes of Health) (http://www.nih.gov/) has
created an interagency group, is planning
another meeting in 2005, and has started
a number of bioinformatic centers that
should help, while DARPA (Defense Ad-
vanced Research Projects Agency) has
charged ahead with the Virtual Soldier
Program (http://www.darpa.mil/dso/
thrust/biosci/virtualsoldier.htm) and the
BioSPICE program (https://community.
biospice.org/).
Conclusion
Rethinking learning and education, in all
of their dimensions, to successfully ad-
dress the needs in this century is an over-
whelmingly large and complex research
and implementation agenda that will re-
quire a multi-decade—indeed, never-
ending—level of effort on the part of all
those involved in creating and delivering
educational content. Nonetheless, a start
must be made, as a national—indeed,
global—imperative. 
The start we’re proposing here (the
Learning Federation’s R&D roadmaps) is
another first step in the quest to build
next-generation educational content and
tools. This research agenda is meant to
lead to the development not only of next-
generation authoring tools and content
but also of exemplary curricula in the
broadest sense. The research agenda is
predicated on our belief that “hardware
will take care of itself” because of com-
moditization driven by market forces. Ed-
ucational software R&D, on the other
hand, thus far has insufficient commer-
cial appeal and must therefore be consid-
ered a strategic investment by funding
agencies and companies. Industry and
government are certainly investing in
computer-based training; much can be
learned from their successful efforts.
To return to our biology example, we
believe that the creation of families of in-
teroperable clip models that will de-
scribe the human body as a system of in-
terconnected biological components at
all levels—from the molecular to the
gross anatomical—will provide an un-
precedented learning resource. Even
though the creation of such families of
clip models in a variety of disciplines will
necessitate the integration of work from
thousands of contributors over decades,
even a beginning but very ambitious and
comprehensive effort, such as the Digital
Human Project, at building biological
system components will have a payoff.
We should not be daunted by the sheer
magnitude of the task but should make
steady progress along a clearly articu-
lated path. 
Furthermore, clip models are not, by
themselves, the answer: there is no magic
bullet, no single style of educational con-
tent that can encompass the enormously
diverse set of requirements for this
agenda. Creating high-quality next-
generation educational content, across all
disciplines and at all levels, will require a
Grand Challenge effort on a scale such as
the Manhattan Project, the Man on the
Moon (Apollo) Project, and the Human
Genome Project. The U.S., European, and
several Asian economies certainly have
both the ability and the need to cultivate
the will to invest the same amount in cre-
ating exemplar interactive courses as they
do in videogames and special-effects
movies. Indeed, the U.S. Department of
Defense is making significant modern IT-
based investments for its training needs,
mostly notably in “America’s Army”
(http://www.americasarmy.com/) and the
42 EDUCAUSE r e v i e w  March/Apr i l  2005
E ducational software R&D thus farhas insufficient commercial appeal
and must therefore be considered a
strategic investment by funding agencies
and companies. 
funding of institutes such as the USC 
Institute for Creative Technologies
(http://www.ict.usc.edu/). We cannot af-
ford to have the civilian sector left be-
hind. The Learning Federation has made
a start in working with the government
with the DOIT (Digital Opportunity In-
vestment Trust) report (http://www.digital
promise.org/), which articulates a poten-
tial funding mechanism based on spec-
trum sales.
The payoff from the huge investment
of time, energy, and money cannot be
overstated. Beyond education, the clip-
model architecture will help advance sci-
ence itself. The architecture will enable
the “development” aspect of R&D to rap-
idly integrate advances in basic research.
Th e  c o m i n g  ava l a n c h e  o f  d at a  i n
genomics and proteomics will require
massively interconnected simulation sys-
tems; otherwise, how will the identifica-
tion of a gene in Japan link to a class of
pharmaceutical candidates for a rare dis-
ease being researched in Switzerland? In-
formation sharing must be augmented by
model sharing as an intrinsic part of the
research process if connections are to be
drawn between advances in different spe-
cialized fields—not sharing simply by
publishing papers in research journals
but by publishing information as soft-
ware objects that can be used immediately
(subject to accommodating the relevant
IP and commercialization issues) in other
research projects. We cannot predict the
insights that will be revealed by happy ac-
cident when two or three unrelated
strands of knowledge are unified in an in-
tegrated model, but we can eagerly antici-
pate the leverage that will be gained from
the synergy.e
Notes
We gratefully acknowledge the support of our spon-
sors: NSF and Sun Microsystems, which supported the
Exploratories Project led by Anne Morgan Spalter;
NSF and Microsoft, which support the work on Math-
Pad2 led by Joseph LaViola; and Hewlett Packard,
which provided twenty Tablet PCs to enable us to con-
duct a user study of ChemPad, led by Dana Tenneson,
in an organic chemistry class. In addition we thank
Janet Bruesselbach, the clip-model illustrator, and our
four reviewers: Henry Kelly and Randy Hinrichs (who
also are co-founders of the Learning Federation),
Anne Morgan Spalter, and Debbie van Dam.
1. Rosemary Simpson, Allen Renear, Elli Mylonas,
and Andries van Dam, “50 Years after ‘As We May 
Think’: The Brown/MIT Vannevar Bush Sympo-
sium,” ACM Interactions, vol. 3, no. 2 (March 1996):
47–67.
2. John Seely Brown, “Growing Up Digital: How the
Web Changes Work, Education, and the Ways
P e o p l e  L e a r n , ”  C h a n g e , M a r c h / A p r i l  2 0 0 0 ,
<http://www.aahe.org/change/digital.pdf>.
3. Alan Kay and Adele Goldberg “Personal Dynamic
Media,” IEEE Computer, vol. 10, no. 3 (March
1977): 31-41.
4. Neal Stephenson, The Diamond Age; or, Young Lady’s
Illustrated Primer (New York: Bantam Books, 1995).
5. See Mark Weiser’s Web site: <http://www.ubiq.
com/hypertext/weiser/weiser.html>. 
6. We know all too little about effective group learn-
ing using digital media. A lot could be learned, for
example, from studying the kind of informal
learning ecology that typifies massive multiplayer
games, such as “The Sims,” “EverQuest,” and
“Asheron’s Call.”
7. Howard Gardner, Frames of Mind: The Theory of Mul-
tiple Intelligences, 10th anniversary ed. (New York:
Basic Books, 1993).
8. Kay and Goldberg, “Personal Dynamic Media.” 
9. Robert C. Zeleznik, Kenneth P. Herndon, and
John F. Hughes, “SKETCH: An Interface for
Sketching 3D Scenes,” Proceedings of the 23rd Annual
Conference on Computer Graphics and Interactive Tech-
niques (New York: ACM Press, 1996), 163–70. 
10. Jerome S. Bruner, The Process of Education (Cam-
bridge: Harvard University Press, 1960), 33. 
11. Clyde W. Holsapple and K. D. Joshi, “A Collabora-
tive Approach to Ontology Design,” Communica-
tions of the ACM, vol. 45, no. 2 (February 2002):
42–47. 
43March/Apr i l  2005 EDUCAUSE r e v i e w

