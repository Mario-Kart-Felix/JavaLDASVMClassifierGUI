DEVELOPING THEORY THROUGH SIMULATION
METHODS
JASON P. DAVIS
KATHLEEN M. EISENHARDT
Stanford University
CHRISTOPHER B. BINGHAM
University of Maryland
We describe when and how to use simulation methods in theory development. We
develop a roadmap that describes theory development using simulation and position
simulation in the “sweet spot” between theory-creating methods, such as multiple
case inductive studies and formal modeling, and theory-testing methods. Simulation
strengths include internal validity and facility with longitudinal, nonlinear, and
process phenomena. Simulation’s primary value occurs in creative experimentation to
produce novel theory. We conclude with evaluation guidelines.
Simulation is an increasingly significant
methodological approach to theory develop-
ment in the literature focused on strategy and
organizations (e.g., Adner, 2002; Lant & Mezias,
1990; Repenning, 2002; Rivkin & Siggelkow, 2003;
Zott, 2003), Indeed, several influential research
efforts (e.g., Cohen, March, & Olsen, 1972; March,
1991) have used simulation as their primary
method. Yet while simulation has become an
important methodology, its value for theory de-
velopment remains clouded and even controver-
sial.
On the one hand, some argue that simulation
methods contribute effectively to theory devel-
opment. For example, simulation can provide
superior insight into complex theoretical rela-
tionships among constructs, especially when
challenging empirical data limitations exist
(Zott, 2003). It can also provide an analytically
precise means of specifying the assumptions
and theoretical logic that lie at the heart of ver-
bal theories (Carroll & Harrison, 1998; Kreps,
1990). In addition, simulation can clearly reveal
the outcomes of the interactions among multiple
underlying organizational and strategic pro-
cesses, especially as they unfold over time (Re-
penning, 2002). From these perspectives, simula-
tion can be a powerful method for sharply
specifying and extending extant theory in useful
ways.
On the other hand, some researchers maintain
that simulation methods often yield very little in
terms of actual theory development. They sug-
gest that simulations are simply “toy models” of
actual phenomena, in that they either replicate
the obvious or strip away so much realism that
they are simply too inaccurate to yield valid
theoretical insights (Chattoe, 1998; Fine & Els-
bach, 2000). For example, simulation research is
usually based on at least some clearly unreal-
istic assumptions, such as zero search costs
(Rivkin, 2000) and all strategic rules are effective
(Davis, Eisenhardt, & Bingham, 2007). In addi-
tion, simulation constructs are often “measured”
by empirically distant approaches, such as “0”
and “1” bit strings as representations of organi-
zations (Bruderer & Singh, 1996) and strategies
(Rivkin, 2001). The results of research using sim-
ulation methods can also be dynamically inde-
terminate and overly complex (Fichman, 1999).
From these perspectives, the value of simula-
tion methods for theoretical development is
tenuous.
The controversy surrounding the value of sim-
ulation methods for theory development par-
tially arises, in our view, from a lack of clarity
about the method and its related link to theory
development. There appears to be limited un-
derstanding within the broad research commu-
We appreciate helpful conversations with Ron Adner, Pe-
ter Glynn, Ashish Goel, Riitta Katila, Bruce Kogut, Nelson
Repenning, Jan Rivkin, and Jesper Sorenson; the suggestions
of our anonymous reviewers; and the support of the Stanford
Technology Ventures Program and the National Science
Foundation (IOC Award #0323176).
 Academy of Management Review
2007, Vol. 32, No. 2, 480–499.
Copyright of the Academy of Management, all rights reserved. Contents may not
be copied, emailed, posted to a listserv, or otherwise transmitted without the
copyright holder’s express written permission. Users may print, download, or email
articles for individual use only.
480
nity about (1) when simulation is a useful meth-
odological choice for theory development, (2)
how to select among the various simulation ap-
proaches (e.g., system dynamics versus genetic
algorithms), (3) the appropriate steps for per-
forming simulation research, and (4) the rele-
vant criteria for evaluating simulation research.
Most significant, there seems to be limited rec-
ognition within the research community of how
simulation methods fit into the broader scheme
of relating methodological choices to theoretical
development.
Our purpose is to address these issues by
clarifying when and how to use simulation
methods in theory development. Although schol-
ars writing about theory development (e.g., Du-
bin, 1976; Pfeffer, 1982; Priem & Butler, 2001; Sut-
ton & Staw, 1995; Whetten, 1989) may have
different emphases, most agree that theory has
four elements: constructs, propositions that link
those constructs together, logical arguments
that explain the underlying theoretical rationale
for the propositions, and assumptions that de-
fine the scope or boundary conditions of the
theory. Consistent with these views, we define
theory as consisting of constructs linked to-
gether by propositions that have an underlying,
coherent logic and related assumptions.
Broadly, we attempt to make two contribu-
tions. First, we offer a roadmap for how to use
simulation methods to develop theory. This
roadmap synthesizes prior work on the design of
simulation research (e.g., Sterman, 2000) and ex-
tends that work into specific areas, such as iden-
tifying appropriate research questions, choos-
ing among simulation approaches, developing
computational representations, elaborating the
fundamental role of verification versus experi-
mentation, and evaluating simulation research.
We rely on exemplars from the extant literature
to ground our observations. The intended result
is a more complete roadmap for executing and
evaluating theory development research using
simulation methods than has previously ex-
isted. This roadmap is summarized in Table 1.
Second, we position simulation methodology
within the broad context of theoretical develop-
ment in the organizations and strategy litera-
ture. Relying on the related theory development
literature (e.g., Priem & Butler, 2001; Sutton &
Staw, 1995; Weick, 1989), we explore the ratio-
nale for using simulation, its relationship to
other methods such as laboratory experiments,
simulation’s strengths and weaknesses, and
guidelines for evaluating theoretical develop-
ment using simulation. We argue that simula-
tion is especially useful in the “sweet spot” be-
tween theory-creating research using such
methods as inductive multiple case studies
(Eisenhardt, 1989) and formal modeling (Freese,
1980), and theory-testing research using multi-
variate, statistical analysis (Pfeffer, 1993). That
is, simulation enables the elaboration of rough,
basic (or what we term simple) theory that is
often derived from inductive cases or formal
modeling into logically precise and comprehen-
sive theory. This theory can then be effectively
examined further using deductive logic and em-
pirical evidence. Simulation is particularly use-
ful when the theoretical focus is longitudinal,
nonlinear, or processual, or when empirical
data are challenging to obtain.
We begin by discussing the background of
simulation methods. We then turn to the basic
steps of conducting simulation research, includ-
ing selecting research questions, choosing a
computational approach, and developing exper-
iments. We conclude with a discussion of the
strengths and weaknesses of simulation re-
search and guidelines for its evaluation.
BACKGROUND
We define simulation as a method for using
computersoftwaretomodeltheoperationof“real-
world” processes, systems, or events (Law &
Kelton, 1991). This definition is consistent with
other definitions that describe simulation mod-
els as virtual experiments (Carley, 2001) or as
simplified pictures of the world having some,
but not all, of the characteristics of that world
(Lave & March, 1975). In particular, simulation
involves creating a computational representa-
tion of the underlying theoretical logic that links
constructs together within these simplified
worlds. These representations are then coded
into software that is run repeatedly under vary-
ing experimental conditions (e.g., alternative as-
sumptions, varied construct values) in order to
obtain results.
While simulation can be used purely for de-
scription or exploration,1 our focus is on using
1 There are a number of uses of simulation, including
description of complicated systems, such as telecommuni-
2007 481Davis, Eisenhardt, and Bingham
simulation for theory development when simple
theory exists (e.g., Rivkin, 2000; Rudolph & Re-
penning, 2002). By simple theory, we mean un-
developed theory that has only a few constructs
and related propositions with modest empirical
or analytic grounding such that the propositions
are in all likelihood correct but are currently
limited by weak conceptualization of constructs,
few propositions linking these constructs to-
gether, and/or rough underlying theoretical
logic. Simple theory also includes basic pro-
cesses that may be known (e.g., competition,
imitation) but that have interactions that are
only vaguely understood, if at all. Thus, simple
theory contrasts with well-developed theory,
such as institutional and transaction cost theo-
ries that have multiple and clearly defined the-
oretical constructs (e.g., normative structures,
mimetic diffusion, asset specificity, uncertainty),
cation networks, and purely exploratory efforts, such as ob-
serving what emerges from simulating complex relation-
ships. In contrast, our focus is specifically on using
simulation for theory development when some simple theory
exists. Its application ranges from incremental extensions of
that theory to exploratory work that may rely on only a rough
understanding of theoretical mechanisms. We appreciate
the comments of an anonymous reviewer in clarifying this
scope condition of our efforts and in suggesting language.
TABLE 1
Roadmap for Developing Theory Using Simulation Methods
Step Activities Rationale
Begin with a research question • Determine a theoretically intriguing
research question
• Look for a basic tension like structure
versus chaos, long versus short run
Focuses efforts on a theoretically
relevant issue for which
simulation is especially effective
Identify simple theory • Select simple theory that addresses
the research question
• Look for intertwined processes (e.g.,
competition and legitimation),
nonlinearities, and longitudinal
effects
• Look for theory that requires data
that are challenging to obtain
• Forms basis of computational
representation by giving shape to
theoretical logic, propositions,
constructs, and assumptions
• Focuses efforts on theoretical
development for which simulation
is especially effective
Choose a simulation approach • Choose simulation approach that fits
with research question, assumptions,
and theoretical logic
Ensures that the research uses an
appropriate simulation approach
given the research at hand
• If the research does not fit an
approach or if the approach requires
extensive modification, choose
stochastic processes
Create computational
representation
• Operationalize theoretical constructs
• Build computational algorithm that
mirrors theoretical logic
• Specify assumptions
• Ensure that computational
representation allows theoretically
valuable experimentation
• Embodies theory in software
• Provides construct validity
• Improves internal validity by
requiring precise constructs, logic,
and assumptions
• Sets the stage for theoretical
contributions
Verify computational
representation
• Replicate propositions of simple
theory with simulation results
• Conduct robustness checks of
computational representation
• Confirms accuracy and robustness
of computational representation
• Confirms internal validity of the
theory
• If verification fails, correct theory
and/or software coding
Experiment to build novel
theory
Create experimental design (e.g., vary
construct values, unpack constructs,
alter assumptions, add new features)
based on likely theoretical
contribution and realism
• Focuses experimentation on
theory development
• Builds new theory through
exploration, elaboration, and
extension of simple theory
Validate with empirical data Compare simulation results with
empirical data
Strengthens external validity of the
theory
482 AprilAcademy of Management Review
well-established theoretical propositions that
have received extensive empirical grounding,
and well-elaborated theoretical logic. Simple
theory also contrasts with situations where
there is no real theoretical understanding of the
phenomena.
Simulation is particularly suited to the devel-
opment of simple theory because of its strengths
in enhancing theoretical precision and related
internal validity and in enabling theoretical
elaboration and exploration through computa-
tional experimentation. In particular, simulation
relies on some theoretical understanding of the
focal phenomena in order to construct a compu-
tational representation. Yet simulation also de-
pends on an incomplete theoretical understand-
ing such that fresh theoretical insights are
possible from the precision that simulation en-
forces and the experimentation that simulation
enables.
In contrast, while it is possible to simulate
well-developed theories, such as institutional
and transaction cost theories, they offer fewer
opportunities for new theoretical insights using
the logical precision and experimentation of
simulation. These theories are already well-
elaborated. Thus, there is likely to be less payoff
from simulation because it is likely just to rep-
licate known theoretical ideas.
At the other end of the theory development
spectrum are “clean-slate” theoretical situa-
tions. Here there is not much to simulate, and
other methods such as multiple case induction
and formal modeling are typically better meth-
odological choices. In contrast with these ex-
tremes, simple theory has enough theoretical
development to construct a computational rep-
resentation, but also sufficient room to improve
internal validity and to develop novel theoreti-
cal insights. In addition, simulation is espe-
cially useful for theory development when the
focal phenomena involve multiple and interact-
ing processes, time delays, or other nonlinear
effects such as feedback loops and thresholds.
In these situations, simulation is likely to reveal
nonintuitive elaborations of simple theory that
are difficult to uncover using other methods, in-
cluding armchair thought processes.
Overall, a central value of simulation for the-
ory development lies in the exploration, elabo-
ration, and extension of simple theories. Theory
development begins with one or several theoret-
ical ideas (what we term simple theory) that are
then represented in computer software, verified,
and explored through computational experi-
mentation. These experiments can range from
incremental (e.g., adding a moderating relation-
ship) to elaborate, involving numerous, wide-
ranging experiments that push the theory well
beyond its immediate application—for example,
examining alternative theoretical logics (Lee,
Mitchell, & Sablynski, 1999).2 Several exemplar
studies using simulation in theory development
are listed in Table 2.
ROADMAP FOR DEVELOPING THEORIES WITH
SIMULATIONS
Begin with an Intriguing Research Question
and Simple Theory
Like all good research, studies that develop
theory through simulation should begin with an
intriguing research question that reflects deep
understanding of the extant literature and re-
lates to a substantial theoretical issue (Weick,
1989). Without such a question, simulation re-
search simply becomes a “fishing expedition” in
which the researcher lacks focus and theoretical
relevance and risks becoming overwhelmed by
computational complexity.
Research questions can originate from many
sources. Sometimes they come from conundrums
within basic science. March (1991), for example,
relied on complexity theory from the biological
and computer sciences to conceptualize a re-
search question that examined the trade-off be-
tween the exploration of new possibilities and
the exploitation of old certainties. Sometimes
research questions are motivated by intriguing
observations from inductive case study re-
search. For example, Rudolph and Repenning
(2002) used the counterintuitive observations
from a case study describing the 1977 Tenerife
airport disaster (Weick, 1993) as the inspiration
for their research question asking how small
problems can become major catastrophes.
Sometimes research questions emerge from
combining process theories, such as when
Gavetti and Levinthal (2000) asked how cogni-
tion influences experiential learning. Research
questions may also come from the classic tradi-
2 We appreciate the comments of one of our anonymous
reviewers in assisting us in highlighting this continuum and
language.
2007 483Davis, Eisenhardt, and Bingham
tion extending formal analytic models (Adner,
2002).
While simulation shares an emphasis on in-
triguing and theoretically relevant research
questions with other methods, simulation is par-
ticularly suited to the theoretical development
of simple theory. As described earlier, simple
theory is undeveloped theory that involves a few
constructs and related propositions with some
empirical or analytic grounding but that is lim-
ited by weak conceptualization, few proposi-
tions, and/or rough underlying theoretical logic.
TABLE 2
Recent Examples of Simulation Research
Study Research Question Key Processes Approach
Representative
Findings
Rudolph & Repenning
(2002)
When do small
interruptions
create major
catastrophes?
Adaptation and
selection
System
dynamics
Variance in the rate of
interruptions affects
emergence of tipping
points
Sastry (1997) How do
organizations
undergo
fundamental
change?
Change and
inertia
System
dynamics
An additional negative
feedback loop
corrects theoretical
logic of punctuated
equilibrium theory
Rivkin (2000) What is the optimal
strategic
complexity?
Replication and
imitation
NK fitness
landscape
Moderate strategic
complexity is
optimal
Gavetti & Levinthal
(2000)
How does cognition
improve
experiential
learning?
Experiential
learning and
cognition
NK fitness
landcape
Cognition is most
useful in improving
experiential learning
at moderate levels of
K interactions
Zott (2002) How does adaptive
learning occur
within
bargaining?
Adaptive learning Genetic
algorithm
Adaptive failures may
occur even with
complete information
Bruderer & Singh
(1996)
How does organiza-
tional learning
affect the
evolution of a
population of
organizations?
Variation,
adaptation, and
selection
Genetic
algorithm
Environment
influences when
organizational
learning is most
useful for the
population’s
convergence to an
optimal form
Lomi & Larsen (1996) How do competition
and legitimation
affect density
dependence?
Competition and
legitimation
Cellular
automata
Neighborhood size
moderates the
relationship of
density with
founding and failure
rates
March (1991) What is the
relationship
between
exploration and
exploitation?
Exploitation and
exploration
Stochastic
processes
Exploitation processes
are effective in the
short run but
destructive in the
long run
Davis, Eisenhardt, &
Bingham (2007)
What is the optimal
degree of
structure?
Improvisation Stochastic
processes
Unpredictability (not
ambiguity,
complexity, or
velocity) is the driver
of the tension
between structure
and chaos
484 AprilAcademy of Management Review
Simple theory may also include concepts and
basic processes from well-known theories (e.g.,
competition, imitation), especially when the re-
search focus is on their vaguely (if at all) under-
stood interactions. Propositions may be formally
stated (Davis et al., 2007; Rivkin, 2001) or implicit
(Rudolph & Repenning, 2002). The fundamental
idea is that theory development using simula-
tion should begin with a simple theory, rather
than either an extensive theoretical base or a
clean theoretical slate. This simple theory
should address a theoretically intriguing re-
search question that focuses on a fundamental
phenomenon. Such theory can then be a plat-
form from which powerful theory can be devel-
oped through the verification and experimenta-
tion enabled by simulation (Lave & March, 1975;
Stinchcombe, 1968).
Rivkin (2001), for example, focused on a re-
search question that asked, “What is the optimal
level of strategic complexity?” He relied on a
single proposition linking strategic complexity
with performance that was based on case stud-
ies and prior theoretical work. This proposition
stated that a moderate level of strategic com-
plexity leads to the highest performance. Simi-
larly, Rudolph and Repenning (2002) used previ-
ous case study research as the basis of a simple
theory describing how minor events could cre-
ate major catastrophes. The theory consisted of
several propositions that linked quantity of in-
terruptions, stress, and performance.
Simulation is particularly effective when the
simple theory involves several basic processes,
such as competition and legitimation (Lomi &
Larsen, 1996) or imitation and experimentation
(Zott, 2003), with only vaguely (if at all) under-
stood longitudinal interactions. These interac-
tions are often difficult to study with traditional
statistical methods or to anticipate with thought
processes. In contrast, these processes usually
can be computationally represented, verified,
and then explored (separately and in interac-
tion) using simulation. For example, Sastry
(1997) investigated the longitudinal interaction
between inertial and change processes. Her re-
sults included unexpected insights about time-
pacing and dynamic markets that were not an-
ticipated in previous theory and in previous
empirical evidence (Tushman & Romanelli,
1985).
Simulation is also particularly effective for
theory development when the research question
involves a fundamental tension or trade-off. The
tension may be temporal, such as short- versus
long-run implications (March, 1991; Sterman, Re-
penning, & Kofman, 1997); structural, such as too
much structure versus too little (Davis et al.,
2007; Rudolph & Repenning, 2002); or spatial,
such as near versus far away (Lomi & Larson,
1996; Schelling, 1971). These tensions often result
in nonlinear relationships, such as tipping point
transitions and steep thresholds. These and
other nonlinear relationships are difficult to dis-
cover using inductive case methods and difficult
to explore with traditional statistical tech-
niques. Yet they often offer surprising, nonintui-
tive results.
Choose a Simulation Approach
Assuming that simulation is the best way to
proceed (e.g., intriguing research question and
simple theory), the next step is to select a simu-
lation approach. This choice should depend on
the fit of the research question, assumptions,
and the theoretical logic of the simple theory
with those of the simulation approach. This
choice is crucial because the simulation ap-
proach can impose a theoretical logic, type of
research question, and related assumptions.
Much as the choice of a statistical technique
(e.g., OLS regression versus event history) can
shape theory-testing empirical research, the
choice of simulation approach shapes the sub-
sequent results. In fact, the choice of simulation
approach may be closer to choosing a theoreti-
cal framework (e.g., resource dependence versus
transaction cost economics) because of its fram-
ing of research questions, key assumptions, and
theoretical logic.
Several well-known simulation approaches
have been used for theory development in the
organizations and strategy literature.3 Some are
common, such as system dynamics (Repenning,
2002; Rudolph & Repenning, 2002) and NK fitness
3 Other researchers (e.g., Dooley, 2002) have used simula-
tion typologies. We chose our typology because it is a fine-
grained mapping that accurately relates to the major ap-
proaches that are used in the organizations and strategy
literature. That is, it reflects the most frequently used cate-
gorization in the relevant extant research. Given the scope of
our paper, our description of these approaches is necessarily
limited. Interested readers should turn to technical treat-
ments if they wish to use these approaches.
2007 485Davis, Eisenhardt, and Bingham
landscapes (Levinthal, 1997; Rivkin & Sig-
gelkow, 2003). Others are less frequently used,
such as genetic algorithms (Bruderer & Singh,
1996) and cellular automata (Lomi & Larsen,
1996). Each of these is a structured approach that
constrains the theoretical logic, assumptions,
and research questions that can be explored. In
contrast, the stochastic process approach (Davis
et al., 2007; March, 1991; Zott, 2003) is a custom-
ized approach that is useful when the structured
approaches do not fit with the research at hand
(see Table 3 for a comparison of these ap-
proaches).
System dynamics. The system dynamics ap-
proach focuses on how causal relationships
among constructs can influence the behavior of
a system (Forrester, 1961; Sastry, 1997; Sterman
et al., 1997). The approach typically models a
system (e.g., organization) as a series of simple
processes with circular causality (e.g., variable
A influences variable B, which influences vari-
able A). These processes have some common
constructs and so intersect in a set of circular
causal loops. These causal loops can be positive
such that feedback is self-reinforcing and am-
plifying, or negative such that feedback is
dampening (Sterman, 2000). While each process
may be well-understood, their interactions are
often difficult to predict. The system typically
includes stocks, acting as buffers (i.e., constructs
with values that accumulate and dissipate over
time, and so introduce time delays) and flows
(i.e., constructs specifying temporal rates in the
system).
Rudolf and Repenning (2002), for example,
used system dynamics to examine why minor
interruptions sometimes trigger sudden catas-
trophes within organizations. The authors used
intersecting causal loops to model two different
processes (one positive loop, one negative loop)
by which stress, quantity of interruptions, and
performance were causally related (Yerkes &
TABLE 3
Comparison of Simulation Approaches
Simulation Approach Focus
Common Research
Question(s) Key Assumptions Theoretical Logic Common Experiments
System dynamics
Sastry (1997),
Sterman,
Repenning, &
Kofman (1997),
Repenning (2002),
Rudolph &
Repenning (2002)
Behavior of a system
with complex
causality and
timing
What conditions create
system instability?
● System of intersecting, circular
causal loops
● Stocks that accumulate and
dissipate over time
● Flows that specify rates within
system
● Description
● Inputs to a system
of interconnected
causal loops, stocks,
and flows produce
system outcomes
● Add causal loops
● Change mean of flow rates
● Change variance of flow
rates
NK fitness landscapes
Levinthal (1997),
Gavetti & Levinthal
(2000), Rivkin (2000),
Rivkin & Siggelkow
(2003)
Speed and
effectiveness of
adaptation of
modular systems
with tight versus
loose coupling to
an optimal point
● How long does it
take to find an
optimal point (e.g.,
high-performing
strategy)?
● What is the
performance of the
optimal point?
● System of N nodes, K coupling
between nodes
● Fitness landscape that maps
performance of all combinations
● Adaptation via incremental
moves and long jumps
● Optimization
● Adaptation of a
modular system
using search
strategies (i.e., long
jumps, incremental
moves) to find an
optimal point on a
fitness landscape
● Vary N and K
● Change adaptation moves
● Add a “map” of the
landscape
● Create an environmental jolt
Genetic algorithms
Bruderer & Singh
(1996), Zott (2002)
Adaptation of a
population of
agents (e.g.,
organizations) via
simple learning
to an optimal
agent form
● What affects the rate
of adaptation (or
learning or change)?
● When and/or does an
optimal form
emerge?
● Population of agents with genes
● Evolutionary adaptation (v-s-r)
● Variation via mutation
(mistakes) and crossover
(recombinations)
● Selection via fitness
(performance)
● Retention via copying selected
agents
● Optimization
● Adaptation of a
population of
agents using an
evolutionary process
toward an optimal
agent form
● Vary mutation probability
● Vary crossover probability
● Vary length of time of
evolution
● Create an environmental jolt
Cellular automata
Lomi & Larsen
(1996)
Emergence of macro
patterns from
micro interactions
via spatial
processes (e.g.,
competition,
diffusion) in a
population of
agents
● How does the pattern
emerge and change?
● How fast does a
pattern emerge?
● Population of spatially arrayed
and semi-intelligent agents
● Agents use rules (local and
global) for interaction, some
based on spatial processes
● Neighborhood of agents where
local rules apply
● Description
● Interactions among
agents following
rules produce
macrolevel patterns
● Change the rules
● Change the neighborhood
size
Stochastic processes
March (1991),
Carroll & Harrison
(1998), Zott (2003),
Davis, Eisenhardt, &
Bingham (2007)
Flexible approach to
a wide variety of
research
questions,
assumptions, and
theoretical logics
No specific research
questions beyond
asking what the
effects of varying
the stochastic
sources are
● One or more processes by which
system operates
● One or more stochastic sources
(e.g., process elements)
● Probablistic distributions for
each stochastic source
No specific theoretical
logic
● Change stochastic sources
● Vary levels of stochasticity
● Unpack constructs
● Change pieces of theoretical
logic
486 AprilAcademy of Management Review
Dodson, 1908). The number of accumulated inter-
ruptions was included as a stock, and the rate of
interruptions and the rate of their resolution
were flows. By varying these rates, the authors
were able to develop theory about when the
system would be stable, when it would hit tip-
ping points, and whether (and, if so, how fast)
those tipping points would lead to catastrophe.
System dynamics is particularly applicable
for understanding the behavior of systems with
complex causality and timing. Research ques-
tions are often framed in terms of how specific
initial conditions affect the stability of the sys-
tem. That is, researchers are usually interested
in finding the initial conditions that lead to
abrupt, nonlinear changes, such as tipping
points, catastrophes, and the emergence of vi-
cious or virtuous cycles. Researchers often begin
with two causal loops and then add successive
ones in order to build intuition, understanding,
and realism in a structured way. Although the
approach can accommodate some stochasticity,
it relies on deterministic differential equations
and so is not as useful when there are many or
complicated sources of stochasticity.
NK fitness landscapes. The NK approach was
developed in evolutionary biology to study ge-
netic systems (Kauffman, 1993). This approach
focuses on how rapidly and effectively a modu-
lar system adapts to reach an optimal point,
especially when interactions among the system
components are crucial. Specifically, the system
(e.g., organization, product, strategy) is concep-
tualized as a set of N nodes, and K interactions
among the nodes. The system is assumed to use
adaptation (sometimes termed search) strate-
gies, such as incremental moves and long
jumps, to find the optimal point (e.g., best orga-
nization, best product). To illustrate, Rivkin
(2001) focused on strategy. He defined N as the
elements of a strategy (e.g., manufacturing and
advertising choices) and K as the degree of in-
teraction among the elements. The optimal point
was the highest performing strategy.
A key concept is the fitness landscape (Wright,
1931) that is created by assigning performance
values (termed fitness) to every combination of
values. The shape of the landscape depends on
the interaction (K) among the nodes (Kauffman,
1989). When there is little interaction (low K),
there is one optimal combination (or perhaps a
few). The corresponding fitness landscape is
“smooth,” with only one or a few hills such that
it is easy to find the optimal point. As interaction
(K) increases, more combinations become lo-
cally optimal. The corresponding fitness land-
scape is “rugged,” with many hills of varying
heights that correspond to varying performance
levels. This landscape type is hard to traverse to
find the optimal point (Kauffman, 1989). The
overall theoretical logic of the NK approach em-
phasizes the adaptation of a modular system
using specific search strategies to find an opti-
mal point on a fitness landscape.
For example, Gavetti and Levinthal (2000) ex-
amined how experiential learning (alone and
with cognition) affected the time needed to find
an optimal policy for an organization. They rep-
resented organizational policy as N policy ele-
ments and K interactions among them. They
then compared the time to find an optimal policy
(and its performance) using only experiential
learning (i.e., incremental moves) with using
both experiential learning and cognition (i.e., a
basic “map” of the landscape that enabled long
jumps) under varying levels of coupling (K)
among elements.
The NK approach is particularly applicable
for understanding how the speed and effective-
ness of adaptation to an optimum within a mod-
ular system are influenced by tight versus loose
coupling among the system’s components. Re-
search questions are often framed as “problem
solving” or “searching” to find an optimal point.
Researchers often ask how long it takes to find
an optimal point (e.g., high-performing strategy,
high-performing product) or what the effective-
ness or performance of the optimal point is (e.g.,
local versus global optimum). They are typically
interested in how the number of nodes (e.g.,
number of product features), the interaction
among nodes (e.g., tight versus loose coupling),
types of adaptation (i.e., long jumps versus in-
cremental moves), or the use of landscape
“maps” (e.g., cognition, science) influences the
time to find an optimal solution and/or the per-
formance of that solution. Although the ap-
proach can be modified to accommodate some
environmental dynamism (e.g., environmental
jolts) or node intelligence, it is less useful when
the effects of market dynamism, the intelligence
and/or uniqueness of nodes in a modular system
(e.g., differences among business units in an
organization), or varied types of interaction (e.g.,
strong versus weak ties) are a central interest.
2007 487Davis, Eisenhardt, and Bingham
Genetic algorithms. Like the NK approach, ge-
netic algorithms are an optimization approach
with roots in biology. But unlike the NK ap-
proach, genetic algorithms focus on how rapidly
and effectively a population of heterogeneous
agents composed of genes (e.g., population of
organizations composed of routines, population
of consumers with preferences) adaptively
learns (Goldberg, 1989; Holland, 1975).4 Adapta-
tion occurs through a stochastic evolutionary
process (i.e., variation-selection-retention) that
favors gradual improvement through accumu-
lated experience (Aldrich, 1999; Holland, 1975).
Variations occur in two ways: mutation (i.e., ran-
dom change of one or a few genes that corre-
sponds to mistakes) and crossover (i.e., random
switching of sets of genes among agents that
corresponds to recombination of existing genes).
Selection of agents (also termed genes) occurs
according to their performance (also termed fit-
ness) with respect to some metric. Retention
(also termed reproduction) is the copying of the
selected agents from one generation to the next.
Over time, successful variations are more likely
to be retained and form the basis of future vari-
ations (Andreoni & Miller, 1995; Arifovic et al.,
1997). Eventually, only high-performing agents
remain in the population, and, ultimately, often
only a single agent form survives (e.g., dominant
design). The theoretical logic emphasizes the
evolutionary adaptation of a population toward
an optimal form that is path dependent and
combinatorial.
For example, Bruderer and Singh (1996) used a
genetic algorithm to examine organizational
evolution within a population of organizations.
In their model, each of the 250 organizations in
the population was composed of 20 routines that
could change as a result of both mutation and
crossover variation processes. The authors were
particularly interested in the effect of learning
on organizational evolution. They found that
learning accelerated the discovery of an effec-
tive organizational form. Genetic algorithms
can also be used to examine the evolution of
specific types of strategies within an agent. For
example, Zott (2002) used a genetic algorithm
approach to model the bargaining behavior of a
negotiator with private information. The genes
were the negotiator’s bargaining rules that de-
termined when to offer a contract and when to
accept or reject another’s offer. Among other in-
sights, the study clarified the effects of complete
and incomplete information in producing ineffi-
cient (i.e., nonoptimal) outcomes, such as delays
and failure to agree.
Genetic algorithms are particularly applica-
ble for describing how heterogeneous agents
(e.g., organizations, consumers) learn improved
solutions (e.g., better organizational form, better
strategy) through experimentation. As such, this
approach is consistent with some important as-
pects of human learning, such as experience-
based action, importance of recent experience,
creative synthesis of ideas, and occurrence of
mistakes (Zott, 2002). Research questions are
typically framed in terms of what affects the rate
of adaptation (or sometimes learning or change)
and whether a dominant form emerges. Re-
searchers often experiment with rates and pro-
cesses of mutation and crossover, as well as
performance metrics. Although the approach
can be modified, it is less useful when the value
of the performance metrics varies (e.g., as in
dynamic markets), the agents can engage in
sophisticated cognitive processes that are not
well-captured by experiential learning (e.g.,
foresight), or crossover is unlikely (e.g., techni-
cal standards limit reuse of product components
across products in a population, internal labor
markets limit mobility across organizations in a
population).
Cellular automata. Although not widely used
in the organizations and strategy literature,
cellular automata have gained traction in the
physical sciences as an approach that focuses
on the emergence of macrolevel system pat-
terns from microlevel interactions among spa-
tially related and semi-intelligent agents
(Wolfram, 2002). Cellular automata assume a
system of agents that are spatially related
(e.g., competitors in an industry, cities in a
country). Spatial relatedness implies that the
degree to which agents influence each other
depends on the distance between them. The
4 Although there has been only limited use of genetic
algorithms in the organizations and strategy literature we
are focusing on, there is an emerging tradition in the eco-
nomics literature. Applications include economic growth
(e.g., Arifovic, Bullard, & Duffy, 1997), auctions (e.g., Andreoni
& Miller, 1995), and forecasting (e.g., Bullard & Duffy, 2001).
This work is often framed in juxtaposition with highly ra-
tional models of agent behavior, such as game theoretic
formulations. Readers with particular interest in genetic al-
gorithms should consult these sources, as well as more gen-
eral sources (e.g., Goldberg, 1989; Holland, 1975).
488 AprilAcademy of Management Review
agents behave according to a few simple rules
(i.e., semi-intelligent agents), some of which
relate to how the agents influence each other
(Langton, 1984). Typically, the rules relate to
spatial processes such that nearby agents are
influenced more than distant ones—for exam-
ple, diffusion, propagation, and competition
processes (Schelling, 1971). Although not re-
quired, the rules are usually uniform (i.e., all
agents have the same rules) and deterministic
(i.e., all rules are fixed). An important concept
is the neighborhood that defines which agents
are local (and so neighbors) and which are not.
At least some rules designate behaviors taken
in interaction with neighbors, but not with
more distant agents.
For example, Lomi and Larsen (1996) used cel-
lular automata to study density-dependence
theory and the tension between competition and
legitimation processes (Hannan & Freeman,
1989). The organizations were spatially arrayed
relative to one another in a two-dimensional
grid. The organizations possessed rules for com-
petition (affecting only those in the neighbor-
hood) and legitimation (affecting all organiza-
tions). The authors then observed how the
competitive and legitimating interactions
among organizations (i.e., microlevel interac-
tions) affected population density, founding
rates, and failure rates (i.e., macrolevel patterns)
over time.
Cellular automata are particularly useful for
examining how macrolevel patterns emerge
from spatial processes (e.g., diffusion, competi-
tion, propagation, and segregation) that operate
at a micro level. In these processes, the behav-
iors of agents affect each other, but their influ-
ence diminishes with distance. Research ques-
tions usually ask how macrolevel patterns
emerge and change. Researchers typically vary
the size of the neighborhood, the relevant pro-
cesses (e.g., diffusion, segregation) and their re-
lated rules, and the spatial array of agents (e.g.,
density of agents) in order to describe the emer-
gence and change of macrolevel phenomena
from microlevel interactions among agents.
While cellular automata can be modified, the
approach is less useful when intelligence re-
sides at the system level (e.g., CEO in an orga-
nization of business units) or the interactions
among agents are not spatially dependent.
Stochastic processes. Stochastic processes are
a flexible approach that enables researchers to
custom design their simulations (Gallager,
1996).5 The approach makes no particular as-
sumptions about the system, research question,
or theoretical logic. It is especially useful for
exploring theories that do not fit with the theo-
retical logic and assumptions of the structured
simulation approaches. For example, the envi-
ronment may be dynamic (Davis et al., 2007;
March, 1991), or the theoretical logic may involve
temporal transmission processes (Carroll & Har-
rison, 1998). Researchers typically piece together
distinct processes that mirror the theoretical
logic. They also build in several sources of sto-
chasticity (e.g., environmental input, timing, el-
ements of the process) and endow them with
stochastic distributions that can be simple (e.g.,
50/50 draw) or complex (e.g., normal and Poisson
distributions). A key point is that while the sim-
ulation is custom designed and may have some
original processes or constructs, researchers of-
ten use existing building blocks, such as known
processes—for instance, Markov chains—and
familiar sources of stochasticity and their re-
lated distributions—for instance, Poisson distri-
bution for input arrival times (Law & Kelton,
1991).
An example is research by Carroll and Harri-
son (1998). Building on their previous work (Har-
rison & Carroll, 1991), these authors developed
several underlying theoretical logics for the
simple, well-known proposition positing that
heterogeneity of tenure is related to heterogene-
ity of culture. These logics related to temporal
transmission of culture and so did not fit with
the theoretical logics of structured simulation
approaches, such as system dynamics (circular
causality) and NK (search for the optimal point).
The authors custom designed their simulation
by combining several processes, including turn-
over and socialization, and designating sources
of stochasticity, such as the hiring and firing
processes, and their related distributions.
Stochastic processes are particularly applica-
ble when the research question, assumptions, or
theoretical logic does not fit with a structured
approach. Although it is sometimes possible to
5 By stochastic processes, we mean a broad class of sim-
ulations that are unified by the use of custom-designed
algorithms or combinations of algorithms. By contrast, al-
though the structured simulation approaches may also often
involve stochasticity, they have a much more specific struc-
ture.
2007 489Davis, Eisenhardt, and Bingham
modify a structured approach to fit the research
at hand, stochastic processes are the preferred
approach when these modifications are exten-
sive. Extensive modifications can become un-
wieldy, yielding a poor computational represen-
tation. When using stochastic processes,
researchers typically ask how alternative theo-
retical logics, different assumptions, or varying
sources of stochasticity affect system outcomes.
They often hold some sources of stochasticity
constant while varying others. The result is a
flexible approach to theory development, albeit
at the price of a lack of standardization and
increased need for modeling ingenuity.
Create the Computational Representation
In the previous sections we described the
importance of a theoretically intriguing re-
search question, a simple theory that is likely
to be accurate but incomplete, and an appro-
priate simulation approach (i.e., fits with the
research question, assumptions, and theoreti-
cal logic). In this section we turn to computa-
tional representation of the theory. This activ-
ity is central to theory development using
simulation. It involves (1) operationalizing the
theoretical constructs, (2) building the algo-
rithms that mirror the theoretical logic of the
focal theory, and (3) specifying assumptions
that bound the theory and results. Although
we describe them separately, researchers usu-
ally engage in these activities interactively
because constructs, algorithms, and assump-
tions are highly interdependent. Creating the
computational representation is roughly anal-
ogous to the activities reported in the methods
section of other types of research.
Operationalizing theoretical constructs con-
cerns defining the computational measures for
each construct (and, if not already done, creat-
ing its verbal definition). This is roughly analo-
gous to creating empirical measures for theoret-
ical constructs in other types of research. As
described below, effective operationalization in-
volves choosing an appropriate computational
measure and range of values for each construct.
It also involves using construct definitions and
names that fit with the extant literature where
possible (i.e., inventing new theoretical con-
structs only when none exists) in order to build
reader intuition and confidence and to enhance
the clarity of the theoretical contributions (Re-
penning, 2003).
One type of computational measure is a sin-
gle measure (termed parameter) that has a
range of possible values. For example, Davis
and colleagues (2007) operationalized their am-
biguity construct by defining a single computa-
tional measure with values ranging from 0 (no
ambiguity) to 1 (complete ambiguity). For con-
structs with a range of values having no natural
bounds (e.g., rates ranging from 0 to infinity), it
is appropriate to artificially bound the range
and then test these bounds to guard against the
possibility that the values outside the bounds
produce qualitatively different or contradictory
results (Law & Kelton, 1991). If so, the range of
values should be adjusted.
A second type of computational measure is a
multidimensional measure (often termed a bit
string). Typically, it is effective to measure the
most central constructs as bit strings (not pa-
rameters) because bit strings have more preci-
sion and dimensionality. These features enable
more refined and better experimentation. An ex-
ample of bit string representation is strategy as
a set of ten decisions (Rivkin, 2000). In this type
of representation, the dimensions of a construct
are measured by specific values (often 0 and 1
and occasionally ? to indicate an absent value).
For example, Bruderer and Singh (1996) opera-
tionalized organizational form as a bit string of
twenty routines with 1 indicating a correct rou-
tine, 0 indicating an incorrect one, and ? indicat-
ing an absent routine that could be learned.
A strength of simulation research is construct
validity—that is, accurate specification and
measurement of constructs (Cook & Campbell,
1979). Simulation requires precise specification
of constructs and their measures, and so avoids
“noisy” measurement that affects construct va-
lidity in empirical research (Rosenthal & Rose-
now, 1991). In addition, as required by its rigor-
ous, step-by-step logic, simulation involves
precise specification of units of analysis (e.g.,
product, strategy) and intervening constructs
that are often poorly conceptualized and unmea-
sured in empirical research. Since simulation
eliminates the measurement errors associated
with empirical data, convergent and discrimi-
nant validity are not germane (Campbell &
Fiske, 1959). Finally, simulation also has the ad-
vantage of quick, flexible adjustment of con-
struct measures (and even the constructs them-
490 AprilAcademy of Management Review
selves) by changing the computer code. Such
adjustment is usually challenging in empirical
research, particularly after the data are col-
lected.
The computational representation also in-
volves building algorithms in software that cap-
tures the step-by-step theoretical logic underly-
ing the simple theory. In other words, the
software code should embody the theoretical
logic. Specifically, the algorithms should consist
of a series of steps for modifying construct val-
ues in accordance with the underlying theoreti-
cal logic of the simple theory. As noted earlier,
structured simulation approaches (e.g., NK, ge-
netic algorithms) offer a standardized approach
for encoding theoretical logic that is useful
when the research fits the simulation approach
(Rivkin, 2001). In contrast, an unstructured ap-
proach such as stochastic processes offers flex-
ibility when no structured approach fits the re-
search.
Algorithmic representations vary in complex-
ity. Some algorithms consist of two or three
steps involving a few constructs (e.g., March,
1991; Rudolph & Repenning, 2002). Others are
multistep, involving several or more constructs
(e.g., Carroll & Harrison, 1998; Sterman et al.,
1997). Algorithmic complexity should depend on
the complexity of the underlying theoretical
logic being represented and should relate to the
well-known theoretical trade-off between parsi-
mony and accuracy (Pfeffer, 1982). Like other
methods, simulation research attempts to bal-
ance parsimony and accuracy by capturing the
central logic while stripping away the nones-
sential. This balance is ultimately a judgment
call. But, unlike other methods, it is often effec-
tive to err on the side of simplicity in simulation
research in order to enhance the intuition and
confidence of readers in the simulation results
(Repenning, 2003). Complexity (and, thus,
greater accuracy and realism) can then be
added through a series of structured experi-
ments.
Finally, computational representation in-
volves specifying assumptions. Some of these
assumptions relate to boundary or scope condi-
tions of the theory. But other assumptions are
simplifications of the simulation itself that en-
able the researcher to strip out complexity in
order to focus on the central logic and con-
structs. Thus, these assumptions are intimately
related to the complexity of the algorithmic rep-
resentation. For example, both Rivkin’s (2001)
assumption of zero search costs and Adner’s
(2002) assumption that all markets are the same
size enable less complicated algorithmic repre-
sentations. As above, the key judgment is the
balance between parsimony and accuracy. This
is, of course, a familiar judgment in other re-
search methods, where choices about sampling
in specific contexts (e.g., single industry studies)
or about using the controlled environment of the
laboratory, in effect, enable a less complicated
theoretical logic (Fine & Elsbach, 2000). This
judgment, however, is more readily changed in
simulation research because of the relative ease
of shifting computer code.
An important strength of theory development
using simulation methods is internal validity
(Campbell & Stanley, 1966). Creating a compu-
tational representation involves the precise
specification of the theoretical logic that is en-
forced through the discipline of algorithmic rep-
resentation in software (Abelson, Sussman, &
Sussman, 1996). Coupled with the precise spec-
ification of constructs, measures, and assump-
tions that is also enforced by the software, the
discipline of algorithmic representation sharp-
ens loose theoretical arguments about the defi-
nition of constructs, relationships among con-
structs, and underlying logic (Carroll & Harrison,
1998; Sastry, 1997). The result is theory that is more
likely to exhibit strong internal validity.
Verify the Computational Representation
A critical step in developing theory using sim-
ulation methods is verification of the computa-
tional representation. Roughly analogous to ma-
nipulation checks in laboratory experiments
and examination of correlation matrices in mul-
tivariate analysis, verification helps to ensure
that the computational representation accu-
rately embodies the theoretical logic, the theory
is internally valid, and the simulation results
can be interpreted with confidence.
Researchers should verify their computational
representation in several ways. The most impor-
tant is comparing simulation results with the
(implicit or explicit) propositions of the simple
theory. If the simulation confirms the proposi-
tions, then the theoretical logic and its compu-
2007 491Davis, Eisenhardt, and Bingham
tational representation are likely to be correct.6
Rivkin (2001), for example, verified his computa-
tional representation by confirming his proposi-
tion that the advantage of a firm’s replication of
its own strategy over another firm’s imitation of
the same strategy was greatest at moderate lev-
els of strategic complexity. He did so by running
the simulation at various values of strategic
complexity and then comparing the simulation
results with his inverted U-shaped prediction. In
simulations where the simple theory centers on
several well-known processes, each process
should be verified.
Simulation researchers should further verify
their computational representation with robust-
ness checks (sometimes termed sensitivity anal-
ysis7) that increase confidence that the compu-
tational representation is stable. For example,
Zott (2003) used alternative starting values of the
decision variables to confirm that his computa-
tional representation was robust to alternative
initial conditions. Davis and colleagues (2007)
used two alternative operationalizations of their
organizational structure construct to verify that
their simulation results were not dependent
upon a specific representation of their central
construct. Researchers should also use other
techniques to verify that their software coding is
correct, such as tracking variables at intermedi-
ate steps in simulation and running the simula-
tion with extreme values of constructs, and
those techniques specific to particular simula-
tion approaches (e.g., pulse tests in system dy-
namics). These tests often have no theoretical
implications but, rather, are a means of verify-
ing the accuracy of the software code.
When researchers find a mismatch between
the propositions of their simple theory and the
simulation results, it is often due to software
coding errors that are subsequently fixed. But,
occasionally, the mismatch reveals shortcom-
ings in the theoretical logic. In these situations,
researchers often have an opportunity to de-
velop fresh theoretical insights. For example,
Davis and colleagues (2007) unexpectedly ob-
served that the anticipated inverted U-shaped
relationship between the amount of organiza-
tional structure and performance was asymmet-
ric (i.e., steep on one side and skewed on the
other). This led the authors to alter their simple
theory to account for this skew. While Davis and
colleagues (2007) modestly improved their initial
theory, Sastry (1997) made a significant improve-
ment. In her attempt to verify her computational
representation of punctuated equilibrium the-
ory, she found shortcomings in the theoretical
logic. Sastry returned to the original theory and
discovered that Tushman and Romanelli (1985)
had failed to account for some aspects of reori-
entation. This unexpected logical flaw led Sas-
try to reformulate the theory by adding assump-
tions and a negative feedback process.
Overall, the key point of verification is to en-
sure that the computational representation ac-
curately represents the underlying theoretical
logic. Thus, given that the researcher attempts
to encode the theoretical logic in the software,
the simulation results should replicate the sim-
ple theory, bolster internal validity, and thereby
increase confidence in the results of the simula-
tion.
Experiment to Build Novel Theory
Experimentation is at the heart of the value of
simulation methods for developing theory.
6 Both verification and experimentation involve running
the simulation, and thus making several specific decisions
about those runs. One is the number of time steps. The
appropriate number depends on researcher objectives (e.g.,
observing the process unfold, assessing the time to reach an
optimal point, and examining steady-state processes), and
so researchers should justify their choice in light of their
objectives. A second decision is the number of runs per set of
specific values of independent constructs. This is roughly
analogous to sample size in hypothesis-testing research—
specifically, the sample size in a cell of an ANOVA labora-
tory study. Here the decision depends on statistical power.
More runs raise the statistical power, so the choice depends
on the power necessary to perform adequate statistical anal-
ysis (i.e., creating confidence intervals for dependent vari-
able values). The third is the number of sets, roughly anal-
ogous to the number of different conditions in a laboratory
study—that is, the number of cells in an ANOVA laboratory
study. However, unlike laboratory researchers, simulation
researchers are able to make many runs, and so often only
report those that are the most significant vis-à-vis conveying
their results. Researchers should also justify these latter
decisions in light of their research. We refer interested read-
ers to texts on simulation (e.g., Law & Kelton, 1991) for more
information.
7 Sensitivity analysis is a widely used term within simu-
lation with several meanings. We use more precise lan-
guage to distinguish three kinds of analysis that are often
termed sensitivity analysis: verification of propositions of
simple theory, robustness checks of the computational rep-
resentation (roughly analogous to the meaning of robust-
ness using empirical methods), and experimentation to cre-
ate new theory.
492 AprilAcademy of Management Review
While verification of the computational repre-
sentation is necessary and useful for demon-
strating the accuracy of the computational rep-
resentation, establishing internal validity, and
enhancing reader confidence, it involves confir-
mation of theory that is already known. In con-
trast, effective experimentation builds new the-
ory by revealing fresh theoretical relationships
and novel theoretical logic. Moreover, experi-
mentation is a particular strength of simulation
methods. Empirical experimentation is con-
strained by data limitations, and formal model-
ing experimentation is constrained by mathe-
matical tractability. In contrast, simulation
methods enable experimentation across a wide
range of conditions, simply by changing the
software code (Bruderer & Singh, 1996; Zott,
2003).
There are several approaches to effective ex-
perimentation. A common one is varying the
value of constructs that were held constant in
the initial simple theory. This approach is par-
ticularly useful for uncovering moderating and
interaction relationships, new main effects, and
intervening constructs. Typically, researchers
experiment with a wide range of values of a
previously fixed construct in order to fully ex-
plore the effects of the construct on outcomes,
but they then only report the most intriguing
results. For example, after verifying their initial
proposition that experiential learning and cog-
nition would lead to a higher performance than
experiential learning alone, Gavetti and
Levinthal (2000) varied the coupling among pol-
icy elements that was previously held constant.
This enabled the authors to investigate how
coupling moderated the relationship between
cognition and performance and to extend their
simple theory with this interaction effect. Simi-
larly, Rudolph and Repenning (2002) examined
the effects of minor interruptions on the emer-
gence of major organizational catastrophes. Af-
ter verifying the anticipated tipping points, the
authors experimented by varying the previously
constant variance in the interruption rate. This
enabled the authors to extend their original sim-
ple theory to include the interaction effects of
interruption variance on performance.
A second approach to experimentation is un-
packing key theoretical constructs. By unpack-
ing, we mean breaking a single construct into
constituent component constructs. Unpacking a
construct is particularly useful when the origi-
nal construct is imprecise, abstract, or multidi-
mensional such that the different dimensions
may have distinct effects. Experiments then fo-
cus on uncovering these possible effects. For
example, after verifying their simple theory,
Davis and colleagues (2007) unpacked their mar-
ket dynamism construct into four granular con-
structs (i.e., velocity, complexity, ambiguity, and
unpredictability) that captured distinct dimen-
sions of market dynamism. They then experi-
mented with each construct by running the sim-
ulation holding three constructs constant and
varying the fourth. This enabled the authors to
extend their theory to the performance implica-
tions of different kinds of market dynamism and
to isolate the specific implications of each con-
struct for their theory.
A third approach to experimentation is vary-
ing assumptions. This approach is particularly
useful when fundamentally different processes
may reasonably exist. Experiments focus on re-
vealing their possible distinct effects. For exam-
ple, Rivkin (2000) verified his theory relating the
speed with which executives find high-perform-
ing strategies to the coupling among elements
of strategy. In doing so, he assumed an “incre-
mental” search process in which executives
change their current strategy only if altering
specific decisions within that strategy leads to
increased performance. He then experimented
with two alternative search processes: “follow-
the-leader” and “hybrid.” These experiments en-
abled him to elaborate his original theory to
include the implications of alternative search
processes.
A fourth approach to experimentation is add-
ing new features to the computational represen-
tation. By adding this complexity in successive
computational representations, researchers can
build theoretical understanding of the phenom-
enon in a structured way that enhances the un-
derstanding of both researchers and readers.
Additional complexity (e.g., processes and con-
structs) is particularly useful when researchers
want to explore the interaction of multiple pro-
cesses that are well-known alone but not in
combination and, more broadly, when greater
realism is desired. For example, Repenning’s
(2002) simple theory related persistence of man-
agerial commitment to the success of new inno-
vations. After verifying this theory with a single
organizational group, he experimented by add-
ing an additional feature (i.e., second organiza-
2007 493Davis, Eisenhardt, and Bingham
tional group) that made the simulation more
complex but also more realistic. In revealing
that a second organizational group is unlikely to
be successful in adopting an innovation, Repen-
ning extended his simple theory to include mul-
tiple groups, thereby enhancing its theoretical
relevance for real organizations.
Several criteria should shape experimenta-
tion design. The most important criterion is the-
oretical contribution. Understanding where the-
oretical contributions are likely to be usually
stems from knowledge of the literature. Al-
though serendipitous discoveries can occur,
knowing the literature often enables the re-
searcher to know where theoretical discrepan-
cies are and so where to experiment to improve
the likelihood of intriguing theoretical insights.
Such insights are the reason d’être of the re-
search. A related criterion for experimentation
design is realism. As noted earlier, it is often
effective to begin with a simple computational
representation in order to build reader intuition
and confidence in the simulation. Experimenta-
tion can then add realism that enhances the
generalizability of the resulting theory.
Experimentation is closely associated with
building theory using “disciplined imagination”
(Weick, 1989). That is, researchers using simula-
tion can readily engage in an evolutionary pro-
cess of speculative experiments to create alter-
native versions of theory (imagination) and then
select the best among them (discipline). Al-
though researchers can engage in disciplined
imagination using other methods, such as sys-
tematic thought experiments, these methods of-
ten fail when the theory involves intertwined
and longitudinal processes, timing effects, and
nonlinearities that are the province of simula-
tion. In effect, the simple theory of a simulation
becomes a platform for elaborating and extend-
ing theory through creative and systematic ex-
perimentation (Weick, 1989). The result may be
fundamental (i.e., widely applicable and nonob-
vious) theory (Lave & March, 1975).
Overall, experimentation is at the heart of the
value of using simulation methods for theory
development. While verification confirms exist-
ing theory and bolsters internal validity, exper-
imentation adds the creative and even surpris-
ing theoretical contributions that build new
theory. Sometimes this new theory is an incre-
mental advance. But sometimes this theory is a
fundamental improvement in which frame-
breaking insights generate qualitatively better
theory.
Validate with Empirical Data
A final step in theory development using sim-
ulation methods is validation. Validation in-
volves comparison of simulation results with
empirical data. If the results of the simulation
match the empirical evidence, then the simula-
tion is validated for that empirical context. This
strengthens the external validity of the theory—
that is, generalizability and predictability of the
theory (Campbell & Stanley, 1966).
There is, however, some debate over the value
of validation. For example, some theorists argue
that the central task of theory development is
creating interesting theory, and so they dimin-
ish the importance of validation (e.g., Van
Maanen, 1995; Weick, 1989). Others disagree.
Our view is contingent—that is, the importance
of validation should depend on the source of the
simple theory that is the basis of the simulation.
If this theory is based primarily on empirical
evidence (e.g., field-based case studies and em-
pirically grounded processes), then validation is
less important, because the theory already has
some external validity. In contrast, if the theory
is based primarily on nonempirical argument
(e.g., formal analytic modeling) or on evidence
from distant scientific disciplines (e.g., physics),
then validation is more important.
There are several approaches to effective val-
idation. One is to compare the results of the
simulation to statistical results derived from
large-scale empirical data. This approach en-
ables a broad-brush validation of the simulation
results (Adner & Levinthal, 2001). A standard
technique in economics, for instance, is to use
theories developed with simulation to “predict”
the results of some well-known data set. For
example, Nelson and Winter (1982) created a
simulation of their theory of evolutionary eco-
nomic change and then used that simulation to
reproduce historical productivity data. Another
approach is to compare the simulation results
and theoretical logic with case study data. This
approach seeks to demonstrate that the simula-
tion is consistent with the specific details of one
or a few examples. This enables granular vali-
dation. For example, Sterman and colleagues
(1997) used interview data from one organization
to show that their theory of organizational
494 AprilAcademy of Management Review
change was plausible in at least this one orga-
nization. The choice between these two ap-
proaches usually depends on data availability.
DISCUSSION
While simulation is an increasingly signifi-
cant methodological approach in the organiza-
tions and strategy literature (e.g., Lant & Mezias,
1990; Rivkin & Siggelkow, 2003; Repenning, 2002;
Zott, 2003), its link to theory development re-
mains unclear and even controversial. Our pur-
pose here has been to clarify when and how to
use simulation methods to develop theory.
Our first contribution was a roadmap for con-
ducting effective theory development using sim-
ulation. Like all effective research, developing
theory from simulation methods depends on be-
ginning with an intriguing research question
that relates to fundamental theoretical issues.
Simulation is especially effective when that re-
search question relates to competing tensions
(e.g., long versus short run, structure versus
chaos) and intertwined processes (e.g., inertia
and change, competition and legitimation) that
are especially well-addressed by simulation.
The central activity is developing an accurate
computational representation of simple (i.e., un-
developed) theory through appropriate selection
of a simulation approach (e.g., genetic algorithm
versusstochasticprocesses), constructoperation-
alization, and algorithmic representation. The
benefits of simulation emerge in verifying the
computational representation, which strength-
ens the internal validity of the simple theory,
and, more significant, in creatively experiment-
ing to produce novel theoretical insights. In-
deed, creative experimentation is at the heart of
the value of simulation methods for theory de-
velopment.
Our second contribution was positioning sim-
ulation methods within the broad context of the-
oretical development in the organizations and
strategy literature. Simulation is particularly
useful for developing theory in the “sweet spot”
between theory creating using such methods as
multiple case inductive research (Eisenhardt,
1989) and formal modeling (Freese, 1980) and
theory testing using empirical evidence and
multivariate statistical techniques (Lattin, 2003;
Pfeffer, 1993). On the one hand, theory-creating
methods can reveal simple theory but are lim-
ited in their ability to elaborate that theory. The
inductive case method is constrained by limited
data, and formal modeling is constrained by
mathematical tractability. Simulation can miti-
gate these weaknesses by exploring, elaborat-
ing, and extending simple theory that is pro-
duced by these theory-creating methods. On the
other hand, as Sutton and Staw (1995) argue,
many theory-testing studies lack conceptual
precision and logical theoretical argument. Sim-
ulation can mitigate these weaknesses in inter-
nal validity by tightening the rigor of the under-
lying theoretical logic, sharpening constructs,
and elaborating theoretical propositions prior to
empirical test.
Strengths and Weaknesses
These observations suggest several important
strengths for developing theory with simulation.
One is internal validity (Cook & Campbell, 1979).
The computational rigor of simulation forces
precise specification of constructs, assumptions,
and theoretical logic that creates strong internal
validity. This emphasis on internal validity is
particularly valuable because it addresses a
common weakness of empirical research: often
poor theoretical logic regarding “why” proposi-
tions might be true (Sutton & Staw, 1995; Whet-
ten, 1989). This emphasis also mitigates another
common weakness of empirical research: weak
specification of boundary conditions. By requir-
ing precise specification of assumptions, simu-
lation typically bounds the scope of the theory
and so clarifies boundary conditions. Thus,
while other methods emphasize constructs and
propositions, simulation puts their underlying
theoretical logic and assumptions center stage.
Another strength is experimentation. Simula-
tion creates a computational laboratory in
which researchers can systematically experi-
ment (e.g., unpack constructs, relax assump-
tions, vary construct values, add new features)
in a controlled setting to produce new theoreti-
cal insights. This experimentation is particu-
larly valuable when the theory seeks to explain
longitudinal and processual phenomena that
are challenging to study using empirical meth-
ods because of their time and data demands.
Simulation is also well-suited to theory develop-
ment related to nonlinear phenomena, such as
tipping points, feedback loops, thresholds and
catastrophes, and asymmetries. These are diffi-
cult to uncover using inductive case methods
2007 495Davis, Eisenhardt, and Bingham
and to examine using standard statistical tech-
niques. Yet, significantly, these phenomena are
becoming central as theory development moves
from cross-sectional and equilibrium perspec-
tives to longitudinal and dynamic ones.
Yet, like all methods, theory development us-
ing simulation has weaknesses. A primary one
is external validity. Simulation eliminates com-
plexity in order to focus on the core aspects of
phenomena and so uses computational repre-
sentations that are often stark, such as repre-
senting an organization by a 0/1 bit string (Bru-
derer & Singh, 1996), or making clearly false
assumptions, such as no disadvantages to being
a second mover (Rivkin, 2000). The result can be
an overly simplistic and distant model that fails
to capture critical aspects of reality.
Toward Better Theory Development Using
Simulation Research
Like all research, theory development using
simulation methods should be evaluated ac-
cording to two fundamental criteria: theoretical
contribution and strength of method. Theoretical
contribution is partially determined by the qual-
ity of the research question and its related
grounding in the literature. That is, the research
question should center on a significant issue
within the related literature. In contrast, re-
search with no grounding in the relevant litera-
ture often has a research question that does not
address a useful theoretical gap, or it has theo-
retical constructs and theory that do not fit with
their conceptualization and terminology within
the literature. This lack of grounding can further
lead to weak integration of the simulation re-
sults with the literature. While awareness of
and at least some grounding in the extant liter-
ature seem obvious, our experience indicates
that simulation researchers are more likely to
misunderstand their research context (i.e.,
choose poor research questions, fail to under-
stand and use well-developed concepts) than
others, or to simply pass over the contributions
of others working with different methods. Yet
without situating the research question and the-
ory in the current literature and then relating
results to that literature, it is difficult to have a
theoretical contribution. Thus, the reader should
ask whether the research is situated within the
relevant research literature and addresses a
substantive research question.
Theoretical contribution is also determined by
the quality of the experimentation. As noted ear-
lier, experimentation is a key strength of simu-
lation and the primary source of theoretical in-
sight. That is, systematic experimentation
focused on unexplored theoretical constructs
(e.g., varying construct values, unpacking con-
structs into more precise constructs), major as-
sumptions (e.g., relaxing them, varying them),
and important elaborations (e.g., adding new
features) is integral to theoretical contribution.
Therefore, when evaluating theory development
using simulation, the reader should focus on the
experimentation to assess whether and to what
extent theoretical contribution has been made.
Simulation authors should likewise distin-
guish between the verification of the simple
theory that confirms the computational repre-
sentation and the internal validity, and the
theoretical insights that emerge from system-
atic experimentation (or occasionally from
failed verification) that creatively elaborate and
extend the theory. Although this may seem ob-
vious, simulation researchers have a tendency
to communicate their findings as if simply sim-
ulation of a phenomenon per se is a theoretical
contribution. That is, they confuse building a
simulation with theoretical contribution. Some
also stop at verification (e.g., various robustness
checks), rather than continuing to genuine ex-
perimentation. But research that only verifies
known theory or jumbles verification and exper-
imentation typically fails to create theoretical
contribution. Indeed, like all research, the theo-
retical contribution of simulation research is in-
dependent of method and relates to the addition
of new theoretical insight. Thus, the reader
should ask whether the results constitute a the-
oretical contribution.
The second criterion is the strength of method.
In the context of the roadmap outlined earlier, a
high-quality method includes justification for
using simulation for the research question at
hand and using a simulation approach (e.g., cel-
lular automata, stochastic processes) that fits
the research. For example, as is common prac-
tice in empirical research, simulation research-
ers should clearly justify the simulation ap-
proach (as one would justify a statistical
approach), define constructs, and indicate the
computational measures (as one would describe
empirical measures). The methods section of
496 AprilAcademy of Management Review
other types of research provides a useful tem-
plate for conveying this information.
In addition, high-quality simulation methods
should have an accurate computational repre-
sentation of the theoretical logic that is con-
veyed to the reader in a way that carefully
builds understanding of assumptions and the
logical flow of the simulation. Given the opacity
of computer coding, this puts a premium on the
effective presentation of the theoretical logic. In
addition to logical argument (roughly analo-
gous to the theoretical development of hypothe-
ses in empirical research), pictures and flow-
charts are helpful in this regard (see Rudolph &
Repenning, 2002, and Zott, 2003, for exemplars).
High-quality simulation methods also include
explicit verification of the computational repre-
sentation that confirms the accuracy and inter-
nal validity of the computational representation
and builds reader confidence in the simulation.
It is helpful to explicitly state the propositions of
the simple theoretical ideas that form the basis
of the simulation and then to verify them clearly
by running the simulation. In contrast, when the
computational representation is poor, poorly de-
scribed, or unverified, the simulation results are
unreliable and/or not believed.
Finally, a high-quality method involves the
appropriate statistical design of the verification
and experimentation simulation runs. This de-
sign should include an appropriate number of
time steps in each simulation run and number of
simulation runs per experiment (related to the
statistical power). As in empirical research,
high-quality simulation methods also include
statistical confidence intervals (Law & Kelton,
1991). In contrast, when research does not justify
the basic statistical properties of the simulation
(e.g., number of time steps, number of runs per
experiment) or provide basic statistical analysis
(i.e., confidence intervals), confidence in the sta-
tistical results is compromised.
Overall, readers should expect that high-
quality theory development using simulation
methods meets the usual standards of strong
theory, such as parsimony, internal validity,
accuracy, and interest (Davis, 1971; Eisen-
hardt, 1989; Pfeffer, 1982; Priem & Butler, 2001).
Or, more simply, the result should explain,
predict, and delight (Weick, 1989). Given the
starting point of simple theory, the result of
effective simulation can be fundamental the-
ory that is parsimonious, internally consistent,
and applicable to a wide range of situations
(Pfeffer, 1982).
CONCLUSION
We began by observing that while simulation
is a significant method for theory development,
its usefulness is unclear and even controversial.
Our purpose was to clarify how and when to use
simulation methods for theory development. We
provide two contributions. One is a roadmap for
developing theory through simulation (Table 1),
including the pivotal role of experimentation in
creating theoretical contribution. The second is
positioning simulation methods in the “sweet
spot” between theory creating using inductive
case methods and formal modeling, and theory
testing using multivariate statistical tech-
niques. We conclude by observing that simula-
tion is likely to become an increasingly promi-
nent method of theory development. As
organizations and strategy scholars move to an
emphasis on theory explaining dynamic and
longitudinal phenomena, simulation methods
will be a natural choice.
REFERENCES
Abelson, H., Sussman, G. J., & Sussman, J. 1996. Structure and
interpretation of computer programs. Cambridge, MA:
MIT Press.
Adner, R. 2002. When are technologies disruptive? A de-
mand-based view of the emergence of competition. Stra-
tegic Management Journal, 23: 667–688.
Adner, R., & Levinthal, D. 2001. Demand heterogeneity and
technology evolution: Implications for product and pro-
cess innovation. Management Science, 47: 611–628.
Aldrich, H. 1999. Organizations evolving. Thousand Oaks,
CA: Sage.
Andreoni, J., & Miller, J. 1995. Auctions with artificial adap-
tive agents. Games and Economic Behavior, 10: 39–64.
Arifovic, J., Bullard, J., & Duffy, J. 1997. The transition from
stagnation to growth: An adaptive learning approach.
Journal of Economic Growth, 2: 185–209.
Bruderer, E., & Singh, J. S. 1996. Organizational evolution,
learning, and selection: A genetic-algorithm-based
model. Academy of Management Journal, 39: 1322–1349.
Bullard, J., & Duffy, J. 2001. Learning and excess volatility.
Macroeconomic Dynamics, 5(2): 272–302.
Campbell, D. T., & Fiske, D. W. 1959. Convergent and dis-
criminant validation by the multitrade-multimethod ma-
trix. Psychological Bulletin, 56: 81–105.
Campbell, D. T., & Stanley, J. C. 1966. Experimental and
2007 497Davis, Eisenhardt, and Bingham
quasi-experimental designs for research. Chicago: Rand
McNally.
Carley, K. M. 2001. Computational approaches to sociologi-
cal theorizing. In J. Turner (Ed.), Handbook of sociologi-
cal theory: 69–84. New York: Kluwer Academic/Plenum.
Carroll, G., & Harrison, J. R. 1998. Organizational demogra-
phy and culture: Insights from a formal model and sim-
ulation. Administrative Science Quarterly, 43: 637–667.
Chattoe, E. 1998. Just how (un)realistic are evolutionary al-
gorithms as representations of social processes? Journal
of Artificial Social Science Simulation, 1(3): 2.1–2.36.
Cohen, M. D., March, J., & Olsen, J. P. 1972. A garbage can
model of organizational choice. Administrative Science
Quarterly, 17: 1–25.
Cook, T. D., & Campbell, D. T. 1979. Quasi-experimentation:
Design and analysis issues for field settings. Boston:
Houghton Mifflin.
Davis, J., Eisenhardt, K., & Bingham, C. 2007. Complexity
theory, market dynamism, and the strategy of simple
rules. Working paper, Stanford Technology Ventures
Program, Stanford University, Stanford, CA.
Davis, M. S. 1971. That’s interesting! Towards a phenomenol-
ogy of sociology and a sociology of phenomenology.
Philosophy of Social Science, 1: 309–344.
Dooley, K. 2002. Simulation research methods. In J. A. C.
Baum (Ed.), Companion to organizations: 829–848. Ox-
ford: Blackwell.
Dubin, R. 1976. Theory building in applied areas. In M. Dun-
nette (Ed.), Handbook of industrial and organizational
psychology: 17–40. Chicago: Rand McNally.
Eisenhardt, K. M. 1989. Building theories from case study
research. Academy of Management Review, 14: 532–550.
Fichman, M. 1999. Variance explained: Why size doesn’t (al-
ways) matter. Research in Organizational Behavior, 21:
295–331.
Fine, G. A., & Elsbach, K. D. 2000. Ethnography and experi-
ment in social psychological theory building. Journal of
Experimental Social Psychology, 36: 51–76.
Forrester, J. 1961. Industrial dynamics. Cambridge, MA: MIT
Press.
Freese, L. 1980. Formal theorizing. Annual Review of Sociol-
ogy, 6: 187–212.
Gallager, R. 1996. Discrete stochastic processes. Boston: Klu-
wer Academic.
Gavetti, G., & Levinthal, D. 2000. Looking forward and look-
ing backward: Cognitive and experiential search. Ad-
ministrative Science Quarterly, 45: 113–137.
Goldberg, D. E. 1989. Genetic algorithms: In search of opti-
mization and machine learning. Reading, MA: Addison-
Wesley.
Hannan, M. T., & Freeman, J. 1989. Organizational ecology.
Cambridge, MA: Harvard University Press.
Harrison, J. R., & Carroll, G. R. 1991. Keeping the faith: A
model of cultural transmission in formal organizations.
Administrative Science Quarterly, 36: 552–582.
Holland, J. H. 1975. Adaptation in natural and artificial sys-
tems. Ann Arbor: University of Michigan Press.
Kauffman, S. 1989. Adaptation on rugged fitness landscapes.
In E. Stein (Ed.), Lectures in the science of complexity,
vol. 1. Reading, MA: Addison-Wesley.
Kauffman, S. 1993. The origins of order. New York: Oxford
University Press.
Kreps, D. M. 1990. Corporate culture and economic theory. In
J. E. Alt & K. A. Shepsle (Eds.), Perspectives on positive
political economy: 90 –143. Cambridge & New York:
Cambridge University Press.
Langton, C. G. 1984. Self-reproduction in cellular automata.
Physica, 10D: 134–144.
Lant, T., & Mezias, S. 1990. Managing discontinuous change:
A simulation study of organizational learning and en-
trepreneurship. Strategic Management Journal, 11: 147–
179.
Lant, T., & Mezias, S. 1992. An organizational learning model
of convergence and reorientation. Organization Science,
3: 47–71.
Lattin, J. 2003. Analyzing multivariate data. Toronto: Brooks/
Cole, Thomson Learning.
Lave, C., & March, J. G. 1975. An introduction to models in the
social sciences. New York: Harper & Row.
Law, A. M., & Kelton, D. W. 1991. Simulation modeling and
analysis (2nd ed.). New York: McGraw-Hill.
Lee, T., Mitchell, T., Sablynski, C. 1999. Qualitative research
in organizational and vocational psychology. Journal of
Vocational Behavior, 55: 161–187.
Levinthal, D. 1997. Adaptation on rugged landscapes. Man-
agement Science, 43: 934–950.
Lomi, A., & Larsen, E. 1996. Interacting locally and evolving
globally: A computational approach to the dynamics of
organizational populations. Academy of Management
Journal, 39: 1287–1321.
March, J. G. 1991. Exploration and exploitation in organiza-
tional learning. Organization Science, 2: 71–87.
Nelson, R. R., & Winter, S. G. 1982. An evolutionary theory of
economic change. Cambridge, MA: Belknap Press of
Harvard University Press.
Pfeffer, J. 1982. Organizations and organization theory. Bos-
ton: Pitman.
Pfeffer, J. 1993. Barriers to the advance of organizational
science: Paradigm development as a dependent vari-
able. Academy of Management Review, 18: 599–620.
Priem, R. L., & Butler, J. E. 2001. Is the resource-based “view”
a useful perspective for strategic management re-
search? Academy of Management Review, 26: 22–41.
Repenning, N. 2002. A simulation-based approach to under-
standing the dynamics of innovation implementation.
Organization Science, 13: 109–127.
Repenning, N. 2003. Selling system dynamics to (other) social
scientists. System Dynamics Review, 19: 303–327.
Rivkin, J. W. 2000. Imitation of complex strategies. Manage-
ment Science, 46: 824–844.
498 AprilAcademy of Management Review
Rivkin, J. W. 2001. Reproducing knowledge: Replication with-
out imitation at moderate complexity. Organization Sci-
ence, 12: 274–293.
Rivkin, J. W. & Siggelkow, N. 2003. Balancing search and
stability: Interdependencies among elements of organi-
zational design. Management Science, 49: 290–311.
Rosenthal, R., & Rosenow, R. L. 1991. Essentials of behavioral
research: Methods and data analysis (2nd ed.). New York:
McGraw-Hill.
Rudolph, J., & Repenning, N. 2002. Disaster dynamics: Under-
standing the role of quantity in organizational collapse.
Administrative Science Quarterly, 47: 1–30.
Sastry, M. A. 1997. Problems and paradoxes in a model of
punctuated organizational change. Administrative Sci-
ence Quarterly, 42: 237–275.
Schelling, T. 1971. Dynamic models of segregation. Journal of
Mathematical Sociology, 1: 143–186.
Sterman, J. 2000. Business dynamics: Systems thinking and
modeling for a complex world. New York: Irwin
McGraw-Hill.
Sterman, J., Repenning, N., & Kofman, F. 1997. Unanticipated
side effects of successful quality programs: Exploring a
paradox of organizational improvement. Management
Science, 43: 503–521.
Stinchcombe, A. 1968. Constructing social theories. Chicago:
University of Chicago Press.
Sutton, R. I., & Staw, B. M. 1995. What theory is not. Admin-
istrative Science Quarterly, 40: 371–384.
Tushman, M., & Romanelli, E. 1985. Organizational evolution:
A metamorphosis model of convergence and reorienta-
tion. Research in Organizational Behavior, 7: 171–222.
Van Maanen, J. 1995. Style as theory. Organization Science,
6: 133–143.
Weick, K. E. 1989. Theory construction as disciplined imagi-
nation. Academy of Management Review, 14: 516–531.
Weick, K. E. 1993. The vulnerable system: An analysis of the
Tenerife air disaster. In K. H. Roberts (Ed.), New chal-
lenges to understanding organizations: 173–198. New
York: Macmillan.
Whetten, D. 1989. What constitutes a theoretical contribu-
tion? Academy of Management Review, 14: 490–495.
Wolfram, S. 2002. A new kind of science. Champaign, IL:
Wolfram Media.
Wright, S. 1931. Evolution in Mendelian populations. Genet-
ics, 16: 97–159.
Yerkes, R. M., & Dodson, J. D. 1908. The relation of strength of
stimulus to rapidity of habit formation. Journal of Com-
parative Neurological Psychology, 18: 459–482.
Zott, C. 2002. When adaptation fails: An agent-based expla-
nation of inefficient bargaining under private informa-
tion. Journal of Conflict Resolution, 46: 727–753.
Zott, C. 2003. Dynamic capabilities and the emergence of
intra-industry differential firm performance: Insights
from a simulation study. Strategic Management Journal,
24: 97–125.
Jason P. Davis (jpdavis@stanford.edu) is a doctoral candidate in management science
and engineering, Stanford University, where he received his M.A. in sociology. His
current research interests include competitive strategy, organizational structure, and
collaborative technology innovation in highly dynamic environments. He uses a
combination of inductive, multicase, and simulation methods.
Kathleen M. Eisenhardt (kme@stanford.edu) is the Stanford W. Ascherman M.D. Pro-
fessor at Stanford University and codirector of the Stanford Technology Ventures
Program. She received her Ph.D. from Stanford University. Her current research inter-
ests include competitive interaction and strategic approaches to power by enterpre-
neurial firms, multibusiness organization, and strategy as simple rules.
Christopher B. Bingham (cbingham@umd.edu) is assistant professor of strategy and
organization at the Robert H. Smith School of Business, University of Maryland. He
received his Ph.D. from Stanford University. His current research interests revolve
around organizational learning and change, dynamic capabilities, and strategy in
unpredictable environments.
2007 499Davis, Eisenhardt, and Bingham


