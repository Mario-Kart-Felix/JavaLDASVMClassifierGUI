 1
Experimental Economics and Experimental Computer 
Science: A Survey
 Jens Grossklags 
School of Information 
UC Berkeley 
102 South Hall, Berkeley, CA 
jensg@ischool.berkeley.edu 
 
 
 
 
 
 
 
 
ABSTRACT 
In surprisingly many computer science research projects, system 
outcomes may be influenced by computerized or human agents 
with different economic incentives. Such studies include P2P 
networks, routing protocols, agent systems, and attacker-defender 
security games. Even the most technical system raises a pressing 
economic question: what incentives will drive the success or 
failure of deployment?  
Traditional computer science techniques, in isolation, may 
overlook factors that are critical to the overall outcome.  In these 
situations, human-subject experiments may form a useful 
complement, broadening our understanding through the 
formulation and testing of economically motivated hypotheses. 
I argue that these efforts can benefit from the large body of work 
that has been conducted in the field of experimental economics in 
the last 30 to 40 years. I discuss the methodology of experimental 
economics and review recent work that falls on the boundary of 
computer science and economics experiments and is likely 
unfamiliar to many professionals and researchers in technical 
fields.  
Categories and Subject Descriptors 
J.4 [Social and Behavioral Sciences] – economics, psychology  
General Terms 
Economics, Experimentation, Human Factors 
Keywords 
Experimental Economics, Human Subjects, Experimental 
methodology  
1. INTRODUCTION 
Computer science is a quickly evolving field covering diverse 
areas such as computer networks, security, artificial intelligence, 
nanotechnology, and computer architecture. Although research 
often emphasizes development of theory, one central goal of 
computer science research remains practical deployment.1 
Methods of experimental computer science, such as simplified or 
small-scale implementation, simulation, and computational 
experimentation, provide tools for analyzing performance, 
reliability, and stability, and highlight plausible deployment 
scenarios while also influencing theory in a feedback loop 
[15][44]. Observed discrepancies between theory and fact can 
motivate incremental progress or even create new paths for 
overcoming dogmatic beliefs held within disciplines [37]. 
The resulting interaction between theory and the gathering and 
analysis of data can follow different experimental approaches 
[57]: theory testing, discrimination between competing theories, 
exploration of a particular theory’s failure, identification of 
empirical regularities as a basis for new theory, comparison of 
institutions or design of new institutions, and evaluation of 
specific policy proposals.2  
In recent years, formal methods of economic modeling and 
analysis were extended to many fields in computer science, based 
on the observation that in computer systems, as in previously 
studied applications, agents compete for scarce resources. 
Examples include shared access to bandwidth [3], scheduling of 
resources aboard a spacecraft [46], and the interaction between 
email spammers and end-users [39]. Such examples have only 
become more common with the recent push for decentralization 
of computing resources.  
Much like computer science, economics has both theoretical and 
experimental modes of inquiry, and a useful parallel may be 
drawn linking these modes in either discipline. Theoretical 
economics, when used in isolation, has fundamental limitations 
that are relevant to the computer science researcher. The favored 
approach in economic theory is to assume that agents behave fully 
rationally in that they aggregate and process all available 
information, and identify and eventually choose the option that 
results in the highest utility. Many issues of concern to computer 
scientists, such as combinatorial algorithms, cannot meet this 
standard due to computational constraints or the infeasibility of 
computing optimal solutions. Although recent work in economics 
offers relaxations from full rationality [9][13][22], there is no 
settled alternative for predicting human behavior. Therefore, 
                                                                 
1 This is particularly important for technologies with little prior 
deployment results such as clean-slate design proposals. 
2 Experiments also have important pedagogical purposes [21]. 
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
ExpCS, June 13-14, 2007, San Diego, CA 
Copyright 2007 ACM 978-1-59593-751-3/07/06...$5.00. 
 
 2
economic theory, much like computer science theory, necessarily 
leaves open questions regarding practical deployment. 
To continue the analogy, experimental economics, in the form of 
human-subject experiments, is used to investigate how humans 
behave “in the wild.” Experimental economics serves as a 
complement to economic theory: It interacts with economic 
theory in all the same ways that are familiar from the computer 
science realm – validating theories, suggesting new theories, and 
so forth. For the computer science researcher, economic 
experimentation provides insight into realistic behaviors by 
human participants and their expected impact on system 
outcomes. 
Of course, many situations of interest to computer scientists 
involve both human actors and the technical systems they interact 
with.  In this arena, the line between experimental economics and 
experimental computer science is blurred, and it often becomes 
impossible to decompose an experiment into separate economic 
and technical components. Some of the most interesting questions 
in both fields appear at this boundary, challenging the researcher 
to evaluate economic, psychological, and technical influences 
under realistic assumptions. 
In this paper, I will review selected studies that straddle this 
narrow frontier between disciplines. Although a wide range of 
economics research is relevant to computer science (such as 
fairness models or auction theory) my focus will be on those 
studies that make a genuine effort to communicate to both 
economics and computer science researchers.  
Such efforts have yielded contributions to many fields of 
computer science. In this review, I restrict attention to research in 
computer networks and agent systems. Other areas where cross-
disciplinary efforts have been successful include security 
[36][38][50], privacy [27][62], and human-computer interaction 
[26]. 
An important theme will emerge from examining the papers in 
this literature: Experimental studies benefit from a clear and 
concise theory about the underlying workings of an economic 
institution (e.g., a market or auction) and expectations about how 
agents react to incentives in this environment. This theory does 
not always require a formal mathematical model but need only be 
sufficient to propose sensible hypotheses concerning differences 
in choices made by agents based on experimental manipulations.  
My goal is to acquaint the computer science researcher with the 
methodology of experimental economics. I hope that the reader 
who completes this survey will be better equipped to read papers 
from this interesting area, applying the framework of 
experimental economics and placing papers in the proper 
perspective. The examples presented here will attest to the value 
of researchers that understand both computer science and 
economics research, and are able to find new opportunities for 
cross-fertilization. 
In Section 2, I discuss the guiding principles of experimental 
economics research and expand on many other important design 
considerations. Section 3 reviews recent experiments with 
relevance for computer networks research and agent systems. 
Section 4 completes the paper with concluding remarks. 
2. METHODOLOGY OF EXPERIMENTAL 
ECONOMICS 
2.1 Guiding principles 
In this section, I will discuss major principles that were developed 
in the experimental literature and have become somewhat 
codified. 
2.1.1 Realism of Experimental Economics Results 
The central goal of experimentalism is to identify causes that 
produce certain results unconditionally of background 
assumptions. However, the complexity of economic systems 
prevents experimenters but also theorists from correctly 
identifying all defining aspects of a decision-making situation or 
market. Therefore, both groups are forced to make simplifying 
assumptions about an economic problem to derive tractable 
solutions. 
In the philosophy of science the resulting problem is related to the 
Duhem-Quine thesis which reminds scientists of the impossibility 
to test a scientific theory in isolation because every empirical test 
requires assumptions about background (or auxiliary) aspects. 
Consequently, a failure of a theory in the laboratory can always 
be attributed to more or less convincing auxiliary hypotheses [57]. 
It appears obvious that few experiments could survive such a 
strong test of external validity. 
Economic experimentalists aim for a more local form of external 
validity test called parallelism in which they evoke a more narrow 
relationship between small-scale experiments and realistic 
markets. Given main characteristics of the economic environment 
observed in the field and modeled in the laboratory they argue 
that observations from the experiment will then carry over into 
these closely related real world institutions and can be further 
validated by field data. Of course, researchers will also speculate 
about conclusions that can be drawn for a wider set of economic 
decision-making situations. This process can lead to critical 
discussion and can be a strong driver for further experimentation 
[58]. Of course, proponents as well as skeptics can and should 
design further experiments. 
2.1.2 Control and Repetition 
A leading tenet of good experimentation (as well as theoretical 
modeling) is simplicity. The researcher wants to be able to relate 
differences in observations to the manipulation of the 
experimental environment.  
Formal economic models form good bases for experimental 
designs. In many aspects, models can be considered more 
simplistic; for example, they leave out important details that need 
to be addressed in an experimental protocol such as when exactly 
and what type of information is disseminated to the subjects and 
in what form. On the other hand, models can also be considered 
more general since experimental setups require specific parameter 
values (such as demand and supply functions). 
In designing an experiment based on theory, a researcher faces a 
tradeoff. Closely following theoretically derived results allows the 
researcher to formulate and rigorously test hypotheses. However, 
sticking too close to the theory will diminish the ability to learn 
whether results also extend to an environment that departs from 
 3
the very stringent assumptions made for analytic purposes. 
Statistical methods or simulations are more likely to reveal logical 
mistakes in theories [21]. 
A strength of the laboratory method is that the experimenter can 
iteratively add and remove aspects of the field environment and 
carefully study the impact of the manipulation.  
An important limitation is that even in an experiment, many 
characteristics of human participation can only be determined 
with difficulty, e.g., intrinsic motivation of subjects, type of risk 
preferences, heterogeneity of prior experience and expectations 
with relevance to the experiment, and strength of selfish or other-
regarding preferences. In fact, many experiments in economics 
and psychology have shown that subtle aspects such as framing of 
instructions can (depending on the study) take significant 
influence on the results. 
It is, therefore, an important and enforced practice in experimental 
economics to clearly describe all aspects of the laboratory 
environment. Instructions and possible screenshots (or code) used 
for the experiments are often available in the appendices or from 
the authors. Many researchers also allow others to access their 
raw data to conduct their own analysis. 
The goal is to allow for repetition, comparability and incremental 
variations of the experiment. In particular, for experiments 
analyzing individual decision-making behavior that is often 
subject to individual biases or misconceptions researchers have 
been motivated to test the robustness of the results repeatedly 
(see, for example, research on preference reversal or the 
endowment effect).  
The result is a much better global understanding of many games, 
e.g., the prisoners’ dilemma that has been subject of hundreds of 
experimental studies. 
2.1.3 Induced value theory 
One central aspect of control in experiments is the design and 
utilization of an incentive structure for participants. The 
experimenter wants to induce certain characteristics in the subject 
population that relate to a market institution. For example, the 
incentive structure in a market should motivate individuals to 
demand or ask for prices and/or quantities that yield the subject a 
higher payoff. Another aspect is that outside characteristics of the 
participants become less important. Experimenters refer to this 
design process as induced value theory [58]. 
Formally, three conditions have to be met to control for agent’s 
characteristics [21]: First, monotonicity requires that subjects 
prefer more of the reward medium to less and that subjects will 
not become satiated. Second, salience is achieved if individuals’ 
payoffs depend on their actions, actions by other agents and the 
characteristics of the institution in an understandable fashion. 
Third, dominance is guaranteed if participants’ utility (and 
motivation) stems overwhelmingly from the reward structure so 
that other influences receive little attention. 
The concept of salience most clearly distinguishes experimental 
economics studies from surveys containing questions about past 
self-reported economically relevant behavior or hypothetical 
choice scenarios. Similarly, most examples from participatory 
simulations with human subjects [30] or business simulations [63] 
do not meet these requirements. To further discuss the permeable 
boundary between agent studies and human subjects experiments 
see also the detailed review article by Duffy [16] that offers a 
comprehensive discussion of the empirical validity of agent-based 
modeling approaches in terms of explaining data from 
experiments with human players. Interested readers might also 
want to consider Guttman et al. [29] who review research on 
agent-mediated electronic commerce. 
Results from these different approaches are often comparable or 
even largely identical to experimental economics results 
depending on the context and the incentive structure of the 
economic institution under investigation. But such a comparison 
has to be made on a case by case basis. See, for example, [32] and 
[60] for a discussion of experimental evidence on this topic. 
Furthermore, experiments bundled with surveys and/or 
hypothetical scenarios can deliver fruitful research on a number of 
issues, e.g., the regressions on experimental results and survey 
data in the paper by Glaeser et al. [24] on social capital and trust. 
2.2 Other design considerations 
The guiding principles set the baseline for experiments, however, 
leave many research design questions unanswered. Below I will 
discuss a few aspects (among several more such as tools and 
methods for analysis of experimental data) that need to be 
considered by every experimenter that aims to design an 
experiment from scratch. 
2.2.1 Type and size of rewards 
Many universities have research laboratories that define 
conventions for an adequate average compensation of subjects in 
experiments. In absence of such regulations researchers should 
take into consideration the average wage that the selected target 
population could receive within the time of the experiment if they 
would conduct their typical occupation. The principle of 
dominance also requires that subjects feel motivated during the 
experiment to earn higher returns, therefore, a combination of 
show-up fees and incentive-compatible payments that depend on 
subjects’ actions should not be structured in a way that the flat fee 
dominates other payments. Note that payments need not always 
be of monetary nature. Subjects can also be paid with physical 
goods (e.g., mugs in endowment experiments) or even immaterial 
goods (such as better recommendations for a planned purchase 
[62]). 
Several types of experiments (such as auctions) can lead to widely 
different variable payments to the subjects. Experimenters can 
balance payments by, for example, switching the role of buyers 
and sellers or changing private valuations for auction goods. 
However, there are limits to such attempts. For example, many 
experiments are conducted only once (so called one-shot games) 
and some participants might be disadvantaged by their initial 
bargaining position. 
An often voiced criticism of experimental economics is that 
conclusions on real world markets or choices are drawn that 
involve high stakes based on experiments with relatively small 
rewards. In response several researchers have replicated well-
known experiments but with larger relative rewards, for example, 
by conducting experiments in countries with a lower personal 
income level. E.g., Slonim and Roth [55] found that behavior of 
inexperienced traders is largely insensitive to the size of rewards. 
 4
However, learning in multiple iterations of a game can reveal 
differences between different reward schemes. 
2.2.2 Subject Pool 
The large majority of experiments have been undertaken with 
student populations at research institutions. In abstract settings 
students generally behave similar to subjects with professional 
experiences [4]. In experiments with rich context differences 
between subject populations are more likely. However, that does 
not mean that professional experts always achieve higher returns 
in experiments that concern their domain of expertise. Abbink and 
Rockenbach [1] found that experienced financial traders received 
lower returns compared to student subjects. They attributed this to 
the more intuitive approach by financial analysts to the game 
experiment.  
There is also a rich body of literature on gender differences 
concerning risk attitudes, ambiguity avoidance, savings behavior 
and many other topics. 
Depending on the nature of the research study self-selection of 
subjects might bias the results, e.g., if a study on social norms that 
includes a survey part is conducted. Similarly, experimenters 
should avoid raising specific expectations with their behavior or 
in the instructions. This can lead to the so-called experimenter 
effect in which subjects are mainly concerned with guessing what 
behavior might be expected from them (e.g., an experiment on 
financial market bubbles should probably avoid mentioning this 
or other terms relating to behavioral exuberance). Experimenters 
should also preserve anonymity of the subjects and secrecy of the 
payments to limit concerns of subjects regarding their privacy. 
To allow for comparison of study results reports should include a 
clear description of the larger subject pool, the recruiting 
procedures and data about the actual participants. 
It is a widely enforced standard in experimental economics to 
avoid deception of subjects. The contention is that misleading 
subjects would lead to psychological responses by the subjects 
that are undesirable and endanger the validity of experimental 
results and even has the potential to pollute the larger subject 
pool. Deception can be desirable when used as a treatment 
variable or as a proxy to influence systematically the beliefs of 
human subjects about other treatment variables. See [5] and [41] 
for a critical discussion of this standard. Note that many 
experiments involve withholding information from subjects (i.e., 
they are ‘economical with the truth’) which is often acceptable. 
However, intentionally distributing false information is met with 
suspicion by experimentalists.  
2.2.3 Complete or Incomplete Information 
An important consideration for experimental design is the amount 
of information that individuals possess about the functioning of 
the economic environment, the number and endowed 
characteristics of other players, and their own strategic choices 
and the mapping of choices to payoffs. Depending on the type of 
experiment the impact of information can be highly different. 
Some market institutions have been shown to achieve very high 
efficiency and high participation under a wide range of design 
choices, for example, the double auction market. These auction 
markets are usually conducted under limited information 
conditions where participants know only their own private 
valuations and payoffs, but market prices and market bids are 
common knowledge. In contrast, Smith et al. [59] and Peterson 
[45] have undertaken pure common value markets. In these cases, 
traders are endowed with only public and no private information 
regarding the expected common value. In these markets occurring 
trades are attributed to factors such as different risk attitudes and 
other unobservable characteristics of the traders (e.g., differences 
in individual price forecasts, accuracy of decision making due to 
experience or bounded rationality, and varying expectations 
concerning the other traders' strategies). Smith and Williams [61] 
found that providing full information about other player’s payoffs 
can stifle participation and increase market volatility. In other 
scenarios such as bargaining games the impact of incomplete 
information is often found to be stronger. 
Leaving information about important aspects of the experimental 
environments incomplete is tempting since in most real world 
scenarios including issues in computer science (e.g., such as 
routing costs) full information about all economically relevant 
variables is unrealistic. Experimenter, however, have to consider 
that subjects in absence of information players will rely on their 
personal beliefs about these characteristics which are hard to 
observe and control. 
2.2.4 Number of players and parameterization 
‘Four are few and six are many’ is one rule of thumb which 
applies to experiments on oligopoly markets [53]. The author 
showed that collusion is sustainable with four, but not with six 
firms. In the context of computer networks Friedman et al. [20] 
demonstrated that altering the group size from 2 to 8 participants 
drastically changed aggregate behavior for a cost sharing game. 
More generally, the exact number of subjects and the resulting 
individual behavior and market performance depends on the 
details of the institution. Fortunately, many experiments have 
been conducted on this topic so that interested researchers can 
orient themselves on past results to determine an appropriate 
group size for experiments that leave this design consideration 
open.  
Experimentation usually involves the choice of specific 
parameters for the economic environment. This is a very crucial 
part of experimentation. The parameters should allow for survival 
of non-equilibrium observations such as overbidding and 
underbidding on auction markets. In games with multiple 
equilibria subjects’ choice should be easily recognizable from the 
data even if individuals do not always choose perfectly. 
Experimenters should be aware of focal points in their setup 
attracting attention of subjects (if that is not intended) which 
might bias the data.  
3. EXPERIMENTAL ECONOMICS 
STUDIES AND COMPUTER SCIENCE 
My review focus is on research experiments concerning 
networked systems and electronic markets.  
The experiments presented in this section range from 
experimentation with careful manipulation of variables and 
hypotheses testing to simple performance of proof-of-concept 
studies. 
 5
3.1 Applications in Networked Systems 
There is an abundance of game-theoretic models regarding 
networked systems. With the introduction of more user control on 
the routing level (e.g., user directed routing), application level 
(e.g., file-sharing clients) and other network related issues human 
behavior increasingly has an impact on network performance. 
Behavioral treats that permeate the research literature such as 
maliciousness, fairness, free-riding, and learning motivate the 
exploration with methods of experimental economics. But not all 
scenarios avail themselves to this methodology since many 
aspects of networks’ functioning are completely opaque from 
users’ perspective. 
Below I discuss selected examples of experiments with rigorous 
methodology that explore areas of relevance for network 
researchers. For these experiments instructions and detailed 
experimental setup descriptions are included in the paper or 
available from the authors. 
3.1.1 Study of Congestion Dynamics 
The real-time experimental study reported in [18] explores 
dynamics for a simple congestion game. It also sheds light on the 
interaction of human players and automated agents in a networked 
system. Players (human or artificial) are rewarded for 
downloading complete data packets but penalized for delay due to 
congestion. Formally, they model network interaction as a game 
with the following objective function for player’s profit Π = rN – 
cL. Value r is the reward per successful download, N is the 
number of successful downloads, c is the delay cost per second, 
and L is the total latency time summed over all download attempts 
in that period. The experimental environment includes a network 
capacity constraint C determining the number of users being able 
to download at the same time without delay penalty. The delay 
algorithm is a noisy version of a single server queue model known 
in the literature as M/M/1. Subjects have asynchronous binary 
choices (to download or not) at endogenously determined times. 
The authors compute equilibrium predictions for a simple but 
intuitive player strategy and derive testable hypotheses for the 
experiment.  
Software agents employed a selfish and myopic strategy. Human 
players were made aware about the agents’ presence and 
informally notified about the approximate strategy of the agent. In 
their setting most players incur an overall loss and artificial 
players do significantly less well on average. Only in treatments 
with high capacity or large background noise agents compete 
successfully with human traders. Humans are observed to be 
slower and less able to exploit excess capacity whereas bots are 
capable of executing their strategy perfectly in those networks. 
However, agents’ failure to internalize the difference between 
observed congestion and anticipated congestion stands in contrast 
to humans who react more flexible to changes in delay. The 
authors observe that the Nash equilibrium comparative statics 
explain the data rather well. Players cannot move towards the 
social optimal outcome. Rather, the common result is 
overdissipation of rent, that is aggregate profits are lower than any 
Nash equilibrium.  
The results by Friedman and Huberman [18] contribute to the 
more general discussion that agents based on statistical analysis of 
historical market data or static strategies may be insufficient for 
the efficient operation of markets (an observation made by Miller 
[43]). For example, van Boening and Wilcox [64] discuss the 
requirement for agents to also possess forward-looking attention. 
In their paper they compare the performance of human agents and 
zero-intelligence agents [25] in a double auction market 
environment with large avoidable cost. They observe that 
efficiency and stability are undermined in markets with human 
traders, however, markets populated with zero-intelligence traders 
perform even less satisfactory. 
3.1.2 Queueing systems with endogenously chosen 
arrival times 
Closely related to Friedman and Huberman’s experiment, 
Rapoport et al. [47] study situations in which the queueing system 
is in a transient state and the arrival time of each player is 
endogenously determined modeling occasions when individuals 
can decide whether and when to join a queue. In contrast to 
Friedman and Huberman [18] individuals cannot leave a queue 
and can order service only once. This queueing game is example 
for an experiment on tacit coordination, however, in a large-scale 
economic system with direct relevance on network performance. 
The authors compute pure and symmetric mixed strategy 
equilibria. Given the experimental data they find the mixed 
strategy equilibrium predictions to map major statistics of 
aggregate behavior in the queueing game well (i.e., the relative 
frequency distribution of arrival time, the relative frequency 
distribution of interarrival time, and the relative frequency 
distribution of queue waiting time). 
Subjects participated in 75 queueing trials and had to decide 
whether they wanted to enter the queue and at what time. 
However, learning behavior across trails was low attributable to 
limited information about the individual payoff drivers. In 
particular, subjects did not know a loss was the result of a 
particular entry time chosen, too many players trying to receive 
service, or both. 
3.1.3 Cost Pricing Mechanisms in Limited 
Information Contexts 
Chen [10] conducted an experimental analysis of the serial and 
average cost-sharing game in discrete time. While in a full 
information treatment both mechanisms converge well to the 
Nash equilibrium, the experiment shows that in limited 
information contexts the serial cost sharing mechanism performs 
more robustly. Chen concludes that traditional solution concepts 
such as Nash equilibrium analysis and dominance-solvability are 
unsuitable to explain and predict these results. Chen and 
Khoroshilov [11] fit learning models to the data but find that none 
of the models explains the experimental data well for the average 
cost mechanism. However, a payoff-assessment learning model 
and experience-weighted attraction learning track the 
experimental data for the serial cost sharing mechanism 
sufficiently.  
Friedman et al. [20] study convergence behavior in Cournot 
oligopoly and serial cost sharing games in an asynchronous 
continuous-time low-information setting. For the oligopoly game 
they report that convergence improves only with an increase in 
the number of players in the game. In the serial cost sharing game 
convergence is significantly more robust, however, deteriorates 
 6
with an increase in the number of players. Friedman et al. propose 
the use of an alternative solution concept to explain these results. 
They define a strategy to be overwhelmed if the maximum payoff 
obtainable under that strategy is less than the minimum payoff 
obtainable under some other strategy [19]. They suggest that in 
limited information environments it is easier to avoid 
overwhelmed than dominated strategies (the cost-sharing game is 
solvable by the iterated deletion of overwhelmed strategies, 
whereas dominance-solvability leads to a unique prediction in the 
Cournot oligopoly game). The authors further observe that 
experimentation by subjects (i.e., periodical trying of different 
values) is common and quite methodical which does not conform 
to assumptions made in most learning models.  
3.1.4 Incentives Engineering for Structured P2P 
Systems 
For many protocols used in unstructured or structured peer-to-
peer (P2P) networks reliability and correctness of query routing 
and response depends on the cooperation of participants of the 
network. In P2P systems as well as in many related economic 
games (e.g., the Ultimatum, Dictator, Trust or Helping game) 
cooperation is predicted to fail if agents are purely self-interested. 
Schosser et al. [51] conduct an experiment with human subjects 
using protocol characteristics of the Content-Addressable-
Network (CAN) proposed by Ratnasamy et al. [48]. 
Using the strategy method to elicit complete strategy profiles of 
participants post-experimentally [52][54] they find that most 
participants use versions of simple cut-off strategies to decide 
whether to forward or respond to a query. For example, 
participants feel motivated to respond if x% of their prior queries 
were routed and responded to successfully or if they received a 
certain number of responses exceeding an absolute threshold. The 
results also indicate that subjects intend to directly reciprocate to 
behavior of other nodes.  
In treatment 1 only few participants in their experiments free-ride 
before the experimenters introduce a malicious agent (they report 
a service rate of above 70%). With the participation of the 
unresponsive agent the service rates and average payoffs drop 
across all observed experimental sessions (treatment 2). While the 
non-cooperating agent receives higher payoffs than all other 
agents his earnings are less than the payoff of agents in the 
treatment without malicious participants. 
The authors note that agents might also be motivated by relative 
payoffs compared to other participants. However, in P2P systems 
payoffs of other players are usually not directly observable. They 
do not provide for an experiment that would test this hypothesis. 
The authors show a weak positive on payoffs for the introduction 
of direct feedback between participants (treatment 3). They argue 
that the impact could likely be higher if the free-riding problem 
would be more prevalent in treatments 1 & 2. 
3.1.5 Demand for Bandwidth by End Users 
In the INDEX project (that was running from April 1998 to 
December 1999) researchers at UC Berkeley provided 70 
residents in the surrounding of the campus with integrated 
services digital network (ISDN) access [68].  
Users’ traffic was routed through a gateway that allowed for 
monitoring, billing and the implementation of an experimental 
regime. During most of the experiment users could choose 
between several different bandwidths with different charges (a 
combination of per minute and a per MB tariff). 
With a free 8-kilobit/sec service available most users did not 
choose faster service rates (75%). The remaining demand was 
approximately equally split between the other available costly 
choices. Regressions showed that about 20 percent of the variance 
in demand could be explained by price variation, about 75 percent 
by individual specific effects, leaving 5 percent unexplained. The 
data motivated the creation of an economic model of user demand 
for bandwidth. 
Researchers found that the voluntary subscribers placed a very 
low value on time (which can be attributed to the participant 
population which mostly consisted out of students with more rigid 
budgetary constraints). Also the state of the Internet of 1999 did 
not allow for more sophisticated applications limiting the demand 
for more bandwidth. Varian called this a chicken and the egg 
problem. Without more application demand will not rise and vise 
versa. 
A highly interesting subexperiment measured the willingness to 
pay by subjects to participate in a flat-fee pricing regime rather 
than the per-minute plan. In this sample 80 percent of the users 
opted for the fixed price plan. Consumers paid, on average, a 50 
percent premium for the flat fee but consumed 9 times more 
bandwidth. Ex-post 26 percent of these consumers would have 
fared better with the flexible plan. 
3.1.6 Bandwidth Trading between Internet Service 
Providers 
Research has demonstrated that ISPs are hampered in their ability 
to create efficient on-demand routes with the result that they have 
to lease or buy the maximum bandwidth they may need in the 
long run rather than reacting flexible to current demand situations 
[17]. Part of the problem is that sharing routing information is a 
cumbersome problem in protocols such as the Border Gateway 
Protocol (BGP). Another aspect is that negotiations with multiple 
network providers can lead to the exposure problem [42] (e.g., in 
sealed bid uniform price auctions for bandwidth without the 
opportunity to bid for link packages). I.e., if strong synergies exist 
between parts of a desired link combination potential bidders will 
be tempted to submit bids above the standalone value of an 
individual link in order to increase the probability of winning the 
desired set. However, bidders could be discouraged from this 
beneficial strategy by the risk of not receiving the complete 
desired set of links and incurring a loss. The opposing forces of 
synergy and exposure may result in out-of-equilibrium play and 
decrease of efficiency as shown experimentally by Kagel and 
Levin [33]. They find evidence for reluctant bidding, but also that 
bids are monotonically increasing in bidders’ valuations of the 
bundles showing that experimental subjects behaved 
economically sensibly. 
The exposure problem can be alleviated through the utilization of 
combinatorial auction mechanisms. Kaskiris et al. [34] conduct 
such a combinatorial auction experiment for bandwidth markets. 
They design a network with three links with different private 
values for buyers and sellers of bandwidth. All buyers experience 
 7
strong synergy effects between the three links, however, the exact 
distribution of valuation is private. The auction market used is 
two-sided allowing buyers and sellers to simultaneously 
participate in the market. During the market clearing process at 
the end of each round the system matches the combination of 
seller and buyer bids which generates the highest surplus. 
Uniform prices are generated for each link and each participant 
pays the combination of prices based on their successful bids.  
The authors report buyer efficiencies of at least 60% and seller 
efficiencies of about 65%, and discuss observed over- and 
underbidding behavior and learning of subjects over time. Future 
experiments are planned to explore the role of varying the number 
of market participants. Further it would be interesting to compare 
the results to an equivalent setup without buyers’ ability to submit 
bids for link bundles. This would enable the authors to quantify 
the efficiency gain that is resulting from using a combinatorial 
mechanism. 
3.2 Electronic Markets 
In the following I will discuss more research on negotiation 
agents and electronic markets. The first set of studies does not 
necessarily conform to the strict guidelines of experimental 
economics, for example, research takes more issue with proof-of-
concept considerations or simple performance testing rather than 
controlled experimentation.   But the experiments show potential 
for further more thorough exploration of important determinant of 
negotiation behavior. The remained of the section is addressing 
market behavior. 
3.2.1 Negotiations Agents 
Byde et al. [8] use a laboratory experiment to study the 
interaction between a procurement negotiation and bidding agent 
and three human bidders. The authors comment on the question 
whether it is possible to identify the agent and whether the agent 
delivers adequate performance compared to human buyers. The 
experimenters informed human bidders that a software agent was 
participating in the negotiations. According to exit interviews, 
experiment participants were not able to identify the artificial 
trader. However, the results indicate that the automated trader is 
not always successful in achieving a good balance between total 
cost and target quantity purchased when human bidders are 
present. 
Providing a more complex setting, Kobayashi and Terano [35] 
study human and artificial players in a business game with 
procurement, manufacturing and sales decisions. Six software 
agents using different strategies (random, simple prediction rules, 
and human trader strategies that were observed earlier) compete 
with four human traders. It is not possible to draw detailed 
conclusions from their reported data. They note, however, that 
one human trader dominates the population in total earnings, 
whereas the other humans perform on average worse than the 
software agents. 
Bosse et al. [6] present a system for the analysis of traces for 
multi-issue negotiations. They formalize the dynamic properties 
of different negotiation processes and implement a prototype for 
test experiments. They conduct an experiment with 74 high 
school subjects on a car sales negotiation example. Subjects use 
the system to negotiate with other anonymous human participants. 
The experiment is mainly exploratory in nature and shows the 
capabilities of the analysis system. Interestingly, none of the 
negotiations resulted in a pareto-efficient allocation or Nash 
equilibrium outcome. This highlights the difficulty of humans to 
negotiate the allocation of complex goods optimally. The 
negotiation process followed rarely a pattern of strict pareto 
monotony, however, some parts of the human negotiation traces 
obeyed weak pareto monotony. 
3.2.2 Agent Tournaments 
Tournaments present a straightforward way to compare agents’ 
performance in a fair environment. In the artificial intelligence 
community agent tournaments are conducted in an increasingly 
complex environment, see for example, the Trading Auction 
Competition (TAC) described by Wellman et al. [66][67].  In the 
2001 TAC agents arranged in groups of eight are assigned the role 
of travel agents charged with the task of arranging and 
automatically shopping for trips. The challenging part for agents’ 
design is to address the interdependence of the tasks necessary to 
complete a trip, and the ability to reason about others’ strategies 
in a thin market of automated agents and in a continuous 
timeframe. 
In experimental economics community work on programmed 
strategies has also been done by conducting tournaments 
[2][49][54]. Rust et al. [49] report on the Santa Fe Double 
Auction tournament, where researchers were invited to submit 
software agents that compete on a continuous double auction 
market against one another. The most successful strategy in this 
tournament can be described as rather parasitic sitting in the 
background and exploiting the strategies of other agents. In 
addition, they report about an evolutionary tournament, where the 
percentage of agents was adjusted in accordance to the success of 
a strategy over time. Parallel to the tournament there has been a 
discussion on the lower bound of trading agents’ intelligence to 
act similarly to human traders in a market institution 
[12][25][64][65]. 
McCabe et al. [40] reiterate an interesting anecdote from the 
Santa Fe tournament: ‘The computer programmer who was 
responsible for developing the protocols and code used to execute 
the tournament would regularly match himself (at a speed of 
course that he could handle) against a randomly chosen subset of 
the automatons submitted. After a brief learning period he would 
never lose, even against the ultimate winners. This is important 
because in a world where multitudes of complex trade 
opportunities (many of them much more complicated than the 
double auction) begin to present themselves, the ability to inject 
sophistication (potential to learn, reciprocate, punish, etc.) into 
intelligent agents will be worth millions, of dollars.’ 
3.2.3 Agents on Markets and Auctions 
Das et al. [14] conducted an experimental series where human 
traders interacted with software agents. They followed the design 
proposed by Smith [56] for two-sided market where participants 
were assigned fixed roles as either buyer (submitting only bids) or 
seller (submitting only asks) and received a private valuation 
(cost) for the traded good as a buyer (seller). In their study the 
experimental conditions of supply and demand were held constant 
over several successive trading periods and were then exposed to 
a random shock that changed market parameters. Experimental 
 8
sessions involved 6 human traders and 6 agents. In addition, a 
baseline session with 12 human traders was run. Two types of 
agents were used that applied either a modified Zero-Intelligence-
Plus strategy [12][25] or a modified Gjerstad-Dickhaut algorithm 
[23] They note in their report that bidding strategies of the 
employed agents were not discussed in detail with the human 
traders during the instructional phase. It appears, however, that 
human participants knew they competed with agents. 
In general, human-agent markets show convergence to the 
predicted equilibrium and improved efficiency compared to a 
market with human traders only [14]. Agents reaped average 
profits well above those of human traders. Between 30 and 50 
percent of the trades were done between agents and human 
traders. 
The results above have been used as a proxy for comparison with 
other agent algorithms. In [31] the authors developed algorithms 
that employ heuristic fuzzy rules and fuzzy reasoning 
mechanisms. In direct comparison to other benchmark algorithms 
(including those in [14]) they achieve superior performance. 
According to them, these results are particularly promising since 
the benchmark strategies have been shown to outperform human 
bidders (referring to the results in [14]).  
Grossklags and Schmidt [28] introduce software agents into a 
more complex and natural trading environment. This includes the 
following: a trader acts both as a buyer and seller; information 
about the fundamental value of the securities changes in every 
round; orders allow for multiple units of a specific contract; and 
the market institution does not provide a spread improvement 
rule. Additionally, many other continuous double auction 
experiments (e.g., [14]) rely on a single observation for each 
treatment. To add robustness to their results Grossklags and 
Schmidt collected six statistically independent observations for 
each treatment.  
The main contribution is the introduction of an information 
condition into a human-agent experiment. Two treatments were 
conducted with experimental parameters held constant except for 
the information available about the software agents: in one 
treatment the participation of the software agent was made 
common knowledge, and in the other treatment subjects were not 
informed about the existence of software agents. In addition, the 
data is compared to a third treatment (called baseline treatment) 
without software agents or information about the presence or 
absence of software agents. 
The authors formulate hypotheses with regards to the influence of 
software agents on human traders. Following the results of related 
work (e.g., [14]) they expect the arbitrage agent will improve 
market efficiency. The agent follows predefined rules and does 
not make mistakes with respect to its algorithm. In addition, the 
arbitrage agent can process more data in a given time span and 
interact faster with the software interface than human traders are 
able to interact with the graphical user interface. 
More importantly, the introduction of the information condition 
allows formulating a central hypothesis about the reactions that 
can be expected from human traders when information on 
software agents is provided. Human traders suffer from the 
uncertainty about the agents’ capabilities, e.g., their speed in 
calculating strategies and in processing transactions. This 
uncertainty might lead agents to crowd out humans from the 
market. It is a strong hypothesis that would require human traders 
not to trade at all when information about the existence of 
software agents is available. However, in the context of the 
double auction market institution, traders cannot observe if a 
particular trade is done with a human or a robot. Thus, an 
alternative hypothesis can be formulated according to which 
humans compare themselves with other human traders only and 
neglect the existence of software traders. This hypothesis would 
predict no difference in human behavior when information is 
provided.  
Grossklags and Schmidt [28] find that agents do not crowd out 
human traders in the treatment with common knowledge on 
software agents. Instead, common knowledge on the presence of 
software agents has a significantly positive effect on human 
traders’ ability to converge to equilibrium in the presence of the 
arbitrageur agent. Furthermore, intuition would suggest a higher 
efficiency in an environment with software agents when 
compared to no software agents. Surprisingly, when compared to 
the baseline treatment the introduction of an arbitrage seeking 
type of software agent results in lower market efficiency in the no 
information treatment. The authors discuss several interpretations 
for this surprising result. 
4. CONCLUSIONS 
I have discussed foundational principles and other important 
design considerations of experimental economics studies. Using 
computer networks and electronic markets as examples, I have 
shown that such experimentation is feasible and promising in a 
wide range of scenarios. Not all studies presented here follow the 
outlined principles closely (and certainly this was not always the 
intended goal). Some aspects of the studies could be improved 
with a different manipulation of treatment variables and more 
thorough exploration of hypotheses in advance. 
Many aspects of experimental practice in economics are worth 
considering for experimental computer scientists (and are partly 
adhered to already). First, providing enough information on 
experimental setup, instructions, source codes, and hardware is 
necessary to allow for replication of results and research that 
builds on published work. Second, making data available to other 
scientists can allow for meta-studies and secondary data analysis 
for a different research question or by different analysis methods. 
Third, drawing on theory wherever possible strengthens the 
experimental hypotheses and allows for clearer interpretation of 
results. Theory need not always be purely mathematical but can 
also be qualitative or cite behavioral regularities. Fourth, studying 
the sensitivity of results to important auxiliary assumptions is 
important and necessary. For example, does the number of 
subjects in a study matter, does experience of subjects play a 
decisive role, or does the choice of parameters favor one 
particular hypothesis? Experimenters, however, should not be 
afraid of results falling apart under more scrutiny, but rather 
recognize that inconsistencies are a stepping stone on the path to a 
more nuanced understanding of the research question. 
The relationship between experimental economics and computer 
science is not a one-way street. In fact, there are already examples 
of successful cross-fertilization. Interdisciplinary teams such as in 
Friedman et al. [20] can make tremendous contributions to both 
fields. Electronic market design is another field in which 
 9
cooperation has already carried fruits. For example, Brewer and 
Plott [7] use a combination of operations research methods and 
experimental economics (they call it a laboratory test-bed 
methodology) to design and validate a cost-minimizing back-haul 
market.  
Of course, experimental economics is just one helpful addition to 
the experimental computer science toolkit. And it is fair to add 
that experimental economics studies often require a substantial 
amount of preparation, such as review by the local internal review 
board for human subject studies, a laboratory that adheres to 
accepted standards, recruitment of subjects, and grant writing to 
receive funding for experiments. Further, attending experimental 
sessions takes time. 
Nevertheless, it is worth it. Researchers in computer science 
should seize the opportunity to bring new and exciting research to 
bear on how technologies and protocols perform under human 
scrutiny and in absence of too many assumptions about agents’ 
behavior. 
5. ACKNOWLEDGMENTS 
I would like to thank the anonymous reviewers and program 
chairs for their comments and editorial guidance. Paul Laskowski, 
Nicolas Christin, and John Chuang greatly improved this 
manuscript with their enormously helpful feedback. This work is 
supported in part by the National Science Foundation under ITR 
award ANI-0331659 (the 100x100 Clean-Slate Design Project). 
6. REFERENCES 
[1] Abbink, K., and Rockenbach, B. Option pricing by students 
and professional traders: a behavioural investigation. 
Managerial and Decision Economics, vol. 27, no. 6 (Sep. 
2006), 497–510. 
[2] Abreu, D., and Rubinstein, A. The Structure of Nash 
Equilibrium in Repeated Games with Finite Automata. 
Econometrica, vol. 56, no. 6 (Nov. 1988), 1259–1281. 
[3] Archer, A., Feigenbaum, J., Krishnamurthy, A., Sami, R., 
and Shenker, S. Approximation and collusion in multicast 
cost sharing. Games and Economic Behavior, vol. 47, no. 1 
(Apr. 2004), 36–71. 
[4] Ball, S., and Cech, P. Subject pool choice and treatment 
effects in economic laboratory research. In Isaac, R.M. (Ed.) 
Research in Experimental Economics, vol. 6, JAI Press, 
Greenwich, CT, 1996, 239–292. 
[5] Bonetti, S. Experimental economics and deception. Journal 
of Economic Psychology, vol. 19, no. 3 (June 1998), 377–
395. 
[6] Bosse, T., Jonker, C.M., and Treur, J. Experiments in Human 
Multi-Issue Negotiation: Analysis and Support. In 
Proceedings of the Third International Conference on 
Autonomous Agents and Multiagent Systems (AAMAS'04) 
(New York, NY, July 19–23, 2004). IEEE Computer 
Society, Washington, DC, 671–678. 
[7] Brewer, P.J., and Plott, C.R. A Decentralized, Smart Market 
Solution to a Class of Back-Haul Transportation Problems: 
Concept and Experimental Test Beds. Interfaces, vol. 32, no. 
5 (Sep.-Oct. 2002), 13–36. 
[8] Byde, A., Yearworth, M., Chen, K., and Bertolino, C. 
AutONA: A System for Automated Multiple 1-1 
Negotiation. In Proceedings of the IEEE International 
Conference on E-Commerce (CEC’03) (Newport Beach, CA, 
June 24–27, 2003). IEEE Computer Society, Los Alamitos, 
CA, 59–67. 
[9] Camerer, C.F. Behavioral Game Theory: Experiments in 
Strategic Interaction, Princeton University Press, Princeton, 
PA, 2003. 
[10] Chen, Y. An Experimental Study of the Serial and Average 
Cost Pricing Mechanisms. Journal of Public Economics, vol. 
87, no. 9–10 (Sep. 2003), 2305–2335. 
[11] Chen, Y., and Khoroshilov, Y. Learning under Limited 
Information. Games and Economic Behavior, vol. 44, no. 1 
(July 2003), 1–25. 
[12] Cliff, D., and Bruten, J. Minimal-intelligence agents for 
bargaining behaviors in market-based environments. HP 
Labs Technical Reports HPL 97–91, Hewlett-Packard 
Laboratories, Bristol, UK, Aug. 1997. 
[13] Christin, N., Grossklags, J., and Chuang, J. Near Rationality 
and Competitive Equilibria in Networked Systems. In 
Proceedings of ACM SIGCOMM'04 Workshop on Practice 
and Theory of Incentives in Networked Systems (PINS’04) 
(Portland, OR, September 3, 2004). ACM Press, New York, 
NY, 213–219. 
[14] Das, R., Hanson, J.E., Kephart, J.O., and Tesauro, G. Agent-
Human Interactions in the Continuous Double Auction. In 
Proceedings of the International Joint Conference on 
Artificial Intelligence (IJCAI 2001) (Seattle, WA, August 4-
10, 2001). Morgan Kaufmann Publishers, San Francisco, 
CA, 1169–1187. 
[15] Denning, P.J. Performance analysis: experimental computer 
science as its best. Comm. ACM, vol. 24, no. 11 (Nov. 1981), 
725–727. 
[16] Duffy, J. Agent-based models and human-subject 
experiments. In Judd, K.L., and Tesfatsion, L. (Eds.) 
Handbook of Computational Economics, Volume 2, Elsevier, 
Amsterdam, The Netherlands, 2006, 949–1011. 
[17] Ferreira, P., Mindel, J., and McKnight, L. Why have 
bandwidth trading markets not matured? Analysis of 
technological and market issues. International Journal of 
Technology, Policy and Management, vol. 3, no.2 (2003), 
142–160. 
[18] Friedman, D., and Huberman, B. Internet Congestion: A 
Laboratory Experiment. In Rapoport, A., and Zwick, R. 
(Eds.) Experimental Business Research, Economic and 
Managerial Perspectives ,Volume II, Springer, New York, 
NY, 2005, 83–102. 
[19] Friedman, E., and Shenker, S. Synchronous and 
asynchronous learning by responsive learning automata. 
Mimeo, Department of Economics, Rutgers University, 
1996. 
[20] Friedman, E., Shor, M., Shenker, S., and Sopher, B. An 
experiment on learning with limited information: 
nonconvergence, experimentation cascades, and the 
 10
advantage of being slow. Games and Economic Behavior, 
vol. 47, no. 2 (May 2004), 325–352. 
[21] Friedman, D., and Sunder, S. Experimental Economics: A 
Primer for Economists. Cambridge University Press, 
Cambridge, UK, 1994. 
[22] Gigerenzer, G., and Selten, R. Bounded Rationality: The 
Adaptive Toolbox. The MIT Press, Cambridge, MA. 
[23] Gjerstad, S., and Dickhaut, J. Price Formation in Double 
Auctions. Games and Economic Behavior, vol. 22, no. 1 
(Jan. 1998), 1–29. 
[24] Glaeser, E.L., Laibson, D.I., Scheinkman, J.A., and Soutter, 
C.L. Measuring trust. The Quarterly Journal of Economics, 
vol. 115, no. 3 (Aug. 2000), 811–846. 
[25] Gode, D.K., and Sunder, S. Allocative Efficiency of Markets 
with Zero-Intelligence Traders: Market as a Partial Substitute 
for Individual Rationality. J. Polit. Economy, vol. 101, no. 1 
(Feb. 1993), 119–137. 
[26] Good, N.S., Grossklags, J., Mulligan, D., and Konstan, J. 
Noticing Notice: A large-scale experiment on the timing of 
software license agreements. In Proceedings of the ACM 
Conference on Human Factors in Computing Systems 
(CHI'07) (San Jose, CA, April 28–May 3, 2007). ACM 
Press, New York, NY, 607–616. 
[27] Grossklags, J., and Acquisti, A. When 25 cents is too much: 
An Experiment on Willingness-To-Sell and Willingness-To-
Protect Personal Information. In Proceedings of the Sixth 
Workshop on the Economics of Information Security (WEIS 
2007) (Pittsburgh, PA, June 7–8, 2007). 
[28] Grossklags, J., and Schmidt, C. Software Agents and Market 
(In)Efficiency - A Human Trader Experiment. IEEE 
Transactions on System, Man, and Cybernetics: Part C, 
Special Issue on Game-theoretic Analysis & Simulation of 
Negotiation Agents, vol. 36, no. 1 (Jan. 2006), 56–67. 
[29] Guttman, R.H., Moukas, A.G., and Maes, P. Agent-mediated 
Electronic Commerce: A Survey. Knowl. Eng. Rev., vol. 13, 
no. 2 (Jul. 1998), 147–159. 
[30] Guyot, P., Drogoul, A. and Honiden, S. Power and 
Negotiation: Lessons from Agent-Based Participatory 
Simulations. In Proceedings of the Fifth International Joint 
Conference on Autonomous Agents and Multiagent Systems 
(AAMAS’06) (Hakodate, Hokkaido, Japan, May 8–12, 2006). 
ACM Press, New York, NY, 27–33. 
[31] He, M., Leung, H., and Jennings, N.R. A fuzzy logic based 
bidding strategy for autonomous agents in continuous double 
auctions. IEEE Trans. Knowledge Data Eng., vol. 15, no. 6 
(Nov. –Dec. 2003), 1345–1363. 
[32] Jamal, K., and Sunder, S. Money vs. Gaming: Effects of 
Salient Monetary Payments in Double Oral Auctions. 
Organizational Behavior and Human Decision Processes, 
vol. 49 (June 1991), 151–166. 
[33] Kagel, J.H., and Levin, D. Multi-Unit Demand Auctions with 
Synergies: Behavior in Sealed-Bid versus Ascending-Bid 
Uniform-Price Auctions. Games and Economic Behavior, 
vol. 53, no. 2 (Nov. 2005), 170–207. 
[34] Kaskiris, C., Jain, R. Rajagopal, R., and Varaiya, P.  
Combinatorial Auction Bandwidth Trading: An 
Experimental Study. In Developments in Experimental 
Economics: New Approaches to Solving Real-World 
Problems, Lecture Notes in Economics and Mathematical 
Systems, Springer, New York, NY, 2007. 
[35] Kobayashi, M., and Terano, T. Human-Agent Participation 
in a Business Simulator,” in Terano, T., Deguchi, H. and 
Takadama, K. (Eds.) Meeting the Challenge of Social 
Problems via Agent-Based Simulation, Springer, Tokyo, 
Japan, 2003, 91–106. 
[36] Kraut, R.E., Sunder, S., Telang, R., and Morris, J. Pricing 
Electronic Mail to Solve the Problem of Spam. Human-
Computer Interaction, vol. 20, no. 1–2 (2005), 195–223.  
[37] Kuhn, T.S. The Structure of Scientific Revolutions. 
University of Chicago Press, Chicago, IL, 1962. 
[38] Lee, U., Choi, M., Cho, J., Sanadidi, M.Y., and Gerla, M. 
Understanding Pollution Dynamics in P2P File Sharing. In 
Proceedings of the 5th International Workshop on Peer-to-
Peer Systems (IPTPS'06) (Santa Barbara, CA, February 27–
28, 2006). 
[39] Loder, T., van Alstyne, M., and Wash, R. An Economic 
Solution to Unsolicited Communications. Advances in 
Economic Analysis & Policy, vol. 6, no. 1, Article 2 (Mar. 
2006). 
[40] McCabe, K., Rassenti, S., and Smith, V.L. Designing auction 
institutions for exchange. IIE Transactions, vol. 31 (1999), 
803–811. 
[41] McDaniel, T., and Starmer, C. Experimental economics and 
deception: A comment. Journal of Economic Psychology, 
vol. 19, no. 3 (June 1998), 403–409. 
[42] Milgrom, P. Putting Auction Theory to Work (Churchill 
Lectures in Economics). Cambridge University Press, 
Cambridge, UK, 2004.  
[43] Miller, R.M. Don’t let your robots grow up to be traders: 
Artificial Intelligence, Human Intelligence, and Asset-Market 
Bubbles. Economics Working Paper No. 0306001, 
Economics Working Paper Archive at WUSTL, Apr. 2003. 
[44] National Research Council, Committee on Academic Careers 
for Experimental Computer Scientists. Academic Careers for 
Experimental Computer Scientists and Engineers. 1994. 
[45] Peterson, S.P. Forecasting Dynamics and Convergence to 
Market Fundamentals. Evidence from Experimental Asset 
Markets. Journal of Economic Behavior & Organization, 
vol. 22, no. 3 (Dec. 1993), 269–284. 
[46] Porter, D. and Wessen, R. A market-based approach for 
manifesting shuttle secondary payloads. Journal of 
Spacecraft and Rockets, vol. 36, no. 11 (Nov. 1998), 142–
147. 
[47] Rapoport, A., Stein, W.E., Parco, J.E., and Seale, D.A. 
Equilibrium play in single-server queues with endogenously 
determined arrival times. Journal of Economic Behavior & 
Organization, vol. 55, no. 1 (Sep. 2004) 67–91. 
[48] Ratnasamy, S., Francis, P., Handley, M., Karp, R., and 
Shenker, S. A Scalable Content-Addressable Network. In 
Proceedings of the 2001 conference on Applications, 
technologies, architectures, and protocols for computer 
 11
communication (SIGCOMM 2001) (San Diego, CA, Aug 27–
31, 2001). ACM Press, New York, NY, 161–172. 
[49] Rust, J., Miller, J.H., and Palmer, R. Characterizing effective 
trading strategies. Insights from a computerized double 
auction tournament. J.  Econ. Dynam. Control, vol. 18, no. 1 
(Jan. 1994) pp. 61–96. 
[50] Schechter, S., Dhamija, R., Ozment, A., and Fischer, I. The 
Emperor's New Security Indicators. In Proceedings of the 
IEEE Symposium on Security and Privacy (S&P 2007) 
(Berkeley/Oakland, CA, May 20-23, 2007). IEEE Computer 
Society. 
[51] Schosser, S., Böhm, K., Schmidt, R., and Vogt, B. Incentives 
engineering for structured P2P systems - a feasibility 
demonstration using economic experiments. In Proceedings 
of the 7th ACM Conference on Electronic Commerce 
(EC’06) (Ann Arbor, MI, June 11–15 2006). ACM Press, 
New York, NY, 280–289. 
[52] Selten, R. Die Strategiemethode zur Erforschung des 
eingeschränkt rationalen Verhaltens im Rahmen eine 
Oligopolexperiments. In Sauermann, H. (Ed.) Beiträge zur 
experimentellen Wirtschaftsforschung, J.C.B. Mohr (Paul 
Siebeck), Tübingen, Germany, 136–168, 1967. 
[53] Selten, R. A simple model of imperfect competition, where 4 
are few and 6 are many. International Journal of Game 
Theory, vol. 2, no. 1 (Dec. 1973), 141–201. 
[54] Selten, R., Mitzkewitz, M., and Uhlich, G.R. Duopoly 
Strategies Programmed by Experienced Players. 
Econometrica, vol. 65, no. 3 (May 1997), 517–555. 
[55] Slonim, R., and Roth, A. Learning in High Stakes Ultimatum 
Games: An Experiment in the Slovak Republic. 
Econometrica, vol. 66, no. 3 (May 1998), 569–596. 
[56] Smith, V.L. An Experimental Study of Competitive Market 
Behavior. J. Polit. Economy, vol. 70, no.2 (Apr. 1962), 111–
137. 
[57] Smith, V.L. Economics in the Laboratory. Journal of 
Economic Perspectives, vol. 8, no. 1 (Winter 1994), 113–
131. 
[58] Smith, V.L. Experimental Economics: Induced Value 
Theory. American Economic Review, vol. 66, no. 2, Papers 
and Proceedings of the Eighty-eight Annual Meeting of the 
American Economic Association (May 1976), 274–279.  
[59] Smith, V.L., Suchanek, G.L. and Williams, A.W. Bubbles, 
Crashes, and Endogenous Expectations in Experimental Spot 
Asset Markets. Econometrica, vol. 56, no. 5 (Sep. 1988), 
1119–1151. 
[60] Smith, V.L., and Walker, J.M. Monetary Rewards and 
Decision Cost in Experimental Economics. Economic 
Inquiry, vol. 31, no. 2 (Apr. 1993), 245–261. 
[61] Smith, V.L., and Williams, A.W. Experimental Market 
Economics. Scientific American (Dec. 1992). 
[62] Spiekermann, S., Grossklags, J., and Berendt, B. E-privacy 
in 2nd generation E-Commerce: privacy preferences versus 
actual behavior. In Proceedings of the Third ACM 
Conference on Electronic Commerce (ACM EC'01) (Tampa, 
FL, October 14-17, 2001). ACM Press, New York, NY, 38–
47. 
[63] Terano, T., Shiozawa, Y., Deguchi, H. Kita, H. Matsui, H. 
Sato, H. Ono, I., and Nakajima, Y. U-Mart: An Artificial 
Market Testbed for Economics and Multiagent Systems. In 
Terano, T., Deguchi, H., and Takadama, K. (Eds.) Meeting 
the Challenge of Social Problems via Agent-Based 
Simulation, Springer, Tokyo, Japan, 2003, 53–65. 
[64] van Boening, M.V., and Wilcox, N.T. Avoidable cost: Ride a 
Double Auction Roller Coaster. American Economic Review, 
vol. 86, no. 3 (June 1996) 461–477. 
[65] Walia, V., Byde, A., and Cliff, D. Evolving Market-Design 
In Zero-Intelligence Trader Markets. In Proceedings of the 
IEEE International Conference on E-Commerce (CEC’03) 
(Newport Beach, CA, June 24–27, 2003). IEEE Computer 
Society, Los Alamitos, CA, 157–164. 
[66] Wellman, M.P., Greenwald, A., Stone, P. and Wurman, P.R. 
The 2001 Trading Agent Competition. In Proceedings of the 
Fourteenth Annual Conference on Innovative Applications of 
Artificial Intelligence (IAAI’02) (Edmonton, Canada, July 28 
– August 1 2002). AAAI Press/MIT Press, Providence, RI, 
935–941. 
[67] Wellman, M.P., Wurman, P.R., O’Malley, K., Bangera, R., 
Lin, S., Reeves, D., and Walsh, W.E. Designing the market 
game for a trading agent competition. IEEE Internet 
Computing, vol. 5, no. 2 (Mar. 2001), 43–51. 
[68] Varian, H.R. The Demand for Bandwidth: Evidence from the 
INDEX Project. In Crandall, R.W., and Alleman, J.H. (Eds.) 
Broadband: Should We Regulate High-Speed Internet 
Access? Aei-Brookings Joint Center for Regulatory Studies, 
American Enterprise Institute Press, 2003, 39–56. 
 
 
 

