Bootstrap Methods and their
Application
Anthony Davison
c©2006
A short course based on the book
‘Bootstrap Methods and their Application’,
by A. C. Davison and D. V. Hinkley
c©Cambridge University Press, 1997
Anthony Davison: Bootstrap Methods and their Application, 1
Summary
◮ Bootstrap: simulation methods for frequentist inference.
◮ Useful when
• standard assumptions invalid (n small, data not normal,
. . .);
• standard problem has non-standard twist;
• complex problem has no (reliable) theory;
• or (almost) anywhere else.
◮ Aim to describe
• basic ideas;
• confidence intervals;
• tests;
• some approaches for regression
Anthony Davison: Bootstrap Methods and their Application, 2
Motivation
 Motivation
 Basic notions
 Confidence intervals
 Several samples
 Variance estimation
 Tests
 Regression
Anthony Davison: Bootstrap Methods and their Application, 3
Motivation
AIDS data
◮ UK AIDS diagnoses 1988–1992.
◮ Reporting delay up to several years!
◮ Problem: predict state of epidemic at end 1992, with
realistic statement of uncertainty.
◮ Simple model: number of reports in row j and column k
Poisson, mean
µjk = exp(αj + βk).
◮ Unreported diagnoses in period j Poisson, mean
∑
k unobs
µjk = exp(αj)
∑
k unobs
exp(βk).
◮ Estimate total unreported diagnoses from
period j by replacing αj and βk by MLEs.
• How reliable are these predictions?
• How sensible is the Poisson model?
Anthony Davison: Bootstrap Methods and their Application, 4
Motivation
Diagnosis Reporting-delay interval (quarters): Total
period reports
to end
Year Quarter 0† 1 2 3 4 5 6 · · · ≥14 of 1992
1988 1 31 80 16 9 3 2 8 · · · 6 174
2 26 99 27 9 8 11 3 · · · 3 211
3 31 95 35 13 18 4 6 · · · 3 224
4 36 77 20 26 11 3 8 · · · 2 205
1989 1 32 92 32 10 12 19 12 · · · 2 224
2 15 92 14 27 22 21 12 · · · 1 219
3 34 104 29 31 18 8 6 · · · 253
4 38 101 34 18 9 15 6 · · · 233
1990 1 31 124 47 24 11 15 8 · · · 281
2 32 132 36 10 9 7 6 · · · 245
3 49 107 51 17 15 8 9 · · · 260
4 44 153 41 16 11 6 5 · · · 285
1991 1 41 137 29 33 7 11 6 · · · 271
2 56 124 39 14 12 7 10 263
3 53 175 35 17 13 11 306
4 63 135 24 23 12 258
1992 1 71 161 48 25 310
2 95 178 39 318
3 76 181 273
4 67 133
Anthony Davison: Bootstrap Methods and their Application, 5
Motivation
AIDS data
◮ Data (+), fits of simple model (solid), complex model
(dots)
◮ Variance formulae could be derived — painful! but useful?
◮ Effects of overdispersion, complex model, . . .?
++++
++
++
++
+
+++
++
+
++
+++
++
+
+
+
+
+
+++
+
+
++
+
+
Time
D
ia
gn
os
es
1984 1986 1988 1990 1992
0
10
0
20
0
30
0
40
0
50
0
Anthony Davison: Bootstrap Methods and their Application, 6
Motivation
Goal
Find reliable assessment of uncertainty when
◮ estimator complex
◮ data complex
◮ sample size small
◮ model non-standard
Anthony Davison: Bootstrap Methods and their Application, 7
Basic notions
 Motivation
 Basic notions
 Confidence intervals
 Several samples
 Variance estimation
 Tests
 Regression
Anthony Davison: Bootstrap Methods and their Application, 8
Basic notions
Handedness data
Table: Data from a study of handedness; hand is an integer measure
of handedness, and dnan a genetic measure. Data due to Dr Gordon
Claridge, University of Oxford.
dnan hand dnan hand dnan hand dnan hand
1 13 1 11 28 1 21 29 2 31 31 1
2 18 1 12 28 2 22 29 1 32 31 2
3 20 3 13 28 1 23 29 1 33 33 6
4 21 1 14 28 4 24 30 1 34 33 1
5 21 1 15 28 1 25 30 1 35 34 1
6 24 1 16 28 1 26 30 2 36 41 4
7 24 1 17 29 1 27 30 1 37 44 8
8 27 1 18 29 1 28 31 1
9 28 1 19 29 1 29 31 1
10 28 2 20 29 2 30 31 1
Anthony Davison: Bootstrap Methods and their Application, 9
Basic notions
Handedness data
Figure: Scatter plot of handedness data. The numbers show the mul-
tiplicities of the observations.
15 20 25 30 35 40 45
1
2
3
4
5
6
7
8
dnan
ha
nd
1 1
1
2 2 1 5
2
1
5
2
3
1
4
1
1
1 1
1
1
Anthony Davison: Bootstrap Methods and their Application, 10
Basic notions
Handedness data
◮ Is there dependence between dnan and hand for these
n = 37 individuals?
◮ Sample product-moment correlation coefficient is θ̂ = 0.509.
◮ Standard confidence interval (based on bivariate normal
population) gives 95% CI (0.221, 0.715).
◮ Data not bivariate normal!
◮ What is the status of the interval? Can we do better?
Anthony Davison: Bootstrap Methods and their Application, 11
Basic notions
Frequentist inference
◮ Estimator θ̂ for unknown parameter θ.
◮ Statistical model: data y1, . . . , yn
iid
∼ F , unknown
◮ Handedness data
• y = (dnan, hand)
• F puts probability mass on subset of R2
• θ̂ is correlation coefficient
◮ Key issue: what is variability of θ̂ when samples are
repeatedly taken from F?
◮ Imagine F known — could answer question by
• analytical (mathematical) calculation
• simulation
Anthony Davison: Bootstrap Methods and their Application, 12
Basic notions
Simulation with F known
◮ For r = 1, . . . , R:
• generate random sample y∗1 , . . . , y
∗
n
iid
∼ F ;
• compute θ̂r using y
∗
1 , . . . , y
∗
n;
◮ Output after R iterations:
θ̂∗1, θ̂
∗
2, . . . , θ̂
∗
R
◮ Use θ̂∗1, θ̂
∗
2, . . . , θ̂
∗
R to estimate sampling distribution of θ̂
(histogram, density estimate, . . .)
◮ If R→ ∞, then get perfect match to theoretical calculation
(if available): Monte Carlo error disappears completely
◮ In practice R is finite, so some error remains
Anthony Davison: Bootstrap Methods and their Application, 13
Basic notions
Handedness data: Fitted bivariate normal model
Figure: Contours of bivariate normal distribution fitted to handedness data;
parameter estimates are bµ1 = 28.5, bµ2 = 1.7, bσ1 = 5.4, bσ2 = 1.5, bρ = 0.509.
The data are also shown.
0.000
0.005
0.010
0.015
0.020
10 15 20 25 30 35 40 45
0
2
4
6
8
10
dnan
ha
nd
1 1
1
2 2 1 5
2
1
5
2
3
1
4
1
1
1 1
1
1
Anthony Davison: Bootstrap Methods and their Application, 14
Basic notions
Handedness data: Parametric bootstrap samples
Figure: Left: original data, with jittered vertical values. Centre and
right: two samples generated from the fitted bivariate normal distribu-
tion.
0 10 20 30 40 50
0
2
4
6
8
10
dnan
ha
nd
Correlation 0.509
0 10 20 30 40 50
0
2
4
6
8
10
dnan
ha
nd
Correlation 0.753
0 10 20 30 40 50
0
2
4
6
8
10
dnan
ha
nd
Correlation 0.533
Anthony Davison: Bootstrap Methods and their Application, 15
Basic notions
Handedness data: Correlation coefficient
Figure: Bootstrap distributions with R = 10000. Left: simulation from
fitted bivariate normal distribution. Right: simulation from the data by
bootstrap resampling. The lines show the theoretical probability density
function of the correlation coefficient under sampling from a fitted bivariate
normal distribution.
Correlation coefficient
P
ro
ba
bi
lit
y 
de
ns
ity
−0.5 0.0 0.5 1.0
0.
0
0.
5
1.
0
1.
5
2.
0
2.
5
3.
0
3.
5
Correlation coefficient
P
ro
ba
bi
lit
y 
de
ns
ity
−0.5 0.0 0.5 1.0
0.
0
0.
5
1.
0
1.
5
2.
0
2.
5
3.
0
3.
5
Anthony Davison: Bootstrap Methods and their Application, 16
Basic notions
F unknown
◮ Replace unknown F by estimate F̂ obtained
• parametrically — e.g. maximum likelihood or robust fit of
distribution F (y) = F (y;ψ) (normal, exponential, bivariate
normal, . . .)
• nonparametrically — using empirical distribution function
(EDF) of original data y1, . . . , yn, which puts mass 1/n on
each of the yj
◮ Algorithm: For r = 1, . . . , R:
• generate random sample y∗1 , . . . , y
∗
n
iid
∼ F̂ ;
• compute θ̂r using y
∗
1 , . . . , y
∗
n;
◮ Output after R iterations:
θ̂∗1, θ̂
∗
2, . . . , θ̂
∗
R
Anthony Davison: Bootstrap Methods and their Application, 17
Basic notions
Nonparametric bootstrap
◮ Bootstrap (re)sample y∗1, . . . , y
∗
n
iid
∼ F̂ , where F̂ is EDF of
y1, . . . , yn
• Repetitions will occur!
◮ Compute bootstrap θ̂∗ using y∗1, . . . , y
∗
n.
◮ For handedness data take n = 37 pairs y∗ = (dnan, hand)∗
with equal probabilities 1/37 and replacement from original
pairs (dnan, hand)
◮ Repeat this R times, to get θ̂∗1, . . . , θ̂
∗
R
◮ See picture
◮ Results quite different from parametric simulation — why?
Anthony Davison: Bootstrap Methods and their Application, 18
Basic notions
Handedness data: Bootstrap samples
Figure: Left: original data, with jittered vertical values. Centre and
right: two bootstrap samples, with jittered vertical values.
10 15 20 25 30 35 40 45
1
2
3
4
5
6
7
8
dnan
ha
nd
Correlation 0.509
10 15 20 25 30 35 40 45
1
2
3
4
5
6
7
8
dnan
ha
nd
Correlation 0.733
10 15 20 25 30 35 40 45
1
2
3
4
5
6
7
8
dnan
ha
nd
Correlation 0.491
Anthony Davison: Bootstrap Methods and their Application, 19
Basic notions
Using the θ̂∗
◮ Bootstrap replicates θ̂∗r used to estimate properties of θ̂.
◮ Write θ = θ(F ) to emphasize dependence on F
◮ Bias of θ̂ as estimator of θ is
β(F ) = E(θ̂ | y1, . . . , yn
iid
∼ F ) − θ(F )
estimated by replacing unknown F by known estimate F̂ :
β(F̂ ) = E(θ̂ | y1, . . . , yn
iid
∼ F̂ ) − θ(F̂ )
◮ Replace theoretical expectation E() by empirical average:
β(F̂ ) ≈ b = θ̂∗ − θ̂ = R−1
R∑
r=1
θ̂∗r − θ̂
Anthony Davison: Bootstrap Methods and their Application, 20
Basic notions
◮ Estimate variance ν(F ) = var(θ̂ | F ) by
v =
1
R− 1
R∑
r=1
(
θ̂∗r − θ̂
∗
)2
◮ Estimate quantiles of θ̂ by taking empirical quantiles of
θ̂∗1, . . . , θ̂
∗
R
◮ For handedness data, 10,000 replicates shown earlier give
b = −0.046, v = 0.043 = 0.2052
Anthony Davison: Bootstrap Methods and their Application, 21
Basic notions
Handedness data
Figure: Summaries of the bθ∗. Left: histogram, with vertical line showing bθ.
Right: normal Q–Q plot of bθ∗.
Histogram of t
t*
D
en
si
ty
−0.5 0.0 0.5 1.0
0.
0
0.
5
1.
0
1.
5
2.
0
2.
5
−4 −2 0 2 4
−
0.
5
0.
0
0.
5
Quantiles of Standard Normal
t*
Anthony Davison: Bootstrap Methods and their Application, 22
Basic notions
How many bootstraps?
◮ Must estimate moments and quantiles of θ̂ and derived
quantities. Nowadays often feasible to take R ≥ 5000
◮ Need R ≥ 100 to estimate bias, variance, etc.
◮ Need R≫ 100, prefer R ≥ 1000 to estimate quantiles
needed for 95% confidence intervals
−4 −2 0 2 4
−
4
−
2
0
2
4
R=199
Theoretical Quantiles
Z
*
−4 −2 0 2 4
−
4
−
2
0
2
4
R=999
Theoretical Quantiles
Z
*
Anthony Davison: Bootstrap Methods and their Application, 23
Basic notions
Key points
◮ Estimator is algorithm
• applied to original data y1, . . . , yn gives original θ̂
• applied to simulated data y∗1 , . . . , y
∗
n gives θ̂
∗
• θ̂ can be of (almost) any complexity
• for more sophisticated ideas (later) to work, θ̂ must often be
smooth function of data
◮ Sample is used to estimate F
• F̂ ≈ F — heroic assumption
◮ Simulation replaces theoretical calculation
• removes need for mathematical skill
• does not remove need for thought
• check code very carefully — garbage in, garbage out!
◮ Two sources of error
• statistical (F̂ 6= F ) — reduce by thought
• simulation (R 6= ∞) — reduce by taking R large (enough)
Anthony Davison: Bootstrap Methods and their Application, 24
Confidence intervals
 Motivation
 Basic notions
 Confidence intervals
 Several samples
 Variance estimation
 Tests
 Regression
Anthony Davison: Bootstrap Methods and their Application, 25
Confidence intervals
Normal confidence intervals
◮ If θ̂ approximately normal, then θ̂
.
∼ N(θ + β, ν), where θ̂
has bias β = β(F ) and variance ν = ν(F )
◮ If β, ν known, (1 − 2α) confidence interval for θ would be
(D1)
θ̂ − β ± zαν
1/2,
where Φ(zα) = α.
◮ Replace β, ν by estimates:
β(F )
.
= β(F̂ )
.
= b = θ̂∗ − θ̂
ν(F )
.
= ν(F̂ )
.
= v = (R − 1)−1
∑
r
(θ̂∗r − θ̂
∗)2,
giving (1 − 2α) interval θ̂ − b± zαv
1/2.
◮ Handedness data: R = 10, 000, b = −0.046, v = 0.2052,
α = 0.025, zα = −1.96, so 95% CI is (0.147, 0.963)
Anthony Davison: Bootstrap Methods and their Application, 26
Confidence intervals
Normal confidence intervals
◮ Normal approximation reliable? Transformation needed?
◮ Here are plots for ψ̂ = 12 log{(1 + θ̂)/(1 − θ̂)}:
Transformed correlation coefficient
D
en
si
ty
−0.5 0.0 0.5 1.0 1.5
0.
0
0.
5
1.
0
1.
5
−4 −2 0 2 4
−
0.
5
0.
0
0.
5
1.
0
1.
5
Quantiles of Standard Normal
T
ra
ns
fo
rm
ed
 c
or
re
la
tio
n 
co
ef
fic
ie
nt
Anthony Davison: Bootstrap Methods and their Application, 27
Confidence intervals
Normal confidence intervals
◮ Correlation coefficient: try Fisher’s z transformation:
ψ̂ = ψ(θ̂) = 12 log{(1 + θ̂)/(1 − θ̂)}
for which compute
bψ = R
−1
R∑
r=1
ψ̂∗r − ψ̂, vψ =
1
R− 1
R∑
r=1
(
ψ̂∗r − ψ̂
∗
)2
,
◮ (1 − 2α) confidence interval for θ is
ψ−1
{
ψ̂ − bψ − z1−αv
1/2
ψ
}
, ψ−1
{
ψ̂ − bψ − zαv
1/2
ψ
}
◮ For handedness data, get (0.074, 0.804)
◮ But how do we choose a transformation in general?
Anthony Davison: Bootstrap Methods and their Application, 28
Confidence intervals
Pivots
◮ Hope properties of θ̂∗1, . . . , θ̂
∗
R mimic effect of sampling from
original model.
◮ Amounts to faith in ‘substitution principle’: may replace
unknown F with known F̂ — false in general, but often
more nearly true for pivots.
◮ Pivot is combination of data and parameter whose
distribution is independent of underlying model.
◮ Canonical example: Y1, . . . , Yn
iid
∼ N(µ, σ2). Then
Z =
Y − µ
(S2/n)1/2
∼ tn−1,
for all µ, σ2 — so independent of the underlying
distribution, provided this is normal
◮ Exact pivot generally unavailable in nonparametric case.
Anthony Davison: Bootstrap Methods and their Application, 29
Confidence intervals
Studentized statistic
◮ Idea: generalize Student t statistic to bootstrap setting
◮ Requires variance V for θ̂ computed from y1, . . . , yn
◮ Analogue of Student t statistic:
Z =
θ̂ − θ
V 1/2
◮ If the quantiles zα of Z known, then
Pr (zα ≤ Z ≤ z1−α) = Pr
(
zα ≤
θ̂ − θ
V 1/2
≤ z1−α
)
= 1 − 2α
(zα no longer denotes a normal quantile!) implies that
Pr
(
θ̂ − V 1/2z1−α ≤ θ ≤ θ̂ − V
1/2zα
)
= 1 − 2α
so (1− 2α) confidence interval is (θ̂− V 1/2z1−α, θ̂− V
1/2zα)
Anthony Davison: Bootstrap Methods and their Application, 30
Confidence intervals
◮ Bootstrap sample gives (θ̂∗, V ∗) and hence
Z∗ =
θ̂∗ − θ̂
V ∗1/2
◮ R bootstrap copies of (θ̂, V ):
(θ̂∗1, V
∗
1 ), (θ̂
∗
2, V
∗
2 ), . . . , (θ̂
∗
R, V
∗
R)
and corresponding
z∗1 =
θ̂∗1 − θ̂
V
∗1/2
1
, z∗2 =
θ̂∗2 − θ̂
V
∗1/2
2
, . . . , z∗R =
θ̂∗R − θ̂
V
∗1/2
R
.
◮ Use z∗1 , . . . , z
∗
R to estimate distribution of Z — for example,
order statistics z∗(1) < · · · < z
∗
(R) used to estimate quantiles
◮ Get (1 − 2α) confidence interval
θ̂ − V 1/2z∗((1−α)(R+1)) , θ̂ − V
1/2z∗(α(R+1))
Anthony Davison: Bootstrap Methods and their Application, 31
Confidence intervals
Why Studentize?
◮ Studentize, so Z
D
−→ N(0, 1) as n→ ∞. Edgeworth series:
Pr(Z ≤ z | F ) = Φ(z) + n−1/2a(z)φ(z) +O(n−1);
a(·) even quadratic polynomial.
◮ Corresponding expansion for Z∗ is
Pr(Z∗ ≤ z | F̂ ) = Φ(z) + n−1/2â(z)φ(z) +Op(n
−1).
Typically â(z) = a(z) +Op(n
−1/2), so
Pr(Z∗ ≤ z | F̂ ) − Pr(Z ≤ z | F ) = Op(n
−1).
Anthony Davison: Bootstrap Methods and their Application, 32
Confidence intervals
◮ If don’t studentize, Z = (θ̂ − θ)
D
−→ N(0, ν). Then
Pr(Z ≤ z | F ) = Φ
( z
ν1/2
)
+n−1/2a′
( z
ν1/2
)
φ
( z
ν1/2
)
+O(n−1)
and
Pr(Z∗ ≤ z | F̂ ) = Φ
( z
ν̂1/2
)
+n−1/2â′
( z
ν̂1/2
)
φ
( z
ν̂1/2
)
+Op(n
−1).
Typically ν̂ = ν +Op(n
−1/2), giving
Pr(Z∗ ≤ z | F̂ ) − Pr(Z ≤ z | F ) = Op(n
−1/2).
◮ Thus use of Studentized Z reduces error from Op(n
−1/2) to
Op(n
−1) — better than using large-sample asymptotics, for
which error is usually Op(n
−1/2).
Anthony Davison: Bootstrap Methods and their Application, 33
Confidence intervals
Other confidence intervals
◮ Problem for studentized intervals: must obtain V , intervals
not scale-invariant
◮ Simpler approaches:
• Basic bootstrap interval: treat θ̂ − θ as pivot, get
θ̂ − (θ̂∗((R+1)(1−α)) − θ̂), θ̂ − (θ̂
∗
((R+1)α) − θ̂).
• Percentile interval: use empirical quantiles of θ̂∗1 , . . . , θ̂
∗
R:
θ̂∗((R+1)α), θ̂
∗
((R+1)(1−α)).
◮ Improved percentile intervals (BCa, ABC, . . .)
• Replace percentile interval with
θ̂∗((R+1)α′), θ̂
∗
((R+1)(1−α′′)),
where α′, α′′ chosen to improve properties.
• Scale-invariant.
Anthony Davison: Bootstrap Methods and their Application, 34
Confidence intervals
Handedness data
◮ 95% confidence intervals for correlation coefficient θ,
R = 10, 000:
Normal 0.147 0.963
Percentile −0.047 0.758
Basic 0.262 1.043
BCa (α
′ = 0.0485, α′′ = 0.0085) 0.053 0.792
Student 0.030 1.206
Basic (transformed) 0.131 0.824
Student (transformed) −0.277 0.868
◮ Transformation is essential here!
Anthony Davison: Bootstrap Methods and their Application, 35
Confidence intervals
General comparison
◮ Bootstrap confidence intervals usually too short — leads to
under-coverage
◮ Normal and basic intervals depend on scale.
◮ Percentile interval often too short but is scale-invariant.
◮ Studentized intervals give best coverage overall, but
• depend on scale, can be sensitive to V
• length can be very variable
• best on transformed scale, where V is approximately
constant
◮ Improved percentile intervals have same error in principle
as studentized intervals, but often shorter — so lower
coverage
Anthony Davison: Bootstrap Methods and their Application, 36
Confidence intervals
Caution
◮ Edgeworth theory OK for smooth statistics — beware
rough statistics: must check output.
◮ Bootstrap of median theoretically OK, but very sensitive to
sample values in practice.
◮ Role for smoothing?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
.
..
.
.
.
.
.
.
.
Quantiles of Standard Normal
T
*-
t f
or
 m
ed
ia
ns
-2 0 2
-1
0
-5
0
5
10
Anthony Davison: Bootstrap Methods and their Application, 37
Confidence intervals
Key points
◮ Numerous procedures available for ‘automatic’ construction
of confidence intervals
◮ Computer does the work
◮ Need R ≥ 1000 in most cases
◮ Generally such intervals are a bit too short
◮ Must examine output to check if assumptions (e.g.
smoothness of statistic) satisfied
◮ May need variance estimate V — see later
Anthony Davison: Bootstrap Methods and their Application, 38
Several samples
 Motivation
 Basic notions
 Confidence intervals
 Several samples
 Variance estimation
 Tests
 Regression
Anthony Davison: Bootstrap Methods and their Application, 39
Several samples
Gravity data
Table: Measurements of the acceleration due to gravity, g, given as
deviations from 980,000 ×10−3 cms−2, in units of cms−2 × 10−3.
Series
1 2 3 4 5 6 7 8
76 87 105 95 76 78 82 84
82 95 83 90 76 78 79 86
83 98 76 76 78 78 81 85
54 100 75 76 79 86 79 82
35 109 51 87 72 87 77 77
46 109 76 79 68 81 79 76
87 100 93 77 75 73 79 77
68 81 75 71 78 67 78 80
75 62 75 79 83
68 82 82 81
67 83 76 78
73 78
64 78
Anthony Davison: Bootstrap Methods and their Application, 40
Several samples
Gravity data
Figure: Gravity series boxplots, showing a reduction in variance, a shift
in location, and possible outliers.
40
60
80
10
0
g
1 2 3 4 5 6 7 8
series
Anthony Davison: Bootstrap Methods and their Application, 41
Several samples
Gravity data
◮ Eight series of measurements of gravitational acceleration g
made May 1934 – July 1935 in Washington DC
◮ Data are deviations from 9.8 m/s2 in units of 10−3 cm/s2
◮ Goal: Estimate g and provide confidence interval
◮ Weighted combination of series averages and its variance
estimate
θ̂ =
∑8
i=1 yi × ni/s
2
i∑8
i=1 ni/s
2
i
, V =
(
8∑
i=1
ni/s
2
i
)−1
,
giving
θ̂ = 78.54, V = 0.592
and 95% confidence interval of θ̂ ± 1.96V 1/2 = (77.5, 79.8)
Anthony Davison: Bootstrap Methods and their Application, 42
Several samples
Gravity data: Bootstrap
◮ Apply stratified (re)sampling to series, taking each series as
a separate stratum. Compute θ̂∗, V ∗ for simulated data
◮ Confidence interval based on
Z∗ =
θ̂∗ − θ̂
V ∗1/2
,
whose distribution is approximated by simulations
z∗1 =
θ̂∗1 − θ̂
V
1/2
1
, . . . , z∗R =
θ̂∗R − θ̂
V
1/2
R
,
giving
(θ̂ − V 1/2z∗((R+1)(1−α)) , θ̂ − V
1/2z∗((R+1)α))
◮ For 95% limits set α = 0.025, so with R = 999 use
z∗(25), z
∗
(975), giving interval (77.1, 80.3).
Anthony Davison: Bootstrap Methods and their Application, 43
Several samples
Figure: Summary plots for 999 nonparametric bootstrap simulations.
Top: normal probability plots of t∗ and z∗ = (t∗ − t)/v∗1/2. Line on
the top left has intercept t and slope v1/2, line on the top right has
intercept zero and unit slope. Bottom: the smallest t∗r also has the
smallest v∗, leading to an outlying value of z∗.
.
..
.
.
..
.
.
.
. .
.
...
.
. .
.
.
..
.
.
.
.
.
.
.
.
.
.
..
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
..
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
Quantiles of Standard Normal
t*
-2 0 2
77
78
79
80
81
.
. .
.
.
..
.
.
...
.
. ..
.
. ....
.
.
.
.
.
.
.
.
.. .
..
.
.
....
.
.
.
. .
.
.
.
..
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Quantiles of Standard Normal
z*
-2 0 2
-1
5
-1
0
-5
0
5
.
.
.. .
.
. . .
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
..
.
.
.
.
.
.
.
.
.
.
. .
. .
.
...
.
.
.
.
. ..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
..
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
. .
.
.
.
..
. .
. ..
.
..
. .
.
.
.
.
..
.
.
.
. .
.
.
. .
.
.
.
.
.
. ..
.
. .
.
..
.
...
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. ... .
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
...
.
.
. .
.
.
.
.
... .
.
..
.
. .
.
.
.
.
.
.
. .
.
.
.
.
. .
.
..
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
..
..
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
. .
.
.. .
.
.
.
.
.
.
. .
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
. .
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. ..
.
. . .
.
. .
.
..
.
.
.
.
.
.
.
.
. .
. .
..
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
.
.
. .
..
. .. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
..
.
.
.
...
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
. .
.
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. ..
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
..
.
.
.
.
.
...
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
..
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
.
.
. ..
.
. .
. .
. .
..
.
.
.
..
. .
. .
.
..
.
. .
.
.
.
.
. ..
.
.
.
.
.
.
.
.
. .
..
.
.
.
. .
.
.
.
.
.
.
..
.
..
.
..
.
.. .
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
..
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
. .
.
.
.
.
.
.
.
t*
sq
rt
(v
*)
77 78 79 80 81
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
0.
7
.
.
.. .
.
. . .
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
..
.
.
.
.
.
.
.
.
.
.
. .
.
...
.
.
.
.
. ..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
.
..
. .
.
.
..
. .
.
.
.
.
..
.
.
. .
.
.
. .
.
.
.
.
. ..
.
. .
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.....
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
... .
.
..
.
..
.
.
.
.
.
.
..
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
. .
..
.
.
.
.
.
.
.
. .
.
.
.
.
..
.
.
..
. ..
.
.
.
.
.
.
.
. .
.
.
.
.
. ..
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
. .
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
..
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
..
.
.
.
.
..
.
.
.
.
.
. .
..
..
.
.
..
..
.
.
.
.
.
.
.
.
.
.
.
..
. .
. .
. .
.
.
.
. .
.
. . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
..
.
..
.
.
. .
..
.
.
..
..
.
.
.
.
.
.
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
z*
sq
rt
(v
*)
-15 -10 -5 0 5
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
0.
7
Anthony Davison: Bootstrap Methods and their Application, 44
Several samples
Key points
◮ For several independent samples, implement bootstrap by
stratified sampling independently from each
◮ Same basic ideas apply for confidence intervals
Anthony Davison: Bootstrap Methods and their Application, 45
Variance estimation
 Motivation
 Basic notions
 Confidence intervals
 Several samples
 Variance estimation
 Tests
 Regression
Anthony Davison: Bootstrap Methods and their Application, 46
Variance estimation
Variance estimation
◮ Variance estimate V needed for certain types of confidence
interval (esp. studentized bootstrap)
◮ Ways to compute this:
• double bootstrap
• delta method
• nonparametric delta method
• jackknife
Anthony Davison: Bootstrap Methods and their Application, 47
Variance estimation
Double bootstrap
◮ Bootstrap sample y∗1, . . . , y
∗
n and corresponding estimate θ̂
∗
◮ Take Q second-level bootstrap samples y∗∗1 , . . . , y
∗∗
n from
y∗1, . . . , y
∗
n, giving corresponding bootstrap estimates
θ̂∗∗1 , . . . , θ̂
∗∗
Q ,
◮ Compute variance estimate V as sample variance of
θ̂∗∗1 , . . . , θ̂
∗∗
Q
◮ Requires total R(Q+ 1) resamples, so could be expensive
◮ Often reasonable to take Q
.
= 50 for variance estimation, so
need O(50 × 1000) resamples — nowadays not infeasible
Anthony Davison: Bootstrap Methods and their Application, 48
Variance estimation
Delta method
◮ Computation of variance formulae for functions of averages
and other estimators
◮ Suppose ψ̂ = g(θ̂) estimates ψ = g(θ), and θ̂
.
∼ N(θ, σ2/n)
◮ Then provided g′(θ) 6= 0, have (D2)
E(ψ̂) = g(θ) +O(n−1)
var(ψ̂) = σ2g′(θ)2/n+O(n−3/2)
◮ Then var(ψ̂)
.
= σ̂2g′(θ̂)2/n = V
◮ Example (D3): θ̂ = Y , ψ̂ = log θ̂
◮ Variance stabilisation (D4): if var(θ̂)
.
= S(θ)2/n, find
transformation h such that var{h(θ̂)}
.
=constant
◮ Extends to multivariate estimators, and to
ψ̂ = g(θ̂1, . . . , θ̂d)
Anthony Davison: Bootstrap Methods and their Application, 49
Variance estimation
Anthony Davison: Bootstrap Methods and their Application, 50
Variance estimation
Nonparametric delta method
◮ Write parameter θ = t(F ) as functional of distribution F
◮ General approximation:
V
.
= VL =
1
n2
n∑
j=1
L(Yj;F )
2.
◮ L(y;F ) is influence function value for θ for observation at y
when distribution is F :
L(y;F ) = lim
ε→0
t {(1 − ε)F + εHy} − t(F )
ε
,
where Hy puts unit mass at y. Close link to robustness.
◮ Empirical versions of L(y;F ) and VL are
lj = L(yj; F̂ ), vL = n
−2
∑
l2j ,
usually obtained by analytic/numerical differentation.
Anthony Davison: Bootstrap Methods and their Application, 51
Variance estimation
Computation of lj
◮ Write θ̂ in weighted form, differentiate with respect to ε
◮ Sample average:
θ̂ = y =
1
n
∑
yj =
∑
wjyj
∣∣∣
wj≡1/n
Change weights:
wj 7→ ε+ (1 − ε)
1
n , wi 7→ (1 − ε)
1
n , i 6= j
so (D5)
y 7→ yε = εyj + (1 − ε)y = ε(yj − y) + y,
giving lj = yj − y and vL =
1
n2
∑
(yj − y)
2 = n−1n n
−1s2
◮ Interpretation: lj is standardized change in y when increase
mass on yj
Anthony Davison: Bootstrap Methods and their Application, 52
Variance estimation
Nonparametric delta method: Ratio
◮ Population F (u, x) with y = (u, x) and
θ = t(F ) =
∫
x dF (u, x)/
∫
u dF (u, x),
sample version is
θ̂ = t(F̂ ) =
∫
x dF̂ (u, x)/
∫
u dF̂ (u, x) = x/u
◮ Then using chain rule of differentiation (D6),
lj = (xj − θ̂uj)/u,
giving
vL =
1
n2
∑
(
xj − θ̂uj
u
)2
Anthony Davison: Bootstrap Methods and their Application, 53
Variance estimation
Handedness data: Correlation coefficient
◮ Correlation coefficient may be written as a function of
averages xu = n−1
∑
xjuj etc.:
θ̂ =
xu− xu
{
(x2 − x2)(u2 − u2)
}1/2 ,
from which empirical influence values lj can be derived
◮ In this example (and for others involving only averages),
nonparametric delta method is equivalent to delta method
◮ Get
vL = 0.029
for comparison with v = 0.043 obtained by bootstrapping.
◮ vL typically underestimates var(θ̂) — as here!
Anthony Davison: Bootstrap Methods and their Application, 54
Variance estimation
Delta methods: Comments
◮ Can be applied to many complex statistics
◮ Delta method variances often underestimate true variances:
vL < var(θ̂)
◮ Can be applied automatically (numerical differentation) if
algorithm for θ̂ written in weighted form, e.g.
xw =
∑
wjxj, wj ≡ 1/n for x
and vary weights successively for j = 1, . . . , n, setting
wj = wi + ε, i 6= j,
∑
wi = 1
for ε = 1/(100n) and using the definition as derivative
Anthony Davison: Bootstrap Methods and their Application, 55
Variance estimation
Jackknife
◮ Approximation to empirical influence values given by
lj ≈ ljack,j = (n− 1)(θ̂ − θ̂−j),
where θ̂−j is value of θ̂ computed from sample
y1, . . . , yj−1, yj+1, . . . , yn
◮ Jackknife bias and variance estimates are
bjack = −
1
n
∑
ljack,j, vjack =
1
n(n− 1)
(∑
l2jack,j − nb
2
jack
)
◮ Requires n+ 1 calculations of θ̂
◮ Corresponds to numerical differentiation of θ̂, with
ε = −1/(n− 1)
Anthony Davison: Bootstrap Methods and their Application, 56
Variance estimation
Key points
◮ Several methods available for estimation of variances
◮ Needed for some types of confidence interval
◮ Most general method is double bootstrap: can be expensive
◮ Delta methods rely on linear expansion, can be applied
numerically or analytically
◮ Jackknife gives approximation to delta method, can fail for
rough statistics
Anthony Davison: Bootstrap Methods and their Application, 57
Tests
 Motivation
 Basic notions
 Confidence intervals
 Several samples
 Variance estimation
 Tests
 Regression
Anthony Davison: Bootstrap Methods and their Application, 58
Tests
Ingredients
◮ Ingredients for testing problems:
• data y1, . . . , yn;
• model M0 to be tested;
• test statistic t = t(y1, . . . , yn), with large values giving
evidence against M0, and observed value tobs
◮ P-value, pobs = Pr(T ≥ tobs |M0) measures evidence
against M0 — small pobs indicates evidence against M0.
◮ Difficulties:
• pobs may depend upon ‘nuisance’ parameters, those of M0;
• pobs often hard to calculate.
Anthony Davison: Bootstrap Methods and their Application, 59
Tests
Examples
◮ Balsam-fir seedlings in 5 × 5 quadrats — Poisson sample?
0 1 2 3 4 3 4 2 2 1
0 2 0 2 4 2 3 3 4 2
1 1 1 1 4 1 5 2 2 3
4 1 2 5 2 0 3 2 1 1
3 1 4 3 1 0 0 2 7 0
◮ Two-way layout: row-column independence?
1 2 2 1 1 0 1
2 0 0 2 3 0 0
0 1 1 1 2 7 3
1 1 2 0 0 0 1
0 1 1 1 1 0 0
Anthony Davison: Bootstrap Methods and their Application, 60
Tests
Estimation of pobs
◮ Estimate pobs by simulation from fitted null hypothesis
model M̂0.
◮ Algorithm: for r = 1, . . . , R:
• simulate data set y∗1 , . . . , y
∗
n from M̂0;
• calculate test statistic t∗r from y
∗
1 , . . . , y
∗
n.
◮ Calculate simulation estimate
p̂ =
1 + #{t∗r ≥ tobs}
1 +R
of
p̂obs = Pr(T ≥ tobs | M̂0).
◮ Simulation and statstical errors:
p̂ ≈ p̂obs ≈ pobs
Anthony Davison: Bootstrap Methods and their Application, 61
Tests
Handedness data: Test of independence
◮ Are dnan and hand positively associated?
◮ Take T = θ̂ (correlation coefficient), with tobs = 0.509; this
is large in case of positive association (one-sided test)
◮ Null hypothesis of independence: F (u, x) = F1(u)F2(x)
◮ Take bootstrap samples independently from
F̂1 ≡ (dnan1, . . . , dnann) and from F̂2 ≡ (hand1, . . . , handn),
then put them together to get bootstrap data
(dnan∗1, hand
∗
1), . . . , (dnan
∗
n, hand
∗
n).
◮ With R = 9, 999 get 18 values of θ̂∗ ≥ θ̂, so
p̂ =
1 + 18
1 + 9999
= 0.0019 :
hand and dnan seem to be positively associated
◮ To test positive or negative association (two-sided test),
take T = |θ̂|: gives p̂ = 0.004.
Anthony Davison: Bootstrap Methods and their Application, 62
Tests
Handedness data: Bootstrap from M̂0
15 20 25 30 35 40 45
1
2
3
4
5
6
7
8
dnan
ha
nd
310
1
32
2
4
2
1
1
1
2 1
2
1
1
Test statistic
P
ro
ba
bi
lit
y 
de
ns
ity
−0.6 −0.2 0.2 0.6
0.
0
0.
5
1.
0
1.
5
2.
0
2.
5
Anthony Davison: Bootstrap Methods and their Application, 63
Tests
Choice of R
◮ Take R big enough to get small standard error for p̂,
typically ≥ 100, using binomial calculation:
var(p̂) = var
(
1 + #{t∗r ≥ tobs}
1 +R
)
.
=
1
R2
Rpobs(1 − pobs) =
pobs(1 − pobs)
R
so if pobs
.
= 0.05 need R ≥ 1900 for 10% relative error (D7)
◮ Can choose R sequentially: e.g. if p̂
.
= 0.06 and R = 99, can
augment R enough to diminish standard error.
◮ Taking R too small lowers power of test.
Anthony Davison: Bootstrap Methods and their Application, 64
Tests
Duality with confidence interval
◮ Often unclear how to impose null hypothesis on sampling
scheme
◮ General approach based on duality between confidence
interval I1−α = (θα,∞) and test of null hypothesis θ = θ0
◮ Reject null hypothesis at level α in favour of alternative
θ > θ0, if θ0 < θα
◮ Handedness data: θ0 = 0 6∈ I0.95, but θ0 = 0 ∈ I0.99, so
estimated significance level 0.01 < p̂ < 0.05: weaker
evidence than before
◮ Extends to tests of θ = θ0 against other alternatives:
• if θ0 6∈ I
1−α = (−∞, θα), have evidence that θ < θ0
• if θ0 6∈ I1−2α = (θα, θ
α), have evidence that θ 6= θ0
Anthony Davison: Bootstrap Methods and their Application, 65
Tests
Pivot tests
◮ Equivalent to use of confidence intervals
◮ Idea: use (approximate) pivot such as Z = (θ̂ − θ)/V 1/2 as
statistic to test θ = θ0
◮ Observed value of pivot is zobs = (θ̂ − θ0)/V
1/2
◮ Significance level is
Pr
(
θ̂ − θ
V 1/2
≥ zobs |M0
)
= Pr(Z ≥ zobs |M0)
= Pr(Z ≥ zobs | F )
.
= Pr(Z ≥ zobs | F̂ )
◮ Compare observed zobs with simulated distribution of
Z∗ = (θ̂∗ − θ̂)/V ∗1/2, without needing to construct null
hypothesis model M̂0
◮ Use of (approximate) pivot is essential for success
Anthony Davison: Bootstrap Methods and their Application, 66
Tests
Example: Handedness data
◮ Test zero correlation (θ0 = 0), not independence; θ̂ = 0.509,
V = 0.1702:
zobs =
θ̂ − θ0
V 1/2
=
0.509 − 0
0.170
= 2.99
◮ Observed significance level is
p̂ =
1 + #{z∗r ≥ zobs}
1 +R
=
1 + 215
1 + 9999
= 0.0216
Test statistic
P
ro
ba
bi
lit
y 
de
ns
ity
−6 −4 −2 0 2 4 6
0.
0
0.
1
0.
2
0.
3
Anthony Davison: Bootstrap Methods and their Application, 67
Tests
Exact tests
◮ Problem: bootstrap estimate is
p̂obs = Pr(T ≥ tobs | M̂0) 6= Pr(T ≥ t |M0) = pobs,
so estimate the wrong thing
◮ In some cases can eliminate parameters from null
hypothesis distribution by conditioning on sufficient
statistic
◮ Then simulate from conditional distribution
◮ More generally, can use Metropolis–Hastings algorithm to
simulate from conditional distribution (below)
Anthony Davison: Bootstrap Methods and their Application, 68
Tests
Example: Fir data
◮ Data Y1, . . . , Yn
iid
∼ Pois(λ), with λ unknown
◮ Poisson model has E(Y ) = var(Y ) = λ: base test of
overdispersion on
T =
∑
(Yj − Y )
2/Y
.
∼ χ2n−1;
observed value is tobs = 55.15
◮ Unconditional significance level:
Pr(T ≥ tobs | M̂0, λ)
◮ Condition on value w of sufficient statistic W =
∑
Yj:
pobs = Pr(T ≥ tobs | M̂0,W = w),
independent of λ, owing to sufficiency of W
◮ Exact test: simulate from multinomial distribution of
Y1, . . . , Yn given W =
∑
Yj = 107.
Anthony Davison: Bootstrap Methods and their Application, 69
Tests
Example: Fir data
Figure: Simulation results for dispersion test. Left panel: R = 999 val-
ues of the dispersion statistic t∗ obtained under multinomial sampling:
the data value is tobs = 55.15 and p̂ = 0.25. Right panel: chi-squared
plot of ordered values of t∗, dotted line shows χ249 approximation to
null conditional distribution.
20 40 60 80
0.
0
0.
01
0.
02
0.
03
0.
04
t*
...
.....
.........
..........
.........
..........
.........
...........
...........
..........
...........
........
..........
...........
....
. .
chi-squared quantiles
di
sp
er
si
on
 s
ta
tis
tic
 t*
30 40 50 60 70 80
20
40
60
80
Anthony Davison: Bootstrap Methods and their Application, 70
Tests
Handedness data: Permutation test
◮ Are dnan and hand related?
◮ Take T = θ̂ (correlation coefficient) again
◮ Impose null hypothesis of independence:
F (u, x) = F1(u)F2(x), but condition so that marginal
distributions F̂1 and F̂2 are held fixed under resampling
plan — permutation test
◮ Take resamples of form
(dnan1, hand1∗), . . . , (dnann, handn∗)
where (1∗, . . . , n∗) is random permutation of (1, . . . , n)
◮ Doing this with R = 9, 999 gives one- and two-sided
significance probabilities of 0.002, 0.003
◮ Typically values of p̂ very similar to those for
corresponding bootstrap test
Anthony Davison: Bootstrap Methods and their Application, 71
Tests
Handedness data: Permutation resample
15 20 25 30 35 40 45
1
2
3
4
5
6
7
8
dnan
ha
nd
Test statistic
P
ro
ba
bi
lit
y 
de
ns
ity
−0.6 −0.2 0.2 0.6
0.
0
0.
5
1.
0
1.
5
2.
0
2.
5
3.
0
Anthony Davison: Bootstrap Methods and their Application, 72
Tests
Contingency table
1 2 2 1 1 0 1
2 0 0 2 3 0 0
0 1 1 1 2 7 3
1 1 2 0 0 0 1
0 1 1 1 1 0 0
◮ Are row and column classifications independent:
Pr(row i, column j) = Pr(row i) × Pr(column j)?
◮ Standard test statistic for independence is
T =
∑
i,j
(yij − ŷij)
2
ŷij
, ŷij =
yi·y·j
y··
◮ Get Pr(χ224 ≥ 38.53) = 0.048, but is T
.
∼ χ224?
Anthony Davison: Bootstrap Methods and their Application, 73
Tests
Exact tests: Contingency table
◮ For exact test, need to simulate distribution of T
conditional on sufficient statistics — row and column totals
◮ Algorithm (D8) for conditional simulation:
1. choose two rows j1 < j2 and two columns k1 < k2 at
random
2. generate new values from hypergeometric distribution
of yj1k1 conditional on margins of 2 × 2 table
yj1k1 yj1k2
yj2k1 yj2k2
3. compute test statistic T ∗ every I = 100 iterations, say
◮ Compare observed value tobs = 38.53 with simulated T
∗ —
get p̂
.
= 0.08
Anthony Davison: Bootstrap Methods and their Application, 74
Tests
Key points
◮ Tests can be performed using resampling/simulation
◮ Must take account of null hypothesis, by
• modifying sampling scheme to satisfy null hypothesis
• inverting confidence interval (pivot test)
◮ Can use Monte Carlo simulation to get approximations to
exact tests — simulate from null distribution of data,
conditional on observed value of sufficient statistic
◮ Sometimes obtain permutation tests — very similar to
bootstrap tests
Anthony Davison: Bootstrap Methods and their Application, 75
Regression
 Motivation
 Basic notions
 Confidence intervals
 Several samples
 Variance estimation
 Tests
 Regression
Anthony Davison: Bootstrap Methods and their Application, 76
Regression
Linear regression
◮ Independent data (x1, y1), . . ., (xn, yn) with
yj = x
T
j β + εj , εj ∼ (0, σ
2)
◮ Least squares estimates β̂, leverages hj, residuals
ej =
yj − x
T
j β̂
(1 − hj)1/2
.
∼ (0, σ2)
◮ Design matrix X is experimental ancillary — should be
held fixed if possible, as
var(β̂) = σ2(XTX)−1
if model y = Xβ + ε correct
Anthony Davison: Bootstrap Methods and their Application, 77
Regression
Linear regression: Resampling schemes
◮ Two main resampling schemes
◮ Model-based resampling:
y∗j = x
T
j β̂ + ε
∗
j , ε
∗
j ∼ EDF(e1 − e, . . . , en − e)
• Fixes design but not robust to model failure
• Assumes εj sampled from population
◮ Case resampling:
(xj , yj)
∗ ∼ EDF{(x1, y1), . . . , (xn, yn)}
• Varying design X but robust
• Assumes (xj , yj) sampled from population
• Usually design variation no problem; can prove awkward in
designed experiments and when design sensitive.
Anthony Davison: Bootstrap Methods and their Application, 78
Regression
Cement data
Table: Cement data: y is the heat (calories per gram of cement) evolved
while samples of cement set. The covariates are percentages by weight
of four constituents, tricalciumaluminate x1, tricalcium silicate x2, tet-
racalcium alumino ferrite x3 and dicalcium silicate x4.
x1 x2 x3 x4 y
1 7 26 6 60 78.5
2 1 29 15 52 74.3
3 11 56 8 20 104.3
4 11 31 8 47 87.6
5 7 52 6 33 95.9
6 11 55 9 22 109.2
7 3 71 17 6 102.7
8 1 31 22 44 72.5
9 2 54 18 22 93.1
10 21 47 4 26 115.9
11 1 40 23 34 83.8
12 11 66 9 12 113.3
13 10 68 8 12 109.4
Anthony Davison: Bootstrap Methods and their Application, 79
Regression
Cement data
◮ Fit linear model
y = β0 + β1x1 + β2x2 + β3x3 + β4x4 + ε
and apply case resampling
◮ Covariates compositional: x1 + · · · + x4
.
= 100% so X
almost collinear — smallest eigenvalue of XTX is
l5 = 0.0012
◮ Plot of β̂∗1 against smallest eigenvalue of X
∗TX∗ reveals
that var∗(β̂∗1) strongly variable
◮ Relevant subset for case resampling — post-stratification of
output based on l∗5?
Anthony Davison: Bootstrap Methods and their Application, 80
Regression
Cement data
.
.
.
.
.
.
. . ..
.
.
.
. .
. .
. . .
.
. .. . ..
.
. .
.
.
..
.
.
.
.
.. .
.
. .
..
..
.
. .
.
.
. .
.
. . .. . .
.
. ...
.
.
.
. ... ..
.. . ... .
.. .
. .
..
..
.
. ..
.
. ..
.
.. .
.
.. .
.
..
.
.
..
.
...
.
.
..
.
.
.
.. .
.
.
.
.
.
.
..
.
.. .. .
. .
.
..
.
...
.
.
. ..
.
.
.
.
.
.
...
.
..
.
..
..
.. .. ..
.
.
.
.
.
.
.
. . .
.
..
. ..
.... .
..
.
.
.
..
.
.
.
.
.
... .
.
.
.
..
.
.
..
.
. .
.
.
.
.
..
.
.
.
.
..
.
..
.
. . .
.
.
.
.
..
. .
. .
.
.
.
.
.. .
..
.
.
.
.
.
.
.
.
.
.
.
.
. ...
.
.
.
.
.
.
.
. .
..
.
.
..
. .
.
.
..
. .
..
.
.
.
.
.
. .
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
..
. .
.
.
.
. .
..
.
. .
.
.
.
.
.
.
. .
.
.
..
. .. .
.
.
.
.
. .
.
.
. ..
.
.
.
..
smallest eigenvalue
be
ta
1h
at
*
1 5 10 50 500
-1
0
-5
0
5
10
.
.
.
.
.
.
. . ..
.
.
.
.
.
.. .
.
. .
.
. .... . ..
.
. .
.
.
. .
.
.
.
. .
.. .
. .
..
.
.
.
.
. .
.
.
.. .
. . .
.
. ... .
. ..
.
. . ..
.
.. . .
. .
. ...
. .
.
. .
.
. .
.
..
.
..
...
.
..
.
.
. ..
.
... .
...
.
.
.
..
.
..
.
.
.
. . ..
.
.
.
. .
...
... .
. .
.
. ..
.
.
..
.
. .
.
..
..
..
.. ..
.
..
.
.
.
.
.
. .
.
. ..
...
.
..
.
.
.
.
.
.
.
.
.
.. ..
.
.
.
. .
. .
.
.
..
.
.
.
.. ..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
. .
.
. .
.. ..
..
.. .
.
..
.
..
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
..
.
..
.
.
.
.
.. .
.
.
.
.
.
.
..
.
..
.
. .
.
.
.
.
. .
. ..
.
.
. .
.
.
. .
. ...
.
.
.
.
.
.
.
smallest eigenvalue
be
ta
2h
at
*
1 5 10 50 500
-1
0
-5
0
5
10
Anthony Davison: Bootstrap Methods and their Application, 81
Regression
Cement data
Table: Standard errors of linear regression coefficients for cement data.
Theoretical and error resampling assume homoscedasticity. Resampling
results use R = 999 samples, but last two rows are based only on those
samples with the middle 500 and the largest 800 values of ℓ∗1.
β̂0 β̂1 β̂2
Normal theory 70.1 0.74 0.72
Model-based resampling, R = 999 66.3 0.70 0.69
Case resampling, all R = 999 108.5 1.13 1.12
Case resampling, largest 800 67.3 0.77 0.69
Anthony Davison: Bootstrap Methods and their Application, 82
Regression
Survival data
dose x 117.5 235.0 470.0 705.0 940.0 1410
survival % y 44.000 16.000 4.000 0.500 0.110 0.700
55.000 13.000 1.960 0.320 0.015 0.006
6.120 0.019
◮ Data on survival % for rats at different doses
◮ Linear model:
log(survival) = β0 + β1dose
•
•
•
•
••
•
• • ••
dose
su
rv
iv
al
 %
200 600 1000 1400
0
10
20
30
40
50
••
••
•
•
•
•
•
•
••
•
•
dose
lo
g 
su
rv
iv
al
 %
200 600 1000 1400
-4
-2
0
2
4
Anthony Davison: Bootstrap Methods and their Application, 83
Regression
Survival data
◮ Case resampling
◮ Replication of outlier: none (0), once (1), two or more (•).
◮ Model-based sampling including residual would lead to
change in intercept but not slope.
sum of squares
es
tim
at
ed
 s
lo
pe
0.5 1.0 1.5 2.0 2.5 3.0
-0
.0
12
-0
.0
08
-0
.0
04
0.
0
0
000
0
0
0
0
0
0
0 0
0 0
0
0
0
0
00
0 0
0
00
0
0
0
0
0
0
0
0
0
0
0 0
0
0 0
0
0
0
0
0
0
00
00
0
00
0
0
11
1
1
1
1
11
111
1
1
111 1
1
1
1 11
1
1
1 1
1
11
1
1
1 1
1 11 111
1
11111
1
1
1
1
11
1
1 1 1
1
1 11
1
1
1
1 1
•
•• •
•
•
•
•
•
•
•
••
•• ••••
• ••
•
•
•
• •••
•
• • •
•
•
•
•
• ••
•• ••
•
• • •• • ••
Anthony Davison: Bootstrap Methods and their Application, 84
Regression
Generalized linear model
◮ Response may be binomial, Poisson, gamma, normal, . . .
yj ∼ mean µj , variance φV (µj),
where g(µj) = x
T
j β is linear predictor; g(·) is link function.
◮ MLE β̂, fitted values µ̂j , Pearson residuals
rPj =
yj − µ̂j
{V (µ̂j)(1 − hj)}
1/2
.
∼ (0, φ).
◮ Bootstrapped responses
y∗j = µ̂j + V (µ̂j)
1/2ε∗j
where ε∗j ∼ EDF(rP1 − rP , . . . , rPn − rP ). However
• possible that y∗j 6∈ {0, 1, 2, . . . , }
• rPj not exchangeable, so may need stratified resampling
Anthony Davison: Bootstrap Methods and their Application, 85
Regression
AIDS data
◮ Log-linear model: number of reports in row j and column k
follows Poisson distribution with mean
µjk = exp(αj + βk)
◮ Log link function
g(µjk) = log µjk = αj + βk
and variance function
var(Yjk) = φ× V (µjk) = 1 × µjk
◮ Pearson residuals:
rjk =
Yjk − µ̂jk
{µ̂jk(1 − hjk)}1/2
◮ Model-based simulation:
Y ∗jk = µ̂jk + µ̂
1/2
jk ε
∗
jk
Anthony Davison: Bootstrap Methods and their Application, 86
Regression
Diagnosis Reporting-delay interval (quarters): Total
period reports
to end
Year Quarter 0† 1 2 3 4 5 6 · · · ≥14 of 1992
1988 1 31 80 16 9 3 2 8 · · · 6 174
2 26 99 27 9 8 11 3 · · · 3 211
3 31 95 35 13 18 4 6 · · · 3 224
4 36 77 20 26 11 3 8 · · · 2 205
1989 1 32 92 32 10 12 19 12 · · · 2 224
2 15 92 14 27 22 21 12 · · · 1 219
3 34 104 29 31 18 8 6 · · · 253
4 38 101 34 18 9 15 6 · · · 233
1990 1 31 124 47 24 11 15 8 · · · 281
2 32 132 36 10 9 7 6 · · · 245
3 49 107 51 17 15 8 9 · · · 260
4 44 153 41 16 11 6 5 · · · 285
1991 1 41 137 29 33 7 11 6 · · · 271
2 56 124 39 14 12 7 10 263
3 53 175 35 17 13 11 306
4 63 135 24 23 12 258
1992 1 71 161 48 25 310
2 95 178 39 318
3 76 181 273
4 67 133
Anthony Davison: Bootstrap Methods and their Application, 87
Regression
AIDS data
◮ Poisson two-way model deviance 716.5 on 413 df —
indicates strong overdispersion: φ > 1, so Poisson model
implausible
◮ Residuals highly inhomogeneous — exchangeability
doubtful
+++++
+++
+++
+++
+++
++
+++++
++
+
++
+++
+
+
++
+
+
D
ia
gn
os
es
1984 1986 1988 1990 1992
0
10
0
20
0
30
0
40
0
50
0
skewness
rP
0 1 2 3 4 5
-6
-4
-2
0
2
4
6
Anthony Davison: Bootstrap Methods and their Application, 88
Regression
AIDS data: Prediction intervals
◮ To estimate prediction error:
• simulate complete table y∗jk;
• estimate parameters from incomplete y∗jk
• get estimated row totals and ‘truth’
µ̂∗+,j = e
bα∗j
∑
k unobs
e
bβ∗k , y∗+,j =
∑
k unobs
y∗jk.
• Prediction error
y∗+,j − µ̂
∗
+,j
µ̂
∗1/2
+,j
studentized so more nearly pivotal.
◮ Form prediction intervals from R replicates.
Anthony Davison: Bootstrap Methods and their Application, 89
Regression
AIDS data: Resampling plans
◮ Resampling schemes:
• parametric simulation, fitted Poisson model
• parametric simulation, fitted negative binomial model
• nonparametric resampling of rP
• stratified nonparametric resampling of rP
◮ Stratification based on skewness of residuals, equivalent to
stratifying original data by values of fitted means
◮ Take strata for which
µ̂jk < 1, 1 ≤ µ̂jk < 2, µ̂jk ≥ 2
Anthony Davison: Bootstrap Methods and their Application, 90
Regression
AIDS data: Results
◮ Deviance/df ratios for the sampling schemes, R = 999.
◮ Poisson variation inadequate.
◮ 95% prediction limits.
0.0 0.5 1.0 1.5 2.0 2.5
0
1
2
3
4
5
6
poisson
deviance/df
0.0 0.5 1.0 1.5 2.0 2.5
0
1
2
3
4
5
6
negative binomial
deviance/df
0.0 0.5 1.0 1.5 2.0 2.5
0
1
2
3
4
5
6
nonparametric
deviance/df
0.0 0.5 1.0 1.5 2.0 2.5
0
1
2
3
4
5
6
stratified nonparametric
deviance/df
+
+
+ +
+
+
+
+ +
+ + +
+
+
+ +
+
D
ia
gn
os
es
1989 1990 1991 1992
20
0
30
0
40
0
50
0
60
0
Anthony Davison: Bootstrap Methods and their Application, 91
Regression
AIDS data: Semiparametric model
◮ More realistic: generalized additive model
µjk = exp {α(j) + βk} ,
where α(j) is locally-fitted smooth.
◮ Same resampling plans as before
◮ 95% intervals now generally narrower and shifted upwards
+++++
++++
++
++++
++
++
+++++
++
+
++
+++
+
+
++
+
+
D
ia
gn
os
es
1984 1986 1988 1990 1992
0
10
0
20
0
30
0
40
0
50
0
60
0
+
+
+ +
+
+
+
+ +
+ + +
+
+
+ +
+
D
ia
gn
os
es
1989 1990 1991 1992
20
0
30
0
40
0
50
0
60
0
Anthony Davison: Bootstrap Methods and their Application, 92
Regression
Key points
◮ Key assumption: independence of cases
◮ Two main resampling schemes for regression settings:
• Model-based
• Case resampling
◮ Intermediate schemes possible
◮ Can help to reduce dependence on assumptions needed for
regression model
◮ These two basic approaches also used for more complex
settings (time series, . . .), where data are dependent
Anthony Davison: Bootstrap Methods and their Application, 93
Regression
Summary
◮ Bootstrap: simulation methods for frequentist inference.
◮ Useful when
• standard assumptions invalid (n small, data not normal,
. . .);
• standard problem has non-standard twist;
• complex problem has no (reliable) theory;
• or (almost) anywhere else.
◮ Have described
• basic ideas
• confidence intervals
• tests
• some approaches for regression
Anthony Davison: Bootstrap Methods and their Application, 94
Regression
Books
◮ Chernick (1999) Bootstrap Methods: A Practicioner’s
Guide. Wiley
◮ Davison and Hinkley (1997) Bootstrap Methods and their
Application. Cambridge University Press
◮ Efron and Tibshirani (1993) An Introduction to the
Bootstrap. Chapman & Hall
◮ Hall (1992) The Bootstrap and Edgeworth Expansion.
Springer
◮ Lunneborg (2000) Data Analysis by Resampling: Concepts
and Applications. Duxbury Press
◮ Manly (1997) Randomisation, Bootstrap and Monte Carlo
Methods in Biology. Chapman & Hall
◮ Shao and Tu (1995) The Jackknife and Bootstrap. Springer
Anthony Davison: Bootstrap Methods and their Application, 95

