IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 13, NO. 4, JULY 2005 1
Active Learning: Theory and Applications to
Automatic Speech Recognition
Giuseppe Riccardi, Senior Member, IEEE, and Dilek Hakkani-T√ºr, Member, IEEE
Abstract‚ÄîWe are interested in the problem of adaptive learning
in the context of automatic speech recognition (ASR). In this
paper, we propose an active learning algorithm for ASR. Au-
tomatic speech recognition systems are trained using human
supervision to provide transcriptions of speech utterances. The
goal of Active Learning is to minimize the human supervision
for training acoustic and language models and to maximize the
performance given the transcribed and untranscribed data. Active
learning aims at reducing the number of training examples to be
labeled by automatically processing the unlabeled examples, and
then selecting the most informative ones with respect to a given
cost function for a human to label. In this paper we describe how
to estimate the confidence score for each utterance through an
on-line algorithm using the lattice output of a speech recognizer.
The utterance scores are filtered through the informativeness
function and an optimal subset of training samples is selected.
The active learning algorithm has been applied to both batch and
on-line learning scheme and we have experimented with different
selective sampling algorithms. Our experiments show that by
using active learning the amount of labeled data needed for a given
word accuracy can be reduced by more than 60% with respect to
random sampling.
Index Terms‚ÄîAcoustic modeling, active learning, language
modeling, large vocabulary continuous speech recognition, ma-
chine learning.
I. INTRODUCTION
I N THE 1990s, there was a large body of research work ondata driven algorithms for Large Vocabulary Continuous
Speech Recognition (LVCSR). This work has had permanent
impact on state-of-the-art automatic speech recognition [1]
and stochastic modeling [2]. In those papers, the fundamental
assumption is on the nature of the input channel statistics. The
assumption is that the performance of the stochastic models are
based on the fact that the training examples are drawn randomly
from a large sample set . Moreover, most of these algorithms
are trained and tested using the identical and independent
distribution (i.i.d.) assumption. This approach leads to models
that are by design suited for stationary channels. While this
assumption holds for a large number of cases, it has two draw-
backs. First it makes inefficient use of data which is expensive
to transcribe. Second it restricts the machines behavior to adapt
dynamically to nonstationary input channels. In the most recent
work the approach to adapt to nonstationary channels has been
Manuscript received July 21, 2003; revised July 21, 2004. The Associate Ed-
itor coordinating the review of this manuscript and approving it for publication
was Dr. Tanja Schultz.
The authors are with AT&T Labs‚ÄîResearch, Florham Park, NJ 07932 USA
(e-mail: dsp3@research.att.com).
Digital Object Identifier 10.1109/TSA.2005.848882
to adapt the model [3]‚Äì[6]. Even, in the adaptation framework
it is not determined how to track time-varying statistics.
In this work we take a fundamentally different approach to
train adaptive LVCSRs based on the concept of active learning
(AL). Active learning has the distinct advantage of efficiently
exploiting transcribed data and thus reduces human effort.
Moreover, modeling under the active learning paradigm has the
intrinsic capability to adapt to nonstationary events by means
of a feedback mechanism in the training algorithm. Active
learning can optimize the performance of LVCSRs by selec-
tively sampling the number of examples that maximizes word
accuracy. In the next two sections we contrast the traditional
method of passive learning versus the active learning approach.
In Section III we define the problem of active learning in its
general formulation. In Sections IV and V we show how active
learning applies to both acoustic and language modeling for
LVCSR. In the last Section VI we give experimental results to
support the claims in the previous sections.
II. PASSIVE LEARNING
The most established method for training acoustic and
language models is supervised training. In this case the set
of training examples are speech utterances, , drawn
at random from the set , which has been selected a-priori
and fixed in time1. All of the examples are transcribed by
human supervision and the transcriptions are provided with
a time delay . This is in contrast with unsupervised
learning [7]‚Äì[10], where a set of training examples is generated
automatically without supervision . In
Fig. 1 we give the architecture of the learning process in the
case of supervised learning. The training examples are
human labeled and their statistics are used to estimate means
and variances of the stochastic models (e.g., -gram,
HMMs). The statistical models are evaluated in terms of error
rates (e.g., Word Error Rate for ASR). In this learning scheme
there is no relation between the expected error rate and the set
of training examples . In other words, if a new set of set of
training examples are provided, it is not possible to predict
if this set would decrease or increase the error rate estimated
on .
This specific type of supervised training is also called passive
learning [11]. The approach is passive for the following reasons.
‚Ä¢ Selection of training set . The training set is fixed
a-priori and for a given time horizon, .
1In practice X is the by-product of the data collection, which is usually done
via Wizard-of-Oz paradigm or by an automated spoken dialog system.
1063-6676/$20.00 ¬© 2005 IEEE
2 IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 13, NO. 4, JULY 2005
Fig. 1. Supervised passive learning.
‚Ä¢ Selection of labeled training examples. Training and
test sets are sampled randomly from . All examples
are considered equally informative for the purpose
of learning. No is disregarded or generated automat-
ically.
III. ACTIVE LEARNING
A. Background
The search for effective training data sampling algorithms
has been studied in the machine learning research. Previous
work in active learning has concentrated on two approaches: cer-
tainty-based methods and committee-based methods. In the cer-
tainty-based methods, an initial system is trained using a small
set of annotated examples [12]. Then, the system examines and
labels the unannotated examples, and determines the certainties
of its predictions of them. The examples with the lowest cer-
tainties are then presented to the labelers for annotation. In the
committee-based methods, a distinct set of classifiers is also cre-
ated using the small set of annotated examples [11], [13]. The
unannotated instances, whose annotations differ most when pre-
sented to different classifiers are presented to the labelers for an-
notation. In both paradigms, a new system is trained using the
new set of annotated examples, and this process is repeated until
the system performance converges. A recent committee-based
method, which is applicable to multiview problems (i.e., prob-
lems with several sets of uncorrelated attributes that can be used
for learning) is co-testing [14]. In co-testing, the committee of
classifiers is trained using different views of the data.
In the language processing research, certainty-based methods
have been used for information extraction, and natural language
parsing [15]‚Äì[17], committee-based methods have been used for
text categorization [18], [14]. Our previous work concentrated
on using active learning for language model training for ASR
[19], [20], and spoken language understanding [21]. Concur-
rently, [22] has proposed a similar selective sampling approach
for ASR, and have shown improvements for training acoustic
models on a small vocabulary task.
Fig. 2. Supervised active learning architecture.  and  are the statistical
parameters of acoustic or language models.
B. The Algorithm
Passive learning delegates the burden of estimating good
models to the estimation techniques (e.g., acoustic or language
modeling). In contrast, active learning emphasizes the role of
the input selection for the purpose of improving the expected
error rate over time.
Active learning is defined in terms of two basic concepts: se-
lective sampling and error rate prediction as function of the
training examples [11]. In Fig. 2 we give the general scheme
of active learning machines. In the general formulation, active
learning considers the input as an example (e.g.,
speech utterance, feature vector) being sampled at time . This
formulation is suitable for dynamic systems which are sought
to track stationary and nonstationary statistics. For ,
we have active learning in the more traditional sense of an opti-
mization problem over a-priori set of training examples [11].
In the rest of the paper we will consider both scenarios as they
apply to automatic speech recognition.
While active learning can act upon each individual sample
, selective sampling is optimized by buffering the
samples over a time horizon,
(1)
Selective sampling searches for the subset of examples that
are mostly informative to decrease the word error rate . In Sec-
tion VI we address the relation between the and the first order
difference for the error rate, .
Let us assume that there exists a function , which
computes an estimate of the error rate for each example .
The informativeness function assigns a weight to each
training example as a function of its error estimate. In Fig. 3 we
have two candidate functions which characterize two different
learning strategies. is the uncertainty based linear func-
tion and it ranks monotonically all examples starting with the
RICCARDI AND HAKKANI-T√úR: ACTIVE LEARNING: THEORY AND APPLICATIONS TO AUTOMATIC SPEECH RECOGNITION 3
Fig. 3. Characterization of the Informativeness function I(). The x axis is
for the error estimates ("). The y axis is for the the informativeness function
I() which could be either linearly related with with (I ) or skewed around
the random guess estimate  ; (I ).
lowest error rate2. In this case the informativeness of an example
will favor the exploitation strategy in active learning and ex-
pecting high reward from human supervision of poorly pre-
dicted samples. However, in general the classes we are learning
are not separable and we should expect examples that are ei-
ther outliers or hard to learn. This leads to an alternate infor-
mativeness function , which penalizes samples with very
high (and low) error rates and emphasizes samples which are
estimated (almost) randomly. In Fig. 3, is plotted for
the case of random predictions occurring at . The triangular
shaped informativeness function has only one parameter,
, to be computed based on held-out data. Let us re-
call here that the examples ranking is dependent on the current
model parameters and thus examples may have different infor-
mativeness score at different time intervals.
The informativeness function allows us to rank each ex-
ample and associate an integer number , which spans from
1 to the size of the buffer, . If more than one independent
error estimator is provided then there is the problem of merging
ranked lists, which is a research topic well studied in statistics
and machine learning [23], [24]. The last step of selective sam-
pling (see Fig. 2) is to take the first examples from the ranked
list and have it labeled (human).3
Once the statistics and the error estimator parameters have
been updated, the learning loop continues by processing a new
batch of training examples. In general if we are not
making any assumptions about the model learning algorithm
(black box paradigm) there is no guarantee that the expected
error rate will decrease as function of the selected samples.
There are convergence results for particular learning algorithms
such as the perceptron [25], however they do not apply in the
general case. In LVCSR, the class of estimation methods is
large, ranging from maximum likelihood (ML) to discrimina-
tive training (DT). For ASR we will show strong experimental
relation between recognition error and the sequence of se-
2Obviously, any strictly monotonic function between  = 0 and  = 1 will
provide one and only example ranking.
3Throughout the paper we will use the term label and depending on the con-
text it will refer to different types of human supervision. For instance, in text
classification text transcriptions are annotated with a label which is one out of
a fixed set of allowed labels. In speech recognition, speech utterances are tran-
scribed using a set of guidelines to describe the event contained in them (e.g.,
spoken words or speech disfluencies).
Fig. 4. Minimum sample size and minimum error rate on the learning curve.
lectively sampled training sets using ML trained stochastic
models.
In the next section we will exploit the active learning concepts
and apply them to automatic speech recognition.
IV. ACTIVE LEARNING FOR ASR
In speech recognition the common training paradigm is the
optimization of the word error rate (WER) for a given training
and test set, both drawn at random from a fixed . In this pro-
cedure, it is assumed that all training examples are useful. In
the case of the so-called data mismatch there is a performance
gap between expected (i.i.d. case) and actual performance (non-
stationary case). Traditionally this problem is approached with
model adaptation techniques with a supervised passive scheme
[26], [6]. However, even in the latter case it is not possible to au-
tomatically detect the mismatch or the time-varying statistics.
Active learning encapsulates the traditional LVCSRs sta-
tistical estimation techniques into the framework of adaptive
machine learning. It detects automatically the training exam-
ples that are difficult to recognize because of data sparseness
or nonstationarities. In general it selects, out of all the avail-
able speech corpora, only those samples that maximizes word
accuracy (1-WER), overcoming the problem of training from
outliers or superfluous data. In Fig. 4 we show the typical
learning curves of LVCSR systems for a large4 training set .
We are interested in minimizing the error rate, , regardless
of the amount of samples needed to train the stochastic models.
In general if we train from all the available training examples,
the error rate will be suboptimal . This is due to
overtraining or to the presence of outliers. From a sample size
point of view, is the optimal value and the performance
saturation occurs at earlier stages of training .
The central concept in active learning is selective sampling
from the training set. There are two components, namely the
ranking of each sample in and the selection of a set
of examples. The latter component controls directly the model
update rate. In the next sections we describe the core algorithms
for ranking , estimating the error rate and training with
AL.
V. TRAINING WITH ACTIVE LEARNING
A. Word Score Estimation
In the literature, there are two leading methods for confidence
score estimation. The first one is based on acoustic measure-
4In practice for LVCSRs, 10 K speech utterances are considered a reasonable
size of the training set.
4 IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 13, NO. 4, JULY 2005
Fig. 5. (Top) Utterance accuracy versus mean confidence scores. For each bin
the mean and the standard deviation are plotted. (Bottom) Relative frequency
histogram for mean confidence scores.
Fig. 6. (Top) Utterance accuracy versus voting confidence scores. for each bin
the mean and the standard deviation are plotted. (Bottom) Relative frequency
histogram for voting scores.
ments [27] and the other one is based on word lattices [28], [29].
The latter one has the advantage that the probability computa-
tion does not require training of an error estimator. There are
also hybrid approaches, which use features from the two types
of methods [30].
We extract word confidence scores from the lattice output of
ASR. A detailed explanation of this algorithm and the compar-
ison of its performance with other approaches is presented in
[31]. A summary of the algorithm is as follows.
1) Compute the posterior probabilities for all transitions in
the lattice.
2) Extract a path from the lattice (which can be the best,
longest or a random path), and call this as the pivot of
the alignment.
3) Traverse the lattice, and align all the transitions with
the pivot, merging the transitions that correspond to the
same word (or label) and occur in the same interval (by
summing their posterior probabilities).
We use the state times or approximate state locations on the
lattice, to align transitions that have occurred at around the same
time interval. We call the final structure as pivot alignment. We
use the word posterior probability estimates on the best path of
the pivot alignments as word confidence scores, , where we
use to denote a word and use the notation to represent
the confidence score of the word sequence .
B. Utterance Score Estimation
For active learning, we used different approaches to obtain
utterance level confidence scores from word confidence scores
[19], and two of them resulted in a better performance. One
approach is to compute the confidence score of an utterance as
the arithmetic mean of the confidence scores of the words that
it contains
(2)
The second approach is using a voting scheme with a threshold.
The words with a score less than the threshold do not have any
contribution to the utterance score. The words with a confidence
score greater than the threshold contribute to the utterance score
by 1. The final utterance score is normalized by the utterance
length.
(3)
where
if
otherwise
(4)
In Fig. 5(top) we plot the joint statistics of the mean confi-
dence scores (2) and the corresponding utterance accuracy
. The confidence scores are binned uniformly (100 utterances
per bin) and we plot the mean and standard variation of the ut-
terance accuracy within each bin. In Fig. 5 (bottom) we plot the
relative frequency histogram of the utterance confidence score.
As can be seen in Fig. 5 the utterance score is linearly correlated
to the utterance accuracy.
In Fig. 6 we plot the joint statistics of the voting confidence
scores (3) and the corresponding utterance accuracy
using the same binning procedure used in Fig. 5.
Both figures are the experimental evidence for the informa-
tiveness functions in Fig. 3.
C. Algorithm
The algorithm for active learning for ASR is depicted in
Fig. 7. In the figure, the solid lines show the operation flow, and
the dashed lines show data flow. As the initialization step, we
first train a speech recognizer, using a small set of transcribed
data, . The can also include off-the-shelf speech corpora.
Using the initial ASR models, we automatically transcribe the
utterances that are candidates for transcription, (see in
Fig. 2). We then compute lattice based confidence scores for
RICCARDI AND HAKKANI-T√úR: ACTIVE LEARNING: THEORY AND APPLICATIONS TO AUTOMATIC SPEECH RECOGNITION 5
each word and use either (2) or (3) to assign an utterance based
score. We apply the informativeness function to predict
which candidates are more likely to reduce the word error
rate. We provide human transcriptions solely for the selected
utterances from . This set of utterances are denoted by
in the figure. We then add the transcribed utterances to
and exclude them from . We iterate this process as long as
there are additional untranscribed utterances and we stop, if the
WER on the development test set has converged.
D. Error Convergence
By treating the model learning algorithms as black boxes, in
general there is no guarantee to minimize the error rate mono-
tonically. For instance for stochastic language modeling, most
of the state-of-the-art techniques are based on ML estimates of
-gram counts [32]. In acoustic modeling, there is a quite effec-
tive set of techniques that address the problem of minimizing
the word error rate [1]. In both cases, it is an open research
problem how to combine the selective sampling into the estima-
tion algorithm. It is straightforward to show that an arbitrarily
biased sampling bears no correlation with word error rate. How-
ever, in the case of ML based model estimation we will show an
empirical monotonic correlation between recognition error rate
and two sampling methods: random and selective sampling of
training sets using ML trained stochastic models.
VI. EXPERIMENTAL RESULTS
We performed a series of experiments to verify that the
posterior probabilities of the ASR pivot alignments can be
used to select more informative utterances to transcribe. For
all these experiments, we used utterances from the How May I
Help You? human-machine speech dialog database [33]. The
language models used in all our experiments are trigram models
based on Variable Ngram Stochastic Automata (VNSA) [34].
The acoustic models are subword unit based, with triphone
context modeling.
A. Training and Test Data
The initial set of transcribed utterances, which is used to train
the initial acoustic and language models consists of 1000 utter-
ances (10 722 words). The additional set of transcription candi-
date utterances consists of 26 963 utterances (307 649 words).
The test data consists of 1 000 utterances (10 646 words). In the
experiments, where we retrained the acoustic model, we used a
1 000 utterance (11 515 words) development test set to tune the
parameters (the number of mixtures, etc.).
B. Active Learning
Using the initial ASR acoustic and language models, we gen-
erated lattices and pivot alignments for our additional training
data, and computed the confidence scores for words and utter-
ances. We ran the algorithm for only a single iteration, with
equal to the additional training set size, and sorted the data in
the order of increasing usefulness for ASR. We then incremen-
tally trained acoustic and language models, every 2000 utter-
ances (100, 250, 500, and 1000 utterances at the initial points),
Fig. 7. The algorithm.
Fig. 8. Word accuracy learning curves.
and generated learning curves for Word Accuracy ,
which are presented in Fig. 8. In that figure, there are three sets
of curves for random sampling and selective sampling. In the
top experiment, both the acoustic and language models are re-
trained as we have more transcribed training data. In the center
experiment, the acoustic model is retrained for each set of new
examples, while the language model is fixed and trained from
the initial set. In the bottom experiment, the language model is
retrained for each set of newly selected examples and the
acoustic model is fixed and trained from the initial set. In all
of them, the random and selective sampling curves meet at the
same point, as all the data is used in that case . Of
course, in practice active learning would stop where the perfor-
mance saturates. We plot the results using the arithmetic mean
of the word confidence scores ( is the mean function in (2)) and
6 IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 13, NO. 4, JULY 2005
, which performed slightly better than .5 From these
curves, we see that selective sampling is effective in minimizing
the amount of labeled data necessary to achieve best word accu-
racy. When we retrained both the acoustic and language models,
the best performance with random sampling was achieved using
all of the training data (27 963 utterances). We achieved the
same word accuracy (69.9%) with selective sampling and using
68% less data (with around 9 000 utterances). Therefore, by se-
lective sampling, it is possible to speed up the learning rate of
ASR with respect to the amount of labeled transcriptions. We
also achieved a higher accuracy with active learning (71.0%)
than using all the data, when we used 19 000 utterances. At each
point on these curves, we selected the acoustic model parame-
ters that maximize the word accuracy on the development test
set, and plotted the real test set word accuracy with these param-
eters.
One reason for the better learning is that, with active
learning, we achieve a faster learning rate for new words and
new -grams. Fig. 9 shows the vocabulary size learning curves
for random and selective sampling. As can be seen from the
figure, we detect new -grams at a higher rate with selectively
sampling as compared to random sampling.
For a given acoustic channel (e.g., telephone), we are inter-
ested to evaluate the performance of AL for learning novel do-
main language. In this experimental scenario we use an off-
the-shelf acoustic model trained on the same acoustic channel
from off-the-shelf speech corpora. We used the set in the pre-
vious experiment to train the initial language model and used
these models to select examples from the buffer. The random
and selective sampling learning curves are plotted in Fig. 10.
In this experiment, the best performance with random sampling
was again achieved using all of the training data (27 963 utter-
ances). We achieved the same word accuracy (68.1%) with se-
lective sampling and using 64% less data (with around 10 000
utterances). We achieved the best accuracy with active learning
(68.6%) when we used 13 000 utterances (less than half of all
the data).
We have simulated the dynamic case by sorting
the utterances according to their time stamps. The time span for
the entire training set is three months of live recordings from the
‚ÄúHow May I Help You?‚Äù system. We have buffered 1 000 utter-
ances at time instant and selectively sampled 500
utterances , and transcribed them. We discarded the
remaining 500 utterances, and used the new set of transcribed
utterances in the selective sampling for . In this experiment
we used the acoustic model from the previous experiment (see
Fig. 10). Fig. 11 depicts the results of such an experiment. In
this case, the selective sampling learning curve ends at around
14 500 utterances, as we are discarding half of the utterances
in the buffer at each instant . According to this plot, we
again achieve the best accuracy with random sampling, using
64% less transcribed utterances with selective sampling. With
only half of the additional data, we achieve a word accuracy of
68.9%, which is 0.8% points better than the accuracy achieved
5We also used the normalized utterance likelihood as a sampling criterion,
and it gave inferior performance.
Fig. 9. Vocabulary size learning curves.
Fig. 10. Learning curves for novel domain language (off-the-shelf acoustic
model).
using all the data, and 1.7% points better than using half of the
data with random sampling.
C. Buffer Size
The buffering of the input in active learning is designed
to increase the probability of minimizing the error rate. To show
this, we plotted the effect of buffer size, , to the active learning
performance in Fig. 12. We selected three set sizes for tran-
scription, utterances. We selectively
sampled these utterances from buffers of different sizes. As can
be seen from the figure, the word accuracy is maximized for a
buffer of size three times the selection size . This shows that
there is a maximum performance we can achieve using a given
transcribed data set size. The performance degrades as we in-
crease the buffer size, as our algorithm is not outlier-proof, and
the percentage of outliers increases as we increase the buffer
size. But in all cases, the performance is better than random sam-
pling, which corresponds to in Fig. 12.
RICCARDI AND HAKKANI-T√úR: ACTIVE LEARNING: THEORY AND APPLICATIONS TO AUTOMATIC SPEECH RECOGNITION 7
Fig. 11. Word accuracy learning curves for the dynamic case of AL (x =
x(t)). Examples are buffered (B(t)) according to the time stamp of the data
collection.
Fig. 12. Effect of buffer (J) size on active learning performance.
VII. CONCLUSION
In this paper, we have proposed a novel approach to automatic
speech recognition based on active learning. Active learning
makes efficient use of data which is expensive to transcribe.
Moreover, active learning has the built-in feature to adapt to
nonstationary events by means of feedback mechanism in the
training algorithm. Active learning can also be seen as an op-
timization algorithm that selects the training examples that op-
timize the test set word accuracy. Our experiments show that
by using active learning the amount of labeled data needed for
a given word accuracy can be reduced by more than 60% with
respect to random sampling and word accuracy is improved as
well.
ACKNOWLEDGMENT
The authors would like to thank M. Rahim, J. Wilpon, and
R. Cox for their continued support on this research topic. They
would also like to thank G. Tur and M. Saraclar for their tech-
nical help and useful discussions.
REFERENCES
[1] L. Rabiner and B. Juang, Fundamentals of Speech Recognition. En-
glewood Cliffs, NJ: Prentice-Hall, 1993.
[2] F. Jelinek, Statistical Methods for Speech Recognition. Cambridge,
MA: MIT Press, 1997.
[3] V. Digalakis, D. Rtischev, and L. G. Neumeyer, ‚ÄúSpeaker adaptation
using constrained estimation of Gaussian mixtures,‚Äù IEEE Trans. Speech
Audio Processing, vol. 3, no. 5, 1995.
[4] C. J. Leggetter and P. C. Woodland, ‚ÄúMaximum likelihood linear re-
gression for speaker adaptation of continuous density hidden Markov
models,‚Äù Comput. Speech Lang., vol. 9, no. 2, 1995.
[5] M. Federico, ‚ÄúBayesian estimation methods for n-gram language model
adaptation,‚Äù in Proc. Int. Conf. Speech and Language Processing,
Philadelphia, PA, 1996, pp. 240‚Äì243.
[6] G. Riccardi and A. L. Gorin, ‚ÄúStochastic language adaptation over time
and state in a natural spoken dialog system,‚Äù IEEE Trans. Speech Audio
Processing, vol. 8, no. 1, pp. 3‚Äì9, 2000.
[7] R. Gretter and G. Riccardi, ‚ÄúOn-line learning of language models with
word error probability distributions,‚Äù in Proc. IEEE Int. Conf. Acoustics,
Speech, Signal Processing, 2001.
[8] T. Kemp and A. Waibel, ‚ÄúLearning to recognize speech by watching
television,‚Äù IEEE Intell. Syst., pp. 51‚Äì58, 1999.
[9] G. Zavaliagkos and T. Colthurst, ‚ÄúUtilizing untranscribed training data
to improve performance,‚Äù in Proc. Broadcast News Transcription and
Understanding Workshop, 1998, pp. 301‚Äì305.
[10] A. Stolcke, ‚ÄúError modeling and unsupervised language modeling,‚Äù in
Proc. 2001 NIST Large Vocabulary Conversational Speech Recognition
Workshop, Linthicum, MD, 2001.
[11] D. Cohn, L. Atlas, and R. Ladner, ‚ÄúImproving generalization with active
learning,‚Äù Mach. Learn., vol. 15, pp. 201‚Äì221, 1994.
[12] D. D. Lewis and J. Catlett, ‚ÄúHeterogeneous uncertainty sampling for
supervised learning,‚Äù in Proc. 11th Int. Conf. Machine Learning, 1994,
pp. 148‚Äì156.
[13] I. Dagan and S. P. Engelson, ‚ÄúCommittee-based sampling for training
probabilistic classifiers,‚Äù in Proc. 12th Int. Conf. Machine Learning,
1995, pp. 150‚Äì157.
[14] I. A. Muslea, ‚ÄúActive Learning with Multiple Views,‚Äù Ph.D. dissertation,
Univ. Southern California, Los Angeles, 2000.
[15] C. Thompson, M. E. Califf, and R. J. Mooney, ‚ÄúActive learning for nat-
ural language parsing and information extraction,‚Äù in Proc. 16th Int.
Conf. Machine Learning, 1999, pp. 406‚Äì414.
[16] M. Tang, X. Luo, and S. Roukos, ‚ÄúActive learning for statistical nat-
ural language parsing,‚Äù in Proc. 40th Anniversary Meeting of Associa-
tion for Computational Linguistics (ACL-02), Philadelphia, PA, 2002,
pp. 120‚Äì127.
[17] R. Hwa, ‚ÄúOn minimizing training corpus for parser acquisition,‚Äù in Proc.
5th Computational Natural Language Learning Workshop, San Fran-
cisco, CA, 2001, pp. 84‚Äì89.
[18] R. Liere, ‚ÄúActive Learning with Committees: An Approach to Efficient
Learning in Text Categorization Using Linear Threshold Algorithms,‚Äù
Ph.D. dissertation, Oregon State Univ., Portland, 2000.
[19] D. Hakkani-T√ºr, G. Riccardi, and A. Gorin, ‚ÄúActive learning for au-
tomatic speech recognition,‚Äù in Proc. Int. Conf. Acoustics, Speech and
Signal Processing (ICASSP), Orlando, FL, 2002, pp. 3904‚Äì3907.
[20] G. Riccardi and D. Hakkani-T√ºr, ‚ÄúActive and unsupervised learning for
automatic speech recognition,‚Äù in Proc. EUROSPEECH, 2003.
[21] G. Tur, R. E. Schapire, and D. Hakkani-T√ºr, ‚ÄúActive learning for spoken
language understanding,‚Äù in Proc. ICASSP, Hong Kong, May 2003.
[22] T. M. Kamm and G. G. L. Meyer, ‚ÄúSelective sampling of training data
for speech recognition,‚Äù in Proc. Human Language Technology Conf.,
San Diego, CA, 2002.
[23] D. Critchlow, Metric Methods for Analyzing Partially Ranked
Data. New York: Springer-Verlag, 1980.
[24] G. Lebanon and J. Lafferty, ‚ÄúCranking: Combining rankings using con-
ditional probability models on permutations,‚Äù in Proc. 19th Int. Conf.
Machine Learning, 2002.
[25] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby, ‚ÄúImproving general-
ization with active learning,‚Äù Mach. Learn., vol. 15, pp. 201‚Äì221, 1994.
[26] J. L. Gauvain and C. H. Lee, ‚ÄúMaximum a posteriori estimation for mul-
tivariate Gaussian mixture observations of Markov chains,‚Äù IEEE Trans.
Speech Audio Processing, vol. 2, no. 2, pp. 291‚Äì298, 1994.
8 IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 13, NO. 4, JULY 2005
[27] R. C. Rose, B. H. Juang, and C. H. Lee, ‚ÄúA training procedure for veri-
fying string hypotheses in continuous speech recognition,‚Äù in Proc. IEEE
Int. Conf. Acoustics, Speech and Signal Processing, 1995, pp. 281‚Äì284.
[28] L. Mangu, E. Brill, and A. Stolcke, ‚ÄúFinding consensus in speech recog-
nition: Word error minimization and other applications of confusion net-
works,‚Äù Comput. Speech Lang., vol. 14, no. 4, pp. 373‚Äì400, 2000.
[29] D. Falavigna, R. Gretter, and G. Riccardi, ‚ÄúAcoustic and word lattice
based algorithms for confidence scores,‚Äù in Proc. Int. Conf. Spoken Lan-
guage Processing, 2002.
[30] R. Zhang and A. Rudnicky, ‚ÄúWord level confidence annotation using
combinations of features,‚Äù in Proc. 7th Eur. Conf. Speech Communica-
tion and Technology, 2001, pp. 2105‚Äì2108.
[31] D. Hakkani-T√ºr and G. Riccardi, ‚ÄúA general algorithm for word graph
matrix decomposition,‚Äù in Proc. ICASSP, Hong Kong, 2003.
[32] R. De Mori, Spoken Dialogues with Computers. New York: Academic,
1998.
[33] A. Gorin, A. Abella, T. Alonso, G. Riccardi, and J. H. Wright, ‚ÄúAuto-
mated natural spoken dialog,‚Äù IEEE Computer, vol. 35, pp. 51‚Äì56, 2002.
[34] G. Riccardi, R. Pieraccini, and E. Bocchieri, ‚ÄúStochastic automata for
language modeling,‚Äù Comput. Speech Lang., vol. 10, pp. 265‚Äì293, 1996.
Giuseppe Riccardi (M‚Äô96-SM‚Äô04) received the
Laurea degree in electrical engineering and Master
degree in information technology, in 1991, from the
University of Padua and CEFRIEL Research Center,
respectively. In 1995, he received the Ph.D. degree
in electrical engineering from the Department of
Electrical Engineering, University of Padua.
From 1990 to 1993, he collaborated with Alcatel-
Telettra Research Laboratories Milan, Italy, and in-
vestigated algorithms for speech and audio coding
for medium-low bit rates for the half-rate GSM stan-
dard (New European Digital Trunking System). In 1993, he spent two years
as research fellow at AT&T Bell Laboratories investigating automata learning
for stochastic language modeling for speech recognition and understanding. In
1996, he joined AT&T Labs-Research where he is currently Technology Leader.
His research on stochastic finite state machines for speech and language pro-
cessing has been applied to a wide range of domains for task automation. He
participated at the creation of the state-of-the-art AT&T spoken language system
used in the 1994 DARPA ATIS evaluation. He has been pioneering the speech
and language research in spontaneous speech within the ‚ÄúHow May I Help You?‚Äù
research program. His research on learning finite state automata and transducers
has lead to the creation of the first large scale finite state chain decoding for ma-
chine translation (Anuvaad). He has co-authored more than 60 papers in the field
of speech and audio coding, speech recognition and understanding and machine
translation. His current research interests are stochastic language modeling, lan-
guage understanding, spoken dialogue, language acquisition and machine trans-
lation. He is on the Editorial Board of the ACM Transactions of Speech and Lan-
guage.
Dr. Riccardi has been on the scientific committees of EUROSPEECH,
ICASSP, and ACL. He co-organized the IEEE ASRU Workshop in 1993,
1999, and 2001. Dr. Riccardi is the Guest Editor of the IEEE Special Issue on
Speech-to-Speech Machine Translation. He is senior member of ACL and the
New York Academy of Science.
Dilek Hakkani-T√ºr (M‚Äô00) received the B.Sc. de-
gree in computer engineering from Middle East Tech-
nical University in 1994, and the M.Sc. and Ph.D. de-
grees from the Department of Computer Engineering,
Bilkent University, in 1996 and 2000, respectively.
Her Ph.D. thesis is on statistical language modeling
for agglutinative languages.
She worked on machine translation during
her visit to the Language Technologies Institute,
Carnegie Mellon University, Pittsburgh, PA, in 1997,
and to the Computer Science Department, Johns
Hopkins University, Baltimore, MD, in 1998. In 1998 and 1999, she visited
the Speech Technology and Research Labs, SRI International, and worked
on lexical and prosodic information for information extraction from speech.
Her research interests include natural language and speech processing, spoken
dialog systems, and machine learning. She co-authored more than 20 papers
in natural language and speech processing. She is a Senior Technical Staff
Member, AT&T Labs.-Research, Florham Park, NJ.
Dr. Hakkani-T√ºr is a member of the Association for Computational Linguis-
tics.

