THIS IS *VERY CLOSE* THE OFFICIAL VERSION, PUBLISHED AS: PROCEEDINGS OF THE IEEE, VOL. 9, NO 10, PP. 2009-2025, OCT. 1998 1
Blind signal separation: statistical principles
Jean-François Cardoso, C.N.R.S. and E.N.S.T.
cardoso@tsi.enst.fr and http://tsi.enst.fr/~cardoso.html
Abstract— Blind signal separation (BSS) and independent
component analysis (ICA) are emerging techniques of array
processing and data analysis, aiming at recovering unob-
served signals or ‘sources’ from observed mixtures (typically,
the output of an array of sensors), exploiting only the as-
sumption of mutual independence between the signals. The
weakness of the assumptions makes it a powerful approach
but requires to venture beyond familiar second order statis-
tics. The objective of this paper is to review some of the
approaches that have been recently developed to address
this exciting problem, to show how they stem from basic
principles and how they relate to each other.
Keywords— Signal separation, blind source separation, in-
dependent component analysis.
I. Introduction
Blind signal separation (BSS) consists in recovering un-
observed signals or ‘sources’ from several observed mix-
tures. Typically, the observations are obtained at the out-
put of a set of sensors, each sensor receiving a different
combination of the ‘source signals’. The adjective ‘blind’
stresses the fact that i) the source signals are not observed
and ii) no information is available about the mixture. This
is a sound approach when modeling the transfer from the
sources to the sensors is too difficult; it is unavoidable when
no a priori information is available about the transfer. The
lack of a priori knowledge about the mixture is compen-
sated by a statistically strong but often physically plausi-
ble assumption of independence between the source signals.
The so-called ‘blindness’ should not be understood nega-
tively: the weakness of the prior information is precisely
the strength of the BSS model, making it a versatile tool
for exploiting the ‘spatial diversity’ provided by an array
of sensors. Promising applications can already be found
in the processing of communications signals e.g. [24], [64],
[68], [6], biomedical signals1 like ECG [31] and EEG [51],
[47] monitoring [38], [36], or as an alternative to principal
component analysis, see e.g. [47], [10], [53], [7].
The simplest BSS model assumes the existence of n in-
dependent signals s1(t), . . . , sn(t) and the observation of as
many mixtures x1(t), . . . , xn(t), these mixtures being lin-
ear and instantaneous, i.e. xi(t) =
∑n
j=1 aijsj(t) for each
i = 1, n. This is compactly represented by the mixing
equation
x(t) = As(t) (1)
where s(t) = [s1(t), . . . , sn(t)]† is an n × 1 column vector
collecting the source signals, vector x(t) similarly collects
the n observed signals and the square n×n ‘mixing matrix’
A contains the mixture coefficients. Here as in the follow-
ing, † denotes transposition. The BSS problem consists in
1See the ICA page of the CNL group at
http://www.cnl.salk.edu/∼tewon/ica cnl.html for [1] several
biomedical applications.
 s1...
sn
 = s - A -x B -y =
 y1...
yn
 = ŝ
Fig. 1. Mixing and separating. Unobserved signals: s; observations:
x, estimated source signals: y.
Fig. 2. Outputs y1(t) (top row) and y2(t) (bottom row) when us-
ing the separating matrix obtained after adaptation based on
0, 50, 100, 150, 200 samples of a 2 × 2 mixture of constant mod-
ulus signals. Each subplot is in the complex plane: the cluster-
ing around circles shows the restoration of the constant modulus
property.
recovering the source vector s(t) using only the observed
data x(t), the assumption of independence between the en-
tries of the input vector s(t) and possibly some a priori
information about the probability distribution of the in-
puts. It can be formulated as the computation of an n× n
‘separating matrix’ B whose output y(t)
y(t) = Bx(t) (2)
is an estimate of the vector s(t) of the source signals.
Figure 2 shows an example of adaptive separation of
(real) digital communications signals: a two-sensor array
collects complex-valued noisy mixtures of two ‘sources sig-
nals’ which both have a constant modulus envelope. Suc-
cessful separation upon adaptation is evidenced by the
restoration of the constant modulus at each output. In
figure 2, the underlying BSS algorithm optimizes a cost
function composed of two penalty terms: one for correla-
tion between outputs and one for deviation of the modulus
from a constant value. This example introduces several
points to be developed below:
• A penalty term involving only pairwise decorrelation
(second order statistics) would not lead to separation:
source separation must go beyond second-order statistics
(see section II);
• Source separation can be obtained by optimizing a ‘con-
trast function’ i.e. a scalar measure of some ‘distributional
property’ of the output y. The constant modulus prop-
erty is very specific; more general contrast functions are
based on other measures: entropy, mutual independence,
2 THIS IS *VERY CLOSE* THE OFFICIAL VERSION, PUBLISHED AS: PROCEEDINGS OF THE IEEE, VOL. 9, NO 10, PP. 2009-2025, OCT. 1998
high-order decorrelations, divergence between the joint dis-
tribution of y and some model,. . . . Contrast functions are
discussed in sec. III where we show how they relate to each
other and can be derived from the maximum likelihood
principle.
• Fast adaptation is possible, even with simple algorithms
(see secs. IV and V) and blind identification can be accu-
rate even with a small number of samples (see sec. VI on
performance analysis).
The basic BSS model can be extended in several direc-
tions. Considering, for instance, more sensors than sources,
noisy observations, complex signals and mixtures, one ob-
tains the standard narrow band array processing/beam-
forming model. Another extension is to consider convolu-
tive mixtures: this results in a multichannel blind decon-
volution problem. These extensions are of practical impor-
tance, but this paper is restricted to the simplest model:
real signals, as many sensors as sources, non-convolutive
mixtures, noise free observations because it captures the
essence of the BSS problem and because our objective is to
present the basic statistical ideas, focusing on principles.
Some pointers are nonetheless provided in the last section
to papers addressing more general models.
The paper is organized as follows: section II dis-
cusses blind identifiability; section III and IV present con-
trast functions and estimating functions, starting from
information-theoretic ideas and moving to suboptimal high
order approximations; adaptive algorithms are described in
section V; section VI addresses some performance issues.
II. Can it be done? Modeling and identifiability.
When is source separation possible? To which extent can
the source signals be recovered? What are the properties
of the source signals allowing for partial or complete blind
recovery? These issues are addressed in this section.
A. The BSS model
Source separation exploits primarily ‘spatial diversity’,
that is the fact that different sensors receive different mix-
tures of the sources. Spectral diversity, if it exists, could
also be exploited but the approach of source separation
is essentially ‘spatial’: looking for structure across the sen-
sors, not across time. The consequence of ignoring any time
structure is that the information contained in the data is
exhaustively represented by the sample distribution of the
observed vector x (as graphically depicted in fig. 3 for in-
stance). Then, BSS becomes the problem of identifying the
probability distribution of a vector x = As given a sample
distribution. In this perspective, the statistical model has
two components: the mixing matrix A and the probability
distribution of the source vector s.
• Mixing matrix. The mixing matrix A is the parameter of
interest. Its columns are assumed to be linearly indepen-
dent (see [14] for the discussion of a more general case) so
that it is invertible.
There is something special about having an invertible ma-
trix as the unknown parameter, because matrices represent
linear transformations. Indeed, model (1) is a particular in-
stance of a transformation model. Furthermore, the set of
all n × n invertible matrices forms a multiplicative group.
This simple fact has a profound impact on source sepa-
ration because it allows to design algorithms with uniform
performance i.e. whose behavior is completely independent
of the particular mixture (sec. V-A and sec. VI-C).
• Source distribution. The probability distribution of each
source is a ‘nuisance parameter’: it means that we are not
primarily interested in it, even though knowing or estimat-
ing these distributions is necessary to estimate efficiently
the parameter of interest. Even if we say nothing about
the distribution of each source, we say a lot about their
joint distribution by the key assumption of mutual source
independence. If each source i = 1, n is assumed to have
a probability density function (pdf) denoted qi(·), the in-
dependence assumption has a simple mathematical expres-
sion: the (joint) pdf q(s) of the source vector s is:
q(s) = q1(s1)× · · · × qn(sn) =
∏
i=1,n
qi(si). (3)
i.e. it is the product of the densities for all sources (the
‘marginal’ densities). Source separation techniques differ
widely by the (explicit or implicit) assumptions made on
the individual distributions of the sources. There is a whole
range of options:
1. The source distributions are known in advance.
2. Some features are known (moments, heavy tails,
bounded support,. . . )
3. They belong to a parametric family.
4. No distribution model is available.
A priori, the stronger the assumption, the narrower the ap-
plicability. However, well designed approaches are in fact
surprisingly robust even to gross errors in modeling the
source distributions, as shown below. For ease of exposi-
tion, zero mean sources are assumed throughout:
Es = 0 i.e. Esi = 0 1 ≤ i ≤ n. (4)
B. Blind identifiability
The issue of blind identifiability is to understand to
which extent matrix A is determined from the sole distri-
bution of the observed vector x = As. The answer depends
on the distribution of s and on what is known about it.
A square matrix is said to be non-mixing if it has one
and only one non-zero entry in each row and each column.
If C is non-mixing then y = Cs is a copy of s i.e. its entries
are identical to those of s up to permutations and changes
of scales and signs. Source separation is achieved if such a
copy is obtained. When the distribution of s is unknown,
one cannot expect to do any better than signal copy but the
situation is a bit different if some prior information about
the distribution of s is available: if the sources have distinct
distributions, a possible permutation can be detected; if
the scale of a given source is known, the amplitude of the
corresponding column of A can be estimated, etc. . .
Some intuition about identifiability can be gained by con-
sidering simple examples of 2 × 2 mixing. Each row of
figure 3 shows (sample) distributions of a pair (s1, s2) of
independent variables after various linear transforms. The
columns successively show (s1, s2), (s2, s1), (−s1,−s2) and
J.-F. CARDOSO, BLIND SIGNAL SEPARATION: STATISTICAL PRINCIPLES 3
Fig. 3. Sample distributions of (x1, x2) when x = As for 5 differ-
ent transformation matrices A, and 3 pairs of distributions for
(s1, s2). From left to right: the identity transform, permuta-
tion of the sources, sign change, a π/3 rotation, a ‘generic’ linear
transform.
the effect of a π/3 rotation and of a nondescript linear
transform. Visual inspection of the transformed distribu-
tion compared to the original one gives a feeling of how well
the transform matrix A can be identified based on the ob-
servation of a mixture. The first row of fig. 3 shows a case
where the second column of A can be identified only up to
sign because s2 is symmetrically distributed about the ori-
gin (and therefore has the same distribution as −s2). The
second row shows a more severe indetermination: there, s1
and s2 have the same symmetric distribution, the trans-
form can be determined only up to arbitrary changes of
sign and a permutation. The last row shows the most se-
vere case: there s1 and s2 are normally distributed with
equal variance so that their joint distribution is invariant
under rotation.
These simple examples suggest that A can be blindly
identified indeed —possibly up to some indeterminations
induced by the symmetries in the distribution of the source
vector— in the case of known source distributions. How-
ever, this knowledge is not necessary: the eye certainly can
capture the distortion in the last columns of figure 3 even
without reference to the undistorted shapes in first column.
This is because the graphical ‘signature of independence’
(the pdf shape in the first column) clearly appears as dis-
torted in the last colum. This intuition is supported by
the following statement (adapted from Comon [26] after a
theorem of Darmois. See also [14]). For a vector s of inde-
pendent entries with at most one Gaussian entry and for
any invertible matrix C, if the entries of y = Cs are inde-
pendent, then y is a copy of s (C is non-mixing). Thus,
unless a linear transform is non-mixing, it turns a vector
of independent entries (at most one being Gaussian) into
a vector whose entries are not independent. This is a key
result because it entails that blind signal separation can be
achieved by restoring statistical independence. This is not
only a theoretical result about blind identifiability: it also
suggests that BSS algorithms could be devised by maxi-
mizing the independence between the outputs of a separat-
ing matrix. Section III shows that the maximum likelihood
principle does support this idea and leads to a specific mea-
sure of independence.
- - - -
s x z y
A W U
B
Fig. 4. Decorrelation leaves an unknown rotation.
Independence and decorrelation. Blind separation can be
based on independence but independence can not be re-
duced to the simple decorrelation conditions that Eyiyj = 0
for all pairs 1 ≤ i 6= j ≤ n. This is readily seen from the
fact that there are, by symmetry, only n(n − 1)/2 such
conditions (one for each pair of sources) while there are n2
unknown parameters.
Second order information (decorrelation), however, can
be used to reduce the BSS problem to a simpler form. As-
sume for simplicity that the source signals have unit vari-
ance so that their covariance matrix is the identity matrix:
Ess† = I; vector s is said to be spatially white. Let W be a
‘whitening matrix’ for x, that is z , Wx is spatially white.
The composite transform WA necessarily is a rotation ma-
trix because it relates two spatially white vectors s and
z = WAs. Therefore, ‘whitening’ or ‘sphering’ the data
reduces the mixture to a rotation matrix. It means that
a separating matrix can be found as a product B = UW
where W is a whitening matrix and U is a rotation matrix.
Note that any further rotation of z into y = Uz preserves
spatial whiteness, so that two equivalent approaches to ex-
ploiting source decorrelation are i) find B as B = UW with
W a spatial whitener and U a rotation or ii) find B under
the whiteness constraint: Eyy† = I. For further reference,
we write the whiteness constraint as
EHw(y) = 0 where Hw(y) , yy† − I. (5)
Spatial whiteness imposes n(n + 1)/2 constraints, leaving
n(n − 1)/2 unknown (rotation) parameters to be deter-
mined by other than second order information: second or-
der information is able to do ‘about half the BSS job’.
The prewhitening approach is sensible from an algorith-
mic point of view but it is not necessarily statistically ef-
ficient (see sec. VI-B). Actually, enforcing the whiteness
constraint amounts to believe that second order statistics
are ‘infinitely more reliable’ than any other kind of statis-
tics. This is, of course, untrue.
C. Likelihood
This section examines in a simple graphical way the like-
lihood of source separation models. The likelihood, in a
given model, is the probability of a data set as a function
of the parameters of the model. The simple model x = As
for vector x discussed in sec. II-A is parameterized by the
pair (A, q) made of the mixing matrix A and of the density
q for the source vector s. The density of x = As for a given
pair (A, q) is classically given by
p(x;A, q) = |detA|−1q(A−1x) (6)
If T samples X1:T , [x(1), . . . ,x(T )] of x are modeled as
independent, then p(X1:T ) = p(x(1))×· · ·×p(x(T )). Thus
4 THIS IS *VERY CLOSE* THE OFFICIAL VERSION, PUBLISHED AS: PROCEEDINGS OF THE IEEE, VOL. 9, NO 10, PP. 2009-2025, OCT. 1998
the normalized (i.e. divided by T ) log-likelihood of X1:T
for the parameter pair (A, q) is
1
T
log p(X1:T ;A, q) =
1
T
T∑
t=1
log q(A−1x(t))− log |detA|.
(7)
Figures 5 to 7 show the ‘likelihood landscape’ when A is
varied while q is kept fixed. For each figure, T = 1000
independent realizations of s = [s1, s2]† are drawn accord-
ing to some pdf r(s) = r1(s1)r2(s2) and are mixed with
a 2 × 2 matrix A to produce T samples of x. Therefore,
this data set X1:T follows exactly model (1) with a ‘true
mixing matrix’ A and a ‘true source distribution’ r(s). The
figures show the log-likelihood when A is varied around its
true value A while model density q(s) is kept fixed. These
figures illustrate the impact of the choice of a particular
model density.
In each of these figures, the matrix parameter A is var-
ied in two directions in matrix space according to A =
AM(u, v) where M(u, v) is the matrix
M(u, v) ,
[
coshu sinhu
sinhu coshu
]
·
[
cos v − sin v
sin v cos v
]
. (8)
This is just a convenient way to generate a neighborhood
of the identity matrix. For small u and v:
M(u, v) ≈ I + u
[
0 1
1 0
]
+ v
[
0 −1
1 0
]
. (9)
Therefore u and v are called symmetric and skew-
symmetric parameters respectively. Each one controls a
particular deviation of M(u, v) away from the identity.
In fig. 5, the true source distributions r1 and r2 are uni-
form on [−1,+1] but the model takes q1 and q2 to be each
a mixture of two normal distributions with same variance
but different means (as in second column of fig. 11). True
and hypothesized sample distributions of s = (s1, s2) are
displayed in upper left and right corners of the plot. Even
though an incorrect model is used for the source distribu-
tion: q 6= r, the figure shows that the likelihood is maximal
around (u, v) = (0, 0) i.e. the most likely mixing matrix
given the data and the model is close to A.
In fig. 6, the true sources are ‘almost binary’ (see up-
per left corner) but a Gaussian model is used: q1(s) =
q2(s) ∝ exp−s2/2. The figure shows that the likelihood
of A = AM(u, v) does not depend on the skew-symmetric
parameter v, again evidencing the insufficiency of Gaussian
modelling.
In fig. 7, the source are modeled as in fig. 5 but the
true (and identical) source distributions r1 and r2 now are
mixtures of normal distributions with the same mean but
different variances (as in second column of fig. 11). A dis-
aster happens: the likelihood is no longer maximum for A
in the vicinity of A. Actually, if the value Â of A maximiz-
ing the likelihood is used to estimate the source signals as
y = ŝ = Âx, one obtains maximally mixed sources! This
is explained in section III-A and fig. 8.
The bottom line of this informal study is the necessity of
non-Gaussian modeling (fig. 6); the possibility of using only
−0.4
0
0.4
−pi/4
  0  
 pi/4
−7
−6.5
−6
−5.5
−5
−4.5
−4
−3.5
sym
met
ric
skew−symmetric
lo
g 
lik
el
ih
oo
d
True distribution Model distribution
Fig. 5. Log-likelihood with a slightly misspecified model for source
distribution: maximum is reached close to the true value.
−0.3
0
0.3
−pi/4
  0  
 pi/4
−5.1
−5
−4.9
−4.8
−4.7
−4.6
sym
met
ric
skew−symmetric
lo
g 
lik
el
ih
oo
d
True distribution Model distribution
Fig. 6. Log-likelihood with a Gaussian model for source distribution:
no ‘contrast’ in the skew-symmetric direction.
an approximate model of the sources (fig. 5); the existence
of a limit to the misspecification of the source model (fig. 7).
How wrong can the source distribution model be? This is
quantified in section VI-A.
III. Contrast functions
This section introduces ‘contrast functions’ which are ob-
jective functions for source separation. The maximum like-
lihood principle is used as a starting point, suggesting sev-
eral information-theoretic objective functions (sec. III-A)
which are then shown to be related to another class of ob-
jective functions based on high-order correlations (sec. III-
B).
Minimum contrast estimation is a general technique of
statistical inference [58] which encompasses several tech-
niques like maximum likelihood or least squares. It is rel-
evant for blind deconvolution (see the inspiring paper [37]
and also [12]) and has been introduced in the related BSS
problem by Comon [26]. In both instances, a contrast func-
tion is a real function of a probability distribution. To deal
with such functions, a special notation will be useful: for x
a given random variable, f(x) generically denotes a func-
tion of x while f [x] denotes a function of the distribution of
J.-F. CARDOSO, BLIND SIGNAL SEPARATION: STATISTICAL PRINCIPLES 5
−0.3
0
0.3
−pi/4
  0  
 pi/4
−11
−10.5
−10
−9.5
−9
−8.5
sym
met
ric
skew−symmetric
lo
g 
lik
el
ih
oo
d
True distribution Model distribution
Fig. 7. Log-likelihood with a widely misspecified model for source
distribution: maximum is reached for a mixing system.
x. For instance, the mean value of x is denoted m[x] , Ex.
Contrast functions for source separation (or ‘contrasts’,
for short) are generically denoted φ[y]. They are real val-
ued functions of the distribution of the output y = Bx
and they serve as objectives: they must be designed in
such a way that source separation is achieved when they
reach their minimum value. In other words, a valid contrast
function should, for any matrix C, satisfy φ[Cs] ≥ φ[s] with
equality only when y = Cs is a copy of the source signals.
Since the mixture can be reduced to a rotation matrix by
enforcing the whiteness constraint Eyy† = I (sect. II-B),
one can also consider ‘orthogonal contrast functions’: these
are denoted φ◦[y] and must be minimized under the white-
ness constraint Eyy† = I.
A. Information theoretic contrasts
The maximum likelihood (ML) principle leads to several
contrasts which are expressed via the Kullback divergence.
The Kullback divergence between two probability density
functions f(s) and g(s) on Rn is defined as
K(f |g) ,
∫
s
f(s) log
(
f(s)
g(s)
)
ds (10)
whenever the integral exists [28]. The divergence between
the distributions of two random vectors w and z is con-
cisely denoted K[w|z]. An important property of K is that
K[w|z] ≥ 0 with equality if and only if w and z have the
same distribution. Even though K is not a distance (it is
not symmetric), it should be understood as a ‘statistical
way’ of quantifying the closeness of two distributions.
A.1 Matching distributions: likelihood and infomax
The likelihood landscapes displayed in figures 5-7 as-
sumes a particular pdf q(·) for the source vector. Denot-
ing s a random vector with distribution q, simple calculus
shows that
1
T
log p(X1:T ;A, q)
T→∞−→ −K[A−1x|s] + cst. (11)
True distribution Hypothesized distribution Estimated distribution
Fig. 8. How the maximum likelihood estimator is misled.
Therefore, figures 5-7 approximately display (up to a con-
stant term) minus the Kullback divergence between the dis-
tribution of y = A−1x and the hypothesized distribution
of the sources. This shows that the maximum likelihood
principle is associated with a contrast function
φML[y] = K[y|s] (12)
and the normalized log-likelihood can be seen, via (11) as
an estimate of −K[y|s] (up to a constant). The ML prin-
ciple thus says something very simple when applied to the
BSS problem: find matrix A such that the distribution of
A−1x is as close as possible (in the Kullback divergence) to
the hypothesized distribution of the sources.
The instability problem illustrated by fig. 7 may now
be understood as follows: in this figure, the likelihood is
maximum when M(u, v) is a ±π/4 rotation because the
true source distribution is closer to the hypothesized source
distribution after it is rotated by ±π/4. As figure 8
shows, after such a rotation the areas of highest density
of y correspond to the points of highest probability of the
hypothesized source model.
A different approach to derive the contrast function (12)
is very popular among the neural network community. De-
note gi(·) the distribution function
gi(s) ,
∫ s
−∞
qi(t)dt ∈ [0, 1] 1 ≤ i ≤ n (13)
so that g′i = qi and denote g(s) = [g1(s1), . . . , gn(sn)]
†. An
interpretation of the infomax principle (see[9], [55], and
references therein) suggests the contrast function
φIM [y] , −H[g(y)] (14)
where H[·] denotes the Shannon entropy (for a random vec-
tor u with density p(u), this is H[u] = −
∫
p(u) log p(u)du
with the convention 0 log 0 = 0). This idea can be under-
stood as follows: on one hand, g(s) is uniformly distributed
on [0, 1]n if s has pdf q; on the other hand, the uniform dis-
tribution has the highest entropy among all distributions
on [0, 1]n [28]. Therefore g(Cs) has the highest entropy
when C = I. The infomax idea, however, yields the same
contrast as the likelihood i.e. in fact φIM [y] = φML[y].
The connection between maximum likelihood and infomax
was noted by several authors (see [57], [19], [50]).
A.2 Matching the structure: mutual information
The simple likelihood approach described above is based
on a fixed hypothesis about the distribution of the sources.
This becomes a problem if the hypothesized source distri-
butions differ too much from the true ones, as illustrated by
6 THIS IS *VERY CLOSE* THE OFFICIAL VERSION, PUBLISHED AS: PROCEEDINGS OF THE IEEE, VOL. 9, NO 10, PP. 2009-2025, OCT. 1998
fig. 7 and 8. This remark suggests that the observed data
should be modeled by adjusting both the unknown system
and the distributions of the sources. In other words, one
should minimize the divergence K[y|s] with respect to A
(via the distribution of y = A−1x) and with respect to the
model distribution of s. The last minimization problem
has a simple and intuitive theoretical solution. Denote ỹ
a random vector with i) independent entries and ii) each
entry distributed as the corresponding entry of y. A classic
property (see e.g. [28]) of ỹ is that
K[y|s] = K[y|ỹ] + K[ỹ|s] (15)
for any vector s with independent entries. Since K[y|ỹ]
does not depend on s, eq. (15) shows that K[y|s] is mini-
mized in s by minimizing its second term i.e. K[ỹ|s]; this
is simply achieved by taking s = ỹ for which K[ỹ|s] = 0
so that mins K[y|s] = K[y|ỹ]. Having minimized the like-
lihood contrast K[y|s] with respect to the source distribu-
tion, leading to K[y|ỹ], our program is completed if we
minimize the latter with respect to y, i.e. if we minimize
the contrast function
φMI [y] , K[y|ỹ]. (16)
The Kullback divergence K[y|ỹ] between a distribution
and the closest distribution with independent entries is tra-
ditionally called the mutual information (between the en-
tries of y). It satisfies φMI [y] ≥ 0 with equality if and only
if y is distributed as ỹ. By definition of ỹ, this happens
when the entries of y are independent. In other words,
φMI [y] measures the independence between the entries of
y. Thus, mutual information apears as the quantitative
measure of independence associated to the maximum like-
lihood principle.
Note further that K[ỹ|s] =
∑n
i=1 K[yi|si] (because both
ỹ and s have independent entries). Therefore,
φML[y] = φMI [y] +
n∑
i=1
K[yi|si] (17)
so that the decomposition (15) or (17) of the ‘global’ dis-
tribution matching criterion φML[y] = K[y|s] should be
understood as(
Total
mismatch
)
=
(
Deviation from
independence
)
+
(
Marginal
mismatch
)
.
Therefore, maximizing the likelihood with fixed assump-
tions about the distributions of the sources amounts to
minimize a sum of two terms: the first term is the true ob-
jective (mutual information as a measure of independence)
while the second term measures how far the (marginal) dis-
tributions of the outputs y1, . . . , yn are from the assumed
distributions.
A.3 Orthogonal contrasts
If the mixing matrix has been reduced to a rotation ma-
trix by whitening, as explained in sect. II-B, contrast func-
tions like φML or φMI can still be used. The latter takes an
k1 = −1.13
k2 = −1.07
k1 = −1.07
k2 = −0.94
k1 = −0.89
k2 = −0.71
k1 = −0.70
k2 = −0.50
k1 = −0.62
k2 = −0.44
Fig. 9. Gaussianization by mixing. Histograms of y1 (top row) and y2
(bottom row) when y rotated by απ/4 for α = 0, 1/4, 1/2, 3/4, 1.
Each subplot also shows the estimated kurtosis k1 and k2 (defined
at eq. (21)) decreasing (in absolute value) upon mixing.
interesting alternative form under the whiteness constraint
Eyy† = I: one can show then that φMI [y] is, up to a con-
stant term, equal to the sum of the Shannon entropies of
each output. Thus, under the whiteness constraint, mini-
mizing the mutual information between the entries of y is
equivalent to minimizing the sum of the entropies of the
entries of y and we define
φ◦MI [y] ,
∑
i
H[yi] (18)
There is a simple interpretation: mixing the entries of s
‘tends’ to increase their entropies; it seems natural to find
separated source signals as those with minimum marginal
entropies. It is also interesting to notice that −H[yi] is (up
to a constant) the Kullback divergence between the distri-
bution of yi and the zero-mean unit-variance normal dis-
tribution. Therefore, minimizing the sum of the marginal
entropies is also equivalent to driving the marginal dis-
tributions of y as far away as possible from normality.
Again, the interpretation is that mixing ‘tends’ to gaus-
sianize the marginal distributions so that a separating tech-
nique should go in the opposite direction. Figure 9 is a
visual illustration of the tendency to normality by mix-
ing. The first column shows histograms for two indepen-
dent variables s1 and s2 with a bimodal distribution and,
superimposed to it as a solid line, the best Gaussian ap-
proximation. The following columns shows the histograms
after rotations by steps of π/16, going from 0 to π/4 where
mixing is maximal. The tendency to normality is very ap-
parent.
The entropic form (18) of the mutual information was
used as starting point by Comon [26]; it remains a valid
contrast under the weaker constraint that B is a volume-
preserving transformation [56].
A.4 Discussion
The ‘canonical’ contrast for source separation is the mu-
tual information φMI because it expresses the key prop-
erty of source independence and nothing else: it does not
include any explicit or implicit assumption about the dis-
tributions of the sources. On the other hand, if the source
distributions are known, φML is more appropriate because
it expresses directly the fit between data and model. Also,
φML is easier to minimize because its gradient is easily
estimated (see eq. (31)) while estimating the gradient of
J.-F. CARDOSO, BLIND SIGNAL SEPARATION: STATISTICAL PRINCIPLES 7
φMI is computationally demanding [60]. Even when the
source distributions are unknown, one may use φML with
hypothesized source distributions which only need to be
‘close enough’ to the true distributions: recall sec. II-C for
a qualitative explanation and see sec. VI-A for a quanti-
tative statement and sec. V-B about adapting the model
distributions). Another approach is to approximate the
Kullback-based contrasts using high-order statistics, as ex-
amined next.
B. High order approximations
High order statistics can be used to define contrast func-
tions which are simple approximations to those derived
from the ML approach. High order information is most
simply expressed by using cumulants. The discussion being
limited to cumulants of order 2 and 4, only the following
definitions are needed. For zero-mean random variables
a, b, c, d, 2nd order cumulants are identical to 2nd order
moments Cum[a, b] , Eab and 4th order cumulants are
Cum[a, b, c, d] , Eabcd−EabEcd−EacEbd−EadEbc. (19)
Whenever the random variables a, b, c, d can be split in two
groups which are mutually independent, their cumulant is
zero. Therefore, independence beyond second-order decor-
relation can be easily tested using high order cumulants.
For simplicity, the following notation for the cumulants
of the elements of a given vector y is used throughout:
Cij [y] , Cum[yi, yj ], Cijkl[y] , Cum[yi, yj , yk, yl].
Since the source vector s has independent entries, all its
cross-cumulants vanish:
Cij [s] = σ2i δij Cijkl[s] = kiδijkl (20)
where δ is the Kronecker symbol and we have defined the
variance σ2i and the kurtosis ki of the i-th source as the
second and fourth order ‘auto-cumulants’ of si:
σ2i , Cii[s] = Es2i ki , Ciiii[s] = Es4i − 3E2s2i . (21)
The likelihood contrast φML[y] = K[y|s] is ‘the’ measure of
mismatch between output distribution and a model source
distribution. A cruder measure can be defined from the
quadratic mismatch between the cumulants:
φ2[y] ,
∑
ij
(Cij [y]− Cij [s])2 =
∑
ij
(
Cij [y]− σ2i δij
)2
φ4[y] ,
∑
ijkl
(Cijkl[y]− Cijkl[s])2 =
∑
ijkl
(Cijkl[y]− kiδijkl)2
Are φ2 and φ4 contrast functions as introduced in the be-
ginning of this section? Clearly φ2 is not a contrast because
φ2[y] = 0 expresses only the decorrelation between the en-
tries of y. On the contrary, one can show that φ4[y] is a
contrast if all the sources have known non-zero kurtosis.
Even though fourth order information is sufficient by it-
self to solve the BSS problem, it is interesting to use φ2
and φ4 in conjunction because they jointly provide an ap-
proximation to the likelihood contrast: if s and y are sym-
metrically distributed with distributions ‘close enough’ to
normal, then
K[y|s] ≈ φ24[y] ,
1
48
(12φ2[y] + φ4[y]) . (22)
Room is lacking to discuss the validity of this approxi-
mation (which stems from an Edgeworth expansion, see
sec. V-B). The point however is not to determine how
closely φ24[y] approximates K[y|s] but rather to follow the
suggestion that second and fourth order information could
be used jointly.
Orthogonal contrasts. We consider cumulant-based orthog-
onal contrasts. The orthogonal approach, which enforces
whiteness i.e. φ2[y] = 0, thus corresponds to replacing the
factor 12 in eq. (22) by an ‘infinite weight’ (optimal weight-
ing is considered in [20]; see also sec. V-B) or equivalently to
minimizing φ4[y] under the whiteness constraint φ2[y] = 0.
Simple algebra shows that, if φ2[y] = 0, then φ4[y] is equal
(up to a constant additive term) to
φ◦4[y] , −2
n∑
i=1
kiCiiii[y] = Ef4(y) (23)
where we have defined f4(y) , −2
∑n
i=1 ki(y
4
i − 3). This
is a pleasant finding: this contrast function being the ex-
pectation of a function of y, it is particularly simple to
estimate by a sample average.
Recall that the contrast function φML defined at eq. (12)
depends on a source model i.e. it is defined using an hypo-
thetical density q(·) for the source distribution. Similarly,
the fourth-order approximation φ◦4 requires an hypothesis
about the sources but it is only a ‘fourth-order hypothesis’
in the sense that only the kurtosis ki for each source must
be specified in definition (23). In the same manner as min-
imizing φML over the source distribution yields the mutual
information contrast φML, minimizing φ◦4[y] (which ap-
proximates φML) over the kurtosis ki of each source yields
an approximation to the mutual information. One finds
φMI [y] by
φ◦ICA[y] =
∑
ijkl 6=iiii
C2ijkl[y] = −
∑
i
C2iiii[y] + cst (24)
as such an orthogonal fourth-order approximation. This
was obtained first by Comon [26] (along a slightly differ-
ent route) and by Lacoume et al. [41] by approximating
the likelihood by a Gram-Charlier expansion. This con-
trast is similar to φMI also in that its first form involves
only terms measuring the (4th order) independence be-
tween the entries of y. Its second form stems from the
fact that
∑
ijkl C2ijkl[y] is constant if φ2[y] = 0 holds (see
e.g. [26]). It is also similar to (23) when ki ≈ Ciiii[y] which
is indeed the case close to separation.
One benefit of considering 4th-order orthogonal contrasts
like φ◦ICA is that they can be optimized by the Jacobi tech-
nique: the ‘unknown rotation’ (sec. II-B) can be found as
8 THIS IS *VERY CLOSE* THE OFFICIAL VERSION, PUBLISHED AS: PROCEEDINGS OF THE IEEE, VOL. 9, NO 10, PP. 2009-2025, OCT. 1998
−1 0 1
−2
0
2
4 k1 = −1,  k2 = −1.
−1 0 1
−2
0
2
4 k1 = −1,  k2 = −0.
−1 0 1
−2
0
2
4 k1 = −0.5,  k2 = 1.
Fig. 10. Variation of orthogonal contrast functions (Solid: φ◦ICA,
dash-dots: φ◦JADE , dashes: φ
◦
m, dots: φ
◦
4) when two sources
with kurtosis k1 and k2 are rotated between −π/2 and π/2. Left:
(k1, k2) = (−1,−1), center: (k1, k2) = (−1, 0), right: (k1, k2) =
(−0.5, 1).
sequence of 2× 2 rotations applied in sequence to all pairs
(yi, yj) for i 6= j with the optimal angle at each step be-
ing often available in close form. Comon [26] has such a
formula for φICA in the case of real signals.
Independence can also be tested on a smaller subset of
cross-cumulants with:
φ◦JADE[y] ,
∑
ijkl 6=ijkk
C2ijkl[y]. (25)
The motivation for using this specific subset is that φJADE
also is a ‘joint diagonalization’ criterion, entailing that it
can be optimized by Jacobi technique for which the rota-
tion angles can be found in close form even in the complex
case [23]. A similar technique is described in [32].
Simpler contrasts can be used if the kurtosis of the
sources are known. For instance, eq. (23) suggests, for
negative kurtosis (ki < 0 ∀i), a very simple contrast:
φ◦m[y] =
n∑
i=1
Ey4i (26)
(see [22], [54], [48], [45]) Actually, the condition that ki +
kj < 0 for all pairs of sources is sufficient for the stationary
points of this orthogonal contrast function to be locally
stable (see sec VI-A).
Some properties of the fourth-order contrasts discussed
above are illustrated by fig. 10 displaying the variation of
some orthogonal contrast functions in the two-source case:
a 2 × 1 source vector s with kurtosis (k1, k2) is rotated
into y by an angle θ ∈ (−π/2, π/2). On the left panel,
the sources have identical kurtosis k1 = k2 = −1: all the
four contrasts are minimized at integer multiples of π/2.
On the center panel, one source is Gaussian (k2 = 0): the
contrasts show smaller variations except for φ◦4. Note that
φ4 ‘knows’ that one source has zero kurtosis, thus distin-
guishing between even and odd multiplies of π/2. On the
right panel, k1 = −0.5 and k2 = 1 so that k1 + k2 > 0
which violates the condition for φ◦m to be a contrast: its
minima become maxima and vice versa. This is the same
phenomenon as illustrated by figure 7.
IV. Estimating functions
By design, all valid contrast functions reach their min-
ima at a separating point when the model holds; in this
sense, no one is better than another. In practice, how-
ever, contrasts are only estimated from a finite data set:
sample-based contrasts depend not on the distribution of
y but on its sample distribution. Estimation from a fi-
nite data set introduces stochastic errors depending on the
available samples and also on the contrast function. Thus a
statistical characterization of the minima of sample-based
contrast functions is needed and will provide a basis for
comparing contrast functions. For this purpose, the no-
tion of estimating function is introduced; it is also closely
related to gradient algorithms for BSS (sec. V-A).
A. Relative gradient
The variation of a contrast function φ[y] under a linear
transform of y is may be expressed by defining a ‘relative
gradient’. This specific notion builds on the fact that the
parameter of interest is a square matrix.
Definition. An infinitesimal transform of y is y → (I +
E)y = y + Ey where E is a ‘small’ matrix.
y - I + E - y + Ey
If φ is smooth enough, φ[y + Ey] can be expanded as
φ[y + Ey] = φ[y] +
n∑
i,j=1
GijEij + o(‖E‖) (27)
with Gij the partial derivative of φ[y + Ey] with respect
to Eij at E = 0. These coefficients form a n × n matrix,
denoted ∇φ[y], called the relative gradient [22] of φ[y] at
[y]. In matrix form, expansion (27) reads
φ[y + Ey] = φ[y] + 〈∇φ[y] | E〉+ o(‖E‖) (28)
where 〈·|·〉 is the Euclidean scalar product between matri-
ces: 〈M |N〉 = trace
(
MN†
)
=
∑n
i,j=1MijNij .
Note that the relative gradient is defined without explicit
reference to the possible dependence of y on B as y = Bx;
thus it actually characterizes the first order variation of the
contrast function itself. It is of course possible to relate
∇φ[y] to a ‘regular’ gradient with respect to B if y = Bx.
Elementary calculus yields
∇φ[Bx] = B† ∂φ[Bx]
∂B
. (29)
The notion of natural gradient was independently intro-
duced by Amari [2]. It is distinct in general from the rela-
tive gradient: the latter is defined in any continuous group
of transformation while the former is defined in any smooth
statistical model. However, for the BSS model which, as
a statistical transformation model combines both features,
the two ideas yield the same class of algorithms (sec. V-A).
Score functions. The source densities q1, . . . , qn used in (3)
and (7) to define the likelihood of a BSS model enter in the
estimating function via their log-derivatives: the so-called
‘score functions’ ϕ1, . . . , ϕn, defined as
ϕi , −(log qi)′ or ϕi(·) = −
qi(·)′
qi(·)
. (30)
J.-F. CARDOSO, BLIND SIGNAL SEPARATION: STATISTICAL PRINCIPLES 9
−2 0 2
0
0.2
0.4
−2 0 2
−5
0
5
−2 0 2
0
0.2
0.4
−2 0 2
−5
0
5
−2 0 2
0
0.5
1
−2 0 2
−2
0
2
Fig. 11. Some densities and their associated scores.
Figure 11 displays some densities and their associated score
functions. Note that the score for ‘the most basic distri-
bution’ is ‘the most basic function’: if s is a zero-mean
unit-variance Gaussian variable: q(s) = (2π)−1/2 exp− s
2
2 ,
then the associated score is ϕ(s) = s. Actually, Gaussian
densities precisely are these densities associated with lin-
ear score functions. Thus, the necessity of non Gaussian
modeling (recall section II) translates in the necessity of
considering non-linear score functions.
Relative gradient of the likelihood contrast. At the core of
the BSS contrast functions is φML[y] associated with the
likelihood given the source densities q1, . . . , qn. Its relative
gradient is found to be [62]
∇φML[y] = EHϕ(y) (31)
where Hϕ : Rn 7→ Rn×n is
Hϕ(y) , ϕ(y)y† − I (32)
with ϕ : Rn 7→ Rn the entry-wise non-linear function
ϕ(y) , [ϕ1(y1), . . . , ϕn(yn)]† (33)
collecting the score functions related to each source. This
is a remarkably simple result: this relative gradient merely
is the expected value of a fixed function Hϕ of y.
Interpretation. The ML contrast function φML is mini-
mum at points where its (relative) gradient cancels, i.e.
by (31), at these points which are solutions of the matrix
equation EHϕ(y) = 0. This is interpreted by examining
the (i, j)-th entry of this matrix equation. For i = j, we
find Eϕi(yi)yi = 1 which depends only on yi and deter-
mines the scale of the i-th source estimate. For i 6= j, the
(i, j)-th entry of EHϕ(y) = 0 reads Eϕi(yi)yj = 0 meaning
that the jth output yj should be uncorrelated to a non-
linear version ϕi(yi) of the ith output. Because ϕi and ϕj
are non-linear functions, the conditions for the pairs (i, j)
and the pair (j, i) are (in general) not equivalent. Note
that if the source signals are modeled as zero-mean unit-
variance normal variables, then ϕi(yi) = yi for all i and
Hϕ(y) = yy† − I = Hw(y) (recalling def. (5)). Then
φML[y] is minimum at points where Eyy† = I: we only
obtain the whiteness condition. Again, this is not sufficient
to determine a separating solution; score functions must be
non-linear (the source model must be non Gaussian).
The idea of using non-linear functions to obtain a suf-
ficient set of independence conditions can be traced back
to the seminal paper of Hérault and Jutten [44] (see [46]
for a reference in English) but the choice of the non-linear
functions was somewhat ad hoc; Féty [39] gave an inter-
pretation of the non-linear functions as ‘amplifiers’ for the
signals of interest; Bar-Ness also produced early work using
non-linear functions [8]. However, the ML principle makes
it clear that the non-linear functions are related via (30) to
a non-Gaussian model of the source distributions.
B. Estimating functions
An estimating function for the BSS problem is a function
H : Rn 7→ Rn×n. It is associated to an estimating equation
1
T
T∑
t=1
H(y(t)) = 0 (34)
thus called because, H being matrix-valued, equation (34)
specifies a priori as many constraints as unknown param-
eters in the BSS problem. Many BSS estimates can be
characterized via an estimating function [18], [3].
A simple instance of estimating function is Hw(y), used
in eq. (5) to express that decorrelation between the entries
of y. Equation (34) with H(y) = Hw(y) is equivalent to
1
T
∑
t y(t)y(t)
† = I i.e. it expresses the empirical white-
ness of a batch of T samples of y as opposed to the ‘ac-
tual’ whiteness i.e. EHw(y) = 0. The estimating function
Hw(y), however, is not appropriate for BSS, since whiten-
ing (or decorrelation) is not sufficient to determine a sepa-
rating matrix.
The simplest example of estimating function for BSS is
obtained in the ML approach. The gradient of the likeli-
hood (7) may be shown [62] to cancel at points AML which
are exactly characterized by eq. (34) with y = A−1MLx and
H = Hϕ as defined in (32). In other words, maximum
likelihood estimates correspond exactly to the solution of
an estimating equation. This equation is nothing but the
sample counterpart of EHϕ(y) = 0 which characterizes the
stationary points of φML[y]. Recall that the latter is ob-
tained (at eqs. (11) and (12)) as a limit of the log-likelihood.
Because the value of an estimating function is a square
matrix, it can be decomposed into a symmetric part (equal
to its transpose) and a skew symmetric part (opposite to
its transpose). This decomposition simply is
H(y) =
H(y) +H(y)†
2
+
H(y)−H(y)†
2
. (35)
If the optimization of some regular contrast function corre-
sponds to an estimating function H(y), it is found that the
optimization of the same contrast under the whiteness con-
straint corresponds to an estimating function H◦y) given
by
H◦(y) = Hw(y) +
1
2
(
H(y)−H(y)†
)
. (36)
Thus, the symmetric part of H(y) replaced by Hw(y) =
yy† − I, already introduced at eq. (5), whose effect is
to enforce the whiteness constraint. In particular, maxi-
mum likelihood estimates under the whiteness constraint
10 THIS IS *VERY CLOSE* THE OFFICIAL VERSION, PUBLISHED AS: PROCEEDINGS OF THE IEEE, VOL. 9, NO 10, PP. 2009-2025, OCT. 1998
are (again) solutions of eq. (34) with the estimating func-
tion H = H◦ϕ:
H◦ϕ(y) , yy
† − I + ϕ(y)y† − yϕ(y)† (37)
Other orthogonal contrast functions are associated to simi-
lar estimating functions. For instance, the simple 4th-order
contrasts φ◦4[y] and φ
◦
m[y] (eqs. (23) and (26) repsectively)
yield estimating equations in the form (37) with non-linear
functions respectively given by
ϕi(yi) = −kiy3i and ϕi(yi) = y3i (38)
Recall that using the contrast function (26) supposes
sources with negative kurtosis ki. Thus the two functions
in (38) ‘agree’ on the sign to be given to a cubic distortion
(as was to be expected).
Some contrast functions, like φ◦ICA and φ
◦
JADE , when
estimated from T samples are minimized at points which
cannot be represented exactly as the solution of (34) for
a fixed estimating function. However, one can often find,
as in [37], an ‘asymptotic’ estimating function in the sense
that the solution of the associated estimating equation is
very close to the minimizer of the estimated contrast. For
instance, the contrast φ◦ICA and φ
◦
JADE are asymptotically
associated to the same estimating function as φ◦4. This
implies that minimizing φ◦ICA, φ
◦
JADE or φ
◦
4 with cumu-
lants estimated from T samples yields estimates which are
equivalent (they differ by a term which is smaller than the
estimation error) for large enough T .
Which functions are appropriate as estimating functions?
One could think of using any H such that EH(s) = 0 as an
estimating function because the estimating equation (34)
would just be the sample counterpart of EH(y) = 0 and
would a priori provide as many scalar equations as un-
known parameters. However, the ML principle suggests
the specific forms (32) and (37) with the non-linear func-
tions in ϕ(y) being (approximations of) the score functions
for the probability densities of the signals to be separated.
V. Adaptive algorithms
A simple generic technique for optimizing an objec-
tive function is gradient descent. In most optimiza-
tion problems, its simplicity is at the expense of perfor-
mance: more sophisticated techniques —such as ‘Newton-
like’ algorithms using second derivatives in addition to the
gradient— can often significantly speed up convergence.
For the BSS problem, however, it turns out that a simple
gradient descent offers ‘Newton-like’ performance (see be-
low). This surprising and fortunate result is obtained by
descending along the relative gradient defined in sec. IV-A.
A. Relative gradient techniques
Relative gradient descent. We first describe a ‘generic’ rela-
tive gradient descent. Generally, the steepest descent tech-
nique of minimization consists in moving by a small step in
a direction opposite to the gradient of the objective func-
tion. The relative gradient of a contrast φ[y] is defined
(sec. IV-A) with respect to a ‘relative variation’ of y by
which y is changed into (I + E)y. The resulting variation
of φ[y] is (at first order) the scalar product 〈∇φ[y] | E〉
between the relative variation E and the relative gradient
∇φ[y] as in eq. (27) or (28). Aligning the direction of
change in the direction opposite to the gradient is to take
E = −µ∇φ[y] for a ‘small’ positive step µ. Thus, one step
of a relative gradient descent can be formally described as
y← (I − µ∇φ[y])y = y − µ∇φ[y] y. (39)
According to (28), the resulting variation of φ[y] is δφ ≈
〈∇φ[y]|E〉 = 〈∇φ[y]| − µ∇φ[y]〉 = −µ‖∇φ[y]‖2 which is
negative for positive µ.
The formal description (39) can be turned into off-line
and on-line algorithms as described next.
Off-line relative gradient descent. Consider the separation
of a batch x(1), . . . ,x(T ) of T samples based on the min-
imization of a contrast function φ[y] with relative gradi-
ent ∇φ[y] = EH(y). One looks for a linear transform
of the data satisfying the corresponding estimating equa-
tion 1T
∑T
t=1H(y(t)) = 0. The relative gradient descent to
solve it goes as follows: Set y(0)(t) = x(t) for 1 ≤ t ≤ T
and iterate through the following two steps
Ĥ ← 1
T
T∑
t=1
H(y(t)), (40)
y(t) ← y(t)− µĤy(t) (1 ≤ t ≤ T ). (41)
The first step computes an estimate Ĥ of the relative gra-
dient for the current values of the data; the second step
updates the data in the (relative) direction opposite to the
relative gradient as in (39). The algorithm stops when
1
T
∑T
t=1H(y(t)) = 0 i.e. when the estimating equation is
solved. It is amusing to note that this implementation does
not need to maintain a separating matrix: it directly oper-
ates on the data set itself with the source signals emerging
during the iterations.
On-line relative gradient descent. On-line algorithms up-
date a separating matrix Bt upon reception of a new sample
x(t). The (relative) linear transform y ← (I + E)y corre-
sponds to changing B into (I + E)B = B + EB. In the
on-line mode, one uses the stochastic gradient technique
where the gradient ∇φ[y] = EH[y] is replaced by its in-
stantaneous value H(y(t)). Hence the stochastic relative
gradient rule
Bt+1 = Bt − µtH(y(t))Bt (42)
where µt is a sequence of positive learning steps.
Uniform performance of relative gradient descent. A strik-
ing feature of BSS model is that the ‘hardness’ (in a statis-
tical sense discussed in section VI-C) of separating mixed
sources does not depend on the particular value of the mix-
ing matrix A: the problem is ‘uniformly hard in the mix-
ture’. Very significantly, the device of relative updating
produces algorithms which also behave uniformly well in
J.-F. CARDOSO, BLIND SIGNAL SEPARATION: STATISTICAL PRINCIPLES 11
the mixture. Right-multiplying the updating rule (42) by
matrix A and using y = Bx = BAs, one readily finds
that the trajectory of the global system Ct , BtA which
combines mixing and unmixing matrices is governed by
Ct+1 = Ct − µtH(Cs(t))Ct. (43)
This trajectory is expressed here as a sole function of the
global system Ct: the only effect of the mixing matrix A
itself is to determine (together with B0) the initial value
C0 = B0A of the global system. This is a very desirable
property: it means that the algorithms can be studied and
optimized without reference to the actual mixture to be
inverted. This is true for any estimating function H(y);
however uniformly good performance can only be expected
if the H(y) is correctly adjusted to the distribution of the
source signals, for instance by deriving it from a contrast
function. Algorithms based on an estimating function in
the form (32) are described in [5] for the on-line version and
in [62] for an off-line version; those based on form (37) are
studied in detail in [22]. The uniform performance property
was also obtained in [25].
Regular gradient algorithms. It is interesting to compare
the relative gradient algorithm to the algorithm obtained
by a ‘regular’ gradient, that is by applying a gradient rule
to the entries of B for the minimization of f(B) , φ[Bx].
This is
Bt+1 = Bt − µtH(y(t))B−†t . (44)
Not only is this form more costly because it requires (in
general) the inversion of Bt at each step, but it lacks the
uniform performance property: the trajectory of the global
system depends on the particular mixture A to be inverted.
B. Adapting to the sources
The iterative and adaptive algorithms described above
require the specification of an estimating function H, for
which two forms Hϕ and H◦ϕ (eqs. (32) and (37)) are sug-
gested by the theory. These forms, in turn, depend on
non-linear functions ϕ1, . . . , ϕn which, ideally, should be
the score functions associated to the distributions of the
sources (sec. IV-B). When the source distributions are un-
known, one may try to estimate them from the data (for
instance using some parametric model as in [57]) or to di-
rectly estimate ‘good’ non-linear functions.
A first idea is to use Edgeworth expansions (see e.g. [52])
which provide approximations to probability densities in
the vicinity of a Gaussian density. The simplest non triv-
ial Edgeworth approximation of a symmetric pdf q in the
vicinity of the standard normal distribution is
q(s) =
1√
2π
exp
(
−s
2
2
) (
1 +
k
24
(s4 − 6s2 + 3) + · · ·
)
where k is the kurtosis of q. The corresponding approxi-
mate score function then is
ϕ(s) = s− k
6
(s3 − 3s) + · · · . (45)
−2 0 2
0
0.1
0.2
0.3
0.4 gamma = 0.1
−4 −2 0 2 4
−4
−2
0
2
4
−2 0 2
0
0.1
0.2
0.3
0.4
0.5 gamma=2.6
−4 −2 0 2 4
−4
−2
0
2
4
−2 0 2
0
0.2
0.4
0.6
0.8 gamma: 2.13
−4 −2 0 2 4
−4
−2
0
2
4
Fig. 12. Top row: three distributions and the values of γ? as a
measure of non-Gaussianity (see sec. VI-B). Bottom row: the
score function (solid) and its linear-cubic approximations: based
on Edgeworth expansion (dashes-dots) and optimal (dashes).
Thus the Edgeworth expansion suggests that in a linear-
cubic approximation to the score function the coefficient of
the cubic part should be −ki/6 for the ith source. Asymp-
totic analysis shows that such a choice at least guarantees
the local stability (sec. VI-A). There are other possibilities
for deriving score functions by a density expansion: see for
instance [69] for a different proposal involving odd and even
terms in ϕ.
A more direct approach than pdf expansion is proposed
by Pham [62] who considers approximating ψ by a linear
combination
ϕα(s) ,
L∑
l=1
αlfl(s) (46)
of a fixed set {f1, . . . , fL} of arbitrary basis functions.
Rather surprisingly, the set {α1, . . . , αL} of coefficients
minimizing the mean square error E(ϕα(s) − ψ(s))2 be-
tween the true score ψ and its approximation can be found
without knowing ψ: the best mean-square approximation
involves only the expectation operator. It is:
ϕ∗(s) = (EF ′(s))
† (EF (s)F (s)†)−1 F (s) (47)
where F (s) , [f1(s), . . . , fL(s)]† is the L × 1 column vec-
tor of basis functions and F ′(s) is the column vector of
their derivatives. This is a nice result because the expres-
sion of ϕ∗ can be simply estimated by replacing in (47)
expectations by sample averages and the values of s by the
estimated source signals.
The two approaches of Edgeworth expansion and mean-
square fit, respectively leading to the approximations (45)
and (47), are compared in figure 12. Three pdf’s are
displayed in the top row; the bottom row shows the cor-
responding score function (solid line), the linear-cubic ap-
proximation by (45) (dash-dotted line) and the Pham ap-
proximation (dashed line) obtained from (47) with F (s) =
[s, s3]. Both approximations are similar in the first example
when the pdf is close to Gaussian; in the second case, the
optimal approximation fits much better the true score in
the area of highest probability. None of the approximations
seem really good in the third example for the simple reason
that the true score there cannot be well approximated by
12 THIS IS *VERY CLOSE* THE OFFICIAL VERSION, PUBLISHED AS: PROCEEDINGS OF THE IEEE, VOL. 9, NO 10, PP. 2009-2025, OCT. 1998
a linear-cubic function. However, the two approximations
fit the score well enough to guarantee the stability of the
gradient algorithms (see sect. VI-A).
VI. Performance issues
This section is concerned with the performance of BSS
algorithms: it presents some asymptotic analysis results.
It has been repeatedly stressed that it was not necessary
to know the source distributions (or equivalently: the asso-
ciated score functions) to a great accuracy to obtain con-
sistent BSS algorithms. There is however a limit to the
misspecification of the source distributions as illustrated
by fig. 7; this is elucidated at sec. VI-A which gives ex-
plicit stability limits. Even if an hypothesized distribu-
tion is good enough to preserve stability, one may expect
a loss of estimation accuracy due to misspecification when
a finite number of samples are available; this is quantified
at sec. VI-B which also describes the ultimate achievable
separation performance. The concluding section VI-C dis-
cusses the general property of ‘equivariance’ which governs
the performance of BSS algorithms.
A. Local stability
A stationary point (or equilibrium point) B of the learn-
ing rule (42) is characterized by EH(y) = EH(Bx) = 0 i.e.
the mean value of the update is zero. We have seen that
separating matrices (with the proper scale) are equilibrium
points; we are now interested in finding when they are lo-
cally stable i.e. when a small deviation from the equilib-
rium is pulled back to the separating point. In other words,
we want the separating matrix to a (local) attractor for the
learning rule (42). In the limit of small learning steps, it
exists a simple criterion for testing local stability which de-
pends on the derivative of EH(Bx) with respect to B. For
both the symmetric form H◦ϕ and for the asymmetric form
Hϕ the stability condition can be worked out exactly. They
are found to depend only the following non-linear moments
κi , Eϕ′i(si) Es
2
i − Eϕi(si)si (48)
where each si is rescaled according to EH(s) = 0, that is
Eϕi(si)si = 1 for H = Hϕ or Es2i = 1 for H = H
◦
ϕ.
Leaving aside the issue of stability with respect to scale,
the stability conditions for the symmetric form (37) are [22]
(1 + κi)(1 + κj) > 1 for 1 ≤ i < j ≤ n (49)
and for the asymmetric form (32), the conditions are [4]
that 1 + κi > 0 for 1 ≤ i ≤ n and that
κi + κj > 0 for 1 ≤ i < j ≤ n. (50)
Therefore stability appears to depend on pairwise condi-
tions. The stability domains for a given pair of sources are
displayed on fig. 13 in the (κi, κj) plane. Note that the
stability domain is larger for the symmetric form (37): this
is a consequence of letting the second order information
(the whiteness constraint) do ‘half the job’ (see sec. II-B).
Some comments are in order. First, it appears that in
both cases, a sufficient stability condition is κi > 0 for
−1 0 4
−1
0
4
Stability regions
Boundary for asymmetric form
Boundary for symmetric form
Fig. 13. Stability domains in the (κi, κj) plane.
all the sources. Thus, regarding stability, tuning the non-
linear functions ϕi’s to the source distributions should be
understood as making the κi’s positive. Second, one can
show that if si is Gaussian, then κi = 0 for any function
ϕi. Therefore the stability conditions can never be met if
there is more than one Gaussian source, in agreement with
the identifiability statements of sec. II. Third, it can also
be shown that if ϕi is taken to be the score function for
the true density of si, then κi ≥ 0 with equality only if si
is Gaussian.
Section II-C illustrated the fact that the hypothesized
source distributions should be ‘close enough’ to the true
distributions for the likelihood to still show a maximum
around a separating point. The definition of κi provides a
quantitative measure of how wrong the hypothesis can be:
they should not allow κi to become negative.
We also note that it is not necessary that all the κi’s
are positive: if κi < 0 for at most one source, this can be
compensated if the moments κj are large enough for all j 6=
i. As seen from the stability domains (fig. 13), one source at
most can have an arbitrarily negative κi if the symmetric
form is used while the stability of the asymmetric form
requests that κi > −1.
We have considered linear-cubic score functions in
secs. IV and V. If ϕi(si) = αisi + βis3i for two constants
αi and βi, then κi = βi(3E2s2i − Es4i ) = −βiki where, as
above, ki denotes the kurtosis. Note that the linear part of
ϕi does not affect the stability and that stability is guaran-
teed if the coefficient βi of the cubic part has a sign opposite
to the sign of the kurtosis. Quite naturally, the functions
in eq. (38) and (45) come up naturally with the right sign.
Therefore, if one wishes to use cubic non-linearities, it is
sufficient to know the sign of the kurtosis of each source
to make separating matrices stable. For other than cubic
scores, stability depends on the sign of κi, not on the sign
of the kurtosis.
B. Accuracy of estimating equations
This section characterizes the accuracy of signal separa-
tion obtained by solving an estimating equation (34) with
T independent realizations of x.
J.-F. CARDOSO, BLIND SIGNAL SEPARATION: STATISTICAL PRINCIPLES 13
If a matrix B is used for separation, the pth entry of
y = Bx = BAs contains the signal of interest sp at
power (BA)2ppσ
2
p and the qth interfering signal sq at power
(BA)2pqσ
2
q . Therefore, for a given matrix B the quantity
ρpq(B) ,
(BA)2pqEs
2
q
(BA)2ppEs2p
p 6= q (51)
measures the interference-to-signal ratio (ISR) provided by
B in rejecting the qth source in the estimate of the pth
source. Let B̂T be the separating matrix obtained via
a particular algorithm using T samples. In general, the
estimation error in regular statistical models decreases as
1/
√
T so that the limit
ISRpq , lim
T→∞
T E ρpq(B̂T ) (52)
usually exists provides an asymptotic measure of perfor-
mance of separation of a given off-line BSS technique.
When H = Hϕ or H = H◦ϕ are used in the estimating
equation, the asymptotic ISR depends on the moments κi
in (48) and also on:
γi , Eϕ2i (si)Es
2
i − E2 (ϕi(si)si) ≥ 0. (53)
For simplicity, we consider identically distributed signals
and identical non-linear functions: ϕi(·) = ϕ(·), so that
κi = κ and γi = γ for 1 ≤ i ≤ n. With a symmetric
estimating function H◦ϕ, one finds
ISR◦pq = ISR
◦ =
1
2
(
γ
κ2
+
1
2
)
p 6= q. (54)
Note that ISR◦ is lower bounded by 1/4 regardless of the
value of γ: this is a general property of orthogonal BSS
techniques [16] and is the ‘price to pay’ for blindly trusting
second order statistics i.e. for whitening. Thus rejection
rates obtained under the whiteness constraint cannot be
(asymptotically) better that 14T .
For an asymmetric estimating function Hϕ the ISR does
not take such a simple form unless the common score ϕ
is obtained by Pham’s method (sec. V-B). One then finds
ISRpq = ISR and ISR◦pq = ISR
◦ as
ISR =
1
2
(
1
γ
+
1
γ + 2
)
, ISR◦ =
1
2
(
1
γ
+
1
2
)
(55)
where the last equation stems from (54) because Pham’s
method guarantees γ = κ. These expressions show that
both ISR and ISR◦ are minimized by maximizing γ; not
surprisingly, γ can be shown to reach its maximum value
γ? precisely when ϕ = ψ where ψ is the score function
corresponding to the true density of the sources:
γ∗ , Eψ2(s)Es2 − E2[ψ(s)s]. (56)
Note that the solution of (34) with H = Hψ then is the
ML estimator based on the true model. It follows that the
expression of ISR in (55) also is the asymptotic Cramér-
Rao bound for source separation i.e. the best achievable
ISR rate with T independent samples (see [59], [70], [62]).
Since the achievable performance depends on the magni-
tude of γ∗, this moment characterizes the hardness of the
BSS problem with respect to source distribution. Not sur-
prisingly, we can relate it to the non-Gaussianity of the
sources as follows. As above, denote ψ the score function
for the (true) distribution of s and denote ψn the score func-
tion for the Gaussian distribution with the same variance
as s (this is just ψn(s) = s/Es2). A ‘large’ non-Gaussianity
translates into a large difference between ψ and ψn. As we
just saw, the measure of non-Gaussianity from the asymp-
totic point of view is measured by γ?. Indeed one finds:
γ? =
E(ψ(s)− ψn(s))2
E(ψn(s))2
. (57)
See fig. 12 for the values of γ? in three examples. For close-
to-Gaussian sources, γ? is (arbitrarily) small: in this case,
according to (55) the best achievable rejection rates are
about 12γ?T for both the symmetric and the asymmetric
forms. This gives an idea of the minimum number of sam-
ples required to achieve a given separation. The other ex-
treme is for sources which are far away from normality: the
moment γ? is not bounded above. In particular, it tends
to ∞ when the source distributions tend to have a discrete
or a bounded support. In the case of discrete sources, de-
terministic (error-free) blind identification is possible with
a finite number of samples. In the case of sources with
bounded support, the MSE of blind identification decreases
at a much faster rate than the 1/T rate obtained for finite
values of γ (see in particular [42]).
C. Equivariance and uniform performance
At first thought, the hardness of the BSS problem seems
to depend on the distributions of the source signals and
on the mixing matrix, with harder problems when sources
are nearly Gaussian and when the mixing matrix is poorly
conditioned. This is not correct however: the BSS problem
is ‘uniformly hard in the mixing matrix’. Let us summarize
the instances where this property appeared: the ultimate
separation performance depends only on γ? (eq. (55)); the
asymptotic performance index in eqs. (54) and (55) de-
pend only on some statistical moments; the stability of the
adaptive algorithms (42) also depends only on the values of
κi’s; even better, the trajectory (43) of the global system
Ct = BtA does not depend on A whose sole effect is to
determine the initial point.
Therefore, not only does the problem appears to be ‘uni-
formly hard in the mixing matrix’, but it exists estimation
techniques with a statistical behavior (regarding signal sep-
aration) which is independent of the particular value of the
system to be inverted. This is a very desirable property:
such algorithms can be studied and tuned independently
of the particular mixture to be inverted; their performance
can also be predicted independently of the mixture [17].
This is an instance of ‘equivariance’, a property holding
more generally in transformation models.
There is a simple prescription to design algorithms with
uniform performance: adjust freely (i.e. without con-
straint) the separating matrix according to a rule expressed
14 THIS IS *VERY CLOSE* THE OFFICIAL VERSION, PUBLISHED AS: PROCEEDINGS OF THE IEEE, VOL. 9, NO 10, PP. 2009-2025, OCT. 1998
only in terms of the output y. To understand why the ‘out-
put only’ prescription ensures uniform performance, con-
sider for instance using a particular estimating function
H(·) to separate a mixture of T samples [s(1), . . . , s(T )].
If the source signals are mixed by a given matrix A, then
a solution of (34) is a matrix B such that BA = Ĉ where
matrix Ĉ is a solution of T−1
∑T
t=1H(Ĉs(t)) = 0. Ma-
trix Ĉ does not depend on A so that the global system
BA = Ĉ is itself independent of A and the estimated sig-
nals are y(t) = Ĉs(t) regardless of A. In particular, the
recovered signals are exactly identical to those that would
be obtained with A = I i.e. when there is no mixing at
all. This argument, based on estimating equations, extends
to the minimizers of contrast functions since the latter are
defined as functions of the distribution of the output (the
argument also apply to orthogonal contrast functions be-
cause the whiteness constraint is expressed only in terms
of y). The argument also justifies the specific definition of
the ‘relative gradient’: a device was needed to express the
first-order variations of a contrast function φ[y] in terms
of a variation of y itself i.e. without reference to B. Fi-
nally, it must be stressed that the argument does not in-
volve asymptotics: equivariance is exactly observed for any
finite value of T .
Not all BSS algorithms are equivariant. For instance,
the original algorithm of Jutten and Hérault imposes con-
straints on the separating matrix resulting in a greatly com-
plicated analysis (and behavior) (see [35], [40], [49]). Other
instances of non equivariant techniques is to be found in
most of the algebraic approaches (see sec. VII) based on
the structure of the cumulants of the observed vector x.
Precisely because the identification is based on x and not
on y, such approaches are not equivariant in general unless
they can be shown to be equivalent to the optimization of
a contrast function of y.
A word of caution is necessary before concluding: equiv-
ariance holds exactly in the noise-free model which we have
considered so far. In practice, there is always some kind
of noise which must be taken into account. Assume that
a better model is x = As + n where n represents an addi-
tive noise. This can be rewritten as x = A(s + A−1n). As
long as A−1n can be neglected with respect to s, this is a
noise-free situation. This shows the limit of equivariance:
a poorly conditioned matrix A has a large inverse which
amplifies the effect of the noise. More precisely, we can
expect equivariance in the high SNR domain i.e. when the
covariance matrix of s remains ‘larger’ than the covariance
matrix of A−1n.
VII. Conclusions
Due to limited space, focus was given to principles and
many interesting issues have been left out: discussion of
the connections between BSS and blind deconvolution; con-
vergence rates of adaptive algorithms; design of consistent
estimators based on noisy observations, detection of the
number of sources, etc. . . Before concluding, we briefly
mention some other points.
Algebraic approaches. The 4th order cumulants of x have
a very regular structure in the BSS model:
Cijkl[x] =
n∑
p=1
kpAipAjpAkpAlp 1 ≤ i, j, k, l ≤ n. (58)
Given sample estimates of the cumulants, the equation
set (58) (or some subset of them) can be solved in A in
a least square sense. This is a cumulant matching ap-
proach [71][43] which does not yield equivariant estimates.
Optimal matching, though, can be shown to correspond to
a contrast function [20]. However, the specific form of (58)
also calls for algebraic approaches. Simple algorithms can
be based on the eigen-structure of ‘cumulant matrices’ built
from cumulants [63], [65]. An exciting direction of research
is to investigate high-order decompositions that would gen-
eralize matrix factorizations like SVD or EVD to 4th order
cumulants [33], [30], [15], [27], [21].
Using temporal correlation. The approaches to BSS de-
scribed above exploit only properties of the distribution of
x(t). If the source signals are temporally correlated, time
structures can also be exploited. It is possible to achieve
separation if all the source signals have distinct spectra
even if each source signal is a Gaussian process [67]. Sim-
ple algebraic techniques can be devised (see [66], [11]); the
Whittle approximation to the likelihood is investigated in
[61]. Cyclostationary properties, when they exist, can also
be exploited [13].
Deterministic identification. As indicated in sec. VI-B,
sources with discrete support allow for deterministic iden-
tification (infinite Fisher information). Specific contrast
functions can be devised [42] to take advantage of discrete-
ness. There is a rich domain of application with digital
communication signals coding information with discrete
symbols by which deterministic identification is possible.
See the review by Van der Veen [68] and the papers on
CMA in this issue.
Open problems and perspectives
1. Learning source distributions. In the BSS problem,
source distributions are a nuisance parameter. For large
enough sample size, it is possible to estimate the distribu-
tions and still obtain the same asymptotic performance as
if the distributions were known in advance[3]; the design
of practical algorithms achieving ‘source adaptivity’ still is
an open question.
2. Dealing with noise. BSS techniques remaining consis-
tent in presence of additive noise have not been described
here. For additive Gaussian noise, such techniques may
resort to high-order cumulants or to noise modeling. It is
not clear however that it is worth combating the noise. As
a matter of fact, one may argue that taking noise effects
into account is unnecessary at high SNR and futile at low
SNR (because the BSS problem becomes too difficult any-
way). Therefore, we believe it is still an open question to
determine which application domains would really benefit
from noise modeling.
3. Global convergence. Some cumulant based contrast
functions can be proved to be free of spurious local min-
J.-F. CARDOSO, BLIND SIGNAL SEPARATION: STATISTICAL PRINCIPLES 15
ima in the two-source case (see e.g. [29]) or in a ‘defla-
tion approach’ (successive extractions of the source sig-
nals) [34][45]. There is however a lack of general under-
standing of the global shape of contrast functions in the
general case.
4. Multidimensional independent components. An inter-
esting original variation of the basic ICA model would be
to decompose a random vector in a sum of independent
components with the requirement that the components are
linearly independent but not necessarily one-dimensional.
In the BSS model, this would be equivalent to grouping
the source signals in subsets with independence between
the subsets but not within the subsets. This more general
decomposition could be called ‘multidimensional indepen-
dent component analysis’ (MICA).
5. Convolutive mixtures. The most challenging open prob-
lem in BSS probably is the extension to convolutive mix-
tures. This is a very active area of research, mainly moti-
vated by applications in the audio-frequency domain where
the BSS is often termed the ‘cocktail-party problem’. The
convolutive problem is significantly harder than the instan-
taneous problem: even input-output (i.e. non blind) iden-
tification is a very challenging because of the large number
of parameters usually necessary to describe audio channels.
6. When the model does not hold. The introduction men-
tioned successful applications of BSS to biomedical sig-
nals. When examining these data, it is very striking to
realize that the extracted source signals seem to be very
far to obeying the simple BSS model. The fact that BSS
still yields apparently meaningful (to the experts) results is
worth of consideration. A partial explanation stems from
basing separation on contrast functions: even if the model
does not hold (there are no independent source signals and
no system A to be inverted), the algorithms still try to pro-
duce output which are ‘as independent as possible’. This
does not tell the whole story though because for many data
sets a stochastic description does not seem appropriate. We
believe it will a very interesting challenge to understand
the behavior of BSS algorithms when applied ‘outside the
model’.
We are indebted to the anonymous reviewers whose con-
structive comments helped us improving on a first version
of this paper.
References
[1] The ICA-CNL group at the Salk Institute.
http://www.cnl.salk.edu/∼tewon/ica cnl.html.
[2] S.-I. Amari. Natural gradient works efficiently in learning. Neu-
ral Computation, 10:251–276, 1998.
[3] S.-I. Amari and J.-F. Cardoso. Blind source separation — semi-
parametric statistical approach. IEEE Trans. on Sig. Proc.,
45(11):2692–2700, Nov. 1997. Special issue on neural networks.
[4] S.-I. Amari, T.-P. Chen, and A. Cichocki. Stability analysis of
adaptive blind source separation. Neural Networks, 10(8):1345–
1351, 1997.
[5] S.-I. Amari, A. Cichocki, and H. Yang. A new learning algorithm
for blind signal separation. In Advances in Neural Information
Processing Systems, 8, pages 757–763, Denver, 1996. MIT Press.
[6] K. Anand, G. Mathew, and V. Reddy. Blind separation of multi-
ple co-channel BPSK signals arriving at an antenna array. IEEE
Signal Proc. Letters, 2(9):176–178, Sept. 1995.
[7] A. Back and A. Weigend. A first application of independent
component analysis to extracting structure from stock returns.
Int. Journal of Neural Systems, vol. 8, no 4, pp. 473-484, Aug.
1997.
[8] Y. Bar-Ness. Bootstraping adaptive interference cancelers: Some
practical limitations. In Proc. Globecom, pages 1251–1255, Nov.
1982.
[9] A. J. Bell and T. J. Sejnowski. An information-maximisation
approach to blind separation and blind deconvolution. Neural
computation, 7(6):1004–1034, 1995.
[10] A. J. Bell and T. J. Sejnowski. Edges are the ‘independent com-
ponents’ of natural scenes. In Advances in Neural Information
Processing Systems, 9. MIT Press, 1996.
[11] A. Belouchrani, K. Abed Meraim, J.-F. Cardoso, and Éric
Moulines. A blind source separation technique based on sec-
ond order statistics. IEEE Trans. on Sig. Proc., 45(2):434–44,
Feb. 1997.
[12] A. Benveniste, M. Goursat, and G. Ruget. Robust identification
of a non-minimum phase system. Blind adjustment of a linear
equalizer in data communication. IEEE Tr. on AC, 25(3):385–
399, 1980.
[13] B.G. Agee and S.V. Schell and W.A. Gardner. Spectral self-
coherence restoral: A new approach to blind adaptive signal
extraction using antenna arrays. Proceedings of the IEEE, pages
753–766, Apr. 1990.
[14] X.-R. Cao and R.-W. Liu. General approach to blind source
separation. IEEE Trans. Signal Processing, 44(3):562–571, Mar.
1996.
[15] J.-F. Cardoso. Super-symmetric decomposition of the fourth-
order cumulant tensor. Blind identification of more sources than
sensors. In Proc. ICASSP, pages 3109–3112, 1991.
[16] J.-F. Cardoso. On the performance of orthogonal source separa-
tion algorithms. In Proc. EUSIPCO, pages 776–779, Edinburgh,
Sept. 1994.
[17] J.-F. Cardoso. The equivariant approach to source separation.
In Proc. NOLTA, pages 55–60, 1995.
[18] J.-F. Cardoso. Estimating equations for source separation. In
Proc. ICASSP’97, pages 3449–52, 1997.
[19] J.-F. Cardoso. Infomax and maximum likelihood for source sep-
aration. IEEE Letters on Signal Processing, 4(4):112–114, Apr.
1997.
[20] J.-F. Cardoso, S. Bose, and B. Friedlander. On optimal source
separation based on second and fourth order cumulants. In Proc.
IEEE Workshop on SSAP, Corfou, Greece, 1996.
[21] J.-F. Cardoso and P. Comon. Tensor based independent compo-
nent analysis. In Proc. EUSIPCO, pp. 673-676, 1990.
[22] J.-F. Cardoso and B. Laheld. Equivariant adaptive source sepa-
ration. IEEE Trans. on Sig. Proc., 44(12):3017–3030, Dec. 1996.
[23] J.-F. Cardoso and A. Souloumiac. Blind beamforming for non
Gaussian signals. IEE Proceedings-F, 140(6):362–370, Dec. 1993.
[24] E. Chaumette, P. Comon, and D. Muller. ICA-based technique
for radiating sources estimation: application to airport surveil-
lance. IEE Proceedings-F, 140(6):395–401, Dec. 1993.
[25] A. Cichocki, R. Unbehauen, and E. Rummert. Robust learn-
ing algorithm for blind separation of signals. Electronic letters,
30(17):1386–87, 1994.
[26] P. Comon. Independent component analysis, a new concept ?
Signal Processing, Elsevier, 36(3):287–314, Apr. 1994. Special
issue on Higher-Order Statistics.
[27] P. Comon and B. Mourrain. Decomposition of quantics in sums
of powers of linear forms. Signal Processing, 53(2):93–107, Sept.
1996.
[28] T. M. Cover and J. A. Thomas. Elements of information theory.
Wiley series in telecommunications. John Wiley, 1991.
[29] A. Dapena, L. Castedo, and C. Escudero. An unconstrained sin-
gle stage criterion for blind source separation. In Proc. ICASSP,
volume 5, pages 2706–9, 1996.
[30] L. De Lathauwer, B. De Moor, and J. Vandewalle. Blind source
separation by higher order singular value decomposition. In
Proc. EUSIPCO, volume 1, pages 175–178, 1994.
[31] L. De Lathauwer, B. De Moor, and J. Vandewalle. Fetal electro-
cardiogram extraction by source subspace separation. In Proc.
HOS’95, pages 134–8, Aiguablava, Spain, June 1995.
[32] L. De Lathauwer, B. De Moor, and J. Vandewalle. Blind source
separation by simultaneous third-order tensor diagonalization.
In Proc. EUSIPCO, Trieste, pp. 2089-2092, 1996.
[33] L. De Lathauwer, B. De Moor, and J. Vandewalle. Independent
component analysis based on higher-order statistics only. In
Proc. IEEE SSAP workshop, Corfu, pages 356–359, 1996.
[34] N. Delfosse and P. Loubaton. Adaptative blind separation of
16 THIS IS *VERY CLOSE* THE OFFICIAL VERSION, PUBLISHED AS: PROCEEDINGS OF THE IEEE, VOL. 9, NO 10, PP. 2009-2025, OCT. 1998
independent sources: a deflation approach. Signal Processing,
45(1):59–83, 1995.
[35] Y. Deville. A unified stability analysis of the Hérault–Jutten
source separation neural network. Signal Processing, 51(3):229–
233, June 1996.
[36] Y. Deville and L. Andry. Application of blind source separation
techniques to multi-tag contactless identification system. IEICE
Transactions on Fundamentals of Electronics, Communications
and Computer Sciences, E79-A(10):1694–99, 1996.
[37] D. Donoho. On minimum entropy deconvolution. In Applied
time-series analysis II, pages 565–609. Academic Press, 1981.
[38] G. d’Urso, P. Prieur, and C. Vincent. Blind identification meth-
ods applied to EDF civil works and power plants monitoring. In
Proc. HOS’97, pages –, Banff, Canada, June 1997.
[39] L. Féty. Méthodes de traitement d’antenne adaptées aux radio-
communications. Thèse de doctorat. Télécom Paris, June 1988.
[40] J.-C. Fort. Stability of the source separation algorithm of Jutten
and Hérault. In T. Kohonen, Makasira, Simula, and Kangas, ed-
itors, Artificial Neural Networks, pages 937–941. Elsevier, 1991.
[41] M. Gaeta and J.-L. Lacoume. Source separation without a pri-
ori knowledge: the maximum likelihood solution. In Proc. EU-
SIPCO, pages 621–624, 1990.
[42] F. Gamboa and Élizabeth Gassiat. Source separation when the
input sources are discrete or have constant modulus. IEEE
Trans. Signal Processing, 45(12):3062–72, 1997.
[43] G. Giannakis and S. Shamsunder. Modelling of non Gaussian
array data using cumulants: DOA estimation of more sources
with less sensors. Signal Processing, 30(3):279–297, July 1993.
[44] J. Hérault, C. Jutten, and B. Ans. Détection de grandeurs prim-
itives dans un message composite par une architecture de cal-
cul neuromimétique en apprentissage non supervisé. In Proc.
GRETSI, pages 1017–1020, Nice, France, 1985.
[45] A. Hyvärinen and E. Oja. A fast fixed-point algorithm for inde-
pendent component analysis. Neural Computation, 9(7):1483–
92, 1997.
[46] C. Jutten and J. Herault. Blind separation of sources I. An
adaptive algorithm based on neuromimetic architecture. Signal
Processing, 24(1):1–10, July 1991.
[47] J. Karhunen, A. Hyvarinen, R. Vigario, J. Hurri, and E. Oja.
Applications of neural blind separation to signal and image pro-
cessing. In Proc. ICASSP, volume 1, pages 131–4, 1997.
[48] J. Karhunen, E. Oja, L. Wang, R. Vigário, and J. Joutsensalo.
A class of neural networks for independent component analy-
sis. IEEE Transactions on Neural Networks, 8(3):486–504, May
1997.
[49] O. Macchi and Éric Moreau. Self-adaptive source separation,
Part I: Convergence analysis of a direct linear network controled
by the Hérault-Jutten algorithm. IEEE Trans. Signal Process-
ing, 45(4):918–926, Apr. 1997.
[50] D. J. C. MacKay. Maximum likelihood and covariant algorithms
for independent component analysis. in preparation, 1996.
[51] S. Makeig, A. Bell, T.-P. Jung, and T. J. Sejnowski. Indepen-
dent component analysis of electroencephalographic data. In
Advances in Neural Information Processing Systems, 8. MIT
Press, 1995.
[52] P. McCullagh. Tensor Methods in Statistics. Monographs on
Statistics and Applied Probability. Chapman and Hall, 1987.
[53] J. Moody and L. Wu. What is the ‘true price’?— State space
models for high frequency financial data. In Progress in Neural
Information Processing. Proceedings of the International Con-
ference on Neural Information Processing, volume 2, pages 697–
704. Springer-Verlag, 1996.
[54] E. Moreau and O. Macchi. High order contrasts for self-adaptive
source separation. International Journal of Adaptive Control
and Signal Processing, 10(1):19–46, jan 1996.
[55] J.-P. Nadal and N. Parga. Nonlinear neurons in the low-noise
limit: a factorial code maximizes information transfer. NET-
WORK, 5:565–581, 1994.
[56] D. Obradovic and G. Deco. An information theory based learn-
ing paradigm for linear feature extraction. Neurocomputing,
12:203–221, 1996.
[57] B. A. Pearlmutter and L. C. Parra. A context-sensitive gener-
alization of ICA. In International Conference on Neural Infor-
mation Processing, Hong Kong, 1996.
[58] J. Pfanzagl. Asymptotic expansions related to minimum contrast
estimators. The Annals of Statistics, 1(6):993–1026, 1973.
[59] D.-T. Pham. Blind separation of instantaneous mixture of
sources via an independent component analysis. Research re-
port RT 119, LMC IMAG, Grenoble, France, 1994.
[60] D.-T. Pham. Blind separation of instantaneous mixture of
sources via an independent component analysis. IEEE Trans.
on Sig. Proc., 44(11):2768–2779, Nov. 1996.
[61] D.-T. Pham and P. Garat. Séparation aveugle de sources tem-
porellement corrélées. In Proc. GRETSI, pages 317–320, 1993.
[62] D.-T. Pham and P. Garat. Blind separation of mixture of inde-
pendent sources through a quasi-maximum likelihood approach.
IEEE Tr. SP, 45(7):1712–1725, July 1997.
[63] V. C. Soon, L. Tong, Y. F. Huang, and R. Liu. An extended
fourth order blind identification algorithm in spatially correlated
noise. In Proc. ICASSP, pages 1365–1368, 1990.
[64] A. Swindlehurst, M. Goris, and B. Ottersten. Some experiments
with array data collected in actual urban and suburban envi-
ronments. In IEEE workshop on Signal Processing Advances in
Wireless Communications, pages 301–304, Apr. 1997.
[65] L. Tong, Y. Inouye, and R. Liu. Waveform preserving blind
estimation of multiple independent sources. IEEE Tr. on SP,
41(7):2461–2470, July 1993.
[66] L. Tong, V. Soon, Y. Huang, and R. Liu. AMUSE: a new blind
identification algorithm. In Proc. ISCAS, 1990.
[67] L. Tong, V. Soon, Y. Huang, and R. Liu. A necessary and suf-
ficient condition for the blind identification of memoryless sys-
tems. In Proc. ISCAS, volume 1, pages 1–4, Singapore, 1991.
[68] A.-J. Van der Veen. Blind beamforming. This issue, 1998.
[69] H. H. Yang and S. Amari. Adaptive on-line learning algorithms
for blind separation — maximum entropy and minimum mutual
information. Neural Computation, 9(7):1457–1482, oct 1997.
[70] D. Yellin and B. Friedlander. Multi-channel system identification
and deconvolution: performance bounds. In Proc. IEEE SSAP
workshop, Corfu, pages 582–585, 1996.
[71] N. Yuen and B. Friedlander. Asymptotic performance analysis
of blind signal copy using fourth-order cumulants. International
Journal of Adaptive Control and Signal Processing, pages 239–
65, mar 1996.

