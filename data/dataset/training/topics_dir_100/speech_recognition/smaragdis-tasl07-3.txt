A Framework for Secure Speech Recognition
Paris Smaragdis *, Senior Member, IEEE and Madhusudana Shashanka, Student Member, IEEE
Abstract— In this paper we present a process which enables
privacy-preserving speech recognition transactions between two
parties. We assume one party with private speech data and one
party with private speech recognition models. Our goal is to
enable these parties to perform a speech recognition task using
their data, but without exposing their private information to
each other. We will demonstrate how using secure multiparty
computation principles we can construct a system where this
transaction is possible, and how this system is computationally
and securely correct. The protocols described herein can be used
to construct a rudimentary speech recognition system and can
easily be extended for arbitrary audio and speech processing.
Index Terms— Secure Multiparty Computation, Gaussian Mix-
ture Models, HMM, Speech Recognition.
EDICS: SPE-GASR
I. INTRODUCTION
The widespread use of networking technology today has
spawned an industry of online services. Business models
based on client-server interactions are common place and
increasingly more prominent. Speech recognition could easily
be a part of this trend where servers can provide speech
recognition services for remote clients. The private nature
of speech data however is a stumbling block for such a
development. Individuals, corporations and governments are
understandably reluctant to send private speech data through
a network to another party that cannot be trusted. In this paper
we address this issue and show how such a cooperative model
can be realized with no privacy leaks from any involved party.
We will specifically focus on the realization of a hidden
Markov model (HMM) in a secure framework that allows
training and classification between multiple parties, some own-
ing speech data and some owning HMM models for speech
recognition. Our formulation is shaped in such a way that the
providers of the speech will not have to share any information
about their data and the providers of the HMM will not share
any information on their model. After evaluation the results
will only be revealed to the parties that have provided the
data, and not to the parties that provide the HMM models,
thereby providing privacy at both the data and the semantic
levels.
We will demonstrate the use of this idea using two scenarios.
One scenario will involve the training of HMMs from data
provided by multiple parties, and the other will deal with eval-
uating already trained HMMs on data from another party. The
utility of these scenarios in collaborative speech recognition
projects is easy to see. The first scenario can enable model
training on multiple speech databases without requiring the
disclosure of actual speech data, whereas the other scenario
can enable speech recognition as a service model to remote
customers who wish to maintain privacy on their data and their
transcription from both the service provider and malicious
network intruders.
Strange as these constraints might seem, they can be
satisfied using secure multiparty computation (SMC) proto-
cols. SMC is a field of cryptography that provides means
to perform arbitrary computations between multiple parties
who are concerned with protecting their data. The field of
SMC originated from the work of Yao [1] who gave a
solution to the millionaire problem: two millionaires want
to find which one has a larger fortune, without revealing
any specific numbers to each other. Recently this concept
has been employed for simple machine learning tasks such
as multiple parties performing k-means [2], computation of
means and related statistics from distributed databases [3] and
rudimentary computer vision applications [4]. See [5] for a
detailed treatment of the topic. In this paper we present a SMC
formulation of training and evaluating HMMs as applied on
speech data. To our knowledge this is the first application of
SMC concepts for privacy constrained speech technology. We
will consider HMMs where the observations are modeled by
mixtures of Gaussians as is common in speech recognition
applications. The main contributions in this paper are the
creation of privacy-preserving protocols that support Gaussian
mixture model and HMM learning and evaluation, as well as
a secure method to combine these protocols so as to ensure
data privacy.
The remainder of this paper is organized as follows. In
section II we formally introduce the problem at hand and
in section III we introduce the secure computation primitives
that are employed for this task. Using these primitives, we
deal with the problem of secure classification using Gaussian
mixtures in section IV and in section V we extend that
to present protocols for secure HMMs. We provide a brief
discussion about security and efficiency of our protocols in
section VI. Finally in section VII we provide conclusions and
directions for future extensions.
II. PROBLEM FORMULATION
Hidden Markov models find use in a wide range of applica-
tions, and have been successfully used in speech recognition.
There are three fundamental problems for HMM design,
namely: the evaluation of the probability (or likelihood) of a
sequence of observations given a specific HMM; the determi-
nation of a best sequence of model states; and the adjustment
of model parameters so as to best account for the observed
signal. The first problem is one of scoring how well a given
model matches a given observation sequence. The second
problem is one in which we attempt to uncover the hidden part
of the model. The third problem is the problem of “training”.
Algorithms for the above three problems are well known and
described in detail in [6].
We will consider these problems using a transaction between
two parties named Alice and Bob. Suppose Bob has a trained
HMM with all the model parameters learned. Let the HMM
be characterized as follows:
• N states {S1, . . . , SN}. Let the state at time t be qt.
• The state transition probability distribution A = {aij}
where
aij = P [qt+1 = Sj |qt = Si], 1 ≤ i, j ≤ N. (1)
• The observation symbol probability distribution in state
j given by a mixture of Gaussians
bj(x) =
M
∑
m=1
cjmN (µjm,Σjm), 1 ≤ j ≤ N, (2)
where x is the variable, cjm is the mixture coefficient
for the m-th mixture in state j, and N (µjm,Σjm) is a
gaussian with mean vector µjm and covariance matrix
Σjm.
• The initial state distribution π = {πi} where
πi = P [q1 = Si] 1 ≤ i ≤ N. (3)
We use λ to denote the entire parameter set of the model.
Consider the first two problems where Bob has a trained
HMM with all the model parameters learned. Let Alice have
an observation sequence X = x1x2 . . .xT . We will show
how Alice can securely compute P (X|λ), the probability of
the observation sequence given the model, using the forward-
backward procedure. We will also show how one can securely
learn the best sequence of model states using the viterbi
algorithm.
Once there is a secure way of computing likelihoods, it
is easy to see how it can be extended to applications like
speech recognition. Suppose Bob has trained several HMMs
which characterize various speech sounds. Each HMM will
correspond to a speech recognition unit. Let Alice’s observa-
tion vector correspond to a small snippet of speech sound (we
assume that Alice knows the features that Bob has used to
train his HMMs on and has represented her sound sample in
terms of those features). We then show how Alice and Bob can
obtain additive shares of the likelihood of Alice’s observation
sequence for every speech recognition unit of Bob and use
them to find out the unit that corresponds to Alice’s sound
snippet.
Now consider the third problem of training. The problem of
security arises when Bob wants to train a HMM (or do data
mining in general) on combined data from private databases
owned by Alice and Charlie. We show how Bob can securely
reestimate parameters of his HMM without gaining knowledge
about the private data. The implicit assumption, of course,
is that Alice and Charlie are willing to let Bob learn about
distributions of their data.
III. SECURE TWO-PARTY COMPUTATIONS; BACKGROUND
The speech-recognition example that we will present is a
specific example of a secure two-party computation. Consider
the case where Alice and Bob have private data a and b
respectively and they want to compute the result of a function
Fig. 1. Implementing an algorithm securely. The algorithm takes in private
inputs a and b. Algorithm is split into steps that can be implemented as
secure primitives (shown as grey boxes). Intermediate results are distributed
as random additive shares and feed into the following steps. Final result c is
obtained by both parties (or the designated receiver).
f(a,b). Consider a trusted third-party who can take the private
data, compute the result c = f(a,b), and intimate the result
to the parties. Any protocol that implements an algorithm
to calculate f(a,b) is said to be secure only if it leaks no
more information about a and b than what one can gain from
learning the result c from the trusted third-party. We assume
a semi-honest model for the parties where they follow the
protocol but could be saving messages and intermediate results
to learn more about other’s private data. In other words, the
parties are honest but curious and will follow the agreed-upon
protocol but will try to learn as much as possible from the
data flow between the parties. 1
To implement an algorithm securely, we will have to im-
plement each step of the algorithm securely. If one of the
steps is insecurely implemented, either party could utilize the
information to work their way backwards to gain knowledge
about the other’s private data. In addition, one must also
consider the results of intermediate steps. If such results of
intermediate steps are available, there is a possibility that one
could also get back to the original private inputs. To prevent
this:
• we express every step of the algorithm in terms of a hand-
ful of basic operations (henceforth called as primitives)
for which secure implementations are already known, and
• we distribute intermediate results randomly between the
two parties such that neither party has access to the entire
result. For example, instead of obtaining the result z of
a certain step, the parties receive random additive shares
z1 and z2 (z1 + z2 = z). See figure 1 for a schematic
illustration.
Secure protocols are often analyzed for correctness, security
and complexity. Correctness is measured by comparing the
proposed protocol with the ideal protocol using a third party.
If the results are indistinguishable, the protocol is correct (note
that one can use secure approximation to an ideal algorithm).
All the protocols we present are exact protocols. For security,
one needs to show what can and cannot be learned from
1In a malicious model, no such assumptions are made about the parties’
behavior. If both parties are malicious, security can be enforced by accom-
panying the protocols with zero-knowledge proofs that protocols are being
followed. If only one of the parties is malicious, the other party can use
conditional disclosure of secrets protocols [7] to make sure he/she receives
valid inputs from the malicious party.
the data exchange. For complexity, one shows the computa-
tional and communication complexity of the secure algorithm.
Based on the choice of primitives used and how they are
implemented, one can achieve different levels of security
and computational/communication efficiency. To evaluate the
efficiency of protocols we propose in later sections, we provide
measures in terms of efficiency of the primitives instead of
absolute measures. Below, we describe the primitives that we
use and briefly discuss about their implementations.
A. Secure Inner Products (SIP )
The primitive which we use most often is for computing
secure inner products. If Alice has vector x and Bob has vector
y, a secure inner product protocol produces two numbers a
and b such that a+ b = xTy. Alice will get the result a and
Bob will get the result b. To simplify notation, we shall denote
a secure inner product computation xTy as SIP (x,y).
Many protocols have been proposed and they can be cate-
gorized as cryptographic protocols (eg. [8], [9]) and algebraic
protocols (eg. [10], [11], [12], [13], [14]). They provide
different levels of security and efficiency. Most of the algebraic
protocols leak some information but are more straightforward
and efficient than their cryptographic counterparts. The proper-
ties and weaknesses of some of these algebraic protocols have
been analyzed in detail ([3], [9]). In this paper we will be
using cryptographic protocols as they tend to be more secure.
In appendices I and II we provide a description of two of the
cryptographic protocols we have used. We have also included
an algebraic protocol in appendix III for comparison.
B. Secure Maximum Index (SMAX)
Let Alice have a vector x = [x1 . . . xd] and Bob have the
vector y = [y1 . . . yd], they would like to compute the index
of the maximum of x + y = [(x1 + y1) . . . (xd + yd)]. At the
end of the protocol, Alice (and/or Bob) will receive the result
but neither party will know the actual value of the maximum.
Notice that the same protocol can be used to compute the
index of the minimum. We denote this as j = SMAX(x,y).
For this primitive, we use the permute protocol proposed by
[15] (see appendix IV). The protocol enables Alice and Bob to
obtain additive shares, q and s, of a permutation of the vector
x+y, π(x+y), where π is chosen by Alice and Bob has no
knowledge of π. The idea is for Alice to send q−r, where r is
a random number chosen by her, to Bob. Bob sends back the
index of the maximum element of q+s−r to Alice who then
computes the real index using the inverse of the permutation
π. Neither party learns the value of the maximum element and
Bob does not learn the index of the maximum element.
If the security requirements of Alice are more strict, she
can encrypt elements of q using a homomorphic encryption
scheme and send them to Bob along with the public key. Bob
can make comparisons among the encrypted values of π(x+y)
using protocols for Yao’s millionaire problem (eg. [16], [17])
and send back the index of the maximum element.
C. Secure Maximum Value (SV AL)
Let Alice have a vector x = [x1 . . . xd] and Bob have the
vector y = [y1 . . . yd], they would like to compute the value of
the maximum element in z = x+y. After the protocol, Alice
and Bob receive additive shares of the result, a and b, but
neither party will know the index of the maximum element.
Notice that the same protocol can be used to compute the value
of the minimum. Let us denote this as a+ b = SV AL(x,y).
For this protocol, we can use the idea presented in [15].
Let us first consider a naive approach. Notice that zi ≥
zj ⇐⇒ (xi − xj) ≥ (yj − yi). Alice and Bob can do
such pairwise comparisons and mimic any standard maximum
finding algorithm to learn the value of the maximum. To
perform the comparisons securely, they can use a protocol for
Yao’s millionaire problem [1].
However, if Alice and Bob follow the above naive approach,
both will be able to also find the index of the maximum.
Hence, the idea is for Alice and Bob to obtain two vectors
whose sum is a random permutation of z. Neither Alice nor
Bob should know the permutation. They can then follow
the above naive approach on their newly obtained vectors
to compute additive shares of the maximum element. See
appendix IV for a description of the permutation protocol.
D. Secure Logsum (SLOG)
This primitive, unlike the other three which we have in-
troduced above, is not a cryptographic primitive. The main
reason we introduce it is because it simplifies the presentation
of many of the protocols we propose in future sections.
Let Alice have a vector x = [x1 . . . xd] and Bob have the
vector y = [y1 . . . yd] such that x+y = ln z = [ln z1 . . . ln zd].
They would like to compute additive shares q and s such that
q + s = ln(
∑d
i=1 zi). Let us denote this secure computation
as q + s = SLOG(x,y).
One can compute logarithm of a sum from the logarithms
of individual terms as follows:
ln
( d
∑
i=1
zi
)
= ln
( d
∑
i=1
exi+yi
)
(4)
This suggests the following protocol.
1) Alice and Bob compute the dot product between vectors
ex−q and ey using SIP (ex−q, ey) where q is a random
number chosen by Alice. Let Bob obtain φ, the result
of the dot product.
2) Notice that Bob has s = lnφ = −q + ln(
∑d
j=1 e
xj+yj )
and Alice has q.
In step (3), Bob receives the entire result of a dot product.
However, this does not reveal any information to him about x
due to the presence of the random number q. In no other step
does either party receive the complete result of an operation.
Thus, the protocol is secure. In terms of efficiency, this
primitive is equivalent to using the SIP primitive once.
IV. SECURE CLASSIFICATION: GAUSSIAN MIXTURE
MODELS
Alice has a d-component data vector x and Bob knows
multivariate gaussian distributions of N classes ωi, i =
{1, . . . , N} that the vector could belong to. They would like
to engage in a protocol that lets Bob classify Alice’s data but
neither of them wants to disclose data to the other person. We
propose protocols which enable such computations.
The idea is to evaluate the value of the discriminant function
gi(x) = ln p(x|ωi) + lnP (ωi) (5)
for all classes ωi and assign x to class ωi if gi(x) > gj(x)
for all j 6= i. Here, p(x|ωi) is the class-conditional probability
density function and P (ωi) is the a priori probability of class
ωi. We consider two cases where: (1) each class is modeled
as a single multivariate gaussian, and (2) each class modeled
as a mixture of gaussians.
A. Case 1: Single Multivariate Gaussian
We assume that the distribution of data is multivariate
gaussian i.e. p(x|ωi) ∼ N (µi,Σi), where µi is the mean
vector and Σi is the covariance matrix of class ωi. Hence, the
log-likelihood is given by:
ln p(x|ωi) = −
1
2
(x−µi)
tΣ−1i (x−µi)−
d
2
ln 2π−
1
2
ln |Σi|
(6)
Ignoring the constant term (d/2) ln 2π, we can write equa-
tion (5) as:
gi(x) = −
1
2
(x−µi)
tΣ−1i (x−µi)−
1
2
ln |Σi|+lnP (ωi) (7)
Simplifying, we have:
gi(x) = x
TW̄ix + w̄
T
i x + wi0 (8)
where
W̄i = −
1
2
Σ−1i , w̄i = Σ
−1
i µi, and
wi0 = −
1
2
µTi Σ
−1
i µi −
1
2
ln |Σi|+ lnP (ωi) (9)
Let us create the (d + 1)-dimensional vectors x̄ and wi
by appending the value 1 to x and appending wi0 to w̄i. By
changing W̄i into a (d+1)×(d+1) matrix Wi where the first
d components of the last row are zeros and the last column
is equal to wTi , we can express equation (8) in a simplified
form:
gi(x) = x̄
TWix̄
Expressing x̄ as x for simplicity, we can write the above
equation as:
gi(x) = x
TWix (10)
Henceforth, we shall use x to denote a (d+1)-dimensional
vector with the last component equal to 1 unless otherwise
mentioned.
Protocol SMG: Single Multivariate Gaussian
Input: Alice has vector x, Bob has Wi for i = 1, 2, . . . , N .
We express the matrix Wi as [W1i W
2
i . . .W
d+1
i ], where W
j
i
is the j-th column of Wi.
Output: Alice learns I such that gI(x) > gj(x) for all j 6= I .
Bob learns nothing about x.
1) For i = 1, 2, . . . , N
a) For j = 1, . . . , d + 1, Alice and Bob perform
SIP (x,Wji ) to obtain the vectors ai = [a
1
i . . .
ad+1i ] and bi = [b
1
i . . . b
d+1
i ] respectively. Alice
then computes aix.
b) Alice and Bob perform SIP (bi,x) to obtain qi
and ri respectively.
2) Alice has vector A = [(a1x + q1) . . . (aNx + qN )] and
Bob has vector B = [r1 . . . rN ].
3) Alice and Bob perform the secure maximum index
protocol between the vectors A and B and Alice obtains
I = SMAX(A,B).
Correctness: In step 1, ai and bi are vectors such that
ai + bi = x
T Wi. Also, bix = qi + ri. Hence, xT Wix is
given by aix + qi + ri. I is the value of i for which xTWix
is maximum.
Efficiency: For a given i = I , the above protocol has (d+ 2)
SIP calls. Hence, it would take N(d+2) SIP calls and one
call of SMAX .
Security: If Bob gets to know the dot products of d different
vectors with x, he can learn x completely. However, we see
that neither Bob nor Alice ever learn the complete result of
any dot product. Hence, if the protocols for SIP and SMAX
are secure, the above protocol is secure.
B. Case 2: Mixture of Gaussians
Let us now consider the case where each class is modeled
as a mixture of gaussians. Let the mean vector and covariance
matrix of the j-th gaussian in class ωi be µij and Σij
respectively. Hence we have p(x|ωi) =
∑Ji
j=1 αijN (µij ,Σij)
where Ji is the number of gaussians describing class ωi and
αij are the mixture coefficients. The log likelihood for the j-th
gaussian in the i-th class is given by:
lij(x) = x
T W̄ijx + w̄
T
ijx + wij (11)
where
W̄ij = −
1
2
Σ−1ij , w̄ij = Σ
−1
ij µij , and
wij = −
1
2
µTijΣ
−1
ij µij −
1
2
ln |Σij |+ lnαij
Expressing x as a (d+1)-dimensional vector and W̄ij , w̄ij ,
wij together as the (d+ 1)× (d+ 1) matrix Wij as done in
the previous case, we can simplify equation (11) as:
lij(x) = x
T Wijx (12)
Hence, the discriminant function for the i-th class can be
written as
gi(x) = logsum
(
li1(x), . . . , liJi(x)
)
+ lnP (ωi)
= ln
( Ji
∑
j=1
elij(x)
)
+ lnP (ωi) (13)
Protocol MoG: Mixture of Gaussians
Input: Alice has vector x, Bob has Wij and P (ωi) for
i = 1, 2, . . . , N , and j = 1, 2, . . . , Ji.
Output: Alice learns I such that gI(x) > gj(x) for all j 6= I .
Bob learns nothing about x.
1) For i = 1, 2, . . . , N
a) Alice and Bob engage in steps 1 and 2 of Protocol
SMG for the Ji gaussians in the i-th mixture to
obtain vectors Ai = [Ai1 . . . AiJi ] and Bi =
[Bi1 . . . BiJi ]. Notice that Aij +Bij = lij(x).
b) Alice and Bob engage in the secure logsum pro-
tocol with vectors Ai and Bi to obtain ui and zi
i.e. ui + zi = SLOG(Ai,Bi).
2) Bob computes the vector v = [v1 . . . vN ] where vi =
zi + lnP (ωi). Alice forms the vector u = [u1 . . . uN ].
3) Alice and Bob perform the secure maximum index
protocol between vectors u and v and Alice obtains
I = SMAX(u,v).
Correctness: If one follows the protocol carefully, it is easy
to see that ui + vi is equal to gi(x).
Efficiency: For a given i, there are (Ji(d+2)+1) SIP calls.
Hence, in all, there are (d + 2)
∑N
i=1 Ji + N SIP calls and
1 SMAX call.
Security: If Protocol SMG and the protocols for SIP , SV AL
and SMAX are secure, the above protocol is secure. In
case Alice and Bob want to compute additive shares of the
likelihood instead of the class label, they can use the SV AL
protocol instead of SMAX in the last step.
C. Training Gaussian Mixture from Data
We now focus on a related problem. Let us suppose Alice
has K d-component vectors x1,x2, . . . ,xK . And she wants to
learn a mixture of c gaussians from the data. She can use the
iterative expectation-maximization (EM) algorithm to estimate
the parameters of the c gaussians and the mixture weights.
Now, consider the scenario when Bob wants to learn the
parameters but Alice does not want to disclose her data to Bob.
One solution is for Alice to estimate the parameters herself
and then give them to Bob. Another approach is for Alice
and Bob to engage in a secure protocol which lets Bob learn
the parameters while Alice’s data remain private. The latter
approach becomes necessary in a scenario where Bob wants
to do data mining on combined data from private databases
owned by Alice and Charlie. Below, we describe a secure
protocol which lets two parties perform such computations.
EM algorithm
We denote the estimate of a particular parameter after
the r-th iteration by using a superscript. Thus, µri , Σ
r
i and
P r(ωi) denote the mean vector, covariance matrix and the
mixture weight for the i-th gaussian after the r-th iteration.
For convenience, let us denote the entire parameter set after
the r-th iteration by λr. At any given iteration, Alice has
access to her data and Bob has access to the parameter Σri .
Alice and Bob have additive shares µriA, µ
r
iB and ℓiA, ℓiB
such that µriA + µ
r
iB = µ
r
i and ℓiA + ℓiB = lnP
r(ωi). We
can write the steps of the EM algorithm as follows:
E Step:
P (ωi|xk, λ
r) =
p(xk|ωi, µri ,Σ
r
i )P
r(ωi)
∑c
j=1 p(xk|ωj,µ
r
j ,Σ
r
j)P
r(ωj)
(14)
Input: Alice has xk, µriA and ℓiA; Bob has µ
r
iB , Σ
r
i and
ℓiB , i = 1, 2, . . . , c.
Output: Alice and Bob obtain uik and vik such that uik +
vik = lnP (ωi|xk, λ
r).
1) Bob forms matrices Wi for i = 1, . . . , c with µriB , Σ
r
i
as described in section IV-A and equation (9) (using
(d/2) ln 2π instead of lnP (ωi) to compute wi0). With
(xk − µriA) as Alice’s input and Wi for i = 1, . . . , c
as Bob’s input and N = c, Alice and Bob engage in
steps 1 and 2 of Protocol SMG (section IV-A) to obtain
vectors A′k and B
′
k.
• Log-likelihood ln p(xk|ωi,µri ,Σ
r
i ) is given by
equation 6. Notice that using (xk − µriA) in place
of xk and µriB in place of µ
r
i in equation 6 yields
the same result as using xk and µri .
• The sum of the i-th elements, A′ik + B
′
ik, is equal
to ln p(xk|ωi,µri ,Σ
r
i ).
2) Alice and Bob obtain vectors Ak and Bk, where for
each i, Aik = A′ik + ℓiA and Bik = B
′
ik + ℓiB .
• Notice that Aik + Bik is the logarithm of the
numerator in equation 14.
3) Alice and Bob engage in the secure logsum protocol
with the vectors Ak and Bk to obtain yk and zk i.e.
yk + zk = SLOG(Ak,Bk).
• Notice that yk + zk is the logarithm of the denom-
inator of equation 14 (follows from equation 4).
4) Alice forms vector uk, where uik = (Aik − yk). Bob
forms the vector vk , where vik = (Bik − zk)
• uik + vik = lnP (ωi|xk, λ
r).
M Step:
µr+1i =
∑K
k=1 P (ωi|xk, λ
r)xk
∑K
k=1 P (ωi|xk, λ
r)
P r+1(ωi) =
∑K
k=1 P (ωi|xk, λ
r)
K
Σr+1i =
∑K
k=1 P (ωi|xk, λ
r)(xk − µ
r+1
i )(xk − µ
r+1
i )
T
∑K
k=1 P (ωi|xk, λ
r)
(15)
Input: Alice has xk, k = 1, . . . ,K . Alice and Bob have
K-vectors E and F such that Ek + Fk = lnP (ωi|xk, λr).
Output: Alice obtains µr+1iA , ℓiA; Bob obtains µ
r+1
iB , Σ
r+1
i
and ℓiB . (µ
r+1
iA +µ
r+1
iB = µ
r+1
i and ℓiA+ℓiB = lnP
r+1(ωi)).
1) Alice and Bob engage in the secure logsum protocol
with vectors E and F to obtain e and f i.e. e + f =
SLOG(E,F).
2) Alice computes ℓiA = e − lnK , and Bob computes
ℓiB = f .
3) For j = 1, 2, . . . , d:
Let hj be the K-vector formed by the j-th elements of
x1, . . . ,xK . Alice and Bob engage in the secure logsum
protocol with vectors E + lnhj and F to obtain e′ and
f ′ i.e. e′ + f ′ = SLOG(E + lnhj ,F).
• Notice that (e′ − e) + (f ′ − f) = lnµr+1ij , the j-th
element of µr+1i .
Alice and Bob obtain the j-th elements of µr+1iA
and µr+1iB respectively as a result of SIP (exp(e
′ −
e), exp(f ′ − f)).
4) Consider the evaluation of σmn, the mn-th element of
the matrix Σr+1i . We first consider evaluating the mn-
th element of (xk−µ
r+1
i )(xk−µ
r+1
i )
T . As mentioned
earlier, this is equivalent to evaluating the mn-th term
of (x̄k − µ̄i)(x̄k − µ̄i)T , where x̄k = (xk −µ
r+1
iA ) and
µ̄i = µ
r+1
iB . Let the j-th elements of x̄k and µ̄i be x̄kj
and µ̄ij respectively. Notice that Alice has access to x̄k
and Bob had access to µ̄i.
• For k = 1, . . . ,K , Alice and Bob engage
in the secure inner product protocol with
vectors exp(γk)[x̄kmx̄kn,−x̄km, x̄kn, 1] and
[1, m̄uin,−µ̄im, µ̄imµ̄in], where γk is a random
scalar chosen by Alice. Let Bob obtain the result
φk.
• Alice forms the K-vector γ = [γ1, . . . , γK ] and Bob
forms the vector φ = [φ1, . . . , φK ].
Alice and Bob engage in the secure logsum protocol
with vectors (E− γ) and (F + lnφ) to obtain ē and f̄
i.e. ē+ f̄ = SLOG((E− γ), (F + lnφ)).
• Notice that (ē− e) + (f̄ − f) = lnσmn, the mn-th
element of Σr+1i .
Alice sends (ē−e) to Bob so that he can calculate σmn.
At the end of all iterations, Alice sends her shares µiA
and ℓiA to Bob so that he can calculate the mean µi and the
mixture weight P (ωi) for i = 1, 2, . . . , c.
Efficiency: We only consider the cost of computations that
occur between Alice and Bob. In the E-step, for a given xk
and for all classes ωi, there are c(d+2) SIP calls with (d+1)-
dimensional vectors and one SIP call involving a c-vector. In
the M-step, to compute a mixture weight, there is an SIP call
involving a K-vector. To calculate a single mean vector, there
are d SIP calls involving K-vectors and d SIP calls with
scalars. To calculate each element of the covariance matrix for
a given class, there are K SIP calls involving 4-dimensional
vectors and one SIP call with a K-vector.
Security: We assume that k ≫ d and d > c. Until the end of
the last iteration, Bob does not learn values of the means or the
mixture weights. He does not learn the values of likelihoods or
posterior probabilities during the iterations. He does learn the
value of the covariance matrix with every iteration. This does
leak some information about the distribution of Alice’s data
vectors but Bob’s aim is to learn the distributions. The goal
of Alice is to prevent Bob from knowing her individual data
vectors and without the mean, Bob cannot gain any knowledge
about the data vectors. Another important constraint is that
Alice does not learn the values of the parameters and following
the protocol closely shows that this holds true.
V. HIDDEN MARKOV MODELS
A. The Forward-Backward Procedure
Consider the forward variable αt(i) defined as
αt(i) = P (x1x2 . . .xt, qt = Si|λ) (16)
We can solve for αt(i) inductively and calculate P (X|λ) as
follows:
1) Initialization:
α1(i) = πibi(x1), 1 ≤ i ≤ N
Input: Bob has the gaussian mixture distribution that
defines bi(x) and the initial state distribution π = {πi};
Alice has an observation x1.
Output: Alice and Bob obtain vectors Q and R such
that Qi +Ri = lnα1(i).
a) Bob forms the matrices Wij as mentioned in sec-
tion IV-B. With matrices Wij and mixture weights
cjm as Bob’s inputs and x1 as Alice’s input, they
perform steps 1 and 2 of the protocol MoG of
section IV-B. Alice and Bob obtain vectors U and
V. Notice that Ui + Vi = ln bi(x1).
b) Alice forms the vector Q = U. Bob forms vector
R, where for each i, Ri = Vi + lnπi. Thus, Qi +
Ri = ln bi(x1) + lnπi = lnα1(i).
2) Induction:
αt+1(j) =
(
N
∑
i=1
αt(i)aij
)
bj(xt+1)
where 1 ≤ t ≤ T − 1, 1 ≤ j ≤ N
Input: Alice and Bob have vectors Q and R such that
Qi + Ri = lnαt(i). Alice and Bob have Uj and Vj
such that Uj + Vj = ln bj(xt+1). Bob has the vector
aj = [a1j , a2j , . . . , aNj ].
Output: Alice and Bob obtain Q̄ and R̄ such that Q̄+
R̄ = lnαt+1(j).
a) Alice and Bob engage in the secure logsum pro-
tocol with vectors Q and (R + lnaj) to obtain y′
and z′ i.e. y′ + z′ = SLOG(Q,R + lnaj).
b) Alice obtains Q̄ = y′ + Uj and Bob obtains R̄ =
z′ + Vj .
3) Termination:
P (X|λ) =
N
∑
i=1
αT (i).
Input: Alice and Bob have vectors Q and R such that
Qi + Ri = lnαT (i).
Output: Alice and Bob obtain y and z such that y+z =
lnP (X|λ).
a) Alice and Bob engage in the secure logsum pro-
tocol with vectors Q and R to obtain y and z i.e.
y + z = SLOG(Q,R).
Efficiency: In the initialization step, there are (d+ 2)MN +
N SIP calls and N SMAX/SVAL calls involving d-
dimensional vectors. In the induction step, for every j and
for every t, there is one SIP call with an N -vector. In the
termination step, there is one SIP call with an N -vector.
Security: Bob does not learn any xk and Alice does not
learn any of Bob’s parameters. Hence, if the primitives SIP ,
SMAX and SV AL are secure, the protocol is secure.
We can obtain a similar procedure for a backward variable
βt(i) defined as
βt(i) = P (xt+1xt+2 . . .xT |qt = Si, λ) (17)
We can solve for βt(i) inductively as follows:
1) Initialization:
βT (i) = 1, 1 ≤ i ≤ N
2) Induction:
βt(i) =
N
∑
j=1
aijbj(xt+1)βt+1(j),
where t = T − 1, T − 2, . . . , 1, 1 ≤ j ≤ N
Input: Alice and Bob have vectors Y and Z such that
Yj + Zj = lnβt+1(j). Alice and Bob have U and V
such that Uj + Vj = ln bj(xt+1). Bob has the vector
a′i = [ai1, ai2, . . . , aiN ].
Output: Alice and Bob obtain Ȳ and Z̄ such that Ȳ +
Z̄ = lnβt(i).
a) Alice and Bob engage in the secure
logsum protocol with vectors Y + U and
(Z + V + lna′i) to obtain Ȳ and Z̄ i.e.
Ȳ + Z̄ = SLOG(Y + U,Z + V + lna′i).
B. Viterbi Algorithm
Consider the quantity
δt(i) = max
q1,q2...qt−1
P [q1q2 . . . qt = Si,x1x2 . . .xt|λ] (18)
δt(i) is the best score (highest probability) along a single path,
at time t, which accounts for the first t observations and ends
in state Si. The procedure for finding the best state sequence
can be stated as follows:
1) Initialization:
δ1(i) = πibi(x1), ψ1(i) = 0 1 ≤ i ≤ N
The procedure is evaluating δ1(i) is analogous to the
initialization step of the forward backward procedure.
After this step, Alice and Bob will have additive shares
of ln δ1(i).
2) Recursion:
δt(j) =
(
max
1≤i≤N
[δt−1(i)aij ]
)
bj(xt)
ψt(j) = argmax1≤i≤N [δt−1(i)aij ]
where 2 ≤ t ≤ T, 1 ≤ j ≤ N
Input: Alice and Bob have vectors Q and R such that
Qi + Ri = ln δt−1(i). Alice and Bob have U and V
such that U + V = ln bj(xt). Bob has the vector aj =
[a1j , a2j , . . . , aNj ].
Output: Alice and Bob obtain Q̄ and R̄ such that Q̄+
R̄ = ln δt(j). Alice obtains ψt(j).
a) Alice and Bob engage in the secure maximum
value protocol with vectors Q and (R + lnaj) to
obtain y and z. They also perform SMAX on the
same vectors and Alice obtains the result which is
equal to ψt(j).
b) Alice computes Q̄ = y + U and Bob computes
R̄ = z + V .
3) Termination:
P ∗ = max
1≤i≤N
[δT (i)] q
∗
T = argmax1≤i≤NδT (i).
Alice and Bob will use SV AL on their additive shares
of ln δT (i) for all i to evaluate lnP ∗. Similarly, they
engage in SMAX on their shares and Alice obtains the
result q∗T .
4) Path backtracking:
q∗t = ψt+1(q
∗
t+1) t = T − 1, T − 2, . . . , 1.
Alice, who has access to qt and ψt, can evaluate the
path sequence. Notice that Bob could be made to get
this result instead of Alice if we let Bob learn the values
of ψt and qt in steps 2 and 3 instead of Alice.
Security and efficiency considerations for this protocol are
similar to what was discussed with regard to the Forward
Backward procedure (section V-A).
C. HMM Training
In the above formulation, we assumed that Bob had already
trained his HMMs. Let us consider the case when Alice has
all the training data and Bob wants to train a HMM using
her data. Below, we show how Bob can securely reestimate
parameters of his HMM.
Consider the variables
γt(i) = P (qt = Si|X, λ) =
(αt(i)βt(i))
P (X|λ)
ξt(i, j) = P (qt = Si, qt+1 = Sj |X, λ)
=
(αt(i)aijbj(xt+1)βt+1(j))
P (X|λ)
In the previous subsections, we have shown how Alice and
Bob can obtain additive shares of lnαt(i) (Qi and Ri), lnβt(i)
(Ȳ and Z̄), ln bj(xt+1) (Uj and Vj ), lnβt+1(j) (Yj and Zj)
and lnP (X|λ) (y and z). It is easy to see that using these
shares, Alice and Bob can compute additive shares et, gt and
ft, ht such that et + ft = ln ξt(i, j) and gt + ht = ln γt(i).
Alice computes gt = Qi+Ȳ−y and et = Qi+Uj+Yj−y. Bob
computes ht = Ri + Z̄−z and ft = Ri +lnaij +Vj +Zj−z.
The variables πi and aij can then be re-estimated as follows:
π̄i = γ1(i)
āij =
∑T−1
t=1 ξt(i, j)
∑T−1
t=1 γt(i)
.
Input: Alice and Bob have (T − 1)-vectors e and f such that
et +ft = ln ξt(i, j). They also have vectors g and h such that
gt + ht = ln γt(i).
Output: Bob obtains ln āij .
1) Alice and Bob engage in secure logsum protocol with
vectors e and f to obtain ē and f̄ . They also engage
in the secure logsum protocol with vectors g and h to
obtain ḡ and h̄ respectively.
2) Alice sends (ē−ḡ) to Bob. Bob computes (ē−ḡ)+(f̄−h̄)
to obtain ln āij .
Notice that instead of Bob obtaining the final result, Alice and
Bob can have additive shares of ln āij . Protocols for forward-
backward and viterbi algorithms will then have to modified so
that Alice and Bob have additive shares of the vector lnaj .
As for the gaussian mixture distributions bi(x), Bob can
learn them from Alice’s data securely as we have shown in
section IV-C. We emphasize here that Bob does not learn all
the parameters in every iteration. He learns the mean vector
for every component gaussian only after the last iteration.
He does learn the covariance matrix in every iteration but
quantities used to calculate the covariance matrix are additive
shares which does not help him in inferring Alice’s data. The
example shown in section IV-C uses two parties but it can
be generalized to the case where Bob learns from multiple
parties. In that case, learned statistics are averaged and provide
an additional layer of security for the data providers.
VI. DISCUSSION
In this section, we discuss the computational efficiency con-
siderations of protocols presented above. As mentioned earlier,
efficiency of the protocols was evaluated in terms of primitives
and absolute measures were not provided. This is due to the
fact that efficiency of the primitives themselves varies widely
and depends on how the primitives are implemented.
If one follows all the protocols carefully, efficiency mainly
depends on the computational complexity of the SIP primi-
tive. We shall focus on one particular implementation of this
primitive: secure inner product using homomorphic encryption
proposed by [9] (see appendix I, the reference provides proof
that the protocol is correct and secure).
To validate the secure model we ran experiments performing
learning and classification. The experiments were run using
a MATLAB implementation and tested both the Gaussian
mixture models and the hidden Markov models. Simulations
were performed twice using the secure and the non-secure
(traditional) methods. In all cases the results from both the
secure and non-secure simulations were numerically identical
as we have predicted. The secure versions were obviously
less efficient due to the increased computational cost of the
cryptographic operations and the increased network traffic.
We did not study the communications complexity in these
experiments and rather focused on the computational load,
which is the primary bottleneck [18]. One simulation used the
a generalized version [19] of the Paillier public-key scheme
[20]. We used cryptographic keys of 1024 bits and the cryp-
tosystem was implemented in Java. The computational load
of this algorithm coupled with a non-optimal implementation
resulted into a processing time per input vector in the order
of a few seconds. An alternative implementation using alge-
braic primitives, which leak some information but are more
computationally efficient, resulted into a significant speedup
of less than a second’s time processing per input vector. As
shown by the last experiment, the choice of implementation
for a primitive (for example, SIP using algorithm in appendix
I instead of SIP using algorithm in appendix II) significantly
impacts performance, communication complexity and security.
A wise choice will have to balance tradeoffs such as compu-
tational efficiency and network bandwidth as opposed to secu-
rity/privacy. A discussion of these issues is out of the scope of
this paper since it is a lengthy research project of its own and a
moving target given the continuous discoveries of increasingly
efficient protocols by the cryptography community.
The figures we have obtained were using non-optimized
implementations, as noted by [18] careful implementation can
produce significant speedups in computation. For practical
implementations it is also possible (and recommended) that
specialized hardware is used for the cryptography layer which
can result into dramatic performance improvements.
VII. CONCLUSIONS AND FUTURE WORK
In this paper we have presented an implementation of pri-
vacy preserving hidden Markov model and Gaussian mixtures
computations. We first proposed a simple privacy-preserving
protocol for computing logsums. Using primitives for comput-
ing scalar products and maxima, we proposed secure protocols
for classification using Gaussian mixture models. We then
proposed secure protocols for the forward-backward algo-
rithm, the viterbi algorithm and HMM training. The protocols
are defined modularly in terms of primitives so that future
advances in cryptography, which will hopefully provide more
robust and efficient protocols, can be readily employed in our
framework by straightforward replacement. The approach we
have taken also illustrates the process required to transform a
signal processing algorithm to its privacy preserving version.
Other data processing and classification algorithms can also
be described in terms of secure primitives and and easily
reformulated for secure multiparty computations.
This paper is intended to be a starting point for secure
audio frameworks and because of that it exposes a lot of
new research directions which warrant more attention. One of
these directions includes the design of alternative classifiers
and algorithms using this process, and there is still ongoing
work on the building block primitives (SIP , SMAX , SV AL,
etc) themselves. These are all topics that present plenty of
opportunities to explore efficiency and security and their
tradeoffs. We expect these to be fruitful areas of research in
the near future. It is our hope that a migration towards secure
algorithms can help promote a more open collaboration setting
where parties can freely exchange data and algorithms without
legal and privacy issues.
The authors would like to acknowledge the help and influ-
ence of Shai Avidan in the making of this work. The authors
also wish to thank anonymous reviewers for their comments
and suggestions.
APPENDIX I
SECURE INNER PRODUCT USING HOMOMORPHIC
ENCRYPTION
The following protocol is based on homomorphic encryp-
tion and was proposed by [9]. Let the triple (Ge, En, De)
denote a public-key homomorphic cryptosystem (probabilistic
polynomial time algorithms for key-generation, encryption
and decryption). The key generation algorithm generates a
valid pair (sk, pk) of private and public keys for a security
parameter k. The encryption algorithm En takes as an input a
plaintext m, a random value r and a public key pk and outputs
the corresponding ciphertext En(pk; m, r). The decryption
algorithm De takes as an input a ciphertext c and a private
key sk (corresponding to the public key pk) and outputs a
plaintext De(sk; c). It is required that De(sk; En(pk; m, r))
= m. A public-key cryptosystem is homomorphic if En(pk;
m1, r1)·En(pk; m2, r2) = En(pk; m1 +m2, r1 + r2), where
+ is a group operation and · is a groupoid operation.
Inputs: Private vectors x and y with Bob and Alice
respectively.
Outputs: Shares a and b such that a+ b = xT y.
1) Setup phase. Bob:
• generates a private and public key pair (sk, pk).
• sends pk to Alice.
2) For i ∈ {1, . . . , d}, Bob:
• generates a random new string ri.
• sends ci = En(pk; xi, ri) to Alice.
3) Alice:
• sets z ←
∏d
i=1 c
yi
i .
• generates a random plaintext b and a random nonce
r′.
• sends z′ = z·En(pk; −b, r′) to Bob.
4) Bob computes a = De(sk; z′) = xTy − b.
See [9] for a proof that the protocol is correct and secure.
APPENDIX II
SECURE INNER PRODUCT FROM OBLIVIOUS POLYNOMIAL
EVALUATION
[8] proposes an elegant protocol for oblivious evaluation
of multivariate polynomials using oblivious transfer [21] as a
cryptographic primitive. It can be easily modified to securely
evaluate dot products. Let Alice represent each xi as xi =
∑
j aij2
j−1 with aij ∈ {0, 1}. Let vij = 2j−1yi. Notice that
for each i, 1 ≤ i ≤ d,
∑
j aijvij = xiyi. The idea is to have
Bob prepare vij and have Alice get those vij with aij = 1 in
some secret way. This is achieved as follows: Bob prepares
the pair (rij , vij +rij) for randomly chosen rij and Alice runs
independent Oblivious Transfer with Bob to get rij if aij = 0
and vij + rij otherwise. At the end of the protocol, Alice will
obtain
∑
i
∑
j(aijvij + rij) =
∑
i xiyi +
∑
i,j rij . Bob will
have−
∑
i,j rij . Thus, Alice and Bob will have additive shares
of the desired dot product.
[8] proves that this protocol is secure when the parties are
semi-honest. The efficiency of the protocol depends on the
implementation of oblivious transfer.
APPENDIX III
SECURE INNER PRODUCT USING LINEAR
TRANSFORMATION
[11] proposes an algebraic approach which assumes that the
dimensionality is even. Let us define x1 as the d/2 dimensional
vector consisting of the first d/2 elements of x and x2 as the
vector consisting of the last d/2 elements of x. We observe
that xT y = xT1 y1 + x
T
2 y2. Alice and Bob jointly generate a
random invertible d×d matrix M . Alice computes x′ = xTM ,
splits it as x′1 and x
′
2 and sends x
′
2 to Bob. Bob computes
y′ = M−1y, splits it as y′1 and y
′
2 and sends y
′
1 to Alice.
Alice computes x′1y
′
1 and Bob computes x
′
2y
′
2 so that their
sum is equal to the desired result.
This protocol has little communication and computational
overhead compared to the cryptographic protocols but it comes
at the cost of security. Alice and Bob learn d/2 linear equations
for the d unknowns that constitute the other party’s vector
which leaks a lot of information. Hence, it is important that
the same matrix M should be used when this protocol is used
multiple times with the same vector x (or y). [3] has analyzed
this protocol which showed serious security flaws and hence
this is not practical when security is crucially important.
APPENDIX IV
PERMUTE PROTOCOL
This protocol was proposed in [15].
Input: Alice and Bob have d-component vectors x and y.
Bob has a random permutation π.
Output: Alice and Bob obtain q and s such that
q + s = π(x) + π(y).
1) Alice generates public and private keys for a homomor-
phic cryptosystem and sends the public key to Bob. Let
E() denote encryption with Alice’s public key.
2) Alice encrypts each element of x and sends the resulting
vector x̄ to Bob.
3) Bob generates a random vector r and computes a new
vector θ where θi = x̄iE(ri) = E(xi + ri), for i =
1, . . . , d.
4) Bob permutes θ and sends π(θ) to Alice. Alice decrypts
the vector to obtain q.
5) Bob computes y − r and then permutes it using π to
obtain s = π(y − r).
Alice and Bob engage in the above permute protocol twice,
the second time with their roles interchanged. After this is
done, Alice and Bob will have two vectors whose sum will be
a random permutation of the original sum but neither of them
will know what the permutation is.
REFERENCES
[1] A. C.-C. Yao, “Protocols for secure computation,” in Proc. of the 23rd
IEEE Symposium on Foundations of Computer Science, 1982, pp. 160–
164.
[2] J. Vaidya and C. Clifton, “Privacy preserving k-means clustering
over vertically partitioned data,” in ACM SIGKDD Conf on
Knowledge Discovery and Data Mining, 2003. [Online]. Available:
citeseer.ist.psu.edu/vaidya03privacypreserving.html
[3] E. Kiltz, G. Leander, and J. Malone-Lee, “Secure computation of
the mean and related statistics,” in Proceedings of the Theory
of Cryptography Conference, ser. Lecture Notes in Computer
Science, vol. 3378, 2005, pp. 283–302. [Online]. Available:
http://eprint.iacr.org/2004/359.pdf
[4] S. Avidan and M. Butman, “Blind vision,” in ECCV, 2006. [Online].
Available: http://www.merl.com/reports/docs/TR2006-006.pdf
[5] O. Goldreich, “Secure multi-party computation,” Working Draft, 2000.
[Online]. Available: citeseer.ist.psu.edu/goldreich98secure.html
[6] L. R. Rabiner, “A tutorial on hidden markov models and selected
applications in speech recognition,” Proceedings of the IEEE, vol. 77,
no. 2, pp. 257–286, Feb 1989.
[7] S. Laur and H. Lipmaa, “Additive conditional disclosure
of secrets and applications,” Cryptology ePrint Archive,
Report 2005/378, 2005, http://eprint.iacr.org/. [Online]. Available:
citeseer.ist.psu.edu/laur05additive.html
[8] Y.-C. Chang and C.-J. Lu, “Oblivious polynomial evaluation and
oblivious neural learning,” in Advances in Cryptology, Asiacrypt ’01,
ser. Lecture Notes in Computer Science, vol. 2248, 2001, pp. 369–384.
[Online]. Available: citeseer.ist.psu.edu/chang01oblivious.html
[9] B. Goethals, S. Laur, H. Lipmaa, and T. Mielikainen, “On
private scalar product computation for privacy-preserving data
mining,” in Intl. Conference on Information Security and
Cryptology, ser. Lecture Notes in Computer Science, C. Park
and S. Chee, Eds., vol. 2506, 2004, pp. 104–120. [Online]. Available:
http://citeseer.ist.psu.edu/goethals04private.html
[10] W. Du and M. J. Atallah, “Privacy-preserving cooperative statistical
analysis,” in Proceedings of the 17th Annual Computer Security
Applications Conference, New Orleans, Louisiana, December 2001.
[Online]. Available: http://citeseer.ist.psu.edu/549869.html
[11] W. Du and Z. Zhan, “A practical approach to solve secure multi-party
computation problems,” in Proceedings of New Security Paradigms
Workshop, Virginia Beach, virginia, USA, September 23-26 2002.
[Online]. Available: citeseer.ist.psu.edu/du02practical.html
[12] I. Ioannidis, A. Grama, and M. Atallah, “A secure
protocol for computing dot-products in clustered and distributed
environments,” in Proceedings of the Intl. Conf. on Parallel
Processing, Vancouver, Canada, 2002. [Online]. Available:
http://ieeexplore.ieee.org/iel5/8068/22315/01040894.pdf
[13] P. Ravikumar, W. W. Cohen, and S. E. Fienberg, “A secure protocol
for computing string distance metrics,” in Proceedings of the Workshop
on Privacy and Security Aspects of Data Mining, Brighton, UK, 2004,
pp. 40–46. [Online]. Available: http://citeseer.ist.psu.edu/719096.html
[14] J. Vaidya and C. Clifton, “Privacy preserving association
rule mining in vertically partitioned data,” in Proceedings of
the Intl. Conf. on Knowledge Discovery and Data Mining.
Edmonton, Canada: ACM SIGKDD, 2002. [Online]. Available:
http://citeseer.ist.psu.edu/vaidya02privacy.html
[15] M. J. Atallah, F. Kerschbaum, and W. Du, “Secure and private sequence
comparisons,” in Proceedings of Workshop on Privacy in the Electronic
Society, Washington, DC, USA, October 2003. [Online]. Available:
http://citeseer.ist.psu.edu/atallah03secure.html
[16] I. F. Blake and V. Kolesnikov, “Strong conditional oblivious
transfer and computing on intervals,” in Proceedings of Advances
in Cryptology - ASIACRYPT’04, ser. LNCS, P. J. Lee, Ed., vol.
3329. Springer-Verlag, 2004, pp. 515–529. [Online]. Available:
http://citeseer.ist.psu.edu/blake04strong.html
[17] H.-Y. Lin and W.-G. Tzeng, “An efficient solution to the millionaires’
problem based on homomorphic encryption,” in Proc of Intl. Conf.
on Applied Cryptography and Network Security, ser. LNCS, vol.
3531. Springer-Verlag, 2005, pp. 456–466. [Online]. Available:
http://citeseer.ist.psu.edu/lin05efficient.html
[18] Z. Yang, R. Wright, and H. Subramaniam, “Experimental
analysis of a privacy-preserving scalar product protocol,”
Intl Jrnl of Computer Systems Science and Engineering,
vol. 21, no. 1, pp. 47–52, 2006. [Online]. Available:
http://www.cs.stevens.edu/ rwright/Publications/csse06.pdf
[19] I. Damgard and M. Jurik, “A generalisation, simplification and
some applications of paillier’s probabilistic public-key system,” in
Proceedings of the Intl. Workshop on Practice and Theory in Public Key
Cryptography, ser. Lecture Notes in Computer Science, vol. 1992, 2001,
pp. 119–136. [Online]. Available: http://citeseer.ist.psu.edu/383099.html
[20] P. Paillier, “Public-key cryptosystems based on composite degree
residuosity classes,” in Proceedings of Advances in Cryptology
- EUROCRYPT ’99, ser. Lecture Notes in Computer Science,
J. Stern, Ed., vol. 1592, 1999, pp. 223–238. [Online]. Available:
citeseer.ist.psu.edu/paillier99publickey.html
[21] M. Naor and B. Pinkas, “Oblivious transfer and polynomial
evaluation,” in Proceedings of the thirty-first annual ACM symposium
on Theory of computing, 1999, pp. 245–254. [Online]. Available:
http://portal.acm.org/citation.cfm?id=301312
PLACE
PHOTO
HERE
Madhusudana Shashanka received BE (Hons) in
computer science from the Birla Institute of Tech-
nology and Science, Pilani, India in June 2003. He
is currently a PhD student with the Department
of Cognitive & Neural Systems and the Hearing
Research Center at Boston University.
PLACE
PHOTO
HERE
Paris Smaragdis is a member of the research
staff in Mitsubishi Electric Research Laboratories in
Cambridge MA, USA. Prior to that position he was
at the Massachusetts Institute of Technology where
he completed his graduate and postdoctoral training.
His interests are computational audition, scene anal-
ysis and the intersection of machine learning with
signal processing.

