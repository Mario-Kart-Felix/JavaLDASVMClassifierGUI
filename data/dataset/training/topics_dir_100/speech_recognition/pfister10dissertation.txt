Tomas Pfister
tjp35@cam.ac.uk
Emotion Detection
from Speech
Computer Science Tripos Part II
Gonville & Caius College
2009-2010
The portrait on the cover shows Charles Darwin (1809–1882), who pioneered
research in emotions with his book The Expression of the Emotions in Man and
Animals.
Proforma
Name and College: Tomas Pfister, Gonville & Caius College
Project Title: Emotion Detection from Speech
Examination: Computer Science Tripos 2010
Word Count: Approx 10,000
Project Originator & Supervisor: Prof P. Robinson
Original Aim
To implement a real-time classification algorithm for inferring emotions from the
non-verbal features of speech. It is expected to successfully extract a set of fea-
tures and use them to train and detect emotions from speech. As an optional
extension, the emotion detector will be applied to perform speech quality assess-
ment.
The underlying theory of machine learning requires an understanding of math-
ematics in machine learning and optimisation not formally covered in the Com-
puter Science Tripos. A significant amount of time would be devoted to acquire
this background knowledge.
Work Completed
Both the core and extension parts of the project are completed. The emotion
detector identifies simultaneously occurring emotion states by identifying correla-
tions between emotions and features such as pitch, loudness and energy. Pairwise
classifiers are constructed for nine classes from the Mind Reading corpus, yielding
an average cross-validation accuracy of 89% for the pairwise machines and 86%
for the fused machine. The system achieves real-time performance. The resulting
classifier outperforms a recent PhD thesis and a number of papers.
Special Difficulties
None.
I
Declaration
I, Tomas Pfister of Gonville & Caius College, being a candidate for Part II of
the Computer Science Tripos, hereby declare that this dissertation and the work
described in it are my own work, unaided except as may be specified below, and
that the dissertation does not contain material that has already been used to any
substantial extent for a comparable purpose.
Signed:
Date:
II
Contents
1 Introduction 1
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 Previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.4 Reading guide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Preparation 5
2.1 Theoretical background . . . . . . . . . . . . . . . . . . . . . . . . 5
2.1.1 Support Vector Machines . . . . . . . . . . . . . . . . . . . 6
2.1.2 Practically computable version . . . . . . . . . . . . . . . . 7
2.1.3 Extended version – mislabelling and kernels . . . . . . . . 8
2.1.4 Grid search . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.1.5 Pairwise fusion . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2 Requirements analysis . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2.1 Priorities, risks and specification . . . . . . . . . . . . . . . 9
2.2.2 Programming language . . . . . . . . . . . . . . . . . . . . 11
2.2.3 Libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2.4 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.2.5 Software engineering techniques . . . . . . . . . . . . . . . 13
3 Implementation 15
III
3.1 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.1.1 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.1.2 Feature extraction . . . . . . . . . . . . . . . . . . . . . . 20
3.1.3 Feature selection . . . . . . . . . . . . . . . . . . . . . . . 21
3.1.4 Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.1.5 Grid search . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.2 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
3.2.1 Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.2.2 Support vector machines . . . . . . . . . . . . . . . . . . . 28
3.2.3 Pairwise classification . . . . . . . . . . . . . . . . . . . . . 28
3.2.4 Pairwise fusion mechanism . . . . . . . . . . . . . . . . . . 29
3.2.5 Speech quality assessment . . . . . . . . . . . . . . . . . . 31
3.2.6 Front-end interface . . . . . . . . . . . . . . . . . . . . . . 33
4 Evaluation 35
4.1 The system implementation . . . . . . . . . . . . . . . . . . . . . 35
4.1.1 Overall accuracy . . . . . . . . . . . . . . . . . . . . . . . 35
4.1.2 Real-time Performance . . . . . . . . . . . . . . . . . . . . 36
4.1.3 Functionality and testing . . . . . . . . . . . . . . . . . . . 37
4.2 Classification results . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.2.1 Experiment 1: Comparison to previous work . . . . . . . . 38
4.2.2 Experiment 2: Machine learning algorithms . . . . . . . . 39
4.2.3 Experiment 3: Grid search for SVM parameter optimisation 41
4.2.4 Experiment 4: Pairwise fusion methods . . . . . . . . . . . 41
4.2.5 Experiment 5: Speech quality assessment . . . . . . . . . . 47
5 Conclusion 49
IV
Bibliography 51
A Sample source code 55
A.1 C++ code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
A.2 Bash script . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
A.3 Unit tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
B Extracted features 59
V
Acknowledgements
This work could not have been completed without support from my supervisor,
Prof P. Robinson.
I also owe a big thank you to the Cambridge-based speech coach company Elo-
quential, who kindly agreed to label their speech tutoring recordings and allowed
them to be used in this project. I also thank Tal Sobol Shikler, who helpfully
located some recordings used for evaluating the system. Finally, I wish to thank
Sophia Zhang for her encouragement and support.
VI
Chapter 1
Introduction
This dissertation describes the creation of a framework for performing real-time
analysis of speech for detecting emotional states and assessing the quality of
speech. It shows how the classification is accomplished in real-time and how the
system outperforms recent research in speech emotion detection. Both the core
and the extension part of the project were completed on schedule.
The core part of this project exploits a balanced mixture of theory and imple-
mentation to create small, compact, highly optimised pairwise emotion classifiers.
Experiment 1 shows that the framework achieves high classification accuracies
that outperform previous research. A closer look at machine learning algorithms
is taken in Experiment 2. The results indicate that highly optimised support
vector machines achieve higher cross-validation accuracies for emotion detection
than other algorithms.
The high improvement in the classification accuracy resulting from using a pa-
rameter optimisation algorithm is illustrated in Experiment 3. Finally, the fusion
of pairwise classifiers is described in Experiment 4. Results show that using max-
imum probability as a criterion for classifier fusion achieves highest accuracies for
single-label outputs.
The extension part of the project was also completed. Namely, the emotion
detector is extended to support speech quality assessment. Experiment 5 shows
that a high classification accuracy is achieved.
Section 1.1 explains the key motivations for studying emotions. Section 1.2 de-
scribes some of the expected challenges, and Section 1.3 gives a short overview of
the previous work in the field. Finally, Section 1.4 gives a brief guide for reading
1.1. MOTIVATION CHAPTER 1. INTRODUCTION
the rest of this dissertation.
1.1 Motivation
Emotions are fundamental for humans, impacting perception and everyday activ-
ities such as communication, learning and decision-making. They are expressed
through speech, facial expressions, gestures and other non-verbal clues.
Speech emotion detection refers to analysing vocal behaviour as a marker of
affect, with focus on the nonverbal aspects of speech. Its basic assumption is
that there is a set of objectively measurable parameters in voice that reflect the
affective state a person is currently expressing. This assumption is supported by
the fact that most affective states involve physiological reactions which in turn
modify the process by which voice is produced. For example, anger often produces
changes in respiration and increases muscle tension, influencing the vibration of
the vocal folds and vocal tract shape and affecting the acoustic characteristics of
the speech [25]. So far, vocal emotion expression has received less attention than
the facial equivalent, mirroring the relative emphasis by pioneers such as Charles
Darwin.
In the past, emotions were considered to be hard to measure and were con-
sequently not studied by computer scientists. Although the field has recently
received an increase in contributions, it remains a new area of study with a num-
ber of potential applications. These include emotional hearing aids for people
with autism; detection of an angry caller at an automated call centre to transfer
to a human; or presentation style adjustment of a computerised e-learning tutor
if the student is bored.
A new application of emotion detection proposed in this dissertation is speech
tutoring. Especially in persuasive communication, special attention is required to
what non-verbal clues the speaker conveys. Untrained speakers often come across
as bland, lifeless and colourless. Precisely measuring and analysing the voice is
a difficult task and has in the past been entirely subjective. By using a similar
approach as for detecting emotions, this report shows that such judgements can
be made objective.
2
CHAPTER 1. INTRODUCTION 1.2. CHALLENGES
1.2 Challenges
This section describes some of the expected challenges in implementing a real-
time speech emotion detector.
Firstly, discovering which features are indicative of emotion classes is a difficult
task. The key challenge, in emotion detection and in pattern recognition in
general, is to maximise the between-class variability whilst minimising the within-
class variability so that classes are well separated. However, features indicating
different emotional states may be overlapping, and there may be multiple ways
of expressing the same emotional state. One strategy is to compute as many
features as possible. Optimisation algorithms can then be applied to select the
features contributing most to the discrimination while ignoring others, creating
a compact emotion code that can be used for classification. This avoids making
difficult a priori assumptions about which features may be relevant.
Secondly, previous studies indicate that several emotions can occur simultane-
ously [14]. For example, co-occurring emotions could include being happy at
the same time as being tired, or feeling touched, surprised and excited when
hearing good news. This requires a classifier that can infer multiple temporally
co-occurring emotions.
Thirdly, real-time classification will require choosing and implementing efficient
algorithms and data structures.
Despite there existing some working systems, implementations are still seen as
challenging and are generally expected to be imperfect and imprecise.
1.3 Previous work
A number of sources were useful in learning about the area and implementing
the project. In particular, Sobol Shikler’s [27] emotion detector that processes
speech samples as batch jobs using pair-wise decision machines has influenced
some of the design decisions. This report will show that the emotion detector in
this project achieves higher accuracy and does so in real-time.
Significant fundamental work on speech emotion detection was done by Dellaert
et al. [6], who proposed the use of statistical pattern recognition techniques for
emotion detection and set the basic system architecture still used today. Since
then, the accuracy has continually been improved [3, 9, 28], albeit the focus has
3
1.4. READING GUIDE CHAPTER 1. INTRODUCTION
been on categorising two to five emotions, in contrast to nine in this work.
The closest implementation to the approach in this project is probably open-
EAR [7], a recently released open-source emotion and affect recognition toolkit.
Its feature extraction library openSMILE is used in this work as it is one of the
most extensive free speech feature extraction libraries. It differs from this project
in that it does not support pair-wise machines or grid search for support vector
machine parameter optimisation. Moreover, the corpora used in this project are
different.
1.4 Reading guide
This project builds upon the following lecture courses:
• Artificial Intelligence I and II
• Information Theory and Coding
• Mathematical Methods for Computer Science
• Digital Signal Processing
• Natural Language Processing
• Programming in C/C++
• Software Engineering
• Algorithms I and II
In addition to these courses, a significant amount of knowledge in machine learn-
ing was gained. A summary of the relevant theory is given in Chapter 2.
The next chapters of the dissertation are organised as follows. Chapter 2 describes
the preparation undertaken before the emotion detector was implemented. In
addition to describing some of the theory required for implementation, it describes
the software engineering concepts applied when designing the emotion detector.
Chapter 3 explains how the emotion detector is implemented, including descrip-
tions of some interesting challenges. Chapter 4 evaluates the emotion detector
by analysing its performance and comparing its accuracy with previous work.
Finally, Chapter 5 presents a concise conclusion.
4
Chapter 2
Preparation
This chapter illustrates the preparation undertaken before implementing the emo-
tion classifier. Section 2.1 gives a description of the theoretical results upon which
the implementation is built.
Section 2.2 discusses the requirements of the emotion classifier, justifying some
of the implementation choices and showing evidence that software engineering
concepts are thoroughly applied. It outlines the audio features used for classifi-
cation, including measures such as signal energy, pitch and voice quality. It also
justifies the choice of labels for emotion classification (absorbed, excited, inter-
ested, joyful, opposed, stressed, sure, thinking and unsure) and for speech quality
assessment (clear, credible, competent, dynamic, persuasive and pleasant).
2.1 Theoretical background
This section describes the theory behind the machine learning algorithm used
in this work. Firstly, Subsection 2.1.1 describes the Support Vector Machine
classification algorithm which lies at the core of the emotion classifier.
Subsection 2.1.2 shows how the classifier can be implemented, and Subsection
2.1.3 shows how the algorithm can be extended to allow mislabelling and the use
of different kernels. Finally, Subsections 2.1.4 and 2.1.5 describe how the classifier
can be optimised and how multiple classifiers can be fused.
2.1. THEORETICAL BACKGROUND CHAPTER 2. PREPARATION
2.1.1 Support Vector Machines
This section defines the classification problem and shows how the Support Vector
Machine classifier presents an approximate solution for it.
A classification task, such as classifying the emotional state, involves a set of
training data Strain and testing data Stest, each containing a set of data instances
i ∈ S. Each instance i is a tuple (li, fi), where li ∈ {1,−1} is a class label, with 1
and −1 indicating the class, and fi ∈ Rn is a set of feature attributes. The goal
of classification is to produce a model using Strain which predicts the class labels
of Stest given a set of features fi.
In this work, it was found that the Support Vector Machine (SVM) machine
learning algorithm produced the highest emotion classification accuracy. In the
case of SVMs, we create the model by constructing an N -dimensional hyperplane
that optimally separates data into two categories. Optimality is taken to be the
maximal separation between the two classes. Any such hyperplane can be written
as the set of points x = fi satisfying
w · x− b = 0
where w is the normal vector perpendicular to the hyperplane.
We want to choose w and b such that the distance between two parallel hyper-
planes separating the data between the two classes is maximised. These hyper-
planes can be described by
w · x− b = 1
w · x− b = −1
with the distance given by 1‖w‖ . Thus to maximise the distance between the two
planes, we wish to minimise ‖w‖, while satisfying the constraints
w · x− b ≥ 1
w · x− b ≤ −1
for the first and second class respectively, or
li(w · x− b) ≥ 1
6
CHAPTER 2. PREPARATION 2.1. THEORETICAL BACKGROUND
since li ∈ {1,−1}. These relations are shown graphically in Figure 2.1.
Figure 2.1: Maximum separating hyperplane with margins for two classes.
2.1.2 Practically computable version
This section shows how the computation of an optimal hyperplane reduces to
solving a Lagrange multiplier optimisation problem.
The norm ||w|| is difficult to compute since it involves a square root, so in practice
||w|| is replaced by 1
2
||w||2, with factor 1
2
used for convenience [8]. The solution
is derived by solving the Lagrange multiplier optimisation problem
min
w,b,β
{1
2
‖w‖2 −
n∑
i=1
βi[li(w · xi − b)− 1]}. (2.1)
where n=|Strain| and βi are Lagrange multipliers.
7
2.1. THEORETICAL BACKGROUND CHAPTER 2. PREPARATION
2.1.3 Extended version – mislabelling and kernels
In this work, an extended version of SVMs that allows for mislabelled examples
is used. This Soft Margin [29] approach will choose a hyperplane as cleanly as
possible even if there is no hyperplane that can split the two classes. We measure
this degree of misclassification by the variable ξi and require the solution of the
optimisation problem
min
w,b,ξ
{1
2
‖w‖2 + C
∑
i
ξi} (2.2)
under constraints
li(w · xi − b) ≥ 1− ξi 1 ≤ i ≤ n
ξi ≥ 0.
where C > 0 is the penalty for mislabelled examples. This can be solved using
Lagrange multipliers as in Equation 2.1.
Moreover, rather than using a linear classifier, this work will use a non-linear
classifier. This replaces the linear dot product xi · xj by a kernel function that
transforms the original input space into a higher-dimensional feature space, al-
lowing the SVM to be non-linear and thus potentially better separate the two
classes. After trialling several possible kernel function candidates, including the
polynomial K(xi,xj) = (xi · xj)d, the Radial Basis Function (RBF)
K(xi,xj) = exp(−γ‖xi − xj‖2) (2.3)
with exponentiation coefficient γ > 0, was found to yield the most promising
results. Using a method described in previous research [16], probability estimates
are attached to the classification results.
2.1.4 Grid search
When using the Radial Basis Function SVM kernel, choosing the penalty C for
mislabelled examples and the exponentiation constant γ in equations 2.2 and
2.3 becomes important. Because the optimal values are model-specific, a search
algorithm is needed for finding a near-optimal set of values.
8
CHAPTER 2. PREPARATION 2.2. REQUIREMENTS ANALYSIS
The goal is to identify good (C, γ) values so that the classifier can accurately
predict unseen testing data, Stest, rather than choosing them to maximise pre-
diction accuracy for the training data, Strain, whose labelling is already known.
To achieve this, pairs of (C, γ) can be tried sequentially in a range, picking the
pair with the highest accuracy on Stest.
2.1.5 Pairwise fusion
To generalise SVMs to more than two classes, pairwise classification is used. A
single multiclass problem is reduced into multiple binary problems by building a
classifier for each pair of classes, using only instances from two classes at a time.
The pairwise machines then output the probabilities for the two classes.
In order to determine the most probable class, the probabilities of the multiple
binary SVMs need to be fused. A number of ways to fuse the probabilities into
a ranked list exist, including for example:
1. Majority voting, where each pairwise decision is counted as a vote for a
class.
2. Maximum combined probability, where classes are ranked according to their
total probability from the binary SVMs.
3. Votes above a threshold, where classes that receive pairwise votes above a
certain threshold are returned.
2.2 Requirements analysis
This section aims to describe the project planning process. Some important
tradeoffs and design choices are explained and justified.
2.2.1 Priorities, risks and specification
A list of requirements and their associated risk, priority and difficulty is given in
Table 2.1, detailing the work to be done to achieve the goals set in the original
project proposal.
The overall system design is shown in Figure 2.2. Brief justifications of the choices
made are given in the subsections below.
9
2.2. REQUIREMENTS ANALYSIS CHAPTER 2. PREPARATION
Component Description Risk Priority Difficulty
Understanding underlying theory Medium High High
Build tools for pre-processing corpora High High Low
Implement SVM training tools Medium High Medium
Implement optimisation algorithms for SVMs Medium Medium High
Implement real-time audio segmentation algorithm Medium High Medium
Implement pairwise classification algorithm High High High
Implement voting algorithms for classification Medium Medium Medium
Build front-end for visualising results Medium Low Medium
Table 2.1: Requirements analysis.
Pre-process corpora Segment live audio
Extract audio features Extract audio features
Compute SVM models Run SVM classifiers
Combine pairwise decisions
to give ranking
? ?
? ?
-
?
Figure 2.2: Schematic flowchart of the functionality implemented for emotion
detection from speech.
10
CHAPTER 2. PREPARATION 2.2. REQUIREMENTS ANALYSIS
2.2.2 Programming language
The program is implemented in C++. This is done to allow interfacing with
the openSMILE feature extraction library and the highest performing machine
learning libraries (see Section 2.2.3).
2.2.3 Libraries
A set of libraries are used, for machine learning, digital signal processing and
graphical user interface development. This section justifies the choices.
Machine learning
In previous work on emotion recognition from speech [27], support vector ma-
chines (SVMs) and tree algorithms such as C4.5 have been found to be effective.
Other methods such as the Naive Bayesian Classifier and Perceptrons were evalu-
ated using the Weka data mining toolkit [12], but the results were unsatisfactory.
Experiment 2 in Section 4.2.2 shows a comparison of C4.5 and SVMs, the two
classifiers achieving highest accuracies. SVMs gave the most promising results by
a considerable margin. The LibSVM [8] support vector machine library is chosen
as it is widely used and well maintained.
Feature extraction
Three feature extraction algorithms were considered:
1. Edinburgh Speech Tools [15]. It is a commonly used open-source C++
speech processing library that provides many feature extraction algorithms.
However, the code has very little documentation and lacks modularity. The
set of feature extraction algorithms included many irrelevant features used
for speech recognition and speech synthesis, and lacked useful features such
as pitch strength, voice quality (harmonics to noise ratio), spectral centroid
and spectral flux.
2. Matlab [18] with toolkits. By extending the basic functionality of Matlab
with the Signal Processing Toolbox and VOICEBOX [5], a large numbers
of features could be extracted. However, Matlab did not achieve real-time
performance.
11
2.2. REQUIREMENTS ANALYSIS CHAPTER 2. PREPARATION
3. OpenSMILE [7] is chosen. It is an open-source C++ feature extractor
whose algorithms can be run in real-time. The code is freely available
and well documented, has a highly modular structure, and provides the
capability of doing on-line incremental processing by freely interconnecting
feature extractor components. It extracts features such as energy, loudness,
pitch, mel-spectra, voice quality, mel-spectrum frequency coefficients, and
can calculate various functionals such as means, extremes, peaks, percentiles
and deviations. A list of supported features is given in Appendix B.
Graphical user interface
The graphical user interface is written in Qt4. Qt is object-oriented, cross-
platform and provides a thorough documentation [20]. For plotting, the Qwt
library [24] is used. It provides a 2D plot widget for showing the detection results
graphically in real-time.
2.2.4 Corpora
A core part of the project is the choice of training corpora for emotion detection
and speech quality analysis. This section justifies the choices and briefly describes
the corpora.
For emotion detection, the Mind Reading corpus is used [2]. Speech quality
assessment required a new corpus, named Speech Tutor, to be defined as part of
this work.
Mind Reading corpus
The Mind Reading corpus is part of the Mind Reading DVD, which is an interac-
tive emotion guide aimed at autistic children. It was developed by psychologists
led by Prof Baron-Cohen at University of Cambridge Autism Research Centre,
aiming to help autistic children to recognise both basic and complex emotions.
The corpus consists of 2927 acted sentences, covering 442 different concepts of
emotions, each with 5–7 sentences, in 24 emotion groups. Its number of sam-
ples is high and the labelling has been thoroughly evaluated [11]. It also allows
comparison of the results with previous work [27].
12
CHAPTER 2. PREPARATION 2.2. REQUIREMENTS ANALYSIS
afraid angry bored bothered1 disbelieved
disgusted excited2 fond happy3 hurt
interested4,5 kind liked romantic sad
sneaky sorry sure6 surprised think7
touched unfriendly8 unsure9 wanting
Table 2.2: The 24 emotion groups in the Mind Reading corpus presented by
Baron-Cohen [11]. The superscripts indicate the main groups from which the
subset of affective states are selected for this project. These were originally chosen
by Sobol Shikler [27], and are used here to allow comparison of the results.
The main emotion groups of Mind Reading are shown in Table 2.2. For the clas-
sifier, a subset of 9 categories representing a large variety of emotions is chosen.
These subsets are: absorbed, excited, interested, joyful, opposed, stressed, sure,
thinking and unsure.
Speech Tutor corpus
Applying machine learning to speech quality assessment is a new idea proposed
in this dissertation. As such, there were no labelled samples available for training
the system. An experienced speech coach was asked to label 124 one-minute-long
samples from 31 different people. The chosen six labels are ones that the pro-
fessional is accustomed to using when assessing the speech quality. The samples
were given a score on a scale 4–10 for each six classes shown in Table 2.3.
clear competent credible
dynamic persuasive pleasant
Table 2.3: The six speech quality classes chosen for labelling by the speech coach
for use with the Speech Tutor corpus.
2.2.5 Software engineering techniques
This subsection gives evidence that the development of the emotion detector
complies with sound software engineering principles.
13
2.2. REQUIREMENTS ANALYSIS CHAPTER 2. PREPARATION
Development models
The program is written using Xcode, which provides an efficient source-code
processing interface.
The Iterative development model is chosen as the implementation strategy. Ini-
tially, a basic prototype of the system in Figure 2.2 is designed, implemented and
tested. At consecutive stages increasing layers of complexity are added. Each
iteration is thoroughly tested before the next iteration, allowing module-specific
debugging. The cross-validation and computation speed are closely monitored at
each iteration, allowing both to be maximised.
Unit testing
Modularising the system into separate components allows a set of unit tests to
be created for each module. The Bash script for running the tests is given in
Appendix A.3.
Redundancy
The Subversion version control system is used to allow going back to a previous
revision in case changes need to be undone. Physical redundancy is guarded for
by automated hourly backups to an external hard disk, and automated daily
backups to PWF and another laptop.
14
Chapter 3
Implementation
The previous chapter gave a description of some important theoretical ideas
upon which the implementation is built and discussed some software engineering
choices. This chapter describes how the theory is applied in practice to create
a real-time emotion detector that achieves high accuracy. The overall system
architecture and the design of the main components are outlined.
All components were successfully implemented, resulting in a fully-working emo-
tion detector and speech quality assessor. An example view of the system is
shown in Figure 3.1.
The implementation work can be roughly divided into two parts. Their respective
key components to be described in this chapter are:
1. Training
1. Preprocessing
Describes the conversion of a single corpus into pairwise corpora, and
discusses the chosen noise reduction and corpus splitting solutions.
2. Feature extraction
Presents the openSMILE feature extraction library and summarises
how it is integrated into the project.
3. Feature selection
Introduces the feature selection algorithm used to reduce the number
of features.
4. Scaling
Briefly discusses the decision to scale the input.
CHAPTER 3. IMPLEMENTATION
Figure 3.1: Screenshot showing the user interface. The peaks represent the de-
tected emotions with their associated probability. New results appear on the right
and move left with a rate of 27 samples per second. In this example, thresholding
is used for fusion of the pairwise machines, allowing co-occurring emotions to be
detected. A history of 100 time samples is shown. The Quality Analysis button
switches the classifier over to do speech quality assessment.
16
CHAPTER 3. IMPLEMENTATION 3.1. TRAINING
5. Grid search
Describes the grid search algorithm implemented to find the optimal
SVM parameters.
2. Classification
1. Segmentation
Describes the audio segmentation algorithm employed for splitting in-
put audio into samples ready to be processed.
2. SVM model computation
Briefly describes the library used for computing the SVM model and
how it is integrated into the project.
3. Pairwise classification
Introduces the pairwise classification algorithm used to combine the
outputs from multiple SVMs.
4. Pairwise fusion mechanism
Explains how the pairwise classification algorithm is used to determine
the winning labels.
5. Speech quality assessment
Describes how the emotion detector was retrained to perform speech
quality assessment for speech tutoring.
6. Front-end interface
Describes the implementation decisions for the front-end interface for
visualising the output.
3.1 Training
The training phase of a supervised machine learning algorithm looks for patterns
in the features extracted from a labelled corpus and computes models that can
later be used for classification. As it can be done as a pre-processing step, run-
time performance is not critical. The training system architecture is shown in
Figure 3.2. Its main components are discussed below.
3.1.1 Preprocessing
Before training the system, some preprocessing is required. This section describes
the work undertaken to prepare the corpora for feature extraction and training.
17
3.1. TRAINING CHAPTER 3. IMPLEMENTATION
Input corpus with labels (l1, ..., ln)
Convert into pairwise corpora C =
{(l1, l2), ..., (l1, ln), (l2, l3)...(ln−1, ln)}
For all i, j.ci,j ∈ C extract feature set F (ci,j)
Select best separating features f(ci,j) ⊆ F (ci,j)
Grid search SVMi,j’s RBF kernel parameters
(Ci,j, γi,j) that maximise cross-validation accuracy
Compute SVM model τi,j from optimal parame-
ters (Ci,j, γi,j)
Output models τi,j
?
?
?
?
?
?
Figure 3.2: The training program architecture. SVMi,j represents the support
vector machine for comparing li with lj.
18
CHAPTER 3. IMPLEMENTATION 3.1. TRAINING
Conversion into pairwise corpora
This section shows how the binary SVM classifier is extended to perform multi-
label classification.
A proposed direct multi-class extension of SVMs leads to a very complex optimi-
sation problem [22], and did not yield satisfactory results in a preliminary study.
Therefore, the multi-class problem was solved by training several binary SVM
classifiers and fusing the outputs to derive a global classification decision.
One of the simplest multi-label classification schemes is the one-against-all ap-
proach. In this method, n pairwise classifiers are built in a way that each classifier
separates one class from the others. However, it results in very heterogeneous
training data sets, leading to poor performance [19].
Previous studies have showed that a more efficient decomposition is pairwise
classification [10]. Here a single multiclass problem is reduced into multiple binary
problems. A classifier is built for each pair of classes, using only instances from the
chosen two classes. Each pairwise machine then outputs the probabilities for the
two classes it has compared. Experiments on the corpora in this study confirmed
that this was the case, so pairwise classification was chosen for implementation.
Given k classes, pairwise classification creates
(
k
2
)
= k!
2!(k−2)! = k(k − 1)/2 SVM
classifiers. Because each SVM only involves instances of two classes, dividing n
instances evenly among k classes given 2n/k instances per problem. Although
this sounds unnecessarily computationally expensive, suppose that SVM runs in
O(n). Then the pairwise execution is
O(
k(k − 1)
2
· 2n
k
) = O((k − 1)n)
so the pairwise method scales linearly with the number of classes.
The chosen multilabelling scheme classifies k labels with k(k−1)/2 binary SVMs.
In the training stage, this required conversion of the corpus with labels (l1, ..., ln)
into a set of pairwise corpora C = {(l1, l2), ..., (l1, ln), (l2, l3)...(ln−1, ln)}. For this,
a script (given in Appendix A.2) is written that generates training corpora for
all
(
k
2
)
binary combinations of the classes.
19
3.1. TRAINING CHAPTER 3. IMPLEMENTATION
Noise reduction
The Mind Reading corpus used for emotion detection was recorded with high-
quality equipment and is noise-free. However, the Speech Tutor corpus was
recorded in different environments and contained background noise. To avoid the
noise affecting the feature extraction and the construction of the hyperplanes, it
was necessary to remove noise from the corpus.
Various noise reduction algorithms, such as Power subtraction [4] and Time fre-
quency block thresholding [30] were tried. The former models the speech signal
as a random process to which uncorrelated random noise is added. The noise is
measured during a silence period in the speech, and the estimated power spec-
trum of the noise is then subtracted from the noisy input signal. This method
resulted in an artefact which sounds like random musical notes caused by nar-
rowband tonal components that appeared in unvoiced sound and silence regions
after the noise reduction.
Time frequency block thresholding dynamically adjusts spectrogram filter param-
eters using the Stein risk estimator, which gives an indication of the estimator’s
accuracy. It was found to eliminate the musical noise of the Power subtraction
method, and was thus used as a pre-processing filter for the Speech Tutor corpus.
Sample splitting
The Speech Tutor corpus consisted of 1–1.5 min long samples. Since the tutor
is to work in real-time and some of the feature extraction algorithms depended
on the sample length, the samples are split into sentences by the segmentation
algorithm described in Section 3.2.1.
3.1.2 Feature extraction
Feature extraction algorithms are one of the main components of the emotion
detection system. For this work, the openSMILE [7] algorithms are used.
OpenSMILE provides sound recording and playback via the open-source PortAu-
dio library, echo cancellation, windowing functions, fast Fourier transforms and
autocorrelation. Moreover, it is capable of extracting features such as energy,
loudness, pitch, mel-spectra, voice quality, mel-spectrum frequency coefficients,
and can calculate various functionals such as means, extremes, peaks, percentiles
20
CHAPTER 3. IMPLEMENTATION 3.1. TRAINING
and deviations. The full set of features and functionals is given in Appendix B.
The set of features to be extracted can be specified in a configuration file.
The ten most commonly used class-differentiating features are shown in Table
3.1. These consist of functionals calculated for two main categories.
The first group consisted of the magnitude of Fourier transforms. The Fourier
transform converts the input signal from time domain into frequency domain.
This allows analysis of the frequency content of speech. The second common
group is Mel-frequency cepstral coefficients (MFCCs). MFCCs are coefficients
that collectively make up a Mel-frequency cepstrum (MFC). MFCs represent the
short-term power spectrum of sound based on the linear cosine transform of a log
power spectrum on a nonlinear mel frequency scale.
Machines Feature name Feature group
12 mfcc sma[12] range Mel-frequency cepstral coefficients
9 pcm Mag fband250-650 sma de centroid Fast Fourier Transform (FFT) mag-
nitude for a frequency band
8 pcm Mag melspec sma de de[4] quartile3 FFT magnitude Mel spectrum
8 mfcc sma de de[4] qregerrQ Mel-frequency cepstral coefficients
8 mfcc sma[8] range Mel-frequency cepstral coefficients
7 pcm Mag melspec sma de[2] quartile2 FFT magnitude Mel spectrum
7 mfcc sma[4] minameandist Mel-frequency cepstral coefficients
7 mfcc sma[4] iqr1-2 Mel-frequency cepstral coefficients
6 pcm Mag melspec sma de de[4] iqr2-3 FFT magnitude Mel spectrum
6 pcm Mag melspec sma de de[19] minPos FFT magnitude Mel spectrum
Table 3.1: Ten most used features in emotion detection. The first column shows
the number of pairwise machines in the total
(
9
2
)
= 36 that used the feature.
3.1.3 Feature selection
Since a large feature set is to be extracted from the speech, it is expected that
there would be some irrelevant and redundant data that would not improve the
SVM prediction performance. It is known that classification algorithms are un-
able to attain high classification accuracy if there is a large number of weakly
relevant and redundant features, a problem known as the curse of dimension-
ality [1]. Algorithms also suffer from computational load incurred by the high
21
3.1. TRAINING CHAPTER 3. IMPLEMENTATION
dimensional data. Hence, a feature selection algorithm is used to reduce the
number of features and to remove redundant and irrelevant data [17].
One strategy is to compute as many features as possible. Optimisation algorithms
can then be applied to select the most differentiating features and reduce their
number. This avoids making difficult a priori decisions about which features may
be relevant.
The approach taken is to use all openSMILE feature extractors, and then pick
the most relevant ones using feature selection. For this, the Correlation-based
Feature Selection (CFS) algorithm [13] is used. It uses a heuristic based on the
assumption that good feature sets contain features highly correlated with the
class and uncorrelated with each other. It defines the score for a feature subset
S as
MeritS =
krcf√
k + k(k − 1)rff
where k is the number of features in S, rff is the average feature-feature in-
tercorrelation and rcf is the average feature-class correlation. After discretising
the features, CFS calculates feature-class and feature-feature correlations using
a symmetric form of information gain
Hsym(X;Y ) =
2I(X;Y )
H(X) +H(Y )
for random variables X, Y where
H(X) = −
∑
x∈X
p(x) log2 p(x)
H(X|Y ) = −
∑
y∈Y
p(y)
∑
x∈X
p(x|y) log2 p(x|y)
with H(X) representing the entropy of X and H(X|Y ) the entropy of X after
observing Y . The information gain is
I(X;Y ) = H(X)−H(X|Y ) = H(Y )−H(Y |X).
Hsym(X;Y ) fixes the problem of I(X;Y ) assigning a higher value for features
with a greater number of values although they may be less informative. It also
normalises the values to the range [0, 1].
22
CHAPTER 3. IMPLEMENTATION 3.1. TRAINING
After calculating the correlations, CFS starts with an empty set of features and
applies forward best first search, terminating when it encounters five consecutive
fully-expanded subsets that show no improvement.
3.1.4 Scaling
After the feature selection stage, the features f(ci,j) ⊆ F (ci,j) are scaled from R
to [−1,+1]. This is done to
1. Prohibit attributes in a greater numeric range from dominating those in
smaller ranges.
2. Avoid numerical difficulties during the calculation, such as division by large
f(ci,j).
3.1.5 Grid search
As noted in Section 2.1.3, when using the Radial Basis Function SVM kernel,
it is important to choose a suitable penalty for mislabelled examples C and the
exponentiation constant γ. Because the optimal values are model-specific, a
search algorithm is needed for finding a near-optimal set of values.
The goal is to identify good (C, γ) values so that the classifier can accurately
predict unseen testing data, rather than choosing them to maximise prediction
accuracy for the training data whose labelling is already known. A common
approach for this is to divide the training data into two parts, one for training
and the other for testing. This allows the optimised accuracy to more precisely
reflect the prediction accuracy on unknown data.
In this work, v-fold cross-validation is used. The training set is divided into v
equal-sized subsets, with each subset sequentially tested using a classifier trained
on the remaining v − 1 subsets. Every subset is predicted once, so the cross-
validation accuracy is the percentage of data which are correctly classified.
The Grid-Search algorithm (Algorithm 1) sequentially tries pairs of (C, γ)
in a given range, and picks the one with the highest cross-validation accuracy.
Exponentially growing sequences were found to work well in practice, as suggested
in previous research [23]. The algorithm is then run recursively on a shrinking
area, with the number of recursions set by the initial value of nstep.
23
3.2. CLASSIFICATION CHAPTER 3. IMPLEMENTATION
As will be discussed in more detail in the evaluation chapter, grid search consid-
erably improved the emotion detection accuracy rates.
Algorithm 1 Grid search algorithm. The algorithm is run by giving the misla-
belling penalty C and exponentiation constant γ ranges initial values. Variable
nsteps defines how fine-grained the grid search is.
Grid-Search([Clow, Chigh, Cstep],[γlow, γhigh, γsteps], nsteps)
1. Initialise P = 0, Copt = 0, γopt = 0.
2. For all i ∈ Z+ ∪ {0} such that Ci = (Clow + iCstep) ≤ Chigh
1. For all j ∈ Z+ ∪ {0} such that γj = (γlow + jγstep) ≤ γhigh
1. Divide training model into v equal subsets (for predefined v).
2. Sequentially classify one subset using a classifier trained on the
remaining v − 1 subsets using parameters C = 2Ci and γ = 2γi .
3. If P < (ncorrect/v), set P = (ncorrect/v), Copt = Ci, γopt = γj.
3. If nsteps ≤ 0, return (Copt, γopt).
4. Else Grid-Search([Copt − nsteps, Copt + nsteps, Cstep/2],[γopt − nsteps, γopt +
nsteps, γstep/2], nsteps − 1).
Once optimal (C, γ) are determined, final SVM models are computed for all
pairwise corpora using the LibSVM [8] library.
3.2 Classification
The classification phase of a supervised machine learning algorithm uses the mod-
els computed in the training phase to predict labels for unseen samples. Run-time
performance in this phase is critical for providing a real-time system. The system
architecture for the real-time classifier is shown in Figure 3.3. Its main compo-
nents are described below.
24
CHAPTER 3. IMPLEMENTATION 3.2. CLASSIFICATION
Input live audio A
For all i, j.ci,j ∈ C read model τi,j and selected
features f(ci,j)
Segment A into utterances A′ = {α1, α2, ...} for
αm with loudness above threshold
Extract selected features f(ci,j)
Run SVM1,2(f(c1,2)) and
extract p1, p2 ...
Run SVMn−1,n(f(cn−1,n))
and extract pn−1, pn
Calculate win count ωi and total probability ψi
Output ψi
n−1 if ωi ≥ λ, where λ is the threshold
?
?
?
? ? ?
? ? ?
?
Figure 3.3: The classifier architecture. SVMi,j computes the probabilities pi and
pj for labels i, j, using features f(ci,j).
25
3.2. CLASSIFICATION CHAPTER 3. IMPLEMENTATION
3.2.1 Segmentation
Before the audio can be classified it needs to be segmented into pieces. Features
can then be extracted, mapped into an N -dimensional space and separated by a
hyperplane.
One approach to the problem would be to just process every 1 second separately.
However, this would create two problems.
1. The single second may only contain silence, but the SVMs will still need
to make a binary decision between two classes by mapping the features on
one side of the hyperplane.
2. Emotions may not be expressible within a short time interval. Furthermore,
some features have a temporal characteristic which will not be extractable
if a single segment length is used.
As a result it was necessary to choose the segment length dynamically, approx-
imating to one segment per sentence. The signal energy could be used for dif-
ferentiating between silence and speech. The choice was between a simple algo-
rithm that uses static thresholds and a more complex algorithm that implements
dynamic thresholding. In accordance with the iterative development model, a
simple static algorithm was implemented first. By adding complexity in layers, it
could at each step be checked that performance sufficient for real-time operation
is achieved.
The Segmentation algorithm (Algorithm 2) achieves this by defining three
thresholds. First, the silence threshold η defines the threshold for the energy
E =
∑n
i |si|2 > η, for signals si in frame of size n. Second, ρstart sets the number
of frames with energy above η that are required until a segment start is detected.
Third, ρend is the number of frames below η until a segment end is detected.
From the start to the end of a segment, the features are extracted. At the end
the pairwise SVMs are run in parallel.
It turned out that the segmentation results of this algorithm were more than
sufficient for detecting pauses in the speech. In practice, separate η could be
used for a quiet room and for a noisier environment.
26
CHAPTER 3. IMPLEMENTATION 3.2. CLASSIFICATION
Algorithm 2 Audio segmentation algorithm. η is the silence threshold based
on the energy of the signal. ρstart and ρend specify the thresholds for the number
of frames that need to be above and below η for a segment to start and end,
respectively.
Segmentation(η, ρstart, ρend)
1. Initialise Cstart = Cend = 0
2. Repeat
1. If
∑n
i |si|2 > η
1. Set Cend = 0
2. Increment Cstart
3. If Cstart > ρstart
1. Send start inference message to all pairwise SVMs.
2. Else
1. Set Cstart = 0
2. Increment Cend
3. If Cend > ρend
1. Send end inference message to all pairwise SVMs.
27
3.2. CLASSIFICATION CHAPTER 3. IMPLEMENTATION
3.2.2 Support vector machines
Once the audio is segmented and the features are extracted, the n(n − 1)/2
pairwise machines can be run in parallel to predict the class for a segment. The
hyperplanes separate two emotion classes in an |f |-dimensional space, where f is
the set of features being considered. Implementation-wise, this required preparing
the features for interfacing with the LibSVM library and then using the results to
do further processing. The LibSVM library provides a highly optimised algorithm
for solving the Lagrange multiplier optimisation problem in Equation 2.2.
The Run-SVM algorithm (Algorithm 3) describes this process. First, all but the
features selected by the correlation-based feature selection algorithm needed to be
disabled. The algorithm then waits for extracted features from openSMILE. Once
it receives these, it scales them by the same ratio as used in the training phase.
It then predicts probabilities for both labels by calling the LibSVM C++ library,
and sends the results paired with the labels to the Pairwise-Combinator mod-
ule.
Algorithm 3 Run-SVM algorithm. τ is the model file, (li, lj) are the pairwise
class labels, S is the scaling file and ζ is the feature selection file. These are
computed in the training phase.
Run-SVM(τ, (li, lj), S, ζ)
1. Load τ , (li, lj), S and ζ from files
2. Filter away all features except ζ
3. Repeat
1. Receive features fi ∈ ζ from openSMILE components
2. Apply scaling S used in training
3. Predict probabilities (pi, pj) by applying LibSVM on (τ, f)
4. Send [(li, pi), (lj, pj)] to Pairwise-Combinator
3.2.3 Pairwise classification
When the pairwise SVMs have been run, their results need to be combined so
that the label of the segment can be predicted. This glues together the SVMs
28
CHAPTER 3. IMPLEMENTATION 3.2. CLASSIFICATION
running in parallel and the voting algorithm.
The Pairwise-Combinator algorithm (Algorithm 4) achieves this by receiving
the results from the 1
2
n(n−1) pairwise SVMs running in parallel. It keeps count of
the labels of the winning classes. Once it has received all pairwise classification
results for a segment, it runs a pairwise voting algorithm that determines the
winning class. The voting is described in the next section.
Algorithm 4 Pairwise combinator algorithm. n is the number of labels. As there
are 1
2
n(n− 1) pairwise machines, each label should receive n− 1 probabilities.
Pairwise-Combinator(n)
1. Initialise P = {S1, ..., Sn} with ∀i.Si = {}, W = {ω1, ..., ωn} with ∀i.ωi = 0
2. Repeat
1. If ∀i.|Si| < n− 1
1. Receive [(li, pi), (lj, pj)] from Run-SVM
2. Insert pi into Si and pj into lj
2. Else
1. For all i set ωi =
∑
p∈Si g(p) where g(p) =
{
1 for p ≥ 1
2
0 otherwise
2. Run Pairwise-Voting(P,W ) and set ∀i.Si = {} ∧ ωi = 0
3.2.4 Pairwise fusion mechanism
In order to determine the most probable class, the probabilities of the multiple
binary classifiers need to be fused.
Studies in psychology indicate that several emotions can occur simultane-
ously [14]. Examples of co-occurring emotions include being happy at the same
time as being tired, or feeling touched, surprised and excited when hearing good
news. This section proposes a fusion method for determining co-occurring emo-
tions. Whereas in traditional single-label classification a sample is associated
with a single label li from a set of disjoint labels L, multi-label classification as-
sociates each sample with a set of labels L′ ⊆ L. A previous study concluded
that the use of complex non-linear fusion methods yielded only marginal benefits
29
3.2. CLASSIFICATION CHAPTER 3. IMPLEMENTATION
(0.3%) over linear methods when used with SVMs [22]. Therefore, three linear
fusion methods are implemented:
1. Majority voting using wins from binary classifiers.
2. Maximum combined probability from binary classifiers.
3. Binary classification wins above a threshold.
In the first method we consider all n−1 SVM outputs per class as votes and select
the class with most votes. Assuming that the classes are mutually exclusive, the
a posteriori probability for feature vector f is pi = P (f ∈ classi). The classifier
SVMi,j computes an estimate p̂i,j of the binary decision probability
pi,j = P (f ∈ classi|f ∈ classi ∪ classj)
between classes i and j. The final classification decision D̂voting is the class i for
which
D̂voting = arg max
1≤i≤n
∑
j 6=i
g(p̂i,j)
where
g(p) =
{
1 for p ≥ 1
2
0 otherwise
as computed by Algorithm 4. Ties are solved by declaring the class with higher
probability to be the winner.
In the second method, the maximum probability ψi =
∑
p∈Si p of the binary
SVMs is determined. The winner of decision D̂probability is i such that
D̂probability = arg max
1≤i≤n
∑
j 6=i
p̂i,j.
Finally, for detecting co-occurring emotions, the classes are ranked according to
the number of wins. The classes with wins above a threshold λ are returned, with
the classification decision D̂threshold being set of classes
30
CHAPTER 3. IMPLEMENTATION 3.2. CLASSIFICATION
D̂threshold = {i|
∑
j 6=i
g(p̂i,j) ≥ λ}.
The Pairwise-Voting algorithm (Algorithm 5) combines these three ap-
proaches. An experiment comparing the results using the three methods is de-
tailed in the evaluation section. The total wins and probabilities are calculated
for each label li. The resulting label and its average probability over the n − 1
SVMs is sent to the GUI over TCP. Sample C++ code for voting is shown in
Appendix A.1.
Algorithm 5 Pairwise voting algorithm. µ is the mean win count and σ is the
standard deviation. The threshold definition follows previous research [27].
Pairwise-Voting(P,W )
1. Set win threshold λ = b(µ+ σ)(n− 1)c
2. For each li with i ≤ n using pi ∈ Si, Si ∈ P and ωi ∈ W
1. Set the total probability ψi =
∑
p∈Si p with norm(ψi) =
ψi
n−1
2. If ωi ≥ λ
1. Send (li, norm(ψi)) to GUI
To allow for comparison with previous research [27], the threshold is set to λ =
b(µ+σ)(n−1)c. By the central limit theorem, the distribution of a sum of many
independent, identically distributed random variables (RVs) tends towards the
normal distribution. By assuming that the SVMs exhibit such RVs, and since
for the normal distribution µ+ σ ≈ 0.841, λ = b0.841(n− 1)c. In particular, for
the 9 classes chosen for evaluation, λ = 6. An example console output for the
Pairwise-Voting algorithm is shown in Figure 3.4.
3.2.5 Speech quality assessment
For assessing speech quality, the classifier is retrained using six labels describing
speech quality given in Table 2.3. This is a novel application of the classifier that
has not been attempted before.
Following the speech classifier requirements set by Schuller et al. [26], the appli-
cation uses non-acted, non-prompted, realistic data with many speakers, using all
31
3.2. CLASSIFICATION CHAPTER 3. IMPLEMENTATION
Figure 3.4: Screenshot showing the console debugging output for the fusion of the
pairwise machines. The Pairwise-Combinator algorithm collects the probabil-
ities of the pairwise SVMs and runs Pairwise-Voting once it has received the
outputs from all 1
2
n(n−1) = 36 pairwise machines, with n = 9. The probabilities
from the pairwise SVMs are shown for the 9 labels. The values on the right of
the equal signs are the normalised total probability and the win count for each
class. The line below the class-wise results shows the winners when using voting
or maximum probability for pairwise fusion.
32
CHAPTER 3. IMPLEMENTATION 3.2. CLASSIFICATION
obtained data. An experienced speech coach was asked to label 124 one-minute-
long samples of natural audio from 31 people attending speech coaching sessions.
The chosen six labels are the ones that the professional is accustomed to using
when assessing the public speaking skills of clients. The samples are labelled on a
scale 4–10 for each class. The samples of classes are divided into higher and lower
halves according to the score. The upper half represents a positive detection of
the class (e.g. clear), and the lower half represents a negative detection (e.g. not
clear).
One binary SVM per class is used to derive a class-wise probability. If a pair-
wise approach similar to that in emotion classification had been used, the same
samples would have existed in several classes, making separating the classes in-
tractable. As a result, unlike in emotion detection where the most prominent
labels describing the speech are selected, for speech quality assessment all classes
are detected, each labelled with a probability. This allows users to attempt to
maximise all class probabilities, a goal which is more useful for speech coaching.
The Speech Tutor corpus consisted of natural audio as opposed to the acted audio
used for emotion detection. This created two new challenges.
Firstly, unambiguous speech qualities are often apparent in only a small portion
of a real corpus, and may thus provide too infrequent data for providing a basis
for consistent annotation and modelling using fine-grained labels. In this study, a
local speech coach was asked to annotate the corpus according to her professional
judgement. However, for some labels the corpus did not present enough variation
to allow for useful labelling, with all samples being scored 9 or 10 on a scale 4–10.
Secondly, there is varying levels of background noise as the corpus was recorded
in multiple locations at varying times, causing some samples to be correlated due
to similar noise rather than similar non-verbal speech. This is solved using the
noise reduction approach described in Section 3.1.1.
3.2.6 Front-end interface
A simple front-end interface is built to allow results to be visualised by the user
and debugged during development. The results are presented graphically using
Qt4 together with the Qwt Qt widget library. The resulting user interface is
shown in Figure 3.1.
To maximise the flexibility for the use-cases, the communication between the
back-end and the front-end is encapsulated within TCP packets. This allows the
33
3.2. CLASSIFICATION CHAPTER 3. IMPLEMENTATION
recording machine to be separate from viewer. For a higher number of classes,
computation can be done on a server with the results presented in real-time on
a lightweight device such as a mobile phone.
The main challenge was to separate a TCP server into a separate thread from
the drawing, allowing real-time graphing of the results. As expected, the Qwt
documentation was also lacking, so time was spent reading its source code. How-
ever, the time provided to be well spent, as once Qwt’s quirks were understood,
the resulting GUI worked smoothly.
34
Chapter 4
Evaluation
The project is evaluated from two perspectives. Firstly, the system implementa-
tion is evaluated in terms of how well it achieves the original overall goals. This is
discussed in Section 4.1. Secondly, a quantitative analysis of the class detection
accuracies and computation performance of the system is presented in a number
of experiments in Section 4.2.
4.1 The system implementation
This section evaluates the training and classification system from the developer’s
point of view. The system is examined in terms of its accuracy, performance and
functionality.
4.1.1 Overall accuracy
The success criteria defined in the project proposal set three main goals:
1. Core: Successfully extract a set of features and use them to train and detect
emotions from speech with a reasonable accuracy.
2. Core: High run-time performance.
3. Extension: Successfully apply the emotion detector to provide feedback
about public speaking skills.
4.1. THE SYSTEM IMPLEMENTATION CHAPTER 4. EVALUATION
As will be shown in this chapter, the detection results outperform a recent PhD
thesis (Table 4.1) and a number of papers [6, 21, 28]. Moreover, unlike the
previous research which runs as a batch job, the detection can be run in real-
time. The extension, which presents a novel application of the emotion detector
for speech quality assessment, provides useful feedback by detecting the best
applicable quality classes.
Type of data Accuracy (%)
70–30 training/testing split 86 (79)
Training data 99 (81)
Table 4.1: Emotion detection accuracies in percentages. The results in the PhD
thesis [27] are shown in parentheses. A random choice would result in 11%
accuracy.
4.1.2 Real-time Performance
One requirement for the system was to run in real-time, in contrast to previous
work which could only be run as a batch job [27]. This section presents evidence
that this goal is achieved.
The average latency in milliseconds of the classification stage is shown in Figure
4.1. It is measured as the time between the detection of the end of a segment
and the presentation of the result. As shown in the figure, the system classifies
normal sentences (1–15 s) in 0.046–0.110 s, which is a barely noticeable delay for
users.
The increase of latency with longer sentences is caused by the feature extractors
that need to process more frames with longer audio segments. However, since
speakers need to pause to breath, and thus limit the segment length, this increase
in latency did not cause problems in practice.
To avoid interfering with concurrently running processes, the resource usage is
capped during development. The resulting average resource usage of the training
and classification stages is shown in Table 4.2. The very low memory consumption
and sub-1/10 CPU usage allows the trainer and classifier to be run on multi-user
systems.
Overall, the performance met the real-time user response requirement set in the
project plan. The low latency can be attributed to the use of fast feature ex-
36
CHAPTER 4. EVALUATION 4.1. THE SYSTEM IMPLEMENTATION
Figure 4.1: Average live classification latency in milliseconds on a 2.66 GHz
MacBook Pro with 4 GB RAM.
Type of data CPU (%) Memory (%)
Training 11.5 0.1
Classification 10.0 0.9
Table 4.2: Average resource usage on a 2.66 GHz MacBook Pro with 4 GB RAM.
traction and machine learning libraries, and choice of efficient data structures for
combining the pairwise SVMs. Moreover, the feature selection stage reduced the
feature set size by over 100x on average, significantly lowering the complexity of
the optimisation and classification problem for the SVMs.
4.1.3 Functionality and testing
Another important area of evaluation is the features implemented and their test-
ing.
All the essential features that the project was set to achieve, namely a fully work-
ing emotion detector and optimisation algorithms, are successfully implemented.
The extension for speech quality analysis is also achieved. This reflects adequate
37
4.2. CLASSIFICATION RESULTS CHAPTER 4. EVALUATION
planning and preparation before starting the implementation.
As the system consists of a number of linearly connected components, each of
which affects the results of the next one, it is crucial to perform thorough and
systematic testing of the components. Throughout the development, a number
of unit tests were written to test both the individual and combined functionality
of the components. A simple unit testing framework was written (Appendix A.3,
Figure 4.2) to run the tests. Each test consisted of a set of predefined inputs and
outputs and logic for checking that the output equals the predefined output.
Figure 4.2: Screenshot showing the unit test framework results.
4.2 Classification results
This section evaluates the project in terms of how well speech is classified. A
wide range of experiments were conducted, with the five most interesting ones
being reported in the subsections below. The purpose of the experiments is
twofold. First, it is to investigate the accuracies obtained and how they compare
to previous work. Second, it is also to illustrate the wide range of algorithms
implemented and how they improved the detection rates.
4.2.1 Experiment 1: Comparison to previous work
This experiment aims to compare the accuracy of the classifier against that pre-
sented by Sobol Shikler’s PhD thesis [27] and a number of papers [6, 21, 28]. The
38
CHAPTER 4. EVALUATION 4.2. CLASSIFICATION RESULTS
approaches towards classification are similar to Sobol Shikler, but this project
runs in real-time and uses a larger full feature set F .
For evaluation, the commonly used 10-fold cross-validation method is employed.
The training set is divided into 10 equal-sized subsets. Each subset is sequen-
tially tested used a classifier trained on the remaining 9 subsets. Every subset is
classified once, so the cross-validation accuracy is the percentage of data which
are correctly classified.
Table 4.3 shows the cross-validation accuracies of the
(
9
2
)
= 36 pairwise machines
for the 9 categories selected in Section 2.2.4. Each entry in the table represents
the 10-fold cross-validation accuracy for one of the SVMs used to differentiate
between two classes.
All accuracies are greater than the values obtained in previous research. The
results are constantly above 80%, in contrast to the lower bound 60% obtained
previously. Overall, the machines achieved remarkably high accuracies in pairwise
decisions.
Compared to Dellaert [6], Petrushin [21] and Steidl [28], who achieved accuracies
60–65% (4 classes), 70% (5 classes) and 60% (4 classes) respectively, the results
shown in Tables 4.6 and 4.7 (72% and 86% using maximum probability and
thresholding for fusion respectively) outperform the earlier papers. This is even
though the number of candidate classes for classification in this study is much
higher (9 as opposed to 4 or 5). Whereas with 9 classes a baseline random classifier
would give 100
9
= 11% accuracy on average, with 4 or 5 classes the baseline would
be 25% or 20%. The higher the number of candidate classes, the more difficult
the classification task becomes.
4.2.2 Experiment 2: Machine learning algorithms
This experiment aims to compare the accuracy of the two machine learning al-
gorithms that were found to achieve highest accuracies. As outlined in Section
2.2.3, a wide range of different classifiers were tried, with SVMs and decision
tree-based C4.5 performing best.
C4.5 constructs a decision tree from a set of data by dividing up the data ac-
cording to the information gain I(X;Y ) defined in Section 3.1.3. It recursively
splits the tree by the attribute with the highest I(X;Y ) in the training, yielding
a decision tree that can be reused for classification.
39
4.2. CLASSIFICATION RESULTS CHAPTER 4. EVALUATION
ab
so
rb
ed
ex
ci
te
d
in
te
re
st
ed
jo
yf
ul
op
po
se
d
st
re
ss
ed
su
re
th
in
ki
ng
un
su
re
absorbed 93
(81)
87
(82)
96
(82)
96
(78)
89
(87)
85
(84)
82
(73)
84
(64)
excited 90
(71)
84
(60)
81
(71)
80
(61)
94
(83)
90
(72)
87
(75)
interested 92
(77)
92
(75)
91
(66)
90
(78)
90
(84)
85
(72)
joyful 86
(71)
85
(61)
99
(83)
95
(72)
92
(75)
opposed 93
(84)
91
(72)
94
(81)
92
(79)
stressed 86
(84)
88
(75)
86
(78)
sure 94
(75)
88
(78)
thinking 90
(89)
unsure
Table 4.3: Ten-fold cross-validation percentages for the pairwise machines. For
comparison, results of previous research [27] are shown in parentheses. The values
that improve upon the results of previous research are shown in bold. The average
accuracy is 89%, compared to 76% in previous research.
40
CHAPTER 4. EVALUATION 4.2. CLASSIFICATION RESULTS
Unlike in previous research [27], the experiment found that SVMs could provide
significantly higher performance than from C4.5 for every pairwise machine. This
is achieved by using grid search to optimise the SVM parameters. The results
are illustrated in Table 4.4, where the SVM results are compared to the results
with C4.5. As a consequence, only SVMs were used for implementation.
Another factor that affected the results is feature selection. The square-bracketed
values in Table 4.4 show the number of features in each SVM. These are only a
small fraction of the original 6669 feature combinations extracted by openSMILE.
When training on all 6669 features, accuracies above 60% were rarely obtained.
4.2.3 Experiment 3: Grid search for SVM parameter op-
timisation
This experiment aims to compare the accuracy obtained with and without using a
grid search algorithm (Algorithm 1) for optimising SVM parameters. In previous
studies optimisation of the machine learning algorithm has not received as much
attention. This study employs a method based on maximising cross-validation
accuracy to obtain a considerable improvement in detection accuracy.
As described in Section 3.1.5, the greedy grid search algorithm chooses optimal
(C, γ) parameters for each pairwise SVM. It first does a rough search over the
pairwise values and then recursively narrows down the search space by search-
ing aroud the values that produced the highest cross-validation accuracy. This
turned out to be a major contributor to the high accuracy of the SVM approach.
Previous work has ignored this subtle but clearly important classifier optimisa-
tion.
The effect for using grid search with the three pairwise fusion mechanisms is
shown in Table 4.5. A significant improvement, between 10% and 38%, is ob-
served. This is as high an improvement as that gained from choosing SVM over
C4.5 in Section 4.2.2. As the optimisation maximises the cross-validation accu-
racy instead of the training data classification accuracy, the optimisation did not
result in overfitting of the model.
4.2.4 Experiment 4: Pairwise fusion methods
This experiment demonstrates the results of using the three different algorithms
described in Section 3.2.4 for fusing the pairwise SVMs. The confusion matrices
41
4.2. CLASSIFICATION RESULTS CHAPTER 4. EVALUATION
ab
so
rb
ed
ex
ci
te
d
in
te
re
st
ed
jo
yf
ul
op
po
se
d
st
re
ss
ed
su
re
th
in
ki
ng
un
su
re
absorbed 93
(79)
[53]
87
(74)
[55]
96
(80)
[79]
96
(85)
[46]
89
(80)
[58]
85
(74)
[39]
82
(76)
[52]
84
(79)
[36]
excited 90
(72)
[44]
84
(70)
[38]
81
(77)
[48]
80
(69)
[31]
94
(70)
[47]
90
(78)
[97]
87
(69)
[57]
interested 92
(81)
[65]
92
(72)
[54]
91
(68)
[46]
90
(77)
[68]
90
(79)
[85]
85
(69)
[34]
joyful 86
(77)
[53]
85
(70)
[54]
99
(75)
[75]
95
(84)
[103]
92
(73)
[62]
opposed 93
(75)
[43]
91
(73)
[28]
94
(75)
[86]
92
(76)
[28]
stressed 86
(68)
[61]
88
(74)
[100]
86
(68)
[34]
sure 94
(80)
[78]
88
(70)
[54]
thinking 90
(74)
[72]
unsure
Table 4.4: Ten-fold cross-validation percentages using grid-searched SVMs and
C4.5. The number without bracketing is the result using SVMs. The results
using C4.5 are shown in parentheses. The number of features used are in square
brackets.
42
CHAPTER 4. EVALUATION 4.2. CLASSIFICATION RESULTS
Type of data Thresholding Max probability Max wins
Training data, grid search 99 (81) 86 88
Training data, no grid search 86 (81) 55 50
70–30 training/testing split, grid
search
86 (79) 72 70 (70)
70–30 training/testing split, no grid
search
76 (79) 47 48 (70)
Table 4.5: Accuracies in percentages for the three different methods to determine
a winner, with and without grid search. The results by previous work [27] are
shown in parentheses.
for the final emotion detection results for each fusion technique are obtained and
shown below.
The confusion matrices for the probability, thresholding and majority voting
methods are shown in Table 4.6 (Figure 4.3), Table 4.7 (Figure 4.4) and Table
4.8 (Figure 4.5) respectively. The shaded values show the percentage of correct
classifications for each class. The unshaded values show the percentage of classi-
fications in which the ground truth class (column) is mistaken for the inference
class (row). For each fusion method, a random choice would result in an overall
11% accuracy. All three fusion methods present average accuracies that are either
equal to or higher than achieved previously [27], as is summarised in Table 4.1.
Notably, the average accuracy of the maximum probability fusion technique is
higher than that achieved by majority voting (72% vs 70%). However, for some
classes the majority voting accuracy is higher (e.g. stressed and interested).
Thus, in future work a slightly higher accuracy could be achieved by combining
these methods, using the one giving best results per individual class.
By inspection of the confusion matrices, some classes were clearly better detected
than others. The classes opposed and sure presents the lowest values using any
method. This is reflected by the lower number of training data samples (38 and
53 samples, compared to the average of 61) resulting from the categorisation
choice to allow comparison to previous research [27]. Similarly, the class with
most samples (joyful, 94 samples) is most frequently mistaken to be the correct
class. In future work classes with equal number of training corpus samples could
be used, albeit it will not allow for direct comparison to previous work.
As expected, the thresholding fusion method yields highest detection accuracies
43
4.2. CLASSIFICATION RESULTS CHAPTER 4. EVALUATION
Inferred (down) Actual (right) ab
so
rb
ed
ex
ci
te
d
in
te
re
st
ed
jo
yf
ul
op
po
se
d
st
re
ss
ed
su
re
th
in
ki
ng
un
su
re
absorbed 74 0 2 0 0 1 2 1 1
excited 0 75 2 6 0 2 6 0 1
interested 4 0 69 0 0 2 2 3 1
joyful 4 10 6 79 16 11 4 3 4
opposed 0 2 0 2 62 1 2 0 0
stressed 4 8 6 3 8 67 9 1 8
sure 0 0 2 2 5 2 63 0 0
thinking 7 0 8 3 0 4 11 86 17
unsure 7 4 6 4 8 8 2 6 68
Table 4.6: Confusion matrix for emotion detection using maximum probability
for pairwise fusion. The column headings show the ground truth and the rows
show inferences. Average accuracy is 72%.
Figure 4.3: Confusion graph for emotion detection using maximum probability
for pairwise fusion.
44
CHAPTER 4. EVALUATION 4.2. CLASSIFICATION RESULTS
Inferred (down) Actual (right) ab
so
rb
ed
ex
ci
te
d
in
te
re
st
ed
jo
yf
ul
op
po
se
d
st
re
ss
ed
su
re
th
in
ki
ng
un
su
re
absorbed 93 4 15 0 0 4 12 23 24
excited 15 85 10 29 27 46 24 6 14
interested 22 2 83 14 3 10 11 17 14
joyful 15 35 21 91 41 39 22 23 22
opposed 0 14 6 22 73 11 17 7 8
stressed 15 60 31 56 51 92 31 24 29
sure 11 19 6 4 16 9 74 11 9
thinking 48 15 42 19 24 19 28 93 56
unsure 48 8 52 24 22 31 26 56 91
Table 4.7: Confusion matrix for emotion detection using thresholding for pairwise
fusion. The column headings show the ground truth and the rows show inferences.
Average accuracy is 86%.
Figure 4.4: Confusion graph for emotion detection using thresholding for pairwise
fusion.
45
4.2. CLASSIFICATION RESULTS CHAPTER 4. EVALUATION
Inferred (down) Actual (right) ab
so
rb
ed
ex
ci
te
d
in
te
re
st
ed
jo
yf
ul
op
po
se
d
st
re
ss
ed
su
re
th
in
ki
ng
un
su
re
absorbed 74 0 0 0 0 1 2 2 3
excited 0 65 3 6 3 2 6 0 1
interested 4 0 73 2 0 2 4 3 0
joyful 4 13 4 76 16 10 4 1 5
opposed 0 2 0 5 59 1 2 1 3
stressed 4 10 4 3 8 71 7 3 8
sure 0 4 2 2 5 2 63 0 0
thinking 7 2 8 3 1 4 10 83 16
unsure 7 4 6 3 8 7 2 7 64
Table 4.8: Confusion matrix for emotion detection using majority voting for
pairwise fusion. The column headings show the ground truth and the rows show
inferences. Average accuracy is 70%.
Figure 4.5: Confusion graph for emotion detection using majority voting for
pairwise fusion.
46
CHAPTER 4. EVALUATION 4.2. CLASSIFICATION RESULTS
since several classes can be selected at a time. This, however, also leads to much
higher confusion values which in some cases can be very high. For example, an
excited voice causes joyful to be detected in 35% of cases, compared to detection
rate 85% for excited. It could be argued that this is because these emotions are
similar, and perhaps the samples that are analysed contain both emotions. How-
ever, confusions such as 60% detection of stressed for samples that are labelled
as excited may be more difficult to explain using this hypothesis. It is likely that
some of the high confusion rates are caused by the uneven distribution of samples
between classes.
4.2.5 Experiment 5: Speech quality assessment
This section evaluates the performance of the extension where the training and
classification systems described in Sections 3.1 and 3.2 are used to assess the
speech quality. The emotion classifier is retrained using six labels describing
speech quality. This is a novel application of the classifier that has not been
attempted before.
The 124 samples are each labelled with scores for all 6 classes. The samples for
each class are halved into the higher and lower parts according to the score. One
binary SVM per class is used to derive a probability. This allows the user to try
to maximise all class probabilities, which is a useful goal for speech coaching.
The results of speech quality assessment using the modified emotion detector de-
scribed in Section 3.2.5 are shown in Table 4.9. Again, a significant improvement
in classification accuracy is observed after optimising the SVM parameters using
grid search, with an average 13% true-positive improvement in accuracy for the
70–30% training/testing split.
All classes can be accurately detected, with a high proportion of true positives
and true negatives. The classes competent and dynamic present slightly lower
detection accuracies, perhaps due to the small variation in scores resulting from
a small corpus size. Overall, however, the speech quality assessment are high
enough (average 79% for a 70–30% split) to provide useful feedback to speakers
for all classes.
47
4.2. CLASSIFICATION RESULTS CHAPTER 4. EVALUATION
Class Training
data
(no grid)
70–30%
split
(no grid)
Training
data (grid)
70–30%
split (grid)
10-fold
cross-
validation
(grid)
clear 79 [83] 80 [70] 95 [98] 90 [76] 80
competent 61 [87] 37 [87] 78 [97] 67 [95] 74
credible 74 [87] 55 [79] 98 [98] 69 [96] 80
dynamic 60 [91] 51 [90] 91 [100] 62 [89] 77
persuasive 87 [71] 85 [58] 94 [62] 87 [60] 82
pleasant 86 [76] 86 [75] 100 [98] 97 [76] 93
Mean 75 [82] 66 [76] 93 [92] 79 [82] 81
Table 4.9: Detection accuracies in percentages for speech quality assessment using
the Speech Tutor corpus. Accuracies are true positive rates, with true negatives in
square brackets. A 70–30% training/testing split is used to allow direct compar-
ison to emotion detection accuracies. Results are shown both with and without
grid search optimisations for choosing SVM parameters.
48
Chapter 5
Conclusion
This study presents a new framework for emotion detection whose accuracy out-
performs a recent PhD thesis [27] and a number of papers [6, 21, 28]. Moreover,
it achieves this in real-time, as opposed to previous work which was run as a
batch job. The novel application of the system for speech quality assessment also
achieves high detection accuracies.
A strong theoretical understanding of machine learning algorithms was the key
to success. Small, compact pairwise classifiers that were highly optimised us-
ing a grid search algorithm were successfully implemented. Experiment 1 shows
that the framework provides high classification accuracies which outperform pre-
vious research. Experiment 2 investigates different machine learning algorithms.
Classification results show that the highly optimised SVMs produce higher cross-
validation accuracies than other algorithms.
The project illustrates a method to optimise the misclassification and exponen-
tiation coefficients (C, γ) in Equations 2.2 and 2.3. Experiment 3 demonstrates
the considerable improvement resulting from this optimisation. The fusion of
pairwise classifiers is described in Experiment 4. Results show that using max-
imum probability for fusion achieves highest accuracies for single-label outputs.
To achieve real-time performance, it was also essential to choose suitable data
structures and algorithms. Section 4.1.2 shows that millisecond-level latency was
achieved running on a standard laptop.
This core of the project was completed on schedule as set out in the project
proposal.
The extension was also successfully completed on schedule. In particular, the
CHAPTER 5. CONCLUSION
emotion detector is extended to support automatic speech quality assessment.
Experiment 5 shows that a high class detection accuracy is achieved using the
Speech Tutor corpus labelled for this project.
In conclusion, this project shows that building a fast and efficient speech emotion
detector is a challenging but achievable goal. By combining a thorough theoreti-
cal understanding of machine learning beyond the Computer Science Tripos with
diligent classifier optimisation, a system that outperforms recent research is at-
tained. The key design principles behind the successful implementation of a large
real-time system included choosing efficient data structures and algorithms, and
employing suitable software engineering tools. In sum, the project employs an
understanding of a wide area of Computer Science to demonstrate that highly
accurate speech emotion detection is possible, and that it can be done in real-
time.
50
Bibliography
[1] H. Altun and G. Polat. New Frameworks to Boost Feature Selection Al-
gorithms in Emotion Detection for Improved Human-Computer Interaction.
Lecture Notes in Computer Science, 2007.
[2] S. Baron-Cohen, O. Golan, S. Wheelwright, and J. J. Hill. Mind Reading:
The Interactive Guide to Emotions. Jessica Kingsley Publishers, University
of Cambridge, 2004. ISBN 1 84310 214 5.
[3] A. Batliner, K. Fisher, R. Huber, J. Spilker, and E. Noth. Desperately seeking
emotions or: Actors, wizards, and human beings. In Proc. of the International
Speech Communication Association Workshop on Speech and Emotion, pages
195–200, 2000.
[4] M. Berouti, R. Schwartz, and J. Makhoul. Enhancement of speech corrupted
by acoustic noise. Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing,
pages 208–211, 1979.
[5] M. Brookes. VOICEBOX, February 2010. http://www.ee.ic.ac.uk/hp/
staff/dmb/voicebox/voicebox.html.
[6] F. Dellaert, T. Polzin, and A. Waibel. Recognizing emotion in speech. In
Proceedings of fourth international conference on spoken language processing,
volume 3, pages 1970–1973, 1996.
[7] F. Eyben, M. Wöllmer, and B. Schuller. openEAR – Introducing the Munich
open-source emotion and affect recognition toolkit. In Proc. 4th International
HUMAINE Association Conference on Affective Computing and Intelligent
Interaction 2009 (ACII 2009), IEEE, Amsterdam, The Netherlands, Septem-
ber 2009.
[8] R.E. Fan, P.H. Chen, and C.J. Lin. Working set selection using second order
information for training SVM. Journal of Machine Learning Research, 6:1889–
1918, 2005.
BIBLIOGRAPHY BIBLIOGRAPHY
[9] K. Forbes-Riley and D. Litman. Predicting emotion from spoken dialogue
from multiple knowledge sources. In Proc. of human language technology
conference of the north american chapter of the association for computational
linguistics, 2004.
[10] J. Friedman. Another approach to polychotomous classication. Technical
report, Stanford University, Department of Statistics, 1996.
[11] O. Golan, S. Baron-Cohen, S. Wheelwright, and J. J. Hill. Systemizing em-
pathy: Teaching adults with asperger syndrome and high functioning autism
to recognize complex emotions using interactive multimedia. Development
and Psychopathology, 18:589–615, 2006.
[12] M. A. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. Wit-
ten. The WEKA data mining software: An update. SIGKDD Explorations,
11, 2009.
[13] M. A. Hall and L. A. Smith. Feature selection for machine learning: com-
paring a correlation-based filter approach to the wrapper. Florida Artificial
Intelligence Symposium, pages 235–239, 1999.
[14] J. D. Haynes and G. Rees. Decoding mental states from brain activity in
humans. Nature Reviews Neuroscience, 7:523–534, 2006.
[15] S. King. Edinburgh speech tools, February 2010. http://www.cstr.ed.ac.
uk/projects/speech_tools/.
[16] H. T. Lin, C. J. Lin, and R. C. Weng. A note on Platt’s probabilistic outputs
for support vector machines. Machine Learning, 68:267–276, 2007.
[17] H. Liu and L. Yu. Toward integrating feature selection algorithms for clas-
sification and clustering. In Trans. on Knowledge and Data Engineering,
volume 17, pages 491–502, 2005.
[18] Mathworks. Matlab, February 2010. http://www.mathworks.com.
[19] P. J. Moreno and R. Rifkin. Using the fisher kernel method for web audio
classification. In Proc. of IEEE International Conference on Acoustics Speech
and Signal Processing, volume 4, pages 2417–2420, 2000.
[20] Nokia. Qt documentation, February 2010. http://qt.nokia.com/doc/.
[21] V. Petrushin. Emotion in speech: Recognition and application to call centers.
Articial Neural Network Intelligence Engineering, pages 7–10, 1999.
52
BIBLIOGRAPHY BIBLIOGRAPHY
[22] S. Pöyhönen, A. Arkkio, P. Jover, and H. Hyötyniemi. Coupling pairwise
support vector machines for fault classification. Control Engineering Practice,
13(6):759–769, 2005.
[23] L. Qing-kun and Q. Pei-wen. Model selection for SVM using mutative scale
chaos optimization algorithm. Journal of Shanghai University, 10:531–534,
2006.
[24] U. Rathmann. Qt widgets, February 2010. http://qwt.sourceforge.net/.
[25] K. R. Scherer. Vocal affect expression: A review and a model for future
research. Psychological bulletin, 99:143–165, 1986.
[26] B. Schuller, S. Steidl, and A. Batliner. The interspeech 2009 emotion chal-
lenge. In Interspeech, Brighton, UK, 2009.
[27] T. Sobol Shikler. Analysis of affective expression in speech. PhD thesis,
Cambridge University, 2007.
[28] S. Steidl, M. Levit, A. Batliner, E. Noeth, and E. Niemann. Of all things
the measure is man – Automatic classication of emotions and interlabeler
consistency. In Proceeding of the IEEE international conference on acoustics,
speech, and signal processing, 2005.
[29] V. N. Vapnik. The nature of statistical learning theory. Springer, 1998.
[30] G. Yu, S. Mallat, and E. Bacry. Audio denoising by time-frequency block
thresholding. IEEE Transactions on signal processing, 56:1830–1839, 2008.
53
BIBLIOGRAPHY BIBLIOGRAPHY
54
Appendix A
Sample source code
A.1 C++ code
The listing below shows some sample code for the Pairwise-Combinator and
Pairwise-Voting algorithms.
int cPairwiseCombinator : : processComponentMessage ( cComponentMessage ∗ msg )
{
i f ( isMessageType ( msg , ” c l a s s i f i c a t i o n R e s u l t ” ) ) {
// ge t data from message
double ∗probEstim = (double∗) msg−>custData ;
int nClasse s = msg−>intData [ 0 ] ;
char ∗∗ classNames = (char∗∗) msg−>custData2 ;
userTime1 = (double ) msg−>userTime1 ;
userTime2 = (double ) msg−>userTime2 ;
// check i f f i n a l ( numClasses=8 by d e f a u l t )
bool f i n a l = fa l se ;
i f ( combinedProbEstim . s i z e ( ) > 0) f i n a l=true ;
map<s t r i ng , mul t i s e t<double> > : : i t e r a t o r i t 1 ; // i t e r a t o r f o r
f i n a l i t y check
for ( i t 1=combinedProbEstim . begin ( ) ; i t 1 !=combinedProbEstim . end ( )
&& f i n a l ; i t 1++) {
i f ( (∗ i t 1 ) . second . s i z e ( ) != ( numClasses−1) && (∗ i t 1 ) .
second . s i z e ( ) != numClasses ) f i n a l = fa l se ;
}
// crea t e new mu l t i s e t f o r p r o b a b i l i t i e s
mult i s e t<double> mult i ;
// crea t e new en t r i e s f o r c l a s s e s in maps i f they don ’ t e x i s t
for ( int i =0; i<nClasse s ; i++) {
i f ( combinedProbEstim . count ( classNames [ i ] ) == 0)
combinedProbEstim . i n s e r t ( pair<s t r i ng , mul t i s e t<
double> >(classNames [ i ] , mult i ) ) ;
i f ( r e s u l t s . count ( classNames [ i ] ) == 0)
A.1. C++ CODE APPENDIX A. SAMPLE SOURCE CODE
r e s u l t s . i n s e r t ( pair<s t r i ng , pair<int , double> >(
classNames [ i ] , make pair ( 0 , 0 ) ) ) ;
}
// increment win count f o r winner
( r e s u l t s [ msg−>msgtext ] . f i r s t )++;
// i t e r a t e over nonzero p r o b a b i l i t y e s t imate s
i f ( probEstim != NULL) {
for ( int i =0; i<nClasse s ; i++) {
// i n s e r t p r o b a b i l i t y es t imate in to map
combinedProbEstim [ classNames [ i ] ] . i n s e r t (
probEstim [ i ] ) ;
// c a l c u l a t e t o t a l p r o b a b i l i t y
r e s u l t s [ classNames [ i ] ] . second += probEstim [ i ] ;
}
}
// pr in t r e s u l t s & findmax i f t h i s i s the f i n a l c a l c u l a t i o n
i f ( f i n a l ) {
p r i n tRe su l t s ( ) ;
findMax ( ) ;
// c l e a r maps a f t e r f i n a l c a l c u l a t i o n
combinedProbEstim . c l e a r ( ) ;
r e s u l t s . c l e a r ( ) ;
}
return 1 ; // message was processed
}
return 0 ; // i f message was not processed
}
void cPairwiseCombinator : : p r i n tRe su l t s ( )
{
map<s t r i ng , mul t i s e t<double> > : : i t e r a t o r i t 1 ;
mul t i s e t<double> : : i t e r a t o r i t 2 ;
int i =0; // count f o r c l a s s ID
for ( i t 1=combinedProbEstim . begin ( ) ; i t 1 !=combinedProbEstim . end ( ) ; i t 1++)
{
s t r i n g name = (∗ i t 1 ) . f i r s t ;
char ∗buf = (char∗)name . c s t r ( ) ;
f p r i n t f ( s tde r r , ”%s : ” , buf ) ;
for ( i t 2 =(∗ i t 1 ) . second . begin ( ) ; i t 2 !=(∗ i t 1 ) . second . end ( ) ; i t 2++)
{
f p r i n t f ( s tde r r , ” %f ” ,∗ i t 2 ) ;
}
f p r i n t f ( s tde r r , ” ( div %d) = %f wins : %d” , numClasses , (
r e s u l t s [ name ] . second ) /numClasses , r e s u l t s [ name ] . f i r s t ) ;
f p r i n t f ( s tde r r , ”\n” ) ;
i++;
}
}
void cPairwiseCombinator : : findMax ( )
{
int maxWins = 0 ;
56
APPENDIX A. SAMPLE SOURCE CODE A.2. BASH SCRIPT
s t r i n g maxWinsName ;
double maxProbabi l i ty = 0 . 0 ;
s t r i n g maxProbabilityName ;
map<s t r i ng , pair<int , double> > : : i t e r a t o r i t 3 ;
// i t e r a t e over r e s u l t s map
for ( i t 3=r e s u l t s . begin ( ) ; i t 3 != r e s u l t s . end ( ) ; i t 3++) {
s t r i n g name = (∗ i t 3 ) . f i r s t ;
// f ind maximum number o f wins
i f ( (∗ i t 3 ) . second . f i r s t > maxWins) {
maxWinsName = name ;
maxWins = (∗ i t 3 ) . second . f i r s t ;
}
// f ind maximum p r o b a b i l i t y
i f ( (∗ i t 3 ) . second . second > maxProbabi l i ty ) {
maxProbabilityName = name ;
maxProbabi l i ty = (∗ i t 3 ) . second . second ;
}
}
i f ( r e s u l t s . s i z e ( ) > 0) {
const char ∗maxWinsNameBuf = maxWinsName . c s t r ( ) ;
const char ∗maxProbabilityNameBuf = maxProbabilityName . c s t r ( ) ;
f p r i n t f ( s tde r r , ”maxWins : %s − %d maxProbabi l i ty : %s − %f ” ,
maxWinsNameBuf , maxWins , maxProbabilityNameBuf ,
maxProbabi l i ty /numClasses ) ;
f p r i n t f ( s tde r r , ”\n\n” ) ;
}
}
A.2 Bash script
The listing below shows the Bash script for constructing pairwise corpora.
#!/ bin /sh
#
# Scr i p t f o r conver t ing a corpus in to a s e t
# of separa te pa i rw i se corpora
#
#source d i r
s d i r=” . . / audio ”
#temporaries
l e t c=0
b=fa l se
for i in ‘ l s $ sd i r ‘ ; do
for j in ‘ l s $ s d i r |awk ”\\$1 != \” $ i \” ” ‘ ; do
#loop through array , see i f j i s in i t
for e in ”${ ar r [@]} ” ; do
i f [ ” $e” = ” $ j ” ] ; then
57
A.3. UNIT TESTS APPENDIX A. SAMPLE SOURCE CODE
b=true
f i
done
#i f j isn ’ t in array ( i . e . hasn ’ t been s e l e c t e d be f o r e )
#crea t e dir , copy f i l e s from s r c d i r
i f [ ”$b” = ” f a l s e ” ] ; then
d i r=$i−$ j ;
mkdir −p $d i r
echo $d i r
cp −r $ s d i r / $ i $ s d i r / $ j $d i r
f i
#se t boolean back to f a l s e
b=fa l se
done
#increment count
( ( c++))
#sto r e in array
ar r [ c ]= $ i
done
A.3 Unit tests
The unit testing script written in Bash is shown below.
#!/ bin /bash
#
# Unit t e s t i n g s c r i p t f o r emoDetect
run ( ) {
sh u n i t t e s t s /$1 . sh
r e s=$?
i f [ $ r e s −eq 0 ] ; then
echo − $1 : FAIL
else
echo − $1 : OK
f i
}
echo ”−−−−−−−−−−−−−−−−−−−−−−−−”
echo ” emoDetect un i t t e s t s ”
echo ”−−−−−−−−−−−−−−−−−−−−−−−−”
echo
run p r ep ro c e s s i ng
run svmtrain
run opt imi se
run segmentat ion
run pa i rw i s e
run vot ing
run gui
58
Appendix B
Extracted features
The low-level features extracted by openSMILE [7] are shown in Table B.1. The
functionals computed using the low-level features are shown in Table B.2.
Feature Group Features #
Signal energy Root mean square; Logarithmic 2
Fundamental frequency (F0) based on
autocorrelation (ACF)
F0 frequency; F0 strength (peak of
ACF); F0 quality (zero-crossing rate of
ACF w.r.t. F0)
3
Mel-Frequency Cepstral Coefficients Coefficients 0–15 16
Spectral Centroid; Roll-off (10%, 25%, 50%,
75%, 90%); Flux; Frequency band en-
ergy (0–250Hz, 0–650Hz, 250–650Hz,
1000–4000Hz); Position of maximum;
Position of minimum
13
Time signal Zero-crossing rate; Maximum value;
Minimum value; Mean (DC)
4
Linear Predictive Coding Coefficients 0–11 12
Voice quality Harmonics to noise ratio; Jitter; Shim-
mer
3
Pitch by harmonic product spectrum F0 frequency; Probability of voicing
(pitch strength)
2
Table B.1: OpenSMILE’s [7] 55 low-level audio features.
APPENDIX B. EXTRACTED FEATURES
Functionals Group Functionals #
Min/max Max/min value; Relative position of max/min
value; Range
5
Mean Arithmetic mean; Arithmetic mean with abso-
lute values; Quadratic; Maximum value arithmetic
mean
4
Non-zero Arithmetic mean of non-zero values; Percentage of
non-zero values
2
Quartiles 20%, 50%, 75% quartiles; inter-quartile range (2–1,
3–2, 3–1)
6
Percentiles 95%, 98% percentiles
Moments Variance; Skewness; Kurtosis; Standard deviation 4
Centroid Centroid (centre of gravity) 1
Threshold crossing rate Zero-crossing rate; Mean-crossing rate 2
Linear regression 2 coefficients; Linear & quadratic regression error 4
Quadratic regression 3 coefficients; Linear & quadratic regression error 5
Peaks Number of maxima; Mean distance between peaks;
Mean value of peaks; Arithmetic mean of peaks
4
Segments Number of segments based on delta thresholding
Time Length of time that values are above 75% of the
total range; Length of time that values are below
25% of the total range; Rise/fall time
5
Discrete Cosine Transform Coefficients 0–5 6
Table B.2: OpenSMILE’s [7] 50 functionals.
60
Tomas Pfister
Gonville & Caius College
tjp35
Computer Science Tripos Part II Project Proposal
Emotion profiling of speech
19 October 2009
Project Originator: Prof P. Robinson
Resources Required: See attached Project Resource Form
Project Supervisor: Prof P. Robinson
Signature:
Director of Studies: Dr G. Titmus and Prof P. Robinson
Signature:
Overseers: Dr N. Dodgson and Prof R. Anderson
Signatures:
Background and Introduction
In persuasive communication, special attention is required to what non-verbal clues one
conveys. Untrained speakers often come across as bland, lifeless and colourless. Precisely
measuring and analysing the voice is a difficult task and has in the past been entirely
subjective. By incorporating the recent developments in computer science with inspiration
from psychology and acoustics, it should be possible to make the analysis more objective.
In previous work on emotion detection from speech, Tal Sobol-Shikler [3], under supervision
of Prof P. Robinson, presented a system for batch processing speech samples using pair-
wise decision machines. This project will combine some of the ideas in her work with more
recent research to build a real-time speech emotion profiling system.
Project Proposal
The purpose of this project is to build a tool that will help in understanding what vocal
information is sent. The aim is to show the speaker in real-time what emotions and non-
verbal signals are projected to allow them to be adjusted. The core part of the project
proposal is to develop methods for the emotional profiling of speech. The extension will
be to use a different database to retrain and revalidate the profiler for public speaking
tutoring.
The core profiler will be trained and validated using the MindReading emotions database,
collected by Professor Simon Baron-Cohen at the Autism Research Centre at the University
of Cambridge. It consists of 4411 acted sentences, covering 756 concepts of emotions within
24 emotion groups.
Once a set of vocal features and extraction algorithms have been defined, the profiler will
use machine learning techniques to train the system based on the vocal features present
in the speech samples of the MindReading database. After the training phase, the profiler
segments the speech, extracts the features, classifies the speaker’s emotions and presents a
summary of the most prevalent emotions in the speech so far. A similar approach will be
used for training a virtual public speaking tutor in the extension.
Special resources
Access to the Rainbow Lab for system testing, to the MindReading database of affective
speech for training data, and to an additional database for the extension.
2
Substance and Structure of the Project
The key challenges to be addressed:
1. Theoretical understanding of the background material. The project will involve learn-
ing and understanding a considerable amount of mathematics of machine learning
not covered in the Computer Science Tripos. This will include investigating applying
pair-wise decision machines and voting mechanisms to training the mapping between
vocal features and emotional states. It is expected that the learning process will take
a significant amount of time.
2. Implementation of a real-time speech emotion detector using the C++ object-oriented
language. This will involve designing the system architecture, implementing feature
extraction algorithms, and possibly improving the machine learning libraries. The
aim is to reuse some of the design ideas by Sobol-Shikler [3] and apply them to a
real-time system. Existing implementations will be reused whenever appropriate.
3. Choice of suitable data structures and algorithms. This is essential for achieving
real-time performance.
Starting Point
I have done some background reading during the summer and have a basic knowledge of
C++. It is expected that I will be able to use some of the open-source code in LibSVM
[2] and openEAR [1] projects. Lecture courses that may be useful for the project include
Mathematical Methods for CS, Natural Language Processing, Programming in C++, Soft-
ware Design and Software Engineering.
Evaluation Criteria
The success of the core project will be evaluated based on the performance and features
of the emotion profiler. It is expected to successfully extract a set of features and use
them to train and detect emotions from speech with a reasonable accuracy. The run-time
performance will also be evaluated to investigate whether suitable data structures and
algorithms were selected.
In the optional extension, the results will be evaluated by considering whether the tutor
successfully applies the profiler from the core part of the project to provide feedback about
public speaking skills.
3
Timetable and Milestones
In the planned timetable the project has been segmented into 10 stages, each lasting about
2 weeks. Notes will be taken in a logbook during all stages to assist with the final write-up.
Stage Date Tasks and milestones
1 Oct 19 – Oct 30 Background study. Reading relevant papers.
2 Nov 2 – Nov 13 Planning and preliminary familiarisation. Choice of tools.
Writing some toy programs to investigate the chosen tools.
Milestone: Thorough understanding of the prepara-
tion chapter’s material.
3 Nov 16 – Nov 27 Implementation and debugging of the core part of emotion
detector.
4 Nov 30 – Dec 11 Finishing off implementation of core. Start working on ex-
tension.
Milestone: Most of the work in the implementation
chapter completed.
5 Jan 4 – Jan 15 Writing up progress report for presentation. Evaluation of
emotion detector according to the success criteria.
Milestone: Progress report written; first draft of
evaluation chapter finished.
6 Jan 18 – Jan 29 Further work on extension. Address any issues arisen from
the progress report.
7 Feb 1 – Feb 12 Finish off extension and any uncompleted features.
Milestone: All implementation work done.
8 Feb 15 – Feb 26 Writing up.
9 Mar 1 – Mar 12 Writing up.
Milestone: First draft sent to supervisor before end
of Lent term.
10 Mar 22 – Apr 2 Writing up second draft to supervisor.
Milestone: Submission of second draft by Friday 2
April. Final submission by Friday 23 April.
4
References
[1] Björn Schuller Florian Eyben, Martin Wöllmer. openEAR – Introducing the Munich
open-source emotion and affect recognition toolkit. In Proc. 4th International HU-
MAINE Association Conference on Affective Computing and Intelligent Interaction
2009 (ACII 2009), IEEE, Amsterdam, The Netherlands, September 2009.
[2] C.J. Lin R.E. Fan, P.H. Chen. Working set selection using second order information
for training SVM. Journal of Machine Learning Research, 6:1889–1918, 2005.
[3] Tal Sobol-Shikler. Analysis of affective expression in speech. PhD thesis, University of
Cambridge, Cambridge, UK, 2007.
5

