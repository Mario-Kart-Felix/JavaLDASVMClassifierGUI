IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 49, NO. 2, FEBRUARY 2003 371
Linear Network Coding
Shuo-Yen Robert Li, Senior Member, IEEE, Raymond W. Yeung, Fellow, IEEE, and Ning Cai
Abstract—Consider a communication network in which certain
source nodes multicast information to other nodes on the network
in the multihop fashion where every node can pass on any of its re-
ceived data to others. We are interested in how fast each node can
receive the complete information, or equivalently, what the infor-
mation rate arriving at each node is. Allowing a node to encode its
received data before passing it on, the question involves optimiza-
tion of the multicast mechanisms at the nodes. Among the simplest
coding schemes is linear coding, which regards a block of data as a
vector over a certain base field and allows a node to apply a linear
transformation to a vector before passing it on. We formulate this
multicast problem and prove that linear coding suffices to achieve
the optimum, which is the max-flow from the source to each re-
ceiving node.
Index Terms—Coding, network routing, switching.
I. INTRODUCTION
DEFINE a communication network as a pair , , whereis a finite directed multigraph and is the unique node
in without any incoming edges. A directed edge in is called
a channel in the communication network , . The special
node is called the source, while every other node may serve
as a sink as we shall explain.
A channel in graph represents a noiseless communication
link on which one unit of information (e.g., a bit) can be trans-
mitted per unit time. The multiplicity of the channels from a
node to another node represents the capacity of direct
transmission from to . In other words, every single channel
has unit capacity.
At the source , a finite amount of information is generated
and multicast to other nodes on the network in the multihop
fashion where every node can pass on any of its received data
to other nodes. At each nonsource node which serves as a sink,
the complete information generated at is recovered. We are
naturally interested in how fast each sink node can receive the
complete information.
As an example, consider the multicast of two data bits, and
, from the source in the communication network depicted by
Manuscript received November 4, 1999; revised September 12, 2002. The
work of S.-Y. R. Li was supported in part by an MTK Computers Grant. The
work of R. W. Yeung and N. Cai was supported in part by the RGC Earmarked
Grant CUHK4343/98E. The material in this paper was presented in part at the
1999 IEEE Information Theory Workshop, Metsovo, Greece, June/July 1999.
S.-Y. R. Li and R. W. Yeung are with the Department of Information En-
gineering, The Chinese University of Hong Kong, N.T., Hong Kong (e-mail:
bobli@ie.cuhk.edu.hk; whyeung@ie.cuhk.edu.hk).
N. Cai was with Department of Information Engineering, The Chinese
University of Hong Kong, N.T., Hong Kong. He is now with the Fakultät
für Mathematik, Universität Bielefeld, 33501 Bielefeld, Germany (e-mail:
cai@mathematik.uni-bielefeld.de).
Communicated by V. Anantharam, Associate Editor for Communication
Networks.
Digital Object Identifier 10.1109/TIT.2002.807285
Fig. 1. Two communication networks.
Fig. 1(a) to both nodes and . One solution is to let the chan-
nels , , , and carry the bit and channels ,
, , and carry the bit . Note that in this scheme,
an intermediate node sends out a data bit only if it receives the
same bit from another node. For example, the node receives
the bit and sends a copy on each of the two channels
and . Similarly, the node receives the bit and sends a
copy into each of the two channels and . In our model,
we assume that there is no processing delay at the intermediate
nodes.
Unlike a conserved physical commodity, information can be
replicated or coded. Introduced in [1] (see also [5, Ch. 11]), the
notion of network coding refers to coding at the intermediate
nodes when information is multicast in a network. Let us now il-
lustrate network coding by considering the communication net-
work depicted by Fig. 1(b). In this network, we want to multicast
two bits and from the source to both the nodes and
. A solution is to let the channels , , carry the bit
, channels , , carry the bit , and channels ,
, carry the exclusive-OR . Then, the node re-
ceives and , from which the bit can be decoded.
Similarly, the node can decode the bit from and .
The coding/decoding scheme is assumed to have been agreed
upon beforehand.
It is not difficult to see that the above scheme is the only solu-
tion to the problem. In other words, without network coding, it
is impossible to multicast two bits per unit time from the source
to both the nodes and . This shows the advantage of net-
work coding. In fact, replication of data can be regarded as a
special case of network coding.
As we have pointed out, the natures of physical commodities
and information are very different. Nevertheless, both of them
are governed by certain laws of flow. For the network flow of a
physical commodity, we have the following.
The law of commodity flow: The total volume of the out-
flow from a nonsource node cannot exceed the total volume
of the inflow.
0018-9448/03$17.00 © 2003 IEEE
372 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 49, NO. 2, FEBRUARY 2003
The counterpart for a communication network is as follows.
The law of information flow: The content of any informa-
tion flowing out of a set of nonsource nodes can be derived
from the accumulated information that has flown into the
set of nodes.
After all, information replication and coding do not increase the
information content.
The information rate from the source to a sink can potentially
become higher and higher when the permitted class of coding
schemes is wider and wider. However, the law of information
flow limits this information rate to the max-flow (i.e., the max-
imum commodity flow) from the source to that particular sink
for a very wide class of coding schemes. The details are given
in [1].
It has been proved in [1] that the information rate from the
source to a set of nodes can reach the minimum of the indi-
vidual max-flow bounds through coding. In the present paper,
we shall prove constructively that by linear coding alone, the
rate at which a message reaches each node can achieve the indi-
vidual max-flow bound. (This result is somewhat stronger than
the one in [1]. Please refer to the example in Section III.) More
explicitly, we treat a block of data as a vector over a certain
base field and allow a node to apply a linear transformation to a
vector before passing it on. A preliminary version of this paper
has appeared in the conference proceedings [3].
The remainder of the paper is organized as follows. In Sec-
tion II, we introduce the basic notions, in particular, the no-
tion of a linear-code multicast (LCM). In Section III, we show
that with a “generic” LCM, every node can simultaneously re-
ceive information from the source at rate equal to its max-flow
bound. In Section IV, we describe the physical implementation
of an LCM first when the network is acyclic and then when the
network is cyclic. In Section V, we present a greedy algorithm
for constructing a generic LCM for an acyclic network. The
same algorithm can be applied to a cyclic network by expanding
the network into an acyclic network. This results in a “time-
varying” LCM, which, however, requires high complexity in
implementation. In Section VI, we introduce the time-invariant
LCM (TILCM) and present a heuristic for constructing a generic
TILCM. Section VII presents concluding remarks.
II. BASIC NOTIONS
In this section, we first introduce some graph-theoretic termi-
nology and notations which will be used throughout the paper.
Then we will introduce the notion of an LCM, an abstract alge-
braic description of a linear code on a communication network.
Convention: The generic notation for a nonsource node will
be , , , , , or . The notation will stand for any
channel from to .
Definition: Over a communication network a flow from the
source to a nonsource node is a collection of channels, to be
called the busy channels in the flow, such that
1) the subnetwork defined by the busy channels is acyclic,
i.e., the busy channels do not form directed cycles;
2) for any node other than and , the number of in-
coming busy channels equals the number of outgoing
busy channels;
3) the number of outgoing busy channels from equals the
number of incoming busy channels to .
In other words, a flow is an acyclic collection of channels that
abides by the law of commodity flow. The number of outgoing
busy channels from will be called the volume of the flow. The
node is called the sink of the flow. All the channels on the
communication network that are not busy channels of the flow
are called the idle channels with respect to the flow.
Proposition 2.1: The busy channels in a flow with volume
can be partitioned into simple paths from the source to the
sink.
The proof of this proposition is omitted.
Notation: For every nonsource node on a network , ,
the maximum volume of a flow from the source to is denoted
as , or simply when there is no am-
biguity.
Definition: A cut on a communication network , be-
tween the source and a nonsource node means a collection
of nodes which includes but not . A channel is said to
be in the cut if and . The number of channels in
a cut is called the value of the cut.
The well-known Max-Flow Min-Cut Theorem (see, for ex-
ample, [2, Ch. 4, Theorem 2.3]) still applies despite the acyclic
restriction in the definition of a flow.
Max-Flow Min-Cut Theorem: For every nonsource node ,
the minimum value of a cut between the source and a node is
equal to .
We are now ready to define a linear code multicast.
Notation: Let be the maximum of over all
. Throughout Sections II–V, the symbol will denote a fixed
-dimensional vector space over a sufficiently large base field.
Convention: The information unit is taken as a symbol in the
base field. In other words, symbol in the base field can be
transmitted on a channel every unit time.
Definition: A linear-code multicast (LCM) on a communi-
cation network , is an assignment of a vector space
to every node and a vector to every channel such
that
1) ;
2) for every channel ;
3) for any collection of nonsource nodes in the network
The notation is for linear span. Condition 3) says that
the vector spaces on all nodes inside together have
the same linear span as the vectors on all channels
to nodes in from outside of . Conditions 2) and 3) show
that an LCM abides by the law of information flow stated in
Section I. The vector assigned to every channel
may be identified with a -dimensional column vector over the
base field of by choosing a basis for .
Applying Condition 3) to the collection of a single nonsource
node , the space is linearly spanned by vectors
LI et al.: LINEAR NETWORK CODING 373
on all incoming channels to . This shows that an LCM
on a communication network is completely determined by the
vectors it assigns to the channels. Together with Condition 2),
we have
4) The vector assigned to an outgoing channel from must
be a linear combination of the vectors assigned to the in-
coming channels of .
Condition 4) may be regarded as the “law of information flow
at a node.” However, unlike in network flow theory, the law of
information flow being observed for every single node does not
necessarily imply it being observed for every set of nodes when
the network contains a directed cycle. A counterexample will
appear in Section IV.
An LCM specifies a mechanism for data transmission over
the network as follows. We encode the information to be trans-
mitted from as a -dimensional row vector, which we shall
call the information vector. Under the transmission mechanism
prescribed by the LCM , the data flowing on a channel
is the matrix product of the information (row) vector with the
(column) vector . In this way, the vector acts as
the kernel in the linear encoder for the channel . As a direct
consequence of the definition of an LCM, the vector assigned
to an outgoing channel from a node is a linear combination
of the vectors assigned to the incoming channels to . Conse-
quently, the data sent on an outgoing channel from a node is
a linear combination of the data sent on the incoming channels
to .
Under this mechanism, the amount of information reaching a
node is given by the dimension of the vector space when
the LCM is used. The physical realization of this mechanism
will be discussed in Section IV.
Example 2.2: Consider the multicast of two bits, and ,
from to and in the communication network in Fig. 1(b).
This is achieved with the LCM specified by
and
The data sent on a channel is the matrix product of the row vector
with the column vector assigned to that channel by .
For instance, the data sent on the channel is . Note
that, in the special case when the base field of is GF , the
vector reduces to the exclusive-OR in an earlier
example.
Proposition 2.3: For every LCM on a network, for all
nodes
Proof: Fix a nonsource node and any cut between the
source and
and
Hence, and , which
is at most equal to the value of the cut. In particular,
is upper-bounded by the minimum value of a cut between
and , which by the Max-Flow Min-Cut Theorem is equal to
.
This corollary says that is an upper bound on the
amount of information received by when an LCM is used.
III. ACHIEVING THE MAX-FLOW BOUND THROUGH A
GENERIC LCM
In this section, we derive a sufficient condition for an LCM
to achieve the max-flow bound on in Proposi-
tion 2.3.
Definition: An LCM on a communication network is said
to be generic if the following condition holds for any collec-
tion of channels for :
for if and only
if the vectors , , , are linearly
independent.
If are linearly indepen-
dent, then since
. A generic LCM requires that the converse is also true.
In this sense, a generic LCM assigns vectors which are as lin-
early independent as possible to the channels.
Example 3.1: With respect to the communication network in
Fig. 1(b), the LCM in Example 2.2 is a generic LCM. How-
ever, the LCM defined by
and
is not generic. This is seen by considering the set of channels
where
Then and , but
and are not linearly independent. Therefore, is not
generic.
Lemma 3.2: Let be a generic LCM. Any collection
of channels from a node with
must be assigned linearly independent
vectors by .
Theorem 3.3: If is a generic LCM on a communication
network, then for all nodes
Proof: Consider a node not equal to . Let be
the common value of and the minimum value
of a cut between and . The inequality
374 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 49, NO. 2, FEBRUARY 2003
follows from Proposition 2.3. Thus, we only have to show that
.
Let and for
any cut between and . We will show that
by contradiction. Assume and let be the
collection of cuts between and such that .
Since implies , where is the set of
all the nodes in , is nonempty.
By the assumption that is a generic LCM, the number of
edges out of is at least , and . Therefore,
. Then there must exist a minimal member in
the sense that for any , . Clearly,
because .
Let be the set of channels in cut and be the set of
boundary nodes of , i.e., if and only if and there
is a channel , such that . Then for all ,
which can be seen as follows. The set of channels in cut
but not in is given by . Since is an
LCM
If , then
the subspace spanned by the channels in cut , is con-
tained by . This implies
a contradiction.
Therefore, for all , .
For all , since
implies
Then, by the definition of a generic LCM,
is a collection of vectors such that . Fi-
nally, by the Max-Flow Min-Cut Theorem, , and since
, . This is a contradiction to the assumption
that . The theorem is proved.
An LCM for which for all pro-
vides a way for broadcasting a message generated at the source
for which every nonsource node receives the message at
rate equal to . This is illustrated by the next ex-
ample, which is based upon the assumption that the base field
of is an infinite field or a sufficiently large finite field. In this
example, we employ a technique which is justified by the fol-
lowing lemma.
Lemma 3.4: Let , and be nodes such that
, , and ,
where and . By removing any edge in the
graph, and are reduced by at most
, and remains unchanged.
Proof: By removing an edge , the value of a cut be-
tween the source and node (respectively, node ) is re-
duced by if edge is in , otherwise, the value of is
unchanged. By the Max-Flow Min-Cut Theorem, we see that
and are reduced by at most when
edge is removed from the graph. Now consider the value of
a cut between the source and node . If contains node ,
then edge is not in , and, therefore, the value of remains
unchanged upon the removal of edge . If does not contain
node , then is a cut between the source and node . By the
Max-Flow Min-Cut Theorem, the value of is at least . Then
upon the removal of edge , the value of is lower-bounded
by . Hence, by the Max-Flow Min-Cut Theorem,
remains to be upon the removal of edge .
The lemma is proved.
Example 3.5: Consider a communication network for which
, , or for nodes in the network. The source
is to broadcast 12 symbols , , taken from a suffi-
ciently large base field . (Note that is the least common
multiple of , , and .) Define the set
for
For simplicity, we use the second as the time unit. We now de-
scribe how , , can be broadcast to the nodes in , ,
in 3, 4, and 12 s, respectively, assuming the existence of an
LCM on the network for .
a) Let be an LCM on the network with . Let
and
In the first second, transmit as the information vector
using , in the second second, transmit , and in the
third second, transmit . After 3 s, all the nodes in
can recover , , and . Throughout this example,
we assume that all transmissions and computations are
instantaneous.
b) Let be a vector in such that intersects trivially
with for all in , i.e.,
for all in . Such a vector can be found when
is sufficiently large because there are a finite number
of nodes in . Define for . Now
remove incoming edges of nodes in , if necessary, so
that becomes if is in , otherwise,
remains unchanged. This is possible by
virtue of Lemma 3.4. Let be an LCM on the resulting
network with . Let and transmit
as the information vector using in the fourth second.
Then all the nodes in can recover and hence ,
, and .
c) Let and be two vectors in such that
intersects with trivially for all in , i.e.,
, , for all in . Define
for . Now remove incoming edges of nodes in
and , if necessary, so that becomes
LI et al.: LINEAR NETWORK CODING 375
if is in or , otherwise, remains
unchanged. Again, this is possible by virtue of Lemma
3.4. Now let be an LCM on the resulting network with
. In the fifth and the sixth seconds, transmit
and as the information vectors using . Then all the
nodes in can recover .
d) Let and be two vectors in such that ,
intersects with trivially for all in , i.e.,
, , , for all in . Define
and . In the seventh and eighth seconds,
transmit and as the information vectors using .
Since all the nodes in already knows , upon re-
ceiving and , can then be recovered.
e) Define and . In the nineth and tenth
seconds, transmit and as the information vectors
using . Then can be recovered by all the nodes in .
f) Define and . In the eleventh and
twelveth seconds, transmit and as the information
vectors using . Then can be recovered by all the
nodes in .
Let us now give a summary of the preceding scheme. In the th
second for via the generic LCM , each node in
receives all four dimensions of , each node in receives
three dimensions of , and each node in receives one di-
mension of . In the fourth second, via the generic LCM ,
each node in receives the vector , which provides the three
missing dimensions of , , and (one dimension for each)
during the first 3 s of multicast by . At the same time, each
node in receives one dimension of . Now, in order to recover
, each node in needs to receive the two missing dimensions
of during the fourth second. This is achieved by the generic
LCM in the fifth and sixth seconds. So far, each node in
has received one dimension of for via during
the first 3 s, and one dimension of for from via
and during the fourth to sixth seconds. Thus, it remains
to provide the six missing dimensions of , , and (two
dimensions for each) to each node in , and this is achieved in
the seventh to the twelveth seconds via the generic LCM .
Remark: The scheme in Example 3.5 can readily be general-
ized to arbitrary sets of max-flow values. The details are omitted
here.
In the scheme in Example 3.5, at the end of the 12-s session,
each node receives a message of 12 symbols taken from the base
field . Thus, the average information rate arriving at each node
over the whole session is 1 symbol/s. The result in [1] asserts
that this rate, which is the minimum of the individual max-flow
bounds of all the nodes in the network, can be achieved. How-
ever, it is seen in our scheme that the nodes in , , can
actually receive the whole message in the first 3, 4, and 12 s, re-
spectively. In this sense, each node in our scheme can receive the
message at rate equal to its individual max-flow bound. Thus,
our result is somewhat stronger than that in [1]. However, our
result does not mean that information can be multicast continu-
ally from the source to each node at rate equal to its individual
max-flow bound.
If it is not necessary for the nodes in to receive the mes-
sage, i.e., the message is multicast to the nodes in and
only, the session can be terminated after 4 s. Then the average
information rate arriving at each node in and over the
truncated session is 3 symbols/s, with the nodes in and
receiving the whole message in the first 3 and 4 s, respectively.
IV. THE TRANSMISSION SCHEME ASSOCIATED WITH AN LCM
Let be an LCM on a communication network , , where
the vectors assigned to outgoing channels linearly
span a -dimensional space. As before, the vector
assigned to a channel is identified with a -dimensional
column vector over the base field of by means of the choice
of a basis. On the other hand, the total information to be trans-
mitted from the source to the rest of the network is represented
by a -dimensional row vector, called the information vector.
Under the transmission scheme prescribed by the LCM , the
data flowing over a channel is the matrix product of the
information vector with the column vector . We now
consider the physical realization of this transmission scheme
associated with an LCM.
Definition: A communication network , is said to be
acyclic if the directed multigraph does not contain a directed
cycle.
Lemma 4.1: An LCM on an acyclic network , is an
assignment of a vector space to every node and a vector
to every channel such that
1) ;
2) for every channel ;
3) for every nonsource node , the space is the linear
span of vectors on all incoming channels
to .
In other words, for an acyclic network, the law of information
flow being observed for every single node implies it being ob-
served for every set of nodes.
Proof: Let be a set of nonsource nodes on an acyclic
network. Let an edge be called an internal edge of when
and . Similarly, let an edge be called an
incoming edge of when and . We need to show
that, for every internal edge of , is generated by
is an incoming edge of
Because the network is acyclic, there exists a node in
without any edge , where . Thus,
1) every incoming edge of is an incoming edge of .
By induction on , we may assume that for every internal edge
of
2) is generated by : is an incoming
edge of .
Given an internal edge of , we need to show that
is generated by : is an incoming edge of . Since
is generated by
is an edge
376 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 49, NO. 2, FEBRUARY 2003
it suffices to show that is generated by :
is an incoming edge of .
If the edge is incoming of , then it is incoming to
by ). Otherwise, is generated by
is an incoming edge of
according to 2) and, therefore, is also generated by
is an incoming edge of
because of 1).
Lemma 4.2: The nodes on an acyclic communication net-
work can be sequentially indexed such that every channel is
from a smaller indexed node to a larger indexed node.
Lemma 4.3: Assume that nodes in a communication network
are sequentially indexed as such that
every channel is from a smaller indexed node to a larger indexed
node. Then, every LCM on the network can be constructed by
the following procedure:
f
for (j = 0; j  n; j++)
f
arrange all outgoing channels XjY from Xj
in an arbitrary order;
take one outgoing channel from Xj at a time
f
let the channel taken be XjY ;
assign v(XjY ) to be a vector in the space
v(Xj);
g
v(Xj+1) = linear span by vectors v(XXj+1) on
all incoming channels XXj+1 to Xj+1;
g
g
On an acyclic network, a straightforward realization of the
above transmission scheme is as follows. Take one node at a
time according to the sequential indexing. For each node, “wait”
until data is received from every incoming channel before per-
forming the linear encoding. Then send the appropriate data on
each outgoing channel.
This physical realization of an LCM over an acyclic network,
however, does not apply to a network that contains a directed
cycle. This is illustrated by the following example.
Example 4.4: Let and be vectors in , where and
are linearly independent. Define
and
This specifies an LCM on the network illustrated in Fig. 2
if the vector is a linear combination of and . Otherwise,
the function gives an example in which the law of informa-
tion flow is observed for every single node but not observed for
Fig. 2. An LCM on a cyclic network.
every set of nodes. Specifically, the law of information flow is
observed for each of the nodes , , and , but not for the set
of nodes , , .
Now, assume that
and
Then, is an LCM. Write the information vector as ,
where and belong to the base field of . According to the
transmission scheme associated with the LCM, all three chan-
nels on the directed cycle transmit the same data . This
leads to the logical problem of how any of these cyclic channels
acquires the data in the first place.
In order to realize the transmission scheme associated with
an LCM over a network containing a directed cycle, we shall in-
troduce the parameter of time into the scheme. Instead of trans-
mitting a single data symbol (i.e., an element of the base field
of ) through each channel, we shall transmit a time-parame-
terized stream of symbols. In other words, the channel will be
time-slotted. Concomitantly, the operation of coding at a node
will be time-slotted, as well.
Definition: Given a communication network , and a
positive integer , the associated memoryless communication
network denoted as , is defined as follows. The set of
nodes in includes the node and all the pairs of the type
, , where is a nonsource node in and ranges through
integers to . The channels in the network , belong
to one of the three types listed below. For any nonsource nodes
and in ,
1) for , the multiplicity of the channel from to ,
is the same as that of the channel in the network ,
;
2) for , the multiplicity of the channel from , to
, is the same as that of the channel in the
network , ;
3) for , the multiplicity of the channel from , to
, is equal to .
Lemma 4.5: The memoryless communication network
, is acyclic.
Lemma 4.6: There exists a fixed number , independent of
, such that for all nonsource nodes in , , the maximum
volume of a flow from to the node , in , is at
least times .
LI et al.: LINEAR NETWORK CODING 377
Proof: Given a nonsource node in , consider a max-
imum flow from to in the network , . This maximum
flow may be partitioned into directed simple paths from to
. Let be the maximum length among these directed simple
paths. Then, there exists a flow from to the node , in
, that is isomorphic to the maximum flow in , .
Moreover, time-shifted isomorphs of this flow in , are
flows from to , for . Moreover, the iso-
morphs are edge-disjoint flows in the network , due to
the acyclic nature in the definition of a flow. The union of these
flows, together with edges from , to , for in
, , constitute a flow from to , with volume
times . The proof is completed by choosing to
be the largest minus among all .
Transmission of data symbols over the network ,
may be interpreted as “memoryless” transmission of data
streams over the network , as follows.
1) A symbol sent from to , in , corresponds
to the symbol sent on the channel in , during
the time slot .
2) A symbol sent from , to , in ,
corresponds to the symbol sent on the channel in
, during the time slot . This symbol is a linear
combination of symbols received by during the time
slot and is unrelated to symbols received earlier by .
3) The channels from , to , for signify the
accumulation of received information by the node in
, over time.
Now suppose an LCM has been constructed on the network
, . Since this is an acyclic network, the LCM can be
physically realized in the way mentioned above. The physical
realization can then be interpreted as a memoryless transmission
of data streams over the original network , .
Transmission, with memory, of data streams over the original
network , is associated with the acyclic network defined
below, which is just a slight modification from , .
Definition: Given a communication network , and a
positive integer , the associated communication network with
memory, denoted as , , is defined as follows. The set of
nodes in includes the node and all pairs of the type , ,
where is a nonsource node in and ranges through integers
to . Channels in the network , belong to one of the
three types listed below. For any nonsource nodes and in
,
1) for , the multiplicity of the channel from to ,
is the same as that of the channel in the network
;
2) for , the multiplicity of the channel from , to
, is the same as that of the channel in the
network ;
3) for , the multiplicity of channels from , to
is equal to times .
Lemma 4.7: The communication network , is
acyclic.
Lemma 4.8: Every flow from the source to the node in the
network , corresponds to a flow with the same volume
from the source to the node , in the network , .
Lemma 4.9: Every LCM on the network , corre-
sponds to an LCM on the network , such that for all
nodes in
Communication networks with or without memory both have
been defined in order to compensate for the lack of a direct
physical realization of an LCM on a network that may contain
a directed cycle. In Section VI, we shall present another way to
make the compensation, which will offer a physical realization
by “time-invariant” linear coding.
V. CONSTRUCTION OF A GENERIC LCM ON AN ACYCLIC
COMMUNICATION NETWORK
We have proved that for a generic LCM, the dimension of
the vector space at each node is equal to the maximum flow
between and . However, we have not shown how one can
construct a generic LCM for a given communication network.
In the next theorem, we present a procedure which constructs a
generic LCM for any acyclic communication network.
Theorem 5.1: A generic LCM exists on every acyclic com-
munication network, provided that the base field of is an
infinite field or a large enough finite field.
Proof: Let the nodes in the acyclic network be sequen-
tially indexed as , such that every
channel is from a smaller indexed node to a larger indexed node.
The following procedure constructs an LCM by assigning a
vector to each channel , one channel at a time.
f
for all channels XY
v(XY ) = the zero vector; // initialization
for (j = 0; j  n; j++)
f
arrange all outgoing channels XjY from Xj
in an arbitrary order;
take one outgoing channel from Xj at a time
f
let the channel taken be XjY ;
choose a vector w in the space v(Xj) such
that w =2 hv(UZ): UZ 2 i for any collection
 of at most d  1 channels with
v(Xj) 6 hv(UZ):UZ 2 i;
v(XjY ) = w;
g
v(Xj+1) = the linear span by vectors
v(XXj+1) on all incoming channels XXj+1
to Xj+1;
g
g
378 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 49, NO. 2, FEBRUARY 2003
The essence of the above procedure is to construct the generic
LCM iteratively and make sure that in each step the partially
constructed LCM is generic. One point in the above construction
procedure of needs to be clarified. Given a node , there can
be only finitely many collections of cardinality at most such
that : . When the base field of is
large enough
where the union is over all such . Hence, the choice of the
vector in the above procedure has been possible.
Let be any collection of channels
such that
for and . In order to assert that the LCM
is generic, we need to prove the linear independence among
the vectors , , , . The proof is by
induction on . Without loss of generality, we may assume that
is the last among the channels to be assigned a vector
in the above construction procedure of . Since
the construction procedure gives
On the other hand, the induction hypothesis asserts the linear
independence among .
Thus, the vectors are linearly
independent. The theorem is proved.
Given a communication network , and a positive in-
teger , there exists a generic LCM on the associated mem-
oryless communication network , by Lemma 4.5 and
Theorem 5.1. From Theorem 3.3, for every node in , ,
the dimension of for the node , in ,
is equal to the maximum volume of a flow from the source to
, . This maximum volume, according to Lemma 4.6, is at
least times , where is a fixed integer.
In view of Lemmas 4.7–4.9, we have the following similar
conclusion about the adapted communication network. For
every node in , , the dimension of the space
for the corresponding node , in , is equal to
the maximum volume of a flow from the source to , in
, , and this maximum volume is at least times
for some fixed number .
We now translate this conclusion about linear-code multicast
over the memoryless network back to the original network ,
. Let be the minimum of over all nonsource
nodes in . Then for a sufficiently large integer , using the
technique in Example 3.5, it is possible to design a broadcast
session of length equal to time units which broadcasts a mes-
sage of approximately symbols from the source. Moreover,
the whole message can be recovered at each nonsource node
after approximately time units.
VI. TIME-INVARIANT LCM AND HEURISTIC CONSTRUCTION
Let be a finite field. Let denote the ring of formal
power series over with the variable . is a principal
ideal domain, and every ideal in it is generated by for some
. Algebraic properties of vector spaces over a field can often
be generalized to modules over a principle ideal domain. In fact,
our previous results on LCM and generic LCM on vector spaces
can readily be generalized to modules over .
The concept of the dimension of a vector space is generalized
to the rank of a module. To facilitate our discussion, we shall
refer to the rank of a module over as its dimension. The
elements of the module will be called vectors.
Throughout this section, we assume that is a module over
with a finite but very large dimension (say, larger than
the number of channels times the length of transmission stream
in the problem instance). Physically, an element of
is the -transform of a stream of symbols , , , , ,
that are sent on a channel, one symbol at a time. The formal
variable is interpreted as a unit-time shift.
Definition: A time-invariant linear-code multicast (TILCM)
on a communication network , is an assignment of a
module over to every node and a vector
to every channel such that
1) ;
2) for every channel ;
3) for any collection of nonsource nodes in the network
In particular, the vector assigned to an outgoing channel from
a node is times a linear combination of the vectors assigned to
incoming channels to the same node. Hence a TILCM is com-
pletely determined by the vectors that it assigns to channels.
The adoption of times a linear combination, instead of simply
any linear combination, allows a unit-time delay for transmis-
sion/coding. This allows a physical realization for the transmis-
sion scheme specified by a TILCM. If a certain physical system
requires nonuniform duration of delay on different channels,
then artificial nodes need to be inserted. For instance, if the delay
on a channel is three unit times, then two serial nodes should be
added on the channel so that the channel becomes a tandem of
three channels.
Example 6.1: On the communication network illustrated in
Fig. 2, define the TILCM by
and
One can readily check that is in fact a TILCM. For example
Thus, is equal to times the linear combination of
and with coefficients and , respectively.
This specifies an encoding process for the channel that
does not change with time. It can be seen that the same is true
for the encoding process of every other channel in the network.
This explains the terminology “time-invariant” for an LCM.
LI et al.: LINEAR NETWORK CODING 379
Write the information vector as , where
and
belong to . The product of the information (row) vector
with the (column) vector assigned to that channel represents the
data stream transmitted over a channel
Adopt the convention that for all . Then the
data symbol flowing over the channel , for example, at the
time slot is for all .
Example 6.2: Another TILCM on the same communication
network can be defined by
and
The data stream transmitted over the channel , for instance,
is represented by
Fig. 3. Redundant channels in a network.
That is, the data symbol
is sent on the channel at the time slot . This TILCM
, besides being time invariant in nature, is a “memoryless”
one because the following linear equations allows an encoding
mechanism that requires no memory:
and
There are potentially various ways to define a generic TILCM
and, as an analog to Theorem 3.3, to establish desirable dimen-
sions of the module assigned to every node. Another desirable
characteristic of a TILCM is to be “memoryless.” Empirically,
it has been relatively easy to construct a TILCM that carries all
conceivable desirable properties.
The remainder of this section presents a heuristic construc-
tion procedure for a “good” TILCM. The heuristic construction
will follow the graph-theoretical block decomposition of the net-
work. For the sake of computational efficiency, the procedure
will first remove “redundant” channels from the network before
identifying the “blocks” so that the “blocks” are smaller.
Definition: A channel in a communication network is said
to be irredundant if it is on a simple path starting at the source.
Else, it is said to be redundant. Moreover, a communication
network is said to be irredundant if it contains no redundant
channels.
Example 6.3: In the network illustrated in Fig. 3, the chan-
nels , , and are redundant.
Lemma 6.4: The deletion of a redundant channel from a net-
work results in a subnetwork with the same set of irredundant
channels. Consequently, the irredundant channels in a network
define an irredundant subnetwork.
Theorem 6.5: Let be an LCM (respectively, a TILCM) on a
network. Then also defines an LCM (respectively, a TILCM)
on the subnetwork that results from the deletion of any redun-
dant channel.
380 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 49, NO. 2, FEBRUARY 2003
Proof: We shall prove the case of LCM, the case of
TILCM being similar. Denote the deleted redundant channel
as . Given a set of nonsource nodes with and
, we need to show
and
and
Due to the redundancy of , any simple path from to
must go through the node . Let denote the set consisting
of the node plus all those nodes such that any path from
to must go through . (If there is no simple path from
to a particular node, then that node fully qualifies as a node in
.) Then, every channel from outside of into must
be toward the node . Therefore,
and
and
and
and
and
This implies the desired equality.
Corollary 6.6: Let be an LCM (respectively, a TILCM) on
a network. Then defines an LCM (respectively, a TILCM) on
the subnetwork formed by irredundant channels.
In a directed multigraph, an equivalence relationship (which
by definition is reflexive, symmetric and transitive) among
nodes can be defined as follows. Two nodes are equivalent if
there exists a directed path leading from one node to the other
and vice versa. An equivalence class under this relationship is
called a block in the graph. The source node by itself always
forms a block. When every block “contracts” into a single
node, the resulting graph is acyclic. In other words, the blocks
can be sequentially indexed so that every interblock channel is
from a smaller indexed block to a larger indexed block.
For the construction of a “good” TILCM, smaller sizes of
blocks tend to facilitate the computation. The extreme favor-
able case of the block decomposition of a network is when the
network is acyclic, which implies that every block consists of a
single node. The opposite extreme is when all nonsource nodes
form a single block exemplified by the network illustrated in
Fig. 3.
The removal of redundant channels sometimes serves for the
purpose of breaking up a block into pieces. For the network
illustrated in Fig. 3, the removal of the three redundant channels
breaks the block , , , , into the three blocks ,
, , , and .
In the construction of a “good” LCM on an acyclic network,
the procedure inside the proof of Theorem 5.1 takes one node
at a time according to the acyclic ordering of nodes and assigns
vectors to outgoing channels from the taken node. For a gen-
eral network, we can start with the trivial TILCM on the net-
work consisting of just the source and then expand it to a “good”
TILCM that covers one more block at a time.
The sequential choices of blocks are according to the acyclic
order in the block decomposition of the network. Thus, the
expansion of the “good” TILCM at each step involves only
incoming channels to nodes in the new block. A heuristic
algorithm for assigning vectors to such channels
is for to be times an arbitrary convenient linear
combination of vectors assigned to incoming channels to .
In this way, a system of linear equations of the form of
is set up, where is a square matrix with the dimension
equal to the total number of channels in the network and is
the unknown column vector whose entries are for all
channels . The elements of and are polynomials in . In
particular, the elements of are either or a polynomial
in containing the factor . Therefore, the determinant of
is a formal power series with the constant term (the zeroth
power of ) being , and, hence is invertible in .
According to Cramer’s rule, a unique solution exists. (This
is consistent with the physical intuition because the whole
network is completely determined once the encoding process
for each channel is specified.) If this unique solution does
not happen to satisfy the requirement for being a “good”
TILCM, then the heuristic algorithm calls for adjustments on
the coefficients of the linear equations on the trial-and-error
basis.
After a “good” TILCM is constructed on the subnetwork
formed by irredundant channels in a given network, we may
simply assign the zero vectors to all redundant channels.
Example 6.7: After the removal of redundant channels, the
network depicted by Fig. 3 consists of four blocks in the order
of , , , , , and . The subnetwork consisting
of the first two blocks is the same as the network in Fig. 2. When
we expand the trivial TILCM on the network consisting of just
the source to cover the block , , , a heuristic trial would
be
together with the following linear equations:
and
The result is the memoryless TILCM in the preceding ex-
ample. This TILCM can be further expanded to cover the block
and then the block .
VII. CONCLUDING REMARKS
In this paper, we have presented an explicit construction of
a code for multicast in a network that achieves the max-flow
bound on the information transmission rate. An important
aspect of our code is its linearity, which makes encoding and
decoding easy to implement in practice. Our greedy algorithm
for code construction works for all networks, but the code
LI et al.: LINEAR NETWORK CODING 381
so constructed is not necessarily the simplest possible. In
fact, there often exist optimal LCMs much simpler than the
ones constructed according to the presented greedy algorithm.
Therefore, there is much room for further research in this
direction.
The code we have constructed for a cyclic network is time
varying, which makes it less appealing in practice. To our
knowledge, there has not been a proof for the existence of an
optimal time-invariant code for a cyclic network. However,
examples of such a code have been given in the present paper
and also in [1]. Therefore, proving the existence of such codes,
in particular, constructing such codes in simple linear form, is
a challenging problem for future research.
Further research problems in network coding include code
construction when two or more sources are simultaneously mul-
ticast in the network. This is the so-called multisource network
coding problem in [1] which is yet unexplored (see also [5, Ch.
15]).
When networking coding is implemented in computer or
satellite networks, synchronization is a problem that needs to be
addressed. Since an encoder at an intermediate node may take
more than one incoming data stream as input, it is necessary
to acquire synchronization among these data streams. This is
not a problem for nonreal-time applications (e.g., file transfer),
but it can be a serious problem for real-time applications (e.g.,
voice and video transmission). On the other hand, certain types
of networks, for example switching networks, are inherently
fully synchronized. These networks are excellent candidates for
network coding, and, in fact, the possible use of linear network
codes in switching networks has been investigated [6].
ACKNOWLEDGMENT
The authors would like to thank Venkat Anantharam for
pointing out an error in the original proof of Theorem 3.3 and
for his very careful reading of the manuscript. They also thank
the anonymous reviewers for their useful comments.
REFERENCES
[1] R. Alshwede, N. Cai, S.-Y. R. Li, and R. W. Yeung, “Network informa-
tion flow: Single source,” IEEE Trans. Inform. Theory, submitted for
publication.
[2] E. L. Lawler, Combinatorial Optimization: Network and Matroid. Fort
Worth, TX: Saunder College Pub., 1976.
[3] S.-Y. R. Li and R. W. Yeung, “Network multicast flow via linear
coding,” in Proc. Int. Symp. Operational Research and Its Applications
(ISORA’98), Kunming, China, Aug. 1998, pp. 197–211.
[4] D. J. A. Welsh, Matroid Theory. New York: Academic , 1976.
[5] R. W. Yeung, A First Course in Information Theory. Norwell,
MA/New York: Kluwer/Plenum, 2002.
[6] F. R. Kschischang, private communication.

