Entropy: a Consolidation Manager for Clusters
Fabien Hermenier, Xavier Lorca, Jean-Marc Menaud, Gilles Muller, Julia L.
Lawall
To cite this version:
Fabien Hermenier, Xavier Lorca, Jean-Marc Menaud, Gilles Muller, Julia L. Lawall. En-
tropy: a Consolidation Manager for Clusters. VEE 2009 - 5th International Conference on
Virtual Execution Environments, Mar 2009, Washington, DC, United States. ACM, ACM
SIGPLAN/SIGOPS, pp.41-50, 2009, .
HAL Id: hal-01102354
https://hal.archives-ouvertes.fr/hal-01102354
Submitted on 12 Jan 2015
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Entropy: a Consolidation Manager for Clusters
Fabien Hermenier
École des Mines de Nantes, France
INRIA, LINA UMR CNRS 6241fabien.hermenieremn.fr Xavier LorcaÉcole des Mines de Nantes, FranceLINA UMR CNRS 6241xavier.loraemn.fr Jean-Marc MenaudÉcole des Mines de Nantes, FranceINRIA, LINA UMR CNRS 6241jean-mar.menaudemn.fr
Gilles Muller
École des Mines de Nantes, France
INRIA-Regalgilles.mulleremn.fr Julia LawallDIKU, University of Copenhagen, Danemarkjuliadiku.dk
Abstract
Clusters provide powerful computing environments, but in prac-
tice much of this power goes to waste, due to the static allocation
of tasks to nodes, regardless of their changing computational re-
quirements. Dynamic consolidation is an approach that migrates
tasks within a cluster as their computational requirements change,
both to reduce the number of nodes that need to be active and to
eliminate temporary overload situations. Previous dynamic consol-
idation strategies have relied on task placement heuristics that use
only local optimization and typically do not take migration over-
head into account. However, heuristics based on only local opti-
mization may miss the globally optimal solution, resulting in un-
necessary resource usage, and the overhead for migration may nul-
lify the benefits of consolidation.
In this paper, we propose the Entropy resource manager for ho-
mogeneous clusters, which performs dynamic consolidation based
on constraint programming and takes migration overhead into ac-
count. The use of constraint programming allows Entropy to find
mappings of tasks to nodes that are better than those found by
heuristics based on local optimizations, and that are frequently
globally optimal in the number of nodes. Because migration over-
head is taken into account, Entropy chooses migrations that can be
implemented efficiently, incurring a low performance overhead.
Categories and Subject Descriptors D.4.7 [Operating systems]:
Distributed systems
General Terms Algorithms, Design
Keywords Virtualization, Dynamic Consolidation, Cluster, Re-
configuration, Migration
1. Introduction
Grid and Cluster computing are increasingly used to meet the
growing computational requirements of scientific applications. In
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. To copy otherwise, to republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee.
VEE’09, March 11–13, 2009, Washington, DC, USA.
Copyright c© 2009 ACM 978-1-60558-375-4/09/03. . . $5.00
this setting, a user organizes a job as a collection of tasks that each
should run on a separate processing unit (i.e, an entire node, a CPU,
or a core) [6]. To deploy the job, the user makes a request to a
resource broker, specifying the number of processing units required
and the associated memory requirements. If the requested CPU and
memory resources are available, the job is accepted. This static
strategy ensures that all jobs accepted into the cluster will have
sufficient processing units and memory to complete their work.
Nevertheless, it can lead to a waste of resources, as many scientific
computations proceed in phases, not all of which use all of the
allocated processing units at all times.
Consolidation is a well-known technique to dynamically reduce
the number of nodes used within a running cluster by liberating
nodes that are not needed by the current phase of the computa-
tion. Liberating nodes can allow more jobs to be accepted into the
cluster, or can allow powering down unused nodes to save energy.
To make consolidation transparent, regardless of the programming
language, middleware, or operating system used by the application,
it is convenient to host each task in a virtual machine (VM), man-
aged by a VMMonitor (VMM) such as Xen [1], for which efficient
migration techniques are available [5]. Consolidation then amounts
to identifying inactive VMs that can be migrated to other nodes that
have sufficient unused memory. A VM that is inactive at one point
in time may, however, later become active, possibly causing the
node that is hosting it to become overloaded. A consolidation strat-
egy must thus also be able to move VMs from overloaded nodes to
underloaded ones.
Several approaches to consolidation have been proposed [3, 7,
11]. These approaches, however, have focused on how to calculate
a new configuration, and have neglected the ensuing migration
time. However, consolidation is only beneficial when the extra
processing unit time incurred for migration is significantly less
than the amount of processing unit time that consolidation makes
available. While migrating a single Xen VM can be very efficient,
incurring an overhead of only between 6 and 26 seconds in our
measurements, it may not be possible to migrate a VM to its
chosen destination immediately; instead other VMs may first have
to be moved out of the way to free sufficient memory. Delaying
the migration of an inactive VM only causes unnecessary node
usage. On the other hand, delaying the migration of an active VM
that is running on a processing unit overloaded with n other VMs
degrades the performance of those VMs for a period of time by
a factor of n as compared to a non-consolidated solution, in which
each VM always has its own processing unit. Increasing the number
of VMs that need to migrate as compared to the amount of available
resources only exacerbates these problems. Thus, it is essential that
consolidation be as efficient and reactive as possible.
In this paper, we propose a new approach to consolidation in a
homogeneous cluster environment that takes into account both the
problem of allocating the VMs to the available nodes and the prob-
lem of how to migrate the VMs to these nodes. Our consolidation
manager, Entropy, works in two phases and is based on constraint
solving [2, 14]. The first phase, based on constraints describing the
set of VMs and their CPU and memory requirements, computes
a placement using the minimum number of nodes and a tentative
reconfiguration plan to achieve that placement. The second phase,
based on a refined set of constraints that take feasible migrations
into account, tries to improve the plan, to reduce the number of
migrations required. In our experiments, using the NASGrid bench-
marks [6] on a cluster of 39 AMDOpteron 2.0GHz CPU uniproces-
sors, we find that a solution without consolidation uses 24.31 nodes
per hour, consolidation based on the previously-used First Fit De-
creasing (FFD) heuristic [3, 17, 18] uses 15.34 nodes per hour, and
consolidation based on Entropy uses only 11.72 nodes per hour, a
savings of more than 50% as compared to the static solution.
The rest of this paper is organized as follows. Section 2 gives
an overview of Entropy. Then, Section 3 describes how Entropy
uses constraint programming to determine the minimum number of
nodes required by a collection of VMs, and Section 4 describes how
Entropy uses constraint programming to minimize the reconfigura-
tion plan. Finally, Section 5 evaluates Entropy using experimental
results on a cluster of the Grid’5000 experimental testbed. Section 6
describes related work, and Section 7 presents our conclusions and
future work.
2. System Architecture
A cluster typically consists of a single node dedicated to cluster
resource management, a collection of nodes that can host user
tasks, and other specialized nodes, such as file servers. Entropy is
built over Xen 3.0.3 [1] and is deployed on the first two. It consists
of a reconfiguration engine that runs on the node that provides
cluster resource management and a set of sensors that run in Xen’s
Domain-0 on each node that can host user tasks, i.e., VMs.
The goal of Entropy is to efficiently maintain the cluster in a
configuration, i.e. a mapping of VMs to nodes, that is (i) viable, i.e.
that gives every VM access to sufficient memory and every active
VM access to its own processing unit, and (ii) optimal, i.e. that uses
the minimum number of nodes. Figure 1 shows the global design
of Entropy. The reconfiguration engine acts as a loop that 1) itera-
tively waits to be informed by the Entropy sensors that a VM has
changed state, from active to inactive or vice versa, 2) tries to com-
pute a reconfiguration plan starting from the current configuration
that requires the fewest possible migrations and leaves the cluster
in a viable, optimal configuration, and 3) if successful, initiates mi-
gration of the VMs, if the new configuration uses fewer nodes than
the current one, or if the current configuration is not viable. The
reconfiguration engine then accumulates new information about re-
source usage, which takes about 5 seconds for our prototype, before
repeating the iteration. In this process, each Entropy sensor period-
ically sends requests to the HTTP interface of the Xen hypervisor
on the current node to obtain the CPU usage of the local VMs, and
infers state changes from this information. An Entropy sensor also
receives a message from the reconfiguration engine when a VM
should be migrated, and sends a migration request to the Xen hy-
pervisor HTTP interface.
Previous approaches to achieving a viable, configuration have
used heuristics in which a locally optimal placement is chosen for
each VM according to some strategy [3, 7, 11, 17]. However, lo-
cal optimization does not always lead to a globally optimal solu-
Figure 1. Reconfiguration loop
tion, and may fail to produce any solution at all. Entropy instead
uses Constraint Programming (CP), which is able to determine a
globally optimal solution, if one exists, by using a more exhaustive
search, based on a depth first search. The idea of CP is to define a
problem by stating constraints (logical relations) that must be sat-
isfied by the solution. A Constraint Satisfaction Problem (CSP) is
defined as a set of variables, a set of domains representing the set
of possible values for each variable and a set of constraints that
represent required relations between the values of the variables. A
solution for a CSP is a variable assignment (a value for each vari-
able) that simultaneously satisfies the constraints. To solve CSPs,
Entropy uses the Choco library [10], which can solve a CSP where
the goal is to minimize or maximize the value of a single variable.
Because Choco and most other constraint solvers can only solve
optimization problems of a single variable, the reconfiguration al-
gorithm proceeds in two phases (see Figure 1). The first phase finds
the minimum number n of nodes are necessary to host all VMs
and a sample viable configuration that uses this number of nodes.
We refer to this problem as the Virtual Machine Packing Problem
(VMPP). The second phase computes an equivalent viable config-
uration that minimizes the reconfiguration time, given the chosen
number of nodes n. We refer to this problem as the Virtual Ma-
chine Replacement Problem (VMRP). Solving these problems may
be time-consuming. While the reconfiguration engine runs on the
cluster resource management node, and thus does not compete with
VMs for CPU and memory, it is important to produce a new con-
figuration quickly to maximize the benefit of consolidation. Choco
has the property that it can be aborted at any time, in which case
it returns the best result computed so far. This makes it possible
to impose a time limit on the solver, to ensure the reactivity of the
approach. Thus, we limit the total computation time for both prob-
lems to 1 minute, of which the first phase has at most 15 seconds,
and the second phase has the remaining time. These durations are
sufficient to give a nontrivial improvement in the solution, as com-
pared to the FFD heuristic, as shown in Section 5. In our initial
experiments, we tried to model the reconfiguration algorithm with
a single problem that proposed a trade-off between the number of
used nodes and the number of migrations to perform. However, the
computation time was much higher for an at best equivalent pack-
ing and reconfiguration cost.
3. The Virtual Machine Packing Problem
The objective of the VMPP is to determine the minimum number
of nodes that can host the VMs, given their current processing-
unit and memory requirements. We first present several examples
that illustrate the constraints on the assignment of VMs to nodes,
then consider how to express the VMPP as a constraint satisfaction
problem, and finally describe some optimizations that we use in
implementing a solver for this problem using Choco.
3.1 Constraints on the assignment of VMs to nodes
Each node in a cluster provides a certain amount of memory and
number of processing units, and each VM requires a certain amount
of memory, and, if active, a processing unit. These constraints must
be satisfied by a viable configuration. For example, if every node is
a uniprocessor, then the configuration in Figure 2(a) is not viable
because it includes two active VMs on node N1. On the other hand,
the configuration in Figure 2(b) is viable because each VM has
access to sufficient memory and each node hosts at most one active
VM.
(a) Non-viable
configuration
(b) Viable configu-
ration
Figure 2. Non-viable and viable configurations. VM2 and VM3 are
active
(a) A minimal vi-
able configuration
(b) Another mini-
mal viable configu-
ration
Figure 3. Viable configurations. VM2 and VM3 are active
To achieve the highest degree of consolidation, we must find a
viable configuration that uses the minimum number of nodes. For
example, the configuration shown in Figure 2(b) is viable, but it
is not minimal, because, as shown in Figure 3(a), VM2 could be
hosted on node N2, using one fewer nodes. The problem of find-
ing a minimal, viable configuration is comparable to the NP-Hard
2-Dimensional Bin Packing Problem [15], where the dimensions
correspond to the amount of memory and number of processing
units.
The VMPP may have multiple solutions, as illustrated by Fig-
ures 3(a) and 3(b), which both use two nodes. These solutions,
however, may not all entail the same number of migrations. For
example, if we perform consolidation with Figure 2(b) as the ini-
tial configuration, we observe that only 1 migration is necessary
to reach the configuration shown in Figure 3(a) (moving VM2 onto
N2), but 2 are necessary to reach the configuration shown in Fig-
ure 3(b) (moving VM3 onto N2 and VM1 onto N3).
3.2 Expressing the VMPP as a constraint satisfaction
problem
To express the VMPP as a CSP, we consider a set of nodes N
and a set of VMs V . The goal is to find a viable configuration
that minimizes the number of nodes used. The notation Hi, defined
below, is used to describe a configuration.
DEFINITION 3.1. For each node ni ∈ N , the bit vector Hi =
〈hi1, . . . , hij , . . . , hik〉 denotes the set of VMs assigned to node ni
(i.e., hij = 1 iff the node ni is hosting the VM vj).
We express the constraints that a viable configuration must
respect each VM’s processing unit and memory requirements as
follows. Let Rp be the vector of processing unit demand of each
VM, Cp be the vector of processing unit capacity associated with
each node, Rm be the vector of memory demand of each VM, and
Cm be the vector of memory capacity associated with each node.
Then, the following inequalities express the processing unit and
memory constraints:
Rp · Hi ≤ Cp(ni) ∀ni ∈ N
Rm · Hi ≤ Cm(ni) ∀ni ∈ N
Given these constraints, our goal is to minimize the value of the
variable X, defined as follows, where the variable ui is 1 if the
node ni hosts at least one VM, and 0 otherwise.
X =
X
i∈N
ui, where ui =
(
1, ∃vj ∈ V | hij = 1
0, otherwise
(1)
We let xvmpp denote this solution.
The solver dynamically evaluates the remaining free place (in
terms of both processing unit and memory availability) on each
node during the search for a minimum value of X. This is done by
solving a Multiple Knapsack problem using a dynamic program-
ming approach [16].
3.3 Optimizations
In principle, the constraint solver must enumerate each possible
configuration, check whether it is viable, and compare the number
of nodes to the minimum found so far. In practice, this approach is
unnecessarily expensive. Our implementation reduces the compu-
tation cost using a number of optimizations.
Choco incrementally checks the viability and minimality of a
configuration as it is being constructed and discards a partial con-
figuration as soon as it is found to be non-viable or to use more than
the minimum number of nodes found so far. This strategy reduces
the number of configurations that must be considered. Choco fur-
thermore tries to detect non-viable configurations as early as pos-
sible, by using a first fail approach [8] in which VMs that are ac-
tive and have greater memory requirements are treated earlier than
VMs with lesser requirements. This strategy reduces the chance of
computing an almost complete configuration and then finding that
the remaining VMs cannot be placed within the current minimum
number of nodes.
In principle, the domain of the variable X is the entire set
of non-negative integers. We can, however, significantly reduce
the search space and improve the performance of the solver by
identifying lower and upper bounds that are close to the optimal
value and are easy to compute. As a lower bound, we take the
number of active VMs divided by number of processing units
available per node (Equation 2). If we find a solution using this
number of VMs, then it is known to be optimal with no further tests.
As an upper bound, we take the value computed by the First Fit
Decreasing (FFD) heuristic, which has been used in other work on
consolidation [3, 17, 18] (Equation 3). The FFD heuristic assigns
each VM to the first node it finds satisfying the VM’s processing
unit and memory requirements, starting with the VMs that require
the biggest amount of memory. This heuristic tends to provide a
good value, in a very short time (less than a second), but the result is
not guaranteed to be optimal and the heuristic may indeed not find
any solution. In the latter case, the upper bound is the minimum of
the number of nodes and the number of VMs.
X ≥
2
6
6
6
6
6
6
X
vj∈V
Rp(vj)
Cp(ni)
3
7
7
7
7
7
7
, ni ∈ N (2)
X ≤
(
xffd
min(|N |, |V|), otherwise
(3)
Furthermore, we observe that some nodes or VMs may be
equivalent, in terms of their processing unit and memory capacity
or demand, and try to exploit this information to improve the
pruning of the search tree. If the resources offered by a node ni
are not sufficient to host a VM vj , then they are also not sufficient
to host any VM vk with the same requirements. Furthermore, the
VM vj cannot be hosted by any other node nk with the same
characteristics as ni. These equivalences are defined as follows:
∀ni, nk ∈ N | ni ≡ nk ⇔ Cp(ni) = Cp(nk) ∧
Cm(ni) = Cm(nk) (4)
∀vj , vk ∈ V | vj ≡ vk ⇔ Rp(vj) = Rp(vk) ∧
Rm(vj) = Rm(vk) (5)
These optimizations reduce the size of the search tree. This
tends to reduce the time required to compute a solution, and allows
solving configurations that involve more nodes or VMs.
4. The Virtual Machine Replacement Problem
The solution to the VMPP provides the minimum number of nodes
required to host the VMs. However, as illustrated in Section 3.1, for
a given collection of VMs, there can be multiple configurations that
minimize the number of used nodes, and the number of migrations
required to reach these configurations can vary. The objective of
the Virtual Machine Replacement Problem (VMRP) is to construct
a reconfiguration plan for each possible configuration that uses the
number of nodes determined by the VMPP, and to choose the one
with the lowest estimated reconfiguration cost. In the rest of this
section, we consider how to construct a reconfiguration plan, how
to estimate its cost, and how to combine these steps into a solution
for the VMRP.
4.1 Constructing a reconfiguration plan
The constraint of viability has to be taken into account not only
in the final configuration but also during migration. A migration
is feasible if the destination node has a sufficient amount of free
memory and, when the migrated VM is active, if the destination
node has a free processing unit. However, to obtain an optimal solu-
tion it is often necessary to consider a configuration in which some
migrations are not immediately feasible. We identify two kinds of
constraints on migrations: sequential constraints and cyclic con-
straints.
A sequential constraint occurs when one migration can only
begin when another one has completed. As an example, consider
the migrations represented by the reconfiguration graph shown in
Figure 4. A reconfiguration graph is an oriented multigraph where
each edge denotes the migration of a VM between two nodes.
Each edge specifies the virtual machine to migrate, the amount
of memory rm required to host it and its state A (active) or I
(inactive). Each node denotes a node of the cluster, with its current
amount of free memory cm and its current free capacity for hosting
active virtual machines cp. In the example in Figure 4, it is possible
to consolidate the VMs onto only two nodes, by moving VM1 from
N1 to N2 and moving VM2 from N2 to N3. But these migrations
cannot happen in parallel, because as long as VM2 is on N2, it
consumes all of the available memory. Thus, the migration of VM1
from N1 to N2 can only begin once the migration of VM2 from N2
to N3 has completed.
N1
cm=400,cp=0
N2
cm=0,cp=1
N3
cm=400,cp=0
VM1
rm=200,A
VM2
rm=400,I
Figure 4. A sequence of migrations
A cyclic constraint occurs when a set of infeasible migrations
forms a cycle. An example is shown in Figure 5(a), where, due
to memory constraints, VM1 can only migrate from node N1 to
node N2 when VM2 has migrated from node N2, and VM2 can only
migrate from node N2 to node N1 when VM1 has migrated from
node N1. We can break such a cycle by inserting an additional
migration. A pivot node outside the cycle is chosen to temporarily
host one or more of the VMs. For example, in Figure 5(b), the cycle
between VM1 and VM2 is broken by migrating VM1 to the node N3,
which is used as a pivot. After breaking all cycles of infeasible
migrations in this way, an order can be chosen for the migrations
as in the previous example. These migrations include moving the
VMs on the pivot nodes to their original destinations.
N1
cm=0,cp=-1
N2
cm=0,cp=1
VM1
rm=256,A
VM2
rm=256,I
(a) Inter-dependant
migrations
N1
cm=0,cp=-1
N3
cm=512,cp=1
N2
cm=0,cp=1
VM1
rm=256,A
VM1
rm=256,A
VM2
rm=256,I
(b) A bypass migration breaks the cycle
Figure 5. Cycle of non-feasible migrations
Taking the above issues into account, the algorithm for con-
structing a reconfiguration plan is as follows. Starting with a re-
configuration graph, the first step is to identify each cycle of in-
feasible migrations, identify a node in each such cycle where the
VMs to migrate have the smallest total memory requirement, and
select a pivot node that can accomodate these VMs’ processing unit
and memory requirements. The result is an extended reconfigura-
tion graph in which for each such chosen VM, the migration from
the current node to the destination node in the desired configuration
is replaced by a migration to the pivot followed by a migration to
the destination node. Subsequently, the goal is to try to do as many
migrations in parallel as possible, so that each migration will take
place with the minimum possible delay. Thus, the migration plan is
composed of a sequence of steps, executed sequentially, where the
first step consists of all of the migrations that are initially feasible,
and each subsequent step consists of all of the migrations that have
been made feasible by the preceding steps. As an example, Figure 6
shows a reconfiguration graph that has been extended with a migra-
tion of VM5 first to node N2 and then to node N3 to break a cycle of
infeasible migrations. From this reconfiguration graph, we obtain
a three-step reconfiguration plan. The first step migrates VM1, VM3,
VM4 and VM5 (to the pivot N2). The second step migrates VM2 and
VM7. Finally, the third step migrates VM5 to its final destination.
N2
cm=512,cp=1
N4
cm=512,cp=1
N5
cm=768,cp=0
N3
cm=512,cp=0
N1
cm=640,cp=0
I. VM4
rm=256,I
I. VM3
rm=256,I
III. VM5
rm=256,A
II. VM7
rm=384,A
II. VM2
rm=512,A
I. VM1
rm=256,II. VM5
Figure 6. A reconfiguration plan
4.2 Estimating the cost of a reconfiguration plan
The cost of performing a reconfiguration includes both the over-
head incurred by the migrations themselves and the degradation in
performance that occurs when multiple active VMs share a process-
ing unit, as occurs when a migration is delayed due to sequential or
cyclic constraints. The latter is determined by the duration of pre-
ceding migrations. In this section, we first measure the cost and
duration of a single migration, and then propose a cost model for
comparing the costs of possible reconfiguration plans.
Migration cost Migrating a VM from one node to another re-
quires some CPU and memory bandwidth on both the source and
destination nodes. When there are active VMs either on the source
or destination node, it will have reduced access to these resources,
and thus will take longer to complete its task. In this section, we
examine these costs in the context of a homogeneous cluster.
Figure 7 shows the set of possible contexts in which a migration
can occur, depending on the state of the affected VMs, in the case
where each node is a uniprocessor. Because a migration only has
an impact on the active and migrated VMs, we ignore the presence
of inactive, non-migrated VMs in this analysis. An inactive VM
can move from an inactive node to a node hosting an active VM
(Inactive To Active, or ITA), from a node hosting an active VM to
an inactive node (Inactive From Active, or IFA), or from one node
hosting an active VM to another (Inactive From Active To Active,
or IFATA). Similarly, an active VM can move to an inactive node
(Active To Inactive, or ATI) or to an active node (Active To Active,
or ATA), although the latter is never interesting in a uniprocessor
setting as a uniprocessor node should not host multiple active VMs
at one time.
In order to evaluate the impact of a migration for each context,
we measure both the duration of the migration and the performance
loss on active VMs. Tests are performed on two identical nodes,
each with a single AMD Opteron 2.4GHz CPU and 4Gb of RAM
interconnected through a 1Gb link. We use three VMs: VM1, which
is inactive, and VM2 and VM3, which are active and execute a NAS-
GRID ED benchmark [6] composed with BT.W tasks. The VMs
are placed on the nodes according to the IFATA, ITA, ATI, and IFA
configurations. We vary the amount of memory allocated to the mi-
grated VM from 512 to 2048 MB. Figure 8 shows the average du-
ration of the migration in terms of the amount of memory allocated
to the migrated VM. Figure 9 shows the increase in the duration of
the benchmark due to the migration of a VM using a given amount
of memory.
We observe first that the duration of the migration mostly de-
pends on the amount of memory used by the migrated VM. Second,
the performance loss varies significantly according to the context of
(a) ITA (b) IFA (c) IFATA
(d) ATI (e) ATA
Figure 7. Different contexts for a migration. VM2 is active
 0
 5
 10
 15
 20
 25
 512  1024  1536  2048
M
ig
ra
ti
o
n
 t
im
e
 i
n
 s
e
c
.
Memory used by the migrated VM, in MB
IFATA
ITA
ATI
IFA
Figure 8. Duration of VM migration
 0
 5
 10
 15
 20
 25
 30
 512  1024  1536  2048
M
ig
ra
ti
o
n
 o
v
e
rh
e
a
d
 i
n
 s
e
c
.
Memory used by the migrated VM, in MB
IFATA
ITA
ATI
IFA
Figure 9. Impact of migration on VM performance
the migration. For the context IFA, the only overhead comes from
reading the memory pages on node N1, as writing the pages on the
inactive node N2 does not have any impact on an active VM. For
the context ATI, it is the active VM that migrates; in this situation,
the migration is a little more expensive: because Xen uses an incre-
mental copy-on-write mechanism to migrate the memory pages of
a VM [5], multiple passes are needed to recopy memory pages that
are updated by the activity of the VM during the migration pro-
cess. The context ITA incurs an even higher overhead, as writing
the memory pages of VM1 on node N2 uses up most of the CPU re-
sources on that node, which are then not available to VM2. Finally,
the context IFATA incurs the highest overhead as the migrations
act on both the source and the destination node. This overhead is
comparable to the sum of the overhead for contexts IFA and ITA.
This evaluation of the cost of migrations shows that migrating
a VM has an impact on both the source and destination nodes. The
migration reduces the performance of co-hosted active virtual ma-
chines for a duration that depends on the context of the migration.
In the worst case, the performance loss of a computational task is
about the same as the duration of the migration. Although the over-
head can be heavy during the migration time, the migration time is
fairly short, and thus has little impact on the overall performance.
Nevertheless, these numbers suggest that the number of migrations
should be kept to a minimum.
Migration cost model Figures 8 and 9 show that the overhead for
a single migration and the delay incurred for preceding migrations
both vary principally in terms of the amount of memory allocated
to the migrated VMs. Thus, we base the cost model on this quantity.
The cost function f is defined as follows. The estimated cost
f(p) of a reconfiguration plan p is the sum of the costs of the
migrations of each migrated VM vj (Equation 6). The estimated
cost f(vj) of the migration of a VM vj is the sum of the estimated
costs of the preceding steps, plus the amount of memory allocated
to vj (Equation 7). Finally, the estimated cost f(s) of a step s is
equal to the largest amount of memory allocated to any VM that
is migrated in step s. This estimated cost conservatively assumes
that one step can only begin when all of the migrations of the
previous step have completed. For the reconfiguration plan shown
in Figure 6, the estimated cost of step II is 512, the estimated cost
of the migration of VM2 is 768, and the estimated cost of the whole
reconfiguration plan is 4224.
f(p) =
X
vj∈Vmigrate
f(vj) (6)
f(vj) = Rm(vj) +
X
s∈prevs(vj )
f(s) (7)
f(s) = max(Rm(vj)), vj ∈ s (8)
4.3 Implementing and optimizing the VMRP
To express the VMRP as a CSP, we again use the constraints that a
configuration must be viable, as described in Section 3.2, and addi-
tionally specify that the number of nodes used in a configuration is
equal to the solution of the VMPP (Equation 9):
X
i∈N
ui = xvmpp (9)
For each configuration that satisfies these constraints, the solver
constructs a reconfiguration plan p, if possible. The optimal solu-
tion k is the one that minimizes the variable K, defined as follows
(Equation 10):
K = f(p) (10)
Minimizing the cost of a reconfiguration provides a plan with fewer
migrations and steps, and a maximum degree of parallelism, thus
reducing the duration and the impact of a reconfiguration.
The lower bound for K is the sum of the cost of migrating each
VM that must migrate i.e. when multiple active VMs are hosted
on the same node. The upper bound corresponds to the cost of
the reconfiguration plan pvmpp associated with the configuration
previously computed by VMPP:
(
X
vj∈Vmigrate
Rm(vj)) ≤ K ≤ f(pvmpp) (11)
Like the VMPP, the VMRP uses equivalences to reduce the time
required to find viable configurations. For the VMRP, however, the
equivalence relation between VMs has to be more restrictive to take
into account the impact of their migration. Indeed, migration of
equivalent VMs must have the same impact on the reconfiguration
process. Thus, equivalent VMs must have the same resource de-
mands and must be hosted on the same nodes. In this situation,
the equivalence relation between two VMs is formalized by Equa-
tion 12.
∃vj , vk ∈ V | vj ≡ vk ⇔Rp(vj) = Rp(vk) ∧
Rm(vj) = Rm(vk) ∧
host(vj) = host(vk) (12)
Entropy dynamically estimates the cost of the plan associated
with the configuration being constructed based on information
about the VMs that have already been assigned to a node. Then,
Entropy estimates a minimum cost for the complete future recon-
figuration plan. For each VM that has not yet been assigned to
a node, the solver looks at VMs that can not be hosted by their
current node and increases the cost by these future migrations. Fi-
nally, the solver determines whether the future configuration based
on this partial assignment might improve the solution or will nec-
essarily be worse. In the latter situation, the solver abandons the
configuration currently being constructed and searches for another
assignment.
5. Evaluation
Entropy uses constraint programming in order to find a better re-
configuration plan than that found using locally optimal heuristics.
Nevertheless, the more exhaustive search performed by constraint
programming is only justified if it leads to a better solution within
a reasonable amount of time. In this section, we evaluate the scala-
bility of Entropy for simulation data of a range of complexities and
compare the benefit that Entropy can provide. We then use Entropy
on a cluster in the Grid’5000 experimental testbed on a collection
of programs from the NASGrid benchmark suite [6].
5.1 Scalability of Entropy
The complexity of a VMPP or VMRP depends on the number of
nodes, the number of classes of equivalence and the ratio of VMs
to nodes. In this evaluation, we have solved VMPPs and VMRPs
of various complexities to observe the scalability of our reconfig-
uration algorithm. For this, we have generated several sets, each
composed of 100 randomly generated configurations that share the
same basic properties: Each VM needs zero or one processing units,
depending on its state, and nodes each have two processing units
and 3GB of memory.
The dedicated node that executes the reconfiguration algorithm
has an AMD Opteron 2.0GHz CPU and 4GB of RAM. The recon-
figuration algorithm is implemented in Java and runs on the stan-
dard Sun Java 1.5 virtual machine.
Impact of equivalence classes Sections 3.3 and 4.3 presented the
notion of classes of equivalence. The number of classes of equiva-
lence for the VMs equals the number of different amounts of mem-
ory requirements per the number of possible CPU states (2 differ-
ent states in our case, as a VM may be either active or inactive).
To observe this impact on the duration of solving process, we have
generated 3 sets of configurations composed of 200 VMs and 200
nodes which differ in the number of classes of equivalence. The
set 2e is composed of configurations with 2 classes of equivalence,
where each VM requires 1GB of memory. The set 4e (4 classes of
equivalence) uses VMs that each require 1GB or 2GB of memory.
The set 8e (8 classes of equivalence) uses VMs that each require
512M, 1GB, 1.5GB or 2GB of memory.
 0
 20
 40
 60
 80
 100
 0  5  10  15  20  25  30
%
 o
f 
m
in
im
iz
e
d
 c
o
n
fi
g
u
ra
ti
o
n
s
Computation time in sec.
2e
4e
8e
(a) VMPP
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60
%
 o
f 
m
in
im
iz
e
d
 c
o
n
fi
g
u
ra
ti
o
n
s
Computation time in sec.
2e
4e
8e
(b) VMRP
Figure 10. Impact of the number of classes of equivalence on the
solving process
Figure 10(a) shows the percentage of problems in each class for
which the minimum number of nodes is determined within varying
amounts of time. We observe that the necessary computation time
for solving the VMPP is affected by the number of classes of equiv-
alence. 6 seconds are necessary to solve 100% of the configurations
of the set 2e, 7 seconds for the set 4e and 11 seconds for the set 8e.
Figure 10(b) shows the percentage of problems in each class
for which a plan with minimum cost, K, is determined within
varying amounts of time. Because of the high cost of creating
and evaluating the reconfiguration plans, the solver is never able
to prove that a configuration has the smallest reconfiguration plan
in the time allotted. Thus, we consider a solution to be minimal
until one with a 10% lower reconfiguration cost is computed.1 The
graph denotes the percentage of solutions where the reconfiguration
cost associated with the computed configuration is minimal, over
time. We observe that the VMRP is also impacted by the number
of classes of equivalence. 5 seconds are necessary to solve 80% of
the configurations in 4e while 8 seconds are necessary for 8e.
Based on these results, we observe that the solving time has to
be increased when the number of classes of equivalence is high.
Indeed, pruning the search tree is less effective when there are
few elements in the same class. However, this behavior is less
significant for VMRP as the definition of the equivalence between
two VMs is more strict than for VMPP and tends by definition to
reduce the number of elements per class.
Impact of the ratio of VMs to nodes We have generated 3 sets of
configurations, each with 4 classes of equivalence. All the sets use
200 nodes but they differ in the number of VMs. The set denoted
as 200/200 uses 200 VMs. The set 300/200 uses 300 VMs. The
set 400/200, with the highest number of VMs per node uses 400
VMs.
Figures 11(a) and 11(b) show the percentage of problems for
which a solution for the VMPP and VMRP, respectively,is deter-
mined within varying amounts of time. We observe that the ratio
of VMs per node has a significant impact on the solving process.
For the VMPP, 5 seconds are necessary to solve 80% of the set200/200, 15 seconds to solve 80% of the set 300/200 and 29 sec-
onds to solve 80% of the set 400/200. For the VMRP, solving 80%
of the configurations in the set 200/200 requires 8 seconds. 16 sec-
onds are necessary for the set 300/200 and 21 for set 400/200.
1We use the threshold of 10% in this figure to account for the fact that the
reconfiguration cost function only provides an estimate.
Thus, increasing the ratio of VMs per node increases the com-
plexity of the problem. Indeed, with a high ratio of VMs per node,
computing a solution is more difficult as the set of the possible hosts
for a VM is reduced. Moreover, as the reconfiguration problem be-
comes more complex, the number of failures also increases. There
are more ordering issues and a pivot is not always available to re-
solve inter-dependant migrations. Thus, more time is necessary to
compute a new configuration that is reachable by a reconfiguration
plan.
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60
%
 o
f 
m
in
im
iz
e
d
 c
o
n
fi
g
u
ra
ti
o
n
s
Computation time in sec.
200/200
300/200
400/200
(a) VMPP
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60
%
 o
f 
m
in
im
iz
e
d
 c
o
n
fi
g
u
ra
ti
o
n
s
Computation time in sec.
200/200
300/200
400/200
(b) VMRP
Figure 11. Impact of the ratio of VMs per node on the solving
process
5.2 Comparison against FFD
The VMPP includes the number of nodes in the configuration
identified by the FFD heuristic as an initial upper bound, and thus
its solution will never use more nodes than the solution of the FFD.
This evaluation assesses the gain provided by Entropy in terms of
the reduction in the number of nodes and the reconfiguration cost.
The comparison is based on 4 sets of configurations. They differ in
the number of nodes or the ratio of VMs to nodes. We use timeouts
of 1 minute for both the VMPP and the VMRP.
Figure 12(a) quantify the improvement of Entropy as compared
as the packing provided with FFD. Entropy find a better packing
by up to 5 fewer nodes for 42% of the configurations. Figure 12(b)
shows the effectiveness of the reduction of K in Entropy by com-
paring the reconfiguration cost for each plan computed with En-
tropy and FFD. However, to provide a fair comparison, we only
consider the configurations where Entropy and FFD compute so-
lutions that use the same number of nodes. Entropy reduces the
reconfiguration cost by at least 96% for 80% of the configurations.
This difference comes because FFD stops after the first complete
assignment of the VMs, while Entropy continues to compute a bet-
ter solution until it times out or proves the optimality of the current
one.
 0
 20
 40
 60
 80
 100
100/100
200/200
300/200
400/200
%
 o
f 
to
ta
l
VMs/nodes
Equiv.
1 node
2 nodes
3 nodes
4 nodes
5 nodes
(a) VMPP
 0
 20
 40
 60
 80
 100
100/100
200/200
300/200
400/200
%
 o
f 
to
ta
l
VMs/nodes
90-94%
96-99%
(b) VMRP
Figure 12. Improvment as compared to FFD
5.3 Experiments on a cluster
We now apply Entropy on a real cluster composed of 39 nodes,
each with a AMD Opteron 2.0 GHz CPU and 2GB of RAM. One
(a) ED (b) HC (c) VP
Figure 13. Computation graphs of NASGrid Benchmarks
00:00
02:00
04:00
06:00
08:00
10:00
12:00
14:00
 0  50000  100000  150000  200000  250000  300000  350000  400000
R
e
c
o
n
fi
g
u
ra
ti
o
n
 d
u
ra
ti
o
n
 i
n
 m
in
.
Reconfiguration cost
FFD (9 reconfigurations)
Entropy (18 reconfigurations)
Figure 14. Reconfiguration costs computed by FFD and Entropy
node is dedicated to the reconfiguration engine and three nodes
are used as file servers that provide the disk images for the VMs.
The remaining 35 nodes run the Xen Virtual Machine Monitor with
200MB of RAM dedicated to Xen’s Domain-0. These nodes host
a total of 35 VMs that run benchmarks of the NASGrid bench-
mark suite [6]. This benchmark suite is a collection of synthetic
distributed applications designed to rate the performance and func-
tionalities of computation grids. Each benchmark is organized as a
graph of tasks where each task corresponds to a scientific computa-
tion that is executed on a single VM. Edges in the graph represent
the task ordering. This ordering implies that the number of active
VMs varies during the experiment; there are typically from 10 to
15 active VMs. Entropy, however, is unaware of these task graphs,
and instead relies on the instantaneous descriptions provided by its
sensors to determine which VMs are active and inactive.
The 35 VMs are assigned to the various tasks of the NASGrid
benchmarks ED, HC, and VP, whose computation graphs are shown
in Figure 13. Each set of VMs associated with a given benchmark
has its own NFS file server that contains the VMs’ disk image. The
ED benchmark uses 10 VMs with 512 MB of RAM each. It has one
phase of computation that uses all of its VMs. The HC benchmark
uses 5 VMs with 764 MB of RAM each. This benchmark is fully
sequential and has only one active task at a time. Finally, the
VP benchmark uses 20 VMs, with 512MB of RAM each. This
benchmark has several phases where the number of active VMs
varies. Before starting the experiment, each VM is started in an
inactive state, in an initial configuration computed using Entropy.
This configuration uses 13 nodes. It corresponds to a maximum
packing based on the memory requirements of the VMs. All three
benchmarks are started at the same time. We test the benchmarks
using FFD and Entropy as the reconfiguration algorithm.
Figure 14 shows the estimated cost of each reconfiguration plan
selected using FFD and Entropy and the duration of its execution.
The relationship between the cost and the execution time is roughly
linear, and thus the cost function f is a reasonable indicator of
performance for plans created using both FFD and Entropy. Fur-
thermore, we observe that reconfiguration based on Entropy plans
typically completes much faster than reconfiguration based on FFD
plans. Indeed, the average execution time for plans computed with
FFD is about 6 minutes and 53 seconds while the average execution
time for plans computed with Entropy is only 1 minute and 47 sec-
onds. With short reconfiguration plans, Entropy is able to quickly
react to the changes in the activity of VMs, and thus quickly de-
tects and corrects non-viable configurations. Entropy performs 18
short reconfigurations over the duration of the experiment, while
the FFD-based algorithm performs 9 longer ones.
Figures 15(a) and 15(b) show the activity of VMs while running
the benchmarks with FFD and Entropy, in terms of the number
of active VMs that are satisfied and unsatisfied. Satisfied VMs are
active VMs that have their own processing unit. Unsatisfied VMs
are active VMs that share a processing unit. The average number of
unsatisfied VMs is 1.75 for FFD and 1.05 for Entropy. The number
of unsatisfied VMs is a significant criterion to rate the benefit of
a reconfiguration algorithm. An unsatisfied VM indicates a non-
viable configuration, and thus a performance loss.
(a) FFD (b) Entropy
Figure 15. Activity of VMs
When the benchmarks start, 12 VMs become active at the same
time. Entropy quickly remaps the VMs and obtains a viable con-
figuration by minute 6. FFD, on the other hand, does not reach a
viable configuration until much later. The total number of active
VMs increases at minute 10, thus increasing the number of unsatis-
fied VMs. As Entropy is not in a reconfiguration state at that time, it
computes a new configuration and migrates the VMs accordingly,
to obtain a viable configuration by minute 11. FFD, on the other
hand, is in the midst of migrating VMs at the point of the first peak
of activity, according to a previously computed, and now outdated,
reconfiguration plan. FFD only reaches a viable configuration in
minute 18. In this situation, we consider that an iteration of the re-
configuration process using FFD takes too much time as compared
to the activity of the VMs. Figure 15(b) shows that number of un-
satisfied VMs is always zero after 1 hour. This is due to the unequal
duration of the benchmarks. At minute 50, the benchmark HC ends
its computation. Then the activity of VP changes at minutes 54 and
58 and requires a reconfiguration. For the remaining time, there is
no new phase that makes unsatisfied VMs: The end of the last phase
of VP at 1 hour and 10 minutes does not entail a reconfiguration and
the activity of the last running benchmark, ED, is constant.
The response time of a reconfiguration process measures the du-
ration between the initial presence of unsatisfied VMs and the next
viable configuration. It indicates the capacity of the reconfiguration
process to scale with the activity of VMs. For this experiment, the
average response time for FFD is 248 seconds. For Entropy, the
average response time is 142 seconds.
Figure 16 shows the number of nodes used to host VMs. Re-
configuration plans computed with FFD require more migrations
and thus tend to require more pivot nodes. For this experiment, the
 10
 12
 14
 16
 18
 20
00:00
00:10
00:20
00:30
00:40
00:50
01:00
01:10
01:20
01:30
01:40
U
s
e
d
 n
o
d
e
s
Time (hours)
FFD
Entropy
Figure 16. Number of nodes used with FFD and Entropy
reconfiguration process based on FFD requires up to 4 additional
pivot nodes. This situation is particularly unfortunate when consol-
idation is used to save energy, by powering down unused nodes,
as nodes have to be turned on just to perform migrations. Entropy,
which creates smaller plans, requires at most one additional pivot
nodes, and thus provides a environment more favorable to the shut-
ting down of unused nodes.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
ED VP HC
R
u
n
ti
m
e
 i
n
 m
in
.
FFD
Entropy
without consolidation
Figure 17. Runtime Comparison
Dynamic consolidation typically incurs a performance loss due
to the migration time and the possibility of introducing usatisfied
VMs. Byminimizing the duration of non-viable configurations, En-
tropy reduces the performance loss due to dynamic consolidation.
Figure 17 shows the runtime of each benchmark for FFD, Entropy
and for an environment without any consolidation. In the latter sit-
uation, each VM is definitively assigned to its own node to avoid
performance loss due to the sharing of processing units. In this con-
text, 35 nodes are required. The global overhead for all benchmarks
compared to a execution without consolidation is 19.2% for FFD.
Entropy reduces this overhead to 11.5%.
We can summarize the resource usage of the various bench-
marks in terms of the number of nodes used per hour. Without any
consolidation, running the benchmarks consumes 53.01 nodes per
hour. Consolidation using FFD reduces this consumption to 24.53
nodes per hour. Consolidation using Entropy further reduces this
consumption to 23.21 nodes per hour. However, these numbers are
affected by the duration of each benchmark. When all benchmarks
are running, the consolidation only comes from the reconfiguration
engine that dynamically mixes inactive VMs with active VMs in
the different phases of the applications. When a benchmark stops, it
creates zombie VMs that still require memory resources but should
be turned off. Thus, to estimate the consumption that only results
from mixing inactive and active non-zombie VMs, we consider the
consumption until the end of the first benchmark to complete, HC.
In this situation, running the three benchmarks without consolida-
tion consumes 24.31 nodes per hour, with FFD consumes 15.34
nodes per hour, and with Entropy consumes only 11.72 nodes per
hour.
6. Related work
Power-Aware VM replacement Nathuji et al. [12] present power
efficient mechanisms to control and coordinate the effects of vari-
ous power management policies. This includes the packing of VMs
through live migration. They later extended their work to focus on
the tradeoff between the Service Level Agreements of the applica-
tions embedded in the VMs and the necessity to satisfy hardware
power constraints [13]. Entropy addresses the reconfiguration is-
sues brought by the live migration of VMs in a cluster and provides
a solution to pack VMs in terms of their requirements for process-
ing units and memory, while minimizing the duration of the recon-
figuration process and its impact on performance.
Verma et al. [17] propose an algorithm that packs VMs accord-
ing to their CPU needs while minimizing the number of migra-
tions. This algorithm is an extension of the FFD heuristic and mi-
grates VMs located on overloaded nodes to under-exploited nodes.
Restricting migrations to only those from overloaded nodes to un-
derloaded nodes has the effect that all selected migrations are di-
rectly feasible; the sequential and cyclic constraints that we have
identified in Section 4 cannot arise. Nevertheless, this implies that
the approach may fail to compute a new viable configuration or
miss opportunities for savings when rearranging the VMs within
the underloaded nodes is essential to reach a viable configuration
or enable more beneficial migrations. Entropy exploits a larger set
of possible VM migrations, and thus can solve the more complex
reconfiguration problems that can occur in a highly loaded environ-
ment.
Performance Management through replacement Khanna et
al. [11] propose a dynamic reconfiguration algorithm that assigns
each VM to a node in order to minimize the unused portion of re-
sources. VMs with high resource requirements are migrated first.
Bobroff et al. [3] base their replacement engine on a forecast ser-
vice that predicts, for the next forecast interval, the resource de-
mands of VMs, according to their history. Then the replacement
algorithm, which is based on an FFD heuristic, selects a node than
can host the VM during this time interval. To ensure efficiency,
the forecast window takes into account the duration of the recon-
figuration process. However, this assignment does not consider
sequential and cyclic constraints, which impact the feasibility of
the reconfiguration process and its duration.
VMs replacement issues Grit et al. [7] consider some VMs re-
placement issues for resource management policies in the context
of Shirako [9], a system for on-demand leasing of shared networked
resources in federated clusters. When a migration is not directly
feasible, due to sequence issues, the VM is paused using suspend-
to-disk. Once the destination node is available for migration, the
VM is resumed on it. Entropy only uses live migrations in order to
prevent failures in the VMs due to suspending part of a distributed
application.
Sandpiper [18] is a reconfiguration engine, based on an FFD
heuristic, to relocate VMs from overloaded to under-utilized nodes.
When a migration between two nodes is not directly feasible, the
system identifies a set of VMs to interchange in order to free a
sufficient amount of resources on the destination node. Then the
sequence of migrations is executed. This approach is able to solve
simple replacement issues but requires some space for temporar-
ily hosting VMs on either the source or the destination node. By
identifying pivot nodes and bypass migrations, Entropy can resolve
cycles without performing multiple interchange operations that in-
crease the number of migrations thus the duration of the reconfigu-
ration process.
7. Conclusion and Future Work
Previous work has rejected the use of constraints in implementing
dynamic consolidation as being too expensive due to the duration
of the solving time. In this paper, we have shown that the overhead
of consolidation is determined not only the time required to choose
a new configuration, but also by the time required to migrate VMs
to that configuration. Our constraint-programming based approach,
which explicitly takes into account the cost of the migration plan,
can indeed reduce the number of nodes and the migration time
significantly, as compared to results obtained with the previously
used FFD heuristic. We have implemented this approach in our
consolidation manager Entropy, and shown that it can reduce the
consumption of cluster nodes per hour for a collection of NASGrid
benchmarks by over 50% as compared to static allocation and by
almost 25% as compared to consolidation using FFD.
The configurations considered in this paper are fairly simple,
because in the clusters available in the Grid’5000 experimental
testbed, every node has only a single processor and all nodes have
the same amount of memory. Our approach, however, is directly ap-
plicable to clusters providing multiprocessors and nodes with non-
homogeneous memory availability, because the number of proces-
sors and the amount of memory available are simply parameters of
the VMPP and VMRP problems. We will extend our results to such
clusters when they become available to us.
In future work, we plan to consider the problem of admission
control for clusters providing consolidation. We expect that simu-
lation results, like those described in Section 5.1, can help to iden-
tify the number of tasks that a cluster providing consolidation can
accept. We also plan to consider the applicability of the approach
to other kinds of software than scientific computations, such as e-
commerce.
Acknowledgments
Experiments presented in this paper were carried out using the
Grid’5000 experimental testbed [4], an initiative from the French
Ministry of Research through the ACI GRID incentive action, IN-
RIA, CNRS and RENATER and other contributing partners.
Availability
The prototype Entropy is available on our webpage:
http://www.emn.fr/x-info/entropy/
References
[1] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho,
R. Neugebauer, I. Pratt, and A. Warfield. Xen and the art of
virtualization. In Proceedings of the 19th ACM Symposium on
Operating Systems Principles, pages 164–177, Bolton Landing, NY,
USA, Oct. 2003. ACM Press.
[2] F. Benhamou, N. Jussien, and B. O’Sullivan, editors. Trends in
Constraint Programming. ISTE, London, UK, May 2007.
[3] N. Bobroff, A. Kochut, and K. Beaty. Dynamic placement
of virtual machines for managing SLA violations. Integrated
Network Management, 2007. IM ’07. 10th IFIP/IEEE International
Symposium on, pages 119–128, May 2007.
[4] R. Bolze, F. Cappello, E. Caron, M. Daydé, F. Desprez, E. Jeannot,
Y. Jégou, S. Lantéri, J. Leduc, N. Melab, G. Mornet, R. Namyst,
P. Primet, B. Quetier, O. Richard, E.-G. Talbi, and T. Iréa. Grid’5000:
a large scale and highly reconfigurable experimental grid testbed.
International Journal of High Performance Computing Applications,
20(4):481–494, Nov. 2006.
[5] C. Clark, K. Fraser, S. Hand, J. G. Hansen, E. Jul, C. Limpach, I. Pratt,
and A. Warfield. Live migration of virtual machines. In Proceedings
of the 2nd ACM/USENIX Symposium on Networked Systems Design
and Implementation (NSDI ’05), pages 273–286, Boston, MA, USA,
May 2005.
[6] M. Frumkin and R. F. V. der Wijngaart. NAS grid benchmarks: A tool
for grid space exploration. Cluster Computing, 5(3):247–255, 2002.
[7] L. Grit, D. Irwin, A. Yumerefendi, and J. Chase. Virtual machine
hosting for networked clusters: Building the foundations for "auto-
nomic" orchestration. In Virtualization Technology in Distributed
Computing, 2006. VTDC 2006. First International Workshop on,
pages 1–8, Nov. 2006.
[8] R. Haralick and G. Elliott. Increasing tree search efficiency for
constraint satisfaction problems. Artificial Intelligence, 14(3):263–
313, October 1980.
[9] D. Irwin, J. Chase, L. Grit, A. Yumerefendi, D. Becker, and K. G.
Yocum. Sharing networked resources with brokered leases. In
ATEC ’06: Proceedings of the annual conference on USENIX ’06
Annual Technical Conference, pages 18–18, Berkeley, CA, USA,
2006. USENIX Association.
[10] N. Jussien, G. Rochart, and X. Lorca. The CHOCO constraint
programming solver. In CPAIOR’08 workshop on Open-Source
Software for Integer and Contraint Programming (OSSICP’08), Paris,
France, June 2008.
[11] G. Khanna, K. Beaty, G. Kar, and A. Kochut. Application
performance management in virtualized server environments.
Network Operations and Management Symposium, 2006. NOMS
2006. 10th IEEE/IFIP, pages 373–381, 2006.
[12] R. Nathuji and K. Schwan. VirtualPower: Coordinated power
management in virtualizaed entreprise systems. In 21st Symposium
on Operating Systems Principles (SOSP), Oct. 2007.
[13] R. Nathuji and K. Schwan. VPM tokens: virtual machine-aware
power budgeting in datacenters. In HPDC ’08: Proceedings of
the 17th international symposium on High performance distributed
computing, pages 119–128, New York, NY, USA, 2008. ACM.
[14] F. Rossi, P. van Beek, and T. Walsh. Handbook of Constraint
Programming (Foundations of Artificial Intelligence). Elsevier
Science Inc., New York, NY, USA, 2006.
[15] P. Shaw. A constraint for bin packing. In Principles and Practice of
Constraint Programming (CP’04), volume 3258 of Lecture Notes in
Computer Science, pages 648–662. Springer, 2004.
[16] M. Trick. A dynamic programming approach for consistency and
propagation for knapsack constraints. In Proceedings of the Third
International Workshop on Integration of AI and OR Techniques in
Constraint Programming for Combinatorial Optimization Problems
(CPAIOR-01), pages 113–124, 2001.
[17] A. Verma, P. Ahuja, and A. Neogi. Power-aware dynamic placement
of hpc applications. In ICS ’08: Proceedings of the 22nd annual
international conference on Supercomputing, pages 175–184, New
York, NY, USA, 2008. ACM.
[18] T.Wood, P. J. Shenoy, A. Venkataramani, and M. S. Yousif. Black-box
and gray-box strategies for virtual machine migration. In Proceedings
of the 4th ACM/USENIX Symposium on Networked Systems Design
and Implementation (NSDI ’07), 2007.

