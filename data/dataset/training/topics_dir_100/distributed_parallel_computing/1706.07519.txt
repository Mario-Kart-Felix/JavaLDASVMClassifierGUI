Interoperable Convergence of Storage,
Networking and Computation
Micah Beck
University of Tennessee
Knoxville, Tennessee 37996
mbeck@utk.edu
Terry Moore
University of Tennessee
Knoxville, Tennessee 37996
tmoore@icl.utk.edu
Piotr Luszczek
University of Tennessee
Knoxville, Tennessee 37996
luszczek@icl.utk.edu
Abstract
In every form of digital store-and-forward communica-
tion, intermediate forwarding nodes are computers, with
attendant memory and processing resources. This has
inevitably given rise to efforts to create a wide area in-
frastructure that goes beyond simple store and forward,
a facility that makes more general and varied use of
the potential of this collection of increasingly powerful
nodes. Historically, efforts in this direction predate the
advent of globally routed packet networking. The desire
for a converged infrastructure of this kind has only in-
tensified over the last 30 years, as memory, storage and
processing resources have both increased in density and
speed and decreased in cost. Although there seems to be
a general consensus that it should be possible to define
and deploy such a dramatically more capable wide area
facility, a great deal of investment in research prototypes
has yet to produce a credible candidate architecture.
Drawing on technical analysis, historical examples, and
case studies, we present a argument for the hypothesis
that in order to realize a distributed system with the kind
of convergent generality and deployment scalability that
might qualify as “future-defining,” we must build it up
from a small set of simple, generic, and limited abstrac-
tions of the low level processing, storage and network
resources of its intermediate nodes.
Keywords network convergence, distributed storage,
distributed processing, active networks, distributed cloud,
edge computing, network layering, scalability
ACM Reference format:
Micah Beck, Terry Moore, and Piotr Luszczek. . Interopera-
ble Convergence of Storage, Networking and Computation.
In Proceedings of , , , 14 pages.
DOI:
,
. .
DOI:
1 Introduction
A variety of technological, economic, and social devel-
opments — most notably the general movement toward
Smart Cities, the Internet of Things, and other forms of
“intelligent infrastructure” [42] — are prompting calls
from various quarters for something that the distributed
systems community has long aspired to create: A next-
generation Internet. For example, the authors of a recent
Computing Community Consortium white paper, writ-
ing with the US “Smart Cities” initiative [45] in view,
express the research challenge as follows:
“What is lacking—and what is neces-
sary to define in the future—is a com-
mon, open, underlying ‘platform’, anal-
ogous to (but much more complex than)
the Internet or Web, allowing applica-
tions and services to be developed as
modular, extensible, interoperable com-
ponents. To achieve the level of in-
teroperation and innovation in Smart
Cities that we have seen in the Inter-
net will require federal investment in
the basic research and development of
an analogous open platform for intel-
ligent infrastructure, tested and eval-
uated openly through the same inclu-
sive, open, consensus-driven approach
that created Internet.” [43] [Emphasis
in source]
The experiences of the last two decades have made
the distributed systems community acutely aware of how
elusive the invention of such a future-defining platform
is likely to be. After all, achieving this vision has been
the explicit or implicit ambition of a succession of well
funded and energetically pursued research and develop-
ment efforts within or around this community, including
Active Networking [53], Grid Computing [35], Plan-
etLab [27], and GENI [41], to name a few. Although
these broad efforts have produced both valuable research
ar
X
iv
:1
70
6.
07
51
9v
1 
 [
cs
.D
C
] 
 2
2 
Ju
n 
20
17
Interoperable Convergence Micah Beck, Terry Moore, and Piotr Luszczek
and useful software results, nothing delivered so far has
achieved the kind of deployment scalability necessary
initiate the kind of viral growth that everyone expects
such an aspirational platform to exhibit. At the same
time, chronic problems with network hotspots were an
early and persistent sign that the Internet’s stateless, uni-
cast datagram service had scalability limitations with
respect to data volume and/or popularity. This fact has
led to of increasingly sophisticated and increasingly ex-
pensive technology “workarounds,” from the FTP mirror
sites and hierarchical Web caches of the early years, to
the content delivery networks (CDN) and commercial
Clouds we see today.
The big idea of this paper is that the appropri-
ate common service on which to base an interoper-
able platform to support distributed systems is an
abstraction of the low layer resources and services
of the intermediate node, i.e., a generalization of the
Internet stack’s layer 2. Drawing on technical analysis,
historical examples, and case studies, in this paper we
argue that in order to build distributed systems with the
kind of interoperability, generality and deployment scal-
ability that might qualify as “future-defining,” we must
implement them using a small set of simple, generic,
and limited abstractions of the data transfer, storage and
processing services available at this layer. In our model,
these abstractions all revolve around the most funda-
mental common resource, namely, the memory/storage
buffer.
2 Background
Given the inclination of computer scientists to add fea-
tures, the fact that every form of digital store-and-forward
communication (including the Internet) has interme-
diate forwarding nodes that are computers, with at-
tendant memory and processing resources, makes at-
tempts to create a wide area infrastructure with services
beyond simple store-and-forward inevitable. Such ef-
forts to make more general use of these increasingly
powerful nodes— a generalized converged network, in
our terminology—predate the advent of globally routed
packet networking (e.g. uux [6]). The exponentially
increasing density and speed, and rapidly decreasing
cost of memory, storage and processing resources over
the past 30 years has only intensified the desire to define
and scalably deploy a converged infrastructure of this
general description; and yet despite the general consen-
sus that it should be possible to do so, this aspiration has
remained unfulfilled.
One problem is that the goal of converged networking
runs in the opposite direction of the traditional archi-
tectural approach of the traditional Internet design com-
munity, which insists that services other than datagram
delivery must be kept out of the Network Layer of the
communication protocol stack. This community main-
tains that the ability of the Internet to function properly
and to continue growing globally depends on keeping
this common service layer ”thin”, in the sense that it pro-
vides services that are simple, generic and limited [52].
From this point of view, services other than datagram
delivery should be implemented in systems connected to
the Internet as communication endpoints. Various ratio-
nales that support this point of view has been collectively
referred to as ”End-to-End Arguments”[52].
Since a router that has substantial system storage (i.e.
other than network buffers) and generalized computa-
tional resources (i.e. other than forwarding) is neither
difficult nor expensive to build, there have been numer-
ous efforts to resist this orthodox point of view. The
simple fact that storage and computational resources
can be provisioned and located throughout the network
at reasonable cost stimulates efforts in this direction.
Moreover, the apparent opportunity to create such a
powerful distributed infrastructure presents a temptation
that is inherently difficult for computer scientists and
engineers to resist. These facts, however, do not make it
a good idea to add extensions to the fundamental service
of the global Internet, nor do they ensure that if it is
built, service creators and users will adopt it in sufficient
numbers to enable economic sustainability beyond the
prototype stage. Indeed, while a number of plausible
network service architectures have been defined that can
provide access to such distributed resources [24, 53], the
widespread deployment of such extended services on a
converged wide area infrastructure has so far not been
achieved.
Perhaps an even more compelling reason for the con-
tinued drive to create such a converged infrastructure
is that some important distributed applications cannot
be efficiently and effectively implemented through de-
composition into two components, one implemented by
a ”thin” datagram delivery service and the other imple-
mented at ”fat” endpoints. For example, some appli-
cations require an implementation that is sensitive to
the location of storage and computation in the network
topology. Point-to-multipoint communication was an
early and obvious example. Using simple repetition
of unicast datagram delivery was viewed as too ineffi-
cient by early Internet architects, but an efficient tree
Submitted ACM/IFIP/USENIX Middleware 2017 2
Interoperable Convergence Micah Beck, Terry Moore, Piotr Luszczek
could be built only through the use of network topology
information. Such low level information was seen as
inappropriate for users of the ”thin” and stable Network
layer to access. Thus, multicast was added to Layer 3,
fattening that thin layer with services that seemed to
address this issue. However, IP multicast has proved
difficult to standardize and has failed to achieve the
universal deployment of ”simple, generic and limited”
unicast IP datagram delivery.
But problems with lack of generality in the interme-
diate nodes were manifest even in highly successful
Internet applications. The early growth of the Internet
was fueled by applications that seemed to fit the uni-
cast datagram delivery model well enough: FTP and
Telnet. Of these, the one-to-many nature of FTP, albeit
asynchronous, created a problem in the distribution of
popular and high-volume files. Ignoring the implications
of topology led to ineffective use of the network, with
hotspots at servers that attracted high volumes of traffic
and unnecessary loads placed on expensive and overbur-
dened wide area links. The result was the creation and
management of collections of FTP mirror sites [31], and
the ubiquitous invitation for users to ”choose a mirror
site near you”, which mean the use of approximate in-
formation about network topology by the end-user, at a
level above even the Internet stack’s Application Layer.
The advent of the World Wide Web exacerbated the
problem of indiscriminate access to servers with no ref-
erence to network topology or even geography. Mirror
sites for file download proliferated, and redundancy in
the storage of all high-traffic Web content became a
necessity. A Network layer ignorant of topology is,
after all, an inherently inadequate platform on which
to build high traffic globally distributed systems. The
need to work around this reality gave rise to automated
Web caching [25, 55] and server replication [15, 37],
which were precursors to modern Content Delivery Net-
works [23, 46].
It should be noted that, although both Web caching
and server replication are obvious examples of the con-
vergence of networking and storage, they also require
computation in the implementation of policy and server-
side processing; and so in fact they represent conver-
gence of all three fundamental computational resources.
Logistical Networking, discussed in Section 7.1, imple-
ments a converged of networking and storage service
that avoids the need for general computation by min-
imizing policy and other server-side processing [16],
but was later extended to include limited server-side
operations [17].
3 The Dialectic of Divergence and
Convergence
The interplay between technological divergence and con-
vergence is a dialectic with a long history. In the area
of computing and communications, there was an early
divergence in the conception and implementation of
several different information technology resources. Be-
cause of the phenomenon of path dependence [28], such
divergence has tended to be self-reinforcing, leading to
a set of familiar technology silos:
• Radio frequency analog audio and video trans-
missions are well adapted to unidirectional one-
to-many communication, and gave rise to tradi-
tional broadcast medium.
• Circuit-based digital and analog audio/video trans-
mission are well adapted to point-to-point com-
munication, and gave rise to traditional telegraph,
telephone and (local area) video networks.
• Tube and transistor amplifiers and switches are
well adapted to the implementation of Boolean
logic, and gave rise to digital computation cir-
cuits and functional units.
• Capacitors and magnetic storage cells are well
adapted to implementation of value persistence,
and gave rise to digital memory and storage sys-
tems.
This early divergence has given rise to conceptual,
technological and organizational silos corresponding to
correspondingly isolated communities. Formal models
and methods of reasoning have been adapted to deal
with the complexity and specific issues of each niche.
For example:
• Boolean logic is a useful tool for modeling the
function of aggregations of transistors (gates)
connected to persistent memory cells (variables).
This has given rise to the development of Boolean
Logic Design as a specialized discipline.
• Circuit-based communication without ”hard state”
is a useful tool for modeling the function of
scaleable, wide area computer-to-computer ser-
vices. This has given rise to the development
of Computer Networking as a specialized disci-
pline.
The development of silos has been an enabling strat-
egy for modeling and optimization of these quickly
evolving technological fields. However, they have also
led to the creation of service stacks, or silos, with highly
specialized services at the top layers (see Figure 1). But
Submitted ACM/IFIP/USENIX Middleware 2017 3
Interoperable Convergence Micah Beck, Terry Moore, and Piotr Luszczek
because the low level resources that these silos encap-
sulate can only be accessed through high level services,
this inevitably tends to create barriers to the natural and
efficient use of constituent low level resources.
The problem with silos as a strategy for dealing with
the complexity and specialization of disparate underly-
ing technologies has become more pronounced due to
the evolution of low level systems toward general mech-
anisms that utilize processors or digital logic controlled
by software, firmware or by hardware designed using
computerized tools. Such generality in low level mecha-
nisms holds out the possibility of the implementation of
highly efficient system architectures, with optimizations
that span traditionally disparate resources. The chal-
lenge is to bridge or eliminate the existing silos, or in
other words, to implement convergence. However, there
are two fundamentally different strategies for achieving
convergence: overlay convergence which combines silos
at a layer above their high level services, and interoper-
able convergence which unifies their foundations. We
discuss these two strategies in turn below.
We use familiar illustrative examples throughout the
remainder of this paper to help the reader understand
the terms being defined. We then use more detailed
case studies to elaborate the implications of applying
the concepts we have introduced.
4 Convergence
We say that a service interface (i.e., an API) is converged
if it gives unified access to multiple low-level resources
(or services) traditionally available only through isolated
service silos.
For example, one of the areas where disparate under-
lying resources have been brought together in common
solid-state implementation technologies and under inter-
operable digital control is the CPU core, system com-
munication buses and memory subsystems of modern
computers, which are not fully siloed.
• A simple example of the convergence of storage
and processing is in the auto-increment register.
• Vector processing represents the convergence in
the control of registers, processor buses and func-
tional units.
• DMA disk operations can require the coordina-
tion of magnetic storage involving complex con-
troller hardware and firmware, system buses with
autonomous control logic, processor buses and
memory subsystems and synchronization with
operating system drivers through interrupts.
N
et
w
or
k
St
or
ag
e
Co
m
pu
te
Overlay	
Convergence
Figure 1. Overlay Convergence of Legacy Silos
A common approach to convergence among non-interoperable
resources is to create a high level interface that provides
access to a number of traditionally separate service silos.
We term this approach overlay convergence because it
typically involves the creation of an overlay service that
accesses the existing service stacks through their high
level client interfaces (see Figure 1).
By contrast, at the other end of the convergence spec-
trum is what we call interoperable convergence, which
is discussed at some length in Section 5. Interoperable
convergence allows access to the underlying common re-
sources without imposing the overhead and restrictions
that are associated with complex and specialized service
silos. Some examples that fall along this continuum are
give below:
• The BSD kernel created an overlay convergence
of Unix process and local file management with
local and wide area networking through the ad-
dition of the socket related system calls. While
some file system calls such as read and write
were extended to operate on sockets, the level
of integration was a convenience that did not ex-
tend deeply into integration of common functions
such as buffer management.
• A distributed file system converges storage and
data movement in a more interoperable manner.
These resources are traditionally are available
through local file management and networked
file transfer tools.
• A database system that allows a filter to be up-
loaded (perhaps as JVM byte-code) and applied
to the contents of a relation in-situ converges stor-
age and computation. This also represents a more
interoperable form of convergence.
Submitted ACM/IFIP/USENIX Middleware 2017 4
Interoperable Convergence Micah Beck, Terry Moore, Piotr Luszczek
4.1 Case Study: Broadcast Media, Telephony and
Internet
Broadcast media (radio and television) have their tech-
nological roots in the propagation properties of radio fre-
quency (RF) waves. A wave emanating from an antenna
spreads in all spatial directions, becoming attenuated as
the inverse of the squared distance from the source. Gov-
ernmental agencies such as the Federal Communication
Commission use extensive application procedures and
public hearings as a means of contending for allocation
of RF bandwidth at fixed frequencies. This allows a
powerful signal to be received and amplified to a use-
ful level within a reception area, the extent of which
depends on a number of factors including intervening
geography, structures, meteorological and astronomical
conditions and events. Each receiver is independent and
has no impact on others.
Telephony has its technological roots in the propaga-
tion of electrons along a wire. A voltage placed on one
end of a wire will be transmitted throughout the extent
of electrical connectivity, so that a signal encoded as
a varying voltage level can be seen and amplified by
one or more receivers. The broadcast form of simple
telephony is used in local area public address systems,
but its main application is as a component in the cre-
ation of wide area circuits controlled and extended using
switches and intermediate amplifiers. The control of
these intermediate elements is greatly simplified by the
restriction of telephony to point-to-point circuits, which
serves the largest application community. In the context
of wide area point-to-point telephony, propagation to
multiple receivers (conference calling) is implemented
using multiple point-to-point circuits connected at a hub.
Packet networking uses either a broadcast or a circuit
infrastructure to connect digital switches which com-
municate data encoded using the underlying analog sig-
naling mechanism. Switches and repeaters can either
implement ”virtual circuits” or a stateless datagram de-
livery model. As is the case with telephony, resource
allocation and control are greatly simplified through the
implementation of point-to-point communication (uni-
cast). Broadcast can be implemented using repeated
unicast in a hub topology (as with telephony) or it can
be implemented using a more complex but more efficient
tree-structured forwarding scheme.
The generality and scalability of the Internet’s data-
gram delivery model has given rise to the idea of using
it to implement the convergence of broadcast, telephony
and data services. The emergence of unicast datagram
delivery as the only universal Internet service (discussed
SERVERPROPRIETARY
NETWORK
Content Delivery Network
SERVER
Cloud
IP Multicast
ROUTER
SOURCE
DESTINATION
Web Caching
CACHE
Figure 2. Overlay Workarounds Addressing Point-to-
Multipoint Distribution
in Section 2) has meant that the underlying capabilities
of analog connectivity mechanisms to implement true
broadcast and to provide quality of service guarantees
through resource reservation are not accessible to Voice
over IP and Streaming Media over IP protocols. In spite
of such limitations, the convenience and cost benefits
of convergence workarounds continue to dominate the
commercial development of these services.
4.1.1 Case Study: Web Caching and CDNs
The absence of a universal point-to-multipoint commu-
nication mechanism within the common Network layer
of the Internet has generated a whole series of overlay
workarounds (see Figure 2). For instance, the distribu-
tion of static Web pages (those that require only min-
imal rewriting of stored HTML pages) can be viewed
as a form of point-to-multipoint application. A browser
cache uses moderate storage resources in the network
endpoint to capture the delivered Web page and asso-
ciated metadata and minimal processing to implement
the cache policy and mechanism. A proxy cache uses
larger scale storage and has a greater processing load,
which is supplied by a substantially provisioned net-
work intermediate node. The convergence of resources
in Web caches led to an architectural development in
which application-specific proxies are uploaded to the a
Submitted ACM/IFIP/USENIX Middleware 2017 5
Interoperable Convergence Micah Beck, Terry Moore, and Piotr Luszczek
”middlebox” platform which implements both caching
and general processing.
An alternative approach is to start from the source,
and to replicate the functionality of the Web server on
multiple network nodes. Manual procedures for FTP
mirroring led to automated mechanisms like Netlib [31],
and high traffic Web sites gave rise to sophisticated
cluster and geographically distributed server replication
schemes [15, 37]. Modern Content Delivery Networks
use a combination of server side caching, distributed
file and database systems and complex streaming and
synchronization protocols implemented on proprietary
international networks of application-specific servers.
As Figure 2 suggests, the Content Delivery Network
industry is based on workarounds. Commercial CDNs
even make use of lower layer Internet mechanisms through
now-commonplace layering violations (such as topology-
sensitive DNS resolution). CDNs are thus a kind of
Chimera, patched together from proprietary components
and standard, low level components of the Internet.
5 Interoperable Convergence
We say that a converged platform is interoperable if
it minimizes the imposition of unnecessary high-level
structure or performance costs when applying of differ-
ent low-level services.
5.1 Examples
• The POSIX kernel interface supports both net-
working and file storage. However, in order to
move data stored in a file to a TCP stream, it was
originally necessary to move it into a process’
address space using the read() system call and
then inject it into the TCP stream using send().
A more interoperable approach is a combined
sendfile() system call was added to Linux
that allows data to be transferred from storage
into a kernel memory buffer and from there di-
rectly to the network without moving it to process
memory or using a dedicated network data buffer.
• A database system can store a set of tuples with-
out order, but traditional data movement tools
operate on files. Thus, it is necessary to serial-
ize a set of tuples as a file in order to send it
to a remote database system. The file is trans-
ferred serially, using TCP with retransmission to
keep the serialized data in order. A somewhat
interoperable approach would generate the serial-
ized stream representing the tuple set on demand,
rather than creating and storing it as a complete
file. A more interoperable approach would be to
implement a specialized protocol that takes ad-
vantage of the lack of natural sequentiality in the
tuple set to perform retransmission out-of-order.
This might require additional work to ensure that
the new protocol was “TCP-friendly” when used
in public shared networks.
• A data analysis system (such as MapReduce [29])
traditionally consists of a deep data store and a
dedicated compute resource such as a cluster or a
shared-memory parallel computer. Visualization
requires data to be moved from the data store to
the compute resource which produces its results
to the data store. User access then requires that
the visualization output be moved to and inter-
preted by a human interaction system. A more in-
teroperable approach would allow computations
to be applied to the data in the data store (in-situ),
and for the user to interact with the results of that
computation directly as it occurs.
5.2 Case Study: Web Caching and CDNs
Web caching played a pivotal role in the expansion of
the Web as a global data distribution service during
the period when intercontinental data links were too
expensive to allow unfettered access by academics. A
hierarchical system of large scale caches was developed
and deployed in US Research and Education Networks
[25, 55] and use of national caches to access Web data
across intercontinental links was made mandatory in
many countries including the UK [44].
In spite of its effectiveness in reducing the traffic loads
due to delivery of static Web pages, the popularity of
intermediate caches has waned dramatically in the past
decade. There are several reasons for this trend includ-
ing:
• The correctness of Web caching relies on lifetime
metadata being provided by origin servers which
is often missing or inaccurate.
• The growth of dynamic Web applications means
that many Web objects are not cachable.
• The lack of an accurate and universal mecha-
nisms for reporting views interferes with the dom-
inant business model of Web advertisers.
• Reliance on a complex cache infrastructure de-
creases the control of the implementer of a Web
service over the Quality of Service experienced
by customers.
Submitted ACM/IFIP/USENIX Middleware 2017 6
Interoperable Convergence Micah Beck, Terry Moore, Piotr Luszczek
STORAGE BLOCK
FILE BUFFER
READ SEND
STORELOAD
PROCESS
MEMORY
SOCKET
BUFFER
COMMON
BUFFER
NETWORK BUFFER STORAGE BLOCK
STORELOAD
NETWORK BUFFER
Figure 3. Read-Send vs. Sendfile
Many of these factors stem from the implementation
of Web caching on top of the HTTP application proto-
col, albeit with some modifiations having been made to
increase control over intermediate and browser caches
by origin Web servers. Cache networks are an overlay
which accesses Web services from the top of the protocol
stack and thus does not allow the degree of fine-grained
control that is required for seamless convergence.
Content Delivery Networks have approached the prob-
lem in a different way, using HTTP and streaming pro-
tocols for client access almost unchanged. This is analo-
gous to the way that online services (e.g. Compuserve
and AOL) and ISPs used telephone services. CDNs have
instead focused their innovation on the underpinnings
of the Internet in order to improve the effectiveness and
their control over server replication.
Content Delivery Network Web and DNS servers ap-
pear to be implemented at the application level, but they
use knowledge of network topology and other low level
information that is intended to be encapsulated within
the Network Layer of the Internet architecture. Modern
extensions to the Network Layer may allow this to be
implemented without violation of layering, at the ex-
pense of creating a ”fatter” and less generic Network
layer (see Section 6).
One way of looking at the growth of CDNs is that
they are creating a proprietary, specialized network with
their own services as the spanning layer, using the Inter-
net as tools in their implementation and as a means of
reaching end users. This view is supported by the trend
toward using private or non-scalable mechanisms to im-
plement internal communication among centralized and
distributed CDN nodes.
5.3 Case Study: Edge Content Delivery
An extreme form of content delivery moves storage and
processing resources to a server located within the edge
network, either having a dedicated connection or be-
ing topologically very near to the end user interface.
This approach has been long been pursued in consumer
entertainment, with strategies have ranging from storage-
intensive (eg TiVo, PVRs and Boxee) to near-stateless
streaming (eg Roku, Smart TV) with high end offerings
incorporating both (eg Apple TV, Multimedia PCs).
Efforts to use intermittently or marginally connected
servers to overcome backbone connectivity challenges
in rural and other isolated areas date back over twenty
years [13, 47] and continue as approaches to reaching
schools and communities in the developing world, in-
cluding nonprofit projects OuterNet, Kolibri, Critical
Links (C3), and Libraries without Borders (Koombox)
and companies such as BRCK.
5.4 Case Study: In-locus Data Analysis
Data Analytics (DA) has emerged as a new paradigm
of understanding unreliably varying environments. It
goes beyond logging, reporting, and thresholding to per-
form meaningful analysis of large scale data sources that
are networked through dynamic and distributed infras-
tructure. DA is capable of extracting latent knowledge
and providing insight from field sensors, computational
units, and large mobile networks. At the same time, the
number of these data sources and the resulting ingest
rate are growing dramatically with increased end-point
hardware integration and hybridization. This requires
new algorithmic approaches across the network, I/O,
and computational software stacks that are low-overhead
and provide non-trivial data metrics. The emerging field
of approximate and/or randomized algorithms position
themselves perfectly in this role as they combine new
methods for matrix approximation via random sampling
that have recently been developed by the Applied Math-
ematics and Machine Learning communities.
Submitted ACM/IFIP/USENIX Middleware 2017 7
Interoperable Convergence Micah Beck, Terry Moore, and Piotr Luszczek
Due to the recent interest [8, 32] in randomized and
approximate algorithms, such methods have become a
much better fit in an inherently unstable and constantly
changing distributed environments by attaching a prob-
abilistic measure to the result. In fact, there are many
statistical techniques in the Randomized Linear Alge-
bra class of algorithms that lend themselves perfectly
to accommodate the convergence principles of in-locus
computing (as manifest in IBP’s best effort Network
Function Unit operations as discussed in Section 7) and
respond algorithmically to assimilate the inherent fail-
ures that naturally occur in a widely distributed system
at the scale that we target. The iterative nature of most
approximate methods allows us to incorporate erroneous
response from a sensor or a network transmission and
gradually remove the malformed data from the multidi-
mensional subspace that is being worked on. Similarly,
an intermittent lack of response from a sensor or a net-
work element may naturally be incorporated as a sam-
pling and selection operator that is triggered by a system-
reported event as opposed to the classical method that
uses a pseudo random number generator (PRNG). Also,
the probabilistic nature of the approximate algorithms
allows us to weigh the data sources based on their his-
tory of reliable responses and the quality of the data
they delivered (if a measure of quality can be obtained,
from, for example, a duplicate sensor). High quality
sensors and network connections will, over time, gain
large weights and thus render them highly probable to
be approximately correct as envisioned by the Probably
Approximately Correct (PAC) learning framework [54].
6 Deployment Scalability
We define deployment scalability as widespread accep-
tance, implementation and use of a service specification.
All the workarounds we have described are building
overlay converged network but they are not interopera-
ble and cannot achieve deployment scalability.
In [10], Beck makes an argument for a fundamental
design principle underlying systems that exhibit deploy-
ment scalability:
The Deployment Scalability Tradeoff There
is an inherent tradeoff between the deploy-
ment scalability of a specification and the
degree to which that specification is weak,
simple, general and resource limited.
The terms ”simple, generic and resource limited” are
derived from the classic paper ”End-to-End Arguments
in System Design” by Saltzer, Reed and Clark which
discusses them in the context of Internet architecture.
The term ”weak” refers to logical weakness of the ser-
vice specification as a theory of program logic, and is
due to Beck’s partial formalization of the arguments
in that paper. Stating this principle as a tradeoff is a
further refinement by Beck of the usual (and perhaps
more accurate) interpretation of the original paper as an
absolute rule requiring or prohibiting particular design
choices [5].
6.1 Case Study: Fault Detection in TCP/IP
The classic example of the application of the End-to-
End Principle, from which its name is derived, is the
location of the detection of data corruption or packet
loss or reordering in the TCP/IP stack [52].
The scalability argument for end-to-end detection of
faults is that removing such functions from the spanning
layer makes it weaker, and therefore potentially admits
more possible implementations. Because fault detection
can be implemented above the spanning layer, the set of
applications supported is not reduced.
6.2 Case Study: Process Creation in Unix
In early operating systems it was common for the cre-
ation of a new process to be a privileged operation that
could be invoked only from code running with supervi-
sory privileges. There were multiple reasons for such
caution, but one was that the power to allocate operating
system resources that comprisee a new process was seen
as too great to be delegated to the application level. An-
other reason was that the power of process creation (for
example changing the identity under which the newly
created process would run) was seen as too dangerous.
This led to a situation in which command line interpre-
tation was a near-immutable function of the operating
system that could only be changed by the installation of
new supervisory code modules, often a privilege open
only to the vendor or system administrator.
In Unix, process creation was reduced to the fork()
operation, a logically much weaker operation that did
not allow any of the attributes of the child process to be
determined by the parent, but instead required that the
child inherit such attributes from the parent [51]. Opera-
tions that changed sensitive properties of a process were
factored out into orthogonal calls such as chown() and
nice(), which were fully or partially restricted to op-
erating in supervisory mode; and exec() which was
not so restricted but which was later extended with prop-
erties such as the setuid bit that were implemented as
authenticated or protected features of the file system.
Submitted ACM/IFIP/USENIX Middleware 2017 8
Interoperable Convergence Micah Beck, Terry Moore, Piotr Luszczek
The decision was made to allow the allocation of kernel
resources by applications, leaving open the possibility of
dynamic management of such allocation by the kernel
at runtime, and creating the possibility of “Denial of
Service” type attacks that persists to this day.
The result of this design was not only the ability to im-
plement a variety of different command line interpreters
as non-privileged user processes, leading to innovations
and the introduction of powerful new language features,
but also the flexible use of fork() as a tool in the de-
sign of multitasking applications. This design approach
has led to the adaptation of Unix-like kernels to highly
varied user interfaces (such as mobile devices) that were
not within the original Unix design space.
7 Exposed Buffer Processing
The core resource that is used by all forms of storage,
networking and computing is the persistent memory or
storage buffer.
• In storage, storage blocks or objects are used in
the implementation of higher level file and data-
base systems, along with RAM memory buffers
that are used to improve performance, enable ap-
plication/OS parallelism and allow for flexible
exchange of data with other operating system
data structures.
• In networking, buffers are used at the endpoints
for much the same reasons as storage, and are
used at intermediate nodes to allow for asyn-
chrony in the operation of store-and-forward packet
networking.
• In computing, memory pages make up process
address spaces, are also used to enable asyn-
chrony in interprocess communication, and hold
all other operating system data structures used
in the implementation of functions on behalf of
processes.
While operating system interfaces such as POSIX
provide access to storage, networking and computing
services, they do so in ways that conform to these tradi-
tional silos.
• File system calls do not have explicit access to
general networking or computation resources.
• The sockets interface does not provide access to
general storage and or computation resources.
• The POSIX process management functions do
have only the minimal necessary overlap with
storage and network functions (notably speci-
fying an executable file image in the exec()
system call).
Convergence of storage, networking and computation
is possible through conventional operating system in-
terfaces using the generality of the user process as a
gateway between these silos. However, a more inter-
operable approach is to expose a common abstraction
of the underlying resource that all of these high level
silos operate on, namely persistent storage blocks or
memory buffers, an approach to convergence that we
call Exposed Buffer Processing.
7.1 Logistical Networking as EBP in Overlay
Over the past 15 years the Logistical Networking project [16,
17, 49] has worked to define an approach to Exposed
Buffer Processing that is implemented as an overlay on
the Internet. An examination of the key elements of that
implementation provides an EBP proof of concept.
7.1.1 Components of Logistical Networking
The Internet Backplane Protocol (IBP) IBP is a
generalized Internet-based storage service that
is encapsulated as remote procedure call over
TCP. IBP was designed to be simple, generic and
limited following the example of the Internet Pro-
tocol (IP) [52]. It is a best effort service, its byte
array allocations are named only by long random
keys (capabilities) and represent leases whose
duration and size are limited by the individual
intermediate node (in analogy to the IP MTU).
The intermediate node that implements IBP is
called a depot, and it is intended as a storage ana-
log to IP routers. In many ways IBP is closer to
a network implementation of malloc() than
a conventional Internet storage service like FTP,
and in addition every IBP allocation is a lease of
storage resources which can be limited in dura-
tion.
The exNode Because IBP is such a limited service,
the abstraction of an allocation that it supports
does not have the expressiveness of the file ab-
straction that users typically expect of a high
level data management system. The exNode is
an abstract data structure that holds the structural
metadata required to compose IBP allocations
into a file of very large extent, with replication
across IBP depots, identified by their DNS name
or IP address [11]. The exNode can be thought
of as an analog to the inode used in early Unix
Submitted ACM/IFIP/USENIX Middleware 2017 9
Interoperable Convergence Micah Beck, Terry Moore, and Piotr Luszczek
file system implementations. The exNode has a
standard XML sequentialization.
The Logistical Runtime System (LoRS) The exN-
ode can be used as a file descriptor to implement
standard file operations such as read and write.
The Logisticsal Runtime System (LoRS) uses
the exNode to implement efficient, robust and
high performing data transfer operations. Some
of the techniques used in the implementation of
LoRS are comparable to those used in parallel
and peer-to-peer protocols [48].
The Logistical Distribution Network (LoDN) While
the exNode implements topological composition
of IBP allocations to implement large distributed
and replicated files, it does not deal with the tem-
poral dimension introduced by IBP’s use of stor-
age leases. LoDN is an active service which
holds exNodes and applies storage allocation,
lease renewal and data movement operations as
required to maintain policy objectives set by end
users through a declarative language and man-
ageable by an intuitive human interface.
The Network Functional Unit (NFU) The NFU
was introduced as a means to allow simple, generic
and limited in-situ operations by a depot to data
stored in its IBP allocations. The NFU has been
used in numerous experimental deployments, and
has been shown to enable robust fault tolerance
and high performance is a wide variety of appli-
cations [14, 39, 40]. However, the middleware
stack that supported such experimentation has
never been fully integrated with the deployed
versions of LoRS and LoDN or the Data Logis-
tics Toolkit (discussed below), and so the NFU
has never been used in a persistent large scale
deployment.
7.1.2 Deployments, SW Distribution and Apps
The National Logistical Networking Testbed (NLNT)
and the Research and Education Data Depot Network
(REDDnet) were two NSF-funded infrastructures that
deployed IBP at roughtly a dozen cites nationwide (in-
cluding Hawaii) and in Europe. The NLNT (2002-2007)
was a terascale project based at the Univeristy of Ten-
nessee’s LoCI Lab, while REDDnet (est. 2007) was
a petascale project based at Vanderbilt’s ACCRE. In
addition to these dedicated deployments, IBP was per-
sistently deployed on the nodes of the shared PlanetLab
infrastructure and has also been deployed on GENI [41]
through the efforts of the Data Logistics Toolkit project.
Leveraging the work of the NLNT and REDDnet
projects, the L-Store project [3], based at Vanderbilt
University’s Academic Computing Center for Research
and Education, has constructed an alternate stack upon
the common IBP service that is more adapted to sup-
porting large scale enterprise storage. It has been in use
for over a decade and currently supports multi-petabyte
collections and high-performance local and wide-area
data access through a variety of standard file access
protocols.
The Data Logistics Toolkit (DLT) is an NSF-funded
effort based at the Indiana University’s Center for Re-
search in Extreme Scale Technologies to collect, pack-
age and harden the components of the Logistical Net-
working Stack, L-Store and related tools including perf-
Sonar and Periscope, installable via Linux package man-
ager. The exNode repository and active management
function of LoDN has been reimplemented using IU’s
Unified Network Information Service (UNIS) [33].
Numerous experimental and preproduction applica-
tions of Logistical Networking have been extensively
documented over the past 20 years [12, 14, 18, 30, 34,
36, 39, 56]. These applications include functions such
as large email attachments, large file storage and de-
livery, reliable multicast, large scale data management
and access and edge processing for volume reduction
(e.g. filtering) or conditioning (e.g. sorting). Many
of these functions were implemented using Logistical
Networking infrastructure with no or minimal help from
application-specific servers or persistent managers long
before the advent of commercial cloud services that ad-
dress the same requirements. Some have implemented
advanced functionality not yet replicated in any con-
ventional paradigm of distributed wide area infrastruc-
ture [22].
7.2 EBP Below the Network Layer
The question of where in any stack of services conver-
gence between disparate underlying resources should
be located is one that has gone on for decades. The ef-
fects of path dependence, the entrenchment of silos and
the pressures against disruption of low-level standards
and practices in highly developed engineering niches
are very strong. These factors can lead to a belief that
current silos cannot be disrupted, and that trying to do
so is foolhardy or wrong. Anecdotal accounts abound
of resistance to the dominance of the Internet in the late
20th Century.
On the other hand, the argument for creating a con-
verged layer to support the Internet and other global
Submitted ACM/IFIP/USENIX Middleware 2017 10
Interoperable Convergence Micah Beck, Terry Moore, Piotr Luszczek
LAYER 7
LAYER 4
LAYER 3
LAYER 2
LAYER 1
IP
TCP, UDP
APPLICATIONS
PHYSICAL
LINK
STORAGE
IP
TRAN
SFER
TCP, UDP
PROCESSING
APPLICATIONS
PHYSICAL
LINK
A B C
Figure 4. The Hourglass vs. The Anvil
distributed services is compelling. The need for dis-
tributed systems to have access to and control over low
layer network characteristics including topology and
performance is clear in the steps that have been taken to
work around the stricture that forbids such direct access
in the Internet architecture. The situation is similar in
other resources which do not have such restrictive rules,
leading to lopsided designs in which high layers of the
Internet stack are combined with low layers of other
services (e.g. transportation).
The use of a standard spanning layer to enable inter-
operability carries with it the issues associated with path
dependence. Once a standard has been set, it becomes
difficult and costly to change (witness the decades-long
delays in replacement of IPv4 with IPv6). Anderson,
Peterson, Shenker and Turner discuss the barriers to
innovation (network ossification in their parlance) that
have resulted from the standardization on IP at the net-
work layer [7].
For these reasons, innovation increases when the span-
ning layer is placed lower in the network stack. Attempts
to enforce overlay convergence as a standard above the
network layer within the application community has the
result of isolating the community of users of that con-
verged interface while acceding to the ossification due
to the IP standard at layer 3, as in Grid Computing [35].
This presents the challenge of defining a converged span-
ning layer below layer 3 (a generalization of the Link
Layer of the Internet stack to include storage and pro-
cessing) that is simple, generic and limited to achieve
deployment scalability.
We propose the creation of a platform based on a com-
mon service similar to IBP but which models the net-
working capabilities of the Link Layer. We use the term
Exposed Buffer Processing for this as-yet-unrealized
service. The Big Idea advocated in this paper is that the
appropriate platform for the creation of distributed sys-
tems is some form of EBP. We emphasize that EBP need
not follow the design of IBP, as long as it takes appro-
priate account of the Deployment Scalability Tradeoff.
However we offer experience with IBP as an overlay
form of EBP for the consideration of the community.
8 Applications of EBP
In Section 5 we discussed a simple, well-known example
occurs when data that resides in a file is to be sent on
a connected TCP stream. An operation that allows the
direct movement of data between the file system and
network buffers, or even better one that would allow
network transfers directly buffers that are also used for
file system operations can eliminate some or all of this
unnecessary copying.
8.1 Case Study: Scientific Content Delivery
Dissemination of data is one of the fundamental chal-
lenges of modern experimental and observational sci-
ence. There is a general move toward the open sharing
of raw data sets, enabling replication of analyses, cross-
cutting studies, innovative reexamination of previously
collected data and historical examination of collection
and analysis techniques [38, 50]. In many case the data
collected is large and observation is continuous, as in
remote data from satellites and other sensors [20], exper-
iments such as the Large Hadron Collider [19], or broad
harvesting of multimedia content [21]. The resources
required to make such data streams instantaneously and
persistently available can exceed the centralized capabil-
ities of institutions or government agencies.
Commercial CDN or Cloud solutions may be too ex-
pensive, and may not adequately serve the entire global
user community (see discussion of the Digital Divide
Submitted ACM/IFIP/USENIX Middleware 2017 11
Interoperable Convergence Micah Beck, Terry Moore, and Piotr Luszczek
below) and may not adequately support the publication
by users of secondary data products resulting from their
processing of raw data. However, the ICT resources
required to address such problems may be affordable,
and the community of user institutions may be capable
of hosting them in a distributed manner. Using shared
EBP infrastructure, we can build a distributed, federated
content management system using the resources of the
content provider and user communities
8.2 Case Study: Digital Divide and Disaster
Recovery
Modern network services take full advantage of the
strong assumptions that can be made about the imple-
mentation of the Internet in the industrial world. It is
common for services to rely on continual low-latency
datagram delivery, always-connected servers, stable and
uninterrupted datagram routing paths and high band-
width connectivity to take just a few examples. Services
implemented at Cloud Computing centers are among
those that place the greatest demands on the Internet
backbone and ”last mile” connectivity to edge networks.
Many services can be decomposed into synchronous
and asynchronous components, and different “Data Lo-
gistics” strategies applied to each part. Techniques used
in Content Delivery Networks, including caching and
prestaging can be applied on a fine-grained and even
per-client basis. It is sometimes the case that the en-
tire service can be implemented using edge resources.
In other cases there is a component that can only be
implemented using synchronous end-to-end datagram
delivery across the backbone, but it requires only low
bandwidth so that scarce high-quality network resources
can be used. In some cases, a careful analysis of the
application combined with reconsideration of the truely
necessary characteristics of the service delivered to the
end-user can reduce the need for high qualtiy synchro-
nous connectivity to the vanishing point. In a sense,
strong network assumptions are a crutch that allows
wasteful application design and ease of development.
Today, some environments cannot support strong net-
work assumptions, even when local IT resources are
available. Examples are communities isolated through
geography, economic (poverty, discrimination) or po-
litical circumstances (famine, war), or social factors.
Disasters create environments where infrastructure is
disrupted even in the most advanced societies (e.g. Hur-
ricane Katrina). The recent response of modern network
technologists has been to bring fixed or mobile wireless
technology (satellite, 4G) into remote locations and to
the scene of disasters or to create complex wireless in-
frastructures based on continuous aviation drones such
as Google’s baloon-based project Loon [4] and Face-
book’s drone-based project Aquila [1]. Some aggres-
sively optimistic projects have already been abandoned,
such as Google’s drone-based Project Titan [2]. The al-
ternative of using a mix of interoperable heterogeneous
synchronous and asynchronous data transport integrated
into a flexible platform to support a variety of distributed
applications can, by contrast, be cheap, robust and easily
deployed.
8.3 Case Study: Big Data and Edge Processing
One of the inexorable trends in the collection of data
is the emergence of large scale online sensors and in-
strument that produce data that must be subjected to
volume-reducing processing before it can be passed over
the network. Growing trends in sensor networks, the
Internet of Things, and Smart Cities will severely exac-
erbate this problem, to say the least [9]. The historical
approach of sending all such data to computation cen-
ters that are either self-contained or connected to their
peers through heroic networking that may be private
or even proprietary in nature is no longer sufficient to
address the total size, globally distributed generation,
and need for immediate use by applications that we see
today [26].
An alternative possible using EBP is to apply limited
edge processing on the collection device or in the edge
network using a converged infrastructure that can also
store and transport data.
9 Conclusions
In this paper, we have argued that interoperable conver-
gence of storage, networking and processing is neces-
sary in building a platform to support distributed systems
which exhibits deployment scalability, and that the most
effective implementation is a form of Exposed Buffer
Processing at a layer below that which implements the
Internet. Our argument rests on practical historical ex-
amples of the problems caused by the Internet’s lack
of expressiveness and an argument based on a partially
formalized design methodology that the spanning layer
of any converged infrastructure must be simple, generic
and limited.
Acknowledgements
The ideas in this paper were influenced by many spirited
discussions with Martin Swany on the integration of
Submitted ACM/IFIP/USENIX Middleware 2017 12
Interoperable Convergence Micah Beck, Terry Moore, Piotr Luszczek
storage and processing with scalable networking, and
by recent conversations with Glenn Ricart on the defini-
tion and justification of interoperable convergence. The
concept of ”exposed buffer protocol/processing” was
coined during discussions between Swany and Beck,
although its best definition and implementation are still
subject to debate. The authors are also indebted to David
Rogers for his professional rendering of the artwork in
this any many other papers and presentations, and to
Chris Brumgard for his helpful comments.
References
[1] Facebook Takes Flight. https://www.theverge.com/a/mark-
zuckerberg-future-of-facebook/aquila-drone-internet. (????).
[2] Google halts drone project to beam internet to rural ar-
eas. http://money.cnn.com/2017/01/12/technology/google-
drone-titan-shut-down/. (????).
[3] L-Store. http://www.lstore.org. (????).
[4] Project Loon. https://x.company/loon/. (????).
[5] Will the Real End-to-End Argument Please Stand Up?
http://mercury.lcs.mit.edu/ jnc/tech/end end.html. (????).
[6] 2013. UUX(1P) POSIX Programmer’s Manual. (2013). http:
//www.unix.com/man-page/posix/1p/uux/
[7] Thomas Anderson, Larry Peterson, Scott Shenker, and
Jonathan Turner. 2005. Overcoming Barriers to Disruptive
Innovation in Networking. In In Report of National Science
Foundation Workshop.
[8] Haim Avron, Petar Maymounkov, and Sivan Toledo. 2010.
Blendenpik: Supercharging LAPACK’s least-squares solver.
SIAM Journal on Scientific Computing 32, 3 (2010), 1217–
1236.
[9] Suman Banerjee and Dapeng Oliver Wu. 2013. Final report
from the NSF Workshop on Future Directions in Wireless
Networking. (2013).
[10] Micah Beck. 2016. On The Hourglass Model, The End-to-End
Principle and Deployment Scalability. (November 2016). http:
//philsci-archive.pitt.edu/12626/ Submitted to Communications
of the ACM.
[11] Micah Beck, Dorian Arnold, Ro Bassi, Fran Berman, Henri
Casanova, Terry Moore, Graziano Obertelli, James Plank, Mar-
tin Swany, Sathish Vadhiyar, and Rich Wolski. 2001. Logis-
tical computing and internetworking: Middleware for the use
of storage in communication. In In 3rd Annual International
Workshop on Active Middleware Services (AMS.
[12] Micah Beck, Ying Ding, Erika Fuentes, and Sharmila
Kancherla. 2003. An Exposed Approach to Reliable Mul-
ticast in Heterogeneous Logistical Networks. In In Workshop
on Grids and Advanced Networks (GAN03. 12–15.
[13] Micah Beck and Joe Kirby. 1997. Tennessee cache box project.
In 2nd Web Caching Workshop.
[14] Micah Beck, Huadong Liu, Jian Huang, and Terry Moore.
2007. Scalable Distributed Execution Environment for Large
Data Visualization. IEEE Explorer (11 Nov. 2007).
[15] Micah Beck and Terry Moore. 1998. The Internet2 Distributed
Storage Infrastructure Project: An Architecture for Internet
Content Channels. In Computer Networking and ISDN Systems.
2141–2148.
[16] Micah Beck, Terry Moore, and James S. Plank. 2002. An
end-to-end approach to globally scalable network storage. In
In ACM SIGCOMM 2002.
[17] Micah Beck, Terry Moore, and James S. Plank. 2003. An
End-to-End Approach to Globally Scalable Programmable
Networking. In Future Diretions in Network Arachitecture.
ACM Press, 328–339.
[18] Viraj Bhat, Scott Klasky, Scott Atchley, Micah Beck, Doug Mc-
cune, and Manish Parashar. 2004. High performance threaded
data streaming for large scale simulations. In 5th IEEE/ACM
International Workshop on Grid Computing (Grid 2004. IEEE
Computer Society Press, 243–250.
[19] Ian Bird. 2011. Computing for the large hadron collider. An-
nual Review of Nuclear and Particle Science 61 (2011), 99–
118.
[20] Space Studies Board, National Research Council, and others.
2014. Landsat and Beyond: Sustaining and Enhancing the
Nation’s Land Imaging Program. National Academies Press.
[21] Marshall Breeding. 2003. Building a digital library of televi-
sion news. Computers in libraries 23, 6 (2003), 47–49.
[22] Christopher David Brumgard. 2016. Substituting Failure Avoid-
ance for Redundancy in Storage Fault Tolerance. Ph.D. Dis-
sertation. University of Tennessee, Knoxville.
[23] Rajkumar Buyya, Mukaddim Pathan, and Athena Vakali (Eds.).
2008. Content Delivery Networks. Springer.
[24] B. Carpenter and S. Brim. 2002. Middleboxes: Taxonomy
and Issues. RFC 3234. (Feb. 2002). https://tools.ietf.org/html/
rfc3234 Network Working Group.
[25] Anawat Chankhunthod, Peter B. Danzig, Chuck Neerdaels,
Michael F. Schwartz, and Kurt J. Worrell. 1995. A Hierarchical
Internet Object Cache. In IN PROCEEDINGS OF THE 1996
USENIX TECHNICAL CONFERENCE. 153–163.
[26] Min Chen, Shiwen Mao, and Yunhao Liu. 2014. Big data:
A survey. Mobile Networks and Applications 19, 2 (2014),
171–209.
[27] Brent Chun, David Culler, Timothy Roscoe, Andy Bavier,
Larry Peterson, Mike Wawrzoniak, and Mic Bowman. 2003.
PlanetLab: An Overlay Testbed for Broad-coverage Services.
SIGCOMM Comput. Commun. Rev. 33, 3 (July 2003), 3–12.
https://doi.org/10.1145/956993.956995
[28] Paul A David. 2007. Path dependence: a foundational concept
for historical social science. Cliometrica 1, 2 (2007), 91–114.
[29] Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce: Sim-
plified Data Processing on Large Clusters. Commun. ACM
51, 1 (Jan. 2008), 107–113. https://doi.org/10.1145/1327452.
1327492
[30] Jin Ding, Jian Huang, Micah Beck, Shaotao Liu, Terry Moore,
and Stephen Soltesz. 2003. Remote Visualization by Brows-
ing Image Based Databases with Logistical Networking. In
Proceedings of the 2003 ACM/IEEE Conference on Super-
computing (SC ’03). ACM, New York, NY, USA, 34–. https:
//doi.org/10.1145/1048935.1050185
[31] Jack Dongarra, Gene H. Golub, Eric Grosse, Cleve Moler, and
Keith Moore. 2008. Netlib and NA-Net: Building a Scien-
tific Computing Community. IEEE Annals of the History of
Computing 30 (April 2008), 30–41. Issue 2.
[32] Petros Drineas and Michael W. Mahoney. 2016. RandNLA:
Randomized Numerical Linear Algebra. Commun. ACM 59, 6
Submitted ACM/IFIP/USENIX Middleware 2017 13
Interoperable Convergence Micah Beck, Terry Moore, and Piotr Luszczek
(2016), 80–90.
[33] Ahmed El-Hassany, Ezra Kissel, Dan Gunter, and Martin
Swany. 2013. Design and Implementation of a Unified Net-
work Information Service. In Proceedings of the 2013 IEEE
International Conference on Services Computing (SCC ’13).
IEEE Computer Society, Washington, DC, USA, 224–231.
https://doi.org/10.1109/SCC.2013.81
[34] Wael R. Elwasif, James S. Plank, Micah Beck, and Rich Wol-
ski. 1999. IBP-Mail: Controlled Delivery of Large Mail Files.
(1999).
[35] I. Foster and C. Kesselman. 1999. The Grid: Blueprint for a
New Computing Infrastructure. Morgan Kaufmann Publishers,
47–48.
[36] John Allen Garrison. 2004. Availability in IBPro, a Per-
sonal Video Recorder Built on Faulty Storage Components.
Master’s thesis. University of Tennessee, Knoxville. http:
//trace.tennessee.edu/utk chanhonoproj/741/
[37] David Kirkpatrick. 1996. IBM’S Olympic Fiasco
Department of Groundless Optimism. (9 Sept.
1996). http://archive.fortune.com/magazines/fortune/
fortune archive/1996/09/09/216607/index.htm
[38] Rob Kitchin. 2014. The data revolution: Big data, open data,
data infrastructures and their consequences. Sage.
[39] Huadong Liu. 2008. Scalable, Data- intensive Network Compu-
tation. Ph.D. Dissertation. University of Tennessee, Knoxville.
[40] Huadong Liu, Micah Beck, and Jian Huang. 2006. Dynamic
Co-Scheduling of Distributed Computation and Replication.
In IEEE International Symposium on Cluster Computing and
the Grid (Singapore).
[41] Rick McGeer, Mark Berman, Chip Elliott, and Robert Ricci
(Eds.). 2016. The GENI Book. Springer.
[42] Elizabeth Mynatt, Jennifer Clark, Greg Hager, Dan Lopresti,
Greg Morrisett, Klara Nahrstedt, George Pappas, Shwetak
Patel, Jennifer Rexford, Helen Wright, and others. 2017.
A National Research Agenda for Intelligent Infrastructure.
arXiv preprint arXiv:1705.01920 (2017). http://cra.org/ccc/
resources/ccc-led-whitepapers/
[43] Klara Nahrstedt, Christos G Cassandras, and Charlie
Catlett. 2017. City-Scale Intelligent Systems and Platforms.
arXiv preprint arXiv:1705.01990 (2017). http://cra.org/ccc/
resources/ccc-led-whitepapers/
[44] George Neisser. 1999. The National JANET Web Cache:
Progress Report. (21 Dec. 1999). Issue 22. http://www.ariadne.
ac.uk/issue22/web-cache/
[45] Networking, Information Technology Research, and
Development (NITRD) Program. 2015. Smart
and Connected Cities Framework. (2015).
https://www.nitrd.gov/sccc/materials/scccframework.pdf.
[46] Erik Nygren, Ramesh K. Sitaraman, and Jennifer Sun. 2010.
The Akamai network: A platform for high-performance Inter-
net applications. SIGOPS Oper. Syst. Rev (2010).
[47] Alex (Sandy) Pentland, Richard Fletcher, and Amir Has-
son. 2004. DakNet: Rethinking Connectivity in Develop-
ing Nations. Computer 37, 1 (Jan. 2004), 78–83. https:
//doi.org/10.1109/MC.2004.1260729
[48] James S. Plank, Scott Atchley, Ying Ding, and Micah Beck.
2002. Algorithms for High Performance, Wide-Area, Dis-
tributed File Downloads. Technical Report. LETTERS.
[49] James S. Plank, Alessandro Bassi, Micah Beck, Terence
Moore, D. Martin Swany, and Rich Wolski. 2001. Manag-
ing Data Storage in the Network. IEEE Internet Computing 5,
5 (Sept. 2001), 50–58. https://doi.org/10.1109/4236.957895
[50] O James Reichman, Matthew B Jones, and Mark P Schildhauer.
2011. Challenges and opportunities of open data in ecology.
Science 331, 6018 (2011), 703–705.
[51] D. M. Ritchie and K. Thompson. 1974. The Unix Time-
Sharing System. Commun. ACM 17 (1974), 365–375.
[52] J. H. Saltzer, D. P. Reed, and D. D. Clark. 1984. End-to-
end Arguments in System Design. ACM Trans. Comput. Syst.
2, 4 (Nov. 1984), 277–288. https://doi.org/10.1145/357401.
357402
[53] David L. Tennenhouse and David J. Wetherall. 1996. Towards
an Active Network Architecture. Computer Communication
Review 26 (1996), 5–18.
[54] L. G. Valiant. 1984. A theory of the learnable. Commun. ACM
27 (1984), 1134–1142.
[55] Duane Wessels and k claffy. 1998. ICP and the Squid Web
Cache. IEEE JOURNAL ON SELECTED AREAS IN COMMU-
NICATION 16 (1998), 345–357.
[56] Jason Zurawski, Martin Swany, Micah Beck, and Ying Ding.
2005. Logistical multicast for data distribution. In In Workshop
on Grids and Advanced Networks.
Submitted ACM/IFIP/USENIX Middleware 2017 14

