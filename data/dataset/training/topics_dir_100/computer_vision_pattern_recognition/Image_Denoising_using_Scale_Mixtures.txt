Image Denoising using Scale Mixtures of
Gaussians in the Wavelet Domain
Javier Portilla
Universidad de Granada
Vasily Strela
Drexel University
Martin J. Wainwright
University of California
Eero P. Simoncelli
New York University
Published in: IEEE Transactions on Image Processing, vol. 12, no. 11 pp. 1338-1351, November 2003.
Abstract— We describe a method for removing noise from dig-
ital images, based on a statistical model of the coefficients of an
overcomplete multiscale oriented basis. Neighborhoods of coeffi-
cients at adjacent positions and scales are modeled as the product
of two independent random variables: a Gaussian vector and a
hidden positive scalar multiplier. The latter modulates the local
variance of the coefficients in the neighborhood, and is thus able
to account for the empirically observed correlation between the co-
efficient amplitudes. Under this model, the Bayesian least squares
estimate of each coefficient reduces to a weighted average of the lo-
cal linear estimate over all possible values of the hidden multiplier
variable. We demonstrate through simulations with images con-
taminated by additive white Gaussian noise that the performance
of this method substantially surpasses that of previously published
methods, both visually and in terms of mean squared error.
The artifacts arising from many imaging devices are quite
different from the images that they contaminate, and this differ-
ence allows humans to “see past” the artifacts to the underlying
image. The goal of image restoration is to relieve human ob-
servers from this task (and perhaps even to improve upon their
abilities) by reconstructing a plausible estimate of the original
image from the distorted or noisy observation. A prior proba-
bility model for both the noise and for uncorrupted images is of
central importance for this application.
Modeling the statistics of natural images is a challenging
task, partly because of the high dimensionality of the signal.
Two basic assumptions are commonly made in order to reduce
dimensionality. The first is that the probability structure may
be defined locally. Typically, one makes a Markov assumption,
that the probability density of a pixel, when conditioned on a
set of neighbors, is independent of the pixels beyond the neigh-
borhood. The second is an assumption of spatial homogene-
ity: the distribution of values in a neighborhood is the same
for all such neighborhoods, regardless of absolute spatial posi-
tion. The Markov random field model that results from these
two assumptions is commonly simplified by assuming the dis-
tributions are Gaussian. This last assumption is problematic for
image modeling, where the complexity of local structures is not
well described by Gaussian densities.
The power of statistical image models can be substantially
improved by transforming the signal from the pixel domain to
a new representation. Over the past decade, it has become stan-
dard to initiate computer-vision and image processing tasks by
decomposing the image with a set of multiscale bandpass ori-
During the development of this work, VS was on leave from Drexel Uni-
versity, and was supported by an AMS Centennial Fellowship. MW was sup-
ported by a NSERC-1967 Fellowship. JP and EPS were supported by an NSF
CAREER grant and Alfred P. Sloan Fellowship to EPS, and by the Howard
Hughes Medical Institute. JP was also supported by an FPI fellowship, and
subsequently by a ”Ramón y Cajal” grant (both from the Spanish government).
ented filters. This kind of representation, loosely referred to
as a wavelet decomposition, is effective at decoupling the high-
order statistical features of natural images. In addition, it shares
some basic properties of neural responses in the primary visual
cortex of mammals which are presumably adapted to efficiently
represent the visually relevant features of images.
A number of researchers have developed homogeneous lo-
cal probability models for images in multiscale oriented rep-
resentations. Specifically, the marginal distributions of wavelet
coefficients are highly kurtotic, and can be described using suit-
able long-tailed distributions. Recent work has investigated the
dependencies between coefficients, and found that the ampli-
tudes of coefficients of similar position, orientation and scale
are highly correlated. These higher order dependencies, as
well as the higher order marginal statistics, may be modeled by
augmenting a simple parametric model for local dependencies
(e.g., Gaussian) with a set of “hidden” random variables that
govern the parameters (e.g., variance). Such hidden Markov
models have become widely used, for example, in speech pro-
cessing.
In this article, we develop a model for neighborhoods of
oriented pyramid coefficients based on a Gaussian scale mix-
ture [1]: the product of a Gaussian random vector, and an inde-
pendent hidden random scalar multiplier. We have previously
demonstrated that this model can account for both marginal
and pairwise joint distributions of wavelet coefficients [2], [3].
Here, we develop a local denoising solution as a Bayesian
least squares estimator, and demonstrate the performance of this
method on images corrupted by simulated additive white Gaus-
sian noise of known variance.
I. BACKGROUND: STATISTICAL IMAGE MODELS AND
DENOISING
Contemporary models of image statistics are rooted in the
television engineering of the 1950s (see [4] for review), which
relied on a characterization of the autocovariance function for
purposes of optimal signal representation and transmission.
This work, and nearly all work since, assumes that image statis-
tics are spatially homogeneous (i.e., strict-sense stationary).
Another common assumption in image modeling is that the
statistics are invariant, when suitably normalized, to changes
in spatial scale. The translation- and scale-invariance assump-
tions, coupled with an assumption of Gaussianity, provides the
baseline model found throughout the engineering literature: im-
ages are samples of a Gaussian random field, with variance
falling as f−γ in the frequency domain. In the context of de-
noising, if one assumes the noise is additive and independent
2
of the signal, and is also a Gaussian sample, then the optimal
estimator is linear.
A. Modeling non-Gaussian image properties
In recent years, models have been developed to account for
non-Gaussian behaviors of image statistics. One can see from
casual observation that individual images are highly inhomo-
geneous: they typically contain many regions that are smooth,
interspersed with “features” such as contours, or surface mark-
ings. This is reflected in the observed marginal distributions
of bandpass filter responses, which show a large peak at zero,
and tails that fall significantly slower than a Gaussian of the
same variance [5], [6], [7] (see Fig. 1(a)). When one seeks a
linear transformation that maximizes the non-Gaussianity1 of
the marginal responses, the result is a basis set of bandpass ori-
ented filters of different sizes spanning roughly an octave in
bandwidth [e.g., 8], [9].
Due to the combination of these qualitative properties, as
well as an elegant mathematical framework, multiscale oriented
subband decompositions have emerged as the representations
of choice for many image processing applications. Within the
subbands of these representations, the kurtotic behaviors of co-
efficients allow one to remove noise using a point nonlinearity.
Such approaches have become quite popular in the image de-
noising literature, and typically are chosen to perform a type
of thresholding operation, suppressing low-amplitude values
while retaining high-amplitude values. The concept was devel-
oped originally in the television engineering literature (where it
is known as “coring”[e.g., 10]), and specific shrinkage func-
tions have been derived under a variety of formulations, in-
cluding minimax optimality under a smoothness condition [11],
[12], [13], and Bayesian estimation with non-Gaussian pri-
ors [e.g. 14], [15], [16], [17], [18], [19], [20], [21].
In addition to the non-Gaussian marginal behavior, the re-
sponses of bandpass filters exhibit important non-Gaussian
joint statistical behavior. In particular, even when they are
second-order decorrelated, the coefficients corresponding to
pairs of basis functions of similar position, orientation and scale
exhibit striking dependencies [22], [23]. Casual observation in-
dicates that large-amplitude coefficients are sparsely distributed
throughout the image, and tend to occur in clusters. The condi-
tional histograms of pairs of coefficients indicates that the stan-
dard deviation of a coefficient scales roughly linearly with the
amplitude of nearby coefficients [23], [24], [2] (see Fig. 1(c)).
The dependency between local coefficient amplitudes, as
well as the associated marginal behaviors, can be modeled us-
ing a random field with a spatially fluctuating variance. A par-
ticularly useful example arises from the product of a Gaussian
vector and a hidden scalar multiplier, known as a Gaussian
scale mixture [1] (GSM). GSM distributions represent an im-
portant subset of the elliptically symmetric distributions, which
are those that can be defined as functions of a quadratic norm
of the random vector. Embedded in a random field, these kinds
of models have been found useful in the speech-processing
community [25]. A related set of models, known as autore-
gressive conditional heteroskedastic (ARCH) models [e.g., 26],
1Different authors have used different measures of non-Gaussianity, but have
obtained similar results.
−50 0 50
10
0
10
5
−50 0 50
10
0
10
5
(a) Observed (b) Simulated
(c) Observed (d) Simulated
Fig. 1. Comparison of coefficient statistics from an example image subband (a
vertical subband of the Boats image, left panels) with those arising from simula-
tion of a local GSM model (right panels). Model parameters (covariance matrix
and the multiplier prior density) are estimated by maximizing the likelihood of
the observed set of wavelet coefficients. (a,b) Log marginal histograms. (c,d)
Conditional histograms of two spatially adjacent coefficients. Brightness corre-
sponds to probability, except that each column has been independently rescaled
to fill the range of display intensities.
have proven useful for many real signals that suffer from abrupt
fluctuations, followed by relative “calm” periods (stock market
prices, for example). These kinds of ideas have also been found
effective in describing visual images. For example, Baraniuk
and colleagues used a 2-state hidden multiplier variable to char-
acterize the two modes of behavior corresponding to smooth or
low-contrast textured regions and features [27], [28]. Our own
work, as well as that of others, assumes that the local variance
is governed by a continuous multiplier variable [29], [2], [30],
[3]. This model can capture the strongly leptokurtotic behavior
of the marginal densities of natural image wavelet coefficients,
as well as the correlation in their local amplitudes, as illustrated
in Fig. 1.
B. Empirical Bayes denoising using variance-adaptive models
More than 20 years ago, Lee [31] suggested a two-step pro-
cedure for image denoising, in which one first estimates the
local signal variance from a neighborhood of observed pixels,
and then (proceeding as if this were the true variance) applies
the standard linear least squares (LLS) solution. This method is
a type of empirical Bayes estimator [32], in that a parameter of
the local model is first estimated from the data, and this estimate
is subsequently used to estimate the signal. This two-step de-
noising solution can be applied to any of the variance-adaptive
models described in the previous section, and is substantially
more powerful when applied in a multiscale oriented represen-
tation. Specifically, a number of authors have estimated the lo-
cal variance from a collection of wavelet coefficients at nearby
positions, scales, and/or orientations, and then used these esti-
mated variances in order to denoise the coefficients [33], [23],
3
[34], [18], [30], [35].
Solutions based on GSM models, with different prior as-
sumptions about the hidden variables, have produced some of
the most effective methods for removing homogeneous additive
noise from natural images to date. Our initial work in this area
developed a maximum likelihood (ML) estimator [36]. Mihçak
et. al. used a maximum a posteriori (MAP) estimator based on
an exponential marginal prior [30], as did Li and Orchard [37],
whereas Portilla et. al. used a lognormal prior [38]. Wainwright
et. al. developed a tree-structured Markov model to provide a
global description for the set of multiplier variables [3]. De-
spite these successes, the two-step empirical Bayes approach
is suboptimal, even when the local variance estimator is op-
timal, because the second step does not take into account the
uncertainty associated with the variance estimated in the first
step. In this paper we derive a least squares optimal single-step
Bayesian estimator.
II. IMAGE PROBABILITY MODEL
As described in the previous section, multiscale representa-
tions provide a useful front-end for representing the structures
of visual images. But the widely used orthonormal or biorthog-
onal wavelet representations are problematic for many appli-
cations, including denoising. Specifically, they are critically
sampled (the number of coefficients is equal to the number of
image pixels), and this constraint leads to disturbing visual arti-
facts (i.e., “aliasing” or “ringing”). A widely followed solution
to this problem is to use basis functions designed for orthogonal
or biorthogonal systems, but to reduce or eliminate the decima-
tion of the subbands [e.g., 39].
Once the constraint of critical sampling has been dropped,
however, there is no need to limit oneself to these basis func-
tions. Significant improvement comes from the use of repre-
sentations with a higher degree of redundancy, as well as in-
creased selectivity in orientation [40], [18], [36], [21]. For
the current paper, we have used a particular variant of an over-
complete tight frame representation known as a steerable pyra-
mid [40], [41]. The basis functions of this multiscale lin-
ear decomposition are spatially localized, oriented, and span
roughly one octave in bandwidth. They are polar-separable in
the Fourier domain, and are related by translation, dilation, and
rotation. Other authors have developed representations with
similar properties [42], [43], [44], [21]. Details of the steerable
pyramid representation are provided in Appendix I.
A. Gaussian scale mixtures
Consider an image decomposed into oriented subbands at
multiple scales. We denote as xs,oc (n,m) the coefficient cor-
responding to a linear basis function at scale s, orientation
o, and centered at spatial location (2sn, 2sm). We denote as
xs,o(n,m) a neighborhood of coefficients clustered around this
reference coefficient.2 In general, the neighborhood may in-
clude coefficients from other subbands (i.e., corresponding to
basis functions at nearby scales and orientations), as well as
from the same subband. In our case, we use a neighborhood
2For notational simplicity, we drop the superscripts s, o and indices (n, m)
in the following development.
of coefficients drawn from two subbands at adjacent scales,
thus taking advantage of the strong statistical coupling observed
through scale in multiscale representations. Details are pro-
vided in section IV.
We assume the coefficients within each local neighborhood
around a reference coefficient of a pyramid subband are charac-
terized by a Gaussian scale mixture (GSM) model. Formally, a
random vector x is a Gaussian scale mixture [1] if and only if it
can be expressed as the product of a zero-mean Gaussian vector
u and an independent positive scalar random variable
√
z:
x
d
=
√
zu, (1)
where
d
= indicates equality in distribution. The variable z is
known as the multiplier. The vector x is thus an infinite mixture
of Gaussian vectors, whose density is determined by the covari-
ance matrix Cu of vector u and the mixing density, pz(z):
px(x) =
∫
p(x|z) pz(z)dz
=
∫
exp
(
−xT (zCu)−1x/2
)
(2π)N/2|zCu|1/2
pz(z)dz, (2)
where N is the dimensionality of x and u (in our case, the size
of the neighborhood). Without loss of generality, one can as-
sume E{z} = 1, which implies Cx = Cu.
The conditions under which a random vector may be repre-
sented using a GSM have been studied [1]. The GSM fam-
ily includes a variety of well-known families of random vari-
ables such as the α-stable family (including the Cauchy dis-
tribution), the generalized Gaussian (or stretched exponential)
family and the symmetrized Gamma family [3]. GSM densi-
ties are symmetric and zero-mean, and they have leptokurtotic
marginal densities (i.e., heavier tails than a Gaussian). A key
property of the GSM model is that the density of x is Gaussian
when conditioned on z. Also, the normalized vector x/
√
z is
Gaussian.
B. GSM model for the wavelet coefficients
As explained in Section I and illustrated in Fig. 1, a GSM
model can account for both the shape of wavelet coefficient
marginals and the strong correlation between the amplitudes of
neighbor coefficients [2], [3]. In order to construct a global
model for images from this local description, one must spec-
ify both the neighborhood structure of the coefficients, and the
distribution of the multipliers. The definition of (and calcula-
tions using) the global model is considerably simplified by par-
titioning the coefficients into non-overlapping neighborhoods.
One can then specify either a marginal model for the multipli-
ers (treating them as independent variables) [45], or specify a
joint density over the full set of multipliers [3]. Unfortunately,
the use of disjoint neighborhoods leads to noticeable denoising
artifacts at the discontinuities introduced by the neighborhood
boundaries.
An alternative approach is to use a GSM as a local descrip-
tion of the behavior of the cluster of coefficients centered at
each coefficient in the pyramid. Since the neighborhoods over-
lap, each coefficient will be a member of many neighborhoods.
4
The local model implicitly defines a global (Markov) model,
described by the conditional density of a coefficient in the clus-
ter given its surrounding neighborhood, assuming conditional
independence on the rest of the coefficients. But the structure of
the resulting model is such that performing statistical inference
(i.e., computing Bayes estimates) in an exact way is quite chal-
lenging. In this paper, we simply solve the estimation problem
for the reference coefficient at the center of each neighborhood
independently.
C. Prior density for multiplier
To complete the model, we need to specify the probability
density, pz(z), of the multiplier. Several authors have sug-
gested the generalized Gaussian (stretched exponential) fam-
ily of densities as an appropriate description of wavelet coeffi-
cient marginal densities [7], [14], [19]: px(x) ∝ exp
(
−
∣
∣
x
s
∣
∣
p)
,
where the scaling variable s controls the width of the distribu-
tion, and the exponent p controls the shape (in particular, the
heaviness of the tails), and is typically estimated to lie in the
range [0.5, 0.8] for image subbands. Although these can be ex-
pressed as GSMs, the density of the associated multiplier has
no closed form expression, and thus this solution is difficult to
implement.
In previous work [38], we noted that for the case N = 1,
the density of the log coefficient magnitude, log |x|, may be
written as a convolution of the densities of log |u| and log√z.
Since the density of u is known, this means that estimation of
the density of log
√
z may be framed as a deconvolution prob-
lem. The resulting estimated density may be approximated by
a Gaussian, corresponding to a lognormal prior for the z. This
solution has two important drawbacks. First, it is only extrapo-
lable to the N > 1 case when all the neighbors have the same
marginal statistics, which, in practice requires they all belong to
the same subband. Second, it is estimated from the noise-free
coefficients, and it is difficult to extend it for use in the noisy
case.
We have also investigated a more direct maximum likeli-
hood approach for estimating a nonparametric pz(z) from an
observed set of neighborhood vectors:
p̂z(z) = arg max
pz(z)
M
∑
m=1
log
(
∫ ∞
0
p(xm|z)pz(z)dz
)
, (3)
where the sum is over the neighborhoods. Note that the esti-
mate, p̂z(z), must be constrained to positive values, and must
have unit area. We have developed an efficient algorithm for
computing this solution numerically. One advantage of the ML
solution is that it is easily extended for use with the noisy ob-
servations, by replacing xm with the noisy observation.
A fourth choice is a so-called noninformative prior [46],
which has the advantage that it does not require the fitting of any
parameters to the noisy observation. Such solutions have been
used in establishing marginal priors for image denoising [47].
We have examined the most widely used solution, known as
Jeffrey’s prior (see Ref. [46]). In the context of estimating the
multiplier z from coefficients x, this takes the form:
pz(z) ∝
√
I(z), I(z) = E{−∂
2log p(x|z)
∂z2
}
where I(z) is the Fisher information matrix. Computing this
for the GSM model is straightforward:
−∂
2log p(x|z)
∂z2
=
∂2
∂z2
[
1
2
(
N log(z) + log |Cu| +
xT Cu
−1x
z
)]
=
N
2z2
+
xT Cu
−1x
2z3
.
Taking the square root of the expectation, and using the fact that
E{xT Cu−1x} = z we obtain Jeffrey’s prior:
1
which correspond
is an improper pro
to ignore this fact
problems at the es
prior to zero in the
where zmin is a sm
tails).
Of the four alte
expected) that the
the best results fo
least squares optim
not necessarily lea
image pixels, sinc
We were surprise
cally leads to bette
(roughly +0.15 dB
and more efficient
results shown in th
Our procedure
structure as most p
pose the image in
orientations; (2) d
residual band; and
the denoised imag
pendent additive w
that the method c
known covariance
hood of N observ
can be expressed a
y
Note that the assu
pled with the ass
noise, means that
of (5) are indepen
Both u and w a
ated covariance m
served neighborho
Gaussian, with co
p(y|z) =pz(z) ∝
z
, (4)
s to a constant prior on log(z). Note that this
bability density. Nevertheless it is common
as long as it does not create computational
timation stage. In our case, we have set the
interval [0, zmin) to prevent such problems,
all positive constant (see section IV for de-
rnatives described above, we have found (as
ML-estimated nonparametric prior produces
r denoising the pyramid coefficients. But a
al estimate for the pyramid coefficients does
d to a least-squares optimal estimate for the
e the pyramid representation is overcomplete.
d to find that the noninformative prior typi-
r denoising performance in the image domain
, on average). Given that it is also simpler
to implement, we have used it for all of the
e following sections.
III. IMAGE DENOISING
for image denoising uses the same top-level
reviously published approaches: (1) decom-
to pyramid subbands at different scales and
enoise each subband, except for the lowpass
(3) invert the pyramid transform, obtaining
e. We assume the image is corrupted by inde-
hite Gaussian noise of known variance (note
an also handle non-white Gaussian noise of
). A vector y corresponding to a neighbor-
ed coefficients of the pyramid representation
s:
= x + w =
√
zu + w. (5)
med GSM structure of the coefficients, cou-
umption of independent additive Gaussian
the three random variables on the right side
dent.
re zero-mean Gaussian vectors, with associ-
atrices Cu and Cw. The density of the ob-
od vector conditioned on z is a zero-mean
variance Cy|z = zCu + Cw:
exp
(
−yT (zCu + Cw)−1y/2
)
√
(2π)N |zCu + Cw|
. (6)
5
The neighborhood noise covariance, Cw, is obtained by de-
composing a delta function σ
√
NyNxδ(n,m) into pyramid
subbands, where (Ny, Nx) are the image dimensions. This sig-
nal has the same power spectrum as the noise, but it is free from
random fluctuations. Elements of Cw may then be computed
directly as sample covariances (i.e., by averaging the products
of pairs of coefficients over all the neighborhoods of the sub-
band). This procedure is easily generalized for non-white noise,
by replacing the delta function with the inverse Fourier trans-
form of the square root of the noise power spectral density.
Note that the entire procedure may be performed off-line, as
it is signal-independent.
Given Cw, the signal covariance Cu can be computed from
the observation covariance matrix Cy. We compute Cy from
Cy|z by taking the expectation over z:
Cy = E{z}Cu + Cw,
Without loss of generality, we set E{z} = 1, resulting in:
Cu = Cy − Cw. (7)
We force Cu to be positive semidefinite by performing an
eigenvector decomposition and setting any possible negative
eigenvalues (non-existing or negligible, in most cases) to zero.
A. Bayes least squares estimator
For each neighborhood, we wish to estimate xc, the reference
coefficient at the center of the neighborhood, from y, the set of
observed (noisy) coefficients. The Bayes least squares (BLS)
estimate is just the conditional mean:
E{xc|y} =
∫
xc p(xc|y) dxc
=
∫ ∫ ∞
0
xc p(xc, z|y) dz dxc
=
∫ ∫ ∞
0
xc p(xc|y, z) p(z|y) dz dxc
=
∫ ∞
0
p(z|y) E{xc|y, z} dz, (8)
where we have assumed uniform convergence in order to ex-
change the order of integration. Thus, the solution is the aver-
age of the Bayes least squares estimate of x when conditioned
on z, weighted by the posterior density, p(z|y). We now de-
scribe each of these individual components.
B. Local Wiener estimate
The key advantage of the GSM model is that the coefficient
neighborhood vector x is Gaussian when conditioned on z.
This fact, coupled with the assumption of additive Gaussian
noise means that the expected value inside the integral of (8)
is simply a local linear (Wiener) estimate. Writing this for the
full neighborhood vector:
E{x|y, z} = zCu(zCu + Cw)−1y, (9)
We can simplify the dependence of this expression on z by
diagonalizing the matrix zCu + Cw. Specifically, let S be the
symmetric square root of the positive definite matrix Cw (i.e.,
Cw = SS
T ), and let {Q,Λ} be the eigenvector/eigenvalue
expansion of the matrix S−1CuS−T . Then:
zCu + Cw = zCu + SS
T
= S
(
zS−1CuS
−T + I
)
ST
= SQ (zΛ + I)QT ST . (10)
Note this diagonalization does not depend on z, and thus need
only be computed once for each subband. We can now simplify
(9) as follows:
E{x|y, z} = zCuS−T Q(zΛ + I)−1QT S−1y
= zSS−1CuS
−T Q(zΛ + I)−1QT S−1y
= zSQΛ(zΛ + I)−1QT S−1y
= zMΛ(zΛ + I)−1v, (11)
where M = SQ, and v = M−1y. Finally, we restrict the
estimate to the reference coefficient, as needed for the solution
of (8):
E{xc|y, z} =
N
∑
n=1
zmcnλnvn
zλn + 1
, (12)
where mij represents an element (i-th row, j-th column) of the
matrix M, λn are the diagonal elements of Λ, vn the elements
of v, and c is the index of the reference coefficient within the
neighborhood vector.
C. Posterior distribution of the multiplier
The other component of the solution given in (8) is the distri-
bution of the multiplier, conditioned on the observed neighbor-
hood values. We use Bayes’ rule to compute this:
p(z|y) = p(y|z) pz(z)∫ ∞
0
p(y|α) pz(α) dα
. (13)
As discussed in section II-C, we choose a noninformative Jef-
frey’s prior, corrected at the origin, for the function pz(z). The
conditional density p(y|z) is given in (6), and its computation
may be simplified using the relationship in (10) and the defini-
tion of v:
p(y|z) =
exp(− 12
∑N
n=1
v2
n
zλn+1
)
√
(2π)N |Cw|
∏N
n=1 (zλn + 1)
. (14)
Summarizing our denoising algorithm:
6
1) Decompose the image into subbands
2) For each subband (except the lowpass residual):
a) Compute neighborhood noise covariance, Cw,
from the image-domain noise covariance
b) Estimate noisy neighborhood covariance, Cy
c) Estimate Cu from Cw and Cy using (7)
d) Compute Λ and M (sec III-B)
e) For each neighborhood:
i) For each value z in the integration range:
A) Compute E{xc|y, z} using (12)
B) Compute p(y|z) using (14)
ii) Compute p(z|y) using (13) and (4)
iii) Compute E{xc|y} numerically using (8)
3) Reconstruct the denoised image from the processed sub-
bands and the lowpass residual
IV. IMPLEMENTATION
We decompose the image into subbands using a specialized
variant of the steerable pyramid. The representation consists
of oriented bandpass bands at 8 orientations and 5 scales, 8
oriented highpass residual subbands, and one lowpass (non-
oriented) residual band, for a total of 49 subbands. A detailed
description of the decomposition is given in Appendix I.
We have hand-optimized the neighborhood structure (i.e.,
choice of spatial positions, scales and orientations). A 3× 3 re-
gion surrounding the reference coefficient, together with the co-
efficient at the same location and orientation at the next coarser
scale (the parent), maximizes the denoising performance, on
average. Inclusion of parent coefficient has been found to pro-
vide a significant improvement in performance in a number of
applications [e.g., 48], [23], [24], [27], [28]. Note that since the
parent subband is sampled at half the density of the reference
subband, it must be upsampled and interpolated in order to ob-
tain values for neighborhoods at every choice of reference co-
efficient. Two exceptions must be applied: (1) the highpass ori-
ented subbands, whose parents have the same number of sam-
ples as them (no interpolation is required for those parents); and
(2) the subbands at the coarsest scale, which have no parent sub-
band (we simply use the 3 × 3 spatial neighborhood for those
subbands). Note that in terms of image pixels, the spatial extent
of the neighborhood depends on the scale of the subband (the
basis functions grow in size as 2s) as is appropriate under the
assumption that image statistics are scale-invariant [49], [50].
In our implementation, the integral of (8) is computed nu-
merically. The range and sample spacing for this integration are
chosen as a compromise between accuracy and computational
cost. Specifically, we sample z with logarithmically uniform
spacing, which we have observed to require fewer samples, for
the same quality, than linear sampling. Note also that Jeffrey’s
improper prior for z is a constant under a logarithmic represen-
tation. We use only Sz = 13 samples of log(z) over an interval
[log(zmin), log(zmax)] using steps of size 2. We have chosen
log(zmin) = −20.5 and log(zmax) = 3.5. The value zmax is
chosen as the minimal value that guarantees in practice that the
right-tails of all the posteriors are properly covered by the in-
tegration interval. In contrast, zmin plays the role of ensuring
that the left tail of the posterior is integrable. We have hand-
optimized zmin to maximize the performance of the algorithm,
and have found that denoising performance is relatively insen-
sitive to changes in this parameter. Only slightly worse results
(' −0.01 to −0.05 dB) result from choosing log(zmin) within
the interval [−40,−10], and reasonable performance (' −0.1
to −0.2 dB) is obtained with values as low as −200 (which
corresponds to zmin ' 10−37).
The computational cost of the pyramid transform (both
forward and inverse) scales as Ix Iy log2(IxIy), where
I{x,y} are the dimensions of the image. The com-
putational cost of the estimation procedure scales as
(
Ix +
(Nx+Bx)
2
) (
Iy +
(Ny+By)
2
)
N K Sz, where N{x,y} are
the dimensions of the spatial subband neighborhood (3 in our
case), B{x,y} the dimensions of the bandpass convolution ker-
nels (roughly 9 in our implementation), N the full size of the
neighborhood (10 in our case), K the number of orientations,
and Sz the number of samples used for the distributions over
z. The terms added to the image dimensions correspond to
the padded boundary region that must be estimated in order
to properly reconstruct the image. As a guide, running times
in our current unoptimized Matlab implementation, on a Linux
workstation with 1.7 GHz Intel Pentium-III CPU, are roughly
40 seconds for 256 × 256 images. Finally, the primary mem-
ory cost is due to storage of the pyramid coefficients (roughly
7KNxNy/3 floating point numbers).
V. RESULTS
We have tested our method on a set of 8-bit grayscale test
images, of size 512× 512 and 256× 256 pixels, each contami-
nated with computer-generated additive Gaussian white noise
at 10 different variances. Further information about the im-
ages is provided in Appendix II. TableI shows error variances
of the denoised images, expressed as peak signal-to-noise ra-
tios (PSNR) in decibels, for the full range of input noise levels.
Note that for all images, there is very little improvement at the
lowest noise level. This makes sense, since the “clean” images
in fact include quantization errors, and have an implicit PSNR
of 58.9dB. At the other extreme, improvement is substantial
(roughly 17dB in the best cases).
A. Comparison to model variants
In order to understand the relative contribution of various as-
pects of our method, we considered two restricted versions of
our model that are representative of the two primary denoising
concepts found in the literature. The first is a Gaussian model,
arising from the restriction of our model to a prior density pz(z)
which is a delta function concentrated at one. This model is
not variance-adaptive, and, thus, is globally Gaussian. Note,
though, that the signal covariance is modeled only locally (over
the extent of the neighborhood) for each pyramid subband. As
such, this denoising solution may be viewed as a regularized
version of the classical linear (Wiener filter) solution. In order
to implement this, we simply estimate each coefficient using
(12), with z set to one.
The second restricted form of our model uses a neighborhood
containing only the reference coefficient (i.e., 1 × 1). Under
7
σ / PSNR Lena Barb Boats Fgrpt House Peprs σPSNR
1 / 48.13 48.46 48.37 48.44 48.46 48.85 48.38 0.009
2 / 42.11 43.23 43.29 42.99 43.05 44.07 43.00 0.012
5 / 34.15 38.49 37.79 36.97 36.68 38.65 37.31 0.014
10 / 28.13 35.61 34.03 33.58 32.45 35.35 33.77 0.017
15 / 24.61 33.90 31.86 31.70 30.14 33.64 31.74 0.024
20 / 22.11 32.66 30.32 30.38 28.60 32.39 30.31 0.031
25 / 20.17 31.69 29.13 29.37 27.45 31.40 29.21 0.037
50 / 14.15 28.61 25.48 26.38 24.16 28.26 25.90 0.049
75 / 10.63 26.84 23.65 24.79 22.40 26.41 24.00 0.061
100 / 8.13 25.64 22.61 23.75 21.22 25.11 22.66 0.070
TABLE I
DENOISING PERFORMANCE EXPRESSED AS PEAK SIGNAL-TO-NOISE RATIO, 20 log10(255/σe) IN DB, WHERE σe IS THE ERROR STANDARD DEVIATION.
EVERY ENTRY IS THE AVERAGE USING EIGHT DIFFERENT NOISE SAMPLES. LAST COLUMN SHOWS THE ESTIMATED STANDARD DEVIATION OF THESE
RESULTS FOR EACH NOISE LEVEL.
these conditions, the model describes only the marginal density
of the coefficients, and the estimator reduces to application of a
scalar function to the observed noisy coefficients. The function
resulting from the reduction of our model to a single-element
neighborhood is shown in Fig. 2(a). This is similar to the BLS
solutions derived in [14], [18] for a generalized Gaussian prior,
except that it is independent of the clean signal statistics, and
its normalized form x̂(y)/y scales with the noise standard de-
viation of the subband, σw (as in [11]).
It is also instructive to examine the non-linear estimator as-
sociated with the case of two neighbors. Fig. 2(b) shows the
estimator obtained as a function of the reference coefficient and
a coarse-scale (parent) coefficient. Loosely speaking, the refer-
ence coefficient is suppressed only when both its own amplitude
and the parent’s amplitude are small. Adjacent neighbors in the
same subband have a similar effect on the estimation. Şendur
and Selesnick have recently developed a MAP estimator based
on a circular-symmetric Laplacian density model for a coeffi-
cient and its parent [51], [52]. Their resulting shrinkage func-
tion is qualitatively similar to that of Fig. 2(b), except that ours
is smoother and, due to covariance adaptation, its ”dead zone”
is not necessary aligned with the input axes.
Figure 3(a) shows a comparison of our full model and the
two reduced forms explained above. Note that the 1D shrinkage
solution outperforms the jointly Gaussian (non-adaptive) solu-
tion, which still provides relatively good results, especially at
low SNR rates. The full model (adaptive and context-sensitive)
incorporates the advantages of the two subcases, and thus out-
performs both of them.
We have also examined the relative importance of other as-
pects of our method. Table II shows the decrease in PSNR that
results when each of a set of features is removed. The first
three columns correspond to features of the representation, the
next two to features of the model, and the last to the estimation
method. Within the first group, decreasing the number of orien-
tation bands from K = 8 to K = 4 (Ori8) leads to a significant
drop in performance. We have also found that further increasing
the number of orientations leads to additional PSNR improve-
ment, at the expense of considerable computational and storage
cost. The second column (OrHPR) shows the effect of not par-
titioning the highpass residual band into oriented components
(the standard form of the pyramid, as used in our previous de-
noising work [36], [38], has only a single non-oriented highpass
residual band). The third column (Bdry) shows the reduction in
performance that results when switching from mirror-reflected
extension to periodic boundary handling.
The first feature of the model we examined is the inclusion
of the coarse-scale parent coefficient in the neighborhood. The
fourth column (Prnt) shows that eliminating the coarse-scale
parent from the neighborhood decreases performance signifi-
cantly only at high noise levels. This should not be taken to
mean that the parent coefficient does not provide information
about the reference coefficient, but that the information is some-
what redundant with that provided by the other neighbors [24].
The next column (Cov), demonstrates the result of assuming un-
correlated Gaussian vectors in describing both noise and signal.
The coefficients in our representation are strongly correlated,
both because of inherent spectral features of the image and be-
cause of the redundancy induced by the overcomplete repre-
sentation, and ignoring this correlation in the model leads to a
significant loss in performance. The last column (BLS) demon-
strates a substantial reduction in performance when we replace
the full BLS estimator with the two-step estimator (MAP esti-
mation of the local multiplier, followed by linear estimation of
the coefficient), as used in [38].
B. Comparison to standard methods
We have compared our method to two well-known and
widely-available denoising algorithms: a local variance-
adaptive method in the pixel domain [31] (as implemented by
the Matlab function wiener2), and a hard thresholding method
using an undecimated representation [39] with five scales based
on the minimum-phase Daubechies 8-tap wavelet filter. In both
cases, a single parameter (the neighborhood size or a common
threshold for all the subbands) was optimized independently for
each image at each noise level. Results are shown in Fig. 3(b).
Our method is seen to clearly outperform the other two over
8
NOISY COEFF.
E
S
T
IM
A
T
E
D
 C
O
E
F
F
.
σ
w
−30
0
30
−60
0
60
−30
0
30
NOISY COEFF.NOISY PARENT
E
S
T
IM
A
T
E
D
 C
O
E
F
F
.
(a) (b)
Fig. 2. Nonlinear estimation functions resulting from restriction of our method to smaller neighborhoods. (a) neighborhood of size one (reference coefficient
only); (b) neighborhood of size two (reference coefficient plus parent).
σ / PSNR Ori8 OrHPR Bdry Prnt Cov BLS
10 / 28.13 0.18 0.21 0.12 -0.01 0.47 0.13
25 / 20.17 0.29 0.21 0.15 0.05 0.69 0.30
50 / 14.15 0.29 0.15 0.15 0.09 0.77 0.38
TABLE II
REDUCTION IN DENOISING PERFORMANCE (DB) RESULTING FROM
REMOVAL OF MODEL COMPONENTS, SHOWN AT 3 DIFFERENT NOISE
CONTAMINATION RATES. RESULTS ARE AVERAGED OVER Lena, Barbara
AND Boats. SEE TEXT FOR FURTHER INFORMATION.
the entire range of noise levels. We also see the superiority of
the two multiscale methods over the pixel-domain method. Fig-
ure 4 provides a visual comparison of example images denoised
using these two algorithms. Our method produces artifacts that
are significantly less visible, and at the same time is able to
better preserve the features of the original image.
It is natural to ask to what extent the results in the previ-
ous comparison are due to the representation (steerable pyra-
mid) as opposed to the estimation method itself (BLS-GSM).
In order to answer this, we have performed two more sets of
experiments, comparing the performance of different combina-
tions of representation and estimator. First, we have applied the
two estimation methods used in Fig. 3(b) to the coefficients of
the steerable pyramid representation. For the adaptive Wiener
method [31], we have found that the hand-optimized neigh-
borhood size within the subbands is roughly 19 × 19 – much
larger than in the pixel domain. For the translation invariant
hard-thresholding method [39], we have optimized a common
threshold for each image and noise level (note that the steer-
able pyramid subband impulse responses are not normalized
in energy, so the common threshold needs to be properly re-
scaled for each subband). Results are plotted in Fig. 3(c). It is
clear that the use of the new representation improves the results
and reduces the difference between the methods. The adaptive
Wiener method is even seen to outperform ours at very high in-
put SNRs. But significant differences in performance remain,
and these are due entirely to the use of the BLS-GSM estima-
tion method.
In a second experiment, we compared the performance of
our estimation method when applied to coefficients of two dif-
ferent representations. We have chosen the most widely-used
multiscale representations: the decimated and undecimated ver-
sions of a separable wavelet decomposition. In order to use the
BLS-GSM method under an aliasing-free version of an orthog-
onal wavelet, we have used two fully undecimated levels for the
two highest frequency scales, and have decimated by factors of
two the rest of scales, producing very little aliasing and recon-
struction error. This representation is analogous to the steerable
pyramid: both the highpass oriented subbands and the bandpass
highest frequency oriented subbands are kept at full resolution,
and the rest are downsampled in a dyadic scheme. Results are
plotted in Fig. 3(d), and indicate a somewhat modest decrease
in performance when replacing the steerable pyramid with an
undecimated separable wavelet transform. The decrease is sub-
stantial, however, in the case of the critically sampled repre-
sentation. From the comparison of the outcomes of both sets of
experiments, one may conclude that both our representation and
estimation strategy contribute significantly to the performance
advantage shown in Fig. 3(b).
C. Comparison to state-of-the-art methods
Finally, we have compared our method to some of the best
available published results, and these are shown in Fig. 5. Since
there are many different versions of the test images available on
the Internet, whenever it was possible we have verified directly
with the authors that we are using the same images (Refs. [37],
[53], [54], [52]), or have used other authors’ data included in
previous comparisons from those authors (Refs. [33], [34]) (see
Appendix II for more details about the origin of the images).
9
10 20 30 40 50
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
0.5
INPUT PSNR (dB)
R
E
LA
T
IV
E
 S
N
R
 (
dB
)
Non−adaptive
Single neighbor
10 20 30 40 50
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
0.5
INPUT PSNR (dB)
Thresh/UndWvlt
Wiener2/Pixel
(a) (b)
10 20 30 40 50
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
0.5
INPUT PSNR (dB)
R
E
LA
T
IV
E
 S
N
R
 (
dB
)
Thresh/SteerPyr
Wiener2/SteerPyr
10 20 30 40 50
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
0.5
INPUT PSNR (dB)
BLS−GSM/UndWvlt
BLS−GSM/OrthWvlt
(c) (d)
Fig. 3. Performance of other denoising methods relative to our method. Curves depict PSNR differences (in dB), averaged over three representative images
(Lena, Barbara and Boats) as a function of input PSNR. (a) Comparison to two restricted cases: A non-adaptive (globally Gaussian) GSM model resulting from
using pz(z) = δ(z − 1) (diamonds), and a GSM model with a neighborhood of size one (circles). (b) Comparison to hard-thresholding in an undecimated
(minimum-phase, Daubechies 8-tap, 5 scales) wavelet decomposition (diamonds) [39], and a local variance-adaptive method in the image domain (circles) [31],
as implemented by Matlab’s wiener2 function. Parameters for both methods have been optimized for each image and noise level: a threshold level for the first
method, and a neighborhood size (ranging from 3 to 11) for the second. (c) Two denoising algorithms applied to our steerable pyramid representation: adaptive
Wiener [31], using a hand-optimized size of neighborhood (19 × 19 for all the images and noise levels) (circles), and hard-thresholding, optimizing the threshold
for every image and noise level (diamonds). (d) Application of our BLS-GSM estimation method to coefficients of two different representations: an undecimated
minimum-phase Daubechies 8-tap wavelet, using 5 scales (diamonds), and the same decomposition in its original decimated version (circles).
Figure 6 provides a visual comparison of an example image
(Barbara) denoised using the algorithm of Li et al. [37], which
is based on a variance-adaptive model in an overcomplete sep-
arable wavelet representation. Note that the noisy images were
created using different samples of noise, and thus the artifacts
in the two images appear at different locations. Our method is
seen to provide fewer artifacts as well as better preservation of
edges and other details. The separation of diagonal orientations
in the steerable pyramid allows more selective removal of the
noise in diagonally oriented image regions (see parallel diago-
nal lines on the left side of the face).
VI. CONCLUSIONS
We have presented a denoising method based on a local
Gaussian scale mixture model in an overcomplete oriented
pyramid representation. Our statistical model differs from pre-
vious models in a number of important ways. First, many pre-
vious models have been based on either separable orthogonal
wavelets, or redundant versions of such wavelets. In contrast,
our model is based on an overcomplete tight frame that is free
from aliasing, and that includes basis functions that are se-
lective for oblique orientations. The increased redundancy of
4These two plotted PSNR values have been obtained by Dr. Starck using
the standard Lena image provided by us, which differs from the version used
in [54].
the representation and the higher ability to discriminate orien-
tations results in improved performance. Second, our model
explicitly incorporates the covariance between neighboring co-
efficients (for both signal and noise), as opposed to consider-
ing only marginal responses or local variance. Thus, the model
captures correlations induced by the overcomplete representa-
tion as well as correlations inherent in the underlying image,
and it can handle Gaussian noise of arbitrary power spectral
density. Third, we have included a neighbor from the same ori-
entation and spatial location at a coarser scale (a parent), as
opposed to considering only spatial neighbors within each sub-
band. This modeling choice is consistent with the empirical
findings of strong statistical dependence across scale in natural
images [e.g., 4], [48]. Note, however, that the inclusion of the
parent results in only a modest increase in performance com-
pared to the other elements shown in table II. We believe the
impact of including a parent is limited by the simplicity of our
model, which only characterizes the correlation of the coeffi-
cients and the correlation of their amplitudes (see below).
In addition to these modeling differences, there are also dif-
ferences between our denoising method and previous methods
based on continuous hidden-variable models [34], [36], [30],
[38], [3]. First, we compute the full optimal local Bayesian
least squares solution, as opposed to first estimating the local
variance, and then using this to estimate the coefficient. We
10
(a) Boats
(b) Fingerprint
Fig. 4. Comparison of denoising results on two images (cropped to 128 ×
128 for visibility of the artifacts). (a) Boats image. Top-left: original image.
Top-right: noisy image, PSNR = 22.11dB (σ = 20). Bottom-left: denoising
result using adaptive local Wiener in the image domain [31], PSNR = 28.0dB.
Bottom-right: our method, PSNR = 30.4dB. (b) Fingerprint image. Top-left:
original image. Top-right: noisy image, PSNR = 8.1dB (σ = 100). Bottom-
left: denoising result using hard thresholding in an undecimated wavelet [39]
with a single optimized threshold, PSNR = 19.0dB. Bottom-right: our method,
PSNR = 21.2dB.
20 22 24 26 28
31
32
33
34
35
36 35.61
33.9
32.66
31.69
20 22 24 26 28
28
29
30
31
32
33
34
34.03
31.86
30.32
29.13
(a) Lena (b) Barbara
14 16 18 20 22 24
28
29
30
31
32
33
34 33.64
32.39
31.4
28.26
14 16 18 20 22
25
26
27
28
29
30
30.31
29.21
25.9
(c) House (d) Peppers
Fig. 5. Comparison of denoising performance of several recently published
methods. Curves depict output PSNR as a function of input PSNR. Square
symbols indicate our results, taken from Table I. (a,b) circles [34]; crosses
[37]; asterisks [54]4; diamonds [52]. (c,d) crosses [33]; diamonds [53].
Fig. 6. Comparison of denoising results on Barbara image (cropped to 150×
150 for visibility of the artifacts). From left to right and top to bottom: Original
image; Noisy image (σ = 25, PSNR = 20.2dB); Results of Li et al. [37] (PSNR
= 28.2dB); Our method (PSNR = 29.1dB).
11
have shown empirically that this approach yields an important
improvement in the results. Also, we use the vectorial form
of the LLS solution (Eq. 9), so taking full advantage of the in-
formation provided by the covariance modeling of signal and
noise. These enhancements, together with a convenient choice
for the prior of the hidden multiplier (a noninformative prior,
independent of the observed signal), result in a substantial im-
provement in the quality of the denoised images, while keeping
the computational cost reasonably low.
We are currently working on several extensions of the estima-
tor presented here. First, we have begun developing a variant
of this method to denoise color images taken with a commer-
cial digital camera [55]. We find that the sensor noise of such
cameras has two important features that must be characterized
through calibration measurements: spatial and cross-channel
correlation, and signal-dependence. We are also extending the
denoising solution to address the complete image restoration
problem, by incorporating a model of image blur [56]. Fi-
nally, we are developing an ML estimator for the noise variance,
when the normalized power spectral density of the noise is as-
sumed known. Preliminary results of these extensions appear
very promising.
We believe that the current image model can be improved in
a number of ways. It would be desirable to develop a method
for efficiently estimating a prior for the multiplier by maximiz-
ing the joint likelihood of the observed subbands, as opposed
to the somewhat heuristic choice of noninformative prior (cor-
rected at the origin) we have presented. Similarly, it would also
be desirable to minimize the expected quadratic error in the im-
age domain, instead of doing it for the subband coefficients.
In addition, it is worth exploring the transformation of the lo-
cal GSM model into an explicit Markov model with overlap-
ping neighborhoods, as opposed to the non-overlapping tree-
structured models previously developed [27], [3]. This concep-
tual simplification would facilitate other applications requiring
a conditional local density model (e.g., synthesis or coding). Fi-
nally, from a longer-term perspective, major improvements are
likely to come from statistical models that capture important
structural properties of local image features, by including addi-
tional dependencies such as phase congruency between the co-
efficients of complex multiscale oriented transforms [e.g., 57],
[58].
APPENDIX I
THE STEERABLE PYRAMID
We use a transform known as a steerable pyramid [40],
[41] to decompose images into frequency subbands. The
transform is implemented in the Fourier domain, allowing ex-
act reconstruction of the image from the subbands, as well
as a flexible choice of the number of orientations (K) and
scales (J). A software implementation (in Matlab) is avail-
able at http://www.cns.nyu.edu/∼lcv/software.html. As with con-
ventional orthogonal wavelet decompositions, the pyramid is
implemented by recursively splitting an image into a set of ori-
ented subbands, and a lowpass residual band which is subsam-
pled by a factor of two along both axes. Unlike conventional
orthogonal wavelet decompositions, the oriented bands are not
subsampled, and the subsampling of the lowpass band does not
produce aliasing artifacts, as the lowpass filter is designed to
obey the Nyquist sampling criterion. When performing convo-
lutions, the boundaries are handled by mirror extension (reflec-
tion) of the image, thereby maintaining continuity. Since it is a
tight frame, the transformation may be inverted by convolving
each subband with its associated complex-conjugated filter and
adding the results. The redundancy factor of this overcomplete
representation is (for J → ∞) 73K.
The system diagram for the transform is shown in Fig. 77(a).
The filters are polar-separable in the Fourier domain, where
they may be written as:
L(r, θ) =



cos
(
π
2 log2
(
4r
π
))
, π4 < r <
π
2
1, r ≤ π4
0, r ≥ π2
Bk(r, θ) = H(r)Gk(θ), k ∈ [0,K − 1],
where r, θ are polar frequency coordinates, and
H(r) =



cos
(
π
2 log2
(
2r
π
))
, π4 < r <
π
2
1, r ≥ π2
0, r ≤ π4
Gk(θ) =
(K − 1)!
√
K[2(K − 1)]!
[
2 cos(θ − πk
K
)
]K−1
.
The recursive procedure is initialized by splitting the input im-
age into lowpass and oriented highpass portions, using the fol-
lowing filters:
L0(r, θ) = L(
r
2
, θ), Ak(r, θ) = H(
r
2
)Gk(θ).
Figure 7(b) also shows the impulse response of an example
bandpass oriented filter (for K = 8), at the highest resolution
level, together with its (purely imaginary) Fourier transform.
APPENDIX II
ORIGIN OF THE TEST IMAGES
All 8-bit grayscale test images used in obtain-
ing our results are available on the Internet from
http://decsai.ugr.es/∼javier/denoise. Five of the images, com-
monly known as Lena, Barbara, Boats, House and Peppers, are
widely used in the image processing literature. Unfortunately,
most test images are available in more than one version, with
differences between them due to cropping, scanning, resizing,
compression or conversion from color to gray-level. In the
versions used in this paper the first three are 512 × 512 and the
last two are 256× 256. We also included a 512× 512 image of
a fingerprint, which unlike the other images, is a homogeneous
texture.
Among the several versions of 512 × 512 8-bit gray-
level Lena, we chose the one that seems the most standard,
from http://www.ece.rice.edu/∼wakin/images/Lena512.bmp. For
the comparison of Fig. 5(a), Dr. Starck generously offered
to run his algorithm on our test image, and Dr. Li [37]
and Dr. Şendur [52] kindly confirmed they were using the
same version of the image. The Barbara image was ob-
tained from Marco Schmidt’s standard test images database at
12
L(-ω)
L0(-ω)
BK-1(-ω)
2↑2↓
L0(ω)
B1(-ω)
B0(-ω)
B1(ω)
B0(ω)
L(ω)
BK-1(ω)
A0(-ω)
A1(-ω)
AK-1(-ω)
A0(ω)
A1(ω)
A K-1(ω)
(a) (b)
Fig. 7. (a) System diagram for the extended version of the steerable pyramid used in this paper [40]. The input image is first split into a lowpass band and a set of
highpass oriented bands. The lowpass band is then split into a lower-frequency band and a set of oriented subbands. The pyramid recursion consists of inserting
the diagram contents of the shaded region at the lowpass branch (solid circle). (b) Basis function corresponding to an example oriented subband, and idealized
depiction of the frequency domain partition (K = 8, J = 2), with gray region corresponding to this basis function.
http://jiu.sourceforge.net/testimages/index.html. This version had
been previously used in [37], where, in turn, there is a com-
parison to [34]. It has also been used in [52]. The Boats image
was taken from University of Southern California SIPI image
database at http://sipi.usc.edu/services/database/database.cgi. This
same version has been used in [52]. The House and Peppers
images were kindly provided by Dr. Pižurica, for proper com-
parison to her results reported in [53].
REFERENCES
[1] D Andrews and C Mallows, “Scale mixtures of normal distributions,” J.
Royal Stat. Soc., vol. 36, pp. 99–, 1974.
[2] M J Wainwright and E P Simoncelli, “Scale mixtures of Gaussians and
the statistics of natural images,” in Adv. Neural Information Processing
Systems, S. A. Solla, T. K. Leen, and K.-R. Müller, Eds., Cambridge, MA,
May 2000, vol. 12, pp. 855–861, MIT Press.
[3] M J Wainwright, E P Simoncelli, and A S Willsky, “Random cascades on
wavelet trees and their use in modeling and analyzing natural imagery,”
Applied and Computational Harmonic Analysis, vol. 11, no. 1, pp. 89–
123, Jul 2001, Special issue on wavelet applications.
[4] D L Ruderman, “The statistics of natural images,” Network: Computation
in Neural Systems, vol. 5, pp. 517–548, 1996.
[5] D J Field, “Relations between the statistics of natural images and the
response properties of cortical cells,” J. Opt. Soc. Am. A, vol. 4, no. 12,
pp. 2379–2394, 1987.
[6] J G Daugman, “Entropy reduction and decorrelation in visual coding by
oriented neural receptive fields,” IEEE Trans. Biomedical Engineering,
vol. 36, no. 1, pp. 107–114, 1989.
[7] S G Mallat, “A theory for multiresolution signal decomposition: The
wavelet representation,” IEEE Pat. Anal. Mach. Intell., vol. 11, pp. 674–
693, Jul 1989.
[8] B A Olshausen and D J Field, “Emergence of simple-cell receptive field
properties by learning a sparse code for natural images,” Nature, vol. 381,
pp. 607–609, 1996.
[9] A J Bell and T J Sejnowski, “The ’independent components’ of natural
scenes are edge filters,” Vision Research, vol. 37, no. 23, pp. 3327–3338,
1997.
[10] J P Rossi, “Digital techniques for reducing television noise,” JSMPTE,
vol. 87, pp. 134–140, 1978.
[11] D L Donoho and I M Johnstone, “Ideal spatial adaptation by wavelet
shrinkage,” Biometrika, vol. 81, no. 3, pp. 425–455, 1994.
[12] A Chambolle, R A DeVore, N Lee, and B J Lucier, “Nonlinear wavelet
image processing: Variational problems, compression, and noise removal
through wavelet shrinkage,” IEEE Trans. Image Processing, vol. 7, pp.
319–335, Mar 1998.
[13] D Leporini and J C Pesquet, “Multiscale regularization in Besov spaces,”
in 31st Asilomar Conf on Signals, Systems and Computers, Pacific Grove,
CA, Nov 1998.
[14] E P Simoncelli and E H Adelson, “Noise removal via Bayesian wavelet
coring,” in Third Int’l Conf on Image Proc, Lausanne, Sep 1996, vol. I,
pp. 379–382, IEEE Sig Proc Society.
[15] H A Chipman, E D Kolaczyk, and R M McCulloch, “Adaptive bayesian
wavelet shrinkage,” J American Statistical Assoc, vol. 92, no. 440, pp.
1413–1421, 1997.
[16] F Abramovich, T Sapatinas, and B W Silverman, “Wavelet thresholding
via a Bayesian approach,” J R Stat Soc B, vol. 60, pp. 725–749, 1998.
[17] B Vidakovic, “Nonlinear wavelet shrinkage with Bayes rules and Bayes
factors,” J American Statistical Assoc, vol. 93, pp. 173–179, 1998.
[18] E P Simoncelli, “Bayesian denoising of visual images in the wavelet
domain,” in Bayesian Inference in Wavelet Based Models, P Müller and
B Vidakovic, Eds., chapter 18, pp. 291–308. Springer-Verlag, New York,
Spring 1999, Lecture Notes in Statistics, vol. 141.
[19] P Moulin and J Liu, “Analysis of multiresolution image denoising
schemes using a generalized Gaussian and complexity priors,” IEEE
Trans. Info. Theory, vol. 45, pp. 909–919, 1999.
[20] A Hyvarinen, “Sparse code shrinkage: Denoising of nongaussian data by
maximum likelihood estimation,” Neural Computation, vol. 11, no. 7, pp.
1739–1768, 1999.
[21] J Starck, E J Candes, and D L Donoho, “The curvelet transform for
image denoising,” IEEE Trans. Image Proc., vol. 11, no. 6, pp. 670–684,
Jun 2002.
[22] B Wegmann and C Zetzsche, “Statistical dependence between orientation
filter outputs used in an human vision based image code,” in Proc Visual
Comm. and Image Processing, Lausanne, Switzerland, 1990, vol. 1360,
pp. 909–922.
[23] E P Simoncelli, “Statistical models for images: Compression, restoration
and synthesis,” in Proc 31st Asilomar Conf on Signals, Systems and Com-
puters, Pacific Grove, CA, Nov 1997, pp. 673–678, IEEE Computer So-
ciety, Available from http://www.cns.nyu.edu/∼eero/publications.html.
[24] R W Buccigrossi and E P Simoncelli, “Image compression via joint sta-
tistical characterization in the wavelet domain,” IEEE Trans Image Proc,
vol. 8, no. 12, pp. 1688–1701, Dec 1999.
[25] H Brehm and W Stammler, “Description and generation of spherically
invariant speech-model signals,” Signal Processing, vol. 12, pp. 119–141,
1987.
[26] T Bollersley, K Engle, and D Nelson, “ARCH models,” in Handbook of
Econometrics V, B Engle and D McFadden, Eds. 1994.
[27] M S Crouse, R D Nowak, and R G Baraniuk, “Wavelet-based statisti-
cal signal processing using hidden Markov models,” IEEE Trans. Signal
Proc., vol. 46, pp. 886–902, Apr 1998.
[28] J Romberg, H Choi, and R Baraniuk, “Bayesian tree-structured image
modeling using Wavelet-domain hidden Markov models,” IEEE Trans
Image Proc, vol. 10, no. 7, Jul 2001.
[29] S M LoPresto, K Ramchandran, and M T Orchard, “Wavelet image cod-
13
ing based on a new generalized Gaussian mixture model,” in Data Com-
pression Conf, Snowbird, Utah, Mar 1997.
[30] M K Mihç ak, I Kozintsev, K Ramchandran, and P Moulin, “Low-
complexity image denoising based on statistical modeling of wavelet co-
efficients,” IEEE Trans. on Signal Processing, vol. 6, no. 12, pp. 300–303,
Dec 1999.
[31] J S Lee, “Digital image enhancement and noise filtering by use of local
statistics,” IEEE Pat. Anal. Mach. Intell., vol. PAMI-2, pp. 165–168, Mar
1980.
[32] H Robbins, “The empirical Bayes approach to statistical decision prob-
lems,” Ann. Math. Statistics, vol. 35, pp. 1–20, 1964.
[33] M Malfait and D Roose, “Wavelet-based image denoising using a Markov
random field a priori model,” IEEE Trans. Image Proc., vol. 6, pp. 549–
565, Apr 1997.
[34] S G Chang, B Yu, and M Vetterli, “Spatially adaptive wavelet threshold-
ing with context modeling for image denoising,” in Fifth IEEE Int’l Conf
on Image Proc, Chicago, Oct 1998, IEEE Computer Society.
[35] F Abramovich, T Besbeas, and T Sapatinas, “Empirical Bayes approach
to block wavelet function estimation,” Computational Statistics and Data
Analysis, vol. 39, pp. 435–451, 2002.
[36] V Strela, J Portilla, and E Simoncelli, “Image denoising using a local
Gaussian scale mixture model in the wavelet domain,” Proc. SPIE Vol.
4119, pp. 363-371, Wavelet Applications in Signal and Image Processing
VIII, San Diego, Dec 2000.
[37] X Li and M T Orchard, “Spatially adaptive image denoising under over-
complete expansion,” in IEEE Int’l Conf on Image Proc, Vancouver, Sep
2000, IEEE.
[38] J Portilla, V Strela, M Wainwright, and E Simoncelli, “Adaptive Wiener
denoising using a Gaussian scale mixture model in the wavelet domain,”
in Proc 8th IEEE Int’l Conf on Image Proc, Thessaloniki, Greece, Oct
7-10 2001, pp. 37–40, IEEE Computer Society.
[39] R R Coifman and D L Donoho, “Translation–invariant de–noising,” in
Wavelets and statistics, A Antoniadis and G Oppenheim, Eds. Springer-
Verlag lecture notes, San Diego, 1995.
[40] E P Simoncelli, W T Freeman, E H Adelson, and D J Heeger, “Shiftable
multi-scale transforms,” IEEE Trans Information Theory, vol. 38, no. 2,
pp. 587–607, Mar 1992, Special Issue on Wavelets.
[41] W T Freeman and E H Adelson, “The design and use of steerable filters,”
IEEE Pat. Anal. Mach. Intell., vol. 13, no. 9, pp. 891–906, 1991.
[42] A B Watson, “The cortex transform: rapid computation of simulated
neural images,” Comp. Vis. Graphics Image Proc., vol. 39, pp. 311–327,
1987.
[43] E J Candes and D L Donoho, “Ridgelets: A key to higher-dimensional
intermittency?,” Phil Trans R Soc Lond A, vol. 357, pp. 2495–2509, 1999.
[44] N Kingsbury, “Complex wavelets for shift invariant analysis and filtering
of signals,” Applied and Computational Harmonic Analysis, vol. 10, no.
3, pp. 234–253, May 2001.
[45] V Strela, “Denoising via block Wiener filtering in wavelet domain,” in
3rd European Congress of Mathematics, Barcelona, Jul 2000, Birkhäuser
Verlag.
[46] G E P Box and C Tiao, Bayesian Inference in Statistical Analysis,
Addison-Wesley, Reading, MA, 1992.
[47] M Figueiredo and R Nowak, “Wavelet-based image estimation: An em-
pirical Bayes approach using Jeffrey’s noninformative prior,” IEEE Trans.
Image Proc., vol. 10, no.9, pp. 1322-1331, Sep 2001.
[48] J Shapiro, “Embedded image coding using zerotrees of wavelet coeffi-
cients,” IEEE Trans Sig Proc, vol. 41, no. 12, pp. 3445–3462, Dec 1993.
[49] D L Ruderman, “Origins of scaling in natural images,” Vision Research,
vol. 37, pp. 3385–3398, 1997.
[50] A Tabernero, J Portilla, and R Navarro, “Duality of log-polar image rep-
resentations in the space and the spatial-frequency domains,” IEEE Trans.
Signal Proc., vol. 47, no. 9, pp. 2469–2479, Sep 1999.
[51] L Ş endur and I W Selesnick, “Bivariate shrinkage functions for wavelet-
based denoising exploiting interscale dependency,” IEEE Trans. Signal
Proc., vol. 50, no. 11, pp. 2744–2756, Nov 2002.
[52] L Ş endur and I W Selesnick, “Bivariate shrinkage with local variance
estimation,” IEEE Signal Processing Letters, vol. 9, no. 12, pp. 438–441,
Dec 2002.
[53] A Pižurica, W Philips, I Lemahieu, and M Acheroy, “A joint inter- and
intrascale statistical model for Bayesian wavelet based image denoising,”
IEEE Trans. Image Proc., vol. 11, no. 5, pp. 545–557, May 2002.
[54] J L Starck, D L Donoho, , and E Candes, “Very high quality image
restoration,” in Proc. SPIE conf. Signal and Image Processing,, San
DIego, Aug 2001, vol. 4478, pp. 9–19, A Laine, M Unser and A Aldroubi,
Eds.
[55] J Portilla, V Strela, M Wainwright, and E Simoncelli, “Image denois-
ing using Gaussian scale mixtures in the wavelet domain,” Tech. Rep.
TR2002-831, Courant Inst. of Mathematical Sciences, New York Univer-
sity, Sep 2002.
[56] J Portilla and E P Simoncelli, “Image restoration using Gaussian scale
mixtures in the wavelet domain,” in Proc IEEE Int’l Conf on Image Proc,
Barcelona, Spain, Sep 2003, IEEE Computer Society, To appear.
[57] P D Kovesi, “Image features from phase congruency,” Videre: Journal of
Computer Vision Research, vol. 1, no. 3, Summer 1999.
[58] J Portilla and E P Simoncelli, “A parametric texture model based on joint
statistics of complex wavelet coefficients,” Int’l Journal of Computer
Vision, vol. 40, no. 1, pp. 49–71, 2000.
PLACE
PHOTO
HERE
Javier Portilla received his M.S. in 1994 and his
Ph.D. in 1999, both in Electrical Engineering, from
the Universidad Politécnica de Madrid. From 1995
to 1999 he worked as a research assistant at the Insti-
tuto de Óptica, Consejo Superior de Investigaciones
Cientı́ficas, Madrid. From 1999 to 2001 he was a re-
search associate in Prof. Simoncelli’s laboratory, at
the Center for Neural Science, New York University.
Currently he is an associate investigator within the
Visual Information Processing Group, at the Com-
puter Science and Artificial Intelligence department
of the Universidad de Granada. His research is focused on visual-statistical
representation models for natural images and textures, and their application to
image processing and synthesis.
PLACE
PHOTO
HERE
Vasily Strela received his Ph.D. from the Mas-
sachusetts Institute of Technology. He held visiting
positions at the University of South Carolina, Im-
perial College, Dartmouth College, New York Uni-
vercity, and an assistant professorship at Drexel Uni-
versity. Currently he is working in industry. Vasily’s
research interests include wavelet and multiwavelet
theory, signal processing, financial mathematics.
PLACE
PHOTO
HERE
Martin J. Wainwright received his Ph.D. in Electri-
cal Engineering and Computer Science (EECS) from
Massachusetts Institute of Technology (MIT), Cam-
bridge, MA in January 2002. He is currently a post-
doctoral research associate in EECS at the Univer-
sity of California, Berkeley, CA. His interests include
statistical signal and image processing, variational
methods and convex optimization, machine learning,
and information theory. His doctoral dissertation re-
ceived the George M. Sprowls award from the MIT
EECS department.
PLACE
PHOTO
HERE
Eero P. Simoncelli received the B.S. degree in
Physics in 1984 from Harvard University, studied ap-
plied mathematics at Cambridge University for a year
and a half, and then received the M.S. degree in 1988
and the Ph.D. degree in 1993, both in Electrical Engi-
neering from the Massachusetts Institute of Technol-
ogy. He was an assistant professor in the Computer
and Information Science department at the University
of Pennsylvania from 1993 until 1996. He moved to
New York University in September of 1996, where he
is currently an Associate Professor in Neural Science
and Mathematics. In August 2000, he became an Associate Investigator of the
Howard Hughes Medical Institute, under their new program in Computational
Biology. His research interests span a wide range of issues in the representation
and analysis of visual images, in both machine and biological vision systems.

