Pattern Recognition
Pattern Recognition is a branch of science that concerns the 
description or classification (or identification) of measurements. It is an 
important component of intelligent systems and are used for both data 
processing and decision making.
Measurement
Space,
M
Pattern
Space,
P
Class
Membership
Space,
C
G F
C  = G-1 (P) = G-1 ( F-1(M) ) 
Shape Representation and Recognition
Significant 
Impact
• Mathematics
• Engineering
• Medicine
• Remote sensing
• Computer Science
An Important Area of Research in 
Computer Vision and Visual Perception
Popular Features 
Used for analysis
• Moments
• Euler Number
• Chain Code
• Polygonal  
Approximation
• Distance 
Transform
• B-Spline
• Spectral domain
Application
• Automatic 
Target 
Recognition
• Generic Object 
Recognition
• Scene 
Correlation and 
Matching
• Landmark 
Identification 
from remote    
sensed data
• Biometry
Features must be invariant to:
• Translation
• Rotation
• Scale
• Noise
• Projective (?)
Computational cost must not be high
Must be distinct and unique for a given shape.
Preferably have graceful degradation due to discontinuities and missing parts
Statistical Features
The features used in pattern recognition and segmentation are 
generally geometric or intensity gradient based.
One approach is to work directly with regions of pixels in the 
image, and to describe them by various statistical measures. Such 
measures are usually represented by a single value. These can be 
calculated as a simple by-product of the segmentation procedures 
previously described. 
Such statistical descriptions may be divided into two distinct 
classes. Examples of each class are given below:
• Geometric descriptions: area, length, perimeter, elongation, 
average radius, compactness and moment of inertia. 
• Topological descriptions: connectivity and Euler number. 
Elongation
- sometimes called eccentricity. This is the ratio of the maximum 
length of line or chord that spans the region to the minimum length chord. 
We can also define this in terms of moments, as we will see shortly. 
Compactness
- this is the ratio of the square of the perimeter to the area of the 
region
Connectivity -
- the number of neighboring features adjoining the region. 
Euler Number
- for a single region, one minus the number of holes in that region. 
The Euler number for a set of connected regions can be calculated as the 
number of regions minus the number of holes. 
Elongatedness: 
A ratio between the length and width of the region bounding 
rectangle = a/b = Area/sqr(thickness).
Compactness 
Compactness is independent of linear transformations 
= sqr(perimeter)/Area
Moments of Inertia
The ij-th discrete central moment mij, of a region is defined by:
where the sums are taken over all points (x, y) contained within the 
region and (x~, y~) are the center of gravity of the region:
Note that, n, the total number of points contained in the region, is 
a measure of its area ( = μ00 ).
 −−= jiij yyxx )()(
~~
μ
 == i ii i ynyandxnx
1     1
~~
We can form seven new moments from the central moments that 
are invariant to changes of position, scale and orientation ( RTS ) of the 
object represented by the region, although these new moments are not
invariant under perspective projection. Let, the normalized unscaled 
central moments be:
Up to order seven, the RST-invariant moments are: 
( ) 1)2(; +
+== qpm
oo
pq
pq γμ
μ
γ
We can also define eccentricity, using moments as 
We can also find principal axes of inertia that define a natural 
coordinate system for a region. It is given by:
Geometric properties in terms of moments:
00
01
~
00
10
~
00  ;   ; m
my
m
mxmArea ===
Axis of minimal inertia
tan2α =  2mxy/(my −mx)
Polar Signature, Skeletons (MAT), B-splines are also used.
Θ  
R
ad
ia
l  
  
D
is
ta
nc
e,
 ρ
Polar Signature
Phase of DFT-based Signature Function
iii jyxz +=
Coordinates  
of the  
boundary points 
of the shape 
are expressed 
as: (x0, y0),
(x1, y1)…….
(xN, yN) 
OR
Coefficient 
of the 
Fourier 
Transform
is given as
 −=
n
nj
k
j ezeZ )()( ωω
Shape is rotated by 
an angle θ, and the 
starting point by l0
)2(' 0ljmj
mm eeZZ
πθ −=
mj
mm eRZ
θ=
Shift in phase, due to 
rotation of the shape 
and change in starting 
point is 
Nmlmm /2 0
' πθθθ −+=
This leads to the 
derivation of the 
modified DFT 
coefficients for 
normalization 
against the 
scaling, rotation 
and shift in the 
starting point as 
depicted in the 
table below:
Invariance Modified coefficients
Translation Z’0 = 0
Scale R’m = Rm /S
Rotation Θ’m = Θm+ θ - (Θ-1 + Θ+1 )/2
Starting point Θ’m = Θm + m(Θ-1 - Θ+1 )/2
Read about :
• CSS,  a multi-scale representation;
• MCC
• Wavelet based descriptors
• Contour descriptor moments
• Distance functions – Hausdorff
• Shape Context
Skeletons - MAT 
An Example of Classification
• “Sorting incoming Fish on a conveyor 
according to species using optical sensing
Species
Sea bass
Salmon
– Some properties that could be possibly used to 
distinguish between the two types of fishes is
• Length
• Lightness
• Width
• Number and shape of fins
• Position of the mouth, etc…
– This is the set of all suggested features to explore for use 
in our classifier!
Features
Feature is a property (or characteristics) of 
an object (quantifiable or non quantifiable) which 
is used to distinguish between (or classify) two 
objects.
Feature vector
• A Single feature may not be useful always for 
classification
• A set of features used for classification form a feature 
vector
Fish xT = [x1, x2]
Lightness Width
Feature space
• The samples of input (when represented by their features) are 
represented as points in the feature space
• If a single feature is used, then work on a one- dimensional feature 
space.
Point representing samples
• If number of features is 2, then we get points in 2D-
space as shown in the next slide.
• We can also have an n-dimensional feature space
Decision boundary in one-dimensional case with two 
classes.
Decision boundary in 2 (or 3) 
dimensional case with three 
classes
Area
Perimeter
Elongation
Compactness
Area
Elongation
F1
F2
Sample points in a two-dimensional feature space
Class 1
Class 2
Class 3
Some Terminologies:
• Pattern
• Feature
• Feature vector
• Feature space
• Classification
• Decision Boundary
• Decision Region
• Discriminant function
• Hyperplanes and Hypersurfaces
• Learning
• Supervised and unsupervised
• Error
• Noise
• PDF
• Baye’s Rule
• Parametric and Non-parametric approaches
Decision region and Decision Boundary
• Our goal of pattern recognition is to reach an optimal 
decision rule to categorize the incoming data into their 
respective categories
• The decision boundary separates points belonging to one 
class from points of other
• The decision boundary partitions the feature space into 
decision regions.
• The nature of the decision boundary is decided by the 
discriminant function which is used for decision. It is a 
function of the feature vector.
Hyper planes and Hyper surfaces
• For two category case, a positive value of discriminant 
function decides class 1 and a negative value decides the 
other.
• If the number of dimensions is three. Then the decision 
boundary will be a plane or a 3-D surface. The decision 
regions become semi-infinite volumes
• If the number of dimensions increases to more than three, 
then the decision boundary becomes a hyper-plane or a 
hyper-surface. The decision regions become semi-infinite 
hyperspaces.
Learning
• The classifier to be designed is built using input samples 
which is a mixture of all the classes.
• The classifier learns how to discriminate between samples 
of different classes. 
• If the Learning is offline i.e. Supervised method then, the 
classifier is first given a set of training samples and the 
optimal decision boundary found, and then the 
classification is done.
• If the learning is online then there is no teacher and  no 
training samples (Unsupervised). The input samples are 
the test samples itself. The classifier learns and classifies at 
the same time.
Error
• The accuracy of classification depends on two 
things
– The optimality of decision rule used: The central task is 
to find an optimal decision rules which can generalize to 
unseen samples as well as categorize the training samples 
as correctly as possible. This decision theory leads to a 
minimum error-rate classification.
– The accuracy in measurements of feature vectors: This 
inaccuracy is because of presence of noise. Hence our 
classifier should deal with noisy and missing features too.
Classifier Types
Statistical Syntactic Neural
Supervised or Unsupervised
Categories of Statistical Classifiers:
• Linear
• Quadratic
• Piecewise
• Non-parametric
Parametric Decision making (Statistical) - Supervised
Goal of most classification procedures is to estimate the probabilities 
that a pattern to be classified belongs to various possible classes, based on the 
values of some feature or set of features. 
In most cases, we decide which is the most likely class. We need a 
mathematical decision making algorithm, to obtain classification.
Bayesian decision making or Bayes Theorem
This method refers to choosing the most likely class, given 
the value of the feature/s. Bayes theorem calculates the probability 
of class membership. 
Define:
P(wi) - Prior Prob. for class wi ;        P(X) - Prob. (Uncondl.) for  feature vector X.
P(wi |X) - Measured-conditioned or posteriori probability
P(X | wi) - Prob. (Class-Condnl.) Of feature vector X in class wi
)(
)()|()|(
XP
wPwXPXwP iii =Bayes Theorem:
P(X) is the probability distribution for feature X in the entire 
population. Also called unconditional density function (or evidence).
P(wi) is the prior probability that a random sample is a 
member of the class Ci. 
P(X | wi)  is the class conditional probability (or likelihood) 
of obtaining  feature value X given that the sample is from class wi. 
It is equal to the number of times (occurrences) of X, if it belongs to 
class wi. 
The goal is to measure:  P(wi |X) –
Measured-conditioned or posteriori probability, 
from the above three values. 
This is the Prob. of any vector X 
being assigned to class wi. 
BAYES RULEP(w) P(w|X)
X, P(X)
P(X|w)
Take an example:
Two class problem:  
Cold (C) and not-cold (C’).  Feature is fever (f).
Prior probability of a person having a cold, P(C) = 0.01.
Prob. of having a fever, given that a person has a cold is, 
P(f|C) = 0.4.     Overall prob. of fever P(f) = 0.02.
Then using Bayes Th., the Prob. that a person has a cold, given 
that she (or he) has a fever is:
2.0
02.0
01.0*4.0
)(
)()|()|( ===
fP
CPCfPfCP
let us take an example with values to verify:
Total Population =1000. Thus, people having cold = 10. People having 
both fever and cold = 4. Thus, people having only cold = 10 – 4 = 6. 
People having fever (with and without cold)  = 0.02 * 1000 = 20. 
People having fever without cold  = 20 – 4 = 16 (may use this later).
So, probability (percentage) of people having cold along with fever, 
out of all those having fever, is: 4/20 = 0.2 (20%).
IT WORKS, GREAT
Not convinced that it works?
C f
P(C) = 0.01
P(f) = 0.02
P(C and f) = P(C).P(f|C) = 0.004
Probability of a joint event - a sample comes from class C and 
has the feature value X:
P(C and X) = P(C).P(X|C) =   P(X).P(C|X)
=  0.01*0.4      =   0.02*0.2
A Venn diagram,
illustrating the
two class,
one feature problem.
Also verify, for a K class problem:
P(X) = P(w1)P(X|w1) + P(w2)P(X|w2) + ……. + P(wk)P(X|wk) 
Thus:
)|()(....)|()()|()(
)()|()|(
2211 kk
ii
i wXPwPwXPwPwXPwP
wPwXPXwP
+++
=
With our last example:
P(f)  = P(C)P(f|C) + P(C’)P(f|C’) 
= 0.01 *0.4 +  0.99 *0.01616    =   0.02
Decision or Classification algorithm according to Baye’s Theorem:



>
>
)()|()()|( if   ;
)()|()()|( if   ;
    
11222
22111
wpwXpwpwXpw
wpwXpwpwXpw
Choose
Errors in decision making:
Let d = 1, C = 2,
P(C1) = P(C2) = K;
Bayes decision rule:
Choose C1 , if P(x|C1) > P(x|C2)
μ1 μ2α x 
P(x|C1) P(x|C2)
This gives α, and hence the 
two decision regions.
Classification error (the shaded region – minimum of the two curves):
P(E) = P(Chosen C1, when x belongs to C2) + 
P(Chosen C2, when x belongs to C1) 
= 
−∞
∞−
+
α
α
γγγγ dCPCPdCPCP )|()()|()( 1122
])(
2
1exp[
2
1)|( 2
σ
μ
πσ
i
i
xCxp −−=
A minimum distance (NN) supervised classifier
Rule: Assign X to Ri, where X is closest to μi.
An example of 1-D DRs: 
R1 and R2.
An example of 2-D DRs: 
R1 and R2; with a non-linear DB.
Decision based on
arbitrary Posteriors,
for an example:
Apples
Vs. Oranges.
Commonly used Discriminant functions
based on Baye’s decision rule:
Some examples of dense distribution of instances, 
with non-linear decision boundaries
K-means Clustering (unsupervised)
• Given a fixed number of k clusters, assign observations to 
those clusters so that the means across clusters for all 
variables are as different from each other as possible.
• Input
– Number of Clusters, k 
– Collection of n, d dimensional vectors xj , j=1, 2, …, n
• Goal: find the k mean vectors μ1, μ2, …, μk
• Output
– k x n binary membership matrix U where


 ∈
=
else    0
 if     1 ii
ij
Gx
u
& Gj, j=1, 2, …, k represent the k clusters
If n is the number of known patterns and c the 
desired number of clusters, the k-means algorithm is:
Begin
initialize n, c, μ1,μ2,…,μc(randomly 
selected)
do
1.classify n samples according
to nearest μi
2.recompute μi
until no change in μi
return μ1, μ2, …, μc
End
Classification Stage
• The samples have to be assigned to clusters in order to 
minimize the cost function which is:
  
= = ∈ 







−==
c
i
c
i Gxk
iki
ik
xJJ
1 1 ,
2μ
• This is the Euclidian Distance of the samples from 
its cluster center; for all clusters this sum should 
be minimum
• The classification of a point xk is done by:



 ≠∀−≥−=
otherwise         0
 ,    if          1
22 ikxxu jkiki
μμ
Re-computing the Means
• The means are recomputed according to:








= 
∈ ik Gxk
k
i
i xG ,
1μ
• Disadvantages
• What happens when there is overlap between classes… 
that is a point is equally close to two cluster centers…… 
Algorithm will not terminate
• The Terminating condition is modified to “Change in 
cost function (computed at the end of the Classification) 
is below some threshold rather than 0”.
An Example
• The no of clusters is 
two in this case.
• But still there is 
some overlap
Some necessary elements of 
Probability theory and Statistics
The NORMAL DISTRIBUTION
The normal (or Gaussian) distribution, is a very 
commonly  used (occurring) function in the fields of 
probability theory, and has wide applications in the 
fields of:
- Pattern Recognition;
- Machine Learning;
- Artificial Neural Networks and Soft computing;
- Digital Signal  (image, sound , video etc.) processing
- Vibrations, Graphics etc.
Its also called a BELL function/curve. 
The formula for the normal distribution is:
])(
2
1exp[
2
1)( 2
σ
μ
πσ
−−= xxp
The parameter μ is called the mean or expectation (or 
median or mode) of the distribution. 
The parameter σ is the standard deviation; 
and variance is thus σ2.
])(
2
1exp[
2
1)( 2
σ
μ
πσ
−−= xxp
https://en.wikipedia.org/wiki/File:Normal_Distribution_PDF.svg
(2013)
X 
P(
x)
 
The 68 – 95 - 99.7% Rule:
All normal density curves satisfy the following property 
which is often referred to as the Empirical Rule: 
- 68% of the observations fall within
1 standard deviation of the mean, 
that is, between
- 95% of the observations fall within
2 standard deviations of the mean, 
that is, between 
- 99.7% of the observations fall within 
3 standard deviations of the mean, 
that is, between
 )(  and )( σμσμ +−
 )2(  and )2( σμσμ +−
 )3(  and )3( σμσμ +−
])(
2
1exp[
2
1)( 2
σ
μ
πσ
−−= xxp
The normal distribution p(x), with any mean μ and 
any positive deviation σ, has the following properties:
• It is symmetric around the mean (μ) of the distribution.
• It is unimodal: its first derivative is positive for x < μ, 
negative for x > μ, and zero only at x = μ.
• It has two inflection points (where the second derivative 
of f is zero and changes sign), located one standard 
deviation away from the mean,  x = μ − σ and x = μ + σ.
• It is log-concave.
• It is infinitely differentiable, indeed supersmooth of 
order 2.
Also, the standard normal distribution 
p (with μ = 0 and σ = 1) also has the following properties:
• Its first derivative p′(x) is:    −x.p(x).
• Its second derivative p′′(x) is:    (x2 − 1).p(x)
• More generally, its n-th derivative :
p(n)(x) is:                 (-1)nHn(x)p(x),
where, Hn is the Hermite polynomial of order n.
Normal Density: ])(
2
1exp[
2
1)( 2
σ
μ
πσ
−−= xxp
Bivariate Normal Density:
)1(2
),(
2
])(
))((2
)[(
)1(2
1 22
2
xyyx
yyxx
y
y
yx
yxxy
x
x
xyeyxp
ρσπσ
σ
μ
σσ
μμρ
σ
μ
ρ
−
=
−
+
−−
−−
−
−
tCoefficienn Correlatio -       S.D.;   -      ;Mean   -  xyρσμ
Visualize ρ as equivalent to the orientation of the 2-D Gabor filter.
For x as a discrete random variable, 
the expected value of x: 
=
==
n
i
xii xPxxE
1
)()( μ
E(x) is also called the first moment of the distribution.
The kth moment is defined as:

=
=
n
i
i
k
i
k xPxxE
1
)()(
P(xi) is the probability of x = xi. 
Second, third,…  moments of the distribution p(x) are the expected values of: 
x2, x3,… 
The kth central moment is defined as:

=
−=−
n
i
i
k
x
k
x xPxxE
1
)()(])[( μμ
Thus, the second central moment (also called Variance) of a random variable x is 
defined as:
])[(])}([{ 222 xx xExExE μσ −=−=
S.D. of x is σx.
If z is a new variable: z= ax + by; Then E(z) = E(ax + by)=aE(x) + bE(y).
22222
222
)(2)(
])[(])}([{
xxx
xx
xExE
xExExE
μμμ
μσ
−=+−=
−=−=
222 )( μσ +=xE
Thus
Covariance of x and y, is defined as: )])([( yxxy yxE μμσ −−=
Covariance indicates how much x and y vary together. The value 
depends on how much each variable tends to deviate from its mean, and also 
depends on the degree of association between x and y.
Correlation between x and y: )])([(
y
y
x
x
yx
xy
xy
yxE
σ
μ
σ
μ
σσ
σ
ρ
−−==
11 ≤≤− xyρProperty of correlation coefficient:
For Z = ax + by ;
22222
22222
   ,0   
;2])[(
yxzxy
yxyxz
baIf
babazE
σσσσ
σσσμ
+==
++=−
The correlation coefficient can also be viewed as the cosine of the angle 
between the two vectors (R D) of samples drawn from the two random variables.
This method only works with centered data, i.e., data which have been 
shifted by the sample mean so as to have an average of zero. 
)])([(
y
y
x
x
yx
xy
xy
yxE
σ
μ
σ
μ
σσ
σ
ρ
−−==
Other PDFs:
0   
!
)( ; >= − λλ λe
x
xP
x
Poisson 
 Binomial Cauchy
LAPLACE:
;
2
1)( b
ax
e
b
xP
−−
=
Double Exponential Density:
Read about:
• Central Limit Theorem
• Uniform Distribution
• Geometric Distribution
• Quantile-Quantile (QQ) Plot
• Probability-Probability (P-P) Plot
Sample mean is defined as: 
==
==
n
i
i
n
i
ii xn
xPxx
11
~ 1)( where, 
P(xi) = 1/n.
Sample Variance is: 
=
−=
n
i
ix xxn 1
2
~
2 )(1σ
Higher order moments may also be computed:
4
~
3
~
)(  ;)( xxExxE ii −−
Covariance of a bivariate distribution:

=
−−=−−=
n
i
yxxy yyxxn
yxE
1
~~
))((1)])([( μμσ
PROB. & STAT. Contd.
MAXIMUM LIKELIHOOD ESTIMATE (MLE)
The ML estimate (MLE) of a parameter is that value which, when substituted 
into the probability distribution (or density), produces that distribution for which 
the probability of obtaining the entire observed set of samples is maximized.
Problem:     Find the maximum likelihood estimate for μ in a normal distribution.
Normal Density: ])(
2
1exp[
2
1)( 2
σ
μ
πσ
−−= xxp
Assuming all random samples to be independent:
=
Π==
=
)()().....(),,,,(
111 i
n
inn
xpxpxpxxp
])(
2
1exp[
)2(
1
1
2
22/ 
=
−−
n
i
nn
x
σ
μ
σπσ
Taking derivative (w.r.t. μ ) 
of the LOG of the above:

==
−=−
n
i
i
n
i
i nxx
1
2
1
2 ][
12).(
2
1 μ
σ
μ
σ
Setting this term = 0, we get:
~
1
1 xx
n
n
i
i == 
=
μ
Also read about MAP estimate – Baye’s is an example.
Multi-variate Case:   X = [x1 x2 …… xd]T
Mean vector:
















==
d
XE
μ
μ
μ
μ
.
.)(
2
1
Covariance matrix (symmetric):
















=
















=
2
21
2
2
212
112
2
1
21
22221
11211
..
.....
.....
..
..
..
.....
.....
..
..
ddd
d
d
dddd
d
d
σσσ
σσσ
σσσ
σσσ
σσσ
σσσ
d-dimensional normal density is:
)]()(
2
1exp[
)2)(det(
1          
]
2
)()(
exp[
)2)(det(
1)(
1
jj
ij
ijiid
T
d
xsx
XX
Xp
μμ
μμ
π
π
−−−
Σ
=
−Σ−
−
Σ
=

−
)]()(
2
1exp[
)2)(det(
1          
]
2
)()(
exp[
)2)(det(
1)(
1
jj
ij
ijiid
T
d
xsx
XX
Xp
μμ
μμ
π
π
−−−
Σ
=
−Σ−
−
Σ
=

−
where, sij is the i-jth component of Σ
−1 (the inverse of covariance matrix Σ).
Special case, d = 2; where X = (x y)T; Then:
and






=
y
x
μ
μμ








=







= 2
2
2
2
yyxxy
yxxyx
yxy
xyx
σσσρ
σσρσ
σσ
σσ
Can you now obtain this, 
as given earlier: 
)1(2
),(
2
])(
))((2
)[(
)1(2
1 22
2
xyyx
yyxx
y
y
yx
yxxy
x
x
xyeyxp
ρσπσ
σ
μ
σσ
μμρ
σ
μ
ρ
−
=
−
+
−−
−−
−
−
















==
d
XE
μ
μ
μ
μ
.
.)(
2
1
        
);()(
)(
1 μμ XX
Xd
D
T −Σ−
=
−
Contours have constant density
of the distant term (d=2):
The contours are lines of constant Mahalanobis distance (determined 
by the matrix Σ), and are quadratic functions.
The contours of constant density may also be hyper-ellipsoids (non-
diagonal Σ) of constant Mahalanobis distance to μ.
















=
















=
2
21
2
2
212
112
2
1
21
22221
11211
..
.....
.....
..
..
..
.....
.....
..
..
ddd
d
d
dddd
d
d
σσσ
σσσ
σσσ
σσσ
σσσ
σσσ
Diagonal covariance;
;0
;
=
=
xy
yx
ρ
σσ
Diagonal covariance;
;0
;
=
>
xy
yx
ρ
σσ
Non-Diagonal 
covariance;
;0
;
>
=
xy
yx
ρ
σσ
;0
;
<
=
xy
yx
ρ
σσ
Remember, 
asymmetric and oriented
Gaussians

Decision Regions and Boundaries
A classifier partitions a feature space into class-labeled 
decision regions (DRs). 
If  decision regions are used for a possible and unique class 
assignment, the regions must cover Rd and be disjoint (non-
overlapping. In Fuzzy theory, decision regions may be overlapping.
The border of each decision region is a Decision Boundary (DBs).
Typical classification approach is as follows:
Determine the decision region (in Rd) into which X falls, and 
assign X to this class.
This strategy is simple. But determining the DRs is a 
challenge.
It may not be possible to visualize, DRs and DBs, in a general 
classification task with a large number of classes and higher feature 
space (dimension).
Classifiers are based on Discriminant functions. 
In a C-class case,  Discriminant functions are denoted by:
gi(X), i = 1,2,…,C.
This partitions the Rd into C distinct (disjoint) regions, and the 
process of classification is implemented using the Decision Rule:
Assign X to class Cm (or region m), where: .,),()( miiXgXg im ≠∀>
Decision Boundary is defined by the locus of points, where:  
lkXgXg lk ≠= ),()(
Minimum distance (also NN) classifier:
Discriminant function is based on the distance to the class mean:
2211 )(   ;)( μμ −=−= XXgXXgμ1
μ2
R1
R2
This does not take into account 
class PDFs and priors.
)(
)()|()|(
XP
wPwXPXwP iii =Remember Baye’s:
Consider 
discriminant function as:
and class-conditional Prob. as:
]
2
)()(
exp[
)2)(det(
1)|(
1 μμ XX
wXp i
T
d
i
i
−Σ−
−
Σ
=
−
π
Many cases arise, due to the varying nature of Σ:
• Diagonal (equal or unequal elements);
• Off-diagonal (+ve or –ve).
qdk
iμXiμXCXPXG
i
i
T
d
i
ii
+=
−Σ−
−
Σ
==
−
2
1
.           
2
)()(
]
)2)(det(
1log[)]|(log[)(
π
Let the discrimination function for the ith class be:
 .;,),(  )( assume and   ),|()( jijiCPCPXCPXg jiii ≠∀==
]
2
)()(
exp[
)2)(det(
1)|()(
1
iμXiμXCXPXg
i
T
d
i
ii
−Σ−
−
Σ
==
−
π
Remember, multivariate Gaussian density?
Define:
)()( 1
2
iμXiμXd i
T
i −Σ−= −
Thus the classification is now influenced by the square 
distance (hyper-dimensional) of X from μi, weighted by the Σ-1. 
Let us examine:
This quadratic term (scalar) is known as the 
Mahalanobis distance (the distance from X to μi in feature space). 
)()( 1
2
iμXiμXd i
T
i −Σ−= −
For a given X, some Gm(X) is largest where (dm)2 is the 
smallest, for a class i = m (assign X to class m, based on NN Rule) . 
Simplest case:  Σ = I, the criteria becomes the Euclidean 
distance norm (and hence the NN classifier).
This is equivalent to obtaining the mean μm, for which X is 
the nearest, for all μi. The distance function is then:
2
 and   ,
2/)(2/)(2/)(    Thus,
notations) vector (all   2
0
0
2
22
i
T
i
ii
T
i
i
T
i
i
T
i
T
i
T
ii
T
i
T
i
T
ii
where
X
XXXdXG
XXXXd
μμωμω
ωω
μμμ
μμμμ
−==
+=
+−==
+−=−=
Neglecting the class-invariant term.
This gives the simplest 
linear discriminant function 
or correlation detector.
x1
x2
xd
w1
w2
wd wi0
O(X)
The perceptron (ANN) built to form the linear discriminant function
0)()( i
i
ii wxwXO += 
View this as (in 2-D space):
CYMXG +−=
Generalized results (Gaussian case) of a discriminant function:
)log(
2
1)2log()
2
()()(
2
1
2
)()(
]
)2)(det(
1log[)]|(log[)(
1
1
ii
T
i
T
d
i
ii
d
iμXiμX
iμXiμXCXPXG
−−−Σ−−=
−Σ−
−
Σ
==
−
−
π
π
The mahalanobis distance (quadratic term) spawns a number 
of different surfaces, depending on Σ-1. It is basically a vector 
distance using a Σ-1 norm. It is denoted as:
The decision region boundaries are determined by solving :
0)()(: ),()( 00
T
j
T
igiveswhich =−+−= jiji XXGXG ωωωω
This is an expression of a hyperplane separating the decision 
regions in Rd. The hyperplane will pass through the origin, if:
00 ji ωω =
2
1−
−
i
iX μ
Make the case of Baye’s rule more general for class assignment. 
Earlier we has assumed that:
 .;,),(  )( assuming   ),|()( jijiCPCPXCPXg jiii ≠∀==
Now, )]log[P()]|(log[ )]().|(log[)( iiii CCXPXPXCPXG +==
)](log[)log(
2
1)()(
2
1
)](log[)log(
2
1)2log()
2
()()(
2
1
)](log[
2
)()(
]
)2)(det(
1log[)(
1
1
1
iii
T
iii
T
i
i
T
d
i
i
CPiμXiμX
CPdiμXiμX
CPi
μXiμXXG
+−−Σ−−=
+−−−Σ−−=
+
−Σ−
−
Σ
=
−
−
−
π
π
Neglecting the
constant term
Simpler case:  Σi = σ2I, and eliminating the class-independent bias, 
we have:
)](log[)()(
2
1)( 2 i
T
i CPiμXiμXXG +−−−= σ
These are loci of constant hyper-spheres, centered at class mean.
More on this later on…..
If Σ is a diagonal matrix, with equal/unequal σii2:




















=
















= −
2
2
2
2
1
1
2
2
2
2
1
1..00
.....
.....
0..10
0..01
    
..00
.....
.....
0..0
0..0
d
d
and
σ
σ
σ
σ
σ
σ
Considering the discriminant function:
)](log[)log(
2
1)()(
2
1)( 1 iii
T
i CPiμXiμXXG +−−Σ−−=
−
This now will yield a weighted distance classifier. Depending 
on the covariance term (more spread/scatter or not), we tend to put 
more emphasis on some feature vector components than the other.
Check out the following:
This will give hyper-elliptical surfaces in Rd, for each class.
It is also possible to linearise it.
iμ
T
iμX
T
iμXiG
iμ
T
iμX
T
iμXXiμXiμXd iii
T
i
T
i
11
11112
2
1)()(
2)()(
−−
−−−−
Σ−Σ=
Σ−Σ+Σ−=−Σ−=
More general decision boundaries
Take P(Ci) = K for all i, and eliminating the class independent 
terms yield:
)()()( 1 iμXiμXXG i
T
i −Σ−=
−
as Σi =Σ, and are symmetric.
i
T
iiii
i
T
ii
where
XXG
μμωμω
ωω
1
0
1
0
2
1 and   
)(    Thus,
−− Σ−=Σ=
+=
Thus the decision surfaces are hyperplanes and decision 
boundaries will also be linear (use Gi(X) = Gj(X), as done earlier)
Beyond this, if a diagonal Σ is class-dependent or off-diagonal terms 
are non-zero, we get non-linear DFs, DRs or DBs.
The discriminant function (DF) for linearly separable classes is:
0)( i
T
ii XXg ωω +=
where, ωi is a dx1 vector of weights used for class i.
This function leads to DBs that are hyperplanes. It’s a point in 
1D, line in 2-D, planar surfaces in 3-D, and …….  .
3-D case:
0)(
3
2
1
321 =










x
x
x
ωωω is a plane passing through the origin.
0;0)( =−=>=− dXXX Td
T ωωIn general, the equation:
represents a plane H passing through any point (position vector) Xd. 
This plane partitions the space into two mutually exclusive regions, 
say Rp and Rn. The assignment of the vector X 
to either the +ve side, or 
–ve side or along H, 
can be implemented by:





∈<
∈=
∈>
−
n
p
T
RX
HX
RX
dX
 if   0
 if   0
 if   0
ω
A relook at,
Linear Discriminant Function g(X):
dXXg T −= ω)(
Orientation of H is determined by ω.
Location of H is determined by d.
H is a hyperplane for d > 3. The figure shows a 2D representation.
ω
Xd
H
+ve side, Rp
-ve side, Rn
x1
x2
XTW=0
Xk
H’
w1
w2
WTX=0The complementary role of 
a sample in parametric space:
Pattern/feature Space
Weight 
Space
ω
Xd
H
C1
C2
x1
x2
Xk
H’
w1
w2
WTX=0
w1
w2
X3
X1
X2
X4
T1 = [X1, X2];
T2 = [X3, X4];
XTW=0
SOLUTION
SPACE
w1
w2
X1
X2
X3
X4
T1 = [X1, X2];
T2 = [X3, X4];
0)( 1 >Tg
0)( 2 <Tg
LMS learning Law in BPNN or FFNN modelsx1
x2
xd
w1
w2
wd wi0
O(X)
Read about perceptron
vs. multi-layer feedforward network
0X if
0X if
   T
k
T
k
1 ≥
≤


 +
=+
k
k
k
kkk
k W
W
W
XW
W
η
ηκ is the learning rate parameter
Xk
H
w1
w2
0=k
T XW
Wk
Wk+1
0 and   if
0 and   if
   
0
1
1 ≥Χ∈
≤Χ∈



−
+
=+
k
T
kk
k
T
kk
kkk
kkk
k WXX
WXX
XW
XW
W
η
η
w1
w2
X1
X2
X3
X4
T1 = [X1, X2];
T2 = [X3, X4];
0 and   if
0 and   if
   
0
1
1 ≥Χ∈
≤Χ∈



−
+
=+
k
T
kk
k
T
kk
kkk
kkk
k WXX
WXX
XW
XW
W
η
η
ηκ decreases with each iteration
In case of FFNN, the objective is to minimize the error term:
k
T
kkkkk WXdsde −=−=
^
:Algorithm Learning 
kkk XeW
LMS
η
α
=Δ
−
Xk
Wk
Wk+1
ΔWk
MSE error surface (in case of multi-layer perceptron):
.)2/1(2/][
2
1 2 RWWWPEWXd TTk
T
kkk +−=−=ξ
RWP
www
T
n
+−==∇ ),......,,(
10 δ
δξ
δ
δξ
δ
δξξ
PRW
Thus
1
^
,
−=
















==
=
k
n
k
n
kk
n
k
n
k
n
kkkk
k
n
k
T
kk
T
kk
T
xxxxx
xxxxx
xx
EXXER
XdEP
1
1111
11
[][
];[
Effect of class Priors – revisiting DBs in a more general case.
)(
)()|()|(
XP
wPwXPXwP iii =
]
2
)()(
exp[
)2)(det(
1
)|(
1 μμ XX
wXp
T
d
i
−Σ−
−
Σ
=
−
π
CASE A. – Same diagonal Σ, with identical diagonal elements.
)(ln)]()[(
2
1)( 2 ii
T
ii wPXXXg +−−
−= μμ
σ
Canceling in class-invariant terms:
)(ln]2[
2
1)( 2 ii
T
i
T
i
T
i wPXXXXg ++−
−= μμμ
σ
)(ln]2[
2
1)( 2 ii
T
i
T
i
T
i wPXXXXg ++−
−= μμμ
σ
)(ln
2
 and   
)(    Thus,
202
0
i
i
T
i
i
i
i
i
T
ii
wPwhere
XXg
+−==
+=
σ
μμωσ
μω
ωω
lkXgXg lk ≠= ),()(The linear DB is thus:
which is: ;0)()( 00 =−+− lk
T
l
T
k X ωωωω
Prove that the 2nd constant term:
)(
)(ln)(
2
1
 where;)()(
2
2
0
000
l
k
lk
lk
lk
T
kllk
P
PX
X
ω
ω
μμ
μμσμμ
ωωωω
−
−−+=
−=−
Thus the linear DB is:
lk
T
W
XXW
μμ −=
=−
   where,
;0)( 0
Nothing new,
seen earlier
lk
T
W
XXW
μμ −=
=−
   where,
;0)( 0
)(
)(ln)(
2
1
2
2
0
l
k
lk
lk
lk P
PX
ω
ω
μμ
μμσμμ
−
−−+=
Linear DB:
CASE – A. – Same diagonal Σ, with identical diagonal elements (Contd.)


CASE – B. – Arbitrary Σ, but identical for all class.
)(ln)]()[(
2
1)( 1 ii
T
ii wPXXXg +−Σ−
−= − μμ
)(ln)(
2
1)( 11 i
T
ii
T
ii wPXXg +Σ+Σ
−= −− μμμ
Removing the class-invariant quadratic term:
)(ln
2
1 and   
)(    Thus,
1
0
1
0
ii
T
iiii
i
T
ii
wPwhere
XXg
+Σ−=Σ=
+=
−− μμωμω
ωω
lkXgXg lk ≠= ),()(The linear DB is thus:
which is: ;0)()( 00 =−+− lk
T
l
T
k X ωωωω
)(
)(ln
)()(
)(
2
1
 where;)()(
10
000
l
k
lk
T
lk
lk
lk
T
kllk
P
PX
X
ω
ω
μμμμ
μμμμ
ωωωω
−Σ−
−−+=
−=−
−  Prove it.
Thus the linear DB is:
lk
T
W
XXW
ωω −=
=−
   where,
;0)( 0
iiwhere μω
1  −Σ=
);(  , 1 lkWThus μμ −Σ=
−
The normal to the DB, “W”, is thus  the transformed line 
joining the two means. 
The transformation matrix is a symmetric Σ−1.
The DB is thus -
a tilted (rotated) vector joining the two means.
Let Σ (2−D) be diagonal, with non-identical diagonal elements: σ1 and σ2.
case. 2
;  ,
2
22
1
11
=





 −−=
d
WThen lklkD σ
μμ
σ
μμ
μκ
μl
X0
21 σσ >
DB





 −−−
=
1
11
2
22
 
σ
μμ
σ
μμ lklk
DBofDirection
Thus the linear DB is:
lk
T
W
XXW
ωω −=
=−
   where,
;0)( 0
iiwhere μω
1  −Σ=
);(  , 1 lkWThus μμ −Σ=
−
Special case:
Let, Σ (2−D) be arbitrary, but with diagonal elements (=1)..






−−−
−−−
−
=
)()(
)()(
1
1  , 1122
2211
2
lklk
lklkWThen
μμσμμ
μμσμμ
σ
; 
2
22
1
11





 −−=
σ
μμ
σ
μμ lklk
DW
Solve for W in this case, and compare with the diagonal Σ case.
Diagonal Σ
in all cases.
Increasing σ2 and decreasing σ1
Diagonal elements
in Σ  are both 1.0,
in all cases

Point P is actually closer (in the 
Euclidean sense) to the mean for the 
Orange class.
The discriminant function evaluated 
at P is smaller for class 'apple' than it 
is for class 'orange'.
)(lnln
2
1)]()[(
2
1)( 1 iiii
T
ii wPXXXg +Σ
−−−Σ−−= − μμ
)(lnln
2
1
2
1
 and 
  ;
2
1  W
      ;)(    Thus,
1
0
1
1
i
0
iiii
T
ii
iii
i
i
T
ii
T
i
wP
where
XXWXXg
+Σ−Σ−=
Σ=
Σ−=
++=
−
−
−
μμω
μω
ωω
CASE C. – Arbitrary Σ, all parameters are class dependent.
The DBs and DFs are hyper-quadrics.
We shall first look into a few cases of such surfaces next.
lkXgXg lk ≠= ),()(
;
20
02
     ;
2
3
;
20
02/1
     ;
6
3
22
11






=Σ





−
=






=Σ





=
μ
μ
Example [Duda, Hart]:
;
2/10
02/1
   
;
2/10
02
  
1
2
1
1






=Σ






=Σ
−
−
Draw and Visualize (qualitatively) 
the iso-contours
Assume; P(w1) = P(w1) =  0.5;
Get expression of DB:
Quadratic Decision Boundaries
In Rd with X = (x1, x2, …,xd)T, consider the equation:
1..     0
1
1
1 1 1
2  
=
−
= += =
=+++
d
i
d
i
d
ij
d
i
oiijiijiii wxwxxwxw
The above equation is defined by a quadric discriminant 
function, which yields a quadric surface.
If d=2, X = (x1, x2)T equation (1) becomes:
..2      0022112112
2
222
2
111 =+++++ wxwxwxxwxwxw
..2      0022112112
2
222
2
111 =+++++ wxwxwxxwxwxw
Special cases of equation:
Case 1:
w11 = w22 = w12 = 0; Eqn. (2) defines a line.
Case 2:
w11 = w22 = K; w12 = 0; defines a circle.
Case 3:
w11 = w22 = 1; w12 = w1 = w2 = 0; defines a circle whose center is at the origin.
Case 4:
w11 = w22 = 0; defines a bilinear constraint.
Case 5:
w11 = w12 = w2 = 0; defines a parabola with a specific orientation.
Case 6:
defines a simple ellipse.
Selecting suitable values of wi’s, gives other conic sections;    Hyperbolic ??
For d > 3, we define a family of hyper-surfaces in Rd.
0;,0,0 211222112211 ===≠≠≠ wwwwwww
In the above equation, the total number of parameters is:           ??
2d + 1 + d(d-1)/2 = (d+1)(d+2)/2.
1..     0
1
1
1 1 1
2  
=
−
= += =
=+++
d
i
d
i
d
ij
d
i
oiijiijiii xwxxwxw ω
Organize these parameters, and manipulate the equation to obtain:
..3      0=++ o
TT XwXWX ω
w has d terms, ωo has one term, and W (ωij) is a dxd matrix as:




≠
=
=
ji if   
2
1
ji if      
ij
ii
ij w
w
ω(d2-d) non-diagonal terms of the matrix W, 
is obtained by duplicating (split into two parts):
d(d-1)/2   wijs.
In equation 3, the symmetric part of matrix W, contributes to 
the Quadratic terms. Equation 3 generally defines a 
hyperhyperboloidal surface. 
If W = I/0, we get a hyper-spheres/planes.
Example of linearization:
063)( 1
2
12 =+−−= xxxXg
To Linearize, let x3 = x12. Then:
]1 ,1 ,3[   and
],,[ where,
63)(
321
132
−−=
=
+=+−−=
T
T
o
T
W
xxxX
wXWxxxXg
iμμXμXXiμXiμXd
T
i
T
i
TT
i
11112 2)()( −−−− Σ−Σ+Σ−=−Σ−=
0=++ o
TT XwXWX ω
)(lnln
2
1)]()[(
2
1)( 1 iiii
T
ii wPXXXg +Σ
−−−Σ−−= − μμ
)(lnln
2
1
2
1   and   
;
2
1  W     ;)(    Thus,
1
0
1
1
i0
iii
T
iiiii
ii
T
ii
T
i
wP
whereXXWXXg
+Σ−Σ−=Σ=
Σ−=++=
−−
−
μμωμω
ωω
CASE – C. – Arbitrary Σ, all parameters are class dependent – contd..

;;
;0
;;
2121
21
2121
yyxx
xyyx
μμμμ
ρρ
σσσσ
=<
==
==
;;
;0
;;
2121
21
2121
CC yyxx
xyyx
μμμμ
ρρ
σσσσ
=±=
==
==
Read about GMM, 
and estimation using 
MLE or EM methods.
Kullback-Leibler divergence
The directed Kullback-Leibler divergence between 
Exp(λ0) ('true' distribution) and Exp(λ) ('approximating' 
distribution) is given by:
entropy(p)   -   Q) & opy(Pcross_entr
H(p)   -            ),(                   
)(log)()(log)(),(
)()(
)(
)(log)(),(
=
=
+−=
+−=

 
qpH
ipipiqipqpD
or
iqip
iq
ipipqpD
ii
KL
i
KL

• Bregman divergence
• Jensen–Shannon divergence:  The Bregman distance 
associated with F for points (P, Q), is the difference 
between the value of F at point P and the value of the first-
order Taylor expansion of F around point Q evaluated at 
point P. F is a continuously-differentiable real-valued and 
strictly convex function defined on a closed convex set.
• Deviance information criterion
• Bayesian information criterion
• Quantum relative entropy
• Information gain in decision trees
• Solomon Kullback and Richard Leibler
• Information theory and measure theory
• Entropy power inequality
• Information gain ratio
• F-divergence
qpqFqFpFqpDBG −∇−−= ),()()(),(
2/)(    where;
2
),(),(),( QPMMQDMPDqpD KLJS +=
+=
Principal Component Analysis
 Eigen analysis, Karhunen-Loeve transform 
 Eigenvectors: derived from Eigen decomposition of the 
scatter matrix 
 A projection set that best explains the distribution of 
the representative features of an object of interest.
 PCA techniques choose a dimensionality-reducing 
linear projection that maximizes the scatter of all 
projected samples.
Principal Component Analysis Contd.
• Let us consider a set of N sample images {x1, x2, ……., xN} 
taking values in n-dimensional image space.
• Each image belongs to one of c classes {X1, X2,..…, Xc}.
• Let us consider a linear transformation, mapping the 
original n-dimensional image space to m-dimensional 
feature space, where    m < n.
• The new feature vectors  yk є Rm are defined by the linear 
transformation –
k = 1, 2,……, N
where,  W є Rnxm is a matrix with orthogonal columns 
representing the basis in feature space.
Principal Component Analysis Contd..
T
k
N
k
kT xxS )()(
1
μμ −−=
=
• Total scatter matrix ST is defined as 
where, N is the number of samples , and μ € Rn is the mean 
image of all samples .
• The scatter of transformed feature vectors {y1,y2,….yN} is 
WTSTW.
• In PCA, Wopt is chosen to maximize the determinant of the 
total scatter matrix of projected samples, i.e.,
WSWW T
T
W
opt maxarg=
where {wi | i= 1,2,….,m} is the set of n dimensional eigenvectors 
of ST corresponding to m largest eigenvalues (check proof).
)])([( jjiiij xxE μμσ −−=
• Eigenvectors are called eigen images/pictures and also 
basis images/facial basis for  faces.
• Any data (say, face) can be reconstructed approximately as 
a weighted sum of a small collection of images that define a 
facial basis (eigen images) and a mean image of the face. 
Principal Component Analysis Contd.
• Data form a scatter in the feature space through 
projection set (eigen vector set)
• Features (eigenvectors) are extracted from the training 
set without prior class information 
Unsupervised learning
Demonstration of KL Transform
First 
eigen 
vector
Second 
eigen 
vector
Another One
Another Example
Source:  SQUID Homepage
Principal components analysis (PCA) is a technique 
used to reduce multi-dimensional data sets to lower 
dimensions for analysis. 
The applications include exploratory data analysis and 
generating predictive models. PCA involves the computation of the 
eigenvalue decomposition or Singular value decomposition of a data 
set, usually after mean centering the data for each attribute.
PCA is mathematically defined as an orthogonal linear 
transformation, that transforms the data to a new coordinate 
system such that the greatest variance by any projection of 
the data comes to lie on the first coordinate (called the first 
principal component), the second greatest variance on the 
second coordinate, and so on.
PCA can be used for dimensionality reduction in a data 
set by retaining those characteristics of the data set that 
contribute most to its variance, by keeping lower-order 
principal components and ignoring higher-order ones. Such 
low-order components often contain the "most important" 
aspects of the data. But this is not necessarily the case, 
depending on the application.
For a data matrix, XT, with zero empirical mean (the 
empirical mean of the distribution has been subtracted from 
the data set), where each column is made up of results for a 
different subject, and each row the results from a different 
probe. This will mean that the PCA for our data matrix X will 
be given by:
.X of (SVD)ion decomposit aluesingular v  theis   where
,
T
TT
VW
VXWY
Σ
Σ==
Unlike other linear transforms (DCT, DFT, DWT etc.), 
PCA does not have a fixed set of basis vectors. Its basis 
vectors depend on the data set. 
Goal of PCA:
Find some orthonormal matrix WT, where Y = WTX; 
such that 
COV(Y) ≡ (1/(n−1))YYT is diagonalized.
The rows of W are the principal components of X, 
which are also the eigenvectors of COV(X).
SVD – the theorem
Suppose M is an m-by-n matrix whose entries come from the field K, 
which is either the field of real numbers or the field of complex numbers. Then 
there exists a factorization of the form
M = UΣV*
where U is an m-by-m unitary matrix over K, the matrix Σ is m-by-n with 
nonnegative numbers on the diagonal and zeros off the diagonal, and V* 
denotes the conjugate transpose of V, an n-by-n unitary matrix over K. Such a 
factorization is called a singular-value decomposition of M.
The matrix V thus contains a set of orthonormal "input" or "analysing" 
basis vector directions for M. 
The matrix U contains a set of orthonormal "output" basis vector 
directions for M. The matrix Σ contains the singular values, which can be 
thought of as scalar "gain controls" by which each corresponding input is 
multiplied to give a corresponding output. 
A common convention is to order the values Σi,i in non-increasing 
fashion. In this case, the diagonal matrix Σ is uniquely determined by M 
(though the matrices U and V are not).
For p = min(m,n) — U is m-by-p, Σ is p-by-p, and V is n-by-p.
The Karhunen-Loève transform is therefore equivalent 
to finding the singular value decomposition of the data matrix 
X, and then obtaining the reduced-space data matrix Y by 
projecting X down into the reduced space defined by only the 
first L singular vectors, WL:
The matrix W of singular vectors of X is equivalently 
the matrix W of eigenvectors of the matrix of observed 
covariances C = X XT (find out?) =:
The eigenvectors with the largest eigenvalues
correspond to the dimensions that have the strongest 
correlation in the data set. PCA is equivalent to empirical 
orthogonal functions (EOF).
PCA is a popular technique in pattern recognition. But it 
is not optimized for class separability. An alternative is the 
linear discriminant analysis, which does take this into 
account. PCA optimally minimizes reconstruction error under 
the L2 norm.
T
LL
T
L
T VXWYVWX Σ==Σ=    ;
TTTT WDWWWXXXCOV =ΣΣ==)(
PCA by COVARIANCE Method
We need to find a dxd orthonormal transformation matrix WT, such that:
XWY T=with the constraint that:
Cov(Y) is a diagonal matrix, and  W-1 =  WT.
DWWDWWWXCOVW
WXXEWWXXWE
XWXWEYYEYCOV
TTT
TTTT
TTTT
===
==
==
)()(
][)])([(
]))([(][)(
WXCOVWXCOVWWYWCOV T )()()( ==
Can you derive from the above, that:
])(,.....,)(,)([
],.....,,[
21
2211
d
dd
WXCOVWXCOVWXCOV
WWW =λλλ














−
−
−
−=
236
33
14
3
25
63
25
3
62
)2/1(2/)(
~~ T
XX
Example of PCA
Samples: ;
3
0
4
;
1
3
2
;
2
1
1
321










=









−
=









−
= xxx










=
312
031
42-1-
X
3-D problem, with N = 3. 
Each column is an observation (sample) and each row a variable (dimension), 
Method – 1 (easiest)
Mean of the samples: 
;
2
3
4
3
1












=xμ ;
1
3
4
3
11
;
1
3
5
3
7
;
0
3
1
3
4
3
~
2
~
1
~












−=












−
−
=












−
−
= xxx
;
110
3
4
3
5
3
1
3
11
3
7
3
4
~












−
−−
−−
=X
COVAR = 
Method – 2 (PCA defn.) T
k
N
k
kT xxN
S )()()
1
1(
1
μμ −−
−
= 
=
;
1
3
4
3
11
;
1
3
5
3
7
;
0
3
1
3
4
3
~
2
~
1
~












−=












−
−
=












−
−
= xxxC1 =
1.7778    0.4444         0
0.4444    0.1111         0
0         0                 0
C2 =
5.4444   -3.8889    2.3333
-3.8889    2.7778   -1.6667
2.3333   -1.6667    1.0000
C3 =
13.4444   -4.8889    3.6667
-4.8889    1.7778   -1.3333
3.6667   -1.3333    1.0000
SigmaC =
20.6667   -8.3333    6.0000
-8.3333    4.6667   -3.0000
6.0000   -3.0000    2.0000
COVAR =
SigmaC/2 =
10.3333   -4.1667    3.0000
-4.1667    2.3333   -1.5000
3.0000   -1.5000    1.0000
Next do SVD, to get vectors.
For a face image with N samples and dimension d (=w*h, very large), we have:
The array X or Xavg of size d*N (N vertical samples stacked horizontally)
Thus XXT will be of d*d, which will be very large. To perform eigen-
analysis on such large dimension is time consuming and may be erroneous.
Thus often XTX of dimension N*N is considered for eigen-analysis. Will 
it result in the same, after SVD? Lets check:
=












−
−
−
−==
236
33
14
3
25
63
25
3
62
)2/1(
~~ T
XXS
==
~~
XXS Tm
10.3333   -4.1667    3.0000
-4.1667    2.3333   -1.5000
3.0000   -1.5000    1.0000
0.9444    1.2778   -2.2222
1.2778    4.6111   -5.8889
-2.2222   -5.8889    8.1111
Lets do SVD of both:
U =
-0.8846   -0.4554   -0.1010
0.3818   -0.8313    0.4041
-0.2680    0.3189    0.9091
S =
13.0404         0         0
0    0.6263         0
0         0    0.0000
V =
-0.8846   -0.4554    0.1010
0.3818   -0.8313   -0.4041
-0.2680    0.3189   -0.9091
U =
-0.2060    0.7901    0.5774
-0.5812   -0.5735    0.5774
0.7872   -0.2166    0.5774
S =
13.0404         0         0
0    0.6263         0
0         0    0.0000
V =
-0.2060    0.7901    0.5774
-0.5812   -0.5735    0.5774
0.7872   -0.2166    0.5774
==
T
XXS
~
10.3333   -4.1667    3.0000
-4.1667    2.3333   -1.5000
3.0000   -1.5000    1.0000
0.9444    1.2778   -2.2222
1.2778    4.6111   -5.8889
-2.2222   -5.8889    8.1111
==
~~
XXS Tm
Example, where d <> N:Samples:
;
7
6
;
5
5
;
4
4
;
1
1
;
2
2
;
3
3
654321 





=





=





=





−
−
=





−
−
=





−
−
= xxxxxx
2-D problem (d=2), with N = 6. 
Each column is an observation (sample) 
and each row a variable (dimension), 
Mean of the samples: 
;
3/5
2/3






=xμ
X =
-3    -2    -1     4     5     6
-3    -2    -1     4     5     7
XM=
-4.5000   -3.5000   -2.5000    2.5000   3.5000  4.5000
-4.6667   -3.6667   -2.6667    2.3333   3.3333  5.3333
COVAR(X) = XM * XMT 
=     77.5000   82.0000
82.0000   87.3333
XMT * XM = 
42.0278   32.8611   23.6944  -22.1389   -31.3056  -45.1389
32.8611   25.6944   18.5278  -17.3056   -24.4722  -35.3056
23.6944   18.5278   13.3611  -12.4722   -17.6389  -25.4722
-22.1389  -17.3056  -12.4722   11.6944    16.5278   23.6944
-31.3056  -24.4722  -17.6389   16.5278    23.3611   33.5278
-45.1389  -35.3056  -25.4722   23.6944    33.5278   48.6944
COVAR(X) = XM * XMT 
=     77.5000   82.0000
82.0000   87.3333
XMT * XM = 
42.0278   32.8611   23.6944  -22.1389   -31.3056  -45.1389
32.8611   25.6944   18.5278  -17.3056   -24.4722  -35.3056
23.6944   18.5278   13.3611  -12.4722   -17.6389  -25.4722
-22.1389  -17.3056  -12.4722   11.6944    16.5278   23.6944
-31.3056  -24.4722  -17.6389   16.5278    23.3611   33.5278
-45.1389  -35.3056  -25.4722   23.6944    33.5278   48.6944
U =
-0.6856   -0.7280
-0.7280    0.6856
S =
164.5639         0
0  0.2694
V =
-0.6856   -0.7280
-0.7280    0.6856
U =
-0.5053   -0.1469   -0.7547    0.3882    0.0214    0.0486
-0.3951   -0.0654    0.3632    0.0984   -0.4091    0.7284
-0.2849    0.0162   -0.0433   -0.3456   -0.7396   -0.5002
0.2660    0.4241   -0.5083   -0.5306   -0.1150    0.4429
0.3762    0.5057   -0.0258    0.6601   -0.4043   -0.0539
0.5432   -0.7337   -0.1938    0.0541   -0.3293    0.1332
S = 
164.5639         0         0         0 0 0
0    0.2694         0         0 0 0
0            0        0.0        0 0 0
0            0         0        0.0 0 0
0            0         0         0 0.0 0
0            0         0         0 0 0.0
V = U ??
Scatter Matrices and Separability criteria
Scatter matrices used to formulate criteria of class 
separability:
Within-class scatter Matrix: It shows the scatter 
of samples around their respective class expected 
vectors.
T
ik
c
i Xx
ikW xxS
ik
)()(
1
μμ −−= 
= ∈
 Between-class scatter Matrix: It is the scatter 
of the expected vectors around the mixture 
mean…..μ is the mixture mean..
T
ii
c
i
iB NS ))((
1
μμμμ −−=
=
Scatter Matrices and Separability criteria
 Mixture scatter matrix: It is the covariance matrix of 
all samples regardless of their class assignments.
BW
T
k
N
k
kT SSxxS +=−−=
=
)()(
1
μμ
• The criteria formulation for class separability 
needs to convert these matrices into a number. 
• This number should be larger when between-
class scatter is larger or the within-class scatter is 
smaller.
Several Criteria are..     
)( 1
1
21 SStrJ
−= 211
1
22 lnlnln SSSSJ −==
−
)()( 213 ctrSStrJ −−= μ
2
1
4 trS
trS
J =
Linear Discriminant Analysis
• Learning set is labeled – supervised learning
• Class specific method in the sense that it tries to ‘shape’ the  
scatter in order to make it more reliable for classification.
• Select W to maximize the ratio of the between-class 
scatter and the within-class scatter.
Between-class scatter matrix is defined by-
T
ii
c
i
iB NS ))((
1
μμμμ −−=
=
µi is the mean of class Xi
Ni   is the no. of samples in class Xi.
Within-class scatter matrix 
is:
T
ik
c
i Xx
ikW xxS
ik
)()(
1
μμ −−= 
= ∈
Linear Discriminant Analysis
• If SW is nonsingular, Wopt is chosen to satisfy 
WSW
WSW
W
T
B
T
Wopt maxarg=
Wopt = [w1, w2, ….,wm]
{wi | i = 1,2,…..,m} is the set of eigenvectors of SB and SW
corresponding to m largest eigen values.i.e.
iWiiB wSwS λ=
• There are at most (c-1) non-zero eigen values. So upper 
bound of m is (c-1).
Linear Discriminant Analysis
SW is singular most of the time. It’s rank is at most N-c
Solution – Use an alternative criterion.
• Project the samples to a lower dimensional space.
• Use PCA to reduce dimension of the feature space to N-c. 
• Then apply standard FLD to reduce dimension to c-1.
Wopt is given by
T
pca
T
fldopt WWW =
WSW T
TWpca maxarg=
W WWSWW
WWSWW
pcaW
T
pca
T
pcaB
T
pca
T
Wfld maxarg=
W
Demonstration for LDA
1     2     3     5     4     6     8       -2    -1     1     3      4     2     5
1     2     3     4     5     6     7        3 4     5 6     7     8     9
Hand workout EXAMPLE:
Data Points:
Class: 1     1     1     1     1     1     1          2     2     2     2     2     2     2
Lets try PCA first :
Overall data mean: 2.9286
5.0000
COVAR of the mean-subtracted data:
7.3022    3.3077
3.3077    5.3846
Eigenvalues after SVD of above:
9.7873        2.8996
Finally, the eigenvectors:
-0.7995 -0.6007
-0.6007 0.7995
Same EXAMPLE for LDA :
Data Points:
Class: 1     1     1     1     1     1     1 2     2     2     2      2      2     2
Sw =  10.6122    8.5714
8.5714    8.0000
Sb =  20.6429     -17.00
-17.00         14.00
Eigenvalues of Sw-1 Sb : 53.687
0
INV(Sw) . Sb =
27.20   -22.40
-31.268 25.75
Perform Eigendecomposition 
on above:
Eigenvectors:
- 0.7719 0.6357
0.6357 0.7719
1     2     3     5     4     6     8        -2    -1     1      3      4      2     5
1     2     3     4     5     6     7         3 4     5 6      7      8     9
Eigenvectors: -0.7355    -0.6775
0.6775    0.7355
Eigenvectors: - 0.7719    0.6357
0.6357    0.7719
Sw =  10.6122    8.5714
8.5714    8.0000
Sb =  20.6429     - 17.00
- 17.00         14.00
Eigenvalues of Sw-1 Sb : 53.687
0
Sw =  10.6122    8.5714
8.5714    8.0000
Sb =  203.143     - 95.00
- 95.00         87.50
Eigenvalues of Sw-1 Sb : 297.83
0.0
After linear projection, using LDA:
Same EXAMPLE for LDA, with C = 3:
Data Points:
Class: 1     1     1     2     2 3     3 1     1     1 2      2 3     3
Sw =  8.0764    - 2.125
- 2.125      4.1667
Sb =  56.845     52.50
52.50      50.00
Eigenvalues of Sw-1 Sb : 30.5
0.097
INV(Sw) . Sb =
11.958    11.155
18.7 17.69
Perform Eigendecomposition 
on above:
Eigenvectors:
- 0.728 - 0.69
- 0.69 0.728
1     2     3     5     4     6     8        -2    -1     1     3      4      2     5
1     2     3     4     5     6     7         3 4     5 6     7      8     9
Data projected along
1st eigenvector:
Data projected along
2nd eigenvector:
Hence, one may need ICA
Some of the latest advancements in Pattern recognition technology deal with:
• Neuro-fuzzy (soft computing) concepts
• Multi-classifier Combination – decision and feature fusion
• Reinforcement learning
• Learning from small data sets
• Generalization capabilities
• Evolutionary Computations
• Genetic algorithms
• Pervasive computing
• Neural dynamics
• Support Vector machines  - kernel methods
• Modern ML methods – semi-supervised, transfer learning, domain adaptation
Manifold based learning, deep learning, MKL, ….
REFERENCES
• “Pattern Recognition: Statistical. Structural and Neural 
Approaches”; Robert J. Schallkoff; John Wiley and Sons; 
1992+.
• Pattern Classification;  Duda R.O., Hart P.E. & D. G. Stork:. 
John Wiley and Sons,  Singapore (2001).
• Statistical pattern Recognition; S. Fukunaga; 
Academic Press, 2000.
• Bishop – PR 


