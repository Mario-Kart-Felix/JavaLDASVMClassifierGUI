Pegasos: Primal Estimated sub-GrAdient SOlver for SVM
Shai Shalev-Shwartz SHAIS@CS.HUJI.AC.IL
School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel
Yoram Singer SINGER@GOOGLE.COM
Google inc., Mountain View, USA and The Hebrew University, Jerusalem, Israel
Nathan Srebro NATI@UCHICAGO.EDU
Toyota Technological Institute, Chicago, USA
Abstract
We describe and analyze a simple and effec-
tive iterative algorithm for solving the optimiza-
tion problem cast by Support Vector Machines
(SVM). Our method alternates between stochas-
tic gradient descent steps and projection steps.
We prove that the number of iterations required
to obtain a solution of accuracy ǫ is Õ(1/ǫ). In
contrast, previous analyses of stochastic gradient
descent methods require Ω(1/ǫ2) iterations. As
in previously devised SVM solvers, the number
of iterations also scales linearly with 1/λ, where
λ is the regularization parameter of SVM. For a
linear kernel, the total run-time of our method
is Õ(d/(λǫ)), where d is a bound on the num-
ber of non-zero features in each example. Since
the run-time does not depend directly on the size
of the training set, the resulting algorithm is es-
pecially suited for learning from large datasets.
Our approach can seamlessly be adapted to em-
ploy non-linear kernels while working solely on
the primal objective function. We demonstrate
the efficiency and applicability of our approach
by conducting experiments on large text classi-
fication problems, comparing our solver to ex-
isting state-of-the-art SVM solvers. For exam-
ple, it takes less than 5 seconds for our solver to
converge when solving a text classification prob-
lem from Reuters Corpus Volume 1 (RCV1) with
800, 000 training examples.
Appearing in Proceedings of the 24 th International Conference
on Machine Learning, Corvallis, OR, 2007. Copyright 2007 by
the author(s)/owner(s).
1. Introduction
Support Vector Machines (SVMs) are effective and popu-
lar classification learning tool (Vapnik, 1998; Cristianini
& Shawe-Taylor, 2000). The task of learning a support
vector machine is cast as a constrained quadratic program-
ming problem. However, in its native form, it is in fact an
unconstrained empirical loss minimization with a penalty
term for the norm of the classifier that is being learned.
Formally, given a training set S = {(xi, yi)}mi=1, where
xi ∈ Rn and yi ∈ {+1,−1}, we would like to find the
minimizer of the problem
min
w
λ
2
‖w‖2 + 1
m
∑
(x,y)∈S
ℓ(w; (x, y)) , (1)
where
ℓ(w; (x, y)) = max{0, 1 − y 〈w,x〉} . (2)
We denote the objective function of Eq. (1) by f(w).
An optimization method finds an ǫ-accurate solution ŵ if
f(ŵ) ≤ minw f(w) + ǫ. The original SVM problem
also includes a bias term, b. We omit the bias throughout
the first sections and defer the description of an extension
which employs a bias term to Sec. 4.
We describe and analyze in this paper a simple iterative al-
gorithm, called Pegasos, for solving Eq. (1). The algorithm
performs T iterations and also requires an additional pa-
rameter k, whose role is explained in the sequel. Pegasos
alternates between stochastic subgradient descent steps and
projection steps. The parameter k determines the number
of examples from S the algorithm uses on each iteration for
estimating the subgradient. When k = m, Pegasos reduces
to a variant of the subgradient projection method. We show
that in this case the number of iterations that is required in
order to achieve an ǫ- accurate solution is Õ(1/(λǫ)). At
the other extreme, when k = 1, we recover a variant of
the stochastic (sub)gradient method. In the stochastic case,
Pegasos: Primal Estimated sub-GrAdient SOlver for SVM
we analyze the probability of obtaining a good approximate
solution. Specifically, we show that with probability of at
least 1− δ our algorithm finds an ǫ-accurate solution using
only Õ(1/(δλǫ)) iterations, while each iteration involves a
single inner product between w and x. This rate of conver-
gence does not depend on the size of the training set and
thus our algorithm is especially suited for large datasets.
Before indulging in the description and analysis of Pega-
sos, we would like to draw connections to and put our
work in context of some of the more recent work on SVM.
For a more comprehensive and up-to-date overview of rel-
evant work see the references in the papers cited below
as well as the web site dedicated to kernel methods at
http://www.kernel-machines.org . Due to the centrality of
the SVM optimization problem, quite a few methods were
devised and analyzed. The different approaches can be
roughly divided into the following categories.
Interior Point (IP) methods: IP methods (see for instance
(Boyd & Vandenberghe, 2004) and the references therein)
cast SVM learning as a quadratic optimization problem
subject to linear constraints. The constraints are replaced
with a barrier function. The result is a sequence of uncon-
strained problems which can be optimized very efficiently
using Newton or Quasi-Newton methods. The advantage of
IP methods is that the dependence on the accuracy ǫ is dou-
ble logarithmic, namely, log(log(1/ǫ)). Alas, IP methods
typically require run time which is cubic in the number of
examples m. Moreover, the memory requirements of IP are
O(m2) which renders a direct use of IP methods very dif-
ficult when the training set has many examples. It should
be noted that there have been several attempts to reduce
the complexity based on additional assumptions (see e.g.
(Fine & Scheinberg, 2001)). However, the dependence on
m remains super linear. In addition, while the focus of the
paper is the optimization problem cast by SVM, one needs
to bare in mind that the optimization problem is a proxy
method for obtaining good classification error on unseen
examples. Achieving very high accuracy in the optimiza-
tion process is usually unnecessary and does not translate to
a significant increase in generalization accuracy. The time
spent by IP methods for finding a single accurate solution
may, for instance, be better utilized for finding numerous
approximate solutions for multiple choices of λ.
Decomposition methods: To overcome the quadratic
memory requirement of IP methods, decomposition meth-
ods such as SMO (Platt, 1998) and SVM-Light (Joachims,
1998) switch to the dual representation of the SVM opti-
mization problem, and employ an active set of constraints
thus working on a subset of dual variables. In the extreme
case, called row-action methods (Censor & Zenios, 1997),
the active set consists of a single constraint. While algo-
rithms in this family are fairly simple to implement and en-
tertain general asymptotic convergence properties (Censor
& Zenios, 1997), the time complexity of most of the algo-
rithms in this family is typically super linear in the training
set size m. Moreover, since decomposition methods find a
feasible dual solution and their goal is to maximize the dual
objective function, they often result in a rather slow conver-
gence rate to the optimum of the primal objective function
(See also the discussion in (Hush et al., 2006)).
Some of the decomposition methods do yield though a re-
gret bound in the online learning setting. For instance, the
Passive Aggressive (Crammer et al., 2006) applies the ob-
jective function of SVM to each example. Online learn-
ing algorithms were also suggested as fast alternatives to
SVM (see (Freund & Schapire, 1999)). Such algorithms
can be used to obtain a predictor with low generalization
error using an online-to-batch conversion scheme (Cesa-
Bianchi et al., 2004). However, the conversion schemes
do not necessarily yield ǫ-accurate solutions to the original
SVM problem and their performance is typically inferior
to direct batch optimizers. As noted above, Pegasos shares
the simplicity and speed of online learning algorithms but
is guaranteed to converge to the actual SVM solution.
Gradient based methods: Unconstrained gradient meth-
ods were very common in optimization until the emergence
of the ultra-fast IP methods. While gradient based methods
are known to exhibit slow convergence rates, the compu-
tational demands imposed by large scale classification and
regression problems of high dimension feature space, re-
vived the theoretical and applied interest in gradient meth-
ods. The Pegasos algorithm is an improved stochastic sub-
gradient method. Two concrete algorithms that are closely
related to the Pegasos algorithm that are based on gradient
methods are the NORMA algorithm (Kivinen et al., 2002)
and a stochastic gradient algorithm by Zhang (2004). The
Pegasos algorithm uses a sub-sample of k training exam-
ples to compute an approximate sub-gradient. When k = 1
the Pegasos algorithm becomes very similar to the afore-
mentioned methods of Kivienen et. al and Zhang with a few
notable and crucial differences. First, after each gradient-
based update, Pegasos employs a projection step of w onto
the L2 ball of radius 1/
√
λ. This modification enables the
usage of a very aggressive decrease in the learning rate
and yields an improved Õ(1/ǫ) rate of convergence rather
than O(1/ǫ2) rate. This theoretical improvement is also re-
flected in our experiments. When k = m Pegasos results
in a modified gradient-descent algorithm with an improved
convergence rate. In addition to the superior rate of con-
vergence, Pegasos can incorporate a bias term (discussed
in Sec. 4) and can utilize parallel computation power for
appropriate choices of k.
Last, we would like to point to the SVM-Perf algorithm
recently proposed by Joachims (2006) for linear SVMs.
Pegasos: Primal Estimated sub-GrAdient SOlver for SVM
SVM-Perf uses cutting planes to find a solution with accu-
racy ǫ in time O(md/(λǫ2)). The complexity guarantee for
Pegasos avoids the dependence on the data set size m and
reduces the dependence on the accuracy to only Õ(1/ǫ). In
practice, while SVM-Perf yields very significant improve-
ments over decomposition methods for large data sets, our
experiments (see Sec. 5) demonstrate that Pegasos is sub-
stantially faster than SVM-Perf.
2. The Pegasos Algorithm
In this section we describe the Pegasos algorithm for solv-
ing the optimization problem given in Eq. (1). The algo-
rithm receives as input two parameters: T - the number of
iterations to perform; k - the number of examples to use for
calculating sub-gradients. Initially, we set w1 to any vector
whose norm is at most 1/
√
λ. On iteration t of the algo-
rithm, we first choose a set At ⊆ S of size k. Then, we
replace the objective in Eq. (1) with an approximate objec-
tive function,
f(w;At) =
λ
2
‖w‖2 + 1
k
∑
(x,y)∈At
ℓ(w; (x, y)) .
Note that we overloaded our original definition of f as
the original objective can be denoted either as f(w) or as
f(w;S). We interchangeably use both notations depending
on the context . Next, we set the learning rate ηt = 1/(λt)
and define A+t to be the set of examples for which w suf-
fers a non-zero loss. We now perform a two-step update as
follows. We scale wt by (1 − ηt λ) and for all examples
(x, y) ∈ A+t we add to w the vector yηtk x. We denote the
resulting vector by wt+ 1
2
. This step can be also written as
wt+ 1
2
= wt − ηt∇t, where
∇t = λwt − 1|At|
∑
(x,y)∈A+
t
y x . (3)
The definition of the hinge-loss implies that ∇t is a sub-
gradient of f(w;At) at wt. Last, we set wt+1 to be the
projection of wt+ 1
2
onto the set
B = {w : ‖w‖ ≤ 1/
√
λ} . (4)
That is, wt+1 is obtained by scaling wt+ 1
2
by
min
{
1, 1/(
√
λ‖wt+ 1
2
‖)
}
. As we show in our analysis be-
low, the optimal solution of SVM is in the set B. Informally
speaking, we can always project back onto the set B as we
only get closer to the optimum. The output of Pegasos is
the last vector wT+1. The pseudo-code of Pegasos is given
in Fig. 1.
Note that if we choose At = S on each round t then we
obtain the subgradient projection method. On the other ex-
treme, if we choose At to contain a single randomly se-
lected example, then we recover a variant of the stochastic
INPUT: S, λ, T , k
INITIALIZE: Choose w1 s.t. ‖w1‖ ≤ 1/
√
λ
FOR t = 1, 2, . . . , T
Choose At ⊆ S, where |At| = k
Set A+t = {(x, y) ∈ At : y 〈wt,x〉 < 1}
Set ηt = 1λt
Set wt+ 1
2
= (1 − ηt λ)wt + ηtk
∑
(x,y)∈A+
t
y x
Set wt+1 = min
{
1, 1/
√
λ
‖w
t+ 1
2
‖
}
wt+ 1
2
OUTPUT: wT+1
Figure 1. The Pegasos Algorithm.
gradient method. In general, we allow At to be a set of k
examples sampled i.i.d. from S.
We conclude this section with a short discussion of imple-
mentation details when the instances are sparse, namely,
when each instance has very few non-zero elements. In this
case, we can represent w as a triplet (v, a, ν) where v is a
dense vector and a, ν are scalars. The vector w is defined
through the triplet as follows: w = av and ν stores the
squared norm of w, ν = ‖w‖2. Using this representation,
it is easily verified that the total number of operations re-
quired for performing one iteration of Pegasos with k = 1
is O(d), where d is the number of non-zero elements in x.
3. Analysis
In this section we analyze the convergence properties of
Pegasos. Throughout this section we denote
w
⋆ = argmin
w
f(w) . (5)
Recall that on each iteration of the algorithm, we focus on
an instantaneous objective function f(w;At). We start by
bounding the average instantaneous objective of the algo-
rithm relatively to the average instantaneous objective of
the optimal solution. We first need the following lemma
which generalizes a result from (Hazan et al., 2006). The
lemma relies on the notion of strongly convex functions.
A detailed proof and further explanations can be found in
(Shalev-Shwartz & Singer, 2007).
Lemma 1. Let f1, . . . , fT be a sequence of λ-strongly con-
vex functions w.r.t. the function 12‖ · ‖2. Let B be a closed
convex set and define ΠB(w) = arg minw′∈B ‖w − w′‖.
Let w1, . . . ,wT+1 be a sequence of vectors such that w1 ∈
B and for t ≥ 1, wt+1 = ΠB(wt − ηt∇t), where ∇t is a
subgradient of ft at wt and ηt = 1/(λt). Assume that for
all t, ‖∇t‖ ≤ G. Then, for all u ∈ B we have
1
T
T
∑
t=1
ft(wt) ≤
1
T
T
∑
t=1
ft(u) +
G2(1 + ln(T ))
2λT
.
Pegasos: Primal Estimated sub-GrAdient SOlver for SVM
Based on the above lemma, we are now ready to bound the
average instantaneous objective of Pegasos.
Theorem 1. Assume that for all (x, y) ∈ S the norm of
x is at most R. Let w⋆ be as defined in Eq. (5) and let
c = (
√
λ + R)2. Then, for T ≥ 3,
1
T
T
∑
t=1
f(wt;At) ≤
1
T
T
∑
t=1
f(w⋆;At) +
c ln(T )
λT
.
Proof. To simplify our notation we use the shorthand
ft(w) = f(w;At). The update of the algorithm can be
rewritten as wt+1 = ΠB(wt − ηt∇t), where ∇t is de-
fined in Eq. (3) and B is defined in Eq. (4). Thus, for
proving the theorem it suffices to show that the condi-
tions stated in Lemma 1 hold. Since ft is a sum of a λ-
strongly convex function ( λ2 ‖w‖2) and a convex function
(the average hinge-loss over At), it is also λ-strongly con-
vex (see Lemma 1 in (Shalev-Shwartz & Singer, 2007)).
Next, we derive a bound on ‖∇t‖. Using the facts that
‖wt‖ ≤ 1/
√
λ and that ‖x‖ ≤ R combined with the tri-
angle inequality we obtain ‖∇t‖ ≤
√
λ + R. Finally, we
need to prove that w⋆ ∈ B. To do so, we use the fact that
there exists a vector α⋆ ∈ [0, 1]m such that
λ
2
‖w⋆‖2+ 1
m
∑
(x,y)∈S
ℓ(w⋆; (x, y)) = −λ
2
‖w⋆‖2+ ‖α
⋆‖1
m
.
(The above equality is derived by applying the strong dual-
ity theorem to the SVM optimization problem.) Rearrang-
ing the above and using the non-negativity of the hinge-loss
gives that ‖w⋆‖ ≤ 1/
√
λ. The bound in the theorem fol-
lows now using simple algebraic manipulations.
Note that the convexity of f implies that
f
(
1
T
∑T
t=1 wt
)
≤ 1T
∑T
t=1 f(wt) .
Using the above inequality and Thm. 1, we immediately
obtain the following corollary which gives a convergence
analysis for the case At = S.
Corollary 1. Assume the conditions stated in Thm. 1 and
that At = S for all t. Let w̄ = 1T
∑T
t=1 wt. Then,
f (w̄) ≤ f(w⋆) + c ln(T )
λT
.
Based on the above corollary, the number of itera-
tions required for achieving a solution of accuracy ǫ
is Õ(R2/(λ ǫ)). Joachims (2006) recently suggested a
method, called SVM-Perf, which requires O(R2/(λ ǫ2)) it-
erations. The cost of each iteration of SVM-Perf is O(md),
where m is the number of examples and d is the effective
dimension of the examples. The complexity of a single iter-
ation of Pegasos when At = S is also O(md), with smaller
constants.1 We therefore obtain a significant improvement
with a simpler algorithm.
When At 6= S, Corollary 1 no longer holds. In addition,
the final hypothesis we use in Fig. 1 is wT+1 rather than
the average hypothesis. The next theorem bridges this gap
as it implies that the same convergence rate still holds in
expectation if we randomly choose a stopping time.
Theorem 2. Assume that the conditions stated in Thm. 1
hold and for all t, At is chosen i.i.d. from S. Let r be an
integer picked uniformly at random from [T ]. Then,
EA1,...,AT Er[f(wr)] ≤ f(w⋆) +
c ln(T )
λT
.
Proof. To simplify our notation, denote by Aji the sequence
of sets (Ai, . . . , Aj). Taking expectation of the inequality
given in Thm. 1 we obtain
EAT
1
[
1
T
T
∑
t=1
f(wt;At)] ≤ EAT
1
[
1
T
T
∑
t=1
f(w⋆;At)]
+
c ln(T )
λT
.
(6)
We now analyze the two expectations given in Eq. (6).
Since w⋆ does not depend on the choice of AT1 , we have,
EAT
1
[
1
T
T
∑
t=1
f(w⋆;At)] =
1
T
T
∑
t=1
EAT
1
[f(w⋆;At)]
=
1
T
T
∑
t=1
EAt [f(w
⋆;At)]
= f(w⋆) .
(7)
Next, we analyze the expectation at the left-hand side of
Eq. (6). Note that wt only depends on A
t−1
1 . Thus,
EAT
1
[
1
T
T
∑
t=1
f(wt;At)] =
1
T
T
∑
t=1
EAt
1
[f(wt;At)] . (8)
Recall that the law of total expectation implies that
for any two random variables X,Y , EX [f(X)] =
EY EX [f(X)|Y ]. Setting X = At1 and Y = At−11 we get
that
EAt
1
[f(wt;At)] = EAt−1
1
[EAt
1
[f(wt;At)|At−11 ]]
= EAt−1
1
[f(wt)] = EAT
1
[f(wt)] .
Combining the above with Eq. (8) we obtain
EAT
1
[
1
T
T
∑
t=1
f(wt;At)] = EAT
1
[
1
T
T
∑
t=1
f(wt)] .
1Seemingly, to calculate w̄ we need additional O(n) opera-
tions at each iteration, where n is the dimension of w. However,
if n > m d we can also save wt as a linear combination of the
m instances in the training set and update only the coefficients.
Thus, the cost of each iteration never exceeds O(m d).
Pegasos: Primal Estimated sub-GrAdient SOlver for SVM
Combining the above with Eq. (7) and Eq. (6), and noting
that Er[f(wr)] = 1T
∑T
t=1 f(wt) we conclude our proof.
The above theorem states that, in expectation, the stochas-
tic version of the algorithm will converge as fast as the de-
terministic version. The next theorem provides a very sim-
ple concentration bound.
Theorem 3. Assume that the conditions stated in Thm. 2
hold. Let δ ∈ (0, 1). Then, with probability of at least 1−δ
over the choices of (A1, . . . , AT ) and the index r we have
that
f(wr) ≤ f(w⋆) +
c ln(T )
δ λ T
.
Proof. Let Z be the random variable f(wr) − f(w⋆).
From the definition of w⋆ as the minimizer of f(w) we
clearly have that Z is a non-negative random variable.
Thus, from Markov inequality P[Z ≥ a] ≤ E[Z]/a.
Setting E[Z]/a = δ and using Thm. 2 we obtain that
a = E[Z]δ ≤
c ln(T )
δ λ T .
Let us now discuss the implications of Thm. 3. First, by
taking T = ∞ we immediately obtain from Thm. 3 con-
vergence in the limit. In addition, we can use Thm. 3
for analyzing the convergence of the last weight vector.
We do so by viewing T as a random index drawn from
{1, . . . , T̂}, where T̂ > T . Since wT does not depend
on AT+1, . . . , AT̂ , we can terminate the algorithm after T
iterations and return wT . Using Thm. 3 we know that
f(wT ) − f(w⋆) ≤
c ln(T̂ )
δ λ T̂
≤ c ln(T )
δ λ T
.
We conclude this section with a discussion on the depen-
dence of the convergence rate on the confidence parameter
δ and on the accuracy parameter ǫ. From Thm. 3 we ob-
tain that to achieve accuracy ǫ with confidence 1 − δ we
need Õ( 1λ δ ǫ ) iterations. In contrast, by applying previ-
ously studied conversions of online algorithms in the PAC
setting (e.g. (Cesa-Bianchi et al., 2004; Cesa-Bianchi &
Gentile, 2006)) one can obtain accuracy of ǫ with confi-
dence 1 − δ using Õ( ln(1/δ)λ ǫ2 ) iterations. Thus, as long as
the desired confidence is not too high, our convergence rate
is significantly better. If we would like to have a very high
confidence, we can use a simple amplification technique
(a.k.a. boosting the confidence), to construct a few candi-
date vectors such that with confidence 1 − δ at least one
of the vectors has accuracy of Õ( ln(1/δ)λ T ). Let s denote the
smallest integer larger than ln(1/δ). We run the algorithm s
times while setting the number of iterations for each run to
T/s. We then randomly choose one vector from the vectors
constructed by each run. Denote by ŵ1, . . . , ŵs the result-
ing weight vectors. Using Thm. 3 with a confidence value
of 1/e, we know that with probability of at least 1 − 1/e
we have that
f(ŵi)−f(w⋆) ≤
c e ln(T )
λT/s
≤ c e ln(T )
⌈
ln
(
1
δ
)⌉
λT
. (9)
Therefore, the probability that for all runs the above in-
equality does not hold is at most e−s ≤ δ. In other words,
with probability of at least 1− δ, at least one of the vectors
ŵi satisfies Eq. (9). We have therefore shown a method
that uses Õ( ln(1/δ)λ ǫ ) iterations for constructing ⌈ln(1/δ)⌉
weight vectors, where at least one of them is ǫ-accurate.
Finally, we need to pick an ǫ-accurate vector from the set
of s vectors. This requires O( 1λǫ2 ) examples in the general
case, since estimating the objective of each ŵi up to accu-
racy of ǫ requires O( 1λǫ2 ) examples. Note however that if
we would like to simply choose the best performing vec-
tor with respect to the zero-one error over a validation set
(rather than based on the objective value of SVM), we only
need O( 1ǫ2 ) examples. That is, we gain a factor of 1/λ. We
leave further exploration of this issue to future work.
4. Extensions
In this section we discuss a few extensions to our basic clas-
sification learning algorithm. These extensions broaden the
set of applications that can be tackled by our approach. Due
to the lack of space we confine ourselves to a rather high
level overview of two extensions and defer the complete de-
tails to a long version of this paper. We would like to note
though that we have devised generalizations of Pegasos to
complex decision problems, from multiclass categorization
to learning with structured data.
Incorporating a bias term: In many applications, the
weight vector w is augmented with a bias term which is
a scalar, typically denoted as b. The prediction for an in-
stance x becomes 〈w,x〉 + b and the loss is accordingly
defined as,
ℓ ((w, b); (x, y)) = max{0, 1 − y(〈w,x〉 + b)} . (10)
The bias term often plays a crucial role when the distribu-
tion of the labels is uneven as is typically the case in text
processing applications where the negative examples vastly
outnumber the positive ones. We now briefly describe three
different approaches to learn the bias term and underscore
the advantages and disadvantages of each approach.
The first approach is rather well known and its roots go
back to early work on pattern recognition (Duda & Hart,
1973). This approach simply amounts to adding one more
feature to each instance x thus increasing the dimension to
n + 1. The artificially added feature always take the same
value. We assume w.l.o.g that the value of the constant fea-
ture is 1. Once the constant feature is added the rest of the
Pegasos: Primal Estimated sub-GrAdient SOlver for SVM
algorithm remains intact, thus the bias term is not explicitly
introduced. The analysis can be repeated verbatim and we
therefore obtain the same convergence rate for this modifi-
cation. Note however that by equating the n+1 component
of w with b, the norm-penalty counterpart of f becomes
‖w‖2 + b2. The disadvantage of this approach is thus that
we solve a slightly different optimization problem. On the
other hand, an obvious advantage of this approach is that it
requires no modifications to the algorithm itself rather than
a modest increase in the dimension and it thus can be used
without any restriction on At.
The second approach incorporates b explicitly by defining
the loss as given in Eq. (10) while not penalizing for b.
Formally, the task is to find an approximate solution to the
following problem,
min
w,b
λ
2
‖w‖2 + 1
m
∑
(x,y)∈S
[1 − y(〈w,x〉 + b)]+ . (11)
Note that all the sub-gradients calculations w.r.t w remain
intact. The sub-gradient with respect to b is also simple to
compute. For a sample At it amounts to, −1|At|
∑
(x,y)∈A+
t
y
and thus requires only k additions and subtractions and a
single devision. This approach is also very simple to im-
plement and can be used with any choice of At, in par-
ticular, sets consisting of a single instance. The caveat of
this approach is that the function f ceases to be strongly
convex. This is due to the fact that with the incorporation
of b, the objective function f becomes piece-wise linear in
the direction of b and is thus no longer strongly convex.
Therefore, the analysis presented in the previous section
no longer holds. An alternative proof technique yields a
slower convergence rate of O(1/
√
T ).
The last method entertains the advantages of the two meth-
ods above at the price of a more complex algorithm that
is applicable only for large values of k. The main idea is
to rewrite the optimization problem given in Eq. (11) as
minw
λ
2 ‖w‖2 + g(w;S) where
g(w;S) = min
b
1
m
∑
(x,y)∈S [1 − y(〈w,x〉 + b)]+ . (12)
Based on the above, we redefine f(w;At) to be λ2 ‖w‖2 +
g(w;At). On each iteration of the algorithm, we find a
subgradient of f(w;At) and subtract it (multiplied by ηt)
from wt. Finally, we project the resulting vector so that its
norm will not exceed 1/
√
λ. The problem however is how
to find a subgradient of g(w;At), as g(w;At) is defined
through a minimization problem over b. It can be shown
that the complexity of finding a subgradient of g(w;At) is
equivalent to the complexity of solving the minimization
problem in Eq. (12). The latter problem is a generalized
weighted median problem that can be solved efficiently in
time O(k). We omit the details due to the lack of space.
Table 1. Training time in CPU-seconds
Pegasos SVM-Perf SVM-Light
CCAT 2 77 20,075
Covertype 6 85 25,514
astro-ph 2 5 80
The above adaptation indeed work for the case At = S and
we obtain the same rate of convergence as in the no-bias
case. However, when At 6= S we cannot apply the analysis
from the previous section to our case since the expectation
of f(w;At) over the choice of At is no longer equal to
f(w;S). When At is large enough, we can use more in-
volved measure concentration tools to show that the expec-
tation of f(w;At) is close enough to f(w;S). We again
omit the details due to the lack of space.
Using Mercer kernels: One of the main benefits of support
vector machines is their ability to incorporate and construct
non-linear predictors using kernels which satisfy Mercer’s
conditions. The crux of this property stems from the rep-
resenter theorem (Kimeldorf & Wahba, 1971), which im-
plies that the optimal solution of SVM can be expressed
as a linear combination of its constraints. In the classifi-
cation problem, the representer theorem implies that w is
a linear combination of the instances xi. The common ap-
proach for solving the optimization problem for SVM when
kernels are employed is to switch to the dual problem and
find the optimal set of dual variables. Following (Freund &
Schapire, 1999; Kivinen et al., 2002), we outline a different
approach and directly minimize the primal problem while
still using kernels. The main observation is that if w1 is
initialized to be the zero vector, then at each iteration of the
algorithm wt can be written as wt =
∑
i∈It αixi, where
It is a subset of {1, . . . ,m}. The above claim can be eas-
ily proved using an inductive argument. Therefore, we can
store the set It and the coefficients αi instead of storing wt.
It is now easy to verify that the algorithm in Fig. 1 can be
used in conjunction with kernels, by representing wt us-
ing It and αi, calculating inner product operations using
〈wt,xt〉 =
∑
i∈It αi 〈xi,xt〉, and evaluating the norm of
wt using ‖wt‖2 =
∑
i,j∈It αiαj 〈xi,xj〉. Based on the
analysis in previous sections, Pegasos finds an ǫ-accurate
solution using Õ(1/(δλǫ)) iterations, while each iteration
involves a single inner product between w and x. Note
however that each inner product operation between w and
x may require min{m, Õ(1/(δλǫ)}) evaluations of the ker-
nel function.
5. Experiments
In this section we present experimental results that demon-
strate different merits of our algorithm and its accompa-
nying analysis. We start by showing that Pegasos is in-
Pegasos: Primal Estimated sub-GrAdient SOlver for SVM
deed a practical tool for solving large scale problems. In
particular, we compare its runtime to a new state-of-the-
art solver (Joachims, 2006) on three large datasets. Next,
we compare Pegasos to two previously proposed methods
that are based on stochastic gradient descent, namely to
Norma (Kivinen et al., 2002) and to the method given in
(Zhang, 2004). Finally, we explore the empirical behavior
of the algorithm with respect to the parameter k. In all of
the experiments we did not incoprorate a bias term since
(Joachims, 2006; Kivinen et al., 2002; Zhang, 2004) do
not incorporate that term either. Additionally, we used the
algorithm as in Fig. 1, omitting the stage of boosting the
confidence, as we found empirically that in practice it was
not necessary.
In our first experiment we compared Pegasos to the SVM-
Perf algorithm (Joachims, 2006). We used the following
datasets, which were provided to us by T. Joachims.
(1) The binary text classification task CCAT from the
Reuters RCV1 collection. There are 804,414 examples
and there are 47,236 features with sparsity 0.16% in this
dataset.
(2) Classification of abstracts of scientific papers from the
Physics ArXiv according to whether they are in the Astro-
physics section. There are 99,757 features of high sparsity
(0.08%). There are 62,369 examples in this dataset.
(3) Class 1 in the Covertype dataset of Blackard, Jock &
Dean, which is comparably low-dimensional with 54 fea-
tures and a sparsity of 22.22%. There are 581,012 examples
in this dataset.
Table 4 lists the cpu-time of Pegasos and SVM-Perf on the
datasets described above. SVM-Perf (Joachims, 2006) is
a cutting plane algorithm for solving SVM that is based
on a reformulation of the SVM problem. It was shown
in (Joachims, 2006) that SVM-Perf is substantially faster
than SVM-Light, achieving a speedup of several orders
of magnitude on most datasets. We run both Pegasos
and SVM-Perf on the three datasets with values of λ as
given in (Joachims, 2006), namely, λ = 10−4 for CCAT,
λ = 2 · 10−4 for Astro-physics, and λ = 10−6 for Cover-
type. We used the latest version of SVM-perf, implemented
in C, as provided by T. Joachims. We implemented Pegasos
in C++ and run all the experiments on a 2.8GHz Intel Xeon
processor with 4GB of main memory under Linux. For
completeness, we added to the table the runtime of SVM-
Light as reported in (Joachims, 2006). As can be seen in
the table, although SVM-Perf is by itself very fast, Pegasos
still achieves a significant improvement in run-time. We
calculated the objective value of the solutions obtained by
Pegasos and SVM-Perf. For all three datasets, the objec-
tive value of Pegasos never exceeded that of SVM-Perf by
more than 0.001. In addition, the generalization error of
both methods was virtually identical. It is interesting to
note that the performance of Pegasos does not depend on
10
2
10
3
10
4
10
5
10
6
10
−1
10
0
10
1
10
2
T
Pegasos
Norma
10
2
10
3
10
4
10
5
10
6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
Pegasos
Zhang
Figure 2. Comparisons of Pegasos to Norma (left) and Pegasos to
stochastic gradient descent with a fixed learning rate (right) on the
Astro-Physics datset. In the left plot, the solid lines designate the
objective value and the dashed lines depict the loss on the test set.
the number of examples but rather on the value of λ. In-
deed, the runtime of Pegasos for the Covertype dataset is
larger than its runtime for CCAT, although the latter dataset
is larger.
In our next experiment, we compared Pegasos to
Norma (Kivinen et al., 2002) and to a variant of stochastic
gradient descent described in (Zhang, 2004). Both meth-
ods are similar to Pegasos when setting k = 1 with two
differences. First, there is no projection step. Second,
the scheduling of the learning rate, ηt, is different. In
Norma (Thm. 4), it is suggested to set ηt = p/(λ
√
t),
where p ∈ (0, 1). Based on the bound given in Thm.
4 of (Kivinen et al., 2002), the optimal choice of p is
0.5(2 + 0.5T−1/2 )1/2, which for t ≥ 100 is in the range
[0.7, 0.716]. Plugging the optimal value of p into Thm. 4 in
(Kivinen et al., 2002) yields the bound O(1/(λ
√
T )). We
therefore hypothesized that Pegasos would converge much
faster than Norma. In Fig. 5 (left) we compare Pegasos to
Norma on the Astro-Physics dataset. We split the dataset
into a training set with 29,882 examples and a test set with
32,487 examples and report the final objective value and
the average hinge-loss over the test set. As in (Joachims,
2006), we set λ = 2 ·10−4. As can be seen, Pegasos clearly
outperforms Norma. In fact, Norma fails to converge even
after 106 iterations. This can be attributed to the fact that
the value of λ here is rather small. As mentioned before,
the differences between Pegasos and Norma are both the
different learning rate and the projection step which is ab-
sent in Norma. We also experimented with a version of
Pegasos without the projection step and with a version of
Norma that includes a projection step. We found that the
projection step is important for the convergence of Pegasos,
especially when T is small, and that a projection step also
improves the performance of Norma. However, Pegasos
still outperforms the version of Norma that includes an ad-
ditional projection step. We omit the graphs due to the lack
of space. We now turn to comparing Pegasos to the algo-
rithm from (Zhang, 2004) which simply sets ηt = η, where
η is a (fixed) small number. A major disadvantage of this
Pegasos: Primal Estimated sub-GrAdient SOlver for SVM
10
0
10
1
10
2
10
3
0.1
0.15
0.2
0.25
0.3
0.35
0.4
k
T=1250
T=31250
10
0
10
1
10
2
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
k
kT=104
kT=105
kT=106
Figure 3. The effect of k on the objective value of Pegasos on the
Astro-Physics dataset. Left: T is fixed. Right: kT is fixed.
approach is that finding an adequate value for η is a diffi-
cult task on its own. Based on the analysis given in (Zhang,
2004) we started by setting η to be 10−5. Surprisingly, this
turned out to be a poor choice and the optimal choice of η
was substantially larger. In Fig. 5 (right) we illustrate the
convergence of stochastic gradient descent with ηt set to be
a fixed value from the set {0.001, 0.01, 0.1, 1, 10}. It is ap-
parent that for some choices of η the method converges at
about the same rate of Pegasos while for other choices of η
the method fails to converge. We would like to emphasize
that for large datasets, the time required for evaluating the
objective is much longer than the time required for training
a model. Therefore, searching for η is significantly more
expensive than running the algorithm a single time. The
apparent advantage of Pegasos is due to the fact that we do
not need to search for a good value for η but rather have a
predefined schedule of ηt.
In our last experiment, we examined the effect of the pa-
rameter k on the convergence of the algorithm. Our analy-
sis implies that the convergence of Pegasos does not depend
on k. Based on this fact, the optimal choice of k in terms of
run time should be k = 1. In Fig. 3 (left) we depict the ob-
jective value obtained by Pegasos as a function of k when
T is fixed. It is clear from the figure that, in contrast to
our bounds, the convergence of Pegasos improves as k gets
larger. This fact may be important in a distributed comput-
ing environment. As long as k is smaller than the number
of CPUs, the complexity of Pegasos still depends solely on
T (and on log(k)), while the throughput greatly improves.
An interesting question is how to set k for a single CPU. In
this case, the runtime of Pegasos is of the order of kT . In
Fig. 3 (right) we show that the convergence rate of Pega-
sos as a function of k is approximately constant, for a wide
range of values of k, so long as kT is kept fixed. We leave
further research of both of the theoretical and practical as-
pects of the choice of k to future work.
6. Conclusions
We described and analyzed a simple and effective algo-
rithm for approximately minimizing the objective func-
tion of SVM. The algorithm, called Pegasos, is a modified
stochastic gradient method in which every gradient descent
step is accompanied with a projection step. We derived fast
rate of convergence results and experimented with the algo-
rithm. Our empirical results indicate that for linear kernels,
Pegasos achieves state-of-the-art results, despite or because
of its simplicity. We plan to investigate all the questions we
surfaced in this paper as well as to conduct thorough exper-
iments with non-linear kernels. In addition, we have started
investigating the usage of similar paradigms in other learn-
ing problems such as L1-SVM and other loss functions.
Acknowledgements Part of this work was done while SS and
NS were visiting IBM research labs, Haifa, Israel. This work
was supported by grant I-773-8.6/2003 from the German Israeli
Foundation (GIF).
References
Boyd, S., & Vandenberghe, L. (2004). Convex optimization. Cam-
bridge University Press.
Censor, Y., & Zenios, S. (1997). Parallel optimization: Theory,
algorithms, and applications. Oxford University Press, NY.
Cesa-Bianchi, N., Conconi, A., & Gentile, C. (2004). On the gen-
eralization ability of on-line learning algorithms. IEEE Trans-
actions on Information Theory, 50, 2050–2057.
Cesa-Bianchi, N., & Gentile, C. (2006). Improved risk tail bounds
for on-line algorithms. NIPS.
Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S., & Singer,
Y. (2006). Online passive aggressive algorithms. JMLR, 7.
Cristianini, N., & Shawe-Taylor, J. (2000). An introduction to
support vector machines. Cambridge University Press.
Duda, R. O., & Hart, P. E. (1973). Pattern classification and scene
analysis. Wiley.
Fine, S., & Scheinberg, K. (2001). Efficient svm training using
low-rank kernel representations. JMLR, 2, 242–264.
Freund, Y., & Schapire, R. E. (1999). Large margin classification
using the perceptron algorithm. Mach. Learning, 37, 277–296.
Hazan, E., Kalai, A., Kale, S., & Agarwal, A. (2006). Logarithmic
regret algorithms for online convex optimization. COLT.
Hush, D., Kelly, P., Scovel, C., & Steinwart, I. (2006). Qp al-
gorithms with guaranteed accuracy and run time for support
vector machines. JMLR.
Joachims, T. (1998). Making large-scale support vector machine
learning practical. In B. Schölkopf, C. Burges and A. Smola
(Eds.), Advances in kernel methods - support vector learning.
MIT Press.
Joachims, T. (2006). Training linear svms in linear time. KDD.
Kimeldorf, G., & Wahba, G. (1971). Some results on tchebychef-
fian spline functions. J. Math. Anal. Applic., 33, 82–95.
Kivinen, J., Smola, A. J., & Williamson, R. C. (2002). Online
learning with kernels. IEEE’ TSP, 52, 2165–2176.
Platt, J. C. (1998). Fast training of Support Vector Machines using
sequential minimal optimization. In B. Schölkopf, C. Burges
and A. Smola (Eds.), Advances in kernel methods - support
vector learning. MIT Press.
Shalev-Shwartz, S., & Singer, Y. (2007). Logarithmic regret algo-
rithms for strongly convex repeated games (Technical Report).
The Hebrew University.
Vapnik, V. N. (1998). Statistical learning theory. Wiley.
Zhang, T. (2004). Solving large scale linear prediction problems
using stochastic gradient descent algorithms. ICML.

