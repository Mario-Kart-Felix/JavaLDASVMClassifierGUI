IEEE TRANSACTIONS ON COMPUTERS, VOL. C-25, NO. 12, DECEMBER 1976
Pattern Recognition and Image Processing
KING-SUN FU, FELLOW, IEEE, AND AZRIEL ROSENFELD, FELLOW, IEEE
Abstract-Extensive research and development has taken place
over the last 20 years in the areas of pattern recognition and image
processing. Areas to which these disciplines have been applied in-
clude business (e.g., character recognition), medicine (diagnosis,
abnormality detection), automation (robot vision), military intel-
ligence, communications (data compression, speech recognition),
and many others. This paper presents a very brief survey of recent
developments in basic pattern recognition and image processing
techniques.
Index Terms-Decision-theoretic recognition, image processing,
image recognition, pattern recognition, syntactic recognition.
I. INTRODUCTION
DURING the past twenty years, there has been
a considerable growth of interest in problems of
pattern recognition and image processing. This interest
has created an increasing need for theoretical methods and
experimental software and hardware for use in the design
of pattern recognition and image processing systems. Over
twenty books have been published in the area of pattern
recognition [5], [8], [10], [11], [15], [16], [35], [41], [47], [79],
[82], [86], [89], [110], [111], [118], [122], [123], [136], [137].
In addition, a number of edited books, conference pro-
ceedings, and journal special issues have also been pub-
lished [40], [43], [45], [46], [57], [65], [67], [69], [77], [80],
[96], [113], [121], [127], [128]. Cover [25] has given a com-
prehensive review of the five books published in 1972-1973
[5], [35], [47], [79], [89]. A specialized journal has existed
for nearly ten years [73], and some special pattern recog-
nition machines have been designed and built for practical
use. Applications of pattern recognition and image pro-
cessing include character recognition [37], [71], [123], target
detection, medical diagnosis, analysis of biomedical signals
and images [45], [57], [97], remote sensing [44], [57],
identification of human faces and fingerprints [83], reli-
ability [90], socio-economics [13], archaeology [12], speech
recognition and understanding [43], [45], [98], and machine
part recognition [3].
Many of the books and paper collections on pattern
recognition contain material on image processing and
recognition. In addition, there are four textbooks [4], [35],
[99], [108] and several hardcover paper collections [21],
[51], [60], [67], [74], [106], [132], [138] devoted especially
to the subject, as of the end of 1976. There is a specialized
Manuscript received April 16, 1976: revised June 14, 1976. This work
was supported by the National Science Foundation under Grant ENG
74-17586 and by the National Science Foundation under Grant MCS-
72-03610.
K.-S. Fu is with the School of Electrical Engineering, Purdue Uni-
versity, West Lafayette, IN 47907.
A Rosenfeld is with the Computer Science Center, University of
Maryland, College Park, MD 20742.
journal in the field [107], and there have also been special
issues of several other journals on the topic [1], [6], [7], [53].
For further references, the reader may consult a series of
annual survey papers [100]-[105] which cover a significant
fraction of the English language literature.
Although pattern recognition and image processing have
developed as two separate disciplines, they are very closely
related. The area of image processing consists not only of
coding, filtering, enhancement, and restoration, but also
analysis and recognition of images. On the other hand, the
area of pattern recognition includes not only feature ex-
traction and classification, but also preprocessing and
description of patterns. It is true that image processing
appears to consider only two-dimensional pictorial pat-
terns and pattern recognition deals with one-dimensional,
two-dimensional, and three-dimensional patterns in
general. However, in many cases, information about one-
dimensional and three-dimensional patterns is easily ex-
pressed as two-dimensional pictures, so that they are ac-
tually treated as pictorial patterns. Furthermore, many of
the basic techniques used for pattern recognition and
image processing are very similar in nature. Differences
between the two disciplines do exist, but we also see an
increasing overlap in interest and a sharing of methodol-
ogies between them in the future.
Within the length limitations of this paper, we provide
a very brief survey of recent developments in pattern
recognition and image processing.
II. PATTERN RECOGNITION
Pattern recognition is concerned primarily with the
description and classification of measurements taken from
physical or mental processes. Many definitions of pattern
recognition have been proposed [112], [125], [127]. Our
discussion is based on the above loose definition. In order
to provide an effective and efficient description of patterns,
preprocessing is often required to-remove noise and re-
dundancy in the measurements. Then a set of character-
istic measurements, which could be numerical and/or
nonnumerical, and relations among these measurements,
are extracted for the representation of patterns. Classifi-
cation and/or description of the patterns with respect to
a specific goal is performed on the basis of the represen-
tation.
In order to determine a good set of characteristic mea-
surements and their relations for the representation of
patterns so good recognition performance can be expected,
a careful analysis of the patterns under study is necessary.
Knowledge about the statistical and structural charac-
1336
FU AND ROSENFELD: PATTERN AND IMAGE PROCESSING
teristics of patterns should be fully. utilized. From this
point of view, the study of pattern recognition includes
both the analysis of pattern characteristics and the design
of recognition systems.
The many different mathematical techniques used to
solve pattern recognition problems may be grouped into
two general approaches. They are the decision-theoretic
(or discriminant) approach and the syntactic (or struc-
tural) approach. In the decision-theoretic approach, a set
of characteristic measurements, called features, are ex-
tracted from the patterns. Each pattern is represented by
a feature vector, and the recognition of each pattern is
usually made by partitioning the feature space. On the
other hand, in the syntactic approach, each pattern is ex-
pressed as a composition of its components, called sub-
patterns or pattern primitives. This approach draws an
analogy between the structure of patterns and the syntax
of a language. The recognition of each pattern is usually
made by parsing the pattern structure according to a given
set of syntax rules. In some applications, both of these
approaches may be used. For example, in a problem deal-
ing with complex pattems, the decision-theoretic approach
is usually effective in the recognition of pattern primitives,
and the syntactic approach is then used for the recognition
of subpatterns and of the pattern itself.
A. Decision- Theoretic Methods
A block diagram of a decision-theoretic pattern recog-
nition system is shown in Fig. 1. The upper half of the di-
agram represents the recognition part, and the lower half
the analysis part. The process of preprocessing is usually
treated in the area of signal and image processing. Our
discussions are limited to the feature extraction and se-
lection, and the classification and learning. Several more
extensive surveys on this subject have also appeared re-
cently [18], [26], [32], [66], [120].
Feature extraction and selection: Recent developments
in feature extraction and selection fall into the following
two major approaches.
Feature space transformation: The purpose of this
approach is to transform the original feature space into
lower dimensional spaces for pattern representation and/or
class discrimination. For pattern representation, least
mean-square error and entropy criteria are often used as
optimization criteria in determining the best transfor-
mation. For class discrimination, the maximization of in-
terclass distances and/or the minimization of intraclass
distances is often suggested as an optimization criterion.
Both linear and nonlinear transformations have been
suggested. Fourier, Walsh-Hadamard, and Haar trans-
forms have been suggested for generating pattern features
[5]. The Karhunen-Loeve expansion and the method of
principal components [5], [39], [47] have been used quite
often in practical applications for reducing the dimen-
sionality of feature space.
In terms of the enhancement of class separability,
nonlinear transformations are in general superior to linear
transformations. A good class separation in feature space
will certainly result in a simple classifier structure (e.g., a
linear classifier). However, the implementation of non-
linear transformations usually requires complex compu-
tations compared with that of linear transformations.
Results of transformations need to be updated when new
pattern samples are taken into consideration. Iterative
algorithms and/or interactive procedures are often sug-
gested for implementing nonlinear transformations [45],
[57].
In some cases, the results of transformations based on
pattern representation and class discrimination respec-
tively are in conflict. An optimization criterion for feature
space transformation should be able to reflect the true
performance of the recognition system. Some recent work
appears to move in this direction [31].
Information and distance measures: The main goal
of feature selection is to select a subset of 1 features from
a given set ofN features (1 <N) without significantly de-
grading the performance of the recognition system, that
is, the probability of misrecognition, or more generally, the
risk of decision. Unfortunately, a direct calculation of the
probability of misrecognition is often impossible or im-
practical partially due to the lack of general analytic ex-
pressions which are simple enough to be treated. One ap-
proach is to find indirect criteria to serve as a guide' for
feature selection.
The most common approach is to define an information
or (statistical) distance measure, which is related to the
upper and/or lower bounds on the probability of misre-
cognition, for feature selection [17], [19], [54], [66]. That
is, the best feature subset is selected in the sense of maxi-
mizing a prespecified information or distance measure.
Recently, Kanal [66] provided a fairly complete list of
distance measures and their corresponding error bounds.
Assuming that the most important characteristic of the
distance measure is the resultant upper bound on the
probability of misrecognition, the various measures can
be arranged in increasing order of importance. For a two-
class recognition problem, denoting the upper bound on
the probability of misrecognition by Pe, for Bhattacha-
ryya's distance by UB, for Matusita distance by UM, for
equivocation by UE, for Vajda's entropy by Uv, for Dev-
ijver's Bayesian distance by UD, for Ito's measure (for n
= 0) by UI, for Kolmogorov's variational distance by UK,
and for the MO-distance of Toussaint by UT, the following
point-wise relations hold [75]:
Pe = UK ' UV = UD = UI = UT ' UE ' UB = UM.
The divergence and Kullback-Leibler numbers, which are
simply related to each other, are excluded from the or-
dering because of the lack of a known upper bound except
for the case of a normal distribution where its bound is
larger than UB. In terms of computational difficulty,
however, the divergences and Bhattacharyya distance are
easier to compute than the other distance measures.
It is interesting that the best bound on the probability
of misrecognition (except UK which is nothing but Pe it-
self) derived from the distance measures is equal to the
1337
IEEE TRANSACTIONS ON COMPUTERS, DECEMBER 1976
RECOGNITION
ANALYSIS I
Fig. 1. Block diagram of a decision-theoretical pattern recognition system.
asymptotic error of the single nearest neighbor classifier.
In addition to the information and distance measures
mentioned above, a generalized Kolmogorov distance,
called the Ja, separability measure, was recently proposed
as a feature selection criterion, and its upper and lower
bounds on the probability of misrecognition derived [75].
When a = 1, J, is equivalent to the Kolmogorov distance.
For a = 2, the upper bound of the probability of misre-
cognition is equal to the asymptotic probability of error of
the single nearest neighbor classifier.
Classification and learning: Most developments in
pattern recognition involve classification and learning.
When the conditional probability density functions of the
feature vectors for each class (which we may call the class
density functions) are known or can be accurately esti-
mated, the Bayes classification rule that minimizes the
average risk or the probability of misrecognition can be
derived. When the class density functions are unknown,
nonparametric classification schemes need to be used. In
practice, when a large number of pattern samples is
available, class density functions can be estimated or
learned from the samples [24], [28], [126], and then an
optimal classification rule can be obtained. If the para-
metric form of each class density function is known, only
parameters need to be learned from the pattern samples.
When the number of available pattern samples is small,
the performance of density and parameter estimations is
poor. Nonparametric classification schemes usually
suggest a direct learning of the classification rule from
pattern samples, for example, the learning of parameters
of a decision boundary.
Depending upon whether or not the correct classiflcation
of the available pattern samples is known, the process of
learning can be classified into supervised learning (or
learning with a teacher) and nonsupervised learning (or
learning without a teacher). Bayesian estimation and
stochastic approximation and the potential function
method have been suggested for the learning of class
density functions or a decision boundary. When the
learning is nonsupervised, a mixture density function can
be formed from all the individual class density functions
and a priori class probabilities. Nonsupervised learning
of the parameters of each class density function can be
treated as a supervised learning of parameters of the
mixture density function from the unclassified pattern
samples followed by a decomposition procedure. Under
certain conditions, the decomposition can be accomplished
and the estimates of the parameters of each class recov-
ered. A related topic which has received an increasing
amount of attention recently is learning with finite mem-
ory [26].
When the a priori information is sufficient, the classifier
may be able to make decisions with good performance. In
this case, the learning process could be carried out using
the classifier's own decisions; that is, the unclassified
pattern samples are now classified by the classifier itself.
This type of nonsupervised learning is called decision-
directed learning. When the classification of the pattern
samples is incompletely known, learning with an imperfect
teacher and learning with a probabilistic teacher have re-
cently been proposed [27], [64]. An appropriate combina-
tion of supervised and nonsupervised modes of learning
could result in a system of lower cost than those using a
single learning mode [18], [23].
Classification based on clustering analysis has been re-
garded as a practically attractive approach, particularly
in a nonsupervised situation with the number of classes not
precisely known. Various similarity and (deterministic)
distance measures have been suggested as criteria for
clustering pattern samples in the feature space [33], [34].
Both hierarchical and nonhierarchical strategies are pro-
posed for the clustering process. Often, some of the clus-
tering parameters, such as the similarity measure and
threshold, criteria for merging and/or splitting clusters,
etc., need to be selected heuristically or through an inter-
active technique. It should be interesting to relate directly
the distance measures for feature selection to those for
clustering analysis [19], [135]. Recently, clustering algo-
rithms using adaptive distance were proposed [34]. The
similarity measure used in the clustering process varies
according to the structure of the clusters already observed.
Mode estimation, least mean-square optimization, graph
theory and combinatorial optimization have been used as
a possible theoretical basis for clustering analysis [5], [20],
1338
FU AND ROSENFELD: PATTERN AND IMAGE PROCESSING
[70], [80], [133]. Nevertheless, clustering analysis, at its
present state-of-the-art, still appears to be an experi-
ment-oriented "art."
Remarks: Most results obtained in feature selection and
learning are based on the assumption that a large number
of pattern samples is available, and, consequently, the
required statistical information can be accurately esti-
mated. The relationship between the dimensionality of
feature space and the number of pattern samples required
for learning has been an important subject of study. In
many practical problems, a large number of pattern sam-
ples may not be available, and the results of small sample
analysis could be quite misleading. The recognition system
so designed will usually result in an unreliable perfor-
mance. In such cases, the study of finite sample behavior
of feature selection and learning is very important. The
degradation of performance in feature selection, learning
and error estimation [48], [119] due to the availability of
only a small number of samples needs to be investi-
gated.
In some practical applications, the number of features
N and the number of pattern classes m are both very large.
In such cases, it would be advantageous to use a multilevel
recognition system based on a decision tree scheme. At the
first level, m classes are classified into i groups using only
N1 features. Here, i << m and N1 << N, and the N1 features
selected are the best features to classify these i groups. In
an extreme case, i = 2 so a two-class classifier can be used,
or N1 = 1 so a one-dimensional (thresholding) classifier can
be used. The same procedure is then repeated at the second
and third levels, etc., until each of the original m classes
can be separately identified. Now, following each path in
the decision tree, we should be able to recognize each of the
m classes.
The idea of adaptively selecting a smaller number of
features at different levels of classification appears to be
very attractive in applications. An optimal design of such
a tree scheme may be computationally quite complex.
However, several heuristic design techniques have recently
been suggested [44], [58], [72], [134].
B. Syntactic (or Structural) Methods
A block diagram of a syntactic pattern recognition sys-
tem is shown in Fig. 2. Again, we divide the block diagram
into the recognition part and the analysis part, where the
recognition part consists of preprocessing, primitive ex-
traction (including relations among primitives and sub-
patterns), and syntax (or structural) analysis, and the
analysis part includes primitive selection and grammatical,
(or structural) inference.
In syntactic methods, a pattern is represented by a
sentence in a language which is specified by a grammar.
The language which provides the structural description
of patterns, in terms of a set of pattern primitives and their
composition relations, is sometimes called the "pattern
description language." The rules governing the composi-
tion of primitives into patterns are specified by the so-
called "pattern grammar." An alternative representation
of the structural information of a pattern is to use a "re-
lational graph," of which the nodes represent the subpat-
terns and the branches represent the relations between
subpatterns.
Primitive extraction and selection: Since pattern
primitives are the basic components of a pattern, pre-
sumably they are easy to recognize. Unfortunately, this is
not necessarily the case in some practical applications. For
example, strokes are considered good primitives for script
handwriting, and so are phonemes for continuous speech;
however, neither strokes nor phonemes can easily be ex-
tracted by machine. The segmentation problems for script
handwriting and continuous speech, respectively, are still
subjects of research. An approach to waveform segmen-
tation through functional approximation has recently been
reported [92]. Segmentation of pictorial patterns is dis-
cussed in Section III under Segmentation.
There is no general solution for the primitive selection
problem at this time. For line patterns or patterns de-
scribed by boundaries or skeletons, line segments are often
suggested as primitives. A straight line segment could be
characterized by the locations of its beginning (tail) and
end (head), its length, and/or slope. Similarly, a curve
segment might be described in terms of its head and tail
and its curvature. The information characterizing the
primitives can be considered as their associated semantic
information or as features used for primitive recognition.
Through the structural description and the semantic
specification of a pattern, the semantic information asso-
ciated with its subpatterns or the pattern itself can then
be determined. For pattern description in terms of regions,
half-planes have been proposed as primitives [91]. Shape
and texture measurements are often used for the de-
scription of regions; see Section III under Properties.
Pattern grammars: After pattern primitives are se-
lected, the next step is the construction of a grammar (or
grammars) which will generate a language (or languages)
to describe the patterns under study. It is known that in-
creased descriptive power of a language is paid for in terms
of increased complexity of the syntax analysis system
(recognizer or acceptor). Finite-state automata are capable
of recognizing finite-state languages although the de-
scriptive power of finite-state languages is also known to
be weaker than that of context-free and context-sensitive
languages. On the other hand, nonfinite, nondeterministic
procedures are required, in general, to recognize languages
generated by context-free and context-sensitive grammars.
The selection of a particular grammar for pattern de-
scription is affected by the primitives selected, and by the
tradeoff between the grammar's descriptive power and
analysis efficiency. Context-free programmed grammars,
which maintain the simplicity of context-free grammars
but can generate context-sensitive languages, have recently
been suggested for pattern description [41].
A number of special languages have been proposed for
the description of patterns such as English and Chinese
characters, chromosome images, spark chamber pictures,
1339
IEEE TRANSACTIONS ON COMPUTERS, DECEMBER 1976
RECOGNITION T
ANALYSIS
Fig. 2. Block diagram of a syntactic pattern recognition system.
two-dimensional mathematics, chemical structures, spo-
ken words, and fingerprint patterns [41], [46]. For the
purpose of effectively describing high dimensional pat-
terns, high dimensional grammars such as web grammars,
graph grammars, tree grammars, and shape grammars
have been used for syntactic pattern recognition [41], [50],
[88], [131]. Initial applications include fingerprint pattern
recognition and the interpretation of Earth Resources
Technology Satellite data [44], [46], [83].
Ideally speaking, it would be nice to have a grammatical
(or structural) inference machine which would infer a
grammar or structural description from a given set of
patterns. Unfortunately, such a machine has not been
available except for some very special cases [42]. In most
cases so far, the designer constructs the grammar based on
the a priori knowledge available and his experience.
-In some practical applications, a certain amount of un-
certainty exists in the process under study. For example,
due to the presence of noise and variation in the pattern
measurements, segmentation error and primitive extrac-
tion error may occur, causing ambiguities in the pattern
description languages. In order to describe noisy and dis-
torted patterns under ambiguous situations, the use of
stochastic languages has been suggested [41], [52]. With
probabilities associated with grammar rules, a stochastic
grammar generates sentences with a probability distri-
bution. The probability distribution of the sentences can
be used to model the noisy situations. Other approaches
for the description of noisy and distorted patterns using
syntactic methods include the use of approximation and
transformational grammars [43], [93]. The effectiveness
of these approaches remains to be developed and tested.
Syntactic recognition: Conceptually, the simplest form
of recognition is probably "template-matching." The
sentence describing an input pattern is matched against
sentences representing each prototype or reference pat-
tern. Based on a selected "matching" or "similarity" cri-
terion, the input pattern is classified in the same class as
the prototype pattern which is the "best" to match the
input. The structural information is not recovered. If a
complete pattern description is required for recognition,
a parsing or syntax analysis is necessary. In between the
two extreme situations, there are a number of intermediate
approaches. For example, a series of tests can be designed
to test the occurrence or nonoccurrence of certain sub-
patterns (or primitives) or certain combinations of them.
The result of the tests, through a table lookup, a decision
tree, or a logical operation, is used for a classification de-
cision.
A parsing procedure for recognition is, in general, non-
deterministic and, hence, is regarded as computationally
inefficient. Efficient parsing could be achieved by using
special classes of languages such as finite-state and
deterministic languages for pattern description. The
tradeoff here between the descriptive power of the pattern
grammar and its parsing efficiency is very much like that
between the feature space selected and the classifier's
discrimination power in a decision-theoretic recognition
system. Special parsers using sequential procedures or
other heuristic means for efficiency improvement in syn-
tactic pattern recognition have recently been constructed
[76], [94], [114].
Error-correcting parsers have been proposed for the
recognition of noisy and distorted patterns [49], [117].
Different types of segmentation and primitive extraction
errors (substitution, deletion and addition) are introduced
into the pattern grammar. The recognition process is then
based on the parsers designed according to the expanded
pattern grammar. The error-correcting capability is
achieved by using a minimum-distance criterion. Since the
original grammar is expanded to include all possible error
situations, the parser so designed is less efficient than that
designed according to the original grammar. This tradeoff
between error-correcting capability and parsing efficiency
seems to be expected. Nevertheless, it could be a very se-
rious drawback in practical applications.
When stochastic grammars are used for pattern de-
scription, the probability information is useful in resolving
ambiguous situations. For example, if a sentence is found
to be generated by two different pattern grammars, the
ambiguity can be resolved by comparing the generation
probabilities of the sentence in the two grammars. A
maximum-likelihood or Bayes decision rule based on the
two generation probabilities will yield the final recognition.
Besides, the probability information can also be utilized
to speed up the parsing process [41]. The use of a sequen-
tial decision procedure could result in further reducing the
parsing time by slightly increasing the probability of
1340
Ftl AND ROSENFELD: PATTERN AND IMAGE PROCESSING
misrecognition [76]. Of course, when a sequential proce-
dure is used, the parsing procedure stops most of the time
before a sentence is completely scanned, and, conse-
quently, in these cases the complete structural information
on the pattern cannot be recovered.
Remarks: Compared with decision-theoretic pattern
recognition, syntactic pattern recognition is a newer area
of research. When the patterns are complex and the
number of pattern class is very large, it would be advan-
tageous to describe each pattern in terms of its components
and to consider description and classification of patterns
rather than classification only. Of course, the practical
utility of the syntactic approach depends upon our ability
to recognize the simple pattern primitives and their rela-
tionships represented by the composition operations.
As research in both decision-theoretic and syntactic
approaches is still in progress, heuristic methods are also
being developed for specific purposes. New approaches
proposed recently for pattern recognition include vari-
able-value logic [81] and relation theory [55]. The effec-
tiveness of these approaches still has to be tested.
III. IMAGE PROCESSING AND RECOGNITION
From its earliest beginnings, pattern recognition has
dealt to a substantial extent with pictorial patterns. Sec-
tion 3.2 of this paper reviews a few of the major themes in
"image recognition." At the same time, extensive work has
been done on aspects of image processing that are not di-
rectly related to pattern recognition-in particular, on
image coding (for reduced-bandwidth transmission) and
on image enhancement (for improving the appearance of
images). Some aspects of this work are briefly discussed
in Section 3.1.
For the purposes of this review, image processing refers
to operations that transform images into other images,
while image recognition is the mapping of images into
(nonimage) descriptions. Different from both of these is
computer graphics, which deals primarily with the com-
puter synthesis and manipulation of images that are
specified by descriptions; this subject is not reviewed here.
We also do not cover optical (or other analog) methods of
image processing, which has a large literature of its own.
A. Image Processing
Coding: In order to acceptably approximate a standard
television image digitally, one normally needs an array of
about 500 X 500 samples, each quantized to about 50 dis-
crete gray levels-i.e., a total of about 6 bits for each of the
250 000 samples, or 1.5 million bits in all. The goal of image
compression (or, as it is more commonly called, image
coding) is to represent the image acceptably using a much
smaller number of bits [61].
One basic approach to image coding is to apply an in-
vertible transform to the given image, approximate the
transform and then construct the approximated image by
inverting the transform. The transform can be designed
so that it can be approximated more economically than the
original image, and so that errors in this approximation
become less noticeable when an image is reconstructed
from its transform. For example, if we use the Fourier
transform, we can achieve economical approximations
because many of the Fourier coefficients (in the transform
of a normal image) have negligible magnitudes, and so can
be ignored or at least quantized very coarsely. Moreover,
errors in approximating the Fourier coefficients are gen-
erally hard to notice when the image is reconstructed, be-
cause their effects are distributed over the entire image.
Image compressions of as much as 10:1 can be achieved
using this "transform coding" approach [7].
Many other approaches to image coding have been ex-
tensively investigated, but only a few of these can be
mentioned here. One class of approaches takes differences
between successive image samples; since these have a very
nonuniform probability density (peaked at zero), they can
be quantized acceptably using relatively few quantization
levels. Note, however, that when the image is reconstructed
from such differences by summing them, errors in the
differences will tend to propagate, so that care is needed
in using this type of approach. The differences used can
be either spatial (intraframe) or temporal (interframe)
[7].
The expected accuracy of an image coding system can
be predicted theoretically if we assume a model for the
class of images being encoded (usually, a homogeneous
random field) and a specific error criterion (usually, mean
squared error). Both of these assumptions are question-
able. Images usually consist of distinct parts (objects or
regions), so that a homogeneous random field model is
inappropriate. On the other hand, the human visual sys-
tem's sensitivity to errors is highly context-dependent, so
that an integrated squared error criterion is inadequate.
Work is needed on image coding techniques which segment
the image into significant parts before attempting to ap-
proximate it; some image segmentation methods will be
discussed here under Segmentation. At the same time,
increased understanding of human visual capabilities is
needed so that better error criteria for image coding sys-
tems can be developed.
Enhancement and restoration: There has been in-
creasing interest in recent years in techniques for designing
two-dimensional digital filters [60]. At the same time,
much work is being done on digital methods of enhancing
or restoring degraded images. Some enhancement tech-
niques are conceptually very simple, and involve only
pointwise modification of the image's grayscale. For ex-
ample, one can analyze the gray levels in a neighborhood
of each image point, determine a grayscale transformation
that stretches these levels over the full displayable range,
and apply this transformation to the given point (and the
points in its immediate vicinity); this and similar tech-
niques tend to give very good enhancement results [62].
A more sophisticated class of image enhancement
techniques are designed to undo the effects of degradations
on the image. It is customary to model these degradations
as additive combinations of blurring and noise operations,
1341
IEEE TRANSACTIONS ON COMPUTERS, DECEMBER 1976
where the blurring takes the form of a weighted sum or
integral operation applied to the ideal image, and the noise
is uncorrelated with the ideal image. A variety of methods
have been developed for inverting the effects of the blur-
ring operator; for example, pseudoinverse techniques can
be used to define a deblurring operator which yields the
best approximation to the ideal image in the expected least
squares sense [7], [60]. Other classes of methods, e.g., based
on Kalman filtering, have been devised to yield least-
squares estimates of an ideal image corrupted by additive
noise [7], [60]. As in the case of image coding, these ap-
proaches have usually been based on homogeneous random
field models for the images (and noise), and on least-
squares error criteria, both of which are questionable as-
sumptions. Here too, image models based on segmentation
of the image, and success criteria more closely related to
human perceptual abilities, would be highly desirable.
A problem closely related to image restoration is that of
reconstructing images; or three-dimensional objects, from
sets of projections, e.g., from X-ray views taken from many
angles. (The gray levels on a projection are linear combi-
nations of the ideal gray levels, just like the gray levels on
a blurred image.) Much work has been done in this area in
the past few years, especially in connection with medical
radiographic applications [22].
B. Image Recognition
The goal of image recognition is the classification or
structural description of images. Image classification in-
volves feature detection sr property measurement; image
description involves, in addition, segmentation and rela-
tional structure extraction. Some significant ideas in each
of these areas are reviewed in the following paragraphs.
Historically, the techniques used have usually been de-
veloped on heuristic grounds, but there is increasing in-
terest in deriving optimum techniques based on models for
the classes of images to be analyzed.
Matching and feature detection: Detecting the presence
of a specified pattern (such as an edge, a line, a particular
shape, etc.) in an image requires matching the image with
a "template," or standardized version of the pattern. This
is a computationally costly process, but techniques have
been developed for reducing its expected cost [9]. For ex-
ample, one can match a subtemplate (or a reduced-reso-
lution "coarse template") with the image at every point,
and use the remainder of the template (or the full-resolu-
tion template) only at points where the initial match value
is above some threshold. The subtemplate size, or the de-
gree of coarseness, can be chosen to minimize the expected
cost of this process [124]. In computing these matches, one
should first check parts of the template that have large
expected mismatch values (with a randomly chosen part
of the picture), in order to minimize the expected amount
of comparison that must be done before the possibility of
a match at the given point is rejected [84]. Of course, the
savings in computational cost must be weighed against the
possible increased costs of false alarms or dismissals.
Template matching is often implemented as a linear
operation in which the degree of match at a point is mea-
sured by a linear combination of image gray levels in a
neighborhood of the point. However, the result of such a
linear operation is generally ambiguous; for example, it
may have the same value for a high-contrast partial match
as it does for a lower contrast, but more complete, match.
Such ambiguities can often be eliminated by breaking the
template up into parts and requiring that specified match
conditions be satisfied for each part, or for the most of the
parts. This approach has been used to detect curves in
noise [109], it needs to be extended to other types of image
matching problems.
The use of template parts can also help overcome the
sensitivity of template matching to geometrical distortion.
Rather than matching the entire template with the image,
one can match the parts, and then look for combinations
of positions. Optimal combinations can be determined by
mathematical programming techniques [38], or by simul-
taneous iterated reinforcement of the partial matches
based on the presence of the other needed matches [63].
Research on these approaches is still at an exploratory
stage.
Segmentation: Images are often composed of regions
that have different ranges of gray levels, or of the values
of some other local property. Such an image can be seg-
mented by examining its gray level (or local property)
histogram for the presence of peaks corresponding to the
ranges, and using thresholds to single out individual peaks
[30], [87], [116]. Detection of the peaks can be facilitated
by hitogramming only a selected set of image points, e.g.,
points where the local property value is a local maximum
[140], or points that lie on or near region boundaries (which
can be identified by the presence of high values of a de-
rivative operator) [130].
Parallel methods of region extraction based on thresh-
olding are potentially less flexible than sequential meth-
ods, which can "learn as they go" about the geometrical,
textural, and gray level properties of the region being ex-
tracted, and can compare them with any available infor-
mation about the types of regions or objects that are sup-
posed to be present in the image. Such information can be
used to control merging and splitting processes with the
aim of creating an acceptable partition of the image into
regions [36], [59], [116].
An important special case of sequential region growing
is tracking, which extracts regions (or region boundaries)
in the form of thin curves. This technique can be regarded
as a type of piecewise template matching, where the pieces
are short line or curve segments, and a curve is any com-
bination of these that smoothly continue one another; thus,
here again, curves can be extracted by mathematical pro-
gramming or iterated reinforcement techniques. The same
is true for a wide variety of problems involving the selection
of image parts that satisfy given sets of constraints [63].
Properties: Once regions have been extracted from an
image, it becomes possible-to describe the image in terms
of properties of these regions. Much work has been done
1342
FU AND ROSENFELD: PATTERN AND IMAGE PROCESSING
on defining and measuring 'basic geometrical properties
of regions in a digitized image, such as connectedness,
convexity, compactness, etc. Describing the shape of a
region involves not only global properties such as those just
listed, but also a hierarchically structured description in
terms of "angles" and "sides" (i.e., polygonal approxima-
tion, of varying degrees of coarseness), symmetries, and so
on [29].
Two "dual" methods of describing a region involve its
boundary and its "skeleton." A region is determined by
specifying the equations of its boundary curves; and it is
also determined by specifying the centers and radii of the
maximal disks that it contains [14]. (These disks define a
sort of minimal piecewise approximation to the shape; such
approximations can also be defined for grayscale images
composed of regions that are approximately piecewise
constant.) Skeleton descriptions can also be used in three
dimensions, where a shape can be constructed out of
"generalized cylinders," each of which is specified by a
locus of centers and an associated radius function [2],
[85].
Grayscale, as well as geometrical, properties of regions
are of importance in image description. Of particular im-
portance are textural properties (e.g., coarseness and di-
rectionality), which can be measured in terms of certain
statistics of the second-order probability density of gray
levels in the region [561-or equivalently, statistics of the
first-order probability density of gray level differences (or
other local property values) [129]. Textures can be modeled
as distorted periodic patterns [139], as two-dimensional
"seasonal time series" [78], or in terms of random geome-
try; such models can be used to predict the values of the
property measures for real-world textures.
Image and scene analysis: Image descriptions can
usually be expressed in the form of relational structures
which represent relationships among, and properties of,
image parts. A major area of artificial intelligence research
has been the study ofhow knowledge about the given class
of scenes can be used to control the process of extracting
such descriptions from an image. (See also the article on
artificial intelligence in this issue.) In addition to the study
of control structures for image analysis, there has also been
recent interest in special data structures for image pro-
cessing and description, e.g., "cone" or "pyramid" struc-
tures for variable-resolution image analysis [68], [115] and
"fuzzy" structures for representing incompletely specified
image parts.
The term "scene analysis" is generally used in connec-
tion with the description of images of three-dimensional
objects seen from nearby, so that perspective and occlusion
play major roles in the description [116]. (Note that the
images being analyzed in applications such as document
processing, photomicrography, radiology, and remote
sensing are all basically two-dimensional.) Much work has
been done on the extraction of three-dimensional depth
information about scenes, using range sensors, stereo pairs
of images, or single-image depth cues such as shading and
texture gradients. These techniques are beginning to be
applied to the analysis of various types of real-world indoor
and outdoor scenes.
Another approach to image analysis involves the use of
formal models derived from the theory of multidi-
mensional formal languages. On this topic see the discus-
sion of syntactic pattern analysis in Section II-B of this
paper.
IV. CONCLUSIONS
It has been felt that in the past there was an unbalanced
development between theory and practice in pattern rec-
ognition. Many theoretical results, especially in connection
with the decision-theoretic approach, have been published.
Practical applications have been gradually emphasized
during the last five years, particularly in medical and re-
mote sensing areas. Most of the practical results are con-
sidered inconclusive and require further refinement. Im-
plementation of a practical system is often on a general
purpose computer facility rather than on special purpose
hardware. There is no doubt that, though heavily moti-
vated by practical applications, pattern recognition is still
very much an active research area.
In the decision-theoretic approach, we are still looking
for effective and efficient feature extraction and selection
techniques, particularly in nonparametric and small
sample situations. The computational complexity of pat-
tern recognition systems, in terms of time and memory,
should be an interesting subject for investigation. In the
syntactic approach, the problem of primitive extraction
and selection certainly needs further attention. An ap-
propriate selection of the pattern grammar directly affects
the computational complexity or analysis efficiency of the
resulting recognition system. Grammatical inference al-
gorithms which are computationally feasible are still highly
in demand.
In image processing, better models are needed for both
the images and their user (the human visual system).
Image models should also be used more extensively in the
design of optimal image segmentation and feature ex-
traction procedures. When the goal is not objectively
specifiable, but rather involves general-purpose man-
machine dialog about images, the computer will be also
need to understand the visual capabilities and limitations
of its human partner. Thus image and visual models need
further development in both image processing and recog-
nition.
REFERENCES
[1] J. K. Aggarwal and R. 0. Duda, Eds., "Special issue on digital fil-
tering and image processing," IEEE Trans. Circuits Syst., vol.
CAS-2 pp. 161-304,1975.
[2] G. J. Agin and T. 0. Binford, "Computer description of curved
objects," in Proc. 3rd Int. Joint Conf. Artificial Intelligence, 1973,
pp. 629-640.
[3] G. J. Agin and R. 0. Duda, "SRI vision research for advanced in-
dustrial automatic," in Proc. 2nd USA-Japan Comput. Conf., Aug.
26-28, 1975, Tokyo, Japan.
[4] H. C. Andrews, Computer Techniques in Image Processing. New
York: Academic, 1970.
[51 , Introduction to Mathematical Techniques in Pattern
1343
IEEE TRANSACTIONS ON COMPUTERS, DECEMBER 1976
Recognition, New York: Wiley, 1972.
[6] H. C. Andrews, Ed., "Special issue on digital picture processing,"
Comput., vol. 7, pp. 17-87, May 1974.
[7] H. C. Andrews and L. H. Enloe, Fjds., "Special issue on digital
picture processing," Proc. IEEE, vol. 60, pp. 766-898, July
1972.
[8] A. G. Arkadev and E. M. Braverman, Learning in Pattern Clas-
sification Machines. Moscow: Nauka, 1971.
[9] D. I. Barnea and H. F. Silverman, "A class of algorithms for fast
digital image registration," IEEE Trans. Comput., vol. C-21, pp.
179-186, 1972.
[101 B. G. Batchelor, Practical Approach to Pattern Classification.
New York: Plenum, 1974.
[11] P. W. Becker, An Introduction to the Design of Pattern Recog-
nition Devices. New York: Springer, 1971.
[12] P. W. Becker, "Pattern recognition applications in work with an-
cient objects," in Proc. NATO Advanced Study Institute on
Pattern Recognition-Theory and Applications, Sept. 8-17,
1975.
[13] J. M. Blin, Patterns and Configurations in Economic Science.
Dordrect, The Netherlands: Reidel, 1973.
[14] H. Blum, "A transformation for extracting new descriptors of
shape," in Models for the Perception of Speech and Visual Form,
W. Wathen-Dunn, Ed. Cambridge, MA: M.I.T. Press, 1967, pp.
362-380.
[15] M. Bongard, Pattern Recognition. Moscow: Nauka, 1967 (English
transl., Washington, DC: Spartan, 1970).
[16] C. H. Chen, Statistical Pattern Recognition. New York: Hayden,
1973.
[17] --, "On a class of computationally efficient feature selection
criteria," Pattern Recognition, vol. 7, no. 1/2, June 1975.
[18] --, "Statistical pattern recognition-Review and outlook,"
Southeastern Massachusetts Univ., North Dartmouth, MA, Tech.
Rep. TR-EE 75-4, June 25, 1975.
[19]--, "On the use of distance and information measures in pattern
recognition and applications," in Proc. NATO Advanced Study
Institute on Pattern Recognition-Theory and Applications,
Sept. 8-17, 1975.
[20] Z. Chen and K. S. Fu, "On the connectivity of clusters," Inform.
Sci., vol. 8, pp. 283-299, 1975.
[21] G. C. Cheng, D. K. Pollock, R. S. Ledley, and A. Rosenfeld, Eds.,
Pictorial Pattern Recognition. Washington, DC: Thompson,
1968.
[22] Z. H. Cho, Ed., Special issue on physical and computational as-
pects of 3-dimensional image reconstruction, IEEE Trans. Nu-
clear Sci., vol. NS-21, June 1974.
[23] D. B. Cooper, "When should a learning machine ask for help,"
IEEE Trans. Inform. Theory, vol. IT-20, July 1974.
[24] T. Cover, "A hierarchy of probability density function estimates,"
in Frontiers of Pattern Recognition, S. Wantanabe, Ed. New
York: Academic, 1972.
[25] ,"Recent books on pattern recognition," IEEE Trans. Inform.
Theory, vol. IT-19, Nov. 1973.
[26] T. M. Cover and T. J. Wagner, "Topics in statistical pattern rec-
ognition," in Digital Pattern Recognition, K. S. Fu, Ed. New
York: Springer, 1976.
[27] B. V. Dasarathy and A. L. Lakshminarasimhan, "Sequential
learning employing unfamiliar teacher hypothesis with concurrent
estimation of both the parameters and teacher characteristics,"
Int. J. Comput. Inform. Sci., vol. 5, Mar. 1976.
[28] H. I. Davies and E. J. Wagman, "Sequential nonparametric density
estimation," IEEE Trans. Inform. Theory, vol. IT-21, Nov.
1975.
[29] L. S. Davis, "Understanding shape: Angles and sides," IEEE Trans.
Comput., to be published.
[30] L. S. Davis, A. Rosenfeld, and J. S. Weszka, "Region extraction by
averaging and thresholding," IEEE Trans. Syst., Man, Cybern.,
vol. SMC-5, pp. 383-388, 1975.
[31] R. J. P. de Figueiredo, "Optical linear and nonlinear feature ex-
traction based on the minimization of the increased risk of mis-
classification," Rice Univ. Inst. Comput. Services Appl., Tech. Rep.
275-025-014, June 1974.
[32] P. A. Devijver, "Decision-theoretic and related approaches to
pattern classification," in Proc. NATO Advanced Study Institute
on Pattern Recognition-Theory and Applications, Sept. 8-17,
1975.
[33] E. Diday, "Recent progress in distance and similarity measures
in pattern recognition," in Proc. 2nd Int. Joint Conf. Pattern
Recognition, Aug. 13-15,1974.
[34] E. Diday and J. C. Simon, "Clustering analysis," in Digital Pattern
Recognition, K. S. Fu, Ed. New York: Springer, 1976.
[35] R. 0. Duda and P. E. Hart, Pattern Classification and Scene
Analysis. New York: Wiley, 1973.
[36] J. A. Feldman and Y. Yakimovsky, "Decision theory and artificial
intelligence: I. A semantics-based region analyzer, "Artificial
Intelligence vol. 5, pp. 349-471, 1975.
[37] G. L. Fischer, Jr., D. K. Pollock, B. Radack, and M. E. Stevens,
Eds., Optical Character Recognition. Washington, DC: Spartan,
1962.
[38] M. A. Fischler and R. A. Elschlager, "The representation and
matching of pictorial structures," IEEE Trans. Comput., vol. C-22,
pp. 67-92, 1973.
[39] K. S. Fu, Sequential Methods in Pattern Recognition and Ma-
chine Learning. New York: Academic, 1968.
[40] K. S. Fu, Ed., "Special issue on feature extraction and selection
in pattern recognition," IEEE Trans. Comput., vol. C-20, pp.
965-1120, Sept. 1971.
[41] --, Syntactic Methods in Pattern Recognition. New York:
Academic, 1974.
[42] K. S. Fu and T. L. Booth, "Grammatical inference-Introduction
and survey," IEEE Trans. Syst., Man, Cybern., vol. SMC-5, Jan.
and July 1975.
[43] K. S. Fu, Ed., Digital Pattern Recognition, Communication and
Cybernetics, vol. 10. New York: Springer, 1976.
[44] --, "Pattern recognition in remote sensing of the earth's re-
sources," IEEE Trans. Geosci. Electron., vol. GE-14, Jan. 1976.
[45] K. S. Fu, Ed., Special issue on pattern recognition, Computer May
1976.
[46] K. S. Fu, Ed., Applications of Syntactic Pattern Recognition.
New York: Springer, 1976.
[47] K. Fukunaga, Introduction to Statistical Pattern Recognition.
New York: Academic, 1972.
[48] K. Fukunaga and L. D. Hostetler, "k-nearest-neighbor Bayes-risk
estimation," IEEE Trans. Inform. Theory, vol. IT-21, May
1975.
[49] L. W. Fung and K. S. Fu, "Stochastic syntactic decoding for pattern
classification," IEEE Trans. Comput., vol. C-24, June 1975.
[50] J. Gips, Shape Grammars and Their Uses. Birkhauiser: Verlag,
Basel and Stuttgart, 1975.
[51] A. Grasselli, Ed., Automatic Interpretation and Classification of
Images. New York: Academic, 1969.
[52] U. Grenander, "Foundations of pattern analysis," Quart. Appl.
Math., vol. 27, pp. 1-55, 1969.
[53] E. L. Hall and C. F. George, Jr., Eds., "Special issue on two-di-
mensional digital signal processing," IEEE Trans. Comput., vol.
C-21, pp. 633-820, July 1972.
[54] K. Hanakata, "Feature selection and extraction for decision the-
oretic approach and structural approach," in Proc. NATO Ad-
vanced Study Institute on Pattern Recognition-Theory and
Applications, Sept. 8-17, 1975.
[55] R. M. Haralick, "The pattern discrimination problem from the
perspective of relation theory," Pattern Recognition, vol. 7, June
1975.
[56] R. M. Haralick, K. Shanmugam, and I. Dinstein, "Textural features
for image classification," IEEE Trans. Syst., Man, Cybern., vol.
SMC-3, pp. 610-621,1973.
[57] L. D. Harmon, Ed., "Special issue on digital pattern recognition,"
Proc. IEEE, vol. 60, pp. 1117-1233, Oct. 1972.
[58] H. Hauska and P. H. Swain, "The decision tree classifier: Design
and potential," in Proc. 2nd Symp. Machine Processing of Re-
motely Sensed Data, June 3-5,1975.
[59] S. L. Horowitz and T. Pavlidis, "Picture segmentation by a directed
split-merge procedure," Proc. 2nd Int. Joint Conf. Pattern Rec-
ognition, 1974, pp. 424-433.
[60] T. S. Huang, Ed., Picture Processing and Digital Filtering. New
York: Springer, 1975.
[61] T. S. Huang and 0. J. Tretiak, Picture Bandwidth Compression.
New York: Gordon and Breach, 1972.
[62] R. A. Hummel, "Histogram modification techniques," Comput.
Graphics Image Processing, vol. 5, 1976.
[63] R. A. Hummel, S. W. Zucker, and A. Rosenfeld, "Scene labelling
by relaxation operations," IEEE Trans. Syst., Man, Cybern., to
be published.
[64] T. Imai and M. Shimura, "Learning with probabilistic labelling,"
Pattern Recognition, vol. 8, Jan. 1976.
[651 L. Kanal, Ed., Pattern Recognition. Washington, DC: Thompson,
1968.
[66] --, "Patterns in pattern recognition: 1968-1974, IEEE Trans.
1344
FU AND ROSENFELD: PATTERN AND IMAGE PROCESSING
Inform. Theory, vol. IT-20, Nov. 1974.
[67] S. Kaneff, Ed., Picture Language Machines. New York: Aca-
demic, 1971.
[68] A. Klinger and C. R. Dyer, "Experiments on picture representations
using regular decomposition," Comput. Graphics Image Pro-
cessing, vol. 5, pp. 68-105, 1976.
[69] P. A. Kolers and M. Eden, Eds., Recognizing Patterns. Cam-
bridge, MA: M.I.T. Press, 1968.
[70] W. L. G. Koontz, P. M. Narendra, and K. Fukunaga, "A branch
and bound clustering algorithm," IEEE Trans. Comput., vol. C-24,
Sept. 1975.
[71] V. A. Kovalevsky, Character Readers and Pattern Recognition.
Washington, DC: Spartan, 1968.
[72] A. V. Kulkarni and L. N. Kanal, "An optimization approach to the
design of decision trees," Dep. Comp. Sci., Univ. Maryland, College
Park, MD, Tech. Rep. TR-396, Aug. 1975.
[73] R. S. Ledley, Ed., Pattern Recognition. New York: Pergamon,
1968.
[74] B. S. Lipkin and A. Rosenfeld, Eds., Picture Processing and Psy-
chopictorics. New York: Academic, 1970.
[75] T. Lissack and K. S. Fu, "Error estimation in pattern recognition
via L'-distance between posterior density functions," IEEE Trans.
Inform. Theory, vol. IT-22, Jan. 1976.
[76] S. Y. Lu and K. S. Fu, "Efficient error-correcting syntax analysis
for recognition of noisy patterns," School Elec. Eng., Purdue Univ.,
West Lafayette, IN, Tech. Rep. TR-EE 76-9, Mar. 1976.
[77] Machine Perception of Patterns and Pictures, Conference Series
13, The Inst. Physics, London, England, 1972.
[78] B. H. McCormick and S. N. Jayarmamurthy, "Time series model
for texture synthesis," J. Comput. Inform. Sci., vol. 3, pp. 329-343,
1974.
"A decision theory model for the analysis of texture," J.
Comput. Inform. Sci., vol. 4, pp. 1-38,1975.
[79] W. Meisel, Computer-Oriented Approaches to Pattern Recogni-
tion. New York: Academic, 1972.
[80] J. M. Mendel and K. S. Fu, Ed., Adaptive, Learning and Pattern
Recognition Systems: Theory and Applications. New York:
Academic, 1970.
[81] R. S. Michalski, "A variable-valued logic system as applied to
picture description and recognition," in Graphic Languages, F.
Nake and A. Rosenfeld, Ed. Amsterdam, The Netherlands:
Holland, 1972.
[82] M. Minsky and S. Papert, Perceptrons. Cambridge, MA: M.I.T.
Press, 1969.
[83] B. Moayer and K. S. Fu, "A tree system approach for fingerprint
pattern recognition," IEEE Trans. Comput., voL. C-25, Mar.
1976.
[84] R. N. Nagel and A. Rosenfeld, "Ordered search techniques in
template matching," Proc. IEEE, vol. 60, pp. 242-244, 1972.
[85] R. Nevatia and T. 0. Binford, "Structured descriptions of complex
objects," in Proc. 3rd Int. Joint Conf. Artificial Intelligence, 1973,
pp. 641-647.
[86] N. J. Nilsson, Learning Machines-Foundations of Trainable
Pattern-Classifying Systems. New York: McGraw-Hill, 1965.
[87] R. B. Ohlander, "Analysis of natural scenes," Ph.D. dissertation,
dep. Comput. Sci., Carnegie-Mellon Univ., Pittsburgh, PA, Apr.
1975.
[88] P. A. Ota, "Mosaic grammers," Pattern Recognition, vol. 7, June
1975.
[89] E. A. Patrick, Fundamentals on Pattern Recognition. Englewood
Cliffs, NJ: Prentice-Hall, 1972.
[90] L. F. Pau, Diagnostic Des Pannes Dans Les Systemas. Cepa-
dues-Edition, 1975.
[91] T. Pavlidis, "Analysis of set patterns," Pattern Recognition, vol.
1, Nov. 1968.
[92] --, "Waveform segmentation through functional approxima-
tion," IEEE Trans. Comput., vol. C-22, July 1973.
[93] T. Pavlidis and F. Ali, "Computer recognition of handwritten
numerals by polygonal approximations," IEEE Trans. Syst., Man,
Cybern., vol. SMC-5, Nov. 1975.
[94] E. Persoon and K. S. Fu, "Sequential classification of strings
generated by SCFG's," Int. J. Comput. Inform. Sci., vol. 4,
Sept. 1975.
[95] J. M. S. Prewitt and M. L. Mendelsohn, "The analysis of cell im-
ages," Annals NY Academy of Sci., vol. 128, pp. 1035-1053, Jan.
1966.
[96] Proc. Int. Joint Conf. Pattern Recognition (First: Washington, DC,
Oct. 1973; Second: Copenhagen, Denmark, Aug. 1974; Third: Co-
ronado, CA, Nov. 1976).
[97] D. M. Ramsey, Ed., Image Processing in Biological Sciences.
Berkeley, CA: Univ. California Press, 1969.
[981 D. R. Reddy, Ed., Speech Recognition. New York: Academic,
1975.
[99] A. Rosenfeld, Picture Processing by Computer. New York: Aca-
demic, 1969.
[100] --, "Picture processing by computer," Comput. Surveys, vol.
1, pp. 147-176, 1969.
[101] "Progress in picture processing: 1969-71," Comput. Serveys,
vol. 5, pp. 81-108, 1973.
[102] --, "Picture processing: 1972," Comput. Graphics Image Pro-
cessing, vol. 1, pp. 394-416, 1972.
[103] --, "Picture processing: 1973," Comput. Graphics Image Pro-
cessing, vol. 3, pp. 178-194, 1974.
[104] --, "Picture processing: 1974," Comput. Graphics Image Pro-
cessing, vol. 4, pp. 133-155, 1975.
[105] --, "Picture processing: 1975," Comput. Graphics Image Pro-
cessing, vol. 5, 1976.
[106] A. Rosenfeld, Ed., Digital Picture Analysis. New York: Springer,
1976.
[1071 A. Rosenfeld, H. Freeman, T. S. Huang, and A. van Dam, Eds.,
Comput. Graphics Image Processing. New York: Academic,
1972.
[108] A. Rosenfeld and A. C. Kak, Digital Picture Processing. New
York: Academic, 1976.
[109] A. Rosenfeld and M. Thurston, "Edge and curve detection for vi-
sual scene analysis," IEEE Trans. Comput., vol. C-20, pp. 562-569,
1971.
[110] K. M. Sayre, Recognition: A Study in the Philosophy of Artificial
Intelligence. Notre Dame, IN: Univ. Notre Dame Press, 1965.
[111] G. S. Sebestyen, Decision Processes in Pattern Recognition. New
York: Macmillan, 1962.
[112] J. C. Simon, "Recent progress to a formal approach of pattern
recognition and scene analysis," in Proc. 2nd Int. Joint Conf.
Pattern Recognition, Aug. 13-15, 1974.
[113] J. Sklansky, Ed., Pattern Recognition: Introduction and Foun-
dations. Dowden, Hutchinson and Ross, 1973.
[114] G. Stockman, L. N. Kanal, and M. C. Kyle, "Structural pattern
recognition of waveforms using a general waveform parsing sys-
tem," Dep. Comput. Sci., Univ. Maryland, College Park, MD, Tech.
Rep. TR-390, July 1975.
[115] S. Tanimoto and T. Pavlidis, "A hierarchical data structure for
picture processing," Comput. Graphics Image Processing, vol. 4,
pp. 104-119, 1975.
[116] J. M. Tenenbaum and H. G. Barrow, "Experiments in interpre-
tation-guided segmentation," Standard Res. Inst., Menlo Park,
CA, Tech. Note 123, Mar. 1976.
[117] R. A. Thompson, "Language correction using probabilistic gram-
mars," IEEE Trans. Comput., vol. C-25, Mar. 1976.
[118] J. T. Tou and R. C. Gonzalez, Pattern Recognition Principles.
New York: Addison-Wesley, 1974.
[119] G. T. Toussaint, "Bibliography on estimation of misclassification,"
IEEE Trans. Inform. Theory, vol. IT-20, July 1974.
[120] --, "Recent progress in statistical methods applied to pattern
recognition," in Proc. 2nd Int. Joint Conf. Pattern Recognition,
Aug. 13-15, 1974.
[121] L. Uhr, Ed., Pattern Recognition. New York: Wiley, 1966.
[122] --, Pattern Recognition, Learning and Thought. Englewood
Cliffs, NJ: Prentice-Hall, 1973.
[123] J. R. Ullmann, Pattern Recognition Techniques. Crane, Russak,
& Co., 1973.
[124] G. J. VanderBrug and A. Rosenfeld, "Two-stage template
matching," IEEE Trans. Comput., to be published.
[125] C. J. D. M. Verhagen, "Some general remarks about pattern rec-
ognition; Its definition; Its relation with other disciplines; A lit-
erature survey," Pattern Recognition, vol. 7, Sept. 1975.
[126] T. J. Wagner, "Nonparametric estimates of probability densities,"
IEEE Trans. Inform. Theory, vol. IT-21, July 1975.
[127] S. Watanabe, Ed., Methodologies of Pattern Recognition. New
York: Academic, 1969.
[128] --, Frontiers of Pattern Recognition. New York: Academic,
1972.
[129] J. S. Weszka, C. R. Dyer, and A. Rosenfeld, "A comparative study
of texture measures for terrain classification," IEEE Trans. Syst.,
Man, Cybern., vol. SMC-6, pp. 269-285,1976.
[130] J. S. Weszka, R. N. Nagel, and A. Rosenfeld, "A threshold selection
technique," IEEE Trans. Comput., vol. C-23, pp. 1322-1326,
1974.
[131] K. L. Williams, "A multidimensional approach to syntactic pattern
1345
IEEE TRANSACTIONS ON COMPUTERS, VOL. C-25, NO. 12, DECEMBER 1976
recognition," Pattern Recognition, vol. 7, Sept. 1975.
[132] P. H. Winston, Ed., The Psychology of Computer Vision. New
York: McGraw-Hill, 1975.
[133] S. S. Yau and S. C. Chang, "A direct method for cluster analysis,"
Pattern Recognition, vol. 7, Dec. 1975.
[1341 K. C. You and K. S. Fu, "An approach to the design of a linear bi-
nary tree classifier," in Proc. 3rd Symp. Machine Processing of
Remotely Sensed Data, June 29-July 1, 1976.
[135] 1. T. Young, "The prediction of performance in multi-class pattern
classification," in Proc. 2nd Int. Joint Conf. Pattern Recognition,
Aug. 13-15, 1974.
[136] T. Y. Young and T. W. Calvert, Classification, Estimation, and
Pattern Recognition. New York: Elsevier, 1973.
[137] N. G. Zagoruyko, Recognition Methods and Their Applications,
Radio Sovetskoe. Moscow, 1972.
[1381 N. V. Zavalishin and I. B. Muchnik, Models of Visual Perception
and Algorithms for Image Analysis. Moscow:Nauka, 1974.
[139] S. W. Zucker, "On the structure of texture," Comput. Graphics
Image Processing, vol. 5, 1976.
[1401 S. W. Zucker, A Rosenfeld, and L. S. Davis, "Picture segmentation
by texture discrimination," IEEE Trans. Comput., vol. C-24, pp.
1228-1233, 1975.
King-Sun Fu (S'56-M'59-SM'69-F'71) re-
ceived the B.S. degree from the National Tai-
wan University, Taiwan, China, in 1953, the
M.A.Sc. degree from the University of Toronto,
Ont., Canada, in 1955, and the Ph.D. degree in
electrical engineering from the University of Il-
linois, Urbana, in 1959.
He is presently Goss Distinguished Professor
of Engineering and Professor of Electrical En-
gineering, Purdue University, Lafayette, IN.
Dr. Fu is a member of the National Acad-
emy of Engineering and a Guggenheim Fellow. He is also an Associate
Editor of the IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBER-
NETICS, Pattern Recognition, the Journal of Cybernetics, and Infor-
mation Sciences, and a member of the editorial board of the Interna-
tional Journal of Computer and Information Sciences. He was the
Chairman of the First International Joint Conference on Pattern Rec-
ognition, and is presently the Chairman of the Standing Conference
Committee of the International Joint Conference on Pattern Recognition.
He is the author ofthe books Sequential Methods in Pattern Recognition
and Machine Learning (New York: Academic, 1968) and Syntactic
Methods in Pattern Recognition (New York: Academic, 1974). He is
currently serving as the Chairman of the Machine Intelligence and Pat-
tern Analysis Technical Committee of the IEEE Computer Society.
Azriel Rosenfeld (M'60-F'72) was born in
New York, NY, on February 19, 1931. He re-
ceived the B.A. degree in physics from Yeshiva
University, New York, NY and the Ph.D. de-
gree in mathematics from Columbia Universi-
ty, New York, NY, in 1950 and 1957, respec-
tively.
From 1954 to 1956, he was a Physicist with
the Fairchild Controls Corporation, New York,
NY; from 1956 to 1959, he was an Engineer
with the Ford Instrument Company, Long
Island City, NY; and from 1959 to 1964, he was Manager of Research
with the Budd Company Electronics Division, Long Island City, NY, and
Information Sciences Center, McLean, VA. Since 1964, he has been a
Professor of Computer Science at the University of Maryland, College
Park. He is the author ofAn Introduction to Algebraic Structures and
Picture Processing by Computer, co-editor of Pictorial Pattern Recog-
nition, Picture Processing and Psychopictorics, and Graphic Languages,
editor of the journal Computer Graphics and Image Processing, and has
published nearly a hundred papers on a variety of subjects.
Artificial Intelligence: Cooperative Computation and
Man- Machine Symbiosis
MICHAEL A. ARBIB
Abstract-Artificial intelligence (Al) joins forces with brain
research, cognitive psychology, and linguistics to probe the nature
of intelligeice. It is asserted that the current Al emphasis on rep-
resentation of knowledge will be augmented by increasing attention
to cooperative computation and learning. Al also contributes to a
man-machine symbiosis: providing specialized packets of intelli-
gence to augment our own. As we augment our intelligence with
our machines, building on current success with robotics, scene
Manuscript received June 15, 1976; revised July 19, 1976. This work
was supported in part by the National Institutes of Health under Grant
5 R01 NS09755-07 COM.
The author is on sabbatical at the School of Epistemics, University of
Edinburgh, Edinburgh, Scotland. He is with the Department of Com-
puter and Information Science and the Center for Systems Neuroscience,
University of Massachusetts, Amherst, MA 01002.
analysis, language understanding, and expert systems, we must
better understand our own intelligence to enable the construction
of high-bandwidth man-machine interfaces. This, too, is a form of
cooperative computation.
Index Terms-Artificial intelligence, brain theory, cognitive
psychology, cooperative computation, data-base management,
expert systems, man-machine symbiosis, personal computers,
representation of knowledge, robots.
I. INTRODUCTION
ATYPICAL definition of artificial intelligence (AI) is
as a field of computer science whose goal is "to
1346

