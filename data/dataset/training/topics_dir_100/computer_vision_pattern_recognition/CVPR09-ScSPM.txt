Linear Spatial Pyramid Matching Using Sparse Coding
for Image Classification
Jianchao Yang†, Kai Yu‡, Yihong Gong‡, Thomas Huang†
†Beckman Institute, University of Illinois at Urbana-Champaign
‡NEC Laboratories America, Cupertino, CA 95014, USA
†{jyang29, huang}@ifp.uiuc.edu, ‡{kyu, ygong}@sv.nec-lab.com
Abstract
Recently SVMs using spatial pyramid matching (SPM)
kernel have been highly successful in image classification.
Despite its popularity, these nonlinear SVMs have a com-
plexity O(n2 ∼ n3) in training and O(n) in testing, where
n is the training size, implying that it is nontrivial to scale-
up the algorithms to handle more than thousands of training
images. In this paper we develop an extension of the SPM
method, by generalizing vector quantization to sparse cod-
ing followed by multi-scale spatial max pooling, and pro-
pose a linear SPM kernel based on SIFT sparse codes. This
new approach remarkably reduces the complexity of SVMs
to O(n) in training and a constant in testing. In a num-
ber of image categorization experiments, we find that, in
terms of classification accuracy, the suggested linear SPM
based on sparse coding of SIFT descriptors always signif-
icantly outperforms the linear SPM kernel on histograms,
and is even better than the nonlinear SPM kernels, leading
to state-of-the-art performance on several benchmarks by
using a single type of descriptors.
1. Introduction
In recent years the bag-of-features (BoF) model has been
extremely popular in image categorization. The method
treats an image as a collection of unordered appearance
descriptors extracted from local patches, quantizes them
into discrete “visual words”, and then computes a compact
histogram representation for semantic image classification,
e.g. object recognition or scene categorization.
The BoF approach discards the spatial order of local de-
scriptors, which severely limits the descriptive power of the
image representation. By overcoming this problem, one
particular extension of the BoF model, called spatial pyra-
mid matching (SPM) [12], has made a remarkable success
on a range of image classification benchmarks like Caltech-
101 [14] and Caltech-256 [8], and was the major compo-
Figure 1. Schematic comparison of the original nonlinear SPM
with our proposed linear SPM based on sparse coding (ScSPM).
The underlying spatial pooling function for nonlinear SPM is av-
eraging, while the spatial pooling function in ScSPM is max pool-
ing.
nent of the state-of-the-art systems, e.g., [2]. The method
partitions an image into 2l × 2l segments in different scales
l = 0, 1, 2, computes the BoF histogram within each of the
21 segments, and finally concatenates all the histograms to
form a vector representation of the image. In case where
only the scale l = 0 is used, SPM reduces to BoF.
People have empirically found that, in order to obtain
good performances, both BoF and SPM must be applied to-
gether with a particular type of nonlinear Mercer kernels,
e.g. the intersection kernel or the Chi-square kernel. Ac-
cordingly, the nonlinear SVM has to pay a computational
complexity O(n3) and a memory complexity O(n2) in the
training phase, where n is the training size. Furthermore,
since the number of support vectors grows linearly with n,
the computational complexity in testing is O(n). This scal-
ability implies a severe limitation — it is nontrivial to apply
them to real-world applications, whose training size is typi-
cally far beyond thousands.
1
In this paper, we propose an extension of the SPM ap-
proach, which computes a spatial-pyramid image represen-
tation based on sparse codes (SC) of SIFT features, instead
of the K-means vector quantization (VQ) in the traditional
SPM. The approach is naturally derived by relaxing the re-
strictive cardinality constraint of VQ. Furthermore, unlike
the original SPM that performs spatial pooling by comput-
ing histograms, our approach, called ScSPM, uses max spa-
tial pooling that is more robust to local spatial translations
and more biological plausible [24]. The new image repre-
sentation captures more salient properties of visual patterns,
and turns out to work surprisingly well with linear classi-
fiers. Our approach using simple linear SVMs dramatically
reduces the training complexity to O(n), and obtains a con-
stant complexity in testing, while still achieving an even
better classification accuracy in comparison with the tra-
ditional nonlinear SPM approach. Schematic comparison
between the original SPM with ScSPM is shown in Fig. 1.
The rest of the paper is organized as follows. In Sec. 2
we will talk about some related works. Sec. 3 presents the
framework of our proposed algorithm and we give our ef-
ficient implementation in Sec. 4, followed by experiment
results in Sec. 5. Finally, Sec. 6 concludes our paper.
2. Related Work
Over the years many works have been done to im-
prove the traditional BoF model, such as generative meth-
ods in [7, 21, 3, 1] for modeling the co-occurrence of the
codewords or descriptors, discriminative codebook learning
in [10, 5, 19, 27] instead of standard unsupervised K-means
clustering, and spatial pyramid matching kernel (SPM) [12]
for modeling the spatial layout of the local features, all
bringing promising progress. Among these extensions, mo-
tivated by Grauman and Darrell’s pyramid matching in the
feature space, the SPM proposed by Lazebnik et al. is par-
ticular successful.
As being easy and simple to construct, the SPM ker-
nel turns out to be highly effective in practice. It con-
tributes as the major component to the state-of-the-art sys-
tems, e.g., [2], and the systems of the top performers in
PASCAL Challenge 2008 [6]. Despite of such a popular-
ity, SPM has to run together with nonlinear kernels, such
as the intersection kernel and the Chi-square kernel, in or-
der to achieve a good performance, which requires inten-
sive computation and a large storage. Realizing this, Anna
Bosch et al. [2] used randomized trees instead of SVMs for
faster training and testing. Most recently, Maji et al. [16]
showed that one can build histogram intersection kernel
SVMs much efficiently. However, the efficiency comes
only for pre-trained nonlinear SVMs. In real applications
which involves more than tens of thousands of training ex-
amples, linear kernel SVMs are far more favored as they
enjoy both much faster training and testing speeds, with sig-
nificantly less memory requirements compared to nonlinear
kernels. Therefore, our proposed linear SPM using SIFT
sparse codes is very promising in real applications.
Sparse modeling of image patches has been successfully
applied to tasks such as image and video denoising, in-
painting, demosaicing, super-resolution[5, 17, 26] and seg-
mentation [18]. There are already some works devoting
to image categorization through sparse coding on raw im-
age patches [23, 22]. However, their performances are still
behind the state-of-the-art achieved by [12, 1, 9] on pub-
lic benchmarks. Our approach differs from them at using
sparse coding on appearance descriptors like SIFT features,
and the development of the whole system that achieves
state-of-the-art performances on several benchmarks.
3. Linear SPM Using SIFT Sparse Codes
3.1. Encoding SIFT: From VQ to SC
Let X be a set of SIFT appearance descriptors in a D-
dimensional feature space, i.e. X = [x1, . . . ,xM ]> ∈
RM×D. The vector quantization (VQ) method applies the
K-means clustering algorithm to solve the following prob-
lem
min
V
M∑
m=1
min
k=1...K
‖xm − vk‖2 (1)
where V = [v1, . . . ,vK ]> are the K cluster centers to be
found, called codebook, and ‖ · ‖ denotes the L2-norm of
vectors. The optimization problem can be re-formulated
into a matrix factorization problem with cluster member-
ship indicators U = [u1, . . . ,uM ]>,
min
U,V
M∑
m=1
‖xm − umV‖2 (2)
subject to Card(um) = 1, |um| = 1,um  0,∀m
where Card(um) = 1 is a cardinality constraint, meaning
that only one element of um is nonzero, um  0 means
that all the elements of um are nonnegative, and |um| is
the L1-norm of um, the summation of the absolute value
of each element in um. After the optimization, the index
of the only nonzero element in um indicates which cluster
the vector xm belongs to. In the training phase of VQ, the
optimization Eq. (2) is solved with respect to both U and
V. In the coding phase, the learned V will be applied for a
new set of X and Eq. (2) will be solved with respect to U
only.
The constraint Card(um) = 1 may be too restrictive,
giving rise to often a coarse reconstruction of X. We can
relax the constraint by instead putting a L1-norm regular-
ization on um, which enforces um to have a small number
of nonzero elements. Then the VQ formulation is turned
into another problem known as sparse coding (SC):
min
U,V
M∑
m=1
‖xm − umV‖2 + λ|um| (3)
subject to ‖vk‖ ≤ 1, ∀k = 1, 2, . . . ,K
where a unit L2-norm constraint on vk is typically applied
to avoid trivial solutions 1. Normally, the codebook V is an
overcomplete basis set, i.e. K > D. Note that we drop out
the nonnegativity constraint um  0 as well, because the
sign of um is not essential — it can be easily absorbed by
letting V> ← [V>,−V>] and u>m ← [u>m+,−u>m−] so
that the constraint can be trivially satisfied, where um+ =
min(0,um) and um− = max(0,um).
Similar to VQ, SC has a training phase and a coding
phase. First, a descriptor set X from a random collection
of image patches is used to solve Eq. (3) with respect to U
and V, where V is retained as the codebook; In the coding
phase, for each image represented as a descriptor set X, the
SC codes are obtained by optimizing Eq. (3) with respect to
U only.
We choose SC to derive image representations because it
has a number of attractive properties. First, compared with
the VQ coding, SC coding can achieve a much lower recon-
struction error due to the less restrictive constraint; Second,
sparsity allows the representation to be specialize, and to
capture salient properties of images; Third, research in im-
age statistics clearly reveals that image patches are sparse
signals.
3.2. Linear SPM
For any image represented by a set of descriptors, we can
compute a single feature vector based on some statistics of
the descriptors’ codes. For example, if U is obtained via
Eq. (2), a popular choice is to compute the histogram
z =
1
M
M∑
m=1
um (4)
The bag-of-words approach to image classification com-
putes such a histogram z for each image I represented by an
unordered set of local descriptors. In the more sophisticated
SPM approach, the image’s spatial pyramid histogram rep-
resentation z is a concatenation of local histograms in vari-
ous partitions of different scales. After normalization z can
be seen as again a histogram. Let zi denote the histogram
representation for image Ii. For a binary image classifica-
tion problem, an SVM aims to learn a decision function
f(z) =
n∑
i=1
αiκ(z, zi) + b (5)
1For example, the objective can be decreased by respectively dividing
and multiplying um and V by a constant factor.
Figure 2. The illustration architecture of our algorithm based on
sparse coding. Sparse coding measures the responses of each local
descriptor to the dictionary’s ”visual elements”. These responses
are pooled across different spatial locations over different spatial
scales.
where {(zi, yi)}ni=1 is the training set, and yi ∈ {−1,+1}
indicates labels. For a test image represented by z, if
f(z) > 0 then the image is classified as positive, otherwise
as negative. In theory κ(·, ·) can be any reasonable Mer-
cer kernel function, but in practice the intersection kernel
and Chi-square kernel have been found the most suitable on
histogram representations. Our experiment shows that lin-
ear kernel on histograms leads to always substantially worse
results, partially due to the high quantization error of VQ.
However, using these two nonlinear kernels, the SVM has
to pay a high training cost, i.e. O(n3) in computation, and
O(n2) in storage (for the n× n kernel matrix). This means
that it is difficult to scale up the algorithm to the case where
n is more than tens of thousands. Furthermore, as the num-
ber of support vectors scales linearly to the training size, the
testing cost is O(n).
In this paper we advocate an approach of using linear
SVMs based SC of SIFT. Let U be the result of applying
the sparse coding Eq. (3) to a descriptor set X, assuming
the codebook V to be pre-learned and fixed, we compute the
following image feature by a pre-chosen pooling function
z = F(U), (6)
where the pooling function F is defined on each column of
U. Recall that each column of U corresponds to the re-
sponses of all the local descriptors to one specific item in
dictionary V. Therefore, different pooling functions con-
struct different image statistics. For example, in 4, the un-
derlying pooling function is defined as the averaging func-
tion, yielding the histogram feature. In this work, we de-
fined the pooling function F as a max pooling function on
the absolute sparse codes
zj = max{|u1j |, |u2j |, ..., |uMj |}, (7)
where zj is the j-th element of z, uij is the matrix element
at i-th row and j-th column of U, and M is the number of
local descriptors in the region. This max pooling proce-
dure is well established by biophysical evidence in visual
cortex (V1) [24] and is empirically justified by many algo-
rithms applied to image categorization. In our case, we also
find that max pooling outperforms other alternative pooling
methods (see Sec. 5.5.4).
Similar to the construction of histograms in SPM, we do
max pooling Eq. (7) on a spatial pyramid constructed for
an image. By max pooling across different locations and
over different spatial scales of the image, the pooled feature
is more robust to local transformations than mean statistics
in histogram. Fig. 2 illustrates the whole structure of our
algorithm based on sparse coding. The pooled features from
various locations and scales are then concatenated to form
a spatial pyramid representation of the image.
Let image Ii be represented by zi, we use a simple linear
SPM kernel
κ(zi, zj) = z>i zj =
2∑
l=0
2l∑
s=1
2l∑
t=1
〈zli(s, t), zlj(s, t)〉 (8)
where 〈zi, zj〉 = z>i zj , and zli(s, t) is the max pooling
statistics of the descriptor sparse codes in the (s, t)-th seg-
ment of image Ii in the scale level l. Then the binary SVM
decision function becomes
f(z) =
(
n∑
i=1
αizi
)>
z + b = w>z + b (9)
In the literature, Eq. (5) is called the dual formulation of
SVMs, while Eq. (9) is the primal formulation. As the ma-
jor advantage of the linear kernel, now we can directly work
in the primal, which means that the training cost is O(n) in
computation, and the testing cost for each image is even
constant! In Sec. 4.2, we will describe our large-scale im-
plementation for binary and multi-class linear SVMs.
Despite that the linear SPM kernel based on histograms
leads to very poor performances, we find that the lin-
ear SPM kernel based on sparse coding statistics always
achieves excellent classification accuracy. This success is
largely due to three factors: (1) SC has much less quantiza-
tion errors than VQ; (2) It is well known that image patches
are sparse in nature, and thus sparse coding is particularly
suitable for image data; (3) The computed statistics by max
pooling are more salient and robust to local translations.
4. Implementation
4.1. Sparse Coding
The optimization problem Eq. (3) is convex in V (with
U fixed) and convex in U (with V fixed), but not in both
simultaneously. The conventional way for such a problem is
to solve it iteratively by alternatingly optimizing over V or
U while fixing the other. Fixing V, the optimization can be
solved by optimizing over each coefficient um individually:
min
um
‖xm − umV‖22 + λ|um|. (10)
This is essentially a linear regression problem with L1 norm
regularization on the coefficients, well known as Lasso in
the Statistical literature. The optimization can be solved
very efficiently by algorithms such as the recently proposed
feature-sign search algorithm. [13]. Fixing U, the problem
reduces to a least square problem with quadratic constraints:
min
V
‖X−UV‖2F
s.t. ‖vk‖ ≤ 1, ∀k = 1, 2, ...,K.
(11)
The optimization can be done efficiently by the Lagrange
dual as used in [13].
In our experiments, we use 50, 000 SIFT descriptors ex-
tracted from random patches to train the codebook, by iter-
ating the steps Eq. (10) and Eq. (11). Once we get the code-
book V in this off-line training, we can do on-line sparse
coding efficiently as in Eq. (10) on each descriptor of an
image.
4.2. Multi-class Linear SVM
We introduce a simple implementation of linear SVMs
that was used in our experiments. Given the training data
{(zi, yi)}ni=1, yi ∈ Y = {1, . . . , L}, a linear SVM aims to
learn L linear functions {w>c z|c ∈ Y}, such that, for a test
datum z, its class label is predicted by2
y = max
c∈Y
w>c z (12)
We take a one-against-all strategy to train L binary linear
SVMs, each solving the following unconstraint convex op-
timization problem
min
wc
{
J(wc) = ‖wc‖2 + C
n∑
i=1
` (wc; yci , zi)
}
(13)
where yci = 1 if yi = c, otherwise y
c
i = −1, and
` (wc; yci , zi) is a hinge loss function. The standard hinge
loss function is not differentiable everywhere, which ham-
pers the use of gradient-based optimization methods. Here
we adopt a differentiable quadratic hinge loss,
` (wc; yci , zi) =
[
max
(
0,w>c z · yci − 1
)]2
such that the training can be easily done with simple
gradient-based optimization methods. In our work we used
2The more general form of linear functions, i.e. f(z) = w>z + b,
can still be written as f(z) = w>z by adopting the reparameterization
w> ← [w>, b] and z> ← [z>, 1].
LBFGS. Other choices like conjugate gradient are also ap-
plicable. The only implementation on our side is providing
the cost J(w) and the gradient ∂J(w)/∂w. The computa-
tion linearly scans over the training examples and thus has
the linear complexity O(n). In our experiment in Sec. 5.4,
the SVM training on about 200, 000 examples with 5376-
dimensional features was usually finished in 5 minutes.
5. Experiments and Results
In the experiments, we implemented and evaluated three
classes of SPM methods on four diverse datasets: Caltech
101 [14], Caltech 256 [8], 15 Scenes [12], and TRECVID
2008 surveillance video. The three methods are
1. KSPM: the popular nonlinear kernel SPM that uses
spatial-pyramid histograms and Chi-square kernels;
2. LSPM: the simple linear SPM that uses linear kernel
on spatial-pyramid histograms;
3. ScSPM: the linear SPM that uses linear kernel on
spatial-pyramid pooling of SIFT sparse codes,
Besides our own implementations, we also quote some re-
sults directly from the literature, especially those of KSPM
from [12] and [8]. We note that sometimes we could not re-
produce their results, largely due to subtle engineering de-
tails, e.g. the way of dealing with high-contrast and low-
contrast patches. It thus makes more sense to compare our
own implementations, since they were based on exactly the
same set of descriptors.
Our implementations used a single descriptor type, the
popular SIFT descriptor,3 as in [12, 1, 9]. The SIFT de-
scriptors extracted from 16× 16 pixel patches were densely
sampled from each image on a grid with stepsize 8 pixels.
The images were all preprocessed into gray scale. To train
the codebooks, we used standard K-means clustering for
KSPM and LSPM, and the sparse coding scheme for our
proposed ScSPM algorithm. For all the experiments ex-
cept TRECVID 2008, we fixed the codebook size as 512
for LSPM and 1024 for ScSPM, to achieve optimal perfor-
mances for both. For training the linear classifiers, we used
our implemented SVM described in 4.2. The KSPM was
trained using the LIBSVM [4] package.
Following the common benchmarking procedures, we
repeat the experimental process by 10 times with different
random selected training and testing images to obtain reli-
able results. The average of per-class recognition rates were
recorded for each run. And we report our final results by the
mean and standard deviation of the recognition rates.
3It is straightforward that the approach can be generalized to handle
other descriptors and also multiple descriptors.
5.1. Caltech-101 Dataset
The Caltech-101 dataset contains 101 classes (including
animals, vehicles, flowers, etc.) with high shape variabil-
ity. The number of images per category varies from 31
to 800. Most images are medium resolution , i.e. about
300 × 300 pixels. We followed the common experiment
setup for Caltech-101, training on 15 and 30 images per cat-
egory and testing on the rest. Detailed comparison results
are shown in Table 1. As shown, our sparse coding scheme
outperforms linear SPM by more than 14 percent, and even
outperform the nonlinear SPM [12] by a large margin (about
11 percent for 15 training and 9 percent for 30 training per
category). One work needs to mention is the Kernel Code-
books [25], where the author assigned each descriptor into
multiple bins instead of hard assignment. This scheme gen-
erally improves their baseline SPM by 5 ∼ 6 percent 4.
However, their method is still based on nonlinear kernels.
Table 1. Classification rate (%) comparison on Caltech-101.
Algorithms 15 training 30 training
Zhang et al. [28] 59.10± 0.60 66.20± 0.50
KSPM [12] 56.40 64.40± 0.80
NBNN [1] 65.00± 1.14 70.40
ML+CORR [9] 61.00 69.60
KC [25] – 64.14± 1.18
KSPM 56.44± 0.78 63.99± 0.88
LSPM 53.23± 0.65 58.81± 1.51
ScSPM 67.0± 0.45 73.2± 0.54
5.2. Caltech-256 Dataset
The Caltech-256 dataset holds 29,780 images falling into
256 categories with much higher intra-class variability and
higher object location variability compared with Caltech-
101. Each category contains at least 80 images. We tried
our algorithm on 15, 30, 45, and 60 training images per
class respectively. The results are shown in Table 2. For all
the cases, our ScSPM outperforms LSPM by more than 15
percent, and outperforms our own KSPM by more than 4
percent. In the cases of 45 and 60 training images per cate-
gory, KSPM was not tried due to its very high computation
cost for training.
5.3. 15 Scenes Categorization
We also tried our algorithm on the 15-Scenes dataset
compiled by several researchers [20, 7, 12]. This dataset
contains totally 4485 images falling into 15 categories, with
the number of images each category ranging from 200 to
400. The 15 categories vary from living room and kitchen
4Because the codebook baseline scores are lower, the improved abso-
lute performance obtained by the kernel codebook is not as high as may be
obtained with a better baseline
Table 2. Classification rate (%) comparison on Caltech-256 dataset.
Algorithms 15 train 30 train 45 train 60 train
KSPM [8] – 34.10 – –
KC [25] – 27.17± 0.46 – –
KSPM 23.34± 0.42 29.51± 0.52 – –
LSPM 13.20± 0.62 15.45± 0.37 16.37± 0.47 16.57± 1.01
ScSPM 27.73± 0.51 34.02± 0.35 37.46± 0.55 40.14± 0.91
to street and industrial. Following the same experiment pro-
cedure of Lazebnik et al. [12], we took 100 images per class
for training and used the left for testing. The detailed com-
parison results are shown in Table 3. In this experiment,
our implementation of kernel SPM was not able to repro-
duce the results reported in [12], probably due to the SIFT
descriptor extraction and normalization process. Follow-
ing our own baseline, the Linear ScSPM algorithm again
achieves much better performance than KSPM and KC [25].
Table 3. Classification rate (%) comparison on 15 scenes.
Algorithms Classification Rate
KSPM [12] 81.40± 0.50
KC [25] 76.67± 0.39
KSPM 76.73± 0.65
LSPM 65.32± 1.02
ScSPM 80.28± 0.93
5.4. TRECVID 2008 Surveillance Video
Figure 3. Examples of Events in TRECVID Surveillance Video
This time, we tried our algorithm on the large-scale data
of 2008 TRECVID Surveillance Event Detection Evalua-
tion, sponsored by National Institute of Standard and Tech-
nology (NIST). The data are 100 hours of surveillance
videos, 10 hours each day, from London Gatwick Interna-
tional Airport. NIST defined 10 classes of events to detect,
and provided 50 hours of annotated videos for training, as
well as the other 50 hours videos for testing. The proposed
algorithm of this paper was one of the main components in a
system participating in 3 tasks of the evaluation, i.e. detect-
ing CellToEar, ObjectPut, and Pointing, and being among
the top performers. Some sample frames of these events are
shown in Fig. 3. In addition to the event duration annotated
by NIST, we manually marked the locations of persons per-
forming the 3 events of interests.
The tasks are extremely challenging in two aspects:
(1) The people subjects have a huge degree of variances
in viewpoints and appearances, and are always in highly
crowed and cluttered environments; (2) The detection sys-
tem has to process 9 millions of 720 × 576 frames – the
computation load is far beyond most of the research efforts
known from the literature. To make the computation af-
fordable, our system took a simple frame-based approach:
first used a human detector to detect people subjects on each
frame, and then applied classifiers on each detected region
to further detect the events of interest. For each of the 3
events, we trained a binary classifier.
Table 4. AUC comparison on TRECVID 2008 surveillance video.
Algorithms CellToEar ObjectPut Pointing
LSPM 0.688 0.714 0.744
ScSPM 0.744 0.773 0.769
Since the training videos were recorded in 5 different
days, we used 5-fold cross validation to develop and evalu-
ate our methods, where each fold corresponded to one day.
In total, we got 2114, 2172, and 8725 positive examples of
CellToEar, ObjectPut, and Pointing, respectively, and about
200,000 negative examples (only a small subset!) in the
training set. Each example was a cropped image containing
a detected human subject with the annotated event, resized
into a 100 × 100 image. For each example, we extracted
SIFT descriptors for every 16 × 16 patches on a grid of
stepsize 8. The codebook sizes of both VQ and SC were set
to be 256. Nonlinear SVM does not work on such a large-
scale training set, therefore we only compared the two linear
methods, ScSPM and LSPM. Due to the extremely unbal-
anced class distribution, we used ROC curves, as well as
the AUC (area under ROC curve) scores to evaluate the ac-
curacy. The average AUC results over 5 folds are shown
in Table 4. Typically, the SVM training on about 200, 000
examples with 5376-dimensional features was usually fin-
ished in 5 minutes.
5.5. Experiment Revisit
5.5.1 Patch Size
In our experiments, we only used one patch size to to extract
SIFT descriptors, namely, 16×16 pixels as in SPM [12]. In
NBNN[1], they used four patch scales to extract the descrip-
tors in order to boost their performance. In our experiments,
we didn’t observe any substantial improvements by pooling
over multiple patch scales, probably because max pooling
over sparse codes can capture the salient properties of local
regions that are irrelevant to the scale of local patches.
5.5.2 Codebook Size
We also investigated the effects of codebook sizes on these
SPM algorithms. Intuitively, if the codebook size is too
small, the histogram feature looses discriminant power; if
the codebook size is too large, the histograms from the same
class of images will never match. In Lazebnik et al.’s work,
they used two codebook sizes 200 and 400 and reported that
there was little difference. In our experiments on ScSPM
and LSPM, we tried three sizes: 256, 512 and 1024. As
shown in Table 5, the performance for LSPM increases ini-
tially and then decreases as the codebook size grows further.
The performance for ScSPM continues to increase when the
codebook size goes up to 1024.
Table 5. The effects of codebook size on ScSPM and LSPM re-
spectively on Caltech 101 dataset.
Codebook size 256 512 1024
30 train ScSPM 68.26 71.20 73.20
LSPM 57.42 58.81 58.56
15 train ScSPM 61.97 63.23 69.70
LSPM 51.84 53.23 51.74
5.5.3 Sparse Coding Parameter
There is one free parameter λ as in Eq. (10) we need to
determine when we do sparse coding on each feature vec-
tor. λ enforces the sparsity of the solution; the bigger λ is,
more sparse the solution will be. Empirically, we found that
keeping the sparsity to be around 10% yields good results.
For all our experiments, we simply fixed λ to be 0.3 ∼ 0.4
and the mean number of supports (non-zero coefficients) is
around 10.
5.5.4 Comparison of Pooling Methods
We also studied two other straightforward pooling methods,
namely, the square root of mean squared statistics (Sqrt) and
the mean of absolute values (Abs), in comparison with max
pooling. To be more precise, the other two pooling methods
are defined as
Sqrt : zj =
√√√√ 1
M
M∑
i=1
u2ij
Abs : zj =
1
M
M∑
i=1
|uij |,
(14)
where the meanings of the notations are the same as in Eqn.
7. Experiments using three pooling methods on Caltech-
101 for 30 training per categories and 15 Scenes for 100
training are listed in Table 6. As shown, max pooling pro-
duces the best performance, probably due to its robustness
to local spatial variations.
Table 6. The performance comparison using different pooling
methods on Caltech-101 and 15 Scenes for ScSPM.
Sqrt Abs Max
Caltech 71.09± 1.47 66.68± 0.66 73.2± 0.54
Scenes 76.20± 0.77 73.92± 1.03 80.4± 0.45
5.5.5 Linear Kernel vs. Nonlinear Kernels
To justify the use of linear classifiers in our approach, we
tried the popular intersection kernel and Chi-square ker-
nel on our sparse coding features for comparison. We
conducted the experiments on Caltech-101 (with 15 train-
ing examples) and 15 Scenes, and the results are shown
in Table 7. As shown, our ScSPM based on linear kernel
achieves a much better performance on both Caltech-101
and 15 Scenes compared to the nonlinear counterparts, not
to mention that the nonlinear methods require much more
computation. The compatibility of linear models with SIFT
sparse codes is a very interesting phenomenon. One intu-
itive explanation is that, patterns with sparse features are
more linearly separable, which is indeed the case for text
classification.
Table 7. The performance comparison between linear and nonlin-
ear kernels on ScSPM.
Dataset Linear Chi-Square Intersection
Caltech 67.0± 0.45 60.7± 0.11 60.4± 0.98
Scene 80.4± 0.45 77.3± 0.75 77.7± 0.66
6. Conclusion and Future Work
In this paper we proposed a spatial pyramid matching
approach based on SIFT sparse codes for image classifica-
tion. The method uses selective sparse coding instead of
traditional vector quantization to extract salient properties
of appearance descriptors of local image patches. Further
more, instead of averaging pooling in the histogram, sparse
coding enables us to operate local max pooling on multiple
spatial scales to incorporate translation and scale invariance.
The most encouraging result of this paper is, the obtained
image representation works surprisingly well with simple
linear SVMs, which dramatically improves the scalability
of training and the speed of testing, and even improves the
classification accuracy. Our experiments on a variety of im-
age classification tasks demonstrated the effectiveness of
this approach. Since the nonlinear SPM based on vector
quantization is very popular in top-performing image clas-
sification systems, we believe the suggested linear SPM will
greatly improve state-of-the-art by allowing to use much
larger sets of training data.
As an indication from our work, the sparse codes of SIFT
features might serve as a better local appearance descrip-
tor for general image processing tasks. Further research of
this in empirical study and theoretical understanding is an
interesting direction. Another issue is the efficiency of en-
coding. Currently encoding the SIFT descriptors of each
Caltech image takes about 1 second in average. A recent
work shows that sparse coding can be dramatically accel-
erated by using a feed-forward network [11]. It will be in-
teresting to try such methods to make our approach faster.
Moreover, the accuracy could be further improved by learn-
ing the codebook in a supervised fashion, as suggested by
another recent work [15].
References
[1] O. Boiman, E. Shechtman, and M. Irani. In defense of
nearest-neighbor based image classification. In CVPR, 2008.
2, 5, 7
[2] A. Bosch, A. Zisserman, and X. Munoz. Image classification
using random forests and ferns. In ICCV, 2007. 1, 2
[3] A. Bosch, A. Zisserman, and X. Munoz. Scene classification
using a hybrid generative/dicriminative approach. TPAMI,
2008. 2
[4] C.-C. Chang and C.-J. Lin. LIBSVM: a library for sup-
port vector machines, 2001. Software available at http:
//www.csie.ntu.edu.tw/˜cjlin/libsvm. 5
[5] M. Elad and M. Aharon. Image denoising via sparse and
redundant representations over learned dictionaries. IEEE
Transaction on Image Processing, 2006. 2
[6] M. Everingham, L. V. Gool, C. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes challenge
2008 (voc2008). In ECCV Workshop, 2008. 2
[7] L. Fei-Fei and P. Perona. A bayesian hierarchical model for
learning natural scene categories. In CVPR, 2005. 2, 5
[8] Griffin, G. Holub, and P. AD. Perona. Caltech-256 object
category dataset. Technical Report 7694, California Institute
of Technology, 2007. 1, 5, 6
[9] P. Jain, B. Kullis, and K. Grauman. Fast image search for
learned metrics. In CVPR, 2008. 2, 5
[10] F. Jurie and B. Triggs. Creating efficient codebooks for vi-
sual recognition. In ICCV, 2005. 2
[11] K. Kavukcuoglu, M. Ranzato, and Y. LeCun. Fast infer-
ence in sparse coding algorithms with applications to object
recognition. Technical report, Computational and Biological
Learning Lab, NYU, 2008. 8
[12] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of
features: Spatial pyramid matching for recognizing natural
scene categories. In CVPR, 2006. 1, 2, 5, 6, 7
[13] H. Lee, A. Battle, R. Raina, and A. Y. Ng. Efficient sparse
coding algorithms. In NIPS, 2006. 4
[14] F.-F. Li, R. Fergus, and P. Perona. Learning generative visual
models from few training examples:an incremental bayesian
approach tested on 101 object categories. In CVPR Workshop
on Generative-Model Based Vision, 2004. 1, 5
[15] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman.
Supervised dictionary learning. In NIPS, 2009. 8
[16] S. Maji, A. C. Berg, and J. Malik. Classification using inter-
section kernel support vector machine is efficient. In CVPR,
2008. 2
[17] J. Malik, S. Belongie, T. Leung, and J. Shi. Sparse repre-
sentation for color image restoration. IEEE Transaction on
Image Processing, 2008. 2
[18] J. Mariral, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman.
Discriminative learned dictionaries for local image analysis.
In CVPR, 2008. 2
[19] F. Moosmann, B. Triggs, and F. Jurie. Randomized cluster-
ing forests for building fast and discriminative visual vocab-
ularies. In NIPS, 2007. 2
[20] A. Oliva and A. Torraba. Modeling the shape of the scene:
A holistic representation of the spatial envelop. IJCV, 2001.
5
[21] P. Quelhas, F. Monay, J. Odobez, D. G.-P. T. Tuytelaars, and
L. V. Gool. Modeling scenes with local descriptors and latent
aspects. In ICCV, 2005. 2
[22] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng. Self-
taught learning: Transfer learning from unlabeled data. In
ICML, 2007. 2
[23] M. Ranzato, F. Huang, Y. Boureau, and Y. LeCun. Unsuper-
vised learning of invariant feature hierarchies with applica-
tions to object recognition. In CVPR, 2007. 2
[24] T. Serre, L. Wolf, and T. Poggio. Object recognition with
features inspired by visual cortex. In CVPR, 2005. 2, 4
[25] J. C. van Gemert, J.-M. Geusebroek, C. J. Veenman, and
A. W. M. Smeulders. Kernel codebooks for scene catego-
rization. In ECCV, 2008. 5, 6
[26] J. Yang, J. Wright, T. Huang, and Y. Ma. Image super-
resolution as sparse representation of raw image patches. In
CVPR, 2008. 2
[27] L. Yang, R. Jin, R. Sukthankar, and F. Jurie. Unifying dis-
criminative visual codebook generation with classifier train-
ing for object category recognition. In CVPR, 2008. 2
[28] H. Zhang, A. Berg, M. Maire, and J. Malik. Svm-knn: Dis-
criminative nearest heighbor classification for visual cate-
gory recognition. In CVPR, 2006. 5

