Novel approach to nonlinear/non-Gaussian 
Bayesian state estimation 
N.J. Gordon 
D.J. Salmond 
A.F.M. Smith 
Indexing terms. Kalmanfiller, Sequential estimation, Bayesianfilter 
Abstract: An algorithm, the bootstrap filter, is 
proposed for implementing recursive Bayesian 
filters. The required density of the state vector is 
represented as a set of random samples, which are 
updated and propagated by the algorithm. The 
method is not restricted by assumptions of linear- 
ity or Gaussian noise: it may be applied to any 
state transition or measurement model. A simula- 
tion example of the bearings only tracking 
problem is presented. This simulation includes 
schemes for improving the efficiency of the basic 
algorithm. For this example, the performance of 
the bootstrap filter is greatly superior to the 
standard extended Kalman filter. 
1 Introduction 
Bayesian methods provide a rigorous general framework 
for dynamic state estimation problems. The Bayesian 
approach is to construct the probability density function 
(PDF) of the state based on all the available information. 
For the linear-Gaussian estimation problem, the required 
PDF remains Gaussian at every iteration of the filter, 
and the Kalman filter relations propagate and update the 
mean and covariance of the distribution. For nonlinear 
or non-Gaussian problems there is no general analytic 
(closed form) expression for the required PDF. 
The extended Kalman filter (EKF) is the most popular 
approach to recursive nonlinear estimation 113. Here the 
estimation problem is linearised about the predicted state 
so that the Kalman filter can be applied. In this case the 
required PDF is still approximated by a Gaussian, which 
may be a gross distortion of the true underlying structure 
and may lead to filter divergence. Other analytic approx- 
imations include the Gaussian sum filter [2] and 
methods based on approximating the first two moments 
of the density [3,4]. A more direct numerical approach is 
to evaluate the required PDF over a grid in state space 
15-91, The choice of an efficient grid is nontrivial, and in 
a multidimensional state space a very large number of 
grid points may be necessary. A significant computation 
must be performed at each point. 
f; IEE, 1993 
Paper 9241F (E5), first received 27th April and in revised form 8th 
October 1992 
N.J. Gordon and D.J. Salmond are with the Defence Research Agency, 
Farnborough, Hampshire, United Kingdom 
A.F.M. Smith is with the Department of Statistics, Imperial College, 
London, United Kingdom 
IEE PROCEEDINGS-F, Vol. 140, N o .  2, A P R I L  1993 
In this paper we propose a new way of representing 
and recursively generating an approximation to the state 
PDF. The central idea is to represent the required PDF 
as a set of random samples, rather than as a function 
over state space [lo-121. As the number of samples 
becomes very large, they effectively provide an exact, 
equivalent, representation of the required PDF. Estim- 
ates of moments (such as mean and covariance) of per- 
centiles of the state vector PDF can be obtained directly 
from the samples. If necessary, a functional estimate of 
the PDF could also be constructed from the samples [13] 
and from this estimates of highest posterior density 
(HPD) intervals or the mode can be obtained. In Refer- 
ence 11, an adaptive importance sampling algorithm is 
presented. This results in large mixture distributions and 
the technique is computationally expensive. 
A recursive algorithm, which we call the bootstrap 
filter, for propagating and updating these samples for the 
discrete time problem (see Section 2) is presented and 
justified in Section 3. The filter is so called because the 
key update stage of the algorithm (Bayes rule) is imple- 
mented as a weighted bootstrap. This was motivated by a 
result from Smith and Gelfand 1141. A related algorithm 
is described in the paper by Muller 1121. These sampling 
techniques avoid the need to define a grid in state space; 
the samples being naturally concentrated in regions of 
high probability density. They also have the great advan- 
tage of being able to handle any functional nonlinearity, 
and system or measurement noise of any distribution. 
In Section 4, two simulation examples are presented. 
The second of these is the bearings-only tracking 
problem. The performance of the bootstrap filter is com- 
pared with that of the EKF. 
2 Recursive Bayesian estimation 
We are concerned with the discrete time estimation 
problem. The state vector, xk  E R" is assumed to evolve 
according to the following system model 
X k + l  z . h k ( x k >  w k )  (1) 
where f k :  [w" x R" + R" is the system transition function 
and wk E R" is a zero mean, white-noise sequence inde- 
pendent of past and current states. The PDF of wk is 
assumed to be known. At discrete times, measurements 
This work has been carried out with the support 
of DRA Farnborough. 
107 
y,  E Rp become available. These measurements are 
related to the state vector via the observation equation 
yk = hk(xk > (2) 
where h,: R" x R' --t Rp is the measurement function and 
U, E R' is another zero mean, white-noise sequence of 
known PDF, independent of past and present states and 
the system noise. It is assumed that the initial PDF 
A x l  1 Do) = A x l )  of the state vector is available together 
with the functional formsf, and hi for i = 1, ..., k .  The 
available information at time step k is the set of measure- 
ments D, = { y , :  i = 1,. . . , k } .  
The requirement is to construct the PDF of the 
current state x , ,  given all the available information: 
A x k  IDk). In principle this PDF may be obtained recursi- 
vely in two stages: prediction and update. Suppose that 
the required PDF p ( ~ ~ - ~  ID ,_ , )  at time step k - 1 is 
available. Then using the system model it is possible to 
obtain the prior PDF of the state at time step k 
dXX I D k -  1) = s d X k  I x k -  l)dxk- 1 I D k -  1) d x k -  I (3)  
Here the probabilistic model of the state evolution, 
~ ( X , ~ X , ~ ~ ) ,  which is a Markov model, is defined by the 
system equation and the known statistics of w,-  
I x k -  1) = dxP I x k  - 1, w k -  ])Awn- I I X L -  1) d w k -  1 s 
s 
Since by assumption p(wk-, 1 = p(w,-  we have 
P ( X , I X a - 1 )  = 6(x ,  - fk - I (xk - l>  W t - J  
(4) x A%- 1) dwk- 1 
where 6( .) is the Dirac delta function. The delta function 
arises because if x , - ~  and w,-] are known, then x ,  is 
obtained from a purely deterministic relationship (eqn. 1). 
Then at time step k a measurement y ,  becomes available 
and may be used to update the prior via Bayes rule 
where the normalising denominator is given by 
AYklD,-l) = P(Y,lX,)P(X,ID,-l) dx ,  (6) s 
The conditional PDF of yk given x k ,  
by the measurement model and the known statistics of U, 
1 x,), is defined 
d y k  I X t )  = s - hk(x, > uk))P(uk)  duk (7) 
In the update equation, eqn. 5 ,  the measurement y ,  is 
used to modify the predicted prior from the previous time 
step to obtain the required posterior of the state. 
The recurrence relations of eqns. 3 and 5 constitute the 
formal solution to the Bayesian recursive estimation 
problem. Analytic solutions to this problem are only 
available for a relatively small and restrictive choice of 
system and measurement models, the most important 
being the Kalman filter, which assumes f, and h, are 
linear and w, and ut are additive Gaussian noise of 
known variance (see for example Reference 15 or 16). 
Considerations of realism imply that these assumptions 
are unreasonable for many applications. Hence the need 
to find a method of implementation which allows the 
108 
models to be constructed realistically rather than con- 
veniently. 
3 Bayesian bootstrap filter 
3.1 Filter algorithm 
Suppose we have a set of random samples { x , -  ] ( i ) :  i = 1, 
. , . , N }  from the PDF p(x,-  1 D,-]). The bootstrap filter 
is an algorithm for propagating and updating these 
samples to obtain a set of values {x , ( i ) :  i = 1, ..., N } ,  
which are approximately distributed as p(x,  I D,). Thus 
the filter is an approximate mechanisation (simulation) of 
the relations in eqns. 3 and 5. 
Prediction: Each sample is passed through the system 
model to obtain samples from the prior at time step k :  
x:(i) = X - l ( x k - l ( i ) ,  ~ , - ~ ( i ) ) ,  where ~ , - ~ ( i )  s a sample 
drawn from the PDF of the system noise p(w,- 
Update:  On receipt of the measurement y,, evaluate the 
likelihood of each prior sample and obtain a normalised 
weight for each sample 
Thus define a discrete distribution over {x:( i ) :  i = 1, ..., 
N } ,  with probability mass qi associated with element i .  
Now resample N times from the discrete distribution to 
generate samples {x,( i ) :  i = 1, ..., N } ,  so that for any j ,  
Pr { x , ( j )  = x:( i )}  = q i .  
The above steps of prediction and update form a 
single iteration of the recursive algorithm. To initiate the 
algorithm, N samples x:(i) are drawn from the known 
prior p ( x l ) .  These samples feed directly into the update 
stage of the filter. We contend that the samples x,(i) are 
approximately distributed as the required PDF d x ,  1 0,). 
3.2 Justification 
Prediction: The prediction phase of the algorithm is 
intuitively reasonable. If ~ , - ~ ( i )  is a sample from 
p ( x k - ,  IDk_ and w,- I ( i )  is a sample from p(w,- l ) r  then 
x:(i) = X - l ( x k - l ( i ) ,  ~ , - ~ ( i ) )  is distributed as p ( x k l D , - , ) .  
Repeating this process for each of the N samples x , -  I ( i )  
in turn gives the values {x: ( i ) :  i = 1, ..., N )  which are 
independently distributed as A x ,  I D,- ]). 
Update:  The justification for the update phase relies on a 
result from Smith and Gelfand 1141. They show that 
Bayes theorem can be implemented as a weighted boot- 
strap. Suppose that samples {x:( i ) :  i = 1 ,  ..., N }  are 
available from a continuous density function G(x)  and 
that samples are required from the PDF proportional to 
L(x)G(x) ,  where L(x)  is a known function. The theorem 
states that a sample drawn from the discrete distribution 
over {x: ( i ) :  i = 1, ..., N }  with probability mass 
L(x: ( i ) ) /x  L(x:(j)) on x:(i), tends in distribution to the 
required density as N tends to infinity. If G ( x )  is identified 
with A x k  1 D,- 1) (the prior) and L(x)  with p ( y ,  1 x,) (the 
likelihood), then this theorem provides a justification for 
the update procedure. 
3.3 Discussion 
As already noted, the great strength of this technique is 
that no restrictions are placed on the functionsf, and h k ,  
or on the distributions of the system or measurement 
noise. The only requirements are that 
I E E  PROCEEDINGS-F, Vol .  140, No .  2.  A P R I L  1993 
(a) p ( x l )  is available for sampling 
(b)  the likelihood p(y,  I x k )  is a known functional form 
(c) p(w,) is available for sampling. 
Also the basic algorithm is very simple and easy to 
program. The resampling update stage is performed by 
drawing a random sample ui from the uniform distribu- 
tion over (0, 11. The value x: (M)  corresponding to 
M -  1 M c 4j < ui c 4j 
j = O  j = O  
where qo = 0, is selected as a sample for the posterior. 
This procedure is repeated for i = 1 ,  . . . , N .  It would also 
be straightforward to implement this algorithm on mas- 
sively parallel computers, raising the possibility of real 
time operation with very large sample sets. 
The output of the algorithm as a set of samples of the 
required posterior is convenient for many applications. 
For example, the posterior probability of the state falling 
within any region of interest may be estimated by calcu- 
lating the proportion of samples within that region. Like- 
wise, if there is good reason to believe that the posterior 
is unimodal, it is easy to estimate the HPD region for 
each component of the state vector (i.e. for the 
marginals). It is also straightforward to obtain estimates 
of the mean and covariance of the state, and indeed any 
function of the state. 
The justification for the bootstrap filter given in 
Section 3.2 is based on asymptotic results. It is most diffi- 
cult to prove any general result for a finite number of 
samples. Likewise it is most difficult to make any precise, 
provable statement on the crucial question of how many 
samples are required to give a satisfactory representation 
of the densities for filter operation. However it is clear 
that the required number N depends on at least three 
factors 
(a) the dimension of the state space 
(b) the typical ‘overlap’ between the prior and the like- 
(c) the required number of time steps. 
lihood 
Taking the first of these, one must expect N to rise 
rapidly with the dimension of the space. The rate of 
increase is governed by the interdependencies between 
the components of the state vector. In the most benign 
case of independent components, the required number of 
state vector samples should not increase with the dimen- 
sion of the space. 
The dependency of N on factors (b )  and (c) is a direct 
consequence of the resampling update stage of the filter. 
If the region of state space where the likelihood p(ya I x,) 
takes significant values is small in comparison with the 
region where the prior p(x ,  I D,- ,) is significant, many of 
the samples x:(i) will receive a very small weighting q i ,  
and will not be selected in the resampling procedure. 
Thus, samples of the prior remote from the likelihood are 
effectively wasted, and those nearby are reselected many 
times. The problem is exacerbated if the likelihood falls in 
a region of low prior density, where there are relatively 
few samples (also see Reference 14). Through this process, 
the representation of the PDF may become most inade- 
quate within a few time steps. Indeed if there is no system 
noise, all of the N samples may rapidly collapse to a 
single value. In principle one could overcome this diffi- 
culty by the ‘brute force’ approach of employing an enor- 
mous number of samples. However to develop practical 
filters for interesting problems, a more subtle approach is 
necessary. In the simulation example of Section 4.2, we 
IEE PROCEEDINGS-F, Vol.  140, No .  2, A P R I L  1993 
suggest and implement two modifications to the basic 
algorithm for this purpose: a roughening procedure to 
jitter the resampled values and a prior editing scheme to 
boost the number of prior samples in the vicinity of sig- 
nificant likelihood. There is much scope for research into 
the development of such modifications to the basic algo- 
rithm to improve efficiency. 
The sampling technique proposed by Muller [12] 
differs from our method in the update stage; the predic- 
tion phase being identical. Muller’s update scheme is an 
accept/reject procedure. Each prior sample x:(i) is con- 
sidered in turn. It is accepted as a posterior sample with a 
probability proportional to p(y,  I x:(i)), otherwise it is 
rejected. One disadvantage of this scheme is that the 
sample size is random and decreasing. The main subject 
of Reference 12 is an algorithm for expanding the prior 
sample by resampling from an envelope density fitted to 
the original prior sample. 
4 Simulation 
We present two examples which illustrate the operation 
of the bootstrap filter. Estimation performance is com- 
pared with the standard EKF. The first example is a uni- 
variate nonstationary growth model taken from 
References 8 and 10. The second is a practical, bearings- 
only tracking problem, over a four-dimensional state 
space. 
4.1 One-dimensional nonlinear example 
Consider the following nonlinear model [SI 
X, = O . ~ X , - ,  + 2 5 ~ , - , / ( 1  + x : - ~ )  
+ 8 COS (1.2(k - 1)) + W ,  (9) 
(10) y ,  = x:/20 + U, 
where w, and U, are zero-mean Gaussian white noise with 
variances 10.0 and 1.0, respectively. This example is 
severely nonlinear, both in the system and the measure- 
ment equation. Note that the form of the likelihood 
p(y,Ix,)  adds an interesting twist to the problem. For 
measurements y, < 0 the likelihood is unimodal at zero 
and symmetric about zero. However, for positive meas- 
urements the likelihood is symmetric about zero with 
modes at ( 2 0 ~ ~ ) ” ~ .  
The initial state was taken to be x o  = 0.1 and Fig. 1 
shows a 50 step realisation of eqn. 9. The EKF and 
40 l  
-20 ~ V 1 
_LO~-LLYLYYLIYL~, , , . , ,  , , , , , , , , , , , I . .  , , , , . , , I  
10 20 30 40 50 
Fig. 1 
Initial state xo = 0.1 
50 point realisation of eqn. 9 
109 
bootstrap filters were both initialised with the prior PDF 
p(xo) = N(O,2). Fig. 2 shows the result of applying the 
EKF to 50 measurements generated according to eqn. 10. 
401 I !  
Fig. 2 E K F  estimate of posterior mean 
True state 
EKF estimate of posterior mean 
Estimated limits of 95% probabilily region 
~ 
- - - - - - -  
The true state is represented by a star, EKF mean is 
given as a solid line and the dashed lines give the 95% 
probability region (note that for convenience we refer to 
the interval between the 2.5 and 97.5 percentile points as 
the 95% probability region). The graph has been clipped 
at k40. The actual value of the state is within the 95% 
probability region on only about 30% of occasions. 
Fig. 3 shows the result of directly applying the boot- 
strap algorithm of Section 3.1 with a sample size of 
4 0 ~  
-400L.. . . 1 . 1  , . . . j jL- .  . I l , r > ,   , , , . , , ,  , , , , , , , , 
30 40 50 
k 
IO 
Fig. 3 Booistrapfilter estimate of posierior mean 
True state 
Bootstrap estimate of posterior mean 
Estimated limits 0195% probability region 
~ 
. . . . . . . 
N = 500. The solid line gives the mean of the posterior 
samples and the dashed lines estimate the 95% probabil- 
ity region. These may not represent the true 95% HPD 
region, since, as Fig. 4 shows, the posterior can be 
bimodal in this example. At a couple of time steps, the 
actual state is just outside these percentile estimates, and 
quite often it is close to one of the limits. However, most 
of the time the actual state is very close to the posterior 
mean, and performance is obviously greatly superior to 
the EKF. Running the bootstrap filter with larger sample 
sets gave results indistinguishable from Fig. 3, and this is 
taken as confirmation that our sample set size is s u a -  
cient. The relatively high system noise probably accounts 
for the reasonable performance of the basic algorithm 
110 
using only 500 samples : the system noise automatically 
roughens the prior samples. 
Fig. 4 shows estimates of the posterior density from 
both the bootstrap filter and the EKF at k = 21. The 
L - ~ d C l L .  t . i d  0 40 -20 0 20 
x21 
Fig. 4 Estimated posterior PDF ai time step 21 
~ Bootstrap estimate 
. . . . . . . EKF estimate 
bootstrap PDF is a kernel density estimate 
Ll 
40 
131 recon- 
structed from the posterior samples. It has a bimodal 
structure, with the true value of located close to the 
larger mode. The Gaussian PDF from the EKF is remote 
from the true state value at this time step. 
4.2 Bearings-only tracking example 
In this example, the target moves within the x-y plane 
according to the standard second-order model 
x, = Q X ~  - + rw, (11) 
where xk  = (x, k, y, j):, w k  = (w,, w$ 
1 1 0 0  0.5 0 .-[. 0 0 0 1  ; ;)and r= [ ;  ;5) 
Here x and y denote the Cartesian co-ordinates of the 
target: there should be no confusion with the notation 
for the state vector and measurement vector in our earlier 
general discussion of the filter. The system noise is a zero 
mean Gaussian white noise process with covariance 
Q: E[wkwT]  = Qajk ,  where Q = ql ,  and I ,  is the 2 x 2 
identity matrix. A fixed observer at the origin of the plane 
takes noisy measurements zk of the target bearing 
zk = tan ~ ' ( y k / x k )  + uk (12) 
The measurement noise is a zero mean Gaussian white 
noise process with variance r :  E [ u k u j ]  = dk j .  Before 
measurements are taken at k = 1, the initial state vector 
is assumed to have a Gaussian distribution with known 
mean X1, and covariance 
/ U :  0 0 o \  
0 u ; o o  
o o o u :  
0 U: o j  
A target trajectory and associated measurements over 24 
time steps have been generated according to eqns. 1 1  and 
12 with the parameter values (arbitrary units): 
J(q) = 0.001, J ( r )  = 0.005 and the actual initial state of 
the target xI = (-0.05, 0.001, 0.7, -0.055)T. Fig. 5 gives 
IEE PROCEEDINGS-F, Vol. 140, No.  2, APRIL 1YY3 
1 0 -  
0 5 
yk 
-0 
-1 0 
-1 
-01 
A more detailed analysis of the filter performance for 
the x and i co-ordinates is given in Figs. 8 and 9 for the 
bootstrap filter, and in Figs. 10 and 11 for the EKF. For 
the bootstrap filter, the actual co-ordinate value is 
practically always within the 95% probability region. 
L ~ k z l  
~ 
' a  k 2 2 4  
* A  0- 
- 
5- 
5- 
, , , , , / , , . ,  , , , , I , , , , , . , , , ,  
0 01 0 2  0 3  
-0 LO1 
006 008 010 0 12 0 14 
Scatterplor of 500 bootstrap x -y  samplesfrom p(xZ4 I D J  
- 0 8 0  
X 
Fig. 7 
A Target position 
xk E 
- l  I,, , , , ,  , , , ,  , , - ,  , , , , , ,  , , , , , ,  l',~,~,;;,, , , , /  
-1 5 
-0 1 0 01 0 2  0 3  
'k 
Fig. 6 
A Target position 
~ Bootstrap estimate 
Estimated posterior mean of the target track in the x -y  plane 
EKF estimate ~~~~ 
The continuous line shows the trajectory estimate (the 
mean of the posterior PDF) from the bootstrap filter and 
the dashed line gives the EKF estimate. After an initial 
period of uncertainty the bootstrap filter quickly homes 
onto the target, whereas the EKF rapidly diverges. The 
performance of the bootstrap filter is clearly superior to 
that of the EKF for this example. 
Although the mean of the posterior distribution has 
been chosen as a point estimate of the target state, there 
is of course more information available from the boot- 
strap filter. This is typified by Fig. 7 which shows the 
(x, y )  co-ordinates of 500 out of the 4000 samples from 
the posterior PDF at time step 24. The true state is 
shown by a triangle. The skewed nature of the samples 
towards the lower right corner highlights the non- 
Gaussian nature of the PDF. 
Fig. 8 
region. x co-ordinate 
Bootstrap estimate of the posterior mean and 95% probability 
A True target state 
i Bootstrap estimated target state 
~~~~ Estimated limits of 95% probability region 
0 06 i 
0 041 
Fig. 9 
region i co-ordinate 
Bootstrap estimate of the posterior mean and 95% probability 
A True large1 state * Bootstrap eslimated target stale 
~~~~ Estimated limits of 95% probability region 
IEE PROCEEDINGS-F, Vol. 140, No .  2, APRIL 1993 111 
However, the EKF is consistently over-optimistic about 
its tracking performance, and serious divergence occurs 
after k = 13. 
0 3c 
10 15 20 25 
- 0 1 L -  ' , , , , ' " ' ' 1 '  ' ' ' ' ' ' ' ' 
0 
Fig. 10 
region: x co-ordinate 
EKF estimate of the posterior mean and 95"% probability 
A True targel State * EKF estimated target state 
-~~~ Estimated limits 0195% probability region 
0 06 c 
d 
10 15 20 25 
Fig. 11 
region: i co-ordinate 
EKF estimate of the posterior mean and 95% probability 
A True target state * EKF estimated target slate 
~~~~ Estimated limits of 95% probability region 
The implementation of the bootstrap filter for this 
example using a modest number of samples requires 
some ingenuity. This is for reasons discussed in Section 
3.3: at the start of the track and as the target approaches 
the observer, there is only a small 'overlap' between the 
prior and the likelihood. Two schemes have been imple- 
mented for combating the consequent reduction in the 
number of truly distinct sample values. 
Roughening: The first of these is a roughening procedure. 
An independent jitter ci is added to each sample drawn in 
the update procedure. ci is a sample from N(0, Jk) ,  where 
J ,  is a diagonal covariance matrix. The standard devi- 
ation U of the Gaussian jitter corresponding to a particu- 
lar component of the state vector is given by 
U = where E is the length of the interval 
between the maximum and the minimum samples of this 
component (before roughening), d is the dimension of the 
state space, and K is a constant tuning parameter. By 
taking the standard deviation of the jitter to be inversely 
proportional to the dth root of the sample size, the degree 
112 
of roughening is normalised to the spacing between 
nodes of the corresponding uniform rectangular grid of N 
points. In this example, K = 0.2 has been chosen. Thus 
the standard deviation of the Gaussian jitter is 20% of 
the node spacing on the equivalent rectangular grid. 
Clearly the choice of K is a compromise. Too large a 
value would blur the distribution but too small a value 
would produce tight clusters of points around the orig- 
inal samples. 
Prior editing: The second procedure artificially boosts 
the number of samples of the prior in the vicinity of the 
likelihood. If one is prepared to delay the state estimate 
by one time step, this may be achieved by subjecting the 
samples from the update stage to a coarse, pragmatic 
acceptance test (which effectively edits the prior): 
(a) Take the sample xk(i) and pass it through the 
roughening procedure and system model to generate 
x:+ A i ) .  
(b)  Assuming that the measurement from the next time 
step is available, calculate v k +  l ( i )  = zk+ - h(x:+ l ( i ) ) .  
(c) If I v k +  l(i) I,> 6J(r) ,  then the sample x:+ l ( i )  is 
remote from the likelihood and so is most unlikely to be 
selected at the k + lth update stage. In this case reject 
the sample xk(i)  and repeat step (a) to draw a replacement 
sample from the update stage k.  Otherwise accept xk(i), 
increment i and repeat if i < N .  
Note that this procedure has the effect of a crude, single 
stage smoothing operation on the samples xk(i). Thus the 
accepted samples xk(i) are vaguely distributed as 
p(x,  I Dk+J. However, the bootstrap results shown in 
Figs. 8 and 9 are obtained from samples taken without 
editing by the acceptance test, and so correctly represent 
the unsmoothed PDF A x k  I Dt). 
By employing these techniques of roughening and of 
accepting only 'useful' samples, the results presented have 
been obtained by propagating only N = 4000 samples. In 
four-dimensional space a grid of this size would have 
only about 8 points on each co-ordinate. Note that the 
number of samples rejected by the prior editing test is in 
some sense a measure of the useful information contained 
in the measurement. The few measurements taken as the 
target flies past the observer are most informative, and 
for these time steps the number of rejected samples rose 
to about 1OOOOO. Before and after the fly-past period, the 
target is moving approximately along a radius vector of 
the observer, and so the observed target-bearing is 
almost constant. During these periods the number of 
rejected samples is low (between about 10 and 100) as 
successive measurements contribute little new informa- 
tion. This effect is also clear in Fig. 8: the limits of the 
95% probability region converge during fly-past. The 
EKF performance after time step 15 is especially poor 
because the filter has seriously misinterpreted the valu- 
able measurements at fly-past. 
It should be noted that the EKF results are from a 
naive application of the filter, directly to the given system 
and measurement model. A reparameterisation of the 
problem using modified polar co-ordinates (see Aidala 
and Hammel [17]), to help separate the observable and 
unobservable elements of the state, may well have per- 
formed better. Also the prior editing test employed with 
the bootstrap filter has no direct equivalent in the EKF 
formulation. However, it is possible to perform a gating 
test on each bearing measurement. For example if an 
EKF innovation were unexpectedly large, the corres- 
ponding measurement could be ignored. With the accep- 
IEE PROCEEDINGS-F, Vol. 140, N o .  2, A P R I L  1993 
tance threshold set to + 3  standard deviations, I O  of the 
bearing measurements after the target passed the obser- 
ver were ignored. Although the EKF was then acting 
only as a predictor, the estimation error was much 
reduced. However the actual state was still only rarely 
within the 95% probability region. 
5 Conclusions 
A new algorithm, the bootstrap filter, for implementing 
recursive Bayesian filters has been presented. The 
required posterior distribution is produced as a set of 
samples. and the method is not restricted by consider- 
ations of analytical tractability. 
However for many interesting problems, the number 
of truly distinct values in the sample set may rapidly col- 
lapse. To remedy this difficulty, two somewhat ad hoc 
schemes (a roughening and a prior editing procedure) 
have been proposed. These procedures have been imple- 
mented in the simulation of a nonlinear bearings-only 
tracking problem with a four-dimensional state vector. 
The simulation shows that even with a modest number of 
samples, the bootstrap filter is far superior to the stand- 
ard extended Kalman filter. 
Further work is necessary to derive practical methods, 
with rigorous theoretical foundations, for avoiding the 
collapse in the number of distinct values in the sample 
set. It would also be most useful to develop some guide 
to the number of samples required for satisfactory boot- 
strap performance. Further Monte Carlo simulation 
studies should be performed to provide a quantitative 
assessment of filter performance for important nonlinear 
problems. 
6 References 
1 JAZWINSKI, A.H.: ‘Stochastic processes and filtering theory’ 
(Academic Press, New York, 1970) 
2 ALSPACH, D.L., and SORENSON, H.W.: ‘Nonlinear Bayesian 
estimation using Gaussian sum approximation’, IEEE Trans. Auto. 
Control, 1972,17, pp. 439-447 
3 MASRELIEZ, C.J.: ‘Approximate non-Gaussian filtering with linear 
state and observation relations’, IEEE Trans. Auto. Control, 1975, 
20, pp. 107-110 
4 WEST, M., HARRISON, P.J., and MIGON, H.S.: ‘Dynamic gener- 
alised linear models and Bayesian forecasting (with discussion)’, J. 
Amer. Statistical Assoc., 1985, 80, pp. 73-97 
5 BUCY, R.S.: ‘Bayes theorem and digital realization for nonlinear 
filters’, J. Astronaut. Sci., 1969, 17, pp. 80-94 
6 KRAMER, S.C., and SORENSON, H.W.: ‘Recursive Bayesian esti- 
mation using piece-wise constant approximations’, Automntica, 
1988, 24, (6), pp. 789-801 
7 SORENSON, H.W.: ‘Recursive estimation for nonlinear dynamic 
systems’, in SPALL, J.C. (Ed.): ‘Bayesian analysis of time series and 
dynamic models’ (Dekker, 1988) 
8 KITTAGAWA, G.: ’Non-Gaussian state-space modelling of non- 
stationary time series (with discussion)’, J .  Amer. Statistical Assoc., 
1987.82, pp. 1032-1063 
9 POLE, A., and WEST, M.: ‘Eflicient numerical integration in 
dynamic models’. Research report, 136, Department of statistics, 
University of Warwick, 1988 
10 CARLIN, B.P., POLSON, N.G., and STOFFER, D.S.: ‘A Monte- 
Carlo approach to nonnormal and nonlinear state space modelling’, 
J. Amer. Statistical Assoc., 1992,87, pp. 493-500 
11 WEST, M.: ‘Modelling with mixtures (with discussion)’, in BER- 
NARDO, J.M., BERGER, J.O., DAWID, A.P., and SMITH, A.F.M. 
(Eds.): ‘Bayesian statistics 4‘ (Oxford University Press, 1992), 
pp. 503-524 
12 MULLER, P.: ‘Monte Carlo integration in general dynamic 
models’, Contemp. Math., 1991, 115, pp. 145-163 
13 SILVERMAN, B.W.: ‘Density estimation for statistics and data 
analysis’ (Chapman & Hall, London, 1986) 
14 SMITH, A.F.M., and GELFAND, A.E.: ‘Bayesian statistics without 
tears: a sampling-resampling perspective’, Amer. Stat., 1992, 46, 
pp. 84-88 
15 HO, Y.C., and LEE, R.C.K.: ‘A Bayesian approach to problems in 
stochastic estimation and control’, IEEE Trans. Auto. Control., 
1964,9, pp. 333-339 
16 HARRISON, P.J., and STEVENS, C.F.: ‘Bayesian forecasting (with 
discussion)’, J. R.  Stat. Soc. E, 1976.38, pp. 205-247 
17 AIDALA, V.J., and HAMMEL, S.E.: ‘Utilization of modified polar 
coordinates for hearings-only tracking’, IEEE Trans. Auto. Control, 
1983, AC-28, (3), pp. 283-294 
IEE PROCEEDINGS-F, Vol. 140, No .  2, APRIL I993 113 

