Robust Principal Component Analysis?
Emmanuel J. CandeÌ€s1,2, Xiaodong Li2, Yi Ma3,4, and John Wright4
1 Department of Statistics, Stanford University, Stanford, CA 94305
2 Department of Mathematics, Stanford University, Stanford, CA 94305
3,4 Electrical and Computer Engineering, UIUC, Urbana, IL 61801
4 Microsoft Research Asia, Beijing, China
December 17, 2009
Abstract
This paper is about a curious phenomenon. Suppose we have a data matrix, which is the
superposition of a low-rank component and a sparse component. Can we recover each component
individually? We prove that under some suitable assumptions, it is possible to recover both the
low-rank and the sparse components exactly by solving a very convenient convex program called
Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted
combination of the nuclear norm and of the `1 norm. This suggests the possibility of a principled
approach to robust principal component analysis since our methodology and results assert that
one can recover the principal components of a data matrix even though a positive fraction of its
entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries
are missing as well. We discuss an algorithm for solving this optimization problem, and present
applications in the area of video surveillance, where our methodology allows for the detection of
objects in a cluttered background, and in the area of face recognition, where it offers a principled
way of removing shadows and specularities in images of faces.
Keywords. Principal components, robustness vis-a-vis outliers, nuclear-norm minimization,
`1-norm minimization, duality, low-rank matrices, sparsity, video surveillance.
1 Introduction
1.1 Motivation
Suppose we are given a large data matrix M , and know that it may be decomposed as
M = L0 + S0,
where L0 has low-rank and S0 is sparse; here, both components are of arbitrary magnitude. We
do not know the low-dimensional column and row space of L0, not even their dimension. Similarly,
we do not know the locations of the nonzero entries of S0, not even how many there are. Can we
hope to recover the low-rank and sparse components both accurately (perhaps even exactly) and
efficiently?
A provably correct and scalable solution to the above problem would presumably have an impact
on todayâ€™s data-intensive scientific discovery.1 The recent explosion of massive amounts of high-
1Data-intensive computing is advocated by Jim Gray as the fourth paradigm for scientific discovery [24].
1
ar
X
iv
:0
91
2.
35
99
v1
  [
cs
.I
T
] 
 1
8 
D
ec
 2
00
9
dimensional data in science, engineering, and society presents a challenge as well as an opportunity
to many areas such as image, video, multimedia processing, web relevancy data analysis, search,
biomedical imaging and bioinformatics. In such application domains, data now routinely lie in
thousands or even billions of dimensions, with a number of samples sometimes of the same order
of magnitude.
To alleviate the curse of dimensionality and scale,2 we must leverage on the fact that such data
have low intrinsic dimensionality, e.g. that they lie on some low-dimensional subspace [15], are
sparse in some basis [13], or lie on some low-dimensional manifold [4,46]. Perhaps the simplest and
most useful assumption is that the data all lie near some low-dimensional subspace. More precisely,
this says that if we stack all the data points as column vectors of a matrix M , the matrix should
have (approximately) low-rank: mathematically,
M = L0 +N0,
where L0 has low-rank and N0 is a small perturbation matrix. Classical Principal Component
Analysis (PCA) [15,25,27] seeks the best (in an `2 sense) rank-k estimate of L0 by solving
minimize â€–M âˆ’ Lâ€–
subject to rank(L) â‰¤ k.
(Throughout the paper, â€–Mâ€– denotes the 2-norm; that is, the largest singular value of M .) This
problem can be efficiently solved via the singular value decomposition (SVD) and enjoys a number
of optimality properties when the noise N0 is small and i.i.d. Gaussian.
Robust PCA. PCA is arguably the most widely used statistical tool for data analysis and dimen-
sionality reduction today. However, its brittleness with respect to grossly corrupted observations
often puts its validity in jeopardy â€“ a single grossly corrupted entry in M could render the estimated
LÌ‚ arbitrarily far from the true L0. Unfortunately, gross errors are now ubiquitous in modern appli-
cations such as image processing, web data analysis, and bioinformatics, where some measurements
may be arbitrarily corrupted (due to occlusions, malicious tampering, or sensor failures) or simply
irrelevant to the low-dimensional structure we seek to identify. A number of natural approaches
to robustifying PCA have been explored and proposed in the literature over several decades. The
representative approaches include influence function techniques [26,47], multivariate trimming [19],
alternating minimization [28], and random sampling techniques [17]. Unfortunately, none of these
existing approaches yields a polynomial-time algorithm with strong performance guarantees under
broad conditions3. The new problem we consider here can be considered as an idealized version of
Robust PCA, in which we aim to recover a low-rank matrix L0 from highly corrupted measurements
M = L0+S0. Unlike the small noise term N0 in classical PCA, the entries in S0 can have arbitrarily
large magnitude, and their support is assumed to be sparse but unknown4.
2We refer to either the complexity of algorithms that increases drastically as dimension increases, or to their
performance that decreases sharply when scale goes up.
3Random sampling approaches guarantee near-optimal estimates, but have complexity exponential in the rank of
the matrix L0. Trimming algorithms have comparatively lower computational complexity, but guarantee only locally
optimal solutions.
4The unknown support of the errors makes the problem more difficult than the matrix completion problem that
has been recently much studied.
2
Applications. There are many important applications in which the data under study can natu-
rally be modeled as a low-rank plus a sparse contribution. All the statistical applications, in which
robust principal components are sought, of course fit our model. Below, we give examples inspired
by contemporary challenges in computer science, and note that depending on the applications,
either the low-rank component or the sparse component could be the object of interest:
â€¢ Video Surveillance. Given a sequence of surveillance video frames, we often need to identify
activities that stand out from the background. If we stack the video frames as columns
of a matrix M , then the low-rank component L0 naturally corresponds to the stationary
background and the sparse component S0 captures the moving objects in the foreground.
However, each image frame has thousands or tens of thousands of pixels, and each video
fragment contains hundreds or thousands of frames. It would be impossible to decompose M
in such a way unless we have a truly scalable solution to this problem. In Section 4, we will
show the results of our algorithm on video decomposition.
â€¢ Face Recognition. It is well known that images of a convex, Lambertian surface under varying
illuminations span a low-dimensional subspace [1]. This fact has been a main reason why
low-dimensional models are mostly effective for imagery data. In particular, images of a
humanâ€™s face can be well-approximated by a low-dimensional subspace. Being able to correctly
retrieve this subspace is crucial in many applications such as face recognition and alignment.
However, realistic face images often suffer from self-shadowing, specularities, or saturations
in brightness, which make this a difficult task and subsequently compromise the recognition
performance. In Section 4, we will show how our method is able to effectively remove such
defects in face images.
â€¢ Latent Semantic Indexing. Web search engines often need to analyze and index the content
of an enormous corpus of documents. A popular scheme is the Latent Semantic Indexing
(LSI) [14, 42]. The basic idea is to gather a document-versus-term matrix M whose entries
typically encode the relevance of a term (or a word) to a document such as the frequency it
appears in the document (e.g. the TF/IDF). PCA (or SVD) has traditionally been used to
decompose the matrix as a low-rank part plus a residual, which is not necessarily sparse (as
we would like). If we were able to decompose M as a sum of a low-rank component L0 and a
sparse component S0, then L0 could capture common words used in all the documents while
S0 captures the few key words that best distinguish each document from others.
â€¢ Ranking and Collaborative Filtering. The problem of anticipating user tastes is gaining in-
creasing importance in online commerce and advertisement. Companies now routinely collect
user rankings for various products, e.g., movies, books, games, or web tools, among which
the Netflix Prize for movie ranking is the best known [40]. The problem is to use incomplete
rankings provided by the users on some of the products to predict the preference of any given
user on any of the products. This problem is typically cast as a low-rank matrix completion
problem. However, as the data collection process often lacks control or is sometimes even
ad hoc â€“ a small portion of the available rankings could be noisy and even tampered with.
The problem is more challenging since we need to simultaneously complete the matrix and
correct the errors. That is, we need to infer a low-rank matrix L0 from a set of incomplete
and corrupted entries. In Section 1.6, we will see how our results can be extended to this
situation.
3
Similar problems also arise in many other applications such as graphical model learning, linear
system identification, and coherence decomposition in optical systems, as discussed in [12]. All in
all, the new applications we have listed above require solving the low-rank and sparse decomposition
problem for matrices of extremely high dimension and under much broader conditions, a goal this
paper aims to achieve.
1.2 A surprising message
At first sight, the separation problem seems impossible to solve since the number of unknowns to
infer for L0 and S0 is twice as many as the given measurements in M âˆˆ Rn1Ã—n2 . Furthermore, it
seems even more daunting that we expect to reliably obtain the low-rank matrix L0 with errors in
S0 of arbitrarily large magnitude.
In this paper, we are going to see that very surprisingly, not only can this problem be solved,
it can be solved by tractable convex optimization. Let â€–Mâ€–âˆ— :=
âˆ‘
i Ïƒi(M) denote the nuclear
norm of the matrix M , i.e. the sum of the singular values of M , and let â€–Mâ€–1 =
âˆ‘
ij |Mij | denote
the `1-norm of M seen as a long vector in Rn1Ã—n2 . Then we will show that under rather weak
assumptions, the Principal Component Pursuit (PCP) estimate solving5
minimize â€–Lâ€–âˆ— + Î»â€–Sâ€–1
subject to L+ S = M
(1.1)
exactly recovers the low-rank L0 and the sparse S0. Theoretically, this is guaranteed to work even if
the rank of L0 grows almost linearly in the dimension of the matrix, and the errors in S0 are up to a
constant fraction of all entries. Algorithmically, we will see that the above problem can be solved by
efficient and scalable algorithms, at a cost not so much higher than the classical PCA. Empirically,
our simulations and experiments suggest this works under surprisingly broad conditions for many
types of real data. In Section 1.5, we will comment on the similar approach taken in the paper [12],
which was released during the preparation of this manuscript.
1.3 When does separation make sense?
A normal reaction is that the objectives of this paper cannot be met. Indeed, there seems to not be
enough information to perfectly disentangle the low-rank and the sparse components. And indeed,
there is some truth to this, since there obviously is an identifiability issue. For instance, suppose
the matrix M is equal to e1eâˆ—1 (this matrix has a one in the top left corner and zeros everywhere
else). Then since M is both sparse and low-rank, how can we decide whether it is low-rank or
sparse? To make the problem meaningful, we need to impose that the low-rank component L0 is
not sparse. In this paper, we will borrow the general notion of incoherence introduced in [8] for the
matrix completion problem; this is an assumption concerning the singular vectors of the low-rank
component. Write the singular value decomposition of L0 âˆˆ Rn1Ã—n2 as
L0 = UÎ£V âˆ— =
râˆ‘
i=1
Ïƒiuiv
âˆ—
i ,
5Although the name naturally suggests an emphasis on the recovery of the low-rank component, we reiterate that
in some applications, the sparse component truly is the object of interest.
4
where r is the rank of the matrix, Ïƒ1, . . . , Ïƒr are the positive singular values, and U = [u1, . . . , ur],
V = [v1, . . . , vr] are the matrices of left- and right-singular vectors. Then the incoherence condition
with parameter Âµ states that
max
i
â€–Uâˆ—eiâ€–2 â‰¤
Âµr
n1
, max
i
â€–V âˆ—eiâ€–2 â‰¤
Âµr
n2
, (1.2)
and
â€–UV âˆ—â€–âˆ â‰¤
âˆš
Âµr
n1n2
. (1.3)
Here and below, â€–Mâ€–âˆ = maxi,j |Mij |, i.e. is the `âˆ norm of M seen as a long vector. Note
that since the orthogonal projection PU onto the column space of U is given by PU = UUâˆ—, (1.2)
is equivalent to maxi â€–PUeiâ€–2 â‰¤ Âµr/n1, and similarly for PV . As discussed in earlier references
[8, 10, 22], the incoherence condition asserts that for small values of Âµ, the singular vectors are
reasonably spread out â€“ in other words, not sparse.
Another identifiability issue arises if the sparse matrix has low-rank. This will occur if, say, all
the nonzero entries of S occur in a column or in a few columns. Suppose for instance, that the first
column of S0 is the opposite of that of L0, and that all the other columns of S0 vanish. Then it is
clear that we would not be able to recover L0 and S0 by any method whatsoever since M = L0 +S0
would have a column space equal to, or included in that of L0. To avoid such meaningless situations,
we will assume that the sparsity pattern of the sparse component is selected uniformly at random.
1.4 Main result
The surprise is that under these minimal assumptions, the simple PCP solution perfectly recovers
the low-rank and the sparse components, provided of course that the rank of the low-rank compo-
nent is not too large, and that the sparse component is reasonably sparse. Below, n(1) = max(n1, n2)
and n(2) = min(n1, n2).
Theorem 1.1 Suppose L0 is nÃ— n, obeys (1.2)â€“(1.3), and that the support set of S0 is uniformly
distributed among all sets of cardinality m. Then there is a numerical constant c such that with
probability at least 1âˆ’ cnâˆ’10 (over the choice of support of S0), Principal Component Pursuit (1.1)
with Î» = 1/
âˆš
n is exact, i.e. LÌ‚ = L0 and SÌ‚ = S0, provided that
rank(L0) â‰¤ ÏrnÂµâˆ’1(log n)âˆ’2 and m â‰¤ Ïs n2. (1.4)
Above, Ïr and Ïs are positive numerical constants. In the general rectangular case where L0 is
n1Ã—n2, PCP with Î» = 1/âˆšn(1) succeeds with probability at least 1âˆ’cnâˆ’10(1) , provided that rank(L0) â‰¤
Ïrn(2) Âµ
âˆ’1(log n(1))âˆ’2 and m â‰¤ Ïs n1n2.
In other words, matrices L0 whose singular vectorsâ€”or principal componentsâ€”are reasonably
spread can be recovered with probability nearly one from arbitrary and completely unknown cor-
ruption patterns (as long as these are randomly distributed). In fact, this works for large values of
the rank, i.e. on the order of n/(log n)2 when Âµ is not too large. We would like to emphasize that
the only â€˜piece of randomnessâ€™ in our assumptions concerns the locations of the nonzero entries
of S0; everything else is deterministic. In particular, all we require about L0 is that its singular
vectors are not spiky. Also, we make no assumption about the magnitudes or signs of the nonzero
5
entries of S0. To avoid any ambiguity, our model for S0 is this: take an arbitrary matrix S and set
to zero its entries on the random set â„¦c; this gives S0.
A rather remarkable fact is that there is no tuning parameter in our algorithm. Under the
assumption of the theorem, minimizing
â€–Lâ€–âˆ— +
1
âˆš
n(1)
â€–Sâ€–1, n(1) = max(n1, n2)
always returns the correct answer. This is surprising because one might have expected that one
would have to choose the right scalar Î» to balance the two terms in â€–Lâ€–âˆ— + Î»â€–Sâ€–1 appropriately
(perhaps depending on their relative size). This is, however, clearly not the case. In this sense, the
choice Î» = 1/âˆšn(1) is universal. Further, it is not a priori very clear why Î» = 1/
âˆš
n(1) is a correct
choice no matter what L0 and S0 are. It is the mathematical analysis which reveals the correctness
of this value. In fact, the proof of the theorem gives a whole range of correct values, and we have
selected a sufficiently simple value in that range.
Another comment is that one can obtain results with larger probabilities of success, i.e. of the
form 1âˆ’O(nâˆ’Î²) (or 1âˆ’O(nâˆ’Î²(1) )) for Î² > 0 at the expense of reducing the value of Ïr.
1.5 Connections with prior work and innovations
The last year or two have seen the rapid development of a scientific literature concerned with the
matrix completion problem introduced in [8], see also [7,10,22,23,43] and the references therein. In
a nutshell, the matrix completion problem is that of recovering a low-rank matrix from only a small
fraction of its entries, and by extension, from a small number of linear functionals. Although other
methods have been proposed [43], the method of choice is to use convex optimization [7,10,22,23,45]:
among all the matrices consistent with the data, simply find that with minimum nuclear norm.
The papers cited above all prove the mathematical validity of this approach, and our mathematical
analysis borrows ideas from this literature, and especially from those pioneered in [8]. Our methods
also much rely on the powerful ideas and elegant techniques introduced by David Gross in the
context of quantum-state tomography [22,23]. In particular, the clever golfing scheme [22] plays a
crucial role in our analysis, and we introduce two novel modifications to this scheme.
Despite these similarities, our ideas depart from the literature on matrix completion on several
fronts. First, our results obviously are of a different nature. Second, we could think of our sep-
aration problem, and the recovery of the low-rank component, as a matrix completion problem.
Indeed, instead of having a fraction of observed entries available and the other missing, we have
a fraction available, but do not know which one, while the other is not missing but entirely cor-
rupted altogether. Although, this is a harder problem, one way to think of our algorithm is that
it simultaneously detects the corrupted entries, and perfectly fits the low-rank component to the
remaining entries that are deemed reliable. In this sense, our methodology and results go beyond
matrix completion. Third, we introduce a novel de-randomization argument that allows us to fix
the signs of the nonzero entries of the sparse component. We believe that this technique will have
many applications. One such application is in the area of compressive sensing, where assumptions
about the randomness of the signs of a signal are common, and merely made out of convenience
rather than necessity; this is important because assuming independent signal signs may not make
much sense for many practical applications when the involved signals can all be non-negative (such
as images).
6
We mentioned earlier the related work [12], which also considers the problem of decomposing a
given data matrix into sparse and low-rank components, and gives sufficient conditions for convex
programming to succeed. These conditions are phrased in terms of two quantities. The first is the
maximum ratio between the `âˆ norm and the operator norm, restricted to the subspace generated
by matrices whose row or column spaces agree with those of L0. The second is the maximum ratio
between the operator norm and the `âˆ norm, restricted to the subspace of matrices that vanish
off the support of S0. Chandrasekaran et. al. show that when the product of these two quantities
is small, then the recovery is exact for a certain interval of the regularization parameter [12].
One very appealing aspect of this condition is that it is completely deterministic: it does not
depend on any random model for L0 or S0. It yields a corollary that can be easily compared to
our result: suppose n1 = n2 = n for simplicity, and let Âµ0 be the smallest quantity satisfying (1.2),
then correct recovery occurs whenever
max
j
{i : [S0]ij 6= 0} Ã—
âˆš
Âµ0r/n < 1/12.
The left-hand side is at least as large as Ïs
âˆš
Âµ0nr, where Ïs is the fraction of entries of S0 that are
nonzero. Since Âµ0 â‰¥ 1 always, this statement only guarantees recovery if Ïs = O((nr)âˆ’1/2); i.e.,
even when rank(L0) = O(1), only vanishing fractions of the entries in S0 can be nonzero.
In contrast, our result shows that for incoherent L0, correct recovery occurs with high probability
for rank(L0) on the order of n/[Âµ log2 n] and a number of nonzero entries in S0 on the order of n2.
That is, matrices of large rank can be recovered from non-vanishing fractions of sparse errors. This
improvement comes at the expense of introducing one piece of randomness: a uniform model on
the error support.6
Our analysis has one additional advantage, which is of significant practical importance: it iden-
tifies a simple, non-adaptive choice of the regularization parameter Î». In contrast, the conditions
on the regularization parameter given by Chandrasekaran et al. depend on quantities which in
practice are not known a-priori. The experimental section of [12] suggests searching for the correct
Î» by solving many convex programs. Our result, on the other hand, demonstrates that the simple
choice Î» = 1/
âˆš
n works with high probability for recovering any square incoherent matrix.
1.6 Implications for matrix completion from grossly corrupted data
We have seen that our main result asserts that it is possible to recover a low-rank matrix even
though a significant fraction of its entries are corrupted. In some applications, however, some
of the entries may be missing as well, and this section addresses this situation. Let Pâ„¦ be the
orthogonal projection onto the linear space of matrices supported on â„¦ âŠ‚ [n1]Ã— [n2],
Pâ„¦X =
{
Xij , (i, j) âˆˆ â„¦,
0, (i, j) /âˆˆ â„¦.
Then imagine we only have available a few entries of L0 + S0, which we conveniently write as
Y = Pâ„¦obs(L0 + S0) = Pâ„¦obsL0 + S
â€²
0;
6Notice that the bound of [12] depends only on the support of S0, and hence can be interpreted as a worst case
result with respect to the signs of S0. In contrast, our result does not randomize over the signs, but does assume that
they are sampled from a fixed sign pattern. Although we do not pursue it here due to space limitations, our analysis
also yields a result which holds for worst case sign patterns, and guarantees correct recovery with rank(L0) = O(1),
and a sparsity pattern of cardinality Ïn1n2 for some Ï > 0.
7
that is, we see only those entries (i, j) âˆˆ â„¦obs âŠ‚ [n1]Ã— [n2]. This models the following problem: we
wish to recover L0 but only see a few entries about L0, and among those a fraction happens to be
corrupted, and we of course do not know which one. As is easily seen, this is a significant extension
of the matrix completion problem, which seeks to recover L0 from undersampled but otherwise
perfect data Pâ„¦obsL0.
We propose recovering L0 by solving the following problem:
minimize â€–Lâ€–âˆ— + Î»â€–Sâ€–1
subject to Pâ„¦obs(L+ S) = Y.
(1.5)
In words, among all decompositions matching the available data, Principal Component Pursuit
finds the one that minimizes the weighted combination of the nuclear norm, and of the `1 norm. Our
observation is that under some conditions, this simple approach recovers the low-rank component
exactly. In fact, the techniques developed in this paper establish this result:
Theorem 1.2 Suppose L0 is n Ã— n, obeys the conditions (1.2)â€“(1.3), and that â„¦obs is uniformly
distributed among all sets of cardinality m obeying m = 0.1n2. Suppose for simplicity, that each
observed entry is corrupted with probability Ï„ independently of the others. Then there is a numerical
constant c such that with probability at least 1 âˆ’ cnâˆ’10, Principal Component Pursuit (1.5) with
Î» = 1/
âˆš
0.1n is exact, i.e. LÌ‚ = L0, provided that
rank(L0) â‰¤ Ïr nÂµâˆ’1(log n)âˆ’2, and Ï„ â‰¤ Ï„s. (1.6)
Above, Ïr and Ï„s are positive numerical constants. For general n1 Ã— n2 rectangular matrices, PCP
with Î» = 1/
âˆš
0.1n(1) succeeds from m = 0.1n1n2 corrupted entries with probability at least 1âˆ’cnâˆ’10(1) ,
provided that rank(L0) â‰¤ Ïr n(2)Âµâˆ’1(log n(1))âˆ’2.
In short, perfect recovery from incomplete and corrupted entries is possible by convex optimization.
On the one hand, this result extends our previous result in the following way. If all the entries
are available, i.e. m = n1n2, then this is Theorem 1.1. On the other hand, it extends matrix
completion results. Indeed, if Ï„ = 0, we have a pure matrix completion problem from about a
fraction of the total number of entries, and our theorem guarantees perfect recovery as long as r
obeys (1.6), which for large values of r, matches the strongest results available. We remark that
the recovery is exact, however, via a different algorithm. To be sure, in matrix completion one
typically minimizes the nuclear norm â€–Lâ€–âˆ— subject to the constraint Pâ„¦obsL = Pâ„¦obsL0. Here, our
program would solve
minimize â€–Lâ€–âˆ— + Î»â€–Sâ€–1
subject to Pâ„¦obs(L+ S) = Pâ„¦obsL0,
(1.7)
and return LÌ‚ = L0, SÌ‚ = 0! In this context, Theorem 1.2 proves that matrix completion is stable
vis a vis gross errors.
Remark. We have stated Theorem 1.2 merely to explain how our ideas can easily be adapted
to deal with low-rank matrix recovery problems from undersampled and possibly grossly corrupted
data. In our statement, we have chosen to see 10% of the entries but, naturally, similar results
hold for all other positive fractions provided that they are large enough. We would like to make it
clear that a more careful study is likely to lead to a stronger version of Theorem 1.2. In particular,
for very low rank matrices, we expect to see similar results holding with far fewer observations;
8
that is, in the limit of large matrices, from a decreasing fraction of entries. In fact, our techniques
would already establish such sharper results but we prefer not to dwell on such refinements at the
moment, and leave this up for future work.
1.7 Notation
We provide a brief summary of the notations used throughout the paper. We shall use five norms
of a matrix. The first three are functions of the singular values and they are: 1) the operator norm
or 2-norm denoted by â€–Xâ€–; 2) the Frobenius norm denoted by â€–Xâ€–F ; and 3) the nuclear norm
denoted by â€–Xâ€–âˆ—. The last two are the `1 and `âˆ norms of a matrix seen as a long vector, and
are denoted by â€–Xâ€–1 and â€–Xâ€–âˆ respectively. The Euclidean inner product between two matrices
is defined by the formula ã€ˆX,Y ã€‰ := trace(Xâˆ—Y ), so that â€–Xâ€–2F = ã€ˆX,Xã€‰.
Further, we will also manipulate linear transformations which act on the space of matrices, and
we will use calligraphic letters for these operators as in Pâ„¦X. We shall also abuse notation by also
letting â„¦ be the linear space of matrices supported on â„¦. Then Pâ„¦âŠ¥ denotes the projection onto
the space of matrices supported on â„¦c so that I = Pâ„¦ +Pâ„¦âŠ¥ , where I is the identity operator. We
will consider a single norm for these, namely, the operator norm (the top singular value) denoted
by â€–Aâ€–, which we may want to think of as â€–Aâ€– = sup{â€–Xâ€–F=1} â€–AXâ€–F ; for instance, â€–Pâ„¦â€– = 1
whenever â„¦ 6= âˆ….
1.8 Organization of the paper
The paper is organized as follows. In Section 2, we provide the key steps in the proof of Theorem
1.1. This proof depends upon on two critical properties of dual certificates, which are established in
the separate Section 3. The reason why this is separate is that in a first reading, the reader might
want to jump to Section 4, which presents applications to video surveillance, and computer vision.
Section 5 introduces algorithmic ideas to find the Principal Component Pursuit solution when M
is of very large scale. We conclude the paper with a discussion about future research directions in
Section 6. Finally, the proof of Theorem 1.2 is in the Appendix, Section 7, together with those of
intermediate results.
2 Architecture of the Proof
This section introduces the key steps underlying the proof of our main result, Theorem 1.1. We
will prove the result for square matrices for simplicity, and write n = n1 = n2. Of course, we shall
indicate where the argument needs to be modified to handle the general case. Before we start,
it is helpful to review some basic concepts and introduce additional notation that shall be used
throughout. For a given scalar x, we denote by sgn(x) the sign of x, which we take to be zero if
x = 0. By extension, sgn(S) is the matrix whose entries are the signs of those of S. We recall that
any subgradient of the `1 norm at S0 supported on â„¦, is of the form
sgn(S0) + F,
where F vanishes on â„¦, i.e. Pâ„¦F = 0, and obeys â€–Fâ€–âˆ â‰¤ 1.
We will also manipulate the set of subgradients of the nuclear norm. From now on, we will
assume that L0 of rank r has the singular value decomposition UÎ£V âˆ—, where U, V âˆˆ RnÃ—r just as
9
in Section 1.3. Then any subgradient of the nuclear norm at L0 is of the form
UV âˆ— +W,
where Uâˆ—W = 0, WV = 0 and â€–Wâ€– â‰¤ 1. Denote by T the linear space of matrices
T := {UXâˆ— + Y V âˆ—, X, Y âˆˆ RnÃ—r}, (2.1)
and by TâŠ¥ its orthogonal complement. It is not hard to see that taken together, Uâˆ—W = 0 and
WV = 0 are equivalent to PTW = 0, where PT is the orthogonal projection onto T . Another way to
put this is PTâŠ¥W = W . In passing, note that for any matrix M , PTâŠ¥M = (I âˆ’UUâˆ—)M(I âˆ’V V âˆ—),
where we recognize that I âˆ’ UUâˆ— is the projection onto the orthogonal complement of the linear
space spanned by the columns of U and likewise for (I âˆ’ V V âˆ—). A consequence of this simple
observation is that for any matrix M , â€–PTâŠ¥Mâ€– â‰¤ â€–Mâ€–, a fact that we will use several times in the
sequel. Another consequence is that for any matrix of the form eieâˆ—j ,
â€–PTâŠ¥eieâˆ—jâ€–2F = â€–(I âˆ’ UUâˆ—)eiâ€–2â€–(I âˆ’ V V âˆ—)ejâ€–2 â‰¥ (1âˆ’ Âµr/n)2,
where we have assumed Âµr/n â‰¤ 1. Since â€–PT eieâˆ—jâ€–2F + â€–PTâŠ¥eieâˆ—jâ€–2F = 1, this gives
â€–PT eieâˆ—jâ€–F â‰¤
âˆš
2Âµr
n
. (2.2)
For rectangular matrices, the estimate is â€–PT eieâˆ—jâ€–F â‰¤
âˆš
2Âµr
min(n1,n2)
.
Finally, in the sequel we will write that an event holds with high or large probability whenever
it holds with probability at least 1âˆ’O(nâˆ’10) (with n(1) in place of n for rectangular matrices).
2.1 An elimination theorem
We begin with a useful definition and an elementary result we shall use a few times.
Definition 2.1 We will say that Sâ€² is a trimmed version of S if supp(Sâ€²) âŠ‚ supp(S) and Sâ€²ij = Sij
whenever Sâ€²ij 6= 0.
In words, a trimmed version of S is obtained by setting some of the entries of S to zero. Having said
this, the following intuitive theorem asserts that if Principal Component Pursuit correctly recovers
the low-rank and sparse components of M0 = L0 + S0, it also correctly recovers the components of
a matrix M â€²0 = L0 + S
â€²
0 where S
â€²
0 is a trimmed version of S0. This is intuitive since the problem is
somehow easier as there are fewer things to recover.
Theorem 2.2 Suppose the solution to (1.1) with input data M0 = L0 + S0 is unique and exact,
and consider M â€²0 = L0 + S
â€²
0, where S
â€²
0 is a trimmed version of S0. Then the solution to (1.1) with
input M â€²0 is exact as well.
Proof Write Sâ€²0 = Pâ„¦0S0 for some â„¦0 âŠ‚ [n]Ã— [n] and let (LÌ‚, SÌ‚) be the solution of (1.1) with input
L0 + Sâ€²0. Then
â€–LÌ‚â€–âˆ— + Î»â€–SÌ‚â€–1 â‰¤ â€–L0â€–âˆ— + Î»â€–Pâ„¦0S0â€–1
and, therefore,
â€–LÌ‚â€–âˆ— + Î»â€–SÌ‚â€–1 + Î»â€–Pâ„¦âŠ¥0 S0â€–1 â‰¤ â€–L0â€–âˆ— + Î»â€–S0â€–1.
10
Note that (LÌ‚, SÌ‚ + Pâ„¦âŠ¥0 S0) is feasible for the problem with input data L0 + S0, and since â€–SÌ‚ +
Pâ„¦âŠ¥0 S0â€–1 â‰¤ â€–SÌ‚â€–1 + â€–Pâ„¦âŠ¥0 S0â€–1, we have
â€–LÌ‚â€–âˆ— + Î»â€–SÌ‚ + Pâ„¦âŠ¥0 S0â€–1 â‰¤ â€–L0â€–âˆ— + Î»â€–S0â€–1.
The right-hand side, however, is the optimal value, and by unicity of the optimal solution, we must
have LÌ‚ = L0, and SÌ‚ + Pâ„¦âŠ¥0 S0 = S0 or SÌ‚ = Pâ„¦0S0 = S
â€²
0. This proves the claim.
The Bernoulli model. In Theorem 1.1, probability is taken with respect to the uniformly
random subset â„¦ = {(i, j) : Sij 6= 0} of cardinality m. In practice, it is a little more convenient to
work with the Bernoulli model â„¦ = {(i, j) : Î´ij = 1}, where the Î´ij â€™s are i.i.d. variables Bernoulli
taking value one with probability Ï and zero with probability 1âˆ’Ï, so that the expected cardinality
of â„¦ is Ïn2. From now on, we will write â„¦ âˆ¼ Ber(Ï) as a shorthand for â„¦ is sampled from the
Bernoulli model with parameter Ï.
Since by Theorem 2.2, the success of the algorithm is monotone in |â„¦|, any guarantee proved for
the Bernoulli model holds for the uniform model as well, and vice versa, if we allow for a vanishing
shift in Ï around m/n2. The arguments underlying this equivalence are standard, see [9, 10], and
may be found in the Appendix for completeness.
2.2 Derandomization
In Theorem 1.1, the values of the nonzero entries of S0 are fixed. It turns out that it is easier
to prove the theorem under a stronger assumption, which assumes that the signs of the nonzero
entries are independent symmetric Bernoulli variables, i.e. take the value Â±1 with probability
1/2 (independently of the choice of the support set). The convenient theorem below shows that
establishing the result for random signs is sufficient to claim a similar result for fixed signs.
Theorem 2.3 Suppose L0 obeys the conditions of Theorem 1.1 and that the locations of the nonzero
entries of S0 follow the Bernoulli model with parameter 2Ïs, and the signs of S0 are i.i.d. Â±1 as
above (and independent from the locations). Then if the PCP solution is exact with high probability,
then it is also exact with at least the same probability for the model in which the signs are fixed and
the locations are sampled from the Bernoulli model with parameter Ïs.
This theorem is convenient because to prove our main result, we only need to show that it is true
in the case where the signs of the sparse component are random.
Proof Consider the model in which the signs are fixed. In this model, it is convenient to think of
S0 as Pâ„¦S, for some fixed matrix S, where â„¦ is sampled from the Bernoulli model with parameter
Ïs. Therefore, S0 has independent components distributed as
(S0)ij =
{
Sij , w. p. Ïs,
0, w. p. 1âˆ’ Ïs.
Consider now a random sign matrix with i.i.d. entries distributed as
Eij =
ï£±ï£´ï£²ï£´ï£³
1, w. p. Ïs,
0, w. p. 1âˆ’ 2Ïs,
âˆ’1, w. p. Ïs,
11
and an â€œeliminationâ€ matrix âˆ† with entries defined by
âˆ†ij =
{
0, if Eij [sgn(S)]ij = âˆ’1,
1, otherwise.
Note that the entries of âˆ† are independent since they are functions of independent variables.
Consider now Sâ€²0 = âˆ† â—¦ (|S| â—¦E), where â—¦ denotes the Hadamard or componentwise product so
that, [Sâ€²0]ij = âˆ†ij (|Sij |Eij). Then we claim that Sâ€²0 and S0 have the same distribution. To see why
this is true, it suffices by independence to check that the marginals match. For Sij 6= 0, we have
P([Sâ€²0]ij = Sij) = P(âˆ†ij = 1 and Eij = [sgn(S)]ij)
= P(Eij [sgn(S)]ij 6= âˆ’1 and Eij = [sgn(S)]ij)
= P(Eij = [sgn(S)]ij) = Ïs,
which establishes the claim.
This construction allows to prove the theorem. Indeed, |S|â—¦E now obeys the random sign model,
and by assumption, PCP recovers |S| â—¦ E with high probability. By the elimination theorem, this
program also recovers Sâ€²0 = âˆ† â—¦ (|S| â—¦E). Since Sâ€²0 and S0 have the same distribution, the theorem
follows.
2.3 Dual certificates
We introduce a simple condition for the pair (L0, S0) to be the unique optimal solution to Principal
Component Pursuit. These conditions are stated in terms of a dual vector, the existence of which
certifies optimality. (Recall that â„¦ is the space of matrices with the same support as the sparse
component S0, and that T is the space defined via the the column and row spaces of the low-rank
component L0 (2.1).)
Lemma 2.4 Assume that â€–Pâ„¦PT â€– < 1. With the standard notations, (L0, S0) is the unique solu-
tion if there is a pair (W,F ) obeying
UV âˆ— +W = Î»(sgn(S0) + F ),
with PTW = 0, â€–Wâ€– < 1, Pâ„¦F = 0 and â€–Fâ€–âˆ < 1.
Note that the condition â€–Pâ„¦PT â€– < 1 is equivalent to saying that â„¦ âˆ© T = {0}.
Proof We consider a feasible perturbation (L0 +H,S0âˆ’H) and show that the objective increases
whenever H 6= 0, hence proving that (L0, S0) is the unique solution. To do this, let UV âˆ— +W0 be
an arbitrary subgradient of the nuclear norm at L0, and sgn(S0) + F0 be an arbitrary subgradient
of the `1-norm at S0. By definition of subgradients,
â€–L0 +Hâ€–âˆ— + Î»â€–S0 âˆ’Hâ€–1 â‰¥ â€–L0â€–âˆ— + Î»â€–S0â€–1 + ã€ˆUV âˆ— +W0, Hã€‰ âˆ’ Î»ã€ˆsgn(S0) + F0, Hã€‰.
Now pick W0 such that ã€ˆW0, Hã€‰ = â€–PTâŠ¥Hâ€–âˆ— and F0 such that ã€ˆF0, Hã€‰ = âˆ’â€–Pâ„¦âŠ¥Hâ€–1.7 We have
â€–L0 +Hâ€–âˆ— + Î»â€–S0 âˆ’Hâ€–1 â‰¥ â€–L0â€–âˆ— + Î»â€–S0â€–1 + â€–PTâŠ¥Hâ€–âˆ— + Î»â€–Pâ„¦âŠ¥Hâ€–1 + ã€ˆUV âˆ— âˆ’ Î»sgn(S0), Hã€‰.
7For instance, F0 = âˆ’sgn(Pâ„¦âŠ¥H) is such a matrix. Also, by duality between the nuclear and the operator norm,
there is a matrix obeying â€–Wâ€– = 1 such that ã€ˆW,PTâŠ¥Hã€‰ = â€–PTâŠ¥Hâ€–âˆ—, and we just take W0 = PTâŠ¥(W ).
12
By assumption
|ã€ˆUV âˆ— âˆ’ Î»sgn(S0), Hã€‰| â‰¤ |ã€ˆW,Hã€‰|+ Î»|ã€ˆF,Hã€‰| â‰¤ Î²(â€–PTâŠ¥Hâ€–âˆ— + Î»â€–Pâ„¦âŠ¥Hâ€–1)
for Î² = max(â€–Wâ€–, â€–Fâ€–âˆ) < 1 and, thus,
â€–L0 +Hâ€–âˆ— + Î»â€–S0 âˆ’Hâ€–1 â‰¥ â€–L0â€–âˆ— + Î»â€–S0â€–1 + (1âˆ’ Î²)
(
â€–PTâŠ¥Hâ€–âˆ— + Î»â€–Pâ„¦âŠ¥Hâ€–1
)
.
Since by assumption, â„¦ âˆ© T = {0}, we have â€–PTâŠ¥Hâ€–âˆ— + Î»â€–Pâ„¦âŠ¥Hâ€–1 > 0 unless H = 0.
Hence, we see that to prove exact recovery, it is sufficient to produce a â€˜dual certificateâ€™ W
obeying ï£±ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£³
W âˆˆ TâŠ¥,
â€–Wâ€– < 1,
Pâ„¦(UV âˆ— +W ) = Î»sgn(S0),
â€–Pâ„¦âŠ¥(UV âˆ— +W )â€–âˆ < Î».
(2.3)
Our method, however, will produce with high probability a slightly different certificate. The idea
is to slightly relax the constraint Pâ„¦(UV âˆ—+W ) = Î»sgn(S0), a relaxation that has been introduced
by David Gross in [22] in a different context. We prove the following lemma.
Lemma 2.5 Assume â€–Pâ„¦PT â€– â‰¤ 1/2 and Î» < 1. Then with the same notation, (L0, S0) is the
unique solution if there is a pair (W,F ) obeying
UV âˆ— +W = Î»(sgn(S0) + F + Pâ„¦D)
with PTW = 0 and â€–Wâ€– â‰¤ 12 , Pâ„¦F = 0 and â€–Fâ€–âˆ â‰¤
1
2 , and â€–Pâ„¦Dâ€–F â‰¤
1
4 .
Proof Following the proof of Lemma 2.4, we have
â€–L0 +Hâ€–âˆ— + Î»â€–S0 âˆ’Hâ€–1 â‰¥ â€–L0â€–âˆ— + Î»â€–S0â€–1 +
1
2
(
â€–PTâŠ¥Hâ€–âˆ— + Î»â€–Pâ„¦âŠ¥Hâ€–1
)
âˆ’ Î»ã€ˆPâ„¦D,Hã€‰
â‰¥ â€–L0â€–âˆ— + Î»â€–S0â€–1 +
1
2
(
â€–PTâŠ¥Hâ€–âˆ— + Î»â€–Pâ„¦âŠ¥Hâ€–1
)
âˆ’ Î»
4
â€–Pâ„¦Hâ€–F .
Observe now that
â€–Pâ„¦Hâ€–F â‰¤ â€–Pâ„¦PTHâ€–F + â€–Pâ„¦PTâŠ¥Hâ€–F
â‰¤ 1
2
â€–Hâ€–F + â€–PTâŠ¥Hâ€–F
â‰¤ 1
2
â€–Pâ„¦Hâ€–F +
1
2
â€–Pâ„¦âŠ¥Hâ€–F + â€–PTâŠ¥Hâ€–F
and, therefore,
â€–Pâ„¦Hâ€–F â‰¤ â€–Pâ„¦âŠ¥Hâ€–F + 2â€–PTâŠ¥Hâ€–F .
In conclusion,
â€–L0 +Hâ€–âˆ— + Î»â€–S0 âˆ’Hâ€–1 â‰¥ â€–L0â€–âˆ— + Î»â€–S0â€–1 +
1
2
(
(1âˆ’ Î»)â€–PTâŠ¥Hâ€–âˆ— +
Î»
2
â€–Pâ„¦âŠ¥Hâ€–1
)
,
13
and the term between parenthesis is strictly positive when H 6= 0.
As a consequence of Lemma 2.5, it now suffices to produce a dual certificate W obeyingï£±ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£³
W âˆˆ TâŠ¥,
â€–Wâ€– < 1/2,
â€–Pâ„¦(UV âˆ— âˆ’ Î»sgn(S0) +W )â€–F â‰¤ Î»/4,
â€–Pâ„¦âŠ¥(UV âˆ— +W )â€–âˆ < Î»/2.
(2.4)
Further, we would like to note that the existing literature on matrix completion [8] gives good
bounds on â€–Pâ„¦PT â€–, see Theorem 2.6 in Section 2.5.
2.4 Dual certification via the golfing scheme
In the papers [22, 23], Gross introduces a new scheme, termed the golfing scheme, to construct a
dual certificate for the matrix completion problem, i.e. the problem of reconstructing a low-rank
matrix from a subset of its entries. In this section, we will adapt this clever golfing scheme, with
two important modifications, to our separation problem.
Before we introduce our construction, our model assumes that â„¦ âˆ¼ Ber(Ï), or equivalently that
â„¦c âˆ¼ Ber(1âˆ’ Ï). Now the distribution of â„¦c is the same as that of â„¦c = â„¦1 âˆªâ„¦2 âˆª . . .âˆªâ„¦j0 , where
each â„¦j follows the Bernoulli model with parameter q, which has an explicit expression. To see
this, observe that by independence, we just need to make sure that any entry (i, j) is selected with
the right probability. We have
P((i, j) âˆˆ â„¦) = P(Bin(j0, q) = 0) = (1âˆ’ q)j0 ,
so that the two models are the same if
Ï = (1âˆ’ q)j0 ,
hence justifying our assertion. Note that because of overlaps between the â„¦j â€™s, q â‰¥ (1âˆ’ Ï)/j0.
We now propose constructing a dual certificate
W = WL +WS ,
where each component is as follows:
1. Construction of WL via the golfing scheme. Fix an integer j0 â‰¥ 1 whose value shall be
discussed later, and let â„¦j , 1 â‰¤ j â‰¤ j0, be defined as above so that â„¦c = âˆª1â‰¤jâ‰¤j0â„¦j . Then
starting with Y0 = 0, inductively define
Yj = Yjâˆ’1 + qâˆ’1Pâ„¦jPT (UV âˆ— âˆ’ Yjâˆ’1),
and set
WL = PTâŠ¥Yj0 . (2.5)
This is a variation on the golfing scheme discussed in [22], which assumes that the â„¦j â€™s
are sampled with replacement, and does not use the projector Pâ„¦j but something more
complicated taking into account the number of times a specific entry has been sampled.
14
2. Construction of WS via the method of least squares. Assume that â€–Pâ„¦PT â€– < 1/2. Then
â€–Pâ„¦PTPâ„¦â€– < 1/4 and, thus, the operator Pâ„¦ âˆ’Pâ„¦PTPâ„¦ mapping â„¦ onto itself is invertible;
we denote its inverse by (Pâ„¦ âˆ’ Pâ„¦PTPâ„¦)âˆ’1. We then set
WS = Î»PTâŠ¥(Pâ„¦ âˆ’ Pâ„¦PTPâ„¦)âˆ’1sgn(S0). (2.6)
Clearly, an equivalent definition is via the convergent Neumann series
WS = Î»PTâŠ¥
âˆ‘
kâ‰¥0
(Pâ„¦PTPâ„¦)ksgn(S0). (2.7)
Note that Pâ„¦WS = Î»Pâ„¦(I âˆ’ PT )(Pâ„¦ âˆ’ Pâ„¦PTPâ„¦)âˆ’1sgn(S0) = Î»sgn(S0). With this, the
construction has a natural interpretation: one can verify that among all matrices W âˆˆ TâŠ¥
obeying Pâ„¦W = Î»sgn(S0), WS is that with minimum Frobenius norm.
Since both WL and WS belong to TâŠ¥ and Pâ„¦WS = Î»sgn(S0), we will establish that WL +WS is
a valid dual certificate if it obeysï£±ï£´ï£²ï£´ï£³
â€–WL +WSâ€– < 1/2,
â€–Pâ„¦(UV âˆ— +WL)â€–F â‰¤ Î»/4,
â€–Pâ„¦âŠ¥(UV âˆ— +WL +WS)â€–âˆ < Î»/2.
(2.8)
2.5 Key lemmas
We now state three lemmas, which taken collectively, establish our main theorem. The first may
be found in [8].
Theorem 2.6 [8, Theorem 4.1] Suppose â„¦0 is sampled from the Bernoulli model with parameter
Ï0. Then with high probability,
â€–PT âˆ’ Ïâˆ’10 PTPâ„¦0PT â€– â‰¤ , (2.9)
provided that Ï0 â‰¥ C0 âˆ’2 Âµr lognn for some numerical constant C0 > 0 (Âµ is the incoherence param-
eter). For rectangular matrices, we need Ï0 â‰¥ C0 âˆ’2
Âµr logn(1)
n(2)
.
Among other things, this lemma is important because it shows that â€–Pâ„¦PT â€– â‰¤ 1/2, provided
|â„¦| is not too large. Indeed, if â„¦ âˆ¼ Ber(Ï), we have
â€–PT âˆ’ (1âˆ’ Ï)âˆ’1PTPâ„¦âŠ¥PT â€– â‰¤ ,
with the proviso that 1âˆ’ Ï â‰¥ C0 âˆ’2 Âµr lognn . Note, however, that since I = Pâ„¦ + Pâ„¦âŠ¥ ,
PT âˆ’ (1âˆ’ Ï)âˆ’1PTPâ„¦âŠ¥PT = (1âˆ’ Ï)âˆ’1(PTPâ„¦PT âˆ’ ÏPT )
and, therefore, by the triangular inequality
â€–PTPâ„¦PT â€– â‰¤ (1âˆ’ Ï) + Ïâ€–PT â€– = Ï+ (1âˆ’ Ï).
Since â€–Pâ„¦PT â€–2 = â€–PTPâ„¦PT â€–, we have established the following:
15
Corollary 2.7 Assume that â„¦ âˆ¼ Ber(Ï), then â€–Pâ„¦PT â€–2 â‰¤ Ï+, provided that 1âˆ’Ï â‰¥ C0 âˆ’2 Âµr lognn ,
where C0 is as in Theorem 2.6. For rectangular matrices, the modification is as in Theorem 2.6.
The lemma below is proved is Section 3.
Lemma 2.8 Assume that â„¦ âˆ¼ Ber(Ï) with parameter Ï â‰¤ Ïs for some Ïs > 0. Set j0 = 2dlog ne
(use log n(1) for rectangular matrices). Then under the other assumptions of Theorem 1.1, the
matrix WL (2.5) obeys
(a) â€–WLâ€– < 1/4,
(b) â€–Pâ„¦(UV âˆ— +WL)â€–F < Î»/4,
(c) â€–Pâ„¦âŠ¥(UV âˆ— +WL)â€–âˆ < Î»/4.
Since â€–Pâ„¦PT â€– < 1 with large probability, WS is well defined and the following holds.
Lemma 2.9 Assume that S0 is supported on a set â„¦ sampled as in Lemma 2.8, and that the signs
of S0 are i.i.d. symmetric (and independent of â„¦). Then under the other assumptions of Theorem
1.1, the matrix WS (2.6) obeys
(a) â€–WSâ€– < 1/4,
(b) â€–Pâ„¦âŠ¥WSâ€–âˆ < Î»/4.
The proof is also in Section 3. Clearly, WL and WS obey (2.8), hence certifying that Principal
Component Pursuit correctly recovers the low-rank and sparse components with high probabil-
ity when the signs of S0 are random. The earlier â€œderandomizationâ€ argument then establishes
Theorem 1.1.
3 Proofs of Dual Certification
This section proves the two crucial estimates, namely, Lemma 2.8 and Lemma 2.9.
3.1 Preliminaries
We begin by recording two results which shall be useful in proving Lemma 2.8. While Theorem 2.6
asserts that with large probability,
â€–Z âˆ’ Ïâˆ’10 PTPâ„¦0Zâ€–F â‰¤ â€–Zâ€–F ,
for all Z âˆˆ T , the next lemma shows that for a fixed Z, the sup-norm of Z âˆ’ Ïâˆ’10 PTPâ„¦0(Z) also
does not increase (also with large probability).
Lemma 3.1 Suppose Z âˆˆ T is a fixed matrix, and â„¦0 âˆ¼ Ber(Ï0). Then with high probability,
â€–Z âˆ’ Ïâˆ’10 PTPâ„¦0Zâ€–âˆ â‰¤ â€–Zâ€–âˆ (3.1)
provided that Ï0 â‰¥ C0 âˆ’2 Âµr lognn (for rectangular matrices, Ï0 â‰¥ C0 
âˆ’2 Âµr logn(1)
n(2)
) for some numer-
ical constant C0 > 0.
16
The proof is an application of Bernsteinâ€™s inequality and may be found in the Appendix. A similar
but somewhat different version of (3.1) appears in [44].
The second result was proved in [8].
Lemma 3.2 [8, Theorem 6.3] Suppose Z is fixed, and â„¦0 âˆ¼ Ber(Ï0). Then with high probability,
â€–(I âˆ’ Ïâˆ’10 Pâ„¦0)Zâ€– â‰¤ C
â€²
0
âˆš
n log n
Ï0
â€–Zâ€–âˆ (3.2)
for some small numerical constant C â€²0 > 0 provided that Ï0 â‰¥ C0
Âµ logn
n (or Ï0 â‰¥ C
â€²
0
Âµ logn(1)
n(2)
for
rectangular matrices in which case n(1) log n(1) replaces n log n in (3.2)).
As a remark, Lemmas 3.1 and 3.2, and Theorem 2.6 all hold with probability at least 1âˆ’O(nâˆ’Î²),
Î² > 2, if C0 is replaced by CÎ² for some numerical constant C > 0.
3.2 Proof of Lemma 2.8
We begin by introducing a piece of notation and set Zj = UV âˆ— âˆ’ PTYj obeying
Zj = (PT âˆ’ qâˆ’1PTPâ„¦jPT )Zjâˆ’1.
Obviously Zj âˆˆ T for all j â‰¥ 0. First, note that when
q â‰¥ C0 âˆ’2
Âµr log n
n
, (3.3)
(for rectangular matrices, take q â‰¥ C0 âˆ’2
Âµr logn(1)
n(2)
), we have
â€–Zjâ€–âˆ â‰¤ â€–Zjâˆ’1â€–âˆ (3.4)
by Lemma 3.1. (This holds with high probability because â„¦j and Zjâˆ’1 are independent, and this
is why the golfing scheme is easy to use.) In particular, this gives that with high probability
â€–Zjâ€–âˆ â‰¤ jâ€–UV âˆ—â€–âˆ.
When q obeys the same estimate,
â€–Zjâ€–F â‰¤ â€–Zjâˆ’1â€–F (3.5)
by Theorem 2.6. In particular, this gives that with high probability
â€–Zjâ€–F â‰¤ jâ€–UV âˆ—â€–F = j
âˆš
r. (3.6)
Below, we will assume  â‰¤ eâˆ’1.
17
Proof of (a). We prove the first part of the lemma and the argument parallels that in [22], see
also [44]. From
Yj0 =
âˆ‘
j
qâˆ’1Pâ„¦jZjâˆ’1,
we deduce
â€–WLâ€– = â€–PTâŠ¥Yj0â€–âˆ â‰¤
âˆ‘
j
â€–qâˆ’1PTâŠ¥Pâ„¦jZjâˆ’1â€–
=
âˆ‘
j
â€–PTâŠ¥(qâˆ’1Pâ„¦jZjâˆ’1 âˆ’ Zjâˆ’1)â€–
â‰¤
âˆ‘
j
â€–qâˆ’1Pâ„¦jZjâˆ’1 âˆ’ Zjâˆ’1â€–
â‰¤ C â€²0
âˆš
n log n
q
âˆ‘
j
â€–Zjâˆ’1â€–âˆ
â‰¤ C â€²0
âˆš
n log n
q
âˆ‘
j
jâˆ’1â€–UV âˆ—â€–âˆ
â‰¤ C â€²0(1âˆ’ )âˆ’1
âˆš
n log n
q
â€–UV âˆ—â€–âˆ.
The fourth step follows from Lemma 3.2 and the fifth from (3.5). Since â€–UV âˆ—â€– â‰¤ âˆšÂµr/n, this gives
â€–WLâ€– â‰¤ C â€²
for some numerical constant C â€² whenever q obeys (3.3).
Proof of (b). Since Pâ„¦Yj0 = 0,
Pâ„¦(UV âˆ— + PTâŠ¥Yj0) = Pâ„¦(UV âˆ— âˆ’ PTYj0) = Pâ„¦(Zj0),
and it follows from (3.6) that
â€–Zj0â€–F â‰¤ j0â€–UV âˆ—â€–F = j0
âˆš
r.
Since  â‰¤ eâˆ’1 and j0 â‰¥ 2 log n, j0 â‰¤ 1/n2 and this proves the claim.
Proof of (c). We have UV âˆ—+WL = Zj0 +Yj0 and know that Yj0 is supported on â„¦
c. Therefore,
since â€–Zj0â€–F â‰¤ Î»/8, it suffices to show that â€–Yj0â€–âˆ â‰¤ Î»/8. We have
â€–Yj0â€–âˆ â‰¤ qâˆ’1
âˆ‘
j
â€–Pâ„¦jZjâˆ’1â€–âˆ
â‰¤ qâˆ’1
âˆ‘
j
â€–Zjâˆ’1â€–âˆ
â‰¤ qâˆ’1
âˆ‘
j
jâ€–UV âˆ—â€–âˆ.
18
Since â€–UV âˆ—â€–âˆ â‰¤
âˆš
Âµr/n, this gives
â€–Yj0â€–âˆ â‰¤ C â€²
2âˆš
Âµr(log n)2
for some numerical constant C â€² whenever q obeys (3.3). Since Î» = 1/
âˆš
n, â€–Yj0â€–âˆ â‰¤ Î»/8 if
 â‰¤ C
(Âµr(log n)2
n
)1/4
.
Summary. We have seen that (a) and (b) are satisfied if  is sufficiently small and j0 â‰¥ 2 log n. For
(c), we can take  on the order of (Âµr(log n)2/n)1/4, which will be sufficiently small as well provided
that Ïr in (1.4) is sufficiently small. Note that everything is consistent since C0 âˆ’2 Âµr lognn < 1.
This concludes the proof of Lemma 2.8.
3.3 Proof of Lemma 2.9
It is convenient to introduce the sign matrix E = sgn(S0) distributed as
Eij =
ï£±ï£´ï£²ï£´ï£³
1, w. p. Ï/2,
0, w. p. 1âˆ’ Ï,
âˆ’1, w. p. Ï/2.
(3.7)
We shall be interested in the event {â€–Pâ„¦PT â€– â‰¤ Ïƒ} which holds with large probability when Ïƒ =âˆš
Ï+ , see Corollary 2.7. In particular, for any Ïƒ > 0, {â€–Pâ„¦PT â€– â‰¤ Ïƒ} holds with high probability
provided Ï is sufficiently small.
Proof of (a). By construction,
WS = Î»PTâŠ¥E + Î»PTâŠ¥
âˆ‘
kâ‰¥1
(Pâ„¦PTPâ„¦)kE
:= PTâŠ¥WS0 + PTâŠ¥WS1 .
For the first term, we have â€–PTâŠ¥WS0 â€– â‰¤ â€–WS0 â€– = Î»â€–Eâ€–. Then standard arguments about the norm
of a matrix with i.i.d. entries give [48]
â€–Eâ€– â‰¤ 4âˆšnÏ
with large probability. Since Î» = 1/
âˆš
n, this gives â€–WS0 â€– â‰¤ 4
âˆš
Ï. When the matrix is rectangular,
we have
â€–Eâ€– â‰¤ 4âˆšn(1)Ï
with high probability. Since Î» = 1/âˆšn(1) in this case, â€–WS0 â€– â‰¤ 4
âˆš
Ï as well.
SetR =
âˆ‘
kâ‰¥1(Pâ„¦PTPâ„¦)k and observe thatR is self-adjoint. For the second term, â€–PTâŠ¥WS1 â€– â‰¤
â€–WS1 â€–, where WS1 = Î»R(E). We need to bound the operator norm of the matrix R(E), and use a
standard covering argument to do this. Throughout, N denotes an 1/2-net for Snâˆ’1 of size at most
6n (such a net exists, see [30, Theorem 4.16]). Then a standard argument [48] shows that
â€–R(E)â€– = sup
x,yâˆˆSnâˆ’1
ã€ˆy,R(E)xã€‰ â‰¤ 4 sup
x,yâˆˆN
ã€ˆy,R(E)xã€‰.
19
For a fixed pair (x, y) of unit-normed vectors in N Ã—N , define the random variable
X(x, y) := ã€ˆy,R(E)xã€‰ = ã€ˆR(yxâˆ—), Eã€‰.
Conditional on â„¦ = supp(E), the signs of E are i.i.d. symmetric and Hoeffdingâ€™s inequality gives
P(|X(x, y)| > t |â„¦) â‰¤ 2 exp
(
âˆ’ 2t
2
â€–R(xyâˆ—)â€–2F
)
.
Now since â€–yxâˆ—â€–F = 1, the matrix R(yxâˆ—) obeys â€–R(yxâˆ—)â€–F â‰¤ â€–Râ€– and, therefore,
P
(
sup
x,yâˆˆN
|X(x, y)| > t |â„¦
)
â‰¤ 2|N |2 exp
(
âˆ’ 2t
2
â€–Râ€–2
)
.
Hence,
P(â€–R(E)â€– > t |â„¦) â‰¤ 2|N |2 exp
(
âˆ’ t
2
8â€–Râ€–2
)
.
On the event {â€–Pâ„¦PT â€– â‰¤ Ïƒ},
â€–Râ€– â‰¤
âˆ‘
kâ‰¥1
Ïƒ2k =
Ïƒ2
1âˆ’ Ïƒ2
and, therefore, unconditionally,
P(â€–R(E)â€– > t) â‰¤ 2|N |2 exp
(
âˆ’Î³
2t2
2
)
+ P(â€–Pâ„¦PT â€– â‰¥ Ïƒ), Î³ =
1âˆ’ Ïƒ2
2Ïƒ2
.
This gives
P(Î»â€–R(E)â€– > t) â‰¤ 2Ã— 62n exp
(
âˆ’Î³
2t2
2Î»2
)
+ P(â€–Pâ„¦PT â€– â‰¥ Ïƒ).
With Î» = 1/
âˆš
n,
â€–WSâ€– â‰¤ 1/4,
with large probability, provided that Ïƒ, or equivalently Ï, is small enough.
Proof of (b). Observe that
Pâ„¦âŠ¥WS = âˆ’Î»Pâ„¦âŠ¥PT (Pâ„¦ âˆ’ Pâ„¦PTPâ„¦)âˆ’1E.
Now for (i, j) âˆˆ â„¦c, WSij = ã€ˆei,WSejã€‰ = ã€ˆeieâˆ—j ,WSã€‰, and we have
WSij = Î»ã€ˆX(i, j), Eã€‰,
where X(i, j) is the matrix âˆ’(Pâ„¦ âˆ’ Pâ„¦PTPâ„¦)âˆ’1Pâ„¦PT (eieâˆ—j ). Conditional on â„¦ = supp(E), the
signs of E are i.i.d. symmetric, and Hoeffdingâ€™s inequality gives
P(|WSij | > tÎ» |â„¦) â‰¤ 2 exp
(
âˆ’ 2t
2
â€–X(i, j)â€–2F
)
,
and, thus,
P
(
sup
i,j
|WSij | > tÎ» |â„¦
)
â‰¤ 2n2 exp
(
âˆ’ 2t
2
supi,j â€–X(i, j)â€–2F
)
.
20
Since (2.2) holds, we have
â€–Pâ„¦PT (eieâˆ—j )â€–F â‰¤ â€–Pâ„¦PT â€–â€–PT (eieâˆ—j )â€–F â‰¤ Ïƒ
âˆš
2Âµr/n
on the event {â€–Pâ„¦PT â€– â‰¤ Ïƒ}. On the same event, â€–(Pâ„¦âˆ’Pâ„¦PTPâ„¦)âˆ’1â€– â‰¤ (1âˆ’Ïƒ2)âˆ’1 and, therefore,
â€–X(i, j)â€–2F â‰¤
2Ïƒ2
(1âˆ’ Ïƒ2)2
Âµr
n
.
Then unconditionally,
P
(
sup
i,j
|WSij | > tÎ»
)
â‰¤ 2n2 exp
(
âˆ’nÎ³
2t2
Âµr
)
+ P(â€–Pâ„¦PT â€– â‰¥ Ïƒ), Î³ =
(1âˆ’ Ïƒ2)2
2Ïƒ2
.
This proves the claim when Âµr < Ïâ€²rn(log n)
âˆ’1 and Ïâ€²r is sufficiently small.
4 Numerical Experiments and Applications
In this section, we perform numerical experiments corroborating our main results and suggesting
their many applications in image and video analysis. We first investigate Principal Component
Pursuitâ€™s ability to correctly recover matrices of various rank from errors of various density. We then
sketch applications in background modeling from video and removing shadows and specularities
from face images.
While the exact recovery guarantee provided by Theorem 1.1 is independent of the particular
algorithm used to solve Principal Component Pursuit, its applicability to large scale problems
depends on the availability of scalable algorithms for nonsmooth convex optimization. For the
experiments in this section, we use the an augmented Lagrange multiplier algorithm introduced
in [32, 51].8 In Section 5, we describe this algorithm in more detail, and explain why it is our
algorithm of choice for sparse and low-rank separation.
One important implementation detail in our approach is the choice of Î». Our analysis identifies
one choice, Î» = 1/
âˆš
max(n1, n2), which works well for incoherent matrices. In order to illustrate the
theory, throughout this section we will always choose Î» = 1/
âˆš
max(n1, n2). For practical problems,
however, it is often possible to improve performance by choosing Î» according to prior knowledge
about the solution. For example, if we know that S is very sparse, increasing Î» will allow us to
recover matrices L of larger rank. For practical problems, we recommend Î» = 1/
âˆš
max(n1, n2) as
a good rule of thumb, which can then be adjusted slightly to obtain the best possible result.
4.1 Exact recovery from varying fractions of error
We first verify the correct recovery phenomenon of Theorem 1.1 on randomly generated problems.
We consider square matrices of varying dimension n = 500, . . . , 3000. We generate a rank-r matrix
L0 as a product L0 = XY âˆ— where X and Y are nÃ— r matrices with entries independently sampled
from a N (0, 1/n) distribution. S0 is generated by choosing a support set â„¦ of size k uniformly at
random, and setting S0 = Pâ„¦E, where E is a matrix with independent Bernoulli Â±1 entries.
Table 1 (top) reports the results with r = rank(L0) = 0.05 Ã— n and k = â€–S0â€–0 = 0.05 Ã— n2.
Table 1 (bottom) reports the results for a more challenging scenario, rank(L0) = 0.05 Ã— n and
8Both [32,51] have posted a version of their code online.
21
Dimension n rank(L0) â€–S0â€–0 rank(LÌ‚) â€–SÌ‚â€–0 â€–LÌ‚âˆ’L0â€–Fâ€–L0â€–F # SVD Time(s)
500 25 12,500 25 12,500 1.1Ã— 10âˆ’6 16 2.9
1,000 50 50,000 50 50,000 1.2Ã— 10âˆ’6 16 12.4
2,000 100 200,000 100 200,000 1.2Ã— 10âˆ’6 16 61.8
3,000 250 450,000 250 450,000 2.3Ã— 10âˆ’6 15 185.2
rank(L0) = 0.05Ã— n, â€–S0â€–0 = 0.05Ã— n2.
Dimension n rank(L0) â€–S0â€–0 rank(LÌ‚) â€–SÌ‚â€–0 â€–LÌ‚âˆ’L0â€–Fâ€–L0â€–F # SVD Time(s)
500 25 25,000 25 25,000 1.2Ã— 10âˆ’6 17 4.0
1,000 50 100,000 50 100,000 2.4Ã— 10âˆ’6 16 13.7
2,000 100 400,000 100 400,000 2.4Ã— 10âˆ’6 16 64.5
3,000 150 900,000 150 900,000 2.5Ã— 10âˆ’6 16 191.0
rank(L0) = 0.05Ã— n, â€–S0â€–0 = 0.10Ã— n2.
Table 1: Correct recovery for random problems of varying size. Here, L0 = XY âˆ— âˆˆ RnÃ—n
with X,Y âˆˆ RnÃ—r; X,Y have entries i.i.d. N (0, 1/n). S0 âˆˆ {âˆ’1, 0, 1}nÃ—n has support chosen
uniformly at random and independent random signs; â€–S0â€–0 is the number of nonzero entries
in S0. Top: recovering matrices of rank 0.05 Ã— n from 5% gross errors. Bottom: recovering
matrices of rank 0.05 Ã— n from 10% gross errors. In all cases, the rank of L0 and `0-norm of
S0 are correctly estimated. Moreover, the number of partial singular value decompositions (#
SVD) required to solve PCP is almost constant.
k = 0.10Ã—n2. In all cases, we set Î» = 1/
âˆš
n. Notice that in all cases, solving the convex PCP gives
a result (L, S) with the correct rank and sparsity. Moreover, the relative error â€–L âˆ’ L0â€–F /â€–L0â€–F
is small, less than 10âˆ’5 in all examples considered.9
The last two columns of Table 1 give the number of partial singular value decompositions
computed in the course of the optimization (# SVD) as well as the total computation time. This
experiment was performed in Matlab on a Mac Pro with dual quad-core 2.66 GHz Intel Xenon
processors and 16 GB RAM. As we will discuss in Section 5 the dominant cost in solving the
convex program comes from computing one partial SVD per iteration. Strikingly, in Table 1, the
number of SVD computations is nearly constant regardless of dimension, and in all cases less than
17.10 This suggests that in addition to being theoretically well-founded, the recovery procedure
advocated in this paper is also reasonably practical.
4.2 Phase transition in rank and sparsity
Theorem 1.1 shows that convex programming correctly recovers an incoherent low-rank matrix
from a constant fraction Ïs of errors. We next empirically investigate the algorithmâ€™s ability to
recover matrices of varying rank from errors of varying sparsity. We consider square matrices of
9We measure relative error in terms of L only, since in this paper we view the sparse and low-rank decomposition
as recovering a low-rank matrix L0 from gross errors. S0 is of course also well-recovered: in this example, the relative
error in S is actually smaller than that in L.
10One might reasonably ask whether this near constant number of iterations is due to the fact that random problems
are in some sense well-conditioned. There is some validity to this concern, as we will see in our real data examples. [32]
suggests a continuation strategy (there termed â€œInexact ALMâ€) that produces qualitatively similar solutions with a
similarly small number of iterations. However, to the best of our knowledge its convergence is not guaranteed.
22
dimension n1 = n2 = 400. We generate low-rank matrices L0 = XY âˆ— with X and Y independently
chosen n Ã— r matrices with i.i.d. Gaussian entries of mean zero and variance 1/n. For our first
experiment, we assume a Bernoulli model for the support of the sparse term S0, with random signs:
each entry of S0 takes on value 0 with probability 1âˆ’ Ï, and values Â±1 each with probability Ï/2.
For each (r, Ï) pair, we generate 10 random problems, each of which is solved via the algorithm of
Section 5. We declare a trial to be successful if the recovered LÌ‚ satisfies â€–Lâˆ’L0â€–F /â€–L0â€–F â‰¤ 10âˆ’3.
Figure 1 (left) plots the fraction of correct recoveries for each pair (r, Ï). Notice that there is a
large region in which the recovery is exact. This highlights an interesting aspect of our result: the
recovery is correct even though in some cases â€–S0â€–F  â€–L0â€–F (e.g., for r/n = Ï, â€–S0â€–F is
âˆš
n = 20
times larger!). This is to be expected from Lemma 2.4: the existence (or non-existence) of a dual
certificate depends only on the signs and support of S0 and the orientation of the singular spaces
of L0.
However, for incoherent L0, our main result goes one step further and asserts that the signs of
S0 are also not important: recovery can be guaranteed as long as its support is chosen uniformly
at random. We verify this by again sampling L0 as a product of Gaussian matrices and choosing
the support â„¦ according to the Bernoulli model, but this time setting S0 = Pâ„¦sgn(L0). One might
expect such S0 to be more difficult to distinguish from L0. Nevertheless, our analysis showed that
the number of errors that can be corrected drops by at most 1/2 when moving to this more difficult
model. Figure 1 (middle) plots the fraction of correct recoveries over 10 trials, again varying r and
Ï. Interestingly, the region of correct recovery in Figure 1 (middle) actually appears to be broader
than that in Figure 1 (left). Admittedly, the shape of the region in the upper-left corner is puzzling,
but has been â€˜confirmedâ€™ by several distinct simulation experiments (using different solvers).
Finally, inspired by the connection between matrix completion and robust PCA, we compare
the breakdown point for the low-rank and sparse separation problem to the breakdown behavior of
the nuclear-norm heuristic for matrix completion. By comparing the two heuristics, we can begin
to answer the question how much is gained by knowing the location â„¦ of the corrupted entries?
Here, we again generate L0 as a product of Gaussian matrices. However, we now provide the
algorithm with only an incomplete subset M = Pâ„¦âŠ¥L0 of its entries. Each (i, j) is included in â„¦
independently with probability 1 âˆ’ Ï, so rather than a probability of error, here, Ï stands for the
probability that an entry is omitted. We solve the nuclear norm minimization problem
minimize â€–Lâ€–âˆ— subject to Pâ„¦âŠ¥L = Pâ„¦âŠ¥M
using an augmented Lagrange multiplier algorithm very similar to the one discussed in Section 5.
We again declare L0 to be successfully recovered if â€–Lâˆ’L0â€–F /â€–L0â€–F < 10âˆ’3. Figure 1 (right) plots
the fraction of correct recoveries for varying r, Ï. Notice that nuclear norm minimization successfully
recovers L0 over a much wider range of (r, Ï). This is interesting because in the regime of large
k, k = â„¦(n2), the best performance guarantees for each heuristic agree in their order of growth
â€“ both guarantee correct recovery for rank(L0) = O(n/ log2 n). Fully explaining the difference in
performance between the two problems may require a sharper analysis of the breakdown behavior
of each.
4.3 Application sketch: background modeling from surveillance video
Video is a natural candidate for low-rank modeling, due to the correlation between frames. One
of the most basic algorithmic tasks in video surveillance is to estimate a good model for the
23
(a) Robust PCA, Random Signs (b) Robust PCA, Coherent Signs (c) Matrix Completion
Figure 1: Correct recovery for varying rank and sparsity. Fraction of correct recoveries
across 10 trials, as a function of rank(L0) (x-axis) and sparsity of S0 (y-axis). Here, n1 = n2 =
400. In all cases, L0 = XY âˆ— is a product of independent nÃ— r i.i.d. N (0, 1/n) matrices. Trials
are considered successful if â€–LÌ‚âˆ’L0â€–F /â€–L0â€–F < 10âˆ’3. Left: low-rank and sparse decomposition,
sgn(S0) random. Middle: low-rank and sparse decomposition, S0 = Pâ„¦sgn(L0). Right: matrix
completion. For matrix completion, Ïs is the probability that an entry is omitted from the
observation.
background variations in a scene. This task is complicated by the presence of foreground objects:
in busy scenes, every frame may contain some anomaly. Moreover, the background model needs to
be flexible enough to accommodate changes in the scene, for example due to varying illumination.
In such situations, it is natural to model the background variations as approximately low rank.
Foreground objects, such as cars or pedestrians, generally occupy only a fraction of the image
pixels and hence can be treated as sparse errors.
We investigate whether convex optimization can separate these sparse errors from the low-
rank background. Here, it is important to note that the error support may not be well-modeled
as Bernoulli: errors tend to be spatially coherent, and more complicated models such as Markov
random fields may be more appropriate [11,52]. Hence, our theorems do not necessarily guarantee
the algorithm will succeed with high probability. Nevertheless, as we will see, Principal Component
Pursuit still gives visually appealing solutions to this practical low-rank and sparse separation
problem, without using any additional information about the spatial structure of the error.
We consider two example videos introduced in [31]. The first is a sequence of 200 grayscale
frames taken in an airport. This video has a relatively static background, but significant foreground
variations. The frames have resolution 176 Ã— 144; we stack each frame as a column of our matrix
M âˆˆ R25,344Ã—200. We decompose M into a low-rank term and a sparse term by solving the convex
PCP problem (1.1) with Î» = 1/
âˆš
n1. On a desktop PC with a 2.33 GHz Core2 Duo processor
and 2 GB RAM, our Matlab implementation requires 806 iterations, and roughly 43 minutes to
converge.11 Figure 2(a) shows three frames from the video; (b) and (c) show the corresponding
columns of the low rank matrix LÌ‚ and sparse matrix SÌ‚ (its absolute value is shown here). Notice
that LÌ‚ correctly recovers the background, while SÌ‚ correctly identifies the moving pedestrians. The
person appearing in the images in LÌ‚ does not move throughout the video.
11The paper [32] suggests a variant of ALM optimization procedure, there termed the â€œInexact ALMâ€ that finds
a visually similar decomposition in far fewer iterations (less than 50). However, since the convergence guarantee for
that variant is weak, we choose to present the slower, exact result here.
24
(a) Original frames (b) Low-rank LÌ‚ (c) Sparse SÌ‚ (d) Low-rank LÌ‚ (e) Sparse SÌ‚
Convex optimization (this work) Alternating minimization [47]
Figure 2: Background modeling from video. Three frames from a 200 frame video sequence
taken in an airport [31]. (a) Frames of original video M . (b)-(c) Low-rank LÌ‚ and sparse
components SÌ‚ obtained by PCP, (d)-(e) competing approach based on alternating minimization
of an m-estimator [47]. PCP yields a much more appealing result despite using less prior
knowledge.
Figure 2 (d) and (e) compares the result obtained by Principal Component Pursuit to a state-of-
the-art technique from the computer vision literature, [47].12 That approach also aims at robustly
recovering a good low-rank approximation, but uses a more complicated, nonconvex m-estimator,
which incorporates a local scale estimate that implicitly exploits the spatial characteristics of natural
images. This leads to a highly nonconvex optimization, which is solved locally via alternating
minimization. Interestingly, despite using more prior information about the signal to be recovered,
this approach does not perform as well as the convex programming heuristic: notice the large
artifacts in the top and bottom rows of Figure 2 (d).
In Figure 3, we consider 250 frames of a sequence with several drastic illumination changes.
Here, the resolution is 168 Ã— 120, and so M is a 20, 160 Ã— 250 matrix. For simplicity, and to
illustrate the theoretical results obtained above, we again choose Î» = 1/
âˆš
n1.13 For this example,
on the same 2.66 GHz Core 2 Duo machine, the algorithm requires a total of 561 iterations and 36
minutes to converge.
Figure 3 (a) shows three frames taken from the original video, while (b) and (c) show the
recovered low-rank and sparse components, respectively. Notice that the low-rank component
correctly identifies the main illuminations as background, while the sparse part corresponds to the
12We use the code package downloaded from http://www.salleurl.edu/~ftorre/papers/rpca/rpca.zip, modi-
fied to choose the rank of the approximation as suggested in [47].
13For this example, slightly more appealing results can actually be obtained by choosing larger Î» (say, 2/
âˆš
n1).
25
(a) Original frames (b) Low-rank LÌ‚ (c) Sparse SÌ‚ (d) Low-rank LÌ‚ (e) Sparse SÌ‚
Convex optimization (this work) Alternating minimization [47]
Figure 3: Background modeling from video. Three frames from a 250 frame sequence taken in
a lobby, with varying illumination [31]. (a) Original video M . (b)-(c) Low-rank LÌ‚ and sparse SÌ‚
obtained by PCP. (d)-(e) Low-rank and sparse components obtained by a competing approach
based on alternating minimization of an m-estimator [47]. Again, convex programming yields
a more appealing result despite using less prior information.
motion in the scene. On the other hand, the result produced by the algorithm of [47] treats some
of the first illumination as foreground. PCP again outperforms the competing approach, despite
using less prior information. These results suggest the potential power for convex programming as
a tool for video analysis.
Notice that the number of iterations for the real data is typically higher than that of the
simulations with random matrices given in Table 1. The reason for this discrepancy might be
that the structures of real data could slightly deviate from the idealistic low-rank and sparse
model. Nevertheless, it is important to realize that practical applications such as video surveillance
often provide additional information about the signals of interest, e.g. the support of the sparse
foreground is spatially piecewise contiguous, or even impose additional requirements, e.g. the
recovered background needs to be non-negative etc. We note that the simplicity of our objective
and solution suggests that one can easily incorporate additional constraints and more accurate
models of the signals so as to obtain much more efficient and accurate solutions in the future.
4.4 Application sketch: removing shadows and specularities from face images
Face recognition is another problem domain in computer vision where low-dimensional linear models
have received a great deal of attention. This is mostly due to the work of Basri and Jacobs, who
showed that for convex, Lambertian objects, images taken under distant illumination lie near an
approximately nine-dimensional linear subspace known as the harmonic plane [1]. However, since
26
(a) M (b) LÌ‚ (c) SÌ‚ (a) M (b) LÌ‚ (c) SÌ‚
Figure 4: Removing shadows, specularities, and saturations from face images. (a) Cropped
and aligned images of a personâ€™s face under different illuminations from the Extended Yale
B database. The size of each image is 192 Ã— 168 pixels, a total of 58 different illuminations
were used for each person. (b) Low-rank approximation LÌ‚ recovered by convex programming.
(c) Sparse error SÌ‚ corresponding to specularities in the eyes, shadows around the nose region,
or brightness saturations on the face. Notice in the bottom left that the sparse term also
compensates for errors in image acquisition.
faces are neither perfectly convex nor Lambertian, real face images often violate this low-rank
model, due to cast shadows and specularities. These errors are large in magnitude, but sparse in
the spatial domain. It is reasonable to believe that if we have enough images of the same face,
Principal Component Pursuit will be able to remove these errors. As with the previous example,
some caveats apply: the theoretical result suggests the performance should be good, but does not
guarantee it, since again the error support does not follow a Bernoulli model. Nevertheless, as we
will see, the results are visually striking.
Figure 4 shows two examples with face images taken from the Yale B face database [18]. Here,
each image has resolution 192 Ã— 168; there are a total of 58 illuminations per subject, which we
stack as the columns of our matrix M âˆˆ R32,256Ã—58. We again solve PCP with Î» = 1/âˆšn1. In
this case, the algorithm requires 642 iterations to converge, and the total computation time on the
same Core 2 Duo machine is 685 seconds.
Figure 4 plots the low rank term LÌ‚ and the magnitude of the sparse term SÌ‚ obtained as the
solution to the convex program. The sparse term SÌ‚ compensates for cast shadows and specular
regions. In one example (bottom row of Figure 4 left), this term also compensates for errors in image
acquisition. These results may be useful for conditioning the training data for face recognition, as
well as face alignment and tracking under illumination variations.
27
5 Algorithms
Theorem 1.1 shows that incoherent low-rank matrices can be recovered from nonvanishing fractions
of gross errors in polynomial time. Moreover, as the experiments in the previous section attest,
the low computation cost is guaranteed not only in theory, the efficiency is becoming practical for
real imaging problems. This practicality is mainly due to the rapid recent progress in scalable
algorithms for nonsmooth convex optimization, in particular for minimizing the `1 and nuclear
norms. In this section, we briefly review this progress, and discuss our algorithm of choice for this
problem.
For small problem sizes, Principal Component Pursuit
minimize â€–Lâ€–âˆ— + Î»â€–Sâ€–1
subject to L+ S = M
can be performed using off-the-shelf tools such as interior point methods [21]. This was suggested
for rank minimization in [16, 45] and for low-rank and sparse decomposition [12] (see also [35]).
However, despite their superior convergence rates, interior point methods are typically limited to
small problems, say n < 100, due to the O(n6) complexity of computing a step direction.
The limited scalability of interior point methods has inspired a recent flurry of work on first-order
methods. Exploiting an analogy with iterative thresholding algorithms for `1-minimization [49,50],
Cai et. al. developed an algorithm that performs nuclear-norm minimization by repeatedly shrinking
the singular values of an appropriate matrix, essentially reducing the complexity of each iteration
to the cost of an SVD [6]. However, for our low-rank and sparse decomposition problem, this
form of iterative thresholding converges slowly, requiring up to 104 iterations. Ma et. al. [20, 36]
suggest improving convergence using continuation techniques, and also demonstrate how Bregman
iterations [41] can be applied to nuclear norm minimization.
The convergence of iterative thresholding has also been greatly improved using ideas from
Nesterovâ€™s optimal first-order algorithm for smooth minimization [37], which was extended to non-
smooth optimization in [2, 38], and applied to `1-minimization in [2, 3, 39]. Based on [2], Toh et.
al. developed a proximal gradient algorithm for matrix completion which they termed Accelerated
Proximal Gradient (APG). A very similar APG algorithm was suggested for low-rank and sparse
decomposition in [33]. That algorithm inherits the optimal O(1/k2) convergence rate for this class of
problems. Empirical evidence suggests that these algorithms can solve the convex PCP problem at
least 50 times faster than straightforward iterative thresholding (for more details and comparisons,
see [33]).
However, despite its good convergence guarantees, the practical performance of APG depends
strongly on the design of good continuation schemes. Generic continuation does not guarantee good
accuracy and convergence across a wide range of problem settings.14 In this paper, we have chosen
to instead solve the convex PCP problem (1.1) using an augmented Lagrange multiplier (ALM)
algorithm introduced in [32, 51]. In our experience, ALM achieves much higher accuracy than
APG, in fewer iterations. It works stably across a wide range of problem settings with no tuning
of parameters. Moreover we observe an appealing (empirical) property: the rank of the iterates
often remains bounded by rank(L0) throughout the optimization, allowing them to be computed
especially efficiently. APG, on the other hand, does not have this property.
14In our experience, the optimal choice may depend on the relative magnitudes of the L and S terms and the
sparsity of the corruption.
28
The ALM method operates on the augmented Lagrangian
l(L, S, Y ) = â€–Lâ€–âˆ— + Î»â€–Sâ€–1 + ã€ˆY,M âˆ’ Lâˆ’ Sã€‰+
Âµ
2
â€–M âˆ’ Lâˆ’ Sâ€–2F . (5.1)
A generic Lagrange multiplier algorithm [5] would solve PCP by repeatedly setting (Lk, Sk) =
arg minL,S l(L, S, Yk), and then updating the Lagrange multiplier matrix via Yk+1 = Yk + Âµ(M âˆ’
Lk âˆ’ Sk).
For our low-rank and sparse decomposition problem, we can avoid having to solve a sequence of
convex programs by recognizing that minL l(L, S, Y ) and minS l(L, S, Y ) both have very simple and
efficient solutions. Let SÏ„ : R â†’ R denote the shrinkage operator SÏ„ [x] = sgn(x) max(|x| âˆ’ Ï„, 0),
and extend it to matrices by applying it to each element. It is easy to show that
arg min
S
l(L, S, Y ) = SÎ»Âµ(M âˆ’ L+ Âµâˆ’1Y ). (5.2)
Similarly, for matrices X, let DÏ„ (X) denote the singular value thresholding operator given by
DÏ„ (X) = USÏ„ (Î£)V âˆ—, where X = UÎ£V âˆ— is any singular value decomposition. It is not difficult to
show that
arg min
L
l(L, S, Y ) = DÂµ(M âˆ’ S âˆ’ Âµâˆ’1Y ). (5.3)
Thus, a more practical strategy is to first minimize l with respect to L (fixing S), then minimize l
with respect to S (fixing L), and then finally update the Lagrange multiplier matrix Y based on
the residual M âˆ’ Lâˆ’ S, a strategy that is summarized as Algorithm 1 below.
Algorithm 1 (Principal Component Pursuit by Alternating Directions [32,51])
1: initialize: S0 = Y0 = 0, Âµ > 0.
2: while not converged do
3: compute Lk+1 = DÂµ(M âˆ’ Sk âˆ’ Âµâˆ’1Yk);
4: compute Sk+1 = SÎ»Âµ(M âˆ’ Lk+1 + Âµâˆ’1Yk);
5: compute Yk+1 = Yk + Âµ(M âˆ’ Lk+1 âˆ’ Sk+1);
6: end while
7: output: L, S.
Algorithm 1 is a special case of a more general class of augmented Lagrange multiplier algorithms
known as alternating directions methods [51]. The convergence of these algorithms has been well-
studied (see e.g. [29,34] and the many references therein, as well as discussion in [32,51]). Algorithm
1 performs excellently on a wide range of problems: as we saw in Section 3, relatively small
numbers of iterations suffice to achieve good relative accuracy. The dominant cost of each iteration
is computing Lk+1 via singular value thresholding. This requires us to compute those singular
vectors of Mâˆ’Skâˆ’Âµâˆ’1Yk whose corresponding singular values exceed the threshold Âµ. Empirically,
we have observed that the number of such large singular values is often bounded by rank(L0),
allowing the next iterate to be computed efficiently via a partial SVD.15 The most important
implementation details for this algorithm are the choice of Âµ and the stopping criterion. In this
work, we simply choose Âµ = n1n2/4â€–Mâ€–1, as suggested in [51]. We terminate the algorithm when
â€–M âˆ’ Lâˆ’ Sâ€–F â‰¤ Î´â€–Mâ€–F , with Î´ = 10âˆ’7.
15Further performance gains might be possible by replacing this partial SVD with an approximate SVD, as suggested
in [20] for nuclear norm minimization.
29
Very similar ideas can be used to develop simple and effective augmented Lagrange multiplier
algorithms for matrix completion [32], and for the robust matrix completion problem (1.5) discussed
in Section 1.6, with similarly good performance. In the preceding section, all simulations and
experiments are therefore conducted using ALM-based algorithms. For a more thorough discussion,
implementation details and comparisons with other algorithms, please see [32,51].
6 Discussion
This paper delivers some rather surprising news: one can disentangle the low-rank and sparse
components exactly by convex programming, and this provably works under very broad conditions
that are much broader than those provided by the best known results. Further, our analysis has
revealed rather close relationships between matrix completion and matrix recovery (from sparse
errors) and our results even generalize to the case when there are both incomplete and corrupted
entries (i.e. Theorem 1.2). In addition, Principal Component Pursuit does not have any free
parameter and can be solved by simple optimization algorithms with remarkable efficiency and
accuracy. More importantly, our results may point to a very wide spectrum of new theoretical and
algorithmic issues together with new practical applications that can now be studied systematically.
Our study so far is limited to the low-rank component being exactly low-rank, and the sparse
component being exactly sparse. It would be interesting to investigate when either or both these
assumptions are relaxed. One way to think of this is via the new observation model M = L0 +S0 +
N0, where N0 is a dense, small perturbation accounting for the fact that the low-rank component
is only approximately low-rank and that small errors can be added to all the entries (in some sense,
this model unifies the classical PCA and the robust PCA by combining both sparse gross errors and
dense small noise). The ideas developed in [7] in connection with the stability of matrix completion
under small perturbations may be useful here. Even more generally, the problems of sparse signal
recovery, low-rank matrix completion, classical PCA, and robust PCA can all be considered as
special cases of a general measurement model of the form
M = A(L0) + B(S0) + C(N0),
whereA,B, C are known linear maps. An ambitious goal might be to understand exactly under what
conditions, one can effectively retrieve or decompose L0 and S0 from such noisy linear measurements
via convex programming.
The remarkable ability of convex optimizations in recovering low-rank matrices and sparse
signals in high-dimensional spaces suggest that they will be a powerful tool for processing massive
data sets that arise in image/video processing, web data analysis, and bioinformatics. Such data
are often of millions or even billions of dimensions so the computational and memory cost can be
far beyond that of a typical PC. Thus, one important direction for future investigation is to develop
algorithms that have even better scalability, and can be easily implemented on the emerging parallel
and distributed computing infrastructures.
30
7 Appendix
7.1 Equivalence of sampling models
We begin by arguing that a recovery result under the Bernoulli model automatically implies a
corresponding result for the uniform model. Denote by PUnif(m) and PBer(p) probabilities calculated
under the uniform and Bernoulli models and let â€œSuccessâ€ be the event that the algorithm succeeds.
We have
PBer(p)(Success) =
n2âˆ‘
k=0
PBer(p)(Success | |â„¦| = k) PBer(p)(|â„¦| = k)
â‰¤
mâˆ’1âˆ‘
k=0
PBer(p)(|â„¦| = k) +
n2âˆ‘
k=m
PUnif(k)(Success) PBer(p)(|â„¦| = k)
â‰¤ PBer(p)(|â„¦| < m) + PUnif(m)(Success),
where we have used the fact that for k â‰¥ m, PUnif(k)(Success) â‰¤ PUnif(m)(Success), and that the
conditional distribution of â„¦ given its cardinality is uniform. Thus,
PUnif(m)(Success) â‰¥ PBer(p)(Success)âˆ’ PBer(p)(|â„¦| < m).
Take p = m/n2 + , where  > 0. The conclusion follows from PBer(p)(|â„¦| < m) â‰¤ e
âˆ’ 
2n2
2p . In the
other direction, the same reasoning gives
PBer(p)(Success) â‰¥
mâˆ‘
k=0
PBer(p)(Success | |â„¦| = k) PBer(p)(|â„¦| = k)
â‰¥ PUnif(m)(Success)
mâˆ‘
k=0
PBer(p)(|â„¦| = k)
= PUnif(m)(Success) P(|â„¦| â‰¤ m),
and choosing m such that P(|â„¦| > m) is exponentially small, establishes the claim.
7.2 Proof of Lemma 3.1
The proof is essentially an application of Bernsteinâ€™s inequality, which states that for a sum of
uniformly bounded independent random variables with |Yk âˆ’ EYk| < c,
P
( nâˆ‘
k=1
(Yk âˆ’ EYk) > t
)
â‰¤ 2 exp
(
âˆ’ t
2
2Ïƒ2 + 2ct/3
)
, (7.1)
where Ïƒ2 is the sum of the variances, Ïƒ2 â‰¡
âˆ‘n
k=1 Var(Yk).
Define â„¦0 via â„¦0 = {(i, j) : Î´ij = 1} where {Î´ij} is an independent sequence of Bernoulli
variables with parameter Ï0. With this notation, Z â€² = Z âˆ’ Ïâˆ’10 PTPâ„¦0Z is given by
Z â€² =
âˆ‘
ij
(1âˆ’ Ïâˆ’10 Î´ij)ZijPT (eie
âˆ—
j )
31
so that Z â€²i0j0 is a sum of independent random variables,
Z â€²i0j0 =
âˆ‘
ij
Yij , Yij = (1âˆ’ Ïâˆ’10 Î´ij)Zijã€ˆPT (eie
âˆ—
j ), ei0e
âˆ—
j0ã€‰.
We have âˆ‘
ij
Var(Yij) = (1âˆ’ Ï0)Ïâˆ’10
âˆ‘
ij
|Zij |2|ã€ˆPT (eieâˆ—j ), ei0eâˆ—j0ã€‰|
2
â‰¤ (1âˆ’ Ï0)Ïâˆ’10 â€–Zâ€–
2
âˆ
âˆ‘
ij
|ã€ˆeieâˆ—j ,PT (ei0eâˆ—j0)ã€‰|
2
= (1âˆ’ Ï0)Ïâˆ’10 â€–Zâ€–
2
âˆâ€–PT (ei0eâˆ—j0)â€–
2
F
â‰¤ (1âˆ’ Ï0)Ïâˆ’10 â€–Zâ€–
2
âˆ
2Âµr
n
,
where the last inequality holds because of (2.2). Also, it follows from (1.2) that |ã€ˆPT (eieâˆ—j ), ei0eâˆ—j0ã€‰| â‰¤
â€–PT (eieâˆ—j )â€–F â€–PT (ei0eâˆ—j0)â€–F â‰¤ 2Âµr/n so that |Yij | â‰¤ Ï
âˆ’1
0 â€–Zâ€–âˆÂµr/n. Then Bernsteinâ€™s inequality
gives
P(|Z â€²ij | > â€–Zâ€–âˆ) â‰¤ 2 exp
(
âˆ’ 3
16
2nÏ0
Âµr
)
.
If Ï0 is as in Lemma 3.1, the union bound proves the claim.
7.3 Proof of Theorem 1.2
This section presents a proof of Theorem 1.2, which resembles that of Theorem 1.1. Here and
below, Sâ€²0 = Pâ„¦obsS0 so that the available data are of the form Y = Pâ„¦obsL0 + Sâ€²0. We make three
observations.
â€¢ If PCP correctly recovers L0 from the input data Pâ„¦obsL0 + Sâ€²0 (note that this means that
LÌ‚ = L0 and SÌ‚ = Sâ€²0), then it must correctly recover L0 from Pâ„¦obsL0 + Sâ€²â€²0 , where Sâ€²â€²0 is
a trimmed version of Sâ€²0. The proof is identical to that of our elimination result, namely,
Theorem 2.2. The derandomization argument then applies and it suffices to consider the case
where the signs of Sâ€²0 are i.i.d. symmetric Bernoulli variables.
â€¢ It is of course sufficient to prove the theorem when each entry in â„¦obs is revealed with
probability p0 := 0.1, i.e. when â„¦obs âˆ¼ Ber(p0).
â€¢ We establish the theorem in the case where n1 = n2 = n as slight modifications would give
the general case.
Further, there are now three index sets of interest:
â€¢ â„¦obs are those locations where data are available.
â€¢ Î“ âŠ‚ â„¦obs are those locations where data are available and clean; that is, PÎ“Y = PÎ“L0.
â€¢ â„¦ = â„¦obs \ Î“ are those locations where data are available but totally unreliable.
The matrix Sâ€²0 is thus supported on â„¦. If â„¦obs âˆ¼ Ber(p0), then by definition, â„¦ âˆ¼ Ber(p0Ï„).
32
Dual certification. We begin with two lemmas concerning dual certification.
Lemma 7.1 Assume â€–PÎ“âŠ¥PT â€– < 1. Then (L0, Sâ€²0) is the unique solution if there is a pair (W,F )
obeying
UV âˆ— +W = Î»(sgn(Sâ€²0) + F ),
with PTW = 0, â€–Wâ€– < 1, PÎ“âŠ¥F = 0 and â€–Fâ€–âˆ < 1.
The proof is about the same as that of Lemma 2.4, and is discussed in very brief terms. The idea
is to consider a feasible perturbation of the form (L0 +HL, Sâ€²0âˆ’HS) obeying Pâ„¦obsHL = Pâ„¦obsHS ,
and show that this increases the objective functional unless HL = HS = 0. Then a sequence of
steps similar to that in the proof of Lemma 2.4 establishes
â€–L0 +HLâ€–âˆ— + Î»â€–Sâ€²0 âˆ’HSâ€–1 â‰¥ â€–L0â€–âˆ— + Î»â€–Sâ€²0â€–1 + (1âˆ’ Î²)(â€–PTâŠ¥HLâ€–âˆ— + Î»â€–PÎ“HLâ€–1), (7.2)
where Î² = max(â€–Wâ€–, â€–Fâ€–âˆ). Finally, â€–PTâŠ¥HLâ€–âˆ—+Î»â€–PÎ“HLâ€–1 vanishes if and only if HL âˆˆ Î“âŠ¥âˆ©T =
{0}.
Lemma 7.2 Assume that for any matrix M , â€–PTPÎ“âŠ¥Mâ€–F â‰¤ nâ€–PTâŠ¥PÎ“âŠ¥Mâ€–F and take Î» > 4/n.
Then (L0, Sâ€²0) is the unique solution if there is a pair (W,F ) obeying
UV âˆ— +W + PTD = Î»(sgn(Sâ€²0) + F ),
with PTW = 0, â€–Wâ€– < 1/2, PÎ“âŠ¥F = 0 and â€–Fâ€–âˆ < 1/2, and â€–PTDâ€–F â‰¤ nâˆ’2.
Note that â€–PTPÎ“âŠ¥Mâ€–F â‰¤ nâ€–PTâŠ¥PÎ“âŠ¥Mâ€–F implies Î“âŠ¥ âˆ© T = {0}, or equivalently â€–PÎ“âŠ¥PT â€– < 1.
Indeed if M âˆˆ Î“âŠ¥ âˆ© T , PTPÎ“âŠ¥M = M while PTâŠ¥PÎ“âŠ¥M = 0, and thus M = 0.
Proof It follows from (7.2) together with the same argument as in the proof of Lemma 7.2 that
â€–L0 +HLâ€–âˆ— + Î»â€–Sâ€²0 âˆ’HSâ€–1 â‰¥ â€–L0â€–âˆ— + Î»â€–Sâ€²0â€–1 +
1
2
(
â€–PTâŠ¥HLâ€–âˆ— + Î»â€–PÎ“HLâ€–1
)
âˆ’ 1
n2
â€–PTHLâ€–F .
Observe now that
â€–PTHLâ€–F â‰¤ â€–PTPÎ“HLâ€–F + â€–PTPÎ“âŠ¥HLâ€–F
â‰¤ â€–PTPÎ“HLâ€–F + nâ€–PTâŠ¥PÎ“âŠ¥HLâ€–F
â‰¤ â€–PTPÎ“HLâ€–F + n(â€–PTâŠ¥PÎ“HLâ€–F + â€–PTâŠ¥HLâ€–F )
â‰¤ (n+ 1)â€–PÎ“HLâ€–F + nâ€–PTâŠ¥HLâ€–F .
Using both â€–PÎ“HLâ€–F â‰¤ â€–PÎ“HLâ€–1 and â€–PTâŠ¥HLâ€–F â‰¤ â€–PTâŠ¥HLâ€–âˆ—, we obtain
â€–L0 +HLâ€–âˆ— + Î»â€–Sâ€²0 âˆ’HSâ€–1 â‰¥ â€–L0â€–âˆ— + Î»â€–Sâ€²0â€–1 +
(1
2
âˆ’ 1
n
)
â€–PTâŠ¥HLâ€–âˆ— +
(Î»
2
âˆ’ n+ 1
n2
)
â€–PÎ“HLâ€–1.
The claim follows from Î“âŠ¥ âˆ© T = {0}.
Lemma 7.3 Under the assumptions of Theorem 1.2, the assumption of Lemma 7.2 is satisfied with
high probability. That is, â€–PTPÎ“âŠ¥Mâ€–F â‰¤ nâ€–PTâŠ¥PÎ“âŠ¥Mâ€–F for all M .
33
Proof Set Ï0 = p0(1 âˆ’ Ï„) and M â€² = PÎ“âŠ¥M . Since Î“ âˆ¼ Ber(Ï0), Theorem 2.6 gives â€–PT âˆ’
Ïâˆ’10 PTPÎ“PT â€– â‰¤ 1/2 with high probability. Further, because â€–PÎ“PTM â€²â€–F = â€–PÎ“PTâŠ¥M â€²â€–F , we
have
â€–PÎ“PTM â€²â€–F â‰¤ â€–PTâŠ¥M â€²â€–F .
In the other direction,
Ïâˆ’10 â€–PÎ“PTM
â€²â€–2F = Ïâˆ’10 ã€ˆPTM
â€²,PTPÎ“PTM â€²)
= ã€ˆPTM â€²,PTM â€²ã€‰+ ã€ˆPTM â€², (Ïâˆ’10 PTPÎ“PT âˆ’ PT )M
â€²)
â‰¥ â€–PTM â€²â€–2F âˆ’
1
2
â€–PTM â€²â€–2F =
1
2
â€–PTM â€²â€–2F .
In conclusion, â€–PTâŠ¥M â€²â€–F â‰¥ â€–PÎ“PTM â€²â€–F â‰¥
Ï0
2 â€–PTM
â€²â€–F , and the claim follows since Ï02 â‰¥
1
n .
Thus far, our analysis shows that to establish our theorem, it suffices to construct a pair
(Y L,WS) obeying
ï£±ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£³
â€–PTâŠ¥Y Lâ€– < 1/4,
â€–PTY L âˆ’ UV âˆ—â€–F â‰¤ nâˆ’2,
PÎ“âŠ¥Y L = 0,
â€–PÎ“Y Lâ€–âˆ < Î»/4,
and
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£³
PTWS = 0,
â€–WSâ€– â‰¤ 1/4,
Pâ„¦WS = Î»sgn(Sâ€²0),
Pâ„¦âŠ¥obsW
S = 0,
â€–PÎ“WSâ€–âˆ â‰¤ Î»/4.
(7.3)
Indeed, by definition, Y L +WS obeys
Y L +WS = Î»(sgn(Sâ€²0) + F ),
where F is as in Lemma 7.2, and it can also be expressed as
Y L +WS = UV âˆ— +W + PTD,
where W and PTD are as in this lemma as well.
Construction of the dual certificate Y L. We use the golfing scheme to construct Y L. Think
of Î“ âˆ¼ Ber(Ï0) with Ï0 = p0(1âˆ’ Ï„) as âˆª1â‰¤jâ‰¤j0Î“j , where the sets Î“j âˆ¼ Ber(q) are independent, and
q obeys Ï0 = 1âˆ’ (1âˆ’q)j0 . Here, we take j0 = d3 log ne, and observe that q â‰¥ Ï0/j0 as before. Then
starting with Y0 = 0, inductively define
Yj = Yjâˆ’1 + qâˆ’1PÎ“jPT (UV âˆ— âˆ’ Yjâˆ’1),
and set
Y L = Yj0 = q
âˆ’1
âˆ‘
j
PÎ“jZjâˆ’1, Zj = (PT âˆ’ qâˆ’1PTPÎ“jPT )Zjâˆ’1. (7.4)
By construction, PÎ“âŠ¥Y L = 0. Now just as in Section (3.2), because q is sufficiently large, â€–Zjâ€– â‰¤
eâˆ’jâ€–UV âˆ—â€–âˆ and â€–Zjâ€–F â‰¤ eâˆ’j
âˆš
r, both inequality holding with large probability. The proof is now
identical to that in (2.5). First, the same steps show that
â€–PTâŠ¥Y Lâ€– â‰¤ C
âˆš
n log n
q
â€–UV âˆ—â€–âˆ = C â€²
âˆš
Âµr(log n)2
nÏ0
.
34
Whenever Ï0 â‰¥ C0 Âµr(logn)
2
n for a sufficiently large value of the constant C0 (which is possible
provided that Ïr in (1.6) is sufficiently small), this terms obeys â€–PTâŠ¥Y Lâ€– â‰¤ 1/4 as required.
Second,
â€–PTY L âˆ’ UV âˆ—â€–F = â€–Zj0â€–F â‰¤ eâˆ’3 logn
âˆš
r â‰¤ nâˆ’2.
And third, the same steps give
â€–Y Lâ€–âˆ â‰¤ qâˆ’1â€–UV âˆ—â€–âˆ
âˆ‘
j
eâˆ’j â‰¤ 3(1âˆ’ eâˆ’1)
âˆš
Âµr(log n)2
Ï20n
2
.
Now it suffices to bound the right-hand side by Î»4 =
1
4
âˆš
1âˆ’Ï„
nÏ0
. This is automatic when Ï0 â‰¥
C0
Âµr(logn)2
n whenever C0 is sufficiently large and, thus, the situation is as before. In conclusion, we
have established that Y L obeys (7.3) with high probability.
Construction of the dual certificate WS. We first establish that with high probability,
â€–PTPâ„¦â€– â‰¤
âˆš
Ï„ â€²p0, Ï„
â€² = Ï„ + Ï„0, (7.5)
where Ï„0(Ï„) is a continuous function of Ï„ approaching zero when Ï„ approaches zero. In other words,
the parameter Ï„ â€² may become arbitrary small constant by selecting Ï„ small enough. This claim is
a straight application of Corollary 2.7. We also have
â€–Pâ„¦P(T+â„¦âŠ¥obs)Pâ„¦â€– â‰¤ 2Ï„
â€². (7.6)
with high probability. This second claim uses the identity
Pâ„¦P(T+â„¦âŠ¥obs)Pâ„¦ = Pâ„¦PT (PTPâ„¦obsPT )
âˆ’1PTPâ„¦.
This is well defined since the restriction of PTPâ„¦obsPT to T is invertible. Indeed, Theorem 2.6 gives
PTPâ„¦obsPT â‰¥
p0
2 PT and, therefore, â€–(PTPâ„¦obsPT )
âˆ’1â€– â‰¤ 2pâˆ’10 . Hence,
â€–Pâ„¦P(T+â„¦âŠ¥obs)Pâ„¦â€– â‰¤ 2p
âˆ’1
0 â€–Pâ„¦PT â€–
2,
and (7.6) follows from (7.5).
Setting E = sgn(Sâ€²0), this allows to define W
S via
WS = Î»(I âˆ’ P(T+â„¦âŠ¥obs))(Pâ„¦ âˆ’ Pâ„¦P(T+â„¦âŠ¥obs)Pâ„¦)
âˆ’1E
:= (I âˆ’ P(T+â„¦âŠ¥obs))(W
S
0 +W
S
1 ),
where WS0 = Î»E, and W
S
1 = RE with R =
âˆ‘
kâ‰¥1(Pâ„¦P(T+â„¦âŠ¥obs)Pâ„¦)
k. The operator R is self-
adjoint and obeys â€–Râ€– â‰¤ 2Ï„ â€²1âˆ’2Ï„ â€² with high probability. By construction, PTW
S = Pâ„¦âŠ¥obsW
S = 0
and Pâ„¦WS = Î»sgn(Sâ€²0). It remains to check that both events â€–WSâ€– â‰¤ 1/4 and â€–PÎ“WSâ€–âˆ â‰¤ Î»/4
hold with high probability.
Control of â€–WSâ€–. For the first term, we have â€–(I âˆ’ P(T+â„¦âŠ¥obs))W
S
0 â€– â‰¤ â€–WS0 â€– = Î»â€–Eâ€–. Because
the entries of E are i.i.d. and take the value Â±1 each with probability p0Ï„/2, and the value 0 with
probability 1âˆ’ p0Ï„ , standard arguments give
â€–Eâ€– â‰¤ 4
âˆš
np0(Ï„ + Ï„0)
35
with large probability. Since Î» = 1/
âˆš
p0n, â€–WS0 â€– â‰¤ 4
âˆš
Ï„ + Ï„0 < 1/8 with high probability, provided
Ï„ is small enough.
For the second term, â€–(Iâˆ’P(T+â„¦âŠ¥obs))W
S
1 â€– â‰¤ Î»â€–REâ€–, and the same covering argument as before
gives
P(Î»â€–R(E)â€– > t) â‰¤ 2Ã— 62n exp
(
âˆ’ t
2
2Î»2Ïƒ2
)
+ P(â€–Râ€– â‰¥ Ïƒ).
Since Î» = 1/
âˆš
np0 this shows that â€–WSâ€– â‰¤ 1/4 with high probability, since one can always choose
Ïƒ, or equivalently Ï„ â€² = Ï„ + Ï„0, sufficiently small.
Control of â€–PÎ“WSâ€–âˆ. For (i, j) âˆˆ Î“, we have
WSij = ã€ˆeieâˆ—j ,WSã€‰ = Î»ã€ˆX(i, j), Eã€‰,
where
X(i, j) = (Pâ„¦ âˆ’ Pâ„¦P(T+â„¦âŠ¥obs)Pâ„¦)
âˆ’1Pâ„¦P(T+â„¦âŠ¥obs)âŠ¥eie
âˆ—
j .
The same strategy as before gives
P
(
sup
(i,j)âˆˆG
|WSij | >
Î»
4
)
â‰¤ 2n2 exp
(
âˆ’ 1
8Ïƒ2
)
+ P
(
sup
(i,j)âˆˆG
â€–X(i, j)â€–F > Ïƒ
)
.
It remains to control the Frobenius norm of X(i, j). To do this, we use the identity
Pâ„¦P(T+â„¦âŠ¥obs)âŠ¥eie
âˆ—
j = Pâ„¦PT (PTPâ„¦obsPT )
âˆ’1PT eieâˆ—j ,
which gives
â€–Pâ„¦P(T+â„¦âŠ¥obs)âŠ¥eie
âˆ—
jâ€–F â‰¤
âˆš
4Ï„ â€²
p0
â€–PT eieâˆ—jâ€–F â‰¤
âˆš
8ÂµrÏ„ â€²
np0
with high probability. This follows from the fact that â€–(PTPâ„¦obsPT )âˆ’1â€– â‰¤ 2p
âˆ’1
0 and â€–Pâ„¦PT â€– â‰¤âˆš
p0Ï„ â€² as we have already seen. Since we also have â€–(Pâ„¦ âˆ’ Pâ„¦P(T+â„¦âŠ¥obs)Pâ„¦)
âˆ’1â€– â‰¤ 11âˆ’2Ï„ â€² with high
probability,
sup
(i,j)âˆˆÎ“
â€–X(i, j)â€–F â‰¤
1
1âˆ’ 2Ï„ â€²
âˆš
8ÂµrÏ„ â€²
np0
.
This shows that â€–PÎ“WSâ€–âˆ â‰¤ Î»/4 if Ï„ â€², or equivalently Ï„ , is sufficiently small.
Acknowledgements
E. C. is supported by ONR grants N00014-09-1-0469 and N00014-08-1-0749 and by the Waterman
Award from NSF. Y. M. is partially supported by the grants NSF IIS 08-49292, NSF ECCS 07-
01676, and ONR N00014-09-1-0230. E. C. would like to thank Deanna Needell for comments on an
earlier version of this manuscript. We would also like to thank Zhouchen Lin (MSRA) for his help
with the ALM algorithm, and Hossein Mobahi (UIUC) for his help with some of the simulations.
36
References
[1] R. Basri and D. Jacobs. Lambertian reflectance and linear subspaces. IEEE Trans. on Pattern Analysis
and Machine Intelligence, 25(2):218â€“233, 2003.
[2] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183â€“202, Mar 2009.
[3] S. Becker, J. Bobin, and E. J. CandeÌ€s. NESTA: A fast and accuract first-order method for sparse
recovery. preprint, 2009.
[4] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.
Neural Computation, 15(6):1373â€“1396, 2003.
[5] D.P. Bertsekas. Constrained Optimization and Lagrange Multiplier Method. Academic Press, 1982.
[6] J. Cai, E. J. CandeÌ€s, and Z. Shen. A singular value thresholding algorithm for matrix completion.
preprint, 2008.
[7] E. J. CandeÌ€s and Y. Plan. Matrix completion with noise. Proceedings of the IEEE (to appear), 2009.
[8] E. J. CandeÌ€s and B. Recht. Exact matrix completion via convex optimzation. Found. of Comput. Math.,
9:717â€“772, 2009.
[9] E. J. CandeÌ€s, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction from
highly incomplete frequency information. IEEE Trans. Inform. Theory, 52(2):489â€“509, 2006.
[10] E. J. CandeÌ€s and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE
Trans. Inf. Theory (to appear), 2009.
[11] V. Cevher, A. Sankaranarayanan, M. Duarte, D. Reddy, R. Baraniuk, and R. Chellappa. Compres-
sive sensing for background subtraction. In Proceedings of European Conference on Computer Vision
(ECCV), 2009.
[12] V. Chandrasekaran, S. Sanghavi, P. Parrilo, and A. Willsky. Rank-sparsity incoherence for matrix
decomposition. preprint, 2009.
[13] S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit. SIAM Review, 43(1):129â€“
159, 2001.
[14] S. Dewester, S. Dumains, T. Landauer, G. Furnas, and R. Harshman. Indexing by latent semantic
analysis. Journal of the Society for Information Science, 41(6):391â€“407, 1990.
[15] C. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika,
1:211â€“218, 1936.
[16] M. Fazel, H. Hindi, and S. Boyd. Log-det heuristic for matrix rank minimization with applications to
Hankel and Euclidean distance matrices. In Proceedings of the American Control Conference, pages
2156â€“2162, Jun 2003.
[17] M. Fischler and R. Bolles. Random sample consensus: A paradigm for model fitting with applications
to image analysis and automated cartography. Communications of the ACM, 24:381â€“385, 1981.
[18] A. Georghiades, P. Belhumeur, and D. Kriegman. From few to many: Illumination cone models for
face recognition under variable lighting and pose. IEEE Trans. on Pattern Analysis and Machine
Intelligence, 23(6), 2001.
[19] R. Gnanadesikan and J. Kettenring. Robust estimates, residuals, and outlier detection with multire-
sponse data. Biometrics, 28:81â€“124, 1972.
[20] D. Goldfarb and S. Ma. Convergence of fixed point continuation algorithms for matrix rank minimiza-
tion. preprint, 2009.
37
[21] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming (web page and
software). http://stanford.edu/âˆ¼boyd/cvx, June 2009.
[22] D. Gross. Recovering low-rank matrices from few coefficients in any basis. CoRR, abs/0910.1879, 2009.
[23] D. Gross, Y-K. Liu, S. T. Flammia, S. Becker, and J. Eisert. Quantum state tomography via compressed
sensing. CoRR, abs/0909.3304, 2009.
[24] T. Hey, S. Tansley, and K. Tolle. The Fourth Paradigm: Data-Intensive Scientific Discovery. Microsoft
Research, 2009.
[25] H. Hotelling. Analysis of a complex of statistical variables into principal components. Journal of
Educational Psychology, 24:417â€“441, 1933.
[26] P. Huber. Robust Statistics. Wiley and Sons, 1981.
[27] I. Jolliffe. Principal Component Analysis. Springer-Verlag, 1986.
[28] Q. Ke and T. Kanade. Robust `1-norm factorization in the presence of outliers and missing data. In
Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition, 2005.
[29] S. Kontogiorgis and R. Meyer. A variable-penalty alternating direction method for convex optimization.
Mathematical Programming, 83:29â€“53, 1989.
[30] M. Ledoux. The Concentration of Measure Phenomenon. American Mathematical Society, 2001.
[31] L. Li, W. Huang, I. Gu, and Q. Tian. Statistical modeling of complex backgrounds for foreground
object detection. IEEE Transactions on Image Processing, 13(11):1459â€“1472, 2004.
[32] Z. Lin, M. Chen, L. Wu, and Y. Ma. The augmented Lagrange multiplier method for exact recovery of
a corrupted low-rank matrices. Mathematical Programming, submitted, 2009.
[33] Z. Lin, A. Ganesh, J. Wright, L. Wu, M. Chen, and Y. Ma. Fast convex optimization algorithms for
exact recovery of a corrupted low-rank matrix. In Computational Advances in Multi-Sensor Adaptive
Processing (CAMSAP), 2009.
[34] P. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM Journal
on Numerical Analysis, 16(6):964â€“979, 1979.
[35] Z. Liu and L. Vandenberge. Interior-point method for nuclear norm approximation with application to
system identification. SIAM Journal on Matrix Analysis and Applications, 31(3):1235â€“1256, 2009.
[36] S. Ma, D. Goldfarb, and L. Chen. Fixed point and Bregman iterative methods for matrix rank mini-
mization. preprint, 2009.
[37] Y. Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2).
Soviet Mathematics Doklady, 27(2):372â€“376, 1983.
[38] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1), 2005.
[39] Y. Nesterov. Gradient methods for minimizing composite objective functions. Technical Report - CORE
- Universite Catholique de Louvain, 2007.
[40] Netflix, Inc. The Netflix prize. http://www.netflixprize.com/.
[41] S. Osher, M. Burger, D. Goldfarb, J. Xu, and W. Yin. An iterative regularization method for total
variation-based image restoration. Multiscale Modeling and Simulation, 4:460â€“489, 2005.
[42] C. Papadimitriou, P. Rghavan, H. Tamaki, and S. Vempala. Latent semantic indexing, a probabilistic
analysis. Journal of Computer and System Sciences, 61(2):217â€“235, 2000.
[43] A. Montanari R. Keshavan and S. Oh. Matrix completion from a few entries. 2009.
38
[44] B. Recht. A simpler approach to matrix completion. CoRR, abs/0910.0651, 2009.
[45] B. Recht, M. Fazel, and P. Parillo. Guaranteed minimum rank solution of matrix equations via nuclear
norm minimization. submitted to SIAM Review, 2008.
[46] J. Tenenbaum, V. de Silva, and J. Langford. A global geometric framework for nonlinear dimensionality
reduction. Science, 290(5500):2319â€“2323, 2000.
[47] F. De La Torre and M. Black. A framework for robust subspace learning. International Journal on
Computer Vision, 54:117â€“142, 2003.
[48] R. Vershynin. Math 280 lecture notes. Available at http://www-stat.stanford.edu/~dneedell/280.
html, 2007.
[49] W. Yin, E. Hale, and Y. Zhang. Fixed-point continuation for `1-minimization: Methodology and
convergence. preprint, 2008.
[50] W. Yin, S. Osher, D. Goldfarb, and J. Darbon. Bregman iterative algorithms for `1-minimization with
applications to compressed sensing. SIAM Journal on Imaging Sciences, 1(1):143â€“168, 2008.
[51] X. Yuan and J. Yang. Sparse and low-rank matrix decomposition via alternating direction methods.
preprint, 2009.
[52] Z. Zhou, A. Wagner, H. Mobahi, J. Wright, and Y. Ma. Face recognition with contiguous occlusion
using Markov random fields. In Proceedings of International Conference on Computer Vision (ICCV),
2009.
39

