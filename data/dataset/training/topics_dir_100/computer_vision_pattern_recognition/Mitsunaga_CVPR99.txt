Radiometric Self Calibration * 
Tomoo Mitsunaga 
Media Processing Laboratories 
Sony Corporation 
mitsunag @av.crl.sony.co.jp 
Abstract 
A simple algorithm is described that computes the ra- 
diometric response function of an imaging system, from 
images of an arbitrar), scene taken using different ex- 
posures. The exposure is varied by changing either the 
aperture setting or the shutter speed. The algorithm 
does not require precise estimates of the exposures used. 
Rough estimates of the ratios of the exposures (e.g. F- 
number settings on an inexpensive lens) are sufJicient 
for accurate recovery of the response function as well 
as the actual exposure ratios. The computed response 
function is used to fuse the multiple images into a sin- 
gle high dynamic range radiance image. Robustness is 
tested using a variety of scenes and cameras as well as 
noisy synthetic images generated using IO0 randomly 
selected response curves. Automatic rejection of im- 
age areas that have large vignetting effects or temporal 
scene variations make the algorithm applicable to not 
just photographic but also video cameras. Code for the 
algorithm and several results are publicly available at 
h t tp://w w w. cs. col um bia . edu/CAVE/. 
1. Radiometric Response Function 
Brightness images are inputs to virtually all computer 
vision systems. Most vision algorithms either explic- 
itly or indirectly assume that brightness measured by the 
imaging system is linearly related to scene radiance. In 
practice, this is seldom the case. Almost always, there 
exists a non-linear mapping between scene radiance and 
measured brightness. We will refer to this mapping as 
the radiometric response function of the imaging sys- 
tem. The goal of this paper is to present a convenient 
technique for estimating the response function. 
Let us begin by exploring the factors that influence the 
response function. In a typical image formation system, 
image irradiance E is related to scene radiance L as [SI: 
n d  
E = L - ( - ) 2 ~ ~ ~ 4 q 5 ,  4 h  
where, h is the focal length of the imaging lens, d is 
the diameter of its aperture and q5 is the angle subtended 
‘This work was supported in parts by an ONRDARPA MURl grant 
under ONR contract No. N00014-97-1-0553 and a David and Lucile 
Packard Fellowship. Tomoo Mitsunaga is supported by the Sony Cor- 
poration. 
0-7695-0149-4/99 $10.00 0 1999 IEEE 
374 
Shree K .  Nayar 
Department of Computer Science 
Columbia University 
nayar@ cs. Columbia. edu 
by the principal ray from the optical axis. If our imag- 
ing system were ideal, the brightness it records would 
be I = E t ,  where, t is the time the image detector is 
exposed to the scene. This ideal system would have a 
linear radiometric response: 
I = L k e ,  (2) 
where, k = cos4 41 h2 and e = (n d 2  / 4) t .  We will 
refer to e as the exposure of the image, which could be 
altered by varying either the aperture size d or the dura- 
tion of exposure t . 
An image detector such as a CCD is designed to produce 
electrical signals that are linearly related to I [14]. Un- 
fortunately, there are many other stages to image acqui- 
sition that introduce non-linearities. Video cameras of- 
ten include some form of “gamma” mapping (see [ 131). 
Further, the image digitizer, inclusive of A/D conversion 
and possible remappings, could introduce its own non- 
linearities. In the case of film photography, the film it- 
self is designed to have a non-linear response [9]. Such 
film-like responses are often built into off-the-shelf digi- 
tal cameras. Finally, the development of film into a slide 
or a print and its scanning into a digital format typically 
introduces further non-linearities (see [ 5 ] ) .  
The individual sources of radiometric non-linearities are 
not of particular relevance here. What we d o  know is 
that the final brightness measurement M produced by 
an imaging system is related to the (scaled) scene ra- 
diance I via a response function g, i.e. M = g ( I  ). 
To map all measurements M to scaled radiance values 
I ,  we need to find the inverse function f = g- l ,  where 
I = f ( M ). Recovery off is therefore the radiometric 
calibration problem. It is common practice to estimate f 
by showing the imaging system a uniformly lit calibra- 
tion chart, such as the Macbeth chart [4], which includes 
patches of known relative reflectances. The known rel- 
ative radiances I ,  of the patches and the corresponding 
measurements M ,  are samples that are interpolated to 
obtain an approximation to f .  It is our objective to avoid 
such a calibration procedure which is only suited to con- 
trolled environment. 
2. Calibration Without Charts 
Recent work by Mann and Picard [ l l ]  and Debevec 
and Malik [5 ]  has demonstrated how the radiometric re- 
sponse function can be estimated using images of ar- 
bitrary scenes taken under different known exposures. 
While the measured brightness values change with ex- 
posure, scene radiance values L remain constant. This 
observation permits the estimation of the inverse func- 
tion f without prior knowledge of scene radiance. Once 
f has been determined, the images taken under different 
exposures can be fused into a single high dynamic range 
radiance image'. 
Mann and Picard [ 1 11 use the function M = cr + p IT 
to model the response curve 9. The bias parameter cr is 
estimated by taking an image with the lens covered, and 
the scale factor p is set to an arbitrary value. Consider 
two images taken under different exposures with known 
ratio R = el /e2.  The measurement M ( I )  in the first im- 
age ( I  is unknown) produces the measurement M ( R I )  
in the second. A pixel with brightness M ( R I )  is sought 
in the first image that would then produce the brightness 
M( R21) in the second image. This search process is re- 
peated to obtain the measurement series M ( I ) ,  M (  R I ) ,  
...., M ( R n I ) .  Regression is applied to these samples to 
estimate the parameter y. Since the model used by Mann 
and Picard is highly restrictive, the best one can hope for 
in the case of a general imaging system is a qualitative 
calibration result. Nonetheless, the work of Mann and 
Picard is noteworthy in that it brings to light the main 
challenges of this general approach to radiometric cali- 
bration. 
Debevec and Malik [5] have developed an algorithm that 
can be expected to yield more accurate results. They use 
a sequence of high quality digital photographs (about 
11) taken using precisely known exposures. This al- 
gorithm is novel in that it does not assume a restric- 
tive model for the response function, and only requires 
that the function be smooth. If p and q denote pixel 
and exposure indices, we have MP,, = g(Ep.tq) or 
In f ( M p , p )  = In Ep + In t,. With MP,, and t, known, 
the algorithm seeks to find discrete sample of Inf( MP,,) 
and scaled irradiances Ep. These results are used to 
compute a high dynamic range radiance map. The al- 
gorithm of Debevec and Malik is well-suited when the 
images themselves are not noisy and precise exposure 
times are available. 
Though our goals are similar to those of the above inves- 
tigators, our approach is rather different. We use a flex- 
ible parametric model which can accurately represent a 
wide class of response functions encountered in practice. 
On the other hand, the finite parameters of the model al- 
low us to recover the response function without precise 
exposure inputs. As we show, rough estimates (such as 
F-number readings on a low-quality lens) are sufficient 
to accurately calibrate the system as well as recover the 
actual exposure ratios. To ensure that our algorithm can 
lFor imaging systems with known response functions, the problem 
of fusing multiple images taken under different exposures has been 
addressed by several others (see [3], [IO], [7] for examples). In [2], a 
CMOS imaging chip is described where each pixel measures the expo- 
sure time required to attain a fixed charge accumulation level. These 
exposure times can be mapped to a high dynamic range image. 
1. 
0 0.2 0.4 0.6 0.8 1 
M 
Figure 1 : Response functions of a few popular films and video 
cameras provided by their manufacturers. These examples il- 
lustrate that a high-order polynomial may be used to model the 
response function. 
be applied to video systems, where it is easier to vary the 
aperture setting than the exposure time, we have imple- 
mented a pre-processing algorithm that rejects measure- 
ments with large vignetting effects and temporal changes 
during image acquisition. The recovered response func- 
tion is used to fuse the multiple images into a high qual- 
ity radiance map. We have conducted extensive testing 
of our algorithm by using noisy synthetic data. In addi- 
tion, we have compared our calibration results for real 
images with those produced using calibration charts. 
3. A Flexible Radiometric Model 
It is worth noting that though the response curve can vary 
appreciably from one imaging system to the next, it is 
not expected to have exotic forms. The primary reason 
is that the detector output (be it film or solid state) is 
monotonic or at least semi-monotonic. Unless unusual 
mappings are intentionally built into the remaining com- 
ponents of the imaging system, measured brightness ei- 
ther increases or stays constant with increase in scene ra- 
diance (or exposure). This is illustrated by the response 
functions of a few popular systems shown in Figure 1. 
Hence, we claim that virtually any response function can 
be modeled using a high-order polynomial: 
N 
I = f ( M )  = C h M n .  (3) 
n=o 
The minimum order of the polynomial required clearly 
depends on the response function itself. Hence, calibra- 
tion could be viewed as determining the order N in ad- 
dition to the coefficients c,,. 
4. The Self-Calibration Algorithm 
Consider two images of a scene taken using two differ- 
ent exposures, e,  and e,+,,  where R,,,,,, = e, /e4+,. 
The ratio of the scaled radiance at any given pixel p can 
375 
written using expression (2) as: 
Hence, the response function of the imaging system is 
related to the exposure ratio as: 
We order our images such that e, < e,+? and hence 
0 < R,,,,, < 1. Substituting our polynomial model for 
the response function, we have: 
The above relation may be viewed as the basis for 
the joint recovery of the response function and the ex- 
posure ratio. However, an interesting ambiguity sur- 
faces at this point. Note that from ( 5 )  we also have 
in general, f and R can only be recovered up to an un- 
known exponent U. In other words, an infinite number 
of f  -R pairs would satisfy equation (5). 
Interestingly, this u-ambiguity is greatly alleviated by 
the use of the polynomial model. Note that, i f f  is a 
polynomial, f" can be a polynomial only if u = U 
or U = 1 / ~  where v is a natural number, i.e. u = 
1/3,1/2,1,2,3, .) . This by no means implies that, 
for any given polynomial, all these multiple solutions 
must exist. For instance, i f f  ( M )  = M 3 ,  u = 1/2 does 
not yield a polynomial. On the other hand, u = 1/3 does 
result in f ( M )  = M which, in turn, can have its own u- 
ambiguities. In any case, the multiple solutions that arise 
are well-spaced with respect to each other. Shortly, the 
benefits of this restriction will become clear. 
For now, let us assume that the exposure ratios R,,,+] 
are known to us. Then, the response function can be 
recovered by formulating an error function that is the 
sum of the squares of the errors in expression (6): 
(f (Mp, , ) / f (Mp,q+ l  1)" = Rq,q+lU. This implies that, 
In most inexpensive imaging systems, photographic or 
video, it  is difficult to obtain accurate estimates of the 
exposure ratios R4,,+]. The user only has access to the 
F-number of the imaging lens or the speed of the shutter. 
In consumer products these readings can only be taken 
as approximations to the actual values. In such cases, 
the restricted u-ambiguity provided by the polynomial 
model proves valuable. Again, consider the case of two 
images. If the initial estimate for the ratio provided by 
the user is a reasonable guess, the actual ratio is easily 
determined by searching in the vicinity of the initial esti- 
mate; we search for the R that produces the c, that min- 
imize E .  Since the solution for cn is linear, this search is 
very efficient. 
However, when more than two images are used the di- 
mensionality of the search for R,,,+] is Q - 1. When 
Q is large, the search can be time consuming. For such 
cases, we use an efficient iterative scheme where the cur- 
rent ratio estimates Rq,q+ j  ( k - l )  are used to compute the 
next set of coefficients c , ( ~ ) .  These coefficients are then 
used to update the ratio estimates using (6): 
where, the initial ratio estimates I?,,,+, (O)  are provided 
by the user. The algorithm is deemed to have converged 
when: 
where E is a small number. 
It is hard to embed the recovery of the order N into the 
above algorithm in an elegant manner. Our approach 
is to place an upper bound on N and run the algorithm 
repeatedly to find the N that gives the lowest error E .  In 
our experiments, we have used an upper bound of N=10. 
5. Evaluation: Noisy Synthetic Data 
Shortly, we will present experiments with real scenes 
that demonstrate the performance of our calibration al- 
gorithm. However, a detailed analysis of the behavior 
For this, 100 monotonic response functions were gen- 
a fifth-order polynomial. Using each response function, 
erated using random numbers for the coefficients cn of 
four synthetic images were generated with random ra- 
diance values and random exposure ratios in the range 
0.45 5 R 5 0.55. This range of ratio values is of par- 
ticular interest to us because in almost all commercial 
C N  = Imaz - +. ( 8 )  lenses and cameras a single step in the F-number setting 
or shutter speed setting results in an exposure ratio of ap- 
proximately 0.5. The pixel values were normalized such 
that 0 L M L 1. Next, normally distributed noise with 
0 = 0.005 was added to each pixel value. This translates 
to U = 1.275 gray levels when 0 5 M 5 255, as in the 
case of a typical imaging system. The images were then 
* 
Q - 1  P N 
q=i p i  n=o n=o 
E = 1 [ 5 c,Mp'qn - ""+' cnMp7q+' ' of the algorithm requires the use of noisy synthetic data. 
(7) 
where, Q is the total number of images used. If we nor- 
malize all measurements such that 0 5 M 5 1 and fix 
the indeterminable scale using f(1) = I,,,, we get the 
additional constraint: 
N-1 
n=o 
The response function coefficients are determined by 
solving the system of linear equations that result from 
setting: 
(9) = 0. 
a& 
dcn 
-
376 
1.00E-05 
/ i  
iterative Updating 1 \ / I  1.00E-06 1.00E-07 
0 0 2  0 4  O B  t M O 6  
Figure 2: Self-calibration was tested using 100 randomly geri- 
erated response functions. Here a few of the recovered (solid) 
and actual (dots) response functions are shown. In each case, 
four noisy test images were generated using random exposure 
ratios between image pairs in the range 0.45 5 R 5 0.55. 
The initial ratio estimates were chosen to be 0.5. 
8 '" 
E 
2 0  
H 
p .  
I 
$ 6  
LI 
z - 
LI 
il 
2 2  b 
E o  
0 10 20 m 40 x) 60 70 Eo 90 Im 
TRIAL NUMBER 
Figure 3: The percentage average error between actual and es- 
timated response curves for the 100 synthetic trials. The max- 
imum error was found to be 2.7%. 
quantized to have 256 discrete gray levels. The algo- 
rithm was applied to each set of four images using initial 
exposure ratios of Rq,q+,  (O) = 0.5. 
All 100 response functions and exposure ratios were ac- 
curately estimated. Figure 2 shows a few of the actual 
(dots) and computed (solid) response functions. As can 
be seen, the algorithm is able to recover a large variety 
of response functions. In Figure 3 the percentage of the 
average error (over all M values) between the actual and 
estimated curves are shown for the 100 cases. Despite 
the presence of noise, the worst-case error was found to 
be less than 2.7%. 
Figure 4 shows the error & plotted as a function of ex- 
posure ratio R. In this example, only two images are 
used and the actual and initial exposure ratios are 0.7 
and 0.625, respectively. The multiple local minima cor- 
respond to solutions that arise from the u-ambiguity de- 
scribed before. The figure illustrates how the ratio value 
converges from the initial estimate to the final one. In all 
our experiments, the algorithm converged to the correct 
solution in less than 10 iterations. 
E 
1.00E-08 
1.00E-09 
1.00E-10 
0 1  0 3  0 5  0 7  0 9  
Figure 4: An example that Lows convergence of the algo- 
rithm to the actual exposure ratio (0.7) from a rough initial 
ratio estimate (0.625), in the presence of the u-ambiguity. 
6. Implementation 
We now describe a few issues that need to be addressed 
while implementing radiometric self-calibration. 
6.1. Reducing Video Noise 
Particularly in the context of video, it is important to en- 
sure that the data that is provided to the self-calibration 
algorithm has minimal noise. To this end, we have im- 
plemented a pre-processing step that uses temporal and 
spatial averaging to obtain robust pixel measurements. 
Noise arises from three sources, namely, electrical read- 
out from the camera, quantization by the digitizer hard- 
ware [6 ] ,  and motion of scene objects during data acqui- 
sition. The random component of the former two noise 
sources can be reduced by temporal averaging o f t  im- 
ages (typically t = 100). The third source can be omit- 
ted by selecting pixels of spatially flat area. To check 
spatial flatness, we assume normally distributed noise 
N(0 ,  c'). Under this assumption, if an area is spatially 
flat, sS2/a2 can be modeled by the x2 distribution [ 121, 
where S is the spatial variance of s pixel values (typi- 
cally s = 5 x 5). We approximate S2 with the spatial 
variance 0: of the temporally averaged pixel values, and 
0' with the temporal variance 0:. Therefore, only those 
pixels are selected that pass the following test: 
where, $J is the rejection ratio which is set to 0.05. When 
temporal averaging is not applied, t=l and ot is set to 
0.01. 
6.2. Vignetting 
In most compound lenses vignetting increases with the 
aperture size [ I ] .  This introduces errors in the measure- 
ments which are assumed to be only effected by expo- 
sure changes. Vignetting effects are minimal at the cen- 
ter of the image and increase towards the periphery. We 
have implemented an algorithm that robustly detects pix- 
els that are corrupted by vignetting. Consider two con- 
secutive images q and q + 1. Corresponding brightness 
measurements Mp,q and Mp,q+l are plotted against each 
other. In the absence of vignetting, all pixels with the 
same measurement value in one image should produce 
377 
Figure 5:  Fusing multiple images taken under different expo- 
sures into a single scaled radiance image. 
equal measurement values in the second image, irrespec- 
tive of their locations in the image. The vignetting-free 
area for each image pair is determined by finding the 
smallest image circle within which the Mp,q-Mp,q+l plot 
is a compact curve with negligible scatter. 
6.3. 
Once the response function of the imaging system has 
been computed, the Q images can be fused into a sin- 
gle high dynamic range radiance image, as suggested in 
[ 1 I ]  and [SI. This procedure is simple and is illustrated 
in Figure 5. Three steps are involved. First, each mea- 
surement Mp,q is mapped to its scaled radiance value 
Ip,q using the computed response function f. Next, the 
scaled radiance is normalized by the scaled exposure .Eq 
so that all radiance value end up with the same effec- 
tive exposure. Since the absolute value of each .Eq is 
not important for normalization, we simply compute the 
scaled exposures so that their arithmetic mean is equal 
to 1. The final radiance value at a pixel is then computed 
as a weighted average of its individual normalized radi- 
ance values. In previous work, a hat function [SI and 
the gradient of the response function [ 1 I ]  were used for 
the weighting function. Both these choices are some- 
what ad-hoc, the latter less so than the former. Note that 
a measurement can be trusted most when its signal-to- 
noise ratio (SNR) as well as its sensitivity to radiance 
changes are maximum. The SNR for the scaled radiance 
value I is: 
High Dynamic Range Radiance Images 
where, CTN ( M )  is the standard deviation of the measure- 
ment noise. Using the assumption that the noise ON in 
(13) is independent of the measurement pixel value M ,  
we can define the weighting function as: 
6.4. Handling Color 
In the case of color images, a separate response func- 
tion is computed for each color channel (red, green and 
blue). This is because, in principle, each color chan- 
nel could have its own response to irradiance. Since 
each of the response functions can only be determined 
up to a scale factor, the relative scalings between the 
three computed radiance images remain unknown. We 
resolve this problem by assuming that the three response 
functions preserve the chromaticity of scene points. Let 
the measurements be denoted as M = [Mr, M,, &IT 
and the computed scaled radiances be I = [ I r ,  Igl & I T .  
Then, the color-corrected radiance values are I, = 
[&Ir, kgIg ,  kbIbIT where, k,./kb and kg/kb are deter- 
mined by applying least-squares minimization to the 
chromaticity constraint IC/ 1 1  I, I I =  M/ 1 1  M 1 1 .  
7. Experimental Results 
The source code for our self-calibration algorithm 
and several experimental results are made available at 
http://www.cs.columbia.edu/CAVW. Here, due to lim- 
ited space, we will present just a couple of examples. 
Figure 6 shows results obtained using a Canon Optura 
video camera. In this example, the gray scale output of 
the camera was used. Figure 6(a) shows 4 of the 5 im- 
ages of an outdoor scene obtained using five different 
F-number settings. As can be seen, the low exposure 
images are too dark in areas of low scene radiance and 
the high exposure ones are saturated in bright scene re- 
gions. To reduce noise, temporal averaging using t = l O O  
and spatial averaging using s=3x3 were applied. Next, 
automatic vignetting detection was applied to the 5 aver- 
aged images. Figure 6(b) shows the final selected pixels 
(in black) for one of the image pairs. 
The solid line in Figure 6(d) is the response function 
computed from the 5 scene images. The initial expo- 
sure ratio estimates were set to 0.5,0.25,0.5 and 0.5 and 
the final computed ratios were 0.419, 0.292, 0.570 and 
0.5 12. These results were verified using a Macbeth cali- 
bration chart with patches of known relative reflectances 
(see Figure 6(c)). The chart calibration results are shown 
as dots which are in strong agreement with the computed 
response function. Figure 6(e) shows the radiance im- 
age computed using the 5 input images and the response 
function. Since conventional printers and displays have 
dynamic ranges of 8 or less bits, we have applied his- 
togram equalization to the radiance image to bring out 
the details within it. The small windows on the sides 
of the radiance image show further details; histogram 
equalization was applied locally within each window. 
Figure 7 shows two results obtained using a Nikon 2020 
film camera. In each case 5 images were captured of 
which only 4 are shown in Figures 7(a) and 7(d). In the 
first experiment, Kodachrome slide film with IS0 200 
was used. These slides were taken using F-number = 8 
and approximate (manually selected) exposure times of 
1/30, 1/15, 1/8, 1/2 and 1 (seconds). In the second ex- 
periment, the pictures were taken with the same camera 
but using F-number = 1 1 and IS0 100 slide film. The ex- 
posure settings were 1/500, 11250, 11125, 1/60 and 1/30. 
The developed slides were scanned using a Nikon LS- 
35 I OAF scanner that produces a 24 bit image (8 bits per 
color channel). 
The self-calibration algorithm was applied separately to 
each color channel (R, G ,  B), using initial ratio estimates 
of 0.5, 0.25, 0.5 and 0.5 in the first experiment and all 
378 
1 
0.8 
0.6 
0.4 
0.2 
0 
I 
0 0.2 0.4 0.6 0.8 1 
( e )  
Figure 6: (a) Self-calibration results for gray scale video images taken using a Canon Optura camera. (b) Temporal averaging, 
spatial averaging and vignetting detection are used to locate pixels (shown in black) that produce robust measurements. (c) The 
self-calibration results are verified using a uniformly lit  Macbeth color chart with patches of known reflectances. (d) The computed 
response function (solid line) is in strong agreement with the chart calibration results (dots). (e) The computed radiance image 
is histogram equalized to convey some of the details it  includes. The image windows on the two sides of the radiance image are 
locally histogram equalized to bring forth further details. 
ratios set to 0.5 in the second. The three (R, G ,  B) com- 
puted response functions are shown in Figures 7(b) and 
7(e) . A radiance image was computed for each color 
channel and then the three images were scaled to pre- 
serve chromaticity, as described in section 6.4.. The final 
radiance images shown in Figure 7(c) and 7(f) are his- 
togram equalized. As before, the small windows within 
the radiance images show further details brought out by 
local histogram equalization. Noteworthy are the details 
of the lamp and the chain on the wooden window in Fig- 
ure 7(c) and the clouds in the sky and the logs of wood 
inside the clay oven in Figure 7(f). 
For the exploration of high dynamic range images, we 
have developed a simple interactive visualization tool, 
referred to as the “Detail Brush.” It is a window of ad- 
justable size and magnification that can be slided around 
the image while histogram equalization within the win- 
dow is performed in real time. This tool is also available 
at h ttp://w ww. cs. colurn bia. eddCAVW. 
References 
[ I ]  N. Asada, A. Amano, and M. Baba. Photometric Cali- 
bration of Zoom Lens Systems. Proc. of IEEE Interna- 
tional Conference on Pattern Recognition (ICPR), pages 
186-1 90, 1996. 
[2] V. Brajovic and T. Kanade. A Sorting Image Sensor: An 
Example of Massively Parallel Intensity-to-Time Pro- 
cessing for Low-Latency Computational Sensors. Proc. 
of IEEE Conference on Robotics and Automation, pages 
1638-1643, April 1996. 
[3] P. Burt and R. J. Kolczynski. Enhanced Image Capture 
Through Fusion. Proc. of International Conference on 
Computer vision (ICCV), pages 173-182, 1993. 
379 
0.8 ’I(a) 
0.6 
0.4 
0.2 
0 
I 
1 
0.6 
0.6 
0.4 
0.2 
0 
I 
0 0.2 0.4 0.6 0.8 1 
M 
( b )  
0 0.2 0.4 0.6 0.8 1 
M 
( e >  ( f )  
Figure 7: Self-calibration results for color slides taken using a Nikon 2020 photographic camera. The slides are taken using film 
with IS0 rating of 200 in (a) and 100 in (d). Response functions are computed for each color channel and are shown in (b) and (e). 
The radiance images in (c) and (0 are histogram equalized and the three small windows shown in each radiance image are locally 
histogram equalized. The details around the lamp and within the wooden window in (c) and the clouds in the sky and the objects 
within the clay oven in (0 illustrate the richness of visual information embedded in the computed radiance images. 
Y.-C. Chang and J. E Reid. RGB Calibration for Anal- 
ysis in Machine Vision. IEEE Trans. on Pattern Analy- 
sis and Machine Intelligence, 5(10):1414-1422, October 
1996. 
P. Debevec and J. Malik. Recovering High Dynamic 
Range Radiance Maps from Photographs. Proc. ofACM 
SICGRAPH 1997, pages 369-378, 1997. 
G. Healey and R. Kondepudy. Radiometric CCD Camera 
Calibration and Noise Estimation. IEEE Trans. on Pat- 
tern Analysis and Machine Intelligence, 16(3):267-276, 
March 1994. 
K. Hirano, 0. Sano, and J. Miyamichi. A Detail En- 
hancement Method by Merging Multi Exposure Shift 
Images. IEICE, 181 -D-11(9):2097-2103, 1998. 
B. K. P. Horn. Robot Vision. MIT Press, Cambridge, 
MA, 1986. 
T. James, editor. The Theory of the Photographic Pro- 
cess. Macmillan, New York, 1977. 
[lo] B. Madden. Extended Intensity Range Imaging. Techni- 
cal Report MS-CIS-93-96, Grasp Laboratory, University 
of Pennsylvania, 1996. 
[ I l l  S .  Mann and R. Picard. Being ‘Undigital’ with Digi- 
tal Cameras: Extending Dynamic Range by Combining 
Differently Exposed Pictures. Proc. of IST’s 48th Annual 
Conference, pages 422-428, May 1995. 
[ 121 S. L. Meyer. Data Analysis for  Scientists and Engineers. 
John Wiley and Sons, 1992. 
[I31 C. A. Poynton. A Technical Introduction to Digital 
Video. John Wiley and Sons, 1996. 
[I41 A. J. P. Theuwissen. Solid State Imaging with Charge- 
Coupled Devices. Kluwer Academic Press, Boston, 
1995. 
3 80 

