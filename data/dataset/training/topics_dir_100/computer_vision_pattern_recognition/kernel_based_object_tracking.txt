Kernel-Based Object Tracking
Dorin Comaniciu, Senior Member, IEEE, Visvanathan Ramesh, Member, IEEE, and
Peter Meer, Senior Member, IEEE
Abstract—A new approach toward target representation and localization, the central component in visual tracking of nonrigid objects,
is proposed. The feature histogram-based target representations are regularized by spatial masking with an isotropic kernel. The
masking induces spatially-smooth similarity functions suitable for gradient-based optimization, hence, the target localization problem
can be formulated using the basin of attraction of the local maxima. We employ a metric derived from the Bhattacharyya coefficient as
similarity measure, and use the mean shift procedure to perform the optimization. In the presented tracking examples, the new method
successfully coped with camera motion, partial occlusions, clutter, and target scale variations. Integration with motion filters and data
association techniques is also discussed. We describe only a few of the potential applications: exploitation of background information,
Kalman tracking using motion models, and face tracking.
Index Terms—Nonrigid object tracking, target localization and representation, spatially-smooth similarity function, Bhattacharyya
coefficient, face tracking.
æ
1 INTRODUCTION
REAL-TIME object tracking is the critical task in manycomputer vision applications such as surveillance [44],
[16], [32], perceptual user interfaces [10], augmented reality
[26], smart rooms [39], [75], [47], object-based video compres-
sion [11], and driver assistance [34], [4].
Two major components can be distinguished in a typical
visual tracker. Target Representation and Localization is mostly a
bottom-up process which has also to cope with the changes in
the appearance of the target. Filtering and Data Association is
mostly a top-down process dealing with the dynamics of the
tracked object, learning of scene priors, and evaluation of
different hypotheses. The way the two components are
combined and weighted is application dependent and plays
a decisive role in the robustness and efficiency of the tracker.
For example, face tracking in a crowded scene relies more on
target representation than on target dynamics [21], while in
aerial video surveillance, e.g., [74], the target motion and the
ego-motion of the camera are the more important compo-
nents. In real-time applications, only a small percentage of the
system resources can be allocated for tracking, the rest being
required for the preprocessing stages or to high-level tasks
such as recognition, trajectory interpretation, and reasoning.
Therefore, it is desirable to keep the computational complex-
ity of a tracker as low as possible.
The most abstract formulation of the filtering and data
association process is through the state space approach for
modeling discrete-time dynamic systems [5]. The informa-
tion characterizing the target is defined by the state
sequence fxkgk¼0;1;..., whose evolution in time is specified
by the dynamic equation xk ¼ fkðxkÿ1;vkÞ. The available
measurements fzkgk¼1;... are related to the corresponding
states through the measurement equation zk ¼ hkðxk;nkÞ. In
general, both f k and hk are vector-valued, nonlinear, and
time-varying functions. Each of the noise sequences,
fvkgk¼1;... and fnkgk¼1;... is assumed to be independent and
identically distributed (i.i.d.).
The objective of tracking is to estimate the state xk given all
the measurements z1:k up that moment, or equivalently to
construct the probability density function (pdf)pðxkjz1:kÞ. The
theoretically optimal solution is provided by the recursive
Bayesian filter which solves the problem in two steps. The
prediction step uses the dynamic equation and the already
computed pdf of the state at time t ¼ kÿ 1, pðxkÿ1jz1:kÿ1Þ, to
derive the prior pdf of the current state, pðxkjz1:kÿ1Þ. Then, the
update step employs the likelihood function pðzkjxkÞ of the
current measurement to compute the posterior pdf pðxkjz1:k).
When the noise sequences are Gaussian and fk and hk
are linear functions, the optimal solution is provided by the
Kalman filter [5, p. 56], which yields the posterior being also
Gaussian. (We will return to this topic in Section 6.2.) When
the functions fk and hk are nonlinear, by linearization the
Extended Kalman Filter (EKF) [5, p. 106] is obtained, the
posterior density being still modeled as Gaussian. A recent
alternative to the EKF is the Unscented Kalman Filter (UKF)
[42] which uses a set of discretely sampled points to
parameterize the mean and covariance of the posterior
density. When the state space is discrete and consists of a
finite number of states, Hidden Markov Models (HMM)
filters [60] can be applied for tracking. The most general
class of filters is represented by particle filters [45], also
called bootstrap filters [31], which are based on Monte Carlo
integration methods. The current density of the state is
represented by a set of random samples with associated
weights and the new density is computed based on these
samples and weights (see [23], [3] for reviews). The UKF can
be employed to generate proposal distributions for particle
filters, in which case the filter is called Unscented Particle
Filter (UPF) [54].
When the tracking is performed in a cluttered environ-
ment where multiple targets can be present [52], problems
564 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
. D. Comaniciu and V. Ramesh are with the Real-Time Vision and Modeling
Department, Siemens Corporate Research, 755 College Road East,
Princeton, NJ 08540. E-mail: comanici@scr.siemens.com.
. P. Meer is with the Electrical and Computer Engineering Department,
Rutgers University, 94 Brett Road, Piscataway, NJ 08854-8058.
Manuscript received 21 May 2002; revised 13 Oct. 2002; accepted 16 Oct.
2002.
Recommended for acceptance by M. Irani.
For information on obtaining reprints of this article, please send e-mail to:
tpami@computer.org, and reference IEEECS Log Number 116595.
0162-8828/03/$10.00 ß 2003 IEEE Published by the IEEE Computer Society
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
related to the validation and association of the measurements
arise [5, p. 150]. Gating techniques are used to validate only
measurements whose predicted probability of appearance is
high. After validation, a strategy is needed to associate the
measurements with the current targets. In addition to the
Nearest Neighbor Filter, which selects the closest measure-
ment, techniques such as Probabilistic Data Association Filter
(PDAF) are available for the single target case. The under-
lying assumption of the PDAF is that for any given target only
one measurement is valid, and the other measurements are
modeled as random interference, that is, i.i.d. uniformly
distributed random variables. The Joint Data Association
Filter (JPDAF) [5, p. 222], on the other hand, calculates the
measurement-to-target association probabilities jointly
across all the targets. A different strategy is represented by
the Multiple Hypothesis Filter (MHF) [63], [20], [5, p. 106]
which evaluates the probability that a given target gave rise to
a certain measurement sequence. The MHF formulation can
be adapted to track the modes of the state density [13]. The
data association problem for multiple target particle filtering
is presented in [62], [38].
The filtering and association techniques discussed above
were applied in computer vision for various tracking
scenarios. Boykov and Huttenlocher [9] employed the Kal-
man filter to track vehicles in an adaptive framework. Rosales
and Sclaroff [65] used the Extended Kalman Filter to estimate
a 3D object trajectory from 2D image motion. Particle filtering
was first introduced, in vision, as the Condensation algorithm
by Isard and Blake [40]. Probabilistic exclusion for tracking
multiple objects was discussed in [51]. Wu and Huang
developed an algorithm to integrate multiple target clues [76].
Li and Chellappa [48] proposed simultaneous tracking and
verification based on particle filters applied to vehicles and
faces. Chen et al. [15] used the Hidden Markov Model
formulation for tracking combined with JPDAF data associa-
tion. Rui and Chen proposed to track the face contour based
on the unscented particle filter [66]. Cham and Rehg [13]
applied a variant of MHF for figure tracking.
The emphasis in this paper is on the other component of
tracking: target representation and localization. While the
filtering and data association have their roots in control
theory, algorithms for target representation and localization
are specific to images and related to registration methods [72],
[64], [56]. Both target localization and registration maximizes
a likelihood type function. The difference is that in tracking,
as opposed to registration, only small changes are assumed in
the location and appearance of the target in two consecutive
frames. This property can be exploited to develop efficient,
gradient-based localization schemes using the normalized
correlation criterion [6]. Since the correlation is sensitive to
illumination, Hager and Belhumeur [33] explicitly modeled
the geometry and illumination changes. The method
was improved by Sclaroff and Isidoro [67] using robust
M-estimators. Learning of appearance models by employing
a mixture of stable image structure, motion information, and
an outlier process, was discussed in [41]. In a different
approach, Ferrari et al. [26] presented an affine tracker based
on planar regions and anchor points. Tracking people, which
raises many challenges due to the presence of large 3D,
nonrigid motion, was extensively analyzed in [36], [1], [30],
[73]. Explicit tracking approaches of people [69] are time-
consuming and often the simpler blob model [75] or adaptive
mixture models [53] are also employed.
The main contribution of the paper is to introduce a new
framework for efficient tracking of nonrigid objects. We
show that by spatially masking the target with an isotropic
kernel, a spatially-smooth similarity function can be defined
and the target localization problem is then reduced to a
search in the basin of attraction of this function. The
smoothness of the similarity function allows application of
a gradient optimization method which yields much faster
target localization compared with the (optimized) exhaus-
tive search. The similarity between the target model and the
target candidates in the next frame is measured using the
metric derived from the Bhattacharyya coefficient. In our
case, the Bhattacharyya coefficient has the meaning of a
correlation score. The new target representation and
localization method can be integrated with various motion
filters and data association techniques. We present tracking
experiments in which our method successfully coped with
complex camera motion, partial occlusion of the target,
presence of significant clutter, and large variations in target
scale and appearance. We also discuss the integration of
background information and Kalman filter based tracking.
The paper is organized as follows: Section 2 discusses
issues of target representation and the importance of a
spatially-smooth similarity function. Section 3 introduces
the metric derived from the Bhattacharyya coefficient. The
optimization algorithm is described in Section 4. Experi-
mental results are shown in Section 5. Section 6 presents
extensions of the basic algorithm and the new approach is
put in the context of computer vision literature in Section 7.
2 TARGET REPRESENTATION
To characterize the target, first a feature space is chosen.
The reference target model is represented by its pdf q in the
feature space. For example, the reference model can be
chosen to be the color pdf of the target. Without loss of
generality, the target model can be considered as centered
at the spatial location 0. In the subsequent frame, a target
candidate is defined at location y, and is characterized by the
pdf pðyÞ. Both pdfs are to be estimated from the data. To
satisfy the low-computational cost imposed by real-time
processing discrete densities, i.e., m-bin histograms should
be used. Thus, we have
target model : q̂ ¼ q̂uf gu¼1...m
Xm
u¼1
q̂u ¼ 1
target candidate : p̂ðyÞ ¼ p̂uðyÞf gu¼1...m
Xm
u¼1
p̂u ¼ 1:
The histogram is not the best nonparametric density
estimate [68], but it suffices for our purposes. Other discrete
density estimates can be also employed.
We will denote by
̂ðyÞ  ½p̂ðyÞ; q̂ ð1Þ
a similarity function between p̂ and q̂. The function ̂ðyÞplays
the role of a likelihood and its local maxima in the image
indicate the presence of objects in the second frame having
representations similar to q̂ defined in the first frame. If only
spectral information is used to characterize the target, the
similarity function can have large variations for adjacent
locations on the image lattice and the spatial information is
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 565
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
lost. To find the maxima of such functions, gradient-based
optimization procedures are difficult to apply and only an
expensive exhaustive search can be used. We regularize the
similarity function by masking the objects with an isotropic
kernel in the spatial domain. When the kernel weights,
carrying continuous spatial information, are used in defining
the feature space representations, ̂ðyÞ becomes a smooth
function in y.
2.1 Target Model
A target is represented by an ellipsoidal region in the
image. To eliminate the influence of different target
dimensions, all targets are first normalized to a unit circle.
This is achieved by independently rescaling the row and
column dimensions with hx and hy.
Let x?i
 	
i¼1...n be the normalized pixel locations in the
region defined as the target model. The region is centered at
0. An isotropic kernel, with a convex and monotonic
decreasing kernel profile kðxÞ,1 assigns smaller weights to
pixels farther from the center. Using these weights increases
the robustness of the density estimation since the peripheral
pixels are the least reliable, being often affected by
occlusions (clutter) or interference from the background.
The function b : R2 ! 1 . . .mf g associates to the pixel at
location x?i the index bðx?i Þ of its bin in the quantized feature
space. The probability of the feature u ¼ 1 . . .m in the target
model is then computed as
q̂u ¼ C
Xn
i¼1
k kx?i k
2
 
 bðx?i Þ ÿ u
 
; ð2Þ
where  is the Kronecker delta function. The normalization
constantC is derived by imposing the condition
Pm
u¼1 q̂u ¼ 1,
from where
C ¼ 1Pn
i¼1 k kx?i k
2
  ; ð3Þ
since the summation of delta functions for u ¼ 1 . . .m is
equal to one.
2.2 Target Candidates
Let xif gi¼1...nh be the normalized pixel locations of the target
candidate, centered at y in the current frame. The normal-
ization is inherited from the frame containing the target
model. Using the same kernel profile kðxÞ, but with
bandwidth h, the probability of the feature u ¼ 1 . . .m in the
target candidate is given by
p̂uðyÞ ¼ Ch
Xnh
i¼1
k
yÿ xi
h
 2  bðxiÞ ÿ u½ ; ð4Þ
where
Ch ¼
1Pnh
i¼1 kðk
yÿxi
h k
2Þ
ð5Þ
is the normalization constant. Note that Ch does not depend
on y, since the pixel locations xi are organized in a regular
lattice and y is one of the lattice nodes. Therefore, Ch can be
precalculated for a given kernel and different values of h. The
bandwidth h defines the scale of the target candidate, i.e., the
number of pixels considered in the localization process.
2.3 Similarity Function Smoothness
The similarity function (1) inherits the properties of the kernel
profile kðxÞ when the target model and candidate are
represented according to (2) and (4). A differentiable kernel
profile yields a differentiable similarity function and efficient
gradient-based optimizations procedures can be used for
finding its maxima. The presence of the continuous kernel
introduces an interpolation process between the locations on
the image lattice. The employed target representations do not
restrict the way similarity is measured and various functions
can be used for . See [59] for an experimental evaluation of
different histogram similarity measures.
3 METRIC BASED oN BHATTACHARYYA
COEFFICIENT
The similarity function defines a distance among target
model and candidates. To accommodate comparisons among
various targets, this distance should have a metric structure.
We define the distance between two discrete distributions as
dðyÞ ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1ÿ  p̂ðyÞ; q̂½ 
p
; ð6Þ
where we chose
̂ðyÞ   p̂ðyÞ; q̂½  ¼
Xm
u¼1
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
p̂uðyÞq̂u
p
; ð7Þ
the sample estimate of the Bhattacharyya coefficient
between p and q [43].
The Bhattacharyya coefficient is a divergence-type mea-
sure [49] which has a straightforward geometric interpreta-
tion. It is the cosine of the angle between the m-dimensional
unit vectors
ffiffiffiffiffi
p̂1
p
; . . . ;
ffiffiffiffiffiffi
p̂m
pÿ >
and
ffiffiffiffi
q̂1
p
; . . . ;
ffiffiffiffiffi
q̂m
pÿ >
. The fact
that p and q are distributions is thus explicitly taken into
account by representing them on the unit hypersphere. At the
same time, we can interpret (7) as the (normalized) correlation
between the vectors
ffiffiffiffiffi
p̂1
p
; . . . ;
ffiffiffiffiffiffi
p̂m
pÿ >
and
ffiffiffiffi
q̂1
p
; . . . ;
ffiffiffiffiffi
q̂m
pÿ >
.
Properties of the Bhattacharyya coefficient such as its relation
to the Fisher measure of information, quality of the sample
estimate, and explicit forms for various distributions are
given in [22], [43].
The statistical measure (6) has several desirable properties:
1. It imposes a metric structure (see Appendix). The
Bhattacharyya distance [28, p. 99] or Kullback
divergence [19, p. 18] are not metrics since they violate
at least one of the distance axioms.
2. It has a clear geometric interpretation. Note that the
Lp histogram metrics (including histogram intersec-
tion [71]) do not enforce the conditions
Pm
u¼1 q̂u ¼ 1
and
Pm
u¼1 p̂u ¼ 1.
3. It uses discrete densities and, therefore, it is invariant
to the scale of the target (up to quantization effects).
4. It is valid for arbitrary distributions, thus being
superior to the Fisher linear discriminant, which
yields useful results only for distributions that are
separated by the mean-difference [28, p. 132].
566 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
1. The profile of a kernel K is defined as a function k : ½0;1Þ ! R such
that KðxÞ ¼ kðkxk2Þ.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
5. It approximates the chi-squared statistic, while
avoiding the singularity problem of the chi-square
test when comparing empty histogram bins [2].
Divergence-based measures were already used in com-
puter vision. The Chernoff and Bhattacharyya bounds have
been employed in [46] to determine the effectiveness of edge
detectors. The Kullback divergence between joint distribu-
tion and product of marginals (e.g., the mutual information)
has been used in [72] for registration. Information theoretic
measures for target distinctness were discussed in [29].
4 TARGET LOCALIZATION
To find the location corresponding to the target in the current
frame, the distance (6) should be minimized as a function of y.
The localization procedure starts from the position of the
target in the previous frame (the model) and searches in the
neighborhood. Since our distance function is smooth, the
procedure uses gradient information which is provided by
the mean shift vector [17]. More involved optimizations
based on the Hessian of (6) can be applied [58].
Color information was chosen as the target feature,
however, the same framework can be used for texture and
edges, or any combination of them. In the sequel, it is assumed
that the following information is available: 1) detection and
localization in the initial frame of the objects to track (target
models) [50], [8] and 2) periodic analysis of each object to
account for possible updates of the target models due to
significant changes in color [53].
4.1 Distance Minimization
Minimizing the distance (6) is equivalent to maximizing the
Bhattacharyya coefficient ̂ðyÞ. The search for the new target
location in the current frame starts at the location ŷ0 of the
target in the previous frame. Thus, the probabilities
p̂uðŷ0Þf gu¼1...m of the target candidate at location ŷ0 in the
current frame have to be computed first. Using Taylor
expansion around the values p̂uðŷ0Þ, the linear approxima-
tion of the Bhattacharyya coefficient (7) is obtained after
some manipulations as
 p̂ðyÞ; q̂½   1
2
Xm
u¼1
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
p̂uðŷ0Þq̂u
p
þ 1
2
Xm
u¼1
p̂uðyÞ
ffiffiffiffiffiffiffiffiffiffiffiffiffi
q̂u
p̂uðŷ0Þ
s
: ð8Þ
The approximation is satisfactory when the target candidate
p̂uðyÞf gu¼1...m does not change drastically from the initial
p̂uðŷ0Þf gu¼1...m, which is most often a valid assumption
between consecutive frames. The condition p̂uðŷ0Þ > 0 (or
some small threshold) for all u ¼ 1 . . .m, can always be
enforced by not using the feature values in violation.
Recalling (4) results in
 p̂ðyÞ; q̂½   1
2
Xm
u¼1
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
p̂uðŷ0Þq̂u
p
þ Ch
2
Xnh
i¼1
wik
yÿ xi
h
 2 ; ð9Þ
where
wi ¼
Xm
u¼1
ffiffiffiffiffiffiffiffiffiffiffiffiffi
q̂u
p̂uðŷ0Þ
s
 bðxiÞ ÿ u½ : ð10Þ
Thus, to minimize thedistance (6), the second term in (9) has to
be maximized, the first term being independent of y. Observe
that the second term represents the density estimate
computed with kernel profile kðxÞ at y in the current frame,
with the data being weighted by wi (10). The mode of this
density in the local neighborhood is the sought maximum that
can be found employing the mean shift procedure [17]. In this
procedure, the kernel is recursively moved from the current
location ŷ0 to the new location ŷ1 according to the relation
ŷ1 ¼
Pnh
i¼1 xiwig
ŷ0ÿxi
h
 2 
Pnh
i¼1 wig
ŷ0ÿxi
h
 2  ; ð11Þ
where gðxÞ ¼ ÿk0ðxÞ, assuming that the derivative of kðxÞ
exists for all x 2 ½0;1Þ, except for a finite set of points. The
complete target localization algorithm is presented below:
Bhattacharyya Coefficient  p̂ðyÞ; q̂½ Maximization:
Given:
The target model q̂uf gu¼1...m and its location ŷ0 in the
previous frame.
1. Initialize the location of the target in the current frame
with ŷ0, compute p̂uðŷ0Þf gu¼1...m, and evaluate
 p̂ðŷ0Þ; q̂½  ¼
Xm
u¼1
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
p̂uðŷ0Þq̂u
p
:
2. Derive the weights wif gi¼1...nh according to (10).
3. Find the next location of the target candidate according
to (11).
4. Compute p̂uðŷ1Þf gu¼1...m, and evaluate
 p̂ðŷ1Þ; q̂½  ¼
Xm
u¼1
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
p̂uðŷ1Þq̂u
p
:
5. While  p̂ðŷ1Þ; q̂½  <  p̂ðŷ0Þ; q̂½ 
Do ŷ1  12 ŷ0 þ ŷ1ð Þ
Evaluate  p̂ðŷ1Þ; q̂½ 
6. If kŷ1 ÿ ŷ0k <  Stop.
Otherwise Set ŷ0  ŷ1 and go to Step 2.
4.2 Implementation of the Algorithm
The stopping criterion threshold used in Step 6 is derived by
constraining the vectors ŷ0 and ŷ1 to be within the same pixel
in original image coordinates. A lower threshold will induce
subpixel accuracy. From real-time constraints (i.e., uniform
CPU load in time), we also limit the number of mean shift
iterations toNmax, typically taken equal to 20. In practice, the
average number of iterations is much smaller, about 4.
Implementation of the tracking algorithm can be much
simpler than as presented above. The role of Step 5 is only to
avoid potential numerical problems in the mean shift based
maximization. These problems can appear due to the linear
approximation of the Bhattacharyya coefficient. However, a
large set of experiments tracking different objects for long
periods of time, has shown that the Bhattacharyya coefficient
computed at the new location ŷ1 failed to increase in only
0.1 percent of the cases. Therefore, the Step 5 is not used in
practice, and as a result, there is no need to evaluate the
Bhattacharyya coefficient in Steps 1 and 4.
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 567
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
In the practical algorithm, we only iterate by computing
the weights in Step 2, deriving the new location in Step 3, and
testing the size of the kernel shift in Step 6. The Bhattacharyya
coefficient is computed only after the algorithm completion to
evaluate the similarity between the target model and the
chosen candidate.
Kernels with Epanechnikov profile [17]
kðxÞ ¼
1
2 c
ÿ1
d ðdþ 2Þð1ÿ xÞ if x  1
0 otherwise:

ð12Þ
are recommended to be used. In this case, the derivative of
the profile, gðxÞ, is constant and (11) reduces to
ŷ1 ¼
Pnh
i¼1 xiwiPnh
i¼1 wi
; ð13Þ
i.e., a simple weighted average.
The maximization of the Bhattacharyya coefficient can be
also interpreted as a matched filtering procedure. Indeed,
(7) is the correlation coefficient between the unit vectors
ffiffiffî
q
p
and
ffiffiffiffiffiffiffiffiffiffi
p̂ðyÞ
p
, representing the target model and candidate.
The mean shift procedure thus finds the local maximum of
the scalar field of correlation coefficients.
Will call the operational basin of attraction the region in the
current frame in which the new location of the target can be
found by the proposed algorithm. Due to the use of kernels,
this basin is at least equal to the size of the target model. In
other words, if in the current frame the center of the target
remains in the image area covered by the target model in the
previous frame, the local maximum of the Bhattacharyya
coefficient is a reliable indicator for the new target location.
We assume that the target representation provides sufficient
discrimination, such that the Bhattacharyya coefficient
presents a unique maximum in the local neighborhood.
The mean shift procedure finds a root of the gradient as
function of location, which can, however, also correspond to a
saddle point of the similarity surface. The saddle points are
unstable solutions and since the image noise acts as an
independent perturbation factor across consecutive frames,
they cannot influence the tracking performance in an image
sequence.
4.3 Adaptive Scale
According to the algorithm described in Section 4.1, for a
given target model, the location of the target in the current
frame minimizes the distance (6) in the neighborhood of the
previous location estimate. However, the scale of the target
often changes in time and, thus, in (4) the bandwidth h of
the kernel profile has to be adapted accordingly. This is
possible due to the scale invariance property of (6).
Denote by hprev the bandwidth in the previous frame. We
measure the bandwidth hopt in the current frame by running
the target localization algorithm three times, with band-
widths h ¼ hprev, h ¼ hprev þh, and h ¼ hprev ÿh. A
typical value is h ¼ 0:1hprev. The best result, hopt, yielding
the largest Bhattacharyya coefficient, is retained. To avoid
oversensitive scale adaptation, the bandwidth associated
with the current frame is obtained through filtering
hnew ¼ hopt þ ð1ÿ Þhprev; ð14Þ
where the default value for  is 0.1. Note that the sequence
of hnew contains important information about the dynamics
of the target scale which can be further exploited.
4.4 Computational Complexity
Let N be the average number of iterations per frame. In Step 2,
the algorithm requires computing the representation
p̂uðyÞf gu¼1...m. Weighted histogram computation has roughly
the same cost as the unweighted histogram since the kernel
values are precomputed. In Step 3, the centroid (13) is
computed which involves a weighted sum of items repre-
senting the square-root of a division of two terms. We
conclude that the mean cost of the proposed algorithm for one
scale is approximately given by
CO ¼ NðcH þ nhcSÞ  NnhcS; ð15Þ
where cH is the cost of the histogram and cS the cost of an
addition, a square-root, and a division. We assume that the
number of histogram entries m and the number of target
pixels nh are in the same range.
It is of interest to compare the complexity of the new
algorithm with that of target localization without gradient
optimization, as discussed in [25]. The search area is assumed
to be equal to the operational basin of attraction, i.e., a region
covering the target model pixels. The first step is to compute
nh histograms. Assume that each histogram is derived in a
squared window of nh pixels. To implement the computation
efficiently, we obtain a target histogram and update it by
sliding the window nh times (
ffiffiffiffiffi
nh
p
horizontal steps timesffiffiffiffiffi
nh
p
vertical steps). For each move, 2
ffiffiffiffiffi
nh
p
additions are
needed to update the histogram, hence, the effort is
cH þ 2nh
ffiffiffiffiffi
nh
p
cA, where cA is the cost of an addition. The
second step is to compute nh Bhattacharyya coefficients. This
can also be done by computing one coefficient and, then,
updating it sequentially. The effort is mcS þ 2nh
ffiffiffiffiffi
nh
p
cS . The
total effort for target localization without gradient optimiza-
tion is then
CNO ¼ cH þ 2nh
ffiffiffiffiffi
nh
p
cA þ ðmþ 2nh
ffiffiffiffiffi
nh
p ÞcS  2nh
ffiffiffiffiffi
nh
p
cS: ð16Þ
The ratio between (16) and (15) is 2  ffiffiffiffiffinhp =N . In a typical
setting (as it will be shown in Section 5), the target has
about 50 50 pixels (i.e., ffiffiffiffiffinhp ¼ 50) and the mean
number of iterations is N ¼ 4:19. Thus, the proposed
optimization technique reduces the computational time
2  50=4:19  24 times.
An optimized implementation of the new tracking
algorithm has been tested on a 1GHz PC. The basic
framework with scale adaptation (which involves three
optimizations at each step) runs at a rate of 150 fps allowing
simultaneous tracking of up to five targets in real time. Note
that without scale adaptations these numbers should be
multiplied by three.
5 EXPERIMENTAL RESULTS
The kernel-based visual tracker was applied to many
sequences and it is integrated into several applications. Here,
we just present some representative results. In all but the last
experiment, the RGB color space was taken as feature space
and it was quantized into 16 16 16 bins. The algorithm
was implemented as discussed in Section 4.2. The Epanech-
nikov profile was used for histogram computations and the
mean shift iterations were based on weighted averages.
The Football sequence (Fig. 1) has 154 frames of
352 240 pixels, and the movement of player number
75 was tracked. The target was initialized with a hand-drawn
568 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
elliptical region (frame 30) of size 71 53 (yielding normal-
ization constants equal to ðhx; hyÞ ¼ ð35; 26ÞÞ. The kernel-
based tracker proves to be robust to partial occlusion, clutter,
distractors (frame 140), and camera motion. Since no motion
model has been assumed, the tracker adapted well to the
nonstationary character of the player’s movements which
alternates abruptly between slow and fast action. In addition,
the intense blurring present in some frames due to the camera
motion, does not influence the tracker performance (frame
150). The same conditions, however, can largely perturb
contour based trackers.
The number of mean shift iterations necessary for each
frame (one scale) is shown in Fig. 2. The two central peaks
correspond to the movement of the player to the center of
the image and back to the left side. The last and largest peak
is due to a fast move from the left to the right. In all these
cases, the relative large movement between two consecutive
frames puts more of a burden on the mean shift procedure.
To demonstrate the efficiency of our approach, Fig. 3
presents the surface obtained by computing the Bhattachar-
yya coefficient for the 81 81 pixels rectangle marked in
Fig. 1, frame 105. The target model (the elliptical region
selected in frame 30) has been compared with the target
candidates obtained by sweeping in frame 105 the elliptical
region inside the rectangle. The surface is asymmetric due to
neighboring colors that are similar to the target. While most of
the tracking approaches based on regions [7], [27], [50] must
perform an exhaustive search in the rectangle to find the
maximum, our algorithm converged in four iterations as
shown in Fig. 3. Note that the operational basin of attraction of
the mode covers the entire rectangular window.
In controlled environments with fixed camera, additional
geometric constraints (such as the expected scale) and
background subtraction [24] can be exploited to improve
the tracking process. The Subway-1 sequence (Fig. 4) is
suitable for such an approach, however, the results
presented here has been processed with the algorithm
unchanged. This is a two minute sequence in which a
person is tracked from the moment she enters the subway
platform until she gets on the train (about 3,600 frames).
The tracking is made more challenging by the low quality of
the sequence due to image compression artifacts. Note the
changes in the size of the tracked target.
The minimum value of the distance (6) for each frame,
i.e., the distance between the target model and the chosen
candidate, is shown in Fig. 5. The compression noise
elevates the residual distance value from 0 (perfect match)
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 569
Fig. 1. Football sequence, tracking player number 75. The frames 30, 75, 105, 140, and 150 are shown.
Fig. 2. The number of mean shift iterations function of the frame index for
the Football sequence. The mean number of iterations is 4:19 per frame.
Fig. 3. The similarity surface (values of the Bhattacharyya coefficient)
corresponding to the rectangle marked in frame 105 of Fig. 1. The initial
and final locations of the mean shift iterations are also shown.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
to a about 0:3. Significant deviations from this value
correspond to occlusions generated by other persons or
rotations in depth of the target (large changes in the
representation). For example, the d  0:6 peak corresponds
to the partial occlusion in frame 3; 697. At the end of the
sequence, the person being tracked gets on the train, which
produces a complete occlusion.
The shorter Subway-2 sequence (Fig. 6) of about 600 frames
is even more difficult since the camera quality is worse and
the amount of compression is higher, introducing clearly
visible artifacts. Nevertheless, the algorithm was still able to
track a person through the entire sequence.
In the Mug sequence (Fig. 7) of about 1,000 frames, the
image of a cup (frame 60) was used as target model. The
normalization constants were ðhx ¼ 44; hy ¼ 64Þ. The tracker
was tested for fast motion (frame 150), dramatic changes in
appearance (frame 270), rotations (frame 270), and scale
changes (frames 360-960).
6 EXTENSIONS OF THE TRACKING ALGORITHM
We present three extensions of the basic algorithm: integra-
tion of the background information, Kalman tracking, and an
application to face tracking. It should be emphasized,
however, that there are many other possibilities through
which the visual tracker can be further enhanced.
6.1 Background-Weighted Histogram
The background information is important for at least two
reasons. First, if some of the target features are also present in
the background, their relevance for the localization of the
target is diminished. Second, in many applications, it is
difficult to exactly delineate the target, and its model might
contain background features as well. At the same time, the
improper use of the background information may affect the
scale selection algorithm, making impossible to measure
similarity across scales, hence, to determine the appropriate
target scale. Our approach is to derive a simple representation
of the background features and to use it for selecting only the
salient parts from the representations of the target model and
target candidates.
Let ôuf gu¼1...m (with
Pm
u¼1 ôu ¼ 1) be the discrete repre-
sentation (histogram) of the background in the feature space
and ô be its smallest nonzero entry. This representation is
computed in a region around the target. The extent of the
region is application dependent and we used an area equal to
three times the target area. The weights
vu ¼ min
ô
ôu
; 1
  
u¼1...m
ð17Þ
are similar in concept to the ratio histogram computed for
backprojection [71]. However, in our case, these weights
are only employed to define a transformation for the
570 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
Fig. 4. Subway-1 sequence. The frames 3,140, 3,516, 3,697, 5,440, 6,081, and 6,681 are shown.
Fig. 5. The minimum value of distance d function of the frame index for
the Subway-1 sequence.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
representations of the target model and candidates. The
transformation diminishes the importance of those features
which have low vu, i.e., are prominent in the background.
The new target model representation is then defined by
q̂u ¼ Cvu
Xn
i¼1
kðkx?i k
2Þ bðx?i Þ ÿ u
 
ð18Þ
with the normalization constant C expressed as
C ¼ 1Pn
i¼1 kðkx?i k
2Þ
Pm
u¼1 vu bðx?i Þ ÿ u
  : ð19Þ
Compare with (2) and (3). Similarly, the new target
candidate representation is
p̂uðyÞ ¼ Chvu
Xnh
i¼1
k
yÿ xi
h
 2  bðxiÞ ÿ u½ ; ð20Þ
where now Ch is given by
Ch ¼
1Pnh
i¼1 kðk
yÿxi
h k
2Þ
Pm
u¼1 vu bðxiÞ ÿ u½ 
: ð21Þ
An important benefit of using background-weighted histo-
grams is shown for the Ball sequence (Fig. 8). The movement
of the ping-pong ball from frame to frame is larger than its
size. Applying the technique described above, the target
model can be initialized with a 21 31 size region (frame 2),
larger than one obtained by accurate target delineation. The
larger region yields a satisfactory operational basin of
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 571
Fig. 6. Subway-2 sequence. The frames 30, 240, 330, 450, 510, and 600 are shown.
Fig. 7. Mug sequence. The frames 60, 150, 240, 270, 360, and 960 are shown.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
attraction, while the probabilities of the colors that are part of
the background are considerably reduced. The ball is reliably
tracked over the entire sequence of 60 frames.
The last example is also taken from the Football sequence.
This time the head and shoulder of player number 59 is
tracked (Fig. 9). Note the changes in target appearance along
the entire sequence and the rapid movements of the target.
6.2 Kalman Prediction
It was already mentioned in Section 1 that the Kalman filter
assumes that the noise sequences vk and nk are Gaussian
and the functions fk and hk are linear. The dynamic
equation becomes xk ¼ Fxkÿ1 þ vk, while the measurement
equation is zk ¼ Hxk þ nk. The matrix F is called the
system matrix and H is the measurement matrix. As in the
general case, the Kalman filter solves the state estimation
problem in two steps: prediction and update. For more
details, see [5, p. 56].
The kernel-based target localization method was inte-
grated with the Kalman filtering framework. For a faster
implementation, two independent trackers were defined for
horizontal and vertical movement. A constant-velocity
dynamic model with acceleration affected by white noise
[5, p. 82] has been assumed. The uncertainty of the
measurements has been estimated according to [55]. The
idea is to normalize the similarity surface and represent it as
a probability density function. Since the similarity surface is
smooth, for each filter only three measurements are taken
into account, one at the convergence point (peak of the
surface) and the other two at a distance equal to half of the
target dimension, measured from the peak. We fit a scaled
Gaussian to the three points and compute the measurement
uncertainty as the standard deviation of the fitted Gaussian.
A first set of tracking results incorporating the Kalman
filter is presented in Fig. 10 for the 120 frames Hand sequence
where the dynamic model is assumed to be affected by a noise
572 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
Fig. 8. Ball sequence. The frames 2, 12, 16, 26, 32, 40, 48, and 51 are shown.
Fig. 9. Football sequence, tracking player number 59. The frames 70, 96, 108, 127, 140, and 147 are shown.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
with standard deviation equal to 5. The size of the green cross
marked on the target indicates the state uncertainty for the
two trackers. Observe that the overall algorithm is able to
track the target (hand) in the presence of complete occlusion
by a similar object (the other hand). The presence of a similar
object in the neighborhood increases the measurement
uncertainty in frame 46, determining an increase in state
uncertainty. In Fig. 11a, we present the measurements
(dotted) and the estimated location of the target (continuous).
Note that the graph is symmetric with respect to the number
of frames since the sequence has been played forward and
backward. The velocity associated with the two trackers is
shown in Fig. 11b.
A second set of results showing tracking with Kalman
filter is displayed in Fig. 12. The sequence has 187 frames of
320 240 pixels each and the initial normalization constants
were ðhx; hyÞ ¼ ð11; 18Þ. Observe the adaptation of the
algorithm to scale changes. The uncertainty of the state is
indicated by the green cross.
6.3 Face Tracking
We applied the proposed framework for real-time face
tracking. The face model is an elliptical region whose
histogram is represented in the intensity normalized rg space
with 128 128 bins. To adapt to fast scale changes, we also
exploit the gradient information in the direction perpendi-
cular to the border of the hypothesized face region. The scale
adaptation is thus determined by maximizing the sum of two
normalized scores, based on color and gradient features,
respectively. In Fig. 13, we present the capability of the
kernel-based tracker to handle scale changes, the subject
turning away (frame 150), in-plane rotations of the head
(frame 498), and foreground/background saturation due to
back-lighting (frame 576). The tracked face is shown in the
small upper-left window.
7 DISCUSSION
The kernel-based tracking technique introduced in this
paper uses the basin of attraction of the similarity function.
This function is smooth since the target representations are
derived from continuous densities. Several examples
validate the approach and show its efficiency. Extensions
of the basic framework were presented regarding the use of
background information, Kalman filtering, and face track-
ing. The new technique can be further combined with more
sophisticated filtering and association approaches such as
multiple hypothesis tracking [13].
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 573
Fig. 10. Hand sequence. The frames 40, 46, 50, and 57 are shown.
Fig. 11. Measurements and estimated state for Hand sequence. (a) The measurement value (dotted curve) and the estimated location (continuous
curve) function of the frame index. Upper curves correspond to the y filter, while the lower curves correspond to the x filter. (b) Estimated velocity.
Dotted curve is for the y filter and continuous curve is for the x filter.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
Centroid computation has been also employed in [53].
The mean shift is used for tracking human faces by
projecting the histogram of a face model onto the incoming
frame [10]. However, the direct projection of the model
histogram onto the new frame can introduce a large bias in
the estimated location of the target, and the resulting
measure is scale variant (see [37, p. 262] for a discussion).
We mention that since its original publication in [18], the
idea of kernel-based tracking has been exploited and
developed forward by various researchers. Chen and Liu
[14] experimented with the same kernel-weighted histo-
grams, but employed the Kullback-Leibler distance as
dissimilarity while performing the optimization based on
trust-region methods. Haritaoglu and Flickner [35] used an
appearance model based on color and edge density in
conjunction with a kernel tracker for monitoring shopping
groups in stores. Yilmaz et al. [78] combined kernel tracking
with global motion compensation for forward-looking infra-
red (FLIR) imagery. Xu and Fujimura [77] used night vision
for pedestrian detection and tracking, where the detection is
performed by a support vector machine and the tracking is
kernel-based. Rao et al. [61] employed kernel tracking in their
system for action recognition, while Caenen et al. [12]
followed the same principle for texture analysis. The benefits
of guiding random particles by gradient optimization are
discussed in [70] and a particle filter for color histogram
tracking based on the metric (6) is implemented in [57].
Finally, we would like to add a word of caution. The
tracking solution presented in this paper has several desirable
properties: it is efficient, modular, has straightforward
574 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
Fig. 12. Subway-3 sequence: The frames 470, 529, 580, 624, and 686 are shown.
Fig. 13. Face sequence: The frames 39, 150, 163, 498, 576, and 619 are shown.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
implementation, and provides superior performance on most
image sequences. Nevertheless, we note that this technique
should not be applied blindly. Instead, the versatility of the
basic algorithm should be exploited to integrate a priori
information that is almost always available when a specific
application is considered. For example, if the motion of the
target from frame to frame is known to be larger than the
operational basin of attraction, one should initialize the
tracker in multiple locations in the neighborhood of basin of
attraction, according to the motion model. If occlusions are
present, one should employ a more sophisticated motion
filter. Similarly, one should verify that the chosen target
representation is sufficiently discriminant for the application
domain. The kernel-based tracking technique, when com-
bined with prior task-specific information, can achieve
reliable performance.2
APPENDIX
PROOF THAT THE DISTANCE dðp̂;q̂Þ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1ÿðp̂;q̂Þ
p
IS
A METRIC
The proof is based on the properties of the Bhattacharyya
coefficient (7). According to the Jensen’s inequality [19, p. 25],
we have
ðp̂; q̂Þ ¼
Xm
u¼1
ffiffiffiffiffiffiffiffiffi
p̂uq̂u
p
¼
Xm
u¼1
p̂u
ffiffiffiffiffi
q̂u
p̂u
s

ffiffiffiffiffiffiffiffiffiffiffiffiXm
u¼1
q̂u
s
¼ 1; ðA:1Þ
with equality iff p̂ ¼ q̂. Therefore, dðp̂; q̂Þ ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1ÿ ðp̂; q̂Þ
p
exists for all discrete distributions p̂ and q̂, is positive,
symmetric, and is equal to zero iff p̂ ¼ q̂.
To prove the triangle inequality consider three discrete
distributions defining the m-dimensional vectors p̂, q̂, and
r̂, associated with the points p ¼
ffiffiffiffiffi
p̂1
p
; . . . ;
ffiffiffiffiffiffi
p̂m
pÿ >
,
q ¼
ffiffiffiffi
q̂1
p
; . . . ;
ffiffiffiffiffi
q̂m
pÿ >
, and r ¼
ffiffiffiffi
r̂1
p
; . . . ;
ffiffiffiffiffiffi
r̂m
pÿ >
on the
unit hypersphere. From the geometric interpretation of
the Bhattacharyya coefficient, the triangle inequality
dðp̂; r̂Þ þ dðq̂; r̂Þ  dðp̂; q̂Þ ðA:2Þ
is equivalent toffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1ÿ cosðp; rÞ
q
þ
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1ÿ cosðq; rÞ
q

ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1ÿ cosðp; qÞ
q
: ðA:3Þ
If we fix the points p and q, and the angle between p and
r, the left side of inequality (A.3) is minimized when the
vectors p, q, and r lie in the same plane. Thus, the
inequality (A.3) can be reduced to a 2-dimensional problem
that can be easily proven by employing the half-angle sinus
formula and a few trigonometric manipulations.
ACKNOWLEDGMENTS
A preliminary version of the paper was presented at the
2000 IEEE Conference on Computer Vision and Pattern
Recognition, Hilton Head, South Carolina. Peter Meer was
supported by the US National Science Foundation under
the award IRI 99-87695.
REFERENCES
[1] J. Aggarwal and Q. Cai, “Human Motion Analysis: A Review,”
Computer Vision and Image Understanding, vol. 73, pp. 428-440, 1999.
[2] F. Aherne, N. Thacker, and P. Rockett, “The Bhattacharyya Metric
as an Absolute Similarity Measure for Frequency Coded Data,”
Kybernetika, vol. 34, no. 4, pp. 363-368, 1998.
[3] S. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, “A Tutorial
on Particle Filters for On-Line Non-Linear/Non-Gaussian Baye-
sian Tracking,” IEEE Trans. Signal Processing, vol. 50, no. 2, pp. 174-
189, 2002.
[4] S. Avidan, “Support Vector Tracking,” Proc. IEEE Conf. Computer
Vision and Pattern Recognition, vol. I, pp. 184-191, 2001.
[5] Y. Bar-Shalom and T. Fortmann, Tracking and Data Association.
Academic Press. 1988.
[6] B. Bascle and R. Deriche, “Region Tracking through Image
Sequences,” Proc. Fifth Int’l Conf. Computer Vision, pp. 302-307, 1995.
[7] S. Birchfield, “Elliptical Head Tracking Using Intensity Gradients
and Color Histograms,” Proc. IEEE Conf. Computer Vision and
Pattern Recognition, pp. 232-237, 1998.
[8] M. Black and D. Fleet, “Probabilistic Detection and Tracking of
Motion Boundaries,” Int’l J. Computer Vision, vol. 38, no. 3, pp. 231-
245, 2000.
[9] Y. Boykov and D. Huttenlocher, “Adaptive Bayesian Recognition
in Tracking Rigid Objects,” Proc. IEEE Conf. Computer Vision and
Pattern Recognition, pp. 697-704, 2000.
[10] G.R. Bradski, “Computer Vision Face Tracking as a Component of
a Perceptual User Interface,” Proc. IEEE Workshop Applications of
Computer Vision, pp. 214-219, Oct. 1998.
[11] A.D. Bue, D. Comaniciu, V. Ramesh, and C. Regazzoni, “Smart
Cameras with Real-Time Video Object Generation,” Proc. IEEE
Int’l Conf. Image Processing, vol. III, pp. 429-432, 2002.
[12] G. Caenen, V. Ferrari, A. Zalesny, L. VanGool, “Analyzing the
Layout of Composite Textures,” Proc. Texture 2002 Workshop,
pp. 15-19, 2002.
[13] T. Cham and J. Rehg, “A Multiple Hypothesis Approach to Figure
Tracking,” Proc. IEEE Conf. Computer Vision and Pattern Recogni-
tion, vol. II, pp. 239-219, 1999.
[14] H. Chen and T. Liu, “Trust-Region Methods for Real-Time
tracking,” Proc. Eigth Int’l Conf. Computer Vision, vol. II, pp. 717-
722, 2001.
[15] Y. Chen, Y. Rui, and T. Huang, “JPDAF-Based HMM for Real-
Time Contour Tracking,” Proc. IEEE Conf. Computer Vision and
Pattern Recognition, vol. I, pp. 543-550, 2001.
[16] R. Collins, A. Lipton, H. Fujiyoshi, and T. Kanade, “Algorithms
for Cooperative Multisensor Surveillance,” Proc. IEEE, vol. 89,
no. 10, pp. 1456-1477, 2001.
[17] D. Comaniciu and P. Meer, “Mean Shift: A Robust Approach
Toward Feature Space Analysis,” IEEE Trans. Pattern Analysis and
Machine Intelligence, vol. 24, no. 5, pp. 603-619, May 2002.
[18] D. Comaniciu, V. Ramesh, and P. Meer, “Real-Time Tracking of
Non-Rigid Objects Using Mean Shift,” Proc. IEEE Conf. Computer
Vision and Pattern Recognition, vol. II, pp. 142-149, June 2000.
[19] T. Cover and J. Thomas, Elements of Information Theory. New York:
John Wiley & Sons, 1991.
[20] I. Cox and S. Hingorani, “An Efficient Implementation of Reid’s
Multiple Hypothesis Tracking Algorithm and Its Evaluation for
the Purpose of Visual Tracking,” IEEE Trans. Pattern Analysis and
Machine Intelligence, vol. 18, no. 2, pp. 138-150, Feb. 1996.
[21] D. DeCarlo and D. Metaxas, “Optical Flow Constraints on
Deformable Models with Applications to Face Tracking,” Int’l J.
Computer Vision, vol. 38, no. 2, pp. 99-127, 2000.
[22] A. Djouadi, O. Snorrason, and F. Garber, “The Quality of Training-
Sample Estimates of the Bhattacharyya Coefficient,” IEEE Trans.
Pattern Analysis and Machine Intelligence, vol. 12, pp. 92-97, 1990.
[23] A. Doucet, S. Godsill, and C. Andrieu, “On Sequential Monte
Carlo Sampling Methods for Bayesian Filtering,” Statistics and
Computing, vol. 10, no. 3, pp. 197-208, 2000.
[24] A. Elgammal, D. Harwood, and L. Davis, “Non-Parametric Model
for Background Subtraction,” Proc. European Conf. Computer Vision,
vol. II, pp. 751-767, June 2000.
[25] F. Ennesser and G. Medioni, “Finding Waldo, or Focus of Attention
Using Local Color Information,” IEEE Trans. Pattern Analysis and
Machine Intelligence, vol. 17, no. 8, pp. 805-809, Aug. 1995.
[26] V. Ferrari, T. Tuytelaars, and L.V. Gool, “Real-Time Affine Region
Tracking and Coplanar Grouping,” Proc. IEEE Conf. Computer
Vision and Pattern Recognition, vol. II, pp. 226-233, 2001.
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 575
2. A patent application has been filed covering the tracking algorithm
together with the extensions and various applications [79].
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
[27] P. Fieguth and D. Teropoulos, “Color-Based Tracking of Heads
and Other Mobile Objects at Video Frame Rates,” Proc. IEEE Conf.
Computer Vision and Pattern Recognition, pp. 21-27, 1997.
[28] K. Fukunaga, Introduction to Statistical Pattern Recognition, second
ed. Academic Press, 1990.
[29] J. Garcia, J. Valdivia, and X. Vidal, “Information Theoretic Measure
for Visual Target Distinctness,” IEEE Trans. Pattern Analysis and
Machine Intelligence, vol. 23, no. 4, pp. 362-383, Apr. 2001.
[30] D. Gavrila, “The Visual Analysis of Human Movement: A Survey,”
Computer Vision and Image Understanding, vol. 73, pp. 82-98, 1999.
[31] N. Gordon, D. Salmond, and A. Smith, “A Novel Approach to
Non-Linear and Non-Gaussian Bayesian State Estimation,” IEE
Proc.-F, vol. 140, pp. 107-113, 1993.
[32] M. Greiffenhagen, D. Comaniciu, H. Niemann, and V. Ramesh,
“Design, Analysis and Engineering of Video Monitoring Systems:
An Approach and a Case Study,” Proc. IEEE, vol. 89, no. 10,
pp. 1498-1517, 2001.
[33] G. Hager and P. Belhumeur, “Real-Time Tracking of Image
Regions with Changes in Geometry and Illumination,” Proc. IEEE
Conf. Computer Vision and Pattern Recognition, pp. 403-410, 1996.
[34] U. Handmann, T. Kalinke, C. Tzomakas, M. Werner, and W. von
Seelen, “Computer Vision for Driver Assistance Systems,” Proc.
SPIE, vol. 3364, pp. 136-147, 1998.
[35] I. Haritaoglu and M. Flickner, “Detection and Tracking of
Shopping Groups in Stores,” Proc. IEEE Conf. Computer Vision
and Pattern Recognition, vol. 1, pp. 431-438, 2001.
[36] I. Haritaoglu, D. Harwood, and L. Davis, “W4: Who? When?
Where? What?—A Real Time System for Detecting and Tracking
People,” Proc. of Int’l Conf. Automatic Face and Gesture Recognition,
pp. 222-227, 1998.
[37] J. Huang, S. Kumar, M. Mitra, W. Zhu, and R. Zabih, “Spatial Color
Indexing and Applications,” Int’l J. Computer Vision, vol. 35, no. 3,
pp. 245-268, 1999.
[38] C. Hue, J. Cadre, and P. Perez, “Sequential Monte Carlo Filtering
for Multiple Target Tracking and Data Fusion,” IEEE Trans. Signal
Processing, vol. 50, no. 2, pp. 309-325, 2002.
[39] S. Intille, J. Davis, and A. Bobick, “Real-Time Closed-World
Tracking,” Proc. IEEE Conf. Computer Vision and Pattern Recogni-
tion, pp. 697-703, 1997.
[40] M. Isard and A. Blake, “Condensation-Conditional Density
Propagation for Visual Tracking,” Int’l J. Computer Vision, vol. 29,
no. 1, 1998.
[41] A. Jepson, D. Fleet, and T. El-Maraghi, “Robust Online Appear-
ance Models for Visual Tracking,” Proc. IEEE Conf. Computer
Vision and Pattern Recognition, vol. I, pp. 415-422, 2001.
[42] S. Julier and J. Uhlmann, “A New Extension of the Kalman Filter
to Nonlinear Systems,” Proc. SPIE, vol. 3068, pp. 182-193, 1997.
[43] T. Kailath, “The Divergence and Bhattacharyya Distance Mea-
sures in Signal Selection,” IEEE Trans. Comm. Technology, vol. 15,
pp. 52-60, 1967.
[44] V. Kettnaker and R. Zabih, “Bayesian Multi-Camera Surveil-
lance,” Proc. IEEE Conf. Computer Vision and Pattern Recognition,
pp. 253-259, 1999.
[45] G. Kitagawa, “Non-Gaussian State-Space Modeling of Nonsta-
tionary Time Series,” J. Am. Statistical Assoc., vol. 82, pp. 1032-
1063, 1987.
[46] S. Konishi, A. Yuille, J. Coughlan, and S. Zhu, “Fundamental
Bounds on Edge Detection: An Information Theoretic Evaluation
of Different Edge Cues,” Proc. IEEE Conf. Computer Vision and
Pattern Recognition, pp. 573-579, 1999.
[47] J. Krumm, S. Harris, B. Meyers, B. Brumitt, M. Hale, and S. Shafer,
“Multi-Camera Multi-Person Tracking for Easy Living,” Proc.
IEEE Int’l Workshop Visual Surveillance, pp. 3-10, 2000.
[48] B. Li and R. Chellappa, “Simultaneous Tracking and Verification
via Sequential Posterior Estimation,” Proc. IEEE Conf. Computer
Vision and Pattern Recognition, vol. II, pp. 110-117, 2000.
[49] J. Lin, “Divergence Measures Based on the Shannon Entropy,”
IEEE Trans. Information Theory, vol. 37, pp. 145-151, 1991.
[50] A. Lipton, H. Fujiyoshi, and R. Patil, “Moving Target Classifica-
tion and Tracking from Real-Time Video,” Proc. IEEE Workshop
Applications of Computer Vision, pp. 8-14, 1998.
[51] J. MacCormick and A. Blake, “A Probabilistic Exclusion Principle
for Tracking Multiple Objects,” Int’l J. Computer Vision, vol. 39,
no. 1, pp. 57-71, 2000.
[52] R. Mahler, “Engineering Statistics for Multi-Object Tracking,”
Proc. IEEE Workshop Multi-Object Tracking, pp. 53-60, 2001.
[53] S. McKenna, Y. Raja, and S. Gong, “Tracking Colour Objects Using
Adaptive Mixture Models,” Image and Vision Computing J., vol. 17,
pp. 223-229, 1999.
[54] R. Merwe, A. Doucet, N. Freitas, and E. Wan, “The Unscented
Particle Filter,” Technical Report CUED/F-INFENG/TR 380, Eng.
Dept., Cambridge Univ., 2000.
[55] K. Nickels and S. Hutchinson, “Estimating Uncertainty in SSD-
Based Feature Tracking,” Image and Vision Computing, vol. 20,
pp. 47-58, 2002.
[56] C. Olson, “Image Registration by Aligning Entropies,” Proc. IEEE
Conf. Computer Vision and Pattern Recognition, vol. II, pp. 331-336,
2001.
[57] P. Perez, C. Hue, J. Vermaak, and M. Gangnet, “Color-Based
Probabilistic Tracking,” Proc. European Conf. Computer Vision, vol. I,
pp. 661-675, 2002.
[58] W. Press, S. Teukolsky, W. Vetterling, and B. Flannery, Numerical
Recipes in C, second ed. Cambridge Univ. Press, 1992.
[59] J. Puzicha, Y. Rubner, C. Tomasi, and J. Buhmann, “Empirical
Evaluation of Dissimilarity Measures for Color and Texture,” Proc.
Seventh Int’l Conf. Computer Vision, pp. 1165-1173, 1999.
[60] L. Rabiner, “A Tutorial on Hidden Markov Models and Selected
Applications in Speech Recognition,” Proc. IEEE, vol. 77, no. 2,
pp. 257-285, 1989.
[61] C. Rao, A. Yilmaz, and M. Shah, “View-Invariant Representation
and Recognition of Actions,” Int’l J. Computer Vision, vol. 50, no. 2,
pp. 203-226, Nov. 2002.
[62] C. Rasmussen, G. Hager, “Probabilistic Data Association Methods
for Tracking Complex Visual Objects,” IEEE Trans. Pattern
Analysis and Machine Intelligence, vol. 23, no. 6, pp. 560-576, June
2001.
[63] D. Reid, “An Algorithm for Tracking Multiple Targets,” IEEE
Trans. Automatic Control, vol. AC-24, pp. 843-854, 1979.
[64] A. Roche, G. Malandain, and N. Ayache, “Unifying Maximum
Likelihood Approaches in Medical Image Registration,” Technical
Report 3741, INRIA, 1999.
[65] R. Rosales and S. Sclaroff, “3D Trajectory Recovery for Tracking
Multiple Objects and Trajectory Guided Recognition of Actions,”
Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 117-
123, 1999.
[66] Y. Rui and Y. Chen, “Better Proposal Distributions: Object
Tracking Using Unscented Particle Filter,” Proc. IEEE Conf.
Computer Vision and Pattern Recognition, vol. II, pp. 786-793, 2001.
[67] S. Sclaroff and J. Isidoro, “Active Blobs,” Proc. Sixth Int’l Conf.
Computer Vision, pp. 1146-1153, 1998.
[68] D.W. Scott, Multivariate Density Estimation. Wiley, 1992.
[69] C. Sminchisescu and B. Triggs, “Covariance Scaled Sampling for
Monocular 3D Body Tracking,” Proc. IEEE Conf. Computer Vision
and Pattern Recognition, vol. I, pp. 447-454, 2001.
[70] J. Sullivan and J. Rittscher, “Guiding Random Particles by
Deterministic Search,” Proc. Eighth Int’l Conf. Computer Vision,
vol. I, pp. 323-330, 2001.
[71] M. Swain and D. Ballard, “Color Indexing,” Int’l J. Computer
Vision, vol. 7, no. 1, pp. 11-32, 1991.
[72] P. Viola and W. Wells, “Alignment by Maximization of Mutual
Information,” Int’l J. Computer Vision, vol. 24, no. 2, pp. 137-154,
1997.
[73] S. Wachter and H. Nagel, “Tracking Persons in Monocular Image
Sequences,” Computer Vision and Image Understanding, vol. 74, no. 3,
pp. 174-192, 1999.
[74] R. Wildes, R. Kumar, H. Sawhney, S. Samasekera, S. Hsu, H. Tao,
Y. Guo, K. Hanna, A. Pope, D. Hirvonen, M. Hansen, and P. Burt,
“Aerial Video Surveillance and Exploitation,” Proc. IEEE, vol. 89,
no. 10, pp. 1518-1539, 2001.
[75] C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland, “Pfinder:
Real-Time Tracking of the Human Body,” IEEE Trans. Pattern
Analysis and Machine Intelligence, vol. 19, pp. 780-785, 1997.
[76] Y. Wu and T. Huang, “A Co-Inference Approach to Robust
Tracking,” Proc. Eigth Int’l Conf. Computer Vision, vol. II, pp. 26-33,
2001.
[77] F. Xu and K. Fujimura, “Pedestrian Detection and Tracking with
Night Vision,” Proc. IEEE Intelligent Vehicle Symp., 2002.
[78] A. Yilmaz, K. Shafique, N. Lobo, X. Li, T. Olson, and M. Shah,
“Target Tracking in FLIR Imagery Using Mean Shift and Global
Motion Compensation,” IEEE Workshop Computer Vision Beyond
Visible Spectrum, 2001.
[79] “Real-Time Tracking of Non-Rigid Objects Using Mean Shift,”US
patent pending, 2000.
576 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
Dorin Comaniciu received the PhD degrees in
electrical engineering from the Polytechnic Uni-
versity of Bucharest in 1995 and from Rutgers
University in 1999. He is currently a project
manager and scientist in the Real-Time Vision
and Modeling Department at Siemens Corporate
Research in Princeton, New Jersey. His re-
search interests include robust methods for
autonomous computer vision, multiple motion
estimation, nonparametric analysis, real-time
vision systems, medical imaging, content-based access to visual data,
and data compression. He has coauthored more than 60 papers,
conference papers, and book chapters in the area of visual information
processing. He is an associate editor of Pattern Analysis and
Applications. Dr. Comaniciu has received the Best Paper Award at the
IEEE Conference on Computer Vision and Pattern Recognition 2000
and is a coauthor of the upcoming book Nonparametric Analysis of
Visual Data: The Mean Shift Paradigm. He is a senior member of the
IEEE.
Visvanathan Ramesh received the BE degree
from the College of Engineering, Guindy, Ma-
dras, India. He received the MS degree in
electrical engineering from Virginia Tech and
the doctoral degree from the Department of
Electrical Engineering at the University of
Washington, where he defended his PhD dis-
sertation titled “Performance Characterization of
Image Understanding Algorithms” in 1994. A
major focus of his research is to build robust
video-understanding systems and to quantify robustness of video-
understanding algorithms. During the course of his PhD work, he
developed a systems engineering methodology for computer vision
algorithm performance characterization and design. Dr. Ramesh is
currently the head of the Real-Time Vision and Modeling Department at
Siemens Corporate Research. His research program has developed
real-time vision systems (tracking and low-level image processing) for
applications in video surveillance and monitoring, computer vision
software systems, statistical modeling techniques for computer vision
algorithm design, analysis, and performance characterization. He was
also the coauthor of a paper on real-time tracking that received the Best
Paper Award at Computer Vision and Pattern Recognition in 2000. He is
a member of the IEEE.
Peter Meer received the Dipl. Engn. degree
from the Bucharest Polytechnic Institute, Bu-
charest, Romania, in 1971, and the DSc degree
from the Technion, Israel Institute of Technol-
ogy, Haifa, Israel, in 1986, both in electrical
engineering. From 1971 to 1979, he was with the
Computer Research Institute, Cluj, Romania,
working on the R&D of digital hardware. Be-
tween 1986 and 1990, he was an assistant
research scientist at the Center for Automation
Research, University of Maryland at College Park. In 1991, he joined the
Department of Electrical and Computer Engineering, Rutgers University,
Piscataway, New Jersey, where he is currently a full professor. He has
held visiting appointments in Japan, Korea, Sweden, Israel, and France,
and was on the organizing committees of numerous international
workshops and conferences. He is an associate editor of the IEEE
Transaction on Pattern Analysis and Machine Intelligence, a member of
the editorial board of Pattern Recognition, and was a guest editor of
Computer Vision and Image Understanding for the April 2000 special
issue on “Robust Statistical Techniques in Image Understanding.” He
was a coauthor of an award winning paper in pattern recognition in 1989,
the best student paper in the 1999, and the best paper at the 2000 IEEE
Conference on Computer Vision and Pattern Recognition. His research
interest is in application of modern statistical methods to image
understanding problems. He is a senior member of the IEEE.
. For more information on this or any other computing topic,
please visit our Digital Library at http://computer.org/publications/dlib.
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 577
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.

