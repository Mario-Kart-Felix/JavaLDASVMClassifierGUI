Kernel-Based Object Tracking
Dorin Comaniciu, Senior Member, IEEE, Visvanathan Ramesh, Member, IEEE, and
Peter Meer, Senior Member, IEEE
Abstractâ€”A new approach toward target representation and localization, the central component in visual tracking of nonrigid objects,
is proposed. The feature histogram-based target representations are regularized by spatial masking with an isotropic kernel. The
masking induces spatially-smooth similarity functions suitable for gradient-based optimization, hence, the target localization problem
can be formulated using the basin of attraction of the local maxima. We employ a metric derived from the Bhattacharyya coefficient as
similarity measure, and use the mean shift procedure to perform the optimization. In the presented tracking examples, the new method
successfully coped with camera motion, partial occlusions, clutter, and target scale variations. Integration with motion filters and data
association techniques is also discussed. We describe only a few of the potential applications: exploitation of background information,
Kalman tracking using motion models, and face tracking.
Index Termsâ€”Nonrigid object tracking, target localization and representation, spatially-smooth similarity function, Bhattacharyya
coefficient, face tracking.
Ã¦
1 INTRODUCTION
REAL-TIME object tracking is the critical task in manycomputer vision applications such as surveillance [44],
[16], [32], perceptual user interfaces [10], augmented reality
[26], smart rooms [39], [75], [47], object-based video compres-
sion [11], and driver assistance [34], [4].
Two major components can be distinguished in a typical
visual tracker. Target Representation and Localization is mostly a
bottom-up process which has also to cope with the changes in
the appearance of the target. Filtering and Data Association is
mostly a top-down process dealing with the dynamics of the
tracked object, learning of scene priors, and evaluation of
different hypotheses. The way the two components are
combined and weighted is application dependent and plays
a decisive role in the robustness and efficiency of the tracker.
For example, face tracking in a crowded scene relies more on
target representation than on target dynamics [21], while in
aerial video surveillance, e.g., [74], the target motion and the
ego-motion of the camera are the more important compo-
nents. In real-time applications, only a small percentage of the
system resources can be allocated for tracking, the rest being
required for the preprocessing stages or to high-level tasks
such as recognition, trajectory interpretation, and reasoning.
Therefore, it is desirable to keep the computational complex-
ity of a tracker as low as possible.
The most abstract formulation of the filtering and data
association process is through the state space approach for
modeling discrete-time dynamic systems [5]. The informa-
tion characterizing the target is defined by the state
sequence fxkgkÂ¼0;1;..., whose evolution in time is specified
by the dynamic equation xk Â¼ fkÃ°xkÃ¿1;vkÃ. The available
measurements fzkgkÂ¼1;... are related to the corresponding
states through the measurement equation zk Â¼ hkÃ°xk;nkÃ. In
general, both f k and hk are vector-valued, nonlinear, and
time-varying functions. Each of the noise sequences,
fvkgkÂ¼1;... and fnkgkÂ¼1;... is assumed to be independent and
identically distributed (i.i.d.).
The objective of tracking is to estimate the state xk given all
the measurements z1:k up that moment, or equivalently to
construct the probability density function (pdf)pÃ°xkjz1:kÃ. The
theoretically optimal solution is provided by the recursive
Bayesian filter which solves the problem in two steps. The
prediction step uses the dynamic equation and the already
computed pdf of the state at time t Â¼ kÃ¿ 1, pÃ°xkÃ¿1jz1:kÃ¿1Ã, to
derive the prior pdf of the current state, pÃ°xkjz1:kÃ¿1Ã. Then, the
update step employs the likelihood function pÃ°zkjxkÃ of the
current measurement to compute the posterior pdf pÃ°xkjz1:k).
When the noise sequences are Gaussian and fk and hk
are linear functions, the optimal solution is provided by the
Kalman filter [5, p. 56], which yields the posterior being also
Gaussian. (We will return to this topic in Section 6.2.) When
the functions fk and hk are nonlinear, by linearization the
Extended Kalman Filter (EKF) [5, p. 106] is obtained, the
posterior density being still modeled as Gaussian. A recent
alternative to the EKF is the Unscented Kalman Filter (UKF)
[42] which uses a set of discretely sampled points to
parameterize the mean and covariance of the posterior
density. When the state space is discrete and consists of a
finite number of states, Hidden Markov Models (HMM)
filters [60] can be applied for tracking. The most general
class of filters is represented by particle filters [45], also
called bootstrap filters [31], which are based on Monte Carlo
integration methods. The current density of the state is
represented by a set of random samples with associated
weights and the new density is computed based on these
samples and weights (see [23], [3] for reviews). The UKF can
be employed to generate proposal distributions for particle
filters, in which case the filter is called Unscented Particle
Filter (UPF) [54].
When the tracking is performed in a cluttered environ-
ment where multiple targets can be present [52], problems
564 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
. D. Comaniciu and V. Ramesh are with the Real-Time Vision and Modeling
Department, Siemens Corporate Research, 755 College Road East,
Princeton, NJ 08540. E-mail: comanici@scr.siemens.com.
. P. Meer is with the Electrical and Computer Engineering Department,
Rutgers University, 94 Brett Road, Piscataway, NJ 08854-8058.
Manuscript received 21 May 2002; revised 13 Oct. 2002; accepted 16 Oct.
2002.
Recommended for acceptance by M. Irani.
For information on obtaining reprints of this article, please send e-mail to:
tpami@computer.org, and reference IEEECS Log Number 116595.
0162-8828/03/$10.00 ÃŸ 2003 IEEE Published by the IEEE Computer Society
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
related to the validation and association of the measurements
arise [5, p. 150]. Gating techniques are used to validate only
measurements whose predicted probability of appearance is
high. After validation, a strategy is needed to associate the
measurements with the current targets. In addition to the
Nearest Neighbor Filter, which selects the closest measure-
ment, techniques such as Probabilistic Data Association Filter
(PDAF) are available for the single target case. The under-
lying assumption of the PDAF is that for any given target only
one measurement is valid, and the other measurements are
modeled as random interference, that is, i.i.d. uniformly
distributed random variables. The Joint Data Association
Filter (JPDAF) [5, p. 222], on the other hand, calculates the
measurement-to-target association probabilities jointly
across all the targets. A different strategy is represented by
the Multiple Hypothesis Filter (MHF) [63], [20], [5, p. 106]
which evaluates the probability that a given target gave rise to
a certain measurement sequence. The MHF formulation can
be adapted to track the modes of the state density [13]. The
data association problem for multiple target particle filtering
is presented in [62], [38].
The filtering and association techniques discussed above
were applied in computer vision for various tracking
scenarios. Boykov and Huttenlocher [9] employed the Kal-
man filter to track vehicles in an adaptive framework. Rosales
and Sclaroff [65] used the Extended Kalman Filter to estimate
a 3D object trajectory from 2D image motion. Particle filtering
was first introduced, in vision, as the Condensation algorithm
by Isard and Blake [40]. Probabilistic exclusion for tracking
multiple objects was discussed in [51]. Wu and Huang
developed an algorithm to integrate multiple target clues [76].
Li and Chellappa [48] proposed simultaneous tracking and
verification based on particle filters applied to vehicles and
faces. Chen et al. [15] used the Hidden Markov Model
formulation for tracking combined with JPDAF data associa-
tion. Rui and Chen proposed to track the face contour based
on the unscented particle filter [66]. Cham and Rehg [13]
applied a variant of MHF for figure tracking.
The emphasis in this paper is on the other component of
tracking: target representation and localization. While the
filtering and data association have their roots in control
theory, algorithms for target representation and localization
are specific to images and related to registration methods [72],
[64], [56]. Both target localization and registration maximizes
a likelihood type function. The difference is that in tracking,
as opposed to registration, only small changes are assumed in
the location and appearance of the target in two consecutive
frames. This property can be exploited to develop efficient,
gradient-based localization schemes using the normalized
correlation criterion [6]. Since the correlation is sensitive to
illumination, Hager and Belhumeur [33] explicitly modeled
the geometry and illumination changes. The method
was improved by Sclaroff and Isidoro [67] using robust
M-estimators. Learning of appearance models by employing
a mixture of stable image structure, motion information, and
an outlier process, was discussed in [41]. In a different
approach, Ferrari et al. [26] presented an affine tracker based
on planar regions and anchor points. Tracking people, which
raises many challenges due to the presence of large 3D,
nonrigid motion, was extensively analyzed in [36], [1], [30],
[73]. Explicit tracking approaches of people [69] are time-
consuming and often the simpler blob model [75] or adaptive
mixture models [53] are also employed.
The main contribution of the paper is to introduce a new
framework for efficient tracking of nonrigid objects. We
show that by spatially masking the target with an isotropic
kernel, a spatially-smooth similarity function can be defined
and the target localization problem is then reduced to a
search in the basin of attraction of this function. The
smoothness of the similarity function allows application of
a gradient optimization method which yields much faster
target localization compared with the (optimized) exhaus-
tive search. The similarity between the target model and the
target candidates in the next frame is measured using the
metric derived from the Bhattacharyya coefficient. In our
case, the Bhattacharyya coefficient has the meaning of a
correlation score. The new target representation and
localization method can be integrated with various motion
filters and data association techniques. We present tracking
experiments in which our method successfully coped with
complex camera motion, partial occlusion of the target,
presence of significant clutter, and large variations in target
scale and appearance. We also discuss the integration of
background information and Kalman filter based tracking.
The paper is organized as follows: Section 2 discusses
issues of target representation and the importance of a
spatially-smooth similarity function. Section 3 introduces
the metric derived from the Bhattacharyya coefficient. The
optimization algorithm is described in Section 4. Experi-
mental results are shown in Section 5. Section 6 presents
extensions of the basic algorithm and the new approach is
put in the context of computer vision literature in Section 7.
2 TARGET REPRESENTATION
To characterize the target, first a feature space is chosen.
The reference target model is represented by its pdf q in the
feature space. For example, the reference model can be
chosen to be the color pdf of the target. Without loss of
generality, the target model can be considered as centered
at the spatial location 0. In the subsequent frame, a target
candidate is defined at location y, and is characterized by the
pdf pÃ°yÃ. Both pdfs are to be estimated from the data. To
satisfy the low-computational cost imposed by real-time
processing discrete densities, i.e., m-bin histograms should
be used. Thus, we have
target model : qÌ‚ Â¼ qÌ‚uf guÂ¼1...m
Xm
uÂ¼1
qÌ‚u Â¼ 1
target candidate : pÌ‚Ã°yÃ Â¼ pÌ‚uÃ°yÃf guÂ¼1...m
Xm
uÂ¼1
pÌ‚u Â¼ 1:
The histogram is not the best nonparametric density
estimate [68], but it suffices for our purposes. Other discrete
density estimates can be also employed.
We will denote by
Ì‚Ã°yÃ  Â½pÌ‚Ã°yÃ; qÌ‚ÂŠ Ã°1Ã
a similarity function between pÌ‚ and qÌ‚. The function Ì‚Ã°yÃplays
the role of a likelihood and its local maxima in the image
indicate the presence of objects in the second frame having
representations similar to qÌ‚ defined in the first frame. If only
spectral information is used to characterize the target, the
similarity function can have large variations for adjacent
locations on the image lattice and the spatial information is
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 565
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
lost. To find the maxima of such functions, gradient-based
optimization procedures are difficult to apply and only an
expensive exhaustive search can be used. We regularize the
similarity function by masking the objects with an isotropic
kernel in the spatial domain. When the kernel weights,
carrying continuous spatial information, are used in defining
the feature space representations, Ì‚Ã°yÃ becomes a smooth
function in y.
2.1 Target Model
A target is represented by an ellipsoidal region in the
image. To eliminate the influence of different target
dimensions, all targets are first normalized to a unit circle.
This is achieved by independently rescaling the row and
column dimensions with hx and hy.
Let x?i
 	
iÂ¼1...n be the normalized pixel locations in the
region defined as the target model. The region is centered at
0. An isotropic kernel, with a convex and monotonic
decreasing kernel profile kÃ°xÃ,1 assigns smaller weights to
pixels farther from the center. Using these weights increases
the robustness of the density estimation since the peripheral
pixels are the least reliable, being often affected by
occlusions (clutter) or interference from the background.
The function b : R2 ! 1 . . .mf g associates to the pixel at
location x?i the index bÃ°x?i Ã of its bin in the quantized feature
space. The probability of the feature u Â¼ 1 . . .m in the target
model is then computed as
qÌ‚u Â¼ C
Xn
iÂ¼1
k kx?i k
2
 
 bÃ°x?i Ã Ã¿ u
 
; Ã°2Ã
where  is the Kronecker delta function. The normalization
constantC is derived by imposing the condition
Pm
uÂ¼1 qÌ‚u Â¼ 1,
from where
C Â¼ 1Pn
iÂ¼1 k kx?i k
2
  ; Ã°3Ã
since the summation of delta functions for u Â¼ 1 . . .m is
equal to one.
2.2 Target Candidates
Let xif giÂ¼1...nh be the normalized pixel locations of the target
candidate, centered at y in the current frame. The normal-
ization is inherited from the frame containing the target
model. Using the same kernel profile kÃ°xÃ, but with
bandwidth h, the probability of the feature u Â¼ 1 . . .m in the
target candidate is given by
pÌ‚uÃ°yÃ Â¼ Ch
Xnh
iÂ¼1
k
yÃ¿ xi
h
 2  bÃ°xiÃ Ã¿ uÂ½ ÂŠ; Ã°4Ã
where
Ch Â¼
1Pnh
iÂ¼1 kÃ°k
yÃ¿xi
h k
2Ã
Ã°5Ã
is the normalization constant. Note that Ch does not depend
on y, since the pixel locations xi are organized in a regular
lattice and y is one of the lattice nodes. Therefore, Ch can be
precalculated for a given kernel and different values of h. The
bandwidth h defines the scale of the target candidate, i.e., the
number of pixels considered in the localization process.
2.3 Similarity Function Smoothness
The similarity function (1) inherits the properties of the kernel
profile kÃ°xÃ when the target model and candidate are
represented according to (2) and (4). A differentiable kernel
profile yields a differentiable similarity function and efficient
gradient-based optimizations procedures can be used for
finding its maxima. The presence of the continuous kernel
introduces an interpolation process between the locations on
the image lattice. The employed target representations do not
restrict the way similarity is measured and various functions
can be used for . See [59] for an experimental evaluation of
different histogram similarity measures.
3 METRIC BASED oN BHATTACHARYYA
COEFFICIENT
The similarity function defines a distance among target
model and candidates. To accommodate comparisons among
various targets, this distance should have a metric structure.
We define the distance between two discrete distributions as
dÃ°yÃ Â¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1Ã¿  pÌ‚Ã°yÃ; qÌ‚Â½ ÂŠ
p
; Ã°6Ã
where we chose
Ì‚Ã°yÃ   pÌ‚Ã°yÃ; qÌ‚Â½ ÂŠ Â¼
Xm
uÂ¼1
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
pÌ‚uÃ°yÃqÌ‚u
p
; Ã°7Ã
the sample estimate of the Bhattacharyya coefficient
between p and q [43].
The Bhattacharyya coefficient is a divergence-type mea-
sure [49] which has a straightforward geometric interpreta-
tion. It is the cosine of the angle between the m-dimensional
unit vectors
ffiffiffiffiffi
pÌ‚1
p
; . . . ;
ffiffiffiffiffiffi
pÌ‚m
pÃ¿ >
and
ffiffiffiffi
qÌ‚1
p
; . . . ;
ffiffiffiffiffi
qÌ‚m
pÃ¿ >
. The fact
that p and q are distributions is thus explicitly taken into
account by representing them on the unit hypersphere. At the
same time, we can interpret (7) as the (normalized) correlation
between the vectors
ffiffiffiffiffi
pÌ‚1
p
; . . . ;
ffiffiffiffiffiffi
pÌ‚m
pÃ¿ >
and
ffiffiffiffi
qÌ‚1
p
; . . . ;
ffiffiffiffiffi
qÌ‚m
pÃ¿ >
.
Properties of the Bhattacharyya coefficient such as its relation
to the Fisher measure of information, quality of the sample
estimate, and explicit forms for various distributions are
given in [22], [43].
The statistical measure (6) has several desirable properties:
1. It imposes a metric structure (see Appendix). The
Bhattacharyya distance [28, p. 99] or Kullback
divergence [19, p. 18] are not metrics since they violate
at least one of the distance axioms.
2. It has a clear geometric interpretation. Note that the
Lp histogram metrics (including histogram intersec-
tion [71]) do not enforce the conditions
Pm
uÂ¼1 qÌ‚u Â¼ 1
and
Pm
uÂ¼1 pÌ‚u Â¼ 1.
3. It uses discrete densities and, therefore, it is invariant
to the scale of the target (up to quantization effects).
4. It is valid for arbitrary distributions, thus being
superior to the Fisher linear discriminant, which
yields useful results only for distributions that are
separated by the mean-difference [28, p. 132].
566 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
1. The profile of a kernel K is defined as a function k : Â½0;1Ã ! R such
that KÃ°xÃ Â¼ kÃ°kxk2Ã.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
5. It approximates the chi-squared statistic, while
avoiding the singularity problem of the chi-square
test when comparing empty histogram bins [2].
Divergence-based measures were already used in com-
puter vision. The Chernoff and Bhattacharyya bounds have
been employed in [46] to determine the effectiveness of edge
detectors. The Kullback divergence between joint distribu-
tion and product of marginals (e.g., the mutual information)
has been used in [72] for registration. Information theoretic
measures for target distinctness were discussed in [29].
4 TARGET LOCALIZATION
To find the location corresponding to the target in the current
frame, the distance (6) should be minimized as a function of y.
The localization procedure starts from the position of the
target in the previous frame (the model) and searches in the
neighborhood. Since our distance function is smooth, the
procedure uses gradient information which is provided by
the mean shift vector [17]. More involved optimizations
based on the Hessian of (6) can be applied [58].
Color information was chosen as the target feature,
however, the same framework can be used for texture and
edges, or any combination of them. In the sequel, it is assumed
that the following information is available: 1) detection and
localization in the initial frame of the objects to track (target
models) [50], [8] and 2) periodic analysis of each object to
account for possible updates of the target models due to
significant changes in color [53].
4.1 Distance Minimization
Minimizing the distance (6) is equivalent to maximizing the
Bhattacharyya coefficient Ì‚Ã°yÃ. The search for the new target
location in the current frame starts at the location yÌ‚0 of the
target in the previous frame. Thus, the probabilities
pÌ‚uÃ°yÌ‚0Ãf guÂ¼1...m of the target candidate at location yÌ‚0 in the
current frame have to be computed first. Using Taylor
expansion around the values pÌ‚uÃ°yÌ‚0Ã, the linear approxima-
tion of the Bhattacharyya coefficient (7) is obtained after
some manipulations as
 pÌ‚Ã°yÃ; qÌ‚Â½ ÂŠ  1
2
Xm
uÂ¼1
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
pÌ‚uÃ°yÌ‚0ÃqÌ‚u
p
Ã¾ 1
2
Xm
uÂ¼1
pÌ‚uÃ°yÃ
ffiffiffiffiffiffiffiffiffiffiffiffiffi
qÌ‚u
pÌ‚uÃ°yÌ‚0Ã
s
: Ã°8Ã
The approximation is satisfactory when the target candidate
pÌ‚uÃ°yÃf guÂ¼1...m does not change drastically from the initial
pÌ‚uÃ°yÌ‚0Ãf guÂ¼1...m, which is most often a valid assumption
between consecutive frames. The condition pÌ‚uÃ°yÌ‚0Ã > 0 (or
some small threshold) for all u Â¼ 1 . . .m, can always be
enforced by not using the feature values in violation.
Recalling (4) results in
 pÌ‚Ã°yÃ; qÌ‚Â½ ÂŠ  1
2
Xm
uÂ¼1
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
pÌ‚uÃ°yÌ‚0ÃqÌ‚u
p
Ã¾ Ch
2
Xnh
iÂ¼1
wik
yÃ¿ xi
h
 2 ; Ã°9Ã
where
wi Â¼
Xm
uÂ¼1
ffiffiffiffiffiffiffiffiffiffiffiffiffi
qÌ‚u
pÌ‚uÃ°yÌ‚0Ã
s
 bÃ°xiÃ Ã¿ uÂ½ ÂŠ: Ã°10Ã
Thus, to minimize thedistance (6), the second term in (9) has to
be maximized, the first term being independent of y. Observe
that the second term represents the density estimate
computed with kernel profile kÃ°xÃ at y in the current frame,
with the data being weighted by wi (10). The mode of this
density in the local neighborhood is the sought maximum that
can be found employing the mean shift procedure [17]. In this
procedure, the kernel is recursively moved from the current
location yÌ‚0 to the new location yÌ‚1 according to the relation
yÌ‚1 Â¼
Pnh
iÂ¼1 xiwig
yÌ‚0Ã¿xi
h
 2 
Pnh
iÂ¼1 wig
yÌ‚0Ã¿xi
h
 2  ; Ã°11Ã
where gÃ°xÃ Â¼ Ã¿k0Ã°xÃ, assuming that the derivative of kÃ°xÃ
exists for all x 2 Â½0;1Ã, except for a finite set of points. The
complete target localization algorithm is presented below:
Bhattacharyya Coefficient  pÌ‚Ã°yÃ; qÌ‚Â½ ÂŠMaximization:
Given:
The target model qÌ‚uf guÂ¼1...m and its location yÌ‚0 in the
previous frame.
1. Initialize the location of the target in the current frame
with yÌ‚0, compute pÌ‚uÃ°yÌ‚0Ãf guÂ¼1...m, and evaluate
 pÌ‚Ã°yÌ‚0Ã; qÌ‚Â½ ÂŠ Â¼
Xm
uÂ¼1
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
pÌ‚uÃ°yÌ‚0ÃqÌ‚u
p
:
2. Derive the weights wif giÂ¼1...nh according to (10).
3. Find the next location of the target candidate according
to (11).
4. Compute pÌ‚uÃ°yÌ‚1Ãf guÂ¼1...m, and evaluate
 pÌ‚Ã°yÌ‚1Ã; qÌ‚Â½ ÂŠ Â¼
Xm
uÂ¼1
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
pÌ‚uÃ°yÌ‚1ÃqÌ‚u
p
:
5. While  pÌ‚Ã°yÌ‚1Ã; qÌ‚Â½ ÂŠ <  pÌ‚Ã°yÌ‚0Ã; qÌ‚Â½ ÂŠ
Do yÌ‚1  12 yÌ‚0 Ã¾ yÌ‚1Ã° Ã
Evaluate  pÌ‚Ã°yÌ‚1Ã; qÌ‚Â½ ÂŠ
6. If kyÌ‚1 Ã¿ yÌ‚0k <  Stop.
Otherwise Set yÌ‚0  yÌ‚1 and go to Step 2.
4.2 Implementation of the Algorithm
The stopping criterion threshold used in Step 6 is derived by
constraining the vectors yÌ‚0 and yÌ‚1 to be within the same pixel
in original image coordinates. A lower threshold will induce
subpixel accuracy. From real-time constraints (i.e., uniform
CPU load in time), we also limit the number of mean shift
iterations toNmax, typically taken equal to 20. In practice, the
average number of iterations is much smaller, about 4.
Implementation of the tracking algorithm can be much
simpler than as presented above. The role of Step 5 is only to
avoid potential numerical problems in the mean shift based
maximization. These problems can appear due to the linear
approximation of the Bhattacharyya coefficient. However, a
large set of experiments tracking different objects for long
periods of time, has shown that the Bhattacharyya coefficient
computed at the new location yÌ‚1 failed to increase in only
0.1 percent of the cases. Therefore, the Step 5 is not used in
practice, and as a result, there is no need to evaluate the
Bhattacharyya coefficient in Steps 1 and 4.
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 567
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
In the practical algorithm, we only iterate by computing
the weights in Step 2, deriving the new location in Step 3, and
testing the size of the kernel shift in Step 6. The Bhattacharyya
coefficient is computed only after the algorithm completion to
evaluate the similarity between the target model and the
chosen candidate.
Kernels with Epanechnikov profile [17]
kÃ°xÃ Â¼
1
2 c
Ã¿1
d Ã°dÃ¾ 2ÃÃ°1Ã¿ xÃ if x  1
0 otherwise:

Ã°12Ã
are recommended to be used. In this case, the derivative of
the profile, gÃ°xÃ, is constant and (11) reduces to
yÌ‚1 Â¼
Pnh
iÂ¼1 xiwiPnh
iÂ¼1 wi
; Ã°13Ã
i.e., a simple weighted average.
The maximization of the Bhattacharyya coefficient can be
also interpreted as a matched filtering procedure. Indeed,
(7) is the correlation coefficient between the unit vectors
ffiffiffiÌ‚
q
p
and
ffiffiffiffiffiffiffiffiffiffi
pÌ‚Ã°yÃ
p
, representing the target model and candidate.
The mean shift procedure thus finds the local maximum of
the scalar field of correlation coefficients.
Will call the operational basin of attraction the region in the
current frame in which the new location of the target can be
found by the proposed algorithm. Due to the use of kernels,
this basin is at least equal to the size of the target model. In
other words, if in the current frame the center of the target
remains in the image area covered by the target model in the
previous frame, the local maximum of the Bhattacharyya
coefficient is a reliable indicator for the new target location.
We assume that the target representation provides sufficient
discrimination, such that the Bhattacharyya coefficient
presents a unique maximum in the local neighborhood.
The mean shift procedure finds a root of the gradient as
function of location, which can, however, also correspond to a
saddle point of the similarity surface. The saddle points are
unstable solutions and since the image noise acts as an
independent perturbation factor across consecutive frames,
they cannot influence the tracking performance in an image
sequence.
4.3 Adaptive Scale
According to the algorithm described in Section 4.1, for a
given target model, the location of the target in the current
frame minimizes the distance (6) in the neighborhood of the
previous location estimate. However, the scale of the target
often changes in time and, thus, in (4) the bandwidth h of
the kernel profile has to be adapted accordingly. This is
possible due to the scale invariance property of (6).
Denote by hprev the bandwidth in the previous frame. We
measure the bandwidth hopt in the current frame by running
the target localization algorithm three times, with band-
widths h Â¼ hprev, h Â¼ hprev Ã¾h, and h Â¼ hprev Ã¿h. A
typical value is h Â¼ 0:1hprev. The best result, hopt, yielding
the largest Bhattacharyya coefficient, is retained. To avoid
oversensitive scale adaptation, the bandwidth associated
with the current frame is obtained through filtering
hnew Â¼ hopt Ã¾ Ã°1Ã¿ Ãhprev; Ã°14Ã
where the default value for  is 0.1. Note that the sequence
of hnew contains important information about the dynamics
of the target scale which can be further exploited.
4.4 Computational Complexity
Let N be the average number of iterations per frame. In Step 2,
the algorithm requires computing the representation
pÌ‚uÃ°yÃf guÂ¼1...m. Weighted histogram computation has roughly
the same cost as the unweighted histogram since the kernel
values are precomputed. In Step 3, the centroid (13) is
computed which involves a weighted sum of items repre-
senting the square-root of a division of two terms. We
conclude that the mean cost of the proposed algorithm for one
scale is approximately given by
CO Â¼ NÃ°cH Ã¾ nhcSÃ  NnhcS; Ã°15Ã
where cH is the cost of the histogram and cS the cost of an
addition, a square-root, and a division. We assume that the
number of histogram entries m and the number of target
pixels nh are in the same range.
It is of interest to compare the complexity of the new
algorithm with that of target localization without gradient
optimization, as discussed in [25]. The search area is assumed
to be equal to the operational basin of attraction, i.e., a region
covering the target model pixels. The first step is to compute
nh histograms. Assume that each histogram is derived in a
squared window of nh pixels. To implement the computation
efficiently, we obtain a target histogram and update it by
sliding the window nh times (
ffiffiffiffiffi
nh
p
horizontal steps timesffiffiffiffiffi
nh
p
vertical steps). For each move, 2
ffiffiffiffiffi
nh
p
additions are
needed to update the histogram, hence, the effort is
cH Ã¾ 2nh
ffiffiffiffiffi
nh
p
cA, where cA is the cost of an addition. The
second step is to compute nh Bhattacharyya coefficients. This
can also be done by computing one coefficient and, then,
updating it sequentially. The effort is mcS Ã¾ 2nh
ffiffiffiffiffi
nh
p
cS . The
total effort for target localization without gradient optimiza-
tion is then
CNO Â¼ cH Ã¾ 2nh
ffiffiffiffiffi
nh
p
cA Ã¾ Ã°mÃ¾ 2nh
ffiffiffiffiffi
nh
p ÃcS  2nh
ffiffiffiffiffi
nh
p
cS: Ã°16Ã
The ratio between (16) and (15) is 2  ffiffiffiffiffinhp =N . In a typical
setting (as it will be shown in Section 5), the target has
about 50 50 pixels (i.e., ffiffiffiffiffinhp Â¼ 50) and the mean
number of iterations is N Â¼ 4:19. Thus, the proposed
optimization technique reduces the computational time
2  50=4:19  24 times.
An optimized implementation of the new tracking
algorithm has been tested on a 1GHz PC. The basic
framework with scale adaptation (which involves three
optimizations at each step) runs at a rate of 150 fps allowing
simultaneous tracking of up to five targets in real time. Note
that without scale adaptations these numbers should be
multiplied by three.
5 EXPERIMENTAL RESULTS
The kernel-based visual tracker was applied to many
sequences and it is integrated into several applications. Here,
we just present some representative results. In all but the last
experiment, the RGB color space was taken as feature space
and it was quantized into 16 16 16 bins. The algorithm
was implemented as discussed in Section 4.2. The Epanech-
nikov profile was used for histogram computations and the
mean shift iterations were based on weighted averages.
The Football sequence (Fig. 1) has 154 frames of
352 240 pixels, and the movement of player number
75 was tracked. The target was initialized with a hand-drawn
568 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
elliptical region (frame 30) of size 71 53 (yielding normal-
ization constants equal to Ã°hx; hyÃ Â¼ Ã°35; 26ÃÃ. The kernel-
based tracker proves to be robust to partial occlusion, clutter,
distractors (frame 140), and camera motion. Since no motion
model has been assumed, the tracker adapted well to the
nonstationary character of the playerâ€™s movements which
alternates abruptly between slow and fast action. In addition,
the intense blurring present in some frames due to the camera
motion, does not influence the tracker performance (frame
150). The same conditions, however, can largely perturb
contour based trackers.
The number of mean shift iterations necessary for each
frame (one scale) is shown in Fig. 2. The two central peaks
correspond to the movement of the player to the center of
the image and back to the left side. The last and largest peak
is due to a fast move from the left to the right. In all these
cases, the relative large movement between two consecutive
frames puts more of a burden on the mean shift procedure.
To demonstrate the efficiency of our approach, Fig. 3
presents the surface obtained by computing the Bhattachar-
yya coefficient for the 81 81 pixels rectangle marked in
Fig. 1, frame 105. The target model (the elliptical region
selected in frame 30) has been compared with the target
candidates obtained by sweeping in frame 105 the elliptical
region inside the rectangle. The surface is asymmetric due to
neighboring colors that are similar to the target. While most of
the tracking approaches based on regions [7], [27], [50] must
perform an exhaustive search in the rectangle to find the
maximum, our algorithm converged in four iterations as
shown in Fig. 3. Note that the operational basin of attraction of
the mode covers the entire rectangular window.
In controlled environments with fixed camera, additional
geometric constraints (such as the expected scale) and
background subtraction [24] can be exploited to improve
the tracking process. The Subway-1 sequence (Fig. 4) is
suitable for such an approach, however, the results
presented here has been processed with the algorithm
unchanged. This is a two minute sequence in which a
person is tracked from the moment she enters the subway
platform until she gets on the train (about 3,600 frames).
The tracking is made more challenging by the low quality of
the sequence due to image compression artifacts. Note the
changes in the size of the tracked target.
The minimum value of the distance (6) for each frame,
i.e., the distance between the target model and the chosen
candidate, is shown in Fig. 5. The compression noise
elevates the residual distance value from 0 (perfect match)
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 569
Fig. 1. Football sequence, tracking player number 75. The frames 30, 75, 105, 140, and 150 are shown.
Fig. 2. The number of mean shift iterations function of the frame index for
the Football sequence. The mean number of iterations is 4:19 per frame.
Fig. 3. The similarity surface (values of the Bhattacharyya coefficient)
corresponding to the rectangle marked in frame 105 of Fig. 1. The initial
and final locations of the mean shift iterations are also shown.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
to a about 0:3. Significant deviations from this value
correspond to occlusions generated by other persons or
rotations in depth of the target (large changes in the
representation). For example, the d  0:6 peak corresponds
to the partial occlusion in frame 3; 697. At the end of the
sequence, the person being tracked gets on the train, which
produces a complete occlusion.
The shorter Subway-2 sequence (Fig. 6) of about 600 frames
is even more difficult since the camera quality is worse and
the amount of compression is higher, introducing clearly
visible artifacts. Nevertheless, the algorithm was still able to
track a person through the entire sequence.
In the Mug sequence (Fig. 7) of about 1,000 frames, the
image of a cup (frame 60) was used as target model. The
normalization constants were Ã°hx Â¼ 44; hy Â¼ 64Ã. The tracker
was tested for fast motion (frame 150), dramatic changes in
appearance (frame 270), rotations (frame 270), and scale
changes (frames 360-960).
6 EXTENSIONS OF THE TRACKING ALGORITHM
We present three extensions of the basic algorithm: integra-
tion of the background information, Kalman tracking, and an
application to face tracking. It should be emphasized,
however, that there are many other possibilities through
which the visual tracker can be further enhanced.
6.1 Background-Weighted Histogram
The background information is important for at least two
reasons. First, if some of the target features are also present in
the background, their relevance for the localization of the
target is diminished. Second, in many applications, it is
difficult to exactly delineate the target, and its model might
contain background features as well. At the same time, the
improper use of the background information may affect the
scale selection algorithm, making impossible to measure
similarity across scales, hence, to determine the appropriate
target scale. Our approach is to derive a simple representation
of the background features and to use it for selecting only the
salient parts from the representations of the target model and
target candidates.
Let oÌ‚uf guÂ¼1...m (with
Pm
uÂ¼1 oÌ‚u Â¼ 1) be the discrete repre-
sentation (histogram) of the background in the feature space
and oÌ‚ be its smallest nonzero entry. This representation is
computed in a region around the target. The extent of the
region is application dependent and we used an area equal to
three times the target area. The weights
vu Â¼ min
oÌ‚
oÌ‚u
; 1
  
uÂ¼1...m
Ã°17Ã
are similar in concept to the ratio histogram computed for
backprojection [71]. However, in our case, these weights
are only employed to define a transformation for the
570 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
Fig. 4. Subway-1 sequence. The frames 3,140, 3,516, 3,697, 5,440, 6,081, and 6,681 are shown.
Fig. 5. The minimum value of distance d function of the frame index for
the Subway-1 sequence.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
representations of the target model and candidates. The
transformation diminishes the importance of those features
which have low vu, i.e., are prominent in the background.
The new target model representation is then defined by
qÌ‚u Â¼ Cvu
Xn
iÂ¼1
kÃ°kx?i k
2Ã bÃ°x?i Ã Ã¿ u
 
Ã°18Ã
with the normalization constant C expressed as
C Â¼ 1Pn
iÂ¼1 kÃ°kx?i k
2Ã
Pm
uÂ¼1 vu bÃ°x?i Ã Ã¿ u
  : Ã°19Ã
Compare with (2) and (3). Similarly, the new target
candidate representation is
pÌ‚uÃ°yÃ Â¼ Chvu
Xnh
iÂ¼1
k
yÃ¿ xi
h
 2  bÃ°xiÃ Ã¿ uÂ½ ÂŠ; Ã°20Ã
where now Ch is given by
Ch Â¼
1Pnh
iÂ¼1 kÃ°k
yÃ¿xi
h k
2Ã
Pm
uÂ¼1 vu bÃ°xiÃ Ã¿ uÂ½ ÂŠ
: Ã°21Ã
An important benefit of using background-weighted histo-
grams is shown for the Ball sequence (Fig. 8). The movement
of the ping-pong ball from frame to frame is larger than its
size. Applying the technique described above, the target
model can be initialized with a 21 31 size region (frame 2),
larger than one obtained by accurate target delineation. The
larger region yields a satisfactory operational basin of
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 571
Fig. 6. Subway-2 sequence. The frames 30, 240, 330, 450, 510, and 600 are shown.
Fig. 7. Mug sequence. The frames 60, 150, 240, 270, 360, and 960 are shown.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
attraction, while the probabilities of the colors that are part of
the background are considerably reduced. The ball is reliably
tracked over the entire sequence of 60 frames.
The last example is also taken from the Football sequence.
This time the head and shoulder of player number 59 is
tracked (Fig. 9). Note the changes in target appearance along
the entire sequence and the rapid movements of the target.
6.2 Kalman Prediction
It was already mentioned in Section 1 that the Kalman filter
assumes that the noise sequences vk and nk are Gaussian
and the functions fk and hk are linear. The dynamic
equation becomes xk Â¼ FxkÃ¿1 Ã¾ vk, while the measurement
equation is zk Â¼ Hxk Ã¾ nk. The matrix F is called the
system matrix and H is the measurement matrix. As in the
general case, the Kalman filter solves the state estimation
problem in two steps: prediction and update. For more
details, see [5, p. 56].
The kernel-based target localization method was inte-
grated with the Kalman filtering framework. For a faster
implementation, two independent trackers were defined for
horizontal and vertical movement. A constant-velocity
dynamic model with acceleration affected by white noise
[5, p. 82] has been assumed. The uncertainty of the
measurements has been estimated according to [55]. The
idea is to normalize the similarity surface and represent it as
a probability density function. Since the similarity surface is
smooth, for each filter only three measurements are taken
into account, one at the convergence point (peak of the
surface) and the other two at a distance equal to half of the
target dimension, measured from the peak. We fit a scaled
Gaussian to the three points and compute the measurement
uncertainty as the standard deviation of the fitted Gaussian.
A first set of tracking results incorporating the Kalman
filter is presented in Fig. 10 for the 120 frames Hand sequence
where the dynamic model is assumed to be affected by a noise
572 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
Fig. 8. Ball sequence. The frames 2, 12, 16, 26, 32, 40, 48, and 51 are shown.
Fig. 9. Football sequence, tracking player number 59. The frames 70, 96, 108, 127, 140, and 147 are shown.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
with standard deviation equal to 5. The size of the green cross
marked on the target indicates the state uncertainty for the
two trackers. Observe that the overall algorithm is able to
track the target (hand) in the presence of complete occlusion
by a similar object (the other hand). The presence of a similar
object in the neighborhood increases the measurement
uncertainty in frame 46, determining an increase in state
uncertainty. In Fig. 11a, we present the measurements
(dotted) and the estimated location of the target (continuous).
Note that the graph is symmetric with respect to the number
of frames since the sequence has been played forward and
backward. The velocity associated with the two trackers is
shown in Fig. 11b.
A second set of results showing tracking with Kalman
filter is displayed in Fig. 12. The sequence has 187 frames of
320 240 pixels each and the initial normalization constants
were Ã°hx; hyÃ Â¼ Ã°11; 18Ã. Observe the adaptation of the
algorithm to scale changes. The uncertainty of the state is
indicated by the green cross.
6.3 Face Tracking
We applied the proposed framework for real-time face
tracking. The face model is an elliptical region whose
histogram is represented in the intensity normalized rg space
with 128 128 bins. To adapt to fast scale changes, we also
exploit the gradient information in the direction perpendi-
cular to the border of the hypothesized face region. The scale
adaptation is thus determined by maximizing the sum of two
normalized scores, based on color and gradient features,
respectively. In Fig. 13, we present the capability of the
kernel-based tracker to handle scale changes, the subject
turning away (frame 150), in-plane rotations of the head
(frame 498), and foreground/background saturation due to
back-lighting (frame 576). The tracked face is shown in the
small upper-left window.
7 DISCUSSION
The kernel-based tracking technique introduced in this
paper uses the basin of attraction of the similarity function.
This function is smooth since the target representations are
derived from continuous densities. Several examples
validate the approach and show its efficiency. Extensions
of the basic framework were presented regarding the use of
background information, Kalman filtering, and face track-
ing. The new technique can be further combined with more
sophisticated filtering and association approaches such as
multiple hypothesis tracking [13].
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 573
Fig. 10. Hand sequence. The frames 40, 46, 50, and 57 are shown.
Fig. 11. Measurements and estimated state for Hand sequence. (a) The measurement value (dotted curve) and the estimated location (continuous
curve) function of the frame index. Upper curves correspond to the y filter, while the lower curves correspond to the x filter. (b) Estimated velocity.
Dotted curve is for the y filter and continuous curve is for the x filter.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
Centroid computation has been also employed in [53].
The mean shift is used for tracking human faces by
projecting the histogram of a face model onto the incoming
frame [10]. However, the direct projection of the model
histogram onto the new frame can introduce a large bias in
the estimated location of the target, and the resulting
measure is scale variant (see [37, p. 262] for a discussion).
We mention that since its original publication in [18], the
idea of kernel-based tracking has been exploited and
developed forward by various researchers. Chen and Liu
[14] experimented with the same kernel-weighted histo-
grams, but employed the Kullback-Leibler distance as
dissimilarity while performing the optimization based on
trust-region methods. Haritaoglu and Flickner [35] used an
appearance model based on color and edge density in
conjunction with a kernel tracker for monitoring shopping
groups in stores. Yilmaz et al. [78] combined kernel tracking
with global motion compensation for forward-looking infra-
red (FLIR) imagery. Xu and Fujimura [77] used night vision
for pedestrian detection and tracking, where the detection is
performed by a support vector machine and the tracking is
kernel-based. Rao et al. [61] employed kernel tracking in their
system for action recognition, while Caenen et al. [12]
followed the same principle for texture analysis. The benefits
of guiding random particles by gradient optimization are
discussed in [70] and a particle filter for color histogram
tracking based on the metric (6) is implemented in [57].
Finally, we would like to add a word of caution. The
tracking solution presented in this paper has several desirable
properties: it is efficient, modular, has straightforward
574 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
Fig. 12. Subway-3 sequence: The frames 470, 529, 580, 624, and 686 are shown.
Fig. 13. Face sequence: The frames 39, 150, 163, 498, 576, and 619 are shown.
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
implementation, and provides superior performance on most
image sequences. Nevertheless, we note that this technique
should not be applied blindly. Instead, the versatility of the
basic algorithm should be exploited to integrate a priori
information that is almost always available when a specific
application is considered. For example, if the motion of the
target from frame to frame is known to be larger than the
operational basin of attraction, one should initialize the
tracker in multiple locations in the neighborhood of basin of
attraction, according to the motion model. If occlusions are
present, one should employ a more sophisticated motion
filter. Similarly, one should verify that the chosen target
representation is sufficiently discriminant for the application
domain. The kernel-based tracking technique, when com-
bined with prior task-specific information, can achieve
reliable performance.2
APPENDIX
PROOF THAT THE DISTANCE dÃ°pÌ‚;qÌ‚ÃÂ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1Ã¿Ã°pÌ‚;qÌ‚Ã
p
IS
A METRIC
The proof is based on the properties of the Bhattacharyya
coefficient (7). According to the Jensenâ€™s inequality [19, p. 25],
we have
Ã°pÌ‚; qÌ‚Ã Â¼
Xm
uÂ¼1
ffiffiffiffiffiffiffiffiffi
pÌ‚uqÌ‚u
p
Â¼
Xm
uÂ¼1
pÌ‚u
ffiffiffiffiffi
qÌ‚u
pÌ‚u
s

ffiffiffiffiffiffiffiffiffiffiffiffiXm
uÂ¼1
qÌ‚u
s
Â¼ 1; Ã°A:1Ã
with equality iff pÌ‚ Â¼ qÌ‚. Therefore, dÃ°pÌ‚; qÌ‚Ã Â¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1Ã¿ Ã°pÌ‚; qÌ‚Ã
p
exists for all discrete distributions pÌ‚ and qÌ‚, is positive,
symmetric, and is equal to zero iff pÌ‚ Â¼ qÌ‚.
To prove the triangle inequality consider three discrete
distributions defining the m-dimensional vectors pÌ‚, qÌ‚, and
rÌ‚, associated with the points p Â¼
ffiffiffiffiffi
pÌ‚1
p
; . . . ;
ffiffiffiffiffiffi
pÌ‚m
pÃ¿ >
,
q Â¼
ffiffiffiffi
qÌ‚1
p
; . . . ;
ffiffiffiffiffi
qÌ‚m
pÃ¿ >
, and r Â¼
ffiffiffiffi
rÌ‚1
p
; . . . ;
ffiffiffiffiffiffi
rÌ‚m
pÃ¿ >
on the
unit hypersphere. From the geometric interpretation of
the Bhattacharyya coefficient, the triangle inequality
dÃ°pÌ‚; rÌ‚Ã Ã¾ dÃ°qÌ‚; rÌ‚Ã  dÃ°pÌ‚; qÌ‚Ã Ã°A:2Ã
is equivalent toffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1Ã¿ cosÃ°p; rÃ
q
Ã¾
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1Ã¿ cosÃ°q; rÃ
q

ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1Ã¿ cosÃ°p; qÃ
q
: Ã°A:3Ã
If we fix the points p and q, and the angle between p and
r, the left side of inequality (A.3) is minimized when the
vectors p, q, and r lie in the same plane. Thus, the
inequality (A.3) can be reduced to a 2-dimensional problem
that can be easily proven by employing the half-angle sinus
formula and a few trigonometric manipulations.
ACKNOWLEDGMENTS
A preliminary version of the paper was presented at the
2000 IEEE Conference on Computer Vision and Pattern
Recognition, Hilton Head, South Carolina. Peter Meer was
supported by the US National Science Foundation under
the award IRI 99-87695.
REFERENCES
[1] J. Aggarwal and Q. Cai, â€œHuman Motion Analysis: A Review,â€
Computer Vision and Image Understanding, vol. 73, pp. 428-440, 1999.
[2] F. Aherne, N. Thacker, and P. Rockett, â€œThe Bhattacharyya Metric
as an Absolute Similarity Measure for Frequency Coded Data,â€
Kybernetika, vol. 34, no. 4, pp. 363-368, 1998.
[3] S. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, â€œA Tutorial
on Particle Filters for On-Line Non-Linear/Non-Gaussian Baye-
sian Tracking,â€ IEEE Trans. Signal Processing, vol. 50, no. 2, pp. 174-
189, 2002.
[4] S. Avidan, â€œSupport Vector Tracking,â€ Proc. IEEE Conf. Computer
Vision and Pattern Recognition, vol. I, pp. 184-191, 2001.
[5] Y. Bar-Shalom and T. Fortmann, Tracking and Data Association.
Academic Press. 1988.
[6] B. Bascle and R. Deriche, â€œRegion Tracking through Image
Sequences,â€ Proc. Fifth Intâ€™l Conf. Computer Vision, pp. 302-307, 1995.
[7] S. Birchfield, â€œElliptical Head Tracking Using Intensity Gradients
and Color Histograms,â€ Proc. IEEE Conf. Computer Vision and
Pattern Recognition, pp. 232-237, 1998.
[8] M. Black and D. Fleet, â€œProbabilistic Detection and Tracking of
Motion Boundaries,â€ Intâ€™l J. Computer Vision, vol. 38, no. 3, pp. 231-
245, 2000.
[9] Y. Boykov and D. Huttenlocher, â€œAdaptive Bayesian Recognition
in Tracking Rigid Objects,â€ Proc. IEEE Conf. Computer Vision and
Pattern Recognition, pp. 697-704, 2000.
[10] G.R. Bradski, â€œComputer Vision Face Tracking as a Component of
a Perceptual User Interface,â€ Proc. IEEE Workshop Applications of
Computer Vision, pp. 214-219, Oct. 1998.
[11] A.D. Bue, D. Comaniciu, V. Ramesh, and C. Regazzoni, â€œSmart
Cameras with Real-Time Video Object Generation,â€ Proc. IEEE
Intâ€™l Conf. Image Processing, vol. III, pp. 429-432, 2002.
[12] G. Caenen, V. Ferrari, A. Zalesny, L. VanGool, â€œAnalyzing the
Layout of Composite Textures,â€ Proc. Texture 2002 Workshop,
pp. 15-19, 2002.
[13] T. Cham and J. Rehg, â€œA Multiple Hypothesis Approach to Figure
Tracking,â€ Proc. IEEE Conf. Computer Vision and Pattern Recogni-
tion, vol. II, pp. 239-219, 1999.
[14] H. Chen and T. Liu, â€œTrust-Region Methods for Real-Time
tracking,â€ Proc. Eigth Intâ€™l Conf. Computer Vision, vol. II, pp. 717-
722, 2001.
[15] Y. Chen, Y. Rui, and T. Huang, â€œJPDAF-Based HMM for Real-
Time Contour Tracking,â€ Proc. IEEE Conf. Computer Vision and
Pattern Recognition, vol. I, pp. 543-550, 2001.
[16] R. Collins, A. Lipton, H. Fujiyoshi, and T. Kanade, â€œAlgorithms
for Cooperative Multisensor Surveillance,â€ Proc. IEEE, vol. 89,
no. 10, pp. 1456-1477, 2001.
[17] D. Comaniciu and P. Meer, â€œMean Shift: A Robust Approach
Toward Feature Space Analysis,â€ IEEE Trans. Pattern Analysis and
Machine Intelligence, vol. 24, no. 5, pp. 603-619, May 2002.
[18] D. Comaniciu, V. Ramesh, and P. Meer, â€œReal-Time Tracking of
Non-Rigid Objects Using Mean Shift,â€ Proc. IEEE Conf. Computer
Vision and Pattern Recognition, vol. II, pp. 142-149, June 2000.
[19] T. Cover and J. Thomas, Elements of Information Theory. New York:
John Wiley & Sons, 1991.
[20] I. Cox and S. Hingorani, â€œAn Efficient Implementation of Reidâ€™s
Multiple Hypothesis Tracking Algorithm and Its Evaluation for
the Purpose of Visual Tracking,â€ IEEE Trans. Pattern Analysis and
Machine Intelligence, vol. 18, no. 2, pp. 138-150, Feb. 1996.
[21] D. DeCarlo and D. Metaxas, â€œOptical Flow Constraints on
Deformable Models with Applications to Face Tracking,â€ Intâ€™l J.
Computer Vision, vol. 38, no. 2, pp. 99-127, 2000.
[22] A. Djouadi, O. Snorrason, and F. Garber, â€œThe Quality of Training-
Sample Estimates of the Bhattacharyya Coefficient,â€ IEEE Trans.
Pattern Analysis and Machine Intelligence, vol. 12, pp. 92-97, 1990.
[23] A. Doucet, S. Godsill, and C. Andrieu, â€œOn Sequential Monte
Carlo Sampling Methods for Bayesian Filtering,â€ Statistics and
Computing, vol. 10, no. 3, pp. 197-208, 2000.
[24] A. Elgammal, D. Harwood, and L. Davis, â€œNon-Parametric Model
for Background Subtraction,â€ Proc. European Conf. Computer Vision,
vol. II, pp. 751-767, June 2000.
[25] F. Ennesser and G. Medioni, â€œFinding Waldo, or Focus of Attention
Using Local Color Information,â€ IEEE Trans. Pattern Analysis and
Machine Intelligence, vol. 17, no. 8, pp. 805-809, Aug. 1995.
[26] V. Ferrari, T. Tuytelaars, and L.V. Gool, â€œReal-Time Affine Region
Tracking and Coplanar Grouping,â€ Proc. IEEE Conf. Computer
Vision and Pattern Recognition, vol. II, pp. 226-233, 2001.
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 575
2. A patent application has been filed covering the tracking algorithm
together with the extensions and various applications [79].
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
[27] P. Fieguth and D. Teropoulos, â€œColor-Based Tracking of Heads
and Other Mobile Objects at Video Frame Rates,â€ Proc. IEEE Conf.
Computer Vision and Pattern Recognition, pp. 21-27, 1997.
[28] K. Fukunaga, Introduction to Statistical Pattern Recognition, second
ed. Academic Press, 1990.
[29] J. Garcia, J. Valdivia, and X. Vidal, â€œInformation Theoretic Measure
for Visual Target Distinctness,â€ IEEE Trans. Pattern Analysis and
Machine Intelligence, vol. 23, no. 4, pp. 362-383, Apr. 2001.
[30] D. Gavrila, â€œThe Visual Analysis of Human Movement: A Survey,â€
Computer Vision and Image Understanding, vol. 73, pp. 82-98, 1999.
[31] N. Gordon, D. Salmond, and A. Smith, â€œA Novel Approach to
Non-Linear and Non-Gaussian Bayesian State Estimation,â€ IEE
Proc.-F, vol. 140, pp. 107-113, 1993.
[32] M. Greiffenhagen, D. Comaniciu, H. Niemann, and V. Ramesh,
â€œDesign, Analysis and Engineering of Video Monitoring Systems:
An Approach and a Case Study,â€ Proc. IEEE, vol. 89, no. 10,
pp. 1498-1517, 2001.
[33] G. Hager and P. Belhumeur, â€œReal-Time Tracking of Image
Regions with Changes in Geometry and Illumination,â€ Proc. IEEE
Conf. Computer Vision and Pattern Recognition, pp. 403-410, 1996.
[34] U. Handmann, T. Kalinke, C. Tzomakas, M. Werner, and W. von
Seelen, â€œComputer Vision for Driver Assistance Systems,â€ Proc.
SPIE, vol. 3364, pp. 136-147, 1998.
[35] I. Haritaoglu and M. Flickner, â€œDetection and Tracking of
Shopping Groups in Stores,â€ Proc. IEEE Conf. Computer Vision
and Pattern Recognition, vol. 1, pp. 431-438, 2001.
[36] I. Haritaoglu, D. Harwood, and L. Davis, â€œW4: Who? When?
Where? What?â€”A Real Time System for Detecting and Tracking
People,â€ Proc. of Intâ€™l Conf. Automatic Face and Gesture Recognition,
pp. 222-227, 1998.
[37] J. Huang, S. Kumar, M. Mitra, W. Zhu, and R. Zabih, â€œSpatial Color
Indexing and Applications,â€ Intâ€™l J. Computer Vision, vol. 35, no. 3,
pp. 245-268, 1999.
[38] C. Hue, J. Cadre, and P. Perez, â€œSequential Monte Carlo Filtering
for Multiple Target Tracking and Data Fusion,â€ IEEE Trans. Signal
Processing, vol. 50, no. 2, pp. 309-325, 2002.
[39] S. Intille, J. Davis, and A. Bobick, â€œReal-Time Closed-World
Tracking,â€ Proc. IEEE Conf. Computer Vision and Pattern Recogni-
tion, pp. 697-703, 1997.
[40] M. Isard and A. Blake, â€œCondensation-Conditional Density
Propagation for Visual Tracking,â€ Intâ€™l J. Computer Vision, vol. 29,
no. 1, 1998.
[41] A. Jepson, D. Fleet, and T. El-Maraghi, â€œRobust Online Appear-
ance Models for Visual Tracking,â€ Proc. IEEE Conf. Computer
Vision and Pattern Recognition, vol. I, pp. 415-422, 2001.
[42] S. Julier and J. Uhlmann, â€œA New Extension of the Kalman Filter
to Nonlinear Systems,â€ Proc. SPIE, vol. 3068, pp. 182-193, 1997.
[43] T. Kailath, â€œThe Divergence and Bhattacharyya Distance Mea-
sures in Signal Selection,â€ IEEE Trans. Comm. Technology, vol. 15,
pp. 52-60, 1967.
[44] V. Kettnaker and R. Zabih, â€œBayesian Multi-Camera Surveil-
lance,â€ Proc. IEEE Conf. Computer Vision and Pattern Recognition,
pp. 253-259, 1999.
[45] G. Kitagawa, â€œNon-Gaussian State-Space Modeling of Nonsta-
tionary Time Series,â€ J. Am. Statistical Assoc., vol. 82, pp. 1032-
1063, 1987.
[46] S. Konishi, A. Yuille, J. Coughlan, and S. Zhu, â€œFundamental
Bounds on Edge Detection: An Information Theoretic Evaluation
of Different Edge Cues,â€ Proc. IEEE Conf. Computer Vision and
Pattern Recognition, pp. 573-579, 1999.
[47] J. Krumm, S. Harris, B. Meyers, B. Brumitt, M. Hale, and S. Shafer,
â€œMulti-Camera Multi-Person Tracking for Easy Living,â€ Proc.
IEEE Intâ€™l Workshop Visual Surveillance, pp. 3-10, 2000.
[48] B. Li and R. Chellappa, â€œSimultaneous Tracking and Verification
via Sequential Posterior Estimation,â€ Proc. IEEE Conf. Computer
Vision and Pattern Recognition, vol. II, pp. 110-117, 2000.
[49] J. Lin, â€œDivergence Measures Based on the Shannon Entropy,â€
IEEE Trans. Information Theory, vol. 37, pp. 145-151, 1991.
[50] A. Lipton, H. Fujiyoshi, and R. Patil, â€œMoving Target Classifica-
tion and Tracking from Real-Time Video,â€ Proc. IEEE Workshop
Applications of Computer Vision, pp. 8-14, 1998.
[51] J. MacCormick and A. Blake, â€œA Probabilistic Exclusion Principle
for Tracking Multiple Objects,â€ Intâ€™l J. Computer Vision, vol. 39,
no. 1, pp. 57-71, 2000.
[52] R. Mahler, â€œEngineering Statistics for Multi-Object Tracking,â€
Proc. IEEE Workshop Multi-Object Tracking, pp. 53-60, 2001.
[53] S. McKenna, Y. Raja, and S. Gong, â€œTracking Colour Objects Using
Adaptive Mixture Models,â€ Image and Vision Computing J., vol. 17,
pp. 223-229, 1999.
[54] R. Merwe, A. Doucet, N. Freitas, and E. Wan, â€œThe Unscented
Particle Filter,â€ Technical Report CUED/F-INFENG/TR 380, Eng.
Dept., Cambridge Univ., 2000.
[55] K. Nickels and S. Hutchinson, â€œEstimating Uncertainty in SSD-
Based Feature Tracking,â€ Image and Vision Computing, vol. 20,
pp. 47-58, 2002.
[56] C. Olson, â€œImage Registration by Aligning Entropies,â€ Proc. IEEE
Conf. Computer Vision and Pattern Recognition, vol. II, pp. 331-336,
2001.
[57] P. Perez, C. Hue, J. Vermaak, and M. Gangnet, â€œColor-Based
Probabilistic Tracking,â€ Proc. European Conf. Computer Vision, vol. I,
pp. 661-675, 2002.
[58] W. Press, S. Teukolsky, W. Vetterling, and B. Flannery, Numerical
Recipes in C, second ed. Cambridge Univ. Press, 1992.
[59] J. Puzicha, Y. Rubner, C. Tomasi, and J. Buhmann, â€œEmpirical
Evaluation of Dissimilarity Measures for Color and Texture,â€ Proc.
Seventh Intâ€™l Conf. Computer Vision, pp. 1165-1173, 1999.
[60] L. Rabiner, â€œA Tutorial on Hidden Markov Models and Selected
Applications in Speech Recognition,â€ Proc. IEEE, vol. 77, no. 2,
pp. 257-285, 1989.
[61] C. Rao, A. Yilmaz, and M. Shah, â€œView-Invariant Representation
and Recognition of Actions,â€ Intâ€™l J. Computer Vision, vol. 50, no. 2,
pp. 203-226, Nov. 2002.
[62] C. Rasmussen, G. Hager, â€œProbabilistic Data Association Methods
for Tracking Complex Visual Objects,â€ IEEE Trans. Pattern
Analysis and Machine Intelligence, vol. 23, no. 6, pp. 560-576, June
2001.
[63] D. Reid, â€œAn Algorithm for Tracking Multiple Targets,â€ IEEE
Trans. Automatic Control, vol. AC-24, pp. 843-854, 1979.
[64] A. Roche, G. Malandain, and N. Ayache, â€œUnifying Maximum
Likelihood Approaches in Medical Image Registration,â€ Technical
Report 3741, INRIA, 1999.
[65] R. Rosales and S. Sclaroff, â€œ3D Trajectory Recovery for Tracking
Multiple Objects and Trajectory Guided Recognition of Actions,â€
Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 117-
123, 1999.
[66] Y. Rui and Y. Chen, â€œBetter Proposal Distributions: Object
Tracking Using Unscented Particle Filter,â€ Proc. IEEE Conf.
Computer Vision and Pattern Recognition, vol. II, pp. 786-793, 2001.
[67] S. Sclaroff and J. Isidoro, â€œActive Blobs,â€ Proc. Sixth Intâ€™l Conf.
Computer Vision, pp. 1146-1153, 1998.
[68] D.W. Scott, Multivariate Density Estimation. Wiley, 1992.
[69] C. Sminchisescu and B. Triggs, â€œCovariance Scaled Sampling for
Monocular 3D Body Tracking,â€ Proc. IEEE Conf. Computer Vision
and Pattern Recognition, vol. I, pp. 447-454, 2001.
[70] J. Sullivan and J. Rittscher, â€œGuiding Random Particles by
Deterministic Search,â€ Proc. Eighth Intâ€™l Conf. Computer Vision,
vol. I, pp. 323-330, 2001.
[71] M. Swain and D. Ballard, â€œColor Indexing,â€ Intâ€™l J. Computer
Vision, vol. 7, no. 1, pp. 11-32, 1991.
[72] P. Viola and W. Wells, â€œAlignment by Maximization of Mutual
Information,â€ Intâ€™l J. Computer Vision, vol. 24, no. 2, pp. 137-154,
1997.
[73] S. Wachter and H. Nagel, â€œTracking Persons in Monocular Image
Sequences,â€ Computer Vision and Image Understanding, vol. 74, no. 3,
pp. 174-192, 1999.
[74] R. Wildes, R. Kumar, H. Sawhney, S. Samasekera, S. Hsu, H. Tao,
Y. Guo, K. Hanna, A. Pope, D. Hirvonen, M. Hansen, and P. Burt,
â€œAerial Video Surveillance and Exploitation,â€ Proc. IEEE, vol. 89,
no. 10, pp. 1518-1539, 2001.
[75] C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland, â€œPfinder:
Real-Time Tracking of the Human Body,â€ IEEE Trans. Pattern
Analysis and Machine Intelligence, vol. 19, pp. 780-785, 1997.
[76] Y. Wu and T. Huang, â€œA Co-Inference Approach to Robust
Tracking,â€ Proc. Eigth Intâ€™l Conf. Computer Vision, vol. II, pp. 26-33,
2001.
[77] F. Xu and K. Fujimura, â€œPedestrian Detection and Tracking with
Night Vision,â€ Proc. IEEE Intelligent Vehicle Symp., 2002.
[78] A. Yilmaz, K. Shafique, N. Lobo, X. Li, T. Olson, and M. Shah,
â€œTarget Tracking in FLIR Imagery Using Mean Shift and Global
Motion Compensation,â€ IEEE Workshop Computer Vision Beyond
Visible Spectrum, 2001.
[79] â€œReal-Time Tracking of Non-Rigid Objects Using Mean Shift,â€US
patent pending, 2000.
576 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 25, NO. 5, MAY 2003
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.
Dorin Comaniciu received the PhD degrees in
electrical engineering from the Polytechnic Uni-
versity of Bucharest in 1995 and from Rutgers
University in 1999. He is currently a project
manager and scientist in the Real-Time Vision
and Modeling Department at Siemens Corporate
Research in Princeton, New Jersey. His re-
search interests include robust methods for
autonomous computer vision, multiple motion
estimation, nonparametric analysis, real-time
vision systems, medical imaging, content-based access to visual data,
and data compression. He has coauthored more than 60 papers,
conference papers, and book chapters in the area of visual information
processing. He is an associate editor of Pattern Analysis and
Applications. Dr. Comaniciu has received the Best Paper Award at the
IEEE Conference on Computer Vision and Pattern Recognition 2000
and is a coauthor of the upcoming book Nonparametric Analysis of
Visual Data: The Mean Shift Paradigm. He is a senior member of the
IEEE.
Visvanathan Ramesh received the BE degree
from the College of Engineering, Guindy, Ma-
dras, India. He received the MS degree in
electrical engineering from Virginia Tech and
the doctoral degree from the Department of
Electrical Engineering at the University of
Washington, where he defended his PhD dis-
sertation titled â€œPerformance Characterization of
Image Understanding Algorithmsâ€ in 1994. A
major focus of his research is to build robust
video-understanding systems and to quantify robustness of video-
understanding algorithms. During the course of his PhD work, he
developed a systems engineering methodology for computer vision
algorithm performance characterization and design. Dr. Ramesh is
currently the head of the Real-Time Vision and Modeling Department at
Siemens Corporate Research. His research program has developed
real-time vision systems (tracking and low-level image processing) for
applications in video surveillance and monitoring, computer vision
software systems, statistical modeling techniques for computer vision
algorithm design, analysis, and performance characterization. He was
also the coauthor of a paper on real-time tracking that received the Best
Paper Award at Computer Vision and Pattern Recognition in 2000. He is
a member of the IEEE.
Peter Meer received the Dipl. Engn. degree
from the Bucharest Polytechnic Institute, Bu-
charest, Romania, in 1971, and the DSc degree
from the Technion, Israel Institute of Technol-
ogy, Haifa, Israel, in 1986, both in electrical
engineering. From 1971 to 1979, he was with the
Computer Research Institute, Cluj, Romania,
working on the R&D of digital hardware. Be-
tween 1986 and 1990, he was an assistant
research scientist at the Center for Automation
Research, University of Maryland at College Park. In 1991, he joined the
Department of Electrical and Computer Engineering, Rutgers University,
Piscataway, New Jersey, where he is currently a full professor. He has
held visiting appointments in Japan, Korea, Sweden, Israel, and France,
and was on the organizing committees of numerous international
workshops and conferences. He is an associate editor of the IEEE
Transaction on Pattern Analysis and Machine Intelligence, a member of
the editorial board of Pattern Recognition, and was a guest editor of
Computer Vision and Image Understanding for the April 2000 special
issue on â€œRobust Statistical Techniques in Image Understanding.â€ He
was a coauthor of an award winning paper in pattern recognition in 1989,
the best student paper in the 1999, and the best paper at the 2000 IEEE
Conference on Computer Vision and Pattern Recognition. His research
interest is in application of modern statistical methods to image
understanding problems. He is a senior member of the IEEE.
. For more information on this or any other computing topic,
please visit our Digital Library at http://computer.org/publications/dlib.
COMANICIU ET AL.: KERNEL-BASED OBJECT TRACKING 577
Authorized licensed use limited to: University of Michigan Library. Downloaded on March 5, 2009 at 17:12 from IEEE Xplore.  Restrictions apply.

