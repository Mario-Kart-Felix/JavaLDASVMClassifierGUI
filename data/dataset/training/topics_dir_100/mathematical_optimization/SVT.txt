A Singular Value Thresholding Algorithm for Matrix Completion
Jian-Feng Caiâ€  Emmanuel J. CandeÌ€sâ™¯ Zuowei ShenÂ§
â€  Temasek Laboratories, National University of Singapore, Singapore 117543
â™¯ Applied and Computational Mathematics, Caltech, Pasadena, CA 91125
Â§ Department of Mathematics, National University of Singapore, Singapore 117543
September 2008
Abstract
This paper introduces a novel algorithm to approximate the matrix with minimum nuclear
norm among all matrices obeying a set of convex constraints. This problem may be understood as
the convex relaxation of a rank minimization problem, and arises in many important applications
as in the task of recovering a large matrix from a small subset of its entries (the famous Netflix
problem). Off-the-shelf algorithms such as interior point methods are not directly amenable to
large problems of this kind with over a million unknown entries.
This paper develops a simple first-order and easy-to-implement algorithm that is extremely
efficient at addressing problems in which the optimal solution has low rank. The algorithm is
iterative and produces a sequence of matrices {ğ‘¿ğ‘˜,ğ’€ ğ‘˜} and at each step, mainly performs a
soft-thresholding operation on the singular values of the matrix ğ’€ ğ‘˜. There are two remarkable
features making this attractive for low-rank matrix completion problems. The first is that
the soft-thresholding operation is applied to a sparse matrix; the second is that the rank of
the iterates {ğ‘¿ğ‘˜} is empirically nondecreasing. Both these facts allow the algorithm to make
use of very minimal storage space and keep the computational cost of each iteration low. On
the theoretical side, we provide a convergence analysis showing that the sequence of iterates
converges. On the practical side, we provide numerical examples in which 1, 000Ã—1, 000 matrices
are recovered in less than a minute on a modest desktop computer. We also demonstrate that
our approach is amenable to very large scale problems by recovering matrices of rank about 10
with nearly a billion unknowns from just about 0.4% of their sampled entries. Our methods are
connected with the recent literature on linearized Bregman iterations for â„“1 minimization, and
we develop a framework in which one can understand these algorithms in terms of well-known
Lagrange multiplier algorithms.
Keywords. Nuclear norm minimization, matrix completion, singular value thresholding, La-
grange dual function, Uzawaâ€™s algorithm and linearized Bregman iteration.
1 Introduction
1.1 Motivation
There is a rapidly growing interest in the recovery of an unknown low-rank or approximately low-
rank matrix from very limited information. This problem occurs in many areas of engineering and
1
applied science such as machine learning [1, 3, 4], control [45] and computer vision, see [52]. As
a motivating example, consider the problem of recovering a data matrix from a sampling of its
entries. This routinely comes up whenever one collects partially filled out surveys, and one would
like to infer the many missing entries. In the area of recommender systems, users submit ratings
on a subset of entries in a database, and the vendor provides recommendations based on the userâ€™s
preferences. Because users only rate a few items, one would like to infer their preference for unrated
items; this is the famous Netflix problem [2]. Recovering a rectangular matrix from a sampling of
its entries is known as the matrix completion problem. The issue is of course that this problem is
extraordinarily ill posed since with fewer samples than entries, we have infinitely many completions.
Therefore, it is apparently impossible to identify which of these candidate solutions is indeed the
â€œcorrectâ€ one without some additional information.
In many instances, however, the matrix we wish to recover has low rank or approximately low
rank. For instance, the Netflix data matrix of all user-ratings may be approximately low-rank
because it is commonly believed that only a few factors contribute to anyoneâ€™s taste or preference.
In computer vision, inferring scene geometry and camera motion from a sequence of images is a well-
studied problem known as the structure-from-motion problem. This is an ill-conditioned problem
for objects may be distant with respect to their size, or especially for â€œmissing dataâ€ which occur
because of occlusion or tracking failures. However, when properly stacked and indexed, these images
form a matrix which has very low rank (e.g. rank 3 under orthography) [23, 52]. Other examples
of low-rank matrix fitting abound; e.g. in control (system identification), machine learning (multi-
class learning) and so on. Having said this, the premise that the unknown has (approximately) low
rank radically changes the problem, making the search for solutions feasible since the lowest-rank
solution now tends to be the right one.
In a recent paper [14], CandeÌ€s and Recht showed that matrix completion is not as ill-posed as
people thought. Indeed, they proved that most low-rank matrices can be recovered exactly from
most sets of sampled entries even though these sets have surprisingly small cardinality, and more
importantly, they proved that this can be done by solving a simple convex optimization problem.
To state their results, suppose to simplify that the unknown matrix ğ‘´ âˆˆ â„ğ‘›Ã—ğ‘› is square, and that
one has available ğ‘š sampled entries {ğ‘´ğ‘–ğ‘— : (ğ‘–, ğ‘—) âˆˆ Î©} where Î© is a random subset of cardinality
ğ‘š. Then [14] proves that most matrices ğ‘´ of rank ğ‘Ÿ can be perfectly recovered by solving the
optimization problem
minimize âˆ¥ğ‘¿âˆ¥âˆ—
subject to ğ‘‹ğ‘–ğ‘— = ğ‘€ğ‘–ğ‘— , (ğ‘–, ğ‘—) âˆˆ Î©, (1.1)
provided that the number of samples obeys
ğ‘š â‰¥ ğ¶ğ‘›6/5ğ‘Ÿ log ğ‘› (1.2)
for some positive numerical constant ğ¶.1 In (1.1), the functional âˆ¥ğ‘¿âˆ¥âˆ— is the nuclear norm of the
matrix ğ‘´ , which is the sum of its singular values. The optimization problem (1.1) is convex and
can be recast as a semidefinite program [31,32]. In some sense, this is the tightest convex relaxation
of the NP-hard rank minimization problem
minimize rank(ğ‘¿)
subject to ğ‘‹ğ‘–ğ‘— = ğ‘€ğ‘–ğ‘— , (ğ‘–, ğ‘—) âˆˆ Î©, (1.3)
1Note that an ğ‘›Ã— ğ‘› matrix of rank ğ‘Ÿ depends upon ğ‘Ÿ(2ğ‘›âˆ’ ğ‘Ÿ) degrees of freedom.
2
since the nuclear ball {ğ‘¿ : âˆ¥ğ‘¿âˆ¥âˆ— â‰¤ 1} is the convex hull of the set of rank-one matrices with
spectral norm bounded by one. Another interpretation of CandeÌ€s and Rechtâ€™s result is that under
suitable conditions, the rank minimization program (1.3) and the convex program (1.1) are formally
equivalent in the sense that they have exactly the same unique solution.
1.2 Algorithm outline
Because minimizing the nuclear norm both provably recovers the lowest-rank matrix subject to
constraints (see [48] for related results) and gives generally good empirical results in a variety of
situations, it is understandably of great interest to develop numerical methods for solving (1.1). In
[14], this optimization problem was solved using one of the most advanced semidefinite programming
solvers, namely, SDPT3 [50]. This solver and others like SeDuMi are based on interior-point
methods, and are problematic when the size of the matrix is large because they need to solve huge
systems of linear equations to compute the Newton direction. In fact, SDPT3 can only handle
ğ‘›Ã— ğ‘› matrices with ğ‘› â‰¤ 100. Presumably, one could resort to iterative solvers such as the method
of conjugate gradients to solve for the Newton step but this is problematic as well since it is well
known that the condition number of the Newton system increases rapidly as one gets closer to the
solution. In addition, none of these general purpose solvers use the fact that the solution may have
low rank. We refer the reader to [42] for some recent progress on interior-point methods concerning
some special nuclear norm-minimization problems.
This paper develops the singular value thresholding algorithm for approximately solving the
nuclear norm minimization problem (1.1) and by extension, problems of the form
minimize âˆ¥ğ‘¿âˆ¥âˆ—
subject to ğ’œ(ğ‘¿) = ğ’ƒ, (1.4)
where ğ’œ is a linear operator acting on the space of ğ‘›1Ã—ğ‘›2 matrices and ğ’ƒ âˆˆ â„ğ‘š. This algorithm is
a simple first-order method, and is especially well suited for problems of very large sizes in which
the solution has low rank. We sketch this algorithm in the special matrix completion setting and
let ğ’«Î© be the orthogonal projector onto the span of matrices vanishing outside of Î© so that the
(ğ‘–, ğ‘—)th component of ğ’«Î©(ğ‘¿) is equal to ğ‘‹ğ‘–ğ‘— if (ğ‘–, ğ‘—) âˆˆ Î© and zero otherwise. Our problem may be
expressed as
minimize âˆ¥ğ‘¿âˆ¥âˆ—
subject to ğ’«Î©(ğ‘¿) = ğ’«Î©(ğ‘´), (1.5)
with optimization variable ğ‘¿ âˆˆ â„ğ‘›1Ã—ğ‘›2 . Fix ğœ > 0 and a sequence {ğ›¿ğ‘˜}ğ‘˜â‰¥1 of scalar step sizes.
Then starting with ğ’€ 0 = 0 âˆˆ â„ğ‘›1Ã—ğ‘›2 , the algorithm inductively defines{
ğ‘¿ğ‘˜ = shrink(ğ’€ ğ‘˜âˆ’1, ğœ),
ğ’€ ğ‘˜ = ğ’€ ğ‘˜âˆ’1 + ğ›¿ğ‘˜ğ’«Î©(ğ‘´ âˆ’ğ‘¿ğ‘˜)
(1.6)
until a stopping criterion is reached. In (1.6), shrink(ğ’€ , ğœ) is a nonlinear function which applies a
soft-thresholding rule at level ğœ to the singular values of the input matrix, see Section 2 for details.
The key property here is that for large values of ğœ , the sequence {ğ‘¿ğ‘˜} converges to a solution which
very nearly minimizes (1.5). Hence, at each step, one only needs to compute at most one singular
value decomposition and perform a few elementary matrix additions. Two important remarks are
in order:
3
1. Sparsity. For each ğ‘˜ â‰¥ 0, ğ’€ ğ‘˜ vanishes outside of Î© and is, therefore, sparse, a fact which can
be used to evaluate the shrink function rapidly.
2. Low-rank property. The matrices ğ‘¿ğ‘˜ turn out to have low rank, and hence the algorithm has
minimum storage requirement since we only need to keep principal factors in memory.
Our numerical experiments demonstrate that the proposed algorithm can solve problems, in
Matlab, involving matrices of size 30, 000Ã—30, 000 having close to a billion unknowns in 17 minutes
on a standard desktop computer with a 1.86 GHz CPU (dual core with Matlabâ€™s multithreading
option enabled) and 3 GB of memory. As a consequence, the singular value thresholding algorithm
may become a rather powerful computational tool for large scale matrix completion.
1.3 General formulation
The singular value thresholding algorithm can be adapted to deal with other types of convex
constraints. For instance, it may address problems of the form
minimize âˆ¥ğ‘¿âˆ¥âˆ—
subject to ğ‘“ğ‘–(ğ‘¿) â‰¤ 0, ğ‘– = 1, . . . ,ğ‘š, (1.7)
where each ğ‘“ğ‘– is a Lipschitz convex function (note that one can handle linear equality constraints by
considering pairs of affine functionals). In the simpler case where the ğ‘“ğ‘–â€™s are affine functionals, the
general algorithm goes through a sequence of iterations which greatly resemble (1.6). This is useful
because this enables the development of numerical algorithms which are effective for recovering
matrices from a small subset of sampled entries possibly contaminated with noise.
1.4 Contents and notations
The rest of the paper is organized as follows. In Section 2, we derive the singular value threshold-
ing (SVT) algorithm for the matrix completion problem, and recasts it in terms of a well-known
Lagrange multiplier algorithm. In Section 3, we extend the SVT algorithm and formulate a gen-
eral iteration which is applicable to general convex constraints. In Section 4, we establish the
convergence results for the iterations given in Sections 2 and 3. We demonstrate the performance
and effectiveness of the algorithm through numerical examples in Section 5, and review additional
implementation details. Finally, we conclude the paper with a short discussion in Section 6.
Before continuing, we provide here a brief summary of the notations used throughout the
paper. Matrices are bold capital, vectors are bold lowercase and scalars or entries are not bold.
For instance, ğ‘¿ is a matrix and ğ‘‹ğ‘–ğ‘— its (ğ‘–, ğ‘—)th entry. Likewise, ğ’™ is a vector and ğ‘¥ğ‘– its ğ‘–th
component. The nuclear norm of a matrix is denoted by âˆ¥ğ‘¿âˆ¥âˆ—, the Frobenius norm by âˆ¥ğ‘¿âˆ¥ğ¹
and the spectral norm by âˆ¥ğ‘¿âˆ¥2; note that these are respectively the 1-norm, the 2-norm and the
sup-norm of the vector of singular values. The adjoint of a matrix ğ‘¿ is ğ‘¿âˆ— and similarly for
vectors. The notation diag(ğ’™), where ğ’™ is a vector, stands for the diagonal matrix with {ğ‘¥ğ‘–} as
diagonal elements. We denote by âŸ¨ğ‘¿,ğ’€ âŸ© = trace(ğ‘¿âˆ—ğ’€ ) the standard inner product between two
matrices (âˆ¥ğ‘¿âˆ¥2ğ¹ = âŸ¨ğ‘¿,ğ‘¿âŸ©). The Cauchy-Schwarz inequality gives âŸ¨ğ‘¿,ğ’€ âŸ© â‰¤ âˆ¥ğ‘¿âˆ¥ğ¹ âˆ¥ğ’€ âˆ¥ğ¹ and it is
well known that we also have âŸ¨ğ‘¿,ğ’€ âŸ© â‰¤ âˆ¥ğ‘¿âˆ¥âˆ—âˆ¥ğ’€ âˆ¥2 (the spectral and nuclear norms are dual from
one another), see e.g. [14, 48].
4
2 The Singular Value Thresholding Algorithm
This section introduces the singular value thresholding algorithm and discusses some of its ba-
sic properties. We begin with the definition of a key building block, namely, the singular value
thresholding operator.
2.1 The singular value shrinkage operator
Consider the singular value decomposition (SVD) of a matrix ğ‘¿ âˆˆ â„ğ‘›1Ã—ğ‘›2 of rank ğ‘Ÿ
ğ‘¿ = ğ‘¼Î£ğ‘½ âˆ—, Î£ = diag({ğœğ‘–}1â‰¤ğ‘–â‰¤ğ‘Ÿ), (2.1)
where ğ‘¼ and ğ‘½ are respectively ğ‘›1 Ã— ğ‘Ÿ and ğ‘›2 Ã— ğ‘Ÿ matrices with orthonormal columns, and the
singular values ğœğ‘– are positive (unless specified otherwise, we will always assume that the SVD of
a matrix is given in the reduced form above). For each ğœ â‰¥ 0, we introduce the soft-thresholding
operator ğ’Ÿğœ defined as follows:
ğ’Ÿğœ (ğ‘¿) := ğ‘¼ğ’Ÿğœ (Î£)ğ‘½ âˆ—, ğ’Ÿğœ (Î£) = diag({ğœğ‘– âˆ’ ğœ)+}), (2.2)
where ğ‘¡+ is the positive part of ğ‘¡, namely, ğ‘¡+ = max(0, ğ‘¡). In words, this operator simply applies a
soft-thresholding rule to the singular values of ğ‘¿, effectively shrinking these towards zero. This is
the reason why we will also refer to this transformation as the singular value shrinkage operator.
Even though the SVD may not be unique, it is easy to see that the singular value shrinkage operator
is well defined and we do not elaborate further on this issue. In some sense, this shrinkage operator
is a straightforward extension of the soft-thresholding rule for scalars and vectors. In particular,
note that if many of the singular values of ğ‘¿ are below the threshold ğœ , the rank of ğ’Ÿğœ (ğ‘¿) may
be considerably lower than that of ğ‘¿, just like the soft-thresholding rule applied to vectors leads
to sparser outputs whenever some entries of the input are below threshold.
The singular value thresholding operator is the proximity operator associated with the nuclear
norm. Details about the proximity operator can be found in e.g. [37].
Theorem 2.1 For each ğœ â‰¥ 0 and ğ’€ âˆˆ â„ğ‘›1Ã—ğ‘›2, the singular value shrinkage operator (2.2) obeys
ğ’Ÿğœ (ğ’€ ) = argmin
ğ‘¿
{
1
2
âˆ¥ğ‘¿ âˆ’ ğ’€ âˆ¥2ğ¹ + ğœâˆ¥ğ‘¿âˆ¥âˆ—
}
. (2.3)
2 Proof. Since the function â„0(ğ‘¿) := ğœâˆ¥ğ‘¿âˆ¥âˆ— + 12âˆ¥ğ‘¿ âˆ’ğ’€ âˆ¥2ğ¹ is strictly convex, it is easy to see that
there exists a unique minimizer, and we thus need to prove that it is equal to ğ’Ÿğœ (ğ’€ ). To do this,
recall the definition of a subgradient of a convex function ğ‘“ : â„ğ‘›1Ã—ğ‘›2 â†’ â„. We say that ğ’ is a
subgradient of ğ‘“ at ğ‘¿0, denoted ğ’ âˆˆ âˆ‚ğ‘“(ğ‘¿0), if
ğ‘“(ğ‘¿) â‰¥ ğ‘“(ğ‘¿0) + âŸ¨ğ’,ğ‘¿ âˆ’ğ‘¿0âŸ© (2.4)
for all ğ‘¿. Now ?Ì‚? minimizes â„0 if and only if 0 is a subgradient of the functional â„0 at the point
?Ì‚?, i.e.
0 âˆˆ ?Ì‚? âˆ’ ğ’€ + ğœâˆ‚âˆ¥?Ì‚?âˆ¥âˆ—, (2.5)
2One reviewer pointed out that a similar result had been mentioned in a talk given by Donald Goldfarb at the
Foundations of Computational Mathematics conference which took place in Hong Kong in June 2008.
5
where âˆ‚âˆ¥?Ì‚?âˆ¥âˆ— is the set of subgradients of the nuclear norm. Let ğ‘¿ âˆˆ â„ğ‘›1Ã—ğ‘›2 be an arbitrary
matrix and ğ‘¼Î£ğ‘½ âˆ— be its SVD. It is known [14,39,55] that
âˆ‚âˆ¥ğ‘¿âˆ¥âˆ— =
{
ğ‘¼ğ‘½ âˆ— +ğ‘¾ : ğ‘¾ âˆˆ â„ğ‘›1Ã—ğ‘›2 , ğ‘¼âˆ—ğ‘¾ = 0, ğ‘¾ğ‘½ = 0, âˆ¥ğ‘¾ âˆ¥2 â‰¤ 1
}
. (2.6)
Set ?Ì‚? := ğ’Ÿğœ (ğ’€ ) for short. In order to show that ?Ì‚? obeys (2.5), decompose the SVD of ğ’€ as
ğ’€ = ğ‘¼0Î£0ğ‘½
âˆ—
0 +ğ‘¼1Î£1ğ‘½
âˆ—
1 ,
where ğ‘¼0, ğ‘½0 (resp. ğ‘¼1, ğ‘½1) are the singular vectors associated with singular values greater than ğœ
(resp. smaller than or equal to ğœ). With these notations, we have
?Ì‚? = ğ‘¼0(Î£0 âˆ’ ğœğ‘°)ğ‘½ âˆ—0
and, therefore,
ğ’€ âˆ’ ?Ì‚? = ğœ(ğ‘¼0ğ‘½ âˆ—0 +ğ‘¾ ), ğ‘¾ = ğœâˆ’1ğ‘¼1Î£1ğ‘½ âˆ—1 .
By definition, ğ‘¼âˆ—0ğ‘¾ = 0, ğ‘¾ğ‘½0 = 0 and since the diagonal elements of Î£1 have magnitudes
bounded by ğœ , we also have âˆ¥ğ‘¾ âˆ¥2 â‰¤ 1. Hence ğ’€ âˆ’ ?Ì‚? âˆˆ ğœâˆ‚âˆ¥?Ì‚?âˆ¥âˆ—, which concludes the proof.
2.2 Shrinkage iterations
We are now in the position to introduce the singular value thresholding algorithm. Fix ğœ > 0 and
a sequence {ğ›¿ğ‘˜} of positive step sizes. Starting with ğ’€0, inductively define for ğ‘˜ = 1, 2, . . .,{
ğ‘¿ğ‘˜ = ğ’Ÿğœ (ğ’€ ğ‘˜âˆ’1),
ğ’€ ğ‘˜ = ğ’€ ğ‘˜âˆ’1 + ğ›¿ğ‘˜ğ’«Î©(ğ‘´ âˆ’ğ‘¿ğ‘˜)
(2.7)
until a stopping criterion is reached (we postpone the discussion this stopping criterion and of the
choice of step sizes). This shrinkage iteration is very simple to implement. At each step, we only
need to compute an SVD and perform elementary matrix operations. With the help of a standard
numerical linear algebra package, the whole algorithm can be coded in just a few lines. As we
will see later, the iteration (2.7) is the linearized Bregman iteration, which is a special instance of
Uzawaâ€™s algorithm.
Before addressing further computational issues, we would like to make explicit the relationship
between this iteration and the original problem (1.1). In Section 4, we will show that the sequence
{ğ‘¿ğ‘˜} converges to the unique solution of an optimization problem closely related to (1.1), namely,
minimize ğœâˆ¥ğ‘¿âˆ¥âˆ— + 12âˆ¥ğ‘¿âˆ¥2ğ¹
subject to ğ’«Î©(ğ‘¿) = ğ’«Î©(ğ‘´). (2.8)
Furthermore, it is intuitive that the solution to this modified problem converges to that of (1.5) as
ğœ â†’ âˆ as shown in Section 3. Thus by selecting a large value of the parameter ğœ , the sequence of
iterates converges to a matrix which nearly minimizes (1.1).
As mentioned earlier, there are two crucial properties which make this algorithm ideally suited
for matrix completion.
6
âˆ™ Low-rank property. A remarkable empirical fact is that the matrices in the sequence {ğ‘¿ğ‘˜}
have low rank (provided, of course, that the solution to (2.8) has low rank). We use the word
â€œempiricalâ€ because all of our numerical experiments have produced low-rank sequences but
we cannot rigorously prove that this is true in general. The reason for this phenomenon is,
however, simple: because we are interested in large values of ğœ (as to better approximate the
solution to (1.1)), the thresholding step happens to â€˜killâ€™ most of the small singular values
and produces a low-rank output. In fact, our numerical results show that the rank of ğ‘¿ğ‘˜ is
nondecreasing with ğ‘˜, and the maximum rank is reached in the last steps of the algorithm,
see Section 5.
Thus, when the rank of the solution is substantially smaller than either dimension of the
matrix, the storage requirement is low since we could store each ğ‘¿ğ’Œ in its SVD form (note
that we only need to keep the current iterate and may discard earlier values).
âˆ™ Sparsity. Another important property of the SVT algorithm is that the iteration matrix ğ’€ ğ‘˜
is sparse. Since ğ’€ 0 = 0, we have by induction that ğ’€ ğ‘˜ vanishes outside of Î©. The fewer
entries available, the sparser ğ’€ ğ‘˜. Because the sparsity pattern Î© is fixed throughout, one can
then apply sparse matrix techniques to save storage. Also, if âˆ£Î©âˆ£ = ğ‘š, the computational cost
of updating ğ’€ ğ‘˜ is of order ğ‘š. Moreover, we can call subroutines supporting sparse matrix
computations, which can further reduce computational costs.
One such subroutine is the SVD. However, note that we do not need to compute the entire
SVD of ğ’€ ğ‘˜ to apply the singular value thresholding operator. Only the part corresponding
to singular values greater than ğœ is needed. Hence, a good strategy is to apply the iterative
Lanczos algorithm to compute the first few singular values and singular vectors. Because
ğ’€ ğ‘˜ is sparse, ğ’€ ğ‘˜ can be applied to arbitrary vectors rapidly, and this procedure offers a
considerable speedup over naive methods.
2.3 Relation with other works
Our algorithm is inspired by recent work in the area of â„“1 minimization, and especially by the work
on linearized Bregman iterations for compressed sensing, see [10â€“12,25,47,57] for linearized Bregman
iterations and [15â€“18, 28] for some information about the field of compressed sensing. In this line
of work, linearized Bregman iterations are used to find the solution to an underdetermined system
of linear equations with minimum â„“1 norm.In fact, Theorem 2.1 asserts that the singular value
thresholding algorithm can be formulated as a linearized Bregman iteration. Bregman iterations
were first introduced in [46] as a convenient tool for solving computational problems in the imaging
sciences, and a later paper [57] showed that they were useful for solving â„“1-norm minimization
problems in the area of compressed sensing. Linearized Bregman iterations were proposed in [25]
to improve performance of plain Bregman iterations, see also [57]. Additional details together
with a technique for improving the speed of convergence called kicking are described in [47]. On
the practical side, the paper [12] applied Bregman iterations to solve a deblurring problem while
on the theoretical side, the references [10, 11] gave a rigorous analysis of the convergence of such
iterations. New developments keep on coming out at a rapid pace and recently, [34] introduced a
new iteration, the split Bregman iteration, to extend Bregman-type iterations (such as linearized
Bregman iterations) to problems involving the minimization of â„“1-like functionals such as total-
variation norms, Besov norms, and so forth.
7
When applied to â„“1-minimization problems, linearized Bregman iterations are sequences of
soft-thresholding rules operating on vectors. Iterative soft-thresholding algorithms in connection
with â„“1 or total-variation minimization have quite a bit of history in signal and image processing
and we would like to mention the works [13, 41] for total-variation minimization, [26, 27, 33] for
â„“1 minimization, and [5, 8, 9, 21, 22, 29, 30, 49] for some recent applications in the area of image
inpainting and image restoration. Just as iterative soft-thresholding methods are designed to find
sparse solutions, our iterative singular value thresholding scheme is designed to find a sparse vector
of singular values. In classical problems arising in the areas of compressed sensing, and signal or
image processing, the sparsity is expressed in a known transformed domain and soft-thresholding is
applied to transformed coefficients. In contrast, the shrinkage operatorğ’Ÿğœ is adaptive. The SVT not
only discovers a sparse singular vector but also the bases in which we have a sparse representation.
In this sense, the SVT algorithm is an extension of earlier iterative soft-thresholding schemes.
Finally, we would like to contrast the SVT iteration (2.7) with the popular iterative soft-
thresholding algorithm used in many papers in imaging processing and perhaps best known under
the name of Proximal Forward-Backward Splitting method (PFBS), see [9, 24, 26, 33, 35, 53, 54] for
example. The constrained minimization problem (1.5) may be relaxed into
minimize ğœ†âˆ¥ğ‘¿âˆ¥âˆ— + 1
2
âˆ¥ğ’«Î©(ğ‘¿)âˆ’ ğ’«Î©(ğ‘´)âˆ¥2ğ¹ (2.9)
for some ğœ† > 0. Theorem 2.1 asserts that ğ’Ÿğœ† is the proximity operator of ğœ†âˆ¥ğ‘¿âˆ¥âˆ— and Proposition
3.1(iii) in [24] gives that the solution to this unconstrained problem is characterized by the fixed
point equation ğ‘¿ = ğ’Ÿğœ†ğ›¿(ğ‘¿ + ğ›¿ğ‘ƒÎ©(ğ‘´ âˆ’ ğ‘¿)) for each ğ›¿ > 0. One can then apply a simplified
version of the PFBS method (see (3.6) in [24]) to obtain iterations of the form
ğ‘¿ğ‘˜ = ğ’Ÿğœ†ğ›¿ğ‘˜âˆ’1(ğ‘¿ğ‘˜âˆ’1 + ğ›¿ğ‘˜âˆ’1ğ‘ƒÎ©(ğ‘´ âˆ’ğ‘¿ğ‘˜âˆ’1)).
Introducing an intermediate matrix ğ’€ ğ‘˜, this algorithm may be expressed as{
ğ‘¿ğ‘˜ = ğ’Ÿğœ†ğ›¿ğ‘˜âˆ’1(ğ’€ ğ‘˜âˆ’1),
ğ’€ ğ‘˜ = ğ‘¿ğ‘˜ + ğ›¿ğ‘˜ğ‘ƒÎ©(ğ‘´ âˆ’ğ‘¿ğ‘˜).
(2.10)
The difference with (2.7) may seem subtle at firstâ€”replacing ğ‘¿ğ‘˜ in (2.10) with ğ’€ ğ‘˜âˆ’1 and setting
ğ›¿ğ‘˜ = ğ›¿ gives (2.7) with ğœ = ğœ†ğ›¿â€”but has enormous consequences as this gives entirely different
algorithms. First, they have different limits: while (2.7) converges to the solution of the constrained
minimization (2.8), (2.10) converges to the solution of (2.9) provided that the sequence of step sizes
is appropriately selected. Second, selecting a large ğœ† (or a large value of ğœ = ğœ†ğ›¿) in (2.10) gives a
low-rank sequence of iterates and a limit with small nuclear norm. The limit, however, does not fit
the data and this is why one has to choose a small or moderate value of ğœ† (or of ğœ = ğœ†ğ›¿). However,
when ğœ† is not sufficiently large, the ğ‘¿ğ‘˜â€™s may not have low rank even though the solution has
low rank (and one may need to compute many singular vectors), and thus applying the shrinkage
operation accurately to ğ’€ ğ‘˜ may be computationally very expensive. Moreover, the limit does not
necessary have a small nuclear norm. These are some of the reasons why (2.10) does not seems
to be very suitable for very large-scale matrix completion problems (in cases where computing the
SVD is prohibitive). Since the original submission of this paper, however, we note that several
papers proposed some working implementations [43,51].
8
2.4 Interpretation as a Lagrange multiplier method
In this section, we recast the SVT algorithm as a type of Lagrange multiplier algorithm known as
Uzawaâ€™s algorithm. An important consequence is that this will allow us to extend the SVT algorithm
to other problems involving the minimization of the nuclear norm under convex constraints, see
Section 3. Further, another contribution of this paper is that this framework actually recasts linear
Bregman iterations as a very special form of Uzawaâ€™s algorithm, hence providing fresh and clear
insights about these iterations.
In what follows, we set ğ‘“ğœ (ğ‘¿) = ğœâˆ¥ğ‘¿âˆ¥âˆ— + 12âˆ¥ğ‘¿âˆ¥2ğ¹ for some fixed ğœ > 0 and recall that we wish
to solve (2.8)
minimize ğ‘“ğœ (ğ‘¿)
subject to ğ’«Î©(ğ‘¿) = ğ’«Î©(ğ‘´).
The Lagrangian for this problem is given by
â„’(ğ‘¿,ğ’€ ) = ğ‘“ğœ (ğ‘¿) + âŸ¨ğ’€ ,ğ’«Î©(ğ‘´ âˆ’ğ‘¿)âŸ©,
where ğ’€ âˆˆ â„ğ‘›1Ã—ğ‘›2 . Strong duality holds and ğ‘¿â˜… and ğ’€ â˜… are primal-dual optimal if (ğ‘¿â˜…,ğ’€ â˜…) is a
saddlepoint of the Lagrangian â„’(ğ‘¿,ğ’€ ), i.e. a pair obeying
sup
ğ’€
inf
ğ‘¿
â„’(ğ‘¿,ğ’€ ) = â„’(ğ‘¿â˜…,ğ’€ â˜…) = inf
ğ‘¿
sup
ğ’€
â„’(ğ‘¿,ğ’€ ). (2.11)
The function ğ‘”0(ğ’€ ) = infğ‘¿ â„’(ğ‘¿,ğ’€ ) is called the dual function. Here, ğ‘”0 is continuously differ-
entiable and has a gradient which is Lipschitz with Lipschitz constant at most one, as this is a
consequence of well-known results concerning conjugate functions. Uzawaâ€™s algorithm approaches
the problem of finding a saddlepoint with an iterative procedure. From ğ’€0 = 0, say, inductively
define {
â„’(ğ‘¿ğ‘˜,ğ’€ ğ‘˜âˆ’1) = minğ‘¿ â„’(ğ‘¿,ğ’€ ğ‘˜âˆ’1)
ğ’€ ğ‘˜ = ğ’€ ğ‘˜âˆ’1 + ğ›¿ğ‘˜ğ’«Î©(ğ‘´ âˆ’ğ‘¿ğ‘˜),
(2.12)
where {ğ›¿ğ‘˜}ğ‘˜â‰¥1 is a sequence of positive step sizes. Uzawaâ€™s algorithm is, in fact, a subgradient
method applied to the dual problem, where each step moves the current iterate in the direction of
the gradient or of a subgradient. Indeed, observe that the gradient of ğ‘”0(ğ’€ ) is given by
âˆ‚ğ’€ ğ‘”0(ğ’€ ) = âˆ‚ğ’€ â„’(?Ìƒ?,ğ’€ ) = ğ’«Î©(ğ‘´ âˆ’ ?Ìƒ?), (2.13)
where ?Ìƒ? is the minimizer of the Lagrangian for that value of ğ’€ so that a gradient descent update
for ğ’€ is of the form
ğ’€ ğ‘˜ = ğ’€ ğ‘˜âˆ’1 + ğ›¿ğ‘˜âˆ‚ğ’€ ğ‘”0(ğ’€ ğ‘˜âˆ’1) = ğ’€ ğ‘˜âˆ’1 + ğ›¿ğ‘˜ğ’«Î©(ğ‘´ âˆ’ğ‘¿ğ‘˜).
It remains to compute the minimizer of the Lagrangian (2.12), and note that
argmin ğ‘“ğœ (ğ‘¿) + âŸ¨ğ’€ ,ğ’«Î©(ğ‘´ âˆ’ğ‘¿)âŸ© = argmin ğœâˆ¥ğ‘¿âˆ¥âˆ— + 1
2
âˆ¥ğ‘¿ âˆ’ ğ’«Î©ğ’€ âˆ¥2ğ¹ . (2.14)
However, we know that the minimizer is given by ğ’Ÿğœ (ğ’«Î©(ğ’€ )) and since ğ’€ ğ‘˜ = ğ’«Î©(ğ’€ ğ‘˜) for all ğ‘˜ â‰¥ 0,
Uzawaâ€™s algorithm takes the form{
ğ‘¿ğ‘˜ = ğ’Ÿğœ (ğ’€ ğ‘˜âˆ’1)
ğ’€ ğ‘˜ = ğ’€ ğ‘˜âˆ’1 + ğ›¿ğ‘˜ğ’«Î©(ğ‘´ âˆ’ğ‘¿ğ‘˜),
9
which is exactly the update (2.7). This point of view brings to bear many different mathemat-
ical tools for proving the convergence of the singular value thresholding iterations. For an early
use of Uzawaâ€™s algorithm minimizing an â„“1-like functional, the total-variation norm, under linear
inequality constraints, see [13].
3 General Formulation
This section presents a general formulation of the SVT algorithm for approximately minimizing the
nuclear norm of a matrix under convex constraints.
3.1 Linear equality constraints
Set the objective functional ğ‘“ğœ (ğ‘¿) = ğœâˆ¥ğ‘¿âˆ¥âˆ— + 12âˆ¥ğ‘¿âˆ¥2ğ¹ for some fixed ğœ > 0, and consider the
following optimization problem:
minimize ğ‘“ğœ (ğ‘¿)
subject to ğ’œ(ğ‘¿) = ğ’ƒ, (3.1)
where ğ’œ is a linear transformation mapping ğ‘›1Ã—ğ‘›2 matrices into â„ğ‘š (ğ’œâˆ— is the adjoint of ğ’œ). This
more general formulation is considered in [14] and [48] as an extension of the matrix completion
problem. Then the Lagrangian for this problem is of the form
â„’(ğ‘¿,ğ’š) = ğ‘“ğœ (ğ‘¿) + âŸ¨ğ’š, ğ’ƒâˆ’ğ’œ(ğ‘¿)âŸ©, (3.2)
where ğ‘¿ âˆˆ â„ğ‘›1Ã—ğ‘›2 and ğ’š âˆˆ â„ğ‘š, and starting with ğ’š0 = 0, Uzawaâ€™s iteration is given by{
ğ‘¿ğ‘˜ = ğ’Ÿğœ (ğ’œâˆ—(ğ’šğ‘˜âˆ’1)),
ğ’šğ‘˜ = ğ’šğ‘˜âˆ’1 + ğ›¿ğ‘˜(ğ’ƒâˆ’ğ’œ(ğ‘¿ğ‘˜)).
(3.3)
The iteration (3.3) is of course the same as (2.7) in the case where ğ’œ is a sampling operator
extracting ğ‘š entries with indices in Î© out of an ğ‘›1 Ã— ğ‘›2 matrix. To verify this claim, observe
that in this situation, ğ’œâˆ—ğ’œ = ğ’«Î©, and let ğ‘´ be any matrix obeying ğ’œ(ğ‘´) = ğ’ƒ. Then defining
ğ’€ ğ‘˜ = ğ’œâˆ—(ğ’šğ‘˜) and substituting this expression in (3.3) gives (2.7).
3.2 General convex constraints
One can also adapt the algorithm to handle general convex constraints. Suppose we wish to
minimize ğ‘“ğœ (ğ‘¿) defined as before over a convex set ğ‘¿ âˆˆ ğ’. To simplify, we will assume that this
convex set is given by
ğ’ = {ğ‘¿ : ğ‘“ğ‘–(ğ‘¿) â‰¤ 0, âˆ€ğ‘– = 1, . . . ,ğ‘š},
where the ğ‘“ğ‘–â€™s are convex functionals (note that one can handle linear equality constraints by
considering pairs of affine functionals). The problem of interest is then of the form
minimize ğ‘“ğœ (ğ‘¿)
subject to ğ‘“ğ‘–(ğ‘¿) â‰¤ 0, ğ‘– = 1, . . . ,ğ‘š. (3.4)
Just as before, it is intuitive that as ğœ â†’ âˆ, the solution to this problem converges to a minimizer
of the nuclear norm under the same constraints (1.7) as shown in Theorem 3.1 at the end of this
section.
10
Put â„±(ğ‘¿) := (ğ‘“1(ğ‘¿), . . . , ğ‘“ğ‘š(ğ‘¿)) for short. Then the Lagrangian for (3.4) is equal to
â„’(ğ‘¿,ğ’š) = ğ‘“ğœ (ğ‘¿) + âŸ¨ğ’š,â„±(ğ‘¿)âŸ©,
where ğ‘¿ âˆˆ â„ğ‘›1Ã—ğ‘›2 and ğ’š âˆˆ â„ğ‘š is now a vector with nonnegative components denoted, as usual,
by ğ’š â‰¥ 0. One can apply Uzawaâ€™s method just as before with the only modification that we will
use a subgradient method with projection to maximize the dual function since we need to make
sure that the successive updates ğ’šğ‘˜ belong to the nonnegative orthant. This gives{
ğ‘¿ğ‘˜ = argmin {ğ‘“ğœ (ğ‘¿) + âŸ¨ğ’šğ‘˜âˆ’1,â„±(ğ‘¿)âŸ©},
ğ’šğ‘˜ = [ğ’šğ‘˜âˆ’1 + ğ›¿ğ‘˜â„±(ğ‘¿ğ‘˜)]+.
(3.5)
Above, ğ’™+ is of course the vector with entries equal to max(ğ‘¥ğ‘–, 0). When â„± is an affine mapping
of the form ğ’ƒâˆ’ğ’œ(ğ‘¿) so that one solves
minimize ğ‘“ğœ (ğ‘¿)
subject to ğ’œ(ğ‘¿) â‰¥ ğ’ƒ,
this simplifies to {
ğ‘¿ğ‘˜ = ğ’Ÿğœ (ğ’œâˆ—(ğ’šğ‘˜âˆ’1)),
ğ’šğ‘˜ = [ğ’šğ‘˜âˆ’1 + ğ›¿ğ‘˜(ğ’ƒâˆ’ğ’œ(ğ‘¿ğ‘˜))]+,
(3.6)
and thus the extension to linear inequality constraints is straightforward.
3.3 Examples
Suppose we have available linear measurements ğ’ƒ about a matrix ğ‘´ , which take the form
ğ’ƒ = ğ’œ(ğ‘´) + ğ’› (3.7)
where ğ’› âˆˆ â„ğ‘š is a noise vector. Then under these circumstances, one might want to find the matrix
which minimizes the nuclear norm among all matrices which are consistent with the data ğ’ƒ.
3.3.1 Linear inequality constraints
A possible approach to this problem consists in solving
minimize âˆ¥ğ‘¿âˆ¥âˆ—
subject to âˆ£vec(ğ’œâˆ—(ğ’“))âˆ£ â‰¤ vec(ğ‘¬), ğ’“ := ğ’ƒâˆ’ğ’œ(ğ‘¿), (3.8)
where ğ‘¬ is an array of tolerances, which is adjusted to fit the noise statistics. Above, vec(ğ‘¨) â‰¤
vec(ğ‘©), for any two matrices ğ‘¨ and ğ‘©, means componentwise inequalities; that is, ğ´ğ‘–ğ‘— â‰¤ ğµğ‘–ğ‘— for all
indices ğ‘–, ğ‘—. We use this notation as not to confuse the reader with the positive semidefinite ordering.
In the case of the matrix completion problem where ğ’œ extracts sampled entries indexed by Î©, one
can always see the data vector as the sampled entries of some matrix ğ‘© obeying ğ’«Î©(ğ‘©) = ğ’œâˆ—(ğ’ƒ);
the constraint is then natural for it may be expressed as
âˆ£ğµğ‘–ğ‘— âˆ’ğ‘‹ğ‘–ğ‘— âˆ£ â‰¤ ğ¸ğ‘–ğ‘— , (ğ‘–, ğ‘—) âˆˆ Î©,
11
If ğ’› is white noise with standard deviation ğœ, one may want to use a multiple of ğœ for ğ¸ğ‘–ğ‘— . In
words, we are looking for a matrix with minimum nuclear norm under the constraint that all of its
sampled entries do not deviate too much from what has been observed.
Let ğ’€+ âˆˆ â„ğ‘›1Ã—ğ‘›2 (resp. ğ’€âˆ’ âˆˆ â„ğ‘›1Ã—ğ‘›2) be the Lagrange multiplier associated with the compo-
nentwise linear inequality constraints vec(ğ’œâˆ—(ğ’“)) â‰¤ vec(ğ‘¬) (resp. âˆ’vec(ğ’œâˆ—(ğ’“)) â‰¤ vec(ğ‘¬)). Then
starting with ğ’€ 0Â± = 0, the SVT iteration for this problem is of the form{
ğ‘¿ğ‘˜ = ğ’Ÿğœ (ğ’œâˆ—ğ’œ(ğ’€ ğ‘˜âˆ’1+ âˆ’ ğ’€ ğ‘˜âˆ’1âˆ’ )),
ğ’€ ğ‘˜Â± = [ğ’€
ğ‘˜âˆ’1
Â± + ğ›¿ğ‘˜(Â±ğ’œâˆ—(ğ’“ğ‘˜)âˆ’ğ‘¬)]+, ğ’“ğ‘˜ = ğ’ƒğ‘˜ âˆ’ğ’œ(ğ‘¿ğ‘˜),
(3.9)
where again [â‹…]+ is applied componentwise (in the matrix completion problem, ğ’œâˆ—ğ’œ = ğ’«Î©).
3.3.2 Quadratic constraints
Another natural solution is to solve the quadratically constrained nuclear-norm minimization prob-
lem
minimize âˆ¥ğ‘¿âˆ¥âˆ—
subject to âˆ¥ğ’ƒâˆ’ğ’œ(ğ‘¿)âˆ¥ â‰¤ ğœ–. (3.10)
When ğ‘§ is a stochastic error term, ğœ– would typically be adjusted to depend on the noise power.
To see how we can adapt our ideas in this setting, we work with the approximate objective
functional ğœâˆ¥ğ‘¿âˆ¥âˆ— + 12âˆ¥ğ‘¿âˆ¥2ğ¹ as before, and rewrite our program in the conic form
minimize ğœâˆ¥ğ‘¿âˆ¥âˆ— + 12âˆ¥ğ‘¿âˆ¥2ğ¹
subject to
[
ğ’ƒâˆ’ğ’œ(ğ‘¿)
ğœ–
]
âˆˆ ğ’¦, (3.11)
where ğ’¦ is the second-order cone
ğ’¦ = {(ğ’™, ğ‘¡) âˆˆ â„ğ‘š+1 : âˆ¥ğ’™âˆ¥ â‰¤ ğ‘¡}.
This cone is self-dual. The Lagrangian is then given by
â„’(ğ‘¿;ğ’š, ğ‘ ) = ğœâˆ¥ğ‘¿âˆ¥âˆ— + 1
2
âˆ¥ğ‘¿âˆ¥2ğ¹ + âŸ¨ğ’š, ğ’ƒâˆ’ğ’œ(ğ‘¿)âŸ© âˆ’ ğ‘ ğœ–,
where (ğ’š, ğ‘ ) âˆˆ â„ğ‘š+1 âˆˆ ğ’¦âˆ— = ğ’¦; that is, âˆ¥ğ’šâˆ¥ â‰¤ ğ‘ . Letting ğ‘ƒğ’¦ be the orthogonal projection onto ğ’¦,
this leads to the simple iterationâ§ï£´â¨ï£´â©
ğ‘¿ğ‘˜ = ğ’Ÿğœ (ğ’œâˆ—(ğ’šğ‘˜)),[
ğ’šğ‘˜
ğ‘ ğ‘˜
]
= ğ‘ƒğ’¦
([
ğ’šğ‘˜âˆ’1
ğ‘ ğ‘˜âˆ’1
]
+ ğ›¿ğ‘˜
[
ğ’ƒâˆ’ğ’œ(ğ‘¿ğ‘˜)
âˆ’ğœ–
])
.
(3.12)
This is an explicit algorithm since the projection is given by
ğ‘ƒğ’¦ : (ğ‘¥, ğ‘¡) 7â†’
â§ï£´â¨ï£´â©
(ğ‘¥, ğ‘¡), âˆ¥ğ‘¥âˆ¥ â‰¤ ğ‘¡,
âˆ¥ğ‘¥âˆ¥+ğ‘¡
2âˆ¥ğ‘¥âˆ¥ (ğ‘¥, âˆ¥ğ‘¥âˆ¥), âˆ’âˆ¥ğ‘¥âˆ¥ â‰¤ ğ‘¡ â‰¤ âˆ¥ğ‘¥âˆ¥,
(0, 0), ğ‘¡ â‰¤ âˆ’âˆ¥ğ‘¥âˆ¥.
12
3.3.3 General conic constraints
Clearly, one could apply this methodology with general cone constraints of the form â„±(ğ‘¿)+ğ’… âˆˆ ğ’¦,
where ğ’¦ is some closed and pointed convex cone. Inspired by the work on the Dantzig selector [19],
which was originally developed for estimating sparse parameter vectors from noisy data, another
approach is to set a constraint on the spectral norm of ğ’œâˆ—(ğ’“)â€”recall that ğ’“ is the residual vector
ğ’ƒâˆ’ğ’œ(ğ‘¿)â€”and solve
minimize âˆ¥ğ‘¿âˆ¥âˆ—
subject to âˆ¥ğ’œâˆ—(ğ’“)âˆ¥ â‰¤ ğœ–. (3.13)
Developing our approach in this setting is straightforward and involves projections of the dual
variable onto the positive semi-definite cone.
3.4 When the proximal problem gets close
We now show that minimizing the proximal objective ğ‘“ğœ (ğ‘¿) = ğœâˆ¥ğ‘¿âˆ¥âˆ— + 12âˆ¥ğ‘¿âˆ¥2ğ¹ is the same as
minimizing the nuclear norm in the limit of large ğœ â€™s. The theorem below is general and covers the
special case of linear equality constraints as in (2.8).
Theorem 3.1 Let ğ‘¿â˜…ğœ be the solution to (3.4) and ğ‘¿âˆ be the minimum Frobenius-norm solution
to (1.7) defined as
ğ‘¿âˆ := argmin
ğ‘¿
{âˆ¥ğ‘¿âˆ¥2ğ¹ : ğ‘¿ is a solution of (1.7)}. (3.14)
Assume that the ğ‘“ğ‘–(ğ‘¿)â€™s, 1 â‰¤ ğ‘– â‰¤ ğ‘š, are convex and lower semi-continuous. Then
lim
ğœâ†’âˆ âˆ¥ğ‘¿
â˜…
ğœ âˆ’ğ‘¿âˆâˆ¥ğ¹ = 0. (3.15)
Proof. It follows from the definition of ğ‘¿â˜…ğœ and ğ‘¿âˆ that
âˆ¥ğ‘¿â˜…ğœ âˆ¥âˆ— +
1
2ğœ
âˆ¥ğ‘¿â˜…ğœ âˆ¥2ğ¹ â‰¤ âˆ¥ğ‘¿âˆâˆ¥âˆ— +
1
2ğœ
âˆ¥ğ‘¿âˆâˆ¥2ğ¹ , and âˆ¥ğ‘¿âˆâˆ¥âˆ— â‰¤ âˆ¥ğ‘¿â˜…ğœ âˆ¥âˆ—. (3.16)
Summing these two inequalities gives
âˆ¥ğ‘¿â˜…ğœ âˆ¥2ğ¹ â‰¤ âˆ¥ğ‘¿âˆâˆ¥2ğ¹ , (3.17)
which implies that âˆ¥ğ‘¿â˜…ğœ âˆ¥2ğ¹ is bounded uniformly in ğœ . Thus, we would prove the theorem if we
could establish that any convergent subsequence {ğ‘¿â˜…ğœğ‘˜}ğ‘˜â‰¥1 must converge to ğ‘¿âˆ.
Consider an arbitrary converging subsequence {ğ‘¿â˜…ğœğ‘˜} and set ğ‘¿ğ‘ := limğ‘˜â†’âˆğ‘¿â˜…ğœğ‘˜ . Since for
each 1 â‰¤ ğ‘– â‰¤ ğ‘š, ğ‘“ğ‘–(ğ‘¿â˜…ğœğ‘˜) â‰¤ 0 and ğ‘“ğ‘– is lower semi-continuous, ğ‘¿ğ‘ obeys
ğ‘“ğ‘–(ğ‘¿ğ‘) â‰¤ 0, ğ‘– = 1, . . . ,ğ‘š. (3.18)
Furthermore, since âˆ¥ğ‘¿â˜…ğœ âˆ¥2ğ¹ is bounded, (3.16) yields
lim sup
ğœâ†’âˆ
âˆ¥ğ‘¿â˜…ğœ âˆ¥âˆ— â‰¤ âˆ¥ğ‘¿âˆâˆ¥âˆ—, âˆ¥ğ‘¿âˆâˆ¥âˆ— â‰¤ lim infğœâ†’âˆ âˆ¥ğ‘¿
â˜…
ğœ âˆ¥âˆ—.
An immediate consequence is limğœâ†’âˆ âˆ¥ğ‘¿â˜…ğœ âˆ¥âˆ— = âˆ¥ğ‘¿âˆâˆ¥âˆ— and, therefore, âˆ¥ğ‘¿ğ‘âˆ¥âˆ— = âˆ¥ğ‘¿âˆâˆ¥âˆ—. This
shows that ğ‘¿ğ‘ is a solution to (1.1). Now it follows from the definition of ğ‘¿âˆ that âˆ¥ğ‘¿ğ‘âˆ¥ğ¹ â‰¥
âˆ¥ğ‘¿âˆâˆ¥ğ¹ , while we also have âˆ¥ğ‘¿ğ‘âˆ¥ğ¹ â‰¤ âˆ¥ğ‘¿âˆâˆ¥ğ¹ because of (3.17). We conclude that âˆ¥ğ‘¿ğ‘âˆ¥ğ¹ =
âˆ¥ğ‘¿âˆâˆ¥ğ¹ and thus ğ‘¿ğ‘ = ğ‘¿âˆ since ğ‘¿âˆ is unique.
13
4 Convergence Analysis
This section establishes the convergence of the SVT iterations. We begin with the simpler proof
of the convergence of (2.7) in the special case of the matrix completion problem, and then present
the argument for the more general constraints (3.5). We hope that this progression will make the
second and more general proof more transparent.
4.1 Convergence for matrix completion
We begin by recording a lemma which establishes the strong convexity of the objective ğ‘“ğœ .
Lemma 4.1 Let ğ’ âˆˆ âˆ‚ğ‘“ğœ (ğ‘¿) and ğ’ â€² âˆˆ âˆ‚ğ‘“ğœ (ğ‘¿ â€²). Then
âŸ¨ğ’ âˆ’ğ’ â€²,ğ‘¿ âˆ’ğ‘¿ â€²âŸ© â‰¥ âˆ¥ğ‘¿ âˆ’ğ‘¿ â€²âˆ¥2ğ¹ . (4.1)
Proof. An element ğ’ of âˆ‚ğ‘“ğœ (ğ‘¿) is of the form ğ’ = ğœğ’0 +ğ‘¿, where ğ’0 âˆˆ âˆ‚âˆ¥ğ‘¿âˆ¥âˆ—, and similarly
for ğ’ â€². This gives
âŸ¨ğ’ âˆ’ğ’ â€²,ğ‘¿ âˆ’ğ‘¿ â€²âŸ© = ğœ âŸ¨ğ’0 âˆ’ğ’ â€²0,ğ‘¿ âˆ’ğ‘¿ â€²âŸ©+ âˆ¥ğ‘¿ âˆ’ğ‘¿ â€²âˆ¥2ğ¹
and it thus suffices to show that the first term of the right-hand side is nonnegative. From (2.6),
we have that any subgradient of the nuclear norm at ğ‘¿ obeys âˆ¥ğ’0âˆ¥2 â‰¤ 1 and âŸ¨ğ’0,ğ‘¿âŸ© = âˆ¥ğ‘¿âˆ¥âˆ—. In
particular, this gives
âˆ£âŸ¨ğ’0,ğ‘¿ â€²âŸ©âˆ£ â‰¤ âˆ¥ğ’0âˆ¥2âˆ¥ğ‘¿ â€²âˆ¥âˆ— â‰¤ âˆ¥ğ‘¿ â€²âˆ¥âˆ—, âˆ£âŸ¨ğ’ â€²0,ğ‘¿âŸ©âˆ£ â‰¤ âˆ¥ğ’ â€²0âˆ¥2âˆ¥ğ‘¿âˆ¥âˆ— â‰¤ âˆ¥ğ‘¿âˆ¥âˆ—.
Whence,
âŸ¨ğ’0 âˆ’ğ’ â€²0,ğ‘¿ âˆ’ğ‘¿ â€²âŸ© = âŸ¨ğ’0,ğ‘¿âŸ©+ âŸ¨ğ’ â€²0,ğ‘¿ â€²âŸ© âˆ’ âŸ¨ğ’0,ğ‘¿ â€²âŸ© âˆ’ âŸ¨ğ’ â€²0,ğ‘¿âŸ©
= âˆ¥ğ‘¿âˆ¥âˆ— + âˆ¥ğ‘¿ â€²âˆ¥âˆ— âˆ’ âŸ¨ğ’0,ğ‘¿ â€²âŸ© âˆ’ âŸ¨ğ’ â€²0,ğ‘¿âŸ© â‰¥ 0,
which proves the lemma.
This lemma is key in showing that the SVT algorithm (2.7) converges.
Theorem 4.2 Suppose that the sequence of step sizes obeys 0 < inf ğ›¿ğ‘˜ â‰¤ sup ğ›¿ğ‘˜ < 2/âˆ¥ğ’œâˆ¥2. Then
the sequence {ğ‘¿ğ‘˜} obtained via (3.3) converges to the unique solution to (3.1). In particular,
the sequence {ğ‘¿ğ‘˜} obtained via (2.7) converges to the unique solution of (2.8) provided that 0 <
inf ğ›¿ğ‘˜ â‰¤ sup ğ›¿ğ‘˜ < 2.
Proof. The second assertion follows from the first by takingğ’œ to be the sampling operator extracting
those entries in Î© since âˆ¥ğ’œâˆ¥ = 1. To prove the first claim then, let (ğ‘¿â˜…,ğ’šâ˜…) be primal-dual optimal
for the problem (3.1). The optimality conditions give
0 = ğ’ğ‘˜ âˆ’ğ’œâˆ—(ğ’šğ‘˜âˆ’1)
0 = ğ’â˜… âˆ’ğ’œâˆ—(ğ’šâ˜…),
for some ğ’ğ‘˜ âˆˆ âˆ‚ğ‘“ğœ (ğ‘¿ğ‘˜) and some ğ’â˜… âˆˆ âˆ‚ğ‘“ğœ (ğ‘¿â˜…). We then deduce that
(ğ’ğ‘˜ âˆ’ğ’â˜…)âˆ’ğ’œâˆ—(ğ’šğ‘˜âˆ’1 âˆ’ ğ’šâ˜…) = 0
14
and, therefore, it follows from Lemma 4.1 that
âŸ¨ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…,ğ’œâˆ—(ğ’šğ‘˜âˆ’1 âˆ’ ğ’šâ˜…)âŸ© = âŸ¨ğ’ğ‘˜ âˆ’ğ’â˜…,ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…âŸ© â‰¥ âˆ¥ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…âˆ¥2ğ¹ . (4.2)
We continue and observe that because ğ’œ(ğ‘¿â˜…) = ğ’œ(ğ‘´),
âˆ¥ğ’šğ‘˜ âˆ’ ğ’šâˆ¥ğ¹ = âˆ¥ğ’šğ‘˜âˆ’1 âˆ’ ğ’šâ˜… + ğ›¿ğ‘˜ğ’œ(ğ‘¿â˜… âˆ’ğ‘¿ğ‘˜)âˆ¥ğ¹ .
Therefore, setting ğ‘Ÿğ‘˜ = âˆ¥ğ’šğ‘˜ âˆ’ ğ’šâ˜…âˆ¥ğ¹ ,
ğ‘Ÿ2ğ‘˜ = ğ‘Ÿ
2
ğ‘˜âˆ’1 âˆ’ 2ğ›¿ğ‘˜âŸ¨ğ’œâˆ—(ğ’šğ‘˜âˆ’1 âˆ’ ğ’šâ˜…),ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…âŸ©+ ğ›¿2ğ‘˜âˆ¥ğ’œ(ğ‘¿â˜… âˆ’ğ‘¿ğ‘˜)âˆ¥2ğ¹
â‰¤ ğ‘Ÿ2ğ‘˜âˆ’1 âˆ’ 2ğ›¿ğ‘˜âˆ¥ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…âˆ¥2ğ¹ + ğ›¿2ğ‘˜âˆ¥ğ’œâˆ¥2âˆ¥ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…âˆ¥2ğ¹ . (4.3)
Under our assumptions, we have 2ğ›¿ğ‘˜ âˆ’ ğ›¿2ğ‘˜âˆ¥ğ’œâˆ¥2 â‰¥ ğ›½ for all ğ‘˜ â‰¥ 1 and some ğ›½ > 0 and thus
ğ‘Ÿ2ğ‘˜ â‰¤ ğ‘Ÿ2ğ‘˜âˆ’1 âˆ’ ğ›½âˆ¥ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…âˆ¥2ğ¹ . (4.4)
Two properties follow from this:
1. The sequence {âˆ¥ğ’œâˆ—(ğ’šğ‘˜ âˆ’ ğ’šâ˜…)âˆ¥ğ¹ } is nonincreasing and, therefore, converges to a limit.
2. As a consequence, âˆ¥ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…âˆ¥2ğ¹ â†’ 0 as ğ‘˜ â†’ âˆ.
The theorem is established.
4.2 General convergence theorem
Our second result is more general and establishes the convergence of the SVT iterations to the
solution of (3.4) under general convex constraints. From now now, we will only assume that the
function â„±(ğ‘¿) is Lipschitz in the sense that
âˆ¥â„±(ğ‘¿)âˆ’â„±(ğ’€ âˆ¥ â‰¤ ğ¿(â„±)âˆ¥ğ‘¿ âˆ’ ğ’€ âˆ¥ğ¹ , (4.5)
for some nonnegative constant ğ¿(â„±). Note that if â„± is affine, â„±(ğ‘¿) = ğ’ƒ âˆ’ ğ’œ(ğ‘¿), we have
ğ¿(â„±) = âˆ¥ğ’œâˆ¥2 where âˆ¥ğ’œâˆ¥2 is the spectrum norm of the linear transformation ğ’œ defined as âˆ¥ğ’œâˆ¥2 :=
sup{âˆ¥ğ’œ(ğ‘¿)âˆ¥â„“2 : âˆ¥ğ‘¿âˆ¥ğ¹ = 1}. We also recall that â„±(ğ‘¿) = (ğ‘“1(ğ‘¿), . . . , ğ‘“ğ‘š(ğ‘¿)) where each ğ‘“ğ‘– is
convex, and that the Lagrangian for the problem (3.4) is given by
â„’(ğ‘¿,ğ’š) = ğ‘“ğœ (ğ‘¿) + âŸ¨ğ’š,â„±(ğ‘¿)âŸ©, ğ’š â‰¥ 0.
To simplify, we will assume that strong duality holds which is automatically true if the constraints
obey constraint qualifications such as Slaterâ€™s condition [6].
We first establish the following preparatory lemma.
Lemma 4.3 Let (ğ‘¿â˜…,ğ’šâ˜…) be a primal-dual optimal pair for (3.4). Then for each ğ›¿ > 0, ğ’šâ˜… obeys
ğ’šâ˜… = [ğ’šâ˜… + ğ›¿â„±(ğ‘¿â˜…)]+. (4.6)
15
Proof. Recall that the projection ğ’™0 of a point ğ’™ onto a convex set ğ’ is characterized by{
ğ’™0 âˆˆ ğ’,
âŸ¨ğ’š âˆ’ ğ’™0,ğ’™âˆ’ ğ’™0âŸ© â‰¤ 0, âˆ€ğ’š âˆˆ ğ’.
In the case where ğ’ = â„ğ‘š+ = {ğ’™ âˆˆ â„ğ‘š : ğ’™ â‰¥ 0}, this condition becomes ğ’™0 â‰¥ 0 and
âŸ¨ğ’š âˆ’ ğ’™0,ğ’™âˆ’ ğ’™0âŸ© â‰¤ 0, âˆ€ğ’š â‰¥ 0.
Now because ğ’šâ˜… is dual optimal we have
â„’(ğ‘¿â˜…,ğ’šâ˜…) â‰¥ â„’(ğ‘¿â˜…,ğ’š), âˆ€ğ’š â‰¥ 0.
Substituting the expression for the Lagrangian, this is equivalent to
âŸ¨ğ’š âˆ’ ğ’šâ˜…,â„±(ğ‘¿â˜…)âŸ© â‰¤ 0, âˆ€ğ’š â‰¥ 0,
which is the same as
âŸ¨ğ’š âˆ’ ğ’šâ˜…,ğ’šâ˜… + ğœŒâ„±(ğ‘¿â˜…)âˆ’ ğ’šâ˜…âŸ© â‰¤ 0, âˆ€ğ’š â‰¥ 0, âˆ€ğœŒ â‰¥ 0.
Hence it follows that ğ’šâ˜… must be the projection of ğ’šâ˜… + ğœŒâ„±(ğ‘¿â˜…) onto the nonnegative orthant â„ğ‘š+ .
Since the projection of an arbitrary vector ğ’™ onto â„ğ‘š+ is given by ğ’™+, our claim follows.
We are now in the position to state our general convergence result.
Theorem 4.4 Suppose that the sequence of step sizes obeys 0 < inf ğ›¿ğ‘˜ â‰¤ sup ğ›¿ğ‘˜ < 2/âˆ¥ğ¿(â„±)âˆ¥2,
where ğ¿(â„±) is the Lipschitz constant in (4.5). Then assuming strong duality, the sequence {ğ‘¿ğ‘˜}
obtained via (3.5) converges to the unique solution of (3.4).
Proof. Let (ğ‘¿â˜…,ğ’šâ˜…) be primal-dual optimal for the problem (3.4). We claim that the optimality
conditions give that for all ğ‘¿
âŸ¨ğ’ğ‘˜,ğ‘¿ âˆ’ğ‘¿ğ‘˜âŸ©+ âŸ¨ğ’šğ‘˜âˆ’1,â„±(ğ‘¿)âˆ’â„±(ğ‘¿ğ‘˜)âŸ© â‰¥ 0,
âŸ¨ğ’â˜…,ğ‘¿ âˆ’ğ‘¿â˜…âŸ©+ âŸ¨ğ’šâ˜…,â„±(ğ‘¿)âˆ’â„±(ğ‘¿â˜…)âŸ© â‰¥ 0, (4.7)
for some ğ’ğ‘˜ âˆˆ âˆ‚ğ‘“ğœ (ğ‘¿ğ‘˜) and some ğ’â˜… âˆˆ âˆ‚ğ‘“ğœ (ğ‘¿â˜…). We justify this assertion by proving one of the
two inequalities since the other is exactly similar. For the first, ğ‘¿ğ‘˜ minimizes â„’(ğ‘¿,ğ’šğ‘˜âˆ’1) over all
ğ‘¿ and, therefore, there exist ğ’ğ‘˜ âˆˆ âˆ‚ğ‘“ğœ (ğ‘¿ğ‘˜) and ğ’ğ‘˜ğ‘– âˆˆ âˆ‚ğ‘“ğ‘–(ğ‘¿ğ‘˜), 1 â‰¤ ğ‘– â‰¤ ğ‘š, such that
ğ’ğ‘˜ +
ğ‘šâˆ‘
ğ‘–=1
ğ‘¦ğ‘˜âˆ’1ğ‘– ğ’
ğ‘˜
ğ‘– = 0.
Now because each ğ‘“ğ‘– is convex,
ğ‘“ğ‘–(ğ‘¿)âˆ’ ğ‘“ğ‘–(ğ‘¿ğ‘˜) â‰¥ âŸ¨ğ’ğ‘˜ğ‘– ,ğ‘¿ âˆ’ğ‘¿ğ‘˜âŸ©
and, therefore,
âŸ¨ğ’ğ‘˜,ğ‘¿ âˆ’ğ‘¿ğ‘˜âŸ©+
ğ‘šâˆ‘
ğ‘–=1
ğ‘¦ğ‘˜âˆ’1ğ‘– (ğ‘“ğ‘–(ğ‘¿)âˆ’ ğ‘“ğ‘–(ğ‘¿ğ‘˜)) â‰¥ âŸ¨ğ’ğ‘˜ +
ğ‘šâˆ‘
ğ‘–=1
ğ‘¦ğ‘˜âˆ’1ğ‘– ğ’
ğ‘˜
ğ‘– ,ğ‘¿ âˆ’ğ‘¿ğ‘˜âŸ© = 0.
16
This is (4.7).
Now write the first inequality in (4.7) for ğ‘¿â˜…, the second for ğ‘¿ğ‘˜ and sum the two inequalities.
This gives
âŸ¨ğ’ğ‘˜ âˆ’ğ’â˜…,ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…âŸ©+ âŸ¨ğ’šğ‘˜âˆ’1 âˆ’ ğ’šâ˜…,â„±(ğ‘¿ğ‘˜)âˆ’â„±(ğ‘¿â˜…)âŸ© â‰¤ 0.
The rest of the proof is essentially the same as that of Theorem 4.2. It follows from Lemma 4.1
that
âŸ¨ğ’šğ‘˜âˆ’1 âˆ’ ğ’šâ˜…,â„±(ğ‘¿ğ‘˜)âˆ’â„±(ğ‘¿â˜…)âŸ© â‰¤ âˆ’âŸ¨ğ’ğ‘˜ âˆ’ğ’â˜…,ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…âŸ© â‰¤ âˆ’âˆ¥ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…âˆ¥2ğ¹ . (4.8)
We continue and observe that because ğ’šâ˜… = [ğ’šâ˜… + ğ›¿ğ‘˜â„±(ğ‘¿)]+ by Lemma 4.3, we have
âˆ¥ğ’šğ‘˜ âˆ’ ğ’šâ˜…âˆ¥ = âˆ¥[ğ’šğ‘˜âˆ’1 + ğ›¿ğ‘˜â„±(ğ‘¿ğ‘˜)]+ âˆ’ [ğ’šâ˜… + ğ›¿ğ‘˜â„±(ğ‘¿â˜…)]+âˆ¥
â‰¤ âˆ¥ğ’šğ‘˜âˆ’1 âˆ’ ğ’šâ˜… + ğ›¿ğ‘˜(â„±(ğ‘¿ğ‘˜)âˆ’â„±(ğ‘¿â˜…))âˆ¥
since the projection onto the convex set â„ğ‘š+ is a contraction. Therefore,
âˆ¥ğ’šğ‘˜ âˆ’ ğ’šâ˜…âˆ¥2 = âˆ¥ğ’šğ‘˜âˆ’1 âˆ’ ğ’šâ˜…âˆ¥2 + 2ğ›¿ğ‘˜ âŸ¨ğ’šğ‘˜âˆ’1 âˆ’ ğ’šâ˜…,â„±(ğ‘¿ğ‘˜)âˆ’â„±(ğ‘¿â˜…)âŸ©+ ğ›¿2ğ‘˜âˆ¥â„±(ğ‘¿ğ‘˜)âˆ’â„±(ğ‘¿â˜…)âˆ¥2
â‰¤ âˆ¥ğ’šğ‘˜âˆ’1 âˆ’ ğ’šâ˜…âˆ¥2 âˆ’ 2ğ›¿ğ‘˜âˆ¥ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…âˆ¥2ğ¹ + ğ›¿2ğ‘˜ğ¿2 âˆ¥ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…âˆ¥2ğ¹ ,
where we have put ğ¿ instead of ğ¿(â„±) for short. Under our assumptions about the size of ğ›¿ğ‘˜, we
have 2ğ›¿ğ‘˜ âˆ’ ğ›¿2ğ‘˜ğ¿2 â‰¥ ğ›½ for all ğ‘˜ â‰¥ 1 and some ğ›½ > 0. Then
âˆ¥ğ’šğ‘˜ âˆ’ ğ’šâ˜…âˆ¥2 â‰¤ âˆ¥ğ’šğ‘˜âˆ’1 âˆ’ ğ’šâ˜…âˆ¥2 âˆ’ ğ›½âˆ¥ğ‘¿ğ‘˜ âˆ’ğ‘¿â˜…âˆ¥2ğ¹ , (4.9)
and the conclusion is as before.
5 Implementation and Numerical Results
This section provides implementation details of the SVT algorithmâ€”as to make it practically
effective for matrix completionâ€”such as the numerical evaluation of the singular value thresholding
operator, the selection of the step size ğ›¿ğ‘˜, the selection of a stopping criterion, and so on. This
section also introduces several numerical simulation results which demonstrate the performance
and effectiveness of the SVT algorithm. We show that 30, 000 Ã— 30, 000 matrices of rank 10 are
recovered from just about 0.4% of their sampled entries in a matter of a few minutes on a modest
desktop computer with a 1.86 GHz CPU (dual core with Matlabâ€™s multithreading option enabled)
and 3 GB of memory.
5.1 Implementation details
5.1.1 Evaluation of the singular value thresholding operator
To apply the singular value thresholding operator at level ğœ to an input matrix, it suffices to know
those singular values and corresponding singular vectors above the threshold ğœ . In the matrix
completion problem, the singular value thresholding operator is applied to sparse matrices {ğ’€ ğ‘˜}
since the number of sampled entries is typically much lower than the number of entries in the
unknown matrix ğ‘´ , and we are hence interested in numerical methods for computing the dominant
singular values and singular vectors of large sparse matrices. The development of such methods is
a relatively mature area in scientific computing and numerical linear algebra in particular. In fact,
17
many high-quality packages are readily available. Our implementation uses PROPACK, see [38]
for documentation and availability. One reason for this choice is convenience: PROPACK comes
in a Matlab and a Fortran version, and we find it convenient to use the well-documented Matlab
version. More importantly, PROPACK uses the iterative Lanczos algorithm to compute the singular
values and singular vectors directly, by using the Lanczos bidiagonalization algorithm with partial
reorthogonalization. In particular, PROPACK does not compute the eigenvalues and eigenvectors
of (ğ’€ ğ‘˜)âˆ—ğ’€ ğ‘˜ and ğ’€ ğ‘˜(ğ’€ ğ‘˜)âˆ—, or of an augmented matrix as in the Matlab built-in function â€˜svdsâ€™ for
example. Consequently, PROPACK is an efficientâ€”both in terms of number of flops and storage
requirementâ€”and stable package for computing the dominant singular values and singular vectors
of a large sparse matrix. For information, the available documentation [38] reports a speedup
factor of about ten over Matlabâ€™s â€˜svdsâ€™. Furthermore, the Fortran version of PROPACK is about
3â€“4 times faster than the Matlab version. Despite this significant speedup, we have only used the
Matlab version but since the singular value shrinkage operator is by-and-large the dominant cost in
the SVT algorithm, we expect that a Fortran implementation would run about 3 to 4 times faster.
As for most SVD packages, though one can specify the number of singular values to compute,
PROPACK can not automatically compute only those singular values exceeding the threshold ğœ .
One must instead specify the number ğ‘  of singular values ahead of time, and the software will
compute the ğ‘  largest singular values and corresponding singular vectors. To use this package, we
must then determine the number ğ‘ ğ‘˜ of singular values of ğ’€
ğ‘˜âˆ’1 to be computed at the ğ‘˜th iteration.
We use the following simple method. Let ğ‘Ÿğ‘˜âˆ’1 = rank(ğ‘¿ğ‘˜âˆ’1) be the number of nonzero singular
values of ğ‘¿ğ‘˜âˆ’1 at the previous iteration. Set ğ‘ ğ‘˜ = ğ‘Ÿğ‘˜âˆ’1+1 and compute the first ğ‘ ğ‘˜ singular values
of ğ’€ ğ‘˜âˆ’1. If some of the computed singular values are already smaller than ğœ , then ğ‘ ğ‘˜ is a right
choice. Otherwise, increment ğ‘ ğ‘˜ by a predefined integer â„“ repeatedly until some of the singular
values fall below ğœ . In the experiments, we choose â„“ = 5. Another rule might be to repeatedly
multiply ğ‘ ğ‘˜ by a positive numberâ€”e.g. 2â€”until our criterion is met. Incrementing ğ‘ ğ‘˜ by a fixed
integer works very well in practice; in our experiments, we very rarely need more than one update.
We note that it is not necessary to rerun the Lanczos iterations for the first ğ‘ ğ‘˜ vectors since they
have been already computed; only a few new singular values (â„“ of them) need to be numerically
evaluated. This can be done by modifying the PROPACK routines. We have not yet modified
PROPACK, however. Had we done so, our run times would be decreased.
5.1.2 Step sizes
There is a large literature on ways of selecting a step size but for simplicity, we shall use step sizes
that are independent of the iteration count; that is ğ›¿ğ‘˜ = ğ›¿ for ğ‘˜ = 1, 2, . . .. From Theorem 4.2,
convergence for the completion problem is guaranteed (2.7) provided that 0 < ğ›¿ < 2. This choice
is, however, too conservative and the convergence is typically slow. In our experiments, we use
instead
ğ›¿ = 1.2
ğ‘›1ğ‘›2
ğ‘š
, (5.1)
i.e. 1.2 times the undersampling ratio. We give a heuristic justification below.
Consider a fixed matrix ğ‘¨ âˆˆ â„ğ‘›1Ã—ğ‘›2 . Under the assumption that the column and row spaces of
ğ‘¨ are not well aligned with the vectors taken from the canonical basis of â„ğ‘›1 and â„ğ‘›2 respectivelyâ€”
the incoherence assumption in [14]â€”then with very large probability over the choices of Î©, we have
(1âˆ’ ğœ–)ğ‘ âˆ¥ğ‘¨âˆ¥2ğ¹ â‰¤ âˆ¥ğ’«Î©(ğ‘¨)âˆ¥2ğ¹ â‰¤ (1 + ğœ–)ğ‘ âˆ¥ğ‘¨âˆ¥2ğ¹ , ğ‘ := ğ‘š/(ğ‘›1ğ‘›2), (5.2)
18
provided that the rank of ğ‘¨ is not too large. The probability model is that Î© is a set of sampled
entries of cardinality ğ‘š sampled uniformly at random so that all the choices are equally likely. In
(5.2), we want to think of ğœ– as a small constant, e.g. smaller than 1/2. In other words, the â€˜energyâ€™
of ğ‘¨ on Î© (the set of sampled entries) is just about proportional to the size of Î©. The near isometry
(5.2) is a consequence of Theorem 4.1 in [14], and we omit the details.
Now returning to the proof of Theorem 4.2, we see that a sufficient condition for the convergence
of (2.7) is
âˆƒğ›½ > 0, âˆ’2ğ›¿âˆ¥ğ‘¿â˜… âˆ’ğ‘¿ğ‘˜âˆ¥2ğ¹ + ğ›¿2âˆ¥ğ’«Î©(ğ‘¿â˜… âˆ’ğ‘¿ğ‘˜)âˆ¥2ğ¹ â‰¤ âˆ’ğ›½âˆ¥ğ‘¿â˜… âˆ’ğ‘¿ğ‘˜âˆ¥2ğ¹ ,
compare (4.4), which is equivalent to
0 < ğ›¿ < 2
âˆ¥ğ‘¿â˜… âˆ’ğ‘¿ğ‘˜âˆ¥2ğ¹
âˆ¥ğ’«Î©(ğ‘¿â˜… âˆ’ğ‘¿ğ‘˜)âˆ¥2ğ¹
.
Since âˆ¥ğ’«Î©(ğ‘¿)âˆ¥ğ¹ â‰¤ âˆ¥ğ‘¿âˆ¥ğ¹ for any matrix ğ‘¿ âˆˆ â„ğ‘›1Ã—ğ‘›2 , it is safe to select ğ›¿ < 2. But suppose that
we could apply (5.2) to the matrix ğ‘¨ = ğ‘¿â˜… âˆ’ ğ‘¿ğ‘˜. Then we could take ğ›¿ inversely proportional
to ğ‘; e.g. with ğœ– = 1/4, we could take ğ›¿ â‰¤ 1.6ğ‘âˆ’1. Below, we shall use the value ğ›¿ = 1.2ğ‘âˆ’1 which
allows us to take large steps and still provides convergence, at least empirically.
The reason why this is not a rigorous argument is that (5.2) cannot be applied to ğ‘¨ = ğ‘¿â˜…âˆ’ğ‘¿ğ‘˜
even though this matrix difference may obey the incoherence assumption. The issue here is that
ğ‘¿â˜… âˆ’ ğ‘¿ğ‘˜ is not a fixed matrix, but rather depends on Î© since the iterates {ğ‘¿ğ‘˜} are computed
with the knowledge of the sampled set.
5.1.3 Initial steps
The SVT algorithm starts with ğ’€ 0 = 0, and we want to choose a large ğœ to make sure that the
solution of (2.8) is close enough to a solution of (1.1). Define ğ‘˜0 as that integer obeying
ğœ
ğ›¿âˆ¥ğ’«Î©(ğ‘´)âˆ¥2 âˆˆ (ğ‘˜0 âˆ’ 1, ğ‘˜0]. (5.3)
Since ğ’€ 0 = 0, it is not difficult to see that
ğ‘¿ğ‘˜ = 0, ğ’€ ğ‘˜ = ğ‘˜ğ›¿ğ’«Î©(ğ‘´), ğ‘˜ = 1, . . . , ğ‘˜0.
To save work, we may simply skip the computations of ğ‘¿1, . . . ,ğ‘¿ğ‘˜0 , and start the iteration by
computing ğ‘¿ğ‘˜0+1 from ğ’€ ğ‘˜0 .
This strategy is a special case of a kicking device introduced in [47]; the main idea of such
a kicking scheme is that one can â€˜jump overâ€™ a few steps whenever possible. Just like in the
aforementioned reference, we can develop similar kicking strategies here as well. Because in our
numerical experiments the kicking is rarely triggered, we forgo the description of such strategies.
5.1.4 Stopping criteria
Here, we discuss stopping criteria for the sequence of SVT iterations (2.7), and present two possi-
bilities.
19
The first is motivated by the first-order optimality conditions or KKT conditions tailored to the
minimization problem (2.8). By (2.14) and letting âˆ‚ğ’€ ğ‘”0(ğ’€ ) = 0 in (2.13), we see that the solution
ğ‘¿â˜…ğœ to (2.8) must also verify {
ğ‘¿ = ğ’Ÿğœ (ğ’€ ),
ğ’«Î©(ğ‘¿ âˆ’ğ‘´) = 0,
(5.4)
where ğ’€ is a matrix vanishing outside of Î©ğ‘. Therefore, to make sure that ğ‘¿ğ‘˜ is close to ğ‘¿â˜…ğœ , it
is sufficient to check how close (ğ‘¿ğ‘˜,ğ’€ ğ‘˜âˆ’1) is to obeying (5.4). By definition, the first equation in
(5.4) is always true. Therefore, it is natural to stop (2.7) when the error in the second equation is
below a specified tolerance. We suggest stopping the algorithm when
âˆ¥ğ’«Î©(ğ‘¿ğ‘˜ âˆ’ğ‘´)âˆ¥ğ¹
âˆ¥ğ’«Î©(ğ‘´)âˆ¥ğ¹ â‰¤ ğœ–, (5.5)
where ğœ– is a fixed tolerance, e.g. 10âˆ’4. We provide a short heuristic argument justifying this choice
below.
In the matrix completion problem, we know that under suitable assumptions
âˆ¥ğ’«Î©(ğ‘´)âˆ¥2ğ¹ â‰ ğ‘ âˆ¥ğ‘´âˆ¥2ğ¹ ,
which is just (5.2) applied to the fixed matrix ğ‘´ (the symbol â‰ here means that there is a constant
ğœ– as in (5.2)). Suppose we could also apply (5.2) to the matrix ğ‘¿ğ‘˜âˆ’ğ‘´ (which we rigorously cannot
since ğ‘¿ğ‘˜ depends on Î©), then we would have
âˆ¥ğ’«Î©(ğ‘¿ğ‘˜ âˆ’ğ‘´)âˆ¥2ğ¹ â‰ ğ‘ âˆ¥ğ‘¿ğ‘˜ âˆ’ğ‘´âˆ¥2ğ¹ , (5.6)
and thus
âˆ¥ğ’«Î©(ğ‘¿ğ‘˜ âˆ’ğ‘´)âˆ¥ğ¹
âˆ¥ğ’«Î©(ğ‘´)âˆ¥ğ¹ â‰
âˆ¥ğ‘¿ğ‘˜ âˆ’ğ‘´âˆ¥ğ¹
âˆ¥ğ‘´âˆ¥ğ¹ .
In words, one would control the relative reconstruction error by controlling the relative error on
the set of sampled locations.
A second stopping criterion comes from duality theory. Firstly, the iterates ğ‘¿ğ‘˜ are generally
not feasible for (2.8) although they become asymptotically feasible. One can construct a feasible
point from ğ‘¿ğ‘˜ by projecting it onto the affine space {ğ‘¿ : ğ’«Î©(ğ‘¿) = ğ’«Î©(ğ‘´)} as follows:
?Ìƒ?ğ‘˜ = ğ‘¿ğ‘˜ + ğ’«Î©(ğ‘´ âˆ’ğ‘¿ğ‘˜).
As usual let ğ‘“ğœ (ğ‘¿) = ğœâˆ¥ğ‘¿âˆ¥âˆ— + 12âˆ¥ğ‘¿âˆ¥2ğ¹ and denote by ğ‘â˜… the optimal value of (2.8). Since ?Ìƒ?ğ‘˜ is
feasible, we have
ğ‘â˜… â‰¤ ğ‘“ğœ (?Ìƒ?ğ‘˜) := ğ‘ğ‘˜.
Secondly, using the notations of Section 2.4, duality theory gives that
ğ‘ğ‘˜ := ğ‘”0(ğ’€
ğ‘˜âˆ’1) = â„’(ğ‘¿ğ‘˜,ğ’€ ğ‘˜âˆ’1) â‰¤ ğ‘â˜….
Therefore, ğ‘ğ‘˜ âˆ’ ğ‘ğ‘˜ is an upper bound on the duality gap and one can stop the algorithm when this
quantity falls below a given tolerance.
20
For very large problems in which one holds ğ‘¿ğ‘˜ in reduced SVD form, one may not want to
compute the projection ?Ìƒ?ğ‘˜ since this matrix would not have low rank and would require signifi-
cant storage space (presumably, one would not want to spend much time computing this projection
either). Hence, the second method only makes practical sense when the dimensions are not pro-
hibitively large, or when the iterates do not have low rank.
Similarly, one can derive stopping criteria for all the iterations (3.3), (3.5) and (3.6). For
example, we can stop (3.3) for general linear constraints when âˆ¥ğ’œ(ğ‘¿ğ‘˜)âˆ’ ğ’ƒâˆ¥/âˆ¥ğ’ƒâˆ¥ â‰¤ ğœ–. We omit the
detailed discussions here.
5.1.5 Algorithm
We conclude this section by summarizing the implementation details and give the SVT algorithm
for matrix completion below (Algorithm 1). Of course, one would obtain a very similar structure
for the more general problems of the form (3.1) and (3.4) with linear inequality constraints. For
convenience, define for each nonnegative integer ğ‘  â‰¤ min{ğ‘›1, ğ‘›2},
[ğ‘¼ğ‘˜,Î£ğ‘˜,ğ‘½ ğ‘˜]ğ‘ , ğ‘˜ = 1, 2, . . . ,
where ğ‘¼ğ‘˜ = [ğ’–ğ‘˜1, . . . ,ğ’–
ğ‘˜
ğ‘  ] and ğ‘½
ğ‘˜ = [ğ’—ğ‘˜1 , . . . ,ğ’—
ğ‘˜
ğ‘  ] are the first ğ‘  singular vectors of the matrix ğ’€
ğ‘˜,
and Î£ğ‘˜ is a diagonal matrix with the first ğ‘  singular values ğœğ‘˜1 , . . . , ğœ
ğ‘˜
ğ‘  on the diagonal.
Algorithm 1: Singular Value Thresholding (SVT) Algorithm
Input: sampled set Î© and sampled entries ğ’«Î©(ğ‘´), step size ğ›¿, tolerance ğœ–, parameter
ğœ , increment â„“, and maximum iteration count ğ‘˜max
Output: ğ‘¿opt
Description: Recover a low-rank matrix ğ‘´ from a subset of sampled entries
(1)Set ğ’€ 0 = ğ‘˜0ğ›¿ğ’«Î©(ğ‘´) (ğ‘˜0 is defined in (5.3))
(2)Set ğ‘Ÿ0 = 0
(3)for ğ‘˜ = 1 to ğ‘˜max
(4)Set ğ‘ ğ‘˜ = ğ‘Ÿğ‘˜âˆ’1 + 1
(5)repeat
(6) Compute [ğ‘¼ğ‘˜âˆ’1,Î£ğ‘˜âˆ’1,ğ‘½ ğ‘˜âˆ’1]ğ‘ ğ‘˜
(7) Set ğ‘ ğ‘˜ = ğ‘ ğ‘˜ + â„“
(8)until ğœğ‘˜âˆ’1ğ‘ ğ‘˜âˆ’â„“ â‰¤ ğœ
(9)Set ğ‘Ÿğ‘˜ = max{ğ‘— : ğœğ‘˜âˆ’1ğ‘— > ğœ}
(10)Set ğ‘¿ğ‘˜ =
âˆ‘ğ‘Ÿğ‘˜
ğ‘—=1(ğœ
ğ‘˜âˆ’1
ğ‘— âˆ’ ğœ)ğ’–ğ‘˜âˆ’1ğ‘— ğ’—ğ‘˜âˆ’1ğ‘—
(11)
if âˆ¥ğ’«Î©(ğ‘¿ğ‘˜ âˆ’ğ‘´)âˆ¥ğ¹ /âˆ¥ğ’«Î©ğ‘´âˆ¥ğ¹ â‰¤ ğœ– then break
(12)
Set ğ‘Œ ğ‘˜ğ‘–ğ‘— =
{
0 if (ğ‘–, ğ‘—) âˆ•âˆˆ Î©,
ğ‘Œ ğ‘˜âˆ’1ğ‘–ğ‘— + ğ›¿(ğ‘€ğ‘–ğ‘— âˆ’ğ‘‹ğ‘˜ğ‘–ğ‘—) if (ğ‘–, ğ‘—) âˆˆ Î©
(13)end for ğ‘˜
(14)Set ğ‘¿opt = ğ‘¿ğ‘˜
21
Unknown ğ‘´ Computational results
size (ğ‘›Ã— ğ‘›) rank (ğ‘Ÿ) ğ‘š/ğ‘‘ğ‘Ÿ ğ‘š/ğ‘›2 time(s) # iters relative error
10 6 0.12 23 117 1.64Ã— 10âˆ’4
1, 000Ã— 1, 000 50 4 0.39 196 114 1.59Ã— 10âˆ’4
100 3 0.57 501 129 1.68Ã— 10âˆ’4
10 6 0.024 147 123 1.73Ã— 10âˆ’4
5, 000Ã— 5, 000 50 5 0.10 950 108 1.61Ã— 10âˆ’4
100 4 0.158 3,339 123 1.72Ã— 10âˆ’4
10 6 0.012 281 123 1.73Ã— 10âˆ’4
10, 000Ã— 10, 000 50 5 0.050 2,096 110 1.65Ã— 10âˆ’4
100 4 0.080 7,059 127 1.79Ã— 10âˆ’4
10 6 0.006 588 124 1.73Ã— 10âˆ’4
20, 000Ã— 20, 000
50 5 0.025 4,581 111 1.66Ã— 10âˆ’4
30, 000Ã— 30, 000 10 6 0.004 1,030 125 1.73Ã— 10âˆ’4
Table 1: Experimental results for matrix completion. The rank ğ‘Ÿ is the rank of the unknown
matrix ğ‘´ , ğ‘š/ğ‘‘ğ‘Ÿ is the ratio between the number of sampled entries and the number of
degrees of freedom in an ğ‘›Ã—ğ‘› matrix of rank ğ‘Ÿ (oversampling ratio), and ğ‘š/ğ‘›2 is the fraction
of observed entries. All the computational results on the right are averaged over five runs.
5.2 Numerical results
5.2.1 Linear equality constraints
Our implementation is in Matlab and all the computational results we are about to report were
obtained on a desktop computer with a 1.86 GHz CPU (dual core with Matlabâ€™s multithreading
option enabled) and 3 GB of memory. In our simulations, we generate ğ‘› Ã— ğ‘› matrices of rank ğ‘Ÿ
by sampling two ğ‘›Ã— ğ‘Ÿ factors ğ‘´ğ¿ and ğ‘´ğ‘… independently, each having i.i.d. Gaussian entries, and
setting ğ‘´ = ğ‘´ğ¿ğ‘´
âˆ—
ğ‘… as it is suggested in [14]. The set of observed entries Î© is sampled uniformly
at random among all sets of cardinality ğ‘š.
The recovery is performed via the SVT algorithm (Algorithm 1), and we use
âˆ¥ğ’«Î©(ğ‘¿ğ‘˜ âˆ’ğ‘´)âˆ¥ğ¹ /âˆ¥ğ’«Î©ğ‘´âˆ¥ğ¹ < 10âˆ’4 (5.7)
as a stopping criterion. As discussed earlier, the step sizes are constant and we set ğ›¿ = 1.2ğ‘âˆ’1.
Throughout this section, we denote the output of the SVT algorithm by ğ‘¿opt. The parameter ğœ
is chosen empirically and set to ğœ = 5ğ‘›. A heuristic argument is as follows. Clearly, we would like
the term ğœâˆ¥ğ‘´âˆ¥âˆ— to dominate the other, namely, 12âˆ¥ğ‘´âˆ¥2ğ¹ . For products of Gaussian matrices as
above, standard random matrix theory asserts that the Frobenius norm of ğ‘´ concentrates around
ğ‘›
âˆš
ğ‘Ÿ, and that the nuclear norm concentrates around about ğ‘›ğ‘Ÿ (this should be clear in the simple
case where ğ‘Ÿ = 1 and is generally valid). The value ğœ = 5ğ‘› makes sure that on the average, the
value of ğœâˆ¥ğ‘´âˆ¥âˆ— is about 10 times that of 12âˆ¥ğ‘´âˆ¥2ğ¹ as long as the rank is bounded away from the
dimension ğ‘›.
Our computational results are displayed in Table 1. There, we report the run time in seconds, the
number of iterations it takes to reach convergence (5.7), and the relative error of the reconstruction
relative error = âˆ¥ğ‘¿opt âˆ’ğ‘´âˆ¥ğ¹ /âˆ¥ğ‘´âˆ¥ğ¹ , (5.8)
22
where ğ‘´ is the real unknown matrix. All of these quantities are averaged over five runs. The table
also gives the percentage of entries that are observed, namely, ğ‘š/ğ‘›2 together with a quantity that
we may want to think as the information oversampling ratio. Recall that an ğ‘›Ã— ğ‘› matrix of rank
ğ‘Ÿ depends upon ğ‘‘ğ‘Ÿ := ğ‘Ÿ(2ğ‘›âˆ’ ğ‘Ÿ) degrees of freedom. Then ğ‘š/ğ‘‘ğ‘Ÿ is the ratio between the number of
sampled entries and the â€˜true dimensionalityâ€™ of an ğ‘›Ã— ğ‘› matrix of rank ğ‘Ÿ.
The first observation is that the SVT algorithm performs extremely well in these experiments.
In all of our experiments, it takes fewer than 200 SVT iterations to reach convergence. As a
consequence, the run times are short. As indicated in the table, we note that one recovers a
1, 000Ã—1, 000 matrix of rank 10 in less than a minute. The algorithm also recovers 30, 000Ã—30, 000
matrices of rank 10 from about 0.4% of their sampled entries in just about 17 minutes. In addition,
higher-rank matrices are also efficiently completed: for example, it takes between one and two
hours to recover 10, 000Ã—10, 000 matrices of rank 100 and 20, 000Ã—20, 000 matrices of rank 50. We
would like to stress that these numbers were obtained on a modest CPU (1.86GHz). Furthermore,
a Fortran implementation is likely to cut down on these numbers by a multiplicative factor typically
between three and four.
We also check the validity of the stopping criterion (5.7) by inspecting the relative error defined
in (5.8). The table shows that the heuristic and nonrigorous analysis of Section 5.1 holds in practice
since the relative reconstruction error is of the same order as âˆ¥ğ’«Î©(ğ‘¿optâˆ’ğ‘´)âˆ¥ğ¹ /âˆ¥ğ’«Î©ğ‘´âˆ¥ğ¹ âˆ¼ 10âˆ’4.
Indeed, the overall relative errors reported in Table 1 are all less than 2Ã— 10âˆ’4.
We emphasized all along an important feature of the SVT algorithm, which is that the matrices
ğ‘¿ğ‘˜ have low rank. We demonstrate this fact empirically in Figure 1, which plots the rank of
ğ‘¿ğ‘˜ versus the iteration count ğ‘˜, and does this for unknown matrices of size 5, 000 Ã— 5, 000 with
different ranks. The plots reveal an interesting phenomenon: in our experiments, the rank of ğ‘¿ğ‘˜
is nondecreasing so that the maximum rank is reached in the final steps of the algorithm. In fact,
the rank of the iterates quickly reaches the value ğ‘Ÿ of the true rank. After these few initial steps,
the SVT iterations search for that matrix with rank ğ‘Ÿ minimizing the objective functional. As
mentioned earlier, the low-rank property is crucial for making the algorithm run fast.
0 20 40 60 80 100 120
0
1
2
3
4
5
6
7
8
9
10
11
Itertion Step k
R
an
k 
of
 X
k
0 20 40 60 80 100
0
10
20
30
40
50
60
Iteration step k
R
an
k 
of
 X
k
0 20 40 60 80 100 120
0
10
20
30
40
50
60
70
80
90
100
110
Iteration step k
R
an
k 
of
 X
k
ğ‘Ÿ = 10 ğ‘Ÿ = 50 ğ‘Ÿ = 100
Figure 1: Rank of ğ‘¿ğ‘˜ as a function ğ‘˜ when the unknown matrix ğ‘´ is of size 5, 000Ã— 5, 000
and of rank ğ‘Ÿ.
We now present a limited study examining the role of the parameters ğ›¿ and ğœ in the convergence.
We consider a square 1000 Ã— 1000 matrix of rank 10, and select a number ğ‘š of entries equal to
6 times the number of degrees of freedom; that is, ğ‘š = 6ğ‘‘ğ‘Ÿ. Numerical results are reported in
Table 2, which gives the number of iterations needed to achieve convergence (5.7) and the average
23
ğ›¿ = 0.8ğ‘âˆ’1 ğ›¿ = 1.2ğ‘âˆ’1 ğ›¿ = 1.6ğ‘âˆ’1
# of iters rank # of iters rank # of iters rank
mean std mean mean std mean mean std mean
ğœ = 2ğ‘› 322 192 15.4 764 1246 11.9 DNC DNC DNC
ğœ = 3ğ‘› 117 2.6 10.0 77 1.8 10.0 1310 2194 10.0
ğœ = 4ğ‘› 146 3.1 10.0 97 2.0 9.9 266 435 10.0
ğœ = 5ğ‘› 177 4.1 10.0 117 2.8 10.0 87 2.3 10.0
ğœ = 6ğ‘› 207 6.2 10.0 136 2.7 10.0 102 1.9 10.0
Table 2: Mean and standard deviation over five runs of the number of iterations needed to
achieve (5.7) for different values of the parameters ğ›¿ and ğœ , together with the average ranks of
ğ‘¿ğ‘˜. The test example is a random 1000Ã— 1000 matrix of rank 10, and the number of sampled
entries is ğ‘š = 6ğ‘‘ğ‘Ÿ. We also report â€˜DNCâ€™ when none of the five runs obeys (5.7) after 1,000
iterations.
rank of each iteration for different values of ğ›¿ and ğœ . This table suggests that for each value of ğ›¿,
there exists an optimal ğœ for which the SVT algorithm performs best. In more details, when ğœ is
smaller than this optimal value, the number of iterations needed to achieve convergence is larger
(and also more variable). In addition, the average rank of each iteration is also larger, and thus the
computational cost is higher. When ğœ is close to the optimal value, the SVT algorithm exhibits
a rapid convergence, and there is little variability in the number of iterations needed to achieve
convergence. When ğœ is too large, the SVT algorithm may overshrink ğ’€ ğ‘˜ at each iterate which, in
turn, leads to slow convergence. Table 2 also indicates that the convergence of the SVT algorithm
depends on the step size ğ›¿.
Finally, we demonstrate the results of the SVT algorithm for matrix completion from noisy
sampled entries. Suppose we observe data from the model
ğµğ‘–ğ‘— = ğ‘€ğ‘–ğ‘— + ğ‘ğ‘–ğ‘— , (ğ‘–, ğ‘—) âˆˆ Î©, (5.9)
where ğ’ is a zero-mean Gaussian white noise with standard deviation ğœ. We run the SVT algorithm
but stop early, as soon as ğ‘¿ğ‘˜ is consistent with the data and obeys
âˆ¥ğ’«Î©(ğ‘¿ğ‘˜ âˆ’ğ‘©)âˆ¥2ğ¹ â‰¤ (1 + ğœ–)ğ‘šğœ2, (5.10)
where ğœ– is a small parameter. Since âˆ¥ğ’«Î©(ğ‘´ âˆ’ğ‘©)âˆ¥2ğ¹ is very close to ğ‘šğœ2 for large values of ğ‘š, we
set ğœ– = 0. Our reconstruction ?Ì‚? is the first ğ‘¿ğ‘˜ obeying (5.10). The results are shown in Table 3
(the quantities are averages of 5 runs). Define the noise ratio as
âˆ¥ğ’«Î©(ğ’)âˆ¥ğ¹ /âˆ¥ğ’«Î©(ğ‘´)âˆ¥ğ¹ ,
and the relative error by (5.8). From Table 3, we see that the SVT algorithm works well as the
relative error between the recovered and the true data matrix is just about equal to the noise ratio.
The theory of low-rank matrix recovery from noisy data is nonexistent at the moment, and is
obviously beyond the scope of this paper. Having said this, we would like to conclude this section
with an intuitive and nonrigorous discussion, which may explain why the observed recovery error
24
Unknown matrix ğ‘´ Computational resultsnoise ratio
size (ğ‘›Ã— ğ‘›) rank (ğ‘Ÿ) ğ‘š/ğ‘‘ğ‘Ÿ ğ‘š/ğ‘›2 time(s) # iters relative error
10 6 0.12 10.8 51 0.78Ã— 10âˆ’2
10âˆ’2 1, 000Ã— 1, 000 50 4 0.39 87.7 48 0.95Ã— 10âˆ’2
100 3 0.57 216 50 1.13Ã— 10âˆ’2
10 6 0.12 4.0 19 0.72Ã— 10âˆ’1
10âˆ’1 1, 000Ã— 1, 000 50 4 0.39 33.2 17 0.89Ã— 10âˆ’1
100 3 0.57 85.2 17 1.01Ã— 10âˆ’1
10 6 0.12 0.9 3 0.52
1 1, 000Ã— 1, 000 50 4 0.39 7.8 3 0.63
100 3 0.57 34.8 3 0.69
Table 3: Simulation results for noisy data. The computational results are averaged over five
runs. For each test, the table shows the results of Algorithm 1 applied with an early stopping
criterion
is within the noise level. Suppose again that ?Ì‚? obeys (5.6), namely,
âˆ¥ğ’«Î©(?Ì‚? âˆ’ğ‘´)âˆ¥2ğ¹ â‰ ğ‘âˆ¥?Ì‚? âˆ’ğ‘´âˆ¥2ğ¹ . (5.11)
As mentioned earlier, one condition for this to happen is that ğ‘´ and ?Ì‚? have low rank. This is
the reason why it is important to stop the algorithm early as we hope to obtain a solution which
is both consistent with the data and has low rank (the limit of the SVT iterations, limğ‘˜â†’âˆğ‘¿ğ‘˜,
will not generally have low rank since there may be no low-rank matrix matching the noisy data).
From
âˆ¥ğ’«Î©(?Ì‚? âˆ’ğ‘´)âˆ¥ğ¹ â‰¤ âˆ¥ğ’«Î©(?Ì‚? âˆ’ğ‘©)âˆ¥ğ¹ + âˆ¥ğ’«Î©(ğ‘© âˆ’ğ‘´)âˆ¥ğ¹ ,
and the fact that both terms on the right-hand side are on the order of
âˆš
ğ‘šğœ2, we would have
ğ‘âˆ¥?Ì‚? âˆ’ğ‘´âˆ¥2ğ¹ = ğ‘‚(ğ‘šğœ2) by (5.11). In particular, this would give that the relative reconstruction
error is on the order of the noise ratio since âˆ¥ğ’«Î©(ğ‘´)âˆ¥2ğ¹ â‰ ğ‘âˆ¥ğ‘´âˆ¥2ğ¹â€”as observed experimentally.
5.2.2 Inequality constraints
We now examine the speed at which one can solve similar problems with inequality constraints
instead of linear equality constraints. We assume the model (5.9), where the matrix ğ‘´ of rank ğ‘Ÿ
is sampled as before.
We use the noise-aware variant with quadratic constraints (3.10)â€“(3.11). We set ğœ– to ğœ–2 =
ğœ2(ğ‘š + 2
âˆš
2ğ‘š) as this provides a likely upper bound on âˆ¥ğ’›âˆ¥ so that the true matrix ğ‘´ is in
the feasible set with high probability. The step size is as before and set to 1.2/ğ‘. As a stopping
criterion, we stop the iterations (3.12) when the quadratic constraint is very nearly satisfied; in
details, we terminate the algorithm when
âˆ¥ğ’ƒâˆ’ğ’œ(ğ‘¿ğ‘˜)âˆ¥ğ¹ â‰¤ (1 + tol) ğœ–
where tol is some small scalar, typically 0.05 so that the constraint is nearly enforced.
The experimental results are shown in Table 4. Our experiments suggest that the algorithm
(3.12) is fast, and provides statistically accurate answers since it predicts the unseen entries with an
25
tol time(s) # iters âˆ¥?Ì‚? âˆ’ğ‘´âˆ¥ğ¹ /(ğ‘›ğœ) âˆ¥?Ì‚?âˆ¥âˆ— rank(?Ì‚?)
0.25 32.8 126 1.11 9034 10
0.2 45.1 158 1.06 9119 15
0.15 94.2 192 1.04 9212 26
0.1 248 232 1.04 9308 39
0.05 447 257 1.03 9415 45
Table 4: Simulation results for the noise-aware variant (3.12), which solves (3.11). The
unknown matrix ğ‘´ is 1000 Ã— 1000 and of rank ğ‘Ÿ = 10. We get to see 6 entries per degree of
freedom; i.e. ğ‘š = 6ğ‘‘ğ‘Ÿ. The noise ratio added is 0.1. The averaged true nuclear norm is 9961.
We choose ğœ = 5ğ‘› and ğ›¿ = 1.2ğ‘âˆ’1. The computational results are averaged over five runs. The
computer here is a quad-core 2.30GHz AMD Phenom running Matlab 7.6.0 with 3 threads.
accuracy which is about equal to the standard deviation of the noise. In fact, very recent work [20]
performed after the original submission of this paper suggests that even with considerable side
information about the unknown matrix, one would not be able to do much better.
As seen in the table, although the reconstruction is accurate, the ranks of the iterates ğ‘¿ğ‘˜ seem
to increase with the iteration count ğ‘˜. This is unlike the case with equality constraints, and we
have witnessed this phenomenon in other settings as well such as in the case of linear inequality
constraints; e.g. with the iteration (3.9) for solving (3.8). Because a higher rank slows down each
iteration, it would be of interest to find methods which stabilize the rank and keep it low in general
settings. We leave this important issue for future research.
5.3 An example with real data
We conclude the numerical section by applying our algorithms to a real dataset. We downloaded
from the website [7] a matrix of geodesic distances (in miles) between 312 cities located in the
United States and Canada. The geodesic distances were computed from latitude and longitude
information, and rounded to the nearest integer. It is well known that the squared Euclidean
distance matrix is a low rank matrix. With geodesic distances, however, a numerical test suggests
that the geodesic-distance matrix ğ‘´ can be well approximated by a low-rank matrix. Indeed,
letting ğ‘´3 be the best rank-3 approximation, we have âˆ¥ğ‘´3âˆ¥ğ¹ /âˆ¥ğ‘´âˆ¥ğ¹ = 0.9933 or, equivalently,
âˆ¥ğ‘´3 âˆ’ğ‘´âˆ¥ğ¹ /âˆ¥ğ‘´âˆ¥ğ¹ = 0.1159. Now sample 30% of the entries of ğ‘´ and obtain and estimate ?Ì‚?
by the SVT algorithm and its noise aware variant (3.6). Here, we set ğœ = 107 which happens to be
about 100 times the largest singular value of ğ‘´ , and set ğ›¿ = 2. For completion, we use the SVT
algorithm and the iteration (3.6), which solves (3.8) with ğ¸ğ‘–ğ‘— = 0.01âˆ£ğ‘€ âˆ£ğ‘–ğ‘— . In Figure 2, we plot the
relative error âˆ¥ğ‘´âˆ’ğ‘¿ğ‘˜âˆ¥ğ¹ /âˆ¥ğ‘´âˆ¥ğ¹ , the relative residual error âˆ¥ğ’«Î©(ğ‘´âˆ’ğ‘¿ğ‘˜)âˆ¥ğ¹ /âˆ¥ğ’«Î©(ğ‘´)âˆ¥ğ¹ and the
error of the best approximation with the same rank. Let ğ‘˜ğ‘– be the smallest integer such that the
rank of ğ‘¿ğ‘˜ğ‘– is ğ‘– and the rank of ğ‘¿ğ‘˜ğ‘–+1 is ğ‘–+1. The computational times needed to reach the ğ‘˜ğ‘–th
iteration are shown in Table 5. This table indicates that in a few seconds and in a few iterations,
both the SVT algorithm and the iteration (3.6) give a completion, which is nearly as accurate as
the best possible low-rank approximation to the unknown matrix ğ‘´ .
26
50 100 150 200 250 300 350 400
10
âˆ’1
10
0
 
 
Relative Error
Relative Residual Error
 Best Possible Relative Error Using the same rank
50 100 150 200 250 300 350 400
10
âˆ’1
10
0
 
 
Relative Error
Relative Residual Error
 Best Possible Relative Error Using the same rank
(a) SVT (b) Noise aware variant (3.6)
50 100 150 200 250 300 350 400
0
1
2
3
4
5
k
R
an
k
 
 
SVT
DS
(c) Rank vs. iteration count
Figure 2: Computational results for the city-to-city distance dataset. (a) Plot of the recon-
struction errors from of the SVT algorithm. The blue dashed line is the relative error âˆ¥ğ‘¿ğ‘˜ âˆ’
ğ‘´âˆ¥ğ¹ /âˆ¥ğ‘´âˆ¥ğ¹ , the red dotted line is the relative residual error âˆ¥ğ’«Î©(ğ‘¿ğ‘˜ âˆ’ğ‘´)âˆ¥ğ¹ /âˆ¥ğ’«Î©(ğ‘´)âˆ¥ğ¹
and the black line is the best possible relative error achieved by truncating the SVD of ğ‘´ and
keeping a number of terms equal to the rank of ğ‘¿ğ‘˜. (b) Same as (a) but with the iteration
(3.6). (c) Rank of the successive iterates ğ‘¿ğ‘˜; the SVT algorithm is in blue and the noise aware
variant (3.6) is in red.
6 Discussion
This paper introduced a novel algorithm, namely, the singular value thresholding algorithm for
matrix completion and related nuclear norm minimization problems. This algorithm is easy to
implement and surprisingly effective both in terms of computational cost and storage requirement
when the minimum nuclear-norm solution is also the lowest-rank solution. We would like to close
this paper by discussing a few open problems and research directions related to this work.
Our algorithm exploits the fact that the sequence of iterates {ğ‘¿ğ‘˜} have low rank when the
minimum nuclear solution has low rank. An interesting question is whether one can prove (or
disprove) that in a majority of the cases, this is indeed the case.
It would be interesting to explore other ways of computing ğ’Ÿğœ (ğ’€ )â€”in words, the action of
the singular value shrinkage operator. Our approach uses the Lanczos bidiagonalization algorithm
with partial reorthogonalization which takes advantages of sparse inputs but other approaches are
27
Algorithm rank ğ‘˜ğ‘– time âˆ¥ğ‘´ âˆ’ğ‘´ğ‘–âˆ¥ğ¹ /âˆ¥ğ‘´âˆ¥ğ¹ âˆ¥ğ‘´ âˆ’ğ‘¿ğ‘˜ğ‘–âˆ¥ğ¹ /âˆ¥ğ‘´âˆ¥ğ¹
1 58 1.4 0.4091 0.4170
SVT 2 190 4.8 0.1895 0.1980
3 343 8.9 0.1159 0.1252
1 47 2.6 0.4091 0.4234
(3.6) 2 166 7.2 0.1895 0.1998
3 310 13.3 0.1159 0.1270
Table 5: Speed and accuracy of the completion of the city-to-city distance matrix. Here,
âˆ¥ğ‘´ âˆ’ğ‘´ğ‘–âˆ¥ğ¹ /âˆ¥ğ‘´âˆ¥ğ¹ is the best possible relative error achieved by a matrix of rank ğ‘–.
possible. We mention two of them.
1. A series of papers have proposed the use of randomized procedures for the approximation
of a matrix ğ’€ with a matrix ğ’ of rank ğ‘Ÿ [40, 44]. When this approximation consists of the
truncated SVD retaining the part of the expansion corresponding to singular values greater
than ğœ , this can be used to evaluate ğ’Ÿğœ (ğ’€ ). Some of these algorithms are efficient when the
input ğ’€ is sparse [44], and it would be interesting to know whether these methods are fast
and accurate enough to be used in the SVT iteration (2.7).
2. A wide range of iterative methods for computing matrix functions of the general form ğ‘“(ğ’€ )
are available today, see [36] for a survey. A valuable research direction is to investigate
whether some of these iterative methods, or other to be developed, would provide powerful
ways for computing ğ’Ÿğœ (ğ’€ ).
In practice, one would like to solve (2.8) for large values of ğœ . However, a larger value of ğœ
generally means a slower rate of convergence. A good strategy might be to start with a value of
ğœ , which is large enough so that (2.8) admits a low-rank solution, and at the same time for which
the algorithm converges rapidly. One could then use a continuation method as in [56] to increase
the value of ğœ sequentially according to a schedule ğœ0, ğœ1, . . ., and use the solution to the previous
problem with ğœ = ğœğ‘–âˆ’1 as an initial guess for the solution to the current problem with ğœ = ğœğ‘– (warm
starting). We hope to report on this in a separate paper.
Acknowledgments
J-F. C. is supported by the Wavelets and Information Processing Programme under a grant from DSTA,
Singapore. E. C. is partially supported by the Waterman Award from the National Science Foundation
and by an ONR grant N00014-08-1-0749. Z. S. is supported in part by Grant R-146-000-113-112 from the
National University of Singapore. E. C. would like to thank Benjamin Recht and Joel Tropp for fruitful
conversations related to this project, and Stephen Becker for his help in preparing the computational results
of Section 5.2.2.
References
[1] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. Low-rank matrix factorization with attributes.
Technical Report N24/06/MM, Ecole des Mines de Paris, 2006.
28
[2] ACM SIGKDD and Netflix. Proceedings of KDD Cup and Workshop, 2007. Proceedings available online
at http://www.cs.uic.edu/Ëœliub/KDD-cup-2007/proceedings.html.
[3] Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering shared structures in multiclass classification.
In Proceedings of the Twenty-fourth International Conference on Machine Learning, 2007.
[4] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Neural Information Processing
Systems, 2007.
[5] J. Bect, L. Blanc-FeÌraud, G. Aubert, and A. Chambolle, A â„“1 unified variational framework for image
restoration, in Proc. Eighth Europ. Conf. Comput. Vision, 2004.
[6] S. Boyd, and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[7] J. Burkardt, CITIES â€“ City distance datasets. http://people.sc.fsu.edu/Ëœburkardt/datasets/
cities/cities.html.
[8] J.-F. Cai, R. Chan, L. Shen, and Z. Shen. Restoration of chopped and nodded images by framelets.
SIAM J. Sci. Comput., 30(3):1205â€“1227, 2008.
[9] J.-F. Cai, R. H. Chan, and Z. Shen. A framelet-based image inpainting algorithm. Appl. Comput.
Harmon. Anal., 24(2):131â€“149, 2008.
[10] J.-F. Cai, S. Osher, and Z. Shen. Convergence of the linearized Bregman iteration for â„“1-norm mini-
mization. Math. Comp., to appear.
[11] J.-F. Cai, S. Osher, and Z. Shen. Linearized Bregman iterations for compressed sensing. Math. Comp.,
78(267):1515â€“1536, 2009.
[12] J.-F. Cai, S. Osher, and Z. Shen. Linearized Bregman iterations for frame-based image deblurring.
SIAM J. Imaging Sci., 2(1):226â€“252, 2009.
[13] E. J. CandeÌ€s, and F. Guo. New multiscale transforms, minimum total variation synthesis: Applications
to edge-preserving image reconstruction. Signal Processing, 82:1519â€“1543, 2002.
[14] E. J. CandeÌ€s and B. Recht. Exact Matrix Completion via Convex Optimization, 2008.
[15] E. J. CandeÌ€s and J. Romberg. Sparsity and incoherence in compressive sampling. Inverse Problems,
23(3):969â€“985, 2007.
[16] E. J. CandeÌ€s, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction from
highly incomplete frequency information. IEEE Trans. Inform. Theory, 52(2):489â€“509, 2006.
[17] E. J. CandeÌ€s and T. Tao. Decoding by linear programming. IEEE Trans. Inform. Theory, 51(12):4203â€“
4215, 2005.
[18] E. J. CandeÌ€s and T. Tao. Near-optimal signal recovery from random projections: universal encoding
strategies? IEEE Trans. Inform. Theory, 52(12):5406â€“5425, 2006.
[19] E. J. CandeÌ€s and T. Tao. The Dantzig selector: statistical estimation when ğ‘ is much larger than ğ‘›.
Annals of Statistics 35:2313â€“2351, 2007.
[20] E. J. CandeÌ€s and Y. Plan. Matrix completion with noise. Submitted to Proceedings of the IEEE, March
2009.
[21] A. Chai and Z. Shen. Deconvolution: A wavelet frame approach. Numer. Math., 106(4):529â€“587, 2007.
[22] R. H. Chan, T. F. Chan, L. Shen, and Z. Shen. Wavelet algorithms for high-resolution image recon-
struction. SIAM J. Sci. Comput., 24(4):1408â€“1432 (electronic), 2003.
[23] P. Chen, and D. Suter. Recovering the missing components in a large noisy low-rank matrix: application
to SFM source. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(8):1051-1063, 2004.
29
[24] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. Multiscale
Model. Simul., 4(4):1168â€“1200 (electronic), 2005.
[25] J. Darbon and S. Osher. Fast discrete optimization for sparse approximations and deconvolutions, 2007.
preprint.
[26] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems
with a sparsity constraint. Comm. Pure Appl. Math., 57(11):1413â€“1457, 2004.
[27] I. Daubechies, G. Teschke, and L. Vese. Iteratively solving linear inverse problems under general convex
constraints. Inverse Probl. Imaging, 1(1):29â€“46, 2007.
[28] D. L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289â€“1306, 2006.
[29] M. Elad, J.-L. Starck, P. Querre, and D. L. Donoho. Simultaneous cartoon and texture image inpainting
using morphological component analysis (MCA). Appl. Comput. Harmon. Anal., 19(3):340â€“358, 2005.
[30] M. J. Fadili, J.-L. Starck, and F. Murtagh. Inpainting and zooming using sparse representations. The
Computer Journal, to appear.
[31] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002.
[32] M. Fazel, H. Hindi, and S. Boyd, Log-det heuristic for matrix rank minimization with applications to
Hankel and Euclidean distance matrices. in Proc. Am. Control Conf., June 2003.
[33] M. Figueiredo, and R. Nowak, An EM algorithm for wavelet-based image restoration. IEEE Transactions
on Image Processing, 12(8):906â€“916, 2003.
[34] T. Goldstein and S. Osher. The Split Bregman Algorithm for L1 Regularized Problems, 2008. UCLA
CAM Reprots (08-29).
[35] E. T. Hale, W. Yin, and Y. Zhang. Fixed-point continuation for l1-minimization: methodology and
convergence. 2008. preprint.
[36] N. J. Higham. Functions of Matrices: Theory and Computation. Society for Industrial and Applied
Mathematics, Philadelphia, PA, USA, 2008.
[37] J.-B. Hiriart-Urruty and C. LemareÌchal. Convex analysis and minimization algorithms. I, volume 305
of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences].
Springer-Verlag, Berlin, 1993. Fundamentals.
[38] R. M. Larsen, PROPACK â€“ Software for large and sparse SVD calculations, Available from http:
//sun.stanford.edu/Ëœrmunk/PROPACK/.
[39] A. S. Lewis. The mathematics of eigenvalue optimization. Math. Program., 97(1-2, Ser. B):155â€“176,
2003. ISMP, 2003 (Copenhagen).
[40] E. Liberty, F. Woolfe, P.-G. Martinsson, V. Rokhlin, and M. Tygert. Randomized algorithms for the
low-rank approximation of matrices. Proc. Natl. Acad. Sci. USA, 104(51): 20167â€“20172, 2007.
[41] S. Lintner, and F. Malgouyres. Solving a variational image restoration model which involves â„“âˆ con-
straints. Inverse Problems, 20:815â€“831, 2004.
[42] Z. Liu, and L. Vandenberghe. Interior-point method for nuclear norm approximation with application
to system identification. submitted to Mathematical Programming, 2008.
[43] S. Ma, D. Goldfarb, and L. Chen. Fixed point and Bregman iterative methods for matrix rank mini-
mization. Preprint, 2008.
[44] P.-G. Martinsson, V. Rokhlin, and M. Tygert. A randomized algorithm for the approximation of
matrices Department of Computer Science, Yale University, New Haven, CT, Technical Report 1361,
2006.
30
[45] M. Mesbahi and G. P. Papavassilopoulos. On the rank minimization problem over a positive semidefinite
linear matrix inequality. IEEE Transactions on Automatic Control, 42(2):239â€“243, 1997.
[46] S. Osher, M. Burger, D. Goldfarb, J. Xu, and W. Yin. An iterative regularization method for total
variation-based image restoration. Multiscale Model. Simul., 4(2):460â€“489 (electronic), 2005.
[47] S. Osher, Y. Mao, B. Dong, and W. Yin. Fast Linearized Bregman Iteration for Compressed Sensing
and Sparse Denoising, 2008. UCLA CAM Reprots (08-37).
[48] B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum rank solutions of matrix equations via nuclear
norm minimization. 2007. Submitted to SIAM Review.
[49] J.-L. Starck, D. L. Donoho, and E. J. CandeÌ€s, Astronomical image representation by the curvelet
transform. Astronom. and Astrophys., 398:785â€“800, 2003.
[50] K. C. Toh, M. J. Todd, and R. H. TuÌˆtuÌˆncuÌˆ. SDPT3 â€“ a MATLAB software package for semidefinite-
quadratic-linear programming, Available from http://www.math.nus.edu.sg/Ëœmattohkc/sdpt3.html.
[51] K.-C. Toh, and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized least
squares problems. Preprint, 2009.
[52] C. Tomasi and T. Kanade. Shape and motion from image streams under orthography: a factorization
method. International Journal of Computer Vision, 9(2):137â€“154, 1992.
[53] P. Tseng. Applications of a splitting algorithm to decomposition in convex programming and variational
inequalities. SIAM J. Control Optim., 29(1):119â€“138, 1991.
[54] P. Tseng. A modified forward-backward splitting method for maximal monotone mappings. SIAM J.
Control Optim., 38:431â€“446, 2000.
[55] G. A. Watson. Characterization of the subdifferential of some matrix norms. Linear Algebra Appl.,
170:33â€“45, 1992.
[56] S. J. Wright, R. Nowak, and M. Figueiredo. Sparse reconstruction by separable approximation. Sub-
mitted for publication, 2007.
[57] W. Yin, S. Osher, D. Goldfarb, and J. Darbon. Bregman iterative algorithms for â„“1-minimization with
applications to compressed sensing. SIAM J. Imaging Sci., 1(1):143â€“168, 2008.
31

