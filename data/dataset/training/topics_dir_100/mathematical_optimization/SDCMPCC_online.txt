Math. Program., Ser. A
DOI 10.1007/s10107-013-0735-z
FULL LENGTH PAPER
First order optimality conditions for mathematical
programs with semidefinite cone complementarity
constraints
Chao Ding · Defeng Sun · Jane J. Ye
Received: 15 November 2010 / Accepted: 30 November 2013
© Springer-Verlag Berlin Heidelberg and Mathematical Optimization Society 2013
Abstract In this paper we consider a mathematical program with semidefinite cone
complementarity constraints (SDCMPCC). Such a problem is a matrix analogue of
the mathematical program with (vector) complementarity constraints (MPCC) and
includes MPCC as a special case. We first derive explicit formulas for the proximal
and limiting normal cone of the graph of the normal cone to the positive semidefinite
cone. Using these formulas and classical nonsmooth first order necessary optimality
conditions we derive explicit expressions for the strong-, Mordukhovich- and Clarke-
(S-, M- and C-)stationary conditions. Moreover we give constraint qualifications under
D. Sun’s research is supported in part by Academic Research Fund under grant R-146-000-149-112.
The research of J. J. Ye was partially supported by NSERC.
Part of this work was done while C. Ding was with Department of Mathematics, National University of
Singapore. The research of this author is supported by the National Science Foundation for Distinguished
Young Scholars of China (Grant No. 11301515).
C. Ding
National Center for Mathematics and Interdisciplinary Sciences,
Chinese Academy of Sciences, Beijing, People’s Republic of China
e-mail: dingchao@amss.ac.cn
D. Sun
Department of Mathematics and Risk Management Institute,
National University of Singapore, 10 Lower Kent Ridge Road, Singapore 119076,
Republic of Singapore
e-mail: matsundf@nus.edu.sg
J. J. Ye (B)
Department of Mathematics and Statistics, University of Victoria,
Victoria, BC V8W 3R4, Canada
e-mail: janeye@uvic.ca
123
C. Ding et al.
which a local solution of SDCMPCC is a S-, M- and C-stationary point. Moreover
we show that applying these results to MPCC produces new and weaker necessary
optimality conditions.
Keywords Mathematical program with semidefinite cone complementarity
constraints · Necessary optimality conditions · Constraint qualifications · S-stationary
conditions · M-stationary conditions · C-stationary conditions
Mathematics Subject Classification 49K10 · 49J52 · 90C30 · 90C22 · 90C33
1 Introduction
Let Sn be the linear space of all n×n real symmetric matrices equipped with the usual
Frobenius inner product 〈·, ·〉 and its induced norm ‖ · ‖. For the given positive integer
n, let Sn+ (Sn−) be the closed convex cone of all n × n positive (negative) semidefinite
matrices in Sn . Let ni , i = 1, . . . , m be given positive integers. The mathematical pro-
gram with (semidefinite) cone complementarity constraints (MPSCCC or SDCMPCC)
is defined as follows
(SDCMPCC) min f (z)
s.t. h(z) = 0,
g(z) Q 0,
Sni+  Gi (z) ⊥ Hi (z) ∈ Sni− , i = 1, . . . , m, (1)
where Z and H are two finite dimensional real Euclidean spaces; f : Z → 
, h :
Z → 
p, g : Z → H and Gi : Z → Sni , Hi : Z → Sni , i = 1, . . . , m are
continuously differentiable mappings; Q ⊆ H is a closed convex symmetric cone
with a nonempty interior (such as the nonnegative orthant, the second order cone, or
the cone of symmetric and positive semidefinite real matrices); for each i ∈ {1, . . . , m},
“Gi (z) ⊥ Hi (z)” means that the matrices Gi (z) and Hi (z) are perpendicular to each
other, i.e., 〈Gi (z), Hi (z)〉 = 0; “g(z) Q 0” means that −g(z) ∈ Q. In particular, for
a given symmetric matrix Z ∈ Sn , we use Z  0 and Z  0 to denote Z ∈ Sn− and
Z ∈ Sn+, respectively.
Our research on SDCMPCC is motivated by a number of important applications in
diverse areas. Below we describe some of them.
A rank constrained nearest correlation matrix problem. A matrix is said to be a
correlation matrix if it is real symmetric positive semidefinite and its diagonal entries
are all ones. Let C be a given matrix in Sn . Let 1 ≤ r ≤ n be a given integer. The
rank constrained nearest correlation matrix problem takes the following form
min fC (X)
s.t. Xii = 1, i = 1, . . . , n,
X ∈ Sn+,
rank(X) ≤ r,
(2)
123
First order optimality conditions
where fC : Sn → 
 is a given cost function that measures the closeness of X to
a targeted matrix C . For instance, fC can be simply chosen as 12‖X − C‖2 in some
applications. Problem (2) has many important applications in quantitative finance and
engineering, e.g., [7,8,24,28,48,56,64] and the references therein. We may easily cast
(2) in a SDCMPCC form
min
X,U
fC (X)
s.t. Xii = 1, i = 1, . . . , n,
〈I, U 〉 = r, U ∈ Sn+,
Sn+  X ⊥ (U − I ) ∈ Sn−.
(3)
We refer to [23] for details on the equivalence of these two formulations. More
SDCMPCC examples concerning the matrix rank minimization problems can be found
in [5,65].
A bilinear matrix inequality (BMI) problem. Bilinear matrix inequalities arise fre-
quently from pooling and blending problems [54], system analysis and robust design
[18,46,53]. In particular, many problems including robustness analysis [11,37] and
robust process design problems [45,54,55] can be stated as the following optimization
problem with the BMI constraint
min bT u + dT v
s.t. D +
m∑
i=1
ui A
(i) +
n∑
j=1
v j B
( j) +
m∑
i=1
n∑
j=1
uiv j C
(i j)  0, (4)
where u ∈ 
m and v ∈ 
n are decision variables, b ∈ 
m and d ∈ 
n are given,
and D, A(i), B( j), and C (i j), i = 1, . . . , m, j = 1, . . . , n are given p by p symmetric
matrices. Denote x := (u, v) ∈ 
m+n, c := (b, d) ∈ 
m+n . Then, the optimization
problem (4) can be rewritten as the following optimization problem [15]
min cT x
s.t. D +
m+n∑
i=1
xi A
(i) +
m+n∑
i, j=1
Wi j C
(i j)  0,
W = xxT ,
(5)
where A
(i) = (A(1), . . . , A(m), B(1), . . . , B(n)) and for each i, j ∈ {1, . . . , m} ×
{1, . . . , n}, C (i j) = C (i j) if i ∈ {1, . . . , m} and j ∈ {1, . . . , n} and C (i j) = 0 other-
wise. It is easy to see that the second constraint in the problem (5) can be replaced by
the following constraints [15]
Z =
[
W x
xT 1
]
 0 and rank(Z) ≤ 1.
Therefore, similarly as the previous example, we know that the problem (5) can be
cast in the following SDCMPCC form
123
C. Ding et al.
min cT x
s.t. D +
m+n∑
i=1
xi A
(i) +
m+n∑
i, j=1
Wi j C
(i j)  0,
〈I, U 〉 = 1, U ∈ Sn+,
Sn+ 
[
W x
xT 1
]
⊥ (U − I ) ∈ Sn−.
A single-firm model in electric power market with uncertain data. The electric
power market is an oligopolistic market, which means that there are several dominant
firms in this market. Each dominant firm has some number of generators, which submit
the hourly bids to an independent system operator (ISO). The firm can be thought of as
a leader of a Stackelberg game, which calculates its bids based on what it anticipates
the followers would do, which is the ISO in this case.
Without the uncertain data, it is well-known that this single-firm problem in the
electric power market can be modeled as a bilevel programming problem [21]. In
this bilevel programming model, the upper-level problem is the single firm’s profit
maximization problem and the lower-level problem is the ISO’s single spatial price
equilibrium problem. In practice it is more realistic to assume that the lower-level
problem involves uncertainty. For instance, the coefficients of the marginal demand
functions, which are decided by the information of consumers, usually contain uncer-
tainty. Therefore, it makes sense to consider a robust bilevel programming problem
where for a fixed upper-level decision variable x , the lower-level problem is replaced
by its robust counterpart:
Px : min
y
{ f (x, y, ζ ) : g(x, y, ζ ) ≤ 0 ∀ ζ ∈ U},
where U is some “uncertainty set” in the space of the data. It is well-known (see
[2,3]) that if the uncertainty set U is given by a system of linear matrix inequalities,
then the deterministic counterpart of the problem Px is a semidefinite program. If
this semidefinite programming problem can be equivalently replaced by its Karush–
Kuhn–Tucker (KKT) condition, then it yields a SDCMPCC problem.
SDCMPCC is a broad framework, which includes the mathematical program with
(vector) complementarity constraints (MPCC) as a special case. In fact, if Q ≡ 
q+, the
nonnegative orthant in H ≡ 
q and ni ≡ 1, i = 1, . . . , m, the SDCMPCC becomes
the following MPCC problem
(MPCC) min f (z)
s.t. h(z) = 0,
g(z) ≤ 0,

+  Gi (z) ⊥ Hi (z) ∈ 
−, i = 1, . . . , m. (6)
Denote G(z) = (G1(z), . . . , Gm(z))T : Z → 
m and H(z) = (H1(z), . . . , Hm(z))T :
Z → 
m . Then the constraints (6) can be replaced by the following standard vector
complementarity constraint
123
First order optimality conditions

m+  G(z) ⊥ H(z) ∈ 
m−.
MPCC is a class of very important problems since they arise frequently in applica-
tions where the constraints come from equilibrium systems and hence is also known
as the mathematical program with equilibrium constraints (MPEC); see [26,34] for
references. One of the main sources of MPCCs comes from bilevel programming
problems which have numerous applications; see [12].
In this paper, we study first order necessary optimality conditions for SDCMPCC.
For simplicity, we consider the SDCMPCC problem which has only one semidefinite
cone complementarity constraint. However all results can be generalized to the case
of more than one semidefinite cone complementarity constraints in a straightforward
manner.
MPCC is notoriously known as a difficult class of optimization problems since if
one treats a MPCC as a standard nonlinear programming problem, then Mangasarian
Fromovitz constraint qualification (MFCQ) fails to hold at each feasible point of the
feasible region; see [63, Proposition 1.1]. One of the implications of the failure of
MFCQ is that the classical KKT condition may not hold at a local optimizer. The
classical KKT condition for MPCC is known to be equivalent to the strong stationary
condition (S-stationary condition). Consequently weaker stationary conditions such
as the Mordukhovich stationary condition (M-stationary condition) and the Clarke
stationary condition (C-stationary condition) have been proposed and the constraint
qualifications under which a local minimizer is a M-(C-)stationary point have been
studied; see e.g., [47,61] for a detailed discussion.
The same difficulties exist for SDCMPCC. The cone complementarity constraint
(1) amounts to the following convex cone constraints:
〈G(z), H(z)〉 = 0, G(z) ∈ Sn+, H(z) ∈ Sn−.
For an optimization problem with convex cone constraints, the usual constraint qual-
ification is Robinson’s CQ. In this paper we show that if we consider SDCMPCC as
an optimization problem with cone constraints, Robinson’s CQ fails to hold at each
feasible point of the SDCMPCC. Hence SDCMPCC is also a difficult class of opti-
mization problems. One of the implications of the failure of Robinson’s CQ is that
the classical KKT condition may not hold at a local optimizer. It is obvious that the
complementarity constraint (1) can be reformulated as a nonconvex cone constraint:
(G(z), H(z)) ∈ gph NSn+ ,
where gph NSn+ is the graph of the normal cone to the positive semidefinite cone.
We first derive the exact expressions for the proximal and limiting normal cone of
gph NSn+ . As in the vector case, the first order necessary optimality condition based
on the proximal and limiting normal cones are called S- and M-stationary condition
respectively. To derive the C-stationary condition, we reformulate the complementarity
constraint (1) as a nonsmooth equation constraint:
G(z) − Sn+(G(z) + H(z)) = 0,
123
C. Ding et al.
where Sn+ denotes the metric projection to the positive semidefinite cone. As in
the vector case, based on this reformulation and the classical nonsmooth necessary
optimality condition we derive the necessary optimality condition in terms of the
C-stationary condition. We also show that the classical KKT condition implies the
S-stationary condition but not vice versa.
To the best of our knowledge, this is the first time explicit expressions for S-, M-
and C-stationary conditions for SDCMPCC are given. In [58], a smoothing algorithm
is given for mathematical program with symmetric cone complementarity constraints
and the convergence to C-stationary points is shown. Although the problem studied in
[58] may include our problem as a special case, there is no explicit expression for C-
stationary condition given. It is also the first time precise formulas for the proximal and
limiting normal cone of gph NSn+ are developed. In particular the precise expression for
the limiting normal cone of gph NSn+ is not only important for deriving the M-stationary
condition but also useful in the so-called Mordukhovich criterion for characterizing
the Aubin continuity [44, Theorem 9.40] of a perturbed generalized equation such as:
S(x) := {z : x ∈ H(z) + NSn+(z)}.
We organize our paper as following. In Sect. 2 we introduce the preliminaries and
preliminary results on the background in variational analysis, first order conditions
for a general problem and background in variational analysis in matrix spaces. In
Sect. 3, we give the precise expressions for the proximal and limiting normal cones of
the graph of the normal cone NSn+ . In Sect. 4, we show that if SDCMPCC is considered
as an optimization problem with convex cone constraints then Robinson’s CQ fails at
every feasible solution of SDCMPCC and derive the classical KKT condition under
the Clarke calmness condition. Explicit expressions for S-stationary conditions are
given in Sect. 5 where it is also shown that the classical KKT condition implies the
S-stationary condition. Explicit expressions for M- and C-stationary conditions are
given in Sects. 6 and 7 respectively. In Sect. 8 we reformulate MPCC as a particular
case of SDCMPCC by taking the vector complementarity functions as matrices with
diagonal values. Comparisons between the S-, M- and C-stationary points are made.
We show that the S-stationary condition for the two formulations are equivalent while
the M- and C-stationary conditions for SDCMPCC may be weaker.
2 Preliminaries and preliminary results
We first give the following notation that will be used throughout the paper. Let X and
Y be finite dimensional spaces. We denote by ‖·‖ the Euclidean norm in X . We denote
by B(x, δ) := {y ∈ X | ‖y − x‖ < δ} the open ball centered at x with radius δ > 0
and B the open unit ball centered at 0. Given a set S ⊆ X and a point x ∈ X , the
distance from x to S is denoted by
dist(x, S) := inf{‖y − x‖ | y ∈ S}.
Given a linear operator A : X → Y,A∗ denotes the adjoint of the linear operator A.
Given a matrix A, we denote by AT the transpose of the matrix A. For a mapping
123
First order optimality conditions
F : X → Y and x ∈ X, F ′(x) stands for the classical derivative or the Jacobian of
F at x and ∇F(x) the adjoint of the Jacobian. We denote by F ′(x; d) the directional
derivative of F at x in direction d. For a set-valued mapping  : X ⇒ Y , we denote
by gph the graph of , i.e., gph  := {(z, v) ∈ X × Y | v ∈ (z)}. For a set C,
we denote by int C, clC, coC its interior, closure and convex hull respectively. For a
function g : X → 
, we denote g+(x) := max{0, g(x)} and if it is vector-valued
then the maximum is taken componentwise.
• Let On be the set of all n × n orthogonal matrices.
• For any Z ∈ 
m×n , we denote by Zi j the (i, j)th entry of Z .
• For any Z ∈ 
m×n and a given index set J ⊆ {1, . . . , n}, we use ZJ to denote the
sub-matrix of Z obtained by removing all the columns of Z not in J . In particular,
we use Z j to represent the j-th column of Z , j = 1, . . . , n.
• Let I ⊆ {1, . . . , m} and J ⊆ {1, . . . , n} be two index sets. For any Z ∈ 
m×n , we
use ZIJ to denote the |I|× |J | sub-matrix of Z obtained by removing all the rows
of Z not in I and all the columns of Z not in J .
• We use “◦” to denote the Hardamard product between matrices, i.e., for any two
matrices A and B in
m×n the (i, j)th entry of Z := A◦B ∈ 
m×n is Zi j = Ai j Bi j .
• Let diag(·) : 
m → Sm be a linear mapping defined by for any x ∈ 
n , diag(x)
denotes the diagonal matrix whose i th diagonal entry is xi , i = 1, . . . , n.
2.1 Background in variational analysis
In this subsection we summarize some background materials on variational analysis
which will be used throughout the paper. Detailed discussions on these subjects can
be found in [9,10,31,32,44]. In this subsection X is a finite dimensional space.
Definition 2.1 (see e.g., [10, Proposition 1.5(a)] or [44, page 213]) Let  be a non-
empty subset of X . Given x̄ ∈ cl , the following convex cone
Nπ(x̄) := {ζ ∈ X : ∃ M > 0, such that 〈ζ, x − x̄〉 ≤ M‖x − x̄‖2 ∀ x ∈ } (7)
is called the proximal normal cone to set  at point x̄ .
Definition 2.2 (see e.g., [10, page 62 and Theorem 6.1(b)]) Let  be a nonempty
subset of X . Given x̄ ∈ cl , the following closed cone
N(x̄) :=
{
lim
i→∞ ζi : ζi ∈ N
π
(xi ), xi → x̄, xi ∈ 
}
(8)
is called the limiting normal cone (also known as Mordukhovich normal cone or basic
normal cone) to set  at point x̄ and the closed convex hull of the limiting normal
cone
N c(x̄) := clco N(x̄).
is the Clarke normal cone [9] to set  at point x̄ .
123
C. Ding et al.
Alternatively in a finite dimensional space, the limiting normal cone can be also
defined by the Fréchet (also called regular) normal cone instead of the proximal
normal cone, see [31, Definition 1.1 (ii)]. In the case when  is convex, the prox-
imal normal cone, the limiting normal cone and the Clarke normal cone coincide
with the normal cone in the sense of the convex analysis [43], i.e., N(x̄) :=
{ζ ∈ X : 〈ζ, x − x̄〉 ≤ 0 ∀ x ∈ } .
Definition 2.3 Let f : X → 
∪{+∞} be a lower semicontinuous function and finite
at x̄ ∈ X . The proximal subdifferential ([44, Definition 8.45]) of f at x̄ is defined as
∂π f (x̄) := {ζ ∈ X : ∃ σ >0, δ>0 such that f (x) ≥ f (x̄) + 〈ζ, x − x̄〉−σ‖x− x̄‖2
∀ x ∈ B(x̄, δ)}
and the limiting (Mordukhovich or basic [31]) subdifferential of f at x̄ is defined as
∂ f (x̄) :=
{
lim
k→∞ ζk : ζk ∈ ∂
π f (xk), xk → x̄, f (xk) → f (x̄)
}
.
When f is Lipschitz continuous near x̄ ,
∂c f (x̄) := co ∂ f (x̄)
is the Clarke subdifferential [9] of f at x̄ .
Note that in a finite dimensional space, alternatively the limiting subgradient can
be also constructed via Fréchet subgradients (also known as regular subgradients), see
[31, Theorem 1.89]. The equivalence of the two definitions is well-known, see the
commentary by Rockafellar and Wets [44, page 345]. In the case when f is convex
and locally Lipschitz, the proximal subdifferential, the limiting subdifferential and
the Clarke subdifferential coincide with the subdifferential in the sense of convex
analysis [43]. In the case when f is strictly differentiable, the limiting subdifferenial
and the Clarke subdifferential reduce to the classical gradient of f at x̄ , i.e., ∂c f (x̄) =
∂ f (x̄) = {∇ f (x̄)}.
2.2 First order optimality conditions for a general problem
In this subsection we discuss constraint qualifications and first order necessary opti-
mality conditions for the following general optimization problem:
(G P) min f (z)
s.t. h(z) = 0,
g(z) ≤ 0,
G(z) ∈ K ,
where Y, Z are finite dimensional spaces, K is a closed subset of Y, f : Z → 
, h :
Z → 
p, g : Z → 
q and G : Z → Y are locally Lipschitz mappings.
123
First order optimality conditions
We denote the set of feasible solutions for (GP) by F and the perturbed feasible
region by
F(r, s, P) := {z ∈ Z : h(z) + r = 0, g(z) + s ≤ 0, G(z) + P ∈ K }. (9)
Then F(0, 0, 0) = F . The following definition is the Clarke calmness [9] adapted to
our setting.
Definition 2.4 (Clarke calmness) We say that problem (GP) is (Clarke) calm at a local
optimal solution z̄ if there exist positive ε and μ such that, for all (r, s, P) in εB, for
all z ∈ (z̄ + εB) ∩ F(r, s, P), one has
f (z) − f (z̄) + μ‖(r, s, P)‖ ≥ 0.
The following equivalence is obvious.
Proposition 2.1 Problem (GP) is Clarke calm at a local optimal solution z̄ if and only
if (z̄, G(z̄)) is a local optimal solution to the penalized problem for some μ > 0:
(GP)μ min
z,X
f (z) + μ(‖h(z)‖ + ‖max{g(z), 0}‖ + ‖G(z) − X‖)
s.t. X ∈ K .
Theorem 2.1 Let z̄ be a local optimal solution of (GP). Suppose that (GP) is Clarke
calm at z̄. Then there exist λh ∈ 
p, λg ∈ 
q and G ∈ Sn such that
0 ∈ ∂ f (z̄) + ∂〈h, λh〉(z̄) + ∂〈g, λg〉(z̄) + ∂〈G,G〉(z̄),
λg ≥ 0, 〈g(z̄), λg〉 = 0 G ∈ NK (G(z̄)).
Proof The results follow from applying the limiting subdifferential version of the
generalized Lagarange multiplier rule (see e.g., Mordukhovich [32, Proposition 5.3]),
calculus rules for limiting subdifferentials in particular the chain rule in Mordukhovich
and Shao [33, Proposition 2.5 and Corollary 6.3]). 
The calmness condition involves both the constraint functions and the objective
function. It is therefore not a constraint qualification in classical sense. Indeed it is a
sufficient condition under which KKT type necessary optimality conditions hold. The
calmness condition may hold even when the weakest constraint qualification does not
hold. In practice one often uses some verifiable constraint qualifications sufficient to
the calmness condition.
Definition 2.5 (Calmness of a set-valued map) A set-valued map  : Z ⇒ Y is said
to be calm at a point (z̄, v̄) ∈ gph  if there exist a constant M > 0 and a neighborhood
U of z̄, a neighborhood V of v̄ such that
(z) ∩ V ⊆ (z̄) + M‖z − z̄‖cl B ∀ z ∈ U.
123
C. Ding et al.
Although the term “calmness” was coined in Rockafellar and Wets [44], the concept
of calmness of a set-valued map was first introduced by Ye and Ye in [62] under the term
“pseudo upper-Lipschitz continuity” which comes from the fact that it is a combination
of Aubin’s pseudo Lipschitz continuity [1] and Robinson’s upper-Lipschitz continuity
[39,40].
For recent discussion on the properties and the criterion of calmness of a set-valued
mapping, see Henrion and Outrata [19,20]. In what follows, we consider the calmness
of the perturbed feasible region F(r, s, P) at (r, s, P) = (0, 0, 0) to establish the
Clarke calmness of the problem.
The proposition below is an easy consequence of Clarke’s exact penalty principle
[9, Proposition 2.4.3] and the calmness of the perturbed feasible region of the problem.
See [60, Proposition 4.2] for a proof.
Proposition 2.2 If the objective function of (GP) is Lipschitz near z̄ ∈ F and the
perturbed feasible region of the constraint system F(r, s, P) defined as in (9) is calm
at (0, 0, 0, z̄), then the problem (GP) is Clarke calm at z̄.
From the definition it is easy to verify that the set-valued mapping F(r, s, P) is
calm at (0, 0, 0, z̄) if and only if there exist a constant M > 0 and U , a neighborhood
of z̄, such that
dist (z,F) ≤ M‖(r, s, P)‖ ∀ z ∈ U ∩ F(r, s, P).
The above property is also referred to the existence of a local error bound for the
feasible region F . Hence any results on the existence of a local error bound of the
constraint system may be used as a sufficient condition for calmness of the perturbed
feasible region (see e.g., Wu and Ye [57] for such sufficient conditions).
By virtue of Proposition 2.2, the following four constraint qualifications are stronger
than the Clarke calmness of (GP) at a local minimizer when the objective function of
the problem (GP) is Lipschitz continuous.
Proposition 2.3 Let F(r, s, P) be defined as in (9) and z̄ ∈ F . Then the set-
valued map F(r, s, P) is calm at (0, 0, 0, z̄) under one of the following constraint
qualifications:
(i) There is no singular Lagrange multiplier for problem (GP) at z̄:
{
0 ∈ ∂〈h, λh〉(z̄) + ∂〈g, λg〉(z̄) + ∂〈G,G〉(z̄),
G ∈ NK (G(z̄)), λg ≥ 0, 〈g(z̄), λg〉 = 0 ⇒ (λ
h, λg,G) = 0.
(ii) Robinson’s CQ [41] holds at z̄: h, g and G are continuously differentiable at z̄.
K is a closed convex cone with a nonempty interior. The gradients h′i (z̄)∗(i =
1, . . . , p) are linearly independent and there exists a vector d ∈ Z such that
hi (z̄)
′d = 0, i = 1, . . . , p,
gi (z̄)
′d < 0, i ∈ Ig(z̄),
G(z̄) + G ′(z̄)d ∈ int K ,
where Ig(z̄) := {i : gi (z̄) = 0} is the index of active inequality constraints.
123
First order optimality conditions
(iii) Linear Independence Constraint Qualification (LICQ) holds at z̄:
0 ∈ ∂〈h, λh〉(z̄) + ∂〈g, λg〉(z̄) + ∂〈G,G〉(z̄),G ∈ NK (G(z̄))
⇒ (λh, λg,G) = 0.
(iv) h, g and G are affine mappings and the set K is a union of finitely many polyhedral
convex sets.
Proof It is obvious that (iii) implies (i). By [6, Propositions 3.16 (ii) and 3.19 (iii)],
Robinson’s CQ (ii) is equivalent to (i) when all functions h, g, G are continuously
differentiable and K is a closed convex cone with a nonempty interior. By Mor-
dukhovich’s criteria for pseudo-Lipschitz continuity, (i) implies that the set-valued
map F(r, s, P) is pseudo-Lipschitz continuous around (r, s, P) = (0, 0, 0) (see e.g.,
[33, Theorem 6.1]) and hence calm. By Robinson [42], (iv) implies the upper-Lipschitz
continuity and hence the calmness of the set-valued map F(r, s, P) at (0, 0, 0, z̄). 
Combining Theorem 2.1 and Propositions 2.2 and 2.3, we have the following.
Theorem 2.2 Let z̄ be a local optimal solution of (GP). Suppose the problem is Clarke
calm at z̄; in particular one of the constraint qualifications in Proposition 2.3 holds.
Then the KKT condition in Theorem 2.1 holds at z̄.
2.3 Background in variational analysis in matrix spaces
Let A ∈ Sn be given. We use λ1(A) ≥ λ2(A) ≥ · · · ≥ λn(A) to denote the eigenvalues
of A (all real and counting multiplicity) arranging in nonincreasing order and use λ(A)
to denote the vector of the ordered eigenvalues of A. Denote (A) := diag(λ(A)).
Consider the eigenvalue decomposition of A, i.e., A = P(A)PT , where P ∈ On is
a corresponding orthogonal matrix of the orthonormal eigenvectors. By considering
the index sets of positive, zero, and negative eigenvalues of A, we are able to write A
in the following form
A = [ Pα Pβ Pγ
]
⎡
⎣
(A)αα 0 0
0 0 0
0 0 (A)γ γ
⎤
⎦
⎡
⎢⎣
P
T
α
P
T
β
P
T
γ
⎤
⎥⎦ . (10)
where α := {i : λi (A) > 0}, β := {i : λi (A) = 0} and γ := {i : λi (A) < 0}.
Proposition 2.4 (see e.g., [16, Theorem 2.1]) For any X ∈ Sn+ and Y ∈ Sn−,
NSn+(X) = {X∗ ∈ Sn− : 〈X, X∗〉 = 0} = {X∗ ∈ Sn− : X X∗ = 0},
NSn−(Y ) = {Y ∗ ∈ Sn+ : 〈Y, Y ∗〉 = 0} = {Y ∗ ∈ Sn+ : Y Y ∗ = 0} .
We say that X, Y ∈ Sn have a simultaneous ordered eigenvalue decomposition
provided that there exists P ∈ On such that X = P(X)PT and Y = P(Y )PT .
The following theorem is well-known and can be found in e.g., [22].
123
C. Ding et al.
Theorem 2.3 (von Neumann-Theobald) Any matrices X and Y in Sn satisfy the
inequality
〈X, Y 〉 ≤ λ(X)λ(Y ) ;
the equality holds if and only if X and Y admit a simultaneous ordered eigenvalue
decomposition.
Proposition 2.5 The graph of the set-valued map NSn+ can be written as
gph NSn+ = {(X, Y ) ∈ Sn+ × Sn− : Sn+(X + Y ) = X} (11)
= {(X, Y ) ∈ Sn+ × Sn− : Sn−(X + Y ) = Y } (12)
= {(X, Y ) ∈ Sn+ × Sn− : XY = Y X = 0, 〈X, Y 〉 = 0}. (13)
Proof Equations (11) and (12) are well-known (see [13]). Let X ∈ Sn+. Since
NSn+(X) = ∂δSn+(X), where δC is the indicate function of a set C , by [22, Theo-
rem 3], since the function δSn+(X) is an eigenvalue function, for any Y ∈ NSn+(X), X
and Y commute. Equation (13) then follows from the expression for the normal cone
in Proposition 2.4. 
From [50, Theorem 4.7] we know that the metric projection operator Sn+(·) is
directionally differentiable at any A ∈ Sn and the directional derivative of Sn+(·) at
A along direction H ∈ Sn is given by
′Sn+(A; H) = P
⎡
⎢⎣
H̃αα H̃αβ αγ ◦ H̃αγ
H̃ Tαβ S |β|+ (H̃ββ) 0
Tαγ ◦ H̃ Tαγ 0 0
⎤
⎥⎦ P
T
, (14)
where H̃ := PT H P , ◦ is the Hadamard product and
i j := max{λi (A), 0} − max{λ j (A), 0}
λi (A) − λ j (A) , i, j = 1, . . . , n, (15)
where 0/0 is defined to be 1. Since Sn+(·) is global Lipschitz continuous on Sn , it is
well-known that Sn+(·) is B(ouligand)-differentiable (c.f. [14, Definition 3.1.2]) onSn . In the following proposition, we will show that Sn+(·) is also calmly B(ouligand)-
differentiable on Sn . This result is not only of its own interest, but also is crucial for
the study of the proximal and limiting normal cone of the normal cone mapping NSn+
in the next section.
Proposition 2.6 The metric projection operator Sn+(·) is calmly B-differentiable for
any given A ∈ Sn, i.e., for Sn  H → 0,
Sn+(A + H) − Sn+(A) − ′Sn+(A; H) = O(‖H‖
2). (16)
Proof See the “Appendix”. 
123
First order optimality conditions
3 Expression of the proximal and limiting normal cones
In order to characterize the S-stationary and M-stationary conditions, we need to give
the precise expressions for the proximal and limiting normal cones of the graph of the
normal cone mapping NSn+ at any given point (X, Y ) ∈ gph NSn+ . The purpose of this
section is to provide such formulas. The result is also of independent interest.
3.1 Expression of the proximal normal cone
By using the directional derivative formula (14), Qi and Fusek [38] characterized the
Fréchet normal cone of gph NSn+ . In this subsection, we will establish the representation
of the desired proximal normal cone by using the same formula and the just proved
calmly B-differentiability of the metric projection operator. The proximal normal cone
is in general smaller than the Fréchet normal cone. For the set gph N
n+ , however, it is
well-known that the Fréchet normal cone coincides with the proximal normal cone. The
natural question to ask is that whether this statement remains true for the set gph NSn+ .
Our computations in this section give an affirmative answer, that is, the expression for
the proximal normal cone coincides with the one for the Fréchet normal cone derived
by Qi and Fusek in [38].
From Proposition 2.6, we know that for any given X∗ ∈ Sn and any fixed X ∈ Sn
there exist M1, M2 > 0 (depending on X and X∗ only) such that for any X ′ ∈ Sn
sufficiently close to X ,
〈X∗,Sn+(X ′) − Sn+(X)〉 ≤ 〈X∗,′Sn+(X; X
′ − X)〉 + M1‖X ′ − X‖2, (17)
〈X∗,Sn−(X ′) − Sn−(X)〉 ≤ 〈X∗,′Sn−(X; X
′ − X)〉 + M2‖X ′ − X‖2. (18)
Proposition 3.1 For any given (X, Y ) ∈ gph NSn+ , (X∗, Y ∗) ∈ Nπgph NSn+ (X, Y ) if
and only if (X∗, Y ∗) ∈ Sn × Sn satisfies
〈X∗,′Sn+(X + Y ; H)〉 + 〈Y
∗,′Sn−(X + Y ; H)〉 ≤ 0 ∀ H ∈ S
n . (19)
Proof “⇐” Suppose that (X∗, Y ∗) ∈ Sn × Sn is given and satisfies the condition
(19).
By Proposition 2.5, (17) and (18), we know that there exist a constant δ > 0 and a
constant M̃ > 0 such that for any (X ′, Y ′) ∈ gph NSn+ and ‖(X ′, Y ′) − (X, Y )‖ ≤ δ,
123
C. Ding et al.
〈(X∗, Y ∗), (X ′, Y ′) − (X, Y )〉
= 〈(X∗, Y ∗), (Sn+(X ′ + Y ′),Sn−(X ′ + Y ′)) − (Sn+(X + Y ),Sn−(X + Y ))〉
≤ M̃‖(X ′, Y ′) − (X, Y )‖2.
By taking M = max {M̃, ‖(X∗, Y ∗)‖/δ}, we know that for any (X ′, Y ′) ∈ gph NSn+ ,
〈(X∗, Y ∗), (X ′, Y ′) − (X, Y )〉 ≤ M‖(X ′, Y ′) − (X, Y )‖2,
which implies, by the definition of the proximal normal cone, that (X∗, Y ∗) ∈
Nπgph NSn+
(X, Y ).
“⇒” Let (X∗, Y ∗) ∈ Nπgph NSn+ (X, Y ) be given. Then there exists M > 0 such
that for any (X ′, Y ′) ∈ gph NSn+ ,
〈(X∗, Y ∗), (X ′, Y ′) − (X, Y )〉 ≤ M‖(X ′, Y ′) − (X, Y )‖2. (20)
Let H ∈ Sn be arbitrary but fixed. For any t ↓ 0, let
X ′t = Sn+(X + Y + t H) and Y ′t = Sn−(X + Y + t H).
By noting that (X ′t , Y ′t ) ∈ gph NSn+ (c.f., (11)–(12) in Proposition 2.5) and Sn+(·) and
Sn−(·) are globally Lipschitz continuous with modulus 1, we obtain from (20) that
〈X∗,′Sn+(X + Y ; H)〉 + 〈Y
∗,′Sn−(X + Y ; H)〉
≤ M lim
t↓0
1
t
(‖X ′t − X‖2 + ‖Y ′t − Y‖2) ≤ M limt↓0
1
t
(2t2‖H‖2) = 0.
Therefore, we know that (X∗, Y ∗) ∈ Sn × Sn satisfies the condition (19). The proof
is completed. 
For any given (X, Y ) ∈ gph NSn+ , let A = X+Y have the eigenvalue decomposition
(10). From (11)–(12), we know that X = Sn+(A) and Y = Sn−(A). It follows from
the directional derivative formula (14) that for any H ∈ Sn ,
′Sn−(A; H) = P
⎡
⎢⎣
0 0 (Eαγ − αγ ) ◦ H̃αγ
0 S |β|− (H̃ββ) H̃βγ
(Eαγ −αγ )T ◦ H̃ Tαγ H̃βγ H̃γ γ
⎤
⎥⎦ P
T
,
(21)
where E is a n × n matrix whose entries are all ones. Denote
1 :=
⎡
⎣
Eαα Eαβ αγ
ETαβ 0 0
Tαγ 0 0
⎤
⎦ and 2 :=
⎡
⎣
0 0 Eαγ − αγ
0 0 Eβγ
(Eαγ − αγ )T ETβγ Eγ γ
⎤
⎦ .
(22)
123
First order optimality conditions
We are now in a position to derive the precise expression of the proximal normal cone
to gph NSn+ .
Proposition 3.2 For any (X, Y ) ∈ gph NSn+ , let A = X + Y have the eigenvalue
decomposition (10). Then
Nπgph NSn+
(X, Y )
=
{
(X∗, Y ∗) ∈ Sn × Sn : 1 ◦ X̃∗ + 2 ◦ Ỹ ∗ = 0, X̃∗ββ  0 and Ỹ∗ββ  0
}
,
where X̃∗ := PT X∗P and Ỹ ∗ := PT Y ∗P.
Proof By Proposition 3.1, (X∗, Y ∗) ∈ Nπgph NSn+ (X, Y ) if and only if
〈X∗,′Sn+(A; H)〉 + 〈Y
∗,′Sn−(A; H)〉 ≤ 0 ∀ H ∈ S
n,
which, together with the directional derivative formulas (14) and (21) implies that
(X∗, Y ∗) ∈ Nπgph NSn+ (X, Y ) if and only if
〈1 ◦ X̃∗, H̃〉 + 〈2 ◦ Ỹ ∗, H̃〉 + 〈X̃∗ββ,S |β|+ (H̃ββ)〉
+〈Ỹ ∗ββ,S |β|− (H̃ββ)〉 ≤ 0 ∀ H ∈ S
n .
The conclusion of the proposition holds. 
3.2 Expression of the limiting normal cone
In this subsection, we will use the formula of the proximal normal cone Nπgph NSn+
(X, Y )
obtained in Proposition 3.2 to characterize the limiting normal cone Ngph NSn+
(X, Y ).
For any given (X, Y ) ∈ gphNSn+ , let A = X+Y have the eigenvalue decomposition
(10) and β be the index set of zero eigenvalues of A. Denote the set of all partitions of
the index set β by P(β). Let 
|β| be the set of all vectors in 
|β| whose components
being arranged in non-increasing order, i.e.,

|β| :=
{
z ∈ 
|β| : z1 ≥ · · · ≥ z|β|
}
.
For any z ∈ 
|β| , let D(z) represent the generalized first divided difference matrix for
f (t) = max{t, 0} at z, i.e.,
(D(z))i j =
⎧
⎪⎪⎨
⎪⎪⎩
max{zi , 0} − max{z j , 0}
zi − z j ∈ [0, 1] if zi  = z j ,
1 if zi = z j > 0,
0 if zi = z j ≤ 0,
i, j = 1, . . . , |β|.
(23)
123
C. Ding et al.
Denote
U|β| := { ∈ S |β| :  = lim
k→∞ D(z
k), zk → 0, zk ∈ 
|β| }. (24)
Let 1 ∈ U|β|. Then, from (23), it is easy to see that there exists a partition π(β) :=
(β+, β0, β−) ∈ P(β) such that
1 =
⎡
⎣
Eβ+β+ Eβ+β0 (1)β+β−
ETβ+β0 0 0
(1)
T
β+β− 0 0
⎤
⎦ , (25)
where each element of (1)β+β− belongs to [0, 1]. Let
2 :=
⎡
⎣
0 0 Eβ+β− − (1)β+β−
0 0 Eβ0β−
(Eβ+β− − (1)β+β−)T ETβ0β− Eβ−β−
⎤
⎦ . (26)
We first characterize the limiting normal cone Ngph NSn+
(X, Y ) for the special case
when (X, Y ) = (0, 0) and β = {1, 2, . . . , n}.
Proposition 3.3 The limiting normal cone to the graph of the limiting normal cone
mapping NSn+ at (0, 0) is given by
Ngph NSn+
(0, 0) =
⋃
Q ∈ On
1 ∈ Un
{
(U∗, V ∗) : 1 ◦ QT U∗Q + 2 ◦ QT V ∗Q = 0,
QTβ0U
∗Qβ0  0, QTβ0 V ∗Qβ0  0
}
.
(27)
Proof See the “Appendix”. 
We now characterize the limiting normal cone Ngph NSn+
(X, Y ) for any (X, Y ) ∈
gph NSn+ for the general case in the following theorem.
Theorem 3.1 For any (X, Y ) ∈ gph NSn+ , let A = X +Y have the eigenvalue decom-
position (10).
Then, (X∗, Y ∗) ∈ Ngph NSn+ (X, Y ) if and only if
X∗ = P
⎡
⎣
0 0 X̃∗αγ
0 X̃∗ββ X̃∗βγ
X̃∗γα X̃∗γβ X̃∗γ γ
⎤
⎦ PT and Y ∗ = P
⎡
⎣
Ỹ ∗αα Ỹ ∗αβ Ỹ ∗αγ
Ỹ ∗βα Ỹ ∗ββ 0
Ỹ ∗γα 0 0
⎤
⎦ PT (28)
with
(X̃∗ββ, Ỹ ∗ββ) ∈ Ngph NS|β|+ (0, 0) and αγ ◦ X̃
∗
αγ +(Eαγ −αγ ) ◦ Ỹ ∗αγ =0, (29)
123
First order optimality conditions
where  is given by (15), X̃∗ = PT X∗P, Ỹ ∗ = PT Y ∗P and
Ngph NS|β|+
(0, 0) =
⋃
Q ∈ O|β|
1 ∈ U|β|
{
(U∗, V ∗) : 1 ◦ QT U∗Q + 2 ◦ QT V ∗Q = 0,
QTβ0U
∗Qβ0  0, QTβ0 V ∗Qβ0  0
}
.
Proof See the “Appendix”. 
Remark 3.1 For any given (X, Y ) ∈ gph NSn+ , the (Mordukhovich) coderivative
D∗NSn+(X, Y ) of the normal cone to the set Sn+ can be calculated by using Theo-
rem 3.1 and the definition of coderivative, i.e., for given Y ∗ ∈ Sn ,
X∗ ∈ D∗NSn+(X, Y )(Y ∗) ⇐⇒ (X∗,−Y ∗) ∈ Ngph NSn+ (X, Y ).
Furthermore, by (11) in Proposition 2.5, we know that
gph NSn+ = {(X, Y ) ∈ Sn × Sn : L(X, Y ) ∈ gph Sn+},
where L : Sn × Sn → Sn × Sn is a linear function defined by
L(X, Y ) := (X + Y, X), (X, Y ) ∈ Sn × Sn .
By noting that the derivative of L is nonsingular and self-adjoint, we know from [30,
Theorem 6.10] that for any given (X, Y ) ∈ gph NSn+ and Y ∗ ∈ Sn ,
D∗NSn+(X, Y )(−Y ∗) = {X∗ ∈ Sn : (X∗, Y ∗) ∈ L ′(X, Y )Ngph Sn+ (X + Y, X)}.
Thus, for any given U∗ ∈ Sn, V ∗ ∈ D∗Sn+(X + Y )(U∗) if and only if there exists
(X∗, Y ∗) ∈ Ngph NSn+ (X, Y ) such that (X
∗, Y ∗) = L(V ∗,−U∗), that is,
X∗ = V ∗ − U∗ and Y ∗ = V ∗.
Note that for any given Z ∈ Sn , there exists a unique element (X, Y ) ∈ gph NSn+ such
that Z = X + Y . Hence, the coderivative of the metric projector operator Sn+(·) at
any Z ∈ Sn can also be computed by Theorem 3.1.
4 Failure of Robinson’s CQ
Since for any (G(z), H(z)) ∈ Sn+ × Sn−, by the von Neumann-Theobald theorem
(Theorem 2.3), one always has
〈G(z), H(z)〉 ≤ λ(G(z))T λ(H(z)) ≤ 0.
123
C. Ding et al.
Consequently one can rewrite the SDCMPCC problem in the following form:
(C P − SDC M PCC) min f (z)
s.t. h(z) = 0,
g(z) Q 0,
〈G(z), H(z)〉 ≥ 0,
(G(z), H(z)) ∈ Sn+ × Sn−.
Rewriting the constraints g(z) Q 0 and (G(z), H(z)) ∈ Sn+ × Sn− as the cone
constraint
(g(z), G(z), H(z)) ∈ −Q × Sn+ × Sn−,
we know that the above problem belongs to the class of general optimization problems
with a cone constraint (GP) as discussed in Sect. 2.2. Hence, the necessary optimality
condition stated in Sect. 2.2 can be applied to obtain the following classical KKT
condition.
Definition 4.1 Let z̄ be a feasible solution of SDCMPCC. We call z̄ a classical KKT
point if there exists (λh, λg, λe,G ,H ) ∈ 
p × H × 
 × Sn × Sn with λg ∈
Q, λe ≤ 0,G  0 and H  0 such that
0 = ∇ f (z̄) + h′(z̄)∗λh + g′(z̄)∗λg + λe[H ′(z̄)∗G(z̄) + G ′(z̄)∗H(z̄)]
+G ′(z̄)∗G + H ′(z̄)∗H , 〈g(z̄), λg〉 = 0, G(z̄)G = 0, H(z̄)H = 0.
Theorem 4.1 Let z̄ be a local optimal solution of SDCMPCC. Suppose that the prob-
lem CP-SDCMPCC is Clarke calm at z̄; in particular the set-valued map
F(r, s, t, P) := {z : h(z) + r = 0, g(z) + s Q 0,−〈G(z), H(z)〉
+t ≤ 0, (G(z), H(z)) + P ∈ Sn+ × Sn−} (30)
is calm at (0, 0, 0, 0, z̄). Then z̄ is a classical KKT point.
Proof By Theorem 2.2, there exists a Lagrange multiplier (λh, λe, λg, G , H ) ∈

p ×
q ×
× H × Sn × Sn with λe ≤ 0 such that
0 = ∇ f (z̄) + h′(z̄)∗λh + g′(z̄)∗λg + λe[H ′(z̄)∗G(z̄) + G ′(z̄)∗H(z̄)] + G ′(z̄)∗G
+H ′(z̄)∗H , (λg, G , H ) ∈ N−Q×Sn+×Sn−(g(z̄), G(z̄), H(z̄)).
Since Q is a symmetric cone it follows that λg ∈ Q and 〈g(z̄), λg〉 = 0. The desired
result follows from the normal cone expressions in Proposition 2.4. 
Definition 4.2 We say that (λh, λg, λe,G ,H ) ∈ 
p×H×
×Sn ×Sn with λg ∈
Q, λe ≤ 0,G  0,H  0 is a singular Lagrange multiplier for CP-SDCMPCC if
it is not equal to zero and
123
First order optimality conditions
0 = h′(z̄)∗λh + g′(z̄)∗λg + λe[H ′(z̄)∗G(z̄) + G ′(z̄)∗H(z̄)] + G ′(z̄)∗G
+H ′(z̄)∗H , 〈g(z̄), λg〉 = 0, G(z̄)G = 0, H(z̄)H = 0.
For a general optimization problem with a cone constraint such as CP-SDCMPCC,
the following Robinson’s CQ is considered to be a usual constraint qualification:
h′(z̄) is onto ( equivalently h′i (z̄)(i = 1, . . . , p) are linearly independent),
∃ d such that
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
h′i (z̄)d = 0, i = 1, . . . , p,−g(z̄) − g′(z̄)d ∈ int Q
(H ′(z̄)∗G(z̄) + G ′(z̄)∗H(z̄))d > 0,
G(z̄) + G ′(z̄)d ∈ int Sn+,
H(z̄) + H ′(z̄)d ∈ int Sn−.
It is well-known that the MFCQ never holds for MPCCs. We now show that Robin-
son’s CQ will never hold for CP-SDCMPCC.
Proposition 4.1 For CP-SDCMPCC, Robinson’s constraint qualification fails to hold
at every feasible solution of SDCMPCC.
Proof By the von Neumann-Theobald theorem, G(z)  0 and H(z)  0 implies that
〈G(z), H(z)〉 ≤ 0. Hence any feasible solution z̄ of SDCMPCC must be a solution to
the following nonlinear semidefinite program:
min −〈G(z), H(z)〉
s.t. G(z)  0, H(z)  0.
Since for this problem, f (z) = −〈G(z), H(z)〉, we have∇ f (z) = −H ′(z)∗G(z)−
G ′(z)∗H(z). By the first order necessary optimality condition, there exist λe =
1,G  0,H  0 such that
0 = −λe[H ′(z̄)∗G(z̄) + G ′(z̄)∗H(z̄)] + G ′(z̄)∗G + H ′(z̄)∗H ,
G(z̄)G = 0, H(z̄)H = 0.
Since (−λe,G ,H )  = 0, it is clear that (0, 0, 0,−λe,G ,H ) is a singular
Lagrange multiplier of CP-SDCMPCC. By [6, Propositions 3.16 (ii) and 3.19(iii)]),
a singular Lagrange multiplier exists if and only if Robinson’s CQ does not hold.
Therefore we conclude that the Robinson’s CQ does not hold at z̄ for CP-SDCMPCC.

5 S-stationary conditions
In the MPCC literature [26,59], using the so-called “piecewise programming
approach” to rewrite the feasible region as a union of branches which consist of only
ordinary equality and inequality constraints, one derives the S-stationary condition as
a necessary optimality condition for a local optimal solution under the condition that
123
C. Ding et al.
each branch has a common multiplier. Moreover it is well-known that the S-stationary
condition is equivalent to the classical KKT condition; see e.g., [17]. In this section we
introduce the concept of S-stationary condition and show that the classical KKT con-
dition implies the S-stationary condition. Unfortunately for SDCMPCC, “piecewise
programming approach” is not applicable any more. Hence the converse implication
may not be true in general.
For MPCC, the S-stationary condition is shown to be equivalent to the necessary
optimality condition of a reformulated problem involving the proximal normal cone to
the graph of the normal cone operator (see [59, Theorem 3.2]). Motivated by this fact
and the precise expression for the proximal normal cone formula in Proposition 3.2,
we introduce the concept of a S-stationary point for SDCMPCC.
Definition 5.1 Let z̄ be a feasible solution of SDCMPCC. Let A := G(z̄)+H(z̄) have
the eigenvalue decomposition (10). We say that z̄ is a S-stationary point of SDCMPCC
if there exists (λh, λg, G , H ) ∈ 
p × H × Sn × Sn such that
0 = ∇ f (z̄) + h′(z̄)∗λh + g′(z̄)∗λg + G ′(z̄)∗G + H ′(z̄)∗H , (31)
λg ∈ Q, 〈λg, g(z̄)〉 = 0, (32)
̃Gαα = 0, ̃Gαβ = 0, ̃Gβα = 0, (33)
̃Hγ γ = 0, ̃Hβγ = 0, ̃Hγβ = 0, (34)
αγ ◦ ̃Gαγ + (Eαγ − αγ ) ◦ ̃Hαγ = 0, (35)
̃Gββ  0, ̃Hββ  0, (36)
where E is a n × n matrix whose entries are all ones and  ∈ Sn is defined by (15),
and ̃G = PT G P and ̃H = PT H P .
We now show that for SDCMPCC, the classical KKT condition implies the
S-stationary condition.
Proposition 5.1 Let z̄ be a feasible solution of SDCMPCC. If z̄ is a classical KKT
point, i.e., there exists a classical Lagrange multiplier (λh, λg, λe,G ,H ) ∈ 
p ×
H ×
× Sn × Sn with λg ∈ Q, λe ≤ 0,G  0 and H  0 such that
0 = ∇ f (z̄) + h′(z̄)∗λh + g′(z̄)∗λg + λe[H ′(z̄)∗G(z̄) + G ′(z̄)∗H(z̄)] + G ′(z̄)∗G
+H ′(z̄)∗H , 〈λg, g(z̄)〉 = 0, G(z̄)G = 0, H(z̄)H = 0,
then it is also a S-stationary point.
Proof Denote  := (A). Define G := G + λe H(z̄) and H := H + λeG(z̄).
Then (31) and (32) hold. It remains to show (33)–(36). By the assumption we have
Sn+  G(z̄) ⊥ G ∈ Sn− and Sn−  H(z̄) ⊥ H ∈ Sn+.
By Theorem 2.3, we know that G(z̄) and G (H(z̄) and H ) admit a simultaneous
ordered eigenvalue decomposition, i.e., there exist two orthogonal matrices P̃, P̂ ∈ On
such that
123
First order optimality conditions
G = P̃
[
0 0
0 (G)γ ′γ ′
]
P̃T , G(z̄) = P̃
⎡
⎣
αα 0 0
0 0 0
0 0 0
⎤
⎦ P̃T
and
H = P̂
[
(H )α′α′ 0
0 0
]
P̂T , H(z̄) = P̂
⎡
⎣
0 0 0
0 0 0
0 0 γγ
⎤
⎦ P̂T ,
where α′ := {i | λi (H ) > 0} and γ ′ := {i | λi (G) < 0}. Moreover, we have
γ ′ ⊆ ᾱ and α′ ⊆ γ̄ , (37)
where ᾱ := β ∪ γ , γ̄ := α ∪ β.
On the other hand, we know that
G(z̄)=Sn+(A)= P
⎡
⎣
αα 0 0
0 0 0
0 0 0
⎤
⎦ PT and H(z̄) = Sn−(A)= P
⎡
⎣
0 0 0
0 0 0
0 0 γγ
⎤
⎦ PT .
Therefore, it is easy to check that there exist two orthogonal matrices S, T ∈ On such
that
P = P̃ S and P = P̂T,
with
S =
[
Sαα 0
0 Sᾱᾱ
]
and T =
[
Tγ̄ γ̄ 0
0 Tγ γ
]
,
where Sαα ∈ O|α|, Sᾱᾱ ∈ O|ᾱ| and Tγ̄ γ̄ ∈ O|γ̄ |, Tγ γ ∈ O|γ |. Denote
Sᾱᾱ = [S1 S2] and Tγ̄ γ̄ = [T1 T2]
with S1 ∈ 
|ᾱ|×|β|, S2 ∈ 
|ᾱ|×|γ | and T1 ∈ 
|γ̄ |×|α| and T2 ∈ 
|γ̄ |×|β|. Then, we have
̃G = PT (G + λe H(z̄))P = ST P̃T G P̃ S + λe
⎡
⎣
0 0 0
0 0 0
0 0 γγ
⎤
⎦
=
[
STαα 0
0 STᾱᾱ
] [
0 0
0 (G)ᾱᾱ
] [
Sαα 0
0 Sᾱᾱ
]
+ λe
⎡
⎣
0 0 0
0 0 0
0 0 γγ
⎤
⎦
=
⎡
⎢⎣
0 0 0
0 ST1 (
G)ᾱᾱ S1 ST1 (
G)ᾱᾱ S2
0 ST2 (
G)ᾱᾱ S1 ST2 (
G)ᾱᾱ S2 + λeγγ
⎤
⎥⎦
123
C. Ding et al.
and
̃H = PT (H + λeG(z̄))P = T T P̃T H P̃T + λe
⎡
⎣
αα 0 0
0 0 0
0 0 0
⎤
⎦
=
[
T Tγ̄ γ̄ 0
0 T Tγ γ
] [
(H )γ̄ γ̄ 0
0 0
] [
Tγ̄ γ̄ 0
0 Tγ γ
]
+ λe
⎡
⎣
αα 0 0
0 0 0
0 0 0
⎤
⎦
=
⎡
⎣
T T1 (
H )γ̄ γ̄ T1 + λeαα T T1 (H )γ̄ γ̄ T2 0
T T2 (
H )γ̄ γ̄ T1 T T2 (
H )γ̄ γ̄ T2 0
0 0 0
⎤
⎦ .
Therefore it is easy to see that (33)–(35) hold.
Since (G)ᾱᾱ  0, (H )γ̄ γ̄  0 and Sᾱᾱ, Tγ̄ γ̄ are orthogonal, we know that
STᾱᾱ(
G)ᾱᾱ Sᾱᾱ  0 and T Tγ̄ γ̄ (H )γ̄ γ̄ Tγ̄ γ̄  0.
Hence, we have
̃Gββ = ST1 (G)ᾱᾱ S1  0 and ̃Hββ = T T2 (H )γ̄ γ̄ T2  0,
which implies (36). Therefore z̄ is also a S-stationary point. 
Combining Theorem 4.1 and Proposition 5.1 we have the following necessary
optimality condition in terms of S-stationary conditions.
Corollary 5.1 Let z̄ be an optimal solution of SDCMPCC. Suppose the problem CP-
SDCMPCC is Clarke calm at z̄; in particular the set-valued map defined by (30) is
calm at (0, 0, 0, 0, z̄). Then z̄ is a S-stationary point.
6 M-stationary conditions
In this section we study the M-stationary conditon for SDCMPCC. For this purpose
rewrite the SDCMPCC as an optimization problem with a cone constraint:
(GP-SDCMPCC) min f (z)
s.t. h(z) = 0,
g(z) Q 0,
(G(z), H(z)) ∈ gph NSn+ .
Definition 6.1 Let z̄ be a feasible solution of SDCMPCC. Let A = G(z̄)+H(z̄) have
the eigenvalue decomposition (10). We say that z̄ is a M-stationary point of SDCMPCC
if there exists (λh, λg, G , H ) ∈ 
p × H × Sn × Sn such that (31)–(35) hold and
there exist Q ∈ O|β| and 1 ∈ U|β| (with a partition π(β) = (β+, β0, β−) of β and
the form (25)) such that
123
First order optimality conditions
1 ◦ QT ̃Gββ Q + 2 ◦ QT ̃Hββ Q = 0, (38)
QTβ0 ̃
G
ββ Qβ0  0, QTβ0 ̃Hββ Qβ0  0, (39)
where ̃G = PT G P , ̃H = PT H P and 2 is defined by (26).
We say that (λh, λg, G , H ) ∈ 
p × H × Sn × Sn is a singular M-multiplier
for SDCMPCC if it is not equal to zero and all conditions above hold except the term
∇ f (z̄) vanishes in (31).
The following result is on the first order necessary optimality condition of SDCM-
PCC in terms of M-stationary conditions.
Theorem 6.1 Let z̄ be a local optimal solution of SDCMPCC. Suppose that the prob-
lem GP-SDCMPCC is Clarke calm at z̄; in particular one of the following constraint
qualifications holds.
(i) There is no singular M-multiplier for problem SDCMPCC at z̄.
(ii) SDCMPCC LICQ holds at z̄: there is no nonzero (λh, λg, G , H ) ∈ 
p ×H×
Sn × Sn such that
h′(z̄)∗λh + g′(z̄)∗λg + G ′(z̄)∗G + H ′(z̄)∗H = 0,
̃Gαα = 0, ̃Gαβ = 0, ̃Gβα = 0,
̃Hγ γ = 0, ̃Hβγ = 0, ̃Hγβ = 0, (40)
αγ ◦ ̃Gαγ + (Eαγ − αγ ) ◦ ̃Hαγ = 0.
(iii) Assume that there is no inequality constraint g(z) Q 0. Assume also that Z =
X × Sn where X is a finite dimensional space and G(x, u) = u. The following
generalized equation is strongly regular in the sense of Robinson:
0 ∈ −F(x, u) + N
q×Sn+(x, u),
where F(x, u) = (h(x, u), H(x, u)).
(iv) Assume that there is no inequality constraint g(z) Q 0. Assume also that Z =
X × Sn, G(z) = u and F(x, u) = (h(x, u), H(x, u)). −F is locally strongly
monotone in u uniformly in x with modulus δ > 0, i.e., there exist neighborhood
U1 of x̄ and U2 of ū such that
〈−F(x, u) + F(x, v), u − v〉 ≥ δ‖u − v‖2 ∀ u ∈ U2 ∩ Sn+, v ∈ Sn+, x ∈ U1.
Then z̄ is a M-stationary point of SDCMPCC.
Proof Condition (ii) is obviously stronger than (i). Condition (i) is a necessary and
sufficient condition for the perturbed feasible region of the constraint system to be
pseudo Lipschitz continuous; see e.g., [33, Theorem 6.1]. See [60, Theorem 4.7] for
the proof of the implication of (iii) to (i). (iv) is a sufficient condition for (iii) and the
direct proof can be found in [62, Theorem 3.2(b)]. The desired result follows from
Theorem 2.2 and the expression of the limiting normal cone in Theorem 3.1. 
123
C. Ding et al.
Next, we give two SDCMPCC examples to illustrate the M-stationary conditions.
Note that in the first example the local solution is a M-stationary point, but not a
S-stationary point.
Example 6.1 Consider the following SDCMPCC problem
min −〈I, X〉 + 〈I, Y 〉
s.t. X + Y = 0,
Sn+  X ⊥ Y ∈ Sn−.
(41)
Since the unique feasible point of (41) is (0, 0), we know that (X∗, Y ∗) = (0, 0) is
the optimal solution of (41). Note that A = X∗ + Y ∗ = 0, which implies that
α = ∅, β = {1, . . . , n} and γ = ∅.
Without loss of generality, we may choose P = I . Therefore, by considering the
equation (31), we know that
[−I
I
]
+
[
e
e
]
+
[
G
0
]
+
[
0
H
]
=
[
0
0
]
,
which implies that
G = I − e and H = −I − e, (42)
where e ∈ Sn . Let e = I . Then, it is clear that the equation (38) holds for G = 0
and H = −2I with β+ = β = {1, . . . , n}, β0 = β− = ∅, and Q = I ∈ On .
By noting that β0 = ∅, we know that the optimal solution (X∗, Y ∗) = (0, 0) is a
M-stationary point with the multiplier (I, 0,−2I ) ∈ Sn × Sn × Sn . However, the
optimal solution (X∗, Y ∗) = (0, 0) is not a S-stationary point. In fact, we know from
(42) that if there exists some e ∈ Sn such that (36) holds, then
e  I and e  −I,
which is a contradiction.
Example 6.2 As a direct application, we characterize the M-stationary condition of the
rank constrained nearest correlation matrix problem (2). For any given feasible point
X ∈ Sn of (2), suppose that X has the eigenvalue decomposition X = P(X)PT .
It is easy to check that (X , U ) ∈ Sn × Sn is a feasible solution of (3) if and only if
U = ∑ri=1 Pi PTi (see e.g., [23,27,35,36] for details). Assume rank(X) = r̄ ≤ r .
Then, the index sets of positive, zero and negative eigenvalues of A = X + (U − I )
are given by α = {1, . . . , r̄}, β = {r̄ +1, . . . , r} and γ = {r +1, . . . , n}, respectively.
Therefore, we say that the feasible X ∈ Sn is a M-stationary point of (2), if there exist
(λh1, λ
h
2, λ
g, G , H ) ∈ 
n ×
×Sn ×Sn ×Sn , Q ∈ O|β| and 1 ∈ U|β| such that
[
0
0
]
=
[∇ fC (X)
0
]
+
[
diag(λh1)
λh2 I + λg
]
+
[
G
0
]
+
[
0
H
]
, (43)
123
First order optimality conditions
0  U ⊥ λg  0, (44)
̃Gαα = 0, ̃Gαβ = 0, ̃Gβα = 0, ̃Hγ γ = 0, ̃Hβγ = 0, ̃Hγβ = 0, (45)
λi (X)
λi (X) + 1
̃Gi j + (1 −
λi (X)
λi (X) + 1
)̃Hi j = 0, i ∈ α, j ∈ γ, (46)
1 ◦ QT ̃Gββ Q + 2 ◦ QT ̃Hββ Q = 0, (47)
QTβ0 ̃
G
ββ Qβ0  0, QTβ0 ̃Hββ Qβ0  0, (48)
where ̃G = PT G P , ̃H = PT H P and 2 is defined by (26).
Remark 6.1 SDCMPCC LICQ is the analogue of the well-known MPCC LICQ (also
called MPEC LICQ). However, we would like to remark that unlike in MPCC case, we
can only show that SDCMPCC LICQ is a constraint qualification for a M-stationary
condition instead of a S-stationary condition.
7 C-stationary conditions
In this section, we consider the C-stationary condition by reformulating SDCMPCC
as a nonsmooth problem:
(NS − SDCMPCC) min f (z)
s.t. h(z) = 0,
g(z) Q 0,
G(z) − Sn+(G(z) + H(z)) = 0.
From (11), we know that the reformulation NS-SDCMPCC is equivalent to SDCM-
PCC. As in the MPCC case, the C-stationary condition introduced below is the non-
smooth KKT condition of NS-SDCMPCC by using the Clarke subdifferential.
Definition 7.1 Let z̄ be a feasible solution of SDCMPCC. Let A = G(z̄)+H(z̄) have
the eigenvalue decomposition (10). We say that z̄ is a C-stationary point of SDCMPCC
if there exists (λh, λg, G , H ) ∈ 
p ×
q × Sn × Sn such that (31)–(35) hold and
〈̃Gββ , ̃Hββ〉 ≤ 0, (49)
where ̃G = PT G P and ̃H = PT H P . We say that (λh, λg, G , H ) ∈ 
p ×
H × Sn × Sn is a singular C-multiplier for SDCMPCC if it is not equal to zero and
all conditions above hold except the term ∇ f (z̄) vanishes in (31).
Remark 7.1 It is easy to see that as in MPCC case,
S-stationary condition ⇒ M-stationary condition ⇒ C-stationary condition.
Indeed, since the proximal normal cone is included in the limiting normal cone,
it is obvious that the S-stationary condition implies the M-stationary condition.
123
C. Ding et al.
We now show that the M-stationary condition implies the C-stationary condition.
In fact, suppose that z̄ is a M-stationary point of SDCMPCC. Then, there exists
(λh, λg, G , H ) ∈ 
p × 
q × Sn × Sn such that (31)–(35) hold and there exist
Q ∈ O|β| and 1 ∈ U|β| (with a partition π(β) = (β+, β0, β−) of β and the form (25))
such that (38) and (39) hold. Let A = G(z̄)+H(z̄) have the eigenvalue decomposition
(10). Therefore, we know that
QTβ+ ̃
G
ββ Qβ+ = 0, QTβ+ ̃Gββ Qβ− = 0, QTβ− ̃Gββ Qβ+ = 0,
QTβ− ̃
H
ββ Qβ− = 0, QTβ0 ̃Hββ Qβ− = 0, QTβ− ̃Hββ Qβ0 = 0,
which implies that
〈̃Gββ, ̃Hββ〉 = 〈QT ̃Gββ Q, QT ̃Hββ Q〉
= 2〈QTβ+ ̃Gββ Qβ− , QTβ+ ̃Hββ Qβ−〉 + 2〈QTβ0 ̃Gββ Qβ0 , QTβ0 ̃Hββ Qβ0〉.
Note that for each (i, j) ∈ β+ × β−, (1)i j ∈ [0, 1] and (2)i j = 1 − (1)i j .
Therefore, we know from (38) that
〈QTβ+ ̃Gββ Qβ− , QTβ+ ̃Hββ Qβ−〉 ≤ 0.
Finally, together with (39), we know that
〈̃Gββ, ̃Hββ〉 ≤ 0,
which implies that z̄ is also a C-stationary point of SDCMPCC.
We present the first order optimality condition of SDCMPCC in terms of
C-stationary conditions in the following result.
Theorem 7.1 Let z̄ be a local optimal solution of SDCMPCC. Suppose that the prob-
lem NS-SDCMPCC is Clarke calm at z̄; in particular suppose that there is no singular
C-multiplier for problem SDCMPCC at z̄. Then z̄ is a C-stationary point of SDCM-
PCC.
Proof By Theorem 2.2 with K = {0}, we know that there exist λh ∈ 
p, λg ∈ 
q
and  ∈ Sn such that
0 ∈ ∂ cz L(z̄, λh, λg, ), λg ≥ 0 and 〈λg, g(z̄)〉 = 0, (50)
where L(z, λh, λg, ) := f (z)+ 〈λh, h(z)〉 + 〈λg, g(z)〉 + 〈, G(z)−Sn+(G(z)+
H(z))〉.
Consider the Clarke subdifferential of the nonsmooth part S(z) := 〈,Sn+(G(z)+
H(z))〉 of L .
By the chain rule [9, Corollary pp.75], for any v ∈ Z , we have
∂ c S(z̄)v ⊆ 〈, ∂ cSn+(A)(G ′(z̄)v + H ′(z̄)v)〉.
123
First order optimality conditions
Therefore, since any element of the Clarke subdifferential of the metric projection
operator to a close convex set is self-adjoint (see e.g., [29, Proposition 1(a)]), we
know from (50) that there exists V ∈ ∂ cSn+(A) such that
∇ f (z̄) + h′(z̄)∗λh + g′(z̄)∗λg + G ′(z̄)∗ − (G ′(z̄)∗ + H ′(z̄)∗)V () = 0. (51)
Define G :=  − V () and H := −V (). Then (31)–(32) follow from (50) and
(51) immediately. By [49, Proposition 2.2], we know that there exists W ∈ ∂ cS |β|+ (0)
such that
V () = P
⎡
⎢⎣
̃αα ̃αβ αγ ◦ ̃αγ
̃Tαβ W (̃ββ) 0
̃Tαγ ◦ Tαγ 0 0
⎤
⎥⎦ P
T
,
where  ∈ Sn is defined by (15). Therefore, it is easy to see that (33)–(35) hold.
Moreover, from [29, Proposition 1(c)], we know that
〈W (̃ββ), ̃ββ − W (̃ββ)〉 ≥ 0,
which implies 〈̃Gββ , ̃Hββ〉 ≤ 0. Hence, we know z̄ is a C-stationary point of SDCM-
PCC. 
Next, we give an example whose optimal solution is a C-stationary point but not a
M-stationary point.
Example 7.1 Consider the following SDCMPCC problem
min 12 z1 − 12 z2 − z3 − 12 z4
s.t. −2z1 + z3 + z4 ≤ 0,
2z2 + z3 ≤ 0,
z24 ≤ 0,
S3+  G(z) ⊥ H(z) ∈ S3−,
(52)
where G : 
4 → S3 and H : 
4 → S3 are the linear operators defined as follows
for any z = (z1, z2, z3, z4)T ∈ 
4,
G(z) :=
⎡
⎢⎣
1 + z16 −1 + z16 − z13
−1 + z16 1 + z16 − z13
− z13 − z13 2z13
⎤
⎥⎦ and
H(z) :=
⎡
⎢⎣
z2
6 − 1 z26 − 1 − z23 − 1
z2
6 − 1 z26 − 1 − z23 − 1
− z23 − 1 − z23 − 1 2z23 − 1
⎤
⎥⎦ .
Since 〈G(z), H(z)〉 = z1z2, one can verify that z̄ = (0, 0, 0, 0) is the unique optimal
solution of the problem (52). Thus, we have
123
C. Ding et al.
A = G(z̄) + H(z̄) = P
⎡
⎣
2 0 0
0 0 0
0 0 −3
⎤
⎦ PT ,
where P is the 3 by 3 orthogonal matrix given by
P =
⎡
⎢⎣
1√
2
1√
6
1√
3
− 1√
2
1√
6
1√
3
0 −2√
6
1√
3
⎤
⎥⎦ ,
and the index sets of positive, zero and negative eigenvalues are α = {1}, β = {2}
and γ = {3}. In the following we denote by ∂G
∂z1
the derivative of the mapping G with
respect to variable z1. Since G(z) only depends on z1 and H(z) only depends on z2,
(31) can be written as
⎡
⎢⎢⎣
0
0
0
0
⎤
⎥⎥⎦ =
⎡
⎢⎢⎣
1
2− 12−1
− 12
⎤
⎥⎥⎦+
⎡
⎢⎢⎣
−2
0
1
1
⎤
⎥⎥⎦ λ
g
1 +
⎡
⎢⎢⎣
0
2
1
0
⎤
⎥⎥⎦ λ
g
2 +
⎡
⎢⎢⎣
0
0
0
0
⎤
⎥⎥⎦ λ
g
3 +
⎡
⎢⎢⎣
〈 ∂G
∂z1
, G〉
0
0
0
⎤
⎥⎥⎦
+
⎡
⎢⎢⎣
0
〈 ∂ H
∂z2
, H 〉
0
0
⎤
⎥⎥⎦ ,
for some (λg, G , H ) ∈ 
3 × S3 × S3. From the above equation and (32), we
obtain that λg1 = λg2 = 12 > 0, λg3 ≥ 0. Let ̃G = P
T
G P and ̃H = PT H P .
Let (G, H ) be such that all entries are zero except the entries (̃G22, ̃
H
22) left to be
determined. Then (33)–(35) hold and
〈
∂G
∂z1
, G
〉
=
〈
∂G
∂z1
, P̃G P
T
〉
=
〈
P
T ∂G
∂z1
P, ̃G
〉
= ̃G22.
Similarly we have 〈 ∂ H
∂z2
, H 〉 = ̃H22. Therefore we obtain ̃G22 = 12 > 0, ̃H22 = − 12 <
0. Since ̃G22̃
H
22 < 0, we know that there exists a multiplier (λ
g, G , H ) ∈ 
3 ×
S3 ×S3 such that (31)–(35) and (49) hold. Thus, the optimal solution z̄ = (0, 0, 0, 0)
is a C-stationary point. We now verify that the conditions (38) and (39) do not hold.
Since |β| = 1,O|β| = {1,−1}. Let 1 ∈ U1 and Q ∈ {1,−1}. If β0  = ∅, then
it is obvious that (39) does not hold. On the other hand if β0 = ∅ then β = β+ or
β = β−. If β = β+, then 1 = [1] and 2 = [0] and hence it is clear that the
condition (38) does not hold. Alternatively if β = β−, then 1 = [0] and 2 = [1]
and hence the condition (38) does not hold. Therefore, we know that the optimal
solution z̄ = (0, 0, 0, 0) is not a M-stationary point.
123
First order optimality conditions
8 New optimality conditions for MPCC via SDCMPCC
As we mentioned in the introduction, the vector MPCC problem (6) can be considered
as a SDCMPCC problem with m one dimensional SDP complementarity constraints.
Consequently, in this way, all the stationary conditions developed for SDCMPCC
coincide with those for MPCC.
On the other hand, the vector MPCC problem (6) can also be considered as the
following SDCMPCC with one m dimensional SDP complementarity constraint:
min f (z)
s.t. h(z) = 0,
g(z) ≤ 0,
Sm+  D(G(z)) ⊥ D(H(z)) ∈ Sm− ,
(53)
where G(z) = (G1(z), . . . , Gm(z))T : Z → 
m and H(z) = (H1(z), . . . , Hm(z))T :
Z → 
m and D : 
m → Sm is the linear operator defined by D(y) = diag(y) for
any y ∈ 
m . We now compare the resulting S-, M- and C-stationary conditions for
the two formulations. Since in this SDCMPCC reformulation the multipliers for the
matrix complementarity constraints are matrices, it may provide more flexibilities and
hence the resulting necessary optimality conditions may be weaker and more likely
to hold at an optimal solution. We now demonstrate this point.
First, consider the S-stationary condition. It is easy to see that if a feasible point
z̄ is a S-stationary point (see e.g., [47,61] for the definitions) of the original vector
MPCC problem, then z̄ is a S-stationary point of the special SDCMPCC problem
(53). We now show that the converse is also true. In fact, by the Definition 5.1, we
know that if the feasible point z̄ of (53) is a S-stationary point, then there exists
(λh, λg, G , H ) ∈ 
p ×
q × Sm × Sm such that (31)–(36) hold. In particular, we
have
0 = ∇ f (z̄) + h′(z̄)∗λh + g′(z̄)∗λg + G ′(z̄)∗D∗(G) + H ′(z̄)∗D∗(H ),
where D∗ : Sm → 
m is the adjoint of the linear operator D given by
D∗(A) = (a11, . . . , amm)T , A ∈ Sm .
Denote by ηG := D∗(G) ∈ 
m and ηH := D∗(H ) ∈ 
m . Also, since A =
D(G(z̄))+D(H(z̄)) is a diagonal matrix, we can just choose P ≡ I in the eigenvalue
decomposition (10) of A. Therefore, by (33) and (34), we have that
ηGi = 0 if Gi (z̄) > 0 and Hi (z̄) = 0,
ηHi = 0 if Gi (z̄) = 0 and Hi (z̄) < 0.
Moreover, since Gββ = ̃Gββ  0 and Hββ = ̃Hββ  0, we know that the diagonal
elements ηG and ηH satisfy
ηGi ≤ 0 and ηHi ≥ 0 if Gi (z̄) = 0 and Hi (z̄) = 0. (54)
123
C. Ding et al.
Therefore, we conclude that the feasible point z̄ is also a S-stationary point of the
original vector MPCC problem with the Lagrange multiplier (λh, λg, ηG , ηH ) ∈ 
p×

q ×
m ×
m .
For the M- and C-stationary conditions, it is easy to check that if a feasible point
z̄ is a M- (or C-)stationary point (see e.g., [47,61] for the definitions) of the original
MPCC problem, then z̄ is also a M- (or C-)stationary point of the SDCMPCC problem
(53). However, the converse may not hold. For example, consider the following vector
MPCC problem
min z1 − 258 z2 − z3 − 12 z4
s.t. z24 ≤ 0,
0 ≤ G(z) ⊥ H(z) ≤ 0,
(55)
where G : 
4 → 
2 and H : 
4 → 
2 are defined as
G(z) :=
[
6z1 − z3 − z4
z1
]
and H(z) :=
[
6z2 + z3
z2
]
, z ∈ 
4.
It is easy to see that z∗ = (0, 0, 0, 0) is the unqiue optimal solution of (55). By
considering the weakly stationary condition (see e.g., [61] for the definition) of (55),
we know that the corresponding Lagrange multiplier (λg, ηG , ηH ) ∈ 
 × 
2 × 
2
satisfies
λg ≥ 0, ηG =
[−1/2
2
]
and ηH =
[
1/2
1/8
]
.
Therefore, the optimal solution z∗ = (0, 0, 0, 0) is a weakly stationary point. However,
by noting that z∗1 = z∗2 = 0, but ηG2 > 0 and ηH2 > 0, we know that z∗ is neither the
M-stationary point nor the C-stationary point.
Next, consider the corresponding SDCMPCC problem (53), i.e.,
min z1 − 258 z2 − z3 − 12 z4
s.t. z24 ≤ 0,
S2+  D(G(z)) ⊥ D(H(z)) ∈ S2−.
(56)
We know that the Lagrange multiplier (λg, G , H ) ∈ 
 × S2 × S2 with respect to
the optimal solution z∗ satisfies
λg ≥ 0, G11 = −1/2, G22 = 2, H11 = 1/2 and H22 = 1/8.
Choose
G =
[−1/2 0
0 2
]
and H =
[
1/2 1/4
1/4 1/8
]
.
Let
Q =
[−2/√5 1/√5
−1/√5 −2/√5
]
∈ O2.
123
First order optimality conditions
Then, we have
QT G Q =
[
0 1
1 3/2
]
and QT H Q =
[
5/8 0
0 0
]
.
Conisder the partition β+ = ∅, β0 = {1}, β− = {2}. Since QTβ0G Qβ0 = 0 and
QTβ0
H Qβ0 = 5/8, we know that there exist Q ∈ O|β| and a partition π(β) =
(β+, β0, β−) of β such that the Lagrange multiplier (λg, G , H ) ∈ 
 × S2 ×
S2 satisfies (31)–(35) and (38)–(39). Therefore, although the optimal solution z∗ =
(0, 0, 0, 0) is not even a C-stationary point of the original MPCC (55), it is a M-
stationary point (also a C-stationary point) of the corresponding SDCMPCC (56).
Acknowledgments The authors are grateful to the anonymous referees for their constructive suggestions
and comments which helped to improve the presentation of the materials in this paper.
9 Appendix
Proof of Proposition 2.6 Firstly, we will show that (16) holds for the case that A =
(A). For any H ∈ Sn , denote Y := A + H . Let P ∈ On (depending on H ) be such
that
(A) + H = P(Y )PT . (57)
Let δ > 0 be any fixed number such that 0 < δ < λ|α|2 if α  = ∅ and be any fixed
positive number otherwise. Then, define the following continuous scalar function
f (t) :=
⎧
⎨
⎩
t if t > δ,
2t − δ if δ2 < t < δ,
0 if t < δ2 .
Therefore, we have
{λ1(A), . . . , λ|α|(A)} ∈ (δ,+∞) and {λ|α|+1(A), . . . , λn(A)} ∈
(
−∞, δ
2
)
.
For the scalar function f , let F : Sn → Sn be the corresponding Löwner’s operator
[25], i.e., for any Z ∈ Sn ,
F(Z) :=
n∑
i=1
f (λi (Z))ui u
T
i , (58)
where U ∈ On satisfies that Z = U(Z)U T . Since f is real analytic on the open
set (−∞, δ2 ) ∪ (δ,+∞), we know from [52, Theorem 3.1] that F is analytic at A.
Therefore, since A = (A), it is well-known (see e.g., [4, Theorem V.3.3]) that for
H sufficiently close to zero,
F(A + H) − F(A) − F ′(A)H = O(‖H‖2) (59)
123
C. Ding et al.
and
F ′(A)H =
⎡
⎢⎣
Hαα Hαβ αγ ◦ Hαγ
H Tαβ 0 0
Tαγ ◦ H Tαγ 0 0
⎤
⎥⎦ ,
where  ∈ Sn is given by (15) . Let R(·) := Sn+(·) − F(·). By the definition of f ,
we know that F(A) = A+ := Sn+(A), which implies that R(A) = 0. Meanwhile, it
is clear that the matrix valued function R is directionally differentiable at A, and from
(14), the directional derivative of R for any given direction H ∈ Sn , is given by
R′(A; H) = ′Sn+(A; H) − F
′(A)H =
⎡
⎣
0 0 0
0 S |β|+ (Hββ) 0
0 0 0
⎤
⎦ . (60)
By the Lipschitz continuity of λ(·), we know that for H sufficiently close to zero,
{λ1(Y ), . . . , λ|α|(Y )} ∈ (δ,+∞), {λ|α|+1(Y ), . . . , λ|β|(Y )} ∈
(
−∞, δ
2
)
and
{λ|β|+1(Y ), . . . , λn(Y )} ∈ (−∞, 0).
Therefore, by the definition of F , we know that for H sufficiently close to zero,
R(A + H) = Sn+(A + H) − F(A + H) = P
⎡
⎣
0 0 0
0 ((Y )ββ)+ 0
0 0 0
⎤
⎦ PT . (61)
Since P satisfies (57), we know that for any Sn  H → 0, there exists an orthogonal
matrix Q ∈ O|β| such that
Pβ =
⎡
⎣
O(‖H‖)
Pββ
O(‖H‖)
⎤
⎦ and Pββ = Q + O(‖H‖2), (62)
which was stated in [51] and was essentially proved in the derivation of Lemma 4.12
in [50]. Therefore, by noting that ((Y )ββ)+ = O(‖H‖), we obtain from (60), (61)
and (62) that
R(A + H) − R(A) − R′(A; H)
=
⎡
⎢⎣
O(‖H‖3) O(‖H‖2) O(‖H‖3)
O(‖H‖2) Pββ((Y )ββ)+PTββ − S |β|+ (Hββ) O(‖H‖
2)
O(‖H‖3) O(‖H‖2) O(‖H‖3)
⎤
⎥⎦
123
First order optimality conditions
=
⎡
⎣
0 0 0
0 Q((Y )ββ)+QT − S |β|+ (Hββ) 0
0 0 0
⎤
⎦+ O(‖H‖2).
By (57) and (62), we know that
(Y )ββ = PTβ (A)Pβ+PTβ H Pβ = PTββ Hββ Pββ+O(‖H‖2)=QT Hββ Q+O(‖H‖2).
Since Q ∈ O|β|, we have
Hββ = Q(Y )ββ QT + O(‖H‖2).
By noting that S |β|+ (·) is globally Lipschitz continuous and S |β|+ (Q(Y )ββ Q
T ) =
Q((Y )ββ)+QT , we obtain that
Q((Y )ββ)+QT − S |β|+ (Hββ)
= Q((Y )ββ)+QT − S |β|+ (Q(Y )ββ Q
T ) + O(‖H‖2)
= O(‖H‖2).
Therefore,
R(A + H) − R(A) − R′(A; H) = O(‖H‖2). (63)
By combining (59) and (63), we know that for any Sn  H → 0,
Sn+((A) + H) − Sn+((A)) − ′Sn+((A); H) = O(‖H‖
2). (64)
Next, consider the case that A = PT (A)P . Re-write (57) as
(A) + PT H P = PT P(Y )PT P.
Let H̃ := PT H P . Then, we have
Sn+(A + H) = P Sn+((A) + H̃)P
T
.
Therefore, since P ∈ On , we know from (64) and (14) that for any Sn  H → 0,
(16) holds. 
Proof of Proposition 3.3 Denote the set in the righthand side of (27) by N . We
first show that Ngph NSn+
(0, 0) ⊆ N . By the definition of the limiting normal cone
in (8), we know that (U∗, V ∗) ∈ Ngph NS|β|+ (0, 0) if and only if there exist two
sequences {(U k∗, V k∗)} converging to (U∗, V ∗) and {(U k, V k)} converging to (0, 0)
with (U k
∗
, V k
∗
) ∈ Nπgph NSn+ (U
k, V k) and (U k, V k) ∈ gph NSn+ for each k.
123
C. Ding et al.
For each k, denote Ak :=U k+V k ∈ Sn and let Ak = Pk(Ak)(Pk)T with Pk ∈On
be the eigenvalue decomposition of Ak . Then for any i ∈{1, . . . , n}, we have
lim
k→∞ λi (A
k) = 0.
Since {Pk}∞k=1 is uniformly bounded, by taking a subsequence if necessary, we may
assume that {Pk}∞k=1 converges to an orthogonal matrix Q := limk→∞Pk ∈ On . For
each k, we know that the vector λ(Ak) is an element of 
n. By taking a subsequence
if necessary, we may assume that for each k, (Ak) has the same form, i.e.,
(Ak) =
⎡
⎣
(Ak)β+β+ 0 0
0 (Ak)β0β0 0
0 0 (Ak)β−β−
⎤
⎦ ,
where β+, β0 and β− are the three index sets defined by
β+ := {i : λi (Ak) > 0}, β0 := {i : λi (Ak) = 0} and β− := {i : λi (Ak) < 0}.
Since (U k
∗
, V k
∗
) ∈ Nπgph NSn+ (U
k, V k), we know from Proposition 3.2 that for each
k, there exist
k1 =
⎡
⎢⎣
Eβ+β+ Eβ+β0 
k
β+β−
ETβ+β0 0 0
(kβ+β−)
T 0 0
⎤
⎥⎦
and
k2 =
⎡
⎣
0 0 Eβ+β− − kβ+β−
0 0 Eβ0β−
(Eβ+β− − kβ+β−)T (Eβ0β−)T Eβ−β−
⎤
⎦
such that
k1 ◦ Ũ∗k + k2 ◦ Ṽ k
∗ = 0, Ũ k∗β0β0  0 and Ṽ k
∗
β0β0
 0, (65)
where Ũ k
∗ = (Pk)T U k∗Pk , Ṽ k∗ = (Pk)T V k∗Pk and
(k)i, j = max{λi (A
k), 0} − max{λ j (Ak), 0}
λi (Ak) − λ j (Ak) ∀ (i, j) ∈ β+ × β−. (66)
Since for each k, each element of kβ+β− belongs to the interval [0, 1], by further
taking a subsequence if necessary, we may assume that the limit of {kβ+β−}∞k=1 exists.
123
First order optimality conditions
Therefore, by the definition of Un in (24), we know that
lim
k→∞
k
1 = 1 ∈ Un and limk→∞
k
2 = 2,
where 1 and 2 are given by (26). Therefore, we obtain from (65) that (U∗, V ∗) ∈ N .
The other direction, i.e., Ngph NSn+
(0, 0) ⊇ N can be proved in a similar but simpler
way to that of the second part of Theorem 3.1. We omit it here. 
Proof of Theorem 3.1 “⇒” Suppose that (X∗, Y ∗) ∈ Ngph NSn+ (X, Y ). By the defin-
ition of the limiting normal cone in (8), we know that (X∗, Y ∗) = limk→∞(Xk∗, Y k∗)
with
(Xk
∗
, Y k
∗
) ∈ Nπgph NSn+ (X
k, Y k) k = 1, 2, . . . ,
where (Xk, Y k) → (X, Y ) and (Xk, Y k) ∈ gph NSn+ . For each k, denote Ak :=
Xk +Y k and let Ak = Pk(Ak)(Pk)T be the eigenvalue decomposition of Ak . Since
(A) = limk→∞(Ak), we know that (Ak)αα $ 0, (Ak)γ γ ≺ 0 for k sufficiently
large and limk→∞(Ak)ββ = 0.
Since {Pk}∞k=1 is uniformly bounded, by taking a subsequence if necessary, we
may assume that {Pk}∞k=1 converges to an orthogonal matrix P̂ ∈ On(A). We can
write P̂ = [Pα Pβ Q Pγ
]
, where Q ∈ O|β| can be any |β| × |β| orthogonal matrix.
By further taking a subsequence if necessary, we may also assume that there exists a
partition π(β) = (β+, β0, β−) of β such that for each k,
λi (A
k) > 0 ∀ i ∈ β+, λi (Ak) = 0 ∀ i ∈ β0 and λi (Ak) < 0 ∀ i ∈ β−.
This implies that for each k,
{i : λi (Ak) > 0}=α ∪ β+, {i : λi (Ak) = 0}=β0 and {i : λi (Ak) < 0}=β− ∪ γ.
Then, for each k, since (Xk
∗
, Y k
∗
) ∈ Nπgph NSn+ (X
k, Y k), we know from Proposition
3.2 that there exist
k1 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
Eαα Eαβ+ Eαβ0 
k
αβ− 
k
αγ
ETαβ+ Eβ+β+ Eβ+β0 
k
β+β− 
k
β+γ
ETαβ0 E
T
β+β0 0 0 0
kαβ−
T
kβ+β−
T
0 0 0
kαγ
T
kβ+γ
T
0 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
123
C. Ding et al.
and
k2 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0 0 0 Eαβ− − kαβ− Eαγ − kαγ
0 0 0 Eβ+β− − kβ+β− Eβ+γ − kβ+γ
0 0 0 Eβ0β− Eβ0γ
(Eαβ− − kαβ−)T (Eβ+β− − kβ+β− )T ETβ0β− Eβ−β− Eβ−γ
(Eαγ − kαγ )T (Eβ+γ − kβ+γ )T ETβ0γ ETβ−γ Eγ γ
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
such that
k1 ◦ X̃ k
∗ + k2 ◦ Ỹ k
∗ = 0, X̃ k∗β0β0  0 and Ỹ k
∗
β0β0
 0, (67)
where X̃ k
∗ = (Pk)T Xk∗Pk, Ỹ k∗ = (Pk)T Y k∗Pk and
(k)i, j = max{λi (A
k), 0} − max{λ j (Ak), 0}
λi (Ak) − λ j (Ak) ∀ (i, j) ∈ (α∪β+)×(β−∪γ ). (68)
By taking limits as k → ∞, we obtain that
X̃ k
∗ → P̂T X∗ P̂ =
⎡
⎢⎢⎣
X̃∗αα X̃∗αβ Q X̃∗αγ
(X̃∗αβ Q)T QT X̃∗ββ Q QT X̃∗βγ
(X̃∗αγ )T (QT X̃∗βγ )T X̃γ γ
⎤
⎥⎥⎦
and
Ỹ k
∗ → P̂T Y ∗ P̂ =
⎡
⎢⎢⎣
Ỹ ∗αα Ỹ ∗αβ Q Ỹ ∗αγ
(Ỹ ∗αβ Q)T QT Ỹ ∗ββ Q QT Ỹ ∗βγ
(Ỹ ∗αγ )T (QT Ỹ ∗βγ )T Ỹγ γ
⎤
⎥⎥⎦ .
By simple calculations, we obtain from (68) that
lim
k→∞
k
αβ− = Eαβ− , limk→∞
k
β+γ = 0 and limk→∞
k
αγ = αγ .
This, together with the definition of U|β|, shows that there exist 1 ∈ U|β| and the
corresponding 2 such that
lim
k→∞
k
1 =
⎡
⎣
Eαα Eαβ αγ
Eβα 1 0
Tαγ 0 0
⎤
⎦ = 1 +
⎡
⎣
0 0 0
0 1 0
0 0 0
⎤
⎦
and
lim
k→∞
k
2 =
⎡
⎣
0 0 Eαγ − αγ
0 2 Eβγ
(Eαγ − αγ )T Eγβ Eγ γ
⎤
⎦ = 2 +
⎡
⎣
0 0 0
0 2 0
0 0 0
⎤
⎦ ,
123
First order optimality conditions
where 1 and 2 are given by (22). Meanwhile, since Q ∈ O|β|, by taking limits in
(67) as k → ∞, we obtain that
1 ◦ X̃∗ + 2 ◦ Ỹ ∗ = 0, 1 ◦ QT X̃∗ββ Q + 2 ◦ QT Ỹ ∗ββ Q = 0 (69)
and
QTβ0 X̃
∗
ββ Qβ0  0 and QTβ0 Ỹ ∗ββ Qβ0  0.
Hence, by Proposition 3.3, we conclude that (X̃∗ββ, Ỹ ∗ββ) ∈ Ngph NS|β|+ (0, 0). From
(69), it is easy to check that (X∗, Y ∗) satisfies the conditions (28) and (29).
“⇐” Let (X∗, Y ∗) satisfies (28) and (29). We shall show that there exist two
sequences {(Xk, Y k)} converging to (X, Y ) and {(Xk∗, Y k∗)} converging to (X∗, Y ∗)
with (Xk, Y k) ∈ gph NSn+ and (Xk
∗
, Y k
∗
) ∈ Nπgph NSn+ (X
k, Y k) for each k.
Since (X̃∗ββ, Ỹ ∗ββ) ∈ Ngph NS|β|+ (0, 0), by Proposition 3.3, we know that there exist
an orthogonal matrix Q ∈ O|β| and 1 ∈ U|β| such that
1 ◦ QT X̃∗ββ Q + 2 ◦ QT Ỹ ∗ββ Q = 0, QTβ0 X̃∗ββ Qβ0  0 and QTβ0 Ỹ ∗ββ Qβ0  0.
(70)
Since 1 ∈ U|β|, we know that there exists a sequence {zk} ∈ 
|β| converging to 0
such that 1 = limk→∞D(zk). Without loss of generality, we can assume that there
exists a partition π(β) = (β+, β0, β−) ∈ P(β) such that for all k,
zki > 0 ∀ i ∈ β+, zki = 0 ∀ i ∈ β0 and zki < 0 ∀ i ∈ β−.
For each k, let
Xk = P̂
⎡
⎢⎢⎢⎢⎣
(A)αα 0 0 0 0
0 (zk)+ 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
⎤
⎥⎥⎥⎥⎦
P̂T and Y k = P̂
⎡
⎢⎢⎢⎢⎣
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 (zk)− 0
0 0 0 0 (A)γ γ
⎤
⎥⎥⎥⎥⎦
P̂T,
where P̂ = [Pα Pβ Q Pγ
] ∈ On(A). Then, it is clear that {(Xk , Y k)} ∈ gph NSn+ converging
to (X, Y ). For each k, denote
Ak = Xk + Y k , k1 =
⎡
⎢⎢⎢⎢⎢⎢⎣
Eαα Eαβ+ Eαβ0 
k
αβ− αγ
ETαβ+ Eβ+β+ Eβ+β0 
k
β+β− 
k
β+γ
ETαβ0 E
T
β+β0 0 0 0
(kαβ−)
T (kβ+β−)
T 0 0 0
(αγ )
T (kβ+γ )
T 0 0 0
⎤
⎥⎥⎥⎥⎥⎥⎦
123
C. Ding et al.
and
k2 =
⎡
⎢⎢⎢⎢⎢⎣
0 0 0 Eαβ− − kαβ− Eαγ − αγ
0 0 0 Eβ+β− − kβ+β− Eβ+γ − kβ+γ
0 0 0 Eβ0β− Eβ0γ
(Eαβ− − kαβ−)T (Eβ+β− − kβ+β− )T ETβ0β− Eβ−β− Eβ−γ
(Eαγ − αγ )T (Eβ+γ − kβ+γ )T ETβ0γ ETβ−γ Eγ γ
⎤
⎥⎥⎥⎥⎥⎦
,
where
(k)i, j = max{λi (A
k)), 0} − max{λ j (Ak)), 0}
λi (Ak) − λ j (Ak) ∀ (i, j) ∈ (α ∪ β+) × (β− ∪ γ ).
Next, for each k, we define two matrices X̂ k
∗
, Ŷ k
∗ ∈ Sn . Let i, j ∈ {1, . . . , n}. If
(i, j) and ( j, i) /∈ (α × β−) ∪ (β+ × γ ) ∪ (β × β). We define
X̂ k
∗
i, j ≡ X̃∗i, j , Ŷ k
∗
i, j ≡ Ỹ ∗i, j , k = 1, 2, . . . . (71)
Otherwise, denote ck := (k)i, j , k = 1, 2, . . .. We consider the following four cases.
Case 1 (i, j) or ( j, i) ∈ α × β−. In this case, we know from (28) that X̃∗i, j = 0. Since
ck  = 0 for all k and ck → 1 as k → ∞, we define
Ŷ k
∗
i, j ≡ Ỹ ∗i, j and X̂ k
∗
i, j =
ck − 1
ck
Ŷ k
∗
i, j , k = 1, 2, . . . . (72)
Then, we have
ck X̂k
∗
i, j + (1 − ck)Ŷ k
∗
i, j = 0 ∀ k and (X̂ k
∗
i, j , Ŷ
k∗
i, j ) → (X̃∗i, j , Ỹ ∗i, j ) as k → ∞.
Case 2 (i, j) or ( j, i) ∈ β+ × γ . In this case, we know from (28) that Ỹ ∗i, j = 0. Since
ck  = 1 for all k and ck → 0 as k → ∞, we define
X̂ k
∗
i, j ≡ X̃∗i, j and Ŷ k
∗
i, j =
ck
ck − 1 X̂
k∗
i, j , k = 1, 2, . . . . (73)
Then, we know that
ck X̂k
∗
i, j + (1 − ck)Ŷ k
∗
i, j = 0 ∀ k and (X̂ k
∗
i, j , Ŷ
k∗
i, j ) → (X̃∗i, j , Ỹ ∗i, j ) as k → ∞.
Case 3 (i, j) or ( j, i) ∈ (β × β)\(β+ × β−). In this case, we define
X̂ k
∗
i, j ≡ QTi X̃∗ββ Q j , Ŷ k
∗
i, j ≡ QTi Ỹ ∗ββ Q j , k = 1, 2, . . . . (74)
Case 4 (i, j) or ( j, i) ∈ β+ × β−. Since c ∈ [0, 1], we consider the following two
sub-cases:
123
First order optimality conditions
Case 4.1 c  = 1. Since ck  = 1 for all k large enough, we define
X̂ k
∗
i, j ≡ QTi X̃∗ββ Q j and Ŷ k
∗
i, j =
ck
ck − 1 X̂
k∗
i, j , k = 1, 2, . . . . (75)
Then, from (70), we know that
Ŷ k
∗
i, j →
c
c − 1 Q
T
i X̃
∗
ββ Q j = QTi Ỹ ∗ββ Q j as k → ∞.
Case 4.2 c = 1. Since ck  = 0 for all k large enough, we define
Ŷ k
∗
i, j ≡ QTi Ỹ ∗ββ Q j and X̂ k
∗
i, j =
ck − 1
ck
Ŷ k
∗
i, j , k = 1, 2, . . . . (76)
Then, again from (70), we know that
X̂ k
∗
i, j →
c − 1
c
QTi Ỹ
∗
ββ Q j = QTi X̃∗ββ Q j as k → ∞.
For each k, define Xk
∗ = P̂ X̂ k∗ P̂T and Y k∗ = P̂Ŷ k∗ P̂T . Then, from (71)–(76)
we obtain that
k1 ◦ P̂T Xk∗ P̂ + k2 ◦ P̂T Y k∗ P̂ = 0, k = 1, 2, . . . .
and
(P̂T Xk
∗
P̂, P̂T Y k
∗
P̂) → (P̂T X∗ P̂, P̂T Y ∗ P̂) as k → ∞. (77)
Moreover, from (74) and (70), we have
QTβ0 X̃
k∗
ββ Qβ0 ≡ QTβ0 X̃∗ββ Qβ0  0 and QTβ0 Ỹ k
∗
ββ Qβ0 ≡ QTβ0 Ỹ ∗ββ Qβ0  0,
k = 1, 2, . . . .
From Proposition 3.2 and (77), we know that
(Xk
∗
, Y k
∗
) ∈ Nπgph NSn+ (X
k, Y k) and (X∗, Y ∗) = lim
k→∞(X
k∗, Y k∗).
Hence, the assertion of the theorem follows.
References
1. Aubin, J.-P.: Lipschitz behavior of solutions to convex minimization problems. Math. Oper. Res. 9,
87–111 (1984)
2. Ben-Tal, A., Nemirovski, A.: Robust convex optimization. Math. Oper. Res. 23, 769–805 (1998)
3. Ben-Tal, A., Nemirovski, A.: Robust convex optimization-methodology and applications. Math. Pro-
gram. 92, 453–480 (2002)
4. Bhatia, R.: Matrix Analysis. Springer, New York (1997)
5. Bi, S., Han, L., Pan, S.: Approximation of rank function and its application to the nearest low-rank
correlation matrix. J. Glob. Optim. 57, 1113–1137 (2013)
123
C. Ding et al.
6. Bonnans, J.F., Shapiro, A.: Perturbation Analysis of Optimization Problems. Springer, New York
(2000)
7. Brigo, D., Mercurio, F.: Calibrating LIBOR. Risk Mag. 15, 117–122 (2002)
8. Burge, J.P., Luenberger, D.G., Wenger, D.L.: Estimation of structured covariance matrices. Proc. IEEE
70, 963–974 (1982)
9. Clarke, F.H.: Optimization and Nonsmooth Analysis. Wiley-Interscience, New York (1983)
10. Clarke, F.H., Ledyaev, Yu.S, Stern, R.J., Wolenski, P.R.: Nonsmooth Analysis and Control Theory.
Springer, New York (1998)
11. de Gaston, R.R.E., Safonov, M.G.: Exact calculation of the multiloop stability margin. IEEE Trans.
Autom. Control 33, 156–171 (1988)
12. Dempe, S.: Foundations of Bilevel Programming. Kluwer, Berlin (2002)
13. Eaves, B.C.: On the basic theorem for complementarity. Math. Program. 1, 68–75 (1971)
14. Faccchinei, F., Pang, J.S.: Finite-Dimensional Variational Inequalities and Complementarity Problem.
Springer, New York (2003)
15. Fazel, M.: Matrix Rank Minimization with Applications. PhD thesis, Stanford University (2002)
16. Fletcher, R.: Semi-definite matrix constraints in optimization. SIAM J. Control Optim. 23, 493–513
(1985)
17. Flegel, M.L., Kanzow, C.: On the Guignard constraint qualification for mathematical programs with
equilibrium constraints. Optimization 54, 517–534 (2005)
18. Goh, K.C., Ly, J.C., Safonov, M.G., Papavassilopoulos, G., Turan, L.: Biaffine matrix inequality prop-
erties and computational methods. In: Proceeding of the American Control Conference, Baltimore,
Maryland, pp. 850–855 (1994)
19. Henrion, R., Outrata, J.: On the calmness of a class of multifunctions. SIAM J. Optim. 13, 603–618
(2002)
20. Henrion, R., Outrata, J.: Calmness of constraint systems with applications. Math. Program. Ser. B 104,
437–464 (2005)
21. Hobbs, B.F., Metzler, C.B., Pang, J.S.: Strategic gaming analysis for electric power systems: an MPEC
approach. IEEE Trans. Power Syst. 15, 638–645 (2000)
22. Lewis, A.S.: Nonsmooth analysis of eigenvalues. Math. Program. 84, 1–24 (1999)
23. Li, Q.N., Qi, H.D.: A sequential semismooth Newton method for the nearest low-rank correlation
matrix problem. SIAM J. Optim. 21, 1641–1666 (2011)
24. Lillo, F., Mantegna, R.N.: Spectral density of the correlation matrix of factor models: a random matrix
theory approach. Phys. Rev. E 72, 016219-1–016219-10 (2005)
25. Löwner, K.: Über monotone matrixfunktionen. Mathematische Zeitschrift 38, 177–216 (1934)
26. Luo, Z.Q., Pang, J.S., Ralph, D.: Mathematical Programs with Equilibrium Constraints. Cambridge
University Press, Cambridge (1996)
27. Hiriart-Urruty, J.-B., Ye, D.: Sensitivity analysis of all eigenvalues of a symmetric matrix. Numerische
Mathematik 70, 45–72 (1995)
28. Hoge, W.: A subspace identification extension to the phase correlation method. IEEE Trans. Med.
Imaging 22, 277–280 (2003)
29. Meng, F., Sun, D.F., Zhao, G.Y.: Semismoothness of solutions to generalized equations and Moreau-
Yosida regularization. Mathe. Program. 104, 561–581 (2005)
30. Mordukhovich, B.S.: Generalized differential calculus for nonsmooth and set-valued mappings. J.
Math. Anal. Appl. 183, 250–288 (1994)
31. Mordukhovich, B.S.: Variational Analysis and Generalized Differentiation, I: Basic Theory,
Grundlehren Series (Fundamental Principles of Mathematical Sciences), vol. 330. Springer, Berlin
(2006)
32. Mordukhovich, B.S.: Variational Analysis and Generalized Differentiation, II: Applications,
Grundlehren Series (Fundamental Principles of Mathematical Sciences), vol. 331. Springer, Berlin
(2006)
33. Mordukhovich, B.S., Shao, Y.: Nonsmooth sequential analysis in Asplund space. Trans. Am. Math.
Soc. 348, 215–220 (1996)
34. Outrata, J.V., Koc̆vara, M., Zowe, J.: Nonsmooth Approach to Optimization Problem with Equilibrium
Constraints: Theory, Application and Numerical Results. Kluwer, Dordrecht (1998)
35. Overton, M., Womersley, R.S.: On the sum of the largest eigenvalues of a symmetric matrix. SIAM J.
Matrix Anal. Appl. 13, 41–45 (1992)
123
First order optimality conditions
36. Overton, M., Womersley, R.S.: Optimality conditions and duality theory for minimizing sums of the
largest eigenvalues of symmetric matrices. Math. Program. 62, 321–357 (1993)
37. Psarris, P., Floudas, C.A.: Robust stability analysis of linear and nonlinear systems with real parameter
uncertainty. AIChE Annual Meeting, p. 127e. Florida, Miami Beach (1992)
38. Qi, H.D., Fusek, P.: Metric regularity and strong regularity in linear and nonlinear semidefinite pro-
gramming. Technical Report, School of Mathematics, University of Southampton (2007)
39. Robinson, S.M.: Stability theory for systems of inequalities, part I: linear systems. SIAM J. Numer.
Anal. 12, 754–769 (1975)
40. Robinson, S.M.: Stability theory for systems of inequalities, part II: nonlinear systems. SIAM J. Numer.
Anal. 13, 473–513 (1976)
41. Robinson, S.M.: First order conditions for general nonlinear optimization. SIAM J. Appl. Math. 30,
597–607 (1976)
42. Robinson, S.M.: Some continuity properties of polyhedral multifunctions. Math. Program. Stud. 14,
206–214 (1981)
43. Rockafellar, R.T.: Convex Analysis. Princeton University Press, Princeton (1970)
44. Rockafellar, R.T., Wets, R.J.-B.: Variational Analysis. Springer, Berlin (1998)
45. Ryoo, H.S., Sahinidis, N.V.: Global optimization of nonconvex NLPs and MINLPs with applications
in process design. Comput. Chem. Eng. 19, 551–566 (1995)
46. Safonov, M.G., Goh, K.C., Ly, J.H.: Control system synthesis via bilinear matrix inequalities. In:
Proceeding of the American Control Conference, pp. 45–49. Baltimore, Maryland (1994)
47. Scheel, H., Scholtes, S.: Mathematical programs with complementarity constraints: stationarity, opti-
mality and sensitivity. Math. Oper. Res. 25, 1–22 (2000)
48. Simon, D.: Reduced order Kalman filtering without model reduction. Control Intell. Syst. 35, 169–174
(2007)
49. Sun, D.F.: The strong second-order sufficient condition and constraint nondegeneracy in nonlinear
semidefinite programming and their implications. Math. Oper. Res. 31, 761–776 (2006)
50. Sun, D.F., Sun, J.: Semismooth matrix valued functions. Math. Oper. Res. 27, 150–169 (2002)
51. Sun, D.F., Sun, J.: Strong semismoothness of eigenvalues of symmetric matrices and its applications
in inverse eigenvalue problems. SIAM J. Numer. Anal. 40, 2352–2367 (2003)
52. Tsing, N.K., Fan, M.K.H., Verriest, E.I.: On analyticity of functions involving eigenvalues. Linear
Algebra Appl. 207, 159–180 (1994)
53. VanAntwerp, J.G., Braatz, R.D., Sahinidis, N.V.: Globally optimal robust control for systems with
nonlinear time-varying perturbations. Comput. Chem. Eng. 21, S125–S130 (1997)
54. Visweswaran, V., Floudas, C.A.: A global optimization algorithm (GOP) for certain classes of non-
convex NLPs—I. Theory. Comput. Chem. Eng. 14, 1397–1417 (1990)
55. Visweswaran, V., Floudas, C.A.: A global optimization algorithm (GOP) for certain classes of noncon-
vex NLPs—II. Application of theory and test problems. Comput. Chem. Eng. 14, 1419–1434 (1990)
56. Wu, L.X.: Fast at-the-money calibration of the LIBOR market model using Lagrange multipliers. J
Comput. Financ. 6, 39–77 (2003)
57. Wu, Z., Ye, J.J.: First and second order condition for error bounds. SIAM J. Optim. 14, 621–645 (2003)
58. Yan, T., Fukushima, M.: Smoothing method for mathematical programs with symmetric cone comple-
mentarity constraints. Optimization 60, 113–128 (2011)
59. Ye, J.J.: Optimality conditions for optimization problems with complementarity constraints. SIAM J.
Optim. 9, 374–387 (1999)
60. Ye, J.J.: Constraint qualifications and necessary optimality conditions for optimization problems with
variational inequality constraints. SIAM J. Optim. 10, 943–962 (2000)
61. Ye, J.J.: Necessary and sufficient optimality conditions for mathematical programs with equilibrium
constraints. J. Math. Anal. Appl. 307, 305–369 (2005)
62. Ye, J.J., Ye, X.Y.: Necessary optimality conditions for optimization problems with variational inequality
constraints. Math. Oper. Res. 22, 977–977 (1997)
63. Ye, J.J., Zhu, D.L., Zhu, Q.J.: Exact penalization and necessary optimality conditions for generalized
bilevel programming problems. SIAM J. Optim. 7, 481–507 (1997)
64. Zhang, Z.Y., Wu, L.X.: Optimal low-rank approximation to a correlation matrix. Linear Algebra Appl.
364, 161–187 (2003)
65. Zhao, Y.B.: An approximation theory of matrix rank minimization and its application to quadratic
equations. Linear Algebra Appl. 437, 77–93 (2012)
123

