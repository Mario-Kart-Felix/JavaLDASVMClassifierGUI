Network Optimization:
Continuous and Discrete Models
Dimitri P. Bertsekas
Massachusetts Institute of Technology
WWW site for book information and orders
http://www.athenasc.com
Athena Scientific, Belmont, Massachusetts
Athena Scientific
Post Office Box 391
Belmont, Mass. 02178-9998
U.S.A.
Email: info@athenasc.com
WWW: http://www.athenasc.com
Cover Design: Ann Gallager
c© 1998 Dimitri P. Bertsekas
All rights reserved. No part of this book may be reproduced in any form
by any electronic or mechanical means (including photocopying, recording,
or information storage and retrieval) without permission in writing from
the publisher.
Publisher’s Cataloging-in-Publication Data
Bertsekas, Dimitri P.
Network Optimization: Continuous and Discrete Models
Includes bibliographical references and index
1. Network analysis (Planning). 2. Mathematical Optimization. I. Title.
T57.85.B44 1998 658.4’032-dc20 98-70298
ISBN 1-886529-02-7
ABOUT THE AUTHOR
Dimitri Bertsekas studied Mechanical and Electrical Engineering at
the National Technical University of Athens, Greece, and obtained his
Ph.D. in system science from the Massachusetts Institute of Technology.
He has held faculty positions at Stanford University and the Uni-
versity of Illinois. Since 1979 he has been teaching at the Massachusetts
Institute of Technology (M.I.T.), where he is currently McAfee Professor
of Engineering. He consults regularly with private industry and has held
editorial positions in several journals. His research spans several fields,
including optimization, control, large-scale computation, and data commu-
nication networks. He has written many research papers and he is the
author or coauthor of thirteen textbooks and research monographs.
Professor Bertsekas was awarded the INFORMS 1997 Prize for Re-
search Excellence in the Interface Between Operations Research and Com-
puter Science for his book ”Neuro-Dynamic Programming” (co-authored
with John Tsitsiklis), the 2000 Greek National Award for Operations Re-
search, and the 2001 ACC John R. Ragazzini Education Award. In 2001,
he was elected to the United States National Academy of Engineering.
iii
ATHENA SCIENTIFIC
OPTIMIZATION AND COMPUTATION SERIES
1. Convex Analysis and Optimization, by Dimitri P. Bertsekas, with
Angelia Nedić and Asuman E. Ozdaglar, 2003, ISBN 1-886529-
45-0, 560 pages
2. Introduction to Probability by Dimitri P. Bertsekas and John
Tsitsiklis, 2002, ISBN 1-886529-40-X, 430 pages
3. Dynamic Programming and Optimal Control, Vols. I and II, 2nd
Edition, by Dimitri P. Bertsekas, 2001, ISBN 1-886529-08-6, 704
pages
4. Nonlinear Programming, 2nd Edition, by Dimitri P. Bertsekas,
1999, ISBN 1-886529-00-0, 800 pages
5. Network Optimization: Continuous and Discrete Models by Dim-
itri P. Bertsekas, 1998, ISBN 1-886529-02-7, 608 pages
6. Network Flows and Monotropic Optimization by R. Tyrrell Rock-
afellar, 1998, ISBN 1-886529-06-X, 634 pages
7. Introduction to Linear Optimization by Dimitris Bertsimas and
John N. Tsitsiklis, 1997, ISBN 1-886529-19-1, 608 pages
8. Parallel and Distributed Computation: Numerical Methods by
Dimitri P. Bertsekas and John N. Tsitsiklis, 1997, ISBN 1-886529-
01-9, 718 pages
9. Neuro-Dynamic Programming, by Dimitri P. Bertsekas and John
N. Tsitsiklis, 1996, ISBN 1-886529-10-8, 512 pages
10. Constrained Optimization and Lagrange Multiplier Methods, by
Dimitri P. Bertsekas, 1996, ISBN 1-886529-04-3, 410 pages
11. Stochastic Optimal Control: The Discrete-Time Case by Dimitri
P. Bertsekas and Steven E. Shreve, 1996, ISBN 1-886529-03-5,
330 pages
iv
Contents
1. Introduction . . . . . . . . . . . . . . . . . . . . p. 1
1.1. Graphs and Flows . . . . . . . . . . . . . . . . . . . . p. 3
1.1.1. Paths and Cycles . . . . . . . . . . . . . . . . . . p. 4
1.1.2. Flow and Divergence . . . . . . . . . . . . . . . . p. 6
1.1.3. Path Flows and Conformal Decomposition . . . . . . . p. 7
1.2. Network Flow Models – Examples . . . . . . . . . . . . . p. 8
1.2.1. The Minimum Cost Flow Problem . . . . . . . . . . p. 9
1.2.2. Network Flow Problems with Convex Cost . . . . . . . p. 16
1.2.3. Multicommodity Flow Problems . . . . . . . . . . . p. 17
1.2.4. Discrete Network Optimization Problems . . . . . . . p. 19
1.3. Network Flow Algorithms – An Overview . . . . . . . . . p. 20
1.3.1. Primal Cost Improvement . . . . . . . . . . . . . . p. 21
1.3.2. Dual Cost Improvement . . . . . . . . . . . . . . . p. 24
1.3.3. Auction . . . . . . . . . . . . . . . . . . . . . . p. 27
1.3.4. Good, Bad, and Polynomial Algorithms . . . . . . . . p. 35
1.4. Notes, Sources, and Exercises . . . . . . . . . . . . . . . p. 37
2. Shortest Path Problems . . . . . . . . . . . . . . . p. 51
2.1. Problem Formulation and Applications . . . . . . . . . . p. 52
2.2. A Generic Shortest Path Algorithm . . . . . . . . . . . . p. 57
2.3. Label Setting (Dijkstra) Methods . . . . . . . . . . . . . p. 65
2.3.1. Performance of Label Setting Methods . . . . . . . . . p. 68
2.3.2. The Binary Heap Method . . . . . . . . . . . . . . p. 69
2.3.3. Dial’s Algorithm . . . . . . . . . . . . . . . . . . p. 70
2.4. Label Correcting Methods . . . . . . . . . . . . . . . . p. 73
2.4.1. The Bellman-Ford Method . . . . . . . . . . . . . . p. 73
2.4.2. The D’Esopo-Pape Algorithm . . . . . . . . . . . . p. 75
2.4.3. The SLF and LLL Algorithms . . . . . . . . . . . . p. 76
2.4.4. The Threshold Algorithm . . . . . . . . . . . . . . p. 78
2.4.5. Comparison of Label Setting and Label Correcting . . . p. 80
2.5. Single Origin/Single Destination Methods . . . . . . . . . p. 81
2.5.1. Label Setting . . . . . . . . . . . . . . . . . . . . p. 81
v
vi Contents
2.5.2. Label Correcting . . . . . . . . . . . . . . . . . . p. 81
2.6. Auction Algorithms . . . . . . . . . . . . . . . . . . . p. 86
2.7. Multiple Origin/Multiple Destination Methods . . . . . . . p. 96
2.8. Notes, Sources, and Exercises . . . . . . . . . . . . . . . p. 98
3. The Max-Flow Problem . . . . . . . . . . . . . . p. 115
3.1. The Max-Flow and Min-Cut Problems . . . . . . . . . . p. 116
3.1.1. Cuts in a Graph . . . . . . . . . . . . . . . . . p. 119
3.1.2. The Max-Flow/Min-Cut Theorem . . . . . . . . . . p. 121
3.1.3. The Maximal and Minimal Saturated Cuts . . . . . . p. 123
3.1.4. Decomposition of Infeasible Network Problems . . . . p. 124
3.2. The Ford-Fulkerson Algorithm . . . . . . . . . . . . . p. 125
3.3. Price-Based Augmenting Path Algorithms . . . . . . . . p. 132
3.3.1. A Price-Based Path Construction Algorithm . . . . . p. 135
3.3.2. A Price-Based Max-Flow Algorithm . . . . . . . . . p. 139
3.4. Notes, Sources, and Exercises . . . . . . . . . . . . . . p. 139
4. The Min-Cost Flow Problem . . . . . . . . . . . . p. 151
4.1. Transformations and Equivalences . . . . . . . . . . . . p. 152
4.1.1. Setting the Lower Flow Bounds to Zero . . . . . . . p. 152
4.1.2. Eliminating the Upper Flow Bounds . . . . . . . . . p. 153
4.1.3. Reduction to a Circulation Format . . . . . . . . . p. 154
4.1.4. Reduction to an Assignment Problem . . . . . . . . p. 154
4.2. Duality . . . . . . . . . . . . . . . . . . . . . . . p. 155
4.2.1. Interpretation of CS and the Dual Problem . . . . . . p. 162
4.2.2. Duality and CS for Nonnegativity Constraints . . . . p. 163
4.3. Notes, Sources, and Exercises . . . . . . . . . . . . . . p. 164
5. Simplex Methods for Min-Cost Flow . . . . . . . . . p. 169
5.1. Main Ideas in Simplex Methods . . . . . . . . . . . . . p. 170
5.1.1. Using Prices to Obtain the In-Arc . . . . . . . . . . p. 176
5.1.2. Obtaining the Out-Arc . . . . . . . . . . . . . . p. 179
5.1.3. Dealing with Degeneracy . . . . . . . . . . . . . . p. 183
5.2. The Basic Simplex Algorithm . . . . . . . . . . . . . . p. 186
5.2.1. Termination Properties of the Simplex Method . . . . p. 187
5.2.2. Initialization of the Simplex Method . . . . . . . . . p. 188
5.3. Extension to Problems with Upper and Lower Bounds . . . p. 195
5.4. Implementation Issues . . . . . . . . . . . . . . . . . p. 199
5.5. Notes, Sources, and Exercises . . . . . . . . . . . . . . p. 203
6. Dual Ascent Methods for Min-Cost Flow . . . . . . . p. 213
6.1. Dual Ascent . . . . . . . . . . . . . . . . . . . . . p. 214
Contents vii
6.2. The Primal-Dual (Sequential Shortest Path) Method . . . p. 221
6.3. The Relaxation Method . . . . . . . . . . . . . . . . p. 234
6.4. Solving Variants of an Already Solved Problem . . . . . . p. 243
6.5. Implementation Issues . . . . . . . . . . . . . . . . . p. 243
6.6. Notes, Sources, and Exercises . . . . . . . . . . . . . . p. 244
7. Auction Algorithms for Min-Cost Flow . . . . . . . p. 251
7.1. The Auction Algorithm for the Assignment Problem . . . p. 252
7.1.1. The Main Auction Algorithm . . . . . . . . . . . . p. 253
7.1.2. Approximate Coordinate Descent Interpretation . . . p. 257
7.1.3. Variants of the Auction Algorithm . . . . . . . . . p. 257
7.1.4. Computational Complexity – -Scaling . . . . . . . . p. 259
7.1.5. Dealing with Infeasibility . . . . . . . . . . . . . p. 265
7.2. Extensions of the Auction Algorithm . . . . . . . . . . p. 268
7.2.1. Reverse Auction . . . . . . . . . . . . . . . . . p. 268
7.2.2. Auction Algorithms for Asymmetric Assignment . . . p. 272
7.2.3. Auction Algorithms with Similar Persons . . . . . . p. 279
7.3. The Preflow-Push Algorithm for Max-Flow . . . . . . . . p. 282
7.3.1. Analysis and Complexity . . . . . . . . . . . . . . p. 285
7.3.2. Implementation Issues . . . . . . . . . . . . . . . p. 293
7.3.3. Relation to the Auction Algorithm . . . . . . . . . p. 294
7.4. The -Relaxation Method . . . . . . . . . . . . . . . p. 304
7.4.1. Computational Complexity – -Scaling . . . . . . . . p. 310
7.4.2. Implementation Issues . . . . . . . . . . . . . . . p. 318
7.5. The Auction/Sequential Shortest Path Algorithm . . . . . p. 320
7.6. Notes, Sources, and Exercises . . . . . . . . . . . . . . p. 326
8. Nonlinear Network Optimization . . . . . . . . . . p. 337
8.1. Convex and Separable Problems . . . . . . . . . . . . p. 339
8.2. Problems with Side Constraints . . . . . . . . . . . . . p. 346
8.3. Multicommodity Flow Problems . . . . . . . . . . . . p. 349
8.4. Integer Constraints . . . . . . . . . . . . . . . . . . p. 355
8.5. Networks with Gains . . . . . . . . . . . . . . . . . p. 360
8.6. Optimality Conditions . . . . . . . . . . . . . . . . . p. 365
8.7. Duality . . . . . . . . . . . . . . . . . . . . . . . p. 370
8.8. Algorithms and Approximations . . . . . . . . . . . . p. 375
8.8.1. Feasible Direction Methods . . . . . . . . . . . . . p. 375
8.8.2. Piecewise Linear Approximation . . . . . . . . . . p. 380
8.8.3. Interior Point Methods . . . . . . . . . . . . . . p. 382
8.8.4. Penalty and Augmented Lagrangian Methods . . . . . p. 384
8.8.5. Proximal Minimization . . . . . . . . . . . . . . p. 386
8.8.6. Smoothing . . . . . . . . . . . . . . . . . . . . p. 387
8.8.7. Transformations . . . . . . . . . . . . . . . . . p. 389
viii Contents
8.9. Notes, Sources, and Exercises . . . . . . . . . . . . . . p. 398
9. Convex Separable Network Problems . . . . . . . . p. 407
9.1. Convex Functions of a Single Variable . . . . . . . . . . p. 408
9.2. Optimality Conditions . . . . . . . . . . . . . . . . . p. 412
9.3. Duality . . . . . . . . . . . . . . . . . . . . . . . p. 414
9.4. Dual Function Differentiability . . . . . . . . . . . . . p. 426
9.5. Algorithms for Differentiable Dual Problems . . . . . . . p. 430
9.6. Auction Algorithms . . . . . . . . . . . . . . . . . . p. 433
9.6.1. The -Relaxation Method . . . . . . . . . . . . . p. 441
9.6.2. Auction/Sequential Shortest Path Algorithm . . . . . p. 446
9.7. Monotropic Programming . . . . . . . . . . . . . . . p. 449
9.8. Notes, Sources, and Exercises . . . . . . . . . . . . . . p. 463
10. Network Problems with Integer Constraints . . . . . p. 467
10.1. Formulation of Integer-Constrained Problems . . . . . . p. 469
10.2. Branch-and-Bound . . . . . . . . . . . . . . . . . . p. 483
10.3. Lagrangian Relaxation . . . . . . . . . . . . . . . . p. 492
10.3.1. Subgradients of the Dual Function . . . . . . . . . p. 497
10.3.2. Subgradient Methods . . . . . . . . . . . . . . . p. 499
10.3.3. Cutting Plane Methods . . . . . . . . . . . . . . p. 503
10.3.4. Decomposition and Multicommodity Flows . . . . . p. 507
10.4. Local Search Methods . . . . . . . . . . . . . . . . p. 512
10.4.1. Genetic Algorithms . . . . . . . . . . . . . . . p. 514
10.4.2. Tabu Search . . . . . . . . . . . . . . . . . . . p. 515
10.4.3. Simulated Annealing . . . . . . . . . . . . . . . p. 516
10.5. Rollout Algorithms . . . . . . . . . . . . . . . . . . p. 517
10.6. Notes, Sources, and Exercises . . . . . . . . . . . . . p. 525
Appendix A: Mathematical Review . . . . . . . . . . p. 545
A.1. Sets . . . . . . . . . . . . . . . . . . . . . . . . p. 546
A.2. Euclidean Space . . . . . . . . . . . . . . . . . . . p. 547
A.3. Matrices . . . . . . . . . . . . . . . . . . . . . . p. 547
A.4. Analysis . . . . . . . . . . . . . . . . . . . . . . . p. 548
A.5. Convex Sets and Functions . . . . . . . . . . . . . . p. 551
A.6. Subgradients . . . . . . . . . . . . . . . . . . . . . p. 553
References . . . . . . . . . . . . . . . . . . . . . . . . . p. 555
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . p. 587
Preface
Network optimization lies in the middle of the great divide that separates
the two major types of optimization problems, continuous and discrete.
The ties between linear programming and combinatorial optimization can
be traced to the representation of the constraint polyhedron as the convex
hull of its extreme points. When a network is involved, however, these ties
become much stronger because the extreme points of the polyhedron are in-
teger and represent solutions of combinatorial problems that are seemingly
unrelated to linear programming. Because of this structure and also be-
cause of their intuitive character, network models provide ideal vehicles for
explaining many of the fundamental ideas in both continuous and discrete
optimization.
Aside from their interesting methodological characteristics, network
models are also used extensively in practice, in an ever expanding spec-
trum of applications. Indeed collectively, network problems such as short-
est path, assignment, max-flow, transportation, transhipment, spanning
tree, matching, traveling salesman, generalized assignment, vehicle rout-
ing, and multicommodity flow constitute the most common class of practi-
cal optimization problems. There has been steady progress in the solution
methodology of network problems, and in fact the progress has accelerated
in the last fifteen years thanks to algorithmic and technological advances.
The purpose of this book is to provide a fairly comprehensive and up-
to-date development of linear, nonlinear, and discrete network optimization
problems. The interplay between continuous and discrete structures has
been highlighted, the associated analytical and algorithmic issues have been
treated quite extensively, and a guide to important network models and
applications has been provided.
Regarding continuous network optimization, we focus on two ideas,
which are also fundamental in general mathematical programming: dual-
ity and iterative cost improvement . We provide an extensive treatment of
iterative algorithms for the most common linear cost problem, the mini-
mum cost flow or transhipment problem, and for its convex cost extensions.
The discussion of duality is comprehensive: it starts with linear network
ix
x Preface
programming duality, and culminates with Rockafellar’s development of
monotropic programming duality.
Regarding discrete network optimization, we illustrate problem for-
mulation through major paradigms such as traveling salesman, generalized
assignment, spanning tree, matching, and routing. This is essential because
the structure of discrete optimization problems is far less streamlined than
the structure of their continuous counterparts, and familiarity with impor-
tant types of problems is important for modeling, analysis, and algorith-
mic solution. We also develop the main algorithmic approaches, including
branch-and-bound, Lagrangian relaxation, Dantzig-Wolfe decomposition,
heuristics, and local search methods.
This is meant to be an introductory book that covers a very broad
variety of topics. It is thus inevitable that some topics have been treated in
less detail than others. The choices made reflect in part personal taste and
expertise, and in part a preference for simple models that can help most
effectively the reader develop insight. At the same time, our analysis and
presentation aims to enhance the reader’s mathematical modeling ability in
two ways: by delineating the range of problems for which various algorithms
are applicable and efficient, and by providing many examples of problem
formulation.
The chapter-by-chapter description of the book follows:
Chapter 1: This is an introductory chapter that establishes terminology
and basic notions about graphs, discusses some examples of network mod-
els, and provides some orientation regarding linear network optimization
algorithms.
Chapter 2: This chapter provides an extensive treatment of shortest path
problems. It covers the major methods, and discusses their theoretical and
practical performance.
Chapter 3: This chapter focuses on the max-flow problem and develops
the class of augmenting path algorithms for its solution. In addition to the
classical variants of the Ford-Fulkerson method, a recent algorithm based
on auction ideas is discussed.
Chapter 4: The minimum cost flow problem (linear cost, single commod-
ity, no side constraints) and its equivalent variants are introduced here.
Subsequently, the basic duality theory for the problem is developed and
interpreted.
Chapter 5: This chapter focuses on simplex methods for the minimum
cost flow problem. The basic results regarding the integrality of solutions
are developed here constructively, using the simplex method. Furthermore,
the duality theory of Chapter 4 is significantly strengthened.
Chapter 6: This chapter develops dual ascent methods, including primal-
dual, sequential shortest path, and relaxation methods.
Preface xi
Chapter 7: This chapter starts with the auction algorithm for the assign-
ment problem, and proceeds to show how this algorithm can be extended
to more complex problems. In this way, preflow-push methods for the
max-flow problem and the -relaxation method for the minimum cost flow
problem are obtained. Several additional variants of auction algorithms
are developed.
Chapter 8: This is an important chapter that marks the transition from
linear to nonlinear network optimization. The primary focus is on continu-
ous (convex) problems, and their associated broad variety of structures and
methodology. In particular, there is an overview of the types of algorithms
from nonlinear programming that are useful in connection with various con-
vex network problems. There is also some discussion of discrete (integer)
problems with an emphasis on their ties with continuous problems.
Chapter 9: This is a fairly sophisticated chapter that is directed primar-
ily towards the advanced and/or research-oriented reader. It deals with
separable convex problems, discusses their connection with classical net-
work equilibrium problems, and develops their rich theoretical structure.
The salient features of this structure are a particularly sharp duality the-
ory, and a combinatorial connection of descent directions with the finite
set of elementary vectors of the subspace defined by the conservation of
flow constraints. Besides treating convex separable network problems, this
chapter provides an introduction to monotropic programming, which is the
largest class of nonlinear programming problems that possess the strong
duality and combinatorial properties of linear programs. This chapter also
develops auction algorithms for convex separable problems and provides an
analysis of their running time.
Chapter 10: This chapter deals with the basic methodological approaches
for integer-constrained problems. There is a treatment of exact methods
such as branch-and-bound, and the associated methods of Lagrangian re-
laxation, subgradient optimization, and cutting plane. There is also a
description of approximate methods based on local search, such as genetic
algorithms, tabu search, and simulated annealing. Finally, there is a dis-
cussion of rollout algorithms, a relatively new and broadly applicable class
of approximate methods, which can be used in place of, or in conjunction
with local search.
The book can be used for a course on network optimization or for part
of a course on introductory optimization at the first-year graduate level.
With the exception of some of the material in Chapter 9, the prerequisites
are fairly elementary. The main one is a certain degree of mathematical
maturity, as provided for example by a rigorous mathematics course beyond
the calculus level. One may cover most of the book in a course on linear
and nonlinear network optimization. A shorter version of this course may
consist of Chapters 1-5, and 8. Alternatively, one may teach a course that
xii Preface
focuses on linear and discrete network optimization, using Chapters 1-5,
a small part of Chapter 8, and Chapter 10. Actually, in these chapter
sequences, it is not essential to cover Chapter 5, if one is content with
weaker versions of duality results (given in Chapter 4) and one establishes
the integrality properties of optimal solutions with a line of argument such
as the one given in Exercise 1.34. The following figure illustrates the chapter
dependencies.
Chapters 1-5
(Intro/Linear)
Chapter 6
(Dual Methods)
Chapter 7
(Auction)
Chapter 10
(Integer)
Chapter 9
(Convex)
Chapter 8
(Nonlinear/Discrete)
The book contains a large number of examples and exercises, which
should enhance its suitability for classroom instruction. Some of the exer-
cises are theoretical in nature and supplement substantially the main text.
Solutions to a subset of these (as well as errata and additional material)
will be posted and periodically updated on the book’s web page:
http://www.athenasc.com/netsbook.html
Also, the author’s web page
http://web.mit.edu/dimitrib/www/home.html
contains listings of FORTRAN codes implementing many of the algorithms
discussed in the book.
There is a very extensive literature on continuous and discrete net-
work optimization, and to give a complete bibliography and a historical
account of the research that led to the present form of the subject would
have been impossible. Thus I have not attempted to compile a compre-
hensive list of original contributions to the field. I have cited sources that
I have used extensively, that provide important extensions to the material
of the book, that survey important topics, or that are particularly well
suited for further reading. I have also cited selectively a few sources that
are historically significant, but the reference list is far from exhaustive in
this respect. Generally, to aid researchers in the field, I have preferred to
cite surveys and textbooks for subjects that are relatively mature, and to
Preface xiii
give a larger number of references for relatively recent developments.
A substantial portion of this book is based on the author’s research
on network optimization over the last twenty years. I was fortunate to
have several outstanding collaborators in this research, and I would like
to mention those with whom I have worked extensively. Eli Gafni assisted
with the computational experimentation using the auction algorithm and
the relaxation method for assignment problems in 1979. The idea of -
scaling arose during my interactions with Eli at that time. Furthermore,
Eli collaborated extensively with me on various routing methods for data
networks, including projection methods for convex multicommodity flow
problems. Paul Tseng worked with me on network optimization starting
in 1982. Together we developed the RELAX codes, we developed several
extensions to the basic relaxation method and we collaborated closely on a
broad variety of other subjects, including the recent auction algorithms for
convex network problems and network problems with gains. David Cas-
tanon has worked extensively with me on a broad variety of algorithms
for assignment, transportation, and minimum cost flow problems, for both
serial and parallel computers, since 1987. John Tsitsiklis has been my coau-
thor and close collaborator for many years on a variety of optimization and
large scale computation topics, including some that deal with networks.
In addition to Eli, Paul, David, and John, I have had substantial research
collaborations with several colleagues, the results of which have been re-
flected in this book. In this regard, I would like to mention Jon Eckstein,
Bob Gallager, Francesca Guerriero, Roberto Musmanno, Stefano Pallot-
tino, and Maria-Grazia Scutellà. Several colleagues proofread portions of
the book, and contributed greatly with their suggestions. David Castanon,
Stefano Pallottino, Steve Patek, Serap Savari, Paul Tseng, and John Tsit-
siklis were particularly helpful in this regard. The research support of NSF
under grants from the DDM and the CCI divisions are very much appreci-
ated. My family has been a source of stability and loving support, without
which the book would not have been written.
Dimitri P. Bertsekas
Cambridge, Mass.
Spring 1998
1
Introduction
Contents
1.1. Graphs and Flows
1.1.1. Paths and Cycles
1.1.2. Flow and Divergence
1.1.3. Path Flows and Conformal Decomposition
1.2. Network Flow Models – Examples
1.2.1. The Minimum Cost Flow Problem
1.2.2. Network Flow Problems with Convex Cost
1.2.3. Multicommodity Flow Problems
1.2.4. Discrete Network Optimization Problems
1.3. Network Flow Algorithms – An Overview
1.3.1. Primal Cost Improvement
1.3.2. Dual Cost Improvement
1.3.3. Auction
1.3.4. Good, Bad, and Polynomial Algorithms
1.4. Notes, Sources, and Exercises
1
2 Introduction Chap. 1
Network flow problems are one of the most important and most frequently
encountered class of optimization problems. They arise naturally in the
analysis and design of large systems, such as communication, transporta-
tion, and manufacturing networks. They can also be used to model impor-
tant classes of combinatorial problems, such as assignment, shortest path,
and traveling salesman problems.
Loosely speaking, network flow problems consist of supply and de-
mand points, together with several routes that connect these points and
are used to transfer the supply to the demand. These routes may contain
intermediate transhipment points. Often, the supply, demand, and tran-
shipment points can be modeled by the nodes of a graph, and the routes can
be modeled by the paths of the graph. Furthermore, there may be multiple
“types” of supply/demand (or “commodities”) sharing the routes. There
may also be some constraints on the characteristics of the routes, such as
their carrying capacities, and some costs associated with using particu-
lar routes. Such situations are naturally modeled as network optimization
problems whereby, roughly speaking, we try to select routes that minimize
the cost of transfer of the supply to the demand.
This book deals with a broad spectrum of network optimization prob-
lems, involving linear and nonlinear cost functions. We pay special atten-
tion to four major classes of problems:
(a) The transhipment or minimum cost flow problem, which involves a
single commodity and a linear cost function. This problem has several
important special cases, such as the shortest path, the max-flow, the
assignment, and the transportation problems.
(b) The single commodity network flow problem with convex cost . This
problem is identical to the preceding transhipment problem, except
that the cost function is convex rather than linear.
(c) The multicommodity network flow problem with linear or convex cost .
This problem generalizes the preceding two classes of problems to the
case of multiple commodities.
(d) Discrete network optimization problems. These are problems where
the quantities transferred along the routes of the network are re-
stricted to take one of a finite number of values. Many combinatorial
optimization problems can be modeled in this way, including some
problems where the network structure is not immediately apparent.
Some discrete optimization problems are computationally very diffi-
cult, and in practice can only be solved approximately. Their algorith-
mic solution often involves the solution of “continuous” subproblems
that belong to the preceding three classes.
All of the network flow problems above can be mathematically mod-
eled in terms of graph-related notions. In Section 1.1, we introduce the
associated notation and terminology. In Section 1.2, we provide mathe-
Sec. 1.1 Graphs and Flows 3
matical formulations and practical examples of network optimization mod-
els. Finally, in Section 1.3, we give an overview of some of the types of
computational algorithms that we develop in subsequent chapters.
1.1 GRAPHS AND FLOWS
In this section, we introduce some of the basic definitions relating to graphs,
paths, flows, and other related notions. Graph concepts are fairly intuitive,
and can be understood in terms of suggestive figures, but often involve
hidden subtleties. Thus the reader may wish to revisit the present section
and pay close attention to some of the fine points of the definitions.
A directed graph, G = (N ,A), consists of a set N of nodes and a set
A of pairs of distinct nodes from N called arcs. The numbers of nodes and
arcs are denoted by N and A, respectively, and it is assumed throughout
that 1 ≤ N < ∞ and 0 ≤ A < ∞. An arc (i, j) is viewed as an ordered
pair, and is to be distinguished from the pair (j, i). If (i, j) is an arc, we
say that (i, j) is outgoing from node i and incoming to node j; we also say
that j is an outward neighbor of i and that i is an inward neighbor of j. We
say that arc (i, j) is incident to i and to j, and that i is the start node and
j is the end node of the arc. We also say that i and j are the end nodes of
arc (i, j). The degree of a node i is the number of arcs that are incident to
i. A graph is said to be complete if it contains all possible arcs; that is, if
there exists an arc for each ordered pair of nodes.
We do not exclude the possibility that there is a separate arc connect-
ing a pair of nodes in each of the two directions. However, we do not allow
more than one arc between a pair of nodes in the same direction, so that we
can refer unambiguously to the arc with start i and end j as arc (i, j). This
is done for notational convenience.† Our analysis can be simply extended
to handle multiple arcs with start i and end j; the extension is based on
modifying the graph by introducing for each such arc, an additional node,
call it n, together with the two arcs (i, n) and (n, j). On occasion, we will
pause to provide examples of this type of extension.
We note that much of the literature of graph theory distinguishes
between directed graphs where an arc (i, j) is an ordered pair to be distin-
guished from arc (j, i), and undirected graphs where an arc is associated
with a pair of nodes regardless of order. One may use directed graphs, even
in contexts where the use of undirected graphs would be appropriate and
conceptually simpler. For this, one may need to replace an undirected arc
(i, j) with two directed arcs (i, j) and (j, i) having identical characteristics.
† Some authors use a single symbol, such as a, to denote an arc, and use
something like s(a) and e(a) to denote the start and end nodes of a, respectively.
This notational method allows the existence of multiple arcs with the same start
and end nodes, but is also more cumbersome and less suggestive.
4 Introduction Chap. 1
We have chosen to deal exclusively with directed graphs because in our
development there are only a few occasions where undirected graphs are
convenient. Thus, all our references to a graph implicitly assume that the
graph is directed . In fact we often omit the qualifier “directed” and refer
to a directed graph simply as a graph.
1.1.1 Paths and Cycles
A path P in a directed graph is a sequence of nodes (n1, n2, . . . , nk) with
k ≥ 2 and a corresponding sequence of k−1 arcs such that the ith arc in the
sequence is either (ni, ni+1) (in which case it is called a forward arc of the
path) or (ni+1, ni) (in which case it is called a backward arc of the path).
Nodes n1 and nk are called the start node (or origin) and the end node (or
destination) of P , respectively. A path is said to be forward (or backward)
if all of its arcs are forward (respectively, backward) arcs. We denote by
P+ and P− the sets of forward and backward arcs of P , respectively.
A cycle is a path for which the start and end nodes are the same. A
path is said to be simple if it contains no repeated arcs and no repeated
nodes, except that the start and end nodes could be the same (in which
case the path is called a simple cycle). A Hamiltonian cycle is a simple
forward cycle that contains all the nodes of the graph. These definitions
are illustrated in Fig. 1.1. We mention that some authors use a slightly
different terminology: they use the term “walk” to refer to a path and they
use the term “path” to refer to a simple path.
Note that the sequence of nodes (n1, n2, . . . , nk) is not sufficient to
specify a path; the sequence of arcs may also be important, as Fig. 1.1(c)
shows. The difficulty arises when for two successive nodes ni and ni+1 of
the path, both (ni, ni+1) and (ni+1, ni) are arcs, so there is ambiguity as
to which of the two is the corresponding arc of the path. If a path is known
to be forward or is known to be backward, it is uniquely specified by the
sequence of its nodes. Otherwise, however, the intended sequence of arcs
must be explicitly defined.
A graph that contains no simple cycles is said to be acyclic. A graph
is said to be connected if for each pair of nodes i and j, there is a path
starting at i and ending at j; it is said to be strongly connected if for each
pair of nodes i and j, there is a forward path starting at i and ending
at j. Thus, for example, the graph of Fig. 1.1(b) is connected but not
strongly connected. It can be shown that if a graph is connected and each
of its nodes has even degree, there is a cycle (not necessarily forward) that
contains all the arcs of the graph exactly once (see Exercise 1.5). Such
a cycle is called an Euler cycle, honoring the historically important work
of Euler; see the discussion in Section 10.1 about the Königsberg bridge
problem. Figure 1.2 gives an example of an Euler cycle.
We say that a graph G′ = (N ′,A′) is a subgraph of a graph G = (N ,A)
if N ′ ⊂ N and A′ ⊂ A. A tree is a connected acyclic graph. A spanning
Sec. 1.1 Graphs and Flows 5
(a)  A simple forward path P = (n1, n2 , n3 , n4 ).
n 2n1 n3 n4Start Node End Node
n5n 2n1 n3 n4Start Node End Node
(c) Path P = (n1, n2 , n3 , n 4 , n 5) with corresponding sequence of arcs
{ (n1, n2) , ( n3 , n 2),  ( n3 , n 4), ( n5 , n 4)}.
n3
n1 n 2
Set of forward arcs C +
Set of backward arcs C-
(b)  A simple cycle C = (n1, n2 , n3 , n 1) which is neither forward nor backward.
Figure 1.1: Illustration of various types of paths and cycles. The cycle in (b)
is not a Hamiltonian cycle; it is simple and contains all the nodes of the graph,
but it is not forward. Note that for the path (c), in order to resolve ambiguities,
it is necessary to specify the sequence of arcs of the path (rather than just the
sequence of nodes) because both (n3, n4) and (n4, n3) are arcs.
(b)
1
5
6
2
8
4
3
7
(c)
1 2 3
4 5
6 7 8
(a)
Figure 1.2: Example of an Euler cycle. Consider a 3 × 3 chessboard, where the
middle square has been deleted. A knight starting at one of the squares of the
board can visit every other square exactly once and return to the starting square
as shown in the graph (b), or equivalently in (c). In the process, the knight will
make all the possible moves (in one direction only), or equivalently, it will cross
every arc of the graph in (b) exactly once. The knight’s tour is an Euler cycle for
the graph of (b).
6 Introduction Chap. 1
tree of a graph G is a subgraph of G, which is a tree and includes all the
nodes of G. It can be shown [Exercise 1.14(c)] that a subgraph is a spanning
tree if and only if it is connected and it contains N − 1 arcs.
1.1.2 Flow and Divergence
In many applications involving graphs, it is useful to introduce a variable
that measures the quantity flowing through each arc, like for example,
electric current in an electric circuit, or water flow in a hydraulic network.
We refer to such a variable as the flow of an arc. Mathematically, the flow
of an arc (i, j) is simply a scalar (real number), which we usually denote
by xij . It is convenient to allow negative as well as positive values for flow.
In applications, a negative arc flow indicates that whatever is represented
by the flow (material, electric current, etc.), moves in a direction opposite
to the direction of the arc. We can always change the sign of a negative
arc flow to positive as long as we change the arc direction, so in many
situations we can assume without loss of generality that all arc flows are
nonnegative. For the development of a general methodology, however, this
device is often cumbersome, which is why we prefer to simply accept the
possibility of negative arc flows.
Given a graph (N ,A), a set of flows
{
xij | (i, j) ∈ A
}
is referred to
as a flow vector . The divergence vector y associated with a flow vector x
is the N -dimensional vector with coordinates
yi =
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji, ∀ i ∈ N . (1.1)
Thus, yi is the total flow departing from node i less the total flow arriving
at i; it is referred to as the divergence of i.
We say that node i is a source (respectively, sink) for the flow vector
x if yi > 0 (respectively, yi < 0). If yi = 0 for all i ∈ N , then x is called
a circulation. These definitions are illustrated in Fig. 1.3. Note that by
adding Eq. (1.1) over all i ∈ N , we obtain∑
i∈N
yi = 0.
Every divergence vector y must satisfy this equation.
The flow vectors x that we will consider will often be constrained to
lie between given lower and upper bounds of the form
bij ≤ xij ≤ cij , ∀ (i, j) ∈ A.
Given a flow vector x that satisfies these bounds, we say that a path P is
unblocked with respect to x if, roughly speaking, we can send some positive
flow along P without violating the bound constraints; that is, if flow can
Sec. 1.1 Graphs and Flows 7
x    = 1
12
13x    = 0 34
x    = 2
32x    = 0x    = 123
x    = -2
24
y   = -2  (Sink)2
3y   = 1  (Source)  
1y   = 1  (Source)  
(a)
1 4
3
2
(b)  A circulation
24x     = -1x    = 112
x     = 1
23
x     = -132
x     = 134
y   = 02
3y   = 0
4y   = 0
4y   = 01y   = 0 1 4
3
2
13x    = -1
 (Neither a source
  nor a sink)
Figure 1.3: Illustration of flows xij and the corresponding divergences yi. The
flow in (b) is a circulation because yi = 0 for all i.
be increased on the set P+ of the forward arcs of P , and can be decreased
on the set P− of the backward arcs of P :
xij < cij , ∀ (i, j) ∈ P+, bij < xij , ∀ (i, j) ∈ P−.
For example, in Fig. 1.3(a), suppose that all arcs (i, j) have flow bounds
bij = −2 and cij = 2. Then the path consisting of the sequence of nodes
(1, 2, 4) is unblocked, while the reverse path (4, 2, 1) is not unblocked.
1.1.3 Path Flows and Conformal Decomposition
A simple path flow is a flow vector that corresponds to sending a positive
amount of flow along a simple path; more precisely, it is a flow vector x
with components of the form
xij =
 a if (i, j) ∈ P
+,
−a if (i, j) ∈ P−,
0 otherwise,
(1.2)
where a is a positive scalar, and P+ and P− are the sets of forward and
backward arcs, respectively, of some simple path P . Note that the path P
may be a cycle, in which case x is also called a simple cycle flow .
8 Introduction Chap. 1
It is often convenient to break down a flow vector into the sum of
simple path flows. This leads to the notion of a conformal realization,
which we proceed to discuss.
We say that a path P conforms to a flow vector x if xij > 0 for all
forward arcs (i, j) of P and xij < 0 for all backward arcs (i, j) of P , and
furthermore either P is a cycle or else the start and end nodes of P are a
source and a sink of x, respectively. Roughly, a path conforms to a flow
vector if it “carries flow in the forward direction,” i.e., in the direction
from the start node to the end node. In particular, for a forward cycle to
conform to a flow vector, all its arcs must have positive flow. For a forward
path which is not a cycle to conform to a flow vector, its arcs must have
positive flow, and in addition the start and end nodes must be a source
and a sink, respectively; for example, in Fig. 1.3(a), the path consisting of
the sequence of arcs (1,2), (2,3), (3,4) does not conform to the flow vector
shown, because node 4, the end node of the path, is not a sink.
We say that a simple path flow xs conforms to a flow vector x if the
path P corresponding to xs via Eq. (1.2) conforms to x. This is equivalent
to requiring that
0 < xij for all arcs (i, j) with 0 < xsij ,
xij < 0 for all arcs (i, j) with xsij < 0,
and that either P is a cycle or else the start and end nodes of P are a
source and a sink of x, respectively.
An important fact is that any flow vector can be decomposed into a
set of conforming simple path flows, as illustrated in Fig. 1.4. We state
this as a proposition. The proof is based on an algorithm that can be used
to construct the conforming components one by one (see Exercise 1.2).
Proposition 1.1: (Conformal Realization Theorem) A nonzero
flow vector x can be decomposed into the sum of t simple path flow
vectors x1, x2, . . . , xt that conform to x, with t being at most equal to
the sum of the numbers of arcs and nodes A + N . If x is integer, then
x1, x2, . . . , xt can also be chosen to be integer. If x is a circulation,
then x1, x2, . . . , xt can be chosen to be simple cycle flows, and t ≤ A.
1.2 NETWORK FLOW MODELS – EXAMPLES
In this section we introduce some of the major classes of problems that will
be discussed in this book. We begin with the minimum cost flow problem,
which, together with its special cases, will be the subject of the following
six chapters.
Sec. 1.2 Network Flow Models – Examples 9
4
3
2
4
3
2Flow = -1
Flow = 1
Flow = 1
Flow = 1
Flow = -1
1
Flow = 1 2
x    = 1
12
13x    = 0 34
x    = 2
32x    = 0x    = 123
x    = -2
24
y   = -2  (Sink)2
3y   = 1  (Source)  
1y   = 1  (Source)  1 4
3
2
4y   = 0
 (Neither a source
  nor a sink)
Figure 1.4: Decomposition of a flow vector x into three simple path flows con-
forming to x. Consistent with the definition of conformance of a path flow, each
arc (i, j) of the three component paths carries positive (or negative) flow only if
xij > 0 (or xij < 0, respectively). The first two paths [(1, 2) and (3, 4, 2)] are not
cycles, but they start at a source and end at a sink, as required. Arcs (1, 3) and
(3, 2) do not belong to any of these paths because they carry zero flow. In this
example, the decomposition is unique, but in general this need not be the case.
1.2.1 The Minimum Cost Flow Problem
This problem is to find a set of arc flows that minimize a linear cost function,
subject to the constraints that they produce a given divergence vector and
they lie within some given bounds; that is,
minimize
∑
(i,j)∈A
aijxij (1.3)
subject to the constraints∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N , (1.4)
bij ≤ xij ≤ cij , ∀ (i, j) ∈ A, (1.5)
where aij , bij , cij , and si are given scalars. We use the following terminol-
ogy:
aij : the cost coefficient (or simply cost) of (i, j),
bij and cij : the flow bounds of (i, j),
[bij , cij ]: the feasible flow range of (i, j),
10 Introduction Chap. 1
si: the supply of node i (when si is negative, the scalar −si is called
the demand of i).
We also refer to the constraints (1.4) and (1.5) as the conservation of flow
constraints, and the capacity constraints, respectively. A flow vector satis-
fying both of these constraints is called feasible, and if it satisfies just the
capacity constraints, it is called capacity-feasible. If there exists at least
one feasible flow vector, the minimum cost flow problem is called feasible;
otherwise it is called infeasible. On occasion, we will consider the variation
of the minimum cost flow problem where the lower or the upper flow bound
of some of the arcs is either −∞ or ∞, respectively. In these cases, we will
explicitly state so.
For a typical application of the minimum cost flow problem, think
of the nodes as locations (cities, warehouses, or factories) where a certain
product is produced or consumed. Think of the arcs as transportation
links between the locations, each with transportation cost aij per unit
transported. The problem then is to move the product from the production
points to the consumption points at minimum cost while observing the
capacity constraints of the transportation links.
However, the minimum cost flow problem has many applications that
are well beyond the transportation context just described, as will be seen
from the following examples. These examples illustrate how some impor-
tant discrete/combinatorial problems can be modeled as minimum cost flow
problems, and highlight the important connection between continuous and
discrete network optimization.
Example 1.1. The Shortest Path Problem
Suppose that each arc (i, j) of a graph is assigned a scalar cost aij , and suppose
that we define the cost of a forward path to be the sum of the costs of its
arcs. Given a pair of nodes, the shortest path problem is to find a forward
path that connects these nodes and has minimum cost. An analogy here is
made between arcs and their costs, and roads in a transportation network and
their lengths, respectively. Within this transportation context, the problem
becomes one of finding the shortest route between two geographical points.
Based on this analogy, the problem is referred to as the shortest path problem,
and the arc costs and path costs are commonly referred to as the arc lengths
and path lengths, respectively.
The shortest path problem arises in a surprisingly large number of con-
texts. For example in a data communication network, aij may denote the
average delay of a packet to cross the communication link (i, j), in which case
a shortest path is a minimum average delay path that can be used for routing
the packet from its origin to its destination. As another example, if pij is
the probability that a given arc (i, j) in a communication network is usable,
and each arc is usable independently of all other arcs, then the product of the
probabilities of the arcs of a path provides a measure of reliability of the path.
With this in mind, it is seen that finding the most reliable path connecting
Sec. 1.2 Network Flow Models – Examples 11
two nodes is equivalent to finding the shortest path between the two nodes
with arc lengths (− ln pij).
The shortest path problem also arises often as a subroutine in algo-
rithms that solve other more complicated problems. Examples are the primal-
dual algorithm for solving the minimum cost flow problem (see Chapter 6),
and the conditional gradient and projection algorithms for solving multicom-
modity flow problems (see Chapter 8).
It is possible to cast the problem of finding a shortest path from node
s to node t as the following minimum cost flow problem:
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji =
{
1 if i = s,
−1 if i = t,
0 otherwise,
0 ≤ xij , ∀ (i, j) ∈ A.
(1.6)
To see this, let us associate with any forward path P from s to t the flow
vector x with components given by
xij =
{
1 if (i, j) belongs to P ,
0 otherwise.
(1.7)
Then x is feasible for problem (1.6) and the cost of x is equal to the length
of P . Thus, if a vector x of the form (1.7) is an optimal solution of problem
(1.6), the corresponding path P is shortest.
Conversely, it can be shown that if problem (1.6) has at least one op-
timal solution, then it has an optimal solution of the form (1.7), with a
corresponding path P that is shortest. This is not immediately apparent, but
its proof can be traced to a remarkable fact that we will show in Chapter 5
about minimum cost flow problems with node supplies and arc flow bounds
that are integer: such problems, if they have an optimal solution, they have
an integer optimal solution, that is, a set of optimal arc flows that are integer
(an alternative proof of this fact is sketched in Exercise 1.34). From this it
follows that if problem (1.6) has an optimal solution, it has one with arc flows
that are 0 or 1, and which is of the form (1.7) for some path P . This path is
shortest because its length is equal to the optimal cost of problem (1.6), so it
must be less or equal to the cost of any other flow vector of the form (1.7),
and therefore also less or equal to the length of any other path from s to t.
Thus the shortest path problem is essentially equivalent with the minimum
cost flow problem (1.6).
Example 1.2. The Assignment Problem
Suppose that there are n persons and n objects that we have to match on a
one-to-one basis. There is a benefit or value aij for matching person i with
object j, and we want to assign persons to objects so as to maximize the total
12 Introduction Chap. 1
1
i
n
1
1
1
PERSONS
..
.
..
.
1
j
n
aij
1
1
1
OBJECTS
..
.
..
.
Figure 1.5: The graph represen-
tation of an assignment problem.
benefit. There is also a restriction that person i can be assigned to object j
only if (i, j) belongs to a given set of pairs A. Mathematically, we want to find
a set of person-object pairs (1, j1), . . . , (n, jn) from A such that the objects
j1, . . . , jn are all distinct, and the total benefit
∑n
i=1
aiji is maximized.
The assignment problem is important in many practical contexts. The
most obvious ones are resource allocation problems, such as assigning em-
ployees to jobs, machines to tasks, etc. There are also situations where the
assignment problem appears as a subproblem in methods for solving various
complex combinatorial problems (see Chapter 10).
We may associate any assignment with the set of variables {xij | (i, j) ∈
A}, where xij = 1 if person i is assigned to object j and xij = 0 otherwise.
The value of this assignment is
∑
(i,j)∈A aijxij . The restriction of one object
per person can be stated as
∑
j
xij = 1 for all i and
∑
i
xij = 1 for all j. We
may then formulate the assignment problem as the linear program
maximize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij = 1, ∀ i = 1, . . . , n,∑
{i|(i,j)∈A}
xij = 1, ∀ j = 1, . . . , n,
0 ≤ xij ≤ 1, ∀ (i, j) ∈ A.
(1.8)
Actually we should further restrict xij to be either 0 or 1. However, as we
will show in Chapter 5, the above linear program has the property that if it
has a feasible solution at all, then it has an optimal solution where all xij
are either 0 or 1 (compare also with the discussion in the preceding example
and Exercise 1.34). In fact, the set of its optimal solutions includes all the
optimal assignments.
We now argue that the assignment/linear program (1.8) is a minimum
cost flow problem involving the graph shown in Fig. 1.5. Here, there are
2n nodes divided into two groups: n corresponding to persons and n corre-
sponding to objects. Also, for every possible pair (i, j) ∈ A, there is an arc
connecting person i with object j. The variable xij is the flow of arc (i, j).
Sec. 1.2 Network Flow Models – Examples 13
The constraint ∑
{j|(i,j)∈A}
xij = 1
indicates that the divergence of person/node i should be equal to 1, while the
constraint ∑
{i|(i,j)∈A}
xij = 1
indicates that the divergence of object/node j should be equal to -1. Finally,
we may view (−aij) as the cost coefficient of the arc (i, j) (by reversing the
sign of aij , we convert the problem from a maximization to a minimization
problem).
Example 1.3. The Max-Flow Problem
In the max-flow problem, we have a graph with two special nodes: the source,
denoted by s, and the sink , denoted by t. Roughly, the objective is to move as
much flow as possible from s into t while observing the capacity constraints.
More precisely, we want to find a flow vector that makes the divergence of all
nodes other than s and t equal to 0 while maximizing the divergence of s.
Source Sinks t
Cost coefficient  = -1
Artificial feedback arc
All cost coefficients are
zero except for ats
Figure 1.6: The minimum cost flow representation of a max-flow problem.
At the optimum, the flow xts equals the maximum flow that can be sent from
s to t through the subgraph obtained by deleting the artificial arc (t, s).
The max-flow problem arises in many practical contexts, such as calcu-
lating the throughput of a highway system or a communication network. It
also arises often as a subproblem in more complicated problems or algorithms;
in particular, it bears a fundamental connection to the question of existence of
a feasible solution of a general minimum cost flow problem (see our discussion
14 Introduction Chap. 1
in Chapter 3). Finally, several discrete/combinatorial optimization problems
can be formulated as max-flow problems (see the Exercises in Chapter 3).
We formulate the problem as a special case of the minimum cost flow
problem by assigning cost 0 to all arcs and by introducing an artificial arc
(t, s) with cost −1, as shown in Fig. 1.6. Mathematically, the problem is:
maximize xts
subject to∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = 0, ∀ i ∈ N with i = s and i = t,∑
{j|(s,j)∈A}
xsj =
∑
{i|(i,t)∈A}
xit = xts,
bij ≤ xij ≤ cij , ∀ (i, j) ∈ A with (i, j) = (t, s).
Viewing the problem as a maximization is consistent with its intuitive inter-
pretation. Alternatively, we could write the problem as a minimization of
−xts subject to the same constraints. Also, we could introduce upper and
lower bounds on xts, ∑
{i|(i,t)∈A}
bit ≤ xts ≤
∑
{i|(i,t)∈A}
cit,
but these bounds are actually redundant since they are implied by the other
upper and lower arc flow bounds.
Example 1.4. The Transportation Problem
This problem is the same as the assignment problem except that the node
supplies need not be 1 or −1, and the numbers of sources and sinks need not
be equal. It has the form
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij = αi, ∀ i = 1, . . . , m,∑
{i|(i,j)∈A}
xij = βj , ∀ j = 1, . . . , n,
0 ≤ xij ≤ min{αi, βj}, ∀ (i, j) ∈ A.
(1.9)
Here αi and βj are positive scalars, which for feasibility must satisfy
m∑
i=1
αi =
n∑
j=1
βj ,
Sec. 1.2 Network Flow Models – Examples 15
(add the conservation of flow constraints). In an alternative formulation,
the upper bound constraint xij ≤ min{αi, βj} could be discarded, since it is
implied by the conservation of flow and the nonnegativity constraints.
As a practical example of a transportation problem that has a combi-
natorial flavor, suppose that we have m communication terminals, each to be
connected to one of n traffic concentrators. We introduce variables xij , which
take the value 1 if terminal i is connected to concentrator j. Assuming that
concentrator j can be connected to no more than bj terminals, we obtain the
constraints
m∑
i=1
xij ≤ bj , ∀ j = 1, . . . , n.
Also, since each terminal must be connected to exactly one concentrator, we
have the constraints
n∑
j=1
xij = 1, ∀ i = 1, . . . , m.
Assuming that there is a cost aij for connecting terminal i to concentrator j,
the problem is to find the connection of minimum cost, that is, to minimize
m∑
i=1
n∑
j=1
aijxij
subject to the preceding constraints. This problem is not yet a transportation
problem of the form (1.9) for two reasons:
(a) The arc flows xij are constrained to be 0 or 1.
(b) The constraints
∑m
i=1
xij ≤ bj are not equality constraints, as required
in problem (1.9).
It turns out, however, that we can ignore the 0-1 constraint on xij . As
discussed in connection with the shortest path and assignment problems,
even if we relax this constraint and replace it with the capacity constraint
0 ≤ xij ≤ 1, there is an optimal solution such that each xij is either 0 or
1. Furthermore, to convert the inequality constraints to equalities, we can
introduce a total of
∑n
j=1
bj − m “dummy” terminals that can be connected
at zero cost to all of the concentrators. In particular, we introduce a special
supply node 0 together with the constraint
n∑
j=1
x0j =
n∑
j=1
bj − m,
and we change the inequality constraints
∑n
j=1
xij ≤ bj to
x0j +
m∑
i=1
xij = bj .
The resulting problem has the transportation structure of problem (1.9), and
is equivalent to the original problem.
16 Introduction Chap. 1
1.2.2 Network Flow Problems with Convex Cost
A more general version of the minimum cost flow problem arises when the
cost function is convex rather than linear. An important special case is the
problem
minimize
∑
(i,j)∈A
fij(xij)
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
xij ∈ Xij , ∀ (i, j) ∈ A,
where fij is a convex function of the flow xij of arc (i, j), si are given
scalars, and Xij are convex intervals of real numbers, such as for example
Xij = [bij , cij ],
where bij and cij are given scalars. We refer to this as the separable convex
cost network flow problem, because the cost function separates into the sum
of cost functions, one per arc. This problem will be discussed in detail in
Chapters 8 and 9.
Example 1.5. The Matrix Balancing Problem
Here the problem is to find an m× n matrix X that has given row sums and
column sums, and approximates a given m × n matrix M in some optimal
manner. We can formulate such a problem in terms of a graph consisting of
m sources and n sinks. In this graph, the set of arcs consists of the pairs
(i, j) for which the corresponding entry xij of the matrix X is allowed to be
nonzero. The given row sums ri and the given column sums cj are expressed
as the constraints ∑
{j|(i,j)∈A}
xij = ri, i = 1, . . . , m,
∑
{i|(i,j)∈A}
xij = cj , j = 1, . . . , n.
There may be also bounds for the entries xij of X. Thus, the structure of
this problem is similar to the structure of a transportation problem. The cost
function to be optimized has the form∑
(i,j)∈A
fij(xij),
Sec. 1.2 Network Flow Models – Examples 17
and expresses the objective of making the entries of X close to the corre-
sponding entries of the given matrix M . A commonly used example is the
quadratic function
fij(xij) =
∑
(i,j)∈A
wij(xij − mij)2,
where wij are given positive scalars.
Another interesting cost function is the logarithmic
fij(xij) = xij
[
ln
(
xij
mij
)
− 1
]
,
where we assume that mij > 0 for all (i, j) ∈ A. Note that this function is not
defined for xij ≤ 0, so to obtain a problem that fits our framework, we must
use a constraint interval of the form Xij = (0,∞) or Xij = (0, cij ], where cij
is a positive scalar.
An example of a practical problem that can be addressed using the
preceding optimization model is to predict the distribution matrix X of tele-
phone traffic between m origins and n destinations. Here we are given the
total supplies ri of the origins and the total demands cj of the destinations,
and we are also given some matrix M that defines a nominal traffic pattern
obtained from historical data.
There are other types of network flow problems with convex cost that
often arise in practice. We generically represent such problems in the form
minimize f(x)
subject to x ∈ F
where F is a convex subset of flow vectors in a graph and f is a convex
function over the set F . We will discuss in some detail various classes
of problems of this type in Chapter 8, and we will see that they arise in
several different ways; for example, the cost function may be nonseparable
because of coupling of the costs of several arc flows, and/or there may be
side constraints, whereby the flows of several arcs are jointly restricted by
the availability of resource. An important example is multicommodity flow
problems, which we discuss next.
1.2.3 Multicommodity Flow Problems
Multicommodity network flow problems involve several flow “types” or
commodities, which simultaneously use the network and are coupled through
either the arc flow bounds, or through the cost function. Important exam-
ples of such problems arise in communication, transportation, and man-
ufacturing networks. For example, in communication networks the com-
modities are the streams of different classes of traffic (telephone, data,
18 Introduction Chap. 1
video, etc.) that involve different origin-destination pairs. Thus there is
a separate commodity per class of traffic and origin-destination pair. The
following example introduces this context. In Chapter 8, we will discuss
similar and/or more general multicommodity network flow problems that
arise in other practical contexts.
Example 1.6. Routing in Data Networks
We are given a directed graph, which is viewed as a model of a data com-
munication network. We are also given a set of ordered node pairs (im, jm),
m = 1, . . . , M , referred to as origin-destination (OD) pairs. The nodes im
and jm are referred to as the origin and the destination of the OD pair. For
each OD pair (im, jm), we are given a scalar rm that represents its input
traffic. In the context of routing of data in a communication network, rm
(measured for example in bits/second) is the arrival rate of traffic entering
the network at node im and exiting at node jm. The routing objective is to
divide each rm among the many paths from the origin im to the destination
jm in a way that the resulting total arc flow pattern minimizes a suitable cost
function (see Fig. 1.7).
Origin of 
OD pair (im , jm )
rm
Destination of 
OD pair (im, jm )
rm
im jm
Figure 1.7: Illustration of how the input rm of the OD pair (im, jm) is
divided into nonnegative path flows that start at im and end at jm. The
flows of the different OD pairs interact by sharing the arcs of the network.
If we denote by xij(m) the flow on arc (i, j) of OD pair (im, jm), we
have the conservation of flow constraints
∑
{j|(i,j)∈A}
xij(m) −
∑
{j|(j,i)∈A}
xji(m) =
{
rm if i = im,
−rm if i = jm,
0 otherwise,
∀ i ∈ N ,
for each m = 1, . . . , M . Furthermore, the flows xij(m) are required to be
nonnegative, and possibly to satisfy additional constraints, such as upper
bounds. The cost function often has the form
f(x) =
∑
(i,j)∈A
fij(yij),
Sec. 1.2 Network Flow Models – Examples 19
where fij is a function of the total flow of arc (i, j)
yij =
M∑
m=1
xij(m).
Such a cost function is often based on a queueing model of average delay (see
for example the data network textbook by Bertsekas and Gallager [1992]).
1.2.4 Discrete Network Optimization Problems
Many linear or convex network flow problems, in addition to the conser-
vation of flow constraints and arc flow bounds, involve some additional
constraints. In particular, there may be constraints that couple the flows
of different arcs, and there may also be integer constraints on the arc flows,
such as for example that each arc flow be either 0 or 1. Several famous
combinatorial optimization problems, such as the following one, are of this
type.
Example 1.7. The Traveling Salesman Problem
This problem refers to a salesman who wants to find a minimum mileage/cost
tour that visits each of N given cities exactly once and returns to the city
he started from. To convert this to a network flow problem, we associate a
node with each city i = 1, . . . , N , and we introduce an arc (i, j) with traversal
cost aij for each ordered pair of nodes i and j. A tour is synonymous to
a Hamiltonian cycle, which was earlier defined to be a simple forward cycle
that contains all the nodes of the graph. Equivalently, a tour is a connected
subgraph that consists of N arcs, such that there is exactly one incoming and
one outgoing arc for each node i = 1, . . . , N . The problem is to find a tour
with minimum sum of arc costs.
To formulate this problem as a network flow problem, we denote by xij
the flow of arc (i, j) and we require that this flow is either 1 or 0, indicating
that the arc is or is not part of the tour, respectively. The cost of a tour T is
then ∑
(i,j)∈T
aijxij .
The constraint that each node has a single incoming and a single outgoing arc
on the tour is expressed by the following two conservation of flow equations:∑
j=1,...,N
j =i
xij = 1, i = 1, . . . , N,
∑
i=1,...,N
i=j
xij = 1, j = 1, . . . , N.
20 Introduction Chap. 1
There is one additional connectivity constraint:
the subgraph with node set N and arc set {(i, j) | xij = 1} is connected.
If this constraint was not present, the problem would be an ordinary assign-
ment problem. Unfortunately, this constraint is essential, since without it,
there would be feasible solutions involving multiple disconnected cycles.
Despite the similarity, the traveling salesman problem is far more dif-
ficult than the assignment problem. Solving problems having a mere few
hundreds of nodes can be very challenging. By contrast, assignment prob-
lems with hundreds of thousands of nodes can be solved in reasonable time
with the presently available methodology.
Actually, we have already described some discrete/combinatorial prob-
lems that fall within the framework of the minimum cost flow problem, such
as shortest path and assignment (cf. Examples 1.1 and 1.2). These prob-
lems require that the arc flows be 0 or 1, but, as mentioned earlier, we can
neglect these 0-1 constraints because it turns out that even if we relax them
and replace them with flow bound intervals [0, 1], we can obtain optimal
flows that are 0 or 1 (for a proof, see Section 5.2 or Exercise 1.34).
On the other hand, once we deviate from the minimum cost flow struc-
ture and we impose additional constraints or use a nonlinear cost function,
the integer character of optimal solutions is lost, and all integer constraints
must be explicitly imposed. This often complicates dramatically the so-
lution process, and in fact it may be practically impossible to obtain an
exactly optimal solution. As we will discuss in Chapter 10, there are sev-
eral approximate solution approaches that are based on simplified versions
of the problem, such as relaxing the integer constraints. These simpli-
fied problems can often be addressed with the efficient minimum cost flow
algorithms that we will develop in Chapters 2-7.
1.3 NETWORK FLOW ALGORITHMS – AN OVERVIEW
This section, which may be skipped without loss of continuity, provides
a broad classification of the various classes of algorithms for linear and
convex network optimization problems. It turns out that these algorithms
rely on just a few basic ideas, so they can be easily grouped in a few
major categories. By contrast, there is a much larger variety of algorithmic
ideas for discrete optimization problems. For this reason, we postpone the
corresponding discussion for Chapter 10.
Network optimization problems typically cannot be solved analyti-
cally. Usually they must be addressed computationally with one of several
available algorithms. One possibility, for linear and convex problems, is to
use a general purpose linear or nonlinear programming algorithm. How-
ever, the network structure can be exploited to speed up the solution by
Sec. 1.3 Network Flow Algorithms – An Overview 21
using either an adaptation of a general purpose algorithm such as the sim-
plex method, or by using a specialized network optimization algorithm. In
practice, network optimization problems can often be solved hundreds and
even thousands of times faster than general linear or convex programs of
comparable dimension.
The algorithms for linear and convex network problems that we will
discuss in this book can be grouped in three main categories:
(a) Primal cost improvement . Here we try to iteratively improve the
cost to its optimal value by constructing a corresponding sequence of
feasible flows.
(b) Dual cost improvement . Here we define a problem related to the orig-
inal network flow problem, called the dual problem, whose variables
are called prices. We then try to iteratively improve the dual cost to
its optimal value by constructing a corresponding sequence of prices.
Dual cost improvement algorithms also iterate on flows, which are
related to the prices through a property called complementary slack-
ness.
(c) Auction. Here we generate a sequence of prices in a way that is rem-
iniscent of real-life auctions. Strictly speaking, there is no primal or
dual cost improvement here, although we will show that auction can
be viewed as an approximate dual cost improvement process. In ad-
dition to prices, auction algorithms also iterate on flows, which are
related to prices through a property called -complementary slack-
ness; this is an approximate form of the complementary slackness
property mentioned above.
All of the preceding types of algorithms can be used to solve both
linear and convex network problems (although the structure of the given
problem may favor significantly the use of some types of methods over
others). For simplicity, in this chapter we will explain these ideas primarily
through the assignment problem, deferring a more detailed development to
subsequent chapters. Our illustrations, however, are relevant to the general
minimum cost flow problem and to its convex cost extensions. Some of our
explanations are informal. Precise statements of algorithms and results will
be given in subsequent chapters.
1.3.1 Primal Cost Improvement
Primal cost improvement algorithms for the minimum cost flow problem
start from an initial feasible flow vector and then generate a sequence of
feasible flow vectors, each having a better cost than the preceding one.
Let us derive an important characterization of the differences between suc-
cessive vectors, which is the basis for algorithms as well as for optimality
conditions.
22 Introduction Chap. 1
Let x and x be two feasible flow vectors, and consider their difference
z = x − x . This difference must be a circulation with components
zij = xij − xij ,
since both x and x are feasible. Furthermore, if the cost of x is smaller
than the cost of x, the circulation z must have negative cost, i.e.,∑
(i,j)∈A
aijzij < 0.
We can decompose z into the sum of simple cycle flows by using the confor-
mal realization theorem (Prop. 1.1). In particular, for some positive integer
K, we have
z =
K∑
k=1
wkξk,
where wk are positive scalars, and ξk are simple cycle flows whose nonzero
components ξkij are 1 or -1, depending on whether zij > 0 or zij < 0,
respectively. It is seen that the cost of z is∑
(i,j)∈A
aijzij =
K∑
k=1
wkck,
where ck is the cost of the simple cycle flow ξk. Thus, since the scalars wk
are positive, if the cost of z is negative, at least one ck must be negative.
Note that if Ck is the cycle corresponding to ξk, we have
ck =
∑
(i,j)∈A
aijξkij =
∑
(i,j)∈C+
k
aij −
∑
(i,j)∈C−
k
aij ,
where C+k and C
−
k are the sets of forward and backward arcs of the cycle
Ck, respectively. We refer to the expression in the right-hand side above
as the cost of the cycle Ck.
The preceding argument has shown that if x is feasible but not opti-
mal, and x is feasible and has smaller cost than x, then at least one of the
cycles corresponding to a conformal decomposition of the circulation x− x
as above has negative cost . This is used to prove the following important
optimality condition.
Proposition 1.2: Consider the minimum cost flow problem. A flow
vector x∗ is optimal if and only if x∗ is feasible and every simple cycle
C that is unblocked with respect to x∗ has nonnegative cost; that is,∑
(i,j)∈C+
aij −
∑
(i,j)∈C−
aij ≥ 0.
Sec. 1.3 Network Flow Algorithms – An Overview 23
Proof: Let x∗ be an optimal flow vector and let C be a simple cycle that
is unblocked with respect to x∗. Then there exists an  > 0 such that
increasing (decreasing) the flow of arcs of C+ (of C−, respectively) by 
results in a feasible flow that has cost equal to the cost of x∗ plus  times
the cost of C. Thus, since x∗ is optimal, the cost of C must be nonnegative.
Conversely, suppose, to arrive at a contradiction, that x∗ is feasible
and has the nonnegative cycle property stated in the proposition, but is not
optimal. Let x be a feasible flow vector with cost smaller that the one of
x∗, and consider a conformal decomposition of the circulation z = x − x∗.
From the discussion preceding the proposition, we see that there is a simple
cycle C with negative cost, such that x∗ij < xij for all (i, j) ∈ C+, and such
that x∗ij > xij for all (i, j) ∈ C−. Since x is feasible, we have bij ≤ xij ≤ cij
for all (i, j). It follows that x∗ij < cij for all (i, j) ∈ C+, and x∗ij > bij for
all (i, j) ∈ C−, so that C is unblocked with respect to x∗. This contradicts
the hypothesis that every simple cycle that is unblocked with respect to x∗
has nonnegative cost. Q.E.D.
Most primal cost improvement algorithms (including for example the
simplex method, to be discussed in Chapter 5) are based on the preceding
proposition. They employ various mechanisms to construct negative cost
cycles along which flow is pushed without violating the bound constraints.
The idea of improving the cost by pushing flow along a suitable cycle often
has an intuitive meaning as we illustrate in the context of the assignment
problem.
Example 1.7. Multi-Person Exchanges in Assignment
Consider the n × n assignment problem (cf. Example 1.2) and suppose that
we have a feasible assignment, that is, a set of n pairs (i, j) involving each
person i exactly once and each object j exactly once. In order to improve
this assignment, we could consider a two-person exchange, that is, replacing
two pairs (i1, j1) and (i2, j2) from the assignment with the pairs (i1, j2) and
(i2, j1). The resulting assignment will still be feasible, and it will have a
higher value if and only if
ai1j2 + ai2j1 > ai1j1 + ai2j2 .
We note here that, in the context of the minimum cost flow representation of
the assignment problem, a two-person exchange can be identified with a cycle
involving the four arcs (i1, j1), (i2, j2), (i1, j2), and (i2, j1). Furthermore, this
cycle is the difference between the assignment before and the assignment after
the exchange, while the preceding inequality is equivalent to the cycle having
a positive value.
Unfortunately, it may be impossible to improve the current assignment
by a two-person exchange, even if the assignment is not optimal; see Fig.
1.8. An improvement, however, is possible by means of a k-person exchange,
for some k ≥ 2, where a set of pairs (i1, j1), . . . , (ik, jk) from the current as-
signment is replaced by the pairs (i1, j2), . . . , (ik−1, jk), (ik, j1). To see this,
24 Introduction Chap. 1
1 1
3 3
1 1
2
1
2
1
1 1
1
Value = 1
0
2
2
1
0
0
2
1
Figure 1.8: An example of a nonoptimal
feasible assignment that cannot be improved
by a two-person exchange. The value of
each pair is shown next to the correspond-
ing arc. Here, the value of the assignment
{(1, 1), (2, 2), (3, 3)} is left unchanged at 3
by any two-person exchange. Through a
three-person exchange, however, we obtain
the optimal assignment, {(1, 2), (2, 3), (3, 1)},
which has value 6.
1
3
2
11
3
2
Figure 1.9: Illustration of the correspon-
dence of a k-person exchange to a simple
cycle. This is the same example as in the
preceding figure. The backward arcs of the
cycle are (1, 1), (2, 2), and (3, 3), and corre-
spond to the current assignment pairs. The
forward arcs of the cycle are (1, 2), (2, 3),
and (3, 1), and correspond to the new as-
signment pairs. This three-person exchange
is value-improving because the sum of the
values of the forward arcs (2 + 2 + 2) is
greater than the sum of the values of the
backward arcs (1 + 1 + 1).
note that in the context of the minimum cost flow representation of the as-
signment problem, a k-person exchange corresponds to a simple cycle with
k forward arcs (corresponding to the new assignment pairs) and k backward
arcs (corresponding to the current assignment pairs that are being replaced);
see Fig. 1.9. Thus, performing a k-person exchange is equivalent to pushing
one unit of flow along the corresponding simple cycle. The k-person exchange
improves the assignment if and only if
aikj1 +
k−1∑
m=1
aimjm+1 −
k∑
m=1
aimjm ,
which is equivalent to the corresponding cycle having positive value. Further-
more, by Prop. 1.2, a cost improving cycle exists if the flow corresponding to
the current assignment is not optimal.
1.3.2 Dual Cost Improvement
Duality theory deals with the relation between the original network opti-
mization problem and another optimization problem called the dual . To
develop an intuitive understanding of duality, we will focus on an n×n as-
signment problem (cf. Example 1.2) and consider a closely related economic
equilibrium problem. In particular, let us consider matching the n objects
Sec. 1.3 Network Flow Algorithms – An Overview 25
with the n persons through a market mechanism, viewing each person as
an economic agent acting in his/her own best interest. Suppose that object
j has a price pj and that the person who receives the object must pay the
price pj . Then the net value of object j for person i is aij − pj , and each
person i will logically want to be assigned to an object ji with maximal
value, that is, with
aiji − pji = max
j∈A(i)
{aij − pj}, (1.10)
where
A(i) =
{
j | (i, j) ∈ A
}
is the set of objects that can be assigned to person i. When this condition
holds for all persons i, we say that the assignment and the price vector
p = (p1, . . . , pn) satisfy complementary slackness (CS for short); this name
is standard in linear programming. The economic system is then at equi-
librium, in the sense that no person would have an incentive to unilaterally
seek another object. Such equilibrium conditions are naturally of great
interest to economists, but there is also a fundamental relation with the
assignment problem. We have the following proposition.
Proposition 1.3: If a feasible assignment and a set of prices satisfy
the complementary slackness condition (1.10) for all persons i, then
the assignment is optimal and the prices are an optimal solution of
a dual problem, which is to minimize over p = (p1, . . . , pn) the cost
function
n∑
i=1
qi(p) +
n∑
j=1
pj ,
where the functions qi are given by
qi(p) = max
j∈A(i)
{
aij − pj
}
, i = 1, . . . , n.
Furthermore, the value of the optimal assignment and the optimal cost
of the dual problem are equal.
Proof: The total value of any feasible assignment {(i, ki) | i = 1, . . . , n}
satisfies
n∑
i=1
aiki ≤
n∑
i=1
max
j∈A(i)
{
aij − pj
}
+
n∑
j=1
pj , (1.11)
26 Introduction Chap. 1
for any set of prices {pj | j = 1, . . . , n}, since the first term of the right-hand
side is no less than
n∑
i=1
(aiki − pki) ,
while the second term is equal to
∑n
i=1 pki . On the other hand, the given
assignment and set of prices, denoted by {(i, ji) | i = 1, . . . , n} and {pj |
j = 1, . . . , n}, respectively, satisfy the CS conditions, so we have
aiji − pji = maxj∈A(i){aij − pj}, i = 1, . . . , n.
By adding this relation over all i, we have
n∑
i=1
(
max
j∈A(i)
{
aij − pj
}
+ pji
)
=
n∑
i=1
aiji
and by using Eq. (1.11), we obtain
n∑
i=1
aiki ≤
n∑
i=1
(
max
j∈A(i)
{
aij − pj
}
+ pji
)
=
n∑
i=1
aiji
≤
n∑
i=1
max
j∈A(i)
{
aij − pj
}
+
n∑
j=1
pj ,
for every feasible assignment {(i, ki) | i = 1, . . . , n} and every set of prices
{pj | j = 1, . . . , n}. Therefore, the assignment {(i, ji) | i = 1, . . . , n} is
optimal for the primal problem, and the set of prices {pj | j = 1, . . . , n}
is optimal for the dual problem. Furthermore, the two optimal values are
equal. Q.E.D.
In analogy with primal cost improvement algorithms, one may start
with a price vector and try to successively obtain new price vectors with
improved dual cost. The major algorithms of this type involve price changes
of the form
pi :=
{
pi + γ if i ∈ S,
pi if i /∈ S, (1.12)
where S is a connected subset of nodes, and γ is some positive scalar that
is small enough to ensure that the new price vector has an improved dual
cost.
The existence of a node subset S that results in cost improvement at
a nonoptimal price vector, as described above, will be shown in Chapter 6.
Sec. 1.3 Network Flow Algorithms – An Overview 27
This is an important and remarkable result, which may be viewed as a dual
version of the result of Prop. 1.2 (at a nonoptimal flow vector, there exists
at least one unblocked simple cycle with negative cost). In fact both results
are special cases of a more general theorem concerning elementary vectors
of subspaces, which is central in the theory of monotropic programming
(see Chapter 9).
Most dual cost improvement methods, simultaneously with changing
p along a direction of dual cost improvement, also iterate on a flow vector
x satisfying CS together with p. They terminate when x becomes feasible,
at which time, by Prop. 1.3, the pair (x, p) must consist of a primal and a
dual optimal solution.
In Chapter 6 we will discuss two main methods that select subsets S
and corresponding directions of dual cost improvement in different ways:
(a) In the primal-dual method , the direction has a steepest ascent prop-
erty , that is, it provides the maximal rate of improvement of the dual
cost per unit change in the price vector.
(b) In the relaxation (or coordinate ascent) method , the direction is com-
puted so that it has a small number of nonzero elements (i.e., the set
S has few nodes). Such a direction may not be optimal in terms of
rate of dual cost improvement, but can typically be computed much
faster than the steepest ascent direction. Often the direction has only
one nonzero element, in which case only one node price coordinate is
changed; this motivates the name “coordinate ascent.” Note, how-
ever, that coordinate ascent directions cannot be used exclusively to
improve the dual cost, as is shown in Fig. 1.10.
1.3.3 Auction
Our third type of algorithm represents a significant departure from the
cost improvement idea; at any one iteration, it may deteriorate both the
primal and the dual cost, although in the end it does find an optimal primal
solution. It is based on an approximate version of complementary slackness,
called -complementary slackness, and while it implicitly tries to solve a
dual problem, it actually attains a dual solution that is not quite optimal.
This subsection introduces the main ideas underlying auction algorithms.
Chapters 7 and 9 provide a detailed discussion for the minimum cost flow
problem and for the separable convex cost problem, respectively.
Naive Auction
Let us return to the assignment problem, and consider a natural process
for finding an equilibrium assignment and price vector. We will call this
process the naive auction algorithm, because it has a serious flaw, as will be
28 Introduction Chap. 1
2
Surfaces of Equal 
Dual Cost
p1
p
(a)
Surfaces of Equal 
Dual Cost
2p
(b)
p1
Figure 1.10: (a) The difficulty with
using exclusively coordinate ascent it-
erations to solve the dual problem.
Because the dual cost is piecewise lin-
ear, it may be impossible to improve
it at some corner points by chang-
ing any single price coordinate. (b)
As will be discussed in Chapter 6, a
dual cost improvement is possible by
changing several price coordinates by
equal amounts, as in Eq. (1.12).
seen shortly. Nonetheless, this flaw will help motivate a more sophisticated
and correct algorithm.
The naive auction algorithm proceeds in iterations and generates a
sequence of price vectors and partial assignments. By a partial assignment
we mean an assignment where only a subset of the persons have been
matched with objects. A partial assignment should be contrasted with a
feasible or complete assignment where all the persons have been matched
with objects on a one-to-one basis. At the beginning of each iteration, the
CS condition [cf. Eq. (1.10)]
aiji − pji = max
j∈A(i)
{aij − pj}
is satisfied for all pairs (i, ji) of the partial assignment. If all persons
are assigned, the algorithm terminates. Otherwise some person who is
unassigned, say i, is selected. This person finds an object ji which offers
maximal value, that is,
ji = arg max
j∈A(i)
{aij − pj},
and then:
(a) Gets assigned to the best object ji; the person who was assigned to
ji at the beginning of the iteration (if any) becomes unassigned.
Sec. 1.3 Network Flow Algorithms – An Overview 29
(b) Sets the price of ji to the level at which he/she is indifferent between
ji and the second best object; that is, he/she sets pji to
pji + γi,
where
γi = vi − wi, (1.13)
vi is the best object value,
vi = max
j∈A(i)
{aij − pj}, (1.14)
and wi is the second best object value,
wi = max
j∈A(i), j =ji
{aij − pj}. (1.15)
(Note that as pji is increased, the value aiji − pji offered by object ji
to person i is decreased. γi is the largest increment by which pji can
be increased, while maintaining the property that ji offers maximal
value to i.)
This process is repeated in a sequence of iterations until each person has
been assigned to an object.
We may view this process as an auction where at each iteration the
bidder i raises the price of a preferred object by the bidding increment γi.
Note that γi cannot be negative, since vi ≥ wi [compare Eqs. (1.14)and
(1.15)], so the object prices tend to increase. The choice γi is illustrated
in Fig. 1.11. Just as in a real auction, bidding increments and price in-
creases spur competition by making the bidder’s own preferred object less
attractive to other potential bidders.
-Complementary Slackness
Unfortunately, the naive auction algorithm does not always work (although
it is an excellent initialization procedure for other methods, such as primal-
dual or relaxation, and it is useful in other specialized contexts). The diffi-
culty is that the bidding increment γi is 0 when two or more objects are tied
in offering maximum value for the bidder i. As a result, a situation may be
created where several persons contest a smaller number of equally desirable
objects without raising their prices, thereby creating a never ending cycle;
see Fig. 1.12.
To break such cycles, we introduce a perturbation mechanism, moti-
vated by real auctions where each bid for an object must raise its price by
a minimum positive increment, and bidders must on occasion take risks to
win their preferred objects. In particular, let us fix a positive scalar , and
30 Introduction Chap. 1
w   : The value of the second best object for person ii
v    : The value of j  , the best object for person ii i
Values 
of objects j
for person i
a   - pij j
- - - - - - - - - - - - - 
- - - - - - - - - - - - -  
γiBidding increment     of person i for its best
object j 
i
Figure 1.11: In the naive auction algorithm, even after the price of the best
object ji is increased by the bidding increment γi, ji continues to be the best
object for the bidder i, so CS is satisfied at the end of the iteration. However, we
have γi = 0 if there is a tie between two or more objects that are most preferred
by i.
say that a partial assignment and a price vector p satisfy -complementary
slackness (-CS for short) if
aij − pj ≥ max
k∈A(i)
{aik − pk} − 
for all assigned pairs (i, j). In words, to satisfy -CS, all assigned persons
of the partial assignment must be assigned to objects that are within  of
being best.
The Auction Algorithm
We now reformulate the previous auction process so that the bidding in-
crement is always at least equal to . The resulting method, the auction
algorithm, is the same as the naive auction algorithm, except that the
bidding increment γi is
γi = vi − wi +  (1.16)
rather than γi = vi − wi as in Eq. (1.13). With this choice, the -CS
condition is satisfied, as illustrated in Fig. 1.13. The particular increment
γi = vi−wi + used in the auction algorithm is the maximum amount with
this property. Smaller increments γi would also work as long as γi ≥ ,
but using the largest possible increment accelerates the algorithm. This
is consistent with experience from real auctions, which tend to terminate
faster when the bidding is aggressive.
Sec. 1.3 Network Flow Algorithms – An Overview 31
Initial price = 0
Here a   = C > 0 for all (i,j) with i  = 1,2,3 and j = 1,2
and a   = 0 for all (i,j) with i = 1,2,3 and j = 3
ij
ij
PERSONS OBJECTS
1
2
3 Initial price = 0
Initial price = 0Initially assigned
to object 1
Initially
unassigned
Initially assigned
to object 2
1
2
3
At Start of Object Assigned Bidder Preferred Bidding
Iteration # Prices Pairs Object Increment
1 0,0,0 (1,1), (2,2) 3 2 0
2 0,0,0 (1,1), (3,2) 2 2 0
3 0,0,0 (1,1), (2,2) 3 2 0
Figure 1.12: Illustration of how the naive auction algorithm may never terminate
for a problem involving three persons and three objects. Here objects 1 and 2
offer benefit C > 0 to all persons, and object 3 offers benefit 0 to all persons. The
algorithm cycles as persons 2 and 3 alternately bid for object 2 without changing
its price because they prefer equally object 1 and object 2.
It can be shown that this reformulated auction process terminates,
necessarily with a feasible assignment and a set of prices that satisfy -
CS. To get a sense of this, note that if an object receives a bid during
m iterations, its price must exceed its initial price by at least m. Thus,
for sufficiently large m, the object will become “expensive” enough to be
judged “inferior” to some object that has not received a bid so far. It follows
that only for a limited number of iterations can an object receive a bid while
some other object still has not yet received any bid. On the other hand,
once every object has received at least one bid, the auction terminates.
(This argument assumes that any person can bid for any object, but it can
be generalized to the case where the set of feasible person-object pairs is
limited, as long as at least one feasible assignment exists; see Prop. 7.2 in
Chapter 7.) Figure 1.14 shows how the auction algorithm, based on the
bidding increment γi = vi − wi +  [see Eq. (1.16)], overcomes the cycling
difficulty in the example of Fig. 1.12.
When the auction algorithm terminates, we have an assignment sat-
isfying -CS, but is this assignment optimal? The answer depends strongly
32 Introduction Chap. 1
ij
a   - p
jValues 
of objects j
 for person i
- - - - - - - - - - - - - 
- - - - - - - - - - - - -  
- - - - - - - - - - - - 
w   : The value of the second best object for person ii
ε
γ
iBidding increment     of person i for its best
object j 
i
v    : The value of j  , the best object for person i
i i
Figure 1.13: In the auction algorithm, even after the price of the preferred
object ji is increased by the bidding increment γi, ji will be within  of being
most preferred, so the -CS condition holds at the end of the iteration.
on the size of . In a real auction, a prudent bidder would not place an
excessively high bid for fear of winning the object at an unnecessarily high
price. Consistent with this intuition, we can show that if  is small, then
the final assignment will be “almost optimal.” In particular, we will show
that the total benefit of the final assignment is within n of being optimal .
The idea is that a feasible assignment and a set of prices satisfying -CS
may be viewed as satisfying CS for a slightly different problem, where all
benefits aij are the same as before except the benefits of the n assigned
pairs, which are modified by no more than .
Proposition 1.4: A feasible assignment satisfying -complementary
slackness, together with some price vector, attains within n the opti-
mal primal value. Furthermore, the price vector attains within n the
optimal dual cost.
Proof: Let A∗ be the optimal total assignment benefit
A∗ = max
ki, i=1,...,n
ki =km if i=m
n∑
i=1
aiki
and let D∗ be the optimal dual cost (cf. Prop. 1.3):
D∗ = min
pj
j=1,...,n

n∑
i=1
max
j∈A(i)
{
aij − pj
}
+
n∑
j=1
pj
 .
Sec. 1.3 Network Flow Algorithms – An Overview 33
Initial price = 0
Here a   = C > 0 for all (i,j) with i  = 1,2,3 and j = 1,2
and a   = 0 for all (i,j) with i = 1,2,3 and j = 3
ij
ij
PERSONS OBJECTS
1
2
3 Initial price = 0
Initial price = 0Initially assigned
to object 1
Initially
unassigned
Initially assigned
to object 2
1
2
3
At Start of Object Assigned Bidder Preferred Bidding
Iteration # Prices Pairs Object Increment
1 0,0,0 (1,1), (2,2) 3 2 
2 0,,0 (1,1), (3,2) 2 1 2
3 2,,0 (2,1), (3,2) 1 2 2
4 2,3,0 (1,2), (2,1) 3 1 2
5 4,3,0 (1,2), (3,1) 2 2 2
6 · · · · · · · · · · · · · · ·
Figure 1.14: Illustration of how the auction algorithm, by making the bidding
increment at least , overcomes the cycling difficulty for the example of Fig. 1.12.
The table shows one possible sequence of bids and assignments generated by
the auction algorithm, starting with all prices equal to 0 and with the partial
assignment {(1, 1), (2, 2)}. At each iteration except the last, the person assigned
to object 3 bids for either object 1 or 2, increasing its price by  in the first iteration
and by 2 in each subsequent iteration. In the last iteration, after the prices of 1
and 2 reach or exceed C, object 3 receives a bid and the auction terminates.
If {(i, ji) | i = 1, . . . , n} is the given assignment satisfying the -CS condi-
tion together with a price vector p, we have
max
j∈A(i)
{aij − pj} −  ≤ aiji − pji .
By adding this relation over all i, we see that
D∗ ≤
n∑
i=1
(
max
j∈A(i)
{
aij − pj
}
+ pji
)
≤
n∑
i=1
aiji + n ≤ A∗ + n.
Since we showed in Prop. 1.3 that A∗ = D∗, it follows that the total
assignment benefit
∑n
i=1 aiji is within n of the optimal value A∗, while
the dual cost of p is within n of the optimal dual cost. Q.E.D.
34 Introduction Chap. 1
Suppose now that the benefits aij are all integer, which is the typical
practical case. (If aij are rational numbers, they can be scaled up to integer
by multiplication with a suitable common number.) Then the total benefit
of any assignment is integer, so if n < 1, any complete assignment that is
within n of being optimal must be optimal. It follows that if
 <
1
n
and the benefits aij are all integer, then the assignment obtained upon ter-
mination of the auction algorithm is optimal .
Figure 1.15 shows the sequence of generated object prices for the ex-
ample of Fig. 1.12 in relation to the contours of the dual cost function.
It can be seen from this figure that each bid has the effect of setting the
price of the object receiving the bid nearly equal (within ) to the price
that minimizes the dual cost with respect to that price, with all other
prices held fixed (this will be shown rigorously in Section 7.1). Successive
minimization of a cost function along single coordinates is a central fea-
ture of coordinate descent and relaxation methods, which are popular for
unconstrained minimization of smooth functions and for solving systems
of smooth equations. Thus, the auction algorithm can be interpreted as
an approximate coordinate descent method; as such, it is related to the
relaxation method discussed in the previous subsection.
Scaling
Figure 1.15 also illustrates a generic feature of auction algorithms. The
amount of work needed to solve the problem can depend strongly on the
value of  and on the maximum absolute object benefit
C = max
(i,j)∈A
|aij |.
Basically, for many types of problems, the number of iterations up to termi-
nation tends to be proportional to C/. This can be seen from the figure,
where the total number of iterations is roughly C/, starting from zero
initial prices.
Note also that there is a dependence on the initial prices; if these
prices are “near optimal,” we expect that the number of iterations needed
to solve the problem will be relatively small. This can be seen from Fig.
1.15; if the initial prices satisfy p1 ≈ p3 + C and p2 ≈ p3 + C, the number
of iterations up to termination is quite small.
The preceding observations suggest the idea of -scaling, which con-
sists of applying the algorithm several times, starting with a large value of
 and successively reducing  until it is less than some critical value (for
example, 1/n, when aij are integer). Each application of the algorithm pro-
vides good initial prices for the next application. This is a common idea
Sec. 1.3 Network Flow Algorithms – An Overview 35
C
ε
ε
ε
ε
ε
Contours of the
dual function
p
1
ε
C
p
2
Price p   is fixed at 03
Figure 1.15: A sequence of prices p1 and p2 generated by the auction algorithm
for the example of Figs. 1.12 and 1.14. The figure shows the equal dual cost
surfaces in the space of p1 and p2, with p3 fixed at 0. The arrows indicate the
price iterates as given by the table of Fig. 1.14. Termination occurs when the prices
reach an -neighborhood of the point (C, C), and object 3 becomes “sufficiently
inexpensive” to receive a bid and to get assigned. The total number of iterations
is roughly C/, starting from zero initial prices.
in nonlinear programming; it is encountered, for example, in barrier and
penalty function methods (see Section 8.8). In practice, scaling is typically
beneficial, and accelerates the termination of the auction algorithm.
1.3.4 Good, Bad, and Polynomial Algorithms
We have discussed several types of methods, so the natural question arises:
is there a best method and what criterion should we use to rank methods?
A practitioner who has a specific type of problem to solve, perhaps
repeatedly, with the data and size of the problem within some limited range,
will usually be interested in one or more of the following:
(a) Fast solution time.
(b) Flexibility to use good starting solutions (which the practitioner can
usually provide, based on his/her knowledge of the problem, or based
on a known solution of some similar problem).
36 Introduction Chap. 1
(c) The ability to perform sensitivity analysis (resolve the problem with
slightly different problem data) quickly.
(d) The ability to take advantage of parallel computing hardware.
Given the diversity of these considerations, it is not surprising that
there is no algorithm that will dominate the others in all or even most
practical situations. Otherwise expressed, every type of algorithm that we
will discuss is best given the right type of practical situation. Thus, to
make intelligent choices, the practitioner needs to understand the proper-
ties of different algorithms relating to speed of convergence, flexibility, par-
allelization, and suitability for specific problem structures. For challenging
problems, the choice of algorithm is often settled by experimentation with
several candidates.
A theoretical analyst may also have difficulty ranking different algo-
rithms for specific types of problems. The most common approach for this
purpose is worst-case computational complexity analysis. For example, for
the minimum cost flow problem, one tries to bound the number of elemen-
tary numerical operations needed by a given algorithm with some measure
of the “problem size,” that is, with some expression of the form
Kf(N, A, C, U, S),
where
N is the number of nodes,
A is the number of arcs,
C is the arc cost range max(i,j)∈A |aij |,
U is the maximum arc flow range max(i,j)∈A(cij − bij),
S is the supply range maxi∈N |si|,
f is some known function,
K is a (usually unknown) constant.
If a bound of this form can be found, we say that the running time or
operation count of the algorithm is O
(
f(N, A, C, U, S)
)
. If f(N, A, C, U, S)
can be written as a polynomial function of the number of bits needed to
express the problem data, the algorithm is said to be polynomial . Exam-
ples of polynomial complexity bounds are O
(
NαAβ
)
and O
(
NαAβ log C
)
,
where α and β are positive integers, and the numbers aij are assumed in-
teger. The bound O
(
NαAβ
)
is sometimes said to be strongly polynomial
because it involves only the graph size parameters. A bound of the form
O
(
NαAβC
)
is not polynomial, even assuming that the aij are integer, be-
cause C is not a polynomial expression of log C, the number of bits needed
to express a single number aij . Bounds like O
(
NαAβC
)
, which are poly-
nomial in the problem data rather than in the number of bits needed to
express the data, are called pseudopolynomial .
Sec. 1.4 Notes, Sources, and Exercises 37
A common assumption in theoretical computer science is that poly-
nomial algorithms are “better” than pseudopolynomial, and pseudopoly-
nomial algorithms are “better” than exponential [for example, those with
a bound of the form K2g(N,A), where g is a polynomial in N and A]. Fur-
thermore, it is thought that two polynomial algorithms can be compared in
terms of the degree of the polynomial bound; e.g., an O(N2) algorithm is
“better” than an O(N3) algorithm. Unfortunately, quite often this assump-
tion is not supported by computational practice in linear programming and
network optimization. Pseudopolynomial and even exponential algorithms
are often faster in practice than polynomial ones. In fact, the simplex
method for general linear programs is an exponential algorithm, as shown
by Klee and Minty [1972] (see also the textbooks by Chvatal [1983], or
Bertsimas and Tsitsiklis [1997]), and yet it is used widely, because of its
excellent practical properties.
There are two main reasons why worst-case complexity estimates may
fail to predict the practical performance of network flow algorithms. First,
the estimates, even if they are tight, may be very pessimistic as they may
correspond to problem instances that are highly unlikely in practice. (Av-
erage complexity estimates would be more appropriate for such situations.
However, obtaining these is usually hard, and the statistical assumptions
underlying them may be inappropriate for many types of practical prob-
lems.) Second, worst-case complexity estimates involve the (usually un-
known) constant K, which may dominate the estimate for all except for
unrealistically large problem sizes. Thus, a comparison between two algo-
rithms that is based on the size-dependent terms of running time estimates,
and does not take into account the corresponding constants may be unre-
liable.
Despite its shortcomings, computational complexity analysis is valu-
able because it often illuminates the computational bottlenecks of many al-
gorithms and motivates the use of efficient data structures. For this reason,
throughout the book, we will comment on available complexity results, we
will prove some of the most important estimates, and we will try to relate
these estimates to computational practice. For some classes of problems,
however, it turns out that the methods with the best computational com-
plexity are impractical, because they are either too complicated or too slow
in practice. In such cases, we will refer to the literature, without providing
a detailed discussion.
1.4 NOTES, SOURCES, AND EXERCISES
Network problems are discussed in many books (Berge [1962], Berge and
Ghouila-Houri [1962], Ford and Fulkerson [1962], Dantzig [1963], Busacker
and Saaty [1965], Hu [1969], Iri [1969], Frank and Frisch 1970], Christofides
38 Introduction Chap. 1
[1975], Zoutendijk [1976], Minieka [1978], Jensen and Barnes [1980], Ken-
nington and Helgason [1980], Papadimitriou and Steiglitz [1982], Chvatal
[1983], Gondran and Minoux [1984], Luenberger [1984], Rockafellar [1984],
Bazaraa, Jarvis, and Sherali [1990], Bertsekas [1991a], Murty [1992], Bert-
simas and Tsitsiklis [1997]). Several of these books discuss linear program-
ming first and develop linear network optimization as a special case. An
alternative approach that relies heavily on duality, is given by Rockafellar
[1984]. The conformal realization theorem (Prop. 1.1) has been developed
in different forms in several sources, including Ford and Fulkerson [1962],
Busacker and Saaty [1965], and Rockafellar [1984].
The primal cost improvement approach for network optimization was
initiated by Dantzig [1951], who specialized the simplex method to the
transportation problem. The extensive subsequent work using this ap-
proach is surveyed at the end of Chapter 5.
The dual cost improvement approach was initiated by Kuhn [1955]
who proposed the Hungarian method for the assignment problem. (The
name of the algorithm honors its connection with the research of the Hun-
garian mathematicians Egervary [1931] and König [1931].) Work using this
approach is surveyed in Chapter 6.
The auction approach was initiated in Bertsekas [1979a] for the as-
signment problem, and in Bertsekas [1986a], [1986b] for the minimum cost
flow problem. Work using this approach is surveyed at the end of Chapter
7.
E X E R C I S E S
1.1
Consider the graph and the flow vector of Fig. 1.16.
(a) Enumerate the simple paths and the simple forward paths that start at
node 1.
(b) Enumerate the simple cycles and the simple forward cycles of the graph.
(c) Is the graph connected? Is it strongly connected?
(d) Calculate the divergences of all the nodes and verify that they add to 0.
(e) Give an example of a simple path flow that starts at node 1, ends at node
5, involves four arcs, and conforms to the given flow vector.
(f) Suppose that all arcs have arc flow bounds -1 and 5. Enumerate all the
simple paths that start at node 1, end at node 5, and are unblocked with
Sec. 1.4 Notes, Sources, and Exercises 39
respect to the given flow vector.
1 4
3
2
1
-1
-1
2
3
2 52
5
1
Figure 1.16: Flow vector for Ex-
ercise 1.1. The arc flows are the
numbers shown next to the arcs.
1.2 (Proof of the Conformal Realization Theorem)
Prove the conformal realization theorem (Prop. 1.1) by completing the details
of the following argument. Assume first that x is a circulation. Consider the
following procedure by which given x, we obtain a simple cycle flow x′ that
conforms to x and satisfies
0 ≤ x′ij ≤ xij for all arcs (i, j) with 0 ≤ xij ,
xij ≤ x′ij ≤ 0 for all arcs (i, j) with xij ≤ 0,
xij = x
′
ij for at least one arc (i, j) with xij = 0;
(see Fig. 1.17). Choose an arc (i, j) with xij = 0. Assume that xij > 0. (A
similar procedure can be used when xij < 0.) Construct a sequence of node
subsets T0, T1, . . ., as follows: Take T0 = {j}. For k = 0, 1, . . ., given Tk, let
Tk+1 =
{
n /∈ ∪kp=0Tp | there is a node m ∈ Tk, and either an arc (m, n)
such that xmn > 0 or an arc (n, m) such that xnm < 0
}
,
and mark each node n ∈ Tk+1 with the label “(m, n)” or “(n, m),” where m
is a node of Tk such that xmn > 0 or xnm < 0, respectively. The procedure
terminates when Tk+1 is empty.
At the end of the procedure, trace labels backward from i until node j is
reached. (How do we know that i belongs to one of the sets Tk?) In particular,
let “(i1, i)” or “(i, i1)” be the label of i, let “(i2, i1)” or “(i1, i2)” be the label
of i1, etc., until a node ik with label “(ik, j)” or “(j, ik)” is found. The cycle
C = (j, ik, ik−1, . . . , i1, i, j) is simple, it contains (i, j) as a forward arc, and is
such that all its forward arcs have positive flow and all its backward arcs have
negative flow. Let a = min(m,n)∈C |xmn| > 0. Then the simple cycle flow x′,
where
x′ij =
{
a if (i, j) ∈ C+,
−a if (i, j) ∈ C−,
0 otherwise,
has the required properties.
Now subtract x′ from x. We have xij − x′ij > 0 only for arcs (i, j) with
xij > 0, xij − x′ij < 0 only for arcs (i, j) with xij < 0, and xij − x′ij = 0 for at
40 Introduction Chap. 1
T
T
0
T1
2
Tk
j
i
m
n
i1
i2
ik
ik-1
ik-2
C
x   > 0ij
Flow > 0
Flow < 0
Flow < 0
Flow < 0
Flow > 0
Flow > 0
Flow > 0
Flow > 0
Figure 1.17: Construction of a cycle of arcs with nonzero flow used in the proof
of the conformal realization theorem.
least one arc (i, j) with xij = 0. If x is integer, then x′ and x − x′ will also be
integer. We then repeat the process (for at most A times) with the circulation x
replaced by the circulation x − x′ and so on, until the zero flow is obtained.
If x is not a circulation, we form an enlarged graph by introducing a new
node s and by introducing for each node i ∈ N an arc (s, i) with flow xsi equal
to the divergence yi. The resulting flow vector is seen to be a circulation in the
enlarged graph (why?). This circulation, by the result just shown, can be decom-
posed into at most A + N simple cycle flows of the enlarged graph, conforming
to the flow vector. Out of these cycle flows, we consider those containing node
s, and we remove s and its two incident arcs while leaving the other cycle flows
unchanged. As a result we obtain a set of at most A+N path flows of the original
graph, which add up to x. These path flows also conform to x, as required.
1.3
Use the algorithm of Exercise 1.2 to decompose the flow vector of Fig. 1.16 into
conforming simple path flows.
1.4 (Path Decomposition Theorem)
(a) Use the conformal realization theorem (Prop. 1.1) to show that a forward
path P can be decomposed into a (possibly empty) collection of simple
forward cycles, together with a simple forward path that has the same
start node and end node as P . (Here “decomposition” means that the
Sec. 1.4 Notes, Sources, and Exercises 41
union of the arcs of the component paths is equal to the set of arcs of P
with the multiplicity of repeated arcs properly accounted for.)
(b) Suppose that a graph is strongly connected and that a length aij is given for
every arc (i, j). Show that if all forward cycles have nonnegative length,
then there exists a shortest path from any node s to any node t. Show
also that if there exists a shortest path from some node s to some node t,
then all forward cycles have nonnegative length. Why is the connectivity
assumption needed?
1.5 (Cycle Decomposition - Euler Cycles)
Consider a graph such that each of the nodes has even degree.
(a) Give an algorithm to decompose the graph into a collection of simple cycles
that are disjoint, in the sense that they share no arcs (although they may
share some nodes). (Here “decomposition” means that the union of the
arcs of the component cycles is equal to the set of arcs of the graph.) Hint :
Given a connected graph where each of the nodes has even degree, the
deletion of the arcs of any cycle creates some connected subgraphs where
each of the nodes has even degree (including possibly some isolated nodes).
(b) Assume in addition that the graph is connected. Show that there is an
Euler cycle, i.e., a cycle that contains all the arcs of a graph exactly once.
Hint : Apply the decomposition of part (a), and successively merge an Euler
cycle of a subgraph with a simple cycle.
1.6
In the graph of Fig. 1.16, consider the graph obtained by deleting node 1 and
arcs (1, 2), (1, 3), and (5, 4). Decompose this graph into a collection of simple
cycles that are disjoint (cf. Exercise 1.5) and construct an Euler cycle.
1.7
(a) Consider an n × n chessboard, and a rook that is allowed to make the
standard moves along the rows and columns. Show that the rook can start
at a given square and return to that square after making each of the possible
legal moves exactly once and in one direction only [of the two moves (a, b)
and (b, a) only one should be made]. Hint : Construct an Euler cycle in a
suitable graph.
(b) Consider an n× n chessboard with n even, and a bishop that is allowed to
make two types of moves: legal moves (which are the standard moves along
the diagonals of its color), and illegal moves (which go from any square of
its color to any other square of its color). Show that the bishop can start at
a given square and return to that square after making each of the possible
legal moves exactly once and in one direction only, plus n2/4 illegal moves.
42 Introduction Chap. 1
For every square of its color, there should be exactly one illegal move that
either starts or ends at that square.
1.8 (Forward Euler Cycles)
Consider a graph and the question whether there exists a forward cycle that
passes through each arc of the graph exactly once. Show that such a cycle exists
if and only if the graph is connected and the number of incoming arcs to each
node is equal to the number of outgoing arcs from the node.
1.9
Consider an n × n chessboard with n ≥ 4. Show that a knight starting at any
square can visit every other square, with a move sequence that contains every
possible move exactly once [a move (a, b) as well as its reverse (b, a) should be
made]. Interpret this sequence as a forward Euler cycle in a suitable graph (cf.
Exercise 1.8).
1.10 (Euler Paths)
Consider a graph and the question whether there exists a path that passes through
each arc of the graph exactly once. Show that such a path exists if and only if
the graph is connected, and either the degrees of all the nodes are even, or else
the degrees of all the nodes except two are even.
1.11
In shatranj, the old version of chess, the firz (or vizier, the predecessor to the
modern queen) can move one square diagonally in each direction. Show that
starting at a corner of an n × n chessboard where n is even, the firz can reach
the opposite corner after making each of the possible moves along its diagonals
exactly once and in one direction only [of the two moves (a, b) and (b, a) only one
should be made].
1.12
Show that the number of nodes with odd degree in a graph is even.
1.13
Assume that all the nodes of a graph have degree greater than one. Show that
the graph must contain a cycle.
Sec. 1.4 Notes, Sources, and Exercises 43
1.14
(a) Show that every tree with at least two nodes has at least two nodes with
degree one.
(b) Show that a graph is a tree if and only if it is connected and the number
of arcs is one less than the number of nodes.
1.15
Consider a volleyball net that consists of a mesh with m squares on the horizontal
dimension and n squares on the vertical. What is the maximum number of strings
that can be cut before the net falls apart into two pieces.
1.16 (Checking Connectivity)
Consider a graph with A arcs.
(a) Devise an algorithm with O(A) running time that checks whether the graph
is connected, and if it is connected, simultaneously constructs a path con-
necting any two nodes. Hint : Start at a node, mark its neighbors, and
continue.
(b) Repeat part (a) for the case where we want to check strong connectedness.
(c) Devise an algorithm with O(A) running time that checks whether there
exists a cycle that contains two given nodes.
(d) Repeat part (c) for the case where the cycle is required to be forward.
1.17 (Inequality Constrained Minimum Cost Flows)
Consider the following variant of the minimum cost flow problem:
minimize
∑
(i,j)∈A
aijxij
subject to si ≤
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji ≤ si, ∀ i ∈ N ,
bij ≤ xij ≤ cij , ∀ (i, j) ∈ A,
where the bounds si and si on the divergence of node i are given. Show that
this problem can be converted to a standard (equality constrained) minimum
cost flow problem by adding an extra node A and an arc (A, i) from this node to
every other node i, with feasible flow range [0, si − si].
44 Introduction Chap. 1
1.18 (Node Throughput Constraints)
Consider the minimum cost flow problem with the additional constraints that
the total flow of the outgoing arcs from each node i must lie within a given range
[ti, ti], that is,
ti ≤
∑
{j|(i,j)∈A}
xij ≤ ti.
Convert this problem into the standard form of the minimum cost flow problem
by splitting each node into two nodes with a connecting arc.
1.19 (Piecewise Linear Arc Costs)
Consider the minimum cost flow problem with the difference that, instead of the
linear form aijxij , each arc’s cost function has the piecewise linear form
fij(xij) =
{
a1ijxij if bij ≤ xij ≤ mij ,
a1ijmij + a
2
ij(xij − mij) if mij ≤ xij ≤ cij ,
where mij , a
1
ij , and a
2
ij are given scalars satisfying bij ≤ mij ≤ cij and a1ij ≤ a2ij .
(a) Show that the problem can be converted to a linear minimum cost flow
problem where each arc (i, j) is replaced by two arcs with arc cost coeffi-
cients a1ij and a
2
ij , and arc flow ranges [bij , mij ] and [0, cij − mij ], respec-
tively.
(b) Generalize to the case of piecewise linear cost functions with more than
two pieces.
1.20 (Asymmetric Assignment and Transportation Problems)
Consider an assignment problem where the number of objects is larger than the
number of persons, and we require that each person be assigned to one object.
The associated linear program (cf. Example 1.2) is
maximize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij = 1, ∀ i = 1, . . . , m,∑
{i|(i,j)∈A}
xij ≤ 1, ∀ j = 1, . . . , n,
0 ≤ xij ≤ 1, ∀ (i, j) ∈ A,
where m < n.
(a) Show how to formulate this problem as a minimum cost flow problem by
introducing extra arcs and nodes.
Sec. 1.4 Notes, Sources, and Exercises 45
(b) Repeat part (a) for the case where there may be some persons that are
left unassigned; that is, the constraint
∑
{j|(i,j)∈A} xij = 1 is replaced by∑
{j|(i,j)∈A} xij ≤ 1. Give an example of a problem with aij > 0 for all
(i, j) ∈ A, which is such that in the optimal assignment some persons are
left unassigned, even though there exist feasible assignments that assign
every person to some object.
(c) Formulate an asymmetric transportation problem where the total supply
is less than the total demand, but some demand may be left unsatisfied,
and appropriately modify your answers to parts (a) and (b).
1.21 (Bipartite Matching)
Bipartite matching problems are assignment problems where the coefficients (i, j)
are all equal to 1. In such problems, we want to maximize the cardinality of the
assignment, that is, the number of assigned pairs (i, j). Formulate a bipartite
matching problem as an equivalent max-flow problem.
1.22 (Production Planning)
Consider a problem of scheduling production of a certain item to meet a given
demand over N time periods. Let us denote:
xi: The amount of product stored at the beginning of period i, where
i = 0, . . . , N − 1. There is a nonnegativity constraint on xi.
ui: The amount of product produced during period i. There is a constraint
0 ≤ ui ≤ ci, where the scalar ci is given for each i.
di: The amount of product demanded during period i. This is a given
scalar for each i.
The amount of product stored evolves according to the equation
xi+1 = xi + ui − di, i = 0, . . . , N − 1.
Given x0, we want to find a feasible production sequence {u0, . . . , uN−1} that
minimizes
N−1∑
i=0
(aixi + biui),
where ai and bi are given scalars for each i. Formulate this problem as a minimum
cost flow problem. Hint : For each i, introduce a node that connects to a special
artificial node.
1.23 (Capacity Expansion)
The capacity of a certain facility is to be expanded over N time periods by adding
an increment ui ∈ [0, ci] at time period i = 0, . . . , N−1, where ci is a given scalar.
Thus, if xi is the capacity at the beginning of period i, we have
xi+1 = xi + ui, i = 0, . . . , N − 1.
46 Introduction Chap. 1
Given x0, consider the problem of finding ui, i = 0, . . . , N − 1, such that each xi
lies within a given interval [xi, x̄i] and the cost
N−1∑
i=0
(aixi + biui)
is minimized, where ai and bi are given scalars for each i. Formulate the problem
as a minimum cost flow problem.
1.24 (Dynamic Transhipment Problems)
Consider a transhipment context for the minimum cost flow problem where the
problem is to optimally transfer flow from some supply points to some demand
points over arcs of limited capacity. In a dynamic version of this context, the
transfer is to be performed over N time units, and transferring flow along an arc
(i, j) requires time τij , which is a given positive integer number of time units.
This means that at each time t = 0, . . . , N − τij , we may send from node i along
arc (i, j) a flow xij ∈ [0, cij ], which will arrive at node j at time t+τij . Formulate
this problem as a minimum cost flow problem involving a copy of the given graph
for each time period.
1.25 (Concentrator Assignment)
We have m communication terminals, each to be connected to one out of a
given collection of concentrators. Suppose that there is a cost aij for connecting
terminal i to concentrator j, and that each concentrator j has an upper bound
bj on the number of terminals it can be connected to. Also, each terminal i can
be connected to only a given subset of concentrators.
(a) Formulate the problem of finding the minimum cost connection of terminals
to concentrators as a minimum cost flow problem. Hint : You may use the
fact that there exists an integer optimal solution to a minimum cost flow
problem with integer supplies and arc flow bounds. (This will be shown in
Chapter 5.)
(b) Suppose that a concentrator j can operate in an overload condition with
a number of connected terminals greater than bj , up to a number bj > bj .
In this case, however, the cost per terminal connected becomes aij > aij .
Repeat part (a).
(c) Suppose that when no terminals are connected to concentrator j there is
a given cost savings cj > 0. Can you still formulate the problem as a
minimum cost flow problem?
1.26
Consider a round-robin chess tournament involving n players that play each other
once. A win scores 1 for the winner and 0 for the loser, while a draw scores 1/2
Sec. 1.4 Notes, Sources, and Exercises 47
for each player. We are given a set of final scores (s1, . . . , sn) for the players, from
the range [0, n−1], whose sum is n(n−1)/2, and we want to check whether these
scores are feasible [for example, in a four-player tournament, a set of final scores
of (3, 3, 0, 0) is impossible]. Show that this is equivalent to checking feasibility of
some transportation problem.
1.27 (k-Color Problem)
Consider the k-color problem, which is to assign one out of k colors to each node
of a graph so that for every arc (i, j), nodes i and j have different colors.
(a) Suppose we want to choose the colors of countries in a world map so that
no two adjacent countries have the same color. Show that if the number of
available colors is k, the problem can be formulated as a k-color problem.
(b) Show that the k-color problem has a solution if and only if the number of
nodes can be partitioned in k or less disjoint subsets such that there is no
arc connecting a pair of nodes from the same subset.
(c) Show that when the graph is a tree, the 2-color problem has a solution.
Hint : First color some node i and then color the remaining nodes based on
their “distance” from i.
(d) Show that if each node has at most k − 1 neighbors, the k-color problem
has a solution.
1.28 (k-Coloring and Parallel Computation)
Consider the n-dimensional vector x = (x1, . . . , xn) and an iteration of the form
xj := fj(x), j = 1, . . . , n,
where f = (f1, . . . , fn) is a given function. The dependency graph of f has nodes
1, . . . , n and an arc set such that (i, j) is an arc if the function fj exhibits a
dependence on the component xi. Consider an ordering j1, . . . , jn of the indices
1, . . . , n, and a partition of {j1, . . . , jn} into disjoint subsets J1, . . . , JM such that:
(1) For all k, if jk ∈ Jm, then jk+1 ∈ Jm ∪ · · · ∪ JM .
(2) If jp, jq ∈ Jm and p < q, then fjq does not depend on xjp .
Show that such an ordering and partition exist if and only if the nodes of the
dependency graph can be colored with M colors so that there exists no forward
cycle with all the nodes on the cycle having the same color. Note: This is
challenging (see Bertsekas and Tsitsiklis [1989], Section 1.2.4, for discussion and
analysis). An ordering and partition of this type can be used to execute Gauss-
Seidel iterations in M parallel steps.
48 Introduction Chap. 1
1.29 (Replacing Arc Costs with Reduced Costs)
Consider the minimum cost flow problem and let pj be a scalar price for each
node j. Show that if the arc cost coefficients aij are replaced by aij + pj − pi,
we obtain a problem that is equivalent to the original (except for a scalar shift
in the cost function value).
1.30
Consider the assignment problem.
(a) Show that every k-person exchange can be accomplished with a sequence
of k − 1 successive two-person exchanges.
(b) In light of the result of part (a), how do you explain that a nonoptimal
assignment may not be improvable by any two-person exchange?
1.31 (Dual Cost Improvement Directions)
Consider the assignment problem. Let pj denote the price of object j, let T be a
subset of objects, and let
S =
{
i | the maximum of aij − pj over j ∈ A(i)
is attained by some element of T
}
.
Assume that:
(1) For each i ∈ S, the maximum of aij − pj over j ∈ A(i) is attained only by
elements of T .
(2) S has more elements than T .
Show that the direction d = (d1, . . . , dn), where dj = 1 if j ∈ T and dj = 0 if
j /∈ T , is a direction of dual cost improvement. Note: Directions of this type are
used by the most common dual cost improvement algorithms for the assignment
problem.
1.32
Use -CS to verify that the assignment of Fig. 1.18 is optimal and obtain a bound
on how far from optimal the given price vector is. State the dual problem and
verify the correctness of the bound by comparing the dual value of the price
vector with the optimal dual value.
Sec. 1.4 Notes, Sources, and Exercises 49
Value = C
0
0
CC
C
C
C
p = C - 1/8
p = C + 1/8
p = 0
0
1
3
2
1
3
2
1
Figure 1.18: Graph of an assignment prob-
lem. Objects 1 and 2 have value C for all
persons. Object 3 has value 0 for all per-
sons. Object prices are as shown. The
thick lines indicate the given assignment.
1.33 (Generic Negative Cycle Algorithm)
Consider the following minimum cost flow problem
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
0 ≤ xij ≤ cij , ∀ (i, j) ∈ A,
and assume that the problem has at least one feasible solution. Consider first
the circulation case where si = 0 for all i ∈ N . Construct a sequence of flow
vectors x0, x1, . . . as follows: Start with x0 = 0. Given xk, stop if xk is optimal,
and otherwise find a simple cycle Ck that is unblocked with respect to xk and
has negative cost (cf. Prop. 1.2). Increase (decrease) the flow of the forward
(backward, respectively) arcs of Ck by the maximum possible increment.
(a) Show that the cost of xk+1 is smaller than the cost of xk by an amount
that is proportional to the cost of the cycle Ck and to the increment of the
corresponding flow change.
(b) Assume that the flow increment at each iteration is greater or equal to
some scalar δ > 0. Show that the algorithm must terminate after a finite
number of iterations with an optimal flow vector. Note: The assumption
of existence of such a δ is essential (see Exercise 3.7 in Chapter 3).
(c) Extend parts (a) and (b) to the general case where we may have si = 0 for
some i, by converting the problem to the circulation format (a method for
doing this is given in Section 4.1.3).
1.34 (Integer Optimal Solutions of Min-Cost Flow Problems)
Consider the minimum cost flow problem of Exercise 1.33, where the upper
bounds cij are given positive integers and the supplies si are given integers.
Assume that the problem has at least one feasible solution. Show that there
exists an optimal flow vector that is integer. Hint : Show that the flow vectors
generated by the negative cycle algorithm of Exercise 1.33 are integer.
50 Introduction Chap. 1
1.35 (The Original Hamiltonian Cycle)
The origins of the traveling salesman problem can be traced (among others) to the
work of the Irish mathematician Sir William Hamilton. In 1856, he developed a
system of commutative algebra, which inspired a puzzle marketed as the “Icosian
Game.” The puzzle is to find a cycle that passes exactly once through each
of the 20 nodes of the graph shown in Fig. 1.19, which represents a regular
dodecahedron. Find a Hamiltonian cycle on this graph using as first four nodes
the ones marked 1-4 (all arcs are considered bidirectional).
1
2
3 4
Figure 1.19: Graph for the Icosian Game (cf. Exercise 1.35). The arcs and nodes
correspond to the edges and vertices of the regular dodecahedron, respectively.
The name “icosian” comes from the Greek word “icosi,” which means twenty.
Adjacent nodes of the dodecahedron correspond to adjacent faces of the regular
icosahedron.
1.36 (Hamiltonian Cycle on the Hypercube)
The hypercube of dimension n is a graph with 2n nodes, each corresponding to
an n-bit string where each bit is either a 0 or a 1. There is a bidirectional arc
connecting every pair of nodes whose n-bit strings differ by a single bit. Show
that for every n ≥ 2, the hypercube contains a Hamiltonian cycle. Hint : Use
induction.
1.37 (Hardy’s Theorem)
Let {a1, . . . , an} and {b1, . . . , bn} be monotonically nondecreasing sequences of
numbers. Consider the problem of associating with each i = 1, . . . , n a distinct
index ji in a way that maximizes
∑n
i=1
aibji . Formulate this as an assignment
problem and show that it is optimal to select ji = i for all i. Hint : Use the
complementary slackness conditions with prices defined by p1 = 0 and pk =
pk−1 + ak(bk − bk−1) for k = 2, . . . , n.
2
The Shortest Path Problem
Contents
2.1. Problem Formulation and Applications
2.2. A Generic Shortest Path Algorithm
2.3. Label Setting (Dijkstra) Methods
2.2.1. Performance of Label Setting Methods
2.3.2. The Binary Heap Method
2.3.3. Dial’s Algorithm
2.4. Label Correcting Methods
2.4.1. The Bellman-Ford Method
2.4.2. The D’Esopo-Pape Algorithm
2.4.3. The SLF and LLL Algorithms
2.4.4. The Threshold Algorithm
2.4.5. Comparison of Label Setting and Label Correcting
2.5. Single Origin/Single Destination Methods
2.5.1. Label Setting
2.5.2. Label Correcting
2.6. Auction Algorithms
2.7. Multiple Origin/Multiple Destination Methods
2.8. Notes, Sources, and Exercises
51
52 The Shortest Path Problem Chap. 2
The shortest path problem is a classical and important combinatorial prob-
lem that arises in many contexts. We are given a directed graph (N ,A)
with nodes numbered 1, . . . , N . Each arc (i, j) ∈ A has a cost or “length”
aij associated with it. The length of a forward path (i1, i2, . . . , ik) is the
length of its arcs
k−1∑
n=1
ainin+1 .
This path is said to be shortest if it has minimum length over all forward
paths with the same origin and destination nodes. The length of a shortest
path is also called the shortest distance. The shortest path problem deals
with finding shortest distances between selected pairs of nodes. [Note that
here we are optimizing over forward paths; when we refer to a path (or a
cycle) in connection with the shortest path problem, we implicitly assume
that the path (or the cycle) is forward.]
The range of applications of the shortest path problem is very broad.
In the next section, we will provide some representative examples. We
will then develop a variety of algorithms. Most of these algorithms can be
viewed as primal cost or dual cost improvement algorithms for an appro-
priate special case of the minimum cost flow problem, as we will see later.
However, the shortest path problem is simple, so we will discuss it based
on first principles, and without much reference to cost improvement. This
serves a dual purpose. First, it provides an opportunity to illustrate some
basic graph concepts in the context of a problem that is simple and rich in
intuition. Second, it allows the early development of some ideas and results
that will be used later in a variety of other algorithmic contexts.
2.1 PROBLEM FORMULATION AND APPLICATIONS
The shortest path problem appears in a large variety of contexts. We
discuss a few representative applications.
Example 2.1. Routing in Data Networks
Data network communication involves the use of a network of computers
(nodes) and communication links (arcs) that transfer packets (groups of bits)
from their origins to their destinations. The most common method for se-
lecting the path of travel (or route) of packets is based on a shortest path
formulation. In particular, each communication link is assigned a positive
scalar which is viewed as its length. A shortest path routing algorithm routes
each packet along a minimum length (or shortest) path between the origin
and destination nodes of the packet.
There are several possibilities for selecting the link lengths. The sim-
plest is for each link to have unit length, in which case a shortest path is
Sec. 2.1 Problem Formulation and Applications 53
simply a path with minimum number of links. More generally, the length
of a link, may depend on its transmission capacity and its projected traffic
load. The idea here is that a shortest path should contain relatively few and
uncongested links, and therefore be desirable for routing. Sophisticated rout-
ing algorithms also allow the length of each link to change over time and to
depend on the prevailing congestion level of the link. Then a shortest path
may adapt to temporary overloads and route packets around points of con-
gestion. Within this context, the shortest path routing algorithm operates
continuously, solving the shortest path problem with lengths that vary over
time.
A peculiar feature of shortest path routing algorithms is that they are
often implemented using distributed and asynchronous communication and
computation. In particular, each node of the communication network mon-
itors the traffic conditions of its adjacent links, calculates estimates of its
shortest distances to various destinations, and passes these estimates to other
nodes who adjust their own estimates, etc. This process is based on stan-
dard shortest path algorithms that will be discussed in this chapter, but it
is also executed asynchronously, and with out-of-date information because of
communication delays between the nodes. Despite this fact, it turns out that
these distributed asynchronous algorithms maintain much of the validity of
their synchronous counterparts (see the textbooks by Bertsekas and Tsitsiklis
[1989], and Bertsekas and Gallager [1992] for related analysis).
There is an important connection between shortest path problems
and problems of deterministic discrete-state dynamic programming, which
involve sequential decision making over a finite number of time periods.
The following example shows that dynamic programming problems can be
formulated as shortest path problems. The reverse is also possible; that is,
any shortest path problem can be formulated as a dynamic programming
problem (see e.g., Bertsekas [1995a], Ch. 2).
Example 2.2. Dynamic Programming
Here we have a discrete-time dynamic system involving N stages. The state
of the system at the start of the kth stage is denoted by xk and takes values
in a given finite set, which may depend on the index k. The initial state x0 is
given. During the kth stage, the state of the system changes from xk to xk+1
according to an equation of the form
xk+1 = fk(xk, uk), (2.1)
where uk is a control that takes values from a given finite set, which may
depend on the index k. This transition involves a cost gk(xk, uk). The final
transition from xN−1 to xN , involves an additional terminal cost G(xN ).
Here, the functions fk, gk, and G are given.
Given a control sequence (u0, . . . , uN−1), the corresponding state se-
quence (x0, . . . , xN ) is determined from the given initial state x0 and the
system of Eq. (2.1). The objective in dynamic programming is to find a
54 The Shortest Path Problem Chap. 2
control sequence and a corresponding state sequence such that the total cost
G(xN ) +
N−1∑
k=0
gk(xk, uk)
is minimized.
For an example, consider an inventory system that operates over N
time periods, and let xk and uk denote the number of items held in stock and
number of items purchased at the beginning of period k, respectively. We
require that uk be an integer from a given range [0, rk]. We assume that the
stock evolves according to the equation
xk+1 = xk + uk − vk,
where vk is a known integer demand for period k; this is the system equa-
tion [cf. Eq. (2.1)]. A negative xk here indicates unsatisfied demand that is
backordered. A common type of cost used in inventory problems has the form
gk(xk, uk) = hk(xk) + ckuk,
where ck is a given cost per unit stock at period k, and hk(xk) is a cost either
for carrying excess inventory (xk > 0) or for backordering demand (xk < 0).
For example hk(xk) = max{akxk,−bkxk} or hk(xk) = dkx2k, where ak, bk, and
dk are positive scalars, are both reasonable choices for cost function. Finally,
we could take G(xN ) = 0 to indicate that the final stock xN has no value
[otherwise G(xN ) indicates the cost (or negative salvage value) of xN ]. The
objective in this problem is roughly to determine the sequence of purchases
over time to minimize the costs of excess inventory and backordering demand
over the N time periods.
To convert the dynamic programming problem to a shortest path prob-
lem, we introduce a graph such as the one of Fig. 2.1, where the arcs corre-
spond to transitions between states at successive stages and each arc has a
cost associated with it. To handle the final stage, we also add an artificial
terminal node t. Each state xN at stage N is connected to the terminal node
t with an arc having cost G(xN ). Control sequences correspond to paths
originating at the initial state x0 and terminating at one of the nodes corre-
sponding to the final stage N . The optimal control sequence corresponds to a
shortest path from node x0 to node t. For an extensive treatment of dynamic
programming and associated shortest path algorithms we refer to Bertsekas
[1995a].
Shortest path problems arise often in contexts of scheduling and se-
quencing. The following two examples are typical.
Example 2.3. Project Management
Consider the planning of a project involving several activities, some of which
must be completed before others can begin. The duration of each activity is
Sec. 2.1 Problem Formulation and Applications 55
.  .  .
.  .  .
.  .  .
Stage 0 Stage 1 Stage 2 Stage N  - 1 Stage N
Initial State
               x0
Artificial Terminal
Node
Terminal Arcs
with Cost Equal
to Terminal Cost
.  .  .
.  .  .x1
x2 xN-1
xN
t
Figure 2.1: Converting a deterministic finite-state N -stage dynamic program-
ming problem to a shortest path problem. Nodes correspond to states. An arc
with start and end nodes xk and xk+1, respectively, corresponds to a transition
of the form xk+1 = fk(xk, uk). The length of this arc is equal to the cost of
the corresponding transition gk(xk, uk). The problem is equivalent to finding a
shortest path from the initial state/node x0 to the artificial terminal node t. Note
that the state space and the possible transitions between states may depend on
the stage index k.
known in advance. We want to find the time required to complete the project,
as well as the critical activities, those that even if slightly delayed will result
in a corresponding delay of completion of the overall project.
The problem can be represented by a graph where nodes represent
completion of some phase of the project (cf. Fig. 2.2). An arc (i, j) represents
an activity that starts once phase i is completed and has known duration
tij > 0. A phase (node) j is completed when all activities or arcs (i, j) that
are incoming to j are completed. Two special nodes 1 and N represent the
start and end of the project, respectively. Node 1 has no incoming arcs,
while node N has no outgoing arcs. Furthermore, there is at least one path
from node 1 to every other node. An important characteristic of an activity
network is that it is acyclic. This is inherent in the problem formulation and
the interpretation of nodes as phase completions.
For any path p =
{
(1, j1), (j1, j2, ), . . . , (jk, i)
}
from node 1 to a node
i, let Dp be the duration of the path defined as the sum of durations of its
activities; that is,
Dp = tij1 + tj1j2 + · · · + tjki.
Then the time Ti required to complete phase i is
Ti = max
paths p
from 1 to i
Dp.
The maximum above is attained by some path because there can be only a
finite number of paths from 1 to i, since the network is acyclic. Thus to find
56 The Shortest Path Problem Chap. 2
1
2
3
3
Start
1
2 2
2
4 5
4
End
Construction
Train
Personnel
Order
Material
Transport
Material
Train
Personnel
Hire
Personnel
Figure 2.2: Example graph of an activity network. Arcs (i, j) represent
activities and are labeled by the corresponding duration tij . Nodes represent
completion of some phase of the project. A phase is completed if all activities
associated with incoming arcs at the corresponding node are completed. The
project is completed when all phases are completed. The project duration
time is the longest sum of arc durations over paths that start at node 1 and
end at node 5. The path of longest duration, also called a critical path, is
shown with thick line. Because the graph is acyclic, finding this path is a
shortest path problem with the length of each arc (i, j) being −tij . Activities
on the critical path have the property that if any one of them is delayed, a
corresponding delay of completion of the overall project will result.
Ti, we should find the longest path from 1 to i. Because the graph is acyclic,
this problem may also be viewed as a shortest path problem with the length
of each arc (i, j) being −tij . In particular, finding the duration of the project
is equivalent to finding the shortest path from 1 to N . For further discussion
of project management problems, we refer to the literature, e.g., the textbook
by Elmaghraby [1978].
Example 2.4. The Paragraphing Problem
This problem arises in a word processing context, where we want to break
down a given paragraph consisting of N words into lines for “optimal” ap-
pearance and readability. Suppose that we have a heuristic rule, which assigns
to any sequence of words a cost that expresses the undesirability of grouping
these words together in a line. Based on such a rule, we can assign a cost
cij to a line starting with word i and ending with word j − 1 of the given
paragraph. An optimally divided paragraph is one for which the sum of the
costs of its lines is minimal.
We can formulate this as a shortest path problem. There are N nodes,
which correspond to the N words of the paragraph, and there is an arc (i, j)
with cost cij connecting any two words i and j with i < j. The arcs of the
shortest path from node/word 1 to node/word N correspond to the lines of
the optimally broken down paragraph.
Sec. 2.2 A Generic Shortest Path Algorithm 57
The exercises contain a number of additional examples that illustrate
the broad range of applications of the shortest path problem.
2.2 A GENERIC SHORTEST PATH ALGORITHM
The shortest path problem can be posed in a number of ways; for example,
finding a shortest path from a single origin to a single destination, or finding
a shortest path from each of several origins to each of several destinations.
We focus initially on problems with a single origin and many destinations.
For concreteness, we take the origin node to be node 1. The arc lengths aij
are given scalars. They may be negative and/or noninteger, although on
occasion we will assume in our analysis that they are nonnegative and/or
integer, in which case we will state so explicitly.
In this section, we develop a broad class of shortest path algorithms
for the single origin/all destinations problem. These algorithms maintain
and adjust a vector (d1, d2, . . . , dN ), where each dj , called the label of node
j, is either a scalar or ∞. The use of labels is motivated by a simple
optimality condition, which is given in the following proposition.
Proposition 2.1: Let d1, d2, . . . , dN be scalars satisfying
dj ≤ di + aij , ∀ (i, j) ∈ A, (2.2)
and let P be a path starting at a node i1 and ending at a node ik. If
dj = di + aij , for all arcs (i, j) of P , (2.3)
then P is a shortest path from i1 to ik.
Proof: By adding Eq. (2.3) over the arcs of P , we see that the length of
P is equal to the difference dik − di1 of labels of the end node and start
node of P . By adding Eq. (2.2) over the arcs of any other path P ′ starting
at i1 and ending at ik, we see that the length of P ′ must be no less than
dik − di1 . Therefore, P is a shortest path. Q.E.D.
The conditions (2.2) and (2.3) are called the complementary slackness
(CS) conditions for the shortest path problem. This terminology is moti-
vated by the connection of the shortest path problem with the minimum
cost flow problem (cf. Section 1.2.1); we will see in Chapter 4 that the CS
conditions of Prop. 2.1 are a special case of a general optimality condition
(also called CS condition) for the equivalent minimum cost flow problem
58 The Shortest Path Problem Chap. 2
(in fact they are a special case of a corresponding CS condition for general
linear programs; see e.g., Bertsimas and Tsitsiklis [1997], Dantzig [1963]).
Furthermore, we will see that the scalars di in Prop. 2.1 are related to dual
variables.
Let us now describe a prototype shortest path method that contains
several interesting algorithms as special cases. In this method, we start
with some vector of labels (d1, d2, . . . , dN ), we successively select arcs (i, j)
that violate the CS condition (2.2), i.e., dj > di + aij , and we set
dj := di + aij .
This is continued until the CS condition dj ≤ di + aij is satisfied for all
arcs (i, j).
A key idea is that, in the course of the algorithm, di can be interpreted
for all i as the length of some path Pi from 1 to i.† Therefore, if dj > di+aij
for some arc (i, j), the path obtained by extending path Pi by arc (i, j),
which has length di + aij , is a better path than the current path Pj , which
has length dj . Thus, the algorithm finds successively better paths from the
origin to various destinations.
Instead of selecting arcs in arbitrary order to check violation of the CS
condition, it is usually most convenient and efficient to select nodes, one-at-
a-time according to some order, and simultaneously check violation of the
CS condition for all of their outgoing arcs. The corresponding algorithm,
referred to as generic, maintains a list of nodes V , called the candidate
list , and a vector of labels (d1, d2, . . . , dN ), where each dj is either a real
number or ∞. Initially,
V = {1},
d1 = 0, di = ∞, ∀ i 	= 1.
The algorithm proceeds in iterations and terminates when V is empty. The
typical iteration (assuming V is nonempty) is as follows:
Iteration of the Generic Shortest Path Algorithm
Remove a node i from the candidate list V . For each outgoing arc
(i, j) ∈ A, if dj > di + aij , set
dj := di + aij
and add j to V if it does not already belong to V .
† In the case of the origin node 1, we will interpret the label d1 as either the
length of a cycle that starts and ends at 1, or (in the case d1 = 0) the length of
the trivial “path” from 1 to itself.
Sec. 2.2 A Generic Shortest Path Algorithm 59
3
3
1
1
2
11 4
3
2
Origin
Iteration # Candidate List V Node Labels Node out of V
1 {1} (0,∞,∞,∞) 1
2 {2, 3} (0, 3, 1,∞) 2
3 {3, 4} (0, 3, 1, 5) 3
4 {4, 2} (0, 2, 1, 4) 4
5 {2} (0, 2, 1, 4) 2
Ø (0, 2, 1, 4)
Figure 2.3: Illustration of the generic shortest path algorithm. The numbers
next to the arcs are the arc lengths. Note that node 2 enters the candidate list
twice. If in iteration 2 node 3 was removed from V instead of node 2, each node
would enter V only once. Thus, the order in which nodes are removed from V is
significant.
It can be seen that, in the course of the algorithm, the labels are
monotonically nonincreasing. Furthermore, we have
di < ∞ ⇐⇒ i has entered V at least once.
Figure 2.3 illustrates the algorithm. The following proposition gives its
main properties.
Proposition 2.2: Consider the generic shortest path algorithm.
(a) At the end of each iteration, the following conditions hold:
(i) If dj < ∞, then dj is the length of some path that starts
at 1 and ends at j.
(ii) If i /∈ V , then either di = ∞ or else
dj ≤ di + aij , ∀ j such that (i, j) ∈ A.
60 The Shortest Path Problem Chap. 2
(b) If the algorithm terminates, then upon termination, for all j with
dj < ∞, dj is the shortest distance from 1 to j and
dj =
{
min(i,j)∈A{di + aij} if j 	= 1,
0 if j = 1.
(2.4)
Furthermore, upon termination we have dj = ∞ if and only if
there is no path from 1 to j.
(c) If the algorithm does not terminate, then there exists some node
j and a sequence of paths that start at 1, end at j, and have
lengths that diverge to −∞.
(d) The algorithm terminates if and only if there is no path that
starts at 1 and contains a cycle with negative length.
Proof: (a) We prove (i) by induction on the iteration count. Indeed, (i)
holds at the end of the first iteration since the nodes j 	= 1 with dj < ∞
are those for which (1, j) is an arc and their labels are dj = a1j , while for
the origin 1, we have d1 = 0, which by convention is viewed as the length
of the trivial “path” from 1 to itself. Suppose that (i) holds at the start
of some iteration at which the node removed from V is i. Then di < ∞
(which is true for all nodes of V by the rules of the algorithm), and (by the
induction hypothesis) di is the length of some path Pi starting at 1 and
ending at i. When a label dj changes as a result of the iteration, dj is set
to di + aij , which is the length of the path consisting of Pi followed by arc
(i, j). Thus property (i) holds at the end of the iteration, completing the
induction proof.
To prove (ii), note that for any i, each time i is removed from V , the
condition dj ≤ di + aij is satisfied for all (i, j) ∈ A by the rules of the
algorithm. Up to the next entrance of i into V , di stays constant, while
the labels dj for all j with (i, j) ∈ A cannot increase, thereby preserving
the condition dj ≤ di + aij .
(b) We first introduce the sets
I = {i | di < ∞ upon termination},
I = {i | di = ∞ upon termination},
and we show that we have j ∈ I if and only if there is no path from 1 to j.
Indeed, if i ∈ I, we have di < ∞ and therefore dj < ∞ for all j such that
(i, j) is an arc in view of condition (ii) of part (a), so that j ∈ I. It follows
that there is no path from any node of I (and in particular, node 1) to
any node of I. Conversely, if there is no path from 1 to j, it follows from
Sec. 2.2 A Generic Shortest Path Algorithm 61
condition (i) of part (a) that we cannot have dj < ∞ upon termination, so
j ∈ I.
We show now that for all j ∈ I, upon termination, dj is the shortest
distance from 1 to j and Eq. (2.4) holds. Indeed, conditions (i) and (ii) of
part (a) imply that upon termination we have, for all i ∈ I,
dj ≤ di + aij , ∀ j such that (i, j) ∈ A, (2.5)
while di is the length of some path from 1 to i, denoted Pi. Fix a node
m ∈ I, and consider any path P from 1 to m. By adding the condition
(2.5) over the arcs of P , we see that the length of P is no less than dm−d1,
which is less or equal to dm (we have d1 ≤ 0, since initially d1 = 0 and all
node labels are monotonically nonincreasing). Hence Pm is a shortest path
from 1 to m and the shortest distance is dm. Furthermore, the equality
dj = di + aij must hold for all arcs (i, j) on the shortest paths Pm, m ∈ I,
implying that dj = min(i,j)∈A{di + aij} for all j ∈ I with j 	= 1, while
d1 = 0.
(c) If the algorithm never terminates, some label dj must decrease strictly
an infinite number of times, generating a corresponding sequence of distinct
paths Pj as per condition (i) of part (a). Each of these paths can be
decomposed into a simple path from 1 to j plus a collection of simple
cycles, as in Exercise 1.4 of Chapter 1. Since the number of simple paths
from 1 to j is finite, and the length of Pj is monotonically decreasing, it
follows that Pj eventually must involve a cycle with negative length. By
replicating this cycle a sufficiently large number of times, one can obtain
paths from 1 to j with arbitrarily small length.
(d) Using part (c), we have that the algorithm will terminate if and only if
there is a lower bound on the length of all paths that start at node 1. Thus,
the algorithm will terminate if and only if there is no path that starts at
node 1 and contains a cycle with negative length. Q.E.D.
When some arc lengths are negative, Prop. 2.2 points to a way to
detect existence of a path that starts at the origin 1 and contains a cycle
of negative length. If such a path exists, it can be shown under mild
assumptions that the label of at least one node will diverge to −∞ (see
Exercise 2.32). We can thus monitor whether for some j we have
dj < (N − 1) min
(i,j)∈A
aij .
When this condition occurs, the path from 1 to j whose length is equal to
dj [as per Prop. 2.2(a)] must contain a negative cycle [if it were simple, it
would consist of at most N − 1 arcs, and its length could not be smaller
than (N − 1) min(i,j)∈A aij ; a similar argument would apply if it were not
simple but it contained only cycles of nonnegative length].
62 The Shortest Path Problem Chap. 2
Bellman’s Equation and Shortest Path Construction
When all cycles have nonnegative length and there exists a path from node
1 to every node j, then Prop. 2.2 shows that the generic algorithm termi-
nates and that, upon termination, all labels are equal to the corresponding
shortest distances, and satisfy d1 = 0 and
dj = min
(i,j)∈A
{di + aij}, ∀ j 	= 1. (2.6)
This is known as Bellman’s equation and it has an intuitive meaning: it
indicates that the shortest distance from 1 to j is obtained by optimally
choosing the predecessor i of node j in order to minimize the sum of the
shortest distance from 1 to i and the length of arc (i, j). It also indicates
that if Pj is a shortest path from 1 to j, and a node i belongs to Pj , then
the portion of Pj from 1 to i, is a shortest path from 1 to i.
From Bellman’s equation, we can obtain the shortest paths (in addi-
tion to the shortest path lengths) if all cycles not including node 1 have
strictly positive length. To do this, select for each j 	= 1 one arc (i, j)
that attains the minimum in dj = min(i,j)∈A{di + aij} and consider the
subgraph consisting of these N − 1 arcs; see Fig. 2.4. To find the short-
est path to any node j, start from j and follow the corresponding arcs of
the subgraph backward until node 1 is reached. Note that the same node
cannot be reached twice before node 1 is reached, since a cycle would be
formed that, on the basis of Eqs. (2.6), would have zero length. [To see
this, let (i1, i2, . . . , ik, i1) be the cycle and add the equations
di1 = di2 + ai2i1
. . .
dik−1 = dik + aikik−1
dik = di1 + ai1ik
obtaining ai2i1 + · · ·+aikik−1 +ai1ik = 0.] Since the subgraph is connected
and has N − 1 arcs, it must be a spanning tree. We call this subgraph a
shortest path spanning tree, and we note its special structure: it has a root
(node 1) and every arc of the tree is directed away from the root. The
preceding argument can also be used to show that Bellman’s equation has
no solution other than the shortest distances; see Exercise 2.5.
A shortest path spanning tree can also be constructed in the process
of executing the generic shortest path algorithm by recording the arc (i, j)
every time dj is decreased to di + aij ; see Exercise 2.4.
Sec. 2.2 A Generic Shortest Path Algorithm 63
3
Origin
d  = 22
d  = 1
d  = 3
4
4
3
1
1
11 4
3
2
1
Figure 2.4: Example of construction
of shortest path spanning tree. The arc
lengths are shown next to the arcs, and
the shortest distances are shown next
to the nodes. For each j = 1, we select
an arc (i, j) such that
dj = di + aij
and we form the shortest path spanning
tree. The arcs selected in this example
are (1, 3), (3, 2), and (2, 4).
Advanced Initialization
The generic algorithm need not be started with the initial conditions
V = {1}, d1 = 0, di = ∞, ∀ i 	= 1,
in order to work correctly. Any set of labels (d1, . . . , dN ) and candidate
list V can be used initially, as long as they satisfy the conditions of Prop.
2.2(a). It can be seen that the proof of the remaining parts of Prop. 2.2 go
through under these conditions.
In particular, the algorithm works correctly if the labels and the can-
didate list are initialized so that d1 = 0 and:
(a) For each node i, di is either ∞ or else it is the length of a path from
1 to i.
(b) The candidate list V contains all nodes i such that
di + aij < dj for some (i, j) ∈ A. (2.7)
This kind of initialization is very useful in reoptimization contexts,
where we have to solve a large number of similar problems that differ
slightly from each other; for example they may differ by just a few arc
lengths or they may have a slightly different node set. The lengths of the
shortest paths of one problem can be used as the starting labels for another
problem, and substantial computational savings may be obtained, because
it is likely that many of the nodes will maintain their shortest path lengths
and will never enter V .
Another important situation where an advanced initialization is very
useful arises if, by using heuristics or an available solution of a similar
shortest path problem, we can construct a set of “good” paths from node 1
to the other nodes. Then we can use the lengths of these paths as the initial
labels in the generic shortest path algorithm and start with a candidate list
consisting of the nodes where the CS condition is violated [cf. Eq. (2.7)].
64 The Shortest Path Problem Chap. 2
Finally, let us note another technique that is sometimes useful in
reoptimization settings. Suppose that we have some scalars δ1, . . . , δN and
we change the arc lengths to
âij = aij + δi − δj .
Then it can be seen that the length of any path from a node m to a node
n will be increased by δm − δn, while the shortest paths will be unaffected.
Thus it may be advantageous to use the modified arc lengths âij instead
of the original lengths aij , if this will enhance the application of a suitable
shortest path algorithm. For example, we may be able with proper choice
of δi, to reduce the arc cost range max(i,j) |âij | (this is helpful in some
algorithms) or to make âij nonnegative (see Section 2.7 for an application
of this idea).
Implementations of the Generic Algorithm
There are many implementations of the generic algorithm. They differ in
how they select the node to be removed from the candidate list V , and
they are broadly divided into two categories:
(a) Label setting methods. In these methods, the node i removed from
V is a node with minimum label. Under the assumption that all arc
lengths are nonnegative, these methods have a remarkable property:
each node will enter V at most once, as we will show shortly; its label
has its permanent or final value at the first time it is removed from
V . The most time-consuming part of these methods is calculating
the minimum label node in V at each iteration; there are several
implementations, that use a variety of creative procedures to obtain
this minimum.
(b) Label correcting methods. In these methods the choice of the node i
removed from V is less sophisticated than in label setting methods,
and requires less calculation. However, a node may enter V multiple
times.
There are several worst-case complexity bounds for label setting and
label correcting methods. The best bounds for the case of nonnegative arc
lengths correspond to label setting methods. The best practical methods,
however, are not necessarily the ones with the best complexity bounds, as
will be discussed in the next two sections.
In practice, when the arc lengths are nonnegative, the best label set-
ting methods and the best label correcting methods are competitive. As a
general rule, a sparse graph favors the use of a label correcting over a label
setting method for reasons that will be explained later (see the discussion at
the end of Section 2.4). An important advantage of label correcting meth-
ods is that they are more general, since they do not require nonnegativity
of the arc lengths.
Sec. 2.3 Label Setting (Dijkstra) Methods 65
2.3 LABEL SETTING (DIJKSTRA) METHODS
In this section we discuss various implementations of the label setting ap-
proach. The prototype label setting method, first published by Dijkstra
[1959] but also discovered independently by several other researchers, is
the special case of the generic algorithm where the node i removed from
the candidate list V at each iteration has minimum label, that is,
di = min
j∈V
dj .
For convenient reference, let us state this method explicitly.
Initially, we have
V = {1},
d1 = 0, di = ∞, ∀ i 	= 1.
The method proceeds in iterations and terminates when V is empty. The
typical iteration (assuming V is nonempty) is as follows:
Iteration of the Label Setting Method
Remove from the candidate list V a node i such that
di = min
j∈V
dj .
For each outgoing arc (i, j) ∈ A, if dj > di + aij , set
dj := di + aij
and add j to V if it does not already belong to V .
Figure 2.5 illustrates the label setting method. Some insight into the
method can be gained by considering the set W of nodes that have already
been in V but are not currently in V :
W = {i | di < ∞, i /∈ V }.
We will prove later, in Prop. 2.3(a), that as a consequence of the policy of
removing from V a minimum label node, W contains nodes with “small”
labels throughout the algorithm, in the sense that
dj ≤ di, if j ∈ W and i /∈ W. (2.8)
Using this property and the assumption aij ≥ 0, it can be seen that when
a node i is removed from V , we have, for all j ∈ W for which (i, j) is an
arc,
dj ≤ di + aij .
66 The Shortest Path Problem Chap. 2
Origin
2
3
1
1
1
1
0
0
1 4
3
2
5
Iteration # Candidate List V Node Labels Node out of V
1 {1} (0,∞,∞,∞,∞) 1
2 {2, 3} (0, 2, 1,∞,∞) 3
3 {2, 4} (0, 2, 1, 4,∞) 2
4 {4, 5} (0, 2, 1, 3, 2) 5
5 {4} (0, 2, 1, 3, 2) 4
Ø (0, 2, 1, 3, 2)
Figure 2.5: Example illustrating the label setting method. At each iteration,
the node with the minimum label is removed from V . Each node enters V only
once.
Hence, once a node enters W , it stays in W and its label does not change
further. Thus, W can be viewed as the set of permanently labeled nodes,
that is, the nodes that have acquired a final label, which by Prop. 2.2, must
be equal to their shortest distance from the origin.
The following proposition makes the preceding argument precise and
proves some additional facts.
Proposition 2.3: Assume that all arc lengths are nonnegative.
(a) For any iteration of the label setting method, the following hold
for the set
W = {i | di < ∞, i /∈ V }. (2.9)
(i) No node belonging to W at the start of the iteration will
enter the candidate list V during the iteration.
(ii) At the end of the iteration, we have di ≤ dj for all i ∈ W
and j /∈ W .
Sec. 2.3 Label Setting (Dijkstra) Methods 67
(iii) For each node j, consider simple paths that start at 1, end
at j, and have all their other nodes in W at the end of the
iteration. Then the label dj at the end of the iteration is
equal to the length of the shortest of these paths (dj = ∞
if no such path exists).
(b) The label setting method will terminate, and all nodes with a
final label that is finite will be removed from the candidate list
V exactly once in order of increasing shortest distance from node
1; that is, if the final labels of i and j are finite and satisfy di < dj ,
then i will be removed before j.
Proof: (a) Properties (i) and (ii) will be proved simultaneously by induc-
tion on the iteration count. Clearly (i) and (ii) hold for the initial iteration
at which node 1 exits V and enters W .
Suppose that (i) and (ii) hold for iteration k − 1, and suppose that
during iteration k, node i satisfies di = minj∈V dj and exits V . Let W
and W be the set of Eq. (2.9) at the start and at the end of iteration k,
respectively. Let dj and dj be the label of each node j at the start and at
the end of iteration k, respectively. Since by the induction hypothesis we
have dj ≤ di for all j ∈ W , and aij ≥ 0 for all arcs (i, j), it follows that
dj ≤ di + aij for all arcs (i, j) with j ∈ W . Hence, a node j ∈ W cannot
enter V at iteration k. This completes the induction proof of property (i),
and shows that
W = W ∪ {i}.
Thus, at iteration k, the only labels that may change are the labels dj
of nodes j /∈ W such that (i, j) is an arc; the label dj at the end of the
iteration will be min{dj , di + aij}. Since aij ≥ 0, di ≤ dj for all j /∈ W ,
and di = di, we must have di ≤ dj for all j /∈ W . Since by the induction
hypothesis we have dm ≤ di and dm = dm for all m ∈ W , it follows that
dm ≤ dj for all m ∈ W and j /∈ W . This completes the induction proof of
property (ii).
To prove property (iii), choose any node j and consider the subgraph
consisting of the nodes W ∪ {j} together with the arcs that have both
end nodes in W ∪ {j}. Consider also a modified shortest path problem
involving this subgraph, and the same origin and arc lengths as in the
original shortest path problem. In view of properties (i) and (ii), the label
setting method applied to the modified shortest path problem yields the
same sequence of nodes exiting V and the same sequence of labels as when
applied to the original problem up to the current iteration. By Prop.
2.2, the label setting method for the modified problem terminates with the
labels equal to the shortest distances of the modified problem at the current
68 The Shortest Path Problem Chap. 2
iteration. This means that the labels at the end of the iteration have the
property stated in the proposition.
(b) Since there is no cycle with negative length, by Prop. 2.2(d), we see
that the label setting method will terminate. At each iteration the node
removed from V is added to W , and according to property (i) (proved
above), no node from W is ever returned to V . Therefore, each node
with a final label that is finite will be removed from V and simultaneously
entered in W exactly once, and, by the rules of the algorithm, its label
cannot change after its entrance in W . Property (ii) then shows that each
new node added to W has a label at least as large as the labels of the nodes
already in W . Therefore, the nodes are removed from V in the order stated
in the proposition. Q.E.D.
2.3.1 Performance of Label Setting Methods
In label setting methods, the candidate list V is typically maintained with
the help of some data structure that facilitates the removal and the addition
of nodes, and also facilitates finding the minimum label node from the list.
The choice of data structure is crucial for good practical performance as
well as for good theoretical worst-case performance.
To gain some insight into this, we first consider a somewhat naive
implementation that will serve as a yardstick for comparison. By Prop.
2.3, there will be exactly N iterations, and in each of these, the candidate
list V will be searched for a minimum label node. Suppose this is done
by examining all nodes in sequence, checking whether they belong to V ,
and finding one with minimum label among those who do. Searching V
in this way requires O(N) operations per iteration, for a total of O(N2)
operations. Also during the algorithm, we must examine each arc (i, j)
exactly once to check whether the condition dj > di + aij holds, and to set
dj := di +aij if it does. This requires O(A) operations, which is dominated
by the preceding O(N2) estimate.
The O(A) operation count for arc examination is unavoidable and
cannot be reduced [each arc (i, j) must be checked at least once just to ver-
ify the optimality condition dj ≤ di + aij ]. However, the O(N2) operation
count for minimum label searching can be reduced considerably by using
appropriate data structures. The best estimates of the worst-case running
time that have been thus obtained are O(A+N log N) and O(A+N
√
log C),
where C is the arc length range C = max(i,j)∈A aij ; see Fredman and Tar-
jan [1984], and Ahuja, Mehlhorn, Orlin, and Tarjan [1990]. On the basis
of present experience, however, the implementations that perform best in
practice have considerable less favorable running time estimates. The ex-
planation for this is that the O(·) estimates involve a different constant for
each method and also correspond to worst-case problem instances. Thus,
the worst-case complexity estimates may not provide a reliable practical
Sec. 2.3 Label Setting (Dijkstra) Methods 69
comparison of various methods. We now discuss two of the most popular
implementations of the label setting method.
2.3.2 The Binary Heap Method
Here the nodes are organized as a binary heap on the basis of label values
and membership in V ; see Fig. 2.6. The node at the top of the heap is the
node of V that has minimum label, and the label of every node in V is no
larger than the labels of all the nodes that are in V and are its descendants
in the heap. Nodes that are not in V may be in the heap but may have no
descendants that are in V .
Label = 7
Label = 5
Label = 2
Label = 1
Label = 4
Label = 2
Label = 6Label = 4 Node not in VNode not in V
Node not in V
Node not in V
Label = 4
Figure 2.6: A binary heap organized on the basis of node labels is a binary
balanced tree such that the label of each node of V is no larger than the labels of
all its descendants that are in V . Nodes that are not in V may have no descendants
that are in V . The topmost node, called the root , has the minimum label. The
tree is balanced in that the numbers of arcs in the paths from the root to any
nodes with no descendants differ by at most 1. If the label of some node decreases,
the node must be moved upward toward the root, requiring O(log N) operations.
[It takes O(1) operations to compare the label of a node i with the label of one
of its descendants j, and to interchange the positions of i and j if the label of j
is smaller. Since there are log N levels in the tree, it takes at most log N such
comparisons and interchanges to move a node upward to the appropriate position
once its label is decreased.] Similarly, when the topmost node is removed from V ,
moving the node downward to the appropriate level in the heap requires at most
log N steps and O(log N) operations. (Each step requires the interchange of the
position of the node and the position of one of its descendants. The descendant
must be in V for the step to be executed; if both descendants are in V , the one
with smaller label is selected.)
At each iteration, the top node of the heap is removed from V . Fur-
thermore, the labels of some nodes already in V may decrease, so these
may have to be repositioned in the heap; also, some other nodes may enter
70 The Shortest Path Problem Chap. 2
V for the first time and have to be inserted in the heap at the right place.
It can be seen that each of these removals, repositionings, and insertions
can be done in O(log N) time. There are a total of N removals and N
node insertions, so the number of operations for maintaining the heap is
O
(
(N + R) log N
)
, where R is the total number of node repositionings.
There is at most one repositioning per arc, since each arc is examined at
most once, so we have R ≤ A and the total operation count for maintaining
the heap is O(A log N). This dominates the O(A) operation count to ex-
amine all arcs, so the worst-case running time of the method is O(A log N).
On the other hand, practical experience indicates that the number of node
repositionings R is usually a small multiple of N , and considerably less
than the upper bound A. Thus, the running time of the method in prac-
tice typically grows approximately like O(A + N log N).
2.3.3 Dial’s Algorithm
This algorithm, due to Dial [1969], requires that all arc lengths are non-
negative integers. It uses a naive yet often surprisingly effective method
for finding the minimum label node in V . The idea is to maintain for every
possible label value, a list of the nodes that have that value. Since every
finite label is equal to the length of some path with no cycles [Prop. 2.3(a),
part (iii)], the possible label values range from 0 to (N − 1)C, where
C = max
(i,j)∈A
aij .
Thus, we may scan the (N − 1)C + 1 possible label values (in ascending
order) and look for a label value with nonempty list, instead of scanning
the candidate list V .
To visualize the algorithm, it is useful to think of each integer in
the range [0, (N − 1)C] as some kind of container, referred to as a bucket .
Each bucket b holds the nodes with label equal to b. Tracing steps, we see
that the method starts with the origin node 1 in bucket 0 and all other
buckets empty. At the first iteration, each node j with (1, j) ∈ A enters
the candidate list V and is inserted in bucket a1j . After we are done with
bucket 0, we proceed to check bucket 1. If it is nonempty, we repeat the
process, removing from V all nodes with label 1 and moving other nodes
to smaller numbered buckets as required; if not, we check bucket 2, and so
on. Figure 2.7 illustrates the method with an example.
Let us now consider the efficient implementation of the algorithm. We
first note that a doubly linked list (see Fig. 2.8) can be used to maintain the
set of nodes belonging to a given bucket, so that checking the emptiness of
a bucket and inserting or removing a node from a bucket are easy, requiring
O(1) operations. With such a data structure, the time required for mini-
mum label node searching is O(NC), and the time required for adjusting
node labels and repositioning nodes between buckets is O(A). Thus the
Sec. 2.3 Label Setting (Dijkstra) Methods 71
2
1
1
3
1
0
0
1
4
3
2
51Origin
Iter. Cand. Node Buck. Buck. Buck. Buck. Buck. Out
# List V Labels 0 1 2 3 4 of V
1 {1} (0,∞,∞,∞,∞) 1 – – – – 1
2 {2, 3} (0, 2, 1,∞,∞) 1 3 2 – – 3
3 {2, 4} (0, 2, 1, 4,∞) 1 3 2 – 4 2
4 {4, 5} (0, 2, 1, 3, 2) 1 3 2,5 4 – 5
5 {4} (0, 2, 1, 2, 2) 1 3 2,4,5 – – 4
Ø (0, 2, 1, 2, 2) 1 3 2,4,5 – –
Figure 2.7: An example illustrating Dial’s method.
overall running time is O(A + NC). The algorithm is pseudopolynomial,
but for small values of C (much smaller than N) it performs very well in
practice.
In problems where the minimum arc length
a = min
(i,j)∈A
aij
is greater than 1, the performance of the algorithm can be improved by
using a device suggested by Denardo and Fox [1979]. The idea is that the
label of a node cannot be reduced below b + a while searching bucket b,
so that no new nodes will be added to buckets b + 1, . . . , b + a − 1 while
searching bucket b. As a result, buckets b, b+1, . . . , b+a−1 can be lumped
into a single bucket. To take advantage of this idea, we can use⌈
(N − 1)C + 1
a
⌉
buckets, and follow the strategy of placing node i into bucket b if
ab ≤ di ≤ a(b + 1) − 1.
The running time of the algorithm is then reduced to O
(
A + (NC/a)
)
.
72 The Shortest Path Problem Chap. 2
Bucket b 0 1 2 3 4 5 6 7 8
Contents of b 1 – 3,4,5 2,7 – 6 – – –
FIRST (b) 1 0 3 2 0 6 0 0 0
Node i 1 2 3 4 5 6 7
Label di 0 3 2 2 2 5 3
NEXT (i) 0 7 4 5 0 0 0
PREVIOUS (i) 0 0 0 3 4 0 2
Figure 2.8: Illustration of a doubly linked list data structure to maintain the can-
didate list V in buckets. In this example, the nodes in V are numbered 1, 2, . . . , 7,
and the buckets are numbered 0, 1, . . . , 8. A node i belongs to bucket b if di = b.
As shown in the first table, for each bucket b we maintain the first node of
the bucket in an array element FIRST (b), where FIRST (b) = 0 if bucket b is
empty.
As shown in the second table, for every node i we maintain two array
elements, NEXT (i) and PREV IOUS(i), giving the next node and the pre-
ceding node, respectively, of node i in the bucket where i is currently residing
[NEXT (i) = 0 or PREV IOUS(i) = 0 if i is the last node or the first node in its
bucket, respectively].
Another useful idea is that it is sufficient to maintain only C + 1
buckets, rather than (N − 1)C +1, thereby significantly saving in memory.
The reason is that if we are currently searching bucket b, then all buckets
beyond b+C are known to be empty. To see this, note that the label dj of
any node j must be of the form di +aij , where i is a node that has already
been removed from the candidate list. Since di ≤ b and aij ≤ C, it follows
that dj ≤ b + C.
The idea of using buckets to maintain the nodes of the candidate
list can be generalized considerably. In particular, buckets of width larger
than max
{
1, min(i,j)∈A aij
}
may be used. This results in fewer buckets to
search over, thereby alleviating the O(NC) bottleneck of the running time
of the algorithm. There is a price for this, namely the need to search for a
minimum label node within the current bucket. This search can be speeded
up by using buckets with nonuniform widths, and by breaking down buckets
of large width into buckets of smaller width at the right moment. With
Sec. 2.4 Label Correcting Methods 73
intelligent strategies of this type, one may obtain label setting methods
with very good polynomial complexity bounds; see Johnson [1977], Denardo
and Fox [1979], Ahuja, Mehlhorn, Orlin, and Tarjan [1990]. In practice,
however, the simpler algorithm of Dial has been more popular than these
methods.
2.4 LABEL CORRECTING METHODS
We now turn to the analysis of label correcting methods. In these methods,
the selection of the node to be removed from the candidate list V is simpler
and requires less overhead than in label setting methods, at the expense of
multiple entrances of nodes in V . All of these methods use some type of
queue to maintain the candidate list V . They differ in the way the queue
is structured, and in the choice of the queue position into which nodes
are inserted. In this section, we will discuss some of the most interesting
possibilities.
2.4.1 The Bellman-Ford Method
The simplest label correcting method uses a first-in first-out rule to update
the queue that is used to store the candidate list V . In particular, a node is
always removed from the top of the queue, and a node, upon entrance in the
candidate list, is placed at the bottom of the queue. Thus, it can be seen
that the method operates in cycles of iterations: the first cycle consists of
just iterating on node 1; in each subsequent cycle, the nodes that entered
the candidate list during the preceding cycle, are removed from the list
in the order that they were entered. We will refer to this method as the
Bellman-Ford method , because it is closely related to a method proposed
by Bellman [1957] and Ford [1956] based on dynamic programming ideas
(see Exercise 2.6).
The complexity analysis of the method is based on the following prop-
erty, which we will prove shortly:
Bellman-Ford Property
For each node i and integer k ≥ 1, let
dki = Shortest distance from 1 to i using paths that have k arcs or less,
where dki = ∞ if there is no path from 1 to i with k arcs or less. Then
the label di at the end of the kth cycle of iterations of the Bellman-Ford
method is less or equal to dki .
74 The Shortest Path Problem Chap. 2
In the case where all cycles have nonnegative length, the shortest
distance of every node can be achieved with a path having N − 1 arcs or
less, so the above Bellman-Ford property implies that the method finds
all the shortest distances after at most N − 1 cycles. Since each cycle
of iterations requires a total of O(A) operations (each arc is examined at
most once in each cycle), the running time of the Bellman-Ford method is
O(NA).
To prove the Bellman-Ford property, we first note that
dk+1j = min
{
dkj , min
(i,j)∈A
{dki + aij}
}
, ∀ j, k ≥ 1, (2.10)
since dk+1j is either the length of a path from 1 to j with k arcs or less, in
which case it is equal to dkj , or else it is the length of some path that starts
at 1 goes to a predecessor node i with k arcs or less, and then goes to j
using arc (i, j). We now prove the Bellman-Ford property by induction.
At the end of the 1st cycle, we have for all i,
di =

0 if i = 1,
a1i if i 	= 1 and (1, i) ∈ A,
∞ if i 	= 1 and (1, i) /∈ A,
while
d1i =
{
a1i if (1, i) ∈ A,
∞ if (1, i) /∈ A,
so that di ≤ d1i for all i. Let di and V be the node labels and the contents
of the candidate list at the end of the kth cycle, respectively. Let also di be
the node labels at the end of the (k + 1)st cycle. We assume that di ≤ dki
for all i, and we will show that di ≤ dk+1i for all i. Indeed, by condition
(ii) of Prop. 2.2(a), we have
dj ≤ di + aij , ∀ (i, j) ∈ A with i /∈ V,
and since dj ≤ dj , it follows that
dj ≤ di + aij , ∀ (i, j) ∈ A with i /∈ V. (2.11)
We also have
dj ≤ di + aij , ∀ (i, j) ∈ A with i ∈ V, (2.12)
since at the time when i is removed from V , its current label, call it d̃i,
satisfies d̃i ≤ di, and the label of j is set to d̃i + aij if it exceeds d̃i + aij .
By combining Eqs. (2.11) and (2.12), we see that
dj ≤ min
(i,j)∈A
{di + aij} ≤ min
(i,j)∈A
{dki + aij}, ∀ j, (2.13)
Sec. 2.4 Label Correcting Methods 75
where the second inequality follows by the induction hypothesis. We also
have dj ≤ dj ≤ dkj by the induction hypothesis, so Eq. (2.13) yields
dj ≤ min
{
dkj , min
(i,j)∈A
{dki + aij}
}
= dk+1j ,
where the last equality holds by Eq. (2.10). This completes the induction
proof of the Bellman-Ford property.
The Bellman-Ford method can be used to detect the presence of a
negative cycle. Indeed, from Prop. 2.2, we see that the method fails to
terminate if and only if there exists a path that starts at 1 and contains
a negative cycle. Thus in view of the Bellman-Ford property, such a path
exists if and only if the algorithm has not terminated by the end of N − 1
cycles.
The best practical implementations of label correcting methods are
more sophisticated than the Bellman-Ford method. Their worst-case run-
ning time is no better than the O(NA) time of the Bellman-Ford method,
and in some cases it is considerably slower. Yet their practical performance
is often considerably better. We will discuss next three different types of
implementations.
2.4.2 The D’Esopo-Pape Algorithm
In this method, a node is always removed from the top of the queue used
to maintain the candidate list V . A node, upon entrance in the queue, is
placed at the bottom of the queue if it has never been in the queue before;
otherwise it is placed at the top.
The idea here is that when a node i is removed from the queue, its
label affects the labels of a subset Bi of the neighbor nodes j with (i, j) ∈ A.
When the label of i changes again, it is likely that the labels of the nodes
in Bi will require updating also. It is thus argued that it makes sense to
place the node at the top of the queue so that the labels of the nodes in Bi
get a chance to be updated as quickly as possible.
While this rationale is not quite convincing, it seems to work well in
practice for a broad variety of problems, including types of problems where
there are some negative arc lengths. On the other hand, special examples
have been constructed (Kershenbaum [1981], Shier and Witzgall [1981]),
where the D’Esopo-Pape algorithm performs very poorly. In particular, in
these examples, the number of entrances of some nodes in the candidate
list V is not polynomial. Computational studies have also shown that for
some classes of problems, the practical performance of the D’Esopo-Pape
algorithm can be very poor (Bertsekas [1993a]). Pallottino [1984], and
Gallo and Pallottino [1988] give a polynomial variant of the algorithm,
whose practical performance, however, is roughly similar to the one of the
original version.
76 The Shortest Path Problem Chap. 2
2.4.3 The SLF and LLL Algorithms
These methods are motivated by the hypothesis that when the arc lengths
are nonnegative, the queue management strategy should try to place nodes
with small labels near the top of the queue. For a supporting heuristic
argument, note that for a node j to reenter V , some node i such that
di + aij < dj must first exit V . Thus, the smaller dj was at the previous
exit of j from V the less likely it is that di+aij will subsequently become less
than dj for some node i ∈ V and arc (i, j). In particular, if dj ≤ mini∈V di
and the arc lengths aij are nonnegative, it is impossible that subsequent
to the exit of j from V we will have di + aij < dj for some i ∈ V .
We can think of Dijkstra’s method as implicitly placing at the top of
an imaginary queue the node with the smallest label, thereby resulting in
the minimal number N of iterations. The methods of this section attempt
to emulate approximately the minimum label selection policy of Dijkstra’s
algorithm with a much smaller computational overhead. They are primarily
suitable for the case of nonnegative arc lengths. While they will work even
when there are some negative arc lengths as per Prop. 2.2, there is no
reason to expect that in this case they will terminate faster (or slower)
than any of the other label correcting methods that we will discuss.
A simple strategy for placing nodes with small label near the top of the
queue is the Small Label First method (SLF for short). Here the candidate
list V is maintained as a double ended queue Q. At each iteration, the
node exiting V is the top node of Q. The rule for inserting new nodes is
given below:
SLF Strategy
Whenever a node j enters Q, its label dj is compared with the label
di of the top node i of Q. If dj ≤ di, node j is entered at the top of
Q; otherwise j is entered at the bottom of Q.
The SLF strategy provides a rule for inserting nodes in Q, but always
removes (selects for iteration) nodes from the top of Q. A more sophis-
ticated strategy is to make an effort to remove from Q nodes with small
labels. A simple possibility, called the Large Label Last method (LLL for
short) works as follows: At each iteration, when the node at the top of Q
has a larger label than the average node label in Q (defined as the sum of
the labels of the nodes in Q divided by the cardinality |Q| of Q), this node
is not removed from Q, but is instead repositioned to the bottom of Q.
LLL Strategy
Let i be the top node of Q, and let
Sec. 2.4 Label Correcting Methods 77
a =
∑
j∈Q dj
|Q| .
If di > a, move i to the bottom of Q. Repeat until a node i such that
di ≤ a is found and is removed from Q.
It is simple to combine the SLF queue insertion and the LLL node
removal strategies, thereby obtaining a method referred to as SLF/LLL.
Experience suggests that, assuming nonnegative arc lengths, the SLF,
LLL, and combined SLF/LLL algorithms perform substantially faster than
the Bellman-Ford and the D’Esopo-Pape methods. The strategies are also
well-suited for parallel computation (see Bertsekas, Guerriero, and Mus-
manno [1996]). The combined SLF/LLL method consistently requires a
smaller number of iterations than either SLF or LLL, although the gain in
number of iterations is sometimes offset by the extra overhead.
Regarding the theoretical worst-case performance of the SLF and the
combined SLF/LLL algorithms, an example has been constructed by Chen
and Powell [1997], showing that these algorithms do not have polynomial
complexity in their pure form. However, nonpolynomial behavior seems
to be an extremely rare phenomenon in practice. In any case, one may
construct polynomial versions of the SLF and LLL algorithms, when the
arc lengths are nonnegative. A simple approach is to first sort the outgoing
arcs of each node by length. That is, when a node i is removed from Q, first
examine the outgoing arc from i that has minimum length, then examine
the arc of second minimum length, etc. This approach, due to Chen and
Powell [1997], can be shown to have complexity O(NA2) (see Exercise
2.9). Note, however, that sorting the outgoing arcs of a node by length
may involve significant overhead.
There is also another approach to construct polynomial versions of
the SLF and LLL algorithms (as well as other label correcting methods),
which leads to O(NA) complexity, assuming nonnegative arc lengths. To
see how this works, suppose that in the generic label correcting algorithm,
there is a set of increasing iteration indices t1, t2, . . . , tn+1 such that t1 = 1,
and for i = 1, . . . , n, all nodes that are in V at the start of iteration ti
are removed from V at least once prior to iteration ti+1. Because all arc
lengths are nonnegative, this guarantees that the minimum label node of
V at the start of iteration ti will never reenter V after iteration ti+1. Thus
the candidate list must have no more than N − i nodes at the start of
iteration ti+1, and must become empty prior to iteration tN+1. Thus, if
the running time of the algorithm between iterations ti and ti+1 is bounded
by R, the total running time of the algorithm will be bounded by NR, and
if R is polynomially bounded, the running time of the algorithm will also
be polynomially bounded.
Specializing now to the SLF and LLL cases, assume that between
78 The Shortest Path Problem Chap. 2
iterations ti and ti+1, each node is inserted at the top of Q for a number
of times that is bounded by a constant and that (in the case of SLF/LLL)
the total number of repositionings is bounded by a constant multiple of
A. Then it can be seen that the running time of the algorithm between
iterations ti and ti+1 is O(A), and therefore the complexity of the algorithm
is O(NA).
To modify SLF or SLF/LLL so that they have an O(NA) worst-case
complexity, based on the preceding result, it is sufficient that we fix an inte-
ger k > 1, and that we separate the iterations of the algorithm in successive
blocks of kN iterations each. We then impose an additional restriction that,
within each block of kN iterations, each node can be inserted at most k−1
times at the top of Q [that is, after the (k− 1)th insertion of a node to the
top of Q within a given block of kN iterations, all subsequent insertions of
that node within that block of kN iterations must be at the bottom of Q].
In the case of SLF/LLL, we also impose the additional restriction that the
total number of repositionings within each block of kN iterations should
be at most kA (that is, once the maximum number of kA repositionings is
reached, the top node of Q is removed from Q regardless of the value of its
label). The worst-case running times of the modified algorithms are then
O(NA). In practice, it is highly unlikely that the restrictions introduced
into the algorithms to guarantee O(NA) complexity will ever be exercised
if k is larger than a small number such as 3 or 4.
2.4.4 The Threshold Algorithm
Similar to the SLF/LLL methods, the premise of this algorithm is also
that, for nonnegative arc lengths, the number of iterations is reduced by
removing from the candidate list V nodes with relatively small label. In
the threshold algorithm, V is organized into two distinct queues Q′ and Q′′
using a threshold parameter s. The queue Q′ contains nodes with “small”
labels; that is, it contains only nodes whose labels are no larger than s.
At each iteration, a node is removed from Q′, and any node j to be added
to the candidate list is inserted at the bottom of Q′ or Q′′ depending on
whether dj ≤ s or dj > s, respectively. When the queue Q′ is exhausted,
the entire candidate list is repartitioned. The threshold is adjusted, and
the queues Q′ and Q′′ are recalculated, so that Q′ consists of the nodes
with labels that are no larger than the new threshold.
To understand how the threshold algorithm works, consider the case
of nonnegative arc lengths, and suppose that at time t the candidate list
is repartitioned based on a new threshold value s, and that at some sub-
sequent time t′ > t the queue Q′ gets exhausted. Then at time t′, all the
nodes of the candidate list have label greater than s. In view of the nonneg-
ativity of the arc lengths, this implies that all nodes with label less than or
equal to s will not reenter the candidate list after time t′. In particular, all
nodes that exited the candidate list between times t and t′ become perma-
Sec. 2.4 Label Correcting Methods 79
nently labeled at time t′ and never reenter the candidate list. We may thus
interpret the threshold algorithm as a block version of Dijkstra’s method ,
whereby a whole subset of nodes becomes permanently labeled when the
queue Q′ gets exhausted.
The preceding interpretation suggests that the threshold algorithm is
suitable primarily for the case of nonnegative arc lengths (even though it
will work in general). Furthermore, the performance of the algorithm is
quite sensitive to the method used to adjust the threshold. For example, if
s is taken to be equal to the current minimum label, the method is identical
to Dijkstra’s algorithm; if s is larger than all node labels, Q′′ is empty and
the algorithm reduces to the generic label correcting method. With an
effective choice of threshold, the practical performance of the algorithm
is very good. A number of heuristic approaches have been developed for
selecting the threshold (see Glover, Klingman, and Phillips [1985], and
Glover, Klingman, Phillips, and Schneider [1985]). If all arc lengths are
nonnegative, a bound O(NA) on the operation count of the algorithm can
be shown; see Exercise 2.8(c).
Combinations of the Threshold and the SLF/LLL Methods
We mentioned earlier that the threshold algorithm may be interpreted as
a block version of Dijkstra’s method, whereby attention is restricted to the
subset of nodes that belong to the queue Q′, until this subset becomes per-
manently labeled. The algorithm used to permanently label the nodes of Q′
is essentially the Bellman-Ford algorithm restricted to the subgraph defined
by Q′. It is possible to use a different algorithm for this purpose, based for
example on the SLF and LLL strategies. This motivates combinations of
the threshold and the SLF/LLL algorithms.
In particular, the LLL strategy can be used when selecting a node
to exit the queue Q′ in the threshold algorithm (the top node of Q′ is
repositioned to the bottom of Q′ if its label is found smaller than the
average label in Q′). Furthermore, whenever a node enters the queue Q′,
it is added, according to the SLF strategy, at the bottom or the top of Q′
depending on whether its label is greater than the label of the top node of
Q′ or not. The same policy is used when transferring to Q′ the nodes of
Q′′ whose labels do not exceed the current threshold parameter. Thus the
nodes of Q′′ are transferred to Q′ one-by-one, and they are added to the
top or the bottom of Q′ according to the SLF strategy. Finally, the SLF
strategy is also followed when a node enters the queue Q′′.
Generally, the threshold strategy and the SLF/LLL strategy are com-
plementary and work synergistically. Computational experience suggests
that their combination performs extremely well in practice, and typically
results in an average number of iterations per node that is only slightly
larger than the minimum of 1 achieved by Dijkstra’s method. At the same
80 The Shortest Path Problem Chap. 2
time, these combined methods require considerably less overhead than Di-
jkstra’s method.
2.4.5 Comparison of Label Setting and Label Correcting
Let us now try to compare the two major special cases of the generic
algorithm, label setting and label correcting methods, assuming that the
arc lengths are nonnegative.
We mentioned earlier that label setting methods offer a better guar-
antee of good performance than label correcting methods, because their
worst-case running time is more favorable. In practice, however, there
are several considerations that argue in favor of label correcting methods.
One such consideration is that label correcting methods, because of their
inherent flexibility, are better suited for exploiting advanced initialization.
Another consideration is that when the graph is acyclic, label cor-
recting methods can be adapted to exploit the problem’s structure, so that
each node enters and exits the candidate list only once, thereby nullifying
the major advantage of label setting methods (see Exercise 2.10). The cor-
responding running time is O(A), which is the minimum possible. Note
that an important class of problems involving an acyclic graph is dynamic
programming (cf. Fig. 2.1).
A third consideration is that in practice, the graphs of shortest path
problems are often sparse; that is, the number of arcs is much smaller
than the maximum possible N2. In this case, efficient label correcting
methods tend to have a faster practical running time than label setting
methods. To understand the reason, note that all shortest path methods
require the unavoidable O(A) operations needed to scan once every arc, plus
some additional time which we can view as “overhead.” The overhead of
the popular label setting methods is roughly proportional to N in practice
(perhaps times a slowly growing factor, like log N), as argued earlier for the
binary heap method and Dial’s algorithm. On the other hand, the overhead
of label correcting methods grows linearly with A (times a factor that likely
grows slowly), because for the most popular methods, the average number
of node entrances in the queue per node is typically not much larger than
1. Thus, we may conclude that the overhead ratio of label correcting to
label setting methods is roughly
A
N
· (a constant factor).
The constant factor above depends on the particular method used and
may vary slowly with the problem size, but is typically much less than 1.
Thus, the overhead ratio favors label correcting methods for a sparse graph
(A << N2), and label setting methods for a dense graph (A ≈ N2). This
is consistent with empirical observations.
Sec. 2.5 Single Origin/Single Destination Methods 81
Let us finally note that label setting methods can take better advan-
tage of situations where only a small subset of the nodes are destinations, as
will be seen in the next section. This is also true of the auction algorithms
to be discussed in Section 2.6.
2.5 SINGLE ORIGIN/SINGLE DESTINATION METHODS
In this section, we discuss the adaptation of our earlier single origin/all
destination algorithms to the case where there is only one destination, call
it t, and we want to find the shortest distance from the origin node 1 to
t. We could of course use our earlier all-destinations algorithms, but some
improvements are possible.
2.5.1 Label Setting
Suppose that we use the label setting method. Then we can stop the
method when the destination t becomes permanently labeled; further com-
putation will not improve the label dt (Exercise 2.13 sharpens this criterion
in the case where min{i|(i,t)∈A} aij > 0). If t is closer to the origin than
many other nodes, the saving in computation time will be significant. Note
that this approach can also be used when there are several destinations.
The method is stopped when all destinations have become permanently
labeled.
Another possibility is to use a two-sided label setting method ; that is,
a method that simultaneously proceeds from the origin to the destination
and from the destination to the origin. In this method, we successively label
permanently the closest nodes to the origin (with their shortest distance
from the origin) and the closest nodes to the destination (with their shortest
distance to the destination). It can be shown that when some node gets
permanently labeled from both sides, the labeling can stop; by combining
the forward and backward paths of each labeled node and by comparing
the resulting origin-to-destination paths, one can obtain a shortest path.
Exercise 2.14 develops in some detail this approach, which can often lead
to a dramatic reduction in the total number of iterations. However, the
approach does not work when there are multiple destinations.
2.5.2 Label Correcting
Unfortunately, when label correcting methods are used, it may not be easy
to realize the savings just discussed in connection with label setting. The
difficulty is that even after we discover several paths to the destination t
(each marked by an entrance of t into V ), we cannot be sure that better
paths will not be discovered later. In the presence of additional problem
82 The Shortest Path Problem Chap. 2
structure, however, the number of times various nodes will enter V can be
reduced considerably, as we now explain.
Suppose that at the start of the algorithm we have, for each node i, an
underestimate ui of the shortest distance from i to t (we require ut = 0).
For example, if all arc lengths are nonnegative we may take ui = 0 for
all i. (We do not exclude the possibility that ui = −∞ for some i, which
corresponds to the case where no underestimate is available for the shortest
distance of i.) The following is a modified version of the generic shortest
path algorithm.
Initially
V = {1},
d1 = 0, di = ∞, ∀ i 	= 1.
The algorithm proceeds in iterations and terminates when V is empty. The
typical iteration (assuming V is nonempty) is as follows.
Iteration of the Generic Single Origin/Single Destination Al-
gorithm
Remove a node i from V . For each outgoing arc (i, j) ∈ A, if
di + aij < min{dj , dt − uj},
set
dj := di + aij
and add j to V if it does not already belong to V .
The preceding iteration is the same as the one of the all-destinations
generic algorithm, except that the test di + aij < dj for entering a node j
into V is replaced by the more stringent test di + aij < min{dj , dt − uj}.
(In fact, when the trivial underestimate uj = −∞ is used for all j 	= t the
two iterations coincide.) To understand the idea behind the iteration, note
that the label dj corresponds at all times to the best path found thus far
from 1 to j (cf. Prop. 2.2). Intuitively, the purpose of entering node j in
V when its label is reduced is to generate shorter paths to the destination
that pass through node j. If Pj is the path from 1 to j corresponding to
di + aij , then di + aij + uj is an underestimate of the shortest path length
among the collection of paths Pj that first follow path Pj to node j and
then follow some other path from j to t. However, if
di + aij + uj ≥ dt,
the current best path to t, which corresponds to dt, is at least as short as
any of the paths in the collection Pj , which have Pj as their first component.
Sec. 2.5 Single Origin/Single Destination Methods 83
2
1
1
1
1
0
0
1
4
3
2
5
Origin Destination
Iter. # Candidate List V Node Labels Node out of V
1 {1} (0,∞,∞,∞,∞) 1
2 {2, 3} (0, 2, 1,∞,∞) 2
3 {3, 5} (0, 2, 1,∞, 2) 3
4 {5} (0, 2, 1,∞, 2) 5
Ø (0, 2, 1,∞, 2)
Figure 2.9: Illustration of the generic single origin/single destination algorithm.
Here the destination is t = 5 and the underestimates of shortest distances to t are
ui = 0 for all i. Note that at iteration 3, when node 3 is removed from V , the
label of node 4 is not improved to d4 = 2 and node 4 is not entered in V . The
reason is that d3 + a34 (which is equal to 2) is not smaller than d5 − u4 (which is
also equal to 2). Note also that upon termination the label of a node other than
t may not be equal to its shortest distance (e.g. d4).
It is unnecessary to consider such paths, and for this reason node j need
not be entered in V . In this way, the number of node entrances in V may
be sharply reduced.
Figure 2.9 illustrates the algorithm. The following proposition proves
its validity.
Proposition 2.4: Consider the generic single origin/single destina-
tion algorithm.
(a) At the end of each iteration, if dj < ∞, then dj is the length of
some path that starts at 1 and ends at j.
(b) If the algorithm terminates, then upon termination, either dt <
∞, in which case dt is the shortest distance from 1 to t, or else
there is no path from 1 to t.
(c) If the algorithm does not terminate, there exist paths of arbi-
trarily small length that start at 1.
84 The Shortest Path Problem Chap. 2
Proof: (a) The proof is identical to the corresponding part of Prop. 2.2.
(b) If upon termination we have dt = ∞, then the extra test di +aij +uj <
dt for entering V is always passed, so the algorithm generates the same
label sequences as the generic (all destinations) shortest path algorithm.
Therefore, Prop. 2.2(b) applies and shows that there is no path from 1 to t.
It will thus be sufficient to prove this part assuming that we have dt < ∞
upon termination.
Let dj be the final values of the labels dj obtained upon termination
and suppose that dt < ∞. Assume, to arrive at a contradiction, that there
is a path Pt = (1, j1, j2, . . . , jk, t) that has length Lt with Lt < dt. For
m = 1, . . . , k, let Ljm be the length of the path Pm = (1, j1, j2, . . . , jm).
Let us focus on the node jk preceding t on the path Pt. We claim that
Ljk < djk . Indeed, if this were not so, then jk must have been removed at
some iteration from V with a label djk satisfying djk ≤ Ljk . If dt is the
label of t at the start of that iteration, we would then have
djk + ajkt ≤ Ljk + ajkt = Lt < dt ≤ dt,
implying that the label of t would be reduced at that iteration from dt to
djk + ajkt, which is less than the final label dt – a contradiction.
Next we focus on the node jk−1 preceding jk and t on the path Pt. We
use a similar (though not identical) argument to show that Ljk−1 < djk−1 .
Indeed, if this were not so, then jk−1 must have been removed at some
iteration from V with a label djk−1 satisfying djk−1 ≤ Ljk−1 . If djk and dt
are the labels of jk and t at the start of that iteration, we would then have
djk−1 + ajk−1jk ≤ Ljk−1 + ajk−1jk = Ljk < djk ≤ djk ,
and since Ljk + ujk ≤ Lt < dt ≤ dt, we would also have
djk−1 + ajk−1jk < dt − ujk .
From the above two equations, it follows that the label of jk would be
reduced at that iteration from djk to djk−1 + ajk−1t, which is less than the
final label djk – a contradiction.
Proceeding similarly, we obtain Ljm < djm for all m = 1, . . . , k, and
in particular a1j1 = Lj1 < dj1 . Since
a1j1 + uj1 ≤ Lt < dt,
and dt is monotonically nonincreasing throughout the algorithm, we see
that at the first iteration we will have a1j1 < min{dj1 , dt − uj1}, so j1 will
enter V with the label a1j1 , which cannot be less than the final label dj1 .
This is a contradiction; the proof of part (b) is complete.
(c) The proof is identical to the proof of Prop. 2.2(c). Q.E.D.
Sec. 2.5 Single Origin/Single Destination Methods 85
There are a number of possible implementations of the algorithm of
this subsection, which parallel the ones given earlier for the many destina-
tions problem. An interesting possibility to speed up the algorithm arises
when an overestimate vj of the shortest distance from j to t is known a
priori . (We require that vt = 0. Furthermore, we set vj = ∞ if no overes-
timate is known for j.) The idea is that the method still works if the test
di +aij < dt−uj is replaced by the possibly sharper test di +aij < D−uj ,
where D is any overestimate of the shortest distance from 1 to t with D ≤ dt
(check the proof of Prop. 2.4). We can obtain estimates D that may be
strictly smaller than dt by using the scalars vj as follows: each time the
label of a node j is reduced, we check whether dj + vj < D; if this is so, we
replace D by dj + vj . In this way, we make the test for future admissibility
into the candidate list V more stringent and save some unnecessary node
entrances in V .
Advanced Initialization
We finally note that similar to the all-destinations case, the generic sin-
gle origin/single destination method need not be started with the initial
conditions
V = {1}, d1 = 0, di = ∞, ∀ i 	= 1.
The algorithm works correctly using several other initial conditions. One
possibility is to use for each node i, an initial label di that is either ∞ or
else it is the length of a path from 1 to i, and to take V = {i | di < ∞}.
A more sophisticated alternative is to initialize V so that it contains all
nodes i such that
di + aij < min{dj , dt − uj} for some (i, j) ∈ A.
This kind of initialization can be extremely useful when a “good”
path
P = (1, i1, . . . , ik, t)
from 1 to t is known or can be found heuristically, and the arc lengths are
nonnegative so that we can use the underestimate ui = 0 for all i. Then
we can initialize the algorithm with
di =
{
Length of portion of path P from 1 to i if i ∈ P ,
∞ if i /∈ P ,
V = {1, i1, . . . , ik}.
If P is a near-optimal path and consequently the initial value dt is near its
final value, the test for future admissibility into the candidate list V will
be relatively tight from the start of the algorithm and many unnecessary
entrances of nodes into V may be saved. In particular, it can be seen that
all nodes whose shortest distances from the origin are greater or equal to
the length of P will never enter the candidate list.
86 The Shortest Path Problem Chap. 2
2.6 AUCTION ALGORITHMS
In this section, we discuss another class of algorithms for finding a shortest
path from an origin s to a destination t. These are called auction algorithms
because they can be shown to be closely related to the naive auction algo-
rithm for the assignment problem discussed in Section 1.3 (see Bertsekas
[1991a], Section 4.3.3, or Bertsekas [1991b]). The main algorithm is very
simple. It maintains a single path starting at the origin. At each iteration,
the path is either extended by adding a new node, or contracted by deleting
its terminal node. When the destination becomes the terminal node of the
path, the algorithm terminates.
To get an intuitive sense of the algorithm, think of a mouse moving in
a graph-like maze, trying to reach a destination. The mouse criss-crosses
the maze, either advancing or backtracking along its current path. Each
time the mouse backtracks from a node, it records a measure of the desir-
ability of revisiting and advancing from that node in the future (this will
be represented by a suitable variable). The mouse revisits and proceeds
forward from a node when the node’s measure of desirability is judged
superior to those of other nodes. The algorithm emulates efficiently this
search process using simple data structures.
The algorithm maintains a path P =
(
(s, i1), (i1, i2), . . . , (ik−1, ik)
)
with no cycles, and modifies P using two operations, extension and con-
traction. If ik+1 is a node not on P and (ik, ik+1) is an arc, an extension of
P by ik+1 replaces P by the path
(
(s, i1), (i1, i2), . . . , (ik−1, ik), (ik, ik+1)
)
.
If P does not consist of just the origin node s, a contraction of P replaces
P by the path
(
(s, i1), (i1, i2), . . . , (ik−2, ik−1)
)
.
We introduce a variable pi for each node i, called the price of node i.
We denote by p the price vector consisting of all node prices. The algorithm
maintains a price vector p satisfying together with P the following property
pi ≤ aij + pj , for all arcs (i, j), (2.14)
pi = aij + pj , for all arcs (i, j) of P . (2.15)
If we view the prices pi as the negative of the labels di that we used earlier,
we see that the above conditions are equivalent to the CS conditions (2.2)
and (2.4). Consequently, we will also refer to Eqs. (2.14) and (2.15) as the
CS conditions. We assume that the initial pair (P, p) satisfies CS. This is
not restrictive, since the default pair
P = (s), pi = 0, for all i
satisfies CS in view of the nonnegative arc length assumption. To define
the algorithm we also need to assume that all cycles have positive length;
Exercise 2.17 indicates how this assumption can be relaxed.
Sec. 2.6 Auction Algorithms 87
It can be shown that if a pair (P, p) satisfies the CS conditions, then
the portion of P between node s and any node i ∈ P is a shortest path
from s to i, while ps−pi is the corresponding shortest distance. To see this,
note that by Eq. (2.15), pi − pk is the length of the portion of P between
i and k, and that every path connecting i to k must have length at least
equal to pi − pk [add Eq. (2.14) along the arcs of the path].
The algorithm proceeds in iterations, transforming a pair (P, p) sat-
isfying CS into another pair satisfying CS. At each iteration, the path P
is either extended by a new node or is contracted by deleting its terminal
node. In the latter case the price of the terminal node is increased strictly.
A degenerate case occurs when the path consists of just the origin node s;
in this case the path is either extended or is left unchanged with the price
ps being strictly increased. The iteration is as follows.
Iteration of the Auction Algorithm
Let i be the terminal node of P . If
pi < min
{j|(i,j)∈A}
{
aij + pj
}
,
go to Step 1; else go to Step 2.
Step 1 (Contract path): Set
pi := min
{j|(i,j)∈A}
{
aij + pj
}
,
and if i 	= s, contract P . Go to the next iteration.
Step 2 (Extend path): Extend P by node ji where
ji = arg min
{j|(i,j)∈A}
{
aij + pj
}
(ties are broken arbitrarily). If ji is the destination t, stop; P is the
desired shortest path. Otherwise, go to the next iteration.
It is easily seen that the algorithm maintains CS. Furthermore, the
addition of the node ji to P following an extension does not create a cycle,
since otherwise, in view of the condition pi ≤ aij + pj , for every arc (i, j)
of the cycle we would have pi = aij + pj . By adding this equality along
the cycle, we see that the length of the cycle must be zero, which is not
possible by our assumptions.
Figure 2.10 illustrates the algorithm. It can be seen from the exam-
ple of this figure that the terminal node traces the tree of shortest paths
from the origin to the nodes that are closer to the origin than the given
88 The Shortest Path Problem Chap. 2
destination. This behavior is typical when the initial prices are all zero (see
Exercise 2.19).
Shortest path problem with arc
lengths as shown
3
1
2
4
1.5
2 3
Origin Destination
Trajectory of terminal node
and final prices generated by
the algorithm
1
2
4
3
p1  = 2.51
p3  = 3
p2  = 1.5
p4  = 0
Iteration # Path P prior Price vector p prior Type of action
to iteration to iteration during iteration
1 (1) (0, 0, 0, 0) contraction at 1
2 (1) (1, 0, 0, 0) extension to 2
3 (1, 2) (1, 0, 0, 0) contraction at 2
4 (1) (1, 1.5, 0, 0) contraction at 1
5 (1) (2, 1.5, 0, 0) extension to 3
6 (1, 3) (2, 1.5, 0, 0) contraction at 3
7 (1) (2, 1.5, 3, 0) contraction at 1
8 (1) (2.5, 1.5, 3, 0) extension to 2
9 (1, 2) (2.5, 1.5, 3, 0) extension to 4
10 (1, 2, 4) (2.5, 1.5, 3, 0) stop
Figure 2.10: An example illustrating the auction algorithm starting with P = (1)
and p = 0.
There is an interesting interpretation of the CS conditions in terms of
a mechanical model, due to Minty [1957]. Think of each node as a ball, and
for every arc (i, j), connect i and j with a string of length aij . (This requires
that aij = aji > 0, which we assume for the sake of the interpretation.) Let
the resulting balls-and-strings model be at an arbitrary position in three-
Sec. 2.6 Auction Algorithms 89
dimensional space, and let pi be the vertical coordinate of node i. Then
the CS condition pi − pj ≤ aij clearly holds for all arcs (i, j), as illustrated
in Fig. 2.11(b). If the model is picked up and left to hang from the origin
node (by gravity – strings that are tight are perfectly vertical), then for
all the tight strings (i, j) we have pi − pj = aij , so any tight chain of
strings corresponds to a shortest path between the end nodes of the chain,
as illustrated in Fig. 2.11(c). In particular, the length of the tight chain
connecting the origin node s to any other node i is ps −pi and is also equal
to the shortest distance from s to i.
(a)
1
2
3
4
1 1.5
2 3
Shortest path problem with 
arc lengths shown next to the arcs.
Node 1 is the origin.
Node 4 is the destination.
4
2
1
3
(b)
1
1.5
3
2
p1  
p3  
p2 
p4  
3
4
1
(c)
2
1
1.5
2
3
p1  = 2.5
p3  = 0.5
p2  = 1.5
p4  = 0
Figure 2.11: Illustration of the CS conditions for the shortest path problem. If
each node is a ball, and for every arc (i, j), nodes i and j are connected with a
string of length aij , the vertical coordinates pi of the nodes satisfy pi − pj ≤ aij ,
as shown in (b) for the problem given in (a). If the model is picked up and left
to hang from the origin node s, then ps − pi gives the shortest distance to each
node i, as shown in (c).
The algorithm can also be interpreted in terms of the balls-and-strings
model; it can be viewed as a process whereby nodes are raised in stages as
illustrated in Fig. 2.12. Initially all nodes are resting on a flat surface. At
each stage, we raise the last node in a tight chain that starts at the origin
to the level at which at least one more string becomes tight.
The following proposition establishes the validity of the auction algo-
rithm.
90 The Shortest Path Problem Chap. 2
4
(b)
(a)
1
2
3
4
1
2 3
Shortest path problem with 
arc lengths shown next to the arcs.
Node 1 is the origin.
Node 4 is the destination.
1.5
Initial position After 1st stage After 2nd stage
After 3rd stage After 4th stage After 5th stage
0
0.5
1.0
1.5
2.0
0
0.5
1.0
1.5
2.0
2.5
3.0
2 3
1
4
4
2
3
1
4
2
3
1
2
3
1
3
1
4
2
2 31 4
The ball marked by gray (the terminal node of the
current path P) is raised at each stage.
Figure 2.12: Illustration of the auction algorithm in terms of the balls-and-
strings model for the problem shown in (a). The model initially rests on a flat
surface, and various balls are then raised in stages. At each stage we raise a
single ball i = t (marked by gray), which is at a lower level than the origin s
and can be reached from s through a sequence of tight strings; i should not have
any tight string connecting it to another ball, which is at a lower level, that is, i
should be the last ball in a tight chain hanging from s. (If s does not have any
tight string connecting it to another ball, which is at a lower level, we use i = s.)
We then raise i to the first level at which one of the strings connecting it to a
ball at a lower level becomes tight. Each stage corresponds to a contraction plus
all the extensions up to the next contraction. The ball i, which is being raised,
corresponds to the terminal node of the current path P .
Sec. 2.6 Auction Algorithms 91
Proposition 2.5: If there exists at least one path from the origin
to the destination, the auction algorithm terminates with a shortest
path from the origin to the destination. Otherwise the algorithm never
terminates and ps → ∞.
Proof: We first show by induction that (P, p) satisfies the CS conditions
pi ≤ aij + pj , for all arcs (i, j), (2.16)
pi = aij + pj , for all arcs (i, j) of P , (2.17)
throughout the algorithm. Indeed, the initial pair satisfies CS by assump-
tion. Consider an iteration that starts with a pair (P, p) satisfying CS and
produces a pair (P , p). Let i be the terminal node of P . If
pi = aiji + pji = min{j|(i,j)∈A}
{
aij + pj
}
, (2.18)
then P is the extension of P by the node ji and p = p, implying that the
CS condition (2.17) holds for all arcs of P as well as arc (i, ji) [since ji
attains the minimum in Eq. (2.18)].
Suppose next that
pi < min
{j|(i,j)∈A}
{
aij + pj
}
.
Then if P is the degenerate path (s), the CS conditions hold vacuously.
Otherwise, P is obtained by contracting P , and for all nodes j ∈ P , we
have pj = pj , implying the CS conditions (2.16) and (2.17) for arcs outgoing
from nodes of P . Also, for the terminal node i, we have
pi = min{j|(i,j)∈A}
{
aij + pj
}
,
implying the CS condition (2.16) for arcs outgoing from that node as well.
Finally, since pi > pi and pk = pk for all k 	= i, we have pk ≤ akj + pj for
all arcs (k, j) outgoing from nodes k /∈ P . This completes the induction
proof that (P, p) satisfies CS throughout the algorithm.
Assume first that there is a path from node s to the destination t.
By adding the CS condition (2.16) along that path, we see that ps − pt is
an underestimate of the (finite) shortest distance from s to t. Since ps is
monotonically nondecreasing, and pt is fixed throughout the algorithm, it
follows that ps must stay bounded.
We next claim that pi must stay bounded for all i. Indeed, in order to
have pi → ∞, node i must become the terminal node of P infinitely often.
92 The Shortest Path Problem Chap. 2
Each time this happens, ps − pi is equal to the shortest distance from s to
i, which is a contradiction since ps is bounded.
We next show that the algorithm terminates. Indeed, it can be seen
with a straightforward induction argument that for every node i, pi is either
equal to its initial value, or else it is the length of some path starting at i
plus the initial price of the final node of the path; we call this the modified
length of the path. Every path from s to i can be decomposed into a path
with no cycles together with a finite number of cycles, each having positive
length by assumption, so the number of distinct modified path lengths
within any bounded interval is bounded. Now pi was shown earlier to be
bounded, and each time i becomes the terminal node by extension of the
path P , pi is strictly larger over the preceding time i became the terminal
node of P , corresponding to a strictly larger modified path length. It follows
that the number of times i can become a terminal node by extension of the
path P is bounded. Since the number of path contractions between two
consecutive path extensions is bounded by the number of nodes in the
graph, the number of iterations of the algorithm is bounded, implying that
the algorithm terminates.
Assume now that there is no path from node s to the destination.
Then, the algorithm will never terminate, so by the preceding argument,
some node i will become the terminal node by extension of the path P
infinitely often and pi → ∞. At the end of iterations where this happens,
ps − pi must be equal to the shortest distance from s to i, implying that
ps → ∞. Q.E.D.
Nonpolynomial Behavior and Graph Reduction
A drawback of the auction algorithm as described above is that its running
time can depend on the arc lengths. A typical situation arises in graphs
involving a cycle with relatively small length, as illustrated in Fig. 2.13.
It is possible to turn the algorithm into one that is polynomial, by using
some variations of the algorithm. In these variations, in addition to the
extension and contraction operations, an additional reduction operation is
introduced whereby some unnecessary arcs of the graph are deleted. We
briefly describe the simplest of these variations, and we refer to Bertsekas,
Pallottino, and Scutellà [1995] for other more sophisticated variations and
complexity analysis.
This variant of the auction algorithm has the following added feature:
each time that a node j becomes the terminal node of the path P through
an extension using arc (i, j), all incoming arcs (k, j) of j with k 	= i are
deleted from the graph. Also, each time that a node j with no outgoing
arcs becomes the terminal node of P , the path P is contracted and the
node j is deleted from the graph. It can be seen that the arc deletion
process leaves the shortest distance from s to t unaffected, and that the
Sec. 2.6 Auction Algorithms 93
1
1 1
11
L
Origin Destination
2 3
4
5
Figure 2.13: Example graph for which
the number of iterations of the algo-
rithm is not polynomially bounded. The
lengths are shown next to the arcs and
L > 1. By tracing the steps of the algo-
rithm starting with P = (1) and p = 0,
we see that the price of node 3 will be
first increased by 1 and then it will be
increased by increments of 3 (the length
of the cycle) as many times as is neces-
sary for p3 to reach or exceed L.
algorithm terminates either by finding a shortest path from s to t or by
deleting s, depending on whether there exists at least one path from s to t
or not. It can also be seen that this is also true even if there are cycles of
zero length. Thus, in addition to addressing the nonpolynomial behavior,
the graph reduction process deals effectively with the case where there are
zero length cycles.
As an illustration, the reader may apply the algorithm with graph
reduction to the example of Fig. 2.13. After the first iteration when node 2
becomes the terminal node of P for the first time, the arc (4, 2) is deleted,
and the cycle (2, 3, 4, 2) that caused the nonpolynomial behavior is elim-
inated. Furthermore, once node 4 becomes the terminal node of P , it
gets deleted because it no longer has any outgoing arcs. The number of
iterations required is greatly reduced.
The effect of graph reduction may be enhanced by introducing a fur-
ther idea due to Cerulli (see Cerulli, Festa, and Raiconi [1997a]). In par-
ticular, if in the process of eliminating arcs, a node i is left with only one
outgoing arc (i, j), it may be “combined” with node j. This can be done
efficiently, and may result in significant computational savings for some
problem types (particularly those involving a sparse graph).
In addition to graph reduction, there are a number of ideas that can be
used to implement efficiently the auction algorithm; see Bertsekas [1991b],
Bertsekas, Pallottino, and Scutellà [1995], and Cerulli, Festa, and Raiconi
[1997b].
The Case of Multiple Destinations or Multiple Origins
To solve the problem with multiple destinations and a single origin, one
can simply run the algorithm until every destination becomes the terminal
node of the path at least once. Also, to solve the problem with multiple
origins and a single destination, one can combine several versions of the
algorithm – one for each origin. However, the different versions can share a
common price vector, since regardless of the origin considered, the condition
pi ≤ aij + pj is always maintained. There are several ways to operate such
a method; they differ in the policy used for switching between different
94 The Shortest Path Problem Chap. 2
origins. One possibility is to run the algorithm for one origin and, after the
shortest path is obtained, to switch to the next origin (without changing the
price vector), and so on, until all origins are exhausted. Another possibility,
which is probably preferable in most cases, is to rotate between different
origins, switching from one origin to another, if a contraction at the origin
occurs or the destination becomes the terminal node of the current path.
The Reverse Algorithm
For problems with one origin and one destination, a two-sided version of
the algorithm is particularly effective. This method maintains, in addition
to the path P , another path R that ends at the destination. To understand
this version, we first note that in shortest path problems, one can exchange
the role of origins and destinations by reversing the direction of all arcs.
It is therefore possible to use a destination-oriented version of the auction
algorithm that maintains a path R that ends at the destination and changes
at each iteration by means of a contraction or an extension. This algorithm,
called the reverse algorithm, is mathematically equivalent to the earlier
(forward) auction algorithm. Initially, in the reverse algorithm, R is any
path ending at the destination, and p is any price vector satisfying CS
together with R; for example,
R = (t), pi = 0, for all i,
if all arc lengths are nonnegative.
Iteration of the Reverse Algorithm
Let j be the starting node of R. If
pj > max
{i|(i,j)∈A}
{
pi − aij
}
,
go to Step 1; else go to Step 2.
Step 1: (Contract path) Set
pj := max
{i|(i,j)∈A}
{
pi − aij
}
,
and if j 	= t, contract R, (that is, delete the starting node j of R). Go
to the next iteration.
Step 2: (Extend path) Extend R by node ij , (that is, make ij the
starting node of R, preceding j), where
Sec. 2.6 Auction Algorithms 95
ij = arg max
{i|(i,j)∈A}
{
pi − aij
}
(ties are broken arbitrarily). If ij is the origin s, stop; R is the desired
shortest path. Otherwise, go to the next iteration.
The reverse algorithm is most helpful when it is combined with the
forward algorithm. In a combined algorithm, initially we have a price
vector p, and two paths P and R, satisfying CS together with p, where P
starts at the origin and R ends at the destination. The paths P and R
are extended and contracted according to the rules of the forward and the
reverse algorithms, respectively, and the combined algorithm terminates
when P and R have a common node. Both P and R satisfy CS together
with p throughout the algorithm, so when P and R meet, say at node i,
the composite path consisting of the portion of P from s to i followed by
the portion of R from i to t will be shortest.
Combined Forward/Reverse Auction Algorithm
Step 1: (Run forward algorithm) Execute several iterations of the
forward algorithm (subject to the termination condition), at least one
of which leads to an increase of the origin price ps. Go to Step 2.
Step 2: (Run reverse algorithm) Execute several iterations of the
reverse algorithm (subject to the termination condition), at least one
of which leads to a decrease of the destination price pt. Go to Step 1.
The combined forward/reverse algorithm can also be interpreted in
terms of the balls-and-strings model of Fig. 2.11. Again, all nodes are
resting initially on a flat surface. When the forward part of the algorithm
is used, we raise nodes in stages as illustrated in Fig. 2.12. When the
reverse part of the algorithm is used, we lower nodes in stages; at each
stage, we lower the top node in a tight chain that ends at the destination
to the level at which at least one more string becomes tight.
The combined forward/reverse auction algorithm can be easily ex-
tended to handle single-origin/many-destination problems. One may start
the reverse portion of the algorithm from any destination for which a
shortest path has not yet been found. Based on experiments with ran-
domly generated problems, the combined forward/reverse auction algo-
rithm (with graph reduction to eliminate nonpolynomial behavior) out-
performs substantially and often dramatically its closest competitors for
single-origin/few-destination problems (see Bertsekas [1991b], and Bert-
sekas, Pallottino, and Scutellà [1995]). The intuitive reason for this is that
through the mechanism of the reverse portion of the algorithm, the selected
96 The Shortest Path Problem Chap. 2
destinations are reached by the forward portion faster than other nodes,
thereby leading to faster termination.
2.7 MULTIPLE ORIGIN/MULTIPLE DESTINATION METHODS
In this section, we consider the all-pairs shortest path problem, where we
want to find a shortest path from each node to each other node. The Floyd-
Warshall algorithm is specifically designed for this problem, and it is not
any faster when applied to the single destination problem. It starts with
the initial condition
D0ij =
{
aij if (i, j) ∈ A,
∞ otherwise,
and generates sequentially for all k = 0, 1, . . . , N − 1, and all nodes i and
j,
Dk+1ij =
{
min
{
Dkij , D
k
i(k+1) + D
k
(k+1)j
}
if j 	= i,
∞ otherwise.
An induction argument shows that Dkij gives the shortest distance
from node i to node j using only nodes from 1 to k as intermediate nodes.
Thus, DNij gives the shortest distance from i to j (with no restriction on
the intermediate nodes). There are N iterations, each requiring O(N2)
operations, for a total of O(N3) operations.
Unfortunately, the Floyd-Warshall algorithm cannot take advantage
of sparsity of the graph. It appears that for sparse problems it is typically
better to apply a single origin/all destinations algorithm separately for each
origin. If all the arc lengths are nonnegative, a label setting method can
be used separately for each origin. If there are negative arc lengths (but no
negative length cycles), one can of course apply a label correcting method
separately for each origin, but there is another alternative that results in
a superior worst-case complexity. It is possible to apply a label correcting
method only once to a single origin/all destinations problem and obtain
an equivalent all-pairs shortest path problem with nonnegative arc lengths;
the latter problem can be solved using N separate applications of a label
setting method. This alternative is based on the following proposition,
which applies to the general minimum cost flow problem.
Proposition 2.7: Every minimum cost flow problem with arc costs
aij such that all simple forward cycles have nonnegative cost is equiv-
alent to another minimum cost flow problem involving the same graph
and nonnegative arc costs âij of the form
Sec. 2.7 Multiple Origin/Multiple Destination Methods 97
âij = aij + di − dj , ∀ (i, j) ∈ A,
where the scalars di can be found by solving a single origin/all des-
tinations shortest path problem. The two problems are equivalent in
the sense that they have the same constraints, and the cost function
of one is the same as the cost function of the other plus a constant.
Proof: Let (N ,A) be the graph of the given problem. Introduce a new
node 0 and an arc (0, i) for each i ∈ N , thereby obtaining a new graph
(N ′,A′). Consider the shortest path problem involving this graph, with
arc lengths aij for the arcs (i, j) ∈ A and 0 for the arcs (0, i). Since all
incident arcs of node 0 are outgoing, all simple forward cycles of (N ′,A′) are
also simple forward cycles of (N ,A) and, by assumption, have nonnegative
length. Since any forward cycle can be decomposed into a collection of
simple forward cycles (cf. Exercise 1.4 in Chapter 1), all forward cycles
(not necessarily simple) of (N ′,A′) have nonnegative length. Furthermore,
there is at least one path from node 0 to every other node i, namely the
path consisting of arc (0, i). Therefore, the shortest distances di from node
0 to all other nodes i can be found by a label correcting method, and by
Prop. 2.2, we have
âij = aij + di − dj ≥ 0, ∀ (i, j) ∈ A.
Let us now view
∑
(i,j)∈A âijxij as the cost function of a minimum
cost flow problem involving the graph (N ,A) and the constraints of the
original problem. We have∑
(i,j)∈A
âijxij =
∑
(i,j)∈A
(
aij + di − dj
)
xij
=
∑
(i,j)∈A
aijxij +
∑
i∈N
di
 ∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji

=
∑
(i,j)∈A
aijxij +
∑
i∈N
disi,
where si is the supply of node i. Thus, the two cost functions
∑
(i,j)∈A âijxij
and
∑
(i,j)∈A aijxij differ by the constant
∑
i∈N disi. Q.E.D.
It can be seen now that the all-pairs shortest path problem can be
solved by using a label correcting method to solve the single origin/all
destinations problem described in the above proof, thereby obtaining the
scalars di and
âij = aij + di − dj , ∀ (i, j) ∈ A,
98 The Shortest Path Problem Chap. 2
and by then applying a label setting method N times to solve the all-
pairs shortest path problem involving the nonnegative arc lengths âij . The
shortest distance Dij from i to j is obtained by subtracting di−dj from the
shortest distance from i to j found by the label setting method. To estimate
the running time of this approach, note that the label correcting method
requires O(NA) computation using the Bellman-Ford method, and each
of the N applications of the label setting method require less than O(N2)
computation (the exact count depends on the method used). Thus the
overall running time is less that the O(N3) required by the Floyd-Warshall
algorithm, at least for sparse graphs.
Still another possibility for solving the all-pairs shortest path problem
is to solve N separate single origin/all destinations problems but to also use
the results of the computation for one origin to start the computation for
the next origin; see our earlier discussion of initialization of label correcting
methods and also the discussion at the end of Section 5.2.
2.8 NOTES, SOURCES, AND EXERCISES
The work on the shortest path problem is very extensive, so we will re-
strict ourselves to citing the references that relate most to the material
presented. Literature surveys are given by Dreyfus [1969], Deo and Pang
[1984], and Gallo and Pallottino [1988]. The latter reference also contains
codes for the most popular shortest path methods, and extensive compu-
tational comparisons. A survey of applications in transportation networks
is given in Pallottino and Scutellà [1997a]. Parallel computation aspects of
shortest path algorithms, including asynchronous versions of some of the
algorithms developed here, are discussed in Bertsekas and Tsitsiklis [1989],
and Kumar, Grama, Gupta, and Karypis [1994].
The generic algorithm was proposed as a unifying framework of many
of the existing shortest path algorithms in Pallottino [1984], and Gallo
and Pallottino [1986]. The first label setting method was suggested in
Dijkstra [1959], and also independently in Dantzig [1960], and Whitting
and Hillier [1960]. The binary heap method was proposed by Johnson
[1972]. Dial’s algorithm (Dial [1969]) received considerable attention after
the appearance of the paper by Dial, Glover, Karney, and Klingman [1979];
see also Denardo and Fox [1979].
The Bellman-Ford algorithm was proposed in Bellman [1957] and
Ford [1956] in the form given in Exercise 2.6, where the labels of all nodes
are iterated simultaneously. The D’Esopo-Pape algorithm appeared in
Pape [1974] based on an earlier suggestion of D’Esopo. The SLF and
SLF/LLL methods were proposed by Bertsekas [1993a], and by Bertsekas,
Guerriero, and Musmanno [1996]. Chen and Powell [1997] gave a simple
polynomial version of the SLF method (Exercise 2.9). The threshold al-
Sec. 2.8 Notes, Sources, and Exercises 99
gorithm was developed by Glover, Klingman, and Phillips [1985], Glover,
Klingman, Phillips, and Schneider [1985], and Glover, Glover, and Kling-
man [1986].
Two-sided label setting methods for the single origin/single destina-
tion problem (Exercise 2.14) were proposed by Nicholson [1966]; see also
Helgason, Kennington, and Stewart [1993], which contains extensive com-
putational results. The idea of using underestimates of the shortest dis-
tance to the destination in label correcting methods originated with the A∗
algorithm, a shortest path algorithm that is popular in artificial intelligence
(see Nilsson [1971], [1980], and Pearl [1984]).
The Floyd-Warshall algorithm was given in Floyd [1962] and uses a
theorem due to Warshall [1962]. Alternative algorithms for the all-pairs
problem are given in Dantzig [1967] and Tabourier [1973]. Reoptimization
approaches that use the results of a shortest path computation for one
origin to initialize the computation for other origins are given by Gallo and
Pallottino [1982], and Florian, Nguyen, and Pallottino [1981].
The auction algorithm for shortest paths is due to Bertsekas [1991b].
The idea of graph reduction was proposed by Pallottino and Scutellà [1991],
and an O(N3) implementation of an auction algorithm with graph reduc-
tion was given by Bertsekas, Pallottino, and Scutellà [1995]. An analysis
of a parallel asynchronous implementation is given by Polymenakos and
Bertsekas [1994]. Some variants of the auction algorithm that use slightly
different price updating schemes have been proposed in Cerulli, De Leone,
and Piacente [1992], and Bertsekas [1992b] (see Exercise 2.33). A method
that combines the auction algorithm with some dual price iterations was
given by Pallottino and Scutellà [1997b].
E X E R C I S E S
2.1
Consider the graph of Fig. 2.14. Find a shortest path from 1 to all nodes using
the binary heap method, Dial’s algorithm, the D’Esopo-Pape algorithm, the SLF
method, and the SLF/LLL method.
2.2
Suppose that the only arcs that have negative lengths are outgoing from the
origin node 1. Show how to adapt Dijkstra’s algorithm so that it solves the
all-destinations shortest path problem in at most N − 1 iterations.
100 The Shortest Path Problem Chap. 2
1
4
3
2
5
6
54 2
8
1
5
5
0
9
1
0 Figure 2.14: Graph for Exercise
2.1. The arc lengths are the num-
bers shown next to the arcs.
2.3
Give an example of a problem where the generic shortest path algorithm will
reduce the label of node 1 to a negative value.
2.4 (Shortest Path Tree Construction)
Consider the single origin/all destinations shortest path problem and assume that
all cycles have nonnegative length. Consider the generic algorithm of Section 2.2,
and assume that each time a label dj is decreased to di+aij the arc (i, j) is stored
in an array PRED(j ). Consider the subgraph of the arcs PRED(j ), j ∈ N , j = 1.
Show that at the end of each iteration this subgraph is a tree rooted at the origin,
and that upon termination it is a tree of shortest paths.
2.5 (Uniqueness of Solution of Bellman’s Equation)
Assume that all cycles have positive length. Show that if the scalars d1, d2, . . . , dN
satisfy
dj = min
(i,j)∈A
{di + aij}, ∀ j = 1,
d1 = 0,
then for all j, dj is the shortest distance from 1 to j. Show by example that this
need not be true if there is a cycle of length 0. Hint : Consider the arcs (i, j)
attaining the minimum in the above equation and consider the paths formed by
these arcs.
2.6 (The Original Bellman-Ford Method)
Consider the single origin/all destinations shortest path problem. The Bellman-
Ford method, as originally proposed by Bellman and Ford, updates the labels of
all nodes simultaneously in a single iteration. In particular, it starts with the
initial conditions
d01 = 0, d
0
j = ∞, ∀ j = 1,
Sec. 2.8 Notes, Sources, and Exercises 101
and generates dkj , k = 1, 2, . . ., according to
dk1 = 0, d
k
j = min
(i,j)∈A
{dk−1i + aij}, ∀ j = 1.
(a) Show that for all j = 1 and k ≥ 1, dkj is the shortest distance from 1 to
j using paths with k arcs or less, where dkj = ∞ means that all the paths
from 1 to j have more than k arcs.
(b) Assume that all cycles have nonnegative length. Show that the algorithm
terminates after at most N iterations, in the sense that for some k ≤ N we
have dkj = d
k−1
j for all j. Conclude that the running time of the algorithm
is O(NA).
2.7 (The Bellman-Ford Method with Arbitrary Initialization)
Consider the single origin/all destinations shortest path problem and the follow-
ing variant of the Bellman-Ford method of Exercise 2.6:
dk1 = 0, d
k
j = min
(i,j)∈A
{dk−1i + aij}, ∀ j = 1,
where each of the initial iterates d0i is an arbitrary scalar or ∞, except that
d01 = 0. We say that the algorithm terminates after k iterations if d
k
i = d
k−1
i for
all i.
(a) Given nodes i = 1 and j = 1, define
wkij = minimum path length over all paths starting at i, ending at j,
and having k arcs (wkij = ∞ if there is no such path).
For i = 1 and j = 1, define
wk1j = minimum path length over all paths from 1 to j having k arcs or less
(wk1j = ∞ if there is no such path).
Show by induction that
dkj = min
i=1,...,N
{d0j + wkij}, ∀ j = 2, . . . , N, and k ≥ 1.
(b) Assume that there exists a path from 1 to every node i and that all cycles
have positive length. Show that the method terminates at some iteration
k, with dki equal to the shortest distances d
∗
i . Hint : For all i = 1 and j = 1,
limk→∞ w
k
ij = ∞, while for all j = 1, wk1j = d∗j for all k ≥ N − 1.
(c) Under the assumptions of part (b), show that if d0i ≥ d∗i for all i = 1, the
method terminates after at most m∗ + 1 iterations, where
m∗ = max
i=1
mi ≤ N − 1,
102 The Shortest Path Problem Chap. 2
and mi is the smallest number of arcs contained in a shortest path from 1
to i.
(d) Under the assumptions of part (b), let
β = max
i=1
{d∗i − d0i },
and assume that β > 0. Show that the method terminates after at most
k + 1 iterations, where k = N − 1 if the graph is acyclic, and k = N − 2 −
β/L	 if the graph has cycles, where
L = min
All simple cycles
Length of the cycle
Number of arcs on the cycle
,
is the, so called, minimum cycle mean of the graph. Note: See Section 4.1 of
Bertsekas and Tsitsiklis [1989] for related analysis, and an example showing
that the given upper bound on the number of iterations for termination is
tight.
(e) (Finding the minimum cycle mean) Consider the following Bellman-Ford-
like algorithm:
dk(i) = min
(i,j)∈A
{aij + dk−1(j)}, ∀ i = 1, . . . , N,
d0(i) = 0, ∀ i = 1, . . . , N.
We assume that there exists at least one cycle, but we do not assume that
all cycles have positive length. Show that the minimum cycle mean L of
part (d) is given by
L = min
i=1,...,N
max
k=0,...,N−1
dN (i) − dk(i)
N − k .
Hint : Show that dk(i) is equal to the minimum path length over all paths
that start at i and have k arcs.
2.8 (Complexity of the Generic Algorithm)
Consider the generic algorithm, assuming that all arc lengths are nonnegative.
(a) Consider a node j satisfying at some time
dj ≤ di, ∀ i ∈ V.
Show that this relation will be satisfied at all subsequent times and that j
will never again enter V . Furthermore, dj will remain unchanged.
(b) Suppose that the algorithm is structured so that it removes from V a node
of minimum label at least once every k iterations (k is some integer). Show
that the algorithm will terminate in at most kN iterations.
(c) Show that the running time of the threshold algorithm is O(NA). Hint :
Define a cycle to be a sequence of iterations between successive repartition-
ings of the candidate list V . In each cycle, the node of V with minimum
label at the start of the cycle will be removed from V during the cycle.
Sec. 2.8 Notes, Sources, and Exercises 103
2.9 (Complexity of the SLF Method)
The purpose of this exercise, due to Chen and Powell [1997], is to show one way
to use the SLF method so that it has polynomial complexity. Suppose that the
outgoing arcs of each node have been presorted in increasing order by length.
The effect of this, in the context of the generic shortest path algorithm, is that
when a node i is removed from the candidate list, we first examine the outgoing
arc from i that has minimum length, then we examine the arc of second minimum
length, etc. Show an O(NA2) complexity bound for the method.
2.10 (Label Correcting for Acyclic Graphs)
Consider the problem of finding shortest paths from the origin node 1 to all
destinations, and assume that the graph does not contain any forward cycles.
Let Tk be the set of nodes i such that every path from 1 to i has k arcs or more,
and there exists a path from 1 to i with exactly k arcs. For each i, if i ∈ Tk define
INDEX(i) = k. Consider a label setting method that selects a node i from the
candidate list that has minimum INDEX(i).
(a) Show that the method terminates and that each node visits the candidate
list at most once.
(b) Show that the sets Tk can be constructed in O(A) time, and that the
running time of the algorithm is also O(A).
2.11
Consider the graph of Fig. 2.14. Find a shortest path from node 1 to node 6
using the generic single origin/single destination method of Section 2.5 with all
distance underestimates equal to zero.
2.12
Consider the problem of finding a shortest path from the origin 1 to a single des-
tination t, subject to the constraint that the path includes a given node s. Show
how to solve this problem using the single origin/single destination algorithms of
Section 2.5.
2.13 (Label Setting for Few Destinations)
Consider a label setting approach for finding shortest paths from the origin node
1 to a selected subset of destinations T . Let
a = min
{(i,t)∈A|t∈T}
ait,
and assume that a > 0. Show that one may stop the method when the node of
minimum label in V has a label dmin that satisfies
dmin + a ≥ max
t∈T
dt.
104 The Shortest Path Problem Chap. 2
2.14 (Two-Sided Label Setting)
Consider the shortest path problem from an origin node 1 to a destination node
t, and assume that all arc lengths are nonnegative. This exercise considers an
algorithm where label setting is applied simultaneously and independently from
the origin and from the destination. In particular, the algorithm maintains a
subset of nodes W , which are permanently labeled from the origin, and a subset
of nodes V , which are permanently labeled from the destination. When W and
V have a node i in common the algorithm terminates. The idea is that a shortest
path from 1 to t cannot contain a node j /∈ W ∪V ; any such path must be longer
than a shortest path from 1 to i followed by a shortest path from i to t (unless j
and i are equally close to both 1 and to t).
Consider two subsets of nodes W and V with the following properties:
(1) 1 ∈ W and t ∈ V .
(2) W and V have nonempty intersection.
(3) If i ∈ W and j /∈ W , then the shortest distance from 1 to i is less than or
equal to the shortest distance from 1 to j.
(4) If i ∈ V and j /∈ V , then the shortest distance from i to t is less than or
equal to the shortest distance from j to t.
Let d1i be the shortest distance from 1 to i using paths all the nodes of which,
with the possible exception of i, lie in W (d1i = ∞ if no such path exists), and let
dti be the shortest distance from i to t using paths all the nodes of which, with
the possible exception of i, lie in V (dti = ∞ if no such path exists).
(a) Show that such W , V , d1i , and d
t
i can be found by applying a label setting
method simultaneously for the single origin problem with origin node 1 and
for the single destination problem with destination node t.
(b) Show that the shortest distance D1t from 1 to t is given by
D1t = min
i∈W
{
d1i + d
t
i
}
= min
i∈W∪V
{
d1i + d
t
i
}
= min
i∈V
{
d1i + d
t
i
}
.
(c) Show that the nonempty intersection condition (2) can be replaced by the
condition mini∈W
{
d1i + d
t
i
}
≤ maxi∈W d1i + maxi∈V dti.
2.15
Apply the forward/reverse auction algorithm to the example of Fig. 2.13, and
show that it terminates in a number of iterations that does not depend on the
large arc length L. Construct a related example for which the number of iterations
of the forward/reverse algorithm is not polynomially bounded.
2.16 (Finding an Initial Price Vector)
In order to initialize the auction algorithm, one needs a price vector p satisfying
the condition
pi ≤ aij + pj , ∀ (i, j) ∈ A. (2.19)
Sec. 2.8 Notes, Sources, and Exercises 105
Such a vector may not be available if some arc lengths are negative. Further-
more, even if all arc lengths are nonnegative, there are many cases where it is
important to use a favorable initial price vector in place of the default choice
p = 0. This possibility arises in a reoptimization context with slightly different
arc length data, or with a different origin and/or destination. This exercise gives
an algorithm to obtain a vector p satisfying the condition (2.19), starting from
another vector p satisfying the same condition for a different set of arc lengths
aij .
Suppose that we have a vector p̄ and a set of arc lengths {āij}, satisfying
pi ≤ aij + pj for all arcs (i, j), and we are given a new set of arc lengths {aij}.
(For the case where some arc lengths aij are negative, this situation arises with
p = 0 and aij = max{0, aij}.) Consider the following algorithm that maintains a
subset of arcs E and a price vector p, and terminates when E is empty. Initially
E = {(i, j) ∈ A | aij < āij , i = t}, p = p̄.
The typical iteration is as follows:
Step 1 (Select arc to scan): If E is empty, stop; otherwise, remove an arc
(i, j) from E and go to Step 2.
Step 2 (Add affected arcs to E): If pi > aij + pj , set
pi := aij + pj
and add to E every arc (k, i) with k = t that does not already belong to E .
Assuming that each node i is connected to the destination t with at least one
path, and that all cycle lengths are positive, show that the algorithm terminates
with a price vector p satisfying
pi ≤ aij + pj , ∀ (i, j) ∈ A with i = t.
2.17 (Extension for the Case of Zero Length Cycles)
Extend the auction algorithm for the case where all arcs have nonnegative length
but some cycles may consist exclusively of zero length arcs. Hint : Any cycle of
zero length arcs generated by the algorithm can be treated as a single node. An
alternative is the idea of graph reduction discussed in Section 2.6.
2.18
Consider the two single origin/single destination shortest path problems shown
in Fig. 2.15.
(a) Show that the number of iterations required by the forward auction algo-
rithm is estimated accurately by
nt − 1 +
∑
i∈I, i=t
(2ni − 1),
106 The Shortest Path Problem Chap. 2
where ni is the number of nodes in a shortest path from 1 to i. Show also
that the corresponding running times are O(N2).
(b) Show that for the problem of Fig. 2.15(a) the running time of the for-
ward/reverse auction algorithm (with a suitable “reasonable” rule for switch-
ing between the forward and reverse algorithms) is O(N2) (the number of
iterations is roughly half the corresponding number for the forward algo-
rithm). Show also that for the problem of Fig. 2.15(b) the running time of
the forward/reverse algorithm is O(N).
1 2 3 N - 1 t.  .  .  . (a)
1
2
3
N-1
t. . . .
(b)
Figure 2.15: Shortest path problems for Exercise 2.18. In problem (a) the arc
lengths are equal to 1. In problem (b), the length of each arc (1, i) is i, and the
length of each arc (i, t) is N .
2.19
In the auction algorithm of Section 2.6, let ki be the first iteration at which node
i becomes the terminal node of the path P . Show that if ki < kj , then the
shortest distance from 1 to i is less or equal to the shortest distance from 1 to j.
2.20 (A Forward/Reverse Version of Dijkstra’s Algorithm)
Consider the single origin/single destination shortest path problem and assume
that all arc lengths are nonnegative. Let node 1 be the origin, let node t be
the destination, and assume that there exists at least one path from 1 to t.
This exercise provides a forward/reverse version of Dijkstra’s algorithm, which
is motivated by the balls-and-strings model analogy of Figs. 2.11 and 2.12. In
particular, the algorithm may be interpreted as alternately lifting the model
upward from the origin (the following Step 1), and pulling the model downward
from the destination (the following Step 2). The algorithm maintains a price
vector p and two node subsets W1 and Wt. Initially, p satisfies the CS condition
pi ≤ aij + pj , ∀ (i, j) ∈ A, (2.20)
Sec. 2.8 Notes, Sources, and Exercises 107
W1 = {1}, and Wt = {t}. One may view W1 and Wt as the sets of permanently
labeled nodes from the origin and from the destination, respectively. The algo-
rithm terminates when W1 and Wt have a node in common. The typical iteration
is as follows:
Step 1 (Forward Step): Find
γ+ = min{aij + pj − pi | (i, j) ∈ A, i ∈ W1, j /∈ W1}
and let
V1 = {j /∈ W1 | γ+ = aij + pj − pi for some i ∈ W1}.
Set
pi :=
{
pi + γ
+ if i ∈ W1,
pi if i /∈ W1.
Set
W1 := W1 ∪ V1.
If W1 and Wt have a node in common, terminate the algorithm; otherwise, go to
Step 2.
Step 2 (Backward Step): Find
γ− = min{aji + pi − pj | (j, i) ∈ A, i ∈ Wt, j /∈ Wt}
and let
Vt = {j /∈ Wt | γ− = aji + pi − pj for some i ∈ Wt}.
Set
pi :=
{
pi − γ− if i ∈ Wt,
pi if i /∈ Wt.
Set
Wt := Wt ∪ Vt.
If W1 and Wt have a node in common, terminate the algorithm; otherwise, go to
Step 1.
(a) Show that throughout the algorithm, the condition (2.20) is maintained.
Furthermore, for all i ∈ W1, p1 −pi is equal to the shortest distance from 1
to i. Similarly, for all i ∈ Wt, pi − pt is equal to the shortest distance from
i to t. Hint : Show that if i ∈ W1, there exists a path from 1 to i such that
pm = amn + pn for all arcs (m, n) of the path.
(b) Show that the algorithm terminates and that upon termination, p1 − pt is
equal to the shortest distance from 1 to t.
(c) Show how the algorithm can be implemented so that its running time is
O(N2). Hint : Let dmn denote the shortest distance from m to n. Maintain
the labels
v+j = min{d1i + aij | i ∈ W1, (i, j) ∈ A}, ∀ j /∈ W1,
108 The Shortest Path Problem Chap. 2
v−j = min{aji + dit | i ∈ Wt, (j, i) ∈ A}, ∀ j /∈ Wt.
Let p0j be the initial price of node j. Show that
γ+ = min
{
min
j /∈W1, j /∈Wt
(
v+j + p
0
j
)
, pt + min
j /∈W1, j∈Wt
(
v+j + djt
)}
−p1, (2.21)
γ− = min
{
min
j /∈W1, j /∈Wt
(
v−j − p0j
)
, −p1 + min
j∈W1, j /∈Wt
(
v−j + d1j
)}
+ pt.
(2.22)
Use these relations to calculate γ+ and γ− in O(N) time.
(d) Show how the algorithm can be implemented using binary heaps so that
its running time is O(A log N). Hint : One possibility is to use four heaps
to implement the minimizations in Eqs. (2.21) and (2.22).
(e) Apply the two-sided version of Dijkstra’s algorithm with arc lengths aij +
pj − pi of Exercise 2.14, and with the termination criterion of part (c) of
that exercise. Show that the resulting algorithm is equivalent to the one of
the present exercise.
2.21
Consider the all-pairs shortest path problem, and suppose that the minimum
distances dij to go from any i to any j have been found. Suppose that a single
arc length amn is reduced to a value amn < amn. Show that if dnm + amn ≥ 0,
the new shortest distances can be obtained by
dij = min{dij , dim + amn + dnj}.
What happens if dnm + amn < 0?
2.22 (The Doubling Algorithm)
The doubling algorithm for solving the all-pairs shortest path problem is given by
D1ij =
{
aij if (i, j) ∈ A,
0 if i = j,
∞ otherwise,
D2kij =
{
minm
{
Dkim + D
k
mj
}
if i = j, k = 1, 2, . . . , 
log(N − 1),
0 if i = j, k = 1, 2, . . . , 
log(N − 1).
Show that for i = j, Dkij gives the shortest distance from i to j using paths with
2k−1 arcs or fewer. Show also that the running time is O
(
N3 log m∗
)
, where m∗
is the maximum number of arcs in a shortest path.
Sec. 2.8 Notes, Sources, and Exercises 109
2.23 (Dynamic Programming)
Consider the dynamic programming problem of Example 2.2. The standard dy-
namic programming algorithm is given by the recursion
Jk(xk) = min
uk
{
gk(xk, uk) + Jk+1(xk+1)
}
, k = 0, . . . , N − 1,
starting with
JN (xN ) = G(xN ).
(a) In terms of the shortest path reformulation in Fig. 2.1, interpret Jk(xk) as
the shortest distance from node xk at stage k to the terminal node t.
(b) Show that the dynamic programming algorithm can be viewed as a spe-
cial case of the generic label correcting algorithm with a special order for
selecting nodes to exit the candidate list.
(c) Assume that gk(xk, uk) ≥ 0 for all xk, uk, and k. Suppose that by us-
ing some heuristic we can construct a “good” suboptimal control sequence
(u0, u1, . . . , uN−1). Discuss how to use this sequence for initialization of a
single origin/single destination label correcting algorithm (cf. the discussion
of Section 2.5).
2.24 (Forward Dynamic Programming)
Given a problem of finding a shortest path from node s to node t, we can obtain
an equivalent “reverse” shortest path problem, where we want to find a shortest
path from t to s in a graph derived from the original by reversing the direction of
all the arcs, while keeping their length unchanged. Apply this transformation to
the dynamic programming problem of Example 2.2 and Exercise 2.23, and derive
a dynamic programming algorithm that proceeds forwards rather than backwards
in time.
2.25 (k Shortest Node-Disjoint Paths)
The purpose of this exercise, due to Castañon [1990], is to formulate a class of
multiple shortest path problems and to indicate the method for their solution.
Consider a graph with an origin 1, a destination t, and a length for each arc.
We want to find k paths from 1 to t which share no node other 1 and t and
which are such that the sum of the k path lengths is minimum. Formulate this
problem as a minimum cost flow problem. (For an auction algorithm that solves
this problem, see Bertsekas and Castañon [1993c].) Hint : Replace each node i
other than 1 and t with two nodes i and i′ and a connecting arc (i, i′) with flow
bounds 0 ≤ xii′ ≤ 1.
110 The Shortest Path Problem Chap. 2
2.26 (k-Level Shortest Path Problems)
The purpose of this exercise, due to Shier [1979], and Guerriero, Lacagnina,
Musmanno, and Pecorella [1997], is to introduce an approach for extending the
generic algorithm to the solution of a class of multiple shortest path problems.
Consider the single origin/many destinations shortest path context, where node
1 is the origin, assuming that no cycles of negative length exist. Let di(1) denote
the shortest distance from node 1 to node i. Sequentially, for k = 2, 3, . . ., denote
by di(k) the minimum of the lengths of paths from 1 to i that have length greater
than di(k − 1) [if there is no path from 1 to i with length greater than di(k − 1),
then di(k) = ∞]. We call di(k) the k-level shortest distance from 1 to i.
(a) Show that for k > 1, {di(k) | i = 1, . . . , N} are the k-level shortest distances
if and only if di(k − 1) ≤ di(k) with strict inequality if di(k − 1) < ∞, and
furthermore
di(k) = min
(i,j)∈A
{
li(k, j) + aij
}
, i = 1, . . . , N,
where
li(k, j) =
{
di(k − 1) if dj(k − 1) < di(k − 1) + aij ,
di(k) if dj(k − 1) = di(k − 1) + aij .
(b) Extend the generic shortest path algorithm of Section 2.2 so that it simul-
taneously finds the k-level shortest distances for all k = 1, 2, . . . , K, where
K is some positive integer.
2.27 (Clustering)
We have a set of N objects 1, . . . , N arranged in a given order. We want to
group these objects in clusters that contain consecutive objects. For each subset
i, i+1, . . . , i+k, there is an associated cost c(i, k). We want to find the grouping
that minimizes the sum of the clusters’ cost. Use the ideas of the paragraphing
problem (Example 2.4) to formulate this problem as a shortest path problem.
2.28 (Path Bottleneck Problem)
Consider the framework of the shortest path problem. For any path P , define
the bottleneck arc of P as an arc that has maximum length over all arcs of P .
Consider the problem of finding a path connecting two given nodes and having
minimum length of bottleneck arc. Derive an analog of Prop. 2.1 for this problem.
Consider also a single origin/all destinations version of this problem. Develop an
analog of the generic algorithm of Section 2.2 and prove an analog of Prop. 2.2.
Hint : Replace di + aij with max{di, aij}.
Sec. 2.8 Notes, Sources, and Exercises 111
2.29 (Shortest Path Problems with Negative Cycles)
Consider the problem of finding a simple forward path between an origin and a
destination node that has minimum length. Show that even if there are negative
cycles, the problem can be formulated as a minimum cost flow problem involving
node throughput constraints of the form
0 ≤
∑
{j|(i,j)∈A}
xij ≤ 1, ∀ i.
2.30 (Minimum Weight Spanning Trees)
Given a graph (N ,A) and a weight wij for each arc (i, j), consider the problem
of finding a spanning tree with minimum sum of arc weights. This is not a
shortest path problem and in fact it is not even a special case of the minimum
cost flow problem. However, it has a similar graph structure to the one of the
shortest path problem. Note that the orientation of the arcs does not matter
here. In particular, if (i, j) and (j, i) are arcs, any one of them can participate
in a spanning tree solution, and the arc having greater weight can be a priori
eliminated.
(a) Consider the problem of finding a shortest path from node 1 to all nodes
with arc lengths equal to wij . Give an example where the shortest path
spanning tree is not a minimum weight spanning tree.
(b) Let us define a fragment to be a subgraph of a minimum weight spanning
tree; for example the subgraph consisting of any subset of nodes and no
arcs is a fragment. Given a fragment F , let us denote by A(F ) the set of
arcs (i, j) such that either i or j belong to F , and if (i, j) is added to F
no cycle is closed. Show that if F is a fragment, then by adding to F an
arc of A(F ) that has minimum weight over all arcs of A(F ) we obtain a
fragment.
(c) Consider a greedy algorithm that starts with some fragment, and at each
iteration, adds to the current fragment F an arc of A(F ) that has minimum
weight over all arcs of A(F ). Show that the algorithm terminates with a
minimum weight spanning tree.
(d) Show that the complexity of the greedy algorithm is O(NA), where N is
the number of nodes and A is the number of arcs.
(e) The Prim-Dijkstra algorithm is the special case of the greedy algorithm
where the initial fragment consists of a single node. Provide an O(N2), im-
plementation of this algorithm. Hint : Together with the kth fragment Fk,
maintain for each j /∈ Fk the node nk(i) ∈ Fk such that the arc connecting
j and nk(i) has minimum weight.
112 The Shortest Path Problem Chap. 2
2.31 (Shortest Path Problems with Losses)
Consider a vehicle routing/shortest path-like problem where a vehicle wants to
go on a forward path from an origin node 1 to a destination node t in a graph
that has no forward cycles. For each arc (i, j) there is a given length aij , but
there is also a given probability pij ∈ [0, 1] that the vehicle will be destroyed in
crossing the arc. The length of a path is now a random variable, and is equal
to the sum of the arc lengths on the path up to the time the vehicle reaches its
destination or gets destroyed, whichever comes first. We want to find a forward
path P = (1, i1, . . . , ik, t) whose expected length, given by
p1i1
(
a1i1 + pi1i2
(
ai1i2 + pi2i3(· · · + piktaikt) · · ·
)
,
is minimized, where pij = 1 − pij is the probability of survival in crossing the
arc (i, j). Give an algorithm of the dynamic programming type for solving this
problem (cf. Exercise 2.5). Does the problem always make sense when the graph
has some forward cycles?
2.32
Consider the one origin-all destinations problem and the generic algorithm of
Section 2.2. Assume that there exists a path that starts at node 1 and contains
a cycle with negative length. Assume also that the generic algorithm is operated
so that if a given node belongs to the candidate list for an infinite number of
iterations, then it also exits the list an infinite number of times. Show that there
exists at least one node j such that the sequence of labels dj generated by the
algorithm diverge to −∞. Hint : Argue that if the limits dj of all the label nodes
are finite, then we have dj ≤ di + aij for all arcs (i, j).
2.33 (A Modified Auction Algorithm for Shortest Paths)
Consider the problem of finding a shortest path from node 1 to a node t, as-
suming that there exists at least one such path and that all cycles have positive
length. This exercise deals with a modified version of the auction algorithm,
which was developed in Bertsekas [1992b], motivated by a similar earlier algo-
rithm by Cerulli, De Leone, and Piacente [1994]. This modified version aims to
use larger price increases than the original method. The algorithm maintains a
price vector p and a simple path P that starts at the origin, and is initialized
with P = (1) and any price vector p satisfying
p1 = ∞,
pi ≤ aij + pj , ∀ (i, j) ∈ A with i = 1.
The algorithm terminates when the destination t becomes the terminal node of
P . To describe the algorithm, define
A(i) = {j | (i, j) ∈ A} ∪ {i}, ∀ i ∈ N ,
Sec. 2.8 Notes, Sources, and Exercises 113
aii = 0, ∀ i ∈ N .
The typical iteration is as follows:
Let i be the terminal node of P , and let ji be such that
ji = arg min
j∈A(i)
{
aij + pj
}
,
with the extra requirement that ji = i whenever possible; that is, we choose
ji = i whenever the minimum above is attained for some j = i. Set
pji := min
j∈A(i), j =ji
{
aij + pj
}
− aiji .
If ji = i contract P ; otherwise extend P by node ji.
Note that if a contraction occurs, we have ji = i = 1 and the price of the terminal
node pi is strictly increased. Note also that when an extension occurs from the
terminal node i to a neighbor ji = i, the price pji may be increased strictly , while
in the original auction algorithm there is no price change. Furthermore, the CS
condition pi ≤ aij + pj for all (i, j) is not maintained. Show that:
(a) The algorithm maintains the conditions
πi = aij + pj , ∀ (i, j) ∈ P,
πi = pi, ∀ i /∈ P,
where
πi = min
{
pi, min
{j|(i,j)∈A}
{aij + pj}
}
, ∀ i ∈ N .
(b) Throughout the algorithm, P is a shortest path between its endnodes. Hint :
Show that if P̃ is another path with the same endnodes, we have
Length of P̃ − Length of P =
∑
{k|k∈P̃ , k/∈P}
(πk − pk) −
∑
{k|k∈P, k/∈P̃}
(πk − pk)
≥ 0.
(c) The algorithm terminates with a shortest path from 1 to t. Note: This is
challenging. A proof is given in Bertsekas [1992b].
(d) Convert the shortest path problem to an equivalent assignment problem
for which the conditions of part (a) are the complementary slackness con-
ditions. Show that the algorithm is essentially equivalent to a naive auction
algorithm applied to the equivalent assignment problem.
114 The Shortest Path Problem Chap. 2
2.34 (Continuous Space Shortest Path Problems)
Consider a continuous-time dynamic system whose state x(t) =
(
x1(t), x2(t)
)
evolves in two-dimensional space according to the differential equations
ẋ1(t) = u1(t), ẋ2(t) = u2(t)
where for each time t, u(t) =
(
u1(t), u2(t)
)
is a two-dimensional control vector
with unit norm. We want to find a state trajectory that starts at a given point
x(0), ends at another given point x(T ), and minimizes∫ T
0
r
(
x(t)
)
dt,
where r(·) is a given nonnegative and continuous function. The final time T and
the control trajectory {u(t) | 0 ≤ t ≤ T} are subject to optimization. Suppose
we discretize the plane with a mesh of size δ that passes through x(0) and x(T ),
and we introduce a shortest path problem of going from x(0) to x(T ) using moves
of the following type: from each mesh point x = (x1, x2) we can go to each of
the mesh points (x1 + δ, x2), (x1 − δ, x2), (x1, x2 + δ), and (x1, x2 − δ), at a cost
r(x)δ. Show by example that this is a bad discretization of the original problem
in the sense that the shortest distance need not approach the optimal cost of
the original problem as δ → 0. Note: This exercise illustrates a common pitfall.
The difficulty is that the control constraint set (the surface of the unit sphere)
should be finely discretized as well. For a proper treatment of the problem
of discretization, see the original papers by Gonzalez and Rofman [1985], and
Falcone [1987], the survey paper by Kushner [1990], the monograph by Kushner
and Dupuis [1992], and the references cited there. For analogs of the label setting
and label correcting algorithms of the present chapter, see the papers by Tsitsiklis
[1995], and by Polymenakos, Bertsekas, and Tsitsiklis [1998].
3
The Max-Flow Problem
Contents
3.1. The Max-Flow and Min-Cut Problems
3.1.1. Cuts in a Graph
3.1.2. The Max-Flow/Min-Cut Theorem
3.1.3. The Maximal and Minimal Saturated Cuts
3.1.4. Decomposition of Infeasible Network Problems
3.2. The Ford-Fulkerson Algorithm
3.3. Price-Based Augmenting Path Algorithms
3.3.1. A Price-Based Path Construction Algorithm
3.3.2. A Price-Based Max-Flow Algorithm
3.4. Notes, Sources, and Exercises
115
116 The Max-Flow Problem Chap. 3
In this chapter, we focus on the max-flow problem introduced in Example
1.3 of Section 1.2. We have a graph (N ,A) with flow bounds xij ∈ [bij , cij ]
for each arc (i, j), and two special nodes s and t. We want to maximize
the divergence out of s over all capacity-feasible flow vectors having zero
divergence for all nodes except s and t.
The max-flow problem arises in a variety of practical contexts and
also as a subproblem in the context of algorithms that solve other more
complex problems. For example, it can be shown that checking the exis-
tence of a feasible solution of a minimum cost flow problem, and finding a
feasible solution if one exists, is essentially equivalent to a max-flow prob-
lem (see Fig. 3.1, and Exercises 3.3 and 3.4). Furthermore, a number of
interesting combinatorial problems can be posed as max-flow problems (see
for example Exercises 3.8-3.10).
Like the shortest path problem, the max-flow problem embodies a
number of methodological ideas that are central to the more general min-
imum cost flow problem. In fact, whereas the shortest path problem can
be viewed as a minimum cost flow problem where arc capacities play no
role, the max-flow problem can be viewed as a minimum cost flow problem
where arc costs play no role. In this sense, the structures of the short-
est path and max-flow problems are complementary, and together provide
the foundation upon which much of the algorithmic methodology of the
minimum cost flow problem is built.
Central to the max-flow problem is the max-flow/min-cut theorem,
which is one of the most celebrated theorems of network optimization. In
Section 3.1, we derive this result, and we discuss some of its applications.
Later, in Chapter 4, we will interpret this result as a duality theorem
(see Exercise 4.4). In Section 3.2, we introduce a central algorithm for
solving the max-flow problem, the Ford-Fulkerson method. This is a fairly
simple method, which however can behave in interesting and surprising
ways. Much research has been devoted to developing clever and efficient
implementations of the Ford-Fulkerson method. We describe some of these
implementations in Sections 3.2 and 3.3, and in the exercises.
3.1 THE MAX-FLOW AND MIN-CUT PROBLEMS
The key idea in the max-flow problem is very simple: a feasible flow x can
be improved if we can find a path from s to t that is unblocked with respect
to x. Pushing a positive increment of flow along such a path results in larger
divergence out of s, while maintaining flow feasibility. Most (though not
all) of the available max-flow algorithms are based on iterative application
of this idea.
We may also ask the reverse question. If we can’t find an unblocked
path from s to t, is the current flow maximal? The answer is positive,
Sec. 3.1 The Max-Flow and Min-Cut Problems 117
Source Sinks t
Source Node
s1 > 0
Sink Node
Sink Node
1
1
2
3
4
5
2
3
4
5
s4 < 0
Source Node
s2 > 0 s5 < 0
s3 = 0
[0, s1 ]
[0, s2 ]
[0,- s4]
[0,- s5]
Figure 3.1: Essential equivalence of the problem of finding a feasible solution of a
minimum cost flow problem and a max-flow problem. Given a set of divergences
si satisfying
∑
i
si = 0, and capacity intervals [0, cij ], consider the feasibility
problem of finding a flow vector x satisfying∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N , (3.1)
0 ≤ xij ≤ cij , ∀ (i, j) ∈ A. (3.2)
Denote by I+ = {i | si > 0} the set of source nodes ({1, 2} in the figure) and by
I− = {i | si < 0} the set of sink nodes ({4, 5} in the figure). If both these sets
are empty, the zero vector is a feasible flow, and we are done. Otherwise, these
sets are both nonempty (since
∑
i
si = 0). We introduce a node s, and for all
i ∈ I+, the arcs (s, i) with flow range [0, si]. We also introduce a node t, and
for all i ∈ I−, the arcs (i, t) with flow range [0,−si]. Now consider the max-flow
problem of maximizing the divergence out of s and into t, while observing the
capacity constraints. Then there exists a solution to the feasibility problem of
Eqs. (3.1) and (3.2), if and only if the maximum divergence out of s is equal to∑
i∈I+ si. If this condition is satisfied, solutions of the feasibility problem are in
one-to-one correspondence with optimal solutions of the max-flow problem.
If the capacity constraints involve lower bounds, bij ≤ xij ≤ cij , we may
convert first the feasibility problem to one with zero lower flow bounds by a
translation of variables, which replaces each variable xij with a variable zij =
xij − bij .
Also, a max-flow problem can (in principle) be solved by an algorithm
that solves the feasibility problem (we try to find a sequence of feasible flows
with monotonically increasing divergence out of s, stopping with a maximum flow
when no further improvement is possible). In fact, this is the main idea of the
Ford-Fulkerson method, to be discussed in Section 3.2.
118 The Max-Flow Problem Chap. 3
although the reason is not entirely obvious. For a brief justification, con-
sider the minimum cost flow formulation of the max-flow problem, given in
Example 1.3, which involves the artificial feedback arc (t, s) (see Fig. 3.2).
Then, a cycle has negative cost if and only if it includes the arc (t, s), since
this arc has cost -1 and is the only arc with nonzero cost. By Prop. 1.2, if
a feasible flow vector x is not optimal, there must exist a simple cycle with
negative cost that is unblocked with respect to x; this cycle must consist
of the arc (t, s) and a path from s to t, which is unblocked with respect to
x. Thus, if there is no path from s to t that is unblocked with respect to a
given flow vector x, then there is no cycle of negative cost and x must be
optimal.
Source Sinks t
Cost coefficient  = -1
Artificial feedback arc
All cost coefficients are
zero except for ats
Figure 3.2: Minimum cost flow formulation of a max-flow problem, involving
a feedback (t, s) arc with cost -1 and unconstrained arc flow (−∞ < xts < ∞).
For a nonoptimal flow x, there must exist a cycle that is unblocked with respect
to x and has negative cost. Since all arcs other than the feedback arc have zero
length, this cycle must contain the feedback arc. This implies that there must
exist a path from s to t, which is unblocked with respect to x. Many max-flow
algorithms push flow along such a path to iteratively improve an existing flow
vector x.
The max-flow/min-cut theorem and the Ford-Fulkerson algorithm,
to be described shortly, are based on the preceding ideas. However, rather
than appealing to Prop. 1.2 (whose proof relies on the notion of a conformal
decomposition), we couch the analysis of this chapter on first principles,
taking advantage of the simplicity of the max-flow problem. This will also
serve to develop some concepts that will be useful later. We first introduce
some definitions.
Sec. 3.1 The Max-Flow and Min-Cut Problems 119
3.1.1 Cuts in a Graph
A cut Q in a graph (N ,A) is a partition of the node set N into two
nonempty subsets, a set S and its complement N −S. We use the notation
Q = [S,N − S].
Note that the partition is ordered in the sense that the cut [S,N − S] is
distinct from the cut [N − S,S]. For a cut Q = [S,N − S], we use the
notation
Q+ =
{
(i, j) ∈ A | i ∈ S, j /∈ S
}
,
Q− =
{
(i, j) ∈ A | i /∈ S, j ∈ S
}
,
and we say that Q+ and Q− are the sets of forward and backward arcs of
the cut , respectively. We say that the cut Q is nonempty if Q+ ∪Q− 	= Ø;
otherwise we say that Q is empty . We say that the cut [S,N −S] separates
node s from node t if s ∈ S and t /∈ S. These definitions are illustrated in
Fig. 3.3.
1
4
3
2
6
5
Figure 3.3: Illustration of a cut
Q = [S,N − S],
where S = {1, 2, 3}. We have
Q+ = {(2, 4), (1, 6)},
Q− = {(4, 1), (6, 3), (5, 3)}.
Given a flow vector x, the flux across a nonempty cut Q = [S,N −S]
is defined to be the total net flow coming out of S, i.e., the scalar
F (Q) =
∑
(i,j)∈Q+
xij −
∑
(i,j)∈Q−
xij .
Let us recall from Section 1.1.2 the definition of the divergence of a node i:
yi =
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji, ∀ i ∈ N .
120 The Max-Flow Problem Chap. 3
The following calculation shows that F (Q) is also equal to the sum of the
divergences yi of the nodes in S:
F (Q) =
∑
{(i,j)∈A|i∈S,j /∈S}
xij −
∑
{(i,j)∈A|i/∈S,j∈S}
xij
=
∑
i∈S
 ∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji

=
∑
i∈S
yi.
(3.3)
(The second equality holds because the flow of an arc with both end nodes
in S cancels out within the parentheses; it appears twice, once with a
positive and once with a negative sign.)
Given lower and upper flow bounds bij and cij for each arc (i, j), the
capacity of a nonempty cut Q is
C(Q) =
∑
(i,j)∈Q+
cij −
∑
(i,j)∈Q−
bij . (3.4)
Clearly, for any capacity-feasible flow vector x, the flux F (Q) across Q is
no larger than the cut capacity C(Q). If F (Q) = C(Q), then Q is said to
be a saturated cut with respect to x; the flow of each forward (backward)
arc of such a cut must be at its upper (lower) bound. By convention, every
empty cut is also said to be saturated. The following is a simple but useful
result.
Proposition 3.1: Let x be a capacity-feasible flow vector, and let s
and t be two nodes. Then exactly one of the following two alternatives
holds:
(1) There exists a simple path from s to t that is unblocked with
respect to x.
(2) There exists a saturated cut that separates s from t.
Proof: The proof is obtained by constructing an algorithm that terminates
with either a path as in (1) or a cut as in (2). This algorithm is a special
case of a general method, known as breadth-first search, and used to find a
simple path between two nodes in a graph (see Exercise 3.2). The algorithm
generates a sequence of node sets {Tk}, starting with T0 = {s}; each set Tk
represents the set of nodes that can be reached from s with an unblocked
path of k arcs.
Sec. 3.1 The Max-Flow and Min-Cut Problems 121
Unblocked Path Search Algorithm
For k = 0, 1, . . ., given Tk, terminate if either Tk is empty or t ∈ Tk;
otherwise, set
Tk+1 =
{
n /∈ ∪ki=0Ti| there is a node m ∈ Tk, and either an arc (m, n)
with xmn < cmn, or an arc (n, m) with bnm < xnm
}
and mark each node n ∈ Tk+1 with the label “(m, n)” or “(n, m),”
where m is a node of Tk and (m, n) or (n, m) is an arc with the property
stated in the above equation, respectively.
Figure 3.4 illustrates the preceding algorithm. Since the algorithm
terminates if Tk is empty, and Tk must consist of nodes not previously
included in ∪k−1i=0 Ti, the algorithm must eventually terminate. Let S be the
union of the sets Ti upon termination. There are two possibilities:
(a) The final set Tk contains t, in which case, by tracing labels backward
from t, a simple unblocked path P from s to t can be constructed.
The forward arcs of P are of the form (m, n) with xmn < cmn and
the label of n being “(m, n)”; the backward arcs of P are of the
form (n, m) with bnm < xnm and the label of n being “(n, m).” Any
cut separating s from t must contain a forward arc (m, n) of P with
xmn < cmn or a backward arc (n, m) of P with bnm < xnm, and
therefore cannot be saturated. Thus, the result is proved in this case.
(b) The final set Tk is empty, in which case from the equation defining Tk,
it can be seen that the cut Q = [S,N −S] is saturated and separates
s from t. To show that there is no simple unblocked path from s
to t, note that any such path must have either an arc (m, n) ∈ Q+
with xmn < cmn or an arc (n, m) ∈ Q− with bnm < xnm, which is
impossible, since Q is saturated.
Q.E.D.
Exercise 3.11 provides some variations of Prop. 3.1. In particular, in
place of s and t, one may use two disjoint subsets of nodes N+ and N−.
Furthermore, “simple path” in alternative (1) may be replaced by “path.”
3.1.2 The Max-Flow/Min-Cut Theorem
Consider now the max-flow problem, where we want to maximize the diver-
gence out of s over all capacity-feasible flow vectors having zero divergence
for all nodes other than s and t. Given any such flow vector and any cut
Q separating s from t, the divergence out of s is equal to the flux across Q
[cf. Eq. (3.3)], which in turn is no larger than the capacity of Q. Thus, if
122 The Max-Flow Problem Chap. 3
(-1,0,1)
(0,1,1)
(0,0,1)
(0,0,1)
(0,0,1)
(1,2,2)
(a)
(b)
(0,0,1)
(1,1,2)
(-1,0,1)
(0,0,1)
(0,0,1)
(0,0,1)
(1,1,2)
(1,2,2)
(0,1,1)
(0,1,1)
(1,1,2)
T1
T0
T2
T3
T0
T1
T2
(lower bound, flow, upper bound)
shown next to each arc
1
4
3
2
5
6
1
4
3
2
5
6
(1,1,2) Figure 3.4: Illustration of the un-
blocked path search algorithm for
finding an unblocked path from node
1 to node 6, or a saturated cut sep-
arating 1 from 6. The triplet (lower
bound, flow, upper bound) is shown
next to each arc. The figure shows
the successive sets Tk generated by
the algorithm. In case (a) there ex-
ists a unblocked path from 1 to 6,
namely the path (1, 3, 5, 6). In case
(b), where the flow of arc (6, 5) is
at the lower bound rather than the
upper bound, there is the saturated
cut [S,N − S] separating 1 from 6,
where S = {1, 2, 3, 4, 5} is the union
of the sets Tk. Note that the algo-
rithm works for any arc flows, and,
in particular, does not require that
the nodes other than the start node
1 and the end node 6 have zero di-
vergence.
the max-flow problem is feasible, we have
Maximum Flow ≤ Capacity of Q. (3.5)
The following max-flow/min-cut theorem asserts that equality is attained
for some Q. Part (a) of the theorem assumes the existence of an optimal
solution to the max-flow problem. This assumption need not be satisfied;
indeed it is possible that the max-flow problem has no feasible solution at
all (consider a graph consisting of a single two-arc path from s to t, the
arcs of which have disjoint feasible flow ranges). In Chapter 5, however,
we will show using the theory of the simplex method (see Prop. 5.7), that
the max-flow problem (and indeed every minimum cost flow problem) has
an optimal solution if it has at least one feasible solution. [Alternatively,
this can be shown using a fundamental result of mathematical analysis,
the Weierstrass theorem, which states that a continuous function attains
a maximum over a nonempty and compact set (see Appendix A and the
sources given there).] If the lower flow bound is zero for every arc, the max-
flow problem has at least one feasible solution, namely the zero flow vector.
Sec. 3.1 The Max-Flow and Min-Cut Problems 123
Thus the theory of Chapter 5 (or the Weierstrass theorem) guarantees that
the max-flow problem has an optimal solution in this case. This is stated
as part (b) of the following theorem, even though its complete proof must
await the developments of Chapter 5.
Proposition 3.2: (Max-Flow/Min-Cut Theorem)
(a) If x∗ is an optimal solution of the max-flow problem, then the
divergence out of s corresponding to x∗ is equal to the minimum
cut capacity over all cuts separating s from t.
(b) If all lower arc flow bounds are zero, the max-flow problem has
an optimal solution, and the maximal divergence out of s is equal
to the minimum cut capacity over all cuts separating s from t.
Proof: (a) Let F ∗ be the value of the maximum flow, that is, the diver-
gence out of s corresponding to x∗. There cannot exist an unblocked path
P from s to t with respect to x∗, since by increasing the flow of the forward
arcs of P and by decreasing the flow of the backward arcs of P by a com-
mon positive increment, we would obtain a flow vector with a divergence
out of s larger than F ∗. Therefore, by Prop. 3.1, there must exist a cut
Q, that is saturated with respect to x∗ and separates s from t. The flux
across Q is equal to F ∗ and is also equal to the capacity of Q [since Q is
saturated; see Eqs. (3.3) and (3.4)]. Since we know that F ∗ is less or equal
to the minimum cut capacity [cf. Eq. (3.5)], the result follows.
(b) See the discussion preceding the proposition. Q.E.D.
3.1.3 The Maximal and Minimal Saturated Cuts
Given an optimal solution x∗ of the max-flow problem, there may exist
several saturated cuts [S,N −S] separating s and t. We will show that out
of these cuts, there exists one, called maximal , corresponding to the union
of the sets S. Similarly, there is a minimal saturated cut, corresponding to
the intersection of the sets S. (The maximal and minimal cuts coincide if
and only if there is a unique saturated cut.)
Indeed, let S be the union of all node sets S such that [S,N − S] is
a saturated cut separating s and t. Consider the cut
Q = [S,N − S].
Clearly Q separates s and t. If (i, j) ∈ Q+, then we have x∗ij = cij because
i belongs to one of the sets S such that [S,N −S] is a saturated cut, and j
does not belong to S since j /∈ S. Thus we have x∗ij = cij for all (i, j) ∈ Q
+
.
124 The Max-Flow Problem Chap. 3
Similarly, we obtain x∗ij = bij for all (i, j) ∈ Q
−
. Thus Q is a saturated cut
separating s and t, and in view of its definition, it is the maximal such cut.
By using set intersection in place of set union in the preceding argument, it
is seen that we can similarly form the minimal saturated cut that separates
s and t.
The maximal and minimal saturated cuts can be used to deal with
infeasibility in the context of various network flow problems, as we discuss
next.
3.1.4 Decomposition of Infeasible Network Problems
Consider the minimization of a separable cost function of the flow vector
x, ∑
(i,j)∈A
fij(xij),
subject to conservation of flow constraints∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
and capacity constraints
0 ≤ xij ≤ cij , ∀ (i, j) ∈ A.
We assume that the scalars si are given and satisfy
∑
i∈N si = 0, but that
the problem is infeasible, because the capacities cij are not sufficiently large
to carry all the supply from the set of supply nodes
I+ = {i | si > 0}
to the set of demand nodes
I− = {i | si < 0}.
Then it may make sense to minimize the cost function over the set of all
maximally feasible flows, which is the set of flow vectors x whose diver-
gences
yi =
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji
satisfy
yi ≥ 0 if i ∈ I+,
yi ≤ 0 if i ∈ I−,
yi = 0 if i /∈ I+ ∪ I−,
Sec. 3.2 The Ford-Fulkerson Algorithm 125
and minimize ∑
i∈N
|si − yi|.
Thus, roughly, a flow vector is maximally feasible if it is capacity-feasible,
and it satisfies as much of the given demand as possible by using as much
of the given supply as possible.
Note that we can find a maximally feasible flow x∗ by solving the
max-flow problem given in Fig. 3.1. The vector x∗ defines corresponding
minimal and maximal saturated cuts
[Smin,N − Smin], [Smax,N − Smax],
respectively, separating the supply node set P from the demand node set D.
Furthermore, the flows of all arcs (i, j) that belong to these cuts are equal to
x∗ij for every maximally feasible flow vector. It can now be seen that given
x∗, we can decompose the problem of minimizing the cost function over the
set of maximally feasible flows into two or three feasible and independent
subproblems, depending on whether Smin = Smax or not. The node sets of
these problems are Smin, N − Smax, and Smax − Smin, (if Smax 	= Smin).
The supplies for these problems are appropriately adjusted to take into
account the arc flows x∗ij for the arcs (i, j) of the corresponding cuts, as
illustrated in Fig. 3.5.
3.2 THE FORD-FULKERSON ALGORITHM
In this section, we focus on a fundamental algorithm for solving the max-
flow problem. This algorithm is of the primal cost improvement type,
because it improves the primal cost (the divergence out of s) at every
iteration. The idea is that, given a feasible flow vector x (i.e., one that is
capacity-feasible and has zero divergence out of every node other than s
and t), and a path P from s to t, which is unblocked with respect to x, we
can increase the flow of all forward arcs (i, j) of P and decrease the flow of
all backward arcs (i, j) of P . The maximum increment of flow change is
δ = min
{
{cij − xij | (i, j) ∈ P+}, {xij − bij | (i, j) ∈ P−}
}
,
where P+ is the set of forward arcs of P and P− is the set of backward
arcs of P . The resulting flow vector x, given by
xij =
 xij + δ if (i, j) ∈ P
+,
xij − δ if (i, j) ∈ P−,
xij otherwise,
126 The Max-Flow Problem Chap. 3
ts
1
2
4
5
3
6
1/1
1/1
1/1
1/1
1/1
1/1
0/10/1
2/3
2/3
2/3
2/3
2/3
Max-Flow/Capacity
shown next to each arc.
All lower flow bounds are 0.
Minimal
Saturated Cut
Maximal
Saturated Cut
6 6
Figure 3.5: Decomposition of the problem of minimizing a separable cost func-
tion
∑
(i,j)∈A fij(xij) over the set of maximally feasible flow vectors into three
(feasible) optimization problems. The problem here is to send 6 units of flow from
node s to node t, while satisfying capacity constraints [0, cij ] and minimizing a
cost function
∑
(i,j)∈A fij(xij). In this example, all arcs have capacity 1, ex-
cept for arc (3, 4) and the incident arcs to nodes s and t, which have capacity 3.
The problem is infeasible, so we consider optimization over all maximally feasible
solutions. We solve the max-flow problem from s to t, and we obtain the corre-
sponding minimal and maximal saturated cuts, as shown in the figure. Note that
the flows of the arcs across these cuts are unique, although the max-flow vector
is not unique.
We can now decompose the original (infeasible) optimization problem into
three (feasible) optimization problems, each with the cost function
∑
(i,j)
fij(xij),
where the summation is over the relevant set of arcs. These problems are:
(1) The problem involving the nodes s, 1, and 2, with conservation of flow
constraints
xs1 + xs2 = 4, −x21 − xs1 = −2, x21 − xs2 = −2.
(2) The problem involving the nodes 3 and 4, with conservation of flow con-
straint (for both nodes) x34 = 2.
(3) The problem involving the nodes 5, 6, and t, with conservation of flow
constraints
x5t + x56 = 2, x6t − x56 = 2, −x5t − x6t = −4.
Note that while in this example the 2nd problem is trivial (has only one feasible
solution), the 1st and 3rd problems have multiple feasible solutions.
Sec. 3.2 The Ford-Fulkerson Algorithm 127
is feasible, and it has a divergence out of s that is larger by δ than the
divergence out of s corresponding to x. We refer to P as an augmenting
path, and we refer to the operation of replacing x by x as a flow augmenta-
tion along P . Such an operation may also be viewed as a modification of x
along the negative cost cycle consisting of P and an artificial arc (t, s) that
has cost −1; see the formulation of the max-flow problem as a minimum
cost flow problem in Fig. 3.2, and the discussion at the beginning of Section
3.1.
The Ford-Fulkerson algorithm starts with a feasible flow vector. If
the lower flow bound is zero for all arcs, the zero flow vector can be used
as a starting vector; otherwise, a preliminary phase is needed to obtain a
feasible starting flow vector. This involves solving an auxiliary max-flow
problem with zero lower flow bounds starting from the zero flow vector
and using the Ford-Fulkerson algorithm described below (cf. Fig. 3.1 and
Exercise 3.4). At each iteration the algorithm has a feasible flow vector
and uses the unblocked path search method, given in the proof of Prop.
3.1, to either generate a new feasible flow vector with larger divergence out
of s or terminate with a maximum flow and a minimum capacity cut.
Iteration of Ford-Fulkerson Algorithm
Use the unblocked path search method to either
(1) find a saturated cut separating s from t or
(2) find an unblocked path P with respect to x starting from s and
ending at t.
In case (1), terminate the algorithm; the current flow vector solves the
max-flow problem. In case (2), perform an augmentation along P and
go to the next iteration.
Figure 3.6 illustrates the Ford-Fulkerson algorithm. Based on the pre-
ceding discussion, we see that with each augmentation, the Ford-Fulkerson
algorithm improves the primal cost (the divergence out of s) by the augmen-
tation increment δ. Thus, if δ is bounded below by some positive number,
the algorithm can execute only a finite number of iterations and must ter-
minate with an optimal solution. In particular, if the arc flow bounds are
integer and the initial flow vector is also integer, δ is a positive integer at
each iteration, and the algorithm terminates. The same is true even if the
arc flow bounds and the initial flow vector are rational; by multiplication
with a suitably large integer, one can scale these numbers up to integer
while leaving the problem essentially unaffected.
On the other hand, if the problem data are irrational, proving termi-
nation of the Ford-Fulkerson algorithm is nontrivial. The proof (outlined
in Exercise 3.12) depends on the use of the specific unblocked path search
128 The Max-Flow Problem Chap. 3
1 4
3
2
5[0,2] [0,1]
[0,2] [0,3]
[0,5]
[0,4] [0,1]
[0,1]
1
0
1
0 0
0
0
0
1 4
3
2
5
2
1
1
0
1
0 0
0
1 4
3
2
5
2 2
3
2
1
1
0 01 4
3
2
5
3
3
4
1
2
1
1
01 4
3
2
5
1
1
1
1
1 4
3
2
5
2
2 2
1 4
3
5
1
1 1
1 4
2
5
1 1
1
2
5
Figure 3.6: Illustration of the Ford-Fulkerson algorithm for finding a maximum
flow from node s = 1 to node t = 5. The arc flow bounds are shown next to the arcs
in the top left figure, and the starting flow is zero. The sequence of successive flow
vectors is shown on the left, and the corresponding sequence of augmentations is
shown on the right. The saturated cut obtained is [{1, 2, 3}, {4, 5}]. The capacity
of this cut as well as the maximum flow is 5.
method of Prop. 3.1; this method (also referred to as breadth-first search,
see Exercise 3.2) yields augmenting paths with as few arcs as possible (see
Exercises 3.2 and 3.12). If unblocked paths are constructed using a dif-
ferent method, then, surprisingly, the Ford-Fulkerson algorithm need not
terminate, and the generated sequence of divergences out of s may con-
verge to a value strictly smaller than the maximum flow (for an example,
see Exercise 3.7, and for a different example, see Ford and Fulkerson [1962],
or Papadimitriou and Steiglitz [1982], p. 126, or Rockafellar [1984], p. 92).
Even with integer problem data, if the augmenting paths are constructed
Sec. 3.2 The Ford-Fulkerson Algorithm 129
using a different unblocked path search method, the Ford-Fulkerson algo-
rithm may require a very large (pseudopolynomial) number of iterations to
terminate; see Fig. 3.7.
Augmenting Path for Odd 
Numbered Iterations
Augmenting Path for Even
Numbered Iterations
1 4
3
2
[0,1]
[0,C]
[0,C]
[0,C]
[0,C]
1
1
1
1 4
3
2
1
-1 11 4
3
2
Figure 3.7: An example showing
that if the augmenting paths used
in the Ford-Fulkerson algorithm do
not have a number of arcs that
is as small as possible, the num-
ber of iterations may be very large.
Here, C is a large integer. The
maximum flow is 2C, and can be
produced after a sequence of 2C
augmentations using the three-arc
augmenting paths shown in the fig-
ure. Thus, the running time is
pseudopolynomial (it is proportio-
nal to C).
If on the other hand the two-
arc augmenting paths (1, 2, 4) and
(1, 3, 4) are used, only two augmen-
tations are needed.
Polynomial Max-Flow Algorithms
Using “shortest” augmenting paths (paths with as few arcs as possible) not
only guarantees termination of the Ford-Fulkerson algorithm. It turns out
that it also results in polynomial running time, as the example of Fig. 3.7
illustrates. In particular, the number of augmentations of the algorithm
with shortest augmenting paths can be estimated as O(NA); see Exercise
3.12. This yields an O(NA2) running time to solve the problem, since
each augmentation requires O(A) operations to execute the unblocked path
search method and to carry out the subsequent flow update.
Much research has been devoted to developing max-flow algorithms
with better than O(NA2) running time. The algorithms that we will discuss
can be grouped into two main categories:
(a) Variants of the Ford-Fulkerson algorithm, which use special data
structures and preprocessing calculations to generate augmenting paths
efficiently. We will describe some algorithms of this type in what fol-
lows in this chapter.
130 The Max-Flow Problem Chap. 3
(b) Algorithms that depart from the augmenting path approach, but in-
stead move flow from the source to the sink in a less structured fash-
ion than the Ford-Fulkerson algorithm. These algorithms, known as
preflow-push methods, will be discussed in Section 7.3. Their underly-
ing mechanism is related to the one of the auction algorithm described
in Section 1.3.3.
The algorithms that have the best running times at present are the preflow-
push methods. In particular, in Section 7.3 we will demonstrate an O(N3)
running time for one of these methods, and we will describe another method
with an O(N2A1/2) running time. Preflow-push algorithms with even bet-
ter running times exist (see the discussion in Chapter 7). It is unclear,
however, whether the best preflow-push methods outperform in practice
the best of the Ford-Fulkerson-like algorithms of this chapter.
In the remainder of this chapter, we will discuss efficient variants of
the Ford-Fulkerson algorithm. These variants are motivated by a clear
inefficiency of the unblocked path search algorithm: it discards all the la-
beling information collected from the construction of each augmenting path.
Since, in a large graph, an augmentation typically has a relatively small ef-
fect on the current flow vector, each augmenting path problem is similar to
the next augmenting path problem. One would thus think that the search
for an augmenting path could be organized to preserve information for use
in subsequent augmentations.
A prime example of an algorithm that cleverly preserves such infor-
mation is the historically important algorithm of Dinic [1970], illustrated
in Figure 3.8. Let us assume for simplicity that each lower arc flow bound
is zero. One possible implementation of the algorithm starts with the zero
flow vector and operates in phases. At the start of each phase, we have a
feasible flow vector x and we construct an acyclic network, called the lay-
ered network , which is partitioned in layers (subsets) of nodes as follows:
Construction of the Layered Network
Layer 0 consists of just the sink node t, and layer k consists of all nodes
i such that the shortest unblocked path from i to t has k arcs. Let
k(i) be the layer number of each node i [k(i) = ∞ if i does not belong
to any layer].
If the source node s does not belong to any layer, there must exist a
saturated cut separating s from t, so the current flow is maximal and
the algorithm terminates. Otherwise, we form the layered network as
follows: we delete all nodes i such that k(i) ≥ k(s) and their incident
arcs, and we delete all remaining arcs except the arcs (i, j) such that
k(i) = k(j) + 1 and xij < cij , or k(j) = k(i) + 1 and xij > 0.
Sec. 3.2 The Ford-Fulkerson Algorithm 131
Initial flows/capacities
0/2
0/2
0/1
0/1
0/1
0/1
0/2
1
4
3
2
6
5
0/1
0/2
Flows/capacities after 1st phase
0/2
1/2
1/1
1/1
1/1
0/1
0/2
1
4
3
2
6
5
1/1
1/2
Flows/capacities after 2nd phase
1/2
2/2
1/1
1/1
1/1
0/1
0/2
1
4
3
2
6
5
1/1
2/2
Layered network for 1st phase
1
4
3
2
6
5
Layered network for 2nd phase
1 42
6
5
Figure 3.8: Illustration of Dinic’s algorithm for the problem shown at the top
left (node 1 is the source and node 6 is the sink).
In the first phase, there are three layers, as shown in the top right figure.
There are three augmentations in the layered network (1 → 2 → 6, 1 → 3 → 6,
and 1 → 4 → 6), and the resulting flows are shown in the middle left figure. In
the second phase, there are four layers, as shown in the bottom right figure. There
is only one augmenting path in the layered network (1 → 2 → 4 → 6), and the
resulting flows are shown in the bottom left figure. The algorithm then terminates
because in constructing the layered network, no augmenting paths from 1 to 6
can be found.
Notice a key property of the algorithm: with each new phase, the layer
number of the source node is strictly increased (from 2 to 3 in this example).
This property shows that the number of phases is at most N − 1.
132 The Max-Flow Problem Chap. 3
Each phase consists of successively performing augmentations using
only arcs of the layered network constructed at the start of the phase, until
no more augmentations can be performed.
It can be seen that with proper implementation, the layered network
can be constructed in O(A) time. Furthermore, the number of augmen-
tations in each phase is at most A, since each augmentation makes at
least one arc unusable for transferring flow from s to t. Given that the
flow changes of each augmentation require O(N) time, it follows that each
phase requires O(NA) time. Finally, it can be shown that with each phase,
the layer number k(s) of the source node s increases strictly, so that there
can be at most N −1 phases (we leave this as Exercise 3.13 for the reader).
It thus follows that the running time of the algorithm is O(N2A).
We note that the Dinic algorithm motivated a number of other max-
flow algorithms with improved complexity, including an algorithm of Karza-
nov [1974], which has a O(N3) running time (see the sources cited at the
end of the chapter). The Karzanov algorithm in turn embodied some of
the ideas that were instrumental for the development of the preflow-push
algorithms for max-flow, which will be discussed in Section 7.3.
3.3 PRICE-BASED AUGMENTING PATH ALGORITHMS
In this section, we develop another type of Ford-Fulkerson algorithm, which
reuses information from one augmentation to the next, but does not con-
struct shortest augmenting paths. With proper implementation, this al-
gorithm can be shown to have an O(N2A) running time. However, there
is evidence that in practice it outperforms the Dinic and the Karzanov
algorithms, as well as the preflow-push algorithms of Section 7.3.
We mentioned earlier that constructing shortest augmenting paths
provides some guarantee of computational efficiency in the Ford-Fulkerson
algorithm. We can in fact view formally the problem of constructing such
an augmenting path as a shortest path problem in a certain graph, which
we will call the reduced graph. In particular, given a capacity-feasible flow
vector x, this graph has a node set that is the same as the one of the original
graph, and an arc set that is constructed from the one of the original graph
by reversing the direction of some of the arcs and by duplicating some arcs
and then reversing their direction. In particular, it contains:
(a) An arc (i, j) for each arc (i, j) of the original problem’s graph with
xij < cij .
(b) An arc (j, i) for each arc (i, j) of the original problem’s graph with
bij < xij .
Thus each incident arc of a node i (either outgoing or incoming) in the
original graph along which flow can be pushed from i towards the opposite
Sec. 3.3 Price-Based Augmenting Path Algorithms 133
node, corresponds to an outgoing arc from i in the reduced graph. Fur-
thermore, a path in the original graph is unblocked if it corresponds to
a forward path of the reduced graph. Figure 3.9 illustrates the reduced
graph.
1/1
1/2
0/2
0/1
1/1
0/1
0/1
(a)
0/1
0/2
1
4
3
2
6
(b)
REDUCED GRAPH
1
4
3
2
6
ORIGINAL GRAPH
Max-Flow/Capacity
shown next to each arc.
All lower flow bounds are 0.
5
5
0/1
Figure 3.9: Illustration of the reduced
graph corresponding to a given flow vec-
tor. Node 1 is the source, and node 6 is
the sink.
Figure (a) shows the original graph,
and the flow and upper flow bound next
to each arc (all lower flow bounds are
0). Figure (b) shows the reduced graph.
The arc (4,2) is added because the flow
of arc (2,4) is strictly between the arc
flow bounds. The arcs (1,2) and (4,6)
are reversed because their flows are at
the corresponding upper bounds.
Note that every forward path in
the reduced graph, such as (1, 4, 2, 6),
corresponds to an unblocked path in the
original graph.
It can now be seen that, given a capacity-feasible flow vector, the
problem of finding an augmenting path from s to t with a minimum number
of arcs is equivalent to the problem of finding a shortest path from s to t
in the corresponding reduced graph, with each arc having length 1. This
suggests the simple idea of embedding one of the shortest path algorithms of
Chapter 2 within the Ford-Fulkerson method. The shortest path algorithm
will be used to construct the sequence of augmenting paths from s to t.
Ideally, the algorithm should reuse some information from one shortest
path construction to the next; we mentioned earlier that this is a key to
computational efficiency.
Reusing information for a shortest path method amounts to provid-
134 The Max-Flow Problem Chap. 3
ing some form of advanced initialization, such as label information in the
context of label correcting methods or price information in the context
of auction algorithms. In particular, following a shortest path augmenta-
tion, and the attendant change of the reduced graph, one would like to
be able to reuse at least some of the final data of the preceding shortest
path construction, to provide an advanced start for the next shortest path
construction. Unfortunately, label correcting methods do not seem well
suited for this purpose, because it turns out that following a change of the
reduced graph due to an augmentation, many of the corresponding node
labels can become unusable.
On the other hand, the auction algorithm of Section 2.6 is much
better suited. The reason is that the node prices in the auction algorithm
are required to satisfy the CS condition
pi ≤ pj + 1 (3.6)
for all arcs (i, j) of the reduced graph. Furthermore, upon discovery of a
shortest augmenting path, there holds
pi = pj + 1
for all arcs (i, j) of the augmenting path. It can be seen that this equality
guarantees that following a flow augmentation, the CS condition (3.6) will
be satisfied for all newly created arcs of the reduced graph. As a result,
following an augmentation along a shortest path found by the auction al-
gorithm, the node prices can be reused without modification to start the
auction algorithm for finding the next shortest augmenting path.
The preceding observations can be used to formally define a max-
flow algorithm, where each augmenting path is found as a shortest path
from s to t in the reduced graph using the auction algorithm as a shortest
path subroutine. The initial node prices can be all equal to 0, and the
prevailing prices upon discovery of a shortest augmenting path are used as
the starting prices for searching for the next augmenting path. The auction
algorithm maintains a path starting at s, which is contracted or extended
at each iteration. The price of the terminal node of the path increases by at
least 1 whenever there is a contraction. An augmentation occurs whenever
the terminal node of the path is the sink node t. The overall algorithm is
terminated when the price of the terminal node exceeds N − 1, indicating
that there is no path starting at s and ending at t.
It is possible to show that, with proper implementation, the max-
flow algorithm just described has an O(N2A) running time. Unfortunately,
however, the practical performance of the algorithm is not very satisfactory,
because the computation required by the auction/shortest path algorithm
is usually much larger than what is needed to find an augmenting path. The
reason is that one needs just a path from s to t in the reduced graph and
Sec. 3.3 Price-Based Augmenting Path Algorithms 135
insisting on obtaining a shortest path may involve a substantial additional
computational cost . In what follows, we will give a price-based method that
constructs a (not necessarily shortest) path from s to t. This method is
similar to the auction/shortest path algorithm, but when embedded within
a sequential augmenting path construction scheme, it results in a max-flow
algorithm that is much faster in practice.
3.3.1 A Price-Based Path Construction Algorithm
We will describe a special method for finding a simple forward path in a
directed graph (N ,A) that starts at a given node s and ends at a given
node t. This method will be subsequently embedded within a max-flow
context to construct augmenting paths. The algorithm maintains (except
upon termination) a simple forward path P = (s, n1, . . . , nk) and a set of
integer node prices pi, i ∈ N , satisfying
pi ≤ pj + 1, ∀ (i, j) ∈ A, (3.7)
ps < N, pt = 0, (3.8)
pi ≥ pj , ∀ (i, j) ∈ P. (3.9)
[Note the difference with the auction/shortest path algorithm of Section
2.6, where we require that pi = pj + 1 for all arcs (i, j) of the path P ,
rather than pi ≥ pj .]
At the start of the algorithm, we require that P = (s), and that p
is such that Eqs. (3.7) and (3.8) hold. The path P is modified repeatedly
using the following two operations:
(a) A contraction of P , which deletes the last arc of P , that is, replaces the
path P = (s, n1, . . . , nk) by the path P = (s, n1, . . . , nk−1). [In the
degenerate case where P = (s), a contraction leaves P unchanged.]
(b) An extension of P , which adds to P an arc outgoing from its end
node, that is, replaces the path P = (s, n1, . . . , nk) by a path P =
(s, n1, . . . , nk, nk+1), where (nk, nk+1) is an arc.
The prices pi may also be increased in the course of the algorithm so that,
together with P , they satisfy the conditions (3.7)-(3.9). A contraction
always involves a price increase of the end node nk. An extension may or
may not involve such a price increase. An extension of P is always done to
a neighbor node of nk that has minimal price. The algorithm terminates if
either node t becomes the end node of P (then P is the desired path), or
else ps ≥ N [in view of pt = 0 and pi ≤ pj +1 for all arcs (i, j), as per Eqs.
(3.7) and (3.8), this means that there is no forward path from s to t].
136 The Max-Flow Problem Chap. 3
Path Construction Algorithm
Set P = (s), and select p such that Eqs. (3.7) and (3.8) hold.
Step 1 (Check for contraction or extension): Let nk be the
end node of the current path P and if nk 	= s, let pred(nk) be the
predecessor node of nk on P . If the set of downstream neighbors of
nk,
N(nk) = {j | (nk, j) ∈ A},
is empty, set pnk = N and go to Step 3. Otherwise, find a node in
N(nk) with minimal price and denote it succ(nk),
succ(nk) = arg min
j∈N(nk)
pj . (3.10)
Set
pnk = psucc(nk) + 1. (3.11)
If nk = s, or if
nk 	= s and ppred(nk) > psucc(nk),
go to Step 2; otherwise go to Step 3.
Step 2 (Extend path): Extend P by node succ(nk) and the corre-
sponding arc
(
nk, succ(nk)
)
. If succ(nk) = t, terminate the algorithm;
otherwise go to Step 1.
Step 3 (Contract path): If P = (s) and ps ≥ N , terminate the
algorithm; otherwise, contract P and go to Step 1.
Figure 3.10 illustrates the preceding path construction algorithm. In
the special case where all initial prices are zero and there is a path from each
node to t, by tracing the steps, it can be seen that the algorithm will work
like depth-first search, raising to 1 the prices of the nodes of some path from
s to t in a sequence of extensions with no intervening contractions. More
generally, the algorithm terminates without performing any contractions if
the initial prices satisfy pi ≥ pj for all arcs (i, j) and there is a path from
each node to t.
Note that the algorithm does not necessarily generate a shortest path.
Instead, it can be shown that it solves a special type of assignment problem
by means of the auction algorithm of Section 1.3.3 (which will be further
developed in Chapter 7); see Exercise 3.17.
We make the following observations:
(1) The prices remain integer throughout the algorithm [cf. Eq. (3.11)].
Sec. 3.3 Price-Based Augmenting Path Algorithms 137
Path construction problem with
initial prices as shown
3
1
2
4
Origin Destination
Trajectory of end node of the
path P and final prices
generated by the algorithm
1
2
4
3
p1 = 0 p4 = 0
p3 = 1
p2 = 1
p1 = 1p4 = 0
p3 = 0
p2 = -1
Iteration # Path P prior Type of action Price vector p after
to iteration during iteration the iteration
1 (1) extension to 2 (0,−1, 0, 0)
2 (1, 2) contraction at 2 (0, 1, 0, 0)
3 (1) extension to 3 (1, 1, 0, 0)
4 (1, 3) extension to 4 (1, 1, 1, 0)
5 (1, 3, 4) stop
Figure 3.10: An example illustrating the path construction algorithm from s = 1
to t = 4, where the initial price vector is p = (0,−1, 0, 0).
(2) The conditions (3.7)-(3.9) are satisfied each time Step 1 is entered.
The proof is by induction. These conditions hold initially by assump-
tion. Condition (3.8) is maintained by the algorithm, since termi-
nation occurs as soon as ps ≥ N or t becomes the end node of P .
To verify conditions (3.7) and (3.9), we note that only the price of
nk can change in Step 1, and by Eqs. (3.10) and (3.11), this price
change maintains condition (3.7) for all arcs, and condition (3.9) for
all arcs of P , except possibly for the arc
(
pred(nk), nk
)
in the case of
an extension with the condition
ppred(nk) > psucc(nk)
holding. In the latter case, we must have
ppred(nk) ≥ psucc(nk) + 1
because the prices are integer, so by Eq. (3.11), we have
ppred(nk) ≥ pnk
138 The Max-Flow Problem Chap. 3
at the next entry to Step 1. This completes the induction.
(3) A contraction is always accompanied by a price increase. Indeed
by Eq. (3.9), which was just established, upon entering Step 1 with
nk 	= s, we have
pnk ≤ ppred(nk),
and to perform a contraction, we must have
ppred(nk) ≤ psucc(nk).
Hence
pnk ≤ psucc(nk),
implying by Eq. (3.11) that p(nk) must be increased to psucc(nk) + 1.
It can be seen, however, by example (see Fig. 3.10), that an extension
may or may not be accompanied by a price increase.
(4) Upon return to Step 1 following an extension, the end node nk satisfies
[cf. Eq. (3.11)]
ppred(nk) = pnk + 1.
This, together with the condition pi ≥ pj for all (i, j) ∈ P [cf. Eq.
(3.9)], implies that the path P will not be extended to a node that
already belongs to P , thereby closing a cycle. Thus P remains a
simple path throughout the algorithm.
The following proposition establishes the termination properties of
the algorithm.
Proposition 3.3: If there exists a forward path from s to t, the
path construction algorithm terminates via Step 2 with such a path.
Otherwise, the algorithm terminates via Step 3 when ps ≥ N .
Proof: We first note that the prices of the nodes of P are upper bounded
by N in view of Eqs. (3.8) and (3.9). Next we observe that there is a
price change of at least one unit with each contraction, and since the prices
of the nodes of P are upper bounded by N , there can be only a finite
number of contractions. Since P never contains a cycle, there can be at
most N − 1 successive extensions without a contraction, so the algorithm
must terminate. Throughout the algorithm, we have pt = 0 and pi ≤ pj +1
for all arcs (i, j). Hence, if a forward path from s to t exists, we must have
ps < N throughout the algorithm, including at termination, and since
termination via Step 3 requires that ps ≥ N , it follows that the algorithm
must terminate via Step 2 with a path from s to t. If a forward path from
s to t does not exist, termination can only occur via Step 3, in which case
we must have ps ≥ N . Q.E.D.
Sec. 3.4 Notes, Sources, and Exercises 139
3.3.2 A Price-Based Max-Flow Algorithm
Let us now return to the max-flow problem. We can construct an aug-
menting path algorithm of the Ford-Fulkerson type that uses the path con-
struction algorithm just presented. The algorithm consists of a sequence
of augmentations, each performed using the path construction algorithm
to obtain a path of the reduced graph that starts at the source node s and
ends at the sink node t. As starting price vector we can use the zero vector.
An important point here is that, following an augmentation, the price
vector of the path construction algorithm can remain unchanged . The rea-
son is that the node prices in the path construction algorithm are required
to satisfy the condition
pi ≤ pj + 1 (3.12)
for all arcs (i, j) of the reduced graph. Furthermore, upon discovery of an
augmenting path P , there holds
pi ≥ pj
for all arcs (i, j) of P . It follows that as the reduced graph changes due to
the corresponding augmentation, for every newly created arc (j, i) of the
reduced graph, the arc (i, j) must belong to P , so that pi ≥ pj . Hence the
newly created arc (j, i) of the reduced graph will also satisfy the required
condition pj ≤ pi + 1 [cf. Eq. (3.12)].
For a practically efficient implementation of the max-flow algorithm
just described, a number of fairly complex modifications may be needed. A
description of these and a favorable computational comparison with other
competing methods can be found in Bertsekas [1995c], where an O(N2A)
complexity bound is also shown for a suitable variant of the method.
3.4 NOTES, SOURCES, AND EXERCISES
The max-flow/min-cut theorem was independently given in Dantzig and
Fulkerson [1956], Elias, Feinstein, and Shannon [1956], and Ford and Fulk-
erson [1956b]. The material of Section 3.1.4 on decomposition of infeasible
problems is apparently new.
The proof that the Ford-Fulkerson algorithm with breadth-first search
has polynomial complexity O(NA2) (Exercise 3.12) is due to Edmonds and
Karp [1972]. Using the idea of a layered network, this bound was improved
to O(N2A) by Dinic [1970], whose work motivated a lot of research on max-
flow algorithms with improved complexity. In particular, Dinic’s complex-
ity bound was improved to O(N3) by Karzanov [1974] and by Malhotra,
Kumar, and Maheshwari [1978], to O(N2A1/2) by Cherkasky [1977], to
O(N5/3A2/3) by Galil [1980], and to O
(
NA log2 N
)
by Galil and Naamad
140 The Max-Flow Problem Chap. 3
[1980]. Dinic’s algorithm when applied to the maximal matching problem
(Exercise 3.9) can be shown to have running time O(N1/2A) (see Hopcroft
and Karp [1973]). The survey paper by Ahuja, Magnanti, and Orlin [1989]
provides a complexity-oriented account of max-flow algorithms.
The max-flow algorithm of Section 3.3 is due to Bertsekas [1995c].
This reference contains several variants of the basic method, a discussion of
implementation issues, and extensive computational results that indicate
a superior practical performance over competing methods, including the
preflow-push algorithms of Chapter 7.
There are two important results in network optimization that deal
with the existence of feasible solutions for minimum cost flow problems.
The first is the feasible distribution theorem, due to Gale [1957] and Hoff-
man [1960], which is a consequence of the max-flow/min-cut theorem (Ex-
ercise 3.3). The second is the feasible differential theorem, due to Minty
[1960], which deals with the existence of a set of prices satisfying certain
constraints. This theorem is a consequence of the duality theory to be
fully developed in Chapter 5, and will be given in Exercise 5.11 (see also
Exercise 5.12).
E X E R C I S E S
3.1
Consider the max-flow problem of Fig. 3.11, where s = 1 and t = 5.
(a) Enumerate all cuts of the form [S,N − S] such that 1 ∈ S and 5 /∈ S.
Calculate the capacity of each cut.
(b) Find the maximal and minimal saturated cuts.
(c) Apply the Ford-Fulkerson method to find the maximum flow and verify the
max-flow/min-cut equality.
[0,5]
[0,3][0,2]
[0,1]
[0,4]
[0,1]
1 4
3
2
5
[0,1]
[0,5]
Figure 3.11: Max-flow problem
for Exercise 3.1. The arc capac-
ities are shown next to the arcs.
Sec. 3.4 Notes, Sources, and Exercises 141
3.2 (Breadth-First Search)
Let i and j be two nodes of a directed graph (N ,A).
(a) Consider the following algorithm, known as breadth-first search, for finding
a path from i to j. Let T0 = {i}. For k = 0, 1, . . ., let
Tk+1 = {n /∈ ∪kp=0Tp | for some node m ∈ Tk, (m, n) or (n, m) is an arc},
and mark each node n ∈ Tk+1 with the label “(m, n)” or “(n, m),” where
m is a node of Tk such that (m, n) or (n, m) is an arc, respectively. The
algorithm terminates if either (1) Tk+1 is empty or (2) j ∈ Tk+1. Show
that case (1) occurs if and only if there is no path from i to j. If case (2)
occurs, how would you use the labels to construct a path from i to j?
(b) Show that a path found by breadth-first search has a minimum number of
arcs over all paths from i to j.
(c) Modify the algorithm of part (a) so that it finds a forward path from i to
j.
3.3 (Feasible Distribution Theorem)
Show that the minimum cost flow problem introduced in Section 1.2.1, has a
feasible solution if and only if
∑
i∈N si = 0 and for every cut Q = [S,N −S] we
have
Capacity of Q ≥
∑
i∈S
si.
Show also that feasibility of the problem can be determined by solving a max-
flow problem with zero lower flow bounds. Hint : Assume first that all lower flow
bounds bij are zero. Use the conversion to a max-flow problem of Fig. 3.1, and
apply the max-flow/min-cut theorem. In the general case, transform the problem
to one with zero lower flow bounds.
3.4 (Finding a Feasible Flow Vector)
Describe an algorithm of the Ford-Fulkerson type for checking the feasibility and
finding a feasible solution of a minimum cost flow problem (cf., Section 1.2.1). If
the supplies si and the arc flow bounds bij and cij are integer, your algorithm
should be guaranteed to find an integer feasible solution (assuming at least one
feasible solution exists). Hint : Use the conversion to a max-flow problem of Fig.
3.1.
3.5 (Integer Approximations of Feasible Solutions)
Given a graph (N ,A) and a flow vector x with integer divergence, show that
there exists an integer flow vector x having the same divergence vector as x and
satisfying
|xij − xij | < 1, ∀ (i, j) ∈ A.
142 The Max-Flow Problem Chap. 3
Hint : For each arc (i, j), define the integer flow bounds
bij = 
xij, cij = xij	.
Use the result of Exercise 3.3.
3.6
Consider a graph with arc flow range [0, cij ] for each arc (i, j), and let x be a
capacity-feasible flow vector.
(a) Consider any subset S of nodes all of which have nonpositive divergence
and at least one of which has negative divergence. Show that there must
exist at least one arc (i, j) with i /∈ S and j ∈ S such that xij > 0.
(b) Show that for each node with negative divergence there is an augmenting
path that starts at that node and ends at a node with positive divergence.
Hint : Construct such a path using an algorithm that is based on part (a).
3.7 (Ford-Fulkerson Method Counterexample)
This counterexample (from Chvatal [1983]) illustrates how the version of the Ford-
Fulkerson method where augmenting paths need not have as few arcs as possible
may not terminate for a problem with irrational arc flow bounds. Consider the
max-flow problem shown in Fig. 3.12.
(a) Verify that an infinite sequence of augmenting paths is characterized by
the table of Fig. 3.12; each augmentation increases the divergence out of
the source s but the sequence of divergences converges to a value, which
can be arbitrarily smaller than the maximum flow.
(b) Solve the problem with the Ford-Fulkerson method (where the augmenting
paths involve a minimum number of arcs, as given in Section 3.2).
3.8 (Graph Connectivity – Menger’s Theorem)
Let s and t be two nodes in a directed graph. Use the max-flow/min-cut theorem
to show that:
(a) The maximum number of forward paths from s to t that do not share any
arcs is equal to the minimum number of arcs that when removed from the
graph, eliminate all forward paths from s to t.
(b) The maximum number of forward paths from s to t that do not share any
nodes (other than s and t) is equal to the minimum number of nodes that
when removed from the graph, eliminate all forward paths from s to t.
Sec. 3.4 Notes, Sources, and Exercises 143
s
t
1
2
3
4
5
6
After Iter. # Augm. Path x12 x36 x46 x65
6k + 1 (s, 1, 2, 3, 6, t) σ 1 − σ3k+2 σ − σ3k+1 0
6k + 2 (s, 2, 1, 3, 6, 5, t) σ − σ3k+2 1 σ − σ3k+1 σ3k+2
6k + 3 (s, 1, 2, 4, 6, t) σ 1 σ − σ3k+3 σ3k+2
6k + 4 (s, 2, 1, 4, 6, 3, t) σ − σ3k+3 1 − σ3k+3 σ σ3k+2
6k + 5 (s, 1, 2, 5, 6, t) σ 1 − σ3k+3 σ σ3k+4
6k + 6 (s, 2, 1, 5, 6, 4, t) σ − σ3k+4 1 − σ3k+3 σ − σ3k+4 0
6(k + 1) + 1 (s, 1, 2, 3, 6, t) σ 1 − σ3(k+1)+2 σ − σ3(k+1)+1 0
Figure 3.12: Max-flow problem illustrating that if the augmenting paths in the
Ford-Fulkerson method do not have a minimum number of arcs, then the method
may not terminate. All lower arc flow bounds are zero. The upper flow bounds
are larger than one, with the exception of the thick-line arcs; these are arc (3, 6)
which has upper flow bound equal to one, and arcs (1, 2) and (4, 6) which have
upper flow bound equal to σ =
(
− 1 +
√
5
)
/2. (Note a crucial property of σ; it
satisfies σk+2 = σk − σk+1 for all integer k ≥ 0.) The table gives a sequence of
augmentations.
3.9 (Max Matching/Min Cover Theorem (König-Egervary))
Consider a bipartite graph consisting of two sets of nodes S and T such that
every arc has its start node in S and its end node in T . A matching is a subset of
arcs such that all the start nodes of the arcs are distinct and all the end nodes of
the arcs are distinct. A maximal matching is a matching with a maximal number
of arcs.
(a) Show that the problem of finding a maximal matching can be formulated
as a max-flow problem.
(b) Define a cover C to be a subset of S ∪T such that for each arc (i, j), either
i ∈ C or j ∈ C (or both). A minimal cover is a cover with a minimal number
of nodes. Show that the number of arcs in a maximal matching and the
number of nodes in a minimal cover are equal. (Variants of this theorem
were independently published by König [1931] and Egervary [1931].) Hint :
Use the max-flow/min-cut theorem.
144 The Max-Flow Problem Chap. 3
3.10 (Theorem of Distinct Representatives, Hall [1956])
Given finite sets S1, S2, . . . , Sk, we say that the collection {s1, s2, . . . , sk} is a
system of distinct representatives if si ∈ Si for all i and si = sj for i = j. (For
example, if S1 = {a, b, c}, S2 = {a, b}, S3 = {a}, then s1 = c, s2 = b, s3 = a is a
system of distinct representatives.) Show that there exists no system of distinct
representatives if and only if there exists an index set I ⊂ {1, 2, . . . , k} such
that the number of elements in ∪i∈ISi is less than the number of elements in I.
Hint : Consider a bipartite graph with each of the right side nodes representing
an element of ∪i∈ISi, with each of the left side nodes representing one of the sets
S1, S2, . . . Sk, and with an arc from a left node S to a right node s if s ∈ S. Use
the maximal matching/minimal cover theorem of Exercise 3.9. For additional
material on this problem, see Hoffman and Kuhn [1956], and Mendelssohn and
Dulmage [1958].
3.11
Prove the following generalizations of Prop. 3.1:
(a) Let x be a capacity-feasible flow vector, and let N+ and N− be two disjoint
subsets of nodes. Then exactly one of the following two alternatives holds:
(1) There exists a simple path that starts at some node of N+, ends at
some node of N−, and is unblocked with respect to x.
(2) There exists a saturated cut Q = [S,N − S] such that N+ ⊂ S and
N− ⊂ N − S.
(b) Show part (a) with “simple path” in alternative (1) replaced by “path”.
Hint : Use the path decomposition theorem of Exercise 1.4.
3.12 (Termination of the Ford-Fulkerson Algorithm)
Consider the Ford-Fulkerson algorithm as described in Section 3.2 (augmenting
paths have as few arcs as possible). This exercise shows that the algorithm
terminates and solves the max-flow problem in polynomial time, even when the
problem data are irrational.
Let x0 be the initial feasible flow vector; let xk, k = 1, 2, . . ., be the flow
vector after the kth augmentation; and let Pk be the corresponding augmenting
path. An arc (i, j) is said to be a k+-bottleneck if (i, j) is a forward arc of Pk and
xkij = cij , and it is said to be a k
−-bottleneck if (i, j) is a backward arc of Pk and
xkij = bij .
(a) Show that if k < k and an arc (i, j) is a k+-bottleneck and a k
+
-bottleneck,
then for some m with k < m < k, the arc (i, j) is a backward arc of Pm.
Similarly, if an arc (i, j) is a k−-bottleneck and a k
−
-bottleneck, then for
some m with k < m < k, the arc (i, j) is a forward arc of Pm.
(b) Show that Pk is a path with a minimal number of arcs over all augmenting
paths with respect to xk−1. (This property depends on the implementation
of the unblocked path search as a breadth-first search.)
Sec. 3.4 Notes, Sources, and Exercises 145
(c) For any path P that is unblocked with respect to xk, let nk(P ) be the
number of arcs of P , let a+k (i) be the minimum of nk(P ) over all unblocked
P from s to i, and let a−k (i) be the minimum of nk(P ) over all unblocked
P from i to t. Show that for all i and k we have
a+k (i) ≤ a+k+1(i), a−k (i) ≤ a−k+1(i).
(d) Show that if k < k and arc (i, j) is both a k+-bottleneck and a k
+
-
bottleneck, or is both a k−-bottleneck and a k
−
-bottleneck, then a+k (t) <
a+
k
(t).
(e) Show that the algorithm terminates after O(NA) augmentations, for an
O(NA2) running time.
3.13 (Layered Network Algorithm)
Consider the algorithm described near the end of Section 3.2, which uses phases
and augmentations through a layered network.
(a) Provide an algorithm for constructing the layered network of each phase in
O(A) time.
(b) Show that the number of augmentations in each phase is at most A, and
provide an implementation whereby these augmentations require O(NA)
total time.
(c) Show that with each phase, the layer number k(s) of the source node s
increases strictly, so that there can be at most N − 1 phases.
(d) Show that with the implementations of (a) and (b), the running time of
the algorithm is O(N2A).
3.14 (O(N2/3A) Complexity for Unit Capacity Graphs)
Consider the max-flow problem in the special case where the arc flow range is
[0,1] for all arcs.
(a) Show that each path from the source to the sink that is unblocked with
respect to the zero flow has at most 2N/
√
M arcs, where M is the value of
the maximum flow. Hint : Let Nk be the number of nodes i such that the
shortest unblocked path from s to i has k arcs. Argue that k(k + 1) ≥ M .
(b) Show that the running time of the layered network algorithm (cf. Fig.
3.8) is reduced to O(N2/3A). Hint : Argue that each arc of the layered
network can be part of at most one augmenting path in a given phase, so
the augmentations of each phase require O(A) computation. Use part (a)
to show that the number of phases is O(N2/3).
146 The Max-Flow Problem Chap. 3
3.15
(a) Solve the problem of Exercise 3.1 using the layered network algorithm (cf.
Fig. 3.8).
(b) Construct an example of a max-flow problem where the layered network
algorithm requires N − 1 phases.
3.16
Solve the problem of Exercise 3.1 using the max-flow algorithm of Section 3.3.2.
3.17 (Relation of Path Construction and Assignment)
The purpose of this exercise (from Bertsekas [1995c]) is to show the connection
of the path construction algorithm of Section 3.3.1 with the assignment auction
algorithm of Section 1.3.3.
(a) Show that the path construction problem can be converted into the problem
of finding a solution of a certain assignment problem with all arc values
equal to 0, as shown by example in Fig. 3.13. In particular, a forward path
of a directed graph G that starts at node s and ends at node t corresponds
to a feasible solution of the assignment problem, and conversely.
(b) Show how to relate the node prices in the path construction algorithm with
the object prices of the assignment problem, so that if we apply the auction
algorithm with  = 1, the sequence of generated prices and assignments
corresponds to the sequence of generated prices and paths by the path
construction algorithm.
3.18 (Decomposition of Infeasible Assignment Problems)
Apply the decomposition approach of Section 3.1.4 to an infeasible n×n assign-
ment problem. Show that the set of persons can be partitioned in three disjoint
subsets I1, I2, and I3, and that the set of objects can be partitioned in three
disjoint subsets J1, J2, and J3 with the following properties (cf. Fig. 3.14):
(1) I1, J1, I2, and J2 are all nonempty, while I3 and J3 may be empty.
(2) There is no pair (i, j) ∈ A such that i /∈ I1 and j ∈ J1, or i ∈ I2 and j /∈ J2.
(3) If I3 and J3 are nonempty, then all pairs (i, j) ∈ A with i ∈ I3 are such
that j ∈ J3.
Identify the three component problems of the decomposition in terms of the sets
I1, J1, I2, J2, I3, and J3. Show that two of these problems are feasible asymmetric
assignment problems (the numbers of persons and objects are unequal), while the
third is a feasible symmetric assignment problem (the numbers of persons and
objects are equal).
Sec. 3.4 Notes, Sources, and Exercises 147
2
3
(3,2)
1
1
1
1
1
PERSONS OBJECTS
Equivalent Assignment Problem
t
1
1
1
2'
(s,3)
(s,2)
2
3
s t
s
Figure 3.13: Converting the path construction problem into an equivalent feasi-
bility problem of assigning “persons” to “objects.” Each arc (i, j) of the graph G,
with i = t, is replaced by an object labeled (i, j). Each node i = t is replaced by
R(i) persons, where R(i) is the number of arcs of G that are incoming to node i
(for example node 2 is replaced by the two persons 2 and 2′). Finally, there is one
person corresponding to node s and one object corresponding to node t. For every
arc (i, j) of G, with i = t, there are R(i) + R(j) incoming arcs from the persons
corresponding to i and j. For every arc (i, t) of G, there are R(i) incoming arcs
from the persons corresponding to i. Each path that starts at s and ends at t can
be associated with a feasible assignment. Conversely, given a feasible assignment,
one can construct an alternating path (a sequence of alternatively assigned and
unassigned pairs) starting at s and ending at t, which defines a path from s to t.
I2
J3I3
1J
I1
2J
Figure 3.14: Decomposition of
an infeasible assignment problem
(cf. Exercise 3.18).
148 The Max-Flow Problem Chap. 3
3.19 (Perfect Bipartite Matchings)
Consider the problem of matching n persons with n objects on a one-to-one basis
(cf. Exercises 1.21 and 3.9). For each person i there is a given set of objects A(i)
that can be matched with i. A matching is a subset of pairs (i, j) with j ∈ A(i),
such that there is at most one pair for each person and each object. A perfect
matching is one that consists of n pairs, i.e., one where every person is matched
with a distinct object.
(a) Assume that there exists a perfect matching. Consider an imperfect match-
ing S =
{
(i, j) | i ∈ I
}
, where I is a set of m < n distinct persons, and
let J =
{
j | there exists i ∈ I with (i, j) ∈ S
}
. Show that given any i /∈ I,
there exists a sequence {i, j1, i1, j2, i2, . . . , jk, ik, j} such that j /∈ J , the
pairs (i1, j1), . . . , (ik, jk) belong to S, and j1 ∈ A(i), j2 ∈ A(i1), . . . , jk ∈
A(ik−1), j ∈ A(ik). Hint : Use a max-flow formulation, and try to find an
augmenting path in a suitable graph.
(b) Show that there exists a perfect matching if and only if there is no subset
I ⊂ {1, . . . , n} such that the set ∪i∈IA(i) has fewer elements than I.
(c) (König’s Theorem on Perfect Matchings) Assume that all the sets A(i),
i = 1, . . . , n, and all the sets B(j) =
{
i | j = A(i)
}
, j = 1, . . . , n, contain
the same number of elements. Show that there exists a perfect matching.
3.20
Consider a feasible max-flow problem. Show that if the upper flow bound of each
arc is increased by α > 0, then the value of the maximum flow is increased by no
more than αA, where A is the number of arcs.
3.21
A town has m dating agencies that match men and women. Agency i has a
list of men and a list of women, and may match a maximum of ci man/woman
pairs from its lists. A person may be in the list of several agencies but may be
matched with at most one other person. Formulate the problem of maximizing
the number of matched pairs as a max-flow problem.
3.22
Consider an n × n chessboard and let A and B be two given squares.
(a) Consider the problem of finding the maximal number of knight paths that
start at A, end at B, and do not overlap, in the sense that they do not
share a square other than A and B. Formulate the problem as a max-flow
problem.
(b) Solve the problem of part (a) using the max-flow algorithm of Section 3.3.2
for the case where n = 8, and the squares A and B are two opposite corners
of the board.
Sec. 3.4 Notes, Sources, and Exercises 149
3.23 (Min-Flow Problem)
Consider the “opposite” to the max-flow problem, which is to minimize the di-
vergence out of s over all capacity-feasible flow vectors having zero divergence
for all nodes other than s and t.
(a) Show how to solve this problem by first finding a feasible solution, and by
then using a max-flow algorithm.
(b) Derive an analog to the max-flow/min-cut theorem.
4
The Min-Cost Flow Problem
Contents
4.1. Transformations and Equivalences
4.1.1. Setting the Lower Flow Bounds to Zero
4.1.2. Eliminating the Upper Flow Bounds
4.1.3. Reduction to a Circulation Format
4.1.4. Reduction to an Assignment Problem
4.2. Duality
4.2.1. Interpretation of CS and the Dual Problem
4.2.2. Duality and CS for Nonnegativity Constraints
4.3. Notes, Sources, and Exercises
151
152 The Min-Cost Flow Problem Chap. 4
In this and the following three chapters, we focus on the minimum cost
flow problem, introduced in Section 1.2:
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
bij ≤ xij ≤ cij , ∀ (i, j) ∈ A,
where aij , bij , cij , and si are given scalars.
We begin by discussing several equivalent ways to represent the prob-
lem. These are useful because different representations lend themselves
better or worse for various analytical and computational purposes. We
then develop duality theory and the associated optimality conditions. This
theory is fundamental for the algorithms of the following three chapters,
and richly enhances our insight into the problem’s structure.
4.1 TRANSFORMATIONS AND EQUIVALENCES
In this section, we describe how the minimum cost flow problem can be
represented in several equivalent “standard” forms. This is often useful,
because depending on the analytical or algorithmic context, a particular
representation may be more convenient than the others.
4.1.1 Setting the Lower Flow Bounds to Zero
The lower flow bounds bij can be changed to zero by a translation of vari-
ables, that is, by replacing xij by xij −bij , and by adjusting the upper flow
bounds and the supplies according to
cij := cij − bij ,
si := si −
∑
{j|(i,j)∈A}
bij +
∑
{j|(j,i)∈A}
bji.
Optimal flows and the optimal value of the original problem are obtained by
adding bij to the optimal flow of each arc (i, j) and adding
∑
(i,j)∈A aijbij
to the optimal value of the transformed problem, respectively. Working
with the transformed problem saves computation time and storage, and
for this reason most network flow codes assume that all lower flow bounds
are zero.
Sec. 4.1 Transformations and Equivalences 153
4.1.2 Eliminating the Upper Flow Bounds
Once the lower flow bounds have been changed to zero, it is possible to
eliminate the upper flow bounds, obtaining a problem with just a nonneg-
ativity constraint on all the flows. This can be done by introducing an
additional nonnegative variable zij that must satisfy the constraint
xij + zij = cij .
(In linear programming terminology, zij is known as a slack variable.) The
resulting problem is a minimum cost flow problem involving for each arc
(i, j), an extra node with supply cij , and two outgoing arcs, corresponding
to the flows xij and zij ; see Fig. 4.1.
i j
Cost aij
x ijFlow
cij
ijzFlow
i j
Cost  = 0
xijFlow
Cost aij
Original arc After the transformation
s
i
s j m
c
im−Σsi −Σm jmcsj
Feasible flow range: [0, c   ]ij Feasible flow range: [0,     )∞
Figure 4.1: Eliminating the upper capacity bound by replacing each arc with a
node and two outgoing arcs. Since for feasibility we must have zij = cij − xij ,
the upper bound constraint xij ≤ cij is equivalent to the lower bound constraint
0 ≤ zij . Furthermore, in view again of xij = cij − zij , the conservation of flow
equation
−
∑
j
zij −
∑
j
xji = si −
∑
j
cij
for the modified problem is equivalent to the conservation of flow equation∑
j
xij −
∑
j
xji = si
for the original problem. Using these facts, it can be seen that the feasible flow
vectors (x, z) of the modified problem can be paired on a one-to-one basis with
the feasible flow vectors x of the original problem, and that the corresponding
costs are equal. Thus, the modified problem is equivalent to the original problem.
Eliminating the upper flow bounds simplifies the statement of the
problem, but complicates the use of some algorithms. The reason is that
problems with upper (as well as lower) flow bounds are guaranteed to have
154 The Min-Cost Flow Problem Chap. 4
at least one optimal solution if they have at least one feasible solution, as
we will see in Chapter 5. However, a problem with just nonnegativity con-
straints may be unbounded , in the sense that it may have feasible solutions
of arbitrarily small cost. This is one reason why most network flow codes
require that upper and lower bound restrictions be placed on all the flow
variables.
4.1.3 Reduction to a Circulation Format
The problem can be transformed into the circulation format , in which
all node supplies are zero. One way to do this is to introduce an artificial
“accumulation” node t and an arc (t, i) for each node i with nonzero supply
si. We may then introduce the constraint si ≤ xti ≤ si and an arbitrary
cost for the flow xti. Alternatively, we may introduce an arc (t, i) and
a constraint 0 ≤ xti ≤ si for all i with si > 0, and an arc (i, t) and a
constraint 0 ≤ xit ≤ −si for all i with si < 0. The cost of these arcs should
be very small (i.e., large negative) to force the corresponding flows to be
at their upper bound at the optimum; see Fig. 4.2.
Original Network
Artificial Accumulation
Node t
1
2
3
t
s   = 21
s   = 22
s   = - 13
t20 < x    < 2
t10  < x    < 2
4
s   = - 34
4t0 < x    < 3
3t
0 < x    < 1
_
__
_
_
_
_
_
Figure 4.2: A transformation of the minimum cost flow problem into a circulation
format by using an artificial “accumulation” node t and corresponding artificial
arcs connecting t with all the nodes as shown. These arcs have very large negative
cost, to force the corresponding flows to their upper bounds at the optimum.
4.1.4 Reduction to an Assignment Problem
Finally, the minimum cost flow problem may be transformed into a trans-
Sec. 4.2 Duality 155
portation problem of the form
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij = αi, ∀ i = 1, . . . , m,
∑
{i|(i,j)∈A}
xij = βj , ∀ j = 1, . . . , n,
0 ≤ xij ≤ min{αi, βj}, ∀ (i, j) ∈ A;
see Fig. 4.3. This transportation problem can itself be converted into an
assignment problem by creating αi unit supply sources (βj unit demand
sinks) for each transportation problem source i (sink j, respectively). For
this reason, any algorithm that solves the assignment problem can be ex-
tended into an algorithm for the minimum cost flow problem. This mo-
tivates a useful way to develop and analyze new algorithmic ideas; apply
them to the simpler assignment problem and generalize them using the
construction just given to the minimum cost flow problem.
4.2 DUALITY
We have already introduced some preliminary duality ideas in the context
of the assignment problem in Section 1.3.2. In this section, we consider the
general minimum cost flow problem, and we obtain a dual problem using
a procedure that is standard in duality theory. We introduce a Lagrange
multiplier, also called a price pi for the conservation of flow constraint for
node i and we form the corresponding Lagrangian function
L(x, p) =
∑
(i,j)∈A
aijxij +
∑
i∈N
si − ∑
{j|(i,j)∈A}
xij +
∑
{j|(j,i)∈A}
xji
 pi
=
∑
(i,j)∈A
(aij + pj − pi)xij +
∑
i∈N
sipi. (4.1)
Here, we use p to denote the vector whose components are the prices pi.
Let us now fix p and consider minimizing L(x, p) with respect to
x without the requirement to meet the conservation of flow constraints.
It is seen that pi may be viewed as a penalty per unit violation of the
conservation of flow constraint. If pi is too small (or too large), there is an
incentive for positive (or negative, respectively) violation of the constraint.
This suggests that we should search for the correct values pi for which,
156 The Min-Cost Flow Problem Chap. 4
. . .
. . .
. . .
. . .
. . .
(i,j)
i
j
Cost Coeff. = 0
Cost Coeff. = a
c    - bijij
ij
Σ
m
(c    - b    ) - s
im im i
Σ
m jjm jm
(c    - b    ) - s
SOURCES
(Arcs of original
network)
SINKS
(Nodes of original 
network)
Figure 4.3: Transformation of a minimum cost flow problem into a transportation
problem. The idea is to introduce a new node for each arc and introduce a slack
variable for every arc flow; see Fig. 4.1. This not only eliminates the upper bound
constraint on the arc flows, as in Fig. 4.1, but also creates a bipartite graph
structure. In particular, we take as sources of the transportation problem the
arcs of the original network, and as sinks of the transportation problem the nodes
of the original network. Each transportation problem source has two outgoing
arcs with cost coefficients as shown. The supply of each transportation problem
source is the feasible flow range length of the corresponding original network arc.
The demand of each transportation problem sink is the sum of the feasible flow
range lengths of the outgoing arcs from the corresponding original network node
minus the supply of that node, as shown. An arc flow xij in the minimum cost flow
problem corresponds to flows equal to xij and cij −bij −xij on the transportation
problem arcs
(
(i, j), j
)
and
(
(i, j), i
)
, respectively.
when L(x, p) is minimized over all capacity-feasible x, there is no incentive
for either positive or negative violation of all the constraints.
We are thus motivated to introduce the dual function value q(p) at a
vector p, defined by
q(p) = min
x
{
L(x, p) | bij ≤ xij ≤ cij , (i, j) ∈ A
}
. (4.2)
Because the Lagrangian function L(x, p) is separable in the arc flows xij , its
minimization decomposes into a separate minimization for each arc (i, j).
Each of these minimizations can be carried out in closed form, yielding
q(p) =
∑
(i,j)∈A
qij(pi − pj) +
∑
i∈N
sipi, (4.3)
Sec. 4.2 Duality 157
where
qij(pi − pj) = min
bij≤xij≤cij
(aij + pj − pi)xij
=
{
(aij + pj − pi)bij if pi ≤ aij + pj ,
(aij + pj − pi)cij if pi > aij + pj .
(4.4)
Consider now the problem
maximize q(p)
subject to no constraint on p,
where q is the dual function given by Eqs. (4.3) and (4.4). We call this the
dual problem, and we refer to the original minimum cost flow problem as the
primal problem. We also refer to the dual function as the dual cost function
or dual cost , and we refer to the optimal value of the dual problem as the
optimal dual cost .† We will see that solving the dual problem provides the
correct values of the prices pi, which will allow the optimal flows to be
obtained by minimizing L(x, p).
ijDual Cost q   (p   - p  )
for Arc (i,j)
i j
bijSlope = - 
cijSlope = - 
ija
i jp  - pbij cij
Slope = aij
Primal Cost 
for Arc (i,j)
Figure 4.4: Form of the dual cost function qij for arc (i, j).
Figure 4.4 illustrates the form of the functions qij . Since each qij is
piecewise linear, the dual function q is also piecewise linear. The dual func-
tion also has some additional interesting structure. In particular, suppose
† There is a slight abuse of terminology here, since in a dual context we
are not minimizing a cost but rather maximizing a value, but there is some
uniformity advantage in referring to cost in both the primal and the dual context.
Besides, some problems such as the assignment problem in Section 1.3, are cast as
maximization problems and their duals become minimization problems, so using
the term “dual value” rather than “dual cost” would be inappropriate.
158 The Min-Cost Flow Problem Chap. 4
that all node prices are changed by the same amount. Then the values of
the functions qij do not change, since these functions depend on the price
differences pi − pj . If in addition we have
∑
i∈N si = 0, as we must if the
problem is feasible, we see that the term
∑
i∈N sipi also does not change.
Thus, the dual function value does not change when all node prices are
changed by the same amount, implying that the equal cost surfaces of the
dual cost function are unbounded. Figure 4.5 illustrates the dual function
for a simple example.
We now turn to the development of the basic duality results for the
minimum cost flow problem. To this end we appropriately generalize the
notion of complementary slackness, introduced in Section 1.3 within the
context of the assignment problem:
Definition 4.1: We say that a flow-price vector pair (x, p) satisfies
complementary slackness (or CS for short) if x is capacity-feasible and
pi − pj ≤ aij , ∀ (i, j) ∈ A with xij < cij , (4.5)
pi − pj ≥ aij , ∀ (i, j) ∈ A with bij < xij . (4.6)
Note that the CS conditions imply that
pi = aij + pj , ∀ (i, j) ∈ A with bij < xij < cij .
An equivalent way to write the CS conditions is that, for all arcs (i, j), we
have bij ≤ xij ≤ cij and
xij =
{
cij if pi > aij + pj ,
bij if pi < aij + pj .
Another equivalent way to state the CS conditions is that xij attains the
minimum in the definition of qij
xij = arg min
bij≤zij≤cij
(aij + pj − pi)zij (4.7)
for all arcs (i, j). Figure 4.6 provides a graphical interpretation of the CS
conditions.
The following proposition is an important duality theorem, and will
later form the basis for developing a more complete duality analysis with
the aid of the simplex-related algorithmic developments of Chapter 5.
Proposition 4.1: A feasible flow vector x∗ and a price vector p∗ sat-
isfy CS if and only if x∗ and p∗ are optimal primal and dual solutions,
respectively, and the optimal primal and dual costs are equal.
Sec. 4.2 Duality 159
Cost = 1
s  = 11
s  = 02
s  = -13
Cost = 3
Flow range: [0,1]
Cost = 1
Flow range: [0,1] Flow range: [0,1]
(a)
1
2
3
Price of Node 2
Price of Node 1
Dual function
Price of Node 3 is Fixed at 0
(b)
Figure 4.5: Form of the dual cost function q for the 3-node problem in (a). The
optimal flow is x12 = 1, x23 = 1, x13 = 0. The dual function is
q(p1, p2, p3) = min{0, 1 + p2 − p1} + min{0, 1 + p3 − p2}
+ min{0, 3 + p3 − p1} + p1 − p3.
Diagram (b) shows the graph of the dual function in the space of p1 and p2, with
p3 fixed at 0. For a different value of p3, say γ, the graph is “translated” by the
vector (γ, γ); that is, we have q(p1, p2, 0) = q(p1 + γ, p2 + γ, γ) for all (p1, p2).
The dual function is maximized at the vectors p that satisfy CS together with the
optimal x. These are the vectors of the form (p1 + γ, p2 + γ, γ), where
1 ≤ p1 − p2, p1 ≤ 3, 1 ≤ p2.
Proof: We first show that for any feasible flow vector x and any price
vector p, the primal cost of x is no less than the dual cost of p. Indeed, we
160 The Min-Cost Flow Problem Chap. 4
0
aij
b ij cij x ij
p jpi -
Figure 4.6: Illustration of CS for a flow-price pair (x, p). For each arc (i, j), the
pair (xij , pi − pj) should lie on the graph shown.
have from the definitions (4.1) and (4.2) of L and q, respectively,
q(p) ≤ L(x, p)
=
∑
(i,j)∈A
aijxij +
∑
i∈N
si − ∑
{j|(i,j)∈A}
xij +
∑
{j|(j,i)∈A}
xji
 pi
=
∑
(i,j)∈A
aijxij ,
(4.8)
where the last equality follows from the feasibility of x.
If x∗ is feasible and satisfies CS together with p∗, we have by the
definition (4.2) of q
q(p∗) = min
x
{
L(x, p∗) | bij ≤ xij ≤ cij , (i, j) ∈ A
}
= L(x∗, p∗)
=
∑
(i,j)∈A
aijx∗ij ,
(4.9)
where the second equality is true because
(x∗,p∗) satisfies CS if and only if
x∗ij minimizes (aij + p
∗
j − p∗i )xij over all xij ∈ [bij , cij ], ∀ (i, j) ∈ A,
[cf. Eq. (4.7)], and the last equality follows from the Lagrangian expression
(4.1) and the feasibility of x∗. Therefore, Eq. (4.9) implies that x∗ attains
Sec. 4.2 Duality 161
the minimum of the primal cost on the right-hand side of Eq. (4.8), and p∗
attains the maximum of q(p) on the left-hand side of Eq. (4.8), while the
optimal primal and dual values are equal.
Conversely, suppose that x∗ and p∗ are optimal primal and dual so-
lutions, respectively, and the two optimal costs are equal, that is,
q(p∗) =
∑
(i,j)∈A
aijx∗ij .
We have by definition
q(p∗) = min
x
{
L(x, p∗) | bij ≤ xij ≤ cij , (i, j) ∈ A
}
,
and also, using the Lagrangian expression (4.1) and the feasibility of x∗,∑
(i,j)∈A
aijx∗ij = L(x∗, p∗).
Combining the last three equations, we obtain
L(x∗, p∗) = min
x
{
L(x, p∗) | bij ≤ xij ≤ cij , (i, j) ∈ A
}
.
Using the Lagrangian expression (4.1), it follows that for all arcs (i, j), we
have
x∗ij = arg min
bij≤xij≤cij
(aij + p∗j − p∗i )xij .
This is equivalent to the pair (x∗, p∗) satisfying CS. Q.E.D.
There are also several other important duality results. In particular,
in Prop. 5.8 of Chapter 5 we will use a constructive algorithmic approach
to show the following:
Proposition 4.2: If the minimum cost flow problem (with upper and
lower bounds on the arc flows) is feasible, then there exist optimal
primal and dual solutions, and the optimal primal and dual costs are
equal.
Proof: See Prop. 5.8 of Chapter 5. Q.E.D.
By combining Props. 4.1 and 4.2, we obtain the following variant
of Prop. 4.1, which includes no statement on the equality of the optimal
primal and dual costs:
162 The Min-Cost Flow Problem Chap. 4
Proposition 4.3: A feasible flow vector x∗ and a price vector p∗ sat-
isfy CS if and only if x∗ and p∗ are optimal primal and dual solutions.
Proof: The forward statement is part of Prop. 4.1. The reverse statement,
is obtained by using the equality of the optimal primal and dual costs (Prop.
4.2) and the reverse part of Prop. 4.1. Q.E.D.
4.2.1 Interpretation of CS and the Dual Problem
The CS conditions have a nice economic interpretation. In particular, think
of each node i as choosing the flow xij of each of its outgoing arcs (i, j) from
the range [bij , cij ], on the basis of the following economic considerations:
For each unit of the flow xij that node i sends to node j along arc (i, j),
node i must pay a transportation cost aij plus a storage cost pj at node
j; for each unit of the residual flow cij − xij that node i does not send
to j, node i must pay a storage cost pi. Thus, the total cost to node j is
(aij + pj)xij + (cij − xij)pi, or
(aij + pj − pi)xij + cijpi.
It can be seen that the CS conditions (4.5) and (4.6) are equivalent to
requiring that node i act in its own best interest by selecting the flow that
minimizes the corresponding costs for each of its outgoing arcs (i, j); that
is,
(x, p) satisfies CS if and only if
xij minimizes (aij + pj − pi)zij over all zij ∈ [bij , cij ], ∀ (i, j) ∈ A,
[cf. Eq. (4.7)].
To interpret the dual function q(p), we continue to view aij and pi
as transportation and storage costs, respectively. Then, for a given price
vector p and supply vector s, the dual function
q(p) = min
bij≤xij≤cij
(i,j)∈A
{ ∑
(i,j)∈A
aijxij
+
∑
i∈N
si − ∑
{j|(i,j)∈A}
xij +
∑
{j|(j,i)∈A}
xji
 pi}
is the minimum total transportation and storage cost to be incurred by the
nodes, by choosing flows that satisfy the capacity constraints.
Sec. 4.2 Duality 163
Suppose now that we introduce an organization that sets the node
prices, and collects the transportation and storage costs from the nodes.
We see that if the organization wants to maximize its total revenue (given
that the nodes will act in their own best interest), it must choose prices
that solve the dual problem optimally.
4.2.2 Duality and CS for Nonnegativity Constraints
We finally note that there are variants of CS and Props. 4.1-4.3 for the
versions of the minimum cost flow problem where bij = −∞ and/or cij = ∞
for some arcs (i, j). In particular, in the case where in place of the capacity
constraints bij ≤ xij ≤ cij , there are only nonnegativity constraints 0 ≤ xij ,
the CS conditions take the form
pi − pj ≤ aij , ∀ (i, j) ∈ A,
pi − pj = aij , ∀ (i, j) ∈ A with 0 < xij ,
(see Fig. 4.7).
0
aij
x ij
p jpi -
Figure 4.7: Illustration of CS for a flow-price pair (x, p) in the case of nonneg-
ativity constraints 0 ≤ xij for the flow of each arc (i, j). The pair (xij , pi − pj)
should lie on the graph shown.
Some of the modifications needed to prove counterparts of the du-
ality results for nonnegativity constraints are outlined in Exercise 4.3. In
particular, Prop. 4.1 holds for this case as stated. However, showing a
counterpart of Prop. 4.2 involves a slight complication. In the case of non-
negativity constraints, it is possible that there exist feasible flow vectors
of arbitrarily small cost; a problem where this happens will be called un-
bounded in Chapter 5. Barring this possibility, the existence of primal and
164 The Min-Cost Flow Problem Chap. 4
dual optimal solutions with equal cost (cf. Prop. 4.2) will be shown in Prop.
5.6 of Section 5.2.
4.3 NOTES, SOURCES, AND EXERCISES
The minimum cost flow problem was formulated in the early days of lin-
ear programming. There has been extensive research on the algorithmic
solution of the problem, much of which will be the subject of the following
three chapters. This research has followed two fairly distinct directions.
On one hand there has been intensive development of practically efficient
algorithms. These algorithms were originally motivated by general lin-
ear programming methods such as the primal simplex, dual simplex, and
primal-dual methods, but gradually other methods, such as auction algo-
rithms, were proposed, which have no general linear programming coun-
terparts. The focus of research in these algorithms was to establish their
validity through a proof of guaranteed termination, to analyze their special
properties, and to establish their practical computational efficiency through
experimentation with “standard” test problems.
On the other hand there have been efforts to explore the worst-case
complexity limits of the minimum cost flow problem using polynomial al-
gorithms. Edmonds and Karp [1972] developed the first polynomial algo-
rithm, using a version of the out-of-kilter method (a variant of the primal-
dual method to be discussed in Chapter 6) that employed cost and capac-
ity scaling. Subsequently, in the late 70s, polynomial algorithms for the
general linear programming problem started appearing, and these were of
course applicable to the minimum cost flow problem. All of these poly-
nomial algorithms are not strongly polynomial because their running time
depends not just on the number of nodes and arcs, but also on the arc
costs and capacities. A strongly polynomial algorithm for the minimum
cost flow problem was given by Tardos [1985]. The existence of a strongly
polynomial algorithm distinguishes the minimum cost flow problem from
the general linear programming problem, for which there is no known al-
gorithm with running time that depends only on the number of variables
and constraints. However, a point made earlier in Section 1.3.4 should be
repeated: a polynomial running time does not guarantee good practical
performance. For example, Tardos’ algorithm has not been seriously con-
sidered for algorithmic solution of practical minimum cost flow problems.
Thus, to select an algorithm for a practical problem one must typically rely
on criteria other than worst-case complexity.
Duality theory is of central importance in linear programming, and
is similarly important in network optimization. It has its origins in the
work of von Neuman on zero sum games, and was first formalized by Gale,
Kuhn, and Tucker [1951]. Similar to linear programming, there are several
Sec. 4.3 Notes, Sources, and Exercises 165
possible dual problems, depending on which of the constraints are “dual-
ized” (assigned a Lagrange multiplier). The duality theory of this chapter,
where the conservation of flow constraints are dualized, is the most common
and useful for the minimum cost flow problem. We will develop alterna-
tive forms of duality when we discuss other types of network optimization
problems in Chapters 8-10.
We finally note that one can illustrate the relation between the primal
and the dual problems in terms of an intuitive geometric interpretation (see
Fig. 4.8). This interpretation is directed toward the advanced reader and
will not be needed later. It demonstrates why the cost of any feasible flow
vector is no less than the dual cost of any price vector (later, in Chapter
8, this will be called the weak duality theorem), and why thanks to the
linearity of the cost function and the constraints, the optimal primal and
dual costs are equal.
E X E R C I S E S
4.1 (Reduction to One Source/One Sink Format)
Show how the minimum cost flow problem can be transformed to an equivalent
problem where all node supplies are zero except for one node that has positive
supply and one node that has negative supply.
4.2 (Duality for Assignment Problems)
Consider the assignment problem of Example 1.2. Derive the dual problem and
the CS conditions, and show that they are mathematically equivalent to the ones
introduced in Section 1.3.2.
4.3 (Duality for Nonnegativity Constraints)
Consider the version of the minimum cost flow problem where there are nonneg-
ativity constraints
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
0 ≤ xij , ∀ (i, j) ∈ A.
166 The Min-Cost Flow Problem Chap. 4
Divergence y
Cost z
(-p,1)
Optimal primal cost
q(p)
s
Set S of pairs (y,z ) as x ranges over the
set of all capacity-feasible flow vectors
Vertical line L = {(s,z ) | z : real number}
Figure 4.8: Geometric interpretation of duality for the reader who is familiar
with the notion and the properties of hyperplanes in a vector space. Consider the
(polyhedral) set S consisting of all pairs (y, z), where y is the divergence vector
corresponding to x and z is the cost of x, as x ranges over all capacity-feasible
flow vectors. Then feasible flow vectors correspond to common points of S and
the vertical line
L = {(s, z) | z : real number}.
The optimal primal cost corresponds to the lowest common point.
On the other hand, for a given price vector p, the dual cost q(p) can be
expressed as [cf. Eq. (4.2)]
q(p) = min
x: capacity feasible
L(x, p) = min
(y,z)∈S
{
z −
∑
i∈N
yipi
}
+
∑
i∈N
sipi.
Based on this expression, it can be seen that q(p) corresponds to the intersection
point of the vertical line L with the hyperplane{
(y, z)
∣∣∣ z − ∑
i∈N
yipi = q(p) −
∑
i∈N
sipi
}
,
which supports from below the set S, and is normal to the vector (−p, 1). The
dual problem is to find a price vector p for which the intersection point is as high
as possible. The figure illustrates the equality of the lowest common point of
S and L (optimal primal cost), and the highest point of intersection of L by a
hyperplane that supports S from below (optimal dual cost).
Sec. 4.3 Notes, Sources, and Exercises 167
Show that a feasible flow vector x∗ and a price vector p∗ satisfy the following CS
conditions
p∗i − p∗j ≤ aij , ∀ (i, j) ∈ A,
p∗i − p∗j = aij , ∀ (i, j) ∈ A with 0 < x∗ij ,
if and only if x∗ is primal optimal, p∗ is an optimal solution of the following dual
problem:
maximize
∑
i∈N
sipi
subject to pi − pj ≤ aij , ∀ (i, j) ∈ A,
and the optimal primal and dual costs are equal. Hint : Complete the details of
the following argument. Define
q(p) =
{∑
i∈N sipi if pi − pj ≤ aij , ∀ (i, j) ∈ A,
−∞ otherwise,
and note that
q(p) =
∑
(i,j)∈A
min
0≤xij
(
aij + pj − pi
)
xij +
∑
i∈N
sipi
= min
0≤x
 ∑
(i,j)∈A
aijxij +
∑
i∈N
si − ∑
{j|(i,j)∈A}
xij +
∑
{j|(j,i)∈A}
xji
 pi
 .
Thus, for any feasible x and any p, we have
q(p) ≤
∑
(i,j)∈A
aijxij +
∑
i∈N
si − ∑
{j|(i,j)∈A}
xij +
∑
{j|(j,i)∈A}
xji
 pi
=
∑
(i,j)∈A
aijxij .
(4.10)
On the other hand, we have
q(p∗) =
∑
i∈N
sip
∗
i =
∑
(i,j)∈A
(
aij + p
∗
j − p∗i
)
x∗ij +
∑
i∈N
sip
∗
i =
∑
(i,j)∈A
aijx
∗
ij ,
where the second equality holds because the CS conditions imply that (aij +p
∗
j −
p∗i )x
∗
ij = 0 for all (i, j) ∈ A, and the last equality follows from the feasibility of
x∗. Therefore, x∗ attains the minimum of the primal cost on the right-hand side
of Eq. (4.10). Furthermore, p∗ attains the maximum of q(p) on the left-hand side
of Eq. (4.10), which means that p∗ is an optimal solution of the dual problem.
168 The Min-Cost Flow Problem Chap. 4
4.4 (Duality and the Max-Flow/Min-Cut Theorem)
Consider a feasible max-flow problem and let Q = [S,N − S] be a minimum
capacity cut separating s and t. Consider also the minimum cost flow problem
formulation for the max-flow problem (see Example 1.3). Show that the price
vector
pi =
{
1 if i ∈ S,
0 if i /∈ S,
is an optimal solution of the dual problem. Furthermore, show that the max-
flow/min-cut theorem expresses the equality of the primal and dual optimal costs.
Hint : Relate the capacity of Q with the dual function value corresponding to p.
4.5 (Min-Path/Max-Tension Theorem)
Consider a shortest path problem with arc lengths aij . For a price vector p =
(p1, . . . , pN ), define the tension of arc (i, j) as tij = pi − pj and the tension of a
forward path P as TP =
∑
(i,j)∈P tij . Show that the shortest distance between
two nodes i1 and i2 is equal to the maximal value of TP over all forward paths
P starting at i1 and ending at i2, and all price vectors p satisfying the constraint
tij ≤ aij for all arcs (i, j). Interpret this as a duality result.
5
Simplex Methods
Contents
5.1. Main Ideas in Simplex Methods
5.1.1. Using Prices to Obtain the In-Arc
5.1.2. Obtaining the Out-Arc
5.1.3. Dealing with Degeneracy
5.2. The Basic Simplex Algorithm
5.2.1. Termination Properties of the Simplex Method
5.2.2. Initialization of the Simplex Method
5.3. Extension to Problems with Upper and Lower Bounds
5.4. Implementation Issues
5.5. Notes, Sources, and Exercises
169
170 Simplex Methods Chap. 5
Primal cost improvement methods start with a feasible flow vector x and
generate a sequence of other feasible flow vectors, each having a smaller
primal cost than its predecessor. The main idea is that if the current flow
vector is not optimal, an improved flow vector can be obtained by pushing
flow along a simple cycle C with negative cost (see Prop. 1.2 and Exercise
1.33 in Chapter 1).
There are several methods for finding negative cost cycles, but the
most successful in practice are specialized versions of the simplex method
for linear programming. This chapter focuses on algorithms of this type.
Simplex methods are not only useful for algorithmic solution of the
problem; they also provide constructive proofs of some important analytical
results. Chief among these are duality theorems asserting the equality
of the primal and the dual optimal values, and the existence of optimal
primal and dual solutions, which are integer if the problem data are integer
(see Prop. 5.6 in Section 5.2 and Prop. 5.8 in Section 5.3). There are
alternative proofs that do not rely on the simplex method for the duality
results (see e.g., Bertsimas and Tsitsiklis [1997], Rockafellar [1984]), and for
the integrality results (see Exercise 1.34 in Chapter 1 and the discussion of
unimodularity in Section 5.5). However, given our independent algorithmic
interest in the simplex method, our approach to duality and the integrality
of optimal solutions is simple and economical.
5.1 MAIN IDEAS IN SIMPLEX METHODS
In this section, we develop the basic concepts underlying simplex methods.
To simplify the presentation, we first consider the version of the minimum
cost flow problem with only nonnegativity constraints on the flows:
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
0 ≤ xij , ∀ (i, j) ∈ A,
(5.1)
where aij and si are given scalars. We saw in Section 4.2 that the general
minimum cost flow problem with upper and lower bounds on the arc flows
can be converted to one with nonnegativity constraints. Thus, once we
develop the main method for nonnegativity constraints, the extension to
the more general problem will be straightforward (see Section 5.3).
The most important difference between the minimum cost flow prob-
lem with nonnegativity constraints and the one with upper and lower
bounds is that the former can be unbounded . By this we mean that there
Sec. 5.1 Main Ideas in Simplex Methods 171
exist feasible flows that take arbitrarily large values, while the correspond-
ing cost takes arbitrarily small (i.e., large negative) values. In particular,
the problem is unbounded if it is feasible and there exists a simple forward
cycle with negative cost , since then we can reduce the cost to arbitrarily
large negative values by adding arbitrarily large flow along the negative
cost cycle to any feasible flow vector. The converse is also true: if the prob-
lem is unbounded, there must exist a simple forward cycle with negative
cost . This follows from Prop. 2.7, which implies that if the cost of every
simple forward cycle is nonnegative, then the cost function of the problem
is bounded from below by some constant.
Spanning Trees and Basic Flow Vectors
The main idea in simplex methods is to generate negative cost cycles by
using a spanning tree of the given graph. Recall from Section 1.1 that a tree
is an acyclic connected graph, and that a spanning tree of a given graph is
a subgraph that is a tree and includes all nodes of the given graph. A leaf
node of a tree is defined to be a node with a single incident arc. Figure 5.1
illustrates a spanning tree and a leaf node. The following lemma collects
some important properties of spanning trees that will be useful later.
Simple cycle closed
by adding arc (i,j) to T
A leaf node
i
j
Figure 5.1: Illustration of a span-
ning tree T . Note that that there is
a unique simple path of T connect-
ing any pair of nodes. Furthermore,
the addition of any arc to T [arc
(i, j) in the figure] creates a unique
simple cycle in which (i, j) is a for-
ward arc.
Lemma 1.1: Let T be a subgraph of a graph with N nodes.
(a) If T is acyclic and has at least one arc, then it must have at least
one leaf node.
(b) T is a spanning tree if and only if it is connected and has N
nodes and N − 1 arcs.
172 Simplex Methods Chap. 5
(c) If T is a tree, for any two nodes i and j of T there is a unique
simple path of T starting at i and ending at j. Furthermore, any
arc e that is not in T and has both of its end nodes in T , when
added to T , creates a unique simple cycle in which e is a forward
arc.
(d) If T is a tree and an arc (i, j) of T is deleted, the remaining arcs
of T form two trees that are disjoint (share no nodes or arcs),
one containing i and the other containing j.
Proof: (a) Choose a node n1 of T with at least one incident arc e1 and
let n2 be the opposite node of that arc. If n2 is a leaf node, the result is
proved; else choose an arc e2 	= e1 that is incident to n2, and let n3 be the
opposite end node. If n3 is a leaf node, the result is proved; else continue
similarly. Eventually a leaf node will be found, for otherwise some node
will be repeated in the sequence, which is impossible since T is acyclic.
(b) Let T be a spanning tree. Then T has N nodes, and since it is connected
and acyclic, it must have a leaf node n1 by part (a). (We assume without
loss of generality that N ≥ 2.) Delete n1 and its unique incident arc from
T , thereby obtaining a connected graph T1, which has N − 1 nodes and is
acyclic. Repeat the process with T1 in place of T , obtaining T2, T3, and so
on. After N − 1 steps and N − 1 arc deletions, we will obtain TN−1, which
consists of a single node. This proves that T has N − 1 arcs.
Conversely, suppose that T is connected and has N nodes and N − 1
arcs. If T had a simple cycle, by deleting any arc of the cycle, we would
obtain a graph T1 that would have N −2 arcs and would still be connected.
Continuing similarly if necessary, we obtain for some k ≥ 1 a graph Tk,
which has N −k−1 arcs, and is connected and acyclic (i.e., it is a spanning
tree). This is a contradiction, because we proved earlier that a spanning
tree has exactly N − 1 arcs. Hence, T has no simple cycle and must be a
spanning tree.
(c) There is at least one simple path starting at a node i and ending at a
node j because T is connected. If there were a second path starting at i
and ending at j, by reversing this path so that it starts at j and ends at i,
and by concatenating it to the first path, we would form a cycle. It can be
seen that this cycle must contain a simple cycle, since otherwise the two
paths from i to j would be identical. This contradicts the hypothesis that
T is a tree.
If arc e is added to T , it will form a simple cycle together with any
simple path that lies in T and connects its end nodes. There is only one
such path, so together with this path, e forms a unique simple cycle in
which e is a forward arc.
Sec. 5.1 Main Ideas in Simplex Methods 173
(d) It can be seen that removal of a single arc from any connected graph
either leaves the graph connected or else creates exactly two connected
components. The unique simple path of T connecting i to j consists of
arc (i, j); with the removal of this arc, no path connecting i to j remains,
and the graph cannot stay connected. Hence, removal of (i, j) must cre-
ate exactly two connected components, which must be trees since, being
subgraphs of T , they must be acyclic. Q.E.D.
Suppose that we have a feasible problem and we are given a spanning
tree T . A key property for our purposes is that there is a flow vector x,
which satisfies the conservation of flow constraints, and is such that only
arcs of T can have a nonzero flow. Such a flow vector is called basic† and
is uniquely determined by T , as the following proposition shows.
Proposition 5.1: Consider the minimum cost flow problem with non-
negativity constraints, and assume that
∑
i∈N si = 0. Then, for any
spanning tree T , there exists a unique flow vector x that satisfies the
conservation of flow constraints∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
and is such that all arcs not in T have zero flow. In particular, if an
arc (i, j) of T separates T into two components Ti and Tj , containing
i and j respectively, we have
xij =
∑
n∈Ti
sn.
Proof: To show uniqueness, note that for any flow vector x and arc (i, j) ∈
T the flux across the cut [Ti,N − Ti] is equal to the sum of divergences
of the nodes of Ti (cf. Section 3.1). Thus, if x satisfies the conservation of
flow constraints, the flux across the cut must be
∑
n∈Ti sn. If in addition
all arcs of the cut carry zero flow except for (i, j), this flux is just xij , so
† The term “basic” comes from linear programming, where solutions of the
constraint equations that have nonzero components only for suitably specified
subsets of indices are called basic (see e.g., Dantzig [1963], Chvatal [1983], Bert-
simas and Tsitsiklis [1997]). Our definition of basic flow vector is equivalent to
the definition of a basic solution when the minimum cost flow problem of this
section is viewed as a linear program.
174 Simplex Methods Chap. 5
we must have
xij =
{ ∑
n∈Ti sn if (i, j) ∈ T ,
0 if (i, j) /∈ T .
Thus, if a flow vector has the required properties, it must be equal to the
vector x defined by the preceding formula.
To show existence, i.e. that the flow vector x, defined by the preceding
formula, satisfies the conservation of flow constraints, we use a constructive
proof based on the algorithm of Fig. 5.2. (An alternative algorithm is out-
lined in Exercise 5.4.) Q.E.D.
Note that a basic flow vector need not be feasible; some of the arc flows
may be negative, violating the lower bound constraints (see the example
of Fig. 5.2). A spanning tree is called (with slight abuse of terminology) a
feasible tree if the corresponding basic flow vector is feasible.
Overview of Simplex Methods
Simplex methods start with a feasible tree and proceed in iterations, gen-
erating another feasible tree and a corresponding feasible basic flow vector
at each iteration. The cost of each basic flow vector is no worse than the
cost of its predecessor. Each iteration, also called a pivot in the standard
terminology of linear programming, operates as follows:
(a) It adds a single arc to the tree such that the simple cycle created has
negative cost.
(b) It pushes along the cycle as much flow as possible without violating
feasibility.
(c) It discards one arc of the cycle, thereby obtaining another feasible
tree to be used at the next iteration.
Any method that uses iterations of the type described above will be called
a simplex method . There are several possible ways to add an arc to the
tree and to discard an arc from the tree, so the above description defines
a broad class of methods. However, in all cases the cost corresponding to
the new tree is no larger than the cost corresponding to the preceding tree.
In what follows, we will discuss and analyze various possibilities for arc
selection, and we will delineate some methods that have sound theoretical
properties.
Note that each tree T in the sequence generated by a simplex method
differs from its predecessor T by two arcs: the out-arc e, which belongs to
T but not to T , and the in-arc e, which belongs to T but not to T ; see Fig.
5.3. We will use the notation
T = T + e − e
to express this relation. The arc e when added to T closes a unique simple
cycle in which e is a forward arc. This is the cycle along which we try to
Sec. 5.1 Main Ideas in Simplex Methods 175
1 2
3
4
5
1
- 2
1
- 1
s   = 1 1 s   = - 22
s   = 33
s   = - 14
s   = - 15
Component tree T2
Component tree T3
Iteration # Leaf Node Selected Arc Flow Computed
1 1 x12 = 1
2 5 x53 = −1
3 3 x23 = −2
4 2 x24 = 1
Figure 5.2: Method for constructing the flow vector corresponding to T , starting
from the arc incident to some leaf node and proceeding “inward.” The algorithm
maintains a tree R, a flow vector x, and scalars w1, . . . , wN . Upon termination,
x is the desired flow vector. Initially, R = T , x = 0, and wi = si for all i ∈ N .
Step 1: Choose a leaf node i ∈ R. If (i, j) is the unique incident arc of i, set
xij := wi, wj := wj + wi;
if (j, i) is the unique incident arc of i, set
xji := −wi, wj := wj − wi.
Step 2: Delete i and its incident arc from R. If R now consists of a single node,
terminate; else, go to Step 1.
We now show that if
∑
n∈N sn = 0, the flow vector thus constructed
satisfies the conservation of flow equations. Consider the typical iteration where
the leaf node i of R is selected in Step 1. Suppose that (i, j) is the unique
incident arc of R [the proof is similar if (j, i) is the incident arc]. Then just
before this iteration, wi is equal by construction to si −
∑
{k =j|(i,k)∈A} xik +∑
{k|(k,i)∈A} xki, so by setting xij to wi, the conservation of flow constraint is
satisfied at node i. Upon termination, it is seen that for the last node i of R, wi
is equal to both
∑
n∈N sn and si −
∑
{k|(i,k)∈A} xik +
∑
{k|(k,i)∈A} xki. Since∑
n∈N sn = 0, the conservation of flow constraint is satisfied at this last node as
well.
176 Simplex Methods Chap. 5
push flow. (By convention, we require that the orientation of the cycle is
the same as the orientation of the arc e.)
Tree T together with in-arc e
Out-Arc eCycle C
In-Arc e
Tree T = T + e – e
Figure 5.3: Successive trees T and T generated by a simplex method.
Leaving aside for the moment the issue of how to select an initial
feasible tree, the main questions now are:
(1) How to select the in-arc so as to close a cycle with negative cost or
else detect that the current flow is optimal.
(2) How to select the out-arc so as to obtain a new feasible tree and
associated flow vector.
(3) How to ensure that the method makes progress, eventually improving
the primal cost. (The problem here is that even if a negative cost cycle
is known, it may not be possible to push a positive amount of flow
along the cycle because some backward arc on the cycle has zero flow.
Thus, the flow vector may not change and the primal cost may not
decrease strictly at any one pivot; in linear programming terminology,
such a pivot is called degenerate. Having to deal with degeneracy is
the price for simplifying the search for a negative cost cycle.)
We take up these questions in sequence.
5.1.1 Using Prices to Obtain the In-Arc
While simplex methods are primal cost improvement algorithms, they typ-
ically make essential use of price vectors and duality ideas. In particular,
we will see how the complementary slackness (CS) conditions of Section
4.2.2,
pi − pj ≤ aij , ∀ (i, j) ∈ A, (5.2)
pi − pj = aij , for all (i, j) ∈ A with 0 < xij , (5.3)
Sec. 5.1 Main Ideas in Simplex Methods 177
play an important role. Note here that if x is feasible and together with p
satisfies these CS conditions, then x is an optimal solution of the problem
and p is an optimal solution of its dual problem
maximize
∑
i∈N
sipi
subject to pi − pj ≤ aij , ∀ (i, j) ∈ A;
the proof of this closely parallels the proof of Prop. 4.1 and is outlined in
Exercise 4.3 of Chapter 4.
Along with a feasible tree T , a simplex method typically maintains a
price vector p = (p1, . . . , pN ) such that
pi − pj = aij , ∀ (i, j) ∈ T.
This price vector is obtained using the following steps:
(a) Fix a node r, called the root of the tree, and set pr to some arbitrary
scalar value.
(b) For each node i, let Pi be the unique simple path of T starting at the
root node r and ending at i.
(c) Define pi by
pi = pr −
∑
(m,n)∈P+
i
amn +
∑
(m,n)∈P−
i
amn, (5.4)
where P+i and P
−
i are the sets of forward and backward arcs of Pi,
respectively.
To see that with this definition of pi we have pi −pj = aij for all (i, j) ∈ T ,
write Eq. (5.4) for nodes i and j, subtract the two equations, and note that
the paths Pi and Pj differ by just the arc (i, j).
For an equivalent construction method, select pr arbitrarily, set the
prices of the outward neighbors j of r with (r, j) ∈ T to pj = pr − arj and
the prices of the inward neighbors j of r with (j, r) ∈ T to pj = pr + ajr,
and then repeat the process with the neighbors j replacing r. Figure 5.4
gives an example.
It can be seen from Eq. (5.4), that for each pair of nodes i and j, the
price difference (pi − pj) is independent of the arbitrarily chosen root node
price pr; write Eq. (5.4) for node i and for node j, and subtract. Therefore,
for each arc (i, j), the scalar
rij = aij + pj − pi, (5.5)
called the reduced cost of the arc, is uniquely defined by the spanning tree
T . By the definition of p, we have
rij = 0, ∀ (i, j) ∈ T,
178 Simplex Methods Chap. 5
13
2
5
4
6
7
p   = -1
p   = 0
p   = 0
Root
Cost = 1
Cost = 2
Cost = -2
Cost = 1
Cost = -1
Cost = -1
1
2
p   = 23
p   = 24
p   = 15
6
p   = 27
Figure 5.4: Illustration of the prices associated with a spanning tree. The root
is chosen to be node 1, and its price is arbitrarily chosen to be 0. The other node
prices are then uniquely determined by the requirement pi − pj = aij for all arcs
(i, j) of the spanning tree.
so if in addition we have
rij ≥ 0, ∀ (i, j) /∈ T,
the pair (x, p) satisfies the CS conditions (5.2) and (5.3). It then follows
from Prop. 4.1 of Section 4.3 (more precisely, from the version of that
proposition, given in Exercise 4.3 of Chapter 4, that applies to the problem
with only nonnegativity constraints) that x is an optimal primal solution
and p is an optimal dual solution.
If on the other hand, we have
ri j < 0 (5.6)
for some arc e = (i, j) not in T , then we claim that the unique simple
cycle C formed by T and the arc (i, j) [cf. Lemma 1.1(c)] has negative cost.
Indeed, the cost of C can be written in terms of the reduced costs of its
arcs as∑
(i,j)∈C+
aij −
∑
(i,j)∈C−
aij =
∑
(i,j)∈C+
(
aij + pj − pi
)
−
∑
(i,j)∈C−
(
aij + pj − pi
)
=
∑
(i,j)∈C+
rij −
∑
(i,j)∈C−
rij .
(5.7)
Since rij = 0 for all (i, j) ∈ T [see Eq. (5.5)], and (i, j) is a forward arc of
C by convention, we have
Cost of C = ri j
Sec. 5.1 Main Ideas in Simplex Methods 179
Reduced Cost = Cycle Cost = -3
Cost = 1
Cost = -1
Cost = 0
Cost = 1
Cost = 1
Cost = 1
Cost = -2
Price = 3 Price = -1
Price = 0
Price = 0Price = 4
Price = 2
Price = 1
In-Arc
Figure 5.5: Obtaining a negative
cost cycle in a simplex method. All
the tree arcs of the cycle have zero
reduced cost, so the reduced cost of
the in-arc is also the cost of the cy-
cle, based on the calculation of Eq.
(5.7). Thus, if the in-arc is chosen
to have negative reduced cost, the
cost of the cycle is also negative.
which is negative by Eq. (5.6); see Fig. 5.5.
The role of the price vector p associated with a feasible tree now
becomes clear. By checking the sign of the reduced cost
rij = aij + pj − pi
of all arcs (i, j) not in T , we will either verify optimality if rij is nonnegative
for all (i, j), or else we will obtain a negative cost cycle by discovering an
arc (i, j) for which rij is negative. The latter arc may be used as the in-arc
to enter the tree of the next iteration.
There are a number of methods, also called pivot rules, for selecting
the in-arc. For example, one may search for an in-arc with most negative
reduced cost; this rule requires a lot of computation – a comparison of rij
for all arcs (i, j) not in the current tree. A simpler alternative is to search
the list of arcs not in the tree and to select the first arc with negative
reduced cost. Most practical simplex codes use an intermediate strategy.
They maintain a candidate list of arcs, and at each iteration they search
through this list for an arc with most negative reduced cost; in the process,
arcs with nonnegative reduced cost are deleted from the list. If no arc
in the candidate list has a negative reduced cost, a new candidate list is
constructed. One way to do this is to scan the full arc list and enter in the
candidate list all arcs with negative reduced cost, up to the point where
the candidate list reaches a maximum size, which is chosen heuristically.
This procedure can also be used to construct the initial candidate list.
5.1.2 Obtaining the Out-Arc
Let T be a feasible tree generated by a simplex method with corresponding
flow vector x and price vector p which are nonoptimal. Suppose that we
have chosen the in-arc e and we have obtained the corresponding negative
cost cycle C formed by T and e. There are two possibilities:
180 Simplex Methods Chap. 5
(a) All arcs of C are oriented like e, that is, C− is empty. Then C is
a forward cycle with negative cost, indicating that the problem is
unbounded. Indeed, since C− is empty, we can increase the flows
of the arcs of C by an arbitrarily large common increment, while
maintaining feasibility of x. The primal cost function changes by an
amount that is equal to the cost of C for each unit flow change along
C. Since C has negative cost, we see that the primal cost can be
decreased to arbitrarily small (i.e. large negative) values.
(b) The set C− of arcs of C with orientation opposite to that of e is
nonempty. Then
δ = min
(i,j)∈C−
xij (5.8)
is the maximum increment by which the flow of all arcs of C+ can be
increased and the flow of all arcs of C− can be decreased, while still
maintaining feasibility. A simplex method computes δ and changes
the flow vector from x to x, where
xij =
 xij if (i, j) /∈ C,xij + δ if (i, j) ∈ C+,
xij − δ if (i, j) ∈ C−.
(5.9)
Any arc e = (i, j) ∈ C− that attains the minimum in the equation
δ = min(i,j)∈C− xij satisfies xij = 0 and can serve as the out-arc; see
Fig. 5.6. (A more specific rule for selecting the out-arc will be given
later.) The new tree is
T = T + e − e (5.10)
and its associated basic flow vector is x, given by Eq. (5.9).
Figures 5.7 and 5.8 illustrate the method for some simple examples.
Candidate
Out-Arc
In-Arc
Flow = 1
Flow = 2
Flow = 1Flow = 1
Flow = 0
Flow = 3
Flow = 2
Flow = 2
Flow = 2
Candidate
Out-Arc
Figure 5.6: Choosing the out-arc
in a simplex method. The in-arc,
shown at the bottom, closes a cycle
C. The orientation of C is in the
direction of the in-arc. There are
three arcs in C−, and they define
the flow increment
δ = min
(i,j)∈C−
xij = 2.
Out of these arcs, the two attaining
the minimum are the candidates for
out-arc, as shown.
Sec. 5.1 Main Ideas in Simplex Methods 181
(a)
1 1
cost = 1
cost = 0 cost = 0
1
2
3
(c)
1
flow =1 flow = 1
1
2
3
p  = 0
1 3
p  = 0
2
p  = 0
reduced cost = 1
(b)
1 1
flow = 1
flow = 0
1
2
3
p  = -1
3
p  = -1
2
p  = 0
1
In-arc
reduced cost = -1
1
Figure 5.7: Illustration of a simplex method for the problem described in figure
(a). The starting tree consists of arcs (1, 3) and (2, 3), and the corresponding
flows and prices are as shown in figure (b). Arc (1, 2) has negative reduced cost
and is thus eligible to be an in-arc. Arc (1, 3) is the only arc eligible to be the
out-arc. The new tree is shown in figure (c). The corresponding flow is optimal
because the reduced cost of arc (1, 3) is positive.
(b)(a)
p  = 1
3
p  = 1
2
1
flow = 1
flow = 0
1 2
3
p  = 0
1
In-arc
reduced cost = -1
cost = -1
1 1
cost = 0 cost = 0
1 2
3
1
Figure 5.8: Illustration of a simplex method for the problem described in figure
(a); this is an unbounded problem because the cycle (1, 2, 3, 1) has negative cost.
The starting tree consists of arcs (1, 2) and (2, 3), and the corresponding flows
and prices are as shown in figure (b). Arc (3, 1) has negative reduced cost and
is thus eligible to be an in-arc. However, all the arcs of the corresponding cycle
have the same orientation, so the problem is declared to be unbounded.
Note that the price vector p associated with the new tree T can be
conveniently obtained from p as follows: Let e = (i, j) be the in-arc and
let e be the out-arc. If we remove e from T we obtain two trees, Ti and
Tj , containing the nodes i and j, respectively; see Fig. 5.9. Then it is seen
182 Simplex Methods Chap. 5
from the definition (5.4) that a price vector p associated with T is given by
pi =
{
pi if i ∈ Ti,
pi − ri j if i ∈ Tj ,
(5.11)
where
ri j = ai j + p j − p i
is the reduced cost of the in-arc (i, j). Thus, to update the price vector, one
needs to increase the prices of the nodes in Tj by the common increment
(−ri j). We may also use any other price vector, obtained by adding the
same constant to all the prices pi defined above; it will simply correspond
to a different price for the root node. The formula
pi =
{
pi + ri j if i ∈ Ti,
pi if i ∈ Tj ,
(5.12)
which involves a decrease of the prices of the nodes in Ti, is useful in some
implementations.
(a) (b)
Out-Arc e
In-Arc ( i, j )
_
i
_
j
T
i
_
T _
j
Root
Out-Arc e
In-Arc ( i, j )
i
T
j
Root
j
_ _
_
__
_ _
Figure 5.9: Component trees T
i
and T
j
, obtained by deleting the out-arc e from
T , where e = (i, j) is the in-arc; these are the components that contain i and j,
respectively. Depending on the position of the out-arc e, the root node may be
contained in T
i
as in figure (a), or in T
j
as in figure (b).
Note that if the flow increment δ = min(i,j)∈C− xij [cf. Eq. (5.8)] is
positive, then the cost corresponding to x will be strictly smaller than the
cost corresponding to x (by δ times the cost of the cycle C). Thus, when
Sec. 5.1 Main Ideas in Simplex Methods 183
δ > 0, a simplex method will never reproduce x and the corresponding tree
T in future iterations.
On the other hand, if δ = 0, then x = x and the pivot is degener-
ate. In this case there is no guarantee that the tree T will not be repeated
after several degenerate iterations with no interim improvement in the pri-
mal cost. We thus need to provide a mechanism that precludes this from
happening.
5.1.3 Dealing with Degeneracy
Suppose that the feasible trees generated by a simplex method are all dis-
tinct (which is true in particular when all pivots are nondegenerate). Then,
since the number of distinct feasible trees is finite, the method will eventu-
ally terminate. Upon termination, there are two possibilities:
(a) The final flow and price vectors are primal and dual optimal, respec-
tively.
(b) The problem is shown to be unbounded because at the final iteration,
the cycle closed by the current tree and the in-arc e has no arc with
orientation opposite to that of e.
Unfortunately, if the tree sequence is not generated with some care,
there is no guarantee that a tree will not be infinitely repeated. To rule
out this possibility, thereby ensuring termination of the method, we will
use feasible trees with a special property called strong feasibility . We will
make sure that the initial tree has this property, and we will choose the
out-arc in a way that the property is maintained by the algorithm.
Let us fix the root node r used to compute the price vectors associated
with feasible trees. Given a feasible tree T , we say that arc (i, j) ∈ T is
oriented away from the root if the unique simple path of T from the root
to j passes through i. A feasible tree T with corresponding flow vector x
is said to be strongly feasible if every arc (i, j) of T with xij = 0 is oriented
away from the root. Figure 5.10 illustrates strongly feasible trees.
The following proposition motivates the use of strongly feasible trees.
Proposition 5.2: If the feasible trees generated by a simplex method
are all strongly feasible, then these trees are distinct.
Proof: With each feasible tree T , with corresponding basic feasible vector
x and price vector p, we associate the two scalars
c(T ) =
∑
(i,j)∈A
aijxij ,
184 Simplex Methods Chap. 5
Root
Flow > 0
(a) Not Strongly Feasible
Flow > 0
Flow > 0
Flow > 0
Flow > 0
Flow > 0Flow > 0
Flow > 0
Flow = 0
Not oriented away
from  the root
Root
(b) Strongly  Feasible
Flow > 0
Flow > 0
Flow > 0
Flow > 0
Flow > 0
Flow > 0
Flow > 0
Flow > 0
Flow = 0
Oriented away 
from the root
Figure 5.10: Illustration of a strongly feasible tree. The tree in (a) is not strongly
feasible because the arc with zero flow on the tree is not oriented away from the
root. The tree in (b) is strongly feasible. Note that these two trees are obtained
from the strongly feasible tree in Fig. 5.6 by choosing a different out-arc.
w(T ) =
∑
i∈N
(
pr − pi
)
,
where r is the root node. [The price differences pr − pi are uniquely deter-
mined by T according to
pr − pi =
∑
(m,n)∈P+
i
amn −
∑
(m,n)∈P−
i
amn
[see Eq. (5.4)], so w(T ) is uniquely determined by T . Note that, w(T ) may
be viewed as the “aggregate length” of T ; it is the sum of the lengths of the
paths Pi from the root to the nodes i along the tree T , where the length
of an arc (m, n) is amn or −amn depending on whether (m, n) is or is not
oriented away from the root, respectively.]
We will show that if T and T = T + e − e are two successive feasible
trees generated by the simplex method, then either c(T ) < c(T ) or else
c(T ) = c(T ) and w(T ) < w(T ). This proves that no tree can be repeated.
Indeed, if the pivot that generates T from T is nondegenerate, we
have c(T ) < c(T ), and if it is degenerate we have c(T ) = c(T ). In the
former case the result is proved, so assume the latter case holds, and let
e = (i, j) be the in-arc. Then after the pivot, e still has zero flow, and since
T is strongly feasible, e must be oriented away from the root node r. This
implies that r belongs to the subtree Ti, and by Eq. (5.11) we have
w(T ) = w(T ) + |Tj |ri j ,
where ri j is the reduced cost of e, and |Tj | is the number of nodes in the
subtree Tj . Since ri j < 0, it follows that w(T ) < w(T ). Q.E.D.
Sec. 5.1 Main Ideas in Simplex Methods 185
The next proposition shows how to select the out-arc in a simplex
iteration in order to maintain strong feasibility of the generated trees.
Proposition 5.3: Let T be a strongly feasible tree generated by a
simplex method, let e = (i, j) be the in-arc, let C be the cycle formed
by T and e, and suppose that C− is nonempty. Let δ = min(i,j)∈C− xij ,
and let Ĉ be the set of candidate out-arcs, that is, the set
Ĉ =
{
(i, j) ∈ C− | xij = δ
}
.
Define the join of C as the first node of C that lies on the unique
simple path of T that starts from the root and ends at i (see Fig. 5.11).
Suppose that the out-arc e is chosen to be the arc of Ĉ encountered first
as C is traversed in the forward direction (the direction of e) starting
from the join node. Then the next tree T = T + e − e generated by
the method is strongly feasible.
Flow = 1
Flow = 2 Flow = 3
Candidate Out-Arc
Flow = 0
First Encountered 
Candidate Out-Arc
Join
j
_
i
_
(b) Degenerate Pivot
Flow = 1 Flow = 0
In-Arc e
_
C
First Encountered
Candidate Out-Arc
Flow = 1
Flow = 2 Flow = 3
Flow = 2
Candidate Out-Arc
Flow = 2
Candidate Out - Arc
Join
j
_
In-Arc e
_
Flow = 1
(a) Nondegenerate Pivot
C
i
_
Figure 5.11: Maintaining a strongly feasible tree in a simplex method. Suppose
that the in-arc e = (i, j) is added to a strongly feasible T , closing the cycle C.
Let Ĉ be the set of candidates for out-arc (the arcs of C− attaining the minimum
in δ = min(i,j)∈C− xij), and let e be the out-arc. The arcs of T with zero flow
will be the arcs of Ĉ − e together with e if the pivot is degenerate. By choosing
as out-arc the first encountered arc of Ĉ as C is traversed in the direction of e
starting from the join, all of these arcs will be oriented away from the join and
also from the root, so strong feasibility is maintained. Note that if the pivot is
degenerate as in (b), then all arcs of Ĉ will be encountered after e (by strong
feasibility of T ), so the out-arc e must be encountered after e. Thus, the in-arc e
will be oriented away from the root in the case of a degenerate pivot, as required
for strong feasibility of T .
186 Simplex Methods Chap. 5
Proof: We first note that the flow or orientation relative to the root of the
arcs of T which are not in C will not change during the simplex iteration.
Therefore, to check strong feasibility of T , we need only be concerned with
the arcs of C + e− e for which xij = 0. These will be the arcs of Ĉ − e and
possibly arc e (in the case δ = 0). By choosing e to be the first encountered
arc of Ĉ, all of the arcs of Ĉ − e will be encountered after e, and following
the pivot, they will be oriented away from the join and therefore also from
the root. If δ = 0, the arcs (i, j) of Ĉ satisfy xij = 0, so by strong feasibility
of T , all of them, including e, must be encountered after e as C is traversed
in the direction of e starting from the join. Therefore, e will also be oriented
away from the root following the pivot. Q.E.D.
5.2 THE BASIC SIMPLEX ALGORITHM
In this section we will focus on a particular simplex algorithm based on the
ideas of the preceding section. This algorithm may be viewed as the basic
form of the simplex method for the minimum cost flow problem, and will
be shown to have solid theoretical properties.
At the beginning of each iteration of the algorithm we have a strongly
feasible tree T , an associated basic flow vector x such that
xij = 0, ∀ (i, j) /∈ T,
and a price vector p such that
rij = aij + pj − pi = 0, ∀ (i, j) ∈ T.
The iteration has three possible outcomes:
(a) We will verify that x and p are primal and dual optimal, respectively.
(b) We will determine that the problem is unbounded.
(c) We will obtain by the method of Prop. 5.3 a strongly feasible tree
T = T + e − e, differing from T by the in-arc e and the out-arc e.
Simplex Iteration
Select an in-arc e = (i, j) /∈ T such that
ri j = ai j + p j − p i < 0.
Sec. 5.2 The Basic Simplex Algorithm 187
(If no such arc can be found, terminate; x is primal optimal and p
is dual optimal.) Consider the cycle C formed by T and e. If C− is
empty, terminate (the problem is unbounded); else, obtain the out-arc
e ∈ C− as described in Prop. 5.3.
5.2.1 Termination Properties of the Simplex Method
We now collect the facts already proved into a proposition that also deals
with the integrality of the solutions obtained.
Proposition 5.4: Suppose that the simplex method just described
is applied to the minimum cost flow problem with nonnegativity con-
straints, starting with a strongly feasible tree.
(a) If the problem is not unbounded, the method terminates with
an optimal primal solution x and an optimal dual solution p,
and the optimal primal cost is equal to the optimal dual cost.
Furthermore, if the supplies si are all integer, the optimal pri-
mal solution x is integer. Also, if the starting price of the root
node and the cost coefficients aij are all integer, the optimal dual
solution p is integer.
(b) If the problem is unbounded, the method verifies this after a
finite number of iterations.
Proof: (a) The trees generated by the method are strongly feasible, and by
Prop. 5.2 these trees are all distinct, so the method terminates. Termina-
tion can only occur with either an optimal pair (x, p) or with the indication
that the problem is unbounded. Thus, if the problem is not unbounded,
the only possibility is termination with an optimal pair (x, p). Since upon
termination x and p satisfy complementary slackness, the equality of the
optimal primal and dual costs follows from Prop. 4.1 in Section 4.3. Also,
if the supplies si are all integer, from Prop. 5.1 it follows that all basic
flow vectors are integer, including the one obtained at termination. If the
starting price of the root node and the cost coefficients aij are all integer, it
can be checked that all operations of the algorithm maintain the integrality
of p.
(b) If the problem is unbounded, there is no optimal solution, so the simplex
method cannot terminate with an optimal pair (x, p). The only other pos-
sibility is that the method terminates with an indication that the problem
is unbounded. Q.E.D.
188 Simplex Methods Chap. 5
5.2.2 Initialization of the Simplex Method
In the absence of an apparent choice for an initial strongly feasible tree,
one may use the so called big-M method . In this method, some artificial
variables are introduced to simplify the choice of an initial basic solution,
but the cost coefficient M for these variables is chosen large enough so that
the optimal solutions of the problem are not affected. The big-M method
is also useful in problems where the graph is not connected and therefore
it has no spanning tree at all.
Original Network
Artificial Node 0
1
s   = 01
2
s   = 22
3
s   = -13
4
s   = 34
5
s   = - 45
Artificial Arcs with
Large Cost M
Figure 5.12: Artificial arcs used in the big-M method to modify the problem so
as to facilitate the choice of an initial strongly feasible tree.
In the big-M method, we modify the problem by introducing an extra
node, labeled 0 and having zero supply s0 = 0, together with a set of
artificial arcs A consisting of an arc (i, 0) for each node i with si > 0, and
an arc (0, i) for each node i with si ≤ 0; see Fig. 5.12. The cost coefficient
of all these arcs is taken to be a scalar M , and its choice will be discussed
shortly. We thus arrive at the following problem, referred to as the big-M
version of the original problem:
minimize
∑
(i,j)∈A
aijxij + M
 ∑
(i,0)∈A
xi0 +
∑
(0,i)∈A
x0i

subject to
∑
{j|(i,j)∈A∪A}
xij −
∑
{j|(j,i)∈A∪A}
xji = si, ∀ i ∈ N ∪ {0},
0 ≤ xij , ∀ (i, j) ∈ A ∪A.
The artificial arcs constitute a readily available initial spanning tree
for the big-M version; see Fig. 5.12. It can be seen that the corresponding
Sec. 5.2 The Basic Simplex Algorithm 189
basic flow vector is given by
xi0 = si, ∀ i with si > 0,
x0i = −si, ∀ i with si ≤ 0,
xij = 0, ∀ (i, j) ∈ A,
and is therefore feasible. Let us choose the root to be the artificial node 0.
By construction, the artificial arcs that carry zero flow are then oriented
away from the root, so the tree is strongly feasible.
The cost M of the artificial arcs should be taken to be large enough so
that these arcs will carry zero flow at every optimal solution of the big-M
version. In this case, the flows of the nonartificial arcs define an optimal
solution of the original problem. The following proposition quantifies the
appropriate level of M for this to happen, and collects a number of related
facts.
Proposition 5.5: Consider the minimum cost flow problem with non-
negativity constraints (referred to as the original problem), and con-
sider also its big-M version. Suppose that
2M >
∑
(i,j)∈P+
aij −
∑
(i,j)∈P−
aij (5.13)
for all simple paths P of the original problem graph, where P+ and
P− are the sets of forward and backward arcs of P , respectively. Then:
(a) If the original problem is feasible but not unbounded, the big-M
version has at least one optimal solution, and each of its solutions
is of the form
xij =
{
xij if (i, j) ∈ A,
0 if (i, j) ∈ A,
(5.14)
where x is an optimal solution of the original. Furthermore, every
optimal solution x of the original problem gives rise to an optimal
solution x of the big-M version via the preceding relation.
(b) If the original problem is unbounded, the big-M version is also
unbounded.
(c) If the original problem is infeasible, then in every feasible solution
of the big-M version some artificial arc carries positive flow.
Proof: (a) We first note that the big-M version cannot be unbounded
unless the original problem is. To prove this, we argue by contradiction.
If the big-M version is unbounded and the original problem is not, there
190 Simplex Methods Chap. 5
would exist a simple forward cycle with negative cost in the big-M version.
This cycle cannot consist of arcs of A exclusively, since the original is not
unbounded. On the other hand, if the cycle consisted of the arcs (m, 0)
and (0, n), and a simple path of the original graph, then by the condition
(5.13) the cycle would have positive cost, arriving at a contradiction.
Having proved that the big-M version is not unbounded, we now
note that, by Prop. 5.4(a), the simplex method starting with the strongly
feasible tree of all the artificial arcs will terminate with optimal primal and
dual solutions of the big-M version. Thus, optimal solutions of the big-
M version exist, and for every optimal solution x of the form (5.14), the
corresponding vector x = {xij | (i, j) ∈ A} with xij = xij for all (i, j) ∈ A
is an optimal solution of the original problem.
To prove that all optimal solutions x of the big-M version are of
the form (5.14), we argue by contradiction. Suppose that x is an optimal
solution such that some artificial arcs carry positive flow. Let
N+ = {m | sm > 0, xm0 > 0},
N− = {n | sn ≤ 0, x0n > 0}.
We observe that N+ and N− must be nonempty and that there is no
simple path P that starts at some m ∈ N+, ends at some n ∈ N−, and
is unblocked with respect to x; such a path, together with arcs (m, 0) and
(0, n), would form an unblocked simple cycle, which would have negative
cost in view of condition (5.13). Consider now the flow vector x = {xij |
(i, j) ∈ A} with xij = xij for all (i, j) ∈ A. Then, there is no path of
the original problem graph (N ,A) that starts at a node of N+, ends at a
node of N−, and is unblocked with respect to x. By using a very similar
argument as in the proof of Prop. 3.1, we can show (see Exercise 3.11 in
Ch. 3) that there must exist a saturated cut [S,N −S] such that N+ ⊂ S,
N− ⊂ N−S. The capacity of this cut is equal to the sum of the divergences
of the nodes i ∈ S,
∑
i∈S
yi =
∑
i∈S
 ∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji
 ,
which is also equal to∑
i∈S
(
si − xi0
)
=
∑
i∈S
si −
∑
i∈N+
xi0 <
∑
i∈S
si.
On the other hand, if the original problem is feasible, the capacity of any
cut [S,N − S] cannot be less than
∑
i∈S si, so we obtain a contradiction.
Finally, let x be an optimal solution of the original problem, and let
x be given by Eq. (5.14). We will show that x is optimal for the big-M
Sec. 5.2 The Basic Simplex Algorithm 191
version. Indeed, every simple cycle that is unblocked with respect to x in
the big-M version either consists of arcs in A and is therefore unblocked
with respect to x in the original, or else consists of the arcs (m, 0) and
(0, n), and a simple path P that starts at n and ends at m. In the former
case, the cost of the cycle is nonnegative, since x is optimal for the original
problem; in the latter case, the cost of the cycle is positive by condition
(5.13) (with the path P being the reverse of path P ). Hence, x is optimal
for the big-M version.
(b) Note that every feasible solution x of the original problem defines a
feasible solution x of equal cost in the big-M version via Eq. (5.14). There-
fore, if the cost of the original is unbounded from below, the same is true
of the big-M version.
(c) Observe that any feasible solution of the big-M version having zero
flow on the artificial arcs defines a feasible solution x of the original via Eq.
(5.14). Q.E.D.
Note that to satisfy the condition (5.13), it is sufficient to take
M >
(N − 1)C
2
,
where C is the arc cost range C = max(i,j)∈A |aij |. Note also that if M does
not satisfy the condition (5.13), then the big-M version may be unbounded,
even if the original problem has an optimal solution (Exercise 5.7). Many
practical simplex codes use an adaptive strategy for selecting M , whereby a
moderate value of M is used initially, and this value is gradually increased
if positive flows on the artificial arcs persist.
By combining the results of the preceding two propositions, we obtain
the following:
Proposition 5.6: Assume that the minimum cost flow problem with
nonnegativity constraints is feasible and is not unbounded. Then:
(a) There exists an optimal primal solution and an optimal dual
solution, and the optimal primal cost is equal to the optimal
dual cost.
(b) If the supplies si are all integer, there exists an optimal primal
solution which is integer.
(c) If the cost coefficients aij are all integer, there exists an optimal
dual solution which is integer.
Proof: Apply the simplex method to the big-M version with the initial
strongly feasible tree of all the artificial arcs, and with M sufficiently large
192 Simplex Methods Chap. 5
to satisfy condition (5.13). Then, by Prop. 5.5, the big-M version has
optimal solutions, so by Prop. 5.4 the simplex method will provide an
optimal pair (x, p), with x integer if the supplies are integer, and p integer
if the cost coefficients are integer. By Prop. 5.5, the vector x defined by
xij = xij , for all (i, j) ∈ A will be an optimal solution of the original
problem, while the price vector p defined by pi = pi, for all i ∈ N will
satisfy the CS conditions together with x. Hence, p will be an optimal dual
solution. Q.E.D.
A Shortest Path Example
Consider a single origin/all destinations shortest path problem involving
the graph of Fig. 5.13. We will use this example to illustrate the simplex
method and some of its special properties when applied to shortest path
problems. The corresponding minimum cost flow problem is
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(1,j)∈A}
x1j −
∑
{j|(j,1)∈A}
xj1 = 3,
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = −1, i = 2, 3, 4,
0 ≤ xij , ∀ (i, j) ∈ A.
3
3
1
1
1
11 4
3
2
s   = 31 s   = - 14
s   = - 12
s   = - 13
Figure 5.13: Example of a single ori-
gin/all destinations shortest path pro-
blem. Node 1 is the origin. The arc
lengths are shown next to the arcs.
We select as root the origin node 1. To deal with the problem of
the initial choice of a strongly feasible tree, we use a variant of the big-
M method. We introduce artificial arcs connecting the origin with each
node i 	= 1 with very large cost M , and we use as an initial tree the set of
artificial arcs with root node the origin (with this choice, there will be two
arcs connecting the origin with each of its neighbors, but this should not
cause any confusion). In the corresponding flow vector, every artificial arc
Sec. 5.2 The Basic Simplex Algorithm 193
carries unit flow, so the initial tree is strongly feasible (all arcs are oriented
away from the root).
The corresponding price vector is (0,−M,−M,−M) and the associ-
ated reduced costs of the nonartificial arcs are
r1j = a1j − M, ∀ (1, j) ∈ A,
rij = aij , ∀ (i, j) ∈ A, i 	= 1, j 	= 1.
One possible outcome of the first iteration is to select some arc (1, j) ∈ A
as in-arc, and to select the artificial arc connecting 1 and j as out-arc. The
process will then be continued, first obtaining the flow and price vectors
corresponding to the new tree, then obtaining the out-arc, then the in-arc,
etc.
Final Tree
2nd Pivot1st Pivot
1
2
3
4
p   = 0
p   = - M
p   = - 1
1
2
3
p   = - M4
Out - Arc
In - Arc
32
r    = 2 -  M
Out - Arc
In - Arc
Root
r    = 1 -  M
13
1
2
3
4
p   = 0
p   = - M
p   = - M
1
2
3
p   = - M4
3rd Pivot
p   =  - 22
Out - Arc
In - Arc
24
r    = 3 -  M
1
2
3
4
p   = 0
p   = -1
1
3
p   = - M4
p   = - 22
1
2
3
4
p   = -1
p   = 01
3
p   = - 34
Figure 5.14: A possible sequence of pivots for the simplex method. The initial
tree consists of the artificial arcs (1, 2), (1, 3), and (1, 4), each carrying one unit
of flow. The in-arc is selected to be the arc with minimum reduced cost and the
method behaves like Dijkstra’s algorithm, requiring only N − 1 (= 3) pivots.
Figures 5.14 and 5.15 show two possible sequences of pivots. The
following can be noted:
(a) Each artificial arc eventually becomes the out-arc but never becomes
the in-arc.
194 Simplex Methods Chap. 5
2nd Pivot1st Pivot
3rd Pivot 4th Pivot
(Final)
Out - Arc
In - Arc
Root
r    = 1 -  M
13
1
2
3
4
p   = 0
p   = - M
p   = - 1
1
2
3
p   = - M4
p   = - 3
2
In - Arc
Out - Arc
1
2
3
4
p   = - 1
p   = 01
3
p   = - M4
p   = - 32
p   = - 44
1
2
3
4
p   = 0
p   = - 1
1
3
Out - Arc
In - Arc
1
2
3
4
p   = 01
p   = - M2
p   = - 13
p   = - M4
Out - Arc
In - Arc
Figure 5.15: Another possible sequence of pivots for the simplex method. More
than three pivots are required, in contrast with the sequence of Fig. 5.14.
(b) In all trees, all the arcs are oriented away from the origin and carry
unit flow.
(c) In Fig. 5.14, we use the rule that the in-arc is an arc with minimum
reduced cost. As a result, there are exactly N − 1 (= 3) pivots, and
each time the out-arc is an artificial arc. In this case the simplex
method works exactly like Dijkstra’s algorithm, permanently setting
the label of one additional node with every pivot; here, node labels
should be identified with the negative of node prices.
It can be shown that observations (a) and (b) above hold in general
for the simplex method applied to feasible shortest path problems, and
observation (c) also holds in general provided aij ≥ 0 for all arcs (i, j).
The proof of this is left as Exercise 5.13 for the reader.
The simplex method can also be used effectively to solve the all-pairs
shortest path problem. In particular, one may first use the simplex method
to solve the shortest path problem for a single origin, say node 1, and then
modify the final tree T1 to obtain an initial tree T2 for applying the simplex
method with another origin, say node 2. This can be done by deleting the
unique arc of T1 that is incoming to node 2, and replacing it with an
artificial arc from 2 to 1 that has a very large length; see Fig. 5.16.
Sec. 5.3 Extension to Problems with Upper and Lower Bounds 195
Artificial Arc with 
Large Length
Tree T   
Rooted at 1
1
1
2
2Tree T   
Rooted at 2
1
2
Figure 5.16: Obtaining an initial tree T2 for the simplex method applied to the
shortest path problem with origin 2, from the final tree T1 of the simplex method
applied for origin 1. We delete the unique arc of T1 that is incoming to node 2,
and replace it with an artificial arc from 2 to 1 that has a very large length.
5.3 EXTENSION TO PROBLEMS WITH UPPER AND LOWER
BOUNDS
In this section, we consider the extension of the simplex method of the
preceding section to the general minimum cost flow problem that involves
upper and lower bounds
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
bij ≤ xij ≤ cij , ∀ (i, j) ∈ A.
(5.15)
To simplify the presentation, we assume that bij < cij for all arcs (i, j); any
arc (i, j) with bij = cij can be eliminated, and its flow, which is equal to
the common bound, can be incorporated into the supplies si and sj . A nice
aspect of the problem is that we don’t have to worry about unboundedness,
since all arc flows are constrained to lie in a bounded interval.
The extension of the simplex method to the problem with upper and
lower bounds is straightforward, and we will simply state the algorithm
196 Simplex Methods Chap. 5
and the corresponding results without much elaboration. In fact, one may
derive the simplex method for this problem by converting it to the mini-
mum cost flow problem with nonnegativity constraints (cf. the discussion of
Section 4.2), by applying the simplex method of the preceding section, and
by appropriately streamlining the computations. We leave the verification
of this as Exercise 5.15 for the reader.
The method uses at each iteration a spanning tree T . Only arcs of T
can have flows that are neither at the upper bound nor at the lower bound.
However, to uniquely associate a basic flow vector with T , we must also
specify for each arc (i, j) /∈ T whether xij = bij or xij = cij . Thus, the
simplex method maintains a triplet
(T, L, U),
where
T is a spanning tree,
L is the set of arcs (i, j) /∈ T with xij = bij ,
U is the set of arcs (i, j) /∈ T with xij = cij .
Such a triplet will be called a basis. It uniquely specifies a flow vector x,
called the basic flow vector corresponding to (T, L, U). In particular, if an
arc (i, j) belongs to T and separates T into the subtrees Ti and Tj , we have
xij =
∑
n∈Ti
sn −
∑
{(m,n)∈L|m∈Ti,n∈Tj}
bmn −
∑
{(m,n)∈U |m∈Ti,n∈Tj}
cmn
+
∑
{(m,n)∈L|m∈Tj ,n∈Ti}
bmn +
∑
{(m,n)∈U |m∈Tj ,n∈Ti}
cmn.
If x is feasible, then the basis (T, L, U) is called feasible.
Similar to the preceding section, we fix a root node r throughout
the algorithm. A basis (T, L, U) specifies a price vector p using the same
formula as in the preceding section:
pi = pr −
∑
(m,n)∈P+
i
amn +
∑
(m,n)∈P−
i
amn, ∀ i ∈ N ,
where Pi is the unique simple path of T starting at the root node r and
ending at i, and P+i and P
−
i are the sets of forward and backward arcs of
Pi, respectively.
We say that the feasible basis (T, L, U) is strongly feasible if all arcs
(i, j) ∈ T with xij = bij are oriented away from the root and if all arcs
(i, j) ∈ T with xij = cij are oriented toward the root (that is, the unique
simple path from the root to i passes through j).
Sec. 5.3 Extension to Problems with Upper and Lower Bounds 197
Given the strongly feasible basis (T, L, U) with a corresponding flow
vector x and price vector p, an iteration of the simplex method produces
another strongly feasible basis (T , L, U) as follows.
Simplex Iteration for Problems with Upper and Lower Bounds
Find an in-arc e = (i, j) /∈ T such that either
rij < 0 if e ∈ L
or
rij > 0 if e ∈ U.
(If no such arc can be found, x is primal optimal and p is dual optimal.)
Let C be the cycle closed by T and e. Define the forward direction of
C to be the same as the one of e if e ∈ L and opposite to e if e ∈ U
(that is, e ∈ C+ if e ∈ L and e ∈ C− if e ∈ U). Also let
δ = min
{
min
(i,j)∈C−
{xij − bij}, min
(i,j)∈C+
{cij − xij}
}
,
and let Ĉ be the set of arcs where this minimum is obtained:
Ĉ =
{
(i, j) ∈ C− | xij − bij = δ
}
∪
{
(i, j) ∈ C+ | cij − xij = δ
}
.
Define the join of C as the first node of C that lies on the unique
simple path of T that starts from the root and ends at i. Select as
out-arc the arc e of Ĉ that is encountered first as C is traversed in
the forward direction starting from the join node. The new tree is
T = T + e − e, and the corresponding flow vector x is obtained from
x by
xij =
 xij if (i, j) /∈ C,xij + δ if (i, j) ∈ C+,
xij − δ if (i, j) ∈ C−.
Note that it is possible that the in-arc is the same as the out-arc, in
which case T is unchanged. In this case, the flow of this arc will simply
move from one bound to the other, affecting the sets L and U , and thus
affecting the basis. The proofs of the preceding section can be modified to
show that the algorithm maintains a strongly feasible tree.
The following proposition deals with the validity of the method and
the integrality of the optimal primal and dual solutions obtained. Its proof
is very similar to the one of Prop. 5.4, and is omitted.
198 Simplex Methods Chap. 5
Proposition 5.7: Assume that the simplex method is applied to the
minimum cost flow problem with upper and lower bounds, starting
from a strongly feasible tree. Then:
(a) The method terminates with an optimal primal solution x and
an optimal dual solution p.
(b) The optimal primal cost is equal to the optimal dual cost.
(c) If the supplies si and the flow bounds bij , cij are all integer, the
optimal primal solution x is integer.
(d) If the starting price of the root node and the cost coefficients aij
are all integer, the optimal dual solution p is integer.
If an initial strongly feasible tree is not readily available, we can solve
instead a big-M version of the problem with suitably large value of M .
This problem is
minimize
∑
(i,j)∈A
aijxij + M
 ∑
(i,0)∈A
xi0 +
∑
(0,i)∈A
x0i

subject to
∑
{j|(i,j)∈A∪A}
xij −
∑
{j|(j,i)∈A∪A}
xji = si, ∀ i ∈ N ∪ {0},
bij ≤ xij ≤ cij , ∀ (i, j) ∈ A,
0 ≤ xi0 ≤ si, ∀ i with si > 0,
0 ≤ x0i ≤ si, ∀ i with si ≤ 0,
where
si = si −
∑
{j|(i,j)∈A}
bij +
∑
{j|(j,i)∈A}
bji,
si = −si +
∑
{j|(i,j)∈A}
bij −
∑
{j|(j,i)∈A}
bji.
The initial strongly feasible tree consists of the artificial arcs. The corre-
sponding basic flow vector x is given by xij = bij for all (i, j) ∈ A, xi0 = si,
for all i with si > 0, and x0i = −si, for all i with si ≤ 0.
Similar to the case of the problem with nonnegativity constraints (cf.
Prop. 5.6), we obtain the following.
Proposition 5.8: Assume that the minimum cost flow problem with
upper and lower bounds is feasible. Then:
Sec. 5.4 Implementation Issues 199
(a) There exists an optimal primal solution and an optimal dual
solution, and the optimal primal cost is equal to the optimal
dual cost.
(b) If the supplies si and the flow bounds bij , cij are all integer, there
exists an optimal primal solution which is integer.
(c) If the cost coefficients aij are all integer, there exists an optimal
dual solution which is integer.
5.4 IMPLEMENTATION ISSUES
To implement a network optimization algorithm efficiently it is essential to
exploit the graph nature of the problem using appropriate data structures.
There are two main issues here:
(a) Representing the problem in a way that facilitates the application of
the algorithm.
(b) Using additional data structures that expedite the operations of the
algorithm.
For simplex methods, the appropriate representations of the problem
tend to be quite simple. However, additional fairly complex data structures
are needed to implement efficiently the various operations related to flow
and price computation, and tree manipulation. This is quite contrary to
what happens in the methods to be discussed in the next two chapters,
where the appropriate problem representations are quite complex but the
additional data structures are simple.
Problem Representation for Simplex Methods
For concreteness, consider the following problem with zero lower arc flow
bounds
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
0 ≤ xij ≤ cij , ∀ (i, j) ∈ A.
This has become the standard form for commonly available minimum cost
flow codes. As discussed in Section 4.2, a problem with nonzero lower arc
200 Simplex Methods Chap. 5
flow bounds bij can be converted to one with nonnegativity constraints by
using a flow translation (replacing each xij by xij − bij and appropriately
adjusting cij , si, and sj).
One way to represent this problem, which is the most common in
simplex codes, is to use the following four arrays of length A
START (a): The start node of arc a,
END(a): The end node of arc a,
COST (a): The cost coefficient of arc a,
CAPACITY (a): The upper flow bound of arc a,
and the following array of length N
SUPPLY (i): The supply of node i.
Figure 5.17 gives an example of a problem represented in this way.
An alternative representation is to store the costs aij and the upper
flow bounds cij in two-dimensional N × N arrays (or in one-dimensional
arrays of length N2, with the elements of each row stored contiguously).
This wastes memory and requires a lot of extra overhead when the problem
is sparse (A << N2), but it may be a good choice for dense problems since
it avoids the storage of the start and end nodes of each arc.
Data Structures for Tree Operations
Taking a closer look at the operations of the simplex method, we see that
the main computational steps at each iteration are the following:
(a) Finding an in-arc with negative reduced cost.
(b) Identifying the cycle formed by the current tree and the in-arc.
(c) Modifying the flows along the cycle and obtaining the out-arc.
(d) Recalculating the node prices.
As mentioned in Section 5.1.1, most codes maintain a candidate list,
i.e., a subset of arcs with negative reduced cost. The arc with most neg-
ative reduced cost from this list is selected as the in-arc at each iteration.
The maximum size of the candidate list is set at some reasonable level
(chosen heuristically), thereby avoiding a costly search and comparison of
the reduced costs of all the arcs.
To identify the cycle and the associated flow increment at each it-
eration, simplex codes commonly use the following two arrays of length
N :
(a) PRED(i): The arc preceding node i on the unique path from the root
to i on the current tree, together with an indication (such as a plus
or a minus sign) of whether this is an incoming or outgoing arc of i.
Sec. 5.4 Implementation Issues 201
5/2
2/3
4/2
0/1
2/1
3/1 -5/10
-2/10
Cost/Capacity shown
next to each arc
0/5 11
2
2
1 4
3
2
5
0
Supply or demand shown
next to each node
ARC START END COST CAPACITY
1 1 2 5 2
2 1 3 0 1
3 2 3 4 2
4 3 2 3 1
5 2 5 -2 10
6 2 4 2 1
7 3 4 2 3
8 5 4 0 5
9 4 5 -5 10
NODE SUPPLY
1 1
2 2
3 -2
4 0
5 -1
Figure 5.17: Representation of a minimum cost flow problem in terms of the
five arrays START , END , COST , CAPACITY , and SUPPLY .
202 Simplex Methods Chap. 5
(b) DEPTH (i): The number of arcs of the unique path from the root to
i on the current tree.
The PRED array (together with the START and END arrays) is sufficient
both to represent the current tree and to construct the unique path on the
tree from any node i to any other node j. (Construct the paths from i
to the root and from j to the root, and subtract out the common portion
of these paths.) In particular, if (i, j) is the in-arc, the cycle formed by
(i, j) and the current tree could be obtained by finding the path joining i
and j in this way. By using the DEPTH array, however, the cycle can be
constructed more quickly without having to go from i to j all the way to the
root. In particular, one can start constructing the paths from i and j to the
root simultaneously, adding a new node to the path whose current end node
has greater DEPTH (ties are broken arbitrarily). The join of the cycle can
then be identified as the first encountered common node in the two paths.
The following procedure starting with the in-arc (i, j) accomplishes this.
In this procedure, i and j represent successive nodes of the paths starting
at i and j, respectively, and ending at the join of the cycle.
Identifying the Join of the Cycle Corresponding to the In-Arc
(i, j)
Set i = i, j = j.
Step 1: If DEPTH (i) ≥ DEPTH (j ), go to Step 2; else go to Step 3.
Step 2: Set i := START (PRED(i)) if PRED(i) is an incoming arc
to i, and set i := END(PRED(i)) if PRED(i) is an outgoing arc from
i. Go to Step 4.
Step 3: Set j := START (PRED(j )) if PRED(j ) is an incoming arc
to j, and set i := END(PRED(j )) if PRED(j ) is an outgoing arc from
j. Go to Step 4.
Step 4: If i = j, terminate; i is the join of the cycle corresponding to
the in-arc (i, j). Else go to Step 1.
The cycle corresponding to the in-arc consists of the arcs PRED(i)
and PRED(j ) encountered during the above procedure. With a simple
modification of the procedure, we can simultaneously obtain the out-arc
and calculate the flow increment. With little additional work, we can also
change the flow along the cycle, and update the PRED and DEPTH arrays
consistently with the new tree.
We must still provide for a mechanism to calculate efficiently the
prices corresponding to a given tree. This can be done iteratively, using
the prices of the preceding tree as shown in Section 5.1; cf. Eqs. (5.11) and
(5.12). To apply these equations, it is necessary to change the prices of
Sec. 5.5 Notes, Sources, and Exercises 203
i
j Out - Arc (i,j)
Subtree of 
Descendants of i
Root
Figure 5.18: The two subtrees obtained when the out-arc is deleted from the
current tree. The subtree containing the end node of the out-arc with larger
DEPTH (node i in the example of the figure) consists of all the descendants of
that end node.
the descendants of one of the end nodes of the out-arc, whichever has the
larger value of DEPTH ; cf. Fig. 5.18. Thus, it is sufficient to be able to
calculate the descendants of a given node i in the current tree (the nodes
whose unique path to the root passes through i). For this it is convenient
to use one more array, called THREAD . It defines a traversal order of
the nodes of the tree in depth-first fashion. To understand this order, it
is useful to think of the tree laid out in a plane, and to consider visiting
all nodes starting from the root, and going “top to bottom” and “left to
right.” An example is given in Fig. 5.19. It can be seen that every node
i appears in the traversal order immediately before all of its descendants.
Hence the descendants of i are all the nodes immediately following node i
in the traversal order up to the first node j with DEPTH (j ) ≤ DEPTH (i).
The array THREAD encodes the traversal order by storing in THREAD(i)
the node following node i; cf. Fig. 5.19. An important fact is that when
the tree changes, the THREAD array can be updated quite efficiently [with
O(N) operations]. The details, however, are too tedious and complicated
to be included here; for a clear presentation, see Chvatal [1983], p. 314.
5.5 NOTES, SOURCES, AND EXERCISES
The first specialized version of the simplex method for the transportation
problem was given by Dantzig [1951]. This method was also described and
extended to the minimum cost flow problem by Dantzig [1963]. A general
204 Simplex Methods Chap. 5
Root3
7
9
10
11
12 1314
1
2
45
6
8
Traversal Order:  3, 2, 1, 5, 4, 6, 9, 8, 7, 14, 11, 12, 13, 10   
i 1 2 3 4 5 6 7 8 9 10 11 12 13 14
THREAD(i) 5 1 2 6 4 9 14 7 8 0 12 13 10 11
Figure 5.19: Illustration of the THREAD array, which defines a depth-first
traversal order of the nodes in the tree. Given the set S of already traversed
nodes, the next node traversed is an immediate descendant of one of the nodes in
S, which has maximum value of DEPTH . For each node i, THREAD(i) defines
the successor of node i in this order (for the last node, THREAD is equal to 0).
primal cost improvement algorithm involving flow changes along negative
cost cycles was given by Klein [1967]. Strongly feasible trees and their use
in resolving degeneracy were introduced by Cunningham [1976].
The subject of pivot selection has received considerable attention in
the literature. Examples of poor performance of the simplex method are
given by Zadeh [1973a], [1973b]. The performance of various pivot rules was
studied empirically by Barr, Glover, and Klingman [1977], [1978], [1979],
Bradley, Brown, and Graves [1977], Gavish, Schweitzer, and Shlifer [1977],
Goldfarb and Reid [1977], Mulvey [1978a], [1978b], and Gibby, Glover,
Klingman, and Mead [1983]. Generally, even with the use of strongly fea-
sible trees, it is possible that the number of successive degenerate pivots
is not polynomial. Pivot rules with guaranteed polynomial upper bounds
on the lengths of sequences of degenerate pivots are given by Cunningham
[1979], and Goldfarb, Hao, and Kai [1990a]. One of the simplest such rules
maintains a strongly feasible tree and operates as follows: if the in-arc at
Sec. 5.5 Notes, Sources, and Exercises 205
some iteration has start node i, the in-arc at the next iteration must be the
outgoing arc from node (i+ k) modulo N that has minimum reduced cost,
where k is the smallest nonnegative integer such that node (i+k) modulo N
has at least one outgoing arc with negative reduced cost. For a textbook
discussion of a variety of pivot rules, see Bazaraa, Jarvis, and Sherali [1990].
Specialized simplex methods have been developed for the assignment
problem; see Barr, Glover, and Klingman [1977], Hung [1983], Balinski
[1985], [1986], and Goldfarb [1985]. For analysis and application of sim-
plex methods in shortest path and max-flow problems, see Fulkerson and
Dantzig [1955], Florian, Nguyen, and Pallottino [1981], Glover, Klingman,
Mote, and Whitman [1984], Goldfarb, Hao, and Kai [1990b], and Goldfarb
and Hao [1990].
The existence of integer solutions of the minimum cost flow prob-
lem is a fundamental property that links linear network optimization with
combinatorial optimization. This property can be generalized through the
notion of unimodular matrices. In particular, a square matrix A with in-
teger components is called unimodular if its determinant is 0, 1, or -1.
Unimodularity can be used to assert the integrality of solutions of linear
systems of equations. To see this, note that by Kramer’s rule, it follows that
if A is invertible and unimodular, then the inverse matrix A−1 has integer
components. Therefore, the system Ax = b has a unique solution x, which
is integer for every integer vector b. A rectangular matrix with integer
components is called totally unimodular if each of its square submatrices
is unimodular. Using the property of unimodular matrices just described,
we can show that all the extreme points (vertices) of polyhedra of the form
{x | Ex = s, x ≥ 0}, where E is totally unimodular and s is an integer
vector, are integer. The constraint set of the minimum cost flow problem
(with nonnegativity constraints) can be expressed as {x | Ex = s, x ≥ 0},
where s is the vector of supplies, and E is the, so-called, arc incidence
matrix of the graph. This matrix has a row for each node and a column
for each arc. The component corresponding to the ith row and a given
arc is a 1 if the arc is outgoing from i, is a -1 if the arc is incoming to
i, and is a 0 otherwise. Basic flow vectors can be identified with extreme
points of the polyhedron {x | Ex = s, x ≥ 0}, while the matrix E can be
shown to be totally unimodular (see Exercise 5.18). Thus the integrality
property of solutions of the minimum cost flow problem is a special case of
the result just mentioned about polyhedra involving unimodular matrices.
For a development of the properties of unimodular matrices we refer to
the literature (see e.g., Papadimitriou and Steiglitz [1982], Schrijver [1986],
Nemhauser and Wolsey [1988], and Murty [1992]).
The development of good implementation techniques played a cru-
cial role in the efficient use of the simplex method. Important contribu-
tions in this area include Johnson [1966], Srinivasan and Thompson [1973],
Glover, Karney, and Klingman [1974], Glover, Karney, Klingman, and
Napier [1974], Glover, Klingman, and Stutz [1974], Bradley, Brown, and
206 Simplex Methods Chap. 5
Graves [1977], and Mulvey [1978a], [1978b]. Presentations of these tech-
niques that supplement ours are given by Kennington and Helgason [1980],
Chvatal [1983], Bazaraa, Jarvis, and Sherali [1990], and Helgason and Ken-
nington [1995]. The papers by Miller, Pekny, and Thompson [1990], Pe-
ters [1990], and Barr and Hickman [1994] describe implementations of the
network simplex method in a parallel computing system. A code, called
NETFLO, which implements the simplex method for the minimum cost
flow problem is given by Kennington and Helgason [1980].
E X E R C I S E S
5.1
Consider the tree of Fig. 5.11(a).
(a) Suppose that the in-arc is (j, i) [instead of (i, j)]. Which arc should be the
out-arc?
(b) Suppose that the in-arc is the arc starting at the join and ending at j
[instead of (i, j)]. Which arc should be the out-arc in order to preserve
strong feasibility of the tree?
5.2
Consider the minimum cost flow problem with nonnegativity constraints given
in Fig. 5.20 (supplies and demands are shown next to the nodes, arc costs are
immaterial). Find all basic flow vectors and their associated trees. Specify which
of these are feasible and which are strongly feasible (the root node is node 1).
1
2
3
4
2
3 2
3
Supply or demand shown
next to each node
Figure 5.20: Graph for Exercise 5.2.
Sec. 5.5 Notes, Sources, and Exercises 207
5.3 (From a Feasible to a Basic Feasible Flow Vector)
Consider a feasible minimum cost flow problem such that the corresponding graph
is connected. Suppose we are given a feasible flow vector x. Construct an al-
gorithm that suitably modifies x to obtain a basic feasible flow vector and an
associated spanning tree. Hint : For a feasible flow vector x there are two possi-
bilities: (1) The subgraph S consisting of the set of arcs
Ax =
{
(i, j) ∈ A | xij > 0
}
and the corresponding set of incident nodes is acyclic, in which case show that x
is basic. (2) The subgraph S is not acyclic, in which case show how to construct
a feasible flow vector x′ differing from x by a simple cycle flow, and for which the
arc set Ax′ has at least one arc less than the set Ax.
5.4 (Alternative Construction of a Basic Feasible Flow Vector)
Consider the following algorithm that tries to construct a flow vector that has a
given divergence vector s, and is zero on arcs which are not in a given spanning
tree T . For any vector x, define the surplus of each node i by
gi =
∑
{j|(j,i)∈A}
xji −
∑
{j|(i,j)∈A}
xij + si.
The algorithm is initialized with x = 0. The typical iteration starts with a flow
vector x and produces another flow vector x that differs from x along a simple
path consisting of arcs of T . It operates as follows: a node i with gi > 0 and a
node j with gj < 0 are selected, and the unique path Pij that starts at i, ends
at j, and has arcs in T is constructed (if no such nodes i and j can be found the
algorithm stops). Then the flow of the forward arcs of Pij are increased by δ and
the flow of the backward arcs of Pij are decreased by δ, where δ = min{gi,−gj}.
Show that the algorithm terminates in a finite number of iterations, and that upon
termination, we have gi = 0 for all i if and only if
∑
i∈N si = 0. Hint : Show
that all the nodes with zero surplus with respect to x also have zero surplus with
respect to x. Furthermore, at least one node with nonzero surplus with respect
to x has zero surplus with respect to x.
5.5
Consider a transportation problem involving the set of sources S and the set of
sinks T (cf. Example 1.4 in Ch. 1). Suppose that there is no strict subset S of S
and strict subset T of T such that∑
i∈S
αi =
∑
j∈T
βj .
Show that for every feasible tree, the corresponding flow of every arc of the tree
is positive. Conclude that for such a problem, starting from a feasible initial tree,
degeneracy never arises in the simplex method.
208 Simplex Methods Chap. 5
5.6
Use the simplex method with the big-M initialization to solve the problem in
Fig. 5.21.
Cost shown next to each arc.
Supply or demand shown
next to each node.
5
2
6
2
- 2
3
3
2
01
2
1
2
1 4
3
2
5
0
Figure 5.21: Minimum cost flow problem with nonnegativity constraints for
Exercise 5.6.
5.7
Construct an example where M does not satisfy the condition (5.13), and the
original problem has an optimal solution, while the big-M version is unbounded.
Hint : It is sufficient to consider a graph with two nodes.
5.8
Construct an example where M satisfies the condition (5.13), and the original
problem is infeasible, while the big-M version is unbounded. Hint : Consider
problems that are infeasible and also contain a simple forward cycle of negative
cost.
5.9 (An Example of Cycling)
Consider an assignment problem with sources 1, 2, 3, 4 and sinks 5, 6, 7, 8. There
is an arc between each source and each sink. The arc costs are as follows:
a16 = a17 = a25 = a27 = a35 = a36 = a48 = 1, aij = 0 otherwise.
Let the initial feasible tree consist of arcs (1,5), (1,6), (2,6), (2,8), (4,8), (4,7),
(3,7), with corresponding arc flows
x15 = x26 = x37 = x48 = 1, xij = 0 otherwise.
Sec. 5.5 Notes, Sources, and Exercises 209
Suppose that the simplex method is applied without restriction on the choice of
the out-arc (so the generated trees need not be strongly feasible). Verify that
one possible sequence of in-arc/out-arc pairs is given by(
(1, 8), (2, 8)
)
,
(
(3, 6), (1, 6)
)
,
(
(4, 6), (4, 7)
)
,(
(3, 5), (3, 6)
)
,
(
(3, 8), (1, 8)
)
,
(
(2, 5), (3, 5)
)
,(
(4, 5), (4, 6)
)
,
(
(2, 7), (2, 5)
)
,
(
(2, 8), (3, 8)
)
,(
(1, 7), (2, 7)
)
,
(
(4, 7), (4, 5)
)
,
(
(1, 6), (1, 7)
)
,
and that after these twelve pivots we obtain the initial tree again. (This example
comes from Chvatal [1983].)
5.10 (Rank of the Conservation of Flow Equations)
Let us say that the conservation of flow equations∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
have rank r if one can find a subset A of r arcs such that for every supply vector
s = {si | i ∈ N}, the conservation of flow equations have a unique solution x(s)
with xij(s) = 0 for all (i, j) ∈ A. (This definition is consistent with the standard
definition of rank in linear algebra.)
(a) Show that if the graph is connected, the conservation of flow equations
have rank N − 1, where N is the number of nodes. Hint : Use a spanning
tree of the graph.
(b) Show that the conservation of flow equations have rank N − r if the graph
is the union of r disconnected subgraphs, each of which is connected by
itself.
5.11 (Feasible Differential Theorem, Minty [1960])
Consider a directed graph (N ,A). For each arc (i, j) ∈ A, we are given two
scalars a−ij ∈ [−∞,∞) and a+ij ∈ (−∞,∞], with a−ij ≤ a+ij .
(a) Show that there exist scalar prices pi, i ∈ N , satisfying
a−ij ≤ pi − pj ≤ a+ij , ∀ (i, j) ∈ A, (5.16)
if and only if for every cycle C, we have
0 ≤
∑
(i,j)∈C+
a+ij −
∑
(i,j)∈C−
a−ij . (5.17)
210 Simplex Methods Chap. 5
Hint : Consider a minimum cost flow problem with arcs and cost coefficients
constructed as follows:
(1) For each arc (i, j) ∈ A with a+ij < ∞, introduce an arc (i, j) with cost
coefficient a+ij and feasible flow range [0, 1].
(2) For each arc (i, j) ∈ A with a−ij > −∞, introduce an arc (j, i) with
cost coefficient −a−ij and feasible flow range [0, 1].
Show that a price vector p and the zero flow vector satisfy CS if and only if
Eq. (5.16) holds. Use Prop. 1.2 to show that Eq. (5.17) is a necessary and
sufficient condition for the zero flow vector to be optimal. Apply Props.
4.2 and 4.3, which rely on Prop. 5.8.
(b) For the case where a−ij = a
+
ij = aij for all (i, j), show the following version
of the theorem: there exist pi, i ∈ N , such that
pi = pj + aij , ∀ (i, j) ∈ A,
if and only if for every cycle C, we have
∑
(i,j)∈C+
aij =
∑
(i,j)∈C−
aij .
Hint : Show that the condition (5.17) is equivalent to
∑
(i,j)∈C+
a−ij −
∑
(i,j)∈C−
a+ij ≤ 0 ≤
∑
(i,j)∈C+
a+ij −
∑
(i,j)∈C−
a−ij ,
for all cycles C.
5.12 (Dual Feasibility Theorem)
Consider the minimum cost flow problem with nonnegativity constraints. Show
that the dual problem is feasible, i.e., there exists a price vector p with
pi − pj ≤ aij , ∀ (i, j) ∈ A,
if and only if all forward cycles have nonnegative cost. Hint : Assume without loss
of generality that the primal is feasible (take si = 0 if necessary), and note that
all forward cycles have nonnegative cost if and only if the primal problem is not
unbounded (see the discussion near the beginning of Section 5.1). Alternatively,
apply the feasible differential theorem (Exercise 5.11) with a+ij = aij and a
−
ij =
−∞.
Sec. 5.5 Notes, Sources, and Exercises 211
5.13 (Relation of Dijkstra and Simplex for Shortest Paths)
Consider the single origin/all destinations shortest path problem
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(1,j)∈A}
x1j −
∑
{j|(j,1)∈A}
xj1 = N − 1,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = −1, ∀ i = 1,
0 ≤ xij , ∀ (i, j) ∈ A.
Introduce an artificial arc (1, i) for all i = 1 with very large cost M , and consider
the simplex method starting with the strongly feasible tree of artificial arcs. Let
the origin node 1 be the root node.
(a) Show that all the arcs of the trees generated by the simplex method are
oriented away from the origin and carry unit flow.
(b) How can a negative length cycle be detected with the simplex method?
(c) Assume that aij ≥ 0 for all (i, j) ∈ A and suppose that the in-arc is selected
to have minimum reduced cost out of all arcs that are not in the tree. Use
induction to show that after the kth pivot the tree consists of a shortest
path tree from node 1 to the k closest nodes to node 1, together with the
artificial arcs (1, i) for all i that are not among the k closest nodes to node
1. Prove that this implementation of the simplex method is equivalent to
Dijkstra’s method.
5.14
Use the simplex method to solve the minimum cost flow problem with the data
of Fig. 5.21, and with the arc flow bounds 0 ≤ xij ≤ 1 for all (i, j) ∈ A.
5.15
Suppose that the minimum cost flow problem with upper and lower bounds of
Section 5.3 is transformed to a problem with nonnegativity constraints as in
Section 4.2. Show that the simplex method of Section 5.2, when applied to the
latter problem, is equivalent to the simplex method of Section 5.3. In particular,
relate feasible trees, basic flow vectors, and price vectors generated by the two
methods, and show that they are in one-to-one correspondence.
212 Simplex Methods Chap. 5
5.16 (Birkhoff’s Theorem for Doubly Stochastic Matrices)
A doubly stochastic n × n matrix X = {xij} is a matrix such that the elements
of each of its rows and columns are nonnegative, and add to one, that is, xij ≥ 0
for all i and j,
∑n
j=1
xij = 1 for all i, and
∑n
i=1
xij = 1 for all j. A permutation
matrix is a doubly stochastic matrix whose elements are either one or zero. Then,
there is a single one in each row and each column, and all other elements are zero.
(a) Show that given a doubly stochastic matrix X, there exists a permutation
matrix X∗ such that, for all i and j, if x∗ij = 1, then xij > 0. Hint : View
X as a feasible solution of the minimum cost flow version of an assignment
problem, and view X∗ as a feasible assignment.
(b) Use part (a) to show constructively that every doubly stochastic matrix
X can be written as
∑k
i=1
γiX
∗
i , where X
∗
i are permutation matrices and
γi ≥ 0,
∑k
i=1
γi = 1. Hint : Define a sequence of matrices X0, X1, . . . , Xk,
which are nonnegative multiples of doubly stochastic matrices, such that
X0 = X, Xk = 0, and for all i, Xi − Xi+1 is a positive multiple of a
permutation matrix.
5.17 (Hall’s Theorem for Perfect Matrices)
A perfect matrix is a matrix with nonnegative integer elements such that the
elements of each of its rows and each of its columns add to the same integer k.
Show that a perfect matrix can be written as the sum of k permutation matrices
(defined in Exercise 5.16). Hint : Use the hints and constructions of Exercise
5.16.
5.18 (Total Unimodularity)
Consider the arc incidence matrix E of a graph. This matrix has a row for each
node and a column for each arc. The component corresponding to the ith row
and a given arc is a 1 if the arc is outgoing from i, is a -1 if the arc is incoming to
i, and is a 0 otherwise. Show that E is totally unimodular (cf. the discussion in
Section 5.5). Hint : We must show that the determinant of each square submatrix
of E is 0, 1, or -1. Complete the details of the following argument, which uses
induction on the dimension of the submatrix. The submatrices of dimension 1
of E are the scalar components of E, which are 0, 1, or -1. Suppose that the
determinant of each square submatrix of dimension n ≥ 1 is 0, 1, or -1. Consider
a square submatrix of dimension n + 1. If this matrix has a column with all
components 0, the matrix is singular, and its determinant is 0. If the matrix
has a column with a single nonzero component (a 1 or a -1), by expanding its
determinant along that component and using the induction hypothesis, we see
that the determinant is 0, 1, or -1. Finally, if each column of the matrix has two
components (a 1 and a -1), the sum of its rows is 0, so the matrix is singular,
and its determinant is 0.
6
Dual Ascent Methods
Contents
6.1. Dual Ascent
6.2. The Primal-Dual (Sequential Shortest Path) Method
6.3. The Relaxation Method
6.4. Solving Variants of an Already Solved Problem
6.5. Implementation Issues
6.6. Notes, Sources, and Exercises
213
214 Dual Ascent Methods Chap. 6
In this chapter, we discuss our second major class of algorithms for the
minimum cost flow problem. In Chapter 4 we introduced the dual problem,
and in Chapter 5 we established, as a byproduct of our development of
simplex methods, the full extent of the relationship between the primal and
dual problems. We are now ready to develop iterative methods for solving
the dual problem. These methods generate sequences of dual variables,
that is, price vectors. Each new price vector has strictly improved dual
cost over the preceding one, unless it is already optimal.
Together with price vectors, dual ascent methods generate corre-
sponding capacity-feasible vectors that satisfy complementary slackness.
These flow vectors violate the conservation of flow constraints, except upon
termination of the method. We may view dual ascent methods as iterating
on flow-price pairs, while maintaining complementary slackness and striv-
ing to satisfy flow feasibility, but we will not emphasize this viewpoint.
Instead, in our development, we will focus on the dual ascent (cost im-
provement) property of the successive price vectors, and we will view the
corresponding flow vectors merely as a convenient device for generating
dual ascent directions.
We will concentrate on two main algorithms: the primal-dual method,
developed in Section 6.2, and the relaxation method, developed in Section
6.3. These methods use different ascent directions, but admit fairly similar
implementation.
6.1 DUAL ASCENT
In this section we develop the main ideas underlying the dual ascent ap-
proach. We focus on the minimum cost flow problem
minimize
∑
(i,j)∈A
aijxij
subject to the constraints∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
bij ≤ xij ≤ cij , ∀ (i, j) ∈ A.
Throughout the chapter we will assume that the scalars aij , bij , cij , and
si are all integer. Usually, this is not an important practical restriction.
However, there are extensions of the algorithms of this chapter that handle
noninteger problem data, as will be discussed later.
The main idea of dual cost improvement (or dual ascent) algorithms
is to start with a price vector and successively obtain new price vectors
Sec. 6.1 Dual Ascent 215
with improved dual cost, with the aim of solving the dual problem. Recall
from Section 4.3 that this problem is
maximize q(p)
subject to no constraint on p,
(6.1)
where the dual function q is given by
q(p) =
∑
(i,j)∈A
qij(pi − pj) +
∑
i∈N
sipi, (6.2)
with
qij(pi − pj) = min
bij≤xij≤cij
{
(aij + pj − pi)xij
}
=
{
(aij + pj − pi)bij if pi ≤ aij + pj ,
(aij + pj − pi)cij if pi > aij + pj .
(6.3)
It is helpful here to introduce some terminology. For any price vector
p, we say that an arc (i, j) is
inactive if pi < aij + pj ,
balanced if pi = aij + pj ,
active if pi > aij + pj .
The complementary slackness (CS) conditions for a flow-price vector pair
(x, p), introduced in Section 4.3, can be restated as follows:
xij = bij , for all inactive arcs (i, j), (6.4)
bij ≤ xij ≤ cij , for all balanced arcs (i, j), (6.5)
xij = cij , for all active arcs (i, j), (6.6)
(see Fig. 6.1).
0
aij
bij cij x ij
p jpi -
inactive
balanced
active Figure 6.1: Illustration of the com-
plementary slackness conditions. For
each arc (i, j), the pair (xij , pi −pj)
should lie on the graph shown. The
arc is inactive, active, or balanced
in the regions shown.
216 Dual Ascent Methods Chap. 6
We restate for convenience the following basic duality result, proved
in Section 4.3 (cf. Prop. 4.1).
Proposition 6.1: If a feasible flow vector x∗ and a price vector p∗
satisfy the complementary slackness conditions (6.4)-(6.6), then x∗ is
an optimal solution of the minimum cost flow problem and p∗ is an
optimal solution of the dual problem (6.1).
The major dual ascent algorithms select at each iteration a connected
subset of nodes S, and change the prices of these nodes by equal amounts,
while leaving the prices of all other nodes unchanged. In other words,
each iteration involves a price vector change along a direction of the form
dS = (d1, . . . , dN ), where
di =
{
1 if i ∈ S,
0 if i /∈ S,
and S is a connected subset of nodes. Such directions will be called ele-
mentary ; see also Section 9.7.
To check whether dS is a direction of dual ascent, we need to calculate
the corresponding directional derivative of the dual cost along dS and check
whether it is positive. From the dual cost expression (6.2)-(6.3), it is seen
that this directional derivative is
q′(p; dS) = lim
α↓0
q(p + αdS) − q(p)
α
=
∑
(j,i) : active
j /∈S, i∈S
cji +
∑
(j,i) : inactive or balanced
j /∈S, i∈S
bji
−
∑
(i,j) : active or balanced
i∈S, j /∈S
cij −
∑
(i,j) : inactive
i∈S, j /∈S
bij
+
∑
i∈S
si.
(6.7)
In words, the directional derivative q′(p; dS) is the difference between inflow
and outflow across the node set S when the flows of the inactive and active
arcs are set at their lower and upper bounds, respectively, and the flow of
each balanced arc incident to S is set to its lower or upper bound depending
on whether the arc is incoming to S or outgoing from S.
To obtain a suitable set S, with positive directional derivative q′
(
p, dS
)
,
it is convenient to maintain a flow vector x satisfying CS together with p.
This helps to organize the search for an ascent direction and to detect
optimality, as we will now explain.
Sec. 6.1 Dual Ascent 217
For a flow vector x, let us define the surplus gi of node i as the
difference between total inflow into i minus the total outflow from i, that
is,
gi =
∑
{j|(j,i)∈A}
xji −
∑
{j|(i,j)∈A}
xij + si. (6.8)
We have∑
i∈S
gi =
∑
{(j,i)∈A|j /∈S, i∈S}
xji −
∑
{(i,j)∈A|i∈S, j /∈S}
xij +
∑
i∈S
si, (6.9)
and if x satisfies CS together with p [implying xij = bij for all (i, j):
inactive, and xij = cij for all (i, j): active], we obtain using Eqs. (6.7) and
(6.9) ∑
i∈S
gi = q′(p; dS) +
∑
(j,i) : balanced
j /∈S, i∈S
(xji − bji)
+
∑
(i,j): balanced
i∈S, j /∈S
(cij − xij)
≥ q′(p; dS).
(6.10)
We see, therefore, that only a node set S that has positive total surplus
is a candidate for generating a direction dS of dual ascent. In particular,
if there is no balanced arc (i, j) with i ∈ S, j /∈ S, and xij < cij , and no
balanced arc (j, i) with j /∈ S, i ∈ S, and bji < xji, then∑
i∈S
gi = q′(p; dS). (6.11)
Thus, if S has positive total surplus, then dS is an ascent direction. The fol-
lowing lemma expresses this idea and provides the basis for the subsequent
algorithms.
Lemma 6.1: Suppose that x and p satisfy the CS conditions, and let
S be a subset of nodes. Let dS = (d1, d2, . . . , dN ) be the vector with
di = 1 if i ∈ S and di = 0 otherwise, and assume that∑
i∈S
gi > 0.
Then either dS is a dual ascent direction, that is,
q′(p; dS) > 0,
or else there exist nodes i ∈ S and j /∈ S such that either (i, j) is a
balanced arc with xij < cij or (j, i) is a balanced arc with bji < xji.
218 Dual Ascent Methods Chap. 6
Proof: Follows from Eq. (6.10). Q.E.D.
Overview of Dual Ascent Algorithms
The algorithms of this chapter start with and maintain an integer flow-price
vector pair (x, p), satisfying CS. They operate iteratively. At the beginning
of each iteration, a subset of nodes S is selected such that∑
i∈S
gi > 0.
Initially S consists of one or more nodes with positive surplus, which are
chosen based on some rule that is algorithm-dependent. According to the
preceding lemma, there are two possibilities (which are not mutually ex-
clusive):
(a) Dual ascent is possible: S defines a dual ascent direction
dS = (d1, d2, . . . , dN ),
where di = 1 if i ∈ S, and di = 0 otherwise.
(b) Enlargement of S is possible: S can be enlarged by adding a node
j /∈ S with the property described in Lemma 6.1, that is, for some
i ∈ S, either (i, j) is a balanced arc with xij < cij , or (j, i) is a
balanced arc with bji < xji.
In case (b), there are two possibilities:
(1) gj ≥ 0, in which case, ∑
i∈S∪{j}
gi > 0,
and the process can be continued with
S ∪ {j}
replacing S.
(2) gj < 0, in which case, it can be seen that there is a path originating
at some node i of the starting set S and ending at node j that is
unblocked , that is, all its arcs have room for a flow increase in the
direction from i to j (see Fig. 6.2). We refer to such a path as an
augmenting path. By increasing the flow of the forward arcs (direction
from i to j) of the path and by decreasing the flow of the backward
arcs (direction from j to i) of the path, we can bring both surpluses gi
and gj closer to zero by an integer amount while leaving the surplus
of all other nodes unaffected and maintaining CS.
Sec. 6.1 Dual Ascent 219
Direction of Flow Change
i
Positive
Surplus g
Backward arcForward arc
i i1
x    <  c
ii 1
b      <  x
ii 12i1i2
Forward arc
i2 . . . . 
j
Negative
Surplus g
Backward arc
jik
b     < x
ij kj ikii 1
Figure 6.2: Illustration of an augmenting path. The initial node i and the final
node j have positive and negative surplus, respectively. Furthermore, the path is
unblocked, that is, each arc on the path has room for flow change in the direction
from i to j. A flow change of magnitude δ > 0 in this direction reduces the total
absolute surplus
∑
m∈N |gm| by 2δ provided δ ≤ min{gi,−gj}.
Since the total absolute surplus
∑
i∈N |gi| cannot be indefinitely re-
duced by integer amounts, it is seen that starting from an integer flow-price
vector pair satisfying CS, after at most a finite number of iterations in which
flow augmentations occur without finding an ascent direction, one of three
things will happen:
(a) A dual ascent direction will be found; this direction can be used to
improve the dual cost by an integer amount.
(b) gi = 0 for all i; in this case the flow vector x is feasible, and since it
satisfies CS together with p, by Prop. 6.1, x is primal-optimal and p
is dual-optimal.
(c) gi ≤ 0 for all i but gi < 0 for at least one i; since by adding Eq.
(6.9) over all i ∈ N we have
∑
i∈N si =
∑
i∈N gi it follows that∑
i∈N si < 0, so the problem is infeasible.
Thus, for a feasible problem, the procedure just outlined can be used to
find a dual ascent direction and improve the dual cost starting at any
nonoptimal integer price vector. Figure 6.3 provides an illustration for a
very simple problem.
In the next two sections, we discuss two different dual ascent methods.
The first, known as primal-dual , in its classical form, tries at each iteration
to use the steepest ascent direction, that is, the elementary direction with
maximal directional derivative. We will show how this method can also
be implemented by means of a shortest path computation. The second
method, called relaxation, is usually faster in practice. It tries to use
directions that are not necessarily steepest, but can be computed more
quickly than the steepest ascent direction.
Another way to describe the difference between the primal-dual and
the relaxation methods, is to consider the set S and the two possibilities
described in Lemma 6.1:
220 Dual Ascent Methods Chap. 6
Cost = 0
Feasible flow range: [0,5]
s  = 11 s  = - 13
(a)
Flow = 0Flow = 0
g  = 1
1
g  = 0
2 g  = - 1
3
p  = 01 p  = 02 p  = 03
(b)
Problem Data
Prior to the 1st Iteration
After the 1st Iteration
After the 2nd Iteration
Flow = 0Flow = 0
g  = 1
1
g  = 0
2 g  = - 1
3
p  = 11 p  = 12
p  = 0
3
(c)
Flow = 1Flow = 1
g  = 01
g  = 0
2 g  = 0
3
(d)
1 2 3
1 2 3
1 2 3
1 2 3
p  = 11
p  = 1
2
p  = 03
Cost = 1
Figure 6.3: Illustration of a dual ascent method for the simple problem described
in (a). Initially, we have x = (0, 0) and p = (0, 0, 0), as shown in (b).
The first iteration starts with S = {1}. It can be seen using Eq. (6.10), that
the directional derivative q′(p; dS) is -4 (s1−c12 = 1−5 = −4), so dS = (1, 0, 0) is
not a direction of ascent. We thus enlarge S by adding node 2 using the balanced
arc (1, 2). Since there is no incident balanced arc to S = {1, 2}, the direction
dS = (1, 1, 0) is a direction of ascent [using Eq. (6.10), q′(p; dS) = s1 + s2 = 1].
We thus increase the prices of the nodes in S by a common increment γ, and
we choose γ = 1 because this is the increment that maximizes the dual function
along the direction dS starting from p; this can be seen by checking the directional
derivative of q at the price vector (γ, γ, 0) along the direction dS and finding that
it switches from positive (= 1) to negative (= −4) at γ = 1 where the arc (2, 3)
becomes balanced.
The second iteration starts again with S = {1}. As in the first iteration, S
is enlarged to S = {1, 2}. Since the corresponding direction dS = (1, 1, 0) is not
a direction of ascent [q′(p; dS) = −4], we explore the balanced incident arc (2, 3)
and we discover the negative surplus node 3. The augmenting path (1, 2, 3) has
now been obtained, and the corresponding augmentation sets the flows of the arcs
(1, 2) and (2, 3) to 1. Since now all node surpluses become zero, the algorithm
terminates; x = (1, 1) is an optimal primal solution and p = (1, 1, 0) is an optimal
dual solution.
Sec. 6.2 The Primal-Dual (Sequential Shortest Path) Method 221
(a) Use S to define an ascent direction [if q′(p; dS) > 0] or
(b) Enlarge the set S with a node j /∈ S such that either (i, j) is a balanced
arc with xij < cij or (j, i) is a balanced arc with bji < xji (if such a
node j exists).
The primal-dual and the relaxation methods operate identically when only
one of the alternatives (a) and (b) is available. When, however, S and p are
such that both alternatives are possible, the primal-dual method chooses
alternative (b) while the relaxation method chooses alternative (a).
6.2 THE PRIMAL-DUAL (SEQUENTIAL SHORTEST PATH)
METHOD
The primal-dual algorithm starts with any integer pair (x, p) satisfying
CS. One possibility is to choose the integer vector p arbitrarily and to
set xij = bij if (i, j) is inactive or balanced, and xij = cij otherwise.
(Prior knowledge could be built into the initial choice of x and p using, for
example, the results of an earlier optimization.) The algorithm preserves
the integrality and CS property of the pair (x, p) throughout.
At the start of the typical iteration, we have an integer pair (x, p)
satisfying CS. The iteration indicates that the primal problem is infeasible,
or else indicates that (x, p) is optimal, or else transforms this pair into
another pair satisfying CS.
In particular, if gi ≤ 0 for all i, then in view of the fact
∑
i∈N gi =∑
i∈N si [see Eq. (6.9) with S = N ], there are two possibilities:
(1) gi < 0 for some i, in which case
∑
i∈N si < 0 and the problem is
infeasible.
(2) gi = 0 for all i, in which case x is feasible and therefore also optimal,
since it satisfies CS together with p.
In either case, the algorithm terminates.
If on the other hand we have gi > 0 for at least one node i, the
iteration starts by selecting a nonempty subset I of nodes i with gi > 0.
The iteration maintains two sets of nodes S and L, with S ⊂ L. Initially, S
is empty and L consists of the subset I. We use the following terminology.
S: Set of scanned nodes (these are the nodes whose incident arcs have
been “examined” during the iteration).
L: Set of labeled nodes (these are the nodes that have either been scanned
during the iteration or are current candidates for scanning).
In the course of the iteration we continue to add nodes to L and S until
either an augmenting path is found or L = S, in which case dS will be
shown to be an ascent direction. The iteration also maintains a label for
222 Dual Ascent Methods Chap. 6
every node i ∈ L − I, which is an incident arc of i. The labels are useful
for constructing augmenting paths (see Step 3 of the following iteration).
Primal-Dual Iteration
Step 0 (Initialization): Select a set I of nodes i with gi > 0. [If
no such node can be found, terminate; the pair (x, p) is optimal if
gi = 0 for all i; otherwise the problem is infeasible.] Set L := I and
S := empty, and go to Step 1.
Step 1 (Choose a Node to Scan): If S = L, go to Step 4; else
select a node i ∈ L − S, set S := S ∪ {i}, and go to Step 2.
Step 2 (Label Neighbor Nodes of i): Add to L all nodes j /∈ L
such that either (j, i) is balanced and bji < xji or (i, j) is balanced
and xij < cij ; also for every such j, give to j the label “(j, i)” if (j, i)
is balanced and bji < xji, and otherwise give to j the label “(i, j).” If
for all the nodes j just added to L we have gj ≥ 0, go to Step 1. Else
select one of these nodes j with gj < 0 and go to Step 3.
Step 3 (Flow Augmentation): An augmenting path P has been
found that begins at a node i belonging to the initial set I and ends
at the node j identified in Step 2. The path is constructed by tracing
labels backward starting from j, and is such that we have
xmn < cmn, ∀ (m, n) ∈ P+,
xmn > bmn, ∀ (m, n) ∈ P−,
where P+ and P− are the sets of forward and backward arcs of P ,
respectively. Let
δ = min
{
gi,−gj ,
{
cmn − xmn | (m, n) ∈ P+
}
,{
xmn − bmn | (m, n) ∈ P−
}}
.
Increase by δ the flows of all arcs in P+, decrease by δ the flows of all
arcs in P−, and go to the next iteration.
Step 4 (Price Change): Let
γ = min
{
{pj + aij − pi | (i, j) ∈ A, xij < cij , i ∈ S, j /∈ S},
{pj − aji − pi | (j, i) ∈ A, bji < xji, i ∈ S, j /∈ S}
}
.
(6.12)
Sec. 6.2 The Primal-Dual (Sequential Shortest Path) Method 223
Set
pi :=
{
pi + γ if i ∈ S,
pi otherwise.
Add to L all nodes j for which the minimum in Eq. (6.11) is attained
by an arc (i, j) or an arc (j, i); also for every such j, give to j the label
“(i, j)” if the minimum in Eq. (6.12) is attained by an arc (i, j), and
otherwise give to j the label “(j, i).” If for all the nodes j just added
to L we have gj ≥ 0, go to Step 1. Else select one of these nodes j with
gj < 0 and go to Step 3. [Note: If there is no arc (i, j) with xij < cij ,
i ∈ S, and j /∈ S, or arc (j, i) with bji < xji, i ∈ S, and j /∈ S, the
problem is infeasible and the algorithm terminates; see Prop. 6.2 that
follows.]
Note the following regarding the primal-dual iteration:
(a) All operations of the iteration preserve the integrality of the flow-price
vector pair.
(b) The iteration maintains CS of the flow-price vector pair. To see this,
note that arcs with both ends in S, which are balanced just before
a price change, continue to be balanced after a price change. This
means that a flow augmentation step, even if it occurs following sev-
eral executions of Step 4, changes only flows of balanced arcs, so it
cannot destroy CS. Also, a price change in Step 4 maintains CS be-
cause no arc flow is modified in this step and the price increment γ of
Eq. (6.12) is such that no arc changes status from active to inactive
or vice versa.
(c) At all times we have S ⊂ L. Furthermore, when Step 4 is entered, we
have S = L and L contains no node with negative surplus. Therefore,
based on the logic of Step 2, there is no balanced arc (i, j) with
xij < cij , i ∈ S, and j /∈ S, and no balanced arc (j, i) with bji < xji,
i ∈ S, and j /∈ S. It follows from the discussion preceding Lemma 6.1
[cf. Eq. (6.11)] that dS is an ascent direction.
(d) Only a finite number of price changes occur at each iteration, so
each iteration executes to completion, either terminating with a flow
augmentation in Step 3, or with an indication of infeasibility in Step
4. To see this, note that between two price changes, the set L is
enlarged by at least one node, so there can be no more than N price
changes per iteration.
(e) Only a finite number of flow augmentation steps are executed by
the algorithm, since each of these reduces the total absolute surplus∑
i∈N |gi| by an integer amount [by (a) above], while price changes
do not affect the total absolute surplus.
224 Dual Ascent Methods Chap. 6
(f) The algorithm terminates. The reason is that each iteration will ex-
ecute to completion [by (d) above], and will involve exactly one aug-
mentation, while there will be only a finite number of augmentations
[cf. (e) above].
The following proposition establishes the validity of the method.
Proposition 6.2: Consider the minimum cost flow problem and as-
sume that aij , bij , cij , and si are all integer.
(a) If the problem is feasible, then the primal-dual method termi-
nates with an integer optimal flow vector x and an integer opti-
mal price vector p.
(b) If the problem is infeasible, then the primal-dual method termi-
nates either because gi ≤ 0 for all i and gi < 0 for at least one i
or because there is no arc (i, j) with xij < cij , i ∈ S, and j /∈ S,
or arc (j, i) with bji < xji, i ∈ S, and j /∈ S in Step 4.
Proof: The algorithm terminates as argued earlier, and there are three
possibilities:
(1) The algorithm terminates because all nodes have zero surplus. In this
case the flow-price vector pair obtained upon termination is feasible
and satisfies CS, so it is optimal.
(2) The algorithm terminates because gi ≤ 0 for all i and gi < 0 for at
least one i. In this case the problem is infeasible, since for a feasible
problem we must have
∑
i∈N gi = 0.
(3) The algorithm terminates because there is no arc (i, j) with xij < cij ,
i ∈ S, and j /∈ S, or arc (j, i) with bji < xji, i ∈ S, and j /∈ S in
Step 4. Then the flux across the cut Q = [S,N − S] is equal to the
capacity C(Q) and is also equal to the sum of the divergences of the
nodes of S, which is
∑
i∈S(si − gi) [cf. Eq. (6.8)]. Since gi ≥ 0 for all
i ∈ S, gi > 0 for the nodes i ∈ I, and I ⊂ S, we see that
C(Q) <
∑
i∈S
si.
This implies that the problem is infeasible, since for any feasible flow
vector we must have ∑
i∈S
si = F (Q) ≤ C(Q),
where F (Q) is the corresponding flux across Q. [Another way to show
that the problem is infeasible in this case is to observe that dS is a
Sec. 6.2 The Primal-Dual (Sequential Shortest Path) Method 225
dual ascent direction, and if no arc (i, j) with the property stated
exists, the rate of increase of the dual function remains unchanged as
we move indefinitely along dS starting from p. This implies that the
dual optimal value is infinite or equivalently (by Prop. 5.8) that the
primal problem is infeasible.]
Since termination can occur only under the above circumstances, the
desired conclusion follows. Q.E.D.
There are a number of variations of the primal-dual method, using
different choices of the initial set I of positive surplus nodes. The two most
common possibilities are:
(1) I consists of a single node i with gi > 0.
(2) I consists of all nodes i with gi > 0.
The primal-dual method was originally proposed with the latter choice. In
this case, whenever there is a price change, the set S contains all nodes with
positive surplus, and from the directional derivative formulas (6.10) and
(6.11), it follows that the ascent direction used in Step 4 has the maximum
possible directional derivative among elementary directions. This leads to
the interpretation of the primal-dual method as a steepest ascent method.
Figure 6.4 traces the steps of the primal-dual method for a simple
example.
The Shortest Path Implementation
We will now provide an alternative implementation of the primal-dual
method in terms of a shortest path computation. This is known as the
sequential shortest path method ; it will be seen to be mathematically equiv-
alent with the primal-dual method given earlier in the sense that it produces
the same sequence of flow-price vector pairs.
Given a pair (x, p) satisfying CS, define the reduced cost of an arc
(i, j) by
rij = aij + pj − pi. (6.13)
Recall that an unblocked path P with respect to x is a path such that
xij < cij for all forward arcs (i, j) ∈ P+ and bij < xij for all backward
arcs (i, j) ∈ P−. Furthermore, P is an augmenting path if its start and
end nodes have positive and negative surplus, respectively. We define the
length of an unblocked path P by
LP =
∑
(i,j)∈P+
rij −
∑
(i,j)∈P−
rij . (6.14)
Note that since (x, p) satisfies CS, all forward arcs of an unblocked path P
must be inactive or balanced, while all backward arcs of P must be active
226 Dual Ascent Methods Chap. 6
p   = 3
g   = 0
1
1
p   = 6
g   = 0
2
2
3
p   = 2
g   = 0
3
p   = 0
g   = 0
4
4
0
1 1
1
1 0(h) 1 4
3
2
2
p   = 1
g   = 0
1
1
p   = 2
g   = 1
2
2
3
p   = 0
g   = 0
3
p   = 0
g   = - 1
4
40
1 0
1
0 0(f) 1 4
3
2
p   = 0
g   = 1
1
1
p   = 0
g   = 2
2
2
p   = 0
g   = - 13
3
p   = 0
g   = - 2
4
4
0
0 0
0
0 0(b) 1 4
3
2
p   = 1
g   = 0
1
1
p   = 0
g   = 2
2
2
3
p   = 0
g   = 0
3
p   = 0
g   = - 2
4
4
0
1 0
0
0 0(d) 1 4
3
2
1
1
1
1
(c)
(e)
(g)
p   = 23∆
p   = 42∆
1
p   = 2∆ p   = 0∆ 4
p   = 0∆ 4
p   = 0∆ 4
p   = 02∆
p   = 22∆
p   = 03∆
p   = 03
∆
1
p   = 0∆
1
p   = 1∆
1 4
3
2
1 4
3
2
1 4
3
2
Cost/upper flow bound shown next
to each arc (lower flow bound = 0).
Supply shown next to each node.
1
5/2
2/3
4/2
1/2
2/1
3/1
1 2
(a) 1 4
3
2
Figure 6.4: Example illustrating the primal-dual method, starting with zero
prices.
(a) Problem data.
(b) Initial flows, prices, and surpluses.
(c) Augmenting path and price changes ∆pi of first iteration (I = {1}).
(d) Flows, prices, and surpluses after the first iteration.
(e) Augmenting path and price changes ∆pi of second iteration (I = {2}).
(f) Flows, prices, and surpluses after the second iteration.
(g) Augmenting path and price changes ∆pi of third iteration (I = {2}). There
are two price changes here: first p2 increases by 2, and then p1, p2, and p3 increase
by 2.
(h) Flows, prices, and surpluses after the third iteration. The algorithm termi-
nates with an optimal flow-price pair, since all node surpluses are zero.
Sec. 6.2 The Primal-Dual (Sequential Shortest Path) Method 227
or balanced [cf. Eqs. (6.4)-(6.6)], so we have
rij ≥ 0, ∀ (i, j) ∈ P+, (6.15)
rij ≤ 0, ∀ (i, j) ∈ P−. (6.16)
Thus, the length of P is nonnegative.
The sequential shortest path method starts each iteration with an
integer pair (x, p) satisfying CS and with a set I of nodes i with gi > 0,
and proceeds as follows.
Sequential Shortest Path Iteration
Construct an augmenting path P with respect to x that has minimum
length over all augmenting paths with respect to x that start at some
node i ∈ I. Then, carry out an augmentation along P (cf. Step 3 of
the primal-dual iteration) and modify the node prices as follows:
Let d be the length of P and for each node m ∈ N , let dm be the
minimum of the lengths of the unblocked paths with respect to x that
start at some node in I and end at m (dm = ∞ if no such path exists).
The new price vector p is given by
pm = pm + max{0, d − dm}, ∀ m ∈ N . (6.17)
The method terminates under the following circumstances:
(a) All nodes i have zero surplus; in this case it will be seen that the
current pair (x, p) is primal and dual optimal.
(b) gi ≤ 0 for all i and gi < 0 for at least one i; in this case the problem
is infeasible, since
∑
i∈N si =
∑
i∈N gi < 0.
(c) There is no augmenting path with respect to x that starts at some
node in I; in this case it will be seen that the problem is infeasible.
We will show shortly that the method preserves the integrality and
the CS property of the pair (x, p), and that it terminates.
It is important to note that the shortest path computation can be
executed using the standard shortest path algorithms described in Chapter
2. The idea is to use rij as the length of each forward arc (i, j) of an
unblocked path, and to reverse the direction of each backward arc (i, j) of
an unblocked path and to use −rij as its length [cf. the unblocked path
length formula (6.14)]. In particular, the iteration can be executed using
the following procedure:
Consider the residual graph, which has the same node set N of the
original problem graph, and has
an arc (i, j) with length rij for every arc (i, j) ∈ A with xij < cij ,
228 Dual Ascent Methods Chap. 6
an arc (j, i) with length −rij for every arc (i, j) ∈ A with bij < xij .
[If this creates two arcs in the same direction between two nodes, discard
the arc with the larger length (in case of a tie, discard either arc).] Find
a path P that is shortest among paths of the residual graph that start at
some node in I and end at some node with negative surplus. Find also the
shortest distances dm from nodes of I to all other nodes m [or at least to
those nodes m with dm less than the length of P ; cf. Eq. (6.17)].
Note here that by Eqs. (6.15) and (6.16), the arc lengths of the resid-
ual graph are nonnegative, so Dijkstra’s method can be used for the shortest
path computation. Since all forward paths in the residual graph correspond
to unblocked paths in the original problem graph, and corresponding paths
have the same length, it is seen that the shortest path P is an augment-
ing path as required and that the shortest distances dm yield the vector p
defined by Eq. (6.17).
Figure 6.5 illustrates the sequential shortest path method and shows
the sequence of residual graphs for the example worked out earlier (cf. Fig.
6.4). We now prove the validity of the method.
Proposition 6.3: Consider the minimum cost flow problem and as-
sume that aij , bij , cij , and si are all integer. Then, for the sequential
shortest path method, the following hold:
(a) Each iteration maintains the integrality and the CS property of
the pair (x, p).
(b) If the problem is feasible, then the method terminates with an
integer optimal flow vector x and an integer optimal price vector
p.
(c) If the problem is infeasible, then the method terminates either
because gi ≤ 0 for all i and gi < 0 for at least one i, or because
there is no augmenting path starting at some node of the set I
and ending at some node with negative surplus.
Proof: (a) We will show that if the starting pair (x, p) of an iteration
is integer and satisfies CS, the same is true for a pair (x, p) produced
by the iteration. Indeed, a flow augmentation maintains the integrality
of the flows, since the upper and lower flow bounds are assumed integer.
Furthermore, the arc lengths of the residual graph are integer, so by Eq.
(6.17), p is integer.
To show that (x, p) satisfies CS, consider an arc (i, j) for which xij <
cij . We will show that pi − pj ≤ aij . We distinguish two cases:
(1) xij = cij . In this case, we have bij < xij , the direction of (i, j) is
reversed in the residual graph, and the reverse arc (j, i) lies on the
Sec. 6.2 The Primal-Dual (Sequential Shortest Path) Method 229
Cost/upper flow bound shown next
to each arc (lower flow bound = 0).
Supply shown next to each node.
1
5/2
2/3
4/2
1/2
2/1
3/1
1
2
2
(a) 1 4
3
2
1
(c)
1
p   = 1
1 4
3
2
p   = 3
1
p   = 23
1
1 (g)
p   = 62
1 4
3
2
1
(e)
p   = 22
1 4
3
2
5
2
2
+
0
0
5
0
(f) 1 4
3
2
(b) 1 4
3
2
0
(d) 1 4
3
2
4
+
0
2
3
+
0
2
2
4 3
6
1 2
4
+
0
0
0
–
–
–
–
Figure 6.5: The sequential shortest path method applied to the problem of Fig.
6.4, starting with all zero prices. The sequences of flows, prices, and surpluses are
the same as those generated by the primal-dual method.
(a) Problem data.
(b) Initial residual graph with the arc lengths shown next to the arcs. The nodes
with positive, zero, and negative surplus are indicated by “+”, “0”, and “−”,
respectively.
(c) Shortest augmenting path and changed prices of first iteration (I = {1}).
(d) Residual graph with the arc lengths shown next to the arcs after the first
iteration.
(e) Shortest augmenting path and changed prices of second iteration (I = {2}).
(f) Residual graph with the arc lengths shown next to the arcs after the second
iteration.
(g) Shortest augmenting path and changed prices of third (and final) iteration
(I = {2}).
230 Dual Ascent Methods Chap. 6
shortest augmenting path P . Hence, we have
di ≤ d, dj ≤ d, di = dj − rij .
Using these equations, and Eqs. (6.13) and (6.17), we obtain
pi − pj = pi − pj + max{0, d − di} − max{0, d − dj}
= pi − pj − (di − dj)
= pi − pj + rij
= aij .
(2) xij < cij . In this case we have
dj ≤ di + rij ,
since (i, j) is an arc of the residual graph with length rij . Using this
relation and the nonnegativity of rij , we see that
max{0, d − di} ≤ max{0, d − dj + rij}
≤ max{rij , d − dj + rij}
= max{0, d − dj} + rij .
Hence, we have
pi−pj = pi−pj +max{0, d−di}−max{0, d−dj} ≤ pi−pj +rij = aij .
Thus, in both cases we have pi − pj ≤ aij . We can similarly show that if
bij < xij , then pi − pj ≥ aij , completing the proof of the CS property of
the pair (x, p).
(b) and (c) Every completed iteration in which a shortest augmenting path
is found reduces the total absolute surplus
∑
i∈N |gi| by an integer amount,
so termination must occur. Part (a) shows that at the start of each itera-
tion, the pair (x, p) satisfies CS. There are two possibilities:
(1) gi ≤ 0 for all i. In this case, either gi = 0 for all i in which case x is
feasible, and x and p are primal and dual optimal, respectively, since
they satisfy CS, or else gi < 0 for some i, in which case the problem
is infeasible.
(2) gi > 0 for at least one i. In this case we can select a nonempty set I of
nodes with positive surplus, form the residual graph, and attempt the
corresponding shortest path computation. There are two possibilities:
either a shortest augmenting path is found, in which case the iteration
will be completed with an attendant reduction of the total absolute
Sec. 6.2 The Primal-Dual (Sequential Shortest Path) Method 231
surplus, or else there is no unblocked path with respect to x from a
node of I to a node with negative surplus. In the latter case, we claim
that the problem is infeasible. Indeed, by Prop. 3.1 (more accurately,
the generalization given in Exercise 3.11 of Ch. 3), there exists a
saturated cut Q = [S,N −S] such that all nodes of I belong to S and
all nodes with negative surplus belong to N − S. The flux across Q
is equal to the capacity C(Q) of Q and is also equal to the sum of the
divergences of the nodes of S, which is
∑
i∈S(si − gi) [cf. Eq. (6.8)].
Since gi ≥ 0 for all i ∈ S, gi > 0 for the nodes i ∈ I, and I ⊂ S, we
see that
C(Q) <
∑
i∈S
si.
This implies that the problem is infeasible, since for any feasible flow
vector we must have
∑
i∈S si = F (Q) ≤ C(Q), where F (Q) is the
corresponding flux across Q.
Thus, termination of the algorithm must occur in the manner stated in the
proposition. Q.E.D.
By appropriately adapting the shortest path algorithms of Chapter 2,
one can obtain a variety of implementations of the sequential shortest path
iteration. Here is an example, which adapts the generic single origin/single
destination algorithm of Section 2.5.2 and supplements it with a labeling
procedure that constructs the augmenting path. We introduce a candidate
list V , a label di for each node i, a shortest distance estimate d, and a node
j whose initial choice is immaterial. Given a pair (x, p) satisfying CS and
a set I of nodes with positive surplus, we set initially
V = I, d = ∞,
di =
{ 0 if i ∈ I,
∞ if i /∈ I.
The shortest path computation proceeds in steps and terminates when V
is empty. The typical step (assuming V is nonempty) is as follows:
Shortest Path Step in a Sequential Shortest Path Iteration
Remove a node i from V . For each outgoing arc (i, j) ∈ A, with
xij < cij , if
di + rij < min{dj , d},
give the label “(i, j)” to j, set
dj := di + rij ,
232 Dual Ascent Methods Chap. 6
add j to V if it does not already belong to V , and if gj < 0, set
d = di + rij and j = j. Also, for each incoming arc (j, i) ∈ A, with
bji < xji, if
di − rji < min{dj , d},
give the label “(j, i)” to j, set
dj := di − rji,
add j to V if it does not already belong to V , and if gj < 0, set
d = di − rji and j = j.
When the shortest path computation terminates, an augmenting path
of length d can be obtained by tracing labels backward from the node j
to some node i ∈ I. The new price vector p is obtained via the equation
pm = pm + max{0, d − dm} for all m ∈ N [cf. Eq. (6.17)]. Note that if the
node i removed from V has the minimum label property
di = min
j∈V
dj ,
the preceding algorithm corresponds to Dijkstra’s method. However, other
methods can also be used for selecting the node removed from V , including
the SLF and threshold methods discussed in Section 2.4.
We finally note that the primal-dual method discussed earlier and
the sequential shortest path method are mathematically equivalent in that
they produce identical sequences of pairs (x, p), as shown by the follow-
ing proposition (for an example, compare the calculations of Figs. 6.4 and
6.5). In fact with some thought, it can be seen that the primal-dual itera-
tion amounts to the use of a form of Dijkstra’s algorithm to calculate the
shortest augmenting path and the corresponding distances.
Proposition 6.4: Suppose that a primal-dual iteration starts with a
pair (x, p), and let I be the initial set of nodes i with gi > 0. Then:
(a) An augmenting path P may be generated in the augmentation
Step 3 of the iteration (through some order of operations in Steps
1 and 2) if and only if P has minimum length over all augmenting
paths with respect to x that start at some node in I.
(b) If p is the price vector produced by the iteration, then
pm = pm + max{0, d − dm}, ∀ m ∈ N , (6.18)
Sec. 4.2 The Primal-Dual (Sequential Shortest Path) Method 233
where d is the length of the augmenting path P of the iteration and
for each m ∈ N , dm is the minimum of the lengths of the unblocked
paths with respect to x that start at some node in I and end at m.
Proof: Let k ≥ 0 be the number of price changes that occur in the given
iteration. If k = 0, i.e., no price change occurs, then any augmenting
path P that can be produced by the iteration consists of balanced arcs,
so its length is zero. Hence P has minimum length as stated in part (a).
Furthermore, p = p, which verifies Eq. (6.18).
Assume that k ≥ 1, let Sk, k = 1, . . . , k, be the set of scanned nodes
S when the kth price change occurs, and let γk, k = 1, . . . , k, be the
corresponding price increment [cf. Eq. (6.12)]. Let also Sk+1 be the set S
at the end of the iteration. We note that the sets Sk (and hence also γk)
depend only on (x, p) and the set I, and are independent of the order of
operations in Steps 1 and 2. In particular, S1 − I is the set of all nodes j
such that there exists an unblocked path of balanced arcs [with respect to
(x, p)] that starts at some node i ∈ I and ends at j. Thus, S1 and also γ1,
is uniquely defined by I and (x, p). Proceeding inductively, it is seen that
Sk+1 −Sk is the set of all nodes j such that there exists an unblocked path
of balanced arcs [with respect to (x, pk), where pk is the price vector after
k price changes] that starts at some node i ∈ Sk and ends at j. Thus, Sk+1
and γk+1 are uniquely defined by I and (x, p) if S1, . . . ,Sk and γ1, . . . , γk
are.
It can be seen from Eq. (6.12) that for all k,
γk = minimum over the lengths of all (single arc) unblocked paths
starting at a node i ∈ Sk and ending at a node j /∈ Sk.
Using this property, and an induction argument (left for the reader), we
can show that dm, which is defined as the minimum over the lengths of all
unblocked paths that start at some node i ∈ I and end at node m, satisfies
for all k,
dm = γ1 + γ2 + · · · + γk, ∀ m ∈ Sk+1 − Sk. (6.19)
Furthermore, the length of any unblocked path that starts at some node
i ∈ I and ends at a node m /∈ Sk+1 is larger than γ1 + γ2 + · · · + γk. In
particular, the length of any augmenting path produced by the iteration is
γ1 + γ2 + · · · + γk,
so it has the property stated in part (a). Also, the price vector p produced
by the primal-dual iteration is given by
pm =
{
pm + γ1 + γ2 + · · · + γk if m ∈ Sk+1 − Sk, k = 1, . . . , k,
pm otherwise,
which in view of Eq. (6.19), agrees with Eq. (6.18). Q.E.D.
234 Dual Ascent Methods Chap. 6
6.3 THE RELAXATION METHOD
The relaxation method admits a similar implementation to the one of the
primal-dual method, but computes ascent directions much faster. In par-
ticular, while in the primal-dual method we continue to enlarge the scanned
set S until it is equal to the labeled set L (in which case we are sure that dS
is an ascent direction), in the relaxation method we stop adding nodes to S
immediately after dS becomes an ascent direction [this is done by comput-
ing the directional derivative q′(p; dS) using an efficient incremental method
and by checking its sign]. In practice, S often consists of a single node,
in which case the ascent direction is a single price coordinate, leading to
the interpretation of the method as a coordinate ascent method . Unlike the
primal-dual method, the relaxation method cannot be implemented using
a shortest path computation.
As in the primal-dual method, at the start of the typical iteration we
have an integer pair (x, p) satisfying CS. The iteration indicates that the
primal problem is infeasible, or else indicates that (x, p) is optimal, or else
transforms this pair into another pair satisfying CS. In particular, if gi ≤ 0
for all i, then there are two possibilities: (1) gi < 0 for some i, in which
case
∑
i∈N si < 0 and the problem is infeasible, or (2) gi = 0 for all i,
in which case x is feasible and therefore also optimal, since it satisfies CS
together with p. In either case, the algorithm terminates.
If on the other hand we have gi > 0 for at least one node i, the
iteration starts by selecting a node i with gi > 0. As in the primal-dual
method, the iteration maintains two sets of nodes S and L, with S ⊂ L.
At the start of the iteration, S is empty and L consists of the node i with
gi > 0. The iteration also maintains a label for every node i ∈ L except for
the starting node i; the label is an incident arc of i.
Relaxation Iteration
Step 0 (Initialization): Select a node i with gi > 0. [If no such
node can be found, terminate; the pair (x, p) is optimal if gi = 0 for all
i; otherwise the problem is infeasible.] Set L := {i} and S := empty,
and go to Step 1.
Step 1 (Choose a Node to Scan): If S = L, go to Step 4; else
select a node i ∈ L − S, set S := S ∪ {i}, and go to Step 2.
Step 2 (Label Neighbor Nodes of i): If
q′(p; dS) > 0, (6.20)
go to Step 4; else add to L all nodes j /∈ L such that either (j, i) is
balanced and bji < xji or (i, j) is balanced and xij < cij ; also for every
Sec. 6.3 The Relaxation Method 235
such j, give to j the label “(j, i)” if (j, i) is balanced and bji < xji, and
otherwise give to j the label “(i, j).” If for every node j just added to
L, we have gj ≥ 0, go to Step 1; else select one of these nodes j with
gj < 0 and go to Step 3.
Step 3 (Flow Augmentation): An augmenting path P has been
found that begins at the starting node i and ends at the node j iden-
tified in Step 2. The path is constructed by tracing labels backward
starting from j, and is such that we have
xmn < cmn, ∀ (m, n) ∈ P+, (6.21)
xmn > bmn, ∀ (m, n) ∈ P−, (6.22)
where P+ and P− are the sets of forward and backward arcs of P ,
respectively. Let
δ = min
{
gi,−gj ,{cmn − xmn | (m, n) ∈ P+},
{xmn − bmn | (m, n) ∈ P−}
}
.
Increase by δ the flows of all arcs in P+, decrease by δ the flows of all
arcs in P−, and go to the next iteration.
Step 4 (Price Change): Set
xij = cij , ∀ balanced arcs (i, j) with i ∈ S, j /∈ S, (6.23)
xji = bji, ∀ balanced arcs (j, i) with i ∈ S, j /∈ S. (6.24)
Let
γ = min
{
{pj + aij − pi | (i, j) ∈ A, xij < cij , i ∈ S, j /∈ S},
{pj − aji − pi | (j, i) ∈ A, bji < xji, i ∈ S, j /∈ S}
}
.
(6.25)
Set
pi :=
{
pi + γ if i ∈ S,
pi otherwise.
(6.26)
Go to the next iteration. [Note: As in the case of the primal-dual
iteration, if after the flow adjustments of Eqs. (6.23) and (6.24) there
is no arc (i, j) with xij < cij , i ∈ S, and j /∈ S, or arc (j, i) with
bji < xji, i ∈ S, and j /∈ S, the problem is infeasible and the algorithm
terminates.]
It can be seen that the relaxation iteration is quite similar to the
primal-dual iteration. However, there are two important differences. First,
236 Dual Ascent Methods Chap. 6
in the relaxation iteration, after a price change in Step 4, we do not return
to Step 1 to continue the search for an augmenting path like we do in the
primal-dual method. Thus, the relaxation iteration terminates with either
an augmentation as in Step 3 or a price change as in Step 4, in contrast
with the primal-dual iteration, which can only terminate with an augmen-
tation. The second and more important difference is that in the relaxation
iteration, a price change may be performed in Step 4 even if S 	= L [cf.
Eq. (6.20)]. It is because of this feature that the relaxation method iden-
tifies ascent directions faster than the primal-dual method. Note that in
contrast with the primal-dual method, the total absolute surplus
∑
i∈N |gi|
may increase as a result of a relaxation iteration.
An important property of the method is that each time we enter Step
4, dS is an ascent direction. To see this note that there are two possibilities:
(1) we have S = L (cf. Step 1) in which case dS is an ascent direction similar
to the corresponding situation in the primal-dual method, or (2) we have
S 	= L (cf. Step 2) in which case by Eq. (6.20) dS is an ascent direction.
Note that it is possible to “combine” several iterations of the relax-
ation method into a single iteration in order to save computation time.
This is done judiciously in the RELAX codes, which are publicly available
implementations of the relaxation method (Bertsekas and Tseng [1988b],
[1990], [1994]). Figure 6.6 traces the steps of the method for a simple
example.
The following proposition establishes the validity of the method.
Proposition 6.5: Consider the minimum cost flow problem and as-
sume that aij , bij , cij , and si are all integer. If the problem is feasible,
then the relaxation method terminates with an integer optimal flow
vector x and an integer optimal price vector p.
Proof: The proof is similar to the corresponding proof for the primal-dual
method (cf. Prop. 6.2). We first note that all operations of the iteration
preserve the integrality of the flow-price vector pair. To see that CS is
also maintained, note that a flow augmentation step changes only flows
of balanced arcs and therefore cannot destroy CS. Furthermore, the flow
changes of Eqs. (6.23) and (6.24), and the price changes of Eqs. (6.25) and
(6.26) maintain CS, because they set the flows of the balanced arcs that
the price change renders active (or inactive) to the corresponding upper
(or lower) bounds.
Every time there is a price change in Step 4, there is a strict im-
provement in the dual cost by the integer amount γq′(p; dS) [using the CS
property, it can be seen that γ > 0, and as argued earlier, dS is an ascent
direction so q′(p; dS) > 0]. Thus, for a feasible problem, we cannot have
an infinite number of price changes. On the other hand, it is impossible to
Sec. 6.3 The Relaxation Method 237
Cost/upper flow bound
shown next to each arc 
(lower flow bound = 0).
Supply or demand shown
next to each node.
3
(i)
1
5/2
0/5
4/3
1/2
2/1
3/2
3 4
(a) 1 4
3
2
(f)
(c)
(e)
(g)
(b)
(d)
(h)
p   = 7
g   =  0
1
1
p   =  2
g   =  2
2
2
3
p   = 0
g   =  1
3
p   = 0
g   =  - 3
4
4
1
2 0
1
0 01 4
3
2
p   = 0
g   =  3
1
1
p   = 0
g   =  2
2
2
p   = 0
g   =  - 13
3
p   = 0
g   =  - 4
4
4
0
0 0
0
0 01 4
3
2
p   = 5
g   =  1
1
1
p   = 0
g   =  2
2
2
3
p   = 0
g   =  1
3
p   = 0
g   =  - 4
4
4
0
2 0
0
0 01 4
3
2
p   = 9
g   =  0
1
1
3
p   = 0
g   = 1
3
p   = 0
g   =  - 1
4
4
1
2 2
1
2 01 4
3
2
p   = 9
g   =  0
1
1
p   =  4
g   =  2
2
2
3
p   = 0
g   =  1
3
p   = 0
g   =  - 3
4
4
1
1 0
1
0 01 4
3
2
p   = 1
g   =  3
1
1
p   = 0
g   =  2
2
2
p   = 0
g   =  - 13
3
0
0 0
0
0 01 4
3
2
p   = 7
g   = 1
1
1
p   = 2
g   =  2
2
2
3
p   = 0
g   =  1
3
p   = 0
g   =  - 4
4
4
0
2 0
0
0 01 4
3
2
p   = 9
g   =  0
1
1
p   = 4
g   =  0
2
2
3
p   = 0
g   =  0
3
p   = 0
g   = 0
4
4
1
2
1
2 01 4
3
2
2
p   = 0
g   =  - 4
4
4
p   =  4
g   =  0
2
2
Figure 6.6 An illustration of the relaxation method, starting with all zero prices.
(a) Problem data.
(b) Initial flows, prices, and surpluses.
(c) After the first iteration, which consists of a price change of node 1.
(d) After the second iteration, which consists of another price change of node 1 [note the
flow change of arc (1,3); cf. Eq. (6.23)].
(e) After the third iteration, which consists of a price change of nodes 1 and 2.
(f) After the fourth iteration, which consists of an augmentation along the path (1, 2, 4).
(g) After the fifth iteration, which consists of a price change of nodes 1 and 2.
(h) After the sixth iteration, which consists of an augmentation along the path (2, 3, 4).
(i) After the seventh iteration, which consists of an augmentation along the path (3, 4).
238 Dual Ascent Methods Chap. 6
have an infinite number of flow augmentations between two successive price
changes, since each of these reduces the total absolute surplus by an integer
amount. It follows that the algorithm can execute only a finite number of
iterations, and must terminate. Since upon termination x is feasible and
satisfies CS together with p, it follows that x is primal-optimal and p is
dual-optimal. Q.E.D.
If the problem is infeasible, the method may terminate because gi ≤ 0
for all i and gi < 0 for at least one i, or because after the flow adjustments
of Eqs. (6.23) and (6.24) in Step 4, there is no arc (i, j) with xij < cij , i ∈ S,
and j /∈ S, or arc (j, i) with bji < xji, i ∈ S, and j /∈ S. However, there
is also the possibility that the method will execute an infinite number of
iterations and price changes, with the prices of some of the nodes increasing
to ∞. Exercise 6.6 shows that, when the problem is feasible, the node prices
stay below a certain precomputable bound in the course of the algorithm.
This fact can be used as an additional test to detect infeasibility.
It is important to note that the directional derivative q′(p; dS) needed
for the ascent test (6.20) in Step 2 can be calculated incrementally (as new
nodes are added one-by-one to S) using the equation
q′(p; dS) =
∑
i∈S
gi −
∑
(j,i): balanced, j /∈S, i∈S
(xji − bji)
−
∑
(i,j): balanced, i∈S, j /∈S
(cij − xij);
cf. Eq. (6.10). Indeed, it follows from this equation that, given q′(p; dS) and
a node i /∈ S, one can calculate the directional derivative corresponding to
the enlarged set S ∪ {i} using the formula
q′(p; dS∪{i}) = q′(p; dS) +
∑
{j|(i,j): balanced, j∈S}
(xij − bij)
+
∑
{j|(j,i): balanced, j∈S}
(cji − xji)
−
∑
{j|(j,i): balanced, j /∈S}
(xji − bji)
−
∑
{j|(i,j): balanced, j /∈S}
(cij − xij).
This formula is convenient because it involves only the incident balanced
arcs of the new node i, which must be examined anyway while executing
Step 2.
In practice, the method is implemented using iterations that start
from both positive and negative surplus nodes. This seems to improve
Sec. 6.3 The Relaxation Method 239
substantially the performance of the method. It can be shown that for a
feasible problem, the algorithm terminates properly under these circum-
stances (Exercise 6.6). Another important practical issue has to do with
the initial choice of flows and prices. One possibility is to try to choose
an initial price vector that is as close to optimal as possible (for example,
using the results of some earlier optimization); one can then choose the arc
flows to satisfy the CS conditions.
Line Search and Coordinate Ascent Iterations
The stepsize γ of Eq. (6.25) corresponds to the first break point of the
piecewise linear dual function along the ascent direction dS . It is also pos-
sible to calculate through a line search an optimal stepsize that maximizes
the dual function along dS . We leave it for the reader to verify that this
computation can be done quite economically, using Eq. (6.7) or Eq. (6.10)
to test the sign of the directional derivative of the dual function at succes-
sive break points along dS . Computational experience shows that a line
search is beneficial in practice. For this reason, it has been used in the
RELAX codes mentioned earlier.
Consider now the case where there is a price change via Step 4 and
the set S consists of just the starting node, say node i. This happens
when the iteration scans the incident arcs of i at the first time Step 2 is
entered and finds that the corresponding coordinate direction leads to a
dual cost improvement [q′
(
p; d{i}
)
> 0]. If a line search of the type just
described is performed, the price pi is changed to a break point where the
right derivative is nonpositive and the left derivative is nonnegative (cf.
Fig. 6.7).
A precise description of this single-node relaxation iteration with line
search, starting from a pair (x, p) satisfying CS, is as follows:
Single-Node Relaxation Iteration
Choose a node i with gi > 0. Let
B+i = {j | (i, j) : balanced, xij < cij}, (6.27)
B−i = {j | (j, i) : balanced, bji < xji}. (6.28)
Step 1: If
gi ≥
∑
j∈B+
i
(cij − xij) +
∑
j∈B−
i
(xji − bji),
go to Step 4. Otherwise, if gi > 0, choose a node j ∈ B+i with gj < 0
and go to Step 2, or choose a node j ∈ B−i with gj < 0 and go to Step
240 Dual Ascent Methods Chap. 6
3; if no such node can be found, or if gi = 0, go to the next iteration.
Step 2 (Flow Adjustment on Outgoing Arc): Let
δ = min{gi,−gj , cij − xij}.
Set
xij := xij + δ, gi := gi − δ, gj := gj + δ
and if xij = cij , delete j from B+i ; go to Step 1.
Step 3 (Flow Adjustment on Incoming Arc): Let
δ = min{gi,−gj , xji − bji}.
Set
xji := xji − δ, gi := gi − δ, gj := gj + δ
and if xji = bji, delete j from B−i ; go to Step 1.
Step 4 (Increase Price of i): Set
gi := gi −
∑
j∈B+
i
(cij − xij) −
∑
j∈B−
i
(xji − bji), (6.29)
xij = cij , ∀ j ∈ B+i ,
xji = bji, ∀ j ∈ B−i ,
pi := min
{
{pj + aij | (i, j) ∈ A, pi < pj + aij},
{pj − aji | (j, i) ∈ A, pi < pj − aji}
}
.
(6.30)
If after these changes gi > 0, recalculate the sets B+i and B
+
i using
Eqs. (6.27) and (6.28), and go to Step 1; else, go to the next iteration.
[Note: If the set of arcs over which the minimum in Eq. (6.30) is
calculated is empty, there are two possibilities: (a) gi > 0, in which
case it can be shown that the dual cost increases without bound along
pi and the primal problem is infeasible, or (b) gi = 0, in which case
the cost stays constant along pi; in this case we leave p unchanged and
go to the next iteration.]
Note that the single-node iteration may be unsuccessful in that it
may fail to change either x or p. In this case, it should be followed by a
regular relaxation iteration that labels the appropriate neighbors of node
i, etc. Experience has shown that the most efficient way to implement
the relaxation iteration is to first attempt its single-node version; if this
Sec. 6.3 The Relaxation Method 241
1 2
3 4
i
[0,20] [0,10]
[0,20] [0,30]
Price of node i
Dual cost along pi
Values of p   for which the corresponding
incident arcs become balanced
i
Slope = 40
Slope = 20
Slope = 10 Slope = -10
Slope = -40
Maximizing point
p   - a1 1i p   + a4 i 43 3 ip   - a2 i 2p   + a
Figure 6.7: Illustration of single-node relaxation iteration. Here, node i has four
incident arcs (1, i), (3, i), (i, 2), and (i, 4) with flow ranges [0, 20], [0, 20], [0, 10],
and [0, 30], respectively, and supply si = 0. The arc costs and current prices are
such that
p1 − a1i ≤ p2 + ai2 ≤ p3 − a3i ≤ p4 + ai4,
as shown in the figure. The break points of the dual cost along the price pi
correspond to the values of pi at which one or more incident arcs to node i
become balanced. For values between two successive break points, there are no
balanced arcs. For any price pi to the left of the maximizing point, the surplus gi
must be positive to satisfy CS. A single-node iteration with line search increases
pi to the maximizing point.
fails to change x or p, then we proceed with the multiple node version,
while salvaging whatever computation is possible from the single-node at-
tempt. The RELAX codes make use of this idea. Experience shows that
single-node iterations are very frequent in the early stages of the relaxation
algorithm and account for most of the total dual cost improvement, but
become much less frequent near the algorithm’s termination.
A careful examination of the single-node iteration logic shows that in
Step 4, after the surplus change of Eq. (6.29), the surplus gi may be equal to
zero; this will happen if gi = 0 and simultaneously there is no balanced arc
(i, j) with xij < cij , or balanced arc (j, i) with bji < xji. In this case, it can
242 Dual Ascent Methods Chap. 6
be shown (see also Fig. 6.7) that the price change of Eq. (6.30) leaves the
dual cost unchanged, corresponding to movement of pi along a flat segment
to the next breakpoint of the dual cost, as shown in Fig. 6.8. This is known
as a degenerate ascent iteration. Computational experience has shown that
it is generally preferable to allow such iterations whenever possible. For
special types of problems such as assignment, the use of degenerate ascent
iterations can reduce significantly the overall computation time.
Dual cost along pi
Slope = 30
Slope = 10 Slope = -10Slope = 0
Slope = -40
Set of maximizing points
Values of p   for which the corresponding
incident arcs become balanced
i
1 2
3 4
i
[0,20] [0,10]
[0,10] [0,30]
Price of node ip   - a1 1i
p   + a4 i 43 3 ip   - a2 i 2p   + a
Figure 6.8: Illustration of a degenerate price increase. The difference between
this example and the example of Fig. 6.8 is that the feasible flow range of arc
(3, i) is now [0, 10] instead of [0, 20]. Here, there is a flat segment of the graph of
the dual cost along pi, corresponding to maximizing points. A degenerate price
increase moves pi from the extreme left maximizing point to the extreme right
maximizing point.
We finally note that single-node relaxation iterations may be used to
initialize the primal-dual method. In particular, one may start with several
cycles of single-node iterations, where each node with nonzero surplus is
taken up for relaxation once in each cycle. The resulting pair (x, p) is then
used as a starting pair for the primal-dual method. Experience has shown
that this initialization procedure is very effective.
Sec. 6.5 Implementation Issues 243
6.4 SOLVING VARIANTS OF AN ALREADY SOLVED PROBLEM
In many practical situations, we need to solve not just one network prob-
lem, but a large number of similar problems. For example, we may want to
perform sensitivity analysis, that is, change the problem data and observe
the effect on the optimal solution. In particular, we may wish to check
whether small changes in the data result in small changes in the optimal
cost or the optimal solution structure. In other cases, some of the problem
data may be under our control, and we may want to know if by chang-
ing them we can favorably influence the optimal solution. Still in other
situations, the problem involves parameters whose values are estimates of
some unknown true values, and we may want to evaluate the effect of the
corresponding estimation errors.
In another context, prominently arising in the solution of integer-
constrained problems (see Sections 10.2 and 10.3), we may have to solve
many problems with slightly different cost function and/or constraints. For
example, in the Lagrangian relaxation method, to be discussed in Section
10.3, the arc cost coefficients depend on values of Lagrange multipliers,
which are modified as the method progresses.
In order to deal with such situations efficiently, it is important to
be able to use the computed optimal solution of a problem as a starting
point for solving slightly different problems. The dual ascent methods of
the present chapter and the auction algorithms of the next chapter are
generally better suited for this purpose than the simplex method.
For example, suppose we solve a problem and then modify it by chang-
ing a few arc capacities, and/or some node supplies. To solve the modi-
fied problem using the primal-dual or the relaxation method, we can use
as starting node prices the prices obtained from the earlier solution, and
set to the appropriate bounds the arc flows that violate the new arc flow
bounds or the CS conditions. Typically, this starting flow-price pair is close
to optimal, and the solution of the modified problem is extremely fast.
By contrast, to solve the modified problem using the simplex method
one must provide a starting feasible tree. The optimal tree obtained from
the previous problem will often be infeasible for the modified problem. As
a result, a new starting tree must be constructed, and there are no simple
ways to choose this tree to be nearly optimal.
6.5 IMPLEMENTATION ISSUES
To apply the methods of this chapter, one can represent the problem us-
ing the five arrays START , END , COST , CAPACITY , and SUPPLY ,
as in simplex methods (cf. Section 5.4). For an efficient implementation,
however, it is essential to provide additional data structures that facilitate
the labeling operations, the ascent steps of Step 4, and the shortest path
244 Dual Ascent Methods Chap. 6
computations. In particular, it is necessary to have easy access to the set
of all incident arcs of each node. This can be done with the help of the
following four additional arrays.
FIRST IN (i): The first arc incoming to node i (= 0 if i has no
incoming arcs).
FIRST OUT (i): The first arc outgoing from node i (= 0 if i has no
outgoing arcs).
NEXT IN (a): The arc following arc a with the same end node as a
(= 0 if a is the last incoming arc of the end node of a).
NEXT OUT (a): The arc following arc a with the same start node
as a (= 0 if a is the last outgoing arc of the start node of a).
Figure 6.9 illustrates these arrays. As an example of their use, suppose
that we want to scan all the incoming arcs of node i. We first obtain the
arc a1 = FIRST IN(i), then the arc a2 = NEXT IN(a1), then the arc
a3 = NEXT IN(a2), etc., up to the arc ak for which NEXT IN(ak) = 0.
It is possible to forgo the use of the array NEXT OUT if the arcs are
stored in the order of their starting node, that is, the arcs outgoing from
each node i are arcs FIRST OUT (i) to FIRST OUT (i +1 )−1 . Then the
array FIRST OUT is sufficient to generate all arcs outgoing from any one
node. This saves storage of one array (and usually some computation as
well). Unfortunately, this also complicates sensitivity analysis. In particu-
lar, when the problem data are changed to add or remove some arcs, the
modification of the arrays describing the problem become more elaborate.
In the relaxation method, it is useful to employ an additional data
structure that stores the balanced incident arcs of each node in order to
facilitate the labeling step (Step 2). These arcs can be stored in two arrays
of length N and two arrays of length A, much like the arrays FIRST IN ,
FIRST OUT , NEXT IN , and NEXT OUT . However, as the set of bal-
anced arcs changes in the course of the algorithm, the arrays used to store
this set must be updated. We will not go into further details, but the in-
terested reader can study the publicly available source code of the RELAX
implementation (Bertsekas and Tseng [1988b], [1990], [1994]) to see how
this can be done efficiently.
Overall it can be seen that dual ascent methods require more arrays
of length A than simplex methods, and therefore also more storage space
(roughly twice as much).
6.6 NOTES, SOURCES, AND EXERCISES
An interesting dual ascent method that we have not discussed is the dual
simplex method . This is a general linear programming method that has
Sec. 6.6 Notes, Sources, and Exercises 245
4/2
0/1
2/1
-5/10
Cost/upper flow bound
shown next to each arc.
Supply or demand shown
next to each node.
5/2
2/3
3/1
-2/10
0/51
2
1
2
1 4
3
2
5
0
ARC START END COST CAPACITY NEXT IN NEXT OUT
1 1 2 5 2 4 2
2 1 3 0 1 3 0
3 2 3 4 2 0 5
4 3 2 3 1 0 7
5 2 5 -2 10 0 6
6 2 4 2 1 7 0
7 3 4 2 3 8 0
8 5 4 0 5 0 0
9 4 5 -5 10 5 0
NODE SUPPLY FIRST IN FIRST OUT
1 1 0 1
2 2 1 3
3 -2 2 4
4 0 6 9
5 -1 9 8
Figure 6.9: Representation of the data of a minimum cost flow problem in terms
of the nine arrays START , END , COST , CAPACITY , SUPPLY , FIRST IN ,
FIRST OUT , NEXT IN , and NEXT OUT .
been specialized to the minimum cost flow problem by several authors
(see, for example, Helgason and Kennington [1977], and Jensen and Barnes
[1980]). However, the method has not achieved much popularity because
its practical performance has been mediocre.
246 Dual Ascent Methods Chap. 6
The primal-dual method was first proposed by Kuhn [1955] for as-
signment problems under the name “Hungarian method.” The method
was generalized to the minimum cost flow problem by Ford and Fulkerson
[1956a], [1957]. A further generalization, the out-of-kilter method, was pro-
posed independently by Fulkerson [1961], Ford and Fulkerson [1962], and
Minty [1960]; see Rockafellar [1984], Bazaraa, Jarvis, and Sherali [1990],
and Murty [1992] for detailed discussions. The out-of-kilter method can be
initialized with any flow-price vector pair, not necessarily one that satisfies
CS. It appears, however, that there isn’t much that can be gained in prac-
tice by this extra flexibility, since for any given flow-price vector pair one
can modify very simply the arc flows to satisfy CS.
A method that is closely related to the primal-dual method and em-
phasizes the shortest path implementation was given by Busacker and
Gowen [1961]. An extension of the primal-dual method to network prob-
lems with gains was given by Jewell [1962], and extensions of the primal-
dual and out-of-kilter methods to network flow problems with separable
convex cost functions are proposed by Rockafellar [1984]. Primal-dual
methods for the assignment problem are discussed by Engquist [1982],
McGinnis [1983], Derigs [1985], Carraresi and Sodini [1986], Glover, Glover,
and Klingman [1986], and Carpaneto, Martello, and Toth [1988]. Combi-
nations of naive auction and sequential shortest path methods are given by
Bertsekas [1981], and Jonker and Volgenant [1986], [1987]. Variations of
the Hungarian and the primal-dual methods that are well-suited for parallel
asynchronous computation have been developed by Balas, Miller, Pekny,
and Toth [1991], and by Bertsekas and Castañon [1993a], [1993b].
One can show a pseudopolynomial worst-case bound on the running
time of the primal-dual method. The (practical) average running time of
the method, however, is much better than the one suggested by this bound.
It is possible to convert the algorithm to a polynomial one by using scaling
procedures; see Edmonds and Karp [1972], and Bland and Jensen [1985].
Unfortunately, these procedures do not seem to improve the algorithm’s
performance in practice.
Despite the fundamentally different principles underlying the simplex
and the primal-dual methods (primal cost versus dual cost improvement),
these methods are surprisingly related. It can be shown that the big-M
version of the simplex method with a particular pivot selection rule is equiv-
alent to the steepest ascent version of the primal-dual method, where the
starting set of nodes I consists of all i with gi > 0 (Zadeh [1979]). This
suggests that the simplex method with the empirically best pivot selec-
tion rule should be more efficient in practice than the primal-dual method.
Computational experience tends to agree with this conjecture. However, as
noted in Section 6.4, in many practical contexts, the primal-dual method
has an advantage: it can easily use a good starting flow and price vector
pair, obtained for example from the solution of a slightly different problem
by modifying some of the arc flows to satisfy CS; this is true of all the
Sec. 6.6 Notes, Sources, and Exercises 247
methods of this chapter and the next. Simplex methods are generally less
capable of exploiting such prior knowledge.
Primal-dual methods have a long history, yet it is not clear that their
potential has been exhausted. Most of the implementations of the se-
quential shortest path approach use a version of Dijkstra’s algorithm as
a subroutine, which makes it hard to transfer price information from one
iteration to the next. In particular, the labels di used in the shortest path
step described in Section 6.2 are reinitialized following each augmentation.
It would be more sensible to use an alternative shortest path method,
which allows some information transfer between shortest path construc-
tions. One such method, based on the auction/shortest path algorithm,
is given in Section 7.5, but other possibilities, based for example on label
correcting methods, have not been sufficiently explored.
The relaxation method was first proposed in the context of the as-
signment problem by Bertsekas [1981], and was extended to the general
minimum cost flow problem by Bertsekas [1985]. An implementation of
the method, the RELAX code, was given by Bertsekas and Tseng [1988b],
[1990], [1994]. Extensions for problems with noninteger data, and for net-
works with gains are given in Tseng [1986], and Bertsekas and Tseng
[1988a]. The method has also been extended to general linear programs
(Tseng [1986], and Tseng and Bertsekas [1987]), to network flow problems
with convex arc cost functions (Bertsekas, Hosein, and Tseng [1987]), to
monotropic programming problems (Tseng and Bertsekas [1990]), and to
large scale linear programs with a decomposable structure and side con-
straints (Tseng [1991]).
Extensive computational experience with randomly generated prob-
lems shows that the relaxation method typically outperforms primal-dual
methods substantially for general minimum cost flow problems. In fact,
primal-dual methods can often be speeded up considerably by initialization
with a number of single-node relaxation iterations, although apparently not
to the point of challenging the relaxation method.
The comparison between the relaxation method and simplex meth-
ods is less clear, although the relaxation method seems much faster for
randomly generated problems. The relaxation method is also more capa-
ble of exploiting prior knowledge about an optimal solution; this advantage
is shared with the primal-dual method. On the other hand, in contrast with
the simplex method, the relaxation method requires that the problem data
be integer (or rational, since by multiplication with a suitable integer, ra-
tional problem data can be turned to integer). Modified versions of the
relaxation method that can handle irrational problem data are available
(Tseng [1986], and Bertsekas and Tseng [1988a]). These methods, how-
ever, need not terminate, although they can be shown to yield optimal
solutions asymptotically.
The preceding empirical comparisons between the simplex, primal-
dual, and relaxation methods are only meant to provide a general guide,
248 Dual Ascent Methods Chap. 6
which, however, has many exceptions. A lot of the documented compara-
tive computational tests use randomly generated problems that either have
a randomly obtained graph or a highly artificial graph, such as a grid. On
the other hand, special types of practical problems may have a structure
that is not captured by random generators. As a result, two codes may
compare quite differently for randomly generated problems and for spe-
cific types of practical problems. Practical experience has shown that an
important structural characteristic of the problem’s graph is its diameter
(even though the diameter does not appear in any of the known complexity
estimates for the minimum cost flow problem). Generally, the performance
of all the algorithms discussed in this book tends to deteriorate as the
graph diameter becomes relatively large (as for example in grid graphs).
However, a relatively large graph diameter affects the performance of the
primal-simplex method less than it affects the primal-dual and the relax-
ation methods. A plausible conjecture here is that when the graph diam-
eter is large, the cycles that the simplex method constructs, as well the
augmenting paths that the dual ascent methods use, tend to have many
arcs. This has an adverse effect on the amount of computation needed by
both types of methods, but the effect on the dual ascent methods seems to
be more serious because of the special nature of the data structures that
they use and the associated computations. A related phenomenon may be
conjectured for the case of the auction algorithms of the next chapter. It
may be said that there is no universally best method, so for challenging
problems, it is advisable to try a variety of methods.
E X E R C I S E S
6.1
Use the primal-dual method and the sequential shortest path method to solve
the problem of Fig. 6.10. Verify that the two methods yield the same sequence of
flows and prices (with identical initial data and appropriate choices of the initial
sets I and augmenting paths).
6.2 (Relation of Primal-Dual and Ford-Fulkerson)
Consider the Ford-Fulkerson algorithm for the max-flow problem, where bij = 0
for all (i, j) ∈ A. Show that the method can be interpreted as an application of
the primal-dual method to the minimum cost flow formulation of the max-flow
problem of Example 1.3 in Section 1.2, starting with p = 0 and x = 0 [except for
Sec. 6.6 Notes, Sources, and Exercises 249
5/2
2/3
6/2
2/1
3/1
3/10
2/10
0/51
2
1
2
1 0
3
2
5
-2/1
0
Figure 6.10: Minimum cost flow
problem for Exercise 6.1. The
cost/upper flow bound pair are
shown next to each arc (the lower
flow bound is 0). The supply or
demand is shown next to each
node.
the flow of the artificial arc (t, s), which must be at its upper bound to satisfy CS].
Show in particular that all iterations of the primal-dual method start at node s
and terminate with an augmentation along a path ending at node t. Furthermore,
the method executes only one price change, which occurs after a minimum cut is
identified. The last iteration consists of an augmentation along the artificial arc
(t, s).
6.3 (Relation of Primal-Dual and Dijkstra)
Consider the shortest path problem with node 1 being the origin and all other
nodes being destinations. Formulate this problem as a minimum cost flow prob-
lem with the origin having supply N − 1 and all destinations having demand 1.
Assume that all arc lengths are nonnegative. Start with all flows and prices equal
to zero, and apply the primal-dual method. Show that the method is equivalent
to Dijkstra’s algorithm. In particular, each augmentation uses a shortest path
from the origin to some destination, the augmentations are done in the order of
the destinations’ proximity to the origin, and upon termination, p1 −pi gives the
shortest distance from 1 to each destination i that can be reached from the origin
via a forward path.
6.4 (Noninteger Problem Data)
Verify that the primal-dual method terminates even when the arc costs are non-
integer. (Note, however, that the arc flow bounds must still be integer; the max-
flow example of Exercise 3.7 in Chapter 3 applies to the primal-dual method as
well, in view of the relation described in Exercise 6.2.) Modify the primal-dual
method so that augmenting paths have as few arcs as possible. Show that with
this modification, the arc flow bounds need not be integer for the method to
terminate. How should the sequential shortest path method be modified so that
it terminates even when the problem data are not integer?
6.5
Use the relaxation method to solve the problem of Fig. 6.10.
250 Dual Ascent Methods Chap. 6
6.6 (An Infeasibility Test for the Relaxation Method)
Consider the relaxation method, let p0i be the initial price of node i, and let M
be the set of nodes that have negative surplus initially. For every simple path
P that ends at a node j ∈ M, let HP be the sum of the costs of the forward
arcs of the path minus the sum of the costs of the backward arcs of the path,
and let H = maxP HP . Show that, if the problem is feasible, then during the
course of the algorithm, the price of any positive surplus node cannot exceed its
initial price by more than H + maxj∈M p
0
j − mini∈N p0i . Discuss how to use this
bound to test for problem infeasibility in the relaxation method. Hint : Observe
that at any point in the algorithm the prices of all nodes with negative surplus
have not changed since the start of the algorithm. Show also that if i is a node
with positive surplus, there must exist some node with negative surplus j and an
unblocked path starting at i and ending at j.
6.7
Write the form of the relaxation iteration starting from both positive and negative
surplus nodes. Show that the method terminates at an optimal flow-price vector
pair if a feasible solution exists. Hint : Show that each price change improves the
dual cost by an integer amount, while there can be only a finite number of flow
augmentations between successive price changes.
7
Auction Algorithms
Contents
7.1. The Auction Algorithm for the Assignment Problem
7.1.1. The Main Auction Algorithm
7.1.2. Approximate Coordinate Descent Interpretation
7.1.3. Variants of the Auction Algorithm
7.1.4. Computational Complexity – -Scaling
7.1.5. Dealing with Infeasibility
7.2. Extensions of the Auction Algorithm
7.2.1. Reverse Auction
7.2.2. Auction Algorithms for Asymmetric Assignment
7.2.3. Auction Algorithms with Similar Persons
7.3. The Preflow-Push Algorithm for Max-Flow
7.3.1. Analysis and Complexity
7.3.2. Implementation Issues
7.3.3. Relation to the Auction Algorithm
7.4. The -Relaxation Method
7.4.1. Computational Complexity – -Scaling
7.4.2. Implementation Issues
7.5. The Auction/Sequential Shortest Path Algorithm
7.6. Notes, Sources, and Exercises
251
252 Auction Algorithms Chap. 7
In this chapter we discuss the third major class of algorithms for minimum
cost flow problems. These algorithms stem from and indeed are mathe-
matically equivalent to the auction algorithm for the assignment problem,
described in Section 1.3.3. The underlying reason is that the minimum cost
flow problem can be transformed into an assignment problem, as shown in
Section 4.2 and as will be discussed in more detail in Section 7.3.3.
Contrary to the algorithms of the preceding chapters, the algorithms
of this chapter do not rely on cost improvement. At any one iteration, they
may deteriorate both the primal and the dual cost. On the other hand,
they can be interpreted as approximate coordinate ascent methods, as will
be discussed in Section 7.1 for the case of an assignment problem, and in
Section 7.4 for the general minimum cost flow problem.
Because all the major insights regarding auction algorithms can be
obtained via the assignment problem, we pay particular attention to this
problem, and we develop in detail the corresponding convergence and com-
putational complexity theory in Section 7.1. In Section 7.2, we develop
auction algorithms for special types of assignment problems. In Section 7.3,
we analyze in some detail the preflow-push algorithm for max-flow, and we
derive the computational complexity of some of its implementations. We
also show that this algorithm is mathematically equivalent to applying auc-
tion to a special type of assignment problem. Finally, in Sections 7.4 and
7.5, we analyze in some detail two auction algorithms for the minimum cost
flow problem, the -relaxation method and the auction/sequential shortest
path algorithm, respectively.
Generally, auction algorithms perform well in practice, particularly
for some simple types of minimum cost flow problems, such as assignment
and max-flow. Furthermore, they have excellent computational complexity
properties. Their running times are competitive and often superior to those
of their primal and dual cost improvement competitors, as we will show
in this chapter and in Chapter 9, in the context of the convex separable
network flow problem.
7.1 THE AUCTION ALGORITHM FOR THE ASSIGNMENT
PROBLEM
In this section we consider the assignment problem where we want to match
n persons and n objects on a one-to-one basis. We are given a “value” or
“benefit” aij for matching person i with object j, and we want to assign
persons to objects so as to maximize the total benefit. The set of objects
to which person i can be assigned is a nonempty set denoted A(i). The set
of all possible pairs that can be assigned is denoted by A,
A =
{
(i, j) | j ∈ A(i), i = 1, . . . , n
}
.
Sec. 7.1 The Auction Algorithm for the Assignment Problem 253
Note that A is the set of arcs of the underlying assignment graph. The
number of elements of A is denoted by A.
An assignment S is a (possibly empty) set of person-object pairs (i, j)
such that j ∈ A(i) for all (i, j) ∈ S; for each person i there can be at most
one pair (i, j) ∈ S; and for every object j there can be at most one pair
(i, j) ∈ S. Given an assignment S, we say that person i is assigned if
there exists a pair (i, j) ∈ S; otherwise we say that i is unassigned . We
use similar terminology for objects. An assignment is said to be feasible
or complete if it contains n pairs, so that every person and every object is
assigned; otherwise the assignment is called partial .
We call the problem just described the symmetric assignment prob-
lem, to distinguish it from the asymmetric assignment problem where the
number of persons is smaller than the number of objects. We will discuss
the asymmetric problem and associated auction algorithms later in Section
7.2.
7.1.1 The Main Auction Algorithm
We recall the auction algorithm, described somewhat loosely in Section
1.3.3. It was motivated by the simpler but flawed naive auction algorithm.
A key notion, which made possible the correct operation of the algorithm
was the notion of -complementary slackness (-CS for short) that relates
a partial assignment S and a price vector p = (p1, . . . , pn). We say that S
and p satisfy -CS if for every pair (i, j) ∈ S, object j is within  of being
the “best” object for person i, i.e.,
aij − pj ≥ max
k∈A(i)
{aik − pk} − , ∀ (i, j) ∈ S. (7.1)
The auction algorithm proceeds iteratively and terminates when a
complete assignment is obtained. At the start of the generic iteration we
have a partial assignment S and a price vector p that satisfy -CS. As
an initial choice, we may use an arbitrary set of prices together with the
empty assignment, which trivially satisfies -CS. We will show later that the
iteration preserves the -CS condition. The iteration consists of two phases:
the bidding phase and the assignment phase, which we now describe.
Bidding Phase of the Auction Iteration
Let I be a nonempty subset of persons i that are unassigned under the
assignment S. For each person i ∈ I:
1. Find a “best” object ji having maximum value, i.e.,
ji = arg max
j∈A(i)
{aij − pj},
254 Auction Algorithms Chap. 7
and the corresponding value
vi = max
j∈A(i)
{aij − pj}, (7.2)
and find the best value offered by objects other than ji
wi = max
j∈A(i), j =ji
{aij − pj}. (7.3)
[If ji is the only object in A(i), we define wi to be −∞, or for
computational purposes, a number that is much smaller than vi.]
2. Compute the “bid” of person i given by
biji = pji + vi − wi +  = aiji − wi + . (7.4)
(Abusing terminology somewhat, we say that person i bid for
object ji, and that object ji received a bid from person i.)
Assignment Phase of the Auction Iteration
For each object j, let P (j) be the set of persons from which j received a
bid in the bidding phase of the iteration. If P (j) is nonempty, increase
pj to the highest bid,
pj := max
i∈P (j)
bij , (7.5)
remove from the assignment S any pair (i, j) (if j was assigned to some
i under S), and add to S the pair (ij , j), where ij is a person in P (j)
attaining the maximum above.
Note that there is some freedom in choosing the subset of persons I
that bid during an iteration. One possibility is to let I consist of a sin-
gle unassigned person. This version is known as the Gauss-Seidel version
because of its similarity with Gauss-Seidel methods for solving systems of
nonlinear equations, and usually works best in a serial computing environ-
ment. The version where I consists of all unassigned persons is the one
best suited for parallel computation; it is known as the Jacobi version be-
cause of its similarity with Jacobi methods for solving systems of nonlinear
equations.
During an iteration, the objects whose prices are changed are the
ones that received a bid during the iteration. Each price change involves
an increase of at least . To see this, note that if person i bids for object
Sec. 7.1 The Auction Algorithm for the Assignment Problem 255
ji, from Eqs. (7.2)-(7.4) the corresponding bid is
biji = aiji − wi +  ≥ aiji − vi +  = pji + ,
and exceeds the object’s current price by at least . At the end of the
iteration, we have a new assignment that differs from the preceding one in
that each object that received a bid is now assigned to some person that
was unassigned at the start of the iteration. However, the assignment at
the end of the iteration need not have more pairs than the one at the start
of the iteration, because it is possible that all objects that received a bid
were assigned at the start of the iteration.
The choice of bidding increment [cf. Eq. (7.4)] is such that -CS is
preserved by the algorithm, as shown by the following proposition (in fact,
it can be seen that it is the largest bidding increment for which this is so).
Proposition 7.1: The auction algorithm preserves -CS throughout
its execution; that is, if the assignment and the price vector avail-
able at the start of an iteration satisfy -CS, the same is true for the
assignment and the price vector obtained at the end of the iteration.
Proof: Let pj and p′j be the object prices before and after a given iteration,
respectively. Suppose that object j∗ received a bid from person i and was
assigned to i during the iteration. Then we have [see Eqs. (7.4) and (7.5)]
p′j∗ = aij∗ − wi + .
Using this equation, we obtain
aij∗ − p′j∗ = wi −  = max
j∈A(i), j =j∗
{aij − pj} − .
Since p′j ≥ pj for all j, this equation implies that
aij∗ − p′j∗ ≥ max
j∈A(i)
{aij − p′j} − , (7.6)
which shows that the -CS condition (7.1) continues to hold after the assign-
ment phase of an iteration for all pairs (i, j∗) that entered the assignment
during the iteration.
Consider also any pair (i, j∗) that belonged to the assignment just
before the iteration, and also belongs to the assignment after the iteration.
Then, j∗ must not have received a bid during the iteration, so p′j∗ = pj∗ .
Therefore, Eq. (7.6) holds in view of the -CS condition that held prior to
the iteration and the fact p′j ≥ pj for all j. Hence, the -CS condition (7.1)
256 Auction Algorithms Chap. 7
holds for all pairs (i, j∗) that belong to the assignment after the iteration,
proving the result. Q.E.D.
The next result establishes the validity of the algorithm. The proof
relies on the following observations:
(a) Once an object is assigned, it remains assigned throughout the re-
mainder of the algorithm’s duration. Furthermore, except at termi-
nation, there will always exist at least one object that has never been
assigned, and has a price equal to its initial price. The reason is that
a bidding and assignment phase can result in a reassignment of an
already assigned object to a different person, but cannot result in the
object becoming unassigned.
(b) Each time an object receives a bid, its price increases by at least 
[see Eqs. (7.4) and (7.5)]. Therefore, if the object receives a bid an
infinite number of times, its price increases to ∞.
(c) Every |A(i)| bids by person i, where |A(i)| is the number of objects
in the set A(i), the scalar vi defined by the equation
vi = max
j∈A(i)
{aij − pj} (7.7)
decreases by at least . The reason is that a bid by person i either
decreases vi by at least , or else leaves vi unchanged because there is
more than one object j attaining the maximum in Eq. (7.7). However,
in the latter case, the price of the object ji receiving the bid will
increase by at least , and object ji will not receive another bid by
person i until vi decreases by at least . The conclusion is that if a
person i bids an infinite number of times, vi must decrease to −∞.
Proposition 7.2: If at least one feasible assignment exists, the auc-
tion algorithm terminates with a feasible assignment that is within n
of being optimal (and is optimal if the problem data are integer and
 < 1/n).
Proof: We argue by contradiction. If termination did not occur, the subset
J∞ of objects that received an infinite number of bids is nonempty. Also,
the subset of persons I∞ that bid an infinite number of times is nonempty.
As argued in (b) above, the prices of the objects in J∞ must tend to ∞,
while as argued in (c) above, the scalars vi = maxj∈A(i){aij − pj} must
decrease to −∞ for all persons i ∈ I∞. In view of -CS, this implies that
A(i) ⊂ J∞, ∀ i ∈ I∞, (7.8)
Sec. 7.1 The Auction Algorithm for the Assignment Problem 257
and that after a finite number of iterations, each object in J∞ will be
assigned to a person from I∞. Since after a finite number of iterations at
least one person from I∞ will be unassigned at the start of each iteration, it
follows that the number of persons in I∞ is strictly larger than the number
of objects in J∞. This contradicts the existence of a feasible assignment,
since by Eq. (7.8), persons in I∞ can only be assigned to objects in J∞.
Therefore, the algorithm must terminate. The feasible assignment obtained
upon termination satisfies -CS by Prop. 7.1, so by Prop. 1.4 of Section
1.3.3, this assignment is within n of being optimal. Q.E.D.
7.1.2 Approximate Coordinate Descent Interpretation
The Gauss-Seidel version of the auction algorithm resembles coordinate
descent algorithms, and the relaxation method of Chapter 6 in particular,
because it involves the change of a single object price with all other prices
held fixed. In contrast with the relaxation method, however, such a price
change may worsen strictly the value of the dual function
q(p) =
n∑
i=1
max
j∈A(i)
{
aij − pj
}
+
n∑
j=1
pj , (7.9)
which was introduced in Prop. 1.3 of Section 1.3.3.
Generally we can interpret the bidding and assignment phases as a
simultaneous “approximate” coordinate descent step for all price coordi-
nates that increase during the iteration. The coordinate steps are aimed
at minimizing approximately the dual function. In particular, it can be
shown that the price pj of each object j that receives a bid during the as-
signment phase is increased to either a value that minimizes q(p) when all
other prices are kept constant or else exceeds the largest such value by no
more than .
Figure 7.1 shows this property and suggests that the amount of dete-
rioration of the dual cost is at most . Indeed, for the Gauss-Seidel version
of the algorithm this can be deduced from the argument given in Fig. 7.1
and is left for the reader as Exercise 7.1.
7.1.3 Variants of the Auction Algorithm
There are several variants of the auction algorithm that differ from each
other in small details. For example, as mentioned earlier, one or several
persons may bid simultaneously with objects being awarded to the highest
bidders, the price increment may be slightly different than the one of Eq.
(7.5), etc. The important ingredients of the method are that for each
iteration:
(a) -CS is maintained.
258 Auction Algorithms Chap. 7
Dual cost along pj
Slope = -3
Slope = -2
Slope = -1
Slope = 0
Slope =1
Breakpoints y  ; these are the price
levels at which j becomes the best
object for various persons i
Highest possible bid level of p  after 
the assignment phase
j
ε
Range of possible values of p
after an iteration at which
p  is increasedj
j
p
j
ij
Figure 7.1: Form of the dual cost along the price coordinate pj . From the
definition (7.9) of the dual cost q, the right directional derivative of q along pj is
d+j = 1 − (number of persons i with j ∈ A(i) and pj < yij),
where
yij = aij − max
k∈A(i), k =j
{aik − pk}
is the level of pj below which j is the best person for person i. The break points
are yij for all i such that j ∈ A(i). Let y = max{i|j∈A(i)}{aij − pj}, let i be a
person such that y = y
ij
, let ŷ = max{i|j∈A(i), i=i}{aij − pj}, let î be a person
such that î = i and ŷ = yîj . Note that the interval [ŷ, y] is the set of points that
minimize q along the coordinate pj .
Let pj be the price of j just before an iteration at which j receives a bid and
let p′j be the price of j after the iteration. We claim that ŷ ≤ p′j ≤ y + . Indeed,
if i is the person that bids and wins j during the iteration, then p′j = yij + ,
implying that p′j ≤ y + . To prove that p′j ≥ ŷ, we note that if pj ≥ ŷ, we must
also have p′j ≥ ŷ, since p′j ≥ pj . On the other hand, if p′j < ŷ, there are two
possibilities:
(1) At the start of the iteration, i was not assigned to j. In this case, either i was
unassigned in which case i will bid for j so that p′j = y + , or else i was assigned
to an object j = j, in which case by -CS,
a
ij
− pj −  ≤ ai j − pj ≤ max
k∈A(i), k =j
{a
ik
− pk} = aij − y.
Thus, pj ≥ y − , implying that p′j ≥ y (since a bid increases a price by at least
). In both cases we have p′j ≥ y ≥ ŷ.
(2) At the start of the iteration, i was assigned to j. In this case, î was not
assigned to j, so by repeating the argument of the preceding paragraph with î
and ŷ replacing i and y, respectively, we obtain p′j ≥ ŷ.
Sec. 7.1 The Auction Algorithm for the Assignment Problem 259
(b) At least one unassigned person gets assigned to some object, and the
price of this object is increased by at least β, where β is some fixed
positive constant. Furthermore, the person previously assigned to
an object that receives a bid during the iteration (if any) becomes
unassigned.
(c) No price is decreased and every object that was assigned at the start
of the iteration remains assigned at the end of the iteration (although
the person assigned to it may change).
Any variant of the auction algorithm that obeys these three rules can be
readily shown to have the termination property given in Prop. 7.2.
For example, in Section 7.2.3, we will focus on a special type of assign-
ment problem, which involves groups of persons that are indistinguishable
in the sense that they can be assigned to the same objects and with the
same corresponding benefits. We will develop there a special variant of
the auction algorithm that combines many bids into a “collective” bid for
an entire group of similar persons. Not only this improves the efficiency
of the method, but it also provides the vehicle for extending the auction
algorithm to other problems, such as max-flow and minimum cost flow.
7.1.4 Computational Complexity – -Scaling
As discussed in Section 1.3.3, the running time of the auction algorithm
can depend strongly on the value of  as well as the maximum absolute
object value
C = max
(i,j)∈A
|aij |.
In practice, the dependence of the running time on  and C can be signif-
icant, as can be seen in the examples of Section 1.3.3 (cf. Figs. 1.13 and
1.14).
The practical performance of the auction algorithm is often consid-
erably improved by using the idea of -scaling , which was briefly discussed
in Section 1.3.3. -scaling consists of applying the algorithm several times,
starting with a large value of  and successively reducing  up to some fi-
nal value  such that n is deemed sufficiently small (cf. Prop. 7.2). Each
application of the algorithm, called a scaling phase, provides good initial
prices for the next application. The value of  used for the (k +1)st scaling
phase is denoted by k. The sequence k is generated by
k+1 =
k
θ
, k = 0, 1, . . . , (7.10)
260 Auction Algorithms Chap. 7
where 0 is a suitably chosen starting value of , and θ is an integer with
θ > 1.†
In this section we derive an estimate of the worst-case running time
of the auction algorithm with -scaling. This estimate is O
(
nA ln(0/)
)
,
where A is the number of arcs in the underlying graph of the assignment
problem, and 0 and  are the initial and final values of , respectively. Our
analysis requires a few assumptions about the way the auction algorithm
and the scaling process are implemented. In particular:
(a) We assume that a Gauss-Seidel implementation is used, where only
one person submits a bid at each iteration.
(b) We require that each scaling phase begins with the empty assignment.
(c) We require that the initial prices for the first scaling phase are 0, and
the initial prices for each subsequent phase are the final prices of the
preceding phase. Furthermore, at each scaling phase, we introduce a
modification of the scalars aij , which will be discussed later.
(d) We introduce a data structure, which ensures that the bid of a person
is efficiently computed.
The above requirements are essential for obtaining a favorable worst-case
estimate of the running time. It is doubtful, however, that strict adherence
to these requirements is essential for good practical performance.
We first focus on the case where  is fixed . For the data structure
mentioned in (d) above to work properly, we must assume that the values
aij − pj are integer multiples of  throughout the auction algorithm. This
will be so if the aij and the initial prices pj are integer multiples of , since
in this case it is seen that the bidding increment, as given by Eq. (7.4),
will be an integer multiple of . (We will discuss later how to fulfill the
requirement that  evenly divides the aij and the initial pj .) To motivate
the data structure, suppose that each time a person i scans all the objects
j ∈ A(i) to calculate a bid for the best object ji, he/she records in a list
denoted Cand(i) all the objects j 	= ji that are tied for offering the best
† In practice, if aij are integer, they are usually first multiplied by n + 1 and
the auction algorithm is applied with progressively lower values of , to the point
where  becomes 1 or smaller. In this case, typical values for sparse problems,
where A << n2, are
nC
5
≤ 0 ≤ nC, 4 ≤ θ ≤ 10.
For nonsparse problems, sometimes 0 = 1, which in effect bypasses -scaling,
works quite well. Note also that practical implementations of the auction al-
gorithm sometimes use an adaptive form of -scaling, whereby, within the kth
scaling phase, the value of  is gradually increased to the value k given above,
starting from a relatively small value, based on the results of the computation.
Sec. 7.1 The Auction Algorithm for the Assignment Problem 261
value; that is, they attain the maximum in the relation [cf. Eq. (7.2)]
vi = max
j∈A(i)
{aij − pj}. (7.11)
Along with each object j ∈ Cand(i), the price p′j of j that prevailed for
j at the time of the last scan of j is also recorded. The list Cand(i) is
called the candidate list of i, and can be used to save some computation in
iterations where there are ties in the best object calculation of Eq. (7.11).
In particular, if node i is unassigned and its candidate list Cand(i) contains
an object j whose current price pj is equal to the price p′j , we know that j
is the best object for i. Furthermore, the presence of a second object j in
the list with pj = p′j indicates that the bidding increment is exactly equal
to . This suggests the following implementation for a bid of a person i,
which will be assumed in the subsequent Prop. 7.3.
Bid Calculation
Step 1: Choose an unassigned person i.
Step 2: Examine the pairs (j, p′j) corresponding to the candidate list
Cand(i), starting at the top. Discard any for which p′j < pj . Continue
until reaching the end of the list, or the second element for which
p′j = pj . If the end is reached, empty the candidate list and go to Step
4.
Step 3: Let ji be the first element on the list for which p′j = pj .
Discard the contents of the list up to, but not including, the second
such element. Place a bid on ji at price level pji + , assigning i to ji
and breaking any prior assignment of ji.
Step 4: Scan the objects in A(i), determining an object ji of max-
imum value, the next best value wi, as given by Eq. (7.3), and all
objects (other than ji) tied at value level wi, and record these objects
in the candidate list together with their current prices. Submit a bid
for ji at price level biji , as given by Eq. (7.4), assigning i to ji and
breaking any prior assignment of ji.
We note that candidate lists are often used in the calculations of
various auction algorithms to improve theoretical efficiency. For example
they will also be used later in the algorithms of Sections 7.3 and 7.4.
The complexity analysis of the auction algorithm is based on the
following proposition, which estimates the amount of computation needed
to reduce the violation of CS by a given factor r > 1; that is, to obtain a
feasible assignment and price vector satisfying -CS, starting from a feasible
assignment and price pair satisfying r − CS. Because each price increase
262 Auction Algorithms Chap. 7
is of size at least , the value
vi = max
j∈A(i)
{aij − pj}
decreases by at least  each time the prices pj of all the objects j ∈ A(i) that
attain the maximum above increase by at least . The significance of the
preceding method for bid calculation is that for vi to decrease by at least
, it is sufficient to scan the objects in A(i) in Step 4 only once. Assuming
that the problem is feasible, we will provide in the following proposition an
upper bound on the amount by which vi can decrease, thereby bounding
the number of bids that a person can submit in the course of the algorithm,
and arriving at a running time estimate.
Proposition 7.3: Let the auction algorithm be applied to a feasible
assignment problem, with a given  > 0 and with the bid calculation
method just described. Assume that:
(1) All the scalars aij and all the initial object prices are integer
multiples of .
(2) For some scalar r ≥ 1, the initial object prices satisfy r-CS
together with some feasible assignment.
Then the running time of the algorithm is O(rnA).
Proof: Let p0 be the initial price vector and let S0 be the feasible as-
signment together with which p0 satisfies r-CS. Let also (S, p) be an
assignment-price pair generated by the algorithm prior to termination (so
that S is infeasible). Define for all persons i
vi = max
j∈A(i)
{aij − pj}, v0i = max
j∈A(i)
{aij − p0j}.
The values vi are monotonically nonincreasing in the course of the algo-
rithm. We will show that the differences v0i − vi are upper bounded by
(r + 1)(n − 1).
Let i be a person that is unassigned under S. We claim that there
exists a path of the form
(i, j1, i1, . . . , jm, im, jm+1)
where m ≥ 0 and:
(1) jm+1 is unassigned under S.
(2) If m > 0, then for k = 1, . . . , m, ik is assigned to jk under S and is
assigned to jk+1 under S0.
Sec. 7.1 The Auction Algorithm for the Assignment Problem 263
This can be shown constructively using the following algorithm: Let j1 be
the object assigned to i under S0. If j1 is unassigned under S, stop; else
let i1 be the person assigned to j1 under S, and note that i1 	= i. Let j2
be the person assigned to i1 under S0, and note that j2 	= j1 since j1 is
assigned to i under S0 and i1 	= i. If j2 is unassigned under S, stop; else
continue similarly. This procedure cannot produce the same object twice,
so it must terminate with the properties (1) and (2) satisfied after m + 1
steps, where 0 ≤ m ≤ n − 2.
Since the pair (S0, p0) satisfies r-CS, we have
v0i = max
j∈A(i)
{aij − pj} ≤ aij1 − p0j1 + r,
ai1j1 − p0j1 ≤ ai1j2 − p
0
j2
+ r,
. . .
aimjm − p0jm ≤ aimjm+1 − p0jm+1 + r.
Since the pair (S, p) satisfies -CS, we have
vi ≥ aij1 − pj1 − ,
ai1j1 − pj1 ≥ ai1j2 − pj2 − ,
. . .
aimjm − pjm ≥ aimjm+1 − pjm+1 − .
Since jm+1 is unassigned under S, we have pjm+1 = p
0
jm+1
, so by adding
the preceding inequalities, we obtain the desired relation
v0i − vi ≤ (r + 1)(m + 1) ≤ (r + 1)(n − 1), ∀ i. (7.12)
We finally note that because aij and p0j are integer multiples of ,
all subsequent values of pj , aij − pj , and vi = maxj∈A(i){aij − pj} will
also be integer multiples of . Therefore, with the use of the candidate list
Cand(i), the typical bid calculation, as given earlier, scans only once the
objects in A(i) in Step 4 to induce a reduction of vi by at least . It follows
that the total number of computational operations for the bids of node i is
proportional to (r + 1)(n − 1)|A(i)|, where |A(i)| is the number of objects
in A(i). Thus, the algorithm’s running time is (r +1)(n− 1)
∑n
i=1 |A(i)| =
O(rnA), as claimed. Q.E.D.
264 Auction Algorithms Chap. 7
Complexity with -Scaling
We will now estimate the running time of the auction algorithm with -
scaling. A difficulty here is that in order to use the estimate of Prop.
7.3, the aij and pj at each scaling phase must be integer multiples of the
prevailing  for that phase. We bypass this difficulty as follows:
(a) We start the first scaling phase with pj = 0 for all j.
(b) We use the final prices of each scaling phase as the initial prices for
the next scaling phase.
(c) We choose , the final value of , to divide evenly all the aij . [We
assume that such a common divisor can be found. This will be true
if the aij are rational. Otherwise, the aij may be approximated arbi-
trarily closely, say within some δ > 0, by rational numbers, and the
final assignment will be within n( + δ) of being optimal. If the aij
are integer, we choose  = 1/(n+1), which also guarantees optimality
of the final assignment.] Furthermore, we choose 0 to be equal to a
fraction of the range
C = max
(i,j)∈A
|aij |,
which is fixed and independent of the problem data.
(d) We replace each aij at the beginning of the (k + 1)st scaling phase
with a corrected value akij that is divisible by k. The correction is of
size at most k. In particular, we may use in place of aij ,
akij =
⌈aij
k
⌉
k, ∀ (i, j) ∈ A, k = 0, 1, . . .
However, no correction is made in the last scaling phase, since each
aij is divisible by  [cf. (c) above].
It can be seen that since the a0ij and the initial (zero) pj used in the
first scaling phase are integer multiples of 0, the final prices of the first
scaling phase are also integer multiples of 0, and thus also integer multiples
of 1 = 0/θ (since θ is integer). Therefore, the a1ij and initial pj used in the
second scaling phase are integer multiples of 1, which similarly guarantees
that the final prices of the second scaling phase are also integer multiples
of 2 = 1/θ. Continuing in this manner (or using induction), we see that
the object benefits and prices are integer multiples of the prevailing value
of  throughout the algorithm.
Thus, we can use Prop. 7.3 to estimate the complexity of the (k+1)st
scaling phase as O(rknA), where rk is such that the initial prices pkj of the
scaling phase satisfy rkk-CS with some feasible assignment Sk, and with
respect to the object benefits akij . Take Sk to be the final assignment of the
preceding (the kth) scaling phase, which must satisfy k−1-CS (or θk-CS)
Sec. 7.1 The Auction Algorithm for the Assignment Problem 265
with respect to the object benefits ak−1ij . Since, for all (i, j) ∈ A and k, we
have
|akij − ak−1ij | ≤ |akij − aij | + |aij − ak−1ij | ≤ k + k−1 = (1 + θ)k,
it can be seen, using the definition of -CS, that Sk and pkj must satisfy(
θ + 2(1 + θ)
)
k-CS. It follows that we can use rk = θ + 2(1 + θ) in the
complexity estimate O(rknA) of the (k + 1)st scaling phase. Thus the
running time of all scaling phases except for the first is O(nA). Because 0
is equal to a fixed fraction of the range C, the initial scaling phase will also
have a running time O(nA), since then the initial (zero) price vector will
satisfy r0-CS with any feasible assignment, where r is some fixed constant.
Since k = θk−1 for all k = 0, 1, . . ., the total number of scaling phases is
O
(
log(0/)
)
, and it follows that the running time of the auction algorithm
with -scaling is O
(
nA log(0/)
)
.
Suppose now that the aij are integer, and that we use  equal to
1/(n+1) and 0 equal to a fixed fraction of the benefit range C. Then 0/ =
O(nC), and an optimal assignment will be found with O
(
nA log(nC)
)
com-
putation. This is a worst-case estimate. In practice, the average running
time of the algorithm with -scaling seems to grow proportionally to some-
thing like A log n log(nC); see also Exercise 7.3. Exercise 7.20 shows how
to combine the auction algorithm with a primal-dual method to achieve an
O
(
n1/2A log(nC)
)
worst-case running time. This is the best running time
known at present for the assignment problem.
We note that the implementation using the candidate lists was im-
portant for the proof of Prop. 7.3 and the O
(
nA log(0/)
)
running time of
the method with -scaling. However, it is doubtful that the overhead for
maintaining the candidate lists is justified. In practice, a simpler implemen-
tation is usually preferred, whereby each person scans all of its associated
objects at each bid, instead of using candidate lists. Also the approach of
modifying the aij to make them divisible by the prevailing value of , while
important for the complexity analysis, is of questionable practical use. It
is simpler and typically as effective in practice to forego this modification.
An alternative approach to the complexity analysis, which uses a slightly
different method for selecting the object that receives a bid, is described
in Section 9.6, in the context of auction algorithms for separable convex
problems.
7.1.5 Dealing with Infeasibility
Since termination of the auction algorithm can only occur with a feasible
assignment, when the problem is infeasible, the auction algorithm will keep
on iterating, as the user is wondering whether the problem is infeasible or
just hard to solve. Thus for problems where existence of a feasible assign-
ment is not known a priori, one must supplement the auction algorithm
266 Auction Algorithms Chap. 7
with a mechanism to detect infeasibility. There are several such mecha-
nisms, which we will now discuss.
One criterion that can be used to detect infeasibility is based on the
maximum values
vi = max
j∈A(i)
{aij − pj}.
It can be shown that if the problem is feasible, then in the course of the
auction algorithm, all of these values will be bounded from below by a
precomputable bound, but if the problem is infeasible, some of these values
will be eventually reduced below this bound. In particular, suppose that
the auction algorithm is applied to a symmetric assignment problem with
initial object prices {p0j}. Then as shown in the proof of Prop. 7.3, if person
i is unassigned with respect to the current assignment S and the problem is
feasible, then there is an augmenting path with respect to S that starts at
i. Furthermore, by adding the -CS condition along the augmenting path,
as in the proof of Prop. 7.3, we obtain
vi ≥ −(2n − 1)C − (n − 1) − max
j
{p0j}, (7.13)
where C = max(i,j)∈A |aij |. If the problem is feasible, then as discussed
earlier, there exists at all times an augmenting path starting at each unas-
signed person, so the lower bound (7.13) on vi will hold for all unassigned
persons i throughout the auction algorithm. On the other hand, if the
problem is infeasible, some persons i will be submitting bids infinitely of-
ten, and the corresponding values vi will be decreasing towards −∞. Thus,
we can apply the auction algorithm and keep track of the values vi as they
decrease. Once some vi gets below its lower bound, we know that the
problem is infeasible.
Unfortunately, it may take many iterations for some vi to reach its
lower bound, so the preceding method may not work well in practice. An
alternative method to detect infeasibility is to convert the problem to a
feasible problem by adding a set of artificial pairs A to the original set A.
The benefits aij of the artificial pairs (i, j) should be very small, so that
none of these pairs participates in an optimal assignment unless the problem
is infeasible. In particular, it can be shown that if the original problem
is feasible, no pair (i, j) ∈ A will participate in the optimal assignment,
provided that
aij < −(2n − 1)C, ∀ (i, j) ∈ A, (7.14)
where C = max(i,j)∈A |aij |. To prove this by contradiction, assume that
by adding to the set A the set of artificial pairs A we create an optimal
assignment S∗ that contains a nonempty subset S of artificial pairs. Then,
for every assignment S consisting exclusively of pairs from the original set
A we must have ∑
(i,j)∈S
aij +
∑
(i,j)∈S∗−S
aij ≥
∑
(i,j)∈S
aij ,
Sec. 7.1 The Auction Algorithm for the Assignment Problem 267
from which ∑
(i,j)∈S
aij ≥
∑
(i,j)∈S
aij −
∑
(i,j)∈S∗−S
aij ≥ −(2n − 1)C.
This contradicts Eq. (7.14). Note that if aij ≥ 0 for all (i, j) ∈ A, the
preceding argument can be modified to show that it is sufficient to have
aij < −(n − 1)C for all artificial pairs (i, j).
On the other hand, the addition of artificial pairs with benefit −(2n−
1)C as per Eq. (7.14) expands the cost range of the problem by a factor
of (2n − 1). In the context of -scaling, this necessitates a much larger
starting value for  and correspondingly large number of -scaling phases.
If the problem is feasible, these extra scaling phases are wasted. Thus for
problems which are normally expected to be feasible, it may be better to
introduce artificial pairs with benefits that are of the order of −C, and then
gradually scale downward these benefits towards the −(2n−1)C threshold if
artificial pairs persist in the assignments obtained by the auction algorithm.
This procedure of scaling downward the benefits of the artificial pairs can
be embedded in a number of ways within the -scaling procedure.
A third method to deal with infeasibility is based on the notion of
maximally feasible flows and the decomposition method discussed in Sec-
tion 3.1.4. It uses the property that even when the problem is infeasible,
the auction algorithm will find an assignment of maximal cardinality in a
finite number of iterations (this can be seen by a simple modification of the
proof of Prop. 7.2). The idea now is to modify the auction algorithm so
that during the first scaling phase we periodically check for the existence
of an augmenting path from some unassigned person to some unassigned
object (we can use a simple search of the breadth-first type, such as the one
of Section 3.2). Once the cardinality of the current assignment becomes
maximal while some person still remains unassigned, this check will es-
tablish that the problem is infeasible. With this modification, the auction
algorithm will either find a feasible assignment and a set of prices satisfying
-CS, or it will establish that the problem is infeasible and simultaneously
obtain an assignment of maximal cardinality. In the former case, the al-
gorithm will proceed with subsequent scaling phases of the algorithm, but
with the breadth-first feature suppressed. In the latter case, we can use the
maximal cardinality assignment obtained to decompose the problem into
two or three component problems, as discussed in Section 3.1.4. Each of
these problems is either a symmetric or an asymmetric assignment problem,
which can be solved separately (see also Exercise 3.18).
Note a nice feature of the approach just described: In the case of a
feasible problem, it involves little additional computation (the breadth-first
searches of the first scaling phase) over the unmodified algorithm. In the
case of an infeasible problem, the computation of the first scaling phase is
not wasted, since it provides good starting prices for the subsequent scaling
phases.
268 Auction Algorithms Chap. 7
7.2 EXTENSIONS OF THE AUCTION ALGORITHM
The auction algorithm can be extended to deal effectively with the special
features of modified versions of the assignment problem. In this section,
we develop several such extensions.
7.2.1 Reverse Auction
In the auction algorithm, persons compete for objects by bidding and rais-
ing the price of their best object. It is possible to use an alternative form of
the auction algorithm, called reverse auction, where, roughly, the objects
compete for persons by essentially offering discounts.
To describe this algorithm, we introduce a profit variable πi for each
person i. Profits play for persons a role analogous to the role prices play
for objects. We can describe reverse auction in two equivalent ways: one
where unassigned objects lower their prices as much as possible to attract
an unassigned person or to lure a person away from its currently held object
without violating -CS, and another where unassigned objects select a best
person and raise his or her profit as much as possible without violating -
CS. For analytical convenience, we will adopt the second description rather
than the first, leaving the proof of their equivalence as Exercise 7.8 for the
reader.
Let us consider the following -CS condition for a (partial) assignment
S and a profit vector π:
aij − πi ≥ max
k∈B(j)
{akj − πk} − , ∀ (i, j) ∈ S, (7.15)
where B(j) is the set of persons that can be assigned to object j,
B(j) =
{
i | (i, j) ∈ A
}
.
We assume that this set is nonempty for all j, which is of course required
for feasibility of the problem. Note the symmetry of this condition with the
corresponding one for prices; cf. Eq. (7.1). The reverse auction algorithm
starts with and maintains an assignment and a profit vector π satisfying
the above -CS condition. It terminates when the assignment is feasible.
At the beginning of each iteration, we have an assignment S and a profit
vector π satisfying the -CS condition (7.15).
Iteration of Reverse Auction
Let J be a nonempty subset of objects j that are unassigned under
the assignment S. For each object j ∈ J :
Sec. 7.2 Extensions of the Auction Algorithm 269
1. Find a “best” person ij such that
ij = arg max
i∈B(j)
{aij − πi},
and the corresponding value
βj = max
i∈B(j)
{aij − πi},
and find
ωj = max
i∈B(j), i =ij
{aij − πi}.
[If ij is the only person in B(j), we define ωj to be −∞ or, for
computational purposes, a number that is much smaller than βj .]
2. Each object j ∈ J bids for person ij an amount
bijj = πij + βj − ωj +  = aijj − ωj + .
3. For each person i that received at least one bid, increase πi to
the highest bid,
πi := max
j∈P (i)
bij ,
where P (i) is the set of objects from which i received a bid;
remove from the assignment S any pair (i, j) (if i was assigned
to some j under S), and add to S the pair (i, ji), where ji is an
object in P (i) attaining the maximum above.
Note that reverse auction is identical to (forward) auction with the
roles of persons and objects, and the roles of profits and prices interchanged.
Thus, by using the corresponding (forward) auction result (cf. Prop. 7.2),
we have the following proposition.
Proposition 7.4: If at least one feasible assignment exists, the re-
verse auction algorithm terminates with a feasible assignment that is
within n of being optimal (and is optimal if the problem data are
integer and  < 1/n).
Combined Forward and Reverse Auction
One of the reasons we are interested in reverse auction is to construct
270 Auction Algorithms Chap. 7
algorithms that switch from forward to reverse auction and back. Such
algorithms must simultaneously maintain a price vector p satisfying the
-CS condition (7.1) and a profit vector π satisfying the -CS condition
(7.15). To this end we introduce an -CS condition for the pair (π, p),
which (as we will see) implies the other two. Maintaining this condition is
essential for switching gracefully between forward and reverse auction.
Definition 7.1: An assignment S and a pair (π, p) are said to satisfy
-CS if
πi + pj ≥ aij − , ∀ (i, j) ∈ A, (7.16)
πi + pj = aij , ∀ (i, j) ∈ S. (7.17)
We have the following proposition.
Proposition 7.5: Suppose that an assignment S together with a
profit-price pair (π, p) satisfy -CS. Then:
(a) S and π satisfy the -CS condition
aij − πi ≥ max
k∈B(j)
{akj − πk} − , ∀ (i, j) ∈ S. (7.18)
(b) S and p satisfy the -CS condition
aij − pj ≥ max
k∈A(i)
{aik − pk} − , ∀ (i, j) ∈ S. (7.19)
(c) If S is feasible, then S is within n of being an optimal assign-
ment.
Proof: (a) In view of Eq. (7.17), for all (i, j) ∈ S, we have pj = aij − πi,
so Eq. (7.16) implies that aij − πi ≥ akj − πk −  for all k ∈ B(j). This
shows Eq. (7.18).
(b) The proof is similar to part (a), with the roles of π and p interchanged.
(c) Since by part (b) the -CS condition (7.19) is satisfied, Prop. 1.4 of
Section 1.3.3 implies that S is within n of being optimal. Q.E.D.
We now introduce a combined forward/reverse auction algorithm.
The algorithm starts with and maintains an assignment S and a profit-price
pair (π, p) satisfying the -CS conditions (7.16) and (7.17). It terminates
Sec. 7.2 Extensions of the Auction Algorithm 271
when the assignment is feasible.
Combined Forward/Reverse Auction Algorithm
Step 1 (Run forward auction): Execute a finite number of iter-
ations of the forward auction algorithm (subject to the termination
condition), and at the end of each iteration (after increasing the prices
of the objects that received a bid) set
πi = aiji − pji (7.20)
for every person-object pair (i, ji) that entered the assignment during
the iteration. Go to Step 2.
Step 2 (Run reverse auction): Execute a finite number of iterations
of the reverse auction algorithm (subject to the termination condition),
and at the end of each iteration (after increasing the profits of the
persons that received a bid) set
pj = aijj − πij (7.21)
for every person-object pair (ij , j) that entered the assignment during
the iteration. Go to Step 1.
Note that the additional overhead of the combined algorithm over
the forward or the reverse algorithm is minimal; just one update of the
form (7.20) or (7.21) is required per iteration for each object or person
that received a bid during the iteration. An important property is that
these updates maintain the -CS conditions (7.16) and (7.17) for the pair
(π, p), and therefore, by Prop. 7.5, maintain the required -CS conditions
(7.18) and (7.19) for π and p, respectively. This is shown in the following
proposition.
Proposition 7.6: If the assignment and the profit-price pair available
at the start of an iteration of either the forward or the reverse auction
algorithm satisfy the -CS conditions (7.16) and (7.17), the same is
true for the assignment and the profit-price pair obtained at the end
of the iteration, provided Eq. (7.20) is used to update π (in the case
of forward auction), and Eq. (7.21) is used to update p (in the case of
reverse auction).
Proof: Assume for concreteness that forward auction is used, and let (π, p)
and (π, p) be the profit-price pair before and after the iteration, respec-
272 Auction Algorithms Chap. 7
tively. Then, pj ≥ pj for all j (with strict inequality if and only if j
received a bid during the iteration). Therefore, we have πi + pj ≥ aij − 
for all (i, j) such that πi = πi. Furthermore, we have πi+pj = πi+pj = aij
for all (i, j) that belong to the assignment before as well as after the itera-
tion. Also, in view of the update (7.20), we have πi +pji = aiji for all pairs
(i, ji) that entered the assignment during the iteration. What remains is
to verify that the condition
πi + pj ≥ aij − , ∀ j ∈ A(i), (7.22)
holds for all persons i that submitted a bid and were assigned to an object,
say ji, during the iteration. Indeed, for such a person i, we have, by Eq.
(7.4),
pji = aiji − maxj∈A(i), j =ji
{aij − pj} + ,
which implies that
πi = aiji − pji ≥ aij − pj −  ≥ aij − pj − , ∀ j ∈ A(i).
This shows the desired relation (7.22). Q.E.D.
Note that during forward auction the object prices pj increase while
the profits πi decrease, but exactly the opposite happens in reverse auction.
For this reason, the termination proof that we used for forward and for
reverse auction does not apply to the combined method. Indeed, it is
possible to construct examples of feasible problems where the combined
method never terminates if the switch between forward and reverse auctions
is done arbitrarily. However, it is easy to provide a device guaranteeing that
the combined algorithm terminates for a feasible problem; it is sufficient to
ensure that some “irreversible progress” is made before switching between
forward and reverse auction. One easily implementable possibility is to
refrain from switching until the number of assigned person-object pairs
increases by at least one.
The combined forward/reverse auction algorithm often works sub-
stantially faster than the forward version. It seems to be affected less by
“price wars,” that is, protracted sequences of small price rises by a number
of persons bidding for a smaller number of objects. Price wars can still oc-
cur in the combined algorithm, but they arise through more complex and
unlikely problem structures than in the forward algorithm. For this reason
the combined forward/reverse auction algorithm depends less on -scaling
for good performance than its forward counterpart; in fact, starting with
 = 1/(n + 1), thus bypassing -scaling, is sometimes the best choice.
7.2.2 Auction Algorithms for Asymmetric Assignment
Reverse auction can be used in conjunction with forward auction to pro-
vide algorithms for solving the asymmetric assignment problem, where the
Sec. 7.2 Extensions of the Auction Algorithm 273
number of objects n is larger than the number of persons m. Here we still
require that each person be assigned to some object, but we allow objects
to remain unassigned. As before, an assignment S is a (possibly empty)
set of person-object pairs (i, j) such that j ∈ A(i) for all (i, j) ∈ S; for each
person i there can be at most one pair (i, j) ∈ S; and for every object j
there can be at most one pair (i, j) ∈ S. The assignment S is said to be
feasible if all persons are assigned under S.
The corresponding linear programming problem is
maximize
∑
(i,j)∈A
aijxij
subject to
∑
j∈A(i)
xij = 1, ∀ i = 1, . . . , m,
∑
i∈B(j)
xij ≤ 1, ∀ j = 1, . . . , n,
0 ≤ xij , ∀ (i, j) ∈ A.
We can convert this program to the minimum cost flow problem
minimize
∑
(i,j)∈A
(
−aij
)
xij
subject to
∑
j∈A(i)
xij = 1, ∀ i = 1, . . . , m,
∑
i∈B(j)
xij + xsj = 1, ∀ j = 1, . . . , n,
n∑
j=1
xsj = n − m,
0 ≤ xij , ∀ (i, j) ∈ A,
0 ≤ xsj , ∀ j = 1, . . . , n,
by replacing maximization by minimization, by reversing the sign of aij ,
and by introducing a supersource node s, which is connected to each object
node j by an arc (s, j) of zero cost and feasible flow range [0,∞) (see Fig.
7.2).
Using the duality theory of Section 4.2, it can be seen that the corre-
sponding dual problem is
minimize
m∑
i=1
πi +
n∑
j=1
pj − (n − m)λ
subject to πi + pj ≥ aij , ∀ (i, j) ∈ A,
λ ≤ pj , ∀ j = 1, . . . , n,
(7.23)
274 Auction Algorithms Chap. 7
n-m
SUPERSOURCE
1 1
i j
m
- aij
1 1
1 1
1 1
PERSONS OBJECTS
m+1
s n
1
1
m
..
.
..
.
..
.
..
.
..
.
Figure 7.2: Converting an asymmetric assignment problem into a minimum cost
flow problem involving a supersource node s and a zero cost artificial arc (s, j)
with feasible flow range [0,∞) for each object j.
where we have converted maximization to minimization, we have used −πi
in place of the price of each person node i, and we have denoted by λ the
price of the supersource node s.
We now introduce an -CS condition for an assignment S and a pair
(π, p).
Definition 7.2: An assignment S and a pair (π, p) are said to satisfy
-CS if
πi + pj ≥ aij − , ∀ (i, j) ∈ A, (7.24)
πi + pj = aij , ∀ (i, j) ∈ S, (7.25)
pj ≤ min
k: assigned under S
pk, ∀ j that are unassigned under S.
(7.26)
The following proposition clarifies the significance of the preceding
-CS condition.
Sec. 7.2 Extensions of the Auction Algorithm 275
Proposition 7.7: If a feasible assignment S satisfies the -CS con-
ditions (7.24)-(7.26) together with a pair (π, p), then S is within m
of being optimal for the asymmetric assignment problem. The triplet
(π̂, p̂, λ), where
λ = min
k: assigned under S
pk, (7.27)
π̂i = πi + , ∀ i = 1, . . . , m, (7.28)
p̂j =
{
pj if j is assigned under S,
λ if j is unassigned under S,
∀ j = 1, . . . , n, (7.29)
is within m of being an optimal solution of the dual problem (7.23).
Proof: For any feasible assignment
{
(i, ki) | i = 1, . . . , m
}
and for any
triplet (π, p, λ) satisfying the dual feasibility constraints πi + pj ≥ aij for
all (i, j) ∈ A and λ ≤ pj for all j, we have
m∑
i=1
aiki ≤
m∑
i=1
πi +
m∑
i=1
pki ≤
m∑
i=1
πi +
n∑
j=1
pj − (n − m)λ.
By maximizing over all feasible assignments
{
(i, ki) | i = 1, . . . , m
}
and by
minimizing over all dual-feasible triplets (π, p, λ), we see that
A∗ ≤ D∗,
where A∗ is the optimal assignment value and D∗ is the minimal dual cost.
Let now S =
{
(i, ji) | i = 1, . . . , m
}
be the given assignment satisfying
-CS together with (π, p), and consider the triplet (π̂, p̂, λ) defined by Eqs.
(7.27)-(7.29). Since for all i we have π̂i + p̂ji = aij + , we obtain
A∗ ≥
m∑
i=1
aiji
=
m∑
i=1
π̂i +
m∑
i=1
p̂ji − m
≥
m∑
i=1
π̂i +
n∑
j=1
p̂j − (n − m)λ − m
≥ D∗ − m,
where the last inequality holds because the triplet (π̂, p̂, λ) is feasible for
the dual problem. Since we showed earlier that A∗ ≤ D∗, the desired
conclusion follows. Q.E.D.
276 Auction Algorithms Chap. 7
Consider now trying to solve the asymmetric assignment problem by
means of auction. We can start with any assignment S and pair (π, p)
satisfying the first two -CS conditions (7.24) and (7.25), and perform a
forward auction (as defined earlier for the symmetric assignment problem)
up to the point where each person is assigned to a distinct object. For
a feasible problem, by essentially repeating the proof of Prop. 7.2 for the
symmetric case, it can be seen that this will yield, in a finite number of
iterations, a feasible assignment S satisfying the first two conditions (7.24)
and (7.25). However, this assignment may not be optimal, because the
prices of the unassigned objects j are not minimal; that is, they do not
satisfy the third -CS condition (7.26).
To remedy this situation, we introduce a modified form of reverse
auction to lower the prices of the unassigned objects so that, after several
iterations in which persons may be reassigned to other objects, the third
condition, (7.26), is satisfied. We will show that the assignment thus ob-
tained satisfies all the -CS conditions (7.24)-(7.26), and by Prop. 7.7, is
optimal within m (and thus optimal if the problem data are integer and
 < 1/m).
The modified reverse auction starts with a feasible assignment S and
with a pair (π, p) satisfying the first two -CS conditions (7.24) and (7.25).
[For a feasible problem, such an S and (π, p) can be obtained by regular
forward or reverse auction, as discussed earlier.] Let us denote by λ the
minimal assigned object price under the initial assignment,
λ = min
j: assigned under the
initial assignment S
pj .
The typical iteration of modified reverse auction is the same as the one of
reverse auction, except that only unassigned objects j with pj > λ par-
ticipate in the auction. In particular, the algorithm maintains a feasible
assignment S and a pair (π, p) satisfying Eqs. (7.24) and (7.25), and ter-
minates when all unassigned objects j satisfy pj ≤ λ, in which case it will
be seen that the third -CS condition (7.26) is satisfied as well. The scalar
λ is kept fixed throughout the algorithm.
Iteration of Reverse Auction for Asymmetric Assignment
Select an object j that is unassigned under the assignment S and satis-
fies pj > λ (if no such object can be found, the algorithm terminates).
Find a “best” person ij such that
ij = arg max
i∈B(j)
{aij − πi},
and the corresponding value
Sec. 7.2 Extensions of the Auction Algorithm 277
βj = max
i∈B(j)
{aij − πi}, (7.30)
and find
ωj = max
i∈B(j), i =ij
{aij − πi}. (7.31)
[If ij is the only person in B(j), we define ωj to be −∞.] If λ ≥ βj − ,
set pj := λ and go to the next iteration. Otherwise, let
δ = min{βj − λ, βj − ωj + }. (7.32)
Set
pj := βj − δ, (7.33)
πij := πij + δ, (7.34)
add to the assignment S the pair (ij , j), and remove from S the pair
(ij , j′), where j′ is the object that was assigned to ij under S at the
start of the iteration.
Note that the formula (7.32) for the bidding increment δ is such that
the object j enters the assignment at a price which is no less that λ [and
is equal to λ if and only if the minimum in Eq. (7.32) is attained by the
first term]. Furthermore, when δ is calculated (that is, when λ > βj −
) we have δ ≥ , so it can be seen from Eqs. (7.33) and (7.34) that,
throughout the algorithm, prices are monotonically decreasing and profits
are monotonically increasing. The following proposition establishes the
validity of the method.
Proposition 7.8: The preceding reverse auction algorithm for the
asymmetric assignment problem terminates with an assignment that
is within m of being optimal.
Proof: In view of Prop. 7.7, the result will follow once we prove the fol-
lowing:
(a) The modified reverse auction iteration preserves the first two -CS
conditions (7.24) and (7.25), as well as the condition
λ ≤ min
j: assigned under the
current assignment S
pj , (7.35)
so upon termination of the algorithm (necessarily with the prices of
all unassigned objects less or equal to λ) the third -CS condition,
(7.26), is satisfied.
278 Auction Algorithms Chap. 7
(b) The algorithm terminates.
We will prove these facts in sequence.
We assume that the conditions (7.24), (7.25), and (7.35) are satisfied
at the start of an iteration, and we will show that they are also satisfied at
the end of the iteration. First consider the case where there is no change
in the assignment, which happens when λ ≥ βj − . Then Eqs. (7.25)
and (7.35) are automatically satisfied at the end of the iteration; only pj
changes in the iteration according to
pj := λ ≥ βj −  = max
i∈B(j)
{aij − πi} − ,
so the condition (7.24) is also satisfied at the end of the iteration.
Next consider the case where there is a change in the assignment
during the iteration. Let (π, p) and (π, p) be the profit-price pair before and
after the iteration, respectively, and let j and ij be the object and person
involved in the iteration. By construction [cf. Eqs. (7.33) and (7.34)], we
have πij +pj = aijj , and since πi = πi and pk = pk for all i 	= ij and k 	= j,
we see that the condition (7.25) (πi + pk = aik) is satisfied for all assigned
pairs (i, k) at the end of the iteration.
To show that Eq. (7.24) holds at the end of the iteration, i.e.,
πi + pk ≥ aik − , ∀ (i, k) ∈ A, (7.36)
consider first objects k 	= j. Then, pk = pk and since πi ≥ πi for all i,
the above condition holds, since our hypothesis is that at the start of the
iteration we have πi + pk ≥ aik −  for all (i, k). Consider next the case
k = j. Then condition (7.36) holds for i = ij , since πij + pj = aijj . Also
using Eqs. (7.30)-(7.33) and the fact δ ≥ , we have for all i 	= ij
πi + pj = πi + pj
≥ πi + βj − (βj − ωj + )
= πi + ωj − 
≥ πi + (aij − πi) − 
= aij − ,
so condition (7.36) holds for i 	= ij and k = j, completing its proof. To
see that condition (7.35) is maintained by the iteration, note that by Eqs.
(7.30), (7.31), and (7.33), we have
pj = βj − δ ≥ βj − (βj − λ) = λ.
Finally, to show that the algorithm terminates, we note that in the
typical iteration involving object j and person ij there are two possibilities:
(1) The price of object j is set to λ without the object entering the
assignment; this occurs if λ ≥ βj − .
Sec. 7.2 Extensions of the Auction Algorithm 279
(2) The profit of person ij increases by at least  [this is seen from the
definition (7.32) of δ; we have λ < βj −  and βj ≥ ωj , so δ ≥ ].
Since only objects j with pj > λ can participate in the auction, possibility
(1) can occur only a finite number of times. Thus, if the algorithm does
not terminate, the profits of some persons will increase to ∞. This is
impossible, since when person i is assigned to object j, we must have by
Eqs. (7.25) and (7.35)
πi = aij − pj ≤ aij − λ,
so the profits are bounded from above by max(i,j)∈A aij − λ. Thus the
algorithm must terminate. Q.E.D.
Note that one may bypass the modified reverse auction algorithm by
starting the forward auction with all object prices equal to zero. Upon ter-
mination of the forward auction, the prices of the unassigned objects will
still be at zero, while the prices of the assigned objects will be nonnega-
tive. Therefore the -CS condition (7.26) will be satisfied, and the modified
reverse auction will be unnecessary (see Exercise 7.9).
Unfortunately the requirement of zero initial object prices is incom-
patible with -scaling. The principal advantage offered by the modified
reverse auction algorithm is that it allows arbitrary initial object prices for
the forward auction, thereby also allowing the use of -scaling. This can
be shown to improve the theoretical worst-case complexity of the method,
and is often beneficial in practice.
The method for asymmetric assignment problems just described op-
erates principally as a forward algorithm and uses reverse auction only
near the end, after the forward algorithm has terminated, to rectify viola-
tions of the -CS conditions. An alternative is to switch more frequently
between forward and reverse auction, similar to the algorithm described
earlier in this section for symmetric problems. We refer to Bertsekas and
Castañon [1992] for methods of this type, together with computational
results suggesting a more favorable practical performance over the asym-
metric assignment method given earlier.
Reverse auction can also be used in the context of other types of
network flow problems. One example is the variation of the asymmetric
assignment problem where persons (as well as objects) need not be as-
signed if this degrades the assignment’s value (see Exercise 7.11). Another
assignment-like problem where reverse auction finds use is the multiassign-
ment problem, discussed in Exercise 7.10.
7.2.3 Auction Algorithms with Similar Persons
In this section, we develop an auction algorithm to deal efficiently with
assignment problems that involve groups of persons that are indistinguish-
able in the sense that they can be assigned to the same objects and with
280 Auction Algorithms Chap. 7
the same corresponding benefits. This algorithm provides a general ap-
proach to extend the auction algorithm to the minimum cost flow problem
and some of its special cases, such as the max-flow and the transportation
problems, as we will show in Section 7.3.3.
We introduce the following definition in the context of the asymmetric
or the symmetric assignment problem:
Definition 7.3: We say that two persons i and i′ are similar , if
A(i) = A(i′), and aij = ai′j ∀ j ∈ A(i).
For each person i, the set of all persons similar to i is called the
similarity class of i.
When there are similar persons, the auction algorithm can get bogged
down into a long sequence of bids (known as a “price war”), whereby a
number of similar persons compete for a smaller number of objects by
making small incremental price changes. An example is given in Fig. 7.3.
It turns out that if one is aware of the presence of similar persons, one
can “compress” a price war within a similarity class into a single iteration.
It is important to note that the corresponding algorithm is still a special
case of the auction algorithms of Section 7.1; the computations are merely
streamlined by combining many bids into a “collective” bid by the persons
of a similarity class.
The method to resolve a price war within a similarity class is to let
the auction algorithm run its course, then look at the final results and see
how they can be essentially reproduced with less calculation. In particular,
suppose that we have an assignment-price pair (S, p) satisfying -CS, and
that a similarity class M has m persons, only q < m of which are assigned
under S. Suppose that we restrict the auction algorithm to run within M ;
that is, we require the bidding person to be from M , until all persons in
M are assigned. We call this the M -restricted auction.
The final results of an M -restricted auction are quite predictable. In
particular, the set
Anew = The m objects that are assigned to persons in M at the end
of the M -restricted auction
consists of the set
Aold = The q objects that were assigned to persons in M at the beginning
of the M -restricted auction
plus m − q extra objects that are not in Aold. These extra objects are
those objects not in Aold that offered the best value aij −pj for the persons
Sec. 7.2 Extensions of the Auction Algorithm 281
Initial price = 0
PERSONS OBJECTS
1
2
3
Initial price = 4
Initial price = 0Initially assigned
to object 1
Initially
unassigned
Initially assigned
to object 2
1
2
3
Initially
unassigned
Initial price = 3
4 4
Class of Similar
Persons
Solid lines indicate pairs (i,j) with a   = C >> 1.
Broken lines indicate pairs (i,j) with a   = 0.
The optimal assignment is {(1,1), (2,2), (4,3), (3,4)}.
ij
ij
Figure 7.3: An example of an assignment problem with similar persons. Here
the persons 1, 2, and 3 form a similarity class. This structure induces a price war
in the auction algorithm. The persons 1, 2, and 3 will keep on bidding up the
prices of objects 1 and 2 until the prices p1 and p2 reach a sufficiently high level
(at least C +3), so that either object 3 or object 4 receives a bid from one of these
persons. The price increments will be at most 2.
i ∈ M (under the price vector p that prevailed at the start of the M -
restricted auction). For a more precise description, let us label the set of
objects not in Aold in order of decreasing value, that is,
{j | j /∈ Aold} = {j1, . . . , jm−q, jm−q+1, . . . , jn−q}, (7.37)
where for all persons i ∈ M ,
aijr − pjr ≥ aijr+1 − pjr+1 , r = 1, . . . , n − q − 1. (7.38)
Then
Anew = Aold ∪ {j1, . . . , jm−q}. (7.39)
The price changes of the objects as a result of the M -restricted auction
can also be predicted to a great extent. In particular, the prices of the
objects that are not in Anew will not change, since these objects do not
receive any bid during the M -restricted auction. The ultimate prices of
the objects j ∈ Anew will be such that the corresponding values aij − pj
282 Auction Algorithms Chap. 7
for the persons i ∈ M will all be within  of each other, and will be no less
than the value aijm−q+1 − pjm−q+1 of the next best object jm−q+1 minus .
At this point, to simplify the calculations, we can just raise the prices of
the objects j ∈ Anew so that their final values aij − pj for persons i ∈ M
are exactly equal to the value aijm−q+1 − pjm−q+1 of the next best object
jm−q+1 minus ; that is, we set
pj := aij −
(
aijm−q+1 − pjm−q+1
)
+ , ∀ j ∈ Anew, (7.40)
where i is any person in M . It can be seen that this maintains the -
CS property of the resulting assignment-price pair, and that the desirable
termination properties of the algorithm are maintained (see the discussion
of the variants of the auction algorithm in Section 7.1.3).
To establish some terminology, consider the operation that starts with
an assignment-price pair (p, S) satisfying -CS and a similarity class M that
has m persons, only q of which are assigned under S, and produces through
an M -restricted auction an assignment-price pair specified by Eqs. (7.37)-
(7.40). We call this operation an M -auction iteration. Note that when
the similarity class M consists of a single person, an M -auction iteration
produces the same results as the simpler auction iteration given earlier.
Thus the algorithm that consists of a sequence of M -auction iterations
generalizes the auction algorithm given earlier, and deals effectively with
the presence of similarity classes. The table of Fig. 7.4 illustrates this
algorithm.
Suppose now that this algorithm is started with an assignment-price
pair for which the following property holds:
If AM is the set of objects assigned to persons of a similarity class
M , the values
aij − pj , i ∈ M, j ∈ AM ,
are all equal, and no less than the values offered by all other objects
j /∈ AM minus .
Then it can seen from Eqs. (7.37)-(7.40) that throughout the algorithm
this property is maintained. Thus, if in particular the benefits aij of the
objects in a subset A′M ⊂ AM are equal, the prices pj , j ∈ A′M must all be
equal. This property will be useful in Section 7.3.3, where we will develop
the connection between the auction algorithm and some other price-based
algorithms for the max-flow and the minimum cost flow problems.
7.3 THE PREFLOW-PUSH ALGORITHM FOR MAX-FLOW
In this section, we discuss the preflow-push algorithm for the max-flow
problem. This algorithm was developed independently of the auction algo-
rithm, and was motivated by the notion of a preflow , which is central in the
Sec. 7.3 The Preflow-Push Algorithm for Max-Flow 283
At Start Object Assigned Bidder Preferred
of Itera- Prices Pairs Class M Object(s)
tion #
1 0,0,3,4 (1,1),(2,2) {1, 2, 3} 1,2,3
2 C + 4 + , C + 4 + , 4 + , 4 (1,1),(2,2),(3,3) {4} 3
3 C + 4 + , C + 4 + , C + 4 + , 4 (1,1),(2,2),(4,3) {1, 2, 3} 1,2,4
Final 2C + 4 + 2, 2C + 4 + 2 (1,1),(2,2)
C + 4 + , C + 4 + 2 (4,3),(3,4)
Figure 7.4: Illustration of the algorithm based on M -auction iterations for the
problem of Fig. 7.3. In this example, the initial price vector is (0, 0, 3, 4) and the
initial partial assignment consists of the pairs (1, 1) and (2, 2). We first perform
an M -auction iteration for the similarity class {1, 2, 3}. We then perform an
iteration for person 4, and then again an M -auction iteration for the similarity
class {1, 2, 3}. The last iteration assigns the remaining object 4, and the algorithm
terminates without a price war of the type discussed in Fig. 7.3.
max-flow algorithm of Karzanov [1974] (a preflow is a capacity-feasible flow
vector, which has nonpositive divergence for each node except the source).
In retrospect, however, it was found to be closely related to the auction
algorithm. In particular, we will show in Section 7.3.3 that it is mathemat-
ically equivalent to a version of the auction algorithm applied to a special
type of assignment problem.
We consider the max-flow problem of maximizing the flow out of the
source node 1 (or the flow into the sink node N)∑
{j|(1,j)∈A}
x1j
over all capacity-feasible flow vectors x such that the divergence of every
node i except for the source node 1 and the sink node N is zero. We assume
that the lower arc flow bounds are 0, so the capacity constraints are
0 ≤ xij ≤ cij , ∀ (i, j) ∈ A.
Furthermore, to avoid degenerate cases, we assume that each node has at
least one incident arc and that the upper flow bound cij is positive for each
arc (i, j).
The preflow-push algorithm shares some of the features of the price-
based algorithm for the max-flow problem of Section 3.3. In particular,
both algorithms use prices to guide flow changes, and maintain a valid
284 Auction Algorithms Chap. 7
flow-price pair that satisfies the same -CS condition. Let us introduce
some definitions. Given a capacity-feasible flow vector x, the set of eligible
arcs of i is
A(i, x) =
{
(i, j) | xij < cij
}
∪
{
(j, i) | 0 < xji
}
,
and the corresponding set of eligible neighbors of i is
N(i, x) =
{
j | (i, j) ∈ A(i, x) or (j, i) ∈ A(i, x)
}
.
The candidate list of a node i is defined to be the (possibly empty) set of its
eligible incident arcs (i, j) or (j, i) such that pi = pj +1. A capacity-feasible
flow vector x together with a price vector p = {pi | i ∈ N} are said to be
a valid pair if
pi ≤ pj + 1, ∀ j that are eligible neighbors of i.
We will see in Section 7.3.3 that the above relation is a form of the -
CS condition introduced in connection with the auction algorithm for the
assignment problem. Finally, the opposite of the divergence of a node i,
gi =
∑
{j|(j,i)∈A}
xji −
∑
{j|(i,j)∈A}
xij ,
is called the surplus of i.
The preflow-push algorithm starts with and maintains a valid flow-
price pair (x, p) such that x is capacity-feasible, p has integer components,
and
g1 ≤ 0, gi ≥ 0, ∀ i 	= 1,
p1 = N, pN = 0, 0 ≤ pi < N, ∀ i 	= 1. (7.41)
A possible initial choice is the pair (x, p) given by
xij =
{
c1j if i = 1,
0 if i 	= 1, (7.42)
pi =
{
N if i = 1,
0 if i 	= 1. (7.43)
There are two types of operations in the preflow-push algorithm:
(1) A flow change, which modifies the flow of some arc belonging to the
candidate list of some node. The flow change is always in the direction
from the node of higher price to the node of lower price.
(2) A price rise, which increases the price of some node whose candidate
list is empty. The increment of price increase is the largest that main-
tains the validity of the flow-price pair. With this price increment,
the candidate list of the node becomes nonempty.
Sec. 7.3 The Preflow-Push Algorithm for Max-Flow 285
The idea of the algorithm is to direct flow from nodes of higher price to
nodes of lower price. By setting the price of the source node to N and the
price of the sink node to 0 [cf. Eq. (7.41)], the algorithm moves flow in the
desired general direction from source to sink. This idea is shared with the
price-based augmenting path algorithm of Section 3.3.
At the start of each iteration of the preflow-push algorithm, a node
i 	= N with pi < N and gi > 0 is selected; if no such node can be found,
the algorithm terminates. The typical iteration is as follows:
Iteration of the Preflow-Push Algorithm
Step 1: (Scan candidate arc) Select an arc (i, j) of the candidate
list of i and go to Step 2, or an arc (j, i) of the candidate list of i and
go to Step 3; if the candidate list is empty, go to Step 4.
Step 2: (Push flow forward along arc (i, j)) Increase xij by
δ = min{gi, cij − xij}. If now gi = 0 and xij < cij , stop; else go
to Step 1.
Step 3: (Push flow backward along arc (j, i)) Decrease xji by
δ = min{gi, xji}. If now gi = 0 and 0 < xji, stop; else go to Step 1.
Step 4: (Increase price of node i) Raise pi to the level
pi = min
{
pj + 1 | (i, j) ∈ A and xij < cij , or (j, i) ∈ A and 0 < xji
}
.
(7.44)
Go to Step 1.
Figure 7.5 illustrates the preflow-push algorithm. As this figure shows,
and as we will demonstrate shortly, the algorithm terminates with a flow
vector under which a minimum cut separating the source from the sink is
saturated. This flow vector is not necessarily maximum or even feasible,
because some nodes other than the source and the sink may have nonzero
surplus. We will demonstrate later how, starting from this flow vector, we
can construct a maximum flow.
7.3.1 Analysis and Complexity
We will now establish the validity of the preflow-push algorithm, and we
will estimate its running time. For purposes of easy reference, let us call
the operation of Step 4 a price rise at node i, and let us call the operation
of Step 2 (or Step 3) a flow push on arc (i, j) [a flow push on arc (j, i),
respectively]. A flow push on arc (i, j) [or arc (j, i)] is said to be saturating
if it results in setting the flow xij to its upper bound cij (the flow xji
to its lower bound 0, respectively); otherwise, the flow push is said to be
nonsaturating.
286 Auction Algorithms Chap. 7
Max-flow problem with
arc flow bounds shown
next to the arcs
1 4
3
2
[0,2]
[0,5]
[0,1]
[0,1]
[0,5]
Prices and  flows after the 
first iteration
1 4
3
2
Source Sink
1 4
3
2
Initial prices; flows shown
next to the arcs
1 4
3
2
Prices and  flows after the 
second iteration
p1 = 4 5
0
0
p4 = 0
p3 = 0
p2 = 0
5
0
p1 = 4 3
1
1
p4 = 0
p3 = 6
p2 = 5
0
1
p1 = 4 3
2
0
p4 = 0
p3 = 0
p2 = 5
5
1
Saturated Cut
Figure 7.5: Preflow-push algorithm for the max-flow problem shown at the top
left. The initialization of Eqs. (7.42) and (7.43) is shown at the top right.
1st Iteration: Node 2 is selected, its price is first raised to p2 = 1, thereby creating
the two candidate list arcs (2, 3) and (2, 4). Then 2 units of flow are pushed along
(2, 3), and 1 unit of flow is pushed along (2, 4), thereby saturating these two arcs.
The surplus of node 2 continues to be positive, so its price is again raised to
p2 = 5, thereby creating the candidate list arc (1, 2). Then 2 units of flow are
pushed (backward) along this arc, to set the surplus of node 2 to 0. The resulting
flow-price pair is shown at the bottom left.
2nd Iteration: Node 3 is selected, its price is first raised to p3 = 1, thereby
creating the candidate list arc (3, 4). Then 1 unit of flow is pushed along (3, 4).
The surplus of node 3 continues to be positive, so its price is again raised to
p3 = 5, thereby creating the candidate list arc (1, 3). Then 5 units of flow are
pushed (backward) along (1, 3). The surplus of node 3 continues to be positive,
so its price is again raised to p3 = 6, thereby creating the candidate list arc (2, 3).
Then 1 unit of flow is pushed (backward) along this arc, to set the surplus of node
3 to 0. The resulting flow-price pair is shown at the bottom right.
The algorithm terminates because all nodes other than the sink have prices
that are no less than N = 4. Note that upon termination, the saturated cut
obtained, [{1, 2, 3}, {4}], is optimal. However, the flow obtained is not maximal
or even feasible, because node 2 has positive surplus (g2 = 1).
For the purpose of calculating the running time of the algorithm, we
assume that each time a price rise is performed at a node i, the candidate
list of i is constructed and stored. At each iteration at node i, and up to
the next price rise at i, arcs are selected from the top of the stored list
Sec. 7.3 The Preflow-Push Algorithm for Max-Flow 287
at Step 1 of the iteration, and examined for eligibility. If an arc of the
list is found not eligible or if the iteration results in a saturating push on
that arc, the arc is removed from the list. In this way we are assured that
between two successive price rises at a node, the incident arcs of the node
are scanned only once in order to construct the candidate list.
The preflow-push algorithm leaves free the choice of the node i se-
lected for iteration. It is possible to affect both the theoretical and the
practical performance of the algorithm by proper choice of this node. We
consider three particular choice rules:
(1) Arbitrary Choice: Here the node i chosen for iteration is arbitrary
(subject to i 	= N , pi < N , and gi > 0).
(2) First In-First Out Choice: Here the nodes i 	= N with pi < N and
gi > 0 are maintained in a first in-first out list and the node at the
top of the list is chosen for iteration.
(3) Highest Price Choice: Here the node i 	= N with pi < N and gi > 0
whose price is maximum is chosen for iteration.
In all cases, we assume that the nodes i 	= N with pi < N and gi > 0
are kept in some list or data structure, which is such that the overhead for
finding a node to iterate on is negligible in the sense that it does not affect
the algorithm’s complexity.
The following proposition shows that the algorithm terminates with
an optimal saturated cut, and that with the first two methods for choice
of node, the running time of the algorithm is O(N2A) and O(N3), respec-
tively, where N is the number of nodes and A is the number of arcs. It turns
out that the highest price choice method, with appropriate implementation,
has a running time O(N2A1/2), which is faster than the running times of
the other two methods. The proof of this is quite complex, however, so we
refer to the original source, which is the paper by Cheriyan and Mahesh-
wari [1989]. Based on the results of computational experimentation, the
highest price choice method also appears to be the fastest in practice. For
an example that provides some intuition for the reason, see Exercise 7.5.
Following the proof of the proposition, we will show how a maximum flow
can be constructed from the saturated cut by using a separate computation.
Proposition 7.9: The preflow-push algorithm terminates, and upon
termination the flow vector x is such that there is a saturated cut
[N+,N−] with
1 ∈ N+, N ∈ N−, (7.45)
gi ≥ 0, ∀ i 	= 1 with i ∈ N+, (7.46)
gi = 0, ∀ i 	= N with i ∈ N−. (7.47)
288 Auction Algorithms Chap. 7
This saturated cut is a minimum cut. Furthermore:
(a) With arbitrary choice of the node chosen for iteration, the run-
ning time is O(N2A).
(b) With first in-first out choice of the node chosen for iteration, the
running time is O(N3).
Proof: We first make the following observations:
(1) All the flow-price pairs generated by the algorithm are valid.
(2) All the node prices are monotonically nondecreasing integers through-
out the algorithm. Furthermore, a price rise at a node at Step 4
increases the price of the node by at least 1. This follows from Eq.
(7.44) and the fact that a price rise at a node can be performed only
if the candidate list of that node is empty.
(3) All the node prices range between 0 and 2N , since all the initial prices
are less or equal to N , a price rise at node i can set pi to at most
1+maxj∈N pj and once pi reaches or exceeds N , it remains constant.
(4) The surplus of every node other than node 1 is nonnegative through-
out the algorithm. The reason is that a flow push from a node i
cannot drive the surplus of i below zero, and cannot decrease the
surplus of neighboring nodes.
Since by (2) above, a price rise at i increases pi by at least 1 and once
pi exceeds N−1, it increases no further, it follows that there can be at most
N price rises at each node. When iterating on node i and a saturating flow
push occurs on an arc with end nodes i and j, we must have pi = pj + 1,
so that one of the at most N increases of pj must occur before this arc can
become unsaturated and then saturated again in the direction from i to j.
Thus the number of saturating flow pushes is at most 2N per arc, for a
total of at most 2NA.
We now argue by contradiction that the number of nonsaturating flow
pushes is finite and therefore the algorithm terminates. Indeed, assume the
contrary, i.e., that there is an infinite number of nonsaturating flow pushes.
Since the number of price rises and saturating flow pushes is finite as ar-
gued earlier, it follows that there is an iteration after which the prices of all
nodes remain constant at some final levels pi while all flow pushes are non-
saturating. Since there is an infinite number of nonsaturating flow pushes,
there must exist a pair of nodes i2 and i1 such that the number of flow
pushes from i2 to i1 is infinite, implying that pi2 = pi1 + 1. Each of these
nonsaturating flow pushes exhausts the surplus of i2, so there must exist a
node i3 such that there is an infinite number of nonsaturating flow pushes
from i3 to i2, implying that pi3 = pi2 + 1. Proceeding similarly, we can
Sec. 7.3 The Preflow-Push Algorithm for Max-Flow 289
construct an infinite sequence of nodes ik, k = 1, 2, . . ., with corresponding
prices satisfying pik+1 = pik +1 for all k. This is a contradiction since there
is only a finite number of nodes.
Let us now show that upon termination of the algorithm, there is a
saturated cut satisfying the conditions (7.45)-(7.47). Indeed, consider any
node i 	= N such that upon termination, we have pi ≥ N . We claim that
there is no simple unblocked path from i to N upon termination. The
reason is that if there exists such a path, and i1 and i2 are two successive
nodes on this path, we must have pi1 ≤ pi2 + 1, implying that pi cannot
exceed pN (which is 0) by more than the number of arcs on the path, which
is at most N − 1 – a contradiction. Thus, by Prop. 3.1, it follows that
there must be a saturated cut [N+,N−] separating node N from all the
nodes i with pi ≥ N . The latter nodes include node 1 (by the algorithm’s
initialization), as well as the nodes i with gi > 0 upon termination (by the
rule for termination of the algorithm). This proves the conditions (7.45)-
(7.47).
Consider a saturated cut [N+,N−] obtained on termination and sat-
isfying conditions (7.45)-(7.47). We will show that this cut is a minimum
cut. To this end, we introduce a max-flow problem, referred to as the mod-
ified problem, which is the same as the original max-flow problem except
that it contains an additional arc (i, 1) with capacity gi for each node i 	= N
with gi > 0. We observe that each cut of the modified problem that sepa-
rates the source 1 from the sink N has the same capacity as the same cut
in the original problem, since the additional arcs (i, 1) do not contribute to
the cut’s capacity. We will show that the cut [N+,N−] is a minimum cut
in the modified problem, and therefore also in the original max-flow prob-
lem. Indeed, consider a flow vector for the modified problem constructed
as follows: the flow of each arc (i, j) of the original problem is the same
as the flow obtained upon termination of the preflow-push algorithm, and
the flow of each of the additional arcs (i, 1) is gi. It is seen that for this
flow vector, the divergence of each node except 1 and N is 0. Furthermore,
the cut [N+,N−], which separates 1 and N , is saturated, and its capacity
is equal to the divergence out of node 1. By the max-flow/min-cut theo-
rem, this flow vector solves the modified max-flow problem, and the cut
[N+,N−] is a minimum cut.
To estimate the running time of the algorithm, we note that the
dominant computational requirements are:
(1) The computation required for price rises and for constructing the
candidate lists.
(2) The computation required for saturating flow pushes.
(3) The computation required for nonsaturating flow pushes.
Since there are O(N) price rises per node and there is one candidate
list construction between two successive price rises of a node, the total
290 Auction Algorithms Chap. 7
computation for (1) above is O(NA). Since there are O(N) saturating flow
pushes per arc, and each saturating flow push requires O(1) computation,
the total computation for (2) above is also O(NA). We will next estimate
the number of nonsaturating flow pushes for each of the two methods for
choosing a node for iteration.
(a) Assume an arbitrary choice of node. Denote
I = {i 	= N | gi > 0, pi < N},
M =
{ ∑
i∈I pi if I is nonempty,
0 if I is empty,
and note that M is an integer that in the course of the algorithm ranges
between 0 and 2N2 (since 0 ≤ pi ≤ 2N , as noted earlier). Furthermore,
we have M = 0 upon termination. We consider the effect of an iteration
on M .
As a result of a price rise at node i in Step 4, M will increase by at
most the corresponding price increment (in the case where i ∈ I after the
price rise). Since the total price increase per node is at most 2N , it follows
that the total increase of M as a result of price rises is at most 2N2.
As a result of a saturating flow push from a node i to a node j, M
may increase by as much as the price pj (if gj = 0 and pj < N prior to the
saturating flow push), so the total increase of M as a result of saturating
flow pushes in Steps 2 and 3 is N times the number of saturating flow
pushes, which as argued earlier is at most 2NA. Thus the total increase of
M as a result of price rises and saturating flow pushes is at most 2N2 +
2N2A.
On the other hand, when a nonsaturating flow push occurs from a
node i to a node j, M decreases by pi (since the surplus gi is set to 0 as
a result of the nonsaturating flow push), while as a result of the surplus
change of j, M increases by pj or by 0 (depending on whether gj = 0
and pj < N or not prior to the nonsaturating flow push). Since we must
have pi = pj + 1 in order for (i, j) to be in the candidate list of i, it
follows that M decreases by at least 1 with every nonsaturating flow push.
This implies that the total number of nonsaturating flow pushes is at most
2N2 +2N2A. Each nonsaturating flow push requires O(1) computation, so
the total computation for nonsaturating flow pushes is O(N2A). Thus the
overall running time of the preflow-push method with an arbitrary choice
of node is O(N2A).
(b) Assume a first in-first out choice of node, and denote again
I = {i 	= N | gi > 0, pi < N}.
It can be seen that with this choice rule, the algorithm can be divided in
cycles. The first cycle consists of a single iteration at each node i in the
Sec. 7.3 The Preflow-Push Algorithm for Max-Flow 291
initial set I. The (k + 1)st cycle consists of a single iteration at each node
i in the set I obtained at the end of the kth cycle. We will first show that
the total number of cycles is O(N2).
To this end, we define
M =
{
maxi∈I pi if I is nonempty,
0 if I is empty,
and we consider the effect on M of a single cycle. There are two possibilities:
(1) M increases or stays constant during the cycle. Then there must be
at least one price rise during the cycle, since otherwise the surplus of
every node iterated on during the cycle would be shifted to a node
with lower price and M would be decreased by at least 1. Since the
total number of price rises is O(N2), it follows that the number of
cycles where M increases or stays constant is O(N2). Furthermore,
the sum of increases in M is bounded above by the sum of price
increases of all the nodes, which was shown earlier to be O(N2).
(2) M decreases during the cycle. Since M ≥ 0, the sum of decreases in
M can exceed the sum of increases in M , which was shown above to
be O(N2), by no more than the maximum initial price value, which
is no more than N . Since M can decrease only in integer increments,
we see that the number of cycles where M decreases is O(N2).
Thus the total number of cycles is O(N2). Since in each cycle there can be
only one nonsaturating flow push per node, it follows that the total num-
ber of nonsaturating flow pushes is O(N3), resulting in an overall O(N3)
running time. Q.E.D.
The preceding proof suggests that the complexity bottleneck is the
computation for nonsaturating flow pushes. Computational experience,
however, indicates that, in practice, the O(NA) operations associated with
price rises is at least as much of a bottleneck.
The Second Phase: Constructing a Maximum Flow
Let us now discuss how to construct a maximum flow from the saturated
cut and the flow vector obtained upon termination of the preflow-push
algorithm. Suppose that the algorithm has terminated, and that we have
obtained the saturated cut [N+,N−] and the flow vector x such that
1 ∈ N+, N ∈ N−,
gi ≥ 0, ∀ i 	= 1 with i ∈ N+,
gi = 0, ∀ i 	= N with i ∈ N−.
292 Auction Algorithms Chap. 7
A maximum flow can be computed by solving a certain feasibility
problem, which aims to return to the source the excess flow that has entered
the graph from the source and has accumulated at the other nodes of N+.
In particular, we delete all nodes in N− and all arcs with at least one of
their end nodes in N−, and for each node i 	= 1 with i ∈ N+ and∑
{(i,j)|j∈N−}
cij > 0,
we introduce an arc (i, 1) with flow and capacity
xi1 = ci1 =
∑
{(i,j)|j∈N−}
cij (7.48)
[if the arc (i, 1) already exists, we just change its capacity and flow to
the above value]. In the resulting graph, we solve the feasibility problem
of finding a capacity-feasible flow vector x such that the corresponding
surpluses are all zero. Given a solution x, the vector x∗ defined by
x∗ij =
{
xij if i /∈ N−, j /∈ N−,
xij otherwise,
(7.49)
can be shown to be a maximum flow. Indeed, it can be seen, using also the
fact gi = 0 for all i ∈ N− with i 	= N , that x∗ has surpluses g∗i satisfying
g∗i = 0 for all i 	= 1, N , g∗1 < 0, g∗N > 0, and saturates the cut [N+,N−].
Since [N+,N−] was shown to be a minimum capacity cut, it follows that
x∗ is a maximum flow.
The feasibility problem just described can be solved with a suitably
modified version of the preflow-push algorithm, illustrated in Fig. 7.6 (feasi-
bility problems are essentially equivalent to max-flow problems as discussed
in Section 3.1). It can be verified that the running time estimates of Prop.
7.9 apply to the second phase of the preflow-push algorithm, so that the es-
timates obtained for the first phase apply to the combined first and second
phases as well.
We note that the two-phase implementation of the preflow-push algo-
rithm that we have given is by far the most effective in practice, particularly
when it is combined with a method for saturated cut detection, to be dis-
cussed shortly. The algorithm can be modified, however, so that it finds
a maximum flow in a single phase. What is needed for this is to allow
iterations at all nodes i 	= N with gi > 0, even if pi ≥ N . The termination
and running time assertions of Prop. 7.9 can then be shown as stated, with
a simple modification of the proof given above. Furthermore, the flow ob-
tained upon termination is a maximum flow. We leave the verification of
these facts as an exercise for the reader.
Sec. 7.3 The Preflow-Push Algorithm for Max-Flow 293
1
3
2
Starting flows and prices
for the feasibility problem
of the 2nd phase
p1 = 4
3
1
1
p3 = 6
p2 = 5
0
1
1 4
3
2
Max-flow obtained after
solving the feasibility
problem
2
1
10
1
Prices and  flows
obtained on termination
of the1st phase
1 4
3
2
p1 = 4 3
1
1
p4 = 0
p3 = 6
p2 = 5
0
1
Saturated Cut
Figure 7.6: Illustration of the second phase of the preflow-push for the max-flow
problem of Fig. 7.5. The final flows and prices of the first phase are shown at
the top (cf. the bottom right graph of Fig. 7.5). The node 4 (which constitutes
N−) is deleted, together with the connecting arcs (2, 4) and (3, 4). The arcs (2, 1)
and (3, 1) are then added with flows and capacities equal to 1, and the feasibility
problem of finding a circulation in the graph at the bottom left is considered. The
solution of this problem is obtained by pushing (backward) to the source along
arc (1, 2) the 1 unit of surplus of node 2. This yields the max-flow shown at the
bottom right [cf. Eq. (7.49)].
7.3.2 Implementation Issues
In practice, it has been observed that for some problems (particularly those
involving a sparse graph, where A << N2), the preflow-push algorithm
can create a saturated cut very quickly and may then spend a great deal
of additional time to raise to the level N the prices of the nodes that
are left with positive surplus. Computational studies have shown that for
efficiency, it is extremely important to use a procedure that detects early
the presence of a saturated cut. Several schemes are possible.
One approach, called global repricing , uses breadth-first search from
the sink to find periodically, in the course of the algorithm, the set
S = {i | there exists an unblocked path from i to the sink}.
294 Auction Algorithms Chap. 7
If all nodes in S have zero surplus, then S defines a minimum cut. Other-
wise, the prices of the nodes in S are set to their shortest distances from the
sink. Furthermore, all the nodes not in S can effectively be purged from
the computation by setting their price equal to N . While global repricing
can add substantial overhead to the algorithm, it has been generally shown
to be beneficial in computational experiments. It is important to use an
appropriate heuristic scheme that ensures that global repricing is not too
frequent, in view of the associated overhead. In practice, repeating the test
after a number of iterations, which is of the order of N , seems to work well.
Another approach (due to Derigs and Meier [1989] and called the gap
method) is to maintain in a suitable data structure, for each integer k in
the range [1, N − 1], the number of nodes m(k) whose price is equal to k.
If for some k we have m(k) = 0 (this is called a gap at price k), then it
can be shown (Exercise 7.22) that there is a saturated cut separating all
nodes with price greater than k from all nodes whose price is less than k.
All the nodes with price greater than k can effectively be purged from the
computation by setting their price equal to N . Furthermore, if all nodes
with price less than k have zero surplus, the separating saturated cut is a
minimum cut.
Note a key advantage of the two saturated cut detection procedures
given: they can purge from the computation a significant number of nodes
before finding a minimum cut, thus saving the purposeless iterations that
involve these nodes.
7.3.3 Relation to the Auction Algorithm
We will now develop the relationship between the preflow-push algorithm
and the auction algorithm for the assignment problem, using the method-
ology for similar persons described in Section 7.2.3. This relationship pro-
vides insight into the convergence mechanism of the preflow-push method,
but will not be used further in the sequel. Thus, the present section can
be skipped without loss of continuity.
We start with a special type of feasibility problem, where we want to
transfer a given amount of flow from a source node to a sink node in a given
network. The benefit of the transfer is zero, but each arc has a capacity
constraint on the flow that it can carry. In particular, we have a directed
graph with set of nodes N and set of arcs A. Node 1 is called the source
and node N is called the sink . We assume that there are no incoming arcs
to the source and no outgoing arcs from the sink. Each arc (i, j) carries a
flow xij . We are given a positive integer s, and we consider the problem of
finding a flow vector satisfying
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = 0, ∀ i ∈ N , i 	= 1, N,
Sec. 7.3 The Preflow-Push Algorithm for Max-Flow 295∑
{j|(1,j)∈A}
x1j =
∑
{j|(j,N)∈A}
xjN = s,
0 ≤ xij ≤ cij , ∀ (i, j) ∈ A,
where cij are given positive integers.
We call the above problem the fixed-flow problem to distinguish it
from the max-flow problem, where s is an optimization variable that we try
to maximize. The fixed-flow and max-flow problems are closely related, as
we have shown in Chapter 3 (see Fig. 3.1). In particular, if s is equal to
its (generally unknown) maximum value, the two problems coincide. Many
max-flow algorithms solve in effect the fixed-flow problem for appropriate
values of s. For example, the Ford-Fulkerson algorithm of Section 3.2 solves
the fixed-flow problem for an increasing sequence of values of s until a sat-
urated cut separating the source and the sink is constructed, in which case
s cannot be increased further and the algorithm terminates. For conve-
nience we will work with the fixed-flow problem, but the interpretations
and conversions to be given have straightforward analogs for the max-flow
case.
The fixed-flow problem can be converted to an equivalent feasibil-
ity/transportation problem by replacing each arc (i, j) that is not incident
to the source or the sink (i 	= 1 or j 	= N) by a node labeled (i, j), and two
arcs
(
i, (i, j)
)
and
(
j, (i, j)
)
that are incoming to that node as shown in Fig.
7.7. The flows of these arcs are denoted yi(i,j) and zj(i,j), and correspond
to the arc flow xij via the transformation
yi(i,j) = xij , zj(i,j) = cij − xij .
All arc benefits are zero; see Fig. 7.8. This transportation problem can
in turn be transformed to a feasibility/assignment problem with zero arc
benefits and with similar persons by means of the following two devices
(see Fig. 7.9):
(a) Create
∑
{j|(j,i)∈A} cji similar persons in place of each node/source
i 	= 1, N of the transportation problem, and s persons in place of the
source node 1.
(b) Create cij duplicate objects in place of each arc/sink (i, j), j 	= N of
the transportation problem, and s duplicate objects in place of the
sink node N .
We will now use this equivalence to transcribe the algorithm based
on M -auction iterations of Section 7.2.3 into the fixed-flow context. The
auction algorithm starts with all object prices being zero. The initial as-
signment corresponds to the zero flow vector [xij = 0 for all arcs (i, j) ∈ A],
which implies that all the persons corresponding to the nodes i 	= 1, N are
assigned to the objects corresponding to the artificial arcs [zj(i,j) = cij for
296 Auction Algorithms Chap. 7
Arc Flow Arc Flow 
(i,j )
c   ij
ji
ijxc   ij -z       =j(i,j)ijxi(i,j )
y     = 
ijx
ijx
ijxArc Flow 
i j
ijx
ijx
c   ij
Figure 7.7: Transformation of a fixed-flow problem into a feasibility/transportation
problem. Each arc (i, j) is replaced by a node labeled (i, j) and two incoming arcs(
i, (i, j)
)
and
(
j, (i, j)
)
to that node.
j
PERSONS
(Nodes of original 
network except
sink)
i
j
 
 cmj
m
 
c mi
m
 
. . .
. . .
(i,j)
c   ij
OBJECTS
(Arcs of original
network plus
the sink)
ij
Arc Flow = y      = x
i(i,j)
Arc Flow = z      = c   - x
ijijj(i,j)
1s
Source
N s
Sink
N -1
Arc Flow = x 1k (1,k )
Arc Flow = c     - x
iN iN
. . .
. . .
Σ
Σ
Figure 7.8: The equivalent feasibility/transportation problem. By viewing each
arc (i, j) as cij duplicate objects and the sink as s duplicate objects, this problem
can be viewed as an assignment problem with similar persons.
Sec. 7.3 The Preflow-Push Algorithm for Max-Flow 297
1
2
3
(1,2)
(3,2)
(1,3)
2
2
3
1
3
Equivalent Transportation Problem
4
2
1
1
2
4
Original Fixed-Flow Problem
3 - 3Capacity = 1
Capacity = 1
Capacity = 2
3
Capacity = 3
Capacity = 2
PERSONS OBJECTS
3
2'
4'
4''
4
3'
2
Similar Persons
Duplicate Objects
Similar Persons
Equivalent Assignment Problem
1'
1"
1
(3,2)
(1,2)
(1,3)'
(1,3)
Similar Persons
Duplicate Objects
Figure 7.9: Example of a fixed-flow problem, and its corresponding equivalent
feasibility/transportation and assignment problems. All arc benefits are zero.
all artificial arcs
(
j, (i, j)
)
]. Thus initially, only the s persons corresponding
to the source and the s objects corresponding to the sink are unassigned.
As the auction algorithm executes, the objects corresponding to an
arc (i, j) with j 	= N are always assigned to some person and are divided
in two classes (one of which may be empty):
(a) The objects assigned to some person of the similarity class of i. The
number of these objects is xij , and their common price (see the remark
at the end of Section 7.2.3) is denoted pij .
(b) The objects assigned to some person of the similarity class of j. The
number of these objects is cij − xij , and their common price (see the
remark at the end of Section 7.2.3) is denoted p
ij
.
298 Auction Algorithms Chap. 7
Similarly, the objects corresponding to an incoming arc (i, N) of the sink
are divided in two classes:
(a) The objects assigned to some person of the similarity class of i. The
number of these objects is xiN , and their common price is denoted
piN .
(b) The objects that are unassigned. The number of these objects is
ciN−xiN , and their common price is zero. For notational convenience,
we define p
iN
= 0.
As remarked at the end of Section 7.2.3, all objects assigned to persons
of the same similarity class must offer the same value for all persons of the
class. Since the arc benefits for the underlying assignment problem are
zero, it follows that all objects assigned to persons of the same similarity
class must have equal prices. We see therefore that, in the course of the
algorithm, for each node i 	= 1, there is a scalar pi such that
pi = pij , ∀ (i, j) ∈ A such that xij > 0, (7.50)
and
pi = pji, ∀ (j, i) ∈ A such that xji < cji. (7.51)
Regarding the source 1, a slightly different definition of p1 must be given
because initially all outgoing arcs of 1 have zero flow. We define
p1 =
{
0 if x1j = 0 for all (1, j) ∈ A,
p1j otherwise, where (1, j) is any arc with x1j > 0.
We call pi the implicit price of i. Figure 7.10 illustrates the definition of
the implicit prices.
(i,j)
Arc Flow = y      = xiji(i,j)
Arc Flow = z         = c   - xijijj(i,j)
p
i
pj
pj = p      if     x   < cij ij ij_
p i ij
= p      if     x   > 0
ij
_
j
i
Figure 7.10: Definition of the implicit prices of the person/nodes in terms of the
prices of the object/prices.
The assignment-price pairs generated by the auction algorithm satisfy
-CS. Taking into account that all arc benefits are zero, the -CS condition
Sec. 7.3 The Preflow-Push Algorithm for Max-Flow 299
for the transportation/assignment problem becomes
−pij ≥ max
{
max
{(i,k)|xik>0}
−pik, max{(i,k)|xik<cik}
−p
ik
,
max
{(k,i)|xki>0}
−pki, max{(k,i)|xki<cki}
−p
ki
}
− , if xij > 0,
(7.52)
−p
ji
≥ max
{
max
{(i,k)|xik>0}
−pik, max{(i,k)|xik<cik}
−p
ik
,
max
{(k,i)|xki>0}
−pki, max{(k,i)|xki<cki}
−p
ki
}
− , if xji < cji,
(7.53)
where in the above relations, and in similar relations in this section, we
adopt the convention that the maximum and the minimum over the empty
set is −∞ and +∞, respectively. By Eqs. (7.50) and (7.51), we have that
if xij > 0, then
pij = pik, ∀ (i, k) with xik > 0, pij = pki, ∀ (k, i) with xki < cki,
while if xji < cji, then
p
ji
= pik, ∀ (i, k) with xik > 0, pji = pki, ∀ (k, i) with xki < cki.
Therefore, Eqs. (7.52) and (7.53) can be equivalently written as
pij ≤ min
{
min
{(i,k)|xik<cik}
p
ik
, min
{(k,i)|xki>0}
pki
}
+ , if xij > 0,
and
p
ji
≤ min
{
min
{(i,k)|xik<cik}
p
ik
, min
{(k,i)|xki>0}
pki
}
+ , if xji < cji.
When these relations are combined with the definition (7.50) and (7.51) of
pi, they can be written in the equivalent form
pi ≤ min
{
min
{(i,k)|xik<cik}
p
ik
, min
{(k,i)|xki>0}
pki
}
+ .
Using again Eqs. (7.50) and (7.51), we see that this condition is equivalent
to
pi ≤ pk +  if xik < cik or xki > 0,
or alternatively
pi ≤ pj +  if xij < cij , (7.54)
300 Auction Algorithms Chap. 7
pj ≤ pi +  if xij > 0. (7.55)
Note that here the value of  does not matter, because all arc benefits
are zero; as long as  > 0 the generated sequence of flows does not depend
on , while the generated prices are just scaled by . We can thus select
 = 1.
Consider now the condition under which the similarity class of a node
i is eligible to bid at an iteration of the auction algorithm. For this, the
similarity class of i must have some unassigned persons. From Fig. 7.8, it
can be seen that this is equivalent to
∑
{j|(j,i)∈A}
cji >
∑
{j|(i,j)∈A}
xij +
∑
{j|(j,i)∈A}
(cji − xji), if i 	= 1,
and
s >
∑
{j|(1,j)∈A}
x1j , if i = 1.
Let us define the surplus of a node i by
gi =
{ ∑
{j|(j,i)∈A} xji −
∑
{j|(i,j)∈A} xij if i 	= 1,
s −
∑
{j|(1,j)∈A} x1j if i = 1.
It is seen that a similarity class is eligible to submit a bid in the auction
algorithm at a given iteration if and only if the surplus of the corresponding
node is positive.
The table of Fig. 7.11 provides a list of the corresponding variables
and relations between the fixed-flow problem and the preflow-push algo-
rithm on one hand, and its equivalent transportation/assignment problem
and the auction algorithm on the other.
Let us now transcribe the auction algorithm by using the correspon-
dences of the table of Fig. 7.11. Initially all arc flows xij are zero and
all implicit prices pi are also zero. At the start of each iteration, a node
i 	= N with positive surplus gi is chosen; if no such node can be found, the
algorithm terminates.
Auction Iteration Applied to the Equivalent
Assignment/Fixed-Flow Problem
Step 1: (Scan incident arc) Select an arc (i, j) such that xij < cij
and pi = pj +1 and go to Step 2, or an arc (j, i) such that 0 < xji and
pi = pj +1 and go to Step 3. If no such arc can be found go to Step 4.
Sec. 7.3 The Preflow-Push Algorithm for Max-Flow 301
Transportation/Assignment Fixed-Flow
Problem Problem
Flows yi(i,j), zj(i,j) = cij − yi(i,j) xij = yi(i,j) = cij − zj(i,j)
Prices pij , pij
pi =
{
pij for all (i, j) with xij > 0
p
ji
for all (j, i) with xji < cji
-CS pij ≤ min
{
minxik<cik pik
, pi ≤ pj + 1 if xij < cij
( = 1) minxki>0 pki
}
+ 1 if xij > 0
p
ji
≤ min
{
minxik<cik pik
, pj ≤ pi + 1 if xij > 0
minxki>0 pki
}
+ 1 if xji < cji
Select unassigned person Select node with positive surplus
Selected person finds best object Selected node finds best incident arc
Selected person gets assigned to best Selected node pushes flow on best arc,
object and displaces current owner opposite node retracts flow from the arc
Selected person raises best object price Selected node raises its implicit price
by max increment maintaining 1-CS by max increment maintaining 1-CS
Figure 7.11: Correspondences between the fixed-flow problem and the preflow-
push algorithm on one hand, and its transportation/assignment equivalent version
and auction algorithm on the other. Here,  = 1.
Step 2: (Push flow forward along arc (i, j)) Increase xij by
δ = min{gi, cij − xij}. If now gi = 0 and xij < cij , stop; else go
to Step 1.
Step 3: (Push flow backward along arc (j, i)) Decrease xji by
δ = min{gi, xji}. If now gi = 0 and 0 < xji, stop; else go to Step 1.
Step 4: (Increase price of node i) Raise pi to the level
min
{
pj + 1 | (i, j) ∈ A and xij < cij , or (j, i) ∈ A and 0 < xji
}
.
(7.56)
Go to Step 1.
302 Auction Algorithms Chap. 7
Note that Steps 2 and 3 correspond to changing the assignment by
associating the persons in the similarity class of node i to their best objects
corresponding to the incident arcs of i, up to the point where the surplus
of i is exhausted. This modification of the assignment is done via perhaps
multiple passes through Steps 2 and 3. When no suitable arc can be found
in Step 1, this means that the price of the best objects for which the persons
of i will bid will be strictly increased, and that the implicit price pi will also
be increased. Step 4 computes the appropriate level. It can be seen that
the above algorithm is essentially equivalent to the preflow-push algorithm
analyzed earlier in Section 7.3.1.
Interpretation of the Algorithm
For an intuitive interpretation of the fixed-flow algorithm as an auction,
think of each node i as a city, and of each arc (i, j) as a transportation link
of capacity cij between cities i and j. Suppose that the objective is to move
s persons from city 1 to city N , while observing the capacity constraints of
the transportation links [the number of forward person crossings minus the
number of backward person crossings of each (i, j) must be no more than
cij at all times]. The method for accomplishing the transfer is to charge
a rent pi to each person in city i. Persons will move from city i to city
j along link (i, j) if pi > pj , to the extent that the capacity of link (i, j)
allows. The rent of a city is successively raised to  plus the minimum level
at which all surplus population will move to a neighboring city. Assuming
 = 1, this level is given by Eq. (7.56). With these rules, we obtain the fixed
flow/auction algorithm of this section, which can thus be seen as an auction
between the cities (except city N) to dispose of their surplus population
by raising the corresponding rents.
Extension to the Min-Cost Flow Problem
Suppose that in the preceding interpretation there is a transportation cost
aij for crossing link (i, j). Then persons will move from city i to city j if
the current rent pi is higher than the rent pj plus the transportation cost
aij . With this as a guide, we can modify in a straightforward way the
preceding arguments for the case of the fixed flow problem and derive an
auction algorithm for the minimum cost flow problem
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = 0, ∀ i ∈ N , i 	= 1, N,
∑
{j|(1,j)∈A}
x1j =
∑
{j|(j,N)∈A}
xjN = s,
Sec. 7.3 The Preflow-Push Algorithm for Max-Flow 303
0 ≤ xij ≤ cij , ∀ (i, j) ∈ A,
where aij are given integers, and cij and s are given positive integers.
The above minimum cost flow problem is somewhat special because it
involves a single source and a single sink, as well as a zero lower bound on
the flow of each arc. However, more general versions can be converted to the
problem above by introducing some artificial arcs and nodes (see Chapter
4), and the analysis of this section can be appropriately generalized. In
fact this will be done implicitly in Section 7.4.
The equivalent transportation/assignment problem has the same graph
as before (cf. Fig. 7.8). Taking into account the change from a maximiza-
tion to a minimization problem, the benefits involved are −aij for each of
the arcs
(
i, (i, j)
)
, j 	= N , −aiN for each of the arcs (i, N), and zero for
each of the other arcs.
The implicit prices are now defined by [cf. Eqs. (7.50) and (7.51)]
pi = aij + pij , ∀ (i, j) ∈ A such that xij > 0,
and
pi = pji, ∀ (j, i) ∈ A such that xji < cji.
The -CS condition becomes
pi ≤ aij + pj +  if xij < cij , (7.57)
aij + pj ≤ pi +  if xij > 0. (7.58)
[cf. Eqs. (7.54) and (7.55)]. Note that here the value of  matters because
the arc benefits are not all zero.
The auction algorithm when applied to the equivalent transporta-
tion/assignment problem can be transcribed similar to the one for the fixed
flow problem. Initially the arc flows xij and the implicit prices pi must sat-
isfy -CS; for example, if aij are all nonnegative, we may use xij = 0 for all
(i, j) and pi = 0 for all i. At the start of each iteration, a node i 	= N with
positive surplus gi is chosen; if no such node can be found the algorithm
terminates.
Auction Iteration Applied to the Equivalent
Assignment/Min Cost Flow Problem
Step 1: (Scan incident arc) Select an arc (i, j) such that xij < cij
and pi = aij +pj + and go to Step 2, or an arc (j, i) such that 0 < xji
and pi = pj − aji +  and go to Step 3. If no such arc can be found go
to Step 4.
304 Auction Algorithms Chap. 7
Step 2: (Push flow forward along arc (i, j)) Increase xij by
δ = min{gi, cij − xij}. If now gi = 0 and xij < cij , stop; else go
to Step 1.
Step 3: (Push flow backward along arc (j, i)) Decrease xji by
δ = min{gi, xji}. If now gi = 0 and 0 < xji, stop; else go to Step 1.
Step 4: (Increase price of node i) Raise pi to the level
pi = min
{
min
{(i,j)∈A|xij<cij}
{
aij + pj + 
}
,
min
{(j,i)∈A|bji<xji}
{
pj − aji + 
}}
.
Go to Step 1.
The preceding algorithm is called -relaxation method , and is dis-
cussed in the next section for the slightly more general version of the mini-
mum cost flow problem where there may be multiple source and sink nodes.
7.4 THE -RELAXATION METHOD
We now consider the minimum cost flow problem
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
bij ≤ xij ≤ cij , ∀ (i, j) ∈ A,
where the scalars aij , bij , cij , and si are given. In this section, we will
introduce and analyze the -relaxation method for solving this problem.
This is a slightly modified version of the method derived in Section 7.3.3
as a special case of the auction algorithm for the assignment problem.
Throughout this section, we assume that all the scalars aij , bij, cij,
and si are integer, and that the problem is feasible. In practice, the method
may be supplemented with additional mechanisms to detect infeasibility,
as will be discussed later in the section. A version of the method that can
deal with noninteger data will be developed in Section 9.6, in the context
of the more general convex separable network problem.
Like all auction algorithms, the -relaxation method is based on the
notion of -complementary slackness (-CS for short). We say that a
Sec. 7.4 The -Relaxation Method 305
capacity-feasible flow vector x and a price vector p satisfy -CS if
pi − pj ≤ aij +  for all (i, j) ∈ A with xij < cij , (7.59)
pi − pj ≥ aij −  for all (i, j) ∈ A with bij < xij , (7.60)
[compare with Eqs. (7.57) and (7.58) in Section 7.3.3; see Fig. 7.12]. The
usefulness of -CS is due in large measure to the following proposition,
which generalizes Prop. 7.2 for the assignment problem. The proposition
relies on the integrality of the cost coefficients aij (see Exercise 7.13 for a
generalization).
0
aij
bij cij x ij
p jpi -
ε
ε
Figure 7.12: Illustration of -CS. All pairs of arc flows xij and price differences
pi −pj should either lie on the thick lines or in the shaded area between the thick
lines.
Proposition 7.10: If  < 1/N , where N is the number of nodes, x is
feasible, and x and p satisfy -CS, then x is optimal for the minimum
cost flow problem.
Proof: If x is not optimal, then by Prop. 1.2 in Section 1.2, there exists a
simple cycle Y that has negative cost, i.e.,∑
(i,j)∈Y +
aij −
∑
(i,j)∈Y −
aij < 0, (7.61)
306 Auction Algorithms Chap. 7
and is unblocked with respect to x, i.e.,
xij < cij , ∀ (i, j) ∈ Y +,
bij < xij , ∀ (i, j) ∈ Y −.
By -CS [cf. Eqs. (7.59) and (7.60)], the preceding relations imply that
pi ≤ pj + aij + , ∀ (i, j) ∈ Y +,
pj ≤ pi − aij + , ∀ (i, j) ∈ Y −.
By adding these relations over all arcs of Y (whose number is no more than
N), and by using the hypothesis  < 1/N , we obtain∑
(i,j)∈Y +
aij −
∑
(i,j)∈Y −
aij ≥ −N > −1.
Since the arc costs aij are integer, we obtain a contradiction of Eq. (7.61).
Q.E.D.
Exercises 7.13 and 7.14 provide various improvements of the tolerance
 < 1/N in some specific contexts.
Let us denote by gi the surplus of node i:
gi =
∑
{j|(j,i)∈A}
xji −
∑
{j|(i,j)∈A}
xij .
In the -relaxation method, flows and prices are changed in a way that
maintains -CS and tends to drive the nonzero node surpluses towards
zero. Furthermore, flow is allowed to change along certain types of arcs,
which we now introduce. Given a flow-price pair (x, p) satisfying -CS, we
say that an arc (i, j) is +-unblocked if
pi = pj + aij +  and xij < cij .
We say that an arc (j, i) is −-unblocked if
pi = pj − aji +  and bji < xji.
The candidate list of a node i is the (possibly empty) set of outgoing arcs
(i, j) that are +- unblocked, and incoming arcs (j, i) that are −-unblocked.
We use a fixed positive value of , and we start with a pair (x, p)
satisfying -CS. Furthermore, the starting arc flows are integer, and it will
be seen that the integrality of the arc flows is preserved thanks to the
integrality of the node supplies and the arc flow bounds. Implementations
that have good worst case complexity also require that all initial arc flows
Sec. 7.4 The -Relaxation Method 307
be at either their upper or their lower bound, as will be explained later.
This can be easily enforced.
At the start of a typical iteration we have a flow-price vector pair
(x, p) satisfying -CS and we select a node i with gi > 0; if no such node
can be found, the algorithm terminates.
Iteration of the -Relaxation Method
Step 1: (Scan incident arc) If the candidate list of node i is empty,
go to Step 4; else select from the candidate list of i either an arc (i, j)
and go to Step 2, or an arc (j, i) and go to Step 3.
Step 2: (Push flow forward along arc (i, j)) Increase xij by
δ = min{gi, cij − xij}. If now gi = 0 and xij < cij , stop; else go
to Step 1.
Step 3: (Push flow backward along arc (j, i)) Decrease xji by
δ = min{gi, xji − bji}. If now gi = 0 and bji < xji, stop; else go to
Step 1.
Step 4: (Increase price of node i) Raise pi to the level
pi = min
{
min
{(i,j)∈A|xij<cij}
{
aij + pj + 
}
,
min
{(j,i)∈A|bji<xji}
{
pj − aji + 
}}
.
(7.62)
Go to Step 1.
There is an exceptional situation in Step 4, which requires special
handling. This is the case where in Eq. (7.62) we have xij = cij for all
outgoing arcs (i, j) and bji = xji for all incoming arcs (j, i); that is, the cut
separating i from the remainder of the graph is saturated, while gi ≥ 0.
This can arise under two circumstances: (1) gi > 0, in which case, the
problem must be infeasible, or (2) gi = 0. To deal with the situation, we
stop the algorithm in case (1), and we keep pi at its current level and stop
the iteration in case (2).
To see that the iteration is well-defined in the sense that it stops after
a finite number of computational operations, observe the following:
(a) Integrality of the arc flows is maintained by the algorithm, since the
starting arc flows, the node supplies, and the arc flow bounds are
integer. In particular, the flow increments δ in Steps 2 and 3 are
integer throughout the algorithm.
(b) At most one flow change per incident arc of node i is performed at each
iteration since a flow change either sets the flow to one of its bounds,
308 Auction Algorithms Chap. 7
which causes the corresponding arc to drop out of the candidate list of
i through the end of the iteration, or else results in gi = 0, which leads
the iteration to branch to Step 4 and subsequently stop. Therefore,
the number of flow changes per iteration is finite. In addition we have
gi > 0 at the start and gi = 0 at the end of an iteration, so at least
one flow change must occur before an iteration can stop.
(c) After each price rise with gi > 0 at least one flow change must be
performed, so from (b) it follows that the number of price changes
per iteration is finite.
Thus the method’s iteration is guaranteed to stop after a finite number of
operations.
Some insight into the -relaxation iteration can be obtained by noting
that in the limit, as  → 0, it yields the single node relaxation iteration
of Section 6.3. Figure 7.13 illustrates the sequence of price rises in an -
relaxation iteration; this figure should be compared with the corresponding
Fig. 6.8 in Section 6.3 for the single node relaxation iteration. As Fig. 7.13
illustrates, the -relaxation iteration can be interpreted as an approximate
coordinate ascent or Gauss-Seidel relaxation iteration. This interpretation
parallels the approximate coordinate descent interpretation of the mathe-
matically equivalent auction algorithm (cf. Fig. 7.1).
The following proposition establishes the validity of the -relaxation
method.
Proposition 7.11: Assume that the minimum cost flow problem is
feasible. Then the -relaxation method terminates with a pair (x, p)
satisfying -CS. The flow vector x is feasible, and is optimal if  < 1/N .
Proof: We first make a few observations.
(a) The algorithm preserves -CS; this can be verified from the price
change formula (7.62).
(b) The prices of all nodes are monotonically nondecreasing during the al-
gorithm [this follows from the -CS property of (x, p) and Eq. (7.62)].
(c) Once a node has nonnegative surplus, its surplus stays nonnegative
thereafter, since a flow change in Step 2 or 3 at a node i cannot drive
the surplus of i below zero (since δ ≤ gi), and cannot decrease the
surplus of neighboring nodes.
(d) If at some time a node has negative surplus, its price must have never
been increased up to that time, and must be equal to its initial price.
This is a consequence of (c) above and of the assumption that only
nodes with nonnegative surplus can be chosen for iteration.
Sec. 7.4 The -Relaxation Method 309
εε
First
price
rise
Starting
price
Final
price
1 2
3 4
i
[0,20] [0,10]
[0,20] [0,30]
Price of node i
Dual cost along pi
Values of p   for which the corresponding
incident arcs become balanced
i
Slope = 40
Slope = 20
Slope = 10 Slope = -10
Slope = -40
Maximizing point
p   - a1 1i p   + a4 i 43 3 ip   - a2 i 2p   + a
Second
price
rise
Figure 7.13: Illustration of the price rises of the -relaxation iteration. Here,
node i has four incident arcs (1, i), (3, i), (i, 2), and (i, 4) with flow ranges [0, 20],
[0, 20], [0, 10], and [0, 30], respectively, and supply si = 0. The arc costs and
current prices are such that
p1 − a1i ≤ p2 + ai2 ≤ p3 − a3i ≤ p4 + ai4,
as shown in the figure. The break points of the dual cost along the price pi
correspond to the values of pi at which one or more incident arcs to node i become
balanced. For values between two successive break points, there are no balanced
arcs. Each price rise of the -relaxation iteration increases pi to the point which is
 to the right of the next break point larger than pi, (assuming that the starting
price of node i is to the left of the maximizing point by more than ). In the
example of the figure, there are two price rises, the second of which sets pi at the
point which is  to the right of the maximizing point, leading to the approximate
(within ) coordinate ascent interpretation.
Suppose, to arrive at a contradiction, that the method does not ter-
minate. Then, since there is at least one flow change per iteration, an
infinite number of flow changes must be performed at some node i on some
arc (i, j). Since for each flow change, the increment δ is integer, an infinite
number of flow changes must also be performed at node j on the arc (i, j).
310 Auction Algorithms Chap. 7
This means that arc (i, j) becomes alternately +-unblocked with gi > 0
and −-unblocked with gj > 0 an infinite number of times, which implies
that pi and pj must increase by amounts of at least 2 an infinite number
of times. Thus we have pi → ∞ and pj → ∞, while either gi > 0 or gj > 0
at the start of an infinite number of flow changes.
Let N∞ be the set of nodes whose prices increase to ∞. To preserve
-CS, we must have, after a sufficient number of iterations,
xij = cij for all (i, j) ∈ A with i ∈ N∞, j /∈ N∞,
xji = bji for all (j, i) ∈ A with i ∈ N∞, j /∈ N∞.
After some iteration, by (d) above, every node in N∞ must have nonnega-
tive surplus, so the sum of surpluses of the nodes in N∞ must be positive
at the start of the flow changes where either gi > 0 or gj > 0. It follows
that
0 <
∑
i∈N∞
si −
∑
{(i,j)∈A|i∈N∞, j /∈N∞}
cij +
∑
{(j,i)∈A|i∈N∞, j /∈N∞}
bji.
For any feasible vector, the above relation implies that the sum of the
divergences of nodes in N∞ exceeds the capacity of the cut [N∞,N −
N∞], which is impossible. It follows that there is no feasible flow vector,
contradicting the hypothesis. Thus the algorithm must terminate. Since
upon termination we have gi ≤ 0 for all i and the problem is assumed
feasible, it follows that gi = 0 for all i. Hence the final flow vector x is
feasible and by (a) above it satisfies -CS together with the final p. By
Prop. 7.10, if  < 1/N , x is optimal. Q.E.D.
7.4.1 Computational Complexity – -Scaling
We now discuss the running time of the -relaxation method. As in Section
7.1.4, we first focus on the case where  is fixed, and we subsequently
consider the -scaling case where  is progressively reduced as in Section
7.1.4. We continue to assume that the problem data and the starting flows
are integer. As in Section 7.1.4, for the case where  is fixed, we assume
that the cost coefficients aij, and all the initial node prices are integer
multiples of . Under this assumption, it is seen from the price change
operation (7.62) in Step 4 that all node prices will be integer multiples of 
throughout the algorithm, implying that each price rise is of size at least
.
For purposes of easy reference, let us call the operation of Step 4 a
price rise at node i, and let us call the operation of Step 2 (or Step 3) a
flow push on arc (i, j) [a flow push on arc (j, i), respectively]. A flow push
on arc (i, j) [or arc (j, i)] is said to be saturating if it results in setting
Sec. 7.4 The -Relaxation Method 311
the flow xij to its upper bound cij (the flow xji to its lower bound bij ,
respectively); otherwise, the flow push is said to be nonsaturating. The
complexity analysis revolves around bounding the number of price rises,
and saturating and nonsaturating flow pushes. We first bound the number
of price rises.
Proposition 7.12: Assume that for some scalar r ≥ 1, the initial
price vector p0 for the -relaxation method satisfies r-CS together with
some feasible flow vector x0. Then, the -relaxation method performs
at most (r + 1)(N − 1) price rises per node.
Proof: Consider the pair (x, p) at the beginning of an -relaxation iter-
ation. Since the surplus vector g = (g1, . . . , gN ) is not zero, and the flow
vector x0 is feasible, we conclude that for each node s with gs > 0 there
exists a node t with gt < 0 and a path H from t to s that contains no cycles
and is such that:
bij ≤ x0ij < xij ≤ cij , ∀ (i, j) ∈ H+, (7.63)
bij ≤ xij < x0ij ≤ cij , ∀ (i, j) ∈ H−, (7.64)
where H+ is the set of forward arcs of H and H− is the set of backward
arcs of H. [This can be seen from the conformal realization theorem (Prop.
1.1) as follows. For the flow vector x − x0, the net outflow from node t is
−gt > 0 and the net outflow from node s is −gs < 0 (here we ignore the
flow supplies), so by the conformal realization theorem, there is a path H
from t to s that contains no cycle and conforms to the flow x − x0, that
is, xij − x0ij > 0 for all (i, j) ∈ H+ and xij − x0ij < 0 for all (i, j) ∈ H−.
Equations (7.63) and (7.64) then follow.]
Since the pair (x, p) satisfies -CS, we have using Eqs. (7.63) and
(7.64),
pi − pj ≥ aij − , ∀ (i, j) ∈ H+, (7.65)
pi − pj ≤ aij + , ∀ (i, j) ∈ H−. (7.66)
Similarly, since the pair (x0, p0) satisfies r-CS, we have
p0i − p0j ≤ aij + r, ∀ (i, j) ∈ H+, (7.67)
p0i − p0j ≥ aij − r, ∀ (i, j) ∈ H−. (7.68)
Combining Eqs. (7.65)-(7.68), we obtain
pi − pj ≥ p0i − p0j − (r + 1), ∀ (i, j) ∈ H+,
pi − pj ≤ p0i − p0j + (r + 1), ∀ (i, j) ∈ H−.
312 Auction Algorithms Chap. 7
Applying the above inequalities for all arcs of the path H, we get
pt − ps ≥ p0t − p0s − (r + 1)|H|, (7.69)
where |H| denotes the number of arcs of the path H. We observed earlier
that if a node has negative surplus at some time, then its price is unchanged
from the beginning of the method until that time. Thus pt = p0t . Since
the path contains no cycles, we also have that |H| ≤ N − 1. Therefore, Eq.
(7.69) yields
ps − p0s ≤ (r + 1)|H| ≤ (r + 1)(N − 1). (7.70)
Since only nodes with positive surplus can increase their prices and each
price rise increment is at least , we conclude from Eq. (7.70) that the
total number of price rises that can be performed for node s is at most
(r + 1)(N − 1). Q.E.D.
The upper bound on the number of price rises given in Prop. 7.12
turns out to be tight, in the sense that examples can be found where rN
price rises occur at a number of nodes that is proportional to N . Under
these circumstances, the total number of price rises performed by the -
relaxation method is no better than O(rN2). The following example, from
Bertsekas and Tsitsiklis [1989], illustrates that the bound O(rN2) cannot
be improved.
Example 7.2:
Consider an assignment problem with 2n nodes, nodes s1, ..., sn being sinks
(persons) and t1, ..., tn being sources (objects). The arcs are (sk, tk) for k =
1, ..., n, and (sk, tk+1) for k = 1, ..., n−1. All arcs have unit capacity and zero
cost. The problem may also be viewed as a max-flow problem by adjoining
a “super source” node s and arcs (s, sk), along with a “super sink” node
t and arcs (tk, t). Suppose that the -relaxation method is applied to the
assignment version of this example, with  = 1, zero initial prices, and the
rule that whenever it is possible to push flow away from a node on more than
one arc, the one that is uppermost in Fig. 7.14(a) is selected. The nodes are
chosen for iteration in the order 1, 2, ..., n.
We claim that the -relaxation algorithm as applied to the example of
Fig. 7.14(a) requires n2 price rises. The final price of node sk is 2k − 1, and
that of tk is 2k − 2. We prove this by induction. When n = 1, a single price
rise at s1 and the ensuing flow adjustment yield a solution in which s1 has
price 1, t1 has price 0, and s1 is assigned to t1. This establishes the base case
of the induction. Now assume the claim is true for the problem of size n− 1;
we establish it for the problem of size n. After n price rises, the configuration
of Fig. 7.14(b) will be attained. This leaves nodes s2, ..., sn and t2, ..., tn in
precisely the same state as after n − 1 price rises in a problem of size n − 1.
By induction, after another
(n − 1)2 − (n − 1) = n2 − 3n + 2
Sec. 7.4 The -Relaxation Method 313
s1
tn
t2
tn-1
t1
sn
s2
sn-1
(a)
s1
tn
t2
tn-1
t1
sn
s2
sn-1
1
1 0
0
2n - 3 2n - 4
2n - 62n - 5
(c)
s1
tn
t2
tn-1
t1
sn
s2
sn-11
1
1
1
0
0
0
0
(b)
Figure 7.14: (a) An assignment example in which the number of price rises
required by the -relaxation method is proportional to N2. Note that the only
feasible solution has each sk assigned to the corresponding tk. (b) The assign-
ment example after n price rises, starting with zero prices. Prices are shown
next to the corresponding node. Only arcs with positive flow are depicted.
(c) The intermediate result after (n − 1)2 + 1 price rises.
price rises, the algorithm reaches the configuration of Fig. 7.14(c). Following
the rules of -relaxation, the reader can confirm that the sequence of nodes
now iterated on is t2, s2, t3, s3, . . . , tn, sn, and the promised prices are obtained
after 2(n − 1) further price rises. Following this, the nodes are processed in
the opposite order, and a primal feasible solution is obtained in 2n additional
iterations (but no further price rises). The total number of price rises is
n + (n2 − 3n + 2) + 2(n − 1) = n2.
This establishes the induction.
The total number of nodes here is N = 2n. Hence the number of
price rises is (N/2)2 = N2/4, and increases with N at the same rate as its
theoretical bound.
We now introduce the notion of the admissible graph, which will play
an important role in the subsequent complexity analysis. For a given pair
(x, p) satisfying -CS, consider an arc set A∗ that contains all candidate
list arcs oriented in the direction of flow change. In particular, for each arc
(i, j) in the forward portion of the candidate list of a node i, we introduce
an arc (i, j) in A∗, and for each arc (j, i) in the backward portion of the
candidate list of node i, we introduce an arc (i, j) in A∗ (thus the direction
of the latter arc is reversed). The set of nodes N and the set A∗ define the
admissible graph G∗ = (N ,A∗). Note that an arc can be in the candidate
list of at most one node, so the admissible graph is well-defined.
For good performance of the -relaxation method, it may be impor-
tant to start with a flow-price vector pair (x, p) satisfying -CS, and such
314 Auction Algorithms Chap. 7
that the corresponding admissible graph G∗ is acyclic. One possibility is to
select an initial price vector p and to set the initial arc flow xij for every
arc (i, j) ∈ A so that the flow-price pair (x, p) satisfies 0-CS; for example
xij =
{
bij if pi − pj ≤ aij ,
cij if pi − pj > aij , ∀ (i, j) ∈ A. (7.71)
It can be seen that with this choice, -CS is satisfied for every arc (i, j) ∈ A,
and that the initial admissible graph is empty and thus acyclic. Figure 7.15
provides an example illustrating the importance of starting with an acyclic
admissible graph.
1
2
s  = 02
Flow range: [0,1]
Cost = 2ε
s   = 11
3
4
s   = - 14
3s   = 0
Flow range: [0,1]
Cost = 2ε
Cost = - ε
Flow range: [0,R ]
Cost = -  ε
Flow range: [0,R ]
Figure 7.15: Example showing the importance of starting with an admissible
graph that is acyclic. Initially, we choose x = 0 and p = 0, which do satisfy -CS.
The initial admissible graph consists of arcs (2, 3) and (3, 2). The algorithm will
start with a price rise of node 1 to p1 = 2, followed by a flow push of 1 unit from
node 1 to node 2. Following this, node 2 will push 1 unit of flow to node 3, node
3 will push 1 unit of flow to node 2, and this will be repeated R times, until the
arcs (2, 3) and (3, 2) become saturated. Thus the running time is proportional to
the capacity R.
On the other hand, it turns out that if we choose the initial flow-price
pair so that the admissible graph is initially acyclic, the algorithm cannot
create cycles in this graph, and the type of poor performance illustrated in
Fig. 7.15 cannot occur. This is shown in the following proposition.
Proposition 7.13: If the admissible graph is initially acyclic, it re-
mains acyclic throughout the -relaxation method.
Proof: We use induction. Assume that the admissible graph G∗ is acyclic
up to the start of the mth iteration, for some m ≥ 1. We will prove that
Sec. 7.4 The -Relaxation Method 315
following the mth iteration G∗ remains acyclic. Clearly, after a flow push
the admissible graph remains acyclic, since it either remains unchanged, or
some arcs are deleted from it. Thus we only have to prove that after a price
rise at a node i, no cycle involving i is created. We note that, after a price
rise at node i, all incident arcs to i in the admissible graph at the start of
the mth iteration are deleted and new arcs incident to i are added. We
claim that i cannot have any incoming arcs which belong to the admissible
graph. To see this, note that, just before a price rise at node i, we have
pj − pi ≤ aji + , ∀ (j, i) ∈ A,
and since each price rise is at least , we must have
pj − pi − aji ≤ 0, ∀ (j, i) ∈ A,
after the price rise. Then, (j, i) cannot be in the candidate list of node j.
By a similar argument, we have that (i, j) cannot be in the candidate list
of j for all (i, j) ∈ A. Thus, after a price rise at i, node i cannot have
any incoming incident arcs belonging to the admissible graph, so no cycle
involving i can be created. Q.E.D.
In order to obtain a sharper complexity result, we introduce a special
implementation of the -relaxation method, called the sweep implementa-
tion, whereby nodes are chosen for iteration in a way that enhances compu-
tational efficiency (for an illustration, see Fig. 7.16). We assume here that
the initial admissible graph is acyclic. We introduce an order in which the
nodes are chosen in iterations. All the nodes are kept in a list T , which is
traversed from the first to the last element. The order of the nodes in the
list is consistent with the successor order implied by the admissible graph,
that is, if a node j is a successor of a node i, then j must appear after i
in the list. If the initial admissible graph is empty, as is the case with the
initialization of Eq. (7.71), the initial list is arbitrary. Otherwise, the ini-
tial list must be consistent with the successor order of the initial admissible
graph. The list is updated in a way that maintains the consistency with
the successor order. In particular, let i be a node on which we perform an
-relaxation iteration, and let Ni be the subset of nodes of T that are after
i in T. If the price of i changes, then node i is removed from its position in
T and placed in the first position of T . The next node chosen for iteration,
if Ni is nonempty, is the node i′ ∈ Ni with positive surplus which ranks
highest in T . Otherwise, the positive surplus node ranking highest in T is
picked. It can be seen that with this rule of repositioning nodes following
a price rise, the list order is consistent with the successor order implied by
the admissible graph throughout the method.
A sweep cycle is a set of iterations whereby all nodes are chosen once
from the list T and an -relaxation iteration is performed on those nodes
that have positive surplus. The idea of the sweep implementation is that
316 Auction Algorithms Chap. 7
1 2
3
Direction of Sweeping
+
-
00
0
0
+
+
+
+
-
Figure 7.16: Illustration of the admissible graph consisting of the + - unblocked
arcs and the − - unblocked arcs with their directions reversed. These arcs spec-
ify the direction along which flow can be changed according to the rules of the
algorithm. A “+” (or “-” or “0”) indicates a node with positive (or negative or
zero) surplus. The algorithm is operated so that the admissible graph is acyclic at
all times. The sweep implementation requires that the high ranking nodes (e.g.,
nodes 1 and 2 in the graph) are chosen for iteration before the low ranking nodes
(e.g., node 3 in the graph).
an -relaxation iteration at a node i that has predecessors with positive
surplus may be wasteful, since the surplus of i will be set to zero and
become positive again through a flow push at a predecessor node.
We have the following proposition that estimates the number of sweep
cycles required for termination.
Proposition 7.14: Assume that for some scalar r ≥ 1, the initial
price vector for the sweep implementation of the -relaxation method
satisfies r-CS together with some feasible flow vector. Then, the
number of sweep cycles up to termination is O(rN2).
Proof: Consider the start of any sweep cycle. Let N+ be the set of nodes
with positive surplus that have no predecessor with positive surplus; let N 0
be the set of nodes with nonpositive surplus that have no predecessor with
positive surplus. Then, as long as no price rise takes place during the cycle,
all nodes in N 0 remain in N 0, and an iteration on a node i ∈ N+ moves
i from N+ to N 0. So if no node changed price during the cycle, then all
nodes in N+ will be moved to N 0 and the method terminates. Therefore,
there is a price rise in every cycle except possibly the last one. Since by
Prop. 7.12 there are O(rN2) price rises, the result follows. Q.E.D.
Sec. 7.4 The -Relaxation Method 317
We now bound the running time for the sweep implementation of the
-relaxation method.
Proposition 7.15: Consider the -relaxation method with the sweep
implementation, and assume that for some scalar r ≥ 1 the initial
price vector p0 satisfies r-CS together with some feasible flow vector
x0. Then, the method requires O(rN3) operations up to termination.
Proof: The dominant computational requirements are:
(1) The computation required for price rises.
(2) The computation required for saturating flow pushes.
(3) The computation required for nonsaturating flow pushes.
According to Prop. 7.12, there are O(rN) price rises per node, so the
requirements for (1) above are O(rNA) operations. Furthermore, when-
ever a flow push at an arc is saturating, it takes at least one price rise
at one of the end nodes of the arc before the arc’s flow can be changed
again. Thus the total requirement for (2) above is O(rNA) operations
also. Finally, for (3) above we note that for each sweep cycle there can be
only one nonsaturating flow push per node. Thus an estimate for (3) is
O(N · total number of sweep cycles) which, by Prop. 7.12, is O(rN3) oper-
ations. Adding the computational requirements for (1), (2), and (3), and
using the fact A ≤ N2, the result follows. Q.E.D.
-Scaling
Let us now apply the -scaling approach to the -relaxation method. Sim-
ilar to the case of the auction algorithm (cf. Section 7.1.4), the idea is
to use repeated applications of the method, called scaling phases, with
progressively smaller values of . Each scaling phase uses price and flow
information obtained from the preceding one. The kth scaling phase con-
sists of applying the -relaxation method with  = k, where k is updated
by
k+1 = max
{
k
θ
,
1
N + 1
}
, k = 0, 1, . . . ,
where θ is an integer with θ > 1. The first scaling phase is started with
zero initial prices and an 0 that is a fixed fraction of the arc cost range
C = max(i,j)∈A aij . The total number of scaling phases is k, which is the
first positive integer k for which k−1 is equal to 1/(N + 1). Thus the
number of scaling phases is O
(
log(NC)
)
.
Let pk denote the initial price vector for the (k + 1)st scaling phase.
We have p0 = 0, and we assume that for k ≥ 1, pk is the price vector
318 Auction Algorithms Chap. 7
obtained at the end of the kth scaling phase. As in Section 7.1.4, at the
beginning of the (k + 1)st scaling phase, we make a correction of size at
most k to each aij so that it is divisible by k [no correction is made in the
last phase since the aij are integer and the final value of  is 1/(N + 1)].
Thus the arc cost coefficients in the (k + 1)st scaling phase, denoted akij ,
are all divisible by k, and satisfy
|akij − aij | ≤ k, ∀ (i, j) ∈ A.
The correction of the arc cost coefficients guarantees that all price rise
increments and prices are integer multiples of the prevailing value of .
The initial flow of each arc (i, j) for the (k + 1)st scaling phase is
xij =
{
bij if pki − pkj ≤ akij ,
cij if pki − pkj > akij .
With this choice, the initial admissible graph is empty and is therefore
acyclic.
As in Section 7.1.4, we observe that in the (k +1)st scaling phase the
initial price vector pk satisfies rk-CS with some feasible flow vector (for
k ≥ 1, this is the flow vector obtained at the end of the kth scaling phase).
Here r is a constant that depends on θ. Furthermore, pk satisfies the other
assumptions needed for Prop. 7.15 to apply. We conclude that the (k+1)st
scaling phase has a running time of O(N3). Since the number of scaling
phases is O
(
log(NC)
)
, we obtain the following:
Proposition 7.16: The running time of the -relaxation method us-
ing the sweep implementation and -scaling as described above is
O
(
N3 log(NC)
)
.
7.4.2 Implementation Issues
The efficient implementation of the -relaxation method requires a num-
ber of techniques that while not suggested by the complexity analysis, are
essential for good practical performance.
Data Structures
The main operations of auction algorithms involve scanning the incident
arcs of nodes; this is a shared feature with dual ascent methods. For this
reason the data structures and implementation ideas discussed in connec-
tion with dual ascent methods, also apply to auction algorithms. In par-
ticular, for the max-flow and the minimum cost flow problems, using the
Sec. 7.4 The -Relaxation Method 319
FIRST IN , FIRST OUT , NEXT IN , and NEXT OUT arrays, described
in Section 6.5, is convenient. In addition, a similar set of arrays can be
used to store the arcs of the candidate lists in the -relaxation method.
Contrary to what complexity analysis suggests, it is not clear whether
the candidate list organization of the sweep implementation improves the
practical performance, in view of the additional overhead it requires.
Surplus Scaling
When applying -scaling, except for the last scaling phase, it is not essential
to reduce the surpluses of all nodes to zero; it is possible to terminate a
scaling phase prematurely, and reduce  further, in an effort to economize
on computation. A technique that is typically quite effective is to iterate
only on nodes whose surplus exceeds some threshold, which is gradually
reduced to zero with each scaling phase. The threshold is usually set by
some heuristic scheme.
Negative Surplus Node Iterations
It is possible to define a symmetric form of the -relaxation iteration that
starts from a node with negative surplus and decreases (rather than in-
creases) the price of that node. Furthermore, one can mix positive surplus
and negative surplus iterations in the same algorithm; this is analogous
to the combined forward/reverse auction algorithm for assignment and the
forward/reverse auction algorithm for shortest paths. However, if the two
types of iterations are mixed arbitrarily, the algorithm is not guaranteed
to terminate even for a feasible problem; for an example, see Bertsekas and
Tsitsiklis [1989], p. 373. For this reason, some care must be exercised in
mixing the two types of iterations in order to guarantee that the algorithm
eventually makes progress.
Dealing with Infeasibility
The issues and methods relating to infeasibility are similar to those dis-
cussed in Section 7.1.5, in connection with the assignment problem. One
possibility is to monitor infeasibility by checking the price levels. If the
problem is infeasible, the -relaxation method will either terminate with
gi ≤ 0 for all i and gi < 0 for at least one i, in which case infeasibility
will be detected, or else it will perform an infinite number of iterations
and, consequently, an infinite number of flow pushes and price rises. In
the latter case, from the proof of Prop. 7.11 it can be seen that the prices
of some of the nodes will diverge to infinity. This, together with a bound
on the total price change of a node given in Exercise 7.15, can be used to
detect infeasibility.
320 Auction Algorithms Chap. 7
Alternatively, similar to the assignment problem, we can detect in-
feasibility by checking periodically for the presence of a saturated cut sep-
arating the set of nodes with positive surplus from the set of nodes with
negative surplus. Such a cut will eventually be discovered if and only if
the problem is infeasible. We may then try to optimize the cost function
over the set of all maximally feasible flows, as discussed in Section 3.1. The
flow obtained by the method upon detection of a saturated cut can be used
to decompose the original problem into two or three component minimum
cost flow problems, as discussed in Section 3.1, and each of these problems
can be solved separately.
7.5 THE AUCTION/SEQUENTIAL SHORTEST PATH
ALGORITHM
In this section, we develop an auction algorithm for the solution of the min-
imum cost flow problem, based on a sequential shortest path augmentation
approach similar to the one discussed in Section 6.2. The main difference
is that the shortest paths are constructed using the auction/shortest path
algorithm of Section 2.6 rather than using a variant of Dijkstra’s algorithm.
An important feature of the auction approach is that it allows useful infor-
mation to be passed from one shortest path construction to the next in the
form of prices, similar to the max-flow algorithm of Section 3.3. This ac-
counts for a better theoretical and practical performance of the algorithm
of this section over the one of Section 6.2.
We recall that the primal-dual (or sequential shortest path) method of
Section 6.2 maintains a pair (x, p) satisfying CS, and that at each iteration
it constructs a shortest path from some node with positive surplus to the set
of nodes with negative surplus, along which it performs an augmentation
of the current flow vector. The shortest path computation is performed
in the reduced graph GR =
(
N ,AR
)
whose arc set AR consists of an arc
(i, j) for each arc (i, j) ∈ A with xij < cij , and an arc (j, i) for each arc
(i, j) ∈ A with bij < xij . The arc lengths are aij + pj − pi for the arcs
(i, j) ∈ A with xij < cij , and pi − aij − pj for the arcs (j, i) corresponding
to arcs (i, j) ∈ A with bij < xij .
It is in principle possible to solve the shortest path problem using
any shortest path method that requires nonnegative arc lengths, such as
the Dijkstra-like method used in Section 6.2. The development of the
auction/max-flow algorithm in Section 3.3 motivates using the auction algo-
rithm for shortest paths because of its ability to transfer price information
from one shortest path computation to the next. This method maintains
a path, which is extended or contracted by a single arc at each iteration.
Unfortunately, however, the method cannot be used conveniently in the
context of the sequential shortest path method because it requires that all
cycles have strictly positive length, while the reduced graph has cycles with
Sec. 7.5 The Auction/Sequential Shortest Path Algorithm 321
zero length [each arc (i, j) with bij < xij < cij gives rise to the zero length
arcs (i, j) and (j, i) in the reduced graph]. Thus the path maintained by
the method can “double up on itself” and close a cycle.
To overcome this difficulty, we use an approach that blends the auc-
tion/shortest path construction process with the remainder of the algo-
rithm. In this approach, we use -perturbations of the arc lengths, related
to -CS, which ensure that the path generated by the auction/shortest path
method does not close a cycle through an extension. We first introduce
some terminology.
We recall from Section 7.4 that given a flow-price pair (x, p) satisfying
-CS, an arc (i, j) is said to be +-unblocked if
pi = pj + aij +  and xij < cij ,
and an arc (j, i) is said to be −-unblocked if
pi = pj − aji +  and bji < xji.
The admissible graph corresponding to (x, p) is defined as G∗ = (N ,A∗),
where the arc set A∗ consists of an arc (i, j) for each +-unblocked arc
(i, j) ∈ A, and an arc (i, j) for each −-unblocked arc (j, i) ∈ A.
We recall that a path P is a sequence of nodes (n1, n2, . . . , nk) and a
corresponding sequence of k − 1 arcs such that the ith arc in the sequence
is either (ni, ni+1) or (ni+1, ni). For any path P , we denote by s(P ) and
t(P ) the start and terminal nodes of P , respectively, and by P+ and P−
the sets of forward and backward arcs of P , respectively. The path P is
said to be -unblocked if all arcs of P+ are +-unblocked, and all arcs of
P− are −-unblocked. If P is -unblocked, and the start node s(P ) has
positive surplus and the terminal node t(P ) has negative surplus, then P
is an augmenting path. An augmentation along such a path consists of
increasing the flow of all arcs in P+ and reducing the flow of all arcs in P−
by the common increment
δ = min
{
gs(P ), −gt(P ), min
(i,j)∈P+
{cij − xij}, min
(i,j)∈P−
{xij − bij}
}
.
Given a path P = (n1, n2, . . . , nk), a contraction of P is the opera-
tion that deletes the terminal node of P together with the corresponding
terminal arc. An extension of P by an arc (nk, nk+1) or an arc (nk+1, nk),
replaces P by the path (n1, n2, . . . , nk, nk+1) and adds to P the correspond-
ing arc. For convenience, we allow a path P to consist of a single node i, in
which case extension by an arc (i, j) or (j, i) gives a path with start node
i and terminal node j.
The algorithm to be presented will be called auction/sequential short-
est path algorithm (abbreviated ASSP). It uses a fixed  > 0, and maintains
322 Auction Algorithms Chap. 7
a flow-price pair (x, p) satisfying -CS and also a simple path P (possibly
consisting of a single node). It terminates when all nodes have nonnega-
tive surplus; then either all nodes have zero surplus and x is feasible, or
else some node has negative surplus showing that the problem is infeasible.
Throughout the algorithm, x is integer, and (x, p) and P satisfy:
(a) The admissible graph corresponding to (x, p) is acyclic.
(b) P belongs to the admissible graph, i.e., it is -unblocked. Further-
more, P starts at a node with positive surplus, and all its nodes have
nonnegative surplus.
We assume that at the start of the algorithm we have a pair (x, p) satisfying
-CS, as well as the above two properties. In particular, initially one may
choose any price vector p, select x according to
xij =
{
cij if pi ≥ aij + pj ,
bij if pi < aij + pj ,
and choose P to consist of a single node with positive surplus. For these
choices, -CS is satisfied and the corresponding admissible graph is acyclic,
since its arc set is empty.
At each iteration, the path P is either extended or contracted. In
the case of a contraction, the price of the terminal node of P is strictly
increased. In the case of an extension, no price rise occurs, but if the
new terminal node has negative surplus, P becomes augmenting, and an
augmentation along P is performed. Then the path P is replaced by the
degenerate path that consists of a single node with positive surplus, and
the process is repeated.
Iteration of the ASSP Algorithm
Let i be the terminal node of P . If
pi < min
{
min
{(i,j)∈A|xij<cij}
{
aij + pj + 
}
,
min
{(j,i)∈A|bji<xji}
{
pj − aji + 
}} (7.72)
go to Step 1; else go to Step 2.
Step 1 (Contract path): Set
pi := min
{
min
{(i,j)∈A|xij<cij}
{
aij + pj + 
}
,
min
{(j,i)∈A|bji<xji}
{
pj − aji + 
}} (7.73)
Sec. 7.5 The Auction/Sequential Shortest Path Algorithm 323
and if i 	= s(P ), contract P . Go to the next iteration.
Step 2 (Extend path): Extend P by an arc (i, ji) or an arc (ji, i)
that attains the minimum in Eq. (7.72). If the surplus of ji is negative
go to Step 3; otherwise, go to the next iteration.
Step 3 (Augmentation): Perform an augmentation along P . If all
nodes have nonpositive surplus, terminate the algorithm; otherwise,
replace P by a path that consists of a single node with positive surplus
and go to the next iteration.
The following proposition establishes that some basic properties are
maintained by the algorithm.
Proposition 7.17: Suppose that at the start of an iteration of the
ASSP algorithm the following two conditions hold:
(1) (x, p) satisfies -CS and the corresponding admissible graph is
acyclic.
(2) P belongs to the admissible graph, starts at a node with positive
surplus, and all its nodes have nonnegative surplus.
Then these two conditions hold at the start of the next iteration.
Proof: Suppose the iteration involves a contraction. Then it can be seen
that the price increase (7.73) preserves -CS. Furthermore, since only the
price of node i changes and no arc flow changes, the admissible graph
remains unchanged except for the incident arcs of node i. In particular, all
the incident arcs of i in the admissible graph at the start of the iteration are
deleted and the arcs of the admissible graph corresponding to the arcs (i, j)
and (j, i) that attain the minimum in Eq. (7.73) are added. Since all these
arcs are outgoing from i in the admissible graph, a cycle cannot be closed.
Finally, following a contraction, P does not contain the terminal node i, so
it belongs to the admissible graph that we had before the iteration. Thus
P consists of arcs that belong to the admissible graph that we obtain after
the iteration.
Suppose the iteration involves an extension. Then by -CS, we must
have
pi = min
{
min
{(i,j)∈A|xij<cij}
{
aij + pj + 
}
, min
{(j,i)∈A|bji<xji}
{
pj − aji + 
}}
,
at the start of the iteration. It follows that the path P obtained by exten-
sion is simple and -unblocked, since the extension arc (i, ji) must belong
to the admissible graph. Since no price or flow changes with an extension,
324 Auction Algorithms Chap. 7
the -CS conditions and the admissible graph stay unchanged following the
extension. If there is a subsequent augmentation at Step 3 because the
new terminal node ji has negative surplus, the -CS conditions will not be
affected, while the admissible graph will not gain any new arcs, so it will
remain acyclic. Q.E.D.
Note that if we were to take  = 0 (rather than  > 0), the preceding
proof would break down, because we would not be able to prove that the
admissible graph remains acyclic following an augmentation. In particular,
if following an augmentation, the flow of some arc (i, j) lies strictly between
its lower and upper bound, the arcs (i, j) and (j, i) would both belong to
the admissible graph, each with zero length, thereby closing a zero length
cycle.
A sequence of iterations between two successive augmentations (or
the sequence of iterations up to the first augmentation) will be called an
augmentation cycle. Let us fix an augmentation cycle and let p be the price
vector at the start of the cycle. The reduced graph GR = (N ,AR), defined
earlier, will not change in the course of this augmentation cycle, since no
arc flow will change during the cycle, except for the augmentation at the
end. Suppose that we take as arc lengths of the reduced graph the reduced
costs at the start of the cycle plus . In particular, during the cycle, the
arc set AR consists of an arc (i, j) with length aij + pj − pi +  for each
arc (i, j) ∈ A with xij < cij , and an arc (j, i) with length pi − aij − pj + 
for each arc (i, j) ∈ A with bij < xij . Note that, because (x, p) satisfies
-CS, the arc lengths of the reduced graph are nonnegative. However, the
reduced graph does not contain zero length cycles, since any such cycle
must belong to the admissible graph, which is acyclic.
Using these observations, it can now be seen that the augmentation
cycle is just the auction/shortest path algorithm of Section 2.6 applied to
the problem of finding a shortest path from the starting node s(P ) to some
node with negative surplus in the reduced graph GR, using the preceding -
perturbed arc lengths. To understand this, one should view pi − pi during
the augmentation cycle as the price of node i that is maintained by the
auction/shortest path algorithm. The price increments pi − pi obtained
by the auction/shortest path algorithm are added in effect to the starting
prices pi at the end of the augmentation cycle to form the new prices that
will be used for the shortest path construction of the next augmentation
cycle.
By the theory of the auction/shortest path algorithm, a shortest path
in the reduced graph will be found in a finite number of iterations if there
exists at least one path from the starting node s(P ) to some node with
negative surplus. Such a path is guaranteed to exist if the problem is
feasible. Since the augmentation will change all the flows of the final path P
by a positive integer amount, we see that each augmentation cycle reduces
the total absolute surplus
∑
i∈N |gi| by a positive integer. Therefore, there
Sec. 7.5 The Auction/Sequential Shortest Path Algorithm 325
can be only a finite number of augmentation cycles, and we have shown
the following proposition.
Proposition 7.18: Assume that the minimum cost flow problem is
feasible. Then the ASSP algorithm terminates with a pair (x, p) sat-
isfying -CS. The flow vector x is feasible and is optimal if  < 1/N .
It is interesting to try to relate the iterations of the algorithm with it-
erations of the -relaxation method. Each iteration of the algorithm involv-
ing a contraction can be viewed as an iteration of an -relaxation method,
except that the iterating terminal node i may have zero surplus. Each
iteration involving an extension without an augmentation changes neither
the flow nor the price vectors; it merely extends the path P by a single
arc. Finally, each iteration involving an augmentation can be viewed as a
sequence of -relaxation iterations, each pushing the flow increment δ along
the +-unblocked forward arcs and the −-unblocked backward arcs of P .
Thus we may view the algorithm as a variant of the -relaxation method.
-Scaling
As in all auction algorithms, the practical performance of the algorithm
may be degraded by “price wars,” that is, prolonged sequences of itera-
tions involving small price increases. There is a built-in potential for price
wars here because with a small , the reduced graph may contain cycles
with small length, which slow down the underlying auction/shortest path
algorithm. (There is a cycle of length 2 for every arc whose flow lies strictly
between the corresponding flow bounds.) This difficulty can be addressed
by -scaling, that is, by applying the algorithm several times, each time
decreasing  by a constant factor, up to the threshold value of 1/(N + 1),
while using the final prices obtained for one value of  as starting prices for
the next value of . A polynomial complexity bound of O
(
N2A log(NC)
)
,
where C is the cost range
C = max
(i,j)∈A
|aij |,
can be proved for the resulting method, after we introduce modifications
similar to the ones of Section 7.4.1 for the -relaxation method. The un-
scaled version of the method, where  is kept fixed at 1/(N + 1), is pseu-
dopolynomial. These complexity bounds can be derived using the lines of
analysis of Section 7.4.1, and will not be proved here.
In addition to -scaling, there are several implementation techniques,
which have been found to improve performance in practice. We refer to
Bertsekas [1992c] for further details and computational results.
326 Auction Algorithms Chap. 7
7.6 NOTES, SOURCES, AND EXERCISES
The auction algorithm, and the notions of -complementary slackness and
-scaling were first proposed by the author (Bertsekas [1979a]; see also
Bertsekas [1988]). The worst-case complexity of the algorithm was given by
Bertsekas and Eckstein [1988], who used an alternative method of scaling
whereby  is kept constant and the aij are successively scaled to their
final values; see also Bertsekas and Tsitsiklis [1989]. Exercise 7.3 that
deals with the average complexity of the auction algorithm was inspired
by Schwartz [1994], which derives related results for the Jacobi version
of the algorithm and its potential for parallelism. Tutorial presentations
of auction algorithms that supplement this chapter are given in Bertsekas
[1990], [1992a].
Auction algorithms are particularly well-suited for parallel computa-
tion because both the bidding and the assignment phases are highly par-
allelizable. In particular, the bids can be computed simultaneously and
in parallel for all persons participating in the auction. Similarly, the sub-
sequent awards to the highest bidders can be computed in parallel by all
objects that received a bid. In fact these operations maintain their valid-
ity in an asynchronous environment where the bidding phase is executed
with price information that is outdated because of communication delays
between the processors of the parallel computing system. The parallel
computation aspects of the auction algorithm have been explored by Bert-
sekas and Tsitsiklis [1989], Bertsekas and Castañon [1991], Wein and Zenios
[1991], Amini [1994], and Bertsekas, Castañon, Eckstein, and Zenios [1995].
The reverse auction algorithm and its application in asymmetric as-
signment problems is due to Bertsekas, Castañon, and Tsaknakis [1993].
This paper also discusses additional related algorithms, including the mul-
tiassignment algorithm of Exercise 7.10. An extensive computational study
of forward and reverse auction algorithms is given in Castañon [1993]. Still
another auction algorithm of the forward-reverse type for asymmetric as-
signment problems is given by Bertsekas and Castañon [1992]. An extension
of the auction algorithm to transportation problems based on the notion
of similar persons is given in Bertsekas and Castañon [1989].
Preflow-push methods for the max-flow problem originated with the
work of Karzanov [1974], and Shiloah and Vishkin [1982]. They have been
the subject of much development in the late eighties; see Goldberg and
Tarjan [1986], Ahuja and Orlin [1989], Ahuja, Magnanti, and Orlin [1989],
Cheriyan and Maheshvari [1989], Derigs and Meier [1989], and the refer-
ences quoted therein. The O
(
N2A1/2
)
estimate on the running time of the
method that uses the highest price node for iteration is due to Cheriyan
and Maheshvari [1989]. Slightly better estimates are possible through the
use of sophisticated but somewhat impractical data structures (see the sur-
vey by Ahuja, Magnanti, and Orlin [1989]). The material of Section 7.3.3
Sec. 7.6 Notes, Sources, and Exercises 327
is from Bertsekas [1993b], which also shows the mathematical equivalence
of the auction algorithm and the -relaxation method.
The -relaxation method is due to the author; it was first published
in Bertsekas [1986a], [1986b], although it was known much earlier (since
the development of the mathematically equivalent auction algorithm). The
sweep implementation was given in Bertsekas [1986b]. Various other imple-
mentations can be found in Bertsekas and Eckstein [1987], [1988], Bertsekas
and Tsitsiklis [1989], Goldberg [1987], and Goldberg and Tarjan [1990].
The worst-case complexity of -scaling for the -relaxation method was
first analyzed by Goldberg [1987]. The best known running time estimate
for a scaled implementation of -relaxation is O
(
NA log(N) log(NC)
)
; this
implementation is due to Goldberg and Tarjan [1990], and uses dynamic
trees, a complicated (and somewhat impractical) data structure. An ef-
ficient implementation of the -relaxation method, and a corresponding
code named CS2 are given by Goldberg [1993]. The -relaxation method
is better suited for parallel computation than the other minimum cost flow
methods described in this book; see Bertsekas and Tsitsiklis [1989], Phillips
and Zenios [1989], Bertsekas, Castañon, Eckstein, and Zenios [1995], Be-
raldi and Guerriero [1997], Beraldi, Guerriero, and Musmanno [1997], and
Censor and Zenios [1997] for a discussion of various implementations and
related issues. The auction/sequential shortest path algorithm is due to
Bertsekas [1992c]. This algorithm is competitive to the -relaxation method
in terms of practical performance. It has also found use as a preprocessor
for other algorithms such as the relaxation method of Chapter 6, and the
RELAX code of Bertsekas and Tseng [1994].
Generally, computational experience suggests that auction algorithms
are competitive with the primal and dual cost improvement methods of
Chapters 5 and 6. This is particularly so for the assignment and for the
max-flow problems, for which good implementations of auction algorithms
seem to outperform their competitors in practice. For general minimum
cost flow problems, the situation is less clear, and much depends on the
structure of the problem being solved. Thus, in practice, one may want
to experiment with several types of algorithms on a given problem.
E X E R C I S E S
7.1
Consider the Gauss-Seidel version of the auction algorithm, where only one person
can bid at each iteration. Show that, as a result of a bid, the dual cost can be
328 Auction Algorithms Chap. 7
degraded by at most .
7.2 (A Refinement of the Termination Tolerance)
Show that the assignment obtained upon termination of the auction algorithm is
within (n−1) of being optimal (rather than n). Also, for every n ≥ 2, construct
an example of an assignment problem with integer data such that the auction
algorithm terminates with a nonoptimal assignment when  = 1/(n − 1). (Try
first n = 2 and n = 3, and generalize.) Hint : Modify slightly the algorithm so
that when the last object is assigned, its price is increased by vi − wi (rather
than vi − wi + ). Then the assignment obtained upon termination satisfies the
-CS condition for n− 1 objects and the CS condition ( = 0) for the last object.
Modify the proof of Prop. 1.4 in Section 1.3.3.
7.3
This problem uses a rough (and flawed) argument to estimate the average com-
plexity of the auction algorithm. We assume that at each iteration, only one
person submits a bid (i.e., the Gauss-Seidel version of the algorithm is used).
Furthermore, every object is the recipient of a bid with equal probability (1/n),
independently of the results of earlier bids. (This assumption clearly does not
hold, but seems to capture somewhat the real situation where the problem is
fairly dense and -scaling is used.)
(a) Show that when k objects are unassigned the average number of iterations
needed to assign a new object is n/k.
(b) Show that, on the average, the number of iterations is n(1+1/2+· · ·+1/n),
which can be estimated as O(n log n).
(c) Assuming that the average number of bids submitted by each person is the
same for all persons, show that the average running time is O(A log n).
7.4
Consider the auction algorithm applied to assignment problems with benefits in
the range [0, C], starting with zero prices.
(a) Show that for dense problems (every person can bid for every object) an
object can receive a bid in at most 1 + C/ iterations.
(b) Use the example of Fig. 7.17 (due to D. Castañon) to show that, in general,
some objects may receive a bid in a number of iterations that is proportional
to nC/.
7.5
Consider the max-flow problem of Fig. 7.18.
Sec. 7.6 Notes, Sources, and Exercises 329
CC
C
C
C
C
0
0
0
C
0
C
C
Figure 7.17: Assignment problem for which some
objects receive a number of bids that is proportional
to nC/. The arc values are shown next to the cor-
responding arcs.
(a) Apply the preflow-push algorithm with initial prices p1 = 0, and pi = N − i
for i = 2, . . . , N . Use two different methods to choose the node for iteration:
(1) Select the node with highest price, and (2) Select the node with lowest
price. Explain why the first method works better, and speculate on the
reason why this might be true in general.
(b) Write a computer program to solve the problem of Fig. 7.18 using the
preflow-push algorithm with initial prices p1 = N and pi = 0 for i =
2, . . . , N . Use two different methods to choose the node for iteration: (1)
Select the node with highest price, and (2) Select the node at random with
equal probability among the possible choices. Plot the number of iterations
required with the two methods as a function of N , starting with N =
1000 and up to some reasonable number. Can you make any experimental
inferences about computational complexity.
7.6
Consider the following graph for an infeasible 7× 7 assignment problem: persons
1, 2, and 3 can be assigned only to objects 1 and 2; persons 4 and 5 can be
assigned only to objects 1,2, 3, 4, and 5; persons 6 and 7 can be assigned only
to objects 6 and 7. Determine the problem’s decomposition into feasible and
independent components (cf. the discussion of Sections 7.1.5 and 3.1.4).
330 Auction Algorithms Chap. 7
1
2 3 N - 1.  .  .  . N 
Figure 7.18: Graph for the max-flow problem of Exercise 7.5. The source is
node 1 and the sink is node N . All arcs (1, i), i = 2, . . . , N have capacity 1. All
other arcs have capacity N .
7.7 (Using the Third Best Value in the Auction Algorithm)
Frequently in the auction algorithm the two best objects for a given person do
not change between two successive bids of that person. This exercise develops an
implementation idea that attempts to exploit this fact by using a test to check
whether the two best objects from the preceding bid continue to be best. If the
test is passed, the computation of the values aij − pj of the remaining objects is
unnecessary.
Suppose that at a given iteration, when we calculate the bid of the person
i on the basis of a price vector p we compute the best value vi = maxj∈A(i){aij −
pj}, the best object j1 = arg maxj∈A(i){aij − pj}, the second best value wi =
maxj∈A(i), j =j1{aij − pj}, the second best object j2 = arg maxj∈A(i), j =j1{aij −
pj}, and the third best value yi = maxj∈A(i), j =j1, j =j2{aij − pj}. Suppose that
at a subsequent iteration when person i bids based on a price vector p, we have
aij1 − pj1 ≥ yi and aij2 − pj2 ≥ yi. Show that j1 and j2 continue to be the two
best objects for i (although j1 need not be better than j2).
7.8 (Equivalence of Two Forms of Reverse Auction)
Show that the iteration of the Gauss-Seidel version of the reverse auction algo-
rithm for the (symmetric) assignment problem can equivalently be described by
the following iteration, which maintains an assignment and a pair (π, p) satisfying
the -CS condition of Section 7.2.1 (cf. Definition 7.1):
Step 1: Choose an unassigned object j.
Step 2: Decrease pj to the highest level for which two or more persons will
increase their profit by at least  after assignment to j, that is, set pj to the
highest level for which aij − pj ≥ πi +  for at least two persons i, where πi is the
profit of i at the start of the iteration.
Step 3: From the persons in Step 2, assign to j a person ij that experiences
maximum profit increase after assignment to j, and cancel the prior assignment
of ij if he or she was assigned at the start of the iteration. Set the profit of ij to
aijj − pj .
Sec. 7.6 Notes, Sources, and Exercises 331
7.9
Consider the asymmetric assignment problem and apply forward auction starting
with the zero price vector and the empty assignment. Show that, for a feasible
problem, the algorithm terminates with a feasible assignment that is within m
of being optimal.
7.10 (Auction Algorithms for Multiassignment Problems)
Consider the following assignment problem, where it is possible to assign more
than one object to a single person:
maximize
∑
(i,j)∈A
aijxij
subject to
∑
j∈A(i)
xij ≥ 1, ∀ i = 1, . . . , m,∑
i∈B(j)
xij = 1, ∀ j = 1, . . . , n,
0 ≤ xij , ∀ (i, j) ∈ A.
We assume that m < n.
(a) Show that a dual problem is given by
minimize
m∑
i=1
πi +
n∑
j=1
pj + (n − m)λ
subject to πi + pj ≥ aij , ∀ (i, j) ∈ A,
λ ≥ πi, ∀ i = 1, . . . , m.
(b) Define a multiassignment S to be a set of pairs (i, j) ∈ A such that for each
object j, there is at most one pair (i, j) in S. A multiassignment S and a
pair (π, p) are said to satisfy -CS if
πi + pj ≥ aij − , ∀ (i, j) ∈ A,
πi + pj = aij , ∀ (i, j) ∈ S,
πi = max
k=1,...,m
πk, if i is multiassigned under S.
Show that if a feasible multiassignment S satisfies -CS together with a
pair (π, p), then S is within n of being optimal for the multiassignment
problem. Furthermore, the triplet (π̂, p, λ̂), where
π̂i = πi + , ∀ i = 1, . . . , m,
λ̂ = max
k=1,...,m
π̂k,
is within n of being an optimal solution of the dual problem.
(c) Derive a forward-reverse auction algorithm that maintains -CS and termi-
nates with a feasible multiassignment that is within n of being optimal.
332 Auction Algorithms Chap. 7
7.11 (A Variation of the Asymmetric Assignment Problem)
Consider a problem which is the same as the asymmetric assignment problem
with the exception that in a feasible assignment S there can be at most one
incident arc for every person and at most one incident arc for every object (that
is, there is no need for every person, as well as for every object, to be assigned).
The corresponding linear program is
maximize
∑
(i,j)∈A
aijxij
subject to
∑
j∈A(i)
xij ≤ 1, ∀ i = 1, . . . , m,∑
i∈B(j)
xij ≤ 1, ∀ j = 1, . . . , n,
0 ≤ xij , ∀ (i, j) ∈ A.
(a) Show that this problem can be converted to an asymmetric assignment
problem where all persons must be assigned. Hint : For each person i
introduce an artificial object i′ and a zero cost arc (i, i′).
(b) Adapt and streamline the auction algorithm for asymmetric assignment
problems of Section 7.1 to solve the problem.
7.12 (A Refinement of the Optimality Conditions)
(a) Consider the asymmetric assignment problem with integer data, and sup-
pose that we have a feasible assignment S and a pair (π, p) satisfying the
first two -CS conditions (7.24) and (7.25) with  < 1/m. Show that in
order for S to be optimal it is sufficient that
pk ≤ pt
for all k and t such that k is unassigned under S, t is assigned under S,
and there exists a path (k, i1, j1, . . . , iq, jq, iq+1, t) such that (ir, jr) ∈ S for
r = 1, . . . , q, and (iq+1, t) ∈ S. Hint : Consider the existence of cycles with
positive value along which S can be modified.
(b) Consider the multiassignment problem (cf. Exercise 7.10). Derive a result
analogous to the one of part (a), with the condition pk ≤ pt replaced by
the condition πk ≥ πt, where k is any multiassigned person and t is any
person for which there exists a path (k, j1, i1, . . . , jq, iq, jq+1, t) such that
(k, j1) ∈ S and (ir, jr+1) ∈ S for r = 1, . . . , q.
Sec. 7.6 Notes, Sources, and Exercises 333
7.13 (Improved Optimality Condition)
Consider the minimum cost flow problem, without assuming that the problem
data are integer. Show that if x is feasible, and x and p satisfy -CS, then x is
optimal, provided
 < min
All simple cycles Y
{
− Cost of Y
Number of arcs of Y
∣∣∣ Cost of Y < 0} ,
where
Cost of Y =
∑
(i,j)∈Y +
aij −
∑
(i,j)∈Y −
aij .
7.14 (Termination Tolerance for Transportation Problems)
Consider a transportation problem with m sources and n sinks, and integer data.
Show that in order for a feasible x to be optimal it is sufficient that it satisfies
-CS together with some p and that
 <
1
2min{m, n}
[instead of  < 1/(m + n)]. Hint : Use the result of Exercise 7.13.
7.15 (Dealing with Infeasibility)
Consider the -relaxation algorithm applied to a minimum cost flow problem with
initial prices p0i .
(a) Assume that the problem is feasible. Show that the total price increase
pi − p0i of any node i prior to termination of the algorithm satisfies
pi − p0i ≤ (N − 1)(C + ) + max
j∈N
p0j − min
j∈N
p0j ,
where C = max(i,j)∈A |aij |. Hint : Let x0 be a feasible flow vector and
let (x, p) be the flow-price vector pair generated by the algorithm prior
to its termination. Show that there exist nodes t and s such that gt > 0
and gs < 0, and a simple path H starting at s and ending at t such that
xij − x0ij > 0 for all (i, j) ∈ H+ and xij − x0ij < 0 for all (i, j) ∈ H−. Now
use -CS to assert that
pj + aij ≤ pi + , ∀ (i, j) ∈ H+,
pi ≤ pj + aij + , ∀ (i, j) ∈ H−.
Add these conditions along H to obtain
pt − ps ≤ (N − 1)(C + ).
334 Auction Algorithms Chap. 7
Use the fact ps = p
0
s to conclude that
pt − p0t ≤ (N − 1)(C + ) + ps − p0s ≤ (N − 1)(C + ) + max
j∈N
p0j − min
j∈N
p0j .
(b) Discuss how the result of part (a) can be used to detect infeasibility.
(c) Suppose we introduce some artificial arcs to guarantee that the problem
is feasible. Discuss how to select the cost coefficients of the artificial arcs
so that optimal solutions are not affected in the case where the original
problem is feasible.
7.16 (Suboptimality of a Feasible Flow Satisfying -CS)
Let x∗ be an optimal flow vector for the minimum cost flow problem and let x
be a feasible flow vector satisfying -CS together with a price vector p.
(a) Show that the cost of x is within 
∑
(i,j)∈A |xij − x
∗
ij | from the optimal.
Hint : Show that (x − x∗) satisfies CS together with p for a minimum cost
flow problem with arcs (i, j) having flow range [bij − x∗ij , cij − x∗ij ] and arc
cost âij that differs from aij by no more than .
(b) Show by example that the suboptimality bound 
∑
(i,j)∈A |cij − bij | de-
duced from part (a) is tight. Hint : Consider a graph with two nodes and
multiple arcs connecting these nodes. All the arcs have cost  except for
one that has cost −.
7.17
Apply the -relaxation method to the problem of Fig. 6.4 of Section 6.2 with
 = 1. Comment on the optimality of the solution obtained.
7.18 (Degenerate Price Rises)
In this exercise, we consider a variation of the -relaxation method that involves
degenerate price rises. A degenerate price rise changes the price of a node that
currently has zero surplus to the maximum possible value that does not violate
-CS with respect to the current flow vector (compare with degenerate price rises
in the context of the single-node relaxation iteration where  = 0, as illustrated
in Fig. 6.8 of Section 6.5).
Consider a variation of the -relaxation method where there are two types
of iterations: (1) regular iterations, which are of the form described in the present
section, and (2) degenerate iterations, which consist of a single degenerate price
rise.
(a) Show that if the problem is feasible and the number of degenerate iterations
is bounded by a constant times the number of regular iterations, then the
method terminates with a pair (x, p) satisfying -CS.
(b) Show that the assumption of part (a) is essential for the validity of the
method.
Sec. 7.6 Notes, Sources, and Exercises 335
7.19 (Deriving Auction from -Relaxation)
Consider the assignment problem formulated as a minimum cost flow problem
(see Example 1.2 in Section 1.2). We say that source i is assigned to sink j if (i, j)
has positive flow. We consider a version of the -relaxation algorithm in which
-relaxation iterations are organized as follows: between iterations (and also at
initialization), only source nodes i can have positive surplus. Each iteration
finds any unassigned source i (i.e., one with positive surplus), and performs an
-relaxation iteration at i, and then takes the sink j to which i was consequently
assigned and performs an -relaxation iteration at j, even if j has zero surplus.
(If j has zero surplus, such an iteration will consist of just a degenerate price rise;
see Exercise 7.18.)
More specifically, an iteration by an unassigned source i works as follows:
(1) Source node i sets its price to pj + aij + , where j minimizes pk + aik + 
over all k for which (i, k) ∈ A. It then sets xij = 1, assigning itself to j.
(2) Node i then raises its price to pj′ + aij′ + , where j
′ minimizes pk + aik + 
for k = j, (i, k) ∈ A.
(3) If sink j had a previous assignment xi′j = 1, it breaks the assignment by
setting xi′j := 0. (One can show inductively that if this occurs, pj = pi′−ai′j +.)
(4) Sink j then raises its price pj to
pi − aij +  = pj′ + aij′ − aij + 2.
Show that the corresponding algorithm is equivalent to the Gauss-Seidel
version of the auction algorithm.
7.20 (O
(
N1/2A log(NC)
)
Hybrid Auction Algorithm)
This exercise, due to Ahuja and Orlin [1987], shows how the auction algorithm can
be combined with a more traditional primal-dual method to obtain an algorithm
with an improved running time bound. The auction algorithm is used to assign
the first N − O
(
N1/2
)
persons and the primal-dual method is used to assign
the rest. Consider the solution of the assignment problem by the Gauss-Seidel
variant of the scaled auction algorithm ( = 1 throughout).
(a) Extend the analysis of Section 7.1 to show that in any subproblem of the
scaled auction algorithm we have
∑
i∈I(π
0
i − πi) ≤ 6N , where I is the set
of unassigned persons, π0i = maxj∈A(i)
{
aij − p0j
}
, and p0 is the vector of
prices prevailing at the outset of the subproblem.
(b) Suppose that at the outset of each subproblem we use a modified Gauss-
Seidel auction procedure in which only persons i with profit margins πi
greater than or equal to π0i −(6N)1/2 are allowed to place bids. Show that
this procedure can be implemented so that at most (6N)1/2 + 1 iterations
are performed at each person node i, and that it terminates in O(N1/2A)
time. Furthermore the number of unassigned persons after termination is
at most (6N)1/2.
336 Auction Algorithms Chap. 7
(c) Assume that there exists some algorithm X which, given an incomplete as-
signment S and a price vector p obeying -CS, produces a new pair (S′, p′)
obeying -CS in O(A) time, with S′ containing one more assignment than
S (Exercise 7.21 indicates how such an algorithm may be constructed).
Outline how one would construct an O
(
N1/2A log(NC)
)
assignment algo-
rithm.
7.21
Consider the primal-dual method of Chapter 6. Show that if the terms “bal-
anced,” “active,” and “inactive” are replaced by “-balanced,” “-active,” and
“-inactive,” then the resulting method terminates in a finite number of itera-
tions and the final pairs (x, p) obtained satisfy -CS.
7.22 (Gap Method for Saturated Cut Detection)
Consider the gap method described at the end of Section 7.3.2. Suppose that in
the course of the preflow-push algorithm the number m(k) of nodes that have
price equal to k is 0. Let S be the set of nodes with price less than k, and let S
be the complementary set of nodes with price greater than k.
(a) Show that the cut [S, S] is saturated. Hint : The prices of the end nodes of
the arcs of the cut differ by at least 2, so by 1-CS, their flows must be at
the upper or lower bounds.
(b) Explain why the nodes in S can be purged from the computation by setting
their prices to N . Hint : For every minimum cut [S
′
, S′], we must have
S ⊂ S′.
8
Nonlinear Network
Optimization
Contents
8.1. Convex and Separable Problems
8.2. Problems with Side Constraints
8.3. Multicommodity Flow Problems
8.4. Integer Constraints
8.5. Networks with Gains
8.6. Optimality Conditions
8.7. Duality
8.8. Algorithms and Approximations
8.8.1. Feasible Direction Methods
8.8.2. Piecewise Linear Approximation
8.8.3. Interior Point Methods
8.8.4. Penalty and Augmented Lagrangian Methods
8.8.5. Proximal Minimization
8.8.6. Smoothing
8.8.7. Transformations
8.9. Notes, Sources, and Exercises
337
338 Nonlinear Network Optimization Chap. 8
With this chapter, we begin our discussion of nonlinear network flow prob-
lems, which generalize the minimum cost flow problem discussed so far in
two ways:
(a) The linear cost function is replaced by a general function f(x) of the
flow vector x.
(b) The capacity constraints are replaced by a general set X.
Thus the problem has the form
minimize f(x)
subject to x ∈ F,
where x is a flow vector in a given directed graph (N ,A), the feasible set
F is
F =
x ∈ X ∣∣∣ ∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N
 ,
and f is a given real-valued function that is defined on the space of flow
vectors x. Here si are given supply scalars and X is a given subset of flow
vectors.
We will focus on two main cases:
(a) The case where the feasible set F is convex and the function f is
convex over F . For this case, we will provide natural extensions of
some of the primal cost improvement, dual cost improvement, and
auction algorithms of Chapters 2-7.
(b) The case where the feasible set F is not convex and involves integer
constraints. For this case, we will derive some of the basic method-
ology for dealing with the integer constraints. We will also explain
how some of the standard approaches to combinatorial optimization
involve the solution of linear and convex network optimization prob-
lems.
In this chapter we discuss broad issues of structure and algorithmic
methodology relating to nonlinear network problems, with an emphasis on
the convex case. We defer some of the more detailed analysis to Chap-
ters 9 and 10. In Sections 8.1-8.5 we focus on problem formulation. We
delineate some important problem structures, involving separability, side
constraints, multiple commodities, integer constraints, and arc gains, and
we discuss their interplay with the analytical and algorithmic methodol-
ogy. Our discussion in these sections covers a very broad spectrum of
problems, including some discrete models (a more detailed discussion of
discrete models will be given in Chapter 10). In Section 8.6, we discuss
optimality conditions based on differentiability of the cost function f . In
Sec. 8.1 Convex and Separable Problems 339
Section 8.7, we develop some preliminary notions of duality (a deeper treat-
ment of duality for separable problems is provided in Chapter 9). Finally,
in Section 8.8, we describe some general techniques of nonlinear program-
ming and we identify the network optimization contexts in which they are
most applicable.
On Mathematical Background
In the remainder of the book, we will assume that the reader has some prior
exposure to the basic notions of analysis and convexity in the n-dimensional
Euclidean space n. We will be reviewing definitions and needed results as
they arise (a summary is provided in Appendix A). We implicitly assume
that all vectors are column vectors. A prime denotes transposition, so that
if x and y are vectors, x′ is a row vector, and x′y denotes the inner product
of x and y. The standard Euclidean norm of a vector x is denoted by ‖x‖,
‖x‖ =
√
x′x.
The reader may find the needed mathematical background in many
standard texts. Some recommended sources are Hoffman and Kunze [1971],
and Strang [1976] (linear algebra), Luenberger [1969], Ortega and Rhein-
boldt [1970], and Rudin [1976] (analysis), Hiriart-Urruty and Lemarechal
[1993], and Rockafellar [1970] (convex analysis). The author’s nonlinear
programming text [1995b] contains two extensive optimization-oriented ap-
pendixes on analysis, linear algebra, and convexity, and uses the same no-
tation as the one used here.
8.1 CONVEX AND SEPARABLE PROBLEMS
In this section, we consider convex network optimization problems and
some of their special cases. We recall that a subset F of n is called
convex if it contains the line segment connecting any two of its points; that
is, αx + (1−α)y belongs to F for all x, y ∈ F and α ∈ [0, 1]. A real-valued
function f , defined on a subset of n that contains a convex set F , is said to
be convex over F if linear interpolation of the function based on its values
at any two points of F provides an overestimate of the true function value;
that is,
f
(
αx + (1 − α)y
)
≤ αf(x) + (1 − α)f(y), ∀ x, y ∈ F, α ∈ [0, 1].
The most general convex network optimization problem has the form
minimize f(x)
subject to x ∈ F,
340 Nonlinear Network Optimization Chap. 8
where:
x is a flow vector in a given graph, and
the feasible set F is
F =
x ∈ X ∣∣∣ ∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N
 , (8.1)
where X is a given convex set and si are given supply scalars.
The cost function f is defined on the space of flow vectors and is assumed
convex over F .
Important special cases of convex network problems involve constraints
and/or a cost function with a structure that is separable with respect to
arcs. In particular, we say that the problem is constraint-separable if the
set X appearing in the feasible set F of Eq. (8.1) has the form
X =
{
x | xij ∈ Xij , (i, j) ∈ A
}
,
where each set Xij is an interval of the real line (for example, Xij is specified
by arc flow bounds, Xij = [bij , cij ]).†
When, the problem is constraint-separable and in addition, the cost
function f has the form
f(x) =
∑
(i,j)∈A
fij(xij),
where each function fij is convex over the corresponding interval Xij , we
say that the problem is separable. Note that the minimum cost flow prob-
lem is obtained as the special case of a separable problem where f is a
linear function and each interval Xij specifies upper and lower bounds on
the corresponding arc flow xij ,
Xij = [bij , cij ].
Another interesting special case of the convex network optimization
problem is the convex network flow problem with side constraints, to be
discussed in Section 8.2. This is the special case where the set X appearing
in the feasible set of Eq. (8.1) has the form
X =
{
x | xij ∈ Xij , (i, j) ∈ A, gt(x) ≤ 0, t = 1, . . . , r
}
,
where Xij are intervals of the real line and each gt is a convex function of
x. The constraints gt(x) ≤ 0 are called side constraints.
† An interval in our terminology is a nonempty and convex subset of the real
line. It can be closed, or open, or neither closed nor open.
Sec. 8.1 Convex and Separable Problems 341
For purposes of easy reference, we list the definitions of the preceding
network optimization problems. Generally, unless otherwise specified, when
we refer to these problems we implicitly assume that they are convex.
Network Optimization Problem
minimize f(x)
subject to x ∈ X,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N .
Constraint-Separable Network Problem
minimize f(x)
subject to xij ∈ Xij , ∀ (i, j) ∈ A,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N .
Separable Network Problem
minimize
∑
(i,j)∈A
fij(xij)
subject to xij ∈ Xij , ∀ (i, j) ∈ A,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N .
Network Problem with Side Constraints
minimize f(x)
subject to xij ∈ Xij , ∀ (i, j) ∈ A,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
gt(x) ≤ 0, t = 1, . . . , r.
There are some additional convex optimization problems that will
receive attention in the remainder of the book. One such problem is the
convex multicommodity flow problem, which will be described in Section
8.3. This problem has in turn separable versions, where the cost function
and/or the constraints are separable with respect to arcs, and there may
or may not be some additional arc capacity constraints. In the case where
342 Nonlinear Network Optimization Chap. 8
there are capacity constraints, the problem can also be viewed as a special
case of the convex network problem with side constraints, as we will see in
Section 8.3. Another interesting problem is the monotropic programming
problem, which generalizes the convex separable network problem described
above, and is the most general type of convex program that exhibits the
favorable combinatorial structure of linear programming; this problem will
be described and analyzed in Section 9.7. Figure 8.1 lists the principal
types of convex problems that we will discuss in this book and shows their
interrelations.
Convex Network
Problem
Convex Constraint-
Separable Problem
Monotropic
Programming 
Problem
Convex Network
Problem with Side
Constraints
Convex
Multicommodity
Flow Problem
Convex Separable
Network Problem
Convex Separable
Multicommodity
Flow Problem
Convex Separable
Multicomm. Flow
w/  Arc Capacities
Figure 8.1: Some of the types of convex problems to be discussed in the remain-
der of the book and their interrelations.
Our development will suggest that separability is the most impor-
tant structural characteristic of convex network problems. Generally, as
the problem’s structure deviates from the separable structure, its solution
becomes more difficult. There are several reasons for this:
(a) A separable structure allows a sharper duality theory, as we will see
in Section 8.7 and in Chapter 9.
(b) Some algorithms, such as certain types of relaxation methods, are
most effective in the presence of a separable structure (see Chapter
9). Furthermore, other algorithms, such as auction and -relaxation,
do not apply in the absence of a separable structure.
(c) Separable problems belong to the class of monotropic programming
problems, which possess some special properties. As we will see in
Chapter 9, for this class of problems, there exists a special finite set
Sec. 8.1 Convex and Separable Problems 343
of directions, called elementary , among which a descent direction can
be found at any nonoptimal vector. In the case of a convex separa-
ble network problem, these directions only depend on the problem’s
graph. For a primal minimum cost flow problem, these directions
are associated with simple cycles (compare with Prop. 1.2), and for
a dual minimum cost flow problem, these directions involve certain
node subsets (compare with the discussion in Section 1.3). These nice
properties do not generalize to nonseparable network problems, even
in the presence of convexity.
Let us now describe some practical network models with a separable
structure.
Example 8.1. Reservoir Control – Production Scheduling
Suppose that we want to construct an optimal schedule of water release from
a reservoir over N time periods. Denote by:
xk: The volume of water held by the reservoir at the start of the kth period
(x0 is assumed known, and xk is constrained to lie within some given
interval [x, x]).
uk: The volume of water released by the reservoir during the kth period and
used for some productive purpose (uk is constrained to lie in a given
interval [0, ck]).
Thus, the volume xk evolves according to
xk+1 = xk − uk, ∀ k = 0, . . . , N − 1.
There is a cost G(xN ) for the terminal volume being xN and there is a cost
gk(uk) for outflow uk at period k. For example, when uk is used for electric
power generation, gk(uk) may be equal to minus the value of power produced
from uk. We want to choose the outflows u0, . . . , uN−1 to minimize
G(xN ) +
N−1∑
k=0
gk(uk),
while observing the constraints on the volume xk and on the outflow uk. It
is natural to assume here that G and gk are monotonically decreasing convex
functions (increasing outflow has diminishing incremental returns).
We can formulate the problem as a convex separable network optimiza-
tion problem. We represent each period k = 0, . . . , N − 1 by a node k with
an outgoing arc (k, k + 1), whose flow is xk (see Fig. 8.2). We introduce an
artificial node A, which “accumulates” the outflow variables uk. There is an
arc from each node k to node A carrying flow uk, there is an arc (N − 1, A)
carrying flow xN , and an arc (A, 0) carrying flow x0. All of the arcs have
capacity constraints [for the arc (A, 0) the lower and upper bounds coincide
with the given initial volume x0], but only the arcs carrying the flows uk and
xN have the nonzero cost function gk(uk) and G(xN ), respectively. Finally,
344 Nonlinear Network Optimization Chap. 8
. . .
x0
x1 x2 xN-2
10 N-2
xN-1
xN
N-1
u0 u1 uN-2 uN-1
A
Figure 8.2: Formulation of the reservoir control problem as a convex sepa-
rable cost network problem. This is a circulation problem; that is, the supply
si of each node i in the conservation of flow equation is 0.
the flow vector must be a circulation; that is, the given supply si of each node
i is 0.
There are several variants of the problem, which can also serve as models
of other production planning contexts. We list some of the possibilities:
(a) There may be a known inflow vk to the reservoir from the environment
during period k, resulting in an equation of the form
xk+1 = xk − uk + vk, ∀ k = 0, . . . , N − 1.
This can be modeled with a known nonzero supply sk = vk at the nodes
k = 0, . . . , N−1, and with a corresponding demand at the accumulation
node A, which is sA = −
∑N−1
k=0
vk.
(b) There may be multiple reservoirs some of which are feeding into others
with a delay of one or more time periods. For example, we may have
two reservoirs in series, the first of which satisfies the equation
x1k+1 = x
1
k − u1k − y12k , ∀ k = 0, . . . , N − 1,
while the second satisfies
x2k+1 = x
2
k − u2k + y12k , ∀ k = 0, . . . , N − 1. (8.2)
Here, x1k, x
2
k and u
1
k, u
2
k are the volumes and outflows of the two reser-
voirs at period k, respectively, and y12k is the water released from reser-
voir 1 to reservoir 2 during period k. [If there is a delay of d time periods
for water to arrive from reservoir 1 to reservoir 2, we should replace y12k
in Eq. (8.2) with y12k−d.] This problem and others like it, involving mul-
tiple reservoirs, can be similarly modeled as convex network problems.
We need to introduce a node km for each period k and reservoir m, as
well as corresponding arcs to an accumulation node and to the nodes
of other reservoirs that carry the corresponding outflows. For example
in the two-reservoir case, there should be an arc from node k1 to node
(k + 1)2 carrying flow y
12
k .
Sec. 8.1 Convex and Separable Problems 345
(c) There may be water losses that are proportional to the current volume,
so that the relevant equation is
xk+1 = βkxk − uk, ∀ k = 0, . . . , N − 1,
where βk are given scalars with 0 ≤ βk < 1. This type of model,
together with its multireservoir version, is often encountered in general
production planning systems. The resulting problem cannot be modeled
as a convex separable network problem, but still involves an important
structure, called network with gains, which will be discussed in Section
8.5.
The multireservoir problem of the preceding example is typical of dy-
namic network flow problems, which involve material flow between nodes of
a network, but also a time dimension, whereby flows at a given time period
affect the network’s condition at future time periods. The mathematical
formulation of the problem involves a time-expanded network , which in-
cludes a copy of the given network for each time period, and arcs that lead
from given time periods to subsequent time periods (see Exercise 8.3).
Example 8.2. Least Squares Network Problems
Suppose that we are given a minimum cost flow problem including supplies si
that do not necessarily add to 0 or that cannot be accommodated by the arc
capacities. An interesting problem is then to obtain a capacity-feasible flow
vector x whose divergences yi are as close as possible to the given supplies si
in a least squares sense. This is the problem
minimize
∑
i∈N
wi(yi − si)2
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = yi, ∀ i ∈ N ,
bij ≤ xij ≤ cij , ∀ (i, j) ∈ A,
where wi are given positive weights, bij , cij , and si are given scalars, and the
optimization variables are the flows xij and the divergences yi.
We can formulate this problem as a convex separable network optimiza-
tion problem by introducing an artificial node A, which “accumulates” the
divergences yi (see Fig. 8.3). There is an arc from each node i to node A,
the flow of which is yi and the cost of which is wi(yi − si)2. In a variation of
this problem, the “target supplies” si may be replaced by “target intervals”
[si, si], in which case the cost of each arc (i, A) is taken to be
wi
(
max{0, yi − si}
)2
+ wi
(
max{0, si − yi}
)2
.
Still another possibility is to use a nonquadratic cost function for each error
yi − si.
346 Nonlinear Network Optimization Chap. 8
yi
i
A
Figure 8.3: Formulation of the least
squares network flow problem as a con-
vex separable cost network problem.
An artificial node A is introduced to-
gether with an arc (i, A) for each node
i. The cost of the arc (i, A) is the
square of the error between the diver-
gence yi of node i and the given target
supply si of i.
In the preceding example, the divergences yi are subject to optimiza-
tion. In a different least squares setting, each yi is required to be equal to
a given supply si, and the cost function consists of the sum of squares∑
(i,j)∈A
wij(xij − mij)2,
where mij are the components of a given matrix and wij are given posi-
tive weights. The matrix balancing problem discussed in Example 1.5 of
Chapter 1, is a special case of this model.
8.2 PROBLEMS WITH SIDE CONSTRAINTS
Many convex network flow problems (in addition to the conservation of
flow constraints and interval constraints on the arc flows) have additional
constraints of the form
gt(x) ≤ 0, t = 1, . . . , r,
which are called side constraints. The problem has the form
minimize f(x)
subject to xij ∈ Xij , ∀ (i, j) ∈ A,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
gt(x) ≤ 0, t = 1, . . . , r,
where Xij are intervals of the real line, and f and gt, t = 1, . . . , r, are
convex functions of x. Here is an example:
Sec. 8.2 Problems with Side Constraints 347
Example 8.3. Inventory Control
Consider an inventory system that involves a single product type and operates
over N time periods. Let us denote:
xk: The amount of stock held by the system at the start of the kth period
(x0 is assumed known, and xk may also take negative values, which
represent back orders).
uk: The amount of stock purchased (or produced) and immediately deliv-
ered at period k, at a cost of ckuk.
vk: The amount of stock demanded at period k. This is given for all k.
Thus, the stock xk evolves according to
xk+1 = xk + uk − vk, ∀ k = 0, . . . , N − 1.
There is a cost hk(xk) for having stock xk at period k. Generally, this involves
a penalty for stock surplus (xk > 0), as well as a penalty for stock shortage
(xk < 0). There is also a cost H(xN ) for the terminal stock being xN , and
possibly a constraint that xN should lie in a given interval. It is fairly natural
to assume here that H and hk are convex functions. We want to choose the
purchases u0, . . . , uN−1 to minimize
H(xN ) +
N−1∑
k=0
(
hk(xk) + ckuk
)
,
while observing the constraints on the volume xk and on the outflow uk.
We can formulate this as a convex separable network optimization prob-
lem, similar to the reservoir control example of Example 8.1. We represent
each period k = 0, . . . , N − 1 by a node k with an outgoing arc (k, k + 1),
whose flow is xk. We introduce an artificial node A, which represents the
“environment.” There is an arc from node A to each node k, carrying flow
uk, there is an arc (N − 1, A) carrying flow xN , and an arc (A, 0) carrying
flow x0. There is also an arc from each node k to node A, carrying flow equal
to the known demand vk (see Fig. 8.4).
. . .
x0
x1 x2 xN-2
10 N-2
xN-1
xN
N-1
u0 u1 vN-2 vN-1
A
v0 v1 uN-2 uN-1
Figure 8.4: Formulation of the inventory control problem as a convex sep-
arable network problem. Once the “accumulation” node A is introduced, we
obtain a circulation problem; that is, the supply si of each node i in the
conservation of flow equation is 0.
348 Nonlinear Network Optimization Chap. 8
If there were no other constraints, the problem would be separable.
However, there may be several types of side constraints that couple the arc
flows. An example is when there is a budget constraint whereby the total
cost for inventory purchase may not exceed a given amount B,
N−1∑
k=0
ckuk ≤ B.
Another example is when there is a space constraint for the system, whereby
the total inventory at the start of any period must not exceed a given constant
S,
xk + uk ≤ S, k = 0, . . . , N − 1.
The preceding side constraints maintain convexity of the problem. How-
ever, in other variants of the inventory problem there may be additional inte-
ger constraints and couplings between the arc flows that destroy the convex
character of the problem. For example, uk may be subject to a positive fixed
charge or startup cost that must be paid when uk is positive, in addition to
the purchase cost ckuk, i.e., the total purchase cost has the form{
C + ckuk if uk > 0,
0 if uk = 0,
where C > 0 is the fixed charge. We will discuss cost structures of this type
in Chapter 10.
Side constraints typically complicate the problem’s solution because
they represent a departure from a pure network structure. In fact, one
should always consider the possibility of eliminating side constraints by
dualization (see Section 8.7) or by some kind of approximation (see Section
8.8), in order to recover a more pure network structure.
Let us finally note that being able to formulate a given practical prob-
lem as a network problem with side constraints is not significant in itself.
The reason is that any convex programming problem can be formulated as
a convex network problem with side constraints, as can be seen from the
construction of Fig. 8.5. Furthermore, it can be seen that any linear pro-
gram can be reformulated as a linear network flow problem with linear
side constraints. Thus the class of convex network problems with side con-
straints is very broad and unstructured. A similar statement can be made
about network problems with side constraints and additional integer con-
straints. This suggests that a problem formulation as a network model
with side constraints may be worth considering only if the side constraints
“do not dominate” the problem. This notion is somewhat vague, but it
roughly means that eliminating the side constraints leaves an “interesting”
network structure intact, and does not change “radically” the character of
the optimal solution. An example of a problem that is profitably viewed as
a network model with side constraints is the multicommodity flow problem
with arc capacity constraints, which will be discussed in the next section.
Sec. 8.3 Multicommodity Flow Problems 349
. . .
x1
x2 x3 xn-2
21 n-2
xn-1
xn
n-1
u1 u2 un-2 un-1
n
Figure 8.5: A convex network reformulation of a general convex optimization
problem of the form
minimize f(x)
subject to xk ∈ Xk, k = 1, . . . , n, gt(x) ≤ 0, t = 1, . . . , r,
where x1, . . . , xn are the scalar components of the vector x. We assume that
Xk is an interval of the real line for each k, and f and gt are convex over 	n.
We introduce additional artificial variables u1, . . . , un−1, and we construct the
network depicted in the figure (the nodes are 1, . . . , n, the arcs are as shown, and
the flows are shown next to the arcs). The cost function is f(x), and in addition
to the conservation of flow constraints
uk = xk − xk+1, k = 1, . . . , n − 1,
we have the arc flow constraints xk ∈ Xk, and the side constraints
gt(x) ≤ 0, t = 1, . . . , r.
8.3 MULTICOMMODITY FLOW PROBLEMS
A multicommodity flow problem involves a collection of several networks
whose flows must independently satisfy conservation of flow constraints,
but are coupled through some other constraints or the cost function. As
an example, consider a communication network that carries two different
types of traffic, say telephone traffic from node A to node B, and video
traffic from node C to node D. The telephone traffic and the video traffic
must each satisfy its own conservation of flow constraints, but there may
be coupling due to a communication capacity constraint of the network
arcs, requiring that the sum of the two traffic flows on each arc be less than
the capacity threshold of the arc. We formulate a general multicommodity
flow problem as follows.
We have a directed graph (N ,A), and we consider a finite collection
of flow vectors x(m), m = 1, . . . , M , on the graph, where M is a given
integer. We call x(m) the flow vector of commodity m, and we denote the
350 Nonlinear Network Optimization Chap. 8
collection of all commodity flow vectors by
x =
(
x(1), . . . , x(M)
)
.
Each flow vector x(m) must satisfy its own conservation of flow constraints∑
{j|(i,j)∈A}
xij(m) −
∑
{j|(j,i)∈A}
xji(m) = si(m), ∀ i ∈ N , m = 1, . . . , M,
(8.3)
where si(m) are given supply scalars. Furthermore, the commodity flows
must together satisfy
x =
(
x(1), . . . , x(M)
)
∈ X, (8.4)
where X is a constraint set, which may encode special restrictions for the
various commodities. For example, to force a commodity m to avoid some
arc (i, j), we may introduce the constraint xij(m) = 0. In this way, we can
model situations where each commodity is restricted to use only a subgraph
of the given graph.
The feasible set is
F =
{
x ∈ X | x satisfies the conservation of flow constraints (8.3)
}
,
and the cost function is of the form
f(x) = f
(
x(1), . . . , x(M)
)
. (8.5)
The general convex multicommodity flow problem is
minimize f(x)
subject to x ∈ F
where we assume that F is convex and f is convex over F .
Note that x may be viewed as a flow vector in an expanded graph
consisting of M (disconnected) copies of the original graph (N ,A). With
this interpretation, it is seen that the only coupling between the commodi-
ties comes from the cost function (8.5) and from the constraint x ∈ X, cf.
Eq. (8.4).
The version of the multicommodity problem that is most amenable to
analysis and algorithmic solution is the convex separable multicommodity
flow problem. In this problem the set X has the form
X =
{
x | xij(m) ∈ Xij(m), ∀ (i, j) ∈ A, m = 1, . . . , M
}
, (8.6)
where Xij(m) are intervals of the real line, and the cost function has the
form
f(x) =
∑
(i,j)∈A
fij(yij), (8.7)
Sec. 8.3 Multicommodity Flow Problems 351
where yij is the total flow of arc (i, j)
yij =
M∑
m=1
xij(m),
and each fij :  →  is a convex function of yij . Note here that the
cost function is not separable with respect to the commodity flows xij(m),
only with respect to the total flows yij . There is also a constraint-separable
version of the multicommodity flow problem, where the constraint set X
has the form (8.6) but the cost function f does not have the separable form
(8.7).
In the separable multicommodity flow problem, commodities are cou-
pled only through the total arc flows yij that appear in the separable cost
function. Another type of commodity coupling in multicommodity prob-
lems arises when the set X includes additional upper bounds on the total
flows of the arcs:
X =
{
x | xij(m) ∈ Xij(m), yij ≤ cij , ∀ (i, j) ∈ A, m = 1, . . . , M
}
, (8.8)
where Xij(m) are given intervals of the real line, and cij are given scalars
representing arc “capacities.” The convex separable version of the resulting
problem is referred to as a convex separable multicommodity flow problem
with arc capacities. This problem may also be viewed as a special case of the
convex network problem with side constraints, where the side constraints
are the capacity constraints yij ≤ cij . For easy reference, we list the
definitions of the various types of multicommodity network problems in
the table of the following page:
Multicommodity flow problems arise in several practical contexts.
Here are some examples:
Example 8.4. Optimal Routing in a Data Network
We are given a directed graph (N ,A), which is viewed as a model of a
data communication network. We are also given a set of ordered node pairs
(im, jm), m = 1, . . . , M , referred to as origin-destination (OD) pairs. The
nodes im and jm are referred to as the origin and the destination of the OD
pair. For each OD pair (im, jm), we are given a scalar rm referred to as its
input rate.
In the context of routing of data in a communication network, rm (mea-
sured in bits per unit time) is the arrival rate of traffic entering the network
at node im and exiting at node jm. (The traffic here is usually modeled by
a stationary stochastic process, in which case rm represents a stochastic av-
erage of the number of bit arrivals per unit time.) In a somewhat different
context, rm may represent the number of ongoing (phone or data) connections
between im and jm [within this context, the arc flows xij(m) are integer, but
they can be reasonably approximated with real numbers when a large number
352 Nonlinear Network Optimization Chap. 8
Multicommodity Flow Problem
minimize f(x)
subject to x ∈ X,∑
{j|(i,j)∈A}
xij(m)−
∑
{j|(j,i)∈A}
xji(m) = si(m), ∀ i ∈ N , m = 1, . . . , M.
Constraint-Separable Multicommodity Flow Problem
minimize f(x)
subject to xij(m) ∈ Xij(m), ∀ (i, j) ∈ A, m = 1, . . . , M,∑
{j|(i,j)∈A}
xij(m)−
∑
{j|(j,i)∈A}
xji(m) = si(m), ∀ i ∈ N , m = 1, . . . , M.
Separable Multicommodity Flow Problem
minimize
∑
(i,j)∈A
fij(yij)
subject to xij(m) ∈ Xij(m), ∀ (i, j) ∈ A, m = 1, . . . , M,∑
{j|(i,j)∈A}
xij(m)−
∑
{j|(j,i)∈A}
xji(m) = si(m), ∀ i ∈ N , m = 1, . . . , M,
yij =
M∑
m=1
xij(m), ∀ (i, j) ∈ A.
Separable Multicommodity Flow Problem with Arc Capaci-
ties
minimize
∑
(i,j)∈A
fij(yij)
subject to xij(m) ∈ Xij(m), ∀ (i, j) ∈ A, m = 1, . . . , M,∑
{j|(i,j)∈A}
xij(m)−
∑
{j|(j,i)∈A}
xji(m) = si(m), ∀ i ∈ N , m = 1, . . . , M,
yij =
M∑
m=1
xij(m), ∀ (i, j) ∈ A,
yij ≤ cij , ∀ (i, j) ∈ A.
Sec. 8.3 Multicommodity Flow Problems 353
of connections is involved]. The routing objective is to divide each rm among
the many paths from origin to destination in a way that the resulting total
arc flow pattern minimizes a suitable cost function.
We view each OD pair (im, jm) as a commodity, and we denote by x(m)
the corresponding flow vector. This vector must satisfy the conservation of
flow equation for all i ∈ N and m = 1, . . . , M ,
∑
{j|(i,j)∈A}
xij(m) −
∑
{j|(j,i)∈A}
xji(m) =
{
rm if i = im,
−rm if i = jm,
0 otherwise.
Typically, there are also constraints of the form
0 ≤ xij(m), yij ≤ cij , ∀ (i, j) ∈ A, m = 1, . . . , M,
where
yij =
M∑
m=1
xij(m)
is the total flow of arc (i, j), and cij is its communication capacity. Frequently,
the cost function has the separable form∑
(i,j)∈A
fij(yij),
where fij is a convex function that provides a measure of communication
“delay” on arc (i, j). This delay depends on the flow of the arc and is usually
based on some queueing model of the traffic flow on the arc (see e.g., the data
network textbook by Bertsekas and Gallager [1992]). With the separable
constraints and cost function above, the problem becomes a special case of
the separable multicommodity flow problem with arc capacities.
There are some variations of the routing problem, which also arise in
other practical applications of multicommodity flow models. For example:
(a) The capacity constraints yij ≤ cij are not present, but instead they may
appear implicitly in the cost functions fij . For example, the constraint
yij ≤ cij may be modeled with a function fij that rises steeply near cij .
This is convenient because we then obtain a separable multicommodity
flow problem, which turns out to be more amenable to algorithmic
solution than the version involving arc capacities (see Section 8.8).
(b) The commodity input rates rm may be subject to optimization within
some given interval [0, rm]. In this case the cost function has the form
∑
(i,j)∈A
fij(yij) +
m∑
m=1
gm(rm),
where gm is a convex monotonically decreasing function within the given
input range [0, rm]. This cost function captures the tradeoff between a
354 Nonlinear Network Optimization Chap. 8
Origin of 
OD pair (im ,jm )
rm
Destination of 
OD pair (im ,jm )
rm
im jm
Overflow arc
Flow = rm  -  rm
_
__
Figure 8.6: Converting a multicommodity problem with commodity input
rates rm that are subject to optimization, to a problem with fixed commodity
input rates rm. For each commodity m, we introduce an overflow arc (im, jm)
that carries flow ximjm = rm − rm.
cost for too much flow on the arcs, and a cost for too much throttling of
input to the network. Note that the functions gm may reflect different
priorities for the different commodities. To convert this problem to
a standard multicommodity problem, we introduce an “overflow” arc
(im, jm) for each commodity m, which carries flow ximjm = rm − rm
and has arc cost function f̃ imjm(ximjm) = gm(rm − ximjm), and we
use rm as the fixed input of the OD pair (im, jm) (see Fig. 8.6).
(c) The input rate of each commodity may be indivisible, that is, each com-
modity may be required to follow the same path through the network,
rather than be divided among multiple paths. This is a major restric-
tion that has a radical impact in the solution methodology. It changes
the constraint set from convex to discrete, since one has to work with
the integer-constrained variables
zmij =
{
1 if commodity m is routed through arc (i, j),
0 otherwise.
In the case where there is only one commodity, this is not a real com-
plication: it can be seen that we can neglect the integer constraints
and transform the problem to a single origin-single destination shortest
path problem, which has an integer solution (see Chapter 5). However,
it turns out that with two or more commodities, the corresponding
problem may have a fractional solution, so the integer constraints cause
genuine complications.
Example 8.5. Traffic Assignment
We are given a directed graph, which is viewed as a model of a transportation
network. The arcs of the graph represent transportation links such as high-
ways, rail lines, etc. The nodes of the graph represent junction points where
Sec. 8.4 Integer Constraints 355
traffic can exit from one transportation link and enter another. Similar to the
preceding example, we are given a set of OD pairs (im, jm), m = 1, . . . , M .
For OD pair (im, jm), there is a known input rm representing rate of traffic
entering the network at the origin node im and exiting at the destination node
jm. The input rm is to be divided among the paths that start at im and end
at jm.
For each arc (i, j), we are given a cost function fij(yij) of the total flow
yij carried by the arc, and we want to minimize the separable cost∑
(i,j)∈A
fij(yij), (8.9)
subject to the conservation of flow constraints and the constraints xij(m) ≥ 0.
Thus the mathematical formulation of this example is similar to the one of the
preceding routing example. The only difference is that in the routing example
there is often the constraint yij ≤ cij for some or all the arcs (i, j), while for
the traffic assignment problem, some arcs may not have such a constraint.
However, even this difference is somewhat artificial, since one can effectively
model a constraint of the form yij ≤ cij by using a cost function that rises
steeply as yij approaches cij .
We note that in some contexts, the separable cost function (8.9) is not
quite appropriate because the traffic flow on a given arc may interact with
the traffic flow on other arcs that share the same start or end node (this is
familiar from everyday experience: a traffic jam in one road of an intersection
often slows down the traffic on the other roads of the intersection). In such
cases, it may be more appropriate for the cost functions fij to depend on the
total flows of several arcs.
We finally mention that the modeling assumption that routes are op-
timally chosen by some central authority is unnatural in situations where
travelers can choose independently their routes through the network. How-
ever, we will see later in this chapter (see Example 8.11) that problems of the
latter type can be reduced to optimization problems of the type described in
the present example.
8.4 INTEGER CONSTRAINTS
We have already discussed in Chapters 1-7 several combinatorial problems
within the framework of the minimum cost flow problem, such as shortest
path and assignment. These problems require that the arc flows be 0
or 1, but we have neglected these 0-1 constraints because even if we relax
them and replace them with capacity intervals [0, 1], we can obtain optimal
flows that are 0 or 1 with the minimum cost flow algorithms that we have
developed so far (e.g., the simplex methods of Chapter 5).
On the other hand, once we deviate from the minimum cost flow
structure and we impose side constraints or use a nonlinear cost function,
356 Nonlinear Network Optimization Chap. 8
the integer character of optimal solutions is lost, and all additional integer
constraints must be explicitly imposed. This often complicates dramat-
ically the solution process. In particular, there is no known polynomial
algorithm for solving an integer-constrained network problem that has side
constraints.
The theory of computational complexity quantifies the difficulty of
solving various classes of problems, and provides a useful guide for for-
mulating combinatorial problems as network flow problems. We mention
in particular the important class of NP-complete problems, for which no
polynomial algorithm is known at present (and none exists according to a
broadly held conjecture, commonly referred to as P 	= NP ). An exam-
ple of an NP-complete problem is the general linear network optimization
problem with linear side constraints and 0-1 integer constraints on the arc
flows. We refer to the books by Garey and Johnson [1979], and Papadim-
itriou and Steiglitz [1982] for detailed discussions of NP-completeness, and
to the book by Bertsimas and Tsitsiklis [1997] for a lighter and more acces-
sible introduction. An important point for our purposes is that, assuming
P 	= NP and given a problem that is NP-complete (or more generally, has
nonpolynomial complexity), we should give up hope of formulating it as a
minimum cost flow problem, which (as we know from Chapter 7) is solvable
with polynomial algorithms. Furthermore, given a candidate algorithm for
an NP-complete problem, we should give up hope of showing that it can
solve the problem exactly if the algorithm is polynomial.
Given the inherent difficulty of solving integer-constrained problems
with side constraints, one may prefer to settle for an approximate solution,
obtained through some heuristic. Two of the simplest and most often used
approaches are the following:
(a) Discard the integer constraints, solve the resulting problem as a “con-
tinuous” network flow problem (possibly having convex cost or side
constraints), and use some ad hoc method to round the solution to
integer.
(b) Discard the complicating side constraints, obtain an integer solution
of the resulting network problem, and use some heuristic to correct
this solution for feasibility of the violated side constraints. A variant
of this approach is to compensate for the discarded side constraints
by adding to the cost function a penalty for their violation. This
tends to produce an integer solution that is closer to feasibility.
For an example of the first approach, based on rounding a frac-
tional solution, consider a transportation problem with supply constraints∑
j xij = αi and demand constraints
∑
i xij ≤ βj . Suppose that there is an
additional indivisibility constraint , which requires that the supply of each
supply node cannot be divided between multiple demand nodes. Then a
simple heuristic is to discard the latter constraint, solve the resulting prob-
Sec. 8.4 Integer Constraints 357
lem using one of the algorithms of Chapters 5-7, and then round or shift
on an ad hoc basis whatever divided node supplies are obtained to satisfy
the indivisibility constraint. While this is a fairly crude heuristic, it may
work well in the context of other more sophisticated procedures, such as
the branch-and-bound and the rollout methods to be discussed in Chapter
10.
Let us also provide an example of the second approach, which is based
on discarding the side constraints.
Example 8.6. Constrained Shortest Path Problem
Consider a shortest path problem where we want to find a simple path P from
the origin node s to the destination node t that minimizes the path length∑
(i,j)∈P
aij . (8.10)
In some contexts, there may be additional requirements on P of the generic
form ∑
(i,j)∈P
ckij ≤ dk, k = 1, . . . , K. (8.11)
For example, there may be a timing constraint , whereby the total time to
traverse P should not exceed a given threshold T , i.e.,∑
(i,j)∈P
τij ≤ T,
where τij is the time required to traverse arc (i, j). Similarly, there could
be a safety constraint , whereby the probability of being able to traverse the
path P safely should be no less than a given threshold. Here, we assume that
traversal of an arc (i, j) will be safe with a given probability pij . Assuming
probabilistic independence of the safety of arc traversals, the probability that
traversal of a path P will be safe is the product Π(i,j)∈P pij . The requirement
that this probability is no less than a given threshold β translates to the
constraint ∑
(i,j)∈P
ln(pij) ≥ ln(β).
We can formulate the shortest path problem with path length given by
Eq. (8.10) and with the constraints (8.11) as the following network problem
with side constraints and integer constraints:
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji =
{
1 if i = s,
−1 if i = t,
0 otherwise,
xij = 0 or 1, ∀ (i, j) ∈ A,∑
(i,j)∈A
ckijxij ≤ dk, ∀ k = 1, . . . , K.
(8.12)
358 Nonlinear Network Optimization Chap. 8
11
Flow x1 = 0 or 1
Length = 1
Flow x2 = 0 or 1
Length = 2
Side Constraint 
2x1 < 1_
ts
Figure 8.7: An example of a two-arc, single-constraint shortest path problem
whose “relaxed” network optimization formulation (no integer constraints)
has a fractional solution. There are two nodes, s and t, and two arcs/paths
connecting s to t, denoted 1 and 2, with lengths 1 and 2, respectively. There
is also the side constraint 2x1 ≤ 1. Thus the only feasible solution is arc/path
2 and the shortest distance is the length 2 of the arc. Denoting by x1 and x2
the flows of arcs 1 and 2, respectively, the corresponding network optimization
problem (8.12) is
minimize x1 + 2x2
subject to x1 + x2 = 1,
x1 = 0 or 1, x2 = 0 or 1,
2x1 ≤ 1.
This problem yields the correct constrained shortest path solution, x1 = 0
and x2 = 1. If the integer constraints are relaxed and replaced by
0 ≤ x1 ≤ 1, 0 ≤ x2 ≤ 1,
the corresponding optimal solution is x1 = 0.5 and x2 = 0.5, and gives no
information about the shortest path.
A path P from s to t is optimal if and only if the flow vector x defined by
xij =
{
1 if (i, j) belongs to P ,
0 otherwise,
is an optimal solution of the problem (8.12).
Note the 0-1 integer constraint on the arc flows xij . Without this
constraint, the network optimization problem (8.12) may have a fractional
solution from which recovery of a constrained shortest path may not be easy.
This is illustrated in Fig. 8.7.
Let us now consider a few algorithms for solving the constrained shortest
path problem (8.12).
(a) The first possibility is to discard the 0-1 arc flow constraints, replacing
them with the flow bounds 0 ≤ xij ≤ 1. The resulting problem is not a
minimum cost flow problem because of the side constraints, but it can
Sec. 8.4 Integer Constraints 359
be solved as a linear program, using for example the simplex method
for general linear programming. The (fractional) solution thus obtained
can be decomposed using the conformal decomposition theorem (Prop.
1.1) into a finite collection of simple path flows that start at s and end
at t (plus possibly some cycle flows). If P is the subset of the corre-
sponding paths that are feasible with respect to the side constraints,
one may select as an approximate solution the path in P that has min-
imum length. This approach will certainly work well for the example
of Fig. 8.7, and is also likely to work well in problems with a single side
constraint, because in such a problem, at least one path in P will satisfy
the side constraint (why?). However, for problems involving multiple
side constraints, this approach is not guaranteed to produce a feasible
solution, even when the problem is feasible, in which case it needs to
be supplemented with some additional heuristic.
(b) A second possibility is to discard the side constraints and to generate an
enumeration of the sequence {P1, P2, . . .} of paths from s to t in order
of increasing length, that is,∑
(i,j)∈P1
aij ≤
∑
(i,j)∈P2
aij ≤ · · ·
Here P1 is the best (shortest) path, P2 is the 2nd best path, and more
generally Pk is the kth best path. There are algorithms for producing
this sequence of paths in order, starting with the shortest path P1 (see
Exercise 2.26 in Chapter 2), assuming there are no cycles of negative
length. As we generate the paths Pk, we can test them for feasibility
with respect to the side constraints. The first path that is found to
be feasible is the (exactly) optimal solution of the original constrained
shortest path problem.
(c) Unfortunately, the preceding method may generate a very large number
of paths before finding an optimal solution. The reason is that the
order in which paths are generated does not take into account at all
the side constraints. To address this deficiency, one may compensate
for the discarded side constraints
∑
(i,j)∈P c
k
ij ≤ dk, by correcting the
arc lengths to reflect a dependence on the cost coefficients ckij . The
corrected arc lengths have the form
âij = aij +
K∑
k=1
µkckij , (8.13)
where µk are some positive penalty coefficients, one per side constraint.
We may view µk as a price or Lagrange multiplier for the constraint∑
(i,j)∈P c
k
ij ≤ dk, so a reasonable choice for µk is the corresponding
Lagrange multiplier of the relaxed version of problem (8.12) with the
0-1 arc flow constraints replaced with the arc flow bounds 0 ≤ xij ≤ 1.
Thus, this approach can be combined with approach (a) above that
is based on solving the relaxed version of the problem. One may also
360 Nonlinear Network Optimization Chap. 8
obtain suitable multipliers µk via the Lagrangian relaxation method to
be discussed in Chapter 10. Now given the corrected arc lengths of Eq.
(8.13), one can follow an approach similar to (b) above. In particular,
one may generate the sequence {P1, P2, . . .} of paths from s to t in order
of increasing length, using the corrected arc lengths âij , check the paths
for feasibility of the side constraints, and pick the first generated path
that is feasible.
As the preceding example illustrates, there is a broad variety of heuris-
tic procedures that are based on integer or side constraint relaxation. Some
of these heuristics can be very sophisticated, and depending on the practi-
cal problem solved, may provide a satisfactory solution. In other cases, a
heuristic may be inadequate and there may be a need for a more systematic
procedure. In Chapter 10, we will discuss procedures of this type, such as
the branch-and-bound method , which is capable in principle to obtain the
optimal solution of an integer-constrained problem, albeit with a greatly
increased computational effort.
We will also discuss in Chapter 10 local search methods, which move
from one feasible solution to another improved “neighboring” feasible solu-
tion based on some scheme. Sometimes, local search methods are modified
to allow excursions into the infeasible region, and/or relax the restriction
of cost improvement at each iteration. Genetic algorithms, tabu search,
and simulated annealing are some of the most popular local search meth-
ods, and will be briefly discussed in Chapter 10. A point that we want
to emphasize here, however, is that heuristics often involve the solution of
network problems without integer constraints, and that the minimum cost
flow algorithms of Chapters 2-7 are frequently applicable.
8.5 NETWORKS WITH GAINS
Our entire discussion of networks so far was based on the conservation of
flow assumption; that is, all the flow arriving at a node must exit the node,
and the flow sent along an arc by the start node of the arc arrives in its
entirety at its end node.
For some practical network models, however, it is useful to relax the
conservation of flow assumption. In particular, for a given arc (j, i), we
may consider introducing a positive multiplier gji, called the gain of (j, i),
which models the factor by which the flow xji is diminished or amplified
as it goes through the arc. Thus, flow xji sent by j arrives at i as gjixji,
and the conservation of flow equation becomes∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
gjixji = si, ∀ i ∈ N . (8.14)
Sec. 8.5 Networks with Gains 361
The corresponding network optimization problem is to minimize a
cost function f(x) subject to the conservation of flow constraints (8.14),
and some additional constraint of the generic form x ∈ X, expressing for ex-
ample arc flow bounds, side constraints, integer constraints, etc. Problems
of this type are referred to as network problems with gains, or generalized
network problems. By distinction, network problems that do not involve
gains are called pure network problems.
Two important examples of network problems with gains are charac-
terized by:
(1) A linear cost function, upper and lower flow bounds on the arc flows,
and the conservation of flow constraints (8.14). This problem gener-
alizes the minimum cost flow problem discussed in Chapters 1-7 to
the case where there are arc gains. It turns out that all the major
algorithms of Chapters 5-7 can be suitably modified to address this
problem (see the sources cited at the end of the chapter).
(2) A convex separable cost function, interval constraints on the arc flows,
and the conservation of flow constraints (8.14). This problem gener-
alizes the convex separable network problem of Section 8.1.
Generally, network problems with gains tend to be considerably more
complex than their pure network counterparts. For example, one of the
peculiarities of networks with gains is that cycles can generate or absorb
net flow . In particular, let us define the gain of a cycle C as the product
of the gains of positively traversed arcs of the cycle (the set of arcs C+)
divided by the product of the gains of the negatively traversed arcs of the
cycle (the set of arcs C−),
GC =
Π(i,j)∈C+gij
Π(i,j)∈C−gij
.
If GC 	= 1, the cycle C is said to be active, and otherwise it is called
passive. An active cycle is said to be flow generating if GC > 1, and it is
said to be flow absorbing if GC < 1. These definitions are illustrated in
Fig. 8.8, where it is seen that the divergence out of a flow generating (or
absorbing) cycle is greater (or smaller, respectively) than the divergence
into the cycle.
Flow  x
1
Flow  1 + gx
1
g
Flow  out of the
cycle: 1 + (g - 1)x
Figure 8.8: Illustration of flow gen-
erating and flow absorbing cycles. If
the gain g of the cycle is larger than
1, the flow out of the cycle can be ar-
bitrarily larger than the flow into the
cycle. The value of x is restricted only
by the capacity of the arcs of the cy-
cle. Similarly, if g < 1, the flow out
of the cycle can be smaller than the
flow into the cycle.
362 Nonlinear Network Optimization Chap. 8
A variation of network problems with gains arises when the diver-
gences of some of the nodes are not fixed, but are instead required to lie
between given bounds. The conservation of flow constraints of Eq. (8.14)
then become
si ≤
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
gjixji ≤ si, ∀ i ∈ N , (8.15)
where the scalars si and si are given. Constraints of this type cause some
difficulty because they cannot be converted to equality constrains as easily
as they can in pure network counterparts. The device used in pure network
problems involves the use of artificial accumulation nodes to convert the
problem to the circulation format (cf. Exercise 1.6). However, when there
are gains, this device does not work because the sum of the node supplies
need not be zero. Figure 8.9 illustrates the difficulty and provides some
ways for dealing with it.
Here are some examples of network problems with gains:
Example 8.7. Generalized Assignment Problems
Consider a problem of assigning m jobs to n machines. If job i is performed at
machine j, it costs aij and requires tij time units. We want to find a minimum
cost assignment of the jobs to the machines, given the total available time Tj
at machine j.
We can formulate this as the following network optimization problem
with gains:
minimize
m∑
i=1
n∑
j=1
aijxij
subject to
n∑
j=1
xij = 1, i = 1, . . . , m,
m∑
i=1
tijxij ≤ Tj , j = 1, . . . , n,
0 ≤ xij ≤ 1, i = 1, . . . , m, j = 1, . . . , n.
The constraints 0 ≤ xij ≤ 1 embody the assumption that jobs can be par-
titioned and performed in multiple machines. The graph representation of
the problem is shown in Fig. 8.10. This is an inequality constrained problem,
since the total flow
∑m
i=1
tijxij out of machine node j is required to lie in the
interval [0, Tj ]. Note that contrary to pure network problems, the total flow
out of the entire set of machine nodes (i.e., the total time that the machines
will be busy) is not known a priori, and depends on the flow vector x and the
arc gains.
In the case where each job must be performed in its entirety at a single
machine, the arc flow constraints must be changed to
xij = 0 or 1, i = 1, . . . , m, j = 1, . . . , n,
Sec. 8.5 Networks with Gains 363
A+
Source Node
Sink Node
Sink Node
1
2
4
5
3
Source Node
[s1, s1 ]
A-
[s2, s2 ]
[-s4, -s4 ]
[-s5, -s5 ]
_
_
_
_
_
_
_
_
Gain g
Figure 8.9: Illustration of the difficulty of converting a network problem with
gains involving the inequality constraints
si ≤
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
gjixji ≤ si, ∀ i ∈ N ,
to one involving equality constraints. For simplicity, suppose that there are two
types of nodes i, sources for which 0 ≤ si ≤ si, and sinks for which si ≤ si ≤ 0.
Let us add a “supersource” node A+ and a “supersink” node A−, an arc (A+, i)
with feasible flow range [si, si] to every source node i, and an arc (i, A
−) with
feasible flow range [−si,−si] from every sink node i. The difficulty now is that
the flow going into the network from A+ is not equal to the flow coming out
of the network to A−. It is possible, however, to reformulate the problem to
one involving conservation of flow constraints of the equality type, and an arc
(A−, A+) whose gain parameter g is unknown and is subject to optimization. An
alternative possibility is to set the supply of node A+ to
∑
{i|si>0}
si and the
supply of node A− to
∑
{i|s
i
<0} si, and to also introduce an artificial cycle at
each of the nodes A+ and A−, with gain that is less than 1. These two cycles
involve two extra nodes A
+
and A
−
, together with the arcs (A+, A
+
), (A
+
, A+),
(A−, A
−
), and (A
−
, A−), each having a gain equal to some β ∈ (0, 1).
thereby obtaining an integer-constrained problem. When there is only one
machine, this problem is equivalent to a classical problem, called the knapsack
problem. Here we want to place in a knapsack the most valuable subcollection
out of a given collection of objects, subject to a total weight constraint
m∑
i=1
wixi ≤ T,
where T is the total weight threshold, wi is the weight of object i, and xi is
a variable which is 1 or 0 depending on whether the ith object is placed in
the knapsack or not. The value to be maximized is
∑m
i=1
vixi, where vi is
364 Nonlinear Network Optimization Chap. 8
1
i
m
1
1
1
JOBS
...
aij
...
1
j
n
MACHINES
j< T_tij
t11
tmn
...
...
Figure 8.10: Illustration of the graph of a generalized assignment problem.
Each arc (i, j) has gain tij . The divergence out of each machine node j is
constrained to lie in the interval [0, Tj ].
the value of the ith object. We will discuss in more detail integer-constrained
problems of this type in Chapter 10.
Example 8.8. Production Scheduling
Consider a system involving production of multiple types of products over N
time periods. The system is similar to the one of Example 8.1, but is more
general in that it allows product consumption and loss, as well as product
conversion from one type to another. These new features introduce gains for
the arc flows.
The system is described by a set of equations
xik+1 = b
i
kx
i
k +
∑
{j|j =i}
(cjik y
ji
k − y
ij
k ) − u
i
k, i = 1, . . . , m, k = 0, . . . , N − 1,
where
xik: The amount of product of type i available at the start of the kth period.
yjik : The amount of product of type j that is used for production of product
of type i during the kth period.
uik: The amount of product of type i that is consumed during the kth period.
The scalars bik and c
ji
k are nonnegative and are known, and there are interval
constraints on all the variables xik, y
ji
k , and u
i
k. The cost function is
m∑
i=1
N−1∑
k=0
gik(u
i
k),
where gik are nonincreasing convex functions, and −gik(uik) expresses the ben-
efit corresponding to production of uik units of product i at time k.
Sec. 8.6 Optimality Conditions 365
We can formulate the problem as a convex separable network problem
with gains by introducing an artificial accumulation node, as shown in Fig.
8.11. The coefficients bik and c
ji
k are the gains. The divergence from all nodes
except the artificial node is constrained to be 0. The divergence from the arti-
ficial node is not constrained in any way, and is subject to optimization. This
corresponds to Eq. (8.15) with the upper and lower bounds on the divergence
being ∞ and −∞, respectively.
u0 u1 uN-2 uN-1
A
. . .10 N-2 N-1
x1
1 x2
1
xN
1
xN-1
1
x0
1
xN
2
x0
2
y0
21 21yN-1yN-2
21y1
21
. . .10 N-2 N-1
x1
2 x2
2 xN-1
2
b 2 b
2
b2
b1 b1b1
c c c c
b2
b1
Figure 8.11: Illustration of the graph of a production scheduling problem for two
products types, where the type 2 product is used to produce the type 1 product.
The arcs have gains as shown. The divergence out of the artificial accumulation
node A is unconstrained.
We finally note two transformations and equivalences that highlight
the differences between networks with gains and pure networks. Figure
8.12 shows that it is possible to transform a network problem with gains
to a pure network problem, by introducing some side constraints. Figure
8.13 shows that a network problem with gains can be transformed to a pure
network problem if all cycles are passive.
8.6 OPTIMALITY CONDITIONS
In this section we develop some basic optimality conditions for convex net-
work flow problems where the cost function f is continuously differen-
tiable. By this we mean that for all flow vectors x, the partial derivatives
∂f(x)/xij , (i, j) ∈ A, exist and are continuous functions of x. The vector
whose components are these partial derivatives is the gradient ∇f(x) of f at
366 Nonlinear Network Optimization Chap. 8
zi
i
A
si
Figure 8.12: Transformation of a network problem with gains to a pure circu-
lation problem with side constraints. We introduce a variable zi for each node i,
and we write the conservation of flow constraints
si ≤
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
gjixji ≤ si
as
si ≤
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji + zi ≤ si,
while simultaneously requiring that the side constraints
zi =
∑
{j|(j,i)∈A}
(1 − gji)xji
be satisfied. We may interpret zi as the flow of an arc from i to an artificial
accumulation node A. With an additional arc (A, i) for each node i, with feasible
flow range [si, si], the problem is converted to a circulation problem without gains
but with side constraints.
x (Appendix A summarizes definitions and results relating to differentiable
functions).
Generally, for a differentiable function f defined on the Euclidean
space n, the gradient is denoted by ∇f(x) and is considered to be a
column vector. A prime denotes transposition, so that ∇f(x)′ is a row
vector, and ∇f(x)′y is the inner product of ∇f(x) with a vector y. A
result that we will often use is that if f is continuously differentiable (over
the entire space), then f is convex over a convex set F if and only if the
first order approximation of f based on f(x) and ∇f(x) underestimates f ;
that is, f is convex over F if and only if
f(y) ≥ f(x) + ∇f(x)′(y − x), ∀ x, y ∈ F. (8.16)
Sec. 8.6 Optimality Conditions 367
1 4
3
2
1 62
2 3
3
1
1 4
3
2
1 1
Network with Gains
Pure Network
Equivalent
Figure 8.13: Illustration of the passivity condition under which a network prob-
lem with gains can be transformed to a pure network problem. Suppose that for
each node i there exists a positive scalar γi such that
γi = gjiγj , ∀ (j, i) ∈ A. (*)
Then the conservation of flow equation∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
gjixji = si, ∀ i ∈ N ,
can be written as∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
γiγ
−1
j xji = si, ∀ i ∈ N .
By using the transformation of variables xij = γiξij , si = γiζi, we obtain∑
{j|(i,j)∈A}
ξij −
∑
{j|(j,i)∈A}
ξji = ζi, ∀ i ∈ N .
Thus, the problem is equivalent to a pure network problem whose arc flows are ξij
(see the figure, where γ1 = 1, γ2 = γ3 = 2, and γ4 = 6). By denoting pi = − ln γi,
we see that the condition (*) holds if and only if there exist scalars pi such that
pi = ln gij + pj , ∀ (i, j) ∈ A.
By the feasible differential theorem (Exercise 5.11 in Chapter 5), this is true if
and only if for every cycle C, we have∑
(i,j)∈C+
ln gij −
∑
(i,j)∈C−
ln gij = 0,
which is equivalent to requiring that all cycles have gain equal to 1, i.e., that they
be passive.
368 Nonlinear Network Optimization Chap. 8
When the cost function f of an optimization problem is differen-
tiable, an important analytical and algorithmic idea is linearization, which
amounts to replacing f with its first order linear approximation around
some vector x
f(x) + ∇f(x)′(x − x),
while leaving the constraint set unchanged. This idea underlies the follow-
ing basic necessary and sufficient condition for optimality.
Proposition 8.1: Consider the minimization of a function f : n →
 over a convex subset F of the Euclidean space n. Assume that
f is continuously differentiable and is convex over F . Then, a vector
x∗ ∈ F is optimal if and only if
∇f(x∗)′(x − x∗) ≥ 0, ∀ x ∈ F. (8.17)
Proof: Assume that x∗ is an optimal solution. Then, for all x ∈ F and
all α ∈ (0, 1], we have f
(
x∗ + α(x − x∗)
)
≥ f(x∗). Hence
f
(
x∗ + α(x − x∗)
)
− f(x∗)
α
≥ 0, ∀ α ∈ (0, 1].
By taking the limit as α → 0, we obtain ∇f(x∗)′(x−x∗) ≥ 0, which is Eq.
(8.17).
Conversely, suppose that x∗ ∈ F and Eq. (8.17) holds. Since f is
convex over F , we have by Eq. (8.16)
f(x) ≥ f(x∗) + ∇f(x∗)′(x − x∗), ∀ x ∈ F.
Hence, using Eq. (8.17), we obtain f(x) ≥ f(x∗) for all x ∈ F . Q.E.D.
The optimality condition (8.17) is illustrated in Fig. 8.14. One way
to interpret the condition is to note that it is equivalent to x∗ being an
optimal solution of the linearized problem
minimize ∇f(x∗)′(x − x∗)
subject to x ∈ F.
Note that the optimality condition (8.17) holds at an optimal solution x∗
even if f is nonconvex (the first part of the proof of Prop. 8.1 still applies
as long as F is convex). However, in this case the condition is not sufficient
to guarantee optimality of x∗.
Sec. 8.6 Optimality Conditions 369
Surfaces of equal cost f (x)
Constraint set F
x
x*
∇f (x*)
Figure 8.14: Geometric interpretation
of the optimality condition of Prop. 8.1.
A vector x∗ ∈ F is optimal if and only
if the gradient ∇f(x∗) makes an angle
less than or equal to 90 degrees with all
feasible variations x − x∗, x ∈ F .
As a special case of Prop. 8.1, let us extend the nonnegative cycle
condition of Prop. 1.2 for the minimum cost flow problem to the case of
the constraint-separable convex network problem
minimize f(x)
subject to xij ∈ Xij , ∀ (i, j) ∈ A,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
(8.18)
where each Xij is an interval of the real line (cf. Section 8.1). Similar to
Section 1.1.2, we say that a cycle C is unblocked with respect to a flow
vector x, if xij ∈ Xij for all arcs (i, j), and there exists a δ > 0 such that
xij + δ ∈ Xij for all arcs (i, j) in C+ (the set of forward arcs of C), and
xij − δ ∈ Xij for all arcs (i, j) in C− (the set of backward arcs of C). We
have the following proposition.
Proposition 8.2: (Nonnegative Cycle Condition) Consider the
constraint-separable convex network flow problem (8.18), and assume
that f is continuously differentiable over the entire space, and is convex
over the feasible set. Then, a vector x∗ is optimal if and only if x∗ is
feasible and for every simple cycle C that is unblocked with respect to
x∗ there holds ∑
(i,j)∈C+
∂f(x∗)
xij
−
∑
(i,j)∈C−
∂f(x∗)
xij
≥ 0. (8.19)
Proof: By Prop. 8.1, x∗ is optimal if and only if x∗ is an optimal solution
370 Nonlinear Network Optimization Chap. 8
of the linearized problem, which is the minimum cost flow problem
minimize
∑
(i,j)∈A
∂f(x∗)
∂xij
xij
subject to xij ∈ Xij , ∀ (i, j) ∈ A,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N .
The result follows by applying Prop. 1.2 (even though this proposition is
stated for the case where the Xij are compact intervals, it is easily extended
to the case where the Xij are arbitrary intervals). Q.E.D.
The idea of the preceding proposition is to use the linearized problem
as a vehicle for generalizing results about linear network flow problems to
nonlinear problems. This idea can be used in several different ways. For
example, one can obtain analogs of the complementary slackness theorems
of Section 4.2 for the constraint-separable convex network problem (see
Exercise 8.7).
When the cost function is nondifferentiable at an optimal solution
x∗, one may still use the argument of the proof of Prop. 8.1 to show that
the directional derivative of f at x∗ cannot be negative along any direction
x−x∗ where x is feasible. We will use this approach for the case of a convex
separable problem in Section 9.2, where we will generalize the nonnegative
cycle condition of Prop. 8.2.
8.7 DUALITY
Duality theory for nonlinear network problems can be developed similar
to the case of a minimum cost flow problem. We eliminate some of the
constraints through the use of prices (or Lagrange multipliers). We then
form a Lagrangian function, and we define a dual function by minimizing
the Lagrangian subject to the remaining constraints. The dual problem
is to maximize the dual function over the prices. We will focus on two
important types of duality analysis in network optimization.
Convex Separable Network Problems
The first type of duality relates to the convex separable network problem
of Section 8.1:
minimize
∑
(i,j)∈A
fij(xij)
subject to xij ∈ Xij , ∀ (i, j) ∈ A,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N .
Sec. 8.7 Duality 371
Here, as in the development of duality for the minimum cost flow problem
in Chapter 4, we use prices to eliminate the conservation of flow constraints.
We introduce a price pi for each node i and we form the Lagrangian function
L(x, p) =
∑
(i,j)∈A
fij(xij) +
∑
i∈N
pi
 ∑
{j|(j,i)∈A}
xji −
∑
{j|(i,j)∈A}
xij + si

=
∑
(i,j)∈A
(
fij(xij) − (pi − pj)xij
)
+
∑
i∈N
sipi.
(8.20)
The dual function value q(p) at a price vector p is obtained by minimizing
L(x, p) over all x satisfying the constraints xij ∈ Xij . Thus, we have for
every p
q(p) = inf
xij∈Xij , (i,j)∈A
L(x, p) =
∑
(i,j)∈A
qij(pi − pj) +
∑
i∈N
sipi,
where
qij(pi − pj) = inf
xij∈Xij
{
fij(xij) − (pi − pj)xij
}
. (8.21)
(The reason for using inf, rather than min, in the above definition of q is
that for a given p, it is not known whether the minimum over x ∈ X is
attained.) The dual problem is
maximize q(p)
subject to no constraint on p.
There is a powerful and elegant theory around this problem, which
is in many ways similar to the duality theory of Chapter 4. The theory
involves a generalized notion of complementary slackness, and, in an algo-
rithmic setting, a notion of -complementary slackness. Another interesting
aspect of this theory is that if the functions fij are strictly convex over the
intervals Xij and the infimum is attained in Eq. (8.21) for all (i, j) and all p,
then the dual function q is differentiable and its gradient can be calculated
with a convenient formula, as will be shown in Section 9.4. We postpone
further discussion of separable problem duality and algorithms for Chapter
9, where we will provide a detailed development.
Convex Network Problems with Side Constraints
The second type of duality relates to the convex network problem with side
constraints, discussed in Section 8.2:
minimize f(x)
subject to xij ∈ Xij , ∀ (i, j) ∈ A,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
gt(x) ≤ 0, t = 1, . . . , r,
(8.22)
372 Nonlinear Network Optimization Chap. 8
where each set Xij is an interval of the real line, and the functions f and
gt are assumed convex over the space of the flow vectors x.
Here, we use prices to eliminate some or all of the side constraints,
thereby enhancing the problem’s separable or other structure. The result-
ing theory is a special case of the general duality theory for convex pro-
gramming programs, and does not have any distinctive features that can
be attributed to the problem’s network structure. Furthermore, we will not
use this theory in a essential way in our subsequent development. For this
reason, we will refer to the standard nonlinear programming literature for
a deeper analysis, and for proofs of the results that we will state.
We introduce a Lagrange multiplier µt for each of the side constraints
gt(x) ≤ 0, and we form the corresponding Lagrangian function
L(x, µ) = f(x) +
r∑
t=1
µtgt(x). (8.23)
Let F̃ denote the set defined by the constraints of the problem except for
the side constraints,
F̃ =
{
x
∣∣∣ xij ∈ Xij , ∀ (i, j) ∈ A,
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N
}
.
The dual function is defined by
q(µ) = inf
x∈F̃
L(x, µ), (8.24)
and the dual problem is
maximize q(p)
subject to µ ≥ 0.
(8.25)
Note that q may not be real-valued because for some µ, the infimum in
Eq. (8.24) can be −∞. Thus the dual problem embodies the additional
implicit constraint µ ∈ Q, where Q is the “effective domain” of q given by
Q =
{
µ | q(µ) > −∞
}
.
We refer to the optimal value attained in the primal and in the dual
problems as the optimal primal cost and optimal dual cost , respectively.
An important fact is that the optimal dual cost is always no greater than
Sec. 8.7 Duality 373
the optimal primal cost. This is known as the weak duality theorem. The
proof is simple: for any µ ≥ 0, we have
q(µ) = inf
x∈F̃
{
f(x) +
r∑
t=1
µtgt(x)
}
≤ inf
x∈F̃ , gt(x)≤0, t=1,...,r
{
f(x) +
r∑
t=1
µtgt(x)
}
≤ inf
x∈F̃ , gt(x)≤0, t=1,...,r
f(x),
where the first inequality follows because the infimum of the Lagrangian
is taken over a subset of F̃ , and the last inequality follows using the non-
negativity of µt. Thus, by taking the supremum of the left-hand side over
µ ≥ 0, we obtain
sup
µ≥0
q(µ) ≤ inf
x∈F̃ , gt(x)≤0, t=1,...,r
f(x). (8.26)
The two expressions on the left and the right above are recognized as the
optimal dual cost and the optimal primal cost, respectively
When the optimal dual cost is strictly smaller than the optimal primal
cost, we say that there is a duality gap. In the convex case of problem
(8.22), typically there is no duality gap. However, to guarantee this we
need some technical assumptions, which are usually satisfied in practice.
The following proposition makes this precise and also gives necessary and
sufficient conditions for primal and dual optimality.
Proposition 8.3: Consider the convex network problem with side
constraints (8.22).
(a) x∗ is an optimal primal solution and µ∗ is an optimal dual solu-
tion if and only if x∗ is primal-feasible, µ∗ ≥ 0, and
x∗ = arg min
x∈F̃
L(x, µ∗), µ∗t gt(x∗) = 0, t = 1, . . . , r.
(b) The optimal primal cost is equal to the optimal dual cost and
there exists an optimal solution of the dual problem if one of the
following two conditions holds:
374 Nonlinear Network Optimization Chap. 8
(1) The intervals Xij are closed, and the functions gt are linear.
(2) There exists a feasible flow vector x such that gt(x) < 0 for
all t and xij lies in the interior of the interval Xij for all
(i, j) ∈ A for which Xij has nonempty interior.
The preceding proposition can be shown in a broader context that
does not relate to network flows, so we refer to the literature for the proof.
In particular, part (a) is shown in Prop. 5.1.5 of Bertsekas [1995b], while
part (b) is shown in Props. 5.2.1 and 5.3.2 of the same reference.
In the two types of duality discussed so far in this section, either the
conservation of flow constraints or the side constraints are dualized. There
is also a third type of duality, where both of these constraints are dualized.
Here, the Lagrangian function is given by [cf. Eqs. (8.20) and (8.23)]
L(x, p, µ) = f(x) +
r∑
t=1
µtgt(x)
+
∑
i∈N
pi
 ∑
{j|(j,i)∈A}
xji −
∑
{j|(i,j)∈A}
xij + si
 .
The dual function is defined by
q(p, µ) = inf
xij∈Xij , (i,j)∈A
L(x, p, µ), (8.27)
and the dual problem is
maximize q(p, µ)
subject to p ∈ N , µ ≥ 0.
It is also possible to derive a corresponding weak duality result and a
proposition that is analogous to the one given above.
We finally mention that there are interesting nonconvex cases of prob-
lem (8.22), and their associated dual problems defined by Eqs. (8.25) and
(8.27). In these cases, Xij are not necessarily intervals and embody integer
constraints, and the cost function f and the side constraint functions gt are
real-valued but not necessarily convex functions of x (for some examples,
see Section 10.3). Then, there is usually a duality gap. However, the weak
duality theorem [cf. Eq. (8.26)] still holds, because its derivation does not
rely on convexity. We will see the utility of this fact when we discuss the
Lagrangian relaxation method in Section 10.3.
Sec. 8.8 Algorithms and Approximations 375
8.8 ALGORITHMS AND APPROXIMATIONS
One of the most useful ideas in nonlinear optimization is to approximate
the given problem with one or more simpler problems. We have already
encountered the idea of linearization, whereby the nonlinear problem is re-
placed by a linear one. There are also other approximation approaches,
where the simpler problems involve a parameter  > 0 that controls the
quality of the approximation. As  → 0, the approximation becomes more
accurate. One then typically considers the solution of a sequence of ap-
proximate problems corresponding to a sequence {k} of approximation
parameters that tends to 0, thereby yielding solution of the original prob-
lem in the limit (under some appropriate continuity conditions). In this
section we discuss some of the major approximation approaches and their
associated algorithmic procedures.
8.8.1 Feasible Direction Methods
We have seen in Section 8.6 the value of the linearization approach for
developing optimality conditions and for providing a link with the minimum
cost flow analysis of Chapters 2-7. In this section, we discuss the use of
the linearization idea for the development of a broad class of algorithms
for convex problems.
Consider the generic problem of minimizing over a convex set F a
function f : n →  that is continuously differentiable and is convex over
F . Given a feasible vector x, a feasible direction at x is a nonzero vector d
such that x + αd is feasible for all α in some interval [0, α], where α > 0
(see Fig. 8.15). We say that d is a feasible descent direction at x if there
exists an α > 0 such that
x + αd ∈ F, f(x + αd) < f(x), ∀ α ∈ (0, α].
Since f is continuously differentiable, the inequality in the above relation
is equivalent to ∇f(x)′d < 0, as can be seen from the first order Taylor
series expansion
f(x + αd) = f(x) + α∇f(x)′d + o(α)
[for an α that is positive but sufficiently small, the term α∇f(x)′d domi-
nates the term o(α), and its sign is the same as the sign of f(x+αd)−f(x)].
The following proposition shows that at every feasible solution that is not
optimal, there exists a feasible descent direction, and that by solving the
linearized problem, we can obtain such a direction.
376 Nonlinear Network Optimization Chap. 8
d
Constraint set F
Feasible 
directions at x
x
Figure 8.15: Feasible directions d at a feasible x. By definition, d is a feasible
direction if changing x by a small amount in the direction d maintains feasibility.
Proposition 8.4: Consider the minimization over a convex set F of
a function f : n →  that is continuously differentiable and is convex
over F . Let x̂ be a feasible vector that is not optimal, and let x be an
optimal solution of the linearized problem
minimize ∇f(x̂)′(x − x̂)
subject to x ∈ F.
Then the vector d = x − x̂ is a feasible descent direction of f at x̂.
Proof: Since x̂ is not optimal, from Prop. 8.1 it follows that there exists
a vector x̃ ∈ F such that ∇f(x̂)′(x̃ − x̂) < 0, so x̃ − x̂ is a feasible descent
direction of f at x̂. If x solves the linearized problem, we have
∇f(x̂)′(x − x̂) ≤ ∇f(x̂)′(x̃ − x̂) < 0,
implying that x − x̂ is a feasible descent direction of f at x̂. Q.E.D.
The preceding proposition suggests an iterative primal cost improve-
ment algorithm, whereby a sequence of flow vectors with decreasing cost is
generated by making flow changes along feasible directions. For example,
we may consider a method, which at each iteration solves the linearized
problem at the current iterate, computes the corresponding feasible de-
scent direction, and effects a correction along that direction (this is the
conditional gradient method to be discussed shortly). More generally, we
consider a feasible direction method , which starts with a feasible vector x0
Sec. 8.8 Algorithms and Approximations 377
and aims to generate a sequence of feasible vectors {xk} according to
xk+1 = xk + αk(xk − xk),
where αk ∈ (0, 1], and
xk ∈ F, ∇f(xk)′(xk − xk) < 0.
For each xk that is not optimal, there must exist such a vector xk, since
otherwise we would have ∇f(xk)′(x − xk) ≥ 0 for all x ∈ F , contradicting
the non-optimality of xk (cf. Prop. 8.1). Figure 8.16 illustrates a feasible
direction method.
x0
x*x
4
x3
x2
x1
Surfaces of
equal cost
Figure 8.16: Sample path of a feasible direction method. At each iteration, we
obtain a feasible point along a feasible descent direction.
There are several rules for choosing the stepsize αk in feasible direc-
tion methods. Typically, αk must be such that the cost is improved, that
is,
f(xk+1) < f(xk).
For example, one may use the minimization rule, whereby αk is chosen to
minimize the cost along the feasible direction, that is,
f
(
xk + αk(xk − xk)
)
= min
α∈[0,1]
f
(
xk + α(xk − xk)
)
. (8.28)
There are general results for feasible direction methods with the minimiza-
tion rule, as well with other stepsize rules, which establish their validity
by showing, under the convexity conditions of Prop. 8.4, that every limit
point of the generated sequence {xk} is optimal. For a fairly extensive
discussion, we refer to Bertsekas [1995b], Chapter 2.
378 Nonlinear Network Optimization Chap. 8
Conditional Gradient Methods and Multicommodity Flows
We now consider a popular feasible direction method where the feasible
descent direction is generated by solving the linearized problem
minimize ∇f(xk)′(x − xk)
subject to x ∈ F,
(8.29)
(we assume here that an optimal solution of this problem exists for every
k). Thus, xk is given by
xk = arg min
x∈F
∇f(xk)′(x − xk).
The corresponding feasible direction method is known as the conditional
gradient method , or the Frank-Wolfe method . The process to obtain xk is
illustrated in Fig. 8.17. Note that in order for the method to make practical
sense, the subproblem (8.29) must be much simpler than the original.
∇f(x)
x
x
_
Constraint set F
Surfaces of
equal cost
Figure 8.17: Finding the feasible descent
direction x − x at a vector x in the condi-
tional gradient method: x is a vector of F
such that the inner product ∇f(x)′(x−x)
is most negative.
Let us describe the conditional gradient method in the context of the
constraint-separable convex multicommodity flow problem introduced in
Section 8.3:
minimize f(x)
subject to xij(m) ∈ Xij(m), ∀ (i, j) ∈ A, m = 1, . . . , M,∑
{j|(i,j)∈A}
xij(m)−
∑
{j|(j,i)∈A}
xji(m) = si(m), ∀ i ∈ N , m = 1, . . . , M.
The linearized problem is to minimize
∇f(xk)′(x − xk)
Sec. 8.8 Algorithms and Approximations 379
over all x satisfying the conservation of flow constraints, and the interval
constraints xij(m) ∈ Xij(m). This problem is easy to solve, because in
view of the separability of the constraint set, it decomposes into a collec-
tion of subproblems, one per commodity. The subproblem for commodity
m is a minimum cost flow problem with cost coefficient of arc (i, j) equal to
∂f(xk)/∂xij(m), and can be solved with the efficient algorithms of Chap-
ters 2-7.
A special case of the multicommodity flow problem is particularly
interesting. This is the case where the constraints xij(m) ∈ Xij(m) have
the form
0 ≤ xij(m), ∀ (i, j) ∈ A, m = 1, . . . , M, (8.30)
and furthermore there is only one supply node per commodity m:
sim(m) > 0, for a unique origin node im.
In this case, it can be seen that for each commodity m, the linearized
problem becomes a shortest path problem, where the origin is node im and
the length of each arc (i, j) is ∂f(xk)/∂xij(m). Thus, the kth iteration of
the conditional gradient method consists of the following steps:
(a) For each commodity m, obtain a shortest path from node im to each
node i with si(m) < 0, where the length of arc (i, j) is ∂f(xk)/∂xij(m).
(b) For each commodity m, route from node im to each node m with
si(m) < 0 the corresponding amount of flow −si(m) along the asso-
ciated shortest path. Let xk be the corresponding multicommodity
flow vector.
(c) Obtain the new flow vector by
xk+1 = xk + αk(xk − xk),
where αk is an appropriately chosen stepsize [e.g., using the mini-
mization rule of Eq. (8.28)].
Unfortunately, the asymptotic rate of convergence of the conditional
gradient method is not very fast. A partial explanation is that the vec-
tors xk used in the algorithm are typically extreme points (vertices) of F .
Thus, the feasible direction used may tend to be orthogonal to the direction
leading to the minimum (see Fig. 8.18). There are other feasible direction
methods, which achieve a faster convergence rate, at the expense of greater
overhead per iteration. For example, gradient projection methods obtain
the feasible descent direction by using a quadratic cost approximation to
f in place of the linear approximation used by the conditional gradient
method. For a description and analysis of gradient projection and other
feasible direction methods, we refer to the books by Bertsekas [1995b], and
by Bertsekas and Gallager [1992], and to the survey by Florian and Hearn
[1995]; see also Section 8.8.7.
380 Nonlinear Network Optimization Chap. 8
x0
x1
x2
x1 x0
__
Constraint set F
Surfaces of
equal cost
x*
Figure 8.18: Illustration of the slow con-
vergence rate of the conditional gradient
method. The feasible direction used may
tend to be orthogonal to the direction lead-
ing to the minimum.
8.8.2 Piecewise Linear Approximation
One possibility for dealing with a convex cost problem is to use efficient
ways to reduce it to an essentially linear cost problem by piecewise lin-
earization of the cost function. Then, if the constraint set is polyhedral,
the resulting approximating problem can be solved using standard linear
programming methods. This approach is often convenient and straightfor-
ward, although it may result in loss of insight, because generally, a nonlin-
ear problem may have elegant features that are lost in a piecewise linear
approximation.
A particularly interesting case is the convex separable network prob-
lem of Section 8.1:
minimize
∑
(i,j)∈A
fij(xij)
subject to xij ∈ Xij , ∀ (i, j) ∈ A,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N .
When piecewise linearization is applied to the arc cost functions fij , the
resulting problem can be converted to a minimum cost flow problem with
extra arcs, as discussed earlier (see Exercise 1.8). Note that one can use in-
ner linearization [approximation from within using a discrete set of points,
as in Fig. 8.19(a)], or outer linearization [approximation from without using
a discrete set of tangent slopes, as in Fig. 8.19(b)].
It is possible to use a one-time piecewise linearization of the cost
function. It is also possible to consider a sequential procedure, whereby
the cost function is repeatedly approximated with ever-increasing approx-
imation accuracy. In the most straightforward application of this idea, a
number of breakpoints for inner linearization within each interval Xij is
Sec. 8.8 Algorithms and Approximations 381
0 x
X
f (x )
0 x
X
f (x )
(a) (b)
Figure 8.19: Inner and outer linearization of a convex function f(x) of a single
variable over an interval X.
chosen. These points are more or less regularly spaced and their number is
gradually increased to improve the accuracy of the approximation. Usually,
the solution obtained from each level of approximation is used as a starting
point for the algorithm to solve the next (finer) level of approximation.
In a more sophisticated approach, one may use an adaptive lineariza-
tion technique, whereby the selection of the breakpoints of the approxima-
tion is guided by the algorithmic progress. This approach aims to make
the accuracy of the approximation better where it matters most, namely
in the neighborhood of the optimal flows. Here is an important example of
a method of this type.
Example 8.9. Cutting Plane Method
This is an iterative method, which will be discussed in more detail and in
greater generality in Chapter 10. Given the initial flow vector x0 and the
flow vectors x1, . . . , xk obtained from the first k iterations, we form an outer
linearization of each arc cost function fij ,
f̂kij(xij) = max
m=0,...,k
{
fij(x
m
ij ) + ∇fij(xmij )(xij − xmij )
}
.
[We have used the gradient ∇fij(xmij ) here, but if fij is not differentiable at
xmij , a subgradient can be used; see Chapter 10.] We then obtain the next
iterate xk+1 as an optimal solution of the approximate problem based on the
outer linearization
xk+1 = arg min
x∈F
∑
(i,j)∈A
f̂kij(xij), k = 0, 1, . . .
382 Nonlinear Network Optimization Chap. 8
where F is the constraint set of the problem. Thus for each iteration m =
1, 2, . . ., a line (linear approximation) fij(x
m
ij )+∇fij(xmij )(xij −xmij ) is added,
and the maximum over all the lines is used to approximate fij(xij) (see Fig.
8.20). It must be assumed here that each of the approximate problems has
an optimal solution, and this is guaranteed if each interval Xij is compact.
The cutting plane method has the nice property that it tends to increase
the approximation accuracy in the neighborhood of the iterates. In fact, it
is possible to prove various convergence results, for which we refer to the
literature cited at the end of the chapter. It is fairly easy to show that if the
functions fij are piecewise linear to start with, the method finds an optimal
solution in a finite number of iterations.
It is also possible to use a variant of the cutting plane method that
is based on inner linearization. Here if f̂kij is an inner approximation of fij
based on k+2 breakpoints, and xk+1 minimizes
∑
(i,j)∈A f̂
k
ij(xij) over x ∈ F ,
a new breakpoint at xk+1ij is added to the approximation of fij . This method
requires that each interval Xij is compact so that its endpoints can be used
as the two extreme breakpoints of f̂0ij(xij).
0 xij
Xij
fij (xij )
fij (xij ) +  ∇fij (xij )(xij  -  xij ) 
0
 
0
 
0
fij (xij ) +  ∇fij (xij )(xij  -  xij )
2 2 2
fij (xij ) +  ∇fij (xij )(xij  -  xij  )
1 1 1
xij 
0 xij 
2 xij
1
Figure 8.20: Illustration of the cutting plane method. At the kth iteration, the
line
fij(x
k
ij) + ∇fij(xkij)(xij − xkij),
corresponding to the optimal solution xk of the current approximate problem, is
added to the approximation.
8.8.3 Interior Point Methods
A standard nonlinear programming approach to deal with troublesome in-
Sec. 8.8 Algorithms and Approximations 383
equality constraints is to eliminate them by means of a barrier function.
In particular, let us consider a convex network problem and let us assume
that it can be written in the form
minimize f(x)
subject to x ∈ F , gt(x) ≤ 0, t = 1, . . . , r,
where f and gt are convex functions of the flow vector x, and F is a
closed convex set. Here F includes the conservation of flow constraints
and possibly some other constraints. Typically, the constraints gt(x) ≤ 0
include side constraints and possibly some arc flow bound constraints.
Consider the set
S =
{
x ∈ F | gt(x) < 0, t = 1, . . . , r
}
,
and assume that it is nonempty. In barrier methods, we add to the cost a
function B(x), called the barrier function, which is defined in the interior
set S. This function is continuous and tends to ∞ as any one of the
constraints gt(x) approaches 0 from negative values. The two most common
examples of barrier functions are:
B(x) = −
r∑
t=1
ln
{
−gt(x)
}
, logarithmic,
B(x) = −
r∑
t=1
1
gt(x)
, inverse.
Note that both of these functions are convex, given the convexity of gt.
The most common barrier method is defined by introducing a param-
eter sequence {k} with
0 < k+1 < k, k = 0, 1, . . . , k → 0.
It consists of finding
xk = arg min
x∈S
{
f(x) + kB(x)
}
, k = 0, 1, . . .
Since the barrier function is defined only on the interior set S, the suc-
cessive iterates of any method used for this minimization must be interior
points. Note that the barrier term kB(x) goes to zero for all interior
points x ∈ S as k → 0. Thus the barrier term becomes increasingly in-
consequential as far as interior points are concerned, while progressively
allowing xk to get closer to the boundary of S (as it should if the optimal
solutions of the original constrained problem lie on the boundary of S). It
can be shown, under our convexity assumptions, that every limit point of
384 Nonlinear Network Optimization Chap. 8
a sequence {xk} generated by a barrier method is an optimal solution of
the original problem. For the proof we refer to Bertsekas [1995b], p. 314.
A major application of the logarithmic barrier method is to linear
and quadratic programming problems. The corresponding methods belong
to the general class of interior point methods, and have been the focus of
much theoretical and applications-oriented research. As a result, there is a
lot of accumulated experience with sophisticated implementations that can
deal with very large problems. In particular, interior point methods have
been applied to the dual minimum cost flow problem of maximizing over a
price vector p the dual cost function
∑
(i,j)∈A
min
[
−bij(aij + pj − pi),−cij(aij + pj − pi)
]
,
where aij are the arc cost coefficients, and bij and cij are the arc flow
bounds (cf. the duality framework of Chapter 4). One may transform this
problem to
minimize
∑
(i,j)∈A
zij
subject to zij ≥ bij(aij + pj − pi), ∀ (i, j) ∈ A,
zij ≥ cij(aij + pj − pi), ∀ (i, j) ∈ A,
where zij is an auxiliary variable for each arc (i, j), and apply the logarith-
mic barrier method. We refer to the specialized literature cited at the end
of the chapter.
8.8.4 Penalty and Augmented Lagrangian Methods
Another standard nonlinear programming approach to deal with trouble-
some constraints is to eliminate them by means of a penalty function. This
is similar to the use of barrier functions, but penalty functions do not re-
quire that the region defined by the eliminated constraints has nonempty
interior, so they can be used for equality constraints as well as for inequal-
ities. Furthermore, their convergence and functionality can be improved
through the use of Lagrange multiplier iterations, leading to augmented
Lagrangian methods, which are among the most reliable and practically
useful methods in nonlinear programming.
The theory of penalty and augmented Lagrangian methods is ex-
tensive and cannot be developed here in much detail. Thus we will just
summarize the principal method and we will briefly discuss its properties.
Sec. 8.8 Algorithms and Approximations 385
We focus on the convex network problem with side constraints
minimize f(x)
subject to xij ∈ Xij , ∀ (i, j) ∈ A,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
gt(x) ≤ 0, t = 1, . . . , r.
Let us group together the constraints other than the side constraints in the
set
F =
{
x | xij ∈ Xij , ∀ (i, j) ∈ A,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N
}
,
so that the problem is written as
minimize f(x)
subject to x ∈ F , g(x) ≤ 0,
where g(x) is the column vector with components g1(x), . . . , gr(x).
Let µ = (µ1, . . . , µr) be a multiplier vector and let c be a positive
scalar, which we call penalty parameter . Define
g+t (x, µ, c) = max
{
gt(x),−µt/c
}
, t = 1, . . . , r,
and let g+(x, µ, c) be the column vector with components g+t (x, µ, c). The
augmented Lagrangian function is defined by
Lc(x, µ) = f(x) + µ′g+(x, µ, c) +
c
2
‖g+(x, µ, c)‖2.
The augmented Lagrangian method consists of a sequence of minimizations
minimize Lck(x, µk)
subject to x ∈ F ,
where {ck} is some positive penalty parameter sequence and {µk} is gen-
erated by the iteration
µk+1 = µk + ckg+(xk, µk, ck).
As an example, consider the separable multicommodity network flow
problem with arc capacities yij ≤ cij (cf. Section 8.3). It turns out that
much of the algorithmic methodology for multicommodity problems applies
only if the capacity constraints are absent (see also Sections 8.8.1 and 8.8.7).
386 Nonlinear Network Optimization Chap. 8
It is thus often expedient to bring to bear this algorithmic methodology
by eliminating the capacity constraints using the augmented Lagrangian
method.
There is extensive convergence analysis and practical experience that
supports the augmented Lagrangian approach, for which we refer to stan-
dard nonlinear programming textbooks. The book by Bertsekas [1982] is
an extensive research monograph that focuses on augmented Lagrangian
methods and their many variations. Generally, the main result for the con-
vex problem discussed here is that if the penalty parameter sequence {ck}
is nondecreasing and a dual optimal solution exists, then the multiplier
sequence {µk} converges to some dual optimal solution. The convergence
of {µk} is accelerated if {ck} is increased at a faster rate. On the other
hand, there is a concern with ill-conditioning in the minimization of the
augmented Lagrangian, if ck is increased “too fast.” Generally, the aug-
mented Lagrangian approach provides a simple and reliable way to deal
with troublesome constraints, and is strongly recommended in practice.
8.8.5 Proximal Minimization
Consider the convex network problem
minimize f(x)
subject to x ∈ F,
(8.31)
where F is convex and f is convex over F . An interesting special case is
when f is strictly convex; that is, for all x, y ∈ F with x 	= y, we have
f
(
αx + (1 − α)y
)
< αf(x) + (1 − α)f(y), ∀ α ∈ (0, 1).
In this case, one may show that the minimum of f is uniquely attained if
it is attained at all. However, strict convexity of f has another and more
far-reaching consequence, which will be shown in Chapter 9 in the con-
text of separable problems (see also Danskin’s theorem in Appendix A):
under mild technical conditions, strict convexity of f implies differentiabil-
ity of the dual function, and allows the use of gradient-based optimization
algorithms that are considerably better-behaved than methods that can
deal with nondifferentiable cost. This motivates an interesting approach,
called proximal minimization, that artificially induces strict convexity by
adding a quadratic term to the cost function f , and uses iterations that
asymptotically eliminate the effect of this term.
Let us introduce an additional vector y, and consider the following
optimization problem
minimize f(x) +
1
2c
‖x − y‖2
subject to x ∈ F, y ∈ F,
Sec. 8.8 Algorithms and Approximations 387
where c is a positive scalar parameter. This problem is equivalent to the
original problem (8.31) because any one of its optimal solutions (x∗, y∗)
satisfies x∗ = y∗, so that x∗ must minimize f over F . The proximal mini-
mization algorithm consists of a sequence of alternate minimizations, first
with respect to x with y held fixed, and then with respect to y with x held
fixed. Thus, with an iteration-dependent parameter ck, the minimization
with respect to x yields
xk+1 = arg min
x∈F
{
f(x) +
1
2ck
‖x − yk‖2
}
,
and the subsequent minimization with respect to y yields
yk+1 = xk+1 = arg min
y∈F
1
2ck
‖xk+1 − y‖2.
Equivalently,
xk+1 = arg min
x∈F
{
f(x) +
1
2ck
‖x − xk‖2
}
.
It can be shown using the strict convexity of ‖x−xk‖2 that if F is a closed
set, the minimum above is uniquely attained for any xk, and the method
is well defined. Note an important property of the proximal minimiza-
tion algorithm: it preserves separability of the problem when it is already
present.
Figure 8.21 illustrates the convergence mechanism of the method.
Generally, it can be shown that if the penalty parameter sequence {ck}
is nondecreasing, the sequence {xk} converges to some optimal solution of
the original problem, provided there is at least one optimal solution for this
problem. As Fig. 8.21 indicates, it may be shown that if f is linear and F
is polyhedral, the convergence of the algorithm is finite. The method of ad-
justing the parameter ck has an important effect on the rate of convergence.
The tradeoff here is similar to the one for the augmented Lagrangian ap-
proach: the convergence of {xk} is accelerated if ck is increased at a faster
rate. On the other hand, large values of ck tend to diminish the effect of
the proximal term (1/2ck)‖x − xk‖2. In fact the proximal minimization
algorithm is closely related to the augmented Lagrangian algorithm, and
the convergence properties of these two algorithms are very similar. We
refer to the sources cited at the end of the chapter for a detailed analysis.
8.8.6 Smoothing
Generally, the analytical and algorithmic methodology for differentiable
problems is richer and more effective than the one for their nondifferentiable
counterparts. Thus, it is usually advantageous when the cost function f is
388 Nonlinear Network Optimization Chap. 8
0 xxk x*xk+1xk+2
f (x )
0 xxk xk+1
 = x*xk+2
f (x ): Linear
F
(a) (b)
||x- xk||21
2ck
_ + Constant
Figure 8.21: Graphical interpretation of the proximal minimization algorithm.
Given the current iterate xk, the graph of the function
− 1
2ck
‖x − xk‖2
is vertically translated by a constant until it just touches the graph of f . The
point of contact defines the new iterate xk+1. It can be seen from figure (a) that
as ck increases, the convergence becomes faster. In the case of a linear problem,
as in (b), the convergence is finite.
differentiable. On the other hand, in many problems, f is not differentiable
but is instead piecewise differentiable of the form
f(x) = max
i∈I
{
fi(x)
}
,
where I is a finite index set, and fi(x) is a differentiable convex function
for each i ∈ I. In this case, it is possible to transform the convex cost
network problem
minimize f(x)
subject to x ∈ F,
to the differentiable problem
minimize z
subject to x ∈ F,
fi(x) ≤ z, ∀ i ∈ I,
where z is a new artificial variable. Unfortunately, this type of trans-
formation may have an undesirable effect: by introducing additional side
constraints, it may adversely affect the network structure of the problem.
Sec. 8.8 Algorithms and Approximations 389
An alternative possibility for dealing with the piecewise differentiable
cost f(x) = maxi∈I
{
fi(x)
}
is to approximate it with the exponential-like
smooth cost
1
c
ln
{∑
i∈I
λiecfi(x)
}
, (8.32)
where c > 0 and λi are positive numbers with
∑
i∈I λi = 1 (see Bertsekas
[1982], Section 5.3). As c increases, the approximation becomes more ac-
curate. It is also possible to improve the approximation by iterating on
the multipliers λi (it turns out that this is related to the augmented La-
grangian approach, discussed earlier). Note that minimizing the smooth
approximation above is equivalent to minimizing the function∑
i∈I
λiecfi(x),
which is separable with respect to the index i. The exponential smoothing
approximation (8.32) is only one of many smoothing possibilities. We refer
to the literature cited at the end of the chapter for further discussion.
The smoothing approach described above can also be extended to the
case where f is the sum of several piecewise differentiable functions that
can be smoothed individually. An example is a separable cost function of
the form
f(x) =
∑
(i,j)∈A
fij(xij),
where each fij is a convex, piecewise differentiable function of xij . The
nondifferentiability of this particular cost function can also be alternatively
treated by introducing some extra arcs (see Exercise 8.4).
8.8.7 Transformations
In some network flow problems it is useful to consider a change of variables,
thereby altering the problem’s structure and making it more suitable for
special methods. We discuss some possibilities, which are applicable, how-
ever, only in the presence of special structure, such as multiple commodi-
ties.
A useful type of transformation is possible when the cost function f
depends on x through a vector y that is related to x by
y = ψ(x),
where ψ(·) is some known function; that is, for some function f̃ , we have
f(x) = f̃
(
ψ(x)
)
= f̃(y).
390 Nonlinear Network Optimization Chap. 8
The most common case is when ψ is linear, i.e.,
ψ(x) = Ax,
where A is a given matrix, but there are interesting cases where ψ is non-
linear.
As an example, in the multicommodity flow problems with commodi-
ties m = 1, . . . , M , discussed in Section 8.3, we saw that the cost function
often depends on the commodity flows xij(m) through the total arc flows
yij =
M∑
m=1
xij(m), ∀ (i, j) ∈ A.
The above relation expresses the vector of total arc flows y in terms of a
linear transformation of the vector of commodity flows x:
y = Ax,
where A is a suitable matrix. The problem of minimizing f(x) over the
feasible set F can be equivalently formulated in terms of y as
minimize f̃(y)
subject to y ∈ Y,
where the set Y is the image of F under the transformation A
Y = {y | y = Ax, x ∈ F}.
Even though Y is a complicated set that is given only implicitly through
the above equation, in specially structured problems one may still apply
particular types of algorithms in the space of the vector y (see the discussion
of the conditional gradient method for multicommodity flow problems at
the end of this section).
Another (nonlinear) transformation for multicommodity flow prob-
lems with nonnegativity constraints on the arc flows is based on expressing
the flows of the outgoing arcs of each node as fractions of the total outgoing
flow from the node. In particular, we introduce a variable φij(m) for each
arc (i, j) and commodity m, and we define a corresponding transformation
by
φij(m) =
xij(m)∑
{k|(i,k)∈A} xik(m)
, ∀ (i, j) ∈ A, m = 1, . . . , M.
[This transformation is valid for nodes i and commodities m such that the
outgoing flow
∑
{k|(i,k)∈A} xik(m) is positive; for i and m such that the
Sec. 8.8 Algorithms and Approximations 391
outgoing flow is 0, the definition of φij(m) does not matter.] A nice feature
of this transformation is that the variables φij(m) are subject to simple
constraints:
∑
{j|(i,j)∈A}
φij(m) = 1, ∀ i ∈ N , m = 1, . . . , M,
φij(m) ≥ 0, ∀ (i, j) ∈ A, m = 1, . . . , M.
There are multicommodity flow algorithms that iterate on the arc flow frac-
tions φij(m) and offer some advantages in certain practical settings related
to communications; see Gallager [1977], Bertsekas [1979b], Gafni [1979],
Bertsekas, Gafni, and Gallager [1984], Ephremides [1986], Ephremides and
Verdu [1989], and Powell, Berkkam, and Lustig [1993] for discussion, algo-
rithms, and analysis.
The next example of transformation for multicommodity flow prob-
lems is based on expressing arc flows in terms of path flows, and will be
discussed in some detail.
Path Flow Formulation for Multicommodity Flows
Our principal formulation of network flow problems so far has been in
terms of the arc flow variables. On the other hand, from the conformal
realization theorem (cf. Prop. 1.1), we know that every flow vector can be
decomposed into a collection of conforming simple path and cycle flows,
so we may consider using these path and cycle flows as the optimization
variables. This viewpoint is well-suited to a multicommodity flow problem,
where the mth commodity is associated with origin-destination (OD) pair
(im, jm) and supply rm; that is, node im is the only source and node jm is
the only sink of commodity m, and the corresponding supply to be routed
from im to jm is a given positive scalar rm.
For a path flow formulation to be applicable, we must introduce
some assumptions guaranteeing that there is an optimal solution x∗ =(
x∗(1), . . . , x∗(M)
)
that does not involve any cycles; that is, for each com-
modity m = 1, . . . , M , there is a conformal decomposition of x∗(m) that
consists only of simple path flows and no cycle flows. We thus assume that:
(a) The arc flows are constrained to be nonnegative.
(b) The cost function f is convex, continuously differentiable, and mono-
tonically nondecreasing with respect to the arc flows xij(m).
(c) Except for the conservation of flow and nonnegativity constraints,
there are no other constraints (such as capacity constraints on the
total arc flows or other side constraints).
392 Nonlinear Network Optimization Chap. 8
The problem has the form
minimize f(x)
subject to xij(m) ≥ 0, ∀ (i, j) ∈ A, m = 1, . . . , M,
∑
{j|(i,j)∈A}
xij(m)−
∑
{j|(j,i)∈A}
xji(m) =

rm if i = im,
−rm if i = jm,
0 otherwise,
∀ i ∈ N , m.
It can be seen, using the monotonicity property of the cost function
f , that an optimal flow vector can be constructed using only simple path
flows. We can thus reformulate the problem in terms of path flows. Let us
use the notation:
Pm: The set of all simple forward paths that start at the origin im and
end at the destination jm of the OD pair (im, jm).
hp: The portion of the supply rm assigned to a path p ∈ Pm.
Then the constraints of the problem are equivalently written as the M
simplex constraints∑
p∈Pm
hp = rm, hp ≥ 0, ∀ p ∈ Pm, m = 1, . . . , M. (8.33)
The arc flows can be expressed in terms of the path flows via the relation
xij(m) =
∑
all paths p∈Pm
containing (i,j)
hp. (8.34)
Let us denote abstractly the above linear transformation as
x = Ah, (8.35)
where h is the vector of path flows {hp}, and let us consider the cost
function in the transformed space of path flow vectors
D(h) = f(Ah). (8.36)
The problem then is to find a path flow vector h that minimizes D(h)
subject to the constraints (8.33). Thus the problem is transformed from
one with network constraints, to one with simplex constraints, which often
results in some simplification.
For a feasible set of path flows
h = {hp | p ∈ Pm, m = 1, . . . , M},
Sec. 8.8 Algorithms and Approximations 393
let
x =
(
x(1), . . . , x(M)
)
be the corresponding flow vector given by Eq. (8.34). Let us view the
partial derivative
dij(x, m) =
∂f(x)
∂xij(m)
as the length of the arc (i, j) for commodity m, and let us define the first
derivative length of a path p ∈ Pm with respect to x to be the sum of the
lengths of the arcs traversed by p:
dp(x) =
∑
all arcs (i,j)
traversed by p
dij(x, m). (8.37)
A key observation here is that, based on Eqs. (8.34)-(8.36), dp(x) is equal
to the partial derivative of D with respect to hp:
dp(x) =
∂D(h)
∂hp
. (8.38)
The following proposition gives an important shortest path-based condition
for optimality of a set of path flows.
Proposition 8.5: Under the preceding assumptions, a set of path
flows {h∗p | p ∈ Pm, m = 1, . . . , M} and the corresponding arc flow
vector x∗ are optimal if and only if every path p with h∗p > 0 has
minimum first derivative length with respect to x∗ over all paths of
the same OD pair as p; that is, for all m and all paths p ∈ Pm, we
have
h∗p > 0 ⇒ dp(x∗) ≤ dp(x∗), ∀ p ∈ Pm. (8.39)
.
The proof of the above proposition will be obtained by specializing
the optimality conditions of Prop. 8.1 to the case of simplex constraints.
This is done in the following proposition.
Proposition 8.6: (Optimization over a Simplex) Let f : n →
 be a continuously differentiable function of the vector x = (x1, . . . , xn),
and let F be the simplex
F =
{
x
∣∣∣ x ≥ 0, n∑
i=1
xi = r
}
,
394 Nonlinear Network Optimization Chap. 8
where r is a given positive scalar. Assume that f is convex over F .
Then, a vector x∗ ∈ F minimizes f over F if and only if
x∗i > 0 ⇒
∂f(x∗)
∂xi
≤ ∂f(x
∗)
∂xj
, ∀ j. (8.40)
Proof: The optimality condition (8.17) of Prop. 8.1 becomes
n∑
i=1
∂f(x∗)
∂xi
(xi − x∗i ) ≥ 0, ∀ xi ≥ 0 with
n∑
i=1
xi = r. (8.41)
Let x∗ be optimal, fix an index i for which x∗i > 0 and let j be any
other index. By using the feasible vector x = (x1, . . . , xn) with xi = 0,
xj = x∗j + x
∗
i , and xm = x
∗
m for all m 	= i, j in Eq. (8.41), we obtain(
∂f(x∗)
∂xj
− ∂f(x
∗)
∂xi
)
x∗i ≥ 0,
or equivalently
x∗i > 0 ⇒
∂f(x∗)
∂xi
≤ ∂f(x
∗)
∂xj
, ∀ j.
Conversely, suppose that x∗ belongs to F and satisfies Eq. (8.40). Let
ξ = min
i=1,...,n
∂f(x∗)
∂xi
.
For every x ∈ F , we have
∑n
i=1(xi − x∗i ) = 0, so that
0 =
n∑
i=1
ξ(xi − x∗i ) ≤
∑
{i|xi>x∗i }
∂f(x∗)
∂xi
(xi − x∗i ) +
∑
{i|xi<x∗i }
ξ(xi − x∗i ).
If i is such that xi < x∗i , we must have x
∗
i > 0 and, by condition (8.40),
ξ = ∂f(x∗)/∂xi. Thus ξ can be replaced by ∂f(x∗)/∂xi in the right-hand
side of the preceding inequality, thereby yielding Eq. (8.41), which by Prop.
8.1, implies that x∗ is optimal. Q.E.D.
Proposition 8.6 admits a straightforward generalization to the case
where F is a Cartesian product of several simplices. Then, there is a
separate condition of the form (8.40) for each simplex; that is, the condition
∂f(x∗)
∂xi
≤ ∂f(x
∗)
∂xj
Sec. 8.8 Algorithms and Approximations 395
holds for all i with x∗i > 0 and all j for which xj is constrained by the same
simplex as xi. We now apply this condition to the path flow formulation
of the multicommodity flow problem, i.e., the minimization of the cost
function D(h) of Eq. (8.36) subject to the simplex constraints of Eq. (8.33).
By using the partial derivative expression (8.38), we obtain the condition
(8.39) and the proof of Prop. 8.5.
Example 8.10. Routing in Data Networks Revisited
Let us consider the routing problem of Example 8.4 with OD pairs (im, jm)
and input flows rm, m = 1, . . . , M . Consider a separable cost function
f(x) =
∑
(i,j)
fij(yij),
where each fij :  →  is convex and continuously differentiable, and
yij =
M∑
m=1
xij(m)
is the total flow of arc (i, j). Assume also that there are no capacity con-
straints of the form yij ≤ cij (in practice, such constraints will always be
present, but they may be introduced implicitly in the cost function through
a barrier or a penalty function).
We can view the problem in terms of the path flow variables {hp}, and
we can apply Prop. 8.5. We see that optimal routing directs traffic exclusively
along paths that are shortest with respect to arc lengths that depend on the
flows carried by the arcs. In particular, a set of path flows is optimal if and
only if, for each OD pair, path flow is positive only on paths with a minimum
first derivative length.
Example 8.11. Traffic Assignment Revisited
Consider a path flow formulation of the traffic assignment problem of Example
8.5. The input rm of OD pair (im, jm) is to be divided among the set Pm
of simple paths starting at the origin node im and ending at the destination
node jm. Let hp denote the portion of rm carried by a path p ∈ Pm, and let
h denote the vector of all path flows.
Suppose now that for each arc (i, j), we are given a function tij(yij)
of the total arc flow yij of arc (i, j). This function models the time required
for traffic to travel from the start node to the end node of the arc (i, j). An
interesting problem is to find a path flow vector h∗ that consists of path flows
that are positive only on paths of minimum travel time. That is, for all paths
p ∈ Pm and all m, we require that
h∗p > 0 ⇒ tp(h∗) ≤ tp′(h∗), ∀ p′ ∈ Pm, m = 1, . . . , M,
396 Nonlinear Network Optimization Chap. 8
where tp(h), the travel time of path p, is defined as the sum of the travel
times of the arcs of the path,
tp(h) =
∑
all arcs (i,j)
on path p
tij(yij), ∀ p ∈ Pm, m = 1, . . . , M.
The preceding problem draws its validity from a hypothesis, called the
user-optimization principle, which asserts that traffic equilibrium is estab-
lished when each user of the network chooses, among all available paths, a
path of minimum travel time. Thus, assuming that the user-optimization
principle holds, a path flow vector h∗ that solves the problem also models
accurately the distribution of traffic through the network, and can be used
for traffic projections when planning modifications to the transportation net-
work.
We now observe that the minimum travel time hypothesis is identical
with the optimality condition of Prop. 8.5 if we introduce a separable cost
function
f(x) =
∑
(i,j)
fij(yij),
and we identify the travel time tij(yij) with the cost derivative ∂fij(yij)/∂yij .
It follows that we can solve the transportation problem by converting it to
the optimal routing problem of the preceding example using the identification
fij(yij) =
∫ yij
0
tij(ξ)dξ.
If we assume that tij is continuous and monotonically nondecreasing, as is
natural in a transportation context, it is straightforward to show that the
function fij as defined above is convex with derivative equal to tij . It follows
that a minimum first derivative length path is a path of minimum travel time.
Algorithms Based on the Path Flow Formulation
Aside from its analytical value, Prop. 8.5 provides the basis and a motiva-
tion for iterative feasible direction methods of the type discussed in Section
8.8.1. The idea is to calculate shortest paths corresponding to the current
iterate and then shift flow from the nonshortest paths to the shortest paths,
in an effort to reduce the violation of the optimality condition. Different
methods for shifting flow define different feasible direction methods.
As an example, consider the conditional gradient method applied to
the path flow formulation of minimizing the cost function D(h) of Eq. (8.36)
subject to the simplex constraints of Eq. (8.33). The typical iteration of
the method is as follows: Given the current feasible set of path flows {hp},
we find a shortest path (with respect to first derivative length) for each OD
Sec. 8.8 Algorithms and Approximations 397
pair. Let {hp} be the set of path flows that would result if all input rm for
each OD pair (im, jm) is routed along the corresponding shortest path:
hp =
{
rm if p is the shortest path for OD pair (im, jm),
0 if p is not the shortest path for any OD pair.
Let α∗ be the stepsize that minimizes D
(
h + α(h − h)
)
over all α ∈ [0, 1],
where D is the cost function in the transformed space of path flows [cf. Eq.
(8.36)]. The iteration defines the new set of path flows by
hp := hp + α∗(hp − hp), ∀ p ∈ Pm, m = 1, . . . , M.
Note that for each nonshortest path p we have hp = 0, so for such a
path the iteration takes the form
hp := (1 − α∗)hp.
Thus, at each iteration of the method, a fraction α∗ of the flow of each non-
shortest path is shifted to the shortest path of the corresponding OD pair.
The characteristic property here is that flow is shifted from the nonshort-
est paths in equal proportions. This distinguishes the conditional gradient
method from other feasible direction methods, which also shift flow from
the nonshortest paths to the shortest paths, however, they do so in gener-
ally unequal proportions.
Another interesting feasible direction method is the gradient projec-
tion method (Bertsekas [1980]; see also Bertsekas and Gafni [1982], [1983],
Gafni and Bertsekas [1984]). This method uses the following iteration for
the flows of the nonshortest paths (these flows also determine the flow on
the shortest paths, since we have
∑
p∈Pm hp = rm):
hp := max
{
0, hp + αH−1p (dp − dp)
}
, ∀ p ∈ Pm, p 	= p, m = 1, . . . , M,
where
p is the shortest path in Pm,
dp is the first derivative length of path p [cf. Eq. (8.37)],
α is a constant positive stepsize,
Hp is a positive path-dependent scaling factor.
In the case of a twice differentiable separable cost function
f(x) =
∑
(i,j)∈A
fij(yij),
where yij is the total flow of arc (i, j),
yij =
M∑
m=1
xij(m),
398 Nonlinear Network Optimization Chap. 8
there is an interesting definition of the scaling factor Hp based on the second
derivatives of the functions fij . It is given by
Hp =
∑
(i,j)∈Lp
∂2fij(yij)
∂y2ij
, (8.42)
where Lp is the set of arcs that belong to either p or the shortest path of
the OD pair corresponding to p, but not both.
When the scaling factor Hp is given by Eq. (8.42), it can be argued
that the gradient projection method works as a diagonal approximation
to a constrained form of Newton’s method, and typically converges faster
than the conditional gradient method. Some trial and error may be needed
to choose the constant stepsize α, which determines the portion of the flow
shifted from the nonshortest paths to the shortest paths (convergence re-
sults require that α should not exceed some unknown threshold). However,
the use of the second derivatives of fij facilitates the stepsize selection pro-
cess, and experience has shown that values of α near 1 typically work and
result in convergence (for further discussion, analysis, and computational
examples, see the book by Bertsekas and Gallager [1992], and the references
given there).
8.9 NOTES, SOURCES, AND EXERCISES
Nonlinear network problems have been approached in the literature from
two opposite ends: from the point of view of convex programming for prob-
lems with a continuous character, and from the point of view of combina-
torial optimization and integer programming for problems with a discrete
character. This is appropriate since the methodologies for continuous and
discrete problems are quite different. However, there are important con-
nections between the two types of problems, which we are trying to bring
out in our presentation. In particular, discrete network problems are often
solved by solving closely related continuous problems. Furthermore, convex
separable problems have a distinct combinatorial character, as evidenced by
the theory and algorithms of Chapters 2-7 for the single commodity-linear
cost case, and as will also be seen in Chapter 9.
Convex separable problems have a number of special properties that
do not readily generalize to nonseparable problems. We refer to Chapter 9
and to the references cited in that chapter.
There is a great variety of approaches for problems with side con-
straints. These include application of the simplex method for linear pro-
gramming and decomposition techniques to be discussed in Section 10.3.4.
The survey by Helgason and Kennington [1995] summarizes the simplex
Sec. 8.9 Notes, Sources, and Exercises 399
method as adapted to problems with side constraints and/or multiple com-
modities. The relaxation method of Chapter 6 has been extended to net-
work problems with side constraints by Tseng [1991].
The algorithmic and applications literature on multicommodity flow
problems is extensive. The surveys by Patricksson [1991], and by Florian
and Hearn [1995] focus primarily on transportation problems, and give a
large number of references. Applications in data communications, trans-
portation, and economics are described in the books by Bertsekas and Gal-
lager [1992], Sheffi [1985], and Nagurney [1993], respectively. These books
give many additional references. There is also a substantial literature on
the use of variational inequality models in the context of multicommod-
ity flows; see the survey by Florian and Hearn [1995], and the book by
Nagurney [1993]. Variational inequality problems cannot be transformed
to optimization problems, but they can be addressed using optimization
algorithms through the use of artificially constructed cost functions; see
Hearn, Lawphongpanish, and Nguyen [1984], Marcotte and Dussault [1987],
Marcotte and Guélat [1988], Auchmuty [1989], and Fukushima [1992].
For a broad discussion of models and applications of network prob-
lems with gains, we refer to Glover, Klingman, and Phillips [1992]. Sim-
plex methods for these problems are described in Dantzig [1963], Kenning-
ton and Helgason [1980], Elam, Glover, and Klingman [1979], Jensen and
Barnes [1980], Brown and McBride [1984], and Helgason and Kennington
[1995]. The first specialized implementation of the simplex method for net-
work problems with gains was given by Glover, Klingman, and Stutz [1973].
Analogs of the primal-dual method and the relaxation method for linear
network problems with gains are given by Jewell [1962], and by Bertsekas
and Tseng [1988a], respectively. The -relaxation method of Section 7.4
has been extended to linear and convex network problems with gains by
Tseng and Bertsekas [1996].
For material on feasible direction, cutting plane methods, and penalty
and augmented Lagrangians, see standard nonlinear programming text-
books, such as Bazaraa, Sherali, and Shetty [1993], Bertsekas [1995b], Gill,
Murray, and Wright [1981], and Luenberger [1984]. The conditional gradi-
ent method was first applied to multicommodity flow problems by Fratta,
Gerla, and Kleinrock [1973], and by Klessig [1974]. A related method that
aims to remedy the slow convergence of the conditional gradient method
is the, so-called, simplicial decomposition method ; see Cantor and Gerla
[1974], Holloway [1974], Lawphongpanich and Hearn [1984], [1986], Pang
and Yu [1984], Hearn, Lawphongpanish, and Ventura [1985], [1987], Lars-
son and Patricksson [1992], and Ventura and Hearn [1993]. For applica-
tions of various types of feasible direction methods to network flow prob-
lems, see Dafermos and Sparrow [1969], Leventhal, Nemhauser, and Trotter
[1973], Florian and Nguyen [1974], [1976], LeBlanc, Morlok, and Pierskalla
[1974], [1975], Nguyen [1974], Dafermos [1980], [1982], Gartner [1980a],
[1980b], Dembo and Klincewicz [1981], Bertsekas and Gafni [1982], [1983],
400 Nonlinear Network Optimization Chap. 8
Fukushima [1984a], [1984b], Gafni and Bertsekas [1984], Pang [1984], Es-
cudero [1985], LeBlanc, Helgason, and Boyce [1985], Marcotte [1985], Tsit-
siklis and Bertsekas [1986], Dembo [1987], Florian, Guélat, and Spiess
[1987], Dembo and Tulowitzki [1988], Nagurney [1988], Klincewitz [1989],
Arezki and Van Vliet [1990], Hearn and Lawphongpanich [1990], Toint and
Tuyttens [1990], Luo and Tseng [1994].
For algorithms for nonlinear network problems using piecewise lin-
ear approximations, see Meyer [1979], Rockafellar [1984], Minoux [1986b],
and Hochbaum and Shantikumar [1990]. The literature on interior point
methods is very extensive. Some representative works, which give many ad-
ditional references, are Nesterov and Nemirovskii [1994], Wright [1997], and
Ye [1997]. For applications of interior point methods to network optimiza-
tion, see Resende and Veiga [1993], and Resende and Pardalos [1996]. The
research monograph by Bertsekas [1982] focuses on penalty and augmented
Lagrangian methods, and includes a description and analysis of smoothing
methods (see also Bertsekas [1975a]). For recent work on smoothing, see
Pinar and Zenios [1992], [1993], [1994]. The proximal minimization algo-
rithm was proposed by Martinet [1970], and was extensively developed in a
more general setting by Rockafellar [1976]. For analysis of the finite termi-
nation property for linear problems, see Bertsekas [1975b], Bertsekas and
Tsitsiklis [1989], and Ferris [1991]. There has been much work on exten-
sions of the algorithm to cases where the proximal term is nonquadratic; see
Censor and Zenios [1992], Guler [1992], Teboulle [1992], Chen and Teboulle
[1993], Tseng and Bertsekas [1993], Eckstein [1994], Iusem, Svaiter, and
Teboulle [1994], and Kiwiel [1997a]. The book by Censor and Zenios [1997]
discusses several nonlinear network optimization techniques and a variety
of applications, with emphasis on parallel computation.
E X E R C I S E S
8.1
Consider the convex separable problem of Fig. 8.22, where each arc cost function
is fij(xij) = x
2
ij .
(a) Find the optimal solution and verify that it satisfies the optimality condi-
tion of Prop. 8.2.
(b) Derive and solve the dual problem based on the first formulation of Section
8.7.
Sec. 8.9 Notes, Sources, and Exercises 401
s   = 1
1
s   = 0
2
s   = -1
3
Flow range: [0,1]
Flow range: [0,1] Flow range: [0,1]
1
2
3
Figure 8.22: Problem for Exercise 8.1.
8.2
Consider a network with two nodes, 1 and 2, with supplies s1 = 1 and s2 = −1,
and three arcs/paths connecting 1 and 2, whose flows are denoted by h1, h2, and
h3. The problem is
minimize (h21) + 2(h
2
2) + (h
2
3)
subject to h1 + h2 + h3 = 1
h1, h2, h3 ≥ 0
(a) Show that the optimal solution is h∗1 = 2/3, h
∗
2 = 1/3, and h
∗
3 = 0.
(b) Write a computer program to carry out several iterations of the conditional
gradient method starting from h0 = (1/3, 1/3, 1/3). Do enough iterations
to demonstrate a clear trend in rate of convergence. Plot the successive
iterates on the simplex of feasible path flows.
8.3 (Dynamic Network Flows)
The arcs (i, j) of a graph carry flow xij(t) in time period t, where t = 1, . . . , T .
Each arc requires one time unit for traversal; that is, flow xij(t) sent from node i
to node j along arc (i, j) at time t arrives at node j at time t + 1. The difference
between the total flow departing and arriving at node i at time t = 2, . . . , T is
a given scalar si(t). The total flows departing from each node i at time 1 and
arriving at each node i at time T + 1 are also given.
(a) Formulate the minimization of a cost function f
(
x(1), . . . , x(T )
)
subject to
x(t) ∈ X(t), t = 1, . . . , T , where
x(t) =
{
xij(t) | (i, j) ∈ A
}
, t = 1, . . . , T,
and X(t) is a given set for each t, as a network optimization problem
involving a suitable graph that consists of multiple copies of the given
graph.
(b) Repeat part (a) for the more general case where traversal of an arc (i, j)
requires a given integer number of periods τij .
402 Nonlinear Network Optimization Chap. 8
8.4 (Piecewise Differentiable Arc Costs)
Consider the convex separable problem of Section 8.1, where each arc cost func-
tion fij is differentiable everywhere except at a finite number of points. Show
that the problem can be converted to a differentiable separable problem involving
one extra arc for each point of nondifferentiability.
8.5 (Constrained Max-Flow Problem)
Consider the max-flow problem of Chapter 3 with the exception that there is a sin-
gle side constraint of the form
∑
(i,j)
aijxij ≤ b, where aij and b are given scalars.
Relate this problem to the min-cost flow problem of minimizing
∑
(i,j)
aijxij sub-
ject to the constraint that the divergence out of the source (and into the sink) is
a given scalar r.
8.6 (Shortest Path Problems with Losses)
Consider the shortest path-like problem of Exercise 2.31 where a vehicle wants
to go on a forward path from an origin node 1 to a destination node t in a graph
with no forward cycles, and for each arc there is a given probability that the
vehicle will be destroyed in crossing the arc. Formulate the problem as a net-
work flow problem with gains. Provide conditions under which your formulation
makes sense when the graph has some forward cycles and the arc lengths are
nonnegative.
8.7 (Complementary Slackness – Constraint-Separable Problems)
Consider the constraint-separable convex network flow problem where
X =
{
x | bij ≤ xij ≤ cij , (i, j) ∈ A
}
,
and assume that f is continuously differentiable over the entire space and is
convex over the feasible set. Show that a vector x∗ is optimal if and only if there
exists a price vector p∗ such that
p∗i − p∗j ≤
∂f(x∗ij)
∂xij
, ∀ (i, j) ∈ A with x∗ij < cij ,
p∗i − p∗j ≥
∂f(x∗ij)
∂xij
, ∀ (i, j) ∈ A with bij < x∗ij .
Hint : Use Prop. 8.1 and the theory of Section 4.2.
Sec. 8.9 Notes, Sources, and Exercises 403
8.8 (Complementary Slackness – Networks with Gains)
Consider a constraint-separable convex network flow problem with gains:
minimize f(x)
subject to bij ≤ xij ≤ cij , ∀ (i, j) ∈ A,∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
gjixji = si, ∀ i ∈ N ,
and assume that f is continuously differentiable over the entire space and is
convex over the feasible set. Show that a vector x∗ is optimal if and only if there
exists a price vector p∗ such that
p∗i − gijp∗j ≤
∂f(x∗ij)
∂xij
, ∀ (i, j) ∈ A with x∗ij < cij ,
p∗i − gijp∗j ≥
∂f(x∗ij)
∂xij
, ∀ (i, j) ∈ A with bij < x∗ij .
Hint : Use Prop. 8.1 and the theory of Section 8.7.
8.9 (Error Bounds in the Conditional Gradient Method)
Consider the conditional gradient method applied to the minimization of a con-
tinuously differentiable function f : n →  over a convex and compact set F .
Assume that f is convex over F . Show that at each iteration k, we have
f(xk) + min
x∈F
∇f(xk)′(x − xk) ≤ min
x∈F
f(x) ≤ f(xk).
Show also that if xk converges to an optimal vector x∗, then the upper and lower
bounds above converge to f(x∗).
8.10
Consider the path flow formulation of the multicommodity flow problem of Sec-
tion 8.8.7. Assume that for each OD pair (im, jm) there is a “reverse” OD pair
(jm, im), and let cm > 0 be the ratio of the supplies of these two OD pairs.
Suppose that there is the restriction that the paths used by the OD pair (im, jm)
must be the reverse of the paths used by the OD pair (jm, im) and the ratios of
the corresponding flows must be cm; that is, if hp is the flow carried by a path
p from im to jm, then cmhp must be the flow of the reverse path of p, from jm
to im. Derive an optimality condition like the one of Prop. 8.5, and the forms of
the conditional gradient and gradient projection methods for this problem.
404 Nonlinear Network Optimization Chap. 8
8.11
Consider the case of a separable cost function
f(x) =
∑
(i,j)∈A
fij(xij),
where each fij is convex over the real line, and except for the conservation of
flow constraints, the only constraints are xij ≥ 0 for all arcs (i, j). Suppose that
fij(xij) ≥ 0 for all xij and that fij(0) = 0. Provide and justify an equivalent
path flow formulation of the problem.
8.12 (Convergence Proof of the Conditional Gradient Method)
Consider the minimization of a continuously differentiable function f : n → 
over a convex and compact set F . Assume that f is convex over F and the
gradient of f satisfies∥∥∇f(x) −∇f(y)∥∥ ≤ L‖x − y‖, ∀ x, y ∈ F,
where L is a positive constant.
(a) Show that if d is a descent direction at x, then
min
α∈[0,1]
f(x + αd) ≤ f(x) + δ,
where δ is the negative scalar given by
δ =
{
1
2∇f(x)
′d if ∇f(x)′d + L‖d‖2 < 0,
− |∇f(x)
′d|2
2LR2
otherwise,
where R is the diameter of F :
R = max
x,y∈F
‖x − y‖.
Hint : Let t be a scalar parameter and define g(t) = f(x + td). Using the
chain rule, we have ∂g(t)/∂t = d′∇f(x + td), and
f(x + d) − f(x) = g(1) − g(0)
=
∫ 1
0
∂g(t)
∂t
dt
=
∫ 1
0
d′∇f(x + td) dt
≤
∫ 1
0
d′∇f(x) dt +
∣∣∣∣∫ 1
0
d′
(
∇f(x + td) −∇f(x)
)
dt
∣∣∣∣
≤
∫ 1
0
d′∇f(x) dt +
∫ 1
0
‖d‖ · ‖∇f(x + td) −∇f(x)‖dt
≤ d′∇f(x) + ‖d‖
∫ 1
0
Lt‖d‖ dt
= d′∇f(x) + L
2
‖d‖2.
Sec. 8.9 Notes, Sources, and Exercises 405
Replace d with αd, and minimize over α ∈ [0, 1] both sides of this inequality.
(b) Consider the conditional gradient method
xk+1 = xk + αk(xk − xk),
where αk minimizes f
(
xk+α(xk−xk)
)
over α ∈ [0, 1]. Show that every limit
of the sequence {xk} is optimal. Hint : Argue that, if {xk} has a limit point
and δk corresponds to xk as in part (a), then δk → 0, and, therefore, also
∇f(xk)′(xk − xk) → 0. Take the limit in the relation ∇f(xk)′(xk − xk) ≤
∇f(xk)′(x − xk) for all x ∈ F .
8.13 (A Variant of the Conditional Gradient Method)
Consider the minimization of a continuously differentiable function f : n → 
over a closed and convex set F , and assume that f is convex over F and that the
gradient of f satisfies∥∥∇f(x) −∇f(y)∥∥ ≤ L‖x − y‖, ∀ x, y ∈ F,
where L is a positive constant. Consider the conditional gradient method
xk+1 = xk + αk(xk − xk)
where αk is given by
αk = min
{
1,
∇f(xk)′(xk − xk)
L‖xk − xk‖2
}
.
Show that every limit point of {xk} is optimal. Hint : Use the line of analysis of
Exercise 8.12.
9
Convex Separable Network
Problems
Contents
9.1. Convex Functions of a Single Variable
9.2. Optimality Conditions
9.3. Duality
9.4. Dual Function Differentiability
9.5. Algorithms for Differentiable Dual Problems
9.6. Auction Algorithms
9.6.1. The -Relaxation Method
9.6.2. Auction/Sequential Shortest Path Algorithm
9.7. Monotropic Programming
9.8. Notes, Sources, and Exercises
407
408 Convex Separable Network Problems Chap. 9
In this chapter, we focus on the convex separable problem introduced in
Section 8.1. It has the form
minimize
∑
(i,j)∈A
fij(xij) (9.1)
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N , (9.2)
xij ∈ Xij , ∀ (i, j) ∈ A, (9.3)
where x is a flow vector in a given directed graph (N ,A), si are given
supply scalars, Xij are nonempty intervals of scalars, and each function
fij : Xij →  is convex.
We have already discussed this problem in Chapter 8, and we now
provide a more extensive development of the associated optimality condi-
tions, duality theory, and algorithmic solution. We begin with a develop-
ment of the mathematical properties of convex functions of one variable,
such as the ones appearing in the separable cost function (9.1). We then
generalize, in Section 9.2, the optimality conditions of Section 8.6 so that
they do not require differentiability of the cost function. In Section 9.3,
we develop a duality theory that generalizes the one of Chapter 4 for the
minimum cost flow problem. In Section 9.4, we show that under special
circumstances (essentially, strict convexity of the primal cost function), the
dual cost function is differentiable.
We then proceed with the development of algorithms for convex sep-
arable problems. In Section 9.5, we discuss gradient-based algorithms for
solving the dual problem, when this problem is differentiable. In Section
9.6, we generalize and discuss in detail the auction algorithms of Chapter 7.
These algorithms can deal with nondifferentiabilities in the dual problem
and are also very efficient in practice. There is a solid theoretical basis for
this efficiency, as we show with a computational complexity analysis.
We close this chapter with the development of a far-reaching general-
ization of the separable convex network problem: we replace the conserva-
tion of flow constraints with arbitrary linear equality constraints, obtain-
ing a so-called monotropic programming problem. In this context, duality
is symmetric, and the distinction between a primal and a dual problem
disappear. Furthermore, the duality results are the sharpest possible. In
fact, monotropic programming problems form the largest class of nonlinear
programming problems with a duality theory that is as sharp as the one
for linear programs.
9.1 CONVEX FUNCTIONS OF A SINGLE VARIABLE
In this section, we introduce some mathematical properties of convex func-
tions of one variable, defined over an interval of the real line . We recall
Sec. 9.1 Convex Functions of a Single Variable 409
that in our terminology, an interval is a nonempty and convex subset of the
real line. The supremum (infimum) of an interval is called the right end-
point (the left endpoint , respectively). Thus, an interval is a set that has
one of the forms (a, b), (a, b], [a, b), [a, b], (−∞, b), (−∞, b], (a,∞), [a,∞),
(−∞,∞), where a and b are scalars. The left endpoint is a (or −∞) and
the right endpoint is b (or ∞). The interior of an interval is the set (a, b)
where a and b are the left and right endpoints, respectively.
Let f : X →  be a convex function defined on an interval X.† The
subset {
(x, γ) | x ∈ X, f(x) ≤ γ
}
of 2 is called the epigraph of f , and is convex if and only if f is convex.
It can be shown (as a consequence of convexity) that f is continuous at all
points in the interior of X; that is, limk→∞ f(xk) = f(x) for all sequences
{xk} ⊂ X converging to an interior point x of X. At an endpoint of X
that is included in X, f may or may not be continuous. A condition that
guarantees continuity of f over the entire interval X is that the epigraph of
f is a closed subset of 2. If this condition holds, we say that f is closed .
Throughout this chapter, we assume that the convex functions fij involved
in the separable problem (9.1)-(9.3) are closed . This assumption facilitates
the analysis and is practically always satisfied.
The right derivative of f at a point x ∈ X that is not the right
endpoint of X is defined by
f+(x) = lim
αk→0+
f(x + αk) − f(x)
αk
,
where the limit is taken over any positive sequence {αk} such that x+αk ∈
X for all k. If X contains its right endpoint b, we define f+(b) = ∞.
Similarly, the left derivative of f at a point x ∈ X that is not the left
endpoint of X is defined by
f−(x) = lim
αk→0+
f(x − αk) − f(x)
αk
,
† Much of the literature of convex analysis treats convex functions as ex-
tended real-valued functions, which are defined over the entire real line but
take the value ∞ outside their (effective) domain. In this format, a function
f : X →  that is convex over the convex interval X is represented by the
function f̂ :  → (−∞,∞] defined by
f̂(x) =
{
f(x) if x ∈ X,
∞ if x /∈ X.
There are notational advantages to this format, particularly for functions of sev-
eral variables, as it is not necessary to keep track of the domains of various
functions explicitly. It is simpler for our limited purposes, however, to maintain
the more common framework of real-valued functions.
410 Convex Separable Network Problems Chap. 9
where the limit is taken over any positive sequence {αk} such that x−αk ∈
X for all k. If X contains its left endpoint a, we define f−(a) = −∞. In the
degenerate case where X consists of a single point a, we define f−(a) = −∞
and f+(a) = ∞. Note that the only point of X where f+ may equal ∞
is the right endpoint (assuming it belongs to X), and the only point of X
where f− may equal −∞ is the left endpoint (assuming it belongs to X).
It can be shown, as a consequence of convexity, that the right and
left derivatives are monotonically nondecreasing and satisfy
f−(x) ≤ f+(x) ≤ f−(y) ≤ f+(y), ∀ x, y ∈ X with x < y. (9.4)
Furthermore, f− is left continuous (f+ is right continuous) over the interval
where it is finite. If f is differentiable at a point x ∈ X, we have
f−(x) = f+(x) = ∇f(x),
where ∇f(x) is the gradient of f at x. The right and left derivatives define
the subset
Γ =
{
(x, t) | x ∈ X, f−(x) ≤ t ≤ f+(x)
}
of 2, which is called the characteristic curve of f , and is illustrated in
Fig. 9.1.
Directional Derivatives of Separable Convex Functions
Consider now a general convex set F in n, and a function f : F → 
that is convex. The directional derivative f ′(x; y) of f at a vector x ∈ F in
the direction y is defined to be the right derivative of the convex function
f(x+αy) of the scalar α at α = 0 (this function is defined over the interval
of all α such that x + αy ∈ F ). In other words,
f ′(x; y) = lim
α→0+
f(x + αy) − f(x)
α
, (9.5)
where we use the convention f(x + αy) = ∞ if x + αy /∈ F . Note that a
vector x∗ ∈ F minimizes f over F if and only if
f ′(x∗; y) ≥ 0, ∀ y. (9.6)
Let us consider the special case of a separable function of the flow
vector x:
f(x) =
∑
(i,j)∈A
fij(xij),
where each fij is a closed convex function over an interval Xij . Then, by
applying the definition (9.5), we see that the directional derivative is given
by
f ′(x; y) =
∑
{(i,j)∈A|yij>0}
f+ij (xij)yij +
∑
{(i,j)∈A|yij<0}
f−ij (xij)yij , (9.7)
Sec. 9.1 Convex Functions of a Single Variable 411
0 x0
f(x) = cx
0 0x
t
x
X X
f(x) = (c/2)x2
X X
∇f(x) = cx
 c
0 0
X
xx
x
t
t
f(x)
X
b b
(a)
(b)
(c)
0 0
f(x) = max{φ1(x),φ2(x)}
X
φ1(x) φ2(x) ∇φ1(x)
X
γ γx
∇φ2(x)
x
(d)
t
Figure 9.1: Illustration of various convex functions f : X → 	 (on the left-hand
side) and their right and left derivatives, and characteristic curves
Γ =
{
(x, t) | x ∈ X, f−(x) ≤ t ≤ f+(x)
}
(on the right-hand side). In example (c), X contains its right endpoint b, but we
have f−(b) = f+(b) = ∞.
412 Convex Separable Network Problems Chap. 9
where f−ij (xij) and f
+
ij (xij) denote the left and the right derivative of fij
at an arc flow xij ∈ Xij . There is an ambiguity in the above equation
when f+ij (xij) = ∞ for some (i, j) with yij > 0 and f−ij (xij) = ∞ for some
(i, j) with yij < 0, in which case the sum ∞−∞ appears. We resolve this
ambiguity by adopting the convention
∞−∞ = ∞.
It can be shown by using the definition (9.5) that with this convention, the
directional derivative formula of Eq. (9.7) is correct even in cases where
the ambiguity arises. To see this, note that if f+ij (xij) = ∞ for some (i, j),
xij must be the right endpoint of the interval Xij , so that if in addition
yij > 0, it follows that xij + αyij /∈ Xij for all α > 0. Thus x + αy is
outside the domain of f for all α > 0, so that, according to our convention,
f(x + αy) = ∞ for all α > 0 and, from Eq. (9.5), f ′(x; y) = ∞.
9.2 OPTIMALITY CONDITIONS
In this and the next two sections, we discuss the main analytical aspects of
convex separable problems. The optimality conditions derived in Section
8.6 require differentiability of the cost function. However, the approach
used there can be extended to a nondifferentiable separable convex cost
by using directional differentiability. In particular, by arguing that the
directional derivative of f cannot be negative along any feasible direction
at x∗ [cf. Eq. (9.6)], we obtain a generalization of the nonnegative cycle
condition for optimality of Props. 1.2 and 8.2.
Proposition 9.1: (Nonnegative Cycle Condition) Consider the
separable convex network problem. A vector x∗ is optimal if and only
if x∗ is feasible and for every simple cycle C that is unblocked with
respect to x∗ there holds∑
(i,j)∈C+
f+ij (x
∗
ij) −
∑
(i,j)∈C−
f−ij (x
∗
ij) ≥ 0. (9.8)
Proof: Let x∗ be an optimal flow vector and let C be a simple cycle
that is unblocked with respect to x∗. Consider the flow vector d(C) with
components
dij(C) =
{
1 if (i, j) ∈ C+,
−1 if (i, j) ∈ C−,
0 otherwise.
(9.9)
Sec. 9.2 Optimality Conditions 413
Then d(C) is a feasible direction at x∗ and using Eq. (9.7), it is seen that
the directional derivative of f at x∗ in the direction d(C) is the left-hand
side of Eq. (9.8). Since x∗ is optimal, this directional derivative must be
nonnegative [cf. Eq. (9.6)].
Conversely, suppose that x∗ is feasible but not optimal. Let x be a
feasible flow vector with cost smaller that the one of x∗. Consider a confor-
mal decomposition of the circulation x−x∗ into simple cycles C1, . . . , CM ,
and the corresponding cycle flow vectors d(C1), . . . , d(CM ) as per Eq. (9.9):
x − x∗ =
M∑
m=1
γmd(Cm), γm > 0, m = 1, . . . , M. (9.10)
Using Eqs. (9.7) and (9.10), we see that the directional derivative of f in
the direction x − x∗ is given by
f ′(x∗; x − x∗) =
∑
{(i,j)|xij−x∗ij>0}
f+ij (x
∗
ij)(xij − x∗ij)
+
∑
{(i,j)|xij−x∗ij<0}
f−ij (x
∗
ij)(xij − x∗ij)
=
∑
{(i,j)|xij−x∗ij>0}
f+ij (x
∗
ij)
M∑
m=1
γmdij(Cm)
+
∑
{(i,j)|xij−x∗ij<0}
f−ij (x
∗
ij)
M∑
m=1
γmdij(Cm)
=
M∑
m=1
γm
( ∑
{(i,j)|dij(Cm)>0}
f+ij (x
∗
ij)dij(Cm)
+
∑
{(i,j)|dij(Cm)<0}
f−ij (x
∗
ij)dij(Cm)
)
=
M∑
m=1
γmf ′
(
x∗; d(Cm)
)
.
[The last equality holds using the definition (9.7) of a directional derivative.
The next-to-last inequality holds because for any arc (i, j) the sign of each
nonzero arc flow dij(Cm) is the same as the sign of xij − x∗ij , since the
decomposition is conformal.] Since f ′(x∗; x − x∗) < 0 and γm > 0 for all
m, we must have f ′
(
x∗; d(Cm)
)
< 0 for at least one m, or∑
(i,j)∈C+m
f+ij (x
∗
ij) −
∑
(i,j)∈C−m
f−ij (x
∗
ij) < 0.
Thus if Eq. (9.8) holds, x∗ must be optimal. Q.E.D.
414 Convex Separable Network Problems Chap. 9
9.3 DUALITY
As in earlier developments of duality, we obtain a dual problem by intro-
ducing a price pi for each node i and by forming the Lagrangian function
L(x, p) =
∑
(i,j)∈A
fij(xij) +
∑
i∈N
pi
 ∑
{j|(j,i)∈A}
xji −
∑
{j|(i,j)∈A}
xij + si

=
∑
(i,j)∈A
(
fij(xij) − (pi − pj)xij
)
+
∑
i∈N
sipi.
(9.11)
The dual function value q(p) at a price vector p is obtained by minimizing
L(x, p) over all x satisfying the constraint xij ∈ Xij . Thus,
q(p) = inf
x∈X
L(x, p) =
∑
(i,j)∈A
qij(pi − pj) +
∑
i∈N
sipi,
where
qij(pi − pj) = inf
xij∈Xij
{
fij(xij) − (pi − pj)xij
}
. (9.12)
The problem
maximize q(p)
subject to no constraint on p,
is referred to as the dual problem, while the original problem of minimizing
f subject to the conservation of flow constraints and x ∈ X is referred to
as the primal problem. The dual function is also referred to as the dual
cost function or dual cost , and the optimal value of the dual problem is
referred to as the optimal dual cost .
Note that qij is concave since it is the pointwise infimum of linear
functions [the epigraph of −qij is a convex set, since it is the intersection
of the epigraphs of the linear functions (pi − pj)xij − fij(xij) as xij ranges
over Xij ]. If Xij is a compact set, then since fij is assumed closed and
hence continuous over Xij , the infimum in the definition (9.12) of qij is
attained (by Weierstrass’ theorem), and it follows that qij is real-valued;
that is, q(p) is a real number for all p. If Xij is not compact, it is possible
that qij is not real-valued. Thus the dual problem embodies the implicit
constraint p ∈ Q, where Q is the “effective domain” of q given by
Q =
{
p | q(p) > −∞
}
.
Sec. 9.3 Duality 415
0 x
Slope = α
β/α
0 α
- β
q(t ) = {− β  if  t = α− ∞  if  t ≠ αf(x) = αx - β
0 x
{0    if  |t| ≤ 1− ∞   if  |t| > 1f(x) = |x| 
0 x
0
q(t ) = -  (1/2c)t2f(x) = (c/2)x2
X = (- ∞, ∞)
X = (- ∞, ∞)
X = (- ∞, ∞)
t
t
0
1-1
t
q(t ) = 
Figure 9.2: Illustration of primal and dual arc cost function pairs. Points where
the primal function is nondifferentiable correspond to linear segments of the dual
function.
We consequently say that a price vector p is feasible if q(p) > −∞. The
dual problem is said to be infeasible if there is no feasible price vector. The
form of qij is illustrated in Fig. 9.2.†
Our objective is to generalize the duality theorems given in Chapter 4
for the minimum cost flow cost problem. For this, we must first generalize
† The relation between the primal and dual arc cost functions fij and qij
is a special case of a conjugacy relation that is central in the theory of convex
functions (see e.g., Rockafellar [1970], [1984]). There is a rich theory around this
relation. Here, we will prove only those facts about conjugacy that we will need
in our analysis.
416 Convex Separable Network Problems Chap. 9
the conditions for complementary slackness.
Definition 9.1: A flow-price vector pair (x, p) is said to satisfy com-
plementary slackness (CS for short) if for all arcs (i, j), we have xij ∈
Xij and
f−ij (xij) ≤ pi − pj ≤ f+ij (xij).
Thus a pair (x, p) satisfies CS if for every arc (i, j), the pair (xij , pi −
pj) lies on the characteristic curve of the function fij (see Fig. 9.3). Note
that an equivalent definition of CS is that xij attains the infimum in the
definition of qij for all arcs (i, j):
fij(xij) − (pi − pj)xij = min
zij∈Xij
{
fij(zij) − (pi − pj)zij
}
.
It can be seen that these conditions generalize the corresponding CS con-
ditions for the minimum cost flow problem.
0 0
fij(xij) = max{φ1(xij),φ2(xij)}
Xij
φ1(xij) φ2(xij)
∇φ1(xij)
Xij
γ γ
xij xij
pi  - pj
Slope c
c
Figure 9.3: Illustration of CS. The pairs (xij , pi − pj) must lie on the corre-
sponding characteristic curves
Γij =
{
(xij , tij) | xij ∈ Xij , f−ij (xij) ≤ tij ≤ f
+
ij (xij)
}
,
shown in the right-hand side.
We are now ready to derive the basic duality results for separable
problems.
Proposition 9.2: (Complementary Slackness Theorem) A fea-
sible flow vector x∗ and a price vector p∗ satisfy CS if and only if x∗
and p∗ are optimal primal and dual solutions, respectively, and the
optimal primal and dual costs are equal.
Sec. 9.3 Duality 417
Proof: We first show that for any feasible flow vector x and any price
vector p, the primal cost of x is no less than the dual cost of p. Indeed,
using the definition of q(p) and L(x, p), we have
q(p) ≤ L(x, p)
=
∑
(i,j)∈A
fij(xij) +
∑
i∈N
pi
si − ∑
{j|(i,j)∈A}
xij +
∑
{j|(j,i)∈A}
xji

=
∑
(i,j)∈A
fij(xij),
(9.13)
where the last equality follows from the feasibility of x.
If x∗ is feasible and satisfies CS together with p∗, we have by the
definition of q
q(p∗) = inf
x
{
L(x, p∗) | xij ∈ Xij , (i, j) ∈ A
}
= L(x∗, p∗)
=
∑
(i,j)∈A
fij(x∗ij) +
∑
i∈N
p∗i
si − ∑
{j|(i,j)∈A}
x∗ij +
∑
{j|(j,i)∈A}
x∗ji

=
∑
(i,j)∈A
fij(x∗ij),
where the last equality follows from the feasibility of x∗, and the second
equality holds because (x∗, p∗) satisfies CS if and only if
fij(x∗ij) − (p∗i − p∗j )x∗ij = min
xij∈Xij
{
fij(xij) − (p∗i − p∗j )xij
}
, ∀ (i, j) ∈ A,
and L(x∗, p∗) can be written as in Eq. (9.11). Therefore, x∗ attains the
minimum of the primal cost on the right-hand side of Eq. (9.13), and p∗
attains the maximum of q(p) on the left-hand side of Eq. (9.13), while the
optimal primal and dual costs are equal.
Conversely, suppose that x∗ and p∗ are optimal flow and price vectors
for the primal and dual problems, respectively, and the two optimal costs
are equal; that is,
q(p∗) =
∑
(i,j)∈A
fij(x∗ij).
We have by definition
q(p∗) = inf
x
{
L(x, p∗) | xij ∈ Xij , (i, j) ∈ A
}
,
and also, using the Lagrangian expression (9.11) and the feasibility of x∗,∑
(i,j)∈A
fij(x∗ij) = L(x∗, p∗).
418 Convex Separable Network Problems Chap. 9
Combining the last three equations, we obtain
L(x∗, p∗) = min
x
{
L(x, p∗) | xij ∈ Xij , (i, j) ∈ A
}
.
Using the Lagrangian expression (9.11), it follows that for all arcs (i, j), we
have
fij(x∗ij) − (p∗i − p∗j )x∗ij = min
xij∈Xij
{
fij(xij) − (p∗i − p∗j )xij
}
.
This is equivalent to the pair (x∗, p∗) satisfying CS. Q.E.D.
An important question, which is left open by Prop. 9.2, is whether
there exists a price vector that satisfies CS together with an optimal flow
vector. For the minimum cost flow problem, this is always true, as we
have seen in Chapter 4 (Prop. 4.2). However, answering this question
for convex but nonlinear problems requires some qualifying condition of
the type assumed in the duality results of Chapter 8 (cf. Prop. 8.3). We
introduce such a condition in the following definition.
Definition 9.2: (Regularity) A flow vector x is called regular if for
all arcs (i, j), we have xij ∈ Xij and
f−ij (xij) < ∞, −∞ < f+ij (xij).
It is quite unusual for a flow vector x not to be regular. For this
to happen, there must exist an arc flow xij that lies at the right (left)
endpoint of the corresponding constraint interval Xij while both the left
and the right slopes of fij at that endpoint are ∞ (or −∞, respectively)
[see Fig. 9.1(c) for an example]. In particular, if xij belongs to the interior
of Xij for all arcs (i, j), then x is regular. Furthermore, all flow vectors are
regular if each fij is the restriction to the interval Xij of some function that
is convex over the entire real line, such as for example a linear function.
While nonregularity is unusual for a feasible flow vector, it is far more
rare for an optimal flow vector. In particular, we claim that if there exists
at least one regular feasible solution, all optimal solutions must be regular .
To show this, note that if x∗ is an optimal solution and x is another feasible
solution, we have
x∗ij < xij ⇒ f+ij (x∗ij) < ∞,
since if x∗ij < xij , then x
∗
ij cannot be the right endpoint of the interval Xij .
Similarly, we have
xij < x∗ij ⇒ f−ij (x∗ij) > −∞.
Sec. 9.3 Duality 419
It follows from the preceding two relations that
f+ij (x
∗
ij)(xij−x∗ij) < ∞, f−ij (x∗ij)(xij−x∗ij) < ∞, ∀ (i, j) ∈ A. (9.14)
Now if x is regular and x∗ is not regular but optimal, there must exist an
arc (i, j) such that either (a) f−ij (x
∗
ij) = ∞, or (b) f+ij (x∗ij) = −∞. In case
(a), x∗ij must be the right endpoint of Xij and xij < x
∗
ij (since x is regular).
Hence the product f−ij (x
∗
ij)(xij − x∗ij) is −∞, and in view of Eq. (9.14), we
have
f ′(x∗; x − x∗) = −∞,
contradicting the optimality of x∗. We similarly obtain a contradiction in
case (b), completing the proof that regularity of at least one feasible flow
vector implies regularity of every optimal flow vector. We use this to show
the following proposition.
Proposition 9.3: Suppose that there exists at least one primal fea-
sible solution that is regular. Then, if x∗ is an optimal solution of
the primal problem, there exists an optimal solution p∗ of the dual
problem that satisfies CS together with x∗.
Proof: By Prop. 9.1, for every simple cycle C that is unblocked with
respect to x∗ there holds∑
(i,j)∈C+
f+ij (x
∗
ij) −
∑
(i,j)∈C−
f−ij (x
∗
ij) ≥ 0.
The discussion preceding the present proposition, implies that x∗ must be
regular. Using this fact, it is seen that the assumptions for the use of the
feasible differential theorem (Exercise 5.11 in Chapter 5) are fulfilled with
a+ij = f
+
ij (x
∗
ij) and a
−
ij = f
−
ij (x
∗
ij). Using the conclusion of this theorem, we
can assert that there exists a price vector p∗ satisfying
f−ij (xij) ≤ p∗i − p∗j ≤ f+ij (xij),
for all arcs (i, j). Thus p∗ satisfies CS together with x∗. Q.E.D.
Figure 9.4 gives an example where the assertion of Prop. 9.3 does not
hold in the absence of a regular feasible solution.
An important question, which is left open by Props. 9.2 and 9.3,
relates to the equality of the optimal primal and dual costs in the absence of
an optimal primal solution that is regular. Generally, for convex programs,
it is possible that the optimal primal cost is strictly greater that the optimal
dual cost, in which case we say that there is a duality gap. Using the
420 Convex Separable Network Problems Chap. 9
x21 < 0
0 < x12
1 2
0
f21(x21)
x21
0
f12(x12)
x12
Figure 9.4: An example of a problem where there is no regular primal feasible
solution, and the dual problem has no optimal solution (cf. Prop. 9.3). The primal
problem is
minimize f12(x12) + f21(x21)
subject to x12 = x21, 0 ≤ x12 < ∞, −∞ < x21 ≤ 0,
where
f12(x12) = −
√
x12, x12 ∈ [0,∞),
f21(x21) = −
√
−x21, x21 ∈ (−∞, 0].
The dual arc functions can be calculated to be
q12(t12) = inf
0≤x12<∞
{
−√x12 − t12x12
}
=
{
1
4t12
if t12 < 0,
−∞ otherwise,
and
q21(t21) = inf
−∞<x21≤0
{
−
√
−x21 − t21x21
}
=
{
− 1
4t21
if t21 > 0,
−∞ otherwise.
The only primal feasible solution is the zero flow vector, which is nonregular. The
optimal primal cost is 0. The dual problem is to maximize
1
4(p1 − p2)
− 1
4(p2 − p1)
over all (p1, p2) with p1 < p2, and has no optimal solution. The dual optimal cost
is 0. Note that the optimal primal and dual costs are equal, consistently with the
following Prop. 9.4.
Sec. 9.3 Duality 421
machinery of the simplex method, we showed that for linear cost problems,
this cannot happen (see Props. 4.2 and 5.8). However, the equality of the
optimal primal and dual costs is a characteristic property of linear programs
and the corresponding proof methods do not easily generalize to the case
of a general convex cost function. It is thus somewhat unexpected that for
the separable problem of this chapter the optimal primal and dual costs are
equal under comparable assumptions to those for linear programs. This is
a remarkable result due to Minty [1960] and Rockafellar ([1967] or [1970]
or [1984]), and requires a fairly sophisticated proof, which will be given in
Section 9.7. Exercise 9.1 outlines the proof of a weaker result, which states
that if the primal problem is feasible and the intervals Xij are compact,
then the optimal primal and dual costs are equal, even though the optimal
primal solutions may not be regular and the dual problem may not have
an optimal solution.
Proposition 9.4: (Duality Theorem for Separable Problems)
If there exists at least one feasible solution to the primal problem, or
at least one feasible solution to the dual problem, the optimal primal
and dual costs are equal.
Note that part of the assertion of Prop. 9.4 is that if the primal
problem is feasible but unbounded, then the dual problem is infeasible (the
optimal costs of both problems are equal to −∞), and that if the dual
problem is feasible but unbounded, the primal problem is infeasible (the
optimal costs of both problems are equal to ∞).
Duality and the Equilibrium Problem
We can use duality and CS to introduce a problem, which is referred to
as the equilibrium problem. The name stems from the association with
some classical problems of finding equilibrium solutions to various physical
systems, as we will explain shortly.
Network Equilibrium Problem
Find a flow-price pair (x, p) such that x satisfies the conservation of
flow equations, and for each arc (i, j), the pair (xij , pi −pj) lies on the
characteristic curve
Γij =
{
(xij , tij) | xij ∈ Xij , f−ij (xij) ≤ tij ≤ f+ij (xij)
}
. (9.15)
422 Convex Separable Network Problems Chap. 9
Thus, the pair (x, p) is an equilibrium solution if and only if x is
feasible and (x, p) satisfies CS. We have the following result:
Proposition 9.5: (Network Equilibrium Theorem) A flow-price
pair (x∗, p∗) solves the equilibrium problem if and only if x∗ and p∗
are optimal primal and dual solutions, respectively
Proof: If (x∗, p∗) solve the equilibrium problem, then (x∗, p∗) satisfy CS,
so by the forward part of Prop. 9.2, x∗ is primal optimal and p∗ is dual
optimal. Conversely, if x∗ is primal optimal and p∗ is dual optimal, then
x∗ is primal feasible, so by Prop. 9.4, the optimal primal and dual costs
are equal. It follows using the reverse part of Prop. 9.2 that x∗ and p∗
satisfy CS, and since x∗ is feasible, they also solve the equilibrium problem.
Q.E.D.
We provide some examples of network equilibrium problems and their
connections to separable network optimization.
Example 9.1. Electrical Networks
Let us view the given graph as an electric circuit, where xij and pi represent
the current of arc (i, j) and the voltage of node i, respectively. Let us assume
that all the supply scalars si are 0. Then, the conservation of flow equations
become Kirchhoff’s current law (all currents into a node add to 0). Each
characteristic curve Γij [cf. Eq. (9.15)] defines the locus for current-voltage
differential pairs (xij , pi−pj), so it corresponds to Ohm’s law. Different types
of curves Γij define different type of electrical elements. For example a linear
curve
Γij =
{
(xij , pi − pj) | pi − pj = Eij + Rijxij
}
(9.16)
corresponds to an arc consisting of a linear resistor with resistance Rij plus
a voltage source of value Eij . A curve
Γij =
{
(xij , pi − pj) | xij = I
}
,
where I is a constant corresponds to a current source of value I. Nonlin-
ear electric circuit branches, such as for example diodes, can similarly be
represented, as long as the corresponding curves Γij have the monotonicity
properties that characterize the directional derivatives of convex functions of
one variable.
Note that Prop. 9.5 asserts that the current-voltage pairs of the elec-
tric circuit that satisfy Kirchhoff’s and Ohm’s laws are exactly the optimal
flow-price pairs of the corresponding optimization problem. In the special
case of a linear resistive circuit with voltage sources, which has characteristic
curves of the form (9.16), the corresponding optimization problem involves
Sec. 9.3 Duality 423
the quadratic cost function∑
(i,j)∈A
(
Eijxij +
1
2
Rijx
2
ij
)
.
This function has an electric energy interpretation. We thus obtain a re-
sult known since Maxwell’s time, namely that the current-voltage pairs that
solve a linear resistive circuit solve a minimum energy problem. Proposition
9.5 provides a generalization of this result that holds for nonlinear resistive
circuits as well.
Example 9.2. Hydraulic Networks
Networks of pipes or other conduits carrying an incompressible fluid admit a
very similar interpretation to the one given above for electric networks. Here
xij correspond to the fluid flow rates through the pipes (i, j), which must
satisfy a conservation of flow equation at each node. Also pi corresponds to
pressure head at node i, that is, to the level that the fluid would rise in an
open pipe located at node i. The pressure differential pi − pj of pipe (i, j)
satisfies together with the flow xij a “resistance” relation, which is expressed
by the curve Γij .
Subnetworks as Black Boxes – Sensitivity
In many applications, it is convenient to be able to aggregate a subnetwork
of the given graph into a single arc for the purpose of optimization of the
remainder of the network. The subnetwork can then be treated as a “black
box” whose impact on the problem depends only on the characteristics of
the aggregate arc. In this way, a complicated subnetwork may be repre-
sented by its “input-output” behavior rather than by its detailed internal
structure.
Mathematically, the simplest case of a black box representation can
be obtained through the problem illustrated in Fig. 9.5(a). This is the
special case of the convex separable problem where the divergences of all
the nodes are required to be 0, except for two distinguished nodes A and
B, whose divergences are required to be s and −s, respectively. Let us
denote by F (s) the feasible set of the problem, i.e., the set of flow vectors
x such that ∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = 0, ∀ i 	= A, B,
∑
{j|(A,j)∈A}
xAj −
∑
{j|(j,A)∈A}
xjA = s,
424 Convex Separable Network Problems Chap. 9∑
{j|(B,j)∈A}
xBj −
∑
{j|(j,B)∈A}
xjB = −s,
xij ∈ Xij , ∀ (i, j) ∈ A.
Let us also denote by V (s) the corresponding optimal cost,
V (s) = inf
x∈F (s)
∑
(i,j)∈A
fij(xij). (9.17)
A key fact, to be shown shortly, is that V (s) is a convex function of s.
A B
A B
(a)
(b)
s
yA = s yB = -s
Figure 9.5: Problem framework for rep-
resentation of a subnetwork as a black
box. In (a), all nodes have divergence
0, except for A and B, which have di-
vergence s and −s, respectively. In (b),
an additional arc (B, A) with flow s has
been connected to the network of (a),
and all nodes have divergence 0.
Suppose now that we want to solve a variant of the problem where
s is instead the flow through an arc with start node B and end node A,
with a given flow range XBA, and with a given cost function G(s) [see Fig.
9.5(b)]. This is the problem
minimize G(s) +
∑
(i,j)∈A
fij(xij)
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = 0, ∀ i 	= A, B,
∑
{j|(A,j)∈A}
xAj −
∑
{j|(j,A)∈A}
xjA = s,
∑
{j|(B,j)∈A}
xBj −
∑
{j|(j,B)∈A}
xjB = −s,
s ∈ XBA, xij ∈ Xij , ∀ (i, j) ∈ A.
Sec. 9.3 Duality 425
Then, knowing V (s), the problem is reduced to the one-dimensional prob-
lem
minimize G(s) + V (s)
subject to s ∈ XBA
and can be easily solved for practically any choice of cost function G(s).
To see that the function V of Eq. (9.17) is convex, we note that for
all s for which the above problem is feasible [i.e., V (s) < ∞], by Prop. 9.4,
V (s) is equal to the dual optimal cost
V (s) = sup
p
Qp(s), (9.18)
where for each fixed p, Qp(s) is the linear function given by
Qp(s) = (pA − pB)s +
∑
(i,j)∈A
qij(pi − pj).
Thus, V (s) is the pointwise supremum of a collection of linear functions,
and must be convex (the epigraph of V is convex because it is the inter-
section of the epigraphs of Qp, which are halfspaces). To be able to apply
the theory of this chapter, it is also necessary that V be a closed function,
which can also be easily shown (the epigraph of V is closed because the
epigraphs of Qp are closed).
Let us now use the preceding ideas to derive a famous theorem from
electrical engineering.†
Example 9.3. Thevenin’s Theorem
Thevenin’s theorem is a classical result of electric circuit theory that often
provides computational and conceptual simplification of the solution of elec-
tric network problems involving linear resistive elements. The theorem shows
that, when viewed from two given terminals, a circuit can be described by a
single branch involving just two electrical elements, a voltage source and a
resistance (see Fig. 9.6). These elements can be viewed as sensitivity param-
eters, characterizing how the current across the given terminals varies as a
function of the external load to the terminals.
Mathematically, Thevenin’s theorem is an application of the black box
representation derived above. In particular, the equilibrium problem for a
linear resistive network involves the minimum energy problem with the arc
cost functions
fij(xij) = Eijxij +
1
2
Rijx
2
ij ,
† Leon Thevenin (1857-1926) was a French telegraph engineer. He formulated
his theorem at the age of 26. His discovery met initially with skepticism and
controversy within the engineering establishment of the time. Eventually the
theorem was published in 1883. A brief biography of Thevenin together with an
account of the development of his theorem is given by Suchet [1949].
426 Convex Separable Network Problems Chap. 9
A B
L
A B
L
E R
Linear Resistive Circuit
Figure 9.6: Illustration of Thevenin’s theorem. A linear resistive circuit acts
on a load connected to two of its terminals A and B like a series connection of
a voltage source E and a resistance R. The parameters E and R depend only
on the circuit and not on the load, so if in particular the load is a resistance
L, the current drawn by the load is
I =
E
L + R
.
The parameters E and R can be obtained by solving the circuit for two dif-
ferent values of L.
(cf. Example 9.1). The corresponding dual function qij can be calculated to
be
qij(pi − pj) = min
xij
{
Eijxij +
1
2
Rijx
2
ij − (pi − pj)xij
}
= −1
2
R−1ij
(
Eij − pi + pj
)2
.
It can be shown [by explicitly carrying out the maximization in Eq. (9.18)]
that the function V (s) is quadratic and has the form
V (s) = Es +
1
2
Rs2,
for suitable scalars E and R. These scalars represent the voltage source and
the resistance of the Thevenin equivalent branch (cf. Fig. 9.6). For further
analysis and algorithms relating to this example, see Bertsekas [1996].
9.4 DUAL FUNCTION DIFFERENTIABILITY
Generally, the dual function q is concave, but not necessarily differentiable,
or even real-valued. However, q can be shown to be differentiable in the
special case where the infimum in the definition of the dual arc cost function
qij(tij) = inf
xij∈Xij
{
fij(xij) − tijxij
}
Sec. 9.4 Dual Function Differentiability 427
is attained for all tij and fij is strictly convex over Xij , that is,
fij
(
αxij + (1 − α)yij
)
< αfij(xij) + (1 − α)fij(yij), ∀ α ∈ (0, 1),
for all xij , yij ∈ Xij with xij 	= yij . We will prove this property and derive
the form of the gradient ∇q. We first need the following result, which
establishes various relations between fij and qij . These relations are basic
in the theory of conjugate functions (see e.g., Rockafellar [1970]).
Proposition 9.6: Let f : X →  be a closed convex function over
an interval X, and let
q(t) = inf
x∈X
{
f(x) − tx
}
. (9.19)
(a) We have
sup
t
{
q(t) + tx
}
=
{
f(x) if x ∈ X,
∞ otherwise, (9.20)
and the following statements are equivalent for any two scalars
x ∈ X and t ∈ :
(1) tx = f(x) − q(t).
(2) x attains the infimum in Eq. (9.19).
(3) t attains the supremum in Eq. (9.20).
(b) Assume that for each t ∈  the infimum in equation (9.19) is
uniquely attained by a scalar denoted x(t). Then q is real-valued
and differentiable, and we have
∇q(t) = −x(t), ∀ t ∈ .
Proof: (a) Figure 9.7 proves Eq. (9.20). From Eqs. (9.19) and (9.20), we
see that statements (2) and (3) are equivalent with statement (1). There-
fore, (2) and (3) are also equivalent.
(b) Since the infimum in equation (9.19) is attained for each t, q is a real-
valued concave function. Let us fix t, and let q+(t) and q−(t) be the right
and left directional derivatives of q, respectively, at t. A scalar y satisfies
q+(t) ≤ y ≤ q−(t), (9.21)
if and only if t maximizes q(ξ) − ξy over all ξ, which is true [by the equiv-
alence of (2) and (3)] if and only if −y attains the minimum in Eq. (9.19).
428 Convex Separable Network Problems Chap. 9
In view of our assumption that this minimum is (the unique) scalar x(t),
it follows that −x(t) is the unique scalar y satisfying Eq. (9.21), and must
be equal to the gradient ∇q(t). Q.E.D.
0
X
x
f(x ) Slope = t
q(t ) = inf{f(x ) - tx | x  ∈X}
(a)
0 x
Slope = t
q(t )
q(t ) + tx
(c)
X
0 x
f(x )
Slope = tq(t )
(b)
q(t ) + tx
X
Figure 9.7: A geometrical proof that
sup
t
{
q(t) + tx
}
is equal to f(x) if x ∈ X and is equal to
∞ otherwise [cf. Eq. (9.20)]. Our proof
assumes that the reader is familiar with
basic facts about hyperplanes and sup-
port properties of convex sets in two di-
mensions.
For any t, q(t) is obtained by con-
structing a supporting line with slope t
to the convex set{
(x, γ) | x ∈ X, f(x) ≤ γ
}
,
(the epigraph of f), and by obtaining the
point where this line intercepts the ver-
tical axis.
For a given x ∈ X, q(t) + tx is
obtained by intercepting the vertical line
passing through
(
x, f(x)
)
with the line
of slope t that supports the epigraph of f .
This point of intercept cannot lie higher
than f(x), and with proper choice of t
lies exactly at f(x). This proves that
sup
t
{
q(t) + tx
}
= f(x)
for x ∈ X.
For x /∈ X, the construction given
shows that with proper choice of t, the
value of q(t)+ tx can be made arbitrarily
large. Hence
sup
t
{
q(t) + tx
}
= ∞.
The figure also illustrates the equivalence
of statements (1)-(3) in Prop. 9.6.
Sec. 9.4 Dual Function Differentiability 429
Assume now that in the convex separable network problem, the func-
tions fij and the intervals Xij are such that the infimum in the equation
qij(tij) = inf
xij∈Xij
{
fij(xij) − tijxij
}
is uniquely attained for each scalar tij . This is true for example if each Xij
is compact and fij is strictly convex over Xij . Let us derive the gradient
of the dual function at a price vector p. We have for all i ∈ N
∂q(p)
∂pi
=
∑
(m,n)∈A
∂qmn(pm − pn)
∂pi
+ si
= −
∑
{j|(j,i)∈A}
∇qji(pj − pi) +
∑
{j|(i,j)∈A}
∇qij(pi − pj) + si
=
∑
{j|(j,i)∈A}
xji(p) −
∑
{j|(i,j)∈A}
xij(p) + si,
(9.22)
where the last equality holds because by Prop. 9.6, the derivatives ∇qij(pi−
pj) are equal to minus the unique arc flows xij(p) satisfying CS together
with p. The last expression in Eq. (9.22) can be recognized as the surplus
of node i. Thus we obtain
∂q(p)
∂pi
= gi(p),
where
gi(p) = Surplus of node i corresponding to the unique flow vector x(p)
satisfying CS together with p
=
∑
{j|(j,i)∈A}
xji(p) −
∑
{j|(i,j)∈A}
xij(p) + si.
(9.23)
Example 9.4. Quadratic Cost Network Problems
Consider the case where each arc cost function fij is a positive definite
quadratic and there are no arc flow bounds. This is the problem
minimize
∑
(i,j)∈A
(
aijxij +
1
2
wijx
2
ij
)
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
where aij , wij , and si are given scalars, and wij > 0 for all arcs (i, j). The
problem is interesting in its own right, but also arises as a subproblem in
430 Convex Separable Network Problems Chap. 9
Newton-like algorithms, which are based on quadratic approximations to a
nonquadratic convex cost function.
The CS conditions here have the linear form
pi − pj = aij + wijxij , ∀ (i, j) ∈ A,
so the unique flow vector x(p) satisfying CS together with p is given by
xij(p) =
pi − pj − aij
wij
, ∀ (i, j) ∈ A.
As a result, the surplus/dual partial derivative gi(p) of Eq. (9.23) has the
linear form
gi(p) =
∑
{j|(j,i)∈A}
pj − pi − aji
wji
−
∑
{j|(i,j)∈A}
pi − pj − aij
wij
+ si, (9.24)
which is particularly convenient for analytical and algorithmic purposes.
9.5 ALGORITHMS FOR DIFFERENTIABLE DUAL PROBLEMS
Dual problem differentiability has an important implication: it allows the
use of standard iterative unconstrained minimization methods for solving
the dual problem, such as steepest descent, and versions of the conjugate
gradient method. As an example, for the strictly convex quadratic cost net-
work problem (Example 9.4), the dual function is quadratic, so it can be
maximized using the conjugate gradient method in a finite number of iter-
ations (see nonlinear programming textbooks, such as Bertsekas [1995b]).
For this, the dual function gradient is needed, and it can be calculated
using the convenient expression (9.24).
Another interesting method that is well-suited to the special structure
of the dual problem is the relaxation method, which is simply a coordinate
ascent method applied to the maximization of the dual function. The re-
laxation method produces a sequence of price vectors each with a larger
dual function value than the preceding one. Successive price vectors differ
in only one coordinate/node price. At the start of the typical iteration
of the relaxation method we have a price vector p. If the corresponding
surplus/dual partial derivative gi(p) is zero for all nodes i, then p and the
unique vector x satisfying CS together with p are dual and primal opti-
mal, respectively, and the algorithm terminates. Otherwise the iteration
proceeds as follows:
Sec. 9.5 Algorithms for Differentiable Dual Problems 431
Relaxation Iteration
Choose any node i such that gi(p) 	= 0 and change the ith coordinate
of p, to obtain a vector p that maximizes q along that coordinate; that
is,
gi(p) = 0.
There is a great deal of flexibility regarding the order in which nodes
are taken up for relaxation. However, for the method to be valid, it is
necessary to assume that every node is chosen as the node i in the relaxation
iteration an infinite number of times.
An important point is that when the primal problem is feasible, the
relaxation iteration is well defined, in the sense that it is possible to adjust
the price pi as required, under very weak assumptions. To see this, suppose
that gi(p) > 0 and that there does not exist a γ > 0 such that gi(p +
γei) = 0, where ei denotes the ith coordinate vector. Consider the price
differentials tij(γ), (i, j) ∈ A and tji(γ), (j, i) ∈ A, corresponding to the
price vector p + γei:
tij(γ) = pi − pj + γ, tji(γ) = pj − pi − γ.
We have tij(γ) → ∞ and tji(γ) → −∞ as γ → ∞. Therefore, the cor-
responding unique arc flows xij(γ) and xji(γ) satisfying CS together with
p + γei tend to the corresponding endpoints
cij = sup
xij∈Xij
xij , bji = inf
xji∈Xji
xji
as γ → ∞, and using the definition of gi(·), it is seen that
lim
γ→∞
gi(p + γei) =
∑
{j|(j,i)∈A}
bji −
∑
{j|(i,j)∈A}
cij + si.
Let us assume now that either −∞ < bmn for all arcs (m, n) ∈ A or
cmn < ∞ for all arcs (m, n) ∈ A, so that the sum ∞ − ∞ does not arise
in the above equation. Then, since gi(p + γei) > 0 for all γ > 0, we must
have bji > −∞ for all arcs (j, i) and cij < ∞ for all arcs (i, j). Therefore,
there exists a finite value of γ such that xji(γ) = bji for all arcs (j, i), and
xij(γ) = cij for all arcs (i, j). It follows that∑
{j|(j,i)∈A}
bji −
∑
{j|(i,j)∈A}
cij + si > 0,
which implies that the surplus of node i is positive for any feasible flow
vector x, and contradicts the primal feasibility assumption. An analogous
432 Convex Separable Network Problems Chap. 9
argument can be made for the case where gi(p) < 0. Thus, for each pair
(x, p) satisfying CS, the relaxation iteration produces a well-defined flow
vector. For an example of what may happen if we have simultaneously
bji = −∞ for some arc (j, i) and cij = ∞ for some arc (i, j), the reader
may wish to work out the relaxation iteration for the example of Fig. 9.4.
We mention also a generalization of the relaxation method, which
allows the maximization along each coordinate to be inexact to some extent,
and to be controlled by a given scalar δ ∈ [0, 1). Here the ith coordinate
of p is changed to obtain a vector p such that
0 ≤ gi(p) ≤ δgi(p) if gi(p) > 0,
δgi(p) ≤ gi(p) ≤ 0 if gi(p) < 0.
With a judicious positive choice of δ, this variant of the relaxation method
tends to be more efficient than the one where δ = 0. Furthermore, it can
seen that when δ > 0 it is always possible to adjust the price pi as required,
without the assumption described in the preceding paragraph.
The relaxation method, with both exact and approximate maximiza-
tion along each coordinate, has satisfactory convergence properties. Its
convergence analysis is, however, quite intricate because of two complicat-
ing factors. The first is that the dual cost is differentiable and concave, but
not necessarily strictly concave; general coordinate ascent methods require
some form of strict concavity for showing convergence (see e.g., Bertsekas
[1995b], Section 2.7). The second feature that complicates the analysis is
that the level sets of the dual function are unbounded (if we change all
prices by the same constant, the value of the dual function is unaffected).
We thus omit this convergence analysis and we refer to the textbook by
Bertsekas and Tsitsiklis [1989], which also contains a lot of material re-
lating to the parallel implementation of the relaxation method. Another
reference for the convergence analysis is the paper by Bertsekas, Hosein,
and Tseng [1987], which in addition to the preceding relaxation method,
develops another method that does not require dual function differentiabil-
ity, and generalizes the relaxation method of Section 6.3 for the minimum
cost flow problem.
Generally, experimentation has shown that the relaxation method has
difficulty dealing with ill-conditioning in the dual cost function q, as man-
ifested by a rate of change of the directional derivative of q along some
directions that is much faster relative to other directions. Ill-conditioning
is a well-known cause of slow convergence in (differentiable) nonlinear pro-
gramming algorithms, and coordinate ascent methods are susceptible to it.
The -relaxation method to be discussed in the next section, operates sim-
ilar to the relaxation method, but has two advantages: it can be applied in
the case of a nondifferentiable dual cost function, and (based on practical
experience) it can deal much better with ill-conditioning.
Sec. 9.6 Auction Algorithms 433
9.6 AUCTION ALGORITHMS
In this section we develop auction algorithms for the separable convex net-
work flow problem. Based on complexity analysis and experimentation,
these algorithms are very efficient. With proper implementation, they ap-
pear to be minimally affected by ill-conditioning in the dual problem. We
first develop an appropriate extension of the notion of -complementary
slackness (-CS for short) that was introduced in Chapter 7. We then de-
rive and analyze generalizations of the -relaxation and auction/sequential
shortest path methods of Sections 7.4 and 7.5. Throughout this section,
we assume that the problem is feasible.
Definition 9.3: Given  ≥ 0, a flow-price vector pair (x, p) is said to
satisfy -CS if for all arcs (i, j), we have xij ∈ Xij and
f−ij (xij) −  ≤ pi − pj ≤ f+ij (xij) + .
Figure 9.8 illustrates the definition of -CS. The intuition behind the
-CS conditions is that a feasible flow-price pair is “approximately” primal
and dual optimal if the -CS conditions are satisfied. This intuition is
quantified in the following proposition:
Proposition 9.7: Let
(
x(), p()
)
be a flow-price pair satisfying -CS
such that x() is feasible, and let ξ() be any flow vector satisfying CS
together with p() [note that ξ() need not satisfy the conservation of
flow constraints].
(a)
0 ≤ f
(
x()
)
− q
(
p()
)
≤ 
∑
(i,j)∈A
|xij() − ξij()| . (9.25)
(b) Assume that all the dual arc cost functions qij are real-valued.
Then
lim
→0
(
f
(
x()
)
− q
(
p()
))
= 0.
Proof: (a) To simplify notation, let us replace x(), p(), and ξ(), by x,
p, and ξ, respectively. Denote tij = pi − pj . Since ξ and p satisfy CS, we
have
fij(xij) = ξijtij + qij(tij), ∀ (i, j) ∈ A.
434 Convex Separable Network Problems Chap. 9
0 bij cij x ij
p jpi -
ε
ε
Xij
Figure 9.8: A visualization of the -CS conditions in terms of a “cylinder” around
the characteristic curve. The shaded area represents flow-price differential pairs
that satisfy the -CS conditions. In this figure, fij is a quadratic function whose
curvature is the slope shown, and the arc flow range Xij is the interval [bij , cij ]
[cf. Fig. 9.1(b)].
Take an arc (i, j) such that xij ≥ ξij . Then
fij(xij) + (ξij − xij)f−ij (xij) ≤ fij(ξij) = ξijtij + qij(tij).
Hence
fij(xij)− qij(tij) ≤ (xij − ξij)
(
f−ij (xij)− tij
)
+xijtij ≤ |xij − ξij |+xijtij ,
where the second inequality follows from -CS. This inequality is similarly
obtained when xij ≤ ξij , so we have
fij(xij) − qij(tij) ≤ |xij − ξij | + xijtij , ∀ (i, j) ∈ A.
From the definition of qij , we also have
xijtij ≤ fij(xij) − qij(tij), ∀ (i, j) ∈ A.
By combining these two inequalities and adding over all arcs, we obtain∑
(i,j)∈A
xijtij ≤
∑
(i,j)∈A
(
fij(xij)−qij(tij)
)
≤ 
∑
(i,j)∈A
|xij−ξij |+
∑
(i,j)∈A
xijtij .
Sec. 9.6 Auction Algorithms 435
Since x is feasible, we have
∑
(i,j)∈A
xijtij =
∑
i∈N
pi
 ∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji
 = ∑
i∈N
pisi.
Combining the last two relations, we obtain
0 ≤
∑
(i,j)∈A
(
fij(xij) − qij(tij)
)
−
∑
i∈N
pisi ≤ 
∑
(i,j)∈A
|xij − ξij |.
Using the definitions of f(x) and q(p), this relation is seen to be equivalent
to the desired Eq. (9.25).
(b) We first argue by contradiction that x() remains bounded as  → ∞.
Indeed, if this is not so, then since x() is feasible for all , there exists a
cycle C and a sequence k converging to 0 such that xij(k) → ∞ for all
(i, j) ∈ C+ and xij(k) → −∞ for all (i, j) ∈ C−. Since all qij are assumed
real-valued, we must have
lim
ξ→∞
f−ij (ξ) = ∞, ∀ (i, j) ∈ C+,
lim
ξ→−∞
f+ij (ξ) = −∞, ∀ (i, j) ∈ C+.
This implies that for k sufficiently large,
tij(k) ≥ f−ij
(
xij(k)
)
− k > tij(0), ∀ (i, j) ∈ C+, (9.26)
tij(k) ≤ f+ij
(
xij(k)
)
− k < tij(0), ∀ (i, j) ∈ C−. (9.27)
On the other hand, since tij(k) = pi(k) − pj(k), we have∑
(i,j)∈C+
tij(k) −
∑
(i,j)∈C−
tij(k) = 0, ∀ k,
which contradicts Eqs. (9.26) and (9.27). Therefore x() is bounded as
 → 0.
We will now show that ξij() − xij() is bounded for all arcs (i, j) as
 → 0, where ξ() is any flow vector satisfying CS together with p(), i.e.,
for all arcs (i, j), we have
ξij() ∈ Xij , f−ij
(
ξij()
)
≤ tij() ≤ f+ij
(
ξij()
)
.
If the interval Xij is unbounded above, we have f−ij (ξ) → ∞ as ξ → ∞.
Since xij() is bounded, we have that tij() is bounded from above, which
in turn implies that ξij() is bounded from above. Similarly, we can argue
436 Convex Separable Network Problems Chap. 9
that ξij() is bounded from below. Therefore, ξij() is bounded for all arcs
(i, j) as  → 0, and it follows that |xij()−ξij()| is also bounded for all arcs
(i, j) as  → 0. This, together with Eq. (9.25), which was shown earlier,
completes the proof. Q.E.D.
Proposition 9.7 does not tell us how small  must be to achieve a
certain tolerance for the sum f
(
x()
)
− q
(
p()
)
. On the other hand, if the
the lengths of the intervals Xij are bounded by some constant L > 0, then
from Eq. (9.25), we obtain
f
(
x()
)
− q
(
p()
)
≤ AL,
where A is the number of arcs.
For the remainder of this section, we assume that the dual arc cost
functions qij are real-valued , as in Prop. 9.7(b). This is true in partic-
ular if the intervals Xij are compact, or if limxij→∞ f+(xij) = ∞ and
limxij→−∞ f−(xij) = −∞ for all arcs (i, j).
We introduce a generic auction algorithm, whereby x and p are al-
ternately adjusted so as to drive the surpluses
gi =
∑
{j|(j,i)∈A}
xji −
∑
{j|(i,j)∈A}
xij + si
to zero while maintaining -CS at all iterations. The only additional re-
quirements are that nodes with nonnegative surplus continue to have non-
negative surplus and that price changes are effected by increasing the price
of a node with positive surplus by the maximum amount possible. We
then consider two special cases of this generic algorithm. The first is the
-relaxation method, which generalizes the method of Section 7.4; the sec-
ond is the auction/sequential shortest path algorithm, which generalizes
the method of Section 7.5.
Given a flow-price vector pair (x, p) satisfying -CS, an iteration of
the generic auction algorithm updates (x, p) as follows:
Iteration of the Generic Auction Algorithm
If there is no node with positive surplus, terminate the algorithm.
Otherwise, perform one of the following two operations:
(a) (Flow change) Adjust the flow vector x in a way that -CS is
maintained and all nodes with nonnegative surplus continue to
have nonnegative surplus. (Here p is unchanged.)
(b) (Price rise) Increase the price pi of some node i with positive
surplus by the maximum amount that maintains -CS. (Here x
and all other coordinates of p are unchanged.)
Sec. 9.6 Auction Algorithms 437
Upon termination of the generic auction algorithm, the flow-price
vector pair (x, p) satisfies -CS and all nodes have surplus that is non-
positive (and is equal to 0 since the problem is assumed to feasible). Thus,
the validity of the method rests on whether it terminates finitely. The
following proposition shows that the total number of price rises is finite
under a suitable assumption.
Proposition 9.8: Let r be any nonnegative scalar such that the ini-
tial price vector p0 for the generic auction algorithm satisfies r-CS to-
gether with some feasible flow vector x0. Also, assume that each price
rise on a node increases the price of that node by at least β, for some
fixed β ∈ (0, 1). Then, the method performs at most (r + 1)(N − 1)/β
price rises on each node.
Proof: Consider the pair (x, p) at the beginning of an iteration of the
generic method. Since the surplus vector g = (g1, . . . , gN ) is not zero, and
the flow vector x0 is feasible, we conclude that for each node s with gs > 0
there exists a node t with gt < 0 and a simple path P from t to s such that:
xij > x0ij , ∀ (i, j) ∈ P+, (9.28)
xij < x0ij , ∀ (i, j) ∈ P−, (9.29)
where P+ is the set of forward arcs of P and P− is the set of backward
arcs of P. [This can be seen from the conformal realization theorem (Prop.
1.1) as follows. For the flow vector x − x0, the divergence of node t is
−gt > 0 and the divergence of node s is −gs < 0. Hence, by the conformal
realization theorem, there is a simple path P from t to s that conforms to
the flow x − x0, that is, xij − x0ij > 0 for all (i, j) ∈ P+ and xij − x0ij < 0
for all (i, j) ∈ P−.]
From Eqs. (9.28) and (9.29), and the convexity of the functions fij
for all (i, j) ∈ A, we have
f−ij (xij) ≥ f+ij (x0ij), ∀ (i, j) ∈ P+, (9.30)
f+ij (xij) ≤ f−ij (x0ij), ∀ (i, j) ∈ P−. (9.31)
Since the pair (x, p) satisfies -CS, we also have that
pi − pj ∈ [f−ij (xij) − , f+ij (xij) + ], ∀ (i, j) ∈ A. (9.32)
Similarly, since the pair (x0, p0) satisfies r-CS, we have
p0i − p0j ∈ [f−ij (x0ij) − r, f+ij (x0ij) + r], ∀ (i, j) ∈ A. (9.33)
438 Convex Separable Network Problems Chap. 9
Combining Eqs. (9.30), (9.32), and (9.33), we obtain for all (i, j) ∈ P+,
pi − pj ≥ f−ij (xij) −  ≥ f+ij (x0ij) −  ≥ p0i − p0j − (r + 1).
Similarly, combining Eqs. (9.31)-(9.33), we obtain for all (i, j) ∈ P−,
pi − pj ≤ p0i − p0j + (r + 1).
Applying the above inequalities for all arcs of the path P , we get
pt − ps ≥ p0t − p0s − (r + 1)|P |, (9.34)
where |P | denotes the number of arcs of the path P. Since only nodes with
positive surplus can change their prices and nodes with nonnegative surplus
continue to have nonnegative surplus, it follows that if a node has negative
surplus at some time, then its price is unchanged from the beginning of the
method until that time. Thus pt = p0t . Since the path is simple, we also
have that |P | ≤ N − 1. Therefore, Eq. (9.34) yields
ps − p0s ≤ (r + 1)|P | ≤ (r + 1)(N − 1). (9.35)
Since only nodes with positive surplus can increase their prices and, by
assumption, each price rise increment is at least β, we conclude from Eq.
(9.35) that the total number of price rises that can be performed for node
s is at most (r + 1)(N − 1)/β. Q.E.D.
The preceding proposition shows that the bound on the number of
price rises is independent of the cost functions, but depends only on
r0 = min
{
r ∈ [0,∞) | (x0, p0) satisfies r-CS
for some feasible flow vector x0
}
,
which is the minimum multiplicity of  with which CS is violated by the
initial price vector together with some feasible flow vector. Note that r0 is
well defined for any p0 because, for all r sufficiently large, r-CS is satisfied
by p0 and any feasible flow vector.
To ensure that the number of flow changes between successive price
rises is finite and that each price rise is at least β, we need to further
specify how the price rises and flow changes should be effected. We thus
proceed to introduce the key mechanisms for achieving this.
For any  > 0, any β ∈ (0, 1), and any flow-price vector pair (x, p)
satisfying -CS, we define for each node i ∈ N its candidate list as the
union of the following two sets of arcs
L+(i) =
{
(i, j) ∈ A | (1 − β) < pi − pj − f+ij (xij) ≤ 
}
, (9.36)
Sec. 9.6 Auction Algorithms 439
βε
0 xij
ε
ε
Xij
βε
0 xji
ε
ε
Xji
(a) (b)
pjpi - pjp i-
Figure 9.9: Visualization of the conditions satisfied by a candidate-list arc. The
shaded area represents flow-price differential pairs corresponding to a candidate-
list arc (i, j) ∈ L+(i) in figure (a), and to a candidate-list arc (j, i) ∈ L−(i) in
figure (b). Note that at the right endpoint of Xij the right derivative f
+
ij is ∞,
so at the right endpoint, L+(i) is empty. Similarly, at the left endpoint, L−(i) is
empty.
L−(i) =
{
(j, i) ∈ A | −(1 − β) > pj − pi − f−ji (xji) ≥ −
}
. (9.37)
The arcs of the candidate list can be visualized in terms of the char-
acteristic curves
Γij =
{
(xij , tij) ∈ 2 | f−ij (xij) ≤ tij ≤ f+ij (xij)
}
.
Thus, (i, j) is in the candidate list of i (respectively, j) if (xij , pi − pj)
belongs to the “strip” at height between (1−β) and  above (respectively,
below) Γij (see Fig. 9.9).
For each arc (i, j) [respectively, (j, i)] in the candidate list of i, the
supremum of δ for which
pi − pj ≥ f+ij (xij + δ)
[respectively, pj − pi ≤ f−ji (xji − δ)] is called the flow margin of the arc
(see Fig. 9.10). An important fact, shown below, is that the flow margins
of these arcs are always positive.
Proposition 9.9: All arcs in the candidate list of a node have positive
flow margins.
440 Convex Separable Network Problems Chap. 9
0
Xij
0 xji
Xji
(a) (b)
δ
xij
δ
pjp i-pjpi -
Figure 9.10: Illustration of the flow margin δ of a candidate-list arc (i, j) ∈ L+(i)
in figure (a), and to a candidate-list arc (j, i) ∈ L−(i) in figure (b).
Proof: Assume that for an arc (i, j) ∈ A the flow margin is not positive;
that is, we have
pi − pj < f+ij (xij + δ), ∀ δ > 0.
Since the function f+ij is right continuous, this yields
pi − pj ≤ lim
δ↓0
f+ij (xij + δ) = f
+
ij (xij),
and thus, based on the definition of Eq. (9.36), (i, j) cannot be in the
candidate list of node i. A similar argument shows that an arc (j, i) ∈ A
such that
pj − pi > f−ji (xji − δ), ∀ δ > 0,
cannot be in the candidate list of node i. Q.E.D.
The method that we will use for flow changes is to decrease the surplus
of a node with positive surplus by changing the flow of candidate-list arcs.
This can be done either one arc at a time, as in the case of the -relaxation
method of Section 7.4, or one path of arcs at a time, as in the case of
the auction/sequential-shortest-path algorithm of Section 7.5. When the
candidate list of the node is empty, we perform a price rise on the node.
An important fact, shown below, is that the price rise increment for a node
with empty candidate list is at least β.
Sec. 9.6 Auction Algorithms 441
Proposition 9.10: If we perform a price rise on a node whose candi-
date list is empty, then the price of that node will increase by at least
β.
Proof: If the candidate list of a node i is empty, then for every arc (i, j) ∈
A we have pi − pj − f+ij (xij) ≤ (1 − β), and for every arc (j, i) ∈ A we
have pj − pi − f−ji (xji) ≥ −(1 − β). This implies that the numbers
pj − pi + f+ij (xij) + , ∀ (i, j) ∈ A,
pj − pi − f−ji (xji) + , ∀ (j, i) ∈ A,
are all greater than or equal to β. Since a price rise on i adds to pi the
minimum of all these numbers, the result follows. Q.E.D.
For any  > 0, any β ∈ (0, 1), and any flow-price vector pair (x, p)
satisfying -CS, let us consider the arc set A∗ that contains all candidate
list arcs oriented in the direction of flow change. In particular, for each
arc (i, j) in the forward portion L+(i) of the candidate list of a node i, we
introduce an arc (i, j) in A∗ and for each arc (j, i) in the backward portion
L−(i) of the candidate list of node i, we introduce an arc (i, j) in A∗ (thus
the direction of the latter arc is reversed). The set of nodes N and the set
A∗ define the admissible graph G∗ = (N ,A∗). We will consider methods
that keep G∗ acyclic at all iterations. Intuitively, because we move flow
in the direction of the arcs in G∗, keeping G∗ acyclic helps to limit the
number of flow changes between price rises, as we have seen in Section 7.4.
To ensure that initially the admissible graph is acyclic, one possibility is to
choose, for any initial price vector p0, the initial flow vector x0 such that
(x0, p0) satisfies 0-CS, that is,
f−ij (x
0
ij) ≤ p0i − p0j ≤ f+ij (x0ij), ∀ (i, j) ∈ A. (9.38)
With this choice, -CS is satisfied by (x0, p0) for any  > 0, and the initial
admissible graph is empty and thus acyclic.
In the next two sections, we will study two specializations of the
generic auction algorithm. These methods perform flow changes by moving
flow out of nodes with positive surplus along candidate-list arcs and they
perform price rises only on nodes with empty candidate lists. In addition,
they keep the admissible graph acyclic at all iterations and have favorable
complexity bounds.
9.6.1 The -Relaxation Method
For fixed  > 0 and β ∈ (0, 1), and a given flow-price vector pair (x, p)
satisfying -CS, an iteration of the -relaxation method updates (x, p) as
follows:
442 Convex Separable Network Problems Chap. 9
Iteration of the -Relaxation Method
Step 1: Select a node i with positive surplus gi; if no such node exists,
terminate the method.
Step 2: (δ-Flow push) If the candidate list of i is empty, go to Step
3. Otherwise, choose an arc from the candidate list of i, and let
δ = min{gi, flow margin of the chosen arc}.
Increase xij by δ if (i, j) is the arc, or decrease xji by δ if (j, i) is
the arc. If as a result the surplus of i becomes zero, go to the next
iteration; otherwise, go to Step 2.
Step 3: (Price rise) Increase the price pi by the maximum amount
that maintains -CS. Go to the next iteration.
To see that the -relaxation method is a specialization of the generic
auction method of Section 2, note that Step 3 is a price rise on node i and
that Step 2 adjusts the flows in such a way that -CS is maintained and
nodes with nonnegative surplus continue to have nonnegative surplus for
all subsequent iterations. The reason for the latter is that when iterating at
a node i, a flow push cannot make the surplus of i negative (by the choice
of δ in Step 2), and cannot decrease the surplus of neighboring nodes.
Furthermore, the -relaxation method performs a price rise only on nodes
with empty candidate list. Then, by Prop. 9.10, each price rise increment
is at least β and, by Prop. 9.8, the number of price rises (i.e., Step 3) on
each node is at most (r + 1)(N − 1)/β, where r is any nonnegative scalar
such that the initial price vector satisfies r-CS together with some feasible
flow vector. Thus, to prove finite termination of the -relaxation method,
it suffices to show that the number of flow pushes (i.e., Step 2) performed
between successive price rises is finite. We show this by first showing that
the method maintains the acyclicity of the admissible graph.
Proposition 9.11: If the admissible graph is initially acyclic, then it
remains acyclic at all iterations of the -relaxation method.
Proof: We use induction. Initially, the admissible graph G∗ is acyclic by
assumption. Assume that G∗ remains acyclic for all subsequent iterations
up to the mth iteration for some m. We will prove that after the mth
iteration G∗ remains acyclic. Clearly, after a flow push in Step 2, the
admissible graph remains acyclic, since it either remains unchanged, or
some arcs are deleted from it. Thus we only have to prove that after a
Sec. 9.6 Auction Algorithms 443
price rise on a node i, no cycle involving i is created. We note that, after
a price rise on node i, all incident arcs to i in the admissible graph at
the start of the mth iteration are deleted and new arcs incident to i are
added. We claim that i cannot have any incoming arcs that belong to the
admissible graph. To see this, note that just before a price rise on node i,
we have
pj − pi − f+ji (xji) ≤ , ∀ (j, i) ∈ A,
and since each price rise increment is at least β, we must have
pj − pi − f+ji (xji) ≤ (1 − β), ∀ (j, i) ∈ A,
after the price rise. Then, by Eq. (9.36), (j, i) cannot be in the candidate
list of node j. By a similar argument, we have that (i, j) cannot be in the
candidate list of j for all (i, j) ∈ A. Thus, after a price rise on node i,
we see that i cannot have any incoming arcs belonging to the admissible
graph, so no cycle involving i can be created. Q.E.D.
We say that a node i is a predecessor of a node j in the admissible
graph G∗ if a directed path (i.e., a path having no backward arc) from i to
j exists in G∗. Node j is then called a successor of i. Observe that, in the -
relaxation method, flow is pushed towards the successors of a node and if G∗
is acyclic, flow cannot be pushed from a node to any of its predecessors. A
δ-flow push along an arc in A is said to be saturating if the flow increment δ
is equal to the flow margin of the arc. By our choice of δ in the -relaxation
method, a nonsaturating flow push always exhausts (i.e., sets to zero) the
surplus of the starting node of the arc. Then, by using Prop. 9.11, we
obtain the following result.
Proposition 9.12: If the admissible graph is initially acyclic, then
the number of flow pushes between two successive price rises (not nec-
essarily at the same node) performed by the -relaxation method is
finite. Furthermore, the algorithm terminates with a flow-price pair
satisfying -CS.
Proof: We observe that a saturating flow push along an arc removes the
arc from the admissible graph, while a nonsaturating flow push does not
add a new arc to the admissible graph. Thus the number of saturating
flow pushes that can be performed between successive price rises is at most
A. It will thus suffice to show that the number of nonsaturating flow
pushes that can be performed between saturating flow pushes is finite.
Assume the contrary, that is, there is an infinite sequence of successive
nonsaturating flow pushes, with no intervening saturating flow push. Then
the admissible graph remains fixed throughout this sequence. Furthermore,
444 Convex Separable Network Problems Chap. 9
the surplus of some node i0 must be exhausted infinitely often during this
sequence. This can happen only if the surplus of some predecessor i1 of i0 is
exhausted infinitely often during the sequence. Continuing in this manner,
we construct an infinite sequence of predecessor nodes {ik}. Thus, some
node in this sequence must be repeated, which is a contradiction since the
admissible graph is acyclic. Hence, the number of flow pushes between two
successive price rises is finite. Since the number of price rises is finite (cf.
Props. 9.8 and 9.10), termination of the algorithm follows. Q.E.D.
By refining the proof of Prop. 9.12, we can further show that the
number of flow pushes between successive price rises is at most (N + 1)A,
from which a complexity bound for the -relaxation method may be readily
derived. However, we will focus on a special implementation of the method
for which we will derive a more favorable running time.
Efficient Implementation
Let us consider a generalization of the sweep implementation, discussed in
Section 7.4. This implementation defines the order in which nodes are se-
lected for an -relaxation iteration. In particular, the nodes are maintained
in a linked list T , which is traversed from the first to the last element. The
order of the nodes in the list is consistent with the successor order implied
by the admissible graph; that is, if a node j is a successor of a node i,
then j must appear after i in the list. If the initial admissible graph is
empty, as is the case with the initialization of Eq. (9.38), the initial list is
arbitrary. Otherwise, the initial list must be consistent with the successor
order of the initial admissible graph. The list is updated in a way that
maintains the consistency with the successor order. In particular, let i be
the node chosen in Step 1 of the iteration, and let Ni be the subset of nodes
of T that are after i in T. If the price of i changes in this iteration, then
node i is removed from its position in T and placed in the first position of
T . The node chosen in the next iteration, if Ni is nonempty, is the node
i′ ∈ Ni with positive surplus which ranks highest in T . Otherwise, the
positive surplus node ranking highest in T is chosen. It can be seen as in
Section 7.4 that, with this rule of repositioning the nodes following a price
change, the list order is consistent with the successor order implied by the
admissible graph at all iterations.
The next proposition gives a bound on the number of flow pushes
made by the sweep implementation of the -relaxation method. This result
is based on the observations that (a) between successive saturating flow
pushes on an arc, there is at least one price rise performed on one of the
end nodes of the arc, and (b) between successive price rises (not necessarily
at the same node), the number of nonsaturating flow pushes is at most N .
The proof parallels the one given in Section 7.4, and will be omitted.
Sec. 9.6 Auction Algorithms 445
Proposition 9.13: Let r be any nonnegative scalar such that the
initial price vector for the sweep implementation of the -relaxation
method satisfies r-CS together with some feasible flow vector. Then,
the number of price rises on each node, the number of saturating flow
pushes, and the number of nonsaturating flow pushes up to termination
of the method are O(rN), O(rNA), and O(rN3), respectively.
We now derive the running time for the sweep implementation of the
-relaxation method. The dominant computational requirements are:
(1) The computation required for price rises.
(2) The computation required for saturating flow pushes.
(3) The computation required for nonsaturating flow pushes.
In contrast to the linear cost case, we cannot express the running time
in terms of the size of the problem data since the latter is not well defined
for convex cost functions. Instead, we introduce a set of simple operations
performed by the -relaxation method, and we estimate the number of these
operations. In particular, in addition to the usual arithmetic operations
with real numbers, we consider the following operations:
(a) Given the flow xij of an arc (i, j), calculate the cost fij(xij), the left
derivative f−ij (xij), and the right derivative f
+
ij (xij).
(b) Given the price differential tij = pi − pj of an arc (i, j), calculate
sup{ξ | f+ij (ξ) ≤ tij} and inf{ξ | f−ij (ξ) ≥ tij}.
Operation (a) is needed to compute the candidate list of a node and a price
increase increment; operation (b) is needed to compute the flow margin of
an arc and the flow initialization of Eq. (9.38). Complexity will thus be
measured in terms of the total number of operations performed by the
method, as in the following proposition, which follows from Prop. 9.13.
Proposition 9.14: Let r be any nonnegative scalar such that the
initial price vector for the sweep implementation of the -relaxation
method satisfies r-CS together with some feasible flow vector. Then,
the method requires O(rN3) operations up to termination.
The theoretical and the practical performance of the -relaxation
method can be further improved by -scaling , whereby we apply the -
relaxation method several times, starting with a large value of , say 0,
and successively reduce  up to a final value, say , that will give the de-
sirable degree of accuracy to our solution. Furthermore, the price and flow
information from one application of the method is passed to the next. Sim-
446 Convex Separable Network Problems Chap. 9
ilar to Section 7.4, it can be shown that if 0 is chosen sufficiently large so
that the initial price vector satisfies 0-CS together with some feasible flow
vector, then the running time of the -relaxation method using the sweep
implementation and -scaling is O
(
N3 ln(0/)
)
operations.
9.6.2 Auction/Sequential Shortest Path Algorithm
We now consider the extension of the auction/sequential shortest path
(ASSP) algorithm of Section 7.5. The algorithm is a special case of the
generic auction method, and differs from the -relaxation method in that
instead of pushing flow along a candidate-list arc to any node, it pushes
flow along a path of candidate-list arcs ending at a node with negative
surplus. In fact, whereas a flow push in the -relaxation method may in-
crease the surplus of a node in absolute value (e.g., when flow is pushed to
a neighboring node with nonnegative surplus), in the ASSP algorithm, the
surplus of each node is nonincreasing in absolute value.
We first introduce some definitions. For a path P , we denote by
s(P ) and t(P ) the starting node and the terminal node, respectively, of
P. For any  > 0 and β ∈ (0, 1), and any flow-price vector pair (x, p)
satisfying -CS, we say that a path P of a graph (N ,A) is augmenting if
each forward (respectively, backward) arc (i, j) of P is in the candidate list
of i (respectively, j) and s(P ) is a source (i.e., has positive surplus) and
t(P ) is a sink (i.e., has negative surplus). As in Section 7.5, we define two
operations on a given path P = (n1, n2, . . . , nk):
(a) A contraction of P , which deletes the terminal node of P and the arc
incident to this node.
(b) An extension of P by an arc (nk, nk+1) or an arc (nk+1, nk), which
replaces P by the path (n1, n2, . . . , nk, nk+1) and adds to P the cor-
responding arc.
For a fixed  > 0 and β ∈ (0, 1), and a given flow-price vector pair
(x, p) satisfying -CS, an iteration of the ASSP algorithm updates (x, p) as
follows:
Iteration of the ASSP Algorithm
Step 1: Select a node i with positive surplus and let the path P consist
of only this node; if no such node exists, terminate the algorithm.
Step 2: Let i be the terminal node of the path P. If the candidate list
of i is empty, then go to Step 3; otherwise, go to Step 4.
Step 3: (Contract Path) Increase the price pi by the maximum
amount that maintains -CS. If i 	= s(P ), contract P. Go to Step 2.
Sec. 9.6 Auction Algorithms 447
Step 4: (Extend Path) Select an arc (i, j) [or (j, i)] from the candi-
date list of i and extend P by this arc. If the surplus of j is negative,
go to Step 5; otherwise, go to Step 2.
Step 5: (Augmentation) Perform an augmentation along the path
P by the amount
δ = min
{
gs(P ),−gt(P ), minimum of flow margins of the arcs of P
}
,
(i.e., increase the flow of all forward arcs of P and decrease the flow
of all backward arcs of P by δ). Go to the next iteration.
Roughly speaking, at each iteration of the ASSP algorithm, the path
P starts as a single source and is successively extended or contracted until
the terminal node of P is a sink. Then an augmentation along P is per-
formed so as to decrease (respectively, increase) the surplus of the starting
node (respectively, terminal node), while leaving the surplus of the remain-
ing nodes unchanged. In case of a contraction, the price of the terminal
node of P is strictly increased.
We note that the ASSP algorithm is a special case of the generic
auction algorithm. To see this, note that Step 2 is a price rise on node i
and that Step 5 adjusts the flows in such a way that -CS is maintained and
nodes with nonnegative surplus continue to have nonnegative surplus for
all subsequent iterations. The reason for the latter is that an augmentation
along P changes the surplus of only two nodes s(P ) and t(P ), and by our
choice of δ, the surplus of the node s(P ) remains nonnegative after the
augmentation.
We also note that the ASSP algorithm performs price rises only on
nodes with empty candidate list. Thus, by Prop. 9.10, each price rise
increment is at least β and, by Prop. 9.8, the number of price rises (i.e.,
path contractions) on each node is at most (r + 1)(N − 1)/β, where r
is any nonnegative scalar such that the initial price vector satisfies r-CS
together with some feasible flow vector. It follows that to prove finite
termination of the ASSP algorithm, it suffices to show that the number
of path extensions (cf. Step 4) and the number of augmentations (cf. Step
5) performed between successive path contractions is finite. Similar to the
case of the -relaxation method, we show this by first showing that the
algorithm keeps the admissible graph acyclic and that the path P , when
its backward arcs are reversed in direction, belongs to the admissible graph.
Proposition 9.15: If initially the admissible graph is acyclic, then
the admissible graph remains acyclic at all iterations of the ASSP
algorithm. Moreover, the path P maintained by the algorithm, when
448 Convex Separable Network Problems Chap. 9
its backward arcs are reversed in direction, belongs to the admissible
graph at all times.
Proof: The admissible graph can change either by a price rise (Step 3)
or by an augmentation (Step 5). An augmentation keeps the admissible
graph acyclic because, after an augmentation, the admissible graph either
remains unchanged or some arcs are deleted from it. A price rise keeps the
admissible graph acyclic, as was shown in the proof of Prop. 9.11.
To show that P , when its backward arcs are reversed in direction,
belongs to the admissible graph at all times, we simply observe that a
path extension maintains this property (since the arc added to P is in
the candidate list of the terminal node of P ) and that a path contraction
also maintains this property (since a price rise on the terminal node of P
changes the admissible graph only by adding/deleting arcs incident to this
node and, after the contraction, this node and its incident arc in P are
both deleted from P ). Q.E.D.
We now use Prop. 9.15 to bound the number of augmentations and
path extensions performed by the ASSP algorithm between successive path
contractions. This shows that the algorithm terminates with a flow-price
pair satisfying -CS.
Proposition 9.16: If initially the admissible graph is acyclic, then
the number of augmentations and path extensions between two succes-
sive path contractions (not necessarily at the same node) performed by
the ASSP algorithm is finite. Furthermore, the algorithm terminates
with a flow-price pair satisfying -CS.
Proof: We observe that an augmentation does not increase the number
of nodes with nonzero surplus and does not add any arc to the admissible
graph. Moreover, after an augmentation, either an arc is removed from the
admissible graph or a node has its surplus set to zero. Thus, the number of
arcs in the admissible graph plus the number of nodes with nonzero surplus
is decreased by at least one after each augmentation. It follows that the
number of augmentations between successive path contractions is at most
A + N .
By Prop. 9.15, the path P always belongs to the admissible graph
which is acyclic, so P cannot have repeated nodes and hence the number
of successive extensions of P (before a contraction or an augmentation is
performed) is at most N . Thus, the number of path extensions between
successive path contractions is at most N · (number of augmentations be-
Sec. 9.7 Monotropic Programming 449
tween successive path contractions) ≤ N(A + N). Since the number of
contractions is finite (cf. Props. 9.8 and 9.10), termination of the algorithm
follows. Q.E.D.
9.7 MONOTROPIC PROGRAMMING
In this section, we consider a substantial generalization of the convex sep-
arable network problem. In particular, we replace the conservation of flow
constraint with a general subspace constraint. Specifically, the problem is
minimize
n∑
j=1
fj(xj)
subject to x ∈ S, (9.39)
xj ∈ Xj , j = 1, . . . , n,
where x denotes a vector in n, consisting of the n scalar components
x1, . . . , xn, and
Xj is a nonempty interval for each j,
fj : Xj →  is a closed convex function for each j,
S is a subspace of n.
We refer to this problem as a monotropic programming problem.†
When x is a flow vector and S is the circulation subspace of a graph
(N ,A),
S =
x ∣∣∣ ∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = 0, ∀ i ∈ N
 ,
we essentially recover the convex separable network problem. The only dif-
ference is that the constraint x ∈ S implies that the node supplies are all
0, instead of being arbitrary scalars, but this is not a real restriction, be-
cause every separable network problem can be converted to the circulation
format as indicated in Section 4.1.3.
Note that problems involving general linear constraints and a sepa-
rable convex cost function can be converted to monotropic programming
† The name “monotropic” means “turning in a single direction” in Greek,
and captures the characteristic monotonicity property of convex functions of a
single variable such as fj .
450 Convex Separable Network Problems Chap. 9
problems. In particular, the problem
minimize
n∑
j=1
fj(xj)
subject to Ax = b, xj ∈ Xj , j = 1, . . . , n, (9.40)
where A is a given matrix and b is a given vector, is equivalent to
minimize
n∑
j=1
fj(xj)
subject to Ax − z = 0, z = b, xj ∈ Xj , j = 1, . . . , n,
where z is a vector of artificial variables. This is a monotropic program-
ming problem with a constraint subspace S =
{
(x, z) | Ax−z = 0
}
. When
the fj(xj) are linear functions, problem (9.40) reduces to the general linear
programming problem. When the fj(xj) are positive semidefinite quadratic
functions, problem (9.40) reduces to a convex separable quadratic program-
ming problem. The general convex quadratic programming problem with
cost function x′C ′Cx, where C is a matrix, can be made separable by using
the linear transformation y = Cx.
It can thus be seen that the monotropic programming problem con-
tains as special cases broad classes of important optimization problems.
These problems share the distinguishing structural characteristics of monot-
ropic programming that we will develop in this section, including a pow-
erful and symmetric duality theory, as well as extensions of many of the
analytical and algorithmic ideas we developed earlier in this chapter.
Duality Theory
To develop the appropriate dual problem, we introduce an auxiliary vector
y ∈ n and we convert the monotropic programming problem (9.39) to the
equivalent form
minimize
n∑
j=1
fj(xj)
subject to x = y, y ∈ S,
xj ∈ Xj , j = 1, . . . , n.
We then assign a Lagrange multiplier vector t ∈ n to the equality con-
straint x = y, obtaining the Lagrangian function
L(x, y, t) =
n∑
j=1
fj(xj) + t′(y − x),
Sec. 9.7 Monotropic Programming 451
and the dual function
q(t) = inf
y∈S, xj∈Xj , j=1,...,n
L(x, y, t)
= inf
y∈S
t′y +
n∑
j=1
inf
xj∈Xj
{
fj(xj) − tjxj
}
=
{ ∑n
j=1 qj(tj) if t ∈ S⊥,
−∞ otherwise,
where
qj(tj) = inf
xj∈Xj
{
fj(xj) − tjxj
}
, j = 1, . . . , n,
and S⊥ is the orthogonal subspace of S,
S⊥ = {t | t′x = 0, ∀ x ∈ S}.
The properties of the functions qj have been developed in Prop. 9.6.
Furthermore, we have noted in Section 9.3 that −qj is a closed convex
function whose domain is the interval
Tj =
{
tj | qj(tj) > −∞
}
.
Thus the dual problem of maximizing q over n can be written as
maximize
n∑
j=1
qj(tj)
subject to t ∈ S⊥,
tj ∈ Tj , j = 1, . . . , n.
(9.41)
It can be seen that with a change of sign to convert maximization to
minimization, the dual problem has the same form as the primal. In fact,
it can be verified using Prop. 9.6(a) [cf. Eq. (9.20)] that when the dual
problem is dualized, it yields the primal problem. Thus the duality is fully
symmetric, and any general algorithm that can solve the primal problem
(without relying on any special structure of the subspace S) can be used
to solve the dual problem and conversely.
Much of the analysis given in Sections 9.2-9.4 for the case where S
is a circulation subspace can be generalized to the monotropic program-
ming problem. In particular, a pair (x, t) is said to satisfy complementary
slackness (CS for short) if it lies on the characteristic curve
Γ =
{
(x, t) | f−j (xj) ≤ tj ≤ f+j (xj), j = 1, . . . , n
}
,
or equivalently, if for all j, xj attains the infimum in the equation
qj(tj) = inf
x∈Xj
{
fj(x) − tjx
}
.
452 Convex Separable Network Problems Chap. 9
By Prop. 9.6(a), this is also equivalent to tj attaining the supremum in the
equation
fj(xj) = sup
t∈Tj
{
qj(t) + txj
}
.
This means that the characteristic curve can alternatively be defined by
Γ =
{
(x, t) | −q−j (tj) ≤ xj ≤ −q+j (tj), j = 1, . . . , n
}
,
where q+j and q
−
j are the right and left derivatives of qj , respectively.
Similar to Section 9.3, we call a vector x regular if
f−j (xj) < ∞, −∞ < f+j (xj), ∀ j = 1, . . . , n.
We also consider a general equilibrium problem, which is to find a pair (x, t)
on the curve Γ that satisfies
x ∈ S, t ∈ S⊥.
The duality theorems of Section 9.3 generalize nearly verbatim. In
particular, we have the following:
Proposition 9.17: (Complementary Slackness Theorem) A
pair (x∗, t∗) such that x∗ ∈ S and t∗ ∈ S⊥ satisfies CS if and only
if x∗ and t∗ are optimal primal and dual solutions, respectively, and
the optimal primal and dual costs are equal.
Proposition 9.18: Suppose that there exists at least one primal fea-
sible solution that is regular. Then, if x∗ is an optimal solution of
the primal problem, there exists an optimal solution p∗ of the dual
problem that satisfies CS together with x∗.
Proposition 9.19: (Duality Theorem) If there exists at least one
feasible solution to the primal problem, or at least one feasible solution
to the dual problem, the optimal primal and dual costs are equal.
Proposition 9.20: (Equilibrium Theorem) A pair (x∗, t∗) solves
the equilibrium problem if and only if x∗ and t∗ are optimal primal
and dual solutions, respectively.
Sec. 9.7 Monotropic Programming 453
The proofs of Props. 9.17, 9.18, and 9.20 are fairly straightforward,
and are nearly identical to the proofs of Props. 9.2, 9.3, and 9.5, respec-
tively. There remains to prove the duality theorem (Prop. 9.19 and its
special case, Prop. 9.4). By repeating the proof of Prop. 9.2, we can show
that weak duality holds; that is,
n∑
j=1
qj(tj) ≤
n∑
j=1
fj(xj), ∀ x ∈ S, t ∈ S⊥ with xj ∈ Xj , tj ∈ Tj , ∀ j.
(9.42)
It will thus be sufficient to show the reverse inequality. Our proof is con-
structive and uses a conceptual descent algorithm, which we now introduce.
-Descent Algorithm
The feasible direction methods discussed in Section 8.8.1 operate on the
principle of iterative cost improvement along feasible descent directions.
These methods improve the cost function at a nonoptimal vector, but they
do not guarantee a fixed amount of improvement. We will introduce a
somewhat different method whereby if the current iterate is not within
 > 0 of being optimal, there is a guarantee of an improvement of at least
β at the next iteration, where β > 0 is a fixed scalar. We will derive
this method for the separable case of a monotropic programming problem,
although the idea can be extended to general convex programming.
For an  > 0, let us define for each xj ∈ Xj , the -subdifferential of
the pair (fj , Xj) at xj as the set
∂fj(xj) =
{
tj | fj(zj) ≥ fj(xj) + tj(zj − xj) − , ∀ zj ∈ Xj
}
. (9.43)
The elements of the -subdifferential are called -subgradients. It is easily
seen that ∂fj(xj) is a closed interval. In particular, its left endpoint is
f−j,(xj) =
{
supδ<0, xj+δ∈Xj
fj(xj+δ)−fj(xj)+
δ if inf Xj < xj ,
−∞ if inf Xj = xj ,
(9.44)
and its right endpoint is
f+j,(xj) =
{
infδ>0, xj+δ∈Xj
fj(xj+δ)−fj(xj)+
δ if xj < supXj ,
∞ if xj = supXj .
(9.45)
Note that we have
f−j,(xj) ≤ f−j (xj) ≤ f+j (xj) ≤ f+j,(xj),
454 Convex Separable Network Problems Chap. 9
0 xj
fj (xj ) 
ε
Slopes =  endpoints of
ε-subdifferential at x
Xj
Figure 9.11: Illustration of the -subdif-
ferential ∂fj(xj). It corresponds to the
set of slopes indicated in the figure. Note
that ∂fj(xj) is nonempty and includes
the gradient of fj at xj if fj is differen-
tiable at xj .
so the -subdifferential ∂fj(xj) contains the left and right derivatives
f−j (xj) and f
+
j (xj). We will also show shortly that ∂fj(xj) is nonempty.
Figure 9.11 illustrates the definition.
Let us derive some properties of -subgradients. We recall the defini-
tion
qj(tj) = inf
x∈Xj
{
fj(x) − tjx
}
, (9.46)
and the relation [cf. Prop. 9.6(a)]
fj(xj) = sup
t∈Tj
{
qj(t) + txj
}
, (9.47)
where Tj is the effective domain of qj
Tj =
{
tj | qj(tj) > −∞
}
.
Comparing these relations with the definition (9.43) of the -subdifferential,
we see that
tj ∈ ∂fj(xj) if and only if fj(xj) ≤ qj(tj) + tjxj + . (9.48)
Thus we have
tj ∈ Tj , ∀ tj ∈ ∂fj(xj),
and furthermore tj is an -subgradient at xj if and only if tj attains within
 the supremum in Eq. (9.47). From this it follows that the -subdifferential
is nonempty at every xj ∈ Xj .
Suppose now that x is a feasible solution such that
n∑
j=1
fj(xj) > f∗ + n, (9.49)
where f∗ is the optimal primal cost. Then we claim that the subspace S⊥
does not intersect the set
B(x) =
{
(t1, . . . , tn) | tj ∈ ∂fj(xj), j = 1, . . . , n
}
.
Sec. 9.7 Monotropic Programming 455
Indeed, if this were not so, i.e., if there existed t = (t1, . . . , tn) ∈ S⊥ with
tj ∈ ∂fj(xj) for all j, we would have by adding Eq. (9.48),
n∑
j=1
fj(xj) ≤
n∑
j=1
qj(tj) +
n∑
j=1
tjxj + n ≤ f∗ + n,
where the last inequality holds because
∑n
j=1 qj(tj) ≤ f∗ (by weak du-
ality) and
∑n
j=1 tjxj = 0 (since x ∈ S and t ∈ S⊥). We thus obtain a
contradiction of Eq. (9.49).
Thus when Eq. (9.49) holds, we have
S⊥ ∩ B(x) = Ø,
and it can be seen that there must exist a direction d = (d1, . . . , dn) ∈ S
such that
t′d < 0, ∀ t ∈ B(x);
see Fig. 9.12. We show in the following proposition that for such a vector
d, we have
inf
α>0
n∑
j=1
fj(xj + αdj) <
n∑
j=1
fj(xj) − , (9.50)
so that it is possible to effect a cost improvement of more than  by searching
along the half line
x + αd, α > 0.
We refer to a vector d satisfying Eq. (9.50) as an -descent direction at x.
0
Subspace S
Set
S   + Bε(x) 
Set Bε(x) 
Vector  d
Figure 9.12: Illustration of the fact that
if
S⊥ ∩ B(x) = Ø,
there must exist a direction d ∈ S such
that
t′d < 0, ∀ t ∈ B(x).
When S⊥ ∩ B(x) = Ø, the set S⊥ +
B(x) does not contain the origin. The
desired vector d is the opposite of the
projection of the origin on the set S⊥ +
B(x).
456 Convex Separable Network Problems Chap. 9
Proposition 9.21: (-Descent Property) Suppose that x is a pri-
mal feasible solution satisfying
n∑
j=1
fj(xj) > f∗ + n
for some  > 0. Then there exists a vector d ∈ S such that
t′d < 0, ∀ t ∈ B(x), (9.51)
and this vector is an -descent direction at x.
Proof: The existence of a vector d ∈ S satisfying Eq. (9.51) was shown in
the preceding discussion, so we only need to prove the -descent property
(9.50). The condition
∑n
j=1 tjdj < 0 is equivalent to∑
{j|dj<0}
f−j,(xj)dj +
∑
{j|dj>0}
f+j,(xj)dj < 0,
where f−j,(xj) and f
+
j,(xj) are the left and right endpoints of the -subdiffe-
rential ∂fj(xj), respectively. Using the expressions Eqs. (9.44) and (9.45)
for these endpoints, the preceding relation can equivalently be written as
n∑
j=1
inf
α>0
fj(xj + αdj) − fj(xj) + 
α
< 0.
Let α1, . . . , αn be positive scalars such that
n∑
j=1
fj(xj + αjdj) − fj(xj) + 
αj
< 0. (9.52)
Define
α =
1∑n
j=1 1/αj
.
As a consequence of the convexity of fj , it can be seen that the ratio(
fj(xj +αdj)−fj(xj)
)
/α is monotonically nondecreasing in α. Thus, since
αj ≥ α for all j, we have
fj(xj + αjdj) − fj(xj)
αj
≥ fj(xj + αdj) − fj(xj)
α
,
Sec. 9.7 Monotropic Programming 457
and Eq. (9.52) together with the definition of α yields
0 >
n∑
j=1
fj(xj + αjdj) − fj(xj) + 
αj
≥ 
α
+
n∑
j=1
fj(xj + αdj) − fj(xj)
α
.
Thus, we have
∑n
j=1 fj(xj +αdj) <
∑n
j=1 fj(xj)−, and the result follows.
Q.E.D.
By using the preceding proposition, we define an algorithm, called
the -descent method , whereby at each iterate x for which
S⊥ ∩ B(x) = Ø,
we find a direction d satisfying Eq. (9.51), we perform a line search along
that direction, and we reduce the primal cost by at least . In this form,
the algorithm is not yet useful for solving the problem, because we have
to specify the method for choosing and perhaps changing , and also the
method by which we find the direction d and perform the line search.
However, here we are not interested in a practical implementation of the
algorithm but rather in its use for proving the duality theorem.
Proof of the Duality Theorem
Suppose that there exists a primal feasible solution. Start the -descent
algorithm from this solution, and continue iterating up to the point where
S⊥ intersects the set B(x). There are two possibilities:
(1) Termination never occurs, in which case the sequence of primal costs
generated will diverge to −∞, since by Prop. 9.21, there is a cost
improvement of at least  at each iteration. Thus the optimal dual
cost must also be −∞ by weak duality [cf. Eq. (9.42)].
(2) Termination occurs with some vector x and some vector t ∈ S⊥ ∩
B(x). In this case, by adding Eq. (9.48), we have
n∑
j=1
fj(xj) ≤
n∑
j=1
qj(tj) +
n∑
j=1
tjxj + n =
n∑
j=1
qj(tj) + n,
where the last equation holds because
∑n
j=1 tjxj = 0, since x ∈ S and
t ∈ S⊥. Thus, since
∑n
j=1 qj(tj) ≤
∑n
j=1 fj(xj) (by weak duality),
the optimal primal and dual costs differ by at most n. Since  can
be taken arbitrarily small, it follows that the optimal primal and dual
costs must be equal.
458 Convex Separable Network Problems Chap. 9
Thus, we have shown that if there exists a primal feasible solution, the
optimal primal and dual costs are equal.
Finally, applying the preceding argument to the dual problem, and
taking into account that the dual of the dual problem is the primal, we see
that if there exists a dual feasible solution, the optimal primal and dual
costs are equal. Thus the proof of the duality theorem is complete.
Additional Properties of Monotropic Programs
Monotropic programming problems have some interesting combinatorial
properties. A complete analysis is beyond our scope, so we will only discuss
some of the main ideas, and describe how they relate to network problems.
These ideas revolve around the notion of the support of a vector z (i.e., the
set of indices {j | zj 	= 0}), and vectors that have minimal support, as in
the following definition.
Definition 9.4: A nonzero vector z of a subspace S of n is said to
be elementary if there is no vector z 	= 0 in S that has smaller support
than z, i.e., for all nonzero z ∈ S, {j | zj 	= 0} is not a strict subset of
{j | zj 	= 0}.
It can be seen that if z and z are two elementary vectors with the same
support, then z and z are scalar multiples of each other (if this were not so,
the vector z − γz would have smaller support than z and z for a suitable
scalar γ). Thus, since the number of supports is finite, each subspace has
only a finite number of elementary vectors, up to scalar multiplication.
From the definition of elementary vector, it can also be seen that given
any nonzero vector y, there exists an elementary vector z with support
contained in the support of y (either y is elementary or else there exists
a nonzero vector z with support strictly contained in the support of y;
continue this argument until an elementary vector z is obtained).
For some examples that illustrate the definition, note that the elemen-
tary vectors of the entire space n are the coordinate vectors that have a
single nonzero component, while the elementary vectors of the subspace{
(z1, z2, z3) | z1 + z2 + z3 = 0
}
are the nonzero scalar multiples of the
vectors (1,−1, 0), (1, 0,−1), (0, 1,−1).
For another example that is particularly relevant to network optimiza-
tion, one can verify that the elementary vectors of the circulation subspace
S of a graph (N ,A),
S =
x ∣∣∣ ∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = 0, ∀ i ∈ N
 ,
Sec. 9.7 Monotropic Programming 459
are the simple cycle flows. Let us also consider the subspace that is orthog-
onal to the circulation subspace, given by
S⊥ =
{
t | there exists a price vector p with tij = pi − pj , ∀ (i, j) ∈ A
}
.
To characterize the elementary vectors of S⊥, let us restrict attention to the
case where the graph is connected, and let us consider cuts Q = [S,N −S],
where S is a nonempty subset of nodes of the graph such that the deletion of
all the arcs of Q leaves the graph with exactly two connected components.
Such cuts are called elementary . We leave it as Exercise 9.7 for the reader
to verify that the elementary vectors of S⊥ have components of the form
tij =
 γ if (i, j) ∈ Q
+,
−γ if (i, j) ∈ Q−,
0 otherwise,
where Q is an elementary cut and γ is a nonzero scalar.
Finally, consider an m×n matrix A. It can be seen that the supports
of the elementary vectors of the nullspace of A correspond to the minimal
sets of linearly dependent columns of A. These are subsets of columns
that are linearly dependent, but are such that any one of the columns in
the set can be uniquely expressed as a linear combination of the remaining
columns in the set. It turns out that this example bears an important
relation with linear programming theory and basic solutions of systems of
linear equations.
Several of the distinctive properties of network optimization involving
simple cycles can be extended to monotropic programming using elemen-
tary vectors. For example, the notion of conformal decomposition can be
generalized. In particular, let us say that a vector x is in harmony with a
vector z if
xjzj ≥ 0, ∀ j = 1, . . . , n.
We have the following generalization of the conformal realization theorem
(Prop. 1.1).
Proposition 9.22: (Conformal Realization) Every nonzero vec-
tor x of a given subspace S can be written in the form
x = z1 + · · · + zm,
where m is an integer with m ≤ n, and each of the vectors z1, . . . , zm is
an elementary vector of S that is in harmony with x, and has support
that is contained in the support of x.
460 Convex Separable Network Problems Chap. 9
Proof: We first show that every nonzero vector y ∈ S has the property
that there exists an elementary vector of S that is in harmony with y and
has support that is contained in the support of y.
We show this by induction on the number of nonzero components of
y. Let Vk be the subset of nonzero vectors in S that have k or less nonzero
components, and let k be the smallest k for which Vk is nonempty. Then
the vectors in Vk must be elementary, so every y ∈ Vk has the desired
property. Assume that all vectors in Vk have the desired property for some
k ≥ k. We let y be a vector in Vk+1 and we show that it also has the desired
property. Let z be an elementary vector whose support is contained in the
support of y. By using the negative of z if necessary, we can assume that
yjzj > 0 for at least one index j. Then there exists a largest value of γ,
call it γ, such that
yj − γzj ≥ 0, ∀ j with yj > 0,
yj − γzj ≤ 0, ∀ j with yj < 0.
The vector y − γz is in harmony with y and has support that is strictly
contained in the support of y. Thus either y − γz = 0, in which case
the elementary vector z is in harmony with y and has support equal to
the support of y, or else y − γz is nonzero. In the latter case, we have
y − γz ∈ Vk, and by the induction hypothesis, there exists an elementary
vector z that is in harmony with y − γz and has support that is contained
in the support of y − γz. The vector z is also in harmony with y and has
support that is contained in the support of y. The induction is complete.
Consider now the given nonzero vector x ∈ S, and choose any ele-
mentary vector z1 of S that is in harmony with x and has support that
is contained in the support of x (such a vector exists by the property just
shown). By using the negative of z1 if necessary, we can assume that
xjz
1
j > 0 for at least one index j. Let γ be the largest value of γ such that
xj − γz1j ≥ 0, ∀ j with xj > 0,
xj − γz1j ≤ 0, ∀ j with xj < 0.
The vector x − z1, where
z1 = γ z1,
is in harmony with x and has support that is strictly contained in the
support of x. There are two cases: (1) x = z1, in which case we are done,
or (2) x 	= z1, in which case we replace x by x − z1 and we repeat the
process. Eventually, after m steps where m ≤ n (since each step reduces
the number of nonzero components by at least one), we will end up with
the desired decomposition x = z1 + · · · + zm. Q.E.D.
Sec. 9.7 Monotropic Programming 461
Using the preceding proposition, it is possible to derive a necessary
and sufficient condition for the optimal solution set of a feasible monotropic
programming problem to be nonempty and compact. This condition is that
for all elementary vectors z of S, we have∑
{j|zj>0}
f̂+j zj +
∑
{j|zj<0}
f̂−j zj > 0,
where
f̂+j =
{
limxj→∞ f
+
j (xj) if Xj is unbounded above,
∞ otherwise,
and
f̂−j =
{
limxj→∞ f
−
j (xj) if Xj is unbounded below,
−∞ otherwise.
In the case of a linear network flow problem with nonnegativity constraints
on the arc flows, this condition is equivalent to requiring that all simple
forward cycles have positive cost (see the discussion in the beginning of
Section 5.1).
As another consequence of Prop. 9.22, we derive an interesting algo-
rithmic property of elementary vectors. To place this property in perspec-
tive, consider the subspace S and a closed convex set B, which is disjoint
from S⊥. According to an important theorem from convex analysis (see
e.g., Rockafellar [1970], Luenberger [1984], Bertsekas [1995b]), there exists
a hyperplane that “separates” S⊥ from B in the sense that it contains S⊥
and is disjoint from B; mathematically, this is expressed by saying that
there exists a vector z ∈ S such that t′z < 0 for all t ∈ B. The following
proposition asserts that if B is a Cartesian product of intervals (not nec-
essarily closed) the vector z can be taken to be an elementary vector of
S.
Proposition 9.23: (Combinatorial Separation Theorem) If S
is a subspace and B is a Cartesian product of nonempty intervals, such
that B ∩ S⊥ = Ø, there exists an elementary vector z of S such that
t′z < 0, ∀ t ∈ B.
Proof: For simplicity, assume that B is the Cartesian product of compact
intervals, so that B has the form
B = {t | bj ≤ tj ≤ bj , j = 1, . . . , n},
where bj and bj are some scalars. The proof is easily modified for the case
where B has a different form. As shown in Fig. 9.12, there exists a vector
462 Convex Separable Network Problems Chap. 9
d ∈ S such that t′d < 0 for all t ∈ B, or equivalently
∑
{j|dj>0}
bjdj +
∑
{j|dj<0}
bjdj < 0. (9.53)
Let
d = z1 + · · · + zm,
be a decomposition of d, where z1, . . . , zm are elementary vectors of S that
are in harmony with x, and have supports that are contained in the support
of d, as per Prop. 9.22. Then the condition (9.53) is equivalently written
as
0 >
∑
{j|dj>0}
bjdj +
∑
{j|dj<0}
bjdj
=
∑
{j|dj>0}
bj
(
m∑
i=1
zij
)
+
∑
{j|dj<0}
bj
(
m∑
i=1
zij
)
=
m∑
i=1
 ∑
{j|zi
j
>0}
bjzij +
∑
{j|zi
j
<0}
bjz
i
j
 ,
where the last equality holds because the vectors zi are in harmony with d
and their supports are contained in the support of d. From the preceding
relation, we see that for at least one elementary vector zi, we must have
0 >
∑
{j|zi
j
>0}
bjzij +
∑
{j|zi
j
<0}
bjz
i
j ,
or equivalently
0 > t′zi, ∀ t ∈ B.
Q.E.D.
From Prop. 9.23, we see that the directions used by the -descent
algorithm can be selected from the finite set of elementary directions of S.
By choosing a sufficiently small , we can also see that given a nonoptimal
primal feasible vector x, it is possible to find a descent direction at x
from among the finite set of elementary vectors of the subspace S. This
generalizes a basic network optimization result that we have shown in Prop.
1.2 (see also Props. 8.2 and 9.1), i.e., that at a feasible nonoptimal flow
vector there exists a simple unblocked cycle with nonnegative cost.
Sec. 9.8 Notes, Sources, and Exercises 463
9.8 NOTES, SOURCES, AND EXERCISES
Our development of this chapter follows Rockafellar’s work on monotropic
programming, which was developed in his 1967 and 1969 papers. Rockafel-
lar generalized and refined the important work of Minty [1960], which deals
with the network case and includes most of the material we have presented
in Sections 9.2 and 9.3. The relation between convex network optimization
and equilibrium problems in electrical engineering goes back to the days of
Maxwell for the quadratic cost case, which corresponds to a linear network.
Prior to Minty, extensions to nonlinear networks were carried out by Duffin
[1947], Birkhoff and Diaz [1956], and Dennis [1959]. Rockafellar’s book on
convex analysis [1970] contains detailed background for the material of the
present chapter, including an extensive treatment of conjugate functions
and duality theory for (nonseparable) convex programming problems.
The convergence of the relaxation method for strictly convex network
problems was analyzed by Cottle and Pang [1982], and Bertsekas, Hosein,
and Tseng [1987]. The method is particularly well suited for parallel imple-
mentation, which may also be asynchronous; see Zenios and Mulvey [1986],
Bertsekas and El Baz [1987], Bertsekas and Tsitsiklis [1989], El Baz [1989],
Tseng, Bertsekas, and Tsitsiklis [1990], and Chajakis and Zenios [1991].
An alternative dual ascent method is given by El Baz [1996]; see also El
Baz, Spiteri, Miellou, and Gazen [1996].
The notion of -complementary slackness for convex network prob-
lems was introduced by Bertsekas, Hosein, and Tseng [1987], where it was
used to generalize the relaxation method of Section 6.3 along lines simi-
lar to the -descent method of Section 9.7. The -relaxation and auction
algorithms of Section 9.6, together with the associated complexity analy-
sis, were developed in Bertsekas, Polymenakos, and Tseng [1997a], [1997b],
and in the Ph.D. thesis by Polymenakos [1996]. The paper by Beraldi,
Guerriero, and Musmanno [1996] discusses parallel computation aspects
of the -relaxation method for separable convex problems. A closely re-
lated algorithm to the -relaxation method was given by De Leone, Meyer,
and Zakarian [1996]. The paper by Tseng and Bertsekas [1996] extends
the -relaxation method to convex separable network problems with gains.
Karzanov and McCormick [1997] give another type of scaling algorithm for
convex separable network problems.
The book of Rockafellar [1984] contains an extensive development of
the theory of monotropic programming and its special cases in network
optimization. The theory of elementary vectors was developed in Rockafel-
lar [1969] (see also Rockafellar [1970], [1984]), where the connection with
the theory of oriented matroids was also described. The proof of the du-
ality theorem that we have presented in Section 9.7 is due to Rockafellar
[1981] (see also Rockafellar [1984]). The -descent algorithm used in this
proof is called fortified descent algorithm by Rockafellar. This algorithm,
as well as the use of the -subdifferential in a descent algorithmic context,
464 Convex Separable Network Problems Chap. 9
were first proposed by Bertsekas and Mitter [1971], [1973] for separable and
for general convex programming problems. Generalizations of the simplex,
primal-dual, and out-of-kilter methods to convex separable network prob-
lems and to monotropic programming problems are developed by Rockafel-
lar [1984] using -descent ideas. Various implementations of the -descent
algorithm have also been used for the numerical optimization of nondiffer-
entiable convex functions in the context of the so-called bundle methods,
introduced by Lemarechal [1974] (see e.g., Hiriart-Uruttu and Lemarechal
[1993]). The relaxation method of Section 6.3 was extended to linear pro-
grams by Tseng and Bertsekas [1987], and to monotropic programming
by Tseng and Bertsekas [1990]. There is no known generalization of auc-
tion algorithms to monotropic programming. However, the primal-dual
and out-of-kilter methods were recently extended to monotropic program-
ming by Tseng [1998], using the notion of -complementary slackness, and
a complexity analysis was also given.
E X E R C I S E S
9.1 (Proof of a Weaker Version of the Duality Theorem)
Show that if the primal problem is feasible and the intervals Xij are compact,
then the optimal primal and dual costs are equal (even though the dual problem
may not have an optimal solution). Hint : Let x∗ be a primal optimal solution.
If x∗ is regular, Prop. 9.3 applies and we are done. If x∗ is not regular, there
are arcs (i, j) where regularity is violated by some x∗ij ∈ Xij . For each such arc,
approximate fij near the endpoint(s) where regularity is violated, using convex
functions f
ij
: Xij →  and f ij : Xij →  such that
fij(xij) −  ≤ f
ij
(xij) ≤ fij(xij), ∀ xij ∈ Xij ,
fij(xij) ≤ f ij(xij) ≤ fij(xij) + , ∀ xij ∈ Xij .
The functions f
ij
and f ij should be such that all flows xij ∈ Xij are regular.
Now use Prop. 9.3.
9.2
Consider a problem with two nodes, 1 and 2, and two arcs (1, 2) and (2, 1). The
node supplies are s1 = s2 = 0. The problem is
minimize f12(x12) + f21(x21)
subject to x12 = x21, 0 ≤ x12 < ∞, 0 ≤ x21 < ∞,
Sec. 9.8 Notes, Sources, and Exercises 465
where
f12(x) = f21(x) = −
√
x, x ∈ [0,∞).
Calculate the dual function and verify that the optimal primal and dual costs are
both equal to −∞, consistently with Prop. 9.4.
9.3
Suppose that (x, p) and (x′, p′) are two solutions of the network equilibrium
problem. Show that (x, p′) and (x′, p) are also solutions.
9.4 (Exact Penalty Functions)
Consider a problem where each function fij is convex over the entire real line,
and there is a compact arc flow range xij ∈ [bij , cij ] for each arc (i, j). Suppose
that we modify the problem by eliminating the bound constraints and by adding
to the cost function the following penalty for their violation:
1

∑
(i,j)∈A
(
max{0, bij − xij} + max{0, xij − cij}
)
,
where  is a positive scalar. Use Prop. 9.1 to show that there exists a threshold
 > 0 such that if  ≤ , the optimal solutions of the problem remain unaffected
by the modification.
9.5
Show that in the special case of a compact arc flow range,
Xij = [bij , cij ],
where bij and cij are scalars, the CS condition of Section 9.3 can be written in
terms of the price differentials
tij = pi − pj ,
as
tij ≤ f+ij (bij) ⇒ xij = bij ,
tij ≥ f−ij (cij) ⇒ xij = cij ,
f+ij (bij) < tij < f
−
ij (cij) ⇒ bij < xij < cij and f−ij (xij) ≤ tij ≤ f+ij (xij).
9.6
Modify the example of Fig. 9.4 to show that the duality theorem (Prop. 9.4) need
not hold if the functions fij are not closed.
466 Convex Separable Network Problems Chap. 9
9.7
Consider a connected graph (N ,A), and the subspace{
t | there exists a price vector p with tij = pi − pj , ∀ (i, j) ∈ A
}
.
Show that the elementary vectors of the subspace have components of the form
tij =
{
γ if (i, j) ∈ Q+,
−γ if (i, j) ∈ Q−,
0 otherwise,
where Q is an elementary cut and γ is a nonzero scalar.
10
Network Problems with
Integer Constraints
Contents
10.1. Formulation of Integer-Constrained Problems
10.2. Branch-and-Bound
10.3. Lagrangian Relaxation
10.3.1. Subgradients of the Dual Function
10.3.2. Subgradient Methods
10.3.3. Cutting Plane Methods
10.3.4. Decomposition and Multicommodity Flows
10.4. Local Search Methods
10.4.1. Genetic Algorithms
10.4.2. Tabu Search
10.4.3. Simulated Annealing
10.5. Rollout Algorithms
10.6. Notes, Sources, and Exercises
467
468 Network Problems with Integer Constraints Chap. 10
In this chapter, we focus again on the general nonlinear network problem
of Chapter 8:
minimize f(x)
subject to x ∈ F,
where x is a flow vector in a given directed graph (N ,A), the feasible set
F is
F =
x ∈ X ∣∣∣ ∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N
 ,
and f : F →  is a given real-valued function. Here si are given supply
scalars and X is a given subset of flow vectors. We concentrate on the
case where the feasible set F is discrete because the set X embodies some
integer constraints and possibly some side constraints.
As we noted in Chapter 8, one may solve approximately problems
with integer constraints and side constraints through some heuristic that
neglects in one way or another the integer constraints. In particular, one
may solve the problem as a “continuous” network flow problem and use
some ad hoc method to round the fractional solution to integer. Alterna-
tively, one may discard the complicating side constraints, obtain an integer
solution of the resulting network problem, and use some heuristic to correct
this solution for feasibility of the violated side constraints.
Unfortunately, there are many problems where heuristic methods of
this type are inadequate, and they cannot be relied upon to produce a
satisfactory solution. In such cases, one needs to strengthen the heuristics
with more systematic procedures that can provide some assurance of an
improved solution.
In this chapter we first describe a few examples of integer-constrained
network problems, and we then focus on various systematic solution meth-
ods. In particular, in Section 10.2, we discuss the branch-and-bound method,
which is in principle capable of producing an exactly optimal solution to an
integer-constrained problem. This method relies on upper and lower bound
estimates of the optimal cost of various problems that are derived from the
given problem. Usually, the upper bounds are obtained with various heuris-
tics, while the lower bounds are obtained through integer constraint relax-
ation or through the use of duality. A popular method for obtaining lower
bounds, the Lagrangian relaxation method, is introduced in Section 10.3.
This method requires the optimization of nondifferentiable functions, and
two of the major algorithms that can be used for this purpose, subgradient
and cutting plane methods, are discussed in Section 10.3.
Unfortunately, the branch-and-bound method is too time-consuming
for exact optimal solution, so in many practical problems it can only be
used as an approximation scheme. There are alternative possibilities, which
do not offer the theoretical guarantees of branch-and-bound, but are much
Sec. 10.1 Formulation of Integer-Constrained Problems 469
faster in practice. Two possibilities of this type, local search methods and
rollout algorithms, are discussed in Sections 10.4 and 10.5, respectively.
10.1 FORMULATION OF INTEGER-CONSTRAINED PROBLEMS
There is a very large variety of integer-constrained network flow problems.
Furthermore, small changes in the problem formulation can often make a
significant difference in the character of the solution. As a result, it is not
easy to provide a taxonomy of the major problems of interest. It is helpful,
however, to study in some detail a few representative examples that can
serve as paradigms when dealing with other problems that have similar
structure. We have already discussed in Section 8.4 an example, the con-
strained shortest path problem. In this section, we provide some additional
illustrative examples of broad classes of integer-constrained problems. In
the exercises, we discuss several variants of these problems.
Example 10.1. Traveling Salesman Problem
An important model for scheduling a sequence of operations is the classical
traveling salesman problem. This is perhaps the most studied of all combi-
natorial optimization problems. In addition to its use as a practical model, it
has served as a testbed for a large variety of formal and heuristic approaches
in discrete optimization.
In a colloquial description of the problem, a salesman wants to find a
minimum mileage/cost tour that visits each of N given cities exactly once
and returns to the city he started form. We associate a node with each city
i = 1, . . . , N , and we introduce an arc (i, j) with traversal cost aij for each
ordered pair of nodes i and j. Note that we assume that the graph is complete;
that is, there exists an arc for each ordered pair of nodes. There is no loss of
generality in doing so because we can assign a very high cost aij to an arc (i, j)
that is precluded from participation in the solution. We allow the possibility
that aij = aji. Problems where aij = aji for all i and j are sometimes called
symmetric or undirected traveling salesman problems, because the direction
of traversal of a given arc does not matter.
A tour (also called a Hamiltonian cycle; see Section 1.1) is defined to be
a simple forward cycle that contains all the nodes of the graph. Equivalently,
a tour is a connected subgraph that consists of N arcs, such that there is
exactly one incoming and one outgoing arc for each node i = 1, . . . , N . If we
define the cost of a subgraph T to be the sum of the traversal costs of its arcs,∑
(i,j)∈T
aij ,
the traveling salesman problem is to find a tour of minimum cost.
470 Network Problems with Integer Constraints Chap. 10
We formulate this problem as a network flow problem with node set
N = {1, . . . , N} and arc set A =
{
(i, j) | i, j = 1, . . . , N, i = j
}
, and with
side constraints and 0-1 integer constraints:
minimize
∑
(i,j)∈A
aijxij
subject to
∑
j=1,...,N
j =i
xij = 1, i = 1, . . . , N,
∑
i=1,...,N
i=j
xij = 1, j = 1, . . . , N,
xij = 0 or 1, ∀ (i, j) ∈ A,
the subgraph with node-arc set
(
N , {(i, j) | xij = 1}
)
is connected. (10.1)
Note that, given the 0-1 constraints on the arc flows and the conservation of
flow equations, the last constraint can be expressed through the set of side
constraints∑
i∈S, j /∈S
(xij + xji) ≥ 2, ∀ nonempty proper subsets S of nodes.
If these constraints were not present, the problem would be an ordinary as-
signment problem. Unfortunately, however, these constraints are essential,
since without them, there would be feasible solutions involving multiple dis-
connected cycles, as illustrated in Fig. 10.1.
1
2
3
4
5
6
8
7
Figure 10.1: Example of an infeasible
solution of a traveling salesman prob-
lem where all the constraints are sat-
isfied except for the connectivity con-
straint (10.1). This solution may have
been obtained by solving an N ×N as-
signment problem and consists of mul-
tiple cycles [(1,2,3), (4,5,6), and (7,8)
in the figure]. The arcs of the cycles
correspond to the assigned pairs (i, j)
in the assignment problem.
A simple approach for solving the traveling salesman problem is the
nearest neighbor heuristic. We start from a path consisting of just a single
node i1 and at each iteration, we enlarge the path with a node that does not
close a cycle and minimizes the cost of the enlargement. In particular, after
k iterations, we have a forward path {i1, . . . , ik} consisting of distinct nodes,
and at the next iteration, we add an arc (ik, ik+1) that minimizes aiki over all
arcs (ik, i) with i = i1, . . . , ik. After N − 1 iterations, all nodes are included
in the path, which is then converted to a tour by adding the final arc (iN , i1).
Sec. 10.1 Formulation of Integer-Constrained Problems 471
Given a tour, one may try to improve its cost by using some method
that changes the tour incrementally. In particular, a popular method for the
symmetric case (aij = aji for all i and j) is the k-OPT heuristic, which creates
a new tour by exchanging k arcs of the current tour with another k arcs that
do not belong to the tour (see Fig. 10.2). The k arcs are chosen to optimize
the cost of the new tour with O(Nk) computation. The method stops when
no improvement of the current tour is possible through a k-interchange.
i j
ji
Figure 10.2: Illustration of the 2-OPT heuristic
for improving a tour of the symmetric traveling
salesman problem. The arcs (i, j) and (i, j) are
interchanged with the arcs (i, j) and (i, j). The
choice of (i, j) and (i, j) is optimized over all pairs
of nonadjacent arcs of the tour.
Another possibility for constructing an initial tour is the following two-
step method:
(1) Discard the side constraints (10.1), and from the resulting assignment
problem, obtain a solution consisting of a collection of subtours such
as the ones shown in Fig. 10.1. More generally, use some method to
obtain a “reasonable” collection of subtours such that each node lies on
exactly one subtour.
(2) Use some heuristic to create a tour by combining subtours. For example,
any two subtours T and T can be merged into a single subtour by
selecting a node i ∈ T and a node i ∈ T , adding the arc (i, i), deleting
the unique outgoing arc (i, j) of i on the subtour T and the unique
incoming arc (j, i) of j on the subtour T , and finally adding the arc
(j, j), as shown in Fig. 10.3. The pair of nodes i and i can be chosen
to minimize the cost of the created subtour. This optimization requires
O(mn) computation, where m and n are the numbers of nodes in T
and T , respectively.
Still another alternative for constructing an initial tour, is to start with
some spanning tree and to gradually convert it into a tour. There are quite a
few heuristics based on this idea; see e.g., the book by Nemhauser and Wolsey
[1988], the survey by Junger, Reinelt, and Rinaldi [1995], and the references
quoted there. Unfortunately, there are no heuristics with practically useful
performance guarantees for the general traveling salesman problem (Sahni
and Gonzalez [1976], and Johnson and Papadimitriou [1985] make this point
precise). The situation is better, however, for some special types of symmetric
472 Network Problems with Integer Constraints Chap. 10
i
j
i
j
Subtour T Subtour T
Figure 10.3: Merging two subtours
T and T into a single subtour by se-
lecting two nodes i ∈ T and i ∈ T ,
and adding and deleting the appro-
priate arcs of T and T .
problems where the arc costs satisfy the relation
aij ≤ aik + akj , for all nodes i, j, k.
known as the triangle inequality (see Exercises 10.7-10.8).
Example 10.2. Fixed Charge Problems
A fixed charge problem is a minimum cost flow problem where there is an
extra cost bij for each arc flow xij that is positive (in addition to the usual
cost aijxij). Thus bij may be viewed as a “purchase cost” for acquiring the
arc (i, j) and using it to carry flow.
An example of a fixed charge problem is the facility location problem,
where we must select a subset of locations from a given candidate set, and
place in each of these locations a “facility” that will serve the needs of certain
“clients.” There is a 0-1 decision variable associated with selecting any given
location for facility placement, at a given cost. Once these variables are cho-
sen, an assignment (or transportation) problem must be solved to optimally
match clients with facilities. Mathematically, we assume that there are m
clients and n locations. By xij = 1 (or xij = 0) we indicate that client i is
assigned to location j at a cost aij (or is not assigned, respectively). We also
introduce a 0-1 integer variable yj to indicate (with yj = 1) that a facility is
placed at location j at a cost bj . The problem is
minimize
∑
(i,j)∈A
aijxij +
n∑
j=1
bjyj
subject to
∑
{j|(i,j)∈A}
xij = 1, i = 1, . . . , m,∑
{i|(i,j)∈A}
xij ≤ yjcj , j = 1, . . . , n,
xij = 0 or 1, ∀ (i, j) ∈ A,
yj = 0 or 1, j = 1, . . . , n,
where cj is the maximum number of customers that can be served by a facility
at location j.
Sec. 10.1 Formulation of Integer-Constrained Problems 473
We can formulate this problem as a network flow problem with side
constraints and integer constraints. In particular, we can view xij as the arc
flows of the graph of a transportation problem (with inequality constraints).
We can also view yj as the arc flows of an artificial graph that is disconnected
from the transportation graph, but is coupled to it through the side con-
straints
∑
i
xij ≤ yjcj (see Fig. 10.4). This formulation does not necessarily
facilitate the algorithmic solution of the problem, but serves to illustrate the
generality of our framework for network problems with side constraints.
1
i
1
j
n
m
aij
1
1
1
CLIENTS LOCATIONS
mxij
1
j
n
1
j
n
yj
bj
...
...
...
...
...
...
...
...
Figure 10.4: Formulation of the facility location problem as a network flow
problem with side constraints and 0-1 integer constraints. There are two
disconnected subgraphs: the first is a transportation-like graph that involves
the flow variables xij and the second is an artificial graph that involves the
flow variables yj . The arc flows of the two subgraphs are coupled through the
side constraints
∑
i
xij ≤ yjcj .
Example 10.3. Optimal Tree Problems
There are many network applications where one needs to construct an optimal
tree subject to some constraints. For example, in data networks, a spanning
tree is often used to broadcast information from some central source to all
the nodes. In this context, it makes sense to assign a cost or weight aij to
each arc (communication link) (i, j) and try to find a spanning tree that has
minimum total weight (minimum sum of arc weights). This is the minimum
weight spanning tree problem, which we have briefly discussed in Chapter 2
(see Exercise 2.30).
We can formulate this problem as an integer-constrained problem in
several ways. For example, let xij be a 0-1 integer variable indicating whether
arc (i, j) belongs to the spanning tree. Then the problem can be written as
minimize
∑
(i,j)∈A
aijxij
subject to
∑
(i,j)∈A
xij = N − 1,
474 Network Problems with Integer Constraints Chap. 10∑
i∈S, j /∈S
(xij + xji) ≥ 1, ∀ nonempty proper subsets S of nodes,
xij = 0 or 1, ∀ (i, j) ∈ A.
The first two constraints guarantee that the graph defined by the set {(i, j) |
xij = 1} has N − 1 arcs and is connected, so it is a spanning tree.
In Exercise 2.30, we discussed how the minimum weight spanning tree
problem can be solved with a greedy algorithm. An example is the Prim-
Dijkstra algorithm, which builds an optimal spanning tree by generating a
sequence of subtrees. It starts with a subtree consisting of a single node
and it iteratively adds to the current subtree an incident arc that has min-
imum weight over all incident arcs that do not close a cycle. We indicated
in Exercise 2.30 that this algorithm can be implemented so that it has an
O(N2) running time. This is remarkable, because except for the minimum
cost flow problems discussed in Chapters 2-7, very few other types of network
optimization problems can be solved with a polynomial-time algorithm.
There are a number of variations of the minimum weight spanning tree
problem. Here are some examples:
(a) There is a constraint on the number of tree arcs that are incident to
a single given node. This is known as the degree constrained minimum
weight spanning tree problem. It is possible to solve this problem using a
polynomial version of the greedy algorithm (see Exercise 10.10). On the
other hand, if there is a degree constraint on every node, the problem
turns out to be much harder. For example, suppose that the degree of
each node is constrained to be at most 2. Then a spanning tree subject
to this constraint must be a path that goes through each node exactly
once, so the problem is essentially equivalent to a symmetric traveling
salesman problem (see Exercise 10.6).
(b) The capacitated spanning tree problem. Here the arcs of the tree are to
be used for routing specified supplies from given supply nodes to given
demand nodes. The tree specifies the routes that will carry the flow
from the supply points to the demand points, and hence also specifies
the corresponding arc flows. We require that the tree is selected so
that the flow of each arc does not exceed a given capacity constraint.
This is an integer-constrained problem, which is not polynomially solv-
able. However, there are some practical heuristic algorithms, such as
an algorithm due to Esau and Williams [1966] (see Fig. 10.5).
(c) The Steiner tree problem, where the requirement that all nodes must be
included in the tree is relaxed. Instead, we are given a subset S of the
nodes, and we want to find a tree that includes the subset S and has
minimum total weight. [J. Steiner (1796-1863), “the greatest geome-
ter since Apollonius,” posed the problem of finding the shortest tree
spanning a given set of points on the plane.] An important application
of the Steiner tree problem arises in broadcasting information over a
communication network from a special node to a selected subset S of
nodes. This broadcasting is most efficiently done over a Steiner tree,
where the cost of each arc corresponds to the cost of communication
over that arc. The Steiner tree problem also turns out to be a difficult
Sec. 10.1 Formulation of Integer-Constrained Problems 475
1
0
2 3
4 1
0
2 3
4
1
0
2 3
4 1
0
2 3
41
0
2 3
4
Spanning Tree Problem
Arc Capacities = 8
1
1
1
99
2
2
2
5
3
5
5
Optimal Spanning Tree 
Starting Tree After 1 Iteration After 2 Iterations
(Final Solution)
Figure 10.5: The Esau-Williams heuristic for solving a capacitated minimum
weight spanning tree problem. Each arc (i, j) has a cost (or weight) aij and a
capacity cij . The problem is symmetric, so that aij = aji and cij = cji. We
assume that the graph is complete [if some arcs (i, j) do not exist, we introduce
them artificially with a very large cost and infinite capacity]. There is a special
concentrator node 0, and for every other node i = 1, . . . , N , there is a supply
si ≥ 0 that must be transferred to node 0 along the arcs of the spanning tree
without violating the arc capacity constraints. The Esau-Williams algorithm
generates a sequence of feasible spanning trees, each having a lower cost than
its predecessor, by using an arc exchange heuristic. In particular, we start
with a spanning tree where the concentrator node 0 is directly connected with
each of the N other nodes, as in the bottom left figure [we assume that the
arcs (i, 0) can carry at least the supply of node i, that is, ci0 ≥ si]. At each
successive iteration, an arc (i, 0) is deleted from the current spanning tree,
and another arc (i, j) is added, so that:
(1) No cycle is formed.
(2) The capacity constraints of all the arcs of the new spanning tree are
satisfied.
(3) The saving ai0−aij in cost obtained by exchanging arcs (i, 0) and (i, j)
is positive and is maximized over all nodes i and and j for which (1)
and (2) above are satisfied.
The figure illustrates the algorithm, for the problem shown at the top left,
where the cost of each arc is shown next to each arc, the capacity of each
arc is 8, and the supplies of the nodes i > 0 are shown next to the arrows.
The algorithm terminates after two iterations with the tree shown, which has
a total cost of 13. Termination occurs because when arc (1, 0) or (4, 0) is
removed and an arc that is not incident to node 0 is added, some arc capacity
is violated. The optimal spanning tree has cost equal to 12.
476 Network Problems with Integer Constraints Chap. 10
integer-constrained problem, for which, however, effective heuristics are
available (see Exercise 10.11). Note that there are degree-constrained
and capacitated versions of the problem, as in (a) and (b) above.
Example 10.4. Matching Problems
A matching problem involves dividing a collection of objects into pairs. There
may be some constraints regarding the objects that can be paired, and there
is a benefit or value associated with matching each of the eligible pairs. The
objective is to find a matching of maximal total value. We have already stud-
ied extensively special cases of matching, namely the assignment problems
of Chapter 7, which are also called bipartite matching problems. These are
matching problems where the objects are partitioned in two groups, and pairs
must involve only one element from each group. Matching problems where
there is no such partition are called nonbipartite.
To pose a matching problem as a network flow problem, we introduce
a graph (N ,A) that has a node for each object, and an arc (i, j) of value aij
connecting any two objects i and j that can be paired. The orientation of
this arc does not matter [alternatively, we may introduce both arcs (i, j) and
(j, i), and assign to them equal values]. We consider a flow variable xij for
each arc (i, j), where xij is 1 or 0 depending on whether objects i and j are
matched or not, respectively. The objective is to maximize∑
(i,j)∈A
aijxij
subject to the constraints∑
{j|(i,j)∈A}
xij +
∑
{j|(j,i)∈A}
xji ≤ 1, ∀ i ∈ N , (10.2)
xij = 0 or 1, ∀ (i, j) ∈ A.
The constraint (10.2) expresses the requirement that an object can be matched
with at most one other object. In a variant of the problem, it is specified that
the matching should be perfect ; that is, every object should be matched with
some other object. In this case, the constraint (10.2) should be changed to∑
{j|(i,j)∈A}
xij +
∑
{j|(j,i)∈A}
xji = 1, ∀ i ∈ N . (10.3)
The special case where aij = 1 for all arcs (i, j) is the maximum cardinal-
ity matching problem, i.e., finding a matching with a maximum number of
matched pairs.
It is possible to view nonbipartite matching as an optimal network flow
problem of the assignment type with integer constraints and with the side con-
straints defined by Eq. (10.2) or Eq. (10.3) (see Exercise 10.15). We would
Sec. 10.1 Formulation of Integer-Constrained Problems 477
thus expect that the problem is a difficult one, and that it is not polynomi-
ally solvable (cf. the discussion of Section 8.4). However, this is not so. It
turns out that nonbipartite matching has an interesting and intricate struc-
ture, which is quite unique among combinatorial and network optimization
problems. In particular, nonbipartite matching problems can be solved with
polynomial-time algorithms. These algorithms share some key structures with
their bipartite counterparts, such as augmenting paths, but they generally be-
come simpler and run faster when specialized to bipartite matching. One such
algorithm, due to Edmonds [1965] can be implemented so that it has O(N3)
running time. Furthermore, nonbipartite matching can be formulated as a
linear program without integer constraints, and admits an analysis based on
linear programming duality. We refer to the literature cited at the end of the
chapter for an account.
Example 10.5. Vehicle Routing Problems
In vehicle routing problems, there is a fleet of vehicles that must pick up a
number of “customers” (e.g., persons, packages, objects, etc.) from various
nodes in a transportation network and deliver them at some other nodes using
the network arcs. The objective is to minimize total cost subject to a variety
of constraints. The cost here may include, among other things, transportation
cost, and penalties for tardiness of pickup and delivery. The constraints may
include vehicle capacity, and pickup and delivery time restrictions.
Vehicle routing problems are among the hardest integer programming
problems because they tend to have a large number of integer variables, and
also because they involve both a resource allocation and a scheduling aspect.
In particular, they combine the difficult combinatorial aspects of two problems
that we have already discussed:
(a) The generalized assignment problem discussed in Section 8.5 (determine
which vehicles will service which customers).
(b) The traveling salesman problem discussed in Example 10.1 (determine
the sequence of customer pickups and deliveries by a given vehicle).
In fact, the traveling salesman problem may itself be viewed as a sim-
ple version of the vehicle routing problem, involving a single vehicle
of unlimited capacity, N customers that must be picked up in some
unspecified order, and a travel cost aij from customer i to customer j.
For a common type of vehicle routing problem, suppose that there are
K vehicles (denoted 1, . . . , K) with corresponding capacities c1, . . . , cK , which
make deliveries to N customers (nodes 1, . . . , N) starting from a central depot
(node 0). The delivery to customer i is of given size di, and the cost of
traveling from node i to node j is denoted by aij . The problem is to find the
route of each vehicle (a cycle of nodes starting from node 0 and returning to
0), that satisfies the customer delivery constraints, and the vehicle capacity
constraints.
There are several heuristic approaches for solving this problem, some
of which bear similarity to the heuristic approaches for solving the traveling
salesman problem. For example, one may start with some set of routes, which
478 Network Problems with Integer Constraints Chap. 10
may be infeasible because their number may exceed the number of vehicles K.
One may then try to work towards feasibility by combining routes in a way
that satisfies the vehicle capacity constraints, while keeping the cost as small
as possible. Alternatively, one may start with a solution of a K-traveling
salesmen problem (see Exercise 10.9), corresponding to the K vehicles, and
then try to improve on this solution by interchanging customers between
routes, while trying to satisfy the capacity constraints. These heuristics often
work well, but generally they offer no guarantee of good performance, and
may occasionally result in a solution that is far from optimal.
An alternative possibility, which is ultimately also based on heuristics,
is to formulate the problem mathematically in a way that emphasizes its
connections to both the generalized assignment problem and the traveling
salesman problem. In particular, we introduce the integer variables
yik =
{
1 if node i is visited by vehicle k,
0 otherwise,
and the vectors yk = (y1k, . . . , yNk). For each k = 1, . . . , K, let fk(yk) denote
the optimal cost of a traveling salesman problem involving the set of nodes
Nk(yk) = {i | yik = 1}.
We can pose the problem as
minimize
K∑
k=1
fk(yk)
subject to
K∑
k=1
yik =
{
K if i = 0,
1 if i = 1, . . . , N,
N∑
i=0
diyik ≤ ck, k = 1, . . . , K,
yik = 0 or 1, i = 0, . . . , N, k = 1, . . . , N,
which is a generalized assignment problem (see Section 8.5).
The difficulty with the generalized assignment formulation is that the
functions fk are generally unknown. It is possible, however, to try to approx-
imate heuristically these functions with some linear functions of the form
f̃k(yk) =
N∑
i=0
wikyik,
solve the corresponding generalized assignment problems for the vectors yk,
and then solve the corresponding traveling salesman problems. The weights
wik can be determined in some heuristic way. For example, first specify a
“seed” customer ik to be picked up by vehicle k, and then set
wik = a0i + aiik − a0ik ,
Sec. 10.1 Formulation of Integer-Constrained Problems 479
which is the incremental cost of inserting customer i into the route 0 → ik →
0. The seed customers specify the general direction of the route taken by
vehicle k, and the weight wik represents the approximate cost for picking up
customer i along the way. One may select the seed customers using one of a
number of heuristics, for which we refer to the literature cited at the end of
the chapter.
There are several extensions and more complex variants of the preceding
vehicle routing problems. For example:
(a) Some of the customers may have a “time window,” in the sense that
they may be served only within a given time interval. Furthermore, the
total time duration of a route may be constrained.
(b) There may be multiple depots, and each vehicle may be restricted to
start from a given subset of the depots.
(c) Delivery to some of the customers may not be required. Instead there
may be a penalty for nondelivery or for tardiness of delivery (in the
case where there are time windows).
(d) There may be precedence constraints, requiring that some of the cus-
tomers be served before some others.
With additional side constraints of the type described above, the prob-
lem can become very complex. Nonetheless, with a combination of heuristics
and the more formal approaches to be described in this chapter, some measure
of success has been obtained in solving practical vehicle routing problems.
Example 10.6. Arc Routing Problems
Arc routing problems are similar to vehicle routing problems, except that
the emphasis regarding cost and constraints is placed on arc traversals rather
than node visits. Here each arc (i, j) has a cost aij , and we want to find a
set of arcs that satisfy certain constraints and have minimum sum of costs.
For example, a classical arc routing problem is the Chinese postman problem,
where we want to find a cycle that traverses every arc of a graph, and has
minimum sum of arc costs; here traversals in either direction and multiple
traversals are allowed.† The costs of all arcs must be assumed nonnegative
here in order to guarantee that the problem has an optimal solution (otherwise
cycles of arbitrarily small cost would be possible by crossing back and forth
an arc of negative cost).
An interesting related question is whether there exists an Euler cycle
in the given graph, i.e., a cycle that contains every arc exactly once, with
arc traversals in either direction allowed (such a cycle, if it exists, solves the
Chinese postman problem since the arc costs are assumed nonnegative). This
† An analogy here is made with a postman who must traverse each arc of the
road network of some town (in at least one direction), while walking the minimum
possible distance. The problem was first posed by the Chinese mathematician
Kwan Mei-Ko [1962].
480 Network Problems with Integer Constraints Chap. 10
question was posed by Euler in connection with the famous Königsberg bridge
problem (see Fig. 10.6). The solution is simple: there exists an Euler cycle
if and only if the graph is connected and every node has even degree (in an
Euler cycle, the number of entrances to a node must be equal to the number
of exits, so the number of incident arcs to each node must be even; for a proof
of the converse, see Exercise 1.5). It turns out that even when there are nodes
of odd degree, a solution to the Chinese postman problem can be obtained
by constructing an Euler cycle in an expanded graph that involves some ad-
ditional arcs. These arcs can be obtained by solving a nonbipartite matching
problem involving the nodes of odd degree (see Exercise 10.17). Thus, since
the matching problem can be solved in polynomial time as noted in Example
10.4, the Chinese postman problem can also be solved in polynomial time
(see also Edmonds and Johnson [1973], who explored the relation between
matching and the Chinese postman problem).
A
C
A
B
C
D
River PregelB
D
Figure 10.6: The Königsberg bridge problem, generally considered to mark
the origin of graph theory. Euler attributed this problem to the citizens of
Königsberg, an old port town that lies north of Warsaw on the Baltic sea
(it is now called Kaliningrad). The problem, addressed by Euler in 1736, is
whether it is possible to cross each of the seven bridges of the river Pregel
in Königsberg exactly once, and return to the starting point. In the graph
representation of the problem, shown in the figure, each bridge is associated
with an arc, and each node is associated with a land area that is incident
to several bridges. The question amounts to asking whether an Euler cycle
exists. The answer is negative since there are nodes with odd degree.
There is also a “directed” version of the Chinese postman problem,
where we want to find a forward cycle that traverses every arc of a graph
(possibly multiple times), and has minimum sum of arc costs. It can be seen
that this problem has a feasible solution if and only if the graph is strongly
connected, and that it has an optimal solution if in addition all forward
cycles have nonnegative cost. The problem is related to the construction
Sec. 10.1 Formulation of Integer-Constrained Problems 481
of forward Euler cycles, in roughly the same way as the undirected Chinese
postman problem was related above to the construction of an (undirected)
Euler cycle. Exercise 1.8 states the basic result about the existence of a
forward Euler cycle: such a cycle exists if and only if the number of incoming
arcs to each node is equal to the number of its outgoing arcs. A forward Euler
cycle, if it exists, is also a solution to the directed Chinese postman problem.
More generally, it turns out that a solution to the directed Chinese postman
problem (assuming one exists) can be obtained by finding a directed Euler
cycle in an associated graph obtained by solving a certain minimum cost flow
problem (see Exercise 10.17).
By introducing different constraints, one may obtain a large variety of
arc routing problems. For example, a variant of the Chinese postman problem
is to find a cycle of minimum cost that traverses only a given subset of the arcs.
This is known as the rural postman problem. Other variants are characterized
by arc time-windows and arc precedence constraints, similar to vehicle routing
problem variants discussed earlier. In fact, it is always possible to convert
an arc routing problem to a “node routing problem,” where the constraints
are placed on some of the nodes rather than on the arcs. This can be done
by replacing each arc (i, j) with two arcs (i, kij) and (kij , j) separated by an
artificial middle node kij . Traversal of an arc (i, j) then becomes equivalent
to visiting the artificial node kij . However, this transformation often masks
important characteristics of the problem. For example it would be awkward
to pose the question of existence of an Euler cycle as a node routing problem.
Example 10.7. Multidimensional Assignment Problems
In the assignment problems we have considered so far, we group the nodes of
the graph in pairs. Multidimensional assignment problems involve the group-
ing of the nodes in subsets with more than two elements, such as triplets or
quadruplets of nodes. For an example of a 3-dimensional assignment problem,
suppose that the performance of a job j requires a machine m and a worker
w, and that there is a given value ajmw corresponding to the triplet (j, m, w).
Given a set of jobs J , a set of machines M , and a set of workers W , we want
to find a collection of job/machine/worker triplets that has maximum total
value.
To pose this problem mathematically, we introduce 0-1 integer variables
xjmw =
{
1 if job j is performed at machine m by worker w,
0 otherwise,
and we maximize ∑
j∈J
∑
m∈M
∑
w∈W
ajmwxjmw
subject to standard assignment constraints. In particular, if the numbers of
jobs, machines, and workers are all equal, and all jobs must be assigned, we
have the constraints ∑
m∈M
∑
w∈W
xjmw = 1, ∀ j ∈ J,
482 Network Problems with Integer Constraints Chap. 10∑
j∈J
∑
w∈W
xjmw = 1, ∀ m ∈ M,
∑
j∈J
∑
m∈M
xjmw = 1, ∀ w ∈ W.
In alternative formulations, some of these constraints may involve inequalities.
An important and particularly favorable special case of the problem
arises when the values ajmw have the separable form
ajmw = βjm + γmw,
where βjm and γmw are given scalars. In this case, there is no coupling
between jobs and workers, and the problem can be solved by solving two
decoupled (2-dimensional) assignment problems: one involving the pairing of
jobs and machines, with the βjm as values, and the other involving the pairing
of machines and workers, with the γmw as values. In general, however, the 3-
dimensional assignment problem is a difficult integer programming problem,
for which there is no known polynomial algorithm.
A simple heuristic approach is based on relaxing each of the constraints
in turn. In particular, suppose that the constraint on the workers is neglected
first. It can then be seen that the problem takes the 2-dimensional assignment
form
maximize
∑
j∈J
∑
m∈M
bjmyjm
subject to
∑
m∈M
yjm = 1, ∀ j ∈ J,∑
j∈J
yjm = 1, ∀ m ∈ M,
yjm = 0 or 1, ∀ j ∈ J, m ∈ M,
where
bjm = max
w∈W
ajmw, (10.4)
and yjm = 1 indicates that job j must be performed at machine m. For each
j ∈ J , let jm be the job assigned to machine m, according to the solution of
this problem. We can now optimally assign machines m to workers w, using
as assignment values
cmw = ajmmw,
and obtain a 3-dimensional assignment {(jm, m, wm) | m ∈ M}. It can be
seen that this approach amounts to enforced separation, whereby we replace
the values ajmw with the separable approximations bjm + cmw. In fact, it
can be shown that if the problem is -separable, in the sense that for some
(possibly unknown) βjm and γmw, and some  ≥ 0, we have
|βjm + γmw − ajmw| ≤ , ∀ j ∈ J, m ∈ M, w ∈ W,
Sec. 10.2 Branch-and-Bound 483
then the assignment {(jm, m, wm) | m ∈ M} obtained using the preceding
enforced separation approach achieves the optimal value of the problem within
4n, where n is the cardinality of the sets J , M , and W (see Exercise 10.31).
The enforced separation approach is simple and can be generalized to
multidimensional assignment problems of dimension more than 3. However,
it often results in significant loss of optimality. A potential improvement is
to introduce some corrections to the values bjm that reflect some dependence
on the values of workers. For example, we can use instead of the values bjm
of Eq. (10.4), the modified values
b̂jm = max
w∈W
{ajmw − µw},
where µw is a nonnegative scalar that can be viewed as a wage to be paid
to worker w. This allows the possibility of adjusting the scalars µw to some
“optimal” values. Methods for doing this will be discussed in Section 10.3 in
the context of the Lagrangian relaxation method, where we will view µw as a
Lagrange multiplier corresponding to the constraint
∑
j∈J
∑
m∈M xjmw = 1.
There are several extensions of the multidimensional assignment prob-
lem. For example, we may have transportation constraints, where multiple
jobs can be performed on the same machine, and/or multiple machines can
be operated by a single worker. In this case, our preceding discussion of the
enforced separation heuristic applies similarly. We may also have generalized
assignment constraints such as∑
j∈J
∑
w∈W
gjmwxjmw ≤ 1, ∀ m ∈ M,
where gjmw represents the portion of machine m needed to perform job j by
worker w. In this case, the enforced separation heuristic results in difficult
integer-constrained generalized assignment problems, which we may have to
solve heuristically. Alternatively, we may use the more formal methodology
of the next two sections.
10.2 BRANCH-AND-BOUND
The branch-and-bound method implicitly enumerates all the feasible so-
lutions, using calculations where the integer constraints of the problem
are relaxed. The method can be very time-consuming, but is in principle
capable of yielding an exactly optimal solution.
To describe the branch-and-bound method, consider the general dis-
crete optimization problem
minimize f(x)
subject to x ∈ F,
484 Network Problems with Integer Constraints Chap. 10
where the feasible set F is a finite set. The branch-and-bound algorithm
uses an acyclic graph known as the branch-and-bound tree, which corre-
sponds to a progressively finer partition of F . In particular, the nodes of
this graph correspond to a collection F of subsets of F , which is such that:
1. F ∈ F (i.e., the set of all solutions is a node).
2. If x is a feasible solution, then {x} ∈ F (i.e., each solution viewed as
a singleton set is a node).
3. If a set Y ∈ F contains more than one solution x ∈ F , then there
exist disjoint sets Y1, . . . , Yn ∈ F such that
n⋃
i=1
Yi = Y.
The set Y is called the parent of Y1, . . . , Yn, and the sets Y1, . . . , Yn
are called the children or descendants of Y .
4. Each set in F other than F has a parent.
The collection of sets F defines the branch-and-bound tree as in Fig. 10.7.
In particular, this tree has the set of all feasible solutions F as its root node
and the singleton solutions {x}, x ∈ F , as terminal nodes. The arcs of the
graph are those that connect parents Y and their children Yi.
The key assumption in the branch-and-bound method is that for every
nonterminal node Y , there is an algorithm that calculates:
(a) A lower bound f
Y
to the minimum cost over Y
f
Y
≤ min
x∈Y
f(x).
(b) A feasible solution x ∈ Y , whose cost f(x) can serve as an upper
bound to the optimal cost of the original problem minx∈F f(x).
The main idea of the branch-and-bound algorithm is to save computation
by discarding the nodes/subsets of the tree that have no chance of con-
taining an optimal solution. In particular, the algorithm selects nodes Y
from the branch-and-bound tree, and checks whether the lower bound f
Y
exceeds the best available upper bound [the minimal cost f(x) over all fea-
sible solutions x found so far]. If this is so, we know that Y cannot contain
an optimal solution, so all its descendant nodes in the tree need not be
considered further.
To organize the search through the tree, the algorithm maintains a
node list called OPEN, and also maintains a scalar called UPPER, which
is equal to the minimal cost over feasible solutions found so far. Initially,
OPEN contains just F , and UPPER is equal to ∞ or to the cost f(x) of
some feasible solution x ∈ F .
Sec. 10.2 Branch-and-Bound 485
F = {1,2,3,4,5}
{4}
Y = {1,2,3} {4,5}
{ } {5}
{1}
Y1 = {1,2} Y2 = {3}
Lower Bound fY_ _
Feasible Solution x  ∈ Y
{2}
Figure 10.7: Illustration of a branch-and-bound tree. Each node Y (a subset
of the feasible set F ), except those consisting of a single solution, is partitioned
into several other nodes (subsets) Y1, . . . , Yn. The original feasible set is divided
repeatedly into subsets until no more division is possible. For each node/subset
Y of the tree, one may compute a lower bound f
Y
to the optimal cost of the
corresponding restricted subproblem minx∈Y f(x), and a feasible solution x ∈ Y ,
whose cost can serve as an upper bound to the optimal cost minx∈F f(x) of the
original problem. The idea is to use these bounds to economize computation by
eliminating nodes of the tree that cannot contain an optimal solution.
Branch-and-Bound Algorithm
Step 1: Remove a node Y from OPEN. For each child Yj of Y , do the
following: Find the lower bound f
Yj
and a feasible solution x ∈ Yj . If
f
Yj
< UPPER,
place Yj in OPEN. If in addition
f(x) < UPPER,
set
UPPER = f(x)
and mark x as the best solution found so far.
Step 2: (Termination Test) If OPEN is nonempty, go to step 1.
Otherwise, terminate; the best solution found so far is optimal.
486 Network Problems with Integer Constraints Chap. 10
A node Yj that is not placed in OPEN in Step 1 is said to be fath-
omed . Such a node cannot contain a better solution than the best solution
found so far, since the corresponding lower bound f
Yj
is not smaller than
UPPER. Therefore nothing is lost when we drop this node from further
consideration and forego the examination of its descendants. Regardless of
how many nodes are fathomed, the branch-and-bound algorithm is guar-
anteed to examine either explicitly or implicitly (through fathoming) all
the terminal nodes, which are the singleton solutions. As a result, it will
terminate with an optimal solution.
Note that a small (near-optimal) value of UPPER and tight lower
bounds f
Yj
contribute to the quick fathoming of large portions of the
branch-and-bound tree, and an early termination of the algorithm, with
either an optimal solution or a solution that is within some given tolerance
of being optimal. In fact, a popular variant, aimed at accelerating the
branch-and-bound algorithm, is to fix an  > 0, and replace the test
f
Y j
< UPPER
with
f
Y j
< UPPER − 
in Step 1. This variant may terminate much faster, while the best solution
obtained upon termination is guaranteed to be within  of optimality.
Other variations of branch-and-bound relate to the method for se-
lecting a node from OPEN in Step 1. For example, a possible strategy
is to choose the node with minimal lower bound; alternatively, one may
choose the node containing the best solution found so far. In fact it is
neither practical nor necessary to generate a priori the branch-and-bound
tree. Instead, one may adaptively decide on the order and the manner in
which the nodes are partitioned into descendants based on the progress of
the algorithm.
Branch-and-bound typically uses “continuous” network optimization
problems (without integer constraints) to obtain lower bounds to the op-
timal costs of the restricted problems minx∈Y f(x) and to construct corre-
sponding feasible solutions. For example, suppose that our original problem
has a convex cost function, and a feasible set F that consists of convex set
constraints and side constraints, plus the additional constraint that all the
arc flows must be 0 or 1 . Then a restricted subset Y may specify that the
flows of some given subset of arcs are fixed at 0 or at 1, while the remaining
arc flows may take either the value 0 or the value 1. A lower bound to the
restricted optimal cost minx∈Y f(x) is then obtained by relaxing the 0-1
constraint on the latter arc flows, thereby allowing them to take any value
in the interval [0, 1] and resulting in a convex network problem with side
constraints. Thus the solution by branch-and-bound of a network problem
Sec. 10.2 Branch-and-Bound 487
with convex cost and side constraints plus additional integer constraints re-
quires the solution of many convex network problems with side constraints
but without integer constraints.
Example 10.8. Facility Location Problems
Let us consider the facility location problem introduced in Example 10.2,
which involves m clients and n locations. By xij = 1 (or xij = 0) we indicate
that client i is assigned to location j at a cost aij (or is not assigned, respec-
tively). We also introduce a 0-1 integer variable yj to indicate (with yj = 1)
that a facility is placed at location j at a cost bj . The problem is
minimize
∑
(i,j)∈A
aijxij +
n∑
j=1
bjyj
subject to
∑
{j|(i,j)∈A}
xij = 1, i = 1, . . . , m,∑
{i|(i,j)∈A}
xij ≤ yjcj , j = 1, . . . , n,
xij = 0 or 1, ∀ (i, j) ∈ A,
yj = 0 or 1, j = 1, . . . , n,
where cj is the maximum number of customers that can be served by a facility
at location j.
The solution of the problem by branch-and-bound involves the partition
of the feasible set F into subsets. The choice of subsets is somewhat arbitrary,
but it is convenient to select subsets of the form
F (J0, J1) =
{
(x, y) ∈ F | yj = 0, ∀ j ∈ J0, yj = 1, ∀ j ∈ J1
}
,
where J0 and J1 are disjoint subsets of the index set {1, . . . , n} of facility
locations. Thus, F (J0, J1) is the subset of feasible solutions such that:
a facility is placed at the locations in J1,
no facility is placed at the locations in J0,
a facility may or may not be placed at the remaining locations.
For each node/subset F (J0, J1), we may obtain a lower bound and a feasible
solution by solving the linear program where all integer constraints are relaxed
except for the variables yj , j ∈ J0 ∪ J1, which have been fixed at either 0 or
1:
minimize
∑
(i,j)∈A
aijxij +
n∑
j=1
bjyj
subject to
∑
{j|(i,j)∈A}
xij = 1, i = 1, . . . , m,
488 Network Problems with Integer Constraints Chap. 10∑
{i|(i,j)∈A}
xij ≤ yjcj , j = 1, . . . , n,
xij ∈ [0, 1], ∀ (i, j) ∈ A,
yj ∈ [0, 1], ∀ j /∈ J0 ∪ J1,
yj = 0, ∀ j ∈ J0, yj = 1, ∀ j ∈ J1.
As an illustration, let us work out the example shown in Figure 10.8,
which involves 3 clients and 2 locations. The facility capacities at the two
locations are c1 = c2 = 3. The cost coefficients aij and bj are shown next
to the corresponding arcs. The optimal solution corresponds to y1 = 0 and
y2 = 1, that is, placing a facility only in location 2 and serving all the clients
at that facility. The corresponding optimal cost is
f∗ = 5.
Let us apply the branch-and-bound algorithm using the tree shown in
Fig. 10.8. We first consider the top node
(
J0 = Ø, J1 = Ø
)
, where neither
y1 nor y2 is fixed at 0 or 1. The lower bound f
Y
is obtained by solving the
(relaxed) linear program
minimize (2x11 + x12) + (2x21 + x22) + (x31 + 2x32) + 3y1 + y2
subject to x11 + x12 = 1, x21 + x22 = 1, x31 + x32 = 1,
x11 + x21 + x31 ≤ 3y1, x12 + x22 + x32 ≤ 3y2,
0 ≤ xij ≤ 1, ∀ (i, j) ∈ A,
0 ≤ y1 ≤ 1, 0 ≤ y2 ≤ 1.
The optimal solution of this program is
xij =
{
1 if (i, j) = (1, 2), (2, 2), (3, 1),
0 otherwise,
y1 = 1/3, y2 = 2/3,
and the corresponding optimal cost (lower bound) is
f
Y
= 4.66.
A feasible solution of the original problem is obtained by rounding the frac-
tional values of y1 and y2 to
y1 = 1, y2 = 1,
and the associated cost is 7. Thus, we set
UPPER = 7,
Sec. 10.2 Branch-and-Bound 489
Lower Bound = 4.66
Feasible Solution Cost = 7
J0 = ∅, J1 = ∅
J0 = ∅, J1 = {1}
J0 = {2}, J1 = {1}J0 = ∅, J1 = {1,2}
J0 = {1}, J1 = ∅
J0 = {1,2}, J1 = ∅J0 = {1}, J1 = {2}
Lower Bound = 5
Feasible Solution Cost = 5 Lower Bound = 6.66
FATHOMED
1
1
1
1
CLIENTS
2
3
LOCATIONS
1
2
< 3y1_
< 3y2_
2
1
1
2
2
1
b1 = 3
b2 = 1
Figure 10.8: Branch-and-bound solution of a facility location problem with
3 clients and 2 locations. The facility capacities at the two locations are c1 =
c2 = 3. The cost coefficients aij and bj are shown next to the corresponding
arcs. The relaxed problem for the top node
(
J0 = Ø, J1 = Ø
)
, corresponding
to relaxing all the integer constraints, is solved first, obtaining the lower and
upper bounds shown. Then the relaxed problem corresponding to the left
node
(
J0 = {1}, J1 = Ø
)
is solved, obtaining the lower and upper bounds
shown. Finally, the relaxed problem corresponding to the right node
(
J0 =
Ø, J1 = {1}
)
is solved, obtaining a lower bound that is higher than the current
value of UPPER. As a result this node can be fathomed, and its descendants
need not be considered further.
and we place in OPEN the two descendants
(
J0 = {1}, J1 = Ø
)
and
(
J0 =
Ø, J1 = {1}
)
, corresponding to fixing y1 at 0 and at 1, respectively.
We proceed with the left branch of the branch-and-bound tree, and
consider the node
(
J0 = {1}, J1 = Ø
)
, corresponding to fixing y1 as well
as the corresponding flows x11, x21, and x31 to 0. The associated (relaxed)
linear program is
minimize x12 + x22 + 2x32 + y2
subject to x12 = 1, x22 = 1, x32 = 1,
x12 + x22 + x32 ≤ 3y2,
0 ≤ x12 ≤ 1, 0 ≤ x22 ≤ 1, 0 ≤ x32 ≤ 1,
0 ≤ y2 ≤ 1.
490 Network Problems with Integer Constraints Chap. 10
The optimal solution (in fact the only feasible solution) of this program is
xij =
{
1 if (i, j) = (1, 2), (2, 2), (3, 2),
0 otherwise,
y2 = 1,
and the corresponding optimal cost (lower bound) is
f
Y
= 5.
The optimal solution of the relaxed problem is integer, and its cost, 5, is lower
than the current value of UPPER, so we set
UPPER = 5.
The two descendants,
(
J0 = {1}, J1 = {2}
)
and
(
J0 = {1, 2}, J1 = Ø
)
,
corresponding to fixing y2 at 1 and at 0, respectively, are placed in OPEN.
We proceed with the right branch of the branch-and-bound tree, and
consider the node
(
J0 = Ø, J1 = {1}
)
, corresponding to fixing y1 to 1. The
associated (relaxed) linear program is
minimize (2x11 + x12) + (2x21 + x22) + (x31 + 2x32) + 3 + y2
subject to x11 + x12 = 1, x21 + x22 = 1, x31 + x32 = 1,
x11 + x21 + x31 ≤ 3, x12 + x22 + x32 ≤ 3y2,
0 ≤ xij ≤ 1, ∀ (i, j) ∈ A,
0 ≤ y2 ≤ 1.
The optimal solution of this program is
xij =
{
1 if (i, j) = (1, 2), (2, 2), (3, 1),
0 otherwise,
y2 = 2/3,
and the corresponding optimal cost (lower bound) is
f
Y
= 6.66.
This is larger than the current value of UPPER, so the node can be fathomed,
and its two descendants are not placed in OPEN.
We conclude that one of the two descendants of the left node,
(
J0 =
{1}, J1 = {2}
)
and
(
J0 = {1, 2}, J1 = Ø
)
(the only nodes in OPEN), contains
the optimal solution. We can proceed to solve the relaxed linear programs
corresponding to these two nodes, and obtain the optimal solution. However,
there is also a shortcut here: since these are the only two remaining nodes
and the upper bound corresponding to these nodes coincides with the lower
bound, we can conclude that the lower bound is equal to the optimal cost
and the corresponding integer solution (y1 = 0, y2 = 1) is optimal.
Sec. 10.2 Branch-and-Bound 491
Generally, for the success of the branch-and-bound approach it is
important that the lower bounds are as tight as possible, because this
facilitates the fathoming of nodes, and leads to fewer restricted problem
solutions. On the other hand, the tightness of the bounds strongly depends
on how the problem is formulated as an integer programming problem.
There may be several possible formulations, some of which are “stronger”
than others in the sense that they provide better bounds within the branch-
and-bound context. As an illustration, consider the following example.
Example 10.9. Facility Location – Alternative Formulation
Consider the following alternative formulation of the preceding facility loca-
tion problem
minimize
∑
(i,j)∈A
aijxij +
n∑
j=1
bjyj
subject to
∑
{j|(i,j)∈A}
xij = 1, i = 1, . . . , m,∑
{i|(i,j)∈A}
xij ≤ cj , j = 1, . . . , n,
xij ≤ yj , ∀ (i, j) ∈ A,
xij = 0 or 1, ∀ (i, j) ∈ A,
yj = 0 or 1, j = 1, . . . , n.
This formulation involves a lot more constraints, but is in fact superior to
the one given earlier (cf. Example 10.8). The reason is that, if we relax the
0-1 constraints on xij and yj , the side constraints
∑
i
xij ≤ yjcj of Example
10.8 are implied by the constraints
∑
i
xij ≤ cj and xij ≤ yj of the present
example. As a result, the lower bounds obtained by relaxing some of the
0-1 constraints are tighter in the alternative formulation just given, thereby
enhancing the effectiveness of the branch-and-bound method. In fact, it can
be verified that for the example of Fig. 10.8, by relaxing the 0-1 constraints
in the stronger formulation of the present example, we obtain the correct
optimal integer solution at the very first node of the branch-and-bound tree.
An important conclusion from the preceding example is that it is
possible to accelerate the branch-and-bound solution of a problem by adding
more side constraints. Even if these constraints do not affect the set of
feasible integer solutions, they can improve the lower bounds obtained by
relaxing the 0-1 constraints. Basically, when the integer constraints are
relaxed, one obtains a superset of the feasible set of integer solutions, so
with more side constraints, the corresponding superset becomes smaller
and approximates better the true feasible set (see Fig. 10.9). It is thus
very important to select a problem formulation such that when the integer
constraints are relaxed, the feasible set is as small as possible.
492 Network Problems with Integer Constraints Chap. 10
Set of Relaxed
Solutions
Additional
Side Constraints
Integer
Solutions
Figure 10.9: Illustration of the effect
of additional side constraints. They do
not affect the set of feasible integer solu-
tions, but they reduce the set of “relaxed
solutions,” that is, those x that satisfy
all the constraints except for the inte-
ger constraints. This results in improved
lower bounds and a faster branch-and-
bound solution.
We note that the subject of characterizing the feasible set of an integer
programming problem, and approximating it tightly with a polyhedral set
has received extensive attention. In particular, there is a lot of theory
and accumulated practical knowledge on characterizing the feasible set in
specific problem contexts; see the references cited at the end of the chapter.
A further discussion of branch-and-bound is beyond our scope. We refer to
sources on linear and combinatorial optimization, such as Zoutendijk [1976],
Papadimitriou and Steiglitz [1982], Schrijver [1986], Nemhauser and Wolsey
[1988], Bertsimas and Tsitsiklis [1997], Cook, Cunningham, Pulleyblank,
and Schrijver [1998], which also describe many applications.
10.3 LAGRANGIAN RELAXATION
In this section, we consider an important approach for obtaining lower
bounds to use in the branch-and-bound method. Let us consider the case
of the network optimization problem with linear cost function, linear side
constraints, and integer constraints on the arc flows:
minimize a′x
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = si, ∀ i ∈ N ,
c′tx ≤ dt, t = 1, . . . , r,
xij ∈ Xij , ∀ (i, j) ∈ A,
where a and ct are given vectors, dt are given scalars, and each Xij is a
finite subset of contiguous integers (i.e., the convex hull of Xij contains
all the integers in Xij , as for example in the cases Xij = {0, 1} or Xij =
{1, 2, 3, 4}). We assume that the supplies si are integer , so that if the
side constraints c′tx ≤ dt were not present, the problem would become a
minimum cost flow problem that has integer optimal solutions, according
to the theory developed in Chapter 5. Note that for this it is not necessary
Sec. 10.3 Lagrangian Relaxation 493
that the arc cost coefficients aij (the components of the vectors a) be
integer.
In the Lagrangian relaxation approach, we eliminate the side con-
straints c′tx ≤ dt by adding to the cost function the terms µt(c′tx − dt),
thereby forming the Lagrangian function
L(x, µ) = a′x +
r∑
t=1
µt(c′tx − dt),
where µ = (µ1, . . . , µr) is a vector of nonnegative scalars. Each µt may be
viewed as a penalty per unit violation of the corresponding side constraint
c′tx ≤ dt, and may also be viewed as a Lagrange multiplier.
A key idea of Lagrangian relaxation is that regardless of the choice
of µ, the minimization of the Lagrangian L(x, µ) over the set of remaining
constraints
F̃ = {x | xij ∈ Xij , x satisfies the conservation of flow constraints},
yields a lower bound to the optimal cost of the original problem (cf. the
weak duality property, discussed in Section 8.7). To see this, note that we
have
min
x∈F̃
L(x, µ) = min
x∈F̃
{
a′x +
r∑
t=1
µt(c′tx − dt)
}
≤ min
x∈F̃ , c′tx−dt≤0, t=1,...,r
{
a′x +
r∑
t=1
µt(c′tx − dt)
}
≤ min
x∈F̃ , c′tx−dt≤0, t=1,...,r
a′x,
where the first inequality follows because the minimum of the Lagrangian in
the next-to-last expression is taken over a subset of F̃ , and the last inequal-
ity follows using the nonnegativity of µt. The lower bound minx∈F̃ L(x, µ)
can in turn be used in the branch-and-bound procedure discussed earlier.
Since in the context of branch-and-bound, it is important to use as
tight a lower bound as possible, we are motivated to search for an optimal
lower bound through adjustment of the vector µ. To this end, we form the
following dual function (cf. Section 8.7)
q(µ) = min
x∈F̃
L(x, µ),
and we consider the dual problem
maximize q(µ)
subject to µt ≥ 0, t = 1, . . . , r.
Solution of this problem yields the tightest lower bound to the optimal cost
of the original problem.
494 Network Problems with Integer Constraints Chap. 10
Example 10.10. Constrained Shortest Path Problem
As an example of the use of Lagrangian relaxation, consider the constrained
shortest path problem discussed in Example 8.6 of Section 8.4. Here, we want
to find a simple forward path P from an origin node s to a destination node
t that minimizes the path length∑
(i,j)∈P
aij ,
subject to the following side constraints on P :∑
(i,j)∈P
ckij ≤ dk, k = 1, . . . , K.
As discussed in Section 8.4, we can formulate this as the following network
flow problem with integer constraints and side constraints:
minimize
∑
(i,j)∈A
aijxij
subject to
∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji =
{
1 if i = s,
−1 if i = t,
0 otherwise,
xij = 0 or 1, ∀ (i, j) ∈ A,∑
(i,j)∈A
ckijxij ≤ dk, k = 1, . . . , K.
(10.7)
Here, a path P from s to t is optimal if and only if the flow vector x defined
by
xij =
{
1 if (i, j) belongs to P ,
0 otherwise,
is an optimal solution of the problem (10.7).
To apply Lagrangian relaxation, we eliminate the side constraints, and
we form the corresponding Lagrangian function assigning a nonnegative mul-
tiplier µk to the kth constraint. Minimization of the Lagrangian now becomes
a shortest path problem with respect to corrected arc lengths âij given by
âij = aij +
K∑
k=1
µkckij .
(We assume here that there are no negative length cycles with respect to the
arc lengths âij ; this will be so if all the aij and c
k
ij are nonnegative.) We
then obtain µ∗ that solves the dual problem maxµ≥0 q(µ) and we obtain a
corresponding optimal cost/lower bound. We can then use µ∗ to obtain a
feasible solution (a path that satisfies the side constraints) as discussed in
Example 8.6.
Sec. 10.3 Lagrangian Relaxation 495
The preceding example illustrates an important advantage of La-
grangian relaxation, as applied to integer-constrained network problems: it
eliminates the side constraints simultaneously with the integer constraints.
In particular, minimizing L(x, µ) over the set
F̃ = {x | xij ∈ Xij , x satisfies the conservation of flow constraints}
is a (linear) minimum cost flow problem that can be solved using the
methodology of Chapters 2-7: the Lagrangian L(x, µ) is linear in x and
the integer constraints do not matter, and can be replaced by the inter-
val constraints xij ∈ X̂ij , where X̂ij is the convex hull of the set Xij .
This should be contrasted with the integer constraint relaxation approach,
where we eliminate just the integer constraints, while leaving the side con-
straints unaffected (see the facility location problem that we solved using
branch-and-bound in Example 10.8). As a result, the minimum cost flow
methodology of Chapters 2-7 does not apply when there are side constraints
and the integer constraint relaxation approach is used. This is the main
reason for the widespread use of Lagrangian relaxation in combination with
branch-and-bound.
Actually, in Lagrangian relaxation it is not mandatory to eliminate
just the side constraints. One may eliminate the conservation of flow con-
straints, in addition to or in place of the side constraints. (The multipliers
corresponding to the conservation of flow constraints should be uncon-
strained in the dual problem, because the conservation of flow is expressed
in terms of equality constraints; cf. the discussion in Section 8.7.) One
still obtains a lower bound to the optimal cost of the original problem,
because of the weak duality property (cf. Section 8.7). However, the mini-
mization of the Lagrangian is not a minimum cost flow problem anymore.
Nonetheless, by choosing properly the constraints to eliminate and by tak-
ing advantage of the special structure of the problem, the minimization
of the Lagrangian over the remaining set of constraints may be relatively
simple. The following is an illustrative example.
Example 10.11. Traveling Salesman Problem
Consider the traveling salesman problem of Example 10.1. Here, we want to
find a minimum cost tour in a complete graph where the cost of arc (i, j) is
denoted aij . We formulate this as the following network problem with side
constraints and 0-1 integer constraints:
minimize
∑
(i,j)∈A
aijxij
subject to
∑
j=1,...,N
j =i
xij = 1, i = 1, . . . , N, (10.8)
496 Network Problems with Integer Constraints Chap. 10∑
i=1,...,N
i=j
xij = 1, j = 1, . . . , N, (10.9)
xij = 0 or 1, ∀ (i, j) ∈ A, (10.10)
the subgraph with node-arc set
(
N , {(i, j) | xij = 1}
)
is connected. (10.11)
We may express the connectivity constraint (10.11) in several different
ways, leading to different Lagrangian relaxation and branch-and-bound algo-
rithms. One of the most successful formulations is based on the notion of
a 1-tree, which consists of a tree that spans nodes 2, . . . , N , plus two arcs
that are incident to node 1. Equivalently, a 1-tree is a connected subgraph
that contains a single cycle passing through node 1 (see Fig. 10.10). Note
that if the conservation of flow constraints (10.8) and (10.9), and the integer
constraints (10.10) are satisfied, then the connectivity constraint (10.11) is
equivalent to the constraint that the subgraph
(
N , {(i, j) | xij = 1}
)
is a
1-tree.
1
2
3
N
Figure 10.10: Illustration of a 1-tree. It con-
sists of a tree that spans nodes 2, . . . , N , plus
two arcs that are incident to node 1.
Let X1 be the set of all x with 0 − 1 components, and such that the
subgraph
(
N , {(i, j) | xij = 1}
)
is a 1-tree. Let us consider a Lagrangian
relaxation approach based on elimination of the conservation of flow equa-
tions. Assigning multipliers ui and vj to the constraints (10.8) and (10.9),
respectively, the Lagrangian function is
L(x, u, v) =
∑
i,j,i=j
(aij + ui + vj)xij −
N∑
i=1
ui −
N∑
j=1
vj .
The minimization of the Lagrangian is over all 1-trees, leading to the problem
min
x∈X1
{ ∑
i,j,i=j
(aij + ui + vj)xij
}
.
If we view aij + ui + vj as a modified cost of arc (i, j), this minimization
is quite easy. It is equivalent to obtaining a tree of minimum modified cost
Sec. 10.3 Lagrangian Relaxation 497
that spans the nodes 2, . . . , N , and then adding two arcs that are incident to
node 1 and have minimum modified cost. The minimum cost spanning tree
problem can be easily solved using the Prim-Dijkstra algorithm (see Exercise
2.30).
Unfortunately, the Lagrangian relaxation method has several weak-
nesses:
(a) Even if we find an optimal µ, we still have only a lower bound to the
optimal cost of the original problem.
(b) The minimization of L(x, µ) over the set
F̃ = {x | xij ∈ Xij , x satisfies the conservation of flow constraints},
may yield an x that violates some of the side constraints c′tx−dt ≤ 0,
so it may be necessary to adjust this x for feasibility using some
heuristic.
(c) The maximization of q(µ) over µ ≥ 0 may be quite nontrivial for a
number of reasons, including the fact that q is typically nondifferen-
tiable.
In what follows in this section, we will discuss the algorithmic method-
ology for solving the dual problem, including the subgradient and cutting
plane methods, which have enjoyed a great deal of popularity. These meth-
ods have also been used widely in connection with various decomposition
schemes for large-scale problems with special structure. For further dis-
cussion, we refer to the nonlinear programming literature (see for example
Lasdon [1970], Auslender [1976], Shapiro [1979], Shor [1985], Poljak [1987],
Hiriart-Urruty and Lemarechal [1993], and Bertsekas [1995b]).
10.3.1 Subgradients of the Dual Function
Let us consider the algorithmic solution of the dual problem
maximize q(µ)
subject to µt ≥ 0, t = 1, . . . , r.
The dual function is
q(µ) = min
x∈F̃
L(x, µ),
where
F̃ = {x | xij ∈ Xij , x satisfies the conservation of flow constraints},
and L(x, µ) is the Lagrangian function
L(x, µ) = a′x +
r∑
t=1
µt(c′tx − dt).
498 Network Problems with Integer Constraints Chap. 10
Recall here that the set F̃ is finite, because we have assumed that each Xij
is a finite set of contiguous integers.
We note that for a fixed x ∈ F̃ , the Lagrangian L(x, µ) is a linear
function of µ. Thus, because the set F̃ is finite, the dual function q is the
minimum of a finite number of linear functions of µ – there is one such
function for each x ∈ F̃ . For conceptual simplification, we may write q in
the following generic form:
q(µ) = min
i∈I
{α′iµ + βi}, (10.12)
where I is some finite index set, and αi and βi are suitable vectors and
scalars, respectively (see Fig. 10.11).
Of particular interest for our purposes are the “slopes” of q at vari-
ous vectors µ, i.e., the vectors αiµ , where iµ ∈ I is an index attaining the
minimum of α′iµ + βi over i ∈ I [cf. Eq. (10.12)]. If iµ is the unique index
attaining the minimum, then q is differentiable (in fact linear) at µ, and its
gradient is aiµ. If there are multiple indices i attaining the minimum, then
q is nondifferentiable at µ (see Fig. 10.11). To deal with such differentia-
bilities, we generalize the notion of a gradient. In particular, we define a
subgradient of q at a given µ ≥ 0 to be any vector g such that
q(ν) ≤ q(µ) + (ν − µ)′g, ∀ ν ≥ 0, (10.13)
(see Fig. 10.11). The right-hand side of the above inequality provides a
linear approximation to the dual function q using the function value q(µ)
at the given µ and the corresponding subgradient g. The approximation
is exact at the vector µ, and is an overestimate at other vectors ν. Some
further properties of subgradients are summarized in Appendix A.
We now consider the calculation of subgradients of the dual function.
For any µ, let xµ minimize the Lagrangian L(x, µ) over x ∈ F̃ ,
xµ = arg min
x∈F̃
L(x, µ).
Let us show that the vector g(xµ) that has components
gt(xµ) = c′txµ − dt, t = 1, . . . , r,
is a subgradient of q at µ. To see this, we use the definition of L, q, and
xµ to write for all ν ≥ 0,
q(ν) = min
x∈F̃
L(x, ν)
≤ L(xµ, ν)
= a′xµ + ν′g(xµ)
= a′xµ + µ′g(xµ) + (ν − µ)′g(xµ)
= q(µ) + (ν − µ)′g(xµ),
Sec. 10.3 Lagrangian Relaxation 499
µ µ
q(µ) = min {α i' µ + βi }
i ∈I
α1' µ + β1
α2' µ + β2 α3' µ + β3
_
µ~
Figure 10.11: Illustration of the dual function q and its subgradients. The
generic form of q is
q(µ) = min
i∈I
{α′iµ + βi},
where I is some finite index set, and αi and βi are suitable vectors and scalars,
respectively. Given µ, and an index iµ ∈ I attaining the minimum in the above
equation, the vector αiµ is a subgradient at µ. Furthermore, any subgradient
at µ is a convex combination of vectors aiµ such that iµ ∈ I and iµ attains the
minimum of α′iµ+βi over i ∈ I. For example, at the vector µ shown in the figure,
there is a unique subgradient, the vector α1. At the vector µ̃ shown in the figure,
the set of subgradients is the line segment connecting the vectors α2 and α3.
so the subgradient inequality (10.13) is satisfied for g = g(xµ). Thus,
for a given µ, the evaluation of q(µ), which requires finding a minimizer
xµ of L(x, µ) over F̃ , yields as a byproduct the subgradient g(xµ). This
convenience in calculating subgradients is particularly important for the
algorithms that we discuss in what follows in this section.
10.3.2 Subgradient Methods
We now turn to algorithms that use subgradients for solving the dual prob-
lem. The subgradient method consists of the iteration
µk+1 =
[
µk + skgk
]+
, (10.14)
where gk is any subgradient of q at µk, sk is a positive scalar stepsize,
and [y]+ is the operation that sets to 0 all the negative components of the
vector y. Thus the iteration (10.14) can be written as
µk+1t = max
{
0, µkt + skgkt
}
, t = 1, . . . , r.
500 Network Problems with Integer Constraints Chap. 10
The simplest way to calculate the subgradient gk is to find an xµk that
minimizes L(x, µk) over x ∈ F̃ , and to set
gk = g(xµk),
where for every x, g(x) is the r-dimensional vector with components
gt(x) = c′tx − dt, t = 1, . . . , r.
An important fact about the subgradient method is that the new
iterate may not improve the dual cost for all values of the stepsize sk; that
is, we may have
q
(
[µk + skgk]+
)
< q(µk), ∀ sk > 0
(see Fig. 10.12). What makes the subgradient method work is that for suf-
ficiently small stepsize sk, the distance of the current iterate to the optimal
solution set is reduced , as illustrated in Fig. 10.12, and as shown in the
following proposition.
µk
µk + skgk
µk+1 =  [µk + skgk]+
µ*
< 90o
Contours of q
µ1
µ2
Figure 10.12: Illustration of how it may not be possible to improve the dual
function by using the subgradient iteration µk+1 = [µk + skgk]+, regardless of
the value of the stepsize sk. However, the distance to any optimal solution µ∗ is
reduced using a subgradient iteration with a sufficiently small stepsize. The crucial
fact, which follows from the definition of a subgradient, is that the angle between
the subgradient gk and the vector µ∗ − µk is less than 90 degrees. As a result,
for sk small enough, the vector µk + skgk is closer to µ∗ than µk. Furthermore,
the vector [µk + skgk]+ is closer to µ∗ than µk + skgk is.
Sec. 10.3 Lagrangian Relaxation 501
Proposition 10.1: If µk is not optimal, then for any dual optimal
solution µ∗, we have
‖µk+1 − µ∗‖ < ‖µk − µ∗‖,
for all stepsizes sk such that
0 < sk <
2
(
q(µ∗) − q(µk)
)
‖gk‖2 . (10.15)
Proof: We have
‖µk + skgk − µ∗‖2 = ‖µk − µ∗‖2 − 2sk(µ∗ − µk)′gk + (sk)2‖gk‖2,
and by using the subgradient inequality (10.13),
(µ∗ − µk)′gk ≥ q(µ∗) − q(µk).
Combining the last two relations, we obtain
‖µk + skgk − µ∗‖2 ≤ ‖µk − µ∗‖2 − 2sk
(
q(µ∗) − q(µk)
)
+ (sk)2‖gk‖2.
We can now verify that for the range of stepsizes of Eq. (10.15) the sum of
the last two terms in the above relation is negative. In particular, with a
straightforward calculation, we can write this relation as
‖µk + skgk − µ∗‖2 ≤ ‖µk − µ∗‖2 − γ
k(2 − γk)
(
q(µ∗) − q(µk)
)2
‖gk‖2 , (10.16)
where
γk =
sk‖gk‖2
q(µ∗) − q(µk) .
If the stepsize sk satisfies Eq. (10.15), then 0 < γk < 2, so Eq. (10.16)
yields
‖µk + skgk − µ∗‖ < ‖µk − µ∗‖.
We now observe that since µ∗ ≥ 0, we have∥∥[µk + skgk]+ − µ∗∥∥ ≤ ‖µk + skgk − µ∗‖,
and from the last two inequalities, we obtain ‖µk+1 − µ∗‖ < ‖µk − µ∗‖.
Q.E.D.
502 Network Problems with Integer Constraints Chap. 10
The inequality (10.16) can also be used to establish convergence and
rate of convergence results for the subgradient method with stepsize rules
satisfying
0 < sk <
2
(
q(µ∗) − q(µk)
)
‖gk‖2
[cf. Eq. (10.15)]. Unfortunately, however, unless we know the dual optimal
cost q(µ∗), which is rare, the range of stepsizes (10.15) is unknown. In
practice, a frequently used stepsize formula is
sk =
αk
(
qk − q(µk)
)
‖gk‖2 , (10.17)
where qk is an approximation to the optimal dual cost and
0 < αk < 2.
Note that we can estimate the optimal dual cost from below with the
best current dual cost
q̂k = max
0≤i≤k
q(µi).
As an overestimate of the optimal dual cost, we can use the cost f(x̄) of any
primal feasible solution x̄; in many circumstances, primal feasible solutions
are naturally obtained in the course of the algorithm. Finally, the special
structure of many problems can be exploited to yield improved bounds to
the optimal dual cost.
Here are two common ways to choose αk and qk in the stepsize formula
(10.17):
(a) qk is the best known upper bound to the optimal dual cost at the kth
iteration and αk is a number, which is initially equal to one and is
decreased by a certain factor (say, two) every few (say, five or ten)
iterations. An alternative formula for αk is
αk =
m
k + m
,
where m is a positive integer.
(b) αk = 1 for all k and qk is given by
qk =
(
1 + β(k)
)
q̂k, (10.18)
where q̂k is the best current dual cost q̂k = max0≤i≤k q(µi). Further-
more, β(k) is a number greater than zero, which is increased by a
certain factor if the previous iteration was a “success,” that is, if it
improved the best current dual cost, and is decreased by some other
factor otherwise. This method requires that q̂k > 0. Also, if upper
Sec. 10.3 Lagrangian Relaxation 503
bounds q̃k to the optimal dual cost are available as discussed earlier,
then a natural improvement to Eq. (10.18) is
qk = min
{
q̃k,
(
1 + β(k)
)
q̂k
}
.
For a convergence analysis of the subgradient method and its variants,
we refer to the literature cited at the end of the chapter (see also Exercises
10.36-10.38). However, the convergence properties of the schemes most
often preferred in practice, including the ones given above, are neither
solid nor well understood. It is easy to find problems where the subgradient
method works very poorly. On the other hand, the method is simple and
works well for many types of problems, yielding good approximate solutions
within a few tens or hundreds of iterations. Also, frequently a good primal
feasible solution can be obtained using effective heuristics, even with a
fairly poor dual solution.
10.3.3 Cutting Plane Methods
Consider again the dual problem
maximize q(µ)
subject to µ ≥ 0.
The cutting plane method, at the kth iteration, replaces the dual function
q by a polyhedral approximation Qk, constructed using the vectors µi and
corresponding subgradients gi, i = 0, 1, . . . , k − 1, obtained so far. It then
solves the problem
maximize Qk(µ)
subject to µ ≥ 0.
In particular, for k = 1, 2, . . ., Qk is given by
Qk(µ) = min
i=0,...,k−1
{
q(µi) + (µ − µi)′gi
}
, (10.19)
and the kth iterate is generated by
µk = arg max
µ≥0
Qk(µ). (10.20)
As in the case of subgradient methods, the simplest way to calculate
the subgradient gi is to find an xµi that minimizes L(x, µi) over x ∈ F̃ ,
and to set
gi = g(xiµ),
where for every x, g(x) is the r-dimensional vector with components
gt(x) = c′tx − dt, t = 1, . . . , r.
504 Network Problems with Integer Constraints Chap. 10
Note that the approximation Qk(µ) is an overestimate of the dual function
q,
q(µ) ≤ Qk(µ), µ ≥ 0, (10.21)
since, in view of the definition of a subgradient [cf. Eq. (10.13)], each of
the linear terms in the right-hand side of Eq. (10.19) is an overestimate of
q(µ).
We assume that, for all k, it is possible to find a maximum µk of
Qk over µ ≥ 0. To ensure this, the method has to be suitably initialized;
for example by selecting a sufficiently large number of vectors µ, and by
computing corresponding subgradients, to form an initial approximation
that is bounded from above over the set {µ | µ ≥ 0}. Thus, in this variant,
we start the method at some iteration k > 0, with the vectors µ0, . . . , µk−1
suitably selected so that Qk(µ) is bounded from above over µ ≥ 0. Al-
ternatively, we may maximize Qk over a suitable bounded polyhedral set
that is known to contain an optimal dual solution, instead of maximizing
over µ ≥ 0. We note that given the iterate µk, the method produces both
the exact and the approximate dual values q(µk) and Qk(µk). It can be
seen, using Eqs. (10.20) and (10.21), that the optimal dual cost is bracketed
between these two values:
q(µk) ≤ max
µ≥0
q(µ) ≤ Qk(µk). (10.22)
Thus, in particular, the equality
q(µk) = Qk(µk) (10.23)
guarantees the optimality of the vector µk. It turns out that because the
dual function is piecewise linear, and consequently only a finite number of
subgradients can be generated, the optimality criterion (10.23) is satisfied
in a finite number of iterations, and the method terminates. This is shown
in the following proposition and is illustrated in Fig. 10.13.
Proposition 10.2: The cutting plane method terminates finitely;
that is, for some k, µk is a dual optimal solution and the termina-
tion criterion (10.23) is satisfied.
Proof: For notational convenience, let us write the dual function in the
polyhedral form
q(µ) = min
i∈I
{α′iµ + βi},
where I is some finite index set and αi, βi, i ∈ I, are suitable vectors
and scalars, respectively. Let ik be an index attaining the minimum in the
equation
q(µk) = min
i∈I
{α′iµk + βi},
Sec. 10.3 Lagrangian Relaxation 505
q(µ)
µ1µ0 µ2µ3 µ
µ*µ4 =
q(µ0) + (µ − µ0)'g0
q(µ1) + (µ − µ1)'g1
Figure 10.13: Illustration of the cutting plane method. With each new iterate
µi, a new hyperplane q(µi)+(µ−µi)′gi is added to the polyhedral approximation
of the dual function. The method converges finitely, since if µk is not optimal, a
new cutting plane will be added at the corresponding iteration, and there can be
only a finite number of cutting planes.
so that αik is a subgradient at µk. If the termination criterion (10.23) is
not satisfied at µk, we must have
α′
ik
µk + βik = q(µk) < Qk(µk).
Since
Qk(µk) = min
0≤m≤k−1
{α′imµk + βim},
it follows that the pair (αik , βik) is not equal to any of the preceding pairs
(αi0 , βi0), . . . , (αik−1 , βik−1). Since the index set I is finite, it follows that
there can be only a finite number of iterations for which the termination
criterion (10.23) is not satisfied. Q.E.D.
Despite its finite convergence property, the cutting plane method may
converge slowly, and in practice one may have to stop it short of finding an
optimal solution [the error bounds (10.22) may be used for this purpose].
An additional drawback of the method is that it can take large steps away
from the optimum even when it is close to (or even at) the optimum. This
phenomenon is referred to as instability , and has another undesirable effect,
namely, that µk−1 may not be a good starting point for the algorithm that
minimizes Qk(µ). A way to limit the effects of this phenomenon is to add
to the polyhedral function approximation a quadratic term that penalizes
large deviations from the current point. In this method, µk is obtained as
µk = arg max
µ≥0
{
Qk(µ) − 1
2ck
‖µ − µk−1‖2
}
,
506 Network Problems with Integer Constraints Chap. 10
where {ck} is a positive nondecreasing scalar parameter sequence. This
is known as the proximal cutting plane algorithm, and is related to the
proximal minimization method discussed in Section 8.8.5. It can be shown
that this variant of the cutting plane method also terminates finitely thanks
to the polyhedral nature of q.
Another interesting variant of the cutting plane method, known as
the central cutting plane method , maintains the polyhedral approximation
Qk(µ) to the dual function q, but generates the next vector µk by using a
somewhat different mechanism. In particular, instead of maximizing Qk,
the method obtains µk by finding a “central pair” (µk, zk) within the subset
Sk =
{
(µ, z) | µ ≥ 0, q̂k ≤ q(µ), q̂k ≤ z ≤ Qk(µ)
}
,
where q̂k is the best lower bound to the optimal dual cost that has been
found so far,
q̂k = max
i=0,...,k−1
q(µi).
The set Sk is illustrated in Fig. 10.14.
q(µ)
µ1µ0 µ2µ* µ
Central Pair (µ2,z2) Set S
2
q 2
q(µ0) + (µ − µ0)'g0
q(µ1) + (µ − µ1)'g1
Figure 10.14: Illustration of the set
Sk =
{
(µ, z) | µ ≥ 0, q̂k ≤ q(µ), q̂k ≤ z ≤ Qk(µ)
}
,
for k = 2, in the central cutting plane method.
There are several possible methods for finding the central pair (µk, zk).
Roughly, the idea is that the central pair should be “somewhere in the
middle” of Sk. For example, consider the case where Sk is polyhedral with
nonempty interior. Then (µk, zk) could be the analytic center of Sk, where
for any polyhedron
P = {y | a′py ≤ cp, p = 1, . . . , m}
Sec. 10.3 Lagrangian Relaxation 507
with nonempty interior, its analytic center is the unique maximizer of∑m
p=1 ln(cp − a′py) over y ∈ P . Another possibility is the ball center of
S, that is, the center of the largest inscribed sphere in S. Assuming that
the polyhedron P given above has nonempty interior, its ball center can
be obtained by solving the following problem with optimization variables
(y, σ):
maximize σ
subject to a′p(y + d) ≤ cp, ∀ ‖d‖ ≤ σ, p = 1, . . . , m.
It can be seen that this problem is equivalent to the linear program
maximize σ
subject to a′py + ‖ap‖σ ≤ cp, p = 1, . . . , m.
While the central cutting plane methods are not guaranteed to ter-
minate finitely, their convergence properties are satisfactory. Furthermore,
the methods have benefited from advances in the implementation of interior
point methods; see the references cited at the end of the chapter.
10.3.4 Decomposition and Multicommodity Flows
Lagrangian relaxation is particularly convenient when by eliminating the
side constraints, we obtain a network problem that decomposes into several
independent subproblems. A typical example arises in multicommodity
flow problems where we want to minimize
M∑
m=1
∑
(i,j)∈A
aij(m)xij(m) (10.24)
subject to the conservation of flow constraints∑
{j|(i,j)∈A}
xij(m) −
∑
{j|(j,i)∈A}
xji(m) = si(m), ∀ i ∈ N , m = 1, . . . , M,
(10.25)
the set constraints
xij(m) ∈ Xij(m), ∀ m = 1, . . . , M, (i, j) ∈ A, (10.26)
and the side constraints
M∑
m=1
A(m)x(m) ≤ b. (10.27)
508 Network Problems with Integer Constraints Chap. 10
Here si(m) are given supply integers for the mth commodity, A(m) are
given matrices, b is a given vector, and x(m) is the flow vector of the
mth commodity, with components xij(m), (i, j) ∈ A. Furthermore, each
Xij(m) is a finite subset of contiguous integers.
The dual function is obtained by relaxing the side constraints (10.27),
and by minimizing the corresponding Lagrangian function. This minimiza-
tion separates into m independent minimizations, one per commodity:
q(µ) = −µ′b +
M∑
m=1
min
x(m)∈F (m)
(
a(m) + A(m)′µ
)′
x(m), (10.28)
where a(m) is the vector with components aij(m), (i, j) ∈ A, and
F (m) =
{
x(m) satisfying Eq. (10.25) | xij(m) ∈ Xij(m), ∀ (i, j) ∈ A
}
.
An important observation here is that each of the minimization sub-
problems above is a minimum cost flow problem that can be solved using
the methods of Chapters 2-7. Furthermore, if xµ(m) solves the mth sub-
problem, the vector
gµ =
M∑
m=1
A(m)xµ(m) − b (10.29)
is a subgradient of q at µ.
Let us now discuss the computational solution of the dual problem
maxµ≥0 q(µ). The application of the subgradient method is straightfor-
ward, so we concentrate on the cutting plane method, which leads to a
method known as Dantzig-Wolfe decomposition. This method consists of
the iteration
µk = arg max
µ≥0
Qk(µ),
where Qk(µ) is the piecewise linear approximation of the dual function
based on the preceding function values q(µ0), . . . , q(µk−1), and the corre-
sponding subgradients g0, . . . , gk−1:
Qk(µ) = min
{
q(µ0) + (µ − µ0)′g0, . . . , q(µk−1) + (µ − µk−1)′gk−1
}
.
Consider now the cutting plane subproblem maxµ≥0 Qk(µ). By in-
troducing an auxiliary variable v, we can write this problem as
maximize v
subject to q(µi) + (µ − µi)′gi ≥ v, i = 0, . . . , k − 1,
µ ≥ 0.
(10.30)
This is a linear program in the variables v and µ. We can form its dual
problem by assigning a Lagrange multiplier ξi to each of the constraints
Sec. 10.3 Lagrangian Relaxation 509
q(µi) + (µ − µi)′gi ≥ v. After some calculation, this dual problem can be
verified to have the form
minimize
k−1∑
i=0
ξi
(
q(µi) − µi′gi
)
subject to
k−1∑
i=0
ξi = 1,
k−1∑
i=0
ξigi ≤ 0,
ξi ≥ 0, i = 0, . . . , k − 1.
(10.31)
Using Eqs. (10.28) and (10.29), we have
q(µi) = −µi′b +
M∑
m=1
(
a(m) + A(m)′µi
)′
xµ
i(m),
gi =
M∑
m=1
A(m)xµi(m) − b,
so the problem (10.31) can be written as
minimize
M∑
m=1
a(m)′
k−1∑
i=0
ξixµ
i(m)
subject to
k−1∑
i=0
ξi = 1,
M∑
m=1
A(m)
k−1∑
i=0
ξixµ
i(m) ≤ b,
ξi ≥ 0, i = 0, . . . , k − 1.
(10.32)
The preceding problem is called the master problem. It is the dual of
the cutting plane subproblem maxµ≥0 Qk(µ), which in turn approximates
the dual problem maxµ≥0 q(µ); in short, it is the dual of the approximate
dual . We may view this problem as an approximate version of the primal
problem where the commodity flow vectors x(m) are constrained to lie in
the convex hull of the already generated vectors xµi(m), i = 0, . . . , k − 1,
rather than in their original constraint set. It can be shown, using linear
programming theory, that if the problem (10.30) has an optimal solution
[i.e., enough vectors µi are available so that the maximum of Qk(µ) over
µ ≥ 0 is attained], then the master problem also has an optimal solution.
Suppose now that we solve the master problem (10.32) using a method
that yields a Lagrange multiplier vector, call it µk, corresponding to the
constraints
M∑
m=1
A(m)
k−1∑
i=0
ξixµ
i(m) ≤ b.
510 Network Problems with Integer Constraints Chap. 10
(Standard linear programming methods, such as the simplex method, can
be used for this.) Then, the dual of the master problem [which is the cutting
plane subproblem maxµ≥0 Qk(µ)] is solved by the Lagrange multiplier µk.
Therefore, µk is the next iterate of the cutting plane method.
We can now piece together the typical cutting plane iteration.
Cutting Plane – Dantzig-Wolfe Decomposition Iteration
Step 1: Given µ0, . . . , µk−1, and the commodity flow vectors xµi(m)
for m = 1, . . . , M and i = 0, . . . , k − 1, solve the master problem
minimize
M∑
m=1
a(m)′
k−1∑
i=0
ξixµ
i(m)
subject to
k−1∑
i=0
ξi = 1,
M∑
m=1
A(m)
k−1∑
i=0
ξixµ
i(m) ≤ b,
ξi ≥ 0, i = 0, . . . , k − 1.
and obtain µk, which is a Lagrange multiplier vector of the constraints
M∑
m=1
A(m)
k−1∑
i=0
ξixµ
i(m) ≤ b.
Step 2: For each m = 1, . . . , M , obtain a solution xµk(m) of the
minimum cost flow problem
min
x(m)∈F (m)
(
a(m) + A(m)′µk
)′
x(m).
Step 3: Use xµk(m) to modify the master problem by adding one
more variable ξk and go to the next iteration.
Decomposition by Right-Hand Side Allocation
There is an alternative decomposition approach for solving the multicom-
modity flow problem with side constraints (10.24)-(10.27). In this ap-
proach, we introduce auxiliary variables y(m), m = 1, . . . , M , and we write
Sec. 10.3 Lagrangian Relaxation 511
the problem as
minimize
M∑
m=1
a(m)′x(m)
subject to x(m) ∈ F (m), m = 1, . . . , M,
M∑
m=1
y(m) = b, A(m)x(m) ≤ y(m), m = 1, . . . , M.
Equivalently, we can write the problem as
minimize
M∑
m=1
min
x(m)∈F (m)
A(m)x(m)=y(m)
a(m)′x(m)
subject to
M∑
m=1
y(m) = b, y(m) ∈ Y (m), m = 1, . . . , M,
(10.33)
where Y (m) is the set of all vectors y(m) for which the inner minimization
problem
minimize a(m)′x(m)
subject to x(m) ∈ F (m), A(m)x(m) ≤ y(m)
(10.34)
has at least one feasible solution.
Let us define
pm
(
y(m)
)
= min
x(m)∈F (m)
A(m)x(m)≤y(m)
a(m)′x(m).
Then, problem (10.33) can be written as
minimize
M∑
m=1
pm
(
y(m)
)
subject to
M∑
m=1
y(m) = b, y(m) ∈ Y (m), m = 1, . . . , M.
This problem, called the master problem, may be solved with nondifferen-
tiable optimization methods, and in particular with the subgradient and
the cutting plane methods. Note, however, that the commodity problems
(10.34) involve the side constraints A(m)x(m) ≤ y(m), and need not be
of the minimum cost flow type, except in special cases. We refer to the
literature cited at the end of the chapter for further details.
512 Network Problems with Integer Constraints Chap. 10
10.4 LOCAL SEARCH METHODS
Local search methods are a broad and important class of heuristics for
discrete optimization. They apply to the general problem of minimizing
a function f(x) over a finite set F of (feasible) solutions. In principle,
one may solve the problem by global enumeration of the entire set F of
solutions (this is what branch-and-bound does). A local search method
tries to economize on computation by using local enumeration, based on
the notion of a neighborhood N(x) of a solution x, which is a (usually very
small) subset of F , containing solutions that are “close” to x in some sense.
In particular, given a solution x, the method selects among the solu-
tions in the neighborhood N(x) a successor solution x, according to some
rule. The process is then repeated with x replacing x (or stops when some
termination criterion is met). Thus a local search method is characterized
by:
(a) The method for choosing a starting solution.
(b) The definition of the neighborhood N(x) of a solution x.
(c) The rule for selecting a successor solution from within N(x).
(d) The termination criterion.
For an example of a local search method, consider the k-OPT heuristic
for the traveling salesman problem that we discussed in Example 10.1. Here
the starting tour is obtained by using some method, based for example on
subtour elimination or a minimum weight spanning tree, as discussed in
Example 10.1. The neighborhood of a tour T is defined as the set N(T )
of all tours obtained from T by exchanging k arcs that belong to T with
another k arcs that do not belong to T . The rule for selecting a successor
tour is based on cost improvement; that is, the tour selected from N(T )
has minimum cost over all tours in N(T ) that have smaller cost than T .
Finally, the algorithm terminates when no tour in N(T ) has smaller cost
than T . Another example of a local search method is provided by the
Esau-Williams heuristic of Fig. 10.5.
The definition of a neighborhood often involves intricate calculations
and suboptimizations that aim to bring to consideration promising neigh-
bors. Here is an example, due to Kernighan and Lin [1970]:
Example 10.12. (Uniform Graph Partitioning)
Consider a graph (N ,A) with 2n nodes, and a cost aij for each arc (i, j). We
want to find a partition of N into two subsets N1 and N2, each with n nodes,
so that the total cost of the arcs connecting N1 and N2,∑
(i,j), i∈N1, j∈N2
aij +
∑
(i,j), i∈N2, j∈N1
aij ,
Sec. 10.4 Local Search Methods 513
is minimized.
Here a natural neighborhood of a partition (N1,N2) is the k-exchange
neighborhood . This is the set of all partitions obtained by selecting a fixed
number k of pairs of nodes (i, j) with i ∈ N1 and j ∈ N2, and interchang-
ing them, that is, moving i into N2 and j into N1. The corresponding local
search algorithm moves from a given solution to its minimum cost neighbor,
and terminates when no neighbor with smaller cost can be obtained. Unfor-
tunately, the amount of work needed to generate a k-exchange neighborhood
increases exponentially with k [there are
(
m
k
)
different ways to select k objects
out of m]. One may thus consider a variable depth neighborhood that involves
multiple successive k-exchanges with small k. As an example, for k = 1 we
obtain the following algorithm:
Given the starting partition (N1,N2), consider all pairs (i, j) with i ∈
N1 and j ∈ N2, and let c(i, j) be the cost change that results from moving i
into N2 and j into N1. If (i, j) is the pair that minimizes c(i, j), move i into
N1 and j into N2, and let c1 = c(i, j). Repeat this process a fixed number M
of times, obtaining a sequence c2, c3, . . . , cM of minimal cost changes resulting
from the sequence of exchanges. Then find
m = arg min
m=1,...,M
m∑
l=1
cl,
and accept as the next partition the one involving the first m exchanges.
This type of algorithm avoids the exponential running time of k-exchange
neighborhoods, while still considering neighbors differing by as many as M
node pairs.
While the definition of neighborhood is often problem-dependent,
some general classes of procedures for generating neighborhoods have been
developed. One such class is genetic algorithms, to be discussed shortly.
In some cases, neighborhoods are dynamically changing, and they may de-
pend not only on the current solution, but also on several past solutions.
The method of tabu search, to be discussed shortly, falls in this category.
The criterion for selecting a solution from within a neighborhood is
usually the cost of the solution, but sometimes a more complex criterion
based on various problem characteristics and/or constraint violation con-
siderations is adopted. An important possibility, which is the basis for the
simulated annealing method , to be discussed shortly, is to use a random
mechanism for selecting the successor solution within a neighborhood.
Finally, regarding the termination criterion, many local search meth-
ods are cost improving , and stop when an improved solution cannot be
found within the current neighborhood. This means that these methods
stop at a local minimum, that is, a solution that is no worse than all other
solutions within its neighborhood. Unfortunately, for many problems, a
local minimum may be far from optimal, particularly if the neighborhood
used is relatively small. Thus, for a cost improving method, there is a basic
tradeoff between using a large neighborhood to diminish the difficulty with
514 Network Problems with Integer Constraints Chap. 10
local minima, and paying the cost of increased computation per iteration.
Note that there is an important advantage to a cost improving method: it
can never repeat the same solution, so that in view of the finiteness of the
feasible set F , it will always terminate with a local minimum.
An alternative type of neighbor selection and termination criterion,
used by simulated annealing and tabu search, is to allow successor solutions
to have worse cost than their predecessors, but to also provide mechanisms
that ensure the future generation of improved solutions with substantial
likelihood. The advantage of accepting solutions of worse cost is that stop-
ping at a local minimum becomes less of a difficulty. For example, the
method of simulated annealing, cannot be trapped at a local minimum,
as we will see shortly. Unfortunately, methods that do not enforce cost
improvement run the danger of cycling through repetition of the same so-
lution. It is therefore essential in these methods to provide a mechanism
by virtue of which cycling is either precluded, or becomes highly unlikely.
As a final remark, we note an important advantage of local search
methods. While they offer no solid guarantee of finding an optimal or
near-optimal solution, they offer the promise of substantial improvement
over any heuristic that can be used to generate the starting solution. Unfor-
tunately, however, one can seldom be sure that this promise will be fulfilled
in a given practical problem.
10.4.1 Genetic Algorithms
These are local search methods where the neighborhood generation mech-
anism is inspired by real-life processes of genetics and evolution. In par-
ticular, the current solution is modified by “splicing” and “mutation” to
obtain neighboring solutions. A typical example is provided by problems of
scheduling, such as the traveling salesman problem. The neighborhood of
a schedule T may be a collection of other schedules obtained by modifying
some contiguous portion of T in some way, while keeping the remainder of
the schedule T intact. Alternatively, the neighborhood of a schedule may
be obtained by interchanging the position of a few tasks, as in the k-OPT
traveling salesman heuristic.
In a variation of this approach, a pool of solutions may be maintained.
Some of these solutions may be modified, while some pairs of these solutions
may be combined to form new solutions. These solutions, are added to the
pool if some criterion, typically based on cost improvement, is met, and
some of the solutions of the existing pool may be dropped. In this way, it
is argued, the pool is “evolving” in a Darwinian way through a “survival
of the fittest” process.
A specific example implementation of this approach operates in phases.
At the beginning of a phase, we have a population X consisting of n feasible
solutions x1, . . . , xn. The phase proceeds as follows:
Sec. 10.4 Local Search Methods 515
Typical Phase of a Genetic Algorithm
Step 1: (Local Search) Starting from each solution xi of the current
population X, apply a local search algorithm up to obtaining a local
minimum xi. Let X = {x1, . . . , xn}.
Step 2: (Mutation) Select at random a subset of elements of X, and
modify each element according to some (problem dependent) mecha-
nism, to obtain another feasible solution.
Step 3: (Recombination) Select at random a subset of pairs of
elements of X, and produce from each pair a feasible solution according
to some (problem dependent) mechanism.
Step 4: (Survivor Selection) Let X̃ be the set of feasible solutions
obtained from the mutation and recombination Steps 3 and 4. Out of
the population X ∪ X̃, select a subset of n elements according to some
criterion. Use this subset to start the next phase.
Mutation allows speculative variations of the local minima at hand,
while recombination (also called crossover) aims to combine attributes of
a pair of local minima. The processes of mutation and recombination are
usually performed with the aid of some data structure that is used to rep-
resent a solution, such as for example a string of bits. There is a very large
number of variants of genetic algorithm approaches. Typically, these ap-
proaches are problem-dependent and require a lot of trial-and-error. How-
ever, genetic algorithms are quite easy to implement, and have achieved
considerable popularity. We refer to the literature cited at the end of the
chapter for more details.
10.4.2 Tabu Search
Tabu search aims to avoid getting trapped at a poor local minimum, by
accepting on occasion a worse or even infeasible solution from within the
current neighborhood. Since cost improvement is not enforced, tabu search
runs the danger of cycling, i.e., repeating the same sequence of solutions
indefinitely. To alleviate this problem, tabu search keeps track of recently
obtained solutions in a “forbidden” (tabu) list. Solutions in the tabu list
cannot be regenerated, thereby avoiding cycling, at least in the short run.
In a more sophisticated variation of this strategy, the tabu list contains
attributes of recently obtained solutions rather than the solutions them-
selves. Solutions with attributes in the tabu list are forbidden from being
generated (except under particularly favorable circumstances, under which
the tabu list is overridden).
Tabu search is also based on an elaborate web of implementation
heuristics that have been developed through experience with a large num-
516 Network Problems with Integer Constraints Chap. 10
ber of practical problems. These heuristics regulate the size of the current
neighborhood, the criterion of selecting a new solution from the current
neighborhood, the criterion for termination, etc. These heuristics may also
involve selective memory storage of previously generated solutions or their
attributes, penalization of the constraints with (possibly time-varying)
penalty parameters, and multiple tabu lists. We refer to the literature
cited at the end of the chapter for further details.
10.4.3 Simulated Annealing
Simulated annealing is similar to tabu search in that it occasionally allows
solutions of inferior cost to be generated. It differs from tabu search in the
manner in which it avoids cycling. Instead of checking deterministically
the preceding solutions for cycling, it simply randomizes its selection of the
next solution. In doing so, it not only avoids cycling, but also provides
some theoretical guarantee of escaping from local minima and eventually
finding a global minimum.
Being able to find a global minimum is not really exciting in itself.
For example, under fairly general conditions, one can do so by using unso-
phisticated random search methods, such as for example a method where
feasible solutions are sampled at random. However, simulated annealing
randomizes the choice of the successor solution from within the current
neighborhood in a way that gives preference to solutions of smaller cost,
and in doing so, it aims to find a global minimum faster than simple-minded
random search methods.
In particular, given a solution x, we select by random sampling a can-
didate solution x from the neighborhood N(x). The sampling probabilities
are positive for all members of N(x), but are otherwise unspecified. The
solution x is accepted if it is cost improving, that is
f(x) < f(x).
Otherwise, x is accepted with probability
e−
(
f(x)−f(x)
)
/T ,
where T is some positive constant, and is rejected with the complementary
probability.
The constant T regulates the likelihood of accepting solutions of worse
cost. It is called the temperature of the process (the name is inspired by a
certain physical analogy that will not be discussed here). The likelihood of
accepting a solution x of worse cost than x decreases as its cost increases.
Furthermore, when T is large (or small), the probability of accepting a
worse solution is close to 1 (or close to 0, respectively). In practice, it is
typical to start with a large T , allowing a better chance of escaping from
Sec. 10.5 Rollout Algorithms 517
local minima, and then to reduce T gradually to enhance the selectivity of
the method towards improved solutions.
Contrary to genetic algorithms and tabu search, which offer no gen-
eral theoretical guarantees of good performance, simulated annealing is
supported by solid theory. In particular, under fairly general conditions,
it can be shown that a global minimum will be eventually visited (with
probability 1), and that with gradual reduction of the temperature T , the
search process will be confined with high likelihood to solutions that are
globally optimal.
For an illustrative analysis, assume that T is kept constant and let
pxy be the probability that when the current solution is x, the next solu-
tion sampled is y. Consider the special case where pxy = pyx for all feasible
solutions x and y, and assume that the Markov chain defined by the prob-
abilities pxy is irreducible, in the sense that there is positive probability to
go from any x to any y, with one or more samples. Then it can be shown
(see Exercise 10.20) that the steady-state probability of a solution x is
e−f(x)/T∑
x∈F e
−f(x)/T .
Essentially, this says that for very small T and far into the future, the
current solution is almost always optimal.
When the condition pxy = pyx does not hold, one cannot obtain a
closed-form expression for the steady-state probabilities of the various so-
lutions. However, as long as the underlying Markov chain is irreducible,
the behavior is qualitatively similar: the steady-state probability of nonop-
timal solutions diminishes to 0 as T approaches 0. There is also related
analysis for the case where the temperature parameter T is time-varying
and converges to 0; see the references cited at the end of the chapter.
The results outlined above should be viewed with a grain of salt. In
practice, speed of convergence is as important as eventual convergence to
the optimum, and solving a given problem by simulated annealing can be
very slow. A nice aspect of the method is that it depends very little on
the structure of the problem being solved, and this enhances its value for
relatively unstructured problems that are not well-understood. For other
problems, where there exists a lot of accumulated insight and experience,
simulated annealing is usually inferior to other local search approaches.
10.5 ROLLOUT ALGORITHMS
The branch-and-bound algorithm is guaranteed to find an optimal flow
vector, but it may require the solution of a very large number of sub-
problems. Basically, the algorithm amounts to an exhaustive search of the
518 Network Problems with Integer Constraints Chap. 10
entire branch-and-bound tree. An alternative is to consider faster methods
that are based on intelligent but nonexhaustive search of the tree. In this
section, we develop one such method, the rollout algorithm, which, in its
simplest version, sequentially constructs a suboptimal flow vector by fixing
the arc flows, a few arcs at a time. The rollout algorithm can be combined
with most heuristics, including the local search methods of the preceding
section, and is capable of magnifying their effectiveness.
Let us consider the minimization of a function f of a flow vector x
over a feasible set F , and let us assume that F is finite (presumably because
of some integer constraints on the arc flows). Define a partial solution to
be a collection of arc flows {xij | (i, j) ∈ S}, corresponding to some proper
subset of arcs S ⊂ A. Such a collection is distinguished from a flow vector
(S = A), which is also referred to as a complete solution.
The rollout algorithm generates a sequence of partial solutions, cul-
minating with a complete solution. For this purpose, it employs some
problem-dependent heuristic algorithm, called the base heuristic. This al-
gorithm, given a partial solution
P =
{
xij | (i, j) ∈ S
}
,
produces a complementary solution
P =
{
xij | (i, j) /∈ S
}
,
and a corresponding (complete) flow vector
x =
{
xij | (i, j) ∈ A
}
= P ∪ P .
The cost of this flow vector is denoted by
H(P ) =
{
f(x) if x ∈ F ,
∞ otherwise,
and is called the heuristic cost of the partial solution P . If P is a com-
plete solution, which is feasible, i.e., a flow vector x ∈ F , by convention the
heuristic cost of P is the true cost f(x). There are no restrictions on the na-
ture of the base heuristic; a typical example is an integer rounding heuristic
applied to the solution of some related linear or convex network problem,
which may be obtained by relaxing/neglecting the integer constraints.
The rollout algorithm starts with some partial solution, or with the
empty set of arcs, S = Ø. It enlarges a partial solution iteratively, with a
few arc flows at a time. The algorithm terminates when a complete solution
is obtained. At the start of the typical iteration, we have a current partial
solution
P =
{
xij | (i, j) ∈ S
}
,
Sec. 10.5 Rollout Algorithms 519
and at the end of the iteration, we augment this solution with some more
arc flows. The steps of the iteration are as follows:
Iteration of the Rollout Algorithm
Step 1: Select a subset T of arcs that are not in S according to some
criterion. (The arc selection method is usually based on some heuristic
preliminary optimization, and is problem-dependent.)
Step 2: Consider the collection FT of all possible values of the arc
flows y =
{
yij | (i, j) ∈ T
}
, and apply the base heuristic to compute
the heuristic cost H(P+y ) of the augmented partial solution
P+y =
{{
xij | (i, j) ∈ S
}
,
{
yij | (i, j) ∈ T
}}
for each y ∈ FT .
Step 3: Choose from the set FT the arc flows y =
{
yij | (i, j) ∈ T
}
that minimize the heuristic cost H(P+y ); that is, find
y = arg min
y∈FT
H(P+y ). (10.35)
Step 4: Augment the current partial solution {xij | (i, j) ∈ S
}
with
the arc flows
{
yij | (i, j) ∈ T
}
thus obtained, and proceed with the
next iteration.
As an example of this algorithm, let us consider the traveling salesman
problem, and let us use as base heuristic the nearest neighbor method,
whereby we start from some simple path and at each iteration, we add a
node that does not close a cycle and minimizes the cost of the enlarged path.
The rollout algorithm operates as follows: After k iterations, we have a path
{i1, . . . , ik} consisting of distinct nodes. At the next iteration, we run the
nearest neighbor heuristic starting from each of the paths {i1, . . . , ik, i}
with i 	= i1, . . . , ik, and obtain a corresponding tour. We then select as
next node ik+1 of the path the node i that corresponds to the best tour
thus obtained. Here, the set of arcs used to augment the current partial
solution in the rollout algorithm is
T =
{
(ik, i) | i 	= i1, . . . , ik
}
,
and at the kth iteration the flows of all of these arcs are set to 0, except
for arc (ik, ik+1) whose flow is set to 1.
Note that a rollout algorithm requires considerably more computation
than the base heuristic. For example, in the case where the subset T in Step
520 Network Problems with Integer Constraints Chap. 10
1 consists of a single arc, the rollout algorithm requires O(mn) applications
of the base heuristic, where
m is the number of arcs, and
n is a bound on the number of possible values of each arc flow.
Nonetheless the computational requirements of the rollout algorithm may
be quite manageable. In particular, if the arc flows are restricted to be
0 or 1, and the base heuristic has polynomial running time, so does the
corresponding rollout algorithm.
An important question is whether, given an initial partial solution,
the rollout algorithm performs at least as well as its base heuristic when
started from that solution. This can be guaranteed if the base heuristic is
sequentially consistent . By this we mean that the heuristic has the following
property:
Suppose that starting from a partial solution
P =
{
xij | (i, j) ∈ S
}
,
the heuristic produces the complementary solution
P =
{
xij | (i, j) /∈ S
}
.
Then starting from the partial solution
P+ =
{
xij | (i, j) ∈ S ∪ T
}
,
the heuristic produces a complementary solution
P
+
=
{
xij | (i, j) /∈ S ∪ T
}
,
which coincides with P on the arcs (i, j) /∈ S ∪ T .
As an example, it can be seen that the nearest neighbor heuristic for
the traveling salesman problem, discussed earlier, is sequentially consistent.
This is a manifestation of a more general property: many common base
heuristics of the greedy type are by nature sequentially consistent (see
Exercise 10.21). It may be verified, based on Eq. (10.35), that a sequentially
consistent rollout algorithm keeps generating the same solution P ∪ P , up
to the point where by examining the alternatives in Eq. (10.35) and by
calculating their heuristic costs, it discovers a better solution. As a result,
sequential consistency guarantees that the costs of the successive solutions
P ∪P produced by the rollout algorithm are monotonically nonincreasing;
that is, we have
H(P+) ≤ H(P )
at every iteration. Thus, the cost f(xt) of the solution xt produced upon
termination of the rollout algorithm is at least as small as the cost f(x0)
Sec. 10.5 Rollout Algorithms 521
of the initial solution x0 produced by the base heuristic. For further elab-
oration of the sequential consistency property, we refer to the paper by
Bertsekas, Tsitsiklis, and Wu [1997], which also discusses some underlying
connections with the policy iteration method of dynamic programming.
A condition that is more general than sequential consistency is that
the algorithm be sequentially improving , in the sense that at each iteration
there holds
H(P+) ≤ H(P ).
This property also guarantees that the cost of the solutions produced by the
rollout algorithm is monotonically nonincreasing. The paper by Bertsekas,
Tsitsiklis, and Wu [1997] discusses situations where this property holds,
and shows that with fairly simple modification, a rollout algorithm can be
made sequentially improving (see also Exercise 10.22).
There are a number of variations of the basic rollout algorithm de-
scribed above. Here are some examples:
(1) We may adapt the rollout framework to use multiple heuristic al-
gorithms. In particular, let us assume that we have K algorithms
H1, . . . ,HK . The kth of these algorithms, given an augmented par-
tial solution P+y , produces a heuristic cost Hk(P+y ). We may then use
in the flow selection via Eq. (10.35) a heuristic cost of the form
H(P+y ) = min
k=1,...,K
Hk(P+y ),
or of the form
H(P+y ) =
K∑
k=1
rkHk(P+y ),
where rk are some fixed scalar weights obtained by trial and error.
(2) We may incorporate multistep lookahead or selective depth lookahead
into the rollout framework. Here we consider augmenting the current
partial solution P =
{
xij | (i, j) ∈ S
}
with all possible values for
the flows of a finite sequence of arcs that are not in S. We run the
base heuristic from each of the corresponding augmented partial so-
lutions, we select the sequence of arc flows with minimum heuristic
cost, and then augment the current partial solution P with the first
arc flow in this sequence. As an illustration, let us recall the traveling
salesman problem with the nearest neighbor method used as the base
heuristic. An example rollout algorithm with two-step lookahead op-
erates as follows: We begin each iteration with a path {i1, . . . , ik}.
We run the nearest neighbor heuristic starting from each of the paths
{i1, . . . , ik, i} with i 	= i1, . . . , ik, and obtain a corresponding tour.
We then form the subset I consisting of the m nodes i 	= i1, . . . , ik
that correspond to the m best tours thus obtained. We run the near-
est neighbor heuristic starting from each of the paths {i1, . . . , ik, i, j}
522 Network Problems with Integer Constraints Chap. 10
with i ∈ I and j 	= i1, . . . , ik, i, and obtain a corresponding tour. We
then select as the next node ik+1 of the path the node i ∈ I that
corresponds to a minimum cost tour.
(3) We may use alternative methods for computing a cost H(P+y ) of a
candidate augmented partial solution P+y for use in the flow selection
via Eq. (10.35). For example, instead of generating this cost via the
base heuristic, we may calculate it as the optimal or approximately
optimal cost of a suitable optimization problem. In particular, it is
possible to use a cost derived from Lagrangian relaxation, whereby
at a given partial solution, an appropriate dual problem is solved,
and its optimal cost is used in place of the heuristic cost H in Eq.
(10.35). Alternatively, a complementary solution may be constructed
based on minimization of the corresponding Lagrangian function. As
another example, one may use as cost of a partial solution, some
heuristic measure of quality of the partial solution; this idea forms
the basis for computer chess, where various positions are evaluated
using a heuristic “position evaluation function.”
Let us provide a few examples of rollout algorithms. The first example
is very simple, but illustrates well the notions of sequential consistency and
sequential improvement.
Example 10.13. (One-Dimensional Walk)
Consider a person who walks on a straight line and at each time period takes
either a unit step to the left or a unit step to the right. There is a cost
function assigning cost f(i) to each integer i. Given an integer starting point
on the line, the person wants to minimize the cost of the point where he will
end up after a given and fixed number N of steps.
We can formulate this problem as a problem of selecting a path in a
graph (see Fig. 10.15). In particular, without loss of generality, let us assume
that the starting point is the origin, so that the person’s position after N
steps will be some integer in the interval [−N, N ]. The nodes of the graph
are identified with pairs (k, m), where k is the number of steps taken so far
(k = 1, . . . , N) and m is the person’s position (m ∈ [−k, k]). A node (k, m)
with k < N has two outgoing arcs with end nodes (k+1, m−1) (corresponding
to a left step) and (k+1, m+1) (corresponding to a right step). Let us consider
paths whose starting node is (0, 0) and the destination node is of the form
(N, m), where m is of the form N − 2l and l ∈ [0, N ] is the number of left
steps taken. The problem then is to find the path of this type such that f(m)
is minimized.
Let the base heuristic be the algorithm, which, starting at a node (k, m),
takes N − k successive steps to the right and terminates at the node (N, m +
N − k). It can be seen that this algorithm is sequentially consistent [the base
heuristic generates the path (k, m), (k+1, m+1), . . . , (N, m+N −k) starting
from (k, m), and also the path (k + 1, m + 1), . . . , (N, m + N − k) starting
from (k + 1, m + 1), so the criterion for sequential consistency is fulfilled].
Sec. 10.5 Rollout Algorithms 523
The rollout algorithm, at node (k, m) compares the cost of the des-
tination node (N, m + N − k) (corresponding to taking a step to the right
and then following the base heuristic) and the cost of the destination node
(N, m + N − k − 2) (corresponding to taking a step to the left and then fol-
lowing the base heuristic). Let us say that an integer i ∈ [−N + 2, N − 2] is
a local minimum if f(i − 2) ≥ f(i) and f(i) ≤ f(i + 2). Let us also say that
N (or −N) is a local minimum if f(N − 2) ≤ f(N) [or f(−N) ≤ f(−N + 2),
respectively]. Then it can be seen that starting from the origin (0, 0), the
rollout algorithm obtains the local minimum that is closest to N , (see Fig.
10.15). This is no worse (and typically better) than the integer N obtained
by the base heuristic. This example illustrates how the rollout algorithm may
exhibit “intelligence” that is totally lacking from the base heuristic.
f (i )
iNN - 2-N 0
(N,0)
(0,0)
(N,-N ) (N,N )
i
i
Time
Position
Figure 10.15: Illustration of the path generated by the rollout algorithm in
Example 10.13. It keeps moving to the left up to the time where the base
heuristic generates two destinations (N, i) and (N, i− 2) with f(i) ≤ f(i− 2).
Then it continues to move to the right ending at the destination (N, i), which
corresponds to the local minimum closest to N .
Consider next the case where the base heuristic is the algorithm that,
starting at a node (k, m), compares the cost f(m + N − k) (corresponding to
taking all of the remaining N−k steps to the right) and the cost f(m−N +k)
(corresponding to taking all of the remaining N − k steps to the left), and
accordingly moves to node
(N, m + N − k) if f(m + N − k) ≤ f(m − N + k),
524 Network Problems with Integer Constraints Chap. 10
or to node
(N, m − N + k) if f(m − N + k) < f(m + N − k).
It can be seen that this base heuristic is not sequentially consistent, but is
instead sequentially improving. It can then be verified that starting from the
origin (0, 0), the rollout algorithm obtains the global minimum of f in the
interval [−N, N ], while the base heuristic obtains the better of the two points
−N and N .
Example 10.14. Constrained Traveling Salesman Problem
Consider the traveling salesman problem of Example 10.1, where we want to
minimize the cost ∑
(i,j)∈T
aij ,
of a tour T , while satisfying the side constraints∑
(i,j)∈T
ckij ≤ dk, ∀ k = 1, . . . , K.
A rollout algorithm starts with the trivial path P = (s), where s is some
initial node, progressively constructs a sequence of paths P = (s, i1, . . . , im),
m = 1, . . . , N − 1, consisting of distinct nodes, and then completes a tour by
adding the arc (iN−1, s). The rollout procedure is as follows.
We introduce nonnegative penalty coefficients µk for the side constraints,
and we form modified arc traversal costs âij , given by
âij = aij +
K∑
k=1
µkckij .
The method of obtaining µk is immaterial for our purposes in this example,
but we note that one possibility is to use the Lagrangian relaxation method
of Section 10.3. We assume that we have a heuristic algorithm that can
complete the current path P = (s, i1, . . . , im) with a path (im+1, . . . , iN−1, s),
thereby obtaining a tour T ∗(P ) that has approximately minimum modified
cost. Some of the heuristics mentioned in Example 10.1, including the k-OPT
heuristic, can be used for this purpose. Furthermore, we assume that by using
another heuristic, we can complete the current path P to a tour T̂ (P ) that
satisfies all the side constraints.
Given the current path P = (s, i1, . . . , im), the rollout algorithm, con-
siders the set Am of all arcs (im, j) ∈ A such that j does not belong to P .
For each of the nodes j such that (im, j) ∈ Am, it considers the expanded
path Pe = (s, i1, . . . , im, j) and obtains the tours T
∗(Pe) and T̂ (Pe), using the
heuristics mentioned earlier. The rollout algorithm then adds to the current
partial path P the node j for which the tour T ∗(Pe) satisfies the side con-
straints and has minimum cost (with respect to the arc costs aij); if no path
Sec. 10.6 Notes, Sources, and Exercises 525
T ∗(Pe) satisfies the side constraints, the algorithm adds to the current path
the node j for which the tour T̂ (Pe) has minimum cost.
One of the drawbacks of the scheme just described is that it requires the
approximate solution of a large number of traveling salesman problems. A
faster variant is obtained if the arc set Am above is restricted to be a suitably
chosen subset of the eligible arcs (im, j), such for example those whose length
does not exceed a certain threshold.
Finally, it is interesting to compare rollout algorithms with the local
search methods of the preceding section. Both types of algorithms generate
a sequence of solutions, but in the case of a rollout algorithm, the generated
solutions are partial (except at termination), while in a local search method,
the generated solutions are complete. In both types of algorithms, the next
solution is generated from within a neighborhood of the current solution,
but the selection criterion in rollout algorithms is the estimated cost of the
solution as obtained by the base heuristic, while in local search methods,
it is typically the true cost of the solution. Finally, in rollout algorithms,
there is no concern about local minima and cycling, but there is also no
provision for improving a complete solution after it is obtained.
There are interesting possibilities for combining a rollout algorithm
with a local search method. In particular, one may use a local search
method as part of a base heuristic in a rollout algorithm; here, the local
search method could be fairly unsophisticated, since one may hope that
the rollout process will provide an effective mechanism for solution im-
provement. Alternatively, one may first use a rollout algorithm to obtain
a complete solution, and then use a local search method in an effort to
improve this solution.
10.6 NOTES, SOURCES, AND EXERCISES
There is a great variety of integer constrained network flow problems, and
the associated methodological and applications literature is vast. For text-
book treatments at various levels of sophistication, which also cover broader
aspects of integer programming, see Lawler [1976], Zoutendijk [1976], Pa-
padimitriou and Steiglitz [1982], Minoux [1986a], Schrijver [1986], Nem-
hauser and Wolsey [1988], Bogart [1990], Pulleyblank, Cook, Cunning-
ham, and Schrijver [1993], Cameron [1994], and Cook, Cunningham, Pul-
leyblank, and Schrijver [1998]. Volumes 7 and 8 of the Handbooks in
Operations Research and Management Science, edited by Ball, Magnanti,
Monma, and Nemhauser [1995a], [1995b], are devoted to network theory
and applications, and include several excellent survey papers with large bib-
liographies. O’hEigeartaigh, Lenstra, and Rinnoy Kan [1985] provide an
extensive bibliography on combinatorial optimization. Von Randow [1982],
526 Network Problems with Integer Constraints Chap. 10
[1985] gives an extensive bibliography on integer programming and related
subjects.
The traveling salesman problem has been associated with many of the
important investigations in discrete optimization. It was first considered
in a modern setting by Dantzig, Fulkerson, and Johnson [1954], whose pa-
per stimulated much interest and research. The edited volume by Lawler,
Lenstra, Rinnoy Kan, and Shmoys [1985] focuses on the traveling sales-
man problem and its variations, and the papers by Junger, Reinelt, and
Rinaldi [1995], and by Johnson and McGeoch [1997] provide extensive sur-
veys of the subject. There is a large literature on the use of polyhedral
approximations to the feasible set of integer programming problems and
the traveling salesman problem in particular; see, for example, the papers
by Cornuejols, Fonlupt, and Naddef [1985], Grötschel and Padberg [1985],
Padberg and Grötschel [1985], Pulleyblank [1983], and the books by Nem-
hauser and Wolsey [1988], and Schrijver [1986]. The papers by Burkard
[1990], Gilmore, Lawler, and Shmoys [1985], and Tsitsiklis [1992] discuss
some special cases of the traveling salesman problem and some extensions.
The monograph by Martello and Toth [1990] is devoted to generalized
assignment problems, including ones with integer constraints. The book
by Kershenbaum [1993] provides a lot of material on tree construction and
network design algorithms for data communications; see also Monma and
Sheng [1986], Minoux [1989], Bertsekas and Gallager [1992], and Grötschel,
Monma, and Stoer [1995]. Exact and heuristic methods for the Steiner tree
problem are surveyed by Winter [1987] and Voß [1992].
Matching problems are discussed in detail in the monograph by Lovasz
and Plummer [1985], the survey by Gerards [1995], and Chapter 10 of
the book by Murty [1992]. For vehicle and arc routing problems, see the
surveys by Assad and Golden [1995], Desrosiers, Dumas, Solomon, and
Soumis [1995], Eiselt, Gendreau, and Laporte [1995a], [1995b], Federgruen
and Simchi-Levi [1995], Fisher [1995], and Powell, Jaillet, and Odoni [1995].
An important application of multidimensional assignment problems
arises in the context of multi-target tracking and data association; see
Blackman [1986], Bar-Shalom and Fortman [1988], Pattipati, Deb, Bar-
Shalom, and Washburn [1992], Poore [1994], Poore and Robertson [1997].
The material on the error bounds for the enforced separation heuristic in
three-dimensional assignment problems (Exercise 10.31) is apparently new.
Integer multicommodity flow problems are discussed by Barnhart,
Hane, and Vance [1997]. Nonlinear, nonconvex network optimization is
discussed by Lamar [1993], Bell and Lamar [1993], as well as in general texts
on global optimization; see Pardalos and Rosen [1987], Floudas [1995], and
Horst, Pardalos, and Thoai [1995]. For a textbook treatment of scheduling
(cf. Exercises 10.23-10.27), see Pinedo [1995].
Branch-and-bound has its origins in the traveling salesman paper by
Dantzig, Fulkerson, and Johnson [1954]. Their paper was followed by Croes
[1958], Eastman [1958], and Land and Doig [1960], who considered versions
Sec. 10.6 Notes, Sources, and Exercises 527
of the branch-and-bound method in the context of various integer program-
ming problems. The term “branch-and-bound” was first used by Little,
Murty, Sweeney, and Karel [1963], in the context of the traveling salesman
problem. Balas and Toth [1985], and Nemhauser and Wolsey [1988] provide
extensive surveys of branch-and-bound.
Lagrangian relaxation was suggested in the context of discrete op-
timization by Held and Karp [1970], [1971]. Subgradient methods were
introduced by Shor in the Soviet Union during the middle 60s. The conver-
gence properties of subgradient methods and their variations are discussed
in a number of sources, including Auslender [1976], Goffin [1977], Shapiro
[1979], Shor [1985], Poljak [1987], Hiriart-Urruty and Lemarechal [1993],
Brannlund [1993], Bertsekas [1995b], and Goffin and Kiwiel [1996].
Cutting plane methods were proposed by Cheney and Goldstein [1959],
and by Kelley [1960]; see also the book by Goldstein [1967]. Central cut-
ting plane methods were introduced by Elzinga and Moore [1975]. More
recent proposals, some of which relate to interior point methods, are dis-
cussed in Goffin and Vial [1990], Goffin, Haurie, and Vial [1992], Ye [1992],
Kortanek and No [1993], Goffin, Luo, and Ye [1993], [1996], Atkinson and
Vaidya [1995], Nesterov [1995], Luo [1996], and Kiwiel [1997b].
Three historically important references on decomposition methods
are Dantzig and Wolfe [1960], Benders [1962], and Everett [1963]. An
early text on large-scale optimization and decomposition is Lasdon [1970];
see also Geoffrion [1970], [1974]. Subgradient methods have been applied
to the solution of multicommodity flow problems using a decomposition
framework by Kennington and Shalaby [1977]. The book by Censor and
Zenios [1997] discusses several applications of decomposition in a variety of
algorithmic contexts.
The literature of local search methods is extensive. The edited volume
by Aarts and Lenstra [1997] contains several surveys of broad classes of
methods. Osman and Laporte [1996] provide an extensive bibliography.
The book by Goldberg [1989] focuses on genetic algorithms. Tabu
search was initiated with the works of Glover [1986] and Hansen [1986].
The book by Glover and Laguna [1997], and the surveys by Glover [1989],
[1990], Glover, Taillard, and de Verra [1993] provide detailed expositions
and give many references.
Simulated annealing was proposed by Kirkpatrick, Gelatt, and Vecchi
[1983] based on earlier suggestions by Metropolis, Rosenbluth, Rosenbluth,
Teller, and Teller [1953]; see also Cerny [1985]. The main theoretical con-
vergence properties of the method were established by Hajek [1988] and
Tsitsiklis [1989]; see also the papers by Connors and Kumar [1989], Gelfand
and Mitter [1989], and Bertsimas and Tsitsiklis [1993], and the book by Ko-
rst, Aarts, and Korst [1989]. A framework for integration of local search
methods is presented by Fox [1993], [1995].
Rollout algorithms for discrete optimization were proposed in the
book by Bertsekas and Tsitsiklis [1996] in the context of the neuro-dynamic
528 Network Problems with Integer Constraints Chap. 10
programming methodology, and in the paper by Bertsekas, Tsitsiklis, and
Wu [1997]. An application to scheduling using the framework of the quiz
problem (cf. Exercises 10.28 and 10.29) is described by Bertsekas and
Castañon [1998]. The idea of sequential selection of candidates for par-
ticipation in a solution is implicit in several combinatorial optimization
contexts. For example this idea is embodied in the sequential fan candi-
date list strategy as applied in tabu search (see Glover, Taillard, and de
Werra [1993]). A similar idea is also used in the sequential automatic test
procedures of Pattipati (see e.g., Pattipati and Alexandridis [1990]).
E X E R C I S E S
10.1
Consider the symmetric traveling salesman problem with the graph shown in Fig.
10.16.
(a) Find a suboptimal solution using the nearest neighbor heuristic starting
from node 1.
(b) Find a suboptimal solution by first solving an assignment problem, and by
then merging subtours.
(c) Try to improve the solutions found in (a) and (b) by using the 2-OPT
heuristic.
1
5
2 3
4
Symmetric Traveling Salesman
Problem Data. 
Costs Shown Next to the Arcs.
Each arc is bidirectional.
1
0
5
9
2
2
9
5
8 8
Figure 10.16: Data for a symmetric trav-
eling salesman problem (cf. Exercise 10.1).
The arc costs are shown next to the arcs.
Each arc is bidirectional.
Sec. 10.6 Notes, Sources, and Exercises 529
10.2 (Minimum Cost Cycles)
Consider a strongly connected graph with a nonnegative cost for each arc. We
want to find a forward cycle of minimum cost that contains all nodes but is not
necessarily simple; that is, a node or an arc may be traversed multiple times.
(a) Convert this problem into a traveling salesman problem. Hint : Construct
a complete graph with cost of an arc (i, j) equal to the shortest distance
from i to j in the original graph.
(b) Apply your method of part (a) to the graph of Fig. 10.17.
3
5
1
1
2
1
1 4
3
2
1 Figure 10.17: Data for a minimum cost cycle
problem (cf. Exercise 10.2). The arc costs are
shown next to the arcs.
10.3
Consider the problem of checking whether a given graph contains a simple cycle
that passes through all the nodes. (The cycle need not be forward.) Formulate
this problem as a symmetric traveling salesman problem. Hint : Consider a
complete graph where the cost of an arc (i, j) is 1 if (i, j) or (j, i) is an arc
of the original graph, and is 2 otherwise.
10.4
Show that an asymmetric traveling salesman problem with nodes 1, . . . , N and arc
costs aij can be converted to a symmetric traveling salesman problem involving
a graph with nodes 1, . . . , N, N + 1, . . . , 2N, and the arc costs
ai(N+j) =
{
aij if i, j = 1, . . . , N, i = j,
−M if i = j,
where M is a sufficiently large number. Hint : All arcs with cost −M must be
included in an optimal tour of the symmetric version.
10.5
Consider the problem of finding a shortest (forward) path from an origin node s
to a destination node t of a graph with given arc lengths, subject to the additional
constraint that the path passes through every node exactly once.
530 Network Problems with Integer Constraints Chap. 10
(a) Show that the problem can be converted to a traveling salesman problem
by adding an artificial arc (t, s) of length −M , where M is a sufficiently
large number.
(b) (Longest Path Problem) Consider the problem of finding a simple forward
path from s to t that has a maximum number of arcs. Show that the
problem can be converted to a traveling salesman problem.
10.6
Consider the problem of finding a shortest (forward) path in a graph with given
arc lengths, subject to the constraint that the path passes through every node
exactly once (the choice of start and end nodes of the path is subject to opti-
mization). Formulate the problem as a traveling salesman problem.
10.7 (Traveling Salesman Problem/Triangle Inequality)
Consider a symmetric traveling salesman problem where the arc costs are non-
negative and satisfy the following triangle inequality :
aij ≤ aik + akj , for all nodes i, j, k.
This problem has some special algorithmic properties.
(a) Consider a procedure, which given a cycle {i0, i1, . . . , iK , i0} that contains
all the nodes (but passes through some of them multiple times), obtains a
tour by deleting nodes after their first appearance in the cycle; e.g., in a
5-node problem, starting from the cycle {1, 3, 5, 2, 3, 4, 2, 1}, the procedure
produces the tour {1, 3, 5, 2, 4, 1}. Use the triangle inequality to show that
the tour thus obtained has no greater cost than the original cycle.
(b) Starting with a spanning tree of the graph, use the procedure of part (a)
to construct a tour with cost equal to at most two times the total cost of
the spanning tree. Hint : The cycle should cross each arc of the spanning
tree exactly once in each direction. “Double” each arc of the spanning tree.
Use the fact that if a graph is connected and each of its nodes has even
degree, there is a cycle that contains all the arcs of the graph exactly once
(cf. Exercise 1.5).
(c) (Double tree heuristic) Start with a minimum cost spanning tree of the
graph, and use part (b) to construct a tour with cost equal to at most
twice the optimal tour cost.
(d) Verify that the problem of Fig. 10.18 satisfies the triangle inequality. Apply
the method of part (c) to this problem.
10.8 (Christofides’ Traveling Salesman Heuristic)
Consider a symmetric traveling salesman problem where the arc costs are non-
negative and satisfy the triangle inequality (cf. the preceding exercise). Let R
Sec. 10.6 Notes, Sources, and Exercises 531
1
5
2 3
4
Symmetric Traveling Salesman
Problem Data. 
Costs Shown Next to the Arcs.
Each arc is bidirectional.
1
3
5
3
2
2
6
5
4 4
Figure 10.18: Data for a symmetric travel-
ing salesman problem (cf. Exercises 10.7 and
10.8). The arc costs are shown next to the
arcs.
be a minimum cost spanning tree of the graph (cf. Exercise 2.30), and let S be
the subset of the nodes that has an odd number of incident arcs in R. A perfect
matching of the nodes of S is a subset of arcs such that every node of S is an end
node of exactly one arc of the subset and each arc of the subset has end nodes
in S. Suppose that M is a perfect matching of the nodes of S that has minimum
sum of arc costs. Construct a tour that consists of the arcs of M and some of the
arcs of R, and show that its weight is no more than 3/2 times the optimal tour
cost. Solve the problem of Fig. 10.18 using this heuristic, and find the ratio of
the solution cost to the optimal tour cost. Hint : Note that the total cost of the
arcs of M is at most 1/2 the optimal tour cost. Also, use the fact that if a graph
is connected and each of its nodes has even degree, there is a cycle that contains
all the arcs of the graph exactly once (cf. Exercise 1.5).
10.9 (K-Traveling Salesmen Problem)
Consider the version of the traveling salesman problem where there are K sales-
men that start at city 1, return to city 1, and collectively must visit all other
cities exactly once. Transform the problem into an ordinary traveling salesman
problem. Hint : Split city 1 into K cities.
10.10 (Degree-Constrained Minimum Weight Spanning Trees)
Consider the minimum weight spanning tree problem, subject to the additional
constraint that the number of tree arcs that are incident to a single given node
s should be no greater than a given integer k. Consider adding a nonnegative
weight w to the weight of all incident arcs of node s, solving the corresponding
unconstrained spanning tree problem, and gradually increasing w until the degree
constraint is satisfied.
(a) State a polynomial algorithm for doing this and derive its running time.
(b) Use this algorithm to solve the problem of Fig. 10.19, where the degree of
node 1 is required to be no more than 2.
532 Network Problems with Integer Constraints Chap. 10
1
5
2 3
4
Spanning Tree Problem Data.
Weights Shown Next to the Arcs.
1
1
9
9
2
2
5
5
Figure 10.19: Data for a minimum weight
spanning tree problem (cf. Exercises 10.10 and
10.11). The arc weights are shown next to the
arcs.
10.11 (Steiner Tree Problem Heuristic)
We are given a connected graph G with a nonnegative weight aij for each arc
(i, j) ∈ A. We assume that if an arc (i, j) is present, the reverse arc (j, i) is also
present, and aij = aji. Consider the problem of finding a tree in G that spans a
given subset of nodes S and has minimum weight over all such trees.
(a) Let W ∗ be the weight of this tree. Consider the graph I(G), which has
node set S and is complete (has an arc connecting every pair of its nodes).
Let the weight for each arc (i, j) of I(G) be equal to the shortest distance in
the graph G from the node i ∈ S to the node j ∈ S. Let T be a minimum
weight spanning tree of I(G). Show that the weight of T is no greater
than 2W ∗. Hint : Consider a minimum weight tour in I(G). Show that the
weight of this tour is no less than the weight of T and no more than 2W ∗.
(b) Construct a heuristic based on part (a) and apply it to the problem of Fig.
10.19, where S = {1, 3, 5}.
10.12 (A General Heuristic for Spanning Tree Problems)
Consider a minimum weight spanning tree problem with an additional side con-
straint denoted by C (for example, a degree constraint on each node). A general
heuristic (given by Deo and Kumar [1997]) is to solve the problem neglecting
the constraint C, and then to add a scalar penalty to the cost of the arcs that
“contribute most” to violation of C. This is then repeated as many times as
desired.
(a) Construct a heuristic of this type for the capacitated spanning tree problem
(cf. Example 10.3).
(b) Adapt this heuristic to a capacitated Steiner tree problem.
10.13
Consider the Königsberg bridge problem (cf. Fig. 10.6).
Sec. 10.6 Notes, Sources, and Exercises 533
(a) Suppose that there existed a second bridge connecting the islands B and
C, and also another bridge connecting the land areas A and D. Construct
an Euler cycle that crosses each of the bridges exactly once.
(b) Suppose the bridge connecting the islands B and C has collapsed. Con-
struct an Euler path, i.e., a path (not necessarily a cycle) that passes
through each arc of the graph exactly once.
(c) Construct an optimal postman cycle assuming all arcs have cost 1.
10.14
Formulate the capacitated spanning tree problem given in Fig. 10.5 as an integer-
constrained network flow problem.
10.15 (Network Formulation of Nonbipartite Matching)
Consider the nonbipartite matching problem of Example 10.4. Replace each
node i with a pair of nodes i and i′. For every arc (i, j) of the original problem,
introduce an arc (i, j′) with value aij and an arc (j, i
′) also with value aij . Show
that the problem can be formulated as the assignment-like problem involving the
conservation of flow inequalities∑
j′
xij′ ≤ 1, ∀ i,
∑
i
xij′ ≤ 1, ∀ j′,
the integer constraints xij′ ∈ {0, 1}, and the side constraints∑
{j|(i,j)∈A}
xij +
∑
{j|(j,i)∈A}
xji ≤ 1, ∀ i ∈ N ,
or ∑
{j|(i,j)∈A}
xij +
∑
{j|(j,i)∈A}
xji = 1, ∀ i ∈ N ,
in the case where a perfect matching is sought.
10.16 (Matching Solution of the Chinese Postman Problem)
Given a Chinese postman problem, delete all nodes of even degree together with
all their incident arcs. Find a perfect matching of minimum cost in the remaining
graph. Create an expanded version of the original problem’s graph by adding an
extra copy of each arc of the minimum cost matching. Show that an Euler cycle
of the expanded graph is an optimal solution to the Chinese postman problem.
534 Network Problems with Integer Constraints Chap. 10
10.17 (Solution of the Directed Chinese Postman Problem)
Consider expanding the graph of the directed Chinese postman problem by du-
plicating arcs so that the number of incoming arcs to each node is equal to the
number of its outgoing arcs. A forward Euler cycle of the expanded graph cor-
responds to a solution of the directed Chinese postman problem. Show that the
optimal expanded graph is obtained by minimizing∑
(i,j)∈A
aijxij
subject to the constraints∑
{j|(i,j)∈A}
xij −
∑
{j|(j,i)∈A}
xji = di, ∀ i ∈ N ,
0 ≤ xij , ∀ (i, j) ∈ A,
where di is the difference between the number of incoming arcs to i and the
number of outgoing arcs from i.
10.18 (Shortest Paths and Branch-and-Bound)
Consider a general integer-constrained problem of the form
minimize f(x1, . . . , xn)
subject to x ∈ X, xi ∈ {0, 1}, i = 1, . . . , n,
where X is some set. Construct a branch-and-bound tree that starts with a sub-
problem where the integer constraints are relaxed, and proceeds with successive
restriction of the variables x1, . . . , xn to the values 0 or 1.
(a) Show that the original integer-constrained problem is equivalent to a single
origin/single destination shortest path problem that involves the branch-
and-bound tree. Hint : As an example, for the traveling salesman problem,
nodes of the tree correspond to sequences (i1, . . . , ik) of distinct cities, and
arcs correspond to pairs of nodes (i1, . . . , ik) and (i1, . . . , ik, ik+1).
(b) Modify the label correcting method of Section 2.5.2 so that it becomes
similar to the branch-and-bound method (see also the discussion in Section
2.5.2).
10.19
Use the branch-and-bound method to solve the capacitated spanning tree problem
of Fig. 10.5.
Sec. 10.6 Notes, Sources, and Exercises 535
10.20 (Simulated Annealing)
In the context of simulated annealing, assume that T is kept constant and let pxy
be the probability that when the current solution is x, the next solution sampled
is y. Consider the special case where pxy = pyx for all feasible solutions x and y,
and assume that the Markov chain defined by the probabilities pxy is irreducible,
in the sense that there is positive probability to go from any x to any y, with one
or more samples. Show that the steady-state probability of a solution x is
πx =
e−f(x)/T
C
,
where
C =
∑
x∈F
e−f(x)/T .
Hint : This exercise assumes some basic knowledge of the theory of Markov chains.
Let qxy be the probability that y is the next solution if x is the current solution,
i.e.,
qxy =
{
pxye
−
(
f(y)−f(x)
)
/T
if f(y) > f(x),
pxy otherwise.
Show that for all x and y we have πyqyx = πxqxy, and that πy =
∑
x∈F πxqxy.
This equality together with
∑
x∈F πx = 1 is sufficient to show the result.
10.21 (Rollout Algorithms Based on Greedy Algorithms)
In the context of the rollout algorithm, suppose that given a partial solution
P =
{
xij | (i, j) ∈ S
}
, we have an estimate c(P ) of the optimal cost over all
feasible solutions that are consistent with P , in the sense that there exists a
complementary solution P =
{
xij | (i, j) /∈ S
}
such that P ∪ P is feasible.
Consider a heuristic algorithm, which is greedy with respect to c(P ), in the sense
that it starts from S = Ø, and given the partial solution P =
{
xij | (i, j) ∈ S
}
,
it selects a set of arcs T , forms the collection FT of all possible values of the arc
flows y =
{
yij | (i, j) ∈ T
}
, and finds
y = arg min
y∈FT
c(P+y ). (10.36)
where
P+y =
{{
xij | (i, j) ∈ S
}
,
{
yij | (i, j) ∈ T
}}
.
It then augments P with the arc flows y thus obtained, and repeats up to ob-
taining a complete solution. Assume that the set of arcs T selected depends only
on P . Furthermore, the ties in the minimization of Eq. (10.36) are resolved in a
fixed manner that depends only on P . Show that the rollout algorithm that uses
the greedy algorithm as a base heuristic is sequentially consistent.
536 Network Problems with Integer Constraints Chap. 10
10.22 (Sequentially Improving Rollout Algorithm)
Consider a variant of the rollout algorithm that starts with the empty set of arcs,
and maintains, in addition to the current partial solution P =
{
xij | (i, j) ∈
S
}
, a complementary solution P ′ =
{
x′ij | (i, j) /∈ S
}
, and the corresponding
(complete) flow vector x′ = P ∪P ′. At the typical iteration, we select a subset T
of arcs that are not in S, and we consider the collection FT of all possible values
of the arc flows y =
{
yij | (i, j) ∈ T
}
. Then, if
min
y∈FT
H(P+y ) < f(x
′),
we augment the current partial solution {xij | (i, j) ∈ S
}
with the arc flows
y =
{
yij | (i, j) ∈ T
}
that attain the minimum above, and we set x′ equal to the
complete solution generated by the base heuristic starting from P+
y
. Otherwise,
we augment the current partial solution to {xij | (i, j) ∈ S
}
with the arc flows{
x′ij | (i, j) ∈ T
}
and we leave x′ unchanged. Prove that this rollout algorithm is
sequentially improving in the sense that the heuristic costs of the partial solutions
generated are monotonically nonincreasing.
10.23 (Scheduling Problems Viewed as Assignment Problems)
A machine can be used to perform a subset of N given tasks over T time periods.
At each time period t, only a subset A(t) of tasks can be performed. Each task
j has value vj(t) when performed at period t.
(a) Formulate the problem of finding the sequence of tasks of maximal total
value as an assignment problem. Hint : Assign time periods to tasks.
(b) Suppose that there are in addition some precedence constraints of the gen-
eral form: Task j must be performed before task j′ can be performed.
Formulate the problem as an assignment problem with side constraints
and integer constraints. Give an example where the integer constraints are
essential.
(c) Repeat part (b) for the case where there are no precedence constraints, but
instead some of the tasks require more than one time period.
10.24 (Scheduling and the Interchange Argument)
In some scheduling problems it is useful to try to characterize a globally optimal
solution based on the fact that it is locally optimal with respect to the 2-OPT
heuristic. This is known as the interchange argument , and amounts to starting
with an optimal schedule and checking to see what happens when any two tasks
in the schedule are interchanged. As an example, suppose that we have N jobs
to process in sequential order with the ith job requiring a given time Ti for its
execution. If job i is completed at time t, the reward is αtRi, where α is a given
discount factor with 0 < α < 1. The problem is to find a schedule that maximizes
Sec. 10.6 Notes, Sources, and Exercises 537
the total reward. Suppose that L = (i0, . . . , ik−1, i, j, ik+2, . . . , iN−1) is an opti-
mal job schedule, and consider the schedule L′ = (i0, . . . , ik−1, j, i, ik+2, . . . , iN−1)
obtained by interchanging i and j. Let tk be the time of completion of job ik−1.
Compare the rewards of the two schedules, and show that
αTiRi
1 − αTi ≥
αTj Rj
1 − αTj
.
Conclude that scheduling jobs in order of decreasing αTiRi/
(
1−αTi
)
is optimal.
10.25 (Weighted Shortest Processing Time First Rule)
We want to schedule N tasks, the ith of which requires Ti time units. Let ti
denote the time of completion of the ith task, i.e.,
ti = Ti +
∑
tasks k
completed before i
Tk.
Let wi denote a positive weight indicating the importance of early completion of
the ith task. Use an interchange argument (cf. Exercise 10.24) to show that in
order to minimize the total weighted completion time
∑N
i=1
witi we must order
the tasks in decreasing order of wi/Ti.
10.26
A busy professor has to complete N projects. Each project i has a deadline di
and the time it takes the professor to complete it is Ti. The professor can work
on only one project at a time and must complete it before moving on to a new
project. For a given order of completion of the projects, denote by ti the time of
completion of project i, i.e.,
ti = Ti +
∑
projects k
completed before i
Tk.
The professor wants to order the projects so as to minimize the maximum tardi-
ness, given by
max
i∈{1,...,N}
max[0, ti − di].
Use an interchange argument (cf. Exercise 10.24) to show that it is optimal to
complete the projects in the order of their deadlines (do the project with the
closest deadline first).
538 Network Problems with Integer Constraints Chap. 10
10.27 (Hardy’s Theorem)
Let {a1, . . . , an} and {b1, . . . , bn} be monotonically nondecreasing sequences of
numbers. Let us associate with each i = 1, . . . , n a distinct index ji, and consider
the expression
∑n
i=1
aibji . Use an interchange argument (cf. Exercise 10.24) to
show that this expression is maximized when ji = i for all i, and is minimized
when ji = n − i + 1 for all i.
10.28 (The Quiz Problem)
Consider a quiz contest where a person is given a list of N questions and can
answer these questions in any order he chooses. Question i will be answered
correctly with probability pi, independently of earlier answers, and the person will
then receive a reward Ri. At the first incorrect answer, the quiz terminates and
the person is allowed to keep his previous rewards. The problem is to maximize
the expected reward by choosing optimally the ordering of the questions.
(a) Show that to maximize the expected reward, questions should be answered
in decreasing order of piRi/(1 − pi). Hint : Use an interchange argument
(cf. Exercise 10.24).
(b) Consider the variant of the problem where there is a maximum number
of questions that can be answered, which is smaller than the number of
questions that are available. Show that it is not necessarily optimal to
answer the questions in order of decreasing piRi/(1 − pi). Hint : Try the
case where only one out of two available questions can be answered.
(c) Give a 2-OPT algorithm to solve the problem where the number of available
questions is one more than the maximum number of questions that can be
answered.
10.29 (Rollout Algorithm for the Quiz Problem)
Consider the quiz problem of Exercise 10.28 for the case where the maximum
number of questions that can be answered is less or equal to the number of
questions that are available. Consider the heuristic which answers questions in
decreasing order of piRi/(1 − pi), and use it as a base heuristic in a rollout
algorithm. Show that the cost of the rollout algorithm is no worse than the cost
of the base heuristic. Hint : Prove sequential consistency of the base heuristic.
10.30
This exercise shows that nondifferentiabilities of the dual function given in Section
10.3, often tend to arise at the most interesting points and thus cannot be ignored.
Show that if there is a duality gap, then the dual function q is nondifferentiable
at every dual optimal solution. Hint : Assume that q has a unique subgradient at
a dual optimal solution µ∗ and derive a contradiction by showing that any vector
xµ∗ that minimizes L(x, µ
∗) is primal optimal.
Sec. 10.6 Notes, Sources, and Exercises 539
10.31 (Enforced Separation in 3-Dimensional Assignment)
Consider the 3-dimensional assignment problem of Example 10.7 that involves a
set of jobs J , a set of machines M , and a set of workers W . We assume that
each of the sets J , M , and W contains n elements, and that the constraints are
equality constraints. Suppose that the problem is -separable, in the sense that
for some βjm and γmw, and some  ≥ 0, we have
|βjm + γmw − ajmw| ≤ , ∀ j ∈ J, m ∈ M, w ∈ W,
where ajmw is the value of the triplet (j, m, w).
(a) Show that if the problem is solved with ajmw replaced by βjm + γmw, the
3-dimensional assignment obtained achieves the optimal cost of the original
problem within 2n.
(b) Suppose that we don’t know βjm and γmw, and that we use the enforced
separation approach of Example 10.7. Thus, we first solve the jobs-to-
machines 2-dimensional assignment problem with values
bjm = max
w∈W
ajmw.
Let jm be the job assigned to machine m, according to the solution of this
problem. We then solve the machines-to-workers 2-dimensional assignment
problem with values
cmw = ajmmw.
Let wm be the worker assigned to machine m, according to the solution of
this problem. Show that the 3-dimensional assignment {(jm, m, wm) | m ∈
M} achieves the optimal value of the original problem within 4n.
(c) Show that the result of part (b) also holds when bjm is defined by
bjm = ajmwm ,
where wm is any worker, instead of bjm = maxw∈W ajmw.
(d) Show that the result of parts (b) and (c) also holds if J and W contain
more than n elements, and we have the inequality constraints∑
m∈M
∑
w∈W
xjmw ≤ 1, ∀ j ∈ J,
∑
j∈J
∑
m∈M
xjmw ≤ 1, ∀ w ∈ W,
in place of equality constraints.
10.32 (Lagrangian Relaxation in Multidimensional Assignment)
Apply the Lagrangian relaxation method to the multidimensional assignment
problem of Example 10.7, in a way that requires the solution of 2-dimensional as-
signment problems. Derive the form of the corresponding subgradient algorithm.
540 Network Problems with Integer Constraints Chap. 10
10.33 (Separable Problems with Integer/Simplex Constraints)
Consider the problem
minimize
n∑
j=1
fj(xj)
subject to
n∑
j=1
xj ≤ A,
xj ∈ {0, 1, . . . , mj}, j = 1, . . . , n,
where A and m1, . . . , mn are given positive integers, and each function fj is con-
vex over the interval [0, mj ]. Consider an iterative algorithm (due to Ibaraki and
Katoh [1988]) that starts at (0, . . . , 0) and maintains a feasible vector (x1, . . . , xn).
At the typical iteration, we consider the set of indices J = {j | xj < mj}. If
J is empty or
∑n
j=1
xj = A, the algorithm terminates. Otherwise, we find an
index j ∈ J that maximizes fj(xj) − fj(xj + 1). If fj(xj) − fj(xj + 1) ≤ 0, the
algorithm terminates. Otherwise, we increase xj by one unit, and go to the next
iteration. Show that upon termination, the algorithm yields an optimal solution.
Note: The book by Ibaraki and Katoh [1988] contains a lot of material on this
problem, and addresses the issues of efficient implementation.
10.34 (Constraint Relaxation and Lagrangian Relaxation)
The purpose of this exercise is to compare the lower bounds obtained by relaxing
integer constraints and by dualizing the side constraints. Consider the nonlinear
network optimization problem with a cost function f(x), the conservation of flow
constraints, and the additional constraint
x ∈ X =
{
x | xij ∈ Xij , (i, j) ∈ A, gt(x) ≤ 0, t = 1, . . . , r
}
,
where Xij are given subsets of the real line and the functions gt are linear . We
assume that f is convex over the entire space of flow vectors x. We introduce a
Lagrange multiplier µt for each of the side constraints gt(x) ≤ 0, and we form
the corresponding Lagrangian function
L(x, µ) = f(x) +
r∑
t=1
µtgt(x).
Let C denote the set of all x satisfying the conservation of flow constraints, let
f∗ denote the optimal primal cost,
f∗ = inf
x∈C, xij∈Xij, gt(x)≤0
f(x),
and let q∗ denote the optimal dual cost,
q∗ = sup
µ≥0
q(µ) = sup
µ≥0
inf
x∈C, xij∈Xij
L(x, µ).
Sec. 10.6 Notes, Sources, and Exercises 541
Let X̂ij denote the interval which is the convex hull of the set Xij , and denote
by f̂ the optimal cost of the problem, where each set Xij is replaced by X̂ij ,
f̂ = inf
x∈C, xij∈X̂ij , gt(x)≤0
f(x). (10.37)
Note that this is a convex problem even if Xij embodies integer constraints.
(a) Show that f̂ ≤ q∗ ≤ f∗. Hint : Use Prop. 8.3 to show that problem (10.37)
has no duality gap and compare its dual cost with q∗.
(b) Assume that f is linear. Show that f̂ = q∗. Hint : The problem involved
in the definition of the dual function of problem (10.37) is a minimum cost
flow problem.
(c) Assume that C is a general polyhedron; that is, C is specified by a finite
number of linear equality and inequality constraints (rather than the con-
servation of flow constraints). Provide an example where f is linear and
we have f̂ < q∗.
10.35 (Duality Gap of the Knapsack Problem)
Given objects i = 1, . . . , n with positive weights wi and values vi, we want to
assemble a subset of the objects so that the sum of the weights of the subset does
not exceed a given T > 0, and the sum of the values of the subset is maximized.
This is the knapsack problem, which is a special case of a generalized assignment
problem (see Example 8.7). The problem can be written as
maximize
n∑
i=1
vixi
subject to
n∑
i=1
wixi ≤ T, xi ∈ {0, 1}, i = 1, . . . , n.
(a) Let f∗ and q∗ be the optimal primal and dual costs, respectively. Show
that
0 ≤ q∗ − f∗ ≤ max
i=1,...,n
vi.
(b) Consider the problem where T is multiplied by a positive integer k and
each object is replaced by k replicas of itself, while the object weights and
values stay the same. Let f∗(k) and q∗(k) be the corresponding primal and
dual costs. Show that
q∗(k) − f∗(k)
f∗(k)
≤ 1
k
maxi=1,...,n vi
f∗
,
so that the relative value of the duality gap tends to 0 as k → ∞. Note:
This exercise illustrates a generic property of many separable problems with
integer constraints: as the number of variables increases, the duality gap
decreases in relative terms (see Bertsekas [1982], Section 5.5, or Bertsekas
[1995b], Section 5.1, for an analysis and a geometrical interpretation of this
phenomenon).
542 Network Problems with Integer Constraints Chap. 10
10.36 (Convergence of the Subgradient Method)
Consider the subgradient method µk+1 = [µk + skgk]+, where the stepsize is
given by
sk =
q∗ − q(µk)
‖gk‖2
and q∗ is the optimal dual cost (this stepsize requires knowledge of q∗, which
is very restrictive, but the following Exercise 10.37 removes this restriction).
Assume that there exists at least one optimal dual solution.
(a) Use Eq. (10.16) to show that {µk} is bounded.
(b) Use the fact that {gk} is bounded (since the dual function is piecewise
linear), and Eq. (10.16) to show that q(µk) → q∗.
10.37 (A Convergent Variation of the Subgradient Method)
This exercise provides a convergence result for a common variation of the subgra-
dient method (the result is due to Brannlund [1993]; see also Goffin and Kiwiel
[1996]). Consider the iteration µk+1 = [µk + skgk]+, where
sk =
q̃ − q(µk)
‖gk‖2 .
(a) Suppose that q̃ is an underestimate of the optimal dual cost q∗ such that
q(µk) < q̃ ≤ q∗. [Here q̃ is fixed and the algorithm stops at µk if q(µk) ≥ q̃.]
Use the fact that {gk} is bounded to show that either for some k̄ we have
q(µk̄) ≥ q̃ or else q(µk) → q̃. Hint : Consider the function min
{
q(µ), q̃
}
and use the results of Exercise 10.36.
(b) Suppose that q̃ is an overestimate of the optimal dual cost, that is, q̃ > q∗.
Use the fact that {gk} is bounded to show that the length of the path
traveled by the method is infinite, that is,
∞∑
k=0
sk‖gk‖ =
∞∑
k=0
q̃ − q(µk)
‖gk‖ = ∞.
(c) Let δ0 and B be two positive scalars. Consider the following version of
the subgradient method. Given µk, apply successive subgradient iterations
with q̃ = q(µk) + δk in the stepsize formula in place of q(µ∗), until one of
the following two occurs:
(1) The dual cost exceeds q(µk) + δk/2.
(2) The length of the path traveled starting from µk exceeds B.
Then set µk+1 to the iterate with highest dual cost thus far. Furthermore,
in case (1), set δk+1 = δk, while in case (2), set δk+1 = δk/2. Use the fact
that {gk} is bounded to show that q(µk) → q∗.
Sec. 10.6 Notes, Sources, and Exercises 543
10.38 (Convergence Rate of the Subgradient Method)
Consider the subgradient method of Exercise 10.36, and let µ∗ be an optimal
dual solution.
(a) Show that
lim inf
k→∞
√
k
(
q(µ∗) − q(µk)
)
= 0.
Hint : Use Eq. (10.16) to show that
∑∞
k=0
(
q(µ∗) − q(µk)
)2
< ∞. Assume
that
√
k
(
q(µ∗) − q(µk)
)
≥  for some  > 0 and arbitrarily large k, and
reach a contradiction.
(b) Assume that for some a > 0 and all k, we have q(µ∗)−q(µk) ≥ a‖µ∗−µk‖.
Use Eq. (10.16) to show that for all k we have
‖µk+1 − µ∗‖ ≤ r‖µk − µ∗‖,
where r =
√
1 − a2/b2 and b is an upper bound on ‖gk‖.
10.39
Consider the cutting plane method.
(a) Give an example where the generated sequence q(µk) is not monotonically
nondecreasing.
(b) Give an example where, at the kth iteration, the method finds an optimal
dual solution µk but does not terminate because the criterion q(µk) =
Qk(µk) is not satisfied.
10.40 (Computational Rollout Problem)
Consider the rollout algorithm for the traveling salesman problem using as base
heuristic the nearest neighbor method, whereby we start from some simple path
and at each iteration, we add a node that does not close a cycle and minimizes
the cost of the enlarged path (see the paragraph following the description of
the rollout algorithm iteration in Section 10.5). Write a computer program to
apply this algorithm to the problem involving Hamilton’s 20-node graph (Exercise
1.35) for the case where all arcs have randomly chosen costs from the range
[0, 1]. For node pairs for which there is no arc, introduce an artificial arc with
cost randomly chosen from the range [100, 101]. Compare the performances of
the rollout algorithm and the nearest neighbor heuristic, and compile relevant
statistics by running a suitable large collection of randomly generated problem
instances. Verify that the rollout algorithm performs at least as well as the
nearest neighbor heuristic for each instance (since it is sequentially consistent).
References
Aarts, E., and Lenstra, J. K., 1997. Local Search in Combinatorial Opti-
mization, Wiley, N. Y.
Ahuja, R. K., Magnanti, T. L., and Orlin, J. B., 1989. “Network Flows,”
in Handbooks in Operations Research and Management Science, Vol. 1,
Optimization, Nemhauser, G. L., Rinnooy-Kan, A. H. G., and Todd M. J.
(eds.), North-Holland, Amsterdam, pp. 211-369.
Ahuja, R. K., Mehlhorn, K., Orlin, J. B., and Tarjan, R. E., 1990. “Faster
Algorithms for the Shortest Path Problem,” J. ACM, Vol. 37, 1990, pp.
213-223.
Ahuja, R. K., and Orlin, J. B., 1987. Private Communication.
Ahuja, R. K., and Orlin, J. B., 1989. “A Fast and Simple Algorithm for
the Maximum Flow Problem,” Operations Research, Vol. 37, pp. 748-759.
Amini, M. M., 1994. “Vectorization of an Auction Algorithm for Linear
Cost Assignment Problem,” Comput. Ind. Eng., Vol. 26, pp. 141-149.
Arezki, Y., and Van Vliet, D., 1990. “A Full Analytical Implementation of
the PARTAN/Frank-Wolfe Algorithm for Equilibrium Assignment,” Trans-
portation Science, Vol. 24, pp. 58-62.
Assad, A. A., and Golden, B. L., 1995. “Arc Routing Methods and Appli-
cations,” Handbooks in OR and MS, Ball, M. O., Magnanti, T. L., Monma,
C. L., and Nemhauser, G. L., (eds.), Vol. 8, North-Holland, Amsterdam,
pp. 375-483.
Atkinson, D. S., and Vaidya, P. M., 1995. “A Cutting Plane Algorithm for
Convex Programming that Uses Analytic Centers,” Math. Programming,
Vol. 69, pp. 1-44.
Auchmuty, G., 1989. “Variational Principles for Variational Inequalities,”
Numer. Functional Analysis and Optimization, Vol. 10, pp. 863-874.
Auslender, A., 1976. Optimization: Methodes Numeriques, Mason, Paris.
555
556 References
Balas, E., Miller, D., Pekny, J., and Toth, P., 1991. “A Parallel Shortest
Path Algorithm for the Assignment Problem,” J. ACM, Vol. 38, pp. 985-
1004.
Balas, E., and Toth, P., 1985. “Branch and Bound Methods,” in The Trav-
eling Salesman Problem, Lawler, E., Lenstra, J. K., Rinnoy Kan, A. H. G.,
and Shmoys, D. B. (eds.), Wiley, N. Y., pp. 361-401.
Balinski, M. L., 1985. “Signature Methods for the Assignment Problem,”
Operations Research, Vol. 33, pp. 527-537.
Balinski, M. L., 1986. “A Competitive (Dual) Simplex Method for the
Assignment Problem,” Math. Programming, Vol. 34, pp. 125-141.
Ball, M. O., Magnanti, T. L., Monma, C. L., and Nemhauser, G. L., 1995a.
Network Models, Handbooks in OR and MS, Vol. 7, North-Holland, Ams-
terdam.
Ball, M. O., Magnanti, T. L., Monma, C. L., and Nemhauser, G. L., 1995b.
Network Routing, Handbooks in OR and MS, Vol. 8, North-Holland, Am-
sterdam.
Bar-Shalom, Y., and Fortman, T. E., 1988. Tracking and Data Association,
Academic Press, N. Y.
Barnhart, C., Hane, C. H., and Vance, P. H., 1997. “Integer Multicommod-
ity Flow Problems,” in Network Optimization, Pardalos, P. M., Hearn, D.
W., and Hager, W. W. (eds.), Springer-Verlag, N. Y., pp. 17-31.
Barr, R., Glover, F., and Klingman, D., 1977. “The Alternating Basis
Algorithm for Assignment Problems,” Math. Programming, Vol. 13, pp.
1-13.
Barr, R., Glover, F., and Klingman, D., 1978. “Generalized Alternating
Path Algorithm for Transportation Problems,” European J. of Operations
Research, Vol. 2, pp. 137-144.
Barr, R., Glover, F., and Klingman, D., 1979. “Enhancement of Spanning
Tree Labeling Procedures for Network Optimization,” INFOR, Vol. 17, pp.
16-34.
Barr, R., and Hickman, B. L., 1994. “Parallel Simplex for Large Pure
Network Problems - Computational Testing and Sources of Speedup,” Op-
erations Research, Vol. 42, pp. 65-80.
Bazaraa, M. S., Jarvis, J. J., and Sherali, H. D., 1990. Linear Programming
and Network Flows (2nd Ed.), Wiley, N. Y.
Bazaraa, M. S., Sherali, H. D., and Shetty, C. M., 1993. Nonlinear Pro-
gramming Theory and Algorithms (2nd Ed.), Wiley, N. Y.
Bell, G. J., and Lamar, B. W., 1997. “Solution Methods for Nonconvex Net-
work Flow Problems,” in Network Optimization, Pardalos, P. M., Hearn,
References 557
D. W., and Hager, W. W. (eds.), Lecture Notes in Economics and Mathe-
matical Systems, Springer-Verlag, N. Y., pp. 32-50.
Bellman, R., 1957. Dynamic Programming, Princeton Univ. Press, Prince-
ton, N. J.
Benders, J. F., 1962. “Partitioning Procedures for Solving Mixed Variables
Programming Problems,” Numer. Math., Vol. 4, pp. 238-252.
Beraldi, P., and Guerriero, F., 1997. “A Parallel Asynchronous Implemen-
tation of the Epsilon-Relaxation Method for the Linear Minimum Cost
Flow Problem,” Parallel Computing, Vol. 23, pp. 1021-1044.
Beraldi, P., Guerriero, F., and Musmanno, R., 1996. “Parallel Algorithms
for Solving the Convex Minimum Cost Flow Problem,” Tech. Report PAR-
COLAB No. 8/96, Dept. of Electronics, Informatics, and Systems, Univ.
of Calabria.
Beraldi, P., Guerriero, F., and Musmanno, R., 1997. “Efficient Parallel Al-
gorithms for the Minimum Cost Flow Problem,” J. of Optimization Theory
and Applications, Vol. 95, pp. 501-530.
Berge, C., 1962. The Theory of Graphs and its Applications, Wiley, N. Y.
Berge, C., and Ghouila-Houri, A., 1962. Programming, Games, and Trans-
portation Networks, Wiley, N. Y.
Bertsekas, D. P., 1975a. “Nondifferentiable Optimization via Approxima-
tion,” Math. Programming Studies, Vol. 3, North-Holland, Amsterdam,
pp. 1-25.
Bertsekas, D. P., 1975b. “Necessary and Sufficient Conditions for a Penalty
Method to be Exact,” Math. Programming, Vol. 9, pp. 87-99.
Bertsekas, D. P., 1979a. “A Distributed Algorithm for the Assignment
Problem,” Lab. for Information and Decision Systems Working Paper,
M.I.T., Cambridge, MA.
Bertsekas, D. P., 1979b. “Algorithms for Nonlinear Multicommodity Net-
work Flow Problems,” in International Symposium on Systems Optimiza-
tion and Analysis, Bensoussan, A., and Lions, J. L. (eds.), Springer-Verlag,
N. Y., pp. 210-224.
Bertsekas, D. P., 1980. “A Class of Optimal Routing Algorithms for Com-
munication Networks,” Proc. of the Fifth International Conference on Com-
puter Communication, Atlanta, Ga., pp. 71-76.
Bertsekas, D. P., 1981. “A New Algorithm for the Assignment Problem,”
Math. Programming, Vol. 21, pp. 152-171.
Bertsekas, D. P., 1982. Constrained Optimization and Lagrange Multiplier
Methods, Academic Press, N. Y. (republished in 1996 by Athena Scientific,
Belmont, MA).
558 References
Bertsekas, D. P., 1985. “A Unified Framework for Minimum Cost Network
Flow Problems,” Math. Programming, Vol. 32, pp. 125-145.
Bertsekas, D. P., 1986a. “Distributed Asynchronous Relaxation Methods
for Linear Network Flow Problems,” Lab. for Information and Decision
Systems Report P-1606, M.I.T., Cambridge, MA.
Bertsekas, D. P., 1986b. “Distributed Relaxation Methods for Linear Net-
work Flow Problems,” Proceedings of 25th IEEE Conference on Decision
and Control, Athens, Greece, pp. 2101-2106.
Bertsekas, D. P., 1988. “The Auction Algorithm: A Distributed Relaxation
Method for the Assignment Problem,” Annals of Operations Research, Vol.
14, pp. 105-123.
Bertsekas, D. P., 1990. “The Auction Algorithm for Assignment and Other
Network Flow Problems: A Tutorial,” Interfaces, Vol. 20, pp. 133-149.
Bertsekas, D. P., 1991a. Linear Network Optimization: Algorithms and
Codes, MIT Press, Cambridge, MA.
Bertsekas, D. P., 1991b. “An Auction Algorithm for Shortest Paths,” SIAM
J. on Optimization, Vol. 1, pp. 425-447.
Bertsekas, D. P., 1992a. “Auction Algorithms for Network Flow Problems:
A Tutorial Introduction,” Computational Optimization and Applications,
Vol. 1, pp. 7-66.
Bertsekas, D. P., 1992b. “Modified Auction Algorithms for Shortest Paths,”
Lab. for Information and Decision Systems Report P-2150, M.I.T., Cam-
bridge, MA.
Bertsekas, D. P., 1992c. “An Auction Sequential Shortest Path Algorithm
for the Minimum Cost Network Flow Problem,” Lab. for Information and
Decision Systems Report P-2146, M.I.T.
Bertsekas, D. P., 1993a. “A Simple and Fast Label Correcting Algorithm
for Shortest Paths,” Networks, Vol. 23, pp. 703-709.
Bertsekas, D. P., 1993b. “Mathematical Equivalence of the Auction Algo-
rithm for Assignment and the -Relaxation (Preflow-Push) Method for Min
Cost Flow,” in Large Scale Optimization: State of the Art, Hager, W. W.,
Hearn, D. W., and Pardalos, P. M. (eds.), Kluwer, Boston, pp. 27-46.
Bertsekas, D. P., 1995a. Dynamic Programming and Optimal Control, Vols.
I and II, Athena Scientific, Belmont, MA.
Bertsekas, D. P., 1995b. Nonlinear Programming, Athena Scientific, Bel-
mont, MA.
Bertsekas, D. P., 1995c. “An Auction Algorithm for the Max-Flow Prob-
lem,” J. of Optimization Theory and Applications, Vol. 87, pp. 69-101.
References 559
Bertsekas, D. P., 1996. “Thevenin Decomposition and Network Optimiza-
tion,” J. of Optimization Theory and Applications, Vol. 89, pp. 1-15.
Bertsekas, D. P., and Castañon, D. A., 1989. “The Auction Algorithm for
Transportation Problems,” Annals of Operations Research, Vol. 20, pp.
67-96.
Bertsekas, D. P., and Castañon, D. A., 1991. “Parallel Synchronous and
Asynchronous Implementations of the Auction Algorithm,” Parallel Com-
puting, Vol. 17, pp. 707-732.
Bertsekas, D. P., and Castañon, D. A., 1992. “A Forward/Reverse Auction
Algorithm for Asymmetric Assignment Problems,” Computational Opti-
mization and Applications, Vol. 1, pp. 277-297.
Bertsekas, D. P., and Castañon, D. A., 1993a. “Asynchronous Hungarian
Methods for the Assignment Problem,” ORSA J. on Computing, Vol. 5,
pp. 261-274.
Bertsekas, D. P., and Castañon, D. A., 1993b. “Parallel Primal-Dual Meth-
ods for the Minimum Cost Flow Problem,” Computational Optimization
and Applications, Vol. 2, pp. 317-336.
Bertsekas, D. P., and Castañon, D. A., 1993c. “A Generic Auction Al-
gorithm for the Minimum Cost Network Flow Problem,” Computational
Optimization and Applications, Vol. 2, pp. 229-260.
Bertsekas, D. P., and Castañon, D. A., 1998. “Solving Stochastic Scheduling
Problems Using Rollout Algorithms,” Lab. for Information and Decision
Systems Report P-12413, M.I.T., Cambridge, MA.
Bertsekas, D. P., Castañon, D. A., Eckstein, J., and Zenios, S., 1995. “Par-
allel Computing in Network Optimization,” Handbooks in OR and MS,
Ball, M. O., Magnanti, T. L., Monma, C. L., and Nemhauser, G. L. (eds.),
Vol. 7, North-Holland, Amsterdam, pp. 331-399.
Bertsekas, D. P., Castañon, D. A., and Tsaknakis, H., 1993. “Reverse Auc-
tion and the Solution of Inequality Constrained Assignment Problems,”
SIAM J. on Optimization, Vol. 3, pp. 268-299.
Bertsekas, D. P., and El Baz, D., 1987. “Distributed Asynchronous Relax-
ation Methods for Convex Network Flow Problems,” SIAM J. on Control
and Optimization, Vol. 25, pp. 74-85.
Bertsekas, D. P., and Eckstein, J., 1987. “Distributed Asynchronous Re-
laxation Methods for Linear Network Flow Problems,” Proc. of IFAC ’87,
Munich, Germany.
Bertsekas, D. P., and Eckstein, J., 1988. “Dual Coordinate Step Methods
for Linear Network Flow Problems,” Math. Programming, Series B, Vol.
42, pp. 203-243.
560 References
Bertsekas, D. P., and Gafni, E. M., 1982. “Projection Methods for Varia-
tional Inequalities with Application to the Traffic Assignment Problem,”
Math. Progr. Studies, Vol. 17, North-Holland, Amsterdam, pp. 139-159.
Bertsekas, D. P., and Gafni, E. M., 1983. “Projected Newton Methods and
Optimization of Multicommodity Flows,” IEEE Trans. on Auto. Control,
Vol. 28, pp. 1090-1096.
Bertsekas, D. P., Gafni, E. M., and Gallager, R. G., 1984. “Second Deriva-
tive Algorithms for Minimum Delay Distributed Routing in Networks,”
IEEE Trans. on Communications, Vol. 32, pp. 911-919.
Bertsekas, D. P., and Gallager, R. G., 1992. Data Networks, (2nd Ed.),
Prentice-Hall, Englewood Cliffs, N. J.
Bertsekas, D. P., Guerriero, F., and Musmanno, R., 1996. “Parallel Asyn-
chronous Label Correcting Methods for Shortest Paths,” J. of Optimization
Theory and Applications, Vol. 88, pp. 297-320.
Bertsekas, D. P., Hosein, P., and Tseng, P., 1987. “Relaxation Methods for
Network Flow Problems with Convex Arc Costs,” SIAM J. on Control and
Optimization, Vol. 25, pp. 1219-1243.
Bertsekas, D. P, and Mitter, S. K., 1971. “Steepest Descent for Optimiza-
tion Problems with Nondifferentiable Cost Functionals,” Proc. 5th Annual
Princeton Confer. Inform. Sci. Systems, Princeton, N. J., pp. 347-351.
Bertsekas, D. P., and Mitter, S. K., 1973. “Descent Numerical Methods for
Optimization Problems with Nondifferentiable Cost Functions,” SIAM J.
on Control, Vol. 11, pp. 637-652.
Bertsekas, D. P., Pallottino, S., and Scutellà, M. G., 1995. “Polynomial
Auction Algorithms for Shortest Paths,” Computational Optimization and
Applications, Vol. 4, pp. 99-125.
Bertsekas, D. P., Polymenakos, L. C., and Tseng, P., 1997a. “An -Relaxati-
on Method for Separable Convex Cost Network Flow Problems,” SIAM J.
on Optimization, Vol. 7, pp. 853-870.
Bertsekas, D. P., Polymenakos, L. C., and Tseng, P., 1997b. “Epsilon-
Relaxation and Auction Methods for Separable Convex Cost Network Flow
Problems,” in Network Optimization, Pardalos, P. M., Hearn, D. W., and
Hager, W. W. (eds.), Lecture Notes in Economics and Mathematical Sys-
tems, Springer-Verlag, N. Y., pp. 103-126.
Bertsekas, D. P., and Tseng, P., 1988a. “Relaxation Methods for Minimum
Cost Ordinary and Generalized Network Flow Problems,” Operations Re-
search, Vol. 36, pp. 93-114.
Bertsekas, D. P., and Tseng, P., 1988b. “RELAX: A Computer Code for
Minimum Cost Network Flow Problems,” Annals of Operations Research,
Vol. 13, pp. 127-190.
References 561
Bertsekas, D. P., and Tseng, P., 1990. “RELAXT-III: A New and Improved
Version of the RELAX Code,” Lab. for Information and Decision Systems
Report P-1990, M.I.T., Cambridge, MA.
Bertsekas, D. P., and Tseng, P., 1994. “RELAX-IV: A Faster Version of
the RELAX Code for Solving Minimum Cost Flow Problems,” Laboratory
for Information and Decision Systems Report P-2276, M.I.T., Cambridge,
MA.
Bertsekas, D. P., and Tsitsiklis, J. N., 1989. Parallel and Distributed Com-
putation: Numerical Methods, Prentice-Hall, Englewood Cliffs, N. J. (re-
published in 1997 by Athena Scientific, Belmont, MA).
Bertsekas, D. P., and Tsitsiklis, J. N., 1996. Neuro-Dynamic Programming,
Athena Scientific, Belmont, MA.
Bertsekas, D. P., Tsitsiklis, J. N., and Wu, C., 1997. “Rollout Algorithms
for Combinatorial Optimization,” Heuristics, Vol. 3, pp. 245-262.
Bertsimas, D., and Tsitsiklis, J. N., 1993. “Simulated Annealing,” Stat.
Sci., Vol. 8, pp. 10-15.
Bertsimas, D., and Tsitsiklis, J. N., 1997. Introduction to Linear Optimiza-
tion, Athena Scientific, Belmont, MA.
Birkhoff, G., and Diaz, J. B., 1956. “Nonlinear Network Problems,” Quart.
Appl. Math., Vol. 13, pp. 431-444.
Bland, R. G., and Jensen, D. L., 1985. “On the Computational Behavior
of a Polynomial-Time Network Flow Algorithm,” Tech. Report 661, School
of Operations Research and Industrial Engineering, Cornell University.
Blackman, S. S., 1986. Multi-Target Tracking with Radar Applications,
Artech House, Dehdam, MA.
Bogart, K. P., 1990. Introductory Combinatorics, Harcourt Brace Jovano-
vich, Inc., New York, N. Y.
Bradley, G. H., Brown, G. G., and Graves, G. W., 1977. “Design and Imple-
mentation of Large-Scale Primal Transshipment Problems,” Management
Science, Vol. 24, pp. 1-38.
Brannlund, U., 1993. On Relaxation Methods for Nonsmooth Convex Opti-
mization, Doctoral Thesis, Royal Institute of Technology, Stockhorm, Swe-
den.
Brown, G. G., and McBride, R. D., 1984. “Solving Generalized Networks,”
Management Science, Vol. 30, pp. 1497-1523.
Burkard, R. E., 1990. “Special Cases of Traveling Salesman Problems and
Heuristics,” Acta Math. Appl. Sin., Vol. 6, pp. 273-288.
Busacker, R. G., and Gowen, P. J., 1961. “A Procedure for Determining a
562 References
Family of Minimal-Cost Network Flow Patterns,” O.R.O. Technical Report
No. 15, Operational Research Office, John Hopkins University, Baltimore,
MD.
Busacker, R. G., and Saaty, T. L., 1965. Finite Graphs and Networks: An
Introduction with Applications, McGraw-Hill, N. Y.
Cameron, P. J., 1994. Combinatorics: Topics, Techniques, Algorithms,
Cambridge Univ. Press, Cambridge, England.
Cantor, D. G., and Gerla, M., 1974. “Optimal Routing in a Packet Switched
Computer Network,” IEEE Trans. on Computers, Vol. 23, pp. 1062-1069.
Carpaneto, G., Martello, S., and Toth, P., 1988. “Algorithms and Codes
for the Assignment Problem,” Annals of Operations Research, Vol. 13, pp.
193-223.
Carraresi, P., and Sodini, C., 1986. “An Efficient Algorithm for the Bipar-
tite Matching Problem,” Eur. J. Operations Research, Vol. 23, pp. 86-93.
Castañon, D. A., 1990. “Efficient Algorithms for Finding the K Best Paths
Through a Trellis,” IEEE Trans. on Aerospace and Electronic Systems,
Vol. 26, pp. 405-410.
Castañon, D. A., 1993. “Reverse Auction Algorithms for Assignment Prob-
lems,” in Algorithms for Network Flows and Matching, Johnson, D. S., and
McGeoch, C. C. (eds.), American Math. Soc., Providence, RI, pp. 407-429.
Censor, Y., and Zenios, S. A., 1992. “The Proximal Minimization Algo-
rithm with D-Functions,” J. Opt. Theory and Appl., Vol. 73, pp. 451-464.
Censor, Y., and Zenios, S. A., 1997. Parallel Optimization: Theory, Algo-
rithms, and Applications, Oxford University Press, N. Y.
Cerny, V., 1985. “A Thermodynamical Approach to the Travelling Sales-
man Problem: An Efficient Simulation Algorithm,” J. Opt. Theory and
Applications, Vol. 45, pp. 41-51.
Cerulli, R., De Leone, R., and Piacente, G., 1994. “A Modified Auction
Algorithm for the Shortest Path Problem,” Optimization Methods and
Software, Vol. 4, pp. 209-224.
Cerulli, R., Festa, P., and Raiconi, G., 1997a. “Graph Collapsing in Short-
est Path Auction Algorithms,” Univ. of Salerno Tech. Report n. 6/97.
Cerulli, R., Festa, P., and Raiconi, G., 1997b. “An Efficient Auction Algo-
rithm for the Shortest Path Problem Using Virtual Source Concept,” Univ.
of Salerno Tech. Report n. 6/97.
Chajakis, E. D., and Zenios, S. A., 1991. “Synchronous and Asynchronous
Implementations of Relaxation Algorithms for Nonlinear Network Opti-
mization,” Parallel Computing, Vol. 17, pp. 873-894.
References 563
Chen, G., and Teboulle, M., 1993. “Convergence Analysis of a Proximal-
Like Minimization Algorithm Using Bregman Functions,” SIAM J. on Op-
timization, Vol. 3, pp. 538-543.
Chen, Z. L., and Powell, W. B., 1997. “A Note on Bertsekas’ Small-Label-
First Strategy,” Networks, Vol. 29, pp. 111-116.
Cheney, E. W., and Goldstein, A. A., 1959. “Newton’s Method for Convex
Programming and Tchebycheff Approximation,” Numer. Math., Vol. I, pp.
253-268.
Cheriyan, J., and Maheshwari, S. N., 1989. “Analysis of Preflow Push
Algorithms for Maximum Network Flow,” SIAM J. Computing, Vol. 18,
pp. 1057-1086.
Cherkasky, R. V., 1977. “Algorithm for Construction of Maximum Flow in
Networks with Complexity of O(V 2
√
E) Operations,” Mathematical Meth-
ods of Solution of Economical Problems, Vol. 7, pp. 112-125.
Christofides, N., 1975. Graph Theory: An Algorithmic Approach, Aca-
demic Press, N. Y.
Chvatal, V., 1983. Linear Programming, W. H. Freeman and Co., N. Y.
Connors, D. P., and Kumar, P. R., 1989. “Simulated Annealing Type
Markov Chains and their Order Balance Equations,” SIAM J. on Control
and Optimization, Vol. 27, pp. 1440-1461.
Cook, W., Cunningham, W., Pulleyblank, W., and Schrijver, A., 1998.
Combinatorial Optimization, Wiley, N. Y.
Cornuejols, G., Fonlupt, J., and Naddef, D., 1985. “The Traveling Salesman
Problem on a Graph and Some Related Polyhedra,” Math. Programming,
Vol. 33, pp. 1-27.
Cottle, R. W., and Pang, J. S., 1982. “On the Convergence of a Block
Successive Over-Relaxation Method for a Class of Linear Complementarity
Problems,” Math. Progr. Studies, Vol. 17, pp. 126-138.
Croes, G. A., 1958. “A Method for Solving Traveling Salesman Problems,”
Operations Research, Vol. 6, pp. 791-812.
Cunningham, W. H., 1976. “A Network Simplex Method,” Math. Program-
ming, Vol. 4, pp. 105-116.
Cunningham, W. H., 1979. “Theoretical Properties of the Network Simplex
Method,” Math. of Operations Research, Vol. 11, pp. 196-208.
Dafermos, S., 1980. “Traffic Equilibrium and Variational Inequalities,”
Transportation Science, Vol. 14, pp. 42-54.
Dafermos, S., 1982. “Relaxation Algorithms for the General Asymmetric
Traffic Equilibrium Problem,” Transportation Science, Vol. 16, pp. 231-240.
564 References
Dafermos, S., and Sparrow, F. T., 1969. “The Traffic Assignment Problem
for a General Network,” J. Res. Nat. Bureau of Standards, Vol. 73B, pp.
91-118.
Dantzig, G. B., 1951. “Application of the Simplex Method to a Transporta-
tion Problem,” in Activity Analysis of Production and Allocation, T. C.
Koopmans (ed.), Wiley, N. Y., pp. 359-373.
Dantzig, G. B., 1960. “On the Shortest Route Problem Through a Net-
work,” Management Science, Vol. 6, pp. 187-190.
Dantzig, G. B., 1963. Linear Programming and Extensions, Princeton Univ.
Press, Princeton, N. J.
Dantzig, G. B., 1967. “All Shortest Routes in a Graph,” in Theory of
Graphs, P. Rosenthier (ed.), Gordan and Breach, N. Y., pp. 92-92.
Dantzig, G. B., and Fulkerson, D. R., 1956. “On the Max-Flow Min-Cut
Theorem of Networks,” in Linear Inequalities and Related Systems, Kuhn,
H. W., and Tucker, A. W. (eds.), Annals of Mathematics Study 38, Prince-
ton Univ. Press, pp. 215-221.
Dantzig, G. B., and Wolfe, P., 1960. “Decomposition Principle for Linear
Programs,” Operations Research, Vol. 8, pp. 101-111.
Dantzig, G. B., Fulkerson, D. R., and Johnson, S. M., 1954. “Solution of
a Large-Scale Traveling-Salesman Problem,” Operations Research, Vol. 2,
pp. 393-410.
De Leone, R., Meyer, R. R., and Zakarian, A., 1995. “An -Relaxation
Algorithm for Convex Network Flow Problems,” Computer Sciences De-
partment Technical Report, University of Wisconsin, Madison, WI.
Dembo, R. S., 1987. “A Primal Truncated Newton Algorithm for Large-
Scale Unconstrained Optimization,” Math. Programming Studies, Vol. 31,
pp. 43-72.
Dembo, R. S., and Klincewicz, J. G., 1981. “A Scaled Reduced Gradi-
ent Algorithm for Network Flow Problems with Convex Separable Costs,”
Math. Programming Studies, Vol. 15, pp. 125-147.
Dembo, R. S., and Tulowitzki, U., 1988. “Computing Equilibria on Large
Multicommodity Networks: An Application of Truncated Quadratic Pro-
gramming Algorithms,” Networks, Vol. 18, pp. 273-284.
Denardo, E. V., and Fox, B. L., 1979. “Shortest-Route Methods: 1. Reach-
ing, Pruning and Buckets,” Operations Research, Vol. 27, pp. 161-186.
Dennis, J. B., 1959. Mathematical Programming and Electical Circuits,
Technology Press of M.I.T., Cambridge, MA.
Deo, N., and Kumar, N., 1997. “Computation of Constrained Spanning
Trees: A Unified Approach,” in Network Optimization, Pardalos, P. M.,
References 565
Hearn, D. W., and Hager, W. W. (eds.), Lecture Notes in Economics and
Mathematical Systems, Springer-Verlag, N. Y., pp. 194-220.
Deo, N., and Pang, C., 1984. “Shortest Path Algorithms: Taxonomy and
Annotation,” Networks, Vol. 14, pp. 275-323.
Derigs, U., 1985. “The Shortest Augmenting Path Method for Solving As-
signment Problems – Motivation and Computational Experience,” Annals
of Operations Research, Vol. 4, pp. 57-102.
Derigs, U., and Meier, W., 1989. “Implementing Goldberg’s Max-Flow Al-
gorithm – A Computational Investigation,” Zeitschrif fur Operations Re-
search, Vol. 33, pp. 383-403.
Desrosiers, J., Dumas, Y., Solomon, M. M., and Soumis, F., 1995. “Time
Constrained Routing and Scheduling,” Handbooks in OR and MS, Ball,
M. O., Magnanti, T. L., Monma, C. L., and Nemhauser, G. L. (eds.), Vol.
8, North-Holland, Amsterdam, pp. 35-139.
Dial, R. B., 1969. “Algorithm 360: Shortest Path Forest with Topological
Ordering,” Comm. ACM, Vol. 12, pp. 632-633.
Dial, R., Glover, F., Karney, D., and Klingman, D., 1979. “A Compu-
tational Analysis of Alternative Algorithms and Labeling Techniques for
Finding Shortest Path Trees,” Networks, Vol. 9, pp. 215-248.
Dijkstra, E., 1959. “A Note on Two Problems in Connexion with Graphs,”
Numer. Math., Vol. 1, pp. 269-271.
Dinic, E. A., 1970. “Algorithm for Solution of a Problem of Maximum Flow
in Networks with Power Estimation,” Soviet Math. Doklady, Vol. 11, pp.
1277-1280.
Dreyfus, S. E., 1969. “An Appraisal of Some Shortest-Path Algorithms,”
Operations Research, Vol. 17, pp. 395-412.
Duffin, R. J., 1947. “Nonlinear Networks. IIa,” Bull. Amer. Math. Soc.,
Vol. 53, pp. 963-971.
Eastman, W. L., 1958. Linear Programming with Pattern Constraints,
Ph.D. Thesis, Harvard University, Cambridge, MA.
Eckstein, J., 1994. “Nonlinear Proximal Point Algorithms Using Bregman
Functions, with Applications to Convex Programming,” Math. of Opera-
tions Research, Vol. 18, pp. 202-226.
Edmonds, J., 1965. “Paths, Trees, and Flowers,” Canadian J. of Math.,
Vol. 17, pp. 449-467.
Edmonds, J., and Johnson, E. L., 1973. “Matching, Euler Tours, and the
Chinese Postman,” Math. Programming, Vol. 5, pp. 88-124.
Edmonds, J., and Karp, R. M., 1972. “Theoretical Improvements in Al-
566 References
gorithmic Efficiency for Network Flow Problems,” J. ACM, Vol. 19, pp.
248-264.
Eiselt, H. A., Gendreau, M., and Laporte, G., 1995a. “Arc Routing Prob-
lems, Part 1: The Chinese Postman Problem,” Operations Research, Vol.
43, pp. 231-242.
Eiselt, H. A., Gendreau, M., and Laporte, G., 1995b. “Arc Routing Prob-
lems, Part 2: The Rural Postman Problem,” Operations Research, Vol. 43,
pp. 399-414.
Elias, P., Feinstein, A., and Shannon, C. E., 1956. “Note on Maximum Flow
Through a Network,” IRE Trans. Info. Theory, Vol. IT-2, pp. 117-119.
Egervary, J., 1931. “Matrixok Kombinatoricus Tulajonsagairol,” Mat. Es
Fiz. Lapok, Vol. 38, pp. 16-28.
El Baz, D., 1989. “A Computational Experience with Distributed Asyn-
chronous Iterative Methods for Convex Network Flow Problems,” Proc.
of the 28th IEEE Conference on Decision and Control, Tampa, Fl., pp.
590-591.
El Baz, D., 1996. “Asynchronous Gradient Algorithms for a Class of Con-
vex Separable Network Flow Problems,” Computational Optimization and
Applications, Vol. 5, pp. 187-205.
El Baz, D., Spiteri, P., Miellou, J. C., and Gazen, D., 1996. “Asynchronous
Iterative Algorithms with Flexible Communication for Nonlinear Network
Flow Problems,” J. of Parallel and Distributed Computing, Vol. 38, pp.
1-15.
Elam, J., Glover, F., and Klingman, D., 1979. “A Strongly Convergent
Primal Simplex Algorithm for Generalized Networks,” Math. of Operations
Research, Vol. 4, pp. 39-59.
Elmaghraby, S. E., 1978. Activity Networks: Project Planning and Control
by Network Models, Wiley, N. Y.
Elzinga, J., and Moore, T. G., 1975. “A Central Cutting Plane Algorithm
for the Convex Programming Problem,” Math. Programming, Vol. 8, pp.
134-145.
Engquist, M., 1982. “A Successive Shortest Path Algorithm for the Assign-
ment Problem,” INFOR, Vol. 20, pp. 370-384.
Ephremides, A., 1986. “The Routing Problem in Computer Networks,”
in Communication and Networks, Blake, I. F., and Poor, H. V. (eds.),
Springer-Verlag, N. Y., pp. 299-325.
Ephremides, A., and Verdu, S., 1989. “Control and Optimization Methods
in Communication Network Problems,” IEEE Trans. on Automatic Con-
trol, Vol. 34, pp. 930-942.
References 567
Esau, L. R., and Williams, K. C., 1966. “On Teleprocessing System Design.
A Method for Approximating the Optimal Network,” IBM System J., Vol.
5, pp. 142-147.
Escudero, L. F., 1985. “Performance Evaluation of Independent Superbasic
Sets on Nonlinear Replicated Networks,” Eur. J. Operations Research, Vol.
23, pp. 343-355.
Everett, H., 1963. “Generalized Lagrange Multiplier Method for Solving
Problems of Optimal Allocation of Resources,” Operations Research, Vol.
11, pp. 399-417.
Falcone, M., 1987. “A Numerical Approach to the Infinite Horizon Problem
of Deterministic Control Theory,” Appl. Math. Opt., Vol. 15, pp. 1-13.
Federgruen, A., and Simchi-Levi, D., 1995. “Analysis of Vehicle and Invento-
ry-Routing Problems,” Handbooks in OR and MS, Ball, M. O., Magnanti,
T. L., Monma, C. L., and Nemhauser, G. L. (eds.), Vol. 8, North-Holland,
Amsterdam, pp. 297-373.
Ferris, M. C., 1991. “Finite Termination of the Proximal Point Algorithm,”
Math. Programming, Vol. 50, pp. 359-366.
Fisher, M., 1995. “Vehicle Routing,” Handbooks in OR and MS, Ball, M.
O., Magnanti, T. L., Monma, C. L., and Nemhauser, G. L. (eds.), Vol. 8,
North-Holland, Amsterdam, pp. 1-33.
Florian, M., Guélat, J., and Spiess, H., 1987. “An Efficient Implementation
of the “PARTAN” Variant of the Linear Approximation Method for the
Network Equilibrium Problem,” Networks, Vol. 17, pp. 319-339.
Florian, M. S., and Hearn, D., 1995. “Network Equilibrium Models and
Algorithms,” Handbooks in OR and MS, Ball, M. O., Magnanti, T. L.,
Monma, C. L., and Nemhauser, G. L. (eds.), Vol. 8, North-Holland, Ams-
terdam, pp. 485-550.
Florian, M. S., and Nguyen, S., 1974. “A Method for Computing Network
Equilibrium with Elastic Demands,” Transportation Science, Vol. 8, pp.
321-332.
Florian, M. S., and Nguyen, S., 1976. “An Application and Validation of
Equilibrium Trip Assignment Methods,” Transportation Science, Vol. 10,
pp. 374-390.
Florian, M. S., Nguyen, S., and Pallottino, S., 1981. “A Dual Simplex
Algorithm for Finding All Shortest Paths,” Networks, Vol. 11, pp. 367-
378.
Floudas, C. A., 1995. Nonlinear and Mixed-Integer Optimization: Funda-
mentals and Applications, Oxford University Press, N. Y.
568 References
Floyd, R. W., 1962. “Algorithm 97: Shortest Path,” Comm. ACM, Vol. 5,
pp. 345.
Ford, L. R., Jr., 1956. “Network Flow Theory,” Report P-923, The Rand
Corporation, Santa Monica, CA.
Ford, L. R., Jr., and Fulkerson, D. R., 1956a. “Solving the Transportation
Problem,” Management Science, Vol. 3, pp. 24-32.
Ford, L. R., Jr., and Fulkerson, D. R., 1956b. “Maximal Flow Through a
Network,” Can. J. of Math., Vol. 8, pp. 339-404.
Ford, L. R., Jr., and Fulkerson, D. R., 1957. “A Primal-Dual Algorithm
for the Capacitated Hitchcock Problem,” Naval Res. Logist. Quart., Vol.
4, pp. 47-54.
Ford, L. R., Jr., and Fulkerson, D. R., 1962. Flows in Networks, Princeton
Univ. Press, Princeton, N. J.
Fox, B. L., 1993. “Integrating and Accelerating Tabu Search, Simulated
Annealing, and Genetic Algorithms,” Annals of Operations Research, Vol.
41, pp. 47-67.
Fox, B. L., 1995. “Faster Simulated Annealing,” SIAM J. Optimization,
Vol. 41, pp. 47-67.
Frank, H., and Frisch, I. T., 1970. Communication, Transmission, and
Transportation Networks, Addison-Wesley, Reading, MA.
Fratta, L., Gerla, M., and Kleinrock, L., 1973. “The Flow-Deviation Method:
An Approach to Store-and-Forward Computer Communication Network
Design,” Networks, Vol. 3, pp. 97-133.
Fredman, M. L., and Tarjan, R. E., 1984. “Fibonacci Heaps and their Uses
in Improved Network Optimization Algorithms,” Proc. 25th Annual Symp.
on Found. of Comp. Sci., pp. 338-346.
Fukushima, M., 1984a. “A Modified Frank-Wolfe Algorithm for Solving
the Traffic Assignment Problem,” Transportation Research, Vol. 18B, pp.
169–177.
Fukushima, M., 1984b. “On the Dual Approach to the Traffic Assignment
Problem,” Transportation Research, Vol. 18B, pp. 235-245.
Fukushima, M., 1992. “Equivalent Differentiable Optimization Problems
and Descent Methods for Asymmetric Variational Inequalities,” Math. Pro-
gramming, Vol. 53, pp. 99-110.
Fulkerson, D. R., 1961. “An Out-of-Kilter Method for Minimal Cost Flow
Problems,” SIAM J. Appl. Math., Vol. 9, pp. 18-27.
Fulkerson, D. R., and Dantzig, G. B., 1955. “Computation of Maximum
Flow in Networks,” Naval Res. Log. Quart., Vol. 2, pp. 277-283.
References 569
Gafni, E. M., 1979. “Convergence of a Routing Algorithm,” M.S. Thesis,
Dept. of Electrical Engineering, Univ. of Illinois, Urbana, Ill.
Gafni, E. M., and Bertsekas, D. P., 1984. “Two-Metric Projection Methods
for Constrained Optimization,” SIAM J. on Control and Optimization, Vol.
22, pp. 936-964.
Gale, D., 1957. “A Theorem of Flows in Networks,” Pacific J. Math., Vol.
7, pp. 1073-1082.
Gale, D., Kuhn, H. W., and Tucker, A. W., 1951. “Linear Programming and
the Theory of Games,” in Activity Analysis of Production and Allocation,
T. C. Koopmans (ed.), Wiley, N. Y.
Galil, Z., 1980. “O
(
V 5/3E2/3
)
Algorithm for the Maximum Flow Problem,”
Acta Informatica, Vol. 14, pp. 221-242.
Galil, Z., and Naamad, A., 1980. “O
(
V E log2 V
)
Algorithm for the Maxi-
mum Flow Problem,” J. of Comput. Sys. Sci., Vol. 21, pp. 203-217.
Gallager, R. G., 1977. “A Minimum Delay Routing Algorithm Using Dis-
tributed Computation,” IEEE Trans. on Communications, Vol. 23, pp. 73-
85.
Gallo, G. S., and Pallottino, S., 1982. “A New Algorithm to Find the Short-
est Paths Between All Pairs of Nodes,” Discrete Applied Mathematics, Vol.
4, pp. 23-35.
Gallo, G. S., and Pallottino, S., 1986. “Shortest Path Methods: A Unified
Approach,” Math. Programming Studies, Vol. 26, pp. 38-64.
Gallo, G. S., and Pallottino, S., 1988. “Shortest Path Algorithms,” Annals
of Operations Research, Vol. 7, pp. 3-79.
Garey, M. R., and Johnson, D. S., 1979. Computers and Intractability: A
Guide to the Theory of NP-Completeness, W. H. Freeman and Co., San
Francisco, Ca.
Gartner, N. H., 1980a. “Optimal Traffic Assignment with Elastic Demands:
A Review. Part I. Analysis Framework,” Transportation Science, Vol. 14,
pp. 174-191.
Gartner, N. H., 1980b. “Optimal Traffic Assignment with Elastic Demands:
A Review. Part II. Algorithmic Approaches,” Transportation Science, Vol.
14, pp. 192-208.
Gavish, B., Schweitzer, P., and Shlifer, E., 1977. “The Zero Pivot Phe-
nomenon in Transportation Problems and its Computational Implications,”
Math. Programming, Vol. 12, pp. 226-240.
Gelfand, S. B., and Mitter, S. K., 1989. “Simulated Annealing with Noisy
or Imprecise Measurements,” J. Opt. Theory and Applications, Vol. 69, pp.
49-62.
570 References
Geoffrion, A. M., 1970. “Elements of Large-Scale Mathematical Program-
ming, I, II,” Management Science, Vol. 16, pp. 652-675, 676-691.
Geoffrion, A. M., 1974. “Lagrangian Relaxation for Integer Programming,”
Math. Programming Studies, Vol. 2, pp. 82-114.
Gerards, A. M. H., 1995. “Matching,” Handbooks in OR and MS, Ball, M.
O., Magnanti, T. L., Monma, C. L., and Nemhauser, G. L. (eds.), Vol. 7,
North-Holland, Amsterdam, pp. 135-224.
Gibby, D., Glover, F., Klingman, D., and Mead, M., 1983. “A Compari-
son of Pivot Selection Rules for Primal Simplex Based Network Codes,”
Operations Research Letters, Vol. 2, pp. 199-202.
Gill, P. E., Murray, W., and Wright, M. H., 1981. Practical Optimization,
Academic Press, N. Y.
Gilmore, P. C., Lawler, E. L., and Shmoys, D. B., 1985. “Well-Solved
Special Cases,” in The Traveling Salesman Problem, Lawler, E., Lenstra,
J. K., Rinnoy Kan, A. H. G., and Shmoys, D. B. (eds.), Wiley, N. Y., pp.
87-143.
Glover, F., 1986. “Future Paths for Integer Programming and Links to
Artificial Intelligence,” Computers and Operations Research, Vol. 13, pp.
533-549.
Glover, F., 1989. “Tabu Search: Part I,” ORSA J. on Computing, Vol. 1,
pp. 190-206.
Glover, F., 1990. “Tabu Search: Part II,” ORSA J. on Computing, Vol. 2,
pp. 4-32.
Glover, F., Glover, R., and Klingman, D., 1986. “The Threshold Shortest
Path Algorithm,” Math. Programming Studies, Vol. 26, pp. 12-37.
Glover, F., Glover, R., and Klingman, D., 1986. “Threshold Assignment
Algorithm,” Math. Programming Studies, Vol. 26, pp. 12-37.
Glover, F., Karney, D., and Klingman, D., 1974. “Implementation and
Computational Comparisons of Primal, Dual, and Primal-Dual Computer
Codes for Minimum Cost Network Flow Problem,” Networks, Vol. 4, pp.
191-212.
Glover, F., Karney, D., Klingman, D., and Napier, A., 1974. “A Com-
putation Study on Start Procedures, Basis Change Criteria, and Solution
Algorithms for Transportation Problems,” Management Science, Vol. 20,
pp. 793-819.
Glover, F., Klingman, D., Mote, J., and Whitman, D., 1984. “A Pri-
mal Simplex Variant for the Maximum Flow Problem,” Naval Res. Logist.
Quart., Vol. 31, pp. 41-61.
Glover, F., Klingman, D., and Phillips, N., 1985. “A New Polynomially
References 571
Bounded Shortest Path Algorithm,” Operations Research, Vol. 33, pp. 65-
73.
Glover, F., Klingman, D., and Phillips, N., 1992. Network Models in Opti-
mization and Their Applications in Practice, Wiley, N. Y.
Glover, F., Klingman, D., Phillips, N., and Schneider, R. F., 1985. “New
Polynomial Shortest Path Algorithms and Their Computational Attributes,”
Management Science, Vol. 31, pp. 1106-1128.
Glover, F., Klingman, D., and Stutz, J., 1973. “Extension of the Augmented
Predecessor Index Method to Generalized Netork Problems,” Transporta-
tion Science, Vol. 7, pp. 377-384.
Glover, F., Klingman, D., and Stutz, J., 1974. “Augmented Threaded Index
Method for Network Optimization,” INFOR, Vol. 12, pp. 293-298.
Glover, F., and Laguna, M., 1997. Tabu Search, Kluwer, Boston.
Glover, F., Taillard, E., and de Verra, D., 1993. “A User’s Guide to Tabu
Search,” Annals of Operations Research, Vol. 41, pp. 3-28.
Goffin, J. L., 1977. “On Convergence Rates of Subgradient Optimization
Methods,” Math. Programming, Vol. 13, pp. 329-347.
Goffin, J. L., Haurie, A., and Vial, J. P., 1992. “Decomposition and Non-
differentiable Optimization with the Projective Algorithm,” Management
Science, Vol. 38, pp. 284-302.
Goffin, J. L., and Kiwiel, K. C, 1996. ‘Convergence of a Simple Subgradient
Level Method,” Unpublished Report, to appear in Math. Programming.
Goffin, J. L., Luo, Z.-Q., and Ye, Y., 1993. “On the Complexity of a Column
Generation Algorithm for Convex or Quasiconvex Feasibility Problems,” in
Large Scale Optimization: State of the Art, Hager, W. W., Hearn, D. W.,
and Pardalos, P. M. (eds.), Kluwer.
Goffin, J. L., Luo, Z.-Q., and Ye, Y., 1996. “Further Complexity Analysis of
a Primal-Dual Column Generation Algorithm for Convex or Quasiconvex
Feasibility Problems,” SIAM J. on Optimization, Vol. 6, pp. 638-652.
Goffin, J. L., and Vial, J. P., 1990. “Cutting Planes and Column Generation
Techniques with the Projective Algorithm,” J. Opt. Th. and Appl., Vol. 65,
pp. 409-429.
Goldberg, A. V., 1987. “Efficient Graph Algorithms for Sequential and Par-
allel Computers,” Tech. Report TR-374, Laboratory for Computer Science,
M.I.T., Cambridge, MA.
Goldberg, A. V., 1993. “An Efficient Implementation of a Scaling Minimum-
Cost Flow Algorithm,” Proc. 3rd Integer Progr. and Combinatorial Opti-
mization Conf., pp. 251-266.
572 References
Goldberg, A. V., and Tarjan, R. E., 1986. “A New Approach to the Maxi-
mum Flow Problem,” Proc. 18th ACM STOC, pp. 136-146.
Goldberg, A. V., and Tarjan, R. E., 1990. “Solving Minimum Cost Flow
Problems by Successive Approximation,” Math. of Operations Research,
Vol. 15, pp. 430-466.
Goldberg, D. E., 1989. Genetic Algorithms in Search, Optimization, and
Machine Learning, Addison Wesley, Reading, MA.
Goldfarb, D., 1985. “Efficient Dual Simplex Algorithms for the Assignment
Problem,” Math. Programming, Vol. 33, pp. 187-203.
Goldfarb, D., and Hao, J., 1990. “A Primal Simplex Algorithm that Solves
the Maximum Flow Problem in at Most nm Pivots and O(n2m) Time,”
Math. Programming, Vol. 47, pp. 353-365.
Goldfarb, D., Hao, J., and Kai, S., 1990a. “Anti-Stalling Pivot Rules for
the Network Simplex Algorithm,” Networks, Vol. 20, pp. 79-91.
Goldfarb, D., Hao, J., and Kai, S., 1990b. “Efficient Shortest Path Simplex
Algorithms,” Operations Research, Vol. 38, pp. 624-628.
Goldfarb, D., and Reid, J. K., 1977. “A Practicable Steepest Edge Simplex
Algorithm,” Math. Programming, Vol. 12, pp. 361-371.
Goldstein, A. A., 1967. Constructive Real Analysis, Harper and Row, N.
Y.
Gondran, M., and Minoux, M., 1984. Graphs and Algorithms, Wiley, N.
Y.
Gonzalez, R., and Rofman, E., 1985. “On Deterministic Control Problems:
An Approximation Procedure for the Optimal Cost, Parts I, II,” SIAM J.
on Control and Optimization, Vol. 23, pp. 242-285.
Graham, R. L., Lawler, E. L., Lenstra, J. K., and Rinnooy Kan, A. H. G.,
1979. “Optimization and Approximation in Deterministic Sequencing and
Scheduling: A Survey,” Annals of Discrete Math., Vol. 5, pp. 287-326.
Grötschel, M., Monma, C. L., and Stoer, M., 1995. “Design of Surviv-
able Networks,” Handbooks in OR and MS, Ball, M. O., Magnanti, T.
L., Monma, C. L., and Nemhauser, G. L. (eds.), Vol. 7, North-Holland,
Amsterdam, pp. 617-672.
Grötschel, M., and Padberg, M. W., 1985. “Polyhedral Theory,” in The
Traveling Salesman Problem, Lawler, E., Lenstra, J. K., Rinnoy Kan, A.
H. G., and Shmoys, D. B. (eds.), Wiley, N. Y., pp. 251-305.
Guerriero, F., Lacagnina, V., Musmanno, R., and Pecorella, A., 1996. “Ef-
ficient Node Selection Strategies in Label-Correcting Methods for the K
Shortest Paths Problem,” Technical Report PARCOLAB No. 6/96, De-
partment of Electronics, Informatics and Systems, University of Calabria.
References 573
Guler, O., 1992. “New Proximal Point Algorithms for Convex Minimiza-
tion,” SIAM J. on Optimization, Vol. 2, pp. 649-664.
Hajek, B., 1988. “Cooling Schedules for Optimal Annealing,” Math. of
Operations Research, Vol. 13, pp. 311-329.
Hall, M., Jr., 1956. “An Algorithm for Distinct Representatives,” Amer.
Math. Monthly, Vol. 51, pp. 716-717.
Hansen, P., 1986. “The Steepest Ascent Mildest Descent Heuristic for Com-
binatorial Optimization,” Presented at the Congress on Numerical Methods
in Combinatorial Optimization, Capri, Italy.
Hearn, D. W., and Lawphongpanich, S., 1990. “A Dual Ascent Algorithm
for Traffic Assignment Problems,” Transportation Research, Vol. 24B, pp.
423-430.
Hearn, D. W., Lawphongpanish, S., and Nguyen, S., 1984. “Convex Pro-
gramming Formulation of the Asymmetric Traffic Assignment Problem,”
Transportation Research, Vol. 18B, pp. 357-365.
Hearn, D. W., Lawphongpanish, S., and Ventura, J. A., 1985. “Finiteness
in Restricted Simplicial Decomposition,” Operations Research Letters, Vol.
4, pp. 125-130.
Hearn, D. W., Lawphongpanish, S., and Ventura, J. A., 1987. “Restricted
Simplicial Decomposition: Computation and Extensions,” Math. Program-
ming Studies, Vol. 31, pp. 99-118.
Held, M., and Karp, R. M., 1970. “The Traveling Salesman Problem and
Minimum Spanning Trees,” Operations Research, Vol. 18, pp. 1138-1162.
Held, M., and Karp, R. M., 1971. “The Traveling Salesman Problem and
Minimum Spanning Trees: Part II,” Math. Programming, Vol. 1, pp. 6-25.
Helgason, R. V., and Kennington, J. L., 1977. “An Efficient Procedure for
Implementing a Dual-Simplex Network Flow Algorithm,” AIIE Transac-
tions, Vol. 9, pp. 63-68.
Helgason, R. V., and Kennington, J. L., 1995. “Primal-Simplex Algorithms
for Minimum Cost Network Flows,” Handbooks in OR and MS, Ball, M.
O., Magnanti, T. L., Monma, C. L., and Nemhauser, G. L. (eds.), Vol. 7,
North-Holland, Amsterdam, pp. 85-133.
Helgason, R. V., Kennington, J. L., and Stewart, B. D., 1993. “The One-
to-One Shortest-Path Problem: An Empirical Analysis with the Two-Tree
Dijkstra Algorithm,” Computational Optimization and Applications, Vol.
1, pp. 47-75.
Hiriart-Urruty, J.-B., and Lemarechal, C., 1993. Convex Analysis and Min-
imization Algorithms, Vols. I and II, Springer-Verlag, Berlin and N. Y.
Hochbaum, D. S., and Shantikumar, J. G., 1990. “Convex Separable Op-
574 References
timization is not Much Harder than Linear Optimization,” J. ACM, Vol.
37, pp. 843-862.
Hoffman, A. J., 1960. “Some Recent Applications of the Theory of Lin-
ear Inequalities to Extremal Combinatorial Analysis,” Proc. Symp. Appl.
Math., Vol. 10, pp. 113-128.
Hoffman, A. J., and Kuhn, H. W., 1956. “Systems of Distinct Representa-
tives and Linear Programming,” Amer. Math. Monthly, Vol. 63, pp. 455-
460.
Hoffman, K., and Kunze, R., 1971. Linear Algebra, Prentice-Hall, Engle-
wood Cliffs, N. J.
Holloway, C. A., 1974. “An Extension of the Frank and Wolfe Method of
Feasible Directions,” Math. Programming, Vol. 6, pp. 14-27.
Hopcroft, J. E., and Karp, R. M., 1973. “A n5/2 Algorithm for Maximum
Matchings in Bipartite Graphs,” SIAM J. on Computing, Vol. 2, pp. 225-
231.
Horst, R., Pardalos, P. M., and Thoai, N. V., 1995. Introduction to Global
Optimization, Kluwer Academic Publishers, N. Y.
Hu, T. C., 1969. Integer Programming and Network Flows, Addison-Wesley,
Reading, MA.
Hung, M., 1983. “A Polynomial Simplex Method for the Assignment Prob-
lem,” Operations Research, Vol. 31, pp. 595-600.
Ibaraki, T., and Katoh, N., 1988. Resource Allocation Problems: Algorith-
mic Approaches, M.I.T. Press, Cambridge, MA.
Iri, M., 1969. Network Flows, Transportation, and Scheduling, Academic
Press, N. Y.
Iusem, A. N., Svaiter, B., and Teboulle, M., 1994. “Entropy-Like Proximal
Methods in Convex Programming,” Math. Operations Research, Vol. 19,
pp. 790-814.
Jensen, P. A., and Barnes, J. W., 1980. Network Flow Programming, Wiley,
N. Y.
Jewell, W. S., 1962. “Optimal Flow Through Networks with Gains,” Op-
erations Research, Vol. 10, pp. 476-499.
Johnson, D. B., 1977. “Efficient Algorithms for Shortest Paths in Sparse
Networks,” J. ACM, Vol. 24, pp. 1-13.
Johnson, D. S., and Papadimitriou, C. H., 1985. “Computational Com-
plexity,” in The Traveling Salesman Problem, Lawler, E., Lenstra, J. K.,
Rinnoy Kan, A. H. G., and Shmoys, D. B. (eds.), Wiley, N. Y., pp. 37-85.
Johnson, D. S., and McGeoch, L., 1997. “The Traveling Salesman Problem:
References 575
A Case Study,” in Local Search in Combinatorial Optimization, Aarts, E.,
and Lenstra, J. K. (eds.), Wiley, N. Y.
Johnson, E. L., 1966. “Networks and Basic Solutions,” Operations Re-
search, Vol. 14, pp. 619-624.
Johnson, E. L., 1972. “On Shortest Paths and Sorting,” Proc. 25th ACM
Annual Conference, pp. 510-517.
Jonker, R., and Volgenant, A., 1986. “Improving the Hungarian Assign-
ment Algorithm,” Operations Research Letters, Vol. 5, pp. 171-175.
Jonker, R., and Volgenant, A., 1987. “A Shortest Augmenting Path Al-
gorithm for Dense and Sparse Linear Assignment Problems,” Computing,
Vol. 38, pp. 325-340.
Junger, M., Reinelt, G., and Rinaldi, G., 1995. “The Traveling Sales-
man Problem,” Handbooks in OR and MS, Ball, M. O., Magnanti, T.
L., Monma, C. L., and Nemhauser, G. L. (eds.), Vol. 7, North-Holland,
Amsterdam, pp. 225-330.
Karzanov, A. V., 1974. “Determining the Maximal Flow in a Network with
the Method of Preflows,” Soviet Math Dokl., Vol. 15, pp. 1277-1280.
Karzanov, A. V., and McCormick, S. T., 1997. “Polynomial Methods for
Separable Convex Optimization in Unimodular Linear Spaces with Ap-
plications to Circulations and Co-circulations in Network,” SIAM J. on
Computing, Vol. 26, pp. 1245-1275.
Kelley, J. E., 1960. “The Cutting-Plane Method for Solving Convex Pro-
grams,” J. Soc. Indust. Appl. Math., Vol. 8, pp. 703-712.
Kennington, J., and Helgason, R., 1980. Algorithms for Network Program-
ming, Wiley, N. Y.
Kennington, J., and Shalaby, M., 1977. “An Effective Subgradient Pro-
cedure for Minimal Cost Multicommodity Flow Problems,” Management
Science, Vol. 23, pp. 994-1004.
Kernighan, B. W., and Lin, S., 1970. “An Efficient Heuristic Procedure for
Partitioning Graphs,” Bell System Tech. Journal, Vol. 49, pp. 291-307.
Kershenbaum, A., 1981. “A Note on Finding Shortest Path Trees,” Net-
works, Vol. 11, pp. 399-400.
Kershenbaum, A., 1993. Network Design Algorithms, McGraw-Hill, N. Y.
Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P., 1983. “Optimization by
Simulated Annealing,” Science, Vol. 220, pp. 621-680.
Kiwiel, K. C., 1997a. “Proximal Minimization Methods with Generalized
Bregman Functions,” SIAM J. on Control and Optimization, Vol. 35, pp.
1142-1168.
576 References
Kiwiel, K. C., 1997b. “Efficiency of the Analytic Center Cutting Plane
Method for Convex Minimization,” SIAM J. on Optimization, Vol. 7, pp.
336-346.
Klee, V., and Minty, G. J., 1972. “How Good is the Simplex Algorithm?,”
in Inequalities III, O. Shisha (ed.), Academic Press, N. Y., pp. 159-175.
Klein, M., 1967. “A Primal Method for Minimal Cost Flow with Appli-
cations to the Assignment and Transportation Problems,” Management
Science, Vol. 14, pp. 205-220.
Klessig, R. W., 1974. “An Algorithm for Nonlinear Multicommodity Flow
Problems,” Networks, Vol. 4, pp. 343-355.
Klincewitz, J. C., 1989. “Implementing an Exact Newton Method for Sep-
arable Convex Transportation Problems,” Networks, Vol. 19, pp. 95-105.
König, D., 1931. “Graphok es Matrixok,” Mat. Es Fiz. Lapok, Vol. 38, pp.
116-119.
Korst, J., Aarts, E. H., and Korst, A., 1989. Simulated Annealing and
Boltzmann Machines: A Stochastic Approach to Combinatorial Optimiza-
tion and Neural Computing, Wiley, N. Y.
Kortanek, K. O., and No, H., 1993. “A Central Cutting Plane Algorithm for
Convex Semi-Infinite Programming Problems,” SIAM J. on Optimization,
Vol. 3, pp. 901-918.
Kuhn, H. W., 1955. “The Hungarian Method for the Assignment Problem,”
Naval Research Logistics Quarterly, Vol. 2, pp. 83-97.
Kumar, V., Grama, A., Gupta, A., and Karypis, G., 1994. Introduction to
Parallel Computing, Benjamin/Cummings, Redwood City, CA.
Kushner, H. J., 1990. “Numerical Methods for Continuous Control Prob-
lems in Continuous Time,” SIAM J. on Control and Optimization, Vol. 28,
pp. 999-1048.
Kushner, H. J., and Dupuis, P. G., 1992. Numerical Methods for Stochastic
Control Problems in Continuous Time, Springer-Verlag, N. Y.
Kwan Mei-Ko, 1962. “Graphic Programming Using Odd or Even Points,”
Chinese Math., Vol. 1, pp. 273-277.
Lamar, B. W., 1993. “An Improved Branch and Bound Algorithm for Min-
imum Concave Cost Network Flow Problems,” in Network Optimization
Problems, Du, D.-Z., and Pardalos, P. M. (eds.), World Scientific Publ.,
Singapore, pp. 261-287.
Land, A. H., and Doig, A. G., 1960. “An Automatic Method for Solving
Discrete Programming Problems,” Econometrica, Vol. 28, pp. 497-520.
Larsson, T., and Patricksson, M., 1992. “Simplicial Decomposition with
References 577
Disaggregated Representation for the Traffic Assignment Problem,” Trans-
portation Science, Vol. 26, pp. 4-17.
Lasdon, L. S., 1970. Optimization Theory for Large Systems, Macmillian,
N. Y.
Lawphongpanich, S., and Hearn, D., 1984. “Simplicial Decomposition of
the Asymmetric Traffic Assignment Problems,” Transportation Research,
Vol. 18B, pp. 123-133.
Lawphongpanich, S., and Hearn, D. W., 1986. “Restricted Simplicial De-
composition with Application to the Traffic Assignment Problem,” Ricerca
Operativa, Vol. 38, pp. 97-120.
Lawler, E., 1976. Combinatorial Optimization: Networks and Matroids,
Holt, Reinhart, and Winston, N. Y.
Lawler, E., Lenstra, J. K., Rinnoy Kan, A. H. G., and Shmoys, D. B., 1985.
The Traveling Salesman Problem, Wiley, N. Y.
LeBlanc, L. J., Helgason, R. V., and Boyce, D. E., 1985. “Improved Ef-
ficiency of the Frank-Wolfe Algorithm for Convex Network Programs,”
Transportation Science, Vol. 19, pp. 445–462.
LeBlanc, L. J., Morlok, E. K., and Pierskalla, W. P., 1974. “An Accurate
and Efficient Approach to Equilibrium Traffic Assignment on Congested
Networks,” Transportation Research Record, TRB-National Academy of
Sciences, Vol. 491, pp. 12-23.
LeBlanc, L. J., Morlok, E. K., and Pierskalla, W. P., 1975. “An Efficient
Approach to Solving the Road Network Equilibrium Traffic Assignment
Problem,” Transportation Research, Vol. 9, pp. 309-318.
Leventhal, T., Nemhauser, G., and Trotter, Jr., L., 1973. “A Column Gen-
eration Algorithm for Optimal Traffic Assignment,” Transportation Sci-
ence, Vol. 7, pp. 168-176.
Lemarechal, C., 1974. “An Algorithm for Minimizing Convex Functions,”
in Information Processing ’74, Rosenfeld, J. L. (ed.), North Holland Publ.
Co., Amsterdam, pp. 552-556.
Little, J. D. C., Murty, K. G., Sweeney, D. W., and Karel, C., 1963. “An
Algorithm for the Traveling Salesman Problem,” Operations Research, Vol.
11, pp. 972-989.
Lovasz, L., and Plummer, M. D., 1985. Matching Theory, North-Holland,
Amsterdam.
Luenberger, D. G., 1969. Optimization by Vector Space Methods, Wiley,
N. Y.
Luenberger, D. G., 1984. Linear and Nonlinear Programming, Addison-
Wesley, Reading, MA.
578 References
Luo, Z.-Q., 1997. “Analysis of a Cutting Plane Method that Uses Weighted
Analytic Center and Multiple Cuts,” SIAM J. of Optimization, Vol. 7, pp.
697-716.
Luo, Z.-Q., and Tseng, P., 1994. “On the Rate of Convergence of a Dis-
tributed Asynchronous Routing Algorithm,” IEEE Trans. on Automatic
Control, Vol. 39, pp. 1123-1129.
Malhotra, V. M., Kumar, M. P., and Maheshwari, S. N., 1978. “An O(|V |3)
Algorithm for Finding Maximum Flows in Networks,” Inform. Process.
Lett., Vol. 7, pp. 277-278.
Marcotte, P., 1985. “A New Algorithm for Solving Variational Inequalities
with Application to the Traffic Assignment Problem,” Math. Programming
Studies, Vol. 33, pp. 339-351.
Marcotte, P., and Dussault, J.-P., 1987. “A Note on a Globally Convergent
Newton Method for Solving Monotone Variational Inequalities,” Opera-
tions Research Letters, Vol. 6, pp. 35-42.
Marcotte, P., and Guélat, J., 1988. “Adaptation of a Modified Newton
Method for Solving the Asymmetric Traffic Equilibrium Problem,” Trans-
portation Science, Vol. 22, pp. 112-124.
Martello, S., and Toth, P., 1990. Knapsack Problems, Wiley, N. Y.
Martinet, B., 1970. “Regularisation d’Inequations Variationnelles par Ap-
proximations Successives,” Rev. Francaise Inf. Rech. Oper., Vol. 4, pp.
154-159.
McGinnis, L. F., 1983. “Implementation and Testing of a Primal-Dual Al-
gorithm for the Assignment Problem,” Operations Research, Vol. 31, pp.
277-291.
Mendelssohn, N. S., and Dulmage, A. L., 1958. “Some Generalizations of
Distinct Representatives,” Canad. J. Math., Vol. 10, pp. 230-241.
Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., and Teller, E.,
1953. “Equation of State Calculations by Fast Computing Machines,” J. of
Chemical Physisc, Vol. 21, pp. 1087-1092.
Meyer, R. R., 1979. “Two-Segment Separable Programming,” Management
Science, Vol. 25, pp. 385-395.
Miller, D., Pekny, J., and Thompson, G. L., 1990. “Solution of Large Dense
Transportation Problems Using a Parallel Primal Algorithm,” Operations
Research Letters, Vol. 9, pp. 319-324.
Minty, G. J., 1957. “A Comment on the Shortest Route Problem,” Opera-
tions Research, Vol. 5, p. 724.
Minty, G. J., 1960. “Monotone Networks,” Proc. Roy. Soc. London, A, Vol.
257, pp. 194-212.
References 579
Minieka, E., 1978. Optimization Algorithms for Networks and Graphs,
Marcel Dekker, N. Y.
Minoux, M., 1986a. Mathematical Programming: Theory and Algorithms,
Wiley, N. Y.
Minoux, M., 1986b. “Solving Integer Minimum Cost Flows with Separable
Convex Cost Objective Polynomially,” Math. Programming Studies, Vol.
26, pp. 237-239.
Minoux, M., 1989. “Network Synthesis and Optimum Network Design
Problems: Models, Solution Methods,and Applications,” Networks, Vol.
19, pp. 313-360.
Monma, C. L., and Sheng, D. D., 1986. “Backbone Network Design and
Performance Analysis: A Methodology for Packet Switching Networks,”
IEEE J. Select. Areas Comm., Vol. SAC-4, pp. 946-965.
Mulvey, J., 1978a. “Pivot Strategies for Primal-Simplex Network Codes,”
J. ACM, Vol. 25, pp. 266-270.
Mulvey, J., 1978b. “Testing a Large-Scale Network Optimization Program,”
Math. Programming, Vol. 15, pp. 291-314.
Murty, K. G., 1992. Network Programming, Prentice-Hall, Englewood Cliffs,
N. J.
Nagurney, A., 1988. “An Equilibration Scheme for the Traffic Assignment
Problem with Elastic Demands,” Transportation Research, Vol. 22B, pp.
73-79.
Nagurney, A., 1993. Network Economics: A Variational Inequality Ap-
proach, Kluwer, Dordrecht, The Netherlands.
Nemhauser, G. L., and Wolsey, L. A., 1988. Integer and Combinatorial
Optimization, Wiley, N. Y.
Nesterov, Y., 1995. “Complexity Estimates of Some Cutting Plane Methods
Based on Analytic Barrier,” Math. Programming, Vol. 69, pp. 149-176.
Nesterov, Y., and Nemirovskii, A., 1994. Interior Point Polynomial Algo-
rithms in Convex Programming, SIAM, Phila., PA.
Nguyen, S., 1974. “An Algorithm for the Traffic Assignment Problem,”
Transportation Science, Vol. 8, pp. 203-216.
Nicholson, T., 1966. “Finding the Shortest Route Between Two Points in
a Network,” The Computer Journal, Vol. 9, pp. 275-280.
Nilsson, N. J., 1971. Problem-Solving Methods in Artificial Intelligence,
McGraw-Hill, N. Y.
Nilsson, N. J., 1980. Principles of Artificial Intelligence, Tioga, Palo Alto,
CA.
580 References
O’hEigeartaigh, M., Lenstra, S. K., and Rinnoy Kan, A. H. G. (eds.), 1985.
Combinatorial Optimization: Annotated Bibliographies, Wiley, N. Y.
Ortega, J. M., and Rheinboldt, W. C., 1970. Iterative Solution of Nonlinear
Equations in Several Variables, Academic Press, N. Y.
Osman, I. H., and Laporte, G., 1996. “Metaheuristics: A Bibliography,”
Annals of Operations Research, Vol. 63, pp. 513-628.
Padberg, M. W., and Grötschel, M., 1985. “Polyhedral Computations,” in
The Traveling Salesman Problem, Lawler, E., Lenstra, J. K., Rinnoy Kan,
A. H. G., and Shmoys, D. B. (eds.), Wiley, N. Y., pp. 307-360.
Pallottino, S., 1984. “Shortest Path Methods: Complexity, Interrelations
and New Propositions,” Networks, Vol. 14, pp. 257-267.
Pallottino, S., and Scutellà, M. G., 1991. “Strongly Polynomial Algorithms
for Shortest Paths,” Ricerca Operativa, Vol. 60, pp. 33-53.
Pallottino, S., and Scutellà, M. G., 1997a. “Shortest Path Algorithms in
Transportation Models: Classical and Innovative Aspects,” Proc. of the
International Colloquium on Equilibrium in Transportation Models, Mon-
treal, Canada.
Pallottino, S., and Scutellà, M. G., 1997b. “Dual Algorithms for the Short-
est Path Tree Problem,” Networks, Vol. 29, pp. 125-133.
Pang, J.-S., 1984. “Solution of the General Multicommodity Spatial Equi-
librium Problem by Variational and Complementarity Methods,” J. of Re-
gional Science, Vol. 24, pp. 403-414.
Pang, J.-S., and Yu, C.-S., 1984. “Linearized Simplicial Decomposition
Methods for Computing Traffic Equilibria on Networks,” Networks, Vol.
14, pp. 427-438.
Papadimitriou, C. H., and Steiglitz, K., 1982. Combinatorial Optimization:
Algorithms and Complexity, Prentice-Hall, Englewood Cliffs, N. J.
Pape, U., 1974. “Implementation and Efficiency of Moore - Algorithms for
the Shortest Path Problem,” Math. Programming, Vol. 7, pp. 212-222.
Pardalos, P. M., and Rosen, J. B., 1987. Constrained Global Optimization:
Algorithms and Applications, Springer-Verlag, N. Y.
Patricksson, M., 1991. “Algorithms for Urban Traffic Network Equilibria,”
Linköping Studies in Science and Technology, Department of Mathematics,
Thesis No. 263, Linköping University, Linköping, Sweden.
Pattipati, K. R., and Alexandridis, M. G., 1990. “Application of Heuris-
tic Search and Information Theory to Sequential Fault Diagnosis,” IEEE
Trans. on Systems, Man, and Cybernetics, Vol. 20, pp. 872-887.
Pattipati, K. R., Deb, S., Bar-Shalom, Y., and Washburn, R. B., 1992. “A
References 581
New Relaxation Algorithm and Passive Sensor Data Association,” IEEE
Trans. Automatic Control, Vol. 37, pp. 198-213.
Pearl, J., 1984. Heuristics, Addison-Wesley, Reading, MA.
Peters, J., 1990. “The Network Simplex Method on a Multiprocessor,”
Networks, Vol. 20, pp. 845-859.
Phillips, C., and Zenios, S. A., 1989. “Experiences with Large Scale Net-
work Optimization on the Connection Machine,” in The Impact of Recent
Computing Advances on Operations Research, Vol. 9, Elsevier, Amster-
dam, The Netherlands, pp. 169-180.
Pinar, M. C., and Zenios, S. A., 1992. “Parallel Decomposition of Multi-
commodity Network Flows Using a Linear-Quadratic Penalty Algorithm,”
ORSA J. on Computing, Vol. 4, pp. 235-249.
Pinar, M. C., and Zenios, S. A., 1993. “Solving Nonlinear Programs with
Embedded Network Structures,” in Network Optimization Problems, Du,
D.-Z., and Pardalos, P. M. (eds.), World Scientific Publ., Singapore, pp.
177-202.
Pinar, M. C., and Zenios, S. A., 1994. “On Smoothing Exact Penalty Func-
tions for Convex Constrained Optimization,” SIAM J. on Optimization,
Vol. 4, pp. 486-511.
Pinedo, M., 1995. Scheduling: Theory, Algorithms, and Systems, Prentice-
Hall, Englewood Cliffs, N. J.
Poljak, B. T., 1987. Introduction to Optimization, Optimization Software
Inc., N. Y.
Polymenakos, L. C., 1995. “-Relaxation and Auction Algorithms for the
Convex Cost Network Flow Problem,” Ph.D. Thesis, Electrical Engineering
and Computer Science Dept, M.I.T., Cambridge, MA.
Polymenakos, L. C., and Bertsekas, D. P., 1994. “Parallel Shortest Path
Auction Algorithms,” Parallel Computing, Vol. 20, pp. 1221-1247.
Polymenakos, L. C., Bertsekas, D. P., and Tsitsiklis, J. N., 1998. “Efficient
Algorithms for Continuous-Space Shortest Path Problems,” IEEE Trans.
on Automatic Control, Vol. AC-43, pp. 278-283.
Poore, A. B., 1994. “Multidimensional Assignment Formulation of Data
Association Problems Arising from Multitarget Tracking and Multisensor
Data Fusion,” Computational Optimization and Applications, Vol. 3, pp.
27-57.
Poore, A. B., and Robertson, A. J. A., 1997. New Lagrangian Relaxation
Based Algorithm for a Class of Multidimensional Assignment Problems,”
Computational Optimization and Applications, Vol. 8, pp. 129-150.
Powell, W. B., Jaillet, P., and Odoni, A., 1995. “Stochastic and Dynamic
582 References
Networks and Routing,” Handbooks in OR and MS, Ball, M. O., Magnanti,
T. L., Monma, C. L., and Nemhauser, G. L. (eds.), Vol. 8, North-Holland,
Amsterdam, pp. 141-295.
Powell, W. B., Berkkam, E., and Lustig, I. J., 1993. “On Algorithms for
Nonlinear Dynamic Networks,” in Network Optimization Problems, Du,
D.-Z., and Pardalos, P. M. (eds.), World Scientific Publ., Singapore, pp.
177-202.
Pulleyblank, W., 1983. “Polyhedral Combinatorics,” in Mathematical Pro-
gramming: The State of the Art - Bonn 1982, by Bachem, A., Grötschel,
M., and Korte, B., (eds.), Springer, Berlin, pp. 312-345.
Pulleyblank, W., Cook, W., Cunningham, W., and Schrijver, A., 1993. An
Introduction to Combinatorial Optimization, Wiley, N. Y.
Resende, M. G. C., and Veiga, G., 1993. “An Implementation of the Dual
Affine Scaling Algorithm for Minimum-Cost Flow on Bipartite Uncapaci-
tated Networks,” SIAM J. on Optimization, Vol. 3, pp. 516-537.
Resende, M. G. C., and Pardalos, P. M., 1996. “Interior Point Algorithms
for Network Flow Problems,” Advances in Linear and Integer Program-
ming, Oxford Lecture Ser. Math. Appl., Vol. 4, Oxford Univ. Press, New
York, pp. 145-185.
Rockafellar, R. T., 1967. “Convex Programming and Systems of Elemen-
tary Monotonic Relations,” J. of Math. Analysis and Applications, Vol. 19,
pp. 543-564.
Rockafellar, R. T., 1969. “The Elementary Vectors of a Subspace of RN ,”
in Combinatorial Mathematics and its Applications, by Bose, R. C., and
Dowling, T. A. (eds.), University of North Carolina Press, pp. 104-127.
Rockafellar, R. T., 1970. Convex Analysis, Princeton Univ. Press, Prince-
ton, N. J.
Rockafellar, R. T., 1976. “Monotone Operators and the Proximal Point
Algorithm,” SIAM J. on Control and Optimization, Vol. 14, pp. 877-898.
Rockafellar, R. T., 1981. “Monotropic Programming: Descent Algorithms
and Duality,” in Nonlinear Programming 4, by Mangasarian, O. L., Meyer,
R. R., and Robinson, S. M. (eds.), Academic Press, N. Y., pp. 327-366.
Rockafellar, R. T., 1984. Network Flows and Monotropic Programming,
Wiley, N. Y.
Rudin, W., 1976. Real Analysis, McGraw Hill, N. Y.
Sahni, S., and Gonzalez, T., 1976. “P -Complete Approximation Problems,”
J. ACM, Vol. 23, pp. 555-565.
Schwartz, B. L., 1994. “A Computational Analysis of the Auction Algo-
rithm,” Eur. J. of Operations Research, Vol. 74, pp. 161-169.
References 583
Sheffi, Y., 1985. Urban Transportation Networks. Equilibrium Analy-
sis with Mathematical Programming Methods, Prentice-Hall, Englewood
Cliffs, N. J.
Shier, D. R., 1979. “On Algorithms for Finding the K Shortest Paths in a
Network,” Networks, Vol. 9, pp. 195-214.
Shier, D. R., and Witzgall, C., 1981. “Properties of Labeling Methods for
Determining Shortest Path Trees,” J. Res. Natl. Bureau of Standards, Vol.
86, pp. 317-330.
Shiloach, Y., and Vishkin, U., 1982. “An O(n2 log n) Parallel Max-Flow
Algorithm,” J. Algorithms, Vol. 3, pp. 128-146.
Schrijver, A., 1986. Theory of Linear and Integer Programming, Wiley, N.
Y.
Shapiro, J. E., 1979. Mathematical Programming Structures and Algo-
rithms, Wiley, N. Y.
Shor, N. Z., 1985. Minimization Methods for Nondifferentiable Functions,
Springer-Verlag, Berlin.
Srinivasan, V., and Thompson, G. L., 1973. “Benefit-Cost Analysis of Cod-
ing Techniques for Primal Transportation Algorithm,” J. ACM, Vol. 20, pp.
194-213.
Strang, G., 1976. Linear Algebra and Its Applications, Academic Press, N.
Y.
Suchet, C., 1949. Electrical Engineering, Vol. 68, pp. 843-844.
Tabourier, Y., 1973. “All Shortest Distances in a Graph: An Improvement
to Dantzig’s Inductive Algorithm,” Disc. Math., Vol. 4, pp. 83-87.
Tardos, E., 1985. “A Strongly Polynomial Minimum Cost Circulation Al-
gorithm,” Combinatorica, Vol. 5, pp. 247-255.
Teboulle, M., 1992. “Entropic Proximal Mappings with Applications to
Nonlinear Programming,” Math. of Operations Research, Vol. 17, pp. 1-
21.
Toint, P. L., and Tuyttens, D., 1990. “On Large Scale Nonlinear Network
Optimization,” Math. Programming, Vol. 48, pp. 125-159.
Tseng, P., 1986. “Relaxation Methods for Monotropic Programming Prob-
lems,” Ph.D. Thesis, Dept. of Electrical Engineering and Computer Science,
M.I.T., Cambridge, MA.
Tseng, P., 1991. “Relaxation Method for Large Scale Linear Programming
Using Decomposition,” Math. of Operations Research, Vol. 17, pp. 859-880.
Tseng, P., 1998. “An -Out-of-Kilter Method for Monotropic Program-
ming,” Department of Mathematics Report, Univ. of Washington, Seattle,
584 References
Wash.
Tseng, P., and Bertsekas, D. P., 1987. “Relaxation Methods for Linear
Programs,” Math. of Operations Research, Vol. 12, pp. 569-596.
Tseng, P., and Bertsekas, D. P., 1990. “Relaxation Methods for Monotropic
Programs,” Math. Programming, Vol. 46, 1990, pp. 127-151.
Tseng, P., and Bertsekas, D. P., 1993. “On the Convergence of the Expo-
nential Multiplier Method for Convex Programming,” Math. Programming,
Vol. 60, pp. 1-19.
Tseng, P., and Bertsekas, D. P., 1996. “An Epsilon-Relaxation Method
for Separable Convex Cost Generalized Network Flow Problems,” Lab. for
Information and Decision Systems Report P-2374, M.I.T., Cambridge, MA.
Tseng, P., Bertsekas, D. P., and Tsitsiklis, J. N., 1990. “Partially Asyn-
chronous Parallel Algorithms for Network Flow and Other Problems,”
SIAM J. on Control and Optimization, Vol. 28, pp. 678-710.
Tsitsiklis, J. N., 1989. “Markov Chains with Rare Transitions and Simu-
lated Annealing,” Math. of Operations Research, Vol. 14, pp. 70-90.
Tsitsiklis, J. N., 1992. “Special Cases of Traveling Salesman and Repairman
Problems with Time Windows,” Networks, Vol. 22, pp. 263-282.
Tsitsiklis, J. N., 1995. “Efficient Algorithms for Globally Optimal Trajec-
tories,” IEEE Trans. on Automatic Control, Vol. 40, pp. 1528-1538.
Tsitsiklis, J. N., and Bertsekas, D. P., 1986. “Distributed Asynchronous
Optimal Routing in Data Networks,” IEEE Trans. on Automatic Control,
Vol. 31, pp. 325-331.
Ventura, J. A., and Hearn, D. W., 1993. “Restricted Simplicial Decomposi-
tion for Convex Constrained Problems,” Math. Programming, Vol. 59, pp.
71-85.
Voß, S., 1992. “Steiner’s Problem in Graphs: Heuristic Methods,”, Discrete
Applied Math., Vol. 40, pp. 45-72.
Von Randow, R., 1982. Integer Programming and Related Areas: A Clas-
sified Bibliography 1978-1981, Lecture Notes in Economics and Mathemat-
ical Systems, Vol. 197, Springer-Verlag, N. Y.
Von Randow, R., 1985. Integer Programming and Related Areas: A Clas-
sified Bibliography 1982-1984, Lecture Notes in Economics and Mathemat-
ical Systems, Vol. 243, Springer-Verlag, N. Y.
Warshall, S., 1962. “A Theorem on Boolean Matrices,” J. ACM, Vol. 9,
pp. 11-12.
Wein, J., and Zenios, S. A., 1991. “On the Massively Parallel Solution of
the Assignment Problem,” J. of Parallel and Distributed Computing, Vol.
References 585
13, pp. 228-236.
Whitting, P. D., and Hillier, J. A., 1960. “A Method for Finding the Short-
est Route Through a Road Network,” Operations Research Quart., Vol.
11, pp. 37-40.
Winter, P., 1987. “Steiner Problem in Networks: A Survey,” Networks, Vol.
17, pp. 129-167.
Wright, S. J., 1997. Primal-Dual Interior Point Methods, SIAM, Phila.,
PA.
Ye, Y., 1992. “A Potential Reduction Algorithm Allowing Column Gener-
ation,” SIAM J. on Optimization, Vol. 2, pp. 7-20.
Ye, Y., 1997. Interior Point Algorithms: Theory and Analysis, Wiley, N.
Y.
Zadeh, N., 1973a. “A Bad Network Problem for the Simplex Method and
Other Minimum Cost Flow Algorithms,” Math. Programming, Vol. 5, pp.
255-266.
Zadeh, N., 1973b. “More Pathological Examples for Network Flow Prob-
lems,” Math. Programming, Vol. 5, pp. 217-224.
Zadeh, N., 1979. “Near Equivalence of Network Flow Algorithms,” Tech-
nical Report No. 26, Dept. of Operations Research, Stanford University,
CA.
Zenios, S. A., and Mulvey, J. M., 1986. “Relaxation Techniques for Strictly
Convex Network Problems,” Annals of Operations Research, Vol. 5, pp.
517-538.
Zoutendijk, G., 1976. Mathematical Programming Methods, North Hol-
land, Amsterdam.

