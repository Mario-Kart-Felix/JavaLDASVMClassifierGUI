Found Comput Math (2009) 9: 717â€“772
DOI 10.1007/s10208-009-9045-5
Exact Matrix Completion via Convex Optimization
Emmanuel J. CandÃ¨s Â· Benjamin Recht
Received: 30 May 2008 / Revised: 6 February 2009 / Accepted: 14 February 2009 /
Published online: 3 April 2009
Â© The Author(s) 2009. This article is published with open access at Springerlink.com
Abstract We consider a problem of considerable practical interest: the recovery of a
data matrix from a sampling of its entries. Suppose that we observe m entries selected
uniformly at random from a matrix M . Can we complete the matrix and recover the
entries that we have not seen?
We show that one can perfectly recover most low-rank matrices from what appears
to be an incomplete set of entries. We prove that if the number m of sampled entries
obeys
m â‰¥ C n1.2r logn
for some positive numerical constant C, then with very high probability, most n Ã— n
matrices of rank r can be perfectly recovered by solving a simple convex optimization
program. This program finds the matrix with minimum nuclear norm that fits the data.
The condition above assumes that the rank is not too large. However, if one replaces
the 1.2 exponent with 1.25, then the result holds for all values of the rank. Similar
results hold for arbitrary rectangular matrices as well. Our results are connected with
the recent literature on compressed sensing, and show that objects other than signals
and images can be perfectly reconstructed from very limited information.
Communicated by Michael Todd.
E.J. CandÃ¨s ()
Applied and Computational Mathematics, Caltech, Pasadena, CA 91125, USA
e-mail: emmanuel@acm.caltech.edu
B. Recht
Center for the Mathematics of Information, Caltech, Pasadena, CA 91125, USA
e-mail: brecht@ist.caltech.edu
718 Found Comput Math (2009) 9: 717â€“772
Keywords Matrix completion Â· Low-rank matrices Â· Convex optimization Â· Duality
in optimization Â· Nuclear norm minimization Â· Random matrices Â· Noncommutative
Khintchine inequality Â· Decoupling Â· Compressed sensing
Mathematics Subject Classification (2000) Primary 90C25 Â· Secondary 90C59 Â·
15A52
1 Introduction
In many practical problems of interest, one would like to recover a matrix from a
sampling of its entries. As a motivating example, consider the task of inferring an-
swers in a partially filled out survey. That is, suppose that questions are being asked
to a collection of individuals. Then we can form a matrix where the rows index each
individual and the columns index the questions. We collect data to fill out this table
but unfortunately, many questions are left unanswered. Is it possible to make an ed-
ucated guess about what the missing answers should be? How can one make such a
guess? Formally, we may view this problem as follows. We are interested in recover-
ing a data matrix M with n1 rows and n2 columns, but only get to observe a number
m of its entries which is comparably much smaller than n1n2, the total number of
entries. Can one recover the matrix M from m of its entries? In general, everyone
would agree that this is impossible without some additional information.
In many instances, however, the matrix we wish to recover is known to be struc-
tured in the sense that it is low-rank or approximately low-rank. (We recall for com-
pleteness that a matrix with n1 rows and n2 columns has rank r if its rows or columns
span an r-dimensional space.) Below are two examples of practical scenarios where
one would like to be able to recover a low-rank matrix from a sampling of its entries.
â€¢ The Netflix problem. In the area of recommender systems, users submit ratings on
a subset of entries in a database, and the vendor provides recommendations based
on the userâ€™s preferences [30, 34]. Because users only rate a few items, one would
like to infer their preference for unrated items.
A special instance of this problem is the now famous Netflix problem [1]. Users
(rows of the data matrix) are given the opportunity to rate movies (columns of the
data matrix), but users typically rate only very few movies so that there are very few
scattered observed entries of this data matrix. Yet, one would like to complete this
matrix so that the vendor (here Netflix) might recommend titles that any particular
user is likely to be willing to order. In this case, the data matrix of all user-ratings
may be approximately low-rank because it is commonly believed that only a few
factors contribute to an individualâ€™s tastes or preferences.
â€¢ Triangulation from incomplete data. Suppose we are given partial information
about the distances between objects and would like to reconstruct the low-
dimensional geometry describing their locations. For example, we may have a
network of low-power wirelessly networked sensors scattered randomly across a
region. Suppose each sensor only has the ability to construct distance estimates
based on signal strength readings from its nearest fellow sensors. From these noisy
distance estimates, we can form a partially observed distance matrix. We can then
Found Comput Math (2009) 9: 717â€“772 719
estimate the true distance matrix whose rank will be equal to two if the sensors are
located in a plane or three if they are located in three dimensional space [25, 33].
In this case, we only need to observe a few distances per node to have enough
information to reconstruct the positions of the objects.
These examples are, of course, far from exhaustive and there are many other problems
which fall in this general category. For instance, we may have some very limited
information about a covariance matrix of interest. Yet, this covariance matrix may
be low-rank or approximately low-rank because the variables only depend upon a
comparably smaller number of factors.
1.1 Impediments and Solutions
Suppose for simplicity that we wish to recover a square n Ã— n matrix M of rank r .1
Such a matrix M can be represented by n2 numbers, but it only has (2nâˆ’ r)r degrees
of freedom. This fact can be revealed by counting parameters in the singular value
decomposition (the number of degrees of freedom associated with the description of
the singular values and of the left and right singular vectors). When the rank is small,
this is considerably smaller than n2. For instance, when M encodes a 10-dimensional
phenomenon, then the number of degrees of freedom is about 20n offering a reduc-
tion in dimensionality by a factor about equal to n/20. When n is large (e.g., in the
thousands or millions), the data matrix carries much less information than its ambient
dimension suggests. The problem is now whether it is possible to recover this matrix
from a sampling of its entries without having to probe all the n2 entries, or more
generally collect n2 or more measurements about M .
1.1.1 Which Matrices?
In general, one cannot hope to be able to recover a low-rank matrix from a sample of
its entries. Consider the rank-1 matrix M equal to
M = e1eâˆ—n =
âŽ¡
âŽ¢âŽ¢âŽ¢âŽ£
0 0 Â· Â· Â· 0 1
0 0 Â· Â· Â· 0 0
...
...
...
...
...
0 0 Â· Â· Â· 0 0
âŽ¤
âŽ¥âŽ¥âŽ¥âŽ¦ , (1.1)
where here and throughout, ei is the ith canonical basis vector in Euclidean space
(the vector with all entries equal to 0 but the ith equal to 1). This matrix has a 1
in the top-right corner and all the other entries are 0. Clearly, this matrix cannot be
recovered from a sampling of its entries unless we pretty much see all the entries.
The reason is that for most sampling sets, we would only get to see zeros so that we
would have no way of guessing that the matrix is not zero. For instance, if we were
1We emphasize that there is nothing special about M being square and all of our discussion would apply
to arbitrary rectangular matrices as well. The advantage of focusing on square matrices is a simplified
exposition and reduction in the number of parameters of which we need to keep track.
720 Found Comput Math (2009) 9: 717â€“772
to see 90% of the entries selected at random, then 10% of the time we would only get
to see zeroes.
It is therefore impossible to recover all low-rank matrices from a set of sampled en-
tries but can one recover most of them? To investigate this issue, we introduce a sim-
ple model of low-rank matrices. Consider the singular value decomposition (SVD) of
a matrix M
M =
râˆ‘
k=1
Ïƒkukv
âˆ—
k, (1.2)
where the ukâ€™s and vkâ€™s are the left and right singular vectors, and the Ïƒkâ€™s are the sin-
gular values (the roots of the eigenvalues of Mâˆ—M). Then we could think of a generic
low-rank matrix as follows: the families {uk}1â‰¤kâ‰¤r and {vk}1â‰¤kâ‰¤r are sampled uni-
formly at random among all families of r orthonormal vectors independently of each
other. (The independence between these two families simplifies the exposition, and is
not crucial.) We make no assumptions about the singular values Ïƒk . In the sequel, we
will refer to this model as the random orthogonal model. This model is convenient in
the sense that it is both very concrete and simple, and useful in the sense that it will
help us fix the main ideas. In the sequel, however, we will consider far more general
models. The question for now is whether or not one can recover such a generic matrix
from a sampling of its entries.
1.1.2 Which Sampling Sets?
Clearly, one cannot hope to reconstruct any low-rank matrix Mâ€”even of rank 1â€”if
the sampling set avoids any column or row of M . Suppose that M is of rank 1 and of
the form xyâˆ—, x,y âˆˆ Rn so that the (i, j)th entry is given by
Mij = xiyj .
Then if we do not have samples from the first row, for example, one could never guess
the value of the first component of x1 by any method whatsoever; no information
about x1 is observed. There is of course nothing special about the first row and this
argument extends to any row or column. To have any hope of recovering an unknown
matrix, one needs at least one observation per row and one observation per column.
We have just seen that if the sampling is adversarial, e.g., one observes all of the
entries of M but those in the first row, then one would not even be able to recover
matrices of rank 1. But what happens for most sampling sets? Can one recover a
low-rank matrix from almost all sampling sets of cardinality m? Formally, suppose
that the set  of locations corresponding to the observed entries ((i, j) âˆˆ  if Mij
is observed) is a set of cardinality m sampled uniformly at random. Then can one
recover a generic low-rank matrix M , perhaps with very large probability, from the
knowledge of the value of its entries in the set ?
1.1.3 Which Algorithm?
If the number of measurements is sufficiently large, and if the entries are sufficiently
uniformly distributed as above, one might hope that there is only one low-rank matrix
Found Comput Math (2009) 9: 717â€“772 721
with these entries. If this were true, one would want to recover the data matrix by
solving the optimization problem
minimize rank(X)
subject to Xij = Mij (i, j) âˆˆ , (1.3)
where X is the decision variable and rank(X) is equal to the rank of the matrix X.
The program (1.3) is a common sense approach which simply seeks the simplest
explanation fitting the observed data. If there were only one low-rank object fitting the
data, this would recover M . This is unfortunately of little practical use because this
optimization problem is not only NP-hard, but all known algorithms which provide
exact solutions require time doubly exponential in the dimension n of the matrix in
both theory and practice [13].
If a matrix has rank r , then it has exactly r nonzero singular values so that the rank
function in (1.3) is simply the number of nonvanishing singular values. In this paper,
we consider an alternative which minimizes the sum of the singular values over the
constraint set. This sum is called the nuclear norm,
â€–Xâ€–âˆ— =
nâˆ‘
k=1
Ïƒk(X) (1.4)
where, here and below, Ïƒk(X) denotes the kth largest singular value of X. The heuris-
tic optimization is then given by
minimize â€–Xâ€–âˆ—
subject to Xij = Mij (i, j) âˆˆ . (1.5)
Whereas the rank function counts the number of nonvanishing singular values, the
nuclear norm sums their amplitude and in some sense, is to the rank functional what
the convex 1 norm is to the counting 0 norm in the area of sparse signal recovery.
The main point here is that the nuclear norm is a convex function and, as we will
discuss in Sect. 1.4 can be optimized efficiently via semidefinite programming.
1.1.4 A First Typical Result
Our first result shows that, perhaps unexpectedly, this heuristic optimization recovers
a generic M when the number of randomly sampled entries is large enough. We will
prove the following theorem.
Theorem 1.1 Let M be an n1 Ã— n2 matrix of rank r sampled from the random or-
thogonal model, and put n = max(n1, n2). Suppose we observe m entries of M with
locations sampled uniformly at random. Then there are numerical constants C and c
such that if
m â‰¥ C n5/4r logn , (1.6)
the minimizer to the problem (1.5) is unique and equal to M with probability at least
1âˆ’cnâˆ’3 logn; that is to say, the semidefinite program (1.5) recovers all the entries of
722 Found Comput Math (2009) 9: 717â€“772
M with no error. In addition, if r â‰¤ n1/5, then the recovery is exact with probability
at least 1 âˆ’ cnâˆ’3 logn provided that
m â‰¥ C n6/5r logn. (1.7)
Furthermore, (1.7) holds even when the two families of singular vectors are depen-
dent; all that is needed is that the marginal distribution of each family is uniform.
The theorem states that a surprisingly small number of entries are sufficient to
complete a generic low-rank matrix. For small values of the rank, e.g., when r = O(1)
or r = O(logn), one only needs to see on the order of n6/5 entries (ignoring logarith-
mic factors) which is considerably smaller than n2â€”the total number of entries of a
square matrix. The real feat, however, is that the recovery algorithm is tractable and
very concrete. Hence, the contribution is twofold:
â€¢ Under the hypotheses of Theorem 1.1, there is a unique low-rank matrix which is
consistent with the observed entries.
â€¢ Further, this matrix can be recovered by the convex optimization (1.5). In other
words, for most problems, the nuclear norm relaxation is formally equivalent to
the combinatorially hard rank minimization problem (1.3).
Theorem 1.1 is in fact a special instance of a far more general theorem that covers a
much larger set of matrices M . We describe this general class of matrices and precise
recovery conditions in the next section.
1.2 Main Results
As seen in our first example (1.1), it is impossible to recover a matrix which is equal
to zero in nearly all of its entries unless we see all the entries of the matrix. To recover
a low-rank matrix, this matrix cannot be in the null space of the â€œsampling operatorâ€
that provides the values of a subset of the entries. Now, it is easy to see that if the
singular vectors of a matrix M are highly concentrated, then M could very well be in
the null-space of the sampling operator. For instance, consider the rank-2 symmetric
matrix M given by
M =
2âˆ‘
k=1
Ïƒkuku
âˆ—
k, u1 = (e1 + e2)/
âˆš
2, u2 = (e1 âˆ’ e2)/
âˆš
2,
where the singular values are arbitrary. Then this matrix vanishes everywhere except
in the top-left 2 Ã— 2 corner and one would basically need to see all the entries of
M to be able to recover this matrix exactly by any method whatsoever. There is an
endless list of examples of this sort. Hence, we arrive at the notion that somehow the
singular vectors need to be sufficiently spreadâ€”that is, uncorrelated with the standard
basisâ€”in order to minimize the number of observations needed to recover a low-rank
matrix.2 This motivates the following definition.
2Both the left and right singular vectors need to be uncorrelated with the standard basis. Indeed, the matrix
e1v
âˆ— has its first row equal to v and all the others equal to zero. Clearly, this rank-1 matrix cannot be
recovered unless we basically see all of its entries.
Found Comput Math (2009) 9: 717â€“772 723
Definition 1.2 Let U be a subspace of Rn of dimension r and P U be the orthogonal
projection onto U . Then the coherence of U (vis-Ã -vis the standard basis (ei )) is
defined to be
Î¼(U) â‰¡ n
r
max
1â‰¤iâ‰¤n
â€–P Ueiâ€–2. (1.8)
Note that for any subspace, the smallest Î¼(U) can be is 1, achieved, for example,
if U is spanned by vectors whose entries all have magnitude 1/
âˆš
n. The largest pos-
sible value for Î¼(U) is n/r which would correspond to any subspace that contains a
standard basis element. We shall be primarily interested in subspaces with low coher-
ence as matrices whose column and row spaces have low coherence cannot really be
in the null space of the sampling operator. For instance, we will see that the random
subspaces discussed above have nearly minimal coherence.
To state our main result, we introduce two assumptions about an n1 Ã— n2 matrix
M whose SVD is given by M =âˆ‘1â‰¤kâ‰¤r Ïƒkukvâˆ—k and with column and row spaces
denoted by U and V, respectively.
A0 The coherences obey max(Î¼(U),Î¼(V )) â‰¤ Î¼0 for some positive Î¼0.
A1 The n1 Ã—n2 matrixâˆ‘1â‰¤kâ‰¤rukvâˆ—k has a maximum entry bounded by Î¼1
âˆš
r/(n1n2)
in absolute value for some positive Î¼1.
The Î¼â€™s above may depend on r and n1, n2. Moreover, note that A1 always holds
with Î¼1 = Î¼0 âˆšr since the (i, j)th entry of the matrix âˆ‘1â‰¤kâ‰¤r ukvâˆ—k is given byâˆ‘
1â‰¤kâ‰¤r uikvjk and by the Cauchyâ€“Schwarz inequality,
âˆ£âˆ£âˆ£âˆ£
âˆ‘
1â‰¤kâ‰¤r
uikvjk
âˆ£âˆ£âˆ£âˆ£â‰¤
âˆš âˆ‘
1â‰¤kâ‰¤r
|uik|2
âˆš âˆ‘
1â‰¤kâ‰¤r
|vjk|2 â‰¤ Î¼0râˆš
n1n2
.
Hence, for sufficiently small ranks, Î¼1 is comparable to Î¼0. As we will see in Sect. 2,
for larger ranks, both subspaces selected from the uniform distribution and spaces
constructed as the span of singular vectors with bounded entries are not only incoher-
ent with the standard basis, but also obey A1 with high probability for values of Î¼1
at most logarithmic in n1 and/or n2. Below, we will assume that Î¼1 is greater than or
equal to 1.
We are in the position to state our main result: if a matrix has row and column
spaces that are incoherent with the standard basis, then nuclear norm minimization
can recover this matrix from a random sampling of a small number of entries.
Theorem 1.3 Let M be an n1 Ã—n2 matrix of rank r obeying A0 and A1 and put n =
max(n1, n2). Suppose we observe m entries of M with locations sampled uniformly
at random. Then there exist constants C, c such that if
m â‰¥ C max(Î¼21,Î¼1/20 Î¼1,Î¼0n1/4
)
nr(Î² logn) (1.9)
for some Î² > 2, then the minimizer to the problem (1.5) is unique and equal to M
with probability at least 1 âˆ’ cnâˆ’Î² . For r â‰¤ Î¼âˆ’10 n1/5 and under A0 only, this estimate
724 Found Comput Math (2009) 9: 717â€“772
can be improved to
m â‰¥ C Î¼0 n6/5r(Î² logn) (1.10)
with the same probability of success.
Theorem 1.3 asserts that if the coherence is low, few samples are required to re-
cover M . For example, if Î¼0 = O(1) and the rank is not too large, then the recovery
is exact with large probability provided that
m â‰¥ C n6/5r logn. (1.11)
We emphasize that if one is interested in low-rank matrices, there is only one as-
sumption: A0. Below, we give two illustrative examples of matrices with incoherent
column and row spaces. This list is by no means exhaustive.
1. The first example is the random orthogonal model. For values of the rank r greater
than logn, Î¼(U) and Î¼(V ) are O(1), Î¼1 = O(logn) both with very large proba-
bility. Hence, the recovery is exact provided that m obeys (1.6) or (1.7). Special-
izing Theorem 1.3 to these values of the parameters gives Theorem 1.1. Hence,
Theorem 1.1 is a special case of our general recovery result.
2. The second example is more general and, in a nutshell, simply requires that the
components of the singular vectors of M are small. Assume that the uj and vj â€™s
obey
max
ij
âˆ£âˆ£ã€ˆei ,uj ã€‰
âˆ£âˆ£2 â‰¤ Î¼B/n, max
ij
âˆ£âˆ£ã€ˆei ,vj ã€‰
âˆ£âˆ£2 â‰¤ Î¼B/n, (1.12)
for some value of Î¼B = O(1). Then the maximum coherence is at most Î¼B since
Î¼(U) â‰¤ Î¼B and Î¼(V ) â‰¤ Î¼B . Further, we will see in Sect. 2 that A1 holds most of
the time with Î¼1 = O(âˆšlogn). Thus, for matrices with singular vectors obeying
(1.12), the recovery is exact provided that m obeys (1.11) for values of the rank
not exceeding Î¼âˆ’1B n1/5.
1.3 Extensions
Our main result (Theorem 1.3) extends to a variety of other low-rank matrix comple-
tion problems beyond the sampling of entries. Indeed, suppose we have two ortho-
normal bases f 1, . . . ,f n and g1, . . . ,gn of R
n, and that we are interested in solving
the rank minimization problem
minimize rank(X)
subject to f âˆ—i Xgj = f âˆ—i Mgj , (i, j) âˆˆ . (1.13)
To see that our theorem provides conditions under which (1.13) can be solved via
nuclear norm minimization, note that there exist unitary transformations F and G
such that ej = Ff j and ej = Ggj for each j = 1, . . . , n. Hence,
f âˆ—i Xgj = eâˆ—i
(
FXGâˆ—
)
ej .
Found Comput Math (2009) 9: 717â€“772 725
Then if the conditions of Theorem 1.3 hold for the matrix FXGâˆ—, it is immediate that
nuclear norm minimization finds the unique optimal solution of (1.13) when we are
provided a large enough random collection of the inner products f âˆ—i Mgj . In other
words, all that is needed is that the column and row spaces of M be respectively
incoherent with the basis (f i ) and (gi ).
From this perspective, we additionally remark that our results likely extend to the
case where one observes a small number of arbitrary linear functionals of a hidden
matrix M . Set N = n2 and let A1, . . . ,AN be an orthonormal basis for the linear
space of n Ã— n matrices with the usual inner product ã€ˆX,Y ã€‰ = trace(Xâˆ—Y ). Then we
expect our results should also apply to the rank minimization problem
minimize rank(X)
subject to ã€ˆAk,Xã€‰ = ã€ˆAk,Mã€‰ k âˆˆ , (1.14)
where  âŠ‚ {1, . . . ,N} is selected uniformly at random. In fact, (1.14) is (1.3) when
the orthobasis is the canonical basis (eieâˆ—j )1â‰¤i,jâ‰¤n. Here, those low-rank matrices
which have small inner product with all the basis elements Ak may be recoverable by
nuclear norm minimization. To avoid unnecessary confusion and notational clutter,
we leave this general low-rank recovery problem for future work.
1.4 Connections, Alternatives and Prior Art
Nuclear norm minimization is a recent heuristic introduced by Fazel in [19], and is an
extension of the trace heuristic often used by the control community; see, e.g., [4, 28].
Indeed, when the matrix variable is symmetric and positive semidefinite, the nuclear
norm of X is the sum of the (nonnegative) eigenvalues and thus equal to the trace
of X. Hence, for positive semidefinite unknowns, (1.5) would simply minimize the
trace over the constraint set:
minimize trace(X)
subject to Xij = Mij (i, j) âˆˆ ,
X  0.
This is a semidefinite program. Even for the general matrix M which may not be
positive definite or even symmetric, the nuclear norm heuristic can be formulated in
terms of semidefinite programming as, for instance, the program (1.5) is equivalent
to
minimize trace(W 1) + trace(W 2)
subject to Xij = Mij (i, j) âˆˆ ,[
W 1 X
Xâˆ— W 2
]
 0
with optimization variables X, W 1 and W 2, (see, e.g., [19, 37]). There are many
efficient algorithms and high-quality software available for solving these types of
problems.
Our work is inspired by results in the emerging field of compressive sampling or
compressed sensing, a new paradigm for acquiring information about objects of inter-
est from what appears to be a highly incomplete set of measurements [10, 12, 16]. In
726 Found Comput Math (2009) 9: 717â€“772
practice, this means for example that high-resolution imaging is possible with fewer
sensors, or that one can speed up signal acquisition time in biomedical applications
by orders of magnitude, simply by taking far fewer specially coded samples. Math-
ematically speaking, we wish to reconstruct a signal x âˆˆ Rn from a small number
measurements y = Î¦x, y âˆˆ Rm, and m is much smaller than n; i.e., we have far
fewer equations than unknowns. In general, one cannot hope to reconstruct x, but
assume now that the object we wish to recover is known to be structured in the sense
that it is sparse (or approximately sparse). This means that the unknown object de-
pends upon a smaller number of unknown parameters. Then it has been shown that 1
minimization allows recovery of sparse signals from remarkably few measurements:
supposing Î¦ is chosen randomly from a suitable distribution, then with very high
probability, all sparse signals with about k nonzero entries can be recovered from on
the order of k logn measurements. For instance, if x is k-sparse in the Fourier do-
main, i.e., x is a superposition of k sinusoids, then it can be perfectly recovered with
high probabilityâ€”by 1 minimizationâ€”from the knowledge of about k logn of its
entries sampled uniformly at random [10].
From this viewpoint, the results in this paper greatly extend the theory of com-
pressed sensing by showing that other types of interesting objects or structures, be-
yond sparse signals and images, can be recovered from a limited set of measure-
ments. Moreover, the techniques for proving our main results build upon ideas from
the compressed sensing literature together with probabilistic tools such as the power-
ful techniques of Bourgain and of Rudelson for bounding norms of operators between
Banach spaces.
Our notion of incoherence generalizes the concept of the same name in compres-
sive sampling. Notably, in [9], the authors introduce the notion of the incoherence of
a unitary transformation. Letting U be an n Ã— n unitary matrix, the coherence of U
is given by
Î¼(U) = nmax
j,k
|Ujk|2.
This quantity ranges in values from 1 for a unitary transformation whose entries
all have the same magnitude to n for the identity matrix. Using this notion, [9]
showed that with high probability, a k-sparse signal could be recovered via lin-
ear programming from the observation of the inner product of the signal with
m = (Î¼(U)k logn) randomly selected columns of the matrix U . This result pro-
vided a generalization of the celebrated results about partial Fourier observations
described in [10], a special case where Î¼(U) = 1. This paper generalizes the notion
of incoherence to problems beyond the setting of sparse signal recovery.
In [29], the authors studied the nuclear norm heuristic applied to a related problem
where partial information about a matrix M is available from m equations of the form
âŒ©
A(k),M
âŒª=
âˆ‘
ij
A
(k)
ij Mij = bk, k = 1, . . . ,m, (1.15)
where for each k, {A(k)ij } is an i.i.d. sequence of Gaussian or Bernoulli random vari-
ables and the sequences {A(k)} are also independent from each other (the sequences
{A(k)} and {bk} are available to the analyst). Building on the concept of restricted
Found Comput Math (2009) 9: 717â€“772 727
isometry introduced in [11] in the context of sparse signal recovery, [29] establishes
the first sufficient conditions for which the nuclear norm heuristic returns the mini-
mum rank element in the constraint set. They prove that the heuristic succeeds with
large probability whenever the number m of available measurements is greater than
a constant times 2nr logn for n Ã— n matrices. Although this is an interesting result, a
serious impediment to this approach is that one needs to essentially measure random
projections of the unknown data matrixâ€”a situation which unfortunately does not
commonly arise in practice. Further, the measurements in (1.15) give some informa-
tion about all the entries of M whereas in our problem, information about most of
the entries is simply not available. In particular, the results and techniques introduced
in [29] do not begin to address the matrix completion problem of interest to us in this
paper. As a consequence, our methods are completely different; for example, they
do not rely on any notions of restricted isometry. Instead, as we discuss below, we
prove the existence of a Lagrange multiplier for the optimization (1.5) that certifies
the unique optimal solution is precisely the matrix that we wish to recover.
We would like to note that other recovery algorithms may be possible when the
sampling happens to be chosen in a very special fashion. For example, suppose that
M is generic and that we precisely observe every entry in the first r rows and columns
of the matrix. Write M in block form as
M =
[
M11 M12
M21 M22
]
with M11 an r Ã— r matrix. In the special case that M11 is invertible and M has
rank r , then it is easy to verify that M22 = M21Mâˆ’111 M12. One can prove this iden-
tity by forming the SVD of M , for example. That is, if M is generic, and the upper
r Ã— r block is invertible, and we observe every entry in the first r rows and columns,
we can recover M . This result immediately generalizes to the case where one ob-
serves precisely r rows and r columns and the r Ã— r matrix at the intersection of the
observed rows and columns is invertible. However, this scheme has many practical
drawbacks that stand in the way of a generalization to a completion algorithm from a
general set of entries. First, if we miss any entry in these rows or columns, we cannot
recover M , nor can we leverage any information provided by entries of M22. Second,
if the matrix has rank less than r , and we observe r rows and columns, a combina-
torial search to find the collection that has an invertible square subblock is required.
Several authors have observed that if the set of rows and columns is selected at ran-
dom, then this rank deficiency is highly improbable. For example, [17, 18] show that
if the columns are sampled from a distribution proportional to the column Euclidean
norms of the matrix, then a low rank matrix can be reconstructed with no error. Again,
even after randomization, this method does not work when some of the entries in the
sampled columns are missing. Moreover, in the setting considered in this paper, there
is no oracle that would inform us about the norm of a row or column.
Finally, we note that in [3] the authors propose to solve a version of the matrix
completion problem where the provided entries are corrupted by noise. They compute
the SVD of the matrix which is equal to Mij when (i, j) âˆˆ  and zeros everywhere
else. They show that when the underlying matrix M has rank r , this procedure results
in a rank-r approximation which constructs most entries of M to error o(1) as long as
728 Found Comput Math (2009) 9: 717â€“772
the noise is small. Unfortunately, this procedure is not useful in the context where the
entries are not noisy: the bound provides a reconstruction error which scales as O(n)
in the Frobenius norm. This is in sharp contrast to our algorithm which reconstructs
the matrix with no error as long as  has sufficiently large cardinality.
1.5 Notations and Organization of the Paper
The paper is organized as follows. We first argue in Sect. 2 that the random orthogo-
nal model and, more generally, matrices with incoherent column and row spaces obey
the assumptions of the general Theorem 1.3. To prove Theorem 1.3, we first establish
sufficient conditions which guarantee that the true low-rank matrix M is the unique
solution to (1.5) in Sect. 3. One of these conditions is the existence of a dual vector
obeying two crucial properties. Section 4 constructs such a dual vector and provides
the overall architecture of the proof which shows that, indeed, this vector obeys the
desired properties provided that the number of measurements is sufficiently large.
Surprisingly, as explored in Sect. 5, the existence of a dual vector certifying that M
is unique is related to some problems in random graph theory including â€œthe coupon
collectorâ€™s problem.â€ Following this discussion, we prove our main result via several
intermediate results which are all proven in Sect. 6. Section 7 introduces numerical
experiments showing that matrix completion based on nuclear norm minimization
works well in practice. Section 8 closes the paper with a short summary of our find-
ings, a discussion of important extensions and improvements. In particular, we will
discuss possible ways of improving the 1.2 exponent in (1.10) so that it gets closer
to 1. Finally, the Appendix provides proofs of auxiliary lemmas supporting our main
argument.
Before continuing, we provide here a brief summary of the notations used through-
out the paper. Matrices are bold capital, vectors are bold lowercase and scalars or en-
tries are not bold. For instance, X is a matrix and Xij its (i, j)th entry. Likewise x is
a vector and xi its ith component. When we have a collection of vectors uk âˆˆ Rn for
1 â‰¤ k â‰¤ d , we will denote by uik the ith component of the vector uk and [u1, . . . ,ud ]
will denote the n Ã— d matrix whose kth column is uk .
A variety of norms on matrices will be discussed. The spectral norm of a matrix
is denoted by â€–Xâ€–. The Euclidean inner product between two matrices is ã€ˆX,Y ã€‰ =
trace(Xâˆ—Y ), and the corresponding Euclidean norm, called the Frobenius or Hilbertâ€“
Schmidt norm, is denoted â€–Xâ€–F . That is, â€–Xâ€–F = ã€ˆX,Xã€‰1/2. The nuclear norm of a
matrix X is â€–Xâ€–âˆ—. For q â‰¥ 1, the Schatten q-norm of a matrix is denoted by
â€–Xâ€–Sq =
(
nâˆ‘
i=1
Ïƒi(X)
q
)1/q
. (1.16)
Note that the nuclear, Frobenius, and operator norms are respectively equal to the
Schatten 1-, 2- and âˆž-norms. The maximum entry of X (in absolute value) is denoted
by â€–Xâ€–âˆž â‰¡ maxij |Xij |. For vectors, we will only consider the usual Euclidean 2
norm, which we simply write as â€–xâ€–.
Further, we will also manipulate linear transformations which act on matri-
ces and will use calligraphic letters for these operators as in A(X). In particular,
Found Comput Math (2009) 9: 717â€“772 729
the identity operator will be denoted by I . The only norm we will consider for
these operators is their spectral norm (the top singular value) denoted by â€–Aâ€– =
supX:â€–Xâ€–F â‰¤1 â€–A(X)â€–F .
Finally, we adopt the convention that C denotes a numerical constant indepen-
dent of the matrix dimensions, rank, and number of measurements, whose value may
change from line to line. Certain special constants with precise numerical values will
be ornamented with subscripts (e.g., CR). Any exceptions to this notational scheme
will be noted in the text.
2 Which Matrices Are Incoherent?
In this section, we restrict our attention to square n Ã— n matrices, but the extension to
rectangular n1 Ã— n2 matrices immediately follows by setting n = max(n1, n2).
2.1 Incoherent Bases Span Incoherent Subspaces
Almost all n Ã— n matrices M with singular vectors {uk}1â‰¤kâ‰¤r and {vk}1â‰¤kâ‰¤r whose
components have magnitude bounded by Î¼B as in (1.12) also satisfy the assump-
tions A0 and A1 with Î¼0 = Î¼B , Î¼1 = CÎ¼Bâˆšlogn for some positive constant C. As
mentioned above, A0 holds automatically, but observe that A1 would not hold with a
small value of Î¼1 if the matrices [u1, . . . ,ur ] and [v1, . . . ,vr ] have a row in common
where all of the entries have magnitude
âˆš
Î¼B/n. It is not hard to see that in this case
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
k
ukv
âˆ—
k
âˆ¥âˆ¥âˆ¥âˆ¥âˆž
= Î¼B r/n.
Certainly, this example is constructed in a very special way, and should occur infre-
quently. We now show that it is generically unlikely.
Consider the matrix
râˆ‘
k=1
kukv
âˆ—
k, (2.1)
where {k}1â‰¤kâ‰¤r is an arbitrary sign sequence. For almost all choices of sign se-
quences, A1 is satisfied with Î¼1 = O(Î¼Bâˆšlogn). Indeed, if one selects the signs
uniformly at random, then for each Î² > 0,
P
(âˆ¥âˆ¥âˆ¥âˆ¥âˆ¥
râˆ‘
k=1
kukvk
âˆ¥âˆ¥âˆ¥âˆ¥âˆ¥âˆž
â‰¥ Î¼B
âˆš
8Î²r logn/n
)
â‰¤ (2n2)nâˆ’Î². (2.2)
This is of interest because suppose the low-rank matrix we wish to recover is of the
form
M =
râˆ‘
k=1
Î»kukv
âˆ—
k (2.3)
730 Found Comput Math (2009) 9: 717â€“772
with scalars Î»k . Since the vectors {uk} and {vk} are orthogonal, the singular values
of M are given by |Î»k| and the singular vectors are given by sgn(Î»k)uk and vk for
k = 1, . . . , r . Hence, in this model, A1 concerns the maximum entry of the matrix
given by (2.1) with k = sgn(Î»k). That is to say, for most sign patterns, the matrix of
interest obeys an appropriate size condition. We emphasize here that the only thing
that we assumed about the ukâ€™s and vkâ€™s was that they had small entries. In particular,
they could be equal to each other as would be the case for a symmetric matrix.
The claim (2.2) is a simple application of Hoeffdingâ€™s inequality. The (i, j)th entry
of (2.1) is given by
Zij =
âˆ‘
1â‰¤kâ‰¤r
kuikvjk,
and is a sum of r zero-mean independent random variables, each bounded by Î¼B/n.
Therefore,
P
(|Zij | â‰¥ Î»Î¼B
âˆš
r/n
)â‰¤ 2eâˆ’Î»2/8.
Setting Î» proportional to
âˆš
logn and applying the union bound gives the claim.
To summarize, we say that M is sampled from the incoherent basis model if it is
of the form
M =
râˆ‘
k=1
kÏƒkukv
âˆ—
k; (2.4)
{k}1â‰¤kâ‰¤r is a random sign sequence, and {uk}1â‰¤kâ‰¤r and {vk}1â‰¤kâ‰¤r have maximum
entries of size at most
âˆš
Î¼B/n.
Lemma 2.1 There exist numerical constants c and C such that for any Î² > 0,
matrices from the incoherent basis model obey the assumption A1 with Î¼1 â‰¤
CÎ¼B
âˆš
(Î² + 2) logn with probability at least 1 âˆ’ cnâˆ’Î² .
2.2 Random Subspaces Are Incoherent
In this section, we prove that the random orthogonal model obeys the two assump-
tions A0 and A1 (with appropriate values for the Î¼â€™s) with large probability.
Lemma 2.2 Set rÌ„ = max(r, logn). Then there exist constants C and c such that the
random orthogonal model obeys:3
1. maxi â€–P Ueiâ€–2 â‰¤ C rÌ„/n
2. â€–âˆ‘1â‰¤kâ‰¤r ukvâˆ—kâ€–âˆž â‰¤ C logn
âˆš
rÌ„/n
with probability 1 âˆ’ cnâˆ’3 logn. The second estimate assumes independence between
the families {uk}1â‰¤kâ‰¤r and {vk}1â‰¤kâ‰¤r .
3When r â‰¥ Câ€²(logn)3 for some positive constant Câ€², a better estimate is possible, namely,
â€–âˆ‘1â‰¤kâ‰¤r ukvâˆ—kâ€–âˆž â‰¤ C
âˆš
r logn/n.
Found Comput Math (2009) 9: 717â€“772 731
We note that an argument similar to the following proof would give that if C is of
the form KÎ² where K is a fixed numerical constant, we can achieve a probability at
least 1 âˆ’ cnâˆ’Î² provided that n is sufficiently large. To establish these facts, we make
use of the standard result below [22].
Lemma 2.3 Let Yd be distributed as a chi-squared random variable with d degrees
of freedom. Then for each t > 0
P
(
Yd âˆ’ d â‰¥ t
âˆš
2d + t2)â‰¤ eâˆ’t2/2 and P(Yd âˆ’ d â‰¤ âˆ’t
âˆš
2d
)â‰¤ eâˆ’t2/2. (2.5)
We will use (2.5) as follows: for each  âˆˆ (0,1), we have
P
(
Yd â‰¥ d (1 âˆ’ )âˆ’1
)â‰¤ eâˆ’2d/4 and P(Yd â‰¤ d (1 âˆ’ )
)â‰¤ eâˆ’2d/4. (2.6)
We begin with the first assertion of Lemma 2.2. Observe that it follows from
â€–P Ueiâ€–2 =
âˆ‘
1â‰¤kâ‰¤r
u2ik, (2.7)
that Zr â‰¡ â€–P Ueiâ€–2 (for fixed i) is the squared Euclidean length of the first r com-
ponents of a unit vector uniformly distributed on the unit sphere in n dimensions.
Now, suppose that x1, x2, . . . , xn are i.i.d. N(0,1). Then the distribution of a unit
vector uniformly distributed on the sphere is that of x/â€–xâ€– and, therefore, the law
of Zr is that of Yr/Yn, where Yr = âˆ‘kâ‰¤r x2k . Fix  > 0 and consider the event
An, = {Yn/n â‰¥ 1 âˆ’ }. For each Î» > 0, it follows from (2.6) that
P
(
Zr âˆ’ r/n â‰¥ Î»
âˆš
2r/n
)= P(Yr â‰¥
[
r + Î»âˆš2r]Yn/n
)
â‰¤ P(Yr â‰¥
[
r + Î»âˆš2r]Yn/n and An,
)+ P(Acn,
)
â‰¤ P(Yr â‰¥
[
r + Î»âˆš2r][1 âˆ’ ])+ eâˆ’2n/4
= P(Yr âˆ’ r â‰¥ Î»
âˆš
2r
[
1 âˆ’  âˆ’ 
âˆš
r/2Î»2
])+ eâˆ’2n/4.
Now pick  = 4(nâˆ’1 logn)1/2, Î» = 8âˆš2 logn and assume that n is sufficiently large
so that

(
1 +
âˆš
r/2Î»2
)
â‰¤ 1/2.
Then
P
(
Zr âˆ’ r/n â‰¥ Î»
âˆš
2r/n
)â‰¤ P(Yr âˆ’ r â‰¥ (Î»/2)
âˆš
2r
)+ nâˆ’4.
Assume now that r â‰¥ 4 logn (which means that Î» â‰¤ 4âˆš2r). Then it follows from
(2.5) that
P
(
Yr âˆ’ r â‰¥ (Î»/2)
âˆš
2r
)â‰¤ P(Yr âˆ’ r â‰¥ (Î»/4)
âˆš
2r + (Î»/4)2)â‰¤ eâˆ’Î»2/32 = nâˆ’4.
732 Found Comput Math (2009) 9: 717â€“772
Hence,
P
(
Zr âˆ’ r/n â‰¥ 16
âˆš
r logn/n
)â‰¤ 2nâˆ’4
and, therefore,
P
(
max
i
â€–P Ueiâ€–2 âˆ’ r/n â‰¥ 16
âˆš
r logn/n
)
â‰¤ 2nâˆ’3 (2.8)
by the union bound. Note that (2.8) establishes the first claim of the lemma (even for
r < 4 logn since in this case Zr â‰¤ Z4 logn).
It remains to establish the second claim. Notice that by symmetry, E =âˆ‘
1â‰¤kâ‰¤r ukvâˆ—k has the same distribution as
F =
râˆ‘
k=1
kukv
âˆ—
k,
where {k} is an independent Rademacher sequence.4 It then follows from Hoeffd-
ingâ€™s inequality that conditional on {uk} and {vk} we have
P
(|Fij | > t
)â‰¤ 2eâˆ’t2/2Ïƒ 2ij , Ïƒ 2ij =
âˆ‘
1â‰¤kâ‰¤r
u2ikv
2
ik.
As shown below, maxij |vij |2 â‰¤ (10 logn)/n with large probability, and thus
Ïƒ 2ij â‰¤ 10
logn
n
â€–P Ueiâ€–2.
Set rÌ„ = max(r, logn). Since â€–P Ueiâ€–2 â‰¤ CrÌ„/n with large probability, we have
Ïƒ 2ij â‰¤ C(logn) rÌ„/n2
with large probability. Hence, the marginal distribution of Fij obeys
P
(|Fij | > Î»
âˆš
rÌ„/n
)â‰¤ 2eâˆ’Î³ Î»2/ logn + P(Ïƒ 2ij â‰¥ C(logn)rÌ„/n2
)
.
for some numerical constant Î³ . Picking Î» = Î³ â€² logn where Î³ â€² is a sufficiently large
numerical constant gives
â€–Fâ€–âˆž â‰¤ C (logn)
âˆš
rÌ„/n
with large probability. Since E and F have the same distribution, the second claim
follows. More sophisticated arguments would provide small improvements but we
choose not to pursue these refinements here.
4A Rademacher sequence is a (possibly infinite) sequence of independent random variables taking the
values 1 and âˆ’1 with equal probability.
Found Comput Math (2009) 9: 717â€“772 733
The claim about the size of maxij |vij |2 is straightforward since our techniques
show that for each Î» > 0
P
(
Z1 â‰¥ Î»(logn)/n
)â‰¤ P(Y1 â‰¥ Î»(1 âˆ’ ) logn
)+ eâˆ’2n/4.
Moreover,
P
(
Y1 â‰¥ Î»(1 âˆ’ ) logn
)= P(|x1| â‰¥
âˆš
Î»(1 âˆ’ ) logn)â‰¤ 2eâˆ’ 12 Î»(1âˆ’) logn.
If n is sufficiently large so that  â‰¤ 1/5, this gives P(Z1 â‰¥ 10(logn)/n) â‰¤ 3nâˆ’4 and,
therefore,
P
(
max
ij
|vij |2 â‰¥ 10(logn)/n
)
â‰¤ 12nâˆ’3 logn
since the maximum is taken over at most 4n logn pairs.
3 Duality
Let R : Rn1Ã—n2 â†’ R|| be the sampling operator which extracts the observed en-
tries, R(X) = (Xij )ijâˆˆ, so that the constraint in (1.5) becomes R(X) = R(M).
Standard convex optimization theory asserts that X is a solution to (1.5) if there exists
a dual vector (or Lagrange multiplier) Î» âˆˆ R|| such that Râˆ— Î» is a subgradient of
the nuclear norm at the point X, which we denote by
Râˆ— Î» âˆˆ âˆ‚â€–Xâ€–âˆ— (3.1)
(see, e.g., [5]). Recall the definition of a subgradient of a convex function f :
R
n1Ã—n2 â†’ R. We say that Y is a subgradient of f at X0, denoted Y âˆˆ âˆ‚f (X0), if
f (X) â‰¥ f (X0) + ã€ˆY ,X âˆ’ X0ã€‰ (3.2)
for all X.
Suppose X0 âˆˆ Rn1Ã—n2 has rank r with a singular value decomposition given by
X0 =
âˆ‘
1â‰¤kâ‰¤r
Ïƒk ukv
âˆ—
k. (3.3)
With these notations, Y is a subgradient of the nuclear norm at X0 if and only if it is
of the form
Y =
âˆ‘
1â‰¤kâ‰¤r
ukv
âˆ—
k + W , (3.4)
where W obeys the following two properties:
(i) The column space of W is orthogonal to U â‰¡ span (u1, . . . ,ur ), and the row
space of W is orthogonal to V â‰¡ span (v1, . . . ,vr ).
(ii) The spectral norm of W is less than or equal to 1.
734 Found Comput Math (2009) 9: 717â€“772
(see, e.g., [24, 38]). To express these properties concisely, it is convenient to introduce
the orthogonal decomposition Rn1Ã—n2 = T âŠ•T âŠ¥ where T is the linear space spanned
by elements of the form ukxâˆ— and yvâˆ—k , 1 â‰¤ k â‰¤ r , where x and y are arbitrary, and
T âŠ¥ is its orthogonal complement. Note that dim(T ) = r(n1 + n2 âˆ’ r), precisely the
number of degrees of freedom in the set of n1 Ã— n2 matrices of rank r . T âŠ¥ is the
subspace of matrices spanned by the family (xyâˆ—), where x (respectively y) is any
vector orthogonal to U (respectively V ).
The orthogonal projection PT onto T is given by
PT (X) = P UX + XP V âˆ’ P UXP V , (3.5)
where P U and P V are the orthogonal projections onto U and V . Note here that while
P U and P V are matrices, PT is a linear operator mapping matrices to matrices. We
also have
PT âŠ¥(X) = (I âˆ’ PT )(X) = (In1 âˆ’ P U)X(In2 âˆ’ P V )
where I d denotes the d Ã— d identity matrix. With these notations, Y âˆˆ âˆ‚â€–X0â€–âˆ— if
(iâ€²) PT (Y ) =âˆ‘1â‰¤kâ‰¤r ukvâˆ—k , and
(iiâ€²) â€–PT âŠ¥Yâ€– â‰¤ 1.
Now that we have characterized the subgradient of the nuclear norm, the lemma
below gives sufficient conditions for the uniqueness of the minimizer to (1.5).
Lemma 3.1 Consider a matrix X0 =âˆ‘rk=1 Ïƒk ukvâˆ—k of rank r which is feasible for
the problem (1.5), and suppose that the following two conditions hold:
1. There exists a dual point Î» such that Y = Râˆ—Î» obeys
PT (Y ) =
râˆ‘
k=1
ukv
âˆ—
k,
âˆ¥âˆ¥PT âŠ¥(Y )
âˆ¥âˆ¥< 1. (3.6)
2. The sampling operator R restricted to elements in T is injective.
Then X0 is the unique minimizer.
Before proving this result, we would like to emphasize that this lemma pro-
vides a clear strategy for proving our main result, namely Theorem 1.3. Letting
M = âˆ‘rk=1 Ïƒk ukvâˆ—k , M is the unique solution to (1.5) if the injectivity condition
holds and if one can find a dual point Î» such that Y = Râˆ—Î» obeys (3.6).
The proof of Lemma 3.1 uses a standard fact which states that the nuclear norm
and the spectral norm are dual to one another.
Lemma 3.2 For each pair W and H , we have
ã€ˆW ,H ã€‰ â‰¤ â€–Wâ€–â€–Hâ€–âˆ—.
In addition, for each H , there is a W obeying â€–Wâ€– = 1 which achieves the equality.
Found Comput Math (2009) 9: 717â€“772 735
A variety of proofs are available for this lemma, and an elementary argument is
sketched in [29]. We now turn to the proof of Lemma 3.1.
Proof of Lemma 3.1 Consider any perturbation X0 + H where R(H ) = 0. Then
for any W 0 obeying (i)â€“(ii),
âˆ‘r
k=1 ukvâˆ—k + W 0 is a subgradient of the nuclear norm
at X0 and, therefore,
â€–X0 + Hâ€–âˆ— â‰¥ â€–X0â€–âˆ— +
âŒ©
râˆ‘
k=1
ukv
âˆ—
k + W 0,H
âŒª
.
Letting W = PT âŠ¥(Y ), we may write
âˆ‘r
k=1 ukvâˆ—k = Râˆ—Î» âˆ’ W . Since â€–Wâ€– < 1 and
R(H ) = 0, it then follows that
â€–X0 + Hâ€–âˆ— â‰¥ â€–X0â€–âˆ— +
âŒ©
W 0 âˆ’ W ,H âŒª.
Now, by construction,
âŒ©
W 0 âˆ’ W ,H âŒª= âŒ©PT âŠ¥
(
W 0 âˆ’ W ),H âŒª= âŒ©W 0 âˆ’ W , PT âŠ¥(H )
âŒª
.
We use Lemma 3.2 and set W 0 = PT âŠ¥(Z) where Z is any matrix obeying â€–Zâ€– â‰¤ 1
and ã€ˆZ, PT âŠ¥(H )ã€‰ = â€–PT âŠ¥(H )â€–âˆ—. Then W 0 âˆˆ T âŠ¥, â€–W 0â€– â‰¤ 1, and
âŒ©
W 0 âˆ’ W ,H âŒªâ‰¥ (1 âˆ’ â€–Wâ€–)âˆ¥âˆ¥PT âŠ¥(H )
âˆ¥âˆ¥âˆ—,
which by assumption is strictly positive unless PT âŠ¥(H ) = 0. In other words, â€–X0 +
Hâ€–âˆ— > â€–X0â€–âˆ— unless PT âŠ¥(H ) = 0. Assume then that PT âŠ¥(H ) = 0 or equivalently
that H âˆˆ T . Then R(H ) = 0 implies that H = 0 by the injectivity assumption. In
conclusion, â€–X0 + Hâ€–âˆ— > â€–X0â€–âˆ— unless H = 0. 
4 Architecture of the Proof
Our strategy to prove that M =âˆ‘1â‰¤kâ‰¤r Ïƒkukvâˆ—k is the unique minimizer to (1.5) is to
construct a matrix Y which vanishes on c and obeys the conditions of Lemma 3.1
(and show the injectivity of the sampling operator restricted to matrices in T along
the way). Set P to be the orthogonal projector onto the indices in  so that the
(i, j)th component of P(X) is equal to Xij if (i, j) âˆˆ  and zero otherwise. Our
candidate Y will be the solution to
minimize â€–Xâ€–F
subject to (PT P)(X) =
râˆ‘
k=1
ukv
âˆ—
k.
(4.1)
The matrix Y vanishes on c as otherwise it would not be an optimal solution since
P(Y ) would obey the constraint and have a smaller Frobenius norm. Hence, Y =
736 Found Comput Math (2009) 9: 717â€“772
P(Y ) and PT (Y ) =âˆ‘rk=1 ukvâˆ—k . Since the Pythagoras formula gives
â€–Yâ€–2F =
âˆ¥âˆ¥PT (Y )
âˆ¥âˆ¥2
F
+ âˆ¥âˆ¥PT âŠ¥(Y )
âˆ¥âˆ¥2
F
=
âˆ¥âˆ¥âˆ¥âˆ¥âˆ¥
râˆ‘
k=1
ukv
âˆ—
k
âˆ¥âˆ¥âˆ¥âˆ¥âˆ¥
2
F
+ âˆ¥âˆ¥PT âŠ¥(Y )
âˆ¥âˆ¥2
F
= r + âˆ¥âˆ¥PT âŠ¥(Y )
âˆ¥âˆ¥2
F
,
minimizing the Frobenius norm of X amounts to minimizing the Frobenius norm of
PT âŠ¥(X) under the constraint PT (X) =
âˆ‘r
k=1 ukvâˆ—k . Our motivation is twofold. First,
the solution to the least-squares problem (4.1) has a closed form that is amenable to
analysis. Second, by forcing PT âŠ¥(Y ) to be small in the Frobenius norm, we hope
that it will be small in the spectral norm as well, and establishing that â€–PT âŠ¥(Y )â€– < 1
would prove that M is the unique solution to (1.5).
To compute the solution to (4.1), we introduce the operator AT defined by
AT (M) = PPT (M).
Then if Aâˆ—T AT = PT PPT has full rank when restricted to T , the minimizer to
(4.1) is given by
Y = AT
(
Aâˆ—T AT
)âˆ’1
(E), E â‰¡
râˆ‘
k=1
ukv
âˆ—
k. (4.2)
We clarify the meaning of (4.2) to avoid any confusion. (Aâˆ—T AT )âˆ’1(E) is meant
to be that element F in T obeying (Aâˆ—T AT )(F ) = E.
To summarize the aims of our proof strategy,
â€¢ We must first show that Aâˆ—T AT = PT PPT is a one-to-one linear mapping
from T onto itself. In this case, AT = PPT â€”as a mapping from T to Rn1Ã—n2 â€”
is injective. This is the second sufficient condition of Lemma 3.1. Moreover, our
ansatz for Y given by (4.2) is well defined.
â€¢ Having established that Y is well defined, we will show that
âˆ¥âˆ¥PT âŠ¥(Y )
âˆ¥âˆ¥< 1,
thus proving the first sufficient condition.
4.1 The Bernoulli Model
Instead of showing that the theorem holds when  is a set of size m sampled uni-
formly at random, we prove the theorem for a subset â€² sampled according to the
Bernoulli model. Here and below, {Î´ij }1â‰¤iâ‰¤n1,1â‰¤jâ‰¤n2 is a sequence of independent
identically distributed 0/1 Bernoulli random variables with
P(Î´ij = 1) = p â‰¡ m
n1n2
, (4.3)
Found Comput Math (2009) 9: 717â€“772 737
and define
â€² = {(i, j) : Î´ij = 1
}
. (4.4)
Note that E |â€²| = m, so that the average cardinality of â€² is that of . Then follow-
ing the same reasoning as the argument developed in Sect. II.C of [10] shows that
the probability of â€œfailureâ€ under the uniform model is bounded by 2 times the prob-
ability of failure under the Bernoulli model; the failure event is the event on which
the solution to (1.5) is not exact. Hence, we can restrict our attention to the Bernoulli
model and from now on, we will assume that  is given by (4.4). This is advanta-
geous because the Bernoulli model admits a simpler analysis than uniform sampling
thanks to the independence between the Î´ij â€™s.
4.2 The Injectivity Property
We study the injectivity of AT , which also shows that Y is well defined. To prove
this, we will show that the linear operator pâˆ’1 PT (P âˆ’ pI)PT has small operator
norm, which we recall is supâ€–Xâ€–F â‰¤1 p
âˆ’1â€–PT (P âˆ’ pI)PT (X)â€–F .
Theorem 4.1 Suppose  is sampled according to the Bernoulli model (4.3)â€“(4.4)
and put n = max(n1, n2). Suppose that the coherences obey max(Î¼(U),Î¼(V )) â‰¤ Î¼0.
Then there is a numerical constant CR such that for all Î² > 1,
pâˆ’1 â€–PT PPT âˆ’ pPT â€– â‰¤ CR
âˆš
Î¼0 nr(Î² logn)
m
(4.5)
with probability at least 1 âˆ’ 3nâˆ’Î² provided that CR
âˆš
Î¼0 nr(Î² logn)
m
< 1.
Proof Decompose any matrix X as X =âˆ‘abã€ˆX, eaeâˆ—bã€‰eaeâˆ—b so that
PT (X) =
âˆ‘
ab
âŒ©
PT (X), eaeâˆ—b
âŒª
eae
âˆ—
b =
âˆ‘
ab
âŒ©
X, PT
(
eae
âˆ—
b
)âŒª
eae
âˆ—
b.
Hence, PPT (X) =âˆ‘ab Î´ab ã€ˆX, PT (eaeâˆ—b)ã€‰ eaeâˆ—b which gives
(PT PPT )(X) =
âˆ‘
ab
Î´ab
âŒ©
X, PT
(
eae
âˆ—
b
)âŒª
PT
(
eae
âˆ—
b
)
.
In other words,
PT PPT =
âˆ‘
ab
Î´ab PT
(
eae
âˆ—
b
)âŠ— PT
(
eae
âˆ—
b
)
,
where x âŠ— y, in which x and y belong to some Hilbert space H , is the outer product
defined as (x âŠ— y)(z) = xã€ˆy,zã€‰H . It follows from the definition (3.5) of PT that
PT
(
eae
âˆ—
b
)= (P Uea)eâˆ—b + ea(P V eb)âˆ— âˆ’ (P Uea)(P V eb)âˆ—. (4.6)
738 Found Comput Math (2009) 9: 717â€“772
This gives
âˆ¥âˆ¥PT
(
eae
âˆ—
b
)âˆ¥âˆ¥2
F
= âŒ©PT
(
eae
âˆ—
b
)
, eae
âˆ—
b
âŒª= â€–P Ueaâ€–2 + â€–P V ebâ€–2 âˆ’ â€–P Ueaâ€–2 â€–P V ebâ€–2
(4.7)
and since â€–P Ueaâ€–2 â‰¤ Î¼(U)r/n1 and â€–P V ebâ€–2 â‰¤ Î¼(V )r/n2,
âˆ¥âˆ¥PT
(
eae
âˆ—
b
)âˆ¥âˆ¥2
F
â‰¤ 2Î¼0r/min(n1, n2). (4.8)
Now, the fact that the operator PT PPT does not deviate from its expected value
E(PT PPT ) = PT (E P)PT = PT (pI)PT = pPT
in the spectral norm is related to Rudelsonâ€™s selection theorem [31]. The first part of
the theorem below may be found in [9], for example, see also [32] for a very similar
statement.
Theorem 4.2 [9] Let {Î´ab} be independent 0/1 Bernoulli variables with P(Î´ab =1)=
p = m
n1n2
and put n = max(n1, n2). Suppose that â€–PT (eaeâˆ—b)â€–2F â‰¤ 2Î¼0r/n. Set
Z â‰¡ pâˆ’1
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
ab
(Î´ab âˆ’ p) PT
(
eae
âˆ—
b
)âŠ— PT
(
eae
âˆ—
b
)âˆ¥âˆ¥âˆ¥âˆ¥= pâˆ’1â€–PT PPT âˆ’ pPT â€–.
1. There exists a constant Câ€²R such that
EZ â‰¤ Câ€²R
âˆš
Î¼0 nr logn
m
(4.9)
provided that the right-hand side is smaller than 1.
2. Suppose EZ â‰¤ 1. Then for each Î» > 0, we have
P
(
|Z âˆ’ EZ| > Î»
âˆš
Î¼0 nr logn
m
)
â‰¤ 3 exp
(
âˆ’Î³ â€²0 min
{
Î»2 logn,Î»
âˆš
m logn
Î¼0 nr
})
(4.10)
for some positive constant Î³ â€²0.
As mentioned above, the first part, namely (4.9) is an application of an established
result which states that if {yi} is a family of vectors in Rd and {Î´i} is a 0/1 Bernoulli
sequence with P(Î´i = 1) = p, then
pâˆ’1
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
i
(Î´i âˆ’ p)yi âŠ— yi
âˆ¥âˆ¥âˆ¥âˆ¥â‰¤ C
âˆš
logd
p
max
i
â€–yiâ€–
Found Comput Math (2009) 9: 717â€“772 739
for some C > 0 provided that the right-hand side is less than 1. The proof may be
found in the cited literature, e.g., in [9]. Hence, the first part follows from apply-
ing this result to matrices of the form PT (eaeâˆ—b) and using the available bound onâ€–PT (eaeâˆ—b)â€–F . The second part follows from Talagrandâ€™s concentration inequality
and may be found in the Appendix.
Set Î» =
âˆš
Î²/Î³ â€²0 and assume that m > (Î²/Î³ â€²0)Î¼0 nr logn. Then the left-hand side
of (4.10) is bounded by 3nâˆ’Î², and thus we established that
Z â‰¤ Câ€²R
âˆš
Î¼0 nr logn
m
+ 1âˆš
Î³ â€²0
âˆš
Î¼0 nr Î² logn
m
with probability at least 1 âˆ’ 3nâˆ’Î² . Setting CR = Câ€²R + 1/
âˆš
Î³ â€²0 finishes the proof. 
Take m large enough so that CR
âˆš
Î¼0 (nr/m) logn â‰¤ 1/2. Then it follows from
(4.5) that
p
2
âˆ¥âˆ¥PT (X)
âˆ¥âˆ¥
F
â‰¤ âˆ¥âˆ¥(PT PPT )(X)
âˆ¥âˆ¥
F
â‰¤ 3p
2
âˆ¥âˆ¥PT (X)
âˆ¥âˆ¥
F
(4.11)
for all X with large probability. In particular, the operator Aâˆ—T AT = PT PPT
mapping T onto itself is well conditioned, and hence invertible. An immediate con-
sequence is the following corollary.
Corollary 4.3 Assume that CR
âˆš
Î¼0nr(logn)/m â‰¤ 1/2. With the same probability
as in Theorem 4.1, we have
âˆ¥âˆ¥PPT (X)
âˆ¥âˆ¥
F
â‰¤âˆš3p/2âˆ¥âˆ¥PT (X)
âˆ¥âˆ¥
F
. (4.12)
Proof We have â€–PPT (X)â€–2F = ã€ˆX, (PPT )âˆ—(PPT )Xã€‰ = ã€ˆX, (PT PPT )Xã€‰,
and thus
âˆ¥âˆ¥PPT (X)
âˆ¥âˆ¥2
F
= âŒ©PT (X), (PT PPT )(X)
âŒªâ‰¤ âˆ¥âˆ¥PT (X)
âˆ¥âˆ¥
F
âˆ¥âˆ¥(PT PPT )(X)
âˆ¥âˆ¥
F
,
where the inequality is due to Cauchyâ€“Schwarz. The conclusion (4.12) follows
from (4.11). 
4.3 The Size Property
In this section, we explain how we will show that â€–PT âŠ¥(Y )â€– < 1. This result will
follow from five lemmas that we will prove in Sect. 6. Introduce
H â‰¡ PT âˆ’ pâˆ’1 PT PPT ,
which obeys â€–H(X)â€–F â‰¤ CR
âˆš
Î¼0(nr/m)Î² lognâ€–PT (X)â€–F with large probability
because of Theorem 4.1. For any matrix X âˆˆ T , (PT PPT )âˆ’1(X) can be expressed
in terms of the power series
(PT PPT )âˆ’1(X) = pâˆ’1
(
X + H(X) + H2(X) + Â· Â· Â· )
740 Found Comput Math (2009) 9: 717â€“772
for H is a contraction when m is sufficiently large. Since Y = PPT (PT PPT )âˆ’1 Ã—
(
âˆ‘
1â‰¤kâ‰¤r ukvâˆ—k), PT âŠ¥(Y ) may be decomposed as
PT âŠ¥(Y ) = pâˆ’1(PT âŠ¥ PPT )
(
E + H(E) + H2(E) + Â· Â· Â· ), E =
âˆ‘
1â‰¤kâ‰¤r
ukv
âˆ—
k.
(4.13)
To bound the norm of the left-hand side, it is of course sufficient to bound the norm
of the summands in the right-hand side. Taking the following five lemmas together
establishes Theorem 1.3.
Lemma 4.4 Fix Î² â‰¥ 2 and Î» â‰¥ 1. There is a numerical constant C0 such that if
m â‰¥ Î»Î¼21 nrÎ² logn, then
pâˆ’1
âˆ¥âˆ¥(PT âŠ¥ PPT )E
âˆ¥âˆ¥â‰¤ C0 Î»âˆ’1/2 (4.14)
with probability at least 1 âˆ’ nâˆ’Î² .
Lemma 4.5 Fix Î² â‰¥ 2 and Î» â‰¥ 1. There are numerical constants C1 and c1 such that
if m â‰¥ Î»Î¼1 max(âˆšÎ¼0,Î¼1) nrÎ² logn, then
pâˆ’1
âˆ¥âˆ¥(PT âŠ¥ PPT )H(E)
âˆ¥âˆ¥â‰¤ C1 Î»âˆ’1 (4.15)
with probability at least 1 âˆ’ c1nâˆ’Î² .
Lemma 4.6 Fix Î² â‰¥ 2 and Î» â‰¥ 1. There are numerical constants C2 and c2 such that
if m â‰¥ Î»Î¼4/30 nr4/3Î² logn, then
pâˆ’1
âˆ¥âˆ¥(PT âŠ¥ PPT )H2(E)
âˆ¥âˆ¥â‰¤ C2 Î»âˆ’3/2 (4.16)
with probability at least 1 âˆ’ c2nâˆ’Î² .
Lemma 4.7 Fix Î² â‰¥ 2 and Î» â‰¥ 1. There are numerical constants C3 and c3 such that
if m â‰¥ Î»Î¼20 nr2Î² logn, then
pâˆ’1
âˆ¥âˆ¥(PT âŠ¥ PPT )H3(E)
âˆ¥âˆ¥â‰¤ C3 Î»âˆ’1/2 (4.17)
with probability at least 1 âˆ’ c3nâˆ’Î² .
Lemma 4.8 Under the assumptions of Theorem 4.1, there is a numerical constant
Ck0 such that if m â‰¥ (2CR)2Î¼0nrÎ² logn, then
pâˆ’1
âˆ¥âˆ¥âˆ¥âˆ¥(PT âŠ¥ PPT )
âˆ‘
kâ‰¥k0
Hk(E)
âˆ¥âˆ¥âˆ¥âˆ¥â‰¤ Ck0
(
n2r
m
)1/2(
Î¼0nrÎ² logn
m
)k0/2
(4.18)
with probability at least 1 âˆ’ 3nâˆ’Î² .
Found Comput Math (2009) 9: 717â€“772 741
Let us now show how we may combine these lemmas to prove our main results.
Under all of the assumptions of Theorem 1.3, consider the four Lemmas 4.4, 4.5, 4.6,
and 4.8, the latter applied with k0 = 3. Together they imply that there are numeri-
cal constants c and C such that â€–PT âŠ¥(Y )â€– < 1 with probability at least 1 âˆ’ cnâˆ’Î²
provided that the number of samples obeys
m â‰¥ C max(Î¼21,Î¼1/20 Î¼1,Î¼4/30 r1/3,Î¼0n1/4
)
nrÎ² logn (4.19)
for some constant C. The four expressions in the maximum come from Lemmas 4.4,
4.5, 4.6, and 4.8 in this order. Now, the bound (4.19) is only interesting in the range
when Î¼0n1/4r is smaller than a constant times n as otherwise the right-hand side
is greater than n2 (this would say that one would see all the entries in which case
our claim is trivial). When Î¼0r â‰¤ n3/4, (Î¼0r)4/3 â‰¤ Î¼0n5/4r, and thus the recovery is
exact provided that m obeys (1.9).
For the case concerning small values of the rank, we consider all five lemmas
and apply Lemma 4.8, the latter applied with k0 = 4. Together they imply that
â€–PT âŠ¥(Y )â€– < 1 with probability at least 1 âˆ’ cnâˆ’Î² provided that the number of sam-
ples obeys
m â‰¥ C max(Î¼20r,Î¼0n1/5
)
nrÎ² logn (4.20)
for some constant C. The two expressions in the maximum come from Lemmas 4.7
and 4.8 in this order. The reason for this simplified formulation is that the terms Î¼21,
Î¼
1/2
0 Î¼1 and Î¼
4/3
0 r
1/3 which come from Lemmas 4.4, 4.5, and 4.6 are bounded above
by Î¼20r since Î¼1 â‰¤ Î¼0
âˆš
r . When Î¼0r â‰¤ n1/5, the recovery is exact provided that m
obeys (1.10).
5 Connections with Random Graph Theory
5.1 The Injectivity Property and the Coupon Collectorâ€™s Problem
We argued in the Introduction that to have any hope of recovering an unknown matrix
of rank 1 by any method whatsoever, one needs at least one observation per row and
one observation per column. Sample m entries uniformly at random. Viewing the
row indices as bins, assign the kth sampled entry to the bin corresponding to its
row index. Then to have any hope of recovering our matrix, all the bins need to be
occupied. Quantifying how many samples are required to fill all of the bins is the
famous coupon collectorâ€™s problem.
Coupon collection is also connected to the injectivity of the sampling operator P
restricted to elements in T . Suppose we sample the entries of a rank 1 matrix equal
to xyâˆ— with left and right singular vectors u = x/â€–xâ€– and v = y/â€–yâ€–, respectively,
and have not seen anything in the ith row. Then we claim that P (restricted to T )
has a nontrivial null space, and thus PT PPT is not invertible. Indeed, consider the
matrix eivâˆ—. This matrix is in T and
P
(
eiv
âˆ—)= 0
742 Found Comput Math (2009) 9: 717â€“772
since eivâˆ— vanishes outside of the ith row. The same applies to the columns as well.
If we have not seen anything in column j , then the rank-1 matrix ueâˆ—j âˆˆ T and
P(ueâˆ—j ) = 0. In conclusion, the invertibility of PT PPT implies a complete col-
lection.
When the entries are sampled uniformly at random, it is well known that one
needs on the order of n logn samples to sample all the rows. What is interesting is
that Theorem 4.1 implies that PT PPT is invertibleâ€”a stronger propertyâ€”when
the number of samples is also on the order of n logn. A particular implication of this
discussion is that the logarithmic factors in Theorem 4.1 are unavoidable.
5.2 The Injectivity Property and the Connectivity Problem
To recover a matrix of rank 1, one needs much more than at least one observation
per row and column. Let R be the set of row indices, 1 â‰¤ i â‰¤ n, and C be the set of
column indices, 1 â‰¤ j â‰¤ n, and consider the bipartite graph connecting vertices i âˆˆ R
to vertices j âˆˆ C if and only if (i, j) âˆˆ , i.e., the (i, j)th entry is observed. We claim
that if this graph is not fully connected, then one cannot hope to recover a matrix of
rank 1.
To see this, we let I be the set of row indices and J be the set of column indices in
any connected component. We will assume that I and J are nonempty as otherwise,
one is in the previously discussed situation where some rows or columns are not
sampled. Consider a rank 1 matrix equal to xyâˆ— as before with singular vectors u =
x/â€–xâ€– and v = y/â€–yâ€–. Then all the information about the values of the xi â€™s with
i âˆˆ I and of the yj â€™s with j âˆˆ J are given by the sampled entries connecting I to
J since all the other observed entries connect vertices in I c to those in J c. Now
even if one observes all the entries xiyj with i âˆˆ I and j âˆˆ J , then at least the signs
of xi , i âˆˆ I , and of yj , j âˆˆ J , would remain undetermined. Indeed, if the values
(xi)iâˆˆI , (yj )jâˆˆJ are consistent with the observed entries, so are the values (âˆ’xi)iâˆˆI ,
(âˆ’yj )jâˆˆJ . However, since the same analysis holds for the sets I c and J c , there are at
least two matrices consistent with the observed entries and exact matrix completion
is impossible.
The connectivity of the graph is also related to the injectivity of the sampling
operator P restricted to elements in T . If the graph is not fully connected, then we
claim that P (restricted to T ) has a nontrivial null space, and thus PT PPT is not
invertible. Indeed, consider the matrix
M = avâˆ— + ubâˆ—,
where ai = âˆ’ui if i âˆˆ I and ai = ui otherwise, and bj = vj if j âˆˆ J and bj = âˆ’vj
otherwise. Then this matrix is in T and obeys
Mij = 0
if (i, j) âˆˆ I Ã— J or (i, j) âˆˆ I c Ã— J c . Note that on the complement, i.e., (i, j) âˆˆ I Ã— J c
or (i, j) âˆˆ I c Ã—J , one has Mij = 2uivj and one can show that M = 0 unless uvâˆ— = 0.
Since  is included in the union of I Ã— J and I c Ã— J c , we have that P(M) = 0. In
conclusion, the invertibility of PT PPT implies a fully connected graph.
Found Comput Math (2009) 9: 717â€“772 743
When the entries are sampled uniformly at random, it is well known that one needs
on the order of n logn samples to obtain a fully connected graph with large probabil-
ity (see, e.g., [6]). Remarkably, Theorem 4.1 implies that PT PPT is invertibleâ€”a
stronger propertyâ€”when the number of samples is also on the order of n logn.
6 Proofs of the Critical Lemmas
In this section, we prove the five lemmas of Sect. 4.3. Before we begin, however, we
develop a simple estimate which we will use throughout. For each pair (a, b) and
(aâ€², bâ€²), it follows from the expression of PT (eaeâˆ—b) (4.6) that
âŒ©
PT
(
eaâ€²e
âˆ—
bâ€²
)
, eae
âˆ—
b
âŒª = ã€ˆea,P Ueaâ€² ã€‰1{b=bâ€²} + ã€ˆeb,P V ebâ€² ã€‰1{a=aâ€²}
âˆ’ ã€ˆea,P Ueaâ€² ã€‰ã€ˆeb,P V ebâ€² ã€‰. (6.1)
Fix Î¼0 obeying Î¼(U) â‰¤ Î¼0 and Î¼(V ) â‰¤ Î¼0 and note that
âˆ£âˆ£ã€ˆea,P Ueaâ€² ã€‰
âˆ£âˆ£= âˆ£âˆ£ã€ˆP Uea,P Ueaâ€² ã€‰
âˆ£âˆ£â‰¤ â€–P Ueaâ€–â€–P Ueaâ€² â€– â‰¤ Î¼0r/n1
and similarly for ã€ˆeb,P V ebâ€² ã€‰. Suppose that b = bâ€² and a = aâ€², then
âˆ£âˆ£âŒ©PT
(
eaâ€²e
âˆ—
bâ€²
)
, eae
âˆ—
b
âŒªâˆ£âˆ£= âˆ£âˆ£ã€ˆea,P Ueaâ€² ã€‰
âˆ£âˆ£(1 âˆ’ â€–P V ebâ€–2
)â‰¤ Î¼0r/n1.
We have a similar bound when a = aâ€² and b = bâ€² whereas when a = aâ€² and b = bâ€²,
âˆ£âˆ£âŒ©PT
(
eaâ€²e
âˆ—
bâ€²
)
, eae
âˆ—
b
âŒªâˆ£âˆ£â‰¤ (Î¼0r)2/(n1n2).
In short, it follows from this analysis (and from (4.8) for the case where (a, b) =
(aâ€², bâ€²)) that
max
ab,aâ€²bâ€²
âˆ£âˆ£âŒ©PT
(
eaâ€²e
âˆ—
bâ€²
)
, eae
âˆ—
b
âŒªâˆ£âˆ£â‰¤ 2Î¼0r/min(n1, n2). (6.2)
A consequence of (4.8) is the estimate:
âˆ‘
aâ€²bâ€²
âˆ£âˆ£âŒ©PT
(
eaâ€²e
âˆ—
bâ€²
)
, eae
âˆ—
b
âŒªâˆ£âˆ£2 =
âˆ‘
aâ€²bâ€²
âˆ£âˆ£âŒ©PT
(
eae
âˆ—
b
)
, eaâ€²e
âˆ—
bâ€²
âŒªâˆ£âˆ£2
= âˆ¥âˆ¥PT
(
eae
âˆ—
b
)âˆ¥âˆ¥2
F
â‰¤ 2Î¼0r/min(n1, n2), (6.3)
which we will apply several times. A related estimate is this:
max
a
âˆ‘
b
|Eab|2 â‰¤ Î¼0r/min(n1, n2), (6.4)
and the same is true by exchanging the role of a and b. To see this, write
âˆ‘
b
|Eab|2 =
âˆ¥âˆ¥eâˆ—aE
âˆ¥âˆ¥2 =
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
jâ‰¤r
vj ã€ˆuj , eaã€‰
âˆ¥âˆ¥âˆ¥âˆ¥
2
=
âˆ‘
jâ‰¤r
âˆ£âˆ£ã€ˆuj , eaã€‰
âˆ£âˆ£2 = â€–P Ueaâ€–2,
and the conclusion follows from the coherence property.
744 Found Comput Math (2009) 9: 717â€“772
We will prove the lemmas in the case where n1 = n2 = n for simplicity, i.e., in
the case of square matrices of dimension n. The general case is treated in exactly the
same way. In fact, the argument only makes use of the bounds (6.2), (6.3), and (6.4),
and the general case is obtained by replacing n with min(n1, n2).
Each of the following subsections computes the operator norm of some random
variable. In each section, we denote S as the quantity whose norm we wish to analyze.
We will also frequently use the notation H for some auxiliary matrix variable whose
norm we will need to bound. Hence, we will reuse the same notation many times
rather than introducing a dozen new namesâ€”just as in computer programming where
one uses the same variable name in distinct routines.
6.1 Proof of Lemma 4.4
In this section, we develop a bound on
pâˆ’1
âˆ¥âˆ¥PT âŠ¥ PPT (E)
âˆ¥âˆ¥= pâˆ’1âˆ¥âˆ¥PT âŠ¥(P âˆ’ pI)PT (E)
âˆ¥âˆ¥
â‰¤ pâˆ’1âˆ¥âˆ¥(P âˆ’ pI)(E)
âˆ¥âˆ¥,
where the equality follows from PT âŠ¥ PT = 0, and the inequality from PT (E) = E
together with â€–PT âŠ¥(X)â€– â‰¤ â€–Xâ€– which is valid for any matrix X. Set
S â‰¡ pâˆ’1(P âˆ’ pI)(E) = pâˆ’1
âˆ‘
ab
(Î´ab âˆ’ p)Eabeaeâˆ—b. (6.5)
We think of S as a random variable since it depends on the random Î´abâ€™s, and note
that ES = 0.
The proof of Lemma 4.4 operates by developing an estimate on the size of
(Eâ€–Sâ€–q)1/q for some q â‰¥ 1 and by applying Markov inequality to bound the tail of
the random variable â€–Sâ€–. To do this, we shall use a symmetrization argument and the
noncommutative Khintchine inequality. Since the function f (S) = â€–Sâ€–q is convex,
Jensenâ€™s inequality gives that
Eâ€–Sâ€–q â‰¤ Eâˆ¥âˆ¥S âˆ’ Sâ€²âˆ¥âˆ¥q,
where Sâ€² = pâˆ’1âˆ‘ab(Î´â€²ab âˆ’p)Eabeaeâˆ—b is an independent copy of S. Since (Î´ab âˆ’Î´â€²ab)
is symmetric, S âˆ’ Sâ€² has the same distribution as
pâˆ’1
âˆ‘
ab
ab
(
Î´ab âˆ’ Î´â€²ab
)
Eabeae
âˆ—
b â‰¡ S âˆ’ Sâ€²,
where {ab} is an independent Rademacher sequence and S = pâˆ’1âˆ‘ab abÎ´abEab Ã—
eae
âˆ—
b . Further, the triangle inequality gives
(
E
âˆ¥âˆ¥S âˆ’ Sâ€²
âˆ¥âˆ¥q)1/q â‰¤ (Eâ€–Sâ€–q
)1/q + (Eâˆ¥âˆ¥Sâ€²
âˆ¥âˆ¥q)1/q = 2(Eâ€–Sâ€–q
)1/q
since S and Sâ€² have the same distribution and, therefore,
(
Eâ€–Sâ€–q)1/q â‰¤ 2pâˆ’1
(
EÎ´ E
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
ab
abÎ´ab Eabeae
âˆ—
b
âˆ¥âˆ¥âˆ¥âˆ¥
q)1/q
.
Found Comput Math (2009) 9: 717â€“772 745
We are now in position to apply the noncommutative Khintchine inequality which
bounds the Schatten norm of a Rademacher series. Recall from Sect. 1.5 that for
q â‰¥ 1, the Schatten q-norm of a matrix is the quantity (1.16). The following theo-
rem was originally proven by Lustâ€“Picquard [26], and was later sharpened by Buch-
holz [7].
Lemma 6.1 (Noncommutative Khintchine inequality) Let (Xi )1â‰¤iâ‰¤r be a finite se-
quence of matrices of the same dimension and let {i} be a Rademacher sequence.
For each q â‰¥ 2,
[
E
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
i
iXi
âˆ¥âˆ¥âˆ¥âˆ¥
q
Sq
]1/q
â‰¤ CK âˆšq max
[âˆ¥âˆ¥âˆ¥âˆ¥
(âˆ‘
i
Xâˆ—i Xi
)1/2âˆ¥âˆ¥âˆ¥âˆ¥
Sq
,
âˆ¥âˆ¥âˆ¥âˆ¥
(âˆ‘
i
XiX
âˆ—
i
)1/2âˆ¥âˆ¥âˆ¥âˆ¥
Sq
]
,
where CK = 2âˆ’1/4âˆšÏ€/e.
For reference, if X is an n Ã— n matrix and q â‰¥ logn, we have
â€–Xâ€– â‰¤ â€–Xâ€–Sq â‰¤ eâ€–Xâ€–,
so that the Schatten q-norm is within a multiplicative constant from the operator
norm. Observe now that with q â€² â‰¥ q
(
EÎ´ E â€–Sâ€–q
)1/q â‰¤ (EÎ´ E â€–Sâ€–qSqâ€²
)1/q â‰¤ (EÎ´ E â€–Sâ€–q
â€²
Sqâ€²
)1/q â€²
.
We apply the noncommutative Khintchine inequality with q â€² â‰¥ logn, and, after a
little algebra, obtain
(
EÎ´ E â€–Sâ€–q
â€²
Sqâ€²
)1/q â€²
â‰¤ CK e
âˆš
q â€²
p
(
EÎ´ max
[âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
ab
Î´abE
2
abeae
âˆ—
a
âˆ¥âˆ¥âˆ¥âˆ¥
q â€²/2
,
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
ab
Î´abE
2
abebe
âˆ—
b
âˆ¥âˆ¥âˆ¥âˆ¥
q â€²/2])1/q â€²
.
The two terms in the right-hand side are essentially the same and if we can bound
any one of them, the same technique will apply to the other. We consider the first and
since
âˆ‘
ab Î´abE
2
abeae
âˆ—
a is a diagonal matrix,
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
ab
Î´abE
2
abeae
âˆ—
a
âˆ¥âˆ¥âˆ¥âˆ¥= maxa
âˆ‘
b
Î´abE
2
ab.
The following lemma bounds the qth moment of this quantity.
746 Found Comput Math (2009) 9: 717â€“772
Lemma 6.2 Suppose that q is an integer obeying 1 â‰¤ q â‰¤ np and assume np â‰¥
2 logn. Then
EÎ´
(
max
a
âˆ‘
b
Î´abE
2
ab
)q
â‰¤ 2 (2np â€–Eâ€–2âˆž
)q
. (6.6)
The proof of this lemma is in the Appendix. The same estimate applies to
E(maxb
âˆ‘
a Î´abE
2
ab)
q, and thus for each q â‰¥ 1
EÎ´ max
[âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
ab
Î´abE
2
abeae
âˆ—
a
âˆ¥âˆ¥âˆ¥âˆ¥
q
,
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
ab
Î´abE
2
abebe
âˆ—
b
âˆ¥âˆ¥âˆ¥âˆ¥
q]
â‰¤ 4 (2np â€–Eâ€–2âˆž
)q
.
(In the rectangular case, the same estimate holds with n = max(n1, n2).)
Take q = Î² logn for some Î² â‰¥ 1, and set q â€² = q . Then since â€–Eâ€–âˆž â‰¤ Î¼1âˆšr/n,
we established that
(
Eâ€–Sâ€–q)1/q â‰¤ C 1
p
âˆš
Î² logn
âˆš
np â€–Eâ€–âˆž = C Î¼1
âˆš
nr Î² logn
m
â‰¡ K0.
Then by Markovâ€™s inequality, for each t > 0,
P
(â€–Sâ€– > tK0
)â‰¤ tâˆ’q,
and for t = e, we conclude that
P
(
â€–Sâ€– > CeÎ¼1
âˆš
nr Î² logn
m
)
â‰¤ nâˆ’Î²
with the proviso that m â‰¥ max(Î²,2) n logn so that Lemma 6.2 holds.
We have not made any assumption in this section about the matrix E (except that
we have a bound on the maximum entry) and, therefore, have proved the theorem
below, which shall be used many times in the sequel.
Theorem 6.3 Let X be a fixed n Ã— n matrix. There is a constant C0 such that for
each Î² > 2
pâˆ’1
âˆ¥âˆ¥(P âˆ’ pI)(X)
âˆ¥âˆ¥â‰¤ C0
(
Î²n logn
p
)1/2
â€–Xâ€–âˆž (6.7)
with probability at least 1 âˆ’ nâˆ’Î² provided that np â‰¥ Î² logn.
Note that this is the same C0 described in Lemma 4.4.
6.2 Proof of Lemma 4.5
We now need to bound the spectral norm of PT âŠ¥ PPT H(E) and will use some of
the ideas developed in the previous section. Just as before,
pâˆ’1
âˆ¥âˆ¥PT âŠ¥ PPT H(E)
âˆ¥âˆ¥â‰¤ pâˆ’1âˆ¥âˆ¥(P âˆ’ pI) H(E)
âˆ¥âˆ¥,
Found Comput Math (2009) 9: 717â€“772 747
and put
S â‰¡ pâˆ’1(P âˆ’ pI) H(E) = pâˆ’2
âˆ‘
ab,aâ€²bâ€²
Î¾abÎ¾aâ€²bâ€² Eaâ€²bâ€²
âŒ©
PT
(
eaâ€²e
âˆ—
bâ€²
)
, eae
âˆ—
b
âŒª
eae
âˆ—
b,
where here and below, Î¾ab â‰¡ Î´ab âˆ’ p. Decompose S as
S = pâˆ’2
âˆ‘
(a,b)=(aâ€²,bâ€²)
+pâˆ’2
âˆ‘
(a,b) = (aâ€²,bâ€²)
â‰¡ S0 + S1. (6.8)
We bound the spectral norm of the diagonal and off-diagonal contributions separately.
We begin with S0 and decompose (Î¾ab)2 as
Î¾2ab = (Î´ab âˆ’ p)2 = (1 âˆ’ 2p)(Î´ab âˆ’ p) + p(1 âˆ’ p) = (1 âˆ’ 2p)Î¾ab + p(1 âˆ’ p),
which allows us to express S0 as
S0 = 1 âˆ’ 2p
p
âˆ‘
ab
Î¾ab Habeae
âˆ—
b + (1 âˆ’ p)
âˆ‘
ab
Habeae
âˆ—
b,
Hab â‰¡ pâˆ’1 Eab
âŒ©
PT
(
eae
âˆ—
b
)
, eae
âˆ—
b
âŒª
. (6.9)
Theorem 6.3 bounds the spectral norm of the first term of the right-hand side, and we
have
pâˆ’1
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
ab
Î¾ab Habeae
âˆ—
b
âˆ¥âˆ¥âˆ¥âˆ¥â‰¤ C0
âˆš
n3Î² logn
m
â€–Hâ€–âˆž
with probability at least 1 âˆ’ nâˆ’Î² . Now, since â€–Eâ€–âˆž â‰¤ Î¼1âˆšr/n and |ã€ˆPT (eaeâˆ—b),
eae
âˆ—
bã€‰| â‰¤ 2Î¼0r/n by (6.2), â€–Hâ€–âˆž â‰¤ Î¼0Î¼1(2r/np)
âˆš
r/n, and
pâˆ’1
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
ab
Î¾ab Habeae
âˆ—
b
âˆ¥âˆ¥âˆ¥âˆ¥â‰¤ CÎ¼0Î¼1
nr
m
âˆš
nrÎ² logn
m
with the same probability. The second term of the right-hand side in (6.9) is deter-
ministic and we develop an argument that we will reuse several times. We record a
useful lemma.
Lemma 6.4 Let X be a fixed matrix and set Z â‰¡ âˆ‘ab Xabã€ˆPT (eaeâˆ—b), eaeâˆ—bã€‰eaeâˆ—b .
Then
â€–Zâ€– â‰¤ 2Î¼0r
n
â€–Xâ€–.
Proof Let U and V be the diagonal matrices with entries â€–P Ueaâ€–2 and â€–P V ebâ€–2,
respectively,
U = diag
(â€–P Ueaâ€–2
)
, V = diag
(â€–P V ebâ€–2
)
. (6.10)
748 Found Comput Math (2009) 9: 717â€“772
To bound the spectral norm of Z, observe that it follows from (4.7) that
Z = UX + XV âˆ’ UXV = UX(I âˆ’ V ) + XV . (6.11)
Hence, since â€–Uâ€– and â€–V â€– are bounded by min(Î¼0r/n,1) and â€–I âˆ’ V â€– â‰¤ 1,
we have
â€–Zâ€– â‰¤ â€–Uâ€–â€–Xâ€–â€–I âˆ’ V â€– + â€–Xâ€–â€–V â€– â‰¤ (2Î¼0r/n)â€–Xâ€–. 
Clearly, this lemma and â€–Eâ€– = 1 give that H defined in (6.9) obeys â€–Hâ€– â‰¤
2Î¼0r/np. In summary,
â€–S0â€– â‰¤ C nr
m
(
Î¼0Î¼1
âˆš
Î²nr logn
m
+ Î¼0
)
for some C > 0 with the same probability as in Lemma 4.4.
It remains to bound the off-diagonal term. To this end, we use a useful decoupling
lemma.
Lemma 6.5 [15] Let {Î·i}1â‰¤iâ‰¤n be a sequence of independent random variables, and
{xij }i =j be elements taken from a Banach space. Then
P
(âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
i =j
Î·iÎ·j xij
âˆ¥âˆ¥âˆ¥âˆ¥â‰¥ t
)
â‰¤ CD P
(âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
i =j
Î·iÎ·
â€²
j xij
âˆ¥âˆ¥âˆ¥âˆ¥> t/CD
)
, (6.12)
where {Î·â€²i} is an independent copy of {Î·i}.
This lemma asserts that it is sufficient to estimate P(â€–Sâ€²1â€– â‰¥ t) where Sâ€²1 is given
by
Sâ€²1 â‰¡ pâˆ’2
âˆ‘
ab =aâ€²bâ€²
Î¾abÎ¾
â€²
aâ€²bâ€² Eaâ€²bâ€²
âŒ©
PT
(
eaâ€²e
âˆ—
bâ€²
)
, eae
âˆ—
b
âŒª
eae
âˆ—
b (6.13)
in which {Î¾ â€²ab} is an independent copy of {Î¾ab}. We write Sâ€²1 as
Sâ€²1 = pâˆ’1
âˆ‘
ab
Î¾ab Habeae
âˆ—
b,
Hab â‰¡ pâˆ’1
âˆ‘
aâ€²bâ€²:(aâ€²,bâ€²)=(a,b)
Î¾ â€²aâ€²bâ€² Eaâ€²bâ€²
âŒ©
PT
(
eaâ€²e
âˆ—
bâ€²
)
, eae
âˆ—
b
âŒª
. (6.14)
To bound the tail of â€–Sâ€²1â€–, observe that
P
(âˆ¥âˆ¥Sâ€²1
âˆ¥âˆ¥â‰¥ t)â‰¤ P(âˆ¥âˆ¥Sâ€²1
âˆ¥âˆ¥â‰¥ t | â€–Hâ€–âˆž â‰¤ K
)+ P(â€–Hâ€–âˆž > K
)
.
By independence, the first term of the right-hand side is bounded by Theorem 6.3.
On the event {â€–Hâ€–âˆž â‰¤ K}, we have
Found Comput Math (2009) 9: 717â€“772 749
pâˆ’1
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
ab
Î¾ab Habeae
âˆ—
b
âˆ¥âˆ¥âˆ¥âˆ¥â‰¤ C
âˆš
n3Î² logn
m
K.
with probability at least 1 âˆ’ nâˆ’Î² . To bound â€–Hâ€–âˆž, we use Bernsteinâ€™s inequality.
Lemma 6.6 Let X be a fixed matrix and define Q(X) as the matrix whose (a, b)th
entry is
[
Q(X)
]
ab
= pâˆ’1
âˆ‘
aâ€²bâ€²:(aâ€²,bâ€²)=(a,b)
(Î´aâ€²bâ€² âˆ’ p)Xaâ€²bâ€²
âŒ©
PT
(
eaâ€²e
âˆ—
bâ€²
)
, eae
âˆ—
b
âŒª
,
where {Î´ab} is an independent Bernoulli sequence obeying P(Î´ab = 1) = p. Then
P
(âˆ¥âˆ¥Q(X)âˆ¥âˆ¥âˆž > Î»
âˆš
Î¼0r
np
â€–Xâ€–âˆž
)
â‰¤ 2n2 exp
(
âˆ’ Î»
2
2 + 23
âˆš
Î¼0r
np
Î»
)
. (6.15)
With Î» = âˆš3Î² logn, the right-hand side is bounded by 2n2âˆ’Î² provided that np â‰¥
4Î²
3 Î¼0r logn. In particular, for Î» =
âˆš
6Î² logn with Î² > 2, the bound is less than 2nâˆ’Î²
provided that np â‰¥ 8Î²3 Î¼0r logn.
Proof The inequality (6.15) is an application of Bernsteinâ€™s inequality, which states
that for a sum of uniformly bounded independent zero-mean random variables obey-
ing |Yk| â‰¤ c,
P
(âˆ£âˆ£âˆ£âˆ£âˆ£
nâˆ‘
k=1
Yk
âˆ£âˆ£âˆ£âˆ£âˆ£> t
)
â‰¤ 2eâˆ’t2/(2Ïƒ 2+2ct/3), (6.16)
where Ïƒ 2 is the sum of the variances, Ïƒ 2 â‰¡âˆ‘nk=1 Var(Yk). We have
Var
([
Q(X)
]
ab
)= 1 âˆ’ p
p
âˆ‘
aâ€²bâ€²:(aâ€²,bâ€²)=(a,b)
|Xaâ€²bâ€² |2
âˆ£âˆ£âŒ©PT
(
eaâ€²e
âˆ—
bâ€²
)
, eae
âˆ—
b
âŒªâˆ£âˆ£2
â‰¤ 1 âˆ’ p
p
â€–Xâ€–2âˆž
âˆ‘
aâ€²bâ€²:(aâ€²,bâ€²)=(a,b)
âˆ£âˆ£âŒ©PT
(
eae
âˆ—
b
)
, eaâ€²e
âˆ—
bâ€²
âŒªâˆ£âˆ£2
â‰¤ 1 âˆ’ p
p
â€–Xâ€–2âˆž 2Î¼0r/n
by (6.3). Also,
pâˆ’1
âˆ£âˆ£(Î´aâ€²bâ€² âˆ’ p)Xaâ€²bâ€²
âŒ©
PT
(
eaâ€²e
âˆ—
bâ€²
)
, eae
âˆ—
b
âŒªâˆ£âˆ£â‰¤ pâˆ’1 â€–Xâ€–âˆž 2Î¼0r/n
and hence, for each t > 0, (6.16) gives
P
(âˆ£âˆ£[Q(X)]
ab
âˆ£âˆ£> t)â‰¤ 2 exp
(
âˆ’ t
2
2Î¼0r
np
â€–Xâ€–2âˆž + 23 Î¼0rnp â€–Xâ€–âˆžt
)
. (6.17)
750 Found Comput Math (2009) 9: 717â€“772
Putting t = Î»âˆšÎ¼0r/npâ€–Xâ€–âˆž for some Î» > 0 and applying the union bound
gives (6.15). 
Since â€–Eâ€–âˆž â‰¤ Î¼1âˆšr/n, it follows that H = Q(E) introduced in (6.14) obeys
â€–Hâ€–âˆž â‰¤ C Î¼1
âˆš
r
n
âˆš
Î¼0nrÎ² logn
m
with probability at least 1 âˆ’ 2nâˆ’Î² for each Î² > 2 and, therefore,
âˆ¥âˆ¥Sâ€²1
âˆ¥âˆ¥â‰¤ C âˆšÎ¼0Î¼1 nrÎ² logn
m
with probability at least 1 âˆ’ 3nâˆ’Î² . In conclusion, we have
pâˆ’1
âˆ¥âˆ¥(P âˆ’pI) H(E)
âˆ¥âˆ¥â‰¤ C nr
m
(âˆš
Î¼0Î¼1
(âˆš
Î¼0nrÎ² logn
m
+Î² logn
)
+Î¼0
)
(6.18)
with probability at least 1 âˆ’ (1 + 3CD)nâˆ’Î² . A simple algebraic manipulation con-
cludes the proof of Lemma 4.5. Note that we have not made any assumption about
the matrix E and, therefore, established the following lemma.
Lemma 6.7 Let X be a fixed n Ã— n matrix. There is a constant Câ€²0 such that
pâˆ’2
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
(a,b)=(aâ€²,bâ€²)
Î¾abÎ¾aâ€²bâ€²Xab
âŒ©
PT
(
eaâ€²e
âˆ—
bâ€²
)
, eae
âˆ—
b
âŒª
eae
âˆ—
b
âˆ¥âˆ¥âˆ¥âˆ¥â‰¤ Câ€²0
âˆš
Î¼0r Î² logn
p
â€–Xâ€–âˆž
(6.19)
with probability at least 1 âˆ’ O(nâˆ’Î²) for all Î² > 2 provided that np â‰¥ 3Î¼0rÎ² logn.
6.3 Proof of Lemma 4.6
To prove Lemma 4.6, we need to bound the spectral norm of pâˆ’1 (P âˆ’ pI) H2(E),
a matrix given by
pâˆ’3
âˆ‘
a1b1,a2b2,a3b3
Î¾a1b1Î¾a2b2Î¾a3b3Ea3b3
âŒ©
PT
(
ea3e
âˆ—
b3
)
, ea2e
âˆ—
b2
âŒªâŒ©
PT
(
ea2e
âˆ—
b2
)
, ea1e
âˆ—
b1
âŒª
ea1e
âˆ—
b1
,
where Î¾ab = Î´ab âˆ’ p as before. It is convenient to introduce notations to compress
this expression. Set Ï‰ = (a, b) (and Ï‰i = (ai, bi) for i = 1,2,3), FÏ‰ = eaeâˆ—b , and
PÏ‰â€²Ï‰ = ã€ˆPT (eaâ€²eâˆ—bâ€²), eaeâˆ—bã€‰ so that
pâˆ’1 (P âˆ’ pI) H2(E) = pâˆ’3
âˆ‘
Ï‰1,Ï‰2,Ï‰3
Î¾Ï‰1Î¾Ï‰2Î¾Ï‰3 EÏ‰3PÏ‰3Ï‰2PÏ‰2Ï‰1FÏ‰1 .
Found Comput Math (2009) 9: 717â€“772 751
Partition the sum depending on whether some of the Ï‰i â€™s are the same or not
1
p
(P âˆ’ pI)H2(E) = 1
p3
[ âˆ‘
Ï‰1=Ï‰2=Ï‰3
+
âˆ‘
Ï‰1 =Ï‰2=Ï‰3
+
âˆ‘
Ï‰1=Ï‰3 =Ï‰2
+
âˆ‘
Ï‰1=Ï‰2 =Ï‰3
+
âˆ‘
Ï‰1 =Ï‰2 =Ï‰3
]
. (6.20)
The meaning should be clear; for instance, the sum
âˆ‘
Ï‰1 =Ï‰2=Ï‰3 is the sum over the
Ï‰â€™s such that Ï‰2 = Ï‰3 and Ï‰1 = Ï‰2. Similarly, âˆ‘Ï‰1 =Ï‰2 =Ï‰3 is the sum over the Ï‰â€™s
such that they are all distinct. The idea is now to use a decoupling argument to bound
each sum in the right-hand side of (6.20) (except for the first which does not need to
be decoupled) and show that all terms are appropriately small in the spectral norm.
We begin with the first term which is equal to
1
p3
âˆ‘
Ï‰
(Î¾Ï‰)
3 EÏ‰P
2
Ï‰Ï‰FÏ‰ =
1 âˆ’ 3p + 3p2
p3
âˆ‘
Ï‰
Î¾Ï‰ EÏ‰P
2
Ï‰Ï‰FÏ‰
+ 1 âˆ’ 3p + 2p
2
p2
âˆ‘
Ï‰
EÏ‰P
2
Ï‰Ï‰FÏ‰, (6.21)
where we have used the identity
(Î¾Ï‰)
3 = (1 âˆ’ 3p + 3p2)Î¾Ï‰ + p
(
1 âˆ’ 3p + 2p2).
Set HÏ‰ = EÏ‰(pâˆ’1PÏ‰Ï‰)2. For the first term in the right-hand side of (6.21), we need to
control â€–âˆ‘Ï‰ Î¾Ï‰ HÏ‰FÏ‰â€–. This is easily bounded by Theorem 6.3. Indeed, it follows
from
|HÏ‰| â‰¤
(
2Î¼0r
np
)2
â€–Eâ€–âˆž
that for each Î² > 0,
pâˆ’1
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
Ï‰
Î¾Ï‰ HÏ‰ FÏ‰
âˆ¥âˆ¥âˆ¥âˆ¥â‰¤ C
(
Î¼0nr
m
)2
Î¼1
âˆš
nrÎ² logn
m
= C Î¼20Î¼1
âˆš
Î² logn
(
nr
m
)5/2
with probably at least 1 âˆ’ nâˆ’Î² . For the second term in the right-hand side of (6.21),
we apply Lemma 6.4 which gives
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
Ï‰
EÏ‰P
2
Ï‰Ï‰FÏ‰
âˆ¥âˆ¥âˆ¥âˆ¥â‰¤ (2Î¼0r/n)2
so that â€–Hâ€– â‰¤ (2Î¼0r/np)2. In conclusion, the first term in (6.20) has a spectral norm
which is bounded by
C
(
nr
m
)2(
Î¼20Î¼1
(
nrÎ² logn
m
)1/2
+ Î¼20
)
with probability at least 1 âˆ’ nâˆ’Î² .
752 Found Comput Math (2009) 9: 717â€“772
We now turn our attention to the second term which can be written as
pâˆ’3
âˆ‘
Ï‰1 =Ï‰2
Î¾Ï‰1(Î¾Ï‰2)
2 EÏ‰2PÏ‰2Ï‰2PÏ‰2Ï‰1FÏ‰1 =
1 âˆ’ 2p
p3
âˆ‘
Ï‰1 =Ï‰2
Î¾Ï‰1Î¾Ï‰2 EÏ‰2PÏ‰2Ï‰2PÏ‰2Ï‰1FÏ‰1
+ 1 âˆ’ p
p2
âˆ‘
Ï‰1 =Ï‰2
Î¾Ï‰1 EÏ‰2PÏ‰2Ï‰2PÏ‰2Ï‰1FÏ‰1 .
Put S1 for the first term; bounding â€–S1â€– is a simple application of Lemma 6.7 with
XÏ‰ = pâˆ’1EÏ‰PÏ‰Ï‰ , which gives
â€–S1â€– â‰¤ C Î¼3/20 Î¼1 (Î² logn)
(
nr
m
)2
since â€–Eâ€–âˆž â‰¤ Î¼1âˆšr/n. For the second term, we need to bound the spectral norm of
S2 where
S2 â‰¡ pâˆ’1
âˆ‘
Ï‰1
Î¾Ï‰1HÏ‰1FÏ‰1, HÏ‰1 = pâˆ’1
âˆ‘
Ï‰2:Ï‰2 =Ï‰1
EÏ‰2PÏ‰2Ï‰2PÏ‰2Ï‰1 .
Note that H is deterministic. The lemma below provides an estimate about â€–Hâ€–âˆž.
Lemma 6.8 The matrix H obeys
â€–Hâ€–âˆž â‰¤ Î¼0r
np
(
3â€–Eâ€–âˆž + 2Î¼0r
n
)
. (6.22)
Proof We begin by rewriting H as
pHÏ‰ =
âˆ‘
Ï‰â€²
EÏ‰â€²PÏ‰â€²Ï‰â€²PÏ‰â€²Ï‰ âˆ’ EÏ‰P 2Ï‰Ï‰.
Clearly, |EÏ‰P 2Ï‰Ï‰| â‰¤ (Î¼0r/n)2â€–Eâ€–âˆž so that it suffices to bound the first term, which
is the Ï‰th entry of the matrix
âˆ‘
Ï‰,Ï‰â€²
EÏ‰â€²PÏ‰â€²Ï‰â€²PÏ‰â€²Ï‰FÏ‰ = PT (UE + EV âˆ’ UEV ).
Now, it is immediate to see that UE âˆˆ T and likewise for EV . Hence,
âˆ¥âˆ¥PT (UE + EV âˆ’ UEV )
âˆ¥âˆ¥âˆž â‰¤ â€–UEâ€–âˆž + â€–EV â€–âˆž +
âˆ¥âˆ¥PT (UEV )
âˆ¥âˆ¥âˆž
â‰¤ 2â€–Eâ€–âˆžÎ¼0r/n +
âˆ¥âˆ¥PT (UEV )
âˆ¥âˆ¥âˆž.
We finally use the crude estimate
âˆ¥âˆ¥PT (UEV )
âˆ¥âˆ¥âˆž â‰¤
âˆ¥âˆ¥PT (UEV )
âˆ¥âˆ¥â‰¤ 2â€–UEV â€– â‰¤ 2(Î¼0r/n)2
to complete the proof of the lemma. 
Found Comput Math (2009) 9: 717â€“772 753
As a consequence of this lemma, Theorem 6.3 gives
â€–S2â€– â‰¤ C
âˆš
Î² logn
(
nr
m
)3/2(
Î¼0Î¼1 + Î¼20
âˆš
r
)
with probability at least 1âˆ’nâˆ’Î² . In conclusion, the second term in (6.20) has spectral
norm bounded by
C
âˆš
Î² logn
(
nr
m
)3/2(
Î¼0Î¼1
âˆš
Î¼0nrÎ² logn
m
+ Î¼0Î¼1 + Î¼20
âˆš
r
)
with probability at least 1 âˆ’ O(nâˆ’Î²).
We now examine the third term which can be written as
pâˆ’3
âˆ‘
Ï‰1 =Ï‰2
(Î¾Ï‰1)
2Î¾Ï‰2 EÏ‰1PÏ‰1Ï‰2PÏ‰2Ï‰1FÏ‰1 =
1 âˆ’ 2p
p3
âˆ‘
Ï‰1 =Ï‰2
Î¾Ï‰1Î¾Ï‰2 EÏ‰1P
2
Ï‰2Ï‰1
FÏ‰1
+ 1 âˆ’ p
p2
âˆ‘
Ï‰1 =Ï‰2
Î¾Ï‰2 EÏ‰1P
2
Ï‰2Ï‰1
FÏ‰1 .
We use the decoupling argument once more so that for the first term of the right-hand
side, it suffices to estimate the tail of the norm of
S1 â‰¡ pâˆ’1
âˆ‘
Ï‰1
Î¾ (1)Ï‰1 EÏ‰1HÏ‰1FÏ‰1, HÏ‰1 â‰¡ pâˆ’2
âˆ‘
Ï‰2:Ï‰2 =Ï‰1
Î¾ (2)Ï‰2 P
2
Ï‰2Ï‰1
,
where {Î¾ (1)Ï‰ } and {Î¾ (2)Ï‰ } are independent copies of {Î¾Ï‰}. It follows from Bernsteinâ€™s
inequality and the estimates
|PÏ‰2Ï‰1 | â‰¤ 2Î¼0r/n
and
âˆ‘
Ï‰2:Ï‰2 =Ï‰1
|PÏ‰2Ï‰1 |4 â‰¤ max
Ï‰2:Ï‰2 =Ï‰1
|PÏ‰2Ï‰1 |2
âˆ‘
Ï‰2:Ï‰2 =Ï‰1
|PÏ‰2Ï‰1 |2 â‰¤
(
2Î¼0r
n
)2 2Î¼0r
n
that for each Î» > 0,5
P
(
|HÏ‰1 | > Î»
(
2Î¼0r
np
)3/2)
â‰¤ 2 exp
(
âˆ’ Î»
2
2 + 23Î»( 2Î¼0rnp )1/2
)
.
It is now not hard to see that this inequality implies that
P
(
â€–Hâ€–âˆž >
âˆš
8Î² logn
(
2Î¼0nr
m
)3/2)
â‰¤ 2nâˆ’2Î²+2
5We would like to remark that one can often get better estimates; when Ï‰1 = Ï‰2, the bound |PÏ‰2Ï‰1 | â‰¤
2Î¼0r/n may be rather crude. Indeed, one can derive better estimates for the random orthogonal model, for
example.
754 Found Comput Math (2009) 9: 717â€“772
provided that m â‰¥ 169 Î¼0nr Î² logn. As a consequence, for each Î² > 2, Theorem 6.3
gives
â€–S1â€– â‰¤ C Î¼3/20 Î¼1 Î² logn
(
nr
m
)2
with probability at least 1 âˆ’ 3nâˆ’Î² . The other term is equal to (1 âˆ’ p) timesâˆ‘
Ï‰1
EÏ‰1HÏ‰1FÏ‰1 , and
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
Ï‰1
EÏ‰1HÏ‰1FÏ‰1
âˆ¥âˆ¥âˆ¥âˆ¥â‰¤
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
Ï‰1
EÏ‰1HÏ‰1FÏ‰1
âˆ¥âˆ¥âˆ¥âˆ¥
F
â‰¤ â€–Hâ€–âˆžâ€–Eâ€–F â‰¤ C
âˆš
Î² logn
(
Î¼0nr
m
)3/2 âˆš
r.
In conclusion, the third term in (6.20) has spectral norm bounded by
C Î¼0
âˆš
Î² logn
(
nr
m
)3/2(
Î¼1
âˆš
Î¼0nrÎ² logn
m
+ âˆšÎ¼0r
)
with probability at least 1 âˆ’ O(nâˆ’Î²).
We proceed to the fourth term which can be written as
pâˆ’3
âˆ‘
Ï‰1 =Ï‰3
(Î¾Ï‰1)
2Î¾Ï‰3 EÏ‰3PÏ‰3Ï‰1PÏ‰1Ï‰1FÏ‰1 =
1 âˆ’ 2p
p3
âˆ‘
Ï‰1 =Ï‰3
Î¾Ï‰1Î¾Ï‰3 EÏ‰3PÏ‰3Ï‰1PÏ‰1Ï‰1FÏ‰1
+ 1 âˆ’ p
p2
âˆ‘
Ï‰1 =Ï‰3
Î¾Ï‰3 EÏ‰3PÏ‰3Ï‰1PÏ‰1Ï‰1FÏ‰1 .
Let S1 be the first term and set HÏ‰1 = pâˆ’2
âˆ‘
Ï‰1 =Ï‰3 Î¾Ï‰1Î¾Ï‰3 EÏ‰3PÏ‰3Ï‰1FÏ‰1 . Then
Lemma 6.4 gives
â€–S1â€– â‰¤ 2Î¼0r
np
â€–Hâ€– â‰¤ C Î¼3/20 Î¼1 (Î² logn)
(
nr
m
)2
where the last inequality is given by Lemma 6.7. For the other termâ€”call it S2â€”set
HÏ‰1 = pâˆ’1
âˆ‘
Ï‰3:Ï‰3 =Ï‰1 Î¾Ï‰3 EÏ‰3PÏ‰3Ï‰1 . Then Lemma 6.4 gives
â€–S2â€– â‰¤ 2Î¼0r
np
â€–Hâ€–.
Notice that HÏ‰1 = pâˆ’1
âˆ‘
Ï‰3
Î¾Ï‰3 EÏ‰3PÏ‰3Ï‰1 âˆ’ pâˆ’1Î¾Ï‰1EÏ‰1PÏ‰1Ï‰1 so that with GÏ‰1 =
EÏ‰1PÏ‰1Ï‰1
H = pâˆ’1[PT (P âˆ’ pI)(E) âˆ’ (P âˆ’ pI)(G)
]
.
Now, for any matrix X, â€–PT (X)â€– = â€–X âˆ’ PT âŠ¥(X)â€– â‰¤ 2â€–Xâ€– and, therefore,
â€–Hâ€– â‰¤ 2pâˆ’1âˆ¥âˆ¥(P âˆ’ pI)(E)
âˆ¥âˆ¥+ pâˆ’1âˆ¥âˆ¥(P âˆ’ pI)(G)
âˆ¥âˆ¥.
Found Comput Math (2009) 9: 717â€“772 755
As a consequence and since â€–Gâ€–âˆž â‰¤ â€–Eâ€–âˆž, Theorem 6.3 gives for each Î² > 2,
â€–Hâ€– â‰¤ CÎ¼1
âˆš
nrÎ² logn
m
with probability at least 1 âˆ’nâˆ’Î² . In conclusion, the fourth term in (6.20) has spectral
norm bounded by
C Î¼0Î¼1
âˆš
Î² logn
(
nr
m
)3/2(âˆš
Î¼0nrÎ² logn
m
+ 1
)
with probability at least 1 âˆ’ O(nâˆ’Î²).
We finally examine the last term
pâˆ’3
âˆ‘
Ï‰1 =Ï‰2 =Ï‰3
Î¾Ï‰1Î¾Ï‰2Î¾Ï‰3 EÏ‰3PÏ‰3Ï‰2PÏ‰2Ï‰1FÏ‰1 .
Now, just as one has a decoupling inequality for pairs of variables, we have a decou-
pling inequality for triples as well, and we thus simply need to bound the tail of
S1 â‰¡ pâˆ’3
âˆ‘
Ï‰1 =Ï‰2 =Ï‰3
Î¾ (1)Ï‰1 Î¾
(2)
Ï‰2
Î¾ (3)Ï‰3 EÏ‰3PÏ‰3Ï‰2PÏ‰2Ï‰1FÏ‰1
in which the sequences {Î¾ (1)Ï‰ }, {Î¾ (2)Ï‰ } and {Î¾ (3)Ï‰ } are independent copies of {Î¾Ï‰}. We
refer to [15] for details. We now argue as in Sect. 6.2 and write S1 as
S1 = pâˆ’1
âˆ‘
Ï‰1
Î¾ (1)Ï‰1 HÏ‰1FÏ‰1,
where
HÏ‰1 â‰¡ pâˆ’1
âˆ‘
Ï‰2:Ï‰2 =Ï‰1
Î¾ (2)Ï‰2 GÏ‰2 PÏ‰2Ï‰1, GÏ‰2 â‰¡ pâˆ’1
âˆ‘
Ï‰3:Ï‰3 =Ï‰1,Ï‰3 =Ï‰2
Î¾ (3)Ï‰3 EÏ‰3 PÏ‰3Ï‰2 .
(6.23)
By Lemma 6.6, we have for each Î² > 2
â€–Gâ€–âˆž â‰¤ C
âˆš
Î¼0nrÎ² logn
m
â€–Eâ€–âˆž
with large probability and the same argument then gives
â€–Hâ€–âˆž â‰¤ C
âˆš
Î¼0nrÎ² logn
m
â€–Gâ€–âˆž â‰¤ C Î¼0nrÎ² logn
m
â€–Eâ€–âˆž
with probability at least 1 âˆ’ 4nâˆ’Î² . As a consequence, Theorem 6.3 gives
â€–Sâ€– â‰¤ C Î¼0Î¼1
(
nrÎ² logn
m
)3/2
756 Found Comput Math (2009) 9: 717â€“772
with probability at least 1 âˆ’ O(nâˆ’Î²).
To summarize the calculations of this section and using the fact that Î¼0 â‰¥ 1 and
Î¼1 â‰¤ Î¼0âˆšr , we have established that if m â‰¥ Î¼0 nr(Î² logn),
pâˆ’1
âˆ¥âˆ¥(P âˆ’ pI) H2(E)
âˆ¥âˆ¥ â‰¤ C
(
nr
m
)2(
Î¼20Î¼1
âˆš
nrÎ² logn
m
+ Î¼20
)
+ CâˆšÎ² logn
(
nr
m
)3/2
Î¼20
âˆš
r + C
(
nrÎ² logn
m
)3/2
Î¼0Î¼1
with probability at least 1 âˆ’ O(nâˆ’Î²). One can check that if m = Î»Î¼4/30 nr4/3Î² logn
for a fixed Î² â‰¥ 2 and Î» â‰¥ 1, then there is a constant C such that
âˆ¥âˆ¥pâˆ’1 (P âˆ’ pI) H2(E)
âˆ¥âˆ¥â‰¤ CÎ»âˆ’3/2
with probability at least 1 âˆ’ O(nâˆ’Î²). This is the content of Lemma 4.6.
6.4 Proof of Lemma 4.7
Clearly, one could continue on the same path and estimate the spectral norm of
pâˆ’1(P âˆ’ pI) H3(E) by the same technique as in the previous sections. That is
to say, we would write
pâˆ’1(P âˆ’ pI) H3(E) = pâˆ’4
âˆ‘
Ï‰1,Ï‰2,Ï‰3,Ï‰4
[
4âˆ
i=1
Î¾Ï‰i
]
EÏ‰4
[
3âˆ
i=1
PÏ‰i+1Ï‰i
]
FÏ‰1
with the same notations as before, and partition the sum depending on whether some
of the Ï‰i â€™s are the same or not. Then we would use the decoupling argument to bound
each term in the sum. Although this is a clear possibility, one would need to consider
18 cases and the calculations would become a little laborious. In this section, we
propose to bound the term pâˆ’1(P âˆ’ pI) H3(E) with a different argument which
has two main advantages: first, it is much shorter and second, it uses much of what
we have already established. The downside is that it is not as sharp.
The starting point is to note that
pâˆ’1(P âˆ’ pI) H3(E) = pâˆ’1
(
 â—¦ H3(E)),
where  is the matrix with i.i.d. entries equal to Î¾ab = Î´ab âˆ’ p and â—¦ denotes the
Hadamard product (componentwise multiplication). To bound the spectral norm of
this Hadamard product, we apply an inequality due to Ando, Horn, and Johnson [2].
An elementary proof can be found in Sect. 5.6 of [20].
Lemma 6.9 [20] Let A and B be two n1 Ã— n2 matrices. Then
â€–A â—¦ Bâ€– â‰¤ â€–Aâ€–Î½(B), (6.24)
where Î½ is the function
Î½(B) = inf{c(X)c(Y ) : XY âˆ— = B},
Found Comput Math (2009) 9: 717â€“772 757
and c(X) is the maximum Euclidean norm of the rows
c(X)2 = max
1â‰¤iâ‰¤n
âˆ‘
j
X2ij .
To apply (6.24), we first notice that one can estimate the norm of  via The-
orem 6.3. Indeed, let Z = 11âˆ— be the matrix with all entries equal to one. Then
pâˆ’1 = pâˆ’1(P âˆ’ pI)(Z), and thus
pâˆ’1â€–â€– â‰¤ C
(
n3Î² logn
m
)1/2
(6.25)
with probability at least 1 âˆ’ nâˆ’Î² . One could obtain a similar result by appealing to
the recent literature on random matrix theory and on concentration of measure. Po-
tentially, this could allow to derive an upper bound without the logarithmic term, but
we will not consider these refinements here. (It is interesting to note in passing, how-
ever, that the two page proof of Theorem 6.3 gives a large deviation result about the
largest singular value of a matrix with i.i.d. entries which is sharp up to a multiplica-
tive factor proportional to at most
âˆš
logn.)
Second, we bound the second factor in (6.24) via the following estimate.
Lemma 6.10 There are numerical constants C and c so that for each Î² > 2, H3(E)
obeys
Î½
(
H3(E)
)â‰¤ CÎ¼0r/n (6.26)
with probability at least 1 âˆ’ O(nâˆ’Î²) provided that m â‰¥ cÎ¼4/30 nr5/3(Î² logn).
The two inequalities (6.25) and (6.26) give
pâˆ’1
âˆ¥âˆ¥ â—¦ H3(E)âˆ¥âˆ¥â‰¤ C
âˆš
Î¼20 nr
2 Î² logn
m
,
with large probability. Hence, when m is substantially larger than a constant times
Î¼20nr
2(Î² logn), we have that the spectral norm of pâˆ’1(P âˆ’ pI) H3(E) is much
less than 1. This is the content of Lemma 4.7.
The remainder of this section proves Lemma 6.10. Set S â‰¡ H3(E) for short. Be-
cause S is in T , S = PT (S) = P US + SP V âˆ’ P USP V . Writing P U =âˆ‘rj=1 ujuâˆ—j
and similarly for P V gives
S =
râˆ‘
j=1
uj
(
uâˆ—jS
)+
râˆ‘
j=1
(
(I âˆ’ P U)Svj
)
vâˆ—j .
For each 1 â‰¤ j â‰¤ r , let Î±j â‰¡ Svj and Î²âˆ—j â‰¡ uâˆ—jS. Then the decomposition
S =
râˆ‘
j=1
ujÎ²
âˆ—
j +
râˆ‘
j=1
(P UâŠ¥Î±j )v
âˆ—
j ,
758 Found Comput Math (2009) 9: 717â€“772
where P UâŠ¥ = I âˆ’ P U , provides a factorization of the form
S = XY âˆ—,
{
X = [u1, . . . ,ur ,P UâŠ¥Î±1, . . . ,P UâŠ¥Î±r ],
Y = [Î²1, . . . ,Î²r ,v1, . . . ,vr ].
It follows from our assumption that
c2
([u1, . . . ,ur ]
)= max
1â‰¤iâ‰¤n
âˆ‘
1â‰¤jâ‰¤r
u2ij = max
1â‰¤iâ‰¤n
â€–P Ueiâ€–2 â‰¤ Î¼0r/n,
and similarly for [v1, . . . ,vr ]. Hence, to prove Lemma 6.10, it suffices to prove
that the maximum row norm obeys c([Î²1, . . . ,Î²r ]) â‰¤ C
âˆš
Î¼0r/n for some constant
C > 0, and similarly for the matrix [P UâŠ¥Î±1, . . . ,P UâŠ¥Î±r ].
Lemma 6.11 There is a numerical constant C such that for each Î² > 2,
c
([Î±1, . . . ,Î±r ]
)â‰¤ CâˆšÎ¼0r/n (6.27)
with probability at least 1 âˆ’ O(nâˆ’Î²) provided that m obeys the condition of
Lemma 6.10.
A similar estimate for [Î²1, . . . ,Î²r ] is obtained in the same way by exchanging the
roles of u and v. Moreover, a minor modification of the argument gives
c
([P UâŠ¥Î±1, . . . ,P UâŠ¥Î±r ]
)â‰¤ CâˆšÎ¼0r/n (6.28)
as well, and we will omit the details. In short, the estimate (6.27) implies Lemma 6.10.
Proof of Lemma 6.11 To prove (6.27), we use the notations of the previous section
and write
Î±j = pâˆ’3
âˆ‘
a1b1,a2b2,a3b3
Î¾a1b1Î¾a2b2Î¾a3b3Ea3b3
âŒ©
PT ea3eâˆ—b3 , ea2e
âˆ—
b2
âŒªâŒ©
PT ea2eâˆ—b2, ea1e
âˆ—
b1
âŒª
Ã— PT
(
ea1e
âˆ—
b1
)
vj
= pâˆ’3
âˆ‘
Ï‰1,Ï‰2,Ï‰3
Î¾Ï‰1Î¾Ï‰2Î¾Ï‰3 EÏ‰3PÏ‰3Ï‰2PÏ‰2Ï‰1 PT (FÏ‰1)vj
= pâˆ’3
âˆ‘
Ï‰1,Ï‰2,Ï‰3
Î¾Ï‰1Î¾Ï‰2Î¾Ï‰3 EÏ‰3PÏ‰3Ï‰2PÏ‰2Ï‰1(FÏ‰1vj )
since for any matrix X, PT (X)vj = Xvj for each 1 â‰¤ j â‰¤ r . We then follow the
same steps as in Sect. 6.3 and partition the sum depending on whether some of the
Ï‰i â€™s are the same or not
Î±j = pâˆ’3
[ âˆ‘
Ï‰1=Ï‰2=Ï‰3
+
âˆ‘
Ï‰1 =Ï‰2=Ï‰3
+
âˆ‘
Ï‰1=Ï‰3 =Ï‰2
+
âˆ‘
Ï‰1=Ï‰2 =Ï‰3
+
âˆ‘
Ï‰1 =Ï‰2 =Ï‰3
]
. (6.29)
Found Comput Math (2009) 9: 717â€“772 759
The idea is this: to establish (6.27), it is sufficient to show that if Î³ j is any of the five
terms above, it obeys
âˆš âˆ‘
1â‰¤jâ‰¤r
|Î³ij |2 â‰¤ C
âˆš
Î¼0r/n (6.30)
(Î³ij is the ith component of Î³ j as usual) with large probability. The strategy for
getting such estimates is to use decoupling whenever applicable.
Just as Theorem 6.3 proved useful to bound the norm of pâˆ’1(P âˆ’ pI)H2(E)
in Sect. 6.3, the lemma below will help bounding the magnitudes of the components
of Î±j .
Lemma 6.12 Define S â‰¡ pâˆ’1âˆ‘ij
âˆ‘
Ï‰ Î¾Ï‰HÏ‰ã€ˆei ,FÏ‰vj ã€‰eieâˆ—j . Then for each Î» > 0
P
(â€–Sâ€–âˆž â‰¥
âˆš
Î¼0/n
)â‰¤ 2n2 exp
(
âˆ’ 1
2n
Î¼0p
â€–Hâ€–2âˆž + 23p
âˆš
râ€–Hâ€–âˆž
)
. (6.31)
Proof The proof is an application of Bernsteinâ€™s inequality (6.16). Note that
ã€ˆei ,FÏ‰vj ã€‰ = 1{a=i}vbj , and hence
Var(Sij ) â‰¤ pâˆ’1â€–Hâ€–2âˆž
âˆ‘
Ï‰
âˆ£âˆ£ã€ˆei ,FÏ‰vj ã€‰
âˆ£âˆ£2 = pâˆ’1â€–Hâ€–2âˆž
since
âˆ‘
Ï‰ |ã€ˆei ,FÏ‰vj ã€‰|2 = 1, and |pâˆ’1HÏ‰ã€ˆei ,FÏ‰vj ã€‰| â‰¤ pâˆ’1 â€–Hâ€–âˆž
âˆš
Î¼0r/n since
|ã€ˆei ,FÏ‰vj ã€‰| â‰¤ |vbj | and
|vbj | â‰¤ â€–P V ebâ€– â‰¤
âˆš
Î¼0r/n. 
Each term in (6.29) is given by the corresponding term in (6.20) after formally
substituting FÏ‰ with FÏ‰vj . We begin with the first term whose ith component is
equal to
Î³ij â‰¡ pâˆ’3
(
1 âˆ’ 3p + 3p2)
âˆ‘
Ï‰
Î¾Ï‰ EÏ‰P
2
Ï‰Ï‰ã€ˆei ,FÏ‰vj ã€‰
+ pâˆ’2(1 âˆ’ 3p + 2p2)
âˆ‘
Ï‰
EÏ‰P
2
Ï‰Ï‰ã€ˆei ,FÏ‰vj ã€‰. (6.32)
Ignoring the constant factor (1 âˆ’ 3p + 3p2) which is bounded by 1, we write the first
of these two terms as
(S0)ij â‰¡ pâˆ’1
âˆ‘
Ï‰
Î¾Ï‰ HÏ‰ã€ˆei ,FÏ‰vj ã€‰, HÏ‰ = EÏ‰
(
pâˆ’1PÏ‰Ï‰
)2
.
Since â€–Hâ€–âˆž â‰¤ (Î¼0nr/m)2 Î¼1âˆšr/n, it follows from Lemma (6.12) that
P
(â€–S0â€–âˆž â‰¥
âˆš
Î¼0/n
)â‰¤ 2n2 eâˆ’1/D, D â‰¤ C
(
Î¼30Î¼
2
1
(
nr
m
)5
+ Î¼20Î¼1
(
nr
m
)3)
760 Found Comput Math (2009) 9: 717â€“772
for some numerical C > 0. Since Î¼1 â‰¤ Î¼0âˆšr , we have that when m â‰¥ Î»Î¼0 nr6/5 Ã—
(Î² logn) for some numerical constant Î» > 0, â€–S0â€–âˆž â‰¥ âˆšÎ¼0/n with probability at
most 2n2eâˆ’(Î² logn)3 ; this probability is inversely proportional to a superpolynomial
in n. For the second term, the matrix with entries EÏ‰P 2Ï‰Ï‰ is given by
2UE + E2V + 2UEV + 2UE2V âˆ’ 22UEV âˆ’ 2UE2V
and thus
âˆ‘
Ï‰
EÏ‰P
2
Ï‰Ï‰ã€ˆei ,FÏ‰vj ã€‰ =
âŒ©
ei ,
(
2UE + E2V + 2UEV + 2UE2V
âˆ’ 22UEV âˆ’ 2UE2V
)
vj
âŒª
.
This is a sum of six terms and we will show how to bound the first three; the last three
are dealt in exactly the same way and obey better estimates. For the first, we have
âŒ©
ei ,
2
UEvj
âŒª= âŒ©2Uei ,Evj
âŒª= â€–P Ueiâ€–4ã€ˆei ,uj ã€‰.
Hence,
pâˆ’2
âˆš âˆ‘
1â‰¤jâ‰¤r
âˆ£âˆ£âŒ©ei ,2UEvj
âŒªâˆ£âˆ£2 = pâˆ’2â€–P Ueiâ€–4
âˆš âˆ‘
1â‰¤jâ‰¤r
âˆ£âˆ£ã€ˆei ,uj ã€‰
âˆ£âˆ£2
= pâˆ’2â€–P Ueiâ€–5 â‰¤
(
Î¼0r
np
)2âˆš
Î¼0r
n
.
In other words, when m â‰¥ Î¼0nr , the right hand-side is bounded by âˆšÎ¼0r/n as de-
sired. For the second term, we have
âŒ©
ei ,E
2
V vj
âŒª=
âˆ‘
b
â€–P V ebâ€–4vbj ã€ˆei ,Eebã€‰ =
âˆ‘
b
â€–P V ebâ€–4vbjEib.
Hence, it follows from the Cauchyâ€“Schwarz inequality and (6.4) that
pâˆ’2
âˆ£âˆ£âŒ©ei ,E2V vj
âŒªâˆ£âˆ£â‰¤
(
Î¼0r
np
)2âˆš
Î¼0r
n
.
In other words, when m â‰¥ Î¼0nr5/4,
pâˆ’2
âˆš âˆ‘
1â‰¤jâ‰¤r
âˆ£âˆ£âŒ©ei ,E2V vj
âŒªâˆ£âˆ£2 â‰¤
âˆš
Î¼0r
n
(6.33)
as desired. For the third term, we have
ã€ˆei ,UEV vj ã€‰ = â€–P Ueiâ€–2
âˆ‘
b
â€–P V ebâ€–2vbjEib.
Found Comput Math (2009) 9: 717â€“772 761
The Cauchyâ€“Schwarz inequality gives
2pâˆ’2
âˆ£âˆ£ã€ˆei ,UEV vj ã€‰
âˆ£âˆ£â‰¤ 2
(
Î¼0r
np
)2âˆš
Î¼0r
n
just as before. In other words, when m â‰¥ Î¼0nr5/4, 2pâˆ’2
âˆšâˆ‘
1â‰¤jâ‰¤r |ã€ˆei ,UEV vj ã€‰|2
is bounded by 2
âˆš
Î¼0r/n. The other terms obey (6.33) as well when m â‰¥ Î¼0nr5/4.
In conclusion, the first term (6.32) in (6.29) obeys (6.30) with probability at least
1 âˆ’ O(nâˆ’Î²) provided that m â‰¥ Î¼0nr5/4(Î² logn).
We now turn our attention to the second term which can be written as
Î³ij â‰¡ pâˆ’3(1 âˆ’ 2p)
âˆ‘
Ï‰1 =Ï‰2
Î¾Ï‰1Î¾Ï‰2 EÏ‰2PÏ‰2Ï‰2PÏ‰2Ï‰1ã€ˆei ,FÏ‰1vj ã€‰
+ pâˆ’2(1 âˆ’ p)
âˆ‘
Ï‰1 =Ï‰2
Î¾Ï‰1 EÏ‰2PÏ‰2Ï‰2PÏ‰2Ï‰1ã€ˆei ,FÏ‰1vj ã€‰.
We decouple the first term so that it suffices to bound
(S0)ij â‰¡ pâˆ’1
âˆ‘
Ï‰1
Î¾ (1)Ï‰1 HÏ‰1ã€ˆei ,FÏ‰1vj ã€‰, HÏ‰1 â‰¡ pâˆ’2
âˆ‘
Ï‰2:Ï‰2 =Ï‰1
Î¾ (2)Ï‰2 EÏ‰2PÏ‰2Ï‰2PÏ‰2Ï‰1,
where the sequences {Î¾ (1)Ï‰ } and {Î¾ (2)Ï‰ } are independent. The method from Sect. 6.2
shows that
â€–Hâ€–âˆž â‰¤ C
âˆš
Î¼0nrÎ² logn
m
sup
Ï‰
âˆ£âˆ£EÏ‰
(
pâˆ’1PÏ‰Ï‰
)âˆ£âˆ£â‰¤ CâˆšÎ² logn
(
Î¼0nr
m
)3/2
â€–Eâ€–âˆž
with probability at least 1 âˆ’ 2nâˆ’Î² for each Î² > 2. Therefore, Lemma 6.12 gives
P
(â€–S0â€–âˆž â‰¥
âˆš
Î¼0/n
)â‰¤ 2n2eâˆ’1/D, (6.34)
where D obeys
D â‰¤ C
(
Î¼20Î¼
2
1(Î² logn)
(
nr
m
)4
+ Î¼3/20 Î¼1
âˆš
Î² logn
(
nr
m
)5/2)
(6.35)
for some positive constant C. Hence, when m â‰¥ Î»Î¼0 nr5/4(Î² logn) for some suffi-
ciently large numerical constant Î» > 0, we have that â€–S0â€–âˆž â‰¥ âˆšÎ¼0/n with proba-
bility at most 2n2eâˆ’(Î² logn)2 . This is inversely proportional to a superpolynomial in n.
We write the second term as
(S1)ij â‰¡ pâˆ’1
âˆ‘
Ï‰1 =Ï‰2
Î¾Ï‰1HÏ‰1ã€ˆei ,FÏ‰1vj ã€‰, HÏ‰1 = pâˆ’1
âˆ‘
Ï‰2:Ï‰2 =Ï‰1
EÏ‰2PÏ‰2Ï‰2PÏ‰2Ï‰1 .
We know from Sect. 6.3 that H obeys â€–Hâ€–âˆž â‰¤ C Î¼20 r2/m since Î¼1 â‰¤ Î¼0
âˆš
r so that
Lemma 6.12 gives
P
(â€–S1â€–âˆž â‰¥
âˆš
Î¼0/n
)â‰¤ 2n2eâˆ’1/D, D â‰¤ C
(
Î¼30
n3r4
m3
+ Î¼20
n2r5/2
m2
)
762 Found Comput Math (2009) 9: 717â€“772
for some C > 0. Hence, when m â‰¥ Î»Î¼0 nr4/3(Î² logn) for some numerical constant
Î» > 0, we have that â€–S1â€–âˆž â‰¥ âˆšÎ¼0/n with probability at most 2n2eâˆ’(Î² logn)2 . This
is inversely proportional to a superpolynomial in n. In conclusion and taking into
account the decoupling constants in (6.12), the second term in (6.29) obeys (6.30)
with probability at least 1 âˆ’ O(nâˆ’Î²) provided that m is sufficiently large as above.
We now examine the third term which can be written as
pâˆ’3(1 âˆ’ 2p)
âˆ‘
Ï‰1 =Ï‰2
Î¾Ï‰1Î¾Ï‰2 EÏ‰1P
2
Ï‰2Ï‰1
ã€ˆei ,FÏ‰1vj ã€‰
+ pâˆ’2(1 âˆ’ p)
âˆ‘
Ï‰1 =Ï‰2
Î¾Ï‰2 EÏ‰1P
2
Ï‰2Ï‰1
ã€ˆei ,FÏ‰1vj ã€‰.
For the first term of the right-hand side, it suffices to estimate the tail of
(S0)ij â‰¡ pâˆ’1
âˆ‘
Ï‰1
Î¾ (1)Ï‰1 EÏ‰1HÏ‰1ã€ˆei ,FÏ‰1vj ã€‰, HÏ‰1 â‰¡ pâˆ’2
âˆ‘
Ï‰2:Ï‰2 =Ï‰1
Î¾ (2)Ï‰2 P
2
Ï‰2Ï‰1
,
where {Î¾ (1)Ï‰ } and {Î¾ (2)Ï‰ } are independent. We know from Sect. 6.3 that â€–Hâ€–âˆž obeys
â€–Hâ€–âˆž â‰¤ C âˆšÎ² logn (Î¼0nr/m)3/2 with probability at least 1 âˆ’ 2nâˆ’Î² for each Î² > 2.
Thus, Lemma (6.12) shows that S0 obeys (6.34)â€“(6.35) just as before. The other
term is equal to (1âˆ’p) timesâˆ‘Ï‰1 EÏ‰1HÏ‰1ã€ˆei ,FÏ‰1vj ã€‰, and by the Cauchyâ€“Schwarz
inequality and (6.4)
âˆ£âˆ£âˆ£âˆ£
âˆ‘
Ï‰1
EÏ‰1HÏ‰1ã€ˆei ,FÏ‰1vj ã€‰
âˆ£âˆ£âˆ£âˆ£ â‰¤ â€–Hâ€–âˆž
âˆ¥âˆ¥eâˆ—i E
âˆ¥âˆ¥
(âˆ‘
b
v2bj
)1/2
â‰¤ C
âˆš
Î¼0
n
âˆš
Î² logn
(
Î¼0nr
4/3
m
)3/2
on the event where â€–Hâ€–âˆž â‰¤ C âˆšÎ² logn (Î¼0nr/m)3/2. Hence, when m â‰¥ Î»Î¼0 nr4/3 Ã—
(Î² logn) for some numerical constant Î» > 0, we have that |âˆ‘Ï‰1 EÏ‰1HÏ‰1ã€ˆei ,FÏ‰1vj ã€‰|
â‰¤ âˆšÎ¼0/n on this event. In conclusion, the third term in (6.29) obeys (6.30) with
probability at least 1 âˆ’ O(nâˆ’Î²) provided that m is sufficiently large as above.
We proceed to the fourth term which can be written as
pâˆ’3(1 âˆ’ 2p)
âˆ‘
Ï‰1 =Ï‰3
Î¾Ï‰1Î¾Ï‰3 EÏ‰3PÏ‰3Ï‰1PÏ‰1Ï‰1ã€ˆei ,FÏ‰1vj ã€‰
+ pâˆ’2(1 âˆ’ p)
âˆ‘
Ï‰1 =Ï‰3
Î¾Ï‰3 EÏ‰3PÏ‰3Ï‰1PÏ‰1Ï‰1ã€ˆei ,FÏ‰1vj ã€‰.
We use the decoupling trick for the first term and bound the tail of
(S0)ij â‰¡ pâˆ’1
âˆ‘
Ï‰1
Î¾ (1)Ï‰1 HÏ‰1
(
pâˆ’1PÏ‰1Ï‰1
) ã€ˆei ,FÏ‰1vj ã€‰,
HÏ‰1 â‰¡ pâˆ’1
âˆ‘
Ï‰3:Ï‰3 =Ï‰1
Î¾ (3)Ï‰3 EÏ‰3PÏ‰3Ï‰1,
Found Comput Math (2009) 9: 717â€“772 763
where {Î¾ (1)Ï‰ } and {Î¾ (3)Ï‰ } are independent. We know from Sect. 6.2 that
â€–Hâ€–âˆž â‰¤ C
âˆš
Î¼0nrÎ² logn
m
â€–Eâ€–âˆž
with probability at least 1 âˆ’ 2nâˆ’Î² for each Î² > 2. Therefore, Lemma 6.12 shows
that S0 obeys (6.34)â€“(6.35) just as before. The other term is equal to (1 âˆ’ p) timesâˆ‘
Ï‰1
HÏ‰1(p
âˆ’1PÏ‰1Ï‰1) ã€ˆei ,FÏ‰1vj ã€‰, and the Cauchyâ€“Schwarz inequality gives
âˆ£âˆ£âˆ£âˆ£
âˆ‘
Ï‰1
HÏ‰1
(
pâˆ’1PÏ‰1Ï‰1
) ã€ˆei ,FÏ‰1vj ã€‰
âˆ£âˆ£âˆ£âˆ£â‰¤
âˆš
nâ€–Hâ€–âˆž Î¼0nr
m
â‰¤ C Î¼1
âˆš
rÎ² lognâˆš
n
(
Î¼0nr
m
)3/2
on the event â€–Hâ€–âˆž â‰¤ C
âˆš
Î¼0nr(Î² logn)/mâ€–Eâ€–âˆž. Because Î¼1 â‰¤ Î¼0âˆšr , we
have that whenever m â‰¥ Î»Î¼4/30 nr5/3 (Î² logn) for some numerical constant Î» > 0,
pâˆ’1|âˆ‘Ï‰1 HÏ‰1PÏ‰1Ï‰1ã€ˆei ,FÏ‰1vj ã€‰| â‰¤
âˆš
Î¼0/n just as before. In conclusion, the fourth
term in (6.29) obeys (6.30) with probability at least 1 âˆ’ O(nâˆ’Î²) provided that m is
sufficiently large as above.
We finally examine the last term
pâˆ’3
âˆ‘
Ï‰1 =Ï‰2 =Ï‰3
Î¾Ï‰1Î¾Ï‰2Î¾Ï‰3 EÏ‰3PÏ‰3Ï‰2PÏ‰2Ï‰1ã€ˆei ,FÏ‰1vj ã€‰.
Just as before, we need to bound the tail of
(S0)ij â‰¡ pâˆ’1
âˆ‘
Ï‰1,Ï‰2,Ï‰3
Î¾ (1)Ï‰1 HÏ‰1ã€ˆei ,FÏ‰1vj ã€‰,
where H is given by (6.23). We know from Sect. 6.3 that H obeys
â€–Hâ€–âˆž â‰¤ C (Î² logn) Î¼0nr
m
Î¼1
âˆš
r
n
with probability at least 1 âˆ’ 4nâˆ’Î² for each Î² > 2. Therefore, Lemma 6.12 gives
P
(
â€–S0â€–âˆž â‰¥ 1
5
âˆš
Î¼0/n
)
â‰¤ 2n2eâˆ’1/D,
D â‰¤ C
(
Î¼0Î¼
2
1(Î² logn)
2
(
nr
m
)3
+ Î¼0Î¼1(Î² logn)
(
nr
m
)2)
for some C > 0. Hence, when m â‰¥ Î»Î¼0 nr4/3(Î² logn) for some numerical constant
Î» > 0, we have that â€–S0â€–âˆž â‰¥ 15
âˆš
Î¼0/n with probability at most 2n2eâˆ’(Î² logn). In
conclusion, the fifth term in (6.29) obeys (6.30) with probability at least 1 âˆ’ O(nâˆ’Î²)
provided that m is sufficiently large as above.
To summarize the calculations of this section, if m = Î»Î¼4/30 nr5/3 (Î² logn) where
Î² â‰¥ 2 is fixed and Î» is some sufficiently large numerical constant, then
âˆ‘
1â‰¤jâ‰¤r
|Î±ij |2 â‰¤ Î¼0r/n
with probability at least 1 âˆ’ O(nâˆ’Î²). This concludes the proof. 
764 Found Comput Math (2009) 9: 717â€“772
6.5 Proof of Lemma 4.8
It remains to study the spectral norm of pâˆ’1(PT âŠ¥ PPT )
âˆ‘
kâ‰¥k0 H
k(E) for some
positive integer k0, which we bound by the Frobenius norm
pâˆ’1
âˆ¥âˆ¥âˆ¥âˆ¥(PT âŠ¥ PPT )
âˆ‘
kâ‰¥k0
Hk(E)
âˆ¥âˆ¥âˆ¥âˆ¥â‰¤ pâˆ’1
âˆ¥âˆ¥âˆ¥âˆ¥(PPT )
âˆ‘
kâ‰¥k0
Hk(E)
âˆ¥âˆ¥âˆ¥âˆ¥
F
â‰¤âˆš3/2p
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
kâ‰¥k0
Hk(E)
âˆ¥âˆ¥âˆ¥âˆ¥
F
,
where the inequality follows from Corollary 4.3. To bound the Frobenius norm of the
series, write
âˆ¥âˆ¥âˆ¥âˆ¥
âˆ‘
kâ‰¥k0
Hk(E)
âˆ¥âˆ¥âˆ¥âˆ¥
F
â‰¤ â€–Hâ€–k0â€–Eâ€–F + â€–Hâ€–k0+1â€–Eâ€–F + Â· Â· Â·
â‰¤ â€–Hâ€–
k0
1 âˆ’ â€–Hâ€– â€–Eâ€–F .
Theorem 4.1 gives an upper bound on â€–Hâ€– since â€–Hâ€– â‰¤ CR âˆšÎ¼0nrÎ² logn/m < 1/2
on an event with probability at least 1 âˆ’ 3nâˆ’Î² . Since â€–Eâ€–F = âˆšr , we conclude that
pâˆ’1
âˆ¥âˆ¥âˆ¥âˆ¥(PPT )
âˆ‘
kâ‰¥k0
Hk(E)
âˆ¥âˆ¥âˆ¥âˆ¥
F
â‰¤ C 1âˆš
p
(
Î¼0nrÎ² logn
m
)k0/2 âˆš
r
= C
(
n2r
m
)1/2(
Î¼0nrÎ² logn
m
)k0/2
with large probability. This is the content of Lemma 4.8.
7 Numerical Experiments
To demonstrate the practical applicability of the nuclear norm heuristic for recover-
ing low-rank matrices from their entries, we conducted a series of numerical experi-
ments for a variety of the matrix sizes n, ranks r , and numbers of entries m. For each
(n,m, r) triple, we repeated the following procedure 50 times. We generated M , an
nÃ—n matrix of rank r , by sampling two nÃ—r factors ML and MR with i.i.d. Gaussian
entries and setting M = MLMâˆ—R . We sampled a subset  of m entries uniformly at
random. Then the nuclear norm minimization
minimize â€–Xâ€–âˆ—
subject to Xij = Mij , (i, j) âˆˆ 
Found Comput Math (2009) 9: 717â€“772 765
Fig. 1 Recovery of full matrices from their entries. For each (n,m, r) triple, we repeated the following
procedure 50 times. A matrix M of rank r and a subset of m entries were selected at random. Then we
solved the nuclear norm minimization for X subject to Xij = Mij on the selected entries. We declared M
to be recovered if â€–Xopt âˆ’ Mâ€–F /â€–Mâ€–F < 10âˆ’3. The results are shown for (a) n = 40 and (b) n = 50.
The color of each cell reflects the empirical recovery rate (scaled between 0 and 1). White denotes perfect
recovery in all experiments, and black denotes failure for all experiments
was solved using the SDP solver SDPT3 [36]. We declared M to be recovered if the
solution returned by the SDP, Xopt, satisfied â€–Xopt âˆ’ Mâ€–F /â€–Mâ€–F < 10âˆ’3. Figure 1
shows the results of these experiments for n = 40 and 50. The x-axis corresponds
to the fraction of the entries of the matrix that are revealed to the SDP solver. The
y-axis corresponds to the ratio between the dimension of the set of rank r matrices,
dr = r(2n âˆ’ r), and the number of measurements m. Note that both of these axes
range from zero to one as a value greater than one on the x-axis corresponds to an
overdetermined linear system where the semidefinite program always succeeds, and
a value of greater than one on the y-axis corresponds to a situation where there is
always an infinite number of matrices with rank r with the given entries. The color
of each cell in the figures reflects the empirical recovery rate of the 50 runs (scaled
between 0 and 1). White denotes perfect recovery in all experiments, and black de-
notes failure for all experiments. Interestingly, the experiments reveal very similar
plots for different n, suggesting that our asymptotic conditions for recovery may be
rather conservative.
For a second experiment, we generated random positive semidefinite matrices and
tried to recover them from their entries using the nuclear norm heuristic. As above,
we repeated the same procedure 50 times for each (n,m, r) triple. We generated M ,
an nÃ—n positive semidefinite matrix of rank r , by sampling an nÃ— r factor MF with
i.i.d. Gaussian entries and setting M = MF Mâˆ—F . We sampled a subset  of m entries
uniformly at random. Then we solved the nuclear norm minimization problem
minimize trace(X)
subject to Xij = Mij , (i, j) âˆˆ ,
X  0.
As above, we declared M to be recovered if â€–Xopt âˆ’ Mâ€–F /â€–Mâ€–F < 10âˆ’3. Figure 2
shows the results of these experiments for n = 40 and 50. The x-axis again corre-
766 Found Comput Math (2009) 9: 717â€“772
Fig. 2 Recovery of positive semidefinite matrices from their entries. For each (n,m, r) triple, we repeated
the following procedure 50 times. A positive semidefinite matrix M of rank r and a set of m entries
were selected at random. Then we solved the nuclear norm minimization subject to Xij = Mij on the
selected entries with the constraint that X  0. The color scheme for each cell denotes empirical recovery
probability and is the same as in Fig. 1. The results are shown for (a) n = 40 and (b) n = 50
sponds to the fraction of the entries of the matrix that are revealed to the SDP solver,
but in this case, the number of measurements is divided by Dn = n(n + 1)/2, the
number of unique entries in a positive-semidefinite matrix and the dimension of the
rank r matrices is dr = nr âˆ’ r(r âˆ’ 1)/2. The color of each cell is chosen in the same
fashion as in the experiment with full matrices. Interestingly, the recovery region is
much larger for positive semidefinite matrices, and future work is needed to inves-
tigate if the theoretical scaling is also more favorable in this scenario of low-rank
matrix completion.
Finally, in Fig. 3, we plot the performance of the nuclear norm heuristic when
recovering low-rank matrices from Gaussian projections of these matrices. In these
cases, M was generated in the same fashion as above, but in place of sampling en-
tries, we generated m random Gaussian projections of the data (see the discussion in
Sect. 1.4). Then we solved the optimization
minimize â€–Xâ€–âˆ—
subject to A(X) = A(M),
with the additional constraint that X  0 in the positive semidefinite case. Here, A(X)
denotes a linear map of the form (1.15) where the entries are sampled i.i.d. from a
zero-mean unit variance Gaussian distribution. In these experiments, the recovery
regime is far larger than in the case of that of sampling entries, but this is not par-
ticularly surprising as each Gaussian observation measures a contribution from every
entry in the matrix M . These Gaussian models were studied extensively in [29].
Found Comput Math (2009) 9: 717â€“772 767
Fig. 3 Recovery of matrices from Gaussian observations. For each (n,m, r) triple, we repeated the fol-
lowing procedure 10 times. In (a), a matrix of rank r was generated as in Fig. 1. In (b) a positive semidef-
inite matrix of rank r was generated as in Fig. 2. In both plots, we select a matrix A from the Gaussian
ensemble with m rows and n2 (in (a)) or Dn = n(n + 1)/2 (in (b)) columns. Then we solve the nuclear
norm minimization subject to A(X) = A(M). The color scheme for each cell denotes empirical recovery
probability and is the same as in Figs. 1 and 2
8 Discussion
8.1 Improvements
In this paper, we have shown that under suitable conditions, one can reconstruct an
n Ã— n matrix of rank r from a small number of its sampled entries provided that this
number is on the order of n1.2r logn, at least for moderate values of the rank. One
would like to know whether better results hold in the sense that exact matrix recovery
would be guaranteed with a reduced number of measurements. In particular, recall
that an n Ã— n matrix of rank r depends on (2n âˆ’ r)r degrees of freedom; is it true
then that it is possible to recover most low-rank matrices from on the order of nrâ€”
up to logarithmic multiplicative factorsâ€”randomly selected entries? Can the sample
size be merely proportional to the true complexity of the low-rank object we wish to
recover?
In this direction, we would like to emphasize that there is nothing in our ap-
proach that apparently prevents us from getting stronger results. Indeed, we devel-
oped a bound on the spectral norm of each of the first four terms (PT âŠ¥ PPT )Hk(E)
in the series (4.13) (corresponding to values of k equal to 0,1,2,3) and used a
general argument to bound the remainder of the series. Presumably, one could
bound higher order terms by the same techniques. Getting an appropriate bound on
â€–(PT âŠ¥ PPT )H4(E)â€– would lower the exponent of n from 6/5 to 7/6. The appropri-
ate bound on â€–(PT âŠ¥ PPT )H5(E)â€– would further lower the exponent to 8/7, and so
on. To obtain an optimal result, one would need to reach k of size about logn. In do-
ing so, however, one would have to pay special attention to the size of the decoupling
constants (the constant CD for two variables in Lemma 6.5) which depend on kâ€”the
number of decoupled variables. These constants grow with k and upper bounds are
known [14, 15].
768 Found Comput Math (2009) 9: 717â€“772
8.2 Further Directions
It would be of interest to extend our results to the case where the unknown matrix is
approximately low-rank. Suppose we write the SVD of a matrix M as
M =
âˆ‘
1â‰¤kâ‰¤n
Ïƒkukv
âˆ—
k,
where Ïƒ1 â‰¥ Ïƒ2 â‰¥ Â· Â· Â· â‰¥ Ïƒn â‰¥ 0 and assume for simplicity that none of the Ïƒkâ€™s vanish.
In general, it is impossible to complete such a matrix exactly from a partial subset of
its entries. However, one might hope to be able to recover a good approximation if, for
example, most of the singular values are small or negligible. For instance, consider
the truncated SVD of the matrix M ,
Mr =
âˆ‘
1â‰¤kâ‰¤r
Ïƒkukv
âˆ—
k,
where the sum extends over the r largest singular values and let M be the solution
to (1.5). Then one would not expect to have M = M but it would be of great interest
to determine whether the size of M âˆ’M is comparable to that of M âˆ’Mr provided
that the number of sampled entries is sufficiently large. For example, one would like
to know whether it is reasonable to expect that â€–M âˆ’ Mâ€–âˆ— is on the same order as
â€–M âˆ’ Mrâ€–âˆ— (one could ask for a similar comparison with a different norm). If the
answer is positive, then this would say that approximately low-rank matrices can be
accurately recovered from a small set of sampled entries.
Another important direction is to determine whether the reconstruction is robust
to noise as in some applications, one would presumably observe
Yij = Mij + zij , (i, j) âˆˆ ,
where z is a deterministic or stochastic perturbation. In this setup, one would perhaps
want to minimize the nuclear norm subject to â€–P(X âˆ’ Y )â€–F â‰¤  where  is an
upper bound on the noise level instead of enforcing the equality constraint P(X) =
P(Y ). Can one expect that this algorithm or a variation thereof provides accurate
answers? That is, can one expect that the error between the recovered and the true
data matrix be proportional to the noise level?
This scenario was considered in [3] where it was shown that most of the entries
can be reconstructed to o(1) error by computing the singular value decomposition
of the matrix which is equal to Mij when (i, j) âˆˆ  and zeros everywhere else. This
bound translates into O(n) error in the Frobenius norm. An adaptation of the analysis
to the nuclear norm minimization problem (1.5) rather than the SVD based algorithm
could lead to a better error estimate.
Finally, we note that the nuclear norm minimization problem (1.5) considered in
this paper is a highly structured semidefinite program. Standard interior point meth-
ods will be impractical for problems where the unknown matrix has hundreds of rows
or columns. However, customized solvers designed to exploit the special structure of
the nuclear norm and the sparsity of the constraints show a great deal of promise for
Found Comput Math (2009) 9: 717â€“772 769
solving incredibly large instances of the matrix completion problems. Since the date
of the original submission of this paper, preliminary research focusing on linearized
Bregman schemes [8] have already demonstrated the solution of instances of the nu-
clear norm minimization problem where the unknown matrix has tens of thousands
of columns; see also the fixed point continuation methods in [27]. Further investi-
gation into such fast algorithms would make the theoretical guarantees developed in
this work practical for very large data analysis problems.
Acknowledgements E.C. was partially supported by a National Science Foundation grant CCF-515362,
by the 2006 Waterman Award (NSF) and by an ONR grant. The authors would like to thank Ali Jadbabaie,
Pablo Parrilo, Ali Rahimi, Terence Tao, and Joel Tropp for fruitful discussions about parts of this paper.
E.C. would like to thank Arnaud Durand for his careful proofreading and comments.
Open Access This article is distributed under the terms of the Creative Commons Attribution Noncom-
mercial License which permits any noncommercial use, distribution, and reproduction in any medium,
provided the original author(s) and source are credited.
Appendix
A.1 Proof of Theorem 4.2
The proof of (4.10) follows that in [9] but we shall use slightly more precise estimates.
Let Y1, . . . , Yn be a sequence of independent random variables taking values in a
Banach space and let Y be the supremum defined as
Y = sup
f âˆˆF
nâˆ‘
i=1
f (Yi), (A.1)
where F is a countable family of real-valued functions such that if f âˆˆ F , then
âˆ’f âˆˆ F . Talagrand [35] proved a concentration inequality about Y; see also [23,
Corollary 7.8].
Theorem A.1 Assume that |f | â‰¤ B and Ef (Yi) = 0 for every f in F and i =
1, . . . , n. Then for all t â‰¥ 0,
P
(|Y âˆ’ EY| > t
)â‰¤ 3 exp
(
âˆ’ t
KB
log
(
1 + Bt
Ïƒ 2 + B EY
))
, (A.2)
where Ïƒ 2 = supf âˆˆF
âˆ‘n
i=1 Ef 2(Yi), and K is a numerical constant.
We note that very precise values of the numerical constant K are known and are
small; see [21].
We will apply this theorem to the random variable Z defined in the statement of
Theorem 4.2. Put Yab = pâˆ’1(Î´ab âˆ’ p) PT (eaeâˆ—b) âŠ— PT (eaeâˆ—b) and Y =
âˆ‘
ab Yab . By
definition,
770 Found Comput Math (2009) 9: 717â€“772
Z = sup âŒ©X1, Y (X2)
âŒª= sup
âˆ‘
ab
âŒ©
X1, Yab(X2)
âŒª
= sup pâˆ’1
âˆ‘
ab
(Î´ab âˆ’ p)
âŒ©
X1, PT (eaeâˆ—b)
âŒª âŒ©
PT
(
eae
âˆ—
b
)
,X2
âŒª
,
where the supremum is over a countable collection of matrices X1 and X2 obeying
â€–X1â€–F â‰¤ 1 and â€–X2â€–F â‰¤ 1. Note that it follows from (4.8)
âˆ£âˆ£âŒ©X1, Yab(X2)
âŒªâˆ£âˆ£= pâˆ’1 |Î´ab âˆ’ p|
âˆ£âˆ£âŒ©X1, PT
(
eae
âˆ—
b
)âŒªâˆ£âˆ£ âˆ£âˆ£âŒ©PT
(
eae
âˆ—
b
)
,X2
âŒªâˆ£âˆ£
â‰¤ pâˆ’1 âˆ¥âˆ¥PT
(
eae
âˆ—
b
)âˆ¥âˆ¥2
F
â‰¤ 2Î¼0r/
(
min(n1, n2)p
)= 2Î¼0 nr/m
(recall that n = max(n1, n2)). Hence, we can apply Theorem A.1 with B =
2Î¼0(nr/m). Also,
E
âˆ£âˆ£âŒ©X1, Yab(X2)
âŒªâˆ£âˆ£2 = pâˆ’1(1 âˆ’ p) âˆ£âˆ£âŒ©X1, PT
(
eae
âˆ—
b
)âŒªâˆ£âˆ£2 âˆ£âˆ£âŒ©X2, PT
(
eae
âˆ—
b
)âŒªâˆ£âˆ£2
â‰¤ pâˆ’1 âˆ¥âˆ¥PT
(
eae
âˆ—
b
)âˆ¥âˆ¥2
F
âˆ£âˆ£âŒ©PT (X2), eaeâˆ—b
âŒªâˆ£âˆ£2
so that
âˆ‘
ab
E
âˆ£âˆ£âŒ©X1, Yab(X2)
âŒªâˆ£âˆ£2 â‰¤ (2Î¼0 nr/m)
âˆ‘
ab
âˆ£âˆ£âŒ©PT (X2), eaeâˆ—b
âŒªâˆ£âˆ£2
= (2Î¼0 nr/m)
âˆ¥âˆ¥PT (X2)
âˆ¥âˆ¥2
F
â‰¤ 2Î¼0nr/m.
Since EZ â‰¤ 1, Theorem A.1 gives
P
(|Z âˆ’ EZ| > t)â‰¤ 3 exp
(
âˆ’ t
KB
log(1 + t/2)
)
â‰¤ 3 exp
(
âˆ’ t log 2
KB
min(1, t/2)
)
,
where we have used the fact that log(1 + u) â‰¥ (log 2) min(1, u) for u â‰¥ 0. Plugging
t = Î»
âˆš
Î¼0 nr logn
m
and B = 2Î¼0 nr/m establishes the claim.
A.2 Proof of Lemma 6.2
We shall make use of the following lemma which is an application of well-known
deviation bounds about binomial variables.
Lemma A.2 Let {Î´i}1â‰¤iâ‰¤n be a sequence of i.i.d. Bernoulli variables with
P(Î´i = 1) = p and Y =âˆ‘ni=1 Î´i . Then for each Î» > 0,
P(Y > Î» EY) â‰¤ exp
(
âˆ’ Î»
2
2 + 2Î»/3 EY
)
. (A.3)
The random variable
âˆ‘
b Î´abE
2
ab is bounded by â€–Eâ€–2âˆž
âˆ‘
b Î´ab and it thus suffices
to estimate the qth moment of Yâˆ— = maxYa where Ya =âˆ‘b Î´ab . The inequality (A.3)
Found Comput Math (2009) 9: 717â€“772 771
implies that
P(Yâˆ— > Î»np) â‰¤ n exp
(
âˆ’ Î»
2
2 + 2Î»/3 np
)
,
and for Î» â‰¥ 2, this gives P(Yâˆ— > Î»np) â‰¤ neâˆ’Î»np/2. Hence
EY
qâˆ— =
âˆ« âˆž
0
P(Yâˆ— > t)qtqâˆ’1 dt â‰¤ (2np)q +
âˆ« âˆž
2np
neâˆ’t/2 qtqâˆ’1 dt.
By integrating by parts, one can check that when q â‰¤ np, we have
âˆ« âˆž
2np
neâˆ’t/2 qtqâˆ’1 dt â‰¤ nq (2np)q eâˆ’np.
Under the assumptions of the lemma, we have nq eâˆ’np â‰¤ 1 and, therefore,
EY
qâˆ— â‰¤ 2 (2np)q.
The conclusion follows.
References
1. ACM SIGKDD, Netflix, Proceedings of KDD Cup and Workshop (2007). Proceedings available on-
line at http://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings.html.
2. T. Ando, R.A. Horn, C.R. Johnson, The singular values of a Hadamard product: A basic inequality,
Linear Multilinear Algebra 21, 345â€“365 (1987).
3. Y. Azar, A. Fiat, A. Karlin, F. McSherry, J. Saia, Spectral analysis of data, in Proceedings of the
Thirty-third Annual ACM Symposium on Theory of Computing (2001).
4. C. Beck, R. Dâ€™Andrea, Computational study and comparisons of LFT reducibility methods, in Pro-
ceedings of the American Control Conference (1998).
5. D.P. Bertsekas, A. Nedic, A.E. Ozdaglar, Convex Analysis and Optimization (Athena Scientific, Bel-
mont, 2003).
6. B. BollobÃ¡s, Random Graphs, 2nd edn. (Cambridge University Press, Cambridge, 2001).
7. A. Buchholz, Operator Khintchine inequality in non-commutative probability, Math. Ann. 319, 1â€“16
(2001).
8. J.-F. Cai, E.J. CandÃ¨s, Z. Shen, A singular value thresholding algorithm for matrix completion, Tech-
nical report (2008). Preprint available at http://arxiv.org/abs/0810.3286.
9. E.J. CandÃ¨s, J. Romberg, Sparsity and incoherence in compressive sampling, Inverse Probl. 23(3),
969â€“985 (2007).
10. E.J. CandÃ¨s, J. Romberg, T. Tao, Robust uncertainty principles: exact signal reconstruction from
highly incomplete frequency information, IEEE Trans. Inf. Theory 52(2), 489â€“509 (2006).
11. E.J. CandÃ¨s, T. Tao, Decoding by linear programming, IEEE Trans. Inf. Theory 51(12), 4203â€“4215
(2005).
12. E.J. CandÃ¨s, T. Tao, Near optimal signal recovery from random projections: Universal encoding strate-
gies?, IEEE Trans. Inf. Theory 52(12), 5406â€“5425 (2006).
13. A.L. Chistov, D.Yu. Grigoriev, Complexity of quantifier elimination in the theory of algebraically
closed fields, in Proceedings of the 11th Symposium on Mathematical Foundations of Computer Sci-
ence. Lecture Notes in Computer Science, vol. 176 (Springer, Berlin, 1984), pp. 17â€“31.
14. V.H. de la PeÃ±a, Decoupling and Khintchineâ€™s inequalities for U -statistics, Ann. Probab. 20(4), 1877â€“
1892 (1992).
15. V.H. de la PeÃ±a, S.J. Montgomery-Smith, Decoupling inequalities for the tail probabilities of multi-
variate U -statistics, Ann. Probab. 23(2), 806â€“816 (1995).
772 Found Comput Math (2009) 9: 717â€“772
16. D.L. Donoho, Compressed sensing, IEEE Trans. Inf. Theory 52(4), 1289â€“1306 (2006).
17. P. Drineas, M.W. Mahoney, S. Muthukrishnan, Subspace sampling and relative-error matrix approxi-
mation: Column-based methods, in Proceedings of the Tenth Annual RANDOM (2006).
18. P. Drineas, M.W. Mahoney, S. Muthukrishnan, Subspace sampling and relative-error matrix approxi-
mation: Column-row-based methods, in Proceedings of the Fourteenth Annual ESA (2006).
19. M. Fazel, Matrix rank minimization with applications, Ph.D. thesis, Stanford University (2002).
20. R.A. Horn, C.R. Johnson, Topics in Matrix Analysis (Cambridge University Press, Cambridge, 1994).
Corrected reprint of the 1991 original.
21. T. Klein, E. Rio, Concentration around the mean for maxima of empirical processes, Ann. Probab.
33(3), 1060â€“1077 (2005).
22. B. Laurent, P. Massart, Adaptive estimation of a quadratic functional by model selection, Ann. Stat.
28(5), 1302â€“1338 (2000).
23. M. Ledoux, The Concentration of Measure Phenomenon (AMS, Providence, 2001).
24. A.S. Lewis, The mathematics of eigenvalue optimization, Math. Programm. 97(1â€“2), 155â€“176
(2003).
25. N. Linial, E. London, Y. Rabinovich, The geometry of graphs and some of its algorithmic applications,
Combinatorica 15, 215â€“245 (1995).
26. F. Lust-Picquard, InÃ©galitÃ©s de Khintchine dans Cp (1 < p < âˆž), C. R. Acad. Sci. Paris, SÃ©r. I 303(7),
289â€“292 (1986).
27. S. Ma, D. Goldfarb, L. Chen, Fixed point and Bregman iterative methods for matrix rank minimiza-
tion, Technical report (2008).
28. M. Mesbahi, G.P. Papavassilopoulos, On the rank minimization problem over a positive semidefinite
linear matrix inequality, IEEE Trans. Automat. Control 42(2), 239â€“243 (1997).
29. B. Recht, M. Fazel, P. Parrilo, Guaranteed minimum rank solutions of matrix equations via
nuclear norm minimization, SIAM Rev. (2007, submitted). Preprint available at http://arxiv.org/
abs/0706.4138.
30. J.D.M. Rennie, N. Srebro, Fast maximum margin matrix factorization for collaborative prediction, in
Proceedings of the International Conference of Machine Learning (2005).
31. M. Rudelson, Random vectors in the isotropic position, J. Funct. Anal. 164(1), 60â€“72 (1999).
32. M. Rudelson, R. Vershynin, Sampling from large matrices: an approach through geometric functional
analysis, J. ACM, 54(4), Art. 21, 19 pp. (electronic) (2007).
33. A.M.-C. So, Y. Ye, Theory of semidefinite programming for sensor network localization, Math. Pro-
gram., Ser. B, 109, 2007.
34. N. Srebro, Learning with matrix factorizations, Ph.D. thesis, Massachusetts Institute of Technology,
(2004).
35. M. Talagrand, New concentration inequalities in product spaces, Invent. Math. 126(3), 505â€“563
(1996).
36. K.C. Toh, M.J. Todd, R.H. TÃ¼tÃ¼ncÃ¼, SDPT3â€”a MATLAB software package for semidefinite-
quadratic-linear programming. Available from http://www.math.nus.edu.sg/~mattohkc/sdpt3.html.
37. L. Vandenberghe, S.P. Boyd, Semidefinite programming, SIAM Rev. 38(1), 49â€“95 (1996).
38. G.A. Watson, Characterization of the subdifferential of some matrix norms, Linear Algebra Appl. 170,
33â€“45 (1992).

