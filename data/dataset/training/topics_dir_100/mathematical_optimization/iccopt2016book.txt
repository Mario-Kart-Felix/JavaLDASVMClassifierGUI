 
 
 
 
The Fifth International Conference on Continuous Optimization of  
the Mathematical Optimization Society 
 
 
 
 
 
 
Program and Abstracts 
 
 
 
 
 
 
 
 
ICCOPT 2016 Tokyo Organizing Committee 
The Fifth International Conference on Continuous Optimization
 of the Mathematical Optimization Society
2016_iccopt.indb   1 2016/07/22   11:58:06
2016_iccopt.indb   2 2016/07/22   11:58:06
Contents
(Venue, Registration Desk, Instructions for Presentation, Other Practical Information)
How to Find Your Session/Table of Sessions
Program at a Glance
Index of Speakers, Session Organizers, and Chairs
Back Cover
(Overview, Summer School Lectures, Plenary Lectures, Semi-Plenary Lectures, and 
Best Paper Prize Finalists)
17
30
31
33
45
114
121
2016_iccopt.indb   3 2016/07/22   11:58:06
2016_iccopt.indb   4 2016/07/22   11:58:06
 
Dear Participants, 
 
On behalf of the organizing committee and the program committee, it is our pleasure to 
welcome you to ICCOPT 2016 Tokyo, the Fifth International Conference on Continuous 
Optimization of the Mathematical Optimization Society. ICCOPT 2016 is co-sponsored by 
the Operations Research Society of Japan.  
 
ICCOPT 2016 includes a Summer School and a Conference. The Summer School (August 
6-7) is being held at the National Olympics Memorial Youth Center (NYC) and it is directed 
at PhD students and young researchers. The topics are First-order, Splitting and Related 
Methods for Big Data Optimization and Links between Continuous and Discrete 
Optimization by 4 lecturers. The Conference (August 8-11) is being held on the campus of 
the National Graduate Institute for Policy Studies (GRIPS).  The conference includes 4 
plenary and 8 semi-plenary talks. We take this opportunity to thank all of our excellent 
course instructors and featured speakers. 
 
The conference program includes over 550 talks grouped in 15 clusters. We would like to 
express our gratitude to all of the cluster co-chairs and session organizers. The program also 
has a session of poster presentations and a session for the Best Paper Prize finalists. The 
conference could not have happened without the dedicated work of many other people. We 
are grateful to all the members of the program committee, the organizing committee, and 
the best paper prize committee. We also thank to the sponsor companies.  
 
Finally, we hope that you will also enjoy our Social Program: the Welcome Reception, the 
Conference Banquet, the Student Social, and other events. 
 
We wish you a pleasant and productive meeting.  
 
Yinyu Ye                                   Shinji Mizuno  
Chair of the program committee             Chair of the organizing committee 
ICCOPT 2016 1
WELCOME
 i l s about 50 talks grouped in 15 clusters. 
2016_iccopt.indb   1 2016/07/22   11:58:06
2016_iccopt.indb   2 2016/07/22   11:58:06
540
ICCOPT 2016 3
WELCOME
2016_iccopt.indb   3 2016/07/22   11:58:07
 
 
 The Operations Research Society of Japan 
 
 
 
Dear Participants, 
 
 It is a great pleasure to host the 5th International Conference on Continuous 
Optimization (ICCOPT) 2016 at the National Graduate Institute for Policy Studies 
(GRIPS) in Tokyo, Japan from July 8 to 11, 2016. On behalf of the members of the Board 
of Trustees at GRIPS and also as the President of the Operations Research Society of 
Japan (ORSJ), I would like to express our great appreciation and sincere thanks for 
giving GRIPS the opportunity to be the venue for this year ’s conference. I firmly believe 
that with more than 500 participants from all over the world, ICCOPT 2016 will be a 
great success. 
 
 Continuous optimization is one of the major academic disciplines within the area of 
optimization, which itself has been a major research topic in the operations research 
field. Optimization theory and its applications have allowed a great many researchers 
and practitioners to challenge solving various types of both theoretical and practical 
problems, and subsequently, they have contributed on a large scale to both the private 
and public sector, including industry, business, military, public service and society as 
well. We can confidently say that continuous optimization occupies a large and 
conspicuous share in the area of optimization. 
 
 We believe that ICCOPT 2016 will surely contribute to further advancing the academic 
level within the field by providing opportunities for researchers and practitioners to 
positively and actively communicate and engage in stimulating discussions at the 
conference. 
 
 To all the participants, including both researchers and practitioners, of ICCOPT 2016, 
please continue to contribute to the field through your efforts in your own discipline 
after enjoying your participation at the conference and your short stay in Tokyo. 
 
 
 
 
 
 
Tatsuo Oyama 
President, The Operations Research Society of Japan 
Member, Board of Trustees, Professor Emeritus 
National Graduate Institute for Policy Studies 
i  okyo, Japan from August 8 to 1 , 2016. On behalf of the members of the
Boa d of Trustees at GRIPS and also as the President of he Op rations Res arch
Society of Japan (ORSJ), I w uld like to express our great appreciation and sincere
tha ks for giving GRIPS the oppor unity to be the venue for this year ’s conferenc . I
firmly believe that with mo e than 500 participants from a l over the world, ICCOPT
2016 will be a great success. 
2016_iccopt.indb   4 2016/07/22   11:58:07
 
 
 The Operations Research Society of Japan 
 
 
 
Dear Participants, 
 
 It is a great pleasure to host the 5th International Conference on Continuous 
Optimization (ICCOPT) 2016 at the National Graduate Institute for Policy Studies 
(GRIPS) in Tokyo, Japan from July 8 to 11, 2016. On behalf of the members of the Board 
of Trustees at GRIPS and also as the President of the Operations Research Society of 
Japan (ORSJ), I would like to express our great appreciation and sincere thanks for 
giving GRIPS the opportunity to be the venue for this year ’s conference. I firmly believe 
that with more than 500 participants from all over the world, ICCOPT 2016 will be a 
great success. 
 
 Continuous optimization is one of the major academic disciplines within the area of 
optimization, which itself has been a major research topic in the operations research 
field. Optimization theory and its applications have allowed a great many researchers 
and practitioners to challenge solving various types of both theoretical and practical 
problems, and subsequently, they have contributed on a large scale to both the private 
and public sector, including industry, business, military, public service and society as 
well. We can confidently say that continuous optimization occupies a large and 
conspicuous share in the area of optimization. 
 
 We believe that ICCOPT 2016 will surely contribute to further advancing the academic 
level within the field by providing opportunities for researchers and practitioners to 
positively and actively communicate and engage in stimulating discussions at the 
conference. 
 
 To all the participants, including both researchers and practitioners, of ICCOPT 2016, 
please continue to contribute to the field through your efforts in your own discipline 
after enjoying your participation at the conference and your short stay in Tokyo. 
 
 
 
 
 
 
Tatsuo Oyama 
President, The Operations Research Society of Japan 
Member, Board of Trustees, Professor Emeritus 
National Graduate Institute for Policy Studies 
i  okyo, Japan from August 8 to 1 , 2016. On behalf of the members of the
Boa d of Trustees at GRIPS and also as the President of he Op rations Res arch
Society of Japan (ORSJ), I w uld like to express our great appreciation and sincere
tha ks for giving GRIPS the oppor unity to be the venue for this year ’s conferenc . I
firmly believe that with mo e than 500 participants from a l over the world, ICCOPT
2016 will be a great success. 
ICCOPT 2016 5
WELCOME
2016_iccopt.indb   5 2016/07/22   11:58:08
Dear Participants in the ICCOPT 2016: 
 
On behalf of the faculty, staff, and students of the National Graduate Institute for Policy Studies (GRIPS), 
I am pleased to welcome you to the ICCOPT 2016. 
Founded in 1997 as a stand-alone graduate institute, GRIPS has been providing interdisciplinary education 
for future leaders in the public sector and conducting research on contemporary policy issues to generate 
innovative solutions to challenging policy problems. Today, GRIPS is an international premier school of 
public policy comprised of world-class academics and distinguished practitioners with expertise in 
public-sector policy formulation and management. The vast majority of our students are mid-career public 
officials with strong leadership skills and managerial potential. Of the 400 students currently enrolled, 
approximately 70 percent are recruited from outside Japan. Our alumni network is a vibrant community of 
over 4,000 graduates actively shaping policy in more than 100 countries.   
The world is faced with new, complex problems, such as terrorism, global migration, environmental 
pollution, energy shortages, and financial crises. Addressing these problems requires a new type of leader, 
one who embraces change, who is capable of using the latest knowledge and research tools to meet new 
challenges, and who can harness innovative collaborations to forge a path to a new age. As a school that 
aims at training a diverse pool of talented public officials to become such leaders, we are very proud to 
host the ICCOPT 2016, and we hope that the unique insights stemming from the conference will enhance 
the teaching and research capabilities not only of scholars working in the field of optimization but also of 
researchers and professionals working in many other fields.   
The ICCOPT 2016 is taking place in Roppongi, the very heart of Tokyo. I am happy to welcome you on 
our beautiful campus, which was designed by Richard Rogers, a prominent architect known for his work 
on the Pompidou Centre in Paris. I am confident that you will find our campus a welcoming and enjoyable 
place and I hope that you will use this opportunity to explore a fascinating mixture of traditional and 
modern Japan that Roppongi has to offer. 
I would like to thank the members of the local organizing committee for their dedicated efforts in 
organizing the conference and I sincerely wish all of you every success. I hope that you will view this 
conference not only as a forum for sharing ideas about research and practice in the field of optimization 
but also as an opportunity to establish and strengthen ties with colleagues from many institutions and 
countries.  
 
 
 
 
Mikitaka Masuyama 
Professor and Dean 
2016_iccopt.indb   6 2016/07/22   11:58:08
Dear Participants in the ICCOPT 2016: 
 
On behalf of the faculty, staff, and students of the National Graduate Institute for Policy Studies (GRIPS), 
I am pleased to welcome you to the ICCOPT 2016. 
Founded in 1997 as a stand-alone graduate institute, GRIPS has been providing interdisciplinary education 
for future leaders in the public sector and conducting research on contemporary policy issues to generate 
innovative solutions to challenging policy problems. Today, GRIPS is an international premier school of 
public policy comprised of world-class academics and distinguished practitioners with expertise in 
public-sector policy formulation and management. The vast majority of our students are mid-career public 
officials with strong leadership skills and managerial potential. Of the 400 students currently enrolled, 
approximately 70 percent are recruited from outside Japan. Our alumni network is a vibrant community of 
over 4,000 graduates actively shaping policy in more than 100 countries.   
The world is faced with new, complex problems, such as terrorism, global migration, environmental 
pollution, energy shortages, and financial crises. Addressing these problems requires a new type of leader, 
one who embraces change, who is capable of using the latest knowledge and research tools to meet new 
challenges, and who can harness innovative collaborations to forge a path to a new age. As a school that 
aims at training a diverse pool of talented public officials to become such leaders, we are very proud to 
host the ICCOPT 2016, and we hope that the unique insights stemming from the conference will enhance 
the teaching and research capabilities not only of scholars working in the field of optimization but also of 
researchers and professionals working in many other fields.   
The ICCOPT 2016 is taking place in Roppongi, the very heart of Tokyo. I am happy to welcome you on 
our beautiful campus, which was designed by Richard Rogers, a prominent architect known for his work 
on the Pompidou Centre in Paris. I am confident that you will find our campus a welcoming and enjoyable 
place and I hope that you will use this opportunity to explore a fascinating mixture of traditional and 
modern Japan that Roppongi has to offer. 
I would like to thank the members of the local organizing committee for their dedicated efforts in 
organizing the conference and I sincerely wish all of you every success. I hope that you will view this 
conference not only as a forum for sharing ideas about research and practice in the field of optimization 
but also as an opportunity to establish and strengthen ties with colleagues from many institutions and 
countries.  
 
 
 
 
Mikitaka Masuyama 
Professor and Dean 
ICCOPT 2016 7
WELCOME
2016_iccopt.indb   7 2016/07/22   11:58:08
2016_iccopt.indb   8 2016/07/22   11:58:08
ORGANIZERS
ICCOPT 2016 9
ORGANIZING COMMITTEE
SHINJI MIZUNO (Chair)
Tokyo Institute of Technology, Japan
MASAO FUKUSHIMA 
Nanzan University, Japan
SATOSHI ITO 
The Institute of Statistical Mathematics, Japan
SATORU IWATA 
The University of Tokyo, Japan
NAOKI KATOH 
Kyoto University, Japan
HIDEFUMI KAWASAKI
Kyushu University, Japan
MASAKAZU KOJIMA
Chuo University, Japan
TOMOMI MATSUI
Tokyo Institute of Technology, Japan
HOZUMI MOROHOSI
National Graduate Institute for Policy Studies, Japan
MASAKAZU MURAMATSU
The University of Electro-Communications, Japan
KAZUO MUROTA
Tokyo Metropolitan University, Japan
TATSUO OYAMA
National Graduate Institute for Policy Studies, Japan
TAMAKI TANAKA
Niigata University, Japan
TETSUZO TANINO
Osaka University, Japan
TAKASHI TSUCHIYA
National Graduate Institute for Policy Studies, Japan
HIROSHI YABE
Tokyo University of Science, Japan
AKIKO YOSHISE
University of Tsukuba, Japan
PROGRAM COMMITTEE
YINYU YE (Chair)
Stanford University, USA
LORENZ T (Larry) BIEGLER 
Carnegie Mellon University, USA
IMMANUEL BOMZE 
University of Vienna, Austria
MASAO FUKUSHIMA 
Nanzan University, Japan
ASUMAN OZDAGLAR 
Massachusetts Institute of Technology, USA
MIKHAIL V SOLODOV
Instituto de Matematica Pura e Aplicada, Brazil
DEFENG SUN 
National University of Singapore, Singapore
YA-XIANG YUAN 
Chinese Academy of Sciences, China
SHINJI MIZUNO 
Tokyo Institute of Technology, Japan
2016_iccopt.indb   9 2016/07/22   11:58:08
2016_iccopt.indb   10 2016/07/22   11:58:09
ORGANIZERS
ICCOPT 2016 11
LOCAL ORGANIZING COMMITTEE
MASAKAZU MURAMATSU (Co-Chair)
The University of Electro-Communications, Japan
TAKASHI TSUCHIYA (Co-Chair)
National Graduate Institute for Policy Studies, Japan
SHINJI MIZUNO (Co-Chair)
Tokyo Institute of Technology, Japan
KATSUKI FUJISAWA
Kyushu University, Japan
MITUHIRO FUKUDA
Tokyo Institute of Technology, Japan
JUN-YA GOTOH
Chuo University, Japan
TOMONARI KITAHARA
Tokyo Institute of Technology, Japan
BRUNO FIGUEIRA LOURENÇO
Seikei University, Japan
TOMOHIKO MIZUTANI
Tokyo Institute of Technology, Japan
KAZUHIDE NAKATA
Tokyo Institute of Technology, Japan
YASUSHI NARUSHIMA
Yokohama National University, Japan
TAKAYUKI OKUNO
Tokyo University of Science, Japan
SATOSHI TAKAHASHI
The University of Electro-Communications, Japan
YUICHI TAKANO
Senshu University, Japan
AKIKO TAKEDA
The Institute of Statistical Mathematics, Japan
MIRAI TANAKA
Tokyo University of Science, Japan
MAKOTO YAMASHITA
Tokyo Institute of Technology, Japan
2016_iccopt.indb   11 2016/07/22   11:58:09
2016_iccopt.indb   12 2016/07/22   11:58:09
ICCOPT 2016 13
CLUSTERS AND CLUSTER CO-CHAIRS
Applications in Energy, Science and Engineering
NEDIALKO B DIMITROV (The University of Texas at Austin, USA)
TOSHIYUKI OHTSUKA (Kyoto University, Japan)
ASGEIR TOMASGARD (Norwegian University of Science and Technology, Norway)
VICTOR ZAVALA (University of Wisconsin-Madison, USA)
Applications in Finance and Economics
WOO CHANG KIM (Korea Advanced Institute of Science and Technology, Korea)
SHUSHANG ZHU (Sun Yat-Sen University, China)
Complementarity and Variational Inequalities
UDAY V SHANBHAG (Pennsylvania State University, USA)
JIE SUN (Curtin University, Australia)
Conic and Polynomial Optimization
ETIENNE DE KLERK (Tilburg University, Netherlands)
JEAN B LASSERRE (LAAS-CNRS, France)
Convex and Nonsmooth Optimization
JALAL FADILI (Ecole Nationale Supérieure dʼIngénieurs de Caen, France)
KIM CHUAN TOH (National University of Singapore, Singapore)
Derivative-free and Simulation-based Optimization
FRANCESCO RINALDI (University of Padova, Italy)
ZAIKUN ZHANG (Hong Kong Polytechnic University, Hong Kong)
Global Optimization
CHRIS FLOUDAS (Texas A&M University, USA)
NIKOLAOS SAHINIDIS (Carnegie Mellon University, USA)
Linear Optimization
ANTOINE DEZA (McMaster University, Canada)
TAMÁS TERLAKY (Lehigh University, USA)
Multi-Objective and Vector Optimization
VLADIMIR SHIKHMAN (Université catholique de Louvain, Belgium)
TAMAKI TANAKA (Niigata University, Japan)
Nonlinear Optimization
SERGE GRATTON (Intitut National Polytechnique de Toulouse, France)
DANIEL P ROBINSON (Johns Hopkins University, USA)
Optimization Implementations and Software
CARL LAIRD (Purdue University, USA)
JOHN SIIROLA (Sandia National Laboratories, USA)
PDE-constrained Optimization
KAZUFUMI ITO (North Carolina State University, USA)
MICHAEL ULBRICH (Technische Universität München, Germany)
Robust Optimization
TIMOTHY CHAN (University of Toronto, Canada)
DANIEL KUHN (Ecole Polytechnique Fédérale de Lausanne, Switzerland)
Sparse Optimization and Information Processing
CORALIA CARTIS (University of Oxford, United Kingdom)
YUESHENG XU (Syracuse University, USA)
Stochastic Optimization
ANDRZEJ RUSZCZYŃSKI (Rutgers University, USA)
HUIFU XU (University of Southampton, United Kingdom)
ORGANIZERS
2016_iccopt.indb   13 2016/07/22   11:58:09
2016_iccopt.indb   14 2016/07/22   11:58:09
ICCOPT 2016 15
INFORMATION
GENERAL INFORMATION
VENUE AND ACCESS
ICCOPT 2016 Tokyo consists of a conference and a summer school.  The conference is held at National Graduate Institute for Policy 
Studies (GRIPS) from August 8 (Mon) to 11 (Thu), 2016.  Some sessions are held at the National Art Center, Tokyo, a national art 
museum next door to GRIPS.  The summer school is held at the Seminar Hall 417 of the Central Building of National Olympics 
Memorial Youth Center (NYC) on August 6 (Sat) and 7 (Sun), 2016.  Special accommodation is arranged at NYC by the organizing 
committee for participantsʼ convenience.
National Graduate Institute for Policy Studies  (Conference Venue)
GRIPS is the newest national (graduate) university founded in 1997 and is now located in Roppongi, the core center of entertainment, 
contemporary culture and art of Japan.  About a hundred professors and four hundred international/domestic students are engaged in 
the study and research of policy sciences. 
See pages 121 and 125 for access to GRIPS from NARITA/HANEDA airports.  A floor map of the venue is located on pages 122–124.
  ・Homepage: http://www.grips.ac.jp/en/
  ・Access: http://www.grips.ac.jp/en/about/access/
National Olympics Memorial Youth Center (Summer School Venue)
NYC is located in Yoyogi, neighboring Meiji Shrine and Yoyogi Park.  NYC was redeveloped at the site of the Olympic village of 
1964 Tokyo Olympic Games as a public facility for study and training equipped with accommodation.  From there, GRIPS is within 
7 minutes by metro (25 minutes including walk from/to stations).
See pages 125 and 126 for access to NYC from NARITA/HANEDA airports.  The location of the Central Building and how to get to 
GRIPS from NYC are also explained.
  ・Homepage: http://nyc.niye.go.jp/en/
  ・Access: http://nyc.niye.go.jp/category/english/access-english/
Further Information
Please check out http://www.iccopt2016.tokyo/travel/index.html for more detailed information about how to get to GRIPS and NYC 
from NARITA/HANEDA airports.  The page also contains useful travel tips in Japan.
REGISTRATION DESK
The registration desk of the Conference is located at the foyer of Soukairou Hall on the first floor (see the map on page 123), and will 
be opened during:
The registration desk of the Summer School is located in front of the Seminar Hall 417 at the Central Building of NYC.  If you 
complete registration at the Summer School venue, you do not have to do it again at the conference venue.  The desk will be opened 
during:
August 7 15:00–19:00
August 8 8:15–19:00
August 9 8:30–19:00
August 10 8:30–18:30
August 11 8:30–16:00
August 6 9:00–17:00
August 7 9:00–12:00
2016_iccopt.indb   15 2016/07/22   11:58:09
ICCOPT 201616
INFORMATION
INSTRUCTIONS FOR PRESENTATION
Instructions to Speakers and Chairs  (Session Structure and Length of Talk)
Each parallel session is 75-minute long, and has three speaker-spots. To allow attendees to do “session jumping,” each speaker is asked 
to finish his/her talk within 25 minutes including questions and discussion time and follow the order of the presentations, as shown in 
pages 33 to 44, even if the session has only one or two speakers. Even if the first or second speaker does not show up in the session, 
the session chairs are requested not to slide the following talks to the empty spot.
Instructions to Speakers  (Presentation Device)
　1.  All session rooms will be equipped with a computer projector and a VGA cable. We ask you to bring your own presentation 
device (laptop/tablet computer) to your session rooms and connect it to the projector with the VGA cable on your own or pre-
arrange to share with others in your sessions. Note that we will not prepare the other type connectors, adapters, or pointers. 
Especially if your laptop is an Apple product or a tablet, you will need appropriate adapters for the external video output.
　Please bring a power adapter. We recommend that you do not attempt to run your presentation off the laptop battery. If your 
laptop is not compatible with AC power, please bring an electrical adapter so you can connect to Japanese electricity. Note that 
the type of the electrical plug in Japan is Type A and the voltage is 100V.
　2. We also ask you to bring an electronic copy of your presentation with a USB flash drive as a backup.
　3.  Please check if you can connect your presentation device without trouble in advance before the session starts. If you have a 
problem, you can ask for help from staff. (They are wearing yellow vest of ICCOPT 2016 Tokyo and are always around the 
rooms where the sessions are going on.)
Instructions for Poster Presentation
Please put up your poster on a board placed at the Foyer on the 1st floor by 17:30, Monday August 8, before the poster session starts. 
The location of your board will be indicated at the site. You can put up your poster from 14:30. Poster boards are 90cm wide and 
180cm height. Please use tape to hang your poster. Tape will be provided by us.
OTHER PRACTICAL INFORMATION
Internet Connection
Free WiFi GRIPS SPOT is available inside the building of GRIPS.  eduroam is also available.  Public free WiFi is also available at 
the National Art Center.  On the other hand, wireless connection at the Summer School venue is limited.  NYC does not have their 
own wireless connection for public users.  We will set up several mobile WiFi routers at the Seminar hall 417 for convenience of 
lecturers and participants.  SSID and password will be announced at the venue.
Restaurants
There are many nice restaurants around GRIPS.  The National Art Center also has a cafeteria (B1), a coffee shop (1F), a tea salon 
(2F), and a restaurant (3F). 
Exhibits
Exhibits take place at the Foyer on the 1st floor of GRIPS.
US Army Facilities
A US army heliport and other army facilities are located just on the west side of GRIPS.  You are kindly asked to refrain from taking 
photos.
Checking in to NYC Accommodation  (For guests)
Please receive your room key at the following time and place.  More detailed information will be e-mailed to all the guests at NYC 
accommodation about one week before their arrival.
From August 4th to 8th
Exhibition of Renoir at the National Art Center, Tokyo
A large exhibition of the famous impressionist Pierre-Auguste Renoir is held at the National Art Center during the conference.  You 
are welcome to visit and enjoy great pictures! (but please donʼt miss talks and sessions :-))
16:00 – 22:00 Entrance Hall, Central Building 1st Floor
22:00 – Lodging Building D 
2016_iccopt.indb   16 2016/07/22   11:58:09
ICCOPT 2016 17
SCIENTIFIC PROGRAM OVERVIEW
SUMMER SCHOOL SCHEDULE
 SATURDAY, AUGUST 6th
 09:00 School and Conference Registration
 09:45 Opening Remarks
 10:00 Lectures by Prof. Friedlander
  (30 min. break included)
 13:00 Lunch
 14:30 Lectures by Prof. Toh
  (30 min. break included, and end at 17:30)
 19:00 Summer School Dinner (ends at 21:00)
 SUNDAY, AUGUST 7th
 09:00 School and Conference Registration
 09:45 Opening Remarks
 10:00 Lectures by Prof. Deza
  (30 min. break included)
 13:00 Lunch
 14:30 Lectures by Prof. Murota
  (30 min. break included, and end at 17:30)
 18:30 (Welcome Reception of ICCOPT at GRIPS, Roppongi)
SCIENTIFIC PROGRAM
SUMMER SCHOOL
The Summer School lectures will take place at the Seminar Hall of the National Olympics Memorial Youth Center (NYC, for short), 
Yoyogi, on August 6 and 7. (See pages 125–126 for the access map to NYC.) The attendees at the School are required to make 
registration and payment of fee online or on site and to check in at the Registration Desk, located in front of the Seminar Hall 417 in 
the Central Building of NYC, and receive a bag containing several conference materials. The ICCOPT badge, which is contained in 
the bag, must be worn to the School as well as to all sessions and events of the Conference. (The attendees who will attend only the 
Conference can check in or make on-site registration at the Registration Desk of GRIPS. See “Registration Desk” on page 15.)
CONFERENCE
The Conference will be held on August 8 to 11 at the National Graduate Institute for Policy Studies (GRIPS, for short). Roppongi. 
and the National Art Center, Tokyo (NACT, for short), located in front of GRIPS. (See pages 121 and 125 for the access map to 
GRIPS.) The four-day conference consists of four Plenary and eight Semi-plenary Talks, one Best Paper Prize (BPP) session, twelve 
Parallel Session slots, one Poster Session, as well as a few social events including Conference Banquet (optional) and Welcome 
Receptions.
◦The Opening, all the Plenary Talks, the BPP Session, and the Closing will take place at Soukairou Hall (“1S” for short), which is 
on the 1st floor of GRIPS. Those plenary events are going to be simulcast live to several nearby rooms, such as Meeting Rooms 
1A, 1B, and 1C, and Cafeteria.
◦Poster Session will be carried out in the foyer on the 1st floor of GRIPS. Refreshment will be served to all participants during 
the Poster Session.
◦Semi-plenary Talks will take place either at Soukairou Hall or at the Auditorium of NACT (“m3S” for short), located in front of 
GRIPS. Meeting Rooms 1A, 1B, and 1C on the 1st floor of GRIPS may also be used for multicasting Semi-plenary Talks at 
Soukairou Hall. 
◦All the Parallel Sessions, whether organized or contributed talks, will be in GRIPS or in NACT. 
See pages 33–44 for the detailed scheduling and pages 122–124 for the layout of the rooms. See page 30 for the detailed information 
on the Conference Banquet and the Welcome Reception.
GENERAL INFORMATION
2016_iccopt.indb   17 2016/07/22   11:58:09
ICCOPT 201618
SCIENTIFIC PROGRAM OVERVIEW
See page 31 for the entire structure of the sessions and pages 33–44 for the detailed schedule of parallel sessions.
CONFERENCE SCHEDULE
 SUNDAY, AUGUST 7th
 15:00 Conference Registration (closes at 19:00)
 18:30 Welcome Reception (ends at 20:30)
 MONDAY, AUGUST 8th
 09:00 Opening (1S + nearby rooms)
 09:15 Plenary Talk by Prof. Zhang (1S + nearby rooms)
 10:15 Coffee Break
 10:45 Parallel Sessions Mon.A.xx
 12:00 Lunch (on your own)
 13:30 Parallel Sessions Mon.B.xx
 14:45 Coffee Break
 15:15 Semi-plenary Talks by Profs. Dür (1S) and Uhler (m3S)
 16:00 Break
 16:15 BPP Session (1S + nearby rooms)
 17:30 Poster Session and Reception (end at 19:30; Foyer)
 TUESDAY, AUGUST 9th
 09:00 Plenary Talk by Prof. Bach (1S + nearby rooms)
 10:00 Coffee Break
 10:30 Parallel Sessions Tue.A.xx
 11:45 Lunch (on your own)
 13:15 Parallel Sessions Tue.B.xx
 14:30 Break
 14:45 Parallel Sessions Tue.C.xx
 16:00 Coffee Break
 16:30 Parallel Sessions Tue.D.xx (end at 17:45)
 19:00 Conference Banquet (ends at 22:00)
 WEDNESDAY, AUGUST 10th
 09:00 Plenary Talk by Prof. Jarre (1S + nearby rooms)
 10:00 Coffee Break
 10:30 Semi-plenary Talks by Profs. Hazan (1S) and Dai (m3S)
 11:30 Semi-plenary Talks by Profs. Fujisawa (1S) and Delage (m3S)
 12:15 Lunch (on your own)
 13:45 Parallel Sessions Wed.A.xx
 15:00 Break
 15:15 Parallel Sessions Wed.B.xx
 16:30 Coffee Break
 17:00 Parallel Sessions Wed.C.xx (ends at 18:15)
 18:30 Student Social (ends at 20:30; Cafeteria)
 THURSDAY, AUGUST 11th
 09:00 Parallel Sessions Thu.A.xx
 10:15 Coffee Break
 10:45 Parallel Sessions Thu.B.xx
 12:00 Lunch (on your own)
 13:30 Parallel Sessions Thu.C.xx
 14:45 Coffee Break
 15:15 Semi-plenary Talks by Profs. Kelner (1S) and Ward (m3S)
 16:00 Break
 16:15 Plenary Talk by Prof. Pang (1S + nearby rooms)
 17:15 Closing (ends at 17:30)
2016_iccopt.indb   18 2016/07/22   11:58:09
ICCOPT 2016 19
SUMMER SCHOOL LECTURES
SUMMER SCHOOL LECTURES AND LECTURERS
Michael Friedlander
University of British Columbia, Canada
Level-Set Methods for Convex Optimization 
Saturday, August 6, 10:00–13:00, Seminar Hall 417 (NYC)
Certain classes of convex optimization problems have computationally favorable objectives but complicating constraints that render 
established algorithms ineffectual for large-scale problems; these often arise in machine learning and signal processing applications. 
Level-set methods are a group of techniques for transforming a constrained problem into a sequence of simpler subproblems that are 
amenable to standard algorithms. The theoretical and computational benefits of the approach are many. This tutorial lays out the basic 
theoretical background needed to understand the approach, including duality and its connection to properties of the optimal-value 
function, and presents the main algorithmic tools needed to solve the subproblems. Various applications and problem formulations are 
used throughout to highlight key aspects of the level-set approach. [Based on joint work with A. Aravkin, J. Burke, D. Drusvyatskiy, 
and S. Roy.]
Michael Friedlander is IBM Professor of Computer Science and Professor of Mathematics at the University of 
British Columbia. He received his PhD in Operations Research from Stanford University in 2002, and his BA in 
Physics from Cornell University in 1993. From 2002 to 2004 he was the Wilkinson Fellow in Scientific Computing 
at Argonne National Laboratory. He has held visiting positions at UCLAʼs Institute for Pure and Applied 
Mathematics (2010), and at Berkeleyʼs Simons Institute for the Theory of Computing (2013).  His research is 
primarily in developing numerical methods for large-scale optimization, their software implementation, and 
applying these to problems in signal processing and machine learning. 

Kim-Chuan Toh
National University of Singapore, Singapore
Large Scale Convex Composite Optimization: Duality, Algorithms and Implementations
Saturday, August 6, 14:30–17:30, Seminar Hall 417 (NYC)
Convex composite optimization problems arise frequently in a wide variety of domains including machine learning, statistics, signal 
processing, semidefinite programming, etc. Typically, the problems are large scale and their underlying structures must be fully 
exploited in the algorithms designed to solve them. In this tutorial, we will survey some basic tools for designing efficient and robust 
algorithms for large scale convex composite optimization problems. In particular, we will describe augmented Lagrangian based 
algorithms which can be designed to solve those problems, and discuss how the subproblems can be solved efficiently by a semismooth 
Newton-CG method. While it is popularly believed that first-order methods are the most appropriate framework for solving those 
large scale problems, we shall convincingly demonstrate that for difficult problems, incorporating the second-order information 
wisely into the algorithmic design is necessary for achieving reasonably good accuracy and computational efficiency. (This talk is 
based on a paper co-authored by Prof. Defeng Sun (National University of Singapore, Singapore).)
Dr Toh is a Professor at the Department of Mathematics, National University of Singapore (NUS). He obtained 
his BSc degree in Mathematics from NUS in 1990 and the PhD degree in Applied Mathematics from Cornell 
University in 1996 under the direction of Nick Trefethen. He is currently an Area Editor for Mathematical 
Programming Computation, and an Associate Editor for the SIAM Journal on Optimization. He also serves as the 
secretary of the SIAM Activity Group on Optimization. He has been invited to speak at numerous conferences 
and workshops, including SIAM Annual Meeting in 2010 and ISMP in 2006. His current research focuses on 
designing efficient algorithms and software for convex programming, particularly large scale optimization 
problems arising from data science and large scale matrix optimization problems such as linear semidefinite 
programming (SDP) and convex quadratic semidefinite programming (QSDP). 
2016_iccopt.indb   19 2016/07/22   11:58:09
ICCOPT 201620
SUMMER SCHOOL LECTURES
Antoine Deza
McMaster University, Canada
Algorithmic and Geometric Aspects of Combinatorial and Continuous Optimization
Sunday, August 7, 10:00–13:00, Seminar Hall 417 (NYC)
The question of whether a linear optimization problem can be solved in strongly polynomial time - that is, the existence of an 
algorithm independent from the input data length and polynomial in the number of constrains and the dimension - is listed by Fields 
medalist Smale as one of the top mathematical problems for the XXI century. The simplex and primal-dual interior point methods are 
the most computationally successful algorithms for linear optimization. While simplex methods follow an edge path, interior point 
methods follow the central path. The curvature of a polytope, defined as the largest possible total curvature of the associated central 
path, can be regarded as a continuous analogue of its diameter. We review results and conjectures dealing with the combinatorial, 
geometric, and algorithmic aspects of linear optimization. In particular, we highlight links between the edge and central paths, and 
between the diameter and the curvature. We recall continuous results of Dedieu, Malajovich, and Shub, and discrete results of Holt, 
Klee, and Walkup, and related conjectures including the Hirsch conjecture that was disproved by Santos. We present results dealing 
with curvature and diameter of polytopes, such as a counterexample to a continuous analogue of the polynomial Hirsch conjecture by 
Allamigeon, Benchimol, Gaubert, and Joswig, a strengthening of the Kleinschmidt and Onn upper bound for the diameter of lattice 
polytopes by Del Pia and Michini, and the strengthening of the Kalai and Kleitman upper bound for the diameter of polytopes by 
Kitahara, Sukegawa, and Todd.
Since 2004, Antoine Deza has been at McMaster University where he has held a Canada Research Chair in 
Combinatorial Optimization. Since 2008, he has been the Head of the Advanced Optimization Laboratory. He 
had previously held a faculty position at the Tokyo Institute of Technology. He has been the Chair of the Fields 
Institute Industrial Optimization Seminar since 2008, a co-organizer of several conferences including the 2011 
Fields Institute Thematic Program on Discrete Geometry and Applications, an Associate Editor for Discrete 
Applied Mathematics, Optimization Letters, and the Journal of Discrete Algorithms. He was elected a Fields 
Institute Fellow in the 2014. He has held visiting positions at Ecole Polytechnique Federale de Lausanne, 
Technion Haifa, Tokyo Institute of Technology, Universite Paris Sud, where he holds a Digiteo Chair in Combinatorics and 
Optimization, Universite Pierre et Marie Curie, and Ecole Nationale des Ponts et Chaussees, Paris.

Kazuo Murota
Tokyo Metropolitan University, Japan
Convex Analysis Approach to Discrete Optimization
Sunday, August 7, 14:30–17:30, Seminar Hall 417 (NYC)
Discrete convex analysis is a theory that aims at a discrete analogue of convex analysis for nonlinear discrete optimization. We first 
introduce fundamental classes of discrete convex functions, including submodular functions, integrally convex functions, M-convex 
functions and L-convex functions. Emphasis is put on the relation between submodularity and convexity/concavity.
Next we explain fundamental properties of discrete convex functions, including:
(i) Operations such as addition, scaling, convolution and transformation by networks,
(ii) Conjugacy relation between L-convexity and M-convexity under Legendre transformation, and
(iii) Duality theorems such as separation theorem and Fenchel-type minimax theorem.
Then we go on to algorithmic aspects of minimization of discrete convex functions, including:
(i) Local optimality criterion for global minimality, which varies with types of discrete convexity,
(ii) Descent algorithms for M-convex and L-convex functions, and
(iii) M-convex intersection algorithm for minimizing the sum of two M-convex functions. 
Kazuo Murota is a professor at School of Business Administration, Faculty of Urban Liberal Arts, Tokyo 
Metropolitan University, where he has been since 2015. He received his Doctor of Engineering from University 
of Tokyo in 1983 and Doctor of Science from Kyoto University in 2002. Murotaʼs research interests is broad in 
mathematical engineering, in particular, discrete and continuous optimization (discrete convex analysis), 
combinatorial matrix theory (mixed matrices), numerical analysis, and economic geography. He is the author of 
five English books, including “Discrete Convex Analysis” and “Systems Analysis by Graphs and Matroids.” He 
was awarded Inoue Prize for Science in 2004.
2016_iccopt.indb   20 2016/07/22   11:58:10
ICCOPT 2016 21
PLENARY TALKS
PLENARY TALKS AND SPEAKERS
Shuzhong Zhang
University of Minnesota, USA
Variants of the ADMM and Their Convergence Properties 
Monday, August 8, 9:15–10:15, Soukairou Hall (1S, GRIPS)
Chair: Tamás Terlaky
The alternating direction method of multipliers (ADMM) proved to be a remarkably stable and effective solution approach to 
solve many structured convex optimization models. In the past few years, an intensive stream of research effort has been paid 
to the performance of the ADMM and its many variants designed to accommodate various formulations arising from 
applications. In this talk, we shall survey several aspects of the afore-mentioned research effort, and present some new results 
including the convergence of a randomized multi-block ADMM. 
Shuzhong Zhang is Professor and Head of Department of Industrial and System Engineering, University of 
Minnesota. He received a B.Sc. degree in Applied Mathematics from Fudan University in 1984, and a Ph.D 
degree in Operations Research and Econometrics from the Tinbergen Institute, Erasmus University, in 1991. He 
had held faculty positions at Department of Econometrics, University of Groningen (1991-1993), and Econometric 
Institute, Erasmus University (1993-1999), and Department of Systems Engineering & Engineering Management, 
The Chinese University of Hong Kong (1999-2010). He received the Erasmus University Research Prize in 
1999, the CUHK Vice-Chancellor Exemplary Teaching Award in 2001, the SIAM Outstanding Paper Prize in 
2003, the IEEE Signal Processing Society Best Paper Award in 2010, and the 2015 SPS Signal Processing 
Magazine Best Paper Award. Dr. Zhang was an elected Council Member at Large of the MPS (Mathematical Programming Society) 
(2006-2009), and served as Vice-President of the Operations Research Society of China (ORSC) (2008-2012). He serves on the 
Editorial Board of several academic journals, including Operations Research, and Management Science.

Francis Bach
INRIA, France
Linearly-convergent Stochastic Gradient Algorithms
Tuesday, August 9, 9:00–10:00, Soukairou Hall (1S, GRIPS)
Chair: Akiko Takeda
Many machine learning and signal processing problems are traditionally cast as convex optimization problems where the 
objective function is a sum of many simple terms. In this situation, batch algorithms compute gradients of the objective 
function by summing all individual gradients at every iteration and exhibit a linear convergence rate for strongly-convex 
problems. Stochastic methods rather select a single function at random at every iteration, classically leading to cheaper 
iterations but with a convergence rate which decays only as the inverse of the number of iterations. In this talk, I will present 
the stochastic averaged gradient (SAG) algorithm which is dedicated to minimizing finite sums of smooth functions; it has a 
linear convergence rate for strongly-convex problems, but with an iteration cost similar to stochastic gradient descent, thus 
leading to faster convergence for machine learning problems. I will also mention several extensions, in particular to saddle-
point problems, showing that this new class of incremental algorithms applies more generally.
Francis Bach is a researcher at Inria, leading since 2011 the Sierra project-team, which is part of the Computer 
Science Department at Ecole Normale Superieure. He completed his Ph.D. in Computer Science at U.C. Berkeley, 
working with Professor Michael Jordan, and spent two years in the Mathematical Morphology group at Ecole des 
Mines de Paris, then he joined the Willow project-team at INRIA/Ecole Normale Superieure from 2007 to 2010. 
Francis Bach is interested in statistical machine learning, and especially in graphical models, sparse methods, 
kernel-based learning, convex optimization, vision and signal processing. He obtained in 2009 a Starting Grant 
from the European Research Council and received in 2012 the INRIA young researcher prize. In 2015, he was 
program co-chair of the International Conference in Machine Learning (ICML).
2016_iccopt.indb   21 2016/07/22   11:58:10
ICCOPT 201622
PLENARY TALKS
Florian Jarre
Heinrich Heine Universität Düsseldorf, Germany
Nonlinear Minimization Techniques without Using Derivatives 
Wednesday, August 10, 9:00–10:00, Soukairou Hall (1S, GRIPS)
Chair: Takashi Tsuchiya
It is mostly a matter of laziness — but with important implications: We discuss possible applications of minimization without 
(explicitly) using derivatives. While automatic differentiation offers an elegant and efficient alternative in many cases, due to 
its simplicity, the minimization without using derivative information is interesting in several respects: On the one side the 
applications mentioned above, on the other side a slight change in the use of the tools for minimization. More precisely, we 
consider the problem of finding a local optimal solution to an optimization problem with a smooth objective function and 
smooth constraints, but without the availability of derivative informations. There is a wealth of methods tuned to very expensive 
and/or noisy function evaluations, and there are methods in Matlab/Octave such as fmincon, fminunc, or minFunc that are 
tailored to situations where the derivative information is provided, and that use finite differences when derivatives are 
unavailable.We discuss modifications of the latter approach taking into account the fact that finite differences are numerically 
expensive compared to standard matrix operations. In particular, we consider a new line search based on least squares spline 
functions, a new finite difference setup, and a Sl2QP method for constrained minimization. Some numerical examples conclude 
the talk.
Florian Jarre is Professor of Mathematics at the Heinrich-Heine-Universität Düsseldorf, Germany, and leads the 
Optimization Group in Düsseldorf. He received his PhD from the University of Würzburg, Germany in 1989. Dr. 
Jarreʼs research is concerned with interior point methods for convex optimization, large scale approaches for 
solving convex conic programs, and applications of semidefinite and completely positive programming. His 
recent interest are applications of optimization without using derivatives. He is an Associate Editor of Optimization 
Methods and Software and Optimization and Engineering.

Jong-Shi Pang
University of Southern California, USA
Difference-of-Convex Optimization for Statistic Learning
Thursday, August 11, 9:00–10:00, Soukairou Hall (1S, GRIPS)
Chair: Jie Sun
We address a fundamental bi-criteria optimization problem for variable selection in statistical learning; the two criteria being 
a loss function which measures the fitness of the model and a penalty function which controls the complexity of the model. 
Motivated by the increased interest in non-convex surrogates of the step `0-function that counts the number of nonzero 
components of the model variables, we show that many well-known sparsity functions existed in the literature admit a unified 
difference-of-convex (dc) representation that facilitates systematic analysis and algorithmic development. Such a representation 
involves non-differentiable functions and thus understanding the associated optimization problems require care. Two classes 
of sparsity functions are considered: exact versus surrogate. Exact sparsity functions are those whose zeros coincide with the 
sparsity pattern of the model unknowns; surrogate sparsity functions are those that are substitutes for the `0-function. We 
derive several theoretical results on the non-convex Lagrangian formulation of the bi-criteria optimization, relating it with a 
penalty constrained formulation in terms of their prospective computable stationary solutions, and giving conditions under 
which a directional stationary solution of the Lagrangean problem is a global minimizer. We present computational algorithms 
for solving these bi-criteria dc optimization problems and present numerical results using data sets existed in the literature. The 
results demonstrate the superiority of using a non-convex formulation over a convex approach on a variety of compressed 
sensing and binary classification problems.
Jong-Shi Pang joined the University of Southern California as the Epstein Family Professor of Industrial and 
Systems Engineering in August 2013. Prior to this position, he was the Caterpillar Professor and Head of the 
Department of Industrial and Enterprise Systems Engineering for six years between 2007 and 2013. He held the 
position of the Margaret A. Darrin Distinguished Professor in Applied Mathematics in the Department of 
Mathematical Sciences and was a Professor of Decision Sciences and Engineering Systems at Rensselaer 
Polytechnic Institute from 2003 to 2007. He was a Professor in the Department of Mathematical Sciences at the 
Johns Hopkins University from 1987 to 2003, an Associate Professor and then Professor in the School of 
Management from 1982 to 1987 at the University of Texas at Dallas, and an Assistant and then an Associate Professor in the Graduate 
2016_iccopt.indb   22 2016/07/22   11:58:10
ICCOPT 2016 23
PLENARY TALKS
School of Industrial Administration at Carnegie-Mellon University from 1977 to 1982. During 1999 and 2001 (full time) and 2002 
(part-time), he was a Program Director in the Division of Mathematical Sciences at the National Science Foundation. Professor Pang 
was a winner of the 2003 George B. Dantzig Prize awarded jointly by the Mathematical Programming Society and the Society for 
Industrial and Applied Mathematics for his work on finite-dimensional variational inequalities, and a co-winner of the 1994 Frederick 
W. Lanchester Prize awarded by the Institute for Operations Research and Management Science. Several of his publications have 
received best paper awards in different engineering fields: signal processing, energy and natural resources, computational management 
science, and robotics and automation. He is an ISI Highly Cited Researcher in the Mathematics Category between 1980–1999; he has 
published 3 widely cited monographs and more than 100 scholarly journals in top peer reviewed journals. Dr. Pang is a member in 
the inaugural 2009 class of Fellows of the Society for Industrial and Applied Mathematics. Professor Pangʼs general research interest 
is in the mathematical modeling and analysis of a wide range of complex engineering and economics systems with focus in operations 
research, (single and multi-agent) optimization, equilibrium programming, and constrained dynamical systems.

2016_iccopt.indb   23 2016/07/22   11:58:10
ICCOPT 201624
SEMIPLENARY TALKS
SEMIPLENARY TALKS AND SPEAKERS
Mirjam Dür
Universität Trier, Germany
Copositive Optimization
Monday, August 8, 15:15–16:00, Soukairou Hall (1S, GRIPS)
Chair: Etienne de Klerk
A copositive optimization problem is a problem in matrix 
variables with a constraint which requires that the matrix be in 
the copositive cone. This means that its quadratic form takes 
nonnegative values over the nonnegative orthant. By definition, 
every positive semidefinite matrix is copositive, and so is every 
entrywise nonnegative matrix, but the copositive cone is 
significantly larger than both the semidefinite and the 
nonnegative matrix cones. Many combinatorial problems like 
for example the maximum clique problem can be equivalently 
formulated as a copositive problem. Burer (2009) showed that 
also any nonconvex quadratic problem with linear constraints 
and binary variables can be reformulated as such a copositive 
problem. This is remarkable, since by this approach a nonconvex 
problem is reformulated equivalently as a convex problem. The 
complexity of the original problem is entirely shifted into the 
cone constraint. We review recent progress in this copositive 
approach, concerning both theoretical results and numerical 
issues. 
Mirjam Dür was born in Vienna, Austria, where 
she received a M.Sc. degree in Mathematics 
from the University of Vienna in 1996. She 
received a PhD in applied mathematics from 
University of Trier (Germany) in 1999. After 
that, she worked as an assistant professor at 
Vienna University of Economics and Business Administration, 
as a junior professor at TU Darmstadt (Germany), and as a 
Universitair Docent at the University of Groningen (The 
Netherlands). Since October 2011, she is a professor of 
Nonlinear Optimization at the University of Trier, Germany. 
Prof. Dür is a member of the editorial boards of the journals 
Mathematical Methods of Operations Research, Journal of 
Global Optimization, and Optimization Methods and Software, 
and of the book series Springer Briefs in Optimization. In 2010, 
she was awarded a VICI-grant by the Dutch Organisation for 
Scientific Research (NWO), and in 2012 a GIF Research grant 
by the German-Israeli Foundation for Scientific Research and 
Development. As of 2016, she is one of the principal 
investigators in the Research Training Group Algorithmic 
Optimization at the University of Trier. 

Caroline Uhler
Institute of Science and Technology, Austria
Brownian Motion Tree Models: Theory and  
Applications
Monday, August 8, 15:15–16:00, Auditorium (m3S, NACT)
Chair: Masao Fukushima
Brownian motion tree models are heavily used for phylogenetic 
analysis based on continuous characters and as network 
tomography models to analyze the connections in the Internet. 
These models are a special instance of Gaussian models with 
linear constraints on the covariance matrix. Maximum 
likelihood estimation in this model class leads to a non-convex 
optimization problem that typically has many local maxima. 
Current methods for parameter estimation are based on 
heuristics with no guarantees. I will present efficient algorithms 
and explain how to initiate the algorithms in a data-informed 
way to obtain provable guarantees for finding the global 
maximum in this model class.
Caroline Uhler is an assistant professor in EECS 
and IDSS at MIT. She holds an MSc in 
Mathematics, a BSc in Biology, and an MEd in 
High School Mathematics Education from the 
University of Zurich. She obtained her PhD in 
Statistics from UC Berkeley in 2011. After short 
postdoctoral positions at the Institute for Mathematics and its 
Applications at the University of Minnesota and at ETH Zurich, 
she joined IST Austria as an assistant professor (2012–2015). 
Her research focuses on mathematical statistics, in particular on 
graphical models and the use of optimization, algebraic and 
geometric methods in statistics, and on applications to biology. 
She is an elected member of the International Statistical 
Institute and she received a Sofja Kovalevskaja Award from the 
Humboldt Foundation and a START Award from the Austrian 
Science Fund.

Elad Hazan
Princeton University, USA
Simulated Annealing with an Efficient  
Universal Barrier
Wednesday, August 10, 10:30–11:15,  
Soukairou Hall (1S, GRIPS)
Chair: Robert M Freund
Interior point methods and random walk approaches 
have been long considered disparate approaches for 
convex optimization. We show how simulated annealing, 
one of the most common random walk algorithms, is 
equivalent, in a certain sense, to the central path interior 
point algorithm applied to the enhanced entropic 
universal barrier function. Using this observation we 
improve the state of the art in polynomial time convex 
optimization in the membership-oracle model.
Elad Hazan is a professor of computer science at 
Princeton University. Previously he had been an 
associate professor of operations research at the 
Technion. His research focuses on the design 
and analysis of algorithms for basic problems in 
machine learning and optimization. Amongst his 
contributions are the co-development of the AdaGrad algorithm 
for training learning machines, and the first sublinear-time 
algorithms for convex optimization. He is the recipient of 
(twice) the IBM Goldberg best paper award in 2012 for 
contributions to sublinear time algorithms for machine learning, 
and in 2008 for decision making under uncertainty, a European 
2016_iccopt.indb   24 2016/07/22   11:58:11
ICCOPT 2016 25
SEMIPLENARY TALKS
Research Council grant , a Marie Curie fellowship and a Google 
Research Award (twice). He serves on the steering committee 
of the Association for Computational Learning and has been 
program chair for COLT 2015.

Yu-hong Dai
Chinese Academy of Sciences, China
The Steepest Descent and Conjugate  
Gradient Methods Revisited
Wednesday, August 10, 10:30–11:15,  
Auditorium (m3S, NACT)
Chair: Hiroshi Yabe
The steepest descent and conjugate gradient methods are basic 
first order methods for unconstrained optimization. More 
efficient variants have been proposed in recent decades by 
forcing them to approximate Newtonʼs method (or quasi-
Newton method). In this talk, I shall review some recent 
advances on steepest descent method and conjugate gradient 
method. While significant numerical improvements have been 
made, the behavior of these more efficient variants are still to be 
understood and more analysis are obviously required.
Yu-Hong Dai received his bachelor degree in 
applied mathematics from the Beijing Institute 
of Technology, China, in 1992. Then he studied 
nonlinear optimization in the Institute of 
Computational Mathematics, Chinese Academy 
of Sciences (CAS), and received his doctor 
degree in 1997. Now he is Feng Kang distinguished professor 
of Academy of Mathematics and Systems Science (AMSS) of 
CAS. Currently, he is also assistant president of AMSS and 
Vice-Director of Center for Optimization and Applications 
(COA) of AMSS. His research interests mainly lie in nonlinear 
optimization and various optimization applications. 
Specifically, he is quite interested in proposing simple but 
efficient optimization methods (for example, the Dai-Yuan 
conjugate gradient method) and in providing theoretical 
properties for existing elegant optimization methods (for 
example, the perfect example for the failure of the BFGS 
method and the R-linear convergence of the Barzilai-Borwein 
gradient method). He has published many papers in various 
journals, including Mathematical Programming, SIOPT, SIIS, 
SIMAX, ITSP, Mathematics of Computation, Numerische 
Mathematics, IMA Journal on Numerical Analysis and Journal 
of the OR Society of China. Because of his accomplishments, 
he received the Fifth ZhongJiaQing Mathematics Award (1998), 
Second Prize of the National Natural Science of China in 2006 
(Rank 2), the Tenth Science and Technology Award for Chinese 
Youth (2007), Best Paper Award of International Conference on 
Communication (2011), the China National Funds for 
Distinguished Young Scientists (2011), Feng Kang Prize of 
Scientific Computing (2015). Dr. Dai is currently the president 
of Chinese Mathematical Programming Subsociety of ORSC. 
Meanwhile, he has held editorial positions for several journals, 
including International Transactions in Operational Research 
(ITOR), Asia Pacific Journal of Optimization (APJOR）, 
Science China: Mathematics （SciChina:Math), Journal of the 
OR Society of China.

Katsuki Fujisawa
Kyushu University, Japan
High-Performance and Power-efficient  
Computing for Large-Scale Optimization  
Problem
Wednesday, August 10, 11:30–12:15,  
Soukairou Hall (1S, GRIPS)
Chair: Masakazu Kojima
In this talk, we present our ongoing research project. The 
objective of this project is to develop advanced computing and 
optimization infrastructures for extremely large-scale graphs 
on post peta-scale supercomputers. We explain our challenge to 
Graph 500 and Green Graph 500 benchmarks that are designed 
to measure the performance of a computer system for 
applications that require irregular memory and network access 
patterns. In 2014 and 2015, our project team was a winner of 
the 8th, 10th, and 11th Graph 500 and the 3rd to 6th Green 
Graph500 benchmarks, respectively. We present our parallel 
implementation for large-scale SDP (SemiDefinite 
Programming) problem. The semidefinite programming (SDP) 
problem is a predominant problem in mathematical optimization. 
The primal-dual interior-point method (PDIPM) is one of the 
most powerful algorithms for solving SDP problems, and many 
research groups have employed it for developing software 
packages. We solved the largest SDP problem (which has over 
2.33 million constraints), thereby creating a new world record. 
Our implementation also achieved 1.774 PFlops in double 
precision for large-scale Cholesky factorization using 2,720 
CPUs and 4,080 GPUs on the TSUBAME 2.5 supercomputer. 
We have also started another research project for developing 
the Urban OS (Operating System). The Urban OS gathers big 
data sets of people and transportation movements by utilizing 
different sensor technologies and storing them to the cloud 
storage system. The Urban OS employs the graph analysis 
system developed by our research project above and provides a 
feedback to a predicting and controlling center to optimize 
many social systems and services. we briefly explain our 
ongoing research project for realizing the Urban OS.
Fujisawa has been a Full Professor at the Institute 
of Mathematics for Industry (IMI) of Kyushu 
University and a research director of the JST 
(Japan Science and Technology Agency) CREST 
(Core Research for Evolutional Science and 
Technology) post-Peta High Performance 
Computing. He received his Ph. D. from the Tokyo Institute of 
Technology in 1998. The objective of the JST CREST project is 
to develop an advanced computing and optimization 
infrastructure for extremely large-scale graphs on post peta-
scale super-computers. His project team has challenged the 
Graph 500 and Green Graph 500 benchmarks, which are 
designed to measure the performance of a computer system for 
applications that require irregular memory and network access 
patterns. In 2014 and 2015, his project team was a winner of the 
8th, 10th, and 11th Graph500 and the 3rd to 6th Green Graph500 
2016_iccopt.indb   25 2016/07/22   11:58:11
ICCOPT 201626
SEMIPLENARY TALKS
benchmarks, respectively.

Erick Delage
HEC Montrál, Canada
Preference Robust Optimization for Decision  
Making under Uncertainty
Wednesday, August 10, 11:30–12:15,  
Auditorium (m3S, NACT)
Chair: Melvyn Sim
Decisions often need to be made in situations where parameters 
of the problem that is addressed are considered uncertain. 
While there are a number of well-established paradigms that 
can be used to design an optimization model that accounts for 
risk aversion in such a context (e.g. using expected utility or 
convex risk measures), such paradigms can often be 
impracticable since they require a detailed characterization of 
the decision makerʼs perception of risk. Indeed, it is often the 
case that the available information about the DMʼs preferences 
is both incomplete, because preference elicitation is time 
consuming, and imprecise, because subjective evaluations are 
prone to a number of well-known cognitive biases. In this talk, 
we introduce preference robust optimization as a way of 
accounting for ambiguity about the DMʼs preferences. In a 
financial environment, an optimal preference robust investment 
will have the guarantee of being preferred to the largest risk-
free return that could be made available. We show how 
preference robust optimization models are quasiconvex 
optimization problems of reasonable dimension when 
parametric uncertainty is described using scenarios and 
preference information takes the form of pairwise comparisons 
of discrete lotteries. Finally, we illustrate numerically our 
findings with a portfolio allocation problem and discuss 
possible extensions.
Erick Delage completed his Ph.D. at Stanford 
University in 2009, is currently associate 
professor in the Department of Decision 
Sciences at HEC Montreal, and was recently 
appointed as chairholder of Canada Research 
Chair in Decision Making Under Uncertainty. 
He serves on the editorial board of Management Science, 
Pacific Journal of Optimization, and Computational 
Management Science where he recently co-edited a special 
issue on recent advances in the field of robust optimization. His 
research interests span the areas of robust optimization, 
stochastic programming, decision theory, artificial intelligence 
and applied statistics.

Jonathan Kelner
Massachusetts Institute of Technology, USA
Bridging the Continuous and the  
Combinatorial:  Emerging Tools,
Techniques, and Design Principles for  
Graph Algorithms and Convex
Optimization 
Thursday, August 11, 15:15–16:00,  
Soukairou Hall (1S, GRIPS)
Chair: Satoru Iwata
Flow and cut problems on graphs are among the most 
fundamental and extensively studied problems in Operations 
Research and Optimization, playing a foundational role in both 
the theory and practice of algorithm design.  While the classical 
algorithms for these problems were largely based on 
combinatorial approaches, the past several years have witnessed 
the emergence of a powerful collection of new approaches 
rooted in continuous optimization.  These have allowed 
researchers to provide better provable algorithms for a wide 
range of graph problems, in some cases breaking algorithmic 
barriers that had stood for several decades.  The key to these 
improvements has been the development of a set of techniques 
for systematically linking the combinatorial structure of a graph 
problem to the numerical and geometric properties of the 
corresponding convex program, and for exploiting this 
connection to design iterative methods with improved 
guarantees.  This relationship between graph theory and 
continuous optimization has proven fruitful in the other 
direction as well, leading to fast algorithms and new insights for 
solving a variety of problems in convex optimization, 
computational linear algebra, and the analysis of random 
processes.  In this talk, I will survey some of the main results, 
recurring themes, and technical tools that arise in this confluence 
of fields, and I will illustrate these by sketching how they can be 
used to find approximately maximum flows on undirected 
graphs in close to linear time.  I will then discuss some recent 
results and highlight several challenges and open problems that 
I believe are within reach. 
Jonathan Kelner is an Associate Professor of 
Applied Mathematics in the MIT Department of 
Mathematics and a member of the MIT Computer 
Science and Artificial Intelligence Laboratory 
(CSAIL). His research focuses on the application 
of techniques from pure mathematics to the 
solution of fundamental problems in algorithms and complexity 
theory.  He was an undergraduate at Harvard University, and he 
received his Ph.D. in Computer Science from MIT in 2006. 
Before joining the MIT faculty, he spent a year as a member of 
the Institute for Advanced Study. He has received a variety of 
awards for his work, including an NSF CAREER Award, an 
Alfred P. Sloan Research Fellowship, the Best Paper Awards at 
STOC 2011, SIGMETRICS 2011, and SODA 2014, the Harold
E. Edgerton Faculty Achievement Award, and the MIT School 
of Science Teaching Prize. 

Rachel Ward
University of Texas at Austin, USA
Clustering Subgaussian Mixtures by 
 Semidefinite Programming
Thursday, August 11, 15:15–16:00,  
Auditorium (m3S, NACT)
Chair: Pablo A Parrilo
We introduce a model-free relax-and-round algorithm for
2016_iccopt.indb   26 2016/07/22   11:58:11
ICCOPT 2016 27
SEMIPLENARY TALKS
k-means clustering based on a semidefinite relaxation of the 
(NP-hard) k-means optimization problem. The algorithm 
interprets the SDP output as a denoised version of the original 
data and then rounds this output to a hard clustering.  We 
provide a generic method for proving performance guarantees 
for this algorithm, and we analyze the algorithm in the context 
of subgaussian mixture models. We also study the fundamental 
limits of estimating Gaussian centers by k-means clustering in 
order to compare our approximation guarantee to the 
theoretically optimal k-means clustering solution.  This is joint 
work with Dustin Mixon and Soledad Villar.
Rachel Ward received the B.Sc. degree in 
mathematics from the University of Texas at 
Austin in 2005 and the Ph.D. degree in applied 
and computational mathematics in 2009 from 
Princeton University. After working as a Post-
Doctoral Fellow at the Courant Institute, New 
York University, she joined the University of Texas at Austin in 
2011 as an Assistant Professor of mathematics. She received 
the Alfred P Sloan Research Fellowship, the Donald D. 
Harrington Faculty Fellowship, and the NSF CAREER Grant. 
Her research interests include image processing, statistical 
machine learning, optimization, compressed sensing, and 
quantization.

2016_iccopt.indb   27 2016/07/22   11:58:11
ICCOPT 201628
BEST PAPER PRIZE
BEST PAPER PRIZE FINALISTS
The ICCOPT 2016 Best Paper Prize for Young Researchers in Continuous Optimization called for submissions of published or nearly 
published papers from graduate students and recent Ph.D recipients. The selection committee (Amir Beck, Nick Gould, Kim-Chuan 
Toh, Akiko Yoshise, chaired by Andrzej Ruszczyński) invited the following contestants to present their work in a dedicated session 
of the conference. The session will be chaired by Akiko Yoshise.

Peyman Mohajerin Esfahani
EPFL & ETH Zurich 
peyman.mohajerin@epfl.ch
Data-driven Distributionally Robust Optimization Using the Wasserstein Metric: Performance Guarantees and Tractable 
Reformulations
We consider stochastic programs where the distribution of the uncertain parameters is only observable through a finite training 
dataset. Using the Wasserstein metric, we construct a ball in the space of (multivariate and non-discrete) probability distributions 
centered at the uniform distribution on the training samples, and we seek decisions that perform best in view of the worst-case 
distribution within this Wasserstein ball. The state-of-the-art methods for solving the resulting distributionally robust optimization 
(DRO) problems rely on global optimization techniques, which quickly become computationally excruciating. In this talk we 
demonstrate that, under mild assumptions, the DRO problems over Wasserstein balls can in fact be reformulated as finite convex 
programs—in many interesting cases even as tractable linear programs. Leveraging recent measure concentration results, we also 
show that their solutions enjoy powerful finite-sample performance guarantees. Our theoretical results are exemplified in mean-risk 
portfolio optimization as well as uncertainty quantification.
The finalist selected based on the paper: P.M. Esfahani and D. Kuhn, Data-driven Distributionally Robust Optimization Using the 
Wasserstein Metric: Performance Guarantees and Tractable Reformulations, conditionally accepted for publication in Mathematical 
Programming.
Mingyi Hong
Iowa State University
mingyi@iastate.edu
Convergence Analysis of Alternating Direction Method of Multipliers for a Family of Nonconvex Problems
The alternating direction method of multipliers (ADMM) is widely used to solve large-scale linearly constrained optimization 
problems, convex or nonconvex, in many engineering fields. However there is a general lack of theoretical understanding of the 
algorithm when the objective function is nonconvex. In this work we analyze the convergence of the ADMM for solving certain 
nonconvex consensus and sharing problems. By using a three-step argument, we show that the classical ADMM converges to the set 
of stationary solutions, provided that the penalty parameter in the augmented Lagrangian is chosen to be sufficiently large. For the 
sharing problems, we show that the ADMM is convergent regardless of the number of variable blocks. Our analysis does not impose 
any assumptions on the iterates generated by the algorithm, and is broadly applicable to many ADMM variants involving proximal 
update rules and various flexible block selection rules. Finally, we discuss a few generalizations of the three-step analysis to a broader 
class of algorithms, with applications in signal processing and machine learning.
The finalist selected based on the paper: M. Hong, Z.-Q. Luo, and M. Razaviyayn, Convergence Analysis of Alternating Direction 
Method of Multipliers for a Family of Nonconvex Problems, accepted for publication in SIAM Journal on Optimization.
Mengdi Wang
Princeton University
mengdiw@princeton.edu
Stochastic Composition Optimization: Algorithms and Sample Complexities
Classical stochastic gradient methods are well suited for minimizing expected-value objective functions. However, they do not apply 
to the minimization of a composition of two expected-value functions, i.e., the stochastic composition optimization problem which 
involves an outer stochastic function and an inner stochastic function:minx    v [ fv(   w [gw(x)])]. Stochastic composition optimization 
finds wide application in learning, estimation, risk-averse optimization, dynamic programming, etc. In order to solve this problem, we 
propose a class of stochastic compositional first-order methods that can be viewed as stochastic versions of quasi-gradient method. 
The algorithms update the solutions based on queries to a sampling oracle and use auxiliary variables to track the unknown inner 
quantities. We prove that the algorithms converge almost surely to an optimal solution for convex optimization problems (or a 
stationary point for nonconvex problems), as long as such a solution exists. The convergence involves the interplay of two martingales 
with different timescales. We obtain rate of convergence results under various assumptions, and show that the algorithms achieve the 
Finalists slected based on the paper: P.M. Esfahani and D. Kuhn, Data-driven Distributionally Robust
Optimization Using the Wasserstein Metric: Performance Guarantees and Tractable Reformulations,
conditionally accepted for publication in Mathematical Programming.
Mingyi Hong
Department of Industrial and Manufacturing Systems Engineering, Iowa State University
peyman.mohajerin@epfl.ch
Convergence Analysis of Alternating Direction Method of Multipliers for a Family
of Nonconvex Problems
The alternating direction method of multipliers (ADMM) is widely used to solve large-scale linearly
constrained optimization problems, convex or nonconvex, in many engineering fields. However there is a
general lack of theoretical understanding of the algorithm when the objective function is nonconvex. In
this work w analyz the c nvergence of the ADMM for solving certai nonconvex cons nsus and sharing
problems. By usi g a three-step argument, we show that the classical ADMM converges to the set of
stationary solutions, provided that the penalty parameter in the augmented Lagrangian is chosen to be
sufficiently large. For the sharing problems, we show that the ADMM is convergent regardless of the
number of variable blocks. Our analysis does not impose any assumptions on the iterates generated by
the algorithm, and is broadly applicable to many ADMM variants involving proximal update rules and
various flexible block selection rules. Finally, we discuss a few generalizations of the three-step analysis
to a broader class of algorithms, with applications in signal processing and machine learning.
Finalists slected based on the paper: M. Hong, Z.-Q. Luo, and M. Razaviyayn, Convergence Analysis
Of Alternating Direction Method Of Multipliers For A Family Of Nonconvex Problems, accepted for
publication in SIAM Journal on Optimization.
Mengdi Wang
Department of Operations Research and Financial Engineering mengdiw@princeton.edu
Stochastic Composition Optimization: Algorithms and Sample Complexities
Classical stochastic gradient methods are well suited for minimizing expected-value objective func-
tions. However, they do not apply to the minimization of a composition of two expected-value functions,
i.e., the stochastic composition optimization problem which involves an outer stochastic function and
an inner stochastic function: i E
[ (
Ew[g (x ]
)]
. Stochastic comp sition optimization finds wide
application in learning, estimatio , risk-averse optimization, dynamic programming, etc. In order to
solve this pr bl m, we propose class of stochastic compositional first-order methods that can be viewed
as tochastic v rsi ns of quasi-gradient method. The lgorithms updat the solutio s based on queries
to a sampling oracle and use auxiliary variables to track the unknown inner quantities. We prove that
the algorithms converge almost urely to an optimal solu ion fo convex ptimization problems (or a
stati nary point fo nonco vex pr blem ), as long as such a solution exist . The convergence involves
the interplay of two martingales with different timescales. We obtain rate of convergence results un-
der various assumptions, and show that the algorithms achieve the optimal sample-error complexity in
several important special cases. These results provide the best-known rate benchmarks for stochastic
composition optimization. Indeed, stochastic composition optimization is very common in practice. We
demonstrate its application to statistical estimation and reinforcement learning.
Finalists slected based on the paper: M. Wang, E.X. Fang, and H. Liu, Stochastic Compositional Gradient
Descent: Algorithms for Minimizing Nonlinear Functions of Expected Values, accepted for publication in
Mathematical Programming.
Please join us on Monday, August 8th from 16:15 to 17:30 (Soukairou Hall, S1) as the finalists
present their papers. The winner will be announced at the conference banquet and awarded by Andrzej
Ruszczyński at the closing of the conference.
10
Finalis s slect based on the paper: P.M. Esf hani and D. Kuhn, Data-driven Distributionally Robust
Op mization Using the Wasserst in Metric: Performance Guarantees and Tractable Reformulations,
c ditionally accepted for publication in M hematical Programming.
Mingyi Hong
Departme t of Industrial d Manufacturing Systems Engineering, Iowa State University
peyman.mohajerin@epfl.ch
Convergence Analysis of Alternating Direction Method of Multipliers for a Family
of Nonconvex Problems
The lternating direction method of multipliers (ADMM) is wid ly used to solv large-scale linearly
constrained op mization problems, c nvex or onconvex, in many ngineering fields. However there is a
general lack f heoretical un erstanding of the algorithm when th objective fu ction is onconvex. In
this work we analyze th co v rgenc of the ADMM for solving certain onc nvex conse su nd sharing
problems. By using a three-step argum nt, we s ow that the classical ADMM converges to he set of
stationary solutions, provided that the pen lty parame er in the augmented Lagrangian is ch sen to be
sufficiently large. For the sha ing problem , we s ow that the ADMM is co ve gent r gardless of the
number of variable blocks. Our analysis does not impose any assumptions on the iterat s g nerated by
the algorithm, and is broad y pplicable to many ADMM variants volving proxim l update rules and
various flexible block selection rules. Finally, we discuss a few generalizations of th three-step analysis
t a broader cl ss of algorithms, with pplications in signal processing nd machine learning.
Finalis s slecte based on the paper: M. Hong, Z.-Q. Luo, and M. R zaviyayn, Co v rge ce Analysis
Of Alternating Direction Method Of Multipliers For A Family Of Nonconvex Problems, accepted for
publication in SIAM Journal on Op mization.
Mengdi Wang
Department of Operations Research and Financial Engineeri mengd w@princeton.edu
Stochastic Composition Optimization: Algorithms and Sample Complexities
Cla sical ochastic gradi nt methods are well suited for m imizing xpected-value objective func-
tions. However, they do not apply to the min mization of a composition of two xpected-value functions,
i.e., the ochastic composition op mization problem which involves an outer ochastic function and
an inner ochastic functio : minx Ev
[
fv
(
Ew[gw(x)]
)]
. S ochast c composition op mization finds wide
pplication i learning, estimation, risk-averse op mization, dynamic programming, etc. In order to
solve this probl m, we propose a class of ochastic compositional first-order me ods that can b viewed
as ocha tic versions of quasi-gradi nt method. The algorithms update the solutions based on queries
t a sampling oracle and use auxi iary variables to rack the u known inner qu ntiti s. W prove that
the algorithms converge almost surely to an optimal solution f r convex op mization problems (or a
stationary point for nonconvex problems), as long as suc a solu on exists. Th co v rgence involves
he interplay of two m rtingales with different timescal s. We obtain rate of co v rg nce results un-
der vario s assumptions, and s ow that the algorithms achi ve he opti al sample-error complexity in
several important special cases. These results provide the best-known rate benchmarks for ochastic
composition op mization. Indeed, ochastic composition op mization is very common in practice. We
demons rate its pplication to st tistical estim tion a d reinforceme t learning.
Finalis s slect based on the paper: M. Wang, E.X. Fang, and H. Liu, S ochastic Compositional Gradient
Descent: Algorithms for M imizi g Nonlinear Functions of Expected V lu s, accepted for publication in
M hematical Programming.
Please join us on Monday, August 8th from 16: 5 to 17:30 (S kairou Hall, S1) as the finalists
pres nt thei papers. The winner will be announced at th confere ce banquet and awarded by Andrzej
Ru zczyński at the cl sing of th conference.
10
2016_iccopt.indb   28 2016/07/22   11:58:11
ICCOPT 2016 29
BEST PAPER PRIZE
optimal sample-error complexity in several important special cases. These results provide the best-known rate benchmarks for 
stochastic composition optimization. Indeed, stochastic composition optimization is very common in practice. We demonstrate its 
application to statistical estimation and reinforcement learning.
The finalist selected based on the paper: M. Wang, E.X. Fang, and H. Liu, Stochastic Compositional Gradient Descent: Algorithms 
for Minimizing Nonlinear Functions of Expected Values, accepted for publication in Mathematical Programming.

Please join us on Monday, August 8th from 16:15 to 17:30 (Soukairou Hall, 1S) as the finalists present their papers. The winner will 
be announced right before the plenary talk in the morning of Tuesday, August 9th.
2016_iccopt.indb   29 2016/07/22   11:58:12
ICCOPT 201630
SOCIAL PROGRAM
SOCIAL PROGRAM
WELCOME RECEPTION
SUNDAY, AUGUST 7, 18:30–20:30
GRIPS Cafeteria (1st Floor)
The Welcome Reception will take place at the Cafeteria located on the 1st floor of GRIPS.  Light meals with beer and soft drinks will 
be served to all participants.
POSTER SESSION AND RECEPTION
MONDAY, AUGUST 8, 17:30–19:30
GRIPS Foyer (1st Floor)
Refreshments will be served to all participants during the Poster Presentations Session, which takes place at the Foyer on the 1st floor 
of GRIPS.
CONFERENCE BANQUET
TUESDAY, AUGUST 9, 19:00–22:00
Sushi Izakaya MACCHAN
8,000 yen per person, not included in the registration fee.
The Conference Banquet will be offered in a cozy Japanese Izakaya (Tavern) style. 
The Izakaya restaurant is located in the heart of Roppongi, within walking distances 
from GRIPS or Tokyo Metro/Toei Subway Roppongi stations.  Our staff will guide 
you to the restaurant from GRIPS.  Enjoy Sushi, Sashimi and other specialties 
exclusively chosen for the banquet.  A limited number of tickets may be available 
on site.  Ask for availability at the Registration Desk if you have not purchased 
yours online.

STUDENT SOCIAL (students only)
WEDNESDAY, AUGUST 10, 18:30–20:30
GRIPS Cafeteria (1st Floor)
The Student Social will take place at the Cafeteria located on the 1st floor of GRIPS.  Light meal with alcoholic and soft drinks will 
be served to all student participants.
COFFEE BREAKS
Coffee, mineral water and light snacks will be served at the Cafeteria on the 1st floor and the Lounge on the 5th floor during 30-minute 
breaks between sessions.
2016_iccopt.indb   30 2016/07/22   11:58:12
ICCOPT 2016 31
HOW TO FIND YOUR SESSION
HOW TO FIND YOUR SESSION
All the rooms for the Parallel Sessions will be on the first, the fourth, or the fifth floor of GRIPS or on the third floor of the National 
Art Center, Tokyo (NACT). See pages 122–124 for the floor plan.
　The session code includes all the information you need to identify your parallel session, whether organized or contributed (take 
Tue.D.5H as an example):
Tue  The day of the week:
Mon  Monday
Tue  Tuesday
Wed  Wednesday
Thu  Thursday
D   The time of the day:
A  1st slot: 10:45–12:00 (Mon), 10:30–11:45 (Tue), 13:45–15:00 (Wed), 9:00–10:15 (Thu)
B  2nd slot: 13:30–14:45 (Mon), 13:15–14:30 (Tue), 15:15–16:30 (Wed), 10:45–12:00 (Thu)
C  3rd slot: 14:45–16:00 (Tue), 17:00–18:15 (Wed), 13:30–14:45 (Thu),
D  4th slot: 16:30–17:45 (Tue)
5H   The room code:
1x  Room x on the 1st floor of GRIPS
4x  Room x on the 4th floor of GRIPS
5x  Room x on the 5th floor of GRIPS
m3x  Room x on the 3rd floor of NACT
where x=S stands for the auditorium of GRIPS or that of NACT (namely, ʻ1Sʼ refers to the Soukairou Hall and ʻm3Sʼ refers to the 
auditorium of NACT).
　The following table summarizes the structure of the scientific program.
Here, the slots with diagonal line indicate that the room is NOT reserved for the conference and the ICCOPT participants are not 
allowed to enter.
2016_iccopt.indb   31 2016/07/22   11:58:12
ICCOPT 201632
HOW TO FIND YOUR SESSION
The cluster code is defined as follows.
Acronym Cluster
AESE Applications in Energy, Science and Engineering  
AFE Applications in Finance and Economics   
CVI Complementarity and Variational Inequalities
CPO Conic and Polynomial Optimization 
CNO Convex and Nonsmooth Optimization 
DSO Derivative-free and Simulation-based Optimization  
GO Global Optimization  
LO Linear Optimization   
M-OVO Multi-Objective and Vector Optimization     
NO Nonlinear Optimization
OIS Optimization Implementations and Software
PDE-O PDE-constrained Optimization  
RO Robust Optimization     
SOIP Sparse Optimization and Information Processing 
SO Stochastic Optimization    
Detailed Information on Parallel Sessions
The following pages 33–44 summarize the detailed information of the parallel sessions.
2016_iccopt.indb   32 2016/07/22   11:58:12
ICCOPT 2016 33
PROGRAM AT A GLANCE
fl room
NO DP Robinson
Frank E Curtis
Lorenz T Biegler
Andreas Waechter
NO F Rinaldi
Joe Naoum-Sawaya
James T Hungerford
Emanuele Frandi
PDE-O T Takeuchi
Takaaki Nara
Benny Hon
Leevan Ling
DSO F Rinaldi/Z Zhang
Anne-Sophie Crélot
Giacomo Nannicini
Christine A Shoemaker
AESE A Mittal
Kei Hirose
Muneki Yasuda
Areesh Mittal
PDE-O A Schiela
Martin Siebenborn
Sebastian Goetschel
Anton Schiela
M-OVO A Schwartz
Axel Dreves
Sonja Steffensen
Michal Cervinka
LO L Faybusovich/T Tsuchiya
Yongdo Lim
Kouhei Harada
Keiichi Morikuni
RO M Sim
Zhi Chen
Shuming Wang
Jianzhe Zhen
CNO Q Tran-Dinh/I Necoara
Ion Necoara
Alp Yurtsever
Adrien B Taylor
AFE C Lin
Frank Wang
Changle Lin
 
CPO Y Xia
Sena Safarina
Kei Takemura
CPO J Nie/JB Lasserre
Etienne de Klerk
Xinzhen Zhang
Panos Parpas
SOIP C Cartis
Francis Bach
Caroline Uhler
Katsuya Tono
Another event will be in progress: ICCOPT participants are not allowed to enter.
m3AB
(Lecture 
Rooms A&B)
A Link between DC Algorithms and Proximal Gradient Methods
Submodular Functions: From Discrete to Continous Domains
Learning Directed Acyclic Graphs Based on Sparsest Permutations
3r
d 
flo
or
 o
f N
AC
T m3S
(Auditorium)
Sparse Optimization and Applications
A Multilevel Method for Semidefinite Programming Relaxations of Polynomial Optimization Problems with Structured Sparsity
Improved Convergence Rates for Lasserre-Type Hierarchies of Upper Bounds for Box-constrained Polynomial Optimization5L
(Lecture 
Room L)
Moments, Positive Polynomials & Optimization: Part I
Real Eigenvalues of Nonsymmetric Tensors
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
5K
(Lecture 
Room K)
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
5J
(Lecture 
Room J)
5I
(Lecture 
Room I)
Interior-Point Methods and Applications for Conic Optimization
A Numerically Stable Primal-Dual Interior-Point Method for SDP
An Efficient Second-Order Cone Programming Approach for Optimal Selection in Tree Breeding
Personalized Asset & Liability System: Goal-based Investing 
Robo-Advisor in China's Market5H
(Lecture 
Room H)
Financial Optimization and Robo Advisors 1
A Universal Primal-Dual Convex Optimization Framework
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
Exact Worst-Case Performance of First-Order Methods for Composite Convex Minimization
5F
(Lecture 
Room F)
Recent Advances on Convergence Rates of First-Order Methods: Part I
Solving Distributionally Robust Multistage Optimization Problems via Fourier-Motzkin Elimination
Linear Convergence of First-Order Methods for Non-strongly Convex Optimization
Tolerance-driven Appointment Scheduling and Sequencing using Perceived Delay Measures
Distributionally Robust Optimization with Semi-infinite Ambiguity Sets5E
(Lecture 
Room E)
Theory and Applications of Robust Optimization
A DC Programming Approach for Long-Short Multi-Factor Model
Wasserstein Barycenters of Gaussian Measures5D
(Lecture 
Room D)
Various Aspects of Conic Optimization and Mathematical Modeling Systems
An Interior Point Algorithm for Equality Constrained GNEPs
How to Select a Solution in GNEPs
Implementation of Interior-Point Methods for LP using Krylov Subspace Methods Preconditioned by Inner Iterations
5t
h 
flo
or
 o
f G
R
IP
S
5A
(Lecture 
Room A)
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
Stability and Sensitivity Analysis of Stationary Points in Mathematical Programs with Complementarity Constraints
5G
(Lecture 
Room G)
5C
(Lecture 
Room C)
Solutions of Equilibrium Problems: Computation and Stability
An Affine Covariant Composite Step Method for Optimization with PDEs as Equality Constraints
Changing Graph Structure for Performing Fast, Approximate Inference in Graphical Models
4t
h 
flo
or
 o
f G
R
IP
S 4A
(Research 
Meeting 
Room 4A)
Role of Optimization in Graphical Models Inference
Robust Estimation for Gaussian Graphical Modeling and Its Application to Gene Expression Data
Approximate Techniques for Boltzmann Machines
4B
(Research 
Meeting 
Room 4B)
Algorithmic Advances in PDE-constrained Optimization
Non-uniform Adaptive Lossy Trajectory Compression for Optimal Control of Parabolic PDEs
Efficient Mulit Objective Surrogate Global Optimization in Parallel with MOPLS and pySOT Toolbox
Shape Optimization Algorithms for Inverse Modeling in Extreme Scales
Numerical Differentiation by Kernel-based Probability Measures
1B
(Meeting 
Room 1B)
Inverse Problems
A Direct Reconstruction Formula for the Conductivity and Permittivity from the Measurements of the Time-harmonic Magnetic Field
Finite Integration Method for Inverse Heat Conduction Problems
Scalable and Sparse Optimization in Machine Learning via Frank-Wolfe Methods
1s
t f
lo
or
 o
f G
R
IP
S
1S
(Soukairou 
Hall)
Nonlinear Optimization and Its Applications I
Self-correcting Variable-Metric Algorithms for Nonlinear Optimization
Solving MPCCs with IPOPT
1A
(Meeting 
Room 1A)
Methods for Large-Scale Problems
Column Generation Approach for the Interceptor Vehicle Routing Problem
A Partially Aggregated Dantzig Wolfe Decomposition Algorithm for Multi-Commodity Flows
A Logical Benders Decomposition Algorithm for Binary-constrained Quadratic Programs with Complementarity Constraints
1C
(Meeting 
Room 1C)
Derivative-free and Simulation-based Optimization with Surrogate Models
Surrogate Strategies for Mixed-Variable Derivative-free Optimization
RBFOpt: An Open-Source Library for Surrogate Model Based Optimization
Mon.A     10:45-12:00  Monday, August 8th
2016_iccopt.indb   33 2016/07/22   11:58:13
ICCOPT 201634
PROGRAM AT A GLANCE
fl room
NO FE Curtis
Daniel P Robinson
Katya Scheinberg
Hao Wang
NO Y Ye/O Hinder
Oliver H Hinder
Yu Watanabe
Roummel Marcia
PDE-O K Ito/M Ulbrich
John A Burns
Constantin Christof
Johann M Schmitt
DSO F Rinaldi/Z Zhang
Simon Wessing
Dimo Brockhoff
Sébastien Le Digabel
AESE A Tomasgard
Yan Gao
Somayeh Moazeni
Chiara Bordin
AESE N Dimitrov
Murat Karatas
Felix Jost
Xi Chen
LO L Faybusovich
Thanasak Mouktonglang
Sangho Kum
Bruno F Lourenço
RO AMC So
Karthik Natarajan
Wing Kin Ma
Man-Chung Yue
CNO Q Tran-Dinh/I Necoara
Lasith Adhikari
Cesar A Uribe
Quoc Tran-Dinh
NO M Takac
Michel Baes
Norbert Trautmann
YiKuan Jong
CPO M Yamashita/M Fukuda
Ellen H Fukuda
Akihiro Tanaka
Makoto Yamashita
CPO T Terlaky/M Anjos/JC Góez
Nathan Krislock
Hongbo Dong
Julio C Goez
CPO J Nie/JB Lasserre
Gonzalo Munoz
Amir A Ahmadi
Guoyin Li
CNO F Bach
Elad Hazan
Julien Mairal
Guanghui Lan
M-OVO AH Hamel
Carola Schrage
Giovanni P Crespi
Andreas H Hamel
Introducing Well-Posedness to Set-Optimization
Set-valued Variational Inequalities in Vector Optimizationm3AB
(Lecture 
Rooms A&B)
Set Optimization: Advances and Applications
The Fundamental Duality Formula in Convex Set Optimization
An Optimal Randomized Incremental Gradient Method 
Second-Order Optimization for Machine Learning in Linear Time
Proximal Minimization by Incremental Surrogate Optimization (MISO) 
3r
d 
flo
or
 o
f N
AC
T m3S
(Auditorium)
Stochastic Optimization
Error Bounds for Parametric Polynomial Systems with Applications to Higher-Order Stability Analysis and Convergence Rate
LP Approximations to Polynomial Optimization Problems with Small Tree-Width5L
(Lecture 
Room L)
Moments, Positive Polynomials & Optimization: Part II
Disjunctive Conic Cuts for Mixed Integer Second Order Cone Optimization
Robust to Dynamics Optimization (RDO) 
BiqCrunch: Solving Binary Quadratic Problems Efficiently using Semidefinite Optimization5K
(Lecture 
Room K)
Conic and Integer Conic Optimization
On a Semidefinite Relaxation for the Sparse Linear Regression Problem
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
5J
(Lecture 
Room J)
An Iterative Method using Boundary Distance for Box-constrained Nonlinear Semidefinite Programs
5I
(Lecture 
Room I)
Theoretical and Computational Aspects of Conic Programs
Some Tractable Subcones for Testing Copositivity 
Second-Order Conditions for Nonlinear Semidefinite Optimization Problems via Slack Variables Approach
On the Dependency among Asian Currency Exchange Rates under the Influence of Financial Tsunami
A Hybrid Approach for Tracking the 1/N Portfolio
Continuous Selections of Optimal Portfolios5H
(Lecture 
Room H)
Optimization in Finance
5G
(Lecture 
Room G)
Non-asymptotic Convergence Rate for Distributed Learning in Graphs
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
Adaptive Smoothing Fast Gradient Methods for Fully Nonsmooth Composite Convex Optimization
5F
(Lecture 
Room F)
Recent Advances on Convergence Rates of First-Order Methods: Part II
Epsilon-Net Techniques for a Class of Uncertain Quadratic Programming and Its Applications in Robust Beamforming with Cognitive Radio Constraints
Limited-Memory Trust-Region Methods for Sparse Reconstruction
Semidefinite Relaxation of a Class of Robust QCQPs: A Verifiable Sufficient Condition for Rank-One Solutions
On Reduced Semidefinite Programs for Second Order Moment Bounds with Applications5E
(Lecture 
Room E)
Robust Optimization in Data and Signal Processing
Incremental Gradient Method for Karcher Mean on Symmetric Cones
Primal-Dual Algorithms for Infinite-dimensional Second-Order Cone Programming Problems and LQ-Problem with Time Dependent Linear Term in the Cost Function5D
(Lecture 
Room D)
Optimization over Symmetric Cones and Related Topics
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
FRA-Poly: Partial Polyhedrality and Facial Reduction
5t
h 
flo
or
 o
f G
R
IP
S
5A
(Lecture 
Room A)
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
5C
(Lecture 
Room C)
Texas Arbovirus Risk Maps and Uncertainty Analysis
Personalized Measurement Time Points by Optimum Experimental Design for Mathematical Leukopenia Models
Smart Charging of Electric Vehicles through Indirect Control and Smart Price Signals
4t
h 
flo
or
 o
f G
R
IP
S 4A
(Research 
Meeting 
Room 4A)
Energy Systems and Markets
Nonsmooth Equations Approach to the Real-Time Pricing for Smart Grid
An Energy Storage Deployment Program under Random Discharge Permissions
4B
(Research 
Meeting 
Room 4B)
Optimization in Healthcare
The Mesh Adaptive Direct Search Algorithm for Discrete Blackbox Optimization
Benchmarking Bi-Objective Derivative-free Optimizers with COCO
Cyber Defense Based on Network Structure
Optimal Control of Hyperbolic Balance Laws with State Constraints 
Sensitivity Analysis for Elliptic Variational Inequalities of the Second Kind: A Model Problem and Applications in Optimal Control
1B
(Meeting 
Room 1B)
Advances in PDE-constrained Optimization I
Optimization for Design and Control of Composite Thermal Fluid Systems
Shape-changing L-SR1 Trust-Region Methods
Inexact Sequential Quadratically Constrained Quadratic Programming Method of Feasible Directions with Global and Superlinear Convergence Properties
1s
t f
lo
or
 o
f G
R
IP
S
1S
(Soukairou 
Hall)
Nonlinear Optimization and Its Applications II
An Evolving Subspace Method for Low Rank Minimization
Convergence Rate of a Trust Region Method for Stochastic Nonconvex Optimization
1A
(Meeting 
Room 1A)
Nonlinear Optimization Solvers
A One Phase Interior Point Method for Non-convex Optimization
A Dynamic Penalty Parameter Updating Strategy for Matrix-free Sequential Quadratic Optimization Methods
1C
(Meeting 
Room 1C)
Computational and Algorithmic Aspects of Derivative-free Optimization
Improved Sampling for Two-Stage Methods
Mon.B     13:30-14:45  Monday, August 8th
2016_iccopt.indb   34 2016/07/22   11:58:13
ICCOPT 2016 35
PROGRAM AT A GLANCE
fl room
NO P Toint
Mohammadreza Samadi
Philippe Toint
NO YF Liu
Simon Foucart
Xiaojun Chen
Takayuki Okuno
PDE-O M Ulbrich/K Ito
Ariana Pitea
Livia Susu
Sven Leyffer
DSO F Rinaldi/Z Zhang
Laurent Dumas
John P Eason
Stefan M Wild
AESE JA Gomez
Bulat Khusainov
Anil V Rao
Jose A Gomez
AFE G Jun
Yongjae Lee
Do-gyun Kwon
Gyun Jeon
M-OVO GM Lee
Chuong Thai Doan
Satoshi Suzuki
Sangwoon Yun
LO AD Sidford/YT Lee
Yin Tat Lee
Aaron D Sidford
Jakub Pachocki
RO K Natarajan
Zhenzhen Yan
Xiaobo Li
Selin D Ahipasaoglu
SOIP E Resmerita
Thomas Möllenhoff
Daniel Gerth
Elena Resmerita
OIS J Siirola
Bethany Nicholson
John D Siirola
 
M-OVO A Loehne
Andreas Loehne
Benjamin Weissing
Shashi K Mishra
CPO M Muramatsu
Henrik A Friberg
Leonid Faybusovich
Gabor Pataki
CNO X Yuan/C Chen
Wenxing Zhang
Xingju Cai
Yuan Shen
CNO B Recht/PA Parrilo
Maryam Fazel
Francois Glineur
Alexander Rakhlin
The National Art Center, Tokyo will be closed on Tuesday.
m3AB
(Lecture 
Rooms A&B)
The National Art Center, Tokyo will be closed on Tuesday.
On Equivalence between Deterministic First-Order Optimization Algorithms and Martingale Inequalities
3r
d 
flo
or
 o
f N
AC
T m3S
(Auditorium)
An Optimal First Order Method based on Optimal Quadratic Averaging5L
(Lecture 
Room L)
Notions of Robustness and Dynamics in Convex Optimization: Part I
Convergence of First-Order Algorithms for Convex Optimization using Inexact Information
5K
(Lecture 
Room K)
An Alternating Minimization Algorithm for Robust Principal Component Analysis
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
Lattice-based Patterned Fabric Inspection by Sparse and Low-Rank Representation5J
(Lecture 
Room J)
Recent Advances in Splitting Methods for Large-Scale Convex Programming: Part I
Exact Duals and Short Certificates of Infeasibility and Weak Infeasibility in Conic Linear Programming: Part 1
A Proximal Point Algorithm with Asymmetric Linear Term
5I
(Lecture 
Room I) Primal-Dual Potentiai-Reduction Algorithm for Symmetric Programming Problem with Nonlinear Objective Function
Facial Reduction in MOSEK
Geometry and Algorithms for Conic Programming
5G
(Lecture 
Room G)
Advances in Optimization Modeling Languages
Optimality and Duality for Mathematical Programmes with Equilibrium Constraints
Duality in Polyhedral Projection Problems
A Set-valued Approach to Matrix Games with Vector Payoffs5H
(Lecture 
Room H)
Vector Optimization
On Convergence of Sparsity-promoting Regularization for Non-sparse Solutions
New Developments in Pyomo
Modeling Abstractions and Automatic Discretization Frameworks for Optimization Problems with Differential Equations in Pyomo
Variable Exponent Penalties for Sparse Solution Reconstruction
5F
(Lecture 
Room F)
Sparse Solution Reconstruction in Inverse Problems
Distributionally Robust Project Crashing with Full, Partial or No Correlation Information
Precise Relaxation of Nonconvex Energies via Structured Sparsity
Analysis of Discrete Choice Models: A Welfare-based Approach
Multi-Product Pricing Optimization with Robust Choice Model
On the Interplay of Choice, Robustness and Optimization
5E
(Lecture 
Room E)
Geometric Median
A Faster Algorithm for Linear Programming and the Maximum Flow Problem
A Faster Algorithm for Linear Programming and the Maximum Flow Problem5D
(Lecture 
Room D)
Theoretical Advances in Linear Optimization ― Barrier Methods
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
A Coordinate Descent Homotopy Method for Bi-Level Problem and Linearly Constrained Minimization
5t
h 
flo
or
 o
f G
R
IP
S
5A
(Lecture 
Room A)
Robust Multi-Objective Optimization Problems
Necessary Optimality Conditions for Nonsmooth Multiobjective Bilevel Optimization Problems
Surrogate Duality for Quasiconvex Vector Optimization with Data Uncertainty
5C
(Lecture 
Room C)
Asian Perspective on ‘Robo-Advisor’: Case of Korean Market
Goal Based Investment via Multi-Stage Stochastic Programming for Robo-Advisor Service ― Part II: Implementation Issues4t
h 
flo
or
 o
f G
R
IP
S
Optimization of Dynamic Systems with Linear Programs Embedded and Its Application to Sustainable Biofuels Production
4A
(Research 
Meeting 
Room 4A)
Dynamics and Optimal Control
Multi-Objective Co-Design for Embedded Optimization-based Control
Novel Computational Framework for the Numerical Solution of Constrained Optimal Control Problems
4B
(Research 
Meeting 
Room 4B)
Financial Optimization and Robo Advisors 2
Model-based Methods for Composite Blackbox Optimization
A Trust Region Method for Glass Box/Black Box Optimization
Goal Based Investment via Multi-Stage Stochastic Programming for Robo-Advisor Service ― Part I: Modeling Issues
1C
(Meeting 
Room 1C)
Mixed-Integer PDE-constrained Optimization
Optimal Control of Nonsmooth Semilinear Parabolic Equations
1B
(Meeting 
Room 1B)
Advances in PDE-constrained Optimization II
A Geometric Approach of Some Multitime Multiobjective Variational Problems
1s
t f
lo
or
 o
f G
R
IP
S
1S
(Soukairou 
Hall)
Nonlinear Optimization Algorithms and Their Complexity II
A Continuous DC Programming Approach to Nonlinear Mixed Integer Programs
Penalty Methods for a Class of Non-Lipschitz Optimization Problems
1A
(Meeting 
Room 1A)
Nonconvex and Non-Lipschitz Optimization: Algorithms and Applications 1
Sparse Recovery via Nonconvex Optimization, with Application in Metagenomics
Derivative-free Optimization Methods for Structured Problems
A New DFO Algorithm for the Optimization of Partially Separable Functions
Tue.A     10:30-11:45  Tuesday, August 9th
A Trust Region Algorithm with a Worst-Case Iteration Complexity of O(e-3/2) for Nonconvex Optimization
Second-Order Optimality and (Sometimes) Beyond
2016_iccopt.indb   35 2016/07/22   11:58:13
ICCOPT 201636
PROGRAM AT A GLANCE
fl room
NO P Toint
Sandra A Santos
Oleg Burdakov
Zaikun Zhang
NO YF Liu
Feng Min Xu
Wei Bian
 
PDE-O M Ulbrich
Reinhold Schneider
Oliver Lass
Michael Ulbrich
DSO F Rinaldi/Z Zhang
Geovani N Grapiglia
Ubaldo M García-Palomares
Dmitri E Kvasov
AESE AW Dowling
Claudia D'Ambrosio
Rui Huang
Alexander W Dowling
AFE WC Kim
Woong Bee Choi
Chong H Won
Woo Chang Kim
M-OVO A Zemkoho
Stephan Dempe
Alain Zemkoho
Patrick Mehlitz
LO AD Sidford/YT Lee
Hariharan Narayanan
Santosh S Vempala
Jacob Abernethy
RO V Goyal
Phebe Vayanos
Melvyn Sim
Vineet Goyal
OIS C Laird
Jose S Rodriguez
Jean-Paul Watson
Ai Kagawa
RO Y Guan
Matthew D Norton
Ye Wang
CPO G Pataki
Minghui Liu
Preston E Faulk
Takashi Tsuchiya
CNO X Yuan/C Chen
WenYi Tian
 
CNO B Recht/PA Parrilo
Laurent Lessard
Nathan Srebro
Benjamin Recht
The National Art Center, Tokyo will be closed on Tuesday.
The National Art Center, Tokyo will be closed on Tuesday.
Stochastic Robustness of Gradient Methods
3r
d 
flo
or
 o
f N
AC
T m3S
(Auditorium)
m3AB
(Lecture 
Rooms A&B)
Automating the Analysis and Design of Large-Scale Optimization Algorithms5L
(Lecture 
Room L)
Notions of Robustness and Dynamics in Convex Optimization: Part II
Stability as the Master Force Behind Stochastic Gradient Descent
5K
(Lecture 
Room K)
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
Faster Alternating Direction Method of Multipliers with an O(1/n2) Convergence Rate5J
(Lecture 
Room J)
Recent Advances in Splitting Methods for Large-Scale Convex Programming: Part II
Solving SDP Completely with an Interior-Point Oracle
Preprocessing Semidefinite Programs
Exact Duals and Short Certificates of Infeasibility and Weak Infeasibility in Conic Linear Programming: Part 25I
(Lecture 
Room I)
Geometry, Duality and Complexity in Conic Linear Programming I
Applications of the Earth Mover’s Distance in Optimization
Buffered Probability of Exceedance, A New Characterization of Uncertainty and Application to Support Vector Machines and Robust Optimization5H
(Lecture 
Room H)
Robust Optimization and Applied Probability
The Rectangular Maximum Agreement Problem
Parallel Scenario-based Decomposition Methods for Solving the Contingency-constrained AC Optimal Power Flow Problem
A Parallel Nonlinear Interior-Point Approach for Dynamic Optimization Problems 5G
(Lecture 
Room G)
Parallel Implementations and Algorithms for Continuous Optimization
5F
(Lecture 
Room F)
Piecewise Affine Policies for Two-Stage Robust Optimization under Demand Uncertainty
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
Satisficing Awakens: Models to Mitigate Uncertainty
Robust Wait Time Estimation in Resource Allocation Systems with an Application to Kidney Allocation5E
(Lecture 
Room E)
Robust Optimization: Theory and Applications
Faster Convex Optimization: Simulated Annealing with an Efficient Universal Barrier
Geodesic Gliding and Polytope Sampling
Randomized Interior Point Methods for Sampling and Optimization5D
(Lecture 
Room D)
Theoretical Advances in Linear Optimization ― Sampling Methods
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
5C
(Lecture 
Room C)
Stationarity Concepts for Bilevel Optimization Problems with Lower Level Constraints in Lebesgue Spaces
5t
h 
flo
or
 o
f G
R
IP
S
5A
(Lecture 
Room A)
Bilevel Optimization: Theory and Solution Methods
Solution Algorithm for Optimistic Bilevel Optimization Problems
Newton Method for Bilevel Optimization
Personalized Asset-Liability Management Service: Products, Markets, Regulations and Technologies
The Peculiarity of Liability of National Pension in Korea and the Way to Sustain Pension Scheme
Extending the Scope of ALM to Social Investment ― Investing in Population Growth to Enhance Sustainability of Korea National Pension Service
A Stochastic Programming Framework for Multi-Stakeholder Decision-Making and Conflict Resolution
4B
(Research 
Meeting 
Room 4B)
Asset-Liability Management
4t
h 
flo
or
 o
f G
R
IP
S 4A
(Research 
Meeting 
Room 4A)
Energy Systems I
Strong Valid Inequalities for the Standard Pooling Problem
Challenges and Opportunities for Optimization-based Workflow in Industry 
On Numerical Comparison of Deterministic and Stochastic Derivative-free Global Optimization Algorithms
An Approach for Solving Mixed Integer Nonlinear Optimization Problems via Derivative Free Optimization Techniques
1C
(Meeting 
Room 1C)
Advances in Derivative-free and Simulation-based Optimization I
Nonmonotone Derivative-free Trust-Region Algorithms for Composite Nonsmooth Optimization
Constrained Optimization with Low-Rank Tensors and Applications to Problems with PDEs under Uncertainty
A Second Order Approximation Technique for Robust Optimization in Parametrized Shape Optimization
1B
(Meeting 
Room 1B)
Numerical Methods for PDE-constrained Optimization under Uncertainty
Hierarchical Tensor Approximation for Optimal Control with Uncertain Coefficients
Optimality and Some Numerical Analysis for Constrained Optimization Problems with Nonconvex Regularization
1A
(Meeting 
Room 1A)
Nonconvex and Non-Lipschitz Optimization: Algorithms and Applications 2
Theory and Algorithms for Sparse Finance Optimization
Tue.B     13:15-14:30  Tuesday, August 9th
1s
t f
lo
or
 o
f G
R
IP
S
1S
(Soukairou 
Hall)
Nonlinear Optimization Algorithms and Their Complexity I
Evaluation Complexity for Nonlinear Constrained Optimization using Unscaled KKT Conditions and High-Order Models
Limited Memory Algorithms with Cubic Regularization
A Space Transformation Framework for Nonlinear Optimization
2016_iccopt.indb   36 2016/07/22   11:58:13
ICCOPT 2016 37
PROGRAM AT A GLANCE
fl room
NO J Griffin/W Zhou
Scott R Pope
Wenwen Zhou
 
NO YF Liu
Bo Jiang
Yun Shi
Ya-Feng Liu
PDE-O R Herzog
Sven-Joachim Kimmerle
Ailyn Stötzner
Cedric Sehrt
DSO F Rinaldi/Z Zhang
Enlu Zhou
Hiva Ghanbari
Raghu Pasupathy
AESE F Gilbert
Morteza Ashraphijuo
Mouhacine Benosman
Francois Gilbert
CVI S Cui
Alexandra Schwartz
Andrew Lu Liu
Tatsuya Hirano
M-OVO S Villa
Hongzhou Lin
Lorenzo A Rosasco
Silvia Villa
LO AD Sidford/YT Lee
Di Wang
Damian Straszak
Daniel N Dadush
RO R Jiang
Yongpei Guan
Ruiwei Jiang
Siqian Shen
OIS C Bueskens
Sören Geffken
Renke Schäfer
Christian Kirches
AFE J Chen
Chanaka Edirisinghe
Shushang Zhu
Jingnan Chen
CPO G Pataki
Frank N Permenter
Shota Yamanaka
Masakazu Muramatsu
CNO H Attouch
Hedy Attouch
Juan Peypouquet
Garrigos Guillaume
CNO B Recht/PA Parrilo
Venkat Chandrasekaran
Pablo A Parrilo
 
The National Art Center, Tokyo will be closed on Tuesday.
3r
d 
flo
or
 o
f N
AC
T
The National Art Center, Tokyo will be closed on Tuesday.m3S
(Auditorium)
m3AB
(Lecture 
Rooms A&B)
Fitting Convex Sets to Data via Matrix Factorization
Notions of Robustness and Dynamics in Convex Optimization: Part III
5L
(Lecture 
Room L) Switched System Analysis via Dual/Sum-of-Squares Techniques
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
5K
(Lecture 
Room K)
Convergence Rates in Convex Optimization: Going beyond the Worst-Case Analysis
The Rate of Convergence of Nesterov's Accelerated Forward-Backward Method is Actually Faster Than 1/k25J
(Lecture 
Room J)
Fast Inertial Proximal-Gradient Methods for Structured Optimization: O(1/k2) and Beyond
Weak Infeasibility, Facial Reduction, and Geometry in Second-Order Cone Programming
A Fast Convergent First-Order Method bearing Second-Order Information
5H
(Lecture 
Room H)
Financial Decision Making under Distress
Duality of a Generalized Absolute Value Optimization Problem
A Reduction Method for SDP Based on Projection Lattices and Jordan Algebras5I
(Lecture 
Room I)
Geometry, Duality and Complexity in Conic Linear Programming II
Optimal Portfolio Deleveraging under Cross-Asset Price Pressure
Optimally Manage Crash Risk
To Track or Not to Track: Can Economic and Financial Indicators Help Smart-Beta Funds?
5G
(Lecture 
Room G)
Numerical Methods for Large Scale Nonlinear Optimisation
SQP Methods and QP Hot-starting for Nonlinear Model Predictive Control
Implementation of a Penalty-Interior-Point Algorithm within WORHP
Parametric Sensitivity Analysis within Sequential Quadratic Programming ― Post Optimality Analysis of Subproblems
5F
(Lecture 
Room F)
Distributionally Robust Chance-constrained Bin Packing Problem
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
Two-Stage Stochastic Program with Distributional Ambiguity
Risk-averse Stochastic Unit Commitment with Incomplete Information5E
(Lecture 
Room E)
Ambiguity-aware Decision Making under Uncertainty
5D
(Lecture 
Room D)
Theoretical Advances in Linear Optimization ― New Perspectives
Solving Linear Programs via Rescalable Geometry
Slime Molds and Sparse Recovery
Faster Approximation for Packing and Covering LPs
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
5C
(Lecture 
Room C)
5t
h 
flo
or
 o
f G
R
IP
S
5A
(Lecture 
Room A)
Convex Optimization for Learning and Data Sciences
Computational Regularization in Learning and Inverse Problems
A Universal Catalyst for First-Order Optimization
Less is More: Optimal Learning with Stochastic Projection Regularization
Multi-Leader Single-Follower Game between Suppliers with a Manufacturer
Distributed Algorithms for Potential Generalized Nash Equilibrium Problems (GNEPs) and Nonseparable Optimization Problems
A Reformulation of Sparse Optimization Problems using Complementarity-Type Constraints
Handling Dynamic Constraints in Power System Optimization
4B
(Research 
Meeting 
Room 4B)
Applications of Complementarity Models: Sparsity and Games
4t
h 
flo
or
 o
f G
R
IP
S
Adaptive Sampling Recursions for Simulation Optimization
AUC Maximization and Tuning Parameters of Cost Sensitive Logistic Regression via Derivative Free Optimization
1C
(Meeting 
Room 1C)
Randomized Methods and Stochastic Problems
Gradient-based Stochastic Search for Simulation Optimization
4A
(Research 
Meeting 
Room 4A)
Energy Systems II
A Strong Semidefinite Programming Relaxation of the Unit Commitment Problem
Data-driven Optimal Reduced Order Model Tuning for Partial Differential Equations: Application to the 3D Boussinesq Equation
Optimal Control of Scalar Transport in Incompressible Fluid Flow
Optimal Control of Thermoviscoplasticity
1B
(Meeting 
Room 1B)
Optimal Control of Coupled Systems
Optimal Control of a Coupled System of a Vehicle Transporting a Fluid Subject to Shallow Water Equations
1s
t f
lo
or
 o
f G
R
IP
S
1S
(Soukairou 
Hall)
Optimization in Machine Learning I
Combining Information from Second-Order Solvers and SGD
A Modified Conjugate Gradient Method with Warm-Starts for Large-Scale Nonconvex Optimization Problems
Composite Lq (0<q<1) Minimization over Polyhedron
Numerical Algorithms for PDE-constrained Optimization with Non-convex Non-smooth Objectives
1A
(Meeting 
Room 1A)
Nonconvex and Non-Lipschitz Optimization: Algorithms and Applications 3
Structured Nonconvex Optimization Models: Algorithms and Iteration Complexity Analysis
Tue.C     14:45-16:00  Tuesday, August 9th
2016_iccopt.indb   37 2016/07/22   11:58:13
ICCOPT 201638
PROGRAM AT A GLANCE
fl room
NO R Marcia/J Erway
William W Hager
Joshua D Griffin
Elizabeth Wong
NO YF Liu
Dong Kang
Cheng Chen
Zhilong Dong
PDE-O T Takeuchi
Tomoaki Hashimoto
Kentaro Yaji
Masato Kimura
DSO F Rinaldi/Z Zhang
Alessandra Papini
Margherita Porcelli
Warren L Hare
AESE J Lavaei
Marc D Vuffray
Abdulrahman Kalbat
Javad Lavaei
CVI M Wang/S Cui
Shisheng Cui
Yue Xie
Sun Jie
M-OVO D Kuroiwa
Matteo Rocca
Daishi Kuroiwa
Kazuki Seto
SOIP C Cartis
John Wright
Mahdi Soltanolkotabi
Rachel Ward
RO V Gupta
Adam Elmachtoub
Paul Grigas
Vishal Gupta
LO SD Ahipasaoglu/G Nannicini
Roland Wunderling
Matthias Miltenberger
Soomin Lee
AFE D Li
Moris S Strub
Jianjun Gao
Duan Li
CPO R Hildebrand
Cristobal Guzman
Ronen Eldan
Roland Hildebrand
CNO Q Lin
Peter Richtarik
Antonin Chambolle
Lin Xiao
CS CHJ Pang
Masaru Ito
Naoki Ito
CH Jeffrey Pang
CPO AA Ahmadi
Greg Blekherman
Ahmadreza Marandi
Jiawang Nie
The National Art Center, Tokyo will be closed on Tuesday.
The National Art Center, Tokyo will be closed on Tuesday.
Positive Maps and Separable Matrices
3r
d 
flo
or
 o
f N
AC
T m3S
(Auditorium)
m3AB
(Lecture 
Rooms A&B)
Spectrahedral Cones with Rank 1 Extreme Rays, Sums of Squares and Matrix Completion5L
(Lecture 
Room L)
Algebraic Methods in Polynomial Optimization
The Supporting Halfspace-quadratic Programming Strategy for the Dual of the Best Approximation Problem
The Bounded SOS Hierarchy for Bilinear Programming
An Adaptive Restarting for Universal Gradient Method of Minimizing Strongly Convex Functions5K
(Lecture 
Room K)
First Order Methods and Applications
A Randomized Asynchronous Algorithm for Distributed Optimization with Parameter Servers
Fast Accelerated Proximal Gradient Method and Its Application to Unified Classification Algorithm
Stochastic Dual Ascent for Solving Linear Systems5J
(Lecture 
Room J)
Primal-Dual Algorithm for Convex Optimization
Barriers on Symmetric Cones
Remarks on Acceleration for Primal-Dual Algorithms
5H
(Lecture 
Room H)
Optimization in Portfolio Selection and Risk Management
The Entropic Barrier: A Universal and Optimal Self Concordant Barrier
New Upper Bounds for the Density of Translative Packings of Three-dimensional Convex Bodies with Tetrahedral Symmetry5I
(Lecture 
Room I)
Barriers in Conic Optimization
Quadratic Convex Reformulations for Semi-continuous Quadratic Programming and Its Application in Cardinality Constrained Mean-Variance Portfolio Selection
On Multiperiod Mean-CVaR Portfolio Optimization
Portfolio Optimization with Non-recursive Reference Point Updating
Primal-Dual Method for Decentralized Online Optimization
5G
(Lecture 
Room G)
Linear Optimization and Computation
Improving the CPLEX LP Solver
LP Solution Polishing to Improve MIP Performance
5F
(Lecture 
Room F)
No session
Empirical Bayes and Optimization in the Small-Data Regime
An Extended Frank-Wolfe Method with “In-Face” Directions, and Its Application to Low-Rank Matrix Completion
Smart "Predict, Then Optimize"
Recent Advances in Data-driven Optimization
5E
(Lecture 
Room E)
5D
(Lecture 
Room D)
Sparse and Low Rank Approximation
A Semidefinite Relaxation for Computing Distances between Metric Spaces
Breaking Sample Complexity Barriers via Nonconvex Optimization?
Nonconvex Recovery of Low Complexity Models
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
5C
(Lecture 
Room C)
5t
h 
flo
or
 o
f G
R
IP
S
Generalized Convexity for Set-valued Maps and Its Applications
5A
(Lecture 
Room A)
Generalized Convexity and Set Optimization
Robust Vector Optimization: Well-Posedness, Sensitivity to Uncertainty and Generalized Convexity of Set-valued Maps
Unified Approach in Set Optimization and Generalized Convexity for Set-valued Maps
A Distributionally Robust Model for Three Stage Stochastic Linear Optimization
On the Resolution of Complementarity Formulations of the L0-Norm Minimization Problem via ADMM Schemes
Stochastic Optimization and Variational Inequality Problems
On the Analysis of Three Stochastic Extragradient Variants for Monotone Stochastic Variational Inequality Problems
Power System State Estimation with a Limited Number of Measurements
4B
(Research 
Meeting 
Room 4B)
Using Inexact Subgradients to Compute Proximal Points of Convex Functions
Global Derivative-free Quasi-Newton Methods for Bound-constrained Nonlinear Systems
1C
(Meeting 
Room 1C)
Advances in Derivative-free and Simulation-based Optimization II
An Implicit Filtering-based Algorithm for Derivative Free Multiobjective Optimization
4t
h 
flo
or
 o
f G
R
IP
S 4A
(Research 
Meeting 
Room 4A)
Optimization Models in Energy
Monotonicity Properties in Dissipative Flow Networks
Optimal Distributed Control of Power Systems with a High Level of Renewable Energy
Shape Optimization Approach to Free Boundary Problems by Traction Method
Topology Optimization for Fluid Dynamics Problems and Its Applications in Flow Channel Design
1B
(Meeting 
Room 1B)
PDE Optimization and Applications I
Receding Horizon Control for Spatiotemporal Dynamic Systems
1s
t f
lo
or
 o
f G
R
IP
S
1S
(Soukairou 
Hall)
Large-Scale Nonlinear Optimization
An Active Set Algorithm for Nonlinear Optimization with Polyhedral Constraints
A New Successive Subspace Method for Solving the Trust-Region Subproblem
A General Proximal Quasi-Newton Method for Large Scale l1 Penalized Optimization Problem
A Subspace Multilevel Method for Nonlinear Optimization
1A
(Meeting 
Room 1A)
Nonconvex and Non-Lipschitz Optimization: Algorithms and Applications 4
New Strategies of Stochastic RBF Method for Expensive Black-Box Global Optimization
Tue.D     16:30-17:45  Tuesday, August 9th
Methods for Large- and Medium-Scale Nonlinear Optimization
2016_iccopt.indb   38 2016/07/22   11:58:13
ICCOPT 2016 39
PROGRAM AT A GLANCE
fl room
NO O Gunluk
Sanjeeb Dash
Andy Sun
Oktay Gunluk
NO X Liu/Y Wang
Yanfei Wang
Cong Sun
Ran Gu
PDE-O T Takeuchi
Yikan Liu
Genta Kawahara
Takeshi Ohtsuka
DSO F Rinaldi/Z Zhang
Anne Auger
Serge Gratton
 
AESE AU Raghunathan
Ermin Wei
Arvind U Raghunathan
 
CVI SK Mishra
Jein-Shan Chen
Balendu B Upadhyay
Mengdi Wang
LO M Anjos
Leo S Liberti
Tamás Terlaky
Miguel Anjos
RO O Nohadani
Anil Aswani
Edwin Romeijn
Omid Nohadani
M-OVO J Garg
Vladimir Shikhman
Jugal Garg
Joseph M Ostroy
LO A Deza
Noriyoshi Sukegawa
George O Manoussakis
Domingos M Cardoso
AFE J Gotoh
Alba V Olivares-Nadal
Jang Ho Kim
Andrew Lim
CS M Michta
Hiroyuki Kasai
Kai A Spürkel
Mariusz Michta
CPO AMC So
Chao Ding
Huikang Liu
Zirui Zhou
CPO J Nie/JB Lasserre
Gue Myung Lee
Jeya Jeyakumar
Victor L Magron
CNO M Teboulle/S Sabach
Edouard Pauwels
Nadav Hallak
Marc Teboulle
GO R Misener
Rohit Kannan
Remigijus Paulavicius
Radu Baltean-Lugojan
Enhancing the Performance of BASBL: Branch-And-Sandwich BiLevel Solver with the Adaptive Branching, Domain Reduction and Parallel Computing Schemes
m3AB
(Lecture 
Rooms A&B)
Advances in Deterministic Global Optimization I
Convergence-Order Analysis of Lower Bounding Schemes for Constrained Global Optimization Problems
A Parametric Approach to Solving the Pooling Problem
Beyond Lipschitz Gradient Continuity: A Novel Path for First Order Methods
3r
d 
flo
or
 o
f N
AC
T m3S
(Auditorium)
Recent Advances in First-Order Methods: Part I
Sequential Convex Programming, Value Function and Convergence
On Computing the Proximal Mapping Associated with the l0-Norm over Symmetric Sets
Convergent Robust SDP Approximations for Semialgebraic Optimization
On Stability and Genericity Results for Polynomial Optimization Problems 5L
(Lecture 
Room L)
Moments, Positive Polynomials & Optimization: Part III
Globally Solving Polynomial Mathematical Programs with Equilibrium Constraints
5K
(Lecture 
Room K)
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
A Unified Approach to Error Bounds for Structured Convex Optimization
Stochastic Optimization: Theory and Algorithms
5J
(Lecture 
Room J)
Matrix Optimization Problems: Recent Advances in Convergence Rate Analysis and Recovery Guarantees
Convex Optimization Learning of Faithful Euclidean Distance Representations in Nonlinear Dimensionality Reduction
Quadratic Optimization with Orthogonality Constraints: Explicit Lojasiewicz Exponent and Linear Convergence of Line-Search Methods
Strong Convexity in Two-Stage Linear Stochastic Programs with Partially Random Right-Hand Side
Riemannian Stochastic Variance Reduced Gradient on Grassmann Manifold
Properties of Weak Solutions to Stochastic Inclusions and Their Applications in Optimization Problems
5I
(Lecture 
Room I)
Robust Empirical Optimization
Higher Factor Dependency of Robust Portfolios for Achieving Robustness
5H
(Lecture 
Room H)
Robust Portfolio Optimization
A Robust Perspective on Transaction Costs in Portfolio Optimization
Star Sets/Star Complements of Graph Eigenvalues and Simplex Like Techniques in Combinatorial Problems
5G
(Lecture 
Room G)
Polynomial-Time Complementary Pivot Algorithms for Market Equilibria
On the Diameter of Lattice Polytopes
Algorithmic and Geometric Aspects of Linear Optimization
Computation of Fisher-Gale Equilibrium by Auction5F
(Lecture 
Room F)
Price-taking Equilibrium in Games
Improving Bounds on the Diameter of a Polyhedron in High Dimensions
Robust Maximum Likelihood Estimation with Application to Radiation Therapy
Mathematical Programming and Economic Equilibria
Accounting for the Tongue-and-Groove Effect in IMRT Treatment Planning using a Robust Direct Aperture Optimization Approach
Numerical Solution of Bilevel Programs using a Duality-based Approach5E
(Lecture 
Room E)
Computational Study of Some Valid Inequalities for k-Way Graph Partitioning
A Polynomial Column-wise Rescaling von Neumann Algorithm
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
5C
(Lecture 
Room C)
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
5D
(Lecture 
Room D)
Computational and Complexity Challenges for Linear Conic Optimization
A Random Projection Method for Solving Linear Programs
Advances in Robust Optimization I
5t
h 
flo
or
 o
f G
R
IP
S
5A
(Lecture 
Room A)
On Relations between Vector Variational-like Inequalities and Vector Optimization Problems in Asplund Spaces
4B
(Research 
Meeting 
Room 4B)4
th
 fl
oo
r o
f G
R
IP
S 4A
(Research 
Meeting 
Room 4A)
Data and Networks I
Parallel Multi-splitting Proximal Method
Dual Decomposition and Nonsmooth Equations
Vector Variational Inequalities and Applications
On New Discrete-Type Complementarity Functions
Direct Search Based on Inaccurate Function Values
On the Linear Convergence of Comparison-based Step-size Adaptive Randomized Search
Online Markovian Decision Problems as a Stochastic Minimax Problem
Optimal Control Problem for Allen-Cahn Type Equation Associated with Total Variation Energy
Optimization of Heat Transfer in Plane Couette Flow
Optimization over Structured Subsets of Positive Semidefinite Matrices via Column Generation
Cutting Planes to Strengthen Second Order Conic Relaxation of the OPF Problem
Semidefinite Penalty Method for Quadratically Constrained Quadratic Programming
1B
(Meeting 
Room 1B)
PDE Optimization and Applications II
Iterative Thresholding Algorithm for Inverse Source Problems for Hyperbolic-Type Equations
1C
(Meeting 
Room 1C)
Theoretical Aspects of Derivative-free Optimization
On a Special Structured Matrix Problem
1A
(Meeting 
Room 1A)
Optimization Methods for Inverse Problems 1
Seismic Diffraction Extraction for Discontinuous Geologies using Sparse Regularization
1s
t f
lo
or
 o
f G
R
IP
S
1S
(Soukairou 
Hall)
MIP + NLP
Wed.A     13:45-15:00  Wednesday, August 10th
Solving Box-constrained Nonconvex QPs
2016_iccopt.indb   39 2016/07/22   11:58:13
ICCOPT 201640
PROGRAM AT A GLANCE
fl room
NO C Sun
Xin Liu
Bo Jiang
Qingna Li
NO X Liu/Y Wang
Xiucui Guan
Bo Wen
 
PDE-O F Tröltzsch/I Yousept
Irwin Yousept
Peter Gangl
Fredi Tröltzsch
DSO F Rinaldi/Z Zhang
Matt Menickelly
Satyajith Amaran
Youssef M Marzouk
AESE NY Chiang
Hassan Mansour
Ruth Misener
Nai-Yuan Chiang
CVI U Shanbhag
Todd Munson
Yura Malitsky
Tianyu Hao
M-OVO R Wangkeeree/N Petrot
 
Rabian Wangkeeree
Narin Petrot
LO T Terlaky
Lukas Schork
Antoine Deza
Pascal Benchimol
RO V Doan
Xuan Vinh Doan
Varun Gupta
Henry Lam
SOIP M Lotz
Ke Wei
Raphael A Hauser
Axel Flinth
LO TD Hansen
Yann Disser
Thomas D Hansen
Walter Morris
AFE C Yiu
Jingtang Ma
Cedric Yiu
 
CNO M Friedlander
Cho-Jui Hsieh
Madeleine Udell
Rene Vidal
CPO A Yoshise
Daigo Narushima
Mirai Tanaka
Akiko Yoshise
CPO J Nie/JB Lasserre
Jean B Lasserre
Jinyan Fan
Georgina Hall
CNO M Teboulle/S Sabach
Yoel Drori
Shoham Sabach
Amir Beck
GO CA Floudas/NV Sahinidis
Syuuji Yamada
Monica G Cojocaru
Pietro Belotti
Generalized Nash Games and Cap and Trade Environmental Models
m3AB
(Lecture 
Rooms A&B)
Advances in Deterministic Global Optimization II
A Branch and Bound Procedure for a Quadratic Reverse Convex Programming Problem by Listing FJ Points
Primal and Dual Predicted Decrease Approximation Methods
3r
d 
flo
or
 o
f N
AC
T m3S
(Auditorium)
Recent Advances in First-Order Methods: Part II
The Exact Information-based Complexity of Smooth Convex Minimization
A First Order Method for Solving Convex Bi-Level Optimization Problems
Solving Hard Mixed Integer Quadratic and Conic Optimization Problems
DC Decomposition of Nonconvex Polynomials with Algebraic Techniques
Moments, Positive Polynomials & Optimization: Part IV
BSOS: A Bounded-Degree SOS Hierarchy for Polynomial Optimization5L
(Lecture 
Room L)
Rank Minimization Approach to Collaborative Filtering Based on the Nuclear Norm Minimization
Computing the Distance between the Linear Matrix Pencil and the Completely Positive Cone
5K
(Lecture 
Room K)
Some New Results on Conic Optimization and Its Applications to Machine Learning
Inner and Outer Approximations of the Semidefinite Cone using SD Bases and Their Applications to Some NP-hard Problems 
Global Optimality in Matrix and Tensor Factorization, Deep Learning, and Beyond
Diversity Extraction via Condition Number Constrained Matrix Factorization
5J
(Lecture 
Room J)
Sparse Optimization: Algorithms and Applications
Inexact Proximal Newton Methods for Composite Minimization
5I
(Lecture 
Room I)
Making Sketchy Decisions: Semidefinite Programming with Optimal Storage
Hybrid Laplace Transform and Finite Difference Methods for Pricing American Options
No session
5H
(Lecture 
Room H)
Optimization Approaches for Derivative Pricing and Risk Management
Optimal Portfolio and Insurance Problems with Risk Constraint
A Directed Steinitz Theorem for Oriented Matroid Programming
Tomography with Nonlinear Compressed Sensing
An Improved Version of the Random-Facet Pivoting Rule for the Simplex Algorithm
5G
(Lecture 
Room G)
Perspectives on Simplex Algorithms
A Provable Nonconvex Algorithm for Spectrally Sparse Signal Reconstruction
Compressed Sensing Stability through High-dimensional Geometry
The Simplex Algorithm is NP-mighty
The Empirical Divergence-based Distributionally Robust Optimization
5F
(Lecture 
Room F)
Low Complexity Models and Applications
Tight Moments-based Bounds for Queueing Systems
Fréchet Bounds and Distributionally Robust Optimization5E
(Lecture 
Room E)
Advances in Robust Optimization II
Long and Winding Central Paths
Inexact Directions in Interior Point Methods
Euler Polytopes and Convex Matroid Optimization
5D
(Lecture 
Room D)
Recent Advances in Linear Optimization
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
5C
(Lecture 
Room C)
5t
h 
flo
or
 o
f G
R
IP
S
5A
(Lecture 
Room A)
Optimality and Algorithm for Convex and Multiple-Objective Optimization
On Optimality Theorems for Multiobjective Optimization Problems over Feasible Set Defined by Tangentially Convex Inequalities
Methods for Finding Solutions of Convex Optimization and Feasibility Problem without Convex Representation
4t
h 
flo
or
 o
f G
R
IP
S 4A
(Research 
Meeting 
Room 4A)
Data and Networks II
Online Blind Deconvolution in Through-the-Wall Radar Imaging
A Regularized Augmented Lagrangian Filter Method for Nonlinear Building MPC Problems 
4B
(Research 
Meeting 
Room 4B)
Algorithms for Complementarity and Equilibrium Problems
Lexicographic Pivoting for Mixed Linear Complementarity Problems
New Projection Methods for Monotone Variational Inequalities
Using Functional Programming to Recognize Named Structure in an Optimization Problem: Application to Pooling
Probabilistically Fully Linear Models in STORM
On the Implementation of a Trust Region-based Algorithm for Derivative-free Optimization over Stochastic Simulations
A Gaussian Process Trust-Region Method for Derivative-free Nonlinear Constrained Stochastic Optimization
Value Function Based Non-cooperative Games
Optimal Control of Some Quasilinear Parabolic Maxwell Equations
1s
t f
lo
or
 o
f G
R
IP
S
1S
(Soukairou 
Hall)
Optimization Methods and Its Applications
Column-wise Block Coordinate Descent Approach for Orthogonal Constrained Optimization Problems
1B
(Meeting 
Room 1B)
PDE-constrained Optimization in Electromagnetism
Optimization of Non-smooth Hyperbolic Evolution Maxwell's Equations in Type-II Superconductivity
Sensitivity-based Topology and Shape Optimization of an Electric Motor
Lp-Norm Regularization Algorithms for Optimization over Permutation Matrices
1C
(Meeting 
Room 1C)
Derivative-free Optimization Algorithms for Stochastic Problems
1A
(Meeting 
Room 1A)
Optimization Methods for Inverse Problems 2
Inverse Max+Sum Spanning Tree Problem under Hamming Distance by Modifying the Sum-Cost Vector
Linear Convergence of Proximal Gradient Algorithm with Extrapolation for a Class of Nonconvex Nonsmooth Minimization Problems
Wed.B     15:15-16:30  Wednesday, August 10th
A Quadratically Convergent Regularized Semismooth Newton Method for Nonlinear Equations under Error Bound Conditions
2016_iccopt.indb   40 2016/07/22   11:58:14
ICCOPT 2016 41
PROGRAM AT A GLANCE
fl room
NO M De Santis
Bissan Ghaddar
Yufei Yang
Marianna De Santis
NO X Liu/Y Wang
Tingting Wu
Deren Han
Lingling Xu
PDE-O S Ulbrich
Michael Hintermüller
Christian Clason
Stefan Ulbrich
DSO F Rinaldi/Z Zhang
Francesco Rinaldi
Jeffrey Larson
AESE G Scutari
Mingyi Hong
Konstantinos Slavakis
Gesualdo Scutari
CVI U Shanbhag
Thanyarat Jitpeera
Gabriel Haeser
Yina Liu
M-OVO T Tanaka
Yutaka Saito
Yuto Ogata
Issei Kuwano
M-OVO T Bajbar
Jerzy Motyl
Yousuke Araya
Tomas Bajbar
CNO W Yin
Lei Yang
Jinshan Zeng
 
RO B Van Parys
Rahul Mazumder
Bart Van Parys
 
SOIP C Cartis
Yuji Nakatsukasa
Michal Kocvara
Robert M Gower
LO S Ma
Amitabh Basu
Christopher T Ryan
Sam Wong
SO M Claus
Huifu Xu
Johanna Burtscheidt
Matthias Claus
CS YS Niu
Bilian Chen
Ryuta Tamura
Yi-Shuai Niu
CPO S Kim/M Kojima
Shinsaku Sakaue
Sunyoung Kim
Masakazu Kojima
CNO F Kilinc-Karzan
Nam Ho-Nguyen
Mert Gurbuzbalaban
Fatma Kilinc-Karzan
Wed.C     17:00-18:15  Wednesday, August 10th
An Active Set Strategy for Nonlinear Programming Problems with Box Constraints
1A
(Meeting
Room 1A)
Optimization Methods for Inverse Problems 3
Solving Constrained TV2L1-L2 MRI Signal Reconstruction via an Efficient Alternating Direction Method of Multipliers
1s
t f
lo
or
 o
f G
R
IP
S
1S
(Soukairou
Hall)
Advances in Large-Scale Optimization
A Global Optimization Approach for the Valve Setting Problem
Worst-Case and Sparse Portfolio Selection: Insights and Alternatives
Asymmetric Proximal Point Algorithms with Moving Proximal Centers
1B
(Meeting
Room 1B)
Recent Developments in PDE-constrained Optimization I
Optimal Control of Multiphase Fluids and Droplets
1C
(Meeting
Room 1C)
Advances in Derivative-free and Simulation-based Optimization III
Preconditioners for Time-dependent PDE-constrained Optimization and an Implementation Based on Parareal Time-Domain Decomposition
A Nonlinear Primal-Dual Extragradient Method for Nonsmooth PDE-constrained Optimization
A Proximal Alternating Direction Method for Multi-Block Coupled Convex Minimization
A New Derivative-free Method for Integer Programming Problems
In-Network Nonconvex Large-Scale Optimization
Characterization of Weakly Sharp Solutions of a Variational Inequality by Its Primal Gap Function
Asynchronously Parallel Optimization Solver for Finding Multiple Minima
4B
(Research
Meeting
Room 4B)
Algorithms for Variational Inequality and Optimization Problems
Convergence Analysis of Fixed Point Optimization Algorithm for the Triple-hierarchical Constrained Optimization Problem
Decomposing Linearly Constrained Nonconvex Problems by a Proximal Primal-Dual Approach
Accelerated Hybrid Steepest Descent Method for Solving Affinely Constrained Composite Convex Optimization Problems
4t
h 
flo
or
 o
f G
R
IP
S 4A
(Research
Meeting
Room 4A)
Data and Networks III
On the Global Convergence of Nonlinear Optimization Algorithms under Weak Assumptions
A Scalar Characterization of Set-valued Optimization Problems
5t
h 
flo
or
 o
f G
R
IP
S
5A
(Lecture
Room A)
Set-valued Analysis and Nonlinear Scalarization
On Generalization of a Fixed Point Theorem for Set-valued Maps
Generalized Alternative Theorems Based on Set-Relations and an Application to Semidefinite Programming Problems
5C
(Lecture
Room C)
Set-valued and Vector Optimization
Order Convex Selections of Set-valued Functions and Their Applications to Convex Optimization
On the Real Jacobian Conjecture and Newton Polytopes
Existence of Set Equilibrium Problem via Ekeland's Variational Principle
5E
(Lecture
Room E)
5D
(Lecture
Room D)
Nonconvex Splitting Methods and Applications
Alternating Direction Method of Multipliers for a Class of Nonconvex and Nonsmooth Problems with Applications to Background/Foreground Extraction
ExtraPush for Convex Decentralized Optimization over Directed Networks with Extensions
Stochastic Optimization with Data: Large Deviation Limits
A New Perspective on Boosting in Linear Regression via Subgradient Optimization and Relatives
Advances in Robust Optimization III
5F
(Lecture
Room F)
Novel Perspectives on Nonlinear Optimization
Global Optimization via Eigenvalues
Randomized Quasi-Newton Updates are Linearly Convergent Matrix Inversion Algorithms
Theoretical and Algorithmic Developments of Linear Optimization and Semi-infinite Linear Optimization
Projection: A Unified Approach to Semi-infinite Linear Programs and Duality in Convex Programming
On Multigrid Methods in Convex Optimization
Strong Duality and Sensitivity Analysis in Semi-infinite Linear Programming
5G
(Lecture
Room G)
Faster Algorithms for Convex and Submodular Function Minimization
5I
(Lecture
Room I)
On Stability of Stochastic Bilevel Programs with Risk Aversion
No session
5H
(Lecture
Room H)
Stability Analysis in Stochastic Programming
Stability Analysis for Mathematical Programs with Distributionally Robust Chance Constraint
On Stability of Risk Averse Complementarity Problems under Uncertainty
A Mixed Integer Semidefinite Programming Approach for Variable Selection avoiding Multicollinearity
5J
(Lecture
Room J)
Advances in Nonlinear Optimization I
On New Classes of Nonnegative Symmetric Tensors and Applications
On Global Optimization of Mixed-01 Nonlinear Program via DC Algorithms
A Robust Lagrangian-DNN Method for a Class of Quadratic Optimization Problems 
5K
(Lecture
Room K)
SDP and DNN Relaxations of Discrete Polynomial Optimization Problems
Exact SDP Relaxations with Truncated Moment Matrix for Binary Polynomial Optimization Problems
A Lagrangian and Doubly Nonnegative Relaxation for Polynomial Optimization Problems in Binary Variables
5L
(Lecture
Room L)
Advances in First-Order Methods and Handling Uncertainty
First-Order Methods for Robust Convex Optimization
Incremental Methods for Additive Convex Cost Optimization
A Second-Order Cone Based Approach for Solving the Trust Region Subproblem and Its Variants
m3S
(Auditorium)
Another event will be in progress: ICCOPT participants are not allowed to enter.
3r
d 
flo
or
 o
f N
AC
T
m3AB
(Lecture
Rooms A&B)
Another event will be in progress: ICCOPT participants are not allowed to enter.
2016_iccopt.indb   41 2016/07/22   11:58:14
ICCOPT 201642
PROGRAM AT A GLANCE
fl room
NO J Eckstein
Necdet S Aybat
Wotao Yin
Jonathan Eckstein
NO X Liu/Y Wang
Qian Dong
Yong Xia
Hongying Liu
PDE-O S Ulbrich
Winnifried Wollner
Hannes Meinlschmidt
Roland Herzog
DSO F Rinaldi/Z Zhang
Sebastian Stich
Nacer E Soualmi
Youhei Akimoto
AESE T Ohtsuka
Toru Namerikawa
Kenji Hirata
Yusuke Okajima
CS RS Maglasang
Shogo Kishimoto
Gu Yan
Renan S Maglasang
CS P Krokhmal
Takako Hoshiyama
Benjamin M Horn
Pavlo Krokhmal
M-OVO C Günther/M Hillmann
Marius Durea
Marcus Hillmann
 
LO D Bremner
Sebastian Pokutta
Hidefumi Hiraishi
David Bremner
RO W Wiesemann
Frans de Ruiter
Daniel Kuhn
Wolfram Wiesemann
CNO S Zhang
Shiqian Ma
Qihang Lin
Simai He
CS A Oliveira
Toshihiro Kosaki
Lucie Schaynová
Aurelio Oliveira
SO H Sun/D Zhang
Shaojian Qu
Dali Zhang
Hailin Sun
CS A Varvitsiotis
Tang Peipei
Anja Kuttich
Antonios Varvitsiotis
CPO Y Xia
Patrick Groetzner
Shinji Yamada
Ting Kei Pong
CPO LF Zuluaga
Ramtin Madani
Jamie Haddock
Juan C Vera
Another event will be in progress: ICCOPT participants are not allowed to enter.
3r
d 
flo
or
 o
f N
AC
T m3S
(Auditorium)
Another event will be in progress: ICCOPT participants are not allowed to enter.
m3AB
(Lecture 
Rooms A&B)
A Sampling Kaczmarz-Motzkin Algorithm for Linear Feasibility
5L
(Lecture 
Room L)
Polynomial Optimization: Theory and Applications I
Penalized Semidefinite Programming Relaxation for Polynominal Optimization Problems
Positive Polynomials on Unbounded Domains
Explicit Estimation of KL Exponent and Linear Convergence of 1st-Order Methods
5K
(Lecture 
Room K)
Algorithms and Applications for Conic and Related Optimization Problems
Finding Decompositions for Completely Positive Matrices using Orthogonal Transformations
Completely Positive Semidefinite Rank
A Fast Approximation Method for Nonconvex Quadratic Optimizations with Few Constraints
5J
(Lecture 
Room J)
Advances in Conic Optimization
A Two-Phase Algorithm for Large-Scale QPLogdet Optimization Problem
Robust Topology Design of Mechanical Systems under Uncertain Dynamic Loads via Nonlinear Semidefinite Programming
5H
(Lecture 
Room H)
Stochastic Complementarity Problems and Sample Average Approximation
Distributionally Robust Games with an Application to Environmental Problem
SAA-Regularized Methods for Multiproduct Price Optimization under the Pure Characteristics Demand Model
5I
(Lecture 
Room I)
No session
Computation of Stochastic Nash Equilibrium via Variable Sample
A Client's Health from the Point of View of the Nutrition Adviser using Operational Research
Reducing Interior Point Method Iterations via Continued Directions
Distributed Stochastic Variance Reduced Gradient Methods and a Lower Bound for Communication Complexity
Weak Duality Theorems for Two Families of Complex Optimization Problems
Advanced Topics of Linear Optimization
5F
(Lecture 
Room F)
Distributional Robust Optimization for IFR Distributions
Barzilai-Borwein Step Size for Stochastic Gradient Descent
5G
(Lecture 
Room G)
Low-Order Algorithms for Nonlinear Optimization
5E
(Lecture 
Room E)
Advances in Robust Optimization IV
Duality in Two-Stage Adaptive Linear Optimization: Faster Computation and Stronger Bounds
Regularization via Mass Transportation
Ambiguous Joint Chance Constraints under Mean and Dispersion Information
Small Linear Programs for Decision Problems
5D
(Lecture 
Room D)
Extended Formulations and Related Topics
Strong Reductions for Linear and Semidefinite Programs
Necessary Optimality Conditions for Some Nonconvex Facility Location Problems
A Note on Extended Formulations of Lower-truncated Transversal Polymatroids
A Semidefinite Programming Approach to Computing Bounds on the Overall Properties of Composite Materials with Randomly Oriented Fibers
5t
h 
flo
or
 o
f G
R
IP
S
5A
(Lecture 
Room A)
Applications in Production and Energy Economics
To Predict the Bottleneck Node by Queueing Network Modeling of a Production Model with Long Lead Time and Large Variety of Small Quantity Production
Shape Optimization for Contact Problems Based on Isogeometric Analysis and Nonconvex Bundle Methods 
5C
(Lecture 
Room C)
Non-convex Vector Optimization and Applications
Minimal Time Function with Respect to a Set of Directions and Applications
A Tri-Level Optimization Model for Private Road Competition Problem with Traffic Equilibrium Constraints
4B
(Research 
Meeting 
Room 4B)
Applications to Practical Problems
A Successive LP Approach with C-VaR Type Constraints for IMRT Optimization
4A
(Research 
Meeting 
Room 4A)
Optimization in Energy Management Systems with Integrated Economic/Physical Models
Distributed Optimal Power Management Based on Dynamic Pricing in Multi-Period Electricity Market
Real-Time Pricing Leading to Optimal Operation and Applications to Energy Management Systems
A Study on Modeling and Optimization of an Energy Demand Network with Strategic Aggregators
The Shelf Space Allocation Problem under Carbon Tax and Emission Trading Policies
4t
h 
flo
or
 o
f G
R
IP
S
Comparison-based Stochastic Algorithm with Adaptive Gaussian Model for Large-Scale Continuous Optimization
An Indicator for the Switch from Derivative-free to Derivative-based Optimization
1C
(Meeting 
Room 1C)
Derivative-free Optimization Algorithms for Large-Scale Problems
Efficiency of Random Search on Structured Problems
Controlling Feasibility and Optimality in Iterative Solvers for Optimality Systems
Optimal Control of the Thermistor Problem in Three Spatial Dimensions
1B
(Meeting 
Room 1B)
Recent Developments in PDE-constrained Optimization II
PDE Constrained Optimization with Pointwise Gradient Constraints
1s
t f
lo
or
 o
f G
R
IP
S
1S
(Soukairou 
Hall)
ADMM-like Methods for Convex Optimization and Monotone Inclusions
Distributed Proximal Gradient Methods for Cooperative Multi-Agent Consensus Optimization
ARock: Asynchronous Parallel Coordinate Update Framework and Its Application to ADMM
Conditional Gradient Algorithms for Rank-k Matrix Approximations with a Sparsity Constraint 
Generalized Newton Method for Globally Solving the Total Least Squares with Tikhonov Regularization
1A
(Meeting 
Room 1A)
Optimization Methods for Inverse Problems 4
A Parallel Line Search Subspace Correction Method for Convex Optimization Problems
Thu.A     9:00-10:15  Thursday, August 11th
Asynchronous Projective Monotone Operator Splitting Algorithms
2016_iccopt.indb   42 2016/07/22   11:58:14
ICCOPT 2016 43
PROGRAM AT A GLANCE
fl room
NO M Takac
Reza B Harikandeh
Niao He
Martin Takac
NO A Sartenaer/D Orban
Jacek Gondzio
Michael Saunders
Dominique Orban
PDE-O D Ridzal/DP Kouri/B van Bloemen Waanders
Thomas M Surowiec
Denis Ridzal
Bart van Bloemen Waanders
DSO F Rinaldi/Z Zhang
Patrick Koch
Matteo Diez
 
AESE MR de Pinho
Thomas A Weber
Ellina V Grigorieva
Maria dR de Pinho
CS DO Theis
Luis F Bueno
Naoshi Shiono
Dirk O Theis
M-OVO LM Briceño-Arias
Hector Ramirez
Nghia TA Tran
Luis M Briceño-Arias
LO Y Okamoto
May K Szedlák
Hiroyuki Miyata
Sonoko Moriyama
RO I Yanıkoğlu
Boris Houska
Krzysztof Postek
Ihsan Yanıkoğlu
SOIP C Cartis
Somayeh Sojoudi
Raphael Louca
 
CS K Kobayashi
Achmad Maulidin
Chulin Likasiri
Kazuhiro Kobayashi
SO H Sun/D Zhang
Qiyu Wang
Zhaolin Hu
Bintong Chen
CS FACC Fontes
Nimit Nimana
Ning Zheng
Fernando ACC Fontes
CPO LF Zuluaga
Janez Povh
Olga Kuryatnikova
Cedric Josz
CPO RM Freund
Defeng Sun
Simon Lacoste-Julien
Robert M Freund
CNO S Becker
Joseph Salmon
Jessica Gronski
Bang Cong Vu Stochastic Numerical Methods for Monotone Inclusions in Hilbert Spaces
GAP Safe Screening Rule for Sparsity Enforcing Penalties
Nuclear Norms for Collaborative Filtering 
m3AB
(Lecture 
Rooms A&B)
Advances in Large-Scale Nonsmooth Optimization
New Computational Guarantees for Solving Convex Optimization Problems with First Order Methods, via a Function Growth Condition Measure
3r
d 
flo
or
 o
f N
AC
T m3S
(Auditorium)
First-Order Methods for Convex Optimization: New Complexity/Convergence Theory
Linear Rate Convergence of the Alternating Direction Method of Multipliers for Convex Composite Quadratic and Semi-definite Programming
On the Global Linear Convergence of Frank-Wolfe Optimization Variants
New Bounds for Scheduling on Two Unrelated Selfish Machines
5L
(Lecture 
Room L)
Moment/Sum-of-Squares Hierarchy for Complex Polynomial Optimization
A New Approximation Hierarchy for Polynomial Conic Optimization
Polynomial Optimization: Theory and Applications II
Modulus Methods for Box Constrained Least Squares Problems
No session
5K
(Lecture 
Room K)
5J
(Lecture 
Room J)
Advances in Nonlinear Optimization II
Optimal Control of Constrained Nonlinear Systems: An Adaptive Time-Grid Refinement Algorithm Guided by the Adjoint Multipliers
A Hybrid Algorithm for Split Hierarchical Optimization Problems with Fixed Point Constraints in Hilbert Spaces
Dynamic Pricing and Return Pricing for Airline Industry
5I
(Lecture 
Room I)
No session
Convex Risk Measures: Efficient Computations via Monte Carlo
Sparse Portfolio Selection via Linear Complementarity Approach5H
(Lecture 
Room H)
Applications of Stochastic Programming in Finance and Economics
MISOCP Formulation for the Optimal Fuel Routing Problem and the Route Generation Algorithm
A Capacitated Vehicle Routing Problem Approach for Solving Clustering Problem: A Case Study from Chiang Mai, Thailand
A Meta-Heuristic for the Location Routing Problem with Time-dependent Travel Times5G
(Lecture 
Room G)
Routing and Related Problems
Bounds on the Rank of Solutions to Sparse Semidefinite Programs
Large-Scale Graphical Lasso Problems
Decision Rule Bounds for Stochastic Bilevel Programs
5F
(Lecture 
Room F)
Sparsity and Semidefinite Programming Connections
5E
(Lecture 
Room E)
Advances in Robust Optimization V
Robust Optimal Control using Generalized Higher Order Moment Expansions
Robust Optimization with Ambiguous Stochastic Constraints under Mean and Dispersion Information
Geometric Optimization Related with an LCP with SPD-Matrices
Redundancy Detection for Linear Programs with Two Variables per Inequality5D
(Lecture 
Room D)
Discrete and Computational Geometry
New Advances in Sensitivity Analysis of Solution Maps to Parameterized Equilibria with Conic Constraints
On the Linear Convergence of Forward-Backward Splitting Methods
On Classes of Oriented Matroids That Admit 2-dimensional Topological (Geometric) Representations
Projected Chambolle-Pock Splitting for Solving Monotone Inclusions
5t
h 
flo
or
 o
f G
R
IP
S
5A
(Lecture 
Room A)
No session
5C
(Lecture 
Room C)
Variational Analysis, Optimization, and Applications
Location Problem of Supply Facilities in Gas Distribution Networks
4B
(Research 
Meeting 
Room 4B)
Informatics and Geometric Problems
Sequential Equality Programming for Topology Optimization
Optimal Control for Path Planning of AUV using Simplified Models
Computing Unique Information
4t
h 
flo
or
 o
f G
R
IP
S 4A
(Research 
Meeting 
Room 4A)
Applications of Optimal Control
Multiattribute Pricing
Optimally Control Treatment of Psoriasis Skin Disease
A Hybrid Global/Local Multi-Objective Approach to Simulation-based Design Optimization: Deterministic Particle Swarm with Derivative-free Local Searches
1C
(Meeting 
Room 1C)
Applications of Derivative-free and Simulation-based Optimization
Derivative Free Optimization for Automated, Efficient Tuning of Predictive Models 
The Rapid Optimization Library: A PDE-constrained Optimization under Uncertainty Framework
Trust-Region Algorithms for Large-Scale Stochastic Optimization with PDE Constraints
1B
(Meeting 
Room 1B)
Risk-averse Optimization with PDE Constraints I
Risk Averse PDE-constrained Optimization using Coherent Measures of Risk
A Tridiagonalization Method for Saddle-Point and Quasi-definite Systems
The DQQ Procedure for Multiscale Optimization
1A
(Meeting 
Room 1A)
Numerical Linear Algebra and Optimization I
Preconditioning KKT Systems in Interior Point Methods
Stop Wasting My Gradients: Practical SVRG
Fast Optimization for Non-Lipschitz Poisson Regression
Thu.B     10:45-12:00  Thursday, August 11th
Primal-Dual Rates and Certificates
1s
t f
lo
or
 o
f G
R
IP
S
1S
(Soukairou 
Hall)
Optimization in Machine Learning II
2016_iccopt.indb   43 2016/07/22   11:58:14
ICCOPT 201644
PROGRAM AT A GLANCE
fl room
NO M Takac
Julie Nutini
Rachael Tappenden
Zheng Qu
NO A Sartenaer/D Orban
Anders Forsgren
Daniela di Serafino
Daniel Ruiz
PDE-O D Ridzal/DP Kouri/B van Bloemen Waanders
Drew P Kouri
Harbir Antil
Philip Kolvenbach
CS PJS Silva
Hiroshige Dan
Shummin Nakayama
Paulo JS Silva
AESE C Büskens/M Echim
Mitja Echim
Matthias Knauer
Clemens Zeile
AESE T Ohtsuka
Andrew Knyazev
Koji Inoue
Mike Huang
M-OVO DT Luc
Gábor Kassay
Radu Strugariu
Dinh T Luc
LO S Chubanov
Austin Buchanan
Petra R Takács
Sergei Chubanov
RO H Xu
William B Haskell
Huan Xu
 
CS A Gaivoronski
Nobusumi Sagara
Jorge R Vera
Alexei A Gaivoronski
CS P Kirst
Hassan S Nor
Pakeeta Sukprasert
Peter Kirst
CS A Uschmajew
Chengjing Wang
Martin Knossalla
André Uschmajew
CNO KC Toh
Xudong Li
Ying Cui
Kim-Chuan Toh
CPO LF Zuluaga
E Alper Yildirim
Van Nguyen
Luis F Zuluaga Copositive Certificates of Non-negativity
Inner Approximations of Completely Positive Reformulations of Mixed Binary Quadratic Programs
On Completely Positive Modeling of Quadratic Problems
m3AB
(Lecture 
Rooms A&B)
Conic and Polynomial Optimization: Copositive Optimization
SDPNAL+: A Matlab Software for Semidefinite Programming with Bound Constraints
3r
d 
flo
or
 o
f N
AC
T m3S
(Auditorium)
Augmented Lagrangian-based Algorithms for Large-Scale Conic Programming
Fast Algorithm for Lasso
Semidefinite Inverse Quadratic Eigenvalue Problem with Prescribed Entries and Partial Eigendata
A Riemannian Gradient Sampling Algorithm for Nonsmooth Optimization on Manifolds
No session
5L
(Lecture 
Room L)
5K
(Lecture 
Room K)
Algorithms for Nonsmooth Optimization
The Common Limit in the Range of Property for Two Nonlinear Mappings 
A Primal Majorized Semismooth Newton-CG Augmented Lagrangian Method for Large-Scale Linearly Constrained Convex Programming
Bundle Trust-Region Method for Marginal Functions using Outer Subdifferentials
5J
(Lecture 
Room J)
Advances in Nonlinear Optimization III
Solving Disjunctive Optimization Problems by Generalized Semi-infinite Optimization Techniques
A Method of Multipliers with Alternating Constraints for Nonlinear Optimization Problems 
Design of Reconfigurable Networks under Uncertainty by Concurrent Stochastic Optimization and Simulation
No session
5I
(Lecture 
Room I)
Achieving Consistency in Intertemporal Decisions via Stochastic and Robust Approaches
Subdifferentials of Nonconvex Integral Functionals in Banach Spaces with Applications to Stochastic Dynamic Programming5H
(Lecture 
Room H)
Stochastic Optimization: Theory and Applications
A lecture of GRIPS will be in progress: ICCOPT participants are not allowed to enter.
No session
5G
(Lecture 
Room G)
Learning the Uncertainty in Robust Markov Decision Processes
5F
(Lecture 
Room F)
Advances in Robust Optimization VI
5E
(Lecture 
Room E)
A Polynomial Projection Algorithm and Its Applications in Integer Linear Programming and Combinatorial Optimization
Simulation-based Algorithms for Robust Markov Decision Processes
Extended Formulations for Vertex Cover5D
(Lecture 
Room D)
Linear Optimization in the Context of Solving NP-hard Problems
Vector Quasi-Equilibrium Problems for the Sum of Two Multivalued Mappings
A New Type of Directional Regularity for Multifunctions with Applications to Optimization
5C
(Lecture 
Room C)
Vector Equilibrium Problems and Vector Optimization
New Search Direction-based Interior-Point Algorithm for P*(K) Horizontal Linear Complementarity Problems over Cartesian Product of Symmetric Cones
5A
(Lecture 
Room A)
No session
On Equilibrium in Multi-Criteria Transportation Networks
5t
h 
flo
or
 o
f G
R
IP
S
Velocity Form Nonlinear Model Predictive Control of a Diesel Engine Air Path
Manycore Execution of Model Predictive Control
4B
(Research 
Meeting 
Room 4B)
Newton-Krylov Methods in Real-Time Optimization for Nonlinear Model Predictive Control
Recent Advances in Newton-Krylov Methods for NMPC
Mixed-Integer Optimal Control Problems with Indicator Constraints in Automotive Applications
4t
h 
flo
or
 o
f G
R
IP
S 4A
(Research 
Meeting 
Room 4A)
Engineering Applications for Large Scale Nonlinear Optimization
Large-Scale Trajectory Optimization for Autonomous Deep Space Missions
Optimization of Large Scale Characteristics for the Automotive Industry
Strict Constraint Qualifications and Sequential Optimality Conditions for Constrained Optimization
A Memoryless Sized Symmetric Rank-One Method with Sufficient Descent Property for Unconstrained Optimization
1C
(Meeting 
Room 1C)
Nonlinear Optimization: Algorithms and Implementations
Implementation of NLP Solver with Multiple Precision Arithmetic and Numerical Behavior Analysis of SQP Method for Ill-posed NLPs
Nonlinear Robust Optimization using Second-Order Approximations and an Application to the Shape Optimization of Hyperelastic Load-carrying Structures
Optimizing the Kelvin Force in a Moving Target Subdomain
1B
(Meeting 
Room 1B)
Risk-averse Optimization with PDE Constraints II
A Data-driven Approach to PDE-constrained Optimization under Uncertainty
Refining the Bounds from Rusten-Winther with Insights on the Interaction between the Blocks (Hessian vs Constraints) in KKT Systems
BFGS-like Updates of Constraint Preconditioners for Sequences of KKT Linear Systems
1A
(Meeting 
Room 1A)
Numerical Linear Algebra and Optimization II
On Solving an Unconstrained Quadratic Program by the Method of Conjugate Gradients and Quasi-Newton Methods
1S
(Soukairou 
Hall)
Recent Advances in Coordinate Descent Algorithms
Is Greedy Coordinate Descent a Terrible Algorithm?
Flexible Coordinate Descent
Thu.C     13:30-14:45  Thursday, August 11th
Coordinate Descent with Arbitrary Sampling: Algorithms and Complexity
1s
t f
lo
or
 o
f G
R
IP
S
2016_iccopt.indb   44 2016/07/22   11:58:14
ABSTRACTS
ICCOPT 2016 45
ABSTRACTS OF TALKS IN PARALLEL SESSIONS
■ Mon.A.1S
Monday, 10:45-12:00, Room 1S
Nonlinear Optimization and Its Applications I
Cluster: Nonlinear Optimization
Session organized by: Daniel P Robinson
1. Self-correcting Variable-Metric Algorithms for Nonlinear 
Optimization
Frank Edward Curtis (frank.e.curtis@gmail.com) Lehigh University, 
USA
From both geometric and algebraic viewpoints, the self-correcting properties of 
BFGS updating are discussed. These properties, expressed in terms of the 
sequence of BFGS Hessian approximations, lead to useful properties of the 
corresponding inverse Hessian approximations. These latter properties are 
exploited in two proposed algorithms, one for stochastic (nonconvex) 
optimization and one for deterministic convex (nonsmooth) optimization. As 
opposed to popular approaches that employ BFGS updating, neither of the 
proposed algorithms employ line searches.
2. Solving MPCCs with IPOPT
Lorenz T Biegler (lb01@andrew.cmu.edu) Carnegie Mellon 
University, USA, Wei Wan
Mathematical Programs with Complementarity Constraints pose well-known 
difficulties, particularly because they violate constraint qualifications at their 
solution. Nevertheless, strongly stationary MPCC solutions, which are 
characterized by solutions to relaxed nonlinear programming (RNLP) problems 
that satisfy LICQ, can be found through NLP reformulations of the MPCC 
problem. Such reformulations include i) inequality relaxations of the 
complementarity constraints, ii) replacing complementarity constraints by 
smoothed NCP functions and iii) embedding complementarity terms as exact 
penalties. This talk discusses our experiences in extending the well-known 
primal-dual barrier solver, IPOPT to deal with the solution of MPCCs. Our 
hybrid approach combines the penalty parameter adjustment from Leyffer, 
Lopez-Calva and Nocedal (2007) along with a recent structured regularization 
approach in IPOPT that effectively deletes degenerate constraints. The combined 
strategy leads to a workable algorithm with modified Newton steps that depend 
on the local behavior of the complementarity constraints. We evaluate this 
approach on problems from the MacMPEC library, examples that do not have 
strongly stationary solutions, and larger MPCCs based on engineering systems. 
In addition, we compare with NLP strategies derived from the above classes of 
problem reformulations. 
3. A Logical Benders Decomposition Algorithm for Binary-
constrained Quadratic Programs with Complementarity 
Constraints
Andreas Waechter (waechter@iems.northwestern.edu) Northwestern 
University, USA, Francisco Jara-Moroni, John E Mitchell, Jong-Shi 
Pang
We present an algorithm for solving convex quadratic programs with 
complementarity constraints and continuous and binary variables to global 
optimality. Following a Benders decomposition approach, LP and QP 
subproblems compute feasibility cuts that are added to a satisfiability master 
problem. To enhance performance of the method, the cuts are sparsified using an 
l1-norm or l0-norm approach, and approximate optimality cuts are generated to 
steer the master problem towards trial points with good objective values. 
Numerical results are presented.

■ Mon.A.1A
Monday, 10:45-12:00, Room 1A
Methods for Large-Scale Problems
Cluster: Nonlinear Optimization
Session organized by: Francesco Rinaldi
1. Column Generation Approach for the Interceptor Vehicle Routing 
Problem
Joe  Naoum-Sawaya (jnaoumsa@uwaterloo.ca) Ivey Business School, 
Canada, Claudio Gambella, Bissan Ghaddar
We address a generalization of the vehicle routing problem that seeks to minimize 
the vehicles travel time given moving pick-up locations and a common 
destination. The problem is formulated as a mixed integer second order cone 
program and a branch-and-price approach is proposed as a solution methodology. 
Strengthening inequalities are proposed and computational results on instances 
of varying sizes are presented.
2. A Partially Aggregated Dantzig Wolfe Decomposition Algorithm 
for Multi-Commodity Flows
James T Hungerford (jamesthungerford@gmail.com) M.A.I.O.R., 
Srl., USA, Alberto Caprara, Antonio Frangioni, Tiziano Parriani
We consider the standard multi-commodity network flow problem, in which 
several commodities must be routed through a network while sharing resources 
along the network arcs and while optimizing a linear objective function. The 
celebrated Dantzig Wolfe Decomposition Algorithm solves the continuous 
relaxation of the problem by alternating between solving a Lagrangian 
subproblem, in which the arc mutual capacity constraints are relaxed, and a 
master problem, which computes an optimal convex combination over a small 
collection of hand-picked flows. Historically, two versions of this approach have 
been employed: an aggregated version, in which each Lagrangian subproblem 
solution is assigned a single convex multiplier in the master problem; and a 
disaggregated version, in which convex combinations are taken with respect to 
each commodity. In this talk, we investigate the effect on performance of 
employing a partial aggregation strategy, in which commodities are aggregated 
either arbitrarily or according to their similarities, and each aggregate is 
represented in the master problem by a different convex multiplier. A quadratic 
stabilization term is used to speed up convergence. We present preliminary 
computational results which indicate that the optimal level of aggregation often 
lies somewhere in between complete aggregation and complete disaggregation.
3. Scalable and Sparse Optimization in Machine Learning via Frank-
Wolfe Methods
Emanuele Frandi (emanuele.frandi@gmail.com) ESAT-STADIUS, 
KU Leuven, Belgium, Ricardo Nanculef, Marcelo Aliquintuy, Stefano 
Lodi, Claudio Sartori, Johan A K Suykens
Machine Learning problems involving large-scale datasets arise in a wide variety 
of domains and applications. As such, there is a constant demand for high-
performance optimization tools with strong theoretical guarantees which can (1) 
handle the scalability challenges posed by large-scale datasets and (2) provide 
sparse solutions while preserving the accuracy of the underlying models. In this 
context, a helpful tool is provided by the class of Frank-Wolfe (FW, a.k.a. 
Conditional Gradient) methods. We present an overview of our recent work on 
the topic, focusing in particular on the role played by FW optimization in the 
context of two classical Machine Learning problems, SVM training and Lasso 
regression. We first consider both problems on their own, and later show how 
they can be effectively combined to obtain sparse approximations of general 
SVM classifiers. We provide experimental results on a variety of benchmark 
large-scale problems, illustrating how carefully implementing FW methods leads 
to efficient and fast algorithms which can compute accurate models while at the 
same time controlling or enforcing the sparsity of the solution.

■ Mon.A.1B
Monday, 10:45-12:00, Room 1B
Inverse Problems
Cluster: PDE-constrained Optimization
Session organized by: Tomoya Takeuchi
1. A Direct Reconstruction Formula for the Conductivity and 
Permittivity from the Measurements of the Time-harmonic 
Magnetic Field
Takaaki Nara (nara@alab.t.u-tokyo.ac.jp) The University of Tokyo, 
Japan, Tetsuya Furuichi, Motofumi Fushimi, Shigeru Ando
Estimation of the electric conductivity and permittivity inside the human body 
gives precious information for tumor diagnosis. Recently, magnetic resonance 
(MR)-based electrical property tomography attracts considerable attention. The 
problem is to estimate the electric conductivity and permittivity in the time-
harmonic Maxwell equation from the transverse magnetic field component of the 
applied RF field at the Larmor frequency measured with an MRI scanner. 
Conventional methods assume that these electrical properties are homogeneous 
inside the human body, which leads to a reconstruction error. Recently, a linear 
PDE for the inverse of admittivity is derived and a method by using FEM is 
proposed. However, the problem is that it requires the Laplacian of the magnetic 
field, which leads to a severe numerical error. In this paper, we propose a stable 
reconstruction formula for the admittivity.
2. Finite Integration Method for Inverse Heat Conduction Problems
2016_iccopt.indb   45 2016/07/22   11:58:14
ABSTRACTS
ICCOPT 201646
Benny Hon (Benny.Hon@cityu.edu.hk) City University of Hong 
Kong, Hong Kong
In this talk we will present the application of a recently developed Finite 
integration method (FIM) for solving inverse heat conduction problems (IHCPs). 
The use of the Laplace transform technique for temporal discretization and 
Lagrange quadrature formula for spatial integration is shown to be effective and 
efficient for solving IHCPs under regular domains. For problems defined on 
irregular domains, the meshless radial basis function (RBF) is combined with the 
FIM to give a highly accurate spatial approximation to the IHCPs. Numerical 
stability analysis indicates that the convergence of the FIM-RBF is less sensitive 
to the chosen value of the shape parameter contained in the multiquadric radial 
basis functions. The optimal choice for this shape parameter is still an open 
problem, whose value is critical to the accuracy of the approximation. For 
tackling the ill-conditioned linear system of algebraic equations, the standard 
regularization methods of singular value decomposition and Tikhonov 
regularization technique are both adopted for solving IHCPs with data 
measurement errors. Keywords: Finite integration method, inverse heat condition 
problem, Laplace transform, Lagrange formula, radial basis function, singular 
value decomposition, Tikhonov regularization technique
3. Numerical Differentiation by Kernel-based Probability Measures
Leevan Ling (lling@hkbu.edu.hk) Hong Kong Baptist University, 
Hong Kong, Qi Ye
We combine techniques in meshfree methods and stochastic regressions to 
construct kernel-based approximants for numerical derivatives from noisy data. 
We construct Bayesian estimators from normal random variables defined on 
some SPD kernel-based probability measures, in which a Tikhonov regularization 
naturally arise in the formulation and the kernelʼs shape parameter also plays the 
role of the regularization parameter. Our analysis provides two important 
features to this novel approach. First, we show that the conditional mean squared 
error of any approximant is computable without knowing the exact derivative 
and can be used as an a posteriori error bound. This allows user to evaluate the 
approximation quality of any given kernel-based approximant to the derivative, 
and hence, select the best one (in the sense of kernel-based probability) out of 
many resulted from some brute-force approaches.

■ Mon.A.1C
Monday, 10:45-12:00, Room 1C
Derivative-free and Simulation-based Optimization 
with Surrogate Models
Cluster: Derivative-free and Simulation-based Optimization
Session organized by: Francesco Rinaldi, Zaikun Zhang
Session chair: Christine A Shoemaker
1. Surrogate Strategies for Mixed-Variable Derivative-free 
Optimization
Anne-Sophie Crélot (anne-sophie.crelot@unamur.be) University of 
Namur, Belgium, Charlotte Beauthier, Dominique Orban, Caroline 
Sainvitu, Annick Sartenaer
We propose a surrogate management framework for challenging derivative-free 
problems in mixed continuous, integer, discrete and categorical variables. Our 
framework employs the globally convergent mesh-adaptive direct search method 
implemented in the NOMAD library. A radial basis function-based surrogate is 
used to guide the optimization during both the local poll and global search steps.
The surrogate is minimized by way of an evolutionary algorithm implemented in 
the MINAMO package. Our numerical results compare several surrogate 
variants.
2. RBFOpt: An Open-Source Library for Surrogate Model Based 
Optimization
Giacomo Nannicini (nannicini@us.ibm.com) IBM Research, USA
COIN-OR RBFOpt is an open-source library for expensive derivative-free 
optimization, that implements variations of Gutmannʼs RBF method and of Regis 
and Shoemakerʼs Metric Stochastic Response Surface Method. This talk will 
discuss the general framework for the two methods, and the specific contributions 
of RBFOpt: an approach to handle noisy but fast objective function evaluations; 
a fast automatic model selection phase; and a methodology to allow parallel 
asynchronous objective function evaluations. Numerical results will be 
presented.
3. Efficient Mulit Objective Surrogate Global Optimization in 
Parallel with MOPLS and pySOT Toolbox
Christine Annette Shoemaker (ceesca@nus.edu.sg) National 
University of Singapore, Singapore, Taimoor Akhtar
We present a new method MOPLS for Multi objective optimization of multimodal 
functions using surrogates. We show in numerical results that MOPLS greatly 
outperforms NSGA-II and also outperforms PAREGO, AMALGUM, and 
GOMORS. Here MOPLS is used with a radial basis function surrogate, but as 
part of the python toolbox pySOT, both single and multi objective problems can 
be.solved with different surrogates and ensembles of surrogates.. 

■ Mon.A.4A
Monday, 10:45-12:00, Room 4A
Role of Optimization in Graphical Models Inference
Cluster: Applications in Energy, Science and Engineering
Session organized by: Areesh Mittal
1. Robust Estimation for Gaussian Graphical Modeling and Its 
Application to Gene Expression Data
Kei Hirose (hirose@imi.kyushu-u.ac.jp) Kyushu University, Japan, 
Hironori Fujisawa
In Gaussian graphical model, a penalized maximum likelihood approach with the 
L1 penalty is often used. However, the penalized maximum likelihood procedure 
is sensitive to outliers. To overcome this problem, we introduce a robust 
estimation procedure based on the gamma-divergence. The proposed method has 
a redescending property, which is known as a desirable property in robust 
statistics. The parameter estimation procedure is constructed using the Majorize-
Minimization algorithm, which guarantees that the objective function 
monotonically decreases at each iteration. Extensive simulation studies showed 
that our procedure performed much better than the existing methods, in particular, 
when the contamination rate was large. A real data analysis was carried out to 
illustrate the usefulness of our proposed procedure. 
2. Approximate Techniques for Boltzmann Machines
Muneki Yasuda (muneki@yz.yamagata-u.ac.jp) Yamagata University, 
Japan
Boltzmann machine is a type of graphical model based on a Markov random 
field, and is a fundamental model in the field of deep learning, for example, 
restricted Boltzmann machine and deep Boltzmann machine. Unfortunately, we 
cannot perform inference and learning on Boltzmann machines without an 
approximation, because they require a summation over all configurations of 
variables, namely, the combinatorial explosion problem. Various approximate 
techniques for Boltzmann machines have been developed, such as advanced 
mean-field approximation, contrastive divergence, maximum pseudo-likelihood 
method, maximum composite likelihood method, and so on. In the first part of 
this talk, we overview Boltzmann machines and some practical learning 
algorithms for Boltzmann machines. In the second part, we tough on some recent 
developments of learning and inference algorithms for Boltzmann machines 
including restricted Boltzmann machines and deep Boltzmann machines.
3. Changing Graph Structure for Performing Fast, Approximate 
Inference in Graphical Models
Areesh Mittal (areeshmittal@utexas.edu) University of Texas at 
Austin, USA, Nedialko Dimitrov
Complexity of exact marginal inference algorithms in probabilistic graphical 
models is exponential in the treewidth of the underlying graph. We develop a 
method to perform approximate inference on discrete graphical models by 
modifying the graph to another graph with a desirable edge structure. If the new 
graph structure has low treewidth, then performing exact inference on it becomes 
tractable. Performing exact inference on the new graph gives an approximate 
solution to the inference on original graph. We derive error bounds on the 
approximate inference solution as compared to the exact inference solution. We 
show that the problem of finding parameters of the new graph which gives the 
tightest error bounds can be formulated as a linear program (LP). The number of 
constraints in the LP grow exponentially with the number of nodes in the graph. 
To solve this issue, we develop a row generation algorithm to solve the LP.

■ Mon.A.4B
Monday, 10:45-12:00, Room 4B
Algorithmic Advances in PDE-constrained Optimization
Cluster: PDE-constrained Optimization
Session organized by: Anton Schiela
1. Shape Optimization Algorithms for Inverse Modeling in Extreme 
Scales
2016_iccopt.indb   46 2016/07/22   11:58:14
ABSTRACTS
ICCOPT 2016 47
Martin Siebenborn (siebenborn@uni-trier.de) Trier University, 
Germany, Volker Schulz
In many applications, modeled by partial differential equations, there is a small 
number of spatially distributed materials or parameters distinguished by 
interfaces forming complex contours. We present an algorithm that utilizes 
multigrid strategies and quasi Newton updates in order to achieve scalability on 
very large shape optimization problems. We show how this can be implemented 
into an augmented Lagrangian method such that geometric constraints on the 
shape can be incorporated additionally. In this context optimizing shapes 
automatically means to deform finite element meshes iteratively. This usually 
deteriorates the quality of the discretization and thus affects the performance of 
the solver. We therefore introduce novel shape metrics that show good 
performance in retaining aspect ratios of discretization elements. The presented 
algorithm is shown to perform on two applications. A standard test case is to 
identify the shape that minimizes energy dissipation in a Stokes flow. Here we 
demonstrate that the proposed algorithm retains good mesh quality. Furthermore, 
it is shown how geometric constraints are incorporated. The second application 
is the identification of the shape of human skin cells. It is demonstrated how the 
distribution of permeability coefficients in a computational model for the human 
skin are fitted to measurements.
2. Non-uniform Adaptive Lossy Trajectory Compression for Optimal 
Control of Parabolic PDEs
Sebastian Goetschel (goetschel@zib.de) Zuse Institute Berlin, 
Germany
For the solution of optimal control problems governed by parabolic PDEs, 
methods working on the reduced objective functional are often employed to 
avoid a full spatio-temporal discretization of the problem. The evaluation of the 
reduced gradient requires one solve of the state equation forward in time, and one 
backward solve of the adjoint equation. As the state enters into the adjoint 
equation, the storage of a 4D data set is needed. To get accurate numerical results, 
in many cases very fine discretizations are necessary, leading to a huge amount 
of data to be stored. Methods for lossy compression of such trajectories were 
developed as a means to reduce these high demands of storage capacity and 
bandwidth, without a significant loss of accuracy or increase of computational 
complexity. For carefully chosen quantization tolerances, convergence of 
optimization algorithms is not impeded. To extend this approach, in this talk we 
consider the sensitivity of the reduced gradient with respect to the reconstruction 
error of the compressed state. It indicates that the finite element solution 
trajectory should be stored at higher precision in some parts of the space-time 
cylinder, while it can be compressed more severely in other parts. We investigate 
the choice of such non-uniform quantization tolerances, their influence on the 
optimization progress, and present application examples.
3. An Affine Covariant Composite Step Method for Optimization 
with PDEs as Equality Constraints
Anton Schiela (anton.schiela@uni-bayreuth.de) Universität Bayreuth, 
Germany, Lars Lubkoll, Martin Weiser
We propose a composite step method, designed for equality constrained 
optimization with partial differential equations. Focus is laid on the construction 
of a globalization scheme, which is based on cubic regularization of the objective 
and an affine covariant damped Newton method for feasibility. Numerical results 
are shown for optimal control problems subject to nonlinear elastic equations 
arising from an implant design problem in craniofacial surgery. 

■ Mon.A.5C
Monday, 10:45-12:00, Room 5C
Solutions of Equilibrium Problems: Computation and 
Stability
Cluster: Multi-Objective and Vector Optimization
Session organized by: Alexandra Schwartz
1. How to Select a Solution in GNEPs
Axel Dreves (axel.dreves@unibw.de) Universität der Bundeswehr 
Munich, Germany
We propose a new solution concept for generalized Nash equilibrium problems 
(GNEPs). This concept leads under suitable assumptions to unique solutions, 
which are generalized Nash equilibria and the result of a mathematical procedure 
modeling the process of finding a compromise. We first compute the best 
possible strategy for each player, if he could dictate the game, and use the best 
response on the others best possible strategies as starting point. Then we perform 
a tracing procedure, where we solve parametrized GNEPs, in which the players 
reduce the weight on the best possible and increase the weight on the current 
strategies of the others. Finally, we define the limiting points of this tracing 
procedure as solutions. Under our assumptions the new concept selects one 
reasonable out of typically infinitely many generalized Nash equilibria. 
Moreover, we present a Semismooth Tracing Algorithm as an algorithmic 
realization of the solution concept together with its convergence theory and some 
numerical results on diverse problems from literature. 
2. An Interior Point Algorithm for Equality Constrained GNEPs
Sonja Steffensen (steffensen@igpm.rwth-aachen.de) RWTH Aachen, 
Germany, Michael Herty
We present an interior point algorithm that is based on a potential reduction 
Newton method and applied to the KKT-system associated with the equality 
constrained GNEP. Furthermore, we discuss the theoretical properties of such 
problems and analyse the global convergence of the interior point method. 
Finally we will report about some computational results of the implementation of 
our algorithm.
3. Stability and Sensitivity Analysis of Stationary Points in 
Mathematical Programs with Complementarity Constraints
Michal Cervinka (cervinka@utia.cas.cz) Czech Academy of Sciences, 
Czech Republic, Jiri V Outrata, Lukas Adam, Miroslav Pistek
We consider parameter-dependent mathematical programs with constraints 
governed by the generalized nonlinear complementarity problem and with 
additional non-equilibrial constraints. We study a local behavior of stationarity 
maps that assign the respective stationarity points (in particular C- or 
M-stationarity points) of the problem to the parameter. Using recent advances of 
generalized differential calculus we provide various criteria for the isolated 
calmness property and the Aubin property of stationarity maps considered. To 
this end, we derive new effective technique of computation of tangent and 
normal cones to unions of convex polyhedral sets and derive formulas of some 
particular objects of the third-order variational analysis useful for our sensitivity 
analysis.

■ Mon.A.5D
Monday, 10:45-12:00, Room 5D
Various Aspects of Conic Optimization and 
Mathematical Modeling Systems
Cluster: Linear Optimization
Session organized by: Leonid Faybusovich, Takashi Tsuchiya
1. Wasserstein Barycenters of Gaussian Measures
Yongdo Lim (ylim@skku.edu) Sungkyunkwan University, Korea
We show that the Wasserstein least squares problem of Gaussian measures can be 
equivalently transformed to a convex optimization problem on the convex cone 
of positive definite matrices by establishing the strict convexity of the Wasserstein 
distance.
2. A DC programming Approach for Long-Short Multi-Factor 
Model
Kouhei Harada (harada@msi.co.jp) NTT DATA Mathematical 
Systems Inc., Japan, Kensuke Otsuki, Takahito Tanabe
In this talk, we introduce a new application of DC (difference of convex) 
programming to a financial problem. DC programming approach is a well-
known framework to handle non-convex intractable problems. Recently, some 
financial problems, such as long-short MAD (Mean-Absolute Deviation) model, 
has been efficiently solved by this framework. We applied the technique to long-
short multi-factor model, and confirmed its superior performance as well as long-
short MAD model. Although the solution is not globally optimal, some favorable 
features, which is not achieved by applying mere heuristics, are observed. We 
also talk about modeling language SIMPLE. Applying DC algorithms, one might 
have some difficulty with implementation since it is necessary to solve similar 
convex programming problems repeatedly. However, it is shown that we can 
handle them easily by using SIMPLE.
3. Implementation of Interior-Point Methods for LP using Krylov 
Subspace Methods Preconditioned by Inner Iterations
Keiichi Morikuni (morikuni@cs.tsukuba.ac.jp) University of Tsukuba, 
Japan, Yiran Cui, Takashi Tsuchiya, Ken Hayami
We apply inner-iteration preconditioned Krylov subspace methods to 
underdetermined systems of linear equations arising in the interior-point 
algorithm for solving linear programming (LP) problems. The inner-iteration 
preconditioners require small amount of memory compared to previous 
preconditioners, and enable us to overcome the severe ill-conditioning of the 
2016_iccopt.indb   47 2016/07/22   11:58:14
ABSTRACTS
ICCOPT 201648
linear systems in the final phase of interior-point iterations. These Krylov 
subspace methods do not break down for LP problems with rank deficient 
constraint matrices even when previous direct methods fail. Numerical 
experiments on 125 instances from Netlib, QAPLIB and Mittelmannʼs LP 
benchmarks show that our implementation is more stable and robust than the 
standard direct methods SeDuMi and SDPT3. As far as we know, this is the first 
result that an interior-point method entirely based on iterative methods succeed 
in solving a fairly large number of standard LP instances under the standard 
stopping criteria. Moreover, the proposed method outperforms the interior-point 
solver of the state-of-the-art commercial code MOSEK for LP problems with 
random dense rank-deficient ill-conditioned constraint matrices.

■ Mon.A.5E
Monday, 10:45-12:00, Room 5E
Theory and Applications of Robust Optimization
Cluster: Robust Optimization
Session organized by: Melvyn Sim
1. Distributionally Robust Optimization with Semi-infinite 
Ambiguity Sets
Zhi Chen (chenzhi@u.nus.edu) National University of Singapore, 
Singapore, Melvyn Sim, Huan Xu
In distributionally robust optimization (DRO), the distribution of uncertainty is 
only known to belong to an ambiguity set. Recent studies lead to significant 
progress on computationally tractable reformulations of DRO over several 
classes of ambiguity sets. With probability and expectation constraints, these 
ambiguity sets provide support, moments, other statistical information and even 
structural properties of uncertainty. However, their modelling power is limited 
by the allowance of only a finite number of constraints. Motivated from 
commonly used moment ambiguity sets and our new approximation of 
independence among uncertain components, we investigate a class of semi-
infinite ambiguity sets that involve infinite expectation constraints. Thought 
DRO over this class of ambiguity set is generally intractable, we approach the 
intractability by first considering relaxed ambiguity sets with finite expectation 
constraints. We then demonstrate an algorithm based on the worst-case 
distribution that could tractably and gradually tighten the relaxation. We present 
expressive examples of this class of ambiguity sets and show cases where the 
worst-case distribution is relatively easy to verify, thus the algorithm could be 
efficient. We are also inspired by the performance of our new approximation of 
independence.
2. Tolerance-driven Appointment Scheduling and Sequencing using 
Perceived Delay Measures
Shuming Wang (wangshuming@ucas.ac.cn) University of Chinese 
Academy of Sciences, China, Teck Meng Marcus Ang, Tsan Sheng 
Adam Ng
In this paper, we study the problem of appointment scheduling and sequencing in 
the healthcare service setting with uncertain service times and the patientsʼs no-
show information. We propose a Perceived Delay Measure (PDM) which 
focuses on the patientsʼ tolerance-to-delay with respect to the waiting time, 
which by illustrative examples shows its advantage in capturing the effect of 
tolerance-to-delay compared with some other measures. Using the PDM 
measure, we develop a design model with known service time distributions or 
samples that incorporates the patientsʼ tolerance-to-delay levels into the 
scheduling and sequencing optimization. Furthermore, we extend the model by 
considering the distributional ambiguity of the service time distribution. The 
PDM design model with known distributions can be formulated into a mixed 
integer linear program, while the PDM model with distributional ambiguity can 
be transformed into a mixed integer second-order conic program, and both can be 
handled by the off-the-shelf MIP solvers. The numerical experiments demonstrate 
the sound performance and some insights of using the proposed PDM models in 
the appointment scheduling and sequencing design.
3. Solving Distributionally Robust Multistage Optimization 
Problems via Fourier-Motzkin Elimination
Jianzhe Zhen (j.zhen@uvt.nl) Tilburg University, Netherlands, Melvyn 
Sim
This paper demonstrates how distributionally robust multistage optimization 
(DRMO) problems can be casted as (single period) distributionally robust 
optimization problems. A scheme based on a blending of classical Fourier-
Motzkin elimination and a simple Linear Programming technique, that can 
efficiently remove redundant constraints, is used to reformulate this general class 
of DRMO problems. This reformulation technique, contrasts with the classical 
approximation scheme via dynamic programming, enables us to solve DRMO 
problems to optimality. We show via numerical experiments that, given limited 
computational resources, for small-size multistage problems, our novel approach 
finds the optimal solutions, and for moderate or large-size instances, we 
successively improve the known approximated solutions.

■ Mon.A.5F
Monday, 10:45-12:00, Room 5F
Recent Advances on Convergence Rates of First-Order 
Methods: Part I
Cluster: Convex and Nonsmooth Optimization
Session organized by: Quoc Tran-Dinh, Ion Necoara
1. Linear Convergence of First-Order Methods for Non-strongly 
Convex Optimization
Ion Necoara (ion.necoara@acse.pub.ro) University Politehnica 
Bucharest, Romania, Yurii Nesterov, Francois Glineur
Usually, in order to show linear convergence of first-order methods for smooth 
convex optimization, we need to require strong convexity of objective function, 
an assumption which does not hold in many applications. In this paper we derive 
linear convergence rates of several first-order methods for solving smooth non-
strongly convex constrained optimization problems, i.e. objective function has 
Lipschitz continuous gradient and satisfies some relaxed strong convexity 
relation. In particular, for smooth constrained convex programming, we prove 
that some relaxations of strong convexity of objective function are sufficient for 
getting linear convergence for many first-order methods. The most general 
relaxation we introduce is a second order growth condition, which shows that 
objective function grows quicker than a quadratic function along the secant 
between any feasible point and its projection on optimal set. We also propose 
other non-strongly convex relaxations, which are more conservative than the 
second order growth condition, and establish relations between them. Moreover, 
we prove that the class of first-order methods achieving linear convergence 
under these strongly convex relaxations is broad. Finally, we show that the 
proposed relaxed non-degeneracy conditions cover important applications: e.g. 
linear systems, linear programming and dual formulations of convex programs.
2. A Universal Primal-Dual Convex Optimization Framework
Alp Yurtsever (alp.yurtsever@epfl.ch) École Polytechnique Fédérale 
de Lausanne (EPFL), Switzerland, Quoc Tran-Dinh, Volkan Cevher
We propose a new primal-dual algorithmic framework for a prototypical 
constrained convex optimization template. The algorithmic instances of our 
framework are universal since they can automatically adapt to the unknown 
Holder continuity degree and constant within the dual formulation. They are also 
guaranteed to have optimal convergence rates in the objective residual and the 
feasibility gap for each Holder smoothness degree. In contrast to existing primal-
dual algorithms, our framework avoids the proximity operator of the objective 
function. We instead leverage computationally cheaper, Fenchel-type operators, 
which are the main workhorses of the generalized conditional gradient (GCG)-
type methods. In contrast to the GCG-type methods, our framework does not 
require the objective function to be differentiable, and can also process additional 
general linear inclusion constraints, while guarantees the convergence rate on the 
primal problem.
3. Exact Worst-Case Performance of First-Order Methods for 
Composite Convex Minimization
Adrien B Taylor (adrien.taylor@uclouvain.be) Université catholique 
de Louvain, Belgium, Francois Glineur, Julien M Hendrickx
We introduce the performance estimation approach. This methodology aims at 
automatically analyzing the convergence properties of first-order algorithms for 
solving (composite) convex optimization problems. In particular, it allows 
obtaining tight guarantees for fixed-step first-order methods involving a variety 
of different oracles — namely explicit, projected, proximal, conditional and 
inexact (sub)gradient steps — and a variety of convergence measures. During 
the presentation, links with other methodologies for automatic algorithmic 
analysis will be emphasized, and we will illustrate how they can be used to 
further develop new algorithmic schemes, i.e., for obtaining better-performing 
first-order methods.

■ Mon.A.5H
Monday, 10:45-12:00, Room 5H
Financial Optimization and Robo Advisors 1
Cluster: Applications in Finance and Economics
Session organized by: Changle Lin
2016_iccopt.indb   48 2016/07/22   11:58:15
ABSTRACTS
ICCOPT 2016 49
1. Robo-Advisor in Chinaʼs Market
Frank Wang (frankwang@creditease.cn) CreditEase, China
The talk will present CreditEaseʼs thoughts about how to develop a robo-advisor 
in China.
2. Personalized Asset & Liability System: Goal-based Investing 
Changle Lin (changlelin1@gmail.com) Princeton University, USA, 
Woo Chang Kim
Modern wealth management industry has been transforming itself from a 
product-driven and sales-driven industry to a more client-centric industry. Goal-
based investing is gaining popularity and becoming a mainstream approach 
adopted by large wealth management institutions to better serve their clients. 
Existing literature mostly model and optimize each goal in a clientʼs holistic 
financial planning separately. This approach, though capturing the behavior 
tendency of private clientʼs “mental accounting” behavior, will inevitably yield 
suboptimal results if taking all goals into account altogether. In this paper, we 
utilize the techniques and methodology used in asset liability management for 
institutions and adapt this financial technology, which has been only available 
for institutional investors, for personal investors.

■ Mon.A.5I
Monday, 10:45-12:00, Room 5I
Interior-Point Methods and Applications for Conic 
Optimization
Cluster: Conic and Polynomial Optimization
Session organized by: Yu Xia
Session chair: Makoto Yamashita
1. An Efficient Second-Order Cone Programming Approach for 
Optimal Selection in Tree Breeding
Sena Safarina (safarina.s.aa@m.titech.ac.jp) Tokyo Institute of 
Technology, Japan, Tim J Mullin, Makoto Yamashita
An important issue in tree breeding is an optimal selection to determine the 
contribution of candidate pedigree members and to produce the highest 
performance of seed orchard while keeping genetic diversity. To address this 
problem from the viewpoint of mathematical optimization, Pong-Wong and 
Wolliams proposed semidefinite programming (SDP) formulation. Ahlinder et 
al. implemented the SDP formulation using SDPA and compared it with 
GENCONT. They reported that SDP approach attained a better optimal value 
than GENCONT, but its formulation consumed much longer computation than 
GENCONT. Our research is focused on second-order cone programming 
(SOCP) formulations for the optimal selection problem to reduce the heavy 
computation time of SDP. Though SOCP is a special case of SDP, simple SOCP 
formulation is not more efficient compared to SDP formulation. Therefore, we 
aggressively exploit a structural sparsity of the Wright numerator matrix and 
employ the Henderson algorithm to accelerate the computation. Numerical 
results show that the proposed SOCP approach reduced computation time from 
39,200 seconds under the SDP approach to less than 2 seconds.
2. A Numerically Stable Primal-Dual Interior-Point Method for SDP
Kei Takemura (takemura.k.ac@m.titech.ac.jp) Tokyo Institute of 
Technology, Japan, Makoto Yamashita
We present a new numerical stable interior-point method for semidefinite 
programming (SDP). Some SDP problems are very hard for interior-point 
methods due to numerical errors. One of the reasons for the numerical errors 
comes from the computation of inverse matrix. Since the matrix variables 
become ill-conditioned as they approach the optimal solution, the inverse matrix 
computation gets less reliable. We utlize an eigenvalue decomposition form of 
symmetric matricies as variable matrices instead of treating the matrix variables 
directly. We propose a path-following interior-point method by classifying the 
introduced variables into two groups. One group has eigenvalues and central 
path parameter. The other group has other variables including eigenvectors. A 
similar study called Q method (Alizadeh, Haeberly and Overton (1994)) did not 
achieve global convergence. In contrast, we give numerical results to show that 
the proposed method achieves numerical stability and global convergence.

■ Mon.A.5L
Monday, 10:45-12:00, Room 5L
Moments, Positive Polynomials & Optimization: Part I
Cluster: Conic and Polynomial Optimization
Session organized by: Jiawang Nie, Jean B Lasserre
1. Improved Convergence Rates for Lasserre-Type Hierarchies of 
Upper Bounds for Box-constrained Polynomial Optimization
Etienne de Klerk (e.deklerk@uvt.nl) Tilburg University, Netherlands 
Roxana Hess, Monique Laurent
We consider the problem of minimizing a given mulltivariate polynomial over 
the hypercube. An idea introduced by Lasserre, is to find a probability distribution 
on the hypercube with polynomial density function of fixed degree, say r, so that 
the expected value of the polynomial is minimized. The expected value then 
gives an upper bound on the minimum value, depending on the value of r. We 
will show a convergence rate for these upper bounds of O(1/r2). The convergence 
rate analysis relies on the theory of polynomial kernels, and in particular on 
Jackson kernels. We also show that the resulting upper bounds may be computed 
as generalized eigenvalue problems.
2. Real Eigenvalues of Nonsymmetric Tensors
Xinzhen Zhang (xzzhang@tju.edu.cn) Tianjin University, China, 
Jiawang Nie
This talk discusses the computation of real Z-eigenvalues of nonsymmetric 
tensors. A general nonsymmetric tensor has finitely many Z-eigenvalues, while 
there may be infinitely many ones for special tensors. We propose Lasserre type 
semidefinite relaxation methods for computing such eigenvalues. For every 
nonsymmetric tensor that has finitely many real Z-eigenvalues, we can compute 
all of them; each of them can be computed by solving a finite sequence of 
semidefinite relaxations. Various examples are presented.
3. A Multilevel Method for Semidefinite Programming Relaxations 
of Polynomial Optimization Problems with Structured Sparsity
Panos Parpas (p.parpas@imperial.ac.uk) Imperial College London, 
United Kingdom
We propose a multilevel paradigm for the global optimisation of polynomials 
with sparse support. Such polynomials arise through the discretisation of PDEs, 
optimal control problems and in global optimization applications in general. We 
construct projection operators to relate the primal and dual variables of the SDP 
relaxation between lower and higher levels in the hierarchy, and theoretical 
results are proven to confirm their usefulness. Numerical results are presented 
for polynomial problems that show how these operators can be used in a 
hierarchical fashion to solve large scale problems with high accuracy.

■ Mon.A.m3S
Monday, 10:45-12:00, Room m3S
Sparse Optimization and Applications
Cluster: Sparse Optimization and Information Processing
Session organized by: Coralia Cartis
Session chair: Raphael A Hauser
1. Submodular Functions: From Discrete to Continous Domains
Francis Bach (francis.bach@ens.fr) INRIA - Ecole Normale 
Supérieure, France
Submodular set-functions have many applications in combinatorial optimization, 
as they can be minimized and approximately maximized in polynomial time. A 
key element in many of the algorithms and analyses is the possibility of extending 
the submodular set-function to a convex function, which opens up tools from 
convex optimization. Submodularity goes beyond set-functions and has naturally 
been considered for problems with multiple labels or for functions defined on 
continuous domains, where it corresponds essentially to cross second-derivatives 
being nonpositive. In this talk, we show that most results relating submodularity 
and convexity for set-functions can be extended to all submodular functions. In 
particular, we naturally define a continuous extension in a set of probability 
measures, show that the extension is convex if and only if the original function is 
submodular, and prove that the problem of minimizing a submodular function is 
equivalent to a typically non-smooth convex optimization problem. Most of 
these extensions from the set-function situation are obtained by drawing links 
with the theory of multi-marginal optimal transport, which provides also a new 
interpretation of existing results for set-functions. Applications to sparse 
proximal operators will be presented. (available at https://hal.archives-ouvertes.
fr/hal-01222319v2/document)
2. Learning Directed Acyclic Graphs Based on Sparsest Permutations
Caroline Uhler (cuhler@mit.edu) Massachusetts Institute of 
Technology, USA
We consider the problem of learning a Bayesian network or directed acyclic 
graph (DAG) model from observational data. We propose the sparsest 
2016_iccopt.indb   49 2016/07/22   11:58:15
ABSTRACTS
ICCOPT 201650
permutation algorithm, a nonparametric approach based on finding the ordering 
of the variables that yields the sparsest DAG. We prove consistency of this 
method under strictly weaker conditions than usually required. We discuss how 
to find the sparsest ordering by introducing the DAG associahedron and a 
simplex-type algorithm on this convex polytope.
3. A Link between DC Algorithms and Proximal Gradient Methods
Katsuya Tono (katsuya_tono@mist.i.u-tokyo.ac.jp) The University of 
Tokyo, Japan, Akiko Takeda, Jun-ya Gotoh
Several nonconvex sparsity-inducing regularizers have been recently proposed 
in the context of sparse learning. Accordingly, nonconvex and nonsmooth 
optimization problems have appeared in many application areas. In this talk, we 
provide an efficient algorithm to deal with such optimization problems where the 
objective function is the sum of a smooth nonconvex function and a nonsmooth 
DC (Difference of Convex) function. While the DC algorithm has been widely 
used for this type of problems, it often requires a large computation time to solve 
a sequence of convex subproblems. Our algorithm, which we call the proximal 
DC algorithm (PDCA), overcomes this issue of the ordinary DC algorithm by 
employing a special DC decomposition of the objective function. In PDCA, 
closed-form solutions can be obtained for the convex subproblems, leading to 
efficient performance in numerical experiments. We also discuss the theoretical 
aspects: PDCA can be viewed as a nonconvex variant of the proximal gradient 
methods (PGM), which provides an insight on the relation between PGM and 
DC algorithms.

■ Mon.B.1S
Monday, 13:30-14:45, Room 1S
Nonlinear Optimization and Its Applications II
Cluster: Nonlinear Optimization
Session organized by: Frank E Curtis
1. An Evolving Subspace Method for Low Rank Minimization
Daniel P Robinson (daniel.p.robinson@gmail.com) Johns Hopkins 
University, USA
I present a method for solving low rank minimization problems that combines 
subspace minimization techniques, inexact subspace conditions to terminate 
exploration of the subspace, and inexact singular value decompositions. Taking 
together, these features allow the algorithm to scale well, and in fact be 
competitive with nonconvex approaches that are often used. Convergence results 
are discussed and preliminary numerical experiments are provided.
2. Convergence Rate of a Trust Region Method for Stochastic 
Nonconvex Optimization
Katya Scheinberg (katyascheinberg@gmail.com) Lehigh University, 
USA, Jose Blanchet, Coralia Cartis, Matt Menickelly
We will discuss a classical variance reducing trust region method applied to the 
stochastic nonconvex smooth problems. We will present a convergence rate 
analysis which shows that canonical convergence rates can be achieved. The 
analysis is based on properties of a stopping time of supermartingales. We will 
discuss the conditions on the rate of variance reduction and its effect on the final 
result. We will also present some applications from machine learning. 
3. A Dynamic Penalty Parameter Updating Strategy for Matrix-free 
Sequential Quadratic Optimization Methods
Hao Wang (wanghao1@shanghaitech.edu.cn) Shanghai Tech 
University, China, James V Burke, Frank E Curtis, Jiashan Wang
This talk focuses on the issue of updating the penalty parameter within a penalty 
Sequential Quadratic Optimization (SQO) algorithm for solving nonlinear 
optimization problems. In contemporary penalty SQO methods, the common 
strategy is to update the penalty parameter after a subproblem (or a sequence of 
them) has been solved. This may lead to inefficiency if the parameter is slow to 
adapt to the problem scaling or structure. By contrast, we propose an approach to 
update a penalty parameter during the optimization process for each subproblem, 
where the goal is to produce a search direction that simultaneously predicts 
progress towards feasibility and optimality. We prove that our approach yields 
reasonable (i.e., not excessively small) values of the penalty parameter and 
illustrate the behavior of our approach via numerical experiments.

■ Mon.B.1A
Monday, 13:30-14:45, Room 1A
Nonlinear Optimization Solvers
Cluster: Nonlinear Optimization
Session organized by: Yinyu Ye, Oliver Hinder
1. A One Phase Interior Point Method for Non-convex Optimization
Oliver H Hinder (ohinder@gmail.com) Stanford University, USA, 
Yinyu Ye
Interior point methods in convex optimization, such as the homogeneous 
algorithm or infeasible start algorithm, can efficiently find an optimal solution or 
prove infeasibility in one phase. However, in non-convex optimization these 
algorithms will not converge to a local minimizer of infeasibility. To date, there 
are two main approaches to solving this issue: a feasibility restoration phase (i.e. 
IPOPT) or a penalty method. We introduce a new one phase interior point 
method that is guaranteed to converge to either a local optimum or a local 
minimizer of the L1 norm of the constraint violation. The basic idea of the 
algorithm is to add dummy variables and one constraint to precisely control the 
rate of L1 constraint violation. Depending on the state of progress, the algorithm 
takes either an aggressive step, which simultaneously searches for optimality and 
feasibility, or a stabilization step, which searches for optimality (without 
reducing the constraint violation). We present early empirical testing on CUTEst 
and compare our performance with IPOPT and KNITRO.
2. Inexact Sequential Quadratically Constrained Quadratic 
Programming Method of Feasible Directions with Global and 
Superlinear Convergence Properties
Yu Watanabe (1415702@ed.tus.ac.jp) Tokyo University of Science, 
Japan, Satoshi Nakatani, Hiroshi Yabe
We consider the sequential quadratically constrained quadratic programming 
(SQCQP) method for nonlinear inequality constrained optimization problems. 
The sequential quadratic programing (SQP) method is one of effective numerical 
methods for solving these problems. However it is known that the Maratos effect 
may occur in the SQP method. This phenomenon is that a unit step size can not 
be always accepted near an optimal solution. In order to overcome this defect, the 
SQCQP method was focused on. The SQCQP method solves the quadratically 
constrained quadratic programming (QCQP) subproblem at each iteration. The 
QCQP subproblem can not always have optimal solutions. Jian gave the SQCQP 
method of feasible directions in which subproblem was always solvable. Based 
on the idea of Jian, we propose an inexact SQCQP method that solves the QCQP 
subproblem inexactly at each iteration. Since QCQP subproblem is usually 
solved by iterative methods, they are not solved exactly. Therefore, it is significant 
to consider the inexact SQCQP method. In this study, we prove its global and 
superlinear convergence properties.
3. Shape-changing L-SR1 Trust-Region Methods
Roummel Marcia (rmarcia@ucmerced.edu) University of California, 
Merced, USA, Johannes Brust, Oleg Burdakov, Jennifer Erway, Ya-
xing Yuan
In this talk, we discuss methods for solving the trust-region subproblem when a 
limited-memory symmetric rank-one matrix is used in place of the true Hessian 
matrix. In particular, we propose methods that exploits two shape-changing 
norms to decompose the trust-region subproblem into two separate problems. 
The proposed solver makes use of the structure of limited-memory symmetric 
rank-one matrices to find solutions that satisfy these optimality conditions.

■ Mon.B.1B
Monday, 13:30-14:45, Room 1B
Advances in PDE-constrained Optimization I
Cluster: PDE-constrained Optimization
Session organized by: Kazufumi Ito, Michael Ulbrich
1. Optimization for Design and Control of Composite Thermal Fluid 
Systems
John Allen Burns (jaburns@vt.edu) Virginia Tech, USA
In this paper we consider optimization problems for design and control of 
composite infinite dimensional systems. These systems arise naturally when the 
problem is multidisciplinary (e.g., aero-elasticity, thermal-fluids) but also occur 
when actuator dynamics are included in the control design problem. We present 
examples to highlight some technical issues that occur when dealing with 
interconnected systems and then focus on a special class of composite systems 
that occur when actuator dynamics are included as part of the model. Numerical 
examples are presented to illustrate the theoretical results. Finally, we suggest 
some new application areas that offer enormous opportunities for researchers 
interested in distributed parameter control.
2016_iccopt.indb   50 2016/07/22   11:58:15
ABSTRACTS
ICCOPT 2016 51
2. Sensitivity Analysis for Elliptic Variational Inequalities of the 
Second Kind: A Model Problem and Applications in Optimal 
Control
Constantin Christof (constantin.christof@tu-dortmund.de) TU 
Dortmund, Germany, Christian Meyer
This talk is concerned with the sensitivity analysis that is needed for the study of 
optimal control problems governed by elliptic variational inequalities of the 
second kind. We demonstrate by means of a prototypical example how to prove 
directional differentiability for the solution map of a variational inequality of the 
second kind, how to compute directional derivatives of the solution operator and 
how to identify points where the Gâteaux derivative exists. Our findings are 
compared with classical results of F. Mignot and A. Shapiro. The talk concludes 
with a discussion of the consequences that our analysis has for the derivation of 
stationarity conditions and the numerical treatment of optimal control problems 
governed by variational inequalities of the second kind.
3. Optimal Control of Hyperbolic Balance Laws with State 
Constraints 
Johann Michael Schmitt (jschmitt@mathematik.tu-darmstadt.de) TU 
Darmstadt, Germany, Stefan Ulbrich
This talk deals with the treatment of pointwise state constraints in the context of 
optimal control of hyperbolic nonlinear scalar balance laws. We study an optimal 
control problem governed by balance laws with an off/on switching device that 
allows to interrupt the flux, where the switching times between on- and off-
modes are the control variables. Such a problem arises, for example, when 
considering the optimal control of the traffic density on a one-way street with a 
traffic light in the middle. The appearance of state constraints presents a special 
challenge, since solutions of nonlinear hyperbolic balance laws develop 
discontinuities after finite time. In this talk, we will therefore mainly focus on 
their treatment and use the recently developed sensitivity- and adjoint calculus 
by Pfaff and Ulbrich to derive necessary optimality conditions. In addition, we 
will use Moreau-Yosida regularization for the algorithmic treatment of the 
pointwise state constraints. Hereby, we will prove convergence of the optimal 
controls and weak convergence of the corresponding Lagrange multiplier 
estimates of the regularized problems.

■ Mon.B.1C
Monday, 13:30-14:45, Room 1C
Computational and Algorithmic Aspects of Derivative-
free Optimization
Cluster: Derivative-free and Simulation-based Optimization
Session organized by: Francesco Rinaldi, Zaikun Zhang
Session chair: Sebastien Le Digabel
1. Improved Sampling for Two-Stage Methods
Simon Wessing (simon.wessing@tu-dortmund.de) Technische 
Universität Dortmund, Germany
In this work, we apply two-stage optimization algorithms, which consist of 
alternating global and local searches, to essentially unconstrained global 
optimization problems. These algorithms are attractive because of their simplicity 
and their demonstrated performance on multimodal problems. The main focus is 
on improving the global stages, as local search is already a thoroughly 
investigated topic. This is done by considering previously sampled points and 
found optima in the global sampling, thus obtaining a super-uniform distribution. 
The approach is based on maximizing the minimal distance in a point set, while 
boundary effects of the box-constrained search space are avoided by correction 
methods. Experiments confirm the superiority of this algorithm over random 
uniform sampling and other methods on benchmark problems.
2. Benchmarking Bi-Objective Derivative-free Optimizers with 
COCO
Dimo Brockhoff (dimo.brockhoff@inria.fr) INRIA, France
Numerical optimization problems are at the core of many industrial design and 
development tasks. Often, multiple (conflicting) objective functions (such as 
cost and energy consumption) have to be optimized and the functions are non-
differentiable, non-convex, multimodal, noisy, or not mathematically tractable. 
Multi-objective black-box optimizers, also called derivative-free vector 
optimization algorithms, are the methods of choice in such cases because they 
interpret problems as black-boxes and only use the function value vectors of 
some query points. Algorithm benchmarking is the mandatory but also a tedious 
(because repetitive) path when designing and recommending efficient and robust 
algorithms. The Comparing Continuous Optimizers platform (COCO) aims at 
automatizing this benchmarking in the case of numerical black-box problems—
freeing the time of algorithm designers and practitioners alike. In 2016, the first 
bi-objective test suite became available within COCO which now allows to 
automatically benchmark optimizers for problems with two objectives for the 
first time. In this talk, we will see, in a tutorial-like style, how to use the COCO 
software and how to interpret its main plots when comparing algorithm 
performance. We will also present benchmarking results that have been collected 
for the first Bi-objective Black-box Optimization Benchmarking workshop 
(BBOB-2016)
3. The Mesh Adaptive Direct Search Algorithm for Discrete Blackbox 
Optimization
Sébastien Le Digabel (sebastien.le.digabel@gerad.ca) École 
Polytechnique de Montréal, Canada, Charles Audet, Christophe Tribes
The Mesh Adaptive Direct Search (MADS) algorithm is designed for blackbox 
optimization problems where the functions defining the objective and the 
constraints are typically the outputs of a simulation. It is a derivative-free method 
designed for continuous variables and which is supported by a convergence 
analysis based on the Clarke calculus. This talk introduces a modification of the 
MADS algorithm for the handling of variables with a controlled number of 
decimals. A corollary of this approach is the ability to treat discrete variables. 
Computational results are presented using the NOMAD software, the free C++ 
distribution of the MADS algorithm.

■ Mon.B.4A
Monday, 13:30-14:45, Room 4A
Energy Systems and Markets
Cluster: Applications in Energy, Science and Engineering
Session organized by: Asgeir Tomasgard
Session chair: Chiara Bordin
1. Nonsmooth Equations Approach to the Real-Time Pricing for 
Smart Grid
Yan Gao (gaoyan@usst.edu.cn) University of Shanghai for Science 
and Technology, China, Hongjie Wang
Smart grid is an electricity delivery system enhanced with communication 
facilities and information technologies. According to the real-time price, the 
users can improve the insulation conditions and try to shift the energy 
consumption schedule of their high-load household appliances to off-peak hours 
to achieving optimal allocation of resources. In this paper, we mainly research 
the real-time pricing under the social utility maximization in smart grid. We 
adopt the utility function to reflect consumerʼs preferences and spending power, 
and set up the social utility model. We discuss the properties of the social utility 
model and adopt the shadow price as real-time price. In existing researches, dual 
method are used to solving this problem. But this method usually need to solve a 
series of unconstrained minimization problem, so the amount of computation is 
huge. According to KKT conditions, we set up a nonsmooth equations based on 
social utility model firstly. Then, we propose a new smooth approximating 
function based on the complementary theory which is more suitable to real-time 
pricing problem. The nonsmooth equations are shifted to smooth ones. The 
smooth equations is solved by quasi-Newton method. It is showed that the 
present is effective by the simulation.
2. An Energy Storage Deployment Program under Random 
Discharge Permissions
Somayeh Moazeni (smoazeni@stevens.edu) Stevens Institute of 
Technology, USA, Boris Defourny
Recent developments in energy storage technology and the greater use of 
renewables have increased interest in energy storage. This interest has created 
the need to design and study efficient energy storage deployment programs, with 
the goal of providing attractive flexibility to storage owners while still indirectly 
supervising their operations. We envision a framework that defines random times 
at which a particular participating storage unit is allowed to discharge. In this 
flexible energy storage deployment program, a participating storage unit receives 
permissions at random to discharge during peak hours in real time. Should 
discharge permission be issued, the storage owner has the option to discharge and 
be paid at a time-dependent reward that is specified contractually, or can wait for 
future permissions. A Poisson process models the discharge permission times. 
We study the problem of optimizing discharge operations from the perspective of 
the storage resource owner. 
3. Smart Charging of Electric Vehicles through Indirect Control and 
Smart Price Signals
Chiara Bordin (chiara.bordin@iot.ntnu.no) Norwegian University of 
Science and Technology, Norway, Stig Ødegaard Ottesen, Asgeir 
2016_iccopt.indb   51 2016/07/22   11:58:15
ABSTRACTS
ICCOPT 201652
Tomasgard, Siri Bruskeland Ager-Hanssen, Siri Olimb Myhre
The increasing demand for Electric Vehicles (EV) charging puts pressure on the 
power grids and highlights the necessity to level out the load curve in order to 
avoid that the power consumption exceeds the grid capacity. We propose a linear 
programming model for the indirect control of EV charging that finds an optimal 
set of price signals to be sent to the drivers according to their flexibility. The 
objective is to satisfy the demand when there is a capacity lack by minimizing 
the curtailment of loads and prioritizing the loads shifting. The key contribution 
is the use of elasticity matrices through which it is possible to forecast the drivers 
reactions to the price signals. As real world data on relating the elasticity values 
to the EV drivers behaviour are currently inexistent, we concentrate on 
investigating the sensitivity of elasticity patterns to the optimal price structure. In 
particular, we study how different combinations of drivers with different 
elasticity may affect the ability of the operator to both handle a capacity problem 
and properly satisfy the charging needs. When a capacity problem arises we 
investigate in which conditions load shifting is not enough and curtailment 
becomes a necessity.

■ Mon.B.4B
Monday, 13:30-14:45, Room 4B
Optimization in Healthcare
Cluster: Applications in Energy, Science and Engineering
Session organized by: Nedialko Dimitrov
1. Cyber Defense Based on Network Structure
Murat Karatas (mkaratas@utexas.edu) The University of Texas at 
Austin, USA, Nedialko Dimitrov
Infrastructures, such as university nuclear reactors, are controlled through cyber-
physical systems. Recent attacks on government offices and foreign consulates 
showed that assessing the vulnerability of these systems is key in directing 
defensive investment. We present an MDP to compute an optimal attack policy. 
The MDP has an exponential number of states, and is based on tracking the set of 
available attacks for each link in the network. We show that it is possible to 
compute values for each MDP state, and optimal attack policies using s-t 
reliability. We also show that finding an optimal defensive investment strategy is 
a network interdiction model. Such a policy can be calculated using a direct and 
a simulation approach. The example we use includes a company network with 20 
nodes and 25 edges.
2. Personalized Measurement Time Points by Optimum Experimental 
Design for Mathematical Leukopenia Models
Felix Jost (felix.jost@ovgu.de) Otto-von-Guericke University 
Magdeburg, Germany, Kristine Rinke, Thomas Fischer, Enrico Schalk, 
Sebastian Sager
In this talk we present and discuss the optimization of measurement time points 
by optimal experimental design for patient specific parameter estimation. The 
standard deviations of the parameter estimates depend strongly on the timing of 
the measurements. The underlying mathematical model describes the dynamics 
of leukocytes of patients having acute myeloid leukemia. The model is used to 
investigate Leukopenia which is a clinically important side effect arising from 
the treatment with chemotherapy. The dynamics of leukocytes are modelled by 
ordinary differential equations and the chemotherapy with cytarabin is described 
by a pharmacokinetics/pharmacodynamics model consisting of two 
compartments and a log-linear function representing the drug effect. The disease 
progression differs for each patient, so that the mathematical model is fitted 
individually to experimental data which results in patient-specific sets of 
parameters. The different dynamics of the leukocytes also result in personalized 
measurement time points which provide maximal information about the model 
parameters. Consequently, the individually optimized designs are superior to the 
usual designs chosen by the physicianʼs experience. We discuss integer and 
relaxed solutions of the mathematical optimization problems providing optimal 
experimental designs and analyze their relation.
3. Texas Arbovirus Risk Maps and Uncertainty Analysis
Xi Chen (carol.chen@utexas.edu) University of Texas at Austin, USA, 
Nedialko Dimitrov
Arbovirus diseases are a major public health concern in Texas. Two viruses that 
have not yet established local transmission but may pose a threat are dengue 
virus and chikungunya. Both of these viruses are consistently imported into 
Texas and have seen an increase in the number of cases over the past few years. 
We introduce a county-level risk assessment framework for identifying areas that 
may be at high risk for importation of these arboviruses. Human importation risk 
is estimated using a maximum entropy algorithm, based on historical dengue 
case data, socioeconomic, demographic, and bio-climatic data. A significant 
reason for the popularity of the maximum entropy methodology is its applicability 
to presence-only data. To address the uncertainty quantification in the point 
estimation of maximum entropy model, we analytically deriving an expression 
of the variance of the target species distribution probabilities and comparing the 
results with bootstrap methods.

■ Mon.B.5D
Monday, 13:30-14:45, Room 5D
Optimization over Symmetric Cones and Related 
Topics
Cluster: Linear Optimization
Session organized by: Leonid Faybusovich
1. Primal-Dual Algorithms for Infinite-dimensional Second-Order 
Cone Programming Problems and LQ-Problem with Time 
Dependent Linear Term in the Cost Function
Thanasak Mouktonglang (mouktonglang.thanasak@gmail.com) 
Chiang Mai University, Thailand, Leonid Faybusovich
In this paper, we consider a tracking problem in a general Hilbert space. It is 
shown that the problem can be rewritten as an infinite dimension second order 
cone programming problem which can be solved by a primal-dual algorithm of 
infinite-dimensional second-order cone problem. We then consider Muti-Criteria 
Linear-Quadratic Control problem (MCLQP). The problem consists of a 
minimization of several maximum quadratic functions. For a decent direction, it 
turns out that solving a number of Linear-Quadratic problems (LQ-problem) 
with time dependent linear term in the cost function is required. We prove the 
existence and uniqueness of the solution of the LQ-problem with time dependent 
linear term the cost function and describe the solution explicitly. We consider 
applications to deterministic version of Kalman filtering and multi-target LQ 
control problem on semi-infinite interval.
2. Incremental Gradient Method for Karcher Mean on Symmetric 
Cones
Sangho Kum (shkum@cbnu.ac.kr) Chungbuk National University, 
Korea, Sangwoon Yun
In this talk, we deal with the minimization problem for computing Karcher mean 
on a symmetric cone. The objective of this minimization problem consists of the 
sum of the Riemannian metric distances with many given points in a symmetric 
cone. Moreover, the problem can be reduced to a bound constrained minimization 
problem. These motivate us to adapt an incremental gradient method. So we 
propose an incremental gradient method and establish its global convergence 
properties exploiting the Lipschitz continuity of the gradient of the Riemannian 
metric distance function.
3. FRA-Poly: Partial Polyhedrality and Facial Reduction
Bruno Figueira Lourenço (lourenco@st.seikei.ac.jp) Seikei University, 
Japan, Masakazu Muramatsu, Takashi Tsuchiya
Facial reduction is a procedure that aims at restoring Slaterʼs condition in conic 
linear programs. Each step consists of obtaining a supporting hyperplane that 
contains the feasible region of the problem, in hopes of confining the problem to 
a smaller face of the underlying cone. Then, it is of paramount importance to 
bound the number of steps necessary to finish the procedure, since each step is 
very costly. In this talk, we discuss FRA-poly, a facial reduction algorithm that 
explores polyhedrality in the lattice of faces of the cone. Under mild assumptions, 
the worst case number of steps is smaller than the amount predicted by the 
classical facial reduction analysis. In particular, a significant improvement is 
obtained for the case of the doubly nonnegative cone, where the complexity 
decreases from quadratic to linear. We will also examine the performance of 
FRA-Poly in the symmetric cone case.

■ Mon.B.5E
Monday, 13:30-14:45, Room 5E
Robust Optimization in Data and Signal Processing
Cluster: Robust Optimization
Session organized by: Anthony Man-Cho So
1. On Reduced Semidefinite Programs for Second Order Moment 
Bounds with Applications
Karthik Natarajan (karthik_natarajan@sutd.edu.sg) Singapore 
University of Technology and Design, Singapore, Chung Piaw Teo
We show that the complexity of computing the second order moment bound on 
2016_iccopt.indb   52 2016/07/22   11:58:15
ABSTRACTS
ICCOPT 2016 53
the expected optimal value of a mixed integer linear program with a random 
objective coecient vector is closely related to the complexity of characterizing 
the convex hull of the points (1 x)(1 x)ʼ for x in X where X is the feasible region. 
As an application of the result, we identify a new polynomial time solvable 
semidefinite relaxation of the distributionally robust multi-item newsvendor 
problem by exploiting results from the Boolean quadric polytope. We illustrate 
the usefulness of the reduced semide nite programming bounds in estimating the 
expected range of random variables with two applications arising in random 
walks and best-worst choice models.
2. Semidefinite Relaxation of a Class of Robust QCQPs: A Verifiable 
Sufficient Condition for Rank-One Solutions
Wing Kin Ma (wkma@ee.cuhk.edu.hk) The Chinese University of 
Hong Kong, Hong Kong, Anthony Man-Cho So
This talk will concentrate on a specific robust quadratically-constrained quadratic 
program (QCQP) that arises in the context of signal processing and 
communications, namely, quality-of-service constrained optimization for 
multiuser downlink transmit beamforming. The problem takes a separable 
QCQP form where we have worst-case robust quadratic constraints under 
spherically bounded uncertainties. In previous work, this problem was tackled by 
semidefinite relaxation (SDR). Curiously, simulation results have suggested that 
the SDR problem admits rank-one solutions—or exactly solves the robust 
QCQP—in most instances. This gives rise to an important question, namely, 
whether we can identify sufficient conditions under which SDR admits a rank-
one solution. This talk will present one such condition, proven by the speaker and 
his collaborators recently. Simply speaking, the condition indicates that if the 
uncertainty sphere radii are not too large and the nominal multiuser channel is 
not too poorly conditioned, then any solution to the SDR problem must have rank 
one. The aforementioned condition appears as a simple verifiable expression 
with respect to the problem instance. As will be further described in the talk, the 
key insight lies in establishing a duality framework through which an equivalence 
relationship between SDR and a related maximin semidefinite program is 
revealed.
3. Epsilon-Net Techniques for a Class of Uncertain Quadratic 
Programming and Its Applications in Robust Beamforming with 
Cognitive Radio Constraints
Man-Chung Yue (mcyue214@gmail.com) The Chinese University of 
Hong Kong, Hong Kong, Xiaoxiao Wu, Wing-Kin Ma, Anthony Man-
Cho So
We consider quadratic optimization subject to multiple uncertain quadratic 
constraints, where the positive semidefinite matrices defining the constraints are 
of rank-one and each of them is the outer product of some unknown vector lying 
a ball. Problems of this form often arise in field of wireless communication. A 
standard approach to this class of quadratic programming is the semidefinite 
relaxation followed a Gaussian randomization. In this work, we study the relative 
accuracy of this approximation scheme. In particular, we establish probabilistic 
bounds on the relative accuracy using the epsilon-net arguments, a technique 
from geometric functional analysis. We also discuss an application of the result 
to the problem of robust beamforming with cognitive radio constraint.

■ Mon.B.5F
Monday, 13:30-14:45, Room 5F
Recent Advances on Convergence Rates of First-Order 
Methods: Part II
Cluster: Convex and Nonsmooth Optimization
Session organized by: Quoc Tran-Dinh, Ion Necoara
1. Limited-Memory Trust-Region Methods for Sparse Reconstruction
Lasith Adhikari (ladhikari@ucmerced.edu) University of California, 
Merced, USA, Jennifer Erway, Shelby Lockhart, Roummel Marcia
In this talk, we solve the L2-L1 sparse reconstruction problem using trust-region 
methods. Specifically, we transform the minimization problem into an 
unconstrained differentiable problem and use a limited-memory BFGS 
approximation to the Hessian to define a sequence of trust-region subproblems. 
We perform numerical experiments to show that the proposed approach 
accurately recovers the true signal and eliminates spurious solutions more 
effectively and efficiently than competing gradient projection-type methods.
2. Non-asymptotic Convergence Rate for Distributed Learning in 
Graphs
Cesar A Uribe (cauribe2@illinois.edu) University of Illinois at 
Urbana-Champaign, USA, Alexander Olshevsky, Cesar M Uribe
We will consider the problem of distributed cooperative learning in a network of 
agents, where the agents are repeatedly gaining partial information about an 
unknown random variable whose distribution is to be jointly estimated. The 
learning is based on Bayesian update adapted to distributed information structure 
inherited from the network. Our results establish consistency and a non-
asymptotic, explicit, geometric convergence rate for the learning dynamics.
3. Adaptive Smoothing Fast Gradient Methods for Fully Nonsmooth 
Composite Convex Optimization
Quoc Tran-Dinh (quoctd@email.unc.edu) University of North 
Carolina at Chapel Hill, USA
We propose an adaptive smoothing fast-gradient algorithm based on Nesterovʼs 
smoothing technique for solving fully nonsmooth composite convex optimization 
problems. Our method combines both Nesterovʼs accelerated proximal gradient 
scheme and a new homotopy strategy for smoothness parameter. By an 
appropriate choice of smoothing functions, we develop new algorithms that have 
up to the O(1/k)-convergence rate while allow one to automatically update the 
smoothness parameter at each iteration. Then, we further exploit the structure of 
problems to select smoothing functions and develop suitable algorithmic variants 
that reduce the complexity-per-iteration. We also specify our algorithm to solve 
constrained convex problems and show its convergence guarantee on a primal 
sequence of iterates. We demonstrate our algorithm through three numerical 
examples and compare it with other algorithms.

■ Mon.B.5H
Monday, 13:30-14:45, Room 5H
Optimization in Finance
Cluster: Nonlinear Optimization
Session organized by: Martin Takac
1. Continuous Selections of Optimal Portfolios
Michel Baes (michel.baes@math.uzh.ch) University of Zurich, 
Switzerland, Pablo Koch, Cosimo Munari
Financial institutions are required by regulators to hold an adequate capital 
buffer (e.g. cash money) to protect policyholders from default risk. Often the 
theory of risk measures implicitly assumes that this capital is invested in a single 
eligible asset. However, this is too restrictive and will typically not be efficient, 
particularly if an institution has assets and liabilities in several currencies. In the 
context of multiple eligible assets, we are confronted with several questions that 
are critical from an operational perspective. Three of them are: 1. What is the set 
of optimal portfolios? 2. Is there a unique optimal portfolio? 3. If several optimal 
portfolios exist, how robust is the choice of a specific portfolio? I particular, can 
we select one that is continuous with respect to the capital position of the 
institution? We present complete answers for these questions for convex risk 
measures, polyhedral risk measures, and for Value-at-Risk. Since these three 
classes of problems are structurally very different, we had to develop for each of 
them a proper set of mathematical tools and techniques.
2. A Hybrid Approach for Tracking the 1/N Portfolio
Norbert Trautmann (norbert.trautmann@pqm.unibe.ch) University of 
Bern, Switzerland, Oliver Strub
The 1/N portfolio represents a naïve diversification strategy which is applied in 
practice and has been widely discussed in the literature. An investor who follows 
this strategy should invest in all N assets from a given investment universe, 
which, in general, incorporates substantial portfolio-management costs due to the 
large number of assets. We consider the problem of constructing a tracking 
portfolio, i.e., a portfolio consisting of far less than N assets, to track the returns 
of the 1/N portfolio. This problem can be formulated as a mixed-integer quadratic 
program, which, however, gets computationally expensive when N grows large. 
Therefore, we propose a hybrid approach that combines an iterated greedy 
heuristic for selecting the assets to be included in the tracking portfolio and a 
quadratic program for determining the weights of these assets. In an experimental 
performance analysis, we defined the constituents of various stock market 
indices as investment universes. We applied a commercial standard solver to the 
mixed-integer quadratic programming formulation, and compared the results 
obtained to those obtained by the novel hybrid approach. It turns out that the 
hybrid approach performs advantageously in terms of tracking accuracy and of 
CPU time.
3. On the Dependency among Asian Currency Exchange Rates under 
the Influence of Financial Tsunami
YiKuan Jong (ykjong@mail.sju.edu.tw) St. Johnʼs University, Taiwan
Due to the development of internet, the information of currency exchange rates 
update almost instantly. Modeling the dependence between currency exchange 
2016_iccopt.indb   53 2016/07/22   11:58:15
ABSTRACTS
ICCOPT 201654
rates is a way to understand the change of relation between them. For financial 
time series, linear structure is often not sufficient to model the dependency 
between the log returns of two exchange rates. Therefore, we try use the 
t-distribution to model the marginal distributions of log-returns and the Clayton 
Copula to model the dependency between log-returns of exchange rates The pair 
dependent structure between Taiwan Dollar (TWD) and other Asian currency 
exchange rates against US. dollars before and after the Financial Tsunami has 
been studied. Evidence shows that dependency of different pairs of log-return for 
currency exchange rates varies. Dependence of currency exchange rates showed 
more correlated after the Financial Tsunami. 

■ Mon.B.5I
Monday, 13:30-14:45, Room 5I
Theoretical and Computational Aspects of Conic 
Programs
Cluster: Conic and Polynomial Optimization
Session organized by: Makoto Yamashita, Mituhiro Fukuda 
1. Second-Order Conditions for Nonlinear Semidefinite Optimization 
Problems via Slack Variables Approach
Ellen Hidemi Fukuda (ellen@i.kyoto-u.ac.jp) Kyoto University, 
Japan, Bruno Figueira Lourenço, Masao Fukushima
Comparing to the usual nonlinear optimization case, the second-order optimality 
conditions for nonlinear semidefinite optimization problems are more complex, 
because they involve extra terms associated to the curvature of the cone. As an 
alternative way for obtaining such optimality conditions, we propose the use of 
squared slack variables, which reformulate the original conic problem into an 
ordinary nonlinear programming problem. Although the original and the 
reformulated problems are equivalent in terms of optimal points, the relation 
between their stationary or Karush-Kuhn-Tucker points is less clear. In order to 
derive the second-order conditions, we first analyze such relation. Then, we will 
show that the second-order conditions of these problems are essentially the same, 
under appropriate regularity conditions.
2. Some Tractable Subcones for Testing Copositivity 
Akihiro Tanaka (tanaka.akihiro@sk.tsukuba.ac.jp) University of 
Tsukuba, Japan, Akiko Yoshise
The authors in previous papers devised certain subcones of the copositive cone 
and showed that one can detect whether a given matrix belongs to each of them 
by solving linear optimization problems. They also devised LP-based algorithms 
for testing copositivity using the subcones. Among others, they introduced a 
semidefinite basis that is a basis of the space of symmetric matrices consisting of 
rank-1 semidefinite matrices. In this talk, we will show that how the semidefinite 
basis can be used for constructing tractable subcones and examine the efficiency 
of those subcones for testing copositivity.
3. An Iterative Method using Boundary Distance for Box-constrained 
Nonlinear Semidefinite Programs
Makoto Yamashita (Makoto.Yamashita@is.titech.ac.jp) Tokyo 
Institute of Technology, Japan, Akihiko Komatsu
In this talk, we propose an iterative method to minimize a nonlinear objective 
function on a symmetric matrix over a box-type constraint in which the 
eigenvalues of the variable matrix are restricted to the interval [0, 1]. Though this 
optimization problem is one of simple nonlinear semidefinite programming 
problems, the size of the variable matrix that can be solved with a penalty barrier 
method is at most only 500. The proposed method uses a quadratic approximation 
function to obtain a search direction, then computes a step length determined by 
a radius in a similar way to the trust-region methods. The separation of the 
computation of the search direction and that of the step length enables us to 
handle large variable matrices whose sizes are more than 5000. We also give a 
global convergence of the proposed method to a first-order optimality point. In 
addition. We verified through numerical tests that the computation time of the 
proposed method is faster than a feasible direction method and the penalty 
barrier method. 

■ Mon.B.5K
Monday, 13:30-14:45, Room 5K
Conic and Integer Conic Optimization
Cluster: Conic and Polynomial Optimization
Session organized by: Tamás Terlaky, Miguel Anjos, Julio César Góez 
1. BiqCrunch: Solving Binary Quadratic Problems Efficiently using 
Semidefinite Optimization
Nathan Krislock (nkrislock@niu.edu) Northern Illinois University, 
USA, Jérôme Malick, Frédéric Roupin
BiqCrunch is a branch-and-bound solver that uses semidefinite optimization to 
compute high-quality bounds to many difficult (in fact, NP-hard) combinatorial 
optimization problems that can be modeled as binary quadratic problems, such as 
MaxCut, Max-k-Cluster, Maximum-Independent-Set, the Exact Quadratic 
Knapsack Problem, and the Quadratic Assignment Problem. BiqCrunch does not 
use an interior-point method for computing its bounds. Instead, an eigenvalue 
solver and a quasi-Newton method are used to efficiently compute tight bounds. 
We will discuss our bounding procedure and give an update on the new features 
and performance enhancements of the latest version of BiqCrunch.
2. On a Semidefinite Relaxation for the Sparse Linear Regression 
Problem
Hongbo Dong (hongbo.dong@wsu.edu) Washington State University, 
USA
We consider stronger convex relaxations for the sparse linear regression problem. 
In a recent work it has been shown that some piecewise quadratic concave 
penalty functions, including the minimax concave penalty (MCP) used in 
statistical community for variable selection, can be derived by using perspective 
relaxations. A related minimax problem providing tightest relaxation (in terms of 
lower bounds for the L2-L0 problem) can be solved by a semidefinite relaxation. 
In this subsequent work we derive sufficient conditions on the signal-noise ratio 
that guarantee high-probability exact support recovery of sparse signals. We 
compare our new condition with similar ones that were derived for the convex 
case of MCP, and show that our sufficient condition can be arbitrarily weaker. 
We further consider the heuristics by solving low-rank approximations for the 
semidefinite relaxation and report our numerical findings.
3. Disjunctive Conic Cuts for Mixed Integer Second Order Cone 
Optimization
Julio Cesar Goez (julio.goez@nhh.no) Norwegian School of 
Economics, Norway, Pietro Belotti, Imre Polik, Ted Ralphs, Tamás 
Terlaky
We investigate the derivation of disjunctive conic cuts for mixed integer second 
order cone optimization (MISOCO). These conic cuts characterize the convex 
hull of the intersection of a disjunctive set and the feasible set of a MISOCO 
problem. We present a full characterization of these inequalities when the 
disjunctive set considered is definned by parallel hyperplanes. 

■ Mon.B.5L
Monday, 13:30-14:45, Room 5L
Moments, Positive Polynomials & Optimization: Part II
Cluster: Conic and Polynomial Optimization
Session organized by: Jiawang Nie, Jean B Lasserre
1. LP Approximations to Polynomial Optimization Problems with 
Small Tree-Width
Gonzalo Munoz (gonzalo@ieor.columbia.edu) Columbia University, 
USA, Daniel Bienstock
We present a class of linear programming approximations for mixed-integer 
polynomial optimization problems that take advantage of structured sparsity in 
the constraints. In particular, we show that if the intersection graph of the 
constraints (obtained by creating a node per variable and an edge whenever two 
variables appear in a common constraint) has tree-width bounded by a constant, 
then for any arbitrary tolerance there is a linear programming formulation of 
polynomial size that approximates the problem. Via an additional reduction, we 
obtain a polynomial-time approximation scheme for the “AC-OPF” problem on 
graphs with bounded tree-width. This improves on a number of results in the 
literature, both from the perspective of formulation size and generality.
2. Robust to Dynamics Optimization (RDO) 
Amir A Ahmadi (a_a_a@princeton.edu) Princeton University, USA, 
Oktay Gunluk
We introduce a new type of robust optimization problems that we call “robust to 
dynamics optimization” (RDO). The input to an RDO problem is twofold: (i) a 
mathematical program (e.g., an LP, SDP, IP), and (ii) a dynamical system (e.g., 
a linear, nonlinear, discrete, or continuous dynamics). The objective is to 
maximize over the set of initial conditions that forever remain feasible under the 
dynamics. We initiate an algorithmic study of RDO and demonstrate tractability 
of some important cases.
2016_iccopt.indb   54 2016/07/22   11:58:15
ABSTRACTS
ICCOPT 2016 55
3. Error Bounds for Parametric Polynomial Systems with 
Applications to Higher-Order Stability Analysis and Convergence 
Rate
Guoyin Li (g.li@unsw.edu.au) University of New South Wales, 
Australia, B S Mordukhovich, T TA Nghia, T S Pham
In this talk, we consider parametric inequality systems described by polynomial 
functions in finite dimensions, where state-dependent infinite parameter sets are 
given by finitely many polynomial inequalities and equalities. Such systems can 
be viewed, in particular, as solution sets to problems of generalized semi-infinite 
programming with polynomial data. Exploiting the imposed polynomial 
structure together with powerful tools of variational analysis and semialgebraic 
geometry, we establish an extension of the Lojasiewicz gradient inequality to the 
general nonsmooth class of supremum marginal functions as well as higher-
order (Holder type) local error bounds results with explicitly calculated 
exponents. The obtained results are applied to higher-order quantitative stability 
analysis for various classes of optimization problems including generalized 
semi-infinite programming with polynomial data, optimization of real 
polynomials under polynomial matrix inequality constraints, and polynomial 
second-order cone programming. Other applications provide explicit 
convergence rate estimates for the cyclic projection algorithm to find common 
points of convex sets described by matrix polynomial inequalities and for the 
asymptotic convergence of trajectories of sub-gradient dynamical systems in 
semi-algebraic settings. 

■ Mon.B.m3S
Monday, 13:30-14:45, Room m3S
Stochastic Optimization
Cluster: Convex and Nonsmooth Optimization
Session organized by: Francis Bach
1. Second-Order Optimization for Machine Learning in Linear Time
Elad Hazan (ehazan@cs.princeton.edu ) Princeton University, USA, 
Naman Agarwal, Brian Bullins
First-order stochastic methods are the state-of-the-art in large-scale machine 
learning optimization due to their extremely efficient per-iteration computational 
cost. Second-order methods, while able to provide faster convergence, have been 
much less explored due to the high cost of computing the second-order 
information. We will present a second-order stochastic method for optimization 
problems arising in machine learning that match the per-iteration cost of gradient 
descent, yet enjoy convergence properties of second-order optimization.
2. Proximal Minimization by Incremental Surrogate Optimization 
(MISO) 
Julien Mairal (julien.mairal@inria.fr) INRIA, France, Hongzhou Lin, 
Zaid Harchaoui
We present an incremental algorithm for solving a large finite sum of strongly-
convex smooth functions with a possibly non-smooth convex regularization 
term. The method achieves a linear convergence rate, which may be independent 
of the condition number of the problem when the sum is large enough, making it 
particularly well suited to big data problems arising in machine learning. This 
strategy can be interpreted as a duality-free variant of stochastic dual coordinate 
ascent (SDCA), with a different step-size and simpler optimality certificate. The 
construction is indeed purely primal and does not rely on duality. Last but not 
least, the algorithm may be accelerated in the sense of Nesterov, providing 
significant improvements when the problem is ill-conditioned. We illustrate the 
effectiveness of our approach on large-scale classification problems, where the 
method achieves competitive results with state-of-the-art solvers for support 
vector machines.
3. An Optimal Randomized Incremental Gradient Method 
Guanghui Lan (george.lan@isye.gatech.edu) Georgia Institute of 
Technology, USA, Yi Zhou
We present a randomized incremental gradient method and show that this 
algorithm possesses unimprovable rate of convergence for convex optimization 
by deriving a lower complexity bound for a general class of randomized methods. 
We provide a natural game theoretic interpretation for this method as well as for 
the related Nesterovʼs optimal method. We also point out the situations when this 
randomized algorithm can outperform the deterministic optimal method.

■ Mon.B.m3AB
Monday, 13:30-14:45, Room m3AB
Set Optimization: Advances and Applications
Cluster: Multi-Objective and Vector Optimization
Session organized by: Andreas H Hamel
1. Set-valued Variational Inequalities in Vector Optimization
Carola Schrage (Carola.Schrage@unibz.it) Free University of Bozen, 
Italy, Giovanni P Crespi, Andreas H Hamel, Matteo Rocca
Variational inequalities of directional derivatives are widely used to solve vector 
optimization problems. Since the ordering in vector spaces generally lacks 
completeness, the introduced methods typically turn out to be somewhat 
awkward; Either, the derivatives attain `∞ valuesʼ which are defined in some way 
or other, or derivatives are defined as set-valued functions, thus the type of 
function under consideration is changed. We propose to follow a different 
approach by introducing the set-valued extension of a vector-valued function. 
Applying the Complete Lattice Approach to set-valued functions, we develop a 
coherent system of variational inequalities characterizing solutions to the set 
optimization problem, containing the vector optimization problem as a special 
case.
2. Introducing Well-Posedness to Set-Optimization
Giovanni Paolo Crespi (giovanni.crespi@uninsubria.it) Università 
degli studi dellʼInsubria, Italy, Andreas Hamel, Matteo Rocca, Carola 
Schrage
Set-optimization provides a new look at Optimization problems involving set-
valued objective functions. The approach allows to handle sets as elements in a 
(con-)linear space and to act mostly as with single-valued functions. Among 
other topics, well-posedness of an optimization problem can be addressed 
through these tools. In the process, we need to provide a suitable notion of 
approximate solution to a set-optimization problem that allows unveiling a new 
notion of well-posed set-optimization.
3. The Fundamental Duality Formula in Convex Set Optimization
Andreas H Hamel (andreas.hamel@unibz.it) Free University Bozen, 
Italy
To an enormous extend, convex duality is responsible for the success and 
applicability of (scalar) optimization theory. It even serves as blueprint for non-
convex theories such as dc-programming and duality in combinatorial 
optimization. A similar success is foreseen for duality in set optimization based 
on the complete lattice approach which uses the so-called set relations as a tool 
for the construction of appropriate image spaces (of sets). The Fenchel-Moreau 
theorem along with a new version of the fundamental duality formula for set 
optimization will be presented along with applications in finance and economics. 
Moreover, the decades-old problem with duality in vector optimization will 
shown to be completely resolvable using the new approach.

■ Tue.A.1S
Tuesday, 10:30-11:45, Room 1S
Nonlinear Optimization Algorithms and Their 
Complexity II
Cluster: Nonlinear Optimization
Session organized by: Philippe Toint
1. A Trust Region Algorithm with a Worst-Case Iteration Complexity 
of O(ε- 3/2) for Nonconvex Optimization
Mohammadreza Samadi (mos213@lehigh.edu) Lehigh University, 
USA, Frank E Curtis, Daniel P Robinson
We present a trust region method for unconstrained nonconvex optimization that, 
in the worst-case, is able to drive the norm of the gradient of the objective below 
a prescribed threshold ε > 0 after at most O(ε- 3/2) iterations. Our work is inspired 
by the recently proposed Adaptive Regularisation framework using Cubics (i.e., 
the ARC algorithm), which attains the same worst-case complexity bound. Our 
algorithm is modeled after a traditional trust region algorithm, but employs 
modified step acceptance criteria and a novel trust region updating mechanism 
that allows it to achieve this desirable property. Importantly, our method also 
maintains standard global and fast local convergence guarantees. Numerical 
results are presented.
2. Second-Order Optimality and (Sometimes) Beyond
Philippe Toint (philippe.toint@unamur.be) University of Namur, 
Belgium, Coralia Cartis, Nick Gould
The talk will discuss optimality conditions for high-order criticality and consider 
2016_iccopt.indb   55 2016/07/22   11:58:16
ABSTRACTS
ICCOPT 201656
algorithms whose purpose is to obtain approximate high-order critical points. 
The unconstrained, convexly-constrained and nonlinear equality constrained 
cases will be considered and worst-case complexity estimates presented for these 
cases. These estimates recover known results for the first- and second-order 
unconstrained cases. Limitations of the present approach(es) to numerical 
optimization will also be discussed in view of the presented results.

■ Tue.A.1A
Tuesday, 10:30-11:45, Room 1A
Nonconvex and Non-Lipschitz Optimization: 
Algorithms and Applications 1
Cluster: Nonlinear Optimization
Session organized by: Ya-Feng Liu
1. Sparse Recovery via Nonconvex Optimization, with Application in 
Metagenomics
Simon Foucart (foucart@tamu.edu) Texas A&M University, USA
After reviewing some fundamental results on sparse recovery via ̀ q-minimization 
with q ≤ 1, we introduce the metagenomics problem of “community profiling”. 
Using k-mer information, it essentially boils down to a sparse recovery problem 
with added nonnegativity constraints. We describe the `1-regularization that we 
selected and solved as a nonnegative least square problem when creating the 
software Quikr. Next, we discuss the corresponding `q-regularization with q < 1. 
We conclude by refining the optimization program with the replacement of the 
`q-quasinorm by the more meaningful notion of q-biodiversity.
2. Penalty Methods for a Class of Non-Lipschitz Optimization 
Problems
Xiaojun Chen (maxjchen@polyu.edu.hk) The Hong Kong Polytechnic 
University, Hong Kong, Zhaosong Lu, Ting Kei Pong
We consider a class of constrained optimization problems with a possibly 
nonconvex non-Lipschitz objective and a convex feasible set being the 
intersection of a polyhedron and a possibly degenerated ellipsoid. Such a 
problem has a wide range of applications in data science, where the objective is 
used for inducing sparsity in the solutions while the constraint set models the 
noise tolerance and incorporates other prior information for data fitting. To solve 
this kind of constrained optimization problems, a common approach is the 
penalty method. However, there is little theory on exact penalization for problems 
with nonconvex and non-Lipschitz objective functions. In this paper, we study 
the existence of exact penalty parameters regarding local minimizers, stationary 
points and ε-minimizers under suitable assumptions. Moreover, we discuss a 
penalty method whose subproblems are solved via a nonmonotone proximal 
gradient method with a suitable update scheme for the penalty parameters, and 
prove the convergence of the algorithm to a KKT point of the constrained 
problem. Preliminary numerical results demonstrate the efficiency of the penalty 
method for finding sparse solutions of underdetermined linear systems.
3. A Continuous DC Programming Approach to Nonlinear Mixed 
Integer Programs
Takayuki Okuno (t_okuno@rs.tus.ac.jp) Tokyo University of Science, 
Japan, Yoshiko Ikebe
In this talk, we consider a mixed integer nonconvex program (MINP). In 
particular, we restrict ourselves to the MINP whose objective function is a DC 
function. Based on a new technique proposed by T. Maehara, et al.(2015), we 
transform the MINP to a certain equivalent continuous DC program. For solving 
it, we propose a proximal point type DC algorithm. Under several mild 
assumptions, we prove that the sequence generated by the proposed method 
converges to some stationary point of the MINP.

■ Tue.A.1B
Tuesday, 10:30-11:45, Room 1B
Advances in PDE-constrained Optimization II
Cluster: PDE-constrained Optimization
Session organized by: Michael Ulbrich, Kazufumi Ito
1. A Geometric Approach of Some Multitime Multiobjective 
Variational Problems
Ariana Pitea (arianapitea@yahoo.com) University Politehnica of 
Bucharest, Romania
In a geometric framework, some multitime multiobjective fractional variational 
problems of minimizing a vector of quotients of functionals subject to certain 
partial differential equations and/or inequations are studied by means of the 
parametric approach. Necessary and sufficient optimality conditions are 
established, by using generalized types of convexity. Also, multiobjective 
variational dual problems are introduced, and various types of duality results are 
stated.
2. Optimal Control of Nonsmooth Semilinear Parabolic Equations
Livia Susu (liviasusu@yahoo.com) TU Dortmund, Germany, Christian 
Meyer
This talk is concerned with an optimal control problem governed by a semilinear, 
nonsmooth operator differential equation. The nonlinearity is locally Lipschitz-
continuous and directionally differentiable, but not Gâteaux-differentiable. Two 
types of necessary optimality conditions are derived, the first one by means of 
regularization, the second one by using the directional differentiability of the 
control-to-state mapping. The talk ends with the application of the general results 
to a semilinear heat equation involving a general Lipschitz-continuous function.
3. Mixed-Integer PDE-constrained Optimization
Sven Leyffer (leyffer@anl.gov) Argonne National Laboratory, USA, 
Bart van Bloemen Waanders Pelin Cay Drew Kouri Anna Thuenen
Many complex applications can be formulated as optimization problems 
constrained by partial differential equations (PDEs) with integer decision 
variables. Examples include the remediation of contaminated sites and the 
maximization of oil recovery; the design of next generation solar cells; the layout 
design of wind-farms; the design and control of gas networks; disaster recovery; 
and topology optimization. We will present emerging applications of mixed-
integer PDE-constrained optimization, review existing approaches to solve these 
problems, and highlight the computational and mathematical challenges of this 
new class of challenging problems. We introduce a new set of benchmark 
problems for this class of problems, and present some early numerical experience 
using both mixed-integer nonlinear solvers and heuristic techniques.

■ Tue.A.1C
Tuesday, 10:30-11:45, Room 1C
Derivative-free Optimization Methods for Structured 
Problems
Cluster: Derivative-free and Simulation-based Optimization
Session organized by: Francesco Rinaldi, Zaikun Zhang
Session chair: Stefan M Wild
1. A New DFO Algorithm for the Optimization of Partially Separable 
Functions
Laurent Dumas (laurent.dumas@uvsq.fr) Versailles University, 
France, Didier Ding, Benjamin Marteau
Many industrial problems involve optimizing a function without any knowledge 
of its derivative. To reduce the number of evaluations of this function, we can try 
to identify and exploit its particular structure. We present a derivative free 
algorithm adapted to partially separable functions. At each iteration of the 
algorithm, a quadratic interpolation model is computed in a dynamic trust region. 
Both the cost of the model update step and the cost of the interpolation set 
improvement step are designed to minimize the number of evaluations of the 
objective function. By exploiting a self-correcting property of the interpolation 
sets, we are able to propose a proof of the global convergence of our algorithm. 
To further validate our method, we performed some numerical tests that give 
some promising results.
2. A Trust Region Method for Glass Box/Black Box Optimization
John Patrick Eason (jeason@andrew.cmu.edu) Carnegie Mellon 
University, USA, Lorenz T Biegler
Modern nonlinear programming solvers can efficiently handle very large scale 
optimization problems when accurate derivative information is available. 
However, black box or derivative free modeling components are often 
unavoidable in practice when the modeled phenomena may cross length and time 
scales. Here we consider the problem of hybrid glass box/black box optimization, 
where part of the system is described by black-box (simulation) models without 
derivative information, and the rest is described by equation oriented constraints. 
We propose an algorithm that combines ideas from SQP filter methods and 
derivative free trust region methods to solve this class of problems. The black 
box portion of the model is replaced by a sequence of reduced models in trust 
region subproblems. By carefully managing reduced model construction, the 
algorithm reduces the number of function calls to the (often expensive) black 
box; global convergence is also proved under mild assumptions. We further 
investigate methods to take advantage of the equation oriented information of the 
2016_iccopt.indb   56 2016/07/22   11:58:16
ABSTRACTS
ICCOPT 2016 57
system to propagate optimality error from reduced model error. This information 
leads to more efficient control of reduced model construction and effective 
termination criteria.
3. Model-based Methods for Composite Blackbox Optimization
Stefan M Wild (wild@anl.gov) Argonne National Laboratory, USA, 
Kamil Khan, Jeffrey Larson, Matt Menickelly
We discuss model-based methods for solving nonlinear optimization problems 
where the objective function and/or some constraints depend on blackbox 
outputs from a numerical or physical simulation. We provide real applications 
where the composite function is nonsmooth, or even discontinuous, but using 
smooth models of the blackbox components can accelerate convergence to 
stationary points. Numerical results will identify benefits of the approach and 
opportunities for further development.

■ Tue.A.4A
Tuesday, 10:30-11:45, Room 4A
Dynamics and Optimal Control
Cluster: Applications in Energy, Science and Engineering
Session organized by: Jose Alberto Gomez
1. Multi-Objective Co-Design for Embedded Optimization-based 
Control
Bulat Khusainov (b.khusainov@imperial.ac.uk) Imperial College 
London, United Kingdom, Eric Colin Kerrigan, George Anthony 
Constantinides
Model Predictive Controller (MPC) relies on solving an optimization problem at 
every sampling instant in order to derive the optimal input sequence based on a 
plant model. An MPC design problem often involves a number of tunable 
parameters that affect different design objectives. Conventional design 
approaches propose manual tuning of design parameters in order to improve the 
closed-loop cost function, while considering computational resources as a fixed 
constraint. We introduce a multi-objective formulation of an MPC design 
problem, which aims to identify Pareto optimal designs with the best 
performance-resources trade-offs. Our approach is based on Bayesian 
optimization and uses a hypervolume expected improvement measure which was 
recently proposed in the multi-objective optimization literature. The key features 
of our algorithm are: (1) simultaneous optimization of high-level algorithmic 
parameters (e.g. prediction horizon length) and low-level hardware design 
variables (e.g. data representation); (2) parallel evaluation of objective functions; 
(3) building separate feasibility model to eliminate infeasible designs on the 
early stages of design flow. A numerical example is presented for a field-
programmable gate array implementation of a model predictive controller.
2. Novel Computational Framework for the Numerical Solution of 
Constrained Optimal Control Problems
Anil V Rao (anilvrao@gmail.com) University of Florida, USA
A novel computational framework is described for solving complex constrained 
nonlinear optimal control problems. The framework has a wide variety of 
applications in mechanical and aerospace engineering. The basis of the 
framework is the new class of hp-adaptive Gaussian quadrature methods that 
transcribe the continuous optimal control problem to a finite-dimensional 
nonlinear optimization problem. The hp-adaptive methods have the feature that 
high accuracy can be obtained with a significantly smaller mesh when compared 
with traditional fixed-order methods while accurately capturing nonsmoothness 
or rapidly changing behavior. The hp-adaptive methods employed using 
advanced sparse nonlinear programming (NLP) solvers. The derivatives required 
by the NLP solvers are obtained using a new approach to algorithmic 
differentiation where efficient derivative source code is produced through a 
method that combines operator overloading with source transformation. The 
mathematical foundation of the framework is provided and examples are given 
that demonstrate the improvement over previously developed approaches.
3. Optimization of Dynamic Systems with Linear Programs 
Embedded and Its Application to Sustainable Biofuels Production
Jose Alberto Gomez (jagomezr@mit.edu) Massachusetts Institute of 
Technology, USA, Kai Höffner, Kamil A Khan, Paul I Barton
Bioprocesses are challenging to model. With genome-scale metabolic network 
reconstructions of the microorganisms, reliable and predictive bioprocess models 
can be constructed using dynamic flux balance analysis. Dynamic flux balance 
analysis results in dynamic systems with linear programs embedded, which are 
nonsmooth. With lexicographic differentiation, sensitivities of dynamic flux 
balance analysis systems can be computed. Therefore, their optimization is 
possible using nonsmooth optimization algorithms. These ideas have been 
applied to a raceway pond system for biomass cultivation. Cellulosic sugars are 
fed to raceway ponds where heterotrophic and autotrophic microorganisms 
interact to convert the sugars and solar energy into lipids. This raceway pond 
system has been modeled using dynamic flux balance analysis, and has been 
optimized to maximize lipids production. This optimization problem yields an 
optimal design and operation of the raceway pond system. This framework can 
be used to discover cost-competitive biodiesel processes. 

■ Tue.A.4B
Tuesday, 10:30-11:45, Room 4B
Financial Optimization and Robo Advisors 2
Cluster: Applications in Finance and Economics
Session organized by: Gyun Jun
1. Goal Based Investment via Multi-Stage Stochastic Programming 
for Robo-Advisor Service — Part I: Modeling Issues
Yongjae Lee (yongjae.lee@kaist.ac.kr) KAIST, Korea, Woo Chang 
Kim, Do-gyun Kwon, Jang Ho Kim
Robo-advisors, which denote recently developed online investment management 
or advisory services, aim to attract non high-net-worth individual investors by 
significantly lowering the entry barrier to professional wealth management 
industry. Unfortunately, existing schemes of robo-advisors have not been 
sophisticated enough to provide fully personalized investment advices, as these 
services make decisions based only on the online questionnaire on the userʼs risk 
appetite and high-level investment goals. However, it surely is challenging to ask 
clients, who might lack financial literacy, to provide their detailed financial 
situation through online platform. Therefore, we propose a goal-based investment 
model that only requires the input of wealth, income, and consumption goals with 
priorities. Note that all the inputs are non-arbitrary numbers that users can easily 
provide. The model employs multi-stage stochastic programming approach, 
while maintaining its linear programming problem structure. With its simplicity, 
flexibility, and computational efficiency, it could suggest a new paradigm change 
in the robo-advisor industry. Part I discusses the detailed problem structure of the 
proposed model, and Part II provides extensive numerical analyses to demonstrate 
its practical advantages.
2. Goal Based Investment via Multi-Stage Stochastic Programming 
for Robo-Advisor Service — Part II: Implementation Issues
Do-gyun Kwon (dogyun@kaist.ac.kr) KAIST, Korea, Woo Chang 
Kim, Yongjae Lee, Jang Ho Kim
In Part I, we proposed a goal-based investment model for robo-advisor service, 
which employs multi-stage stochastic programming approach. In order for an 
investment model to be actually implemented to provide automated financial 
advisory service, it should exhibit two important features. First, the input 
parameters for the model should be user-friendly, or equivalently, specific. As 
noted in Part I, our model does not require any arbitrary inputs; it only requires 
specific and intuitive inputs that users can easily figure out. Second, the model 
should at least support quasi-real-time interaction, as users should be able to see 
how their investment changes with respect to their inputs to the model. In Part II, 
therefore, we provide extensive numerical analyses on the model to demonstrate 
its computational efficiency. The analyses would further reveal its advantages 
from a practical standpoint, such as flexibility and expandability.
3. Asian Perspective on ʻRobo-Advisorʼ: Case of Korean Market
Gyun Jeon (jgrats@hotmail.com) Samsung Securities Co., Ltd., Korea
There is not an unified definition about robo-advisor but some common 
characters in robo-advisor; 1) structuring portfolio with several assets class, 2) 
managing portfolio based investing engine without human managerʼs judgment, 
3) rebalancing portfolio timely corresponding market conditions 4) providing 
on-line advisory service. Robo-advisor used to choose ETFs for low cost and 
public advisory services. The expense cost of ETFs are lower than common 
mutual funds and ETFs listed in U.S. exchanges covers equity and fixed income 
including domestic and other countries, even commodities and REITs. The 
abundant ETF is one of success conditions for robo-advisor. Leading robo-
advisor firms applied some financial theories like as Markowitzʼ Portfolio theory, 
Black-Litterman model, behavior finance, etc. Goal-based investment strategy 
based investorʼs risk budget has to take dynamic rebalancing its portfolio. Since 
the risk-return profile of assets has change continuously, robo-advisor needs the 
big data analysis techniques about assetʼs profiles and automatic rebalancing 
algorithm of catching regime switching. Several robo-advisors utilize machine 
learning technic into analyzing the financial time series and economic data. I 
analyzed the effects of robo-advisor on asset management industry and found the 
rich potential of robo-advisor in Asia, especially Korea.

2016_iccopt.indb   57 2016/07/22   11:58:16
ABSTRACTS
ICCOPT 201658
■ Tue.A.5A
Tuesday, 10:30-11:45, Room 5A
Robust Multi-Objective Optimization Problems
Cluster: Multi-Objective and Vector Optimization
Session organized by: Gue Myung Lee
1. Necessary Optimality Conditions for Nonsmooth Multiobjective 
Bilevel Optimization Problems
Chuong Thai Doan (chuongthaidoan@gmail.com) University of New 
South Wales, Australia
This talk is devoted to the study of a nonsmooth multiobjective bilevel 
optimization problem, which involves the vector-valued objective functions in 
both levels of the considered program. We first formulate a relaxation problem 
for the multiobjective bilevel problem and examine the relationships of solutions 
between them. We then establish Fritz John (FJ) and Karush-Kuhn-Tucker 
(KKT) necessary conditions for the nonsmooth multiobjective bilevel 
optimization problem via its relaxation. This is done by studying a related 
multiobjective optimization problem with operator constraints.
2. Surrogate Duality for Quasiconvex Vector Optimization with Data 
Uncertainty
Satoshi Suzuki (suzuki@math.shimane-u.ac.jp) Shimane University, 
Japan, Daishi Kuroiwa
In this talk, we study surrogate duality for quasiconvex vector optimization with 
data uncertainty. We investigate surrogate strong duality for robust quasiconvex 
vector optimization with its necessary and sufficient constraint qualification. In 
addition, we compare a solution of our robust counterpart with a solution in set 
optimization.
3. A Coordinate Descent Homotopy Method for Bi-Level Problem 
and Linearly Constrained Minimization
Sangwoon Yun (yswmathedu@skku.edu) Sungkyunkwan University, 
Korea, Yoon Mo Jung
We consider the bi-level problem which minimizes a convex function over the 
set of stationary points of a certain smooth function. If the smooth function is a 
linear least square function, then the bi-level problem is a linearly constrained 
convex minimization problem which includes the L1-minimization problem 
arising in compressed sensing. The proposed method solves the bi-level problem 
by applying a coordinate gradient descent method to a sequence of the regularized 
subproblems. We prove that any cluster point of the generated iterates is a 
solution of the bi-level problem. We compare our proposed method with 
Bregman iterative algorithm and linearized Bregman iterative algorithm for 
solving large-scale L1-minimization problems.

■ Tue.A.5D
Tuesday, 10:30-11:45, Room 5D
Theoretical Advances in Linear Optimization — 
Barrier Methods
Cluster: Linear Optimization
Session organized by: Aaron Daniel Sidford, Yin Tat Lee
1. A Faster Algorithm for Linear Programming and the Maximum 
Flow Problem
Yin Tat Lee (YinTatLee@gmail.com) Massachusetts Institute of 
Technology, USA, Aaron Daniel Sidford
In this talk, we will present a new algorithm for solving linear programs. Given 
a linear program with n variables, m > n constraints, and bit complexity L, our 
algorithm runs in  iterations each consisting of solving Õ(1) linear 
systems and additional nearly linear time computation. Our method improves 
upon the convergence rate of previous state-of-the-art linear programming 
methods which required solving either  linear systems [R88] or 
consisted of Õ((mn)
1
4 ) steps of more expensive linear algebra [VA93]. 
Interestingly, our algorithm not only nearly matches the convergence rate of the 
universal barrier of Nesterov and Nemirovskii [NN94], but in the special case of 
the linear programming formulation of various flow problems our methods 
converge at a rate faster than that predicted by any self-concordant barrier. In 
particular, we achieve a running time of  for solving the 
maximum flow problem on a directed graph with |E| edges, |V| vertices, and 
capacity ratio U, thereby improving upon the previous fastest running time for 
solving this problem when |E| > Ω(|V|(1+ε)) for any constant epsilon. This talk will 
be split into two parts given by Yin Tat Lee and Aaron Sidford.
2. A Faster Algorithm for Linear Programming and the Maximum 
Flow Problem
Aaron Daniel Sidford (aaron.sidford@gmail.com) Stanford University, 
USA, Yin Tat Lee
In this talk, we will present a new algorithm for solving linear programs. Given 
a linear program with n variables, m > n constraints, and bit complexity L, our 
algorithm runs in  iterations each consisting of solving Õ(1) linear 
systems and additional nearly linear time computation. Our method improves 
upon the convergence rate of previous state-of-the-art linear programming 
methods which required solving either  linear systems [R88] or 
consisted of Õ((mn)
1
4 ) steps of more expensive linear algebra [VA93]. 
Interestingly, our algorithm not only nearly matches the convergence rate of the 
universal barrier of Nesterov and Nemirovskii [NN94], but in the special case of 
the linear programming formulation of various flow problems our methods 
converge at a rate faster than that predicted by any self-concordant barrier. In 
particular, we achieve a running time of  for solving the 
maximum flow problem on a directed graph with |E| edges, |V| vertices, and 
capacity ratio U, thereby improving upon the previous fastest running time for 
solving this problem when |E| > Ω(|V|(1+ε)) for any constant epsilon. This talk will 
be split into two parts given by Yin Tat Lee and Aaron Sidford.
3. Geometric Median
Jakub Pachocki (merettm@gmail.com) Harvard University, USA, 
Michael Cohen, Yin Tat Lee, Gary Miller, Aaron Sidford
The geometric median of a set of points is the point minimizing the sum of 
distances to the points. The problem of finding an efficient algorithm to compute 
the median, known as the Fermat-Weber problem, has remained an active area of 
research for many years. In this talk we present the first nearly linear time 
algorithm for computing the median.

■ Tue.A.5E
Tuesday, 10:30-11:45, Room 5E
On the Interplay of Choice, Robustness and 
Optimization
Cluster: Robust Optimization
Session organized by: Karthik Natarajan
1. Multi-Product Pricing Optimization with Robust Choice Model
Zhenzhen Yan (a0109727@u.nus.edu) National University of 
Singapore, Singapore, Cong Cheng, Karthik Natarajan, Chung Piaw 
Teo
We study the multi-product pricing problem with robust choice model using 
pricing experiments. In particular, we develop a data driven approach to this 
problem using the theory of marginal distribution. We show that the pricing 
problem is convex for a large class of discrete choice models, including the 
classical logit and nested logit model. In fact, our model remains convex as long 
as the marginal distribution is log-concave. More importantly, by fitting data to 
optimize the selection of choice model, we develop an LP based approach to the 
semi-parametric version of the pricing problem. Preliminary tests using a set of 
automobile data show that this approach provides near optimal solution, even 
with random coefficient logit model.
2. Analysis of Discrete Choice Models: A Welfare-based Approach
Xiaobo Li (lixx3195@umn.edu) University of Minnesota, USA, 
Zizhuo Wang, Guiyun Feng
Based on the observation that many existing discrete choice models admit a 
welfare function of utilities whose gradient gives the choice probability vector, 
we propose a new perspective to view choice models by treating the welfare 
function as the primitive. We call the resulting choice model the welfare-based 
choice model. The welfare-based choice model is meaningful on its own by 
providing an alternative way of constructing choice models. We prove that the 
welfare-based choice model is equivalent to the representative agent choice 
model and the semi-parametric choice model, establishing the equivalence of the 
latter two. We show that these three models are all strictly more general than the 
random utility model, while when there are only two alternatives, those four 
models are equivalent. Moreover, the welfare-based choice model subsumes the 
nested logit model with positive dissimilarity parameters. We then define a new 
concept in choice models: substitutability/complementarity between alternatives. 
We show that the random utility model only allows substitutability between 
different alternatives; while the welfare-based choice model allows more flexible 
substitutability/complementarity patterns. We argue that such flexibility could be 
desirable in capturing certain practical choice patterns, such as the halo effects. 
We present ways of constructing new choice mode.
2016_iccopt.indb   58 2016/07/22   11:58:16
ABSTRACTS
ICCOPT 2016 59
3. Distributionally Robust Project Crashing with Full, Partial or No 
Correlation Information
Selin Damla Ahipasaoglu (ahipasaoglu@sutd.edu.sg) Singapore 
University of Technology and Design, Singapore, Karthik Natarajan, 
Dongjian Shi 
Project crashing is a method for optimally shortening the project makespan by 
reducing the time of one or more activities in a project network to less than its 
normal activity time at the expense of additional costs. The traditional project 
crashing problem assumes that the activity durations are deterministic. However, 
in reality activity durations are uncertain. We propose a distributionally robust 
project crashing problem to minimize the worst-case expected makespan where 
the distributions of the activity durations are specified only up to the first two 
moments. Given mean and variance, we consider three moments models with 
full, partial or no correlation information on the activity durations. The 
complexity results known for the worst-case expected makespan is extended to 
the crashing problems under the three moment models, and second order cone 
program (SOCP) and semidefinite program (SDP) reformulations are provided. 
Furthermore, we show that the distributionally robust project crashing problem 
can be formulated as a saddle point problem with a convex-concave objective 
function. Based on a characterization of the gradients, the problem can be solved 
by a globally convergent projection and contraction algorithm in a short time. 
The numerical results show that the decisions obtained by the distributionally 
robust project crashing models are quite reasonable.

■ Tue.A.5F
Tuesday, 10:30-11:45, Room 5F
Sparse Solution Reconstruction in Inverse Problems
Cluster: Sparse Optimization and Information Processing
Session organized by: Elena Resmerita
1. Precise Relaxation of Nonconvex Energies via Structured Sparsity
Thomas Möllenhoff (thomas.moellenhoff@in.tum.de) Technical 
University of Munich, Germany, Emanuel Laude, Michael Moeller, 
Jan Lellmann, Daniel Cremers
Many inverse problems in computer vision such as depth from stereo, depth from 
focus, optical flow or robust image denoising are inherently ill-posed and require 
regularization. Due to the nonconvexity of the data fidelity term, global 
optimization of corresponding energies is challenging. Functional lifting 
methods find a convex representation of such energies by reformulating the 
original problem in a higher dimensional space. By considering a novel 
structured k-sparsity constraint, we propose a precise piecewise convex 
approximation. We derive a tight convex relaxation of the nonconvex k-sparsity 
constraint and show how the resulting optimization problem can be solved using 
a first-order primal-dual algorithm and efficient epigraphical projections.
2. On Convergence of Sparsity-promoting Regularization for Non-
sparse Solutions
Daniel Gerth (daniel.gerth@dk-compmath.jku.at) TU Chemnitz, 
Germany, Bernd Hofmann
In recent years, sparsity promoting regularization has gained a lot of attention in 
the Inverse Problems community. It has been shown by various authors that, 
assuming the true solution is indeed sparse with respect to some prescribed basis 
and certain smoothness conditions are imposed, this method of regularization 
may render the problem of finding the solution to the Inverse Problem from 
noisy data essentially well-posed. It may, however, be of interest to find a sparse 
approximation to a non-sparse true solution of the Inverse Problem, for example 
to extract certain features of the solution. This is a special case of oversmoothing 
regularization, where it is known a-priori that the sought-after solution does not 
fulfill the smoothness properties implied by the regularization method. In this 
case, not much is known about its convergence properties. In this talk we give a 
short historical overview about the convergence theory for sparsity-promoting 
regularization methods. We focus on theory based on so called variational 
inequalities, an abstract smoothness condition which can often be shown in 
sparsity-promoting regularization. We then review how the case of non-sparse 
solutions is covered in the literature so far. In the main part of the talk we discuss 
further extensions of the existing theory to this situation; to problems, challenges 
and limitations.
3. Variable Exponent Penalties for Sparse Solution Reconstruction
Elena Resmerita (elena.resmerita@aau.at) Alps-Adria University of 
Klagenfurt, Austria, Kristian Bredies, Barbara Kaltenbacher
The seminal paper of Daubechies, Defrise, DeMol made clear that ` p spaces with 
p ∈ [1,2) and p-powers of the corresponding norms are appropriate settings for 
dealing with reconstruction of sparse solutions of ill-posed problems by 
regularization. The case p = 1 provides the best results in most of the situations 
as compared to the cases p ∈ (1,2). An extensive literature gives great credit also 
to using ̀ p spaces with p ∈ (0,1) together with the corresponding quasinorms. In 
any of these settings, the question of how to choose the exponent p has been not 
only a numerical issue, but also a philosophical one. In this work, we introduce a 
more flexible way of sparse regularization by varying exponents. We present the 
corresponding functional analytic framework, that leaves the setting of normed 
spaces but works with so-called F-norms. We collect some known results about 
these F-norms, provide some new results that are of interest in the context of 
sparse regularization and investigate regularization properties of these penalties.

■ Tue.A.5G
Tuesday, 10:30-11:45, Room 5G
Advances in Optimization Modeling Languages
Cluster: Optimization Implementations and Software
Session organized by: John Siirola
1. Modeling Abstractions and Automatic Discretization Frameworks 
for Optimization Problems with Differential Equations in Pyomo
Bethany Nicholson (bethanylnicholson@gmail.com) Sandia National 
Laboratories, USA, Victor M Zavala, John Siirola, Jean-Paul Watson, 
Lorenz T Biegler
This talk presents pyomo.dae, an open-source tool for abstracting and discretizing 
optimization problems constrained by differential equations. Our framework 
does not require differential equations to have a particular form and is able to 
represent high-order differential equations as well as partial differential equations 
on bounded rectangular domains. This provides users a new level of modeling 
flexibility that is not found in most algebraic modeling languages and 
optimization frameworks. Furthermore, pyomo.dae provides several automatic 
discretization methods for converting models containing differential equations to 
finite-dimensional algebraic models that can be solved with standard optimization 
solvers. In this talk we present several illustrative examples showing how 
pyomo.dae can be used to tackle a variety of dynamic optimization problems. We 
then describe the finite difference and collocation discretization schemes that 
have been implemented and demonstrate how simple it is to combine such 
schemes to create new non-intuitive solution strategies. Finally, we discuss the 
next steps for this tool towards developing a modeling framework for more 
advanced dynamic optimization techniques as well as exploring new modeling 
paradigms by combining pyomo.dae with other packages in Pyomo for stochastic 
or disjunctive programming.
2. New Developments in Pyomo
John D Siirola (jdsiiro@sandia.gov) Sandia National Laboratories, 
USA, William E Hart, Carl D Laird, Bethany L Nicholson, Jean-Paul 
Watson, David L Woodruff
Computational tools for modeling mathematical programs are widely used 
within both academia and industry. Available commercial and open-source 
modeling packages support generic modeling by separating modeling constructs 
from instance data through concepts like sets, parameters, and parameterized 
constraints. However, limiting models to “flat” algebraic representation forces 
the modeler to explicitly convert or relax high-level constructs, which can 
obscure much of the structure in the model. In this presentation, we will provide 
an overview of recent developments in Pyomo, an open-source library for 
modeling general algebraic optimization problems in Python. We will focus on 
the application of high-level non-algebraic modeling constructs coupled with 
automated model interrogation and transformation to improve model clarity, 
abstraction, and model validation.

■ Tue.A.5H
Tuesday, 10:30-11:45, Room 5H
Vector Optimization
Cluster: Multi-Objective and Vector Optimization
Session organized by: Andreas Loehne
1. A Set-valued Approach to Matrix Games with Vector Payoffs
Andreas Loehne (andreas.loehne@uni-jena.de) FSU Jena, Germany, 
Andreas H Hamel
Based on set relations, a new solution concept for matrix games with zero sum 
vector payoffs is introduced. It is compared with notions from the literature such 
as Pareto optimal security strategies (POSS) as well as with Shapley equilibria. 
We demonstrate by examples how to choose optimal strategies in practice and 
2016_iccopt.indb   59 2016/07/22   11:58:17
ABSTRACTS
ICCOPT 201660
we present an algorithm to compute optimal strategies.
2. Duality in Polyhedral Projection Problems
Benjamin Weissing (benjamin.weissing@unibz.it) Friedrich Schiller 
Universität, Germany
A ʻPolyhedral Projection Problemʼ (pp) is concerned with computing a 
representation of a polyhedron which is defined as the projection of another, 
high-dimensional polyhedron onto a low-dimensional real vector space. This 
problem class occurs quite naturally when ʻVector Linear Problemsʼ (vlp), i.e. 
linear problems with vector-valued objective, are considered. A solution to (vlp) 
involves a representation of all minimal points (with respect to a partial order 
induced by an ʼorder coneʼ) in the image space. Such a representation can be 
obtained by considering the ʼextended imageʼ of the (vlp). This is a polyhedron 
whose vertices are all minimal and whose extremal directions are either minimal 
or belong to the order cone. The computation of the extended image can be 
interpreted as (pp). The aim of the talk is to define the class of Polyhedral 
Projection Problems as well as an accompanying solution concept. Furthermore, 
two different kinds of dual problems will be presented. One is based on the usual 
understanding of duality in optimisation, while the second one is based on duality 
between polyhedra (also known as polarity). A further topic will be the 
relationship between (pp) and (vlp) and their respective solution concepts.
3. Optimality and Duality for Mathematical Programmes with 
Equilibrium Constraints
Shashi Kant Mishra (bhu.skmishra@gmail.com) Banaras Hindu 
University, Varanasi, India, Yogendra Pandey
We present some new results on Strong Kuhn-Tucker Optimality conditions and 
Duality results for nonsmooth Mathematical Programmes with equilibrium 
Constraints under generalized convexity assumptions. 

■ Tue.A.5I
Tuesday, 10:30-11:45, Room 5I
Geometry and Algorithms for Conic Programming
Cluster: Conic and Polynomial Optimization
Session organized by: Masakazu Muramatsu
1. Facial Reduction in MOSEK
Henrik Alsing Friberg (haf@mosek.com) MOSEK ApS, Denmark
An introduction to interleaved regularization and optimization via self-dual 
embeddings and a discussion on the relevance and practical concerns of facial 
reduction in commercial optimization software.
2. Primal-Dual Potentiai-Reduction Algorithm for Symmetric 
Programming Problem with Nonlinear Objective Function
Leonid Faybusovich (lfaybuso@gmail.com) University of Notre 
Dame, USA
We introduce a generalized Nesterov-Todd direction for the symmetric 
programming problem with a nonlinear convex objective function. We prove a 
global convergence for such an algorithm provided a certain degree of accuracy 
is attained in calculating this direction. Some applications are considered.
3. Exact Duals and Short Certificates of Infeasibility and Weak 
Infeasibility in Conic Linear Programming: Part 1
Gabor Pataki (gabor@unc.edu) University of North Carolina at Chapel 
Hill, USA, Minghui Liu
In conic linear programming — in contrast to linear programming — the Lagrange 
dual may not be a strong dual, and the corresponding Farkasʼ lemma may fail to 
prove infeasibility. Here we describe exact duals, and certificates of infeasibility 
and weak infeasibility for conic LPs which retain most of the simplicity of the 
Lagrange dual, but do not rely on any constraint qualification. Some of our exact 
duals generalize the SDP duals of Ramana, Klep and Schweighofer to the context 
of general conic LPs. Some of our infeasibility certificates generalize the row 
echelon form of a linear system of equations: they consist of a small, trivially 
infeasible subsystem obtained by elementary row operations. We prove 
analogous results for weakly infeasible systems. We obtain some fundamental 
geometric corollaries: an exact characterization of when the linear image of a 
closed convex cone is closed, and an exact characterization of nice cones. Our 
infeasibility certificates provide algorithms to generate all infeasible conic LPs 
over several important classes of cones; and all weakly infeasible SDPs in a 
natural class. Using these algorithms we generate a public domain library of 
infeasible and weakly infeasible SDPs. The status of our instances is easy to 
verify by inspection in exact arithmetic, but they turn out to be challenging for 
commercial and research codes.

■ Tue.A.5J
Tuesday, 10:30-11:45, Room 5J
Recent Advances in Splitting Methods for Large-Scale 
Convex Programming: Part I
Cluster: Convex and Nonsmooth Optimization
Session organized by: Xiaoming Yuan, Caihua Chen
1. Lattice-based Patterned Fabric Inspection by Sparse and Low-
Rank Representation
Wenxing Zhang (wxzh1984@126.com) University of Electronic 
Science and Technology of China, China, Michael Ng, Henry Ngan, 
Xiaoming Yuan
This talk presents a lattice-based patterned fabric inspection based on a 
constrained sparse and low-rank approach. Previously, an image decomposition 
based approach was attempted to tackle the problem of fabric inspection by 
decomposing patterned texture as a combination of cartoon and texture. 
However, the ID model was weak at detecting multi-color or nearly piecewise 
constant patterned fabrics. In short, it has neglected the geometric and regularity 
properties of a patterned texture. Therefore, a modified ID model proposed in 
this paper applies the classification of 17 wallpaper groups in geometry, for 
which each patterned texture can be generated by a specified lattice. This lattice 
is composed by the most fundamental unit, motif, by pre-defined symmetry rules. 
This allows every patterned texture possessing a regularity property via the 
texture generation as well as a sparse and low-rank representation in matrix 
operation. Incorporating these nice properties, fabric inspection can be 
accomplished in a decent convex optimization model using the CSL approach. In 
performance evaluation, the CSL method achieves 99.19% average accuracy on 
star- and 99.07% average accuracy on box-patterned fabrics of a total of 50 
images.
2. A Proximal Point Algorithm with Asymmetric Linear Term
Xingju Cai (caixingju@njnu.edu.cn) Nanjing Normal University, 
China
In this paper, we propose an asymmetric proximal point algorithm (APPA) for 
solving variatioal inequality problems. The algorithm is “asymmetric” in the 
sense that the matrix in the linear proximal term is not required to be a symmetric 
matrix, which makes the method more flexible, especially in dealing with 
problems with separable structures. Under some suitable conditions, we prove 
the global linear convergence of the algorithm. To make the method more 
practical, we allow the subproblem to be solved in an approximate manner and 
an inaccuracy criterion with constant parameter is adopted. Finally, we report 
some preliminary Numerical results.
3. An Alternating Minimization Algorithm for Robust Principal 
Component Analysis
Yuan Shen (ocsiban@126.com) Nanjing University of Finance and 
Economics, China, Xin Liu, Zaiwen Wen, Yin Zhang
We focus on solving the problem of Robust Principal Component Analysis 
(RPCA) arising from various applications in the fields of information theory, 
statistics, engineering, and etc. Convex optimization with nuclear norm is 
effective for small-scale RPCA problem, and the resulting algorithm can 
guarantee convergence to optimal solution. However, the nuclear norm 
minimization algorithm involves the Singular Value Decomposition (SVD), 
hence its efficiency and scalability are limited by the computational complexity 
of SVD as both matrix size and rank increase. Aftermath, nonconvex optimization 
without nuclear norm was then proposed, the resulting algorithm provides 
accelerations of computing speed up to multiple orders of magnitude for large-
scale problem, compared with the algorithms based on nuclear norm 
minimization. However, these algorithm can only guarantee convergence to a 
stationary point. We propose a new efficient algorithm. It utilizes a nonconvex 
model without nuclear norm, hence avoids the SVD. Its framework is relatively 
simple, and is almost parameter-free. Without imposing assumptions, we derive 
the global convergence to strict local optimizer. This is a great improvement 
compared with those which can only converge to stationary point. Additionally, 
the new algorithm shows satisfactory numerical performance in experiments 
with both synthetic and real data.

■ Tue.A.5L
Tuesday, 10:30-11:45, Room 5L
Notions of Robustness and Dynamics in Convex 
2016_iccopt.indb   60 2016/07/22   11:58:17
ABSTRACTS
ICCOPT 2016 61
Optimization: Part I
Cluster: Convex and Nonsmooth Optimization
Session organized by: Benjamin Recht, Pablo A Parrilo
1. An Optimal First Order Method based on Optimal Quadratic 
Averaging
Maryam Fazel (mfazel@uw.edu) University of Washington, USA, 
Dmitriy Drusvyatskiy, Scott Roy
In a recent paper, Bubeck, Lee, and Singh introduced a new first order method for 
minimizing smooth strongly convex functions. Their geometric descent 
algorithm, largely inspired by the ellipsoid method, enjoys the optimal linear rate 
of convergence. Motivated by their work, we propose a close variant that 
iteratively maintains a quadratic global under-estimator of the objective function, 
whose minimal value approaches the true minimum at an optimal rate. The 
resulting intuitive scheme comes equipped with a natural stopping criterion and 
can be numerically accelerated by using accumulated information.
2. Convergence of First-Order Algorithms for Convex Optimization 
using Inexact Information
Francois Glineur (francois.glineur@uclouvain.be) Université 
Catholique de Louvain, Belgium, Yurii Nesterov, Olivier Devolder
Analysis of first-order methods for convex optimization typically assume 
availability of the objective function and its gradient at every iterate. However, 
in many cases, only approximate information can be computed. In particular, this 
happens when the objective function itself is the optimal value of another 
auxiliary problem. We define a class of inexact oracles that extends the classical 
notion of epsilon-subgradient to the smooth (strongly) convex case. Such an 
oracle is naturally available in many situations involving inexact computations, 
including approaches involving an auxiliary problem (useful for smoothing or 
solving dual problems). We study the behavior of first-order methods using such 
an oracle. Convergence of the classical gradient method is mostly unchanged: it 
converges to a solution whose accuracy is comparable to that of the oracle. In 
contrast, the fast gradient method suffers from error accumulation, and the best 
accuracy it can reach is much worse than that of the oracle. We then propose a 
way to remedy this unsatisfactory situation: we introduce a new hybrid method 
that, given a certain oracle accuracy and a target objective accuracy unattainable 
by the fast gradient method, requires a number of steps that is much smaller that 
the classical gradient method. 
3. On Equivalence between Deterministic First-Order Optimization 
Algorithms and Martingale Inequalities
Alexander Rakhlin (rakhlin@wharton.upenn.edu) University of 
Pennsylvania, USA
In this talk, we will argue that existence of deterministic first-order optimization 
algorithms can be certified via certain probabilistic inequalities. Conversely, 
mirror descent style algorithms yield probabilistic inequalities that are otherwise 
difficult to prove. This connection between the two disparate fields facilitates the 
development of novel online learning algorithms for such problems as ranking, 
collaborative filtering, and online shortest path, to name a few.

■ Tue.B.1S
Tuesday, 13:15-14:30, Room 1S
Nonlinear Optimization Algorithms and Their 
Complexity I
Cluster: Nonlinear Optimization
Session organized by: Philippe Toint
1. Evaluation Complexity for Nonlinear Constrained Optimization 
using Unscaled KKT Conditions and High-Order Models
Sandra Augusta Santos (sandra@ime.unicamp.br) University of 
Campinas, Brazil, Ernesto G Birgin, John L Gardenghi, Jose M 
Martinez, Philippe Toint
In this work, the evaluation complexity of general nonlinear, possibly nonconvex, 
constrained optimization is analysed. Under suitable smoothness conditions, it is 
shown that an epsilon-approximate first-order critical point of the problem can 
be computed in order O(ε1 - 2(p+1)/p) evaluations of the problemʼs function and their 
first p derivatives. This is achieved by using a two-phase algorithm inspired by 
previous works of Cartis, Gould, and Toint (2011, 2013). It is also shown that 
strong guarantees (in terms of handling degeneracies) on the possible limit 
points of the sequence of iterates generated by this algorithm can be obtained at 
the cost of increased complexity. At variance with previous results, the 
ε-approximate first-order criticality is defined by satisfying a version of the KKT 
conditions with an accuracy that does not depend on the size of the Lagrange 
multipliers.
2. Limited Memory Algorithms with Cubic Regularization
Oleg Burdakov (oleg.burdakov@liu.se) Linkoping University, 
Sweden, Ya-xiang Yuan, Liang Zhao
We consider a model with a cubic regularization where the cubic term is 
determined by the eigendecomposition of a limited memory Hessian 
approximation. Although the model function may potentially have an exponential 
number of distinct local minima, its global minimizer can be obtained in closed 
form. The required eigenvalue decomposition is produced using an efficient 
approach introduced recently for limited memory Hessian approximations. 
Convergence results are presented for a standard cubic regularization framework. 
The efficiency of our algorithms is demonstrated by results of numerical 
experiments.
3. A Space Transformation Framework for Nonlinear Optimization
Zaikun Zhang (zaikunzhang@gmail.com) Hong Kong Polytechnic 
University, Hong Kong, Serge Gratton, Luis Nunes Vicente
We present a space transformation framework for nonlinear optimization. 
Instead of tackling the problem in the original space, each iteration of this 
framework seeks for a trial step by modelling and approximately solving the 
optimization problem in another space. We establish the global convergence and 
worst case iteration complexity of the framework. Then we show that the 
framework can be specialized to a parallel space decomposition framework for 
nonlinear optimization, which can be regarded as an extension of the domain 
decomposition method for PDEs. A feature of the decomposition framework is 
that it incorporates the restricted additive Schwarz methodology into the 
synchronization phase of the method. We will illustrate how this decomposition 
framework can be applied to design parallel algorithms for optimization 
problems with or without derivatives.

■ Tue.B.1A
Tuesday, 13:15-14:30, Room 1A
Nonconvex and Non-Lipschitz Optimization: 
Algorithms and Applications 2
Cluster: Nonlinear Optimization
Session organized by: Ya-Feng Liu
1. Theory and Algorithms for Sparse Finance Optimization
Feng Min Xu (fengminxu@mail.xjtu.edu.cn) Xiʼan Jiaotong 
University, China
In the practical business environment, portfolio managers often face business-
driven requirements that limit the number of constituents in their optimal port-
folio. A natural sparse finance optimization model is thus to minimize a given 
objective function while enforcing an upper bound on the number of assets in the 
portfolio. In this talk we consider three kinds of sparse finance optimization 
problem, including sparse portfolio selection, sparse index tracking and sparse 
portfolio rebalancing. In particular, we propose an efficient method for solving 
these problem. Under some suitable assumptions, we establish that any accu-
mulation point of the sequence generated by our method is a local minimizer of 
these sparse finance optimization problems. We also conduct empirical tests to 
demonstrate that our approach generally produces sparse portfolios with higher 
out-of-sample performance.
2. Optimality and Some Numerical Analysis for Constrained 
Optimization Problems with Nonconvex Regularization
Wei Bian (bianweilvse520@163.com) Harbin Institute of Technology, 
China, Xiaojun Chen
In this paper, we consider a class of constrained optimization problems where the 
feasible set is a general closed convex set and the objective function has a 
nonsmooth, nonconvex regularizer. Such regularizer includes widely used 
SCAD, MCP, logistic, fraction, hard thresholding and non-Lipschitz Lp penalties 
as special cases. Using the theory of the generalized directional derivative and 
the tangent cone, we derive a first order necessary optimality condition for local 
minimizers of the problem, and define the generalized stationary point of it. The 
new defined generalized stationary point provides a uniform definition for the 
stationary points of this kind of problem, since it is the Clarke stationary point 
when the objective function is Lipschitz continuous at this point, and equivalent 
to the existing stationary points defined for some special models when the 
objective function is not Lipschitz continuous at this point. We prove the 
consistency between the generalized directional derivative and the limit of the 
classic directional derivatives associated with the smoothing function. Moreover, 
when the regularizer is nonconvex, we study the numerical properties of this 
2016_iccopt.indb   61 2016/07/22   11:58:17
ABSTRACTS
ICCOPT 201662
problem from the bound property of its local minimizers and its computational 
complexity.

■ Tue.B.1B
Tuesday, 13:15-14:30, Room 1B
Numerical Methods for PDE-constrained Optimization 
under Uncertainty
Cluster: PDE-constrained Optimization
Session organized by: Michael Ulbrich
1. Hierarchical Tensor Approximation for Optimal Control with 
Uncertain Coefficients
Reinhold Schneider (schneidr@math.tu-berlin.de) TU Berlin, 
Germany, Benjamin Huber
Hierarchical Tucker tensor format (HT-Hackbusch tensors) and Tensor Trains 
(TT-Tyrtyshnikov tensors, I. Oseledets) have been introduced recently for low 
rank tensor product approximation. Hierarchical tensor decompositions are 
based on sub space approximation by extending the Tucker decomposition into a 
multi-level framework. Therefore they inherit favorable properties of Tucker 
tensors, e.g they offer a stable and robust approximation, but still enabling low 
order scaling with respect to the dimensions. This approach extend the reduced 
basis model order reduction to high dimensional problems. For many high 
dimensional problems, this approach offers a novel strategy to circumvent the 
course of dimensionality. For uncertainty quantification the optimal problem, 
with uncertain data problem is reformulated as a high dimensional parametric 
optimal control problem The optimization problems can be constraint further by 
the restriction to tensors of prescribed ranks r. This problem could be solved by 
optimization on manifolds or with the help of hierarchical SVD (HSVD).
2. A Second Order Approximation Technique for Robust 
Optimization in Parametrized Shape Optimization
Oliver Lass (lass@mathematik.tu-darmstadt.de) Technische 
Universität Darmstadt, Germany, Stefan Ulbrich
We consider a nonlinear constrained optimization problem with uncertain 
parameters. It is addressed by a robust worst-case formulation. The resulting 
optimization problem is of bi-level structure and is difficult to treat numerically. 
We propose an approximate robust formulation that employs a quadratic 
approximation. For an efficient realization in application problems we will mix 
the introduced framework with a linear approximation when appropriate. The 
strategy is then applied to the optimal placement of a permanent magnet in the 
rotor of a synchronous machine. The goal is to optimize the volume and position 
while maintaining a desired performance level. These quantities are computed 
from the magnetic vector potentials obtained by the magnetostatic approximation 
of Maxwellʼs equation with transient movement. We arrive at an optimization 
problem governed by a set of elliptic partial differential equations, where one 
PDE has to be solved for every rotor position. The uncertainty is introduced 
through uncertainties in material and production precision. The problem 
formulation as well as the robustification of the optimization lead to high 
computational cost that requires to investigate methods for efficient realization. 
To speed up the computation reduced order models are developed. Numerical 
results are presented to validate the presented approach.
3. Constrained Optimization with Low-Rank Tensors and 
Applications to Problems with PDEs under Uncertainty
Michael Ulbrich (mulbrich@ma.tum.de) Technical University of 
Munich, Germany, Sebastian Garreis
We present Newton-type methods for inequality constrained nonlinear 
optimization using low-rank tensors and apply them to variational inequalities 
with several uncertain parameters and to optimal control problems with PDEs 
under uncertainty. The developed methods are tailored to the usage of low-rank 
tensor arithmetics, which only offer a limited set of operations and require 
truncation (rounding) in between. We show that they can solve huge-scale 
optimization problems with trillions of unknowns to a good accuracy.

■ Tue.B.1C
Tuesday, 13:15-14:30, Room 1C
Advances in Derivative-free and Simulation-based 
Optimization I
Cluster: Derivative-free and Simulation-based Optimization
Session organized by: Francesco Rinaldi, Zaikun Zhang
Session chair: Dmitri E Kvasov
1. Nonmonotone Derivative-free Trust-Region Algorithms for 
Composite Nonsmooth Optimization
Geovani Nunes Grapiglia (geovani_mat@hotmail.com) Universidade 
Federal do Paraná, Brazil
Non-monotone derivative-free trust-region algorithms are considered for 
minimizing a composite function Φ(x) = f(x)+h(c(x)), where f and c are smooth 
and h is convex but may be non-smooth. Global convergence results and worst-
case complexity bounds are discussed. Moreover, numerical results on L1 and 
minimax problems are also presented.
2. An approach for solving Mixed Integer Nonlinear Optimization 
Problems via Derivative Free Optimization Techniques
Ubaldo M García-Palomares (ubaldo@gti.uvigo.es) Universidade de 
Vigo, Spain
In this talk we show how to adapt non monotone derivative free optimization 
techniques to the solution of the same constrained model subjected to integer 
values for some or all variables involved. Under the same well known 
convergence conditions for continuous variables, it is proved that a sequence of 
discrete quasi minimal points converges to a stationary point of a discrete 
optimization problem subjected to linear constraints, including finite or infinite 
bounds on the variables. A minor variation forces the function evaluations on 
discrete points and has the nice property that if the discretization of variables is 
somehow relinquished, the procedure may continue without disruption and it 
converges to a stationary point of the model with continuous variables. 
Convergence is preserved if nonlinear equalities and inequalities are incorporated 
as a penalty to the objective function. A variable separation technique is 
suggested for optimization problems with mixed continuous and discrete 
variables, but we emphasize that, although theoretically it is not necessary, a 
space decomposition in general improve the performance of the procedure. 
Numerical results will be reported in this talk to support this latter statement.
3. On Numerical Comparison of Deterministic and Stochastic 
Derivative-free Global Optimization Algorithms
Dmitri E Kvasov (kvadim@dimes.unical.it) University of Calabria, 
Italy, Yaroslav D Sergeyev, Marat S Mukhametzhanov
In many simulation-based applications requiring global optimization techniques, 
the objective function can be multiextremal and non-differentiable thus 
precluding the use of descending schemes with derivatives. Moreover, the 
function is often given as a black-box and, therefore, each function evaluation 
has a high cost with respect to the available computational resources. Derivative-
free methods can be particularly suitable to tackle these challenging global 
optimization problems and can be either of deterministic or stochastic nature. A 
numerical comparison of these two groups of methods is interesting for several 
reasons and has a practical importance since stochastic (and particularly, 
metaheuristic) and deterministic algorithms are used by two mostly disjoint 
communities. Some difficulties, however, arise when these methods are 
compared because of their different structure. A new methodology called 
operational zones is proposed for a reliable comparison and an intuitive 
visualization of numerical results obtained by stochastic and deterministic global 
optimization algorithms. This technique is inspired by Grishaginʼs approach of 
operational characteristics, known also as data profiles. Several widely used 
metaheuristic global optimization methods (as genetic, differential evolution, 
particle swarm optimization, artificial bee colony, and firefly algorithms) are 
thus compared with Lipschitz-based deterministic methods by using operational 
zones.

■ Tue.B.4A
Tuesday, 13:15-14:30, Room 4A
Energy Systems I
Cluster: Applications in Energy, Science and Engineering
Session organized by: Alexander W Dowling
1. Strong Valid Inequalities for the Standard Pooling Problem
Claudia DʼAmbrosio (dambrosio@lix.polytechnique.fr) CNRS & 
LIX, Ecole Polytechnique, France, Jeff Linderoth, James Luedtke, 
Jonas Schweiger
The focus of this talk will be on the standard pooling problem, i.e., a continuous, 
non-convex optimization problem arising in the chemical engineering context. 
The problem consists of finding the optimal composition of final products 
obtained by blending in pools different percentages of raw materials. Bilinear 
terms arise from the requirements on the quality of certain attributes of the final 
products. The quality is a linear combination of the attributes of the raw materials 
and intermediate products that compose the final product. Three different 
2016_iccopt.indb   62 2016/07/22   11:58:17
ABSTRACTS
ICCOPT 2016 63
classical formulations have been proposed in the literature. We start from the 
strongest of the three, i.e., the one providing a standard linear relaxation closer to 
the original problem, and propose to strengthen that linear relaxation. In 
particular, we studied a structured non-convex subset of some special cases to 
derive valid nonlinear convex inequalities that we proved to define the convex 
hull of the non-convex subset. Preliminary computational results on instances 
from the literature are reported and demonstrate the utility of the inequalities 
when used in a global optimization solver.
2. Challenges and Opportunities for Optimization-based Workflow 
in Industry 
Rui Huang (huangr@utrc.utc.com) United Technologies Research 
Center, USA
Currently buildings consume about 40% of the total US energy and it is the only 
energy end-use sector showing growth in the energy intensity through 2025. 
Moreover, 30% of energy in the building area is used inefficiently or 
unnecessarily. Under the current market condition, United Technologies 
Corporation, which is the world largest building supplier, is investing in model 
based system engineering. This talk provides an overview of the new 
developments in this area and presents the challenges and opportunities in the 
area of robust design and uncertainty analysis. This talk first shows the 
applications of system engineering that lead to economic and environment 
benefit. Then the new optimization-based workflows for system design and 
operation with additional capabilities are discussed. Moreover, challenges and 
opportunities of robust design, uncertainty quantification and the associated 
computational problems will be presented. 
3. A Stochastic Programming Framework for Multi-Stakeholder 
Decision-making and Conflict Resolution
Alexander W Dowling (adowling2@wisc.edu) University of 
Wisconsin-Madison, USA, Victor M Zavala
Engineering decision-making is inherently multiobjective and involves multiple 
stakeholders. As such, it suffers from ambiguity and dimensionality. We propose 
a decision-making framework to compute compromise solutions that balance 
conflicting priorities of multiple stakeholders. In our setting, we shape the 
stakeholder dissatisfaction distribution by solving a conditional-value-at-risk 
(CVaR) minimization problem. The CVaR problem is parameterized by a 
probability level that shapes the tail of the dissatisfaction distribution. The 
proposed approach allows us to compute a family of compromise solutions and 
generalizes multi-stakeholder settings previously proposed in the literature. We 
use the concept of the CVaR norm to give a geometric interpretation to this 
problem and use the properties of this norm to prove that the CVaR minimization 
problem yields Pareto optimal solutions for any choice of the probability level. 
We discuss a broad range of potential applications that involve complex decision-
making processes. We demonstrate the developments by balancing stakeholder 
priorities on transportation, safety, water quality, and capital costs in a biowaste 
facility location case study. We also discuss extensions to energy system design 
and operation, such as combined cooling, heat and power systems, and solar 
power generators with energy storage.

■ Tue.B.4B
Tuesday, 13:15-14:30, Room 4B
Asset-Liability Management
Cluster: Applications in Finance and Economics
Session organized by: Woo Chang Kim
1. Extending the Scope of ALM to Social Investment — Investing in 
Population Growth to Enhance Sustainability of Korea National 
Pension Service
Woong Bee Choi (cwbee@kaist.ac.kr) KAIST, Korea, Min Jeong 
Kim, Woo Chang Kim
Currently, Koreaʼs National Pension Plan has been hugely accumulated; in 
particular, it is the third largest public pension in the world. According to its 
financial projection from 2013, the accumulated amount of the National Pension 
is expected to reach the highest amount as much as 50% of the nationʼs GDP by 
2043. However, many predict that this immense fund will become exhausted by 
2060 due to the aging population and the low fertility rate. In this research, we 
develop an optimization model to calculate the effect of the investment for 
raising the fertility rate. In addition, by using the asset-liability management 
model, we examine whether the investment for raising the fertility rate improves 
the sustainability of the National Pension Fund. As a result, under some specific 
conditions, it is shown that the investment for raising the fertility rate enhances 
the sustainability of the Nation Pension Fund and postpones its exhaustion. Thus, 
we show that socially driven investment can also be a good investment asset in 
which the National Pension Fund should consider to invest.
2. The Peculiarity of Liability of National Pension in Korea and the 
Way to Sustain Pension Scheme
Chong Hyun Won (moimoi1@empal.com) Korea National Assembly 
Research Service, Korea
The Korea national pension service investment management(NPSIM) has 
evolved with reserve of 512 trillion KRW. In Korea national pension fund 
scheme, funding ratio is below 0.5. This fund shall be exhausted around 2060 
subject to not change pension scheme. As a public fund for old-age income 
security, the national pension is belong to both funded scheme and also PAYG 
scheme. It means that the liability of the pension is not same that of other private 
pension. Because existence of national pension system is to guarantee stable 
pension benefit for planholder, the target of the national pension fund should be 
to improve stability of system, which helps support the generational contract. 
Thatʼs why the purpose of the public pension fund is not to raise the profit rate of 
management. It is needed to accept that the national pension system can not be 
preserved in that property and profits applying now in national pension. It is 
required the management of pension fund which emphasizes the stability and 
appropriate profits rather than that of pursuit of high rate risky asset weight 
strongly. Therefore, It is needed to discussion about pension fund management 
policy, which is different from government pension fund management policy.
3. Personalized Asset-Liability Management Service: Products, 
Markets, Regulations and Technologies
Woo Chang Kim (wkim@kaist.ac.kr) KAIST, Korea
ALM has been employed almost exclusively for institutional investors. Recently, 
however, with the rise of automated investment management service, called robo 
advisors, personalized ALM services for individuals are getting available 
especially for retirement planning. In this talk, I will discuss various issues 
including products, markets, regulations, and financial optimization technologies 
required for this service.

■ Tue.B.5A
Tuesday, 13:15-14:30, Room 5A
Bilevel Optimization: Theory and Solution Methods
Cluster: Multi-Objective and Vector Optimization
Session organized by: Alain Zemkoho
1. Solution Algorithm for Optimistic Bilevel Optimization Problems
Stephan Dempe (dempe@tu-freiberg.de) TU Bergakademie Freiberg, 
Germany
In bilevel optimization, an objective function is minimized subject to the graph 
of the solution set mapping of a second, parametric optimization problem. If this 
second or lower level problem is replaced using the Karush-Kuhn-Tucker 
conditions a mathematical program with complementarity constraints (MPCC) 
arises. Both problems are equivalent if global optima are considered. Algorithms 
solving MPCCs compute stationary solutions in general. For computing local 
optimal solutions of the bilevel optimization problem we need to consider all 
Largrange multipliers of the lower level problem or to use carefully adapted 
algorithms. In the talk one such algorithm is presented, its convergence to a 
global or a local optimal solution of the bilevel optimization problem is 
investigated.
2. Newton Method for Bilevel Optimization
Alain Zemkoho (a.b.zemkoho@soton.ac.uk) University of 
Southampton, United Kingdom
We consider a bilevel optimization problem where the lower level problem 
admits more than one optimal solution for some parameters from the upper level. 
Considering the lower level value function (LLVF) reformulation, there are two 
specific classes of stationarity conditions based on the expression of the 
subdifferential of the LLVF. In this talk, we discuss an auxiliary system of 
equations based on some vector-valued map, which allows us to compute the 
aforementioned stationary points by means of a semismooth Newton method. 
Convergence properties of the method and preliminary numerical experience 
will also be discussed. 
3. Stationarity Concepts for Bilevel Optimization Problems with 
Lower Level Constraints in Lebesgue Spaces
Patrick Mehlitz (mehlitz@mailserver.tu-freiberg.de) Technical 
University Bergakademie Freiberg, Germany, Gerd Wachsmuth
We derive necessary optimality conditions for a bilevel programming problem 
whose lower level possesses constraints comprising the cone of nonnegative 
functions in a Lebesgue space. This formulation covers bilevel optimal control 
2016_iccopt.indb   63 2016/07/22   11:58:17
ABSTRACTS
ICCOPT 201664
problems with lower level inequality constraints on the control function. Using 
lower level optimality conditions, we transfer the bilevel programming problem 
into a complementarity constrained single level program. Afterwards, recently 
introduced stationarity notions and constraint qualifications for MPCCs in 
Banach spaces are applied to the surrogate problem. A surprising observation 
regarding Mordukhovichʼs stationarity concept is discussed in more detail. The 
talk is based on a joint work with Gerd Wachsmuth.

■ Tue.B.5D
Tuesday, 13:15-14:30, Room 5D
Theoretical Advances in Linear Optimization — 
Sampling Methods
Cluster: Linear Optimization
Session organized by: Aaron Daniel Sidford, Yin Tat Lee
1. Randomized Interior Point Methods for Sampling and 
Optimization
Hariharan Narayanan (harin@uw.edu) University of Washington, 
USA, Ravi Kannan
We present a Markov Chain, “Dikin walk”, for sampling from a convex body 
equipped with a self-concordant barrier. This Markov Chain corresponds to a 
natural random walk with respect to a Riemannian metric defined using the 
Hessian of the barrier function. For every convex set of dimension n, there exists 
a self-concordant barrier whose self-concordance parameter is O(n). 
Consequently, a rapidly mixing Markov Chain of the kind we describe can be 
defined (but not always be efficiently implemented) on any convex set. We use 
these results to design an algorithm consisting of a single random walk for 
optimizing a linear function on a convex set. This talk includes joint work with 
Ravi Kannan.
2. Geodesic Gliding and Polytope Sampling
Santosh S Vempala (vempala@gatech.edu) Georgia Institute of 
Technology, USA, Yin Tat Lee
We analyze a random walk on a Riemannian manifold defined by following 
geodesics (shortest path curves) of the manifold. We prove its convergence for 
Riemannian metrics induced by Hessians of convex functions. As a consequence, 
we obtain the first sampling process with sub-quadratic mixing time for sampling 
polytopes in R^n. Our process can be viewed as a discrete-time simulation of a 
stochastic differential equation with a drift term. Its implementation is based on 
the efficient solution of first-order ordinary differential equations. Our results 
demonstrate that polytope sampling is more natural (and more efficient) in a 
non-Euclidean space. The proofs draw on Riemannian geometry, stochastic 
calculus, complex analysis and linear algebra. This is joint work with Yin Tat Lee 
(MIT).
3. Faster Convex Optimization: Simulated Annealing with an 
Efficient Universal Barrier
Jacob Abernethy (jabernet@umich.edu) University of Michigan, USA, 
Elad Hazan
This paper explores a surprising equivalence between two seemingly-distinct 
convex optimization methods. We show that simulated annealing, a well-studied 
random walk algorithms, is directly equivalent, in a certain sense, to the central 
path interior point algorithm for the the entropic universal barrier function. This 
connection exhibits several benefits. First, we are able improve the state of the art 
time complexity for convex optimization under the membership oracle model. 
We improve the analysis of the randomized algorithm of Kalai and Vempala by 
utilizing tools developed by Nesterov and Nemirovskii that underly the central 
path following interior point algorithm. We are able to tighten the temperature 
schedule for simulated annealing which gives an improved running time, 
reducing by square root of the dimension in certain instances. Second, we get an 
efficient randomized interior point method with an efficiently computable 
universal barrier for any convex set described by a membership oracle. 
Previously, efficiently computable barriers were known only for particular 
convex sets.

■ Tue.B.5E
Tuesday, 13:15-14:30, Room 5E
Robust Optimization: Theory and Applications
Cluster: Robust Optimization
Session organized by: Vineet Goyal
1. Robust Wait Time Estimation in Resource Allocation Systems with 
an Application to Kidney Allocation
Phebe Vayanos (phebe.vayanos@usc.edu) University of Southern 
California, USA, Chaithanya Bandi, Nikolaos Trichakis
In this paper we study systems that allocate different types of scarce resources to 
heterogeneous allocatees based on pre-determined priority rules. We tackle the 
problem of estimating the wait time of an allocatee who possesses incomplete 
system information, for example, with regard to his relative priority, other 
allocateesʼ preferences, and resource availability. We model the system as a 
multiclass, multiserver queuing system that is potentially unstable or in transient 
regime. We propose a novel robust optimization solution methodology that 
builds on the assignment problem. For first-come, first-served systems, our 
approach yields a mixed-integer programming formulation. For the important 
case where there is a hierarchy in the resource types, we strengthen our 
formulation through a drastic variable reduction and also propose a highly 
scalable heuristic, involving only the solution of a convex optimization problem. 
We back the heuristic with a tight approximation guarantee. We illustrate the 
generalizability of our approach by studying systems that operate under different 
priority rules. We showcase how our methodology can be applied to assist 
patients in the U.S. deceased-donor kidney waitlist. We calibrate our model 
using detailed historical data to estimate patient wait times based on their 
preferences and characteristics.
2. Satisficing Awakens: Models to Mitigate Uncertainty
Melvyn Sim (melvynsim@nus.edu.sg) National University of 
Singapore, Singapore
Satisficing, as an approach to decision-making under uncertainty, aims at 
achieving solutions that satisfy the problemʼs constraints as well as possible. We 
then study the key features of satisficing decision making that are associated 
with these problems and provide the complete functional characterization of a 
satisficing decision criterion. As a consequence, we are able to provide the most 
general framework of a satisficing model, termed the S-model, which seeks to 
maximize a satisficing decision criterion in its objective, and the corresponding 
satisficing-constrained optimization problem that generalizes robust optimization 
and chance-constrained optimization problems. Next, we focus on a tractable 
probabilistic S-model, termed the T-model whose objective is a lower bound of 
the P-model. We show that when probability densities of the uncertainties are 
log-concave, the T-model can admit a tractable concave objective function. In the 
case of discrete probability distributions, the T-model is a linear mixed integer 
program of moderate dimensions. We also show how the T-model can be 
extended to multi-stage decision-making and present the conditions under which 
the problem is computationally tractable. 
3. Piecewise Affine Policies for Two-Stage Robust Optimization 
under Demand Uncertainty
Vineet Goyal (vg2277@columbia.edu) Columbia University, USA, 
Aharon Ben-Tal, Omar El Housni, Brian Lu
We consider the problem of designing good piecewise affine policies for two-
stage adjustable robust linear optimization problems under right hand side 
uncertainty. Such problems arise in many applications where we need to satisfy 
an uncertain demand with minimum possible cost. It is well known that a 
piecewise affine policy is optimal although the number of pieces can be 
exponentially many. One of the significant challenges in designing a piecewise 
affine policy arises from constructing good pieces of the uncertainty set. We 
introduce a new framework for constructing piecewise affine policies where we 
“approximate” the uncertainty set by a simplex and construct a piecewise affine 
policy based on the map from the uncertainty set to the simplex. Our piecewise 
affine policy has exponentially many pieces but can be computed efficiently and 
in many cases, even more efficiently than computing the optimal affine policy. 
Furthermore, the performance of our piecewise affine policy is significantly 
better than the affine policy.

■ Tue.B.5G
Tuesday, 13:15-14:30, Room 5G
Parallel Implementations and Algorithms for 
Continuous Optimization
Cluster: Optimization Implementations and Software
Session organized by: Carl Laird
1. A Parallel Nonlinear Interior-Point Approach for Dynamic 
Optimization Problems 
Jose Santiago Rodriguez (rodri324@purdue.edu) Purdue University, 
Colombia, Carl Laird
Large-scale optimization plays an important role in a variety of areas, including 
2016_iccopt.indb   64 2016/07/22   11:58:17
ABSTRACTS
ICCOPT 2016 65
process design, operation and control. Dynamic optimization problems that 
commonly arise in these areas need to be solved efficiently. These problems are 
characterized by having a large number of equations and variables and can 
become computationally prohibited. However, due to the nature of the problems, 
these problems have a structure that can be exploited using parallel computing 
capabilities offered by modern computers. We present a parallel interior-point 
solver that can exploit the structure of dynamic optimization problems and 
enable efficient solution in parallel. Interior-point methods have proven to be 
effective for large-scale nonlinear programming problems. The dominant 
computational costs are the solution of the KKT system in every iteration of the 
interior-point algorithm, and computation of NLP functions and derivatives. The 
simultaneous discretization approach for dynamic optimization problems 
induces structure in the optimization problem, and our implementation exploits 
this structure with a Schur-complement decomposition strategy to enable 
efficient solution of the KKT system in parallel. In the algorithm, all scale-
dependent operations are parallelized, including model evaluation. We 
demonstrate the performance of this algorithm using an online estimation case 
study.
2. Parallel Scenario-based Decomposition Methods for Solving the 
Contingency-constrained AC Optimal Power Flow Problem
Jean-Paul Watson (jwatson@sandia.gov) Sandia National 
Laboratories, USA, Carl Laird, David Woodruff
We analyze the performance of scenario-based decomposition methods for 
solving the contingency-constrained AC optimal power flow problem, 
specifically progressive hedging, and analyze performance on a number of IEEE 
benchmark systems. 
3. The Rectangular Maximum Agreement Problem
Ai Kagawa (ai.kagawa@gmail.com) Rutgers University, USA, 
Jonathan Eckstein, Noam Goldberg
The NP-hard rectangular maximum agreement (RMA) problem finds a “box” 
that best discriminates between two weighted datasets. Its data analysis 
applications include boosting classification methods and boosted regularized 
regression. We describe a specialized parallel branch-and-bound method for 
RMA.

■ Tue.B.5H
Tuesday, 13:15-14:30, Room 5H
Robust Optimization and Applied Probability
Cluster: Robust Optimization
Session organized by: Yongpei Guan
1. Buffered Probability of Exceedance, A New Characterization of 
Uncertainty and Application to Support Vector Machines and 
Robust Optimization
Matthew David Norton (mdnorton@ufl.edu) University of Florida, 
USA, Alexander Mafusalov, Stan Uryasev
We first present a recently developed characterization of uncertainty called 
Buffered Probability of Exceedance (bPOE) and discuss its benefits as a tool to 
tackle optimization problems involving a probabilistic objective function with 
convex and sometimes even linear programming. We then demonstrate the 
benefits of utilizing bPOE by applying it to machine learning tasks. We show that 
bPOE is intimately connected to the popular Support Vector Machine 
classification algorithm. Specifically, we show that the SVM algorithm is 
equivalent to simple bPOE minimization. Furthermore, we furnish this 
equivalence with new Robust Optimization formulations having both a non-
convex risk seeking and convex risk averse case. Overall, we show that Robust 
bPOE minimization provides a fruitful approach to the classification task.
2. Applications of the Earth Moverʼs Distance in Optimization
Ye Wang (wang141@usc.edu) University of Southern California, 
USA, Medhi Behroozi, John Gunnar Carlsson
Earth moverʼs distance (also known as the Wasserstein or Kantorovich metric) is 
a statistical metric that describes a distance function between two probability 
distributions. In our research, we consider the Entropy maximization problem 
and Highest Posterior Density optimization problem in which we will search 
through all distributions whose earth moverʼs distance to the empirical 
distribution of a given set of data points is sufficiently small. We also show how 
to use the Earth Moverʼs Distance to study the distributionally robust travelling 
salesman problem in the Euclidean plane.

■ Tue.B.5I
Tuesday, 13:15-14:30, Room 5I
Geometry, Duality and Complexity in Conic Linear 
Programming I
Cluster: Conic and Polynomial Optimization
Session organized by: Gabor Pataki
1. Exact Duals and Short Certificates of Infeasibility and Weak 
Infeasibility in Conic Linear Programming: Part 2
Minghui Liu (liu.m.h2010@gmail.com) University of North Carolina 
at Chapel Hill, USA, Gabor Pataki
In conic linear programming – in contrast to linear programming – the Lagrange 
dual may not be a strong dual, and the corresponding Farkasʼ lemma may fail to 
prove infeasibility. Here we describe exact duals, and certificates of infeasibility 
and weak infeasibility for conic LPs which retain most of the simplicity of the 
Lagrange dual, but do not rely on any constraint qualification. Some of our exact 
duals generalize the SDP duals of Ramana, Klep and Schweighofer to the context 
of general conic LPs. Some of our infeasibility certificates generalize the row 
echelon form of a linear system of equations: they consist of a small, trivially 
infeasible subsystem obtained by elementary row operations. We prove 
analogous results for weakly infeasible systems. We obtain some fundamental 
geometric corollaries: an exact characterization of when the linear image of a 
closed convex cone is closed, and an exact characterization of nice cones. Our 
infeasibility certificates provide algorithms to generate all infeasible conic LPs 
over several important classes of cones; and all weakly infeasible SDPs in a 
natural class. Using these algorithms we generate a public domain library of 
infeasible and weakly infeasible SDPs. The status of our instances is easy to 
verify by inspection in exact arithmetic, but they turn out to be challenging for 
commercial and research codes.
2. Preprocessing Semidefinite Programs
Preston Elliott Faulk (preston.faulk@unc.edu) University of North 
Carolina at Chapel Hill, USA, Gabor Pataki, Quoc Tran Dinh
We present a simple preprocessing algorithm for SDPs, and present numerical 
results.
3. Solving SDP Completely with an Interior-Point Oracle
Takashi Tsuchiya (tsuchiya@grips.ac.jp) National Graduate Institute 
for Policy Studies, Japan, Bruno Figueria Lourenço, Masakazu 
Muramatsu
Consider an oracle capable of solving any semidefinite program (SDP) which is 
interior-feasible on both primal and dual sides. This oracle is an idealization of 
primal-dual interior-point algorithm. In this talk, we show how one can use such 
an oracle to “completely solve” an arbitrary SDP. Here we use the term 
“completely solve” an SDP to mean a scheme which works in the following way; 
given an SDP, the scheme determines whether it is feasible or not, and whenever 
feasible, computes its optimal value, and if the optimal value is attained, obtains 
a maximal rank optimal solution. If the original SDP is infeasible, the scheme 
distinguishes strong and weak infeasibility. In the case of unattained optimal 
value and weak infeasibility, a set of generating matrices can be obtained from 
which we can compute an approximate optimal solution of arbitrary precision 
(unattained optimal value case) and an almost feasible solution of arbitrary 
precision (weakly infeasible case), respectively, just with a polynomially 
bounded number of arithmetic operations in n without further solving SDPs, 
where n is the size of the matrix in SDP. We show that the number of oracle calls 
in this scheme is O(n).

■ Tue.B.5J
Tuesday, 13:15-14:30, Room 5J
Recent Advances in Splitting Methods for Large-Scale 
Convex Programming: Part II
Cluster: Convex and Nonsmooth Optimization
Session organized by: Xiaoming Yuan, Caihua Chen
Session chair: WenYi Tian
1. Faster Alternating Direction Method of Multipliers with an O(1/
n2) Convergence Rate
WenYi Tian (wenyi.tian@tju.edu.cn) Tianjin University, China, 
Xiaoming Yuan
The alternating direction method of multipliers (ADMM) has found many 
applications for solving convex programming models with separable structures. 
ADMMʼs worst-case O(1/n) convergence rate measured by the iteration 
2016_iccopt.indb   65 2016/07/22   11:58:17
ABSTRACTS
ICCOPT 201666
complexity has been established in both the ergodic and nonergodic senses, 
where n is the iteration counter. However, its faster O(1/n2) convergence rate can 
only be established for some very special cases such as when both function 
components in the objective of the model are strongly convex. The O(1/n2) 
convergence rate of ADMM for the general situation remains open. Inspired by 
a recent work of Chambolle and Pock, we propose a rule for iteratively choosing 
the penalty parameter and show that ADMM using this rule has an O(1/n2) 
worst-case convergence rate for the general scenario under mild assumptions 
without strong convexity assumption on the objective function.

■ Tue.B.5L
Tuesday, 13:15-14:30, Room 5L
Notions of Robustness and Dynamics in Convex 
Optimization: Part II
Cluster: Convex and Nonsmooth Optimization
Session organized by: Benjamin Recht, Pablo A Parrilo
1. Automating the Analysis and Design of Large-Scale Optimization 
Algorithms
Laurent Lessard (laurent.lessard@wisc.edu) University of Wisconsin-
Madison, USA, Benjamin Recht, Andrew Packard
First-order iterative algorithms such as gradient descent, fast/accelerated 
methods, and operator-splitting methods such as ADMM can be viewed as 
discrete-time dynamical systems. We will show that if the function being 
optimized is strongly convex, for example, computing the worst-case performance 
of a particular algorithm is equivalent to solving a robust control problem. This 
amounts to establishing feasibility of a small semidefinite program whose size is 
independent of the dimension of the functionʼs domain. Our unified approach 
allows for the efficient and automatic evaluation of worst-case performance 
bounds for a wide variety of popular algorithms. The bounds derived in this 
manner either match or improve upon the best known bounds from the literature. 
Finally, our framework can be used to search for algorithms that meet desired 
performance specifications, thus establishing a new and principled methodology 
for designing new algorithms.
2. Stability as the Master Force Behind Stochastic Gradient Descent
Nathan Srebro (nati@ttic.edu) Toyota Technological Institute at 
Chicago, USA
We will show how stability can be used to derive convergence guarantees for 
stochastic optimization.  In particular, we will motivate and derive stochastic 
gradient descent and stochastic mirror descent as stability-inducing methods, and 
show how to obtain their familiar convergence guarantees using stability.  We 
will also discuss more aggressive variants that can be derived in a similar 
fashion.
3. Stochastic Robustness of Gradient Methods
Benjamin Recht (brecht@berkeley.edu) University of California, 
Berkeley, USA
This talk will explore the stability and robustness admitted by the gradient 
method. I will first discuss how the gradient method is robust to perturbations of 
the model and the updates. From a computing systems perspective, this 
robustness enables parallel implementations. I will also show how the gradient 
method is robust to perturbations of the loss function itself, and discuss 
implications for machine learning. I will conclude with a discussion of other 
notions from robust control theory may yield new insights into the design and 
analysis of optimization algorithms.

■ Tue.C.1S
Tuesday, 14:45-16:00, Room 1S
Optimization in Machine Learning I
Cluster: Nonlinear Optimization
Session organized by: Joshua Griffin, Wenwen Zhou
1. Combining Information from Second-Order Solvers and SGD
Scott R Pope (scott.pope@sas.com) SAS Institute Inc., USA
In this talk, we explore ways to combine information from second order solvers 
and SGD to increase the performance of either when training weights in artificial 
neural networks. We look at ways to share information between solvers and 
various strategies to avoid false convergence from poorly chosen hyper-
parameters. This is done in parallel computing environments with both shared 
and distributed memory.
2. A Modified Conjugate Gradient Method with Warm-Starts for 
Large-Scale Nonconvex Optimization Problems
Wenwen Zhou (Wenwen.Zhou@sas.com) SAS Institute Inc., USA, 
Joshua Griffin
This talk will focus on solving large-scale nonconvex unconstrained optimization 
problems with Krylov-based iterative methods when effective preconditioning 
matrices are unknown or unavailable. For such problems, convergence of the 
outer iterations can degrade when the iterative solver repeatedly exits on 
maximum Hessian-vector products rather than relative residual error . To address 
this issue, a new warm start strategy is proposed to accelerate an existing 
modified conjugate gradient approach while maintain important convergence 
properties. Numerical experience for real-life applications and addition to 
convergence results will be provided.

■ Tue.C.1A
Tuesday, 14:45-16:00, Room 1A
Nonconvex and Non-Lipschitz Optimization: 
Algorithms and Applications 3
Cluster: Nonlinear Optimization
Session organized by: Ya-Feng Liu
1. Structured Nonconvex Optimization Models: Algorithms and 
Iteration Complexity Analysis
Bo Jiang (isyebojiang@163.com) Shanghai University of Finance and 
Economics, China, Tianyi Lin, Shiqian Ma, Shuzhong Zhang
In this paper, we propose several first-order algorithms for computing an 
ε-stationary point for some structured nonconvex problems. When there is no 
linear coupled constraints, we propose a generalized conditional gradient (GCG) 
which can find an ε-stationary point in O(ε-q) iterations under certain conditions, 
where q is a parameter in the Holderian condition that characterizes the degree of 
smoothness of the objective function. Moreover, when the smooth part in the 
objective is concave, the iteration complexity can be improved to O(ε-1). For the 
more generic problem with affine constraints, we propose two algorithms (named 
as GADM and LADM) that both can be viewed as variants of the alternating 
direction method of multipliers (ADMM). We prove the O(ε-2) iteration 
complexity of the proposed GADM and LADM under some mild conditions, and 
we do not need to assume the Kurdyka-Lojasiewicz property.
2. Numerical Algorithms for PDE-constrained Optimization with 
Non-convex Non-smooth Objectives
Yun Shi (15900164r@connect.polyu.hk) The Hong Kong Polytechnic 
University, Hong Kong, Yun Shi
We consider a PDE-constrained optimization problem with non-Lipschitz 
objective. We perform a specially designed discretization on the problem before 
examining the optimality conditions and then apply the optimization algorithms 
with smoothing techniques. The convergence results of the algorithm is 
investigated and the influences of the discrtization and smoothing is discussed. 
Variational discretization with a Petrov-Galerkin scheme and Crank-Nicolson 
time stepping is used, which was proved optimal a priori error bounds under 
certain Ansatz, and the smoothing quadratic regularization method is the choice 
of optimization algorithm. 
3. Composite Lq(0 < q < 1) Minimization over Polyhedron
Ya-Feng Liu (yafliu@lsec.cc.ac.cn) Academy of Mathematics and 
Systems Science/Chinese Academy of Sciences, China, Shiqian Ma, 
Yu-Hong Dai, Shuzhong Zhang
The composite Lq(0 < q < 1) minimization problem over a general polyhedron 
has received various applications in machine learning, wireless communications, 
image restoration, signal reconstruction, etc. In this talk, we shall present a 
theoretical study on this problem. Firstly, we show that for any fixed 0 < q < 1, 
finding the global minimizer of the problem, even its unconstrained counterpart, 
is strongly NP-hard. Secondly, we derive Karush-Kuhn-Tucker (KKT) optimality 
conditions for local minimizers of the problem. Thirdly, we propose a smoothing 
sequential quadratic programming framework for solving this problem. The 
framework requires a (approximate) solution of a convex quadratic program at 
each iteration. Finally, we analyze the worst-case iteration complexity of the 
framework for returning an ε-KKT point; i.e., a feasible point that satisfies a 
perturbed version of the derived KKT optimality conditions. To the best of our 
knowledge, the proposed framework is the first one with a worst-case iteration 
complexity guarantee for solving composite Lq minimization over a general 
polyhedron.

2016_iccopt.indb   66 2016/07/22   11:58:18
ABSTRACTS
ICCOPT 2016 67
■ Tue.C.1B
Tuesday, 14:45-16:00, Room 1B
Optimal Control of Coupled Systems
Cluster: PDE-constrained Optimization
Session organized by: Roland Herzog
1. Optimal Control of a Coupled System of a Vehicle Transporting a 
Fluid Subject to Shallow Water Equations
Sven-Joachim Kimmerle (sven-joachim.kimmerle@unibw.de) 
Universitaet der Bundeswehr Muenchen, Germany, Matthias Gerdts
We consider the optimal control of a vehicle transporting an open fluid basin as 
load. The motion of the fluid is modelled by the non-linear hyperbolic shallow 
water (Saint-Venant) equations while the vehicle dynamics are described by 
Newtonʼs equations of motion. The fluid basin is mounted to the vehicle by a 
spring-damper element. The system may be controlled by the acceleration of the 
vehicle. This leads to an optimal control problem with a coupled system of 
partial differential equations (PDEs) and ordinary differential equations (ODEs). 
The PDEs and ODEs are fully coupled by boundary conditions and force terms. 
We derive necessary optimality conditions rigorously and solve by a first-
optimize-then-discretize approach, using a globalized semi-smooth Newton 
method in Hilbert spaces. As well we consider a first-discretize-then-optimize 
approach for the particular case of time optimal control. The Saint-Venant 
equations are discretized by means of a Lax-Friedrich scheme, involving an 
artificial viscosity. Finally, we discuss further examples for fully coupled systems 
of PDEs and ODEs and their particular features and classify our problem within 
this context.
2. Optimal Control of Thermoviscoplasticity
Ailyn Stötzner (ailyn.stoetzner@mathematik.tu-chemnitz.de) 
Technische Universität Chemnitz, Germany, Roland Herzog, Christian 
Meyer
Elastoplastic deformations play a tremendous role in industrial forming. Many of 
these processes happen at non-isothermal conditions. Therefore, the optimization 
of such problems is of interest not only mathematically but also for applications. 
In this talk we will present the analysis of the existence of a global solution of an 
optimal control problem governed by a thermovisco(elasto)plastic model. We 
will point out the difficulties arising from the nonlinear coupling of the heat 
equation with the mechanical part of the model. Finally, we will discuss first 
steps to develop an efficient optimization method based on the directional 
differentiability of the control-to-state mapping.
3. Optimal Control of Scalar Transport in Incompressible Fluid 
Flow
Cedric Sehrt (sehrt@mathematik.tu-darmstadt.de) TU Darmstadt, 
Germany, Stefan Ulbrich
We consider an optimal control problem given by the instationary incompressible 
Navier-Stokes equations and two coupled non-autonomous scalar advection-
diffusion equations, which are driven by the bulk motion of the fluid. This kind 
of PDE system arises in modeling combustion processes with Flamelet Generated 
Manifolds, where a small number of scalar quantities, following advection-
diffusion equations, are used to describe combustion processes. Typically, one 
scalar quantity represents the mixture of fuel and oxidizer and a second scalar 
quantity represents the progress of the combustion. The transport equation for 
the progress variable includes a nonlinear source term depending on the scalar 
quantities. The considered class of objective functionals includes in particular 
tracking-type functionals. The control acts on a subset of the domain. The 
optimal control problem is complemented with control constraints. We will show 
that the objective functional is Frechét-differentiable with respect to the control. 
Moreover, we will derive optimality conditions using the adjoint state.

■ Tue.C.1C
Tuesday, 14:45-16:00, Room 1C
Randomized Methods and Stochastic Problems
Cluster: Derivative-free and Simulation-based Optimization
Session organized by: Francesco Rinaldi, Zaikun Zhang
Session chair: Raghu Pasupathy
1. Gradient-based Stochastic Search for Simulation Optimization
Enlu Zhou (enlu.zhou@isye.gatech.edu) Georgia Institute of 
technology, USA, Shalabh Bhartnagar
A variety of systems arising in finance, engineering design, and manufacturing 
require the use of optimization techniques to improve their performance. Due to 
the complexity and stochastic dynamics of such systems, their performance 
evaluation frequently requires computer simulation, which however often lacks 
structure needed by classical optimization methods. We developed a gradient-
based stochastic search approach, based on the idea of converting the original 
(structure-lacking) problem to a differentiable optimization problem on the 
parameter space of a sampling distribution that guides the search. A two-
timescale updating scheme is further studied and incorporated to improve the 
algorithm efficiency. Convergence properties of our approach are established 
through techniques from stochastic approximation, and the performance of our 
algorithms is illustrated in comparison with some state-of-the-art simulation 
optimization methods. 
2. AUC Maximization and Tuning Parameters of Cost Sensitive 
Logistic Regression via Derivative Free Optimization
Hiva Ghanbari (hig213@lehigh.edu) Lehigh University, USA, Katya 
Scheinberg
Conventional classification problems in machine learning suffer from imbalanced 
data sets. In order to prevent the dominating effect of the majority class, AUC 
maximization rather than error rate minimization is preferred. In this work, we 
have used trust region based derivative free optimization to directly optimize the 
AUC function. We also propose a rigorous optimization scheme to maximize the 
AUC function over the class weights and the regularization term in cost sensitive 
logistic regression.
3. Adaptive Sampling Recursions for Simulation Optimization
Raghu Pasupathy (pasupath@purdue.edu) Purdue University, USA
For roughly six decades since the seminal paper of Robbins and Monro (1951), 
Stochastic Approximation has dominated the landscape of algorithms for solving 
root finding and optimization problems with Monte Carlo observable functions. 
Recently, however, inspired by the rise in parallel computing and advances in 
nonlinear programming methods, there has been increasing interest in alternative 
sampling-based frameworks. Such frameworks are convenient in that they 
(could) use an existing recursive method, e.g., line-search or trust-region, with 
embedded Monte Carlo estimators of objects appearing within the recursion. In 
this talk, after reviewing existing results on optimal sampling rates, we consider 
the question of how to adaptively sample within stochastic recursions. 
Specifically, we will demonstrate that a simple adaptive scheme that has deep 
connections to proportional-width sequential confidence intervals endows 
stochastic recursions with convergence rates that are arbitrarily close to being 
optimal, while remaining practical enough for good finite-time implementation. 
Two illustrative recursions that embed line-search and a fixed step size will be 
presented. The adaptive sampling schemes we advertise were independently 
discovered by Byrd, Chin and Nocedal, but from the viewpoint of the need to 
estimate descent directions within such algorithms.

■ Tue.C.4A
Tuesday, 14:45-16:00, Room 4A
Energy Systems II
Cluster: Applications in Energy, Science and Engineering
Session organized by:  Francois Gilbert
1. A Strong Semidefinite Programming Relaxation of the Unit 
Commitment Problem
Morteza Ashraphijuo (ashraphijuo@berkeley.edu) University of 
California, Berkeley, USA, Javad Lavaei, Alper Atamturk
The unit commitment (UC) problem aims to find an optimal schedule of 
generating units subject to the demand and operating constraints for an electricity 
grid. The majority of existing algorithms for the UC problem rely on solving a 
series of convex relaxations by means of branch-and-bound or cutting planning 
methods. In this work, we develop a strengthened semidefinite program (SDP). 
This approach is based on first deriving certain valid quadratic constraints and 
then relaxing them to linear matrix inequalities. These valid inequalities are 
obtained by the multiplication of the linear constraints of the UC problem. The 
performance of the proposed convex relaxation is evaluated on several hard 
instances of the UC problem. For most of the instances, globally optimal integer 
solutions are obtained by solving a single convex problem. Since the proposed 
technique leads to a large number of valid quadratic inequalities, an iterative 
procedure is devised to impose a small number of such valid inequalities. For the 
cases where the strengthened SDP does give a global integer solution, we 
incorporate other valid inequalities, including a set of Boolean quadric polytope 
constraints. The proposed relaxations are extensively tested on various IEEE 
power systems in simulations.
2. Data-driven Optimal Reduced Order Model Tuning for Partial 
Differential Equations: Application to the 3D Boussinesq Equation
2016_iccopt.indb   67 2016/07/22   11:58:18
ABSTRACTS
ICCOPT 201668
Mouhacine Benosman (benosman@merl.com) Mitsubishi Electric 
Research Laboratories, USA
The problem of reducing a partial differential equation (PDE) model to a system 
of finite dimensional ordinary differential equations (ODE), is of paramount 
importance in engineering and physics where solving such PDE models is too 
time consuming. The idea of being able to reduce the PDE model to a simple 
model, without loosing the main characteristics of the original model, such as 
stability and prediction precision, is appealing for any real-time model-based 
computations. However, this problem remains challenging, since model 
reduction can introduce stability loss and prediction degradation. To remedy 
these problems, many methods have been developed aiming at what is known as 
stable model reduction. In this talk, we focus on the so-called closure models and 
their application in reduced order model (ROM) stabilization. More specifically, 
we introduce few closure-models and propose to auto-tune them online based a 
data-driven optimization algorithms known as ʻextremum-seekingʼ to tune the 
closure modelsʼ parameters for optimal ROM stabilization. We discuss the 
convergence of the proposed extremum-seekers and the corresponding tuning 
performance. The 3D Boussinesq equation is employed as a roomʼs airflow test-
bed for the proposed stabilization method.
3. Handling Dynamic Constraints in Power System Optimization
Francois Gilbert (fgilbert@anl.gov) Argonne National Laboratory, 
USA, Shrirang Abhyankar, Mihai Anitescu, Zhang Hong
The inclusion of dynamic stability constraints is the nominal objective of many 
optimization-based power systems analyses. In current practice, this is typically 
done off-line. We present an approach for the on-line inclusion of dynamic 
constraints in power grid optimization problems. The approach is based on an 
encapsulation that allows for a loose coupling between the optimization and the 
numerical simulations. We demonstrate the benefits of the approach on a IEEE 
118-Bus Systems, for which we solve an economic dispatch with transient 
constraints. 

■ Tue.C.4B
Tuesday, 14:45-16:00, Room 4B
Applications of Complementarity Models: Sparsity and 
Games
Cluster: Complementarity and Variational Inequalities
Session organized by: Shisheng Cui
1. A Reformulation of Sparse Optimization Problems using 
Complementarity-Type Constraints
Alexandra Schwartz (schwartz@gsc.tu-darmstadt.de) TU Darmstadt, 
Germany, Christian Kanzow, Oleg Burdakov
We consider sparse optimization problems, i.e. mathematical programs where 
the objective is not only to minimize a given function but also the number of 
nonzero elements in the solution vector. Possible applications are compressed 
sensing, portfolio optimization and feature selection. In this talk, we present a 
continuous reformulation of the noncontinuous sparsity term in the objective 
function using a complementarity-type constraint. We discuss the relation 
between the original and the reformulated problem, provide suitable optimality 
conditions and provide preliminary numerical results.
2. Distributed Algorithms for Potential Generalized Nash 
Equilibrium Problems (GNEPs) and Nonseparable Optimization 
Problems
Andrew Lu Liu (andrewliu@purdue.edu) Purdue University, USA, 
Run Chen
We present a unified algorithmic framework to parallelly compute a (local) 
solution of an optimization problem and an equilibrium for a subclass of GNEPs. 
The key of the framework is to explore the relationship between potential games 
and optimization problems. Potential games are a subclass of games in which a 
single potential function (similar to a merit function) exists that can reflect 
individual playerʼs payoff changes with different actions, when other playersʼ 
actions are fixed. Under player-wise convexity, the stationary point from 
optimizing the potential function is a Nash equilibrium of the corresponding 
game. On the other hand, any multivariate optimization problem can be viewed 
as a potential game, with each player (who controls a subset of the variables) 
having an identical payoff function. We show that the potential function 
optimization problem can be solved with parallel computing, using the classic 
method of multipliers, but without requiring separability or convexity of either 
the objective or constraint functions. The parallel algorithm is exactly a Guass-
Jacobian-type algorithm applied to solving the game associated with the potential 
function optimization, whose convergence can be established based on recent 
development. Preliminary numerical results will be presented to demonstrate the 
overall frameworkʼs efficiency. 
3. Multi-Leader Single-Follower Game between Suppliers with a 
Manufacturer
Tatsuya Hirano (hirano-tatsuya-fj@ynu.jp) Yokohama National 
University, Japan, Yasushi Narushima
In this talk, we consider a competition occurring in a supply chain. Extending the 
research of Ang et al. (Journal of Global Optimization, 2012), we analyze the 
competition. Ang et al. constructed a model about a bi-level non-cooperative 
game model in a supply chain. In this model, some suppliers (leaders) try to 
maximize their profits by deciding productsʼ delivery frequencies at the upper 
level and one manufacturer (follower) tries to minimize his cost by deciding a 
demand allocation to each supplier at the lower level. This game can be regarded 
as a multi-leader single-follower game. In the model of Ang et al., the variable of 
suppliers is delivery frequency only. On the other hand, we regard not only 
delivery frequency but also productsʼ price as the variables of suppliers. Then, the 
game is formulated as a generalized Nash equilibrium problem (GNEP). 
Moreover, we reformulate this problem as a quasi-variational inequalities (QVI) 
and show the existence of its solution. Finally, we give some numerical results.

■ Tue.C.5A
Tuesday, 14:45-16:00, Room 5A
Convex Optimization for Learning and Data Sciences
Cluster: Multi-Objective and Vector Optimization
Session organized by: Silvia Villa
1. A Universal Catalyst for First-Order Optimization
Hongzhou Lin (hongzhou.lin@inria.fr) INRIA, France, Julien Mairal, 
Zaid Harchaoui
We introduce a generic scheme for accelerating first-order optimization methods 
in the sense of Nesterov, which builds upon a new analysis of the accelerated 
proximal point algorithm. Our approach consists of minimizing a convex 
objective by approximately solving a sequence of well-chosen auxiliary 
problems, leading to faster convergence. This strategy applies to a large class of 
algorithms, including gradient descent, block coordinate descent, SAG, SAGA, 
SDCA, SVRG, Finito/MISO, and their proximal variants. For all of these 
methods, we provide acceleration and explicit support for non-strongly convex 
objectives. In addition to theoretical speed-up, we also show that acceleration is 
useful in practice, especially for ill-conditioned problems where we measure 
significant improvements.
2. Less is More: Optimal Learning with Stochastic Projection 
Regularization
Lorenzo Andrea Rosasco (lrosasco@mit.edu) DIBRIS, Univ. of 
Genoa, and LCSL, Istituto Italiano di Tecnologia and Massachusetts 
Institute of Technology, Italy, Alessandro Rudi, Raffaello Camoriano
In this talk, we will discuss the generalization properies of commonly used 
techniques to scale up kernel methods and Gaussian processes. In particular, we 
will focus on data dependent and independent sub-sampling methods, namely 
Nystrom and random features, and study their generalization properties within a 
statistical learning theory framework. On the one hand we show that these 
methods can achieve optimal learning errors while being computational efficient. 
On the other hand, we show that subsampling can be seen as a form of stochastic 
projection regularization, rather than only a way to speed up computations.
3. Computational Regularization in Learning and Inverse Problems
Silvia Villa (asilviavilla@gmail.com) Istituto Italiano di Tecnologia, 
Italy, Lorenzo Rosasco
Modern high dimensional estimation problems from random noisy data require 
the development of ever more efficient algorithms. A key observation towards 
this goal is that the numerical precision in the computations should be taylored 
to the estimation accuracy allowed by the data rather than only their raw amount. 
Indeed, this suggests that efficient methods can be derived investigating the 
interplay and trade-offs between estimation and computational requirements. 
With this goal in mind in this talk we focus on iterative regularization methods 
where regularization is achieved by early stopping an iteration defined by the 
data. Recent results in the context machine learning and inverse problems will be 
discussed.

■ Tue.C.5D
Tuesday, 14:45-16:00, Room 5D
2016_iccopt.indb   68 2016/07/22   11:58:18
ABSTRACTS
ICCOPT 2016 69
Theoretical Advances in Linear Optimization — New 
Perspectives
Cluster: Linear Optimization
Session organized by: Aaron Daniel Sidford, Yin Tat Lee
1. Faster Approximation for Packing and Covering LPs
Di Wang (wangd@eecs.berkeley.edu) Uniersity of California, 
Berkeley, USA, Michael Mahoney, Satish Rao
In a series of recent breakthroughs, Allen-Zhu and Orecchia leveraged the 
insights from the first-order optimization methods to provide improved 
algorithms for packing and covering linear programs [AO STOCʼ15, AO 
SODAʼ15]. The first result is particularly interesting, as the algorithm for packing 
LP achieves both width-independence and Nesterov-like acceleration, which was 
not known to be possible before. Somewhat surprisingly, however, their result on 
covering LP doesnʼt achieve the same Nesterov-like acceleration. This 
discrepancy is surprising, and it leaves open the question of the exact role that the 
optimization scheme is playing in coordinating the complementary gradient and 
mirror descent step of the algorithm. We clarify these issues for linear coupling 
algorithms for packing and covering LPs, illustrating that the linear coupling 
method can lead to improved O(1/ε) dependence for both packing and covering 
problems in a unified manner. Our main technical result is a novel dimension 
lifting method that reduces the coordinate-wise diameters of the feasible region 
for covering LPs, which is the key structural property to enable the same 
Nesterov-like acceleration as in the case of packing LPs. The technique is of 
independent interest and may be useful in applying the accelerated linear 
coupling method to other combinatorial problems.
2. Slime Molds and Sparse Recovery
Damian Straszak (damian.straszak@epfl.ch) École Polytechnique 
Fédérale de Lausanne (EPFL), Switzerland, Nisheeth K Vishnoi
We present a connection between two dynamical systems arising in entirely 
different contexts: one in sparse recovery and the other in biology. The first is the 
famous Iteratively Reweighted Least Squares (IRLS) algorithm used in sparse 
recovery while the second is the dynamics of a slime mold. Both of these 
dynamics are geared towards finding a minimum `1-norm solution in an affine 
subspace. Despite its simplicity the convergence of the IRLS method has been 
shown only for a certain regularization of it and remains an important open 
problem. We show that the two dynamics are projections of the same dynamical 
system in higher dimensions. Subsequently, we show convergence and obtain 
complexity bounds for a damped version of the IRLS algorithm.
3. Solving Linear Programs via Rescalable Geometry
Daniel N Dadush (dndadush@gmail.com) CWI, Netherlands
In 1979, Khachiyan used a “numerical trick” to show that linear programming 
(LP) is polynomially solvable using the ellipsoid method. Since that time, far 
more powerful algorithms for LP have been developed, e.g. interior point 
methods, with faster and faster convergence rates. Despite the steady progress in 
running times, a deeper understanding of the geometry behind the “numerical 
trickery” has proved elusive. In this work, we provide a new fine grained view of 
the relevant geometry for solving linear programs exactly. More precisely, we 
define a combination of three geometric potentials (which can be bounded by the 
size of the numbers), that together control the complexity of finding a point in the 
relative interior of any linear system together with a certificate of this fact. For 
this purpose, we provide a novel polynomial time “rescaled” interior point 
method for LP. Conversely, we show that given any such solution together with 
the certificate, one can define a simple scaling of the rows of the linear system 
such that all the aforementioned potentials become strongly polynomial.

■ Tue.C.5E
Tuesday, 14:45-16:00, Room 5E
Ambiguity-aware Decision Making under Uncertainty
Cluster: Robust Optimization
Session organized by: Ruiwei Jiang
1. Risk-averse Stochastic Unit Commitment with Incomplete 
Information
Yongpei Guan (guan@ise.ufl.edu) University of Florida, USA, Ruiwei 
Jiang, Jean-Paul Watson
In this paper, we study two risk-averse stochastic unit commitment models with 
incomplete information, with the first to be a chance-constrained unit commitment 
model and the second to be a two-stage stochastic unit commitment model with 
recourse. Based on historical data of renewable energy, we construct a confidence 
set for the probability distribution of the renewable energy and propose data-
driven stochastic unit commitment models to hedge against the information 
incompleteness. Our models also ensure that, with a high probability, a large 
portion of renewable energy is utilized. Furthermore, we develop solution 
approaches to solve the models based on deriving strong valid inequalities and 
Bendersʼ decomposition algorithms. We show that the risk-averseness of both 
models decreases as more data samples are collected and eventually vanishes as 
the sample size goes to infinity. Finally, our case studies verify the effectiveness 
of our proposed models and solution approaches. 
2. Two-Stage Stochastic Program with Distributional Ambiguity
Ruiwei Jiang (ruiwei@umich.edu) University of Michigan, USA, 
Yongpei Guan
We develop a two-stage stochastic program that takes into account the 
distributional ambiguity. We derive an equivalent reformulation for this model 
that applies to both discrete and continuous distributions. Also, the reformulation 
reflects its linkage with a full spectrum of coherent risk measures under varying 
data availability.
3. Distributionally Robust Chance-constrained Bin Packing Problem
Siqian Shen (siqian@umich.edu) University of Michigan, USA, Yiling 
Zhang, Ruiwei Jiang, Saadet Ayca Erdogan
This paper considers a distributionally robust bin packing problem with random 
item sizes. We minimize the number of bins to pack all the items, while requiring 
not exceeding each binʼs capacity limit with high probability. This paper 
considers a distributionally robust bin packing problem with random item sizes. 
We minimize the number of bins to pack all the items, while requiring not 
exceeding each binʼs capacity limit with high probability. We use moment-based 
ambiguity sets and build two relaxations as a 0-1 SDP and a 0-1 SOCP. We also 
derive an exact 0-1 SOCP reformulation of the DR model and exploit 
submodularity to improve the computational efficiency, via a cutting-plane 
algorithm with polynomial-time separation subroutines. We demonstrate the 
computational efficacy and results of different approaches by testing instances of 
server allocation under random service durations, from an outpatient treatment 
application. 

■ Tue.C.5G
Tuesday, 14:45-16:00, Room 5G
Numerical Methods for Large Scale Nonlinear 
Optimisation
Cluster: Optimization Implementations and Software
Session organized by: Christof Bueskens
1. Parametric Sensitivity Analysis within Sequential Quadratic 
Programming — Post Optimality Analysis of Subproblems
Sören Geffken (sgeffken@math.uni-bremen.de) Universität Bremen, 
Germany, Christof Büskens
Various tasks like parameter identification and discretised optimal control 
problems require the solution of typically large scale nonlinear optimisation 
problems. The parametric sensitivity analysis helps to understand the properties 
of optimisation problems. Parametric sensitivity derivatives quantify the effect 
of parametric perturbations to the solution of the problem. Within an SQP 
method quadratic subproblems with linearised constraints must be solved. 
Parametric sensitivity analysis on the subproblems in the SQP method allows to 
study the effect of different internal parameters on the subproblems. During the 
solution of the subproblems using our NLP solver WORHP several factorisations 
of the KKT matrix must be computed and can later be exploited to obtain 
parametric sensitivity derivatives at low computational cost. Multiple interesting 
parametric perturbations with large effect on the subproblems are analysed. Of 
special interest are perturbations within special strategies like Hessian 
regularisation and constraint relaxation. Furthermore, a specially structured 
parametric perturbation of the constraints can be introduced to overcome the 
deficiencies in the search direction due to the linearisation of the constraints in 
the sub problems. Different algorithmic extensions and numerical results are 
presented using the NLP solver WORHP.
2. Implementation of a Penalty-Interior-Point Algorithm within 
WORHP
Renke Schäfer (renke.schaefer@math.uni-bremen.de) University of 
Bremen, Germany, Christof Büskens
Interior-point methods have been shown to be very efficient for large-scale 
nonlinear programming (NLP) and, thus, form the basis for most of the state-of-
the-art NLP solvers. Within our solver WORHP an interior-point method is used 
to solve the quadratic problems for its sequential quadratic programming (SQP) 
2016_iccopt.indb   69 2016/07/22   11:58:18
ABSTRACTS
ICCOPT 201670
algorithm. Penalty-interior-point methods include the natural regularization of 
the constraints of penalty methods, which aims to increase the robustness of the 
algorithm, in particular for degenerate problems. Different attempts have been 
studied within the last decades, among them `1, `2 and augmented Lagrangian 
penalty approaches. We implemented an augmented Lagrangian based penalty-
interior-point algorithm within WORHP as an alternative to its SQP option. In 
this presentation we give insights in our implementation and show a numerical 
study based on the CUTEst test set. The results of our new algorithm are 
compared with the SQP method of WORHP.
3. SQP Methods and QP Hot-starting for Nonlinear Model Predictive 
Control
Christian Kirches (christian.kirches@gmail.com) IWR, Heidelberg 
University, Germany
We present multi-level SQP methods and QP hot starting techniques for nonlinear 
model predictive control of large-scale processes. First, multi-level SQP methods 
address the computational effort involved in derivative generation for large-scale 
dynamic processes by adaptive linearization of state equations. Parts of the QP 
data remain fixed for multiple iterations, and this knowledge may be exploited 
for QP hot starting. Second, for each new QP in the sequence, the method utilizes 
hot-starts that employ information computed by an active-set QP solver during 
the solution of the first QP. This avoids the computation and factorization of the 
full constraint and Hessian matrices for all but the first problem in the sequence. 
The practical performance of the proposed method is demonstrated on a sequence 
of QPs arising in nonlinear model predictive control and during the solution of a 
set of randomly generated nonlinear optimization problems using sequential 
quadratic programming. Part of the results presented in this talk are joint work 
with Felix Lenders, Travis Johnson, and Andreas Waechter.

■ Tue.C.5H
Tuesday, 14:45-16:00, Room 5H
Financial Decision Making under Distress
Cluster: Applications in Finance and Economics
Session organized by: Jingnan Chen
1. To Track or Not to Track: Can Economic and Financial Indicators 
Help Smart-Beta Funds?
Chanaka Edirisinghe (edirin@rpi.edu) Rensselaer Polytechnic 
Institute, USA, Yonggan Zhao
Fund management based on index-tracking is well-studied and widely-practiced. 
Stock index funds represent over than 75% of index mutual funds valued over 
1.7 trillion dollars. Since S&P-500 index is market cap-based, index tracking 
funds suffer from inefficiencies due to biases in large-cap equities. As a remedy, 
smart beta has emerged, where the funds are styled after alternative criteria to 
increase the exposure to a broader group of asset categories. Smart indexing has 
had tremendous growth over the last decade, with market value exceeding 230 
billion dollars. This paper presents a new and powerful optimization approach to 
smart indexing where holdings in market sectors are dynamically-adjusted based 
on shifts in the economic scenarios. Such shifts are evaluated using macro-
economic and financial indicator monthly data under a hidden Markov model. 
The likelihood of a shift determines the risk optimization strategy for the fund, as 
well as in updating asset return distributions. Fund returns so-computed are 
regressed over FF 3-factor model to identify fund alpha.
2. Optimally Manage Crash Risk
Shushang Zhu (zhuss@mail.sysu.edu.cn) Sun Yat-Sen University, 
China, Shushang Zhu, Wei Zhu, Xi Pei, Xueting Cui
Crash of the financial market means that most of the financial assets suddenly 
lose a certain part of their nominal value, which implies almost all the assets 
become perfectly correlated in a crash. The diversification effect of portfolios in 
a typical markets condition and the corresponding risk measures do not work any 
longer in a crash situation. Thus the performance measures (risk and return) and 
the managerial point of view under a crash should be distinguished from the 
traditional ones under the normal conditions. In this work, we integrate crash risk 
into portfolio management and investigate the performance measures, hedging 
and optimization of portfolio selection while involving crash risk. A convex 
programming framework based on parametric method is proposed to formulate 
the problem as a tractable one. Comprehensive simulation and empirical study 
are performed to test the proposed approach.
3. Optimal Portfolio Deleveraging under Cross-Asset Price Pressure
Jingnan Chen (jingnan_chen@sutd.edu.sg) Singapore University of 
Technology and Design, Singapore, Yufei Yang, Jie Zhang
We study an optimal portfolio deleveraging problem, where the objective is to 
meet specified debt/equity requirements at the minimal execution cost. During 
the course of trading, permanent and temporary price impact is taken into 
account. In particular, we include the cross-asset price pressure which measures 
the impact on an asset caused by the trading of other assets. Mathematically, the 
optimal deleveraging problem is formulated as a non-convex quadratic program 
with quadratic and box constraints. We develop a sequential convex QP 
embedded box searching method to obtain the optimal deleveraging strategy.

■ Tue.C.5I
Tuesday, 14:45-16:00, Room 5I
Geometry, Duality and Complexity in Conic Linear 
Programming II
Cluster: Conic and Polynomial Optimization
Session organized by: Gabor Pataki
1. A Reduction Method for SDP Based on Projection Lattices and 
Jordan Algebras
Frank Noble Permenter (fperment@mit.edu) Massachusetts Institute 
of Technology, USA, Pablo A Parrilo
Symmetry reduction is a powerful technique for reducing the size of a structured 
semidefinite program (SDP). In symmetry reduction, one uses group theory to 
find projections mapping feasible points to feasible points and optimal solutions 
to optimal solutions. In this work, we find a minimum rank projection with these 
properties by solving a structured optimization problem over the projection 
lattice. As we show, this optimization problem is solved by lattice-theoretic 
techniques that do not leverage, nor require, any group structure. Moreover, its 
solution identifies a Euclidean Jordan algebra intersecting the SDP solution set, 
allowing reformulation of the SDP over a product of simpler symmetric cones. 
We illustrate effectiveness of our method on examples, showing it can 
significantly reduce total computational cost. Finally, we compare our method to 
related *-algebra-based reduction techniques.
2. Duality of a Generalized Absolute Value Optimization Problem
Shota Yamanaka (s.yamanaka63@gmail.com) Kyoto University, 
Japan, Nobuo Yamashita
We consider a generalized absolute value optimization problem that has 
nonlinear functions satisfying a Cauchy-Schwarz-like inequality. One of such 
nonlinear functions is the absolute value function. The problem is not necessarily 
convex, and it includes absolute value optimization, nonlinear second-order cone 
optimization, and quadratic optimization problems. For the generalized problem, 
we propose a new type of dual problem, which is formulated in a closed form. We 
then present some interesting dual properties. In particular, we discuss the 
relation between the Lagrangian duality and the one proposed here. We finally 
give some sufficient conditions under which these dual problems coincide.
3. Weak Infeasibility, Facial Reduction, and Geometry in Second-
Order Cone Programming
Masakazu Muramatsu (MasakazuMuramatsu@uec.ac.jp) The 
University of Electro-Communications, Japan, Bruno F Lourenço, 
Takashi Tsuchiya
We consider a sequence of feasibility problems which mostly preserve the 
feasibility status of the original second-order cone program. This is used to show 
that for a given weakly infeasible problem at most m directions are needed to get 
a point arbitrarily close to the cone, where m is the number of second-order 
cones. We present some of the related topics including the connection between 
the sequence and the facial reduction, and an extension to general conic linear 
programming.

■ Tue.C.5J
Tuesday, 14:45-16:00, Room 5J
Fast Inertial Proximal-Gradient Methods for Structured 
Optimization: O(1/k2) and Beyond
Cluster: Convex and Nonsmooth Optimization
Session organized by: Hedy Attouch
1. The Rate of Convergence of Nesterovʼs Accelerated Forward-
Backward Method is Actually Faster Than 1/k2
Hedy Attouch (hedy.attouch@univ-montp2.fr) Université Montpellier 
2, France, Juan Peypouquet
The forward-backward algorithm is a powerful tool for solving optimization 
2016_iccopt.indb   70 2016/07/22   11:58:18
ABSTRACTS
ICCOPT 2016 71
problems with a additively separable and smooth plus nonsmooth structure. In 
the convex setting, a simple but ingenious acceleration scheme developed by 
Nesterov improves the theoretical rate of convergence for the function values 
from the standard O(k- 1) down to O(k- 2). In this lecture, we prove that the rate of 
convergence of a slight variant of Nesterovʼs accelerated forward-backward 
method, which produces convergent sequences, is actually o(k- 2) (small “o”) 
rather than O(k- 2) (big “O”). Our arguments rely on the connection between this 
algorithm and a second-order differential inclusion with vanishing damping, 
recently introduced by Su, Boyd and Candès.
2. A Fast Convergent First-Order Method bearing Second-Order 
Information
Juan Peypouquet (juan.peypouquet@usm.cl) Universidad Tecnica 
Federico Santa Maria, Chile, Hedy Attouch, Patrick Redont
We propose a model for a class of first-order methods as an inertial system with 
Hessian-driven damping. The model combines several features of the Levenberg-
Marquardt algorithm and Nesterovʼs acceleration scheme for first-order 
algorithms. We obtain a second-order system (in time and space), which can be 
interpreted as a first-order one by an appropriate transformation. The resulting 
method is easily implementable, more stable than classical accelerated methods, 
and just as fast.
3. Convergence Rates in Convex Optimization: Going beyond the 
Worst-Case Analysis
Garrigos Guillaume (guillaume.garrigos@gmail.com) Istituto Italiano 
di Tecnologia, Italy, Lorenzo Rosasco, Silvia Villa, Pierre Frankel, Juan 
Peypouquet
In general, first-order descent methods for solving convex optimization problems 
enjoy a O(1/n) rate of convergence for the function values. But these rates come 
from a worst-case analysis, with no particular hypothesis being made on the 
function to minimize. It is well-known that, by restricting the analysis to strongly 
convex functions, we can obtain linear rates, for both the function values and the 
iterates. But this is a quite restrictive hypothesis, and one could reasonably ask 
whether these results can be obtained for a more general class of functions. We 
will show that, by exploiting the geometrical information that we know about the 
function to minimize, precise rates for the iterates and the values can be obtained. 
As a by-product, we recover the linear convergence for strongly convex 
functions, but also for any function presenting a quadratic ʻshapeʼ around its 
minimizers, even if it is not coercive. We also obtain a whole spectrum of 
convergence rates between the linear one and the worst case O(1/n), recovering 
some well-known results in linear inverse problems. As an application, we will 
show how the knowledge of these new rates can be used to design a better early 
stopping rule for regularization schemes, which is a central theme in machine 
learning.

■ Tue.C.5L
Tuesday, 14:45-16:00, Room 5L
Notions of Robustness and Dynamics in Convex 
Optimization: Part III
Cluster: Convex and Nonsmooth Optimization
Session organized by: Benjamin Recht, Pablo A Parrilo
1. Fitting Convex Sets to Data via Matrix Factorization
Venkat Chandrasekaran (venkatc@caltech.edu) California Institute of 
Technology, USA, Yong Sheng Soh
High-dimensional datasets arise prominently in a range of contemporary problem 
domains throughout science and technology. In many of these settings, the data 
are often constrained structurally so that they only have a few degrees of freedom 
relative to their ambient dimension. Methods such as manifold learning, 
dictionary learning, and others aim at computationally identifying such latent 
low-dimensional structure. In this talk, we describe a new approach to inferring 
the low-dimensional structure underlying a dataset by fitting a convex set with 
favorable facial structure to the data (in a manner to be suitably defined). Our 
procedure is based on computing a structured matrix factorization, and it includes 
several previous techniques as special cases. We illustrate the utility of our 
method with experimental demonstrations in applications.
2. Switched System Analysis via Dual/Sum-of-Squares Techniques
Pablo A Parrilo (parrilo@mit.edu) Massachusetts Institute of 
Technology, USA, Benoit Legat, Raphael Jungers
How to characterize the asymptotic convergence rate of an infinite product of 
matrices, taken from a given set? This question appears, for instance, when 
analyzing certain classes of decentralized/distributed optimization algorithms. A 
well-known approach is based on the “joint spectral radius” (JSR) of the set, a 
notion of importance in applications such as hybrid systems analysis.  In this talk 
we describe a new approach to this question, using a sum of squares optimization 
program and its dual for JSR approximation. Our methods produce an infinite 
sequence of matrices with an asymptotic growth rate arbitrarily close to the JSR. 
The algorithm naturally extends to the case where the allowable switching 
sequences are determined by a graph or finite automaton. We provide 
approximation guarantees on the closeness of the approximation ratio, and 
numerical examples illustrating the good performance of the method. 

■ Tue.D.1S
Tuesday, 16:30-17:45, Room 1S
Large-Scale Nonlinear Optimization
Cluster: Nonlinear Optimization
Session organized by: Roummel Marcia, Jennifer Erway
1. An Active Set Algorithm for Nonlinear Optimization with 
Polyhedral Constraints
William W Hager (hager@ufl.edu) University of Florida, USA, 
Hongchao Zhang
A polyhedral active set algorithm PASA is developed for solving a nonlinear 
optimization problem whose feasible set is a polyhedron. Phase one of the 
algorithm is the gradient projection method, while phase two is any algorithm for 
solving a linearly constrained optimization problem. Rules are provided for 
branching between the two phases. Global convergence to a stationary point is 
established, while asymptotically PASA performs only phase two when either a 
nondegeneracy assumption holds, or the active constraints are linearly 
independent and a strong second-order sufficient optimality condition holds.
2. A New Successive Subspace Method for Solving the Trust-Region 
Subproblem
Joshua D Griffin (Joshua.Griffin@sas.com) SAS Institute Inc., USA, 
Ioannis Akrotirianakis, Melanie Gratton, Alireza Yektamaram, Wenwen 
Zhou
A resurgence in Hessian-free methods for deep learning has sparked interest in 
new iterative methods capable of handling the unique requirements of deep 
learning problems. This talk will focus on methods for solving the trust-region 
sub-problem in the context of large-scale nonconvex optimization. An effective 
but simple successive subspace method is used to safe-guard the conjugate 
gradient method applied to the Newton equations. A novel feature of our 
approach is that the trust-region solver also doubles as an accurate and efficient 
extreme value eigensolver. Remarkably both the trust-region subproblem and the 
minimum eigenvalue problem are solved simultaneously for the cost of a single 
matrix multiply per iteration and nominal memory overhead. Numerical results 
demonstrate the effectiveness and efficiency of this approach, both as a trust-
region solver and as a stand-alone eigenvalue solver.
3. Methods for Large- and Medium-Scale Nonlinear Optimization
Elizabeth Wong (elwong@ucsd.edu) University of California, San 
Diego, USA, Philip E Gill, Michael A Saunders
We consider some theoretical and practical issues associated with the formulation 
of sequential quadratic programming (SQP) methods for large- and medium-
scale nonlinear optimization problems. Issues that complicate the implementation 
of efficient SQP methods in the large-scale case are discussed. Numerical results 
are presented for the software packages DNOPT and SNOPT, which utilize an 
exact or approximate Hessian.

■ Tue.D.1A
Tuesday, 16:30-17:45, Room 1A
Nonconvex and Non-Lipschitz Optimization: 
Algorithms and Applications 4
Cluster: Nonlinear Optimization
Session organized by: Ya-Feng Liu
1. New Strategies of Stochastic RBF Method for Expensive Black-
Box Global Optimization
Dong Kang (kangdong@lsec.cc.ac.cn) Chinese Academy of Sciences, 
China
We are considering the bound constrained global optimization problem where 
the objective function is a continuous and computationally expensive black-box 
function with multiple local minima. There are many engineering optimization 
2016_iccopt.indb   71 2016/07/22   11:58:18
ABSTRACTS
ICCOPT 201672
problems in which a single objective function evaluation may take from a few 
minutes to many hours, and the objective function formulation or the derivative 
information is unknown. Our goal is to approximate the global minimizer with 
only a relatively small number of function evaluations. Regis and Shoemaker 
(2007) proposed an efficient method (stochastic RBF) to solve this kind of 
problem. In this work, we propose some new strategies to improve the efficiency 
of stochastic RBF method by changing the way to generate candidate points, and 
design a new utility function to measure different candidate points.
2. A Subspace Multilevel Method for Nonlinear Optimization
Cheng Chen (cchen@lsec.cc.ac.cn) Chinese Academy of Sciences, 
China
We propose a new subspace multilevel method for solving general unconstrained 
infinite dimensional optimization problems. At each iteration, the algorithm 
executes a direct step on the current level or a coarse subspace correction step. 
For coarse subspace correction step, we induce a two-dimensional subspace, 
which is spanned by the current point and the gradient direction of current point, 
in the traditional coarse grid space. Following the optimize-then-discretize 
strategy, we derive the formulation of infinite dimensional coarse space 
subproblem first and solve its discretized version. Global convergence and 
convergence rate are proved under minimal conditions on discretized functions 
used at all grid levels. Some numerical experiments are presented in the last part, 
which show that our subspace multilevel method is promising.
3. A General Proximal Quasi-Newton Method for Large Scale l1 
Penalized Optimization Problem
Zhilong Dong (zldong@lsec.cc.ac.cn) Chinese Academy of Sciences, 
China, Yu-Hong Dai, Zhao-Song Lu
In this report, we propose an inexact proximal quasi-Newton method for solving 
large scale l1 penalized optimization problem. The object function consists of one 
smooth convex part as well as a l1 penalized term. Firstly, we use a limited 
memory BFGS framework to approximate the smooth part with a quadratic 
function. Then we solve the reformulated subproblem using an interior point 
method with wide neighborhood. We use Shermon-Morrison-Woodbury 
technique to reduce the computational complexity in each iteration, and propose 
several techniques to overcome the difficulty of generating computation error 
when computing the inverse of a matrix. Global convergence as well as the local 
Q-superlinear convergence rate are guaranteed for this algorithm. Numerical 
experiments show that our LBIPM algorithm runs faster and returns solutions 
not worse than those from the state-of-the-art algorithms.

■ Tue.D.1B
Tuesday, 16:30-17:45, Room 1B
PDE Optimization and Applications I
Cluster: PDE-constrained Optimization
Session organized by: Tomoya Takeuchi
1. Receding Horizon Control for Spatiotemporal Dynamic Systems
Tomoaki Hashimoto (tomoaki.hashimoto@oit.ac.jp) Osaka Institute 
of Technology, Japan
Receding horizon control is a type of optimal feedback control in which control 
performance over a finite future is optimized with a performance index that has 
a moving initial time and terminal time. Spatiotemporal dynamic systems 
characterized by both spatial and temporal variables often occur in many research 
fields. In this talk, a design method of receding horizon control for a generalized 
class of spatiotemporal dynamic systems is illustrated. Using the variational 
principle, we first derive the stationary conditions that must be satisfied for a 
performance index to be optimized. Next, we consider a numerical algorithm to 
solve the stationary conditions via a finite-dimensional approximation. Finally, 
the effectiveness of the control design method is verified by numerical 
simulations. 
2. Topology Optimization for Fluid Dynamics Problems and Its 
Applications in Flow Channel Design
Kentaro Yaji (yaji@mech.eng.osaka-u.ac.jp) Osaka University, Japan
The application of structural optimization in fluid problems has been an attractive 
area of research for mathematicians and engineers. In this study, we show the 
applicability of topology optimization for fluid problems, whereas the most 
widely used approach is based on shape optimization. In particular, we focus on 
a flow channel design considering fluid and scalar transport, which is a pioneering 
research topic in the research field. Several numerical examples are provided to 
confirm the efficacy of proposed methodology for the design of flow channel 
devices such as heatsink and microreactor.
3. Shape Optimization Approach to Free Boundary Problems by 
Traction Method
Masato Kimura (mkimura@se.kanazawa-u.ac.jp) Kanazawa 
University, Japan, Shogen Shioda, Maharani Ahsani Ummi, Hideyuki 
Azegami, Kohji Ohtsuka
For an inverse free boundary problem governed by the Laplace equation, we 
consider a new approach by means of the shape optimization and propose a 
numerical scheme to solve it using the traction method. The traction method is a 
widely used numerical scheme for optimal shape design problems in engineering. 
We numerically check the efficiency of the traction method by using the exact 
solution of the free boundary problem. We can avoid the numerical difficulty in 
treating the curvature term arising in the shape derivative of the cost function 
using its weak formulation coupled with the traction method.

■ Tue.D.1C
Tuesday, 16:30-17:45, Room 1C
Advances in Derivative-free and Simulation-based 
Optimization II
Cluster: Derivative-free and Simulation-based Optimization
Session organized by: Francesco Rinaldi, Zaikun Zhang
Session chair: Warren L Hare
1. An Implicit Filtering-based Algorithm for Derivative Free 
Multiobjective Optimization
Alessandra Papini (alessandra.papini@unifi.it) University of Florence, 
Italy, Guido Cocchi, Giampaolo Liuzzi, Marco Sciandrone
In this work we consider multiobjective optimization problems with bound 
constraints. We assume that the objective functions are continuously 
differentiable and that their gradients are not available. We present an algorithm 
combining direct search and implicit filtering approaches. At each iteration, a 
multiobjective variant of the classical coordinate search is performed. The 
implicit filtering phase starts whenever the coordinate search is unsuccessful, i.e. 
the coordinate search cannot produce a new nondominated point. The computed 
objective function values are employed to approximate the gradients of the 
objective functions, and a linear programming problem is solved to define a 
possible multiobjective descent direction. Then, a line search is performed along 
the computed direction. Global convergence results are stated. Computational 
experiments are performed and the obtained results are presented and discussed.
2. Global Derivative-free Quasi-Newton Methods for Bound-
constrained Nonlinear Systems
Margherita Porcelli (margherita.porcelli@unifi.it) University of 
Firenze, Italy, Benedetta Morini, Leopoldo Marini
We address the solution of nonlinear systems of equations where the variables 
are subject to bound-constraints and where we assume that derivatives of the 
residual function may be difficult to compute or unavailable. We present a new 
globally convergent procedure that may be performed in a derivative-free 
regime. The development of such procedure is motivated by the need to develop 
efficient tools for solving large nonlinear systems accessible to non-expert users, 
i.e. people who have little or no skill in analyzing the specific structure of the 
problem at hand. The proposed method relies on the Quasi-Newton approach and 
comprises spectral residual methods. Since the search directions generated by 
these methods may be uphill for the merit function we employ a globalization 
strategy based on an approximate norm descent condition. We provide a 
theoretical analysis of the proposed approach and illustrate its numerical 
behaviour on problems from the literature and from the simulation of compressor 
trains for oil&gas applications and gas distribution pipeline network. 
Comparisons are also conducted with existing approaches.
3. Using Inexact Subgradients to Compute Proximal Points of 
Convex Functions
Warren L Hare (warren.hare@ubc.ca) University of British Columbia, 
Canada, Chayne Planiden
Proximal points play a central role in a number of nonsmooth optimization 
algorithms. Some recent work has extended these algorithms to a DFO setting. 
However, past work has focused on extending entire (complicated) algorithms, 
and any subroutine to compute a proximate point is hidden within the developed 
method, and only analyzed in light of the developed method. In this work, we 
develop such an inexact bundle method to find the proximal point of a convex 
function at a given point. This method can now be used as a foundation in 
proximal-based methods for nonsmooth convex functions where the oracle 
returns an exact function value and an inexact subgradient vector.

2016_iccopt.indb   72 2016/07/22   11:58:18
ABSTRACTS
ICCOPT 2016 73
■ Tue.D.4A
Tuesday, 16:30-17:45, Room 4A
Optimization Models in Energy
Cluster: Applications in Energy, Science and Engineering
Session organized by: Javad Lavaei
1. Monotonicity Properties in Dissipative Flow Networks
Marc D Vuffray (vuffray@lanl.gov) Los Alamos National Laboratory, 
USA, Sidhant Misra, Anatoly Zlotnik, Misha Chertkov
Dissipative flow networks model flow of fluids or commodities across a network. 
The flow dynamics on edges are governed by non-linear dissipative partial 
differential equations. The dynamics on adjacent edges are coupled through 
Kirchhoff-Neumann boundary conditions that also account for the injection 
parameters at the nodes. We establish a monotonicity property which states that 
the ordering of the initial states (e.g. density) is preserved throughout the time 
evolution of the system whenever the nodal injection parameters also obey the 
same ordering. We show that the dynamic system resulting from an appropriate 
choice of spatial discretization of the system of PDEs inherits this monotonicity 
property and can be used within simulation and optimization. We also prove a 
monotonicity property for dissipative networks in steady state and establish a 
connection between the dynamic and steady state results. These results enable 
significant simplification in the representation and algorithms for robust 
optimization and control problems under uncertain nodal injections.
2. Optimal Distributed Control of Power Systems with a High Level 
of Renewable Energy
Abdulrahman Kalbat (akalbat@gmail.com) United Arab Emirates 
University, United Arab Emirates, Salar Fattahi, Javad Lavaei
This talk is concerned with the optimal distributed control of power systems 
under a high penetration of renewable energy. This optimal control problem is 
highly nonlinear and NP-hard. In this work, we design an efficient computational 
method to find a near-global distributed controller for a large class of systems. 
We also study how the connectivity of its underlying communication network 
affects the optimal performance of the stochastic power system under control. As 
a case study, the proposed technique is used to design a distributed primary 
frequency controller for the IEEE 39-Bus New England test System.
3. Power System State Estimation with a Limited Number of 
Measurements
Javad Lavaei (lavaei@berkeley.edu) University of California, 
Berkeley, USA, Ramtin Madani, Yu Zhang, Morteza Ashraphijuo, Ross 
Baldick
This work is concerned with the power system state estimation (PSSE) problem, 
which aims to find the unknown operating point of a power network based on a 
given set of measurements. The measurements of the PSSE problem are allowed 
to take any arbitrary combination of nodal active powers, nodal reactive powers, 
nodal voltage magnitudes and line flows. This problem is non-convex and NP-
hard in the worst case. We develop a set of convex programs with the property 
that they all solve the non-convex PSSE problem in the case of noiseless 
measurements as long as the voltage angles are relatively small. This result is 
then extended to a general PSSE problem with noisy measurements, and an 
upper bound on the estimation error is derived. The objective function of each 
convex program developed in this paper has two terms: one accounting for the 
non-convexity of the power flow equations and another one for estimating the 
noise levels. The proposed technique is demonstrated on the 1354-bus European 
network.

■ Tue.D.4B
Tuesday, 16:30-17:45, Room 4B
Stochastic Optimization and Variational Inequality 
Problems
Cluster: Complementarity and Variational Inequalities
Session organized by: Mengdi Wang, Shisheng Cui
1. On the Analysis of Three Stochastic Extragradient Variants for 
Monotone Stochastic Variational Inequality Problems
Shisheng Cui (suc256@psu.edu) The Pennsylvania State University, 
USA, Uday V Shanbhag
The stochastic generalizations of the extragradient methods are complicated by a 
key challenge: the scheme requires two projections on a convex set, which is a 
concern  if the sets are complicated. Thus, there is a strong need for developing 
low-complexity schemes equipped with convergence theory and rate statements 
for resolving such problems. This provides a motivation for considering three 
related avenues where every iteration requires a single projection: (i) A projected 
reflected gradient method; (ii) A subgradient extragradient method; and (iii) A 
modified backward-forward splitting method. Unfortunately, little appears to be 
known regarding the convergence properties of these schemes in stochastic 
regimes. To this end, we make the following contributions: (a) We prove almost 
sure convergence of the iterates to a random point in the solution set; (b) Under 
strong monotonicity, we prove that the mean-squared error associated with the 
iterate sequence diminishes to zero at the optimal rate of O(1/K) where K is the 
iteration index; (c) When the map is merely monotone, we prove that the gap 
function associated with the averaged sequence diminishes to zero at the optimal 
rate of . Preliminary numerics suggest that the schemes are competitive 
their extragradient counterparts.
2. On the Resolution of Complementarity Formulations of the L0-
Norm Minimization Problem via ADMM Schemes
Yue Xie (yux111@psu.edu) The Pennsylvania State University, USA, 
Uday V Shanbhag
Recently, there has been an effort to resolve the L0-norm optimization problem 
by considering a complementarity-based formulation. We adopt precisely such 
an approach and consider the use of an ADMM scheme. In particular, by 
exploiting the structure in the subproblems, we develop efficient variants of such 
schemes and provide preliminary numerics.
3. A Distributionally Robust Model for Three Stage Stochastic 
Linear Optimization
Sun Jie (jie.sun@curtin.edu.au) Curtin University, Australia, Bin Li, 
Kok Lay Teo, Changjun Yu
Three-stage stochastic linear program is the most basic model of multi-stage 
stochastic optimization with nontrivial nonanticipativity constraints and 
conditional distributions. However, the practical usage of such model has been 
restricted by the inability of solving (although structured) linear programs of 
huge dimension resulted from the exponential growth of scenarios, which is 
particularly so if the scenarios are generated through discretization of certain 
continuous random varaibles. We therefore consider a distributionally robust 
version of this model that utilizes the worst-case recourse functions over a set of 
possible probability distributions. It is shown that under a standard set of 
regularity assumptions, this distributionally robust three-stage stochastic 
optimization problem is equivalent to a conic optimization problem that can be 
solved in polynomial time. A numerical example is provided to show the 
advantage of the distributionally robust approach.

■ Tue.D.5A
Tuesday, 16:30-17:45, Room 5A
Generalized Convexity and Set Optimization
Cluster: Multi-Objective and Vector Optimization
Session organized by: Daishi Kuroiwa
1. Robust Vector Optimization: Well-Posedness, Sensitivity to 
Uncertainty and Generalized Convexity of Set-valued Maps
Matteo Rocca (matteo.rocca@uninsubria.it) Universitaʼ degli Studi 
dellʼInsubria, Italy, Giovanni P Crespi, Daishi Kuroiwa, Nicolae 
Popovici
Robust Optimization is growing as a powerful tool to handle parameters 
uncertainty in optimization models. Recently the relation between multicriteria 
robustness concepts and set valued optimization has been investigated (Ide, 
Köbis, Kuroiwa, Schöbel, Tammer). In this talk we deal with well-posedness 
properties of robust vector optimization problems. Furthermore, we investigate 
how robust solutions react to changes in the uncertainity set. As generalized 
convexity of set-valued maps plays a crucial role in the obtained results, we 
devote part of the talk to some characterizations of quasi–convexity for set-
valued maps.
2. Unified Approach in Set Optimization and Generalized Convexity 
for Set-valued Maps
Daishi Kuroiwa (kuroiwa@riko.shimane-u.ac.jp) Shimane University, 
Japan
We study set optimization, which is set-valued optimization with respect to set 
relations. Notions of solutions in set-valued optimization are classified into two 
types. One is based on the order of the range vector space. The other is based on 
set relations, which are binary relations on the family of the range space, and the 
problems based on set relations are called set optimization problems. In this talk, 
we propose a unified approach in such set optimization problems, and give 
2016_iccopt.indb   73 2016/07/22   11:58:19
ABSTRACTS
ICCOPT 201674
generalized notions of convexity for set-valued maps.
3. Generalized Convexity for Set-valued Maps and Its Applications
Kazuki Seto (k.seto@math.shimane-u.ac.jp) Shimane University, 
Japan, Daishi Kuroiwa
We study generalized convexity for set-valued maps based on the unified 
apploach which was studied by Kuroiwa, one of the authors, and we consider 
applications with respect to such generalized convexity.

■ Tue.D.5D
Tuesday, 16:30-17:45, Room 5D
Sparse and Low Rank Approximation
Cluster: Sparse Optimization and Information Processing
Session organized by: Coralia Cartis
Session chair: Maryam Fazel
1. Nonconvex Recovery of Low Complexity Models
John Wright (johnwright@ee.columbia.edu) Columbia University, 
USA, Ju Sun, Qing Qu
We consider a complete dictionary recovery problem, in which we are given a 
data matrix Y, and the goal is to factor it into a product Y~A0X0, with A0 a square 
and invertible matrix and X0 is a sparse matrix of coefficients. This is an 
abstraction of the dictionary learning problem, in which we try to learn a concise 
approximation to a given dataset. While dictionary learning is widely (and 
effectively!) used in signal processing and machine learning, relatively little is 
known about it in theory. Much of the difficulty owes to the fact that standard 
learning algorithms solve nonconvex problems, and are difficult to analyze 
globally. We describe an efficient algorithm which provably learns representations 
in which the matrix X0 has as many as O(n) nonzeros per column, under a suitable 
probability model for X0. Our results follow from a reformulation of the 
dictionary recovery problem as a nonconvex optimization over a high 
dimensional sphere. This particular nonconvex problem has a surprising 
property: once about n3 data samples have been observed, with high probability 
the objective function has no spurious local minima. This geometric phenomenon, 
in which seemingly challenging nonconvex problems can be solved globally by 
efficient iterative methods, also arises in problems such as tensor decomposition 
and phase recovery from magnitude measurements. 
2. Breaking Sample Complexity Barriers via Nonconvex 
Optimization?
Mahdi Soltanolkotabi (msoltoon@gmail.com) University of Southern 
California, USA
In the past decade there has been significant progress in understanding when 
convex relaxations are effective for finding low complexity models from a near 
minimal number of data samples (e.g. sparse/low rank recovery from a few 
linear measurements). Despite such advances convex optimization techniques 
are often prohibitive in practice due to computational/memory constraints. 
Furthermore, in some cases convex programs are suboptimal in terms of sample 
complexity and provably require significantly more data samples than what is 
required to uniquely specify the low complexity model of interest. In fact for 
many such problems certain sample complexity “barriers” have emerged so that 
there are no known computationally tractable algorithm that can beat the sample 
complexity achieved by such convex relaxations. Motivated by a problem in 
imaging, In this talk I will discuss my recent results towards breaking such 
barriers via natural nonconvex optimization techniques.
3. A Semidefinite Relaxation for Computing Distances Between 
Metric Spaces
Rachel Ward (rward@math.utexas.edu) University of Texas at Austin, 
USA, Afonso Bandeira, Andrew Blumberg, Soledad Villar
In this talk we explore a semidefinite relaxation for the (NP-hard to compute) 
Gromov-Hausdorff distance between metric spaces, focusing on distances 
between point clouds.  We prove that the relaxed “distance” is in fact a metric, 
and serves as a lower bound for the Gromov-Hausdorff distance.  We derive an 
algorithm based on alternating direction augmented Lagrangian that can 
efficiently compute the distance between point clouds with hundreds of points, 
and illustrate its effectiveness on several data sets.

■ Tue.D.5E
Tuesday, 16:30-17:45, Room 5E
Recent Advances in Data-driven Optimization
Cluster: Robust Optimization
Session organized by: Vishal Gupta
1. Smart “Predict, Then Optimize”
Adam Elmachtoub (adam@ieor.columbia.edu) Columbia University, 
USA, Paul Grigas
We consider a class of optimization problems where the objective is not explicitly 
provided, but contextual information can be used to predict the objective based 
on historical data. A traditional approach would be to simply predict the objective 
based on minimizing prediction error, and then solve the corresponding 
optimization problem. Instead, we provide a prediction framework that leverages 
the structure of the optimization problem that will be solved given the prediction. 
We provide theoretical, algorithmic, and computational results to show the 
validity and practicality of our framework.
2. An Extended Frank-Wolfe Method with “In-Face” Directions, and 
Its Application to Low-Rank Matrix Completion
Paul Grigas (pgrigas@mit.edu) Massachusetts Institute of Technology, 
USA, Robert M Freund, Rahul Mazumder
We present an extension of the Frank-Wolfe method that is designed to induce 
near-optimal low-rank solutions for matrix completion and, for more general 
problems, induces near-optimal well-structured solutions. We establish 
computational guarantees for the method that trade off efficiency in computing 
near-optimal solutions with upper bounds on the rank of iterates. We present 
extensive computational results on both real and artificial datasets that 
demonstrate that our extended Frank-Wolfe method (in different versions) 
shows significant computational advantages over existing related approaches, in 
terms of delivering low rank and low run time to compute a target optimality gap.
3. Empirical Bayes and Optimization in the Small-Data Regime
Vishal Gupta (guptavis@usc.edu) The University of Southern 
California, USA, Paat Rusmevichientong
Optimization applications frequently depend on a huge number of parameters 
that must be estimated from data. In many contexts, however, the amount of data 
per parameter is small. For example, an online retailer might stock millions of 
products, but most products only have a few sales per year. We propose a novel 
approach to optimization in this small-data/high dimensional regime for the 
special class of linear optimization problems with uncertain objectives. Unlike 
state-of-practice methods based on the “estimate then optimize” paradigm, our 
approach leverages ideas from empirical bayes to exploit Steinʼs phenomenon 
and pool information across parameters. Moreover, unlike other empirical bayes 
methods based on maximum likelihood or method of moments, our approach 
utilizes the linear optimization structure to improve performance. Because of 
these two features, we show our approach is never worse than, and often 
significantly better than, both the state-of-practice and traditional empirical 
Bayes methods. We prove that under mild conditions, as the number of objective 
coefficients in the linear optimization tends to infinity with the amount of data 
per coefficient fixed, our approach has performance comparable to the best-in-
class estimator. 

■ Tue.D.5G
Tuesday, 16:30-17:45, Room 5G
Linear Optimization and Computation
Cluster: Linear Optimization
Session organized by: Selin D. Ahipasaoglu, Giacomo Nannicini
1. Improving the CPLEX LP Solver
Roland Wunderling (roland.wunderling@at.ibm.com) IBM, Austria, 
Bo Jensen
Solving Linear Programs out of the box has become standard in the past decades, 
prompting practitioners to build larger and larger problem instances. We will 
discuss how these changes are recognised in the development of the CPLEX LP 
solver and provide a detailed performance evaluation of their effect.
2. LP Solution Polishing to Improve MIP Performance
Matthias Miltenberger (miltenberger@zib.de) Zuse Institute Berlin, 
Germany
In practice, the solution of a linear program (LP) is rarely unique. This is 
especially true in the context of mixed integer programming (MIP), where dual 
degeneracy is common. Here, the solution of the LP relaxation in the root node 
can strongly influence the later solving process. This performance variability 
may be exploited by choosing a particular candidate from this set of optimal 
2016_iccopt.indb   74 2016/07/22   11:58:19
ABSTRACTS
ICCOPT 2016 75
basic solutions. We investigate how the minimization and maximization of the 
number of integer variables with fractional solution values affects MIP 
components such as branching, cutting plane separation, and primal heuristics.
3. Primal-Dual Method for Decentralized Online Optimization
Soomin Lee (s.lee@duke.edu) Duke University, USA, Michael M 
Zavlanos
We present a consensus-based primal-dual method for decentralized online 
convex optimization, where the system objective function varies arbitrarily over 
time subject to some global inequality constraints. At each stage, each agent in 
the network commits to an adaptive decision pertaining only to the past and 
locally available information, and incurs a new cost function reflecting the 
change in the environment. Our algorithm uses weighted averaging of the iterates 
for each agent to keep local estimates of the global constraints and dual variables. 
We define regret as the cost difference with the optimal action over time. We 
show that the proposed decentralized method achieves a regret of order  
with the time horizon T, in scenarios when the underlying communication 
topology is time-varying and jointly-connected. The regret is measured in regard 
to the cost function value as well as the constraint violation. We also address the 
impact of the network topology as a factor on the speed of convergence. 
Numerical results for online routing in wireless multi-hop networks with 
uncertain channel rates will be provided to illustrate the performance of the 
proposed algorithm.

■ Tue.D.5H
Tuesday, 16:30-17:45, Room 5H
Optimization in Portfolio Selection and Risk 
Management
Cluster: Applications in Finance and Economics
Session organized by: Duan Li
1. Portfolio Optimization with Non-recursive Reference Point 
Updating
Moris Simon Strub (msstrub@se.cuhk.edu.hk) Chinese University of 
Hong Kong, Hong Kong, Duan Li
According to cumulative prospect theory, decision makers evaluate prospects in 
comparison to a reference point instead of with regards to resulting absolute 
terminal wealth levels. In a dynamic portfolio optimization setting it is thus 
crucial how investors form and update their reference points, as this directly 
influences optimal strategies. The empirical findings by Baucells et al. (2011) 
suggest that reference levels are updated in a non-recursive manner. Motivated 
by those results, we propose a dynamic portfolio choice model with non-
recursive reference point updating which leads to a time-inconsistent problem. 
We determine the optimal investment strategy for different types of investors and 
compare the resulting trading behavior to those implied by other behavioral 
portfolio choice models in the existing literature.
2. On Multiperiod Mean-CVaR Portfolio Optimization
Jianjun Gao (gao.jianjun@shufe.edu.cn) Shanghai University of 
Finance and Economics, China, Xiangyu Cui, Yun Shi, Shushang Zhu
In this work, we study the multiperiod mean-CVaR portfolio optimization 
problem with conic constraint on portfolio decision. Although the conditional 
value-at-risk (CVaR) is widely used in static portfolio selection and risk 
management, the dynamic mean-CVaR portfolio selection problem in discrete 
time setting are seldom studied due to the time inconsistency and the 
computational issue of this problem. Time inconsistency, which originates from 
the conflicts between the global investment interest and the local investment 
interests of the investor, makes the investor have the tendency of bowing to local 
investment temptations and deviating from the global optimal strategy during the 
investment. Three types of dynamic mean-CVaR investment strategies, pre-
committed strategy, time consistent strategy and self-control strategy, are 
discussed in this paper. The pre-committed strategy and time consistent strategy 
concerns the global investment interest and local investment interests 
respectively, while the self-control strategy balances the global and local 
investment interests. Furthermore, the comparison of three types of strategies is 
also shown through an illustrative example.
3. Quadratic Convex Reformulations for Semi-continuous Quadratic 
Programming and Its Application in Cardinality Constrained 
Mean-Variance Portfolio Selection
Duan Li (dli@se.cuhk.edu.hk) The Chinese University of Hong Kong, 
Hong Kong, Baiyi Wu
Modeling the cardinality constrained portfolio selection problem mathematically 
leads to a semi-continuous quadratic programming problem. We extend the 
quadratic convex reformulation approach in the literature to form new 
reformulations, which adds to the original objective function an additional 
function that is zero at all points of the feasible region and then convexifies the 
new objective function. Exploiting the structure of semi-continuous variables, 
we derive the most general set of quadratic functions that can be used to enhance 
the reformulation. Our numerical tests in cardinality constrained portfolio 
selection have demonstrated the effectiveness of our new reformulations and 
favorable comparison over the state-of-the-art perspective cut approach.

■ Tue.D.5I
Tuesday, 16:30-17:45, Room 5I
Barriers in Conic Optimization
Cluster: Conic and Polynomial Optimization
Session organized by: Roland Hildebrand
1. New Upper Bounds for the Density of Translative Packings of 
Three-dimensional Convex Bodies with Tetrahedral Symmetry
Cristobal Guzman (cguzman@gatech.edu) Centrum Wiskunde & 
Informatica, Netherlands, Maria Dostert, Fernando Oliveira, Frank 
Vallentin
We determine new upper bounds for the maximal density of translative packings 
of superballs in three dimensions (unit balls for the -norm) and of Platonic and 
Archimedean solids having tetrahedral symmetry. These bounds give strong 
indications that some of the lattice packings of superballs found in 2009 by Jiao, 
Stillinger, and Torquato are indeed optimal among all translative packings. We 
improve Zongʼs recent upper bound for the maximal density of translative 
packings of regular tetrahedra from 0.3840 to 0.3745, getting closer to the best 
known lower bound of 0.3673. We apply the linear programming bound of Cohn 
and Elkies which originally was designed for the classical problem of packings 
of round spheres. The proofs of our new upper bounds are computational and 
rigorous. Our main technical contribution is the use of invariant theory of 
pseudo-reflection groups in polynomial optimization.
2. The Entropic Barrier: A Universal and Optimal Self Concordant 
Barrier
Ronen Eldan (roneneldan@gmail.com) Weizmann Institute, Israel, 
Sebastien Bubeck
A fundamental result in the theory of Interior Point Methods is Nesterov and 
Nemirovskiʼs construction of a universal self-concordant barrier. In this talk, I 
will introduce the entropic barrier, a new, simple and in some sense optimal 
universal self-concordant barrier. 
3. Barriers on Symmetric Cones
Roland Hildebrand (roland.hildebrand@imag.fr) Weierstrass Institute, 
Germany
A self-scaled barrier on a symmetric cone is a non-degenerate convex 
combination of the logarithms of the determinants of the irreducible factors of 
the cone. The special properties of the self-scaled barriers are at the heart of the 
interior-point methods used for solving conic optimization problems over 
symmetric cones. We introduce an analytic description of self-scaled barriers 
which is of local character and independent of the notion of a symmetric cone. 
Namely, we identify these barriers as the solutions of a certain quasi-linear 
fourth-order partial differential equation. Given such a solution in the 
neighbourhood of some point, it defines and can be extended to the interior of 
some symmetric cone on which it will represent a self-scaled barrier. This partial 
differential equation has a simple interpretation as the vanishing of a certain 
mixed covariant derivative of the metric defined by the Hessian of the solution 
with respect to the affine connection of the ambient real space and the Levi-
Civita connection of the Riemannian metric defined by this Hessian. More 
precisely, the third derivative of the solution has to be invariant with respect to 
the geodesic flow defined by the Riemannian metric. Thus in a certain sense, 
self-scaled barriers resemble cubic polynomials.

■ Tue.D.5J
Tuesday, 16:30-17:45, Room 5J
Primal-Dual Algorithm for Convex Optimization
Cluster: Convex and Nonsmooth Optimization
Session organized by: Qihang Lin
1. Stochastic Dual Ascent for Solving Linear Systems
Peter Richtarik (richtarik@gmail.com) University of Edinburgh, 
2016_iccopt.indb   75 2016/07/22   11:58:19
ABSTRACTS
ICCOPT 201676
United Kingdom, Robert Mansel Gower
We develop a new randomized iterative algorithm—stochastic dual ascent 
(SDA)—for finding the projection of a given vector onto the solution space of a 
linear system. The method is dual in nature: with the dual being a non-strongly 
concave quadratic maximization problem without constraints. In each iteration 
of SDA, a dual variable is updated by a carefully chosen point in a subspace 
spanned by the columns of a random matrix drawn independently from a fixed 
distribution. The distribution plays the role of a parameter of the method. We 
prove that primal iterates associated with the dual process converge to the 
projection exponentially fast in expectation, and give a formula and an insightful 
lower bound for the convergence rate. We also prove that the same rate applies to 
dual function values, primal function values and the duality gap. Unlike 
traditional iterative methods, SDA converges under no additional assumptions on 
the system (e.g., rank, diagonal dominance) beyond consistency. In fact, our 
lower bound improves as the rank of the system matrix drops. Many existing 
randomized methods for linear systems arise as special cases of SDA, including 
randomized Kaczmarz, randomized Newton, randomized coordinate descent, 
Gaussian descent, and their variants.
2. Remarks on Acceleration for Primal-Dual Algorithms
Antonin Chambolle (antonin.chambolle@cmap.polytechnique.fr) 
Ecole Polytechnique, CNRS, France, Thomas Pock, Pauline Tan
We try to describe what “really” controls the type of quantities which are usually 
bound by accelerated primal-dual first order methods. Exploiting the relationship 
with ADMM type algorithms, we also can deduce relevant bound for some 
objectives of accelerated ADMM methods. This is joint work with T. Pock (Graz, 
Austria) and P. Tan (Palaiseau, France).
3. A Randomized Asynchronous Algorithm for Distributed 
Optimization with Parameter Servers
Lin Xiao (lin.xiao@microsoft.com) Microsoft Research, USA, Adams 
Wei Yu, Qihang Lin, Weizhu Chen
Machine learning with big data often involves big optimization models. For 
distributed optimization over a cluster of machines (each stores its own share of 
data), the number of parameters or optimization variables can be too large for 
frequent communication and synchronization. To overcome this difficulty, we 
can set up dedicated parameter servers, each maintaining a subset of the overall 
model parameters and updating them in an asynchronous distributed manner. 
Such a system may significantly increase our capacity of learning from big data, 
but at the same time, poses new challenges for developing efficient and robust 
distributed algorithms. We propose a DSCOVR (Doubly Stochastic Coordinate 
Optimization with Variance Reduction) algorithm, which exploits the 
simultaneous partitions in both data and model to gain parallelism, and employs 
periodic variance reduction to achieve a fast convergence rate, with small 
communication and computation overhead.

■ Tue.D.5K
Tuesday, 16:30-17:45, Room 5K
First Order Methods and Applications
Cluster: Convex and Nonsmooth Optimization
Session chair: CH Jeffrey Pang
1. An Adaptive Restarting for Universal Gradient Method of 
Minimizing Strongly Convex Functions
Masaru Ito (ito.m@math.cst.nihon-u.ac.jp) Nihon University, Japan
We consider convex optimization problems of minimizing strongly convex 
objective functions with Hölder continuous gradient. In the non-strongly convex 
case, the universal gradient method proposed by Nesterov achieves the optimal 
complexity without knowing the level of smoothness of the objective function. 
We consider a variant of the universal gradient method and propose a restarting 
storategy to adapt the convexity parameter of the objective function. The inputs 
of our method are the initial point, the required accuracy, and the (arbitrary) 
initial guess of the convexity parameter. We show a complexity result of the 
proposed method where the complexity analysis is discussed with respect to the 
norm of “gradient mapping.”
2. Fast Accelerated Proximal Gradient Method and Its Application 
to Unified Classification Algorithm
Naoki Ito (naoki_ito@mist.i.u-tokyo.ac.jp) The University of Tokyo, 
Japan, Akiko Takeda, Kim-Chuan Toh
We develop a fast accelerated proximal gradient (FAPG) method. Our FAPG 
method employs various techniques such as backtracking line search and an 
adaptive restarting scheme in order to speed up the practical convergence. While 
restarting schemes have guaranteed convergence for strongly convex objective 
functions, the proposed algorithm has a rate of convergence O((logk/k)2) even 
for non-strongly convex objective functions, where k is the iteration counter. We 
also provide a practical FAPG, which simplifies the line search procedure, and 
apply it to binary classification. To achieve a good prediction performance in 
binary classification, it is important to find a suitable model for a given dataset. 
Thus, it is desirable to have an efficient unified algorithm for solving different 
classification models to facilitate the evaluation of performance of various 
models for a given dataset. For the purpose, we have provided a unified 
formulation of various binary classification models and apply our FAPG to the 
formulation. Numerical experiments show that our unified algorithm is stable 
and highly competitive to specialized algorithms designed for specific 
classification models.
3. The Supporting Halfspace-quadratic Programming Strategy for 
the Dual of the Best Approximation Problem
CH Jeffrey Pang (matpchj@nus.edu.sg) National University of 
Singapore, Singapore
We consider the best approximation problem (BAP) of projecting a point onto 
the intersection of a number of convex sets. It is known that Dykstraʼs algorithm 
is alternating minimization on the dual problem. We extend Dykstraʼs algorithm 
so that it can be enhanced by the SHQP strategy of using quadratic programming 
to project onto the intersection of supporting halfspaces generated by earlier 
projection operations. By looking at a structured alternating minimization 
problem, we show the convergence rate of Dykstraʼs algorithm when reasonable 
conditions are imposed to guarantee a dual minimizer. We also establish 
convergence of using a warmstart iterate for Dykstraʼs algorithm, show how all 
the results for the Dykstraʼs algorithm can be carried over to the simultaneous 
Dykstraʼs algorithm, and discuss a different way of incorporating the SHQP 
strategy. Lastly, we show that the dual of the best approximation problem can 
have an O(1/k2) accelerated algorithm that also incorporates the SHQP strategy.

■ Tue.D.5L
Tuesday, 16:30-17:45, Room 5L
Algebraic Methods in Polynomial Optimization
Cluster: Conic and Polynomial Optimization
Session organized by: Amir A Ahmadi
1. Spectrahedral Cones with Rank 1 Extreme Rays, Sums of Squares 
and Matrix Completion
Greg Blekherman (greg@math.gatech.edu) Georgia Institute of 
Technology, USA, Rainer Sinn, Mauricio Velasco
A sprectrahedral cone C is a slice of the cone of positive semidefinite matrices 
with a linear subspace L. The ranks of extreme rays of spectrahderal cones have 
been a subject of extensive study. It is natural to ask for what subspaces L do all 
of the extreme rays of C have rank 1? When L is a union of coordinate subspaces 
the answer is given by the PSD Matrix Completion Theorem. It turns out that this 
question has an unexpected connection to algebraic geometry and I will present 
a full classification of such spectrahedral cones. I will also present some related 
new results on matrix completion problems.
2. The Bounded SOS Hierarchy for Bilinear Programming
Ahmadreza Marandi (a.marandi@uvt.nl) Tilburg University, 
Netherlands, Joachim Dahl, Etienne de Klerk
The bounded degree sum-of-squares (BSOS) hierarchy of Lasserre, Toh, and 
Yang [EURO J. Comput. Optim., to appear] constructs lower bounds for a 
general polynomial optimization problem with compact feasible set, by solving a 
sequence of semi-definite programming (SDP) problems. Lasserre, Toh, and 
Yang prove that these lower bounds converge to the optimal value of the original 
problem, under some assumptions. In this talk, we analyze the BSOS hierarchy 
and study its numerical performance on a specific class of bilinear programming 
problems, called pooling problems, that arise in the refinery and chemical process 
industries.
3. Positive Maps and Separable Matrices
Jiawang Nie (njw@math.ucsd.edu) University of California, San 
Diego, USA, Xinzhen Zhang
A linear map between real symmetric matrix spaces is positive if all positive 
semidefinite matrices are mapped to positive semidefinite ones. A real symmetric 
matrix is separable if it can be written as a summation of Kronecker products of 
positive semidefinite matrices. This paper studies how to check if a linear map is 
positive or not and how to check if a matrix is separable or not. We propose 
numerical algorithms, based on Lasserre type semidefinite relaxations, for 
solving such questions. To check the positivity of a linear map, we construct a 
2016_iccopt.indb   76 2016/07/22   11:58:19
ABSTRACTS
ICCOPT 2016 77
hierarchy of semidefinite relaxations for minimizing the associated bi-quadratic 
forms over the unit spheres. We show that the positivity can be detected by 
solving a finite number of such semidefinite relaxations. To check the separability 
of a matrix, we construct a hierarchy of semidefinite relaxations. If it is not 
separable, we can get a mathematical certificate for that; if it is, we can get a 
decomposition for the separability. 

■ Wed.A.1S
Wednesday, 13:45-15:00, Room 1S
MIP + NLP
Cluster: Nonlinear Optimization
Session organized by: Oktay Gunluk
1. Optimization over Structured Subsets of Positive Semidefinite 
Matrices via Column Generation
Sanjeeb Dash (sanjeebd@us.ibm.com) IBM Research, USA, Amir Ali 
Ahmadi, Georgina Hall
We describe algorithms that optimize over certain structured subsets of the cone 
of positive semidefinite matrices (PSD cone) via linear programming and second 
order cone programming. Starting with an initial linear approximation of the 
PSD cone given by Ahmadi and Majumdar (2014), we improve the approximation 
in an iterative fashion using column generation. We apply our techniques to sum-
of-squares (SOS) programming for nonconvex polynomial optimization 
problems, and to a copositive programming relaxation of the stable set problem.
2. Cutting Planes to Strengthen Second Order Conic Relaxation of 
the OPF Problem
Andy Sun (andy.sun@isye.gatech.edu) Georgia Institute of 
Technology, USA, Burak Kocuk, Santanu Subhas Dey
The AC optimal power flow (OPF) problem is a key optimization problem in the 
area of electrical power systems operations. We compare the strength of linear 
programing (LP), second order cone programming (SOCP) and semi-definite 
relaxations (SDP) of two formulations of the OPF formulation. Then we present 
a few families of cutting-planes to strengthen the (standard) SOCP relaxation of 
this problem. The strengthened SOCP relaxation is incomparable to the 
(standard) SDP relaxation. Extensive computational experiments show that 
these relaxations have numerous advantages over existing convex relaxations in 
the literature: (i) their solution quality is extremely close to that of the SDP 
relaxations and consistently outperforms previously proposed convex quadratic 
relaxations of the OPF problem, and (ii) in terms of computation times, the 
strengthened SOCP relaxations can be solved an order of magnitude faster than 
standard SDP relaxations.
3. Solving Box-constrained Nonconvex QPs
Oktay Gunluk (gunluk@us.ibm.com) IBM Research, USA, Pierre 
Bonami, Jeff Linderoth
We discuss effective computational techniques for solving nonconvex quadratic 
programs with box constraints (BoxQP). We first give computational evidence 
that cutting planes obtained from the Boolean Quadric Polytope might be very 
effective at reducing the relaxation gap. We next demonstrate the equivalence 
between the Chvátal-Gomory closure of a natural linear relaxation of BoxQP and 
the relaxation of the Boolean Quadric Polytope consisting of the odd-cycle 
inequalities. By using these cutting planes effectively at nodes of the branch-and-
bound tree, in conjunction with additonal integrality-based branching and a 
strengthened convex quadratic relaxation, we demonstrate that we can effectively 
solve a well-known family of test instances—orders of magnitude faster than 
existing commercial and open-source solvers.

■ Wed.A.1A
Wednesday, 13:45-15:00, Room 1A
Optimization Methods for Inverse Problems 1
Cluster: Nonlinear Optimization
Session organized by: Xin Liu, Yanfei Wang
1. Seismic Diffraction Extraction for Discontinuous Geologies using 
Sparse Regularization
Yanfei Wang (yfwang@mail.iggcas.ac.cn) Chinese Academy of 
Sciences, China, Caixia Yu
Seismic diffractions play an important role in characterizing and identifying 
discontinuous geological structures, such as tiny faults and cavities in Ordovician 
carbonate reservoirs. These faults and cavities are important because of their 
close relationship to the reservoir properties of oil and gas. The seismic responses 
of these objects in the sense of seismic wavelength are encoded in diffractions. 
Since diffractors are usually sparse and non-differentiable, we study diffraction 
extraction based on first and second-order of regularization modeling specially 
for detecting diffraction points. Optimization algorithms are addressed. 
Numerical examples based on synthetic data and field data are given.
2. On a Special Structured Matrix Problem
Cong Sun (suncong86@bupt.edu.cn) Beijing University of Posts and 
Telecommunications, China
A special matrix problem is considered from the application in wireless 
communications. The objective function is approximated by a fraction function. 
The alternating minimization method is applied. Efficient methods are proposed 
for the subproblems as nonconvex quadratic constrained quadratic programming 
and those with orothogonality constraints, where KKT points or optimal solutions 
are guaranteed. Simulations show the good performances of our proposed 
models and algorithms.
3. Semidefinite Penalty Method for Quadratically Constrained 
Quadratic Programming
Ran Gu (guran@lsec.cc.ac.cn) Chinese Academy of Sciences , China, 
Yaxiang Yuan
We present an algorithm for finding approximate global solutions to Quadratically 
Constrained Quadratic Programming with only equality constraints. In our 
method, we add a matrix viable and a penalty matrix to the model. The exactness 
properties of the penalty function is proved. SDP relaxation method can be 
regard as our new model with penalty matrix equaling to zero. We use a fixed 
point method to find a KKT point of the subproblem. Preliminary experiments 
show that our method is easier to obtain global solutions.

■ Wed.A.1B
Wednesday, 13:45-15:00, Room 1B
PDE Optimization and Applications II
Cluster: PDE-constrained Optimization
Session organized by: Tomoya Takeuchi
1. Iterative Thresholding Algorithm for Inverse Source Problems for 
Hyperbolic-Type Equations
Yikan Liu (ykliu@ms.u-tokyo.ac.jp) The University of Tokyo, Japan, 
Daijun Jiang, Masahiro Yamamoto
In this talk, we investigate the reconstruction of the spatial component in the 
source term of hyperbolic-type equations with three kinds of observation data. In 
the cases of partial interior or boundary observation, we prove stability results 
based on newly established Carleman estimates. In case of the final observation, 
we employ the analytic Fredholm theory to show generic well-posedness. 
Numerically, we adopt the classical Tikhonov regularization to reformulate the 
inverse source problems into related optimization problems, for which we 
develop a universal iterative thresholding algorithm by using the corresponding 
adjoint systems. The proposed algorithm is computationally easy and efficient: 
the minimizer at each step has explicit solution. Extensive numerical examples 
are presented to demonstrate the accuracy and efficiency of the proposed 
algorithm.
2. Optimization of Heat Transfer in Plane Couette Flow
Genta Kawahara (kawahara@me.es.osaka-u.ac.jp) Osaka University, 
Japan, Shingo Motoki, Masaki Shimizu
An optimal incompressible steady velocity field has been found numerically for 
heat transfer enhancement with less energy dissipation in plane Couette flow by 
using a variational method. The functional is defined as wall heat flux (scalar 
dissipation), from which energy dissipation has been subtracted, to be optimized 
under the constraints of the continuity equation and the advection-diffusion 
equation for temperature. It is shown that at high Reynolds numbers three-
dimensional (streamwise-dependent) velocity field is optimal, while at low 
Reynolds numbers optimal heat transfer is given by a streamwise-independent 
field. A physical interpretation is proposed for the emergence of three-
dimensional optimal velocity field.
3. Optimal Control Problem for Allen-Cahn Type Equation 
Associated with Total Variation Energy
Takeshi Ohtsuka (tohtsuka@gunma-u.ac.jp) Gunma University, Japan
We introduce an optimal control problem for gradient flow with an energy by 
total variation and a double-well potential with constraint. One observes that the 
2016_iccopt.indb   77 2016/07/22   11:58:19
ABSTRACTS
ICCOPT 201678
equation has singular diffusion and sub differential of the indicator function. For 
this problem we shall the existence of optimal control by the framework of 
variational inequarities. In this talk we also present some numerical results on 
this problem. To overcome the difficulities by the singularities of the equation we 
introduce an approximating problem regularizing the singularities. We derive a 
necessary condition for an optimal control for the approximating problem, and 
introduce a numerical scheme to solve the optimal control problem with this 
approximation proceduler.

■ Wed.A.1C
Wednesday, 13:45-15:00, Room 1C
Theoretical Aspects of Derivative-free Optimization
Cluster: Derivative-free and Simulation-based Optimization
Session organized by: Francesco Rinaldi, Zaikun Zhang
Session chair: Serge Gratton
1. On the Linear Convergence of Comparison-based Step-size 
Adaptive Randomized Search
Anne Auger (anne.auger@inria.fr) INRIA, France, Nikolaus Hansen
This presentation focuses on step-size adaptive randomized search (SARS), a 
general class of randomized derivative-free optimization algorithms that 
iteratively adapt a step-size and a favorite solution. We will present an approach 
to prove the linear convergence of a subclass of SARS algorithms that are 
comparison-based and satisfy scale and translation invariance. This class of 
algorithms includes simplified versions of the state-of-the-art CMA-ES 
algorithm. The approach relies on the following steps: (i) building a normalized 
process underlying the algorithm that derives from the invariance properties 
satisfied by the SARS. This process turns out to be an homogeneous Markov 
chain on scaling-invariant functions and (ii) studying the stability (formally 
irreducibility, positivity, ergodicity) of this Markov chain that entails linear 
convergence of the SARS. This methodology is applied to prove the linear 
convergence of some of the oldest derivative-free stochastic algorithms. 
2. Direct Search Based on Inaccurate Function Values
Serge Gratton (serge.gratton@enseeiht.fr) University of Toulouse, 
IRIT-ENSEEIHT, France, Frédéric Delbos, Benoît Pauwels, Zaikun 
Zhang
We consider direct search applied to a function whose value can only be provided 
by inaccurate oracles. We prove that the classical algorithm without adaptation 
can still achieve a certain accuracy even if the inaccuracy never vanishes, and 
that the worst case complexity to reach such an accuracy is essentially the same 
as the accurate case. We also provide a practical stopping criterion that guarantees 
this accuracy. Based on these results, we address the scenario where we have 
access to oracles with different inaccuracy levels corresponding to different 
costs. We give a simple strategy for choosing oracles according to the progress 
of the algorithm, which ensures the best possible accuracy while calling the low-
cost oracles whenever possible.

■ Wed.A.4A
Wednesday, 13:45-15:00, Room 4A
Data and Networks I
Cluster: Applications in Energy, Science and Engineering
Session organized by: Arvind U Raghunathan
1. Parallel Multi-splitting Proximal Method
Ermin Wei (ermin.wei@northwestern.edu) Northwestern University, 
USA, Yuanzhang Xiao, Chaithanya Bandi
We develop a parallel algorithm based on proximal method to solve the problem 
of minimizing summation of convex (not necessarily smooth) functions. We 
show that this method converges to an optimal solution for any choice of constant 
stepsize for convex objective functions. Under further assumption of Lipschitz-
gradient and strong convexity of objective functions, the method converges 
linearly.
2. Dual Decomposition and Nonsmooth Equations
Arvind U Raghunathan (raghunathan@merl.com) Mitsubishi Electric 
Research Laboratories, USA, Frank E Curtis
Dual Decomposition has been effectively employed for optimization problems 
that possess a nearly-separable structure – objective function, (most) constraints 
are separable in subsets of variables and (a few) constraints couple all the subsets 
of variables. Algorithms that effectively exploit this structure are desired to: (a) 
preserve privacy of problem data (e.g. markets), or (b) employ parallel 
computations, improve scalability. We propose a novel semismooth Newton 
algorithm that works directly on the stationary conditions of the optimization as 
opposed to the dual function. Our approach: (i) retains the separability of dual 
decomposition, (ii) converges locally superlinearly and (iii) is applicable to non-
convex problems. Global convergence of the method is promoted by enforcing 
decrease via a merit function or through a filter mechanism. We provide 
numerical results on solving convex and non-convex instances. Results from 
applications will also be presented. 

■ Wed.A.4B
Wednesday, 13:45-15:00, Room 4B
Vector Variational Inequalities and Applications
Cluster: Complementarity and Variational Inequalities
Session organized by: Shashi Kant Mishra
1. On New Discrete-Type Complementarity Functions
Jein-Shan Chen (jschen@math.ntnu.edu.tw) National Taiwan Normal 
University, Taiwan
It is well known that complementarity functions play an important role in dealing 
with complementarity problems. In this talk, we investigate a few new classes of 
complementarity functions for nonlinear complementarity problems and second-
order cone complementarity problems. The constructions of such new 
complementarity functions are based on discrete generalization which is a novel 
idea in contrast to the continuous generalization of Fischer-Burmeister function. 
Surprisingly, the new families of complementarity functions possess continuous 
differentiability even though they are discrete-oriented extensions. This feature 
enables that some methods like derivative-free algorithm can be employed 
directly for solving nonlinear complementarity problems and second-order cone 
complementarity problems. This is a new discovery to the literature and we 
believe that such new complementarity functions can also be used in many other 
contexts.
2. On Relations between Vector Variational-like Inequalities and 
Vector Optimization Problems in Asplund Spaces
Balendu Bhooshan Upadhyay (bhooshan@nitmanipur.ac.in) National 
Institute of Technology, Manipur, India
In this paper, we consider Minty and Stampacchia type vector variational like 
inequalities in Aplund space setting. Using the properties of Mordukhovich 
limting subdifferentials, we establish the relations between the considered vector 
variational-like inequality problems and nonsmooth vector optimization 
problem. An existence theorem of solutions for the weak Minty vector variational 
inequality is also established. The results presented in the paper are more general 
than those existing in the literature.
3. Online Markovian Decision Problems as a Stochastic Minimax 
Problem
Mengdi Wang (mengdiw@princeton.edu) Princeton University, USA
We consider the online solution of discounted Markovian decision problems 
(MDP). We focus on the black-box model where transition probabilities and 
state transition cost are unknown. Instead, a simulator is available to generate 
random state transitions  under randomized actions. We propose a stochastic 
primal-dual algorithm for solving the minimax formulation of the Bellman 
equation. The algorithm updates by using random sample transitions generated 
by the simulator. We show that the algorithm generates primal and dual iterates 
converging to the optimal value function and optimal policy, respectively. In 
particular, the dual variable produced by using n samples gives a mixed strategy 
for the MDP, achieving an efficiency loss of the order .

■ Wed.A.5D
Wednesday, 13:45-15:00, Room 5D
Computational and Complexity Challenges for Linear 
Conic Optimization
Cluster: Linear Optimization
Session organized by: Miguel Anjos
1. A Random Projection Method for Solving Linear Programs
Leo S Liberti (liberti@lix.polytechnique.fr) CNRS LIX Ecole 
Polytechnique, France, Ky Khac Vu, Pierre-Louis Poirion
The Johnson-Lindenstrauss lemma allows dimension reduction on real vectors 
2016_iccopt.indb   78 2016/07/22   11:58:19
ABSTRACTS
ICCOPT 2016 79
with low distortion on their pairwise Euclidean distances. This result is often 
used in algorithms such as k-means or k-nearest neighbours since they only use 
Euclidean distances, and has sometimes been used in optimization algorithms 
involving the minimization of such distances. We introduce a first attempt at 
using this lemma in the context of linear programming.
2. A Polynomial Column-wise Rescaling von Neumann Algorithm
Tamás Terlaky (terlaky@lehigh.edu) Lehigh University, USA, Dan Li, 
Kees Roos
Recently Chubanov proposed a method which solves homogeneous linear 
equality systems with positive variables in polynomial time. Chubanovʼs method 
can be considered as a column-wise rescaling procedure. We adapt Chubanovʼs 
method to the von Neumann problem, and so we design a polynomial time 
column-wise rescaling von Neumann algorithm. This algorithm is the first 
variant of the von Neumann algorithm with polynomial complexity.
3. Computational Study of Some Valid Inequalities for k-Way Graph 
Partitioning
Miguel Anjos (miguel-f.anjos@polymtl.ca) GERAD & Polytechnique 
Montreal, Canada, Vilmar Jefte Rodrigues de Sousa, Sebastien Le 
Digabel
We consider the maximum k-cut problem that consists in partitioning the vertex 
set of a graph into k subsets such that the sum of the weights of edges joining 
vertices in different subsets is maximized. We focus on identifying effective 
classes of inequalities to tighten the semidefinite programming relaxation. We 
carry out an experimental study of four classes of inequalities from the literature: 
clique, general clique, wheel and bicycle wheel. We considered 10 combinations 
of these classes and tested them on both dense and sparse instances for different 
values of k. Our computational results suggest that the bicycle wheel and wheel 
are the strongest inequalities for k = 3, and that for greater k, the wheel inequalities 
are the strongest by far. Furthermore, we observe an improvement in the 
performance for all choices of k when both bicycle wheel and wheel are used, at 
the cost of 72% more CPU time on average when compared with using only one 
of them.

■ Wed.A.5E
Wednesday, 13:45-15:00, Room 5E
Advances in Robust Optimization I
Cluster: Robust Optimization
Session organized by: Omid Nohadani
1. Numerical Solution of Bilevel Programs using a Duality-based 
Approach
Anil Aswani (aaswani@berkeley.edu) University of California, 
Berkeley, USA, Aurelien Ouattara, Max Zuo-Jun Shen, Auyon Siddiq
Existing numerical algorithms for solving bilevel programs suffer from 
difficulties caused by lack of constraint qualification, lack of differentiability, 
and general nonconvexities. In this talk, we describe a novel reformulation of 
bilevel programs where the (convex) lower level program is replaced by the 
optimality condition of upper-bounding the objective function of the lower level 
program by its dual. This reformulation leads to a single level optimization 
problem with appropriate constraint qualification and differentiability, which 
enables the development of (i) an enumeration algorithm, (ii) a semiparametric 
algorithm, and (iii) a descent algorithm for solving bilevel programs with a 
convex lower level program. This reformulation approach is demonstrated 
through applications to the problems of (1) inverse optimization with noisy data 
(e.g., estimating utility functions from noisy measurements of agent decisions), 
and (2) solving Stackelberg routing games.
2. Accounting for the Tongue-and-Groove Effect in IMRT Treatment 
Planning using a Robust Direct Aperture Optimization Approach
Edwin Romeijn (edwin.romeijn@isye.gatech.edu) Georgia Institute 
of Technology, USA, Ehsan Salari, Chunhua Men
Traditionally, the tongue-and-groove effect due to the multileaf collimator 
architecture in intensity-modulated radiation therapy (IMRT) has typically been 
deferred to the leaf sequencing stage. We propose a new direct aperture 
optimization method for IMRT treatment planning that explicitly incorporates 
dose calculation inaccuracies due to the tongue-and-groove effect into the 
treatment plan optimization stage. Using lower and upper bounds on the dose 
distribution delivered to the patient, we develop a model that yields a treatment 
plan that is robust with respect to the dose calculation inaccuracies. Tests on a set 
of ten clinical head-and-neck cancer cases demonstrate the effectiveness of the 
new method in developing robust treatment plans with tight dose distributions in 
targets and critical structures.
3. Robust Maximum Likelihood Estimation with Application to 
Radiation Therapy
Omid Nohadani (nohadani@northwestern.edu) Northwestern 
University, USA, Dimitris Bertsimas
In many applications, statistical estimators serve to derive conclusions from data, 
most prominently in finance, medical decision-making and clinical trials. 
However, the conclusions are typically dependent on uncertainties in the data. 
We use robust optimization principles to construct robust maximum likelihood 
estimators that are immune against data errors. Both error types are considered: 
a) adversarial type, modeled using the notion of uncertainty sets, and b) 
probabilistic type, modeled by distributions. We provide efficient local and 
global search algorithms to compute the robust estimators and discuss them in 
details for the case of multivariate normally distributed data. The estimator 
performance is demonstrated on two datasets. First, using computer simulations, 
we demonstrate that the proposed estimators are robust against both types of data 
uncertainty and provide significantly more accurate estimates, compared to 
classical estimators which degrade significantly, when errors are encountered. 
We establish a range of uncertainty sizes, for which robust estimators are 
superior. Second, we analyze deviations in cancer radiation therapy planning. 
Uncertainties amongst plans are caused by patientsʼ individual anatomies and the 
trial-and-error nature of the process. Robust estimators prove to result in more 
reliable decisions when applied to a large set of past treatments.

■ Wed.A.5F
Wednesday, 13:45-15:00, Room 5F
Mathematical Programming and Economic Equilibria
Cluster: Multi-Objective and Vector Optimization
Session organized by: Jugal Garg
1. Computation of Fisher-Gale Equilibrium by Auction
Vladimir Shikhman (vladimir.shikhman@uclouvain.be) Catholic 
University of Louvain, Belgium, Yurii Nesterov
We study the Fisher model of a competitive market from the algorithmic 
perspective. For that, the related convex optimization problem due to Gale and 
Eisenberg is used. The latter problem is known to yield a Fisher equilibrium 
under some structural assumptions on consumersʼ utilities, e.g. homogeneity of 
degree 1, homotheticity etc. Our goal is to examine the applicability of the 
convex optimization framework by departing from these traditional assumptions. 
We just assume the concavity of consumersʼ utility functions. For this case we 
suggest a novel concept of Fisher-Gale equilibrium by introducing consumersʼ 
utility prices. The prices of utility transfer the utility of a consumption bundle to 
a common numeraire. We develop a subgradient-type algorithm from Convex 
Analysis to compute a Fisher-Gale equilibrium. In order to decentralize prices, 
we additionally implement the auction design, i.e. consumers settle and update 
their individual prices and producers sell at the highest offer price. Our price 
adjustment is based on a tatonnement procedure, i.e. the prices change 
proportionally to consumersʼ individual excess supplies. Historical averages of 
consumption are shown to clear the market of goods. Our algorithm enjoys a 
convergence rate. In worst case, the number of price updates needed to achieve 
the ε-tolerance is proportional to 1/ε2.
2. Polynomial-Time Complementary Pivot Algorithms for Market 
Equilibria
Jugal Garg (jugal.garg@gmail.com) University of Illinois at Urbana 
Champaign, USA, Ruta Mehta, Milind Sohoni, Nisheeth Vishnoi
We consider the problem of computing market equilibria in the Fisher model for 
utility functions such as linear, spending constraint and perfect price-
discrimination. In each case we start with a convex program that captures market 
equilibria, and in a systematic way, covert it into a linear complementary problem 
(LCP) formulation. To obtain a polynomial-time algorithm, we depart from 
previous approaches of pivoting on a single polyhedron associated with the LCP. 
Instead, we carefully construct a polynomial-length sequence of polyhedra, one 
containing the other, such that starting from an optimal solution to one allows us 
to obtain an optimal solution to the next in the sequence in a polynomial number 
of complementary pivot steps.
3. Price-taking Equilibrium in Games
Joseph M Ostroy (ostroy@ucla.edu) University of California, Los 
Angeles, USA, Joon Song
Similarities between price-taking utility maximization for Walrasian equilibrium 
in quasilinear models of exchange and correlated equilibrium in normal form 
games are demonstrated as expressions of conjugate duality. Similarities between 
2016_iccopt.indb   79 2016/07/22   11:58:19
ABSTRACTS
ICCOPT 201680
tatonnement methods of convergence in exchange extend to games.

■ Wed.A.5G
Wednesday, 13:45-15:00, Room 5G
Algorithmic and Geometric Aspects of Linear 
Optimization
Cluster: Linear Optimization
Session organized by: Antoine Deza
1. Improving Bounds on the Diameter of a Polyhedron in High 
Dimensions
Noriyoshi Sukegawa (sukegawa@ise.chuo-u.ac.jp) Chuo University, 
Japan
In 1992, Kalai and Kleitman proved that the diameter of a d-dimensional 
polyhedron with n facets is at most nlog(d)+2. Recently, in 2014, Todd improved 
Kalai–Kleitman bound to (n - d)log(d), which is tight in low dimensions. In this 
talk, we introduce a proof method to improve the bounds in high dimensions. It 
proves, for example, a bound which improves Todd bound for d ≥ 7, and by three 
orders of magnitude in high dimensions. 
2. On the Diameter of Lattice Polytopes
George Oreste Manoussakis (gomanous@gmail.com) University 
Paris Sud, France, Antoine Deza, Shmuel Onn
A lattice (d,k)-polytope is the convex hull of a set of points in dimension d whose 
coordinates are integers between 0 and k. Let D(d,k) be the maximum possible 
edge-diameter over all lattice (d,k)-polytopes. Naddef showed in 1989 that 
D(d,1) = d, Kleinschmidt and Onn generalized this result in 1992 showing that 
D(d,k) is at most kd, before Del Pia and Michini strengthened in 2016 the upper 
bound for D(d,k) to at most  and showed that . D(2,k) 
was investigated independently in the early nineties by Thiele, Balog and Barany, 
and Acketa and Zunic. We introduce a family of lattice polytopes whose diameter 
achieves D(2,k) in dimension 2 and achieves (k+1)d/2 for any d and k = 2d - 1.
3. Star Sets/Star Complements of Graph Eigenvalues and Simplex 
Like Techniques in Combinatorial Problems
Domingos Moreira Cardoso (dcardoso@ua.pt) University of Aveiro, 
Portugal, Carlos J Luz, Maria F Pacheco
There are many combinatorial problems in graphs equivalent to the recognition 
of a (k,t)-regular set (a vertex subset S inducing a k-regular subgraph such that 
every vertex not in S has t neighbors in it). Considering a graph H, if k-t is not an 
adjacency eigenvalue (an eigenvalue of H), then to decide whether H has a (k,t)-
regular set is easy, otherwise this problem can be hard. When h is an eigenvalue 
of H with multiplicity q, a vertex subset X with cardinality q such that h is not an 
eigenvalue of H - X is called an h-star set of H, while H - X is an h-star 
complement. We deal with the vertex set of a (k - t)-star complement H - X as a 
basis in a simplex-like approach to the determination of (k,t)-regular sets. Several 
combinatorial and spectral properties of (k,t)-regular sets are summarized and a 
simplex like algorithm based on a sequence of star complements, towards to a 
star complement which includes a (0,t)-regular set, when there exists, is 
presented.

■ Wed.A.5H
Wednesday, 13:45-15:00, Room 5H
Robust Portfolio Optimization
Cluster: Applications in Finance and Economics
Session organized by: Jun-ya Gotoh
1. A Robust Perspective on Transaction Costs in Portfolio 
Optimization
Alba Victoria Olivares-Nadal (aolivares@us.es) University of Seville, 
Spain, Víctor DeMiguel
We show how to use a transaction cost term in a portfolio optimization problem 
to compute portfolios that are both efficient in terms of transaction costs and 
robust to estimation error. Theoretically, we prove that the portfolio problem 
with transaction costs is equivalent to three different problems designed to 
alleviate the impact of estimation error: a robust portfolio optimization problem, 
a robust regression problem, and a Bayesian portfolio problem. Motivated by 
these results, we propose a data-driven approach to portfolio optimization that 
calibrates the transaction cost term taking into account both transaction costs and 
estimation error. Our empirical results demonstrate that the data-driven portfolios 
perform favorably because they strike an optimal trade-off between rebalancing 
the portfolio to capture the information in recent historical return data, and 
avoiding the large transaction costs and impact of estimation error associated 
with excessive trading. 
2. Higher Factor Dependency of Robust Portfolios for Achieving 
Robustness
Jang Ho Kim (janghokim@khu.ac.kr) Kyung Hee University, Korea, 
Woo Chang Kim, Frank J Fabozzi
Robust portfolio optimization resolves the sensitivity of mean-variance portfolios 
and thus allows portfolios to achieve robust performance. There have been many 
developments on formulating robust portfolios, and common formulations 
include uncertainty sets on expected security returns defined as intervals or 
ellipsoids. In this study, we focus on these formulations to analyze their attributes, 
mainly the dependency on factors of robust portfolios. We find that robust 
portfolios show higher dependency to the Fama-French three factors and the 
principal components of the data. Furthermore, it is observed that robust 
formulations that penalize variance of returns show higher factor dependency as 
well as increased robustness. We provide empirical and analytical support for 
this finding, which may explain how robust formulations achieve robustness.
3. Robust Empirical Optimization
Andrew Lim (andrewlim@nus.edu.sg) National University of 
Singapore, Singapore, Jun-ya Gotoh, Michael Jong Kim
We analyze the out of sample performance of robust empirical optimization and 
introduce a data driven method for calibrating the robust problem. Applications 
to finance will be discussed.

■ Wed.A.5I
Wednesday, 13:45-15:00, Room 5I
Stochastic Optimization: Theory and Algorithms
Cluster: Stochastic Optimization
Session chair: Mariusz Michta
1. Riemannian Stochastic Variance Reduced Gradient on Grassmann 
Manifold
Hiroyuki Kasai (kasai@is.uec.ac.jp) The University of Electro-
Communications, Japan, Hiroyuki Sato, Bamdev Mishra
Stochastic variance reduction algorithms have recently become popular for 
minimizing the average of a large, but finite, number of loss functions. To the 
best of our knowledge, all the earlier algorithms are proposed in the Euclidean 
space. We propose a novel Riemannian extension of the Euclidean stochastic 
variance reduced gradient algorithm to a compact manifold search space. To this 
end, we show the developments on the Grassmann manifold. The key challenges 
of averaging, addition, and subtraction of multiple gradients are addressed with 
notions like logarithm mapping and parallel transport of vectors on the 
Grassmann manifold. We show that the proposed method generates globally 
convergent sequences under some natural assumptions. The proposed algorithm 
is applied to a number of regression problems on the Grassmann manifold like 
principal components analysis, low-rank matrix completion, and the Karcher 
mean computation. In all these cases, the proposed algorithm outperforms the 
Riemannian stochastic gradient descent algorithm. 
2. Strong Convexity in Two-Stage Linear Stochastic Programs with 
Partially Random Right-Hand Side
Kai Arne Spürkel (kai.spuerkel@uni-due.de) University of Duisburg-
Essen, Germany
In the literature, analysis of structure and stability of two-stage linear programs 
is usually done assuming random right-hand side (RHS), i.e. no constants 
occurring among the RHS-components. For differentiability and strong 
convexity with respect to the full tender-variable randomness of the full RHS is 
required. The talk addresses the relaxation of this condition to partially random 
RHS, presents geometric insights and conclusions on model-structure.
3. Properties of Weak Solutions to Stochastic Inclusions and Their 
Applications in Optimization Problems
Mariusz Michta (m.michta@wmie.uz.zgora.pl) University of Zielona 
Gora, Poland
Stochastic inclusions appear in a natural way as a reduced or theoretical 
description of stochastic control problems (see e.g. [1-6] and ref. therein). The 
talk deals with properties of weak solutions to stochastic inclusions via a 
martingale problem approach. Such approach is used first to analyze compactness 
and continuous dependence of solution sets to stochastic differential inclusions 
2016_iccopt.indb   80 2016/07/22   11:58:20
ABSTRACTS
ICCOPT 2016 81
of Ito type with convex integrands on the initial distributions. Next the problem 
of existence of optimal weak solutions to such inclusions and their dependence 
on initial distributions is discussed. Finally, we show that this general approach 
can be applied to some financial and economic optimization problems. 
References [1] M. Kisielewicz, Stochastic Differential Inclusions and 
Applications, Springer, New York, 2013. [2] M. Kisielewicz, M. Michta, J. Motyl, 
Set-valued approach to stochastic control. Part I. Existence and regularity 
properties, Dynam. Systems Appl., 12:405-431, 2003. [3] M. Michta, Optimal 
solutions to stochastic differential inclusions, Appl. Math. 29(4) (2002) 387-398. 
[4] M. Michta, On weak solutions to stochastic differential inclusions driven by 
semimartingales, Stoch. Anal. Appl. 22(5) (2004) 1341-1361. [5] M. Michta, J. 
Motyl, Stochastic inclusion with a non-lipschitz right hand side, In: Stochastic 
Differential Equations, Ed. N. Halidas, Nova Science Publ. (2011).

■ Wed.A.5J
Wednesday, 13:45-15:00, Room 5J
Matrix Optimization Problems: Recent Advances in 
Convergence Rate Analysis and Recovery Guarantees
Cluster: Conic and Polynomial Optimization
Session organized by: Anthony Man-Cho So
1. Convex Optimization Learning of Faithful Euclidean Distance 
Representations in Nonlinear Dimensionality Reduction
Chao Ding (dingchao@amss.ac.cn) Chinese Academy of Sciences, 
China, Houduo Qi
Classical multidimensional scaling only works well when the noisy distances 
observed in a high dimensional space can be faithfully represented by Euclidean 
distances in a low dimensional space. Advanced models such as Maximum 
Variance Unfolding (MVU) and Minimum Volume Embedding (MVE) use 
Semi-Definite Programming (SDP) to reconstruct such faithful representations. 
While those SDP models are capable of producing high quality configuration 
numerically, they suffer two major drawbacks. One is that there exist no 
theoretically guaranteed bounds on the quality of the configuration. The other is 
that they are slow in computation when the data points are beyond moderate size. 
In this talk, we propose a convex optimization model of Euclidean distance 
matrices. We establish a non-asymptotic error bound for the random graph model 
with sub-Gaussian noise, and prove that our model produces a matrix estimator 
of high accuracy when the order of the uniform sample size is roughly the degree 
of freedom of a low-rank matrix up to a logarithmic factor. Our results partially 
explain why MVU and MVE often work well. Moreover, we develop a fast 
inexact accelerated proximal gradient method. Numerical experiments show that 
the model can produce configurations of high quality on large data points that the 
SDP approach would struggle to cope with.
2. Quadratic Optimization with Orthogonality Constraints: Explicit 
Lojasiewicz Exponent and Linear Convergence of Line-Search 
Methods
Huikang Liu (hkliu@se.cuhk.edu.hk) The Chinese University of Hong 
Kong, Hong Kong, Weijie Wu, Anthony Man-Cho So
A fundamental class of matrix optimization problems that arise in many areas of 
science and engineering is that of quadratic optimization with orthogonality 
constraints. Such problems can be solved using line-search methods on the 
Stiefel manifold, which are known to converge globally under mild conditions. 
To determine the convergence rates of these methods, we give an explicit 
estimate of the exponent in a Łojasiewicz inequality for the (non-convex) set of 
critical points of the aforementioned class of problems. This not only allows us 
to establish the linear convergence of a large class of line-search methods but 
also answers an important and intriguing problem in mathematical analysis and 
numerical optimization. A key step in our proof is to establish a local error bound 
for the set of critical points, which may be of independent interest.
3. A Unified Approach to Error Bounds for Structured Convex 
Optimization
Zirui Zhou (zrzhou@se.cuhk.edu.hk) The Chinese University of Hong 
Kong, Hong Kong, Anthony Man-Cho So
Error bounds, which refer to inequalities that bound the distance of vectors in a 
test set to a given set by a residual function, have proven to be extremely useful 
in analyzing the convergence rates of a host of iterative methods for solving 
optimization problems. In this paper, we present a new framework for establishing 
error bounds for a class of structured convex optimization problems, in which the 
objective function is the sum of a smooth convex function and a general closed 
proper convex function. Such a class encapsulates not only fairly general 
constrained minimization problems but also various regularized loss 
minimization formulations in machine learning, signal processing, and statistics. 
Using our framework, we show that a number of existing error bound results can 
be recovered in a unified and transparent manner. To further demonstrate the 
power of our framework, we apply it to a class of nuclear-norm regularized loss 
minimization problems and establish a new error bound for this class under a 
strict complementarity-type regularity condition. We then complement this result 
by constructing an example to show that the said error bound could fail to hold 
without the regularity condition. We believe that our approach will find further 
applications in the study of error bounds for structured convex optimization 
problems.

■ Wed.A.5L
Wednesday, 13:45-15:00, Room 5L
Moments, Positive Polynomials & Optimization: Part 
III
Cluster: Conic and Polynomial Optimization
Session organized by: Jiawang Nie, Jean B Lasserre
1. On Stability and Genericity Results for Polynomial Optimization 
Problems 
Gue Myung Lee (gmlee@pknu.ac.kr) Pukyong National University, 
Korea, Tien Son Pham 
In this talk, we consider the class of polynomial optimization problems, in which 
the objective functions are perturbed, and stability results of the global solution 
map, of the Karush-Kuhn-Tucker set-valued map, and of the optimal value 
function for all problems in the class. Moreover, we present genericity results 
which hold almost every polynomial optimization problem. 
2. Globally Solving Polynomial Mathematical Programs with 
Equilibrium Constraints
Jeya Jeyakumar (v.jeyakumar@unsw.edu.au) University of New 
South Wales, Australia
In this talk, as an application of a powerful algebraic technique, that employs 
Putinar positivestellentz and semidefinite linear programming (SDP), we 
establish convergent SDP relaxations to globally solving nonconvex polynomial 
optimization problems with broad classes of complex constraints, such as 
generalized equilibrium constraints and bi-level constraints. We first show that 
the global optimal value of a polynomial mathematical program with a 
generalized equilibrium constraint is the limiting value of a sequence of optimal 
values of a hierarchy of its semidefinite linear programming (SDP) relaxations. 
Consequently, we obtain convergent SDP hierarchies for globally solving 
polynomial mathematical programs with equilibrium constraints and for solving 
various classes of global bilevel polynomial mathematical programs. 
3. Convergent Robust SDP Approximations for Semialgebraic 
Optimization
Victor Liev Magron (victor.magron@imag.fr) CNRS VERIMAG, 
France
We present a new hierarchy of convergent robust semidefinite (SDP) 
approximations for certain classes of semialgebraic optimization problems. This 
hierarchy yields a monotone non-increasing sequence of upper bounds 
converging to the global minimum of a polynomial f over a simple compact 
semialgbraic set (e.g. box or simplex) K = X × E, in the case when f has linear 
dependency on the variables in E. By contrast with the converging sequence of 
SDP upper bounds in [J.B. Lasserre, A new look at nonnegativity on closed sets 
and polynomial optimization, SIAM J. Optim. 21, pp. 864–885, 2010], we prove 
that nonnegativity of f over K is equivalent to semidefinite positiveness of 
countably many uncertain moment matrices, with perturbations defined over E. 
Each resulting robust program in this hierarchy can be exactly solved via SDP by 
using [L. El Ghaoui, F. Oustry and H. Lebret, Robust solutions to uncertain 
semidefinite programs, SIAM J. Optim. 9, pp. 33–52, 1998]. This methodology 
is successfully applied to obtain lower bounds on absolute roundoff errors 
occurring while implementing numerical programs with finite-precision. In this 
context, we illustrate the performance and precision of our robust semidefinite 
approximations on non-trivial programs coming from biology, optimization and 
space control.

■ Wed.A.m3S
Wednesday, 13:45-15:00, Room m3S
Recent Advances in First-Order Methods: Part I
Cluster: Convex and Nonsmooth Optimization
Session organized by: Marc Teboulle, Shoham Sabach
1. Sequential Convex Programming, Value Function and 
2016_iccopt.indb   81 2016/07/22   11:58:20
ABSTRACTS
ICCOPT 201682
Convergence
Edouard Pauwels (edouard.pauwels@irit.fr) IRIT, France, Jérôme 
Bolte
Many iterative processes in nonlinear optimization rely on sequentially solving 
simple approximate models of a more difficult problem. In this work, we focus 
on complex geometric settings for which constraints or non smoothness cannot 
be dealt with directly and must be approximated. In this context, sequential 
convex programming approaches consist in producing sequences of iterates 
based on solutions of local convex approximate problems (e.g. SQP, Gauss-
Newton, ...). Contrary to favorable geometric settings (e.g. proximal 
decomposition), the convergence of iterates produced by these types of methods 
is hardly understood. We address this question under the hypothesis that problem 
data is semi-algebraic, a mild and easy to check assumption which admits many 
extensions and encompasses most problems met in practice. The key insight of 
the analysis relies on the introduction of the value function and the understanding 
of sequential convex programs as implicitly performing approximate gradient 
steps for which convergence is well understood.
2. On Computing the Proximal Mapping Associated with the `0-
Norm over Symmetric Sets
Nadav Hallak (ndvhllk@campus.technion.ac.il) Technion, Israel, 
Amir Beck
We consider the problem of computing the proximal mapping associated with a 
symmetric function, in particular with a function composed of a sparsity term 
and the indicator function of a symmetric set. We study the properties of the prox 
mapping and derive an efficient method for computing the prox associated with 
the `0-norm and the indicator function for a symmetric set. We show that under a 
property called “second order monotonicity” (SOM), the problem can be solved 
via a binary search method. Finally, we show that interesting sets such as the 
simplex and the `1, `2, `∞ balls, satisfy the SOM property.
3. Beyond Lipschitz Gradient Continuity: A Novel Path for First 
Order Methods
Marc Teboulle (teboulle@post.tau.ac.il) Tel Aviv University, Israel, 
Heinz H Bauschke, Jerome Bolte
The proximal gradient and its variants is one of the most attractive first order 
algorithm for minimizing the sum of two convex functions, with one being 
nonsmooth. However, it requires the differentiable part of the objective to have a 
Lipschitz continuous gradient, thus precluding its use in many applications. We 
introduce a simple and elegant framework which allows to circumvent the 
intricate question of Lipschitz continuity of the gradient. This translates into a 
new descent lemma which natuarally leads to first oder methods with a proven 
global sublinear rate of convergence. This opens a new path to tackle a broad 
spectrum of problems arising in key applications which were until now, 
considered as out of reach via proximal gradient methods. We illustrate this 
potential by showing how our results can be applied to derive new and simple 
schemes in some applications.

■ Wed.A.m3AB
Wednesday, 13:45-15:00, Room m3AB
Advances in Deterministic Global Optimization I
Cluster: Global Optimization
Session organized by: Ruth Misener
1. Convergence-Order Analysis of Lower Bounding Schemes for 
Constrained Global Optimization Problems
Rohit Kannan (rohitk@mit.edu) Massachusetts Institute of 
Technology, USA, Paul I Barton
The performance of branch-and-bound algorithms for continuous global 
optimization is strongly dependent on the ability to construct tight and rapidly 
convergent schemes of lower bounds. A popular technique for constructing 
lower bounding schemes is to replace the nonconvex functions involved in the 
optimization problem with schemes of (convex and concave) relaxations. 
Recently, Bompadre and coworkers (JOGO, 52(1):1–28, 2012 and JOGO, 
57(1):75–114, 2013) analyzed the propagation of convergence orders of 
McCormick, Taylor, and McCormick-Taylor model relaxations. They derived 
sufficient conditions for schemes of relaxations constructed using the above 
models to have second-order pointwise convergence, which has important 
implications towards mitigating the cluster problem in unconstrained global 
optimization. In this talk, we propose a definition of convergence order for lower 
bounding schemes for constrained problems, and analyze the convergence orders 
of widely applicable full-space and reduced-space lower bounding schemes for 
continuous global optimization. A recent analysis of the cluster problem in 
constrained global optimization is used to evaluate the effectiveness of these 
schemes.
2. Enhancing the Performance of BASBL: Branch-And-Sandwich 
BiLevel Solver with the Adaptive Branching, Domain Reduction 
and Parallel Computing Schemes
Remigijus Paulavicius (remigijus.paulavicius@imperial.ac.uk) 
Imperial College London, United Kingdom, Nikos Kazazakis, Polyxeni 
M Kleniati, Claire S Adjiman
In this talk, we enhance the performance of the recently presented BASBL, a 
Branch-And-Sandwich algorithm [1, 2] based BiLevel solver [3] for nonlinear 
bilevel problems, implemented within the MINOTAUR toolkit [4]. First, the 
BASBL solver is extended to include adaptive branching schemes where at some 
stages of the algorithm branching is allowed only on a subset of the variables. 
Further, we introduce and incorporate a domain reduction scheme for bilevel 
problems. We evaluate the impact of the introduced enhancements on a set of 
nonconvex bilevel problems from a test library. Finally, we will briefly present 
an initial parallel approach for shared memory multi-core computers and will 
compare BASBL performance by using different threading frameworks. 
References [1] Kleniati, P.-M., Adjiman, C.S., Branch-and-Sandwich: a 
deterministic global optimization algorithm for optimistic bilevel programming 
problems. Part I: Theoretical development, J Glob Optim, 60(3), 425–458 
(2014). [2] Kleniati, P.-M., Adjiman, C.S., Branch-and-Sandwich: a deterministic 
global optimization algorithm for optimistic bilevel programming problems. Part 
II: Convergence analysis and numerical results, J Glob Optim, 60(3), 459–481 
(2014). [3] BASBL solverʼs homepage: http://basblsolver.github.io/home/ [4] 
MINOTAUR toolkit homepage: https://wiki.mcs.anl.gov/minotaur/index.php/
MINOTAUR
3. A Parametric Approach to Solving the Pooling Problem
Radu  Baltean-Lugojan (rb2309@ic.ac.uk) Imperial College London, 
United Kingdom, Ruth Misener
We develop an algorithm that solves specialized pooling problem instances to 
global optimality and integrate it within a Branch and Bound framework for 
more generic instances. The approach parameterizes the optimization problem 
with respect to the pool concentration variables and uncovers embedded sparsity 
and polyhedral/topological properties for a variety of instances. The presentation 
generalizes and extends recent work analyzing computational complexity of the 
pooling problem [Boland et al. 2015, Haugland 2016]. Our analysis also 
integrates source-to-output streams and both upper and lower bounds on the 
network parameters.

■ Wed.B.1S
Wednesday, 15:15-16:30, Room 1S
Optimization Methods and Its Applications
Cluster: Nonlinear Optimization
Session organized by: Cong Sun
1. Column-wise Block Coordinate Descent Approach for Orthogonal 
Constrained Optimization Problems
Xin Liu (liuxin@lsec.cc.ac.cn) Chinese Academy of Sciences, China, 
Bin Gao, Xiaojun Chen, Ya-xiang Yuan
We propose a column-wise block coordinate descent approach for solving a class 
of orthogonal constrained optimization problems. This approach combines a 
Gauss-Sedeil type of iteration with a multiplier symmetrization step to guarantee 
the stationarity satisfied at any cluttering point. We prove the global convergence 
of the proposed approach. Preliminary experiments illustrate that the new 
algorithm performs well and is of great potential.
2. Lp-Norm Regularization Algorithms for Optimization over 
Permutation Matrices
Bo Jiang (jiangbo@njnu.edu.cn) Nanjing Normal University, China, 
Ya-Feng Liu, Zaiwen Wen
Optimization problems over permutation matrices appear widely in facility 
layout, chip design, scheduling, pattern recognition, computer vision, graph 
matching, etc. Since this problem is NP-hard due to the combinatorial nature of 
permutation matrices, we relax the variable to be the more tractable doubly 
stochastic matrices and add an Lp-norm (0 < p < 1) regularization term to the 
objective function. The optimal solutions of the Lp-regularized problem are the 
same as the original problem if the regularization parameter is sufficiently large. 
A lower bound estimation of the nonzero entries of the stationary points and 
some connections between the local minimizers and the permutation matrices are 
further established. Then we propose an Lp regularization algorithm with local 
refinements. The algorithm approximately solves a sequence of Lp regularization 
2016_iccopt.indb   82 2016/07/22   11:58:20
ABSTRACTS
ICCOPT 2016 83
subproblems by the projected gradient method using a nonmontone line search 
with the Barzilai-Borwein step sizes. Its performance can be further improved if 
it is combined with certain local search methods, the cutting plane techniques as 
well as a new negative proximal point scheme. Extensive numerical results on 
QAPLIB and the bandwidth minimization problem show that our proposed 
algorithms can often find reasonably high quality solutions within a competitive 
amount of time.
3. A Quadratically Convergent Regularized Semismooth Newton 
Method for Nonlinear Equations under Error Bound Conditions
Qingna Li (qnl@bit.edu.cn) Beijing Institute of Technology, China, Qi 
Zhang, Xueying Ni, Anthony Man-Cho So
In this paper, we propose an inexact regularized semismooth Newton method for 
solving the nonsmooth equations, strongly motivated from minimizing a 
nonsmooth convex function θ = f+g in machine learning fields such as l1 
regularized problem. By exploiting the strongly semismoothness of F and error 
bound assumption, the proposed algorithm has a local quadratically convergence 
rate. Further, A globalized version is proposed while maintaining the local 
quadratic convergence rate. Simulation results demonstrate the efficiency of the 
method. 

■ Wed.B.1A
Wednesday, 15:15-16:30, Room 1A
Optimization Methods for Inverse Problems 2
Cluster: Nonlinear Optimization
Session organized by: Xin Liu, Yanfei Wang
1. Inverse Max+Sum Spanning Tree Problem under Hamming 
Distance by Modifying the Sum-Cost Vector
Xiucui Guan (101010763@seu.edu.cn) Southeast University, China, 
Xinyan He, Binwu Zhang
On an undirected network G(V,E,c,w), a cost c(e) and a weight w(e) are 
prescribed for each e. The max+sum spanning tree (MSST) problem is to find a 
spanning tree T* which makes the combined weight max w(e)+sum c(e) as small 
as possible. Whereas, in an inverse MSST problem, T0 is a given spanning tree, 
which is not an optimal MSST. We modify c to  so that T0 becomes an optimal 
MSST of the new network . The goal is to minimize the cost 
incurred by modifying c under Hamming Distance. First, we present a 
mathematical model for the inverse MSST problem and a method to check the 
feasibility. Then, under the weighted bottleneck-type Hamming distance, we 
design a binary search algorithm. Next, under the unit sum-type Hamming 
distance, which is also called l0 norm, we show that the inverse problem IMSST0 
is NP-hard. Assuming NP , IMSST0 is not approximable 
within a factor of 2log1-εm, for any ε > 0. Finally, we consider the augmented 
problem AIMSST0, whose objective function is M||x||0+||x||1. We show that the 
augmented problem and the l1 norm problem have the same Lagrange dual 
problems. Therefore, the l1 norm problem is the closest convex relaxation of the 
AIMSST0, which has the same optimal solution as that of the inverse problem 
IMSST0.
2. Linear Convergence of Proximal Gradient Algorithm with 
Extrapolation for a Class of Nonconvex Nonsmooth Minimization 
Problems
Bo Wen (bo.wen@connect.polyu.hk) Harbin Institute of Technology/
Hong Kong Polytechnic University, Hong Kong, Xiaojun Chen, Ting 
Kei Pong
In this paper, we study the proximal gradient algorithm with extrapolation for 
minimizing the sum of a Lipschitz differentiable function and a proper closed 
convex function. Under the error bound condition, we show that there exists a 
threshold such that if the extrapolation coefficients are chosen below this 
threshold, then the sequence generated converges R-linearly to a stationary point 
of the problem. Moreover, the corresponding sequence of objective values is also 
R-linearly convergent. In addition, the threshold reduces to 1 for convex problems 
and, as a consequence, we obtain the R-linear convergence of the sequence 
generated by the FISTA with the fixed restart. Finally, again for convex problems, 
we show that the successive changes of the iterates vanish for many choices of 
sequences of extrapolation coefficients that approach the threshold. In particular, 
this conclusion can be shown to hold for the sequence generated by the FISTA.

■ Wed.B.1B
Wednesday, 15:15-16:30, Room 1B
PDE-constrained Optimization in Electromagnetism
Cluster: PDE-constrained Optimization
Session organized by: Fredi Tröltzsch, Irwin Yousept
1. Optimization of Non-smooth Hyperbolic Evolution Maxwellʼs 
Equations in Type-II Superconductivity
Irwin Yousept (irwin.yousept@uni-due.de) University Duisburg-
Essen, Germany
This talk presents recent results on the optimization of an evolution 
electromagnetic process in type-II superconductivity. The optimization problem 
is to find an optimal applied current density, which steers the electromagnetic 
fields to the desired ones in the presence of a type-II superconductor. The 
governing PDE system for the electromagnetic fields consists of hyperbolic 
evolution Maxwellʼs equations with a non-smooth constitutive law for the 
electric field and the current density based on the Bean critical-state model. We 
develop a rigorous mathematical theory including an existence analysis and first-
order necessary optimality conditions for the non-smooth PDE-constrained 
optimization problem.
2. Sensitivity-based Topology and Shape Optimization of an Electric 
Motor
Peter Gangl (gangl@numa.uni-linz.ac.at) Johannes Kepler University 
Linz, Austria, Ulrich Langer, Kevin Sturm, Antoine Laurain, Samuel 
Amstutz
We consider the design optimization of an electric motor by means of sensitivity-
based topology and shape optimization. The optimization problem is subject to 
the equations of nonlinear two-dimensional magnetostatics. The shape derivative 
of a domain-dependent functional provides information about its sensitivity with 
respect to a change of the shape of the underlying domain. On the other hand, the 
topological derivative gives information about the sensitivity of the functional 
with respect to the introduction of a hole or of an inclusion of a different material. 
We aim at finding the optimal distribution of ferromagnetic material in a design 
subregion of the computational domain by means of these two kinds of 
sensitivities. In the course of the optimization procedure, the interface between 
the different materials evolves. We present an easy to implement locally modified 
fitted finite element method that allows to resolve the interface exactly and show 
optimal convergence independent of the location of the interface relative to the 
mesh.
3. Optimal Control of Some Quasilinear Parabolic Maxwell 
Equations
Fredi Tröltzsch (troeltzsch@math.tu-berlin.de) Technische Universität 
Berlin, Germany, Serge Nicaise
An optimal control problem is discussed for a quasilinear parabolic Maxwell 
system. Well-posedness of the quasilinear equation, existence of an optimal 
control, and weak Gateaux-differentiability of the control-to-state mapping are 
proved. Based on these results, first-order necessary optimality conditions and an 
associated adjoint calculus are derived. 

■ Wed.B.1C
Wednesday, 15:15-16:30, Room 1C
Derivative-free Optimization Algorithms for Stochastic 
Problems
Cluster: Derivative-free and Simulation-based Optimization
Session organized by: Francesco Rinaldi, Zaikun Zhang
Session chair: Youssef M Marzouk
1. Probabilistically Fully Linear Models in STORM
Matt Menickelly (mjm412@lehigh.edu) Lehigh University, USA, 
Katya Scheinberg
In previous work, Chen, M., and Scheinberg developed an algorithmic framework, 
called STORM, for the unconstrained minimization of a stochastic nonconvex 
function, based on the class of derivative-free trust-region methods. In that work, 
the authors proved the almost sure convergence of the iterates of STORM to a 
stationary point, under the assumption of the existence of what they called a 
probabilistically fully linear sequence of models. Essentially, this assumption 
says that on each iteration, the models of the stochastic function should be fully 
linear in the iterationʼs trust region with some fixed probability (that need not 
converge to 1). In this talk, we will discuss the use of least squares regression 
models in STORM as a means to satisfy the probabilistically fully linear 
assumption, and discuss how it relates to model-building in general derivative-
free optimization.
2. On the Implementation of a Trust Region-based Algorithm for 
2016_iccopt.indb   83 2016/07/22   11:58:20
ABSTRACTS
ICCOPT 201684
Derivative-free Optimization over Stochastic Simulations
Satyajith Amaran (samaran@dow.com) The Dow Chemical Company, 
USA, Nikolaos V Sahinidis
Simulation optimization involves the optimization over stochastic simulations 
such as discrete-event simulations and stochastic differential equation systems. 
We develop a provably convergent trust region-based method for continuous 
simulation optimization. We describe the details of our implementation which 
alternates between interpolation and regression (with a learned error), and 
incorporates careful management of the trust region to balance model accuracy 
and the ability to distinguish between solution candidates. We also demonstrate 
the practical use of the method through its success on a large test bed, and its 
application to numerous problems from chemical engineering, including 
inventory optimization in chemical supply chains, and optimal sizing of 
obstructions for DNA separation.
3. A Gaussian Process Trust-Region Method for Derivative-free 
Nonlinear Constrained Stochastic Optimization
Youssef M Marzouk (ymarz@mit.edu) Massachusetts Institute of 
Technology, USA, Florian Augustin
We present the algorithm (S)NOWPAC for derivative-free constrained stochastic 
optimization. The method uses a generalized trust region approach that accounts 
for noisy evaluations of the objective and constraints. To reduce the impact of 
noise, we fit Gaussian process models to past evaluations. Our approach 
incorporates a wide variety of probabilistic risk or deviation measures in both the 
objective and the constraints. We demonstrate the efficiency of the approach via 
several numerical benchmarks and comparisons.

■ Wed.B.4A
Wednesday, 15:15-16:30, Room 4A
Data and Networks II
Cluster: Applications in Energy, Science and Engineering
Session organized by: Nai-Yuan Chiang
1. Online Blind Deconvolution in Through-the-Wall Radar Imaging
Hassan Mansour (mansour@merl.com) Mitsubishi Electric Research 
Laboratories, USA, Ulugbek Kamilov, Dehong Liu, Petros Boufounos, 
Philip Orlik, Kieran Parsons, Anthony Vetro
We propose an online radar imaging scheme that recovers a sparse scene and 
removes the multipath ringing induced by the front wall in a Through-the-Wall-
Imaging (TWI) system without prior knowledge of the wall parameters. Our 
approach uses online measurements obtained from individual transmitter-
receiver pairs to incrementally build the primary response of targets behind the 
front wall and find a corresponding delay convolution operator that generates the 
multi-path reflections available in the received signal. In order to perform online 
sparse imaging while removing wall clutter reflections, we developed a 
deconvolution extension of the Sparse Randomized Kaczmarz (SRK) algorithm. 
Our scheme constitutes an online generalization of blind-deconvolution using 
convex programming techniques such as SparseLift.
2. Using Functional Programming to Recognize Named Structure in 
an Optimization Problem: Application to Pooling
Ruth Misener (r.misener@imperial.ac.uk) Imperial College London, 
United Kingdom, Francesco Ceccon, Georgia Kouyialis
Branch-and-cut optimization solvers typically apply generic algorithms, e.g., 
cutting planes or primal heuristics, to expedite performance for many 
mathematical optimization problems. But solver software receives an input 
optimization problem as vectors of equations and constraints containing no 
structural information. This paper proposes automatically detecting named 
special structure using the pattern matching features of functional programming. 
Specifically, we deduce the industrially-relevant nonconvex nonlinear Pooling 
Problem within a mixed-integer nonlinear optimization problem and show that 
we can uncover pooling structure in optimization problems which are not pooling 
problems. Previous work has shown that preprocessing heuristics can find 
network structures; we show that we can additionally detect nonlinear pooling 
patterns. Finding named structures allows us to apply, to generic optimization 
problems, cutting planes or primal heuristics developed for the named structure. 
To demonstrate the recognition algorithm, we use the recognized structure to 
apply primal heuristics to a test set of standard pooling problems.
3. A Regularized Augmented Lagrangian Filter Method for 
Nonlinear Building MPC Problems 
Nai-Yuan Chiang (chiangn@utrc.utc.com) United Technologies 
Research Center, USA, Rui Huang, Victor M Zavala
We present a detailed filter line-search method for nonlinear optimization that 
uses an augmented Lagrangian regularization to compute search steps. The 
method is motivated by real-time optimization applications on embedded 
computing platforms that require low-complexity linear algebra routines such as 
Cholesky decomposition. We prove that the proposed the algorithm is globally 
convergent and we demonstrate the developments using a nonlinear MPC model 
for building HVAC system. Our numerical studies demonstrate that the proposed 
approach is as efficient as common filter line-search method that requires 
symmetric indefinite factorizations.

■ Wed.B.4B
Wednesday, 15:15-16:30, Room 4B
Algorithms for Complementarity and Equilibrium 
Problems
Cluster: Complementarity and Variational Inequalities
Session organized by: Uday Shanbhag
1. Lexicographic Pivoting for Mixed Linear Complementarity 
Problems
Todd Munson (tmunson@mcs.anl.gov) Argonne National Laboratory, 
USA
Degeneracy gives rise to cycling for standard linear complementarity problems 
unless special pivoting rules are applied. A key feature to prevent cycling is 
invertibility: the pivot sequence moving forward is uniquely determined and the 
same sequence in reverse is obtain moving backward. Rules such as devex with 
an index-based tie breaker are not invertible even though they produce unique 
forward and backward pivot sequences, while lexicographic pivoting is 
invertible. For mixed linear complementarity problems, we can transform the 
problems into larger standard linear complementarity problems and apply 
lexicographic pivoting at the cost of extra computations. In this talk, we construct 
invertible pivoting rules for the compact system that guarantee our pivoting 
method cannot cycle.
2. New Projection Methods for Monotone Variational Inequalities
Yura Malitsky (y.malitsky@gmail.com) Graz University of 
Technology, Austria
We consider some new first-order methods for variational inequalities. These 
methods are very flexible and work under quite general assumptions. They use a 
very simple linesearch procedure that takes into account a local information of 
the operator. Moreover, this linesearch does not require any extra projections and 
in the same time it allows to increase steps from iteration to iteration. Because of 
this, the proposed algorithms may be quite efficient for difficult convex 
optimization problems.
3. Value Function Based Non-cooperative Games
Tianyu Hao (tianyuha@usc.edu) University of Southern California, 
USA, Jong-Shi Pang
We introduce value function based Nash games, where the objective function of 
each player includes a point-wise maximum or minimum function. We discuss a 
special class of such games — network interdiction games, which model 
interdictions among multiple interdictors with different objectives operating on a 
common network. Since the resulting optimization problem of each player may 
have a non-convex combined objective, we use a relaxed equilibrium concept, 
called quasi-Nash equilibrium (QNE).We first establish results regarding the 
existence of a QNE. To compute a QNE, we present a reformulation of such 
game, which leads to a pulled-out game with convex objective functions for each 
player. Then we establish an equivalence between a Nash equilibrium (NE) of 
the pulled-out game with a QNE of the value function based game. We show 
further that for the linear-quadratic case, a linear complementarity problem 
(LCP) can be formulated. Under Slater conditions and some other conditions, we 
show that the LCP can be solved by Lemkeʼs algorithm.

■ Wed.B.5A
Wednesday, 15:15-16:30, Room 5A
Optimality and Algorithm for Convex and Multiple-
Objective Optimization
Cluster: Multi-Objective and Vector Optimization
Session organized by: Rabian Wangkeeree, Narin Petrot
1. (No talk is allocated at the first-speaker spot.)
2016_iccopt.indb   84 2016/07/22   11:58:20
ABSTRACTS
ICCOPT 2016 85
2. On Optimality Theorems for Multiobjective Optimization 
Problems over Feasible Set Defined by Tangentially Convex 
Inequalities
Rabian Wangkeeree (rabianw@nu.ac.th) Naresuan University, 
Thailand, Nithirat Sisarat
This paper is concerned with the optimality conditions for nonsmooth Convex 
Multiobjective Optimization problems (MOP) over a feasible set which is 
described by inequality constraints that are tangentially convex. We prove 
nonsmooth optimality theorems for weakly Pareto optimal solutions and properly 
Pareto optimal solutions of (MOP). We present examples illustrating our results.
3. Methods for Finding Solutions of Convex Optimization and 
Feasibility Problem without Convex Representation
Narin Petrot (narinp@nu.ac.th) Naresuan University, Thailand, Nimit 
Nimana, Porntip Promsinchai
In this talk, we will focus on the following two problems: (a) The convex 
optimization problem when the objective function may not smooth and the 
constraint set is represented by constraint functions that are locally Lipschitz and 
directionally differentiable, but neither necessarily concave nor continuously 
differentiable. (b) A type of generalized convex optimization problem, named a 
split quasi-convex feasibility problem. This problem is to find a point in a 
sublevel set of a quasi-convex function in one space and its image under a 
bounded linear operator is contained in a sublevel set of another quasi-convex 
function in the image space. We propose a new algorithm for solving these 
considered problems and discuss the convergence theorems of the constructed 
algorithms.

■ Wed.B.5D
Wednesday, 15:15-16:30, Room 5D
Recent Advances in Linear Optimization
Cluster: Linear Optimization
Session organized by: Tamás Terlaky
1. Inexact Directions in Interior Point Methods
Lukas Schork (L.Schork@ed.ac.uk) University of Edinburgh, United 
Kingdom, Jacek Gondzio
Inexact directions occur in interior point methods when iterative schemes are 
employed to solve the linear equation systems. Basing inexact algorithms on 
solid theory requires an analysis that is close to a practical implementation. We 
argue that the path-following framework, which currently forms the basis for 
most efficient interior point solvers, limits the use of inexact directions. Next, we 
demonstrate how the potential reduction approach combined with particular 
conditions on the inexactness can be used to analyse a practical algorithm.
2. Euler Polytopes and Convex Matroid Optimization
Antoine Deza (deza@mcmaster.ca) McMaster University, Canada, 
George Manoussakis, Shmuel Onn
We introduce a family of polytopes associated with several combinatorial objects 
such as the permutahedron of type Bd, or with the regions of hyperplane 
arrangements formed by normal vector with coordinates drawn from {⊖1,0,1}. 
We consider a class of optimization problems of convex multicritera objective 
functions over matroids. Such problems can be reduced to a number of linear 
optimization counterparts that is independent of the underlying matroid. We 
show that the introduced Euler polytopes improve the bounds on the number of 
linear counterparts needed to solve the convex multicritera optimization problem.
3. Long and Winding Central Paths
Pascal Benchimol (pascal.benchimol@polytechnique.edu) EDF R&D, 
France, Xavier Allamigeon, Stephane Gaubert, Michael Joswig
We construct a family of linear programs with 3r+4 inequalities in dimension 
2r+2 on which an interior point method will perform a number of iterations 
exponential in r. This holds when the iterates produced by the method, and line 
segments in-between, stay within a neighborhood of the central path. Moreover, 
these linear programs have a total curvature exponential in r. This disprove a 
continuous analogue of the Hirsch conjecture proposed by Deza, Terlaky and 
Zinchenko. Our method is to tropicalize the central path in linear programming. 
The tropical central path is the piecewise-linear limit of the central paths of 
parameterized families of classical linear programs viewed through logarithmic 
glasses

■ Wed.B.5E
Wednesday, 15:15-16:30, Room 5E
Advances in Robust Optimization II
Cluster: Robust Optimization
Session organized by: Vinh Doan
1. Fréchet Bounds and Distributionally Robust Optimization
Xuan Vinh Doan (Xuan.Doan@wbs.ac.uk) University of Warwick, 
United Kingdom, Xiaobo Li, Karthik Natarajan
We develop Fréchet bounds for the class of convex piecewise linear functions 
given overlapping marginals. To guarantee the existence of a joint multivariate 
distribution consistent with the overlapping marginal information, we make use 
of a graph theoretic property known as the running intersection property. 
Building on this property, we develop a tight linear programming formulation to 
find the Fréchet bounds under the discrete setting. We compare our bounds with 
some existing bounds and show that they can be better.
2. Tight Moments-based Bounds for Queueing Systems
Varun Gupta (varun9upta@gmail.com) University of Chicago, USA, 
Takayuki Osogami
Despite the long history of queueing theory, numerous fundamental queueing 
systems have defied exact analysis so far. In this talk we will focus on three: (i) 
the classical M/G/k multi-server system; (ii) queueing systems with semi-
markov modulated arrival and service rates; and (iii) the M/G/1 round-robin 
queue. We argue that rather than looking for exact expressions for the mean 
sojourn time as a function of the job-size distribution, a more fruitful approach is 
to obtain tight bounds on the mean sojourn time as functions of the moments of 
the job-size distribution. Analogous to the classical Markov-Krein theorem, we 
conjecture that the extremal distributions achieving these moments-based 
bounds correspond to the upper/lower principal representations of the moment 
sequence. We present both analytical and numerical evidence in support of our 
conjectures.
3. The Empirical Divergence-based Distributionally Robust 
Optimization
Henry Lam (khlam@umich.edu) University of Michigan, USA
We introduce what we call empirical divergence-based distributionally robust 
optimization (DRO) as a tractable tool to provide, close to the best, asymptotic 
statistical guarantees for the feasibility of expectation constraints under uncertain 
probability distributions. Unlike the standard rationale of data-driven DRO, our 
empirical uncertainty sets, which are balls measured by the Burg-entropy 
divergence, can have low or even zero probability of covering the true 
distribution. Rather, their statistical performances are endowed via linking the 
dual of the empirical DRO with likelihood theory in statistics. We show how to 
calibrate the size of these uncertainty sets using the quantiles of the excursion of 
chi-square processes.

■ Wed.B.5F
Wednesday, 15:15-16:30, Room 5F
Low Complexity Models and Applications
Cluster: Sparse Optimization and Information Processing
Session organized by: Martin Lotz
1. A Provable Nonconvex Algorithm for Spectrally Sparse Signal 
Reconstruction
Ke Wei (kewei@math.ucdavis.edu) University of California, Davis, 
USA, Jian-Feng Cai, Tianming Wang
We consider the reconstruction of spectrally sparse signals. Suppose such a 
signal is a mixture of complex sinusoids at r distinct continuous-valued 
frequencies, sampled at n equally-spaced points but only observed at m out of n 
random locations. We propose an efficient non-convex algorithm to reconstruct 
the signal via the low rank Hankel matrix completion. The proposed algorithm 
with a carefully selected initial guess is guaranteed to reconstruct the partial 
observed signal when the number of measurements is proportional to the intrinsic 
dimension of the problem. 
2. Tomography with Nonlinear Compressed Sensing
Raphael Andreas Hauser (hauser@maths.ox.ac.uk) Oxford 
Mathematical Institute, United Kingdom, Maria Klodt
A new generation of low cost 3D tomography systems is based on multiple 
emitters and sensors that partially convolve measurements. A successful 
approach to deconvolve the measurements is to use nonlinear compressed 
2016_iccopt.indb   85 2016/07/22   11:58:20
ABSTRACTS
ICCOPT 201686
sensing models. We discuss such models, as well as algorithms for their solution. 
3. Compressed Sensing Stability through High-dimensional 
Geometry
Axel Flinth (flinth@math.tu-berlin.de) Technische Universität Berlin, 
Germany
The main aim of compressed sensing is to recover a low-dimensional object (e.g. 
a sparse vector, a low-rank matrix, ...) from few linear measurements. Many 
algorithms for this task (e.g. Basis Pursuit, nuclear norm-minimization) consists 
of minimizing a structure-promoting function f over the set of possible solutions. 
Over the course of the last few years, researchers have succeeded at calculating 
threshold amounts of linear measurements that are needed to guarantee that the 
solution of such problems is equal to the ground truth signal with high probability. 
The idea consists of analyzing the probability that the kernel of the measurement 
matrix intersects the descent cone of f at the ground truth signal non-trivially. An 
important feature of programs of the type described above is that regularized 
versions of them can be used to recover signals that are only close to having a 
low-dimensional structure, and when the measurements are contaminated with 
noise. Can we use a geometric approach to explain also this phenomenon? In this 
talk, we will give a positive answer to this question. We will present an intuitive 
criterion and compare it to several other criteria for stability in compressed 
sensing.

■ Wed.B.5G
Wednesday, 15:15-16:30, Room 5G
Perspectives on Simplex Algorithms
Cluster: Linear Optimization
Session organized by: Thomas Dueholm Hansen
1. The Simplex Algorithm is NP-mighty
Yann Disser (yanndisser@gmail.com) TU Berlin, Germany, Martin 
Skutella
We propose to classify the power of algorithms by the complexity of the 
problems that they can be used to solve. Instead of restricting to the problem a 
particular algorithm was designed to solve explicitly, however, we include 
problems that, with polynomial overhead, can be solved ʻimplicitlyʼ during the 
algorithmʼs execution. For example, we allow to solve a decision problem by 
suitably transforming the input, executing the algorithm, and observing whether 
a specific bit in its internal configuration ever switches during the execution. We 
show that the Simplex Method, the Network Simplex Method (both with 
Dantzigʼs original pivot rule), and the Successive Shortest Path Algorithm are 
NP-mighty, that is, each of these algorithms can be used to solve any problem in 
NP. This result casts a more favorable light on these algorithmsʼ exponential 
worst-case running times. Furthermore, as a consequence of our approach, we 
obtain several hardness results.
2. An Improved Version of the Random-Facet Pivoting Rule for the 
Simplex Algorithm
Thomas Dueholm Hansen (tdh@cs.au.dk) Aarhus University, 
Denmark, Uri Zwick
The Random-Facet pivoting rule of Kalai and of Matousek, Sharir, and Welzl is 
an elegant, randomized pivoting rule for the simplex algorithm, the classical 
combinatorial algorithm for solving linear programs. The expected running time 
of the simplex algorithm when using this rule is subexponential in the 
combinatorial size–the number of variables and inequalities–of the linear 
program. This is currently the best known combinatorial bound for solving 
general linear programs. Other polynomial time algorithms are known, but their 
running time depends also on the number of bits in the representation of the 
linear program. We present a slightly improved version of the Random-Facet 
pivoting rule, thus obtaining the fastest known combinatorial algorithm for 
solving linear programs, the first improvement in over 20 years. Our results 
apply not only to linear programs, but also to more general, abstract LP-type 
problems. In particular we also obtain the fastest known algorithm for solving 
two-player turn-based stochastic games, a natural generalization of Markov 
decision processes.
3. A Directed Steinitz Theorem for Oriented Matroid Programming
Walter Morris (wmorris@gmu.edu) George Mason University, USA
Holt and Klee proved that if P is a d-dimensional polytope and f is a linear 
function on P that is not constant on any edge of P, there are d independent 
monotone paths from the source to the sink of the digraph defined by the vertices 
and edges of P directed according to the directions of increase of f. Mihalisin and 
Klee proved that every orientation of the graph of a 3-polytope that is acyclic and 
admits 3 independent monotone paths from the source to the sink is obtained 
from some 3-polytope P and some linear function f on P. We prove analogs of 
Mihalisin and Kleeʼs theorem and the 3 and 4-dimensional versions of Holt and 
Kleeʼs theorem for oriented matroid programs. Here acyclicity is replaced by the 
requirement that there be no directed cycle contained in a face of the polytope.

■ Wed.B.5H
Wednesday, 15:15-16:30, Room 5H
Optimization Approaches for Derivative Pricing and 
Risk Management
Cluster: Applications in Finance and Economics
Session organized by: Cedric Yiu
1. Hybrid Laplace Transform and Finite Difference Methods for 
Pricing American Options
Jingtang Ma (mjt@swufe.edu.cn) Southwestern University of Finance 
and Economics, China, Zhiqiang Zhou, Zhenyu Cui
In this talk, we present a novel Laplace transform and finite difference method to 
price (finite-maturity) American options, which is applicable to a wide variety of 
asset price models including constant elasticity of variance (CEV), hyper-
exponential jump-diffusion (HEJD), and Markov regime switching models. We 
first apply Laplace transforms to free boundary partial differential equations 
(PDEs) governing the Amer-ican option prices with respect to time, and obtain 
second order ordinary differential equations (ODEs) with free boundary. Then 
we develop a novel iterative algorithm based on finite difference methods to 
solve the ODEs together with the unknown free boundary values in the Laplace 
space. Both the early exercise boundary and the prices of American options are 
recovered through inverse Laplace transforms. 
2. Optimal Portfolio and Insurance Problems with Risk Constraint
Cedric Yiu (cedric.yiu@polyu.edu.hk) The Hong Kong Polytechnic 
University, Hong Kong, Jingzhen Liu
We consider the risk-constrained portfolio selection problems arising from an 
ordinary investor or an insurer who can invest her surplus into financial market. 
For an insurer, the optimal investment and reinsurance problem is studied. The 
goal is to maximize the expected utility of terminal wealth. By using the principle 
of dynamic programming, the Hamilton-Jacobi-Bellman (HJB) equation can be 
derived. We will examine a few scenarios with different stochastic processes and 
discuss how to solve the resulting HJB equation. Furthermore, we will investigate 
the impacts of the risk constraint on the optimal strategies. 

■ Wed.B.5J
Wednesday, 15:15-16:30, Room 5J
Sparse Optimization: Algorithms and Applications
Cluster: Convex and Nonsmooth Optimization
Session organized by: Michael Friedlander
1. Inexact Proximal Newton Methods for Composite Minimization
Cho-Jui Hsieh (chohsieh@ucdavis.edu) University of California, 
Davis, USA, Inderjit S Dhillon
Proximal Newton Methods have been widely used for solving composite 
minimization problems. Unfortunately, in many cases the subproblems do not 
have a closed form solution and thus have to be solved by another iterative solver 
with some stopping condition. These “inexact” proximal Newton methods have 
been implemented in many state-of-the-art software libraries, but in many case 
the convergence rate is unknown. In this talk we will formally analyze the global 
and local convergence rate for inexact proximal Newton methods and discuss 
how to make those algorithms efficient for large-scale problems. The resulting 
algorithms are efficient for solving many important machine learning problems, 
including sparse logistic regression and sparse inverse covariance estimation. 
2. Making Sketchy Decisions: Semidefinite Programming with 
Optimal Storage
Madeleine Udell (madeleine.udell@gmail.com) Cornell University, 
USA, Joel A Tropp, Volkan Cevher, Alp Yurtsever
Is it possible to solve an optimization problem using far less memory than the 
natural size of the decision variable? In this talk, we propose an (affirmative) 
answer to this question when both the problem data and the solution have a 
concise representation. We present an algorithm for provably solving many 
semidefinite programming problems (whose natural size is O(n2)) using no more 
than O(n) memory.
2016_iccopt.indb   86 2016/07/22   11:58:21
ABSTRACTS
ICCOPT 2016 87
3. Global Optimality in Matrix and Tensor Factorization, Deep 
Learning, and Beyond
Rene Vidal (rvidal@cis.jhu.edu) Johns Hopkins University, USA, 
Benjamin Haeffele
Matrix, tensor, and other factorization techniques are used in many applications 
and have enjoyed significant empirical success in many fields. However, 
common to a vast majority of these problems is the significant disadvantage that 
the associated optimization problems are typically non-convex due to a 
multilinear form or other convexity destroying transformation. Building on ideas 
from convex relaxations of matrix factorizations, in this talk I will present a very 
general framework which allows for the analysis of a wide range of non-convex 
factorization problems - including matrix factorization, tensor factorization, and 
deep neural network training formulations. In particular, I will present sufficient 
conditions under which a local minimum of the non-convex optimization 
problem is a global minimum and show that if the size of the factorized variables 
is large enough then from any initialization it is possible to find a global 
minimizer using a local descent algorithm.

■ Wed.B.5K
Wednesday, 15:15-16:30, Room 5K
Some New Results on Conic Optimization and Its 
Applications to Machine Learning
Cluster: Conic and Polynomial Optimization
Session organized by: Akiko Yoshise
1. Inner and Outer Approximations of the Semidefinite Cone using 
SD Bases and Their Applications to Some NP-hard Problems 
Daigo Narushima (s1620483@sk.tsukuba.ac.jp) University of 
Tsukuba, Japan, Akihiro Tanaka, Akiko Yoshise
Some of the authors in previous paper introduced a semidefinite basis which is a 
basis of the space of symmetric matrices consisting of rank-1 semidefinite 
matrices. In this talk, we present polyhedral inner and outer approximations of 
the positive semidefinite cone and the completely positive cone by using 
semidefinite bases. The conical hull of a semidefinite basis gives an inner 
approximation of the positive semidefinite cone. If elements of a semidefinite 
basis are nonnegative matrices, it gives an inner approximation of the completely 
positive cone. Supporting hyperplanes at every point of a semidefinite basis 
configure an outer approximation of these cones. Preliminary numerical 
experiments suggest that linear optimization problems over our polyhedral cones 
give a good upper and lower bounds of standard quadratic programming and 
randomly generated doubly nonnegative programming.
2. Diversity Extraction via Condition Number Constrained Matrix 
Factorization
Mirai Tanaka (mirai@rs.tus.ac.jp) Tokyo University of Science, Japan, 
Takanori Maehara
In machine learning context, we often decompose a data matrix to the product of 
two matrices and interpret them as in the singular value decomposition and the 
nonnegative matrix factorization. The columns and rows of the output matrices 
are sometimes referred to as bases and they are interpreted as features of 
extracted topics or clusters. In practice, it is desirable that the bases differ from 
each other since the extracted topics have diversity. In this talk, we propose a 
matrix factorization method that guarantees the diversity of the output. We 
formulate such matrix factorization as an optimization problem with condition 
number constraints and propose an alternative direction multiplier method.
3. Rank Minimization Approach to Collaborative Filtering Based on 
the Nuclear Norm Minimization
Akiko Yoshise (yoshise@sk.tsukuba.ac.jp) University of Tsukuba, 
Japan, Tomotaka Yokoo, Akihiro Tanaka
Recht, Fazel and Parrilo(2010) gave a theoretical characterization of the nuclear 
norm minimization relaxation of the affine rank minimization problem and 
suggested many applications of the result including collaborative filtering. 
However, very few results have been reported on collaborative filtering using the 
rank minimization approach. In this talk, we will present some numerical results 
using this approach and compare them with the results using singular value 
decomposition approach.

■ Wed.B.5L
Wednesday, 15:15-16:30, Room 5L
Moments, Positive Polynomials & Optimization: Part 
IV
Cluster: Conic and Polynomial Optimization
Session organized by: Jiawang Nie, Jean B Lasserre
1. BSOS: A Bounded-Degree SOS Hierarchy for Polynomial 
Optimization
Jean B Lasserre (lasserre@laas.fr) LAAS-CNRS, France, Kim Chuan 
Toh, Shouguang Yang
The powerful SOS-based hierarchy of semidefinite programs based on Putinarʼs 
positivity certificate is penalized by the fast growth of the size of the involved 
semidefinite matrices. The BSOS hierarchy uses a different positivity certificate 
(mixing Krivine-Handelmanʼs and Putinarʼs), and involves semidefinite matrices 
of fixed size. In contrast to the Krivine-Handelman LP-hierarchy, finite 
convergence for SOS-convex programs is guaranteed.
2. Computing the Distance between the Linear Matrix Pencil and the 
Completely Positive Cone
Jinyan Fan (jyfan@sjtu.edu.cn) Shanghai Jiao Tong University, China, 
Anwa Zhou
In this talk, we show how to compute the distance between the linear matrix 
pencil and the completely positive cone. We formulate this problem as a linear 
optimization problem with the moment cone and the second order cone. A 
semidefinite relaxation algorithm is presented. A new model for checking the 
membership in the completely positive cone is also be proposed.
3. DC Decomposition of Nonconvex Polynomials with Algebraic 
Techniques
Georgina Hall (gh4@princeton.edu) Princeton University, USA, Amir 
Ali Ahmadi
The concave-convex procedure is a majorization-minimization algorithm for 
difference of convex (DC) optimization, where the constraints and the objective 
function are given as the difference of two convex functions. Although several 
important problems (e.g., in machine learning) already appear in DC form, such 
a decomposition is not always available. We consider this decomposition 
question for polynomial optimization. We introduce LP, SOCP, and SDP based 
algorithms for finding optimal DC decompositions by appealing to the algebraic 
concepts of ”DSOS-Convex, SDSOS-Convex, and SOSConvex” polynomials. 
We also study structural properties of these polynomials and answer existence 
questions about polynomial DC decompositions

■ Wed.B.m3S
Wednesday, 15:15-16:30, Room m3S
Recent Advances in First-Order Methods: Part II
Cluster: Convex and Nonsmooth Optimization
Session organized by: Marc Teboulle, Shoham Sabach
1. The Exact Information-based Complexity of Smooth Convex 
Minimization
Yoel Drori (yoel.drori@gmail.com) Google, Israel
We present a new lower bound on the information-based complexity of first-
order minimization of smooth and convex functions. The new bound matches the 
worst-case performance of the recently introduced Optimized Gradient Method 
thereby establishing that the bound is tight and can be realized by an efficient 
algorithm. The proof is based on a novel construction technique of smooth and 
convex functions.
2. A First Order Method for Solving Convex Bi-Level Optimization 
Problems
Shoham Sabach (ssabach@ie.technion.ac.il) Technion, Israel, Shimrit 
Shtern
We study convex bi-level optimization problems for which the inner level 
consists of minimization of the sum of smooth and nonsmooth functions. The 
outer level aims at minimizing a smooth and strongly convex function over the 
optimal solutions set of the inner problem. We analyze a first order method 
which is based on an existing fixed-point algorithm. Global sublinear rate of 
convergence of the method is established in terms of the inner objective function 
values.
3. Primal and Dual Predicted Decrease Approximation Methods
Amir Beck (becka@ie.technion.ac.il) Technion, Israel, Edouard 
Pauwels, Shoham Sabach
2016_iccopt.indb   87 2016/07/22   11:58:21
ABSTRACTS
ICCOPT 201688
We introduce the notion of predicted decrease approximation (PDA) for 
constrained optimization, a flexible framework which includes as special cases 
known algorithms such as generalized conditional gradient, proximal gradient, 
greedy coordinate descent for separable constraints and working set methods for 
linear equality constraints with bounds. This allows to provide a unified 
convergence analysis for these methods. We further consider a partially strongly 
convex nonsmooth model and show that dual application of PDA-based methods 
yields new sublinear convergence rate estimates in terms of both primal and dual 
objectives. As an example of an application, we provide an explicit working set 
selection rule for SMO-type methods for training the support vector machine 
with an improved primal convergence analysis.

■ Wed.B.m3AB
Wednesday, 15:15-16:30, Room m3AB
Advances in Deterministic Global Optimization II
Cluster: Global Optimization
Session organized by: Chris Floudas, Nikolaos Sahinidis
Session chair: Pietro Belotti
1. A Branch and Bound Procedure for a Quadratic Reverse Convex 
Programming Problem by Listing FJ Points
Syuuji Yamada (yamada@math.sc.niigata-u.ac.jp) Niigata University, 
Japan
In this talk, we propose a branch and bound procedure for a quadratic reverse 
convex programming problem (QRC) whose feasible set is expressed as the area 
excluded the interior of a convex set from another convex set. It is known that 
many global optimization problems can be transformed into or approximated by 
such a problem. We observe that the feasible set of QRC is not always connected. 
Moreover, one of the reason of the difficulty for solving QRC is that all locally 
optimal solutions do not always satisfy KKT conditions. In order to overcome 
this drawback, we introduce a procedure for listing FJ points of QRC. By 
utilizing such a procedure, we can calculate all FJ points contained in the 
intersection of the boundaries of convex sets defining the feasible set. Further, 
we propose an algorithm for finding a globally optimal solution of QRC by 
incorporating such a procedure into a branch and bound procedure.
2. Generalized Nash Games and Cap and Trade Environmental 
Models
Monica Gabriela Cojocaru (mcojocar@uoguelph.ca) University of 
Guelph, Canada, Allison Small
Environmental policy change is coming to the forefront of discussion and 
innovation due to the negative effects of climate change. Cap and trade is a 
market based policy that aims to reduce emissions of major polluters to protect 
the environment and human health. In this work, we extend the model presented 
in Breton et al., 2005 to three players (or countries) and evaluate the value of a 
cap and trade system.  We aim to assess the merit of such a policy by using a 
generalized Nash game, and compare our results to a cap and “notrade” system 
modelled by a regular Nash game. We apply a newly developed computational 
method based on variational inequalities to solve the generalized game.
3. Solving Hard Mixed Integer Quadratic and Conic Optimization 
Problems
Pietro Belotti (pietrobelotti@fico.com) Fair Isaac, United Kingdom
We report on the recent developments in the Xpress Optimizer for solving large 
scale mixed integer problems with quadratic and second-order cone constraints. 
In particular, we show some presolving and model reformulation techniques and 
their impact on the performance of the Optimizer for public benchmark problems.

■ Wed.C.1S
Wednesday, 17:00-18:15, Room 1S
Advances in Large-Scale Optimization
Cluster: Nonlinear Optimization
Session organized by: Marianna De Santis
1. A Global Optimization Approach for the Valve Setting Problem
Bissan Ghaddar (bghaddar@uwaterloo.ca) IBM Research, Ireland
In this talk, we present a new quadratic optimization model for setting pressure 
reducing valves in water networks. A key advantage of the development of a 
quadratic formulation for the valve setting problem is that it provides 
computationally efficient solutions, and increases the network size that can be 
solved to global optimality. Polynomial optimization techniques are utilized to 
derive globally optimal bounds on the quadratic formulation of the valve setting 
problem and approximate globally optimal solutions. Computational results of 
the new formulation are presented on four water networks.
2. Worst-Case and Sparse Portfolio Selection: Insights and 
Alternatives
Yufei Yang (yufei_yang@mymail.sutd.edu.sg) Singapore University 
of Technology and Design, Singapore, Selin Damla Ahipasaoglu, 
Jingnan Chen
In this talk, we will discuss a robust portfolio selection problem under fixed 
transaction costs. Based on Markowitzʼs mean-variance framework, we 
investigate a stylized model that incorporates an ellipsoidal uncertainty set and 
fixed transaction costs. We explore the portfolio composition and characterize 
the impact of parameter uncertainty and fixed transaction costs on portfolio 
weights.
3. An Active Set Strategy for Nonlinear Programming Problems with 
Box Constraints
Marianna De Santis (marianna.desantis@gmail.com) Alpen-Adria 
Universität Klagenfurt, Austria, Andrea Cristofari, Stefano Lucidi, 
Francesco Rinaldi
We present an active-set strategy that has been recently used in different contexts 
and we will focus in particular on large scale problems with bound constraints. A 
two-stage algorithm will be introduced. At each iteration, in a first stage we 
estimate the active variables and fix them to the bounds, and in a second stage we 
perform a line search along a projected truncated-Newton direction computed in 
the subset of the estimated non-active variables. The proposed algorithm embeds 
these two stages within a nonmonotone stabilization framework. Global 
convergence to stationary points is established. Promising results were obtained 
on bound-constrained problems from the CUTEst collection.

■ Wed.C.1A
Wednesday, 17:00-18:15, Room 1A
Optimization Methods for Inverse Problems 3
Cluster: Nonlinear Optimization
Session organized by: Xin Liu, Yanfei Wang
1. Solving Constrained TV2L1-L2 MRI Signal Reconstruction via 
an Efficient Alternating Direction Method of Multipliers
Tingting Wu (wutt@njupt.edu.cn) Nanjing University of Posts and 
Telecommunications, China, Wenxing Zhang, Ke Guo, Deren Han
High order total variation(TV2) and `1(TV2L1) based model has its advantage 
over the TVL1 for its ability in avoiding the staircases; and a constrained model 
has its advantage over its unconstained counterpart for its simple in estimating 
the parameters. In this paper, we consider the solving the TV2L1 based magnetic 
resonance imaging (MRI) signal reconstruction problem by an efficient 
alternating direction method of multipliers. By sufficiently utilizing the problemʼs 
special structure, we manage to make all subproblems either posses closed-form 
solution or can be solved via Fast Fourier Transform (FFT), which makes the 
cost per iteration is very low. We prove the convergence of the algorithm under 
mild conditions. Experimental results for MRI reconstruction are presented to 
illustrate the new model and algorithm. Comparison with its recent unconstained 
counterpart is also reported.
2. Asymmetric Proximal Point Algorithms with Moving Proximal 
Centers
Deren Han (handeren@njnu.edu.cn) Nanjing Normal University, 
China, Xiaoming Yuan
We discuss the classical proximal point algorithm (PPA) with a metric proximal 
parameter in the variational inequality context. The metric proximal parameter is 
usually required to be positive definite and symmetric in the PPA literature, 
because it plays the role of the measurement matrix of a norm in the convergence 
proof. Our main goal is to show that the metric proximal parameter can be 
asymmetric if the proximal center is shifted appropriately. The resulting 
asymmetric PPA with moving proximal centers maintains the same 
implementation difficulty and convergence properties as the original PPA; while 
the asymmetry of the metric proximal parameter allows us to design highly 
customized algorithms that can effectively take advantage of the structures of the 
model under consideration. In particular, some efficient structure-exploiting 
splitting algorithms can be easily developed for some special cases of the 
variational inequality. We illustrate these algorithmic benefits by a saddle point 
problem and a convex minimization model with a generic separable objective 
function, both of which have wide applications in various fields. We present both 
2016_iccopt.indb   88 2016/07/22   11:58:21
ABSTRACTS
ICCOPT 2016 89
the exact and inexact versions of the asymmetric PPA with moving proximal 
centers; and analyze their convergence including the estimate of their worst-case 
convergence rates measured by the iteration complexity under mild assumptions 
and their asymptotically linear convergence rates under stronger assumptions.
3. A Proximal Alternating Direction Method for Multi-Block 
Coupled Convex Minimization
Lingling Xu (xulingling@njnu.edu.cn) Nanjing Normal University, 
China, Foxiang Liu, Deren Han
In this paper, we extend a proximal alternating direction method (PADM) for 
solving the convex minimization problems with linear constraints whose 
objective function is the sum of multi-block separable functions and coupled 
quadratic function. The algorithm generates iterate via a simple correction step, 
where the decent direction is based on the PADM. For the multiple-block case, 
which is the same structure with three-block case, we prove the convergence of 
the generated sequence under some mild assumptions. Finally, some preliminary 
numerical results are reported to support the efficiency of the new algorithms.

■ Wed.C.1B
Wednesday, 17:00-18:15, Room 1B
Recent Developments in PDE-constrained Optimization 
I
Cluster: PDE-constrained Optimization
Session organized by: Stefan Ulbrich
1. Optimal Control of Multiphase Fluids and Droplets
Michael Hintermüller (michael.hintermueller@wias-berlin.de) 
Weierstrass Institute for Applied Analysis and Stochastics (WIAS) and 
Humboldt-Universität zu Berlin, Germany
Motivated by control problems in fluid dynamics, two classes of mathematical 
programs with equilibrium constraints are considered. As a first problem class, 
Cahn-Hilliard Navier-Stokes systems with non smooth potentials are considered 
for the optimal  control of multiphase fluids. Secondly, for problems involving 
droplets, the Hele-Shaw model with contact line pinning is utilized. For  both 
cases stationarity systems are derived and numerical results are discussed.
2. A Nonlinear Primal-Dual Extragradient Method for Nonsmooth 
PDE-constrained Optimization
Christian Clason (christian.clason@uni-due.de) University Duisburg-
Essen, Germany, Tuomo Valkonen
This talk is concerned with the extension of the Chambolle-Pock primal-dual 
algorithm to nonsmooth optimization problems involving nonlinear operators 
between function spaces. The proof of local convergence rests on verifying the 
Aubin property of the inverse of a monotone operator at the minimizer, which is 
difficult as it involves infinite-dimensional set-valued analysis. However, for 
nonsmooth functionals that are defined pointwise — such as L1 norms or indicator 
functions of pointwise constraints — it is possible to apply simpler tools from the 
finite-dimensional theory, which allows deriving explicit conditions for the 
convergence. This is illustrated using an inverse problem with L1-fitting and an 
optimal control problem with state constraints, where the parameter resp. control 
enters into a potential term.
3. Preconditioners for Time-dependent PDE-constrained 
Optimization and an Implementation Based on Parareal Time-
Domain Decomposition
Stefan Ulbrich (ulbrich@mathematik.tu-darmstadt.de) Technische 
Universitaet Darmstadt, Germany, Anton Schiela
We consider optimization problems governed by time-dependent parabolic 
PDEs and discuss the construction of parallel preconditioners based on the 
parareal method for the solution of quadratic subproblems which arise within 
SQP methods. In the case without control constraints, the optimality system of 
the subproblem is directly reduced to a symmetric PDE system, for which we 
propose a preconditioner that decouples into a forward and backward PDE solve. 
In the case of control constraints we apply a semismooth Newton method and 
apply the preconditioner to the semismooth Newton system. We prove bounds on 
the condition number of the preconditioned system which shows no or only a 
weak dependence on the size of regularization parameters for the control. We 
propose to use the parareal time domain decomposition method for the forward 
and backward PDE solves within the PDE preconditioner to construct an efficient 
parallel preconditioner. Numerical results are presented.

■ Wed.C.1C
Wednesday, 17:00-18:15, Room 1C
Advances in Derivative-free and Simulation-based 
Optimization III
Cluster: Derivative-free and Simulation-based Optimization
Session organized by: Francesco Rinaldi, Zaikun Zhang
Session chair: Jeffrey Larson
1. A New Derivative-free Method for Integer Programming Problems
Francesco Rinaldi (rinaldi@math.unipd.it) University of Padova, 
Italy, Giampaolo Liuzzi, Stefano Lucidi
In this work, we consider integer programming problems with both bound 
constraints on the variables and general nonlinear constraints, where objective 
and constraint function values can only be obtained by querying a black box. We 
define a new derivative-free method that combines the use of suitably generated 
sets of search directions with a specific penalty approach. Furthermore, we report 
the results of some preliminary numerical experiments on both bound constrained 
and nonlinearly constrained problems. 
2. Asynchronously Parallel Optimization Solver for Finding Multiple 
Minima
Jeffrey Larson (jmlarson@anl.gov) Argonne National Laboratory, 
USA, Stefan M Wild
We propose and analyze an asynchronously parallel optimization algorithm for 
finding multiple, high-quality minima of nonlinear optimization problems. Our 
multistart algorithm considers all previously evaluated points when determining 
where to start or continue a local optimization run. Theoretical results show that, 
under certain assumptions, the algorithm almost surely starts a finite number of 
local optimization runs and identifies, or has a single local optimization run 
converging to, every minimum. The algorithm is applicable to general 
optimization settings, but our numerical results focus on the case when 
derivatives are unavailable. In numerical tests, a Python implementation of the 
algorithm is shown to yield good approximations of many minima (including a 
global minimum), and this ability scales well with additional resources. Our 
implementationʼs time to solution is shown also to scale well, even when the time 
to evaluate the function evaluation is highly variable.

■ Wed.C.4A
Wednesday, 17:00-18:15, Room 4A
Data and Networks III
Cluster: Applications in Energy, Science and Engineering
Session organized by: Gesualdo Scutari
1. Decomposing Linearly Constrained Nonconvex Problems by a 
Proximal Primal-Dual Approach
Mingyi Hong (mingyi@iastate.edu) Iowa State University, USA
We propose a new decomposition approach named the proximal primal dual 
algorithm (Prox-PDA) for smooth nonconvex linearly constrained optimization 
problems. The proposed approach is primal-dual based, where the primal step 
minimizes certain approximation of the augmented Lagrangian of the problem, 
and the dual step performs an approximate dual ascent. Theoretically, we show 
that whenever the penalty parameter in the augmented Lagrangian is larger than 
a given threshold, the Prox-PDA converges to the set of stationary solutions, 
globally and in a sublinear manner (i.e., certain measure of stationarity decreases 
in the rate of O(1/r), where r is the iteration counter). Interestingly, when 
applying a variant of the Prox-PDA to the problem of distributed nonconvex 
optimization (over a connected undirected graph), the resulting algorithm 
coincides with the popular EXTRA algorithm, which is only known to work in 
convex cases. Our analysis implies that EXTRA and its variants converge 
globally sublinearly to stationary solutions of certain nonconvex distributed 
optimization problem. 
2. Accelerated Hybrid Steepest Descent Method for Solving Affinely 
Constrained Composite Convex Optimization Problems
Konstantinos Slavakis (kslavaki@buffalo.edu) University at Buffalo, 
USA, Isao Yamada
The hybrid steepest descent method (HSDM) [Yamada ʼ01] was introduced as a 
low-computational complexity tool for solving convex variational inequality 
problems over the fixed point set of nonexpansive mappings in Hilbert spaces. 
Borrowing ideas from conjugate gradient methods, HSDM versions that 
accelerate its rate of convergence were very recently introduced. However, to 
secure strong convergence to an optimal point in general Hilbert spaces, the 
2016_iccopt.indb   89 2016/07/22   11:58:21
ABSTRACTS
ICCOPT 201690
sequence of step-size parameters is required to be diminishing, iterates are forced 
to belong to a bounded set, and the loss function is assumed to be differentiable 
with a strongly monotone gradient. This study offers a notable relaxed version of 
HSDM for affinely constrained composite optimization problems over Euclidean 
spaces, where the convex loss function consists of a smooth and a non-smooth 
part, the step-size parameter stays constant, the domain over which minimization 
is performed need not be bounded, and the smooth part of the loss is only required 
to have a Lipschitz continuous gradient operator. Results on the rate of 
convergence to an optimal point are presented, together with implementations of 
this accelerated version of HSDM in hierarchical optimization tasks for big-data 
applications.
3. In-Network Nonconvex Large-Scale Optimization
Gesualdo Scutari (gscutari@purdue.edu) Purdue University, USA
Consider a network composed of agents aiming to distributively minimize a 
(nonconvex) smooth sum-utility function plus a nonsmooth (nonseparable), 
convex one. The agents have access only to their local functions but not the 
whole objective, and the network is modeled as a directed, time-varying, 
T-strongly connected graph. We propose a distributed solution method for the 
above optimization wherein the agents in parallel minimize a convex surrogate 
of the original nonconvex objective while using dynamic consensus to distribute 
the computations over the network. Convergence to stationary solutions is 
established. Numerical results show that our new algorithm outperforms current 
schemes on both convex and nonconvex problems

■ Wed.C.4B
Wednesday, 17:00-18:15, Room 4B
Algorithms for Variational Inequality and Optimization 
Problems
Cluster: Complementarity and Variational Inequalities
Session organized by: Uday Shanbhag
1. Convergence Analysis of Fixed Point Optimization Algorithm for 
the Triple-hierarchical Constrained Optimization Problem
Thanyarat Jitpeera (t.jitpeera@hotmail.com) Rajamangala Technology 
Lanna University, Thailand, Tamaki Tanaka, Poom Kumam
An explicit algorithm is introduced to solve the monotone variational inequality 
over triple hierarchical problem. The strong convergence for the proposed 
algorithm to the solution is guaranteed under some assumptions. Our results 
extend ones of Iiduka(2009), Iiduka and Yamada(2009), Iiduka(2012) and study 
ones of Ceng et al.(2011), Yao et al.(2011).
2. On the Global Convergence of Nonlinear Optimization Algorithms 
under Weak Assumptions
Gabriel Haeser (ghaeser@gmail.com) University of Sao Paulo, Brazil, 
Roberto Andreani, Alberto Ramos, Paulo JS Silva
In this work we are interested in identifying first and second order properties 
satisfied by a local minimizer of a nonlinear optimization problem. Our main 
goal is to find conditions that can be verified by practical algorithms. We define 
first and second order optimality conditions stronger than the usual ones, 
requiring less restrictive assumptions on the problem, and as a consequence, we 
show global convergence to stationarity of several classes of first and second 
order algorithms under less restrictive assumptions. 
3. Characterization of Weakly Sharp Solutions of a Variational 
Inequality by Its Primal Gap Function
Yina Liu (Yina.Liu@xjtlu.edu.cn) Xiʼan Jiaotong-Liverpool 
University, China, Zili Wu
Our aim is to study weakly sharp solutions of a variational inequality in terms of 
its primal gap function g. We discuss sufficient conditions for the Lipschitz 
continuity and subdifferentiability of the primal gap function. Several sufficient 
conditions for the relevant mapping to be constant on the solutions have also 
been obtained. Based on these, we characterize the weak sharpness of the 
solutions of a variational inequality by g. Some finite convergence results of 
algorithms for solving variational inequality problems are also included.

■ Wed.C.5A
Wednesday, 17:00-18:15, Room 5A
Set-valued Analysis and Nonlinear Scalarization
Cluster: Multi-Objective and Vector Optimization
Session organized by: Tamaki Tanaka
1. On Generalization of a Fixed Point Theorem for Set-valued Maps
Yutaka Saito (ysaito@m.sc.niigata-u.ac.jp) Niigata University, Japan, 
Tamaki Tanaka, Syuuji Yamada
The main topic of this talk is some generalization of several results for real-
valued functions to set-valued map by using scalarization methods. We define a 
preorder induced by a convex cone in a topological vector space, and rewrite 
classical results based on the total order on the one dimensional real space R by 
the preorder. The original theorem related to this talk given by B.Ricceri has a 
conclusion with inequality conditions for some real-valued maps. The conclusion 
is same as Fan-Takahashi minimax inequality, which is equivalent to KyFan 
fixed point theorem. In this talk, we use a certain scalarizing function for set-
valued maps proposed by Kuwano, Tanaka and Yamada. The composite function 
of a set-valued map and its scalarizing function has kinds of some convexities 
and continuities if the set-valued map has a certain convexity and continuity, 
respectively. Moreover, the scalarizing function has monotonicity between some 
set-relations and the classical order on R. We prove a generalized theorem of the 
original theorem to some set-valued versions by inherited properties above. In 
this talk, we introduce the mechanism of scalarization, and prove a Ricceri type 
minimax theorem.
2. Generalized Alternative Theorems Based on Set-Relations and an 
Application to Semidefinite Programming Problems
Yuto Ogata (y-ogata@m.sc.niigata-u.ac.jp) Niigata University, Japan, 
Gue Myung Lee, Jae Hyoung Lee, Yutaka Saito, Tamaki Tanaka
Gordanʼs theorem which is known as the earliest alternative theorem plays 
important roles in optimization. There are some kinds of extensions; Jeyakumar 
produces a generalized Gordanʼs theorem for vector-valued functions in 1986; Li 
in 1999 and Yang et al. in 2000 extend it to the case of set-valued maps. These 
theorems rely on assumptions related to convexity to make their systems in 
bilinear forms. In this talk, I would like to introduce alternative theorems from a 
set-valued analytic point of view, using set-relations proposed by Kuroiwa, 
Tanaka, and Ha in 1997. A similar approach with scalarizing functions for vectors 
had been done by Nishizawa, Onodsuka, and Tanaka in 2005. They omit the 
convexity with nonlinear scalarizations. We revised them to generalize these 
results. Also, we suggest an application of giving a judgement of Slater condition 
in semidefinite programming problems.
3. A Scalar Characterization of Set-valued Optimization Problems
Issei Kuwano (kuwano@kanagawa-u.ac.jp) Kanagawa University, 
Japan
In this talk, we introduce scalarization functions of sets and their some properties 
including monotonicity, continuity and convexity. Moreover, we characterize 
several set-valued optimization problems with set-criterion via scalarization. As 
an application, we give sufficient conditions for the existence of common 
solutions of some vector optimization problems. 

■ Wed.C.5C
Wednesday, 17:00-18:15, Room 5C
Set-valued and Vector Optimization
Cluster: Multi-Objective and Vector Optimization
Session organized by: Tomas Bajbar
1. Order Convex Selections of Set-valued Functions and Their 
Applications to Convex Optimization
Jerzy Motyl (j.motyl@wmie.uz.zgora.pl) Zielona Gora University, 
Poland
Let X be a Banach space while Y a Banach lattice. We consider the class of upper 
separated set-valued functions F acting from X to subsets of Y and investigate the 
problem of the existence of order-convex selections of F. Next we will discuss 
applications of obtained selection results to the theory of differential and 
stochastic inclusions. We will investigate the existence and properties of 
solutions of such inclusions, like stability or lower-upper bounds. In the second 
part of the talk we will discuss the applicability of obtained selection results to 
some deterministic and stochastic optimal control problems. Some examples will 
be presented also. References 1. J. Motyl, Caratheodory convex selections of 
set-valued functions in Banach lattices, Topol. Meth. Nonlin. Anal. 43 (2014) 2. 
J. Motyl, Stochastic retarded inclusion with Caratheodory-upper separated 
multifunctions, Set-Valued and Variational Analysis (2016, to appear) DOI 
10.1007/s11228-015-0324-9 3. J. Motyl, Caratheodory-convex selections of 
multifunctions and their applications, Journal of Nonlinear and Convex Analysis, 
(accepted for publication) 4. J. M. Bismut, Conjugate Convex Functions in 
Optimal Stochastic Control, J. Math. Anal. Appl. 44 (1973),  5. R.T. Rockafellar, 
Duality in optimal controls, Math. Control Theory, Lecture Notes in Math. 680 
(1978), 
2016_iccopt.indb   90 2016/07/22   11:58:21
ABSTRACTS
ICCOPT 2016 91
2. Existence of Set Equilibrium Problem via Ekelandʼs Variational 
Principle
Yousuke Araya (yousuke.araya@p.chibakoudai.jp) Chiba Institute of 
Technology, Japan
There are two types of criteria of solutions for the set-valued optimization 
problem, the vectorial criterion and set optimization criterion. The first criterion 
consists of looking for efficient points of set valued map and is called set-valued 
vector optimization problem. On the other hand, Kuroiwa-Tanaka-Ha and Jahn-
Ha started developing a new approach to set-valued optimization which is based 
on comparison among values of the set-valued map. In this presentation, we treat 
the second type criterion and call set optimization problem. In this presentation, 
first we introduce several types of set equilibrium problem which are 
generalizations of vector equilibrium problem and set optimization problem, 
respectively. We also investigate relationships between the above problem. By 
using some types of nonlinear scalarizing functions for set-valued maps which 
are generalization of Tammer-Weidnerʼs scalarizing functions for vectors, we 
give several types of a set-valued Ekelandʼs variational principles related to 
equilibrium problem. We also investigate relationships between the above 
existence theorems.
3. On the Real Jacobian Conjecture and Newton Polytopes
Tomas Bajbar (bajbar@kit.edu) Karlsruhe Institute of Technology, 
Germany, Oliver Stein
We discuss the relationship between the global diffeomorphism property of 
polynomial maps and the non-vanishing determinant of the corresponding 
Jacobian matrix by analysing the coercivity property of some specific sum of 
squares polynomials via their Newton polytopes.

■ Wed.C.5D
Wednesday, 17:00-18:15, Room 5D
Nonconvex Splitting Methods and Applications
Cluster: Convex and Nonsmooth Optimization
Session organized by: Wotao Yin
1. Alternating Direction Method of Multipliers for a Class of 
Nonconvex and Nonsmooth Problems with Applications to 
Background/Foreground Extraction
Lei Yang (lei.yang@connect.polyu.hk) The Hong Kong Polytechnic 
University, China, Ting Kei Pong, Xiaojun Chen
We study a general optimization model, which covers a large class of existing 
models in many applications as special cases. In particular, it can be nuclear-
norm-free, and can incorporate different possibly nonconvex sparsity inducing 
regularization functions, such as the `p quasi-norm for 0 < p < 1, for the 
background/foreground extraction problem. To solve the resulting possibly 
nonconvex optimization problem, we adapt the alternating direction method of 
multipliers (ADMM) with a general dual step-size to solve a reformulation that 
contains three blocks of variables, and analyze its convergence. We show that for 
any dual step-size less than the golden ratio, there exists a computable threshold 
such that if the penalty parameter is chosen above such a threshold and the 
sequence thus generated by our ADMM is bounded, then the cluster point of the 
sequence gives a stationary point of the nonconvex optimization problem. We 
achieve this via a potential function specifically constructed for our ADMM. 
Moreover, we establish the global convergence of the whole sequence if, in 
addition, this special potential function is a Kurdyka-Łojasiewicz function. Some 
numerical results are given to show the efficiency of our ADMM with a nontrivial 
dual step-size.
2. ExtraPush for Convex Decentralized Optimization over Directed 
Networks with Extensions
Jinshan Zeng (jsh.zeng@gmail.com) Jiangxi Normal University, 
China, Wotao Yin
In this talk, we extend the existing algorithms Extra and subgradient-push to a 
new algorithm ExtraPush for convex consensus optimization over a directed 
network. When the stationary distribution of the network is easily computed in 
advance, we propose a simplified algorithm called Normalized ExtraPush. These 
algorithms use a fixed step size like in Extra and accept the column-stochastic 
mixing matrices like in subgradient-push. We present preliminary analysis for 
ExtraPush under a bounded sequence assumption. For Normalized ExtraPush, 
we show that it naturally produces a bounded, linearly convergent sequence 
provided that the objective function is strongly convex. In numerical experiments, 
ExtraPush and Normalized ExtraPush performed similarly. With a proper step 
size, they are significantly faster than subgradient-push, even if the latter uses a 
hand-optimized step size. Lastly, we show several extensions of the proposed 
algorithms.

■ Wed.C.5E
Wednesday, 17:00-18:15, Room 5E
Advances in Robust Optimization III
Cluster: Robust Optimization
Session organized by: Bart Van Parys
1. A New Perspective on Boosting in Linear Regression via 
Subgradient Optimization and Relatives
Rahul Mazumder (rahul@vanparys.ch) Massachusetts Institute of 
Technology, USA
Boosting is one of the most powerful and popular tools in machine learning/ 
statistics that is widely used in practice. They work extremely well in a variety of 
applications. However little is known about many of the statistical and 
computational properties of the algorithm, and in particular their interplay. We 
analyze boosting algorithms in linear regression from the perspective modern 
first-order methods in convex optimization. We show that classic boosting 
algorithms in linear regression, namely the incremental forward stagewise 
algorithm (FSe) and least squares boosting (LS-Boost-e), can be viewed as 
subgradient descent to minimize the maximum absolute correlation between 
features and residuals. We also propose a modification of FSe that yields an 
algorithm for the LASSO, and that computes the LASSO path. We derive novel 
comprehensive computational guarantees for all of these boosting algorithms, 
which provide, for the first time, a precise theoretical description of the amount 
of data-fidelity and regularization imparted by running a boosting algorithm with 
a pre-specified learning rate for a fixed but arbitrary number of iterations, for any 
dataset.
2. Stochastic Optimization with Data: Large Deviation Limits
Bart Van Parys (bart@vanparys.ch) Massachusetts Institute of 
Technology, USA, Bart PG Van Parys, Peyman Mohajerin Esfahani, 
Daniel Kuhn
A large deviation theory perspective is offered on distributionally robust 
optimization for stochastic programming with both independent identically 
distributed (i.i.d.) as well as Markov dependent data. In any robust approach 
there is an inherent competition between robustness and optimality. An intuitive 
Pareto optimality condition between both interests is offered by large deviation 
theory. In a common framework, we show that the relative entropy and 
conditional relative entropy balls are Pareto optimal for either i.i.d. and Markov 
data respectively.

■ Wed.C.5F
Wednesday, 17:00-18:15, Room 5F
Novel Perspectives on Nonlinear Optimization
Cluster: Sparse Optimization and Information Processing
Session organized by: Coralia Cartis
Session chair: Martin Lotz
1. Global Optimization via Eigenvalues
Yuji Nakatsukasa (Yuji.Nakatsukasa@maths.ox.ac.uk) University of 
Oxford, United Kingdom, Satoru Adachi, Satoru Iwata, Shinsaku 
Sakaue, Akiko Takeda
While non-convex optimization problems are generally regarded as difficult or 
computationally intractable, the Courant-Fischer theorem for symmetric 
eigenvalue problems suggests that each eigenpair corresponds to a stationary 
point of a non-convex optimization problem (in this case the Rayleigh quotient). 
More generally, matrix eigenvalue problems form an important class of problems 
that can be solved efficiently; this includes generalized, polynomial and 
multiparameter eigenvalue problems. This observation suggests conversely that 
perhaps a global solution for some non-convex optimization problems can be 
obtained by solving an eigenvalue problem. In this work we identify such 
optimization problems: namely, we show that a global solution for non-convex 
QCQP with one or two constraints can be reduced to an eigenvalue problem. As 
in the Courant-Fisher theorem, the (multiparameter) eigenvalues represent the 
KKT Lagrange multipliers, and the eigenvectors–which have a low-rank 
structure when matricized–represent the KKT points. This talk focuses on the 
simple case of QCQP with one constraint (including the trust-region subproblem), 
for which our algorithm is a single-step method (without iterations aside from 
the eigenvalue computation) and performs well relative to existing algorithms.
2. On Multigrid Methods in Convex Optimization
2016_iccopt.indb   91 2016/07/22   11:58:21
ABSTRACTS
ICCOPT 201692
Michal Kocvara (m.kocvara@bham.ac.uk) University of Birmingham, 
United Kingdom, Coralia Cartis, Nick Gould
The aim of this talk is to design an efficient multigrid method for constrained 
convex optimization problems arising from discretization of some underlying 
infinite dimensional problems. Due to problem dependency of this approach, we 
only consider bound constraints with (possibly) a linear equality constraint. As 
our aim is to target large-scale problems, we want to avoid computation of 
second derivatives of the objective function, thus excluding Newton like 
methods. We propose a smoothing operator that only uses first-order information 
and study the computational efficiency of the resulting method. In the second 
part, we consider application of multigrid techniques to more general optimization 
problems, in particular, the topology design problem.
3. Randomized Quasi-Newton Updates are Linearly Convergent 
Matrix Inversion Algorithms
Robert Mansel Gower (r.m.gower@sms.ed.ac.uk) University of 
Edinburgh, United Kingdom, Peter Richtarik
We develop and analyze a family of stochastic/randomized algorithms for 
inverting a matrix. We also develop specialized variants maintaining symmetry 
or positive definiteness of the iterates. All methods in the family converge 
globally and linearly (the error decays exponentially), with explicit rates. In 
special cases, we obtain stochastic block variants of several quasi-Newton 
updates, including good/bad Broyden, Powell-symmetric-Broyden (PSB), 
Davidon-Fletcher-Powell (DFP) and Broyden-Fletcher-Goldfarb-Shanno 
(BFGS). Ours are the first stochastic versions of these updates shown to converge 
to an inverse of a fixed matrix. Through a dual viewpoint we uncover a 
fundamental link between quasi-Newton updates and approximate inverse 
preconditioning. Further, we develop an adaptive variant of randomized block 
BFGS, where we modify the distribution underlying the stochasticity of the 
method throughout the iterative process to achieve faster convergence. By 
inverting several matrices from varied applications, we demonstrate that our 
method is highly competitive when compared to the well established Newton-
Schulz and minimal residual methods. On large-scale problems our method 
outperforms the standard methods by orders of magnitude. Development of 
efficient methods for estimating the inverse of very large matrices is a much 
needed tool for  preconditioning and variable metric optimization methods in the 
advent of the big data.

■ Wed.C.5G
Wednesday, 17:00-18:15, Room 5G
Theoretical and Algorithmic Developments of Linear 
Optimization and Semi-infinite Linear Optimization
Cluster: Linear Optimization
Session organized by: Shiqian Ma
1. Projection: A Unified Approach to Semi-infinite Linear Programs 
and Duality in Convex Programming
Amitabh Basu (basu.amitabh@jhu.edu) Johns Hopkins University, 
USA, Richard Kipp Martin, Christopher Thomas Ryan
Fourier-Motzkin elimination is a projection algorithm for solving finite linear 
programs. We extend Fourier-Motzkin elimination to semi-infinite linear 
programs which are linear programs with finitely many variables and infinitely 
many constraints. Applying projection leads to new characterizations of 
important properties for primal-dual pairs of semi-infinite programs such as zero 
duality gap, feasibility, boundedness, and solvability. Extending the Fourier-
Motzkin elimination procedure to semi-infinite linear programs yields a new 
classification of variables that is used to determine the existence of duality gaps. 
In particular, the existence of what we call “dirty” variables can lead to duality 
gaps. Our approach has interesting applications in finite-dimensional convex 
optimization. For example, sufficient conditions for a zero duality gap, such as 
the Slater constraint qualification, are reduced to guaranteeing that there are no 
“dirty” variables. This leads to completely new proofs of such sufficient 
conditions for zero duality.
2. Strong Duality and Sensitivity Analysis in Semi-infinite Linear 
Programming
Christopher Thomas Ryan (chris.ryan@chicagobooth.edu) University 
of Chicago, USA, Amitabh Basu, Kipp Martin
Finite-dimensional linear programs satisfy strong duality (SD) and have the 
“dual pricing” (DP) property. The (DP) property ensures that, given a sufficiently 
small perturbation of the right-hand-side vector, there exists a dual solution that 
correctly “prices” the perturbation by computing the exact change in the optimal 
objective function value. These properties may fail in semi-infinite linear 
programming where the constraint vector space is infinite dimensional. Unlike 
the finite-dimensional case, in semi-infinite linear programs the constraint vector 
space is a modeling choice. We show that, for a sufficiently restricted vector 
space, both (SD) and (DP) always hold, at the cost of restricting the perturbations 
to that space. The main goal of the paper is to extend this restricted space to the 
largest possible constraint space where (SD) and (DP) hold. Once (SD) or (DP) 
fail for a given constraint space, then these conditions fail for all larger constraint 
spaces. We give sufficient conditions for when (SD) and (DP) hold in an 
extended constraint space. Our results require the use of linear functionals that 
are singular or purely finitely additive and thus not representable as finite support 
vectors. We use the extension of the Fourier-Motzkin elimination procedure to 
semi-infinite linear systems to understand these linear functionals.
3. Faster Algorithms for Convex and Submodular Function 
Minimization
Sam Wong (sam.cw.wong@gmail.com) University of California, 
Berkeley, USA, Yin Tat Lee, Aaron D Sidford, Sam Chiu-wai Wong
The ellipsoid method, as analyzed by Khachiyan in 1979, is the first algorithm 
for solving linear and convex programming, and belongs to a larger family 
known as the cutting plane methods. Following a series of improvements over 
Khachiyanʼs work, the cutting plane method of Vaidya from 1989 achieved the 
fastest possible theoretical running time. In this talk I will discuss a new 
improvement to Vaidyaʼs algorithm that nearly matches the theoretical limit of 
the cutting plane methods. Our techniques draw on ideas from recent advances 
in spectral graph theory and randomized linear algebra. Surprisingly, we are able 
to apply our result to obtain faster algorithm not only for convex programming 
but also for classic problems in combinatorial optimization. As applications, we 
demonstrate that our algorithm yields faster algorithms for matroid intersection 
and submodular function minimization, two of the most important problems in 
combinatorial optimization. For matroid intersection this is the first improvement 
since Cunninghamʼs in 1986.

■ Wed.C.5H
Wednesday, 17:00-18:15, Room 5H
Stability Analysis in Stochastic Programming
Cluster: Stochastic Optimization
Session organized by: Matthias Claus
1. Stability Analysis for Mathematical Programs with Distributionally 
Robust Chance Constraint
Huifu Xu (h.xu@soton.ac.uk) University of Southampton, United 
Kingdom, Shaoyan Guo, Liwei Zhang
Stability analysis for optimization problems with chance constraints concerns 
impact of variation of probability measure in the chance constraints on the 
optimal value and optimal solutions and research on the topic has been well 
documented in the literature of stochastic programming. In this paper, we extend 
such analysis to optimization problems with distributionally robust chance 
constraint where the true probability is unknown, but it is possible to construct an 
ambiguity set of distributions and the chance constraint is based on the most 
conservative selection of probability distribution from the ambiguity set. The 
stability analysis focuses on impact of the variation of the ambiguity set on the 
optimal value and optimal solutions. We start by looking into continuity of the 
robust probability function and followed with a detailed analysis of approximation 
of the function. Sufficient conditions have been derived for continuity of the 
optimal value and outer semicontinuity of optimal solution set. Case studies are 
carried out for ambiguity sets being constructed through moments and samples.
2. On Stability of Risk Averse Complementarity Problems under 
Uncertainty
Johanna Burtscheidt (johanna.burtscheidt@uni-due.de) University of 
Duisburg-Essen, Germany
Inspired by structural similarities to risk neutral one-stage stochastic optimization 
a risk averse formulation for general stochastic complementarity problems 
(SCP) based on the expected residual minimization (ERM) model will be 
presented. In particular, qualitative stability of the optimal value function of this 
problem under perturbation of the underlying Borel probability measure will be 
investigated for a special class of risk measures with respect to weak convergence 
of probability measures. An overview of NCP functions which can be used in the 
stated problem concludes the talk.
3. On Stability of Stochastic Bilevel Programs with Risk Aversion
Matthias Claus (matthias.claus@uni-due.de) University of Duisburg-
Essen, Germany
Two-stage stochastic programs and bilevel problems under stochastic uncertainty 
bear significant conceptual similarities. However, the step from the first to the 
2016_iccopt.indb   92 2016/07/22   11:58:21
ABSTRACTS
ICCOPT 2016 93
latter mirrors the step from optimal values to optimal solutions and entails a loss 
of desirable analytical properties. The talk focuses on mean risk formulations of 
stochastic bilevel programs where the lower level problem is quadratic. Based on 
a growth condition, weak continuity of the objective function with respect to 
perturbations of the underlying measure is derived. Implications regarding 
stability for a comprehensive class of risk averse models are pointed out.

■ Wed.C.5J
Wednesday, 17:00-18:15, Room 5J
Advances in Nonlinear Optimization I
Cluster: Nonlinear Optimization
Session chair: Yi-Shuai Niu
1. On New Classes of Nonnegative Symmetric Tensors and 
Applications
Bilian Chen (blchen@xmu.edu.cn) Xiamen University, China, Simai 
He, Zhening Li, Shuzhong Zhang
In this paper we introduce three new classes of nonnegative forms (or 
equivalently, symmetric tensors) and their extensions. The newly identified 
nonnegative symmetric tensors constitute distinctive convex cones in the space 
of general symmetric tensors (order 6 or above). For the special case of quartic 
forms, they collapse into the set of convex quartic homogeneous polynomial 
functions. We discuss the properties and applications of the new classes of 
nonnegative symmetric tensors in the context of polynomial and tensor 
optimization. Numerical experiments for solving certain polynomial optimization 
models based on the new classes of nonnegative symmetric tensors are presented.
2. A Mixed Integer Semidefinite Programming Approach for 
Variable Selection Avoiding Multicollinearity
Ryuta Tamura (s154558y@st.go.tuat.ac.jp) Tokyo University of 
Agriculture and Technology, Japan, Ken Kobayashi, Yuichi Takano, 
Ryuhei Miyashiro, Kazuhide Nakata, Tomomi Matsui
This research considers a mixed integer semidefinite programming approach for 
variable selection avoiding multicollinearity. Variable selection is a problem of 
selecting a better subset of explanatory variables. For variable selection, selected 
subsets are generally evaluated by information criteria, such as AIC and BIC. 
Minimizing those criteria, however, does not necessarily avoid multicollinearity, 
which causes several defects such as unreliable estimation and numerical 
instability. Multicollinearity in linear regression can be observed as a large 
condition number of a correlation coefficient matrix of selected explanatory 
variables. To obtain a good subset of explanatory variables without 
multicollinearity, we formulate it as two mixed integer semidefinite programming 
(MISDP) problems, which minimize the residual sum of squares under the 
constraint such that the corresponding condition number is at most a given upper 
bound. In addition, we perform computational experiments to solve those MISDP 
problems using SCIP and SCIP-SDP, a MIP solver and MISDP plugin for SCIP, 
respectively.
3. On Global Optimization of Mixed-01 Nonlinear Program via DC 
Algorithms
Yi Shuai Niu (niuyishuai@sjtu.edu.cn) Shanghai Jiao Tong University, 
China
Mixed-01 nonlinear program could be reformulated as DC (Difference of convex 
functions) programming problems. We will investigate its DC programming 
reformulations and propose new hybrid global optimization algorithms based on 
a well-known and efficient local optimization algorithm-DCA, and in 
combination with global optimization techniques: Branch-and-Bound (B&B), 
DC/SDP relaxations and DC-Cutting planes. We consider reformulating a 
mixed-01 nonlinear program as a DC program via continuous representation of 
discrete sets and penalization techniques on nonconvex constraints. Then we 
investigate its DC Algorithm for local optimization which will be used as upper 
bound estimation in B&B; The DC/SDP relaxations will be considered as lower 
bound estimation; And the DC-Cutting plane helps to cut off local minimizers so 
as to reduce the feasible set and accelerate the convergence of B&B. Some 
preliminary numerical simulation results will be also reported.

■ Wed.C.5K
Wednesday, 17:00-18:15, Room 5K
SDP and DNN Relaxations of Discrete Polynomial 
Optimization Problems
Cluster: Conic and Polynomial Optimization
Session organized by: Sunyoung Kim, Masakazu Kojima
1. Exact SDP Relaxations with Truncated Moment Matrix for Binary 
Polynomial Optimization Problems
Shinsaku Sakaue (sakaue.1229@gmail.com) NTT Communication 
Science Laboratories, Japan, Akiko Takeda, Sunyoung Kim, Naoki Ito
For binary polynomial optimization problems (POPs) of degree d with n 
variables, we prove that a semidefinite (SDP) relaxation problem in Lasserreʼs 
hierarchy of the SDP relaxations provides the exact optimal value when the 
relaxation order is . If binary POPs involve only even-degree 
monomials, we show that the relaxation order can be further reduced to 
. This bound on the relaxation order coincides with the conjecture 
by Laurent in 2003, which was recently proved by Fawzi, Saunderson and 
Parrilo, on binary quadratic optimization problems where d = 2. More precisely, 
Fawzi et al. proved a more general result on POPs on a finite abelian group, 
which we use to show our aforementioned results. We also numerically confirm 
that the bound is tight; we present instances of binary POPs that require solving 
at least  th SDP relaxation for general binary POPs and 
 th SDP relaxation for even-degree binary POPs to obtain the 
exact optimal values.
2. A Robust Lagrangian-DNN Method for a Class of Quadratic 
Optimization Problems 
Sunyoung Kim (skim@ewha.ac.kr) Ewha University, Korea South, 
Naohiko Arima, Masakazu Kojima, Kim-Chuan Toh
We discuss methods for improving the Lagrangian-doubly nonnegative (DNN) 
relaxation proposed in 2016 to solve a large class of nonconvex quadratic 
optimization problems. To enhance the performance of the bisection method for 
the Lagrangian-DNN relaxation, a new technique is introduced to guarantee the 
validity of the computed lower bound at each iteration of the bisection method. 
Computational results are presented to demonstrate the robustness of the 
proposed method.
3. A Lagrangian and Doubly Nonnegative Relaxation for Polynomial 
Optimization Problems in Binary Variables
Masakazu Kojima (kojimamasakazu@icloud.com) Chuo University, 
Japan, Sunyoung Kim, Kim-Chuan Toh
We discuss an extension of the Lagrangian-DNN method, which was originally 
proposed for a class of quadratic optimization problems with equality and 
complementatity constraints in nonnegative and binary variables, to a hierarchy 
of DNN relaxations of polynomial optimization problems in binary variables. 
We also show its effectiveness and computational efficiency through numerical 
results. 

■ Wed.C.5L
Wednesday, 17:00-18:15, Room 5L
Advances in First-Order Methods and Handling 
Uncertainty
Cluster: Convex and Nonsmooth Optimization
Session organized by: Fatma Kilinc-Karzan
1. First-Order Methods for Robust Convex Optimization
Nam Ho-Nguyen (hnh@andrew.cmu.edu) Carnegie Mellon 
University, USA, Fatma Kilinc-Karzan
Robust optimization is a framework to model parameter uncertainty in 
optimization problems. Inspired by recent developments, we present several 
efficient first-order methods to approximately solve robust convex optimization 
problems. We also introduce the notion of weighted regret online learning and 
the online saddle-point problem, which form key building blocks for our 
methods. Finally, we discuss some proximal-type algorithms for these problems.
2. Incremental Methods for Additive Convex Cost Optimization
Mert Gurbuzbalaban (mert.gurbuzbalaban@gmail.com) 
Massachusetts Institute of Technology, USA, Asu Ozdaglar, Pablo A 
Parrilo
Motivated by machine learning problems over large data sets and distributed 
optimization over networks, we consider the problem of minimizing the sum of 
a large number of convex functions. We develop and study incremental methods 
for solving such problems, in particular for the random reshuffling method we 
provide a sharp convergence rate result which answers an open question.
3. A Second-Order Cone Based Approach for Solving the Trust 
Region Subproblem and Its Variants
2016_iccopt.indb   93 2016/07/22   11:58:22
ABSTRACTS
ICCOPT 201694
Fatma  Kilinc-Karzan (fkilinc@andrew.cmu.edu) Carnegie Mellon 
University, USA, Nam Ho-Nguyen
We study the trust region subproblem (TRS) of minimizing a nonconvex 
quadratic function over the unit ball with additional conic constraints. Despite 
having a nonconvex objective, it is known that the TRS and a number of its 
variants are polynomial-time solvable. In this paper, we follow a second-order 
cone based approach to derive an exact convex formulation of the TRS. As a 
result, our study highlights an explicit connection between the nonconvex TRS 
and smooth convex quadratic minimization, which allows for the application of 
cheap iterative methods such as Nesterovʼs accelerated gradient descent, to the 
TRS. Under slightly stronger conditions, we give a low-complexity 
characterization of the convex hull of its epigraph without any additional 
variables. We also explore the inclusion of additional hollow constraints to the 
domain of the TRS, and convexification of the associated epigraph.

■ Thu.A.1S
Thursday, 9:00-10:15, Room 1S
ADMM-like Methods for Convex Optimization and 
Monotone Inclusions
Cluster: Nonlinear Optimization
Session organized by: Jonathan Eckstein
1. Distributed Proximal Gradient Methods for Cooperative Multi-
Agent Consensus Optimization
Necdet Serhat Aybat (nsa10@psu.edu) Pennsylvania State University, 
USA, Shiqian Ma
In this talk, decentralized methods for solving cooperative multi-agent consensus 
optimization problems over an undirected network of agents will be discussed, 
where only those agents connected by an edge can directly communicate with 
each other. The objective is to minimize the sum of agent-specific composite 
convex functions, i.e., each term in the sum is a private cost function belonging 
to an agent. The first part of the talk is on the unconstrained case, and the second 
part focuses on the constrained case, where each agent has a private conic 
constraint set. For the constrained case the optimal consensus decision should lie 
in the intersection of these private sets. This optimization model abstracts a 
number of applications in machine learning, distributed control, and estimation 
using sensor networks. Distributed methods based on linearized ADMM, and 
saddle point methods will be discussed. I will provide convergence rates both in 
sub-optimality error and consensus violation; examine the effect of underlying 
network topology on the convergence rates of the proposed decentralized 
algorithms; and discuss how to extend these methods to time-varying topology.
2. ARock: Asynchronous Parallel Coordinate Update Framework 
and Its Application to ADMM
Wotao Yin (wotaoyin@math.ucla.edu) University of California, Los 
Angeles, USA, Brent Edmunds, Zhimin Peng, Tianyu Wu, Yangyang 
Xu, Ming Yan
Single-core performance stopped improving around 2005. However, the number 
of cores has grown quikcly. Today 64 cores workstations, 2k-core GPUs, and 
even 8-core cellphones are available. To take advantages of all the cores 
available, we must parallelize our algorithms. With asynchrony, the performance 
of parallel algorithms is no longer determined by the slowest core, the most 
difficult task, or the longest communication delay. This talk explains why 
asynchronous computing is both theoretically sound and practically attractive. In 
particular, we study a randomized version of asynchronous coordinate updates to 
a fixed point problem and show its point convergence. The guarantee is that the 
performance scales linearly with the number of cores used as long as that number 
is no more than the square root of the number of variables. As special cases, 
novel asynchronous algorithms such as ADMM and EXTRA for parallel and 
distributed computing are presented. We present the ARock package for quick 
prototyping of asynchronous algorithms based on coordinate update and operator 
splitting. Numerical results will be presented.
3. Asynchronous Projective Monotone Operator Splitting Algorithms
Jonathan Eckstein (jeckstei@rci.rutgers.edu) Rutgers University, 
USA, Patrick L Combettes
We describe a new family of decomposition algorithms for finding zeroes of 
sums of maximal monotone set-valued operators and solving related problems of 
a generalized Fenchel form. The basic algorithmic framework unifies several 
prior classes of decomposition methods in which the coordination step involves 
projection onto a separating hyperplane constructed from the results of the 
subproblem evaluations. However, the key innovation is that only a subset of the 
subproblems need be evaluated between coordination steps, and subproblem 
evaluations and coordination steps may be overlapped asynchronously. 
Applications include asynchronous ADMM-like convex optimization methods.

■ Thu.A.1A
Thursday, 9:00-10:15, Room 1A
Optimization Methods for Inverse Problems 4
Cluster: Nonlinear Optimization
Session organized by: Xin Liu, Yanfei Wang
1. A Parallel Line Search Subspace Correction Method for Convex 
Optimization Problems
Qian Dong (dongqian@lsec.cc.ac.cn) Chinese Academy of Sciences, 
China, Xin Liu, Zaiwen Wen, Yaxiang Yuan
We investigate a parallel subspace correction framework for composite convex 
optimization based domain decomposition method. At each iteration, the 
algorithms solve subproblems on subspaces simultaneously to construct a search 
direction and take the Armijo line search to find a new point. They are called 
PSCLN and PSCLO, respectively, depending on whether there are overlapping 
variables. Their convergence is established under mild assumptions. We compare 
them with state-of-the-art algorithms for solving LASSO problems, which shows 
that PSCLN and PSCLO can run fast and return solutions no worse than those 
from the others. It is also observed that the overlapping scheme is helpful for the 
structured-data problem.
2. Generalized Newton Method for Globally Solving the Total Least 
Squares with Tikhonov Regularization
Yong Xia (dearyxia@gmail.com) Beihang University, China, Meijia 
Yang, Jiulin Wang
The Tikhonov regularization of the total least squares (TRTLS) is a nonconvex 
optimization problem. According to Dinkelbachʼs parametric strategy, it can be 
equivalently reduced to finding a zero point of a decreasing concave but possibly 
nonsmooth univariate function. We propose a generalized Newton method by 
replacing the derivative with thesubgradient. It globally converges to the root. 
Under a mild assumption, we show that the asymptotic rate of convergence is 
superlinear. The worst-case time complexity is less than that of the existing 
global solution methods based on bisection. Finally, we report numerical results.
3. Conditional Gradient Algorithms for Rank-k Matrix 
Approximations with a Sparsity Constraint 
Hongying Liu (liuhongying@buaa.edu.cn) Beihang University, China, 
Qian Yang
The sparsity constrained rank-k matrix approximation problem is a difficult 
mathematical optimization problem which arises in a wide array of useful 
applications in engineering, machine learning and statistics, and the design of 
algorithms for this problem has attracted intensive research activities. In this talk, 
we propose the gradient method for the task. Two conditional gradient algorithms 
are given for the cases without the orthogonality constraint and with the 
orthogonality constraint respectively. The proposed method can be referred as a 
dual version of the well-known generalized power method for sparse principal 
component analysis. The low complexity of the proposed algorithms makes 
them well suited to handle large-scale problem of sparse rank-k matrix 
approximations with a sparsity constraint. As illustrations, the algorithms are 
applied to real and simulated data with encouraging results.

■ Thu.A.1B
Thursday, 9:00-10:15, Room 1B
Recent Developments in PDE-constrained Optimization 
II
Cluster: PDE-constrained Optimization
Session organized by: Stefan Ulbrich
1. PDE Constrained Optimization with Pointwise Gradient 
Constraints
Winnifried Wollner (wollner@mathematik.tu-darmstadt.de) TU 
Darmstadt, Germany
In this talk, we will review several recent result in PDE constrained optimization 
with pointwise constraints on the gradient of the state. This includes barrier and 
penalty methods in a function space setting to eliminate the constraint on the 
gradient. Convergence of such methods is discussed. Further, we will consider 
the discretization of such problems in particular for non smooth domains, where 
the control to state mapping does not assert the gradient to be Lipschitz.
2016_iccopt.indb   94 2016/07/22   11:58:22
ABSTRACTS
ICCOPT 2016 95
2. Optimal Control of the Thermistor Problem in Three Spatial 
Dimensions
Hannes Meinlschmidt (meinlschmidt@mathematik.tu-darmstadt.de) 
TU Darmstadt, Germany, Joachim Rehberg, Christian Meyer
We consider the optimal control of the thermistor problem in three spatial 
dimensions, where the latter models the heating of a conducting material by 
means of direct current. Here, the aim is to achieve a prescribed temperature 
distribution after a given simulation time. The underlying partial differential 
equations are given as a coupled system of a parabolic- and elliptic PDE, the 
latter with mixed boundary conditions, and the system is complemented by 
control- and state constraints. Both PDEs are of quasilinear structure, with 
nonlinear coupling, and the solutions ultimately depend nonlinearly on the 
control. Nevertheless, we show that solutions to the PDE system exist uniquely, 
albeit generally only locally in time. We thus work with the set of controls 
admitting solutions which exist globally in time and prove existence of optimal 
solutions to the optimal control problem for this class of controls. Moreover, we 
obtain a well-rounded optimality theory which does not need to refer to the set of 
controls which admit only global-in-time solutions. The theoretical findings are 
complemented by numerical results. 
3. Controlling Feasibility and Optimality in Iterative Solvers for 
Optimality Systems
Roland Herzog (roland.herzog@mathematik.tu-chemnitz.de) 
Technische Universität Chemnitz, Germany, Kirk M Soodhalter
Optimality systems for equality constrained optimization exhibit a saddle-point 
structure. In PDE-constrained optimization, the solution of such large-scale 
systems by direct solvers is prohibitive, and iterative solvers must be used. The 
precontioned minimal residual method (MINRES) is a natural candidate due to 
the self-adjointness and indefiniteness of optimality systems. Traditional 
implementations of MINRES provide access only to the norm of the full residual, 
and it serves as a stopping criterion. However, MINRES does not normally 
expose the residual subvector norms, which correspond to optimality and 
feasibility, respectively. We present a modified implementation of MINRES 
which allows to monitor these quantities independently. Applications in PDE-
constrained optimization are given.

■ Thu.A.1C
Thursday, 9:00-10:15, Room 1C
Derivative-free Optimization Algorithms for Large-
Scale Problems
Cluster: Derivative-free and Simulation-based Optimization
Session organized by: Francesco Rinaldi, Zaikun Zhang
Session chair: Youhei Akimoto
1. Efficiency of Random Search on Structured Problems
Sebastian Stich (sebastian.stich@uclouvain.be) Université Catholique 
de Louvain, Belgium
Coordinate Descent methods are among the most efficient schemes for high-
dimensional optimization and their complexity is well understood. Under certain 
structural assumptions, also the use of accelerated versions of these schemes is 
theoretically justified. In this talk, we study the complexity of randomized 
derivative-free schemes. Especially, we will also discuss the benefits of 
acceleration techniques on high-dimensional problems.
2. An Indicator for the Switch from Derivative-free to Derivative-
based Optimization
Nacer Eddine Soualmi (soualmi@cerfacs.fr) Cerfacs, France, Luis 
Nunes Vicente, Segre Gratton
In some optimization problems found in applications, the derivative of the 
objective function are available but at an expensive cost, and it is desirable to 
know when to use derivative-free methods (such as direct search, for instance) or 
derivative-based methods (such as gradient or quasi-Newton methodsà. In 
general, derivative-free methods tend to make a steady initial progress when first 
applied and then become slower or even stagnate due to the lack of derivatives. 
It is thus of interest to provide a way to appropriately switch from a derivative-
free method to a derivative-based one. In this paper we develop a family of 
indicators for such a switch based on decrease properties of both classes ofo 
methods (typically used when deriving worst case complexity bounds).
3. Comparison-based Stochastic Algorithm with Adaptive Gaussian 
Model for Large-Scale Continuous Optimization
Youhei Akimoto (y_akimoto@shinshu-u.ac.jp) Shinshu University, 
Japan, Nikolaus Hansen
The covriance matrix adaptation evolution strategy (CMA-ES) is recognized as 
a state-of-the-art comparison-based stochastic algorithm for continuous 
optimization, especially when the objective function is black-box. Due to its 
ranking-based property and the adaptation of the covariance matrix of the 
Gaussian distribution, from which candidate solutions are generated, the CMA-
ES exhibits several invariance properties such as monotone transformation of the 
objective function and affine transformation of the search space, which are 
essential for a black-box scenario where a prior knowledge is limited. However, 
its computational time and space complexity scales up quadratically and the 
adaptation time for the covariance matrix increases as the number of variables 
incerases. In this talk we present a variant of CMA-ES with a restricted 
covariance model and the adaptation mechanism for the complexity of the 
restricted covariance model. The algorithm is based on the projection between 
the manifold of the positive definite symmetric matrices and its submanifold of 
restricted covariance matrices. The proposed algorithm, VkD-CMA, has 
advantages both in the internal complexity and in the number of function 
evaluations.

■ Thu.A.4A
Thursday, 9:00-10:15, Room 4A
Optimization in Energy Management Systems with 
Integrated Economic/Physical Models
Cluster: Applications in Energy, Science and Engineering
Session organized by: Toshiyuki Ohtsuka
1. Distributed Optimal Power Management Based on Dynamic 
Pricing in Multi-Period Electricity Market
Toru Namerikawa (namerikawa@sd.keio.ac.jp) Keio University, 
Japan, Yoshihiro Okawa
This paper deals with a novel distributed optimal power supply-demand 
management method based on dynamic pricing in the deregulated electricity 
market. Since power consumers and generators determine their own power 
demand or supply selfishly in the deregulated electricity market trading, 
distributed power management methods are required to maintain the power 
supply-demand balance in power grids. For this problem, the proposed method 
integrates two different time-periods electricity market, Day-ahead and Real-
time market, and solves this management problem in a distributed manner using 
electricity prices through market trading. Specifically, the proposed method, first, 
derives the optimal locational electricity prices which maximize social welfare 
of the entire power network in the day-ahead market based on alternating 
decision makings of market players. Then, the proposed method compensates the 
power imbalance caused by some problems such as prediction errors via 
negawatt trading in the real-time market, in which power consumers reduce their 
demand, while they receive monetary incentives from the market operator. The 
proposed method shows the optimal incentive design method using the day-
ahead prices to minimize the power adjustment cost in real-time market trading. 
Finally, numerical simulation results are shown to demonstrate the effectiveness 
of the proposed method.
2. Real-Time Pricing Leading to Optimal Operation and Applications 
to Energy Management Systems
Kenji Hirata (hirata@nagaokaut.ac.jp) Nagaoka University of 
Technology, Japan, Kenko Uchida
We consider interactions between an independent entity, called a utility, and 
multiple agents, which correspond to generators or consumers in power networks. 
The utility wants to realize the socially optimal solution that fulfills power 
supply demand balancing. Each agent is allowed to determine its desired set-
point according to the individual profit. In order to align the individual decision 
makings of the agents with the socially optimal solution, the utility is allowed to 
provide an additional price, which conceptually represents tax or subsidy. We 
propose a real-time pricing strategy of the utility and show three application case 
studies: a distributed voltage regulation of a distribution power gird in which a 
voltage rise occurs due to revers power flow from PV generators of households, 
a distributed voltage regulation for a large scale PV system with experimental 
case study results, a distributed management of EV/PHEV storage charging to 
fulfill the power flow balancing between a prediction based power supply and 
actual demands.
3. A Study on Modeling and Optimization of an Energy Demand 
Network with Strategic Aggregators
Yusuke Okajima (y_okajima@aoni.waseda.jp) Waseda University, 
Japan, Toshiyuki Murao, Takeshi Hatanaka, Kenji Hirata, Kenko 
Uchida
2016_iccopt.indb   95 2016/07/22   11:58:22
ABSTRACTS
ICCOPT 201696
In this research, we address modeling of energy market with aggregators, which 
are expected to implement demand response and negotiate with utility companies 
on behalf of customers. We model the aggregator as a strategic player since it is 
regarded as an organization for profit. The decision variable is the incentive 
transferred to the customers who are also strategic and choose an aggregator 
maximizing their own benefit including the transferred incentive. In the 
formulation of the aggregatorʼs cost, our main focus is placed on the concept of 
market power, which represents the economic impact on the market price. The 
aggregators are assumed to aim at maximizing the market power by collecting as 
many customers as possible in order to maximize the income, while minimizing 
the outcome paid to the customers. Then, after fixing decision-making rules of 
the customers, utility and aggregators, we discuss the benefit of introducing the 
aggregators.

■ Thu.A.4B
Thursday, 9:00-10:15, Room 4B
Applications to Practical Problems
Cluster: Applications in Energy, Science and Engineering
Session chair: Renan S Maglasang
1. A Successive LP Approach with C-VaR Type Constraints for 
IMRT Optimization
Shogo Kishimoto (kishimoto.s.ac@m.titech.ac.jp) Tokyo Institute of 
Technology, Japan, Makoto Yamashita
An important optimization problem arising from intensity-modulated radiation 
therapy (IMRT) is to determine the intensities of beams so that the beam 
irradiation effectively reduces cancer tumors. On the one hand we should give 
high dose volume to tumors, but on the other hand we need to keep the dose 
volume for critical organs under certain threshold. Since the dose volume 
constraints (DVCs) are closely related to the average of a fractional volume of a 
target structure, Romeijn et al. introduced C-VaR (conditional value at risk) type 
constraints in order to model the beam intensity problem as an LP problem. 
However, their approach was very sensitive to outliers in the structure and failed 
in some test instances to find a feasible solution that satisfies all the DVCs. We 
propose a successive LP approach that incorporates the C-VaR type constraints 
and the effective framework due to Romeijn et al. Using the solutions from LP 
problems, we identify the outliers and remove them from the C-VaR type 
constraints. We verified through numerical tests that our approach successfully 
found a feasible solution in practical computation time.
2. A Tri-Level Optimization Model for Private Road Competition 
Problem with Traffic Equilibrium Constraints
Gu Yan (yukiguyan@outlook.com) Kyoto University, Japan, Xingju 
Cai, Deren Han, David ZW Wang
Build–Operate–Transfer scheme is widely applied in many cities, wherein the 
firms compete to invest in the road building and aim at maximizing their profits 
via collecting tolls from users traveling on the roads under their control. In many 
research works on analyzing the private road competition problem in the existing 
literature, it is assumed that all the new roads would be constructed and operated 
by private firms under BOT scheme. In this case, as the primary objective of 
private firms is profit maximization, the system would be less efficient in terms 
of managing the network traffic and achieving the governmentʼs goal of 
maximizing social welfare. Therefore, in this study, we propose that the 
government should take more proactive participation into the new road 
construction and operation, imposing impacts on the private firmsʼ investment 
and tolling strategy so that a certain social objective could be fulfilled. We 
characterize this problem into a tri-level optimization model. In such a multi-
competition game, strategic interaction and market equilibrium must be taken 
into consideration to determine their investment and price. We then propose a 
heuristic approach to solve the model and finally its validity is verified by a 
simple network numerical example. 
3. The Shelf Space Allocation Problem under Carbon Tax and 
Emission Trading Policies
Renan S Maglasang (maglasangrenan@gmail.com) National Taiwan 
University of Science and Technology, Taiwan, Vincent F Yu, Yu-
Chung Tsao
A mixed integer non-linear programming (MINLP) model is formulated for the 
shelf space allocation problem (SSAP) under two common environmental 
policies: the Carbon Tax System and the Emission Trading System. For each 
system, the impact on profitability and the opportunities for emission cuts are 
presented. Specifically, we explore via numerical experiments the impact of 
fixed carbon tax, emission permit, and emission price on two of the most 
important in-store management decisions in retail, the shelf space capacity and 
the product allocation decisions. Real-life retail data of four product categories 
are tested and solved to optimality using COUENNE. Optimal solutions are 
analyzed to gain insights important to inform both retailers and governmental 
decision makers.

■ Thu.A.5A
Thursday, 9:00-10:15, Room 5A
Applications in Production and Energy Economics
Cluster: Applications in Energy, Science and Engineering
Session chair: Parlo Krokhmal
1. To Predict the Bottleneck Node by Queueing Network Modeling of 
a Production Model with Long Lead Time and Large Variety of 
Small Quantity Production
Takako Hoshiyama (thoshiyama@mirror.ocn.ne.jp) The University of 
Tokyo, Japan, Toshio Akimitsu
To evaluate the production operations of large variety of small quantity 
production and long lead time production in a wide range activity, we look for an 
easy to handle evaluation method than ordinary product simulator. This paper is 
adapting the Jackson network model for open networks, which has been utilized 
to mathematically understanding the behaviour of packets in the communication 
and network industry. We have done the scenarios and numerical sampling 
experiments. This was set metrics to evaluate the performance of the model, i.e. 
the waiting time, the standard deviation of the waiting time, and the delay 
probability of a node with multiple receptors. We present a case study of a 
construction vehicle manufacturer. This has large variety small quantity 
production and long lead-time production, i.e. 3 weeks for one vehicle assembly 
lead time, dealing with the mathematical nature of GI/G/1, GI/G/m queues. From 
the numerical sampling experiments, we discovered that has a long production 
lead time imposes a great burden on its node. This evaluation method is able to 
discover the bottleneck nodes (process). This is also able to serve as reference 
materials of the inventory reduction rescheduler by TOC (theory of constraints).
2. Shape Optimization for Contact Problems Based on Isogeometric 
Analysis and Nonconvex Bundle Methods 
Benjamin Manfred Horn (bhorn@mathematik.tu-darmstadt.de) TU 
Darmstadt, Germany, Stefan Ulbrich
Mechanical connectors appear in many real life optimization problems. We 
consider the shape optimization of such a connector, where the connector are 
modeled as a contact problem in linear elasticity. In order to avoid a gap between 
the the representation in CAD systems and the finite element simulation used by 
the mathematical simulation we choose an isogeometric approach to solve the 
contact problem within the optimization method. Additionally we get an exact 
geometry description with smooth boundaries. We handle the contact conditions 
using the mortar method and solve the resulting contact problem with a 
semismooth Newton method. The optimization problem is nonconvex and 
nonsmooth due to the contact conditions. To keep the number of the time 
consuming simulations as low as possible, we use a derivative based optimization 
method. The design derivatives can be efficiently calculated with the adjoint 
approach. The resulting optimization problem is solved with a modified bundle 
trust region algorithm. 
3. A Semidefinite Programming Approach to Computing Bounds on 
the Overall Properties of Composite Materials with Randomly 
Oriented Fibers
Pavlo Krokhmal (krokhmal@email.arizona.edu) University of 
Arizona, USA, Olesya I Zhupanska, Yana Morenko
This work is concerned with evaluation of the overall elastic properties and 
development of improved variational bounds on the overall elastic properties of 
fiber-reinforced composites with arbitrary orientational distribution of fibers. 
The problem of finding the tightest bounds for the elastic tensor of a composite 
material with non-aligned phases is formulated as a nonlinear semidefinite 
programming (SDP) problem. The conducted analysis of the feasible region and 
the objective function allows one to reduce the obtained nonlinear SDP problem 
to a nonlinear programming problem, for which a semi-analytic solution is 
derived. The computational results show that the constructed solution improves 
the Hashin-Schtrikman-Walpole bounds, which are the only known in the 
literature bounds for the composites with non-aligned microstructures and are 
only valid for the case of uniform random distributions of microstructure.

■ Thu.A.5C
Thursday, 9:00-10:15, Room 5C
Non-convex Vector Optimization and Applications
2016_iccopt.indb   96 2016/07/22   11:58:22
ABSTRACTS
ICCOPT 2016 97
Cluster: Multi-Objective and Vector Optimization
Session organized by: Christian Günther, Marcus Hillmann
Session Chair: Marius Durea
1. Minimal Time Function with Respect to a Set of Directions and 
Applications
Marius Durea (durea@uaic.ro) Alexandru Ioan Cuza University, 
Romania
We introduce a new type of minimal time function with respect to a set of 
directions and we study several of its properties such as semicontinuity, 
convexity, Lipschitz behavior, and subdifferential calculus. Then we show that 
these properties are good enough in order to derive a generalized version of 
Ekeland Variational Principle (using this new minimal time function instead of 
the distance function) and to get necessary optimality conditions for a location 
vector optimization problem. At the end of the presentation we show how this 
study opens some new possibilities to define and to explore directional behavior 
of single and set-valued mappings, which act as objectives for various 
optimization problems.

■ Thu.A.5D
Thursday, 9:00-10:15, Room 5D
Extended Formulations and Related Topics
Cluster: Linear Optimization
Session organized by: David Bremner
1. Strong Reductions for Linear and Semidefinite Programs
Sebastian Pokutta (sebastian.pokutta@isye.gatech.edu) Georgia 
Institute of Technology, USA, Gábor Brown, Aurko Roy
Linear and semidefinite programming are two core optimization paradigms with 
many important applications. However, the expressive power of these modeling 
paradigms is only partially understood so far and extended formulations are a 
powerful and natural tool to analyze the possibilities and limitations of linear and 
semidefinite programming formulations. We will present a strong reduction 
mechanism both for LPs and SDPs, which allows to establish strong 
inapproximability results for various problems, including e.g., vertex cover, 
bounded-degree matching, and sparsest cut. Moreover, the reduction mechanism 
induces an ordering of relative hardness of the underlying problems. 
2. A Note on Extended Formulations of Lower-truncated Transversal 
Polymatroids
Hidefumi Hiraishi (hiraishi1729@is.s.u-tokyo.ac.jp) The University 
of Tokyo, Japan, Shuichi Hirahara, Hiroshi Imai
Extended formulations of (k;l)-sparsity matroids of n-vertex and m-edge graph 
are investigated by (Iwata et al., Mathematical Programming, 2015). This talk 
shows the results can be obtained by interpreting results on (k;l)-lower-truncated 
transversal (poly)matroids in (Imai, JORSJ, 1983) from the viewpoint of 
extended formulations, the same O(mn) bound when k ≥ l and a better bound 
O(m2) when k < l. A unified polymatroidal approach is given to derive more 
general understandings. Some extensions of these results will be touched upon.
3. Small Linear Programs for Decision Problems
David Bremner (bremner@unb.ca) University of New Brunswick, 
Canada, David Avis, Hans Raj Tiwary, Osamu Watanabe
In the classical “Edmonds” model of solving combinatorial optimization 
problems with linear programs, one can read the problem objective directly from 
the linear program optimal solution. Results of Rothvo  (2014) showed the 
limitations of this approach, since no polynomial size LP that projects onto 
Edmondʼs matching polytope exists. Several authors have studied more general 
kinds of reductions where strong hardness results still hold. For decision 
problems the picture is rather different. In this talk Iʼll discuss two main ideas - 
“Weak extended formulations”, where we only insist on projecting integer 
vertices, - “Compiling” algorithms to LPs that simulate running those algorithms 
for a bounded amount of time. With those ideas in hand we can derive polynomial 
size polytopes for the (weighted) matching decision problem.

■ Thu.A.5E
Thursday, 9:00-10:15, Room 5E
Advances in Robust Optimization IV
Cluster: Robust Optimization
Session organized by: Wolfram Wiesemann
1. Duality in Two-Stage Adaptive Linear Optimization: Faster 
Computation and Stronger Bounds
Frans de Ruiter (f.j.c.t.deruiter@uvt.nl) Tilburg University, 
Netherlands, Dimitris Bertsimas
In this talk we derive and exploit duality in general two-stage adaptive linear 
optimization models. The resulting model is again a two-stage adaptive linear 
optimization model. The new dualized model differs from the primal formulation 
in its dimension and uses a different description of the uncertainty set. We show 
that the optimal primal affine policy can be directly obtained from the optimal 
affine policy in the dual formulation. We provide empirical evidence that the 
dualized model in the context of two-stage lot-sizing on a network and two-stage 
facility location problems solves an order of magnitude faster than the primal 
formulation with affine policies. We also provide an explanation and associated 
empirical evidence that offer insight on which characteristics of the dualized 
formulation make computations faster. Furthermore, the affine policy of the dual 
formulations can be used to provide stronger lower bounds on the optimality of 
affine policies.
2. Regularization via Mass Transportation
Daniel Kuhn (daniel.kuhn@epfl.ch) École Polytechnique Fédérale de 
Lausanne (EPFL), Switzerland, Soroosh Shafieezadeh Abadeh, 
Peyman Mohajerin Esfahani
The goal of regression and classification methods in supervised learning is to 
minimize the empirical risk, that is, the expectation of some loss function 
quantifying the prediction error under the empirical distribution. When facing 
scarce training data, overfitting is typically mitigated by adding regularization 
terms to the objective that penalize hypothesis complexity. In this paper we 
introduce new regularization techniques using ideas from distributionally robust 
optimization, and we give new probabilistic interpretations to existing techniques. 
Specifically, we propose to minimize the worst-case expected loss, where the 
worst case is taken over the ball of all (continuous or discrete) distributions that 
have a bounded transportation distance from the (discrete) empirical distribution. 
By choosing the radius of this ball judiciously, we can guarantee that it contains 
the unknown data-generating distribution with high confidence, thus facilitating 
new out-of-sample performance guarantees. We prove that the resulting 
regularized learning problems are tractable and can be tractably kernelized for 
many popular loss functions. We validate our theoretical out-of-sample 
guarantees through simulated and empirical experiments.
3. Ambiguous Joint Chance Constraints under Mean and Dispersion 
Information
Wolfram Wiesemann (ww@imperial.ac.uk) Imperial College Business 
School, United Kingdom, Grani Hanasusanto, Vladimir Roitch, Daniel 
Kuhn
We study joint chance constraints where the distribution of the uncertain 
parameters is only known to belong to an ambiguity set characterized by the 
mean and support of the uncertainties and by an upper bound on their dispersion. 
This setting gives rise to pessimistic (optimistic) ambiguous chance constraints, 
which require the corresponding classical chance constraints to be satisfied for 
every (for at least one) distribution in the ambiguity set. We provide tight 
conditions under which pessimistic and optimistic joint chance constraints are 
computationally tractable, and we show numerical results that illustrate the 
power of our tractability results.

■ Thu.A.5F
Thursday, 9:00-10:15, Room 5F
Low-Order Algorithms for Nonlinear Optimization
Cluster: Convex and Nonsmooth Optimization
Session organized by: Shuzhong Zhang
1. Barzilai-Borwein Step Size for Stochastic Gradient Descent
Shiqian Ma (sqma@se.cuhk.edu.hk) The Chinese University of Hong 
Kong, Hong Kong, Conghui Tan, Yu-Hong Dai, Yuqiu Qian
In stochastic gradient descent (SGD) methods, a key issue is how to select an 
appropriate step size. The common practice in SGD is to either use a diminishing 
step size, or to tune a fixed step size by hand. Diminishing step size usually leads 
to very slow convergence. Tuning a fixed step size by hand is usually time 
consuming and needs a priori knowledge about the problem. In this talk, we 
propose to use the Barzilai-Borwein (BB) method to compute step size for SGD 
and SVRG, which lead to two new methods: SGD-BB and SVRG-BB. We prove 
that SVRG-BB converges linearly for strongly convex objective function. As a 
by-product, we also prove the linear convergence of the SVRG with Option I 
proposed in (Johnson & Zhang, 2013), which has been missing in the literature. 
2016_iccopt.indb   97 2016/07/22   11:58:22
ABSTRACTS
ICCOPT 201698
The computational efforts needed by SGD-BB and SVRG-BB are almost the 
same as the ones needed by the original SGD and SVRG, respectively. Numerical 
experiments show that the performance of SGD-BB and SVRG-BB are 
comparable to and sometimes even better than the SGD and SVRG with best-
tuned step sizes.
2. Distributed Stochastic Variance Reduced Gradient Methods and a 
Lower Bound for Communication Complexity
Qihang Lin (qihang-lin@uiowa.edu) The University of Iowa, USA, 
Jason Lee, Tengyu Ma, Tianbao Yang
We study distributed optimization algorithms for minimizing the average of 
convex functions. The applications include empirical risk minimization problems 
in statistical machine learning where the datasets are large and have to be stored 
on different machines. We design a distributed stochastic variance reduced 
gradient algorithm that, under certain conditions on the condition number, 
simultaneously achieves the optimal parallel runtime, amount of communication 
and rounds of communication among all distributed first-order methods up to 
constant factors. Our method and its accelerated extension also outperform 
existing distributed algorithms in terms of the rounds of communication as long 
as the condition number is not too large compared to the size of data in each 
machine. We also prove a lower bound for the number of rounds of communication 
for a broad class of distributed first-order methods including the proposed 
algorithms in this paper. We show that our accelerated distributed stochastic 
variance reduced gradient algorithm achieves this lower bound so that it uses the 
fewest rounds of communication among all distributed first-order algorithms.
3. Distributional Robust Optimization for IFR Distributions
Simai He (simaihe@mail.shufe.edu.cn) Shanghai University of 
Finance and Economics, China, Bo Jiang, Chris Ryan, Teng Zhang
One of the major critiques of the distributional free robust bounds is the discrete 
nature of its optimum distribution. Increasing failure rate (IFR) and log-concave 
are two widely accepted and used distribution classes in various research 
domains. Even though the corresponding math programming problem is highly 
non-convex, we establish its optimal solution structure via analyzing a relaxed 
problem. Furthermore, we develop numerical toolbox for optimally solving the 
distributional free moment problems with IFR and log-concave distribution 
assumptions.

■ Thu.A.5G
Thursday, 9:00-10:15, Room 5G
Advanced Topics of Linear Optimization
Cluster: Linear Optimization
Session chair: Aurelio Oliveira
1. Weak Duality Theorems for Two Families of Complex Optimization 
Problems
Toshihiro Kosaki (toshihirokosaki@gmail.com) Stera Link Co.,Ltd, 
Japan
We consider two families of complex problems, which are LPs, QPs and two 
SOCPs. The former family of formulation is based on complex inner product in 
both objective function and constraint functions. The latter family of formulation 
is based on complex inner product based on complex inner product in only 
objective function. We show weak duality theorems for the complex optimization 
problems.
2. A Clientʼs Health from the Point of View of the Nutrition Adviser 
using Operational Research
Lucie Schaynová (lucie.schaynova@osu.cz) University of Ostrava, 
Czech Republic
In this talk, we analyse daily nutrient requirements of an individual person from 
the point of view of the nutrition adviser. The goal is to simplify the adviserʼs 
menu planning for a client as much as possible. After that, we design an individual 
eating plan for a week using a linear optimization model. The model respects 
eating habits and it follows the clientʼs or adviserʼs recommended recipes taking 
the compatibility of foods into account. The model involves linear constraints to 
ensure that two incompatible foods are not used in the same course. The model 
comprises further constraints to guarantee the diversity of the courses. The 
purpose of other constraints is to use an exact amount of some food, e.g. one 
whole egg or 100 grams of cheese, during the week. The model is made up so that 
the final dietary plan for the client is as natural as possible. The model gives 
recommended amounts of foods for recipe weekly planning.
3. Reducing Interior Point Method Iterations via Continued 
Directions
Aurelio Oliveira (aurelio@ime.unicamp.br) University of Campinas, 
Brazil, Lilian Berti, Carla Ghidini
The continued iteration is used with the predictor corrector interior point method 
with multiple centrality directions in order to reduce the number of iterations to 
achieve an optimal solution for the linear programming problem. The continued 
iteration can be interpreted as the projection of the search direction, already 
determined by the predictor corrector method. It can be used in two different 
forms, before or after of a complete predictor corrector iteration. The new 
direction, called continued direction, is computed with very low effort compared 
to an iteration method. Although there is an increase of the computational effort 
per iteration to use the continued iteration, the expected reduction in the number 
of iterations, enables the reduction of the total computational time. Some 
proposals for the continued direction are developed with the purpose of 
increasing the reduction of primal and dual infeasibility in each iteration of the 
predictor corrector method. The multiple centrality directions are computed only 
after the continued directions are applied. A computational experiments 
comparison with large-scale problems for the predictor corrector method with 
and without continued iteration is performed, showing that the method achieves 
good performance using the proposed approach.

■ Thu.A.5H
Thursday, 9:00-10:15, Room 5H
Stochastic Complementarity Problems and Sample 
Average Approximation
Cluster: Stochastic Optimization
Session organized by: Hailin Sun, Dali Zhang
1. Distributionally Robust Games with an Application to 
Environmental Problem
Shaojian Qu (qushaojian@163.com) University of Shanghai for 
Science and Technology, China, Mark Goh, Robert de Souza
In this paper, we propose a distributionally robust optimization approach for N - 
player, nonzero sum finite state/action games with incomplete information where 
the payoff matrix is stochastic with an imprecise distribution which is assumed 
to be attached to an a-prior known set. A distributionally robust approach is used 
to cope with our setting in the games by combining the stochastic optimization 
approach and the robust optimization approach which can be called the 
distributionally robust games. We show that the existence of the equilibria for the 
distributionally robust games. The computation method for equilibrium point, 
with the first- and second information about the uncertain payoff matrix, can be 
reformulated as semidefinite programming problems which can be tractably 
realized. An environmental game with uncertainty is analyzed by applying the 
distributionally robust game theory.
2. Computation of Stochastic Nash Equilibrium via Variable Sample
Dali Zhang (dali.zhang@outlook.com) Shanghai Jiao Tong University, 
China, Lizhi Wang, Ming Dong
In this paper, we propose a variable sample distributed algorithm for the 
computation of stochastic Nash equilibrium in which the objective functions are 
replaced, at each iteration, by sample average approximations. We investigate the 
contraction mapping properties of the variable sample distributed algorithm and 
show that the accuracy of estimators yielded in the algorithms to their true 
counterparts are determined by both the sample size schedules and the contraction 
mapping parameters. We also investigate conditions on the sample size schedules 
under which the accumulation point generated by the algorithm asymptotically 
converges to the true Nash equilibrium. In the numerical tests, we comparatively 
analyze the accuracy and precision errors of estimators with different sample 
size schedules with respect to the sampling loads and the computational times. 
Finally, we present numerical results on the effectiveness of different cumulative 
sampling schemes for the algorithm.
3. SAA-Regularized Methods for Multiproduct Price Optimization 
under the Pure Characteristics Demand Model
Hailin Sun (hlsun@njust.edu.cn) Nanjing University of Science and 
Technology, China, Chelin Su, Xiaojun Chen
Utility-based choice models are often used to determine a consumerʼs purchase 
decision among a list of available products; to provide an estimate of product 
demands; and, when data on purchase decisions or market shares are available, to 
infer consumersʼ preferences over observed product characteristics. They also 
serve as a building block in modeling firmsʼ pricing and assortment optimization 
problems. We consider a firmʼs multiproduct pricing problem, in which product 
demands are determined by a pure characteristics model. A sample average 
approximation (SAA) method is used to approximate the expected market share 
of products and the firm profit. We propose a SAA-regularized method for the 
2016_iccopt.indb   98 2016/07/22   11:58:22
ABSTRACTS
ICCOPT 2016 99
multiproduct price optimization problem. We present convergence analysis and 
numerical examples to show the efficiency and the effectiveness of the proposed 
method.

■ Thu.A.5J
Thursday, 9:00-10:15, Room 5J
Advances in Conic Optimization
Cluster: Conic and Polynomial Optimization
Session chair: Antonios Varvitsiotis
1. A Two-Phase Algorithm for Large-Scale QPLogdet Optimization 
Problem
Tang Peipei (tangpp@zucc.edu.cn) Zhejiang University City College, 
China, Wang Chengjing, Wang Jiahuan
In this paper, we present a two-phase algorithm to solve a large-scale nonlinear 
semidefinite programming problem whose objective function is a sum of a 
convex quadratic function and a log-determinant term (QPLogdet). In phase I, 
we adopt an inexact symmetric Gauss-Seidel (sGS) technique based alternating 
direction method of multipliers (ADMM)-type method to obtain a moderately 
accurate solution or generate a reasonably good initial point. In Phase II, we 
design an inexact accelerated block coordinate descent (ABCD) based 
augmented Lagrangian method (ALM) to obtain a highly accurate solution, 
where the inexact sGS techinque and the semismooth Newton-CG method are 
employed to solve the inner problem at each iteration. Numerical experiments 
demonstrate that our two-phase algorithm is efficient and robust.
2. Robust Topology Design of Mechanical Systems under Uncertain 
Dynamic Loads via Nonlinear Semidefinite Programming
Anja Kuttich (kuttich@mathematik.tu-darmstadt.de) TU Darmstadt, 
Germany, Stefan Ulbrich
We consider the problem of robust topology optimization of mechanical systems, 
for example truss or beam structures whose dynamic behavior can be described 
by a linear time-invariant system. Of particular interest in our application is the 
consideration of uncertain dynamic loads. We reformulate the robust topology 
optimization of mechanical systems under uncertain dynamic loads as a nonlinear 
semidefinite programming problem by using the H∞-norm of the transfer function 
and the Bounded-Real Lemma. We solve the resulting optimization problem 
using a sequentially semidefinite programming approach. Furthermore we 
extend our model with a static output feedback controller design. This approach 
allows to optimize the topology of the mechanical structure and the design of the 
feedback controller simultaneously. The considerations are complemented by 
numerical results for robust truss topology design.
3. Completely Positive Semidefinite Rank
Antonios Varvitsiotis (avarvits@gmail.com) Nanyang Technological 
University, Singapore, Anupam Prakash, Jamie Sikora, Zhaohui Wei
An n×n matrix X is called completely positive semidefinite (cpsd) if there exist 
d×d Hermitian positive semidefinite matrices  (for some d ≥ 1) such that 
Xij = tr(PiPj), for all 1 ≤ i, j ≤ n. The cpsd-rank of a cpsd matrix is the smallest d 
≥ 1 where such a representation exists. We initiate the study of the cpsd-rank 
which we motivate twofold. First, the cpsd-rank is a non-commutative analogue 
of the cp-rank of a completely positive matrix. Second, the cpsd-rank is 
physically motivated as it can be used to upper and lower bound the size of a 
quantum system needed to generate a quantum behavior. Unlike the cp-rank 
which is at most quadratic in the size, no general upper bound is known on the 
cpsd-rank of a cpsd matrix. In fact, for any n ≥ 1, we construct a cpsd matrix of 
size 2n whose cpsd-rank is . Our construction is based on Gram matrices 
of Lorentz cone vectors which we show are cpsd. The proof relies on a known 
lower bound on the size of matrix representations of extremal quantum 
correlations which we apply to high-rank extreme points of the n-dimensional 
elliptope. We also study cpsd-graphs, i.e., graphs G with the property that every 
doubly nonnegative matrix whose support is given by G is cpsd. We show that a 
graph is cpsd if and only if it has no odd cycle of length at least 5 as a subgraph. 

■ Thu.A.5K
Thursday, 9:00-10:15, Room 5K
Algorithms and Applications for Conic and Related 
Optimization Problems
Cluster: Conic and Polynomial Optimization
Session organized by: Yu Xia
Session chair: Ting Kei Pong
1. Finding Decompositions for Completely Positive Matrices using 
Orthogonal Transformations
Patrick Groetzner (groetzner@uni-trier.de) University of Trier, 
Germany, Mirjam Dür
Completely positive matrices play a crucial role in solving combinatorial or 
quadratic problems. Using this matrix cone helps to keep up the structure of the 
underlying problem and allows to use several additional approaches to solve the 
problem. One key point for this approach is to figure out whether a matrix is 
completely positive or not. For this it is necessary to find a feasible decomposition 
for a given matrix, delivering a certificate for complete positivity. Numerical 
methods to find a decomposition are only available for specially structured 
matrices. In this talk I will introduce a method to derive a factorization in the 
general case using certain orthogonal transformations. The approach provides 
very promising numerical results in nearly every case and also offers the 
possibility to check whether a matrix is in the interior of the completely positive 
cone.
2. A Fast Approximation Method for Nonconvex Quadratic 
Optimizations with Few Constraints
Shinji Yamada (shinji_yamada@mist.i.u-tokyo.ac.jp) The University 
of Tokyo, Japan, Akiko Takeda
In this talk, we propose a new convex relaxation method for Quadratically 
Constrained Quadratic Programming (QCQP). SDP relaxation is one of well-
known approaches for QCQP; especially in the case when there is only one 
constraint (we call it “1-QCQP” for simplicity), the SDP relaxation is known to 
be tight. Our relaxation method for QCQP is weaker than SDP relaxation, 
however, our method can solve 1-QCQP exactly, same as SDP relaxation. The 
numerical experiments show that as far as the number of constraints is much 
smaller than that of variables, our method finds a relaxed solution which has 
almost the same relaxation value with SDP relaxation, and its computational time 
is faster than SDP relaxation.
3. Explicit Estimation of KL Exponent and Linear Convergence of 
1st-Order Methods
Ting Kei Pong (tk.pong@polyu.edu.hk) The Hong Kong Polytechnic 
University, Hong Kong, Guoyin Li
In this talk, we study the Kurdyka-Lojasiewicz (KL) exponent, an important 
quantity for analyzing the convergence rate of first-order methods. Specifically, 
we show that many convex or nonconvex optimization models that arise in 
applications such as sparse recovery have objectives whose KL exponent is 1/2: 
this indicates that various first-order methods are locally linearly convergent 
when applied to these models. Our results cover the sparse logistic regression 
problem and the least squares problem with SCAD or MCP regularization. We 
achieve this by relating the KL inequality with an error bound concept due to Luo 
and Tseng (1992), and developing calculus rules for the KL exponent. This is a 
joint work with Guoyin Li.

■ Thu.A.5L
Thursday, 9:00-10:15, Room 5L
Polynomial Optimization: Theory and Applications I
Cluster: Conic and Polynomial Optimization
Session organized by: Luis F Zuluaga
1. Penalized Semidefinite Programming Relaxation for Polynominal 
Optimization Problems
Ramtin Madani (ramtin.madani@berkeley.edu) University of 
California, Berkelely, USA, Morteza Ashraphijuo, Javad Lavaei
NP-hardness of combinatorial optimization and several other problems is due to 
the complexity of finding the inverse of a set of polynomial equations. In this 
talk, we show that the inverse of an arbitrary polynomial system is equal to the 
argmin of some semidefinite program (SDP) at the neighborhood of any given 
nominal point. We then prove that there is a finite set of SDPs, whose argmins all 
together establish the inverse of the polynomial system globally. Using this 
result, we develop a series of penalized SDPs to find near-global solutions of 
every arbitrary polynomial optimization problem.
2. A Sampling Kaczmarz-Motzkin Algorithm for Linear Feasibility
Jamie Haddock (jhaddock@ucdavis.edu) University of California, 
Davis, USA, Jesus De Loera, Deanna Needell
We combine two algorithmic techniques for determining the feasibility of 
systems of linear inequalities, Ax ≤ b, the relaxation method (RM) of Agmon, 
Motzkin and Schoenberg, and the randomized Kaczmarz method (RK). Each of 
2016_iccopt.indb   99 2016/07/22   11:58:23
ABSTRACTS
ICCOPT 2016100
these are iterative methods which consists of a series of alternating projections. 
In each iteration, these methods select a violated constraint from among the rows 
of A and project towards the hyperplane it defines. Our proposed family of 
methods, which we refer to as the Sampling Kaczmarz-Motzkin (SKM) methods, 
blend the deterministic constraint selection of RM and the randomized constraint 
selection of RK. While we prove that the method will converge linearly in 
expectation and recover the convergence rates of the previously defined methods, 
experiments suggest that the SKM method often vastly outperforms its 
predecessors. Furthermore, we demonstrate that the method can detect feasibility 
and infeasibility of the system.
3. Positive Polynomials on Unbounded Domains
Juan C Vera (juancavera@gmail.com) Tilburg University, Netherlands, 
Javier Pena, Luis F Zuluaga
Certificates of non-negativity such as Putinarʼs Positivstellensatz have been used 
to obtain powerful numerical techniques to solve polynomial optimization (PO) 
problems. These certificates generally assume compactness of the domain. In 
this paper we characterize the existence of a certificate of non-negativity for 
polynomials over a possibly unbounded domain, without the use of the associated 
quadratic module. We also show that this certificate can be used to construct 
convergent SDP hierarchies for PO problems with unbounded feasible sets.

■ Thu.B.1S
Thursday, 10:45-12:00, Room 1S
Optimization in Machine Learning II
Cluster: Nonlinear Optimization
Session organized by: Martin Takac
1. Stop Wasting My Gradients: Practical SVRG
Reza Babanezhad Harikandeh (babanezhad@gmail.com) University 
of British Columbia, Canada, Mohamed Osama Ahmed, Alim Virani, 
Mark Schmidt, Jakub Konecny, Scott Sallinen
I present and analyze several strategies for improving the performance of 
stochastic variance-reduced gradient (SVRG) methods. I first show that the 
convergence rate of these methods can be preserved under a decreasing sequence 
of errors in the control variate, and use this to derive variants of SVRG that use 
growing-batch strategies to reduce the number of gradient calculations required 
in the early iterations. I further (i) show how to exploit support vectors to reduce 
the number of gradient computations in the later iterations, (ii) prove that the 
commonly-used regularized SVRG iteration is justified and improves the 
convergence rate, (iii) consider alternate mini-batch selection strategies, and (iv) 
consider the generalization error of the method.
2. Fast Optimization for Non-Lipschitz Poisson Regression
Niao He (niaohe@illinois.edu) University of Illinois at Urbana-
Champaign, USA, Zaid Harchaoui, Yichen Wang, Le Song
We propose a novel family of first-order optimization algorithms for penalized 
Poisson regression. The Poisson log-likelihood is concave but not Lipschitz-
continuous. Most existing first-order optimization algorithms relying on 
Lipschitz-continuity would fail to work for Poisson regression. We present a new 
perspective allowing to efficiently optimize penalized Poisson regression 
objectives. We show that an appropriate saddle point reformulation enjoys a 
favorable geometry and a smooth structure. Therefore, we can design a composite 
Mirror Prox algorithm with O(1/t) convergence rate, in contrast to the typical 
 rate for non-smooth optimization. To tackle problems with large 
samples, we also develop a randomized block updating scheme with same 
convergence rate yet more efficient iteration cost. Experimental results on 
several applications, including positron emission tomography, social network 
estimation, and temporal recommendation systems, show that the proposed 
algorithm and its randomized block variant outperform existing methods both on 
synthetic and real-world datasets.
3. Primal-Dual Rates and Certificates
Martin Takac (martin.taki@gmail.com) Lehigh University, USA, 
Celestine Dunner, Simone Forte, Martin Jaggi
We propose an algorithm-independent framework to equip existing optimization 
methods with primal-dual certificates. Such certificates and corresponding rate 
of convergence guarantees are important for practitioners to diagnose progress, 
in particular in machine learning applications. We obtain new primal-dual 
convergence rates e.g. for the Lasso as well as many L1, Elastic-Net and group-
lasso-regularized problems. The theory applies to any norm-regularized 
generalized linear model. Our approach provides efficiently computable duality 
gaps which are globally defined, without modifying the original problems in the 
region of interest.

■ Thu.B.1A
Thursday, 10:45-12:00, Room 1A
Numerical Linear Algebra and Optimization I
Cluster: Nonlinear Optimization
Session organized by: Annick Sartenaer, Dominique Orban
1. Preconditioning KKT Systems in Interior Point Methods
Jacek Gondzio (J.Gondzio@ed.ac.uk) Edinburgh University, United 
Kingdom, Lukas Schork
Preconditioning of reduced KKT systems arising in interior point algorithms for 
linear and quadratic programming will be discussed. A new class of methods 
which exploit advantages offered by the (indefinite) augmented system form 
will be presented. The preconditioners rely on the null space representation of 
linear constraints. Some known techniques used in implementations of the 
revised simplex method are combined with new ideas for finding stable null 
space representations.
2. The DQQ Procedure for Multiscale Optimization
Michael Saunders (saunders@stanford.edu) Stanford University, 
USA, Ding Ma
Systems biology models of metabolic networks can lead to challenging 
optimization problems for which standard solvers are not sufficiently accurate, 
while the rational arithmetic of exact simplex solvers is extremely slow. Quad-
precision floating-point (even in software) offers a practical compromise. We 
combine Double and Quad versions of MINOS, with and without scaling, to 
overcome the difficulty of unscaling multiscale problems. This has enabled 
solution of the Meszaros “problematic” LPs and increasingly large linear and 
nonlinear ME models in systems biology.
3. A Tridiagonalization Method for Saddle-Point and Quasi-definite 
Systems
Dominique Orban (dominique.orban@gerad.ca) GERAD and Ecole 
Polytechnique, Canada, Alfredo Buttari, David Titley-Peloquin, Daniel 
Ruiz
The tridiagonalization process of Simon, Saunders and Yip (1988) applied to a 
rectangular operator A gives rise to USYMQR, which solves a least-squares 
problem with A, and USYMLQ, which solves a least-norm problem with Aʼ. 
Symmetric saddle-point systems may be viewed as a pair of least-squares/least-
norm problems. This allows us to merge USYMQR and USYMLQ into a single 
method that solves both problems in one pass. We present preconditioned and 
regularized variants that apply to symmetric and quasi-definite linear systems 
and illustrate the performance of our implementation on systems coming from 
optimization.

■ Thu.B.1B
Thursday, 10:45-12:00, Room 1B
Risk-averse Optimization with PDE Constraints I
Cluster: PDE-constrained Optimization
Session organized by: Denis Ridzal, Drew Philip Kouri, Bart van Bloemen 
Waanders
1. Risk Averse PDE-constrained Optimization using Coherent 
Measures of Risk
Thomas M Surowiec (surowiec@math.hu-berlin.de) Humboldt 
University of Berlin, Germany, Drew P Kouri
The incorporation of uncertainty into models in engineering and the natural 
sciences typically leads to partial differential equations (PDEs) with uncertain 
parameters. As a result, the state and objective functions are implicitly random 
variables. In order to control or optimize these infinite-dimensional systems in a 
risk-averse way, we employ so-called “coherent risk measures.” The potential 
non-smoothness of these risk measure poses additional challenges theoretically 
and numerically. We discuss meaningful conditions on the random variable 
objective functional and derive first-order optimality conditions. In order to 
solve these problems numerically, we make use of an epi-regularization 
technique and a continuation strategy. Restricting to the Conditional-Value-at-
Risk, we illustrate the results by several numerical examples.
2. Trust-Region Algorithms for Large-Scale Stochastic Optimization 
with PDE Constraints
2016_iccopt.indb   100 2016/07/22   11:58:23
ABSTRACTS
ICCOPT 2016 101
Denis Ridzal (dridzal@sandia.gov) Sandia National Laboratories, 
USA, Drew Kouri
Trust regions provide a robust framework for model management in the 
optimization process. In the context of reduced-space formulations for PDE-
constrained optimization, trust-region methods are used to adaptively control the 
accuracy of numerical discretizations. We present conditions for inexact 
objective function and gradient evaluations that are well suited for exploiting 
adaptive sparse-grid discretizations of stochastic optimization problems. Our 
algorithms rapidly identify the stochastic variables that are relevant to obtaining 
an accurate optimal solution, and, in some cases, exhibit computational costs that 
are nearly independent of stochastic dimension. In the context of full-space 
formulations, trust-region methods enable efficient stopping conditions for 
iterative linear system solves. We present multiply distributed scalable solvers 
for optimality systems arising in optimization problems governed by PDEs with 
random inputs. We conclude the presentation with numerical examples including 
risk-neutral acoustic control, risk-averse topology optimization and risk-averse 
control of thermal fluids.
3. The Rapid Optimization Library: A PDE-constrained 
Optimization under Uncertainty Framework
Bart van Bloemen Waanders (bartv@sandia.gov) Sandia National 
Laboratories, USA, Denis Ridzal, Drew Kouri
The Trilinos Rapid Optimization Library (ROL) provides an object-oriented 
framework for large-scale, derivative-based optimization. The library is matrix-
free and linear-algebra agnostic permitting easy interface with application code. 
ROL implements a suite of unconstrained and constrained optimization 
algorithms including: gradient descent, quasi-Newton, and inexact-Newton with 
line-search and trust-region globalization. A stochastic optimization subpackage 
(SOL) supplies default implementations of numerous risk measures and adaptive 
sampling capabilities. Several examples demonstrate the solution of large 
optimization problems with model uncertainties.

■ Thu.B.1C
Thursday, 10:45-12:00, Room 1C
Applications of Derivative-free and Simulation-based 
Optimization
Cluster: Derivative-free and Simulation-based Optimization
Session organized by: Francesco Rinaldi, Zaikun Zhang
Session chair: Matteo Diez
1. Derivative Free Optimization for Automated, Efficient Tuning of 
Predictive Models 
Patrick Koch (patrick.koch@sas.com) SAS Institute, Inc., USA, Josh 
Griffin, Steve Gardner, Oleg Golovidov, Scott Pope
With the continual projected exponential growth rate of digital data the challenge 
of managing, understanding, and capitalizing on this data also continues to grow. 
Facilitating effective decision making requires the transformation of relevant 
data to high quality descriptive and predictive models. Machine learning 
modeling algorithms are commonly used to find hidden value in big data. These 
algorithms are governed by ʻhyper-parametersʼ with no clear defaults agreeable 
to a wide range of applications. Ideal settings for these hyper-parameters 
significantly influences the resulting accuracy of the predictive models. In this 
talk we discuss the use of derivative free optimization for hyper-parameter 
tuning. As a complex black-box to the tuning process, machine learning 
algorithms are well suited to derivative free optimization for tuning. We employ 
a Local Search Optimization (LSO) procedure, which performs parallel hybrid 
derivative-free optimization for problems with functions that are nonsmooth, 
discontinuous, or computationally expensive to evaluate directly. LSO permits 
both continuous and integer decision variables, and can operate in single-
machine mode or distributed mode. We will present tuning results for multiple 
examples, compared to default model training, and discuss and demonstrate the 
use of distributed processing to reduce the tuning expense. 
2. A Hybrid Global/Local Multi-Objective Approach to Simulation-
based Design Optimization: Deterministic Particle Swarm with 
Derivative-free Local Searches
Matteo Diez (matteo.diez@cnr.it) CNR-INSEAN, Natl. Research 
Council-Marine Technology Research Inst., Italy, Riccardo Pellegrini, 
Andrea Serani, Giampaolo Liuzzi, Stefano Lucidi, Francesco Rinaldi, 
Umberto Iemma, Emilio Fortunato Campana
Simulation Based Design Optimization (SBDO) supports the designer in the 
design of complex engineering systems. The process integrates numerical 
simulations with optimization algorithms, to the aim of exploring design 
opportunities and addressing multiple design objectives. These may be noisy and 
often the simulation tools do not directly provide their derivatives. Therefore, 
derivative-free algorithms are used as a viable option in the SBDO process. 
Local or global algorithms are used, whether the search region is known a priori, 
or not. Local algorithms explore accurately a limited region, whereas global 
algorithms explore the entire space, providing approximate solutions. In order to 
combine the accuracy of local algorithms with the exploration capability of 
global methods for multi-objective problems, the multi-objective deterministic 
particle swarm optimization (MODPSO) is combined with a derivative-free 
multi-objective (DFMO) local algorithm. Two implementations of MODPSO 
and their hybridizations with DFMO are presented. The resulting hybrid global/
local implementations are tested using 30 test problems. Their performance is 
assessed considering six metrics, providing the proximity of the solutions to a 
reference Pareto front and the continuity of the approximated Pareto front.

■ Thu.B.4A
Thursday, 10:45-12:00, Room 4A
Applications of Optimal Control
Cluster: Applications in Energy, Science and Engineering
Session organized by: Maria Rosario de Pinho
1. Multiattribute Pricing
Thomas A Weber (thomas.weber@epfl.ch) École Polytechnique 
Fédérale de Lausanne (EPFL), Switzerland
We provide a technique for constructing second-best multiattribute screening 
contracts in a general setting with one-dimensional types based on necessary 
optimality conditions. Our approach allows for type-dependent participation 
constraints and arbitrary risk profiles. As an example we discuss optimal 
insurance contracts.
2. Optimally Control Treatment of Psoriasis Skin Disease
Ellina V Grigorieva (egrigorieva@twu.edu) Texas Womanʼs 
University, USA, Evgenii N Khailov
Psoriasis is a chronic inflammation of the skin that changes the life cycle of skin 
cells. Psoriasis causes these cells to build up quickly on the skin surface, which 
makes the skin appear red, dry with scaly patch. Adequate treatment for psoriasis 
is very challenging, and a total cure of the disease still does not exist. 
Mathematical models have long been effective means to predict cellular 
behaviors of the skin regulation for a normal or pathological circumstance. In 
this paper, a proposed control model of psoriasis skin disease is described by a 
nonlinear control system of three differential equations involving the 
concentration of Dendritic Cells (Tissues Macrophages), T-Lymphocytes and 
Keratinocytes with medication intake as control. An optimal control problem of 
minimizing the release of Keratinocytes to standardize the growth of the 
Dendritic Cells is stated and solved using Pontryagin Maximum Principle. The 
type of the optimal control is obtained analytically. Numerical simulations of the 
optimal solutions at different modelʼs parameters are presented. Possible 
applications to an optimal drug therapy are discussed. 
3. Optimal Control for Path Planning of AUV using Simplified 
Models
Maria do Rosario de Pinho (mrpinho@fe.up.pt) Universidade do 
Porto, Portugal, Anibal Matos
In this talk we illustrate different numerical optimal control techniques when 
applied to a simplified model for path planning of an autonomous underwater 
vehicle (AUV). We consider the movement of an AUV in an horizontal plane. 
The aim is to reach a position with a determined configuration in minimum time. 
Ocean current are considered and bounds on the maximum velocity of the 
vehicle as well as bounds on the controls, the heading angle and the thruster force 
are considered. This turns out to be an optimal control problem. Numerical 
solutions using the direct method implemented via AMPL calling IPOTS are 
considered. We also present some preliminaries results on the numerical 
reconstruction of trajectories using Hamiltion Jacobi Belmmand equation.

■ Thu.B.4B
Thursday, 10:45-12:00, Room 4B
Informatics and Geometric Problems
Cluster: Applications in Energy, Science and Engineering
Session chair: Dirk Oliver Theis
1. Sequential Equality Programming for Topology Optimization
Luis Felipe Bueno (lfelipebueno@gmail.com) Federal University of 
Sao Paulo, Brazil, Thadeu Senne
2016_iccopt.indb   101 2016/07/22   11:58:23
ABSTRACTS
ICCOPT 2016102
In this work we analyse the applicability of the Sequential Equality Programming 
method to solve Topology Optimization problems. In this context, we examine 
how to use an Augmented Lagrangian strategy, penalizing simple constraints, to 
solve subproblems of some methods traditionally used in Topology Optimization. 
In particular, we study how to use Newtonian techniques in the penalized 
subproblems of the Sequential Linear Programming, the Sequential Piecewise 
Linear Programming, and the Moving Asymptotes methods.
2. Location Problem of Supply Facilities in Gas Distribution 
Networks
Naoshi Shiono (gian.naoshi@gmail.com) Tokyo Gas Energy 
Corporation, Japan, Yasufumi Saruwatari
This study addresses the location problem of supply facilities in gas distribution 
networks. In Japan, gas distribution networks consist of high, medium and low 
pressure pipes and pressure regulators are installed in pipes to reduce the lower 
pressure. When we design the distribution networks for a large number of 
customers that use low pressure gas, we have to decide the pipe diameters in 
medium and low pressure distribution networks and the location of regulators to 
minimize the construction cost. In this study, we present a mathematical model 
to decide the number of regulators, i.e. supply facilities for low pressure 
networks. We assume a rectangular grid road network on the plane and each 
customer with an identical demand is placed on an intersection of the network. 
After we formulate the optimization problem where supply facilities are located 
at regular intervals, we solve the problem approximately. As the result, we derive 
the relationship between the number of facilities and demand, length of grid and 
each construction cost. In addition, we incorporate uncertainties, such as demand, 
in the model and obtain some properties.
3. Computing Unique Information
Dirk Oliver Theis (dotheisatutdotee@gmail.com) University of Tartu, 
Estonia
Let X,Y,Z be random variables. Bertschinger, Rauh, Olbrich, Jost, and Ay 
proposed a definition “unique information”: the amount of information about X 
contained exclusively in either Y or Z. They present convex programs by which 
unique information can be computed, and point out some subtle problems with 
the approach. In our talk, we present code and give computational results for 
estimating the unique information of X,Y,Z from a series of independent samples. 
We also discuss stability results.

■ Thu.B.5C
Thursday, 10:45-12:00, Room 5C
Variational Analysis, Optimization, and Applications
Cluster: Multi-Objective and Vector Optimization
Session organized by: Luis Manuel Briceño-Arias
1. New Advances in Sensitivity Analysis of Solution Maps to 
Parameterized Equilibria with Conic Constraints
Hector Ramirez (hramirez@dim.uchile.cl) Universidad de Chile, 
Chile
In this talk we present new calculations of the graphical derivative, limiting 
coderivative and others generalized derivatives for the solution map to 
parameterized generalized equations/KKT systems associated with conic 
constraints. These computations are first derived provided the feasible set 
appearing in the KKT system is convex. They provide verifiable conditions for 
sensitivity properties (such as, for instance, isolated calmness) of the 
corresponding solution map. We are able to extend the computation of the 
graphical derivative to the nonconvex case. The latter requires, however, an 
additional condition of geometric nature imposed on the considered cone. This is 
related to the sigma-term associated with projection onto this cone and has a 
local character. Under this condition our formula for the graphical derivative has 
the same form as the formula resulting in VI over polyehdral sets, and so, it can 
be viewed as its generalization to a class of nonpolyhedral cones. 
2. On the Linear Convergence of Forward-Backward Splitting 
Methods
Nghia TA Tran (nttran@oakland.edu) Oakland University, USA
 In this talk we mainly analyze the local linear convergence of the forward-
backward splitting methods for solving nonsmooth convex problems by using 
tools of generalized differentiations and variational analysis. Local and global 
linear convergences of this method in some special cases such as `1, nuclear 
norm, TV seminorm-regularized optimization problems are also discussed. 
3. Projected Chambolle-Pock Splitting for Solving Monotone 
Inclusions
Luis Manuel Briceño-Arias (luis.briceno@usm.cl) Universidad 
Técnica Federico Santa María, Chile
In this talk a modification of Chambolle-Pock splitting for solving primal-dual 
optimization problems is presented. In the case when the primal solution is 
known to be in some convex closed set, the proposed primal-dual method 
performs an additional projection step which guarantees that the primal sequence 
belongs in that set. This feature is desired, for instance, in optimization problems 
with linear constraints, in which the solution must be feasible and, hence, an 
algorithm whose primal iterates satisfy the constraints is useful. In this case, the 
flexibility of the method allows to consider different sets to perform the 
projection allowing, for example, to choose some of the linear constraints in 
order to obtain a projection easy to compute. In some cases this can help to 
improve the performance of the algorithm with respect to the original method. 
Finally, a generalization to composite primal-dual monotone inclusions is 
provided.

■ Thu.B.5D
Thursday, 10:45-12:00, Room 5D
Discrete and Computational Geometry
Cluster: Linear Optimization
Session organized by: Yoshio Okamoto
1. Redundancy Detection for Linear Programs with Two Variables 
per Inequality
May Krisztina Szedlák (may.szedlak@inf.ethz.ch) ETH Zurich, 
Switzerland, Komei Fukuda
The problem of detecting and removing redundant constraints is fundamental in 
optimization. We focus on the case of linear programs (LPs) in normal form, 
given by d variables with n inequality constraints. A constraint is called 
redundant, if after removing it, the LP still has the same feasible region. The 
currently fastest method to detect all redundancies is the one by Clarkson: it 
solves n linear programs, but each of them has at most s constraints, where s is 
the number of nonredundant constraints. In this talk, we study the special case 
where every constraint has at most two variables with nonzero coefficients. This 
family, denoted by LI(2), has some nice properties. Namely, given a variable x 
and a value a, we can test in time O(nd) whether there is a feasible solution with 
xi = a. Hochbaum and Naor present an O(nd 2logn) algorithm for solving 
feasibility (and finding a solution) in LI(2). Their technique makes use of the 
Fourier-Motzkin elimination method and the aforementioned result by Aspvall 
and Shiloach. We present a strongly polynomial algorithm that solves redundancy 
detection in time O(nd 2slogs). It uses a modification of Clarksonʼs algorithm, 
together with a revised version of Hochbaum and Naorʼs technique.
2. On Classes of Oriented Matroids That Admit 2-dimensional 
Topological (Geometric) Representations
Hiroyuki Miyata (hmiyata@cs.gunma-u.ac.jp) Gunma University, 
Japan
Oriented matroids are a common combinatorial abstraction of various geometric 
objects such as hyperplane arrangements, polytopes and point configurations. 
One of the outstanding results in oriented matroid theory is the Topological 
Representation Theorem, which asserts that every oriented matroid of rank r can 
be represented as an arrangement of pseudospheres in the (r⊖1)-dimensional 
sphere. In this talk, we discuss a possibility that oriented matroids can be 
represented as topological (geometric) objects in the 2-dimensional Euclidean 
space. We show that matroid polytopes of rank 4 and degree-k oriented matroids(, 
which were introduced by the speaker as an abstraction of configurations of 
points and degree-k polynomial functions) of rank k+1 have 2-dimensional 
topological (geometric) representations.
3. Geometric Optimization Related with an LCP with SPD-Matrices
Sonoko Moriyama (moriso@chs.nihon-u.ac.jp) Nihon University, 
Japan, Bernd Gaertner, Hiroshi Imai, Hiroyuki Miyazawa, Jiro 
Nishitoba
The linear complementarity problem (LCP) was introduced as a generalization 
of linear programs, quadratic programs and bimatrix games. Given a matrix 
 and a vector , the problem is to find two non-negative vectors 
 satisfying w⊖Mz = q and wTz = 0. When M is symmetric positive 
semidefinite, the associated LCP is solved in polynomial time. In this talk, we 
focus on the LCP with symmetric positive definite (for short SPD) matrices, and 
provide some geometric interpretations. Firstly, we show that any LCP with 
SPD-matrices corresponds to a polyhedron distance problem, which is to find a 
point of a given polytope with the smallest Euclidean distance to the origin. 
Particularly if an SPD-matrix M and q satisfy some conditions, we show that the 
associated LCP is interpreted as a smallest enclosing ball (for short SEB) 
2016_iccopt.indb   102 2016/07/22   11:58:23
ABSTRACTS
ICCOPT 2016 103
problem, which is to compute the smallest enclosing ball of a given set of points. 
Note that a symmetric matrix is positive definite if and only if it is a P-matrix. In 
the process, we utilize a framework of unique sink orientations representing the 
LCP with P-matrices and the SEB problem.

■ Thu.B.5E
Thursday, 10:45-12:00, Room 5E
Advances in Robust Optimization V
Cluster: Robust Optimization
Session organized by: Ihsan Yanıkoğlu
1. Robust Optimal Control using Generalized Higher Order Moment 
Expansions
Boris Houska (borish@shanghaitech.edu.cn) ShanghaiTech 
University, China
This talk is about an algorithm for solving worst-case robust optimal control 
problems with potentially nonlinear dynamics. The uncertainty is assumed to be 
a time-varying function which is known to be bounded point-wise in time by a 
given compact set. The aim is to optimize the control input in such a way that 
constraints on the output of the dynamic system are satisfied for all possible 
uncertainty scenarios. In this paper, we propose a robust optimal control 
algorithm, which is based on a non-trivial combination of existing uncertainty set 
propagation and chaos expansion based methods. Here, we introduce a new 
concept which considers higher order Taylor expansion with respect to the most 
important moments of the uncertainty functions. The remaining infinite 
dimensional terms are bounded in a rigorous way using ellipsoidal set-bounding 
methods. We illustrate the applicability and precision of our approach with a 
numerical example.
2. Robust Optimization with Ambiguous Stochastic Constraints 
under Mean and Dispersion Information
Krzysztof Postek (k.postek@tilburguniversity.edu) Tilburg University, 
Netherlands, Aharon Ben-Tal, Dick den Hertog, Bertrand Melenberg
We consider ambiguous stochastic constraints under partial information 
consisting of means and dispersion measures of the underlying random 
parameters. Whereas the past literature used the variance as the dispersion 
measure, here we use the mean absolute deviation from the mean (MAD) which 
allows us to use old bounds on the expectations of convex functions. First, we 
use these results to treat ambiguous expected feasibility constraints. This 
approach requires, however, the independence of the random variables and, 
moreover, may lead to an exponential number of terms in the resulting robust 
counterparts. We then show how upper bounds can be constructed that alleviate 
the independence restriction and require only a linear number of terms, by 
exploiting models in which random variables are linearly aggregated. In a 
numerical study, we demonstrate the efficiency of our method in solving 
stochastic optimization problems under mean-MAD ambiguity
3. Decision Rule Bounds for Stochastic Bilevel Programs
Ihsan Yanıkoğlu (ihsan.yanikoglu@ozyegin.edu.tr) Özyeğin 
University, Turkey, Daniel Kuhn
We study stochastic bilevel programs where the leader chooses a binary here-
and-now decision and the follower responds with a continuous wait-and-see-
decision. Using modern decision rule approximations, we construct lower 
bounds on an optimistic version and upper bounds on a pessimistic version of the 
leaderʼs problem. Both bounding problems are equivalent to explicit mixed-
integer linear programs that are amenable to efficient numerical solution. The 
method is illustrated through a facility location problem involving sellers and the 
customers with conflicting preferences.

■ Thu.B.5F
Thursday, 10:45-12:00, Room 5F
Sparsity and Semidefinite Programming Connections
Cluster: Sparse Optimization and Information Processing
Session organized by: Coralia Cartis
Session chair: Martin Lotz
1. Large-Scale Graphical Lasso Problems
Somayeh Sojoudi (sojoudi@berkeley.edu) University of California, 
Berkeley, USA
Sparse inverse covariance estimation from a small number of samples is an 
important problem with a wide variety of applications. Graphical lasso is a 
popular technique for addressing this problem. This technique relies on solving 
a computationally-expensive semidefinite program (SDP). We derive sufficient 
conditions under which the solution of this large-scale SDP has a simple formula. 
We test these conditions on electrical circuits and functional MRI data. This talk 
develops new insights into regularized SDP problems. 
2. Bounds on the Rank of Solutions to Sparse Semidefinite Programs
Raphael Louca (rl553@cornell.edu) Cornell University, USA, 
Subhonmesh Bose, Eilyan Yamen Bitar
We consider semidefinite programs in which the problem data have a collective 
sparsity pattern, which can be described by a graph. For each maximal clique of 
a chordal extension of the graph, we define an affine subspace, which depends on 
both the cardinality of its intersection with neighboring maximal cliques and on 
the problem data characterizing the problem. A new upper bound on the minimum 
rank of feasible solutions to the sparse semidefinite program is derived. This 
bound is a function of the codimension of said affine subspaces. For certain 
problem families, this bound is shown to improve upon related bounds in the 
literature. In addition to the upper bound, we derive, for the special case of 
chordal graphs, a generic lower bound on the rank of optimal solutions to the 
semidefinite program. The lower bound is derived through a characterization of 
the tangent cone to a suitably defined chordal matrix cone at an arbitrary point 
and relies on the notion of constraint nondegeneracy of conic linear programs.

■ Thu.B.5G
Thursday, 10:45-12:00, Room 5G
Routing and Related Problems
Cluster: Applications in Energy, Science and Engineering
Session chair: Kazuhiro Kobayashi
1. A Meta-heuristic for the Location Routing Problem with Time-
dependent Travel Times
Achmad Maulidin (achmad.maulidin@hotmail.com) National Taiwan 
University of Science and Technology, Taiwan, Vincent F Yu, TMA Ari 
Samadhi, Hadi Susanto
We study the location routing problem with time-dependent travel times 
(TDLRP), which is a new variant of the location routing problem (LRP). In 
TDLRP, the travel time between each pair of nodes in the network may change 
depending on the time at which the travel occurs. The objective is to determine 
optimal depot locations and vehicle routes to minimize total cost consisting of 
fixed depot opening cost, fixed vehicle activation cost, and variable vehicle 
travel cost. A mixed integer linear programming model is formulated for the 
problem. Small TDLRP instances can be solved to optimality by commercial 
solver CPLEX. A new metaheuristic is developed to solve large TDLRP 
instances.
2. A Capacitated Vehicle Routing Problem Approach for Solving 
Clustering Problem: A Case Study from Chiang Mai, Thailand
Chulin Likasiri (julin.likasiri@gmail.com) Chiang Mai University, 
Thailand
In northern Thailand, the sizes and topography structures of farmlands make it 
necessary that operators of small-scale waste management systems be able to 
reach their clients in an effective manner. This work focuses on finding clusters 
for these waste disposal centers. Since these facilities are small with limited 
capacities and their clients are scattered, sometimes without decent traffic 
between two points, Euclidean distances are used to estimate distances between 
clients. Capacitated vehicle routing problem (CVRP) is modified to solve the 
clustering problem by creating a dummy node with zero distance to all the other 
nodes. The number of required clusters is equivalent to the number of trucks 
available in the CVRP, where the capacities of the trucks become the capacities 
of the disposal facilities. The model is then solved using the modifying bender 
decomposition method along with branch and bound techniques. The results 
indicate that the number of clients essentially affects the performance of the 
procedure. The case study is maize production management in Chiang Mai, the 
regionʼs economic capital, with 18 entrepreneurs and 73 groups of fields.
3. MISOCP Formulation for the Optimal Fuel Routing Problem and 
the Route Generation Algorithm
Kazuhiro Kobayashi (kazuhir2@gmail.com) Tokyo University of 
Science, Japan, Mirai Tanaka
We consider a problem to find a shipping route and shipping speed which 
minimize the total fuel consumption between two ports. This problem can be 
formulated as an MINLP (mixed-integer nonlinear optimization problem). The 
MINLP is represented as an MISOCP (mixed-integer second-order optimization 
2016_iccopt.indb   103 2016/07/22   11:58:23
ABSTRACTS
ICCOPT 2016104
problem) in particular case. The authors propose a practical algorithm for solving 
this problem named the route generation algorithm. A basic idea of the proposed 
algorithm is to implicitly enumerate feasible shipping routes. The algorithm may 
return an optimal solution without enumerating all feasible shipping routes by 
computing lower bounds for the optimal value. Numerical results that verify the 
effectiveness of our algorithm are also reported.

■ Thu.B.5H
Thursday, 10:45-12:00, Room 5H
Applications of Stochastic Programming in Finance 
and Economics
Cluster: Stochastic Optimization
Session organized by: Hailin Sun, Dali Zhang
1. Sparse Portfolio Selection via Linear Complementarity Approach
Qiyu Wang (qiyu.wang@connect.polyu.hk) Hong Kong Polytechnic 
University, China, Hailin Sun
In the framework of the classical Markowitz mean-variance model when 
multiple solutions exist, among which the sparse solutions are stable and cost-
efficient, we propose a two-phase stochastic linear complementarity approach. 
This approach stabilizes the optimization problem, finds the sparse asset 
allocation that saves the transaction cost, and results in the solution set of the 
Markowitz problem. Our approach could be applied to short-selling-not-allowed 
portfolios and short-selling-allowed portfolios. We apply the sample average 
approximation (SAA) method to the two-phase optimization approach and give 
detailed convergence analysis. The approach is implemented on the empirical 
data-sets of Fama and French 100 (FF100), S&P 500 and the newly-launched 
Shanghai-Hong Kong Stock Connect scheme. With mock investment in training 
data, we construct portfolios, test them in the out-of-sample data and find their 
Sharpe ratios outperform the 1/N strategy, 1-norm regularized portfolios and 
p-norm regularized portfolios. Moreover, we show the advantage in risk 
management of our approach by using Value-at-Risk (VaR) and Conditional 
Value-at-Risk (CVaR).
2. Convex Risk Measures: Efficient Computations via Monte Carlo
Zhaolin Hu (huzhaolin@gmail.com) Tongji University, China, Dali 
Zhang
With the development of financial risk management, the notion of convex risk 
measures has been proposed and has gained more and more attentions. Utility-
based shortfall risk (SR), as a specific and important class of convex risk 
measures, has become popular in recent years. In this paper we focus on 
computational aspects of SR, which are significantly understudied but 
fundamental for risk assessment and management. We discuss efficient 
estimation of SR, sensitivity analysis for SR, as well as optimization of SR, based 
on Monte Carlo techniques and stochastic optimization methods. We also 
conduct extensive numerical study on the proposed approaches. The numerical 
results further demonstrate the effectiveness of these approaches.
3. Dynamic Pricing and Return Pricing for Airline Industry
Bintong Chen (bchen@udel.edu) University of Delaware, USA, Ye 
Tian, Jing Chen
Dynamic pricing has been developed to increase profits of the airline industry for 
decades. This strategy adjusts the ticket prices based on the option value of future 
sales, which varies with time and units available. The return policy, which is also 
an important strategy in the airline industry, however, has not been considered 
yet in previous models. In this paper, we extend the classical continuous time 
dynamic pricing model by taking into consideration several return policies. 
These new models simultaneously decide the optimal dynamic sale prices and 
return prices to maximize the total profit.

■ Thu.B.5J
Thursday, 10:45-12:00, Room 5J
Advances in Nonlinear Optimization II
Cluster: Nonlinear Optimization
Session chair: Fernando ACC Fontes
1. A Hybrid Algorithm for Split Hierarchical Optimization Problems 
with Fixed Point Constraints in Hilbert Spaces
Nimit Nimana (nimitn@hotmail.com) Naresuan University, Thailand, 
Narin Petrot
We emphasize a split-type problem of some integrating ideas of the split 
feasibility problem and the hierarchical optimization problem with fixed point 
constraints. Working on real Hilbert settings, we propose an iterative method for 
approximation a solution of the problem, and we discuss its convergence results. 
We also show an implication of the proposed problem to a problem of multi-
agent networked system, that consists of a centralized mediator and a finite 
number of independent agents in each individual domains. Finally, a 
computational example is discussed.
2. Modulus Methods for Box Constrained Least Squares Problems
Ning Zheng (nzheng@nii.ac.jp) The Graduate University for 
Advanced Studies, Japan, Ken Hayami, Jun-Feng Yin
For the solution of large sparse box constrained least squares problems (BLS), a 
new class of iterative methods is proposed by utilizing modulus transformation, 
which converts the solution of the BLS into a sequence of unconstrained least 
squares problems. Efficient Krylov subspace methods with suitable 
preconditioners are applied to solve the inner unconstrained least squares 
problems for each outer iteration. In addition, the method can be further enhanced 
by incorporating the active set strategy, which contains two stages where the first 
stage consists of modulus iterations to identify the active set, while the second 
stage solves the reduced unconstrained least squares problems only on the 
inactive variables, and projects the solution into the feasible region. We also 
analyze the convergence of the method including the choice of the parameter. 
Numerical experiments show the efficiency of the proposed methods in 
comparison to the gradient projection methods, the Newton like methods and the 
interior point methods.
3. Optimal Control of Constrained Nonlinear Systems: An Adaptive 
Time-Grid Refinement Algorithm Guided by the Adjoint 
Multipliers
Fernando ACC Fontes (faf@fe.up.pt) Universidade do Porto, Portugal, 
Luis T Paiva
This work addresses numerical methods for optimal control problems of 
nonlinear systems with pathwise state-constraints. These are challenging 
optimization problems for which the number of time-discretization points is a 
major factor determining the computational time. Also, the location of these 
points has a major impact in the accuracy of the solutions. We propose an 
algorithm that, guided by information of the adjoint multipliers, iteratively 
selects an adequate time-grid to satisfy some predefined error estimate on the 
obtained trajectories. The results show a favorable comparison against the 
traditional equidistant-spaced time-grid methods, including the ones using 
discrete-time models. This way, continuous-time plant models can be used 
directly: the discretization procedure can be automated and there is no need to 
select a priori an adequate time step. Even if the optimization procedure is forced 
to stop in an early stage, as might be the case in real-time problems, we can still 
obtain a meaningful solution, although a less accurate one. Extension of the 
procedure to a Model Predictive Control (MPC) context is also discussed. By 
defining a time-dependent accuracy threshold, we can generate solutions that are 
more accurate in the initial parts of the receding horizon, which are the most 
relevant for MPC.

■ Thu.B.5L
Thursday, 10:45-12:00, Room 5L
Polynomial Optimization: Theory and Applications II
Cluster: Conic and Polynomial Optimization
Session organized by: Luis F Zuluaga
1. A New Approximation Hierarchy for Polynomial Conic 
Optimization
Janez Povh (janez.povh@fis.unm.si) Faculty of information studies in 
Novo mesto, Slovenia, Peter JC Dickinson
In this talk we consider polynomial conic optimization problems, where the 
feasible set is defined by constraints in the form of given polynomial vectors 
belonging to given nonempty closed convex cones, and we assume that all the 
feasible solutions are nonnegative. After translation, this family of problems 
captures in particular compact polynomial optimization problems, compact 
polynomial semidefinite problems and compact polynomial second order cone 
optimization problems. We propose a general hierarchy of conic linear 
programming relaxations which is under some classical assumptions monotonic 
and converges to the optimal value of the original problem. The members of this 
hierarchy provide strong bounds for the optimum value and are in special cases 
from previous paragraph much easier to compute compared to classical SOS and 
moment approximations.
2. New Bounds for Scheduling on Two Unrelated Selfish Machines
2016_iccopt.indb   104 2016/07/22   11:58:23
ABSTRACTS
ICCOPT 2016 105
Olga Kuryatnikova (o.kuryatnikova@tilburguniversity.edu) Tilburg 
University, Netherlands, Juan Carlos Vera
Consider the minimum makespan problem with n tasks on two unrelated selfish 
machines. Given n, we look for the best approximation ratio R(n) of randomized 
monotone scale free algorithms as they provide for truthful scheduling. We 
optimize over distributions of random bits used in the algorithms and formulate 
two sequences of optimization programs converging to R(n) from below and 
from above respectively. These bounds are obtained using copulas. The upper 
bound computation is constructive: we build a piecewise rational distribution 
such that performance of the corresponding algorithm matches the bound. Our 
method improves upon the existing bounds on R(n) for small n. In particular, it 
shows that |R(2)⊖1.505996| < 10⊖6, which provides the best upper bound for any 
truthful mechanism in the case of two tasks.
3. Moment/Sum-of-Squares Hierarchy for Complex Polynomial 
Optimization
Cedric Josz (cedric.josz@gmail.com) INRIA Paris, France
Polynomial optimization where the variables and data are complex numbers is an 
NP-hard problem that arises in various applications such power systems, signal 
processing, imaging science and control. Complex numbers are typically used to 
model oscillatory phenomena which are omnipresent in physical systems. Based 
on recent advances in algebraic geometry, we transpose Lasserreʼs hierarchy to 
complex numbers for enhanced tractability. Weʼll highlight the differences 
between the real and complex hierarchies and present numerical results on 
problems arising from industry with several thousand complex variables.

■ Thu.B.m3S
Thursday, 10:45-12:00, Room m3S
First-Order Methods for Convex Optimization: New 
Complexity/Convergence Theory
Cluster: Conic and Polynomial Optimization
Session organized by: Robert Freund
1. Linear Rate Convergence of the Alternating Direction Method of 
Multipliers for Convex Composite Quadratic and Semi-definite 
Programming
Defeng Sun (matsundf@nus.edu.sg) National University of Singapore, 
Singapore, Deren Han, Liwei Zhang
Under a mild error bound condition, we establish the global linear rate of 
convergence for a rather general semi-proximal ADMM with the dual steplength 
being restricted to be in  for solving linearly constrained convex 
composite optimization problems. In our analysis, we assume neither the strong 
convexity nor the strict complementarity except the error bound condition, which 
holds automatically for convex composite quadratic programming. This semi-
proximal ADMM, which covers the classic one, has the advantage to resolve the 
potentially non-solvability issue of the subproblems in the classic ADMM and 
possesses the abilities of handling the multi-block cases efficiently. We shall use 
convex composite quadratic programming and quadratic semi-definite 
programming to demonstrate the significance of the obtained results. Of its own 
novelty in second-order variational analysis, a complete characterization is 
provided on the isolated calmness for the convex semi-definite optimization 
problem in terms of its second order sufficient optimality condition and the strict 
Robinson constraint qualification for the purpose of proving the linear rate 
convergence of the semi-proximal ADMM when applied to two- and multi-block 
convex quadratic semi-definite programming. [This is a joint work with Deren 
Han and Liwei Zhang]. 
2. On the Global Linear Convergence of Frank-Wolfe Optimization 
Variants
Simon Lacoste-Julien (simon.lacoste-julien@inria.fr) INRIA / ENS, 
France, Martin Jaggi
The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity 
thanks in particular to its ability to nicely handle the structured constraints 
appearing in machine learning applications. However, its convergence rate is 
known to be slow (sublinear) when the solution lies at the boundary. In this talk, 
I will present some less well-known variants of the FW algorithm for which we 
proved their global linear convergence rate recently for the first time, highlighting 
at the same time an interesting geometric notion of “condition number” for the 
constraint set appearing in the constant.
3. New Computational Guarantees for Solving Convex Optimization 
Problems with First Order Methods, via a Function Growth 
Condition Measure
Robert M Freund (rfreund@mit.edu) Massachusetts Institute of 
Technology, USA, Haihao Lu
Motivated by recent work of Renegar, we present new computational methods 
and associated computational guarantees for solving convex optimization 
problems using first-order methods. Our problem of interest is the general 
convex optimization problem f * = minf(x) subject to x ∈ Q, where we presume 
knowledge of a strict lower bound SLB < f *. [Indeed, SLB is naturally known 
when optimizing many loss functions in statistics and machine learning (least-
squares, logistic loss, exponential loss, total variation loss, etc.) as well as in 
Renegarʼs transformed version of the standard conic optimization problem; in all 
these cases one has SLB = 0 < f *.] We present new computational guarantees for 
the Subgradient Descent Method, for smoothing methods, and for an accelerated 
gradient method, that can improve existing computational guarantees in several 
ways, most notably when the initial iterate x0 is far from the optimal solution set. 
Furthermore, our accelerated gradient method performs parametric increased 
smoothing and periodic re-starting, even when f(·) is not strongly convex.

■ Thu.B.m3AB
Thursday, 10:45-12:00, Room m3AB
Advances in Large-Scale Nonsmooth Optimization
Cluster: Convex and Nonsmooth Optimization
Session organized by: Stephen Becker
1. GAP Safe Screening Rule for Sparsity Enforcing Penalties
Joseph Salmon (joseph.salmon@telecom-paristech.fr) Télécom 
ParisTech, France, Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort
High dimensional regression benefits from sparsity promoting regularizations. 
Screening rules leverage the known sparsity of the solution by ignoring some 
variables in the optimization, hence speeding up solvers. When the procedure is 
proven not to discard features wrongly the rules are said to be “Safe”. I will 
derive new safe rules for generalized linear models regularized with sparsity 
enforcing norms. GAP Safe rules can cope with any iterative solver and we 
illustrate their performance on coordinate descent for various applications (eg. 
multi-task Lasso, binary and multinomial logistic regression) demonstrating 
significant speed ups.
2. Nuclear Norms for Collaborative Filtering 
Jessica Gronski (jessica.gronski@colorado.edu) University of 
Colorado Boulder, USA, Aleksandr Aravkin, Stephen Becker, Derek 
Driggs
Recommender systems are commonly used to predict a userʼs preference or 
rating based off their past behaviors. One approach to designing recommender 
systems is collaborative filtering in which large yet sparse datasets are collected 
and analyzed for user information. For collaborative filtering, it is standard to 
consider a penalized regression model imposing a penalty on the trace or max-
norm. The trace norm is often referred to as the Schatten-1 or “the” nuclear norm 
and is used to promote sparsity in the target solution, preserving the structure of 
the original problem. The aim of our work is to consider nuclear norms in the 
more general setting described by G.J.O. Jameson and test the utility of their 
novel formulations against existing optimization algorithms. We will use 
collaborative filtering for the Netflix prize dataset to compare our formulations. 
3. Stochastic Numerical Methods for Monotone Inclusions in Hilbert 
Spaces
Bang Cong Vu (bang.vu@epfl.ch) École Polytechnique Fédérale de 
Lausanne (EPFL), Switzerland
In this talk, we present some stochastic numerical methods for solving monotone 
inclusions in real Hilbert spaces. We derive the stochastic approximation of the 
forward-backward, forwardbackward-forward and forward-Douglas Rachford 
splitting. The weak almost sure convergence are proved under suitable 
conditions. Applications to large-scale convex optimizations are demonstrated.

■ Thu.C.1S
Thursday, 13:30-14:45, Room 1S
Recent Advances in Coordinate Descent Algorithms
Cluster: Nonlinear Optimization
Session organized by: Martin Takac
1. Is Greedy Coordinate Descent a Terrible Algorithm?
Julie Nutini (jnutini@cs.ubc.ca) University of British Columbia, 
Canada, Mark Schmidt, Issam H Laradji, Michael Friedlander, Hoyt 
2016_iccopt.indb   105 2016/07/22   11:58:24
ABSTRACTS
ICCOPT 2016106
Koepke
There has been significant recent work on the theory and application of 
randomized coordinate descent algorithms, beginning with the work of Nesterov, 
who showed that a random-coordinate selection rule achieves the same 
convergence rate as the Gauss-Southwell selection rule. This result suggests that 
we should never use the Gauss-Southwell rule, as it is typically much more 
expensive than random selection. However, the empirical behaviours of these 
algorithms contradict this theoretical result: in applications where the 
computational costs of the selection rules are comparable, the Gauss-Southwell 
selection rule tends to perform substantially better than random coordinate 
selection. We give a simple analysis of the Gauss-Southwell rule showing that—
except in extreme cases—itʼs convergence rate is faster than choosing random 
coordinates. Further, we (i) show that exact coordinate optimization improves 
the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-
Lipschitz rule that gives an even faster convergence rate given knowledge of the 
Lipschitz constants of the partial derivatives, and (iii) analyze proximal-gradient 
variants of the Gauss-Southwell rule.
2. Flexible Coordinate Descent
Rachael Tappenden (rachael.tappenden@canterbury.ac.nz) University 
of Canterbury, New Zealand, Kimonas Fountoulakis
I will present a novel randomized block coordinate descent method for the 
minimization of a convex composite objective function. The method uses 
(approximate) partial second-order (curvature) information, so that the algorithm 
performance is more robust when applied to highly nonseparable or ill 
conditioned problems. We call the method Flexible Coordinate Descent (FCD). 
At each iteration of FCD, a block of coordinates is sampled randomly, a quadratic 
model is formed about that block and the model is minimized approximately/
inexactly to determine the search direction. An inexpensive line search is then 
employed to ensure a monotonic decrease in the objective function and 
acceptance of large step sizes. I present preliminary numerical results to 
demonstrate the practical performance of the method.
3. Coordinate Descent with Arbitrary Sampling: Algorithms and 
Complexity
Zheng Qu (zhengqu@maths.hku.hk) The University of Hong Kong, 
China, Peter Richtarik
In this talk we present a randomized coordinate descent method for solving the 
problem of minimizing the sum of a smooth convex function and a convex 
block-separable regularizer. Our method at every iteration updates a random 
subset of coordinates, following an arbitrary distribution. In special cases, it 
reduces to deterministic and randomized methods such as gradient descent, 
coordinate descent, parallel coordinate descent and distributed coordinate 
descent both in non-accelerated and accelerated variants. The variants with 
arbitrary (or importance) sampling are new. We provide a unified complexity 
analysis, from which we deduce as direct corollary complexity bounds for its 
many variants, all matching or improving best known bounds.

■ Thu.C.1A
Thursday, 13:30-14:45, Room 1A
Numerical Linear Algebra and Optimization II
Cluster: Nonlinear Optimization
Session organized by: Annick Sartenaer, Dominique Orban
1. On Solving an Unconstrained Quadratic Program by the Method 
of Conjugate Gradients and Quasi-Newton Methods
Anders Forsgren (andersf@kth.se) KTH Royal Institute of Technology, 
Sweden, Tove Odland
Solving an unconstrained quadratic program means solving a linear equation 
where the matrix is symmetric and positive definite. This is a fundamental 
subproblem in nonlinear optimization. We discuss the behavior of the method of 
conjugate gradients and quasi-Newton methods on a quadratic problem. We 
show that by interpreting the method of conjugate gradients as a particular exact 
linesearch quasi-Newton method, necessary and sufficient conditions can be 
given for an exact linesearch quasi-Newton method to generate a search direction 
which is parallel to that of the method of conjugate gradients. The analysis gives 
a condition on the quasi-Newton matrix at a particular iterate, the projection is 
inherited from the method of conjugate gradients. We also analyze update 
matrices and show that there is a family of symmetric rank-one update matrices 
that preserve positive definiteness of the quasi-Newton matrix. This is in contrast 
to the classical symmetric-rank-one update where there is no freedom in choosing 
the matrix, and positive definiteness cannot be preserved.
2. BFGS-like Updates of Constraint Preconditioners for Sequences 
of KKT Linear Systems
Daniela di Serafino (daniela.diserafino@unina2.it) Second University 
of Naples, Italy, Luca Bergamaschi, Valentina De Simone, Ángeles 
Martínez
We focus on the iterative solution of sequences of KKT linear systems such as 
those arising in interior point methods for large-scale quadratic programming. In 
this case, the use of effective preconditioners is a key issue to achieve efficiency 
in the optimization procedure. Constraint Preconditioners (CPs) are a successful 
choice as long as the computational cost of their setup is not too high. When this 
is not the case, a strategy for updating CPs can be a reasonable alternative to 
computing CPs from scratch, since it offers a tradeoff between cost and 
convergence, which can result into enhanced performance of the overall 
optimization method. In this work we present a technique which computes a 
preconditioner for any KKT system of a given sequence by performing a BFGS-
like update of a CP available for a previous system. A theoretical analysis as well 
as numerical experiments support the proposed strategy.
3. Refining the Bounds from Rusten-Winther with Insights on the 
Interaction between the Blocks (Hessian vs Constraints) in KKT 
Systems
Daniel Ruiz (daniel.ruiz@enseeiht.fr) Université Fédérale de Toulouse 
- INPT, France, Annick Sartenaer, Charlotte Tannier
The need to efficiently solve linear systems such as KKT ones, arising from the 
first order necessary optimality conditions, is crucial in many algorithms for 
solving constrained nonlinear continuous optimization problems. Such systems 
can be very ill-conditioned, in particular when the (1,1) block A has few very 
small eigenvalues (see Rusten and Winther, 1992). However, it is commonly 
observed that despite this possible ill-conditioning, some sort of interaction 
between A and the constraints (1,2) block actually occurs, that can either spoil the 
convergence of Krylov subspace methods like MINRES, or not at all. In this talk, 
we highlight some aspects of this interaction and give deeper insights on how 
and in which circumstances the bad conditioning contained in these few very 
small eigenvalues of the (1,1) block A effectively spoils the convergence of 
MINRES. Our study is based on theoretical arguments and supported by 
numerical illustrations.

■ Thu.C.1B
Thursday, 13:30-14:45, Room 1B
Risk-averse Optimization with PDE Constraints II
Cluster: PDE-constrained Optimization
Session organized by: Denis Ridzal, Drew Philip Kouri, Bart van Bloemen 
Waanders
1. A Data-driven Approach to PDE-constrained Optimization under 
Uncertainty
Drew Philip Kouri (dpkouri@sandia.gov) Sandia National 
Laboratories, USA
Many science and engineering applications require the control or design of a 
physical system governed by partial differential equations (PDEs). More often 
then not, PDE inputs such as coefficients, boundary conditions, or initial 
conditions are unknown and estimated from experimental data. In this talk, I will 
discuss some theoretical challenges associated with such PDE-constrained 
optimization problems, including their mathematical formulation and their 
efficient numerical solution. First, I will assume that we know the probability 
distributions that characterize the uncertain PDE inputs. For this case, I will 
introduce the notion of a risk measure as a means to quantify the “hazard” 
associated with large objective function values. Next, to handle the situation of 
an unknown probability distribution, I will introduce and analyze a 
distributionally-robust formulation for the optimization problem. To enable 
numerical solutions, I will present a novel discretization for the unknown 
probability measure and provide rigorous error bounds for this approximation. I 
will conclude with numerical results confirming the aforementioned error 
bounds.
2. Optimizing the Kelvin Force in a Moving Target Subdomain
Harbir Antil (hantil@gmu.edu) George Mason University, USA, 
Ricardo H Nochetto, Pablo Venegas
In order to generate a desired Kelvin (magnetic) force in a target subdomain 
moving along a prescribed trajectory, we propose a minimization problem with a 
tracking type cost functional. We use the so-called dipole approximation to 
realize the magnetic field, where the location and the direction of the magnetic 
sources are assumed to be fixed. The magnetic field intensity acts as the control 
and exhibits limiting pointwise constraints. We address two specific problems: 
2016_iccopt.indb   106 2016/07/22   11:58:24
ABSTRACTS
ICCOPT 2016 107
the first one corresponds to a fixed final time whereas the second one deals with 
an unknown force to minimize the final time. We prove existence of solutions 
and deduce local uniqueness provided that a second order sufficient condition is 
valid. We use the classical backward Euler scheme for time discretization. For 
both problems we prove the H1-weak convergence of this semi-discrete numerical 
scheme. This result is motivated by Γ-convergence and does not require second 
order sufficient condition. If the latter holds then we prove H1-strong local 
convergence. We report computational results to assess the performance of the 
numerical methods. As an application, we study the control of magnetic 
nanoparticles as those used in magnetic drug delivery, where the optimized 
Kelvin force is used to transport the drug to a desired location.
3. Nonlinear Robust Optimization using Second-Order 
Approximations and an Application to the Shape Optimization of 
Hyperelastic Load-carrying Structures
Philip Kolvenbach (kolvenbach@mathematik.tu-darmstadt.de) TU 
Darmstadt, Germany, Stefan Ulbrich
We consider the robust counterpart of a shape optimization problem with a 
nonlinear objective function and a nonlinear state equation, both of which depend 
on uncertain parameters from an elliptic uncertainty set. For a given shape, we 
approximate the worst-case function by the maximum of the second-order Taylor 
expansion of the reduced objective function, which depends solely on the 
uncertain parameters. This maximum can be computed efficiently with trust-
region methods. To bypass differentiability issues — the approximated worst-case 
function is not always differentiable with respect to the shape parameters — we 
reformulate the approximated robust counterpart as an MPEC. Since the resulting 
objective function and constraints are differentiable functions, standard gradient-
based optimization methods can then be used to find a robust optimal shape. We 
discuss how the required derivative terms can be computed efficiently. In the 
second part of the talk, we apply the presented method to the shape optimization 
of a load-carrying structure that is governed by a nonlinear hyperelastic 
constitutive model and is subject to an uncertain load. We briefly consider the 
finite-element discretization and present numerical results.

■ Thu.C.1C
Thursday, 13:30-14:45, Room 1C
Nonlinear Optimization: Algorithms and 
Implementations
Cluster: Nonlinear Optimization
Session chair: Paulo JS Silva
1. Implementation of NLP Solver with Multiple Precision Arithmetic 
and Numerical Behavior Analysis of SQP Method for Ill-posed 
NLPs
Hiroshige Dan (dan@kansai-u.ac.jp) Kansai University, Japan, Yuya 
Matsumoto
For achieving superlinear or quadratic convergence for nonlinear programming 
problems (NLPs) theoretically, algorithms for NLPs usually require some 
assumptions, such as appropriate constraint qualifications, strict complementarity 
conditions, local strictness of a solution, and so on. However, when we solve 
NLPs which do not satisfy such assumptions, algorithms for NLPs often show 
sufficiently fast convergence experimentally. To analyze such situations, we first 
implement an NLP solver based on the SQP method by using multiple precision 
arithmetic for the floating-point computation. Multiple precision arithmetic 
enables us to perform arbitrary precision arithmetic supported by software. In 
this presentation, we would like to introduce the detail of the implementation of 
our NLP solver, including automatic differentiation technique. Moreover, we can 
observe the detail of numerical behavior of the SQP method for NLPs by using 
this solver. In this research, we solve various NLPs which do not satisfy 
assumptions for fast convergence necessarily, and analyze numerical behavior of 
the SQP method. Especially, we would like to clarify the difference of numerical 
behavior between double precision arithmetic and multiple precision arithmetic.
2. A Memoryless Sized Symmetric Rank-One Method with Sufficient 
Descent Property for Unconstrained Optimization
Shummin Nakayama (1416702@ed.tus.ac.jp) Tokyo University of 
Science, Japan, Yasushi Narushima, Hiroshi Yabe
Quasi-Newton methods are widely used for solving unconstrained optimization 
problems. However, it is difficult to apply quasi-Newton methods directly to 
large-scale unconstrained optimization problems, because they need the storage 
of memories for matrices. In this talk, we consider a memoryless quasi-Newton 
method with the sized symmetric rank-one formula that does not need any 
storage of memories for matrices. We call the method the memoryless sized 
symmetric rank-one method. The existing method generates a sufficient descent 
direction for uniformly convex objective functions, but the method may not 
always generate the sufficient descent direction for general objective functions. 
We propose a new memoryless sized symmetric rank-one method which always 
generates the sufficient descent direction under the Wolfe conditions (or the 
strong Wolfe conditions), and present new sizing factors of the method. 
Furthermore, we prove the global convergence properties of the proposed 
method for uniformly convex and general objective functions, respectively. 
Finally, some numerical results are shown to investigate the effectiveness of 
sizing factors of the memoryless sized symmetric rank-one method.
3. Strict Constraint Qualifications and Sequential Optimality 
Conditions for Constrained Optimization
Paulo JS Silva (pjssilva@ime.unicamp.br) University of Campinas, 
Brazil, Roberto Andreani, Alberto Ramos Flor, Jose Mario Martinez
Sequential optimality conditions for constrained optimization are necessarily 
satisfied by local minimizers, independently of the fulfillment of constraint 
qualifications. These conditions support different stopping criteria for practical 
optimization algorithms. On the other hand, when an appropriate strict constraint 
qualification together with the associated sequential optimality condition holds 
at a point, it is possible to show that it satisfies the Karush-Kuhn-Tucker 
conditions. As a consequence, for each sequential optimality condition, it is 
natural to ask for its weakest strict constraint qualification. In this talk we will 
present such contraints qualifications starting with the one associated with the 
Approximate Karush-Kuhn-Tucker sequential optimality condition. Aftwards 
we will also briefly characterize the weakest strict constraint qualifications 
associated with other sequential optimality conditions that are useful for defining 
stopping criteria of algorithms. In addition, we will present all the implications 
between the new strict constraint qualifications and other constraint 
qualifications.

■ Thu.C.4A
Thursday, 13:30-14:45, Room 4A
Engineering Applications for Large Scale Nonlinear 
Optimization
Cluster: Applications in Energy, Science and Engineering
Session organized by: Christof Büskens, Mitja Echim
1. Large-Scale Trajectory Optimization for Autonomous Deep Space 
Missions
Mitja Echim (mitja@math.uni-bremen.de) University of Bremen, 
Germany, Anne Schattel, Christof Büskens
Trajectory planning for deep space missions has become a recent topic of great 
interest. The mathematical field of optimization and optimal control can be used 
to realize autonomous missions while protecting recourses and making them 
safer. The project KaNaRiA (ʻKognitionsbasierte, autonome Navigation am 
Beispiel des Ressourcenabbaus im Allʼ) investigates the possibilities of cognitive 
autonomous navigation on the example of an asteroid mining mission. This 
paper focuses on the specific challenge of the guidance during the cruise phase 
of the spacecraft, i.e. trajectory optimization and optimal control, including 
numerical solutions and results. The movement of the spacecraft due to 
gravitational influences of the Sun and other planets as well as the thrust 
commands is described through ordinary differential equations (ODEs). 
Competitive mission aims like short flight times and low energy consumption 
are considered by using a multi-criteria objective function. A Comparison of two 
different approaches for solving the optimal control problem will be presented. 
The so called full discretization approach leads to large-scale nonlinear 
optimization problems where sparsity information has to be considered for 
efficiency. On the other hand, when using the multiple-shooting approach the 
problem has a lower dimension but a higher degree of nonlinearity.
2. Optimization of Large Scale Characteristics for the Automotive 
Industry
Matthias Knauer (knauer@math.uni-bremen.de) Universität Bremen, 
Germany, Christof Büskens
The automotive industry is highly competitive and ruled by governmental and 
economical laws. For the application of a component in a modern car, e.g. 
engines or brakes, precise mathematical models are needed for the controllers to 
work satisfactorily. Based on physical and technical laws engineers develop 
parameter dependent models. In the simplest case these parameters can consist 
of single values. In reality, parameters depend on the systemʼs states, and can be 
represented by characteristic maps in several dimensions. Using measurement 
data, these characteristics can be identified solving large scale optimization 
problems. The industry is asking for the best data fit while maintaining desired 
smoothness properties and fulfilling user defined constraints for the 
characteristics. To solve these problems in acceptable time using gradient based 
methods, the sparsity of the problem has to be utilized. In this talk we present 
2016_iccopt.indb   107 2016/07/22   11:58:24
ABSTRACTS
ICCOPT 2016108
techniques and results for solving these large scale problems using the NLP 
solver WORHP. Additionally, as models might be time dependent, methods 
closely related to methods of optimal control can be used to perform parameter 
identification of dynamical processes. For this the transcription method 
TransWORHP will be used.
3. Mixed-Integer Optimal Control Problems with Indicator 
Constraints in Automotive Applications
Clemens Zeile (clemens.zeile@ovgu.de) University of Magdeburg, 
Germany, Sebastian Sager
Autonomous driving has become one of the most heavily researched areas in the 
automotive industry. In addition to driver comfort, it offers the opportunity for 
drastic emission reduction due to optimized individual driving and to optimized 
traffic systems, such as systems of traffic lights. We give illustrating examples 
and motivate how both approaches result in optimal control problems with 
indicator constraints, i.e., constraints that only need to be considered for certain 
choices of the integer valued control. In the second part we survey direct methods 
for mixed-integer optimal control and discuss our contributed extensions to treat 
indicator constraints.

■ Thu.C.4B
Thursday, 13:30-14:45, Room 4B
Newton-Krylov Methods in Real-Time Optimization 
for Nonlinear Model Predictive Control
Cluster: Applications in Energy, Science and Engineering
Session organized by: Toshiyuki Ohtsuka
1. Recent Advances in Newton-Krylov Methods for NMPC
Andrew Knyazev (Andrew.Knyazev@merl.com) Mitsubishi Electric 
Research Laboratories, USA, Alexander Malyshev
We present Newton-Krylov (AKA continuation) methods for NMPC, pioneered 
by Prof. Ohtsuka. Our main results is efficient preconditioning, leading to 
dramatically improved real-time Newton-Krylov MPC optimization. One idea is 
solving a forward recursion for the state and a backward recursion for the costate 
approximately, or reusing previously computed solutions, for the purpose of 
preconditioning. Another ingredient is sparse factorizations of high-quality 
preconditioners. Altogether, our fast and efficient preconditioning leads to 
optimal linear complexity in the number of gridpoints on the prediction horizon, 
for iterative solution of forward-difference Newton-Krylov NMPC. We suggest 
scenario/particle Newton-Krylov MPC in the case, where system dynamics or 
constraints discretely change on-line, for ensembles of predictions to various 
scenarios of anticipated changes, and test it for minimum time problems. On-line 
computation of ensembles of controls, for several scenarios of changing in real 
time system dynamics/constraints, allows choosing and adapting the optimal 
destination. The Newton-Krylov MPC approach is extended to the case, where 
the state is implicitly constrained to a manifold, and demonstrate its effectiveness 
for a hemisphere. References: arXiv:1512.00375 DOI:10.1016/j.
ifacol.2015.11.282 DOI:10.1016/j.ifacol.2015.11.102 DOI:10.1016/j.
ifacol.2015.11.071
2. Manycore Execution of Model Predictive Control
Koji Inoue (inoue@ait.kyushu-u.ac.jp) Kyushu University, Japan, 
Satoshi Kawakami, Takatsugu Ono
This talk focuses on a novel manycore execution strategy for real-time model 
predictive controls. The key idea is to exploit predicted input values, which are 
produced by the model predictive control itself, to speculatively solve optimal 
control problems. It is well known that control applications are not suitable for 
manycore processors, because feedback-loop systems inherently stand on 
sequential operations. Since the proposed scheme does not rely on conventional 
thread-/data-level parallelism, it can be easily applied to such control systems. 
An analytical evaluation using a real application demonstrates the potential of 
performance improvement achieved by the proposed speculative executions.
3. Velocity Form Nonlinear Model Predictive Control of a Diesel 
Engine Air Path
Mike Huang (mike.huang@toyota.com) University of Michigan/
TOYOTA, USA, Ilya Kolmanovsky, Ken Butts
The Diesel Air Path (DAP) system has been traditionally challenging to control 
due to its highly coupled nonlinear behavior and the need for constraints to be 
considered for drivability and emissions. Nonlinear Model Predictive Control 
(NMPC) has been viewed as a way to handle these challenges. However, current 
NMPC strategies for DAP control are still limited due to the very limited 
computational resources in engine control units. In this presentation, the 
development of a NMPC strategy for the DAP is given where the objective is to 
track intake manifold pressure and Exhaust Gas Recirculation (EGR) rate targets 
through coordinated control of the variable geometry turbine, EGR valve, and 
throttle. In the past, MPC controller performance has commonly been sacrificed 
to satisfy a stringent computational budget, e.g., short prediction horizons are 
commonly used in DAP MPC applications. To achieve both low complexity and 
high performance, a novel NMPC formulation, velocity form NMPC, and 
associated modelling structure is developed. Additionally, the synergies of this 
strategy to NMPC solver techniques utilizing inexact solutions and distributed-
over-time computations will be explored. Experiment results will be given that 
demonstrate the effectiveness of the resulting NMPC at achieving the control 
objectives.

■ Thu.C.5C
Thursday, 13:30-14:45, Room 5C
Vector Equilibrium Problems and Vector Optimization
Cluster: Multi-Objective and Vector Optimization
Session organized by: Dinh The Luc
1. Vector Quasi-Equilibrium Problems for the Sum of Two 
Multivalued Mappings
Gábor Kassay (kassay@math.ubbcluj.ro) Babes-Bolyai University 
Cluj, Romania, Mihaela Miholca, Nguyen The Vinh
We study vector quasi-equilibrium problems for the sum of two multivalued 
bifunctions. The assumptions are required separately on each of these bifunctions. 
Sufficient conditions for the existence of solutions of such problems are shown 
in the setting of topological vector spaces. The results unify, improve and extend 
some well-known existence theorems from the literature.
2. A New Type of Directional Regularity for Multifunctions with 
Applications to Optimization
Radu Strugariu (rstrugariu@tuiasi.ro) Gheorghe Asachi Technical 
University of Iasi, Romania
The concepts of linear openness, metric regularity and Aubin property of 
mappings were intensively studied in the last three decades, due to their 
importance as qualification conditions in mathematical programming with single 
and set-valued objectives. In this talk, we discuss a new type of directional 
regularity for mappings, constructed by the use of a minimal time function, given 
with respect to a set of directions. We present several properties, concerning 
continuity, convexity, Lipschitz behavior and subdifferential calculus of this 
function. Next, we introduce the directional triplet of regularities for 
multifunctions which naturally appears, as shown by several examples. We 
investigate necessary and sufficient conditions for the new directional regularity 
properties, formulated in terms of generalized differentiation objects of Fréchet 
type. Finally, applications to necessary and sufficient optimality conditions for 
Pareto minima of sets and multifunctions are provided, making use by the 
regularity concepts analyzed before. In all the results we present, the directional 
character of both hypotheses and conclusions is emphasized.
3. On Equilibrium in Multi-Criteria Transportation Networks
Dinh The Luc (dtluc@univ-avignon.fr) Avignon University, France, 
Truong TT Phuong
We develop a new method to generate the set of equilibrium flows of a multi-
criteria transportation network. To this end we introduce two optimization 
problems by using a vector version of the Heaviside Step function and the 
distance function to Pareto minimal elements and show that the optimal solutions 
of these problems are exactly the equilibria of the network. We study the 
objective functions by establishing their generic differentiability and local 
calmness at equilibrium solutions. Then we present an algorithm to generate a 
discrete representation of equilibrium solutions by using a modified Frank-Wolfe 
reduced gradient method and prove its convergence. We give some numerical 
examples to illustrate our algorithm and show its advantage over a popular 
method by using linear scalarization. 

■ Thu.C.5D
Thursday, 13:30-14:45, Room 5D
Linear Optimization in the Context of Solving NP-hard 
Problems
Cluster: Linear Optimization
Session organized by: Sergei Chubanov
1. Extended Formulations for Vertex Cover
2016_iccopt.indb   108 2016/07/22   11:58:24
ABSTRACTS
ICCOPT 2016 109
Austin Buchanan (buchanan@okstate.edu) Oklahoma State 
University, USA
The vertex cover polytopes of graphs do not admit polynomial-size extended 
formulations. This motivates the search for polyhedral analogues to 
approximation algorithms and fixed-parameter tractable (FPT) algorithms. In 
this presentation, we take the FPT approach and study the k-vertex cover polytope 
(the convex hull of vertex covers of size k). Our main result is that there are 
extended formulations of size O(1.47k+kn). We also provide FPT extended 
formulations for solutions of size k to instances of d-hitting set.
2. New Search Direction-based Interior-Point Algorithm for P*(K) 
Horizontal Linear Complementarity Problems over Cartesian 
Product of Symmetric Cones
Petra Renáta Takács (t_petra92@yahoo.com) Babeş-Bolyai 
University, Romania, Zsolt Darvay 
We introduce a new interior-point method, which is suitable for solving P*(κ) 
horizontal linear complementarity problems over Cartesian product of symmetric 
cones. We achieve this by using Euclidean Jordan algebras. The novelty of this 
method consists of the fact that it is based on a new search direction. In order to 
obtain this we use the method of algebraically equivalent transformation on the 
nonlinear equation of the system which defines the central path. This search 
direction can also be derived by considering a special class of barriers that can 
not be determined by usual kernel functions. The particularity of these kernel 
functions is that they are defined only for values that are greater than a strictly 
positive constant. Despite this we prove the polynomiality of the introduced 
algorithm. 
3. A Polynomial Projection Algorithm and Its Applications in Integer 
Linear Programming and Combinatorial Optimization
Sergei Chubanov (sergei.chubanov@uni-siegen.de) University of 
Siegen, Germany
In this talk, I will present a polynomial algorithm for linear programming based 
on a parallel application of the alternating projections. This projection algorithm 
can also solve a class of combinatorial problems, including the non-bipartite 
maximum matchings, in polynomial time. Another application is approximate 
solution of integer linear problems in time which is polynomial in the size of the 
problem and in the reciprocal of a bound on the maximum violation of constraints 
of the extended formulation.

■ Thu.C.5E
Thursday, 13:30-14:45, Room 5E
Advances in Robust Optimization VI
Cluster: Robust Optimization
Session organized by: Huan Xu
1. Simulation-based Algorithms for Robust Markov Decision 
Processes
William Benjamin Haskell (wbhaskell@gmail.com) National 
University of Singapore, Singapore, Huan Xu, Pengqian Yu
Robust Markov decision processes (MDPs) are an important problem class that 
address uncertainty in sequential decision-making problems. However, this 
problem class inherits the computational challenges of classical MDPs - 
especially when the state and action spaces are large. In this talk, we describe 
some methods for the solution of large-scale robust MDPs by using simulation-
based algorithms. In particular, we exploit the connection between robust and 
risk-aware optimization to identify equivalent risk-aware MDPs for which 
simulation is possible. Using the equivalent risk-aware MDPs, we show how to 
construct a near-optimal policy for the original robust MDP.
2. Learning the Uncertainty in Robust Markov Decision Processes
Huan Xu (isexuh@nus.edu.sg) National University of Singapore, 
Singapore, Shiau-Hong Lim, Shie Mannor
Abstract: An important challenge in Markov decision processes is to ensure 
robustness with respect to unexpected or adversarial system behavior, i.e., the 
parameter uncertainty. A standard paradigm to tackle this problem is the robust 
MDP framework, which models the parameters as arbitrary element of pre-
defined “uncertainty sets”, and seeks the minimax policy - the policy that 
performs the best under the worst realization of the parameters in the uncertainty 
set. A crucial problem of robust MDP, largely unaddressed in literature, is how to 
find appropriate description of the uncertainty in a principled data-driven way. In 
this talk we address this problem using an online learning approach: we devise 
an algorithm that, without knowing the true uncertainty model, is able to adapt its 
level of protection to uncertainty, and in the long run performs as good as the 
minimax policy as if the true uncertainty model is known. Indeed, the algorithm 
achieves similar regret bounds as standard MDP where no parameter is 
adversarial, which shows that with virtually no extra cost we can adapt robust 
learning to handle uncertainty in MDPs. To the best of our knowledge, this is the 
first attempt to learn uncertainty in robust MDPs.

■ Thu.C.5H
Thursday, 13:30-14:45, Room 5H
Stochastic Optimization: Theory and Applications
Cluster: Stochastic Optimization
Session chair: Alexei A Gaivoronski
1. Subdifferentials of Nonconvex Integral Functionals in Banach 
Spaces with Applications to Stochastic Dynamic Programming
Nobusumi Sagara (nsagara@hosei.ac.jp) Hosei University, Japan, 
Boris S Mordukhovich
The paper concerns the investigation of nonconvex and nondifferentiable 
integral functionals on general Banach spaces, which may not be reflexive and/
or separable. Considering two major subdifferentials of variational analysis, we 
derive nonsmooth versions of the Leibniz rule on subdifferentiation under the 
integral sign, where the integral of the subdifferential set-valued mappings 
generated by Lipschitzian integrands is understood in the Gelfand sense. Besides 
examining integration over complete measure spaces and also over those with 
nonatomic measures, our special attention is drawn to a stronger version of 
measure nonatomicity, known as saturation, to invoke the recent results of the 
Lyapunov convexity theorem type for the Gelfand integral of the subdifferential 
mappings. The main results are applied to the subdifferential study of the optimal 
value functions and deriving the corresponding necessary optimality conditions 
in nonconvex problems of stochastic dynamic programming with discrete time 
on the infinite horizon.
2. Achieving Consistency in Intertemporal Decisions via Stochastic 
and Robust Approaches
Jorge R Vera (jvera@ing.puc.cl) Pontificia Universidad Catolica de 
Chile, Chile, Alfonso Lobos, Ana Batista
In many applications, decisions are made in different stages or horizons. For 
instance, aggregate production planning decisions are done in tactical horizons 
and then the detail is managed in short term planning. Optimization models have 
been used for long in this area and one typical problem is how to deal with the 
inconsistencies that arise from the fact that different levels of aggregation and 
several sources of uncertainty are present in the different decisions stages. This 
work is motivated by problems in production planning in the forest industry, as 
well as recent questions in a model used for intertemporal planning of hospital 
capacity. In both cases, we want tactical decisions that can generate feasible and 
efficient decisions in the short term. We show how we have addressed this 
question using various approaches: the Robust Optimization paradigm, using 
both polyhedral as well as ellipsoidal uncertainty sets, including some estimates 
of probabilities of consistency; a more classical 2-stage stochastic approach, and 
a recent idea that try to achieve consistency by making tactical decisions in such 
a way that they guarantee certain sensitivity and stability characteristics of the 
short term problem. These results should be relevant in other situations where 
consistency is desirable.
3. Design of Reconfigurable Networks under Uncertainty by 
Concurrent Stochastic Optimization and Simulation
Alexei A Gaivoronski (Alexei.Gaivoronski@iot.ntnu.no) Norwegian 
University of Science and Technology, Norway, Jacopo Napolitano, 
Giovanni Sechi, Paola Zuddas
We consider design of networks consisting of supply nodes, demand nodes and 
transshipment nodes, which have to satisfy demand for some scarce resource. We 
are specifically interested in the situation, when such networks have variable 
topology due to different of reasons: high operation cost of certain links 
prohibiting continuous operation, need for regular maintenance, congestion, etc. 
The network manager can dynamically change the network topology according 
to certain parametrized rules. The network operates under conditions of 
substantial uncertainty due to variable demand, uncertain supply and other 
reasons. Examples of such networks arise in telecommunications, water 
resources management, transportation. We show how to design the optimal rules 
for operation of such networks by combining simulation and stochastic 
optimization with stochastic gradient methods. A practical example is provided, 
which deals with water resource management in southern Sardinia, where 
opening and closing of certain links corresponds to switching on and off water 
pumps.

2016_iccopt.indb   109 2016/07/22   11:58:24
ABSTRACTS
ICCOPT 2016110
■ Thu.C.5J
Thursday, 13:30-14:45, Room 5J
Advances in Nonlinear Optimization III
Cluster: Nonlinear Optimization
Session chair: Peter Kirst
1. A Method of Multipliers with Alternating Constraints for 
Nonlinear Optimization Problems 
Hassan Siti Nor (siti.hassan.82e@st.kyoto-u.ac.jp) Kyoto University, 
Japan, Niimi Tomohiro , Yamashita Nobuo 
In this paper, a new method of multipliers is proposed to solve constrained 
minimization problems which consist of equality and inequality constraints. The 
method solves a sequence of subproblems whose objective functions are an 
augmented Lagrangian function. The distinguishing feature of this method is that 
it allows the augmented Lagrangian function and its minimization problem to 
alternate the constraints at each iteration, that is some constraints are included in 
the augmented Lagrangian function, and the others remain in the subproblems. 
By alternating constraints, this method able to get more accurate estimated 
Lagrange multipliers by exploiting Karush-Kuhn-Tucker (KKT) points of the 
subproblems, and consequently it will converge more efficient and steady. For 
efficiency, some special structures of the subproblems are discussed. Numerical 
experiments are presented where this method were applied with a proximal 
gradient method to solve the subproblems of the large scale convex programming 
problems with linear constraints. 
2. The Common Limit in the Range of Property for Two Nonlinear 
Mappings 
Pakeeta Sukprasert (happy_t_ik@hotmail.com) King Mongkutʼs 
University of Technology Thonburi, Thailand, Pakeeta Sukprasert, 
Poom Kumam
In this work, we give some common fixed point results for two nonlinear 
mappings are satisfying generalized contractive condition by using the common 
limit in the range of property due to Sintunavarat and Kumam. The presented 
results extend, generalize, and improve many existing results in the literature.
3. Solving Disjunctive Optimization Problems by Generalized Semi-
infinite Optimization Techniques
Peter Kirst (peter.kirst@kit.edu) Karlsruhe Institute of Technology, 
Germany, Oliver Stein
We describe a new possibility to model disjunctive optimization problems as 
generalized semi-infinite programs. In contrast to existing methods, for our 
approach neither a conjunctive nor a disjunctive normal form is expected. 
Applying existing lower level reformulations for the corresponding semi-infinite 
program we derive conjunctive nonlinear problems without any logical 
expressions, which can be locally solved by standard nonlinear solvers.

■ Thu.C.5K
Thursday, 13:30-14:45, Room 5K
Algorithms for Nonsmooth Optimization
Cluster: Convex and Nonsmooth Optimization
Session chair: André Uschmajew
1. A Primal Majorized Semismooth Newton-CG Augmented 
Lagrangian Method for Large-Scale Linearly Constrained Convex 
Programming
Chengjing Wang (renascencewang@hotmail.com) Southwest Jiaotong 
University, China, Tang Peipei
In this paper, we propose a primal majorized semismooth Newton-CG augmented 
Lagrangian method for large-scale linearly constrained convex programming 
problems, especially for some difficult problems which are nearly degenerate. 
The basic idea of this method is to apply majorized semismooth Newton-CG 
augmented Lagrangian method to the primal convex problem. And we take two 
special nonlinear semidefinite programming problems as examples to illustrate 
the algorithm. Furthermore, we establish the iteration complexity of the 
algorithm. Numerical experiments demonstrate that our method works very well 
for the testing problems, especially for many ill-conditioned ones.
2. Bundle Trust-Region Method for Marginal Functions using Outer 
Subdifferentials
Martin Knossalla (knossalla@math.fau.de) Friedrich-Alexander 
University Erlangen-Nürnberg, Germany
The theory of subdifferentials provides adequate methods and tools to put 
descent methods for nonsmooth optimization problems into practice. But in 
application it is often difficult to decide on a suitable subdifferential to construct 
a descent method. Furthermore there is often no exact information about the 
whole subdifferential for locally Lipschitz continuous functions, e.g. for marginal 
functions in parametric mathematical programming. In these cases the 
semismoothness of the cost functions cannot be proven or is violated. Basing on 
the (continuous) outer subdifferentials we have developed, this talk presents a 
new strategy for optimization problems with locally Lipschitz continuous cost 
functions. At first a descent method will be developed for arbitrary locally 
Lipschitz continuous functions, which is realized by projections on the outer 
subdifferential of the function. Possibly, it can happen that the computation of 
the whole outer subdifferential is too heavy. For this reason we will approximate 
outer subdifferentials especially for marginal functions. Basing on this 
approximation a bundle trust-region method will be developed and its global 
convergence will be proven.
3. A Riemannian Gradient Sampling Algorithm for Nonsmooth 
Optimization on Manifolds
André Uschmajew (uschmajew@ins.uni-bonn.de) University of Bonn, 
Germany, Seyedehsomayeh Hosseini
We present a generalization of the gradient sampling algorithm for nonsmooth 
locally Lipschitz functions to a Riemannian setting. The method is based on 
approximating the subdifferential of the cost function at every iteration by the 
convex hull of transported gradients from randomly generated nearby points to 
the current iterate. The main advantage is a relatively strong convergence result 
that can be obtained under the assumption that the cost function is continuously 
differentiable on an open dense subset, and that the employed vector transport 
and retraction satisfy certain conditions, which hold for instance for the 
exponential map and parallel transport. Then with probability one the algorithm 
is feasible, and each cluster point of the iterates is a Clarke stationary point 
(provided the cost function is bounded below). We illustrate the efficiency of 
Riemannian gradient sampling algorithms in a numerical comparison regarding 
the problem of finding the sparsest vector in a linear subspace.

■ Thu.C.m3S
Thursday, 13:30-14:45, Room m3S
Augmented Lagrangian-based Algorithms for Large-
Scale Conic Programming
Cluster: Convex and Nonsmooth Optimization
Session organized by: Kim-Chuan Toh
1. Fast Algorithm for Lasso
Xudong Li (matlixu@nus.edu.sg) National University of Singapore, 
Singapore, Defeng Sun, Kim-Chuan Toh
In this talk, we present a fast algorithm for solving large-scale l1-regularized least 
square regression (the Lasso) problems. The algorithm consists of two phases 
with Phase I to solve the problem to moderate accuracy or using it to warm start 
Phase II which aims at obtaining an accurate solution efficiently. The global 
convergence and local (super-)linear convergence results are established. 
Numerical results including the comparison between our approach and several 
state-of-the-art solvers on real data sets are presented to demonstrate the high 
efficiency and robustness of our proposed algorithm in solving large-scale 
problems. 
2. Semidefinite Inverse Quadratic Eigenvalue Problem with 
Prescribed Entries and Partial Eigendata
Ying Cui (matcuiy@nus.edu.sg) National University of Singapore, 
Singapore, Zhengjian Bai, Defeng Sun
The semidefinite inverse quadratic eigenvalue problem (SDIQEP) is to find a 
quadratic pencil such that it is nearest to the original analytic pencil in the 
Frobenius norm, satisfies the measured eigendata and preserves the positive 
semidefiniteness and prescribed entries. In this talk, we first show that SDIQEP 
can be taken as unconstrained multi-block convex composite programming via 
the dual approach. Following that, an efficient accelerated block coordinate 
descent method would be introduced. We shall also discuss the augmented 
Lagrangian method for solving the dual problem if the weighted distance is 
adopted in the primal objective function. Numerical results demonstrate that the 
proposed method outperforms the state-of-the-art algorithms, especially when a 
large amount of eigendata has been observed.
3. SDPNAL+: A Matlab Software for Semidefinite Programming 
with Bound Constraints
Kim-Chuan Toh (mattohkc@nus.edu.sg) National University of 
2016_iccopt.indb   110 2016/07/22   11:58:24
ABSTRACTS
ICCOPT 2016 111
Singapore, Singapore, Defeng Sun, Liuqin Yang, Xinyuan Zhao
 SDPNAL+ is a Matlab software package that implements an augmented 
Lagrangian based method to solve large scale semidefinite programming 
problems with bound constraints. The implementation was initially based on a 
majorized semismooth Newton-CG augmented Lagrangian method, but we 
subsequently implement it within an inexact symmetric Gauss-Seidel based 
semi-proximal ADMM/ALM (alternating direction method of multipliers/
augmented Lagrangian method) framework for the convenience of deriving 
simpler stopping conditions. Numerous problems arising from combinatorial 
optimization and binary integer quadratic programming problems have been 
tested to evaluate the performance of the solver. Extensive numerical test results 
show that the proposed method is quite efficient and robust.

■ Thu.C.m3AB
Thursday, 13:30-14:45, Room m3AB
Conic and Polynomial Optimization: Copositive 
Optimization
Cluster: Conic and Polynomial Optimization
Session organized by: Luis F Zuluaga
1. Inner Approximations of Completely Positive Reformulations of 
Mixed Binary Quadratic Programs
E Alper Yildirim (alperyildirim@ku.edu.tr) Koc University, Turkey
Every quadratic programming problem with a mix of continuous and binary 
variables can be equivalently reformulated as a completely positive optimization 
problem, i.e., a linear optimization problem over the convex but computationally 
intractable cone of completely positive matrices. In this talk, we focus on general 
inner approximations of the cone of completely positive matrices on instances of 
completely positive optimization problems that arise from the reformulation of 
mixed binary quadratic programming problems. We provide a characterization of 
the feasibility of such an inner approximation as well as the optimal value of a 
feasible inner approximation. For polyhedral inner approximations, our 
characterization implies that computing an optimal solution of the corresponding 
inner approximation reduces to an optimization problem over a finite set. Our 
characterization yields, as a byproduct, an upper bound on the gap between the 
optimal value of an inner approximation and that of the original instance. We 
discuss the implications of this error bound for standard and box-constrained 
quadratic programs.
2. On Completely Positive Modeling of Quadratic Problems
Van Nguyen (nguyen@uni-trier.de) Trier University, Germany, 
Mirjam Dür
Copositive programming deals with linear optimization problems over the 
copositive cone and its dual, the completely positive cone. The motivation to 
study this type of problem is that many nonlinear quadratic problems (even with 
binary constraints) can be cast in this framework. In order to have strong duality 
in conic optimization, strict feasibility of the problems is required. Strict 
feasibility is also advantagous in numerical solution approaches, for example 
when inner approximations of the copositive cone are used. We show that not all 
of the known completely positive formulations of quadratic and combinatorial 
problems are strictly feasible and discuss conditions which ensure this property.
3. Copositive Certificates of Non-negativity
Luis F Zuluaga (lzuluagag@gmail.com) Lehigh University, USA, 
Juan Vera, Bissan Ghaddar, Joe Naoum-Sawaya, Xiaolong Kuang
Classical certificates of non-negativity for polynomials over semialgebraic sets 
such as Schmudgenʼs or Putinarʼs Positivstellensatz are typically written in terms 
of sums-of-squares polynomials whose degree is not known a priori. In this talk 
we show that the non-negativity of a polynomial over a general class of 
semialgebraic sets can be certified using copositive polynomials of known 
degree. As a consequence, a very rich class of convergent hierarchies of LMI 
problems can be constructed to approximate the solution of general polynomial 
optimization (PO) problems. In particular, Polyaʼs Positivstellensatz can be used 
to construct new convergent linear, second-order cone, and semidefinite 
programming hierarchies to address the solution of (PO) problems.
 
2016_iccopt.indb   111 2016/07/22   11:58:25
POSTERS
ICCOPT 2016112
ABSTRACTS OF POSTER PRESENTATIONS
The Poster Session will take place on Monday, August 8th at 
the Foyer of GRIPS on the 1st floor from 17:30 to 19:30.
P1. Optimal Wind Turbine Placement considering Power Demand 
and Wind Uncertainties in Taiwan
Peng-Yeng Yin (pengyengyin@gmail.com) National Chi Nan 
University, Taiwan, Tsai-Hung Wu, Ching-Hui Chao, Ping-Yi Hsu
The awareness of climate change has gathered over 196 countries in Paris to put 
forward a protocol for restraining the emission of greenhouse gases. This 
commitment relies on an anticipation of a future rapid growth in renewable 
energy capacity. Wind energy plays a central role in Taiwanʼs renewable energy 
development. The simulation for optimal wind turbine placement is a technology 
for minimizing the cost of energy (COE) and mitigating the wake effect. 
However, little literature addresses this problem considering both power demand 
and wind uncertainties. This talk first presents a multi-expert demand forecasting 
method based on a repository of historical demand data. The multi-expert 
method uses a reinforcement learning framework to optimally select the best 
experts at various prediction stages. Then, a model for obtaining optimal COE 
and reducing the gap between the production and predicted demand is proposed. 
The simulation is conducted with historical data of wind condition and power 
demand in central Taiwan area. The experimental results show that the proposed 
model can deal with uncertainties regarding wind and demand. Convergence and 
worst-case analyses are conducted to provide a reliable evaluation of our 
approach.
P2. Tensor and Its Tucker Core: the Invariance Relationships
Fan Yang (fannyfanyang@stanford.edu) Shanghai University of 
Finance and Economics, China, Bo Jiang, Shuzhong Zhang
In Most Tensor Problems Are NP-hard, Hillar and Lim famously demonstrated 
that “multilinear (tensor) analogues of many efficiently computable problems in 
numerical linear algebra are NP-hard”. Despite many recent advancements, the 
state-of-the-art methods for computing such ʻtensor analoguesʼ still suffer 
severely from the curse of dimensionality. In this paper we show that the Tucker 
core of a tensor however, retains many properties of the original tensor, including 
the CP rank, the border rank, the tensor Schatten quasi norms, and the 
Z-eigenvalues. Since the core is typically smaller than the original tensor, this 
property leads to considerable computational advantages, as confirmed by our 
numerical experiments. In our analysis, we in fact work with a generalized 
Tucker-like decomposition that can accommodate any full column-rank factor 
matrices.
P3. New DC Diagonal Bundle Method for Clustering in Very 
Large Data Sets
Napsu Karmitsa (napsu@karmitsa.fi) University of Turku/
Federation University Australia, Finland, Adil Bagirov, Sona 
Taheri
Clustering is among most important tasks in data mining. This problem in very 
large data sets is challenging for most existing clustering algorithms. It is 
important to develop clustering algorithms which are accurate and can provide 
real time clustering in such data sets. Now we introduce one such algorithm: 
namely, the DC Diagonal Bundle Method (DCD-Bundle). Using nonsmooth 
optimization formulation of the clustering problem the objective function in this 
problem can be represented as a difference of two convex functions (DC). The 
new DCD-Bundle explicitly utilizes this structure to solve clustering problems. 
The method is evaluated using real world data sets with both the large number of 
attributes and/or large number of data points. The new algorithm is also compared 
with an other algorithm based on DC representation.
P4. QPLIB — A Library of Quadratic Programming Instances
Fabio Furini (fabio.furini@dauphine.fr) LAMSADE - Paris 
Dauphine, France, Emiliano Traversi
In this work we present a library of Quadratic Programming Instances (QPLIB). 
Quadratic programming (QP) problems have received an increasing amount of 
attention in recent years, both from theoretical and practical points of view. The 
QPLIB balances instances from real-world applications and academic problems. 
The QPLIB aims at being used as reference for the community and the 
practitioner involved in QP.
P5. Dantzig Wolfe Decomposition and Simplicial Decomposition 
in Quadratic Programming
Emiliano Traversi (emiliano.traversi@gmail.com) University of 
Paris 13, France, Enrico Bettiol, Alberto Ceselli, Lucas Létocart, 
Francesco Rinaldi
In this work we deal with problems with a Quadratic objective function and 
linear constraints. We will show how to solve the Quadratic Knapsack Problem 
with cardinality constraint using Danzig-Wolfe Decomposition and in a second 
step, we will use Simplicial Decomposition to solve the Portfolio Optimization 
problem. A comparison of both decompositions will be finally provided.
P6. An Index Tracking Model Embedded Stratified Sampling in 
Optimal Allocation
Meihua Wang (yuyu0504@163.com) Xiʼan Jiaotong University, 
China, Fengmin Xu, Yu-Hong Dai
This paper focuses on the strategy of tracking portfolio decision in the field of 
passive fund management. To include adequate practical information and 
promote better out-of-sample performance, a novel strategy is proposed which 
combines the existing stratified and optimized sampling strategies. Specifically, 
we build a mixed integer program model to represent the stratification 
information, the cardinality requirement and other practical constraints. The 
resulting model is shown to have satisfactory ability of forecasting and generate 
optimal tracking portfolios owning inspiring performances especially in out-of-
sample time period. Moreover, a stratified hybrid genetic algorithm with a new 
designed crossover operator is proposed for solving the proposed model. 
Computational tests on practical data sets from China Stock Exchange Market 
demonstrate the efficiency of the algorithm and the superiority of the new 
indexation strategy over the existing strategies from various perspectives.
P7. A Decomposition Method for a Class of Distributionally 
Robust Multistage Stochastic Optimization Problems
Haodong Yu (nianchuixiao@msn.com) Shanghai Lixin University 
of Commerce, China, Jie Sun
We consider distributionally robust multistage stochastic optimization problems. 
For simplicity, we assume that the involved random process is stagewise 
independent. At each stage, the decision maker has to solve a min-max problem 
with the ambiguity set described by moment constraints or chi-square distance. 
The method constructs a sequence of approximations to different scenarios and 
each approximation problem is transformed to a conic programming problem. 
Convergence of the approximation procedure is proven. A numerical example is 
presented to show the effectiveness of the proposed method.
P8. Optimal Stopping for Risk Measures
Masayuki Kageyama (kageyama@sda.nagoya-cu.ac.jp) Nagoya 
City University, Japan, Qi Wang
We consider the optimal stopping problems in an uncertain environment. In the 
classical optimal stopping problems, we consider maximizing the expected 
value. However, in Markov decision processes many authors study a problem in 
which we minimizing a risk. We deal with a risk minimizing optimal stopping 
problem concerning with the secretary problem.
P9. A Mixed-Integer SOCP Model for Robust and Power Efficient 
Networks
Bimal Chandra Das (bcdasdiu@gmail.com) The University of 
Electro-Communications, Japan, Ihsen Aziz Ouedraogo, Eiji Oki, 
Masakazu Muramatsu
We propose a model that considers fluctuation in data of power efficient 
networks, formulate it into a mixed-integer second-order cone 
programming(MISOCP), and solve it by using modern MISOCP solvers. In 
power efficient network problems, the operator tries to save energy by powering 
off some of their links based on estimated demands. We deal with the case where 
the estimated demands could have some errors. Compared to the models 
proposed on similar idea, our model has an advantage that we can set the subtotal 
amount of errors. We will report results of experiments of this model on the 
examples in the literatures.
P10. Analysis of an EOQ Inventory Model with Backordering and 
Time-and-Price Dependent Demand
Joaquin Sicilia-Rodriguez (jsicilia@ull.es) Universidad de La 
Laguna, Spain, Luis A San-Jose
The article presents a new economic order quantity (EOQ) model in which 
shortages are allowed and completely backordered. It is assumed that the demand 
rate is the product of a price-dependent algebraic function and a time-dependent 
power function. The net inventory level that describes the evolution of the 
inventory and the cost functions related to the inventory management are 
formulated. The aim consists of obtaining the optimal inventory policy and the 
optimal selling price which maximize the profit per unit time. An exhaustive 
solution procedure to determine the best inventory policy is developed. Several 
2016_iccopt.indb   112 2016/07/22   11:58:25
POSTERS
ICCOPT 2016 113
numerical examples to illustrate the theoretical results are presented.
P11. Establishing Big Data Analysis Framework for Computing 
Optimal Parameters 
Yu-Ching Lee (yclee@ie.nthu.edu.tw) National Tsing Hua 
University, Taiwan, Yi-Hao Huang, Si-Jheng Ciou
Data analysis on scalable framework has already been a popular and widely 
employed technique for plenty of years. The technique is able to process and 
analyze the data that is unable to be dealt with by the general software. Hadoop 
is one of the robust and well-behaved tools in views of parallel computing and 
distributed storage. We employ big data analysis to discover the potential 
information and patterns on Hadoop. The aim is to generate a quantifiable value 
to represent the customersʼ willingness to buy products. Our case study is the 
vehicle data. First, we split the whole data into training data and testing data. 
Second, we combine Hadoop with R so that we can analyze the training data on 
R by keying in the commands of Hadoop Distributed File System. Third, the 
Nash equilibrium of customersʼ choice on R platform is solved. Finally, the 
proposal method is further validated by the real data of vehicles. This paper is 
aimed to provide a method to connect the advanced game-based model with the 
existing scalable computing system.
P12. Interval Constraint Solving Techniques for Prediction and 
Control of Dynamic System Behavior
Martine C Ceberio (mceberio@utep.edu) The University of Texas 
at El Paso, USA, Leobardo Valera
Many natural phenomena are dynamic systems, which can often be modeled as 
nonlinear systems of equations. Solving such nonlinear systems can be tricky 
because fine-grain or long simulations can yield very large dimensions. 
Uncertainty is also an issue as handling it reliably remains a hard problem to 
solve. Finally, what if weʼd like to identify initial conditions or other parameters 
to ensure some conditions / control the behavior of the system at hand? Reduced-
Order Modeling (ROM) allows to address dimension issues, often via Proper 
Orthogonal Decomposition (POD). On the other hand, interval constraint-
solving techniques (ICST) allow to handle uncertainty and ensure reliability and 
completeness of results. In this work, we propose to use and embed ICST in 
traditional approaches to ROM, so as to address both dimension and uncertainty. 
We also show that ICST allow us to model problems in a way that makes it 
possible to not only predict but control dynamic systemsʼ behavior. We present 
and analyze preliminary results on benchmarks such as Burgersʼ equation, Fitz-
Hugh Nagumo equations, and other problems to demonstrate how prediction and 
control are achieved with ICST even when solving in smaller dimensions.
P13. A Gauss-Seidel Method for Multi-Leader-Follower Games
Atsushi Hori (m16ss004@nanzan-u.ac.jp) Nanzan University, 
Japan, Masao Fukushima
The multi-leader-follower game is an important model of noncooperative games. 
The game is comprised of two or more leaders and followers. A special case of 
multi-leader-follower game is the Stackelberg game (single-leader-follower 
game), which has been studied for many years. The Stackelberg game may be 
reformulated as a mathematical problem with equilibrium constraints (MPEC), 
which has also been studied extensively in recent years. On the other hand, the 
multi-leader-follower game may be formulated as an equilibrium problem with 
equilibrium constraints (EPEC), in which each leaderʼs problem is an MPEC. 
But finding an equilibrium in such an EPEC is much more difficult than solving 
a single MPEC, because each leaderʼs MPEC contains those variables which are 
common to other playersʼ MPECs. In this work, we propose a Gauss-Seidel-type 
algorithm with a penalty technique for solving an EPEC associated with the 
multi-leader-follower game. We report numerical results to illustrate the behavior 
of the algorithm.
P14. Effect of Subsidies on Reverse Supply Chains: A Variational 
Inequality Approach
I-Hsuan Hong (ihong@ntu.edu.tw) National Taiwan University, 
Taiwan, Pin-Chun Chen, Hsien-Ting Yu
This study examines the effect of government subsidies on recycled flows in 
reverse supply chains by using the approach of variational inequalities. We 
present a four-tiered chain model consisting of sources, collectors, processors 
and demand markets. The modified projection method is applied to solving for 
the equilibrium flows of each node. The case study results demonstrate the 
scheme of subsidy to processors outperforms other subsidy schemes.
P15. Application of FEM and Abductive Network to Determine the 
Optimum Machine Power and Billet Dimensions of Near Net-
Shape Spiral Bevel Gear Forging
Tung-Sheng Yang (tsyang@nfu.edu.tw) 64, Wunhau Rd, Huwei, 
Yulin, Taiwan, Taiwan, Jia-Hua Liang
In this paper, the use of the finite element method in conjunction with abductive 
network is presented to predict the maximum forging force and the volume of 
billet during near net-shape spiral bevel gear forging. The maximum forging 
load, effective stress and volume of billet are influenced by the process 
parameters such as modules, number of teeth, and workpiece temperature. A 
finite element method is used to investigate the forging of spiral bevel gear. In 
order to verify the prediction of FEM simulation for forging load, the experimental 
data are compared with the results of current simulation. A finite element analysis 
is also utilized to investigate the process parameters on forging load, maximum 
effective stress and volume of billet. Additionally, the abductive network was 
applied to synthesize the data sets obtained from the numerical simulation. The 
prediction models are then established for the maximum forging load, maximum 
effective stress and volume of billet of near net-shape spiral bevel gear forging 
under a suitable range of process parameters. After the predictions of the 
maximum forging force and the volume of billet, the optimum of the power of 
forging machine and the dimensions of billet are determined.
P16. A Multi-Material Phase Field Approach for the Optimal 
Design of Germanium-on-Silicon Microbridges
Lukáš Adam (adam@utia.cas.cz) Humboldt University of Berlin, 
Germany, Michael Hintermüller, Thomas M Surowiec
The reduction in microprocessor size lead to a significant increase in 
computational power. However, there is a physical limit to this increase. Instead 
of transmitting information via electricity, it has been recently proposed to 
transmit it via light. This idea is based on generating large strain in a germanium/
silicon device to allow it to emit light. Unfortunately, due to heating issues the 
operational time of such lasers is currently limited to minutes. By maximizing 
strain in the optical cavity, the lasing threshold should be reduced and the lasing 
device become more heat resistant. We model the laser and strain maximization 
as an optimal control problem. By using optimization techniques to solve it we 
have managed to obtain approximately 30% larger strain than the original 
configuration.
2016_iccopt.indb   113 2016/07/22   11:58:25
INDEX OF SPEAKERS, SESSION ORGANIZERS AND CHAIRS
ICCOPT 2016114
INDEX OF SPEAKERS, SESSION 
ORGANIZERS AND CHAIRS
Sessions where the person is a speaker are listed in 
bold. Sessions where the person is an organizer or a 
chair are listed in regular text.
A
Abernethy, Jacob Tue.B.5D
Adam, Lukáš Poster.P16
Adhikari, Lasith Mon.B.5F
Ahipasaoglu, Selin Damla Tue.A.5E, Tue.D.5G
Ahmadi, Amir Ali Mon.B.5L, Tue.D.5L
Akimoto, Youhei Thu.A.1C
Amaran, Satyajith Wed.B.1C
Anjos, Miguel Mon.B.5K, Wed.A.5D,
 Wed.A.5D
Antil, Harbir Thu.C.1B
Araya, Yousuke Wed.C.5C
Ashraphijuo, Morteza Tue.C.4A
Aswani, Anil Wed.A.5E
Attouch, Hedy Tue.C.5J, Tue.C.5J
Auger, Anne Wed.A.1C
Aybat, Necdet Serhat Thu.A.1S
B
Bach, Francis Mon.A.m3S, Mon.B.m3S
Baes, Michel Mon.B.5H
Bajbar, Tomas Wed.C.5C, Wed.C.5C
Baltean-Lugojan, Radu Wed.A.m3AB
Basu, Amitabh Wed.C.5G
Beck, Amir Wed.B.m3S
Becker, Stephen Thu.B.m3AB
Belotti, Pietro Wed.B.m3AB, Wed.B.m3AB
Benchimol, Pascal Wed.B.5D
Benosman, Mouhacine Tue.C.4A
Bian, Wei Tue.B.1A
Biegler, Lorenz T Mon.A.1S
Blekherman, Greg Tue.D.5L
Bordin, Chiara Mon.B.4A, Mon.B.4A
Bremner, David Thu.A.5D, Thu.A.5D
Briceño-Arias, Luis Manuel Thu.B.5C, Thu.B.5C
Brockhoff, Dimo Mon.B.1C
Buchanan, Austin Thu.C.5D
Bueno, Luis Felipe Thu.B.4B
Bueskens, Christof Tue.C.5G
Burdakov, Oleg Tue.B.1S
Burns, John Allen Mon.B.1B
Burtscheidt, Johanna Wed.C.5H
Büskens, Christof Thu.C.4A
C
Cai, Xingju Tue.A.5J
Cardoso, Domingos Moreira Wed.A.5G
Cartis, Coralia Mon.A.m3S, Tue.D.5D,
 Wed.C.5F, Thu.B.5F
Ceberio, Martine C Poster.P12
Cervinka, Michal Mon.A.5C
Chambolle, Antonin Tue.D.5J
Chandrasekaran, Venkat Tue.C.5L
Chen, Bilian Wed.C.5J
Chen, Bintong Thu.B.5H
Chen, Caihua Tue.A.5J, Tue.B.5J
Chen, Cheng Tue.D.1A
Chen, Jein-Shan Wed.A.4B
Chen, Jingnan Tue.C.5H, Tue.C.5H
Chen, Xi Mon.B.4B
Chen, Xiaojun Tue.A.1A
Chen, Zhi Mon.A.5E
Chiang, Nai-Yuan Wed.B.4A, Wed.B.4A
Cho, Gyeong Mi Mon.A.5I
Choi, Woong Bee Tue.B.4B
Christof, Constantin Mon.B.1B
Chubanov, Sergei Thu.C.5D, Thu.C.5D
Chuong, Thai Doan Tue.A.5A
Clason, Christian Wed.C.1B
Claus, Matthias Wed.C.5H, Wed.C.5H
Cojocaru, Monica Gabriela Wed.B.m3AB
Crélot, Anne-Sophie Tue.A.1C
Crespi, Giovanni Paolo Mon.B.m3AB
Cui, Shisheng Tue.C.4B, Tue.D.4B,
 Tue.D.4B
Cui, Ying Thu.C.m3S
Curtis, Frank Edward Mon.A.1S, Mon.B.1S
D
Dadush, Daniel N Tue.C.5D
DʼAmbrosio, Claudia Tue.B.4A
Dan, Hiroshige Thu.C.1C
Das, Bimal Chandra Poster.P9
Dash, Sanjeeb Wed.A.1S
de Klerk, Etienne Mon.A.5L
de Pinho, Maria do Rosario Thu.B.4A, Thu.B.4A
de Ruiter, Frans Thu.A.5E
De Santis, Marianna Wed.C.1S, Wed.C.1S
Dempe, Stephan Tue.B.5A
Deza, Antoine Wed.A.5G, Wed.B.5D
di Serafino, Daniela Thu.C.1A
Diez, Matteo Thu.B.1C, Thu.B.1C
Dimitrov, Nedialko Mon.B.4B
Ding, Chao Wed.A.5J
Disser, Yann Wed.B.5G
Doan, Xuan Vinh Wed.B.5E, Wed.B.5E
Dong, Hongbo Mon.B.5K
Dong, Qian Thu.A.1A
Dong, Zhilong Tue.D.1A
Dowling, Alexander W Tue.B.4A, Tue.B.4A
Dreves, Axel Mon.A.5C
2016_iccopt.indb   114 2016/07/22   11:58:25
INDEX OF SPEAKERS, SESSION ORGANIZERS AND CHAIRS
ICCOPT 2016 115
Drori, Yoel Wed.B.m3S
Dumas, Laurent Tue.A.1C
Durea, Marius Thu.A.5C
E
Eason, John Patrick Tue.A.1C
Echim, Mitja Thu.C.4A, Thu.C.4A
Eckstein, Jonathan Thu.A.1S, Thu.A.1S
Edirisinghe, Chanaka Tue.C.5H
Eldan, Ronen Tue.D.5I
Elmachtoub, Adam Tue.D.5E
Erway, Jennifer Tue.D.1S
F
Fan, Jinyan Wed.B.5L
Faulk, Preston Elliott Tue.B.5I
Faybusovich, Leonid Mon.A.5D, Mon.B.5D,
 Tue.A.5I
Fazel, Maryam Tue.D.5D, Tue.A.5L
Flinth, Axel Wed.B.5F
Fontes, Fernando ACC Thu.B.5J, Thu.B.5J
Forsgren, Anders Thu.C.1A
Foucart, Simon Tue.A.1A
Frandi, Emanuele Mon.A.1A
Freund, Robert M Thu.B.m3S, Thu.B.m3S
Friberg, Henrik Alsing Tue.A.5I
Friedlander, Michael Wed.B.5J
Fukuda, Ellen Hidemi Mon.B.5I
Fukuda, Mituhiro Mon.B.5I
Furini, Fabio Poster.P4
G
Gaivoronski, Alexei A Thu.C.5H, Thu.C.5H
Gangl, Peter Wed.B.1B
Gao, Jianjun Tue.D.5H
Gao, Yan Mon.B.4A
García-Palomares, Ubaldo M Tue.B.1C
Garg, Jugal Wed.A.5F, Wed.A.5F
Geffken, Sören Tue.C.5G
Gerth, Daniel Tue.A.5F
Ghaddar, Bissan Wed.C.1S
Ghanbari, Hiva Tue.C.1C
Gilbert, Francois Tue.C.4A, Tue.C.4A
Glineur, Francois Tue.A.5L
Goetschel, Sebastian Mon.A.4B
Góez, Julio César Mon.B.5K, Mon.B.5K
Gomez, Jose Alberto Tue.A.4A, Tue.A.4A
Gondzio, Jacek Thu.B.1A
Gotoh, Jun-ya Wed.A.5H
Gower, Robert Mansel Wed.C.5F
Goyal, Vineet Tue.B.5E, Tue.B.5E
Grapiglia, Geovani Nunes Tue.B.1C
Gratton, Serge Wed.A.1C, Wed.A.1C
Griffin, Joshua D Tue.D.1S, Tue.C.1S
Grigas, Paul Tue.D.5E
Grigorieva, Ellina V Thu.B.4A
Groetzner, Patrick Thu.A.5K
Gronski, Jessica Thu.B.m3AB
Gu, Ran Wed.A.1A
Guan, Xiucui Wed.B.1A
Guan, Yongpei Tue.B.5H, Tue.C.5E
Guillaume, Garrigos Tue.C.5J
Gunluk, Oktay Wed.A.1S, Wed.A.1S
Günther, Christian Thu.A.5C
Gupta, Varun Wed.B.5E
Gupta, Vishal Tue.D.5E, Tue.D.5E
Gurbuzbalaban, Mert Wed.C.5L
Guzman, Cristobal Tue.D.5I
H
Haddock, Jamie Thu.A.5L
Haeser, Gabriel Wed.C.4B
Hager, William W Tue.D.1S
Hall, Georgina Wed.B.5L
Hallak, Nadav Wed.A.m3S
Hamel, Andreas H Mon.B.m3AB, Mon.B.m3AB
Han, Deren Wed.C.1A
Hansen, Thomas Dueholm Wed.B.5G, Wed.B.5G
Hao, Tianyu Wed.B.4B
Harada, Kouhei Mon.A.5D
Hare, Warren L Tue.D.1C, Tue.D.1C
Harikandeh, Reza Babanezhad Thu.B.1S
Hashimoto, Tomoaki Tue.D.1B
Haskell, William Benjamin Thu.C.5E
Hauser, Raphael Andreas Mon.A.m3S, Wed.B.5F
Hazan, Elad Mon.B.m3S
He, Niao Thu.B.1S
He, Simai Thu.A.5F
Herzog, Roland Tue.C.1B, Thu.A.1B
Hildebrand, Roland Tue.D.5I, Tue.D.5I
Hillmann, Marcus Thu.A.5C
Hinder, Oliver H Mon.B.1A, Mon.B.1A
Hintermüller, Michael Wed.C.1B
Hiraishi, Hidefumi Thu.A.5D
Hirano, Tatsuya Tue.C.4B
Hirata, Kenji Thu.A.4A
Hirose, Kei Mon.A.4A
Hon, Benny Mon.A.1B
Hong, I-Hsuan Poster.P14
Hong, Mingyi Wed.C.4A
Ho-Nguyen, Nam Wed.C.5L
Hori, Atsushi Poster.P13
Horn, Benjamin Manfred Thu.A.5A
Hoshiyama, Takako Thu.A.5A
Houska, Boris Thu.B.5E
Hsieh, Cho-Jui Wed.B.5J
Hu, Zhaolin Thu.B.5H
Huang, Mike Thu.C.4B
2016_iccopt.indb   115 2016/07/22   11:58:25
INDEX OF SPEAKERS, SESSION ORGANIZERS AND CHAIRS
ICCOPT 2016116
Huang, Rui Tue.B.4A
Hungerford, James T Mon.A.1A
I
Inoue, Koji Thu.C.4B
Ito, Kazufumi Mon.B.1B, Tue.A.1B
Ito, Masaru Tue.D.5K
Ito, Naoki Tue.D.5K
J
Jeon, Gyun Tue.A.4B
Jeyakumar, Jeya Wed.A.5L
Jiang, Bo Tue.C.1A, Wed.B.1S
Jiang, Ruiwei Tue.C.5E, Tue.C.5E
Jie, Sun Tue.D.4B
Jitpeera, Thanyarat Wed.C.4B
Jong, YiKuan Mon.B.5H
Jost, Felix Mon.B.4B
Josz, Cedric Thu.B.5L
Jun, Gyun Tue.A.4B
K
Kagawa, Ai Tue.B.5G
Kageyama, Masayuki Poster.P8
Kalbat, Abdulrahman Tue.D.4A
Kang, Dong Tue.D.1A
Kannan, Rohit Wed.A.m3AB
Karatas, Murat Mon.B.4B
Karmitsa, Napsu Poster.P3
Kasai, Hiroyuki Wed.A.5I
Kassay, Gábor Thu.C.5C
Kawahara, Genta Wed.A.1B
Khusainov, Bulat Tue.A.4A
Kilinc-Karzan, Fatma Wed.C.5L, Wed.C.5L
Kim, Jang Ho Wed.A.5H
Kim, Sunyoung Wed.C.5K, Wed.C.5K
Kim, Woo Chang Tue.B.4B, Tue.B.4B
Kimmerle, Sven-Joachim Tue.C.1B
Kimura, Masato Tue.D.1B
Kirches, Christian Tue.C.5G
Kirst, Peter Thu.C.5J, Thu.C.5J
Kishimoto, Shogo Thu.A.4B
Knauer, Matthias Thu.C.4A
Knossalla, Martin Thu.C.5K
Knyazev, Andrew Thu.C.4B
Kobayashi, Kazuhiro Thu.B.5G, Thu.B.5G
Koch, Patrick Thu.B.1C
Kocvara, Michal Wed.C.5F
Kojima, Masakazu Wed.C.5K, Wed.C.5K
Kolvenbach, Philip Thu.C.1B
Kosaki, Toshihiro Thu.A.5G
Kouri, Drew Philip Thu.B.1B, Thu.C.1B,
 Thu.C.1B
Krislock, Nathan Mon.B.5K
Krokhmal, Pavlo Thu.A.5A, Thu.A.5A
Kuhn, Daniel Thu.A.5E
Kum, Sangho Mon.B.5D
Kuroiwa, Daishi Tue.D.5A, Tue.D.5A
Kuryatnikova, Olga Thu.B.5L
Kuttich, Anja Thu.A.5J
Kuwano, Issei Wed.C.5A
Kvasov, Dmitri E Tue.B.1C, Tue.B.1C
Kwon, Do-gyun Tue.A.4B
L
Lacoste-Julien, Simon Thu.B.m3S
Laird, Carl Tue.B.5G
Lam, Henry Wed.B.5E
Lan, Guanghui Mon.B.m3S
Larson, Jeffrey Wed.C.1C, Wed.C.1C
Lass, Oliver Tue.B.1B
Lasserre, Jean B Mon.A.5L, Mon.B.5L,
 Wed.A.5L, Wed.B.5L,
 Wed.B.5L
Lavaei, Javad Tue.D.4A, Tue.D.4A
Le Digabel, Sébastien Mon.B.1C, Mon.B.1C
Lee, Gue Myung Tue.A.5A, Wed.A.5L
Lee, Soomin Tue.D.5G
Lee, Yin Tat Tue.A.5D, Tue.A.5D,
 Tue.B.5D, Tue.C.5D
Lee, Yongjae Tue.A.4B
Lee, Yu-Ching Poster.P11
Lessard, Laurent Tue.B.5L
Leyffer, Sven Tue.A.1B
Li, Duan Tue.D.5H, Tue.D.5H
Li, Guoyin Mon.B.5L
Li, Qingna Wed.B.1S
Li, Xiaobo Tue.A.5E
Li, Xudong Thu.C.m3S
Liberti, Leo S Wed.A.5D
Likasiri, Chulin Thu.B.5G
Lim, Andrew Wed.A.5H
Lim, Yongdo Mon.A.5D
Lin, Changle Mon.A.5H, Mon.A.5H
Lin, Hongzhou Tue.C.5A
Lin, Qihang Tue.D.5J, Thu.A.5F
Ling, Leevan Mon.A.1B
Liu, Andrew Lu Tue.C.4B
Liu, Hongying Thu.A.1A
Liu, Huikang Wed.A.5J
Liu, Minghui Tue.B.5I
Liu, Xin Wed.A.1A, Wed.B.1A,
 Wed.B.1S, Wed.C.1A,
 Thu.A.1A
Liu, Ya-Feng Tue.A.1A, Tue.B.1A,
 Tue.C.1A, Tue.C.1A,
 Tue.D.1A
Liu, Yikan Wed.A.1B
2016_iccopt.indb   116 2016/07/22   11:58:25
INDEX OF SPEAKERS, SESSION ORGANIZERS AND CHAIRS
ICCOPT 2016 117
Liu, Yina Wed.C.4B
Loehne, Andreas Tue.A.5H, Tue.A.5H
Lotz, Martin Wed.B.5F, Wed.C.5F,
 Thu.B.5F
Louca, Raphael Thu.B.5F
Lourenço, Bruno Figueira Mon.B.5D
Luc, Dinh The Thu.C.5C, Thu.C.5C
M
Ma, Jingtang Wed.B.5H
Ma, Shiqian Wed.C.5G, Thu.A.5F
Ma, Wing Kin Mon.B.5E
Madani, Ramtin Thu.A.5L
Maglasang, Renan S Thu.A.4B, Thu.A.4B
Magron, Victor Liev Wed.A.5L
Mairal, Julien Mon.B.m3S
Malitsky, Yura Wed.B.4B
Manoussakis, George Oreste Wed.A.5G
Mansour, Hassan Wed.B.4A
Marandi, Ahmadreza Tue.D.5L
Marcia, Roummel Mon.B.1A, Tue.D.1S
Marzouk, Youssef M Wed.B.1C, Wed.B.1C
Maulidin, Achmad Thu.B.5G
Mazumder, Rahul Wed.C.5E
Mehlitz, Patrick Tue.B.5A
Meinlschmidt, Hannes Thu.A.1B
Menickelly, Matt Wed.B.1C
Michta, Mariusz Wed.A.5I, Wed.A.5I
Miltenberger, Matthias Tue.D.5G
Misener, Ruth Wed.A.m3AB, Wed.B.4A
Mishra, Shashi Kant Tue.A.5H, Wed.A.4B
Mittal, Areesh Mon.A.4A, Mon.A.4A
Miyata, Hiroyuki Thu.B.5D
Moazeni, Somayeh Mon.B.4A
Möllenhoff, Thomas Tue.A.5F
Morikuni, Keiichi Mon.A.5D
Moriyama, Sonoko Thu.B.5D
Morris, Walter Wed.B.5G
Motyl, Jerzy Wed.C.5C
Mouktonglang, Thanasak Mon.B.5D
Munoz, Gonzalo Mon.B.5L
Munson, Todd Wed.B.4B
Muramatsu, Masakazu Tue.A.5I, Tue.C.5I
N
Nakatsukasa, Yuji Wed.C.5F
Nakayama, Shummin Thu.C.1C
Namerikawa, Toru Thu.A.4A
Nannicini, Giacomo Tue.A.1C, Tue.D.5G
Naoum-Sawaya, Joe Mon.A.1A
Nara, Takaaki Mon.A.1B
Narayanan, Hariharan Tue.B.5D
Narushima, Daigo Wed.B.5K
Natarajan, Karthik Mon.B.5E, Tue.A.5E
Necoara, Ion Mon.A.5F, Mon.A.5F,
 Mon.B.5F
Nguyen, Van Thu.C.m3AB
Nicholson, Bethany Tue.A.5G
Nie, Jiawang Mon.A.5L, Mon.B.5L,
 Tue.D.5L, Wed.A.5L,
 Wed.B.5L
Nimana, Nimit Thu.B.5J
Niu, Yi-Shuai Wed.C.5J, Wed.C.5J
Nohadani, Omid Wed.A.5E, Wed.A.5E
Nor, Hassan Siti Thu.C.5J
Norton, Matthew David Tue.B.5H
Nutini, Julie Thu.C.1S
O
Ogata, Yuto Wed.C.5A
Ohtsuka, Takeshi Wed.A.1B
Ohtsuka, Toshiyuki Thu.A.4A, Thu.C.4B
Okajima, Yusuke Thu.A.4A
Okamoto, Yoshio Thu.B.5D
Okuno, Takayuki Tue.A.1A
Olivares-Nadal, Alba Victoria Wed.A.5H
Oliveira, Aurelio Thu.A.5G, Thu.A.5G
Orban, Dominique Thu.B.1A, Thu.B.1A,
 Thu.C.1A
Ostroy, Joseph M Wed.A.5F
P
Pachocki, Jakub Tue.A.5D
Pang, CH Jeffrey Tue.D.5K, Tue.D.5K
Papini, Alessandra Tue.D.1C
Parpas, Panos Mon.A.5L
Parrilo, Pablo A Tue.A.5L, Tue.B.5L,
 Tue.C.5L, Tue.C.5L
Pasupathy, Raghu Tue.C.1C, Tue.C.1C
Pataki, Gabor Tue.A.5I, Tue.B.5I,
 Tue.C.5I
Paulavicius, Remigijus Wed.A.m3AB
Pauwels, Edouard Wed.A.m3S
Peipei, Tang Thu.A.5J
Permenter, Frank Noble Tue.C.5I
Petrot, Narin Wed.B.5A, Wed.B.5A
Peypouquet, Juan Tue.C.5J
Pitea, Ariana Tue.A.1B
Pokutta, Sebastian Thu.A.5D
Pong, Ting Kei Thu.A.5K, Thu.A.5K
Pope, Scott R Tue.C.1S
Porcelli, Margherita Tue.D.1C
Postek, Krzysztof Thu.B.5E
Povh, Janez Thu.B.5L
Q
Qu, Shaojian Thu.A.5H
Qu, Zheng Thu.C.1S
2016_iccopt.indb   117 2016/07/22   11:58:25
INDEX OF SPEAKERS, SESSION ORGANIZERS AND CHAIRS
ICCOPT 2016118
R
Raghunathan, Arvind U Wed.A.4A, Wed.A.4A
Rakhlin, Alexander Tue.A.5L
Ramirez, Hector Thu.B.5C
Rao, Anil V Tue.A.4A
Recht, Benjamin Tue.A.5L, Tue.B.5L,
 Tue.B.5L, Tue.C.5L
Resmerita, Elena Tue.A.5F, Tue.A.5F
Richtarik, Peter Tue.D.5J
Ridzal, Denis Thu.B.1B, Thu.B.1B,
 Thu.C.1B
Rinaldi, Francesco Mon.A.1A, Mon.A.1C,
 Mon.B.1C, Tue.A.1C,
 Tue.B.1C, Tue.C.1C,
 Tue.D.1C, Wed.A.1C,
 Wed.B.1C, Wed.C.1C,
 Wed.C.1C, Thu.A.1C,
 Thu.B.1C
Robinson, Daniel P Mon.A.1S, Mon.B.1S
Rocca, Matteo Tue.D.5A
Rodriguez, Jose Santiago Tue.B.5G
Romeijn, Edwin Wed.A.5E
Rosasco, Lorenzo Andrea Tue.C.5A
Ruiz, Daniel Thu.C.1A
Ryan, Christopher Thomas Wed.C.5G
S
Sabach, Shoham Wed.A.m3S, Wed.B.m3S,
 Wed.B.m3S
Safarina, Sena Mon.A.5I
Sagara, Nobusumi Thu.C.5H
Saito, Yutaka Wed.C.5A
Sakaue, Shinsaku Wed.C.5K
Salmon, Joseph Thu.B.m3AB
Samadi, Mohammadreza Tue.A.1S
Santos, Sandra Augusta Tue.B.1S
Sartenaer, Annick Thu.B.1A, Thu.C.1A
Saunders, Michael Thu.B.1A
Schäfer, Renke Tue.C.5G
Schaynová, Lucie Thu.A.5G
Scheinberg, Katya Mon.B.1S
Schiela, Anton Mon.A.4B, Mon.A.4B
Schmitt, Johann Michael Mon.B.1B
Schneider, Reinhold Tue.B.1B
Schork, Lukas Wed.B.5D
Schrage, Carola Mon.B.m3AB
Schwartz, Alexandra Mon.A.5C, Tue.C.4B
Scutari, Gesualdo Wed.C.4A, Wed.C.4A
Sehrt, Cedric Tue.C.1B
Seto, Kazuki Tue.D.5A
Shanbhag, Uday Wed.B.4B, Wed.C.4B
Shen, Siqian Tue.C.5E
Shen, Yuan Tue.A.5J
Shi, Yun Tue.C.1A
Shikhman, Vladimir Wed.A.5F
Shiono, Naoshi Thu.B.4B
Shoemaker, Christine Annette Mon.A.1C, Mon.A.1C
Sicilia-Rodriguez, Joaguin  Poster.P10
Sidford, Aaron Daniel Tue.A.5D, Tue.A.5D, 
 Tue.B.5D, Tue.C.5D
Siebenborn, Martin Mon.A.4B
Siirola, John D Tue.A.5G, Tue.A.5G
Silva, Paulo JS Thu.C.1C, Thu.C.1C
Sim, Melvyn Mon.A.5E, Tue.B.5E
Slavakis, Konstantinos Wed.C.4A
So, Anthony Man-Cho Mon.B.5E, Wed.A.5J
Sojoudi, Somayeh Thu.B.5F
Soltanolkotabi, Mahdi Tue.D.5D
Soualmi, Nacer Eddine Thu.A.1C
Spürkel, Kai Arne Wed.A.5I
Srebro, Nathan Tue.B.5L
Steffensen, Sonja Mon.A.5C
Stich, Sebastian Thu.A.1C
Stötzner, Ailyn Tue.C.1B
Straszak, Damian Tue.C.5D
Strub, Moris Simon Tue.D.5H
Strugariu, Radu Thu.C.5C
Sukegawa, Noriyoshi Wed.A.5G
Sukprasert, Pakeeta Thu.C.5J
Sun, Andy Wed.A.1S
Sun, Cong Wed.A.1A, Wed.B.1S
Sun, Defeng Thu.B.m3S
Sun, Hailin Thu.A.5H, Thu.A.5H,
 Thu.B.5H
Surowiec, Thomas M Thu.B.1B
Susu, Livia Tue.A.1B
Suzuki, Satoshi Tue.A.5A
Szedlák, May Krisztina Thu.B.5D
T
Takac, Martin Mon.B.5H, Thu.B.1S,
 Thu.B.1S, Thu.C.1S
Takács, Petra Renáta Thu.C.5D
Takemura, Kei Mon.A.5I
Takeuchi, Tomoya Mon.A.1B, Tue.D.1B,
 Wed.A.1B
Tamura, Ryuta Wed.C.5J
Tanaka, Akihiro Mon.B.5I
Tanaka, Mirai Wed.B.5K
Tanaka, Tamaki Wed.C.5A
Tappenden, Rachael Thu.C.1S
Taylor, Adrien B Mon.A.5F
Teboulle, Marc Wed.A.m3S, Wed.A.m3S,
 Wed.B.m3S
Terlaky, Tamás Mon.B.5K, Wed.A.5D,
 Wed.B.5D
Theis, Dirk Oliver Thu.B.4B, Thu.B.4B
Tian, WenYi Tue.B.5J, Tue.B.5J
2016_iccopt.indb   118 2016/07/22   11:58:26
INDEX OF SPEAKERS, SESSION ORGANIZERS AND CHAIRS
ICCOPT 2016 119
Toh, Kim-Chuan Thu.C.m3S, Thu.C.m3S
Toint, Philippe Tue.A.1S, Tue.A.1S,
 Tue.B.1S
Tomasgard, Asgeir Mon.B.4A
Tono, Katsuya Mon.A.m3S
Tran, Nghia TA Thu.B.5C
Tran-Dinh, Quoc Mon.A.5F, Mon.B.5F,
 Mon.B.5F
Trautmann, Norbert Mon.B.5H
Traversi, Emiliano Poster.P5
Tröltzsch, Fredi Wed.B.1B, Wed.B.1B
Tsuchiya, Takashi Mon.A.5D, Tue.B.5I
U
Udell, Madeleine Wed.B.5J
Uhler, Caroline Mon.A.m3S
Ulbrich, Michael Mon.B.1B, Tue.A.1B,
 Tue.B.1B, Tue.B.1B
Ulbrich, Stefan Wed.C.1B, Wed.C.1B,
 Thu.A.1B
Upadhyay, Balendu Bhooshan Wed.A.4B
Uribe, Cesar A Mon.B.5F
Uschmajew, André Thu.C.5K, Thu.C.5K
V
van Bloemen Waanders, Bart Thu.B.1B, Thu.B.1B,
 Thu.C.1B
Van Parys, Bart Wed.C.5E, Wed.C.5E
Varvitsiotis, Antonios Thu.A.5J, Thu.A.5J
Vayanos, Phebe Tue.B.5E
Vempala, Santosh S Tue.B.5D
Vera, Jorge R Thu.C.5H
Vera, Juan C Thu.A.5L
Vidal, Rene Wed.B.5J
Villa, Silvia Tue.C.5A, Tue.C.5A
Vu, Bang Cong Thu.B.m3AB
Vuffray, Marc D Tue.D.4A
W
Waechter, Andreas Mon.A.1S
Wang, Chengjing Thu.C.5K
Wang, Di Tue.C.5D
Wang, Frank Mon.A.5H
Wang, Hao Mon.B.1S
Wang, Meihua Pster.P6
Wang, Mengdi Tue.D.4B, Wed.A.4B
Wang, Qiyu Thu.B.5H
Wang, Shuming Mon.A.5E
Wang, Yanfei Wed.A.1A, Wed.A.1A,
 Wed.B.1A, Wed.C.1A,
 Thu.A.1A
Wang, Ye Tue.B.5H
Wangkeeree, Rabian Wed.B.5A, Wed.B.5A
Ward, Rachel Tue.D.5D
Watanabe, Yu Mon.B.1A
Watson, Jean-Paul Tue.B.5G
Weber, Thomas A Thu.B.4A
Wei, Ermin Wed.A.4A
Wei, Ke Wed.B.5F
Weissing, Benjamin Tue.A.5H
Wen, Bo Wed.B.1A
Wessing, Simon Mon.B.1C
Wiesemann, Wolfram Thu.A.5E, Thu.A.5E
Wild, Stefan M Tue.A.1C, Tue.A.1C
Wollner, Winnifried Thu.A.1B
Won, Chong Hyun Tue.B.4B
Wong, Elizabeth Tue.D.1S
Wong, Sam Wed.C.5G
Wright, John Tue.D.5D
Wu, Tingting Wed.C.1A
Wunderling, Roland Tue.D.5G
X
Xia, Yong Thu.A.1A
Xia, Yu Mon.A.5I, Thu.A.5K
Xiao, Lin Tue.D.5J
Xie, Yue Tue.D.4B
Xu, Feng Min Tue.B.1A
Xu, Huan Thu.C.5E, Thu.C.5E
Xu, Huifu Wed.C.5H
Xu, Lingling Wed.C.1A
Y
Yaji, Kentaro Tue.D.1B
Yamada, Shinji Thu.A.5K
Yamada, Syuuji Wed.B.m3AB
Yamanaka, Shota Tue.C.5I
Yamashita, Makoto Mon.A.5I, Mon.B.5I, 
 Mon.B.5I
Yan, Gu Thu.A.4B
Yan, Zhenzhen Tue.A.5E
Yang, Fan Poster.P2
Yang, Lei Wed.C.5D
Yang, Tung-Sheng Poster.P15
Yang, Yufei Wed.C.1S
Yanıkoğlu, Ihsan Thu.B.5E, Thu.B.5E
Yasuda, Muneki Mon.A.4A
Ye, Yinyu Mon.B.1A
Yildirim, E Alper Thu.C.m3AB
Yin, Peng-Yeng Poster.P1
Yin, Wotao Wed.C.5D, Thu.A.1S
Yiu, Cedric Wed.B.5H, Wed.B.5H
Yoshise, Akiko Wed.B.5K, Wed.B.5K
Yousept, Irwin Wed.B.1B, Wed.B.1B
Yu, Haodong Poster.P7
Yuan, Xiaoming Tue.A.5J, Tue.B.5J
Yue, Man-Chung Mon.B.5E
Yun, Sangwoon Tue.A.5A
2016_iccopt.indb   119 2016/07/22   11:58:26
INDEX OF SPEAKERS, SESSION ORGANIZERS AND CHAIRS
ICCOPT 2016120
Yurtsever, Alp Mon.A.5F
Z
Zeile, Clemens Thu.C.4A
Zemkoho, Alain Tue.B.5A, Tue.B.5A
Zeng, Jinshan Wed.C.5D
Zhang, Dali Thu.A.5H, Thu.A.5H, 
 Thu.B.5H
Zhang, Shuzhong Thu.A.5F
Zhang, Wenxing Tue.A.5J
Zhang, Xinzhen Mon.A.5L
Zhang, Zaikun Mon.A.1C, Mon.B.1C,
 Tue.A.1C, Tue.B.1C,
 Tue.B.1S, Tue.C.1C,
 Tue.D.1C, Wed.A.1C,
 Wed.B.1C, Wed.C.1C,
 Thu.A.1C, Thu.B.1C
Zhen, Jianzhe Mon.A.5E
Zheng, Ning Thu.B.5J
Zhou, Enlu Tue.C.1C
Zhou, Wenwen Tue.C.1S, Tue.C.1S
Zhou, Zirui Wed.A.5J
Zhu, Shushang Tue.C.5H
Zuluaga, Luis F Thu.A.5L, Thu.B.5L,
 Thu.C.m3AB, Thu.C.m3AB
2016_iccopt.indb   120 2016/07/22   11:58:26
 
 
 
 
 
 
 
 
The two closest stations to GRIPS are Nogizaka and Roppongi. Just follow the dotted lines in the 
map above. You can also reach the Main Gate of GRIPS through the National Art Center, Tokyo 
(NACT) from Nogizaka. In that case, use the exit 6. (This route is not available on August 9th, 
Tuesday when NACT is closed.) 
Important Notice:  
1. The Main Entrance of GRIPS is closed on August 7th, Sunday. Participants are asked to use 
South Gate and South Entrance. 
2. Some sessions are held on the third floor of NACT. The Main Gate of NACT is just next to the 
Main Gate of GRIPS. So it is very easy to get there. (It takes just one minute or even less!)  
 
GRIPS, the National Art Center, Tokyo, and surrounding area 
ICCOPT 2016 121
MAPS
2016_iccopt.indb   121 2016/07/22   11:58:26
3rd and 1st floors of GRIPS  
 
  
 
1st floor 
 
(Foyer) 
Registration Desk 
Cafeteria 
(Coffee Breaks) 
South 
Enterance 
(To the 1st floor  
through the 2nd floor.) 
(“Cafeteria” in GRIPS is just a dining 
place. No food or drinks are served there.) 
Vending Machines 
Soukairou Hall  
(To the 3rd floor.) 
(To the 4th floor.) (To the 1st floor) 
Main 
Entrance 
3rd floor 
 
(To the 3rd floor.) 
♿
 
5th and 4th floors of GRIPS 
5th floor  
企業展示 企業展示 企業展示   
 
 
  
  
５
  
 
 
 
企業展示 企業展示 企業展示
 
 ５
 
５
 
 
５
 
５
 
 
５
 
５
 
 
５
 
５
 
５ ５
 
 
  
 
 
4th floor 
 
 
 
４Ａ
 
 
(Coffee Breaks) 
 
 
 
 
     
 
４Ｂ
♿
５Ｉ  
 
♿
ICCOPT 2016122
MAPS
2016_iccopt.indb   122 2016/07/22   11:58:26
3rd and 1st floors of GRIPS  
 
  
 
1st floor 
 
(Foyer) 
Registration Desk 
Cafeteria 
(Coffee Breaks) 
South 
Enterance 
(To the 1st floor  
through the 2nd floor.) 
(“Cafeteria” in GRIPS is just a dining 
place. No food or drinks are served there.) 
Vending Machines 
Soukairou Hall  
(To the 3rd floor.) 
(To the 4th floor.) (To the 1st floor) 
Main 
Entrance 
3rd floor 
 
(To the 3rd floor.) 
♿
 
5th and 4th floors of GRIPS 
5th floor  
企業展示 企業展示 企業展示   
 
 
  
  
５
  
 
 
 
企業展示 企業展示 企業展示
 
 ５
 
５
 
 
５
 
５
 
 
５
 
５
 
 
５
 
５
 
５ ５
 
 
  
 
 
4th floor 
 
 
 
４Ａ
 
 
(Coffee Breaks) 
 
 
 
 
     
 
４Ｂ
♿
５Ｉ  
 
♿
ICCOPT 2016 123
MAPS
2016_iccopt.indb   123 2016/07/22   11:58:27
 
 
 
 
 
 
From Narita/Haneda Airport to GRIPS 
1. Narita Airport = (Narita Express (70 min)) = Tokyo = (Marunouchi line (7 min)) = 
Kasumigaseki = (Hibiya line (5 min)) = Roppongi = (walk (10 min)) = GRIPS (Required 
Time: 2 hours, Fair: approx. 3000 yen) 
2. Narita Airport = (Keisei Skyliner (36 min)) = Nippori = (Yamanote Line (2 min)) = 
Nishi-Nippori = (Chiyoda Line (21 min)) = Nogizaka = (walk (10 min)) = GRIPS (Required 
Time: 1 hour 30 min, Fair: approx. 2500 yen)) 
3. Haneda Airport = (Tokyo Monorail (25 min)) = Hamamatsucho/Daimon = (Oedo line (10 
min)) = Roppongi = (walk (10 min)) = GRIPS (Required Time: 1 hour, Fair: approx. 500 yen) 
How to Get to the Session Rooms at the National Art Center, Tokyo (NACT): The Main 
Gate of NACT is just next to the Main Gate of GRIPS. So it is very easy to get to NACT. (It 
takes just one minute!). From the Main Gate of NACT, you can see the Main Entrance of NACT 
just a few dozen meters ahead. Enter through the Main Entrance and you will find an escalator 
in front which goes up to the 2nd floor. Get on this escalator, and go up to the 3rd floor (You 
need to change escalator once more on the 2nd floor.) In total, it should be around 4 minutes to 
reach the 3rd floor of NACT from the Main Entrance of GRIPS. 
Access to GRIPS and NYC from Narita/Haneda Airport 3rd floor of the National Art Center, Tokyo 
 
 
 
 
Access to GRIPS and NYC 
 from Narita/Haneda Airport  
Also check out http://www.iccopt2016.tokyo/travel/index.html for tips and additional information on 
transportation. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
How to Get to the Session Rooms at the National Art Center, Tokyo (NACT)  
The Main Gate of NACT is just next to the Main Gate of GRIPS. So it is very 
easy to get to NACT. (It takes just one minute!). From the Main Gate of NACT, 
you can see the Main Entrance of NACT just a few dozen meters ahead. Enter 
through the Main Entrance and you will find an escalator in front which goes up 
to the 2nd floor. Get on this escalator, and go up to the 3rd floor. (You need to 
change escalator once more on the 2nd floor.) In total, it should be around 4 
minutes to reach the 3rd floor of NACT from the Main Entrance of GRIPS.
♿
♿
ｍ３Ｓ
ｍ３ＡＢ
ICCOPT 2016124
MAPS
2016_iccopt.indb   124 2016/07/22   11:58:27
 
 
 
 
 
 
From Narita/Haneda Airport to GRIPS 
1. Narita Airport = (Narita Express (70 min)) = Tokyo = (Marunouchi line (7 min)) = 
Kasumigaseki = (Hibiya line (5 min)) = Roppongi = (walk (10 min)) = GRIPS (Required 
Time: 2 hours, Fair: approx. 3000 yen) 
2. Narita Airport = (Keisei Skyliner (36 min)) = Nippori = (Yamanote Line (2 min)) = 
Nishi-Nippori = (Chiyoda Line (21 min)) = Nogizaka = (walk (10 min)) = GRIPS (Required 
Time: 1 hour 30 min, Fair: approx. 2500 yen)) 
3. Haneda Airport = (Tokyo Monorail (25 min)) = Hamamatsucho/Daimon = (Oedo line (10 
min)) = Roppongi = (walk (10 min)) = GRIPS (Required Time: 1 hour, Fair: approx. 500 yen) 
How to Get to the Session Rooms at the National Art Center, Tokyo (NACT): The Main 
Gate of NACT is just next to the Main Gate of GRIPS. So it is very easy to get to NACT. (It 
takes just one minute!). From the Main Gate of NACT, you can see the Main Entrance of NACT 
just a few dozen meters ahead. Enter through the Main Entrance and you will find an escalator 
in front which goes up to the 2nd floor. Get on this escalator, and go up to the 3rd floor (You 
need to change escalator once more on the 2nd floor.) In total, it should be around 4 minutes to 
reach the 3rd floor of NACT from the Main Entrance of GRIPS. 
Access to GRIPS and NYC from Narita/Haneda Airport 3rd floor of the National Art Center, Tokyo 
 
 
 
 
Access to GRIPS and NYC 
 from Narita/Haneda Airport  
Also check out http://www.iccopt2016.tokyo/travel/index.html for tips and additional information on 
transportation. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
How to Get to the Session Rooms at the National Art Center, Tokyo (NACT)  
The Main Gate of NACT is just next to the Main Gate of GRIPS. So it is very 
easy to get to NACT. (It takes just one minute!). From the Main Gate of NACT, 
you can see the Main Entrance of NACT just a few dozen meters ahead. Enter 
through the Main Entrance and you will find an escalator in front which goes up 
to the 2nd floor. Get on this escalator, and go up to the 3rd floor. (You need to 
change escalator once more on the 2nd floor.) In total, it should be around 4 
minutes to reach the 3rd floor of NACT from the Main Entrance of GRIPS.
♿
♿
ｍ３Ｓ
ｍ３ＡＢ
ICCOPT 2016 125
MAPS
2016_iccopt.indb   125 2016/07/22   11:58:27
From Narita/Haneda Airport to NYC 
1. Narita Airport = (Narita Express (90 min)) = Shinjuku = (Odakyu line (3 min)) = Sangubashi = 
(walk (10 min)) = NYC (Required Time: 1 hour 45 min, Fair: approx. 3000 yen) 
2. Narita Airport = (Keisei Skyliner (36 min)) = Nippori = (Yamanote line (25 min)) = Shinjuku = 
(Odakyu line (3 min)) = Sangubashi = (walk (10 min)) = NYC (Required Time: 1 hour 30 min, 
Fair: approx. 2500 yen) 
3. Haneda Airport = (Keikyu line (25 min)) = Shinagawa = (Yamanote line (20 min)) = Shinjuku 
= (Odakyu line (3 min)) = Sangubashi = (walk (10 min)) = NYC (Required Time: 1 hour 15 min, 
Fair: approx. 500 yen) 
From NYC to GRIPS 
NYC = (walk (9 min)) = Yoyogi-koen = (Chiyoda line (6 min)) = Nogizaka = (walk (10 min)) = 
GRIPS (Required time: 25 min, Fair: 160 yen) 
The area around the National Olympics Memorial Youth Center (NYC) 
From Sangubashi Station to NYC              From Yoyogikoen Station to NYC 
 
ICCOPT 2016126
MAPS
2016_iccopt.indb   126 2016/07/22   11:58:27

