A Case for Redundant Arrays of Inexpensive Disks (RAID) 
Davtd A Patterson, Garth Gibson, and Randy H Katz 
Computer Saence D~v~smn 
Department of Elecmcal Engmeermg and Computer Sclencea 
571 Evans Hall 
Umversity of Cabforma 
Berkeley. CA 94720 
(partrsl@WF -kY du) 
Abstract Increasmg performance of CPUs and memorres wrll be 
squandered lf not matched by a sunrlm peformance ourease m II0 Whde 
the capactty of Smgle Large Expenstve D&T (SLED) has grown rapuily, 
the performance rmprovement of SLED has been modest Redundant 
Arrays of Inexpensive Disks (RAID), based on the magnetic duk 
technology developed for personal computers, offers an attractive 
alternattve IO SLED, promtang onprovements of an or&r of mogm&e m 
pctformance, rehabdlty, power consumption, and scalalnlrty Thu paper 
rntroducesfivc levels of RAIDS, grvmg rheu relative costlpetfotmance, and 
compares RAID to an IBM 3380 and a Fupisu Super Eagle 
1 Background: Rlsrng CPU and Memory Performance 
The users of computers are currently enJoymg unprecedented growth 
m the speed of computers Gordon Bell said that between 1974 and 1984. 
smgle chip computers improved m performance by 40% per year, about 
twice the rate of mmlcomputers [Bell 841 In the followmg year B111 Joy 
predicted an even faster growth [Joy 851 
Mamframe and supercomputer manufacturers, havmg &fficulty keeping 
pace with the rapId growth predicted by “Joy’s Law,” cope by offermg 
m&processors as theu top-of-the-lme product. 
But a fast CPU does not a fast system make Gene Amdahl related 
CPU speed to mam memory s12e usmg this rule [Siewmrek 821 
Each CPU mnstrucaon per second requues one byte of moan memory, 
If computer system costs are not to be dommated by the cost of memory, 
then Amdahl’s constant suggests that memory chip capacity should grow 
at the same rate Gordon Moore pr&cted that growth rate over 20 years 
fransuforslclup = 2y*-1%4 
AK predzted by Moore’s Law, RAMs have quadrupled m capacity every 
twotMoom75110threeyeaFIyers861 
Recently the rauo of megabytes of mam memory to MIPS ha9 been 
defti as ahha [Garcm 841. vvlth Amdahl’s constant meanmg alpha = 1 In 
parl because of the rapti drop of memory prices, mam memory we.9 have 
grownfastexthanCPUspeedsandmanymachmesare~ppedtoday~th 
alphas of 3 or tigha 
To mamtam the balance of costs m computer systems, secondary 
storage must match the advances m other parts of the system A key meas- 
Pemuswn to copy mthout fee all or w of &IS matcnal IS granted pronded that the COP!S 
zzrc not made or lstnbuted for dwct commernal advantage, the ACM copyright notIce 
and the tltk of the pubbcatuon and IW da’, appear, and notxe IS @“en that COPYI"K IS by 
pemtrs~on of the Association for Computing Machtnery To COPY otherwIse, or to 
repubbsh, requres B fee and/or spenfic perm~ss~o” 
0 1988 ACM 0-89791~268-3/88/~/OlOP $1 50 
ure of magneuc tik technology 1s the growth m the maxnnum number of 
bits that can be stored per square mch, or the bits per mch m a track 
umes the number of tracks per mch Called MA D , for maxunal area1 
density, the “Fmt Law m Disk Density” predicts ~rank87] 
MAD = lo(Year-1971)/10 
Magnettc dd technology has doubled capacity and halved pnce every three 
years, m hne with the growth rate of semiconductor memory, and m 
practice between 1967 and 1979 the dtsk capacity of the average IBM data 
processmg system more than kept up with its mam memory [Stevens81 ] 
Capacity IS not the o~rty memory charactensuc that must grow 
rapidly to mamtam system balance, since the speed with which 
msuuctions and data are delivered to a CPU also determmes its ulamdte 
perfarmanceThespeedof~mem~has~tpacefoPtworeasons 
(1) the mvenuon of caches, showmg that a small buff= can be managed 
automamzally to contain a substanttal fractmn of memory refaences. 
(2) and the SRAM technology, used to build caches, whose speed has 
lmpmvedattherateof4O%tolOO%peryear 
In umtmst to pnmary memory technologres, the performance of 
single large expensive ma8netuz d&s (SLED) has improved at a modest 
rate These mechamcal devu~ are dommated by the seek and the rotahon 
delays from 1971 to 1981, the raw seek tune for a high-end IBM disk 
improved by only a factor of two whllt the rocstlon hme did not 
cbange[Harkex811 Greater denslty means a lugher transfer rate when the 
mformatmn 1s found. and extra heads can educe the aveaage seek tnne, but 
the raw seek hme only unproved at a rate of 7% per year There 1s no 
reasontoexpectafasterratemthenearfuture 
To mamtam balance, computer systems have been usmg even larger 
mam memones or solid state d&s to buffer some of the I/O acttvlty 
This may be a fine solutron for apphcattons whose I/O actrvlty has 
locality of reference and for which volatlltty 1s not an issue. but 
appbcauons dommated by a high rate of random muests for small peces 
of data (such BS tmmact~on-pmcessmg) or by a low number of requests for 
massive amounts of data (such as large simulahons nmnmg on 
supercomputers) are facmg a sermus p&mnance hmuatmn 
2. The Pendrng I/O Crisw 
What t3 the Impact of lmprovmg the performance of sOme pieces of a 
problem while leavmg others the same? Amdahl’s answer IS now known 
asAmdahl'sLaw[Amdahl67] 
1 
S z 
(1-n +flk 
Whae 
S = the effecttve speedup, 
f=fractmnofworkmfastermode,and 
k = speedup whde m faster mode 
I/G 
Suppose that some current appbcatmns spend 10% of thev ume In 
Then when computers are 10X faster--accordmg to Bdl Joy m JUSt 
Over thtte years--then Amdahl’s Law predicts efQcove speedup wdl be only 
5X When we have computers lOOX faster--vm evolutmn of umprcuzessors 
or by multiprocessors-&s applrcatlon will be less than 10X faster, 
wastmg 90% of the potenhal speedup 
Whde we can lmagme improvements m software file systems via 
buffcrmg for near term 40 demands, we need mnovaUon to avoid an J./O 
crms [Boral83] 
3 A Solution: Arrays of Inexpensrve Disks 
RapId unprovements m capacity of large disks have not been the only 
target ofd& designers, smce personal computers have created a market for 
inexpensive magnetic disks These lower cost &sks have lower perfor- 
mance as well as less capacity Table I below compares the top-of-the-lme 
IBM 3380 model AK4 mamframe dtsk, FUJ~$U M2361A “Super Eagle” 
muucomputer disk, and the Conner Penpherals CP 3100 personal 
computer d& 
ChoroctensacS IBM FUJUSU Canners 3380 v 2361 v 
3380 M2361A CP3100 3100 31Go 
(>I mmrr 
3100 Is tt?tter) 
D&c dmmeter (mches) 14 105 35 4 3 
Formatted DaraCapaclty (MB) 7500 600 100 01 2 
Pr~ce/MB(controller mcl ) $18-$10 $20517 $lO-$7 l-25 17-3 
MlTFRated (hours) 30,oLw 20@030,ooo 1 15 
MlTF m pracUce (hours) 100,000 3 ? ?V 
No Actuators 4 1 1 2 1 
MaxmuunUO’$econd/ActuaU~ 50 40 30 6 8 
Typical I/O’s/second/Actuator JO 24 20 7 8 
-~wdsecond/box 200 40 30 2 8 
Typical VO’s/secondmox 120 24 20 2 8 
Transfer Rate (MB/set) 3 25 1 3 4 
Power/box (w) 6,600 640 10 660 64 
Volume (cu ft ) 24 34 03 800 110 
Table I Companson of IBM 3380 dtsk model AK4 for marnframe 
computers, the Fuptsu M2361A “Super Eagle” dtsk for rmnrcomputers, 
and the Conners Penpherals CP 3100 dtsk for personal computers By 
“‘MOxtmum Ilo’slsecond” we mean the rMxmtum number of average seeks 
and average rotates for a stngle sector access Cost and rehabthty 
rnfonnatzon on the 3380 comes from w&spread expertence [IBM 871 
[hvh2k87] O?kd the lnformatlon on the FuJltsu from the manual [Fu& 
871, whtle some numbers on the new CP3100 are based on speculatton 
The pnce per megabyte w gven as a range to allow for dflerent prices for 
volume &scount and d@rent mark-up practtces of the vendors (The 8 
watt maximum power of the CP3100 was rncreased to 10 watts to allow 
for the tne&xency of an external power supply. stnce rhe other drives 
contan their awn power supphes) 
One suqmsmg fact is that the number of I/Ck per second per Bctuator in an 
inexpensive &Sk is within a factor of two of the large d&s In several of 
the remammg metrics, mcludmg pnce per megabyte, the mexpenslve disk 
ts supenor or equal to the large Qsks 
The small size and low power are even more Impressive since dsks 
such as the CP31CO contam full track buffers and most funcUons of the 
traditional mainframe controller Small disk manufacturers can provide 
such funcUons m high volume dusks because of the efforts of standards 
comm~ttces m defmmg hrgher level penpheral mterfaces. such as the ANSI 
x3 131-1986 Small Computer System Interface (SCSI) Such standards 
have encouraged companies bke Adeptec to offer SCSI mterfaces as single 
chips, m turn allowing &Sk compames to embed mamfiame controller 
functrons at low cost Figure 1 compares the uadltlonal mamframe dsk 
approach and the small computer disk approach 7%~. sine SCSI mterface 
chip emLxd&d as a controller m every disk can also be uSed aS the dmXt 
memory access @MA) deuce at the other end of the SCSI bus 
Such charactensUcs lead to our proposal for buddmg I/O systems as 
-YS of mexpenslve d&s, either mterleaved for the large tninsfers of 
supercomputers [I(lm 86]@vny 871[Satem861 or mdependent for the many 
small mnsfen of transacUon processmg Usmg the mformamn m ‘fable 
I, 75 ~~xpensrve disks potentmlly have 12 hmcs the I/O bandwIdth of the 
IBM 3380 and the same capacity, with lower power COnSUmpUOn and Cost 
4 Caveats 
We cannot explore all issues associated with such -ys m the space 
avaIlable for this paper, so we ConCefltNte on fundamental estimates of 
price-performance and rehabduy Our reasoning IS that If there are no 
advantages m pnceperformance or temble d&vantages m rehabdlty, then 
there IS IIO need to explore further We chamctenze a transacUon-processing 
workload to evaluate performance of a col&Uon of iexpensive d&s. but 
remember that such a CollecUon is Just one hardware component of a 
complete tranacUon-processmg system While deslgnmg a complete TPS 
based on these ideas 1s enUcmg, we will resst that temptaUon m this 
paper Cabling and packagmg, certamly an issue m the cost and rehablhty 
of an array of many mexpenslve d&s, IS also beyond this paper’s scope 
Mainframe Small Computer 
CPU LJ 
0% Memoly Channel 
. . . 
. . . 
CPU 
a 
dm 
Figure 1 Comparison of organizations for typlca/ mat&me and small 
compter ahk tnterfaces Stngle chrp SCSI tnte@ces such as the Adaptec 
MC-6250 allow the small computer to ure a single crUp to be the DMA 
tnterface as well as pronde an embedded controllerfor each dtsk [Adeptec 
871 (The pnce per megabyte an Table I mcludes evetythtng zn the shaded 
box.?sabovc) 
5. And Now The Bad News: Reliabihty 
The unrehabd~ty of d&s forces computer systems managers to make 
backup versions of mformaUon quite frequently m case of fmlure What 
would be the impact on relmbdlty of havmg a hundredfold Increase m 
disks? Assummg a constant fmlure rate--that is. an exponenhally 
dlsmbuted Ume to fadure--and that failures are Independent--both 
assumptmns made by dtsk manufacturers when cakulaUng the Mean Time 
To Fadure O--the zebablhty of an array of d&s IS 
MITF ofa slngtc &sk 
MTI’F of a Drsk Array = 
Number MDuks m the Array 
Using the mformatron m Table I. the MTTF of 100 CP 3100 d&s 1s 
30,000/100 = 300 hours, or less than 2 weeks Compared to the 30,ooO 
hour (> 3 years) MTTF of the IBM 3380, this IS &smal If we consider 
scaling the army to 1000 disks, lhen the MTTF IS 30 hours or about one 
day, reqmrmg an ad~ecIne. worse rhan dismal 
Without fault tolerance, large arrays of mexpenstve Qsks are too 
unrehable to be useful 
6. A Better Solution’ RAID 
To overcome the rebabtbty challenge, we must make use of extra 
d&s contammg redundant mformaUon to recover the ongmai mformatmn 
when a &Sk fads Our acronym for these Redundant Arrays of Inexpensn’e 
Disks IS RAID To sunplify the explanaUon of our final proposal and to 
avold confusmn wnh previous work, we give a taxonomy of five different 
orgamzaUons of dtsk arrays, begmnmg with murored disks and progressmg 
through a variety of ahemaUves with &ffenng performance and rehablhty 
We refer to each orgamzauon as a RAID level 
The reader should be forewarned that we describe all levels as If 
implemented m hardware solely to slmphfy the presentation, for RAID 
Ideas are apphcable to software implementauons as well as hardware 
Reltabthty Our baste approach will be to break the arrays into 
rellabrhty groups, with each group having extra “check” disks contammg 
redundant mformauon When a disk fads we assume that withm a short 
time the failed disk ~111 be replaced and the mformauon wdl be 
110 
recon~ acted on to the new dlbk usmg the redundant mformauon Th1.s 
time IS Ldled the mean time to repair (MlTR) The MTTR can be reduced 
If the system includes extra d&s to act as “hot” standby spares, when a 
disk fmls, a replacement disk IS swltched m elecrromcally Penodlcally a 
human operator replaces all faded d&s Here are other terms that we use 
D = total number of d&s with data (not mcludmg extra check d&s). 
G = number of data d&s m a group (not mcludmg extra check d&s), 
C = number of check d&s m a group, 
nG =D/G=nUmberOfgoUp& 
As menhoned above we make the same assumptions that disk 
manufacturers make--that fadura are exponenual and mdependent (An 
earthquake or power surge IS a sltuatlon where an array of d&s might not 
foul Independently ) Since these reliability prticuons wdl be very high, 
we want to emphasize that the rehabdlty IS only of the the &sk-head 
assemblies with this fmlure model, and not the whole software and 
electromc system In ad&non, m our view the pace of technology means 
extremely lugh WF are “overlull”--for, independent of expected bfeume, 
users will replace obsolete &sks After all, how many people are stdl 
using 20 year old d&s? 
The general MT’TF calculation for single-error repamng RAID 1s 
given III two steps Fmt, the group MTIF IS 
mFDtsk I 
MrrF,,,, = * 
G+C Probabdrty ofanotherfadure m a group 
b&re repamng the dead oisk 
As more formally denved m the appendix, the probabdlty of a second 
fa&nebeforethefirsthasbeenrepauedIs 
MlTR hill-R 
Probabdrty of = E 
Another Failure bfnF,,,,k /(No DIS~T- 1) MmF/j,k /(w-l) 
The mtmuon behmd the formal calculation m-the appendix comes 
from trymg to calculate the average number of second d& fdures durmg 
the repau time for X single &Sk fadures Since we assume that Qsk fadures 
occur at a umform rate, tha average number of second fa&ues durmg the 
rcpau tune for X first fadures 1s 
X *MlTR 
MlTF of remamtng d&s u) the group 
The average number of second fathues for a smgle d&z 1s then 
MlTR 
bfnFD,& / No Of W?UlUllIl~ drSkS l?l the group 
The MTTF of the retnaming disks IS Just the MTI’F of a smgle disk 
dnwkd by the number of go4 disks m the gmup. gwmg the result above 
The second step IS the reltablhty of the whole system, which IS 
approxl~~~teiy (smcc MITFGrow 1s not qmte titnbuted exponentrally) 
MTrFGrarp 
MTTFRAID = 
Pi 
Pluggmg It all together, we get. 
mFD,sk mFD,sk 1 
MITFRAID = - * *- 
G+C (G+C-l)*MITR “c 
(MmFDtsk)2 
= (G+C)*tlG * (G+C-l)*MITR 
Smce the formula 1s tbe same for each level, we make the abstract 
numbers concrete usmg these parameters as appropriate D=loO total data 
d&s, G=lO data disks per group, M7VDcsk = 30,000 hours, MmR = 1 
hour, with the check d&s per group C detennmed by the RAID level 
Relubrlrty Overhead Cost This IS stmply the extra check 
disks. expressed as a percentage of the number of data &sks D As we shall 
see below, the cost vanes WIUI RAID level fmm 100% down to 4% 
Useable Storage Capacity Percentage Another way to 
express this rellabdlty overhead 1s m terms of the percentage of the total 
capacity of data &sks and check disks that can be used to store data 
Depending on the orgamauon, this vanes from a low of 50% to a high of 
96% 
Performance Smce supercomputer applications and 
transaction-processing systems have &fferent access patterns and rates, we 
need different metncs to evaluate both For supercomputers we count the 
number of reads and wnte.s per second for large blocks of data, with large 
defined as gettmg at least one sector from each data d& III a group Durmg 
large transfers all the disks m a group act as a stngle umt, each readmg or 
wntmg a pomon of the large data block m parallel 
A better measure for transacuon-processmg systems s the number of 
indlvrdual reads or writes per second Smce transacuon-processing 
systems (e g , deblts/cre&ts) use a read-modify-wnte sequence of disk 
accesses, we mclude that metnc as well Ideally durmg small transfers each 
dsk m a group can act mdepe&ndy. e~thez readmg or wntmg mdependent 
mfonnatmn In summary supercomputer applicauons need a hrgh dura rure 
whale transacuon-pmcessm g need a hrgh II0 rate 
For both the large and small transfer calculauons we assume the 
mlmmum user request IS a sector, that a sector 1s small relauve to a track, 
and that there 1s enough work to keep every devtce busy Thus sector size 
affects both dusk storage efficiency and transfer sue Figure 2 shows the 
uiealoperauonoflargeandsmall~accessesmaRAID 
(a) Stngle Large or “Graupcd” Read 
(lreadqwadoverGd&s) 
1tt 1 
q nl .*. 
(b) Several Smll or Indmdual Reads and Writes 
(GndsandlorwntcsqmndawrG&sks) 
Figure 2. Large tramfer vs small tran$ers WI a group of G d&s 
The SIX pelformauce memcs are then the number of reads, wntes, and 
read-mod@-writes per second for both large (grouped) or small (mdlvldual) 
transfers Rather than @ve absolute numbers for each memc, we calculate 
efficiency the number of events per second for a RAID relative to the 
corrcqondmg events per second for a smgle dusk (This ts Boral’s I/O 
bandwidth per ggabyte moral 831 scaled to glgabytes per disk ) In Uns 
pap we are after fundamental Mferences so we use ample. demmmlstlc 
throughput measures for our pezformance memc rather than latency 
Effective Performance Per Dnk The cost of d&s can be a 
large portmn of the cost of a database system, so the I/O performance per 
disk--factonng m the overhead of the check disks--suggests the 
cost/performance of a system ‘flus IS the bottom line for a RAID 
111 
I 
7. First Level RAID: Mwrored Disks 
Mmored dusks are 11 tradmonal approach for lmprovmg rellabdlty of 
magneuc disks This IS the most expensive opuon we consider since all 
tiks are duplicated (G=l and C=l). and eve.ry wnte to a data dusk 1s also a 
wnte to a check &Sk Tandem doubles the number of controllers for fault 
tolerance, allowing an opwnized version of mirrored d&s that lets reads 
occur m parallel Table II shows the memcs for a Level 1 RAID assummg 
this optnnuatton 
MTTF 
Total Number of D&s 
Ovcrhcad Cost 
Usecrble Storage Capacity 
Exceeds Useful Roduct Ltiwne 
(4500.000 hrs or > 500 years) 
2D 
100% 
50% 
Eventslscc vs Smgle Disk Full RAID E@caency Per Disk 
hrge (or Grouped) Readr ws 1 00/s 
Large (or Grouped) Wrues D/S 50/S 
Large (or Grouped) R-M-W 4Dl3S 67/S 
Small (or Indsvuiual) Rends W 100 
Small (or hd~vuiual) Writes D 50 
Small (or In&dual) R-M-W 4D/3 61 
Table II. Charactenstrcs of Level 1 RAID Here we assume that writes 
are not slowed by waztrng jar the second wrote to complete because the 
slowdown for writing 2 dtsks 1s mtnor compared to the slowdown S for 
wntrng a whole group of 10 lo 25 d&s Unltke a “pure” mtrrored scheme 
wtth extra &As that are mvlsrble to the s&ware, we assume an optmuted 
scheme with twice as many controllers allowtng parallel reads to all d&s, 
grvmg full disk bandwidth for large reads and allowtng the reads of 
rea&noaijj-nntes to occw in paralbzl 
When mdwldual accesses am dlsmbuted acmss muluple d&s, average 
queuemg. seek, and rotate delays may &ffer from the smgle Qsk case 
Although bandwidth may be unchanged, it is Qsmbuted more evenly, 
reducing vanance m queuemg delay and, If the disk load IS not too high, 
also reducmg the expected queuemg delay through parallebsm [Llvny 871 
When many arms seek to the same track then rotate to the described sector, 
the average seek and rotate time wdl be larger than the average for a smgle 
disk, tendmg toward the worst case tunes Tlus affect should not generally 
more than double the average access tlmc to a smgle sector whde stdl 
gettmg many sectors m parallel In the special case of rmrrored &sks with 
sufficient controllers, the choice between arms that can read any data sector 
will reduce the tune for the average read seek by up to 45% mltton 881 
To allow for these factors but to retam our fundamental emphasis we 
apply a slowdown factor, S, when there are more than two d&s m a 
group In general, 1 5 S < 2 whenever groups of disk work m parallel 
With synchronous disks the spindles of all disks m the group are 
synchronous so that the correspondmg sectors of a group of d&s pass 
under the heads stmultaneously,[Kmzwd 881 so for synchronous disks 
there IS no slowdown and S = 1 Smce a Level 1 RAID has only one data 
disk m its group, we assume that the large transfer reqmres the same 
number of Qsks actmg in concert 9s found m groups of the higher level 
RAIDS 10 to 25 d&s 
Dupllcatmg all msks can mean doubhng the cost of the database 
system or using only 50% of the disk storage capacity Such largess 
inspires the next levels of RAID 
8 Second Level RAID: Hammmg Code for ECC 
The history of main memory orgaruzauons uggests a way to reduce 
the cost of rehablhty With the introduction of 4K and 16K DRAMS, 
computer designers discovered that these new devices were SubJe.Ct to 
losing information due to alpha part&s Smce there were many single 
bit DRAMS m a system and smce they were usually accessed m groups of 
16 to 64 chips at a ume, system designers added redundant chips to correct 
single errors and to detect double errors m each group This increased the 
number of memory chips by 12% to 38%--depending on the size of the 
group--but 11 slgmdcantly improved rehabdlty 
As long as all the dam bits m a group are read or wntten together, 
there 1s no Impact on performance However, reads of less than the group 
size requue readmg the whole group to be sure the mformatir? IS correct, 
and writes to a poruon of the group mean three steps 
1) a read step to get all the rest ofthe data, 
2) a mad&v step to merge the new and old mformatwn. 
3) a write step to write the full group, tncludmg check lnformatwn 
Smce we have scores of d&s m a RAID and smce some accesses are 
to groups of d&s, we can mimic the DRAM solution by bit-mterleavmg 
the data across the disks of a group and then add enough check d&s to 
detect and correct a smgle error A smgle panty dusk can detect a smgle 
error, but to correct an erroI we need enough check dusks to ulentiy the 
disk with the error For a group sue of 10 data do& (G) we need 4 check 
d&s (C) m total, and d G = 25 then C = 5 [HammmgSO] To keep down 
the cost of redundancy, we assume the group size will vary from 10 to 25 
Since our mdwidual data transfer urn1 is Just a sector, bit- interleaved 
dsks mean that a large transfer for this RAID must be at least G sectors 
L&e DRAMS, reads to a smaller amount unpiles readmg a full “cctor from 
each of the bit-mterleaved disks m a group, and writes of a single unit 
involve the read-modify-wnte cycle to hll the Qsks Table III shows the 
metncs of this Level 2 RAID 
MlTF ExceedsUseful~ehme 
G=lO G=Z 
(494500 hrs (103500 llrs 
or >50 years) OT 12 years) 
Total Number of D&s 14OD 12OD 
overhud Cost 40% 20% 
Useable Storage Capacity 71% 83% 
EventslSec Full RAID Eficlency Per Ask Eflc~ncy Per Dtsk 
(vs Single Disk) L2 L2lLI L2 L2ILI 
hgeRe& D/S 111s 71% 86/S 86% 
Lurgc wrllcs D/S 71/s 143% 86/s 112% 
Large R-M-W D/S 71/s 107% 86/S 129% 
Small Reodr DISC 01/s 6% 03lS 3% 
Small Wrttes D12sG 04/S 6% o2.B 3% 
Small R-M-W DISC 071s 9% 03/S 4% 
Table III Charactenstlcs of a Level 2 RAID The L2lLI column gives 
the % performance of level 2 m terms of lewl 1 (>lOO% means L.2 IS 
faster) As long as the transfer taut ts large enough to spread over all the 
data d& of a group. the large IIOs get the full bandwuith of each &Sk, 
&w&d by S to allow all dtsks m a group to complete Level 1 large reads 
are fmler because &a IS duphcated and so the redwdoncy d&s can also do 
independent accesses Small 110s still reqture accessmg all the Isks tn a 
group. so only DIG small IIOc can happen at a tone, agam dwrded by S to 
allow a group of disks to jintsh Small Level 2 writes are hke small 
R-M-W becalcse full sectors must be read before new &ta can be written 
onto part of each sector 
For large wntes, the level 2 system has the same performance as level 
1 even though it uses fewer check disks, and so on a per disk basis It 
outperforms level 1 For small data transfers the performance 1s &smal 
either for the whole system or per disk, all the disks of a group must be 
accessed for a small transfer, llmltmg the Ipaxrmum number of 
simultaneous accesses to DIG We also include the slowdown factor S 
smce the access must wat for all the disks to complete 
Thus level 2 RAID IS desuable for supercomputers but mapproprmte 
for transaction processmg systems, with increasing group size. increasing 
the disparity m performance per disk for the two applications In 
recognition of this fact, Thrnkmg Machmes Incorporated announced a 
Level 2 RAID this year for its Connecuon Machme supercomputer called 
the “Data Vault,” with G = 32 and C = 8, mcludmg one hot standby spare 
[H&s 871 
Before improving small data transfers, we concentrate once more on 
lowenng the cost 
9 Thwd Level RAID: Single Check Disk Per Group 
Most check disks m the level 2 RAID are used to determme which 
disk faded, for only one redundant panty disk is needed to detect an error 
These extra disks are truly “redundant” smce most drsk controllers can 
already detect If a dusk faded either through special signals provided m the 
disk interface or the extra checking mformauon at the end of a sector used 
to detect and correct soft errors So mformatlon on the failed disk can be 
reconstructed by calculatmg the parity of the remaining good disks and 
then companng bit-by-bit to the panty calculated for the ongmal full 
112 
group When these two parmcs agree, the faded bu was a 0, othcrwtse it RAID levels 2,3, and 4 By stormg a whole transfer umt m a sector, reads 
was a 1 If the check drsk IS the fadure,Just read all the data drsks and store 
the group panty in the replacement drsk 
can be mdependent and operate at the maxrmum rate of a disk yet sull 
detect errors Thus the primary change between level 3 and 4 IS that WC 
Reducmg the check d&s toone per group (C=l) reduces the overhead 
cost to between 4% and 10% for the group stzes considered here The 
performance for the thud level RAID system is the same as the Level 2 
RAID, but the effectrve performance per dtsk mcreases mce it needs fewer 
check d&s This reductron m total d&s also increases relrabdtty, but 
since It is shll larger than the useful hfehme of disks, this IS a minor 
pomt One advantage of a level 2 system over level 3 is that the extra 
check mformatton assocrated with each sector to correct soft errors IS not 
needed, mcreasmg the capactty per dtsk by perhaps 10% Level 2 also 
allows all soft errors to be corrected “on the fly” wnhout havmg to reread a 
sector Table IV summarizes the thud level RAID charactensncs and 
Figure 3 compares the sector layout and check d&s for levels 2 and 3 
mterlcave data 
4 Tran$er 
UIlllS 
a, b, c & d 
Level 4 
Sector 0 
&la 
Disk 1 
MlTF Exceeds Useful Lrfenme Secwr 0 
Data 
Disk 2 
A 
a 
T 
2 A 
Total Number of D&s 
owrhcad cost 
Useable Storage Capacity 
EventslSec Full RAID 
(vs Single Disk) 
LargeRecu& D/S 
Large Writes D/S 
Large R-M-W D/S 
Small Readr DISC 
Small Vyrites D/2sG 
Small R-M-W DISC 
G=lO 
(820,000 hrs 
or >90 years) 
1 1OD 
10% 
91% 
EIficclency Per Disk 
L3 WIL2 WILl 
91/S 127% 91% 
91/S 121% 182% 
91/S 127% 136% 
09/S 127% 8% 
05/S 127% 8% 
09/S 127% 11% 
G=25 
(346,000 hrs 
or 40 years) 
104D 
4% 
96% 
Eflctency Per Disk 
w Lx2 WILI 
96/S 112% 96% 
96/S 112% 192% 
96/S 112% 142% 
041s 112% 3% 
02/S 112% 3% 
041s 112% 5% 
Table IV Characterrstrcs of a Level 3 RAID The L3lL2 column gives 
the % performance of L3 tn terms of L2 and the L3ILl column give; it in 
terms of LI (>loO% means L3 IS faster) The performance for the full 
systems IS the same m RAID levels 2 and 3, but since there are fewer 
check dtsks the performance per dnk tmproves 
Park and Balasubramaman proposed a thud level RAID system 
without suggestmg a partrcular applicauon park861 Our calculattons 
suggest tt 1s a much better match to supercomputer apphcatrons than to 
transacuon processing systems This year two disk manufacturers have 
announced level 3 RAIDS for such apphcanons usmg synchronized 5 25 
mch disks with G=4 and C=l one from IvIaxtor and one from Mtcropohs 
[Magmms 871 
This thud level has brought the rehabrhty overhead cost to its lowest 
level, so in the last two levels we Improve performance of small accesses 
w&out changmg cost or rehabrlny 
10. Fourth Level RAID Independent ReadsbVrltes 
Spreadmg a transfer across all &sks wuhm the group has the 
followmg advantage 
. Large or grouped transfer ttme IS reduced because transfer 
bandwulth of the entue array can be exploned 
But it has the followmg drsadvantagek as well 
. ReadmgAvnhng to a disk m a group requues readmg/wnhng to 
all the d&s m a group, levels 2 and 3 RAIDS can perform only 
one I/O at a Pme per group 
. If the disks are not synchromzed, you do not see average seek 
and rotattonal delays, the observed delays should move towards 
the worst case, hence the S factor m the equatrons above 
This fourth level RAID improves performance of small transfers through 
parallehsm--the abrhty to do more than one I/O per group at a ume We 
no longer spread the mdtvtdual transfer informanon across several &sks, 
but keep each mdrvrdual unit ma smgle disk 
The vutue of bit-mterleavmg 1s the easy calculatron of the Hammmg 
code needed to detect or correct errors in level 2 But recall that m the thud 
level RAID we rely on the drsk controller to detect errors wnhm a single 
drsk sector Hence, rf we store an mdrvrdual transfer umt in a single sector, 
we can detect errors on an mdtvtdual read without accessing any other drsk 
Frgure 3 shows the different ways the mformatron is stored in a sector for 
Sectar 0 
L&a 
Dtsk 3 
Sector 0 
D& 
Disk 4 
Sector 0 
Check 
Disk 5 
Sector 0 
Check 
Ask 6 
Sector 0 
Check 
Disk 7 
aEcc0 
bECC0 
CECCO 
dECC0 
aEcc1 
bECC1 
cECC1 
dEcc1 
aEcc2 
bECC2 
cECC2 
dECC2 
ECCa 
ECCb 
ECCc 
ECCd 
(Only one 
check &Sk 
tn level 3 
Check rnfo 
ts calculated 
aver each 
lran.$er 10~1 
(Each @tier 
umt 1s placed tnto 
a single sector 
Note that the check 
~$0 IS now calculated 
over a pece of each 
tran$er urut ) 
D 
I 
s 
L 
Frgure 3 Comparrson of locatton of data and check mformatlon In 
sectors for RAID levels 2, 3, and 4 for G=4 Not shown IS the small 
amount of check mformatton per sector added by the disk controller to 
detect and correct soft errors wlthm a sector Remember that we use 
physical sector numbers and hardware control to explain these ideas but 
RAID can be unplemented by sofmare ucmg logical sectors and disks 
At fust thought you mrght expect that an mdrvldual wnte to a smglz 
sector stdl mvolves all the disks m a group smce (1) the check disk mutt 
be rewritten wnh the new panty data, and (2) the rest of the data dash> 
must be read to be able to calculate the new panty data Recall that each 
panty bit IS Just a smgle exclusive OR of s+l the correspondmg data NIL 11 
a group In level 4 RAID, unhke level 3, the panty calculatron is ITXFI 
simpler since, if we know the old data value and the old parity balue al 
well as the new data value, we can calculate the new panty mforrmror: sr 
follows 
new panty = (old data xor new data ) xor old pantv 
In level 4 a small wnte then uses 2 dtsks to perform 4 accesses-2 rea& 
and 2 wrnes--whtle a small mad mvolves only one read on one disk Table 
V summarmes the fourth level RAID charactensucs Note that all small 
accesses improve--dramatrcally for the reads--but the small 
read-modrfy-wnte is strll so slow relatrve to a level 1 RAID that ns 
applrcabduy to transactron processmg is doubtful Recently Salem and 
Gama-Molma proposed a Level 4 system [Salem 86) 
Before proceedmg to the next level we need to explam the 
performance of small writes in Table V (and hence small 
read-modify-writes smce they entarl the same operatrons m dus RAID) 
The formula for the small wntes drvrdes D by 2 Instead of 4 becau*e 2 
113 
accesses can proceed m parallel the old data and old panty can be read at 
the same ume and the new data and new panty can be wntten at the same 
nme The performance of small writes IS also d~nded by G because the 
smgle check disk m a group must be read and wntten with every small 
wnte m that group, thereby hmmng the number of writes that can be 
performed at a time to the number of groups 
The check &sk 1s the bouleneck, and the fmal level RAID removes 
thus bottleneck 
MlTF Bxceeds Useful hfetune 
Total Number of D&s 
overhead cost 
Useabk Storage Capacy 
Events&x Full RAID 
(vs Smgk Dtsk) 
Large R& DIS 
Large Writes DIS 
Large R-M-W D/S 
SmallReads D 
Small Wrttes 
Small R-M-W 
G&O 6-25 
(820,ooo hrs (346,000 hrs 
or>90 years) or 40 years) 
11OD 104D 
10% 4% 
91% 96% 
Efitency Per Dtsk Eficwncy Per Dark 
LA L4lL3 L4ILl IL4 L4iL.3 L4lLl 
91/S 100% 91% 961.3 100% 96% 
91/S 100% 182% %/s 100% 192% 
91/s 100% 136% 96/S 100% 146% 
91 1200% 91% 96 3OCKI% 96% 
05 120% 9% 02 120% 4% 
09 120% 14% 04 120% 6% 
Table V. Charactenstrcs of a Level 4 RAID The L4lL3 columt~ gwes 
the % P&Wn0nCe @LA an terms of L3 and the L4lLl column gwes it in 
terms of Ll (>100% means L4 is faster) Small reads improve because 
they no longer trc up a whok group at a time Small writes and R-M-Ws 
improve some because we make the same assumpttons as we made tn 
Table II the slowdown for two related IIOs can be ignored because only 
two d&s are znvolved 
11. Fifth Level RAID: No Single Check Disk 
Whde level 4 RAlQ actieved parallelism for-reads. writes are shll 
limited to one per group smce evay wnte must read and wnte the check 
disk The final level RAID dtsmbutes the data and check mformahon 
across all the d&s--mcludmg the check dlsLs Figure 4 compares the 
locauon of check mformauon m the sectors of d&s for levels 4 and 5 
RAIDS 
The performance Impact of dus small change IS large smce. RAID 
level 5 can support mulnple m&vldual writes per mup For example, 
supposemF~gure4abovewewanttowntesectorOofdrsk2andsectorl 
of du& 3 As shown on the left Figure 4. m RAID level 4 these writes 
must be sequenti smce both sector 0 and sector 1 of disk 5 must be 
wntten However, as shown on the right,, m RAID level 5 the writes can 
proceed m parallel smce a wnte to sector 0 of &sk 2 still involves a wnte 
toQsk5butawntetosectorlofd&3mvolvesawntetodlsk4 
These changes bnng RAID level 5 near the best of both worlds small 
read-mtify-writes now perform close to the speed per d&c of a level 1 
RAID while keeping the large transfer performance per &Sk and high 
useful storage capacity percentage of the RAID levels 3 and 4 Spreadmg 
the data across all Qsks even improves the performance of small reads, 
smce there IS one more &Sk per group that contams data Table VI 
summanze.s the charactens~cs of dus RAID 
Keepmg m mmd the caveats given earher, a Level 5 RAID appears 
very attractlve If you want to do Just SUpfXCOIIIpUbZ apphcatlons, or JUSt 
transaction pmcessmg when storage capacity 1s lmuted. or If you want to 
do both supercomputer appbcanons and Iransacnon pmcessmg 
12. Dwusslon 
Before concludmg the paper, we wish to note a few more mterestmg 
pomts about RAIDs The fti 1s that whde the schemes for disk smpmg 
and panty support were presented as lfthey were done by hardware, there 1s 
no necessny to do so WeJust give the method, and the decmon between 
hardware and software soluuons IS smctly one of cost and benefit. For 
example, m cases where &Sk buffenng 1s effecave, there IS no extra d&s 
reads for level 5 small writes smce the old data and old panty would be m 
mam memory, so software would give the best performance as well as the 
least cost. 
In thus paper we have assumed the transfer umt IS a muluple of the 
sector As the size of the smallest transfer unit grows larger than one 
Check 
IDataD& Disk 
5 D&s 
(contamng Data and Checks) 
(al Check rnforrnarron for 
Level 4 RAID for G=4 and 
C=I The sectors are shown 
below the d&s (The 
checked arem u&ate the 
check mformatwn ) Wrues 
tosoofdtsk2andsl of 
aisk 3 unply writes to So 
and sl of dtsk 5 The 
check dtsk (5) becomes the 
write bottleneck 
II II cl 
II 
B 
(b) Check u@matwn for 
Level 5 RAID for G-4 and 
C=I The sectors are shown 
below the disks. wtth the 
check mJornmaon and &ta 
spreadevenly through all the 
disks Writes to So of& 2 
and sl of dtsk 3 sttll nnply 2 
wntes, but they can be split 
across 2 dtsh to So of dask 5 
and to sl of&Sk 4 
Figure 4 Localton of check informanon per sector for Level 4 RAID 
vs. Level 5 RAID 
MlTF Weeds Useful Lifetune 
G=lO 
(820.000 hrs 
ormyear@ 
Total Number of Disks tlOD 
OWhf?lkiCOSt 10% 
Useable Swmge Capacy 91% 
EventslSec Full RAID Efiuncy Per Disk 
fvs Single Dtsk) L5 LA!.4 LslLl 
L4UgeRmdr D/S 91/s 100% 91% 
Large Writes DIS 91/s 100% 182% 
Lurge R-M-W D/S 91/S 100% 136% 
Small Reads (1-D 100 110% 100% 
Small Writes (l+C/G)DI4 25 550% 50% 
Small R-M-W (l+C/G)&-2 50 550% 75% 
G=.Z 
&woo hrs 
or 40 years) 
104D 
4% 
96% 
Eficuncy Per Dtsk 
Ls LslL.4 L.5lLI 
96/S 100% 96% 
96/s 100% 192% 
96/s 100% 144% 
100 104% 100% 
25 1300% 50% 
so 1300% 75% 
Table VI Charactensttcs of a Level 5 RAID The W/L4 column gives 
the % performance of LT m terms of L4 and the LStLl column gwes w tn 
tenm c$Ll (>I0096 means L5 U farlcr) Because red can be spread over 
all drsks. mcludutg what were check d&s m level 4, all small 110s 
unprove by a factor of 1 +ClG Small writes and R-M-Ws unprove because 
they are no longer constratned by group size, getting the full dtsk 
bandwtdth for the 4 Ilo’s assonated with these accesses We agatn make 
the same assumpttons as we ti m Tables II and V the slowdown for 
two rela@d IIOs can be rgnored beeawe only two d&s are mvolved 
sector per drive--such as a full hack with an Vo protocol that suppats data 
returned out-of-order--then the performance of RAIDS improves 
sigmficantly because of the full track buffer m every disk For example, If 
every disk begms transfemng to ns buffer as soon as u reaches the next 
sector, then S may reduce to less than 1 since there would be vntually no 
rotauonal delay Wnh transfer wuts the size of a track, it IS not even clear 
If synchromzmg the disks m a group Improves RAID performance 
This paper makes two separable pomu the advantages of bmldmg 
I/O systems from personal computer disks and the advantages of five 
different &Sk array oqamzahons, mdependent of disks used m those army 
The later pomt starts wrth the tradmonal mIrrOred d&is to achieve 
acceptable rehablhty, WI~I each succeedmg level lmprovmg 
l the &a rate, characterued by a small number of requests per second 
for massive amounts of sequentml mformauon (supercomputer 
apphcauons). 
114 
. the 110 rate, charactenzcd by a large number of read-mtify-wnles to 
a small amount of random mformauon (Imnsacuon-pmcessmg), 
l or the useable storage caponly, 
or possibly all three 
Figure 5 shows the performance improvements per dusk for each level 
RAID The highest performance per &Sk comes from ather Level 1 or 
Level 5 In transaction-processmg smiauons usmg no more than 50% of 
storage capacity, then the choice IS murored d&s (Level 1) However, if 
the sltuatlon calls for using more than 50% of storage capacity, or for 
supercomputer apphcauons, or for combmed supercomputer apphcanons 
and transacuon processmg. then Level 5 looks best Both the strength and 
weakpess of Level 1 IS that It dupbcates data rather than calculatmg check 
mformauon, for the duphcated data Improves read performance but lowers 
capacity and wnte performance,whde check data IS useful only on a f&lure 
Inspued by the space-tune product of pagmg studies [Denmng 781, we 
propose a smgle figure of ment called the space-speedproducr the useable 
storage fracuon umes the eff&ncy per event Usmg this memc. Level 5 
has an advantage over Level 1 of 17 for reads and 3 3 for writes for G=lO 
Let us return to the fast point, the advantages of buddmg I/O system 
from personal computer disks Compared to tradmonal Single Large 
Expensive D&s (SLED). Redundant Arrays of Inexpensive Disks (RAID) 
offer slgmficant advantages for the same cost Table VII compares a level 5 
RAID using 100 mexpensive data disks with a group size of 10 to the 
IBM 3380 As you can see, a level 5 RAID offers a factor of roughly 10 
Improvement m performance, rehab&y. and power consumption (and 
hence au condmomng costs) and a factor of 3 reduchon m size over this 
SLED Table VII also compares a level 5 RAID usmg 10 mexpenslve data 
dusks with a group size of 10 to a FUJUSU M2361A “Super Eagle” In thus 
comparison RAID offers roughly a factor of 5 improvement m 
performance, power consumption, and size with more than two orders of 
magmtude improvement m (calculated) rehabdlty 
RAID offers the further advantage of modular growth over SLED 
Rather than being hmited to 7.500 MB per mcrease for $100,000 as m 
the case of this model of IBM disk, RAIDs can grow at ather the group 
sue (1000 MB for Sll,ooO) or, if pamal groups are allowed, at the dark 
size (100 MB for $1,100) The fhp side of the corn LS that RAID also 
makes sense in systems considerably smaller than a SLED Small 
incremental costs also makes hot standby spares pracncal to further reduce 
M’ITR and thereby increase the MlTF of a large system For example, a 
1000 disk level 5 RAID with a group size of 10 and a few standby spares 
could have a calculated MTIF of over 45 years 
A fmal comment concerns the prospect of deslgnmg a complete 
transachon processmg system from enher a Level 1 or Level 5 RAID The 
drasucally lower power per megabyte of mexpenslve d&s allows systems 
designers to consider battery backup for the whole dusk array--the power 
needed for 110 PC dusks is less than two FUJITSU Super Eagles Another 
backed-up mam memory m the event of an extended power fadure The 
smaller capacity of these d&s also ties up less of the database during 
reconstrucuon, leading to higher avadabdlty (Note that Level 5 nes up 
all the d&s in a group m event of failure whde Level 1 only needs the 
single moored &Sk dunng reconstrucuon, glvmg Level 1 the edge in 
avadabdlty) 
13. Concluslon 
RAIDS offer a cost effecuve opuon to meet the challenge of 
exponentml growth m the processor and memory speeds We believe the 
size reduction of personal computer disks is a key to the success of d&c 
arrays, gust as Gordon Bell argues that the size reduction of 
microprocessors LS a key to the success m mulupmcessors [Bell 851 In 
both cases the smaller size slmphfies the mterconnectlon of the many 
components as well as packagmg and cabling While large arrays of 
mamframe processors (or SLEDS) are possible. it is certamly easier to 
construct an array from the same number of microprocessors (or PC 
dnves) Just as Bell coined the term “multi” to distmgmsh a 
muluprocessor made from microprocessors, we use the term “RAID” to 
ldenhfy a &Sk array made from personal computer d&s 
With advantages m cost-performance, rebabllrty, power consumptron, 
and modular growth, we expect RAIDS to replace SLEDS m future I/O 
systems There are, however, several open issues that may bare on the 
pracncalnv of RAIDS 
What Is-the impact of a RAID on latency? 
What IS the impact on MlTF calculabons of non-exponential fahre 
assumptwns for mdnhal d&s? 
What will be the real hfetune of a RAID vs calculated MTTF ustng the 
mdependentfdwe model? 
How would synchromred drrks a$ect level 4 and 5 RAID performance? 
How does “slowdown” S actually behave? [Ldvny 871 
How do dcfchve sectors 4gect RAID? 
How do you schedule HO to level 5 RAIDS to maxtmtse write 
pamlkllsrd 
Is there localtty of reference of aisk accesses tn transactwn processmg? 
Can lnformahon he automatccally redrstrtbuted over 100 to lO@ drsks 
to reduce contentwn 7 
Wdl dtsk controller deszgn hnut RAID pe~ormance~ 
How should 100 to 1000 d&s be constructed and phystcally connected 
w the processor? 
What is the impact of cablmg on cost, pe~onnance, and reluabdity~ 
Where should a RAID be connected to a CPU so as not to ltmlt 
pe~ormance? Memory bus? II0 bus? Cachet 
Can a file system allow aiffer stnping polices for d$erentjiles? 
What IS the role of solid state drsks and WORMS m a RAID? 
What IS the zmpact on RAID of “paralkl access” disks (access to every . . ..- 
approach would be to use a few such d&s to save the contents of bat&y 
- 
q LargevO •I Small I/o E Capacity 
90% 
80% 
70% 
60% 
50% 
40% 
30% 
20% 
10% 
0% 
1 2 3 4 5 
RAID Level 
Figure 5 Plot of Large (Grouped) and Small (Indrvrdual) 
Read-Mod&-Writes per second per disk and useable storage 
capacity for all five levels of RAID (D=lOO, G=lO) We 
assume a stngle S factor umformly for all levels wtth S=l3 
where II IS neea’ed 
ti%mcte~hCS RAIDSL SLED RAID RAID5L SLED RAID 
(10010) (IBM v SLED (lOJO) (Fyusu v SLED 
(CP3100) 33&I) (>Ibena (CP3100) M2361) (>I bener 
for MD) fw MD) 
Formatted Data Capacity (MB) 10.000 7,500 133 1,000 600 167 
FncefMB (ContmUer lncl ) $ll-$8 $18-510 22-9 $ll-$8 $20-$1725-15 
Rated MTIT (hours) 820,000 30,000 27 3 8,200,OOO 20,000 410 
MTTF in practice (hours) 9 100,000 9 9 9 9 
No Actuators 110 4 225 11 1 11 
Max I/owctuator 30 50 6 30 40 8 
Max Grouped RMW/box 1250 100 125 125 20 62 
Max Individual RMW/box 825 100 82 83 20 42 
Typ I/OS/Actuator 20 30 7 20 24 8 
Typ Gmuped RMW/hox 833 60 139 83 12 69 
Typ Individual RMW/box 550 60 9 2 55 12 46 
Volume/Box (cubic feet) 10 24 24 1 34 34 
Power/box (W) 1100 6,600 60 110 640 58 
Mm Expansion Size (MB) lOO-loo0 7,500 7 5-75 100-1000 600 06-6 
Table VII Companson of IBM 3380 disk model AK4 to Level 5 RAID usmg 
I00 Conners &Associates CP 3100s d&s and a group sne of 10 and a comparison 
of the Fujitsu M2361A ‘Super Eagle” to a level 5 RAID usrng 10 mexpensrve data 
disks wrth a group sne of 10 Numbers greater than 1 m the comparison columns 
favor the RAID 
115 
Acknowledgements 
We wish to acknowledge the followmg people who parhclpated m the 
dlscusslons from which these Ideas emerged Michael Stonebraker, John 
Ousterhout, Doug Johnson, Ken Lutz, Anapum Bhlde, Gaetano Bone110 
Clark H111, David Wood, and students m SPATS semmar offered at U C 
Berkeley III Fall 1987 We also wish to thank the followmg people who 
gave comments useful m the preparation of this paper Anapum Bhtde, 
Pete Chen, Ron David, Dave D~tzel, Fred Doughs, Dieter Gawlsk, Jim 
Gray, Mark I-h11 Doug Johnson, Joan Pendleton, Martm Schulze, and 
Her& Touau This work was supported by the National Science 
Foundauon under grant # MIP-8715235 
Appendix Rehabhty Calculation 
’ Usmg probabdny theory we can calculate the M’ITFG,,,~ We first 
assume Independent and exponenti fadure rates Our model uses a bmsed 
corn with the probabdlty of heads bemg the probablhty that a second 
failure wdl occur wllhm the MTIR of a first fadure Smce dusk fadures 
are exponential 
Probabhty(at least one of the remammg disks fadmg m MTTR) 
= [ 1 - (e-mDl,)(G+c-l) ] 
In all pracucal cases 
and smce (1 - esX) 1s approxunately X for 0 c X -Z-Z 1 
Probabd@(at least one of the remammg disks fading m MTTX) 
= m*(G+C-l)/MTIFD,k 
Then that on a &sk fiulun we fbp tlus corn 
heads => a system crash, because a second fatlure occurs before the 
fast was reprured, 
tads => recover from error and conanue 
Then 
mGroup = Expectedrune between Fadures] 
* Expectedin of flips unul fmt heads] 
Expectedrune between Fad-1 
= 
Probabfity(heads) 
= 
mD,sk 
(MTTFD,skP 
-Group = 
(G+C)*(G+C-l)*MTIR 
Group farlure IS not precisely exponenual m our model, but we have 
validated thus slmphfymg assumption for pracucal cases of MTTR << 
MTTF/(G+C) This makes the MTTF of the whole system JUSt 
MTl’FGmu,, divided by the number of groups, nG 
References 
[Bell S-t] C G Bell, “The Mm1 and Micro Industries,’ IEEE 
Compur~r Vol 17 No 10 (October 1984). pp 14-30 
[Jo) 851 B Jo) prcscntaoon at ISSCC ‘85 panel session, Feb 1985 
[Slculorcl, S2] D P Slruloreh. C G Bell, and A Newell, Compnler 
Smu IIUL r Prm y’kr and Exm~lec, p 46 
[Moore 751 G E Moore, “Progress m Digital Integrated Electromcs,” 
Proc IEEE Drg~tol lnregrated EIecrromL Dewce Meerng, (1975). p 11 
[Mlcrs 861 G J Mycr\ A Y C Yu, and D I.. House, “Microprocessor 
Technology Trends ” Proc IEEE, Vol 74, no 12, (December 1986). 
pp 1605-1622 
[Garcia 841 H Garcld Molma, R Cullmgford, P Honeyman, R Lipton, 
“The Case for Massive Memory,” Technical Report 326, Dept of EE 
and CS. Prmceton Univ. May 1984 
[Myers 861 W Myers, “The Compeutweness of the Umted States Drsk 
Industry,” IEEE Computer, Vol 19, No 11 (January 1986), pp 85-90 
(Frank 871 P D Frank, “Advances in Head Technology,” presentauon at 
Challenges m Dask Technology Shorf Course, Insutute for Informauon 
Storage Technology, Santa Clara Umversrty, Santa Clara, Cahfomla, 
December 15-17.1987 
[Stevens 811 L D Stevens, “The Evoluuon of Magneuc Storage,” IBM 
Journal of Research and Development, Vol 25, No 5, Sept 1981, pp 
663-675 
[Harker81] J M Harker et al , “A Quarter Century of Disk File 
Innovatton,” tbtd , pp 677-689 
[Amdahl671 G M Amdahl, “Vah&ty of the single processor approach to 
achlevmg large scale compuhng capabdlties,” Proceedrngs AFIPS 1967 
Spnng Joint Computer Conference Vol 30 (Atlanttc City, New Jersey 
Apnl 1%7), pp 483-485 
[Boral83] H Boral and D J DeWm, “Database Machmes An Ideas 
Whose Time Has Passed? A Cntlque of the Future of Database 
Machmes,” Proc Internarwnal Conf on Database Machmes, Edited by 
H -0 Lelhch and M M&off, Spnnger-Verlag, Berlm, 1983 
[IBM 871 “IBM 3380 Duect Access Storage Introducuon.” IBM GC 
26-4491-O. September 1987 
[Gawbck 871 D Gawhck, pnvate commumcauon, Nov ,1987 
[FUJITSU 871 “M2361A Mlm-Disk Dnve Engmeenng Specifications,” 
(revised) Feb ,1987. B03P-4825-OOOlA 
[Adaptec 871 AIC-6250, IC Producr G&e. Adaptec, stock # DBOOO3-00 
rev B, 1987, p 46 
(L1vny871 Llvny, M , S Khoshaflan, H Boral. “Multi-disk 
management algontbms.” ?roc of ACM UGMETRXS, May 1987 
[Kim 861 M Y Kim. “Synchronized disk interleaving,” IEEE Trans 
on Computers, vol C-35, no 11, Nov 1986 
[Salem 861 K Salem and Garcia-Molma, H , “Disk Stnpmg,” IEEE 
1986 Int Conf on Data Engmeenng, 1986 
[Bitton 881 D Bltton and J Gray, “D&c Shadowing,” zn press. 1988 
[Kmzweil88] F Kurzwed, “Small &sk Arrays - The Emergmg 
Approach to High Performance,” presentauon at Sprmg COMPCON 
88, March 1.1988, San Franasco, CA 
[Hammmg50] R W Hammmg, “Error Detectmg and Correcting 
Codes,” The Bell System Techmcal Journal, Vol XXVI, No 2 (Apnl 
1950). pp 147-160 
[Hdlts 871 D Hilhs. pnvate commumcauon, October, 1987 
[Pmk W A Park and K Baiasubramanmn, “Provldmg Fault Tolerance 
m Parallel Secondary Storage Systems,” Department of Computer 
Science, Prmceton Umvemty. CS-TR-O57-86, Nov 7.1986 
[Magmms 871 N B Magmms. “Store More, Spend Less Mid-range 
O$ons Abound.“Comp~rerworid, Nov 16.1987. p 71 
lDermmn 781 P.J Dennmn and D F Slutz, “Generalized Workmg Sets 
for Segment Reference S&gs,” CACM, vol 21, no 9. (Sept. 1978) 
pp 750-759 
[Bell 851 Bell, C G , “Multls a new class of multiprocessor 
computers,“Snence. vol. 228 (Apnl26.1985) 462-467 
116 

