IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 11, NO. 1, FEBRUARY 2003 17
Chord: A Scalable Peer-to-Peer Lookup Protocol
for Internet Applications
Ion Stoica, Robert Morris, David Liben-Nowell, David R. Karger, M. Frans Kaashoek, Frank Dabek, and
Hari Balakrishnan, Member, IEEE
Abstract—A fundamental problem that confronts peer-to-peer
applications is the efficient location of the node that stores a
desired data item. This paper presents Chord, a distributed lookup
protocol that addresses this problem. Chord provides support for
just one operation: given a key, it maps the key onto a node. Data
location can be easily implemented on top of Chord by associating
a key with each data item, and storing the key/data pair at the
node to which the key maps. Chord adapts efficiently as nodes join
and leave the system, and can answer queries even if the system
is continuously changing. Results from theoretical analysis and
simulations show that Chord is scalable: Communication cost and
the state maintained by each node scale logarithmically with the
number of Chord nodes.
Index Terms—Distributed scalable algorithms, lookup protocols,
peer-to-peer networks.
I. INTRODUCTION
PEER-TO-PEER systems and applications are distributedsystems without any centralized control or hierarchical or-
ganization, in which each node runs software with equivalent
functionality. A review of the features of recent peer-to-peer
applications yields a long list: redundant storage, permanence,
selection of nearby servers, anonymity, search, authentication,
and hierarchical naming. Despite this rich set of features, the
core operation in most peer-to-peer systems is efficient location
of data items. The contribution of this paper is a scalable pro-
tocol for lookup in a dynamic peer-to-peer system with frequent
node arrivals and departures.
The Chord protocol supports just one operation: given a key,
it maps the key onto a node. Depending on the application using
Chord, that node might be responsible for storing a value asso-
ciated with the key. Chord uses consistent hashing [12] to assign
keys to Chord nodes. Consistent hashing tends to balance load,
since each node receives roughly the same number of keys, and
requires relatively little movement of keys when nodes join and
leave the system.
Manuscript received September 29, 2001; approved by IEEE/ACM
TRANSACTIONS ON NETWORKING Editor V. Paxson. This work was supported
by the Defense Advanced Research Projects Agency (DARPA) and the
Space and Naval Warfare Systems Center, San Diego, CA under Contract
N66001-00-1-8933. (The authors are listed in reverse alphabetical order.)
I. Stoica is with the Computer Science Division, University of California,
Berkeley, CA 94720-1776 USA (e-mail: istoica@cs.berkeley.edu).
R. Morris, D. Liben-Nowell, D. R. Karger, M. F. Kaashoek, F. Dabek,
and H. Balakrishnan are with the Laboratory for Computer Sci-
ence, Massachusetts Institute of Technology, Cambridge, MA 02139
USA (e-mail: rtm@lcs.mit.edu, dln@lcs.mit.edu, karger@lcs.mit.edu,
kaashoek@lcs.mit.edu, fdabek@lcs.mit.edu, hari@lcs.mit.edu).
Digital Object Identifier 10.1109/TNET.2002.808407
Previous work on consistent hashing assumes that each node
is aware of most of the other nodes in the system, an approach
that does not scale well to large numbers of nodes. In con-
trast, each Chord node needs “routing” information about only
a few other nodes. Because the routing table is distributed, a
Chord node communicates with other nodes in order to perform
a lookup. In the steady state, in an -node system, each node
maintains information about only other nodes, and re-
solves all lookups via messages to other nodes. Chord
maintains its routing information as nodes join and leave the
system.
A Chord node requires information about other
nodes for efficient routing, but performance degrades gracefully
when that information is out of date. This is important in practice
because nodes will join and leave arbitrarily, and consistency of
even state may be hard to maintain. Only one piece of
information per node need be correct in order for Chord to guar-
antee correct (though possibly slow) routing of queries; Chord
has a simple algorithm for maintaining this information in a dy-
namic environment.
The contributions of this paper are the Chord algorithm, the
proof of its correctness, and simulation results demonstrating
the strength of the algorithm. We also report some initial results
on how the Chord routing protocol can be extended to take into
account the physical network topology. Readers interested in an
application of Chord and how Chord behaves on a small Internet
testbed are referred to Dabek et al. [9]. The results reported
therein are consistent with the simulation results presented in
this paper.
The rest of this paper is structured as follows. Section II
compares Chord to related work. Section III presents the
system model that motivates the Chord protocol. Section IV
presents the Chord protocol and proves several of its properties.
Section V presents simulations supporting our claims about
Chord’s performance. Finally, we summarize our contributions
in Section VII.
II. RELATED WORK
Three features that distinguish Chord from many other
peer-to-peer lookup protocols are its simplicity, provable
correctness, and provable performance.
To clarify comparisons with related work, we will assume
in this section a Chord-based application that maps keys onto
values. A value can be an address, a document, or an arbitrary
data item. A Chord-based application would store and find each
value at the node to which the value’s key maps.
1063-6692/03$17.00 © 2003 IEEE
18 IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 11, NO. 1, FEBRUARY 2003
DNS provides a lookup service, with host names as keys
and IP addresses (and other host information) as values. Chord
could provide the same service by hashing each host name to
a key [7]. Chord-based DNS would require no special servers,
while ordinary DNS relies on a set of special root servers.
DNS requires manual management of the routing information
( records) that allows clients to navigate the name server
hierarchy; Chord automatically maintains the correctness of
the analogous routing information. DNS only works well when
host names are structured to reflect administrative boundaries;
Chord imposes no naming structure. DNS is specialized to
the task of finding named hosts or services, while Chord can
also be used to find data objects that are not tied to particular
machines.
The Freenet peer-to-peer storage system [5], [6], like Chord,
is decentralized and symmetric and automatically adapts when
hosts leave and join. Freenet does not assign responsibility for
documents to specific servers; instead, its lookups take the form
of searches for cached copies. This allows Freenet to provide a
degree of anonymity, but prevents it from guaranteeing retrieval
of existing documents or from providing low bounds on retrieval
costs. Chord does not provide anonymity, but its lookup oper-
ation runs in predictable time and always results in success or
definitive failure.
The Ohaha system uses a consistent hashing-like algorithm
that maps documents to nodes, and Freenet-style query routing
[20]. As a result, it shares some of the weaknesses of Freenet.
Archival Intermemory uses an off-line computed tree to map
logical addresses to machines that store the data [4].
The Globe system [2] has a wide-area location service to
map object identifiers to the locations of moving objects. Globe
arranges the Internet as a hierarchy of geographical, topolog-
ical, or administrative domains, effectively constructing a static
world-wide search tree, much like DNS. Information about an
object is stored in a particular leaf domain, and pointer caches
provide search shortcuts [25]. The Globe system handles high
load on the logical root by partitioning objects among multiple
physical root servers using hash-like techniques. Chord per-
forms this hash function well enough that it can achieve scal-
ability without also involving any hierarchy, though Chord does
not exploit network locality as well as Globe.
The distributed data location protocol developed by Plaxton
et al. [21] is perhaps the closest algorithm to the Chord pro-
tocol. The Tapestry lookup protocol [26], used in OceanStore
[13], is a variant of the Plaxton algorithm. Like Chord, it guar-
antees that queries make no more than a logarithmic number
of hops and that keys are well balanced. The Plaxton protocol’s
main advantage over Chord is that it ensures, subject to assump-
tions about network topology, that queries never travel further in
network distance than the node where the key is stored. Chord,
on the other hand, is substantially less complicated and handles
concurrent node joins and failures well. Pastry [23] is a prefix-
based lookup protocol that has properties similar to Chord. Like
Tapestry, Pastry takes into account network topology to reduce
the routing latency. However, Pastry achieves this at the cost of a
more elaborated join protocol which initializes the routing table
of the new node by using the information from nodes along the
path traversed by the join message.
CAN uses a -dimensional Cartesian coordinate space (for
some fixed ) to implement a distributed hash table that maps
keys onto values [22]. Each node maintains state, and the
lookup cost is . Thus, in contrast to Chord, the state
maintained by a CAN node does not depend on the network size
, but the lookup cost increases faster than . If ,
CAN lookup times and storage needs match Chord’s. However,
CAN is not designed to vary as (and thus, ) varies,
so this match will only occur for the “right” corresponding to
the fixed . CAN requires an additional maintenance protocol to
periodically remap the identifier space onto nodes. Chord also
has the advantage that its correctness is robust in the face of
partially incorrect routing information.
Chord’s routing procedure may be thought of as a one-dimen-
sional analogue of the Grid location system (GLS) [15]. GLS
relies on real-world geographic location information to route its
queries; Chord maps its nodes to an artificial one-dimensional
space within which routing is carried out by an algorithm sim-
ilar to Grid’s.
Napster [18] and Gnutella [11] provide a lookup operation
to find data in a distributed set of peers. They search based on
user-supplied keywords, while Chord looks up data with unique
identifiers. Use of keyword search presents difficulties in both
systems. Napster uses a central index, resulting in a single point
of failure. Gnutella floods each query over the whole system, so
its communication and processing costs are high in large sys-
tems.
Chord has been used as a basis for a number of subsequent
research projects. The Chord File System (CFS) stores files
and metadata in a peer-to-peer system, using Chord to locate
storage blocks [9]. New analysis techniques have shown that
Chord’s stabilization algorithms (with minor modifications)
maintain good lookup performance despite continuous failure
and joining of nodes [16]. Chord has been evaluated as a tool to
serve DNS [7] and to maintain a distributed public key database
for secure name resolution [1].
III. SYSTEM MODEL
Chord simplifies the design of peer-to-peer systems and ap-
plications based on it by addressing these difficult problems.
• Load balance: Chord acts as a distributed hash function,
spreading keys evenly over the nodes; this provides a de-
gree of natural load balance.
• Decentralization: Chord is fully distributed; no node is
more important than any other. This improves robustness
and makes Chord appropriate for loosely organized
peer-to-peer applications.
• Scalability: The cost of a Chord lookup grows as the log
of the number of nodes, so even very large systems are
feasible. No parameter tuning is required to achieve this
scaling.
• Availability: Chord automatically adjusts its internal ta-
bles to reflect newly joined nodes as well as node failures,
ensuring that, barring major failures in the underlying net-
work, the node responsible for a key can always be found.
This is true even if the system is in a continuous state of
change.
STOICA et al.: CHORD: SCALABLE PEER-TO-PEER LOOKUP PROTOCOL 19
• Flexible naming: Chord places no constraints on the
structure of the keys it looks up; the Chord keyspace is
flat. This gives applications a large amount of flexibility
in how they map their own names to Chord keys.
The Chord software takes the form of a library to be linked
with the applications that use it. The application interacts with
Chord in two main ways. First, the Chord library provides a
function that yields the IP address of the node re-
sponsible for the key. Second, the Chord software on each node
notifies the application of changes in the set of keys that the node
is responsible for. This allows the application software to, for
example, move corresponding values to their new homes when
a new node joins.
The application using Chord is responsible for providing any
desired authentication, caching, replication, and user-friendly
naming of data. Chord’s flat keyspace eases the implementation
of these features. For example, an application could authenticate
data by storing it under a Chord key derived from a crypto-
graphic hash of the data. Similarly, an application could repli-
cate data by storing it under two distinct Chord keys derived
from the data’s application-level identifier.
The following are examples of applications for which Chord
can provide a good foundation.
• Cooperative mirroring, in which multiple providers of
content cooperate to store and serve each others’ data. The
participants might, for example, be a set of software devel-
opment projects, each of which makes periodic releases.
Spreading the total load evenly over all participants’ hosts
lowers the total cost of the system, since each participant
need provide capacity only for the average load, not for
that participant’s peak load. Dabek et al. describe a real-
ization of this idea that uses Chord to map data blocks onto
servers; the application interacts with Chord to achieve
load balance, data replication, and latency-based server se-
lection [9].
• Time-shared storage for nodes with intermittent connec-
tivity. If someone wishes their data to be always available,
but their server is only occasionally available, they can
offer to store others’ data while they are connected, in re-
turn for having their data stored elsewhere when they are
disconnected. The data’s name can serve as a key to iden-
tify the (live) Chord node responsible for storing the data
item at any given time. Many of the same issues arise as
in the cooperative mirroring application, though the focus
here is on availability rather than load balance.
• Distributed indexes to support Gnutella- or Napster-like
keyword search. A key in this application could be derived
from the desired keywords, while values could be lists of
machines offering documents with those keywords.
• Large-scale combinatorial search, such as code
breaking. In this case, keys are candidate solutions to
the problem (such as cryptographic keys); Chord maps
these keys to the machines responsible for testing them
as solutions.
We have built several peer-to-peer applications using Chord.
The structure of a typical application is shown in Fig. 1. The
highest layer implements application-specific functions such as
Fig. 1. Structure of an example Chord-based distributed storage system.
file-system metadata. The next layer implements a general-pur-
pose distributed hash table that multiple applications use to in-
sert and retrieve data blocks identified with unique keys. The
distributed hash table takes care of storing, caching, and replica-
tion of blocks. The distributed hash table uses Chord to identify
the node responsible for storing a block, and then communicates
with the block storage server on that node to read or write the
block.
IV. CHORD PROTOCOL
This section describes the Chord protocol, which specifies
how to find the locations of keys, how new nodes join the
system, and how to recover from the failure (or planned
departure) of existing nodes. In this paper, we assume that
communication in the underlying network is both symmetric
(if can route to , then can route to ), and transitive (if
can route to and can route to , then can route to ).
A. Overview
At its heart, Chord provides fast distributed computation of
a hash function, mapping keys to nodes responsible for them.
Chord assigns keys to nodes with consistent hashing [12], [14],
which has several desirable properties. With high probability,
the hash function balances load (all nodes receive roughly the
same number of keys). Also with high probability, when an th
node joins (or leaves) the network, only an fraction of
the keys are moved to a different location—this is clearly the
minimum necessary to maintain a balanced load.
Chord improves the scalability of consistent hashing by
avoiding the requirement that every node knows about every
other node. A Chord node needs only a small amount of
“routing” information about other nodes. Because this infor-
mation is distributed, a node resolves the hash function by
communicating with other nodes. In an -node network, each
node maintains information about only other nodes,
and a lookup requires messages.
B. Consistent Hashing
The consistent hash function assigns each node and key an
-bit identifier using SHA-1 [10] as a base hash function. A
node’s identifier is chosen by hashing the node’s IP address,
while a key identifier is produced by hashing the key. We will
use the term “key” to refer to both the original key and its image
20 IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 11, NO. 1, FEBRUARY 2003
Fig. 2. Identifier circle (ring) consisting of ten nodes storing five keys.
under the hash function, as the meaning will be clear from con-
text. Similarly, the term “node” will refer to both the node and
its identifier under the hash function. The identifier length
must be large enough to make the probability of two nodes or
keys hashing to the same identifier negligible.
Consistent hashing assigns keys to nodes as follows. Identi-
fiers are ordered on an identifier circle modulo . Key is
assigned to the first node whose identifier is equal to or follows
(the identifier of) in the identifier space. This node is called
the successor node of key , denoted by . If iden-
tifiers are represented as a circle of numbers from 0 to ,
then is the first node clockwise from . In the re-
mainder of this paper, we will also refer to the identifier circle
as the Chord ring.
Fig. 2 shows a Chord ring with . The Chord ring has
ten nodes and stores five keys. The successor of identifier 10 is
node 14, so key 10 would be located at node 14. Similarly, keys
24 and 30 would be located at node 32, key 38 at node 38, and
key 54 at node 56.
Consistent hashing is designed to let nodes enter and leave
the network with minimal disruption. To maintain the consistent
hashing mapping when a node joins the network, certain keys
previously assigned to ’s successor now become assigned to
. When node leaves the network, all of its assigned keys are
reassigned to ’s successor. No other changes in assignment of
keys to nodes need occur. In the example above, if a node were
to join with identifier 26, it would capture the key with identifier
24 from the node with identifier 32.
The following results are proven in the papers that introduced
consistent hashing [12], [14].
Theorem IV.1: For any set of nodes and keys, with high
probability, the following is true.
1) Each node is responsible for at most keys.
2) When an th node joins or leaves the network, the
responsibility for keys changes hands (and only
to or from the joining or leaving node).
When consistent hashing is implemented as described above,
the theorem proves a bound of . The consistent
hashing paper shows that can be reduced to an arbitrarily small
constant by having each node run virtual nodes, each
with its own identifier. In the remainder of this paper, we will
analyze all bounds in terms of work per virtual node. Thus, if
each real node runs virtual nodes, all bounds should be mul-
tiplied by .
The phrase “with high probability” bears some discussion. A
simple interpretation is that the nodes and keys are randomly
chosen, which is plausible in a nonadversarial model of the
world. The probability distribution is then over random choices
of keys and nodes, and says that such a random choice is un-
likely to produce an unbalanced distribution. A similar model
is applied to analyze standard hashing. Standard hash functions
distribute data well when the set of keys being hashed is random.
When keys are not random, such a result cannot be guaran-
teed—indeed, for any hash function, there exists some key set
that is terribly distributed by the hash function (e.g., the set of
keys that all map to a single hash bucket). In practice, such po-
tential bad sets are considered unlikely to arise. Techniques have
also been developed [3] to introduce randomness in the hash
function; given any set of keys, we can choose a hash function
at random so that the keys are well distributed with high proba-
bility over the choice of hash function. A similar technique can
be applied to consistent hashing; thus, the “high probability”
claim in the theorem above. Rather than select a random hash
function, we make use of the SHA-1 hash which is expected to
have good distributional properties.
Of course, once the random hash function has been chosen,
an adversary can select a badly distributed set of keys for that
hash function. In our application, an adversary can generate a
large set of keys and insert into the Chord ring only those keys
that map to a particular node, thus, creating a badly distributed
set of keys. As with standard hashing, however, we expect that a
nonadversarial set of keys can be analyzed as if it were random.
Using this assumption, we state many of our results below as
“high probability” results.
C. Simple Key Location
This section describes a simple but slow Chord lookup al-
gorithm. Succeeding sections will describe how to extend the
basic algorithm to increase efficiency, and how to maintain the
correctness of Chord’s routing information.
Lookups could be implemented on a Chord ring with little
per-node state. Each node needs only to know how to contact
its current successor node on the identifier circle. Queries for a
given identifier could be passed around the circle via these suc-
cessor pointers until they encounter a pair of nodes that straddle
the desired identifier; the second in the pair is the node the query
maps to.
Fig. 3(a) shows the pseudocode that implements simple key
lookup. Remote calls and variable references are preceded by
the remote node identifier, while local variable references and
procedure calls omit the local node. Thus, .foo denotes a
remote procedure call of procedure foo on node , while .bar,
without parentheses, is an RPC to fetch a variable bar from node
. The notation denotes the segment of the Chord ring
obtained by moving clockwise from (but not including) until
reaching (and including) .
Fig. 3(b) shows an example in which node 8 performs a
lookup for key 54. Node 8 invokes for key
STOICA et al.: CHORD: SCALABLE PEER-TO-PEER LOOKUP PROTOCOL 21
Fig. 3. (a) Simple (but slow) pseudocode to find the successor node of an identifier id. Remote procedure calls and variable lookups are preceded by the remote
node. (b) Path taken by a query from node 8 for key 54, using the pseudocode in Fig. 3(a).
Fig. 4. (a) Finger table entries for node 8. (b) Path of a query for key 54 starting at node 8, using the algorithm in Fig. 5.
54 which eventually returns the successor of that key, node 56.
The query visits every node on the circle between nodes 8 and
56. The result returns along the reverse of the path followed by
the query.
D. Scalable Key Location
The lookup scheme presented in Section IV-C uses a number
of messages linear in the number of nodes. To accelerate
lookups, Chord maintains additional routing information. This
additional information is not essential for correctness, which is
achieved as long as each node knows its correct successor.
As before, let be the number of bits in the key/node iden-
tifiers. Each node maintains a routing table with up to en-
tries (we will see that, in fact, only are distinct), called
the finger table. The th entry in the table at node contains
the identity of the first node that succeeds by at least
on the identifier circle, i.e., , where
(and all arithmetic is modulo ). We call node the
th finger of node , and denote it by (see Table I).
A finger table entry includes both the Chord identifier and the
IP address (and port number) of the relevant node. Note that the
TABLE I
DEFINITION OF VARIABLES FOR NODE n, USING m-BIT IDENTIFIERS
first finger of is the immediate successor of on the circle; for
convenience we often refer to the first finger as the successor.
The example in Fig. 4(a) shows the finger table of node 8.
The first finger of node 8 points to node 14, as node 14 is the
first node that succeeds . Similarly, the last
finger of node 8 points to node 42, as node 42 is the first node
that succeeds .
This scheme has two important characteristics. First, each
node stores information about only a small number of other
nodes, and knows more about nodes closely following it on the
22 IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 11, NO. 1, FEBRUARY 2003
Fig. 5. Scalable key lookup using the finger table.
identifier circle than about nodes farther away. Second, a node’s
finger table generally does not contain enough information to
directly determine the successor of an arbitrary key . For ex-
ample, node 8 in Fig. 4(a) cannot determine the successor of
key 34 by itself, as this successor (node 38) does not appear in
node 8’s finger table.
Fig. 5 shows the pseudocode of the opera-
tion, extended to use finger tables. If falls between and its
successor, is finished and node returns its
successor. Otherwise, searches its finger table for the node
whose ID most immediately precedes , and then invokes
at . The reason behind this choice of is
that the closer is to , the more it will know about the iden-
tifier circle in the region of .
As an example, consider the Chord circle in Fig. 4(b), and
suppose node 8 wants to find the successor of key 54. Since the
largest finger of node 8 that precedes 54 is node 42, node 8 will
ask node 42 to resolve the query. In turn, node 42 will determine
the largest finger in its finger table that precedes 54, i.e., node 51.
Finally, node 51 will discover that its own successor, node 56,
succeeds key 54, and thus, will return node 56 to node 8.
Since each node has finger entries at power of two intervals
around the identifier circle, each node can forward a query at
least halfway along the remaining distance between the node
and the target identifier. From this intuition follows a theorem.
Theorem IV.2: With high probability, the number of nodes
that must be contacted to find a successor in an -node network
is .
Proof: Suppose that node wishes to resolve a query for
the successor of . Let be the node that immediately precedes
. We analyze the number of query steps to reach .
Recall that if , then forward its query to the closest
predecessor of in its finger table. Consider the such that node
is in the interval . Since this interval is
not empty (it contains ), node will contact its th finger, the
first node in this interval. The distance (number of identifiers)
between and is at least . But and are both in the
interval , which means the distance between
them is at most . This means is closer to than to , or
equivalently, that the distance from to is at most half the
distance from to .
If the distance between the node handling the query and the
predecessor halves in each step, and is at most initially,
then within steps the distance will be one, meaning we have
arrived at .
In fact, as discussed above, we assume that node and key
identifiers are random. In this case, the number of forwardings
necessary will be with high probability. After
forwardings, the distance between the current query node and
the key will be reduced to at most . The probability
that any other node is in this interval is at most , which is
negligible. Thus, the next forwarding step will find the desired
node.
In Section V, where we report our experimental results,
we will observe (and justify) that the average lookup time is
.
Although the finger table contains room for entries, in fact,
only fingers need be stored. As we argued in the above
proof, no node is likely to be within distance of any
other node. Thus, the th finger of the node, for any
, will be equal to the node’s immediate successor with
high probability and need not be stored separately.
E. Dynamic Operations and Failures
In practice, Chord needs to deal with nodes joining the
system and with nodes that fail or leave voluntarily. This
section describes how Chord handles these situations.
1) Node Joins and Stabilization: In order to ensure that
lookups execute correctly as the set of participating nodes
changes, Chord must ensure that each node’s successor pointer
is up to date. It does this using a “stabilization” protocol that
each node runs periodically in the background and which
updates Chord’s finger tables and successor pointers.
Fig. 6 shows the pseudocode for joins and stabilization. When
node first starts, it calls , where is any known
Chord node, or to create a new Chord network. The
function asks to find the immediate successor of .
By itself, does not make the rest of the network aware
of .
Every node runs periodically to learn about
newly joined nodes. Each time node runs ,
it asks its successor for the successor’s predecessor , and
decides whether should be ’s successor instead. This would
be the case if node recently joined the system. In addition,
notifies node ’s successor of ’s existence, giving
the successor the chance to change its predecessor to . The
successor does this only if it knows of no closer predecessor
than .
Each node periodically calls to make sure
its finger table entries are correct; this is how new nodes
initialize their finger tables, and it is how existing nodes
incorporate new nodes into their finger tables. Each node
also runs periodically, to clear the node’s
predecessor pointer if has failed; this allows it
to accept a new predecessor in .
STOICA et al.: CHORD: SCALABLE PEER-TO-PEER LOOKUP PROTOCOL 23
Fig. 6. Pseudocode for stabilization.
As a simple example, suppose node joins the system, and
its ID lies between nodes and . In its call to , ac-
quires as its successor. Node , when notified by , acquires
as its predecessor. When next runs , it asks
for its predecessor (which is now ); then acquires as its
successor. Finally, notifies , and acquires as its pre-
decessor. At this point, all predecessor and successor pointers
are correct. At each step in the process, is reachable from
using successor pointers; this means that lookups concurrent
with the join are not disrupted. Fig. 7 illustrates the join proce-
dure, when ’s ID is 26, and the IDs of and are 21 and
32, respectively.
As soon as the successor pointers are correct, calls to
will reflect the new node. Newly joined
Fig. 7. Example illustrating the join operation. Node 26 joins the system
between nodes 21 and 32. The arcs represent the successor relationship.
(a) Initial state: node 21 points to node 32. (b) Node 26 finds its successor (i.e.,
node 32) and points to it. (c) Node 26 copies all keys less than 26 from node
32. (d) The stabilize procedure updates the successor of node 21 to node 26.
nodes that are not yet reflected in other nodes’ finger tables
may cause to initially undershoot, but the
loop in the lookup algorithm will nevertheless follow successor
( ) pointers through the newly joined nodes until the
correct predecessor is reached. Eventually,
will adjust finger table entries, eliminating the need for these
linear scans.
The following result, proved in [24], shows that the inconsis-
tent state caused by concurrent joins is transient.
Theorem IV.3: If any sequence of join operations is executed
interleaved with stabilizations, then at some time after the last
join the successor pointers will form a cycle on all the nodes in
the network.
In other words, after some time each node is able to reach any
other node in the network by following successor pointers.
Our stabilization scheme is guaranteed to add nodes to a
Chord ring in a way that preserves reachability of existing
nodes, even in the face of concurrent joins and lost and
reordered messages. This stabilization protocol by itself will
not correct a Chord system that has split into multiple disjoint
cycles, or a single cycle that loops multiple times around the
identifier space. These pathological cases cannot be produced
by any sequence of ordinary node joins. If produced, these
cases can be detected and repaired by periodic sampling of the
ring topology [24].
2) Impact of Node Joins on Lookups: Here, we consider the
impact of node joins on lookups. We first consider correctness.
If joining nodes affect some region of the Chord ring, a lookup
that occurs before stabilization has finished can exhibit one
24 IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 11, NO. 1, FEBRUARY 2003
of three behaviors. The common case is that all the finger
table entries involved in the lookup are reasonably current,
and the lookup finds the correct successor in steps.
The second case is where successor pointers are correct, but
fingers are inaccurate. This yields correct lookups, but they
may be slower. In the final case, the nodes in the affected region
have incorrect successor pointers, or keys may not yet have
migrated to newly joined nodes, and the lookup may fail. The
higher layer software using Chord will notice that the desired
data was not found, and has the option of retrying the lookup
after a pause. This pause can be short, since stabilization fixes
successor pointers quickly.
Now let us consider performance. Once stabilization has
completed, the new nodes will have no effect beyond increasing
the in the lookup time. If stabilization has not yet
completed, existing nodes’ finger table entries may not reflect
the new nodes. The ability of finger entries to carry queries long
distances around the identifier ring does not depend on exactly
which nodes the entries point to; the distance halving argument
depends only on ID-space distance. Thus, the fact that finger
table entries may not reflect new nodes does not significantly
affect lookup speed. The main way in which newly joined
nodes can influence lookup speed is if the new nodes’ IDs are
between the target’s predecessor and the target. In that case
the lookup will have to be forwarded through the intervening
nodes, one at a time. But unless a tremendous number of nodes
joins the system, the number of nodes between two old nodes
is likely to be very small, so the impact on lookup is negligible.
Formally, we can state the following result. We call a Chord
ring stable if all its successor and finger pointers are correct.
Theorem IV.4: If we take a stable network with nodes
with correct finger pointers, and another set of up to nodes
joins the network, and all successor pointers (but perhaps not
all finger pointers) are correct, then lookups will still take
time with high probability.
Proof: The original set of fingers will, in time,
bring the query to the old predecessor of the correct node. With
high probability, at most new nodes will land between
any two old nodes. So only new nodes will need to
be traversed along successor pointers to get from the old prede-
cessor to the new predecessor.
More generally, as long as the time it takes to adjust fingers is
less than the time it takes the network to double in size, lookups
will continue to take hops. We can achieve such ad-
justment by repeatedly carrying out lookups to update our fin-
gers. It follows that lookups perform well as long as
rounds of stabilization happen between any node joins.
3) Failure and Replication: The correctness of the Chord
protocol relies on the fact that each node knows its successor.
However, this invariant can be compromised if nodes fail. For
example, in Fig. 4, if nodes 14, 21, and 32 fail simultaneously,
node 8 will not know that node 38 is now its successor, since
it has no finger pointing to 38. An incorrect successor will lead
to incorrect lookups. Consider a query for key 30 initiated by
node 8. Node 8 will return node 42, the first node it knows about
from its finger table, instead of the correct successor, node 38.
To increase robustness, each Chord node maintains a suc-
cessor list of size , containing the node’s first successors.
If a node’s immediate successor does not respond, the node can
substitute the second entry in its successor list. All successors
would have to simultaneously fail in order to disrupt the Chord
ring, an event that can be made very improbable with modest
values of . Assuming each node fails independently with prob-
ability , the probability that all successors fail simultaneously
is only . Increasing makes the system more robust.
Handling the successor list requires minor changes in the
pseudocode in Figs. 5 and 6. A modified version of the stabi-
lize procedure in Fig. 6 maintains the successor list. Successor
lists are stabilized as follows. Node reconciles its list with its
successor by copying ’s successor list, removing its last entry,
and prepending to it. If node notices that its successor has
failed, it replaces it with the first live entry in its successor list
and reconciles its successor list with its new successor. At that
point, can direct ordinary lookups for keys for which the failed
node was the successor to the new successor. As time passes,
and stabilize will correct finger table entries and
successor list entries pointing to the failed node.
A modified version of the closest_preceding_node procedure
in Fig. 5 searches not only the finger table but also the successor
list for the most immediate predecessor of . In addition, the
pseudocode needs to be enhanced to handle node failures. If
a node fails during the procedure, the lookup
proceeds, after a timeout, by trying the next best predecessor
among the nodes in the finger table and the successor list.
The following results quantify the robustness of the Chord
protocol, by showing that neither the success nor the perfor-
mance of Chord lookups is likely to be affected even by massive
simultaneous failures. Both theorems assume that the successor
list has length .
Theorem IV.5: If we use a successor list of length
in a network that is initially stable, and
then every node fails with probability 1/2, then with high prob-
ability returns the closest living successor to
the query key.
Proof: Before any nodes fail, each node was aware of its
immediate successors. The probability that all of these succes-
sors fail is , so with high probability every node is aware
of its immediate living successor. As was argued in Section III,
if the invariant that every node is aware of its immediate suc-
cessor holds, then all queries are routed properly, since every
node except the immediate predecessor of the query has at least
one better node to which it will forward the query.
Theorem IV.6: In a network that is initially stable, if every
node then fails with probability 1/2, then the expected time to
execute is .
Proof: Due to space limitations, we omit the proof of this
result, which can be found in [24].
Under some circumstances, the preceding theorems may
apply to malicious node failures as well as accidental failures.
An adversary may be able to make some set of nodes fail, but
have no control over the choice of the set. For example, the
adversary may be able to affect only the nodes in a particular
geographical region, or all the nodes that use a particular access
link, or all the nodes that have a certain IP address prefix. As
was discussed above, because Chord node IDs are generated
STOICA et al.: CHORD: SCALABLE PEER-TO-PEER LOOKUP PROTOCOL 25
by hashing IP addresses, the IDs of these failed nodes will be
effectively random, just as in the failure case analyzed above.
The successor list mechanism also helps higher layer soft-
ware replicate data. A typical application using Chord might
store replicas of the data associated with a key at the nodes
succeeding the key. The fact that a Chord node keeps track of
its successors means that it can inform the higher layer soft-
ware when successors come and go, and thus, when the software
should propagate data to new replicas.
4) Voluntary Node Departures: Since Chord is robust in the
face of failures, a node voluntarily leaving the system could be
treated as a node failure. However, two enhancements can im-
prove Chord performance when nodes leave voluntarily. First,
a node that is about to leave may transfer its keys to its suc-
cessor before it departs. Second, may notify its predecessor
and successor before leaving. In turn, node will remove
from its successor list, and add the last node in ’s successor list
to its own list. Similarly, node will replace its predecessor with
’s predecessor. Here we assume that sends its predecessor to
, and the last node in its successor list to .
F. More Realistic Analysis
Our analysis above gives some insight into the behavior of
the Chord system, but is inadequate in practice. The theorems
proven above assume that the Chord ring starts in a stable state
and then experiences joins or failures. In practice, a Chord ring
will never be in a stable state; instead, joins and departures will
occur continuously, interleaved with the stabilization algorithm.
The ring will not have time to stabilize before new changes
happen. The Chord algorithms can be analyzed in this more
general setting. Other work [16] shows that if the stabilization
protocol is run at a certain rate (dependent on the rate at which
nodes join and fail) then the Chord ring remains continuously in
an “almost stable” state in which lookups are fast and correct.
V. SIMULATION RESULTS
In this section, we evaluate the Chord protocol by simulation.
The packet-level simulator uses the lookup algorithm in Fig. 5,
extended with the successor lists described in Section IV-E3,
and the stabilization algorithm in Fig. 6.
A. Protocol Simulator
The Chord protocol can be implemented in an iterative or re-
cursive style. In the iterative style, a node resolving a lookup
initiates all communication: It asks a series of nodes for infor-
mation from their finger tables, each time moving closer on the
Chord ring to the desired successor. In the recursive style, each
intermediate node forwards a request to the next node until it
reaches the successor. The simulator implements the Chord pro-
tocol in an iterative style.
During each stabilization step, a node updates its immediate
successor and one other entry in its successor list or finger table.
Thus, if a node’s successor list and finger table contain a total of
unique entries, each entry is refreshed once every stabiliza-
tion rounds. Unless otherwise specified, the size of the successor
list is one, that is, a node knows only its immediate successor.
In addition to the optimizations described on Section IV-E4, the
simulator implements one other optimization. When the prede-
cessor of a node changes, notifies its old predecessor about
the new predecessor . This allows to set its successor to
without waiting for the next stabilization round.
The delay of each packet is exponentially distributed with
mean of 50 ms. If a node cannot contact another node within
500 ms, concludes that has left or failed. If is an entry in
’s successor list or finger table, this entry is removed. Other-
wise informs the node from which it learnt about that is
gone. When a node on the path of a lookup fails, the node that
initiated the lookup tries to make progress using the next closest
finger preceding the target key.
A lookup is considered to have succeeded if it reaches the
current successor of the desired key. This is slightly optimistic:
In a real system, there might be periods of time in which the
real successor of a key has not yet acquired the data associated
with the key from the previous successor. However, this method
allows us to focus on Chord’s ability to perform lookups, rather
than on the higher layer software’s ability to maintain consis-
tency of its own data.
B. Load Balance
We first consider the ability of consistent hashing to allocate
keys to nodes evenly. In a network with nodes and keys,
we would like the distribution of keys to nodes to be tight around
.
We consider a network consisting of 10 nodes, and vary the
total number of keys from 10 to 10 in increments of 10 .
For each number of keys, we run 20 experiments with different
random number generator seeds, counting the number of keys
assigned to each node in each experiment. Fig. 8(a) plots the
mean and the first and 99th percentiles of the number of keys
per node. The number of keys per node exhibits large variations
that increase linearly with the number of keys. For example, in
all cases some nodes store no keys. To clarify this, Fig. 8(b)
plots the probability density function (PDF) of the number of
keys per node when there are 5 10 keys stored in the network.
The maximum number of nodes stored by any node in this case
is 457, or 9.1 the mean value. For comparison, the 99th per-
centile is 4.6 the mean value.
One reason for these variations is that node identifiers do not
uniformly cover the entire identifier space. From the perspective
of a single node, the amount of the ring it “owns” is determined
by the distance to its immediate predecessor. The distance to
each of the other nodes is uniformly distributed over the
range , and we are interested in the minimum of these dis-
tance. It is a standard fact that the distribution of this minimum is
tightly approximated by an exponential distribution with mean
. Thus, for example, the owned region exceeds twice the
average value (of ) with probability .
Chord makes the number of keys per node more uniform by
associating keys with virtual nodes, and mapping multiple vir-
tual nodes (with unrelated identifiers) to each real node. This
provides a more uniform coverage of the identifier space. For
example, if we allocate randomly chosen virtual nodes to
each real node, with high probability each of the bins will
contain virtual nodes [17].
26 IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 11, NO. 1, FEBRUARY 2003
Fig. 8. (a) Mean, first, and 99th percentiles of the number of keys stored per
node in a 10 node network. (b) PDF of the number of keys per node. The total
number of keys is 510 .
To verify this hypothesis, we perform an experiment in which
we allocate virtual nodes to each real node. In this case, keys
are associated with virtual nodes instead of real nodes. We con-
sider again a network with 10 real nodes and 10 keys. Fig. 9
shows the first and 99th percentiles for and ,
respectively. As expected, the 99th percentile decreases, while
the first percentile increases with the number of virtual nodes, .
In particular, the 99th percentile decreases from 4.8 to 1.6
the mean value, while the first percentile increases from 0 to
0.5 the mean value. Thus, adding virtual nodes as an indirec-
tion layer can significantly improve load balance. The tradeoff
is that each real node now needs times as much space to store
the finger tables for its virtual nodes.
We make several observations with respect to the com-
plexity incurred by this scheme. First, the asymptotic
value of the query path length, which now becomes
, is not affected. Second,
the total identifier space covered by the virtual nodes1 mapped
1The identifier space covered by a virtual node represents the interval between
the node’s identifier and the identifier of its predecessor. The identifier space
covered by a real node is the sum of the identifier spaces covered by its virtual
nodes.
Fig. 9. First and 99th percentiles of the number of keys per node as a function
of virtual nodes mapped to a real node. The network has 10 real nodes and
stores 10 keys.
on the same real node is with high probability an frac-
tion of the total, which is the same on average as in the absence
of virtual nodes. Since the number of queries handled by a node
is roughly proportional to the total identifier space covered by
that node, the worst case number of queries handled by a node
does not change. Third, while the routing state maintained
by a node is now , this value is still reasonable in
practice; for , is only 400. Finally, while the
number of control messages initiated by a node increases by a
factor of , the asymptotic number of control messages
received from other nodes is not affected. To see why is this,
note that in the absence of virtual nodes, with “reasonable”
probability a real node is responsible for of the
identifier space. Since there are fingers in the
entire system, the number of fingers that point to a real node is
. In contrast, if each real node maps virtual
nodes, with high probability each real node is responsible for
of the identifier space. Since there are
fingers in the entire system, with high probability the number
of fingers that point to the virtual nodes mapped on the same
real node is still .
C. Path Length
Chord’s performance depends in part on the number of nodes
that must be visited to resolve a query. From Theorem IV.2, with
high probability, this number is , where is the total
number of nodes in the network.
To understand Chord’s routing performance in practice, we
simulated a network with nodes, storing 100 2 keys
in all. We varied from 3 to 14 and conducted a separate ex-
periment for each value. Each node in an experiment picked a
random set of keys to query from the system, and we measured
each query’s path length.
Fig. 10(a) plots the mean and the first and 99th percentiles
of path length as a function of . As expected, the mean path
length increases logarithmically with the number of nodes, as
do the first and 99th percentiles. Fig. 10(b) plots the PDF of the
path length for a network with 2 nodes .
STOICA et al.: CHORD: SCALABLE PEER-TO-PEER LOOKUP PROTOCOL 27
Fig. 10. (a) Path length as a function of network size. (b) PDF of the path
length in the case of a 2 node network.
Fig. 10(a) shows that the path length is about .
The value of the constant term (1/2) can be understood as fol-
lows. Consider a node making a query for a randomly chosen
key. Represent the distance in identifier space between node and
key in binary. The most significant (say, th) bit of this distance
can be corrected to 0 by following the node’s th finger. If the
next significant bit of the distance is 1, it too needs to be cor-
rected by following a finger, but if it is 0, then no th finger
is followed—instead, we move on the the th bit. In general,
the number of fingers we need to follow will be the number of
ones in the binary representation of the distance from node to
query. Since the node identifiers are randomly distributed, we
expect half of the bits to be ones. As discussed in Theorem IV.2,
after the most significant bits have been fixed, in expec-
tation there is only one node remaining between the current po-
sition and the key. Thus, the average path length will be about
.
D. Simultaneous Node Failures
In this experiment, we evaluate the impact of a massive failure
on Chord’s performance and on its ability to perform correct
TABLE II
PATH LENGTH AND NUMBER OF TIMEOUTS EXPERIENCED BY A LOOKUP AS
FUNCTION OF THE FRACTION OF NODES THAT FAIL SIMULTANEOUSLY. THE
FIRST AND 99TH PERCENTILES ARE IN PARENTHESES. INITIALLY, THE
NETWORK HAS 1000 NODES
lookups. We consider a network with nodes, where
each node maintains a successor list of size
(see Section IV-E3 for a discussion on the size of the successor
list). Once the network becomes stable, each node is made to fail
with probability . After the failures occur, we perform 10 000
random lookups. For each lookup, we record the number of
timeouts experienced by the lookup, the number of nodes con-
tacted during the lookup (including attempts to contact failed
nodes), and whether the lookup found the key’s true current suc-
cessor. A timeout occurs when a node tries to contact a failed
node. The number of timeouts experienced by a lookup is equal
to the number of failed nodes encountered by the lookup op-
eration. To focus the evaluation on Chord’s performance im-
mediately after failures, before it has a chance to correct its ta-
bles, these experiments stop stabilization just before the failures
occur and do not remove the fingers pointing to failed nodes
from the finger tables. Thus, the failed nodes are detected only
when they fail to respond during the lookup protocol.
Table II shows the mean and the first and the 99th percentiles
of the path length for the first 10 000 lookups after the failure
occurs as a function of , the fraction of failed nodes. As ex-
pected, the path length and the number of timeouts increases as
the fraction of nodes that fail increases.
To interpret these results better, next we present the mean path
length of a lookup when each node has a successor list of size
. By an argument similar to the one used in Section V-C, a
successor list of size eliminates the last hops from
the lookup path on average. The mean path length of a lookup
becomes then . The last term (1)
accounts for accessing the predecessor of the queried key once
this predecessor is found in the successor list of the previous
node. For and , the mean path length is 3.82,
which is very close to the value of 3.84 shown in Table II for
.
Let denote the progress made in the identifier space toward
a target key during a particular lookup iteration, when there are
no failures in the system. Next, assume that each node fails in-
dependently with probability . As discussed in Section IV-E3,
during each lookup iteration every node selects the largest alive
finger (from its finger table) that precedes the target key. Thus,
the progress made during the same lookup iteration in the iden-
tifier space is with probability , roughly with prob-
ability , roughly with probability ,
28 IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 11, NO. 1, FEBRUARY 2003
TABLE III
PATH LENGTH AND NUMBER OF TIMEOUTS EXPERIENCED BY A LOOKUP AS FUNCTION OF NODE JOIN AND LEAVE RATES. FIRST AND 99TH
PERCENTILES ARE IN PARENTHESES. THE NETWORK HAS ROUGHLY 1000 NODES
and so on. The expected progress made toward the target key is
then . As a result,
the mean path length becomes approximately
, where .
As an example, the mean path length for is 5.76. One
reason for which the predicted value is larger than the mea-
sured value in Table II is because the series used to evaluate is
finite in practice. This leads us to underestimating the value of
, which in turn leads us to overestimating the mean path length.
Now, let us turn our attention to the number of timeouts. Let
be the mean number of nodes contacted during a lookup op-
eration. The expected number of timeouts experienced during
a lookup operation is , and the mean path length is
. Given the mean path length in Table II, the expected
number of timeouts is 0.45 for , 1.06 for , 1.90
for , 3.13 for , and 5.06 for . These
values match well the measured number of timeouts shown in
Table II.
Finally, we note that in our simulations all lookups were suc-
cessfully resolved, which supports the robustness claim of The-
orem IV.5.
E. Lookups During Stabilization
In this experiment, we evaluate the performance and accu-
racy of Chord lookups when nodes are continuously joining and
leaving. The leave procedure uses the departure optimizations
outlined in Section IV-E4. Key lookups are generated according
to a Poisson process at a rate of one per second. Joins and volun-
tary leaves are modeled by a Poisson process with a mean arrival
rate of . Each node runs the stabilization routine at intervals
that are uniformly distributed in the interval [15, 45] seconds;
recall that only the successor and one finger table entry are sta-
bilized for each call, so the expected interval between succes-
sive stabilizations of a given finger table entry is much longer
than the average stabilization period of 30 seconds. The network
starts with 1,000 nodes, and each node maintains again a suc-
cessor list of size . Note that even though
there are no failures, timeouts may still occur during the lookup
operation; a node that tries to contact a finger that has just left
will time out.
Table III shows the means and the first and 90th percentiles
of the path length and the number of timeouts experienced by
the lookup operation as a function of the rate at which nodes
join and leave. A rate corresponds to one node joining
and leaving every 20 s on average. For comparison, recall that
each node invokes the stabilize protocol once every 30 s. Thus,
ranges from a rate of one join and leave per 1.5 stabilization
periods to a rate of twelve joins and twelve leaves per one sta-
bilization period.
As discussed in Section V-D, the mean path length in steady
state is about . Again, since
and , the mean path length is 3.82. As shown in
Table III, the measured path length is very close to this value and
does not change dramatically as increases. This is because the
number of timeouts experienced by a lookup is relatively small,
and thus, it has minimal effect on the path length. On the other
hand, the number of timeouts increases with . To understand
this result, consider the following informal argument.
Let us consider a particular finger pointer from node and
evaluate the fraction of lookup traversals of that finger that en-
counter a timeout (by symmetry, this will be the same for all fin-
gers). From the perspective of that finger, history is made up of
an interleaving of three types of events: 1) stabilizations of that
finger; 2) departures of the node pointed at by the finger; and
3) lookups that traverse the finger. A lookup causes a timeout
if the finger points at a departed node. This occurs precisely
when the event immediately preceding the lookup was a depar-
ture—if the preceding event was a stabilization, then the node
currently pointed at is alive; similarly, if the previous event was
a lookup, then that lookup timed out an caused eviction of that
dead finger pointer. So we need merely determine the fraction
of lookup events in the history that are immediately preceded
by a departure event.
To simplify the analysis we assume that, like joins and leaves,
stabilization is run according to a Poisson process. Our history
is then an interleaving of three Poisson processes. The fingered
node departs as a Poisson process at rate . Stabiliza-
tion of that finger occurs (and detects such a departure) at rate .
In each stabilization round, a node stabilizes either a node in its
finger table or a node in its successor list (there are such
STOICA et al.: CHORD: SCALABLE PEER-TO-PEER LOOKUP PROTOCOL 29
nodes in our case). Since the stabilization operation reduces to a
lookup operation (see Fig. 6), each stabilization operation will
use fingers on the average, where is the mean lookup path
length.2 As result, the rate at which a finger is touched by the
stabilization operation is where 1/30
is the average rate at which each node invokes stabilization.
Finally, lookups using that finger are also a Poisson process.
Recall that lookups are generated (globally) as a Poisson process
with rate of one lookup per second. Each such lookup uses fin-
gers on average, while there are fingers in total. Thus, a
particular finger is used with probability , meaning
that the finger gets used according to a Poisson process at rate
.
We have three interleaved Poisson processes (the lookups, de-
partures, and stabilizations). Such a union of Poisson processes
is itself a Poisson process with rate equal to the sum of the
three underlying rates. Each time an “event” occurs in this union
process, it is assigned to one of the three underlying processes
with probability proportional to those processes rates. In other
words, the history seen by a node looks like a random sequence
in which each event is a departure with probability
In particular, the event immediately preceding any lookup
is a departure with this probability. This is the probability
that the lookup encounters the timeout. Finally, the expected
number of timeouts experienced by a lookup operation is
. As
examples, the expected number of timeouts is 0.041 for
, and 0.31 for . These values are reasonably
close to the measured values shown in Table III.
The last column in Table III shows the number of lookup
failures per 10 000 lookups. The reason for these lookup failures
is state inconsistency. In particular, despite the fact that each
node maintains a successor list of nodes, it is possible
that for short periods of time a node may point to an incorrect
successor. Suppose at time , node knows both its first and its
second successor, and . Assume that just after time , a new
node joins the network between and , and that leaves
before had the chance to discover . Once learns that
has left, will replace it with , the closest successor knows
about. As a result, for any key , will return node
instead of . However, the next time invokes stabilization for
, will learn its correct successor .
F. Improving Routing Latency
While Chord ensures that the average path length is only
, the lookup latency can be quite large. This is be-
2Actually, since 2 logN of the nodes belong to the successor list, the mean
path length of the stabilization operation is smaller than the the mean path length
of the lookup operation (assuming the requested keys are randomly distributed).
This explains in part the underestimation bias in our computation.
cause the node identifiers are randomly distributed, and there-
fore nodes close in the identifier space can be far away in the un-
derlying network. In previous work [8], we attempted to reduce
lookup latency with a simple extension of the Chord protocol
that exploits only the information already in a node’s finger
table. The idea was to choose the next-hop finger based on both
progress in identifier space and latency in the underlying net-
work, trying to maximize the former while minimizing the latter.
While this protocol extension is simple to implement and does
not require any additional state, its performance is difficult to an-
alyze [8]. In this section, we present an alternate protocol exten-
sion, which provides better performance at the cost of slightly
increasing the Chord state and message complexity. We em-
phasize that we are actively exploring techniques to minimize
lookup latency, and we expect further improvements in the fu-
ture.
The main idea of our scheme is to maintain a set of alter-
nate nodes for each finger (that is, nodes with similar identi-
fiers that are roughly equivalent for routing purposes), and then
route the queries by selecting the closest node among the al-
ternate nodes according to some network proximity metric. In
particular, every node associates with each of its fingers, , a
list of immediate successors of . In addition, we modify the
find_successor function in Fig. 5 accordingly: Instead of simply
returning the largest finger, , that precedes the queried ID, the
function returns the closest node (in terms of networking dis-
tance) among and its successors. For simplicity, we choose
, where is the length of the successor list; one could
reduce the storage requirements for the routing table by main-
taining, for each finger , only the closest node among ’s
successors. To update , a node can simply ask for its suc-
cessor list, and then ping each node in the list. The node can
update either periodically, or when it detects that has failed.
Observe that this heuristic can be applied only in the recur-
sive (not the iterative) implementation of lookup, as the original
querying node will have no distance measurements to the fin-
gers of each node on the path.
To illustrate the efficacy of this heuristic, we consider a Chord
system with 2 nodes and two network topologies.
• Three-dimensional (3-D) space: The network distance is
modeled as the geometric distance in a 3-D space. This
model is motivated by recent research [19] showing that
the network latency between two nodes in the Internet can
be modeled (with good accuracy) as the geometric dis-
tance in a -dimensional Euclidean space, where .
• Transit stub: A transit-stub topology with 5000 nodes,
where link latencies are 50 ms for intra-transit domain
links, 20 ms for transit-stub links, and 1 ms for intra-stub
domain links. Chord nodes are randomly assigned to stub
nodes. This network topology aims to reflect the hierar-
chical organization of today’s Internet.
We use the lookup stretch as the main metric to evaluate our
heuristic. The lookup stretch is defined as the ratio between 1)
the latency of a Chord lookup from the time the lookup is initi-
ated to the time the result is returned to the initiator and 2) the
latency of an optimal lookup using the underlying network. The
latter is computed as the roundtrip time between the initiator and
the server responsible for the queried ID.
30 IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 11, NO. 1, FEBRUARY 2003
TABLE IV
STRETCH OF LOOKUP LATENCY FOR A CHORD SYSTEM WITH 2 NODES WHEN THE LOOKUP IS PERFORMED BOTH IN THE ITERATIVE AND RECURSIVE STYLE.
TWO NETWORK MODELS ARE CONSIDERED: A 3-D EUCLIDEAN SPACE AND A TRANSIT-STUB NETWORK
Table IV shows the median, the tenth, and the 99th percentiles
of the lookup stretch over 10 000 lookups for both the iterative
and the recursive styles. The results suggest that our heuristic is
quite effective. The stretch decreases significantly as increases
from one to 16.
As expected, these results also demonstrate that recursive
lookups execute faster than iterative lookups. Without any
latency optimization, the recursive lookup style is expected to
be approximately twice as fast as the iterative style: an iterative
lookup incurs a roundtrip latency per hop, while a recursive
lookup incurs a one-way latency.
Note that in a 3-D Euclidean space, the expected distance
from a node to the closest node from a set of random nodes
is proportional to . Since the number of Chord hops
does not change as increases, we expect the lookup latency to
be also proportional to . This observation is consistent
with the results presented in Table IV. For instance, for ,
we have 17 , which is close to the observed reduction
of the median value of the lookup stretch from to .
VI. FUTURE WORK
Work remains to be done in improving Chord’s resilience
against network partitions and adversarial nodes as well as its
efficiency.
Chord can detect and heal partitions whose nodes know of
each other. One way to obtain this knowledge is for every node
to know of the same small set of initial nodes. Another approach
might be for nodes to maintain long-term memory of a random
set of nodes they have encountered in the past; if a partition
forms, the random sets in one partition are likely to include
nodes from the other partition.
A malicious or buggy set of Chord participants could present
an incorrect view of the Chord ring. Assuming that the data
Chord is being used to locate is cryptographically authenticated,
this is a threat to availability of data rather than to authenticity.
One way to check global consistency is for each node to
periodically ask other nodes to do a Chord lookup for ; if the
lookup does not yield node , this could be an indication for
victims that they are not seeing a globally consistent view of the
Chord ring.
Even messages per lookup may be too many for
some applications of Chord, especially if each message must be
sent to a random Internet host. Instead of placing its fingers at
distances that are all powers of 2, Chord could easily be changed
to place its fingers at distances that are all integer powers of
. Under such a scheme, a single routing hop could
decrease the distance to a query to of the original
distance, meaning that hops would suffice. However,
the number of fingers needed would increase to
.
VII. CONCLUSION
Many distributed peer-to-peer applications need to determine
the node that stores a data item. The Chord protocol solves this
challenging problem in decentralized manner. It offers a pow-
erful primitive: given a key, it determines the node responsible
for storing the key’s value, and does so efficiently. In the steady
state, in an -node network, each node maintains routing infor-
mation for only other nodes, and resolves all lookups
via messages to other nodes.
Attractive features of Chord include its simplicity, provable
correctness, and provable performance even in the face of
concurrent node arrivals and departures. It continues to func-
tion correctly, albeit at degraded performance, when a node’s
information is only partially correct. Our theoretical analysis
and simulation results confirm that Chord scales well with the
number of nodes, recovers from large numbers of simultaneous
node failures and joins, and answers most lookups correctly
even during recovery.
We believe that Chord will be a valuable component for
peer-to-peer large-scale distributed applications such as co-
operative file sharing, time-shared available storage systems,
distributed indices for document and service discovery, and
large-scale distributed computing platforms. Our initial expe-
rience with Chord has been very promising. We have already
built several peer-to-peer applications using Chord, including
a cooperative file sharing application [9]. The software is
available at .
REFERENCES
[1] S. Ajmani, D. Clarke, C.-H. Moh, and S. Richman, “ConChord: Coop-
erative SDSI certificate storage and name resolution,” in Proc. 1st Int.
Workshop Peer-to-Peer Systems , Cambridge, MA, Mar. 2002.
[2] A. Bakker, E. Amade, G. Ballintijn, I. Kuz, P. Verkaik, W. I. van der,
M. van Steen, and A. Tanenbaum, “The globe distribution network,” in
Proc. 2000 USENIX Annu. Conf. (FREENIX Track), San Diego, CA,
June 2000, pp. 141–152.
STOICA et al.: CHORD: SCALABLE PEER-TO-PEER LOOKUP PROTOCOL 31
[3] J. L. Carter and M. N. Wegman, “Universal classes of hash functions,”
J. Comput. Syst. Sci., vol. 18, no. 2, pp. 143–154, 1979.
[4] Y. Chen, J. Edler, A. Goldberg, A. Gottlieb, S. Sobti, and P. Yianilos, “A
prototype implementation of archival intermemory,” in Proc. 4th ACM
Conf. Digital Libraries, Berkeley, CA, Aug. 1999, pp. 28–37.
[5] I. Clarke, “A distributed decentralised information storage and retrieval
system,” Master’s thesis, Univ. Edinburgh, Edinburgh, U.K., 1999.
[6] I. Clarke, O. Sandberg, B. Wiley, and T. W. Hong, “Freenet: A
distributed anonymous information storage and retrieval system,” in
Proc. ICSI Workshop Design Issues in Anonymity and Unobservability,
Berkeley, CA, June 2000, [Online]. Available: http://freenet.source-
forge.net.
[7] R. Cox, A. Muthitacharoen, and R. Morris, “Serving DNS using Chord,”
in Proc. 1st Int. Workshop Peer-to-Peer Systems , Cambridge, MA, Mar.
2002.
[8] F. Dabek, “A cooperative file system,” Master’s thesis, Massachusetts
Inst. Technol., Cambridge, 2001.
[9] F. Dabek, F. Kaashoek, D. R. Karger, R. Morris, and I. Stoica,
“Wide-area cooperative storage with CFS,” in Proc. ACM Symp.
Operating Systems Principles, Banff, Canada, 2001, pp. 202–215.
[10] “Secure Hash Standard,” U.S. Dept. Commerce/NIST, National Tech-
nical Information Service, Springfield, VA, FIPS 180-1, Apr. 1995.
[11] Gnutella. [Online]. Available: http://gnutella.wego.com/
[12] D. R. Karger, E. Lehman, F. Leighton, M. Levine, D. Lewin, and R.
Panigrahy, “Consistent hashing and random trees: Distributed caching
protocols for relieving hot spots on the World Wide Web,” in Proc. 29th
Annu. ACM Symp. Theory of Computing, El Paso, TX, May 1997, pp.
654–663.
[13] J. Kubiatowicz, D. Bindel, Y. Chen, S. Czerwinski, P. Eaton, D. Geels,
R. Gummadi, S. Rhea, H. Weatherspoon, W. Weimer, C. Wells, and
B. Zhao, “Oceanstore: An architecture for global-scale persistent
storage,” in Proc. 9th Int. Conf. Architectural Support for Programming
Languages and Operating Systems (ASPLOS 2000), Boston, MA, Nov.
2000, pp. 190–201.
[14] D. Lewin, “Consistent hashing and random trees: Algorithms for
caching in distributed networks,” Master’s thesis, Department of
Electric. Eng. Comput. Sci., Massachusetts Inst. Technol., Cambridge,
1998.
[15] J. Li, J. Jannotti, D. De Couto, D. R. Karger, and R. Morris, “A scalable
location service for geographic ad hoc routing,” in Proc. 6th ACM Int.
Conf. Mobile Computing and Networking, Boston, MA, Aug. 2000, pp.
120–130.
[16] D. Liben-Nowell, H. Balakrishnan, and D. R. Karger, “Analysis of the
evolution of peer-to-peer systems,” in Proc. 21st ACM Symp. Princi-
ples of Distributed Computing (PODC), Monterey, CA, July 2002, pp.
233–242.
[17] R. Motwani and P. Raghavan, Randomized Algorithms. New York:
Cambridge Univ. Press, 1995.
[18] Napster. [Online]. Available: http://www.napster.com/
[19] T. S. E. Ng and H. Zhang, “Toward global network positioning,” pre-
sented at the ACM SIGCOMM Internet Measurements Workshop 2001,
San Francisco, CA, Nov. 2001.
[20] Ohaha: Smart decentralized peer-to-peer sharing. [Online]. Available:
http://www.ohaha.com/design.html
[21] C. Plaxton, R. Rajaraman, and A. Richa, “Accessing nearby copies of
replicated objects in a distributed environment,” in Proc. ACM Symp.
Parallelism in Algorithms and Architectures, Newport, RI, June 1997,
pp. 311–320.
[22] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Shenker, “A
scalable content-addressable network,” in Proc. ACM SIGCOMM, San
Diego, CA, Aug. 2001, pp. 161–172.
[23] A. Rowstron and P. Druschel, “Pastry: Scalable, distributed object lo-
cation and routing for large-scale peer-to-peer systems,” in Proc. 18th
IFIP/ACM Int. Conf. Distributed Systems Platforms (Middleware 2001),
Nov. 2001, pp. 329–350.
[24] I. Stoica, R. Morris, D. Liben-Nowell, D. R. Karger, M. F. Kaashoek,
F. Dabek, and H. C. Balakrishnan, “A scalable peer-to-peer lookup ser-
vice for internet applications,” Lab. Comput. Sci., Massachusetts Inst.
Technol., Tech. Rep. TR-819, 2001.
[25] M. van Steen, F. Hauck, G. Ballintijn, and A. Tanenbaum, “Algorithmic
design of the globe wide-area location service,” Comput. J., vol. 41, no.
5, pp. 297–310, 1998.
[26] B. Zhao, J. Kubiatowicz, and A. Joseph, “Tapestry: An infrastructure for
fault-tolerant wide-area location and routing,” Comput. Sci. Div., Univ.
California, Berkeley, Tech. Rep. UCB/CSD-01-1141, 2001.
Ion Stoica received the Ph.D. degree from Carnegie
Mellon University, Pittsburgh, PA, in 2000.
He is currently an Assistant Professor with the
Department of Electrical Engineering and Computer
Science, University of California, Berkeley, where
he does research on resource management, scalable
solutions for end-to-end quality of service, and
peer-to-peer network technologies in the Internet.
Dr. Stoica is the recipient of a National Science
Foundation CAREER Award in 2002, and the As-
sociation for Computing Machinery (ACM) Doctoral
Dissertation Award in 2001. He is a member of the ACM.
Robert Morris received the Ph.D. degree from Har-
vard University, Cambridge, MA, for work on mod-
eling and controlling networks with large numbers of
competing connections.
He is currently an Assistant Professor with the
Department of Electrical Engineering and Computer
Science, Massachusetts Institute of Technology,
Cambridge, and a Member of the Laboratory for
Computer Science. As a graduate student, he helped
design and build an ARPA-funded ATM switch with
per-circuit hop-by-hop flow control. He led a mobile
communication project which won a Best Student Paper Award from USENIX.
He cofounded Viaweb, an e-commerce hosting service. His current interests
include modular software-based routers, analysis of the aggregation behavior
of Internet traffic, and scalable ad-hoc routing.
David Liben-Nowell received the B.A. degree
from Cornell University, Ithaca, NY, in 1999 and
the M.Phil. degree from Cambridge University,
Cambridge, U.K., in 2000. He is currently working
toward the Ph.D. degree at the Massachusetts
Institute of Technology, Cambridge.
His research interests cover a variety of topics in
theoretical computer science.
Mr. Liben-Nowell is a member of the Association
for Computing Machinery.
David R. Karger received the A.B. degree from
Harvard University, Cambridge, MA, in 1989 and
the Ph.D. degree from Stanford University, Stanford,
CA, in 1994.
He is currently an Associate Professor of com-
puter science and a Member of the Laboratory
for Computer Science, Massachusetts Institute of
Technology, Cambridge. His research interests
include algorithms and information retrieval. Some
of his recent projects include Grid, a scalable ad hoc
networking system, and Haystack, a personalizable
information retrieval system. He also studies problems in graph algorithms,
optical networks, and machine learning.
Dr. Karger was the recipient of the 1994 ACM Doctoral Dissertation Award, a
1996 National Science Foundation CAREER Award, a 1997 Sloan Foundation
Fellowship, the 1997 Tucker Prize, a 1997 Packard Foundation Fellowship, and
the 2002 NAS Award for Initiatives in Research.
32 IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 11, NO. 1, FEBRUARY 2003
M. Frans Kaashoek received the Ph.D. degree from
the Vrije Universiteit, Amsterdam, The Netherlands,
in 1992 for his work on group communication in the
Amoeba distributed operating system.
He is currently a full Professor with the De-
partment of Electrical Engineering and Computer
Science, Massachusetts Institute of Technology,
Cambridge, and a Member of the Laboratory for
Computer Science. In 1995, he was awarded the
Jamieson Career Development Chair. His principal
field of interest is designing and building computer
systems. His recent projects include Exokernel, an extensible operating system,
and Click, a modular software router. Some of his current projects include
SFS, a secure, decentralized global file system, Chord, a scalable peer-to-peer
systems, and RON, a resilient overlay network. In 1998, he cofounded
Sightpath, Inc., which was acquired by Cisco Systems in 2000. He also serves
on the board of Mazu Networks, Inc.
Frank Dabek received the S.B. and M.Eng. degrees
from the Massachusetts Institute of Technology
(MIT), Cambridge. He is currently working toward
the Ph.D. degree in computer science at MIT.
His research interests include distributed systems
and high-performance server architectures.
Hari Balakrishnan (S’95–M’98) received the
B.Tech. degree from the Indian Institute of Tech-
nology, Madras, India, in 1993 and the Ph.D.
degree in computer science from the University of
California, Berkeley, in 1998.
He is currently an Associate Professor in the
Department of Electrical Engineering and Computer
Science, Massachusetts Institute of Technology
(MIT), Cambridge, where he holds the KDD Career
Development Chair. He leads the Networks and
Mobile Systems group at the Laboratory for Com-
puter Science, exploring research issues in network protocols and architecture,
mobile computing systems, and pervasive computing.
Dr. Balakrishnan was the recipient of a Sloan Foundation Fellowship in 2002,
a National Science Foundation CAREER Award in 2000, the ACM doctoral
dissertation award in 1998, and award papers at the ACM MOBICOM in 1995
and 2000, IEEE HotOS in 2001, and USENIX in 1995. He was awarded the
MIT School of Engineering Junior Bose Award for Excellence in Teaching in
2002. He is a Member of the Association for Computing Machinery.

