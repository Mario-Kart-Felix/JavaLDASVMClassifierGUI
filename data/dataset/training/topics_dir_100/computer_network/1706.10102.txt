Tableaux for Policy Synthesis for MDPs with
PCTL* Constraints
Peter Baumgartner, Sylvie Thiébaux, and Felipe Trevizan
Data61/CSIRO and Research School of Computer Science, ANU, Australia
Email: first.last@anu.edu.au
July 3, 2017
Abstract
Markov decision processes (MDPs) are the standard formalism for modelling sequential
decision making in stochastic environments. Policy synthesis addresses the problem of how
to control or limit the decisions an agent makes so that a given specification is met. In
this paper we consider PCTL*, the probabilistic counterpart of CTL*, as the specification
language. Because in general the policy synthesis problem for PCTL* is undecidable, we
restrict to policies whose execution history memory is finitely bounded a priori. Surpris-
ingly, no algorithm for policy synthesis for this natural and expressive framework has been
developed so far. We close this gap and describe a tableau-based algorithm that, given an
MDP and a PCTL* specification, derives in a non-deterministic way a system of (possibly
nonlinear) equalities and inequalities. The solutions of this system, if any, describe the
desired (stochastic) policies. Our main result in this paper is the correctness of our method,
i.e., soundness, completeness and termination.
1 Introduction
Markov decision processes (MDPs) are the standard formalism for modelling sequential decision
making in stochastic environments, where the effects of an agent’s actions are only probabilis-
tically known. The core problem is to synthesize a policy prescribing or restricting the actions
that the agent may undertake, so as to guarantee that a given specification is met. Popular
specification languages for this purpose include CTL, LTL, and their probabilistic counterparts
PCTL and probabilistic LTL (pLTL). Traditional algorithms for policy synthesis and probabilis-
tic temporal logic model-checking [8, 16] are based on bottom-up formula analysis [14, 15] or
Rabin automata [2, 10, 20].
We deviate from this mainstream research in two ways. The first significant deviation is that
we consider PCTL* as a specification language, whereas previous synthesis approaches have been
limited to pLTL and PCTL. PCTL* is the probabilistic counterpart of CTL* and subsumes both
PCTL and pLTL. For example, the PCTL* formula P≥0.8 G ((T > 30°) → P≥0.5 F G (T < 24°))
says “with probability at least 0.8, whenever the temperature exceeds 30° it will eventually stay
1
ar
X
iv
:1
70
6.
10
10
2v
1 
 [
cs
.L
O
] 
 3
0 
Ju
n 
20
17
below 24°with probability at least 0.5”. Because of the nested probability operator P the formula
is not in pLTL, and because of the nested temporal operators FG it is not in PCTL either.
Because in its full generality the policy synthesis problem for PCTL* is highly undecid-
able [4], one has to make concessions to obtain a decidable fragment. In this paper we chose
to restrict to policies whose execution history memory is finitely bounded a priori. (For exam-
ple, policies that choose actions in the current state dependent on the last ten preceding states.)
However, we do target synthesizing stochastic policies, i.e., the actions are chosen according to
a probability distribution (which generalizes the deterministic case and is known to be needed
to satisfy certain formulas [2]). Surprisingly, no algorithm for policy synthesis in this somewhat
restricted yet natural and expressive framework has been developed so far, and this paper closes
this gap.
The second significant deviation from the mainstream is that we pursue a different approach
based on analytic tableau and mathematical programming. Our tableau calculus is goal-oriented
by focusing on the given PCTL* formula, which leads to analysing runs only on a by-need basis.
This restricts the search space to partial policies that only cover the states reachable from the
initial state under the policy and for which the formula imposes constraints on the actions that
can be selected. In contrast, traditional automata based approaches require a full-blown state
space exploration. (However, we do not have an implementation yet that allows us to evaluate
the practical impact of this.) We also believe that our approach, although using somewhat non-
standard tableau features, is conceptually simpler and easier to comprehend. Of course, this is
rather subjective.
On a high level, the algorithm works as follows. The input is an MDP, the finite-history
component of the policy to be synthesized, and a PCTL* formula to be satisfied. Starting from
the MDP’s initial state, the tableau calculus symbolically executes the transition system given
by the MDP by analysing the syntactic structure of given PCTL* formula, as usual with tableau
calculi. Temporal formulas (e.g., FG-formulas) are expanded repeatedly using usual expansion
laws and trigger state transitions. The process stops at trivial cases or when a certain loop
condition is met. The underlying loop checking technique was developed only recently, by Mark
Reynolds, in the context of tableau for satisfiability checking of LTL formulas [17]. It is an
essential ingredient of our approach and we adapted it to our probabilistic setting.
Our tableaux have two kinds of branching. One kind is traditional or-branching, which
represents non-deterministic choice by going down exactly one child node. It is used, e.g., in
conjunction with recursively calling the tableau procedure itself. Such calls are necessary to
deal with nested P-operators, since at the time of analyzing a P-formula it is, roughly speaking,
unknown if the formula will hold true under the policy computed only later, as a result of
the algorithm. The other kind of branching represents a union of alternatives. It is used
for disjunctive formulas and for branching out from a state into successor states. Intuitively,
computing the probability of a disjunctive formula φ1 ∨ φ2 is a function of the probabilities of
both φ1 and φ2, so both need to be computed. Also, the probability of an X-formula Xφ at a
given state is a function of the probability of φ at all successor states, and so, again, all successor
states need to be considered.
The tableau construction always terminates and derives a system of (possibly nonlinear)
equalities and inequalities over the reals. The solutions of this system, if any, describe the
2
desired stochastic, finite-history policies. The idea of representing policies as the solutions
of a set of mathematical constraints is inspired by the abundant work in operations research,
artificial intelligence, and robotics that optimally solves MDPs with simpler constraints using
linear programming [1, 11, 9, 21].
Ourmain result in this paper is the correctness of our algorithm, i.e., soundness, completeness
and termination. To our knowledge, it is the first and only policy synthesis algorithm for PCTL*
that doesn’t restrict the language (but only slightly the policies).
Related Work
Methods for solving the PCTL*model checking problemoverMarkovChains arewell established.
The (general) policy synthesis however is harder than the model checking problem; it is known
to be undecidable for even PCTL. The main procedure works bottom-up from the syntax tree of
the given formula, akin to the standard CTL/CTL* model checking procedure. Embedded P-
formulas are recursively abstracted into boolean variables representing the sets of states satisfying
these formulas, which are computed by LTL model checking techniques using Rabin automata.
Our synthesis approach is rather different. While there is a rough correspondence in terms of
recursive calls to treat P formulas, we do not need Rabin (or any other) automata; they are
supplanted by the loop-check technique mentioned above.
The work the most closely related to ours is that of Brázdil et. al. [7, 5, 6]. Using Büchi
automata, they obtain complexity results depending on the variant of the synthesis problem
studied. However, they consider only qualitative fragments. For the case of interest in this
paper, PCTL*, they obtain results for the fragment qPCTL*. The logic qPCTL* limits the use
of the path quantifier P to formulas of the form P=1 ψ or P=0 ψ, where ψ is a path formula. On
the other hand, we cover the full logic PCTL* which has arbitrary formulas of the form P∼z ψ
where ∼ ∈ {<, ≤, >, ≥} and z ∈ [0, 1]. In contrast to the works mentioned, we have to restrict to
memory-dependent policies with an a priori limited finite memory. Otherwise the logic becomes
highly undecidable [4].
2 Preliminaries
We assume the reader is familiar with basic concepts of Markov Decision Processes (MDPs),
probabilistic model checking, and policy synthesis. See [16, 12, 3] for introductions and
overviews. In the following we summarize the notions relevant to us and we introduce our
notation.
Given a fixed finite vocabulary AP of atomic propositions a, b, c, . . . , a (propositional)
interpretation I is any subset of AP. It represents the assignment of each element in I to true
and each other atomic proposition in AP \ I to false. A distribution on a countable set X is a
function µ : X 7→ [0, 1] such that ∑x∈X µ(x) = 1, and Dist(X) is the set of all distributions on X .
A Markov Decision Process (MDP) is a tupleM = (S, sinit, A, P, L) where: S is a finite set
of states; sinit ∈ S is the initial state; A is a finite set of actions and we denote by A(s) ⊆ A
the set of actions enabled in s ∈ S; P(t |s, α) is the probability of transitioning to t ∈ S after
applying α ∈ A(s) in state s; and L : S 7→ 2AP labels each state in S with an interpretation. We
3
assume that every state has at least one enabled action, i.e., A(s) , ∅ for all s ∈ S, and that P is
a distribution on enabled actions, i.e., P(·|s, α) ∈ Dist(S) iff α ∈ A(s). For any s and α ∈ A(s)
let Succ(s, α) = {t | P(t |s, α) > 0} be the states reachable from s with non-zero probability after
applying α.
Given a state s ∈ S of M, a run from s (of M) is an infinite sequence r = (s = s1)
α1−→
s2
α2−→ s3 · · · of states si ∈ S and actions αi ∈ A(si) such that P(si+1 |si, αi) > 0, for all i ≥ 1. We
denote by Runs(s) the set of all runs from s ∈ S and Runs = ∪s∈S Runs(s). A path from s ∈ S (of
M) is a finite prefix of a run from s and we define Paths(s) and Paths in analogy to Runs(s) and
Runs. We often write runs and paths in abbreviated form as state sequences s1s2 · · · and leave
the actions implicit. Given a path p = s1s2 · · · sn let first(p) = s1 and last(p) = sn. Similarly, for
a run r = s1s2 · · · , first(r) = s1.
A policy π represents a decision rule on how to choose an action given some information
about the environment. In its most general form, a history-dependent (stochastic) policy (forM)
is a function π : Paths 7→ Dist(A) such that, for all p ∈ Paths, π(p)(α) > 0 only if α ∈ A(last(p)).
Technically, the MDPM together with π induces an infinite-state Markov chainMπ over Paths
and this way provides a probability measure for runs ofM under π [13, 3]. However, since Paths
is an infinite set, a history-dependent policy might not be representable; moreover, the problem of
finding such a policy that satisfies PCTL* constraints is undecidable [4]. To address these issues
we limit ourselves to finite-memory policies. Such policies provide a distribution on actions for
a current state from S and a current mode, and are more expressive than Markovian policies.
Formally, a finite-memory policy (forM) is a DFA πfin = (M, start,∆, act)where M is a finite
set of modes, start : S 7→ M returns an initial mode to pair with a state s ∈ S, ∆ : M × S 7→ M
is the (mode) transition function, and act : M × S 7→ Dist(A) is a function such that, for all
〈m, s〉 ∈ M × S, act(m, s)(α) > 0 only if α ∈ A(s). We abbreviate act(m, s)(α) as act(m, s, α).
Any finite-memory can be identified with a history-dependent policy, see again [3] for details.
Essentially, an MDPM together with πfin again induces a Markov chainMπfin , this time over the
finite state space M×S, labelling function Lπfin(〈m, s〉) := L(s), and transition probability function
PMπfin (〈m′, s′〉|〈m, s〉) := Σα∈A(s) act(m, s, α) · P(s′ |s, α) if m′ = ∆(m, s) and 0 otherwise. A path
from 〈m1, s1〉 (ofMπfin) is a sequence of the form 〈m1, s1〉 · · · 〈mn, sn〉 such that mi+1 = ∆(mi, si)
and PMπfin (〈mi+1, si+1〉|〈mi, si〉) > 0, for all 1 ≤ i < n. If m1 = start(s1)we get a path from s1 (of
Mπfin), similarly for runs. The definition of the satisfaction relation “|=” below applies to such
runs 〈start(s1), s1〉 · · · from s1 ofMπfin if π is a finite-memory policy πfin.
The definition of finite-memory policies πfin can be made more sophisticated, e.g., by letting
∆ depend also on actions, or by making modes dependent on a given PCTL* specification. In its
current form, the ∆-component of πfin can be setup already, e.g., to encode in 〈m, s〉 “the last ten
states preceding s”.
2.1 Policy Synthesis for PCTL*
(PCTL*) formulas follow the following grammar:
φ := true | a ∈ AP | φ ∧ φ | ¬φ | P∼z ψ (State formula)
ψ := φ | ψ ∧ ψ | ¬ψ | Xψ | ψ Uψ (Path formula)
4
In the definition of state formulas, ∼ ∈ {<, ≤, >, ≥} and 0 ≤ z ≤ 1. A proper path formula
is a path formula that is not a state formula. A formula is classical iff it is made from atomic
propositions and the Boolean connectives ¬ and ∧ only (no occurrences of P, X or U). We write
false as a shorthand for ¬true.
Given an MDPM, a history-dependent policy π, state s ∈ S and state formula φ, define a
satisfaction relationM, π, s |= φ, briefly s |= φ, as follows:
s |= true s |= φ1 ∧ φ2 iff s |= φ1 and s |= φ2
s |= a iff a ∈ L(s) s |= ¬φ iff s 6 |= φ
s |= P∼z ψ iff PrMπ ({r ∈ RunsMπ (s) | M, π, r |= ψ}) ∼ z
In the preceding line, RunsMπ (s) denotes the set of all runs from s ofMπ , and PrMπ (R) denotes
the probability of a (measurable) set R ⊆ RunsMπ . That is, the probability measure forM and
π is defined via the probability measure of the Markov chainMπ .
We need to define the satisfaction relationM, π, r |= ψ, briefly r |= ψ, for path formulas ψ.
Let r = s1s2 · · · be a run ofM and r[n] := snsn+1 · · · , for any n ≥ 1. Then:
r |= φ iff first(r) |= φ r |= ψ1 ∧ ψ2 iff r |= ψ1 and r |= ψ2
r |= ¬ψ iff r 6 |= ψ r |= Xψ iff r[2] |= ψ
r |= ψ1 Uψ2 iff exists n ≥ 1 s.t. r[n] |= ψ2 and r[m] |= ψ1 for all 1 ≤ m < n
In this paperwe focus on the pro0blemof synthesizing only the act-component of an otherwise
fully specified finite memory policy. More formally:
Definition 2.1 (Policy Synthesis Problem) Let M = (S, sinit, A, P, L) be an MDP, and πfin =
(M, start,∆, ·) be a partially specified finite-memory policy with act unspecified. Given state
formula φ, find act s.th.M, πfin, sinit |= φ if it exists, otherwise report failure.
2.2 Useful Facts About PCTL* Operators
Next we summarize some well-known or easy-to-prove facts about PCTL* operators. By the
expansion laws for the U-operator we mean the following equivalences:
ψ1 Uψ2 ≡ ψ2 ∨ (ψ1 ∧ X (ψ1 Uψ2)) ¬(ψ1 Uψ2) ≡ ¬ψ2 ∧ (¬ψ1 ∨ X¬(ψ1 Uψ2)) (E)
For ∼ ∈ {<, ≤, >, ≥} define the operators ∼ and [∼] as follows:
< = ≥ ≤ = > > = ≤ ≥ = < [<] = > [≤] = ≥ [>] = < [≥] = ≤
Some of the following equivalences cannot be used for “model checking” PCTL* (the left (P1)
equivalence, to be specific) where actions are implicitly universally quantified. In the context of
5
Markov Chains, which we implicitly have, there is no problem:
¬P∼z ψ ≡ P∼z ψ P∼z ¬ψ ≡ P[∼] 1−z ψ (P1)
P≥0 ψ ≡ true P>1 ψ ≡ false (P2)
P≤1 ψ ≡ true P<0 ψ ≡ false (P3)
P≥u P∼z ψ ≡ P∼z ψ if u , 0 P>u P∼z ψ ≡ P∼z ψ if u , 1 (P4)
P≤u P∼z ψ ≡ P≥1−u P∼z ψ P<u P∼z ψ ≡ P>1−u P∼z ψ (P5)
2.3 Nonlinear Programs
Finally, a (nonlinear) program is a set Γ of constraints of the form e1 ./ e2 where ./ ∈ {<, ≤, >, ≥
, } and e1 and e2 are arithmetic expressions comprised of numeric real constants and variables.
The numeric operators are {+,−, ·, /}, all with their expected meaning (the symbol  is equality).
All variables are implicitly bounded over the range [0, 1]. A solver (for nonlinear programs)
is a decision procedure that returns a satisfying variable assignment (a solution) for a given Γ,
and reports unsatisfiability if no solution exists. We do not further discuss solvers in the rest
of this paper, we just assume one as given. Examples of open source solvers include Ipopt and
Couenne.1
3 Tableau Calculus
3.1 Introduction and Overview
We describe a tableau based algorithm for the policy synthesis problem in Definition 2.1. Hence
assume as given an MDPM = (S, sinit, A, P, L) and a partially specified finite-memory policy
πfin = (M, start,∆, ·) with act unspecified.
A labelled formulaF is of the form 〈m, s〉 : Ψwhere 〈m, s〉 ∈ M×S andΨ is a possibly empty
set of path formulas, interpreted conjunctively. When we speak of the probability of 〈m, s〉 : Ψ
we mean the value of PrMπfin ({r ∈ Runs(〈m, s〉) | M, πfin, r |=
∧
Ψ}) for the completed πfin. For
simplicity we also call Ψ a “formula” and call 〈m, s〉 a policy state. A sequent is an expression
of the form Γ ` F where Γ is a program.
Our algorithm consists of three steps, the first one of which is a tableau construction. A
tableau for Γ ` F is a finite tree whose root is labelled with Γ ` F and such that every inner node
is labelled with the premise of an inference rule and its children are labelled with the conclusions,
in order. If Γ ` F is the label of an inner node we call F the pivot of the node/sequent/inference.
By a derivation from Γ ` F , denoted by Tableau(Γ ` F ), we mean any tableau for Γ ` F
obtained by stepwise construction, starting from a root-node only tree and applying an inference
rule to (the leaf of) every branch as long as possible. There is one inference rule, the P-rule,
which recursively calls the algorithm itself. A branch is terminated when no inference rule is
applicable, which is exactly the case when its leaf is labelled by a pseudo-sequent, detailed below.
The inference rules can be applied in any way, subject to only preference constraints.
1http://projects.coin-or.org/.
6
Given a state formula φ, the algorithm starts with a derivation from Γinit ` Finit :=
{x {φ}〈start(sinit),sinit 〉  1} ` 〈start(sinit), sinit〉 : {φ}. (The constraint Γinit forces φ to be “true”.)
The derivation represents the obligation to derive a satisfiable extension Γfinal ⊇ Γinit whose
solutions σ determine the act-component actσ of πfin such thatM, πfin, sinit |= φ. In more detail,
Γfinal will contain constraints of the form xα〈m,s〉  0 or x
α
〈m,s〉 > 0 for the probability of applying
action α in policy state 〈m, s〉. Let the policy domain of a program Γ be the set of all policy
states 〈m, s〉 ∈ M × S such that xα〈m,s〉 occurs in Γ, for some α. This lets us initially define
actσ(m, s, α) := σ(xα〈m,s〉) for every 〈m, s〉 in the policy domain of Γfinal. Only for the purpose of
satisfying the definition of finite memory policies, we then make actσ trivially total by choosing
an arbitrary distribution for actσ(m, s) for all remaining 〈m, s〉 ∈ M × S. (The latter are not
reachable and hence do not matter.) We call πfin(σ) := (M, start,∆, actσ) the policy completed
by σ.
Similarly, Γfinal contains variables of the form xΨ〈m,s〉, and σ(x
Ψ
〈m,s〉) is the probability of
〈m, s〉 : Ψ under the policy πfin(σ). (We actually need these variable indexed by tableau nodes,
see below.) If Ψ is a state formula its value will be 0 or 1, encoding truth values.
Contrary to traditional tableau calculi, the result of the computation – the extension Γfinal –
cannot always be obtained in a branch-local way. To explain, there are two kinds of branching
in our tableaux: don’t-know (non-deterministic) branching and union branching. The former
is always used for exhaustive case analysis, e.g., whether xα〈m,s〉  0 or x
α
〈m,s〉 > 0, and the
algorithm guesses which alternative to take (cf. step 2 below). The latter analyzes the Boolean
structure of the pivot. Unlike as with traditional tableaux, all children need to be expanded, and
each fully expanded branch contributes to Γfinal.
More precisely, we formalize the synthesis algorithm as a three-step procedure. Step one
consists in deriving Tableau(Γinit ` Finit). Step two consists in removing from the step one
tableau every don’t-know branching by retaining exactly one child of the parent node of the
don’t-know branching, and deleting all other children and the subtrees below them. This itself is
a don’t-know non-deterministic process; it corresponds to going down one branch in traditional
tableau. The result is denoted byChoose(T1), whereT1 is the step one tableau. Step three consists
in first building a combined program by taking the union of the Γ’s in the leaves of the branches
of the step two tableau. This program then is extended with a set of constraints by the Force
operator. More precisely, Forceing captures the situation when a run reaches a bottom strongly
connected component (BSCC). Any formula is satisfied in a BSCCwith probability 0 or 1, which
can be determined solely by qualitative formula evaluation in the BSCC. Details are below. For
now let us just defineGamma(T2) =
⋃ {Γ | Γ ` · is the leaf of a branch in T2}∪Force(T2)where
T2 = Choose(T1).
Our main results are the following. See Appendix 7 for proofs.
Theorem 3.1 (Soundness) Let M = (S, sinit, A, P, L) be an MDP, πfin = (M, start,∆, ·) be a
partially specified finite-memory policy with act unspecified, and φ a state formula. Suppose
there is a program Γfinal := Gamma(Choose(Tableau({x {φ}〈start(sinit),sinit 〉  1} ` 〈start(sinit), sinit〉 :
{φ}))) such that Γfinal is satisfiable. Let σ be any solution of Γfinal and πfin(σ) be the policy
completed by σ. Then it holdsM, πfin(σ), sinit |= φ.
7
Theorem 3.2 (Completeness) LetM = (S, sinit, A, P, L) be an MDP, πfin = (M, start,∆, act) a
finite-memory policy, and φ a state formula. SupposeM, πfin, sinit |= φ. Then there is a satisfiable
program Γfinal := Gamma(Choose(Tableau({x {φ}〈start(sinit),sinit 〉  1} ` 〈start(sinit), sinit〉 : {φ})))
and a solution σ of Γfinal such that actσ(m, s, α) = act(m, s, α) for every pair 〈m, s〉 in the policy
domain of Γfinal. MoreoverM, πfin(σ), sinit |= φ.
3.2 Inference Rules
There are two kinds of inference rules, giving two kinds of branching:
Name
Γ ` 〈m, s〉 : Ψ
Γleft ` 〈m, s〉 : Ψ Γright ` 〈m, s〉 : Ψ
if condition
(Don’t-know branching)
The pivot in the premise is always carried over into both conclusions. Only the constraint Γ is
modified into Γleft ⊇ Γ and Γright ⊇ Γ, respectively, for an exhaustive case analysis.
Name
Γ ` 〈m, s〉 : Ψ
Γ1 ` 〈m1, s1〉 : Ψ1 ∪ · · · ∪ Γn ` 〈mn, sn〉 : Ψn
if condition (n ≥ 1)
(Union branching)
All union branching rules satisfy Γi ⊇ Γ, and 〈mi, si〉 = 〈m, s〉 or 〈mi, si〉 = 〈∆(m, s), t〉 for some
state t. The ∪-symbol is decoration for distinguishing the two kinds of branching but has no
meaning beyond that. Union branching stands for the union of the runs from 〈mi, si〉 satisfying
Ψi, and computing its probability requires to develop all n child nodes.
We need to clarify a technical add-on. Let u be the tableau node with the premise pivot
〈m, s〉 : Ψ. An union branching inference extends u with children nodes, say, u1, . . . , un, with
conclusion pivots 〈mi, si〉 : Ψi. The program Γn will contain a constraint that makes a variable
(xu)Ψ〈m,s〉 for the premise dependent on all variables (xui )
Ψi
〈mi,si 〉 for the respective conclusions.
This is a key invariant and is preserved by all inference rules. In order to lighten the notation,
however, we usually drop the variable’s index, leaving the node implicit. For instance, we write
xΨ〈m,s〉 instead of (xu)
Ψ
〈m,s〉. The index u is needed for not inadvertently identifying the same pivot
at different points in the symbolic execution of a run. Fresh names x, y, z, . . . for the variables
would do as well.
Most unary union branching rules have a premise Γ ` 〈m, s〉 : {ψ} ]Ψ and the conclusion is
Γ, γone ` 〈m, s〉 : Ψ′, for some Ψ′. The pivot is specified by pattern matching, where ] is disjoint
union, and γone is a macro that expands to x
{ψ}]Ψ
〈m,s〉  x
Ψ′
〈m,s〉.
Other inference rules derive pseudo-sequents of the form Γ ` 7, Γ ` 3, Γ ` Yes-Loop and
Γ ` No-Loop. They indicate that the probability of the pivot is 0, 1, or that a loop situation arises
that may need further analysis. Pseudo-sequents are always leaves.
Now we turn to the concrete rules. They are listed in decreasing order of preference.
8
Rules for classical formulas
>
Γ ` 〈m, s〉 : {ψ} ] Ψ
Γ, γone ` 〈m, s〉 : Ψ

if ψ is clas-
sical and
L(s) |= ψ
7
Γ ` 〈m, s〉 : {ψ} ] Ψ
Γ, x {ψ}]Ψ〈m,s〉  0 ` 7

if ψ is clas-
sical and
L(s) 6|= ψ
3
Γ ` 〈m, s〉 : ∅
Γ, x∅〈m,s〉  1 ` 3
¬¬
Γ ` 〈m, s〉 : {¬¬ψ} ] Ψ
Γ, γone ` 〈m, s〉 : {ψ} ∪ Ψ
¬P
Γ ` 〈m, s〉 : {¬P∼z ψ} ] Ψ
Γ, γone ` 〈m, s〉 : {P∼z ψ} ∪ Ψ
P¬
Γ ` 〈m, s〉 : {P∼z ¬ψ} ] Ψ
Γ, γone ` 〈m, s〉 : {P[∼] 1−z ψ} ∪ Ψ
These are rules for evaluating classical formulas and for negation. The 7 rule terminates the
branch and assigns a probability of 0 to the premise pivot, as no run from 〈m, s〉 satisfies (the
conjunction of) {ψ} ]Ψ, as ψ is false in s. A similar reasoning applies to the> and 3 rules. The
¬P and P¬ rules are justified by law (P1). The P¬ rule is needed for removing negation between
P-formulas as in P∼z ¬P∼v ψ.
Rules for conjunctions
∧
Γ ` 〈m, s〉 : {ψ1 ∧ ψ2} ] Ψ
Γ, γone ` 〈m, s〉 : {ψ1, ψ2} ∪ Ψ
¬∧
Γ ` 〈m, s〉 : {¬(ψ1 ∧ ψ2)} ] Ψ
Γ ` 〈m, s〉 : {¬ψ1} ∪ Ψ ∪ Γ, γ ` 〈m, s〉 : {ψ1,¬ψ2} ∪ Ψ
where γ = x {¬(ψ1∧ψ2)}]Ψ〈m,s〉  x
{¬ψ1 }∪Ψ
〈m,s〉 + x
{ψ1,¬ψ2 }∪Ψ
〈m,s〉
These are rules for conjunction. Not both ψ1 and ψ2 can be classical by preference of the >
and 7 rules. The ∧ rule is obvious with the conjunctive reading of formula sets. The ¬∧ rule
deals, essentially, with the disjunction ¬ψ1 ∨ ¬ψ2, which requires splitting. However, unlike to
the classical logic case, ¬ψ1 ∨ ¬ψ2 represents the union of the runs from s satisfying ¬ψ1 and
the runs from s satisfying ¬ψ2. As these sets may overlap the rule works with a disjoint union
by taking ¬ψ1 on the one side, and ψ1 ∧ ¬ψ2 on the other side so that it is correct to add their
probabilities up in γ.
Rule for simplification of P-formulas
P1
Γ ` 〈m, s〉 : {P∼z ψ} ] Ψ
Γ, γone ` 〈m, s〉 : {ψ ′} ∪ Ψ
{
if P∼z ψ is the left hand side of an equivalence
(P2)-(P5) and ψ ′ is its right hand side
P2
Γ ` 〈m, s〉 : {P∼z ψ} ] Ψ
Γ, γone ` 〈m, s〉 : {ψ} ∪ Ψ
if see text
P3
Γ ` 〈m, s〉 : {P∼z ψ} ] Ψ
Γ, γone ` 〈m, s〉 : {¬ψ} ∪ Ψ
if see text
9
These are rules for simplifying P-formulas. The condition in P2 is “∼ ∈ {>, ≥} and ψ is a
state formula”, and in P3 it is “∼ ∈ {<, ≤} and ψ is a state formula”. In the rules P2 and P3
trivial cases for z are excluded by preference of P1. Indeed, this preferrence is even needed for
soundness.
The rule P2 can be explained as follows: suppose we want to know ifM, π, 〈m, s〉 |= P∼z ψ.
For that we need the probability of the set of runs from 〈m, s〉 that satisfy ψ and compare it with
z. Because ψ is a state formula this set is comprised of all runs from s ifM, π, 〈m, s〉 |= ψ, or
the empty set otherwise, giving it probability 1 or 0, respectively. With ∼ ∈ {>, ≥} conclude
M, π, s |= P∼z ψ, or its negation, respectively. The rule P3 is justified analogously. The only
difference is that ∼ ∈ {<, ≤} and so the P∼z quantifier acts as a negation operator instead of
idempotency.
At this stage, when all rules above have been applied exhaustively to a given branch, the leaf
of that branch must be of the form Γ ` 〈m, s〉 : {P∼z1 ψ1, . . . ,P∼zn ψn}, for some n ≥ 0, where
each ψi is a non-negated proper path formula.
Rules for P-formulas
P
Γ ` 〈m, s〉 : Ψ
Γ, Γ′, γleft ` 〈m, s〉 : Ψ Γ, Γ′, γright ` 〈m, s〉 : Ψ
{
if P∼z ψ ∈ Ψ, and
γleft < Γ and γright < Γ
P>
Γ ` 〈m, s〉 : {P∼z ψ} ] Ψ
Γ, γone ` 〈m, s〉 : Ψ
if γleft ∈ Γ
P7
Γ ` 〈m, s〉 : {P∼z ψ} ] Ψ
Γ, x {P∼z ψ}]Ψ〈m,s〉  0 ` 7
if γright ∈ Γ
where Γ′ = Gamma(Choose(Tableau(∅ ` 〈start(s), s〉 : {ψ}))),
γleft = x
{ψ}
〈start(s),s〉 ∼ z, and γright = x
{ψ}
〈start(s),s〉 ∼ z
Unlike classical formulas, P-formulas cannot be evaluated in a state, because their truth value
depends on the solution of the program Γfinal. The P rule analyzes P∼z ψ in a deferred way by first
getting a constraint x {ψ}〈start(s),s〉  e, for some expression e, for the probability of 〈start(s), s〉 : {ψ}
by a recursive call.2 This call is not needed if Γ already determines a truth value for P∼z ψ
because of γleft ∈ Γ or γright ∈ Γ. These tests are done modulo node labels of variables, i.e.,
(xu){ψ}〈start(s),s〉 and (xv)
{ψ}
〈start(s),s〉 are considered equal for any u, v. Because the value of e is not
known at the time of the inference, the P rule don’t-know non-deterministically branches out into
whether x {ψ}〈start(s),s〉 ∼ z holds or not, as per the constraints γleft and γright. The P> and P7 rules
then lift the corresponding case to the evaluation of P∼z ψ, which is possible now thanks to γleft
or γright.
Observe the analogy between these rules and their counterparts> and7 for classical formulas.
Note that the rules P, > and P7 cannot be combined into one, because γleft or γright could have
2By the semantics of the P-operator, the sub-derivation has to start from 〈start(s), s〉, not 〈m, s〉.
10
been added earlier, further above in the branch, or in a recursive call. In this case only P>/P7
can applied.
At this stage, in a leaf Γ ` 〈m, s〉 : Ψ the set Ψ cannot contain any state formulas, as they
would all be eliminated by the inference rules above; all formulas in Ψ now are possibly negated
X-formulas or U-formulas.
Rules for U-formulas
U
Γ ` 〈m, s〉 : {ψ1 Uψ2} ] Ψ
Γ ` 〈m, s〉 : {ψ2} ∪ Ψ ∪ Γ, γ ` 〈m, s〉 : {ψ1,¬ψ2, X (ψ1 Uψ2)} ∪ Ψ
where γ = x {ψ1 Uψ2 }]Ψ〈m,s〉  x
{ψ2 }∪Ψ
〈m,s〉 + x
{ψ1,¬ψ2,X (ψ1 Uψ2)}∪Ψ
〈m,s〉
¬U
Γ ` 〈m, s〉 : {¬(ψ1 Uψ2)} ] Ψ
Γ ` 〈m, s〉 : {¬ψ1,¬ψ2} ∪ Ψ ∪ Γ, γ ` 〈m, s〉 : {ψ1,¬ψ2,X¬(ψ1 Uψ2)} ∪ Ψ
where γ = x {¬(ψ1 Uψ2)}]Ψ〈m,s〉  x
{¬ψ1,¬ψ2 }∪Ψ
〈m,s〉 + x
{ψ1,¬ψ2,X¬(ψ1 Uψ2)}∪Ψ
〈m,s〉
These are expansion rules for U-formulas. The standard expansion law is ψ1 Uψ2 ≡ ψ2 ∨ (ψ1 ∧
X (ψ1 Uψ2)). As with the ¬∧ rule, the disjunction in the expanded formula needs to be disjoint
by taking ψ2 ∨ (ψ1 ∧ ¬ψ2 ∧ X (ψ1 Uψ2)) instead. Similarly for ¬U.
Rule for ¬X
¬X
Γ ` 〈m, s〉 : {¬Xψ} ] Ψ
Γ, γone ` 〈m, s〉 : {X¬ψ} ∪ Ψ
The ¬X rule is obvious.
At this stage, if Γ ` 〈m, s〉 : Ψ is a leaf sequent then Ψ is of the form {Xψ1, . . . ,Xψn}, for
some n ≥ 1. This is an important configuration that justifies a name: we say that a labelled
formula 〈m, s〉 : Ψ, a sequent Γ ` 〈m, s〉 : Ψ or a node labelled with Γ ` 〈m, s〉 : Ψ is poised if
Ψ is of the form {Xψ1, . . . ,Xψn} where n ≥ 1. (The notion “poised” is taken from [17].) A
poised 〈m, s〉 : {Xψ1, . . . ,Xψn} will be expanded by transition into the successor states of s by
using enabled actions α ∈ A(s). That some α is enabled does not, however, preclude a policy
with actσ(m, s, α) = 0. The rule A makes a guess whether this is the case or not:
Rules for prescribing actions
A
Γ ` 〈m, s〉 : Ψ
Γ, γleft ` 〈m, s〉 : Ψ Γ, γright ` 〈m, s〉 : Ψ
{
if Γ ` 〈m, s〉 : Ψ is poised,
α ∈ A(s), γleft < Γ and γright < Γ
where γleft = xα〈m,s〉  0 and γright = x
α
〈m,s〉 > 0
With a minor modification we get a calculus for deterministic policies. It only requires to
re-define γright as γright = xα〈m,s〉  1. As a benefit the program Γfinal will be linear.
11
After the A rule has been applied exhaustively, for each α ∈ A(s) either xα〈m,s〉 > 0 ∈ Γ
or xα〈m,s〉  0 ∈ Γ. If x
α
〈m,s〉 > 0 ∈ Γ we say that α is prescribed in 〈m, s〉 by Γ and define
Prescribed(〈m, s〉, Γ) = {α | xα〈m,s〉 > 0 ∈ Γ}.
The set of prescribed actions in a policy state determines the Succ-relation of the Markov
chain under construction. To get the requried distribution over enabled actions, it suffices to
enforce a distribution over prescribed actions, with this inference rule:
Prescribed
Γ ` 〈m, s〉 : Ψ
Γ, γα〈m,s〉 ` 〈m, s〉 : Ψ
{
if Γ ` 〈m, s〉 : Ψ is poised,
α ∈ A(s) and γα〈m,s〉 < Γ
where γα〈m,s〉 = Σα∈Prescribed(〈m,s〉,Γ) x
α
〈m,s〉  1
If the Choose operator in step two selects the leftmost branch among the A-inferences then Γfinal
contains xα〈m,s〉  0, for all α ∈ A(s). This is inconsistent with the constraint introduced by the
Prescribed-inference, corresponding to the fact that runs containing 〈m, s〉 in this case do not
exist.
Blocking
We are now turning to a “loop check” which is essential for termination, by, essentially, blocking
the expansion of certain states into successor states that do not mark progress. For that, we need
some more concepts. For two nodes u and v in a branch we say that u is an ancestor of v and
write u ≤ v if u = v or u is closer to the root than v. An ancestor is proper, written as u < v, if
u ≤ v but u , v. We say that two sequents Γ1 ` F1 and Γ2 ` F2 are indistinguishable iff F1 = F2,
i.e., they differ only in their Γ-components. Two nodes u and v are indistinguishable iff their
sequents are. We write Ψu to denote the formula component of u’s label, i.e., to say that the label
is of the form Γ ` 〈m, s〉 : Ψu; similarly for Fu to denote u’s labelled formula.
Definition 3.3 (Blocking) Let w be a poised leaf and v < w an ancestor node. If (i) v and w
are indistinguishable, and (ii) for every X-eventuality X (ψ1 Uψ2) in Ψv there is a node x with
v < x ≤ w such that ψ2 ∈ Ψx then w is yes-blocked by v. If there is an ancestor node u < v
such that (i) u is indistinguishable from v and v is indistinguishable from w (and hence u is
indistinguishable from w), and (ii) for every X-eventuality X (ψ1 Uψ2) in Ψu, if there is a node
x with ψ2 ∈ Ψx and v < x ≤ w then there is a node y with ψ2 ∈ Ψy and u < y ≤ v, then w is
no-blocked by u.
When we say that a sequent is yes/no-blocked we mean that its node is yes/no-blocked.
In the yes-blocking case all X-eventualities in Ψv become satisfied along the way from v to w.
This is why w represents a success case. In the no-blocking case some X-eventualities in Ψv
may have been satisfied along the way from u to v, but not all, as this would be a yes-blocking
instead. Moreover, no progress has been made along the way from v to w for satisfying the
missing X-eventualities. This is why w represents a failure case. The blocking scheme is adapted
from [17] for LTL satisfiability to our probabilistic case. See [18, 17] for more explanations and
examples, which are instructive also for its usage in our framework.
12
Blocking is used in the following inference rules, collectively called the Loop rules. The
node v there is an ancestor node of the leaf the rule is applied to.
Yes-Loop
Γ ` 〈m, s〉 : Ψ
Γ, xΨ〈m,s〉  (xv)
Ψ
〈m,s〉 ` Yes-Loop
if Γ ` 〈m, s〉 : Ψ is yes-blocked by v
No-Loop
Γ ` 〈m, s〉 : Ψ
Γ, xΨ〈m,s〉  (xv)
Ψ
〈m,s〉 ` No-Loop
if Γ ` 〈m, s〉 : Ψ is no-blocked by v
In either case, if v is indistinguishable from w then the probability of Fv and Fw are exactly the
same, just because Fv = Fw . This justifies adding xΨ〈m,s〉  (xv)
Ψ
〈m,s〉.
The Loop rules have a side-effect that we do not formalize: they add a link from the conclusion
node (the new leaf node) to the blocking node v, called the backlink. It turns the tableau into a
graph that is no longer a tree. The backlinks are used only for reachability analysis in step three
of the algorithm. Figure 1 has a graphical depiction.
By preference of inference rules, the X rule introduced next can be applied only if a Loop
rule does not apply. The Loop rules are at the core of the termination argument.
This argument is standard for calculi based on formula expansion, as embodied in the U and
¬U rules: the sets of formulas obtainable by these rules is a subset of an a priori determined finite
set of formulas. This set consists of all subformulas of the given formula closed under negation
and other operators. Any infinite branch hence would have to repeat one of these sets infinitely
often, which is impossible with the loop rules. Moreover, the state set S and the mode set M are
finite and so the other rules do not cause problems either.
For economy of notation, when Ψ = {ψ1, . . . , ψn}, for some ψ1, . . . , ψn and n > 0, let XΨ
denote the set {Xψ1, . . . ,Xψn}.
X
Γ ` 〈m, s〉 : XΨ
Γ ` 〈m′, t1〉 : Ψ ∪ · · · ∪ Γ ` 〈m′, tk−1〉 : Ψ ∪ Γ, γ1 ` 〈m′, tk〉 : Ψ
where m′ = ∆(m, s)
{t1, ..., tk} =
⋃
α∈Prescribed(〈m,s〉,Γ) Succ(s, α) , for some k ≥ 0
γ1 = xXΨ〈m,s〉  Σα∈Prescribed(〈m,s〉,Γ) [x
α
〈m,s〉 · (Σt∈Succ(s,α) P(t |s, α) · x
Ψ
〈m′,t 〉)]
This is the (only) rule for expansion into successor states. If u is the node the X rule is applied
to and u1, . . . , uk are its children then each ui is called an X-successor (of u).
The X rule follows the set of actions prescribed in 〈m, s〉 by Γ through to successor states.
This requires summing up the probabilities of carrying out α, as represented by xα〈m,s〉, multiplied
by the sums of the successor probabilities weighted by the respective transition probabilities.
This is expressed in the constraint γ1. Only these k successors need to be summed up, as all
other, non-prescribed successors, have probability 0.
13
3.3 Forcing Probabilities
We are now turning to the Force operator which we left open in step three of the algorithm.
/see text
Backlink
Poised node
Figure 1: An example tableau T from
step 2. The subgraph below u2 is a
strongly connected component if u10
is 7-ed.
It forces a probability 0 or 1 for certain labelled formulas
occurring in a bottom strongly connected component in
a tree from step two. The tree in the figure to the
right helps to illustrate the concepts introduced in the
following.
We need some basic notions from graph theory. A
subset M of the nodes N of a given graph is strongly
connected if, for each pair of nodes u and v in M , v
is reachable from u passing only through states in M .
A strongly connected component (SCC) is a maximally
strongly connected set of nodes (i.e., no superset of it is
also strongly connected). A bottom strongly connected
component (BSCC) is a SCC M from which no state
outside M is reachable from M .
LetT = Choose(Tableau(Γ ` F )) be a treewithout don’t-knowbranching obtained in step 2.
We wish to take T together with its backlinks as the graph under consideration and analyse its
BSCCs. However, for doing so we cannot take T as it is. On the one hand, our tableaux describe
state transitions introduced by X rule applications. Intuitively, these are amenable to BSCC
analysis as one would do for state transition systems. On the other hand, T has interspersed rule
applications for analysing Boolean structure, which distort the state transition structure. These
rule applications have to be taken into account prior to the BSCC analysis proper.
For this, we distinguish between X-links and +-links in T . An X-link is an edge between a
node and its child if the X rule was applied to the node, making its child an X-successor, otherwise
it is a +-link. (“+-link” because probabilities are summed up.)
Let u be a node in T and SubtreeT (u), or just Subtree(u), the subtree of T rooted at u without
the backlinks. We say that u is a 0-deadend (in T) if SubtreeT (u) has no X-links and every leaf
in SubtreeT (u) is 7-ed. In a 0-deadend the probabilities all add up to a zero probability for the
pivot of u. This is shown by an easy inductive argument.
Definition 3.4 (Ambiguous node) Let u be a node in T . We say that u is ambiguous (in T) iff
(i) SubtreeT (u) contains no 3-ed leaf, and (ii) SubtreeT (u) contains no X-successor 0-deadend
node. We say that u is unambiguous iff u is not ambiguous.
The main application of Definition 3.4 is when the node u is the root of a BSCCs, defined
below. The probability of u’s pivot 〈m, s〉 : Ψ then is not uniquely determined. This is because
expanding u always leads to a cycle, a node with the same pivot, and there is no escape from that
according to conditions (i) or (ii) in Definition 3.4. In other words, the propability of 〈m, s〉 : Ψ
is defined only in terms of itself.3
3In terms of the resulting program, (xu)ψ〈m,s〉 is not constrained to any specific value in [0..1]. This can be shown
by “substituting in” the equalities in Γfinal for the probabilities of the pivots in the subtree below u and arithmetic
simplifications.
14
In the figure above, the node u1 is unambiguous because of case (i) in Definition 3.4.
Assuming u10 is 3-ed, the node u2 is unambiguous by case (i). The pivot in u10, then, has
probability 1 which is propagated upwards to u4 (and enforces probability 0 for the pivot of u7).
It contributes a non-zero probability to the transition from u2 to u4 and this way esacpes a cycle.
If u10 is 7-ed, the node u2 is ambiguous.
If case (ii) inDefinition 3.4 is violated there is anX-successor nodewhose pivot has probability
0. Because every X-link has a non-zero transition probability, the probabilities obtained through
the other X-successor nodes add up to a value strictly less than 1. This also escapes the cycle
leading to underspecified programs (not illustrated above).
Let 0(T)= {w | w is a node in some 0-deadend of T} be all nodes in all 0-deadends in T . In
the example, 0(T)= {u6, u10, u8} if u10 is 7-ed and 0(T)= {u8} if u10 is 3-ed.
Let u be a node in T and M(u) = {w | w is a node in Subtree(u)} \ 0(T). That is, M(u)
consists of the nodes in the subtree rooted at u after ignoring the nodes from the 0-deadend
subtrees. In the example M(u2) = {u2, u4, u5, u7, u9, u12, u13, u14} if u10 is 7-ed. If u10 is 3-ed
then u6 and u10 have to be added.
We say that u is the root of a BSCC (in T) iff u is poised, ambiguous and M(u) is a BSCC
in T (together with the backlinks). In the example, assume that u10 is 7-ed. Then u2 is poised,
ambiguous and the root of a BSCC. In the example, that M(u2) is a BSCC in T is easy to verify.
Now suppose that u is the root of a BSCC with pivot 〈m, s〉 : XΨ. This means that the
probability of 〈m, s〉 : XΨ is not uniquely determined. This situation then is fixed by means of
the Force operation, generally defined as follows:
Bscc(T) := {u | u is the root of a BSCC in T }
Force(T) := {(xu)XΨ〈m,s〉  χ | u ∈ Bscc(T), and
if some leaf of the subtree rooted at u is a Yes-Loop
then χ = 1 else χ = 0 }
That is, Forceing removes the ambiguity for the probability of the pivot 〈m, s〉 : XΨ at the root
u of a BSCC by setting it to 1 or to 0. If Forceing adds (xu)XΨ〈m,s〉  1 then there is a run that
satisfies every X-eventuality in XΨ, by following the branch to a Yes-Loop. Because we are
looking at a BSCC, for fairness reasons, every run will do this, and infinitely often so, this way
giving XΨ probability 1. Otherwise, if there is no Yes-Loop, there is some X-eventuality in XΨ
that cannot be satisfied, forcing probability 0.
4 Conclusions and Future Work
In this paper we presented a first-of-its kind algorithm for the controller synthesis problem for
Markov Decision Processes whose intended behavior is described by PCTL* formulas. The only
restriction we had to make – to get decidability – is to require policies with finite history. We
like to propose that the description of the algorithm is material enough for one paper, and so we
leave many interesting questions for future work.
The most pressing theoretical question concerns the exact worst-case complexity of the
algorithm. Related to that, it will be interesting to specialize and analyze our framework for
15
fragments of PCTL*, such as probabilistic LTL and CTL or simpler fragments and restricted
classes of policies that might lead to linear programs (and ideally to solving only a polynomial
number of such programs). For instance, we already mentioned that computing deterministic
policies leads to linear programs in our tableau (see the description of the A inference rule how
this is done.) Moreover, it is well-known that cost-optimal stochastic policies for classes ofMDPs
with simple constraints bounding the probability of reaching a goal state can be synthesized in
linear time in the size of the MDP by solving a single linear program [1, 11]. An interesting
question is how far these simple constraints can be generalised towards PCTL* whilst remaining
in the linear programming framework (see e.g. [19]).
On implementation, a naïve implementation of the algorithm as presented above would
perform poorly in practice. However, it is easy to exploit some straightforward observations
for better performance. For instance, steps one (tableau construction) and two (committing to
a don’t-know non-deterministic choice) should be combined into one. Then, if a don’t know
non-deterministic inference rule is carried out the first time, every subsequent inference with
the same rule and pivot can be forced to the same conclusion, at the time the rule is applied.
Otherwise an inconsistent programwould result, which never needs to be searched for. Regarding
space, although all children of a union branching inference rule need to be expanded, this does
not imply they always all need to be kept in memory simultaneously. Nodes can be expanded in
a one-branch-at-a-time fashion and using a global variable for Γfinal for collecting the programs
in the leaves of the branches if they do not belong to a bottom strongly connected component.
Otherwise, the situation is less obvious and we leave it to future work. Another good source of
efficiency improvements comes from more traditional tableau. It will be mandatory to exploit
techniques such as dependency-directed backtracking, lemma learning, and early failure checking
for search space pruning.
Acknowledgements
This research was funded by AFOSR grant FA2386-15-1-4015. We would also like to thank the
anonymous reviewers for their constructive and helpful comments.
5 Additional Operators and Useful Equivalences
Additional operators can be defined on top of the temporal operator U, as usual. In particular,
false := ¬true and
Fψ := true Uψ Gψ := ¬F¬ψ ( ≡ false Rψ)
ψ1 Rψ2 := ¬(¬ψ1 U¬ψ2) ψ1 Wψ2 := (ψ1 Uψ2) ∨ Gψ1 ( ≡ ψ2 R (ψ2 ∨ ψ1))
For the “release” operator R, the formula ψ1 Rψ2 says that ψ2 remains true until and including
once ψ1 becomes true; if ψ1 never become true, ψ2 must remain true forever. Regarding the
“weak until” operator W, the formula ψ1 Wψ2 is similar to ψ1 Uψ2 but the stop condition ψ2 is
not required to occur. In that case ψ1 must remain true forever.
16
Distributivity laws:
X (ψ1 ∨ ψ2) ≡ (Xψ1) ∨ (Xψ2) X (ψ1 ∧ ψ2) ≡ (Xψ1) ∧ (Xψ2) (D1)
X (ψ1 Uψ2) ≡ (Xψ1)U (Xψ2) X (ψ1 Rψ2) ≡ (Xψ1)R (Xψ2) (D2)
F (ψ1 ∨ ψ2) ≡ (Fψ1) ∨ (Fψ2) G (ψ1 ∧ ψ2) ≡ (Gψ1) ∧ (Gψ2) (D3)
ψ U (ψ1 ∨ ψ2) ≡ (ψ Uψ1) ∨ (ψ Uψ2) (ψ1 ∧ ψ2)Uψ ≡ (ψ1 Uψ) ∧ (ψ2 Uψ) (D4)
Negation propagation laws:
¬Xψ1 ≡ X¬ψ1 ¬Gψ1 ≡ F¬ψ1 ¬Fψ1 ≡ G¬ψ1 (N1)
¬(ψ1 Uψ2) ≡ (¬ψ1 R¬ψ2) ¬(ψ1 Rψ2) ≡ (¬ψ1 U¬ψ2) (N2)
Absorption laws:
F Fψ ≡ Fψ G Gψ ≡ Gψ (A1)
F G Fψ ≡ G Fψ G F Gψ ≡ F Gψ (A2)
Expansion laws:
ψ1 Uψ2 ≡ ψ2 ∨ (ψ1 ∧ X (ψ1 Uψ2)) ¬(ψ1 Uψ2) ≡ ¬ψ2 ∧ (¬ψ1 ∨ X¬(ψ1 Uψ2)) (E)
For ∼ ∈ {<, ≤, >, ≥} define the operators ∼ and [∼] as follows:
< = ≥ ≤ = > > = ≤ ≥ = <
[<] = > [≤] = ≥ [>] = < [≥] = ≤
Some of the following equivalences cannot be used for “model checking” PCTL* (the left (P1)
equivalence, to be specific) where actions are implicitly universally quantified. In the context of
Markov Chains, which we implicitly have, there is no problem:
¬P∼z ψ ≡ P∼z ψ P∼z ¬ψ ≡ P[∼] 1−z ψ (P1)
P≥0 ψ ≡ true P>1 ψ ≡ false (P2)
P≤1 ψ ≡ true P<0 ψ ≡ false (P3)
P≥u P∼z ψ ≡ P∼z ψ if u , 0 P>u P∼z ψ ≡ P∼z ψ if u , 1 (P4)
P≤u P∼z ψ ≡ P≥1−u P∼z ψ P<u P∼z ψ ≡ P>1−u P∼z ψ (P5)
Some notes on these equivalences. The left equivalence of (P1) is trivial and the right equivalence
uses the fact that PrM,π{s · · · | M, π, s · · · |= ψ} + PrM,π{s · · · | M, π, s · · · |= ¬ψ} = 1. The
equivalences (P2) and (P3) are again trivial. For the equivalences (P4) it helps to observe that
the subformula P∼z ψ is both a state and a path formula. As a state formula it evaluates in a given
contextM, π, s to either true or false. Taken as a path formula of the outer P quantifier it hence
stands for the set of either all runs from s or the empty set, respectively. With this observation
the equivalences (P4) follow easily from the semantics of the P operator. The equivalences (P5)
are obtained by first applying the right equivalence in (P1) (from right to left) and then the left
equivalence in (P1).
17
6 Example
Consider theMDP inFigure 2 and the partially specifiedfinite-memory policy πfin = ({m}, start,∆, ·)
with a single mode m, making πfin Markovian. The functions start and ∆ hence always return m,
allowing us to abbreviate 〈m, si〉 as just si. Action β leads non-deterministically to states s2 and
s3, each with probability 0.5. The actions αi for i ∈ {1, 2, 3} are self-loops with probability one
(not shown). The label set of s2 is {a} in all other states it is empty.
The example is admittedly simple and is only from the PCTL subset of CTL*. But it suffices
to show the main aspects of the calculus.
Figure 2: The transitions of the example MDPM depicted as a graph. The initial state is s1.
Action β leads non-deterministically to states s2 and s3, each with probability 0.5. The actions
αi for i ∈ {1, 2, 3} are self-loops with probability one (not shown). The label set of s2 is {a} in
all other states it is empty.
Let the state formula of interest be φ = P≥0.3 F G a. We wish to obtain a Γfinal such that
any solution σ synthesizes a suitable actσ , i.e., the policy πfin(σ) completed by σ satisfies
M, πfin(σ), s1 |= φ.
The BSCCs depend on whether actσ(m, s1, β) > 0 holds, i.e., if β can be executed at s1. (This
is why the calculus needs to make a corresponding guess, with its A-rule.) If not, then s2 and
s3 are unreachable, and the self-loop at s1 is the only BSCC, which does not satisfy G a. If yes,
then there are two BSCCs, the self-loop at s2 and the self-loop at s3, and the BSCC at s2 satisfies
G a. By fairness of execution, with probability one some BSCC will be reached, and the BSCC
at s2 is reached with probability 0.5, hence, if actσ(m, s1, β) > 0. In other words, devising any
policy that reaches s2 will hence suffice to satisfy φ. The expected result thus is just a constraint
on σ saying actσ(m, s1, β) > 0 and the derivation will indeed show that by Chooseing a branch
with xβs1 > 0 in Γfinal.
Figures 3 to 7 summarize the derivation from the initial sequent σ1 = x
{φ}
s1  1 ` s1 : φ. In
these figures we write, for brevity, Γ ` ψ,Ψ instead of Γ ` {ψ} ] Ψ. For better readability we
write Gψ as a macro for ¬F¬ψ and Fψ for true Uψ. Please consider the captions in the figures
for further explanations on notation.
Tree ¬ in Figure 3 shows the derivation of Choose(Tableau(σ1)). It has only one branch
which ends in the leaf node u¬. Because there are no BSCCs in tree ¬, Force does not add
constraints, and therefore Γfinal = Γu¬ (the constraint system of u¬). Notice that if the derivation
had Choosen the right branch at the top of tree ¬, Γfinal would be unsatisfiable (as indicated in
Figure 3). This case is uninteresting and we do not consider it further.
Figures 4-7 in combination show the derivation from initial sequent ∅ ` s1 : F G a for
computing Γ­ in Figure 3. The dotted line in Figure 4 represents the contribution of tree ® to
18
Figure 3: Start of derivation for synthesizing a policy for the MDP in Figure 2 for the formula
P≥0.3 F G a. The links are annotated with the name of the inference rule applied. The Choosen
alternative in the P inference is the bold link, the dotted link is the non-Choosen alternative. The
leaf node of the left branch is called u¬.
tree ­ in terms of probabilities for the pivot s1 : F G a in the root of tree ­. More precisely,
the constraint Γ­ will enforce x
{F G a}
s1 = x
{X F G a}
s1 , where x
{X F G a}
s1 holds the probability of
s1 : X F G a as per tree ®. This equality holds because all branches in tree ­ are 7-ed, therefore
each contributing a value 0 to the sums in the union branches. (Figures 6 and 7 have more such
examples.)
Figure 5 has the tree ® with pivot s1 : X F G a at its root. Again the Choosen alternatives are
already highlighted, this time for the A inferences. (Chooseing the xα1s1  0 branch would also
lead to a policy, but obviously β must be prescribed in s1 for being able to reach s2.)
It is instructive to see how tree ® contributes to Γ­ a constraint for x
{X F G a}
s1 , i.e., the pivot at
its root. For that, assume that the trees annotated as BSCC with (without) Yes-Loop all contribute
a probability of one (zero, respectively) to Γ­. (We spell this out in detail only in one case, cf.
tree ¯ in Figure 6.) Combining all constraints in the leaves of tree ® then entails the following
set of equalities (the comment u1 ← u2, u3, u4 indicates the dependency of the left hand side of
19
Figure 4: First part of a derivation from ∅ ` s1 : F G a for computing Γ­ in Figure 3. For space
reasons we only write the right hand sides of sequences or only write interesting parts of the left
hand sides. The dotted line represents a dependency of probability constraints. The derivation
for tree ® is in Figure 5.
its equality from the right hand side in terms of corresponding nodes):
(xu1)
{X F G a}
s1 = x
α1
s1 · (xu2)
{F G a}
s1 + x
β
s1 · 0.5 · (xu3)
{F G a}
s2 + x
β
s1 · 0.5 · (xu4)
{F G a}
s3
(u1 ← u2, u3, u4)
(xu2)
{F G a}
s1 = (xu5)
{X F G a}
s1 (u2 ← u5)
(xu3)
{F G a}
s2 = 1 (u3)
(xu4)
{F G a}
s3 = 0 (u4)
(xu5)
{X F G a}
s1 = x
α1
s1 · (xu6)
{X F G a}
s1 + x
β
s1 · 0.5 · (xu7)
{F G a}
s2 + x
β
s1 · 0.5 · (xu8)
{F G a}
s3
(u5 ← u6, u7, u8)
(xu6)
{X F G a}
s1 = (xu9)
{X F G a}
s1 (u6 ← u9)
(xu7)
{F G a}
s2 = 1 (u7)
(xu8)
{F G a}
s3 = 0 (u8)
(xu9)
{X F G a}
s1 = (xu1)
{X F G a}
s1 (u9 ← u1)
Substituting in yields a simplified set of equalities:
(xu1)
{X F G a}
s1 = x
α1
s1 · (xu5)
{X F G a}
s1 + 0.5 · x
β
s1 (u1 ← u2, u3, u4)
(xu5)
{X F G a}
s1 = x
α1
s1 · (xu1)
{X F G a}
s1 + 0.5 · x
β
s1 (u5 ← u6, u7, u8)
20
Figure 5: Sub-derivation with Tree ® continuing Figure 4. Poised nodes are framed. The links
in the X inference are annotated with actions. An triangle with a 7 is a tree all whose leaves
are 7-ed. A triangle with links at the bottom is a tree with branches into the linked nodes and
all whose leaves are 7-ed. A triangle labelled “BSCC with/without a Yes-Loop” is a tree that
contains a BSCC with/without Yes-Loop and all non-loop leaves are 7-ed. Alternative choices
for the A rule are only indicated as dots, as they are not relevant in this example. See text for
propagation of constraints.
21
Isolating (xu1)
{X F G a}
s1 on the left hand side:
(xu1)
{X F G a}
s1 · (1 − (x
α1
s1 )
2) = 0.5 · xβs1 · (1 + x
α1
s1 )
Both α1 and β are prescribed actions thanks to the two A inferences preceding the node u1,
resulting in {xα1s1 > 0, x
β
s1 > 0} ⊂ Γ­. The X inference at u1 adds the constraint x
α1
s1 + x
β
s1  1 to
Γ­ (cf. γ2 in the definition of the X rule). It follows x
β
s1 = 1− x
α1
s1 . Substituting into the previous
equality we get
(xu1)
{X F G a}
s1 · (1 − (x
α1
s1 )
2) = 0.5 · (1 − xα1s1 ) · (1 + x
α1
s1 ) = 0.5 · (1 − (x
α1
s1 )
2)
Again from xα1s1 + x
β
s1  1 and x
β
s1 > 0 it follows x
α1
s1 < 1 and thus (1 − (x
α1
s1 )2) > 0. This allows
us to divide both sides by this term and we simply get
(xu1)
{X F G a}
s1 = 0.5
Moreover, from the simplified constraints above, we have:
(xu5)
{X F G a}
s1 = x
α1
s1 · (xu1)
{X F G a}
s1 + 0.5 · x
β
s1 = 0.5 · (x
α1
s1 + x
β
s1) = 0.5
From tree ­ we get (xu2)
{F G a}
s1 = (xu5)
{X F G a}
s1 . Substituting into x
{F G a}
s1 ≥ 0.3 from tree ¬ we
get a tautology.
Altogether, the only non-trivial constraints in Γfinal are those introduced by the various A
inferences for constraining probabilities of actions. For the concretely Choosen alternatives in
the example, this means that any solution that satisfies xα1s1 > 0 and x
β
s1 > 0 provides a policy.
Notice that only xβs1 > 0 is essential, and Chooseing the alternative x
α1
s1  0, which entails
xβs1 = 1, will do as well.
At this point it only remains to go through the derivations for tree ¯ and °. With the
explanations so far this should be straightforward.
Figure 6 has the tree ¯ for the pivot s2 : F G a. From the MDP in Figure 2 we expect it has
probability one. It is formally computed by adding the probabilities of s2 : G a, which is one,
and of s2 : ¬G a,X F G a, which is zero. The latter requires identification of a BSCC without
yes-loops, as depicted, which, hence, Forcees zero.
Similarly, Figure 7 has the tree ° for the pivot s1s2 : G a. It is an example for a BSCC with
a yes-loop. Notice that the poised pivot that forms the BSCC has no X-eventualities at all.
22
Figure 6: Sub-derivation with Tree ¯ continuing Figure 5.
23
Figure 7: Sub-derivation with Tree ° continuing Figure 6.
24
7 Proofs
Lemma 7.1 Let b be a branch in a tree with poised nodes u and w with u < w. If Ψu = Ψw then
for every poised node v with u < v < w it holds Ψv ⊇ Ψu.
Proof. Suppose Ψu = Ψw and a poised node v with u < v < w. Let x be the X-successor node
of u on b. The set Ψu is of the form {Xψ1, . . . ,Xψn} and, hence, Ψx = {ψ1, . . . , ψn}.
Now consider the available inference rules. It is straightforward to check that every inference
rule except U and ¬U either leaves the pivot 〈m, s〉 : Ψ untouched or replaces some ψ ∈ Ψ by
zero or more strictly simpler formulas. With “ψ1 is simpler than ψ2” we mean that ψ1 has strictly
less symbols than ψ2 or else the number of symbols are the same but a negation sign in ψ1 has
been pushed inwards to get ψ2 (this is needed for the ¬X rule).
The U and ¬U rules also add simpler subformulas to the conclusion, together with an X-
operator in front of the U formula or negated U formula it was applied to.
The U and ¬U rules are the only ones that can restore the X-operator applications in Ψw =
{Xψ1, . . . ,Xψn}. In other words, the ancestor nodes of w must collectively contain the formulas
ψ1, . . . , ψn in their formula sets and, furthermore, everyψi is anU-formula or a negatedU-formula.
Because it is impossible to derive a U-formula or a negated U-formula from simpler formulas
(there is just no inference rule capable of that) this means they must have been present from
the beginning. No ψi ∈ Ψx can have been replaced by simpler formulas from x down to
w. Furthermore, every ψi must have been subject to a U and ¬U inference above the node v,
otherwise v is not poised. It follows Ψv ⊇ Ψu. 
Theorem 3.1 (Soundness) Let M = (S, sinit, A, P, L) be an MDP, πfin = (M, start,∆, ·) be a
partially specified finite-memory policy with act unspecified, and φ a state formula. Suppose
there is a program Γfinal := Gamma(Choose(Tableau({x {φ}〈start(sinit),sinit 〉  1} ` 〈start(sinit), sinit〉 :
{φ}))) such that Γfinal is satisfiable. Let σ be any solution of Γfinal and πfin(σ) be the policy
completed by σ. Then it holdsM, πfin(σ), sinit |= φ.
Proof. (Sketch) Let Γfinal, σ and πfin(σ) as stated. We have to showM, πfin(σ), sinit |= φ.
We need to look at the collection of sub-derivations from the initial sequent. LetT0,T1, . . . ,Tn
be a sequence of trees such that
T0 = Choose(Tableau({x {φ}〈start(sinit),sinit 〉  1} ` 〈start(sinit), sinit〉 : {φ}))
Tj = Choose(Tableau(∅ |= 〈start(sj), sj〉 : {ψj})) for j = 1 . . . n, where Tj comes
from some P inference in the
derivation
For i = 0 . . . n we call Ti a tree from Γi ` 〈start(si), si〉 : {ψi}.
The sequence is meant to be minimal and closed under P inferences in subderivations.
Formally, it is constructed inductively: starting from T0 one takes the P inferences in T0 and
appends the trees for these P inferences. Then proceed to the next tree in the sequence, do the
same, and so on, until all sub-derivations have been processed.
25
We can accompany the sequence of trees with a sequence Γ0, Γ1, . . . , Γn, where Γi =
Gamma(Ti), for i = 0 . . . n. Together they represent a derivation from {x {φ}〈start(sinit),sinit 〉  1} `
〈start(sinit), sinit〉 : {φ} where Γfinal = Γ0.
To prove the theorem, we prove something more general. For all i = 0 . . . n, if Γ |= 〈m, s〉 : Ψ
is the label of a node in Ti then:
(i) If Ψ is a set of state formulas then v ∈ {0, 1} where v = σ(xΨ〈m,s〉). Moreover, v = 1 iff
M, πfin(σ), s |=
∧
Ψ. (And hence v = 0 iff M, πfin(σ), s 6 |=
∧
Ψ.)
(ii) If Ψ contains at least one proper path formula then
σ(xΨ〈m,s〉) = Pr
Mπfin(σ)({r ∈ RunsMπfin(σ)(〈m, s〉) | M, π, r |= ∧Ψ})
Notice that cases (i) and (ii) are exclusive and exhaustive. If Ψ = ∅ then case (i) applies.
For the proof, fix some i ∈ {0 . . . n} and let Γ |= 〈m, s〉 : Ψ be the label of a node in Ti.
Proof of (i). Suppose Ψ is a set of state formulas. We prove the conclusion by induction on
the structure of Ψ.
If Ψ = ∅ then with the 3 rule this is trivial. If there is a classical φ ∈ Ψ then we have two
cases: if L(s) |= φ (or equivalently: M, πfin(σ), s |= φ) then we get a successor node with a >
inference. The result follows by induction together with the constraint γone of the > rule.
If L(s) 6|= φ then with a 7 inference we get a constraint xΨ〈m,s〉  0 in Γfinal, which trivially
gets the result. In particular it follows M, πfin(σ), s 6 |=
∧
Ψ.
Hence suppose now every φ ∈ Ψ is non-classical. Choose one such φ. If one of the ¬¬,
¬P, P¬, ∧, ¬∧, P1, P2 or P3 inference rules is applicable, we get in each of the (one or two)
conclusions a set of state formulas. This is easy to verify by inspecting the inference rules. In
each case the result follows by induction together with the new constraints introduced by the
inference rules. We spell this out only in the most interesting case, the ¬∧ rule.
For the ¬∧ rule, the formula set Ψ is of the form {¬(ψ1 ∧ψ2)} ∪Ψ′ and φ = ¬(ψ1 ∧ψ2). For
the two conclusions we get results by induction. Notice that the left conclusion has ¬ψ1 in its
formula set {¬ψ1} ∪ Ψ′, whereas the right conclusion has the complement ψ1 in its formula set
{ψ1,¬ψ2} ∪ Ψ′. This entails for the induction results that not both M, πfin(σ), s |= {¬ψ1} ∪ Ψ′
and M, πfin(σ), s |= {ψ1,¬ψ2} ∪ Ψ′ can hold. In terms of corresponding variables this means
vl, vr ∈ {0, 1} but {vl, vr } , {1} where vl = σ(x {¬ψ1 }∪Ψ
′
〈m,s〉 ) and vr = σ(x
{ψ1,¬ψ2 }∪Ψ′
〈m,s〉 ). Finally,
with the definition of the constraint γ in the definition of the ¬∧ rule the result follows easily.
The only missing case is when φ is of the form P∼z ψ. By closure under the inference rules
with higher priority (those mentioned above) we know that ψ is a proper path formula.
If the P rule is applicable then the left conclusion adds the constraint γleft = x
{ψ}
〈m,s〉 ∼ z and
the right conclusion adds the constraint γright = x
{ψ}
〈m,s〉 h z. As the P is a don’t know rule the tree
Tk has Choosen one of them.
In any case, the P rule invokes a tableau derivation which gives us some tree Tm from
∅ ` 〈start(s), s〉 : {ψ} in the above sequence of trees. From property (ii) we get
σ(xΨ〈start(s),s〉) = Pr
Mπfin(σ)({r ∈ RunsMπfin(σ)(〈start(s), s〉) | M, π, r |= ψ})
26
In the left case, with (x {ψ}〈start(s),s〉 ∼ z) ∈ Γfinal and the semantics of the P operator it follows
M, πfin(σ), s |= P∼z ψ. (Recall that Γfinal is satisfiable.) With a, not necessarily immediately,
following P> inference the situation is the same as above, where we had the classical formula φ
such thatM, πfin(σ), s |= φ and a > inference. As above, the result follows by induction.
In the right case, with (x {ψ}〈start(s),s〉 h z) ∈ Γfinal and the semantics of the P operator it follows
M, πfin(σ), s 6 |= P∼z ψ. With a (not necessarily immediately) following P7 the result follows
trivially.
If theP rule is not applicable then Γ contains γleft or γright already, but the same argumentation
as for the left/right case applies.
This concludes the proof of property (i).
Proof of (ii). The proof is similar to the proof of (i) except for BSCCs, which requires special
consideration.
Let u be the node in Ti labelled with Γ |= 〈m, s〉 : Ψ, the sequent we are looking at.
What we do is induction on the structure of Ψ, much like in case (i). We are adding up
probabilities from the children of a union-branching to the parent node, in terms of corresponding
variables. That this is correct is clear from inspecting the design of the inference rules with
respect to the PCTL* semantics. Verifying the X rule may require a little closer look, though,
but is not too difficult. An important detail is that the branching out into X-successor nodes
happens according to prescribed actions, by preference of the A rule over the X rule. If, say,
α ∈ Prescribed(Γ, 〈m, s〉) is a prescribed action, then Γ (and hence Γfinal) contains a variable
xα〈m,s〉. It follows that the policy πfin(σ) will accordingly have actσ(m, s, α) = σ(x
α
〈m,s〉) rather
than an arbitrary choice for making it complete.
If u is also the root of a BSCC in Ti then Forceing adds xΨ〈m,s〉  1 or x
Ψ
〈m,s〉  0 to
Gamma(Ti) and hence also to Γfinal. This entails σ(xΨ〈m,s〉) = χ where χ or χ = 0, respectively.
For proving (ii) this means we need to show
PrMπfin(σ)({r ∈ RunsMπfin(σ)(〈m, s〉) | M, π, r |= ∧Ψ}) = χ (1)
Let us start by some considerations about runs in relation to the BSCC rooted at u. Consider
any run r from 〈m, s〉. We can trace r’s state transitions in Subtree(u) as follows: at a poised
inner node (such as u) select the proper X-successor node, say, v as given by the state transition
in r we are currently looking at (initially the first one, from u). Now consider Subtree(v). It
cannot be a 0-deadend by definition of ambiguity. In other words, some of its branches may lead
to a 7-ed leaf without encountering a poised inner node, but not all of them (and no branch has
a 3-ed leaf either). For any branch of the latter kind we can find the first poised node as we go
down. Let v1, . . . , vn be all these poised fringe nodes, for some n ≥ 1. We can view Subtree(v)
as an analysis of the Boolean structure until reaching a fringe node. The path component does
not advance during that, and so the fringe nodes each have the same path component as v. This
is why we can take any vi for continuing the tracing (with pvi = pv). Should vi be a leaf node
we need to follow its backlink to the inner, indistinguishable node first.
This way we can follow r stepwise in the BSCC rooted at u.
Let us consider the case χ = 1 in (1) above. That is, we have a Yes-Loop.
Should we have the freedom to construct r as we wish, we could do it in such a way that it
always passes through the (or one of the) Yes-Loop leaves. This would give us what is sometimes
27
called a “lasso”: from u go down the proper branch and find that inner node, say w (u = w is
possible), that the Yes-Loop leaf backjumps to. Then there is an initial segment from u to w
followed by circles leading back to u. The run r would just follows this “path” ad infinitum.
This situation is identified by Mark Reynolds for proving his tableau algorithm [17] correct.
(The long version [18] has proofs.) Let us call his tableau “MR tableaux”. An important
difference is that [18] is concerned with LTL satisfiability, not model checking. (A minor
difference is that we have additionally the P operator, which however behaves much like a
propositional symbol by the recursive call to the algorithm, plus induction.) For the soundness
proof he assumes as given an LTL formula and an MR tableau that has (in our words) a Yes-Loop
leaf. From the Yes-Loop he constructs a lasso run as described above, derives states and a
labelling function from it, and shows that this run satisfies the given LTL formula. He actually
has a more general construction which annotates the states in the runs with poised formula sets
and keeps track of the expansions by the X rule and all other subsequent inferences (which are
similar to ours). See the “truth lemma”, Lemma 3 in [18].
We wish to re-use the correctness results for the MR tableaux. The main difficulty is that,
of course, we are not free to construct the run, instead it is given. More precisely, the given run
r might not only follow the lasso, it may include other segments leading to No-Loops as well.
What we do know, however, is that for fairness reasons every leaf must be visited infinitely often
(recall we are dealing with a BSCC). This means that the loop in the lasso must be executed
infinitely often along r , but not necessarily consecutively.
Because of the just said the truth lemma is not immediately applicable to our run r . But
we can think of the run r as executing a modified Yes-Loop loop infinitely often, with execution
segments detouring into No-Loops spliced in. By inspecting the proof of the truth lemma it
becomes clear that this splicing-in does not hurt as long as some essential formulas are preserved
between subsequent poised nodes. These formulas are used to establish that U-formulas or
negated U-formulas, stemming from an X inference to a poised node, are satisfied by the run in
the MR tableau. See Lemmas 2 and 3 in [18]. That these formulas are indeed preserved by our
runs as well then follows from Lemma 7.1 above: if there are additional poised nodes spliced
into the Yes-Loops, they are equal to or supersets of the original poised nodes and, therefore,
harmless. In other words, it can be shown that our run r satisfies Ψ.
Finally as our run r is chosen arbitrarily, every run from last(p) satisfiesΨ. As the probability
of the set of all runs is one, we have shown (1) for the case χ = 1.
It remains to consider the case χ = 0. That is, we have no Yes-Loop in the BSCC rooted at
u. Again we refer to the proofs in [18]. This time it is actually easier. The completeness proof
in [18] (Lemma 5) needs to analyze an arbitrary given run and trace it down the branches in the
MR tableau, much like our run r is allowed to walk down any branch and return to an inner node
from its leaves (which are all No-Loop). So this situation is a better match.
The main argument, in brief, is that there must be an X-eventuality in Ψ that remains
unsatisfied along r , making Ψ unsatisfied by r , if there is no Yes-Loop. For this it is shown that
the loop-check does not prematurely cut branches. In brief, when a leaf is a No-Loop the branch
leading to it repeatedly failed to satisfy an X-eventuality and it would not help to make this same
mistake twice.
Finally as our run r is chosen arbitrarily, no run from 〈m, s〉 satisfies Ψ. As the probability
28
of the empty set of runs is zero, we have shown (1) for the case χ = 0.
This concludes the proof of property (ii).
Finally it is easy to prove the conclusion M, πfin(σ), sinit |= φ of the theorem. Be-
cause φ is a state formula we can apply property (i) to the tree T0 from {x {φ}〈start(sinit),sinit 〉 
1} ` 〈start(sinit), sinit〉 : {φ} and conclude v ∈ {0, 1} where v = σ(x {φ}〈start(sinit),sinit 〉). With
x {φ}〈start(sinit),sinit 〉  1 ∈ Γfinal trivially v = 1. By property (i) againM, πfin(σ), sinit |= φ. 
Theorem 3.2 (Completeness) LetM = (S, sinit, A, P, L) be an MDP, πfin = (M, start,∆, act) a
finite-memory policy, and φ a state formula. SupposeM, πfin, sinit |= φ. Then there is a satisfiable
program Γfinal := Gamma(Choose(Tableau({x {φ}〈start(sinit),sinit 〉  1} ` 〈start(sinit), sinit〉 : {φ})))
and a solution σ of Γfinal such that actσ(m, s, α) = act(m, s, α) for every pair 〈m, s〉 in the policy
domain of Γfinal. MoreoverM, πfin(σ), sinit |= φ.
Proof. (Sketch.) Let φ be as stated and suppose there is a finite-memory policy πfin such that
M, πfin, sinit |= φ.
A preliminary: a binding is a pair(x, v), usually written as x 7→ v, where x is a variable
and v ∈ [0, 1]. A substitution is a finite set of bindings. The set dom(σ) = {x | (x 7→ v) ∈
σ for some v } is called the domain ofσ. We use substitutions to construct solutions of programs.
The proof plan is to construct Tableau({x {φ}〈start(sinit),sinit 〉  1} ` 〈start(sinit), sinit〉 : {φ}) in
a similar fashion as one would do in the soundness proof for classical tableau, by analysing φ
syntactically and going down branches. It is a bit trickier in the completeness case, though. We
have to Choose along the way as needed for complying with the prescribed actions in the given
policy πfin and the P-formulas. We also have to show that the Gamma operations results in a
satisfiable Γfinal. The rest of the proof spells this out in more detail.
We need something general to keep the induction going, as follows.
As in the soundness proof we work with a sequence of trees T0,T1, . . . ,Tn such that
T0 = Choose(Tableau({x {φ}〈start(sinit),sinit 〉  1} ` 〈start(sinit), sinit〉 : {φ}))
Tj = Choose(Tableau(∅ |= 〈start(sj), sj〉 : {ψj})) for j = 1 . . . n, where Tj comes
from some P inference in the
derivation
Again, for i = 0 . . . n we call Ti a tree from Γi ` 〈start(si), si〉 : {ψi}, and we accompany
the sequence of trees with a sequence Γ0, Γ1, . . . , Γn, where Γi = Gamma(Ti), for i = 0 . . . n.
Together they represent a derivation from {x {φ}〈start(sinit),sinit 〉  1} ` 〈start(sinit), sinit〉 : {φ} where
Γfinal = Γ
0.
Unlike as in the soundness proof, these sequences are not given a priori. Indeed, we have to
show they exist. We do this iteratively with the help of a couple of variables, collectively called
the induction variables:
• The sequence of the Ti’s, initialized with a one-tree sequence T0 with a root node only
labelled with {x {φ}〈start(sinit),sinit 〉  1} ` 〈start(sinit), sinit〉 : {φ}
29
• The sequence of the Γi’s initialized with a one-program sequence
Γ0 = {x {φ}〈start(sinit),sinit 〉  1}.
• A current substitution σ, initialized with
σ = {xα〈m,s〉 7→ act(m, s, α) | 〈m, s〉 ∈ M × S and α ∈ A(s)} ∪ {x
{φ}
〈start(sinit),sinit 〉 7→ 1}
To extend this initial state to a derivation we pick any leaf in any Ti and apply any inference rule
to it, subject only to preference constraints. (This freedom is needed to match the claim that the
inference rules can be applied in a don’t-care fashion, subject only to the preference constraints.)
On applying an inference rule we will show that the following invariant is preserved: if
Γ ` 〈m, s〉 : Ψ is the label of a node in some Ti then all of the following holds:
(i) xψ〈m,s〉 ∈ dom(σ) and σ is a solution of Γj , for all j = 0 . . . n.
(ii) actσ(m, s, α) = act(m, s, α) for every 〈m, s〉 ∈ M×S such that 〈m, s〉 is in the policy-domain
of Γ0.
(iii) if Ψ is a set of state formulas then
M, πfin(σ), s |=
∧
Ψ iff M, πfin, s |=
∧
Ψ iff σ(xΨ〈start(s),s〉) = 1.
Moreover σ(xΨ〈start(s),s〉) ∈ {0, 1}.
(iv) if Ψ contains at least one proper path formula then
σ(xΨ〈m,s〉) = Pr
Mπfin(σ)({r ∈ RunsMπfin(σ)(〈m, s〉) | M, πfin(σ), r |=
∧
Ψ})
= PrMπfin ({r ∈ RunsMπfin (〈m, s〉) | M, πfin, r |=
∧
Ψ})
We refer to (i)–(iv) collectively as the invariant.
The invariant holds initially. This is trivial for each (i)–(iv). We will show below that
the invariant is preserved by applying an inference rule, anywhere. This requires updating the
induction variables appropriately. This process will end in the announced derivation, and, by
construction, Γ0 will be Γfinal.
Once this is done, the theorem is proved easily:
• That Γfinal is satisfiable is trivial with (i).
• That actσ(m, s, α) = act(m, s, α) for every m × s in the policy-domain of Γfinal becomes
identical to (ii).
• M, πfin(σ), sinit |= φ follows from (iii) andM, πfin, sinit |= φ as given.
Hence it only remains to prove the invariant.
Choose some Ti and some leaf in Ti arbitrarily. Let Γ ` 〈m, s〉 : Ψ be the leaf’s label. Assume
the invariant holds for Γ ` 〈m, s〉 : Ψ. Choose any inference rule applicable to Ψ, not violating
preference constraints, and apply it. Then show that the invariant holds afterwards.
30
In the first case Ψ is a set of state formulas. Partition Ψ = {φ} ]Ψ′ to match the form of the
inference rule.
If the rule is>, add γone = x {φ}]Ψ
′
〈m,s〉  x
Ψ′
〈m,s〉 to Γi. All inference rules are mutually exclusive
for a fixed selected formula φ. Thus, if Γi , or any other Γj for i , j, already contains an equality
for x {φ}]Ψ
′
〈m,s〉 it must be the same γone. In this case there is nothing to show for (i) and (ii) to carry
over. If, otherwise γone is fresh then we extend σ by the new binding x
{φ}]Ψ′
〈m,s〉 7→ σ(x
Ψ′
〈m,s〉).
Because x {φ}]Ψ
′
p is fresh the new binding cannot interfere with those already in the domain of σ
before the binding was added. In any case
σ(x {φ}]Ψ
′
〈m,s〉 ) = σ(x
Ψ′
〈m,s〉) . (2)
The applicability condition of > tells us L(s) |= φ. With the semantics of PCTL* it follows
M, πfin(σ), s |= φ ∧
∧
Ψ′ iffM, πfin(σ), s |=
∧
Ψ′ (3)
M, πfin, s |= φ ∧
∧
Ψ′ iffM, πfin, s |=
∧
Ψ′ (4)
Substituting (2), (3) and (4) into the iff-chain in (iii) gives
M, πfin, s |=
∧
Ψ′ iff M, πfin(σ), s |=
∧
Ψ′ iff σ(xΨ′〈start(s),s〉) = 1.
which proves (iii) for the new leaf. For (iv) there is nothing to show. This concludes the proof
for the case that the > rule is applied.
The proofs for the inference rules 7, 3, ¬¬, ¬P, P¬, ∧, ¬∧, P1, P2, P3 all are analogous to
the > rule. We do not carry them out in detail. Notice again (as in the soundness proof) that in
the ¬∧ rule the left and right conclusions are mutually exclusive, which enables correctly taking
the sum of the corresponding variables.
It remains to consider the inference rules for P-formulas. The rules are Q, P> and P7.
If P is applicable then there is a formula P∼z φ ∈ Ψ such that γleft < Γi and γright < Γi.
The P inference starts a new derivation from ∅ ` 〈start(s), s〉 : {φ}. In terms of our proof it
starts with a tree Tk for ∅ ` 〈start(s), s〉 : {φ}, for some k > i. The Γ′ mentioned in the P-rule
is Γk after that derivation has finished. Hence assume that this derivation (and possibly further
sub-derivations) have already been carried out. We get as a result new trees starting from Tk and
new programs starting from Γk and the invariant will be preserved by induction. Property (i)
then tells us that σ is a solution of Γj , for all j = 0 . . . n. Now, with Γ ⊆ Γi (by construction, we
add all Γ’s in the pivots as we encounter them) and the fact that in the P inference the program Γ′
is just Γk it follows that σ is a solution for Γ∪ Γ′ and we could add Γ∪ Γ′ to Γi without affecting
property (i).
The conclusion of the P-inference, however, is either Γ∪Γ′∪ {γleft} or Γ∪Γ′{γright}. Notice
we already have the tree Tk for ∅ ` 〈start(s), s〉 : {φ}. By invariant (iv) σ will contain a binding
for x {φ}〈start(s),s〉 (which is x
{φ}
〈start(s),s〉 7→ Pr
Mπfin ({r ∈ RunsMπfin (〈start(s), s〉) | M, π, r |= φ})) This
binding satisfies γleft or γright and, hence, prescribes Chooseing the left or the right conclusion
for extending Ti so that invariant (i) is preserved. Properties (ii), (iii) and (iv) are all trivially
preserved by the extension.
31
The open cases are the >and the P7 rules. Their proofs are analogous to the >and the 7
rules and are omitted.
This completes the proof for the case that Ψ is a set of state formulas. Hence assume now
that Ψ contains at least one state formula. The relevant invariant properties are (i), (ii) and (iv).
The proofs are analogous to the ones when Ψ is a set of state formulas. Essentially, the
invariant follows from the design of the inference rules. This holds true also for the X-rule, which
is a bit tedious to inspect. (Essentially one has to verify that it correctly reflects the transition
probability function of the Markov chainMπfin , cf. Section 2.) Two important issues, though.
First, the A inferences are preceding the X-inferences. Unlike as in the soundness proof,
the A inferences must be carried out consistently with the given policy πfin. More precisely, we
Choose the left conclusion xα〈m,s〉  0 in an A inference if act(m, s, α) = 0, and we Choose the
right conclusion xα〈m,s〉 > 0 otherwise. The variable x
α
〈m,s〉 is already in the domain of σ because
a binding to the value xα〈m,s〉 7→ act(m, s, α) was put there initially. This shows that the invariant
is preserved for A inferences.
At the end of the derivation all bindings for the variables xα〈m,s〉 such that 〈m, s〉 is not in the
policy domain of any Tj can be removed from σ. This allows us to prove the missing detail of
invariant (ii).
Second, BSCCs and Forceing. Suppose the node we are looking at is u, the one with the
sequent Γ ` 〈m, s〉 : Ψ. The derivation adds a constraint xψ〈m,s〉  χ for some χ ∈ {0, 1} if u is
the root of a BSCC in Ti. We need to argue for the correctness of doing that.
That u is the root of a BSCC has a meaning in terms of the Markov Chain induced by the
MDPM and the policy πfin: 〈m, s〉 is an entry node into a BSCC in the state transition diagram
of this Markov Chain. The state transitions are given by the actions prescribed by πfin, and
the same actions are prescribed for the state transitions in the BSCC rooted at u. This follows
from the A inferences as just explained. This is why the runs from 〈m, s〉 in the Markov Chain
are exactly the same as the runs from 〈m, s〉 starting from u in Ti by going down branches and
following backlinks. Because in BSCCs in Markov Chains either all (fair) runs or no run satisfy
Ψ, the probability PrMπfin ({r ∈ RunsMπfin (〈m, s〉) | M, πfin, r |=
∧
Ψ}) is either 1 or 0. By the
correctness of Forceing (cf. the soundness proof) the corresponding constraint xΨ〈m,s〉  1 or
xΨ〈m,s〉  0 will be correctly added in Γi. This is sufficient to prove the invariant (iv). 
References
[1] E. Altman. Constrained Markov Decision Processes, Volume 7. CRC Press, 1999.
[2] C. Baier, M. Größer, M. Leucker, B Bollig, and F. Ciesinski. Controller Synthesis for
Probabilistic Systems. In TCS2004, 2004.
[3] C. Baier and J. Katoen. Principles of Model Checking. MIT Press, 2008.
[4] T. Brázdil, V. Brozek, V. Forejt, and A. Kucera. Stochastic Games With Branching-time
Winning Objectives. In 21th IEEE Symp. on Logic in Computer Science LICS, 2006.
32
[5] T. Brázdil and V. Forejt. Strategy Synthesis for Markov Decision Processes and Branching-
time Logics. In 18th Int. Conf. on Concurrency Theory CONCUR, 2007.
[6] T. Brázdil, V. Forejt, and A. Kucera. Controller Synthesis and Verification for Markov
Decision Processes With Qualitative Branching Time Objectives. In ICALP, 2008.
[7] T. Brázdil, A. Kucera, and O. Strazovský. On the Decidability of Temporal Properties of
Probabilistic Pushdown Automata. In STACS, 2005.
[8] C. Courcoubetis andM. Yannakakis. The Complexity of Probabilistic Verification. J. ACM,
42(4):857–907, 1995.
[9] X. C.Ding, A. Pinto, andA. Surana. Strategic PlanningUnderUncertainties via Constrained
Markov Decision Processes. In IEEE Int. Conf. on Robotics and Automation ICRA, 2013.
[10] X. C. Ding, S. Smith, C. Belta, and D. Rus. Optimal Control of Markov Decision Processes
With Linear Temporal Logic Constraints. IEEE Trans. Automat. Contr., 59(5):1244–1257,
2014.
[11] D. Dolgov and E. Durfee. Stationary Deterministic Policies for Constrained Mdps With
Multiple Rewards, Costs, and Discount Factors. In IJCAI, 2005.
[12] V. Forejt, M. Z. Kwiatkowska, G. Norman, and D. Parker. Automated Verification Tech-
niques for Probabilistic Systems. In SFM, 2011.
[13] J. Kemeny, J. Snell, and A. Knapp. Denumerable Markov Chains: With a Chapter of
Markov Random Fields by David Griffeath, volume 40. Springer, 2012.
[14] A. Kucera and O. Strazovský. On the Controller Synthesis for Finite-state Markov Decision
Processes. In Theoretical Computer Science, 2005.
[15] M. Z. Kwiatkowska, G. Norman, and D. Parker. Stochastic Model Checking. In SFM,
2007.
[16] M. Z. Kwiatkowska and D. Parker. Automated Verification and Strategy Synthesis for
Probabilistic Systems. In ATVA, 2013.
[17] M. Reynolds. A New Rule for LTL Tableaux. In GandALF, 2016.
[18] M. Reynolds. A Traditional Tree-style Tableau for LTL. CoRR, abs/1604.03962, 2016.
[19] J. Sprauel, A. Kolobov, and F. Teichteil-Königsbuch. Saturated Path-constrained MDP:
Planning Under Uncertainty and Deterministic Model-checking Constraints. In AAAI,
2014.
[20] M. Svorenová, I. Cerna, and C. Belta. Optimal Control of Mdps With Temporal Logic
Constraints. In CDC, 2013.
[21] F. Trevizan, S. Thiébaux, P. Santana, and B. Williams. Heuristic Search in Dual Space for
Constrained Stochastic Shortest Path Problems. In ICAPS, 2016.
33

