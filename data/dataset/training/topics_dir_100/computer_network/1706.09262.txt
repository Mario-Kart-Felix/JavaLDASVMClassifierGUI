Hierarchical Attentive Recurrent Tracking
Adam R. Kosiorek
Department of Engineering Science
University of Oxford
adamk@robots.ox.ac.uk
Alex Bewley
Department of Engineering Science
University of Oxford
bewley@robots.ox.ac.uk
Ingmar Posner
Department of Engineering Science
University of Oxford
ingmar@robots.ox.ac.uk
Abstract
Class-agnostic object tracking is particularly difficult in cluttered environments
as target specific discriminative models cannot be learned a priori. Inspired by
how the human visual cortex employs spatial attention and separate “where” and
“what” processing pathways to actively suppress irrelevant visual features, this
work develops a hierarchical attentive recurrent model for single object tracking
in videos. The first layer of attention discards the majority of background by
selecting a region containing the object of interest, while the subsequent layers
tune in on visual features particular to the tracked object. This framework is
fully differentiable and can be trained in a purely data driven fashion by gradient
methods. To improve training convergence, we augment the loss function with
terms for a number of auxiliary tasks relevant for tracking. Evaluation of the
proposed model is performed on two datasets of increasing difficulty: pedestrian
tracking on the KTH activity recognition dataset and the KITTI object tracking
dataset.
1 Introduction
In computer vision, the task of class-agnostic object tracking is challenging since no target-specific
model can be learnt a priori and yet the model has to handle target appearance changes, varying
lighting conditions and occlusion. To make it even more difficult, the tracked object often constitutes
but a small fraction of the visual field. The remaining parts may contain distractors, which are visually
salient objects resembling the target but hold no relevant information. Despite this fact, recent models
often process the whole image, exposing them to noise and increases in associated computational
cost or use heuristic methods to decrease the size of search regions. This in contrast to human visual
perception, which does not process the visual field in its entirety, but rather acknowledges it briefly
and focuses on processing small fractions thereof, which we dub visual attention.
Attention mechanisms have recently been explored in machine learning in a wide variety of contexts
[27, 14], often providing new capabilities to machine learning algorithms [11, 12, 7]. While they
improve efficiency [22] and performance on state-of-the-art machine learning benchmarks [27],
their architecture is much simpler than that of the mechanisms found in the human visual cortex
[5]. Attention has also been long studied by neuroscientists [18], who believe that it is crucial for
visual perception and cognition [4], since it is inherently tied to the architecture of the visual cortex
and can affect the information flow inside it. Whenever more than one visual stimulus is present
in the receptive field of a neuron, all the stimuli compete for computational resources due to the
limited processing capacity. Visual attention can lead to suppression of distractors, by reducing
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
ar
X
iv
:1
70
6.
09
26
2v
1 
 [
cs
.C
V
] 
 2
8 
Ju
n 
20
17
(a) (b) (c)
Figure 1: KITTI image with
the ground-truth and predicted
bounding boxes and an atten-
tion glimpse. The lower row
corresponds to the hierarchi-
cal attention of our model:
1st layer extracts an attention
glimpse (a), the 2nd layer uses
appearance attention to build
a location map (b). The 3rd
layer uses the location map
to suppress distractors, visu-
alised in (c).
the size of the receptive field of a neuron and by increasing sensitivity at a given location in the
visual field (spatial attention). It can also amplify activity in different parts of the cortex, which are
specialised in processing different types of features, leading to response enhancement w. r. t. those
features (appearance attention). The functional separation of the visual cortex is most apparent in
two distinct processing pathways. After leaving the eye, the sensory inputs enter the prefrontal cortex
(known as V1) and then split into the dorsal stream, responsible for estimating spatial relationships
(where), and the ventral stream, which targets appearance-based features (what).
Inspired by the general architecture of the human visual cortex and the role of attention mechanisms,
this work presents a biologically-inspired recurrent model for single object tracking in videos (cf.
section 3). Tracking algorithms typically use simple motion models and heuristics to decrease the size
of the search region. It is interesting to see whether neuroscientific insights can aid our computational
efforts, thereby improving the efficiency and performance of single object tracking. It is worth noting
that visual attention can be induced by the stimulus itself (due to, e. g., high contrast) in a bottom-up
fashion or by back-projections from other brain regions and working memory as top-down influence.
The proposed approach exploits this property to create a feedback loop that steers the three layers of
visual attention mechanisms in our hierarchical attentive recurrent tracking (HART) framework, see
Figure 1. The first stage immediately discards spatially irrelevant input, while later stages focus on
producing deictic filters to emphasise visual features particular to the object of interest.
By factoring the problem into its constituent parts, we arrive at a familiar statistical domain; namely
that of maximum likelihood estimation (MLE). This follows from our interest in estimating the
distribution over object locations, in a sequence of images, given the initial location from whence our
tracking commenced. Formally, given a sequence of images x1:T ∈ RH×W×3 and an initial location
for the tracked object given by a bounding box b1 ∈ R4, the conditional probability distribution
factorises as
p(b2:T | x1:T ,b1) = =
∫
p(h1 | x1,b1)
T∏
t=2
∫
p(bt | ht)p(ht | xt,bt−1,ht−1) dht dh1, (1)
where we assume that motion of an object can be described by a Markovian state ht. Our bounding
box estimates are given by b̂2:T , found by the MLE of the model parameters. In sum, our contributions
are threefold: Firstly, a hierarchy of attention mechanisms that leads to suppressing distractors and
computational efficiency is introduced. Secondly, a biologically plausible combination of attention
mechanisms and recurrent neural networks is presented for object tracking. Finally, our attention-
based tracker is demonstrated using real-world sequences in challenging scenarios where previous
recurrent attentive trackers have failed.
Next we briefly review related work before describing how information flows through the components
of our hierarchical attention in Section 3. Section 3 details the losses applied to guide attention.
Section 5 presents experiments on KTH, KITTI and ImageNet video datasets with comparison to
related neural network based trackers. Section 6 discusses the results and intriguing properties of our
framework and Section 7 concludes the work. Code and results will be available online1.
1https://github.com/akosiorek/hart
2
2 Related Work
A number of recent studies have demonstrated that visual content can be captured through a sequence
of spatial glimpses or foveation [22, 12]. Such a paradigm has the intriguing property that the
computational complexity is proportional to the number of steps as opposed to the image size.
Furthermore, the fovea centralis in the retina of primates is structured with maximum visual acuity
in the centre and decaying resolution towards the periphery, Cheung et al. [4] show that if spatial
attention is capable of zooming, a regular grid sampling is sufficient. Jaderberg et al. [14] introduced
the spatial transformer network (STN) which provides a fully differentiable means of transforming
feature maps, conditioned on the input itself. Eslami et al. [7] use the STN as a form of attention
in combination with a recurrent neural network (RNN) to sequentially locate and identify objects
in an image. Moreover, Eslami et al. [7] use a latent variable to estimate the presence of additional
objects, allowing the RNN to adapt the number of time-steps based on the input. Our spatial
attention mechanism is based on the two dimensional Gaussian grid filters of [16] which is both fully
differentiable and more biologically plausible than the STN.
Whilst focusing on a specific location has its merits, focusing on particular appearance features might
be as important. A policy with feedback connections can learn to adjust filters of a convolutional
neural network (CNN), thereby adapting them to features present in the current image and improving
accuracy [25]. De Brabandere et al. [6] introduced dynamic filter network (DFN), where filters for a
CNN are computed on-the-fly conditioned on input features, which can reduce model size without
performance loss. Karl et al. [17] showed that an input-dependent state transitions can be helpful
for learning latent Markovian state-space system. While not the focus of this work, we follow this
concept in estimating the expected appearance of the tracked object.
In the context of single object tracking, both attention mechanisms and RNNs appear to be perfectly
suited, yet their success has mostly been limited to simple monochromatic sequences with plain
backgrounds [16]. Cheung [3] applied STNs [14] as attention mechanisms for real-world object
tracking, but failed due to exploding gradients potentially arising from the difficulty of the data. Ning
et al. [23] achieved competitive performance by using features from an object detector as inputs to a
long-short memory network (LSTM), but requires processing of the whole image at each time-step.
Two recent state-of-the-art trackers employ convolutional Siamese networks which can be seen as
an RNN unrolled over two time-steps [13, 26]. Both methods explicitly process small search areas
around the previous target position to produce a bounding box offset [13] or a correlation response
map with the maximum corresponding to the target position [26]. We acknowledge the recent work2
of Gordon et al. [10] which employ an RNN based model and use explicit cropping and warping as a
form of non-differentiable spatial attention. The work presented in this paper is closest to [16] where
we share a similar spatial attention mechanism which is guided through an RNN to effectively learn
a motion model that spans multiple time-steps. The next section describes our additional attention
mechanisms in relation to their biological counterparts.
3 Hierarchical Attention
Inspired by the architecture of the human visual cortex, we structure our system around working
memory responsible for storing the motion pattern and an appearance description of the tracked object.
If both quantities are known, it would be possible to compute the expected location of the object at
the next time step. Given a new frame, however, it is not immediately apparent which visual features
correspond to the appearance description. If we were to pass them on to an RNN, it would have to
implicitly solve a data association problem. As it is non-trivial, we prefer to model it explicitly by
outsourcing the computation to a separate processing stream conditioned on the expected appearance.
This results in a location-map, making it possible to neglect features inconsistent with our memory of
the tracked object. We now proceed with describing the information flow in our model.
Given attention parameters at, the spatial attention module extracts a glimpse gt from the input
image xt. We then apply appearance attention, parametrised by appearance αt and comprised of
V1 and dorsal and ventral streams, to obtain object-specific features vt, which are used to update
the hidden state ht of an LSTM. The LSTM’s output is then decoded to predict both spatial and
appearance attention parameters for the next time-step along with a bounding box correction ∆b̂t for
2[10] only became available at the time of submitting this paper.
3
xt
Spatial
Attention
gt V1
Dorsal
Stream
Ventral
Stream
st
νt
 vt
ht−1
LSTM
ht
ot
(
s̃t
ot
)
MLP
αt+1
∆b̂t
at+1
Figure 2: Hierarchical Attentive Recurrent Tracking Framework. Spatial attention extracts a glimpse
gt from the input image xt. V1 and the ventral stream extract appearance-based features while
the dorsal stream computes a foreground and background segmentation of the attention glimpse
st. Masked features vt contribute to the working memory ht. The LSTM output ot is then used
to compute attention at+1, appearance αt+1 and a bounding box correction ∆b̂t. Dashed lines
correspond to temporal connections, while solid lines describe information flow within one time-step.
gt SharedCNN
DFN
CNN  vt
αt−1
Figure 3: Architecture of the appearance attention. V1 is im-
plemented as a CNN shared among the dorsal stream (DFN)
and the ventral stream (CNN). The  symbol represents
the Hadamard product and implements masking of visual
features by a locaiton map.
the current time-step. Spatial attention is driven by top-down signal at, while appearance attention
depends on top-down αt as well as bottom-up (contents of the glimpse gt) signals. Bottom-up
signals have local influence and depend on stimulus salience at a given location, while top-down
signals incorporate global context into local processing. This attention hierarchy, further enhanced by
recurrent connections, mimics that of the human visual cortex [18]. We now describe the individual
components of the system.
Spatial Attention Our spatial attention mechanism is similar to the one used by Kahoú et al. [16].
Given an input image xt ∈ RH×W , it creates two matrices Axt ∈ Rw×W and A
y
t ∈ Rh×H ,
respectively. Each matrix contains one Gaussian per row; the width and positions of the Gaussians
determine which parts of the image are extracted as the attention glimpse. Formally, the glimpse
gt ∈ Rh×w is defined as
gt = A
y
t xt (A
x
t )
T
. (2)
Attention is described by centres µ of the Gaussians, their variances σ2 and strides γ between centers
of Gaussians of consecutive rows of the matrix, one for each axis. In contrast to the work by Kahoú
et al. [16], only centres and strides are estimated from the hidden state of the LSTM, while the
variance depends solely on the stride. This prevents excessive aliasing during training caused when
predicting a small variance (w. r. t. strides) leading to smoother convergence. The relationship between
variance and stride is approximated using linear regression with polynomial basis functions (up to
4th order) before training the whole system. The glimpse size we use depends on the experiment.
Appearance Attention This stage transforms the attention glimpse gt into a fixed-dimensional
vector vt comprising appearance and spatial information about the tracked object. Its architecture
depends on the experiment. In general, however, we implement V1 : Rh×w → Rhv×wv×cv as a
number of convolutional and max-pooling layers. They are shared among later processing stages,
which corresponds to the primary visual cortex in humans [5]. Processing then splits into ventral and
dorsal streams. The ventral stream is implemented as a CNN, and handles visual features and outputs
feature maps νt. The dorsal stream, implemented as a DFN, is responsible for handling spatial
relationships. Let MLP(·) denote a multi-layered perceptron. The dorsal stream uses appearance αt
to dynamically compute convolutional filters ψa×b×ct , where the superscript denotes the size of the
filters and the number of feature maps, as
Ψt =
{
ψhi×bi×cit
}K
i=1
= MLP(αt). (3)
4
The filters with corresponding nonlinearities form K convolutional layers applied to the output of V1.
Finally, a convolutional layer with a 1× 1 kernel and a sigmoid non-linearity is applied to transform
the output into a spatial Bernoulli distribution st. Each value in st represents the probability of the
tracked object occupying the corresponding location.
The location map of the dorsal stream is combined with appearance-based features extracted by the
ventral stream, to imitate the distractor-suppressing behaviour of the human brain. It also prevents
drift and allows occlusion handling, since object appearance is not overwritten in the hidden state
when input does not contain features particular to the tracked object. Outputs of both streams are
combined as3
vt = MLP(vec(νt  st)), (4)
with  being the Hadamard product.
State Estimation Our approach relies upon being able to predict future object appearance and
location, and therefore it heavily depends upon state estimation. We use an LSTM, which can learn to
trade-off spatio-temporal and appearance information in a data-driven fashion. It acts like a working
memory, enabling the system to be robust to occlusions and oscillating object appearance e. g., when
an object rotates and comes back to the original orientation.
ot,ht = LSTM(vt,ht−1), (5)
αt+1,∆at+1,∆b̂t = MLP(ot, vec(st)), (6)
at+1 = at + tanh(c)∆at+1, (7)
b̂t = at + ∆b̂t (8)
Equations (5) to (8) detail the state updates. Spatial attention at time t is formed as a cumulative sum
of attention updates from times t = 1 to t = T , where c is a learnable parameter initialised to a small
value to constrain the size of the updates early in training. Since the spatial-attention mechanism
is trained to predict where the object is going to go (section 4), the bounding box b̂t is estimated
relative to attention at time t.
4 Loss
We train our system by minimising a loss function comprised of a: tracking loss term, a set of terms
for auxiliary tasks and regularisation terms. Auxiliary tasks are essential for real-world data, since
convergence does not occur without them. They also speed up learning and lead to better performance
for simpler datasets. Unlike the auxiliary tasks used by Jaderberg et al. [15], ours are relevant for our
main objective — object tracking. In order to limit the number of hyperparameters, we automatically
learn loss weighting. The loss L(·) is given by
LHART(D, θ) = λtLt(D, θ) + λsLs(D, θ) + λaLa(D, θ) +R(λ) + βR(D, θ), (9)
with dataset D =
{
(x1:T ,b1:T )
i
}M
i=1
, network parameters θ, regularisation terms R(·), adaptive
weights λ = {λt, λs, λd} and a regularisation weight β. We now present and justify components of
our loss, where expectations E[·] are evaluated as an empirical mean over a minibatch of samples{
xi1:T ,b
i
1:T
}M
i=1
, where M is the batch size.
Tracking To achieve the main tracking objective (localising the object in the current frame), we
base the first loss term on Intersection-over-Union (IoU) of the predicted bounding box w. r. t. the
ground truth, where the IoU of two bounding boxes is defined as IoU(a, b) = a∩ba∪b =
area of overlap
area of union .
The IoU is invariant to object and image scale, making it a suitable proxy for measuring the quality
of localisation. Even though it (or an exponential thereof) does not correspond to any probability
3vec : Rm×n → Rmn is the vecorisation operator, which stacks columns of a matrix into a column vector.
5
time
Figure 4: Tracking results on KTH dataset [24]. Starting with the first initialisation frame where all
three boxes overlap exactly, time flows from left to right showing every 16th frame of the sequence
captured at 25fps. The colour coding follows from Figure 1. The second row shows attention glimpses
multiplied with appearance attention.
distribution (as it cannot be normalised), it is often used for evaluation [20]. We follow the work by
Yu et al. [28] and express the loss term as the negative log of IoU:
Lt(D, θ) = Ep(b̂1:T |x1:T ,b1)
[
− log IoU(b̂t,bt)
]
, (10)
with IoU clipped for numerical stability.
Spatial Attention Spatial attention singles out the tracked object from the image. To estimate its
parameters, the system has to predict the object’s motion. In case of an error, especially when the
attention glimpse does not contain the tracked object, it is difficult to recover. As the probability of
such an event increases with decreasing size of the glimpse, we employ two loss terms. The first
one constrains the predicted attention to cover the bounding box, while the second one prevents it
from becoming too large, with logarithmic arguments are appropriately clipped to avoid numerical
instabilities:
Ls(D, θ) = Ep(a1:T |x1:T ,b1)
[
− log
(
at ∩ bt
area(bt)
)
− log(1− IoU(at,xt))
]
. (11)
Appearance Attention The purpose of appearance attention is to suppress distractors while keeping
full view of the tracked object e. g., focus on a particular pedestrian moving within a group. To guide
this behaviour, we put a loss on appearance attention that encourages picking out only the tracked
object. Let τ(at,bt) : R4 × R4 → {0, 1}hv×wv be a target function. Given the bounding box b and
attention a, it outputs a binary mask of the same size as the output of V1. The mask corresponds to
the the glimpse g, with the value equal to one at every location where the bounding box overlaps
with the glimpse and equal to zero otherwise. If we take H(p, q) = −
∑
z p(z) log q(z) to be the
cross-entropy, the loss reads
La(D, θ) = Ep(a1:T ,s1:T |x1:T ,b1)[H(τ(at,bt), st)]. (12)
Regularisation We apply the L2 regularisation to the model parameters θ and to the expected value
of dynamic parameters ψt(αt) as R(D, θ) = 12‖θ‖
2
2 +
1
2
∥∥Ep(α1:T |x1:T ,b1)[Ψt | αt]∥∥22.
Adaptive Loss Weights To avoid hyper-parameter tuning, we follow the work by Kendall et al. [19]
and learn the loss weighting λ. After initialising the weights with a vector of ones, we add the
following regularisation term to the loss function: R(λ) = −
∑
i log(λ
−1
i ).
5 Experiments
5.1 KTH Pedestrian Tracking
Kahoú et al. [16] performed a pedestrian tracking experiment on the KTH activity recognition dataset
[24] as a real-world case-study. We replicate this experiment for comparison. We use code provided
by the authors for data preparation and we also use their pre-trained feature extractor. Unlike them,
we did not need to upscale ground-truth bounding boxes by a factor of 1.5 and then downscale
them again for evaluation. We follow the authors and set the glimpse size (h,w) = (28, 28). We
6
Figure 5: IoU curves on KITTI over 60 timesteps. HART
(train) presents evaluation on the train set (we do not overfit).
Method Avg. IoU
Kahoú et al. [16] 0.14
Spatial Att 0.60
App Att 0.78
HART 0.81
Table 1: Average IoU on KITTI
over 60 time-steps.
replicate the training procedure exactly, with the exception of using the RMSProp optimiser [9] with
learning rate of 3.33 × 10−5 and momentum set to 0.9 instead of the stochastic gradient descent
with momentum. The original work reported an IoU of 55.03% on average, on test data, while the
presented work achieves an average IoU score of 77.11%, reducing the relative error by almost a
factor of two. Figure 4 presents qualitative results.
5.2 Scaling to Real-World Data: KITTI
Since we demonstrated that pedestrian tracking is feasible using the proposed architecture, we proceed
to evaluate our model in a more challenging multi-class scenario on the KITTI dataset [8]. It consists
of 21 high resolution video sequences with multiple instances of the same class posing as potential
distractors. We split all sequences into 80/20 sequences for train and test sets, respectively. As images
in this dataset are much more varied, we implement V1 as the first three convolutional layers of a
modified AlexNet [1]. The original AlexNet takes inputs of size 227× 227 and downsizes them to
14× 14 after conv3 layer. Since too low resolution would result in low tracking performance, and
we did not want to upsample the extracted glimpse, we decided to replace the initial stride of four
with one and to skip one of the max-pooling operations to conserve spatial dimensions. This way,
our feature map has the size of 14× 14× 384 with the input glimpse of size (h,w) = (56, 56). We
apply dropout with probability 0.25 at the end of V1. The ventral stream is comprised of a single
convolutional layer with a 1 × 1 kernel and five output feature maps. The dorsal stream has two
dynamic filter layers with kernels of size 1× 1 and 3× 3, respectively and five feature maps each. We
used 100 hidden units in the RNN with orthogonal initialisation and Zoneout [21] with probability
set to 0.05. The system was trained via curriculum learning [2], by starting with sequences of length
five and increasing sequence length every 13 epochs, with epoch length decreasing with increasing
sequence length. We used the same optimisation settings, with the exception of the learning rate,
which we set to 3.33× 10−6.
Table 1 and Figure 5 contain results of different variants of our model and of the RATM tracker by
Kahoú et al. [16] related works. Spatial Att does not use appearance attention, nor loss on attention
parameters. App Att does not apply any loss on appearance attention, while HART uses all described
modules; it is also our biggest model with 1.8 million parameters. Qualitative results in the form of a
video with bounding boxes and attention are available online 4. We implemented the RATM tracker
of Kahoú et al. [16] and trained with the same hyperparameters as our framework, since both are
closely related. It failed to learn even with the initial curriculum of five time-steps, as RATM cannot
integrate the frame xt into the estimate of bt (it predicts location at the next time-step). Furthermore,
it uses feature-space distance between ground-truth and predicted attention glimpses as the error
measure, which is insufficient on a dataset with rich backgrounds. It did better when we initialised its
feature extractor with weights of our trained model but, despite passing a few stags of the curriculum,
it achieved very poor final performance.
7
(a) The model with appearance attention loss (top) learns to focus on the tracked object, which prevents an ID
swap when a pedestrian is occluded by another one (bottom).
(b) Three examples of glimpses and locations maps for a model with and without appearance loss (left to right).
Attention loss forces the appearance attention to pick out only the tracked object, thereby suppressing distractors.
Figure 6: Glimpses and corresponding location maps for models trained with and without appearance
loss. The appearance loss encourages the model to learn foreground/background segmentation of the
input glimpse.
6 Discussion
The experiments in the previous section show that it is possible to track real-world objects with
a recurrent attentive tracker. While similar to the tracker by Kahoú et al. [16], our approach uses
additional building blocks, specifically: (i) bounding-box regression loss, (ii) loss on spatial attention,
(iii) appearance attention with an additional loss term, and (iv) combines all of these in a unified
approach. We now discuss properties of these modules.
Spatial Attention Loss prevents Vanishing Gradients Our early experiments suggest that using
only the tracking loss causes an instance of the vanishing gradient problem. Early in training, the
system is not able to estimate object’s motion correctly, leading to cases where the extracted glimpse
does not contain the tracked object or contains only a part thereof. In such cases, the supervisory
signal is only weakly correlated with the model’s input, which prevents learning. Even when the
object is contained within the glimpse, the gradient path from the loss function is rather long, since
any teaching signal has to pass to the previous timestep through the feature extractor stage. Penalising
attention parameters directly seems to solve this issue.
Is Appearance Attention Loss Necessary? Given enough data and sufficiently high model capacity,
appearance attention should be able to filter out irrelevant input features before updating the working
memory. In general, however, this behaviour can be achieved faster if the model is constrained
to do so by using an appropriate loss. Figure 6 shows examples of glimpses and corresponding
location maps for a model with and without loss on the appearance attention. In fig. 6a the model
with loss on appearance attention is able to track a pedestrian even after it was occluded by another
human. Figure 6b shows that, when not penalised, location map might not be very object-specific and
can miss the object entirely (left-most figure). By using the appearance attention loss, we not only
improve results but also make the model more interpretable.
Spatial Attention Bias is Always Positive To condition the system on the object’s appearance and
make it independent of the starting location, we translate the initial bounding box to attention parame-
ters, to which we add a learnable bias, and create the hidden state of LSTM from corresponding visual
features. In out experiments, this bias always converged to positive values favouring attention glimpse
slightly larger than the object bounding box. It suggests that, while discarding irrelevant features is
desirable for object tracking, the system as a whole learns to trade off attention responsibility between
the spatial and appearance based attention modules.
4https://youtu.be/Vvkjm0FRGSs
8
7 Conclusion
Inspired by the cascaded attention mechanisms found in the human visual cortex, this work presented
a neural attentive recurrent tracking architecture suited for the task of object tracking. Beyond
the biological inspiration, the proposed approach has a desirable computational cost and increased
interpretability due to location maps, which select features essential for tracking. Furthermore, by
introducing a set of auxiliary losses we are able to scale to challenging real world data, outperforming
predecessor attempts and approaching state-of-the-art performance. Future research will look into
extending the proposed approach to multi-object tracking, as unlike many single object tracking, the
recurrent nature of the proposed tracker offer the ability to attend each object in turn.
References
[1] A. Krizhevsky, I. Sutskever, and Geoffrey E. Hinton. ImageNet Classification with Deep Convolu-
tional Neural Networks. In NIPS, pages 1097–1105, 2012. URL https://papers.nips.cc/paper/
4824-imagenet-classification-with-deep-convolutional-neural-networks.
[2] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the 26th Annual International Conference on Machine Learning - ICML ’09, pages 1–8,
New York, New York, USA, 2009. ACM Press. ISBN 9781605585161. doi: 10.1145/1553374.1553380.
URL http://portal.acm.org/citation.cfm?doid=1553374.1553380.
[3] Brian Cheung. Neural Attention for Object Tracking. In GPU Technology Confer-
ence, 2016. URL http://on-demand.gputechconf.com/gtc/2016/presentation/
s6497-brian-cheung-neural-attention-for-object-tracking.pdf.
[4] Brian Cheung, Eric Weiss, and Bruno Olshausen. Emergence of foveal image sampling from learning to
attend in visual scenes. Arxiv, pages 1–9, 2016. URL http://arxiv.org/abs/1611.09430.
[5] Peter. Dayan and L. F. Abbott. Theoretical neuroscience : computational and mathematical modeling of
neural systems. Massachusetts Institute of Technology Press, 2001. ISBN 9780262041997.
[6] Bert De Brabandere, Xu Jia, Tinne Tuytelaars, and Luc Van Gool. Dynamic Filter Networks. Arxiv,
abs/1605.0:1–14, 2016. URL http://arxiv.org/abs/1605.09673.
[7] S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray Kavukcuoglu,
and Geoffrey E. Hinton. Attend, Infer, Repeat: Fast Scene Understanding with Generative Models. In
Neural Information Processing Systems, mar 2016. URL http://arxiv.org/abs/1603.08575.
[8] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The KITTI dataset. The Inter-
national Journal of Robotics Research, 32(11):1231–1237, sep 2013. ISSN 0278-3649. doi: 10.1177/
0278364913491297. URL http://ijr.sagepub.com/cgi/doi/10.1177/0278364913491297.
[9] Hinton Geoffrey, Nitish Srivastava, and Kevin Swersky. Overview of mini-batch gradi-
ent descent, 2012. URL http://www.cs.toronto.edu/{%}5C{~}tijmen/csc321/slides/
lecture{%}5C{_}slides{%}5C{_}lec6.pdf.
[10] Daniel Gordon, Ali Farhadi, and Dieter Fox. Re3 : Real-Time Recurrent Regression Networks for Object
Tracking. In arXiv:1705.06368, may 2017. URL http://arxiv.org/abs/1705.06368.
[11] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska,
Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adrià Puigdomènech
Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summer-
field, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network
with dynamic external memory. Nature, 538(7626):471–476, oct 2016. ISSN 0028-0836. URL http:
//dx.doi.org/10.1038/nature20101http://10.0.4.14/nature20101http://www.nature.
com/nature/journal/v538/n7626/abs/nature20101.html{#}supplementary-information.
[12] K Gregor, I Danihelka, A Graves, and D Wierstra. DRAW: A Recurrent Neural Network For Image
Generation. arXiv preprint arXiv: . . . , abs/1502.0:11, 2015. URL http://arxiv.org/abs/1502.
04623.
[13] David Held, Sebastian Thrun, and Silvio Savarese. Learning to track at 100 FPS with deep regression
networks. In European Conference on Computer Vision Workshop, volume 9905 LNCS, pages 749–765.
Springer, 2016. ISBN 9783319464473. doi: 10.1007/978-3-319-46448-0_45.
[14] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial Transformer
Networks. In Nips, pages 1–14, 2015. ISBN 9781627480031. doi: 10.1038/nbt.3343.
[15] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and
Koray Kavukcuoglu. Reinforcement Learning with Unsupervised Auxiliary Tasks. In arXiv:1611.05397,
nov 2016. ISBN 2004012439. doi: 10.1051/0004-6361/201527329. URL http://arxiv.org/abs/
1611.05397.
[16] Samira Ebrahimi Kahoú, Vincent Michalski, and Roland Memisevic. RATM: Recurrent Attentive Tracking
Model. Iclr, pages 1–9, 2016. URL http://arxiv.org/pdf/1510.08660v3.pdf.
[17] Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick van der Smagt. Deep Variational Bayes
Filters: Unsupervised Learning of State Space Models from Raw Data. In International Conference
9
on Learning Representation, 2017. URL http://arxiv.org/abs/1605.06432https://arxiv.org/
pdf/1605.06432.pdf.
[18] Sabine Kastner and Leslie G Ungerleider. Mechanisms of visual attention in the human cortex. Annual
Reviews of Neuroscience, 23(1):315–341, 2000. ISSN 0147-006X. doi: 10.1146/annurev.neuro.23.1.315.
[19] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-Task Learning Using Uncertainty to Weigh Losses
for Scene Geometry and Semantics. arXiv:1705.07115, may 2017. URL http://arxiv.org/abs/1705.
07115.
[20] Matej Kristan, Jiri Matas, Aleš Leonardis, Michael Felsberg, Luk Cehovin, Gustavo Fernández, Tomáš
Vojí, Gustav Häger, Georg Nebehay, Roman Pflugfelder, Abhinav Gupta, Adel Bibi, Alan Lukežič,
Alvaro Garcia-Martin, Amir Saffari, Philip H S Torr, Qiang Wang, Rafael Martin-Nieto, Rengarajan
Pelapur, Richard Bowden, Chun Zhu, Stefan Becker, Stefan Duffner, Stephen L Hicks, Stuart Golodetz,
Sunglok Choi, Tianfu Wu, Thomas Mauthner, Tony Pridmore, Weiming Hu, Wolfgang Hübner, Xi-
aomeng Wang, Xin Li, Xinchu Shi, Xu Zhao, Xue Mei, Yao Shizeng, Yang Hua, Yang Li, Yang
Lu, Yuezun Li, Zhaoyun Chen, Zehua Huang, Zhe Chen, Zhe Zhang, Zhenyu He, and Zhibin Hong.
The Visual Object Tracking VOT2016 challenge results. In European Conference on Computer Vision
Workshop, 2016. URL http://personal.ee.surrey.ac.uk/Personal/R.Bowden/publications/
2016/Lebeda{_}VOT2016.pdf.
[21] David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary
Ke, Anirudh Goyal, Yoshua Bengio, Aaron Courville, and Chris Pal. Zoneout: Regularizing RNNs by
Randomly Preserving Hidden Activations. In International Conference on Learning Representations, jun
2017. URL http://arxiv.org/abs/1606.01305.
[22] Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. Recurrent Models of Visual
Attention. In Neural Information Processing Systems, pages 1–9, 2014. ISBN 078036404X. doi: ng. URL
http://arxiv.org/abs/1406.6247.
[23] Guanghan Ning, Zhi Zhang, Chen Huang, Zhihai He, Xiaobo Ren, and Haohong Wang. Spatially
Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking. arXiv preprint
arXiv:1607.05781, jul 2016. URL http://arxiv.org/abs/1607.05781.
[24] Christian Schuldt, Ivan Laptev, and Barbara Caputo. Recognizing human actions: A local SVM approach. In
Proceedings - International Conference on Pattern Recognition, volume 3, pages 32–36. IEEE, 2004. ISBN
0769521282. doi: 10.1109/ICPR.2004.1334462. URL http://ieeexplore.ieee.org/document/
1334462/.
[25] Marijn Stollenga, Jonathan Masci, Faustino Gomez, and Juergen Schmidhuber. Deep Networks with
Internal Selective Attention through Feedback Connections. In arXiv preprint arXiv: . . . , page 13, 2014.
URL http://arxiv.org/abs/1407.3068.
[26] Jack Valmadre, Luca Bertinetto, João F. Henriques, Andrea Vedaldi, and Philip H. S. Torr. End-to-end
representation learning for Correlation Filter based tracking. In Computer Vision and Pattern Recognition,
apr 2017. URL http://arxiv.org/abs/1704.06036.
[27] Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. Grammar as
a Foreign Language. In Neural Information Processing Systems, sep 2015. ISBN 9789078328414. doi:
10.1146/annurev.neuro.26.041002.131047. URL http://arxiv.org/abs/1409.0473http://arxiv.
org/abs/1412.7449.
[28] Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, and Thomas Huang. UnitBox: An Advanced
Object Detection Network. In Proceedings of the 2016 ACM on Multimedia Conference, pages 516–520.
ACM, 2016. ISBN 978-1-4503-3603-1. doi: 10.1145/2964284.2967274. URL http://doi.acm.org/
10.1145/2964284.2967274.
10

