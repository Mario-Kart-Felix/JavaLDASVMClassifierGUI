NEAR SHANNON LIMIT ERROR - CORRECTING 
CODING AND DECODING : TURBO-CODES (1) 
Claude Berrou, Alain Glavieux and Punya Thitimajshima 
Claude Berrou, Integrated Circuits for Telecommunication Laboratory 
Alain Glavieux and Punya Thitimajshima, Digital Communication Laboratory 
Ecole Nationale SuNrieure des Ttlkommunications de Bretagne, France 
(1) Patents No 9105279 (France), No 92460011.7 (Europe), No 07/870,483 (USA) 
Abstract - This paper deals with a new class of convolutional 
codes called Turbo-codes, whose performances in terms of 
Bit Error Rate (BER) are close to the SHANNON limit. The 
Turbo-Code encoder is built using a parallel concatenation of 
two Recursive Systematic Convolutional codes and the 
associated decoder, using a feedback decoding rule, is 
implemented as P pipelined identical elementary decoders. 
I - INTRODUCTION 
Consider a binary rate R=1/2 convolutional encoder with 
constraint length K and memory M=K-1. The input to the 
encoder at time k is a bit dk and the corresponding codeword 
Ck is the binary couple ( X k ,  Y k )  with 
K -I 
i = O  
x k  = z g I i d k - ;  m d . 2  g l i  =0,1 (la) 
K-1 
Yk = zg2idk-i m d . 2  82i =0,1 ( I b )  
i = O  
where GI: {gl i ) ,  G2: ( g 2 i  } are the two encoder generators, 
generally expressed in octal form. 
It is well known, that the BER of a classical Non 
Systematic Convolutional (NSC) code is lower than that of a 
classical Systematic code with the same memory M at large 
SNR. At low SNR, it is in general the other way round. The 
new class of Recursive Systematic Convolutional (RSC) 
codes, proposed in this paper, can be better than the best NSC 
code at any SNR for high code rates. 
A binary rate R=1/2 RSC code is obtained from a 
NSC code by using a feedback loop and setting one of the 
two outputs Xk  or Yk equal to the input bit dk. For an RSC 
code, the shift register (memory) input is no longer the bit dk 
but is a new binary variable ak. If Xk=dk (respectively 
Yk=dk), the output Y k  (resp. X k )  is equal to equation (lb) 
(resp. la) by substituting ak for dk and the variable ak is 
recursively calculated as 
K - 1  
ak = dk + 1 ria,-; m d . 2  ( 2 )  
i = l  
where yi is respectively equal to gli  if Xk=dk and to g 2 i  if 
Yk=dk. Equation (2) can be rewritten as 
K -1 
i = O  
dk = Z 7 ; U k - i  tmd.2. (3) 
One RSC encoder with memory M=4 obtained from an NSC 
encoder defined by generators G1=37, G2=21 is depicted in 
Fig. 1. 
Generally, we assume that the input bit dk takes 
values 0 or 1 with the same probability. From equation (2), 
we can show that variable ak exhibits the same statistical 
property 
0-7803-0950-2/93/$3.00Q1993IEEE 
106.1 
Pr{ak =El,..ak-l =Ek-l)=Pr(dk = E ) = 1 / 2  (4) 
with E is equal to 
K -1 
I=1 
E = C y i e i  m d . 2  E =0,1. ( 5 )  
Thus the trellis structure is identical for the RSC 
code and the NSC code and these two codes have the same 
free distance df However, the two output sequences (Xk} and 
{ Yk ) do not correspond to the same input sequence (dk) for 
RSC and NSC codes. This is the main difference between the 
two codes. 
When punctured code is considered, some output 
bits X k  or Y k  are deleted according to a chosen puncturing 
pattern defined by a matrix P .  For instance, starting from a 
rate R=1/2 code, the matrix P of rate 2/3 punctured code is 
L J  
Fig. 1 a Classical Non Systematic code. Yk 
Fig. 1 b Recursive Systematic code. 'k 
Authorized licensed use limited to: Zhejiang University. Downloaded on November 13, 2008 at 09:33 from IEEE Xplore.  Restrictions apply.
I1 - PARALLEL CONCATENATION OF RSC CODES 
With RSC codes, a new concatenation scheme, 
called parallel concatenation can be used. In Fig. 2, an 
example of two identical RSC codes with parallel 
concatenation is shown. Both elementary encoder (Cl and 
C2) inputs use the same bit dk but according to a different 
sequence due to the presence of an interleaver. For an input 
bit sequence { &), encoder outputs Xk and Yk at time k are 
respectively equal to dk (systematic encoder) and to encoder 
C1 output Y l k ,  or to encoder C2 output Y2k. If the coded 
outputs ( Y l k ,  Y 2 k )  of encoders C1 and C 2  are used 
respectively nl times and n2  times and so on, the encoder C1 
rate R 1 and encoder C2 rate R2 are equal to 
(6) 
n, + n ,  R, = 
2n, +nl * 
*k 
Systemab 
Code (37,21) 
I 
Fig. 2 Recursive Systematic codes 
with parallel concatenation. 
The decoder DEC depicted in Fig. 3a, is made up of two 
elementary decoders (DEC1 and DEC2) in a serial 
concatenation scheme. The first elementary decoder DECl is 
associated with the lower rate R1 encoder C1 and yields a 
soft (weighted) decision. The error bursts at the decoder 
DECl output are scattered by the interleaver and the encoder 
delay L1 is inserted to take the decoder DECl delay into 
account. Parallel concatenation is a very attractive scheme 
because both elemenmy encoder and decoder use a single 
frequency clock. 
For a discrete memoryless gaussian channel and a 
binary modulation, the decoder DEC input is made up of a 
couple Rk of two random variables nk and yk, at time k 
xk = ( 2 4  - 1) + i ,  ( 7 4  
Y, =w, -u+q , ,  (76) 
where ik and qk are two independent noises with the same 
variance 02. The redundant information yk is demultiplexed 
and sent to decoder DECl when Yk = Y l k  and toward decoder 
DEC2 when Yk =Y2k. When the redundant information of a 
given encoder (Cl or C2) is not emitted, the corresponding 
decoder input is set to zero. This is performed by the 
DEMUXDNSERTION block. 
It is well known that soft decoding is better than 
hard decoding, therefore the first decoder DECl must deliver 
to the second decoder DEC2 a weighted (soft) decision. The 
Logarithm of Likelihood Ratio (LLR), Al(dk ) associated 
with each decoded bit dk by the first decoder DECl is a 
relevant piece of information for the second decoder DEC2 
(8) 
P, {dk = 1 /observution) 
" (dk' = Log P, {dk = O/observution) * 
where P,{dk = i  lobservation), i = 0, 1 is the a posteriori 
probability (APP) of the data bit dk. 
- _ -  
DEMUW 
INSERTION 
Fig. 3a Principle of the decoder according to 
a serial concatenation scheme. 
I11 - OPTIMAL DECODING OF RSC CODES WITH 
WEIGHTED DECISION 
The VITJZRBI algorithm is an optimal decoding 
method which minimizes the probability of sequence error 
for convolutional codes. Unfortunately this algorithm is not 
able to yield the APP for each decoded bit. A relevant 
algorithm for this purpose has been proposed by BAHL et al. 
[l]. This algorithm minimizes the bit error probability in 
decoding linear block and convolutional codes and yields the 
APP for each decoded bit. For RSC codes, the BAHL et al. 
algorithm must be modified in order to take into account their 
recursive character. 
I11 - 1 Modified BAHL el al. algorithm for RSC codes 
Consider a RSC code with constraint length K; at 
time k the encoder state Sk is represented by a K-uple 
Also suppose that the information bit sequence (dk )  is made 
up of N independent bits dk, taking values 0 and 1 with equal 
probability and that the encoder initial state So and final state 
SN are both equal to zero, i.e 
The encoder output codeword sequence, noted 
gaussian memoryless channel whose output is the sequence 
R: = (R,. ....... R~ ........ RN ) where Rk =(Xk,yd is defined by 
relations (7a) and (7b). 
s& = (u&.a&-, ....... a,_,+, ). (9) 
so = sN= (0, 0 ...... 0) = 0. (10) 
Cy = {C, ....... Ck ........ CN)is the input to a discrete 
1065 
Authorized licensed use limited to: Zhejiang University. Downloaded on November 13, 2008 at 09:33 from IEEE Xplore.  Restrictions apply.
The APP of a decoded data bit dk can be derived 
from the joint probability AL(m) defined by 
and thus, the APP of a decoded data bit dk is equal to 
Pr[dk =i /R;”]=Z.  Ai(m) ,  i = o , L  (12) 
From relations (8) and (12), the LLR A (dk ) associated with 
a decoded bit dk can be written as 
n ’ , ( m ) = e { d k  = i , s k  = m / R Y ]  (11) 
m 
m 
Finally the decoder can make a decision by comparing 
A (dk ) to a threshold equal to zero 
d k = l  if A ( d k ) > o  
& = o  $ A ( d k ) < o .  (14)  
In order to compute the probability A i ( m ) ,  let us introduce 
the probability functions a; ( m )  , p& ( m )  and yi( Rk, m’, m )  
Thus we obtain 
(19) 
Taking into account that events after time k are not 
influenced by observation R: and bit dk if state Sk is known, 
the probability &(m)  is equal 
A’,(m) = al(m)Pk(m). (20) 
The probabilities a;(m)  and &(m)  can be recursively 
calculated from probability yi(Rk, m’, m ) .  From annex I, we 
obtain 
a;(m)= m‘i=O 
1 
Z Zyi(Rk,m’,m)a:-,(m’) 
zz z z yi(Rk,m’,m)af_,(m’) (21) 1 1  
m m’i=Oj=O 
and 
~Yi(R&+i,m,m’)P&+i(m’) 
* (22) pk(m)=  m’i=O 1 1  
EZ Z. Z.yi(Rk+l,m’,m)ai(m’) 
m m‘i=Oj=O 
The probability yi(Rk,m’, m )  can be determined from 
transition probabilities of the discrete gaussian memoryless 
channel and transition probabilities of the encoder trellis. 
From relation (17), yi (R,, m’, m)  is given by 
Y;(Rk,m’,m)=p(Rk/dk =i,& =m,sk-1 = m ’ )  
q(dk = i / s k  = m,s&-l = m’)z(s& = m/S&-l = m’) (23) 
where p(./.) is the transition probability of the discrete 
gaussian memoryless channel. Conditionally to 
(dk = i, Sk = m ,  Sk-1 = m ’), xk and y k  are two uncorrelated 
gaussian variables and thus we obtain 
p(Rk /dk = i, s k  = m, sk-1 = m ’) = 
p(xk  /dk = i, s k  = m, Sk-1 = m ’) 
p ( y &  /dk = i, s& = m, Sk-1 = m’). (24)  
Since the convolutional encoder is a deterministic machine, 
q(dk = i /Sk  =m,S&-] = m ’ )  is equal to 0 or 1. The 
transition state probabilities x(sk = “/s&-l = m’) of the 
trellis are defined by the encoder input statistic. 
Generally, P, (dk = 1) = P, idk = 0) = 1/2 and since there 
are two possible transitions from each 
state,n(Sk = m/Sk-l = m’) = 1/2 for each of these 
transitions. 
Different steps of modified BAHL et al. algorithm 
initialized according to relation (12) 
-Step 0 : Probabilities a;(m) and P N ( m )  are 
a i (0 )=1  a;(m)=O h # O ,  i = O , l  (25a) 
PN(0) = 1 p”(m) = 0 Vm # 0. 05b)  
-Step 1 : For each observation Rk. the probabilities 
af(nz) and yi(Rk,m’, m )  are computed using relations (21) 
and (23) respectively. 
-Step 2 : When the sequence R;“ has been 
completely received, probabilities p k  ( m )  are computed using 
relation (22), and probabilities a;(m)  and &(m) are 
multiplied in order to obtain &(d. Finally the LLR 
associated with each decoded bit dk is computed from 
relation ( 1 3). 
IV- THE EXTRINSIC INFORMATION OF THE RSC 
DECODER 
In this chapter, we will show that the LLR h(dk) 
associated with each decoded bit dk , is the sum of the LLR 
of dk at the decoder input and of another information called 
extrinsic information, generated by the decoder. 
Using the LLR A(dk) definition (13) and relations (20) and 
(21), we obtain 
A(dk)  = Log ” J - T 0  . (26)  
Since the encoder is systematic (Xk = dk), the transition 
probability p(xk / d k  = i, s& = m,Sk-, = m’) in expression 
yi(Rk,m’,m) is independent of state values Sk and Sk-1. 
Therefore we can factorize this transition probability in the 
numerator and in the denominator of relation (26) 
1 
CC Cy1(Rk,m’,m)a:_,(m’)pk(m) 
ZZ m m ‘ j = O  C m’, m)a:-,(m’)p,(m) 
1066 
Authorized licensed use limited to: Zhejiang University. Downloaded on November 13, 2008 at 09:33 from IEEE Xplore.  Restrictions apply.
feedback loop 
I 
- 
, 
_ - - .  leaving 
DEMUW 
INSERTION 
JI 
decoded output 
Fig. 3b Feedback decoder (under 0 internal delay assumption). Gk 
V-1 Decoding with a feedback loop 
1 zz z yl(yk,”,  m)ai - l (m’)Pk(m)  
Z Z Z yo ( y k ,  m’, m)a:-, ( m  ’)bk t m) 
Log “ j ; O  . (27) 
m m ‘ j = 0  
Conditionally to dk =l (resp. dk =O), variables xk are 
gaussian with mean 1 (resp. -1) and variance 02, thus the 
LLR A (dk) is still equal to 
2 
A ( d k ) = T x k  + wk (28) 
d 
where 
wk = A ( d k )  Ix1=o = 
1 zz Z y l ( y k , m ’ , m ) a ~ - l ( m ’ ) P k ( m )  
zz Z: y o ( y k ,  m’,m)aJ-,(m’)P,(m) “jrO 1 . (29) 
m m ‘ j = 0  
Wk is a function of the redundant information introduced by 
the encoder. In general Wk has the same sign as dk; therefore 
wk may improve the LLR associated with each decoded data 
bit dk. This quantity represents the extrinsic information 
supplied by the decoder and does not depend on decoder 
input xk . This property will be used for decoding the two 
parallel concatenated encoders. 
V - DECODING SCHEME OF PARALLEL 
CONCATENATION CODES 
In the decoding scheme represented in Fig. 3a, 
decoder DECl computes *LLR Al(dk) for each transmitted 
bit dk from sequences (xk} and { yk} , then the decoder DEC2 
performs the decoding of sequence( dk}  from sequences 
(A1 (dk)) and ( y k ] .  Decoder DECl uses the modified BAHL 
et al. algorithm and decoder DEC2 may use the VITERBI 
algorithm. The global decoding rule is not optimal because 
the first decoder uses only a fraction of the available 
redundant information. Therefore it is possible to improve the 
performance of this serial decoder by using a feedback loop. 
We consider now that both decoders DECl and 
DEC2 use the modified BAHL et al. algorithm. We have 
seen in section IV that the LLR at the decoder output can be 
expressed as a sum of two terms if the decoder inputs were 
independent. Hence if the decoder DEC2 inputs Al(dk) and 
y2k are independent, the LLR A2(dk) at the decoder DEC2 
output can be written as 
with 
A2 (dk 1 = f (A1 (dk )) + wzk 
2 
cr2 
(30) 
(3 1) Al(dk)=-X + w1k 
From relation (29), we can see that the decoder DECz 
extrinsic information W2k is a function of the sequence 
{ A1 (d,)]n+k Since A 1 (d,) depends on observationRp. 
extrinsic information W2k is correlated with observations . I C ~  
and ylk. Nevertheless from relation (29), the greater I n-k I is, 
the less correlated are AI (d,) and observations xk , yk. Thus, 
due to the presence of interleaving between decoders DECl 
and DEC2, extrinsic information W2k and observations xb 
ylk are weakly correlated. Therefore extrinsic information 
W2k and observations xk , ylk can be jointly used for carrying 
out a new decoding of bit dk, the extrinsic information 
zk = W a  acting as a diversity effect in an iterative process. 
In Fig. 3b, we have depicted a new decoding scheme 
using the extrinsic information W2k generated by decoder 
DEC2 in a feedback loop. This decoder does not take into 
account the different delays introduced by decoder DECl 
and DEC2 and a more realistic decoding structure will be 
presented later. 
The fist decoder DECl now has three data inputs, 
(xk, y l k ,  zk) and probabilities a i k ( m )  and&,(m) are 
computed in substituting Rk = { x k ,  y l k ]  by Rk =&k, ylk,  a) in 
relations (21) and (22). Taking into account that Q is weakly 
correlated with xk and y lk  and supposing that ,Q can be 
approximated by a gaussian variable with variance a: # 02, 
the transition probability of the discrete gaussian memoryless 
channel can be now factored in three terms 
1067 
Authorized licensed use limited to: Zhejiang University. Downloaded on November 13, 2008 at 09:33 from IEEE Xplore.  Restrictions apply.
p(Rk/dk =i,& =m,sk-1 = m ' ) = P ( X k / . ) P ( Y k / . ) P ( Z k / . )  (32) 
The encoder C1 with initial rate R I ,  through the feedback 
loop, is now equivalent to a rate R'1 encoder with 
( 3 3 )  
The first decoder obbins an additional redundant information 
with zk that may significantly improve its performances; the 
term Turbo-codes is given for this iterative decoder scheme 
with reference to the turbo engine principle. 
With the feedback decoder, the LLR hl(dk) generated by 
decoder DECl is now equal to 
where Wlk depends on sequence ( z ~ ) ~ + ~  As indicated 
above, information zk has been built by decoder DEC2 at the 
previous decoding step. Therefore zk must not be used as 
input information for decoder DEC2. Thus decoder DEC2 
input sequences at step p ( p 2 2 )  will be sequences 
( 3 5 )  
Finally from relation ( 3 0 ) ,  decoder DEC2 extrinsic 
information zk = Wzk, after deinterleaving, can be written as 
and the decision at the decoder DEC output is 
The decoding delays ~ introduced by decoder D E C 
(DEC=DECl+DECz), the interleaver and the deinterleaver 
imply that the feedback information zk must be used through 
an iterative process as represented in Fig. 4a, 4b. In fact, the 
global decoder circuit is composed of P pipelined identical 
elementary decoders (Fig. 4a). The pth decoder D E C  
(Fig. 4b) input, is made up of demodulator output sequences 
( x ) ~  and Cy)p  through a delay line and of extrinsic information 
( z ) ~  generated by the @-1)th decoder DEC. Note that the 
variance 0; of the extrinsic information and the variance of 
i, (dk 1 must be estimated at each decoding step p .  
V-2 Interleaving 
The interleaver uses a square matrix and bits {dk}  
are written row by row and read pseudo-randomly. This non- 
uniform reading rule is able to spread the residual error 
blocks of rectangular form, that may set up in the interleaver 
located behind the first decoder DEC1, and to give the 
greater free distance as possible to the concatenated (parallel) 
code. 
VI - RESULTS 
For a rate R=1/2 encoder with constraint length K=5,  
generators G 1=37, G2=21 and parallel concatenation 
(Rl=R2=2/3), we have computed the Bit Error Rate (BER) 
after each decoding step using the Monte Carlo method, as a 
function of signal to noise ratio Eb/No where Eb is the 
energy received per information bit dk and No is the noise 
monolateral power spectral density. The interleaver consists 
of a 256x256 matrix and the modified BAHL et al. algorithm 
has been used with length data block of N=65536 bits. In 
- -  demodulator 
v v 
Fig. 4a Modular pipelined decoder, corresponding to an 
iterative processus of the feedback decoding. 
(4 p1 
I 
($1 I 
6 DELAY UNE I 
v 
ti, 
(YIP1 
htamabte 
dmdd 
anplt Fig. 4b Decoding module (level p). 
order to evaluate a BER equal to we have considered 
128 data blocks i.e. approximatively 8 x106 bits dk. The BER 
versus Eb/No,  for different values of p is plotted in Fig. 5 .  
For any given signal to noise ratio greater than 0 dB, the BER 
decreases as a function of the decoding step p. The coding 
gain is fairly high for the first values of p @=1,2,3) and 
carries on increasing for the subsequent values of p. For p=18 
for instance, the BER is lower than at &/No= 0,7 dB. 
Remember that the Shannon limit for a binary modulation 
with R=1/2, is P,= 0 (several authors take Pe=10-5 as a 
reference) for @,/NO= 0 dB. With parallel concatenation of 
RSC convolutional codes and feedback decoding, the 
performances are at 0,7 dB from Shannon's limit. 
The influence of the constraint length on the BER 
has also been examined. For K greater than 5, at 
&,/No= 0,7 dB, the BER is slightly worst at the first @ = 1 )  
decoding step and the feedback decoding is inefficient to 
improve the final BER. For K smaller than 5 ,  at 
Eb/No= 0,7 dB, the BER is slightly better at the first 
decoding step than for K equal to 5 ,  but the correction 
capacity of encoders C1 and C2 is too weak to improve the 
BER with feedback decoding. For K=4 (i.e. &state 
elementary decoders) and after iteration 18, a BER of is 
achieved at &/No = 0,9 dB. For K equal to 5, we have tested 
several generators (GI, G2 ) and the best results were 
achieved with G1=37, G2=21. 
1068 
Authorized licensed use limited to: Zhejiang University. Downloaded on November 13, 2008 at 09:33 from IEEE Xplore.  Restrictions apply.
0 1 2 3 4 5 Eb/No (dB) 
theoretical 
h i t  
Fig. 5 Binary error rate given by iterative decoding (p-I, ..., 18) 
of code of fig. 2 (rate:1/2); interleaving 256x256. 
L 
BER could increase during the iterative decoding VI1 CONCLUSION 
For 
that 
process. In order to overcome this effect, we have divided the 
extrinsic information zk by [ 1 + Oli,(d,( ]with 0 = 0,15. 
In Fig. 6, the histogram of extrinsic information (.TI,, 
has been drawn for several values of iteration p, with all data 
bits equal to 1 and for a low signal to noise ratio 
(&,/No= 0,8 dB). For p=l (first iteration), extrinsic 
information (z), is very poor about bit dk, furthermore the 
gaussian hypothesis made above for extrinsic information 
( z ) ~ ,  is not satisfied! Nevertheless when iteration p increases, 
the histogram merges towards a gaussian law with a mean 
equal to 1. For instance, for p13, extrinsic information (z), 
becomes relevant information concerning data bits. 
In this paper, we have presented a new class of 
convolutional codes called Turbo-codes whose performances 
in terms of BER are very close to SHANNON'S limit. The 
decoder is made up of P pipelined identical elementary 
modules and rank p elementary module uses the data 
information coming from the demodulator and the extrinsic 
information generated by the rank @l) module. Each 
elementary module uses a modified BAHL et aI. algorithm 
which is rather complex. A much simpler algorithm yielding 
weighted (soft) decisions has also been investigated for 
Turbo-codes decoding [2], whose complexity is only twice 
the complexity of the VITERBI algorithm, and with 
performances which are very close to those of the BAHL et 
al. algorithm. This new algorithm will enable encoders and 
1069 
Authorized licensed use limited to: Zhejiang University. Downloaded on November 13, 2008 at 09:33 from IEEE Xplore.  Restrictions apply.
decoders to be integrated in silicon with error correcting 
oerformances unmatched at the Dresent time. 
INORMALIZED -1 0 +I  t2  I 
ANNEX I : EVALUATION OF PROBABILITIES af ( m )  
SAMPLES 
Fig. 6 Histograms of extrinsic information z after 
iterations W 1,4,13 at EWNo = 0.8 dB; 
all information bits d=l .  
AND Pk(m) * 
From relation (15) probability a; ( m )  is equal to 
The numerator of a;(m)  can be expressed from state Sk-1 
and bit dk -1. 
Pr{dk =i,sk =m,Rk/R:- ']= 
By using BAYES rule, we can write 
Pr{dk = i, sk = m, Rk / R : - ' ]  = 
e [dk = i, sk = m, Rk /dk-' = J, sk-l = m', R:-']. (A3)  
By taking into account that events after time (k-1) are not 
influenced by observation RF-' and bit dk-1 if state Sk-1 is 
known and from relation (17) we obtain 
Pr{dk =m.Rk/R:-'] = 
1 Z 1 yi(Rk,m'm)aL_,(m'). (A4)  
m ' j = O  
The denominator can be also expressed from bit dk and state 
s k  
and from relation (A4), we can write : 
~ 
1070 
1 1  
Pr{Rk /R:-'] = ZE Z: 1 yi(Rk,m'm)a;-,(m'). (A6) 
Finally probability a; (m) can be expressed from probability 
ai.-l ( m )  by the following relation 
m m' i=O j = O  
1 
Z Z yi (Rk ,  m' m)  ai -1 (m' ) 
XZ z Zyi (Rk ,m'm)&(mt)  * (A7)  
( m )  = "i=O 
1 1  
m m' i=O j = O  
In the same way, probability Pk (m) can be recursively 
calculated from probability Pk+l (m). From relation (16), we 
have 
' r  {R?+l l S k  = m] - 
P k  ( m )  = 
pr {@+I 
By using BAYES rule, the numerator is equal to 
Pk(") = "i=O . (A10) 
8 iRk+l  / R / ]  
In substituting k by (k+I) in relation (A6), the denominator 
of (A10) is equal to 
Finally probability Pk (m)can be expressed from probability 
Pk+l (m'), by the following relation 
1 
Z xyi(Rk+l,m,m')Pk+l(m') 
p k i m ) =  m'i=O 1 1  . (A12) 
1 xYi(Rk+l)"m)aL(") 
m m' i=O j=O 
REFERENCES 
[ l ]  L.R. Bahl, J. Cocke, F. Jeinek and J. Raviv, 
"Optimal decoding of linear codes for minimizing symbol 
error rate", IEEE Trans. Inform. Theory, vol. IT-20, pp. 248- 
287, March 1974. 
[2] C. Berrou, P. Adde, E. Angui and S .  Faudeil, "A 
low complexity soft-output Viterbi decoder architecture", to 
appear at ICC' 93. 
Authorized licensed use limited to: Zhejiang University. Downloaded on November 13, 2008 at 09:33 from IEEE Xplore.  Restrictions apply.

