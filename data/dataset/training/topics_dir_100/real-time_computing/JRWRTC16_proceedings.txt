Proceedings of the
10th Junior Researcher Workshop
on Real-Time Computing
JRWRTC 2016
http://jrwrtc2016.gforge.inria.fr/
Brest, France
October 19-21, 2016

Message from the Workshop Chairs
Welcome to the 10th Junior Researcher Workshop on Real-Time Computing, held in conjunc-
tion with the 24th International Conference on Real-Time and Network Systems (RTNS) in
Brest, October 2016. The workshop provides an informal environment for junior researchers,
where they can present their ongoing work in a relaxed forum and engage in enriching dis-
cussions with other members of the real-time systems community.
Organizing this workshop would not have been possible without the help of many people.
First, we would like to thank Alain Plantec and Frank Singhoff, General Chairs of RTNS
2016 for their guidance. We would also like to thank the local organizing committee, Mickaël
Kerboeuf, Laurent Lemarchand, Steven Costiou, Stephane Rubini, Jalil Boukhobza, Nam
Tran Hai, Arezki Laga, Hamza Ouarnoughi, Mourad Dridi and Rahma Bouaziz for having
put their time in ensuring that all the details were smooth. We would also like to thank
Sébastien Faucou and Luis Miguel Pinho, Program Chairs of RTNS 2016, for their scientific
work. We finally acknowledge Benjamin Lesage (University of York) for his precious advices
in the organisation of the event.
Behind the organization and the realization of the scientific program there is the careful
work of reviewers, who provided their time free of charge and with dedication and devo-
tion. We would like to express them our gratitude. Their work allowed us to put together
a high-quality scientific program. Finally, we would like to thank all the authors for their
submissions to the workshop. We wish you success in your scientific careers and we hope
that the workshop will help you develop your ideas further.
On behalf of the Program Committee, we wish you a pleasant workshop. May the en-
vironment be stimulating, with fruitful discussions and the presentation be enjoyable and
entertaining.
Antoine Bertout (Inria Paris, France) and Martina Maggio (University of Lund, Sweden)
JRWRTC 2016 Workshop Chairs
iii
Program Committee
Pontus Ekberg Uppsala University, Sweden
Fabrice Guet ONERA, France
Tomasz Kloda Inria Paris, France
Angeliki Kritikakou University of Rennes 1, IRISA/INRIA, France
Meng Liu Mälardalen University, Sweden
Cong Liu University of Texas at Dallas, USA
Renato Mancuso University of Illinois at Urbana-Champaign, USA
Alessandra Melani Scuola Superiore Sant’Anna, Italia
Geoffrey Nelissen CISTER/INESC-TEC, ISEP, Polytechnic Institute of Porto, Portugal
Sophie Quinton Inria Grenoble, France
Hamza Rihani University Grenoble Alpes, France
Youcheng Sun University of Oxford, United Kingdom
Houssam-Eddine Zahaf University of Lille, France / University of Oran1, Algeria
iv
Table of Contents
Contents
Message from the Workshop Chairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii
Program Committee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv
Model-Driven Development of Real-Time Applications based on MARTOP and XML . . 1
Andreas Stahlhofen and Dieter Zöbel
Model Checking of Cache for WCET Analysis Refinement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
Valentin Touzeau, Claire Maïza and David Monniaux
On Scheduling Sporadic Non-Preemptive Tasks with Arbitrary Deadline using
Non-Work-Conserving Scheduling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
Homa Izadi and Mitra Nasri
Tester Partitioning and Synchronization Algorithm For Testing Real-Time
Distributed Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Deepak Pal and Juri Vain
Framework to Generate and Validate Embedded Decison Trees with Missing Data . . . . . 17
Arwa Khannoussi, Catherine Dezan and Patrick Meyer
Quantifying the Flexibility of Real-Time Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Rafik Henia, Alain Girault, Christophe Prévot, Sophie Quinton and Laurent Rioux
Towards schedulability analysis of real-time systems with precedence constraints and
different periods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
Slim Ben-Amor, Liliana Cucu-Grosjean and Dorin Maxim
v
Model-Driven Development of Real-Time
Applications based on MARTOP and XML
Andreas Stahlhofen
Research Group for Real-Time Systems
Institute for Computer Science
University of Koblenz-Landau
Koblenz, Germany
astahlhofen@uni-koblenz.de
Dieter Zöbel
Research Group for Real-Time Systems
Institute for Computer Science
University of Koblenz-Landau
Koblenz, Germany
zoebel@uni-koblenz.de
ABSTRACT
Nowadays, there exists still a growing disaffection by the
real-time community with the perceived gap between
real-time scheduling methods on the one hand and the
lack of their availability when implementing real-time
applications on the other hand. This paper is one of
those efforts trying to overcome this annoying gap. Fol-
lowing a model-driven approach, this paper presents the
implementation of a XML schema which aggregates the
major features of a certain real-time application at the
descriptive abstraction of a specification. This schema
is model-based in the sense that it reflects all notational
aspects which are common to the real-time community.
At the same time the XML schema allows for the gener-
ation of verified code fragments which control the time-
liness of the tasks building up the real-time applica-
tion. For reasons of portability these code fragments
are POSIX-compliant.
Keywords
real-time system development; real-time system model-
ing; scheduling; model-driven development
1. INTRODUCTION
1.1 Motivation
At the present time real-time systems are pervasive
and play a significant role in many applications. Ex-
amples can be found in the field of avionics, robotics,
factory automation, medical, or in everyday life such as
in the automotive sector or the field of mobile commu-
nications and application development of smartphones.
Allowing for this fact, one would expect that the de-
velopment process for systems of this nature is well-
specified and straight forward. Taking a glance at the
current practice, this expectation, however, can not be
confirmed. Developers with the intention to implement
a real-time application are in most cases at a complete
loss and often start to reinvent the wheel by beginning
with their implementation from scratch.
In contrast, the real-time community has developed
an enormous theoretical background in the fields of real-
time tasks in the last decade including the specification
of timing constraints and scheduling policies with re-
spective assessment criteria [6, 9]. Despite this already
long-available base of knowledge, there is still a lack
of available procedures to apply the theory in practice.
To bridge this annoying gap, we developed a ready-to-
use generic real-time scheduling library in C++ called
MARTOP (Mapping Real-Time to POSIX) [14] with
the main focus on robustness and usability. Based on
the standardized POSIX API [7] our software library
guarantees a high amount of portability. At this point,
MARTOP supports the single core scheduling policies
fixed priority (FP), earliest deadline first (EDF) and
rate monotonic scheduling (RMS). According to the
model-driven development approach we have developed
a XML Schema in a second step which provides a high
recognition value with regard to the task model from
real-time theory. This allows developers describing real-
time applications using a familiar notation and mas-
sively decreases the usually arising effort during the im-
plementation process. Another profit results from the
benefits of the model-driven approach described in the
basic paper of Schmidt about model-driven engineering
[12], i.e. in this case the feature of code generation and
scheduling analysis based on the model.
1.2 Related Work
In general, the established approach of model-driven
development is well-studied and used in many areas of
software development and engineering [13]. One of the
most prominent modeling language in this field is the
Unified Modeling Language (UML) [11]. There exists
a mass of modeling tools including features like model
checking and automatic code generation especially in
the Java and C/C++ domain. Pure UML, however,
does not support a notation for an appropriate speci-
fication of timing constraints. Although there are ex-
tensions available which integrate this missing feature,
e.g. [5], in our opinion UML as a language is too com-
1
plex and does not reflect the models and notations from
the real-time theory. Available studies also show that
UML is hardly used in many software companies [10, 1].
A more intuitive way for specifying particularly timing
constraints and scheduling policy of a real-time appli-
cation is necessary.
Another alternative to model the timing constraints
of a real-time application is based on timed automata
and was first introduced by Alur and Dill in [2]. The
main goal of this approach is the verification of the sys-
tem behaviour according to a given specification and
based on model-checking algorithm from the field of
formal methods. A common tool with graphical user
interface is UPPAAL [8] which can be used for mod-
eling and verifying real-time application specifications
based on timed automata[4]. But the modeling process
is a complex task and requires a great knowledge in the
field of formal methods and reachability analysis. Thus,
this approach yields no benefit for our purpose because
we want to provide a familiar notation in accordance to
real-time theory.
1.3 Outline of this paper
The remainder of this paper is organized as follows.
In section 2.1 we describe the underlying task model
and the necessary notations which are important for
understanding the rest of the paper. Chapter 2.2 intro-
duces an example application which is used to describe
our model-driven approach. The XML schema to model
real-time applications is described in chapter 2.3. The
features of scheduling analysis and code generation are
introduced in chapters 2.4 and 2.5. Finally, the results
of our work are discussed in chapter 3 and an outlook
to planned future work is given.
2. MODEL-DRIVEN APPROACH
The overall process of the presented model-driven ap-
proach is illustrated in Figure 1 and is divided into an
intellectual and a semi-automated process. The intel-
lectual part is the one where the developer has to lend
a hand whereas the semi-automated process is tool-
supported. The core component is the XML specifi-
cation according to a respective XML schema.
2.1 Task model and notations
The following assumes a set T of independent peri-
odic preemptive tasks which are scheduled on a single
core CPU. A given task τi is specified by a tuple of the
form (Ci, Ti, Ai, Di) where Ci is the worst case execu-
tion time, Ti the rate, Ai the relative arrival time and
Di the relative deadline of the task.
A schedule for a given set of tasks T is a function
N→ T which assigns for each time unit an active task.
The schedule repeats after a duration LT ∈ N according
to all scheduled tasks T with
LT = lcm (T1, ..., Tn) . (1)
theory-based 
design of a real-
time application
XML 
application 
specification
verified 
generation of 
POSIX-based C++ 
code
scheduling 
analysis
semi-automated processintellectual process
Figure 1: Conceptual illustration of the model-
based development of real-time systems using
the presented XML schema.
The schedule is feasible if for the duration of LT no
task τi ∈ T invalidates its relative deadline Di for its
current execution. Furthermore, it is possible to assign
a priority pi ∈ N to a task τi ∈ T . At each clock
cycle of a schedule the task with the highest priority is
executed. The priority value assigned to a task depends
on the used scheduling policy.
2.2 Example application: Ball on a plate
system
As an example application we use a ball on a plate
system. The goal of such a system is balancing a ball
in the center of a plate which is controlled in a two-
dimensional manner using a stepping motor for each
axis. A webcam is mounted vertically above the plate
to capture the ball position. Overall, we can divide the
system into three main tasks:
1. Capturing the ball position using the webcam.
2. A calculation component which is divided in the
following subtasks:
(a) Image processing algorithm to calculate the
exact ball position.
(b) Calculating the manipulated value for the step-
ping motors using a PID controller.
(c) Sending the value to the stepping motors.
3. Sending diagnostic information via UDP port.
Although the calculation component comprises three
subtasks, they are executed as one task due to their
strong sequential nature. The three main tasks are
scheduled using EDF as scheduling policy. This ensures
that a new image can be captured while the previous
one is processed. Diagnostic informations, e.g. the cur-
rent ball position, are sent asynchronously via UDP to
a specified remote computer. The timing constraints
including the wcet and the rate are illustrated in table
1. The wcet of a single task was determined empirical
by executing a set of test runs.
2
Table 1: Timing constraints of the three main
tasks of the ball on a plate system.
Task Ci Ti
Image capturing 16 500µs 35 000µs
Calculation component 33 500µs 70 000µs
UDP component 4300µs 250 000µs
2.3 Textual modeling using XML
The metamodel is formulated as XML Schema (XSD)
so that the properties of a real-time application can be
expressed as XML document. The advantages of this
choice are the mass of established XML validators and
parsers so that the verification of syntax and seman-
tic is straight forward and also the expressiveness and
readability of XML is a benefit. Furthermore, it is com-
monly used as intermediate file exchange format so that
we keep the possibility open to implement a graphical
user interface.
The structure of the XML document which specifies
the ball on a plate system is described as followed. Each
application starts with the root element rt:app which
defines the name using an appropriate attribute.
<rt−app name=”BallOnAPlateSystem ”>
. . .
</ rt−app>
According to the task model described in section 2.1
a real-time application consists of a scheduler modeled
as child element of rt:app named rt:scheduler. The
attribute policy specifies the scheduling policy to use.
We also offer the possibility to model the CPU on which
the scheduler and the tasks are executed using the re-
spective attributes scheduler-cpu and tasks-cpu. This
is especially important if the target platform on which
the application is executed uses a multicore architec-
ture. The CPUs are identified by indexes starting from
0 up to the number of available CPUs minus 1. The
following example models the scheduler which sched-
ules the tasks of our example application using EDF as
scheduling policy. The scheduler itself is executed on
the CPU with id 0 and the remaining real-time tasks
on the CPU with id 1.
<r t : s c h e d u l e r po l i c y=”edf ” scheduler−cpu=”0 ”
tasks−cpu=”1 ”>
. . .
</ r t : s c h e d u l e r>
The set of tasks T are modeled as child elements of
rt:scheduler named rt:task. Each task has a name
and optionally a priority attribute. The timing con-
straints of a tasks are modeled as child elements and
are specified in microseconds. Alternatively, the timing
constraints can also modeled in nanoseconds (ns), mil-
liseconds (ms) or seconds (s). In the following listing
the image capturing task of the example application is
modeled.
<r t : t a s k name=”ImageCapturingTask ”>
<r t :wc e t value=”17500 ” un i t=”us ”/>
< r t : r a t e va lue=”40000 ” un i t=”us ”/>
< r t : r e l −a r r i v a l t ime value=”0 ” un i t=”us ”/>
< r t : r e l −dead l ine va lue=”40000 ” un i t=”us ”/>
</ r t : t a s k>
The remaining tasks of the example application are mod-
eled similarly and differ only from name and timing con-
straints.
2.4 Scheduling analysis
By parsing the given model using a XML parser we
get the specified tasks including their timing constraints.
The followed utilization test is applied for proving that
the given set of tasks is feasible according to EDF as
scheduling policy:
U ≤ 1 (2)
with U for a set of tasks T as
U =
n∑
i=1
Ci
Ti
, n = |T | . (3)
For the given set of tasks T of the example application
the utilization U is
U =
16500
35000
+
33500
70000
+
4300
250000
= 0, 9672 ≤ 1. (4)
Consequently, the tasks in T of the example applica-
tion are feasible according to EDF as scheduling policy.
If the chosen scheduling policy is FP or RMS the well-
known response time test (RT-Test) for static schedul-
ing policies is used [3].
2.5 Code generation
The transformation from a model of a system to pro-
gramming code of a specific language is a tedious and
error-prone process. Therefore, we offer the possibility
to generate C++ code according to a present real-time
application model specified in XML with regards to the
described XML schema of section 2.3. The generated
code is based on MARTOP and offers the developers a
full functional template as entry point for implement-
ing the target system. A big advantage of our code
generator in contrast to others is the preservation of
readability and maintainability of the generated code.
Furthermore, the compliance with the specified timing
constraints of the modeled system are guaranteed.
Figure 2 shows a UML class diagram of the gener-
ated software. The class EDFSchedulingPolicy which
is associated with the MartopScheduler specifies to use
EDF as scheduling policy. Furthermore, a new sub-
class derived from MartopTask is generated for each
task described in the model. The concept of a cus-
tom MARTOP task is oriented towards the Java thread
paradigm, i.e. implementing an abstract class includ-
ing a single function called run() which represents the
working routine of the task. Thus, the remaining task
of the developer is to implement the working routines
in each resulting MARTOP task.
3
Figure 2: UML class diagram for demonstrating the structure and resulting classes which are gener-
ated based on the MARTOP library.
3. CONCLUSION AND FUTURE WORK
In this paper we presented an approach to model the
software part of a real-time application using XML. The
structure of the XML document is described via a XML
schema and can be appropriately validated. The result-
ing XML structure is based on the task model from the
real-time theory and offers developers an intuitive pos-
sibility for describing scheduling policy and timing con-
straints of a real-time application. With the help of an
example application the features of scheduling analysis
and code generation are presented.
The aim of our work is to support real-time appli-
cation developers and make their life easier. In our
opinion, a step in the right direction has been made
with the approach presented in this paper. But there
are still lacking features. First, we assume a single-core
processor but multi-core processors are already state
of the art. Accordingly, a future development in the
direction of multi-core scheduling is urgent and neces-
sary. Second, the modeling capabilities are kept simple
and the full advantage of this approach is not taken.
On the one hand, this fact makes it easy for develop-
ers to learn how to model a real-time application using
our approach. On the other hand, it is only possible to
model applications with low complexity that makes our
approach for most real-world applications insufficient
at this time. Our major request is to offer our software
and tools to other developers and support them in de-
veloping real-time applications, so these future works
will make our approach applicable for the practice.
4. REFERENCES
[1] L. T. W. Agner, I. W. Soares, P. C. Stadzisz, and
J. M. SimãO. A Brazilian survey on UML and
model-driven practices for embedded software
development. J. Syst. Softw., 86(4):997–1005,
2013.
[2] R. Alur and D. L. Dill. Automata for modeling
real-time systems. In Proc. seventeenth Int.
Colloq. Autom. Program., pages 322–335.
Springer, 1990.
[3] N. Audsley, A. Burns, M. Richardson, K. Tindell,
and A. J. Wellings. Applying new scheduling
theory to static priority pre-emptive scheduling.
Softw. Eng. J., 8(5):284–292, 1993.
[4] J. Bengtsson, K. G. Larsen, F. Larsson,
P. Pettersson, and W. Yi. Uppaal — a Tool Suite
for Automatic Verification of Real–Time Systems.
Work. Verif. Control Hybrid Syst. III,
(1066):232–243, 1995.
[5] S. Burmester, H. Giese, and W. Schäfer.
Model-driven architecture for hard real-time
systems: From platform independent models to
code. In Eur. Conf. Model Driven Archit. Appl.,
pages 25–40. Springer, 2005.
[6] G. Buttazzo. Hard Real-Time Computing
Systems: Predictable Scheduling Algorithms and
Applications. Springer, 2011.
[7] ISO/IEC 9945-1. Information Technology -
Portable Operating System Interface (POSIX) -
Part 1: System Application Program Interface
(API) [C Language]. Technical report, Institute of
Electrical and Electronic Engineers (IEEE), 1996.
[8] K. Larsen, P. Pettersson, and W. Yi. UPPAAL in
a nutshell. Int. J. Softw. Tools . . . , pages
134–152, 1997.
[9] F. Liu, A. Narayanan, and Q. Bai. Real-time
systems. 2000.
[10] M. Petre. UML in practice. In Proc. 2013 Int.
Conf. Softw. Eng., pages 722–731. IEEE Press,
2013.
[11] J. Rumbaugh, I. Jacobson, and G. Booch. Unified
Modeling Language Reference Manual. Pearson
Higher Education, 2004.
[12] D. Schmidt. Guest Editor’s Introduction:
Model-Driven Engineering. Computer (Long.
Beach. Calif)., 39(2):25–31, 2006.
[13] D. C. Schmidt. Model-driven engineering. IEEE
Comput., 39(2):25–31, 2006.
[14] A. Stahlhofen and D. Zöbel. Mapping Real-Time
to POSIX. In R. Stojanović, L. Jóźwiak, and
D. Jurǐsić, editors, 4th Mediterr. Conf. Embed.
Comput., pages 89–92, Budva, Montenegro, 2015.
4
Model Checking of Cache for WCET Analysis Refinement
Valentin Touzeau
Univ. Grenoble Alpes
CNRS, VERIMAG, F-38000
Grenoble, France
Valentin.Touzeau@imag.fr
Claire Maïza
Univ. Grenoble Alpes
CNRS, VERIMAG, F-38000
Grenoble, France
Claire.Maiza@imag.fr
David Monniaux
Univ. Grenoble Alpes
CNRS, VERIMAG, F-38000
Grenoble, France
David.Monniaux@imag.fr
ABSTRACT
On real-time systems running under timing constraints,
scheduling can be performed when one is aware of the
worst case execution time (WCET) of tasks. Usually, the
WCET of a task is unknown and schedulers make use of
safe over-approximations given by static WCET analysis.
To reduce the over-approximation, WCET analysis has to
gain information about the underlying hardware behavior,
such as pipelines and caches. In this paper, we focus on the
cache analysis, which classifies memory accesses as
hits/misses according to the set of possible cache states.
We propose to refine the results of classical cache analysis
using a model checker, introducing a new cache model for
the least recently used (LRU) policy.
Keywords
Worst Case Execution Time, Cache Analysis, Model
Checking, Least Recently Used Cache
1. INTRODUCTION
On critical systems, one should be able to guarantee that
each task will meet its deadline. This strong constraint can
be satisfied when the scheduler has bounds on every tasks’
execution time. The aim of a WCET analysis is to compute
such safe bounds statically. In order to provide a satisfiable
bound, the WCET analysis needs to model the execution of
instructions at the hardware level. However, to avoid the
huge latency of main memory access, one can copy parts
of the main memory into small but fast memories called
caches. In order to retrieve precise bounds on the execution
time of instructions, it is thus mandatory to know which
instructions are in the cache and which are not. In this
paper we focus on instruction caches, ie. we aim at knowing
whether instructions of the program are in the cache when
they are executed.
For efficiency reasons, the main memory is partitioned
into fixed size blocks. To avoid repeated accesses to the
same block, they are temporary copied into the cache when
requested by the CPU. This way, they can be retrieved faster
on the next access, without requesting the main memory
again. Moreover, to speed up the retrieval of blocks from
the cache, caches are usually partitioned into sets of equal
sizes. A memory block can only be stored in one set that
is uniquely determined by the address of the block. Thus,
when searching a block in the cache, it is looked for in only
one set. Since the main memory is bigger than the cache, it
may happen that a set is already full when trying to store
a new block in it. In this case, one block of the set has
to be evicted in order to store the new one. This choice
does not depend on the content of the other sets and is done
according to the cache replacement policy. In our case, we
focus on the Least Recently Used (LRU) policy (for other
policies, refer to [5]). A cache set using the LRU policy can
be represented as a queue containing blocks ordered from
the most recently accessed (or used) to the least recently
accessed. When the CPU requests a block that is not in
the cache (cache miss), it is stored at the beginning of the
queue (it becomes the most recently used block) and the
last block (the least recently used) is evicted. On the other
hand, when the requested block is already in the cache, it is
moved to the beginning of the queue, delaying its eviction.
The position of a block in the queue is called the age of the
block: the youngest block is the most recently used and the
oldest is the least recently used.
The aim of a cache analysis is to provide a classification
of memory accesses as ”cache hit”, ”cache miss” or
”unknown” (not always a hit, and not always a miss) to be
used as part of the WCET analysis. This classification is
usually established by abstract interpretations called ”May
Analysis” and ”Must Analysis” that respectively compute a
lower and upper bound of every block’s age. For more
details about these analysis, refer to [3]. Must analysis is
used to predict the cache hits (if a block must be in the
cache when accessing it, access is a hit), whereas may
analysis is used to predict the misses (if a block may not be
in the cache when accessing it, access is a miss). However,
if a block is in the may cache but not in the must cache, it
is classified as unknown. This can happen because this
access is sometimes a miss and sometimes a hit, or because
the abstract interpretation is too coarse. An example is
given on Figure 1. Program states (basic blocks) are on the
left, whereas abstract cache states (may and must) at the
exit of basic blocks are on the right. For simplicity, every
basic block is stored in exactly one memory block. For
example, at the exit of block 5, the minimum age of blocks
a, b, c and d computed by the may analysis are respectively
1, 0, 2 and 1. At block 6, a is accessed and is in the cache
(because there are only 4 different blocks, and they all fit
together in the cache), thus it should be classified as a hit.
However, it is classified as ”unknown” by the may and must
analysis because of an over-approximation performed by
the must analysis. Indeed, at entry of block 5, the must
cache is [⊥,⊥,⊥, a] because a is the only block that must
be accessed, and b, c and d may be accessed since the last
access to a. An other possibility to classify memory access
5
1. a
2. b
3. c
4. d
5. b
6. a
[a,⊥,⊥,⊥]May
[a,⊥,⊥,⊥]Must
[b, a,⊥,⊥]May
[b, a,⊥,⊥]Must
[c, b, a,⊥]May
[c, b, a,⊥]Must
[d, c, b, a]May
[d, c, b, a]Must
[b, {a, d}, c,⊥]May
[b,⊥,⊥,⊥]Must
[a, b, d, c]May
[a, b,⊥,⊥]Must
Figure 1: Example of access classified as ”unknown”
as hit or miss is to use a model checker. Both the program
and the cache are encoded as a transition system. Then,
the question of the existence in the cache of a given block
at a given program point is encoded as a logical formula.
Both the formula to check and the transition system are
provided to the model checker, that classifies the block as
“in the cache”, “out of the cache” or “unknown”. Since the
model deals with every reachable program/cache states
separately, model checking is usually more precise than
abstract interpretation. However, it is also much slower
and often requires more memory during the analysis.
To avoid the precision loss of abstract interpretation
without performing a heavy analysis using a model
checker, we propose to mix both analysis. We first perform
the classical may/must analysis by abstract interpretation,
and then refine the classification using a model checker.
Thus, we only use the model checker to classify blocks that
were classified as ”unknown” by the abstract interpretation.
Moreover, we introduce a new abstract model that can be
used by the model checker to efficiently represent LRU
cache states.
2. WCET ANALYSIS
This section gives some basic notions about WCET
analysis and explain the link with cache analysis.
Usually, WCET analysis are performed by following steps:
First, a control flow graph is retrieved from the binary code
under analysis by grouping instructions into basic blocks
(sequences of instructions with only one entry point and one
exit point). Then, the WCET of the program is computed
by bounding the execution time of every basic block and
finding the “longest” path inside the CFG.
The computation of basic blocks execution time is done
by micro-architectural analysis that models pipelines and
caches. Independently, several other analysis like value
analysis and loop bound analysis are performed to provide
information about the semantics to the WCET analyzer.
At hardware level, the uncertainty about execution times
of instructions comes from pipelines (which can start
executing an instruction before the previous one is
finished) and caches (which avoid costly main memory
accesses). The aim of a cache analysis is to classify
memory accesses as cache hit or cache miss. Since an
access to the main memory can be 100 times slower than
an access to a cache, it is mandatory to classify memory
access as hit/miss in order to provide accurate bounds on
WCET estimations. Moreover, when a memory access is
not classified as a hit or a miss, the WCET analysis must
treat the both cases [2]. Thus, it can make the analysis
slower and increase the pessimism.
To avoid this precision loss, we aim at refining the
classification of unknown block using a model checker.
3. RELATED WORK
Some previous works use a model checker for performing
WCET analysis of programs. The approach of Lv, Yi,
Guan and Yu [6] is the following. Using abstract
interpretation, they classify memory access as ”cache
misses”, ”cache hits” or ”unknown”. Every memory access
is translated in a timed automaton that describe the access
to the memory bus by introducing some delay. Finally,
these automata are connected together according to the
CFG and the model checker explores the transition system,
computing a WCET estimation. This approach allows
them to perform WCET analysis of multicore systems.
Although they are using a model checker in a WCET
analysis, our approach is different and complementary.
Indeed, they are using the model checker to refine the
timing analysis itself, and not the classification of memory
accesses as we do. Therefore, our refinement of the cache
content can be added to the first step of their analysis to
retrieve better bounds. The work of Chattopadhyay and
Roychoudhury [1] is closer to our approach. They use the
model checker to analyze behavior of caches shared by
several cores. Moreover, by instrumenting the program at
source code level they can take the program semantics into
account and do not treat infeasible path, make the analysis
more precise. Since they are performing a may/must
analysis as a first step, we believe our analysis can be used
to refine this first step, before taking shared caches into
account.
4. OUR ABSTRACT MODEL
To perform the cache analysis using a model checker, we
have to provide models both for the program and for the
cache. We use these models to answer the following
questions: “At a given program point, is a given block in
the cache whatever the path to reach this point is ?”
(classify as hit) and “Is the given block never in the cache
at the given program point, whatever the path used to
reach this point is ?” (classify as miss).
To model the cache, we use an abstraction of the real cache
state to avoid the state space explosion problem one can
meet when using a model checker. In the following formal
description of our model, we adapt the notations from Jan
Reineke’s PhD [7]:
Definition 1. Cache size
k ∈ N is the size of the cache set (in blocks)
Definition 2. Set of memory blocks
M represents the set of memory blocks.
M⊥ = M ∪ {⊥} includes the empty line.
6
Definition 3. Set of Cache States
CLRU ⊂Mk⊥ symbolizes the set of reachable cache states
[b1, b2, . . . , bk] ∈ CLRU represents a reachable state
b1 is the least recently used block
In addition of these notation, to define our abstract model,
we introduce the following notations: A, represents the set
of abstract cache states, εa ∈ A, represents cache states
that does not contain a, and [S]a ∈ A, represents cache
states that contains a and some other blocks younger than
a (forming the set S), where a ∈M is a memory block.
Definition 4. Set of Abstract Cache States
A = P(CLRU ) is the power set of reachable cache states.
Definition 5. Abstract Cache States
εa =
{
[b1, ..., bk] ∈ CLRU ,∀i ∈ J1, kK, bi 6= a
}
∈ A
[S]a =
{
[b1, ..., bk] ∈ CLRU ,
∃i ∈ J1, kK, bi = a ∧ (bj ∈ S ⇔ j < i)
}
∈ A
The idea behind the abstract model we define below is to
track only one block (noted a). Indeed, to know whether a
block is in a LRU cache, you only have to count the number
of accesses made to pairwise different blocks since the last
access to it. In other words, you do not have to know the age
of others blocks, you are only interested in knowing if they
are younger than the block you are tracking. Therefore, we
group together cache states that have the same set of blocks
younger than the block we want to track.
To every cache state p, we associate an abstract state
αa(p) which consists of the set of values younger than a in
the cache or a special value in the case where a is not in
the cache.
Definition 6. Abstraction of Cache States
αa : C
LRU → A
αa ([b1, ..., bk]) =
{
εa if ∀i ∈ J1, kK, bi 6= a
[{b1, ..., bi−1}]a if ∃i ∈ J1, kK, bi = a
For example, when tracking block a, the abstract cache
state associated to the exit of block 1 of Figure 1 is [{}]a,
symbolizing that a is the least recently used block at this
point (the set of younger blocks is empty). At the exit of
block 4, the abstract cache state is [{b, c, d}]a.
Additionally, we define the partial function updateaLRU(k),
which models the effect of accessing a block on an abstract
state. When the abstract cache state does not contain a (i.e.
is equal to εa), it remains unchanged until an access to a is
made. When a is accessed, every new block access appears
into the set S. When the cardinal of S reaches k−1 (a is the
least recently used block), a new access to a different block
evicts a (and new abstract cache state is reset to εa). If an
access to a is done in the meantime, the set S of younger
block is reset to {}.
Definition 7. Abstract State Update
updateaLRU(k) : A×M → A
updateaLRU(k)(εa, c) =
{
[{}]a if a = c
εa if a 6= c
updateaLRU(k)([S]a, c) =
[{}]a if c = a
[S]a if c 6= a and c ∈ S
[S ∪ {c}]a if c 6= a and c /∈ S and |S| < k − 1
εa if c 6= a and c /∈ S and |S| = k − 1
Considering the example of Figure 1, the model checker
associates two different abstract states to block 5 depending
on the incoming flow from block 1 or block 4. These states
are respectively [{}]a and [{b, c, d}]a. Thus, applying the
update function for treating the access made to b in block 5,
we obtain [{b}]a and [{b, c, d}]a. Therefore, we know that a
is not evicted from the cache by block 5 and access made to
a in block 6 is not classified as unknown anymore but as a
hit.
The second part of our model is the model of the
program. Since we focus on instruction caches, the model
we use for the program is a graph obtained from the CFG
by splitting basic blocks (when needed) into blocks of the
size of a memory block. Thus, a path in the model
represents the sequence of memory access that the
instruction cache handles during the execution of the
program. However, because we only track one memory
block at a time, it is also possible to simplify the control
flow graph according to this block. Indeed, one can slice
the CFG according to the cache set associated to the block
we want to track, removing every memory access to an
other cache set. Moreover, we can remove from the
obtained graph every node that is not an access to a and
that does not contribute to a eviction. Thus, it appears
that every node that does not contain a in their entry may
cache can be removed.
[. . . ]a
εa
[. . . ]a
access to a
eviction of a
Figure 2: Simplifying CFG according to access to a
This simplification of the CFG is illustrated on figure 2.
Plain arrows represent program flow potentially
manipulating block a, whereas dashed arrows represent
flow that does not and that can be simplified in only one
arrow. At some point after an access to block a, we can be
sure that a is not in the cache anymore. Therefore, it is
possible to remove all nodes (dashed arrows) from this
point until the next access to a.
5. IMPLEMENTATION / EXPERIMENTS
This section describe the prototype we build and the
experiments we made to valid our proof of concept. The
workflow of our analysis is illustrated on Figure 3.
Our implementation does not use directly the binary
code to analyze but runs on the LLVM bytecode
representation of it. We first build the CFG of the program
from the bytecode using the LLVM framework. Since the
7
Program Size
4 ways 8 ways 16 ways
Un Nc Un Nc Un Nc
recursion 26 34.6% 11.1% 53.8% 7.1% 53.8% 21.4%
fac 26 34.6% 11.1% 46.1% 8.3% 46.1% 41.6%
binarysearch 48 12.5% 0% 56.2% 29.6% 52.0% 12.0%
prime 57 10.5% 0% 29.8% 35.2% 57.8% 18.1%
insertsort 58 23.7% 28.5% 28.8% 11.7% 55.9% 9.0%
bsort 62 30.6% 57.8% 53.2% 6.0% 62.9% 5.1%
duff 64 10.9% 0% 37.5% 12.5% 37.5% 12.5%
countnegative 65 21.5% 21.4% 43.0% 21.4% 52.3% 20.5%
st 137 14.5% 30.0% 43.7% 13.3% 69.3% 5.2%
ludcmp 179 11.1% 5.0% 39.6% 15.4% 67.5% 4.1%
minver 265 20.7% 29.0% 44.1% 12.8% 63.0% 10.7%
statemate 582 7.5% 2.2% 7.9% 4.3% 8.2% 2.0%
Table 1: Precision of May/Must analysis and Model Checker
CFG
Cache
Configuration
Hit/Miss
Classification
May/Must
Analysis
Cache
Modeling
Model
Checker
Unknown
Not unknown
Figure 3: Workflow of our prototype
LLVM bytecode does not affect any address to
instructions, we have to provide a mapping of instructions
to the main memory. For our prototype, we assume that
every instruction has the same size in memory. Thus,
memory blocks contain a fixed number of instructions and
can be obtain by splitting basic blocks of the CFG into
fixed size blocks. Using this mapping and the CFG, our
prototype performs a may/must analysis of the program.
For every block access classified as unknown, we build an
abstract model of the cache and provide it to the model
checker together with the CFG (simplified as explain
above). It would be possible to use real addresses from
binary code and a correspondence between LLVM bytecode
and binary code, as done in [4], but this requires significant
engineering and falls outside of the scope of this
experiment.
We experiment our prototype with benchmarks of the
TacleBench1. Table 1 contains the results of our
experiments. Size of program is given in number of
memory block. We run our experiments with caches of
only one cache set, with different sizes: 4, 8 or 16 ways.
For every experiment, we measure both the amount of
accesses classified as “unknown” by the may/must analysis
(column “Un”) and the amount of accesses newly classified
as “always in the cache” or “always out of the cache” among
the accesses left “unknown” by the may/must analysis
(column Nc). During these experiments, our analysis
classifies up to 57.8% of the accesses left unclassified by the
abstract interpretation analysis.
6. CONCLUSION
We proposed to refine classical cache analysis by using
a model checker. To avoid the common problem of state
space explosion meet when dealing with model checking, we
introduce a new abstract cache model. This model allows
1http://www.tacle.eu/index.php/activities/taclebench
us to compute the exact age of a memory block along an
execution path of the program. Thus, we can select the
memory block we want to refine. Moreover, it allows us
to simplify the program model too, by removing some nodes
useless to the refinement. Finally, we implement a prototype
and test it on a benchmark. Our experiments shows that our
approach is able to refine up to 60% of the memory access
classified as unknown by the abstract interpretation.
Our prototype runs on LLVM bytecode, and use an
unrealistic memory mapping. As future work, we aim at
implementing an analyzer that runs on the binary code. To
finally validate our approach, it is also possible to compare
the performance of our analysis to other analysis refining
may/must analysis, like persistence analysis or analysis
performing virtual inlining and unrolling.
7. REFERENCES
[1] S. Chattopadhyay and A. Roychoudhury. Scalable and
precise refinement of cache timing analysis via model
checking. In RTSS 2011. IEEE Computer Society, 2011.
[2] C. Cullmann, C. Ferdinand, G. Gebhard, D. Grund,
C. Maiza, J. Reineke, B. Triquet, and R. Wilhelm.
Predictability considerations in the design of multi-core
embedded systems.
[3] C. Ferdinand. Cache behavior prediction for real-time
systems. Pirrot, 1997.
[4] J. Henry, M. Asavoae, D. Monniaux, and C. Maiza.
How to compute worst-case execution time by
optimization modulo theory and a clever encoding of
program semantics. In Y. Zhang and P. Kulkarni,
editors, SIGPLAN/SIGBED, LCTES ’14. ACM, 2014.
[5] M. Lv, N. Guan, J. Reineke, R. Wilhelm, and W. Yi. A
survey on static cache analysis for real-time systems.
LITES, 2016.
[6] M. Lv, W. Yi, N. Guan, and G. Yu. Combining
abstract interpretation with model checking for timing
analysis of multicore software. In RTSS2010, pages
339–349. IEEE Computer Society, 2010.
[7] J. Reineke. Caches in WCET Analysis: Predictability -
Competitiveness - Sensitivity. PhD thesis, Saarland
University, 2009.
8
On Scheduling Sporadic Non-Preemptive Tasks with
Arbitrary Deadline using Non-Work-Conserving
Scheduling
Homa Izadi
Islamic Azad University, E-Campus,
Tehran, Iran.
h.izadi@iauec.ac.ir
Mitra Nasri
Max Planck Institute for Software Systems,
Kaiserslautern, Germany.
mitra@mpi-sws.org
ABSTRACT
In this paper, we consider the problem of scheduling a set
of non-preemptive sporadic tasks with arbitrary deadlines
on a uni-processor system. We modify the Precautious-RM
algorithm which is an online non-preemptive scheduling al-
gorithm based on rate monotonic priorities to cope with ar-
bitrary deadlines and sporadic releases. Our solution is an
O(1) non-work-conserving scheduling algorithm.
1. INTRODUCTION
Non-preemptive scheduling reduces both run-time and de-
sign time overheads. At run-time, it avoids context switches,
and hence, preserves the state of the cache and CPU pipelines
for the task. At design-time, it simplifies the required mech-
anisms for guaranteeing mutual exclusion. Besides, due to
these properties, it improves the accuracy of estimating the
worst-case execution-time (WCET) of a task. Thus, it allows
system designers to have a more realistic and less pessimistic
configuration setup for the system.
Moreover, non-preemptive execution improves the quality
of service in control systems because it efficiently reduces
the delay between sampling and actuation [9]. By doing so,
it increases the freshness of a sampled data. In a preemptive
system, if a task is preempted after reading its data from the
environment (and before using that data), the data becomes
old, and hence, after resuming, the task uses the old data to
calculate the output command. Consequently, controller’s
accuracy may reduce.
In the non-preemptive scheduling, a processing resource
(or other resources) can be blocked for a long time, caus-
ing a timing violation for the tasks with shorter deadlines.
Thus, in a feasible task set, the maximum execution time
of a task is restricted. Cai et al., [1] have shown that in a
periodic task set, the maximum permissible execution time
of the tasks except τ1 (the task with the smallest period),
is at most 2(T1 − C1) (where T1 and C1 are the period and
WCET of τ1). For example, if the smallest period is 1ms and
the largest period is 1000ms (which is regularly the case for
the runnables in the automotive industry), then the largest
execution time cannot be greater than 2ms. It means that
those tasks with large period must be heavily under-utilized,
otherwise, the system will not be schedulable.
In order to cope with the limitation of non-preemptive
scheduling, many researches have focused on finding a safe
speed-up factor, using which no task will miss its dead-
line (under a particular scheduling algorithm such as non-
preemptive rate monotonic (NP-RM), or non-preemptive
EDF (NP-EDF) [8]). However, as shown in [3], this speed-
up factor may become an unreasonably large number (e.g.,
120). Such a speed-up factor may not be useful for a real sys-
tem where the weight, size, power consumption, and many
other factors affect the choice of processor.
In many real-time systems (e.g., control or multimedia
systems), finishing a task after its deadline will not cause
a failure for the system (the system might not even be a
safety-critical system to begin with). For example, in con-
trol systems, usually the stability of the controller will not
be affected if one of the samples is taken later than the
expected sampling period [9]. Such event may affect the
quality of control, however, it may probably not affect the
stability of the controller as long as the completion time
is within a certain deadline. In other words, deadlines are
not necessarily smaller than or equal to periods. As a result,
the tasks can tolerate longer blocking time caused by a lower
priority task. It allows us to have a non-preemptive system
with longer execution times than one mentioned previously.
Recently, there have been some advancements in the state
of non-work-conserving scheduling algorithms [4,5,7]. These
algorithms may leave the processor idle even if there are
pending jobs in the system. The idea of these works is to
add an idle-time insertion policy (IIP) to the existing job
scheduling policies such as RM and EDF to increase schedu-
lability. The role of the job scheduling policy is to select a
job with the highest priority, and the role of IIP is to decide
whether to schedule that job or instead, to leave the proces-
sor idle. It has been shown [5] that by adding an IIP to the
job scheduling policy, a significant number of task sets that
are not schedulable by the job scheduling policy itself (e.g.,
RM and EDF) become schedulable if an idle-time insertion
policy is used.
In our work, we focus on the problem of scheduling non-
preemptive sporadic tasks with arbitrary deadlines upon a
uni-processor system. We use the Precautious-RM (P-RM)
from [7] as the basis of our algorithm, and modify it in three
ways: 1) we increase its eagerness to schedule tasks rather
than scheduling idle intervals, 2) we replace the existing IIP
with the one which is based on arbitrary deadlines rather
than implicit deadlines, and 3) we add mechanisms for en-
abling it to schedule sporadic tasks. Then we discuss how a
schedulability test can be developed for the new algorithm.
Our final solution is an online non-preemptive scheduling
algorithm, called AD-PRM (where AD stands for arbitrary
deadline). It has O(1) computational complexity per acti-
vation of the if the number of priorities in the system is a
reasonable constant number.
In the rest of the paper we describe system model in
Sect. 2. The AD-PRM, is presented in Sect. 3. Open chal-
lenges and future work are presented in Sect. 4. Sect. 5
presents the experimental results. In Sect. 4 we discuss
9
Figure 1: Examples of P-RM schedules in compari-
son with an optimal schedule.
about future work and then conclude the paper in Sect. 6.
2. SYSTEM MODEL AND BACKGROUND
We consider a set of independent, non-preemptive spo-
radic tasks, with arbitrary deadlines which must be sched-
uled on a single processor. Task set τ = {τ1, τ2, . . . , τn} has
n tasks and each task is identified by τi : (Ci, Ti, Di), where
Ci is the worst-case execution time (WCET), Ti is the period
(the minimum inter-arrival time), and Di is the deadline of
the task. We assume Ci, Ti, and Di are integer multiples of
the system’s clock, thus they are integer values. We denote
system utilization by U =
∑n
i=1 ui where ui = Ci/Ti is the
utilization of task τi. The hyperperiod is the least common
multiple of the periods. Tasks are indexed according to their
period so that T1 < T2 ≤ . . . ≤ Tn.
Next, we explain how P-RM scheduling algorithm [6] works.
Similar to RM, P-RM (shown in Alg. 1) prioritizes the tasks
based on their period, i.e., τ1 and τn are the tasks with the
highest and the lowest priorities. The IIP of P-RM is based
on one future job of τ1; if executing the current low priority
task τi at time t will cause a deadline miss for the next in-
stance of τ1, P-RM schedules an idle interval until the next
release of τ1. More accurately, in order to schedule τi, one
of the following conditions must hold: a) τi must be finished
before the next release of τ1, or b) the previously executed
task must be τ1 and the current task must be finished be-
fore the latest time at which the next instance of τ1 can start
and finish its execution. This instant is rnext1 + (T1−C1). If
none of these conditions is satisfied, P-RM schedules an idle
interval until the next release of τ1 at r
next
1 (Line 6).
Fig. 1-(a) shows an example of a schedule produced by
P-RM. However, as shown in Fig. 1-(b), the P-RM may be
too cautious due to the condition τ1 is the latest executed
task (in Line 3 of Alg. 1). Also, the first late job of τ2 (the
one released at 36) will cause another late job for its next
job as well. By a late job we mean a job that is not finished
before the next job of its own task is released.
3. NON-WORK-CONSERVING SOLUTION
As it has been shown in Fig. 1-(b), P-RM is not always
efficient since due to Line 2 (of Alg. 1), it adds more idle-
Algorithm 1: P-RM
Input: t: the current time
1 τi ← the highest priority task with a pending job;
2 rnext1 ←
(
bt/T1c+ 1
)
T1 which is the release time of the
next job of τ1 after t;
3 if ((t+Ci ≤ rnext1 ) or (t+Ci ≤ rnext1 + T1 −C1 and τ1
is the latest executed task)) then
4 Schedule τi;
5 else
6 Schedule an idle interval from t to rnext1 ;
7 end
times than it may be necessary. The original motivations
for having this condition was to make the algorithm optimal
to schedule harmonic tasks with ∀i, Ti/Ti−1 = 2 (proven by
Nasri et al., [7]). However, in general, this condition just
makes the P-RM very cautious.
In the current implementation of P-RM (see Alg. 1), τi is
only allowed to be scheduled if it can finish its execution (at
t+Ci) before r
next
1 +T1−C1 which is supposedly the latest
valid start time for the next job of τ1. However, if deadlines
are arbitrary, this condition becomes very pessimistic be-
cause τ1 can be scheduled even after its deadline. Besides, if
one of the tasks has Ci > 2(T1−C1), it will never be sched-
uled by the current implementation of P-RM. Moreover, if a
job of τ1 is not finished before the next job of τ1, it cannot
be scheduled since none of the two conditions in Line 2 of
Alg. 1 allows it.
Algorithm 2: AD-PRM Algorithm
Input: t: the current time
1 τi ← the highest priority task with a pending job;
2 rlast1 ← bt/T1cT1 which is the release time of the
previous job of τ1 before or at t;
3 if (i = 1) or (t+ Ci ≤ rlast1 + T1) or (Ci ≤ D1 − C1)
or (t+ Ci ≤ rlast1 + T1 +D1 − C1) then
4 Schedule τi;
5 else
6 Schedule an idle interval from t until another high
priority job is released;
7 end
Our new solution is shown in Alg. 2. If i = 1, i.e., current
pending task is τ1, AD-PRM schedules it without checking
any other condition. Hence, it allows late jobs of τ1 to be
safely scheduled. Also, if the finish time of the current high
priority job at t+Ci satisfies the following condition, it will
be scheduled (it does not need to be scheduled only after
one job of τ1):
t+ Ci ≤ rlast1 + T1 +D1 − C1 (1)
In Equation (1), the right-hand-side represents the latest
safe start time of τ1 which is not violating its deadline. Doing
so, our algorithm can be applied for arbitrary deadline tasks.
Yet, another important feature in AD-PRM is to use rlast1 +
T1 instead of r
next
1 . In a fully periodic task set which does
not have any sort of release jitter, rlast1 + T1 is always equal
to rnext1 however, if the task set is sporadic, we may not
know the exact release time of the next job of τ1 in the fu-
ture. As a result, the old version of the P-RM could not
10
Figure 2: An example for a blackout due to some
late jobs of τ1.
handle sporadic tasks. To handle sporadic tasks efficiently,
we assume that at time t a sporadic task τi can be sched-
uled if Ci ≤ D1−C1. It means that if a job of τ1 is released
slightly after t, still it will be able to meet its deadline since
the execution time of the low priority task is smaller than
D1 − C1. However, if Ci > D1 − C1, AD-PRM will not al-
low it to be scheduled until one job of τ1 is released in the
system (see Line 6 of Alg. 2). When that job is scheduled,
if τi is still the highest priority job in the system, it can be
scheduled through (1) condition. Note that AD-PRM will
not let any job of τ1 to be missed due to a low priority task.
The last modification is in Line 6 of Alg. 2, where instead
of scheduling an idle interval until rnext1 which is a fixed
value in time, we schedule the idle interval until the next
release event in the system. If the next release is for one of
the jobs of τ1, AD-PRM will behave almost similar to the
P-RM, but if it is another high priority job, then that job
may be able to satisfy conditions of Line 3 (Alg. 2), and
hence, has a chance to be scheduled even without waiting
for the next job of τ1. Note that a release of a low priority
job will not change the situation because AD-PRM is a fixed
priority algorithm and as long as τi has a pending job, it will
not schedule the jobs of lower priority tasks.
It is worth noting that if the release of higher priority
tasks does not happen within a bounded amount of time,
we cannot provide any efficient schedulability test for AD-
PRM due to the condition in Line 6 of the algorithm. Thus,
in order to propose a schedulability test for AD-PRM we
need to have some assumptions about the maximum inter-
arrival time of the tasks (rather than just having an infor-
mation about the minimum inter-arrival times). Providing
a schedulability test is our next future work.
As can be seen in Alg. 2, computational complexity of
AD-PRM is O(1) (per activation of the algorithm), provided
that the highest priority task with a pending job (Line 2 of
Alg. 2) can be detected in O(1). It is worth noting that
since AD-PRM inserts idle-times in the schedule, it may
happen that a particular job, for example from task τi, is
postponed several times by the algorithm, as long as it finds
a better chance to be scheduled. Thus, if we count the total
overheads of the algorithm in the hyperperiod, it might be
slightly larger than RM.
4. DISCUSSIONS AND FUTURE WORK
In this section first we discuss potential challenges and
then provide a list of future work.
Carry-in jobs over hyperperiods: unlike work con-
serving algorithms, AD-PRM may face a situation in which
some jobs are late even after the hyperperiod and even if
the total utilization is smaller than 1. Fig. 2 shows such an
example. As can be seen, the last job of τ1 took one unit
of time from the beginning of the next hyperperiod. In this
particular example, the next hyperperiod will only have one
carry-in job and the schedule repeats afterwards. However,
since 1) the amount of carry-in load, and 2) the priority
of carry-in load, and 3) the number of carry-in jobs, affect
decisions of the IIP, we may need to consider many more
hyperperiods before we can reach to a conclusion about the
schedulability of the jobs. Note that even if U ≤ 1, a carry-
in job may appear. Two important consequences of having
carry-in jobs before a hyperperiod are: 1) experiments must
be performed on a longer periods of time (longer than 1 hy-
perperiod) and 2) if the schedule is stored in a timetable, the
size of the time table may be much larger than the number
of jobs in one hyperperiod.
Effect of utilization of τ1 on the the schedulability
test: As shown in Fig. 2, if τ1 has high utilization and a
low priority job forces τ1 to finish just before its deadline,
then a long blackout happens for the system, namely, for
a long duration of time, the system cannot respond to any
other job until all late jobs of τ1 are finished. Duration of
this blackout will be larger if τ1 has higher utilization. For
example, in Fig. 2, when the first job of τ1 which is released
at 20 becomes late, for 28 units of time (equivalent to 4
executions of τ1), no other low priority task will find any
chance to be executed. The blackouts are important for our
scheduling algorithm because τ1 plays an important role in
the schedule.
An idea for a schedulability test: In [4,7], a schedula-
bility test has been suggested for the P-RM which was based
on counting the number of vacant intervals and checking
whether each low priority task can have at least one vacant
interval in its priority level before its deadline. A vacant in-
terval is basically the slack between two consecutive jobs of
τ1, and has the length 2(T1 −C1). It may appear every 2T1
units of time because the P-RM synchronizes the schedule
of the jobs with the releases of τ1 using idle intervals. A
vacant interval can be used to schedule a low priority task,
as long as Ci ≤ 2(T1 − C1), because then in the worst-case,
each job takes one vacant interval to be scheduled. Thus,
for each priority level, we can count the minimum number
of available vacant intervals and make sure that each task
has at least one of them.
However, in our case, this type of schedulability tests may
not be easily applicable because after scheduling a low prior-
ity task which does not let the existing job of τ1 be finished
before the next job of τ1 is released, no other vacant interval
will be available in the system for a certain period of time
(i.e., the blackout). It means that some tasks may waste (or
need) more vacant intervals than 1. One possible solution is
to calculate the duration of time for which the system will
only execute jobs of τ1 after scheduling one job of τi. We
denote this interval by ωi and call it blackout level-i. To cal-
culate ωi we need to know the worst-case lateness, denoted
by θi which can be caused by a job of τi on a job of τ1. This
value can be upper-bounded by θi ≤ max{0, D1 − C1 − T1}
which happens if τ1 finishes at its deadline. Now, we can
calculate ωi using the following recursive equation:
ωi =
⌈ωi
T1
⌉
C1 + θi (2)
where the initial value of ωi is C1 + θi.
A new necessary schedulability condition: Since
each job can force at least 1 job of τ1 to finish at most
at D1 − C1, the maximum WCET of the tasks is bounded
by
Ci ≤ T1 +D1 − 2C1 (3)
Future Work: Using the concept of vacant intervals, we
plan to work on a schedulability test for AD-PRM. Then
11
we will extend it for sporadic tasks. Since idle-time inser-
tion policies need to have some information about the fu-
ture, without knowing anything about the maximum-inter-
arrival-time of the tasks, we may not be able to design a
schedulability test. Unlike work-conserving solutions, here,
a sequence of periodic releases may not create the worst-case
scenario because of the idle intervals.
5. EXPERIMENTAL RESULTS
We compare AD-PRM with CW-EDF [5], Group-based
EDF [2], NP-RM, and NP-EDF. Group-based EDF has been
developed for soft real-time systems. It groups the tasks
based on deadlines, and then, among the group with the
earliest deadline selects the task with the shortest execution
time. This algorithm does not provide any guarantee for
tasks with arbitrary deadlines.
The parameter of our experiment is the deadline of τ1.
We have measured the schedulability ratio by dividing the
number of randomly generated task sets that could actually
be scheduled by each algorithm divided by the total number
of task sets. Note that we have discarded those task sets that
had some late jobs at the end of hyperperiod. To generate a
random task set, first we choose random periods from range
[10, 500] for 5 tasks. Then we assign u1 from [0.3, 0.95] and
D1 from [T1, D
maxT1] where D
max is the parameter of the
experiment. Then we calculate C1 = u1T1. For the other
tasks, we assign Di = Ti and Ci is randomly selected from
interval [0.0001, T1 + D1 − 2C1]. We generate 1000 task
set per each value of the horizontal axis to obtain Fig. 3.
Average utilization of the each generated task set using these
parameters is about 0.8.
Fig. 3 shows that AD-PRM can efficiently schedule about
80% of the task sets. It is not yet as good as CW-EDF which
is a dynamic priority scheduling. We also observe that with
the increase in D1, EDF has better chance to schedule the
task set because most missed jobs of EDF are jobs of τ1.
Consequently, if τ1 has larger deadline, NP-EDF will have
higher schedulability. NP-EDF has almost 0 miss ratio for
the jobs of τn, and the highest miss ratio for the jobs of
τ1. Note that vertical axis in Fig. 3-(b) shows the ratio of
missed jobs of a task to the total number of its jobs. Thus,
sum of values of a column won’t give the total miss ratio.
6. CONCLUSIONS
In this work, we have shown how to modify Precautious-
RM to handle tasks with arbitrary deadlines and sporadic
releases. We have discussed the effect of the late jobs, i.e.,
those that cannot be finished before their next release, on the
next hyperperiod. We have shown that even if U ≤ 1, some
jobs may become late because our algorithm may schedule
idle intervals to guarantee deadlines. We have also presented
initial ideas for a schedulability test and discussed the future
work.
Acknowledgment
This work is supported by Alexander von Humboldt foun-
dation.
7. REFERENCES
[1] Y. Cai and M. C. Kong. Nonpreemptive scheduling of
periodic tasks in uni- and multiprocessor systems.
Algorithmica, 15(6):572–599, 1996.
Figure 3: Effect of Dmax on schedulability. Note that
Gr-EDF and NP-ED are almost overlapped.
[2] W. Li, K. Kavi, and R. Akl. A non-preemptive
scheduling algorithm for soft real-time systems.
Computers and Electrical Engineering, 33(1):12–29,
2007.
[3] M. Nasri, S. Baruah, G. Fohler, and M. Kargahi. On
the Optimality of RM and EDF for Non-Preemptive
Real-Time Harmonic Tasks. In International
Conference on Real-Time Networks and Systems
(RTNS), pages 211–220. ACM, 2014.
[4] M. Nasri and G. Fohler. Non-Work-Conserving
Scheduling of Non-Preemptive Hard Real-Time Tasks
Based on Fixed Priorities. In International Conference
on Real-Time Networks and Systems (RTNS), pages
309–318. ACM, 2015.
[5] M. Nasri and G. Fohler. Non-work-conserving
non-preemptive scheduling: motivations, challenges,
and potential solutions. In Euromicro Conference on
Real-Time Systems (ECRTS), 2016.
[6] M. Nasri, G. Fohler, and M. Kargahi. A Framework to
Construct Customized Harmonic Periods for Real-Time
Systems. In Euromicro Conference on Real-Time
Systems (ECRTS), pages 211–220, 2014.
[7] M. Nasri and M. Kargahi. Precautious-RM: a
predictable non-preemptive scheduling algorithm for
harmonic tasks. Real-Time Systems, 50(4):548–584,
2014.
[8] A. Thekkilakattil, R. Dobrin, and S. Punnekkat.
Quantifying the sub-optimality of non-preemptive
real-time scheduling. In Euromicro Conference on
Real-Time Systems (ECRTS), pages 113–122, 2013.
[9] Y. Wu. Real-time Control Design with Resource
Constraints. PhD thesis, Scuola Superiore SantAnna,
Pisa, Italy, 2009.
12
Tester Partitioning and Synchronization Algorithm For
Testing Real-Time Distributed Systems
Deepak Pal, Jüri Vain
Department of Computer Science
Tallinn University of Technology
Tallinn, Estonia 12186
{deepak.pal, juri.vain}@ttu.ee
ABSTRACT
Large-scale cyber-physical systems (CPS) have grown to the
size of global geographic distribution with numerous ser-
vices and applications forming an ubiquitous computing net-
work. Timing latencies introduced by geographically dis-
tributed ports of such CPS (or the system under test) affect
the response time of the centralized tester, thereby mak-
ing the testing infeasible. Consequently, the remote test-
ing approaches replaced by a distributed test architecture.
Sufficient timing conditions for remote online testing have
been proposed by David et al for remote ∆-testing method.
We extend the ∆-testing by deploying testers on fully dis-
tributed test architecture. This approach reduces the test
reaction time by almost a factor of two wherein the timing of
coordination messages is implemented based on model-based
testing platform DTRON.
Keywords
Model-based testing; Distributed systems; Low-latency sys-
tems
1. INTRODUCTION
The distributed large-scale systems have grown to the size
of global geographic distribution with numerous services and
applications forming an ubiquitous computing network. Ex-
amples of such systems are smart energy grids, transporta-
tion systems, manufacturing processes etc. which are ex-
pected to behave in a correct manner in real-time. The ge-
ographical distribution, communication and presence of nu-
merous components introduces timing imperfections which,
if not considered during the design and deployment phases
can lead to catastrophic outcomes that affects the reliability
of the system as a whole. Building a reliable CPS require
a system to be tested for performance in the presence of
these timing imperfections induced by various components
and the coordination between them. Timing latencies intro-
duced by geographically distributed ports of such CPS set
strict response time constraints also to the testing tools of
such systems.
Reaching sufficient test coverage by integration testing of
such systems in the presence of numerous latency factors
and their interdependency, is out of the reach of manual off-
line testing. Moreover, off-line testing of such systems is not
possible due to the non-deterministic nature of system un-
der test (SUT). Consequently, the off-line testing approaches
replaced by on-line distributed testing.
2. STATE OF THE ART
Testing distributed systems has been one of the model
based testing (MBT) challenges since the beginning of 90s.
One of the first attempt to standardize the test interfaces for
distributed testing was made in [1]. Early MBT approaches
represented the test configurations as systems that can be
modeled by finite state machines (FSM) with several dis-
tributed interfaces, called ports. An example of abstract
distributed test architecture is proposed in [2]. This archi-
tecture suggests the SUT contains several ports that can be
located physically far from each other. The testers are lo-
cated in these nodes that have direct access to ports. There
are also two strongly limiting assumptions: (i) the testers
cannot communicate and synchronize with one another un-
less they communicate through the SUT, and (ii) no global
clock is available. Under these assumptions a test gener-
ation method was developed in [2] for generating synchro-
nizable test sequences of multi-port finite state machines.
Two major issues which occur in the phase of test execu-
tion are synchronization and fault detectability problems.
However, it was shown in [3] that no method that is based
on the concept of synchronizable test sequences can ensure
full fault coverage for all the testers. The investigation in [4]
used the concept of controllability and observability to study
the problems of synchronization and fault detectability. A
coordination algorithm that allows the testers to exchange
messages through external reliable communication indepen-
dent of the SUT to solve the controllability and observabil-
ity problems was proposed in [5]. However, the introduction
of extra communication messages creates undesirable delays
and communication overhead to be addressed by tester mid-
dleware.
In [6], it is proposed to construct test sequences that cause
no controllability and observability problems. But the ap-
proach relies on assumption that SUT is deterministic which
makes it impractical for realistic testing applications. Alter-
natively, an online tester synthesis method is proposed in
[7] capable of handling non-deterministic behavior of SUT.
This approach can be used for centralized remote testing of
distributed systems but the issue of satisfying test timing
constraints remains. A mechanism to solve the controlla-
bility and observability problems in the presence of timing
constraints for testing distributed systems was proposed in
[8]. The investigation proved that even when the SUT does
not have critical time requirements, still it should respect
some timing constraints for solving controllability and ob-
servability issues.
More recently, pioneering results on testing timing cor-
13
rectness with remote testers was proposed in [9] where a
remote abstract tester was proposed for testing distributed
systems in a centralized manner. It was shown that if the
SUT ports are remotely observable and controllable then
2∆-condition is sufficient for satisfying timing correctness of
the test. Here, ∆ denotes an upper bound of message prop-
agation delay between tester and SUT ports. Though this
approach works reasonably well for systems with sufficient
timing margins, but it cannot be extended to systems with
the timing constraint close to 2-∆. This means that the test
inputs may not reach the input port in time and as a result
the testing becomes infeasible in such systems.
To shorten the tester reaction time, our recent work [10]
proposed an approach for improving the performance of ∆-
testing method (proposed originally for single remote tester)
by introducing multiple local testers attached directly to the
ports of SUT on a fully distributed architecture. It was
shown that this mapping to distributed architecture pre-
serves the correctness of testers so that if the monolithic
remote tester meets 2∆ requirement then the distributed
testers meet (one) ∆-controllability requirement.
In this paper, the main contribution is an improvement of
approach proposed in [10], which includes following
(i) An algorithm that maps a central remote tester model to
a set of communicating local tester models while preserving
the I/O behavior at testing ports;
(ii) A new adapter model to synchronize the communica-
tion between SUT model and local tester and for sending
coordination messages among local testers.
3. PROBLEM STATEMENT
3.1 Model-Based Testing
Model-Based Testing is an approach to the test construc-
tion and test execution process [12]. It is generally under-
stood as black-box conformance testing where the goal is
to check if the behavior observable on system interface con-
forms to a given requirements specification. The formal re-
quirements model of SUT describes how the SUT is required
to behave and allows generating test oracles. The test pur-
pose most often used in MBT is conformance testing. In
conformance testing the SUT is considered as a black-box,
i.e., only the inputs and outputs of the system are externally
controllable and observable respectively. During testing, a
tester executes selected test cases on the SUT and emits a
test verdict (pass, fail, inconclusive). The verdict shows cor-
rectness in the sense of input-output conformance (IOCO)
[11]. Due to native non-determinism of distributed systems
the natural choice is online testing where the test model is
executed in lock step with the SUT. The communication be-
tween the model and the SUT involves controllable inputs
of the SUT and observable outputs of the SUT which makes
easy to detect IOCO violations. For detailed overview of
MBT and related tools we refer to [12].
In our approach Uppaal Timed Automata (UTA) [13] are
used as a formalism for modeling SUT behavior. This choice
is motivated by the need to test the SUT with timing con-
straints so that the impact of propagation delays between
the SUT and the tester can be taken explicitly into account
when the test cases are generated and executed. For the
formal syntax and semantics of UTA we refer the reader to
[13] and [14].
Figure 1: Remote tester communication architec-
ture
Figure 2: SUT and Remote Tester Model
3.2 From Remote to Distributed Testing
Centralized remote testing is an approach that can be used
when the test implementation and the SUT are in differ-
ent locations. Consider a distributed CPS with geographi-
cally distributed and interacting applications that needs to
be tested for timing performance using a remote tester as
shown in Figure 1. This means that the remote tester will
generate an input for the SUT, waits for the result and con-
tinues with the next set of inputs until the test scenario has
been finished. Thus, the tester has to wait for the dura-
tion it takes to transmit the signal from the tester to the
SUT’s ports and the responses back from SUT ports to the
tester. Since, in most real-time distributed CPS, the tim-
ing constraints introduced are significant and time-varying
thereby introducing non-determinism. Under circumstances
where message propagation time is close to required test re-
action time, remote testing may lead to situations wherein
the centralized remote tester is unable to generate the neces-
sary inputs for the SUT within the expected timing window,
thereby rendering the testing infeasible [15].
Consequently, the centralized remote testing approach is
not suitable for testing a distributed system if the system has
strict timing constraints. The shortcomings of the central-
ized remote testing approach are mitigated with extending
the ∆-testing idea by mapping the monolithic remote tester
into multiple local testers as shown in Figure 3. This mod-
ification eliminates the message propagation time between
the local tester and SUT as the testers are attached directly
to the ports and it reduces the overall testing response time.
14
Figure 3: Distributed Local testers communication
architecture
4. PARTITIONING ALGORITHM
We apply Algorithm 1 to transform the centralized testing
architecture depicted in Figure 1 into a set of communicating
distributed local testers, the architecture of which is shown
in Figure 3. Let us consider the remote testing architecture
in Figure 1 and the corresponding model depicted in Figure
2. The SUT has 3 geographically distributed ports (p1, p2,
p3) that interact within the system, inputs in[1], in[2] and
in[3] at ports p1, p2 and p3 respectively and outputs out[1]
at port p1, out[2] at port p2, out[3] at port p3. Now, the
local testers are generated in two steps:
(i) a centralized remote tester is generated, e.g. by applying
the reactive planning online-tester synthesis method of [7];
(ii) a set of synchronizing local testers are derived by map-
ping the monolithic tester into a set of location specific tester
instances.
Each tester instance needs to know only the occurrence of
those I/O events at other ports that influence its behavior.
Possible reactions of the local tester to these events are al-
ready specified in the initial remote tester model and do not
need further feedback to the event sender. After mapping
the monolithic tester into distributed local testers, the mes-
sage propagation time between the local tester and the SUT
port is eliminated because the tester is attached directly to
the port. This means the overall testing response time is
also reduced, since previously the messages were transmit-
ted twice: from remote tester to the port and back from
port to the tester. The resulting architecture mitigates the
timing issue by replacing the bidirectional communication
with a unidirectional broadcast of the SUT output signals
between the distributed local testers.
Algorithm Description: Assume SUT has n geographi-
cally distributed ports, Algorithm 1 generates n communi-
cating local testers from monolithic remote tester. Let MRT
denote a monolithic remote tester model generated by ap-
plying the reactive planning online-tester synthesis method
[7]. Loc(SUT ) denotes a set of geographically different port
locations of SUT . The number of locations Loc(SUT ) =
{ln|n ∈ N}. Let Pln denotes a set of ports accessible in the
location ln. For each port locations ln, ln ∈ Loc(SUT ) we
copy the monolithic remote tester MRT to M ln to be trans-
formed to a location specific local tester instance (Line 4-6).
Line 7 adds an adapter model to each local tester instance.
The purpose of adding an additional automata i.e. adapters
to each location is that it synchronize the local communica-
Algorithm 1 Generating automatically distributed testers.
input : n-port monolithic remote tester model
output : n-communicating (LT1, .., LTn) local testers.
1: for all ln, ln ∈ Loc(SUT ) do
2: //location specific local tester instance
3: copy MRT to M ln
4: add Adapter model to each M ln
5: end for
6: for all M ln , n ∈ N do
7: for all edges ∈ M ln do
8: if (edge.chan[l ] ∧ chan[l ] ∈ Pln) then
9: rename & add same chan[l ] to Adapter
10: end if
11: if (edge.chan[l ]∧ chan[l ] /∈ Pln) then
12: if(chan[l ]! : channel Emission) then
13: replace chan[l ]! with co-action Reception
14: rename & add same chan[l ] to Adapter
15: end if
16: if(chan[l ]? : channel Reception) then
17: rename & add same chan[l ] to Adapter
18: end if
19: end if
20: end for
21: end for
tion between SUT local ports and a local tester with other
testers in different locations. Its model is derived from re-
mote tester model by adding original channels of SUT and
by renaming channels of local testers. The loop in Line 9 and
10 says that for each local testers model M ln we go through
all the edges in M ln . If the edge has a synchronizing channel
and the channel belongs to same port location Pln then we
rename the chan[l ] shown in Figure 4 and add the same
chan[l ] to adapter model (Line 11-13). Second case in Line
14-20, if the edge has a synchronizing channel and the chan-
nel does not belong to same port location Pln then we do the
following (i) if the channel’s action is Emission i.e in[l ]!, we
replace it with the co-action Reception channel, rename it
and add it to the adapter model, (ii) if the channel’s action
is Reception i.e out[l ]?, we rename it and add it to adapter
model.
Figure 4 represents the generated local tester models with
their corresponding parameterised adapter model where pa-
rameter L denotes the geographical location. In this com-
position of models it is shown that local testers work to-
gether with local adapters. The adapters synchronize the
local communication between SUT local ports and a local
tester with other testers in different locations i.e. in[1] and
out[1] are channels between SUT and adapter; in_[1] and
out_[1] are channels between the local adapter and local
tester; and in__[1] and out__[1] are the channels between
the local adapter and other local testers.
5. CORRECTNESS OF TESTER DISTRIBU-
TION ALGORITHM
The generated local testers are correct by construction
which means that the partitioning preserves their correct-
ness because the synchronization of tester local instances is
preserved due to auxiliary synchronization channels intro-
duced by the algorithm. Synchronization is preserved also
among local testers and with SUT. The communication be-
15
Figure 4: Local tester at Geographic Location 1, 2 and 3.
tween adapters and local testers are synchronized without af-
fecting SUT actions and actions on other tester ports which
guarantee that generated local testers are bi-similar by con-
struction. As for method implementation, the local testers
are executed and communicating via distributed test execu-
tion environment DTRON [16]. We demonstrate that the
distributed deployment architecture supported by DTRON
and its message serialization service allows reducing the to-
tal test reaction time by almost a factor of two.
6. CONCLUSION
We have proposed a methodology and algorithm to trans-
form the centralized testing architecture into a set of com-
municating distributed local testers which extend our pre-
vious work [10] by showing well-defined model transforma-
tions and introducing test adapter model templates. As for
method implementation, the local testers are executed and
communicating via distributed test execution environment
DTRON.
7. ACKNOWLEDGMENTS
This research is partially supported by Project IUT33-13
“Strong warranties software methodologies, tools and pro-
cesses”and doctoral studies TTU, Estonia.
8. REFERENCES
[1] ISO. Information Technology, Open Systems
Interconnection, Conformance Testing Methodology
and Framework - Parts 1-5. International Standard
IS-9646. ISO, Geneve 1991.
[2] Luo, G., Dssouli, R., v. Bochmann, G., Venkataram,
P., Ghedamsi, A.: Test generation with respect to
distributed interfaces. Computer Standards &
Interfaces, vol. 16, iss. 2, pp.119–132. Elsevier 1994.
[3] Sarikaya, B., v. Bochmann, G.: Synchronization and
specification issues in protocol testing. In: IEEE
Trans. Commun., pp. 389–395. IEEE Press, New York
1984.
[4] M. Benattou, L. Cacciari, R. Pasini, and O. Rafiq,
Principles and Tools for Testing Open Distributed
Systems, Proc. 12th International Workshop Testing
of Communicating Systems (IWTCS), pp. 77–92,
Sept. 1999.
[5] L. Cacciari and O. Rafiq, Controllability and
Observability in Distributed Testing, Information and
Software Technology, 1999.
[6] Hierons, R. M., Merayo, M. G., Nunez, M.:
Implementation relations and test generation for
systems with distributed interfaces. Distributed
Computing, vol. 25, no. 1, pp. 35–62. Springer-Verlag
2012.
[7] Vain, J., Kaaramees, M., Markvardt, M.: Online
testing of nondeterministic systems with reactive
planning tester. In: Petre, L., Sere, K., Troubitsyna,
E. (eds.) Dependability and Computer Engineering:
Concepts for Software-Intensive Systems, pp. 113–150.
IGI Global, Hershey 2012.
[8] Khoumsi Ahmed, A Temporal Approach for Testing
Distributed Systems, Journal IEEE Transactions on
Software Engineering, Volume 28 Issue 11, Page
1085-1103, IEEE Press Piscataway, NJ, USA,
November 2002.
[9] David, A., Larsen, K. G., Mikuionis, M., Nguena
Timo, O. L., Rollet, A: Remote Testing of Timed
Specifications. In: Proceedings of the 25th IFIP
International Conference on Testing Software and
Systems (ICTSS 2013), pp. 65–81. Springer,
Heidelberg 2013.
[10] Vain, J.; Halling, E.; Kanter, G.; Anier, A.; Pal, D.
Model-based testing of real-time distributed systems.
12th International Baltic Conference on Databases
and Information Systems, Springer, 1-14, Riga, 2016.
[11] Tretmans, J.: Test generation with inputs, outputs
and repetitive quiescence. Software - Concepts and
Tools, vol. 17, no. 3, pp. 103–120. Springer-Verlag,
1996.
[12] Utting, M., Pretschner, A., and Legeard, B: A
taxonomy of Model-based Testing, Software Testing,
Verification & Reliability, vol. 22, iss. 5, pp. 297–312.
John Wiley and Sons Ltd., Chichester, UK, 2012.
[13] Behrmann, G., David, A., Larsen, K. G.: A Tutorial
on Uppaal, In: Bernardo, M., Corradini, F. (eds.)
Formal Methods for the Design of Real-Time Systems.
LNCS, vol. 3185, pp. 200–236. Springer, Heidelberg,
2004.
[14] Bengtsson, J., Yi, W.: Timed Automata: Semantics,
Algorithms and Tools, In: Desel, J., Reisig, W.,
Rozenberg, G. (eds.) Lectures on Concurrency and
Petri Nets: Advances in Petri Nets. LNCS, vol. 3098,
pp. 87–124. Springer, Heidelberg 2004.
[15] Segala, R.: Quiescence, fairness, testing, and the
notion of implementation. In: Best, E. (eds.) 4th
Intrenational Conference on Concurrency Theory
(CONCUR’93). LNCS, vol. 715, pp. 324–338.
Springer, Heidelberg, 1993.
[16] DTRON - Extension of TRON for distributed testing,
http://www.cs.ttu.ee/dtron.
16
Framework to Generate and Validate Embedded
Decision Trees with Missing Data
Arwa Khannoussi*, Catherine Dezan*, Patrick Meyer+
* Université de Bretagne Occidentale
+ Institut Mines-Télécom, Télécom Bretagne
UMR CNRS 6285 Lab-STICC
arwa.khannoussi@etudiant.univ-brest.fr
catherine.dezan@univ-brest.fr
patrick.meyer@telecom-bretagne.eu
ABSTRACT
Autonomous vehicles or devices like drones and missiles
must make decisions without human assistance. Em-
bedded inference engines, based for example on deci-
sion trees, are used to reach this goal which requires
the respect of hardware and real-time constraints.
This paper proposes a generic framework which can
generate automatically the adequate hardware solution
taking into account the real-time and the hardware con-
straints. Moreover, the proposed solution supports the
case of faulty sensors which generate missing data.
Experimental results obtained on FPGA and CPU
suggest that it is better to consider missing data from
learning phase instead of considering it just in the clas-
sification phase. Besides, the use of the pipeline tech-
nique provides a better latency, which is adequate for
real-time applications.
Keywords
Decision Trees; Missing Data; Hardware Implementa-
tion; Real-time; Deadline
1. INTRODUCTION
Autonomous vehicles or devices such as drones or mis-
siles need embedded inference engines to make decision
without human assistance. These engines, in their em-
bedded version, should respect hardware constraints in
terms of available resources, performance and energy
consumption. The embedded decision is made with re-
spect to data provided by different sensors (GPS, IR,...)
with different frequencies. This leads to the definition
of a set of deadlines necessary to manage in real-time
the flow of values provided by the sensors. In this study,
we suppose that the sensors can malfunction and con-
sequently produce no data.
In machine learning such inference engines include
artificial neural networks [5], decision trees (DTs) [10],
support vector machines [13], and many others. In this
work, we study decision trees, because they are one of
the most popular models and they can easily be inter-
preted and validated by humans.
Decision trees are typically constructed from the data
using several algorithms like CART (Classification and
Regression Tree) [6], C4.5 [9].. . . Those algorithms can
be implemented and executed on traditional systems.
This implementation can not bring a big improvement
in term of latency which is an important characteristic
to fulfill the real-time constraints. In order to improve
data processing latency and the throughput when con-
sidering different instances, a hardware implementation
is considered as a solution. However, hardware imple-
mentations of decision trees has not been investigated
thoroughly so far.
To our knowledge, [4, 8, 7] are the only three pa-
pers available in the literature which discuss the hard-
ware implementation of decision trees. Other papers are
based on these seminal references [11, 12]. A. Bermak
and D. Martinez [4] propose a hardware realization of
decision trees using the 2-dimensional systolic array ar-
chitecture. J.R. Struharik [8] describes a hardware im-
plementation of decision trees for a specific problem
only, while in [7], several architectures are proposed
for the hardware realization of arbitrary decision trees.
However, in all those references, the architectures are
implemented manually and each tree has its own imple-
mentation.
In this paper we propose a generic framework that al-
lows generating decision trees using the C4.5 algorithm
because it handles training data with missing values
and generating the adequate hardware solution auto-
matically using Vivado HLS tools [1]. Data quality is a
major concern in our work. It is well known that the
amount of missing data clearly affects the prediction
accuracy of decision trees. Our framework can there-
fore generate, from a configuration, the missing data
and treat them using different methods. As we will
show in this article, this framework actually is a tool to
validate the hardware solution by comparing its results
with those of the software solution.
17
The rest of the article is structured as follows. In
Section 2, we present the different architectures of the
decision trees hardware implementations that are al-
ready proposed in the literature. Section 3 is devoted
to the presentation of our framework, and in Section 4
we present some experiments and the results with the
framework, before concluding in Section 5.
2. RELATED WORK
In general, the input of a machine learning model is
a data set composed of a set of instances evaluated on
a set of attributes. For autonomous vehicles or devices,
an attribute corresponds to one of the sensors and an
instance includes the values of the different sensors at
a given instant. In decision trees, every node of a tree
represents a test that includes one or more attributes,
every branch descending from a node represents one of
the possible outcomes of the test, and every leaf matches
with one of the classes. To evaluate the performance of
decision trees we use the classification accuracy, i.e.,
the rate of correct predictions made by the model over
a data set.
In [8], the realization of the decision tree requires to
implement every node as a separate module. The prob-
lem with this architecture is that new instances cannot
be classified before the end of the classification of the
previous ones. A more advanced hardware realization
is proposed in [4]. In order to provide a faster through-
put, only two levels are used, irrelevant of the depth of
the original decision tree.
Several architectures suitable for the hardware real-
ization of decision trees are proposed in [7] and are
based on the concept of universal node. Actually, to
classify an instance, one node needs to be evaluated per
level so only a subset of nodes will be visited, which
corresponds to the path from the root to the leaf node.
The number of modules required for the realization of
a decision tree is equal to the depth of the tree. One
universal node module per level is enough for the real-
ization. This Single Module per Level (SMpL) archi-
tecture [7] is represented in Figure 1. The classification
of an instance using the SMpL architecture can be ex-
ecuted sequentially or with the use of a pipeline. It is
also possible to use a single universal node to evalu-
ate every tree node. This architecture, called Universal
Node (UN) [7] (Figure 2), is used when the speed of
classification is not critical.
The implementation of those architectures is done
manually and each decision tree is therefore implemented
separately. In this contribution, we propose a frame-
work which handles this problem. It is an automatic
tool that can generate an arbitrary decision tree’s struc-
ture code on hardware, based on the SMpL architecture.
It also allows generating and treat missing data using
several methods, and to handle the whole process of
decision trees learning, from the generation of the deci-
sion tree, to the result of the classification in terms of
Figure 1: The Single
Module per Level (SMpL)
architecture for a decision
tree of depth 3 [7].
Figure 2: The struc-
ture of the Universal Node
(UN) architecture [7]
accuracy.
3. PROPOSED FRAMEWORK
Figure 3 details the structure of our framework and
the different phases of the implementation.
Figure 3: The structure of the framework.
Besides generating the decision tree structure code,
another essential purpose of the framework that we pro-
pose is to study the behavior of decision trees when con-
fronted with missing data. Therefore, the first part of
our framework is to generate automatically the missing
data by taking as input the data set and the configu-
ration of the missing data. Actually, we consider three
such missing data configurations:
• Uni-variate: missing data appear only in one at-
tribute or sensor;
• Monotonic: one or more attributes or sensors are
affected;
• Non-monotonic: missing data appears randomly
in the data set.
18
Note that in our case, typically missing data is caused
by a faulty sensor. Missing data can be taken into ac-
count during both the learning and the classification
phase. The input of the framework is the data set and
missing data configuration to define which type and the
percentage of missing data will be generated and also
the number of attributes that will be affected.
The learning phase involves the generation of the de-
cision tree from the learning data set. In this phase,
an implementation of C4.5 algorithm on R [2] has been
used. C4.5 works with the concept of information en-
tropy. The training data is a set of classified samples
having p-dimensional vectors defining the attributes of
the sample. It generates a decision tree where at each
node C4.5 chooses the attribute that most effectively
splits its set of samples into subsets enriched in one
class or other. The splitting criterion is the normal-
ized information gain. The attribute with the highest
normalized information gain is chosen to make the de-
cision. The output of this phase is the description file
of the decision tree that will be used in the next phase.
The classification phase, can be executed on two dif-
ferent device targets, CPU and FPGA. For the first one,
we also used the C4.5 algorithm for the classification
and the treatment of missing data. In fact C4.5 uses
a probabilistic approach to handle missing data. This
part of the framework allows testing the decision tree
with a number of data sets fixed by the user, and the
outputs are the mean and the standard deviation of the
different accuracy rates.
From the decision tree, that has already been gen-
erated by the learning phase, our framework generates
automatically the decision tree structure code, based on
the SMpL architecture [7]. The framework is generic
tool that can handle any decision tree regardless of the
number of nodes or the type of attributes, given that
tle framework is a generic tool that can handle any de-
cision tree. For the missing data, the mean imputation
method is used. It consists in replacing the missing
value for a given attribute by the mean of all known
values of that attribute. Vivado HLS tools are used to
perform the High-Level Synthesis [1].
This framework can manipulate different type of at-
tributes, which facilitates the study of the resources
used in the hardware implementation of the decision
trees. It produces, the resources used and the latency
as output. In order to validate our hardware imple-
mentation, this framework allows comparing it with the
software version.
4. EXPERIMENTS
In this section, we first study the behavior of decision
trees when confronted with missing data in both the
learning and the classification phase. Then, the effect of
the attribute type and the use of pipeline technique on
the required resources are experimented in the hardware
version of the classification phase.
The following data sets are used to illustrate the use
of the framework: “Iris” (4 numerical attributes and 150
instances) and “Balance Scale” (4 nominal attributes
and 625 instances); see [3] for further details.
As part of the study of the effects of missing data on
the accuracy, we launched a series of experiments on
the software solution. Testing takes place as follows:
from the initial data set, the framework generates 1000
different data sets with different types and percentage of
missing data. These are then used in both the learning
and classification phases. The results are presented in
Figures 4 and 5. Both curves show that classification
Figure 4: Average class.
accuracy on “Iris” w.r.t.
missing data (%).
Figure 5: Average class.
accuracy on “Balance”
w.r.t. missing data (%).
accuracy is higher if missing data is already taken into
account in the learning phase.
For the classification phase, we suppose that the de-
cision tree is already generated. The target device is an
FPGA Zynq of the Xilinx Zedboard. For each data set,
the following output is produced : the accuracy with
missing data, resource occupation in terms of percent-
age of LUT (Lookup Table) and (FF) Flip Flop and the
latency for the performance aspect.
The type of attributes is one of features that can af-
fect the performances of decision trees. We therefore
used three different types of attributes for the hardware
implementation : Float a number coded on 32 bits, ap-
ufixed<8,6> a number on 8 bits with 6 integer bits and
2 decimal places, and, ap-ufixed<4,2> a number coded
on 4 bits with 2 integer bits and 2 decimal places.
Data set
Accuracy
Iris Balance
Float <8,6> <4,2> Float <8,6> <4,2>
10 97.15 96.91 88.66 88,94 88,94 41.88
30 94.90 94.57 84.83 86.12 86.12 42.76
50 93.66 93.41 82.16 82.86 82.86 44.24
80 90.49 90.35 78.00 79.56 79.56 45.36
95 88.58 88.40 75.49 78.28 78.28 46.44
Table 1: Classification accuracies with respect to miss-
ing data percentages and various types of attributes.
Table 1 shows that classification accuracy decreases
with increasing amounts of missing data and number of
affected attributes. Both the type Float and the type
19
ap-ufixed<8,6> have nearly the same accuracy because
the value of the attributes can be represented by 8 bits
without degrading the accuracy.
The use of the pipeline directive was also one the
features used in our study. If we want decision trees to
be suitable for real-time applications, those features can
be used to respect deadlines by reducing the latency.
Attribute Type HLS
directive
Benchmark %
LUT
%
FF
Latency(nb
cycles)
Float
no pipeline
Iris 2.0 2.0 7051
Balance 11.0 3.0 730001
pipeline
Iris 5.0 6.0 6001
Balance 17.0 8.0 65002
Ap-ufixed<8,6>
no pipeline
Iris 1.5 1.3 7051
Balance 10.0 2.2 730001
pipeline
Iris 1.7 5.0 6001
Balance 11.0 6.5 65002
Ap-ufixed<4,2>
no pipeline
Iris 1.0 1.0 7051
Balance 10.0 5.7 730001
pipeline
Iris 0.7 3.7 6001
Balance 9.0 6.1 65002
Table 2: FPGA implementation results.
Table 2 shows that the type Float consumes more re-
sources than ap-ufixed<8,6> and ap-ufixed<4,2>. The
use of the pipeline directive, one of the directives of
Vivado HLS, requires more resources but it ensures a
better latency for a set of instances while maintaining
a sequential access to the data set.
What we can conclude from the results presented in
this section is that it is better to consider the missing
data right from the learning phase (instead of consider-
ing it just in the classification phase). For the hardware
implementation the type of attributes can affect both
the accuracy and the use of resources. We notice that
the type ap-ufixed<8,6> is a better choice in our exper-
iments given that it ensures a good accuracy and uses
less resources. The use of pipeline technique provides a
better latency but consumes more resources than non-
use of pipeline.
5. CONCLUSIONS
In this paper, we presented a framework which, to
our knowledge, is original. It handles a process which
can be summarized as follows: first the generation and
handling of missing data, second the learning phase by
generating the decision tree, third the automatic gener-
ation of the decision trees code, and finally the valida-
tion of the embedded decision trees with missing data
by comparing the software and hardware results.
This framework gives us the possibility to deal with
different types of attributes and can provide parallel im-
plementations on FPGA by means of High Level Syn-
thesis Tool, such as Vivado HLS. This latter ability al-
lows generating the decision trees suitable for real-time
embedded applications.
The perspective of generalizing the framework to dif-
ferent machine learning technologies was considered since
the beginning. It can be extended to use Neural Net-
works (NNs) or Bayesian Networks (BNs) or Support
Vector Machines (SVM) as machine learning models.
6. ACKNOWLEDGMENTS
This work was supported by the LATERAL laboratory,
an industrial cooperation Thales Optronique (TOSA)/Lab-
STICC of UBO.
7. REFERENCES
[1] http://www.xilinx.com/products/design-
tools/vivado/integration/esl-design.html.
[2] https://cran.r-
project.org/web/packages/RWeka/RWeka.pdf.
[3] https://archive.ics.uci.edu/ml/datasets.html.
[4] A. Bermak and D. Martinez. A compact 3d vlsi
classifier using bagging threshold network
ensembles. In IEEE Trans. on Neural Networks,
volume 14, pages 1097–1109, September 2003.
[5] C. Bishop. Neural networks for pattern
recognition. In Oxford University Press, 1995.
[6] L. Breiman, J. H. Friedman, R. A. Olshen, and
C. J. Stone. Classification and regression trees.
Technical report, Wadsworth International,
Monterey, CA, 1984.
[7] J.R.Struharik. Implementing decision trees in
hardware. In IEEE 9th International Symposium
on Intelligent Systems and Informatics. September
8-10, 2011,Subotica,Serbi.
[8] S. Lopez-Estrada and R. Cumplido. Decision tree
based fpga architecture for texture sea state
classification. In Reconfigurable Computing and
FPGA’s ReConFig 2006 IEEE International
Conference,pp. 1-7.
[9] J. R. Quinlan. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA,
1993.
[10] L. Rokach and O. Maimon. Top-down induction
of decision trees – a survey. In IEEE Trans. on
Systems, Man and Cybernetics, vol.35, no.4,
November 2005, pp.476-487.
[11] R. Struharik. Decision tree ensemble hardware
accelerators for embedded applications. In SISY
2015, IEEE 13th International Symposium on
Intelligent Systems and Informatics, September
17–19, 2015, Subotica, Serbia.
[12] R. Struharik. Ip cores for hardware acceleration of
decision tree ensemble classifiers. In INES 2015.
IEEE 19th International Conference on Intelligent
Engineering Systems. September 3–5, 2015,
Bratislava, Slovakia.
[13] V. Vapnik. Statistical learning theory. In New
York, Wiley, 1998.
20
Quantifying the Flexibility of Real-Time Systems
Rafik Henia1, Alain Girault2, Christophe Prévot1,2, Sophie Quinton2, Laurent Rioux1
1 Thales Research & Technology
2 Inria Grenoble Rhône-Alpes
1. INTRODUCTION
The life cycle of many safety or mission critical real-time
systems, such as avionic systems, satellites or software de-
fined radios, is superior to 10 years whereas the supporting
ICT technologies have a much faster evolution rate. This
mismatch between system life cycle on one side and software
life cycle on the other side is a challenge for performance
engineers. It is therefore essential for commercial offers to
allow technology evolutions and upgrades of the architecture
and functions after the initial system deployment. Besides,
this must be done while preserving the satisfaction of timing
performance requirements over the entire system lifetime.
Our objective is to develop novel techniques to quantify
the ability of a real-time system to cope with future soft-
ware evolutions, while remaining schedulable. We propose
to define the flexibility of a system as its ability to schedule
a new software function and show how this concept can be
used to anticipate at design time schedulability bottlenecks
that may prove problematic, after deployment, for system
evolution. In this paper we focus on an evolution scenario
where a single task is added to an existing system running on
a single-core processor. Our task model here is restricted to
independent periodic tasks with implicit deadlines and the
scheduling policy is static priority preemptive. Of course our
goal is to generalize our approach to more complex systems.
The research area that is most closely related to our prob-
lem is sensitivity analysis [9, 2, 5, 7]. Sensitivity analysis is
used (1) to provide guarantees on the schedulability of a sys-
tem in case of uncertainty on the system parameters, or (2)
given a non schedulable system, to find a set of changes that
lead to a schedulable system.
Most related work on sensitivity analysis addresses changes
of one type of parameter, e.g., changes of task worst-case
execution times (WCETs) or changes of periods — but not
both. In [2] for example, the authors analyze the sensitivity
of a system to changes in the arrival frequency of all tasks, or
in the WCET of all tasks. In [7], the authors are interested
in sensitivity analysis of systems with shared resources, for
which they study the impact of an increase of the WCETs
on system schedulability.
Dealing with different parameters (i.e. periods and prior-
ities) is the approach used in [8]. Here the authors define
the sensitivity using evolutionary algorithms and a stochas-
tic approach. It will be interesting to compare our results
with this method.
In contrast with the above mentioned approaches, we do
not allow parameters of the deployed system to be modi-
fied in according with industrial practice. But we need to
handle at the same time changes in the period, priority and
WCET of the task to be added. As a result, the set of pos-
sible system configurations that need to be checked is much
smaller than in classical sensitivity analysis and we hope to
be able to handle much more complex systems that what
state-of-the-art sensitivity analysis can handle.
Other papers like [10, 1, 3] deal with flexibility related to
task allocation, priority assignment or scenario based opti-
mization. In [3], the authors use concepts similar to ours to
define an optimal or a robust priority assignment. In [10],
the authors deal with task allocation and priority assignment
to maximize the extensibility of each task chain, where ex-
tensibility is the maximum execution time it is possible to
increase the WCET before missing the deadline. In [1] the
flexibility of a system is defined according to scenarios. To
define the flexibility it is necessary to define possible scenar-
ios of change.
Related work that deals with flexibility is used to define
at design time what is the best task allocation or priority as-
signment. But most of the time in an industrial context, the
designer cannot take into account only timing constraints
and some functionalities have fixed parameters. Moreover
we are interested in system evolution when most parameters
of the system are already defined. It is at this point unclear
if our problem (adding a task) can be precisely encoded into
existing methods dealing with increased execution times.
As a summary, by restricting ourselves to a simple change
scenario with industrial relevance, we hope to come up with
an approach that can provide guidance for the evolution
of complex systems, which cannot be covered by existing
sensitivity analysis techniques. Finally, to the best of our
knowledge, no existing work discusses the notion of limiting
task, i.e. the task which will miss its deadline first. The
final aim is to help the designer by giving him information to
allow a system to evolve while respecting timing constraints.
2. SYSTEM MODEL
Unless otherwise specified all the parameters defined in
the following have positive integer values. In particular, we
assume a discrete time clock.
We consider a uni-processor real-time system S consisting
of a finite set of n independent tasks scheduled according
to the Static Priority Preemptive (SPP) scheduling policy.
Each task τi, i ∈ [1, n], is defined by a tuple (πi, Ti, Ci) where
πi is the priority, Ti the period, and Ci an upper-bound on
the worst case execution time (WCET). Each task τi has an
implicit deadline Di = Ti, meaning that a given activation
of τi must finish before the next activation of τi.
21
We assume that different tasks have different priorities
and use as convention that πi < πj means that τi has a
higher priority than τj . For the purpose of our evolution
scenario, we also suppose that for any two tasks τi and τj
with πi < πj , it is always possible to define a new task τnew
with πi < πnew < πj . This is done without loss of generality
as priorities are only used to define a total order on tasks.
For each τi in S, hp(i) is the set of tasks of S that have
a higher priority than τi, while lp(i) is the set of tasks of S
that have a lower priority than τi.
The execution of a task τi is triggered periodically with
period Ti and each activation of τi generates a new instance.
A task instance is defined by its activation time, possible
preemption delays, and finish time. Preemption delays are
due to the task instance being blocked by higher priority task
instances. Each instance of τi finishes at the latest after
having being scheduled (i.e., not counting the preemption
delays) for Ci units of time.
3. PROBLEM STATEMENT
We are interested in system evolutions that may happen
after system delivery. We focus here on a simple evolution
scenario where a new task τnew defined by (πnew, Tnew, Cnew)
is added to a schedulable system S, thus yielding the sys-
tem Snew = S ∪ {τnew}. From now on, τnew will denote the
task (πnew, Tnew, Cnew) and Snew will denote the system
S ∪ {τnew}.
We wish to answer the following question: Given a schedu-
lable system S and a task τnew with unknown parameters:
how can we quantify the flexibility of S, that is, its ability
to accommodate τnew while remaining schedulable?
To address these issues, we start below by formalizing the
notions of evolution and flexibility of a system.
Definition 1. Evolution: An evolution of a system S is a
task τnew where πnew belongs to [min
i
πi−1,max
i
πi+1]\{πi}i
and (Tnew, Cnew) belongs to N+ × N+, such that the CPU
load after the evolution does not exceed 100%:
Cnew
Tnew
+
∑
i
Ci
Ti
≤ 1
Definition 2. Valid evolution: S being a schedulable sys-
tem, an evolution τnew is valid for S if Snew is schedulable.
We denote by FS the set of all valid evolutions for system S.
Note that if τnew is in FS , then for any 0 < C
′
new < Cnew,
the task (πnew, Tnew, C
′
new) is also in FS . Moreover for any
T ′new > Tnew, the task (πnew, T
′
new, Cnew) is also in FS . The
flexibility of a system quantifies its ability to schedule a new
task. We focus in this paper on the flexibility w.r.t. Cnew.
The notion of flexibility w.r.t. Tnew is left for future work.
Definition 3. Flexibility: Let S be a schedulable system.
The flexibility of S is the partial function flexS from N
+×N+
into N+ such that (πnew, Tnew,flexS(πnew, Tnew)) is in FS
and flexS(πnew, Tnew) is maximal.
The function flex returns, for all (πnew, Tnew), the value
of the maximum WCET such that S ∪ {(π, T,flexS(π, T ))}
is schedulable. This function is partial as such an WCET
may not exist. In the rest of the paper, we first focus on
methods for computing flex and then we will show how we
can use it for system design.
4. SLACK ANALYSIS
Let us first briefly recall the standard analysis used to es-
tablish the schedulability of systems of independent periodic
tasks on a single processor under the SPP policy [6].
The response time of an instance of a task τi is the de-
lay between its activation and its finish time. The worst
case response time of τi (WCRT) is the maximum response
time over all instances of τi; it is denoted ri. A system is
schedulable if ri ≤ Ti for any task τi.
A critical instant of τi is an activation scenario that max-
imises the response time of τi. A task is said to be schedu-
lable if and only if its WCRT is smaller than its (implicit)
deadline: ri ≤ Ti. For the systems we consider, a crit-
ical instant of τi occurs whenever an activation of a task
instance occurs simultaneously with the activation of all
higher-priority tasks. The WCRT of τi can thus be obtained
by computing:
ri = Ci +
∑
τj∈hp(i)
⌈
ri
Tj
⌉
Cj (1)
To quantify the flexibility of a system, we start by intro-
ducing the slack of a task, which takes into account all the
preemptions from higher priority tasks [4].
Definition 4. The slack of a task τi is the maximum value
it is possible to increase Ci while keeping τi schedulable.
The slack Sl i of a task τi can be computed using the al-
gorithm presented in [4].
Theorem 1. Let S be a schedulable system and τnew a
task. Snew is schedulable if:
∀τi ∈ lp(new),
⌈
Ti
Tnew
⌉
Cnew ≤ Sl i (2)
and
rnew ≤ Tnew (3)
Proof sketch of Theorem 1. We have to prove that,
for each τi ∈ S ∪ {τnew}, we have ri ≤ Ti. Since S is
schedulable, this holds for each τh ∈ hp(new). For τnew
itself, this holds directly by Eq. (3). And for each τ` ∈
lp(new), the preemptions of the tasks in hp(`) are already
taken into account in the slack Sl`, hence there only remains
to take into account the preemptions of τnew on τ`, which is
precisely the goal of Eq. (2).
This gives a sufficient but non necessary condition because
the slack Sli is a lower bound on the slack available for τi.
The reason the criterion is not necessary is that the preemp-
tion delays induced by τnew may be overapproximated.
5. FLEXIBILITY ANALYSIS
Recall that the flexibility of a system is quantified through
the function flex . We now show how to approximate flex
using the above sufficient schedulability condition for Snew.
Definition 5. For a given period Tnew and priority πnew
of τnew, denote
• CS the largest WCET of τnew such that S is schedu-
lable after evolving,
22
• Cτnew the largest WCET of τnew such that τnew is
schedulable after evolving.
Theorem 2. S being a system and τnew an evolution, let
CmaxS be the largest WCET allowed by Eq. (2) i.e. a lower
bound of CS. Let C
max
τnew be the largest WCET allowed by
Eq. (3) i.e. a lower bound of Cτnew . Then
CmaxSnew = min {C
max
S , C
max
τnew}
is a lower bound of flexS(πnew, Tnew).
Proof sketch of Theorem 2. Theorem 1 gives a guar-
antee on the schedulability of Snew depending on τnew which
is a sufficient non necessary condition. As a consequence,
Theorem 2 based on Theorem 1 is also a sufficient and non
necessary condition, i.e., a lower bound of the largest Cnew
that guarantees the schedulability of Snew for fixed πnew and
Tnew — which is given by flexS .
We can compute both CmaxS and C
max
τnew using Theorem 3.
Theorem 3. Let S be a schedulable system and τnew a
task. Then we have CmaxS and C
max
τnew :
CmaxS = min
τi∈lp(new)
 Sl i⌈
Ti
Tnew
⌉
 (4)
and
Cmaxτnew =
Tnew − ∑
τj∈hp(new)
⌈
Tnew
Tj
⌉
Cj
 (5)
Proof. Eq. (2) implies that, for all task τi in lp(new),
we have Cnew ≤ Sli/d TiTnew e. Since Cnew must be in N
+, it
follows that
Cnew ≤ min
τi∈lp(new)
⌊
Sli/d
Ti
Tnew
e
⌋
hence Eq. (4). We found the largest Cnew by computing the
largest Slnew. Again, since Cnew must be in N+, this leads
to Eq. (5).
Intuitively, if τnew has a high priority, then it will be easy
to schedule but it will have a stronger impact on S. And
this impact will be greater if the period Tnew is small. In
contrast, a task τnew with a low priority will have a smaller
impact on the other tasks and it will be harder to schedule
with a small period.
Note that the function flexS is partial. Both Conditions (4)
and (5) on the flexibility of τnew imply that some tuples
(πnew, Tnew) lead to non schedulable systems. These tuples
imply that at least one task in Snew will have not enough
time to be executed. This is the case in a system if we add
τnew with a high priority and a small period.
6. LIMITING TASK OF THE SYSTEM
We now focus on the fact that for a priority πnew and
a period Tnew, C
max
S is limited by one, or possibly several
tasks in lp(new). We call the lowest priority task among
them the limiting task.
Definition 6. The limiting task of S w.r.t. a priority πnew
and a period Tnew, denoted τ`, is the lowest priority task τ`
such that:
` = arg min
τi∈lp(new)
 Sl i⌈
Ti
Tnew
⌉
 (6)
Note that our definition of limiting task is based on the
sufficient condition in Theorem 1. As a result, it is a good
indicator of where the bottlenecks for software evolution are,
but it is not an exact criterion (yet).
We are particularly interested in the fact that some tasks
never limit CmaxS , whatever the priority and period of τnew.
Theorem 4. Consider the system S. The tasks that will
never limit the system are all the tasks τi such that:
∃τj ∈ S s.t. τj ∈ lp(i) ∧ Tj ≤ Ti
Proof sketch of Theorem 4. Tasks for which there ex-
ists another task of lower priority (hence with more preemp-
tions) and lesser or equal period (hence with less or the same
available time to finish) can never be the limiting task.
A good strategy to increase the flexibility of the system is
to decrease the interference of higher priority tasks on the
limiting task τ`, i.e., to increase its slack. One source of
complexity here is that the limiting task is not necessarily
the same for different values of Tnew and πnew.
We therefore identify the values of the period Tnew upon
which τ` may change. The set of these values is:
T =
{
t ∈ N+ | ∃τi ∈ S,
⌈
Ti
t
⌉
6=
⌈
Ti
t− 1
⌉}
(7)
This corresponds to the values of Tnew for which the number
of preemptions of τnew on τi, with i ∈ lp(new), changes. It
follows that τ` remains constant in any interval [T
i
new, T
j
new[
where T inew and T
j
new are two consecutive periods in T . It is
thus sufficient to compute the limiting task of S w.r.t. each
priority πnew and period of Tnew ∈ T using Eq. (6).
7. CASE STUDY
Let us now show how the concepts presented in this paper
are applied on an example. Consider a system:
S = {(2, 10, 1), (4, 5, 1), (6, 15, 1), (8, 10, 2), (10, 30, 2)}
Case 1: Priority and period of τnew are known
Suppose that we want to add a task τnew to S, with πnew = 1
and Tnew = 5. By applying Theorem 3 and Definition 6 we
determine that CmaxS = 1 and τ5 is the limiting task of S.
This means that τnew should not have an execution time
larger than 1 otherwise τ5 may miss its deadline.
Case 2: CmaxS as a function of Tnew
Let us now assume that we know nothing about τnew and
we want to quantify the flexibility of S.
Table 1 shows the values of CmaxS obtained for the possi-
ble values of πnew and τnew. Whenever any WCET larger
than 0 would make one task in S unschedulable we put ⊥.
Note that we group periods of τnew for which the number
of preemptions of tasks in S does not change, based on the
computation of T . In addition, if πnew ≥ 11 then τnew will
have no impact on the system so we do not represent Cmaxnew .
Similarly, Table 2 shows Cmaxτnew for possible values of πnew
and Tnew (we leave out periods larger than 15). Finally,
based on Table 1 and Table 2 we easily obtain Table 3 which
shows the possible values of CmaxSnew .
The concepts introduced in this paper allow the system
designer to: (1) understand when an evolution will be con-
strained by the need to preserve the schedulability of the sys-
tem, and when it will be constrained by the need to schedule
23
aaaaaa
Tnew
πnew 1 3 5 7 9
[2, 3[ ⊥ ⊥ ⊥ ⊥ ⊥
[3, 4[ 1 1 1 1 1
[4, 5[ 1 1 1 1 1
[5, 6[ 1 1 1 1 1
[6, 8[ 2 2 2 2 2
[8, 10[ 2 2 2 2 2
[10, 15[ 3 3 3 3 3
[15, 30[ 3 3 4 4 5
[30, +∞[ 3 3 4 4 11
Table 1: CmaxS for different values of πnew and Tnew
aaaaaa
Tnew
πnew 3 5 7 9 11
2 1 ⊥ ⊥ ⊥ ⊥
3 2 1 ⊥ ⊥ ⊥
4 3 2 1 ⊥ ⊥
5 4 3 2 ⊥ ⊥
6 5 3 2 ⊥ ⊥
7 6 4 3 1 ⊥
8 7 5 4 2 ⊥
9 8 6 5 3 1
10 9 7 6 4 2
11 9 6 5 1 ⊥
12 10 7 6 2 ⊥
13 11 8 7 3 1
14 12 9 8 4 2
15 13 10 9 5 3
Table 2: Cmaxτnew for different values of πnew and Tnew
the new task; (2) identify, through the concept of limiting
task, which task in the system will “break” first in case of a
software update.
8. CONCLUSION
In this paper we have defined the flexibility of a system as
its capability to schedule a new task. We also presented an
approach to quantify the flexibility of a system. More impor-
tantly, we show that it is possible under certain conditions
to identify the task that will directly induce the limitations
on a possible software update. If performed at design time,
such a result can be used to adjust the system design by
giving more slack to the limiting task. We illustrate how
these results apply to a simple system.
We plan to extend our work in several main directions.
First, we want to replace our sufficient conditions for by
necessary and sufficient conditions so as to provide more
relevant information to the system designer.
Second, we want to study further the notion of limiting
task and propose a solution to compare the flexibility of
different systems. Our objective is to come up with a set of
guidelines for the designer to help him factor in flexibility at
design time. Such guidelines must be simple to understand
by someone with limited expertise in schedulability analysis,
and they must take into account the fact that many design
choices are fixed.
Finally, we intend to study more complex systems. We
are in particular interested in systems with jitter, sporadic
aaaaaa
Tnew
πnew 1 3 5 7 9 11
2 ⊥ ⊥ ⊥ ⊥ ⊥ ⊥
3 1 1 1 ⊥ ⊥ ⊥
4 1 1 1 1 ⊥ ⊥
5 1 1 1 1 ⊥ ⊥
6 2 2 2 2 ⊥ ⊥
7 2 2 2 2 1 ⊥
8 2 2 2 2 2 ⊥
9 2 2 2 2 2 1
10 3 3 3 3 3 2
11 3 3 3 3 1 ⊥
12 3 3 3 3 2 ⊥
13 3 3 3 3 3 1
14 3 3 3 3 3 2
15 3 3 4 4 5 3
Table 3: CmaxSnew for different values of πnew and Tnew
bursts, task chains, multiple processing resources, etc. When
compared to the general sensitivity analysis problem, the
problem we consider has many fixed parameters. This is
why we expect to be able to provide useful results even for
such complex systems.
9. REFERENCES
[1] I. Bate and P. Emberson. Incorporating scenarios and
heuristics to improve flexibility in real-time embedded
systems. In RTAS’06, pages 221–230, San Jose (CA),
USA, 2006. IEEE.
[2] E. Bini, M. Di Natale, and G. Buttazzo. Sensitivity
analysis for fixed-priority real-time systems. In
ECRTS’06, pages 13–22. IEEE, 2006.
[3] R. Davis and A. Burns. Robust priority assignment for
fixed priority real-time systems. In RTSS’07, pages
3–14. IEEE, 2007.
[4] R. I. Davis, K. Tindell, and A. Burns. Scheduling
slack time in fixed priority pre-emptive systems. In
RTSS’93, pages 222–231, 1993.
[5] F. Dorin, P. Richard, M. Richard, and J. Goossens.
Schedulability and sensitivity analysis of multiple
criticality tasks with fixed-priorities. Real-Time Syst.,
46(3):305–331, 2010.
[6] M. Joseph and P. Pandya. Finding response times in a
real-time system. The Computer Journal,
29(5):390–395, 1986.
[7] S. Punnekkat, R. Davis, and A. Burns. Sensitivity
analysis of real-time task sets. In ASIAN’97, volume
1345 of LNCS, pages 72–82. Springer-Verlag, 1997.
[8] R. Racu, A. Hamann, and R. Ernst. Sensitivity
analysis of complex embedded real-time systems.
Real-Time Syst., 39(1-3):31–72, 2008.
[9] R. Racu, M. Jersak, and R. Ernst. Applying
sensitivity analysis in real-time distributed systems. In
RTAS’05, pages 160–169. IEEE, 2005.
[10] Q. Zhu, Y. Yang, E. Scholte, M. Di Natale, and
A. Sangiovanni-Vincentelli. Optimizing extensibility in
hard real-time distributed systems. In RTAS’09, pages
275–284. IEEE, 2009.
24
Towards schedulability analysis of real-time systems with
precedence constraints and different periods
Slim Ben-Amor
INRIA de Paris
slim.ben-amor@inria.fr
Dorin Maxim
INRIA de Paris
dorin.maxim@inria.fr
Liliana Cucu-Grosjean
INRIA de Paris
liliana.cucu@inria.fr
ABSTRACT
The complexity of modern architectures has increased the timing
variability of programs (or tasks). In this context new approaches
based on probabilistic methods are proposed to decrease the pes-
simism by associating probabilities to the worst case values of the
programs (tasks) time execution. In this paper we present prelimi-
nary work to extend the original work of Chetto et al. [3] on prece-
dence constraints tasks to the case of tasks with probabilistic peri-
ods and arrival pattern.
1. INTRODUCTION
Time critical systems are currently facing an increased complex-
ity of the hardware architectures with a direct impact on the timing
variability of the programs (and tasks). This increased complexity
of the hardware architectures is motivated by numerous new func-
tionalities that others industries than time critical systems require.
Unfortunately the time critical systems do not have any impact on
the design of such hardware architectures and need to adapt their
timing analysis. Mainly worst case reasoning in presence of larger
timing variability is becoming importantly more pessimistic and
this conclusion has motivated the appearance of mixed-criticality
models [12] with different possible values for the execution time
of a program (or task). To our best knowledge the only exist-
ing methods providing such estimation for the execution time of
a program belong to the realm of probabilistic and statistical meth-
ods [1, 6–8]. In this context the concept of probabilistic worst
case execution time (pWCET) of a program (or task) has been pro-
posed [5] as well as associated new schedulability analyses for task
systems with worst case execution times described by probability
distributions [9]. This paper proposes a schedulability analysis of
task systems with probabilistic arrival times and this in presence of
precedence constraints. To our best knowledge there is currently
no solution to this problem.
Related work: Precedence constraints are together with dead-
lines probably the most used real-time constraints. This is moti-
vated by the reactivity that real-time systems should ensure. This
reactivity to some sensors input is obtained by regularly checking
the sensors for new inputs and then producing a result that is sent
to the actuators. The order of the execution is usually imposed by
precedence constraints.
The precedence constraints are defined by (directed) graphs and
the numerous results [3,4,11,13] indicate the importance as well as
the maturity of this topic. The existing results cover schedulability
analyses, scheduling policies in both uniprocessor and multiproces-
sor case as well as shared resources.
2. EXISTING MODEL OF TASKS WITH
PRECEDENCE CONSTRAINTS
In Chetto et al. [3] the authors study the schedulability anal-
ysis of task systems with precedence constraints on one proces-
sor. Let τ be a set of tasks τi defined by a Tuple (ri, Ci, di),
∀i ∈ {1, · · · , n}, where ri is the release time, Ci the execution
time and di the deadline of τ . Let G be a directed acyclic graph
(τ, E) defining a partial order between the tasks of τ . Two tasks
τi and τj (i 6= j) are related by a precedence if and only if (τi, τj)
corrresponds to an edge in E. All tasks have the same period and
they are released only once.
For instance let τ be a task system τ of 4 tasks {τ1, · · · , τ4}with
an associated oriented precedence graphG as described in Figure 1.
τ1
τ2
τ3
τ4
Figure 1: A graph describing the precedence constraints of a
task system
In order to decide the schedulability of these task systems the au-
thors of [3] propose the modification of the releases and the dead-
lines of each task as follows:
r∗i = max(ri, r
∗
j + Cj : τj → τi) (1)
d∗i = min(di, d
∗
j − Cj : τi → τj) (2)
The notation τi → τj in equations (1) and (2) mean that there is
an edge from τi to τj in grapg G. Thus, we can say that τi is an
immediate predecessor of τj and that τj is an immediate successor
of τi.
Equation (1) imposes that the release of a task cannot be done
before the release of all its successors (with respect to the partial
order defined by G). Equation (2) imposes that a task cannot be
scheduled after its successors.
Equation (1) is applied from the sources (the tasks without pre-
decessors) of G to the sinks (the tasks without successors) of G.
Equation (2) is applied from the sinks of G to the sources of G.
Earliest Deadline First (EDF) is the scheduling policy applied
preemptively to the new task systems τ∗ containing only indepen-
dent tasks and Theorem 1 describes a schedulability condition.
25
THEOREM 1. [3] Let τ be a task set. τ is schedulable under
a preemptive uni-processor EDF scheduling if and only if ∀ j ∈
{1, · · · , n} and ∀ i ∈ {1, · · · , n} such that ri ≤ rj , di ≤ dj ,
k=n∑
k=1, ri≤rk, dk≤dj
Ck ≤ dj − ri (3)
Our contribution In [2] we have presented a probabilistic ex-
tension of the results presented in Section 2 by introducing prob-
abilistic worst case execution times. Our previous extension does
not allow different periods and in order to prepare a probabilistic
description of the periods we are first considering the problem with
sporadic arrivals and different minimal inter-arrival times. In the
following section we present a task model supporting such exten-
sion and an associated architecture model.
3. TASK AND ARCHITECTURE MODEL
In this section we present our enriched task model and the archi-
tecture model.
3.1 Functional task model
The task model or the functional model describes functions and
operations that may correspond to inputs, outputs and computing
performed by the system. As previously functional blocs are related
to each other through arrows indicating data dependencies and/or
precedence constraints. Thus the model is somehow similar to a
data flow model. Each function corresponds to a set of operations
to be executed and it has temporal characteristics such as execution
time and minimal inter-arrival time. We can distinguish two types
of functions:
• Aperiodic or acyclic functions: as a general rule these func-
tions are the input to the system and they can represent dif-
ferent operations like various sensors, buttons and external
commands. For this reason the functions release are sporadic
and a minimal inter-arrival time may be derived, for example
as the refresh rate of the sensors (see Figure 2). Furthermore,
the aperiodic functions have execution times directly related
to the time required to acquire data from sensors.
• Cyclic functions: these functions are characterized only by
an execution time and they are activated as soon as data is
available at one of their inputs. They are called cyclic be-
cause they are scheduled periodically thus they are checked
in a cyclic manner for data availability.
Figure 2: Functional model with corresponding operations
Starting from this flow model, end-to-end and latency constraints
are defined on chains of functions. These constraints require that
each function belonging to a chain is executed at least once within
a time duration defined by the latency. More complex constraints
can also be defined, for example imposing that a certain function is
executed some number of time k every t time units.
3.2 Architecture model
Architecture models used in the industry are often different from
the models used in academic research. This fact has several mo-
tivations such as the adaptation of the system (and hence of the
model) to the surrounding environment and different types of con-
straints which differ from one domain to another. The details of
each model needs to be studied in order to be able to propose an
analysis that is adapted to that model. In this section we present a
model of architecture commonly used by both academia and indus-
try. Architectures belonging to this models are used in the avionics
domain for instance to describe a flight management system.
Figure 3: A joint model of architecture and tasks
The architecture describes the system from a physical point of
view. The system can be composed of several processors and each
processor can have one or more partitions. Partitions can them-
selves contain several tasks (see tasks A, B and C in Figure 3).
Partitions are using the processor time in a Round Robin fashion,
each of them can be executed in a strictly defined time window. On
the other hand, tasks are executed inside a partition according to a
given Preemptive Rate Monotonic (PRM) priority policy. We con-
sider in this paper that tasks’priorities are given in reversed order
of their periods (Rate Monotonic policy).
Partitioning the tasks is assigning each task to a partition for all
its instances. Each task may correspond to a set of functions from
the functional model. Even though the execution time of each func-
tion is known in advance not all functions are activated with the
same instance of the task that contains them, leading to variable
execution times of tasks.
3.3 Functioning
For this current contribution we consider the case of one proces-
sor with one partition. For this particular case the tasks are periodic
and there are no interferences between the different partitions. To
each release the task searches for available functions and it exe-
cutes those that are active. This behaviour brings us to the follow-
ing statements:
• A released task can not use any computation resources if
none of its functions are active.
• Once a function is executed, its status moves from active to
26
non-active even if new data has appeared since its last execu-
tion.
• If the activation of a function comes after checking its status
then the function will be executed only for the next release
of the associated task.
• A function may be within an ongoing execution when the
next release of the task containing this function arrives. In
this case we consider a non-drop mode and the function con-
tinues its previous execution once it is possible (when all
higher priority functions are not active).
• The functional model may contain cycles in the sense that
a function may wait for some data to arrive and part of this
data is its own output. Thus, if other data is not available the
function will use its last produced data. Cycles are allowed as
they do not represent data dependencies. Indeed a function
may be activated once data has arrived on one of its inputs
without being blocked if some inputs are missing. Moreover,
in our model cycles don’t overuse the computation resources.
In fact, executions of a function that represent a cycle is not
successive to the point of overloading the processor because
this function is scheduled according to the period of the task
that contains it.
4. ANALYSIS
After the definition of our model we propose a feasibility anal-
ysis to verify the respect of the latency constraints on a functional
flow or chain. Our proposal relies on a response time analysis. We
compute the worst-case response time rf of each function of the
flow separately. Then, the worst-case response time R of the entire
flow L is obtained according to the following equation :
R =
∑
f∈L
rf +
∑
f∈L
Ttask(f) (4)
In fact, we sum up all the worst-case response times rf of func-
tions on the flow and we add the worst-case waiting time for a func-
tion from its activation to the begin of its execution. This duration
is bounded by the period Ttask(f) of the task that contains the anal-
ysed function. Finally, we compare the worst-case response time
of the flow to the latency constraint to decide the system feasibility.
The computation of rf is based upon the classical response time
analysis used for tasks scheduled by a fixed-task priority algorithm
that we adapt to our model. First, we proceed by period inheritance
as described in Algorithm 1. Therefore, We use the inter-arrival
times of the input functions as a period according to the worst-case
analysis. Then, we set the function period Tf to the task period
Ttask(f) that contains it. If the function period is less than the task,
the operation will be released and executed at every task period.
Otherwise, if the task period is less than the function period, ev-
ery two successive executions of the function can be separated by
the greatest multiple of Ttask(f) that remain less than the function
period Tf .
In order to compute the worst-case response time rf of a given
function f , we add to that functions execution time Cf the impact
of preemptions it may suffer. The preemption may be caused by the
execution of a function belonging to a higher priority task or higher
order function in the same task. The response time is initialized to
Cf then it is computed iteratively by the following equation:
rf = rf +
∑
f ′∈hp(f)
Xf ′ × Cf ′ (5)
for f ∈ L do
if (Tf ≤ Ttask(f)) then
Tf = Ttask(f)
end
else
Tf =
⌊
Tf
Ttask(f)
⌋
× Ttask(f)
end
end
Algorithm 1: Algorithmic description of period inheritance
where X is a state vector of 0 and 1 indicating if the function f ′
is active or not. hp(f) is the set of functions that belong to higher
priority task than the task containing f or that belong to the same
task as f but have higher order.
At each iteration, we check if the response time is superior to the
next activation date of one of the higher priority operations. If so,
we take this activation as a preemption and we update the response
time according to equation 5. If not, the response time analysis ends
(the task can not be preempted anymore) and we analyze the next
function of the functional flow. Notice that we stop the computation
if the task response time is already larger than the latency constraint
because the latency constraint is trivially unsatisfied.
If the function is not an input operation and it has no period or
activation date, we save the status of this function in a vector. If
the function is active, we take it into account during the preemp-
tion. This state vector X is updated after each function execution.
Indeed, when a function ends its execution it becomes non-active
while all of its successors become active.
The tasks of the model we study can be considered as tasks of
the non-cyclic generalized multi-frame task model (NC-GMF) pro-
posed by Moyo [10]. That is, a task of our system model has several
possible execution times depending on the different activation com-
binations of its active functions. For the same reason, each instance
of this task is released in a non cyclic manner. Nevertheless, an
analysis based on the NC-GMF model would be very pessimistic
as it does not consider the dependencies between the functional
blocks. These dependencies have an impact on the possible combi-
nations of active functions and consequently, on the execution time
of tasks.
5. SIMULATOR AND RESULTS
In this section we evaluate the performance of the proposed schedu-
lability analysis. To this extent we apply the analysis on a task-set
conforming to the presented model. Also, in order to have a refer-
ence for comparison, we also simulate the task-set on an in-house
simulator that we have developed.
5.1 Simulator
Similar to the majority of simulators that exist we try to imitate
the passage of time through a variable that is incremented continu-
ously. At each iteration we check which active task is at the head of
the queue (i.e. has the highest priority) and we decrease the execu-
tion time of its job by one unit of time. At the same time the state of
the system is updated, e.g. the priority queue is change according
to new arrivals in the system.
As each task is composed of several functions, when a task is
executed the precedence constraints of the functions need to be re-
spected. If a function finishes its execution, we flag it as inactive
and all its successors are activated. Then the next active function of
the same task is executed. If the current highest priority task has no
more active functions we move on to the next highest priority task.
27
Furthermore, when a task or an input function is activated we
also calculate the time instant of its next activation. The next activa-
tion of a task is equal to the time of the current activation plus its pe-
riod, but in the case of an input function we also add a random value
to its current arrival time together with its minimal inter-arrival time
in order to represent the sporadic nature of their arrivals.
In order to keep track of certain information from one iteration
to another we also use several global variables to represent the state
of the system:
• the status ’active’ or ’inactive’ of each function
• the ID of the active task with highest priority
• the order of the executing functions for each task
• the time instant for the next activation of each task
• the time instant for the next activation of each function
Studied example: The system that we analyze contains the func-
tions described by the functional model on Figure 2 and by timing
characteristics on the Table 1. The architecture model is describe
in the Figure 4.
Operation MITi Ci
Func_A1 100 10
Func_A2 150 15
Func_C1 - 25
Func_C2 - 10
Func_C3 - 10
Table 1: Inter-arrival times and worst case execution times of
the studied example functions
Figure 4: Architecture model of studied example
5.2 First numerical results
While working on a proof for our analysis and in order to eval-
uate its pessimism we consider the task set given in the previous
example. We estimate the latency values with our analysis for two
given chains of functions and compare against the maximum ob-
served values provided by our simulator. The calculated values are
always larger than the observed ones. Nevertheless our analysis is
up to 37% larger than the observed values. This over-estimation
may be decreased by introducing probabilistic description since
large execution times values for instance are rare events.
6. CONCLUSION
In this paper we have presented partial results on the schedulabil-
ity analysis of precedence constraints tasks in presence of sporadic
arrivals of data. Our tasks are partitioned on a processor and their
executions is using a fixed-priority policy. The analysis extends the
classical fixed-point reasoning for response time calculation and
we compare its pessimism with respect to observed response time.
This work is the first step towards for the proposing of a proba-
bilistic model with different periods. As future work we plan to
extend the analysis by introducing the probabilistic description of
the timing parameters like execution times and periods.
7. REFERENCES
[1] S. Altmeyer, L Cucu-Grosjean, and R. Davis. Static
probabilistic timing analysis for real-time systems using
random replacement caches. Real-Time Systems,
51(1):77–123, 2015.
[2] Slim Ben-Amor, Dorin Maxim, and Liliana Cucu-Grosjean.
Schedulability analysis of dependent probabilistic real-time
tasks. In the 24th International Conference on Real-Time and
Networked Systems (RTNS), 2016.
[3] Houssine Chetto, Maryline Silly, and T. Bouchentouf.
Dynamic scheduling of real-time tasks under precedence
constraints. Real-Time Systems, 2(3):181–194, 1990.
[4] L. Cucu, R. Kocik, and Y. Sorel. Real-time scheduling for
systems with precedence, periodicity and latency constraints.
In Real-time and Embedded Systems, 2002.
[5] L Cucu-Grosjean. Independence - a misunderstood property
of and for (probabilistic) real-time systems. In "Real-Time
Systems: the past, the present, and the future", the 6Oth
birthday of Professor Alan Burns, 2013.
[6] L. Cucu-Grosjean, L. Santinelli, M. Houston, C. Lo,
T. Vardanega, L. Kosmidis, J. Abella, E. Mezzeti,
E. Quinones, and F.J. Cazorla. Measurement-based
probabilistic timing analysis for multi-path programs. In the
24th Euromicro Conference on Real-time Systems, 2012.
[7] R. Davis, L. Santinelli, S. Altmeyer, C. Maiza, and
L. Cucu-Grosjean. Analysis of probabilistic cache related
pre-emption delays. In 25th Euromicro Conference on
Real-Time Systems, 2013.
[8] S. Edgar and A. Burns. Statistical analysis of WCET for
scheduling. In the 22nd IEEE Real-Time Systems
Symposium, 2001.
[9] D. Maxim and L. Cucu-Grosjean. Response time analysis for
fixed-priority tasks with multiple probabilistic parameters. In
the IEEE Real-Time Systems Symposium, 2013.
[10] Noel Tchidjo Moyo, Eric Nicollet, Frederic Lafaye, and
Christophe Moy. On schedulability analysis of non-cyclic
generalized multiframe tasks. In the 22nd Euromicro
Conference on Real-Time Systems (ECRTS), pages 271–278,
2010.
[11] Martin Stigge and Wang Yi. Graph-based models for
real-time workload: a survey. Real-Time Systems,
51(5):602–636, 2015.
[12] S. Vestal. Preemptive scheduling of multi-criticality systems
with varying degrees of execution time assurance. In the
IEEE Real-Time Systems Symposium, 2007.
[13] Jia Xu and David Lorge Parnas. Scheduling processes with
release times, deadlines, precedence, and exclusion relations.
IEEE Trans. Software Eng., 16(3):360–369, 1990.
28



