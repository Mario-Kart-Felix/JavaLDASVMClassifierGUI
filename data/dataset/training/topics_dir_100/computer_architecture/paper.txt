A New Direction for Computer Architecture Research
Christoforos E. Kozyrakis David A. Patterson
Computer Science Division
University of California at Berkeley
Berkeley, CA 94720
Abstract
In this paper we suggest a different computing en-
vironment as a worthy new direction for computer ar-
chitecture research: personal mobile computing, where
portable devices are used for visual computing and per-
sonal communications tasks. Such a device supports
in an integrated fashion all the functions provided to-
day by a portable computer, a cellular phone, a digital
camera and a video game. The requirements placed on
the processor in this environment are energy efficiency,
high performance for multimedia and DSP functions,
and area efficient, scalable designs.
We examine the architectures that were recently pro-
posed for billion transistor microprocessors. While
they are very promising for the stationary desktop and
server workloads, we discover that most of them are un-
able to meet the challenges of the new environment and
provide the necessary enhancements for multimedia ap-
plications running on portable devices.
We conclude with Vector IRAM, an initial example of
a microprocessor architecture and implementation that
matches the new environment.
1 Introduction
Advances in integrated circuits technology will soon
provide the capability to integrate one billion transistors
in a single chip [1]. This exciting opportunity presents
computer architects and designers with the challeng-
ing problem of proposing microprocessor organizations
able to utilize this huge transistor budget efficiently and
meet the requirements of future applications. To ad-
dress this challenge, IEEE Computer magazine hosted
The authors can be contacted through email at
fkozyraki,pattersong@cs.berkeley.edu.
a special issue on “Billion Transistor Architectures” [2]
in September 1997. The first three articles of the is-
sue discussed problems and trends that will affect future
processor design, while seven articles from academic
research groups proposed microprocessor architectures
and implementations for billion transistor chips. These
proposals covered a wide architecture space, ranging
from out-of-order designs to reconfigurable systems. In
addition to the academic proposals, Intel and Hewlett-
Packard presented the basic characteristics of their next
generation IA-64 architecture [3], which is expected
to dominate the high-performance processor market
within a few years.
It is no surprise that the focus of these proposals is
the computing domain that has shaped processor archi-
tecture for the past decade: the uniprocessor desktop
running technical and scientific applications, and the
multiprocessor server used for transaction processing
and file-system workloads. We start with a review of
these proposals and a qualitative evaluation of them for
the concerns of this classic computing environment.
In the second part of the paper we introduce a new
computing domain that we expect to play a significant
role in driving technology in the next millennium: per-
sonal mobile computing. In this paradigm, the basic
personal computing and communication devices will be
portable and battery operated, will support multimedia
functions like speech recognition and video, and will be
sporadically interconnected through a wireless infras-
tructure. A different set of requirements for the micro-
processor, like real-time response, DSP support and en-
ergy efficiency, arise in such an environment. We exam-
ine the proposed organizations with respect to this en-
vironment and discover that limited support for its re-
quirements is present in most of them.
Finally we present Vector IRAM, a first effort for
1
Architecture Source Key Idea Transistors
used for Memory
Advanced [4] wide-issue superscalar processor with speculative
Superscalar execution and multilevel on-chip caches 910M
Superspeculative [5] wide-issue superscalar processor with aggressive data
Architecture and control speculation and multilevel on-chip caches 820M
Trace Processor [6] multiple distinct cores, that speculatively execute
program traces, with multilevel on-chip caches 600M 1
Simultaneous [7] wide superscalar with support for aggressive sharing
Multithreaded (SMT) among multiple threads and multilevel on-chip caches 810M
Chip Multiprocessor [8] symmetric multiprocessor system with shared
(CMP) second level cache 450M 1
IA-64 [3] VLIW architecture with support for predicated
execution and long instruction bundling 600M 1
RAW [9] multiple processing tiles with reconfigurable logic and
memory, interconnected through a reconfigurable network 670M
Table 1: The billion transistor microprocessors and the number of transistors used for memory cells for each one1.
We assume a billion transistor implementation for the Trace and IA-64 architecture.
a microprocessor architecture and design that matches
the requirements of the new environment. Vector
IRAM combines a vector processing architecture with
merged logic-DRAM technology in order to provide a
scalable, cost efficient design for portable multimedia
devices.
This paper reflects the opinion and expectations of
its authors. We believe that in order to design suc-
cessful processor architectures for the future, we first
need to explore the future applications of computing
and then try to match their requirements in a scalable,
cost-efficient way. The goal of this paper is to point out
the potential change in applications and motivate archi-
tecture research in this direction.
2 Overview of the Billion Transistor
Processors
Table 1 summarizes the basic features of the billion
transistor implementations for the proposed architec-
tures as presented in the corresponding references. For
the case of the Trace Processor and IA-64, descriptions
of billion transistor implementations have not been pre-
sented, hence certain features are speculated.
1These numbers include transistors for main memory, caches
and tags. They are calculated based on information from the ref-
erenced papers. Note that CMP uses considerably less than one bil-
The first two architectures (Advanced Superscalar
and Superspeculative Architecture) have very similar
characteristics. The basic idea is a wide superscalar or-
ganization with multiple execution units or functional
cores, that uses multi-level caching and aggressive pre-
diction of data, control and even sequences of instruc-
tions (traces) to utilize all the available instruction level
parallelism (ILP). Due their similarity, we group them
together and call them “Wide Superscalar” processors
in the rest of this paper.
The Trace processor consists of multiple superscalar
processing cores, each one executing a trace issued by a
shared instruction issue unit. It also employs trace and
data prediction and shared caches.
The Simultaneous Multithreaded (SMT) processor
uses multithreading at the granularity of issue slot to
maximize the utilization of a wide-issue out-of-order
superscalar processor at the cost of additional complex-
ity in the issue and control logic.
The Chip Multiprocessor (CMP) uses the transistor
budget by placing a symmetric multiprocessor on a sin-
gle die. There will be eight uniprocessors on the chip,
all similar to current out-of-order processors, which
will have separate first level caches but will share a
lion transistors, so 450M transistors is much more than half the bud-
get. The numbers for the Trace processor and IA-64 were based on
lower-limit expectations and the fact that their predecessors spent at
least half their transistor budget on caches.
2
large second level cache and the main memory inter-
face.
The IA-64 can be considered as the commercial rein-
carnation of the VLIW architecture, renamed “Explic-
itly Parallel Instruction Computer”. Its major innova-
tions announced so far are support for bundling multi-
ple long instructions and the instruction dependence in-
formation attached to each one of them, which attack
the problem of scaling and code density of older VLIW
machines. It also includes hardware checks for hazards
and interlocks so that binary compatibility can be main-
tained across generations of chips. Finally, it supports
predicated execution through general-purpose predica-
tion registers to reduce control hazards.
The RAW machine is probably the most revolution-
ary architecture proposed, supporting the case of re-
configurable logic for general-purpose computing. The
processor consists of 128 tiles, each with a process-
ing core, small first level caches backed by a larger
amount of dynamic memory (128 KBytes) used as main
memory, and a reconfigurable functional unit. The tiles
are interconnected with a reconfigurable network in an
matrix fashion. The emphasis is placed on the soft-
ware infrastructure, compiler and dynamic-event sup-
port, which handles the partitioning and mapping of
programs on the tiles, as well as the configuration se-
lection, data routing and scheduling.
Table 1 also reports the number of transistors used
for caches and main memory in each billion transistor
processors. This varies from almost half the budget to
90% of it. It is interesting to notice that all but one do
not use that budget as part of the main system mem-
ory: 50% to 90% of their transistor budget is spent to
build caches in order to tolerate the high latency and low
bandwidth problem of external memory.
In other words, the conventional vision of comput-
ers of the future is to spend most of the billion transis-
tor budget on redundant, local copies of data normally
found elsewhere in the system. Is such redundancy re-
ally our best idea for the use of 500,000,000 transistors2
for applications of the future?
2While die area is not a linear function of the transistor num-
ber (memory transistors can be placed much more densely than
logic transistors and redundancy enables repair of failed rows or
columns), die cost is non-linear function of die area [10]. Thus,
these 500M transistors are very expensive.
3 The Desktop/Server Computing
Domain
Current processors and computer systems are being
optimized for the desktop and server domain, with
SPEC’95 and TPC-C/D being the most popular bench-
marks. This computing domain will likely be signifi-
cant when the billion transistor chips will be available
and similar benchmark suites will be in use. We play-
fully call them “SPEC’04” for technical/scientific ap-
plications and “TPC-F” for on-line transaction process-
ing (OLTP) workloads.
Table 2 presents our prediction of the performance
of these processors for this domain using a grading sys-
tem of ”+” for strength, ”” for neutrality, and ”-” for
weakness.
For the desktop environment, the Wide Superscalar,
Trace and Simultaneous Multithreading processors are
expected to deliver the highest performance on integer
SPEC’04, since out-of-order and advanced prediction
techniques can utilize most of the available ILP of a
single sequential program. IA-64 will perform slightly
worse because VLIW compilers are not mature enough
to outperform the most advanced hardware ILP tech-
niques, which exploit run-time information. CMP and
RAW will have inferior performance since desktop ap-
plications have not been shown to be highly paralleliz-
able. CMP will still benefit from the out-of-order fea-
tures of its cores. For floating point applications on
the other hand, parallelism and high memory bandwidth
are more important than out-of-order execution, hence
SMT and CMP will have some additional advantage.
For the server domain, CMP and SMT will pro-
vide the best performance, due to their ability to utilize
coarse-grain parallelism even with a single chip. Wide
Superscalar, Trace processor or IA-64 systems will per-
form worse, since current evidence is that out-of-order
execution provides little benefit to database-like appli-
cations [11]. With the RAW architecture it is difficult
to predict any potential success of its software to map
the parallelism of databases on reconfigurable logic and
software controlled caches.
For any new architecture to be widely accepted, it
has to be able to run a significant body of software [10].
Thus, the effort needed to port existing software or de-
velop new software is very important. The Wide Super-
scalar and Trace processors have the edge, since they
can run existing executables. The same holds for SMT
and CMP but, in this case, high performance can be de-
3
Wide Trace Simultaneous Chip IA-64 RAW
Superscalar Processor Multithreading Multiprocessor
SPEC’04 Int (Desktop) + + +  + 
SPEC’04 FP (Desktop) + + + + + 
TPC-F (Server)   + +  -
Software Effort + +    -
Physical Design
Complexity -  -   +
Table 2: The evaluation of the billion transistor processors for the desktop/server domain. Wide Superscalar proces-
sors includes the Advanced Superscalar and Superspeculative processors.
livered if the applications are written in a multithreaded
or parallel fashion. As the past decade has taught us,
parallel programming for high performance is neither
easy nor automated. For IA-64 a significant amount
of work is required to enhance VLIW compilers. The
RAW machine relies on the most challenging software
development. Apart from the requirements of sophisti-
cated routing, mapping and run-time scheduling tools,
there is a need for development of compilers or libraries
to make such an design usable.
A last issue is that of physical design complex-
ity which includes the effort for design, verification
and testing. Currently, the whole development of an
advanced microprocessor takes almost 4 years and a
few hundred engineers [2][12][13]. Functional and
electrical verification and testing complexity has been
steadily growing [14][15] and accounts for the major-
ity of the processor development effort. The Wide
Superscalar and Multithreading processors exacerbate
both problems by using complex techniques like ag-
gressive data/control prediction, out-of-order execution
and multithreading, and by having non modular designs
(multiple blocks individuallydesigned). The Chip Mul-
tiprocessor carries on the complexity of current out-
of-order designs with support for cache coherency and
multiprocessor communication. With the IA-64 archi-
tecture, the basic challenge is the design and verifica-
tion of the forwarding logic between the multiple func-
tional units on the chip. The Trace processor and RAW
machine are more modular designs. The trace pro-
cessor employs replication of processing elements to
reduce complexity. Still, trace prediction and issue,
which involves intra-trace dependence check and reg-
ister remapping, as well as intra-element forwarding in-
cludes a significant portion of the complexity of a wide
superscalar design. For the RAW processor, only a sin-
gle tile and network switch need to be designed and
replicated. Verification of a reconfigurable organization
is trivial in terms of the circuits, but verification of the
mapping software is also required.
The conclusion from Table 2 is that the proposed bil-
lion transistor processors have been optimized for such
a computing environment and most of them promise
impressive performance. The only concern for the fu-
ture is the design complexity of these organizations.
4 A New Target for Future Comput-
ers: Personal Mobile Computing
In the last few years, we have experienced a significant
change in technology drivers. While high-end systems
alone used to direct the evolution of computing, current
technology is mostly driven by the low-end systems due
to their large volume. Within this environment, two im-
portant trends have evolved that could change the shape
of computing.
The first new trend is that of multimedia applica-
tions. The recent improvements in circuits technol-
ogy and innovations in software development have en-
abled the use of real-time media data-types like video,
speech, animation and music. These dynamic data-
types greatly improve the usability, quality, productiv-
ity and enjoyment of personal computers [16]. Func-
tions like 3D graphics, video and visual imaging are al-
ready included in the most popular applications and it
is common knowledge that their influence on comput-
ing will only increase:
 “90% of desktop cycles will be spent on ‘media’
applications by 2000” [17]
 “multimedia workloads will continue to increase
in importance” [2]
4
Figure 1: Personal mobile devices of the future will
integrate the functions of current portable devices
like PDAs, video games, digital cameras and cellular
phones.
 “many users would like outstanding 3D graphics
and multimedia” [12]
 “image, handwriting, and speech recognition will
be other major challenges” [15]
At the same time, portable computing and commu-
nication devices have gained large popularity. Inex-
pensive “gadgets”, small enough to fit in a pocket, like
personal digital assistants (PDA), palmtop computers,
webphones and digital cameras were added to the list
of portable devices like notebook computers, cellular
phones, pagers and video games [18]. The functions
supported by such devices are constantly expanded and
multiple devices are converging into a single one. This
leads to a natural increase in their demand for comput-
ing power, but at the same time their size, weight and
power consumption have to remain constant. For ex-
ample, a typical PDA is 5 to 8 inches by 3.2 inches big,
weighs six to twelve ounces, has 2 to 8 MBytes of mem-
ory (ROM/RAM) and is expected to run on the same set
of batteries for a period of a few days to a few weeks
[18]. One should also notice the large software, operat-
ing system and networking infrastructure developed for
such devices (wireless modems, infra-red communica-
tions etc): Windows CE and the PalmPilot development
environment are prime examples [18].
Our expectation is that these two trends together will
lead to a new application domain and market in the
near future. In this environment, there will be a sin-
gle personal computation and communication device,
small enough to carry around all the time. This device
will include the functions of a pager, a cellular phone,
a laptop computer, a PDA, a digital camera and a video
game combined [19][20] (Figure 1) . The most impor-
tant feature of such a device will be the interface and in-
teraction with the user: voice and image input and out-
put (speech and voice recognition) will be key functions
used to type notes, scan documents and check the sur-
rounding for specific objects [20]. A wireless infras-
tructure for sporadic connectivity will be used for ser-
vices like networking (www and email), telephony and
global positioning system (GPS), while the device will
be fully functional even in the absence of network con-
nectivity.
Potentially this device will be all that a person may
need to perform tasks ranging from keeping notes to
making an on-line presentation, and from browsing the
web to programming a VCR. The numerous uses of
such devices and the potential large volume [20] lead us
to expect that this computing domain will soon become
at least as significant as desktop computing is today.
The microprocessor needed for these computing de-
vices is actually a merged general-purpose processor
and digital-signal processor (DSP), at the power budget
of the latter. There are four major requirements: high
performance for multimedia functions, energy/power
efficiency, small size and low design complexity.
The basic characteristics of media-centric applica-
tions that a processor needs to support or utilize in or-
der to provide high-performance were specified in [16]
in the same issue of IEEE Computer:
 real-time response: instead of maximum peak per-
formance, sufficient worst case guaranteed perfor-
mance is needed for real-time qualitative percep-
tion for applications like video.
 continuous-media data types: media functions are
typically processing a continuous stream of input
that is discarded once it is too old, and continu-
ously send results to a display or speaker. Hence,
temporal locality in data memory accesses, the as-
sumption behind 15 years of innovation in con-
ventional memory systems, no longer holds. Re-
markably, data caches may well be an obstacle
to high performance for continuous-media data
types. This data is also narrow, as pixel images
and sound samples are 8 to 16 bits wide, rather
than the 32-bit or 64-bit data of desktop machines.
The ability to perform multiple operations on such
types on a single wide datapath is desirable.
5
Wide Trace Simultaneous Chip Multi IA-64 RAW
Superscalar Processor Multithreading Processor
Real-time - -    
response unpredictability of out-of-order, branch prediction and/or caching techniques
Continuous      
Data-types caches do not efficiently support data streams with little locality
Fine-grained      +
Parallelism MMX-like extensions less efficient than full vector support reconfigurable
logic unit
Coarse-grained   + +  +
Parallelism
Code size     - 
potential use of loop unrolling and software VLIW hardware
pipelining for higher ILP instr. configuration
Memory      
Bandwidth cache-based designs
Energy/ - - -   -
power power penalty for out-of-order schemes, complex issue logic, forwarding
Efficiency and reconfigurable logic
Physical Design -  -   +
Complexity
Design -  -   
Scalability long wires for forwarding data or for reconfigurable interconnect
Table 3: The evaluation of the billion transistor processors for the personal mobile computing domain.
 fine-grained parallelism: in functions like image,
voice and signal processing, the same operation is
performed across sequences of data in a vector or
SIMD fashion.
 coarse-grained parallelism: in many media appli-
cations a single stream of data is processed by a
pipeline of functions to produce the end result.
 high instruction-reference locality: media func-
tions usually have small kernels or loops that dom-
inate the processing time and demonstrate high
temporal and spatial locality for instructions.
 high memory bandwidth: applications like 3D
graphics require huge memory bandwidthfor large
data sets that have limited locality.
 high network bandwidth: streaming data like
video or images from external sources requires
high network and I/O bandwidth.
With a budget of less than two Watts for the whole
device, the processor has to be designed with a power
target less than one Watt, while still being able to pro-
vide high-performance for functions like speech recog-
nition. Power budgets close to those of current high-
performance microprocessors (tens of Watts) are unac-
ceptable.
After energy efficiency and multimedia support, the
third main requirement for personal mobile comput-
ers is small size and weight. The desktop assumption
of several chips for external cache and many more for
main memory is infeasible for PDAs, and integrated so-
lutions that reduce chip count are highly desirable. A
related matter is code size, as PDAs will have limited
memory to keep down costs and size, so the size of pro-
gram representations is important.
A final concern is design complexity, like in the
desktop domain, and scalability. An architecture
should scale efficiently not only in terms of perfor-
mance but also in terms of physical design. Long inter-
connects for on-chip communication are expected to be
a limiting factor for future processors as a small region
of the chip (around 15%) will be accessible in a single
clock cycle [21] and therefore should be avoided.
6
5 Processor Evaluation for Mobile
Multimedia Applications
Table 3 summarizes our evaluation of the billion tran-
sistor architectures with respect to personal mobile
computing.
The support for multimedia applications is limited in
most architectures. Out-of-order techniques and caches
make the delivered performance quite unpredictable
for guaranteed real-time response, while hardware con-
trolled caches also complicate support for continuous-
media data-types. Fine-grained parallelism is exploited
by using MMX-like or reconfigurable execution units.
Still, MMX-like extensions expose data alignment is-
sues to the software and restrict the number of vector or
SIMD elements operations per instruction, limiting this
way their usability and scalability. Coarse-grained par-
allelism, on the other hand, is best on the Simultaneous
Multithreading, Chip Multiprocessor and RAW archi-
tectures.
Instruction reference locality has traditionally been
exploited through large instruction caches. Yet, design-
ers of portable system would prefer reductions in code
size as suggested by the 16-bit instruction versions of
MIPS and ARM [22]. Code size is a weakness for IA-
64 and any other architecture that relies heavily on loop
unrolling for performance, as it will surely be larger
than that of 32-bit RISC machines. RAW may also
have code size problems, as one must “program” the
reconfigurable portion of each datapath. The code size
penalty of the other designs will likely depend on how
much they exploit loop unrolling and in-line procedures
to expose enough parallelism for high performance.
Memory bandwidth is another limited resource for
cache-based architectures, especially in the presence
of multiple data sequences, with little locality, being
streamed through the system. The potential use of
streaming buffers and cache bypassing would help for
sequential bandwidth but would still not address that of
scattered or random accesses. In addition, it would be
embarrassing to rely on cache bypassing when 50% to
90% of the transistors are dedicated to caches!
The energy/power efficiency issue, despite its im-
portance both for portable and desktop domains [23], is
not addressed in most designs. Redundant computation
for out-of-order models, complex issue and dependence
analysis logic, fetching a large number of instructions
for a single loop, forwarding across long wires and use
of the typically power hungry reconfigurable logic in-
crease the energy consumption of a single task and the
power of the processor.
As for physical design scalability, forwarding re-
sults across large chips or communication among mul-
tiple core or tiles is the main problem of most de-
signs. Such communication already requires multiple
cycles in high-performance out-of-order designs. Sim-
ple pipelining of long interconnects is not a sufficient
solution as it exposes the timing of forwarding or com-
munication to the scheduling logic or software and in-
creases complexity.
The conclusion from Table 3 is that the proposed
processors fail to meet many of the requirements of the
new computing model. This indicates the need for mod-
ifications of the architectures and designs or the pro-
posal of different approaches.
6 Vector IRAM
Vector IRAM (VIRAM) [24], the architecture proposed
by the research group of the authors, is a first effort
for a processor architecture and design that matches
the requirements of the mobile personal environment.
VIRAM is based on two main ideas, vector processing
and the integration of logic and DRAM on a single chip.
The former addresses many of the demands of multime-
dia processing, and the latter addresses the energy effi-
ciency, size, and weight demands of PDAs. We do not
believe that VIRAM is the last word on computer ar-
chitecture research for mobile multimedia applications,
but we hope it proves to be an promising first step.
The VIRAM processor described in the IEEE special
issue consists of an in-order dual-issue superscalar pro-
cessor with first level caches, tightly integrated with a
vector execution unit with multiple pipelines (8). Each
pipeline can support parallel operations on multiple me-
dia types, DSP functions like multiply- accumulate and
saturated logic. The memory system consists of 96
MBytes of DRAM used as main memory. It is orga-
nized in a hierarchical fashion with 16 banks and 8 sub-
banks per bank, connected to the scalar and vector unit
through a crossbar. This provides sufficient sequen-
tial and random bandwidth even for demanding appli-
cations. External I/O is brought directly to the on-chip
memory through high-speed serial lines operating at the
range of Gbit/s instead of parallel buses. From a pro-
gramming point of view, VIRAM can be seen as a vec-
tor or SIMD microprocessor.
7
Desktop/Server Computing Personal Mobile Computing
SPEC’04 Int (Desktop) - Real-time Response +
SPEC’04 FP (Desktop) + Continuous Data-types +
TPC-F (Server)  Fine-grained Parallelism +
Software Effort  Coarse-grained Parallelism 
Physical Design Complexity  Code Size +
Memory Bandwidth +
Energy Efficiency +
Design Scalability 
Table 4: The evaluation of VIRAM for the two computing environments. The grades presented are the medians of
those assigned by reviewers.
Table 4 presents the grades for VIRAM for the
two computing environments. We present the median
grades given by reviewers of this paper, including the
architects of some of the other billion transistor archi-
tectures.
Obviously, VIRAM is not competitive within the
desktop/server domain; indeed, this weakness for con-
ventional computing is probably the main reason some
are skeptical of the importance of merged logic-DRAM
technology [25]. For the case of integer SPEC’04 no
benefit can be expected from vector processing for inte-
ger applications. Floating point intensive applications,
on the other hand, have been shown to be highly vector-
izable. All applications will still benefit from the low
memory latency and high memory bandwidth. For the
server domain, VIRAM is expected to perform poorly
due to limited on-chip memory3. A potentially differ-
ent evaluation for the server domain could arise if we
examine decision support (DSS) instead of OLTP work-
loads. In this case, small code loops with highly data
parallel operations dominate execution time [26], so ar-
chitectures like VIRAM and RAW should perform sig-
nificantly better than for OLTP workloads.
In terms of software effort, vectorizing compilers
have been developed and used in commercial environ-
ments for years now. Additional work is required to
tune such compilers for multimedia workloads.
As for design complexity, VIRAM is a highly mod-
ular design. The necessary building blocks are the in-
order scalar core, the vector pipeline, which is repli-
cated 8 times, and the basic memory array tile. Due to
3While the use of VIRAM as the main CPU is not attractive for
servers, a more radical approach to servers of the future places a
VIRAM in eachSIMM module [27] or each disk [28] and have them
communicate over high speed serial lines via crossbar switches.
the lack of dependencies and forwarding in the vector
model and the in-order paradigm, the verification effort
is expected to be low.
The open question in this case is the complications
of merging high-speed logic with DRAM to cost, yield
and testing. Many DRAM companies are investing in
merged logic-DRAM fabrication lines and many com-
panies are exploring products in this area. Also, our
project is submitting a test chip this summer with sev-
eral key circuits of VIRAM in a merged logic-DRAM
process. We expect the answer to this open question to
be clearer in the next year. Unlike the other proposals,
the challenge for VIRAM is the implementation tech-
nology and not the microarchitectural design.
As mentioned above, VIRAM is a good match to the
personal mobile computing model. The design is in-
order and does not rely on caches, making the deliv-
ered performance highly predictable. The vector model
is superior to MMX-like solutions, as it provides ex-
plicit support of the length of SIMD instructions, and it
does not expose data packing and alignment to software
and is scalable. Since most media processing functions
are based on algorithms working on vectors of pixels or
samples, its not surprising that highest performance can
be delivered by a vector unit. Code size is small com-
pared to other architectures as whole loops can specified
in a single vector instruction. Memory bandwidth, both
sequential and random is available from the on-chip hi-
erarchical DRAM.
VIRAM is expected to have high energy efficiency
as well. In the vector model there are no dependen-
cies, so the limited forwarding within each pipeline is
needed for chaining, and vector machines do not re-
quire chaining to occur within a single clock cycle. Per-
formance comes from multiple vector pipelines work-
8
ing in parallel on the same vector operation as well as
from high-frequency operation, allowing the same per-
formance at lower clock rate and thus lower voltage
as long as the functional units are expanded. As en-
ergy goes up with the square of the voltage in CMOS
logic, such tradeoffs can dramatically improve energy
efficiency. In addition, the execution model is strictly
in order. Hence, the logic can be kept simple and power
efficient. DRAM has been traditionally optimized for
low-power and the hierarchical structure provides the
ability to activate just the sub-banks containing the nec-
essary data.
As for physical design scalability, the processor-
memory crossbar is the only place were long wires are
used. Still, the vector model can tolerate latency if suffi-
cient fine-grain parallelism is available, so deep pipelin-
ing is a viable solution without any hardware or soft-
ware complications in this environment.
7 Conclusions
For almost two decades architecture research has been
focussed on desktop or server machines. As a result of
that attention, today’s microprocessors are 1000 times
faster. Nevertheless, we are designing processors of the
future with a heavy bias for the past. For example, the
programs in the SPEC’95 suite were originally written
many years ago, yet these were the main drivers for
most papers in the special issue on billion transistor pro-
cessors for 2010. A major point of this article is that
we believe it is time for some of us in this very success-
ful community to investigate architectures with a heavy
bias for the future.
The historic concentration of processor research on
stationary computing environments has been matched
by a consolidation of the processor industry. Within a
few years, this class of machines will likely be based
on microprocessors using a single architecture from a
single company. Perhaps it is time for some of us to de-
clare victory, and explore future computer applications
as well as future architectures.
In the last few years, the major use of computing
devices has shifted to non-engineering areas. Personal
computing is already the mainstream market, portable
devices for computation, communication and entertain-
ment have become popular, and multimedia functions
drive the application market. We expect that the combi-
nation of these will lead to the personal mobile comput-
ing domain, where portability, energy efficiency and ef-
ficient interfaces through the use of media types (voice
and images) will be the key features.
One advantage of this new target for the architec-
ture community is its unquestionable need for improve-
ments in terms of ”MIPS/Watt”, for either more de-
manding applications like speech input or much longer
battery life are desired for PDAs. Its less clear that
desktop computers really need orders of magnitude
more performance to run “MS-Office 2010”.
The question we asked is whether the proposed new
architectures can meet the challenges of this new com-
puting domain. Unfortunately, the answer is negative
for most of them, at least in the form they were pre-
sented. Limited and mostly “ad-hoc” support for multi-
media or DSP functions is provided, power is not a se-
rious issue and unlimited complexity of design and ver-
ification is justified by even slightly higher peak perfor-
mance.
Providing the necessary support for personal mobile
computing requires a significant shift in the way we de-
sign processors. The key requirements that processor
designers will have to address will be energy efficiency
to allow battery operated devices, focus on worst case
performance instead of peak for real-time applications,
multimedia and DSP support to enable visual comput-
ing, and simple scalable designs with reduced develop-
ment and verification cycles. New benchmarks suites,
representative of the new types of workloads and re-
quirements are also necessary.
We believe that personal mobile computing offers a
vision of the future with a much richer and more excit-
ing set of architecture research challenges than extrap-
olations of the current desktop architectures and bench-
marks. VIRAM is a first approach in this direction.
Put another way, which problem would you rather
work on: improving performance of PCs running
FPPPP or making speech input practical for PDAs?
8 Acknowledgments
The ideas and opinions presented in this paper are
the result of discussions within the IRAM group in
U.C. Berkeley.
In addition, we want to thank the following peo-
ple for their useful feedback, comments and criticism
on earlier drafts, as well as the grades for VIRAM:
Anant Agarwal, Jean-Loup Baer, Gordon Bell, Pradeep
9
Dubey, Lance Hammond, Wang Wen-Hann, John Hen-
nessy, Mark Hill, John Kubiatowicz, Corinna Lee,
Henry Levy, Doug Matzke, Kunle Olukotun, Jim Smith
and Gurindar Sohi.
This research is supported by DARPA (DABT63-
C-0056), the California State MICRO program, NSF
(CDA-9401156) and by research grants from LG Semi-
con, Hitachi, Intel, Microsoft, SGI/Cray, Sun Microsys-
tems and Texas Instruments.
References
[1] Semiconductor Industry Association. The Na-
tional Technology Roadmap for Semiconductors.
SEMATECH Inc., 1997.
[2] D. Burger and D. Goodman. Billion-Transistor
Architectures - Guest Editors’ Introduction. IEEE
Computer, 30(9):46–48, September 1997.
[3] J. Crawford and J. Huck. Motivations and Design
Approach for the IA-64 64-Bit Instruction Set Ar-
chitecture. In the Proceedings of the Microproces-
sor Forum, October 1997.
[4] Y.N. Patt, S.J. Patel, M. Evers, D.H. Friendly, and
J. Stark. One Billion Transistors, One Unipro-
cessor, One Chip. IEEE Computer, 30(9):51–57,
September 1997.
[5] M. Lipasti and L.P. Shen. Superspeculative Mi-
croarchitecture for Beyond AD 2000. IEEE Com-
puter, 30(9):59–66, September 1997.
[6] J. Smith and S. Vajapeyam. Trace Processors:
Moving to Fourth Generation Microarchitectures.
IEEE Computer, 30(9):68–74, September 1997.
[7] S.J. Eggers, J.S. Emer, H.M. Leby, J.L. Lo, R.L.
Stamm, and D.M. Tullsen. Simultaneous Mul-
tithreading: a Platform for Next-Generation Pro-
cessors. IEEE MICRO, 17(5):12–19, October
1997.
[8] L. Hammond, B.A. Nayfeh, and K. Olukotun. A
Single-Chip Multiprocessor. IEEE Computer,
30(9):79–85, September 1997.
[9] E. Waingold, M. Taylor, D. Srikrishna, V. Sarkar,
W. Lee, V. Lee, J. Kim, M. Frank, P. Finch,
R. Barua, J. Babb, S. Amarasinghe, and A. Agar-
wal. Baring It All to Software: Raw Machines.
IEEE Computer, 30(9):86–93, September 1997.
[10] J Hennessy and D. Patterson. Computer Archi-
tecture: A Quantitative Approach, second edition.
Morgan Kaufmann, 1996.
[11] K. Keeton, D.A. Patterson, Y.Q. He, and Baker
W.E. Performance Characterization of the Quad
Pentium Pro SMP Using OLTP Workloads. In the
Proceedings of the 1998 InternationalSymposium
on Computer Architecture (to appear), June 1998.
[12] G. Grohoski. Challenges and Trends in Processor
Design: Reining in Complexity. IEEE Computer,
31(1):41–42, January 1998.
[13] P. Rubinfeld. Challenges and Trends in Processor
Design: Managing Problems in High Speed. IEEE
Computer, 31(1):47–48, January 1998.
[14] R. Colwell. Challenges and Trends in Processor
Design: Maintaining a Leading Position. IEEE
Computer, 31(1):45–47, January 1998.
[15] E. Killian. Challenges and Trends in Processor
Design: Challenges, Not Roadblocks. IEEE Com-
puter, 31(1):44–45, January 1998.
[16] K. Diefendorff and P. Dubey. How Multimedia
Workloads Will Change Processor Design. IEEE
Computer, 30(9):43–45, September 1997.
[17] W. Dally. Tomorrow’s Computing Engines.
Keynote Speech, Fourth International Sympo-
sium on High-Performance Computer Architec-
ture, February 1998.
[18] T. Lewis. Information Appliances: Gadget Ne-
topia. IEEE Computer, 31(1):59–68, January
1998.
[19] V. Cerf. The Next 50 Years of Networking. In the
ACM97 Conference Proceedings, March 1997.
[20] G. Bell and J. Gray. Beyond Calculation, The Next
50 Years of Computing, chapter The Revolution
Yet to Happen. Springer-Verlag, February 1997.
[21] D. Matzke. Will Physical Scalability Sabotage
Performance Gains? IEEE Computer, 30(9):37–
39, September 1997.
10
[22] L. Goudge and S. Segars. Thumb: reducing the
cost of 32-bit RISC performance in portable and
consumer applications. In the Digest of Papers,
COMPCON ’96, February 1996.
[23] T. Mudge. Strategic Directions in Computer Ar-
chitecture. ACM Computing Surveys, 28(4):671–
678, December 1996.
[24] C.E. Kozyrakis, S. Perissakis, D. Patterson, T. An-
derson, K. Asanovic, N. Cardwell, R. Fromm,
J. Golbus, B. Gribstad, K. Keeton, R. Thomas,
N. Treuhaft, and K. Yelick. Scalable Processors
in the Billion-Transistor Era: IRAM. IEEE Com-
puter, 30(9):75–78, September 1997.
[25] D. Lammers. Holy grail of embedded dram chal-
lenged. EE Times, 1997.
[26] P. Trancoso, J. Larriba-Pey, Z. Zhang, and J. Tor-
rellas. The Memory Performance of DSS Com-
mercial Workloads in Shared-Memory Multipro-
cessors. In the Proceeding of the Third Inter-
national Symposium on High-Performance Com-
puter Architecture, January 1997.
[27] K. Keeton, R. Arpaci-Dusseau, and D.A. Patter-
son. IRAM and SmartSIMM: Overcoming the
I/O Bus Bottleneck. In the Workshop on ”Mixing
Logic and DRAM: Chips that Compute and Re-
member”, the 24th Annual International Sympo-
sium on Computer Architecture, June 1997.
[28] K. Keeton, D.A. Patterson, and J.M. Hellerstein.
The Intelligent Disk (IDISK): A Revolutionary
Approach to Database Computing Infrastructure.
submitted for publication, March 1998.
11

