1
Computer Architecture
- Introduction
Chin-Fu Kuo
2
About This Course
Textbook
–J. L. Hennessy and D. A. Patterson, Computer
Architecture: A Quantitative Approach, 3rd Edition,
Morgan Kaufmann Publishing Co., 2002.
Course Grading
–30% Project and Quiz
–35% Mid-term Examination
–35% Final-term Examination
–5~10% Class Participation & Discussion
3
This Is an Advanced Course
Have you taken “Computer Organization”before?
 If you never took “Computer Organization”before
–You MUST take it if you are an undergraduate student;
–You may still take this course if you insist, but be prepared to
work hard and read some chapters in “Computer Organization
and Design (COD)3/e”
4
Reference Resources
 Patterson, UC-Berkeley Spring 2001
http://www.cs.berkeley.edu/~pattrsn/252S01/
 David E. Culler, UC-Berkeley, Spring 2002
http://www.cs.berkeley.edu/~culler/cs252-s02/
 David E. Culler, UC-Berkeley, Spring 2005
http://www.cs.berkeley.edu/~culler/courses/cs252-s05/
 Many slides in this course were adapted from UC Berkeley’s
CS252 Course. Copyright 2005, UC Berkeley.
5
Outline
What is Computer Architecture?
–Fundamental Abstractions & Concepts
 Instruction Set Architecture & Organization
Why Take This Course?
Technology
Performance
Computer Architecture Renaissance
6
What is “Computer Architecture”?
Applications
Instruction Set
Architecture (ISA)
Compiler
Operating
System
Firmware
Coordination of many levels of abstraction
Under a rapidly changing set of forces
Design, Measurement, and Evaluation
I/O systemInstr. Set Proc.
Digital Design
Circuit Design
Datapath & Control
Layout & fab
Semiconductor Materials
7
Outline
What is Computer Architecture?
–Fundamental Abstractions & Concepts
 Instruction Set Architecture & Organization
Why Take This Course?
Technology
Performance
Computer Architecture Renaissance
8
The Instruction Set: a Critical Interface
instruction set
Software (SW)
Hardware (HW)
 Properties of a good abstraction
– Lasts through many generations (portability)
– Used in many different ways (generality)
– Provides convenient functionality to higher levels
– Permits an efficient implementation at lower levels
9
Instruction Set Architecture
... the attributes of a [computing] system as seen by the
programmer, i.e. the conceptual structure and functional
behavior, as distinct from the organization of the data
flows and controls the logic design, and the physical
implementation. –Amdahl, Blaaw, and Brooks, 1964
SOFTWARE
-- Organization of Programmable Storage
-- Data Types & Data Structures:
Encodings & Representations
-- Instruction Formats
-- Instruction (or Operation Code) Set
-- Modes of Addressing and Accessing Data Items and Instructions
-- Exceptional Conditions
10
Computer (Machine) Organization
Logic Designer's View
ISA Level
FUs & Interconnect
Capabilities & Performance Characteristics of
Principal Functional Units (FUs)
–(Registers, ALU, Shifters, Logic Units, ...)
Ways in which these components are
interconnected (Bus, Network, …)
Information flows between components (Data,
Messages, Packets, Data path)
Logic and means by which such information
flow is controlled (Controller, Protocol handler,
Control path, Microcode)
Choreography of FUs to realize the ISA
(Execution, Architectural description)
Register Transfer Level (RTL) Description
(Implementation description)
11
Fundamental Execution Cycle
Instruction
Fetch
Instruction
Decode
Operand
Fetch
Execute
Result
Store
Next
Instruction
Obtain instruction
from program
storage
Determine required
actions and
instruction size
Locate and obtain
operand data
Compute result value
or status
Deposit results in
storage for later
use
Determine successor
instruction
Processor
regs
F.U.s
Memory
program
Data
von Neuman
bottleneck
12
Elements of an ISA
 Set of machine-recognized data types
– bytes, words, integers, floating point, strings, . . .
 Operations performed on those data types
– Add, sub, mul, div, xor, move, ….
 Programmable storage
– regs, PC, memory
 Methods of identifying and obtaining data referenced by instructions
(addressing modes)
– Literal, reg., absolute, relative, reg + offset, …
 Format (encoding) of the instructions
– Op code, operand fields, …
Current Logical State
of the Machine
Next Logical State
of the Machine
13
Computer as a State Machine
State: defined by storage
–Registers, Memory, Disk, …
Next state is influenced by the operation
–Instructions, I/O events, interrupts, …
When is the next state decided?
–Result Store: Register write, Memory write
–Output: Device (disk, network) write
Current Logical State
of the Machine
Next Logical State
of the Machine
14
Time for a Long Break and Please …
 Partner w/ a classmate who you didn’t know
 Get the following information from your partner:
– Personal Information & Interests:
Name, Department, Hometown, Favorite sports, …
– Research Directions:
Research Lab, Advisor, Projects, …
– Career Plan:
Engineer, Manager, Teacher, …
– Why take this course
 Introduce your partner to the class after the break.
15
Example: MIPS R3000
0r0
r1
°
°
°
r31
PC
lo
hi
Programmable storage
2^32 x bytes
31 x 32-bit GPRs (R0=0)
32 x 32-bit FP regs (paired DP)
HI, LO, PC
Data types ?
Format ?
Addressing Modes?
Arithmetic logical
Add, AddU, Sub, SubU, And, Or, Xor, Nor, SLT, SLTU,
AddI, AddIU, SLTI, SLTIU, AndI, OrI, XorI, LUI
SLL, SRL, SRA, SLLV, SRLV, SRAV
Memory Access
LB, LBU, LH, LHU, LW, LWL,LWR
SB, SH, SW, SWL, SWR
Control
J, JAL, JR, JALR
BEq, BNE, BLEZ,BGTZ,BLTZ,BGEZ,BLTZAL,BGEZAL
32-bit instructions on word boundary
16
Basic ISA Classes
Accumulator:
1 address add A acc  acc + mem[A]
1+x addressaddx A acc  acc + mem[A + x]
Stack:
0 address add tos  tos + next
General Purpose Register:
2 address add A B EA(A)  EA(A) + EA(B)
3 address add A B C EA(A)  EA(B) + EA(C)
Load/Store:
3 address add Ra Rb Rc Ra  Rb + Rc
load Ra Rb Ra  mem[Rb]
store Ra Rb mem[Rb]  Ra
17
MIPS Addressing Modes & Formats
•Simple addressing modes
•All instructions 32 bits wide
op rs rt rd
immed
register
Register (direct)
op rs rt
register
Base+index
+
Memory
immedop rs rtImmediate
immedop rs rt
PC
PC-relative
+
Memory
•Register Indirect?
18
Instruction Formats & RISC
Variable:
Fixed:
Hybrid:
…
•Addressing modes
–each operand requires addess specifier => variable format
•Code size => variable length instructions
•Performance => fixed length instructions
–simple decoding, predictable operations
•RISC: With load/store instruction arch, only one memory address
and few addressing modes => simple format, address mode given
by opcode (Why would RISC perform better than CISC?)
19
Cray-1: the Original RISC
Op
015
Rd Rs1 R2
2689
Load, Store and Branch
35
Op
015
Rd Rs1 Immediate
2689 35 15 0
Register-Register
20
VAX-11: the Canonical CISC
 Rich set of orthogonal address modes
– immediate, offset, indexed, autoinc/dec, indirect, indirect+offset
– applied to any operand
 Simple and complex instructions
– synchronization instructions
– data structure operations (queues)
– polynomial evaluation
1. In programming, canonical means "according to the rules.”
2. A canonical book is considered inspired and authoritative and is a part
of the rule or standard of faith.
OpCode A/M A/M A/M
Byte 0 1 n m
Variable format, 2 and 3 address instruction
21
Load/Store Architectures
MEM reg
° Substantial increase in instructions
° Decrease in data BW (due to many registers)
° Even more significant decrease in CPI (pipelining)
° Cycle time, Real estate, Design time, Design complexity
° 3-address GPR
° Register-to-register arithmetic
° Load and store with simple addressing modes (reg + immediate)
° Simple conditionals
compare ops + branch z
compare&branch
condition code + branch on condition
° Simple fixed-format encoding
op
op
op
r r r
r r immed
offset
22
MIPS R3000 ISA (Summary)
 Instruction Categories
– Load/Store
– Computational
– Jump and Branch
– Floating Point
 coprocessor
– Memory Management
– Special
R0 - R31
PC
HI
LO
OP
OP
OP
rs rt rd sa funct
rs rt immediate
jump target
3 Instruction Formats: all 32 bits wide
Registers
23
Evolution of Instruction Sets
Single Accumulator (EDSAC 1950)
Accumulator + Index Registers
(Manchester Mark I, IBM 700 series 1953)
Separation of Programming Model
from Implementation
High-level Language Based (Stack) Concept of a Family
(B5000 1963) (IBM 360 1964)
General Purpose Register Machines
Complex Instruction Sets Load/Store Architecture
RISC
(Vax, Intel 432 1977-80) (CDC 6600, Cray 1 1963-76)
(MIPS,Sparc,HP-PA,IBM RS6000, 1987)iX86?
24
Outline
What is Computer Architecture?
–Fundamental Abstractions & Concepts
 Instruction Set Architecture & Organization
Why Take This Course?
Technology
Performance
Computer Architecture Renaissance
25
Why Take This Course?
 To design the next great instruction set?...well...
– instruction set architecture has largely converged
– especially in the desktop / server / laptop space
– dictated by powerful market forces
 Tremendous organizational innovation relative to established ISA
abstractions
 Many New instruction sets or equivalent
– embedded space, controllers, specialized devices, ...
 Design, analysis, implementation concepts vital to all aspects of EE
& CS
– systems, PL, theory, circuit design, VLSI, comm.
 Equip you with an intellectual toolbox for dealing with a host of
systems design challenges
26
Related Courses
Computer
Organization
Computer
Organization
Computer
Architecture
Computer
Architecture
Parallel & Advanced
Computer Architecture
Parallel & Advanced
Computer Architecture
Embedded
Systems
Software
Embedded
Systems
Software
How to build it,
Implementation
details
Why, Analysis,
Evaluation
Parallel Architectures,
Hardware-Software Interactions
System Optimization
RTOS, Tools-chain,
I/O & Device drivers,
Compilers
Hardware-Software
Co-design
Hardware-Software
Co-design
How to make
embedded systems better
SoftwareSoftware
OS,
Programming Lang,
System Programming
Special Topics on
Computer Performance
Optimization
Special Topics on
Computer Performance
Optimization
Performance tools,
Performance skills,
Compiler optimization tricks
27
Computer Industry
Desktop Computing
–Price-performance, Graphics performance
–Intel, AMD, Apple, Microsoft, Linux
–System integrators & Retailers
Servers
–Availability, Scalability, Throughput
–IBM, HP-Compaq, Sun, Intel, Microsoft, Linux
Embedded Systems
–Application-specific performance
–Power, Integration
28
Forces on Computer Architecture
Computer
Architecture
Technology Programming
Languages
Operating
Systems
History
Applications
29
Course Focus
Understanding the design techniques, machine
structures, technology factors, evaluation
methods that will determine the form of
computers in 21st Century
Technology Programming
Languages
Operating
Systems History
Applications Interface Design
(ISA)
Measurement &
Evaluation
Parallelism
Computer Architecture:
•Instruction Set Design
•Organization
•Hardware/Software Boundary Compilers
30
Outline
What is Computer Architecture?
–Fundamental Abstractions & Concepts
 Instruction Set Architecture & Organization
Why Take This Course?
Technology Trend
Performance
Computer Architecture Renaissance
31
Dramatic Technology Advance
 Prehistory: Generations
– 1st Tubes
– 2nd Transistors
– 3rd Integrated Circuits
– 4th VLSI….
 Discrete advances in each generation
– Faster, smaller, more reliable, easier to utilize
 Modern computing: Moore’s Law
– Continuous advance, fairly homogeneous technology
32
Moore’s Law
 “Cramming More Components onto Integrated Circuits”
– Gordon Moore, Electronics, 1965
 # on transistors on cost-effective integrated circuit double every 18 months
(IC上可容納的電晶體數目，約每隔18個月便會增加一倍，性能也
將提升一倍。 )
33
Year
1000
10000
100000
1000000
10000000
100000000
1970 1975 1980 1985 1990 1995 2000
i80386
i4004
i8080
Pentium
i80486
i80286
i8086
Technology Trends:
Microprocessor Capacity
CMOS improvements:
•Die size: 2X every 3 yrs
•Line width: halve / 7 yrs
Itanium II: 241 million
Pentium 4: 55 million
Alpha 21264: 15 million
Pentium Pro: 5.5 million
PowerPC 620: 6.9 million
Alpha 21164: 9.3 million
Sparc Ultra: 5.2 million
Moore’s Law
34
size
Year
1000
10000
100000
1000000
10000000
100000000
1000000000
1970 1975 1980 1985 1990 1995 2000
Memory Capacity
(Single Chip DRAM)
year size(Mb) cyc time
1980 0.0625 250 ns
1983 0.25 220 ns
1986 1 190 ns
1989 4 165 ns
1992 16 145 ns
1996 64 120 ns
2000 256 100 ns
2003 1024 60 ns
35
Optimizing the Design
 Functional requirements set by:
– market sector
– particular company’s product plan
– what the competition is expected to do
 Usual pressure to do everything
– minimize time to market
– maximize performance
– minimize cost & power
 And you only get 1 shot
– no time to try multiple prototypes and evolve to a polished product
– requires heaps of simulations to quantify everything
 quantify model is focus of this course
– requires deep infrastructure and support
36
Technology Trends
 Integrated Circuits
– density increases at 35%/yr.
– die size increases 10%-20%/yr
– combination is a chip complexity growth rate of 55%/yr
– transistor speed increase is similar but signal propagation doesn’t track
this curve - so clock rates don’t go up as fast
 DRAM
– density quadruples every 3-4 years (40 - 60%/yr) [4x steps]
– cycle time decreases slowly - 33% in 10 years
– interface changes have improved bandwidth however
 Network
– rapid escalation - US bandwidth doubles every year at the machine the
expectation bumps periodically - gigabit ether is here now
37
3 Categories Emerge
Desktop
–optimized for price-performance (frequency is a red herring)
Server
–optimized for: availability, scalability, and throughput
–plus a new one: power ==> cost and physical plant site
Embedded
–fastest growing and the most diverse space
 washing machine controller to a network core router
–optimizations: cost, real-time, specialized performance, power
 minimize memory and logic for the task at hand
38
Outline
What is Computer Architecture?
–Fundamental Abstractions & Concepts
 Instruction Set Architecture & Organization
Why Take This Course?
Technology
Cost and Price
Performance
Computer Architecture Renaissance
39
Cost
Clearly a market place issue
–time, volume, commoditization play a big role
–WCT (whole cost transfer) also a function of volume
 CS is all about the cheap copy
However it’s not that simple –what kind of cost
–cost to buy –this is really price
–cost to maintain
–cost to upgrade –never known at purchase time
–cost to learn to use –Apple won this one for awhile
–cost of ISV (indep. SW vendor) software
–cost to change platforms –the vendor lock
–cost of a failure –pandora’s box opens …
 Let’s focus on hardware costs
–it’s simpler
40
Cost Impact
Fast paced industry
–early use of technology is promoted
 Learning curve & process stabilization
–reduces costs over time
Yield - metric of technology maturity
–yield is % of manufactured chips that actually work
–==> things get cheaper with time till they hit 10-20% of initial
 Increasing cost of fab capital
–price per unit has increased
–BUT - cost/function/second going down very rapidly
 what’s missing from this metric??
41
Cost of an IC
 More integration IC cost is bigger piece of total
42
DRAM costs
43
Cost of Die
 Compute
– # dies/wafer & yield as a
function of die area
44
Modern Processor Die Sizes
Pentium Clones
–AMD
 .35u K6 = 162 mm2
 .25u K6-2 = 68 mm2
 .25u K6-3 (256K L1) = 135 mm2
 .18u Athlon, 37M T’s, 6-layer copper, = 120mm2
–Cyrix (they died)
 6u 6x86 = 394/225 mm2
 .35u 6x86 = 169 mm2
 .25u 6x86 (64K L1) = 65 mm2
–IDT (they died)
 .35u Centaur C6 = 88 mm2
 .25u C6 = 60 mm2
45
More Die Sizes
 Intel
–Pentium
 .8u Pentium 60 = 288 mm2
 .6u Pentium 90 = 156 mm2
 .35u = 91 mm2
 .35u MMX = 140/128 mm2
–Pentium Pro
 .6u = 306 mm2
 .35u = 195 mm2
 .6u w/ 256K L2 = 202 mm2
 .35u w/512K L2 = 242 mm2
Pentium II
 .35u = 205 mm2
 .25u = 105 mm2
46
RISC Die Sizes
 HP
– .5u PA-8200 = ~400 mm2
– .25u PA-8600, 116M T’s, 5-metal = 468mm2
– .18u SOI, 186M T’s, 7-copper = 305mm2
 DEC
– .5u 21164 = 298 mm2
– .35u 21264 = 310 mm2
 Motorola
– .5u PPC 604 = 196 mm2
– .35u PPC 604e = 148/96mm2
– .25u PPC 604e = 47.3 mm2
– .81u, G4 7410, 10.5M T’s, 6-metal = 62mm2
 •Transmeta
– Crusoe TM5600
– .18u, 5-copper, 36.8M T’s = 88 mm2
47
Final Chip Cost vs. Size
48
Turning Cost into Price
Direct Costs: labor costs, purchasing components
49
Outline
What is Computer Architecture?
–Fundamental Abstractions & Concepts
 Instruction Set Architecture & Organization
Why Take This Course?
Technology
Performance
Computer Architecture Renaissance
50
Measuring Performance
Several kinds of time”
–stopwatch - it’s what you see but is dependent on
 load
 I/O delays
 OS overhead
–CPU time - time spent computing your program
 factors out time spent waiting for I/O delays
 but includes the OS + your program
–Hence system CPU time, and user CPU time
51
OS Time
Unix time command reports
27.2u 11.1s 56.6 68%
–27.2 seconds of user CPU time
–11.1 seconts of system CPU time
–56.6 seconds total elapsed time
–% of elapsed time that is user + system CPU time
 tells you how much time you spent waiting as a %
52
Benchmarks
Toy benchmarks
–quicksort, 8-queens
 best saved for intro. to programming homeworks
Synthetic benchmarks
–most commonly used since they try to mimic real programs
–problem is that they don’t - each suite has it’s own bias
–no user really runs them
–they aren’t even pieces of real programs
–they may reside in cache & don’t test memory performance
At the very least you must understand what the
benchmark code is in order to understand what it
might be measuring
53
Benchmarks
 Lots of suites - examples
– Dhrystone - tells you how well integers work
– Loops and Linpack - mostly floating point matrix frobbing
– PC specific
 Business Winstone - composite of browser and office apps
 CC Winstone - content creation version - Photoshop, audio editing etc.
 Winbench - collection of subsystem tests that target CPU, disk, and video
subsystems
 SPEC2000 (text bias lies here - see table 1.12 for details)
– 4th generation - primarily designed to test CPU performance
– CINT2000 - 11 integer benchmarks
– CFP2000 - 14 floating point benchmarks
– SPECviewperf - graphics performance of systems using OpenGL
– SPECapc - several large graphics apps
– SPECSFS - file system test
– SPECWeb - web server test
54
More Benchmarks
 TPC
– transaction processing council
– many variants depending on transaction complexity
 TPC-A: simple bank teller transaction style
 TPC -C: complex database query
 TPC-H: decision support
 TPC-R: decision support but with stylized queries (faster than -H)
 TPC-W: web server
 For embedded systems EEMBC “embassy”
– 35 kernels in 5 classes
– 16 automotive/industrial - arithmetic, pointer chasing, table lookup, bit
manip, ...
– 5 consumer - JPEG codec, RGB conversion, filtering
– 3 networking - shortest path, IP routing, packet classification
– 4 office automation - graphics and text processing
– 6 telecommunications - DSP style autocorrelation, FFT, decode, FIR filter, ...
55
Other Problems
Which is better?
By how much?
Are the programs equally important?
56
Some Aggregate Job Mix Options
57
Weighted Variants
58
Normalized Time Metrics
59
Amdahl’s Law
60
Simple Example
61
P
er
fo
rm
an
ce
0.1
1
10
100
1965 1970 1975 1980 1985 1990 1995
Supercomputers
Minicomputers
Mainframes
Microprocessors
Performance Trends
What do we have Today?
63
0
200
400
600
800
1000
1200
87 88 89 90 91 92 93 94 95 96 97
D
EC
A
lp
ha
21
16
4/
60
0
D
EC
A
lp
ha
5/
50
0
D
EC
A
lp
ha
5/
30
0
D
EC
A
lp
ha
4/
26
6
IB
M
PO
W
ER
10
0
D
EC
A
X
P/
50
0
H
P
90
00
/7
50
S
un
-4
/2
60
IB
M
RS
/6
00
0
M
IP
S
M
/1
20
M
IP
S
M
/2
00
0
Processor Performance
(1.35X before, 1.55X now)
1.54X/yr
64
Will Moore’s Law Continue?
Search “Moore’s Law”on the Internet, and you
will see a lot of predictions and arguments.
Don’t bet your house on it (or any technology
stock)…
65
Performance(X) Execution_time(Y)
n = =
Performance(Y) Execution_time(X)
Definition: Performance
Performance is in units of things per sec
–bigger is better
If we are primarily concerned with response time
performance(x) = 1
execution_time(x)
" X is n times faster than Y" means
66
Metrics of Performance
Compiler
Programming
Language
Application
Datapath
Control
Transistors WiresPins
ISA
Function Units
(millions) of Instructions per second: MIPS
(millions) of (FP) operations per second: MFLOP/s
Cycles per second (clock rate)
Megabytes per second
Answers per day/month
67
Components of Performance
CPU time = Seconds = Instructions x Cycles x Seconds
Program Program Instruction Cycle
CPU time = Seconds = Instructions x Cycles x Seconds
Program Program Instruction Cycle
Inst Count CPI Clock Rate
Program X
Compiler X (X)
Inst. Set. X X
Organization X X
Technology X
inst count
CPI
Cycle time
68
What’s a Clock Cycle?
 State changes as Clock “ticks”
 Old days: 10 levels of gates
 Today: determined by numerous time-of-flight issues +
gate delays
– clock propagation, wire lengths, drivers
Latch
or
register
combinational
logic
69
Integrated Approach
What really matters is the functioning of the
complete system, I.e. hardware, runtime system,
compiler, and operating system
In networking, this is called the “End to End
argument”
Computer architecture is not just about transistors,
individual instructions, or particular
implementations
Original RISC projects replaced complex
instructions with a compiler + simple instructions
70
How do you turn more stuff into more
performance?
Do more things at once
Do the things that you do faster
Beneath the ISA illusion….
71
Pipelined Instruction Execution
I
n
s
t
r.
O
r
d
e
r
Time (clock cycles)
Reg A
LU DMemIfetch Reg
Reg A
LU DMemIfetch Reg
Reg A
LU DMemIfetch Reg
Reg A
LU DMemIfetch Reg
Cycle 1 Cycle 2 Cycle 3 Cycle 4 Cycle 6 Cycle 7Cycle 5
72
Limits to pipelining
Maintain the von Neumann “illusion”of one
instruction at a time execution
Hazards prevent next instruction from executing
during its designated clock cycle
–Structural hazards: attempt to use the same hardware to do two
different things at once
–Data hazards: Instruction depends on result of prior instruction
still in the pipeline
–Control hazards: Caused by delay between the fetching of
instructions and decisions about changes in control flow
(branches and jumps).
73
A take on Moore’s Law
Tr
an
si
st
or
s





 





 



 









  





1,000
10,000
100,000
1,000,000
10,000,000
100,000,000
1970 1975 1980 1985 1990 1995 2000 2005
Bit-level parallelism Instruction-level Thread-level (?)
i4004
i8008
i8080
i8086
i80286
i80386
R2000
Pentium
R10000
R3000
74
Progression of ILP
 1st generation RISC - pipelined
– Full 32-bit processor fit on a chip => issue almost 1 IPC
 Need to access memory 1+x times per cycle
– Floating-Point unit on another chip
– Cache controller a third, off-chip cache
– 1 board per processor multiprocessor systems
 2nd generation: superscalar
– Processor and floating point unit on chip (and some cache)
– Issuing only one instruction per cycle uses at most half
– Fetch multiple instructions, issue couple
 Grows from 2 to 4 to 8 …
– How to manage dependencies among all these instructions?
– Where does the parallelism come from?
 VLIW
– Expose some of the ILP to compiler, allow it to schedule
instructions to reduce dependences
75
Modern ILP
 Dynamically scheduled, out-of-order execution
 Current microprocessor fetch 10s of instructions per
cycle
 Pipelines are 10s of cycles deep
=> many 10s of instructions in execution at once
 Grab a bunch of instructionsdetermine all their
dependences, eliminate dep’s wherever possible,
throw them all into the execution unit, let each one
move forward as its dependences are resolved
 Appears as if executed sequentially
 On a trap or interrupt, capture the state of the
machine between instructions perfectly
 Huge complexity
76
Have we reached the end of ILP?
 Multiple processor easily fit on a chip
 Every major microprocessor vendor has gone
to multithreading
– Thread: loci of control, execution context
– Fetch instructions from multiple threads at once,
throw them all into the execution unit
– Intel: hyperthreading, Sun:
– Concept has existed in high performance computing
for 20 years (or is it 40? CDC6600)
 Vector processing
– Each instruction processes many distinct data
– Ex: MMX
 Raise the level of architecture –many
processors per chip
Tensilica Configurable Proc
77
When all else fails - guess
 Programs make decisions as they go
– Conditionals, loops, calls
– Translate into branches and jumps (1 of 5 instructions)
 How do you determine what instructions for fetch when the
ones before it haven’t executed?
– Branch prediction
– Lot’s of clever machine structures to predict future based on history
– Machinery to back out of mis-predictions
 Execute all the possible branches
– Likely to hit additional branches, perform stores
speculative threads
What can hardware do to make programming (with
performance) easier?
78
Numbers and Pictures
 Numbers talk!
– What is a quantitative approach?
– How to collect VALID data?
– How to analyze data and extract useful information?
– How to derive convincing arguments based on numbers?
 Pictures
– A good picture = a thousand words
– Good for showing trends and comparisons
– High-level managers have no time to read numbers
– Business people want pictures and charts
79
The Memory Abstraction
Association of <name, value> pairs
–typically named as byte addresses
–often values aligned on multiples of size
Sequence of Reads and Writes
Write binds a value to an address
Read of addr returns most recently written value
bound to that address
address (name)
command (R/W)
data (W)
data (R)
done
80
µProc
60%/yr.
(2X/1.5yr
)
DRAM
9%/yr.
(2X/10
yrs)
1
10
100
1000
19
80
19
81
19
83
19
84
19
85
19
86
19
87
19
88
19
89
19
90
19
91
19
92
19
93
19
94
19
95
19
96
19
97
19
98
19
99
20
00
DRAM
CPU
19
82
Processor-Memory
Performance Gap:
(grows 50% / year)
Pe
rf
or
m
an
ce
Time
“Joy’s Law”
Processor-DRAM Memory Gap (latency)
81
Levels of the Memory Hierarchy
CPU Registers
100s Bytes
<< 1s ns
Cache
10s-100s K Bytes
~1 ns
$1s/ MByte
Main Memory
M Bytes
100ns- 300ns
$< 1/ MByte
Disk
10s G Bytes, 10 ms
(10,000,000 ns)
$0.001/ MByte
Capacity
Access Time
Cost
Tape
infinite
sec-min
$0.0014/ MByte
Registers
Cache
Memory
Disk
Tape
Instr. Operands
Blocks
Pages
Files
Staging
Xfer Unit
prog./compiler
1-8 bytes
cache cntl
8-128 bytes
OS
512-4K bytes
user/operator
Mbytes
Upper Level
Lower Level
faster
Larger
circa 1995 numbers
82
The Principle of Locality
 The Principle of Locality:
– Program access a relatively small portion of the address space at any instant of
time.
 Two Different Types of Locality:
– Temporal Locality (Locality in Time): If an item is referenced, it will tend to be
referenced again soon (e.g., loops, reuse)
– Spatial Locality (Locality in Space): If an item is referenced, items whose
addresses are close by tend to be referenced soon
(e.g., straightline code, array access)
 Last 30 years, HW relied on locality for speed
P MEM$
83
The Cache Design Space
 Several interacting dimensions
– cache size
– block size
– associativity
– replacement policy
– write-through vs write-back
 The optimal choice is a compromise
– depends on access characteristics
 workload
 use (I-cache, D-cache, TLB)
– depends on technology / cost
 Simplicity often wins
Associativity
Cache Size
Block Size
Bad
Good
Less More
Factor A Factor B
84
Is it all about memory system design?
 Modern microprocessors are almost all cache
85
Memory Abstraction and Parallelism
Maintaining the illusion of sequential access to
memory
What happens when multiple processors access
the same memory at once?
–Do they see a consistent picture?
Processing and processors embedded in the
P1
$
Interconnection network
$
Pn
Mem Mem
P1
$
Interconnection network
$
Pn
Mem Mem
86
System Organization:
It’s all about communication
Proc
Caches
Busses
Memory
I/O Devices:
Controllers
adapters
Disks
Displays
Keyboards
Networks
Pentium III Chipset
87
Breaking the HW/Software Boundary
 Moore’s law (more and more trans) is all about volume and
regularity
 What if you could pour nano-acres of unspecific digital logic
“stuff”onto silicon
– Do anything with it. Very regular, large volume
 Field Programmable Gate Arrays
– Chip is covered with logic blocks w/ FFs, RAM blocks, and interconnect
– All three are “programmable”by setting configuration bits
– These are huge?
 Can each program have its own instruction set?
 Do we compile the program entirely into hardware?
88
“Bell’s Law”–new class per decade
year
lo
g
(p
eo
p
le
p
er
co
m
p
u
te
r)
streaming
information
to/from physical
world
Number Crunching
Data Storage
productivity
interactive
•Enabled by technological opportunities
•Smaller, more numerous and more intimately connected
•Brings in a new kind of application
•Used in many ways not previously imagined
89
It’s not just about bigger and faster!
 Complete computing systems can be tiny and cheap
 System on a chip
 Resource efficiency
– Real-estate, power, pins, …
90
The Process of Design
Design
Analysis
Architecture is an iterative process:
•Searching the space of possible designs
•At all levels of computer systems
Creativity
Good Ideas
Mediocre Ideas
Bad Ideas
Cost /
Performance
Analysis
91
Amdahl’s Law
 
enhanced
enhanced
enhanced
new
old
overall
Speedup
Fraction
Fraction
1
ExTime
ExTime
Speedup


1
Best you could ever hope to do:
 enhancedmaximum Fraction-1
1Speedup 
  






enhanced
enhanced
enhancedoldnew Speedup
Fraction
FractionExTimeExTime 1
92
Computer Architecture Topics
Instruction Set Architecture
Pipelining, Hazard Resolution,
Superscalar, Reordering,
Prediction, Speculation,
Vector, Dynamic Compilation
Addressing,
Protection,
Exception Handling
L1 Cache
L2 Cache
DRAM
Disks, WORM, Tape
Coherence,
Bandwidth,
Latency
Emerging Technologies
Interleaving
Bus protocols
RAID
VLSI
Input/Output and Storage
Memory
Hierarchy
Pipelining and Instruction
Level Parallelism
Network
Communication
O
th
er
Pr
oc
es
so
rs
93
Computer Architecture Topics
M
Interconnection NetworkS
PMPMPMP
° ° °
Topologies,
Routing,
Bandwidth,
Latency,
Reliability
Network Interfaces
Shared Memory,
Message Passing,
Data Parallelism
Processor-Memory-Switch
Multiprocessors
Networks and Interconnections
94
Course Focus
Understanding the design techniques, machine structures,
technology factors, evaluation methods that will determine
the form of computers in 21st Century
Technology Programming
Languages
Operating
Systems History
Applications Interface Design
(ISA)
Measurement &
Evaluation
Parallelism
Computer Architecture:
•Instruction Set Design
•Organization
•Hardware/Software Boundary Compilers

