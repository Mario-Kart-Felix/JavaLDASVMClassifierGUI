 
 
COMPUTER ARCHITECTURE WITH 
ASSOCIATIVE PROCESSOR REPLACING LAST 
LEVEL CACHE AND SIMD ACCELERATOR 
L. Yavits, A. Morad, R. Ginosar 
Abstractâ€”This study presents a computer architecture where a last level cache and a SIMD accelerator are replaced by an 
Associative Processor. Associative Processor combines data storage and processing, and functions as a parallel SIMD 
processor and a memory at the same time. An analytic performance model of this computer architecture is introduced. 
Comparative analysis supported by cycle-accurate simulation and emulation shows that this architecture may outperform a 
conventional computer architecture comprising a SIMD coprocessor and a shared last level cache while consuming less power.  
Index Termsâ€” Multicore, SIMD, Associative Processor, Processing In Memory, PIM.   
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   ïµ   â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 
1 INTRODUCTION 
achine learning, data mining, network routing, 
search engines and other big data applications can 
be significantly sped up by massively parallel 
SIMD machines [42]. Many of todayâ€™s computing archi-
tectures include vector, or SIMD coprocessors [1][16][24]. 
However data transfer between processing units (PUs) 
and memory significantly limits the performance of SIMD 
architectures [32]. High utilization of SIMD processor 
requires very high computation-to-bandwidth ratio and 
large data sets [30].   
Power dissipation and on-chip communication are 
among the main factors limiting the scalability of parallel 
architectures [8]. Data synchronization and communica-
tion between PUs of SIMD processor and their private 
and shared memories lead to wasting energy on non-
processing tasks and limit the speedup of parallel SIMD 
architectures [46].   
When operating at high rates, arrays of computing el-
ements in SIMD processors are very active, resulting in 
irregular thermal density and hotspots [41] and further 
limiting the scalability of conventional SIMD architec-
tures. 
 The Associative Processor (AP) is a viable alternative to 
conventional SIMD processors [13][40][47]. The AP com-
prises a modified Content Addressable Memory (CAM) 
and facilitates processing in addition to storage. AP can 
be used as an efficient accelerator of massively-parallel 
fine-grain SIMD workloads.    
In this study we propose to replace the last level cache 
(LLC) of a baseline CPU architecture (Fig. 1(a)), or the 
combination of the LLC and a dedicated SIMD coproces-
sor (Fig. 1(b)), by an AP (Fig. 1(c)). The goals we set to 
achieve are as follows: 
ï‚· Convert the data cache into a massively-parallel pro-
cessor capable of performing a variety of data-
parallel fine-grain tasks.  
ï‚· Eliminate a power- and bandwidth-limited SIMD 
coprocessor. 
ï‚· Combine data storage and data processing and elim-
inate performance degradation and energy dissipa-
tion due to massive PU-to-memory data synchroniza-
tion. 
The AP may be operated in two modes: 
ï‚· Conventional Cache mode, in which the AP serves as 
data cache during the execution of the sequential 
segments of a workload;  
ï‚· Associative Processing mode, in which the parallelizable 
segments of a workload are executed on the AP. No 
data synchronization between sequential and parallel 
segments is required since the data is stored in the 
AP prior to the parallel execution and remains there 
after the parallel segment completes.    
The AP delivers a number of advantages over a con-
ventional SIMD architecture:  
ï‚· Data processing and data storage are unified. There is 
no need for data transfer between memory and PUs; 
ï‚· Two basic operations of AP are essentially standard 
memory operations: write and read. The third basic 
operation, compare, is implemented similarly to read, 
and is performed along memory rows rather than 
columns.  Therefore the per-bit power consumption 
of the AP is almost identical to that of RAM, which 
may consume an order of magnitude lower active 
power (and lower leakage power) per area than log-
ic [34]; 
ï‚· In conventional cache mode, use of CAM instead of 
RAM enables full associativity. Consequently, it may 
allow reduction of hardware and software complexi-
ty of the cache (for example, the elimination of costly 
tag array circuitry), as well as elimination of conflict 
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 
ï‚· Leonid Yavits (*), E-mail: yavits@tx.technion.ac.il. 
ï‚· Amir Morad (*), E-mail: amirm@tx.technion.ac.il. 
ï‚· Ran Ginosar (*), E-mail: ran@ee.technion.ac.il. 
 (*) Authors are with the Department of Electrical Engineering, Technion-
Israel Institute of Technology, Haifa 32000, Israel. 
 
M 
 
 
(interference) misses.  
ï‚· There are fewer hotspots. AP power is distributed 
uniformly over the entire processing array rather 
than being concentrated around PUs as in the case of 
conventional SIMD. Since leakage power may super-
linearly depend on temperature [5], this could pro-
vide a significant advantage. 
The drawback in replacing the LLC by the AP is that 
the effective cache size in the conventional cache mode is 
nearly halved, since the AP bit cell is about twice the size 
of a RAM cell (Fig. 3). This may lead to certain perfor-
mance degradation during the execution of the sequential 
portion of a workload, but the speedup achieved during 
the execution of the parallel portion of the workload may 
yield a significant improvement in the overall system per-
formance. 
The first contribution of this paper is the integration of 
an AP on-chip of a standard CPU. The memory of the AP 
replaces the LLC, while the processing of the AP replaces 
the on-chip SIMD accelerator. This contribution leads to 
improvement in performance, reduction in power dissi-
pation, and lower temperature, enabling 3D integration. 
Another contribution of our work is the comparative 
performance and power analysis of AP vs. a conventional 
SIMD processor, supported by analytical modeling, cycle-
accurate simulation and emulation. Thanks to modern 
feature scaling and aggressive memory integration on one 
side, and the rise of big data on the other, we believe we 
are at an inflection point where AP may outperform con-
ventional SIMD in both performance and power. 
The rest of this paper is organized as follows. Section 2 
discusses the related work. Section 3 provides a detailed 
description of the AP and its operation. Section 4 presents 
simulation and analytical modeling of AP and compares 
it to a conventional SIMD processor. Section 5 combines 
analytical, simulation and emulation methods to compare 
performance and power consumption of the three archi-
tectures of Fig. 1, and Section 6 offers conclusions. 
 
CPU
L1
L2/LLC
Baseline CPU Architecture
CPU
L1
SIMD Coprocessor
CPU + LLC and SIMD Coprocessor
CPU
L1
L2/LLC
Associative 
Processor
CPU + Associative Processor
(a) (b)
(c)
 
Fig. 1. (a) Baseline CPU with the LLC, (b) CPU with the LLC and the 
SIMD coprocessor, (c) CPU with the AP replacing the LLC. 
2 RELATED WORK 
A major notion of our work is using AP to unify pro-
cessing and storage, i.e. achieve 'processing in memory' 
(PIM). Hence we place our research in the context of prior 
work on PIM. The concept of mixing memory and logic 
has been around since 1960s. The DAPP, STARAN, CM-2, 
and GAPP computer architectures [36] used large number 
of PUs positioned in proximity of memory arrays to im-
plement massively parallel SIMD computer. 
M. Gokale et al. [15] designed TeraSys, a computer ar-
chitecture comprising a conventional host processor, with 
at least part of its memory replaced by PIM array, inte-
grating memory and ALUs in close proximity. M. Hall et 
al. [19] developed DIVA, the Data-Intensive Architecture, 
combining PIM memories with external host processors. 
One of their main focuses was performing selected com-
putation in processing elements near memory and reduc-
ing the quantity of data transferred across the long and 
slow processor-memory interface. G. AlmÃ¡si et al. [3] de-
veloped Cyclops, an architecture combining memory and 
a large number of simple PUs. According to their find-
ings, standard benchmarks are not efficient when evaluat-
ing the performance of PIM architecture. Instead, they 
focused on scientific kernels including FFT, matrix-matrix 
and matrix-vector multiplication, etc. T. Sterling et al. [43] 
developed Gilgamesh, a PIM based massively parallel 
architecture, with the focus on advanced mechanisms for 
virtualizing tasks and data. P. Kogge et al. [25] developed 
HTMT, a parallel multilevel memory architecture, where 
each RAM level is a PIM memory (memory blocks inter-
connected to ALUs). J. Suh et al. [44] introduced a SLIIC 
QL computer featuring a processor integrated on the 
same die with DRAM. J. Brockman et al. [9] developed 
PIM lite, a PIM architecture featuring a multithreaded 
core with SIMD accelerator integrated with DRAM on the 
same chip. Last, G. Lipovsky et al. [28] developed a dy-
namic associative access memory architecture that com-
bined DRAM and a single-bit processing element, capable 
of associative and conventional arithmetic processing, 
placed in DRAMâ€™s sense amplifier area. More recently, it 
became impractical to embed processing on DRAM chips, 
as the IC technology of DRAM does not support logic 
circuits. All these PIM architectures placed processing in 
proximity of memory. In contrast, this work considers AP, 
in which processing is carried out within each bit cell. 
Prior work on the AP concept was conducted over the 
years. Foster [13] laid the foundations for associative pro-
cessing. J. Potter et al. [35] developed an associative pro-
graming model and language and applied it to a wide 
variety of applications including image processing, graph 
algorithms, data base management, graphics, etc.  I. 
Scherson et al. [40] developed high-speed AP architec-
tures [40]. The present authors have implemented a com-
plete stand-alone AP as a VLSI chip [47]. 
This work progresses from processing in proximity of 
memory to processing combined inside memory PIM. Its 
key contribution is to integrate an AP on-chip of a stand-
ard CPU. The memory of the AP replaces the LLC, while 
the processing of the AP replaces the on-chip SIMD accel-
erator.   
3 THE ASSOCIATIVE PROCESSOR 
In this section we present the architecture of the AP 
 
 
and explain the principles of associative computing. 
3.1 Associative Processor Architecture  
AP is based on modified CAM. The CAM allows com-
paring all data words to a key, tagging the matching 
words, and possibly reading some or all of the tagged 
words one by one. In addition, standard memory read 
and write operations of a single word at a time can also 
take place. 
Unlike CAM, typical operations in AP are consecutive 
compare and write, usually involving just a few bit col-
umns.  The AP enhances the CAM by allowing parallel 
writing into selected bits of all tagged words. The archi-
tecture of AP is presented in Fig. 2. The Associative Pro-
cessing Array comprises bit cells (further described be-
low) organized in bit-columns and word-rows. Typically, 
a word-row makes a PU (although parts of a row, or al-
ternatively multiple rows, may also be configured as a 
PU). Since we operate the AP in dual mode (conventional 
cache and associative processing), single PU may be 
aligned with a cache line (for example 64 bytes) for higher 
efficiency. Several special registers are appended to the 
associative processing array. The KEY register contains a 
key data word to be written or compared against. The 
MASK register defines the active fields for write and read 
operations, enabling bit selectivity. The TAG register 
marks the rows that are matched by the compare opera-
tion and may be affected by consecutive parallel write.  
The AP may require a microcontroller and an instruction 
cache. An optional Interconnect allows PUs of the AP to 
communicate in parallel. Since associative processing op-
eration is mainly bitwise, the Interconnect can be a rela-
tively simple circuit-switched network. The Interconnect 
is further discussed in Section 3.2. Reduction Tree ([37], 
earlier introduced as â€˜response counterâ€™ in [47]) is an ad-
der tree, enabling quick parallel summation of TAG bits. 
This operation is useful whenever a vector needs to be 
reduced into a scalar. 
 
Controller
Associative 
Processing
Array
TA
G
I-Cache
In
te
rc
o
n
n
e
ct
MASK
Bit
Cell
Bit
Cell
Bit
Cell
Bit
Cell
Bit
Cell
Bit
Cell
Bit
Cell
Bit
Cell
Bit
Cell
Bit
Cell
Bit
Cell
Bit
Cell
Bit
Cell
R
e
d
u
ct
io
n
 T
re
e
KEY
 
Fig. 2. Associative processor architecture 
 
A static memory based associative bit cell is shown in 
Fig. 3. Its two main components are the 6-Transistors (6T) 
SRAM bit cell and the 4T N-type XOR. Two additional 
transistors (gated by the Mask wire) are used to mask the 
write operation at the bit (column) level. Alternative de-
signs have also been proposed, to reduce power dissipa-
tion [27], to save area [33] or to exploit non-transistor 
technology [37].   
 
 
Fig. 3. NOR-type Associative Bit Cell 
 
To compare the key data word to the data stored in the 
associative memory (the entire row, a number of bits or a 
single bit), the Match line is precharged and the inverted 
key is set on Bit and Bit-not lines. In the columns that 
should be ignored during comparison, Bit and Bit-not 
lines are set to 0. If all unmasked bits in a row match the 
key (i.e. every unmasked bit in a row is different from the 
corresponding inverted key bit), the Match line remains 
high and a 1 is written into the corresponding TAG bit. If 
the key differs from the row data (even in one bit), the 
Match line discharges and a 0 is written into the TAG bit.    
In AP, compare is typically followed by a parallel write 
into the unmasked bits of all tagged words. To write data 
(from the KEY register) into the associative memory, each 
TAG bit (set earlier by the compare) is connected to the 
corresponding Word line. If a row matched during the 
compare, the key data is written into it in accordance with 
the MASK pattern. Otherwise (in the case of mismatch), 
the write does not affect the row. Typically, 12.5-25% of 
the rows are written during a write in arithmetic opera-
tions as further shown in Section 3.2.  
A compare-write sequence is illustrated in Fig. 4. A 
KEY value â€˜001â€™ with MASK value â€˜011â€™ is compared 
against the associative memory content. Afterwards a 
KEY value â€˜111â€™ with MASK value â€˜110â€™ is written into all 
associative memory rows that matched during Compare. 
In Compare, an inverted KEY bit is compared with each 
associative memory bit of its bit-column in parallel. The 
results of bit-compares are AND-ed in each row to gener-
ate a match or mismatch. The AND output is stored in a 
TAG bit. The masked-out bit columns do not affect the 
Compare result. In consequent Write, the KEY value is 
broadcast to the entire associative memory array. The 
logic AND of a MASK bit and the TAG bit is used to ena-
ble / disable the write operation: only the rows that 
matched during the Compare and only the bits which 
MASK is 1 are written.     
 
 
 
Fig. 4. Compare-Write Logic Sequence 
 
To read data from memory, the Bit and Bit-not lines 
are precharged and the Word line is asserted. Parallel 
write and sequential read operations are enabled only for 
the columns whose mask bits are set in the MASK regis-
ter. 
A complete design of an AP is presented in [47]. 
 
3.2 Associative Computing 
AP is a massively parallel SIMD accelerator. It can im-
plement a wide range of processing tasks, as well as clas-
sical CAM operations such as associative search, sorting 
and ordering. In addition it supports standard memory 
operations (word and block read and write). AP is effi-
cient for computational tasks that require fine-grain mas-
sive data parallelism, such as high-resolution image pro-
cessing or large data set sparse linear algebra algorithms 
as may be required in machine learning.  
Arithmetic operations in the AP can be performed in 
parallel on all PUs in a word-parallel, bit-serial manner. 
For instance, vector addition may be performed as fol-
lows [13]. Two ğ‘š bit columns hold vectors A and B (Fig. 
5). Their sum A+B is written over B. A one-bit column C 
holds the carry bit. The addition is carried out in ğ‘š sin-
gle-bit addition parallel steps (1):  
 
ğ‘[âˆ—] | ğ‘ [âˆ—]ğ‘– = ğ‘[âˆ—]ğ‘– + ğ‘[âˆ—]ğ‘– + ğ‘[âˆ—]   
âˆ€ ğ‘– = 0, â€¦ , ğ‘š âˆ’ 1 
(1) 
 
where ğ‘– is the bit index and â€˜âˆ—â€™ is the word index in the 
vector. The single-bit addition (TABLE 1) is carried out in 
a series of compare-write steps (as illustrated by Fig. 4). In 
each such step, one input entry of the truth table (a three 
bit input pattern) is matched against the contents of the 
ğ‘[âˆ—]ğ‘– , ğ‘[âˆ—]ğ‘– , ğ‘[âˆ—] bit columns in the associative array, and 
the matching rows (PUs) are tagged; then the logic result 
(two-bit output of the truth table in TABLE 1) is written 
into ğ‘ğ‘– and ğ‘ bits of all tagged rows. During each compare 
and write step, all but three input bit columns and two 
output bit columns respectively are masked out, so that 
2.5 bit columns are active on average. Some input combi-
nations do not change the output and therefore can be 
skipped (â€œNo actionâ€ in the table). Since the operation 
overwrites one of the inputs, computation must be carried 
out according to the order indicated in TABLE 1 [13].  
Overall, four compare - write steps are required to 
complete the single-bit addition. Therefore, fixed point ğ‘š 
bit addition takes 8ğ‘š âˆˆ ğ‘‚(ğ‘š)  cycles. Subtraction and 
comparison operations are performed similarly and also 
require ğ‘‚(ğ‘š) cycles. Note the stark contrast with SIMD 
architectures of low PU count that require ğ‘‚(ğ‘) cycles to 
add N data elements (without taking into account the 
load / store / move time). 
Fixed precision multiplication and division in AP are 
implemented by long multiplication and division respec-
tively, consisting of a series of add-shift and subtract-shift 
operations, executed bit-serially but in parallel for all data 
words. The addition or subtraction are done as described 
above (multiplication is usually done â€œMSB firstâ€), while 
shift is implemented by activating different bit columns 
and therefore requires no cycles. Thus, fixed point ğ‘š âˆ’
ğ‘ğ‘–ğ‘¡ Ã— ğ‘š âˆ’ ğ‘ğ‘–ğ‘¡ vector multiplication requires ğ‘‚(ğ‘š2) cy-
cles [13], regardless of the length of the vectors.  
Floating point arithmetic for APs is somewhat more 
complex to implement. Different exponents require shift-
ing mantissas by different lengths, resulting in a sequence 
of bit-serial operations. Still, a direct implementation of 
IEEE single precision floating point element-by-element 
vector multiplication (ğµ =)ğ´ Ã— ğµ requires only 4400 cy-
cles, regardless of the length of the vector.  
 
KEY
T
A
G
...1 0A Amï€­ ...1 0B Bmï€­ï€«C
MASK
 
Fig. 5. Addition Example 
 
TABLE 1 
IMPLEMENTING FULL ADDER IN ASSOCIATIVE PROCESSOR 
Entry Input 
C 
Input 
B 
Input 
A 
Output 
C 
Output 
B 
Comments 
0 0 0 0 0 0 No action 
1 0 0 1 0 1 2nd pass 
2 0 1 0 0 1 No action 
3 0 1 1 1 0 1st pass 
4 1 0 0 0 1 3rd pass 
5 1 0 1 1 0 No action 
6 1 1 0 1 0 4th pass 
7 1 1 1 1 1 No action 
Pass = COMPARE cycle followed by WRITE cycle  
 
Arithmetic operations are presented in this Section 
under the assumption that the relevant operands are lo-
cated in the same PU. However, many workloads require 
inter-PU data communications. Depending on the work-
load, communication requirements may vary from no 
communications (for â€œembarrassingly parallelâ€ tasks such 
as Black-Scholes option pricing) to relatively intense 
 
 
communications (e.g., for FFT). In some cases, support for 
special pre-defined communication patterns or permuta-
tions can be of advantage (e.g., for FFT). The inter-PU 
communication can be implemented serially, through a 
series of associative memory reads and writes. Alterna-
tively, the dedicated Interconnect introduced in Sec-
tion 3.1 can be employed to provide parallel communica-
tion capabilities, i.e. to allow all PUs to communicate in 
parallel. 
4 ANALYTIC MODEL AND COMPARATIVE ANALYSIS 
Analytical modeling is becoming an increasingly im-
portant technique used in the design of chip multiproces-
sors [11][20][23][29][45]. In this section we develop an 
analytical performance and power consumption model of 
the conventional SIMD and the AP and compare their 
relative performance, area and power consumption under 
constrained area and power resources. Here we study 
only the parallelizable portion of a workload. For simplic-
ity, we assume that the parallelizable portion contains 
single-cycle instructions (i.e. arithmetic, control, register 
file access and alike). We also assume the performance of 
the baseline sequential CPU to be 1 for the sake of esti-
mating the relative speedup delivered by the reference 
SIMD coprocessor and the AP.  
We verify our analytical modeling findings using cy-
cle-accurate simulation of the AP. The simulator, our 
simulation methodology and simulation results are de-
scribed in details in Section 4.4. 
4.1 Reference SIMD Processor 
Fig. 6 presents the computer architecture comprising 
the sequential CPU, the shared LLC and the SIMD copro-
cessor, as depicted in Fig. 1(b). The reference SIMD co-
processor contains a number of baseline PUs (BPUs), each 
containing a floating point ALU and a register file. The 
BPUs are connected to the shared LLC through a band-
width-limited interface, and are interconnected using an 
interconnection network (not shown). 
Let the serial execution time of the parallelizable por-
tion ğ‘“ of the program on the baseline sequential CPU be 
ğ‘‡1. The execution time ğ‘‡ğ‘“,ğ‘†ğ¼ğ‘€ğ· of that parallelizable por-
tion on the SIMD coprocessor can then be written as fol-
lows: 
 
ğ‘‡ğ‘“,ğ‘†ğ¼ğ‘€ğ· =
ğ‘‡1
ğ‘›ğ‘†ğ¼ğ‘€ğ·
+
ğ‘‡C
ğ‘›ğ‘†ğ¼ğ‘€ğ·
+ ğ‘‡S (2) 
 
where  ğ‘›ğ‘†ğ¼ğ‘€ğ· is the number of BPUs, ğ‘‡C is the time spent 
exclusively on inter-BPU communication, and ğ‘‡S is the 
time spent exclusively on synchronization of the LLC to 
the private SIMD memory [46]. The synchronization con-
sists of the time to move data from LLC to SIMD before 
the parallel segment begins, and from SIMD to LLC after 
the parallel segment completes. Since it involves access to 
a shared resource, ğ‘‡S might depend on the number of 
BPUs in the SIMD coprocessor [12][39]. This is especially 
the case when the data set size is scaled down to the pro-
cessor size.  
 
LLC (Shared by CPU and SIMD)
BPU BPU BPU BPU BPU
CPU
L1 Cache
LLC to SIMD Synchronization
 
Fig. 6. CPU with SIMD coprocessor and shared LLC 
 
While a number of BPUs (or all BPUs) can communi-
cate with each other in parallel (although using a poten-
tially congested interconnection network that affects ğ‘‡ğ¶), 
the LLC-to-SIMD synchronization is done essentially se-
rially for each BPU. Therefore the inter-BPU communica-
tion time scales by the number of BPUs while ğ‘‡S does not 
scale. The speedup of the SIMD processor over the se-
quential CPU can be written as follows: 
 
ğ‘†ğ‘†ğ¼ğ‘€ğ· =
ğ‘‡1
ğ‘‡ğ‘“,ğ‘†ğ¼ğ‘€ğ·
=
1
1
ğ‘›ğ‘†ğ¼ğ‘€ğ·
+
ğ¼ğ‘
ğ‘›ğ‘†ğ¼ğ‘€ğ·
+ ğ¼ğ‘ 
 
 
(3) 
where ğ¼ğ‘ = ğ‘‡ğ¶/ğ‘‡1 is the connectivity intensity, or ratio of the 
time spent on inter-BPU communication to the serial exe-
cution time, and ğ¼ğ‘  = ğ‘‡ğ‘†/ğ‘‡1 is the synchronization intensity, 
or the ratio of time spent on LLC-to-SIMD synchroniza-
tion to the serial execution time.  
The area of the SIMD processor can be presented as 
follows: 
ğ´ğ‘†ğ¼ğ‘€ğ· = ğ‘›ğ‘†ğ¼ğ‘€ğ·(ğ´ğ´ğ¿ğ‘ˆ + ğ´ğ‘…ğ¹) (4) 
 
where ğ´ğ´ğ¿ğ‘ˆ is the ALU area and ğ´ğ‘…ğ¹ is the register file 
area. As noted above, the inter-PU connection network is 
omitted. 
For easy comparison between PU and memory areas, 
we represent all area values (ALU, registers, memory) in 
terms of baseline SRAM cell area. Let the baseline SRAM 
cell area be 1. In 22nm CMOS technology, the actual fig-
ure is in the range of 0.1ğœ‡ğ‘š2 [4]. Then we can write:  
 
ğ´ğ´ğ¿ğ‘ˆ = ğ´ğ´ğ¿ğ‘ˆğ‘œğ‘š
2  
ğ´ğ‘…ğ¹ = ğ´ğ‘…ğ¹ğ‘œğ‘˜ğ‘š (5) 
 
where ğ´ğ´ğ¿ğ‘ˆğ‘œ is the area of a single bit of the ALU and ğ´ğ‘…ğ¹ğ‘œ 
is the area of a register bit (a flip-flop), both measured in 
baseline SRAM cell area units; ğ‘š is data wordlength and 
ğ‘˜ is the size of the register file. This model is quite basic 
and does not take into account numerous aspects of SIMD 
design (instruction cache, communication and control, 
etc.). Its purpose is providing the best case reference fig-
ures for the comparative analysis of the conventional 
SIMD processorâ€™s speedup, area and power. 
The average power of the SIMD processor (over the 
execution span ğ‘‡ğ‘“,ğ‘†ğ¼ğ‘€ğ·) can be written as follows: 
 
 
 
ğ‘ƒğ‘†ğ¼ğ‘€ğ· =
ğ¸ğ¶ğ‘‚ğ‘€ğ‘ƒ + ğ¸ğ¶ + ğ¸ğ‘† + ğ¸ğ¿ğ¸ğ´ğ¾
ğ‘‡ğ‘“,ğ‘†ğ¼ğ‘€ğ·
 =
=
ğ‘ƒğ¶ğ‘‚ğ‘€ğ‘ƒ
ğ‘›ğ‘†ğ¼ğ‘€ğ·
+
ğ¼ğ‘ğ‘ƒğ¶
ğ‘›ğ‘†ğ¼ğ‘€ğ·
+ ğ¼ğ‘ ğ‘ƒğ‘†
(
1
ğ‘›ğ‘†ğ¼ğ‘€ğ·
+
ğ¼ğ‘
ğ‘›ğ‘†ğ¼ğ‘€ğ·
+ ğ¼ğ‘ )
+ ğ‘ƒğ¿ğ¸ğ´ğ¾ 
(6) 
where ğ‘‡ğ‘“,ğ‘†ğ¼ğ‘€ğ· is the execution time of the parallelizable 
portion of the program on the SIMD processor (2); ğ¸ğ¶ğ‘‚ğ‘€ğ‘ƒ 
and ğ‘ƒğ¶ğ‘‚ğ‘€ğ‘ƒ are the energy and the average power con-
sumption during computation; ğ¸ğ¶ and ğ‘ƒğ¶ are the energy 
and the average power consumed during inter-BPU 
communication; ğ¸ğ‘† and ğ‘ƒğ‘† are the energy and the average 
power consumed during LLC-to-SIMD synchronization; 
ğ¸ğ¿ğ¸ğ´ğ¾ and ğ‘ƒğ¿ğ¸ğ´ğ¾ are the leakage energy and power; ğ¼ğ‘ and 
ğ¼ğ‘  are the connectivity and synchronization intensities as 
defined above.    
Just as in the case of area comparison, we represent all 
power values (ALU, registers, memory) through the write 
power consumption of a baseline SRAM memory cell. Let 
the power consumption of the baseline SRAM cell during 
write from â€˜0â€™ to â€˜1â€™ or from â€˜1â€™ to â€˜0â€™ be 1. In 22nm CMOS 
technology, the actual figure is in the range of 1ğœ‡ğ‘Š [22]. 
Then we can further write the SIMD power consumption 
as follows: 
 
ğ‘ƒğ¶ğ‘‚ğ‘€ğ‘ƒ = ğ‘›ğ‘†ğ¼ğ‘€ğ·(ğ‘ƒğ´ğ¿ğ‘ˆğ‘œğ‘š
2 + ğ‘ƒğ‘…ğ¹ğ‘œğ‘˜ğ‘š)  
ğ‘ƒğ¶ = ğ‘›ğ‘†ğ¼ğ‘€ğ·ğ‘ƒğ¶ğ‘œğ‘š (7) 
ğ‘ƒğ‘† = ğ‘ƒğ‘†ğ‘œğ‘š  
  
where ğ‘ƒğ´ğ¿ğ‘ˆğ‘œ and  ğ‘ƒğ‘…ğ¹ğ‘œ are the average per-bit power con-
sumptions of the ALU and RF respectively during com-
putation. ğ‘ƒğ¶ğ‘œ is the per-bit power consumption during the 
inter-BPU communication. ğ‘ƒğ‘†ğ‘œ is the per-bit power con-
sumed performing LLC-to-SIMD synchronization. We 
assume the amount of data that needs to be synchronized 
with LLC, as well as transferred during inter-BPU com-
munication, is limited to a single data word per BPU. 
ğ‘ƒğ¶ğ‘‚ğ‘€ğ‘ƒ, ğ‘ƒğ¶ and ğ‘ƒğ‘† are measured in SRAM cell write power 
consumption units.   
Leakage power can be expressed as follows: 
   
ğ‘ƒğ¿ğ¸ğ´ğ¾ = Î²Ağ‘‰
ğ›¼ = Î³A (8) 
 
where ğ´ is the area, ğ‘‰ is the supply voltage, Î± and Î² are 
constants, and Î³ is the leakage area coefficient that de-
pends on silicon process and operating conditions. There-
fore the total power can be written as follows: 
 
ğ‘ƒğ‘†ğ¼ğ‘€ğ· =
ğ‘ƒğ´ğ¿ğ‘ˆğ‘œğ‘š
2 + ğ‘ƒğ‘…ğ¹ğ‘œğ‘˜ğ‘š + ğ¼ğ¶ğ‘ƒğ¶ğ‘œğ‘š + ğ¼ğ‘ ğ‘ƒğ‘†ğ‘œğ‘š
1
ğ‘›ğ‘†ğ¼ğ‘€ğ·
+
ğ¼ğ‘
ğ‘›ğ‘†ğ¼ğ‘€ğ·
+ ğ¼ğ‘ 
+ 
+Î³ğ‘›ğ‘†ğ¼ğ‘€ğ·(ğ´ğ´ğ¿ğ‘ˆğ‘œğ‘š
2 + ğ´ğ‘…ğ¹ğ‘œğ‘˜ğ‘š) 
(9) 
 
   
4.2 Associative Processor  
In this section we construct the analytical model for the 
speedup, area and power consumption of the AP. The 
execution time of the parallelizable portion ğ‘“ of the pro-
gram on the AP can be written as follows: 
 
ğ‘‡ğ‘“,ğ´ğ‘ƒ =
ğ‘‡1
ğ‘ ğ´ğ‘ƒğ¸ğ‘›ğ´ğ‘ƒ
+
ğ‘‡C
ğ‘›ğ´ğ‘ƒ
+ ğ‘‡S(ğ‘ âˆ’ ğ‘›ğ´ğ‘ƒ) 
(10) 
 
 
where ğ‘›ğ´ğ‘ƒ is the number of PUs in the AP, ğ‘ is the data 
set size, ğ‘ ğ´ğ‘ƒğ¸ is the speedup of associative PU relative to 
the BPU, ğ‘‡C and ğ‘‡S are as defined in (2). Since AP in our 
research replaces the LLC, there is no need for data syn-
chronization unless the entire data set does not fit in the 
AP. In other words, ğ‘‡S(ğ‘ âˆ’ ğ‘›ğ´ğ‘ƒ) = 0 for ğ‘ â‰¤ ğ‘›ğ´ğ‘ƒ. 
Assuming single precision floating point arithmetic, 
the longest among frequently used arithmetic operations 
is multiplication, which in one direct implementation 
takes 4400 cycles vs. 1 cycle on the baseline sequential 
CPU or the BPU. Lacking a-priori knowledge of the work-
loads to be executed on the AP, we assume the worst case 
scenario comprising a continuous series of floating point 
multiplications. In this case ğ‘ ğ´ğ‘ƒğ¸ = 1/4400. The speedup 
of the AP can then be written as follows: 
 
ğ‘†ğ´ğ‘ƒ =
1
1
ğ‘ ğ´ğ‘ƒğ¸ğ‘›ğ´ğ‘ƒ
+
ğ¼ğ‘
ğ‘›ğ´ğ‘ƒ
+ ğ¼ğ‘ 
â€²
 
(11) 
 
where ğ¼ğ‘ 
â€² =ğ¼ğ‘ (ğ‘ âˆ’ ğ‘›ğ´ğ‘ƒ). The area of the AP can be written 
as follows: 
 
ğ´ğ´ğ‘ƒ = ğ‘›ğ´ğ‘ƒ(ğ´ğ´ğ‘ƒğ‘œğ‘˜ğ‘š + 2ğ´ğ´ğ¿ğ‘ˆğ‘œ) (12) 
 
where ğ‘˜ is the size of the associative PU (in data words), 
including temporary storage, ğ´ğ´ğ‘ƒğ‘œ is the AP cell area, 
measured in SRAM cell area units, and 2ğ´ğ´ğ¿ğ‘ˆğ‘œ is the per-
PU reduction tree size. Similarly to the reference SIMD 
coprocessor, we ignore the area of the interconnection 
network. 
The average power of the AP can be written as follows:   
 
ğ‘ƒğ´ğ‘ƒ =
ğ¸ğ¶ğ‘‚ğ‘€ğ‘ƒ + ğ¸ğ¶ + ğ¸S(ğ‘ âˆ’ ğ‘›ğ´ğ‘ƒ)+ğ¸ğ¿ğ¸ğ´ğ¾
ğ‘‡ğ‘“,ğ´ğ‘ƒ
= 
=
ğ‘ƒğ¶ğ‘‚ğ‘€ğ‘ƒ
ğ‘›ğ´ğ‘ƒ
+
ğ¼ğ‘ğ‘ƒğ¶
ğ‘›ğ´ğ‘ƒ
+ ğ¼ğ‘ 
â€²ğ‘ƒğ‘†
1
ğ‘›ğ´ğ‘ƒ
+
ğ¼ğ‘
ğ‘›ğ´ğ‘ƒ
+ ğ¼ğ‘ 
â€²
+ Î³ğ‘›ğ´ğ‘ƒğ´ğ´ğ‘ƒğ‘œğ‘˜ğ‘š 
 
ğ‘ƒğ¶ = ğ‘›ğ´ğ‘ƒğ‘ƒğ¶ğ‘œ (13) 
ğ‘ƒğ‘† = ğ‘ƒğ‘†ğ‘œğ‘š  
 
where ğ¸ğ¶ğ‘‚ğ‘€ğ‘ƒ and ğ‘ƒğ¶ğ‘‚ğ‘€ğ‘ƒ are the AP computation energy 
and power consumption; ğ¸ğ¶ and ğ‘ƒğ¶ are the AP energy 
and power consumption during inter-PU communica-
tion; ğ¸ğ‘† and ğ‘ƒğ‘† are the energy and the average power con-
sumed during synchronization, if the entire data set does 
not fit in the AP; ğ¸ğ¿ğ¸ğ´ğ¾ and ğ‘ƒğ¿ğ¸ğ´ğ¾ are the AP leakage ener-
gy and power. ğ‘ƒğ¶ğ‘œ is the per-bit power consumption dur-
ing the inter-PU communication; ğ‘ƒğ‘†ğ‘œ is the per-bit power 
during synchronization. Note that for comparison pur-
poses we use the same leakage power (represented as a 
function of area only as in (8)) for both the AP and the 
SIMD processor. This might be somewhat unfair to the 
 
 
AP: First, the leakage power per area could be lower for 
memory than for logic [34]. Second, the AP has fewer 
hotspots [48]. Since the leakage power is highly tempera-
ture dependent, hotspots may lead to higher leakage in 
the SIMD processor [5].  
In order to further detail ğ‘ƒğ¶ğ‘‚ğ‘€ğ‘ƒ, recall the implementa-
tion of single-bit addition (on which other arithmetic op-
erations are based) described in Section 3.2. In each pass 
of the single-bit addition, a three bit input combination 
ğ‘[âˆ—]ğ‘– , ğ‘[âˆ—]ğ‘– , ğ‘[âˆ—] is compared in parallel in all PUs and af-
terwards a two bit result ğ‘[âˆ—]ğ‘– , ğ‘[âˆ—] is written into the 
tagged PUs; that sequence is repeated m times for m-bit 
words. Since there are eight independent logic combina-
tions (TABLE 1), each PU has 1/8 probability of match 
and 7/8 of mismatch (in which case the Match line dis-
charges). Similarly, each PU has 1/8 probability of write 
and 7/8 probability of a miswrite (when Bit and Bit-not 
lines are charged without Word line being asserted). Since 
we define the power consumption of a single SRAM cell 
during write operation as 1, ğ‘ƒğ¶ğ‘‚ğ‘€ğ‘ƒ can be presented as:   
 
ğ‘ƒğ¶ğ‘‚ğ‘€ğ‘ƒ =   
2 âˆ™ (1 8â„ +
7
8â„ âˆ™ ğ‘ğ‘šğ‘¤) + 3 âˆ™ (
1
8â„ âˆ™ ğ‘ğ‘š +
7
8â„ âˆ™ ğ‘ğ‘šğ‘š)
2
ğ‘›ğ´ğ‘ƒ 
(14) 
 
 
for 2-bit write and 3-bit compare operations, where ğ‘ğ‘šğ‘¤ is 
the normalized per-bit power consumption of a miswrite, 
ğ‘ğ‘šğ‘š is the normalized per-bit power consumption of a 
mismatch, and ğ‘ğ‘š is the normalized per-bit power con-
sumption of a match (TABLE 3).  
Model (13) is fairly basic and does not account for cer-
tain statistics that work in favor of the AP. For example, a 
certain percentage of associative memory cells that are 
written a new value in fact do not change (consuming 
considerably less power); similarly, a certain percentage 
of asserted bit lines do not recharge (or discharge) since 
the same value is asserted. Our goal is to create a simple 
power model that reflects the worst case power consump-
tion of the AP. 
4.3 Modeling under constrained area 
The number of AP PUs may be derived as the function 
of the constrained area budget ğ´ using (12) as follows: 
 
ğ‘›ğ´ğ‘ƒ =
ğ´
ğ´ğ´ğ‘ƒğ‘œğ‘˜ğ‘š + 2ğ´ğ´ğ¿ğ‘ˆğ‘œ
 
 
(15) 
We can further substitute ğ‘›ğ´ğ‘ƒ in (11) and (13) by (15) and 
obtain the speedup and the power consumption of the AP 
as function of the area budget. The area parameters we 
use for modeling purposes are presented in TABLE 2. 
Speedup vs. area for the reference SIMD coprocessor 
and the AP is shown in Fig. 7. For mathematical simplici-
ty, synchronization intensity ğ¼ğ‘  is assumed to be constant 
0.01 (namely, synchronization takes 1% of the serial exe-
cution time).  
TABLE 2 
AREA MODEL PARAMETERS 
Parameter Description Attributed 
to 
Value 
ğ´ğ´ğ¿ğ‘ˆğ‘œ ALU bit cell area  SIMD 20 
(1) 
ğ´ğ‘…ğ¹ğ‘œ Register bit (FF) area  SIMD 3 
(1) 
ğ‘†ğ´ğ‘ƒğ¸ AP speedup relative to se-
quential CPU   
AP 1/4400 
ğ´ğ´ğ‘ƒğ‘œ AP bit area  AP 2 
(1) 
ğ‘š Data wordlength  Both 32 
ğ‘˜ Register file size in SIMD, 
AP PU size (in 32-bit words)  
Both 8 
(1) Area parameters are relative to the area of SRAM bit cell; the values 
are based on typical standard cell libraries. 
 
As the area budget increases, the speedup of the refer-
ence SIMD coprocessor exhibits diminishing returns 
caused by the LLC-to-SIMD synchronization. Eventually 
the speedup saturates:  
 
lim
ğ‘›ğ‘†ğ¼ğ‘€ğ·â†’âˆ
ğ‘†ğ‘†ğ¼ğ‘€ğ· = lim
ğ‘›ğ‘†ğ¼ğ‘€ğ·â†’âˆ
1
1
ğ‘›ğ‘†ğ¼ğ‘€ğ·
+
ğ¼ğ¶
ğ‘›ğ‘†ğ¼ğ‘€ğ·
+ ğ¼ğ‘ 
=
1
ğ¼ğ‘ 
 
(16) 
 
As evident from Fig. 7, the speedup of the AP is lower 
than the speedup of the reference SIMD coprocessor at 
low area, but it increases to reach the breakeven point at 
around 30ğ‘šğ‘š2. Diminishing returns affect the AP 
speedup to a lesser extent, since they only occur when the 
data set does not fit into the AP. To demonstrate this ef-
fect, we assume that the data set size grows with the AP 
size (same as for SIMD) until ğ‘›ğ´ğ‘ƒ = ğ‘ = 10
4, after which 
the data set size ğ‘ grows twice as fast as the ğ‘›ğ´ğ‘ƒ. This is 
what causes the AP speedup to eventually saturate as 
well.   
 
(c)
(a)
(d)
(b)
 
Fig. 7. Analytical results under constrained area: (a) Speedup (b) Power (c) 
Performance / Power ratio (d) ğ¸ğ‘›ğ‘’ğ‘Ÿğ‘”ğ‘¦ Ã— ğ·ğ‘’ğ‘™ğ‘ğ‘¦  
 
The power consumption vs. area budget for the SIMD 
and the APs is shown in Fig. 7(b). The power consump-
tion of the AP is lower than that of the SIMD processor 
when area is under 30ğ‘šğ‘š2. For larger area, the AP con-
sumes more power than the SIMD processor. Note that 
 
 
even when the speedups saturate, power consumption 
continues to grow with area, due to the leakage.  
The performance/power ratio vs. area for the SIMD 
and the APs is shown in Fig. 7(c). For the SIMD processor, 
the performance/power ratio drops because speedup 
saturates while power dissipation continues to grow with 
increasing area. For lower area, AP underperforms SIMD 
in terms of performance/power ratio. But as SIMDâ€™s 
speedup saturates, AP yields better performance/power 
ratio. Eventually, the latter also drops, as the AP speedup 
saturates. 
The energy-delay product (ğ¸ğ·) vs. area for SIMD and 
AP is shown in Fig. 7(d). Since the task size is not con-
stant, we use normalized delay, i.e. the ratio of the delay 
to the serial execution time. The SIMD processorâ€™s ğ¸ğ· 
reaches a minimum around 5ğ‘šğ‘š2 (where the speedup 
saturates) and begins to grow again due to growing pow-
er consumption. The APâ€™s ğ¸ğ· follows a similar pattern 
but reaches its minimum at much larger area budget. 
Both performance/power and ğ¸ğ· product breakeven 
points (at which AP begins to outperform SIMD) occur at 
20ğ‘šğ‘š2. 
4.4 AP Simulation 
The purpose of the simulation is to validate the analyt-
ic results obtained in Section 4.3. The workloads are de-
fined, followed by description of the cycle-accurate AP 
simulator, our simulation methodology and simulation 
results.  
4.4.1 Workloads 
The following workloads have been selected for per-
formance and power consumption simulations: 
ï‚· ğ‘-option pairs Black-Scholes option pricing (BSC) 
ï‚· ğ‘-point Fast Fourier Transform (FFT) 
ï‚· Dense Matrix Multiplication  of two âˆšğ‘Ã—âˆšğ‘ ma-
trices (DMM) 
ï‚· ğ‘-point Vector Reduction (VR) 
where ğ‘ is the data set size, for simplicity scaled to the 
processor size (following the methodology suggested 
in  [17]), i.e. ğ‘ = ğ‘›ğ´ğ‘ƒ. Note that simulations do not cover 
the cases where the data size exceeds the size of the pro-
cessor (requiring data synchronization).    
 
4.4.2 Simulator 
We simulate the AP using an in-house cycle-accurate 
simulator. The workloads are hand-coded. For FFT, we 
use optimized parallel implementation outlined in [38]. 
For Black-Scholes, we used a direct implementation opti-
mized for associative processing, based on formulation 
in [7]. Vector reduction is implemented using the reduc-
tion tree. Matrix multiplication uses APâ€™s compare and 
arithmetic capabilities to match the input matrix element 
pairs and multiply them. The singleton products are 
summed by the reduction tree.    
The first step of AP programing is identification of the 
finest data parallelization level and mapping of the work-
loads on the associative processing array. For matrix mul-
tiplication, each pair of elements to be multiplied is pro-
cessed by a single PU. For FFT, each multiply-accumulate 
operation is carried out by a single PU. For Black-Scholes 
option pricing, a single PU handles a single call option of 
a single security at a single strike price and a single expi-
ration time. For vector reduction, a single PU retains a 
single vector element. At the next step, we break each 
fine-grain data thread into a series of arithmetic and data 
communication operations, and manually allocate tempo-
rary storage. At the last step, each arithmetic and com-
munication operation is converted into a series of com-
pares, writes and data moves. Simulation times are pre-
sented in TABLE 4.  
For power simulation, we follow the methodology of 
SimpleScalar [10], which allows keeping track of what 
units are active during execution and records the total 
energy consumed for a workload. During the AP execu-
tion, we record and count all baseline operations (match, 
mismatch, write, miswrite, data move, reduction). Using 
power models of each baseline operation, detailed in TA-
BLE 3, we are able to estimate the total energy consumed 
during execution of each case.  
 
TABLE 3 
POWER MODEL PARAMETERS 
Parameter Description Value 
ğ‘ğ‘›ğ‘¤ per-bit power consumption 
during a miswrite  
0.1 (1) 
ğ‘ğ‘š per-bit power consumption 
during a match 
0.1 (1) 
ğ‘ğ‘šğ‘š per-bit power consumption 
during a mismatch 
0.75 (1) 
Î³ static power coefficient 5 âˆ™ 10âˆ’2 W/mm2(2) 
(1) Based on [22], relative to the power consumption of SRAM bit cell 
during write from 0 to 1 or from 1 to 0 operation 
(2) Based on typical industry data at typical conditions for advanced 
technologies 
 
TABLE 4 
DATA SET SIZES AND SIMULATION TIMES 
Workload Date Set Size Simulation Time 
BSC 28 Ã· 220 4 sec Ã·  1hr 50min 
FFT 28 Ã· 220 3 sec Ã·  2hr 35min 
DMM 28 Ã· 220 2 sec Ã· 12hr 55min 
VR 28 Ã· 220 2 sec Ã·  6 sec 
Simulations performed on IntelÂ® Coreâ„¢2 Quad CPU Q8400 with 8GB 
RAM 
4.4.3 Results 
We simulate speedup and power per workload for 16 
different values of area. In all cases, the PU size is 256 bits 
(TABLE 2). 
Simulated speedup results are presented in Fig. 8(a). 
DMM uses the reduction tree as an accelerator. BSC is an 
embarrassingly parallel workload. Hence DMM and BSC 
obtain higher speedup than FFT. VR is an outlier, since it 
is implemented using the word- and bit-parallel reduc-
tion tree rather than bit-serial associative arithmetic, thus 
achieving considerably higher speedup.    
Power consumption results are presented in Fig. 8(b). 
All workloads consume power of the same order of mag-
nitude (hence we use linear rather than log-log scale).  
 
 
This happens because all workloads are implemented 
using mostly identical associative primitives (compare 
and write). Although VR and to a lesser extent DMM use 
the relatively power-hungry reduction tree, reduction 
time is almost negligible compared to the time of associa-
tive operations. Performance/power ratio and ğ¸ğ· prod-
uct are shown in Fig. 8(c) and (d) respectively. Among 
DMM, FFT and BSC workloads, DMM shows the best 
performance/power and ğ¸ğ·, thanks to the accelerated 
reduction operation. Since BSC is an embarrassingly par-
allel workload, its performance/power ratio remains al-
most constant with data set size / area. The power con-
sumption of VR is significantly higher than that of the rest 
of the workloads. However since its speedup is also at 
least an order of magnitude higher, VR exhibits consider-
ably better performance/power ratio and ğ¸ğ· product.   
 
 
Fig. 8. Simulation results: (a) Speedup (b) Power (c) Performance / Power 
ratio (d) ğ¸ğ‘›ğ‘’ğ‘Ÿğ‘”ğ‘¦ Ã— ğ·ğ‘’ğ‘™ğ‘ğ‘¦ 
 
 
Fig. 9. Speedup Breakeven Points Distribution 
4.5 Sensitivity to parameter variation 
The parameters used in our modeling are technology 
and design dependent. In order to determine how the 
changes in these parameters affect the results, we ran-
domize the parameters in TABLE 2 using uniform distri-
bution of âˆ“50%.  
Fig. 9 shows the distribution of the speedup breakeven 
point (i.e. the area at which the speedup of the SIMD pro-
cessor is the same as that of the associative processor,  
~30ğ‘šğ‘š2 in Fig. 7). As expected, the distribution of 
speedup breakeven point is close to lognormal (because 
at least some of the independent random parameters are 
positive and multiplicative), with a mean value of 
~32ğ‘šğ‘š2. 
5 CPU WITH AP VS. CPU WITH LLC AND SIMD 
COPROCESSOR 
While in the previous section the AP has been com-
pared with a standalone SIMD processor, in this section 
they are considered in the context of a CPU architecture.  
Complexity and runtime requirements make it chal-
lenging to rely on cycle-accurate simulation for the large-
scale design space exploration that we undertake.  We use 
analytical modeling to compare the performance and the 
power consumption of a CPU with an AP (Fig. 1(c)) vs. 
CPU with a LLC and a SIMD coprocessor (Fig. 1(b)), un-
der constrained area resource. In this analysis we assume 
that the areas of the CPU and the L1 cache are constant. 
The variable area budget is therefore assigned entirely to 
the LLC in the baseline architecture (Fig. 1(a)), or divided 
among the LLC and the SIMD coprocessor (Fig. 1(b)), or 
assigned entirely to the AP (Fig. 1(c)).  We begin our 
comparative analysis with performance and follow with 
power consumption.  
We confirm our analytic results by emulation, whereby 
the workload is executed and performance is measured 
on a state-of-art computer system with SIMD accelerator, 
as explained in Section 5.3. Emulation results are also 
combined with the results of cycle-accurate simulations of 
AP, to derive the performance of CPU with the AP.  
5.1 Performance Modeling 
Following [11] and [31], we can present the execution 
time of a workload on the baseline CPU architecture (Fig. 
1(a)) as a function of its LLC size ğ´ğ¿ğ¿ğ¶ = ğ´ as follows: 
 
ğ‘‡1(ğ´ğ¿ğ¿ğ¶) = ğ‘€[ğ‘” âˆ™ ğ¶ğ‘ƒğ¼ğ‘€ğ¸ğ‘€ + (1 âˆ’ ğ‘”) âˆ™ ğ¶ğ‘ƒğ¼ğ¶ğ‘ƒğ‘ˆ] (17) 
 
where ğ´ is the area budget, ğ‘€ is the number of instruc-
tions in the workload, ğ‘” is the fraction of memory access 
instructions, ğ¶ğ‘ƒğ¼ğ¶ğ‘ƒğ‘ˆ is the average number of cycles per 
instruction for instructions that require no memory access 
(assumed to be constant, as defined in TABLE 5), and 
ğ¶ğ‘ƒğ¼ğ‘€ğ¸ğ‘€ is the average number of cycles per memory ac-
cess. ğ¶ğ‘ƒğ¼ğ‘€ğ¸ğ‘€ can in turn be presented as follows [49]: 
 
ğ¶ğ‘ƒğ¼ğ‘€ğ¸ğ‘€ = (1 âˆ’ ğ‘š1)ğ‘‘ğ¿1 + ğ‘š1(1 âˆ’ ğ‘š2)ğ‘‘ğ¿ğ¿ğ¶
+ ğ‘š1ğ‘š2ğ‘‘ğ· 
(18) 
 
where ğ‘š1 and ğ‘š2 are miss rates of L1 and LLC respective-
ly, ğ‘‘ğ¿1 and ğ‘‘ğ¿ğ¿ğ¶ are access times of L1 and LLC respective-
ly, and ğ‘‘ğ· is the off-chip DRAM access time.  
The miss rate of the LLC can be written as follows [21]: 
 
ğ‘š2 = ğ‘š1âˆš
ğ´ğ¿1
ğ´ğ¿ğ¿ğ¶
â„  (19) 
 
where ğ´ğ¿1 and ğ´ğ¿ğ¿ğ¶ = ğ´ are the areas of the L1 and the 
 
 
LLC respectively.  
The execution time of the same workload on the CPU 
with the LLC and the SIMD coprocessor can be written as 
follows: 
 
ğ‘‡2 = (1 âˆ’ ğ‘“) âˆ™ ğ‘‡1(ğ´ğ¿ğ¿ğ¶ğ‘œ) +
ğ‘“ âˆ™ ğ‘€
ğ‘ ğ‘†ğ¼ğ‘€ğ·(ğ´ğ‘†ğ¼ğ‘€ğ·ğ‘œ)
 (20) 
 
where ğ‘“ is the parallelizable portion of the program and 
ğ‘ ğ‘†ğ¼ğ‘€ğ· is the speedup of the SIMD coprocessor as defined 
in (3); ğ´ğ¿ğ¿ğ¶ğ‘œ and ğ´ğ‘†ğ¼ğ‘€ğ·ğ‘œ are the areas of the LLC and the 
SIMD coprocessor, respectively, so that ğ´ğ¿ğ¿ğ¶ğ‘œ + ğ´ğ‘†ğ¼ğ‘€ğ·ğ‘œ =
ğ´. The parallelizable portion of the workload is assumed 
to contain single-cycle instructions, similarly to Section 4. 
The execution time of the same workload on the CPU 
with the AP can be written as: 
 
ğ‘‡3 = (1 âˆ’ ğ‘“) âˆ™ ğ‘‡1(ğ´ ğ´ğ´ğ‘ƒğ‘œâ„ ) +
ğ‘“ âˆ™ ğ‘€
ğ‘†ğ´ğ‘ƒ
 (21) 
 
where ğ‘†ğ´ğ‘ƒ is the speedup of the AP as defined in (11); 
ğ´ ğ´ğ´ğ‘ƒğ‘œâ„  is the effective area of the LLC implemented by 
the AP (operated in the conventional cache mode during 
the execution of the serial fraction of the workload), 
where ğ´ğ´ğ‘ƒğ‘œ (the area of the AP cell in SRAM cell units) is 
2, as defined in TABLE 2.  
Following (20) and (21), the effective number of cycles 
per instruction for the CPU with the LLC and the SIMD 
coprocessor (Fig. 1(b)) and for the CPU with the AP (Fig. 
1(c)) can be written as follows: 
 
ğ¶ğ‘ƒğ¼ğ‘†ğ¼ğ‘€ğ· =
ğ‘‡2
ğ‘€
;        ğ¶ğ‘ƒğ¼ğ´ğ‘ƒ =
ğ‘‡3
ğ‘€
 (22) 
 
We further define the overall speedup of these two ar-
chitectures as follows: 
 
ğ‘†ğ‘ˆğ‘†ğ¼ğ‘€ğ· =
ğ‘‡1(ğ´)
ğ‘‡2
;             ğ‘†ğ‘ˆğ´ğ‘ƒ =
 ğ‘‡1(ğ´)
ğ‘‡3
 (23) 
The timing and area parameters used for modeling 
purposes are specified in TABLE 2 and TABLE 5. 
Fig. 10 shows the effective number of cycles per in-
struction vs. area budget for both architectures for ğ‘“ =
0.75, 0.9, 0.99 and 0.999. Fig. 11 shows the overall speedup 
of these architectures for the same ğ‘“.  
In Section 4 we established that if area budget and data 
set size are sufficiently large, the AP may outperform the 
SIMD coprocessor. This outcome is supported by our 
findings here. For high ğ‘“ (e.g., 0.9 and above), the effec-
tive CPI and overall speedup breakeven points occur at 
relatively low area budget, and the overall speedup is 
relatively high. For lower ğ‘“ (e.g., 0.75 and below), the da-
ta set size and the area budget required for the AP to out-
perform the SIMD coprocessor are considerably more 
significant. 
 
TABLE 5 
TIMING AND AREA MODEL PARAMETERS 
Parameter Description Value 
ğ‘” fraction of memory access 
instructions 
0.2(3) 
ğ¶ğ‘ƒğ¼ğ¶ğ‘ƒğ‘ˆ average number of cycles 
per instruction for instruc-
tions with no memory ac-
cess 
1(3) 
ğ´ğ¶ğ‘ƒğ‘ˆ CPU area 10
8 (1) (3) 
ğ´ğ¿1 L1 cache area 10
8 (1) (3) 
ğ‘‘ğ¿1 L1 cache access time 1 
(2) (3) 
ğ‘‘ğ¿ğ¿ğ¶ LLC access time 5 
(2) (3) 
ğ‘‘ğ· DRAM access time 100 
(2) (3) 
ğ‘š1 L1 cache miss rate 0.05
(3) 
(1) Area parameters are relative to the area of SRAM bit cell 
(2) Timing parameters are in cycles 
(3) Values based on typical industry data in advanced technologies 
5.2 Power Modeling 
The power consumption of the baseline architecture in 
Fig. 1(a) can be presented as a function of its LLC size 
ğ´ğ¿ğ¿ğ¶ = ğ´ based on [18]:  
 
ğ‘ƒ1(ğ´ğ¿ğ¿ğ¶) = ğ‘” âˆ™ ğ‘ƒğ‘€ğ¸ğ‘€ + (1 âˆ’ ğ‘”) âˆ™ ğ‘ƒğ¶ğ‘ƒğ‘ˆ + ğ‘ƒğ¿ğ¸ğ´ğ¾ (24) 
 
where ğ‘ƒğ‘€ğ¸ğ‘€ can be written as follows: 
 
ğ‘ƒğ‘€ğ¸ğ‘€ = (1 âˆ’ ğ‘š1)ğ‘ƒğ¿1 + ğ‘š1(1 âˆ’ ğ‘š2)ğ‘ƒğ¿ğ¿ğ¶ + ğ‘š1ğ‘š2ğ‘ƒğ· (25) 
 
where ğ‘ƒğ¿1, ğ‘ƒğ¿ğ¿ğ¶  and ğ‘ƒğ· are the power consumption of L1 
cache, LLC, and off-chip DRAM access, respectively. ğ‘ƒğ¿1 
is assumed to be constant and defined in TABLE 6 below; 
ğ‘ƒğ¿ğ¿ğ¶  to ğ‘ƒğ¿1 ratio equals the square root of the LLC to L1 
areas ratio, while ğ‘ƒğ¶ğ‘ƒğ‘ˆ is proportional to the CPU ar-
ea [11]: 
ğ‘ƒğ¿ğ¿ğ¶ = ğ‘ƒğ¿1 âˆ™ âˆš
ğ´ğ¿ğ¿ğ¶
ğ´ğ¿1
â„ ;              ğ‘ƒğ¶ğ‘ƒğ‘ˆ = ğ‘ƒğ¶ğ‘ƒğ‘ˆğ‘œ âˆ™ ğ´ğ¶ğ‘ƒğ‘ˆ (26) 
 
where ğ‘ƒğ¶ğ‘ƒğ‘ˆğ‘œ is the power consumption of the baseline 
CPU, assumed to be constant and defined in TABLE 6 
below.   
The power consumption of the CPU with the LLC and 
the SIMD coprocessor can be written as: 
 
ğ‘ƒ2 = (1 âˆ’ ğ‘“) âˆ™ ğ‘ƒ1(ğ´ğ¿ğ¿ğ¶ğ‘œ) + ğ‘“ âˆ™ ğ‘ƒğ‘†ğ¼ğ‘€ğ·(ğ´ğ‘†ğ¼ğ‘€ğ·ğ‘œ) + ğ‘ƒğ¿ğ¸ğ´ğ¾ (27) 
 
where ğ‘ƒğ‘†ğ¼ğ‘€ğ· is the power dissipation of the SIMD co-
processor, which is the dynamic component of (6) above. 
The leakage power ğ‘ƒğ¿ğ¸ğ´ğ¾ is defined in (8) above (with ğ´ 
being the sum of ğ´ğ¶ğ‘ƒğ‘ˆ , ğ´ğ¿1, ğ´ğ‘†ğ¼ğ‘€ğ· and ğ´ğ¿ğ¿ğ¶).  
The power consumption of the CPU with the AP can 
similarly be written as follows: 
 
ğ‘ƒ3 = (1 âˆ’ ğ‘“) âˆ™ ğ‘ƒ1(ğ´ ğ´ğ´ğ‘ƒğ‘œâ„ ) + ğ‘“ âˆ™ ğ‘ƒğ´ğ‘ƒ + ğ‘ƒğ¿ğ¸ğ´ğ¾ (28) 
 
where ğ‘ƒğ´ğ‘ƒ is the power dissipation of the AP, as de-
fined in (13) above. The leakage power ğ‘ƒğ¿ğ¸ğ´ğ¾ is defined in 
(8) above (with ğ´ being the sum of ğ´ğ¶ğ‘ƒğ‘ˆ , ğ´ğ¿1 and ğ´ğ´ğ‘ƒ). The 
power parameters used for modeling are presented in 
TABLE 3 and TABLE 6. 
 
 
 
 
Fig. 10. Effective ğ¶ğ‘ƒğ¼ğ‘†ğ¼ğ‘€ğ· and ğ¶ğ‘ƒğ¼ğ´ğ‘ƒ vs. Area  
 
 
Fig. 11. ğ‘†ğ‘ˆğ‘†ğ¼ğ‘€ğ· and ğ‘†ğ‘ˆğ´ğ‘ƒ vs. Area  
 
TABLE 6 
POWER MODEL PARAMETERS 
Parameter Description Value 
ğ‘ƒğ¶ğ‘ƒğ‘ˆğ‘œ Baseline CPU power 5 âˆ™ 10
âˆ’3 (1) (3) 
ğ‘ƒğ¿1 L1 power  5 âˆ™ 10
4 (2) (3) 
ğ‘ƒğ· Power of off-chip 
DRAM access 
103 (2) (3) 
(1) Power of SRAM bit cell unit over area of SRAM bit cell unit 
(2) Relative to the power consumption of SRAM bit cell during write 
(3) Values based on typical industry data in advanced technologies 
 
The power consumption, the performance/power ratio 
and the ğ¸ğ· product of the CPU with the LLC and the 
SIMD coprocessor vs. the CPU with the AP for ğ‘“ =
0.75, 0.9, 0.99 and 0.999 are shown in Fig. 12, Fig. 13 and 
Fig. 14, respectively. Similarly to speedup, the results for 
higher ğ‘“ are in line with the findings of Section 4.  
Note the significant difference in behavior of the per-
formance/power ratio and the ğ¸ğ· product for higher 
values of ğ‘“. While at lower ğ‘“ the AP consumes a large 
portion of the overall power while making small contri-
bution to the overall speedup, for higher ğ‘“ the AP adds to 
the overall speedup quite significantly. Consequently, for 
f=0.999, the CPU with the AP charts exhibit a different 
trend vs. the CPU with the LLC and the SIMD curves.  
Fig. 15 shows the effective ğ¶ğ‘ƒğ¼ğ´ğ‘ƒ vs. ğ¶ğ‘ƒğ¼ğ‘†ğ¼ğ‘€ğ· breakeven 
point, namely the area above which the CPU with the AP 
outperforms the CPU with the LLC and the SIMD, as a 
function of ğ‘“ and ğ‘”. The plateau marks the region of ğ‘“ 
and ğ‘” values for which a breakeven point cannot be 
achieved under the maximum area budget used in our 
analysis (8ğ´ğ¿1). In other words, the CPU with the AP does 
not have enough area to outperform the CPU with the 
LLC and the SIMD coprocessor. However, the SIMD pro-
cessor is also less than useful in the plateau region: it is 
well established that conventional SIMD accelerators are 
inefficient in implementing low parallelizable / low 
arithmetic intensity (the ratio of computations to memory 
traffic [25] )workloads, characteristic of low ğ‘“ and high ğ‘” 
(the plateau region) [30] [32].    
 
 
Fig. 12. Power vs. Area  
 
On the other hand, the combination of ğ‘“ close to 1 and 
low ğ‘”, which is typical for workloads with high level of 
fine-grain data parallelism, is advantageous for the CPU 
with the AP architecture, allowing it to outperform the 
SIMD accelerated architecture over a wide span of the 
area budget.    
5.3 Emulation Methodology 
We validate our analytic modeling findings using 
emulation. We follow the methodology established 
in [31]. To emulate the CPU with the LLC and the SIMD 
coprocessor of Fig. 1(b), we use a stand-alone server fea-
turing Intel Â® Pentium Â® 4 processor with the SSE2 SIMD 
accelerator, operated under Fedora Linux. We evaluate 
the performance using a dense matrix multiplication ker-
nel since it has the highest arithmetic intensity among the 
workloads considered in Section 4.4.1. With arithmetic 
intensity of ğ‘‚(âˆšğ‘) (where ğ‘ is the data set size and âˆšğ‘ is 
the matrix dimension), matrix multiplication is a better fit 
for SIMD implementation than FFT (with arithmetic in-
tensity of ğ‘‚(log2 ğ‘) and Black-Scholes (with arithmetic 
intensity of ğ‘‚(1)). 
 
 
 
 
Fig. 13. Performance / Power vs. Area  
 
 
Fig. 14. ğ¸ğ· vs. Area  
 
 
Fig. 15. Effective CPI breakeven point vs. ğ‘“ and ğ‘” 
  
Synchronization in SSE2 SIMD accelerator has two 
main components: MOV instructions that synchronize 
SSE2 registers with L1 data cache, and L1 to L2 synchro-
nization when the entire data set does not fit into L1. To 
unwind the dependency of execution time on the cache 
size and hierarchy (which are constantly improving in 
newer CPU revisions), we deduct the data cache miss 
penalty time from the execution time.  
 
 
Fig. 16. Speedup vs. Data Set Size: (a) emulated CPU + LLC + SIMD, (b) 
emulated CPU + LLC + SIMD vs. emulated CPU + AP  
 
The results of the CPU with the LLC and the SIMD ac-
celeratorâ€™ speedup (over naÃ¯ve serial execution that takes 
ğ‘‚(ğ‘1.5) cycles) vs. matrix dimension are presented in Fig. 
16(a). They are consistent with findings of D. Aberdeen et 
al. [2] who researched matrix multiplication using Intelâ€™s 
SSE instruction set. 
The analytical model of the CPU with the AP is veri-
fied using a combination of the cycle accurate simulation 
of standalone AP (Section 4.4) and the baseline CPU emu-
lation. In this approach, the sequential fraction of the 
workload is emulated on the CPU, followed by the cycle-
accurate simulation of the parallel fraction of the work-
load on the AP simulator. The sum of emulated and simu-
lated run times gives us total execution time. The results 
of the CPU with AP speedup (over naÃ¯ve serial execution) 
vs. matrix dimension are presented in Fig. 16(b), along 
with the speedup of the CPU with the LLC and the SIMD 
accelerator taken from Fig. 16(a). These results are quite 
expected, since the latter is limited by the SSE2 size, while 
the AP scales up with the data set size.  
To estimate how the comparison would change if we 
had the ability to enlarge the vector accelerator, we sepa-
rate the MOV instructions (analogous to ğ‘‡ğ‘† of (2)) from 
the rest of the code (which is analogous to ğ‘‡1 + ğ‘‡ğ¶ of (2)), 
and scale the latter by ğ‘›ğ‘†ğ¼ğ‘€ğ· ğ‘›ğ‘†ğ‘†ğ¸2â„ , where ğ‘›ğ‘†ğ¼ğ‘€ğ· is the size 
of the hypothetical SIMD accelerator and ğ‘›ğ‘†ğ‘†ğ¸2 = 4 is the 
size of SSE2. We then sum up the ğ‘‡ğ‘† and the ideally-
scaled ğ‘‡1 + ğ‘‡ğ¶ , to estimate SIMD execution time. The 
speedup vs. area results are presented in Fig. 17. Area is 
received by substituting ğ‘›ğ‘†ğ¼ğ‘€ğ· into (4). The hypothetical 
speedup of the CPU with the LLC and the SIMD accelera-
tor grows with area (with ğ‘›ğ‘†ğ¼ğ‘€ğ·), but it is still affected by 
synchronization (MOV part of the code) and therefore 
eventually underperforms the CPU with AP, in line with 
our analytical modeling findings.    
6 DISCUSSION AND CONCLUSIONS 
An associate processor is essentially a large memory 
with massively-parallel processing capability. It offers 
dual use: either the CPU accesses the data in that 
memory, or the data is being processed associatively 
within the same memory. This paper investigates the 
merit of using AP instead of on-chip last level cache 
(LLC) combined with a SIMD accelerator.  
 
 
 
 
Fig. 17. Speedup vs. Area: (a) hypothetical CPU + LLC + SIMD, (b) hypo-
thetical CPU + LLC + SIMD vs. emulated CPU + AP   
 
Converting LLC into AP adds parallel processing ca-
pabilities to otherwise sequential architecture. The price 
of this conversion is the reduction (approximately halv-
ing) of the effective LLC size in its conventional sequen-
tial processing mode. However, our study shows that 
even for workloads with relatively low parallelism (ğ‘“ â‰¤
0.75), replacing the LLC by AP may lead to an overall 
speedup over the baseline CPU architecture. For ğ‘“ close 
to 1, such speedup can be quite significant.  
An alternative way of improving the performance of 
parallelizable workloads is to allocate some of the con-
strained area budget (originally assigned to the LLC) to a 
conventional SIMD coprocessor. This study shows that 
the speedup of SIMD coprocessor is ultimately limited by 
data synchronization between its private memory and the 
shared LLC. This effect becomes more significant as the 
data set size and the SIMD coprocessor size grow. 
The principle advantage of the AP is the unification of 
data storage and processing, which in most cases elimi-
nates the need for data synchronization with a higher 
level cache. The AP speedup grows faster with area than 
the speedup of the conventional SIMD processor. Conse-
quently, when the area budget (and the corresponding 
data set size) is sufficiently large, the AP may outperform 
the conventional SIMD coprocessor. The speedup breake-
ven point is in the area range of a few square millimeters 
to low tens of square millimeters depending on the work-
load, the feature size, the design specifics, etc. AP howev-
er is not universally efficient. While yielding high 
speedup when implementing fine-grain massively data-
parallel workloads (such as sparse linear algebra and ma-
chine learning algorithms), its efficiency is much lower 
under workloads with low data-level parallelism.   
As area budget grows beyond the speedup breakeven 
point, APâ€™s power is similar to that of SIMD coprocessor. 
However, AP seems to outperform the conventional 
SIMD in terms of performance/power ratio and energy-
delay product over a broad spectrum of area and power 
budget for workloads with high data-level parallelism.    
Associative processing has been known and extensive-
ly studied since the 1960s. Commercial associative pro-
cessing never quite took off, because only limited 
amounts of memory could be placed on a single die [33]. 
Due to data sets and tasks of limited size, a standalone 
bit- and word-parallel SIMD significantly outperformed 
APs. However, the progress in IT industry and semicon-
ductor technology in recent years opens the door for re-
considering APs: 
ï‚· The rise of big data pushes the computational re-
quirements to levels never seen before. The amounts 
of data to be processed simultaneously require a new 
parallel computing paradigm. This paper shows that 
APâ€™s performance and efficiency improves with data 
set size. 
ï‚· Power consumption, which used to be a secondary 
factor in the past, has become a principal limitation to 
integration and performance today. The AP is shown 
to achieve higher performance/power ratio and en-
ergy-delay product. 
ï‚· Off-chip memory bandwidth remains to be one of the 
main factors limiting performance and scalability of 
parallel architectures. Associative processing miti-
gates this limitation by intertwining computing with 
data storage.   
ï‚· In high performance dies, thermal density is becom-
ing the limit on total computation capabilities; associ-
ative processing leads to uniform power and thermal 
distribution over the chip area, avoiding hot spots 
and enabling higher power dissipation. 
Thanks to the memory integration and the feature scal-
ing enabled by current silicon technology on one side, 
and the rise of big data on the other, we are at the inflec-
tion point where AP may considerably outperform con-
ventional SIMD in both performance and power.  
ACKNOWLEDGMENT 
This research was partially funded by the Intel Collabora-
tive Research Institute for Computational Intelligence and 
by Hasso-Plattner-Institut. 
REFERENCES 
[1]  â€œThe IntelÂ® Xeon Phiâ„¢ Coprocessorâ€. Available at:  
http://www.intel.com/content/www/us/en/high-performance-
computing/high-performance-xeon-phi-coprocessor-brief.html 
[2] Aberdeen, D., J. Baxter. "Emmerald: a fast matrixâ€“matrix multiply using 
Intel's SSE instructions." Concurrency and Computation: Practice and Expe-
rience 13.2 (2001): 103-119. 
[3] AlmÃ¡si G. et al., "Dissecting Cyclops: A detailed analysis of a multi-
threaded architecture", ACM SIGARCH Computer Architecture 
News 31.1 (2003): 26-38. 
[4] Auth, C., et al. "A 22nm high performance and low-power CMOS 
technology featuring fully-depleted tri-gate transistors, self-aligned con-
tacts and high density MIM capacitors." VLSI Technology (VLSIT), 2012 
Symposium on. IEEE, 2012. 
[5] Banerjee K. et al., â€œA self-consistent junction temperature estimation 
methodology for nanometer scale ICs with implications for perfor-
mance and thermal management,â€ IEEE IEDM, 2003, pp. 887-890. 
[6] Binkert N., et al. "The gem5 simulator." ACM SIGARCH Computer 
Architecture News 39.2 (2011): 1-7. 
[7] Black F. and M. Scholes, â€œThe pricing of options and corporate liabili-
ties,â€ Journal of Political Economy, 81 (1973), pp. 637â€“654, 1973. 
[8] Borkar S.. â€œThousand Core Chips: A Technology Perspective,â€ Proc.  
ACM/IEEE 44th Design Automation Conf. (DAC), 2007, pp. 746-749. 
[9] Brockman J., et al. "A low cost, multithreaded processing-in-memory 
system", 31st international symposium on computer architecture, 2004. 
[10] Burger D., T. Austin. "The SimpleScalar tool set, version 2.0", ACM 
SIGARCH Computer Architecture News 25.3 (1997): 13-25. 
[11] Cassidy A. and A. Andreou, â€œBeyond Amdahl Law - An objective 
function that links performance gains to delay and energyâ€, IEEE 
Transactions on Computers, vol. 61, no. 8, pp. 1110-1126, Aug 2012. 
 
 
[12] Flatt H.., K. Kennedy â€œPerformance of Parallel Processors,â€ Parallel 
Computing, Vol. 12, No. 1, 1989, pp. 1-20. 
[13] Foster C., â€œContent Addressable Parallel Processorsâ€, Van Nostrand 
Reinhold Company, NY, 1976 
[14] Fung Y., â€œAssociative Processor Architecture - a Surveyâ€, ACM Com-
puting Surveys Journal (CSUR), Volume 9, Issue 1, March 1977, Pages 3 
â€“ 27 
[15] Gokhale M. et al., â€œProcessing In Memory: the Terasys Massively Paral-
lel PIM Array,â€ IEEE Computer, 1995, pp. 23-31 
[16] Gschwind M. et. al., â€œSynergistic processing in Cellâ€™s multicore architec-
tureâ€, IEEE Micro 26 (2), 2006, pp. 10â€“24 
[17] Gunther N., S. Subramanyam, S. Parvu, â€œA Methodology for Optimiz-
ing Multithreaded  System  Scalability  on  Multi - Coresâ€, 
http://arxiv.org/abs/1105.4301 
[18] Guz Z. et. al. â€œThreads vs. Caches: modeling the behavior of parallel 
workloadsâ€, 2010 IEEE International Conference on Computer Design 
(ICCD), Oct. 2010, Pages: 274-281. 
[19] Hall M. et al., "Mapping irregular applications to DIVA, a PIM-based 
data-intensive architectureâ€, ACM/IEEE conference on Supercompu-
ting, 1999. 
[20] Hardavellas N. et al., "Toward dark silicon in servers." IEEE Micro 31.4 
(2011): 6-15 
[21] Hartstein A. et. al., â€œOn the nature of cache miss behavior: is it square 
root of 2?â€, Journal of Instruction-Level Parallelism, 2008 
[22] Hentrich D. et al., "Performance evaluation of SRAM cells in 22nm 
predictive CMOS technology," IEEE International Conference on Elec-
tro/Information Technology, 2009. 
[23] Hill M.,  M.. Marty, â€œAmdahlâ€™s law in the multicore eraâ€, IEEE Com-
puter 41 (7) (July 2008) 33â€“38. 
[24] http://www.arm.com/products/processors/technologies/neon.php 
[25] S. Kamil, C. Chan, L. Oliker,, J. Shalf, S. Williams, â€œAn Auto-Tuning 
Framework for Parallel Multicore Stencil Computationsâ€, IEEE Interna-
tional Symposium on Parallel & Distributed Processing (IPDPS)  2010, 
pages 1-12. 
[26] Kogge P. et al., "PIM architectures to support petaflops level computa-
tion in the HTMT machine", International Workshop on Innovative Ar-
chitecture for Future Generation High-Performance Processors and 
Systems, 2000. 
[27] Li H. et al. â€œAn AND-type match line scheme for high-performance 
energy-efficient content addressable memories,â€ IEEE Journal of Solid-
State Circuits , vol. 41, no. 5, pp. 1108 â€“ 1119, May 2006. 
[28] Lipovski G., C. Yu, "The dynamic associative access memory chip and 
its application to SIMD processing and full-text database retrieval.", 
IEEE International Workshop on Memory Technology, Design and 
Testing, 1999. 
[29] Loh G., â€œThe Cost of Uncore in Throughput-Oriented Many-Core 
Processorsâ€, the Workshop on Architectures and Languages for 
Throughput Applications (ALTA), June 2008 
[30] Luebke D., â€œGeneral-purpose computation on graphics hardwareâ€, 
Workshop, SIGGRAPH, 2004 
[31] Morad T. et. al., â€Performance, power efficiency and scalability of 
asymmetric cluster chip multiprocessorsâ€, IEEE Computer Architecture 
Letters, Jan.-June 2006, Volume 5, Issue 1, pages 14 â€“ 17. 
[32] Owens J. et al.,  â€œGPU Computing,â€ Proceedings of the IEEE, Vol. 96, 
No. 5, pp. 879-899, May 2008  
[33] Pagiamtzis K. and A. Sheikholeslami, â€œContent-addressable memory 
(CAM) circuits and architectures: a tutorial and survey,â€ IEEE Journal 
of Solid-State Circuits, vol. 41, no. 3, pp. 712 â€“ 727, March 2006 
[34] Pollack F., â€œNew microarchitecture challenges in the coming genera-
tions of CMOS process technologies (keynote address)â€, MICRO 32, 
1999  
[35] Potter J., et al. "ASC: an associative-computing para-
digm", Computer 27.11 (1994): 19-25.  
[36] Potter, J. and W. Meilander. "Array processor supercomputers", Pro-
ceedings of the IEEE 77, no. 12 (1989): 1896-1914. 
[37] Qing G., X. Guo, R. Patel, E. Ipek, and E. Friedman. "AP-DIMM: Asso-
ciative Computing with STT-MRAM," ISCA 2013 
[38] Quinn M., â€œDesigning Efficient Algorithms for Parallel Computersâ€, 
McGraw-Hill, 1987, page 125. 
[39] Rogers B. et. al., â€œScaling the Bandwidth Wall: Challenges in and Ave-
nues for CMP Scalingâ€. In ISCA â€™09: Proceedings of the 36th annual in-
ternational symposium on Computer architecture, pages 371â€“382, New 
York, NY, USA, 2009. ACM 
[40] Scherson I. et al., â€œBit-Parallel Arithmetic  in a  Massively-Parallel Asso-
ciative Processorâ€, IEEE Transactions on  Computers,  Vol.  41, No. 10,  
October 1992 
[41] Sheaffer J. et al. "Studying thermal management for graphics-processor 
architectures," ISPASS 2005 
[42] Steinkraus D., L. Buck, P. Simard, â€œUsing GPUs for machine learning 
algorithms,â€ IEEE ICDAR 2005. 
[43] Sterling T., H. Zima. "Gilgamesh: a multithreaded processor-in-
memory architecture for petaflops computing." , ACM/IEEE Confer-
ence on Supercomputing, 2002. 
[44] Suh J. et al. "A PIM-based multiprocessor system", 15th International 
Symposium on Parallel and Distributed Processing, 2001. 
[45] Wentzlaff D., et al., â€œCore Count vs. Cache Size for Manycore Architec-
tures in the Cloud. Tech. Rep. MIT-CSAIL-TR-2010-008, MIT, 2010. 
[46] Yavits L. et al., â€œThe effect of communication and synchronization on 
Amdahlâ€™s law in multicore systemsâ€, http://arxiv.org/abs/1306.3302 
[47] Yavits L., â€œArchitecture and design of Associative Processor for image 
processing and computer visionâ€, MSc Thesis, Technion â€“ Israel Insti-
tute of technology, 1994, available at 
http://webee.technion.ac.il/publication-link/index/id/633 
[48] Yavits L. et al., â€œThermal analysis of 3D associative proces-
sorâ€, http://arxiv.org/abs/1307.3853v1 
[49] Yavits  L. et al., â€œCache Hierarchy Optimizationâ€, IEEE Computer Ar-
chitecture Letters, July 2013 
  
Leonid Yavits received his MSc in Electrical Engineer-
ing from the Technion. After graduating, he co-founded 
VisionTech where he co-designed a single chip MPEG2 
codec. Following VisionTechâ€™s acquisition by Broadcom, 
he co-founded Horizon Semiconductors where he co-
designed a Set Top Box on chip for cable and satellite TV. 
Leonid is a PhD student in Electrical Engineering in 
the Technion. He co-authored a number of patents and 
research papers on SoC and ASIC. His research interests 
include Processing in Memory and 3D IC design. 
 
Amir Morad received his BSc and MSc in Electrical Engi-
neering from the Technion. Amir co-founded VisionTech, 
a major provider of ICs for set top boxes market. Follow-
ing VisionTechâ€™s acquisition by Broadcom, Amir co-
founded Horizon Semiconductors, where he co-designed 
SoCs for HD cable and satellite set top boxes.  
Amir is a PhD student in Electrical Engineering in the 
Technion. He co-authored a number of patents and re-
search papers on SoC and ASICs. His research interests 
include analytical modeling and optimization of many-
core architectures. 
 
Ran Ginosar received his BSc from the Technion and 
his PhD from Princeton University. After conducting re-
search at AT&T Bell Laboratories, he joined the Technion 
where he is now professor at the Electrical Engineering 
department and a head of the VLSI Research Center. 
Professor Ginosar has been a visiting Associate Profes-
sor with the University of Utah and co-initiated the Asyn-
chronous Architecture Research Project at Intel (Oregon). 
He has co-founded a number of VLSI companies. Profes-
sor Ginosar has published numerous papers and patents 
on VLSI. His research interests include VLSI architecture, 
asynchronous logic and synchronization. 

