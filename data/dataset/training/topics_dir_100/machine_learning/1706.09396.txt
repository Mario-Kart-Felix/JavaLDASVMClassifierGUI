Landmark Diffusion Maps (L-dMaps):
Accelerated manifold learning out-of-sample
extension
Andrew W. Long† and Andrew L. Ferguson∗,†,‡
†Department of Materials Science and Engineering, University of Illinois at
Urbana-Champaign, 1304 W Green St, Urbana, IL, USA
‡Department of Chemical and Biomolecular Engineering, University of Illinois at
Urbana-Champaign, 600 South Mathews Avenue, Urbana, IL, USA
E-mail: alf@illinois.edu
Phone: (217) 300-2354. Fax: (217) 333-2736
1
ar
X
iv
:1
70
6.
09
39
6v
1 
 [
st
at
.M
L
] 
 2
8 
Ju
n 
20
17
Abstract
Diffusion maps are a nonlinear manifold learning technique based on harmonic analy-
sis of a diffusion process over the data. Out-of-sample extensions with computational
complexity O(N), where N is the number of points comprising the manifold, frus-
trate applications to online learning applications requiring rapid embedding of high-
dimensional data streams. We propose landmark diffusion maps (L-dMaps) to reduce
the complexity to O(M), where M  N is the number of landmark points selected
using pruned spanning trees or k-medoids. Offering (N/M) speedups in out-of-sample
extension, L-dMaps enables the application of diffusion maps to high-volume and/or
high-velocity streaming data. We illustrate our approach on three datasets: the Swiss
roll, molecular simulations of a C24H50 polymer chain, and biomolecular simulations of
alanine dipeptide. We demonstrate up to 50-fold speedups in out-of-sample extension
for the molecular systems with less than 4% errors in manifold reconstruction fidelity
relative to calculations over the full dataset.
2
Keywords
diffusion maps; harmonic analysis; spectral graph theory; nonlinear dimensionality reduction;
molecular simulation
Abbreviations
dMaps – diffusion maps; L-dMaps – landmark diffusion maps; PCA - principal component
analysis; L-Isomap - landmark Isomap; PST – pruned spanning tree; RMS – root mean
squared; RMSD – root mean squared deviation
3
Highlights
• Landmark diffusion maps (L-dMaps) applies diffusion maps over a subset of data points
• L-dMaps offers orders of magnitude speedups in out-of-sample embedding of new data
• These accelerations enable nonlinear embedding of high-velocity streaming data
4
1 Introduction
Linear and nonlinear dimensionality reduction algorithms have found broad applications in
diverse application domains including computer vision,1 recommendation engines,2 anomaly
detection,3 and protein folding.4 These techniques all seek to discover within high-dimensional
datasets low-dimensional projections that preserve the important collective variables govern-
ing the global system behavior and to which the remaining degrees of freedom are effectively
slaved.5–9 Efficient and robust identification of low-dimensional descriptions is of value in im-
proved time-series forecasting,10 understanding protein folding pathways,11 and generating
automated product recommendations.12
Linear techniques such as principal component analysis (PCA),13 multidimensional scal-
ing,14 and random projection,15 have proven successful due to their simplicity and rela-
tive efficiency in identifying these low-dimensional manifolds. Nonlinear methods, such as
Isomap,16 LLE,17 and diffusion maps,18 have proven successful in areas with more complex
manifolds where linear techniques fail, trading the simplicity and efficiency of linear methods
for the capability to discover more complex nonlinear relationships and provide more parsi-
monious representations. Of these methodologies, diffusion maps have received substantial
theoretical and applied interest for their firm mathematical grounding in the harmonic anal-
ysis of diffusive modes over the high-dimensional data.18,19 Specifically, under relatively mild
assumptions on the data, the collective order parameters spanning the low-dimensional non-
linear projection discovered by diffusion maps correspond to the slowest dynamical modes of
the relaxation of a probability distribution evolving under a random walk over the data.18–21
This endows the low-dimensional embedding discovered by diffusion maps with two attrac-
tive properties. First, “diffusion distances” in the high-dimensional space measuring the
accessibility of one system configuration from another map to Euclidean distances in the
low-dimensional projection.19–21 Second, the collective order parameters supporting the low-
dimensional embedding correspond to the important nonlinear collective modes containing
5
the most variance in the data.21 These properties of the diffusion map have been exploited
in diverse applications, including the understanding of protein folding and dynamics,11,22
control of self-assembling Janus colloids,23 tomographic image reconstruction,24 image com-
pletion,25 graph matching,26 and automated tissue classification in pathology slices.19
Practical application of diffusion maps typically involves two distinct but related op-
erations: (i) analysis of a high-dimensional dataset in RK to discover and define a low-
dimensional projection in Rk where k < K and (ii) projection of new out-of-sample points
into the low-dimensional manifold. Discovery of the manifold requires calculation of pairwise
distances between all data points to construct a Markov matrix over the high-dimensional
data, and subsequent diagonalization of this matrix to perform a spectral decomposition of
the corresponding random walk.20,26,27 This eigendecomposition yields an ordered series of
increasingly faster relaxing modes of the random walk, and the identification of a gap in
the eigenvalue spectrum of implied time scales informs the effective dimensionality of the
underlying low-dimensional manifold and collective coordinates with which to parametrize
it.9,18–21,28 Exact calculation of all pairwise distances is of complexity O(N2), although it is
possible to exploit the exponential decay in the hopping probabilities to threshold these ma-
trix dements to zero and to avoid calculation of all distance pairs using clustering or divide
and conquer approaches.29 Diagonalization of the N -by-N matrix has complexity O(N3),30
but the typically sparse nature of the Markov matrix admits sparse Lanczos algorithms that
reduce the complexity to O(N2 + NEnz), where Enz is the number of non-zero matrix ele-
ments.26,31,32 Calculation of only the top l  N eigenvectors can further reduce the cost to
O(lN + lEnz).26,31 The overall complexity of the discovery of the nonlinear low-dimensional
projection is then O(N2 + lN + lEnz) ∼ O(N2).
Projection of out-of-sample points into the nonlinear manifold is complicated by the
unavailability of an explicit transformation matrix for the low-dimensional projection.21
Näıvely, one may augment the original data with the Nnew samples and diagonalize the aug-
mented system, which is an exact but exceedingly expensive operation to perform for each
6
new sample. Accordingly, a number of approximate interpolation techniques have been pro-
posed,33 including principal component analysis-based approaches,34 Laplacian pyramids,35
and the Nyström extension.36–38 The Nyström extension is perhaps the simplest and most
widely used and scales as O(N), requiring the calculation of pairwise distances with the N
points constituting the low-dimensional manifold. Linear scaling with the size of the orig-
inal dataset can be prohibitively costly for online dimensionality reduction applications to
high-velocity and/or high-volume streaming data, where fast and efficient embedding of new
data points is of paramount importance. For example, in online threat or anomaly detection
where excursions of the system into unfavorable regions of the nonlinear manifold must be
quickly recognized in order to take corrective action,39 in robotic motion planning where
movements are generated based on localized paths through a low-dimensional manifold to
maintain kinematic admissibility while navigating obstacles,40,41 and in parallel replica dy-
namics42 or forward flux sampling simulations43,44 of biomolecular folding where departures
from a particular region of the manifold can be used to robustly and rapidly identify confor-
mational changes in any one of the simulation replicas and permit responsive reinitialization
of the replicas to make maximally efficient use of computational resources.
To reduce the computational complexity of the application of diffusion maps to streaming
data we propose a controlled approximation based on the identification of a subset of M 
N “landmark” data points with which to construct the original manifold and embed new
data. Reducing the number of points participating in these operations can offer substantial
computational savings, and the degree of approximation can be controlled and tuned by
the number and location of the landmarks. Our approach is inspired by and analogous
to the landmark Isomap (L-Isomap) adaptation of the original Isomap nonlinear manifold
learning approach due to Tenenbaum, de Silva, and Langford,16,45 which has demonstrated
robust landscape recovery and computational savings considering small numbers of randomly
selected landmark points. Silva et al. subsequently introduced a systematic means to select
landmarks based on L1-regularized minimization of a least-squares objective function.
46 As
7
observed by de Silva and Tenenbaum,45 the underlying principle of this approach is analogous
to global positioning using local distances, whereby a point can be uniquely located within
the manifold given sufficient distance measurements to points distributed over its surface.47
In analogy with this work, we term our approach the landmark diffusion maps (L-dMaps).
It is the purpose of this manuscript to introduce L-dMaps, in which the selection of M 
N landmark points reduces the computational complexity of the out-of-sample projection of a
new data point from O(N) to O(M) offering speedups S ∝ N/M , which can be a substantial
factor when the landmarks constitute a small fraction of the data points constituting the
manifold. The use of landmarks also substantially reduces the memory requirements, leading
to savings in both CPU and RAM requirements that enable applications of diffusion maps
to higher volume and velocity streaming data than is currently possible.
The structure of this paper is as follows. In Materials and Methods, we introduce the
computational and algorithmic details of our L-dMaps approach along with theoretical error
bounds on its fidelity relative to diffusion maps applied to the full dataset. In Results
and Discussion, we demonstrate and analyze the accuracy and performance of L-dMaps on
three test systems – the Swiss roll, molecular simulations of a C24H50 polymer chain, and
biomolecular simulations of alanine dipeptide – in which we report up to 50-fold speedups in
out-of-sample extension with less than 4% errors in manifold reconstruction fidelity relative
to those calculated by dMaps applied to the full dataset. In our Conclusions we close with
an appraisal of our approach and its applications, and an outlook for future work.
2 Materials and Methods
First, we briefly describe the original diffusion maps (dMaps) approach and the Nyström
extension for out-of-sample projection. Second, we introduce landmark diffusion maps (L-
dMaps) presenting two algorithms for systematic identification of landmark points – one fully
automated spanning tree approach, and one based on k-medoids that can be tuned to achieve
8
specific error tolerances – and the subsequent use of these landmarks to perform nonlinear
manifold discovery and Nyström projection of new data. Third, we develop theoretical
estimates of L-dMaps error bounds based on a first-order perturbation expansion in the
errors introduced by the use of landmarks compared to consideration of the full dataset.
Finally, we detail the three datasets upon which we demonstrate and validate our approach:
the Swiss roll, molecular simulations of a C24H50 polymer chain, and biomolecular simulations
of alanine dipeptide.
2.1 Diffusion map dimensionality reduction technique
The diffusion map is a nonlinear dimensionality reduction technique that discovers low-
dimensional manifolds within high-dimensional datasets by performing harmonic analysis of
a random walk constructed over the data to identify nonlinear collective variables containing
the predominance of the variance in the data.9,18–20 The first step in applying diffusion
maps is to compute a measure of similarity between the N high-dimensional data points to
construct the N -by-N pairwise distance matrix d with elements,
dij = ||~xi − ~xj||, (1)
where ||.|| is an appropriate distance metric for the system under consideration (e.g. Eu-
clidean, Hamming, earth movers distance, rotationally and translational aligned root mean
squared deviation (RMSD)). These distances are then used to define a random walk over
the data by defining hopping probabilities Aij from point i to point j as proportional to the
convolution of dij with a Gaussian kernel,
Aij = exp
(
−
d2ij
2
)
, (2)
where  is a soft-thresholding bandwidth that limits transitions between points within an
√

neighborhood. Systematic procedures exist to select appropriate values of  for a particular
9
dataset.7,24 Forming the diagonal matrix D containing the row sums of the A matrix,
Dii =
N∑
j=1
Aij, (3)
we normalize the hopping probabilities to obtain the Markov matrix M,
M = D−1A, (4)
describing a discrete diffusion process over the data. Although other choices of kernels are
possible, the symmetric Gaussian kernel is the infinitesimal generator of a diffusion process
such that the Markov matrix is related to the normalized graph Laplacian,
L = I−M, (5)
where I is the identity matrix, and in the limit of N → ∞ and  → 0 the matrix L
converges to a Fokker-Planck operator describing a continuous diffusion process over the
high-dimensional data.7,18,19,48
Diagonalizing M by solving the N -by-N eigenvalue problem,
MΨ = ΨΛ (6)
where Λ is a diagonal matrix holding the eigenvalues {λi}Ni=1 in non-ascending order and
Ψ = {~ψi}Ni=1 is a matrix of right column eigenvectors corresponding to the relaxation modes
and implied time scales of the random walk.18,19 By the Markov property the top pair {λ1 =
1, ~ψ1 = ~1} are trivial, and the steady state probability distribution over the high-dimensional
point cloud given by the top left eigenvector ~φ1 = diag(D).
7 The graph Laplacian L and
Markov matrix M share left Φ and right Ψ biorthogonal eigenvectors, and the eigenvectors
of L are related to those of M as λLi = 1− λi.7 Accordingly, the leading eigenvectors of M
10
are the slowest relaxing modes of the diffusion process described by the graph Laplacian L.19
A gap in the eigenvalue spectrum exposes a separation of time scales between slow and
fast relaxation modes, informing an embedding into the slow collective modes of the discrete
diffusion process that excludes the remaining quickly relaxing fast degrees of freedom.48
Identifying this gap at λk+1 informs an embedding into the top k non-trivial eigenvectors,
~xi → {~ψ2(i), ~ψ3(i), . . . , ~ψk+1(i)}. (7)
This diffusion mapping can be interpreted as a nonlinear projection of the high-dimensional
data in RK onto a low-dimensional “intrinsic manifold” in Rk discovered within the data
where k < K. The diffusion map determines both the dimensionality k of the intrinsic
manifold and good collective order parameters {~ψl}k+1l=2 with which to parameterize it. As
detailed above, the overall complexity of manifold discovery and projection via diffusion
maps is O(N2), where efficient diagonalization routines leave this calculation dominated by
calculation of pairwise distances.
2.1.1 Nyström extension for out-of-sample points
The Nyström extension presents a means to embed new data points outside the original N
used to define the low-dimensional nonlinear embedding by approximate interpolation of the
new data point within the low-dimensional manifold.33,36–38,49 This operation proceeds by
computing the distances of the new point to the N existing points defining the manifold
dnew,j = ||~xnew−~xj||, and using these values to compute Anew,j = exp
(
−d
2
new,j
2
)
and augment
the M matrix with an additional row Mnew,j =
(∑N
j=1Anew,j
)−1
Anew,j.
33,49 The projected
coordinates of the new point onto the k-dimensional intrinsic manifold defined by the top
l = 2 . . . (k + 1) non-trivial eigenvectors of M are then given by,
~ψl(new) =
1
λl
N∑
j=1
Mnew,j ~ψl(j). (8)
11
The computational complexity for the Nyström projection of a single new data point isO(N),
requiring N distance calculations in the original K-dimensional space and then projection
onto the k-dimensional manifold. Projections are exact for points in the original dataset,
accurate for the interpolation of new points within the kernel bandwidth
√
 of the intrinsic
manifold defined by the N original points, but poor for extrapolations to points residing
beyond this distance away from the manifold.21,33,50
2.2 Landmark diffusion maps (L-dMaps)
The application of diffusion maps to the online embedding of streaming data is limited
by the O(N) computational complexity of the out-of-sample extension that requires the
calculation of pairwise distances of each new data point with the N points defining the
embedding. Here we propose landmark diffusion maps (L-dMaps) that employs a subset
of M  N points providing adequate support for discovery and construction of the low-
dimensional embedding to reduce this complexity to O(M). In this way, we trade-off errors
in the embedding of new data with projection speedups that scale in inverse proportion
to the number of landmarks M . We quantify the reconstruction error introduced by the
landmarking procedure, and demonstrate that these errors can be made arbitrarily small by
selection of sufficiently many landmark points. The use of landmarks has previously been
demonstrated in the L-Isomap variant of the Isomap dimensionality reduction technique to
offer substantial gains in computational efficiency.45
2.2.1 Landmark selection
Landmarking can be conceived as a form of lossy compression that represents localized groups
of points in the high-dimensional feature space by attributing them to a central representative
landmark point. Provided the landmarks are sufficiently well distributed over the intrinsic
manifold mapped out by the data in the high-dimensional space, then the pairwise distances
between landmarks to a new out-of-sample data point provide sufficient distance constraints
12
to accurately embed the new point onto the manifold.45,47 The original L-Isomap algorithm
proposed landmarks be selected randomly,45 and a subsequent sophistication by Silva et
al. proposed a selection procedure based on L1-regularized minimization of a least-squares
objective function.46 In this work, we propose two efficient and systematic approaches to
selection: a pruned spanning tree (PST) approach that offers an automated means to select
landmarks, and a k-medoids approach that allows the user to tune the number of landmarks
to trade-off speed and embedding fidelity to achieve a particular error tolerance. Both
approaches require pre-computation of the N -by-N pairwise distances matrix, making them
expensive for large datasets.46 However, it is the primary goal of this work to select good
landmarks for the rapid and efficient embedding of streaming data into an existing manifold,
so the high one-time overhead associated with their selection is of secondary importance
relative to optimal landmark identification for subsequent online performance.
Pruned spanning tree (PST) landmark selection. The square root of the soft-
thresholding bandwidth
√
 employed by diffusion maps defines the characteristic step size
of the random walk over the high dimensional data (Eqn. 2).7 In order to reliably construct
a low-dimensional embedding, the graph formed by applying this neighborhood threshold to
the pairwise distance matrix must be fully connected to assure than no point is unreachable
from any other (i.e., the Markov matrix M is irreducible).21 This assures that a single
connected random walk can be formed over the data, and that the diffusion map will discover
a single unified intrinsic manifold as opposed to a series of disconnected manifolds each
containing locally connected subsets of the data. This connectivity criterion imposes two
requirements on the selection of landmarks {~zi}Mi=1 ∈ RK as a subset of the data points
{~xi}Ni=1 ∈ RK : (i) all N points are within a
√
 neighborhood of (i.e., covered by) a landmark,
∀~xi,∃~zj | ||~xi − ~zj|| ≤
√
, (9)
and (ii) the graph of pairwise distances over the landmarks is fully connected, with each
13
landmark point within a distance of
√
 of at least one other,
∀~zi, ∃~zj 6=i | ||~zi − ~zj|| ≤
√
. (10)
In practice, a threshold of a few multiples of
√
 may be sufficient to maintain coverage and
connectivity.
These coverage and connectivity conditions motivate a landmark selection procedure
based on spanning trees of the pairwise distances matrix d that naturally enforces both of
these constraints and identifies landmarks that ensure out-of-sample points residing within
the manifold can be embedded within a neighborhood
√
 of a landmark point. Residing
within the characteristic step size of the random walk, this condition is expected to produce
accurate interpolative out-of-sample extensions using the Nyström extension. As described
above, extrapolative extensions are expected to perform poorly for distances greater than
√
.21,33,50 First, we form the binary adjacency matrix G by hard-thresholding the N -by-N
pairwise distances matrix d (Eqn. 1),
Gij =

1 if dij ≤
√
, i 6= j
0 otherwise
. (11)
The binary adjacency matrix defines a new graph in which two data points ~xi and ~xj are
connected if and only if Gij = 1. Next, we seek the minimal subset of edges that contains
no cycles and connects all nodes in the graph, which is equivalent to identifying a spanning
tree representation of the graph T that may be determined in many ways.51 We elect to use
Prim’s algorithm,52 which randomly selects a root node then recursively adds edges between
tree nodes and unassigned nodes until all nodes are incorporated into the tree. As the edge
weights of G are either 0 or 1, Prim’s algorithm at each step randomly selects an edge from
G connecting an unassigned node to a node in T. By only creating new connections between
tree nodes and non-tree nodes, this method guarantees that T is cycle-free and, provided
14
that G is connected, is a spanning tree of G. Finally, we recognize that each leaf node
lies within
√
 of their parent, meaning that all leaves of the tree can be pruned, with the
remaining nodes comprising a pruned spanning tree (PST) defining a set of landmarks {~zi}
satisfying both the covering (Eqn. 9) and connectivity (Eqn. 10) conditions. We summarize
PST landmark identification procedure in Algorithm 1.
Algorithm 1: PST landmark selection
Input: {~xi}Ni=1, G = (VG,EG)
Initialize tree T = (VT,ET) by selecting a random node i, VT = {i},ET = ∅
Construct the spanning tree:
while VT 6= VG do
Gather set of all edges EN between tree and unassigned nodes:
EN = {uv : u ∈ VT, v ∈ VG\VT, uv ∈ EG}
Randomly add edge mn ∈ EN to T:
VT = VT ∪ {n},ET = ET ∪ {mn}
end while
Identify leaf nodes VL of T (nodes of degree 1 in T):
VL = {u : u ∈ VT, deg(u) = 1}
Select all non-leaf nodes:
{~z} = {~xi : i ∈ VT\VL}
Output: {~z} ≡ landmarks
K-medoid landmark selection. Growing and pruning a spanning tree offers an auto-
mated means to identify landmarks that ensure any out-of-sample point can be interpola-
tively embedded within a
√
 neighborhood of a landmark. This procedure is expected to offer
good reconstruction accuracy, but the error tolerance is not directly controlled by the user.
Accordingly, we introduce a second approach to landmark selection that allows the user to
tune the number of landmark points to trade-off computational efficiency against embedding
accuracy in the Nyström extension to achieve a particular runtime target or error tolerance
relative to the use of all N data points. Specifically, we partition the data into a set of M dis-
tinct clusters, and define landmarks within each of these clusters to achieve pseudo-optimal
coverage of the intrinsic manifold for a particular number of landmark points. Numerous
partitioning techniques are available, including spectral clustering,53 affinity propagation,54
and agglomerative hierarchical clustering.55 We use the well-known k-medoids algorithm,
15
using Voronoi iteration to update and select medoid points.56 Compared to k-means cluster-
ing, k-medoids possesses the useful attribute that cluster prototypes are selected as medoids
within the data points constituting the cluster as opposed to as linear combinations defin-
ing the cluster mean. We select the initial set of landmarks randomly from the pool of all
samples, although we note that alternate seeding methods exist such as k-means++.57 We
summarize the k-medoids landmark selection in Algorithm 2.
Algorithm 2: K-medoid landmark selection
Input: {~xi}Ni=1, M , maxItr
Randomly select initial landmarks {~z(0)j }Mj=1 ⊆ {~xi}Ni=1
t = 0
do
Assign points to clusters:
S
(t)
i = {~xj : ||~xj − ~z
(t)
i || ≤ ||~xj − ~z
(t)
m || for all m = 1, . . . ,M}
Update cluster medoid:
~z
(t+1)
i = arg min~xm∈S(t)i
∑
~xj∈S
(t)
i
||~xm − ~xj||
t = t+1
while {~z(t)} 6= {~z(t−1)}) and (t < maxItr)
Output: {~z(t)} ≡ landmarks
2.2.2 Landmark intrinsic manifold discovery
The primary purpose of landmark identification is to define an ensemble of M supports for
the efficient out-of-sample extension projection of streaming data. The nonlinear manifold
can be defined by applying diffusion maps to all N data points. The expensive O(N2)
calculation of the pairwise distances matrix d has already been performed for the purposes
of landmark identification, leaving only a relatively cheapO(lN+lEnz) calculation of the top l
eigenvectors where Enz is the number of non-zero elements in the matrix.
26,31,32 Nevertheless,
having identified these landmarks, additional computational savings may be achieved by
constructing the manifold using only the M landmarks.
Näıve application of diffusion maps to the M landmarks will yield a poor reconstruction
of the original manifold since these landmark points do not preserve the density distribution
16
of the N original points over the high-dimensional feature space. Ferguson et al. previously
proposed a means to efficiently apply diffusion maps to datasets containing multiple copies
of each point in the context of recovering nonlinear manifolds from biased data.50 We adapt
this approach to apply diffusion maps to only the landmark points while approximately
maintaining the density distribution of the full dataset. Given a set of landmark points
{~zi}Mi=1 we may characterize the local density of points in the high-dimensional space around
each landmark by counting the number of data points residing within the Voronoi volume
of each landmark point ~zi defined by the set,
Si = {~xj : ||~xj − ~zi|| ≤ ||~xj − ~zm|| for all m = 1, . . . ,M}. (12)
Following Ref. 50, we now apply diffusion maps to the M landmark points each weighted by
a multiplicity ci = |Si|. Mathematically, this corresponds to solving the M -by-M eigenvalue
problem analogous to that in Eqn. 6 of Section 2.1,
M̃C̃Ψ̃ = Ψ̃Λ̃, (13)
where M̃ = D̃−1Ã, the elements of Ã are given by,
Ãij = exp
(
−||~zi − ~zj||
2
2
)
, (14)
defining the unnormalized hopping probability between landmark points ~zi and ~zj, C̃ is a
diagonal matrix containing the multiplicity of each landmark point,
C̃ii = ci = |Si|, (15)
17
D̃ is a diagonal matrix with elements,
D̃ii =
M∑
j=1
ÃijC̃jj, (16)
and Λ̃ is a diagonal matrix holding the eigenvalues {λ̃i}Mi=1 in non-ascending order, and
Ψ̃ = { ~̃ψi}Mi=1 is the matrix of right column eigenvectors. It can be shown that by enforcing the
normalization condition C̃ ~̃ψi · ~̃ψi = 1 on the eigenvectors, that the diffusion map embedding,
~zi → { ~̃ψ2(i), ~̃ψ3(i), . . . , ~̃ψk+1(i)}, (17)
is precisely that which would be obtained from applying diffusion maps to an ensemble of
points in which each landmark point ~zi is replicated ci times, and the M non-zero eigenvalues
{λi}Mi=1 are identical to {λ̃i}Mi=1.50
The net result of this procedure is that we can define the intrinsic manifold by considering
only the landmark points and diagonalizing a M -by-M matrix as opposed to a N -by-N
matrix with an attendant reduction in computational complexity from O(lN + lEnz) to
O(lM+lEnz). The diffusion mapping in Eqn. 17 defines a reduced M -point intrinsic manifold
that can be considered a lossy compression of the complete N -point manifold, and which
can be stored in a smaller memory footprint.45 For large datasets, the computational and
memory savings associated with calculation and storage of this manifold can be significant.
Although not necessary, if desired the (N −M) non-landmark points can be projected into
the intrinsic manifold using the landmark Nyström extension described in the next section.
This is also precisely the procedure that will be used to perform out-of-sample embeddings
of new data points not contained within the original N data points.
Geometrically, the approximation we make in formulating the reduced eigenvalue problem
is that all points within the Voronoi cell of a landmark point are equivalent to the landmark
point itself, weighting each landmark point according to the number of points inside its
Voronoi volume. This is a good assumption provided that the variation between the points
18
within each Voronoi volume is small relative to the variation over the rest of the high-
dimensional space, and becomes exact in the limit that every point is treated as a landmark
(i.e., M = N). In Section 2.3 we place theoretical bounds on the errors introduced by this
approximation in the nonlinear projection of new out-of-sample points onto the intrinsic
manifold relative to that which would have been computed by explicitly considering all N
points using the original diffusion map.
2.2.3 Landmark Nyström extension
The heart of our L-dMaps approach is the nonlinear projection of new out-of-sample points
using the reduced manifold defined by the M landmark points as opposed to the full N -point
manifold. Nyström embeddings of new points ~xnew over the reduced manifold proceed in an
analogous manner to that detailed in Section 2.1.1, but now considering only the landmark
points. Specifically, we compute the distance of the new point to all landmarks {~zj}Mj=1
to compute elements Ãnew,j = exp
(
− ||~xnew−~zj ||
2
2
)
with which to augment the M̃ matrix
with an additional row with elements M̃new,j =
(∑M
j=1 Ãnew,jC̃jj
)−1
Ãnew,j. The projected
coordinates of the new point onto the k-dimensional reduced intrinsic manifold defined by
the top l = 2 . . . (k + 1) non-trivial eigenvectors of M̃ is,
~̃ψl(new) =
1
λ̃j
M∑
j=1
M̃new,jC̃jj
~̃ψl(j). (18)
This landmark form of the Nyström extension reduces the computational complexity from
O(N) to O(M) by reducing both the number of distance computations and the size of the
matrix-vector multiplication. The attendant runtime speedup S ∝ N/M can be substantial
for M  N , offering accelerations to permit the application of diffusion map out-of-sample
extension to higher volume and higher velocity streaming data than is currently possible.
19
2.3 Landmark error estimation
The diffusion mapping in Eqn. 17 defines a reduced intrinsic manifold comprising the M  N
landmark points that we subsequently use to perform efficient landmark Nyström projections
of out-of-sample data using Eqn. 18. As detailed in Section 2.2.2 the k leading eigenvectors
{~ψ′i}k+1i=2 defining the intrinsic manifold come from the solution of a N -by-N eigenvalue prob-
lem in which each landmark point is weighted by the number of points falling in its Voronoi
volume ci = |Si|, that we solve efficiently and exactly by mapping it to the reduced M -by-M
eigenvalue problem in Eqn. 13.50 The approximation we make in formulating this eigenvalue
problem is to consider each point in the Voroni volume of each landmark point as identical
to the landmark itself. Were we not to make the landmark approximation, we would be
forced to solve a different N -by-N eigenvalue problem explicitly treating all N points using
the original diffusion map with k leading eigenvectors {~ψi}k+1i=2 . In using M  N landmarks
we massively accelerate the out-of-sample embedding of new points, but the penalty we pay
is that the resulting intrinsic manifold we discover is not exactly equivalent to that which
would be discovered by the original diffusion map in the limit M → N . In this section
we estimate the errors in manifold reconstruction introduced by the landmarking procedure
by developing approximate analytical expressions for the discrepancy between the landmark
{~ψ′i}k+1i=2 and true {~ψi}k+1i=2 eigenvectors parameterizing the intrinsic manifold.
We formulate our approximate landmark eigenvalue problem by collapsing the set of
points Sk = {~xj : ||~xj − ~zk|| ≤ ||~xj − ~zm|| for all m = 1, . . . ,M} within the Voronoi volume
of each landmark point {~zk}Mk=1 onto the landmark itself. This amounts to perturbing each
data point ~xi in the high dimensional space by a vector ~∆i = ~zγ−~xi where ~zγ is the landmark
point within the Voronoi volume of which ~xi falls. The elements of the N -by-N pairwise
distance matrix between are correspondingly perturbed from dij = ||~xi−~xj|| to d′ij = ||(~xi +
~∆i)− (~xj + ~∆j)||. This introduces perturbations δij = d′ij − dij into the pairwise distances,
the precise form of which depends on the choice of distance metric ||.||. By propagating
20
these differences through the eigenvalue problem formulated by the original diffusion map
over the true (unshifted) locations of the N points as a first-order perturbation expansion,
we will develop analytical approximations valid in the limit of small perturbations (i.e.,
sufficiently many landmark points) for the corresponding perturbations in the eigenvalues
and eigenvectors introduced by our landmarking procedure. We observe that for a given
choice of landmark points and distance metric, the elements δij are explicitly available,
allowing is to use our terminal expressions to predict the errors introduced for a particular
choice of landmarks. In Section 3.1 we will validate our analytical predictions against errors
calculated by explicit numerical solution of the full and landmark eigenvalue problems.
We first show how to compute the perturbations in our kernel matrix δA and diagonal
row sum matrix δD as a function of the pairwise distance perturbations δij, from which
we estimate perturbations in the Markov matrix δM using this expression. We then use
these values to develop analytical approximations for the perturbations to the eigenvalues
δΛ and eigenvectors δΨ due to the landmark approximation. The perturbation analysis of
the eigenvalue problem presented below follows a similar development to that developed by
Deif.58
Starting from the full diffusion map eigenvalue problem over the original data MΨ = ΨΛ
(Eqn. 6), we define the perturbed eigenvalue problem under the perturbation in the pairwise
distances matrix as,
M′Ψ′ = Ψ′Λ′
⇒ (M + δM)(Ψ + δΨ) = (Ψ + δΨ)(Λ + δΛ), (19)
21
and where from Eqn. 4,
M + δM = (D + δD)−1(A + δA)
= (D−1 − δDD−2)(A + δA) +O(δD2)
= D−1A + D−1δA− δDD−2A +O(δD2, δDδA)
= M + D−1δA− δDD−2A +O(δD2, δDδA)
⇒ δM = M′ −M
= D−1δA− δDD−2A +O(δD2, δDδA) (20)
where in going from the first line to the second we have employed the Maclaurin expansion,
(Dii + δDii)
−1 =
1
Dii
− δDii
D2ii
+O(δD2ii), (21)
and Eqn. 4 in going from the third to the fourth. The perturbations in the elements of the
Markov matrix δM may then be estimated to first order in the perturbations as,
δMij =
δAij
Dii
− δDii
D2ii
Aij. (22)
To estimate the δA and δD required by this expression as a function of the perturbations in
the pairwise distances δij, we commence from the expression for the elements of A
′ keeping
terms in δij to first order,
A′ij = exp
(
−(dij + δij)
2
2
)
= exp
(
−
d2ij
2
)
exp
(
−dijδij

)
exp
(
−
δ2ij
2
)
= Aij
(
1− dijδij

)
+O(δ2ij), (23)
22
from which the perturbations in Aij follow as,
δAij = A
′
ij − Aij
=
(
−dijδij

)
Aij +O(δ2ij). (24)
The elements of the diagonal D′ and D matrices are computed from the A′ and A row sums
respectively, from which the perturbations in Dii follow immediately as,
δDii = D
′
ii −Dii
=
N∑
j=1
A′ij −
N∑
j=1
Aij
=
N∑
j=1
[
Aij
(
1− dijδij

)
− Aij
]
+O(δ2ij)
=
N∑
j=1
(
−dijδij

)
Aij +O(δ2ij) (25)
Using Eqn. 22 we now estimate the corresponding perturbations in the eigenvalues and
eigenvectors. Expanding the perturbed eigenvalue problem in Eqn. 19 and keeping terms to
first order in the perturbations yields,
MΨ + MδΨ + δMΨ = ΨΛ + ΨδΛ + δΨΛ
⇒MδΨ + δMΨ = ΨδΛ + δΨΛ, (26)
and where in going from the first to the second line we have canceled the first term on
each side using original eigenvalue problem MΨ = ΨΛ (Eqn. 6). Treating the unperturbed
eigenvectors Ψ = {~ψi}Ni=1 as an orthonormal basis, we expand the perturbations to each
eigenvector in this basis as,
~δψi =
N∑
j=1
α
(i)
j
~ψj, (27)
23
where {α(i)j }Nj=1 are the expansion coefficients for the perturbation to the ith eigenvector
~δψi = ~ψ
′
i − ~ψi.
To solve for the perturbation to the ith eigenvalue δλi we restrict the perturbed eigen-
value problem in Eqn. 26 to the particular eigenvalue/eigenvector pair {λi, ~ψi} and their
corresponding perturbations {δλi, ~δψi} by extracting the ith column, left multiplying each
side by ~ψTi , and substituting in our expansion for
~δψi,
~ψTi
(
M ~δψi + δM~ψi
)
= ~ψTi
(
~ψiδλi + ~δψiλi
)
⇒ ~ψTi M
(
N∑
j=1
α
(i)
j
~ψj
)
+ ~ψTi δM
~ψi = ~ψ
T
i
~ψiδλi + ~ψ
T
i
(
N∑
j=1
α
(i)
j
~ψj
)
λi
⇒
N∑
j=1
α
(i)
j
~ψTi M
~ψj + ~ψ
T
i δM
~ψi = δλi + λi
N∑
j=1
α
(i)
j
~ψTi
~ψj
⇒
N∑
j=1
α
(i)
j λj
~ψTi
~ψj + ~ψ
T
i δM
~ψi = δλi + λiα
(i)
i
⇒ α(i)i λi + ~ψTi δM~ψi = δλi + λiα
(i)
i
⇒ δλi = ~ψTi δM~ψi
= ~ψTi (D
−1δA− δDD−2A)~ψi (28)
where we have exploited the orthonormality of the eigenvectors, in going from the third
line to the fourth used the original eigenvalue problem in Eqn. 6 to make the substitution
M~ψj = λj ~ψj, and used Eqn. 20 to go from the penultimate to ultimate line.
Using a similar procedure we develop expressions for the expansion coefficients {α(i)j }Nj=1
that we combine with Eqn. 27 to estimate perturbations in the eigenvectors. To compute
α
(i)
l for l 6= i, we again extract the ith column of the perturbed eigenvalue problem in Eqn.
24
26 but this time left multiply by ~ψTl ,
N∑
j=1
α
(i)
j
~ψTl M
~ψj + ~ψ
T
l δM
~ψi = ~ψ
T
l
~ψiδλi + λi
N∑
j=1
α
(i)
j
~ψTl
~ψj
⇒
N∑
j=1
α
(i)
j λj
~ψTl
~ψj + ~ψ
T
l δM
~ψi = λi
N∑
j=1
α
(i)
j
~ψTl
~ψj
⇒ α(i)l =
~ψTl δM
~ψi
λi − λl
=
~ψTl (D
−1δA− δDD−2A)~ψi
λi − λl
, l 6= i. (29)
With the α
(i)
l for l 6= i in hand, we recover α
(i)
i by enforcing normalization of the perturbed
eigenvectors ~ψ′i =
~ψi + ~δψi,
~ψ′ Ti
~ψ′i = 1
⇒ (~ψi + ~δψi)T (~ψi + ~δψi) = 1
⇒ ~ψTi ~ψi + 2~ψTi ~δψi + ~δψ
T
i
~δψi = 1
⇒ 2~ψTi
(
N∑
j=1
α
(i)
j
~ψj
)
+
(
N∑
j=1
α
(i)
j
~ψj
)T ( N∑
l=1
α
(i)
l
~ψl
)
= 0
⇒ 2
N∑
j=1
α
(i)
j
~ψTi
~ψj +
N∑
j=1
N∑
l=1
α
(i)
j α
(i)
l
~ψTj
~ψl = 0
⇒ 2α(i)i +
N∑
l=1
(
α
(i)
l
)2
= 0
⇒
(
α
(i)
i
)2
+ 2α
(i)
i +
N∑
l=1
l 6=i
(
α
(i)
l
)2
= 0
⇒ α(i)i = −1 +
√√√√√1− N∑
l=1
l 6=i
(
α
(i)
l
)2
(30)
where we have appealed to the orthonormality of the eigenvectors {~ψi}Ni=1 and in the last
line solved the quadratic in α
(i)
i by completing the square and taking the positive root that
25
corresponds to shrinking of the perturbed eigenvector along ~ψi to maintain normalization
while preserving its original sense. Finally, we restrict our perturbative analysis (Eqns. 27
and 28) to the subspace spanned by the top l = 2 . . . (k + 1) non-trivial eigenvalues {λi}k+1i=2
and eigenvectors {~ψi}k+1i=2 such that we model only the perturbations within the k-dimensional
subspace containing the intrinsic manifold.
The result of our analysis is a first-order perturbative expression for the errors introduced
by the landmarking procedure in the eigenvalues (Eqn. 28) and eigenvectors (Eqns. 27, 29,
and 30) from our landmarking procedure. In Section 3.1 we demonstrate that for sufficiently
small perturbations in the pairwise distances δij (i.e., sufficiently many landmark points)
that these perturbative analytical predictions are in good agreement with errors calculated
by explicit numerical solutions of the full and landmark eigenvalue problems.
2.4 Datasets
We demonstrate and benchmark the proposed L-dMaps approach in applications to three
datasets: the Swiss roll, molecular simulations of a C24H50 polymer chain, and biomolecular
simulations of alanine dipeptide (Fig. 1).
-10 
-5 
0 
5 
10 
15 
10 
5 
0 
-5 
-10 
0 
20 
40 
60 
z 
y x 
a) b) c) 
Figure 1: The three systems to which L-dMaps was validated and benchmarked. (a) The
Swiss roll, a 2D surface embedded in 3D space that is a canonical test system for nonlin-
ear dimensionality reduction approaches.16 (b) Molecular dynamics simulations of a C24H50
n-alkane chain in water to which diffusion maps have previously been applied to discover
hydrophobic collapse pathways.7,59 (c) Molecular dynamics simulations of alanine dipeptide
– the “hydrogen atom of protein folding” – is the canonical test system for validating and
benchmarking manifold learning, transition sampling, and metastable basin finding tech-
niques in biomolecular simulation.50,60–66
26
Swiss roll. The “Swiss roll” – a 2D surface rolled into 3D space – is a canonical test
system for nonlinear dimensionality reduction techniques.16,21 We illustrate in Figure 1a the
20,000-point Swiss roll dataset employed by Tenenbaum et al. in their introduction of the
Isomap algorithm16 and available for free public download from http://isomap.stanford.
edu/datasets.html. In applying (landmark) diffusion maps to these data, distances be-
tween points are measured using Euclidean distances computed by MATLAB’s L2-norm
function.
Molecular simulations of a C24H50 n-alkane chain. Hydrophobic polymer chains
in water possess rich conformational dynamics, and serve as prototypical models for hy-
drophobic folding that we have previously studied using diffusion maps to determine col-
lapse pathways7 and reconstruct folding funnels from low-dimensional time series.59 We
consider a molecular dynamics trajectory of a C24H50 n-alkane chain in water reported
in Ref. [ 59]. Simulations were conducted in the GROMACS 4.6 simulation suite67 at
298 K and 1 bar employing the TraPPE potential68 for the chain that treats each CH2
and CH3 group as a single united atom, and the SPC water model.
69 This dataset com-
prises 10,000 chain configurations harvested over the course of a 100 ns simulation repre-
sented as 72-dimensional vectors corresponding to the Cartesian coordinates of the 24 united
atoms. Distances between chains are computed as the rotationally and translationally min-
imized root-mean-square deviation (RMSD), calculated using the GROMACS g rms tool
(http://manual.gromacs.org/archive/4.6.3/online/g_rms.html).
Molecular simulations of alanine dipeptide. Finally, we study a molecular dynam-
ics simulation trajectory of alanine dipeptide in water previously reported in Ref. [50]. The
“hydrogen atom of protein folding”, this peptide is a standard test system for new simula-
tion and analysis methods in biomolecular simulation.50,60–66 Unbiased molecular dynamics
simulations were conducted in the Gromacs 4.0.2 suite67 at 298 K and 1 bar modeling the
peptide using the OPLS-AA/L force field70,71 and the TIP3P water model.72 The dataset
comprises 25,001 snapshots of the peptide collected over the course of a 50 ns simulation
27
represented as 66-dimensional vectors comprising the Cartesian coordinates of the 22 atoms
of the peptide. Distances between snapshots were again measured as the rotationally and
translationally minimized RMSD calculated using the GROMACS g rms tool.
3 Results and Discussion
The principal goal of L-dMaps is to accelerate out-of-sample extension by considering only a
subset M  N of landmark points at the expense of the fidelity of the nonlinear embedding
relative to that which would have been achieved using all N samples. First, we quantify the
accuracy of L-dMaps nonlinear embeddings for both PST and k-medoid landmark selection
strategies for the three datasets considered, and compare calculated errors with those esti-
mated from our analytical expressions. Second, we benchmark the speedup and performance
of L-dMaps for out-of-sample projection of new data.
3.1 Landmark diffusion map accuracy
We numerically quantify the accuracy of L-dMaps manifold reconstruction for each of the
three datasets using 5-fold cross validation, where we randomly split the data into five equal
partitions and consider each partition in turn as the test set and the balance as the training
set. We perform a full diffusion map embedding of the training partition to compute the
“true” diffusion map embedding of the complete training partition Ψtrain using Eqn. 6. We
then use the embedding of the training data onto the intrinsic manifold to perform the
Nyström out-of-sample extension projection of the test data onto the manifold using Eqn. 8
to generate their “true” images Ψtest.
We assess the fidelity of the nonlinear projections of the training data generated by our
landmarking approach by taking the ensemble of training data and defining M landmarks
using the PST and k-medoids selection criteria. We then compute the nonlinear embedding
of these landmarks onto the intrinsic manifold by solving the reduced eigenvalue problem in
28
Eqn. 13 and projecting in the remainder of the training data (i.e., the (N−M) non-landmark
points) using the landmark Nyström extension in Eqn. 18. This defines a landmark projection
of the training data Ψ′train. Finally, we use the landmark Nyström extension again to project
in the test partition to generate the landmark embedding of the test data Ψ′test.
To compare the fidelity of the landmark embedding we define a normalized percentage
deviation between the true and landmark embeddings of each point as,
ζ(i) = 100×
√√√√k+1∑
l=2
[
~ψ′l(i)− ~ψl(i)
range(~ψl)
]2
, (31)
where ~ψl(i) and ~ψ
′
l(i) are the true and landmark embeddings of data point i into the l
th
component of the nonlinear embedding into the k-dimensional intrinsic manifold spanned
by the top k non-trivial eigenvectors, range(~ψl) = max(~ψl(i)) − min(~ψl(i)) is the linear
span of dimension l, and the dimensionality k is determined by a gap in the eigenvalue
spectrum at eigenvalue λk+1. The dimensionality of the intrinsic manifolds for the Swiss
roll, C24H50 chain, and alanine dipeptide have previously been determined to be 2, 4, and 2,
respectively.16,50,59 We then compute the root mean squared (RMS) normalized percentage
error as,
Z =
√√√√ 1
P
P∑
i=1
(ζ(i))2, (32)
where P is the number of points constituting either the training or test partition.
We illustrate in Figure 2 the true and landmark embeddings of the training {Ψtrain,Ψ′train}
and test {Ψtest,Ψ′test} data for the Swiss roll dataset, where we selected from the ensemble of
Ntrain = 16,000 points in the training partition a set of M = 4,513 landmarks using the PST
algorithm. The maximum normalized percentage deviation of any one point in either the
training or test data using this set of landmarks is less than 3.2%, and the RMS errors are
Ztrain = 0.643% and Ztest = 0.632%. This demonstrates that using only 28% of the training
set as landmarks, we can reconstruct a high-fidelity embedding with average normalized
29
percentage errors per point of less than 1%.
Figure 2: Landmark error estimates for the N = 20,000 point Swiss roll dataset split into
a 80% training (Ntrain = 16,000) and 20% test (Ntest = 4000) partitions. (a) Application
of the original diffusion map to the full Ntrain = 16,000 training partition yields the 2D
embedding Ψtrain = {~ψ2,train, ~ψ3,train}. (b) Nyström out-of-sample extension of the Ntest =
4000 test points using the full embedding of the training data yields the 2D embedding
Ψtest = {~ψ2,test, ~ψ3,test}. (c) M = 4,513 landmarks were selected from the Ntrain = 16,000
training points using the pruned spanning tree (PST) approach and used to define a landmark
approximation to the intrinsic manifold into which the remaining (Ntrain − M) = 11,487
training points were embedded Ψ′train = {~ψ′2,train, ~ψ′3,train}. (d) Landmark Nyström projection
of the Ntest = 4000 test points into Ψ
′
test = {~ψ′2,test, ~ψ′3,test}. Points are colored in panels (a,b)
by their position along the spiral of the roll as illustrated in Fig. 1a, and in panels (c,d) by
the normalized percentage deviation in the projection ζ(i) defined by Eqn 31.
We present in Table 1 the results of our error analysis for the three datasets using the PST
30
and k-medoids landmark selection strategies at the smallest value of the kernel bandwidth
 supporting a fully connected random walk over the data (cf. Eqn. 2). This value of 
maximally preserves the fine-grained details of the manifold; we consider larger bandwidths
below. For each of the three datasets we observe high-fidelity embeddings using relatively
small fractions of the data as landmarks. For the Swiss roll, ∼30% of the data are required
to achieve a ∼2.5% reconstruction error. Due to the approximately constant density of
points over the manifold, further reductions provide too few points to support accurate
embeddings. For the C24H50 n-alkane chain and alanine dipeptide, the existence of an
underlying energy potential produces large spatial variations in the density over the manifold,
which is exploited by our density weighted landmark diffusion map (Eqns. 13 and 18) to place
landmarks approximately uniformly over the manifold and eliminate large numbers of points
in the high density regions without substantial loss of accuracy. For C24H50, PST landmarks
constituting 1.19% of the training data attain a RMS reconstruction error of ∼8%, and k-
medoids landmarks comprising 3.75% of the data achieve a reconstruction error of ∼3%. For
alanine dipeptide, landmark selection using the PST selection policy achieves a ∼1% error
rate using only 1.75% of the training points, whereas k-medoids requires more than 5% of
the data to achieve that same level of accuracy.
To further explore the relative performance of the PST and k-medoids landmark selection
approaches on the three datasets we conducted a parametric analysis of the error rates for the
two policies at a variety of kernel bandwidths . Small  values better resolve the fine-grained
features of the manifold but require large numbers of landmark points to permit accurate
interpolative embeddings of out-of-sample points by covering the manifold in overlapping
√
 volumes. Large  values sacrifice accurate representations of the details of the manifold,
but permit the use of fewer landmark points. The PST selection procedure does not offer
a means to directly tune the number of landmarks selected, which is controlled by  used
in construction of the spanning tree to ensure coverage and connectivity of the full dataset.
The k-medoids policy permits the user to directly control the error to within a specified
31
Table 1: Root mean squared normalized percentage errors in the landmark nonlinear em-
beddings Z for the PST and k-medoids landmark selection algorithms over the training and
test partitions of the Swiss roll, C24H50, and alanine dipeptide datasets. In each case the
smallest value of  supporting a fully connected random walk was employed in the kernel
bandwidth. We report the mean and standard deviation of Ztrain and Ztest estimated from
5-fold cross validation. For the PST landmark selection policy, we also report the mean and
standard deviation in the number and percentage of landmark points.
Swiss roll (Ntrain = 16000, Ntest = 4000,  = 1)
Method M M / Ntrain(%) Ztrain(%) Ztest(%)
PST 4551.0 (25.5) 28.44 (0.16) 2.42 (1.94) 2.43 (1.95)
2000 12.50 13.43 (13.41) 13.37 (13.30)
K-medoid 4000 25.00 3.74 (3.15) 3.75 (3.15)
8000 50.00 1.22 (1.05) 1.22 (1.04)
C24H50 (Ntrain = 8000, Ntest = 2000,  = 2.87× 10−2)
Method M M / Ntrain(%) Ztrain(%) Ztest(%)
PST 95.2 (2.6) 1.19 (0.03) 7.85 (3.44) 8.48 (3.56)
50 0.63 9.97 (2.86) 10.65 (2.93)
K-medoid 100 1.25 5.38 (1.23) 5.76 (1.47)
300 3.75 3.19 (0.53) 3.49 (0.65)
Alanine dipeptide (Ntrain = 20000, Ntest = 5000,  = 1.06× 10−3)
Method M M / Ntrain(%) Ztrain(%) Ztest(%)
PST 347.2 (8.4) 1.74 (0.04) 0.88 (0.26) 0.94 (0.30)
200 1.00 5.93 (2.81) 6.31 (3.04)
K-medoid 400 2.00 2.92 (0.92) 3.05 (0.84)
1000 5.00 1.43 (0.57) 1.50 (0.60)
threshold by modulating the number of landmarks M . We present the results of our analysis
in Figure 3. The PST approach achieves better accuracy than k-medoids for the same number
of landmarks, offering a good approach for automated landmark selection. The k-medoids
error, however, can be tuned over a large range by adaptively choosing an appropriate number
of landmark points.
To make contact with our analytical error estimates developed in Section 2.3, we present
in Figure 4 a parity plot of the discrepancies in the embedding of the i = 1, . . . , Ntrain training
32
Training Test 
Sw
is
s r
ol
l 
C
24
H
50
 
A
la
ni
ne
 d
ip
ep
tid
e 
Figure 3: Root mean squared normalized percentage errors in the landmark nonlinear em-
beddings Z as a function of the PST (black crosses) or k-medoids (waterfall lines) landmark
selection algorithm and kernel bandwidth  for the training and test partitions of the (a,b)
Swiss roll, (c,d) C24H50, and (e,f) alanine dipeptide datasets. For the PST algorithm the
number of landmarks M is automatically selected as a function of , whereas this is a user-
defined adjustable parameter for the k-medoids approach. We plot the mean Ztrain and Ztest
estimated from 5-fold cross validation.
33
points between the full and landmark embeddings,
σ(i) = ||[~ψ′2,train(i), ~ψ′3,train(i), . . . , ~ψ′k+1,train(i)]− [~ψ2,train(i), ~ψ3,train(i), . . . , ~ψk+1,train(i)]||
= ||[ ~δψ2,train(i), ~δψ3,train(i), . . . , ~δψk+1,train(i)]||
=
√√√√k+1∑
l=2
(
~δψl,train(i)
)2
, (33)
predicted from our analytical first-order perturbative expressions σpred(i) (Eqns. 27, 29, and
30) and calculated directly from our numerical computations σexpt(i). We collate data from
the k-medoids landmark selection process at all numbers of landmarks M with the  band-
widths provided in Table 1; perfect prediction would correspond to all points lying along the
diagonal. The experimental and predicted errors show quite good agreement for large values
of M where the first-order perturbation expansion is a good approximation and the number
of landmarks M is such that the characteristic distance between them is on the order of
√
. The agreement deteriorates for small numbers of landmarks where there are too few
supports covering the manifold to admit an accurate perturbative treatment. In the large-
M / small-error regime where the perturbative treatment is accurate, the analytical error
expressions can be used to predictively identify landmark selections to achieve a specified
error tolerance.
Figure 4: Comparison of the predicted σpred(i) and calculated σexpt(i) errors in the discrep-
ancy of the embedding locations of the training data under the full and landmark embeddings
for the (a) Swiss roll, (b) C24H50 n-alkane chain, and (c) alanine dipeptide datasets.
34
3.2 Out-of-sample extension speedup
In Figure 5 we present as a function of the number of landmarks M the measured speedups
S(M) = tfull/tlandmark(M) for the k-medoids landmark selection data presented in Figure 3 at
the  values given in Table 1. tfull is the measured execution time for the Nyström embedding
of the Ntest training points using the locations of all Ntrain training points computed using
the full diffusion map, and tlandmark(M) is the runtime for the landmark Nyström embedding
using M < Ntrain landmark points. All calculations were performed on an Intel i7-3930
3.2GHz PC with 32GB of RAM, with landmark selection and diffusion mapping performed
in MATLAB and distances computed as described in Section 2.4.
For each of the three systems studied, we observe large accelerations in runtime as we
decrease the number of landmarks employed. In the case of C24H50 and alanine dipeptide,
we observe excellent agreement with the theoretically predicted S ∝ Ntrain/M . Selecting
M/Ntrain ≈ 4% for C24H50 and M/Ntrain ≈ 2% for alanine dipeptide, we achieve 25-fold and
50-fold accelerations in the out-of-sample embedding runtime, respectively, while incurring
only ∼3% errors in the embedding accuracy (Table 1). Reduced accelerations and deviation
from the expected scaling relation are observed for the Swiss roll dataset due to the very
low cost of the Euclidean pairwise distance computation, which leaves these calculations
dominated by computational overhead as opposed to the computational load of the pairwise
distance and Nyström calculations. Nevertheless, we still achieve a two-fold acceleration
in runtime for M/Ntrain ≈ 25% with a ∼4% embedding error (Table 1). Using landmark
diffusion maps can thus drastically reduce the out-of-sample restriction time, with particular
efficacy in embedding samples with computationally expensive distance measures.
4 Conclusions
We have introduced a new technique to accelerate the nonlinear embedding of out-of-sample
data into low-dimensional manifolds discovered by diffusion maps using an approach based
35
103 
102 
101 
S(M)
(M/Ntrain)-1
100 
100 101 102 103 
Swiss roll
C24H50
Alanine dipeptide
Figure 5: Speedup S(M) = tfull/tlandmark(M) in the embedding of the Ntest test points using
a landmark Nyström embedding over M landmark points relative to a full embedding over
all Ntrain points for each of the three datasets. Timings include both the computation of
pairwise distances to the out-of-sample point and the Nyström projection procedure.
on the identification of landmark points over which to perform the embedding. In analogy
with the landmark Isomap (L-Isomap) algorithm,16,45 we term our approach landmark dif-
fusion maps (L-dMaps). By identifying with the N data points a small number of M  N
landmarks to support the calculation of out-of-sample embeddings, we massively reduce the
computational cost associated with this operation to achieve theoretical speedups S ∝ N/M .
We have validated the accuracy and benchmarked the performance of L-dMaps against three
datasets: the Swiss roll, molecular simulations of a C24H50 polymer chain, and biomolecular
simulations of alanine dipeptide. These numerical tests demonstrated the capacity of our
approach to achieve up to 50-fold speedups in out-of-sample embeddings with less than 4%
36
errors in the embedding fidelity for molecular systems. L-dMaps enables the use of diffusion
maps for rapid online embedding of high-volume and high-velocity streaming data, and is
expected to be particularly valuable in applications where runtime performance is critical
or rapid projection of new data points is paramount, such as in threat detection, anomaly
recognition, and high-throughput online monitoring or analysis.
We observe that further accelerations to L-dMaps may be achieved using efficient algo-
rithms to eliminate the need to compute all M pairwise distances between the new out-
of-sample point and the landmarks. For example the recently proposed Fast Library for
Approximate Nearest Neighbors (FLANN)73 uses a tree-search to perform a radial k-nearest
neighbors search, and has been previously employed in the construction of sparse diffu-
sion maps.74 These techniques may also prove valuable in eliminating the need to compute
the N -by-N pairwise distances matrix required for landmark identification, the storage and
analysis of which can prove prohibitive for large datasets. Techniques such as FLANN pro-
vides a means to efficiently construct sparse approximations to the kernel matrix A in which
small hopping probabilities associated with large distances below a user-defined tolerance are
thresholded to zero. Accordingly, we foresee combining approximate nearest neighbor dis-
tance computations, sparse matrix representations, and landmark embedding procedures as
a fruitful area for future development of fast and efficient out-of-sample nonlinear embedding
approaches.
Acknowledgement
Funding: This material is based upon work supported by a National Science Foundation
CAREER Award to A. L. F. (Grant No. DMR-1350008). The sponsor had no role in study
design, collection, analysis and interpretation of data, writing of the report, or decision to
submit the article for publication
37
References
(1) Cho, M.; Lee, J.; Lee, K. M. Reweighted random walks for graph matching. 11th
European Conference on Computer Vision 2010, 492–505.
(2) Sarwar, B.; Karypis, G.; Konstan, J.; Riedl, J. Application of dimensionality reduction
in recommender systems – a case study. Proceedings of the ACM WebKDD 2000 Web
Mining for E-Commerce Workshop. Boston, MA, 2000.
(3) Patcha, A.; Park, J.-M. An overview of anomaly detection techniques: Existing solu-
tions and latest technological trends. Computer Networks 2007, 51, 3448–3470.
(4) Das, P.; Moll, M.; Stamati, H.; Kavraki, L. E.; Clementi, C. Low-dimensional, free-
energy landscapes of protein-folding reactions by nonlinear dimensionality reduction.
Proceedings of the National Academy of Sciences of the United States of America 2006,
103, 9885–9890.
(5) Transtrum, M. K.; Machta, B. B.; Brown, K. S.; Daniels, B. C.; Myers, C. R.;
Sethna, J. P. Perspective: Sloppiness and emergent theories in physics, biology, and
beyond. The Journal of Chemical Physics 2015, 143, 010901.
(6) Machta, B. B.; Chachra, R.; Transtrum, M. K.; Sethna, J. P. Parameter space compres-
sion underlies emergent theories and predictive models. Science 2013, 342, 604–607.
(7) Ferguson, A. L.; Panagiotopoulos, A. Z.; Debenedetti, P. G.; Kevrekidis, I. G. Sys-
tematic determination of order parameters for chain dynamics using diffusion maps.
Proceedings of the National Academy of Sciences of the United States of America 2010,
107, 13597–13602.
(8) Zwanzig, R. Nonequilibrium Statistical Mechanics ; Oxford University Press: New York,
2001.
38
(9) Coifman, R. R.; Kevrekidis, I. G.; Lafon, S.; Maggioni, M.; Nadler, B. Diffusion maps,
reduction coordinates, and low dimensional representation of stochastic systems. Mul-
tiscale Modeling and Simulation 2008, 7, 842–864.
(10) Peña, D.; Poncela, P. Dimension reduction in multivariate time series. In Advances in
Distribution Theory, Order Statistics, and Inference; Balakrishnan, N., Sarabia, J. M.,
Castillo, E., Eds.; Birkhäuser Boston: Boston, MA, 2006; pp 433–458.
(11) Ferguson, A. L.; Zhang, S.; Dikiy, I.; Panagiotopoulos, A. Z.; Debenedetti, P. G.;
Link, A. J. An experimental and computational investigation of spontaneous lasso for-
mation in microcin J25. Biophysical Journal 2010, 99, 3056 – 3065.
(12) Linden, G.; Smith, B.; York, J. Amazon. com recommendations: Item-to-item collabo-
rative filtering. IEEE Internet Computing 2003, 7, 76–80.
(13) Jolliffe, I. T. Principal Component Analysis. In Principal Component Analysis, 2nd ed.;
Springer: New York, 2002.
(14) Borg, I.; Groenen, P. J. Modern Multidimensional Scaling: Theory and Applications ;
Springer: New York, 2005.
(15) Bingham, E.; Mannila, H. Random projection in dimensionality reduction: applications
to image and text data. Proceedings of the Seventh ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining. New York, NY, 2001; pp 245–250.
(16) Tenenbaum, J. B.; De Silva, V.; Langford, J. C. A global geometric framework for
nonlinear dimensionality reduction. Science 2000, 290, 2319–2323.
(17) Roweis, S. T.; Saul, L. K. Nonlinear dimensionality reduction by locally linear embed-
ding. Science 2000, 290, 2323–2326.
(18) Coifman, R. R.; Lafon, S. Diffusion maps. Applied and Computational Harmonic Anal-
ysis 2006, 21, 5–30.
39
(19) Coifman, R. R.; Lafon, S.; Lee, A. B.; Maggioni, M.; Nadler, B.; Warner, F.;
Zucker, S. W. Geometric diffusions as a tool for harmonic analysis and structure defi-
nition of data: Diffusion maps. Proceedings of the National Academy of Sciences of the
United States of America 2005, 102, 7426–7431.
(20) Nadler, B.; Lafon, S.; Coifman, R. R.; Kevrekidis, I. G. Diffusion maps, spectral clus-
tering and eigenfunctions of Fokker-Planck operators. Advances in Neural Information
Processing Systems 18 2006, 955–962.
(21) Ferguson, A. L.; Panagiotopoulos, A. Z.; Kevrekidis, I. G.; Debenedetti, P. G. Non-
linear dimensionality reduction in molecular simulation: The diffusion map approach.
Chemical Physics Letters 2011, 509, 1 – 11.
(22) Mansbach, R. A.; Ferguson, A. L. Machine learning of single molecule free energy
surfaces and the impact of chemistry and environment upon structure and dynamics.
The Journal of Chemical Physics 2015, 142, 105101.
(23) Long, A. W.; Zhang, J.; Granick, S.; Ferguson, A. L. Machine learning assembly land-
scapes from particle tracking data. Soft Matter 2015, 11, 8141–8153.
(24) Coifman, R.; Shkolnisky, Y.; Sigworth, F.; Singer, A. Graph Laplacian tomography
from unknown random projections. IEEE Transactions on Image Processing 2008, 17,
1891–1899.
(25) Gepshtein, S.; Keller, Y. Image completion by diffusion maps and spectral relaxation.
IEEE Transactions on Image Processing 2013, 22, 2983–2994.
(26) Hu, J.; Ferguson, A. L. Global graph matching using diffusion maps. Intelligent Data
Analysis 2016, 20, 637–654.
(27) Pan, V. Y.; Chen, Z. Q. The complexity of the matrix eigenproblem. Proceedings of the
40
Thirty-first Annual ACM Symposium on Theory of Computing. New York, NY, USA,
1999; pp 507–516.
(28) Belkin, M.; Niyogi, P. Laplacian eigenmaps for dimensionality reduction and data rep-
resentation. Neural Computation 2003, 15, 1373–1396.
(29) Kao, M.-Y. Encyclopedia of Algorithms ; Springer Science & Business Media, 2008.
(30) Golub, G.; Van Loan, C. Matrix Computations ; Johns Hopkins Studies in the Mathe-
matical Sciences; Johns Hopkins University Press, 2013.
(31) Bechtold, T.; Rudnyi, E. B.; Korvink, J. G. Fast Simulation of Electro-Thermal MEMS:
Efficient Dynamic Compact Models ; Springer-Verlag: Berlin, Heidelberg, Germany,
2006.
(32) Larsen, R. M. Lanczos bidiagonalization with partial reorthogonalization; DAIMI PB-
357 Technical Report, 1998.
(33) Bengio, Y.; Paiement, J.-F.; Vincent, P.; Delalleau, O.; Le Roux, N.; Ouimet, M.
Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps, and spectral clustering.
Advances in Neural Information Processing Systems 16 2004, 177–184.
(34) Aizenbud, Y.; Bermanis, A.; Averbuch, A. PCA-based out-of-sample extension for di-
mensionality reduction. arXiv:1511.00831v1 2015,
(35) Rabin, N.; Coifman, R. R. Heterogeneous datasets representation and learning using
diffusion maps and Laplacian pyramids. Proceedings of the 2012 SIAM International
Conference on Data Mining. 2012; pp 189–199.
(36) Fowlkes, C.; Belongie, S.; Chung, F.; Malik, J. Spectral grouping using the Nystrom
method. IEEE Transactions on Pattern Analysis and Machine Intelligence 2004, 26,
214–225.
41
(37) Lafon, S.; Keller, Y.; Coifman, R. R. Data fusion and multicue data matching by
diffusion maps. IEEE Transactions on Pattern Analysis and Machine Intelligence 2006,
28, 1784–1797.
(38) Baker, C. T. H. The Numerical Treatment of Integral Equations ; Clarendon Press:
Oxford, 1977; Vol. 13.
(39) Eskin, E.; Arnold, A.; Prerau, M.; Portnoy, L.; Stolfo, S. A geometric framework for
unsupervised anomaly detection. In Applications of Data Mining in Computer Security ;
Springer, 2002; Vol. 6; pp 77–101.
(40) Mahoney, A.; Bross, J.; Johnson, D. Deformable robot motion planning in a reduced-
dimension configuration space. 2010 IEEE International Conference on Robotics and
Automation. 2010; pp 5133–5138.
(41) Chen, Y. F.; Liu, S.-Y.; Liu, M.; Miller, J.; How, J. P. Motion planning with diffusion
maps. 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems.
2016.
(42) Voter, A. F. Parallel replica method for dynamics of infrequent events. Physical Review
B 1998, 57, R13985.
(43) Allen, R. J.; Valeriani, C.; ten Wolde, P. R. Forward flux sampling for rare event
simulations. Journal of Physics: Condensed Matter 2009, 21, 463102.
(44) Escobedo, F. A.; Borrero, E. E.; Araque, J. C. Transition path sampling and forward
flux sampling. Applications to biological systems. Journal of Physics: Condensed Mat-
ter 2009, 21, 333101.
(45) de Silva, V.; Tenenbaum, J. B. Global versus local methods in nonlinear dimensionality
reduction. Advances in Neural Information Processing Systems 15 2003, 721–728.
42
(46) Silva, J.; Marques, J.; Lemos, J. Selecting landmark points for sparse manifold learning.
Advances in Neural Information Processing Systems 18 2006, 1241–1248.
(47) Singer, A. A remark on global positioning from local distances. Proceedings of the
National Academy of Sciences of the United States of America 2008, 105, 9507–9511.
(48) Nadler, B.; Lafon, S.; Coifman, R. R.; Kevrekidis, I. G. Diffusion maps, spectral clus-
tering and reaction coordinates of dynamical systems. Applied and Computational Har-
monic Analysis 2006, 21, 113 – 127.
(49) Sonday, B. E.; Haataja, M.; Kevrekidis, I. G. Coarse-graining the dynamics of a driven
interface in the presence of mobile impurities: Effective description via diffusion maps.
Physical Review E 2009, 80, 031102.
(50) Ferguson, A. L.; Panagiotopoulos, A. Z.; Debenedetti, P. G.; Kevrekidis, I. G. Inte-
grating diffusion maps with umbrella sampling: Application to alanine dipeptide. The
Journal of Chemical Physics 2011, 134, 135103.
(51) Cormen, T. H. Introduction to Algorithms ; MIT Press: Cambridge, MA, 2009.
(52) Prim, R. C. Shortest connection networks and some generalizations. The Bell System
Technical Journal 1957, 36, 1389–1401.
(53) Von Luxburg, U. A tutorial on spectral clustering. Statistics and Computing 2007, 17,
395–416.
(54) Frey, B. J.; Dueck, D. Clustering by passing messages between data points. Science
2007, 315, 972–976.
(55) Day, W. H.; Edelsbrunner, H. Efficient algorithms for agglomerative hierarchical clus-
tering methods. Journal of Classification 1984, 1, 7–24.
(56) Park, H.-S.; Jun, C.-H. A simple and fast algorithm for k-medoids clustering. Expert
Systems with Applications 2009, 36, 3336 – 3341.
43
(57) Arthur, D.; Vassilvitskii, S. k-means++: The advantages of careful seeding. Proceedings
of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms. 2007; pp 1027–
1035.
(58) Deif, A. S. Rigorous perturbation bounds for eigenvalues and eigenvectors of a matrix.
Journal of Computational and Applied Mathematics 1995, 57, 403 – 412.
(59) Wang, J.; Ferguson, A. L. Nonlinear reconstruction of single-molecule free-energy sur-
faces from univariate time series. Physical Review E 2016, 93, 032412.
(60) Zheng, W.; Rohrdanz, M. A.; Clementi, C. Rapid exploration of configuration space
with diffusion-map-directed molecular dynamics. The Journal of Physical Chemistry B
2013, 117, 12769–12776.
(61) Hummer, G.; Kevrekidis, I. G. Coarse molecular dynamics of a peptide fragment:
Free energy, kinetics, and long-time dynamics computations. The Journal of Chem-
ical Physics 2003, 118, 10762–10773.
(62) Chodera, J. D.; Swope, W. C.; Pitera, J. W.; Dill, K. A. Long-time protein folding
dynamics from short-time molecular dynamics simulations. Multiscale Modeling and
Simulation 2006, 5, 1214–1226.
(63) Ma, A.; Dinner, A. R. Automatic method for identifying reaction coordinates in complex
systems. The Journal of Physical Chemistry B 2005, 109, 6769–6779.
(64) Stamati, H.; Clementi, C.; Kavraki, L. E. Application of nonlinear dimensionality reduc-
tion to characterize the conformational landscape of small peptides. Proteins: Structure,
Function, and Bioinformatics 2010, 78, 223–235.
(65) Michielssens, S.; van Erp, T. S.; Kutzner, C.; Ceulemans, A.; de Groot, B. L. Molecular
dynamics in principal component space. The Journal of Physical Chemistry B 2012,
116, 8350–8354.
44
(66) Chodera, J. D.; Singhal, N.; Pande, V. S.; Dill, K. A.; Swope, W. C. Automatic dis-
covery of metastable states for the construction of Markov models of macromolecular
conformational dynamics. The Journal of Chemical Physics 2007, 126, 155101.
(67) Van Der Spoel, D.; Lindahl, E.; Hess, B.; Groenhof, G.; Mark, A. E.; Berendsen, H.
J. C. GROMACS: Fast, flexible, and free. Journal of Computational Chemistry 2005,
26, 1701–1718.
(68) Martin, M. G.; ; Siepmann, J. I. Transferable potentials for phase equilibria. 1. United-
atom description of n-alkanes. The Journal of Physical Chemistry B 1998, 102, 2569–
2577.
(69) Berendsen, H. J.; Postma, J. P.; van Gunsteren, W. F.; Hermans, J. Interaction models
for water in relation to protein hydration. In Intermolecular Forces ; Springer, 1981; pp
331–342.
(70) Jorgensen, W. L.; Chandrasekhar, J.; Madura, J. D.; Impey, R. W.; Klein, M. L.
Comparison of simple potential functions for simulating liquid water. The Journal of
Chemical Physics 1983, 79, 926–935.
(71) Kaminski, G. A.; Friesner, R. A.; Tirado-Rives, J.; Jorgensen, W. L. Evaluation and
reparametrization of the OPLS-AA force field for proteins via comparison with accurate
quantum chemical calculations on peptides. The Journal of Physical Chemistry B 2001,
105, 6474–6487.
(72) Jorgensen, W. L.; Tirado-Rives, J. The OPLS [optimized potentials for liquid sim-
ulations] potential functions for proteins, energy minimizations for crystals of cyclic
peptides and crambin. Journal of the American Chemical Society 1988, 110, 1657–
1666.
(73) Muja, M.; Lowe, D. G. Scalable nearest neighbor algorithms for high dimensional data.
IEEE Transactions on Pattern Analysis and Machine Intelligence 2014, 36, 2227–2240.
45
(74) McQueen, J.; Meila, M.; VanderPlas, J.; Zhang, Z. Megaman: Scalable manifold learn-
ing in Python. Journal of Machine Learning Research 2016, 17, 1–5.
46

