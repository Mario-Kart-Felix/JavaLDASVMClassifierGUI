High-dimensional classification by sparse logistic
regression
Felix Abramovich
Department of Statistics
and Operations Research
Tel Aviv University
Israel
felix@post.tau.ac.il
Vadim Grinshtein
Department of Mathematics
and Computer Science
The Open University of Israel
Israel
vadimg@openu.ac.il
Abstract
We consider high-dimensional binary classification by sparse logistic regression. We propose
a model/feature selection procedure based on penalized maximum likelihood with a complexity
penalty on the model size and derive the non-asymptotic bounds for the resulting misclassifi-
cation excess risk. The bounds can be reduced under the additional low-noise condition. The
proposed complexity penalty is remarkably related to the VC-dimension of a set of sparse linear
classifiers. Implementation of any complexity penalty-based criterion, however, requires a com-
binatorial search over all possible models. To find a model selection procedure computationally
feasible for high-dimensional data, we extend the Slope estimator for logistic regression and
show that under an additional weighted restricted eigenvalue condition it is rate-optimal in the
minimax sense.
Keywords: Complexity penalty; feature selection; high-dimensionality; misclassification excess risk;
sparsity; VC-dimension.
1 Introduction
Classification is one of the most important setups in statistical learning and has been studied in
various contexts. Theoretical foundations of classification are presented in the books of Devroye,
Györfi and Lugosi (1996) and Vapnik (2000), while the surveys of the state-of-the-art can be found
in Boucheron, Bousquet and Lugosi (2005) and Giraud (2015, Section 9).
Consider a general (binary) classification with a (high-dimensional) vector of features x ∈ Rd
and the outcome class label Y |x ∼ Bin(1, p(x)). The accuracy of a classifier η is defined by a
misclassification error R(η) = P (Y 6= η(x)). It is well-known that R(η) is minimized by the Bayes
1
ar
X
iv
:1
70
6.
08
34
4v
1 
 [
m
at
h.
ST
] 
 2
6 
Ju
n 
20
17
classifier η∗(x) = I{p(x) ≥ 1/2}. However, the probability function p(x) is unknown and the
resulting classifier η̂(x) should be designed from the data D: a random sample of n independent
observations (x1, Y1), . . . , (xn, Yn). The design points xi may be considered as fixed or random.
The corresponding (conditional) misclassification error of η̂ is R(η̂) = P (Y 6= η̂(x)|D) and the
goodness of η̂ w.r.t. η∗ is measured by the misclassification excess risk E(η̂, η∗) = ER(η̂)−R(η∗).
A common general (nonparametric) approach for finding a classifier η̂ from the data is empirical
risk minimization (ERM), where minimization of a true misclassification error R(η) is replaced by
minimization of the corresponding empirical risk R̂n(η) =
1
n
∑n
i=1 I{Yi 6= η(xi)} over a given
class of classifiers. Misclassification excess risk of ERM classifiers has been intensively studied in
the literature (see, e.g., Boucheron, Bousquet and Lugosi, 2005 and Giraud, 2015, Section 9 for
surveys and references therein). However, ERM can be hardly used directly in practice due to its
computational cost and is typically relaxed by some related convex minimization surrogate (e.g.,
SVM).
Another possibility to obtain η̂ is to estimate p(x) from the data by some p̂(x) and use a
plug-in classifier of the form η̂(x) = I{p̂(x) ≥ 1/2}. A standard approach is to assume some
parametric model for p(x). In this paper we consider one of the most commonly used models –
logistic regression, where it is assumed that p(x) =
exp(βtx)
1+exp(βtx)
and β ∈ Rd is a vector of unknown
regression coefficients. The corresponding Bayes classifier is a linear classifier η∗(x) = I{p(x) ≥
1/2} = I{βtx ≥ 0}. One then estimates β from the data by the maximum likelihood estimator
(MLE) β̂, plugs-in β̂ (or, equivalently, p̂(x)) and the resulting (linear) classifier is η̂(x) = I{p̂(x) ≥
1/2} = I{β̂
t
x ≥ 0}. Unlike ERM, the MLE β̂ though not available in the closed form, can
be nevertheless obtained numerically by the fast iteratively reweighted least squares algorithm
(McCullagh and Nelder, 1989, Section 2.5).
In the era of “Big Data”, however, the number of features d describing the objects for classifica-
tion might be very large and even larger than the sample size n (large d small n setups) that raises a
severe “curse of dimensionality” problem. Reducing the dimensionality of a feature space by select-
ing a sparse subset of “significant” features becomes essential. Thus, Bickel and Levina (2004), Fan
and Fan (2008) showed that even in simple cases, high-dimensional classification without feature
selection might be as bad as just pure guessing.
Nevertheless, unlike model selection in high-dimensional Gaussian regression that has been
intensively studied in 2000s (see Birgé and Massart, 2001, 2007; Abramovich and Grinshtein, 2010;
Rigollet and Tsybakov, 2011; Verzelen, 2012 among many others), there are much less theoretical
results on model/feature selection in classification. Devroy, Györfi and Lugosi (1996, Chapter 18)
and Vapnik (2000, Chapter 4) considered selection from a sequence of classifiers within a sequence
of classes by penalized ERM with the structural penalty depending on the Vapnik-Chervonenkis
(VC) dimension of a class. They established the oracle inequalities and the upper bounds for the
misclassification excess risk of the selected classifier but did not provide the lower bound to assess
2
its optimality. See also Boucheron, Bousquet and Lugosi (2005, Section 8) for related penalized
ERM approaches and references therein. Recall, however, that a computational cost is a serious
drawback of any ERM-based procedure.
In this paper we investigate feature selection in sparse logistic regression classification. Model
selection in a general framework of generalized linear models (GLM) and in logistic regression
in particular was studied in Abramovich and Grinshtein (2016). They proposed model selection
procedure based on penalized maximum likelihood with a complexity penalty on the model size
and investigated the goodness-of-fit of the resulting estimator in terms of the Kullback-Leibler
risk. They derived the nonasymptotic bounds for this risk and showed that the resulting estimator
is asymptotically minimax and adaptive to the unknown sparsity. In this paper we utilize their
approach for classification and consider the corresponding plug-in classifier. In particular, we show
that the considered complexity penalty is remarkably related to the VC-dimension of a set of sparse
linear classifiers. We establish the non-asymptotic upper bound for misclassification excess risk of
the resulting classifier and construct explicitly the design matrix for which it is sharp in the minimax
sense. We also show that the excess risk bounds can be improved under the additional low-noise
assumption.
Any model selection criterion based on a complexity penalty requires, however, a combinatorial
search over all possible models that makes its usefulness problematic for high-dimensional data.
A common remedy is to replace the original complexity penalty by a related convex surrogate.
The probably most well-known techniques is Lasso. However, it can achieve only the sub-optimal
rates under some extra conditions on the design matrix X (van de Geer, 2008). Recently, for
Gaussian linear regression Bogdan et al. (2015) proposed a Slope estimator. Bellec, Lecué and
Tsybakov (2016) showed that under certain additional conditions on X, Slope is rate-optimal for
linear regression. We adapt it to the logistic regression (and, in fact, to a general GLM) setup
extending the results of Bellec, Lecué and Tsybakov (2016).
The rest of the paper is organized as follows. In Section 2 we present the model (feature)
selection procedure for sparse logistic regression based on a general procedure of Abramovich and
Grinshtein (2016) and provide the upper bounds for the resulting estimator in terms of Kullback-
Leibler risk. In the main Section 3 we apply it for classification, establish the non-asymptotic upper
bound for its misclassification excess risk and derive the corresponding minimax lower bounds.
We also show that the risk bounds can be improved under the additional low-noise assumption.
Finally, we consider the logistic Slope classifier as a convex surrogate for the proposed feature
selection procedure and show that its misclassification excess risk is still rate-optimal under an
extra weighted restricted eigenvalue condition on the design matrix X. All the proofs are given in
the Appendix.
3
2 Notation and preliminaries
Consider a sparse logistic regression model
Yi ∼ Bin(1, pi), ln
pi
1− pi
= βtxi (1)
with deterministic design points xi ∈ Rd i = 1, . . . , n, where we assume that the unknown vector
of regression coefficients β ∈ Rd is sparse.
Let d0 = ||β||0 be the size of true (unknown) model, where the l0 (quasi)-norm of regression
coefficients ||β||0 is the number of nonzero entries. Let Xn×d be the design matrix of rows xi,
r = rank(X) and assume that any r columns of X are linearly independent.
For the model (1) the log-likelihood function is
`(β) =
n∑
i=1
{
βtxi Yi − ln(1 + exp(βtxi)
}
Let M be the set of all 2d possible models M ⊆ {1, . . . , d}. For a given model M , define
BM = {β ∈ Rd : βj = 0 if j 6∈M}. The MLE β̂M of β is then
β̂M = arg max
β̃∈BM
n∑
i=1
{
β̃
t
xiYi − ln
(
1 + exp(β̃
t
xi)
)}
(2)
The β̂M in (2) is not available in the closed form but can be obtained numerically by the iter-
atively reweighted least squares algorithm (see McCullagh and Nelder, 1989, Section 2.5). The
corresponding MLE for probabilities pi are p̂Mi =
exp(β̂
t
Mxi)
1+exp(β̂
t
Mxi)
.
Select the model M̂ by the penalized maximum likelihood model selection criterion of the form
M̂ = arg min
M∈M
{
n∑
i=1
(
ln
(
1 + exp(β̂
t
Mxi)
)
− β̂
t
MxiYi
)
+ Pen(|M |)
}
, (3)
where Pen(|M |) is a complexity penalty on the model size |M |. In fact, we can restrict M in (3)
to models with sizes at most r since for any β with ||β||0 > r, there necessarily exists another β′
with ||β′||0 ≤ r such that Xβ = Xβ′.
Within general GLM framework, Abramovich and Grinshtein (2016) investigated the goodness-
of-fit of the resulting estimator p̂
M̂
. They considered the Kullback-Leibler divergence KL(p, p̂
M̂
)
between the data distribution with the true probabilities p = (p1, . . . , pn) and its empirical distri-
bution generated by p̂
M̂
given by
KL(p, p̂
M̂
) =
1
n
n∑
i=1
{
pi ln
(
pi
p̂
M̂i
)
+ (1− pi) ln
(
1− pi
1− p̂
M̂i
)}
, (4)
and measured the accuracy of p̂
M̂
by the corresponding Kullback-Leibler risk EKL(p, p̂
M̂
) (in
fact, the Kullback-Leibler divergence KL(·, ·) in Abramovich and Grinshtein, 2016 was defined as
n times KL(·, ·) in this paper).
4
Assumption (A). Assume that there exists 0 < δ < 1/2 such that δ < pi < 1−δ or, equivalenelty,
there exists C0 > 0 such that |βtxi| < C0 in (1) for all i = 1, . . . , n.
Assumption (A) prevents the variances V ar(Yi) = pi(1− pi) to be infinitely close to zero.
Consider a set of models of size at most d0, where 1 ≤ d0 ≤ r. Obviously, |M | ≤ d0 iff
||β||0 ≤ d0. Abramovich and Grinshtein (2016) showed that for the complexity penalty
Pen(|M |) = c |M | ln de
|M |
, |M | = 1, . . . , r − 1 and Pen(r) = c r (5)
in (3), where c > 4δ(1−δ) , under Assumption (A), the upper bound of the Kullnack-Leibler risk is
given by
sup
β:||β||0≤d0
EKL(p, p̂
M̂
) ≤ C 1
δ(1− δ)
min
(
d0 ln
de
d0
, r
)
n
(6)
for some C > 0. They also derived the corresponding minimax lower bounds for the Kullback-
Leibler risk and showed that for weakly-collinear design, the upper bound in (6) is of the optimal
order (in the minimax sense).
In what follows we utilize (6) to derive the upper bounds for the misclassification excess risk
of the corresponding plug-in classifier η̂
M̂
(x) = I{β̂
t
M̂x ≥ 0}. To gain more insight into the
complexity penalty (5) within classification framework we present the following lemma on the
Vapnik-Chervonenskis (VC) dimension of the set of all d0-sparse linear classifiers :
Lemma 1. Let C(d0) = {η(x) = I{βtx ≥ 0} : β ∈ Rd, ||β||0 ≤ d0} be the set of all d0-sparse
linear classifiers and V (C(d0)) its VC-dimension. Then,
d0 log2
(
2d
d0
)
≤ V (C(d0)) ≤ 2d0 log2
(
de
d0
)
(7)
Thus, the complexity penalty Pem(|M |) in (5) is essentially proportional to the VC-dimension
of the corresponding class of |M |-sparse linear classifiers C(|M |).
3 Main results
3.1 Misclassification excess risk bounds
In this section we apply the selected model M̂ in (3) for classification and derive the bounds for
the corresponding misclassification exceess risk.
We consider the fixed design. For a given design matrix X the misclassification error of a
classifier η is RX(η) =
1
n
∑n
i=1 P (Yi 6= η(xi)). Following our previous arguments define a (linear)
plug-in classifier
η̂
M̂
(x) = I{β̂
t
M̂x ≥ 0} (8)
5
and consider its misclassification excess risk EX(η̂M̂ , η
∗) = ERX(η̂M̂ ) − RX(η
∗), where recall that
the (ideal) Bayes classifier η∗(x) = I{βtx ≥ 0} with the true (unknown) β in (1). The remarkable
results of Zhang (2004) and Bartlett, Jordan and McAuliffe (2006) establish the relationships
between the Kullback-Leibler risk EKL(p, p̂
M̂
) and the misclassification excess risk EX(η̂M̂ , η
∗):
EX(η̂M̂ , η
∗) ≤
√
2EKL(p, p̂
M̂
) (9)
Thus, (6) and (9) imply immediately the following upper bound for EX(η̂M̂ , η
∗):
Theorem 1. Consider a sparse logistic regression model (1) with ||β||0 ≤ d0. Let M̂ be a model
selected in (3) with the complexity penalty (5) and consider the corresponding plug-in classifier
η̂
M̂
(x) in (8). Under Assumption (A),
sup
η∗∈C(d0)
EX(η̂M̂ , η
∗) ≤ C1
√√√√ 1
δ(1− δ)
min
(
d0 ln
de
d0
, r
)
n
(10)
for some C1 > 0, simultaneously for all 1 ≤ d0 ≤ r.
We now show that there exists a design matrix X0 for which the upper bound for the misclas-
sification excess risk (10) is essentially sharp.
Consider the set of all possible d0-sparse linear classifiers C(d0) defined in Lemma 1 and the
case, where a Bayes classifier η∗(x) is not perfect, that is, R(η∗) > 0, which is sometimes called as
an agnostic model. Then, the following result holds:
Theorem 2. Consider a d0-sparse agnostic logistic regression model (1), where 2 ≤ d0 log2 2dd0 ≤ n.
Then, there exists an n× d design matrix X0 such that
inf
η̃
sup
η∗∈C(d0)
EX0(η̃, η∗) ≥ C2
√
d0 ln
de
d0
n
(11)
for some constant C2 > 0, where the infimum is taken over all classifiers η̃ based on the data
(X0,Y).
Theorem 2 is a particular case of Theorem 4 from Section 3.2 below.
The upper and lower bounds established in Theorem 1 and Theorem 2 allow one to derive
the asymptotic minimax rate for misclassification excess risk in sparse logistic regression model as
n increases. We allow the number of features d to increase with n as well and even faster than
n (d  n setup). The following immediate Corollary 1 shows that the proposed classifier η̂
M̂
is
asymptotically minimax in terms of “the worst case” design and adaptive to the unknown sparsity:
Corollary 1. Consider a d0-sparse logistic regression agnostic model (1), where d0 satisfies 2 ≤
d0 log2
(
2d
d0
)
≤ n. Then, as n and d increase, for a fixed δ in Assumption (A),
6
1. The asymptotic minimax misclassification excess risk supX inf η̃ supη∗∈C(d0) EX(η̃, η
∗) is of the
order √√√√d0 ln(ded0)
n
∼
√
V (C(d0))
n
2. The classifier η̂
M̂
defined in (8), where the model M̂ was selected by (3) with the complexity
penalty (5), attains the minimax rates simultaneously for all 2 ≤ d0 log2
(
2d
d0
)
≤ n.
Finally, we note that if the considered logistic regression model is misspecified and the Bayes
classifier η∗ is not linear, we still have the following risk decomposition
RX(η̂M̂ )−RX(η
∗) =
(
RX(η̂M̂ )−RX(η
∗
L)
)
+ (RX(η
∗
L)−RX(η∗)) , (12)
where η∗L = arg minη∈C(d)RX(η) is the best (ideal) linear classifier. Our previous arguments can
then be applied to the first term in the RHS of (12) representing the estimation error, while the
second term is an approximation error and measures the ability of linear classifiers to perform
as good as η∗. Enriching the class of classifiers may improve the approximation error but will
necessarily increase the stochastic component in (12). In a way, it is similar to the variance/bias
tradeoff in regression.
3.2 Tighter risk bounds under low-noise condition
The main challenges for any classifier occur in regions, where the true p(x) is close to 1/2 and,
therefore, it is hard to predict the class label accurately. However, for regions, where p(x) is bounded
away from 1/2 (margin or, known also, as low-noise condition), the bounds for misclassification
excess risk established in the previous Section 3.1 can be improved. Following Massart and Nédélec
(2006) introduce the following low-noise assumption:
Assumption (B). Consider the logistic regression model (1) and assume that there exists 0 ≤ h <
1/2 such that
|pi − 1/2| ≥ h or, equivalently, |βtxi| ≥ ln
(
1 + 2h
1− 2h
)
(13)
for all i = 1, . . . , n.
Somewhat more general low noise conditions are considered in Mammen and Tsybakov (1999)
and Tsybakov (2004).
For a given design matrix X, define CX(d0, h) = {η : η ∈ C(d0), |βtxi| ≥ ln 1+2h1−2h , i = 1, . . . , n}.
Evidently, CX(d0, 0) = C(d0) for any X.
Theorem 3 below establishes the upper bound for the misclassification excess risk of the proposed
classifier η̂
M̂
under the additional low noise Assumption (B):
7
Theorem 3. Consider a sparse logistic regression model (1), where ||β||0 ≤ d0. Assume that there
exist 0 < h < ∆ < 1/2 such that
h ≤ |pi − 1/2| ≤ ∆ (14)
for all i = 1, . . . , n.
Let M̂ be a model selected in (3) with the complexity penalty (5) and consider the corresponding
classifier η̂
M̂
(x) in (8). Then, for all 1 ≤ d0 ≤ r,
sup
η∗∈CX0 (d0,h)
EX(η̂M̂ , η
∗) ≤ C1 min

√√√√ 1− 4h2
1− 4∆2
min
(
d0 ln
de
d0
, r
)
n
,
1− 4h2
1− 4∆2
min
(
d0 ln
de
d0
, r
)
nh

(15)
for some C1 > 0.
Thus, if the margin parameter is large enough, namely, h >
√
d0 ln
de
d0
n , the misclassification
excess risk bound (10) is reduced. Similar results for random design were obtained in Massart and
Nédélec (2006).
Similar to the previous Section 3.1, one can construct a design matrix for which the upper
bound (15) is sharp:
Theorem 4. Consider a d0-sparse agnostic logistic regression model (1) with 2 ≤ d0 log2 2dd0 ≤ n.
There exists an n× d design matrix X0 such that under Assumption (B)
inf
η̃
sup
η∗∈CX0 (d0,h)
EX0(η̃, η∗) ≥ C2 min

√
d0 ln
de
d0
n
,
d0 ln
de
d0
nh
 (16)
for some C2 > 0.
The design matrix X0 is constructed explicitly in the proof of Theorem 4 in the Appendix. Note
that Theorem 2 may be viewed as a particular case of Theorem 4 for h = 0.
3.3 Logistic Slope classifier
Finding M̂ in (3) requires generally a combinatorial search over all possible models in M that makes
the use of complexity penalties to be computationally problematic for large number of features.
Greedy algorithms (e.g., forward selection) approximate the global solution of (3) by a stepwise
sequence of local ones. However, they require strong constraints on the design matrix X that can
hardly hold for high-dimensional data. A more reasonable approach is convex relaxation, where
the original combinatorial problem is replaced by a related convex surrogate. Thus, for linear-type
complexity penalties of the form Pen(|M |) = λ|M | = λ||β||0, the celebrated Lasso replaces the
8
l0-(quasi) norm by l1-norm:
β̂Lasso = arg min
β̃
{
n∑
i=1
(
ln
(
1 + exp(β̃
t
xi)
)
− β̃
t
xiYi
)
+ λ||β̃||1
}
Assume that all the columns of the design matrix X are normalized to have unit norms. From the
results of Van de Geer (2008) it follows that under an assumption similar to Assumption (A) and
certain extra conditions on X, the logistic Lasso with a tuning parameter λ of the order
√
ln d results
in sub-optimal Kullback-Leibler risk O
(
d0
n ln d
)
and, therefore, sub-optimal misclassification excess
risk O
(√
d0
n ln d
)
.
Recently, for Gaussian regression, Bogdan et al. (2015) suggested the Slope estimator – a
penalized maximum likelihood estimator with a sorted l1-norm penalty defined as follows:
β̂Slope = arg min
β̃
||Y −Xβ̃||2 +
d∑
j=1
λj |β̃|(j)
 , (17)
where || · || denotes the Euclidean norm in Rn, |β̃|(1) ≥ . . . ≥ |β̃|(d) are the descendingly ordered
absolute values of β̃j ’s and λ1 ≥ . . . ≥ λd > 0 are the tuning parameters. It is a convex minimization
problem. Bellec, Lecué and Tsybakov (2016) proved that under certain conditions on X with
normalized columns, the quadratic risk of the Slope estimator (17) with λj = A
√
ln(2d/j) for a
certain constant A is of the rate-optimal order O
(
d0
n ln(
de
d0
)
)
.
We will now extend the above results for Slope for logistic regression and, in fact, for a general
GLM (see the Appendix C). Modifying the definition of the Slope estimator for the considered
logistic regression model (1), define
β̂Slope = arg min
β̃

n∑
i=1
(
ln
(
1 + exp(β̃
t
xi)
)
− β̃
t
xiYi
)
+
d∑
j=1
λj |β̃|(j)
 , (18)
where λ1 ≥ . . . ≥ λd > 0. Note that (18) is also a convex program that makes the logistic
Slope estimator computationally feasible for high-dimensional data. The corresponding estimated
probabilities are p̂Slope,i =
exp(β̂
t
Slopexi)
1+exp(β̂
t
Slopexi)
, i = 1, . . . , n.
As usual, any convex relaxation requires certain extra conditions on the restricted minimal
eigenvalue of the design matrix X over some set of vectors. In particular, similar to Gaussian
regression considered in Bellec, Lecué and Tsybakov (2016), we assume the following Weighted
Restricted Eigenvalue (WRE) condition for Slope estimator (18) :
Assumption. (WRE(d0, c0) condition) Consider the sparse logistic regression model (1) with
||β||0 ≤ d0, where the columns of the desing matrix X are normalized to have unit norms. Consider
the set S(d0, c0) = {u ∈ Rd :
∑d
j=1
√
ln(2d/j)|u|(j) ≤ (1 + c0)||u||
√∑d0
j=1 ln(2d/j)} and assume
that Xu 6= 0 for any u 6= 0 ∈ S(d0, c0).
9
An interesting discussion on WRE condition and those required in Lasso is given in Section 8
of Bellec, Lecué and Tsybakov (2016).
Define a restricted minimal eigenvalue κ(d0, c0) as follows :
κ(d0, c0) = min
u∈S(d0,c0);u6=0
||Xu||
||u||
> 0
Theorem 5. Consider a sparse logistic regression model (1), where ||β||0 ≤ d0, the columns of the
design matrix X are normalized to have unit norms and, in addition, X satisfies the WRE(d0, c0)
condition for some c0 > 1. Assume that Assumption (A) holds.
Let the tuning parameters
λj = A
c0 + 1
c0 − 1
√
ln(2d/j), j = 1, . . . , d (19)
with the constant A ≥ 20
√
6.
Then,
sup
β:||β||0≤d0
EKL(p, p̂Slope) ≤ 8A2
c20
(c0 − 1)2
1
δ(1− δ)
(
2π + 8
ln(2d)
+
1
κ2(d0, c0)
)
d0
n
ln
(
2de
d0
)
(20)
for all 1 ≤ d0 ≤ r.
Note that λj ’s in (19) are of the same form as those in Bellec, Lecué and Tsybakov (2016) for
Gaussian regression but differ in a constant A.
Theorem 5 is a particular case of Theorem 6 for a general GLM (see Appendix C).
Using (9) one immediately gets the corresponding result for the misclassification exceess risk of
the logistic Slope classifier:
Corollary 2. Assume all the conditions of Theorem 5 and choose λj according to (19). Consider
the logistic Slope classifier η̂Slope(x) = I{β̂
t
Slopex ≥ 0}. Then,
EX(η̂Slope, η∗) = O

√
d0 ln
de
d0
n
 (21)
Thus, the logistic Slope estimator is computationally feasible and yet achieves the optimal rates
under the additional WRE(d0, c0) condition on the design for all but very dense models for which
d0 ln(
de
d0
) > r (see Theorem 1).
Acknowledgments
The work was supported by the Israel Science Foundation (ISF), grant ISF-820/13. The authors
would like to thank Noga Alon for his help in the proof of Lemma 1 and Alexander Tsybakov for
valuable remarks.
10
References
[1] Abramovich, F. and Grinshtein, V. (2010). MAP model selection in Gaussian regression. Electr.
J. Statist. 4, 932–949.
[2] Abramovich, F. and Grinshtein, V. (2016). Model selection and minimax estimation in gener-
alized linear models. IEEE Trans. Inf. Theory 62, 3721-3730.
[3] Barron, A., Birgé, L. and Massart P. (1999). Risk bounds for model selection via penalization.
Prob. Theory Relat. Fields, 113, 301-413.
[4] Bartlett, P.L., Jordan, M.I. and McAuliffe, J.D. (2006). Convexity, classification, and risk
bounds. J. Amer. Statist. Assoc., 101, 138–156.
[5] Bellec, P.C., Lecué, G. and Tsybakov, A. (2016). Slope meets Lasso: improved oracle bounds
and optimaility. arXiv:1605.08651.
[6] Bickel, P. and Levina, E. (2004). Some theory for Fisher’s discriminant function, ‘naive Bayes’,
and some alternatives where there are more variables than observations. Bernoulli, 10, 989-
1010.
[7] Birgé, L. and Massart, P. (2001). Gaussian model selection. J. Eur. Math. Soc. 3, 203–268.
[8] Birgé, L. and Massart, P. (2007). Minimal penalties for Gaussian model selection. Probab.
Theory Relat. Fields 138, 33–73.
[9] Bogdan, M., van den Berg, E., Sabatti, C., Su, W. and Candés, E. (2015). SLOPE – adaptive
variable selection via convex programming. Ann. Appl. Statist., 9, 1103–1140.
[10] Boucheron, S., Bousquet, O., and Lugosh, G. (2005) Theory of classification: a survey of some
recent advances. ESAIM: Prob. Statist., 9, 323-375.
[11] Devroye, L., Györfi, L. and Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition.
Springer, New York.
[12] Fan, J., and Fan, Y. (2008). High-dimensional classification using feature annealed indepen-
dence rules. Ann. Statist., 36, 2605–2637.
[13] Giraud, C. (2015). Introduction to High-Dimensional Statistics. CRC Press, Boca Raton.
[14] Mammen, E. and Tsybakov, A. (1999). Smooth discrimination analysis. Ann. Statist. 27,
1808–1829.
[15] Massart, P. and Nédélec, E. (2006). Risk bounds for statistical learning. Ann. Statist. 34,
2326–2366.
11
[16] McCullagh, P. and Nelder, J. A. (1989). Generalized Linear Models, 2nd ed. Chapman and
Hall, London.
[17] Rigollet, P. and Tsybakov, A. (2011). Exponential screening and optimal rates of sparse esti-
mation. Ann. Statist. 39, 731–771.
[18] Tsybakov, A. (2004). Optimal aggregation of classifiers in statistical learning. Ann. Statist.
32, 135–166.
[19] van de Geer, S. (2008). High-dimensional generalized linear models and the Lasso. Ann. Statist.
36, 614–645.
[20] Vapnik, V.N. (2000). The Nature of Statistical Learning, 2nd ed. Springer, New York.
[21] Verzelen, N. (2012). Minimax risks for sparse regressions: Ultra-high dimensionals phe-
nomenon. Electr. J. Statist. 6, 38–90.
[22] Zhang, T. (2004). Statistical behavior and consistency of classification methods based on con-
vex risk minimization. Ann. Statist. 32, 56–85.
Appendix
Throughout the proofs we use various generic positive constants, not necessarily the same each
time they are used even within a single equation.
Appendix A: Proof of Lemma 1
Denote for brevity V = V (C(d0)). For any fixed subset of d0 βj ’s the VC of the corresponding set
of d0-dimensional linear classifiers is known to be d0 (e.g., Giraud, 2015, Exercise 9.5.2). Then,
by Sauer’s lemma the maximal number of different labeling of V points in Rd0 that such set of
classifiers can produce is
∑d0
k=0
(
V
k
)
≤
(
V e
d0
)d0
(see, e.g., Giraud, 2015, Section 9.2.2). The overall
number of different labeling is, therefore,
(
d
d0
)∑d0
k=0
(
V
k
)
, and by the definition of V (C(d0)) we have
2V ≤
(
d
d0
) d0∑
k=0
(
V
k
)
≤
(
de
d0
)d0 (V e
d0
)d0
≤
(
de
d0
)2d0
that implies an upper bound V ≤ 2 d0 log2
(
de
d0
)
.
On the other hand, take k = log2(2d/d0) and let K be the k × 2k−1 matrix whose columns
are all possible vectors with {−1, 1} entries with the first entry 1. Note that d02k−1 = d. Let
W be the d0k × d block-wise matrix consisting of d0 × d0 blocks, each being a k × 2k−1 matrix,
where the diagonal matrices are copies of K, while all others are zero matrices. Thus, W has
d0k = d0 log2(2d/d0) rows. It is easay to verify that these rows are shattered by halfspaces whose
12
supporting vectors w have a single nonzero ±1 entry in each of the d0 blocks and, therefore,
V ≥ d0 log2(2d/d0).
Appendix B: Tighter bounds for low-noise condition
B1: Proof of Theorem 3
Assumption (14) obviously implies Assumption (A) with δ = 1/2 − ∆. In addition, under (14),
V ar(Yi) = pi(1−pi) ≤ (1/2−h)(1/2+h) = (1−4h2)/4. Hence, adapting the results of Abramovich
and Grinshtein (2016) on Kullback-Leibler risk in general GLM framework for logistic regression,
the upper bound (6) for EKL(p, p̂
M̂
) can be improved:
sup
β:||β||0≤d0
EKL(p, p̂
M̂
) ≤ C 1− 4h
2
1− 4∆2
min
(
d0 ln
de
d0
, r
)
n
(22)
and, therefore, from (9) we have
sup
η∗∈CX(d0,h)
EX(η̂M̂ , η
∗) ≤ C1
√√√√ 1− 4h2
1− 4∆2
min
(
d0 ln
de
d0
, r
)
n
On the other hand, we can adapt the general Theorem 3 of Bartlett, Jordan and McAuliffe (2006)
for ψ(f) = (1/2) ((1− f) ln(1− f) + (1 + f) ln(1 + f)) ≥ f2/2 corresponding to the Kullback-
Leibler risk (Zhang, 2004, Section 3.5), α = 1 corresponding to (14) and c = 1/(2∆) to get
EX(η̂M̂ , η
∗) ≤ 4
h
EKL(p, p̂
M̂
)
Applying (22) implies then
EX(η̂M̂ , η
∗) ≤ C1
1− 4h2
1− 4∆2
min
(
d0 ln
de
d0
, r
)
nh
B2: Proof of Theorem 4
For any η̃ and η∗ ∈ C(d0, h) we have
EX(η̃, η∗) =
1
n
n∑
i=1
P (η̃i 6= η∗i )|2pi − 1| ≥
2h
n
E
(
n∑
i=1
I{η̃i 6= η∗i }
)
=
2h
n
E||η̃ − η∗||1 (23)
for any X.
As we have mentioned, the worst case scenario for classification is when pi = 1/2 ± h or,
equivalently, |βtxi| = ln
(
1+2h
1−2h
)
. Let V = d0 log2(2d/d0). In the proof of Lemma 1 we constructed
explicitly the matrix WV×d whose rows w1, . . . ,wV are shattered by C(d0). Then, for any p =
{12 ± h}
V there exists β ∈ Rd such that ||β||0 ≤ d0 and βtwi = ln pi1−pi = ± ln
1+2h
1−2h for all
13
i = 1, . . . , V . Define also the corresponding binary vector b with bi = I{βtwi ≥ 0}, that is, bi = 1
if pi =
1
2 + h and bi = 0 if pi =
1
2 − h. Obviously, the set of all b’s is a hypercube H
V = {0, 1}V .
Define now a n× d design matrix X0 with κ rows of w1, κ rows of w2, ..., κ rows of wV−1 and
the remaining n− (V − 1)κ rows of wV , where an integer 1 ≤ κ ≤ b nV−1c will be defined later.
The proof will now follow the general scheme of the proof of Theorem 4 of Massart and Nédélec
(2006) but with necessary modifications for the fixed design.
For any p ∈ {12 ± h}
V and the corresponding b ∈ HV define an n-dimensional indicator vector
ηb = (b1, . . . , b1︸ ︷︷ ︸
κ
, . . . , bV−1, . . . , bV−1︸ ︷︷ ︸
κ
, bV , . . . , bV︸ ︷︷ ︸
n−(V−1)κ
) and let C̃X0(d0, h) = {ηb, b ∈ HV }. By its design,
C̃X0(d0, h) ⊆ {η : η ∈ C(d0), |βtx0i| = ln 1+2h1−2h , i = 1, . . . , n} ⊆ CX0(d0, h).
Hence, we can reduce the minimax risk over the entire CX0(d0, h) to C̃X0(d0, h):
inf
η̃
sup
η∗∈CX0 (d0,h)
EX0(η̃, η∗) ≥ inf
η̃
sup
η∗∈C̃X0 (d0,h)
EX0(η̃, η∗) (24)
Furthermore, for a given η̃, define η̃∗ = arg min
η∈C̃(d0,h) ||η̃− η||1. Then, for any η
∗ ∈ C̃X0(d0, h)
we have
||η̃∗ − η∗||1 ≤ ||η̃∗ − η̃||1 + ||η̃ − η∗||1 ≤ 2||η̃ − η∗||1 (25)
and, therefore, from (23)-(25)
inf
η̃
sup
η∗∈CX0 (d0,h)
EX0(η̃, η∗) ≥
h
n
inf
η̃∗∈C̃X0 (d0,h)
sup
η∗∈C̃X0 (d0,h)
E||η̃∗ − η∗||1
≥ h
n
κ inf
b̃∈HV
sup
b∗∈HV
E
(
V−1∑
i=1
I{b̃i 6= b∗i }
)
,
(26)
where b̃,b∗ ∈ HV are the binary vectors corresponding to η̃∗ and η∗ respectively (see above).
By a simple calculus one can verify that the square Hellinger distanceH2
(
Bin(1, 12 + h), Bin(1,
1
2 − h)
)
between two Bernoulli distributions Bin(1, 12 +h) and Bin(1,
1
2−h) is 1−
√
1− 4h2. For any b ∈ HV
and the corresponding ηb define pb ∈ Rn as follows: pbi = 12 + h if ηbi = 1 and pbi =
1
2 − h if
ηbi = 0, i = 1, . . . ,κ(V − 1), and pbi = 0, i = κ(V − 1) + 1, . . . , n. Then, for any b1,b2 ∈ HV and
the corresponding pb1 and pb2 we have
H2(pb1 ,pb2) =
1
n
n∑
i=1
H2 (Bin(1, pb1i), Bin(1, pb2i)) =
κ
n
(1−
√
1− 4h2)
V−1∑
i=1
I{b1i 6= b2i}
Hence, applying the version of Assouad’s lemma given in Lemma 7 of Barron, Birgé and Massart
(1999) yields
inf
b̃∈HV
sup
b∗∈HV
E
(
V−1∑
i=1
I{b̃i 6= b∗i }
)
≥ V − 1
2
(
1−
√
2κ(1−
√
1− 4h2
)
≥ V − 1
2
(
1−
√
8κh2
)
14
that togther with (26) implies
inf
η̃
sup
η∗∈CX0 (d0,h)
EX0(η̃, η∗) ≥ κ
h
n
V − 1
2
(
1−
√
8κh2
)
(27)
Consider two cases.
Case 1. h ≤ 16 .
For h ≥
√
V−1
18n , apply (27) for κ = b
1
18h2
c (note that 2 ≤ κ ≤ b nV−1c), to get
inf
η̃
sup
η∗∈CX0 (d0,h)
EX0(η̃, η∗) ≥
V − 1
216nh
≥ C2
d0 ln(
de
d0
)
nh
For h <
√
V−1
18n , one can follow all the above arguments for h̃ =
√
V−1
18n and the corresponding
κ = b nV−1c to have
inf
η̃
sup
η∗∈CX0 (d0,h)
EX0(η̃, η∗) ≥ inf
η̃
sup
η∗∈CX0 (d0,h̃)
EX0(η̃, η∗) ≥ C2
√
d0 ln(
de
d0
)
n
Case 2. h > 16 .
Set κ = 1 and note that CX0(d0, 12) ⊆ CX0(d0, h) for any 0 ≤ h ≤
1
2 . Hence, (26) implies
inf
η̃
sup
η∗∈CX0 (d0,h)
EX0(η̃, η∗) ≥ inf
η̃
sup
η∗∈CX0 (d0,
1
2
)
EX0(η̃, η∗) ≥
1
2n
inf
b̃∈HV
sup
b∗∈HV
E
(
V−1∑
i=1
I{b̃i 6= b∗i }
)
≥ 1
2n
inf
b̃∈HV
1
2V
∑
bj∈HV
E
(
V−1∑
i=1
I{b̃i 6= b∗i }
)
=
1
2n
inf
b̃∈HV
V−1∑
i=1
1
2V
2V∑
j=1
P (b̃i 6= bji)
By obvious combinatoric calculus, for any (binary) vector b̃, 1
2V
∑2V
j=1 P (b̃i 6= bji) =
1
2 for any i
and, therefore,
inf
η̃
sup
η∗∈CX0 (d0,h)
EX0(η̃, η∗) ≥
V − 1
4n
≥ C2
d0 ln
de
d0
nh
for large h > 16 (in fact, larger than any fixed h0).
Appendix C: Slope estimator for a general GLM
Consider a GLM setup with a response variable Y and a set of d predictors x1, ..., xd. We observe
a series of independent observations (xi, Yi), i = 1, . . . , n, where the design points xi ∈ Rp are
deterministic. The distribution fθi(y) of Yi belongs to a (one-parameter) natural exponential family
with a natural parameter θi and a scaling parameter a:
fθi(y) = exp
{
yθi − b(θi)
a
+ c(y, a)
}
(28)
15
The function b(·) is assumed to be twice-differentiable. In this case E(Yi) = b′(θi) and V ar(Yi) =
ab′′(θi). To complete GLM we assume the canonical link θi = β
txi or, equivalently, in the matrix
form, θ = Xβ, where Xn×p is the design matrix and β ∈ Rp is a vector of the unknown regression
coefficients. The logistic regression (1) is a particular case of a general GLM (28) for the Bernoulli
distribution Bin(1, pi), where the natural parameter is θ = ln
p
1−p , b(θ0) = ln(1 + e
θ) and a = 1.
Following Abramovich and Grinshtein (2016) assume the extended version of Assumption (A)
for GLM :
Assumption (A’).
1. Assume that θi ∈ Θ, where the parameter space Θ ⊆ R is a closed (finite or infinite) interval.
2. Assume that there exist constants 0 < L ≤ U < ∞ such that the function b′′(·) satisfies the
following conditions:
(a) supt∈R b
′′(t) ≤ U
(b) inft∈Θ b
′′(t) ≥ L
Conditions on b′′(·) in Assumption (A’) are intended to exclude two degenerate cases, where
the variance V ar(Y ) is infinitely large or small. They also ensure strong convexity of b(·) over
Θ. For the binomial distribution, U = 1/4 and Assumption (A’) reduces to Assumption (A) with
L = δ(1− δ).
Recall that the Slope estimator is a penalized maximum likelihood with an ordered l1-norm
penalty and, therefore, defined for a GLM as follows:
β̂Slope = arg min
β̃
−`(β̃) +
d∑
j=1
λj |β̃|(j)
 = arg minβ̃
b(Xβ̃)t1−YtXβ̃ +
d∑
j=1
λj |β̃|(j)
 (29)
for λ1 ≥ · · · ≥ λd > 0. The corresponding Kullback-Leibler risk
EKL(θ, θ̂Slope) =
1
n
1
a
(
b′(θ)t(θ − E(θ̂Slope))− (b(θ)− Eb(θ̂Slope))t1
)
(30)
where θ = Xβ and θ̂Slope = Xβ̂Slope (see Abramovich and Grinshtein, 2016).
Theorem 6. Consider a GLM (28), where ||β||0 ≤ d0, the columns of the design matrix X are
normalized to have unit norms and X satisfies the WRE(d0, c0) condition for some c0 > 1. Assume
that Assumption (A’) holds.
Let
λj = A
c0 + 1
c0 − 1
√
U
a
√
ln(2d/j), j = 1, . . . , d, (31)
in (29) with the constant A ≥ 40
√
6.
Then, simultaneously for all β ∈ Rd such that ||β||0 ≤ d0,
16
1.
P
(
KL(θ, θ̂Slope) ≤
8A2
n
c20
(c0 − 1)2
U
L
max
{(√
π/2 +
√
2 ln ∆−1
)2
,
d0
κ2(d0, c0)
ln
(
2de
d0
)})
≥ 1−∆
(32)
for any 0 < ∆ < 1.
2.
EKL(θ, θ̂Slope) ≤ 8A2
c20
(c0 − 1)2
U
L
(
2π + 8
ln(2d)
+
1
κ2(d0, c0)
)
d0
n
ln
(
2de
d0
)
(33)
Proof. Since β̂Slope is the minimizer of (29),
−`(β̂Slope) +
d∑
j=1
λj |β̂Slope|(j) ≤ −`(β) +
d∑
j=1
λj |β|(j)
From (29) of Abramovich and Grinshtein (2016) one has
n KL(θ, θ̂Slope) = `(β)− `(β̂Slope) +
1
a
(Y − b′(θ))t(θ̂Slope − θ)
(recall that the Kullback-Leibler divergence KL(·, ·) in Abramovich and Grinshtein, 2016 was de-
fined as n times KL(·, ·) in this paper). Thus,
KL(θ, θ̂Slope) ≤
1
n a
(Y − b′(θ))t(θ̂Slope − θ) +
1
n
 d∑
j=1
λj |β|(j) −
d∑
j=1
λj |β̂Slope|(j)
 (34)
Let u = β̂Slope−β. Applying Lemma A.1 of Bellec, Lecué and Tsybakov (2016) with τ = 0 implies
d∑
j=1
λj |β|(j) −
d∑
j=1
λj |β̂Slope|(j) ≤
√√√√ d0∑
j=1
λ2j ||u|| −
d∑
j=d0+1
λj |u|(j) (35)
Consider now the first term of the RHS in (34). Since the distribution of Y belongs to the expo-
nential family with the bounded variance ab′′(θ) ≤ aU (Assumption (A’)), a centered zero mean
random variable Y − b′(θ) is sub-Gaussian with the scale factor
√
aU , that is, Eet(Y−b′(θ)) ≤ eaUt2/2
and, therefore, Ee(Y−b′(θ))2/(6Ua) ≤ e. Applying Theorem 9.1 of Bellec, Lecué and Tsybakov (2016)
(adapted to our normalization conditions on the columns of X) with probability at least 1 − ∆
yields
1
na
(Y−b′(θ))t(θ̂Slope−θ) ≤
40
√
6U
n
√
a
max
 d∑
j=1
|u|(j)
√
ln(2d/j) , ||θ̂Slope − θ||(
√
π/2 +
√
2 ln ∆−1)

(36)
17
Set
H(u) =
d∑
j=1
|u|(j)
√
ln(2d/j) ≤ ||u||
√√√√ d0∑
j=1
ln(2d/j) +
d∑
j=d0+1
|u|(j)
√
ln(2d/j) = H̃(u) (37)
and
G(u) = ||θ̂Slope − θ||
(√
π/2 +
√
2 ln ∆−1
)
(38)
The proof will now go along the lines of the proof of Theorem 6.1 Bellec, Lecué and Tsybakov
(2016) for Gaussian regression with necessary adaptations to GLM and different normalization
conditions on the columns of X.
To prove (32) consider two cases.
Case 1. H̃(u) ≤ G(u). In this case
||u|| ≤
||θ̂Slope − θ||(
√
π/2 +
√
2 ln ∆−1)√∑d0
j=1 ln(2d/j)
and, therefore, combining (31) and (34)-(38) with probability at least 1−∆ yields
KL(θ, θ̂Slope) ≤
1
n
A
√
U
a
2c0
c0 − 1
||θ̂Slope − θ||(
√
π/2 +
√
2 ln ∆−1)
≤ 1
2n
(
A2U
a
(
2c0
c0 − 1
)2
(
√
π/2 +
√
2 ln ∆−1)2 + ||θ̂Slope − θ||2
) (39)
for any  > 0.
Lemma 1 of Abramovich and Grinshtein (2016) established the equivalence of the Kullback-
Leibler divergence KL(θ, θ̂Slope) and the squared quadratic norm ||θ̂Slope− θ||2 under Assumption
(A’):
L
2a
||θ̂Slope − θ||2 ≤ nKL(θ, θ̂Slope) ≤
U
2a
||θ̂Slope − θ||2 (40)
Hence, taking  = L/(2a) in (39) after a straightforward calculus yields
KL(θ, θ̂Slope) ≤
8
n
c20
(c0 − 1)2
U
L
A2(
√
π/2 +
√
2 ln ∆−1)2 (41)
with probability at least 1−∆.
Case 2. H̃(u) > G(u). Using the definition of λj ’s in (31) and (34)-(38), with probability at least
1−∆ we have
KL(θ, θ̂Slope) ≤
1
n
40
√
6U
a
||u||
√√√√ d0∑
j=1
ln(2d/j) +
d∑
j=d0+1
|u|(j)
√
ln(2d/j)

+
1
n
√√√√ d0∑
j=1
λ2j ||u|| −
d∑
j=d0+1
λj |u|(j)

≤ 1
n
 2c0
c0 + 1
||u||
√√√√ d0∑
j=1
λ2j −
2
c0 + 1
d∑
j=d0+1
λj |u|(j)

(42)
18
The KL(θ, θ̂Slope) ≥ 0 and, therefore, the RHS of (42) is necessarily positive. Thus,
d∑
j=1
|u|(j)
√
ln(2d/j) ≤ ||u||
√√√√ d0∑
j=1
ln(2d/j) +
d∑
j=d0+1
|u|(j)
√
ln(2d/j) ≤ (1 + c0)||u||
√√√√ d0∑
j=1
ln(2d/j)
and, therefore, by WRE(d0, c0) condition (42),
KL(θ, θ̂Slope) ≤
1
n
2c0
c0 + 1
||u||
√√√√ d0∑
j=1
λ2j ≤
1
n
2c0
c0 + 1
||θ̂Slope − θ||
κ(c0, d0)
√√√√ d0∑
j=1
λ2j
≤ 1
n
(
c20
(c0 + 1)2
∑d0
j=1 λ
2
j
κ2(c0, d0)
+ ||θ̂Slope − θ||2
)
for any  > 0. Taking  = L/(4a) and exploiting the equivalence between KL(θ, θ̂Slope) and
||θ̂Slope − θ||2 in (40) imply that with probability at least 1−∆,
KL(θ, θ̂Slope) ≤
1
n
1
L
8ac20
(c0 + 1)2
∑d0
j=1 λ
2
j
κ2(c0, d0)
≤ 8
n
c20
(c0 − 1)2
U
L
A2
d0 ln(2de/d0)
κ2(c0, d0)
,
where we used the definition (31) of λj ’s and the upper bound
∑d0
j=1 ln(2d/j) ≤ d0 ln(2ed/d0) (see,
e.g., (2.7) of Bellec, Lecué and Tsybakov, 2016).
To prove the second statement (33) of the theorem denote C∗ = 8A2
c20
(c0−1)2 and note that
C∗
1
n
U
L
max
{(√
π/2 +
√
2 ln ∆−1
)2
,
d0
κ2(d0, c0)
ln
(
2de
d0
)}
≤ C∗ 1
n
U
L
max
{
2π, 8 ln ∆−1,
d0
κ2(d0, c0)
ln
(
2de
d0
)}
≤ C∗ 1
n
U
L
max
{
max
(
2π
ln(2d)
,
1
κ2(d0, c0)
)
d0 ln
(
2de
d0
)
, 8 ln ∆−1
}
≤ C∗ 1
n
U
L
max
{(
2π
ln(2d)
+
1
κ2(d0, c0)
)
d0 ln
(
2de
d0
)
, 8 ln ∆−1
}
(43)
Then, by integrating, (32) and (43) after a straightforward calculus yield
EKL(θ, θ̂Slope) =
∫ ∞
0
P
(
KL(θ, θ̂Slope) ≥ t
)
dt
≤ C∗ 1
n
U
L
( 2π
ln(2d)
+
1
κ2(d0, c0)
)
d0 ln
(
2de
d0
)
+ 8
(
2de
d0
)− d0
8
max{ 2π
ln(2d)
,κ−2(d0,c0)}

≤ C∗ U
L
(
2π + 8
ln(2d)
+
1
κ2(d0, c0)
)
d0
n
ln
(
2de
d0
)
19

