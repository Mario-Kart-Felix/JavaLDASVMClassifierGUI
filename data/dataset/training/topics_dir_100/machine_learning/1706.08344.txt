High-dimensional classification by sparse logistic
regression
Felix Abramovich
Department of Statistics
and Operations Research
Tel Aviv University
Israel
felix@post.tau.ac.il
Vadim Grinshtein
Department of Mathematics
and Computer Science
The Open University of Israel
Israel
vadimg@openu.ac.il
Abstract
We consider high-dimensional binary classification by sparse logistic regression. We propose
a model/feature selection procedure based on penalized maximum likelihood with a complexity
penalty on the model size and derive the non-asymptotic bounds for the resulting misclassifi-
cation excess risk. The bounds can be reduced under the additional low-noise condition. The
proposed complexity penalty is remarkably related to the VC-dimension of a set of sparse linear
classifiers. Implementation of any complexity penalty-based criterion, however, requires a com-
binatorial search over all possible models. To find a model selection procedure computationally
feasible for high-dimensional data, we extend the Slope estimator for logistic regression and
show that under an additional weighted restricted eigenvalue condition it is rate-optimal in the
minimax sense.
Keywords: Complexity penalty; feature selection; high-dimensionality; misclassification excess risk;
sparsity; VC-dimension.
1 Introduction
Classification is one of the most important setups in statistical learning and has been studied in
various contexts. Theoretical foundations of classification are presented in the books of Devroye,
GyoÌˆrfi and Lugosi (1996) and Vapnik (2000), while the surveys of the state-of-the-art can be found
in Boucheron, Bousquet and Lugosi (2005) and Giraud (2015, Section 9).
Consider a general (binary) classification with a (high-dimensional) vector of features x âˆˆ Rd
and the outcome class label Y |x âˆ¼ Bin(1, p(x)). The accuracy of a classifier Î· is defined by a
misclassification error R(Î·) = P (Y 6= Î·(x)). It is well-known that R(Î·) is minimized by the Bayes
1
ar
X
iv
:1
70
6.
08
34
4v
1 
 [
m
at
h.
ST
] 
 2
6 
Ju
n 
20
17
classifier Î·âˆ—(x) = I{p(x) â‰¥ 1/2}. However, the probability function p(x) is unknown and the
resulting classifier Î·Ì‚(x) should be designed from the data D: a random sample of n independent
observations (x1, Y1), . . . , (xn, Yn). The design points xi may be considered as fixed or random.
The corresponding (conditional) misclassification error of Î·Ì‚ is R(Î·Ì‚) = P (Y 6= Î·Ì‚(x)|D) and the
goodness of Î·Ì‚ w.r.t. Î·âˆ— is measured by the misclassification excess risk E(Î·Ì‚, Î·âˆ—) = ER(Î·Ì‚)âˆ’R(Î·âˆ—).
A common general (nonparametric) approach for finding a classifier Î·Ì‚ from the data is empirical
risk minimization (ERM), where minimization of a true misclassification error R(Î·) is replaced by
minimization of the corresponding empirical risk RÌ‚n(Î·) =
1
n
âˆ‘n
i=1 I{Yi 6= Î·(xi)} over a given
class of classifiers. Misclassification excess risk of ERM classifiers has been intensively studied in
the literature (see, e.g., Boucheron, Bousquet and Lugosi, 2005 and Giraud, 2015, Section 9 for
surveys and references therein). However, ERM can be hardly used directly in practice due to its
computational cost and is typically relaxed by some related convex minimization surrogate (e.g.,
SVM).
Another possibility to obtain Î·Ì‚ is to estimate p(x) from the data by some pÌ‚(x) and use a
plug-in classifier of the form Î·Ì‚(x) = I{pÌ‚(x) â‰¥ 1/2}. A standard approach is to assume some
parametric model for p(x). In this paper we consider one of the most commonly used models â€“
logistic regression, where it is assumed that p(x) =
exp(Î²tx)
1+exp(Î²tx)
and Î² âˆˆ Rd is a vector of unknown
regression coefficients. The corresponding Bayes classifier is a linear classifier Î·âˆ—(x) = I{p(x) â‰¥
1/2} = I{Î²tx â‰¥ 0}. One then estimates Î² from the data by the maximum likelihood estimator
(MLE) Î²Ì‚, plugs-in Î²Ì‚ (or, equivalently, pÌ‚(x)) and the resulting (linear) classifier is Î·Ì‚(x) = I{pÌ‚(x) â‰¥
1/2} = I{Î²Ì‚
t
x â‰¥ 0}. Unlike ERM, the MLE Î²Ì‚ though not available in the closed form, can
be nevertheless obtained numerically by the fast iteratively reweighted least squares algorithm
(McCullagh and Nelder, 1989, Section 2.5).
In the era of â€œBig Dataâ€, however, the number of features d describing the objects for classifica-
tion might be very large and even larger than the sample size n (large d small n setups) that raises a
severe â€œcurse of dimensionalityâ€ problem. Reducing the dimensionality of a feature space by select-
ing a sparse subset of â€œsignificantâ€ features becomes essential. Thus, Bickel and Levina (2004), Fan
and Fan (2008) showed that even in simple cases, high-dimensional classification without feature
selection might be as bad as just pure guessing.
Nevertheless, unlike model selection in high-dimensional Gaussian regression that has been
intensively studied in 2000s (see BirgeÌ and Massart, 2001, 2007; Abramovich and Grinshtein, 2010;
Rigollet and Tsybakov, 2011; Verzelen, 2012 among many others), there are much less theoretical
results on model/feature selection in classification. Devroy, GyoÌˆrfi and Lugosi (1996, Chapter 18)
and Vapnik (2000, Chapter 4) considered selection from a sequence of classifiers within a sequence
of classes by penalized ERM with the structural penalty depending on the Vapnik-Chervonenkis
(VC) dimension of a class. They established the oracle inequalities and the upper bounds for the
misclassification excess risk of the selected classifier but did not provide the lower bound to assess
2
its optimality. See also Boucheron, Bousquet and Lugosi (2005, Section 8) for related penalized
ERM approaches and references therein. Recall, however, that a computational cost is a serious
drawback of any ERM-based procedure.
In this paper we investigate feature selection in sparse logistic regression classification. Model
selection in a general framework of generalized linear models (GLM) and in logistic regression
in particular was studied in Abramovich and Grinshtein (2016). They proposed model selection
procedure based on penalized maximum likelihood with a complexity penalty on the model size
and investigated the goodness-of-fit of the resulting estimator in terms of the Kullback-Leibler
risk. They derived the nonasymptotic bounds for this risk and showed that the resulting estimator
is asymptotically minimax and adaptive to the unknown sparsity. In this paper we utilize their
approach for classification and consider the corresponding plug-in classifier. In particular, we show
that the considered complexity penalty is remarkably related to the VC-dimension of a set of sparse
linear classifiers. We establish the non-asymptotic upper bound for misclassification excess risk of
the resulting classifier and construct explicitly the design matrix for which it is sharp in the minimax
sense. We also show that the excess risk bounds can be improved under the additional low-noise
assumption.
Any model selection criterion based on a complexity penalty requires, however, a combinatorial
search over all possible models that makes its usefulness problematic for high-dimensional data.
A common remedy is to replace the original complexity penalty by a related convex surrogate.
The probably most well-known techniques is Lasso. However, it can achieve only the sub-optimal
rates under some extra conditions on the design matrix X (van de Geer, 2008). Recently, for
Gaussian linear regression Bogdan et al. (2015) proposed a Slope estimator. Bellec, LecueÌ and
Tsybakov (2016) showed that under certain additional conditions on X, Slope is rate-optimal for
linear regression. We adapt it to the logistic regression (and, in fact, to a general GLM) setup
extending the results of Bellec, LecueÌ and Tsybakov (2016).
The rest of the paper is organized as follows. In Section 2 we present the model (feature)
selection procedure for sparse logistic regression based on a general procedure of Abramovich and
Grinshtein (2016) and provide the upper bounds for the resulting estimator in terms of Kullback-
Leibler risk. In the main Section 3 we apply it for classification, establish the non-asymptotic upper
bound for its misclassification excess risk and derive the corresponding minimax lower bounds.
We also show that the risk bounds can be improved under the additional low-noise assumption.
Finally, we consider the logistic Slope classifier as a convex surrogate for the proposed feature
selection procedure and show that its misclassification excess risk is still rate-optimal under an
extra weighted restricted eigenvalue condition on the design matrix X. All the proofs are given in
the Appendix.
3
2 Notation and preliminaries
Consider a sparse logistic regression model
Yi âˆ¼ Bin(1, pi), ln
pi
1âˆ’ pi
= Î²txi (1)
with deterministic design points xi âˆˆ Rd i = 1, . . . , n, where we assume that the unknown vector
of regression coefficients Î² âˆˆ Rd is sparse.
Let d0 = ||Î²||0 be the size of true (unknown) model, where the l0 (quasi)-norm of regression
coefficients ||Î²||0 is the number of nonzero entries. Let XnÃ—d be the design matrix of rows xi,
r = rank(X) and assume that any r columns of X are linearly independent.
For the model (1) the log-likelihood function is
`(Î²) =
nâˆ‘
i=1
{
Î²txi Yi âˆ’ ln(1 + exp(Î²txi)
}
Let M be the set of all 2d possible models M âŠ† {1, . . . , d}. For a given model M , define
BM = {Î² âˆˆ Rd : Î²j = 0 if j 6âˆˆM}. The MLE Î²Ì‚M of Î² is then
Î²Ì‚M = arg max
Î²ÌƒâˆˆBM
nâˆ‘
i=1
{
Î²Ìƒ
t
xiYi âˆ’ ln
(
1 + exp(Î²Ìƒ
t
xi)
)}
(2)
The Î²Ì‚M in (2) is not available in the closed form but can be obtained numerically by the iter-
atively reweighted least squares algorithm (see McCullagh and Nelder, 1989, Section 2.5). The
corresponding MLE for probabilities pi are pÌ‚Mi =
exp(Î²Ì‚
t
Mxi)
1+exp(Î²Ì‚
t
Mxi)
.
Select the model MÌ‚ by the penalized maximum likelihood model selection criterion of the form
MÌ‚ = arg min
MâˆˆM
{
nâˆ‘
i=1
(
ln
(
1 + exp(Î²Ì‚
t
Mxi)
)
âˆ’ Î²Ì‚
t
MxiYi
)
+ Pen(|M |)
}
, (3)
where Pen(|M |) is a complexity penalty on the model size |M |. In fact, we can restrict M in (3)
to models with sizes at most r since for any Î² with ||Î²||0 > r, there necessarily exists another Î²â€²
with ||Î²â€²||0 â‰¤ r such that XÎ² = XÎ²â€².
Within general GLM framework, Abramovich and Grinshtein (2016) investigated the goodness-
of-fit of the resulting estimator pÌ‚
MÌ‚
. They considered the Kullback-Leibler divergence KL(p, pÌ‚
MÌ‚
)
between the data distribution with the true probabilities p = (p1, . . . , pn) and its empirical distri-
bution generated by pÌ‚
MÌ‚
given by
KL(p, pÌ‚
MÌ‚
) =
1
n
nâˆ‘
i=1
{
pi ln
(
pi
pÌ‚
MÌ‚i
)
+ (1âˆ’ pi) ln
(
1âˆ’ pi
1âˆ’ pÌ‚
MÌ‚i
)}
, (4)
and measured the accuracy of pÌ‚
MÌ‚
by the corresponding Kullback-Leibler risk EKL(p, pÌ‚
MÌ‚
) (in
fact, the Kullback-Leibler divergence KL(Â·, Â·) in Abramovich and Grinshtein, 2016 was defined as
n times KL(Â·, Â·) in this paper).
4
Assumption (A). Assume that there exists 0 < Î´ < 1/2 such that Î´ < pi < 1âˆ’Î´ or, equivalenelty,
there exists C0 > 0 such that |Î²txi| < C0 in (1) for all i = 1, . . . , n.
Assumption (A) prevents the variances V ar(Yi) = pi(1âˆ’ pi) to be infinitely close to zero.
Consider a set of models of size at most d0, where 1 â‰¤ d0 â‰¤ r. Obviously, |M | â‰¤ d0 iff
||Î²||0 â‰¤ d0. Abramovich and Grinshtein (2016) showed that for the complexity penalty
Pen(|M |) = c |M | ln de
|M |
, |M | = 1, . . . , r âˆ’ 1 and Pen(r) = c r (5)
in (3), where c > 4Î´(1âˆ’Î´) , under Assumption (A), the upper bound of the Kullnack-Leibler risk is
given by
sup
Î²:||Î²||0â‰¤d0
EKL(p, pÌ‚
MÌ‚
) â‰¤ C 1
Î´(1âˆ’ Î´)
min
(
d0 ln
de
d0
, r
)
n
(6)
for some C > 0. They also derived the corresponding minimax lower bounds for the Kullback-
Leibler risk and showed that for weakly-collinear design, the upper bound in (6) is of the optimal
order (in the minimax sense).
In what follows we utilize (6) to derive the upper bounds for the misclassification excess risk
of the corresponding plug-in classifier Î·Ì‚
MÌ‚
(x) = I{Î²Ì‚
t
MÌ‚x â‰¥ 0}. To gain more insight into the
complexity penalty (5) within classification framework we present the following lemma on the
Vapnik-Chervonenskis (VC) dimension of the set of all d0-sparse linear classifiers :
Lemma 1. Let C(d0) = {Î·(x) = I{Î²tx â‰¥ 0} : Î² âˆˆ Rd, ||Î²||0 â‰¤ d0} be the set of all d0-sparse
linear classifiers and V (C(d0)) its VC-dimension. Then,
d0 log2
(
2d
d0
)
â‰¤ V (C(d0)) â‰¤ 2d0 log2
(
de
d0
)
(7)
Thus, the complexity penalty Pem(|M |) in (5) is essentially proportional to the VC-dimension
of the corresponding class of |M |-sparse linear classifiers C(|M |).
3 Main results
3.1 Misclassification excess risk bounds
In this section we apply the selected model MÌ‚ in (3) for classification and derive the bounds for
the corresponding misclassification exceess risk.
We consider the fixed design. For a given design matrix X the misclassification error of a
classifier Î· is RX(Î·) =
1
n
âˆ‘n
i=1 P (Yi 6= Î·(xi)). Following our previous arguments define a (linear)
plug-in classifier
Î·Ì‚
MÌ‚
(x) = I{Î²Ì‚
t
MÌ‚x â‰¥ 0} (8)
5
and consider its misclassification excess risk EX(Î·Ì‚MÌ‚ , Î·
âˆ—) = ERX(Î·Ì‚MÌ‚ ) âˆ’ RX(Î·
âˆ—), where recall that
the (ideal) Bayes classifier Î·âˆ—(x) = I{Î²tx â‰¥ 0} with the true (unknown) Î² in (1). The remarkable
results of Zhang (2004) and Bartlett, Jordan and McAuliffe (2006) establish the relationships
between the Kullback-Leibler risk EKL(p, pÌ‚
MÌ‚
) and the misclassification excess risk EX(Î·Ì‚MÌ‚ , Î·
âˆ—):
EX(Î·Ì‚MÌ‚ , Î·
âˆ—) â‰¤
âˆš
2EKL(p, pÌ‚
MÌ‚
) (9)
Thus, (6) and (9) imply immediately the following upper bound for EX(Î·Ì‚MÌ‚ , Î·
âˆ—):
Theorem 1. Consider a sparse logistic regression model (1) with ||Î²||0 â‰¤ d0. Let MÌ‚ be a model
selected in (3) with the complexity penalty (5) and consider the corresponding plug-in classifier
Î·Ì‚
MÌ‚
(x) in (8). Under Assumption (A),
sup
Î·âˆ—âˆˆC(d0)
EX(Î·Ì‚MÌ‚ , Î·
âˆ—) â‰¤ C1
âˆšâˆšâˆšâˆš 1
Î´(1âˆ’ Î´)
min
(
d0 ln
de
d0
, r
)
n
(10)
for some C1 > 0, simultaneously for all 1 â‰¤ d0 â‰¤ r.
We now show that there exists a design matrix X0 for which the upper bound for the misclas-
sification excess risk (10) is essentially sharp.
Consider the set of all possible d0-sparse linear classifiers C(d0) defined in Lemma 1 and the
case, where a Bayes classifier Î·âˆ—(x) is not perfect, that is, R(Î·âˆ—) > 0, which is sometimes called as
an agnostic model. Then, the following result holds:
Theorem 2. Consider a d0-sparse agnostic logistic regression model (1), where 2 â‰¤ d0 log2 2dd0 â‰¤ n.
Then, there exists an nÃ— d design matrix X0 such that
inf
Î·Ìƒ
sup
Î·âˆ—âˆˆC(d0)
EX0(Î·Ìƒ, Î·âˆ—) â‰¥ C2
âˆš
d0 ln
de
d0
n
(11)
for some constant C2 > 0, where the infimum is taken over all classifiers Î·Ìƒ based on the data
(X0,Y).
Theorem 2 is a particular case of Theorem 4 from Section 3.2 below.
The upper and lower bounds established in Theorem 1 and Theorem 2 allow one to derive
the asymptotic minimax rate for misclassification excess risk in sparse logistic regression model as
n increases. We allow the number of features d to increase with n as well and even faster than
n (d  n setup). The following immediate Corollary 1 shows that the proposed classifier Î·Ì‚
MÌ‚
is
asymptotically minimax in terms of â€œthe worst caseâ€ design and adaptive to the unknown sparsity:
Corollary 1. Consider a d0-sparse logistic regression agnostic model (1), where d0 satisfies 2 â‰¤
d0 log2
(
2d
d0
)
â‰¤ n. Then, as n and d increase, for a fixed Î´ in Assumption (A),
6
1. The asymptotic minimax misclassification excess risk supX inf Î·Ìƒ supÎ·âˆ—âˆˆC(d0) EX(Î·Ìƒ, Î·
âˆ—) is of the
order âˆšâˆšâˆšâˆšd0 ln(ded0)
n
âˆ¼
âˆš
V (C(d0))
n
2. The classifier Î·Ì‚
MÌ‚
defined in (8), where the model MÌ‚ was selected by (3) with the complexity
penalty (5), attains the minimax rates simultaneously for all 2 â‰¤ d0 log2
(
2d
d0
)
â‰¤ n.
Finally, we note that if the considered logistic regression model is misspecified and the Bayes
classifier Î·âˆ— is not linear, we still have the following risk decomposition
RX(Î·Ì‚MÌ‚ )âˆ’RX(Î·
âˆ—) =
(
RX(Î·Ì‚MÌ‚ )âˆ’RX(Î·
âˆ—
L)
)
+ (RX(Î·
âˆ—
L)âˆ’RX(Î·âˆ—)) , (12)
where Î·âˆ—L = arg minÎ·âˆˆC(d)RX(Î·) is the best (ideal) linear classifier. Our previous arguments can
then be applied to the first term in the RHS of (12) representing the estimation error, while the
second term is an approximation error and measures the ability of linear classifiers to perform
as good as Î·âˆ—. Enriching the class of classifiers may improve the approximation error but will
necessarily increase the stochastic component in (12). In a way, it is similar to the variance/bias
tradeoff in regression.
3.2 Tighter risk bounds under low-noise condition
The main challenges for any classifier occur in regions, where the true p(x) is close to 1/2 and,
therefore, it is hard to predict the class label accurately. However, for regions, where p(x) is bounded
away from 1/2 (margin or, known also, as low-noise condition), the bounds for misclassification
excess risk established in the previous Section 3.1 can be improved. Following Massart and NeÌdeÌlec
(2006) introduce the following low-noise assumption:
Assumption (B). Consider the logistic regression model (1) and assume that there exists 0 â‰¤ h <
1/2 such that
|pi âˆ’ 1/2| â‰¥ h or, equivalently, |Î²txi| â‰¥ ln
(
1 + 2h
1âˆ’ 2h
)
(13)
for all i = 1, . . . , n.
Somewhat more general low noise conditions are considered in Mammen and Tsybakov (1999)
and Tsybakov (2004).
For a given design matrix X, define CX(d0, h) = {Î· : Î· âˆˆ C(d0), |Î²txi| â‰¥ ln 1+2h1âˆ’2h , i = 1, . . . , n}.
Evidently, CX(d0, 0) = C(d0) for any X.
Theorem 3 below establishes the upper bound for the misclassification excess risk of the proposed
classifier Î·Ì‚
MÌ‚
under the additional low noise Assumption (B):
7
Theorem 3. Consider a sparse logistic regression model (1), where ||Î²||0 â‰¤ d0. Assume that there
exist 0 < h < âˆ† < 1/2 such that
h â‰¤ |pi âˆ’ 1/2| â‰¤ âˆ† (14)
for all i = 1, . . . , n.
Let MÌ‚ be a model selected in (3) with the complexity penalty (5) and consider the corresponding
classifier Î·Ì‚
MÌ‚
(x) in (8). Then, for all 1 â‰¤ d0 â‰¤ r,
sup
Î·âˆ—âˆˆCX0 (d0,h)
EX(Î·Ì‚MÌ‚ , Î·
âˆ—) â‰¤ C1 min
ï£«ï£¬ï£¬ï£­
âˆšâˆšâˆšâˆš 1âˆ’ 4h2
1âˆ’ 4âˆ†2
min
(
d0 ln
de
d0
, r
)
n
,
1âˆ’ 4h2
1âˆ’ 4âˆ†2
min
(
d0 ln
de
d0
, r
)
nh
ï£¶ï£·ï£·ï£¸
(15)
for some C1 > 0.
Thus, if the margin parameter is large enough, namely, h >
âˆš
d0 ln
de
d0
n , the misclassification
excess risk bound (10) is reduced. Similar results for random design were obtained in Massart and
NeÌdeÌlec (2006).
Similar to the previous Section 3.1, one can construct a design matrix for which the upper
bound (15) is sharp:
Theorem 4. Consider a d0-sparse agnostic logistic regression model (1) with 2 â‰¤ d0 log2 2dd0 â‰¤ n.
There exists an nÃ— d design matrix X0 such that under Assumption (B)
inf
Î·Ìƒ
sup
Î·âˆ—âˆˆCX0 (d0,h)
EX0(Î·Ìƒ, Î·âˆ—) â‰¥ C2 min
ï£«ï£­
âˆš
d0 ln
de
d0
n
,
d0 ln
de
d0
nh
ï£¶ï£¸ (16)
for some C2 > 0.
The design matrix X0 is constructed explicitly in the proof of Theorem 4 in the Appendix. Note
that Theorem 2 may be viewed as a particular case of Theorem 4 for h = 0.
3.3 Logistic Slope classifier
Finding MÌ‚ in (3) requires generally a combinatorial search over all possible models in M that makes
the use of complexity penalties to be computationally problematic for large number of features.
Greedy algorithms (e.g., forward selection) approximate the global solution of (3) by a stepwise
sequence of local ones. However, they require strong constraints on the design matrix X that can
hardly hold for high-dimensional data. A more reasonable approach is convex relaxation, where
the original combinatorial problem is replaced by a related convex surrogate. Thus, for linear-type
complexity penalties of the form Pen(|M |) = Î»|M | = Î»||Î²||0, the celebrated Lasso replaces the
8
l0-(quasi) norm by l1-norm:
Î²Ì‚Lasso = arg min
Î²Ìƒ
{
nâˆ‘
i=1
(
ln
(
1 + exp(Î²Ìƒ
t
xi)
)
âˆ’ Î²Ìƒ
t
xiYi
)
+ Î»||Î²Ìƒ||1
}
Assume that all the columns of the design matrix X are normalized to have unit norms. From the
results of Van de Geer (2008) it follows that under an assumption similar to Assumption (A) and
certain extra conditions on X, the logistic Lasso with a tuning parameter Î» of the order
âˆš
ln d results
in sub-optimal Kullback-Leibler risk O
(
d0
n ln d
)
and, therefore, sub-optimal misclassification excess
risk O
(âˆš
d0
n ln d
)
.
Recently, for Gaussian regression, Bogdan et al. (2015) suggested the Slope estimator â€“ a
penalized maximum likelihood estimator with a sorted l1-norm penalty defined as follows:
Î²Ì‚Slope = arg min
Î²Ìƒ
ï£±ï£²ï£³||Y âˆ’XÎ²Ìƒ||2 +
dâˆ‘
j=1
Î»j |Î²Ìƒ|(j)
ï£¼ï£½ï£¾ , (17)
where || Â· || denotes the Euclidean norm in Rn, |Î²Ìƒ|(1) â‰¥ . . . â‰¥ |Î²Ìƒ|(d) are the descendingly ordered
absolute values of Î²Ìƒj â€™s and Î»1 â‰¥ . . . â‰¥ Î»d > 0 are the tuning parameters. It is a convex minimization
problem. Bellec, LecueÌ and Tsybakov (2016) proved that under certain conditions on X with
normalized columns, the quadratic risk of the Slope estimator (17) with Î»j = A
âˆš
ln(2d/j) for a
certain constant A is of the rate-optimal order O
(
d0
n ln(
de
d0
)
)
.
We will now extend the above results for Slope for logistic regression and, in fact, for a general
GLM (see the Appendix C). Modifying the definition of the Slope estimator for the considered
logistic regression model (1), define
Î²Ì‚Slope = arg min
Î²Ìƒ
ï£±ï£²ï£³
nâˆ‘
i=1
(
ln
(
1 + exp(Î²Ìƒ
t
xi)
)
âˆ’ Î²Ìƒ
t
xiYi
)
+
dâˆ‘
j=1
Î»j |Î²Ìƒ|(j)
ï£¼ï£½ï£¾ , (18)
where Î»1 â‰¥ . . . â‰¥ Î»d > 0. Note that (18) is also a convex program that makes the logistic
Slope estimator computationally feasible for high-dimensional data. The corresponding estimated
probabilities are pÌ‚Slope,i =
exp(Î²Ì‚
t
Slopexi)
1+exp(Î²Ì‚
t
Slopexi)
, i = 1, . . . , n.
As usual, any convex relaxation requires certain extra conditions on the restricted minimal
eigenvalue of the design matrix X over some set of vectors. In particular, similar to Gaussian
regression considered in Bellec, LecueÌ and Tsybakov (2016), we assume the following Weighted
Restricted Eigenvalue (WRE) condition for Slope estimator (18) :
Assumption. (WRE(d0, c0) condition) Consider the sparse logistic regression model (1) with
||Î²||0 â‰¤ d0, where the columns of the desing matrix X are normalized to have unit norms. Consider
the set S(d0, c0) = {u âˆˆ Rd :
âˆ‘d
j=1
âˆš
ln(2d/j)|u|(j) â‰¤ (1 + c0)||u||
âˆšâˆ‘d0
j=1 ln(2d/j)} and assume
that Xu 6= 0 for any u 6= 0 âˆˆ S(d0, c0).
9
An interesting discussion on WRE condition and those required in Lasso is given in Section 8
of Bellec, LecueÌ and Tsybakov (2016).
Define a restricted minimal eigenvalue Îº(d0, c0) as follows :
Îº(d0, c0) = min
uâˆˆS(d0,c0);u6=0
||Xu||
||u||
> 0
Theorem 5. Consider a sparse logistic regression model (1), where ||Î²||0 â‰¤ d0, the columns of the
design matrix X are normalized to have unit norms and, in addition, X satisfies the WRE(d0, c0)
condition for some c0 > 1. Assume that Assumption (A) holds.
Let the tuning parameters
Î»j = A
c0 + 1
c0 âˆ’ 1
âˆš
ln(2d/j), j = 1, . . . , d (19)
with the constant A â‰¥ 20
âˆš
6.
Then,
sup
Î²:||Î²||0â‰¤d0
EKL(p, pÌ‚Slope) â‰¤ 8A2
c20
(c0 âˆ’ 1)2
1
Î´(1âˆ’ Î´)
(
2Ï€ + 8
ln(2d)
+
1
Îº2(d0, c0)
)
d0
n
ln
(
2de
d0
)
(20)
for all 1 â‰¤ d0 â‰¤ r.
Note that Î»j â€™s in (19) are of the same form as those in Bellec, LecueÌ and Tsybakov (2016) for
Gaussian regression but differ in a constant A.
Theorem 5 is a particular case of Theorem 6 for a general GLM (see Appendix C).
Using (9) one immediately gets the corresponding result for the misclassification exceess risk of
the logistic Slope classifier:
Corollary 2. Assume all the conditions of Theorem 5 and choose Î»j according to (19). Consider
the logistic Slope classifier Î·Ì‚Slope(x) = I{Î²Ì‚
t
Slopex â‰¥ 0}. Then,
EX(Î·Ì‚Slope, Î·âˆ—) = O
ï£«ï£­
âˆš
d0 ln
de
d0
n
ï£¶ï£¸ (21)
Thus, the logistic Slope estimator is computationally feasible and yet achieves the optimal rates
under the additional WRE(d0, c0) condition on the design for all but very dense models for which
d0 ln(
de
d0
) > r (see Theorem 1).
Acknowledgments
The work was supported by the Israel Science Foundation (ISF), grant ISF-820/13. The authors
would like to thank Noga Alon for his help in the proof of Lemma 1 and Alexander Tsybakov for
valuable remarks.
10
References
[1] Abramovich, F. and Grinshtein, V. (2010). MAP model selection in Gaussian regression. Electr.
J. Statist. 4, 932â€“949.
[2] Abramovich, F. and Grinshtein, V. (2016). Model selection and minimax estimation in gener-
alized linear models. IEEE Trans. Inf. Theory 62, 3721-3730.
[3] Barron, A., BirgeÌ, L. and Massart P. (1999). Risk bounds for model selection via penalization.
Prob. Theory Relat. Fields, 113, 301-413.
[4] Bartlett, P.L., Jordan, M.I. and McAuliffe, J.D. (2006). Convexity, classification, and risk
bounds. J. Amer. Statist. Assoc., 101, 138â€“156.
[5] Bellec, P.C., LecueÌ, G. and Tsybakov, A. (2016). Slope meets Lasso: improved oracle bounds
and optimaility. arXiv:1605.08651.
[6] Bickel, P. and Levina, E. (2004). Some theory for Fisherâ€™s discriminant function, â€˜naive Bayesâ€™,
and some alternatives where there are more variables than observations. Bernoulli, 10, 989-
1010.
[7] BirgeÌ, L. and Massart, P. (2001). Gaussian model selection. J. Eur. Math. Soc. 3, 203â€“268.
[8] BirgeÌ, L. and Massart, P. (2007). Minimal penalties for Gaussian model selection. Probab.
Theory Relat. Fields 138, 33â€“73.
[9] Bogdan, M., van den Berg, E., Sabatti, C., Su, W. and CandeÌs, E. (2015). SLOPE â€“ adaptive
variable selection via convex programming. Ann. Appl. Statist., 9, 1103â€“1140.
[10] Boucheron, S., Bousquet, O., and Lugosh, G. (2005) Theory of classification: a survey of some
recent advances. ESAIM: Prob. Statist., 9, 323-375.
[11] Devroye, L., GyoÌˆrfi, L. and Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition.
Springer, New York.
[12] Fan, J., and Fan, Y. (2008). High-dimensional classification using feature annealed indepen-
dence rules. Ann. Statist., 36, 2605â€“2637.
[13] Giraud, C. (2015). Introduction to High-Dimensional Statistics. CRC Press, Boca Raton.
[14] Mammen, E. and Tsybakov, A. (1999). Smooth discrimination analysis. Ann. Statist. 27,
1808â€“1829.
[15] Massart, P. and NeÌdeÌlec, E. (2006). Risk bounds for statistical learning. Ann. Statist. 34,
2326â€“2366.
11
[16] McCullagh, P. and Nelder, J. A. (1989). Generalized Linear Models, 2nd ed. Chapman and
Hall, London.
[17] Rigollet, P. and Tsybakov, A. (2011). Exponential screening and optimal rates of sparse esti-
mation. Ann. Statist. 39, 731â€“771.
[18] Tsybakov, A. (2004). Optimal aggregation of classifiers in statistical learning. Ann. Statist.
32, 135â€“166.
[19] van de Geer, S. (2008). High-dimensional generalized linear models and the Lasso. Ann. Statist.
36, 614â€“645.
[20] Vapnik, V.N. (2000). The Nature of Statistical Learning, 2nd ed. Springer, New York.
[21] Verzelen, N. (2012). Minimax risks for sparse regressions: Ultra-high dimensionals phe-
nomenon. Electr. J. Statist. 6, 38â€“90.
[22] Zhang, T. (2004). Statistical behavior and consistency of classification methods based on con-
vex risk minimization. Ann. Statist. 32, 56â€“85.
Appendix
Throughout the proofs we use various generic positive constants, not necessarily the same each
time they are used even within a single equation.
Appendix A: Proof of Lemma 1
Denote for brevity V = V (C(d0)). For any fixed subset of d0 Î²j â€™s the VC of the corresponding set
of d0-dimensional linear classifiers is known to be d0 (e.g., Giraud, 2015, Exercise 9.5.2). Then,
by Sauerâ€™s lemma the maximal number of different labeling of V points in Rd0 that such set of
classifiers can produce is
âˆ‘d0
k=0
(
V
k
)
â‰¤
(
V e
d0
)d0
(see, e.g., Giraud, 2015, Section 9.2.2). The overall
number of different labeling is, therefore,
(
d
d0
)âˆ‘d0
k=0
(
V
k
)
, and by the definition of V (C(d0)) we have
2V â‰¤
(
d
d0
) d0âˆ‘
k=0
(
V
k
)
â‰¤
(
de
d0
)d0 (V e
d0
)d0
â‰¤
(
de
d0
)2d0
that implies an upper bound V â‰¤ 2 d0 log2
(
de
d0
)
.
On the other hand, take k = log2(2d/d0) and let K be the k Ã— 2kâˆ’1 matrix whose columns
are all possible vectors with {âˆ’1, 1} entries with the first entry 1. Note that d02kâˆ’1 = d. Let
W be the d0k Ã— d block-wise matrix consisting of d0 Ã— d0 blocks, each being a k Ã— 2kâˆ’1 matrix,
where the diagonal matrices are copies of K, while all others are zero matrices. Thus, W has
d0k = d0 log2(2d/d0) rows. It is easay to verify that these rows are shattered by halfspaces whose
12
supporting vectors w have a single nonzero Â±1 entry in each of the d0 blocks and, therefore,
V â‰¥ d0 log2(2d/d0).
Appendix B: Tighter bounds for low-noise condition
B1: Proof of Theorem 3
Assumption (14) obviously implies Assumption (A) with Î´ = 1/2 âˆ’ âˆ†. In addition, under (14),
V ar(Yi) = pi(1âˆ’pi) â‰¤ (1/2âˆ’h)(1/2+h) = (1âˆ’4h2)/4. Hence, adapting the results of Abramovich
and Grinshtein (2016) on Kullback-Leibler risk in general GLM framework for logistic regression,
the upper bound (6) for EKL(p, pÌ‚
MÌ‚
) can be improved:
sup
Î²:||Î²||0â‰¤d0
EKL(p, pÌ‚
MÌ‚
) â‰¤ C 1âˆ’ 4h
2
1âˆ’ 4âˆ†2
min
(
d0 ln
de
d0
, r
)
n
(22)
and, therefore, from (9) we have
sup
Î·âˆ—âˆˆCX(d0,h)
EX(Î·Ì‚MÌ‚ , Î·
âˆ—) â‰¤ C1
âˆšâˆšâˆšâˆš 1âˆ’ 4h2
1âˆ’ 4âˆ†2
min
(
d0 ln
de
d0
, r
)
n
On the other hand, we can adapt the general Theorem 3 of Bartlett, Jordan and McAuliffe (2006)
for Ïˆ(f) = (1/2) ((1âˆ’ f) ln(1âˆ’ f) + (1 + f) ln(1 + f)) â‰¥ f2/2 corresponding to the Kullback-
Leibler risk (Zhang, 2004, Section 3.5), Î± = 1 corresponding to (14) and c = 1/(2âˆ†) to get
EX(Î·Ì‚MÌ‚ , Î·
âˆ—) â‰¤ 4
h
EKL(p, pÌ‚
MÌ‚
)
Applying (22) implies then
EX(Î·Ì‚MÌ‚ , Î·
âˆ—) â‰¤ C1
1âˆ’ 4h2
1âˆ’ 4âˆ†2
min
(
d0 ln
de
d0
, r
)
nh
B2: Proof of Theorem 4
For any Î·Ìƒ and Î·âˆ— âˆˆ C(d0, h) we have
EX(Î·Ìƒ, Î·âˆ—) =
1
n
nâˆ‘
i=1
P (Î·Ìƒi 6= Î·âˆ—i )|2pi âˆ’ 1| â‰¥
2h
n
E
(
nâˆ‘
i=1
I{Î·Ìƒi 6= Î·âˆ—i }
)
=
2h
n
E||Î·Ìƒ âˆ’ Î·âˆ—||1 (23)
for any X.
As we have mentioned, the worst case scenario for classification is when pi = 1/2 Â± h or,
equivalently, |Î²txi| = ln
(
1+2h
1âˆ’2h
)
. Let V = d0 log2(2d/d0). In the proof of Lemma 1 we constructed
explicitly the matrix WVÃ—d whose rows w1, . . . ,wV are shattered by C(d0). Then, for any p =
{12 Â± h}
V there exists Î² âˆˆ Rd such that ||Î²||0 â‰¤ d0 and Î²twi = ln pi1âˆ’pi = Â± ln
1+2h
1âˆ’2h for all
13
i = 1, . . . , V . Define also the corresponding binary vector b with bi = I{Î²twi â‰¥ 0}, that is, bi = 1
if pi =
1
2 + h and bi = 0 if pi =
1
2 âˆ’ h. Obviously, the set of all bâ€™s is a hypercube H
V = {0, 1}V .
Define now a nÃ— d design matrix X0 with Îº rows of w1, Îº rows of w2, ..., Îº rows of wVâˆ’1 and
the remaining nâˆ’ (V âˆ’ 1)Îº rows of wV , where an integer 1 â‰¤ Îº â‰¤ b nVâˆ’1c will be defined later.
The proof will now follow the general scheme of the proof of Theorem 4 of Massart and NeÌdeÌlec
(2006) but with necessary modifications for the fixed design.
For any p âˆˆ {12 Â± h}
V and the corresponding b âˆˆ HV define an n-dimensional indicator vector
Î·b = (b1, . . . , b1ï¸¸ ï¸·ï¸· ï¸¸
Îº
, . . . , bVâˆ’1, . . . , bVâˆ’1ï¸¸ ï¸·ï¸· ï¸¸
Îº
, bV , . . . , bVï¸¸ ï¸·ï¸· ï¸¸
nâˆ’(Vâˆ’1)Îº
) and let CÌƒX0(d0, h) = {Î·b, b âˆˆ HV }. By its design,
CÌƒX0(d0, h) âŠ† {Î· : Î· âˆˆ C(d0), |Î²tx0i| = ln 1+2h1âˆ’2h , i = 1, . . . , n} âŠ† CX0(d0, h).
Hence, we can reduce the minimax risk over the entire CX0(d0, h) to CÌƒX0(d0, h):
inf
Î·Ìƒ
sup
Î·âˆ—âˆˆCX0 (d0,h)
EX0(Î·Ìƒ, Î·âˆ—) â‰¥ inf
Î·Ìƒ
sup
Î·âˆ—âˆˆCÌƒX0 (d0,h)
EX0(Î·Ìƒ, Î·âˆ—) (24)
Furthermore, for a given Î·Ìƒ, define Î·Ìƒâˆ— = arg min
Î·âˆˆCÌƒ(d0,h) ||Î·Ìƒâˆ’ Î·||1. Then, for any Î·
âˆ— âˆˆ CÌƒX0(d0, h)
we have
||Î·Ìƒâˆ— âˆ’ Î·âˆ—||1 â‰¤ ||Î·Ìƒâˆ— âˆ’ Î·Ìƒ||1 + ||Î·Ìƒ âˆ’ Î·âˆ—||1 â‰¤ 2||Î·Ìƒ âˆ’ Î·âˆ—||1 (25)
and, therefore, from (23)-(25)
inf
Î·Ìƒ
sup
Î·âˆ—âˆˆCX0 (d0,h)
EX0(Î·Ìƒ, Î·âˆ—) â‰¥
h
n
inf
Î·Ìƒâˆ—âˆˆCÌƒX0 (d0,h)
sup
Î·âˆ—âˆˆCÌƒX0 (d0,h)
E||Î·Ìƒâˆ— âˆ’ Î·âˆ—||1
â‰¥ h
n
Îº inf
bÌƒâˆˆHV
sup
bâˆ—âˆˆHV
E
(
Vâˆ’1âˆ‘
i=1
I{bÌƒi 6= bâˆ—i }
)
,
(26)
where bÌƒ,bâˆ— âˆˆ HV are the binary vectors corresponding to Î·Ìƒâˆ— and Î·âˆ— respectively (see above).
By a simple calculus one can verify that the square Hellinger distanceH2
(
Bin(1, 12 + h), Bin(1,
1
2 âˆ’ h)
)
between two Bernoulli distributions Bin(1, 12 +h) and Bin(1,
1
2âˆ’h) is 1âˆ’
âˆš
1âˆ’ 4h2. For any b âˆˆ HV
and the corresponding Î·b define pb âˆˆ Rn as follows: pbi = 12 + h if Î·bi = 1 and pbi =
1
2 âˆ’ h if
Î·bi = 0, i = 1, . . . ,Îº(V âˆ’ 1), and pbi = 0, i = Îº(V âˆ’ 1) + 1, . . . , n. Then, for any b1,b2 âˆˆ HV and
the corresponding pb1 and pb2 we have
H2(pb1 ,pb2) =
1
n
nâˆ‘
i=1
H2 (Bin(1, pb1i), Bin(1, pb2i)) =
Îº
n
(1âˆ’
âˆš
1âˆ’ 4h2)
Vâˆ’1âˆ‘
i=1
I{b1i 6= b2i}
Hence, applying the version of Assouadâ€™s lemma given in Lemma 7 of Barron, BirgeÌ and Massart
(1999) yields
inf
bÌƒâˆˆHV
sup
bâˆ—âˆˆHV
E
(
Vâˆ’1âˆ‘
i=1
I{bÌƒi 6= bâˆ—i }
)
â‰¥ V âˆ’ 1
2
(
1âˆ’
âˆš
2Îº(1âˆ’
âˆš
1âˆ’ 4h2
)
â‰¥ V âˆ’ 1
2
(
1âˆ’
âˆš
8Îºh2
)
14
that togther with (26) implies
inf
Î·Ìƒ
sup
Î·âˆ—âˆˆCX0 (d0,h)
EX0(Î·Ìƒ, Î·âˆ—) â‰¥ Îº
h
n
V âˆ’ 1
2
(
1âˆ’
âˆš
8Îºh2
)
(27)
Consider two cases.
Case 1. h â‰¤ 16 .
For h â‰¥
âˆš
Vâˆ’1
18n , apply (27) for Îº = b
1
18h2
c (note that 2 â‰¤ Îº â‰¤ b nVâˆ’1c), to get
inf
Î·Ìƒ
sup
Î·âˆ—âˆˆCX0 (d0,h)
EX0(Î·Ìƒ, Î·âˆ—) â‰¥
V âˆ’ 1
216nh
â‰¥ C2
d0 ln(
de
d0
)
nh
For h <
âˆš
Vâˆ’1
18n , one can follow all the above arguments for hÌƒ =
âˆš
Vâˆ’1
18n and the corresponding
Îº = b nVâˆ’1c to have
inf
Î·Ìƒ
sup
Î·âˆ—âˆˆCX0 (d0,h)
EX0(Î·Ìƒ, Î·âˆ—) â‰¥ inf
Î·Ìƒ
sup
Î·âˆ—âˆˆCX0 (d0,hÌƒ)
EX0(Î·Ìƒ, Î·âˆ—) â‰¥ C2
âˆš
d0 ln(
de
d0
)
n
Case 2. h > 16 .
Set Îº = 1 and note that CX0(d0, 12) âŠ† CX0(d0, h) for any 0 â‰¤ h â‰¤
1
2 . Hence, (26) implies
inf
Î·Ìƒ
sup
Î·âˆ—âˆˆCX0 (d0,h)
EX0(Î·Ìƒ, Î·âˆ—) â‰¥ inf
Î·Ìƒ
sup
Î·âˆ—âˆˆCX0 (d0,
1
2
)
EX0(Î·Ìƒ, Î·âˆ—) â‰¥
1
2n
inf
bÌƒâˆˆHV
sup
bâˆ—âˆˆHV
E
(
Vâˆ’1âˆ‘
i=1
I{bÌƒi 6= bâˆ—i }
)
â‰¥ 1
2n
inf
bÌƒâˆˆHV
1
2V
âˆ‘
bjâˆˆHV
E
(
Vâˆ’1âˆ‘
i=1
I{bÌƒi 6= bâˆ—i }
)
=
1
2n
inf
bÌƒâˆˆHV
Vâˆ’1âˆ‘
i=1
1
2V
2Vâˆ‘
j=1
P (bÌƒi 6= bji)
By obvious combinatoric calculus, for any (binary) vector bÌƒ, 1
2V
âˆ‘2V
j=1 P (bÌƒi 6= bji) =
1
2 for any i
and, therefore,
inf
Î·Ìƒ
sup
Î·âˆ—âˆˆCX0 (d0,h)
EX0(Î·Ìƒ, Î·âˆ—) â‰¥
V âˆ’ 1
4n
â‰¥ C2
d0 ln
de
d0
nh
for large h > 16 (in fact, larger than any fixed h0).
Appendix C: Slope estimator for a general GLM
Consider a GLM setup with a response variable Y and a set of d predictors x1, ..., xd. We observe
a series of independent observations (xi, Yi), i = 1, . . . , n, where the design points xi âˆˆ Rp are
deterministic. The distribution fÎ¸i(y) of Yi belongs to a (one-parameter) natural exponential family
with a natural parameter Î¸i and a scaling parameter a:
fÎ¸i(y) = exp
{
yÎ¸i âˆ’ b(Î¸i)
a
+ c(y, a)
}
(28)
15
The function b(Â·) is assumed to be twice-differentiable. In this case E(Yi) = bâ€²(Î¸i) and V ar(Yi) =
abâ€²â€²(Î¸i). To complete GLM we assume the canonical link Î¸i = Î²
txi or, equivalently, in the matrix
form, Î¸ = XÎ², where XnÃ—p is the design matrix and Î² âˆˆ Rp is a vector of the unknown regression
coefficients. The logistic regression (1) is a particular case of a general GLM (28) for the Bernoulli
distribution Bin(1, pi), where the natural parameter is Î¸ = ln
p
1âˆ’p , b(Î¸0) = ln(1 + e
Î¸) and a = 1.
Following Abramovich and Grinshtein (2016) assume the extended version of Assumption (A)
for GLM :
Assumption (Aâ€™).
1. Assume that Î¸i âˆˆ Î˜, where the parameter space Î˜ âŠ† R is a closed (finite or infinite) interval.
2. Assume that there exist constants 0 < L â‰¤ U < âˆ such that the function bâ€²â€²(Â·) satisfies the
following conditions:
(a) suptâˆˆR b
â€²â€²(t) â‰¤ U
(b) inftâˆˆÎ˜ b
â€²â€²(t) â‰¥ L
Conditions on bâ€²â€²(Â·) in Assumption (Aâ€™) are intended to exclude two degenerate cases, where
the variance V ar(Y ) is infinitely large or small. They also ensure strong convexity of b(Â·) over
Î˜. For the binomial distribution, U = 1/4 and Assumption (Aâ€™) reduces to Assumption (A) with
L = Î´(1âˆ’ Î´).
Recall that the Slope estimator is a penalized maximum likelihood with an ordered l1-norm
penalty and, therefore, defined for a GLM as follows:
Î²Ì‚Slope = arg min
Î²Ìƒ
ï£±ï£²ï£³âˆ’`(Î²Ìƒ) +
dâˆ‘
j=1
Î»j |Î²Ìƒ|(j)
ï£¼ï£½ï£¾ = arg minÎ²Ìƒ
ï£±ï£²ï£³b(XÎ²Ìƒ)t1âˆ’YtXÎ²Ìƒ +
dâˆ‘
j=1
Î»j |Î²Ìƒ|(j)
ï£¼ï£½ï£¾ (29)
for Î»1 â‰¥ Â· Â· Â· â‰¥ Î»d > 0. The corresponding Kullback-Leibler risk
EKL(Î¸, Î¸Ì‚Slope) =
1
n
1
a
(
bâ€²(Î¸)t(Î¸ âˆ’ E(Î¸Ì‚Slope))âˆ’ (b(Î¸)âˆ’ Eb(Î¸Ì‚Slope))t1
)
(30)
where Î¸ = XÎ² and Î¸Ì‚Slope = XÎ²Ì‚Slope (see Abramovich and Grinshtein, 2016).
Theorem 6. Consider a GLM (28), where ||Î²||0 â‰¤ d0, the columns of the design matrix X are
normalized to have unit norms and X satisfies the WRE(d0, c0) condition for some c0 > 1. Assume
that Assumption (Aâ€™) holds.
Let
Î»j = A
c0 + 1
c0 âˆ’ 1
âˆš
U
a
âˆš
ln(2d/j), j = 1, . . . , d, (31)
in (29) with the constant A â‰¥ 40
âˆš
6.
Then, simultaneously for all Î² âˆˆ Rd such that ||Î²||0 â‰¤ d0,
16
1.
P
(
KL(Î¸, Î¸Ì‚Slope) â‰¤
8A2
n
c20
(c0 âˆ’ 1)2
U
L
max
{(âˆš
Ï€/2 +
âˆš
2 ln âˆ†âˆ’1
)2
,
d0
Îº2(d0, c0)
ln
(
2de
d0
)})
â‰¥ 1âˆ’âˆ†
(32)
for any 0 < âˆ† < 1.
2.
EKL(Î¸, Î¸Ì‚Slope) â‰¤ 8A2
c20
(c0 âˆ’ 1)2
U
L
(
2Ï€ + 8
ln(2d)
+
1
Îº2(d0, c0)
)
d0
n
ln
(
2de
d0
)
(33)
Proof. Since Î²Ì‚Slope is the minimizer of (29),
âˆ’`(Î²Ì‚Slope) +
dâˆ‘
j=1
Î»j |Î²Ì‚Slope|(j) â‰¤ âˆ’`(Î²) +
dâˆ‘
j=1
Î»j |Î²|(j)
From (29) of Abramovich and Grinshtein (2016) one has
n KL(Î¸, Î¸Ì‚Slope) = `(Î²)âˆ’ `(Î²Ì‚Slope) +
1
a
(Y âˆ’ bâ€²(Î¸))t(Î¸Ì‚Slope âˆ’ Î¸)
(recall that the Kullback-Leibler divergence KL(Â·, Â·) in Abramovich and Grinshtein, 2016 was de-
fined as n times KL(Â·, Â·) in this paper). Thus,
KL(Î¸, Î¸Ì‚Slope) â‰¤
1
n a
(Y âˆ’ bâ€²(Î¸))t(Î¸Ì‚Slope âˆ’ Î¸) +
1
n
ï£«ï£­ dâˆ‘
j=1
Î»j |Î²|(j) âˆ’
dâˆ‘
j=1
Î»j |Î²Ì‚Slope|(j)
ï£¶ï£¸ (34)
Let u = Î²Ì‚Slopeâˆ’Î². Applying Lemma A.1 of Bellec, LecueÌ and Tsybakov (2016) with Ï„ = 0 implies
dâˆ‘
j=1
Î»j |Î²|(j) âˆ’
dâˆ‘
j=1
Î»j |Î²Ì‚Slope|(j) â‰¤
âˆšâˆšâˆšâˆš d0âˆ‘
j=1
Î»2j ||u|| âˆ’
dâˆ‘
j=d0+1
Î»j |u|(j) (35)
Consider now the first term of the RHS in (34). Since the distribution of Y belongs to the expo-
nential family with the bounded variance abâ€²â€²(Î¸) â‰¤ aU (Assumption (Aâ€™)), a centered zero mean
random variable Y âˆ’ bâ€²(Î¸) is sub-Gaussian with the scale factor
âˆš
aU , that is, Eet(Yâˆ’bâ€²(Î¸)) â‰¤ eaUt2/2
and, therefore, Ee(Yâˆ’bâ€²(Î¸))2/(6Ua) â‰¤ e. Applying Theorem 9.1 of Bellec, LecueÌ and Tsybakov (2016)
(adapted to our normalization conditions on the columns of X) with probability at least 1 âˆ’ âˆ†
yields
1
na
(Yâˆ’bâ€²(Î¸))t(Î¸Ì‚Slopeâˆ’Î¸) â‰¤
40
âˆš
6U
n
âˆš
a
max
ï£«ï£­ dâˆ‘
j=1
|u|(j)
âˆš
ln(2d/j) , ||Î¸Ì‚Slope âˆ’ Î¸||(
âˆš
Ï€/2 +
âˆš
2 ln âˆ†âˆ’1)
ï£¶ï£¸
(36)
17
Set
H(u) =
dâˆ‘
j=1
|u|(j)
âˆš
ln(2d/j) â‰¤ ||u||
âˆšâˆšâˆšâˆš d0âˆ‘
j=1
ln(2d/j) +
dâˆ‘
j=d0+1
|u|(j)
âˆš
ln(2d/j) = HÌƒ(u) (37)
and
G(u) = ||Î¸Ì‚Slope âˆ’ Î¸||
(âˆš
Ï€/2 +
âˆš
2 ln âˆ†âˆ’1
)
(38)
The proof will now go along the lines of the proof of Theorem 6.1 Bellec, LecueÌ and Tsybakov
(2016) for Gaussian regression with necessary adaptations to GLM and different normalization
conditions on the columns of X.
To prove (32) consider two cases.
Case 1. HÌƒ(u) â‰¤ G(u). In this case
||u|| â‰¤
||Î¸Ì‚Slope âˆ’ Î¸||(
âˆš
Ï€/2 +
âˆš
2 ln âˆ†âˆ’1)âˆšâˆ‘d0
j=1 ln(2d/j)
and, therefore, combining (31) and (34)-(38) with probability at least 1âˆ’âˆ† yields
KL(Î¸, Î¸Ì‚Slope) â‰¤
1
n
A
âˆš
U
a
2c0
c0 âˆ’ 1
||Î¸Ì‚Slope âˆ’ Î¸||(
âˆš
Ï€/2 +
âˆš
2 ln âˆ†âˆ’1)
â‰¤ 1
2n
(
A2U
a
(
2c0
c0 âˆ’ 1
)2
(
âˆš
Ï€/2 +
âˆš
2 ln âˆ†âˆ’1)2 + ||Î¸Ì‚Slope âˆ’ Î¸||2
) (39)
for any  > 0.
Lemma 1 of Abramovich and Grinshtein (2016) established the equivalence of the Kullback-
Leibler divergence KL(Î¸, Î¸Ì‚Slope) and the squared quadratic norm ||Î¸Ì‚Slopeâˆ’ Î¸||2 under Assumption
(Aâ€™):
L
2a
||Î¸Ì‚Slope âˆ’ Î¸||2 â‰¤ nKL(Î¸, Î¸Ì‚Slope) â‰¤
U
2a
||Î¸Ì‚Slope âˆ’ Î¸||2 (40)
Hence, taking  = L/(2a) in (39) after a straightforward calculus yields
KL(Î¸, Î¸Ì‚Slope) â‰¤
8
n
c20
(c0 âˆ’ 1)2
U
L
A2(
âˆš
Ï€/2 +
âˆš
2 ln âˆ†âˆ’1)2 (41)
with probability at least 1âˆ’âˆ†.
Case 2. HÌƒ(u) > G(u). Using the definition of Î»j â€™s in (31) and (34)-(38), with probability at least
1âˆ’âˆ† we have
KL(Î¸, Î¸Ì‚Slope) â‰¤
1
n
40
âˆš
6U
a
ï£«ï£­||u||
âˆšâˆšâˆšâˆš d0âˆ‘
j=1
ln(2d/j) +
dâˆ‘
j=d0+1
|u|(j)
âˆš
ln(2d/j)
ï£¶ï£¸
+
1
n
ï£«ï£­âˆšâˆšâˆšâˆš d0âˆ‘
j=1
Î»2j ||u|| âˆ’
dâˆ‘
j=d0+1
Î»j |u|(j)
ï£¶ï£¸
â‰¤ 1
n
ï£«ï£­ 2c0
c0 + 1
||u||
âˆšâˆšâˆšâˆš d0âˆ‘
j=1
Î»2j âˆ’
2
c0 + 1
dâˆ‘
j=d0+1
Î»j |u|(j)
ï£¶ï£¸
(42)
18
The KL(Î¸, Î¸Ì‚Slope) â‰¥ 0 and, therefore, the RHS of (42) is necessarily positive. Thus,
dâˆ‘
j=1
|u|(j)
âˆš
ln(2d/j) â‰¤ ||u||
âˆšâˆšâˆšâˆš d0âˆ‘
j=1
ln(2d/j) +
dâˆ‘
j=d0+1
|u|(j)
âˆš
ln(2d/j) â‰¤ (1 + c0)||u||
âˆšâˆšâˆšâˆš d0âˆ‘
j=1
ln(2d/j)
and, therefore, by WRE(d0, c0) condition (42),
KL(Î¸, Î¸Ì‚Slope) â‰¤
1
n
2c0
c0 + 1
||u||
âˆšâˆšâˆšâˆš d0âˆ‘
j=1
Î»2j â‰¤
1
n
2c0
c0 + 1
||Î¸Ì‚Slope âˆ’ Î¸||
Îº(c0, d0)
âˆšâˆšâˆšâˆš d0âˆ‘
j=1
Î»2j
â‰¤ 1
n
(
c20
(c0 + 1)2
âˆ‘d0
j=1 Î»
2
j
Îº2(c0, d0)
+ ||Î¸Ì‚Slope âˆ’ Î¸||2
)
for any  > 0. Taking  = L/(4a) and exploiting the equivalence between KL(Î¸, Î¸Ì‚Slope) and
||Î¸Ì‚Slope âˆ’ Î¸||2 in (40) imply that with probability at least 1âˆ’âˆ†,
KL(Î¸, Î¸Ì‚Slope) â‰¤
1
n
1
L
8ac20
(c0 + 1)2
âˆ‘d0
j=1 Î»
2
j
Îº2(c0, d0)
â‰¤ 8
n
c20
(c0 âˆ’ 1)2
U
L
A2
d0 ln(2de/d0)
Îº2(c0, d0)
,
where we used the definition (31) of Î»j â€™s and the upper bound
âˆ‘d0
j=1 ln(2d/j) â‰¤ d0 ln(2ed/d0) (see,
e.g., (2.7) of Bellec, LecueÌ and Tsybakov, 2016).
To prove the second statement (33) of the theorem denote Câˆ— = 8A2
c20
(c0âˆ’1)2 and note that
Câˆ—
1
n
U
L
max
{(âˆš
Ï€/2 +
âˆš
2 ln âˆ†âˆ’1
)2
,
d0
Îº2(d0, c0)
ln
(
2de
d0
)}
â‰¤ Câˆ— 1
n
U
L
max
{
2Ï€, 8 ln âˆ†âˆ’1,
d0
Îº2(d0, c0)
ln
(
2de
d0
)}
â‰¤ Câˆ— 1
n
U
L
max
{
max
(
2Ï€
ln(2d)
,
1
Îº2(d0, c0)
)
d0 ln
(
2de
d0
)
, 8 ln âˆ†âˆ’1
}
â‰¤ Câˆ— 1
n
U
L
max
{(
2Ï€
ln(2d)
+
1
Îº2(d0, c0)
)
d0 ln
(
2de
d0
)
, 8 ln âˆ†âˆ’1
}
(43)
Then, by integrating, (32) and (43) after a straightforward calculus yield
EKL(Î¸, Î¸Ì‚Slope) =
âˆ« âˆ
0
P
(
KL(Î¸, Î¸Ì‚Slope) â‰¥ t
)
dt
â‰¤ Câˆ— 1
n
U
L
ï£«ï£­( 2Ï€
ln(2d)
+
1
Îº2(d0, c0)
)
d0 ln
(
2de
d0
)
+ 8
(
2de
d0
)âˆ’ d0
8
max{ 2Ï€
ln(2d)
,Îºâˆ’2(d0,c0)}
ï£¶ï£¸
â‰¤ Câˆ— U
L
(
2Ï€ + 8
ln(2d)
+
1
Îº2(d0, c0)
)
d0
n
ln
(
2de
d0
)
19

