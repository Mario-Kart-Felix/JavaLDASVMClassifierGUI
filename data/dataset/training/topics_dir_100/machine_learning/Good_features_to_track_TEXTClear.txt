Good Features to Track 
Jianbo Shi 
Computer Science Department 
Cornel1 University 
Ithaca, NY 14853 
Abstract 
No feature-based vision sys tem can work unless good 
features can be identified and tracked f rom frame to  
frame. Although tracking itself i s  by and large a solved 
problem, selecting features that can be tracked well and 
correspond t o  physical points in the world i s  still hard. 
W e  propose a feature selection criterion that i s  optimal 
b y  construction because it i s  based o n  how the tracker 
works, and a feature monitoring method that can de- 
tect occlusions, disocclusions, and features that do not 
correspond t o  points in the world. These methods are 
based o n  a new tracking algorithm that extends pre- 
vious Newton-Raphson style search methods t o  work 
under affine image transformations. W e  test  perfor- 
mance with several simulations and experiments. 
1 Introduction 
Is feature tracking a solved problem? The exten- 
sive studies of image correlation [4], [3], [15], [18], [7], 
[17] and sum-of-squared-difference (SSD) methods [2], 
[l] show that all the basics are in place. With small 
inter-frame displacements, a window can be tracked 
by optimizing some matching criterion with respect to 
translation [lo], [I] and linear image deformation [6] ,  
[8], [ll], possibly with adaptive window size[l4]. Fea- 
ture windows can be selected based on some measure 
of texturedness or corner’ness, such as a high standard 
deviation in the spatial intensity profile [13], the pres- 
ence of zero crossings of the Laplacian of the image 
intensity [12], and corners [9], [5]. Yet, even a re- 
gion rich in texture can be poor. For instance, it can 
straddle a depth discontinuity or the boundary of a 
reflection highlight on a glossy surface. In either case, 
the window is not attached to a fixed point in the 
world, making that feature useless or even harmful to 
most structure-from-motion algorithms. Furthermore, 
Carlo Tomasi 
Computer Science Department 
Stanford University 
Stanford, CA 94305 
even good features can become occluded, and trackers 
often blissfully drift away from their original target 
when this occurs. No feature-based vision system can 
be claimed to  really work until these issues have been 
settled. 
In this paper we show how to monitor the quality of 
image features during tracking by using a measure of 
feature dissimilarity that quantifies the change of ap- 
pearance of a feature between the first and the current 
frame. The idea is straightforward: dissimilarity is the 
feature’s rms residue between the first and the current 
frame, and when dissimilarity grows too large the fea- 
ture should be abandoned. However, in this paper we 
make two main contributions to  this problem. First, 
we provide experimental evidence that pure transla- 
tion is not an adequate model for image motion when 
measuring dissimilarity, but affine image changes, that 
is, linear warping and translation, are adequate. Sec- 
ond, we propose a numerically sound and efficient way 
of determining affine changes by a Newton-Raphson 
stile minimization procedure, in the style of what Lu- 
cas and Kanade [lo] do for the pure translation model. 
In addition, we propose a more principled way to se- 
lect features than the more traditional “interest” or 
“cornerness” measures. Specifically, we show that fea- 
tures with good texture properties can be defined by 
optimizing the tracker’s accuracy. In other words, the 
right features are exactly those that make the tracker 
work best. Finally, we submit that using two models of 
image motion is better than using one. In fact, trans- 
lation gives more reliable results than affine changes 
when the inter-frame camera translation is small, but 
affine changes are necessary to  compare distant frames 
to determine dissimilarity. We define these two models 
in the next section. 
2 Two Models of Image Motion 
OThis research was supported by the National Science Foun- As the camera moves, the patterns of image inten- 
sities change in a complex way. However, away from dation under contract IRI-9201751 
593 
1063-6919/94 $3.00 0 1994 IEEE 
occluding boundaries and near surface markings, these 
changes can often be described as image motion: 
Thus, a later image taken at timet+. can be obtained 
by moving every point in the current image, taken at 
time t ,  by a suitable amount. The amount of motion 
6 = ( t , ~ )  is called the displacement of the point at 
The displacement vector 6 is a function of the im- 
age position x, and variations in 6 are often noticeable 
even within the small windows used for tracking. It 
then makes little sense to speak of ((the” displacement 
of a feature window, since there are different displace- 
ments within the same window. An a@ne mot ion f ie ld  
is a better representation: 
x = (x ,  Y). 
S = D x + d  
where 
is a deformation matrix, and d is the translation of 
the feature window’s center. The image coordinates 
x are measured with respect to the window’s center. 
Then, a point x in the first image I moves to point 
Ax + d in the second image J ,  where A = 1 + D and 
1 is the 2 x 2 identity matrix: 
J(Ax + d) = I ( x )  . (2) 
Given two images I and J and a window in image 
I ,  tracking means determining the six parameters that 
appear in the deformation matrix D and displacement 
vector d. The quality of this estimate depends on the 
size of the feature window, the texturedness of the im- 
age within it, and the amount of camera motion be- 
tween frames. When the window is small, the matrix 
D is harder to estimate, because the variations of mo- 
tion within it are smaller and therefore less reliable. 
However, smaller windows are in general preferable 
for tracking because they are less likely to straddle 
a depth discontinuity. For this reason, a pure trans- 
lation model is preferable during tracking, where the 
deformation matrix D is assumed to be zero: 
6 = d .  
The experiments in sections 6 and 7 show that the 
best combination of these two motion models is pure 
translation for tracking, because of its higher reliabil- 
ity and accuracy over the small inter-frame motion of 
the camera, and affine motion for comparing features 
between the first and the current frame in order to 
monitor their quality. In order to address these issues 
quantitatively, however, we first need to introduce our 
tracking method. 
3 Computing Image Motion 
Because of image noise and because the affine mo- 
tion model is not perfect, equation (2) is in general 
not satisfied exactly. The problem of determining the 
motion parameters is then that of finding the A and 
d that minimize the dissimilari ty  
[J(Ax + d) - I(x)I2 W(X) dx (3) 
where W is the given feature window and w(x) is a 
weighting function. In the simplest case, w(x) = 1 .  
Alternatively, w could be a Gaussian-like function to 
emphasize the central area of the window. Under pure 
translation, the matrix A is constrained to be equal to 
the identity matrix. To minimize the residual (3), we 
differentiate it with respect to the unknown entries of 
the deformation matrix D and the displacement vector 
d and set the result to  zero. We then linearize the 
resulting system by the truncated Taylor expansion 
€=/Iw 
J(Ax + d) = J(x)  + g T ( u )  . (4) 
Tz = a (5) 
This yields (see [l6]) the following linear 6 x 6 system: 
wherezT = [ d,, dy, d,, d,, d, d, ] collects 
the entries of the deformation D and displacement d, 
the error vector 
depends on the difference between the two images, and 
the 6 x 6 matrix T ,  which can be computed from one 
image, can be written as 
where 
594 
z =  [ Q2 ”;f”] . 
9x Qy 
Even when affine motion is a good model, equation 
5 is only approximately satisfied, because of the lin- 
earization of equation (4).  However, the correct affine 
change can be found by using equation 5 iteratively in 
a Newton-Raphson style minimization [16]. 
During tracking, the affine deformation D of the 
feature window is likely to be small, since motion be- 
tween adjacent frames must be small in the first place 
for tracking to work at  all. It is then safer to set D 
to  the zero matrix. In fact, attempting to determine 
deformation parameters in this situation is not only 
useless but can lead to poor displacement solutions: 
in fact, the deformation D and the displacement d in- 
teract through the 4 x 2 matrix V of equation (6), and 
any error in D would cause errors in d. Consequently, 
when the goal is to determine d, the smaller system 
Z d = e  (7) 
should be solved, where e collects the last two entries 
of the vector a of equation (5). 
When monitoring features for dissimilarities in 
their appearance between the first and the current 
frame, on the other hand, the full affine motion system 
(5) should be solved. In fact, motion is now too large 
to be described well by the pure translation model. 
Furthermore, in determining dissimilarity, the whole 
transformation between the two windows is of inter- 
est, and a precise displacement is less critical, so it 
is acceptable for D and d to interact to  some extent 
through the matrix I / .  
In the next two sections we discuss these issues 
in more detail: first we determine when system (7) 
yields a good displacement measurement (section 4) 
and then we see when equation (5) can be used reli- 
ably to  monitor a feature’s quality (section 5). 
4 Texturedness 
Regardless of the method used for tracking, not all 
parts of an image contain complete motion informa- 
tion (the aperture problem): for instance, only the ver- 
tical component of motion can be determined for a 
horizontal intensity edge. To overcome this difficulty, 
researchers have proposed to track corners, or win- 
dows with a high spa.tia1 frequency content, or regions 
where some mix of second-order derivatives is suffi- 
ciently high. However, there are two problems with 
these “interest operators”. First, they are often based 
on a preconceived and arbitrary idea of what a good 
window looks like. The resulting features may be in- 
tuitive, but are not guaranteed to be the best for the 
tracking algorithm to produce good results. Second, 
“interest operators” have been usually defined for the 
pure translation model of section 2,  and the underly- 
ing concept are hard to extend to affine motion. 
In this paper, we propose a more principled defini- 
tion of feature quality. With the proposed definition, 
a good feature is one that can be tracked well, so that 
the selection criterion is optimal by construction. 
We can track a window from frame to frame if sys- 
tem 7 represents good measurements, and if it can be 
solved reliably. Consequently, the symmetric 2 x 2 
matrix Z of the system must be both above the image 
noise level and well-conditioned. The noise require- 
ment implies that both eigenvalues of Z must be large, 
while the conditioning requirement means that they 
cannot differ by several orders of magnitude. Two 
small eigenvalues mean a roughly constant intensity 
profile within a window. A large and a small eigen- 
value correspond to a unidirectional texture pattern. 
Two large eigenvalues can represent corners, salt-and- 
pepper textures, or any other pattern that can be 
tracked reliably. 
In practice, when the smaller eigenvalue is suffi- 
ciently large to meet the noise criterion, the matrix 2 
is usually also well conditioned. In fact, the intensity 
variations in a window are bounded by the maximum 
allowable pixel value, so that the greater eigenvalue 
cannot be arbitrarily large. In conclusion, if the two 
eigenvalues of Z are XI and Xz, we accept a window if 
min( XI, A,) > X , (8) 
where X is a predefined threshold. 
Similar considerations hold also when solving the 
full affine motion system (5) for the deformation D 
and displacement d. However, an essential difference 
must be pointed out: deformations are used to  deter- 
mine whether the window in the first frame matches 
that in the current frame well enough during feature 
monitoring. Thus, the goal is not to determine defor- 
mation per se. Consequently, it does not matter if one 
component of deformation cannot be determined reli- 
ably. In fact, this means that that component does not 
affect the window substantially, and any value along 
this component will do in the comparison. In prac- 
tice, the system (5) can be solved by computing the 
pseudo-inverse of 7’. Then, whenever some component 
is undetermined, the minimum norm solution is com- 
puted, that is, the solution with a zero deformation 
along the undetermined component(s). 
595 
5 Dissimilarity 
0.01 5 
f - 
E 
:3 
0.01 
0.005 
formations make the five windows virtually equal to 
each other. 
I - I 
I 
/ +  - 
I 
-+ '  
- 
A feature with a high texture content, as defined 
in the previous section, can still be a bad feature to 
track. For instance, in an image of a tree, a horizontal 
twig in the foreground can intersect a vertical twig in 
the background. This intersection occurs only in the 
image, not in the world, since the two twigs are at dif- 
ferent depths. A.ny selection criterion would pick the 
intersection as a good feature to track, and yet there is 
no real world feature there to speak of. The measure 
of dissimilarity defined in equation (3) can often in- 
dicate that something is going wrong. Because of the 
potentially large number of frames through which a 
given feature can be tracked, the dissimilarity measure 
would not work well with a pure translation model. To 
illustrate this, consider figure 1, which shows three out 
of 21 frame details from Woody Allen's movie, Man- 
hattan. The top row of figure 2 shows the results of 
tracking the traffic sign in this sequence. 
-+---. 
Figure 1: Three frame details from Woody Allen's 
Manhattan. The details are from the ls t ,  l l t h ,  and 
21st frames of a subsequence from the movie. 
Figure 2: The traffic sign windows from frames 
1,6,11,16,21 as tracked (top), and warped by the com- 
puted deformation matrices (bottom). 
While the inter-frame changes are small enough for 
the pure translation tracker to work, the cumulative 
changes over 25 frames are rather large. In fact, the 
size of the sign increases by about 15 percent, and the 
dissimilarity measure (3) increases rather quickly with 
the frame number: as shown by the dashed and crossed 
line of figure 3. The solid and crossed line in the same 
figure shows the dissimilarity measure when also defor- 
mations are accounted for, that is, if the entire system 
(5) is solved for z .  This new measure of dissimilarity 
remains small and roughly constant. The bottom row 
of figure 2 shows the same windows as in the top row, 
but warped by the computed deformations. The de- 
0.025 
0 
I 
I 
I 
0.02 t p- 6 
Figure 3: Pure translation (dashed) and affine motion 
(solid) dissimilarity measures for the window sequence 
of figure 1 (plusses) and 4 (circles). 
Figure 4: Three more frame details from Manhatian. 
The feature tracked is the bright window on the back- 
ground, on the right of the traffic sign. 
Figure 5: The bright window from figure 4 is occluded 
by the traffic sign in the middle frame (top). The bot- 
tom row shows the effects of warping by the computed 
deformation matrices. 
The two circled curves in figure 3 refer to another 
feature from the same sequence, shown in figure 4. 
The top row of figure 5 shows the feature window 
through five frames. In the middle frame the traf- 
fic sign begins to occlude the original feature. The 
circled curves in figure 3 are the dissimilarity mea- 
sures under affine motion (solid) and pure translation 
(dashed). The sharp jump in the affine motion curve 
around frame 4 indicates the occlusion. The bottom 
row of figure 5 shows that the deformation computa- 
tion attempts to deform the traffic sign into a window. 
596 
-..""".*,.l .,,... ".̂  , 
6 Convergence 
Translation 
[ : ] 
[ ] 
The simulations in this section show that when the 
affine motion model is correct our iterative tracking 
algorithm converges even when the starting point is 
far removed from the true solution. The first series of 
simulations are run on the four circular blobs shown 
in the leftmost column of figure 6. The three mo- 
tions of table l are considered. To see their effects, 
compare the first and last column of figure 6. The im- 
ages in the last column are the images warped, trans- 
lated, and corrupted with random Gaussian noise with 
a standard deviation equal to 16 percent of the maxi- 
mum image intensity. The images in the intermediate 
columns are the results of the deformations and trans- 
lations to  which the tracking algorithm subjects the 
images in the leftmost column after 4, 8, and 19 it- 
erations, respectively. The algorithm works correctly, 
and makes the images in the fourth column of figure 
6 as similar as possible to  those in the fifth column. 
Translation 
[ _"cz:7 ] 
2.0920 [ 0.0155 ] 
Figure 6: Original image (leftmost column) and 
warped, translated and noisy versions (rightmost col- 
umn) for three different affine chm-ges. The interme- 
diate columns are the deformations computed by the 
tracker after 4,8,and 19 iterations. 
Figure 7 plots the dissimilarity measure (as a frac- 
tion of the maximum image intensity), translation er- 
ror (in pixels), and deformation error (Frobenius norm 
of the residual deformation matrix) as a function of the 
frame number (first three columns), as well as the in- 
termediate displacements and deformations (last two 
columns). Deformations are represented in the fifth 
column of figure 7 by two vectors each, correspond- 
ing to the two columns of the transformation matrix 
A = 1 + D. Table 1 shows the final numerical values. 
Figure 8 shows a similar experiment with a more 
complex image (from MATLAB). Finally, figure 9 
shows an attempt to match two completely different 
images: four blobs and a cross. The algorithm tries to 
do its best by aligning the blobs with the cross, but 
the dissimilarity (left plot at the bottom of figure 9) 
remains high throughout. 
o::Kl l~ylo.:~~ om 
OO 10 20 OO 10 20 OO 10 m 0 3 0 
0.05 
0.05 
!??!I 
Figure 7: Dissimilarity ( lS* column), displacement er- 
ror (2nd)l and deformation error (3'd) versus iteration 
number for figure 6. The last two columns are dis- 
placements and deformations computed during track- 
ing, starting from zero. See text for units. 
Computed 
Deformation 
1.409 -0.342 
0.658 -0.342 ] I [ 0.670 -0.343 
0.319 0.660 I 1 [ 0.342 0.658 
1 0.802 0.235 0.351 1.227 0.809 0.253 0.342 1.232 
True I Computed 
Table 1: True and computed affine changes (in pixels) 
for the simulations of figure 6. 
Figure 8: The penny at the top left is warped until it 
matches the transformed and noise-corrupted image 
at the top right. The bottom plots are as in figure 7. 
597 
Figure 9: The blobs at the top left are warped as 
shown until they are as close as possible to the cross in 
the rightmost column. The bottom row shows dissim- 
ilarity, translati’on, and deformation versus iteration 
number. 
Figure 10: The first frame of a 26 frame sequence 
taken with a forward moving camera. 
7 Monitoring Features 
This section presents some experiments with real 
images and shows how features can be monitored dur- 
ing tracking to detect potentially bad features. Figure 
10 shows the first frame of a 26-frame sequence. A 
Pulnix camera equipped with a 16” lens moves for- 
ward 2mm per frame. Because of the forward motion, 
features loom larger from frame to frame. The pure 
translation model is sufficient for inter-frame track- 
ing but not to monitor features, as discussed below. 
Figure 11 displays the 102 features selected accord- 
ing to the criterion introduced in section 4. To limit 
the number of features and to use each portion of the 
image at most oiice, the constraint was imposed that 
no two feature windows can overlap in the first frame. 
Figure 12 shows the dissimilarity of each feature under 
the pure translation motion model, that is, with the 
deformation matrix D set to zero for all features. This 
dissimilarity is nearly useless for feature monitoring: 
except for features 58 and 89, all features have compa- 
rable dissimilarities, and no clean discrimination can 
be drawn between good and bad features. 
F’rom figure 13 we see that features 58 is at  the 
boundary of the block with a letter U visible in the 
lower right-hand side of the figure. The feature win- 
dow straddles the vertical dark edge of the block in the 
foreground as well as parts of the letters Cra in the 
word “Crayola” in the background. Six frames of this 
window are visible in the third row of figure 14. As the 
camera moves forward, the pure translation tracking 
stays on top of approximately the same part of the im- 
age. However, the gap between the vertical edge in the 
foreground and the letters in the background widens, 
and it becomes harder to warp the current window 
into the window in the first frame, thereby leading 
Figure 11: The features selected according to the tex- 
turedness criterion of section 4. 
“0 5 10 15 25 30 
frame 
Figure 12: Pure translation dissimilarity for the fea- 
tures in figure 11. This dissimilarity is nearly useless 
for feature discrimination. 
598 
to  the rising dissimilarity. The changes in feature 89 
are seen even more easily. This feature is between 
the edge of the book in the background and a lamp 
partially visible behind it in the top right corner of 
figure 13. As the camera moves forward, the shape of 
the glossy reflection on the lamp shade changes as it 
becomes occluded (see the last row of figure 14). 
0.045 
0.04 
0.035 
Figure 13: Labels of some of the features in figure 11. - 
- 
- 
89 
r‘2. f 
1 6 11 16 21 26 
Figure 14: Six sample features through six sample 
frames. 
Although these bad features would be detected be- 
cause of their high dissimilarity, many other bad fea- 
tures would pass unnoticed. For instance, feature 3 in 
the lower right of figure 13 is affected by a substan- 
tial disocclusion of the lettering on the Crayola box by 
the U block as the camera moves forward, as well as a 
slight disocclusion by the “3M” box on the right (see 
the top row of figure 14). Yet with a pure translation 
model the dissimilarity of feature 3 is not substan- 
tially different from that of all the other features in 
figure 12. In fact, the looming caused by the camera’s 
forward motion dominates, and reflects in the overall 
upward trend of the majority of curves in figure 12. 
Similar considerations hold, for instance, for features 
78 (a disocclusion), 24 (an occlusion), and 4 (a  disoc- 
clusion) labeled in figure 13. 
Now compare the pure translation dissimilarity of 
figure 12 with the affine motion dissimilarity of figure 
15. The thick stripe of curves a t  the bottom represents 
all good features, including features 1,21,30,53, labeled 
in figure 13. These four features are all good, being 
immune from occlusions or glossy reflections: 1 and 
21 are lettering on the (‘Crayola” box (the second row 
of figure 14 shows feature 21 as an example), while 
features 30 and 53 are details of the large title on the 
book in the background (upper left in figure 13). The 
bad features 3,4,58,78,89, on the other hand, stand 
out very clearly in figure 15: discrimination is now 
possible. 
0.05 
frame 
Figure 15: Affine motion dissimilarity for the features 
in figure 11. Notice the good discrimination between 
good and bad features. Dashed plots indicate aliasing 
(see text). 
Features 24 and 60 deserve a special discussion, and 
are plotted with dashed lines in figure 15. These two 
features are lettering detail on the rubber cement bot- 
tle in the lower center of figure 13. The fourth row of 
figure 14 shows feature 60 as an example. Although 
feature 24 suffers an additional slight occlusion as the 
camera moves forward, these two features stand out 
from the very beginning, and their dissimilarity curves 
are very erratic throughout the sequence. This is be- 
cause of aliasing: from the fourth row of figure 14, 
we see that feature 60 (and similarly feature 24) con- 
tains very small lettering, of size comparable to the 
599 
image’s pixel size (the feature window is 25 x 25 pix- 
els). The matching between one frame and the next is 
haphazard, because the characters in the lettering are 
badly aliased. This behavior is not a problem: erratic 
dissimilarities indicate trouble, and the corresponding 
features ought to  be abandoned. 
8 Conclusion 
In this paper, we have proposed a method for fea- 
ture selection, a tracking algorithm based on a model 
of affine image changes, and a technique for moni- 
toring features during tracking. Selection specifically 
maximizes the quality of tracking, and is therefore op- 
timal by construction, as opposed to more ad  hoc mea- 
sures of texturedness. Monitoring is computationally 
inexpensive and sound, and helps discriminating be- 
tween good and bad features based on a measure of 
dissimilarity that uses affine motion as the underlying 
image change model. 
Of course, monitoring feature dissimilarity does not 
solve all the problems of tracking. In some situations, 
a bright spot on a glossy surface is a bad (that is, 
nonrigid) feature, but may change little over a long 
sequence: dissimilarity may not detect the problem. 
However, even in principle, not everything can be de- 
cided locally. Rigidity is not a local feature, so a local 
method cannot be expected to always detect its viola- 
tion. On the other hand, many problems can indeed 
be discovered locally and these are the target of the 
investigation in this paper. Our experiments and sim- 
ulations show that monitoring is indeed effective in 
realistic circumstances. A good discrimination at the 
beginning of the processing chain can reduce the re- 
maining bad feat,ures to a few outliers, rather than 
leaving them an overwhelming majority. Outlier de- 
tection techniques at higher levels in the processing 
chain are then more likely to succeed. 
References 
[l] P. Anandan. A computational framework and an 
algorithm for the measurement of visual motion. 
IJCV, 2(3):283-310, 1989. 
[a] P. J .  Burt, (2. Yen, and X. Xu. Local correla- 
tion measures for motion analysis: a comparative 
study. IEEE CPRIP, 269-274, 1982. 
[3] C. Cafforio and F. Rocca. Methods for measuring 
small displacements in t plevision images. IEEE 
Trans. IT-22:573-579,1976. 
[4] D. J. Connor and J.  0. Limb. Properties of frame- 
difference signals generated by moving images. 
IEEE Trans. COM-22(10):1564-1575, 1974. 
[5] L. Dreschler and H.-H. NageI. Volumetric model 
and 3d trajectory of a moving car derived from 
monocular tv frame sequences of a street scene. 
Reliability analysis of parame- 
ter estimation in linear models with applica- 
tions to mensuration problems in computer vi- 
sion. CVGIP, 40:273-310, 1987. 
[7] W. Forstner and A. Pertl. Photogrammetric Stan- 
dard Methods and Digital Image Matching Tech- 
niques for  High Precision Surface Measurements. 
Elsevier Science Pub., 1986. 
Motion displacement 
estimation using an affine model for matching. 
Optical Engineering, 30(7):881-887, 1991. 
[9] L. Kitchen and A. Rosenfeld. Gray-level corner 
detection. TR, U.  of Maryland, 1980. 
[lo] B. D. Lucas and T .  Kanade. An iterative im- 
age registration technique with an application to 
stereo vision. IJCAI, 1981. 
[ll] R. Manmatha and J .  Oliensis. Extracting affine 
deformations from image patches - I: Finding 
scale and rotation. CVPR, 754-755, 1993. 
[12] D. Man,  T. Poggio, and S. Ullman. Bandpass 
channels, zero-crossings, and early visual infor- 
mation processing. JOSA, 69:914-916, 1979. 
[13] H. Moravec. Obstacle, avoidance and navigation 
in the real world b y  a seeing robot rover. PhD, 
Stanford U., 1980. 
[14] M. Okutomi and T .  Kanade. A locally adaptive 
window for signal matching. IJCV, 7(2):143-162, 
1992. 
[15] T. W. Ryan, R. T .  Gray, and B. R. Hunt. Pre- 
diction of correlation errors in stereo-pair images. 
Optical Engineering, 19(3):312-322, 1980. 
[16] J .  Shi and C. Tomasi. Good features to track. 
T R  93-1399, Cornel1 U , ,  1993. 
[17] Qi Tian and Michael N.  Huhns. Algorithms for 
subpixel registration. CVGIP, 35:220-233, 1986. 
[18] G.  A. Wood. Realities of automatic correla- 
tion problem. Photogram. Eng. and Rem. Sens., 
IJCAI, 692-697, 1981. 
[6] W. Forstner. 
[8] C. Fuh and P. Maragos. 
49:537-538, 1983. 
600 

