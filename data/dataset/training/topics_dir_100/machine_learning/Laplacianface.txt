 1
Face Recognition Using Laplacianfaces 
 
 
Xiaofei He†, Shuicheng Yan‡, Yuxiao Hu*, Partha Niyogi†, Hong-Jiang Zhang* 
 
†Department of Computer Science, The University of Chicago, 1100 E. 58th Street, Chicago, IL 60637 
{xiaofei, niyogi}@cs.uchicago.edu 
 
*Microsoft Research Asia, Beijing 100080, China 
{i-yuxhu, hjzhang}@microsoft.com 
 
‡Department of Information Science, School of Mathematical Sciences, Beijing University, China 
scyan@msrchina.research.microsoft.com 
 
 
ABSTRACT 
We propose an appearance based face recognition method called the Laplacianface approach. By using Locality Preserving 
Projections (LPP), the face images are mapped into a face subspace for analysis. Different from Principal Component 
Analysis (PCA) and Linear Discriminant Analysis (LDA) which effectively see only the Euclidean structure of face space, 
LPP finds an embedding that preserves local information, and obtains a face subspace that best detects the essential face 
manifold structure. The Laplacianfaces are the optimal linear approximations to the eigenfunctions of the Laplace Beltrami 
operator on the face manifold. In this way, the unwanted variations resulting from changes in lighting, facial expression, and 
pose may be eliminated or reduced. Theoretical analysis shows that PCA, LDA and LPP can be obtained from different 
graph models. We compare the proposed Laplacianface approach with Eigenface and Fisherface methods on three different 
face datasets. Experimental results suggest that the proposed Laplacianface approach provides a better representation and 
achieves lower error rates in face recognition.  
 
 
KEYWORDS 
Face Recognition, Principal Component Analysis, Linear Discriminant Analysis, Locality Preserving 
Projections, Face Manifold, Subspace Learning 
 
 2
1. INTRODUCTION 
Many face recognition techniques have been developed over the past few decades. One of the 
most successful and well-studied techniques to face recognition is the appearance-based method 
[28][16]. When using appearance-based methods, we usually represent an image of size n×m pixels by a 
vector in an n×m dimensional space. In practice, however, these n×m dimensional spaces are too large to 
allow robust and fast face recognition. A common way to attempt to resolve this problem is to use di-
mensionality reduction techniques [1][2][8][11][12][14][22][26][28][32][35]. Two of the most popular 
techniques for this purpose are Principal Component Analysis (PCA) [28] and Linear Discriminant 
Analysis (LDA) [2].  
PCA is an eigenvector method designed to model linear variation in high-dimensional data. PCA 
performs dimensionality reduction by projecting the original n-dimensional data onto the k (<<n)-
dimensional linear subspace spanned by the leading eigenvectors of the data’s covariance matrix. Its 
goal is to find a set of mutually orthogonal basis functions that capture the directions of maximum vari-
ance in the data and for which the coefficients are pairwise decorrelated. For linearly embedded mani-
folds, PCA is guaranteed to discover the dimensionality of the manifold and produces a compact 
representation. Turk and Pentland [28] use Principal Component Analysis to describe face images in 
terms of a set of basis functions, or “eigenfaces”.  
LDA is a supervised learning algorithm. LDA searches for the project axes on which the data 
points of different classes are far from each other while requiring data points of the same class to be 
close to each other. Unlike PCA which encodes information in an orthogonal linear space, LDA encodes 
discriminating information in a linear separable space using bases are not necessarily orthogonal. It is 
generally believed that algorithms based on LDA are superior to those based on PCA. However, some 
recent work [14] shows that, when the training dataset is small, PCA can outperform LDA, and also that 
PCA is less sensitive to different training datasets. 
 3
Recently, a number of research efforts have shown that the face images possibly reside on a 
nonlinear submanifold [7][10][18][19][21][23][27]. However, both PCA and LDA effectively see only 
the Euclidean structure. They fail to discover the underlying structure, if the face images lie on a 
nonlinear submanifold hidden in the image space. Some nonlinear techniques have been proposed to 
discover the nonlinear structure of the manifold, e.g. Isomap [27], LLE [18][20], and Laplacian 
Eigenmap [3]. These nonlinear methods do yield impressive results on some benchmark artificial data 
sets. However, they yield maps that are defined only on the training data points and how to evaluate the 
maps on novel test data points remains unclear. Therefore, these nonlinear manifold learning techniques 
[3][5][18][20] [27][33] might not be suitable for some computer vision tasks, such as face recognition. 
In the meantime, there has been some interest in the problem of developing low dimensional rep-
resentations through kernel based techniques for face recognition [13][19]. These methods can discover 
the nonlinear structure of the face images. However, they are computationally expensive. Moreover, 
none of them explicitly considers the structure of the manifold on which the face images possibly reside.  
In this paper, we propose a new approach to face analysis (representation and recognition), which 
explicitly considers the manifold structure. To be specific, the manifold structure is modeled by a near-
est-neighbor graph which preserves the local structure of the image space. A face subspace is obtained 
by Locality Preserving Projections (LPP) [9]. Each face image in the image space is mapped to a low-
dimensional face subspace, which is characterized by a set of feature images, called Laplacianfaces. The 
face subspace preserves local structure, seems to have more discriminating power than the PCA ap-
proach for classification purpose. We also provide theoretical analysis to show that PCA, LDA and LPP 
can be obtained from different graph models. Central to this is a graph structure that is induced from the 
data points. LPP finds a projection that respects this graph structure. In our theoretical analysis, we show 
how PCA, LDA, and LPP arise from the same principle applied to different choices of this graph struc-
ture. 
 4
It is worthwhile to highlight several aspects of the proposed approach here: 
1. While the Eigenfaces method aims to preserve the global structure of the image space, and the Fisher-
faces method aims to preserve the discriminating information; our Laplacianfaces method aims to 
preserve the local structure of the image space. In many real world classification problems, the local 
manifold structure is more important than the global Euclidean structure, especially when nearest-
neighbor like classifiers are used for classification. LPP seems to have discriminating power although 
it is unsupervised. 
2. An efficient subspace learning algorithm for face recognition should be able to discover the nonlinear 
manifold structure of the face space. Our proposed Laplacianfaces method explicitly considers the 
manifold structure which is modeled by an adjacency graph. Moreover, the Laplacianfaces are ob-
tained by finding the optimal linear approximations to the eigenfunctions of the Laplace Beltrami op-
erator on the face manifold. They reflect the intrinsic face manifold structures. 
3. LPP shares some similar properties to LLE, such as a locality preserving character. However, their 
objective functions are totally different. LPP is obtained by finding the optimal linear approximations 
to the eigenfunctions of the Laplace Beltrami operator on the manifold. LPP is linear, while LLE is 
nonlinear. Moreover, LPP is defined everywhere, while LLE is defined only on the training data 
points and it is unclear how to evaluate the maps for new test points. In contrast, LPP may be simply 
applied to any new data point to locate it in the reduced representation space. 
The rest of this paper is organized as follows: Section 2 describes PCA and LDA. The Locality 
Preserving Projections (LPP) algorithm is described in Section 3. In Section 4, we provide a statistical 
view of LPP. We then give a theoretical analysis of LPP and its connections to PCA and LDA in Section 
5. Section 6 presents the manifold ways of face analysis using Laplacianfaces. A variety of experimental 
results are presented in Section 7. Finally, we provide some concluding remarks and suggestions for fu-
ture work in Section 8. 
 5
2. PCA and LDA 
One approach to coping with the problem of excessive dimensionality of the image space is to re-
duce the dimensionality by combining features. Linear combinations are particular attractive because 
they are simple to compute and analytically tractable. In effect, linear methods project the high-
dimensional data onto a lower dimensional subspace. 
Considering the problem of representing all of the vectors in a set of n d-dimensional samples x1, 
x2, …, xn, with zero mean, by a single vector y={y1, y2, …, yn} such that yi represent xi. Specifically, we 
find a linear mapping from the d-dimensional space to a line. Without loss of generality, we denote the 
transformation vector by w. That is, wTxi = yi. Actually, the magnitude of w is of no real significance, 
because it merely scales yi. In face recognition, each vector xi denotes a face image. 
Different objective functions will yield different algorithms with different properties. PCA aims to 
extract a subspace in which the variance is maximized. Its objective function is as follows, 
( )∑
=
−
n
i
i yy
1
2max
w
 
∑
=
=
n
i
in yy 1
1  
The output set of principal vectors w1, w2, …, wk are an orthonormal set of vectors representing the ei-
genvectors of the sample covariance matrix associated with the k < d largest eigenvalues. 
While PCA seeks directions that are efficient for representation, Linear Discriminant Analysis 
seeks directions that are efficient for discrimination. Suppose we have a set of n d-dimensional samples 
x1, x2, …, xn, belonging to l classes of faces. The objective function is as follows, 
ww
ww
w W
T
B
T
S
Smax  
 6
∑
=
−−=
l
i
Tii
iB nS
1
)()( ))(( mmmm  
∑ ∑
= = 







−−=
l
i
n
j
Tii
j
ii
jW
i
S
1 1
)()()()( ))(( mxmx  
where m is the total sample mean vector, ni is the number of samples in the ith class, m(i) is the average 
vector of the ith class, and )(ijx  is the j
th sample in the ith class. We call SW the within-class scatter matrix 
and SB the between-class scatter matrix.  
3. LEARNING A LOCALITY PRESERVING SUBSPACE 
PCA and LDA aim to preserve the global structure. However, in many real world applications, the 
local structure is more important. In this section, we describe Locality Preserving Projection (LPP) [9], 
a new algorithm for learning a locality preserving subspace. The complete derivation and theoretical 
justifications of LPP can be traced back to [9]. LPP seeks to preserve the intrinsic geometry of the data 
and local structure. The objective function of LPP is as follows: 
∑ −
ij
ijji Syy
2)(min  
where yi is the one-dimensional representation of xi and the matrix S is a similarity matrix. A possible 
way of defining S is as follows: 



 <−−=
otherwise.                                   0
||||      ),/exp( 2
2
εjiji
ij
tS xxxx  
or 






 −
=
otherwise.                                   0
 of neighborsnearest   among is or                                       
  of neighborsnearest   among is  if       ),/exp(
2
ij
jiji
ij k
kt
S xx
xxxx
 
 7
where ε is sufficiently small, and ε > 0. Here, ε defines the radius of the local neighborhood. In other 
words, ε defines the “locality”. The objective function with our choice of symmetric weights Sij (Sij = Sji) 
incurs a heavy penalty if neighboring points xi and xj are mapped far apart, i.e. if (yi − yj)2 is large. 
Therefore, minimizing it is an attempt to ensure that if xi and xj are “close” then yi and yj are close as 
well. Following some simple algebraic steps, we see that 
ww
ww
wwww
wwwxxw
wxxwwxxw
xwxw
TT
TT
TTTT
TT
i
T
iiii
T
ij
T
jiji
T
ij
T
iiji
T
ij ijj
T
i
T
ij ijji
XLX
XSDX
XSXXDX
XSXD
SS
S
Syy
=
−=
−=
−=
−=
−=
−
∑
∑∑
∑
∑
)(
)(
)(    
2
2
1
2
2
1
 
where X = [x1, x2, …, xn], and D is a diagonal matrix; its entries are column (or row, since S is symmet-
ric) sums of S, Dii = ∑j Sji. L = D – S is the Laplacian matrix [6]. Matrix D provides a natural measure on 
the data points. The bigger the value Dii (corresponding to yi) is, the more “important” is yi. Therefore, 
we impose a constraint as follows: 
1  
1       
=⇒
=
ww
yy
TT
T
XDX
D  
Finally, the minimization problem reduces to finding: 
ww
ww
w
TT
XDX
XLX
TT
  minarg
1
          
=
 
The transformation vector w that minimizes the objective function is given by the minimum eigenvalue 
solution to the generalized eigenvalue problem: 
ww TT XDXXLX λ=  
 8
Note that the two matrices XLXT and XDXT are both symmetric and positive semi-definite, since the 
Laplacian matrix L and the diagonal matrix D are both symmetric and positive semi-definite. 
The Laplacian matrix for finite graph is analogous to the Laplace Beltrami operator on compact 
Riemannian manifolds. While the Laplace Beltrami operator for a manifold is generated by the Rieman-
nian metric, for a graph it comes from the adjacency relation. Belkin and Niyogi [3] showed that the op-
timal map preserving locality can be found by solving the following optimization problem on the mani-
fold: 
∫ ∇
= Mf
f
ML
2
1
 min
)(2
 
which is equivalent to 
∫
= Mf
ff
ML
)(min
1)(2
L  
where L is the Laplace Beltrami operator on the manifold, i.e. L(f) = − div∇(f). Thus, the optimal f has to 
be an eigenfunction of L. If we assume f to be linear, we have f(x) = wTx. By spectral graph theory, the 
integral can be discretely approximated by wTXLXTw and the L2 norm of f can be discretely approxi-
mated by wTXDXTw, which will ultimately lead to the following eigenvalue problem: 
ww TT XDXXLX λ=  
The derivation reflects the intrinsic geometric structure of the manifold. For details, see [3][6][9]. 
4. STATISTICAL VIEW OF LPP 
LPP can also be obtained from statistical viewpoint. Suppose the data points follow some underly-
ing distribution. Let x and y be two random variables. We define that a linear mapping x → wT x best 
preserves the local structure of the underlying distribution in the L2 sense if it minimizes the L2 distances 
between wT x and wT y provided that || x – y || < ε. Namely: 
)||||  |(|min 2 ε<−− yxywxw TTE  
 9
where ε is sufficiently small, and ε > 0. Here, ε defines the “locality”. Define z = x – y, then we have the 
following objective function: 
)||||  |(|min 2 ε<zzwTE  
It follows that, 
wzzzw
zwzzw
zzw
)||||  ( 
)||||  ( 
)||||  |(|   2
ε
ε
ε
<=
<=
<
TT
TT
T
E
E
E
 
Given a set of sample points x1, x2, …, xn, we first define an indicator function Sij as follows: 



 <−
=
otherwise.        0
||||      ,1 2 εji
ijS
xx
 
Let d be the number of non-zero Sij, and D be a diagonal matrix whose entries are column (or row, since 
S is symmetric) sums of S, Dii=∑j Sji. By the Strong Law of Large Numbers, E(zzT | ||z||<ε) can be esti-
mated from the sample points as follows: 
T
d
TT
d
i ji
ij
T
jiii
T
iid
ji ji ji ji
ij
T
ijij
T
jiij
T
jjij
T
iid
ji
ij
T
jijid
T
jijid
T
T
XLX
XSXXDX
SD
SSSS
S
E
ji
2
2
,
2
, , , ,
1
,
1
||||
1
||||d
1
 
)( 
)( 
)( 
))(( 
))(( 
 
)||||  (   
=
−=
−=
−−+=
−−=
−−=
≈
<
∑ ∑
∑ ∑ ∑ ∑
∑
∑
∑
<−
<
xxxx
xxxxxxxx
xxxx
xxxx
zz
zzz
xx
z
ε
ε
ε
 
where L = D – S is the Laplacian matrix. The ith column of matrix X is xi. By imposing the same con-
straint, we finally get the same minimization problem described in Section 3. 
 10
5. THEORETICAL ANALYSIS OF LPP, PCA AND LDA 
In this section, we present a theoretical analysis of LPP and its connections to PCA and LDA.  
5.1 Connections to PCA 
We first discuss the connections between LPP and PCA. 
It is worthwhile to point out that XLXT is the data covariance matrix, if the Laplacian matrix L 
is T
nn
I ee2
11 − , where n is the number of data points, I is the identity matrix and e is a column vector 
taking 1 at each entry. In fact, the Laplacian matrix here has the effect of removing the sample mean 
from the sample vectors. In this case, the weight matrix S takes 1/n2 at each entry, i.e. Sij = 1/n2, ∀i, j. Dii  
=  ∑j Sji = 1/n. Hence the Laplacian matrix is T
nn
ISDL ee2
11 −=−= . Let m denote the sample mean, 
i.e. m = 1/n∑i xi . We have: 
]))([(           
22]))([(           
))((            
))((            
))((            
)(
1111
11
11
11
2
2
T
TTT
i i i
T
i
T
n
T
in
T
in
T
iin
T
ni
T
iin
T
n
T
n
TT
nn
T
E
E
nn
XXXX
XIXXLX
mxmx
mmmmmxmx
mmmmmxmxmxmx
mmxx
ee
ee
−−=
−+−−=
−−++−−=
−=
−=
−=
∑ ∑ ∑ ∑
∑
 
where E[(x - m)(x - m)T] is just the covariance matrix of the data set.  
The above analysis shows that the weight matrix S plays a key role in the LPP algorithm. When 
we aim at preserving the global structure, we take ε (or k) to be infinity and choose the eigenvectors (of 
the matrix XLXT) associated with the largest eigenvalues. Hence the data points are projected along the 
directions of maximal variance. When we aim at preserving the local structure, we take ε to be suffi-
ciently small and choose the eigenvectors (of the matrix XLXT) associated with the smallest eigenvalues. 
Hence the data points are projected along the directions preserving locality. It is important to note that, 
 11
when ε (or k) is sufficiently small, the Laplacian matrix is no longer the data covariance matrix, and 
hence the directions preserving locality are not the directions of minimal variance. In fact, the directions 
preserving locality are those minimizing local variance. 
5.2 Connections to LDA 
LDA seeks directions that are efficient for discrimination. The projection is found by solving the 
generalized eigenvalue problem 
ww WB SS λ=  
where SB and SW are defined in section 2. Suppose there are l classes. The ith class contains ni sample 
points. Let m(i) denote the average vector of the ith class. Let x(i) denote the random vector associated to 
the ith class and )(ijx  denote the j
th sample point in the ith class. We can rewrite the matrix SW as follows: 
( )( )
( )
( )( )
( )( )
∑
∑
∑
∑ ∑
∑ ∑
∑ ∑
=
=
=
= =
= =
= =
=
−=





 ++++−=








−=








+−−=








−−=
l
i
T
iii
l
i
T
i
T
iiin
T
ii
l
i
Ti
n
ii
n
i
n
T
ii
l
i
Tii
i
n
j
Ti
j
i
j
l
i
n
j
TiiTii
j
Ti
j
iTi
j
i
j
l
i
n
j
Tii
j
ii
jW
XLX
XXXX
XX
n
S
i
iii
i
i
i
1
1
1
1
)()(
1
)()(
1
1
1
)()(
1
)()(
1 1
)()()()()()()()(
1 1
)()()()(
      
      
      
)()(      
)()()()(      
ee
xxxx
mmxx
mmmxxmxx
mxmx
LL
 
where Tiii XLX is the data covariance matrix of the i
th class, and ],,,[ )()(2
)(
1
i
n
ii
i i
X xxx L=  is a d × ni ma-
trix. Li = I – 1/ni ei eiT is a ni × ni matrix where I is the identity matrix and ei = (1,1,…,1)T is an ni dimen-
sional vector. To further simplify the above equation, we define: 
),,,( 21 nX xxx L=  
 12




=
otherwise           0
class   the tobelongboth   and  if     /1 thjik
ij
kn
W
xx
 
WIL −=  
Thus, we get: 
T
W XLXS =  
It is interesting to note that we could regard the matrix W as the weight matrix of a graph with data 
points as its nodes. Specifically, Wij is the weight of the edge (xi, xj). W reflects the class relationships of 
the data points. The matrix L is thus called graph Laplacian, which plays key role in LPP [9].  
Similarly, we can compute the matrix SB as follows: 
( )( )
( )( )
( )
CXLX
XIXXLX
XIIWX
XWX
XXXWX
nXWX
nnXWX
nn
nn
nnn
nS
T
TT
n
T
TT
n
TT
n
TT
n
T
TT
TTT
TTl
i
n
kj
Ti
k
i
jn
TTl
i
Ti
n
ii
n
i
n
Tl
i
i
l
i
i
i
l
i
Tii
i
l
i
Tii
iB
i
i
iii
+−=
−+−=
−+−=
−=
−=
−=
+−=
+−







=
+−





++++=






+





−





=
−−=
∑ ∑
∑
∑∑∑
∑
= =
=
===
=
      
)(      
)(      
)(      
)(      
      
2      
2      
2      
2)(      
1
1
1
1
1 1,
)()(1
1
)()(
1
)()(
1
1
11
)(
1
)()(
1
)()(
ee
ee
ee
ee
mm
mmmm
mmmmxx
mmmmxxxx
mmmmmm
mmmm
LL
 
where e = (1,1,…,1)T is a n dimensional vector and TTn XIXC )(
1 ee−=  is the data covariance matrix. 
Thus, the generalized eigenvector problem of LDA can be written as follows: 
 13
ww
ww
ww
ww
CXLX
XLXC
XLXXLXC
SS
T
T
TT
WB
λ
λ
λ
λ
+
=⇒
+=⇒
=−⇒
=
1
1  
)1(  
)(  
      
 
Thus, the projections of LDA can be obtained by solving the following generalized eigenvalue problem, 
ww CXLX T λ=  
The optimal projections correspond to the eigenvectors associated with the smallest eigenvalues. If the 
sample mean of the data set is zero, the covariance matrix is simply XXT which is close to the matrix 
XDXT in the LPP algorithm. Our analysis shows that LDA actually aims to preserve discriminating in-
formation and global geometrical structure. Moreover, LDA has a similar form to LPP. However, LDA 
is supervised while LPP can be performed in either supervised or unsupervised manner. 
Proposition 1: The rank of L is n – c. 
Proof: Without loss of generality, we assume that the data points are ordered according to which class 
they are in, so that },,{
11 nxx L  are in the first class, },,{ 211 1 nnn ++ xx L  are in the second class, etc. 
Thus, we can write L as follows: 












=
cL
L
L
L
O
2
1
 
where Li is a square matrix, 
 14
















−−−
−
−
−−−
=
iii
i
i
iii
nnn
n
n
nnn
iL
111
1
1
111
1
1
L
OOM
MOO
L
 
By adding all but the first column vectors to the first column vector and then subtracting the first row 
vector from any other row vectors, we get the following matrix 












100
0
10
000
L
OOM
MO
L
 
whose rank is ni -1. Therefore, the rank of Li is ni -1 and hence the rank of L is n – c.   
Proposition 1 tells us that the rank of XLXT is at most n – c. However, in many cases in appear-
ance-based face recognition, the number of pixels in an image (or, the dimensionality of the image space) 
is larger than n-c, i.e. d>n-c. Thus, XLXT is singular. In order to overcome the complication of a singular 
XLXT, Belhumeur et. al. [1] proposed the Fisherface approach that the face images are projected from 
the original image space to a subspace with dimensionality n-c and then LDA is performed in this sub-
space. 
6. MANIFOLD WAYS OF VISUAL ANALYSIS 
In the above sections, we have described three different linear subspace learning algorithms. The 
key difference between PCA, LDA and LPP is that, PCA and LDA aim to discover the global structure 
of the Euclidean space, while LPP aims to discover local structure of the manifold. In this Section, we 
discuss the manifold ways of visual analysis. 
6.1 Manifold Learning via Dimensionality Reduction 
 15
In many cases, face images may be visualized as points drawn on a low-dimensional manifold 
hidden in a high-dimensional ambient space. Specially, we can consider that a sheet of rubber is crum-
pled into a (high-dimensional) ball. The objective of a dimensionality-reducing mapping is to unfold the 
sheet and to make its low-dimensional structure explicit. If the sheet is not torn in the process, the map-
ping is topology-preserving. Moreover, if the rubber is not stretched or compressed, the mapping pre-
serves the metric structure of the original space. In this paper, our objective is to discover the face mani-
fold by a locally topology-preserving mapping for face analysis (representation and recognition). 
6.2 Learning Laplacianfaces for Representation 
LPP is a general method for manifold learning. It is obtained by finding the optimal linear ap-
proximations to the eigenfunctions of the Laplace Betrami operator on the manifold [9]. Therefore, 
though it is still a linear technique, it seems to recover important aspects of the intrinsic nonlinear mani-
fold structure by preserving local structure. Based on LPP, we describe our Laplacianfaces method for 
face representation in a locality preserving subspace. 
In the face analysis and recognition problem one is confronted with the difficulty that the matrix 
XDXT is sometimes singular. This stems from the fact that sometimes the number of images in the train-
ing set (n) is much smaller than the number of pixels in each image (m). In such a case, the rank of 
XDXT is at most n, while XDXT is an m×m matrix, which implies that XDXT is singular. To overcome the 
complication of a singular XDXT, we first project the image set to a PCA subspace so that the resulting 
matrix XDXT is nonsingular. Another consideration of using PCA as preprocessing is for noise reduction. 
This method, we call Laplacianfaces, can learn an optimal subspace for face representation and recogni-
tion.  
The algorithmic procedure of Laplacianfaces is formally stated below: 
1. PCA projection: We project the image set {xi} into the PCA subspace by throwing away the small-
est principal components. In our experiments, we kept 98% information in the sense of reconstruction 
 16
error. For the sake of simplicity, we still use x to denote the images in the PCA subspace in the fol-
lowing steps. We denote by WPCA the transformation matrix of PCA.  
2. Constructing the nearest-neighbor graph: Let G denote a graph with n nodes. The ith node corre-
sponds to the face image xi. We put an edge between nodes i and j if xi and xj are “close”, i.e. xi is 
among k nearest neighbors of xi or xi is among k nearest neighbors of xj. The constructed nearest-
neighbor graph is an approximation of the local manifold structure. Note that, here we do not use the 
ε - neighborhood to construct the graph. This is simply because it is often difficult to choose the op-
timal ε in the real world applications, while k nearest neighbor graph can be constructed more stably. 
The disadvantage is that the k nearest neighbor search will increase the computational complexity of 
our algorithm. When the computational complexity is a major concern, one can switch to the ε - 
neighborhood.   
3. Choosing the weights: If node i and j are connected, put 
tij
ji
eS
2
xx −
−
=  
where t is a suitable constant. Otherwise, put Sij = 0. The weight matrix S of graph G models the face 
manifold structure by preserving local structure. The justification for this choice of weights can be 
traced back to [3]. 
4. Eigenmap: Compute the eigenvectors and eigenvalues for the generalized eigenvector problem: 
                             ww TT XDXXLX λ=                         (1) 
where D is a diagonal matrix whose entries are column (or row, since S is symmetric) sums of S, 
∑= j jiii SD . L = D −  S is the Laplacian matrix. The i
th row of matrix X is xi. 
 17
Let w0, w1, …, wk-1 be the solutions of equation (1), ordered according to their eigenvalues, 
0≤λ0≤λ1≤L≤λk-1. These eigenvalues are equal to or greater than zero, because the matrices XLXT and 
XDXT are both symmetric and positive semi-definite. Thus, the embedding is as follows: 
xyx TW=→  
LPPPCAWWW =  
],,,[ 110 −= kLPPW www L  
where y is a k-dimensional vector. W is the transformation matrix. This linear mapping best preserves 
the manifold’s estimated intrinsic geometry in a linear sense. The column vectors of W are the so called 
Laplacianfaces. 
6.3 Face Manifold Analysis 
Now consider a simple example of image variability. Imagine that a set of face images are gener-
ated while the human face rotates slowly. Intuitively, the set of face images correspond to a continuous 
curve in image space, since there is only one degree of freedom, viz. the angel of rotation. Thus, we can 
say that the set of face images are intrinsically one-dimensional. Many recent works [18][19][21][27] 
have shown that the face images do reside on a low-dimensional submanifold embedded in a high-
dimensional ambient space (image space). Therefore, an effective subspace learning algorithm should be 
able to detect the nonlinear manifold structure. The conventional algorithms, such as PCA and LDA, 
model the face images in Euclidean space. They effectively see only the Euclidean structure; thus, they 
fail to detect the intrinsic low-dimensionality. 
With its neighborhood preserving character, the Laplacianfaces seem to be able to capture the in-
trinsic face manifold structure to a larger extent. Figure 1 shows an example that the face images with 
various pose and expression of a person are mapped into two-dimensional subspace. The face image 
dataset used here is the same as that used in [18]. This dataset contains 1965 face images taken from se-
 18
quential frames of a small video. The size of each image is 20×28 pixels, with 256 gray levels per pixel. 
Thus, each face image is represented by a point in the 560-dimensional ambient space. However, these 
images are believed to come from a submanifold with few degrees of freedom. We leave out 10 samples 
for testing, and the remaining 1955 samples are used to learn the Laplacianfaces. As can be seen, the 
face images are mapped into a two-dimensional space with continuous change in pose and expression. 
Figure 1. Two-dimensional linear embedding of face images by Laplacianfaces. 
As can be seen, the face images are divided into two parts, the faces with open 
mouth and the faces with closed mouth. Moreover, it can be clearly seen that 
the pose and expression of human faces change continuously and smoothly, 
from top to bottom, from left to right. The bottom images correspond to points 
along the right path (linked by solid line), illustrating one particular mode of 
variability in pose. 
 19
The representative face images are shown in the different parts of the space. The face images are divided 
into two parts. The left part includes the face images with open mouth, and the right part includes the 
face images with closed mouth. This is because in trying to preserve local structure in the embedding, 
the Laplacianfaces implicitly emphasizes the natural clusters in the data. Specifically, it makes the 
neighboring points in the image face nearer in the face space, and faraway points in the image face far-
ther in the face space. The bottom images correspond to points along the right path (linked by solid line), 
illustrating one particular mode of variability in pose.  
The ten testing samples can be simply located in the reduced representation space by the Lapla-
cianfaces (column vectors of the matrix W). Figure 2 shows the result. As can be seen, these testing 
Figure 2. Distribution of the 10 testing samples in the reduced representation 
subspace. As can be seen, these testing samples optimally find their coordi-
nates which reflect their intrinsic properties, i.e. pose and expression. 
 20
samples optimally find their coordinates which reflect their intrinsic properties, i.e. pose and expression. 
This observation tells us that the Laplacianfaces are capable of capturing the intrinsic face manifold 
structure to some extent.  
 Recall that both Laplacian Eigenmap and LPP aim to find a map which preserves the local struc-
ture. Their objective function is as follows: 
( )∑ −
ij
ijji
f
Sff 2)()(min xx  
The only difference between them is that, LPP is linear while Laplacian Eigenmap is non-linear. Since 
they have the same objective function, it would be important to see to what extent LPP can approximate 
Laplacian Eigenmap. This can be evaluated by comparing their eigenvalues. Figure 3 shows the eigen-
values computed by the two methods. As can be seen, the eigenvalues of LPP is consistently greater 
than those of Laplacian Eigenmaps, but the difference between them is small. The difference gives a 
measure of the degree of the approximation. 
Figure 3. The eigenvalues of LPP and Laplacian Eigenmap. 
 21
7. EXPERIMENTAL RESULTS 
Some simple synthetic examples given in [9] show that LPP can have more discriminating power 
than PCA and be less sensitive to outliers. In this section, several experiments are carried out to show 
the effectiveness of our proposed Laplacianfaces method for face representation and recognition. 
7.1 Face Representation Using Laplacianfaces 
As we described previously, a face image can be represented as a point in image space. A typical 
image of size m×n describes a point in m×n-dimensional image space. However, due to the unwanted 
variations resulting from changes in lighting, facial expression, and pose, the image space might not be 
an optimal space for visual representation. 
In Section 3, we have discussed how to learn a locality preserving face subspace which is insensi-
tive to outlier and noise. The images of faces in the training set are used to learn such a locality preserv-
ing subspace. The subspace is spanned by a set of eigenvectors of equation (1), i.e. w0, w1, …, wk-1. We 
can display the eigenvectors as images. These images may be called Laplacianfaces. Using the Yale 
face database as the training set, we present the first 10 Laplacianfaces in Figure 4, together with Eigen-
faces and Fisherfaces.  A face image can be mapped into the locality preserving subspace by using the 
Laplacianfaces. It is interesting to note that the Laplacianfaces are somehow similar to Fisherfaces. 
Figure 4. The first 10 Eigenfaces (first row), Fisherfaces (second row) and 
Laplacianfaces (third row) calculated from the face images in the YALE da-
tabase. 
 22
7.2 Face Recognition Using Laplacianfaces 
Once the Laplacianfaces are created, face recognition [2][14][28][29] becomes a pattern classifica-
tion task. In this section, we investigate the performance of our proposed Laplacianfaces method for face 
recognition. The system performance is compared with the Eigenfaces method [28] and the Fisherfaces 
method [2], two of the most popular linear methods in face recognition.  
In this study, three face databases were tested. The first one is the PIE (pose, illumination, and ex-
pression) database from CMU [25], the second one is the Yale database [30], and the third one is the 
MSRA database collected at the Microsoft Research Asia. In all the experiments, preprocessing to locate 
the faces was applied. Original images were normalized (in scale and orientation) such that the two eyes 
were aligned at the same position. Then, the facial areas were cropped into the final images for matching. 
The size of each cropped image in all the experiments is 32×32 pixels, with 256 grey levels per pixel. 
Thus, each image is represented by a 1024-dimensional vector in image space. No further preprocessing 
is done. Figure 5 shows an example of the original face image and the cropped image. Different pattern 
classifiers have been applied for face recognition, including nearest-neighbor [2], Bayesian [15], Sup-
port Vector Machine [17], etc. In this paper, we apply the nearest-neighbor classifier for its simplicity. 
In short, the recognition process has three steps. First, we calculate the Laplacianfaces from the 
training set of face images; then the new face image to be identified is projected into the face subspace 
spanned by the Laplacianfaces; finally, the new face image is identified by a nearest-neighbor classifier. 
Figure 5. The original face image and the cropped image.  
 23
7.2.1 YALE DATABASE 
The Yale face database [30] was constructed at the Yale Center for Computational Vision and 
Control. It contains 165 grayscale images of 15 individuals. The images demonstrate variations in light-
ing condition (left-light, center-light, right-light), facial expression (normal, happy, sad, sleepy, sur-
prised, and wink), and with/without glasses.  
A random subset with six images per individual (hence 90 images in total) was taken with labels 
to form the training set. The rest of the database was considered to be the testing set. The training sam-
ples were used to learn the Laplacianfaces. The testing samples were then projected into the low-
dimensional representation subspace. Recognition was performed using a nearest neighbor classifier. 
We averaged the results over 20 random splits.  
Figure 6. Recognition accuracy vs. dimensionality reduction on Yale database.
Table 1.  Performance comparison on the Yale database  
Approach Dims Error Rate 
Eigenfaces 33 25.3% 
Fisherfaces 14 20.0% 
Laplacianfaces 28 11.3% 
 24
Note that, for LDA, there are at most c-1 nonzero generalized eigenvalues, and so an upper bound 
on the dimension of the reduced space is c-1, where c is the number of individuals [2]. In general, the 
performance of the Eigenfaces method and the Laplacianfaces method varies with the number of dimen-
sions. We show the best results obtained by Fisherfaces, Eigenfaces, and Laplacianfaces.  
The recognition results are shown in Table 1. It is found that the Laplacianfaces method signifi-
cantly outperforms both Eigenfaces and Fisherfaces methods. The recognition approaches the best re-
sults with 14, 28, 33 dimensions for Fisherfaces, Laplacianfaces, and Eigenfaces respectively. The error 
rates of Fisherfaces, Laplacianfaces, and eigenfaces are 20%, 11.3%, and 25.3% respectively. The corre-
sponding face subspaces are called optimal face subspace for each method. There is no significant im-
provement if more dimensions are used. Figure 6 shows the plots of error rate vs. dimensionality reduc-
tion.  
7.2.2 PIE DATABASE 
Figure 7. The sample cropped face images of one individual from PIE database. The 
original face images are taken under varying pose, illumination, and expression. 
Table 2.  Performance comparison on the PIE database 
Approach Dims Error Rate 
Eigenfaces 150 20.6% 
Fisherfaces 67 5.7% 
Laplacianfaces 110 4.6% 
 25
The CMU PIE face database contains 68 subjects with 41,368 face images as a whole. The 
face images were captured by 13 synchronized cameras and 21 flashes, under varying pose, 
illumination and expression. We used 170 face images for each individual in our experiment, 85 
for training and the other 85 for testing. We averaged the results over 20 random splits. Figure 
7 shows some of the faces with pose, illumination and expression variations in the PIE database.  
Table 2 shows the recognition results. As can be seen, Fisherfaces performs comparably to our al-
gorithm on this database, while Eigenfaces performs poorly. The error rate for Laplacianfaces, Fisher-
Figure 8. Recognition accuracy vs. dimensionality reduction on PIE database
Figure 9. The sample cropped face images of 8 individuals from MSRA data-
base. The face images in the first row are taken in the first session, which are 
used for training. The face images in the second row are taken in the second 
session, which are used for testing. The two images in the same column are 
corresponding to the same individual. 
 26
faces, and Eigenfaces are 4.6%, 5.7%, and 20.6%, respectively. Figure 8 shows a plot of error rate vs. 
dimensionality reduction. As can be seen, the error rate of our Laplacianfaces method decreases fast as 
the dimensionality of the face subspace increases, and achieves the best result with 110 dimensions. 
There is no significant improvement if more dimensions are used. Eigenfaces achieves the best result 
with 150 dimensions. For Fisherfaces, the dimension of the face subspace is bounded by c-1, and it 
achieves the best result with c-1 dimensions. The dashed horizontal line in the figure shows the best re-
sult obtained by Fisherfaces. 
7.2.3  MSRA DATABASE 
This database was collected at Microsoft Research Asia. It contains 12 individuals, captured in 
two different sessions with different backgrounds and illuminations. 64 to 80 face images were collected 
Figure 10. Recognition accuracy vs. dimensionality reduction on MSRA database
Table 3.  Performance comparison on the MSRA database 
Approach Dims Error Rate 
Eigenfaces 142 35.4% 
Fisherfaces 11 26.5% 
Laplacianfaces 66 8.2% 
 
 27
for each individual in each session. All the faces are frontal. Figure 9 shows the sample cropped face 
images from this database. In this test, one session was used for training and the other was used for test-
ing. Table 3 shows the recognition results. Laplacianfaces method has lower error rate (8.2%) than those 
of eigenfaces (35.4%) and fisherfaces (26.5%). Figure 10 shows a plot of error rate vs. dimensionality 
reduction. 
7.2.4 IMPROVEMENT WITH THE NUMBER OF TRAINING SAMPLES 
In the above subsections, we have shown that our Laplacianfaces approach outperforms the Eigen-
faces and Fisherfaces approaches on three different face databases. In this section, we evaluate the per-
Figure 11. Performance comparison on the MSRA database with different 
number of traning samples. The training samples are from the set MSRA-
S1. The images in MSRA-S2 are used for testing exclusively. As can be 
seen, our Laplacianfaces method takes advantage of more training sam-
ples. The error rate reduces from 26.04% with 10% training samples to 
8.17% with 100% training samples. There is no evidence for fisherfaces 
that it can take advantage of more training samples.  
0
5
10
15
20
25
30
35
40
45
50
10 40 70 100
Percentage of training samples in the image set MSRA - S1
Er
ro
r r
at
e
Laplacianfaces
Fisherfaces
Eigenfaces
 28
formances of these three approaches with different number of training samples. We expect that a good 
algorithm should be able to take advantage of more training samples. 
We used the MSRA database for this experiment. Let MSRA-S1 denote the image set taken in the 
first session, and MSRA-S2 denote the image set taken in the second session. MSRA-S2 was exclusively 
used for testing. 10%, 40%, 70%, and 100% face images were randomly selected from MSRA-S1 for 
training. For each of the first three cases, we repeated 20 times and computed the error rate on average. 
In Figure 11 and Table 4 we show the experimental results of the improvement of the performance of 
our algorithm with the number of training samples. As can be seen, our algorithm takes advantage of 
more training samples. The performance improves significantly as the number of training sample in-
creases. The error rate of recognition reduces from 26.04% with 10% training samples to 8.17% with 
100% training samples. This is due to the fact that, our algorithm aims at discovering the underlying 
face manifold embedded in the ambient space and more training samples help to recover the manifold 
structure.  Eigenfaces also takes advantage of more training samples to some extent. The error rate of 
Eigenfaces reduces from 46.48% with 10% training samples to 35.42% with 100% training samples. But 
its performance starts to converge at 40% (training samples) and little improvement is obtained if more 
training samples are used. For Fisherfaces, there is no convincing evidence that it can take advantage of 
more training samples.  
Table 4.  Recognition error rate with different number of training samples 
Percentage of training samples in MSRA-S1  
10% 40% 70% 100% 
Laplacianfaces 26.04% 14.71% 13.15% 8.17% 
Fisherfaces 26.96% 20.05% 29.43% 26.43% 
Eigenfaces 46.48% 37.76% 38.54% 35.42% 
 29
7.3  Discussions 
Three experiments on three databases have been systematically performed. These experiments re-
veal a number of interesting points: 
1. All these three approaches performed better in the optimal face subspace than in the original im-
age space. 
2. In all the three experiments, Laplacianfaces consistently performs better than Eigenfaces and Fish-
erfaces. Especially, it significantly outperforms Fisherfaces and Eigenfaces on Yale database and 
MSRA database.  These experiments also show that our algorithm is especially suitable for frontal 
face images. Moreover, our algorithm takes advantage of more training samples, which is impor-
tant to the real world face recognition systems.  
3. Comparing to the Eigenfaces method, the Laplacianfaces method encodes more discriminating in-
formation in the low-dimensional face subspace by preserving local structure which is more im-
portant than the global structure for classification, especially when nearest neighbor like classifiers 
are used. In fact, if there is a reason to believe that Euclidean distances (||xi – xj||) are meaningful 
only if they are small (local), then our algorithm finds a projection that respects such a belief. An-
other reason is that, as we show in Figure 1, the face images probably reside on a nonlinear mani-
fold. Therefore, an efficient and effective subspace representation of face images should be capa-
ble of charactering the nonlinear manifold structure, while the Laplacianfaces are exactly derived 
by finding the optimal linear approximations to the eigenfunctions of the Laplace Betrami operator 
on the face manifold. By discovering the face manifold structure, Laplacianfaces can identify the 
person with different expressions, poses, and lighting conditions. 
8. CONCLUSION AND FUTURE WORK 
The manifold ways of face analysis (representation and recognition) is introduced in this paper in 
order to detect the underlying nonlinear manifold structure in the manner of linear subspace learning. To 
 30
the best of our knowledge, this is the first devoted work on face representation and recognition which 
explicitly considers the manifold structure. The manifold structure is approximated by the adjacency 
graph computed from the data points. Using the notion of the Laplacian of the graph, we then compute a 
transformation matrix which maps the face images into a face subspace. We call this the Laplacianfaces 
approach. The Laplacianfaces are obtained by finding the optimal linear approximations to the eigen-
functions of the Laplace Beltrami operator on the face manifold. This linear transformation optimally 
preserves local manifold structure. Theoretical analysis of the LPP algorithm and its connections to PCA 
and LDA are provided. Experimental results on PIE, Yale and MSRA databases show the effectiveness 
of our method.  
One of the central problems in face manifold learning is to estimate the intrinsic dimensionality of 
the nonlinear face manifold, or, degrees of freedom. We know that the dimensionality of the manifold is 
equal to the dimensionality of the local tangent space. Some previous works [33][34] show that the local 
tangent space can be approximated using points in a neighbor set. Therefore, one possibility is to esti-
mate the dimensionality of the tangent space.  
Another possible extension of our work is to consider the use of the unlabeled samples. It is im-
portant to note that the work presented here is a general method for face analysis (face representation 
and recognition) by discovering the underlying face manifold structure. Learning the face manifold (or, 
learning Laplacianfaces) is essentially an unsupervised learning process. And in many practical cases, 
one finds a wealth of easily available unlabeled samples. These samples might help to discover the face 
manifold. For example, in [4], it is shown how unlabeled samples are used for discovering the manifold 
structure and hence improving the classification accuracy. Since the face images are believed to reside 
on a sub-manifold embedded in a high-dimensional ambient space, we believe that the unlabeled sam-
ples are of great value. We are currently exploring these problems in theory and practice. 
 
 31
REFERENCES 
[1] A. U. Batur and M. H. Hayes, “Linear Subspace for Illumination Robust Face Recognition”, IEEE 
Int. Conf. on Computer Vision and Pattern Recognition, Hawaii, Dec. 11-13, 2001. 
[2] P. N. Belhumeur, J. P. Hespanha and D. J. Kriegman, “Eigenfaces vs. Fisherfaces: Recognition Us-
ing Class Specific Linear Projection”, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 
19, No. 7, 1997, pp. 711-720. 
[3] M. Belkin and P. Niyogi, “Laplacian Eigenmaps and Spectral Techniques for Embedding and Clus-
tering”, Advances in Neural Information Processing System 15, Vancouver, British Columbia, Can-
ada, 2001. 
[4] M. Belkin and P. Niyogi, “Using Manifold Structure for Partially Labeled Classification”, Advances 
in Neural Information Processing System 15, Vancouver, British Columbia, Canada, 2002. 
[5] M. Brand, “Charting a Manifold”, Advances in Neural Information Processing Systems, Vancouver, 
Canada, 2002. 
[6] Fan R. K. Chung, Spectral Graph Theory, Regional Conferences Series in Mathematics, number 92, 
1997. 
[7] Y. Chang, C. Hu and M. Turk, “Manifold of Facial Expression”, Proc. IEEE International Work-
shop on Analysis and Modeling of Faces and Gestures, Nice, France, Oct 2003. 
[8] R. Gross, J. Shi, and J. Cohn, “Where to go with Face Recognition”, Third Workshop on Empirical 
Evaluation Methods in Computer Vision, Kauai, Hawaii, December 11-13, 2001. 
[9] Xiaofei He and Partha Niyogi, “Locality Preserving Projections”, Advances in Neural Information 
Processing Systems, Vancouver, Canada, 2003. 
[10] K.-C. Lee, J. Ho, M.-H. Yang, and D. Kriegman, “Video-Based Face Recognition Using Probabilis-
tic Appearance Manifolds”, Proc. IEEE Conf. on Computer Vision and Pattern Recognition, pp. 
313-320, vol. 1, 2003. 
 32
[11] A. Levin and A. Shashua, “Principal Component Analysis Over Continuous Subspaces and Inter-
section of Half-Spaces”, Proc. of the European Conference on Computer Vision (ECCV), Copenha-
gen, Denmark, May 2002. 
[12] S. Z. Li, X. W. Hou, H. J. Zhang, Q. S. Cheng, “Learning Spatially Localized, Parts-Based Repre-
sentation”, IEEE Int. Conf. on Computer Vision and Pattern Recognition, Hawaii, Dec. 11-13, 2001. 
[13] Q. Liu, R. Huang, H. Lu, and S. Ma, “Face Recognition Using Kernel Based Fisher Discriminant 
Analysis”, in Proc. of the fifth Int. Conf. on Automatic Face and Gesture Recognition, Washington 
D. C., May 2002. 
[14] A. M. Martinez and A. C. Kak, “PCA versus LDA”, IEEE Trans. on Pattern Analysis and Machine 
Intelligence, 23(2):228-233, 2001. 
[15] B. Moghaddam and A. Pentland, “Probabilistic Visual Learning for Object Representation”, IEEE 
Trans. On Pattern Analysis and Machine Intelligence, vol 19, pp. 696-710, 1997. 
[16] H. Murase and S. K. Nayar, “Viasual Learning and Recognition of 3-D objects from appearache”, 
International Journal of Computer Vision, 14:5-24, 1995. 
[17] P. J. Phillips, “Support Vector Machines Applied to Face Recognition”. In Advances in Neural In-
formation Processing Systems 11, pp. 803-809, 1998. 
[18] Sam T. Roweis, and Lawrence K. Saul, “Nonlinear Dimensionality Reduction by Locally Linear 
Embedding”, Science, vol 290, 22 December 2000. 
[19] Sam Roweis, Lawrence Saul and Geoff Hinton, “Global Coordination of Local Linear Models”, Ad-
vances in Neural Information Processing System 14, 2001. 
[20] Lawrence K. Saul and Sam T. Roweis, “Think Globally, Fit Locally: Unsupervised Learning of 
Low Dimensional Manifolds”, Journal of Machine Learning Research, v4, pp. 119-155, 2003. 
[21] H. Sebastian Seung and Daniel D. Lee, “The Manifold Ways of Perception”, Science, vol 290, 22 
December 2000. 
 33
[22] T. Shakunaga, and K. Shigenari, “Decomposed Eigenface for Face Recognition under Various 
Lighting Conditions”, IEEE Int. Conf. on Computer Vision and Pattern Recognition, Hawaii, Dec. 
11-13, 2001. 
[23] A. Shashua, A. Levin and S. Avidan, “Manifold Pursuit: A New Approach to Appearance Based 
Recognition”, International Conference on Pattern Recognition (ICPR), August 2002, Quebec, 
Canada. 
[24] J. Shi and J. Malik, “Normalized Cuts and Image Segmentation”, IEEE Trans. on Pattern Analysis 
and Machine Intelligence, 22 (2000), 888-905. 
[25] T. Sim, S. Baker, and M. Bsat, “The CMU Pose, Illumination, and Expression (PIE) Database”, in 
Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition, 
May, 2002. 
[26] L. Sirovich and M. Kirby, “Low-Dimensional Procedure for the Characterization of Human Faces”, 
Journal of the Optical Society of America A, vol. 4, pp. 519-524, 1987. 
[27] J. B. Tenenbaum, Vin de Silva, and Johh C. Langford, “A Global Geometric Framework for 
Nonlinear Dimensionality Reduction”,Science, vol 290, 22 December 2000. 
[28] M. Turk and A. P. Pentland, “Face Recognition Using Eigenfaces”, IEEE Conference on Computer 
Vision and Pattern Recognition, Maui, Hawaii, 1991. 
[29] L. Wiskott, J.M. Fellous, N. Kruger, and C.v.d. Malsburg, “Face Recognition by Elastic Bunch 
Graph Matching”, IEEE Trans. PAMI, 19:775-779, 1997. 
[30] Yale Univ. Face Database, http://cvc.yale.edu/projects/yalefaces/yalefaces.html, 2002. 
[31] Ming-Hsuan Yang, “Kernel Eigenfaces vs. Kernel Fisherfaces: Face Recognition Using Kernel 
Methods”, in Proc. of the fifth Int. Conf. on Automatic Face and Gesture Recognition, Washington 
D. C., May 2002. 
 34
[32] J. Yang, Y. Yu, and W. Kunz, “An Efficient LDA Algorithm for Face Recognition”, The sixth In-
ternational Conference on Control, Automation, Robotics and Vision, Singapore, 2000. 
[33] Hongyuan Zha and Zhenyue Zhang, “Isometric Embedding and Continuum ISOMAP”, in Proc. Of 
the Twentieth International Conference on Machine Learning, pp. 864-871, 2003. 
[34] Zhengyue Zhang and Hongyuan Zha, “Principal Manifolds and Nonlinear Dimension Reduction via 
Local Tangent Space Alignment”, CSE-02-019, Technical Report, CSE, Penn State Univ., 2002. 
[35] W. Zhao, R. Chellappa, and P. J. Phillips, “Subspace Linear Discriminant Analysis for Face Recog-
nition”, Technical Report CAR-TR-914, Center for Automation Research, University of Maryland, 
1999. 
 

