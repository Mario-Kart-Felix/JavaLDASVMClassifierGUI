A Contemporary Overview of Probabilistic Latent Variable Models
A Contemporary Overview of Probabilistic Latent Variable
Models
Rick Farouni tfarouni@mgh.harvard.edu
Department of Molecular Pathology,
Massachusetts General Hospital,
Harvard Medical School
Editor:
Abstract
In this paper we provide a conceptual overview of latent variable models within a proba-
bilistic modeling framework, an overview that emphasizes the compositional nature and the
interconnectedness of the seemingly disparate models commonly encountered in statistical
practice.
Keywords: Latent Variable Models, Deep Generative Models, Probabilistic Models
1. Introduction
In this paper, we will be concerned with building probabilistic latent variable models for data
that populate an N×P matrix Y whose every element ynp ∈ R is real-valued measurement.
Y =

yT1
...
yTn
...
yTN
 =

y1,1 . . . y1,P
...
. . .
...
yn,1 . . . yn,P
...
. . .
...
yN,1 . . . yN,P
 (1)
In general, we will be limiting our focus mainly to the the multivariate statistics setting
in which we treat an observation as a multivariate random vector yn = (yn1, . . . , ynP )
consisting of P features and the data as a set of N observations y = {y1, · · · ,yn, · · · ,yN}.
Accordingly, we can think of the data matrix Y as consisting of N observations, each a
P -dimensional vector, that populate the rows of the matrix.
2. Basic Definitions
In order to give a rigorous constructive account of probabilistic latent variable models, we
begin with defining several foundational concepts that will later serve as our building blocks.
In what follows, we attempt to convey the intuition at the expense of mathematical rigor.
More rigorous definitions can be found in Durrett (2010) and Athreya and Lahiri (2006).
1
ar
X
iv
:1
70
6.
08
13
7v
1 
 [
st
at
.M
L
] 
 2
5 
Ju
n 
20
17
Farouni
2.1 What is a Probability Model?
Our first building block is a probability model. A probability model is defined as the triplet
consisting of a set, a set of sets, and a function. More precisely given by
(Y, B(Y), p)
where
1. Y is the sample space
2. B (Y) is the Borel σ-algebra
3. p is a probability measure
Whereas the sample space is a relatively easy concept to understand - it is a set of all
possible outcomes - the notion of a Borel σ-algebra tends to be more involved. So what
exactly is a Borel σ-algebra? The short answer is that it is the set of all events of interest;
technically, the set of subsets of Y. For the long answer, we first need to define an algebra.
An algebra in this context is a set along with a collection of operations. Here is an example.
First take our sample space, the set Y, and equip it with two binary operations (∪,∩), the
union and intersection. Now let us consider two events only - the empty set ∅ denoting the
event that nothing happens and the sample space Y denoting the event that all outcomes
enumerated in the sample space occur. Starting with these two events only, we can, using
the two operations, construct the set of all possible subsets of Y, which is called the powerset(
P (Y) ,∩,∪,¬, ∅,Y
)
Since every possible subset of Y is an event, accordingly the powerset consists of all possible
events. With a finite number of outcomes, the number of all possible events is finite as well.
Therefore, the powerset is an algebra that is closed under the union and intersection of
finitely many subsets. However, when our sample space is not finite, but rather countably
infinite, we would instead need a specialized set of subsets to enumerate all possible events.
That set is called a σ-algebra. It is simply a sub-algebra of the powerset, completed to
include countably infinite operations. Lastly, when the events are subsets of the real line
R, we would need to resort to a more specialized algebra, called the Borel σ-algebra. It
is defined as the σ-algebra generated by the open sets and which can be constructed from
open or closed sets by repeatedly taking countable unions and intersections. In notation,
we define it as such.
B(R) := σ(C); C = {(a, b],−∞ ≤ a ≤ b ≤ ∞}
Whereas the powerset is the largest possible σ-algebra, the collection of subsets {∅,Y} is
the smallest. The Borel σ-algebra lies between the two extremes
{∅,Y} ⊂ B(Y) ⊂ P(Y)
and it is the smallest σ-algebra containing all open sets in R (hyper-rectangles in RP ).
The third component of a probability model involves the notion of a measure. What
is a probability measure? It is simply a function p : B (Y) → [0, 1] that maps events to a
2
A Contemporary Overview of Probabilistic Latent Variable Models
real valued number between zero and one, thus assigning each event a finite probability of
occurring. The notion of a probability measure is tightly intertwined with the concept of a
random variable. To illustrate, our multivariate observation y = (y1, . . . , yP ) is a real-valued
continuous random vector with distribution µ that takes values in the measurable space(
Y, B(Y)
)
. Given the probability triplet, we can define a random vector as a measurable
function from the sample space to a P -dimensional real space y : Y 7→ RP such that
{y ≤ y} ∈ B; where B ∈ B
(
RP
)
The important point is that the distribution of y induces a probability measure on the
measurable space
(
Y, B(Y)
)
and is given by
µ(B) = p (y ∈ B) ; where B ∈ B
(
RP
)
where p (y ∈ B) denotes the probability that the event {y ≤ y} lies in a Borel set B. Note
that a Borel set B is an element of a Borel σ-algebra.
2.2 What is a Statistical Model?
We build a probabilistic model starting with the joint distribution of all observable and
unobservable variables. Since we have not introduced any unobservable quantities yet, our
entry point is the following joint probability distribution for all N observations,
p (y) = p (y1,y2, · · · ,yN ) (2)
where p is a joint probability density function corresponding to a specific, known probability
measure, or equivalently, to a known random variable.
Technically, the joint density given by Equation 2 denotes a probability model, not a
statistical model. To illustrate, letM(Y) denote the space of all probability measures on the
measurable space
(
Y, B(Y)
)
. As can be seen, there are infinitely many possible probability
measures we can choose from. That is, M(Y) can be too large a space for many inference
problems with limited data. To make inference a less daunting task we can restrict ourselves
to a indexed subset of the space by introducing an unobservable variable, a parameter θ ∈ Θ
that represents the pattern that explains the data, where the parameter space Θ is the set
of all possible values of θ. As a result, the probability measures Pθ are now elements of
PM(Y), the space of all probability measures on Θ with elements Pθ ∈ PM(Y) indexed
by a parameter θ ∈ Θ.
Accordingly, a statistical model P can be defined as a subset of the space of all probability
measures P ⊂ M(Y) such that P = {Pθ | θ ∈ Θ} where θ → Pθ is a bijective and
measurable assignment. We say that the model P is parametric statistical model if Θ ⊂ Rd
for some d ∈ N. The subset P ⊂ PM(Y) can be even restricted further. We can specify
a family of parametric models G = {Gθ | θ ∈ Θ} where θ → Gθ is smooth. For example,
G = {N(θ, 1) : θ ∈ Θ} specifies the one-dimensional normal location family of models. If
on the other hand we choose the parameter space Θ to be infinite dimensional, then we say
that P is a nonparametric statistical model such that Θ is equivalent to PM(Y), the space
of all probability measures on
(
Y,B(Y)
)
. As a result, we obtain the following hierarchy of
probability model spaces.
G ⊂ P ⊆ PM ⊂M
3
Farouni
Now, by introducing the parameter θ, we can now specify the joint density of a parametric
statistical model for our multivariate observations as such
p (y;θ) = p (y1,y2, · · · ,yN ;θ) (3)
where θ ∈ Θ ⊂ Rd. In the case of a parametric statistical model, the superscript d is a finite
integer, whereas in the case of a nonparametric statistical model, it is an infinite integer.
Note that we use the semicolon to denote that the parameter θ is not a random variable.
2.3 What is a Bayesian Model?
In frequentist statistics, we treat a parameter θ as an unobservable unknown quantity and
estimation involves finding θ∗ ∈ Θ that governs the distribution of the data we observe if
the true generating model indeed lies inside the family of probability models we specify.
In the probabilistic perspective, we treat a parameter as unobservable random quantity
whose prior distributions quantifies our initial uncertainty. Learning involves computing
the posterior distribution of θ that quantifies the residual uncertainty that results after we
observe the data.
Accordingly, the joint probability density of all observable and unobservable random
variables can now be given by
p (y,θ) = p (y1,y2, · · · ,yN ,θ) (4)
Note that Equation 4 defines a general probabilistic model, not a Bayesian model. To reduce
it to a Bayesian model, we need to factorize the joint into the product of a conditional data
distribution and a marginal distribution prior distribution as such
p (y, θ) =
N∏
n=1
p (yn | θ) p (θ) (5)
This factorization entails a particular set of assumptions. To illustrate them, we need to
define a Bayesian model first.
A parametric Bayesian statistical model (P,Π) consists of a model P, the data distri-
bution, and a prior distribution Π on Θ such that θ is a random variable taking values in
Θ given that Π
(
{Pθ : θ ∈ Θ}
)
= 1. A Bayesian model is a generative model in which the
data is generated hierarchically in two stages:
θ ∼ Π
yn | θ
i.i.d.∼ Pθ θ ∈ Θ ⊂ Rd
(6)
A Bayesian model can be thought of as a random mixture model where we first sample from
a mixing measure θ ∼ Π, then sample from a component yn | θ ∼ Pθ. After we observe
data, the prior is then updated to the posterior Π(. | y1,y2, · · · ).
On the other hand, a nonparametric Bayesian model is a Bayesian model whose prior
Π is defined on an infinite dimensional parameter space Θ. The corresponding two-stage
hierarchical model can be given as such.
P ∼ Π
yn | P ∼iid P P ∈ P
(7)
4
A Contemporary Overview of Probabilistic Latent Variable Models
A prior distribution on an infinite dimensional space is a stochastic process. Defining an
infinite dimensional prior distributions is not straightforward, but one way to construct a
prior distribution Π on Θ is through De Finetti’s Theorem.
Theorem 1 (De Finetti’s Theorem) A sequence of random variables {yn}∞n=1 with val-
ues on Y is exchangeable if and only if there is a unique measure Π on Θ such that for all
N
p(y1,y2, ...,yN ) =
∫
Θ
(
N∏
n=1
p(yn | θ)
)
p(θ)dθ (8)
De Finetti’s gives us a infinite mixture representation of the joint probability of the ob-
servations. More importantly, the theorem implies that there exists a random probability
measure P such that the sequence of exchangeable observations are i.i.d conditional on P .
Implicit in the resulting mixture representation is a data-generating process that gives rise
to the observations.
What is exchangeability? We say a sequence of random variables is exchangeable when
their distribution is invariant under permutation of the indices. When building statistical
models, we can use exchangeability and De Finetti’s Theorem to treat an infinite exchange-
able sequence as a conditionally i.i.d sequence of random variables. In data applications,
exchangeability implies that the future is like the past.
Distributional Symmetries It is important to highlight here that exchangeability is one
of the four basic probabilistic symmetries defined by Kallenberg (2006); namely, stationar-
ity, contractability, exchangeability, and rotatability. These four distributional symmetries
correspond to invariance properties under the four respective transformations of shifts,
sub-sequences, permutations, and isometries. The four probabilistic symmetries form a hi-
erarchy with stationarity at the bottom and rotatability, the strongest invariance property,
at the top. Exchangeability in particular allows us, through conditioning, to transform the
joint density into a mixture of densities.
Example of a Bayesian model When Y is finite, there is usually a natural unique mea-
sure Π we can obtain. More specifically, if Y = {1, 2, . . . ,K}, then PM(Y) = {(p1, . . . , pK) :
0 ≤ pk ≤ 1,
∑
pk = 1}. That is, the space of probability measures corresponds to a simplex
parametrized by a K − 1 dimensional vector p = (p1, . . . , pK−1). A natural prior Π to
specify on p is the Dirichet distribution. For example, consider the Bayesian model(P,Π)
where the observation model P is the Categorical distribution defined on the sample space
Y = {1, 2, . . . ,K}, and the prior Π is the Dirichlet distribution defined on the simplex
Θ = {(p1, . . . , pK) : 0 ≤ pk ≤ 1,
∑
pk = 1}
p ∼ Dirichlet(α)
yi | p ∼ Categorical(p)
where α =
( α
K
, · · · , α
K
)
p = (p1, · · · , pK)
Here, the Dirichlet distribution is the conjugate prior of the Categorical distribution and
the concentration hyperparameter vector α represents the number of pseudo-observations,
5
Farouni
Figure 1: Dirichlet Distributions
the a priori weights, for each of the K clusters. Figure 1 shows density and sampling plots
corresponding to Dirichlet distributions for K = 3 and several choices of α.
The Dirichlet distribution is a conjugate prior that allows us to sample iid from Y =
{1, 2, . . . ,K}. To generalize, we can make Y = R thus obtaining the space of all measures
M(R) defined on the measurable space (R,B), which is the real line equipped with the
Borel σ-algebra. It can be shown (?) that for every partition B1, B2, . . . , Bk of the real line
R by Borel sets, there exists a unique measure DPα on M(R) called the Dirichlet Process
with parameter α satisfying
(P (B1), P (B2), . . . , P (Bk)) = DP (αB1, αB2, . . . , αBk))
That is, when Y = R, then M(R), is the space of all probability measures on R. If
the sample space Y = R is partitioned into measurable subsets, then for every partition
(B1, B2, . . . , Bk), the prior probability measure Π on (p(B1), p(B2), . . . , p(Bk)) is a Dirichlet
process prior.
The Dirichlet process DP (G0, α) is the simplest distribution - in terms of the extent
of independence it assumes - that can have this form. That is probably why it has been
referred to as the normal distribution of Bayesian nonparametrics. More specifically, the
location of the atoms, the θ′s, are sampled iid but the weights are sampled as independent
proportions. Now, the weights cannot be sampled iid because then they will not sum up to
one. However, the next best thing to iid sampling is to obtain independent proportions, as
is done using the stick-breaking representation.
6
A Contemporary Overview of Probabilistic Latent Variable Models
3. Constructing Probabilistic Latent Variable Models
We begin by describing the probabilistic modeling approach. Next we explain what a latent
variable is and provide a general formulation of the latent variable models. We end the
section by giving a brief history of latent variable modeling.
3.1 The Probabilistic Modeling Approach
A quick insight into the essential aspect of the probabilistic modeling perspective can be
gained by contrasting it with what it is not. The probabilistic approach does not construct
statistical models using algebraic relations. In particular, reasoning about how observed
and unobserved quantities relate to each other and how their dependencies are induced
does not involve specifying structural relationships using algebraic formulas. Instead, all
observed and unobserved events that are thought to represent the phenomenon of interest
are denoted by random variables and each event is allocated a finite probability of occurring.
That is to say, the generative probabilistic modeling approach is Bayesian in spirit (Pearl,
2014; Lauritzen, 1996; Jebara, 2012). In the probabilistic approach, we start with the joint
distribution of all observed and unobserved quantities that govern the distribution of the
data.
p (y,θ)
We construct a probabilistic model in two steps. In the first step, we factorize the joint
density into a product product of conditional and marginals distributions. Through con-
ditioning, we introduce dependencies that transform the joint density into a mixture of
densities. Implicit in this mixture is a data-generating process that gives rise to the obser-
vations. Conditioning is the soul of statistics (joe Blitzstein). It is the most important tool
in a statistician’s toolkit for it can make inference more tractable by allowing us to factorize
any joint distribution into the product of several conditional probability distributions.In the
second step, we specify particular distributions to the factorized densities.
The generative approach requires much more thought to apply in practice since there
are a large number of ways we can decompose the joint distribution into a product of fac-
tors. Since each possible decomposition implicitly encodes a particular number of modeling
assumptions, the researcher is forced to examine the validity of and the motivation for all
modeling assumptions. Furthermore, each component of the factorization needs to be as-
signed a probability distribution that respects the plausibility of what is known about the
putative mechanism that gives rise to the data. For instance, in order to treat the measure-
ments as random in the first place, we would need to first quantify the sources of uncertainty
in inherent in the randomness using a statistical model. Randomness can arise from either
experimental error, model inadequacy, parametric variability induced by random sampling
from a population, or even parameter uncertainty arising from our inability to exactly know
the true values of the parameters (Kennedy and O’Hagan, 2001).
However, before we start making assumptions about the structure of the model or the
distributional form of its component factors, we need to examine two important questions
first; namely, how the data was collected, and what should its grouping structure be. We
start with the first issue.
7
Farouni
3.1.1 Recording Measurements in Space and Time
In the probabilistic modeling framework, we begin the task of building a statistical model,
not with a particular model in mind, but rather with the data. Consider once again the
data matrix
Y ==

y1,1 . . . y1,P
...
. . .
...
yn,1 . . . yn,P
...
. . .
...
yN,1 . . . yN,P
 (9)
We approach the task of building a statistical model for the data by first examining the
individual measurement events that produce the recorded observations. Since the event of
taking a measurement occurs within a four-dimensional spatiotemporal frame of reference,
we denote a single measurement not as ynp, but rather as y(d1, d2, d3, t), a measurement
recorded at spatial location d = (d1, d2, d3) and time t, where d ∈ D ⊂ R3 and t ∈ T ⊂ R.
Note that we do not consider here point process models in which D is random such that d
denotes the location of random events.
In many applications, spatiotemporal information is assumed to have a miniminal effect
on the observed data and is therefore omitted. For example, if we wish to measure the
heights of a number of individuals at a certain point in time, it could be safe to assume
that the spatial arrangement of the subjects and the brief time interval that passes between
the measurements are inconsequential. If on the other hand, the height measurements for
each subject are taken multiple times over a period of months, then the time index would
need to be retained. To illustrate, if the height of individual i is recorded repeatedly over
three occasions, we would then have the measurements [y(t∗1), y(t
∗
2), y(t
∗
3)]. For uniform time
intervals, the notation can be simplified and the vector of repeated measures can be denoted
as [y1, y2, y3]. In other applications, the location of measurements is an essential aspect of
understanding the phenomenon and cannot be ignored.
At any rate, the form in which the data is presented implicitly imposes certain as-
sumptions on us. For example, if the data come in the form of a data matrix without an
accompanying spatiotemporal index for each measured response ynp, then we might incor-
rectly assume that the measurements were uniformly sampled both in time or space. When
the spatiotemporal indices are absent such is the case for the data matrix Y given in 9, it is
good practice to consider the following few questions before proceeding to model the data.
• Are the P measurements that make up a given observation yn collected at the same
time? If not, are the time lags between them inconsequential?
• Do the spatial indices d vary continuously over the three dimensional frame of refer-
ence D or is the space D discretized?
• Does the time index t vary continuously over its domain T or is the time domain T
discretized?
8
A Contemporary Overview of Probabilistic Latent Variable Models
• In case of discrete time, are we dealing with multivariate time series (i.e. the spatial
coordinates are the same for each time period) or cross-sectional data (i.e. the spatial
coordinates vary over the time periods)?
In what follows, we will be assuming, when applicable, that both the temporal domain
T and spatial domain D are discretized uniformly; more specifically, that D is divided
into a regular P dimensional lattice where dp represents the spatial coordinates of the pth
block and that T is divided into equal time intervals, where each measurement represent
an average over an time interval and a spatial block. We will also be assuming that the
spatial coordinates are fixed for all the time periods and that there is no lag between the P
measurements. Consequently, the time and space indices would no longer be needed since
the row and column indices of the data matrix are sufficient to preserve the spatiotem-
poral structure of the measurements, in the sense that the complex 3D pattern of spatial
dependencies can still be inferred from the one dimensional response vector.
3.1.2 The Grouping Structure of the Data
In statistical studies, the observed data is usually characterized by a grouping structure that
determines what the observational units are. An observation could be either a univariate
random variable yn ∈ R, a multivariate random vector yn = (yn1, . . . , ynP ) ∈ RP , or even
a function fn ∈ L2 ⊂ R∞, where fn in this case is a point in the Lebesgue space L2, an
infinite dimensional function space. Although the grouping structure is usually determined
by the experimental design or by how the data was collected, the statistical model we
specify for the data can either ignore, relax, or even impose a particular grouping structure
on the data. Our choice of what constitutes an observational unit determines whether we
approach the modeling endeavor from a univariate, a multivariate, or a functional data
analysis perspective.
To illustrate, let us begin with the joint probability distribution of all K (= N × P )
variables that populate matrix Y
p (y11, . . . , ynp, . . . , yNP ,θ)
If we map the two-subscript index set to a single -subscript index set, we can express the
joint as such
p (y1, . . . , yk, . . . , yK ,θ)
If we refrain from making any distributional symmetry assumptions regarding the K random
variables, we then have to treat all K measurements as a single high-dimensional sample
from a joint distribution pθ governed by parameter θ. That is, since we do not assume that
the joint distribution of the variables is invariant to permutation or any other transformation
of the indices, our data would effectively consist of one single K dimensional observation
sampled from a distribution pθ
(y1, . . . , yk, . . . , yK | θ) ∼ pθ
Such a model would be plausible if our single observation is a matrix-valued random vari-
able Y ∈ RN×P such that of an image or a two-dimensional spatial data observation. In
principle, the joint model can account for any complex pattern of dependencies among the
9
Farouni
K measurements irrespective of their row or column index. Nonetheless, the joint distri-
bution of a large number of random variables would require a large number of parameters
to enable it to capture the dependencies among all possible combination of the variables.
Furthermore, given the complex form of dependencies that could be present in the data,
statistical inference would be a hopeless task with only one single sample. Note that in
general θ can be a scalar, a multidimensional vector, or even an infinite dimensional vector
function. However, since for any scalar observation, no more than one parameter can be
learned, we are effectively limited by the number of parameters we can learn given the K
measurements. That is, for the parameter vector
θ = (θ1, . . . , θd, . . . , θD)
the dimensionality of θ cannot exceed the number of data points K (i.e. D 6 K). An
example of such a joint model is the matrix normal model given by
vec(Y) ∼ NNP (vec(M),Σ⊗ Ω)
where M is N × P ; Σ is N × N ; Ω is P × P ; and ⊗ denotes the Kronecker product.
Accordingly, our parameter vector of interest would consist of
θ = [vec(M), vech(Σ), vech(Ω)]
with a dimension equal to 2NP+N(N+1)+P (P+1)2 - which is always bigger than N × P . In
general, note that when P  N , the number of features is greater than the number of
observations, the measurement density α =
(
N
P
)
is finite. When α does not converge to
infinity as N →∞, we are faced with a high-dimensional inference problem for which much
of theory of classical statistics does not apply (Advani and Ganguli, 2016).
Now consider the opposite scenario. If we have no grouping information about the K
measurements then the most reasonable thing to do is to assume exchangeablility and thus
treat all the K measurements as if drawn from a common distribution pθ
yk
i.i.d.∼ pθ
Compared to the previous situation in which our data consisted of a single K-dimensional
vector observation, here we have a total of K one-dimensional scalar observations. With
so many samples, inference should be much more manageable. Furthermore, we now have
much more flexibility in choosing the dimension of θ. The dimensionality of the parameter
vector θ controls the flexibility of the model’s distribution. For instance, we can specify a
low information capacity model such as a univariate Gaussian model with unknown mean.
In this case, θ would be one dimensional. At the other end of the spectrum, we can specify
a very flexible kernel density estimator to learn the projected data’s density. Regardless, by
collapsing high-dimensional data into a one-dimension marginal distribution, we would still
be unable to learn the global patterns of dependencies that govern the data distribution in
its original high dimensional space.
In most statistical studies however, we would expect the data matrix to be characterized
by a grouping structure that preserves the distinction between a feature and an observation,
such that the rows of the matrix correspond to the observations and the columns correspond
10
A Contemporary Overview of Probabilistic Latent Variable Models
to the P features. The grouping structure implicit in a data matrix effectively restricts how
the indices can be shuffled and transformed. Nonetheless, the data matrix can still be
characterized by complicated grouping structures. For example, the P variables can be
partitioned into a set of views (i.e. data sets) after which the observations within a view
can be partitioned into a set of clusters or sub-populations (Mansinghka et al., 2016).
In what follows, we will treat the N rows of the matrix as observations and the P
columns as features such that the joint distribution is given by
p (y1,y2, . . . ,yN ,θ)
We can also partition the P -dimensional observation vector into G multiple groups, where
G ≤ P .
yn =
(
yᵀn,1, . . . ,y
ᵀ
n,g, . . . ,y
ᵀ
n,G
)
Or even partition the N observations into C clusters such that
y =
(
{yn}n∈N1 , . . . , {yn}n∈NC
)
To obtain more complicated grouping structures, we can cluster the observations after we
group the features such that observations for group g would belong to cluster Cg. The data
can then be grouped as such
y = {y(n,g)}n∈NCg
By partitioning the data matrix into groups and assuming partial exchangeability, we
can build a hierarchical model for all the groups. In a hierarchical model, the measurements
in each group are exchangeable within a corresponding sub-model and the parameters of
the sub-models are exchangeable in their prior distribution.
3.2 What is a Latent Variable?
Many definitions for a latent variable have been proposed in the literature (see Bollen,
2002, for a review). Although the definitions that have been put forward range from the
informal to the mathematical, many of them rely on distinctions that can be conceptually
problematic when working in a probabilistic modeling framework. For example, defining a
variable as latent based on whether it is treated as random rather than fixed does not carry
over to the probabilistic framework where all variables, known or unknown, are treated as
random. Furthermore, definitions that are based on the notion of unobservability introduce
some conceptual issues that complicate any attempt to give a simple definition based on
that concept alone. In a probabilistic model, a variable is either observable or unobservable,
but not all unobservable variables in a probabilistic model correspond to what we think of
as a latent variable when we are working outside the probabilistic framework. In particular,
definitions based on unobservability break down when considering how to treat the random
noise term n, whether it is found in a simple regression model or in an implicit model
that transforms the distribution of the error into a more complicated distribution g(n;θ)
(Goodfellow et al., 2014). In both cases, the error term is an unobservable random quantity
whose distribution deterministically induces a distribution on the response yn through an
11
Farouni
injective function g whose inverse is g−1. Nonetheless, it cannot be considered a latent
variable since the posterior
p(n|yn) = I[n = g−1(yn)] (10)
is a point mass, a deterministic not a random function (Tran et al., 2017). Note that for a
function to have an inverse, the dimension of n should equal the dimension of yn. It could
be argued that for variable to be considered a latent variable it needs be an unobserved
random variable whose posterior distribution is not degenerate. For example, in the Tobit
model (Tobin, 1958)
i ∼ N(0, σ2)
y∗i = βxi + i
yi = max(0, y
∗
i )
(11)
the error term i accordingly would not be considered a latent variable since the function
g(x) = m+sx is invertible given that both i and y
∗
i are scalars (i.e. both of dimension one).
In contrast, y∗i can be considered a latent variable since its posterior is not a point mass
and the function g(x) = max(0, x) is not invertible, even though both y∗i and the observed
variable yi are scalars. To complicate things further, one could argue that in the Tobit
model, we are in fact dealing with a partially observable variable model yi = max(0, βxi+i)
rather than with a latent variable model per se.
Requiring a latent variable to be an unobservable random variable with a non-degenerate
posterior distribution can provide us with an adequate working definition of a latent variable.
Still we argue that a latent variable can be better understood by the function it serves in a
model. In particular, we propose that the inclusion of a latent variable in a statistical model
allows us to accomplish two tasks: First, to capture statistical dependencies in the data and
second, to learn a latent structure that potentially governs the data generating mechanism
for the observations (i.e. learn the posterior distribution p(z|y)). With this in mind, note
that the notion of a latent variable as widely understood in the majority of modeling context
is most intuitively explained by the local independence definition of a latent variable (Lord,
1952). We qualify the preceding statement by the word majority because in the local
independence definition does not always hold for non-probabilistic models such as SEM
where the latent variables can regressed on each other to induce dependencies among the
latent variable. Furthermore, the local independence definition might not be as intuitive
in the case of models, whether estimable or not, in which there are a great number of
latent variables that combine together to give rise to an observed response of a much lower
dimension (Thomson, 1916).
To illustrate the basic idea, consider the following toy example. We observe N multi-
variate observations, N samples of three dependent variables {(yn1, yn2, yn3)}Nn=1 (Figure
2a). The inclusion of a scalar latent variable zn for the nth observation allows us to factor-
ize the joint distribution of (yn1, yn2, yn3) into a set of conditionally independent factors, as
shown by the directed graph in Figure 2b. For example, if we have a set of P dependent
variables where the dependency is restricted to positive linear association among them, then
conditioning on just one single latent variable can be sufficient to explain the pattern of
dependencies in the P dimensional data. The resulting latent variable model for the N
observations can be concisely expressed by the directed graph in Figure 2c.
12
A Contemporary Overview of Probabilistic Latent Variable Models
yn1 yn2 yn3
n = 1, · · · , N
(a) marginal model
zn
yn1 yn2 yn3
n = 1, · · · , N
(b) conditional model
zn
ynp
P
N
(c) Finite-data model
Figure 2: (a) Marginal graphical model for the three dependent variables. (b) Directed
graph of the observed variables conditional on the latent variable. (c) The latent variable
model as a directed graphical model for a sample of N multivariate observations, each a
P -dimensional vector.
3.3 What is a Probabilistic Latent Variable Model?
Let us define probabilistic latent variable model as a statistical model of observable variables
that incorporates unobservable variables. Accordingly, any parametric or nonparametric
hierarchical Bayesian models subsumed under the decomposition given by Equations 6 and 7
can be considered as special cases in the general class of probabilistic latent variable models.
To formulate a general modeling framework for the family of probabilistic latent variable
models, we incorporate a D-dimensional local latent variable vector zn = (z1, . . . , zD) for
each observation zn and a M -dimensional global latent variable vector θ = (θ1, . . . , θM )
shared across all observations (Blei et al., 2016). The local latent variable, zn, serves the
function of capturing local dependencies within the observation vector yn (e.g. clusters or
common factors) whereas the global latent variable θ captures the dependencies across all,
or even a subset, of the N observable random vectors. Accordingly, the joint probability
distribution for a probabilistic latent variable model can be expressed as such
p(y1, · · · ,yN , z1, · · · , zN ,θ) (12)
Now, there are countless many ways we could factor the joint distribution given by Equation
12. Any chosen factorization encodes a set of implicit assumptions that make the partic-
ular decomposition possible. For example, if the observations in y = {y1,y2, · · · ,yN} are
sequential in nature such that their distribution is no longer invariant under permutation of
indices (i.e. exchangeable), but is still invariant under shifts (i.e. stationary), then a very
general class of temporal generative models (Gemici et al., 2017) can be formulated by the
following decomposition.
p(y ≤N , z ≤N ,θ) = p(θ)
N∏
n=1
p(yn | fx(z ≤n,y <n),θ)p
(
zn | fz(z <n,y <n),θ
)
(13)
The family of generative temporal models give by Equation 13 include hidden Markov
models, non-linear state space models (Tornio et al.), and the Variational Recurrent Neural
13
Farouni
Network (VRNN) (Chung et al., 2015). If we are not modeling temporal or sequential
data, then we can assume that the N multivariate observations are indeed exchangeable
and obtain the following decomposition instead.
p(y1, · · · ,yN , z1, · · · , zN ,θ) = p(θ)
N∏
n=1
p(yn | zn,θ)p(zn | θ) (14)
This decomposition subsumes a very broad class of latent variable models such as hierar-
chical Bayesian regression models (Gelman et al., 2014), Bayesian mixture models, additive
matrix decomposition models (e.g. factor analysis, PCA, and CCA), and Bayesian nonpara-
metric models. A graphical representation of this general latent variable family of models
is shown in Figure 3.
θ
zn ynp
P
N
(a) Unidimensional Latent Variable Model
θ
zn yn
N
(b) Multi-dimensional Latent Variable Model
Figure 3: (a) For each sample observation n, zn is a local latent variable, and the ynp′ are P
observed variables. θ is a vector of global latent variables (parameters) (b) Directed graph
of the observed variables conditional on the latent variable.
Notice that the decomposition given by Equation 14 implies a second assumption; specif-
ically, that the N local latent variables zn are exchangeable in their joint distribution. The
joint distribution here refers to the prior p(zn | θ) which is governed by the global latent
variable θ. The role of global latent variable in the model is to control the extent of in-
formation pooling across the observations. To illustrate, if we set the variance of the prior
distribution to infinity, we obtain a separate model for each group as a result. That is, a
different model is fit to each observation vector, where yn can be thought of as an “exper-
iment” with P measurements. On the other extreme, setting the variance to zero would
result in complete pooling of information, effectively giving us a single model for all N × P
collapsed observations (Gelman et al., 2014). Incidentally, in the context of mixed effects
models, it is interesting to point out that a fixed effect can be thought of as a special case
of a random effect, but one with an infinite variance prior.
3.4 Implicit Vs Explicit Models
Most of the time, we propose models whose likelihoods and priors have an explicit closed-
form probability densities. In some occasions, however, if we are able to sample from
a simulator that is better at modeling the generative mechanism that gives rise to the
data, then we can refrain from specifying an explicit probability density for either the data
14
A Contemporary Overview of Probabilistic Latent Variable Models
distribution or the prior. Models that do not specify explicit densities are known in the
literature as implicit models, first introduced by Diggle and Gratton (1984) to approximate
an intractable likelihood and later extended to approximating the posterior distribution by
Tavaré et al. (1997) in a method that is now known as Approximate Bayesian Computation
(ABC) Pritchard et al. (1999). In an implicit model, instead of defining a likelihood, we
define a deterministic function f(.) whose input is a random noise source n and whose
output is the observation
yn = f(n | zn,θ); n ∼ s() (15)
The function f(.) can for example be implemented as a deep neural network. Furthermore,
the density for the local unobserved variable can also be defined implicitly as such
zn = f(n | θ); n ∼ s() (16)
We can sample from the posterior q(zn | yn;ν) for observation n
zn ∼ Nd
(
zn|NNµ(yn;ν),NNσ(yn;ν)
)
(17)
by using the implicit function g
zn = g(n,yn;ν) = NNµ(yn;ν) + NNσ(yn;ν) n (18)
where  denotes the elementwise product and
n ∼ Nd
(
0, I
)
(19)
If the neural networks NN are not invertible, as is usually the case, then the function g is
not invertible and the likelihood is therefore intractable (i.e. implicit).
3.5 Deep Vs Shallow Models
Also note that the decomposition given by Equation 14 above presumes that the elements
of the local latent variable vector zn form a single layer. Deep models in contrast contain
several hidden layers that form a rich hierarchical structure. To obtain a joint factorization
that includes deep models as a general case, we just need to divide the vector of local un-
observed variables zn into L subsets zn = {zn,1, · · · , zn,L}, such that each zn,1 corresponds
to a separate level in the sampling hierarchy. Unlike shallow models, deep models with a
hierarchy of latent variable dependencies are better able to capture rich latent structure
(Ranganath et al., 2015).
4. Brief History of Latent Variable Models
Although latent variable modeling is a branch of multivariate statistics, many of the field’s
major developments originated and were motivated by research problems, not within statis-
tics, but rather from the psychological and social sciences (Izenman, 2008). Whereas the
early breakthroughs mainly came from psychology, educational measurement, and eco-
nomics, the more recent advances are increasingly coming from the field of machine learning,
15
Farouni
especially from the very active research area of unsupervised learning in which researchers
are trying to accomplish the goal of endowing computers the ability to learn complex data
patterns in an unsupervised automatic manner (Lake et al., 2015; Rezende et al., 2016).
The quest to understand learning and intelligence, whether in humans or machines, has
always been a motivating force in the field from its early beginnings. Indeed, the psychologist
Charles Spearman, influenced by Francis Galton’s development of the correlation coefficient,
introduced factor analysis in 1904 as a statistical method to study the organization of mental
abilities in humans (Spearman, 1904). He proposed that from the correlations of multiple
observed variables (e.g. tests of general ability), we can infer a single latent factor that can
explain the common variance shared among all the observations. He referred to the latent
variable common to all the measured ability tests as g, the general factor of intelligence.
4.1 Summary of Key Developments
The factor analytic approach paved the way for, what seemed to many in the social and
psychological sciences, an objective and quantitative framework for measuring unobservable
hypothetical constructs such as “self-esteem”, “verbal ability”, and “quality of life”. Indeed
Spearman’s single factor model, in which both the one latent variable and the observed
variables are assumed continuous, has laid the foundation for a long research program in
psychometrics that has spanned well over a century. To account for the inadequacy of a sin-
gle factor to account for the covariance pattern in testing data, the psychologist Thurstone
extended the single factor model to multiple factors (Thurstone and L.L., 1947). The factor
model was constantly refined and generalized, culminating in the structural equation model
introduced by (Jöreskog, 1970) in which linear structural relationships between the latent
variables are also modeled. In educational testing, item response theory models (Birnbaum,
1968) were developed to accommodate categorical observed variables encountered in test-
ing in which test items assume two values, correct and incorrect. In sociology, clustering
analysis motivated the development of latent class models (i.e. mixture models) in which
the latent variables are assumed to be categorical (Lazarsfeld, 1950). Since then, a large
body of work has been built on top of these foundational early advances and many authors
(Muthén, 2002; Bartholomew et al., 2011; Hwang and Takane, 2014) have attempted to
bring together the various models under one roof in a general framework. The most encom-
passing as of yet is the Generalized Linear Latent and Mixed model (GLLMM) introduced
by Rabe-Hesketh et al. (2004) as a unified general framework that brings together not just
factor analysis, IRT, structural equations, and latent class models commonly encountered
in psychometrics and social quantitative methods, but also mainstream statistical random
effects models such as multilevel regression, and longitudinal models.
4.2 The Statistical Myth of Measuring Hypothetical Constructs
The enthusiasm regarding the factor analytic approach to uncover underlying theoretical
constructs was not universally shared. In particular, the interpretation of a latent variable
as a hypothetical construct was much criticized. One of the earliest criticism came from
Spearman’s colleague, Karl Pearson, widely regarded as the founder of modern mathemati-
cal statistics. The two men constantly feuded with each other throughout their careers due
to their opposing scientific philosophies (Hägglund, 2001). Whereas Spearman strove to dis-
16
A Contemporary Overview of Probabilistic Latent Variable Models
cover the fundamental laws of psychology and invent a method by which he can objectively
measure latent traits such as intelligence, Pearson did not see a place for unobserved and
subsequently unmeasurable phenomena in the scientific enterprise. Incidentally, Pearson
(1901) had pioneered a method closely related to, and often conflated with Factor Analysis
(FA) - principle component analysis (PCA). Pearson’s PCA, which was later formulated as
multivariate technique by Hotelling (1933), was introduced not as an explanatory model,
but as a dimensionality reduction technique in which the data is geometrically transformed
(i.e. projected) into a lower dimensional subspace.
The criticisms nonetheless did not end with Pearson. The English educational psy-
chologist Thomson (1916) proposed a sampling model that generated the same correlation
structure as a general factor model but that did not assume a single factor. In particular,
whereas Spearman’s model, and later extensions, proposed that positive correlations among
P variables suggest the existence of one or more factors fewer than P, Thomson’s sampling
model assumed the opposite: that there are more factors than we have variables and that
each variable is the result of a great number of independent causes, much greater than P.
Even though it seems more plausible to assume that there are a great number of factors
(mental subprocesses) that contribute to a score on any one ability test or item question-
naire, psychometricians following Spearman’s footsteps ignored these criticisms and pursued
a research program based on the indefensible premise that factor analysis and its extensions
are statistical methods that can explain correlations in the observed data, rather than just
describe them. It can be argued that the explanatory approach in which the existence of
a hypothetical construct is inferred by performing exploratory analysis on limited sample
self-report data cannot substitute the rigorous process of developing operational definitions
of unobservable constructs on theoretical grounds or even on experimental measurements of
the cognitive processes underlying the observed responses. Indeed, throughout the history
of psychometrics, there have been repeated calls to guard against the reification fallacy
in which a latent variable (e.g. mental ability) that captures a common variability in the
data is treated as if it were a physical quantity (e.g. height) that is amenable to direct
measurement (Gould, 1996; Sobel, 1994).
Moreover, given that the definition of a statistical model provided in section 2 does
not say much about the nature of the relationship between a statistical model and its
target phenomenon observed in the physical universe, we would like to point out here that
the modeling perspective we adopt is founded on the premise that a statistical model is
a phenomenological model of reality describing relationships among observed variables in
a mathematical formalism that is not derived from first principles (Hilborn and Mangel,
1997). We consider a model phenomenological in the Kantian sense (Kant and Guyer,
1998) in which the word phenomenon denotes an empirical object that is known through
the senses and whose mental representation is constructed by experience. The phenomenon
is contrasted with the noumenon, the “thing-in-itself” that is not directly knowable by
the senses but which can be known by the mind in the form of a theoretical construct.
For example, whereas the electromagnetic force is a theoretical construct arrived at from
first principles and provides a causal explanation to empirical observations (e.g. effects of
magnetic and electric fields), the empirical observations themselves constitute experienced
phenomenon whose correlative nature can provide nothing more than evidence for noumenal
knowledge.
17
Farouni
4.3 The First Latent Variable Model
If we restrict latent variable models to those models in which latent variables denote hypo-
thetical constructs, arguably Spearman’s factor model could be considered the first formu-
lation of a latent variable model. However, such a restriction based on a narrowly defined
interpretation is not universally accepted neither within psychometrics, the field Spearmen
helped start, nor within the larger statistics community (Skrondal and Rabe-Hesketh, 2004).
If we just consider the statistical form of the model, then it could be argued that the first
latent variable model should be attributed to the astronomer Airy, who in 1861 proposed
a variance components model (i.e. random effects model) for the repeated measurement of
an astronomical object over several nights (Airy, 1861). Both Airy’s variance components
model and Spearman’s factor model have the same graphical representation depicted in
Figure 2c. The main difference lies in that the latent variable in Airy’s model is a ran-
dom effect that captures unobserved heterogeneity whereas in Spearman’s model the latent
variable denotes a hypothetical construct which we seek to measure.
In his model, Airy makes P repeated measurements of an astronomical phenomenon for
each of N nights. See Searle et al. (2009) for a more details and the original formulation.
Assuming a balanced design, the model can be expressed as a probabilistic generative model
by the following two-stage sampling hierarchy.
yn | zn ∼ NP (1µ+ 1zn, σ2 I)
zn ∼ N (0, σ2z) for n = 1, · · · , N
(20)
where yn ∈ RP is the vector of repeated measurements and zn ∈ R is a latent variable (i.e.
random effect).
4.4 Latent Variable Models in Machine Learning
Although the GLLMM framework does indeed treat random coefficient models (which in-
clude hierarchical and longitudinal models) as latent variable models, the framework leaves
out important multivariate data reduction methods such PCA, Independent Component
Analysis (ICA), Canonical Correlation Analysis (CCA) from the formulation. The argu-
ment put forward by Skrondal and Rabe-Hesketh (2004) contends that PCA and similar
procedures are not latent variable models because they cannot be formulated as statis-
tical models with latent variables corresponding to lower dimensional space. However,
their assertion does not seem to have taken into consideration results that had been pre-
viously published in the literature. More specifically, in recent years, research in latent
variable modeling has undergone a very active resurgence in the field of unsupervised ma-
chine learning. Indeed, great progress has been made in developing new latent variable
models and recasting existing clustering and dimensionality reduction methods in a proba-
bilistic graphical model formulation. For example, building on previous research in neural
networks (MacKay, 1995a; Bishop et al., 1998), Tipping and Bishop (1999) demonstrated
that PCA can be indeed be formulated as a probabilistic latent variable model. In their
paper, they showed that Probabilistic PCA is a special case of Factor Analysis (FA). More
specifically, whereas in FA the error covariance matrix is diagonal, in Probabilistic PCA, it
is σI and standard PCA is obtained when σ → 0. Roweis and Ghahramani (1999) review
18
A Contemporary Overview of Probabilistic Latent Variable Models
several common methods showing that not just PCA and FA, but also ICA, Kalman Filters,
and Hidden Markov Models (HMMs) can be considered as special cases of a general linear-
Gaussian latent variable model. A probabilistic formulation of CCA (Bach and Jordan,
2005) later followed. Recently, a Group Factor Analysis (GFA) model has been proposed
by Klami et al. (2015) that generalizes Probabilistic CCA to more than two data sets.
It is these developments that motivate the material in the next two sections. In the
social sciences, latent variable models have been traditionally been viewed and used as
tools to study hypothetical constructs. However, ongoing research in machine learning,
deep learning, and probabilistic graphical modeling has given a new life and meaning to
latent variable modeling. These recent developments present latent variables as capable
of endowing statistical models not only with the ability to perform essentials data analysis
tasks such as dimensionality reduction, statistical clustering, and accounting for unobserved
heterogeneity, but also with explanatory and interpretative powers. The inclusion of latent
variables allows us to capture statistical dependencies among observed variables and thus
learn the hidden structure underlying the data generating mechanism. This is especially true
when latent variable models are cast in a generative probabilistic framework, a perspective
that the next section attempts to describe in detail. The following sections forms the
foundation for subsequent sections where many of the most commonly encountered latent
variable models are constructed from the most basic elements.
5. A Brief Overview of Probabilistic Latent Variable Models
In this section, we will attempt to show how starting from a joint probability density, we can
formulate a multivariate regression model and describe how regression models are related
to latent variable models through reduced rand regression. We will give an overview of the
most commonly encountered latent variable models in practice.
To start, let us consider a two-group partition of the observation vectors that populate
the data matrix Y. To simply notation, we can effectively create a two-group partition
by concatenating the data matrix with a matrix of co-variates such that each row would
consist of a P -dimensional observation vector yn = (yn1, · · · , ynP ) and an M -dimensional
covariate vector xn = (xn1, · · · , xnM ). The concatenated data would be an N by (P +M)
matrix consisting of a total of N × (P +M) measurements
(yT1 ,x
T
1 )
...
(yTn ,x
T
n )
...
(yTN ,x
T
N )
 =

y11 . . . y1P x11 . . . x1M
...
. . .
...
...
. . .
...
yi1 . . . ynP xn1 . . . xnM
...
. . .
...
...
. . .
...
yN1 . . . yNP xN1 . . . xNM
 (21)
with a joint distribution given by
p
(
(y1,x1) , · · · , (yN ,xN ) ,θ
)
5.1 Spatiotemporal Models
We can treat the data as spatiotemporal in nature by letting the row index denote time and
mapping the column index to a given spatial coordinate. Using a spatio-temporal mode,
19
Farouni
we can learn both the temporal and spatial dependencies of the data. In a spatiotemporal
model we do not treat The observations that populate the data matrix as exchangeable
neither in the temporal nor the spatial dimension. In the spatial dimension the patterns
of dependencies can be vary in complexity depending on the type of data. In the temporal
dimension in contrast, the observations be characterized by a less complex pattern of se-
quential dependencies. For example, the observation yn can be dependent on the preceding
observation yn−1.
If we wanted a very general model, we can formulate a non-separable spatio-temporal
model in which the spatial and temporal components are allowed to interact. If we assume
some form of stationarity in the temporal process such that the joint probability distribution
of a finite sequence of consecutive observations does not change when they shifted in time,
then a family of non-separable spatio-temporal models can be specified by the following
decomposition.
p(y ≤N ;θ) =
N∏
n=1
p(yn | f(y <n);θ) (22)
where we let the subscript n index time such that the response yn can be dependent on
preceding responses y <n through a nonlinear function f .
Such a model is commonly encountered in and is appropriate for modeling dynamical
systems in which the current spatial component of the process evolves as a function of past
spatial locations. Although non-separable models tend to be realistic for many real world
applications, their estimation can be computationally intractable, especially in the case
when the dimensionality of N or P is high.
Alternatively, when adequate motivation exists, a separable spatio-temporal model can
be used instead by assuming that the parameters that govern the distribution of the tem-
poral component are independent of those that govern the spatial component. To obtain
a separable model, we let θ = (θS ,θT ) such that θS governs the distribution of the P
elements of yn whereas θ
T governs the distribution of the N observations.
p(y ≤N ; θ
S ,θT ) =
N∏
n=1
p(yn | fθT (y <n), θ
S) (23)
5.2 Multivariate Models
A spatiotemporal model can be viewed either as a temporally varying spatial model or
as spatially varying time series model (Banerjee et al., 2014). In either case, the data
modeled is assumed to be characterized by both spatial and temporal dependencies. In
contrast, in multivariate models, we relax the stationarity assumption that allows us to
model the sequential dependencies in the temporal dimension. Instead, we assume that
the observations are exchangeable in their joint distribution. Exchangability give us the
following factorization.
p
(
(y1,x1) , · · · , (yN ,xN ) ,θ
)
=
N∏
n=1
p
(
(yn,xn) | θ
)
p(θ)
20
A Contemporary Overview of Probabilistic Latent Variable Models
Note that the parameter vector θ governs the distribution of the pairs of yn and xn
jointly. However if we wish to predict yn from xn, we need to assume that the parameters
that govern the distribution of yn are independent of those that govern the distribution of
xn. That is, given the partition θ = (θ
Y ,θX), we have
p(θY ,θX) = p(θY )p(θX)
and we can factorize the joint distribution as such.
p
(
(y1,x1) , · · · , (yN ,xN ) ,θ
)
=
N∏
n=1
p(yn, | fθ(xn),θY )
N∏
n=1
p(xn | θX)p(θY )p(θX) (24)
The factorization implies that the distribution of xn does not influence the conditional
distribution of yn. The assumption allows us to ignore the distribution of xn when the
focus is on determining the parameter θY - such as the case in many regression or path
analysis models for example, where xn is considered an exogenous variable.
5.2.1 Normal Linear Models
Let us split the vector of parameters into two components such that
θY = [vec(B), vech(Σ)]
where
B =

βT1
βT2
...
βTP
 =

β11 · · · β1m
β21 · · · β2m
...
. . .
...
βp1 · · · βpm

and Σ is positive definite matrix, and let us further assume that
µ(xn; B) = Bxn and σ(Σ) = Σ
we arrive at following data distribution for a linear regression model
p
(
yn | xn, θY
)
≡ Np (yn | Bxn, Σ)
Note that since both yn and xn are multivariate vectors, µ(xn; B) is vector-valued function
that transforms xn into yn. Also note that we dropped the index n from both functions.
The joint probability distribution can now be factored into
N∏
n=1
Np(yn | Bxn, Σ)p (B, vech(Σ))
N∏
n=1
p(xn | θX)p(θX)
Let us simplify things further and let Σ = Diag(σ21, · · · , σ2p) so that the elements of the
response vector now become uncorrelated. Also, let us group B as set of P M -dimensional
observations (β1,β2, . . . ,βP ), and let us consider the predictor xn to be fixed, so that
21
Farouni
we just focus on the data distribution for now. These assumptions give us the following
decomposition
N∏
n=1
Np(yn | Bxn, Σ)p
(
β1,β2, . . . ,βP , σ
2
1, · · · , σ2P
)
By grouping the pairs
(
βp, σ
2
p
)
and then assuming conditional exchangeability of the
parameter pairs, we can further decompose the model into
N∏
n=1
Np(yn | Bxn, Σ) =
N∏
n=1
P∏
p=1
N (ynp | βTp xn, σ2p)p
(
βp, σ
2
p | ξ
)
p(ξ)
If we assume that the priors over the parameters
(
βp, σ
2
p
)
to be uninformative, we further
reduce the decomposition to
N∏
n=1
Np(yn | Bxn, Σ) =
N∏
n=1
P∏
p=1
N (ynp | βTp xn, σ2p)
The above simplification implies that a multivariate regression model can be broken down
into a set of P multiple regression problems. For example, if we only consider one response
variable y ∈ y, we can then drop the subscript and obtain the following decomposition for
a multiple linear regression model.
N∏
n=1
p(yn | xn) =
N∏
n=1
N (yn | βTxn, σ2)
Note that the decomposition above is not exactly that of a hierarchical Bayesian re-
gression model. In a hierarchical regression, the N multivariate observation vector pairs
(yn,xn) are partitioned into J clusters and for each cluster j we introduce a regression
coefficient βjp. Accordingly, the decomposition can be expressed as
Nj∏
n=1
J∏
j=1
Np(yn | Bxn, Σ) =
Nj∏
n=1
J∏
j=1
P∏
p=1
N (ynjp | βTjpxnj , σ2p)p
(
βjp, σ
2
p | ξ
)
p(ξ)
5.3 Reduced Rank Regression (RRR)
Our starting point is the decomposition given in Equation 25 for the multivariate regression
model - which is reproduced here.
N∏
n=1
Np(yn | Bxn, Σ)p(B,Σ)
N∏
n=1
p(xn | θX)p(θX)
and the parameters that govern xn and yn to be independent, and
N∏
n=1
Np(yn | Bxn, Σ)
22
A Contemporary Overview of Probabilistic Latent Variable Models
If we allow the rank of the regression coefficient matrix to be deficient such that
rank(B) = d < min(m, p), then the matrix can be decomposed into two non-unique
matrices as such
p×m
B =
p×d
W
d×m
D . The decomposition results in what is known as Reduced
Rank Regression (RRR) (Izenman, 1975)
N∏
n=1
Np(yn |WDxn, Σ) (25)
Now if set the predictor vector to be equal to the response vector then the resulting
data distribution of the reduced rank regression can be expressed as
N∏
n=1
Np(yn |WDyn, Σ) (26)
By letting zn =
d×m
D
m×1
yn , we can now think of yn as a message; zn as an encoded
transformation of the message; and W as the decoding transformation. The model can
then be expressed as
N∏
n=1
Np(yn |Wzn, Σ) (27)
This representation allows us to see that several latent variable models are special cases
of RRR (Izenman, 2008).
5.4 Principle Component Analysis (PCA)
To obtain our first and simplest latent variable model, we specify a distribution to the latent
vector since the lower dimensional encoded message is unobservable. Assigning a standard
Gaussian to the latent variables and restricting the covariance of the errors to be isotropic
gives us probabilistic PCA (Tipping and Bishop, 1999). The model can be expressed as
N∏
n=1
Np(yn |Wzn, σ2I)Nd(zn | 0, I) (28)
or as a generative model
yn | zn ∼ Np(Wzn, σ2I)
zn ∼ Nd(0, I) for n = 1, · · · , N
(29)
Standard PCA is obtained as the maximum likelihood solution if we let σ2 → 0 and
constrain W to be orthogonal.
23
Farouni
5.5 Factor Analysis (FA)
If we relax the constraint of homogeneous variance (i.e. σ2 is the same for all components
of the vector yn), the covariance matrix can be specified as
Σ = Diag(σ2) =

σ21 0 . . . 0
0 σ22 . . . 0
...
...
. . .
...
0 . . . 0 σ2p
 (30)
The result is a multidimensional factor analysis model with a diagonal covariance and d
latent factors.
N∏
n=1
Np(yn |Wzn, Diag(σ2))Nd(zn | 0, I) (31)
or as a generative model
yn | zn ∼ Np(Wzn, Diag(σ2))
zn ∼ Nd(0, I) for n = 1, · · · , N
(32)
Note that the standard normal prior over the latent variables resolves invariance in the scale
and location of the latent space, but not in rotation.
5.6 Independent Component Analysis (ICA)
Independent Component Analysis (Hyvärinen, 2015) is a widely used model for signal source
separation. In order to model signals as latent variables, their distribution cannot be normal
because the Gaussian is the most random of all distributions. In fact, according to Jaynes
principle of maximum entropy (Jaynes, 2003), a Gaussian random variable has maximum
entropy (i.e.uncertainty) among all random variables with equal variance. In contrast,
entropy tends to be small for spiked, or super-Gaussian distributions - distributions that
characterize signals! So if we consider the generalized Gaussian distribution as a prior over
the latent variables, we obtain a highly peaked supergaussian distribution (if the shape
parameter α set to be less than 2) that is appropriate for modeling signals that give rise
to sparse data. Note that the version of the generalized Gaussian distribution we are
considering is that of parametric symmetric distributions. The normal distribution is a
special case obtained when α = 2 and the Laplace distribution is another special case when
α = 1 (Gómez et al., 1998). ICA can be thought of as a linear latent generative model, a
type of a Factor Analysis model but with nongaussian priors over the latent variables (i.e.
sources). The model has the following decomposition.
N∏
n=1
Np(yn |Wzn, Diag(σ2))
N∏
n=1
D∏
d=1
GGd(zd,n | αd) αd < 2 (33)
or as a generative model
yn | zn ∼ Np(Wzn, Diag(σ2))
zd,n ∼ GGd(αd) αd < 2 d = 1 : D; n = 1, · · · , N
(34)
where GG is the generalized Gaussian distribution.
24
A Contemporary Overview of Probabilistic Latent Variable Models
5.7 Canonical Correlation Analysis (CCA)
Hotelling (1936) introduced Canonical Correlation Analysis (CCA) as a dimensionality re-
duction method that projects two sets of random variables (i.e. two data sets) into a shared
subspace such that the projections are maximally correlated. Although almost always com-
monly thought as data reduction technique, CCA can indeed be given a latent variable
interpretation as Bach and Jordan (2005) have shown. More specifically, they showed that
the equivalent probabilistic latent variable model can be formulated as
zn ∼ Nd (0, I) n = 1, · · · , N[
yn
(1) | zn
yn
(2) | zn
]
∼ N(p1+p2)
([
W(1)
W(2)
]
zn,
[
Σ(1) 0
0 Σ(2)
])
(35)
where yn
(1) ∈ Rp1 and yn(2) ∈ Rp2 , each denote a random vector of observations for Data set
1 and Data set 2, respectively. Note that by replacing the block diagonal covariance matrix
of the data with a diagonal matrix, the probabilistic CCA model reduces to a standard
factor analysis model!
They also show that since any covariance matrix of rank D can be written as the
following decomposition of the marginal covariance in the marginal joint distribution of the
data [
yn
(1)
yn
(2)
]
∼ N
(
0,
[
W(1)W(1)
ᵀ
+ Σ(1) W(1)W(2)
ᵀ
W(2)W(1)
ᵀ
W(2)W(2)
ᵀ
+ Σ(2)
])
(36)
it follows that the MLE solution for the joint covariance matrix is such that the cross-
covariance matrix is of rank D and consists of the canonical correlation directions that
would be obtained in regular CCA.
5.8 Inter-Battery Factor Analysis (IBFA)
If we split the vector of latent variables zn into three subsets {zn,0, zn,1, zn,1} such that the
latent vector zn,0 captures the common structure shared across both groups of variables,
yn
(1) and yn
(2), while zn,1 and zn,1 each captures the latent structure specific to each group,
then CCA model can be modified to have an equivalent specification to the Inter-Battery
Factor Analysis (IBFA) model (Tucker, 1958; Browne, 1979).
zn,0zn,1
zn,2
 ∼ Nd (0, I) n = 1, · · · , N
[
x(1) | zn,0, zn,1
x(2) | zn,0, zn,2
]
∼ N
[W(1,0) W(1,1) 0
W(2,0) 0 W(2,2)
]zn,0zn,1
zn,2
 , [Σ(1) 0
0 Σ(2)
] (37)
Note that the above generative formulation makes it apparent that IBFA can be viewed
as a special case of CCA, one obtained when we restrict the projection matrix W to have
a the structure given above; namely
W =
[
W(1)
W(2)
]
=
[
W(1,0) W(1,1) 0
W(2,0) 0 W(2,2)
]
(38)
25
Farouni
5.9 Multi-Battery Factor Analysis (MBFA)
The Inter-Battery Factor Analysis model can be readily extended to multiple groups of
variables. So if we let yn
(g) ∈ Rpg for g = 1, · · · , G denote the groups of variables (i.e.
data sets), the vector of observations can be stacked as P =
∑G
g pg dimensional vector and
the Multi-Battery Factor Analysis (MBFA) model (Browne, 1980; Cudeck, 1982) can be
expressed as follows.
zn ∼ Nd (0, I) n = 1, · · · , N
yn
(1) | zn
yn
(2) | zn
...
yn
(G) | zn
 ∼ NP


W(1)
W(2)
...
W(G)
 zn,

Σ(1) 0 . . . 0
0 Σ(2) . . . 0
...
...
. . .
...
0 . . . 0 Σ(G)

 (39)
Again note that by replacing the block diagonal covariance matrix of the data with a
diagonal matrix, MBFA reduces to a standard factor analysis model.
5.10 Group Factor Analysis (GFA)
Klami et al. (2015) proposed Group Factor Analysis (GFA) as a generalization of multi-
battery factor analysis (MBFA). The generative model does still have the same general
structure of the preceding latent variable models
zn ∼ Nd (0, I) n = 1, · · · , N
yn
(1) | zn
yn
(2) | zn
...
yn
(G) | zn
 ∼ NP


W(1)
W(2)
...
W(G)
 zn,

σ21I 0 . . . 0
0 σ22I . . . 0
...
...
. . .
...
0 . . . 0 σ2GI

 (40)
where
W =

W(1)
W(2)
...
W(G)
 =

w
(1)
1 w
(1)
2 . . . w
(1)
D
w
(2)
1 w
(2)
2 . . . w
(2)
D
...
...
...
...
w
(G)
1 w
(G)
2 . . . w
(G)
D
 (41)
There are two differences however. First, GFA restricts the group specific error covari-
ance to isotropic structure. That is, Σ(g) = σ2gI for g = 1, · · · , G. Second, in order to model
the dependency structure between any subsets of groups and resolve indeterminacy issues,
GFA imposes a structural sparsity prior over the columns of the projection matrix W. The
Automatic Relevance Determination (ARD) prior (MacKay, 1995b) over the G×D group
26
A Contemporary Overview of Probabilistic Latent Variable Models
specific columns of the projection matrix W can be specified as follows.
w
(g)
d ∼ N
(
0, (α
(g)
d )
−1
I
)
g = 1, · · · , G; d = 1, · · · , D
log(α) = UV ᵀ + µu1
ᵀ + 1µᵀv
p(U) =
G∏
g
R∏
r
N (ug,r|0, (λ = 0.1)−1)
p(V ) =
D∏
d
R∏
r
N (vd,r|0, (λ = 0.1)−1)
(42)
for α ∈ RG×D, U ∈ RG×R, and V ∈ RD×R and R min(G,D).
5.11 Structural Equation Models (SEM)
The preceding models can be extended by allowing the latent variables to regress on each
other. The resulting general model is known as a Structural Equation Model (SEM) and was
first introduced by Jöreskog (1970). SEM models are constructed using algebraic equations,
not by formulating a probabilistic statistical model as is done in Bayesian Graphical Models
(Pearl, 2014) for example.
A popular general formulation of SEMs is given by the LISREL model. The LISREL
model consists of two parts. The first part is a structural component in which dependencies
among the latent variables are created by regressing them on each other. The vector of
latent variables we introduce into the model is divided into two groups of independent
and dependent variable zn = (z
ᵀ
n,1, z
ᵀ
n,2) of dimensions D1 and D2, respectively, such that
D1 + D2 = D. The second part consists of a measurement model that is similar in form
to the factor analysis model (i.e. yn = Wzn + n) but for which we also partition the
P -dimensional observation vector (and correspondingly the error vector) into two groups
yn = (y
ᵀ
n,1,y
ᵀ
n,2) and n = (
ᵀ
n,1, 
ᵀ
n,2) such that yn,1 depends only on zn,2 and yn,2 depends
only on zn,1. The dimension of two groups is P1 and P2, such that P1 + P2 = P . The
general algebraic form of the LISREL model is as follows.[
yn,1
yn,2
]
=
[
W1 0
0 W2
] [
zn,1
zn,2
]
+
[
n,1
n,2
]
Bzn,2 = Czn,1 + ξn
(43)
where it is assumed that B is non-singular and
E(yn) = 0;E(zn) = 0 E(n) = 0; E(ξn) = 0
Cov(n) = Ψ; Cov(zn,1) = Φz1 ; Cov(ξn) = Φξ
(44)
and the covariance matrices are diagonal. The regressions imply the following covariance
matrix Σyy for the observations.
[
W1Φz1W
ᵀ
1 + Ψ1 W1Φz1C
ᵀ(Bᵀ)−1Wᵀ2
W2Φz1C
ᵀ(Bᵀ)−1Wᵀ1 W2(B
−1CᵀΦz1C(B
ᵀ)−1 + B−1Φξ(B
ᵀ)−1)Wᵀ2 + Ψ2
]
(45)
27
Farouni
Notice that how the implied covariance structure of the data is determined by the
coefficient matrices ( i.e. B, C, and W) and the covariance matrices (i.e. Ψ, Φz1 and Φξ).
More importantly, notice also that an SEM model implicitly assumes that the relationships
between the observations are linear since covariances are measures of linear association.
Another aspect of SEM models that is worth pointing out is that although we can assess
whether a particular implied decomposition is consistent with the data, we cannot determine
whether it is the only decomposition that is equally consistent with the data. That is, the
same data can be generated given different latent structural relations (Bartholomew et al.,
2011).
Generalized Structured Component Analysis (GSCA) A generalization of the
SEM framework has recently been proposed by Hwang and Takane (2004). The goal of
the generalization is to unify two distinct approaches to SEM; namely the factor-based
and component-based (i.e. partial least squares). The Generalized Structured Component
Analysis (GSCA) model they propose has the following structural formulation.[
I
W
]
yn =
[
C
B
]
Wyn +
[
n
ξn
]
(46)
where the latent variables are obtained as component scores using what the authors refer to
as weighted relation model zn = Wyn. The assumption regarding the means and covariance
of the variables are identical to the LISERL model. It is interesting to note that the weighted
relation model equation is no different conceptually than the encoding transformation that
Izenman (2008) used to construct latent variable models from the reduced rank regression
framework ( Section 5.3).
We argue that by relying on complicated algebraic equations to induce linear dependen-
cies in the latent structure, the SEM framework risks losing statistical rigor especially when
modeling complex phenomena. Real data, most often than not, is noisy and characterized
by complex nonlinear dependencies. We think that we have a better and more statistically
rigorous alternative in the probabilistic generative perspective that approaches the mod-
eling endeavor by first proposing a probabilistic model for all the observed variables and
any latent variables that we wish to incorporate in the model. As part of the probabilistic
model, we can also specify functional relations between the variables in order to capture
any nonlinear dependencies in the data or to construct a rich latent space with complex
structure. For example, a recent method, the Inverse Autoregressive Flow (IAF)(Kingma
et al., 2016), has been proposed in the generative modeling literature that enables us to cre-
ate models with rich latent structure. The key idea behind the IAF is that any multivariate
normal variable with diagonal covariance can be transformed into a multivariate normal
with linear dependencies (i.e. full covariance matrix) by expressing it as an autoregressive
model
zd = µd(z<d) + σd(z<d) ∗  (47)
where µd and σd are the conditional mean vector and covariance matrix of zd on the other
elements of the latent vector zn = (z1, z2, · · · , zd, · · · , zD). Accordingly, a general SEM
28
A Contemporary Overview of Probabilistic Latent Variable Models
model can be conceptually viewed as the following generative model.
yn | zn ∼ Np
(
Wzn,Diag(σ
2)
)
zn ∼ Nr(0,Σz)
(48)
By including structural latent regressions, a specific SEM model would then correspond
to a particular structure of the covariance matrix Σz. In the next section, we briefly describe
Deep Latent Gaussian Models (DLGMs) as an alternative general framework to SEM, one
that is more statistically disciplined and potentially much more powerful, especially when
large amounts of data is available.
5.12 Deep Latent Gaussian Models (DLGM)
A Deep Latent Gaussian Model (DLGM) (Rezende et al., 2014) is a directed graphical
model with Gaussian latent variables that are stacked in L stochastic layers and that are
transformed at each layer by a differentiable nonlinear function parameterized by a neural
network NN(z;θ) consisting of K(l) deterministic hidden layers. Whereas the latent vari-
ables in the first DLGMs such as the VAE (Kingma and Welling, 2013) are assumed to be
independent, recent extensions (Kingma et al., 2016; Maaløe et al., 2016) have introduced
a deep latent hierarchy that endows the latent space with complex rich structure that can
capture complex patterns of dependencies between the latent variables. A general DLGM
model has the following generative formulation.
z(L)n ∼ Nd(L) (0, I) n = 1, · · · , N
z(l)n ∼ Nd(l)
(
z(l)n | NN(l)(z(l+1)n ;θ(l)), Σ(l)
)
l = 1, · · · , L− 1
yn | zn ∼ Expon
(
yn | NN(0)(z(1)n ;θ(0))
) (49)
where
NN(z; θ) = hK ◦ hK−1 ◦ . . . ◦ h0(z)
hk(y) = σk(W
(k)y + b(k))
θ = {(W(k),b(k))}Kk=0
(50)
Such that the observation model Expon refers to the exponential family of models and
the neural network NN(z;θ) consists of the composition of K non-linear transformation
functions, each of which is represented by a hidden layer hk(y). The function σk is an
element-wise activation function such as the sigmoid function or the rectified linear unit
(ReLU). The constant d(l) is the number of latent variables at the l layer so that the total
number of latent variables equals d =
∑L
l=1 d(l)
A Deep Latent Gaussian Model is considered a deep model in two distinct senses. First,
the latent variables that make up the stochastic layers can be stacked in a deep hierarchy.
Second, the neural networks that make up the deterministic nonlinear transformations can
be made up of several hidden layers. In contrast, a nonlinear regression model is a shallow
29
Farouni
nonlinear model in the sense that it uses only one non-linear transformation while a factor
analysis model is a shallow linear model in the sense that the latent variables zn are linearly
projected by a matrix W to a higher dimensional vector.
Appendix A.
References
Madhu Advani and Surya Ganguli. Statistical Mechanics of Optimal Convex Inference in
High Dimensions. Physical Review X, 6(3):031034, 2016.
George Biddell Airy. On the algebraical and numerical theory of errors of observations and
the combination of observations. Macmillan&Company, 1861.
Krishna B Athreya and Soumendra N Lahiri. Measure theory and probability theory.
Springer Science & Business Media, 2006.
Francis R Bach and Michael I Jordan. A probabilistic interpretation of canonical correlation
analysis. techreport 688, Department of Statistics, University of California, Berkeley,
Berkeley, 2005.
Sudipto Banerjee, Bradley P Carlin, and Alan E Gelfand. Hierarchical modeling and analysis
for spatial data. Crc Press, 2014.
David J Bartholomew, Martin Knott, and Irini Moustaki. Latent variable models and factor
analysis: A unified approach, volume 904. John Wiley & Sons, 2011.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review
and new perspectives. IEEE transactions on pattern analysis and machine intelligence,
35(8):1798–1828, 2013.
A. Birnbaum. Some latent trait models and their use in inferring an examinee’s ability.,
1968.
Christopher M Bishop, Markus Svensén, and Christopher KI Williams. Gtm: the generative
topographic mapping. Neural computation, 10(1):215–234, 1998.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for
statisticians. Journal of the American Statistical Association (to appear). arXiv preprint
arXiv:1601.00670, 2016.
Kenneth A Bollen. Latent variables in psychology and the social sciences. Annual review
of psychology, 53(1):605–634, 2002.
Michael W Browne. The maximum-likelihood solution in inter-battery factor analysis.
British Journal of Mathematical and Statistical Psychology, 32(1):75–86, 1979.
Michael W Browne. Factor analysis of multiple batteries by maximum likelihood. British
Journal of Mathematical and Statistical Psychology, 33(2):184–199, 1980.
30
A Contemporary Overview of Probabilistic Latent Variable Models
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and
Yoshua Bengio. A recurrent latent variable model for sequential data. In Advances in
neural information processing systems, pages 2980–2988, 2015.
Robert Cudeck. Methods for estimating between-battery factors. Multivariate Behavioral
Research, 17(1):47–68, 1982.
Peter J Diggle and Richard J Gratton. Monte carlo methods of inference for implicit
statistical models. Journal of the Royal Statistical Society. Series B (Methodological),
pages 193–227, 1984.
Rick Durrett. Probability: theory and examples. Cambridge university press, 2010.
Andrew Gelman, John B Carlin, Hal S Stern, and Donald B Rubin. Bayesian data analysis,
volume 2. Chapman & Hall/CRC Boca Raton, FL, USA, 2014.
Mevlana Gemici, Chia-Chun Hung, Adam Santoro, Greg Wayne, Shakir Mohamed, Danilo J
Rezende, David Amos, and Timothy Lillicrap. Generative temporal models with memory.
arXiv preprint arXiv:1702.04649, 2017.
E Gómez, MA Gomez-Viilegas, and JM Marin. A multivariate generalization of the power
exponential family of distributions. Communications in Statistics-Theory and Methods,
27(3):589–600, 1998.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in
neural information processing systems, pages 2672–2680, 2014.
Stephen Jay Gould. The mismeasure of man. WW Norton & Company, 1996.
G Hägglund. Milestones in the history of factor analysis. In Robert Cudeck, K. G. Jöreskog,
and Dag Sörbom, editors, Structural equation modeling, present and future: A festschrift
in honor of Karl Jöreskog, pages 11–38. Scientific Software International, Lincolnwood,
IL, 2001.
Ray Hilborn and Marc Mangel. The ecological detective: confronting models with data,
volume 28. Princeton University Press, 1997.
Harold Hotelling. Analysis of a complex of statistical variables into principal components.
Journal of Educational Psychology, 24(6):417–441, 1933.
Harold Hotelling. Relations Between Two Sets of Variates. Biometrika, 28(3/4):321, dec
1936.
Heungsun Hwang and Yoshio Takane. Generalized structured component analysis. Psy-
chometrika, 69(1):81–99, 2004.
Heungsun Hwang and Yoshio Takane. Generalized structured component analysis: A
component-based approach to structural equation modeling, 2014.
31
Farouni
Aapo Hyvärinen. A unified probabilistic model for independent and principal component
analysis. Advances in Independent Component Analysis and Learning Machines, pages
1–9, 2015.
Alan Julian Izenman. Reduced-rank regression for the multivariate linear model. Journal
of multivariate analysis, 5(2):248–264, 1975.
Alan Julian Izenman. Modern multivariate statistical techniques. Springer, 2008.
Edwin T Jaynes. Probability theory: The logic of science. Cambridge university press, 2003.
Tony Jebara. Machine learning: discriminative and generative, volume 755. Springer Sci-
ence & Business Media, 2012.
K. G. Jöreskog. A general method for analysis of covariance structures. Biometrika, 57(2):
239–251, 1970.
Olav Kallenberg. Probabilistic symmetries and invariance principles. Springer Science &
Business Media, 2006.
Immanuel Kant and Paul Guyer. Critique of pure reason. Cambridge University Press,
1998.
Marc C. Kennedy and Anthony O’Hagan. Bayesian calibration of computer models. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 63, 2001.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. ICLR, (Ml):1–14,
dec 2013.
Diederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with
inverse autoregressive flow. arXiv preprint arXiv:1606.04934v2, 2016.
Arto Klami, Seppo Virtanen, Eemeli Leppäaho, and Samuel Kaski. Group factor analysis.
IEEE transactions on neural networks and learning systems, 26(9):2136–2147, 2015.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept
learning through probabilistic program induction. Science, 350(6266):1332–1338, 2015.
Steffen L Lauritzen. Graphical models, volume 17. Clarendon Press, 1996.
Paul F Lazarsfeld. The logical and mathematical foundation of latent structure analysis.
Measurement and prediction, 4:362–412, 1950.
Frederic M Lord. The relation of test score to the trait underlying the test. ETS Research
Report Series, 1952(2):517–549, 1952.
Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, and Ole Winther. Auxiliary
deep generative models. arXiv preprint arXiv:1602.05473, 2016.
David JC MacKay. Bayesian neural networks and density networks. Nuclear Instruments
and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and
Associated Equipment, 354(1):73–80, 1995a.
32
A Contemporary Overview of Probabilistic Latent Variable Models
David JC MacKay. Probable networks and plausible predictions - a review of practical
bayesian methods for supervised neural networks. Network: Computation in Neural Sys-
tems, 6(3):469–505, 1995b.
Vikash Mansinghka, Patrick Shafto, Eric Jonas, Cap Petschulat, Max Gasner, and Joshua B
Tenenbaum. Crosscat: A fully bayesian nonparametric method for analyzing heteroge-
neous, high dimensional data. Journal of Machine Learning Research, 17:1–49, 2016.
Bengt O Muthén. Beyond sem: General latent variable modeling. Behaviormetrika, 29(1):
81–117, 2002.
Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hy-
pothesis. In Advances in Neural Information Processing Systems, pages 1786–1794, 2010.
Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference.
Morgan Kaufmann, 2014.
Karl Pearson. LIII. On lines and planes of closest fit to systems of points in space. Philo-
sophical Magazine Series 6, 2(11):559–572, nov 1901.
Jonathan K Pritchard, Mark T Seielstad, Anna Perez-Lezaun, and Marcus W Feldman.
Population growth of human y chromosomes: a study of y chromosome microsatellites.
Molecular biology and evolution, 16(12):1791–1798, 1999.
Sophia Rabe-Hesketh, Anders Skrondal, and Andrew Pickles. Generalized multilevel struc-
tural equation modeling. Psychometrika, 69(2):167–190, 2004.
Rajesh Ranganath, Linpeng Tang, Laurent Charlin, and David M Blei. Deep exponential
families. In AISTATS, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082,
2014.
Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, and Daan Wier-
stra. One-shot generalization in deep generative models. arXiv preprint arXiv:1603.05106,
2016.
Sam Roweis and Zoubin Ghahramani. A Unifying Review of Linear Gaussian Models.
Neural Computation, 1999.
Shayle R Searle, George Casella, and Charles E McCulloch. Variance components, volume
391. John Wiley & Sons, 2009.
Anders Skrondal and Sophia Rabe-Hesketh. Generalized Latent Variable Modeling, volume
20041561 of C&H/CRC Monographs on Statistics & Applied Probability. Chapman and
Hall/CRC, may 2004.
Michael E Sobel. Causal inference in latent variable models. 1994.
33
Farouni
C Spearman. ”General Intelligence,” Objectively Determined and Measured. The American
Journal of Psychology, 15(2):201, apr 1904.
Simon Tavaré, David J Balding, Robert C Griffiths, and Peter Donnelly. Inferring coales-
cence times from dna sequence data. Genetics, 145(2):505–518, 1997.
Godfrey H. Thomson. A HIERARCHY WITHOUT A GENERAL FACTOR. British
Journal of Psychology, 1904-1920, 8(3):271–281, sep 1916.
Thurstone and L.L. Multiple Factor Analysis., 1947.
Michael E. Tipping and Christopher M. Bishop. Probabilistic Principal Component Anal-
ysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):
611–622, aug 1999.
James Tobin. Estimation of relationships for limited dependent variables. Econometrica:
journal of the Econometric Society, pages 24–36, 1958.
Matti Tornio, Antti Honkela, and Juha Karhunen. Time series prediction with variational
bayesian nonlinear state-space models.
Dustin Tran, Rajesh Ranganath, and David M Blei. Deep and hierarchical implicit models.
arXiv preprint arXiv:1702.08896, 2017.
Ledyard R Tucker. An inter-battery method of factor analysis. Psychometrika, 23(2):
111–136, 1958.
34

