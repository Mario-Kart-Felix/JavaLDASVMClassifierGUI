Noisy Networks for Exploration
Meire Fortunato‚àó Mohammad Gheshlaghi Azar‚àó Bilal Piot ‚àó
Jacob Menick Ian Osband Alex Graves Vlad Mnih
Remi Munos Demis Hassabis Olivier Pietquin Charles Blundell
Shane Legg
DeepMind
{meirefortunato,mazar,piot,
jmenick,iosband,gravesa,vmnih,
munos,dhcontact,pietquin,cblundell,
legg}@google.com
Abstract
We introduce NoisyNet, a deep reinforcement learning agent with parametric noise
added to its weights, and show that the induced stochasticity of the agent‚Äôs policy
can be used to aid efficient exploration. The parameters of the noise are learned
with gradient descent along with the remaining network weights. NoisyNet is
straightforward to implement and adds little computational overhead. We find that
replacing the conventional exploration heuristics for A3C, DQN and dueling agents
(entropy reward and -greedy respectively) with NoisyNet yields substantially
higher scores for a wide range of Atari games, in some cases advancing the agent
from sub to super-human performance.
1 Introduction
Despite the wealth of research into efficient methods for exploration in Reinforcement Learning
(RL) [16, 15], most exploration heuristics rely on random perturbations of the agent‚Äôs policy, such
as -greedy [33] or entropy regularisation [37], to induce novel behaviours. However such local
‚Äòdithering‚Äô perturbations are unlikely to lead to the large-scale behavioural patterns needed for efficient
exploration in many environments [24].
Optimism in the face of uncertainty is a common exploration heuristic in reinforcement learning.
Various forms of this heuristic often come with theoretical guarantees on agent performance [2, 17,
15, 1, 16].
However, these methods are often limited to small state-action spaces or to linear function approx-
imations and are not easily applied with more complicated function approximators such as neural
networks. A more structured approach to exploration is to augment the environment‚Äôs reward signal
with an additional intrinsic motivation term [32] that explicitly rewards novel discoveries. Many
such terms have been proposed, including learning progress [26], compression progress [30], vari-
ational information maximisation [14] and prediction gain [4]. One problem is that these methods
separate the mechanism of generalisation from that of exploration; the metric for intrinsic reward, and‚Äì
importantly‚Äìits weighting relative to the environment reward, must be chosen by the experimenter,
rather than learned from interaction with the environment. Without due care, the optimal policy can
be altered or even completely obscured by the intrinsic rewards; furthermore, dithering perturbations
are usually needed as well as intrinsic reward to ensure robust exploration [25]. Exploration in
the policy space itself, for example, with evolutionary or black box algorithms [22, 9, 29], usually
requires many prolonged interactions with the environment. Although these algorithms are quite
‚àóEqual contribution.
ar
X
iv
:1
70
6.
10
29
5v
1 
 [
cs
.L
G
] 
 3
0 
Ju
n 
20
17
generic and can apply to any type of parametric policies (including neural networks), they are usually
not data efficient and require a simulator to allow many policy evaluations.
We propose a simple alternative approach, called NoisyNet, where learned perturbations of the
network weights are used to drive exploration. The key insight is that a single change to the weight
vector can induce a consistent, and potentially very complex, state-dependent change in policy over
multiple timesteps ‚Äì unlike dithering approaches where decorrelated (and, in the case of -greedy,
state-independent) noise is added to the policy at every step. The perturbations are sampled from a
noise distribution. The variance of the perturbation is a parameter that can be considered as the energy
of the injected noise. These variance parameters are learned using gradients from the reinforcement
learning loss function, along side the other parameters of the agent. The approach differs from
parameter compression schemes such as variational inference [12, 7] and flat minima search [13]
since we do not maintain an explicit distribution over weights during training but simply inject
noise in the parameters and tune its intensity automatically. It also differs from variational inference
[11, 8] or Thompson sampling [35, 19] as the distribution on the parameters of our agents does not
necessarily converge to an approximation of a posterior distribution.
At a high level our algorithm induces a randomised network for exploration, with care this approach
can be provably-efficient with suitable linear value functions [24]. Previous attempts to extend this
approach to deep neural networks required many duplicates of sections of the network [23]. By
contrast our NoisyNet approach requires only one extra parameter per weight and applies to policy
gradient methods such as A3C out of the box [20]. Most recently (and independently of our work),
Plappert et al. [27] presented a similar technique where constant Gaussian noise is added to the
parameters of the network. Our method thus differs by the ability of the network to adapt the noise
injection with time and it is not restricted to Gaussian noise distributions. It can also be adapted to
any deep RL algorithm and we demonstrate this versatility by providing NoisyNet versions of DQN
(value-based) [21] and A3C (policy based) [20] algorithms. Experiments on 57 Atari games show
that they both achieve striking gains when compared to the baseline algorithms without significant
extra computational cost.
2 Background
This section provides mathematical background for Markov Decision Processes (MDPs) and deep
RL with Q-learning and actor-critic methods.
2.1 Markov Decision Processes and Reinforcement Learning
MDPs model stochastic, discrete-time and finite action-state space control problems [5, 6, 28]. An
MDP is a tuple M = (X ,A, R, P, Œ≥) where X is the state space, A the action space, R the reward
function, Œ≥ ‚àà]0, 1[ the discount factor and P a stochastic kernel modelling the one-step Markovian
dynamics (P (y|x, a) is the probability of transitioning to state y by choosing action a in state x). A
stochastic policy œÄ maps each state to a distribution over actions œÄ(¬∑|x) and gives the probability
œÄ(a|x) of choosing action a in state x. The quality of a policy œÄ is assessed by the action-value
function QœÄ defined as:
QœÄ(x, a) = EœÄ
[
+‚àû‚àë
t=0
Œ≥tR(xt, at)
]
, (1)
where EœÄ is the expectation over the distribution of the admissible trajectories (x0, a0, x1, a1, . . . )
obtained by executing the policy œÄ starting from x0 = x and a0 = a. Therefore, the quantityQœÄ(x, a)
represents the expected Œ≥-discounted cumulative reward collected by executing the policy œÄ starting
from x and a. A policy is optimal if no other policy yields a higher return. The action-value function
of the optimal policy is Q?(x, a) = arg maxœÄ QœÄ(x, a).
The value function V œÄ for a policy is defined as
V œÄ(x) = Ea ‚àºœÄ(¬∑|x)[QœÄ(x, a)], (2)
and represents the expected Œ≥-discounted return collected by executing the policy œÄ starting from
state x.
2
2.2 Deep Reinforcement Learning
Deep Reinforcement Learning uses deep neural networks as function approximators for RL methods.
Deep Q-Networks (DQN) [21], Asynchronous Advantage Actor-Critic (A3C) [20], Trust Region
Policy Optimisation [31] and Deep Deterministic Policy Gradient [18] are examples of such algo-
rithms. They frame the RL problem as the minimisation of a loss function L(Œ∏), where Œ∏ represents
the parameters of the network. In our experiments we shall consider the DQN and A3C algorithms.
DQN [21] uses a neural network as an approximator for the action-value function of the optimal policy
Q?(x, a). DQN‚Äôs estimate of the optimal action-value function, Q(x, a), is found by minimising the
following loss with respect to the neural network parameters Œ∏:
L(Œ∏) = E(x,a,r,y)‚àºD
[(
r + Œ≥max
b‚ààA
Q(y, b; Œ∏‚àí)‚àíQ(x, a; Œ∏)
)2]
, (3)
where D is a distribution over transitions e = (x, a, r = R(x, a), y ‚àº P (¬∑|x, a)) drawn from a replay
buffer of previously observed transitions. Here Œ∏‚àí represents the parameters of a fixed and separate
target network which is updated (Œ∏‚àí ‚Üê Œ∏) regularly to stabilise the learning. An -greedy policy
is used to pick actions greedily according to the action-value function Q or, with probability , a
random action is taken.
In contrast, A3C [20] is a policy gradient algorithm. A3C‚Äôs network directly learns a policy œÄ and a
value function V of its policy. The gradient of the loss on the A3C policy at step t for the roll-out
(xt+i, at+i ‚àº œÄ(¬∑|xt+i; Œ∏), rt+i)ki=0 is:
‚àáŒ∏LœÄ(Œ∏) = ‚àíEœÄ
[
k‚àë
i=0
‚àáŒ∏ log(œÄ(at+i|xt+i; Œ∏))A(xt+i, at+i; Œ∏) + Œ≤
k‚àë
i=0
‚àáŒ∏H(œÄ(¬∑|xt+i; Œ∏))
]
.
(4)
H[œÄ(¬∑|xt; Œ∏)] denotes the entropy of the policy œÄ and Œ≤ is a hyperparameter that trades off between op-
timising the advantage function and the entropy of the policy. The advantage functionA(xt+i, at+i; Œ∏)
is the difference between observed returns and estimates of the return produced by A3C‚Äôs value
network: A(xt+i, at+i; Œ∏) =
‚àëk
j=i Œ≥
j‚àíirt+j +Œ≥
k‚àíiV (xt+k; Œ∏)‚àíV (xt+i; Œ∏), rt+j being the reward
at step t+ j and V (x; Œ∏) being the agent‚Äôs estimate of value function of state x.
Parameters of the value function are found to match on-policy returns: LV (Œ∏) =
EœÄ
[‚àëk
i=0(QÃÇi ‚àí V (xt+i; Œ∏))2
]
where QÃÇi is the return obtained by executing policy œÄ starting in state
xt+i: QÃÇi =
‚àëk
j=i Œ≥
j‚àíirt+j+Œ≥
k‚àíiV (xt+k; Œ∏). The overall A3C loss is thenL(Œ∏) = LœÄ(Œ∏)+ŒªLV (Œ∏)
where Œª balances optimising the policy loss relative to the baseline value function loss.
3 NoisyNets for Reinforcement Learning
NoisyNets are neural networks whose weights and biases are perturbed by a parametric function
of the noise. These parameters are adapted with gradient descent. More precisely, let y = fŒ∏(x)
be a neural network parameterised by the vector of noisy parameters Œ∏ which takes the input x and
outputs y. We represent the noisy parameters Œ∏ as Œ∏ def= ¬µ + Œ£  Œµ, where Œ∂ def= (¬µ,Œ£) is a set of
vectors of learnable parameters, Œµ is a vector of zero-mean noise with fixed statistics and  represents
element-wise multiplication. The usual loss of the neural network is wrapped by expectation over the
noise Œµ: LÃÑ(Œ∂) def= E [L(Œ∏)]. Optimisation now occurs with respect to the set of parameters Œ∂.
Consider a linear layer of a neural network with p inputs and q outputs, represented by
y = wx+ b (5)
where x ‚àà Rp is the layer input,w ‚àà Rq√óp the weight matrix, and b ‚àà Rq the bias. The corresponding
noisy linear layer is defined as:
y
def
= (¬µw + œÉw  Œµw)x+ ¬µb + œÉb  Œµb, (6)
where ¬µw + œÉw  Œµw and ¬µb + œÉb  Œµb replace w and b in Eq. (5), respectively. The parameters
¬µw ‚àà Rq√óp, ¬µb ‚àà Rq, œÉw ‚àà Rq√óp and œÉb ‚àà Rq are learnable whereas Œµw ‚àà Rq√óp and Œµb ‚àà Rq are
noise random variables (the specific choices of this distribution are described below).
3
We now turn to explicit instances of the noise distributions for linear layers in a noisy network. We
explore two options:
(a) Independent Gaussian noise: the noise applied to each weight and bias is independent, where
each entry Œµwi,j (respectively each entry Œµ
b
j) of the random matrix Œµ
w (respectively of the random
vector Œµb) is drawn from a unit Gaussian distribution. This means that for each noisy linear layer,
there are pq + q noise variables (for p inputs to the layer and q outputs).
(b) Factorised Gaussian noise reduces the number of random variables in the network from one per
weight, to one per input and one per output of each noisy linear layer. By factorising Œµwi,j , we can
use p unit Gaussian variables Œµi for noise of the inputs and and q unit Gaussian variables Œµj for
noise of the outputs (thus p+ q unit Gaussian variables in total). Each Œµwi,j and Œµ
b
j can then be
written as:
Œµwi,j = f(Œµi)f(Œµj) (7)
Œµbj = f(Œµj), (8)
where f is a functions. In our experiments we used f(x) = sgn(x)
‚àö
|x|.
Since the loss of a noisy network, LÃÑ(Œ∂) = E [L(Œ∏)], is an expectation over the noise, the gradients are
straightforward to obtain:
‚àáLÃÑ(Œ∂) = ‚àáE [L(Œ∏)] = E [‚àá¬µ,Œ£L(¬µ+ Œ£ Œµ)] . (9)
We use a Monte Carlo approximation to the above gradients, taking a single sample Œæ at each step of
optimisation:
‚àáLÃÑ(Œ∂) ‚âà ‚àá¬µ,Œ£L(¬µ+ Œ£ Œæ). (10)
3.1 Deep Reinforcement Learning with NoisyNets
We now turn to our application of noisy networks to exploration in deep reinforcement learning. Noise
drives exploration in many methods for reinforcement learning, providing a source of stochasticity
external to the agent and the RL task at hand. Either the scale of this noise is manually tuned across a
wide range of tasks (as is the practice in general purpose agents such as DQN or A3C) or it can be
manually scaled per task. Here we propose automatically tuning the level of noise added to an agent
for exploration, using the noisy networks training to drive down (or up) the level of noise injected
into the parameters of a neural network, as needed.
A noisy network agent samples a new set of parameters after every step of optimisation. Between
optimisation steps, the agent acts according to a fixed set of parameters (weights and biases). This
ensures that the agent always acts according to parameters that are drawn from the current noise
distribution.
Deep Q-Networks (DQN) We modify DQN as follows: first, Œµ-greedy is no longer used, but
instead the policy greedily optimises the value function. Secondly, the fully connected layers of
the value function are parameterised as a noisy network, where the parameters are drawn from the
noisy network parameter distribution after every replay step. We used factorised Gaussian noise as
explained in (b) from Sec. 3. For replay, the current noisy network parameter sample is held fixed.
Since DQN takes one step of optimisation for every action step, the noisy network parameters are
re-sampled before every action. We call this adaptation of DQN, NoisyNet-DQN.
We now provide the details of the loss function that our variant of DQN is minimising. When replacing
the linear layers by noisy layers in the network (respectively in the target network), the parameterised
action-value function Q(x, a, Œµ; Œ∂) (respectively Q(x, a, Œµ‚Ä≤; Œ∂‚àí)) can be seen as a random variable
and the DQN loss becomes the NoisyNet-DQN loss:
LÃÑ(Œ∂) = E
[
E(x,a,r,y)‚àºD[r + Œ≥max
b‚ààA
Q(y, b, Œµ‚Ä≤; Œ∂‚àí)‚àíQ(x, a, Œµ; Œ∂)]2
]
. (11)
where the outer expectation is with respect to distribution of the noise variables Œµ for the noisy
value function Q(x, a, Œµ; Œ∂) and the noise variable Œµ‚Ä≤ of the noisy target value function Q(y, b, Œµ‚Ä≤; Œ∂‚àí).
Computing an unbiased estimate of the loss is straightforward as we only need to compute, for each
transition in the replay buffer, one instance of the target network and one instance of the learning
network. Concerning the action choice, we sample from the target network and we act greedily with
respect to the output action-value function. The algorithm is provided in Sec. C.1.
4
Asynchronous Advantage Actor Critic (A3C) A3C is modified in a similar fashion to DQN:
firstly, the entropy bonus of the policy loss is removed. Secondly, the fully connected layers of
the policy network are parameterised as a noisy network. We used independent Gaussian noise as
explained in (a) from Sec. 3. In A3C, there is no explicit exploratory action selection scheme (such
as -greedy); and the chosen action is always drawn from the current policy. For this reason, an
entropy bonus of the policy loss is often added to discourage updates leading to deterministic policies.
However, when adding noisy weights to the network, sampling these parameters corresponds to
choosing a different current policy which naturally favours exploration. As a consequence of direct
exploration in the policy space, the artificial entropy loss on the policy can thus be omitted. New
parameters of the policy network are sampled after each step of optimisation, and since A3C uses n
step returns, optimisation occurs every n steps. We call this modification of A3C, NoisyNet-A3C.
Indeed, when replacing the linear layers by noisy linear layers (the parameters of the noisy network
are now noted Œ∂), we obtain the following estimation of the return via a roll-out of size k:
QÃÇi =
k‚àë
j=i
Œ≥j‚àíirt+j + Œ≥
k‚àíiV (xt+k; Œ∂, Œµi). (12)
As A3C is an on-policy algorithm the gradients are unbiased when noise of the network is consistent
for the whole roll-out. Consistency among action value functions QÃÇi is ensured by letting each action
value function have its noise be the same, i.e., ‚àÄi, Œµi = Œµ. Additional details are provided in the
Appendix Sec. A.2 and the algorithm is given in Sec. C.2.
4 Results
We evaluated the performance of noisy network agents on 57 Atari games [3] and compared to
baselines that, without noisy networks, rely upon the original exploration methods (Œµ-greedy and
entropy bonus). We used the random start no-ops scheme for training and evaluation as described
the original DQN paper [21]. The mode of evaluation is identical to those of [20] where randomised
restarts of the games are used for evaluation after training has happened.
We consider three baseline agents: DQN [21], duel clip variant of Dueling algorithm [36], and A3C
[20]. The NoisyNet variant of each of the agents was described in the previous section, except for the
dueling case which is a straightforward adaptation of the DQN described in [36]. In each case, we
used the neural network architecture from the corresponding original papers for both the baseline and
NoisyNet variant. For the NoisyNet variants we searched over the same hyperparameters as in the
respective original paper for the baseline.
We compared absolute performance of agents using the human normalised score:
100√ó
Scoreagent ‚àí ScoreRandom
ScoreHuman ‚àí ScoreRandom
, (13)
where human and random scores are the same as those in [36]. Note that the absolute human
normalised score is zero for a random agent and 100 for human level performance. Per-game scores
are computed by taking the maximum performance for each game and then averaging over three seeds.
The overall agent performance is measured by either the mean or median of the human normalised
score across all 57 Atari games.
The aggregate results across all 57 Atari games are shown in Table 1, while the individual scores for
each game are in Table 2 from the Appendix. The median human normalised score is improved in all
agents by using NoisyNet, adding at least 20 percentage points to the median human normalised score
for all agents. The mean human normalised score is also dramatically improved for DQN and A3C
agents, but the mean score is worse for Dueling. Note that the mean human normalised score is often
dominated by the performance of the agent on just a few games and so is not a robust estimator of
overall agent performance. Interestingly the Dueling case demonstrates that NoisyNet is orthogonal
to several other improvements made to DQN.
We also compared relative performance of NoisyNet agents to the respective baseline agent without
noisy networks:
100√ó
ScoreNoisyNet ‚àí ScoreBaseline
max(ScoreHuman,ScoreBaseline)‚àí ScoreRandom
, (14)
5
(a) Improvement in percentage of NoisyNet-DQN over DQN [21]
(b) Improvement in percentage of NoisyNet-Dueling over Dueling [36]
(c) Improvement in percentage of NoisyNet-A3C over A3C [20]
Figure 1: Comparison of NoisyNet agent versus the baseline according to Eq. (14). The maximum
score is truncated at 250%.
Baseline NoisyNet
Mean Median Mean Median
DQN 213 47 1210 89
A3C 418 93 1112 121
Dueling 2102 126 1908 154
Table 1: Comparison between the baseline DQN [21], A3C [20] and Dueling [36] and their NoisyNet
version in terms of median and mean human-normalised scores defined in Eq. (13).
As before, the per-game score is computed by taking the maximum performance for each game and
then averaging over three seeds. The relative human normalised scores are shown in Figure 1. As
can be seen, the performance of NoisyNet agents (DQN, Dueling and A3C) is better for the majority
of games relative to the corresponding baseline, and in some cases by a considerable margin. This
is especially evident in the case of NoisyNet-A3C, where the agent can solve some games that the
baseline A3C previously could not solve (e.g., see the results for freeway and Kangaroo in Fig. 2). In
some other games, NoisyNet agents provide an order of magnitude improvement on the performance
of the vanilla agent; as can be seen in Table 2 in the Appendix with detailed breakdown of individual
6
game scores. More information is provided in the learning curves plots from Figs 3, 4 and 5, for A3C,
DQN and dueling respectively.
Figure 2: Training curves for selected Atari games comparing A3C and NoisyNet-A3C. Please refer
to Fig. 3 in the Appendix for additional games.
5 Conclusion
We have presented a general method for exploration in deep reinforcement learning that shows
significant performance improvements across many Atari games in three different agent architectures.
In particular, we observe that in games such as Asterix and Freeway that the standard DQN and
A3C perform poorly compared with the human player, NoisyNet-DQN and NoisyNet-A3C achieve
super human performance. Our method eliminates the need for -greedy and the entropy bonus
commonly used in Q-learning-style and policy gradient methods, respectively. Instead we show that
better exploration is possible by relying on perturbations in weight space to drive exploration. This is
in contrast to many other methods that add intrinsic motivation signals that may destabilise learning
or change the optimal policy. Another interesting feature of the NoisyNet approach is that the degree
of exploration is contextual and varies from state to state based upon per-weight variances. While
more gradients are needed, the gradients on the mean and variance parameters are related to one
another by a computationally efficient affine function, thus the computational overhead is marginal.
Automatic differentiation makes implementation of our method a straightforward adaptation of many
existing methods. A similar randomisation technique can also be applied to LSTM units [10] and is
easily extended to reinforcement learning, we leave this as future work.
Note NoisyNet exploration strategy is not restricted to the baselines considered in this paper. In
fact, this idea can be applied to any deep RL algorithms that can be trained with gradient descent,
including DDPG [18] and TRPO [31]. As such we believe this work is a step towards the goal of
developing a universal exploration strategy.
Acknowledgements We would like to thank Koray Kavukcuoglu, Oriol Vinyals, Daan Wierstra,
Georg Ostrovski, Joseph Modayil, Simon Osindero, Chris Apps, Stephen Gaffney and many others at
DeepMind for insightful discussions, comments and feedback on this work.
References
[1] Peter Auer and Ronald Ortner. Logarithmic online regret bounds for undiscounted reinforcement
learning. Advances in Neural Information Processing Systems, 19:49, 2007.
[2] Mohammad Gheshlaghi Azar, Ian Osband, and R√©mi Munos. Minimax regret bounds for
reinforcement learning. arXiv preprint arXiv:1703.05449, 2017.
7
[3] Marc Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning
environment: An evaluation platform for general agents. In Twenty-Fourth International Joint
Conference on Artificial Intelligence, 2015.
[4] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi
Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural
Information Processing Systems, pages 1471‚Äì1479, 2016.
[5] Richard Bellman and Robert Kalaba. Dynamic programming and modern control theory.
Academic Press New York, 1965.
[6] Dimitri Bertsekas. Dynamic programming and optimal control, volume 1. Athena Scientific,
Belmont, MA, 1995.
[7] Chris M Bishop. Training with noise is equivalent to Tikhonov regularization. Neural computa-
tion, 7(1):108‚Äì116, 1995.
[8] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty
in neural networks. In Proceedings of The 32nd International Conference on Machine Learning,
pages 1613‚Äì1622, 2015.
[9] Jeremy Fix and Matthieu Geist. Monte-Carlo swarm policy search. In Swarm and Evolutionary
Computation, pages 75‚Äì83. Springer, 2012.
[10] Meire Fortunato, Charles Blundell, and Oriol Vinyals. Bayesian recurrent neural networks.
arXiv preprint arXiv:1704.02798, 2017.
[11] Alex Graves. Practical variational inference for neural networks. In Advances in Neural
Information Processing Systems, pages 2348‚Äì2356, 2011.
[12] Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing
the description length of the weights. In Proceedings of the sixth annual conference on
Computational learning theory, pages 5‚Äì13. ACM, 1993.
[13] Sepp Hochreiter and J√ºrgen Schmidhuber. Flat minima. Neural Computation, 9(1):1‚Äì42, 1997.
[14] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. VIME:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems, pages 1109‚Äì1117, 2016.
[15] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(Apr):1563‚Äì1600, 2010.
[16] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.
Machine Learning, 49(2-3):209‚Äì232, 2002.
[17] Tor Lattimore, Marcus Hutter, and Peter Sunehag. The sample-complexity of general reinforce-
ment learning. In Proceedings of The 30th International Conference on Machine Learning,
pages 28‚Äì36, 2013.
[18] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
[19] Zachary C Lipton, Jianfeng Gao, Lihong Li, Xiujun Li, Faisal Ahmed, and Li Deng. Efficient
exploration for dialogue policy learning with BBQ networks & replay buffer spiking. arXiv
preprint arXiv:1608.05081, 2016.
[20] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,
Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep rein-
forcement learning. In International Conference on Machine Learning, pages 1928‚Äì1937,
2016.
[21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature, 518(7540):529‚Äì533, 2015.
[22] David E Moriarty, Alan C Schultz, and John J Grefenstette. Evolutionary algorithms for
reinforcement learning. Journal of Artificial Intelligence Research, 11:241‚Äì276, 1999.
[23] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped DQN. In Advances In Neural Information Processing Systems, pages 4026‚Äì4034,
2016.
8
[24] Ian Osband, Daniel Russo, Zheng Wen, and Benjamin Van Roy. Deep exploration via random-
ized value functions. arXiv preprint arXiv:1703.07608, 2017.
[25] Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Remi Munos. Count-based
exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.
[26] Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? A typology of
computational approaches. Frontiers in neurorobotics, 1, 2007.
[27] Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration.
arXiv preprint arXiv:1706.01905, 2017.
[28] Martin Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 1994.
[29] Tim Salimans, J. Ho, X. Chen, and I. Sutskever. Evolution Strategies as a Scalable Alternative
to Reinforcement Learning. ArXiv e-prints, 2017.
[30] J√ºrgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990‚Äì2010).
IEEE Transactions on Autonomous Mental Development, 2(3):230‚Äì247, 2010.
[31] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization.
In Proc. of ICML, pages 1889‚Äì1897, 2015.
[32] Satinder P Singh, Andrew G Barto, and Nuttapong Chentanez. Intrinsically motivated reinforce-
ment learning. In NIPS, volume 17, pages 1281‚Äì1288, 2004.
[33] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. Cambridge
Univ Press, 1998.
[34] Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Proc. of NIPS, volume 99,
pages 1057‚Äì1063, 1999.
[35] William R Thompson. On the likelihood that one unknown probability exceeds another in view
of the evidence of two samples. Biometrika, 25(3/4):285‚Äì294, 1933.
[36] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas.
Dueling network architectures for deep reinforcement learning. In Proceedings of The 33rd
International Conference on Machine Learning, pages 1995‚Äì2003, 2016.
[37] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229‚Äì256, 1992.
9
A Algorithm Descriptions
A.1 NoisyNet-DQN implementation details
DQN [21] is an approximate value iteration technique that uses a deep convolutional neural network
to represent the Q-function as Q(x, a; Œ∏), where Œ∏ are the parameters of the network. We used the
network architecture from [21] and also the same set up for interacting with the Atari games. We note,
however, that we did not use -greedy technique, instead relied solely upon NoisyNet for exploration.
In DQN, the neural network is trained by minimising the following loss:
L(Œ∏) = E(x,a,r,y)‚àºD[yDQN ‚àíQ(x, a; Œ∏)]2,
yDQN = r + Œ≥max
b‚ààA
Q(y, b; Œ∏‚àí) (15)
where D is a distribution over transitions e = (x, a, r = R(x, a), y ‚àº P (¬∑|x, a)). Here Œ∏‚àí represents
the parameters of the target network which is updated (Œ∏‚àí ‚Üê Œ∏) regularly and stabilises the learning.
The distribution D is generated by constructing a replay buffer were training examples are uniformly
sampled from previous experience tuples et = (xt, at, rt, xt+1). We now provide the details of the
loss function that our variant of DQN with noisy network is minimising. When replacing the linear
layers by noisy layers (the parameters are now noted Œ∂) in the network (respectively in the target
network), the parameterised action-value function Q(x, a, Œµ; Œ∂) (respectively Q(x, a, Œµ‚Ä≤; Œ∂‚àí)) can be
seen as a random variable and the DQN loss becomes the NoisyNet-DQN loss:
LÃÑ(Œ∂) = E
[
E(x,a,r,y)‚àºD[r + Œ≥max
b‚ààA
Q(y, b, Œµ‚Ä≤; Œ∂‚àí)‚àíQ(x, a, Œµ; Œ∂)]2
]
. (16)
The goal of NoisyNet-DQN is to optimize the loss LÃÑ(Œ∂) in terms of Œ∂.
A.2 NoisyNet-A3C implementation details
In contrast with value-based algorithms, policy-based methods such as A3C [20] parameterise the
policy œÄ(a|x; Œ∏œÄ) directly and update the parameters Œ∏œÄ by performing a gradient ascent on the mean
value-function Es‚àºD[V œÄ(¬∑|¬∑;Œ∏œÄ)(x)] (also called the expected return) [34]. A3C uses a deep neural
network with weights Œ∏ = Œ∏œÄ ‚à™ Œ∏V to parameterise the policy œÄ and the value V . The network has
one softmax output for the policy-head œÄ(¬∑|¬∑; Œ∏œÄ) and one linear output for the value-head V (¬∑; Œ∏V ),
with all non-output layers shared. The parameters Œ∏œÄ (resp. Œ∏V ) are relative to the shared layers
and the policy head (resp. the value head). A3C is an asynchronous and online algorithm that uses
roll-outs of size k + 1 of the current policy to perform a policy improvement step.
In A3C, there is no explicit exploratory action selection scheme (such as -greedy); and the chosen
action is always drawn from the current policy. For this reason, an entropy loss on the policy is often
added to discourage updates leading to deterministic policies. However, when using noisy weights in
the network, sampling corresponds to choosing a different current policy which naturally favours
exploration. As a consequence of direct exploration in the policy space, the artificial entropy loss on
the policy can thus be omitted.
For simplicity, here we present the A3C version with only one thread. For a multi-thread imple-
mentation, refer to the pseudo-code C.2 or to the original A3C paper [20]. In order to train the
policy-head, an approximation of the policy-gradient is computed for each state of the roll-out
(xt+i, at+i ‚àº œÄ(¬∑|xt+i; Œ∏œÄ), rt+i)ki=0:
‚àáŒ∏œÄ log(œÄ(at+i|xt+i; Œ∏œÄ))[QÃÇi ‚àí V (xt+i; Œ∏V )], (17)
where QÃÇi is an estimation of the return QÃÇi =
‚àëk
j=i Œ≥
j‚àíirt+j + Œ≥
k‚àíiV (xt+k; Œ∏V ). The gradients are
then added to obtain the cumulative gradient of the roll-out:
k‚àë
i=0
‚àáŒ∏œÄ log(œÄ(at+i|xt+i; Œ∏œÄ))[QÃÇi ‚àí V (xt+i; Œ∏V )]. (18)
A3C trains the value-head by minimising the error between the estimated return and the value‚àëk
i=0(QÃÇi ‚àí V (xt+i; Œ∏V ))2. Therefore, the network parameters (Œ∏œÄ, Œ∏V ) are updated after each
10
roll-out as follows:
Œ∏œÄ ‚Üê Œ∏œÄ + Œ±œÄ
k‚àë
i=0
‚àáŒ∏œÄ log(œÄ(at+i|xt+i; Œ∏œÄ))[QÃÇi ‚àí V (xt+i; Œ∏V )], (19)
Œ∏V ‚Üê Œ∏V ‚àí Œ±V
k‚àë
i=0
‚àáŒ∏V [QÃÇi ‚àí V (xt+i; Œ∏V )]2, (20)
where (Œ±œÄ, Œ±V ) are hyper-parameters. As mentioned previously, in the original A3C algorithm, it
is recommended to add an entropy term Œ≤
‚àëk
i=0‚àáŒ∏œÄH(œÄ(¬∑|xt+i; Œ∏œÄ)) to the policy update, where
H(œÄ(¬∑|xt+i; Œ∏œÄ)) = ‚àíŒ≤
‚àë
a‚ààA œÄ(a|xt+i; Œ∏œÄ) log(œÄ(a|xt+i; Œ∏œÄ)). Indeed, this term encourages ex-
ploration as it favours policies which are uniform over actions. When replacing the linear layers in
the value and policy heads by noisy layers (the parameters of the noisy network are now Œ∂œÄ and Œ∂V ),
we obtain the following estimation of the return via a roll-out of size k:
QÃÇi =
k‚àë
j=i
Œ≥j‚àíirt+j + Œ≥
k‚àíiV (xt+k; Œ∂V , Œµi). (21)
We would like QÃÇi to be a consistent estimate of the return of the current policy. To do so, we should
force ‚àÄi, Œµi = Œµ. As A3C is an on-policy algorithm, this involves fixing the noise of the network for
the whole roll-out so that the policy produced by the network is also fixed. Hence, each update of the
parameters (Œ∂œÄ, Œ∂V ) is done after each roll-out with the noise of the whole network held fixed for the
duration of the roll-out:
Œ∂œÄ ‚Üê Œ∂œÄ + Œ±œÄ
k‚àë
i=0
‚àáŒ∂œÄ log(œÄ(at+i|xt+i; Œ∂œÄ, Œµ))[QÃÇi ‚àí V (xt+i; Œ∂V , Œµ)], (22)
Œ∂V ‚Üê Œ∂V ‚àí Œ±V
k‚àë
i=0
‚àáŒ∂V [QÃÇi ‚àí V (xt+i; Œ∂V , Œµ)]2. (23)
B Initialisation of Noisy Networks
In the case of an unfactorised noisy networks, the parameters ¬µ and œÉ are initialised as follows. Each
element ¬µi,j is sampled from independent uniform distributions U [‚àí
‚àö
3
p ,+
‚àö
3
p ], where p is the
number of inputs to the corresponding linear layer, and each element œÉi,j is simply set to 0.017 for
all parameters.
For factorised noisy networks, each element ¬µi,j was initialised by a sample from an independent
uniform distributions U [‚àí 1‚àöp ,+
1‚àö
p ] and each element œÉi,j was initialised to a constant
œÉ0‚àö
p . The
hyperparameter œÉ0 is set to 0.4.
11
C Algorithms
C.1 NoisyNet-DQN
Algorithm 1: NoisyNet-DQN
Input :Env Environment; Nf maximum length of list x; Œµ set of random variables of the network
Input :B empty replay buffer; Œ∂ initial network parameters; Œ∂‚àí initial target network parameters
Input :NB replay buffer size; NT training batch size; N‚àí target network replacement frequency
Output :Q(., Œµ; Œ∂) action-value function
1 for episode e ‚àà {1, . . . ,M} do
/* x is a list of states */
2 x‚Üê [ ]
3 Initialise state sequence x0 ‚àº Env
4 x[0]‚Üê x0
5 for t ‚àà {1, . . . } do
/* l[‚àí1] is the last element of the list l */
6 Set x‚Üê x[‚àí1]
7 Sample a noisy network Œæ ‚àº Œµ
8 Select an action a‚Üê argmaxb‚ààAQ(x, b, Œæ; Œ∂‚àí)
9 Sample next state y ‚àº P (¬∑|x, a), receive reward r ‚Üê R(x, a) and append x: x[‚àí1]‚Üê y
10 if |x| > Nf then
11 Delete oldest element from x
12 end
13 Add transition (x, a, r, y) to the replay buffer B[‚àí1]‚Üê (x, a, r, y)
14 if |B| > NB then
15 Delete oldest transition from B
16 end
/* D is a distribution over the replay, it can be uniform or
implementing prioritised replay */
17 Sample a minibatch of NT transitions ((xj , aj , rj , yj) ‚àº D)NTj=1
/* Construction of the target values. */
18 for j ‚àà {1, . . . , NT } do
19 Sample a noisy network Œæj ‚àº Œµ
20 Sample a noisy target network Œæ‚Ä≤j ‚àº Œµ
21 if y is a terminal state then
22 yj ‚Üê rj
23 else
24 yj ‚Üê rj + maxb‚ààAQ(yj , b, Œæ‚Ä≤j ; Œ∂‚àí)
25 Do a gradient step with loss (yj ‚àíQ(xj , aj , Œæj ; Œ∂))2
26 end
27 if t ‚â° 0 (mod N‚àí) then
28 Update the target network: Œ∂‚àí ‚Üê Œ∂
29 end
30 end
31 end
12
C.2 NoisyNet-A3C
Algorithm 2: NoisyNet-A3C for each actor-learner thread
Input :Environment Env, Global shared parameters (Œ∂œÄ, Œ∂V ), global shared counter T and
maximal time Tmax.
Input :Thread-specific parameters (Œ∂ ‚Ä≤œÄ, Œ∂ ‚Ä≤V ), Set of random variables Œµ, thread-specific counter t
and roll-out size tmax.
Output :œÄ(¬∑; Œ∂, Œµ) the policy and V (¬∑; Œ∂V , Œµ) the value.
1 Initial thread counter t‚Üê 1
2 repeat
3 Reset cumulative gradients: dŒ∂œÄ ‚Üê 0 and dŒ∂V ‚Üê 0.
4 Synchronise thread-specific parameters: Œ∂ ‚Ä≤œÄ ‚Üê Œ∂œÄ and Œ∂ ‚Ä≤V ‚Üê Œ∂V .
5 counter ‚Üê 0.
6 Get state xt from Env
7 Choice of the noise: Œæ ‚àº Œµ
/* r is a list of rewards */
8 r ‚Üê [ ]
/* a is a list of actions */
9 a‚Üê [ ]
/* x is a list of states */
10 x‚Üê [ ] and x[0]‚Üê xt
11 repeat
12 Policy choice: at ‚àº œÄ(¬∑|xt; Œ∂œÄ; Œæ)
13 a[‚àí1]‚Üê at
14 Receive reward rt and new state xt+1
15 r[‚àí1]‚Üê rt and x[‚àí1]‚Üê xt+1
16 t‚Üê t+ 1 and T ‚Üê T + 1
17 counter = counter + 1
18 until xt terminal or counter == tmax + 1
19 if xt is a terminal state then
20 Q = 0
21 else
22 Q = V (xt; Œ∂V , Œæ)
23 for i ‚àà {counter ‚àí 1, . . . , 0} do
24 Update Q: Q‚Üê r[i] + Œ≥Q.
25 Accumulate policy-gradient: dŒ∂œÄ ‚Üê dŒ∂œÄ +‚àáŒ∂‚Ä≤œÄ log(œÄ(a[i]|x[i]; Œ∂
‚Ä≤
œÄ, Œæ))[Q‚àí V (x[i]; Œ∂ ‚Ä≤V , Œæ)].
26 Accumulate value-gradient: dŒ∂V ‚Üê dŒ∂V +‚àáŒ∂‚Ä≤V [Q‚àí V (x[i]; Œ∂
‚Ä≤
V , Œæ)]
2.
27 end
28 Perform asynchronous update of Œ∂œÄ: Œ∂œÄ ‚Üê Œ∂œÄ + Œ±œÄdŒ∂œÄ
29 Perform asynchronous update of Œ∂V : Œ∂V ‚Üê Œ∂V ‚àí Œ±V dŒ∂V
30 until T > Tmax
13
Game Human Random DQN NoisyNet-DQN Dueling NoisyNet-Dueling A3C NoisyNet-A3C
alien 7128 228 1860 2899 5899 4460 2148 2809
amidar 1720 6 783 1534 1690 2215 814 819
assault 742 222 1439 3126 2792 3876 3434 7502
asterix 8503 210 3807 11037 23091 32427 8756 34011
asteroids 47389 719 1336 2101 1506 3166 2576 26380
atlantis 29028 12580 60001 7910700 8546500 8782433 415000 491000
bank heist 753 14 235 646 656 1198 1252 999
battle zone 37188 2360 18810 11925 37763 42767 16146 22546
beam rider 16926 364 6850 14689 16397 17092 7746 10783
berzerk 2630 124 497 972 1168 2277 994 1032
bowling 161 23 47 49 71 32 34 60
boxing 12 0 78 89 94 96 92 89
breakout 30 2 240 382 428 421 451 422
centipede 12017 2091 3131 3352 5759 6276 5772 8704
chopper command 7388 811 3628 5343 5333 5565 4473 10389
crazy climber 35829 10780 89944 121229 133065 126772 127750 130400
defender 18689 2874 7923 19880 24725 32042 48604 48883
demon attack 1971 152 8142 26730 29101 77881 32772 42168
double dunk -16 -19 -11 2 5 1 3 0
enduro 860 0 473 909 1679 1483 0 0
fishing derby -39 -92 -14 -6 11 19 -22 16
freeway 30 0 19 32 33 33 0 33
frostbite 4335 65 418 355 986 1338 283 276
gopher 2412 258 6149 10516 22970 24725 7354 9271
gravitar 3351 173 258 255 380 324 405 343
hero 30826 1027 14706 9704 21722 16127 26829 24511
ice hockey 1 -11 -6 -5 -0 -2 -2 0
jamesbond 303 29 574 1061 1339 584 532 4214
kangaroo 3035 52 5088 14763 14005 14678 131 6967
krull 2666 1598 6263 7096 9264 9192 9853 9475
kung fu master 22736 258 16240 27324 35985 42526 44625 42683
montezuma 4753 0 9 5 1 5 15 10
ms pacman 6952 307 2372 2786 3101 4209 2445 2862
name this game 8049 2292 6183 7153 10770 11603 7655 9242
phoenix 7243 761 6798 12540 17932 17073 10844 36839
pitfall 6464 -229 0 -2 0 0 0 0
pong 15 -21 18 21 21 21 21 21
private eye 69571 25 1313 2015 173 911 1942 282
qbert 13455 164 7700 11797 15028 15662 17971 17169
riverraid 17118 1338 5717 8766 14081 14855 8355 8125
road runner 7845 12 25650 34459 47524 55497 39073 10298
robotank 12 2 47 54 58 54 7 45
seaquest 42055 68 2881 3302 12534 9941 1742 927
skiing -4337 -17098 -14779 -17871 -11425 -14835 -15115 -8846
solaris 12327 1263 2522 2913 2177 2360 11830 9173
space invaders 1669 148 1081 1524 2516 4490 1008 1378
star gunner 10250 664 24582 28861 51787 58290 50933 45612
surround 6 -10 -9 -6 1 3 -8 -7
tennis -8 -24 9 0 8 0 0 0
time pilot 5229 3568 4392 6116 10671 11192 8993 11560
tutankham 168 11 141 135 264 264 321 165
up n down 11693 533 5209 9364 14569 18234 90290 145113
venture 1188 0 586 8 8 51 0 17
video pinball 17668 16257 88092 152022 768419 548689 140863 630390
wizard of wor 4756 564 2045 7576 7715 10605 9858 11852
yars revenge 54577 3093 14027 16437 18917 21738 14715 69408
zaxxon 9173 32 4218 9703 12609 9954 15567 1367
Table 2: Raw scores across all games with random starts.
14
0 50 100 150 200 250 300
Frames (in millions)
0
500
1000
1500
2000
2500
3000
3500
A
ve
ra
g
e
 s
co
re
alien
NoisyNet-A3C A3C
0 50 100 150 200 250 300
Frames (in millions)
0
200
400
600
800
1000
A
ve
ra
g
e
 s
co
re
amidar
0 50 100 150 200 250 300
Frames (in millions)
0
1000
2000
3000
4000
5000
6000
7000
A
ve
ra
g
e
 s
co
re
assault
0 50 100 150 200 250 300
Frames (in millions)
0
5000
10000
15000
20000
25000
30000
A
ve
ra
g
e
 s
co
re
asterix
0 50 100 150 200 250 300
Frames (in millions)
0
10000
20000
30000
40000
50000
A
ve
ra
g
e
 s
co
re
asteroids
0 50 100 150 200 250 300
Frames (in millions)
0
100000
200000
300000
400000
500000
A
ve
ra
g
e
 s
co
re
atlantis
0 50 100 150 200 250 300
Frames (in millions)
0
200
400
600
800
1000
1200
1400
A
ve
ra
g
e
 s
co
re
bank_heist
0 50 100 150 200 250 300
Frames (in millions)
0
5000
10000
15000
20000
25000
30000
A
ve
ra
g
e
 s
co
re
battle_zone
0 50 100 150 200 250 300
Frames (in millions)
0
2000
4000
6000
8000
10000
12000
A
ve
ra
g
e
 s
co
re
beam_rider
0 50 100 150 200 250 300
Frames (in millions)
200
400
600
800
1000
A
ve
ra
g
e
 s
co
re
berzerk
0 50 100 150 200 250 300
Frames (in millions)
0
10
20
30
40
50
60
70
80
A
ve
ra
g
e
 s
co
re
bowling
0 50 100 150 200 250 300
Frames (in millions)
40
20
0
20
40
60
80
100
A
ve
ra
g
e
 s
co
re
boxing
0 50 100 150 200 250 300
Frames (in millions)
0
100
200
300
400
500
A
ve
ra
g
e
 s
co
re
breakout
0 50 100 150 200 250 300
Frames (in millions)
1000
2000
3000
4000
5000
6000
7000
8000
9000
A
ve
ra
g
e
 s
co
re
centipede
0 50 100 150 200 250 300
Frames (in millions)
0
2000
4000
6000
8000
10000
12000
A
ve
ra
g
e
 s
co
re
chopper_command
0 50 100 150 200 250 300
Frames (in millions)
0
20000
40000
60000
80000
100000
120000
140000
A
ve
ra
g
e
 s
co
re
crazy_climber
0 50 100 150 200 250 300
Frames (in millions)
0
10000
20000
30000
40000
50000
A
ve
ra
g
e
 s
co
re
defender
0 50 100 150 200 250 300
Frames (in millions)
0
5000
10000
15000
20000
25000
30000
35000
40000
45000
A
ve
ra
g
e
 s
co
re
demon_attack
0 50 100 150 200 250 300
Frames (in millions)
25
20
15
10
5
0
5
A
ve
ra
g
e
 s
co
re
double_dunk
0 50 100 150 200 250 300
Frames (in millions)
0.06
0.04
0.02
0.00
0.02
0.04
0.06
A
ve
ra
g
e
 s
co
re
enduro
0 50 100 150 200 250 300
Frames (in millions)
100
80
60
40
20
0
20
40
A
ve
ra
g
e
 s
co
re
fishing_derby
0 50 100 150 200 250 300
Frames (in millions)
0
5
10
15
20
25
30
35
A
ve
ra
g
e
 s
co
re
freeway
0 50 100 150 200 250 300
Frames (in millions)
50
100
150
200
250
300
A
ve
ra
g
e
 s
co
re
frostbite
0 50 100 150 200 250 300
Frames (in millions)
0
2000
4000
6000
8000
10000
12000
A
ve
ra
g
e
 s
co
re
gopher
0 50 100 150 200 250 300
Frames (in millions)
50
100
150
200
250
300
350
400
450
A
ve
ra
g
e
 s
co
re
gravitar
0 50 100 150 200 250 300
Frames (in millions)
0
5000
10000
15000
20000
25000
30000
35000
A
ve
ra
g
e
 s
co
re
hero
0 50 100 150 200 250 300
Frames (in millions)
18
16
14
12
10
8
6
4
2
0
A
ve
ra
g
e
 s
co
re
ice_hockey
0 50 100 150 200 250 300
Frames (in millions)
0
500
1000
1500
2000
2500
3000
3500
4000
4500
A
ve
ra
g
e
 s
co
re
jamesbond
0 50 100 150 200 250 300
Frames (in millions)
0
2000
4000
6000
8000
10000
12000
A
ve
ra
g
e
 s
co
re
kangaroo
0 50 100 150 200 250 300
Frames (in millions)
2000
3000
4000
5000
6000
7000
8000
9000
10000
11000
A
ve
ra
g
e
 s
co
re
krull
0 50 100 150 200 250 300
Frames (in millions)
0
10000
20000
30000
40000
50000
60000
A
ve
ra
g
e
 s
co
re
kung_fu_master
0 50 100 150 200 250 300
Frames (in millions)
0
5
10
15
20
25
A
ve
ra
g
e
 s
co
re
montezuma_revenge
0 50 100 150 200 250 300
Frames (in millions)
0
500
1000
1500
2000
2500
3000
A
ve
ra
g
e
 s
co
re
ms_pacman
0 50 100 150 200 250 300
Frames (in millions)
1000
2000
3000
4000
5000
6000
7000
8000
9000
A
ve
ra
g
e
 s
co
re
name_this_game
0 50 100 150 200 250 300
Frames (in millions)
0
10000
20000
30000
40000
50000
60000
A
ve
ra
g
e
 s
co
re
phoenix
0 50 100 150 200 250 300
Frames (in millions)
1000
800
600
400
200
0
A
ve
ra
g
e
 s
co
re
pitfall
0 50 100 150 200 250 300
Frames (in millions)
30
20
10
0
10
20
30
A
ve
ra
g
e
 s
co
re
pong
0 50 100 150 200 250 300
Frames (in millions)
500
0
500
1000
1500
2000
2500
3000
3500
4000
A
ve
ra
g
e
 s
co
re
private_eye
0 50 100 150 200 250 300
Frames (in millions)
0
5000
10000
15000
20000
A
ve
ra
g
e
 s
co
re
qbert
0 50 100 150 200 250 300
Frames (in millions)
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
A
ve
ra
g
e
 s
co
re
riverraid
0 50 100 150 200 250 300
Frames (in millions)
0
5000
10000
15000
20000
25000
30000
35000
40000
45000
A
ve
ra
g
e
 s
co
re
road_runner
0 50 100 150 200 250 300
Frames (in millions)
0
5
10
15
20
25
30
35
40
45
A
ve
ra
g
e
 s
co
re
robotank
0 50 100 150 200 250 300
Frames (in millions)
0
200
400
600
800
1000
1200
1400
1600
1800
A
ve
ra
g
e
 s
co
re
seaquest
0 50 100 150 200 250 300
Frames (in millions)
35000
30000
25000
20000
15000
10000
5000
A
ve
ra
g
e
 s
co
re
skiing
0 50 100 150 200 250 300
Frames (in millions)
0
2000
4000
6000
8000
10000
12000
14000
A
ve
ra
g
e
 s
co
re
solaris
0 50 100 150 200 250 300
Frames (in millions)
0
200
400
600
800
1000
1200
1400
A
ve
ra
g
e
 s
co
re
space_invaders
0 50 100 150 200 250 300
Frames (in millions)
0
10000
20000
30000
40000
50000
60000
A
ve
ra
g
e
 s
co
re
star_gunner
0 50 100 150 200 250 300
Frames (in millions)
10.0
9.5
9.0
8.5
8.0
7.5
7.0
A
ve
ra
g
e
 s
co
re
surround
0 50 100 150 200 250 300
Frames (in millions)
25
20
15
10
5
0
5
A
ve
ra
g
e
 s
co
re
tennis
0 50 100 150 200 250 300
Frames (in millions)
0
2000
4000
6000
8000
10000
12000
A
ve
ra
g
e
 s
co
re
time_pilot
0 50 100 150 200 250 300
Frames (in millions)
0
50
100
150
200
250
300
350
A
ve
ra
g
e
 s
co
re
tutankham
0 50 100 150 200 250 300
Frames (in millions)
0
20000
40000
60000
80000
100000
120000
140000
160000
A
ve
ra
g
e
 s
co
re
up_n_down
0 50 100 150 200 250 300
Frames (in millions)
0
5
10
15
20
25
30
35
A
ve
ra
g
e
 s
co
re
venture
0 50 100 150 200 250 300
Frames (in millions)
0
100000
200000
300000
400000
500000
600000
A
ve
ra
g
e
 s
co
re
video_pinball
0 50 100 150 200 250 300
Frames (in millions)
0
2000
4000
6000
8000
10000
12000
A
ve
ra
g
e
 s
co
re
wizard_of_wor
0 50 100 150 200 250 300
Frames (in millions)
0
10000
20000
30000
40000
50000
60000
70000
A
ve
ra
g
e
 s
co
re
yars_revenge
0 50 100 150 200 250 300
Frames (in millions)
0
2000
4000
6000
8000
10000
12000
14000
16000
A
ve
ra
g
e
 s
co
re
zaxxon
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Figure 3: Training curves for all Atari games comparing A3C and NoisyNet-A3C.
15
Figure 4: Training curves for all Atari games comparing DQN and NoisyNet-DQN.
16
Figure 5: Training curves for all Atari games comparing Duelling and NoisyNet-Dueling.
17

