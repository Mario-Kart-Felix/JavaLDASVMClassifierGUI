ar
X
iv
:1
70
6.
09
75
1v
1 
 [
st
at
.M
L
] 
 2
9 
Ju
n 
20
17
Bayesian Semisupervised Learning with Deep Generative Models
Jonathan Gordon José Miguel Hernández-Lobato
Abstract
Neural network based generative models with
discriminative components are a powerful ap-
proach for semi-supervised learning. However,
these techniques a) cannot account for model un-
certainty in the estimation of the model’s dis-
criminative component and b) lack flexibility to
capture complex stochastic patterns in the label
generation process. To avoid these problems, we
first propose to use a discriminative component
with stochastic inputs for increased noise flexi-
bility. We show how an efficient Gibbs sampling
procedure can marginalize the stochastic inputs
when inferring missing labels in this model. Fol-
lowing this, we extend the discriminative com-
ponent to be fully Bayesian and produce esti-
mates of uncertainty in its parameter values. This
opens the door for semi-supervised Bayesian ac-
tive learning.
1. Introduction
Deep generative models (DGMs) can model complex high
dimensional data via the use of latent variables. Re-
cently, advances in variational training procedures such
as stochastic backpropagation (Rezende et al., 2014) and
the reparametrization trick (Kingma & Welling, 2013) have
made training these models feasible and efficient. DGMs
are particularly powerful when neural networks are used
to parameterize generative distributions and inference net-
works, leading to the Variational Autoencoder (VAE, Fig-
ure 1a) (Kingma & Welling, 2013; Rezende et al., 2014).
The ability to efficiently train such models has led to a
plethora of interesting extensions, increasing the flexibility
of posterior approximations, expressiveness, and capabili-
ties of the models (Burda et al., 2015; Tomczak & Welling,
2016; Maaløe et al., 2016; Rezende & Mohamed, 2015).
An important extension to standard VAEs is for semi-
supervised learning (Kingma et al., 2014; Maaløe et al.,
2016), which incorporates labels into the generative model
of the inputs (Figure 1b), extending the VAE to semi-
supervised learning tasks. In this settting, the labels are
treated as latent variables that influence the generative pro-
cess for the inputs and a recognition network is then used
as a discriminative component to infer missing labels.
One major drawback of these previous approaches for
semi-supervised learning with VAEs is that after training,
the generative model is discarded and the recognition net-
work is the only element used as a discriminative compo-
nent (Kingma et al., 2014; Maaløe et al., 2016). This is un-
satisfactory from a modeling perspective as the recognition
network is just a tool for performing approximate infer-
ence, and cannot be used to quantify model uncertainty.
Closely related to deep generative models are Bayesian
neural networks (BNNs) (Neal, 2012). BNNs extend stan-
dard neural networks and explicitly model the uncertainty
in the learned weights increasing robustness and opening
the door to tasks requiring uncertainty such as active
learning (Gal et al., 2017; Hernández-Lobato & Adams,
2015) and Bayesian optimization (Snoek et al., 2015).
Blundell et al. (2015) extend ideas from stochas-
tic variational inference (Kingma & Welling, 2013;
Ranganath et al., 2014) to an efficient inference procedure
for BNNs. Further, Depeweg et al. (2016) show how
learning can be extended to general α-divergence mini-
mization (Hernández-Lobato et al., 2016; Moerland et al.,
2017) and provide empirical evidence of the benefit of
introducing stochastic inputs (Figure 1c). However, in
these works the stochastic inputs are of low (typically one)
dimension.
We extend these ideas and develop a deep generative model
with a discriminative component given by a BNN with
stochastic inputs to accommodate semi-supervised learn-
ing. Our motivation is that after training, this BNN can be
used to infer missing labels, rather than using the inference
networks. This allows us to fully quantify any modeling
uncertainty in the predictions of our discriminative compo-
nent. We introduce two recognition networks and demon-
strate how they can be used for training and prediction, as
well as for posterior inference of high-dimensional stochas-
tic inputs at prediction time. Our goal is to use the proposed
method for semi-supervised Bayesian active learning.
2. Related Work
DGMs have recently shown to be very effective in
semi-supervised learning tasks (Kingma et al., 2014;
Bayesian Semisupervised Learning
x
z
N
(a) Standard VAE
yz
x
N
(b) Conditional VAE
xz
y
N
(c) BNN - stochastic inputs
xz
y
N
(d) Proposed graphical model
Figure 1. Graphical model depiction for VAE and BNN based models and proposed approach.
Maaløe et al., 2016), achieving state-of-the-art perfor-
mance on a number of benchmarks. Our model is most sim-
ilar to the work detailed by Kingma et al. (2014). However,
since our discriminative component is a Bayesian neural
network with random inputs, we use a slightly different in-
ference network architecture.
Similarly, Bayesian deep learning has recently been
shown to be highly effective in active learning regimes
(Hernández-Lobato & Adams, 2015; Gal et al., 2017). In
contrast to these works, the proposed model can per-
form semi-supervised and active learning simultaneously,
which may lead to significant improvements. Another dif-
ference is that while Gal et al. (2017) use dropout as a
proxy for Bayesian inference (Gal & Ghahramani, 2015)
and Hernández-Lobato & Adams (2015) use a technique
called probabilistic backpropagation, we propose leverag-
ing variational inference to explicitly model the weight un-
certainty (Blundell et al., 2015).
The proposed model builds on ideas from both DGMs and
Bayesian deep networks to suggest a principled method for
simultaneous semi-supervised and active learning.
3. Deep Generative Model of Labels
We propose extending the model developed by
Depeweg et al. (2016) (as in Fig. 1d) and including
an inference network for z, allowing scalability of the
latent dimension. Further, we propose inference proce-
dures to allow this model to be used for semi-supervised
learning, similarly to Kingma et al. (2014); Maaløe et al.
(2015).
There are a few motivations for our approach: (i) it builds
on the idea of VAEs, but does so in a manner that results
in an explicit probabilistic model for the labels, (ii) it ex-
tends BNNs with stochastic inputs to include inference net-
works for the stochastic inputs, allowing generalizing these
to high dimensional variables, and (iii) it naturally accom-
modates semi-supervised and active learning with the gen-
erative model. The generative model can be described as:
p(z) = N (z;0, I) , (1)
pθ(x|z) = N (x;µx, νx) , (2)
pθ(y|z, x) = Cat (y;πy) , (3)
where we parameterize the distributions of x, y with deep
neural networks:
µx = NNx(z, θ) , log νx = NNx(z, θ) , (4)
πy = NNy(z, x, θ) , (5)
where NNx and NNy have weights Wx and Wy respec-
tively, and θ = {Wx,Wy}.
3.1. Variational Training of the Model
We propose a variational approach for training the model
with both labeled and unlabeled data. In this section we are
interested in point estimates of θ and Bayesian inference for
z. We first derive the variational lower bound. In the semi-
supervised setting, there are two lower bounds (ELBOs),
for the labeled and unlabeled case.
3.1.1. LABELED DATA ELBO
Following recent advances in variational inference
(Kingma & Welling, 2013; Rezende et al., 2014), we
introduce inference networks qφ(z|x, y) to approximate
the intractable posterior distribution. Taking expectations
w.r.t. q of the log likelihoods we have:
log pθ(x, y) ≥ Eqφ(z|x,y) [log pθ(x|z) + log pθ(y|x, z)]−
DKL (qφ(z|x, y)||p(z)) = L
l(x, y; θ, φ) , (6)
where qφ(z|x, y) is a recognition network parameterized by
φ. The lower-bound contains a term for the likelihood asso-
ciated with the pair of variables x and y, and a regulariza-
tion term for the inference network. We can approximate
Bayesian Semisupervised Learning
expectations w.r.t. qφ(z|x, y) via Monte Carlo estimation:
Ll(x, y; θ, φ) ≈
1
L
L
∑
l=1
[
log pθ(x, y|z
l)−
log qφ(z
l|x) + log p(zl)
]
, (7)
with zl ∼ qφ(z|x, y), and for Gaussian recognition net-
works the KL term can be evaluated analytically.
3.1.2. UNLABELED DATA ELBO
We can follow a similar approach to derive the lower bound
for the unlabeled case. In this setting, we have:
log pθ(x) ≥ Eqφ(y|x)
[
Ll(x, y; θ, φ)
]
+H [qφ(y|x)] =
Lu(x; θ, φ) , (8)
where we have used the decomposition qφ(z, y|x) =
qφ(y|x)qφ(z|x, y), and H [·] computes the entropy of a
probability distribution. Thus, we have the two recognition
networks qφ(z|x, y) and qφ(y|x). This form of the lower
bound has data fit terms for both x and y in the generative
model, as well as regularization terms for both recognition
networks qφ(z|x, y) and qφ(y|x).
The recognition network qφ(z|x, y) is shared by both the
unlabeled and labeled objectives. The recognition network
qφ(y|x) is unique to the unlabeled data. Following the
work in (Kingma et al., 2014; Maaløe et al., 2016), we add
a weighted term to the final objective function to ensure
that qφ(y|x) is trained on all data such that
L(θ, φ) =
∑
(x,y)∼p̃l
Ll(x, y; θ, φ) +
∑
x∼p̃u
Lu(x; θ, φ)+
αE(x,y)∼p̃l [log qφ(y|x)] , (9)
where α is a small positive constant which is initialized in
a similar way as in (Kingma et al., 2014), p̃l is the empir-
ical distribution of labeled points and p̃u is the empirical
distribution of unlabeled points.
3.2. Discrete Outputs
Optimizing Eq. (9) is straightforward when y is continu-
ous as we can approximate expectations by sampling from
qφ(z|x, y) and qφ(y|x) and using stochastic backpropaga-
tion and the reparameterization trick (Kingma & Welling,
2013; Rezende et al., 2014) to yield differentiable estima-
tors.
Despite recent efforts (Jang et al., 2016), reparameteriza-
tion for discrete variables is as yet not a well-understood
process. In the case where y is a discrete variable, that is,
y ∈ {1, ...,K}, optimization of Lu requires approximating
expectations w.r.t. qφ(y|x). Rather than using Monte-Carlo
approximations for this, we propose to directly compute the
expectation by summing over the possible values of y:
Eqφ(y|x) [f(y, z, x)] =
∑
y∈Y
qφ(y|x)f(x, y, z) . (10)
Substituting this for the relevant terms in Eq. (8) yields:
Eqφ(y|x) [log pθ(x, y|z)] =
∑
y∈Y
qφ(y|x)Eqφ(z|x,y) [log pθ(x, y|z)] (11)
and
Eqφ(y|x) [Dkl(qφ(z|x, y)||p(z))] =
∑
y∈Y
qφ(y|x)DKL (qφ(z|x, y)||p(z)) . (12)
Taking explicit expectations w.r.t. qφ(y|x) rather than
Monte-Carlo approximations allows us to extend the train-
ing procedure to cases where y is discrete.
3.3. Introducing Model Uncertainty
A major advantage of this approach is that it enables us to
express model uncertainty in the discriminative component
NNy(z, x, θ) by computing a posterior distribution on the
weights Wy . For this, we consider the following prior and
likelihood functions:
pθ(Wy) = N (Wy ; 0, I) , (13)
pθ(y|x, z,Wy) = Cat(y;πy) , (14)
where πy = NNy(z, x,Wy) is parameterized by a neural
network with weights Wy . Assuming a single labeled data
point, the posterior distribution for the latent variables is:
pθ(Wy , z|x, y) =
pθ(x, y|z,Wy)p(z)p(Wy)
p(x, y)
. (15)
This posterior distribution is intractable. Following the
work by Blundell et al. (2015); Depeweg et al. (2016), we
introduce an approximate posterior distribution for Wy
given by qφ(Wy) = N (Wy ;µw, σ2w) with σ
2
w being a di-
agonal covariance matrix. Note that we are assuming here
that Wy is a posteriori independent of z. We also assume
a posteriori independence between Wy and y in the unla-
beled case. Re-deriving the lower bound for the case of one
single labeled data point yields:
L(x, y; θ, φ) = Eqφ(z,|x,y)qφ(Wy) [log pθ(x, y|z,Wy)]−
DKL (qφ(z|x, y)‖p(z))−
DKL (qφ(Wy)‖p(Wy)) . (16)
The corresponding derviation for the unlabeled lower
bound can be obtained from Eq. (8) in a similar manner.
We follow the work presented by Blundell et al. (2015),
and optimize the objective functions applying reparameter-
ization to the weights Wy as well as z.
Bayesian Semisupervised Learning
(a) (b) (c)
(d) (e)
Figure 2. Preliminary experiments carried with the model. (a) Complete set of labeled data. (b) Contour plots learned by a standard
DNN using only six labeled labeled data. (c) Contour plots learned by the model using only the depicted points with labels (the rest
unsupervised). (d) Contour plots learned with Bayesian training of the model. (e) Samples from the generative model after training.
3.4. Prediction with the Model
To approximate p(y⋆|x⋆) for a new example x⋆, we have
to integrate pθ(y⋆|x⋆, z,Wy) with respect to the posterior
distribution on z and Wy . For this, Wy is sampled from
qφ(Wy), while z is sampled from the recognition network
qφ(z|x⋆, y⋆). Since this recognition network requires y⋆,
we use a Gibbs sampling procedure, drawing the first sam-
ple of y⋆ from the recognition network qφ(y⋆|x⋆). In par-
ticular,
y
(0)
⋆ ∼ qφ(y⋆|x⋆) ,
W
(τ)
y ∼ qφ(Wy) ,
z(τ) ∼ qφ(z|x⋆, y
(τ−1)
⋆ ) ,
y
(τ)
⋆ ∼ pθ(y⋆|x⋆, z(τ),W
(τ)
y ) ,





for τ = 1, . . . , T .
with the final prediction being an average over the sam-
ples. Using qφ(y⋆|x⋆) to initialize the procedure increases
efficiency and negates the need for a burn in period. In our
experiments T = 10 produced good results.
4. Preliminary Results
In this section we detail preliminary results achieved by the
proposed model. We experiment with toy data similar to
that used by Maaløe et al. (2016). The data consists of 1e4
training and test samples, generated from a deterministic
function with additive Gaussian noise (Figure 2a). A small
set is selected as the labeled data, and the rest are unlabeled.
We compare performance to that of a feed forward neural
network.
Table 1. Test accuracy and log-likelihood for each method.
DNN SSLPE SSLAPD
LOG-LIKELIHOOD -1.20 -0.07 -0.01
ACCURACY 83.6 99.2 99.7
All neural networks have two hidden layers with 128 neu-
rons, and z ∈ R5. We set α = 0.1 ∗ Nl, where Nl is the
number of labeled data points, and use RELU activations
for all hidden layers.
The model converges on 100% accuracy in all cases for
Nl > 10. When examining training curves for different
values of Nl (not shown due to space constraints), we see
that as the labeled set is larger, training converges faster and
to better lower bounds.
Figures 2b, 2c, 2d show, respectively, the predictive prob-
abilities learned by a deep neural network (DNN) which
ignores the unlabeled data, the proposed approach for
Semi-Supervised Learning using a Point Estimate for Wy
(SSLPE) and the proposed approach for Semi-Supervised
Learning using an Approximate Posterior Distribution for
Wy (SSLAPD). We use Nl = 6 in all cases: three la-
beled examples selected from each class, in a similar man-
ner to Maaløe et al. (2016). Table 1 shows the average test
log-likelihood and predictive accuracy obtained by each
method. DNN overfits the labeled data and achieves low
predictive accuracy and test log-likelihood. SSLPE is able
to leverage the unlabeled data to learn a smoother deci-
Bayesian Semisupervised Learning
sion boundary that aligns with the data distribution, achiev-
ing much better predictive performance than DNN. Finally,
SSLAPD makes predictions similar to those of SSLPE but
with higher uncertainty in regions far away from the train-
ing data. Overall, SSLAPD is the best performing method.
Finally, Figure 2e shows samples generated by SSLAPD,
indicating this method has learned a good generative pro-
cess and is able to synthesize compelling examples.
5. Discussion and Future Work
DGMs have been successfully applied to semi-supervised
learning tasks, though by discarding the model and using
only an inference network for predicting labels. This ap-
proach does not allow to account for model uncertainty.
In contrast, the proposed approach uses a Bayesian neural
network for label prediction. In addition to being more sat-
isfying from a modeling perspective, this opens the door to
joint semi-supervised and active learning by accounting for
model uncertainty.
Our experiments show that the proposed approach is
promissing and able to produce wider confidence bands far
away from the training data than alternative methods that
ignore parameter uncertainty. Further experiments with al-
ternative datasets such as MNIST are required. Future work
includes developing methods for acquiring new samples
from a pool set, performing joint semisupervised and active
learning, and comparing with recent benchmark methods.
References
Blundell, Charles, Cornebise, Julien, Kavukcuoglu, Koray,
and Wierstra, Daan. Weight uncertainty in neural net-
works. arXiv preprint arXiv:1505.05424, 2015.
Burda, Yuri, Grosse, Roger, and Salakhutdinov, Rus-
lan. Importance weighted autoencoders. arXiv preprint
arXiv:1509.00519, 2015.
Depeweg, Stefan, Hernández-Lobato, José Miguel, Doshi-
Velez, Finale, and Udluft, Steffen. Learning and policy
search in stochastic dynamical systems with Bayesian
neural networks. arXiv preprint arXiv:1605.07127,
2016.
Gal, Yarin and Ghahramani, Zoubin. Dropout as a bayesian
approximation: Representing model uncertainty in deep
learning. arXiv preprint arXiv:1506.02142, 2, 2015.
Gal, Yarin, Islam, Riashat, and Ghahramani, Zoubin. Deep
bayesian active learning with image data. arXiv preprint
arXiv:1703.02910, 2017.
Hernández-Lobato, José Miguel and Adams, Ryan P.
Probabilistic backpropagation for scalable learning of
bayesian neural networks. In ICML, pp. 1861–1869,
2015.
Hernández-Lobato, José Miguel, Li, Yingzhen, Rowland,
Mark, Hernández-Lobato, Daniel, Bui, Thang D, and
Turner, Richard E. Black-box α-divergence minimiza-
tion. 2016.
Jang, Eric, Gu, Shixiang, and Poole, Ben. Categorical repa-
rameterization with Gumbel-softmax. arXiv preprint
arXiv:1611.01144, 2016.
Kingma, Diederik P and Welling, Max. Auto-encoding
variational Bayes. arXiv preprint arXiv:1312.6114,
2013.
Kingma, Diederik P, Mohamed, Shakir, Rezende,
Danilo Jimenez, and Welling, Max. Semi-supervised
learning with deep generative models. In Advances in
Neural Information Processing Systems, pp. 3581–3589,
2014.
Maaløe, Lars, Sønderby, Casper Kaae, Sønderby,
Søren Kaae, and Winther, Ole. Improving semi-
supervised learning with auxiliary deep generative mod-
els. In NIPS Workshop on Advances in Approximate
Bayesian Inference, 2015.
Maaløe, Lars, Sønderby, Casper Kaae, Sønderby,
Søren Kaae, and Winther, Ole. Auxiliary deep gener-
ative models. arXiv preprint arXiv:1602.05473, 2016.
Moerland, Thomas M, Broekens, Joost, and Jonker,
Catholijn M. Learning multimodal transition dynamics
for model-based reinforcement learning. arXiv preprint
arXiv:1705.00470, 2017.
Neal, Radford M. Bayesian learning for neural networks,
volume 118. Springer Science & Business Media, 2012.
Ranganath, Rajesh, Gerrish, Sean, and Blei, David M.
Black box variational inference. In AISTATS, pp. 814–
822, 2014.
Rezende, Danilo Jimenez and Mohamed, Shakir. Varia-
tional inference with normalizing flows. arXiv preprint
arXiv:1505.05770, 2015.
Rezende, Danilo Jimenez, Mohamed, Shakir, and Wier-
stra, Daan. Stochastic backpropagation and approxi-
mate inference in deep generative models. arXiv preprint
arXiv:1401.4082, 2014.
Snoek, Jasper, Rippel, Oren, Swersky, Kevin, Kiros, Ryan,
Satish, Nadathur, Sundaram, Narayanan, Patwary, Md
Mostofa Ali, Prabhat, Mr, and Adams, Ryan P. Scalable
bayesian optimization using deep neural networks. In
ICML, pp. 2171–2180, 2015.
Bayesian Semisupervised Learning
Tomczak, Jakub M and Welling, Max. Improving vari-
ational auto-encoders using householder flow. arXiv
preprint arXiv:1611.09630, 2016.

