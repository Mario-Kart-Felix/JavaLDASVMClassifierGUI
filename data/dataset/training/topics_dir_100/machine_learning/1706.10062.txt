Barankin Vector Locally Best Unbiased Estimatesâˆ—
Bruno Cernuschi-FrÌÄ±asâ€ 
Abstract
The Barankin bound is generalized to the vector case in the mean
square error sense. Necessary and sufficient conditions are obtained to
achieve the lower bound. To obtain the result, a simple finite dimensional
real vector valued generalization of the Riesz representation theorem for
Hilbert spaces is given. The bound has the form of a linear matrix in-
equality where the covariances of any unbiased estimator, if these exist,
are lower bounded by matrices depending only on the parametrized prob-
ability distributions.
Keywords: Parameter estimation, unbiased estimation, optimal estimator,
Barankin bound, performance bounds, linear matrix inequalities, minimal co-
variance matrix, Cramer-Rao bound.
1 Introduction
The problem considered, following Barankin, [2], and results in Banach, [1], is
the optimal unbiased estimation of a deterministic vector of parameters Î½ of
a family of probability measures PÎ½ , or more generally a known real vector
function of these parameters g(Î½), using a realization of a vector random vari-
able X drawn from PÎ½T . The first issue is to find a function Ïˆ such thatâˆ«
Ïˆ(X ) dPÎ½ = g(Î½), for all Î½ in some admissible set. This problem is a vector
integral equation and may or may not have a solution, [6, 20]. Furthermore, even
if it has solution, it may not have a solution with finite covariance matrix for Î½T .
Barankin, under very simple hypothesis, [2], gives an if and only if condition
for the existence of a minimal s-th variance unbiased estimator for the scalar
case, which is tighter than the classical Cramer-Rao or Bhattacharyya bounds
if they exist. In recent years the Barankin bound has attracted attention, since
there are problems for which the Cramer-Rao or Bhattacharyya bounds give
no satisfactory solution, see e.g. [22], and references there. Following [2], the
problem studied here is under what conditions there exists a finite covariance
âˆ—This work was partially supported by the Universidad de Buenos Aires, UBA, grant
UBACYT No. 20020130100357BA, and the Consejo Nacional de Investigaciones CientÌÄ±ficas y
TeÌcnicas, CONICET, Argentina.
â€ B. Cernuschi-FrÌÄ±as is with the Universidad de Buenos Aires, Facultad de IngenierÌÄ±a, and
the Instituto Argentino de MatemaÌticas, IAM, CONICET, Casilla 8, Sucursal 12(B), 1412
Buenos Aires, Argentina (e-mail: bcf@acm.org).
1
ar
X
iv
:1
70
6.
10
06
2v
1 
 [
st
at
.M
L
] 
 3
0 
Ju
n 
20
17
B. Cernuschi-FrÌÄ±as 2
vector unbiased estimator of the true vector parameter Î½T , and in that case if
a minimal covariance vector unbiased estimator exists.
In Section 2 an overview is presented of the relevant results of measure the-
ory and the Lebesgue integral related to the Barankin formulation. In Section
3 the vector Barankin bound generalization is presented as a linear matrix in-
equality (LMI). In Section 4 the Barankin functional analysis formalization is
generalized to handle the vector case. In Section 5 a finite dimensional real vec-
tor valued generalization of the Riesz representation theorem for Hilbert spaces
is presented. In Section 6 necessary and sufficient conditions are given for the
existence of an optimal vector estimator attaining the bound given by the LMI
obtained in Section 3. In Section 7 other alternative LMI formulations for the
existence of an optimal vector estimator are given.
2 Formalization of the vector estimation prob-
lem
2.1 Measure theoretic setup
Let (â„¦,F ) be a measurable space, where â„¦ is a well defined abstract set, and
F is a sigma-algebra of subsets of â„¦, [14]. Let Î˜ be an abstract arbitrary set
of sub-indexes with no conditions on its structure as in [2], p. 477. Let B
be a collection of probability measures PÎ¸ for the measurable space (â„¦,F ),
indexed by the sub-indexes Î¸ âˆˆ Î˜, i.e. B =
{
PÎ¸ : Î¸ âˆˆ Î˜
}
, as in [2] p. 477.
Hence for each Î¸ âˆˆ Î˜, the triple (â„¦,F ,PÎ¸) is a probability space. Let X
be a vector random variable, i.e. a measurable function from the measurable
space (â„¦,F ) to the measurable space (RdS , BdS ), where RdS is the vector
dS-dimensional real space, and BdS is the Borel sigma-algebra for RdS , that
is the minimal sigma-algebra generated, e. g., by the open sets of RdS . Then
X is a real vector random variable iff âˆ€B âˆˆ BdS we have X âˆ’1(B) âˆˆ F , if
and only if each component of the vector is a real random variable, [17] p. 19.
Define for each Î¸ âˆˆ Î˜ the measure PÎ¸, for the measurable space (RdS , BdS ),
induced by the random variable X , [10] p. 34, i.e. for each B âˆˆ BdS define
PÎ¸(B) = PÎ¸(X âˆ’1(B)). Hence for each Î¸ âˆˆ Î˜ the random variable X induces
the probability space (RdS , BdS , PÎ¸).
Let Ïˆ be a real measurable vector function from (RdS , BdS ) to (RdP , BdP ),
that is Ïˆ : RdS â†’ RdP , and for each B âˆˆ BdP we have Ïˆâˆ’1(B) âˆˆ BdS .
For the measurable vector function Ïˆ from (RdS ,BdS ) to (RdP ,BdP ), define
the i-th component of the vector Ïˆ as [Ïˆ]i, which is a measurable function
from (RdS ,BdS ) to (R,B) for each 1 â‰¤ i â‰¤ dP iff Ïˆ is measurable. Define
L1(RdS ,BdS ,PÎ¸) as the collection of all the measurable vector functions Ïˆ from
(RdS ,BdS ) to (RdP ,BdP ), such that
âˆ« âˆ£âˆ£[Ïˆ]iâˆ£âˆ£dPÎ¸ < +âˆ, for 1 â‰¤ i â‰¤ dP , equiva-
lently, [Ïˆ]i âˆˆ L1(RdS ,BdS ,PÎ¸), for all 1 â‰¤ i â‰¤ dP , so that L1(RdS ,BdS ,PÎ¸) =(
L1(RdS ,BdS ,PÎ¸)
)dP
.
Hence Ïˆ(X ) is a random variable from (â„¦,F ) to (RdP ,BdP ), since for
B. Cernuschi-FrÌÄ±as 3
B âˆˆ BdP we have
[
Ïˆ(X )
]âˆ’1
(B) = X âˆ’1
(
Ïˆâˆ’1(B)
)
, but B âˆˆ BdP so that
Ïˆâˆ’1(B) âˆˆ BdS and then X âˆ’1
(
Ïˆâˆ’1(B)
)
âˆˆ F .
Define the integral of a vector of functions as a vector whose elements are
the integrals of each function. Then, [10] p. 45:âˆ«
Ïˆ(X ) dPÎ¸ =
âˆ«
Ïˆ dPÎ¸
Note that the integral on the left is with respect to the probability space
(â„¦,F ,PÎ¸), while the integral on the right is with respect to the probability
space (RdS , BdS , PÎ¸). We will refer indistinctly to Ïˆ(X ) and Ïˆ as an estima-
tor, with the understanding that they refer to different probability spaces linked
by the previous equality of integrals.
For Ïˆ âˆˆ L1(RdS ,BdS ,PÎ¸) define the expectation of Ïˆ as EÎ¸ [Ïˆ] =âˆ«
Ïˆ(X ) dPÎ¸ =
âˆ«
Ïˆ dPÎ¸.
We assume that the random variable X is drawn from some specific prob-
ability measure (p. m.) PÎ¸T , with Î¸T âˆˆ Î˜, i. e. we will use the realization
of this random variable to obtain the estimator for g(Î¸T ). The random vector
Ïˆ(X ) is an unbiased estimator for g(Î¸), âˆ€Î¸ âˆˆ Î˜ , g : Î˜ â†’ RdP , if the integralâˆ«
Ïˆ(X ) dPÎ¸ is well defined, âˆ€Î¸ âˆˆ Î˜, and we have
âˆ«
Ïˆ(X ) dPÎ¸ =
âˆ«
Ïˆ dPÎ¸ =
g(Î¸), âˆ€Î¸ âˆˆ Î˜.
Then the first issue posed in the introduction may be formally stated as:
Problem 2.1 (Basic Problem). Given a function g : Î¸ â†’ RdP , defined for each
Î¸ âˆˆ Î˜, and a family of p.m.â€™s indexed by Î¸ âˆˆ Î˜, find an unbiased estimator,
i.e. find a function Ïˆ âˆˆ L1(RdS ,BdS ,PÎ¸), âˆ€Î¸ âˆˆ Î˜, such that
âˆ«
Ïˆ(X ) dPÎ¸ =âˆ«
Ïˆ dPÎ¸ = g(Î¸), for all Î¸ âˆˆ Î˜.
Define the integral of a matrix Î¨ of dimensions N Ã—M , N,M âˆˆ N, whose
elements belong to L1(RdS ,BdS ,PÎ¸), as a matrix whose elements are the in-
tegrals of the elements of Î¨, so that EÎ¸[Î¨(X )] =
âˆ«
Î¨(X ) dPÎ¸ =
âˆ«
Î¨ dPÎ¸.
For a measurable square integrable function f : (RdS ,BdS ,PÎ¸T ) â†’ (R,B), i.e.
f âˆˆ L2(RdS ,BdS ,PÎ¸T ),
Define L2(RdS ,BdS ,PÎ¸T ) as the collection of all the measurable functions u
from (RdS ,BdS ) to (RdP ,BdP ), such that
âˆ«
[u]
2
i dPÎ¸T < +âˆ, for 1 â‰¤ i â‰¤ dP ,
equivalently, [u]i âˆˆ L2(RdS ,BdS ,PÎ¸T ), for all 1 â‰¤ i â‰¤ dP . If the non-centered
second order moments of the components of the estimator Ïˆ exist for Î¸T , i.e.
[Ïˆ]i âˆˆ L2(RdS ,BdS ,PÎ¸T ), for 1 â‰¤ i â‰¤ dP , so that Ïˆ âˆˆ L2(RdS ,BdS ,PÎ¸T ) â‰¡(
L2(RdS ,BdS ,PÎ¸T )
)dP
, then, the first order moments of the components of the
estimator exist for Î¸T . Also, the correlations
âˆ«
[Ïˆ]i [Ïˆ]j dPÎ¸T , are well defined
and are finite for all i 6= j, 1 â‰¤ i, j â‰¤ dP , and using the Cauchy-Schwarz
inequality, we obtain
âˆ£âˆ£âˆ£ âˆ« [Ïˆ]i [Ïˆ]j dPÎ¸T âˆ£âˆ£âˆ£ â‰¤ â€–[Ïˆ]iâ€–L2 âˆ¥âˆ¥âˆ¥[Ïˆ]jâˆ¥âˆ¥âˆ¥L2 . Additionally as-
sume Ïˆ(X ) is unbiased âˆ€Î¸ âˆˆ Î˜, then the covariance matrix of Ïˆ(X ) exists for
Î¸T , and we have CovÎ¸T (Ïˆ) =
âˆ« [
(Ïˆ(X )âˆ’ g(Î¸T )) (Ïˆ(X )âˆ’ g(Î¸T ))T
]
dPÎ¸T =
EÎ¸T
[
(Ïˆ âˆ’ g(Î¸T )) (Ïˆ âˆ’ g(Î¸T ))T
]
= EÎ¸T
[
Ïˆ ÏˆT
]
âˆ’ g(Î¸T ) g(Î¸T )T .
B. Cernuschi-FrÌÄ±as 4
In the same direction of [2], with s = r = 2, instead of the general Problem
2.1, we pose the problem in terms of estimators with finite covariance matrix at
Î¸T :
Problem 2.2 (Finite Covariance Problem). Given a function g : Î¸ â†’ RdP ,
defined for each Î¸ âˆˆ Î˜, and a family of p.m.â€™s indexed by Î¸ âˆˆ Î˜, find a func-
tion Ïˆ âˆˆ L2(RdS ,BdS ,PÎ¸T ), with Ïˆ âˆˆ L1(RdS ,BdS ,PÎ¸), âˆ€Î¸ âˆˆ Î˜, such thatâˆ«
Ïˆ dPÎ¸ = g(Î¸), for all Î¸ âˆˆ Î˜. If there are several solutions find, if possible, a
solution with minimal covariance matrix at Î¸T .
2.2 Centered definitions
Define Ï• = Ïˆ âˆ’ g(Î¸T ), and h(Î¸) = g(Î¸) âˆ’ g(Î¸T ) so that h(Î¸T ) = 0. If Ïˆ is
unbiased, then, since
âˆ«
Ïˆ dPÎ¸ = g(Î¸) and
âˆ«
g(Î¸T ) dPÎ¸ = g(Î¸T ), for all Î¸ âˆˆ Î˜,
then,
âˆ«
(Ïˆ âˆ’ g(Î¸T )) dPÎ¸ = g(Î¸) âˆ’ g(Î¸T ), for all Î¸ âˆˆ Î˜, so that
âˆ«
Ï• dPÎ¸ =
h(Î¸) âˆ€Î¸ âˆˆ Î˜, and CovÎ¸T (Ïˆ) = EÎ¸T
[
Ï• Ï•T
]
. Also, if Ïˆ is unbiased, then, since
h(Î¸T ) = 0, then EÎ¸T[Ï•] =
âˆ«
Ï• dPÎ¸T = 0.
2.3 Barankin formulation: basic hypothesis
Following Barankin we will introduce some simple additional hypothesis re-
sumed in Barankinâ€™s Postulate in [2] p. 481.
Hypothesis 2.1. The set Î˜ is an arbitrary index set with no conditions on its
structure, [2] p. 477, and B is a collection of probability measures PÎ¸ for the
measurable space (â„¦,F ), i.e. B =
{
PÎ¸ : Î¸ âˆˆ Î˜
}
as in [2], p. 477. The random
variable X : (â„¦,F ) â†’ (RdS ,BdS ) is drawn from the probability measure (p.
m.) PÎ¸T , with Î¸T âˆˆ Î˜. Assume that for each Î¸ âˆˆ Î˜ the p.m. PÎ¸ is absolutely
continuous with respect to PÎ¸T , i.e PÎ¸ << PÎ¸T , with Î¸T âˆˆ Î˜.
Lemma 2.1. If Hypothesis 2.1 is true then for each Î¸ âˆˆ Î˜ the p.m. PÎ¸ is
absolutely continuous with respect to PÎ¸T , i.e. PÎ¸ << PÎ¸T .
Proof. Assume B âˆˆ BdP is such that PÎ¸T (B) = 0, then since PÎ¸T (B) =
PÎ¸T (X
âˆ’1(B)), we obtain PÎ¸T (X
âˆ’1(B)) = 0. But PÎ¸ << PÎ¸T , hence
PÎ¸(X âˆ’1(B)) = 0. Since PÎ¸(B) = PÎ¸(X âˆ’1(B)), then PÎ¸(B) = 0.
Observation 2.1. In the case in which every index Î¸ âˆˆ Î˜ is a possible candidate
for Î¸T , then, Hypothesis 2.1 should require that for each Î¸1 âˆˆ Î˜ the p.m. PÎ¸1
should be absolutely continuous with respect to each other p.m. PÎ¸2 with
Î¸2 âˆˆ Î˜, and then PÎ¸1 << PÎ¸2 for all Î¸1, Î¸2 âˆˆ Î˜.
As a consequence of the previous hypothesis and lemma, the Radon-Nykodim
derivatives dPÎ¸/dPÎ¸T and dPÎ¸/dPÎ¸T exist for all Î¸ âˆˆ Î˜, [15] p. 315.
Definition 2.1. Define Ï€(Î¸) = dPÎ¸/dPÎ¸T , with Ï€(Î¸) â‰¡ Ï€Î¸(x, Î¸T ), x âˆˆ RdS , so
that dPÎ¸/dPÎ¸T = Ï€Î¸(X , Î¸T ). Define B0 = {Ï€(Î¸) : Î¸ âˆˆ Î˜}, see [2], p. 481.
We have Ï€(Î¸) â‰¥ 0 w.p. 1, for all Î¸ âˆˆ Î˜, [15] p. 315, Ï€(Î¸T ) = 1 w.p. 1, andâˆ«
Ï€Î¸(X , Î¸T )dPÎ¸T =
âˆ« (
dPÎ¸/dPÎ¸T
)
dPÎ¸T =
âˆ«
dPÎ¸ = 1, for all Î¸ âˆˆ Î˜.
B. Cernuschi-FrÌÄ±as 5
Hypothesis 2.2. 1. Assume that for each Î¸ there is one and only one Ï€(Î¸) âˆˆ
B0, i.e. the correspondence Ï€ : Î˜â†’ B0 is one-to-one.
2. There are at least two values Î¸1, Î¸2 âˆˆ Î˜, such that g(Î¸1) 6= g(Î¸2).
Observation 2.2. Item 1 avoids the identifiability problem, [12], pp. 58 and
191. Item 2 implies that we do not consider estimators which are constant with
probability 1: if it was Ïˆ = Î±0 w.p. 1 for some Î±0 âˆˆ RdP , then
âˆ«
Ïˆ(X ) dPÎ¸1 =âˆ«
Î±0 dPÎ¸1 = Î±0, similarly
âˆ«
Ïˆ(X ) dPÎ¸2 = Î±0, but since we assume that Ïˆ is
unbiased, it should be
âˆ«
Ïˆ(X ) dPÎ¸1 = g(Î¸1) and
âˆ«
Ïˆ(X ) dPÎ¸2 = g(Î¸2), and
then it should be g(Î¸1) = Î±0 = g(Î¸2), which is a contradiction. Additionally,
Hypothesis 2.2 implies that there exists at least a Î¸0 âˆˆ Î˜ such that g(Î¸0) 6= 0.
Nonetheless, see e.g. [2] p. 482 and [7] p. 2440, for some comments regarding
constant estimators.
2.4 Barankin postulate
The following hypothesis is Barankinâ€™s Postulate in [2], p. 481, for s = r = 2.
Hypothesis 2.3 (Barankin, [2], Postulate p. 481). Assume that
Ï€(Î¸) âˆˆ L2(RdS ,BdS ,PÎ¸T ) âˆ€Î¸ âˆˆ Î˜
equivalently B0 âŠ† L2(RdS ,BdS ,PÎ¸T ).
Observation 2.3. Since
âˆ«
Ï€(Î¸) dPÎ¸T =
âˆ« (
dPÎ¸/dPÎ¸T
)
dPÎ¸T =
âˆ«
dPÎ¸ = 1, for all
Î¸ âˆˆ Î˜, then â€–Ï€(Î¸)â€–L2 6= 0, for all Î¸ âˆˆ Î˜, equivalently â€–uâ€–L2 6= 0, for all u âˆˆ B0.
If not, â€–Ï€(Î¸)â€–L2 = 0 implies Ï€(Î¸) = 0 w.p. 1, and then
âˆ«
Ï€(Î¸) d PÎ¸T = 0,
contradiction. Additionally note, taking in account Hypothesis 2.2, that B0
has at least two elements.
Suppose Ïˆ âˆˆ L2(RdS ,BdS ,PÎ¸T ), since Ï€(Î¸) âˆˆ L2(RdS ,BdS ,PÎ¸T ) for all
Î¸ âˆˆ Î˜, then the integrals
âˆ«
Ïˆ Ï€(Î¸) dPÎ¸T are well defined for all Î¸ âˆˆ Î˜, and we
have all the equivalent forms:âˆ«
Ïˆ Ï€(Î¸) dPÎ¸T =
âˆ«
Ïˆ
dPÎ¸
dPÎ¸T
dPÎ¸T =
âˆ«
Ïˆ dPÎ¸
=
âˆ«
Ïˆ(X ) dPÎ¸ =
âˆ«
Ïˆ(X )
dPÎ¸
dPÎ¸T
dPÎ¸T = EÎ¸T[Ïˆ Ï€(Î¸)]
If Ïˆ âˆˆ L2(RdS ,BdS ,PÎ¸T ) is unbiased, then for Ï• = Ïˆ âˆ’ g(Î¸T ) we have
EÎ¸T[Ï• Ï€(Î¸)] =
âˆ«
Ï• Ï€(Î¸) dPÎ¸T = h(Î¸) âˆ€ Î¸ âˆˆ Î˜ (2.1)
The introduction of the functions Ï€ reduces the consideration of the multi-
ple probability spaces L2(RdS ,BdS ,PÎ¸), âˆ€Î¸ âˆˆ Î˜, to a single probability space
L2(RdS ,BdS ,PÎ¸T ).
B. Cernuschi-FrÌÄ±as 6
2.5 Probability density function form
Call Î» the Lebesgue measure for the measurable space (RdS , BdS ), i.e. the
measure that assigns to parallelepipeds in RdS the value given by the product
of the lengths of the edges of the parallelepiped in each direction. Alternatively
call dÎ» = dx, with x âˆˆ RdS . If in turn we have PÎ¸T << Î», i.e. the p.m. PÎ¸T
is absolutely continuous with respect to the Lebesgue measure, then, PÎ¸ <<
PÎ¸T << Î», so that PÎ¸ << Î», and then the Radon-Nykodim derivatives dPÎ¸/dÎ»
exist, for all Î¸ âˆˆ Î˜. These derivatives are the probability density functions (pdf)
pÎ¸ â‰¡ pÎ¸(x) â‰¡ dPÎ¸/dÎ» with x âˆˆ RdS . Since, [15] p. 328,
pÎ¸ =
dPÎ¸
dÎ»
=
dPÎ¸
dPÎ¸T
dPÎ¸T
dÎ»
= Ï€(Î¸) pÎ¸T Î»âˆ’ae
then, if Ïˆ is unbiased
g(Î¸) =
âˆ«
Ïˆ dPÎ¸ =
âˆ«
Ïˆ pÎ¸ dÎ» =
âˆ«
Ïˆ Ï€(Î¸) pÎ¸T dÎ»
2.6 The Main Problem
With all the previous considerations we may formalize the generalization to the
vector case of the Barankin formulation as:
Problem 2.3 (Main Problem). Given a function g : Î¸ â†’ RdP , defined for each
Î¸ âˆˆ Î˜, and a family of p.m.â€™s indexed by Î¸ âˆˆ Î˜, that satisfy the Hypothesis 2.1,
2.2, and 2.3, find a function Ïˆ âˆˆ L2(RdS ,BdS ,PÎ¸T ), such that
âˆ«
Ïˆ Ï€(Î¸) dPÎ¸T =
g(Î¸), âˆ€Î¸ âˆˆ Î˜. If there are several solutions find, if possible, a solution with
minimal covariance matrix at Î¸T .
The solution to this problem is given below in Theorem 7.3.
3 Matrix bound
For a vector a in a finite vector space denote [a]i the i-th component of the
vector. For a matrix A, define [A]i as the i-th column of the matrix and [A]i,j
as i-th, j-th element of the matrix. We have [A]i,j =
[
[A]j
]
i
. Denote AT the
transpose of the matrix A, Det(A) the determinant of A, and Tr [A] the trace
of A. A square symmetric real matrix A âˆˆ RNÃ—N is a symmetric non-negative
definite (s.n.n.d.) matrix iff, xTAx â‰¥ 0, for all x âˆˆ RN . A real s.n.n.d. matrix
A âˆˆ RNÃ—N is a symmetric positive definite (s.p.d.) matrix if Det(A) 6= 0, iff
xTAx > 0, for all x 6= 0. Two s.n.n.d. matrices A âˆˆ RNÃ—N and B âˆˆ RNÃ—N are
comparable in the LoÌˆwner partial order, [24] p. 166, if either Aâˆ’ B is s.n.n.d.
and then A â‰¥ B, or if B âˆ’ A is s.n.n.d. and then B â‰¥ A, else, they are not
comparable. For A, B and C s.n.n.d of dimensions N Ã—N , then if A â‰¥ B and
B â‰¥ C then A â‰¥ C, and if A â‰¥ B and B â‰¥ A, then A = B, see e.g. [7]
Lemma 3 p. 2444. If A âˆˆ RNÃ—N is s.p.d., and S âˆˆ RNÃ—M is arbitrary, such
B. Cernuschi-FrÌÄ±as 7
that STAS = 0, then S = 0, see e.g. [7] Lemma 2 p. 2444. For A âˆˆ RNÃ—N
denote the Frobenius norm as â€–Aâ€–F =
(
Tr(AAT )
)1/2
.
The following lemma is a variant of the information inequality [25] p. 172,
[13] Lemma 1 p. 1288, [19] pp. 326â€“328.
Lemma 3.1. Let (X,X, Âµ) be an arbitrary measure space. Let L2(X,X, Âµ) be
the collection of all the measurable square integrable real valued functions from
X to R. Let dÎ³ , dÏ, dA âˆˆ N, Î³ âˆˆ (L2(X,X, Âµ))dÎ³ , Ï âˆˆ (L2(X,X, Âµ))dÏ , and A âˆˆ
RdAÃ—dÏ . Call F =
âˆ«
Î³ ÏT dÂµ, F âˆˆ RdÎ³Ã—dÏ , and B =
âˆ«
Ï ÏT dÂµ, B âˆˆ RdÏÃ—dÏ .
If Det
(
A B AT
)
6= 0, then
âˆ«
Î³ Î³T dÂµ â‰¥ F AT
(
A B AT
)âˆ’1
A FT , with
equality if and only if there exists a matrix Î›0 âˆˆ RdÎ³Ã—dA such that Î³ = Î›0 A Ï
Âµ-almost-everywhere (Âµ-ae), and in that case, it is Î›0 = F A
T
(
A B AT
)âˆ’1
.
Proof. For each Î› âˆˆ RdÎ³Ã—dA , let M(Î›) =
âˆ«
(Î³ âˆ’ Î›AÏ) (Î³ âˆ’ Î›AÏ)T dÂµ, M(Î›) âˆˆ
RdÎ³Ã—dÎ³ . Then M(Î›) is s.n.n.d. for all Î› âˆˆ RdÎ³Ã—dA . We have M(Î›) =âˆ«
Î³ Î³T dÂµ âˆ’ F AT Î›T âˆ’ Î› A FT + Î› A B AT Î›T . By assumption
Det
(
A B AT
)
6= 0, so that the matrix A B AT is invertible. Define Î›0 =
F AT
(
A B AT
)âˆ’1
, then, M(Î›0) =
âˆ«
Î³ Î³T dÂµâˆ’ F AT
(
A B AT
)âˆ’1
A FT â‰¥ 0
so that
âˆ«
Î³ Î³T dÂµ â‰¥ F AT
(
A B AT
)âˆ’1
A FT , and there is equality iff
M(Î›0) = 0. From the definition of M(Î›), if there exists Î›? âˆˆ RdÎ³Ã—dA such
that Î³ = Î›? A Ï Âµ-ae, then M(Î›?) = 0. In that case it will be
âˆ«
Î³ ÏTdÂµ =
Î›? A
âˆ«
Ï ÏTdÂµ, so that F = Î›? A B, and then F AT = Î›? A B AT .
Since by hypothesis A B AT is invertible, then Î›? = F AT
(
A B AT
)âˆ’1
=
Î›0 so that M(Î›0) = M(Î›
?) = 0, and then we obtain the equality. Con-
versely, if
âˆ«
Î³ Î³T dÂµ = F AT
(
A B AT
)âˆ’1
A FT , take Î›0 âˆˆ RdÎ³Ã—dA , as
Î›0 = F A
T
(
A B AT
)âˆ’1
, so that by the definition of M(Î›) it results M(Î›0) =âˆ«
(Î³ âˆ’ Î›0 A Ï) (Î³ âˆ’ Î›0 A Ï)T dÂµ =
âˆ«
Î³ Î³T dÂµâˆ’ F AT
(
A B AT
)âˆ’1
A FT = 0.
Hence Tr (M(Î›0)) = Tr
(âˆ«
(Î³ âˆ’ Î›0 A Ï) (Î³ âˆ’ Î›0 A Ï)T dÂµ
)
=âˆ‘dÎ³
i=1
âˆ«
[Î³ âˆ’ Î›0 A Ï]2i dÂµ = 0, and then Î³ = Î›0 A Ï Âµ-ae.
The following definition specifies all the elements required in the proposed
linear matrix inequality (LMI) generalized Barankin bound.
Definition 3.1. Given arbitrary dM âˆˆ N and dA âˆˆ N, an arbitrary real matrix
A of dimensions dA Ã— dM , A âˆˆ RdAÃ—dM , and arbitrary indexes Î¸i âˆˆ Î˜, for
1 â‰¤ i â‰¤ dM , define Ï„ T = (Î¸1, Î¸2, . . . , Î¸dM ), Ï„ âˆˆ Î˜dM , and define the quad-tuple
q, as q = (dM , dA, A,Ï„ ). Define h(Î¸) = g(Î¸)âˆ’ g(Î¸T ).
Define Î²T (Ï„ ) = (Ï€(Î¸1), Ï€(Î¸2), . . . , Ï€(Î¸dM )), i.e. Î²(Ï„ ) âˆˆ B0
dM , define the dP Ã—
dM real matrix G(Ï„ ) as G(Ï„ ) =
(
g(Î¸1) âˆ’ g(Î¸T ), g(Î¸2) âˆ’ g(Î¸T ), . . . , g(Î¸dM ) âˆ’
g(Î¸T )
)
=
(
h(Î¸1), h(Î¸2), . . . , h(Î¸dM )
)
, and define the dM Ã— dM real matrix
B(Ï„ ) as B(Ï„ ) = E
[
Î²(Ï„ ) Î²T (Ï„ )
]
.
B. Cernuschi-FrÌÄ±as 8
Define CA as the collection of all the quad-tuples q with Det
(
A B(Ï„ ) AT )
)
6= 0.,
i.e.
CA =
{
q : âˆ€ dM âˆˆ N,âˆ€ dA âˆˆ N,âˆ€ A âˆˆ RdAÃ—dM ,
âˆ€ Ï„ âˆˆ Î˜dM , with Det
(
A B(Ï„ ) AT )
)
6= 0
}
.
Definition 3.2. Call Ug the family of all the finite covariance at Î¸T unbiased
estimators of g(Î¸), for all Î¸ âˆˆ Î˜, for Problem 2.3. Define WA as the collection
of matrices of the form:
W (q) = G(Ï„ ) AT
(
A B(Ï„ ) AT
)âˆ’1
A GT (Ï„ ) âˆ€q âˆˆ CA
i.e. âˆ€dM âˆˆ N, âˆ€dA âˆˆ N, âˆ€A âˆˆ RdAÃ—dM , âˆ€Ï„ âˆˆ Î˜dM , with Det
(
A B(Ï„ ) AT )
)
6= 0,
with G(Ï„ ) and B(Ï„ ) as in Definition 3.1. Hence WA = {W (q) : q âˆˆ CA}. The
matrices W (q) will be called the Barankin covariance lower bound matrices for
Problem 2.3.
Let S(B0) be the linear span of B0, i.e. S(B0) = {u âˆˆ L2(RdS ,BdS ,PÎ¸T ) : u =âˆ‘dM
i=1 ai Ï€i w.p. 1, âˆ€dM âˆˆ N,âˆ€ ai âˆˆ R for 1 â‰¤ i â‰¤ dM , âˆ€ Ï€i âˆˆ B0 for 1 â‰¤ i â‰¤
dM}.
The following theorem gives the first half of the Barankin vector bound.
Theorem 3.1. If for Problem 2.3 there exists a finite covariance at Î¸T unbiased
estimator Ïˆ(X ) âˆˆ Ug for g(Î¸), âˆ€Î¸ âˆˆ Î˜, then, see Definition 3.1,
CovÎ¸T (Ïˆ) â‰¥ G(Ï„ )AT
(
AB(Ï„ )AT
)âˆ’1
AGT (Ï„ ) âˆ€q âˆˆ CA (3.1)
i.e. (3.1) is true for the set of conditions CA: âˆ€ dM âˆˆ N, âˆ€dA âˆˆ N, âˆ€ A âˆˆ
RdAÃ—dM , âˆ€ Ï„ âˆˆ Î˜dM , with Det
(
A B(Ï„ ) AT
)
6= 0. There is equality in (3.1) for
some Ïˆâˆ— âˆˆ Ug and some qâˆ— = (dâˆ—M , dâˆ—A, Aâˆ—, Ï„ âˆ—), qâˆ— âˆˆ CA, if and only if there
exists a matrix Î›âˆ— âˆˆ RdPÃ—dA such that Ï•âˆ— = Ïˆâˆ— âˆ’ g(Î¸T ) = Î›âˆ— Aâˆ— Î²(Ï„ âˆ—) w.p. 1,
if and only if each component [Ï•âˆ—]i is a linear combination of elements in B0
w.p. 1 for 1 â‰¤ i â‰¤ dP , i.e. Ï•âˆ— = Ïˆâˆ— âˆ’ g(Î¸T ) âˆˆ
(
S(B0)
)dP
, see Definition 3.2.
Proof. The proof will follow from Lemma 3.1. Let Ïˆ âˆˆ Ug be an arbitrary
finite covariance at Î¸T unbiased estimator for Problem 2.3. Take an arbitrary
dM âˆˆ N, and an arbitrary Ï„ âˆˆ Î˜dM , see Definition 3.1. Since Ïˆ is unbiased, see
(2.1),
EÎ¸T
[
Ï• Î²T (Ï„ )
]
=
(
EÎ¸T[Ï• Ï€(Î˜1)] , . . . ,EÎ¸T[ Ï• Ï€(Î˜dM )]
)
=
(âˆ«
Ï• Ï€(Î˜1) dPÎ¸T , . . . ,
âˆ«
Ï• Ï€(Î˜dM ) dPÎ¸T
)
=
(âˆ«
Ï• dPÎ¸1 ,
âˆ«
Ï• dPÎ¸2 , . . . ,
âˆ«
Ï• dPÎ¸dM
)
=
(
h(Î¸1),h(Î¸2), . . . ,h(Î¸dM )
)
= G(Ï„ )
B. Cernuschi-FrÌÄ±as 9
then, G(Ï„ ) = EÎ¸T
[
Ï• Î²T (Ï„ )
]
= EÎ¸T
[
(Ïˆ âˆ’ g(Î¸T )) Î²T (Ï„ )
]
, see Definition 3.1, and
this is true for any unbiased estimator Ïˆ âˆˆ Ug. Additionally, we have,âˆ«
A Î²(Ï„ ) Î²T (Ï„ ) AT dPÎ¸T = A B(Ï„ ) AT , see Definition 3.1. Take an arbitrary
dA âˆˆ N and a matrix A âˆˆ RdAÃ—dM such that Det(A B(Ï„ )AT ) 6= 0 otherwise arbi-
trary. Then the result follows from Lemma 3.1 with Î³ = Ï•, Ï = Î²(Ï„ ), F = G(Ï„ ),
and B = B(Ï„ ). The first if and only if equality condition follows directly from
Lemma 3.1. As for the second equality condition, if there is equality in (3.1)
for some Ïˆâˆ— âˆˆ Ug and some qâˆ— = (dâˆ—M , dâˆ—A, Aâˆ—, Ï„ âˆ—) âˆˆ CA, then from Lemma 3.1,
there exists Î›âˆ— âˆˆ RdPÃ—dâˆ—A such that Ï•âˆ— = Ïˆâˆ— âˆ’ g(Î¸T ) = Î›âˆ— Aâˆ— Î²(Ï„ âˆ—) w.p. 1.
Since Î²(Ï„ âˆ—) âˆˆ Bd
âˆ—
M
0 , then each component [Ï•
âˆ—]i is a linear combination w.p. 1 of
elements in B0, for 1 â‰¤ i â‰¤ dP , i.e. Ï•âˆ— = Ïˆâˆ— âˆ’ g(Î¸T ) âˆˆ
(
S(B0)
)dP
. Conversely,
suppose thatÏˆâˆ— âˆˆ Ug, withÏ•âˆ— = Ïˆâˆ—âˆ’g(Î¸T ), and that each component [Ï•âˆ—]i is a
linear combination w.p. 1 of elements in B0, i.e. Ï•
âˆ— = Ïˆâˆ—âˆ’g(Î¸T ) âˆˆ
(
S(B0)
)dP
.
Since each [Ï•âˆ—]i âˆˆ S(B0) w.p. 1, then, there exist Mi âˆˆ N, ai âˆˆ RMi ,
and Ï„ i âˆˆ Î˜Mi , such that [Ï•âˆ—]i = aTi Î²(Ï„ i) w.p. 1 for 1 â‰¤ i â‰¤ dP . Define
MÎ± =
âˆ‘dP
i=1Mi, and Ï„
T
Î± =
(
Ï„ T1 , Ï„
T
2 , . . . , Ï„
T
dP
)
, Ï„ Î± âˆˆ Î˜MÎ± . Call Î²Î± = Î²(Ï„ Î±),
Î²Î± âˆˆ B0MÎ± . Define the real matrix AÎ± âˆˆ RdPÃ—MÎ± , as the block-diagonal ma-
trix AÎ± = Diag
(
aT1 ,a
T
2 , . . . ,a
T
dP
)
, where each block aTi is of dimension 1Ã— Mi,
for 1 â‰¤ i â‰¤ Mi, so that Ï•âˆ— = AÎ± Î²Î±. Starting with the second component of
Î²Î±, see Observation 2.3, delete the i-th component if it is a linear combination
w.p. 1 of the previous components. There will remain MÎ³ âˆˆ N elements, with
1 â‰¤ MÎ³ â‰¤ MÎ±, see Observation 2.3. Call Ï„ Î³ âˆˆ Î˜MÎ³ the non-deleted indexes of
the previous elimination procedure. Call Î²Î³ = Î²(Ï„ Î³), Î²Î³ âˆˆ B
MÎ³
0 , so that the
components of Î²Î³ are linearly independent w.p. 1. Then, there exists a real ma-
trix AÎ³ âˆˆ RMÎ±Ã—MÎ³ such that Î²Î± = AÎ³ Î²Î³ w.p. 1, and then Ï•âˆ— = AÎ± AÎ³ Î²Î³ w.p.
1. Define the quad-tuple qÎ³ = (MÎ³ ,MÎ³ , IÎ³ , Ï„ Î³), where IÎ³ is the identity matrix
of dimensions MÎ³ Ã— MÎ³ . Call BÎ³ = B(Ï„ Î³) = EÎ¸T
[
Î²Î³ Î²
T
Î³
]
, BÎ³ âˆˆ RMÎ³Ã—MÎ³ ,
so that Det(BÎ³) 6= 0. If not, there would exist Î± âˆˆ RMÎ³ , with Î± 6= 0,
such that Î±TBÎ³Î± = 0, but Î±
TBÎ³Î± = Î±
TEÎ¸T
[
Î²Î³ Î²
T
Î³
]
Î± = EÎ¸T
[
Î±TÎ²Î³ Î²
T
Î³Î±
]
=
EÎ¸T
[(
Î±TÎ²Î³
)2]
=
âˆ¥âˆ¥Î±TÎ²Î³âˆ¥âˆ¥L2 , and then it would be âˆ¥âˆ¥Î±TÎ²Î³âˆ¥âˆ¥L2 = 0, which is a con-
tradiction since the components of Î²Î³ are linearly independent w.p. 1. Hence,
Det(IÎ³BÎ³I
T
Î³ ) = Det(BÎ³) 6= 0, so that qÎ³ âˆˆ CA. Since G(Ï„ Î³) = EÎ¸T
[
Ï•âˆ— Î²TÎ³
]
=
EÎ¸T
[
AÎ± AÎ³ Î²Î³ Î²
T
Î³
]
= AÎ± AÎ³ EÎ¸T
(
Î²Î³ Î²
T
Î³
)
= AÎ± AÎ³ BÎ³ , then W (qÎ³) =
G(Ï„ Î³) B
âˆ’1
Î³ G
T (Ï„ Î³) = AÎ± AÎ³ BÎ³ B
âˆ’1
Î³ BÎ³ A
T
Î³ A
T
Î± = AÎ± AÎ³ BÎ³ A
T
Î³ A
T
Î± .
But CovÎ¸T (Ïˆ
âˆ—) = EÎ¸T
[
Ï•âˆ— (Ï•âˆ—)T
]
= EÎ¸T
[
AÎ± AÎ³ Î²Î³ (AÎ± AÎ³ Î²Î³)
T
]
=
AÎ±AÎ³ EÎ¸T
[
Î²Î³ Î²
T
Î³
]
ATÎ³ A
T
Î± = AÎ± AÎ³ BÎ³ A
T
Î³ A
T
Î± , and then CovÎ¸T (Ïˆ
âˆ—) = W (qÎ³).
The converse of this theorem, is given in Theorem 7.3, see Section 7.3.
Observation 3.1. The previous proof shows that if Ïˆ âˆˆ Ug is a finite covariance
unbiased estimator for Problem 2.3, then G(Ï„ ) =
(
h(Î¸1),h(Î¸2), Â· Â· Â· ,h(Î¸dM )
)
=
B. Cernuschi-FrÌÄ±as 10
EÎ¸T
[
Ï• Î²T (Ï„ )
]
, so that the value of
EÎ¸T
[
Ï• Î²T (Ï„ )
]
is independent of the estimator Ïˆ âˆˆ Ug as a consequence of the
unbiasedness of Ïˆ, see (2.1).
Observation 3.2. Theorem 3.1 shows that any other finite covariance at Î¸T un-
biased estimator will satisfy (3.1). Then, the covariance matrix of any unbiased
estimator in Ug is comparable, in the LoÌˆwner partial order, with any of the
matrices in WA. Hence:
CovÎ¸T (Ïˆ) â‰¥W âˆ€ Ïˆ âˆˆ Ug and âˆ€W âˆˆ WA
with equality if and only if Ï• = Ïˆâˆ’g(Î¸T ) âˆˆ
(
S(B0)
)dP
. The covariance matrices
of estimators in Ug need not be comparable between them, as well as, Barankin
bound matrices in WA need not be comparable between them.
4 Functional analysis setup
4.1 Definition of the operator LB0 : B0 â†’ RdP
From Hypothesis 2.3 we have B0 âŠ† L2(RdS ,BdS ,PÎ¸T ). The subset B0 is not
a linear subspace, since any Ï€ âˆˆ B0, is a Radon-Nykodim derivative of a p.m.
with respect to the p.m. PÎ¸T , then Ï€ â‰¥ 0 w.p. 1, [15], p. 315, with â€–Ï€â€–L2 6= 0,
see Observation 2.3, so that âˆ’Ï€ cannot belong to B0.
Let u0 be an arbitrary element in B0. To this particular element u0 âˆˆ B0
corresponds a unique Î¸0 âˆˆ Î˜, such that u0 â‰¡ Ï€(Î¸0), see Hypothesis 2.2, so that
Î¸0 = Ï€
âˆ’1(u0), and, to this index Î¸0 corresponds a unique well defined value
h(Î¸0) = g(Î¸0)âˆ’ g(Î¸T ) âˆˆ RdP . Hence, to u0 âˆˆ B0 corresponds a unique element
h
(
Ï€âˆ’1(u0)
)
âˆˆ RdP which we define as LB0
(
u0
)
, so that LB0(u0) = h
(
Ï€âˆ’1(u0)
)
.
Hence,
LB0(Ï€(Î¸)) = h(Î¸) âˆ€Î¸ âˆˆ Î˜ (4.1)
equivalently LB0(u) = h
(
Ï€âˆ’1(u)
)
, for all u âˆˆ B0. Note that LB0(Ï€(Î¸T )) =
h(Î¸T ) = 0. Then, we may establish a direct relation from B0 to RdP , as an
operator LB0 from B0 to RdP , i.e. LB0 : B0 â†’ RdP . This operator is not
(without additional conditions) necessarily linear nor bounded. The operator
LB0 is completely defined by the collection of Radon-Nykodim derivatives in
B0, i.e. the elements Ï€(Î¸) âˆˆ B0, for all Î¸ âˆˆ Î˜, and the vectors g(Î¸) âˆˆ RdP ,
for all Î¸ âˆˆ Î˜, and does not depend on the existence or not of any unbiased
estimator, and if it exists, on whether it has finite covariance at Î¸T or not.
4.2 Barankin formulation
The key observation made by Barankin, [2], for dP = 1, where he considers B0 âŠ†
Lr(RdS ,BdS ,PÎ¸T ), r â‰¥ 1, is that if we are able to find an integral representation
of the operator LB0 , then the problem is solved.
In Barankin, [2], the answer is given by the Riesz Representation Theorem
which finds an element in the conjugate space Ï†0 âˆˆ Ls(RdS ,BdS ,PÎ¸T ), with
B. Cernuschi-FrÌÄ±as 11
1/s + 1/r = 1, such that LB0(u) =
âˆ«
Ï†0 u dPÎ¸T , âˆ€u âˆˆ B0, with minimum
s-norm, i.e. minimum s-th variance. In our case, we generalize to vector es-
timates, i.e. dP > 1, but we will only consider the case s = r = 2 which is
the traditional variance and covariance matrices case, which is the most impor-
tant in applications. To solve the problem the idea is to generalize the Riesz
representation theorem to the vector case. The Riesz representation theorem
requires that the represented functional be defined from a linear space to the
reals. Since B0 is not a linear subspace, Barankin, see [1], pp. 479-480, extends
the operator LB0 to a linear operator over the whole space, using indirectly
the Hahn-Banach theorem, invoking a condition first used by Riesz and gener-
alized by Helly as exposed in [1] footnote in p. 56, see also [18]. In the next
sub-section we generalize the Helly-Riesz-Banach condition to handle the vector
case. In Section 5 we generalize the Riesz representation theorem to the vector
case without requiring the Hahn-Banach theorem, and in Section 6 we apply
these results to solve Problem 2.3.
4.3 Vector generalized Barankin hypothesis: Helly, Riesz,
Banach,Barankin (HRBB)
The following is the generalization of the hypothesis in [2], pp. 480 and 483â€“484,
see also [1], Theorems 4 and 5 pp. 55â€“57. This condition will be called here the
HRBB condition for Helly, Riesz, Banach, Barankin. For u âˆˆ L2(RdS ,BdS ,PÎ¸T ),
define the semi-norm â€–uâ€–L2 =
(âˆ«
u2 dPÎ¸T
)1/2
, and call â€–xâ€–RdP the standard Eu-
clidean norm for x âˆˆ RdP .
Definition 4.1. (HRBB condition) The functions h(Î¸) = g(Î¸)âˆ’ g(Î¸T ), h(Î¸) âˆˆ
RdP , and Ï€(Î¸) âˆˆ B0, âˆ€Î¸ âˆˆ Î˜, satisfying the Hypothesis 2.1, 2.2, and 2.3, for
Problem 2.3, satisfy the HRBB condition iff: âˆƒKH âˆˆ R+, i.e. KH â‰¥ 0, such
that: âˆ¥âˆ¥âˆ¥âˆ¥âˆ¥
dMâˆ‘
i=1
ai h(Î¸i)
âˆ¥âˆ¥âˆ¥âˆ¥âˆ¥
RdP
â‰¤ KH
âˆ¥âˆ¥âˆ¥âˆ¥âˆ¥
dMâˆ‘
i=1
ai Ï€(Î¸i)
âˆ¥âˆ¥âˆ¥âˆ¥âˆ¥
L2
(4.2)
for all dM âˆˆ N, for all ai âˆˆ R, i = 1, 2, Â· Â· Â· , dM , for all Î¸i âˆˆ Î˜, i = 1, 2, Â· Â· Â· , dM .
5 Generalized Riesz representation theorem
Here a generalization is given of the Riesz Representation Theorem for Hilbert
spaces real functionals, see e.g. [3] p. 112, to operators from an arbitrary Hilbert
space H , separable or not, to the real finite dimensional vector space RdP , with
dP â‰¥ 1. The proof given here does not require the Hahn-Banach extension
theorem, and then, the non-denumerable Axiom of Choice is not required, or
some less stringent variant, [18]. The bound proposed in Hellyâ€™s theorem, [1]
pp. 55â€“56, is generalized, and will be called the operator OP-HRBB (Helly,
Riesz, Banach, Barankin) condition.
B. Cernuschi-FrÌÄ±as 12
5.1 The Theorem.
Let H denote an arbitrary Hilbert space with semi-inner product ã€ˆu, vã€‰H ,
âˆ€u, v âˆˆ H , and semi-norm â€–uâ€–H = ã€ˆu, uã€‰
1/2
H . If â€–uâ€–H = 0, then we say that
u = 0 in semi-norm, (i.s.n.). Equivalently u = 0 i.s.n. iff â€–uâ€–H = 0. Define
u = v i.s.n., iff â€–uâˆ’ vâ€–H = 0.
Theorem 5.1. Let H be an arbitrary Hilbert space. Let B0 be a non-empty
arbitrary subset of H , B0 6= âˆ…, B0 âŠ† H . Let LB0 be an operator from B0 to
RdP , LB0 : B0 â†’ RdP , such that there exists at least one u0 âˆˆ B0 for which
LB0(u0) 6= 0. Assume that the operator LB0 satisfies the following condition,
that will be called the operator HRBB condition (OP-HRBB): âˆƒKH âˆˆ R+, i.e.
KH â‰¥ 0, such that:
â€–
dMâˆ‘
i=1
ai LB0(ui) â€–RdP â‰¤ KH â€–
dMâˆ‘
i=1
ai ui â€–H (5.1)
for all dM âˆˆ N, for all ai âˆˆ R, i = 1, 2, Â· Â· Â· , dM , for all ui âˆˆ B0, i =
1, 2, Â· Â· Â· , dM .
Call C(B0) âŠ† H the minimal closed linear space containing B0, i.e. the
closed linear span of B0, [11] p. 11. Then:
1. The operator LB0 may be extended to a bounded linear operator LC from
C(B0) to RdP , LC : C(B0)â†’ RdP , with LC(u) = LB0(u), for all u âˆˆ B0.
2. The operator LC has the following representation: There exists dL âˆˆ N,
1 â‰¤ dL â‰¤ dP , and there exist orthonormal uÌ‚iâ€™s, uÌ‚i âˆˆ C(B0), for 1 â‰¤ i â‰¤
dL, such that:
LC(u) =
dLâˆ‘
i=1
ã€ˆu, uÌ‚iã€‰H LC(uÌ‚i) âˆ€u âˆˆ C(B0) (5.2)
Observation 5.1. The standard Riesz representation theorem, corresponds to
B0 = S(B0) = C(B0) = H , and dP = 1. In that case the operator LB0 is
taken as a bounded linear operator, so that the OP-HRBB condition is satisfied,
and then the conclusion is given by (5.2) with dL = 1.
5.2 Proof of the generalized Riesz representation theorem
5.2.1 Extension of the operator LB0 to the span of B0, LS : S(B0)â†’
RdP
This extension follows the exposition of Banach in [1] pp. 55â€“56. Assume
the OP-HRBB condition is true. Call S(B0) the linear span i.s.n. of B0, i.e.
S(B0) = {u âˆˆ H : u =
âˆ‘dM
i=1 ai Ï€i i.s.n., âˆ€dM âˆˆ N,âˆ€ ai âˆˆ R, for 1 â‰¤ i â‰¤
dM , âˆ€ Ï€i âˆˆ B0, for 1 â‰¤ i â‰¤ dM}. The span S(B0) is called [B0] in [2] p.
495. Clearly, S(B0) is a linear space. With the help of the OP-HRBB condition
B. Cernuschi-FrÌÄ±as 13
extend the operator LB0 : B0 â†’ RdP to an operator LS : S(B0) â†’ RdP by
the following procedure: for each u âˆˆ S(B0) there exist, dependent on each
u, dM âˆˆ N, aiâ€™s âˆˆ R, 1 â‰¤ i â‰¤ dM , Ï€iâ€™s âˆˆ B0, 1 â‰¤ i â‰¤ dM , such that u =âˆ‘dM
i=1 ai Ï€i i.s.n. Define LS(u) for u âˆˆ S(B0) as LS(u) =
âˆ‘dM
i=1 ai LB0(Ï€i). This
procedure gives a well defined value for LS(u), since for any other decomposition
of u =
âˆ‘dâ€²M
j=1 a
â€²
j Ï€
â€²
j i.s.n., resulting in L
â€²
S(u) =
âˆ‘dâ€²M
j=1 a
â€²
jLB0(Ï€
â€²
j), because of the
OP-HRBB condition we will have:
â€– LS(u)âˆ’ Lâ€²S(u) â€–RdP â‰¤ KH â€–
dMâˆ‘
i=1
aiÏ€i âˆ’
dâ€²Mâˆ‘
j=1
aâ€²jÏ€
â€²
j â€–H = 0
so that
âˆ‘dM
i=1 ai LB0(Ï€i) =
âˆ‘dâ€²M
j=1 a
â€²
j LB0(Ï€
â€²
j). The important result here is
that now S(B0), unlike B0, is a linear space, and that LS : S(B0) â†’ RdP is
a bounded linear operator with bound KH , i.e. â€– LS(u) â€–RdP â‰¤ KH â€– u â€–H ,
âˆ€u âˆˆ S(B0), and LS(u) = LB0(u), âˆ€u âˆˆ B0.
Observation 5.2. Barankin, [2] pp. 480 and 483â€“484, following [1] Theorems
2 and 4, p. 55, invokes the Hahn-Banach theorem, see e.g. [11] p. 78 or [1]
Theorem 1 p. 27, to extend the operator LS to the whole space. The Hahn-
Banach theorem requires the Axiom of Choice or some slightly less stringent
condition, see e.g. [18]. In [1] arbitrary Banach spaces are considered. The
fact that here we work with Hilbert spaces, permits us to avoid the use of the
Hahn-Banach theorem, and then, the non-denumerable Axiom of Choice is not
required.
5.2.2 Extension of the operator LS to the closure of the span of B0,
LC : C(B0)â†’ RdP
Define the closure of the span of B0 as C(B0) = Closure(S(B0)), i.e. C(B0) =
{u âˆˆH : âˆƒ (un)nâˆˆN with un âˆˆ S(B0) âˆ€n âˆˆ N, such that â€– un âˆ’ u â€–H â†’ 0}. It
is readily checked that C(B0) is a closed linear subspace of H . The set C(B0)
is called {B0} in [2], p. 494. Extend the operator LS : S(B0)â†’ RdP to an op-
erator LC : C(B0)â†’ RdP by continuity: Let u âˆˆ C(B0), then there exists a se-
quence (un)nâˆˆN of elements un âˆˆ S(B0) such that â€– un âˆ’ u â€–Hâ†’ 0. Hence this
sequence is a Cauchy fundamental sequence, i.e. for each  > 0 there exists N()
such that âˆ€ n,m â‰¥ N() we have â€– unâˆ’um â€–H < . But since LS is a bounded
linear operator, then â€– LS(un)âˆ’LS(um) â€–RdP â‰¤ KH â€– un âˆ’ um â€–H < KH .
Then, (LS(un))nâˆˆN is a Cauchy fundamental sequence in the complete finite di-
mensional vector space RdP , [3] p. 23, hence there exists a limit in RdP . Call that
limit LC(u), so that â€– LS(un)âˆ’LC(u) â€–RdPâ†’ 0, and then LS(un)âˆ’LC(u)â†’ 0
component by component (c.b.c.), i.e. [LS(un)]i âˆ’ [LC(u)]i â†’ 0, for 1 â‰¤
i â‰¤ dP . The value LC(u) is well defined: assume that for some other se-
quence (uâ€²j)jâˆˆN of elements u
â€²
j âˆˆ S(B0) with â€– uâ€²j âˆ’ u â€–H â†’ 0, we ob-
tain using the previous procedure a limit Lâ€²C(u) for the sequence
(
LS(u
â€²
j)
)
jâˆˆN,
i.e. â€– LS(uâ€²j) âˆ’ Lâ€²C(u) â€–RdPâ†’ 0. We have: â€– LS(uâ€²j) âˆ’ LS(un) â€–RdP =
â€– LS(uâ€²j âˆ’ un) â€–RdP â‰¤ KH â€– uâ€²j âˆ’ un â€–H = KH â€– uâ€²j âˆ’ u âˆ’ (un âˆ’ u) â€–H
B. Cernuschi-FrÌÄ±as 14
â‰¤ KH (â€– uâ€²jâˆ’u â€–H + â€– unâˆ’u â€–H ). Then, â€– LC(u)âˆ’Lâ€²C(u) â€–RdP = â€– LC(u)âˆ’
LS(un) âˆ’ (Lâ€²C(u) âˆ’ LS(uâ€²j)) âˆ’(LS(uâ€²j)âˆ’ LS(un)) â€–RdP â‰¤
â€– LC(u) âˆ’ LS(un) â€–RdP + â€– Lâ€²C(u)âˆ’ LS(uâ€²j) â€–RdP + KH
(
â€– uâ€²j âˆ’ u â€–H +
â€– un âˆ’ u â€–H
)
, so that taking the limits n â†’ âˆ, and j â†’ âˆ, we obtain
LC(u) = L
â€²
C(u), so that the value LC(u) âˆˆ RdP is independent of the cho-
sen sequence. Hence LC(u) is a well defined operator from the closed lin-
ear subspace C(B0) âŠ† H to RdP . It is immediate to show that this op-
erator is linear and that LC(u) = LS(u), âˆ€u âˆˆ S(B0), and then LC(u) =
LS(u) = LB0(u), âˆ€u âˆˆ B0. Finally, letâ€™s show that the operator LC is bounded
with bound KH . Let u âˆˆ C(B0), and (un)nâˆˆN a sequence of elements un âˆˆ
S(B0) such that â€– u âˆ’ un â€–Hâ†’ 0, and then â€–LC(u)âˆ’ LS(un)â€–RdP â†’ 0.
Since
âˆ£âˆ£ â€–uâ€–H âˆ’ â€–unâ€–H âˆ£âˆ£ â‰¤ â€–uâˆ’ unâ€–H , then â€–unâ€–H â†’ â€–uâ€–H . Hence,
â€–LC(u)â€–RdP = â€–LC(u)âˆ’ LS(un) + LS(un)â€–RdP â‰¤ â€–LC(u)âˆ’ LS(un)â€–RdP +
â€–LS(un)â€–RdP â‰¤ â€–LC(u)âˆ’ LS(un)â€–RdP + KH â€–unâ€–H . Taking the limit
n â†’ âˆ, we obtain â€– LC(u) â€–RdP â‰¤ KH â€– u â€–H . Hence LC(u) is a bounded
linear operator from C(B0) to RdP , such that LC(u) = LS(u), âˆ€u âˆˆ S(B0), and
LC(u) = LS(u) = LB0(u), âˆ€u âˆˆ B0
5.2.3 Null space NL and topological complement N âŠ¥L of the operator
LC : C(B0)â†’ RdP
Define the kernel or null space of the operator LC as NL = {u âˆˆ C(B0) :
LC(u) = 0}. It is readily seen that NL is a closed linear subspace of C(B0),
NL âŠ† C(B0) âŠ†H . The orthogonal complement of NL with respect to C(B0)
is N âŠ¥L = {u âˆˆ C(B0) : ã€ˆu,wã€‰H = 0 âˆ€w âˆˆ NL}. Note that the orthogonal
complement of N âŠ¥L with respect to C(B0) is NL. It is readily shown that N
âŠ¥
L
is a closed linear subspace of C(B0), N âŠ¥L âŠ† C(B0) âŠ†H . Next, letâ€™s show that
C(B0) = N âŠ¥L âŠ•NL, i.e. for each u âˆˆ C(B0) there exist unique elements i.s.n.
v âˆˆ N âŠ¥L and w âˆˆ NL, such that u = v + w i.s.n. We have:
Fact 1, (Minimum Distance to a Convex Set, [11] p. 8) Let u âˆˆ C(B0),
since NL is a closed convex subset of the complete Hilbert vector space H ,
there exists a w(u) âˆˆ NL, such that â€– u âˆ’ w(u) â€–H â‰¤ â€– u âˆ’ z â€–H , âˆ€z âˆˆ NL,
and that element is unique i.s.n., i.e. if there exists another wâ€²(u) âˆˆ NL such
that â€– uâˆ’ wâ€²(u) â€–H â‰¤ â€– uâˆ’ z â€–H , âˆ€z âˆˆ NL, then â€– w(u)âˆ’ wâ€²(u) â€–H = 0.
Fact 2, (Principle of Orthogonality, [11] p. 9) Define v(u) = u âˆ’ w(u),
then v(u) is orthogonal to each of the elements in NL, so that v(u) âˆˆ N âŠ¥L .
Additionally, if Fact 2 is true then Fact 1 is true. The element w(u) is defined
as the orthogonal projection of u on the closed subspace NL denoted as w(u) =
Proj(u | NL), similarly v(u) = Proj(u | N âŠ¥L ).
Hence u âˆˆ C(B0) may be decomposed as u = v(u) + w(u) i.s.n. with
v(u) âˆˆ N âŠ¥L and w(u) âˆˆ NL. This decomposition is unique i.s.n.: if we also
may write u = vâ€²(u) + wâ€²(u) i.s.n., with vâ€²(u) âˆˆ N âŠ¥L and wâ€²(u) âˆˆ NL, then
v(u)âˆ’vâ€²(u) = wâ€²(u)âˆ’w(u) i.s.n. with v(u)âˆ’vâ€²(u) âˆˆ N âŠ¥L and wâ€²(u)âˆ’w(u) âˆˆ NL
by linearity. Then, â€– v(u) âˆ’ vâ€²(u) â€–2H = ã€ˆv(u)âˆ’ vâ€²(u), v(u)âˆ’ vâ€²(u)ã€‰H =
ã€ˆv(u)âˆ’ vâ€²(u), wâ€²(u)âˆ’ w(u)ã€‰H = 0. Similarly â€– w(u) âˆ’ wâ€²(u) â€–2H = 0. Hence
B. Cernuschi-FrÌÄ±as 15
NL and N âŠ¥L are topological complements, [11] p. 93, i.e. C(B0) = N
âŠ¥
L âŠ•NL.
5.2.4 Images of B0, S(B0), C(B0) and N âŠ¥L
The previous properties are valid if we replace the space RdP with an arbitrary
Banach space. The following properties depend strongly on the finite dimen-
sional character of RdP . The main property is that N âŠ¥L is a finite dimensional
sub-space of H as shown below.
Call I[B0] the image of the operator LB0 : B0 â†’ RdP , then, I[B0] âŠ†
RdP . Since RdP has dimension dP then any dP + 1 vectors in RdP are linearly
dependent, and there are dP linearly independent vectors that constitute a basis
for RdP , see e.g. [5] pp. 178-179. Since I[B0] âŠ† RdP then any dP + 1 vectors
in I[B0] are linearly dependent. Since by hypothesis there exists at least one
u0 âˆˆ B0 such that LB0(u0) 6= 0, then there exists dL âˆˆ N with 1 â‰¤ dL â‰¤ dP ,
such that any dL + 1 vectors in I[B0] are linearly dependent, and there are
dL linearly independent vectors LB0(Ï€Ì‚1), LB0(Ï€Ì‚2), Â· Â· Â· , LB0(Ï€Ì‚dL) that belong
to I[B0] with Ï€Ì‚i âˆˆ B0, for 1 â‰¤ i â‰¤ dL. Note that I[B0] is not necessarily a
linear subspace.
The elements Ï€Ì‚i âˆˆ B0 for 1 â‰¤ i â‰¤ dL, are linearly independent i.s.n.,
i.e. whenever there are real coefficients ai âˆˆ R for 1 â‰¤ i â‰¤ dL, for which we
have â€–
âˆ‘dL
i=1 ai Ï€Ì‚i â€–H = 0, then ai = 0 for 1 â‰¤ i â‰¤ dL. If not, there would
exist aiâ€™s, ai âˆˆ R for 1 â‰¤ i â‰¤ dL, not all null, such that â€–
âˆ‘dL
i=1 ai Ï€Ì‚i â€–H =
0, but then, because of the OP-HRBB condition â€–
âˆ‘dL
i=1 ai LB0(Ï€Ì‚i) â€–RdP â‰¤
KHâ€–
âˆ‘dL
i=1 ai Ï€Ì‚i â€–H , see (5.1), it would be â€–
âˆ‘dL
i=1 ai LB0(Ï€Ì‚i) â€–RdP = 0, iffâˆ‘dL
i=1 ai LB0(Ï€Ì‚i) = 0, but the LB0(Ï€Ì‚i)â€™s are l.i., so that it should be ai = 0,
1 â‰¤ i â‰¤ dL, which is a contradiction.
Next, decompose each Ï€Ì‚i as in the previous item 5.2.3, i.e. for 1 â‰¤ i â‰¤ dL,
Ï€Ì‚i = vÌ‚i + wÌ‚i i.s.n., where vÌ‚i = Proj(Ï€Ì‚i | N âŠ¥L ) and
wÌ‚i = Proj(Ï€Ì‚i | NL), so that vÌ‚i âˆˆ N âŠ¥L âŠ† C(B0) and wÌ‚i âˆˆ NL âŠ† C(B0). Note
that, even though Ï€Ì‚i âˆˆ B0, and then Ï€Ì‚i âˆˆ S(B0), in general it may happen
that vÌ‚i /âˆˆ S(B0) and wÌ‚i /âˆˆ S(B0). Since, see item 5.2.2, LB0(Ï€Ì‚i) = LC(Ï€Ì‚i) =
LC(vÌ‚i + wÌ‚i) = LC(vÌ‚i) + LC(wÌ‚i) = LC(vÌ‚i), then the vectors LC(vÌ‚i)â€™s are lin-
early independent. Hence, the elements vÌ‚iâ€™s are linearly independent i.s.n.: if
not, there would exist aiâ€™s, ai âˆˆ R for 1 â‰¤ i â‰¤ dL, not all null, such that
â€–
âˆ‘dL
i=1 ai vÌ‚i â€–H = 0. Then, since the extension LC is a bounded linear opera-
tor, see item 5.2.2, then â€–
âˆ‘dL
i=1 ai LC(vÌ‚i) â€–RdP â‰¤ KHâ€–
âˆ‘dL
i=1 ai vÌ‚i â€–H , so that it
would be â€–
âˆ‘dL
i=1 ai LC(vÌ‚i) â€–RdP = 0, iff
âˆ‘dL
i=1 ai LC(vÌ‚i) = 0. But the LC(vÌ‚i)â€™s
are l.i., so that it should be ai = 0, 1 â‰¤ i â‰¤ dL, which is a contradiction.
Since the vÌ‚iâ€™s are linearly independent i.s.n., and they all belong to N âŠ¥L ,
use the Gram-Schmidt procedure, see e.g. [5], p. 204, to obtain dL orthonormal
elements uÌ‚i âˆˆ N âŠ¥L âŠ† C(B0) âŠ† H , that span the same space than the vÌ‚iâ€™s, so
that â€–uÌ‚iâ€–H = 1, ã€ˆuÌ‚i, uÌ‚jã€‰H = 0 for i 6= j, and ã€ˆuÌ‚i, wã€‰H = 0, for 1 â‰¤ i â‰¤ dL and
âˆ€ w âˆˆ NL. Hence, each uÌ‚i is a linear combination i.s.n. of the vÌ‚iâ€™s, and since
this transformation is invertible, then each vÌ‚i is a linear transformation i.s.n. of
the uÌ‚iâ€™s. The vectors LC(uÌ‚i) are linearly independent: if not, there would exist
B. Cernuschi-FrÌÄ±as 16
aiâ€™s, ai âˆˆ R for 1 â‰¤ i â‰¤ dL, not all null, such that
âˆ‘dL
i=1 ai LC(uÌ‚i) = 0, but
then, LC(
âˆ‘dL
i=1 ai uÌ‚i) = 0, so that
âˆ‘dL
i=1 ai uÌ‚i âˆˆ NL. Then, for 1 â‰¤ k â‰¤ dL, we
have
âŒ©
uÌ‚k,
âˆ‘dL
i=1 ai uÌ‚i
âŒª
H
= 0. But
âŒ©
uÌ‚k,
âˆ‘dL
i=1 ai uÌ‚i
âŒª
H
=
âˆ‘dL
i=1 ai ã€ˆuÌ‚k, uÌ‚iã€‰H =
akâ€–uÌ‚kâ€–2H = ak, so that ak = 0 for 1 â‰¤ k â‰¤ dL, which is a contradiction.
Call IL the span of the linearly independent vectors LC(Ï€Ì‚i), for 1 â‰¤ i â‰¤ dL,
so that I[B0)] âŠ† IL. Since LC(Ï€Ì‚i) = LC(vÌ‚i), then IL is the span of the linearly
independent vectors LC(vÌ‚i), for 1 â‰¤ i â‰¤ dL. Since each vÌ‚i is a linear combi-
nation i.s.n. of the linearly independent i.s.n. elements uÌ‚i, then, since LC is a
linear operator, each vector LC(vÌ‚i) is a linear combination of the linearly inde-
pendent vectors LC(uÌ‚i) and vice-versa, and then IL is the span of the linearly
independent vectors LC(uÌ‚i), for 1 â‰¤ i â‰¤ dL.
Call I[S(B0)] the image of the operator LS : S(B0) â†’ RdP . Recall that
if u âˆˆ S(B0) then LS(u) = LC(u). Clearly, IL âŠ† I[S(B0)]. If u âˆˆ S(B0)
then u is a linear combination i.s.n. of a finite number of elements in B0,
and then the vector LC(u) âˆˆ I[S(B0)], is the same linear combination of the
corresponding finite number of vectors in I[B0]. But since each vector in I[B0]
is a linear combination of the vectors LC(uÌ‚i), for 1 â‰¤ i â‰¤ dL, then LC(u) is a
linear combination of the independent vectors LC(uÌ‚i), for 1 â‰¤ i â‰¤ dL, hence
LC(u) âˆˆ IL, so that I[S(B0)] = IL. If u âˆˆ S(B0), then since I[S(B0)] = IL,
there exist Î±i(u) âˆˆ R, 1 â‰¤ i â‰¤ dL, such that LC(u) =
âˆ‘dL
i=1 Î±i(u) LC(uÌ‚i).
Define w(u) = u âˆ’
âˆ‘dL
i=1 Î±i(u) uÌ‚i, then LC(w(u)) = 0, so that w(u) âˆˆ NL andâˆ‘dL
i=1 Î±i(u) uÌ‚i âˆˆ N âŠ¥L . Hence u âˆˆ S(B0) may be written as u =
âˆ‘dL
i=1 Î±i(u) uÌ‚i+
w(u) i.s.n.
Observation 5.3. Note that, whenever u âˆˆ C(B0) may be written as u =âˆ‘dL
i=1 Î±i(u) uÌ‚i + w(u), where w(u) âˆˆ NL and the uÌ‚iâ€™s, 1 â‰¤ i â‰¤ dL, are or-
thonormal elements in N âŠ¥L , then we have
â€–uâ€–2H =
dLâˆ‘
i=1
|Î±i(u)|2 + â€–w(u)â€–2H (5.3)
Call I[C(B0)] the image of the operator LC : C(B0) â†’ RdP . Since
I[S(B0)] âŠ† I[C(B0)] then IL âŠ† I[C(B0)]. Fix an arbitrary u âˆˆ C(B0), then
there exists a sequence (un)nâˆˆN of elements un âˆˆ S(B0) such that â€–unâˆ’uâ€–H â†’
0, and â€–LC(un) âˆ’ LC(u)â€–RdP â†’ 0. Since â€–un âˆ’ uâ€–H â†’ 0, then (un)nâˆˆN
is a Cauchy fundamental sequence in H . Since un âˆˆ S(B0), then, there
exist sequences (Î±i(un))nâˆˆN, with Î±i(un) âˆˆ R for 1 â‰¤ i â‰¤ dL, âˆ€n âˆˆ N,
and a sequence (w(un))nâˆˆN, with w(un) âˆˆ NL, âˆ€n âˆˆ N, such that un =âˆ‘dL
i=1 Î±i(un) uÌ‚i + w(un). Since (un)nâˆˆN is a Cauchy fundamental sequence
in H , with un âˆˆ S(B0) âŠ† C(B0), then, (5.3) shows that the sequences
(Î±i(un))nâˆˆN for 1 â‰¤ i â‰¤ dL are Cauchy fundamental sequences of real numbers,
and the sequence (w(un))nâˆˆN is a Cauchy fundamental sequence of elements
in NL âŠ† C(B0) âŠ† H . Since the reals are complete, there exist real numbers
ai âˆˆ R for which Î±i(un)â†’ ai for 1 â‰¤ i â‰¤ dL, and, since H is complete and NL
is closed, there exists an element Î· âˆˆ NL such that â€–w(un)âˆ’ Î·â€–H â†’ 0. Define
B. Cernuschi-FrÌÄ±as 17
uâ€² =
âˆ‘dL
i=1 ai uÌ‚i + Î·, then u
â€² âˆˆ C(B0). Then (5.3) shows that â€–un âˆ’ uâ€²â€–H â†’ 0.
Since â€–uâˆ’ uâ€²â€–H â‰¤ â€–uâˆ’ unâ€–H + â€–un âˆ’ uâ€²â€–H , taking the limit, we obtain
u = uâ€² i.s.n. Hence for each u âˆˆ C(B0) we have found real numbers Î±i(u) âˆˆ R,
for 1 â‰¤ i â‰¤ dL, and an element w(u) âˆˆ NL such that
u =
dLâˆ‘
i=1
Î±i(u) uÌ‚i + w(u) i.s.n. âˆ€u âˆˆ C(B0) (5.4)
Then, LC(u) =
âˆ‘dL
i=1 Î±i(u) LC(uÌ‚i), so that LC(u) âˆˆ IL, and then I[C(B0)] =
IL. Additionally, since for arbitrary u âˆˆ C(B0), from (5.4), we have
Proj(u | N âŠ¥L ) =
âˆ‘dL
i=1 Î±i(u) uÌ‚i i.s.n., then, u âˆˆ N âŠ¥L iff u =
âˆ‘dL
i=1 Î±i(u) uÌ‚i
i.s.n., and then N âŠ¥L is a finite dimension subspace, N
âŠ¥
L âŠ† C(B0) âŠ† H , with
dimension dL, even though H might be a non-separable space.
Hence, we have I[NL] = {0}, and I[B0] âŠ† IL = I[S(B0)] = I[C(B0)] =
I[N âŠ¥L ].
5.2.5 Generalized Riesz representation of the operator LC : C(B0)â†’
RdP
From (5.4), it is LC(u) =
âˆ‘dL
i=1 Î±i(u) LC(uÌ‚i), for all u âˆˆ C(B0). Since the uÌ‚iâ€™s
are orthonormal and perpendicular to w(u), then, ã€ˆu, uÌ‚kã€‰H =âŒ©âˆ‘dL
i=1 Î±i(u) uÌ‚i + w(u), uÌ‚k
âŒª
H
= Î±k(u). Hence
LC(u) =
dLâˆ‘
i=1
ã€ˆu, uÌ‚iã€‰H LC(uÌ‚i) âˆ€u âˆˆ C(B0)
see (5.2), which is the vector generalized Riesz representation for the extension
LC of an operator LB0 : B0 â†’ RdP , B0 âŠ† H , satisfying the OP-HRBB
condition.
6 Optimal estimator under the HRBB condition
The space L2(RdS ,BdS ,PÎ¸T ) is a Hilbert space, [15] p. 194, with semi-inner
product ã€ˆu1, u2ã€‰H = ã€ˆu1, u2ã€‰L2 =
âˆ«
u1 u2 dPÎ¸T , âˆ€u1, u2 âˆˆ L2(RdS ,BdS ,PÎ¸T ),
semi-norm â€–uâ€–H = â€–uâ€–L2 =
(âˆ«
u2 dPÎ¸T
)1/2
, and equality in semi-norm (i.s.n.)
given by equality with probability 1 (w.p. 1).
Lemma 6.1. If the HRBB condition holds for Problem 2.3, see Definition 4.1,
then there exists a finite covariance unbiased estimator ÏˆÌ‚c âˆˆ Ug.
Proof. Since L2(RdS ,BdS ,PÎ¸T ) is a Hilbert space, then we take the elements
of H as the functions in L2(RdS ,BdS ,PÎ¸T ). Define the operator LB0(u) =
h
(
Ï€âˆ’1(u)
)
, for all u âˆˆ B0, see Section 4.1. Since the HRBB condition holds for
B. Cernuschi-FrÌÄ±as 18
Problem 2.3, see (4.2), then the OP-HRBB condition holds, see (5.1), and then
we may apply Theorem 5.1. From (5.2) we obtain
LC(u) =
dLâˆ‘
i=1
ã€ˆu, uÌ‚iã€‰H LC(uÌ‚i) =
dLâˆ‘
i=1
LC(uÌ‚i)
âˆ«
u uÌ‚i dPÎ¸T (6.1)
=
âˆ«
u
[
dLâˆ‘
i=1
uÌ‚i LC(uÌ‚i)
]
dPÎ¸T âˆ€u âˆˆ C(B0)
where the uÌ‚iâ€™s are orthonormal, with uÌ‚i âˆˆ N âŠ¥L âŠ† C(B0), for 1 â‰¤ i â‰¤ dL. Note
the importance of working with finite dimensions dP and dL, with 1 â‰¤ dL â‰¤ dP ,
since this permits exchanging sums and integrals invoking elementary properties
of Lebesgue integrals. Define
Ï•Ì‚c =
dLâˆ‘
i=1
uÌ‚i LC(uÌ‚i) (6.2)
so that LC(u) =
âˆ«
Ï•Ì‚c u dPÎ¸T , âˆ€ u âˆˆ C(B0).
Since each LC(uÌ‚i) is some constant real vector, i.e. LC(uÌ‚i) âˆˆ RdP , for
1 â‰¤ i â‰¤ dL, and each uÌ‚i âˆˆ L2(RdS ,BdS ,PÎ¸T ), then each component of the
vector Ï•Ì‚c is square integrable, i.e. [Ï•Ì‚c]i âˆˆ L2(RdS ,BdS ,PÎ¸T ), equivalently Ï•Ì‚c âˆˆ
L2(RdS ,BdS ,PÎ¸T ). Then Ï•Ì‚c is a measurable function from L2(RdS ,BdS ,PÎ¸T )
to RdP , so that Ï•Ì‚c(X ) =
âˆ‘dL
i=1 uÌ‚i(X ) LC(uÌ‚i) is a random vector, Ï•Ì‚c(X ) :
â„¦ â†’ RdP , that does not depend on the sub-indexes Î¸ âˆˆ Î˜. Additionally since
Ï•Ì‚c âˆˆ L2(RdS ,BdS ,PÎ¸T ), then ÏˆÌ‚c = Ï•Ì‚c+g(Î¸T ) has finite covariance as previously
discussed in Section 2.1. Since LC(u) = LB0(u) if u âˆˆ B0, see Section 5.2.2,
and, for each u âˆˆ B0 there exists Î¸ âˆˆ Î˜ such that u = Ï€(Î¸), see Hypothesis
2.2, and LB0(Ï€(Î¸)) = h(Î¸), âˆ€Î¸ âˆˆ Î˜, see (4.1), then, using (6.1) and (6.2),
h(Î¸) = LB0(Ï€(Î¸)) = LC(Ï€(Î¸)) =
âˆ«
Ï•Ì‚c Ï€(Î¸) dPÎ¸T =
âˆ«
Ï•Ì‚c (dPÎ¸/dPÎ¸T ) dPÎ¸T =âˆ«
Ï•Ì‚c dPÎ¸ =
âˆ«
Ï•Ì‚c(X ) dPÎ¸ = EÎ¸ [Ï•Ì‚c], âˆ€Î¸ âˆˆ Î˜, see (2.1). Then, EÎ¸
[
ÏˆÌ‚c
]
=âˆ«
ÏˆÌ‚c dPÎ¸ =
âˆ«
ÏˆÌ‚c(X ) dPÎ¸ = g(Î¸), âˆ€Î¸ âˆˆ Î˜. Hence, ÏˆÌ‚c(X ) is unbiased for all
Î¸ âˆˆ Î˜, and then ÏˆÌ‚c âˆˆ Ug.
Definition 6.1. Define the HRBB estimator as ÏˆÌ‚c = Ï•Ì‚c + g(Î¸T ), where Ï•Ì‚c is
given by (6.2) as discussed in Lemma 6.1, so that ÏˆÌ‚c âˆˆ Ug.
Definition 6.2 (Barankin-efficient estimator). A finite covariance unbiased es-
timator ÏˆÌ‚ âˆˆ Ug for Problem 2.3, will be called Barankin-efficient, if CovÎ¸T (Ïˆ) â‰¥
CovÎ¸T (ÏˆÌ‚), for all Ïˆ âˆˆ Ug. Equivalently, ÏˆÌ‚ is a minimum-covariance unbiased
estimator for Problem 2.3.
Definition 6.3. Let W be a collection of real s.n.n.d. matrices of dimensions
N Ã— N . A s.n.n.d. matrix A âˆˆ RNÃ—N is an upper (lower) bound for W if
A â‰¥ W (A â‰¤ W ), âˆ€W âˆˆ W . Define, if it exists, the matrix-supreme (msup) of
the matrices in W , as a real s.n.n.d. matrix A of dimensions N Ã—N , such that
B. Cernuschi-FrÌÄ±as 19
A â‰¥W , âˆ€W âˆˆ W , and such that for each  âˆˆ R+,  > 0, there exists W () âˆˆ W
such that â€–A âˆ’W ()â€–F < . The notation will be A = msup
WâˆˆW
W . If A âˆˆ W ,
then A will be called the matrix-maximum of W .
Define L as the real matrix, L âˆˆ RdPÃ—dL , with columns [L]i = LC(uÌ‚i), for
1 â‰¤ i â‰¤ dL, and define uÌ‚T = (uÌ‚1, uÌ‚2, Â· Â· Â· , uÌ‚dL), so that Ï•Ì‚c = L uÌ‚, see (6.2). Since
the uÌ‚iâ€™s are orthonormal in L2(RdS ,BdS ,PÎ¸T ), then EÎ¸T
[
uÌ‚ uÌ‚T
]
= IdL , where IdL
is the identity matrix of dimensions dLÃ—dL. From (6.2), we have: CovÎ¸T (ÏˆÌ‚c) =
EÎ¸T
[
Ï•Ì‚c Ï•Ì‚
T
c
]
= EÎ¸T
[
L uÌ‚ (L uÌ‚)T
]
= L EÎ¸T
[
uÌ‚ uÌ‚T
]
LT = L IdL LT = L LT so
that
CovÎ¸T (ÏˆÌ‚c) = L LT =
dLâˆ‘
i=1
LC(uÌ‚i) LC(uÌ‚i)
T (6.3)
Theorem 6.1. If the HRBB condition holds for Problem 2.3, see Definition
4.1, then the HRBB estimator ÏˆÌ‚c, see Definition 6.1, is an unbiased Barankin-
efficient estimator, and CovÎ¸T
(
ÏˆÌ‚c
)
= msup
WâˆˆWA
WA.
Proof. The HRBB estimator ÏˆÌ‚c is unbiased and has finite covariance as a con-
sequence of Lemma 6.1. To show that it is Barankin-efficient letâ€™s consider the
following two cases.
1) All the uÌ‚iâ€™s belong to S(B0). Then, uÌ‚ âˆˆ
(
S(B0)
)dL
. Since Ï•Ì‚c = L uÌ‚,
then Ï•Ì‚c âˆˆ
(
S(B0)
)dP
, and then, see Theorem 3.1 and Observation 3.2, we
have equality in (3.1). More precisely, since each uÌ‚i âˆˆ S(B0), then, there
exist Mi âˆˆ N, ai âˆˆ RMi , and Ï„ i âˆˆ Î˜Mi , such that uÌ‚i = aTi Î²(Ï„ i) w.p. 1,
for 1 â‰¤ i â‰¤ dL. Define MÌ‚ =
âˆ‘dL
i=1Mi, and Ï„Ì‚
T
=
(
Ï„ T1 Â· Â· Â·Ï„ TdL
)
, Ï„Ì‚ âˆˆ Î˜MÌ‚ .
Call Î²Ì‚ = Î²(Ï„Ì‚ ), Î²Ì‚ âˆˆ B0MÌ‚ , so that Î²Ì‚
T
= Î²T (Ï„Ì‚ ) =
(
Î²T (Ï„ 1) Â· Â· Â·Î²T (Ï„ dL)
)
, and
BÌ‚ = EÎ¸T
[
Î²Ì‚ Î²Ì‚
T ]
, BÌ‚ âˆˆ RMÌ‚Ã—MÌ‚ . Define the real matrix AÌ‚ âˆˆ RdLÃ—MÌ‚ , as the
block-diagonal matrix AÌ‚ = Diag
(
aT1 ,a
T
2 , . . . ,a
T
dL
)
, where each block aTi is of
dimension 1 Ã— Mi, for 1 â‰¤ i â‰¤ Mi, so that uÌ‚ = AÌ‚ Î²Ì‚ w.p. 1. Since
the uÌ‚iâ€™s are orthonormal in L2(RdS ,BdS ,PÎ¸T ), we have EÎ¸T
[
uÌ‚ uÌ‚T
]
= IdL .
Since EÎ¸T
[
uÌ‚ uÌ‚T
]
= EÎ¸T
[
AÌ‚ Î²Ì‚
(
AÌ‚ Î²Ì‚
)T]
= AÌ‚ EÎ¸T
[
Î²Ì‚ Î²Ì‚
T ]
AÌ‚T = AÌ‚ BÌ‚ AÌ‚T , then
AÌ‚ BÌ‚ AÌ‚T = IdL , so that Det
(
AÌ‚ BÌ‚ AÌ‚T
)
= 1. Since ÏˆÌ‚c âˆˆ Ug is unbiased, then,
see Observation 3.1, G(Ï„Ì‚ ) = EÎ¸T
[
Ï•Ì‚c Î²
T (Ï„Ì‚ )
]
= EÎ¸T
[
L uÌ‚ Î²T (Ï„Ì‚ )
]
= L EÎ¸T
[
uÌ‚ Î²Ì‚
T ]
=
L EÎ¸T
[
AÌ‚ Î²Ì‚ Î²Ì‚
T ]
= LAÌ‚ EÎ¸T
[
Î²Ì‚ Î²Ì‚
T ]
= L AÌ‚ BÌ‚. Take qÌ‚ =
(
MÌ‚, dL, AÌ‚, Ï„Ì‚
)
, see
Definition 3.1, so that qÌ‚ âˆˆ CA since Det
(
AÌ‚ BÌ‚ AÌ‚T
)
= 1, and then W (qÌ‚) âˆˆ
WA. Hence W (qÌ‚) = G(Ï„Ì‚ ) AÌ‚T
(
AÌ‚ BÌ‚ AÌ‚T
)âˆ’1
AÌ‚ GT (Ï„Ì‚ ) = G(Ï„Ì‚ ) AÌ‚T AÌ‚ GT (Ï„Ì‚ ) =
L (AÌ‚ BÌ‚AÌ‚T )(AÌ‚ BÌ‚T AÌ‚T )LT = L LT = CovÎ¸T (ÏˆÌ‚c), see (6.3). Then CovÎ¸T (ÏˆÌ‚c) âˆˆ
B. Cernuschi-FrÌÄ±as 20
WA, and then, see (3.1) and Observation 3.2, W (qÌ‚) = CovÎ¸T (ÏˆÌ‚c) is a matrix-
maximum for the matrices W âˆˆ WA and a matrix-minimum for the covariances
of any unbiased estimator Ïˆ âˆˆ Ug, so that ÏˆÌ‚c is a minimal covariance unbiased
estimator, i.e. the unbiased HRBB estimator ÏˆÌ‚c is Barankin-efficient.
2) At least for one iâˆ—, 1 â‰¤ iâˆ— â‰¤ dL, we have that uÌ‚iâˆ— belongs to C(B0)
and uÌ‚iâˆ— /âˆˆ S(B0). Since each uÌ‚i belongs to C(B0), then there exist sequences
(sÌ‚i(m))mâˆˆN, for 1 â‰¤ i â‰¤ dL, with sÌ‚i(m) âˆˆ S(B0), 1 â‰¤ i â‰¤ dL,âˆ€m âˆˆ N, such that
lim
mâ†’âˆ
â€–uÌ‚i âˆ’ sÌ‚i(m)â€–L2 = 0, for 1 â‰¤ i â‰¤ dL. As before, for each sÌ‚i(m) âˆˆ S(B0),
there exist Mi(m) âˆˆ N, ai(m) âˆˆ RMi(m), and Ï„ i(m) âˆˆ Î˜Mi(m), such that
sÌ‚i(m) = a
T
i (m) Î²(Ï„ i(m)) w.p. 1, and limmâ†’âˆ
âˆ¥âˆ¥uÌ‚i âˆ’ aTi (m) Î² (Ï„ i(m))âˆ¥âˆ¥L2 = 0. De-
fine MÌ‚(m) =
âˆ‘dL
i=1Mi(m), define Ï„Ì‚
T
(m) =
(
Ï„ T1 (m) Â· Â· Â·Ï„ TdL(m)
)
,
Ï„Ì‚ (m) âˆˆ Î˜MÌ‚(m), and Î²Ì‚
T
m = Î²
T (Ï„Ì‚ (m)) =
(
Î²T (Ï„ 1(m)) Â· Â· Â·Î²T (Ï„ dL(m))
)
, Î²Ì‚m âˆˆ
B
MÌ‚(m)
0 . Define the real matrix AÌ‚(m) âˆˆ RdLÃ—MÌ‚(m), as the block-diagonal matrix
AÌ‚(m) = Diag
(
aT1 (m), . . . ,a
T
dL
(m)
)
, where each block aTi (m) is of dimension 1Ã—
Mi(m), for 1 â‰¤ i â‰¤ dL. Define sÌ‚T (m) = (sÌ‚1(m) Â· Â· Â· sÌ‚dL(m)), sÌ‚(m) âˆˆ
(
S(B0)
)dL
,
so that sÌ‚(m) = AÌ‚(m) Î²Ì‚m w.p. 1. Define S(m) = EÎ¸T
[
sÌ‚(m) sÌ‚T (m)
]
, S(m) âˆˆ
RdLÃ—dL . Then, S(m) = EÎ¸T
[
sÌ‚(m) sÌ‚T (m)
]
=
AÌ‚(m) EÎ¸T
[
Î²Ì‚mÎ²Ì‚
T
m
]
AÌ‚T (m). Call BÌ‚(m) = EÎ¸T
[
Î²Ì‚mÎ²Ì‚
T
m
]
, BÌ‚(m) âˆˆ RMÌ‚(m)Ã—MÌ‚(m), so
that S(m) = AÌ‚(m) BÌ‚(m) AÌ‚T (m). Since ÏˆÌ‚c is unbiased, see Observation 3.1, we
have G (Ï„Ì‚ (m)) = EÎ¸T
[
Ï•Ì‚c Î²
T (Ï„Ì‚ (m))
]
= EÎ¸T
[
L uÌ‚ Î²T (Ï„Ì‚ (m))
]
=
L EÎ¸T
[(
sÌ‚(m) + [uÌ‚âˆ’ sÌ‚(m)]
)
Î²Ì‚
T
m
]
= L EÎ¸T
[(
AÌ‚(m) Î²Ì‚m + [uÌ‚âˆ’ sÌ‚(m)]
)
Î²Ì‚
T
m
]
=
L AÌ‚(m)BÌ‚(m) + L EÎ¸T
[
(uÌ‚âˆ’ sÌ‚(m)) Î²Ì‚
T
m
]
.
Define qÌ‚(m) =
(
MÌ‚(m), dL, AÌ‚(m), Ï„Ì‚ (m)
)
, then, see Appendix Lemma A.1,
for m â‰¥ M0, we have Det
(
AÌ‚(m) BÌ‚(m) AÌ‚T (m)
)
6= 0, so that, for m â‰¥ M0,
qÌ‚(m) âˆˆ CA, and then W (qÌ‚(m)) âˆˆ WA. Then, after some algebra, for m â‰¥ M0
we obtain:
W (qÌ‚(m)) = G(Ï„Ì‚ (m)) AÌ‚T (m)(
AÌ‚(m) BÌ‚(m) AÌ‚T (m)
)âˆ’1
AÌ‚(m) GT (Ï„Ì‚ (m))
= L S(m) LT + L EÎ¸T
[
(uÌ‚âˆ’ sÌ‚(m)) sÌ‚T (m)
]
LT
+ L EÎ¸T
[
sÌ‚(m) (uÌ‚âˆ’ sÌ‚(m))T
]
LT
+ L EÎ¸T
[
(uÌ‚âˆ’ sÌ‚(m)) sÌ‚T (m)
] (
S(m)
)âˆ’1
EÎ¸T
[
sÌ‚(m) (uÌ‚âˆ’ sÌ‚(m))T
]
LT
so that, see Appendix Lemma A.1, W (qÌ‚(m)) â†’ L LT = CovÎ¸T (ÏˆÌ‚c), see (6.3),
component by component and then in Frobenius norm.
B. Cernuschi-FrÌÄ±as 21
Unlike the previous case, if for some iâˆ—, 1 â‰¤ iâˆ— â‰¤ dL, we have that uÌ‚iâˆ—
belongs to C(B0) and uÌ‚iâˆ— /âˆˆ S(B0), then CovÎ¸T (ÏˆÌ‚c) = L LT /âˆˆ WA. If not,
CovÎ¸T (ÏˆÌ‚c) âˆˆ WA, and then we have equality in (3.1), so that, see Theorem 3.1,
[Ï•Ì‚c]i âˆˆ S(B0), for each 1 â‰¤ i â‰¤ dL. But Ï•Ì‚c = LuÌ‚, so that LT Ï•Ì‚c = LTLuÌ‚, with
L =
(
LC(uÌ‚1) Â· Â· Â·LC(uÌ‚dL)
)
. Since the dL columns of L are linearly independent
then Det(LTL) 6= 0, if not there exists Î± âˆˆ RdP , Î± 6= 0, such that LTLÎ± = 0,
so that Î±TLTLÎ± = â€–LÎ±â€–2RdP = 0, and then LÎ± = 0, contradiction. Hence
uÌ‚ =
(
LTL
)âˆ’1LT Ï•Ì‚c, and then [uÌ‚]i âˆˆ S(B0), for each 1 â‰¤ i â‰¤ dL, contradiction.
Since âˆ€Ïˆ âˆˆ Ug, see Theorem 3.1, it is CovÎ¸T (Ïˆ) â‰¥W (qÌ‚(m)), then CovÎ¸T (Ïˆ)âˆ’
W (qÌ‚(m)) â‰¥ 0, so that, taking the limit, see Appendix Lemma A.2, we have
CovÎ¸T (Ïˆ) âˆ’ CovÎ¸T (ÏˆÌ‚c) â‰¥ 0, and then CovÎ¸T (Ïˆ) â‰¥ CovÎ¸T (ÏˆÌ‚c), âˆ€ Ïˆ âˆˆ Ug. Then,
even though CovÎ¸T (ÏˆÌ‚c) /âˆˆ WA, we have CovÎ¸T (Ïˆ) â‰¥ CovÎ¸T (ÏˆÌ‚c), âˆ€ Ïˆ âˆˆ Ug,
and, see Theorem 3.1, CovÎ¸T (ÏˆÌ‚c) â‰¥ W , âˆ€ W âˆˆ WA. Furthermore, as previ-
ously shown, there exists a sequence Wm âˆˆ WA, Wm â‰¡ W (qÌ‚(m)), such thatâˆ¥âˆ¥âˆ¥CovÎ¸T (ÏˆÌ‚c)âˆ’Wmâˆ¥âˆ¥âˆ¥
F
â†’ 0. Hence, though CovÎ¸T (ÏˆÌ‚c) is not a matrix-maximum
for WA, it is a matrix-supreme for WA, and CovÎ¸T (ÏˆÌ‚c) is a matrix-minimum for
all the covariances of the estimators in Ug, so that ÏˆÌ‚c is Barankin-efficient.
Observation 6.1. A key point in Theorem 6.1 is that if WA is bounded above,
then, the optimal covariance CovÎ¸T (ÏˆÌ‚c) may be obtained as the matrix-supreme,
see Definition 6.3, of the matrices W âˆˆ WA, see (3.1).
CovÎ¸T (ÏˆÌ‚c) = msup
qâˆˆCA
G(Ï„ ) AT
(
A B(Ï„ ) AT )
)âˆ’1
A GT (Ï„ )
and the matrix-supreme will be a matrix-maximum if and only if ÏˆÌ‚c âˆ’ g(Î¸T ) =
Ï•Ì‚c âˆˆ
(
S(B0)
)dP
.
7 LMI equivalent formulation
7.1 Equivalence of the LMI bound and the HRBB condi-
tion
The statement that the Barankin covariance lower bounds WA are bounded
above, is a disguised form of the HRBB condition, as a matter of fact the
converse is also true, see Lemma 7.1 and Theorem 7.1 below.
Lemma 7.1. If the Barankin covariance lower bounds WA are bounded above,
i.e. the collection WA is bounded, see Definition 3.2, then the HRBB condition
holds, see Definition 4.1.
Proof. Call BW the bound for WA, i.e.
BW â‰¥W (q) = G(Ï„ ) AT
(
A B(Ï„ ) AT )
)âˆ’1
A GT (Ï„ ) (7.1)
B. Cernuschi-FrÌÄ±as 22
for all q âˆˆ CA. Since this is true for matrices A of all sizes dA âˆˆ N for a given
dM âˆˆ N, A âˆˆ RdAÃ—dM , in particular is true when dA = 1, i.e. when A has
one single row. Call aT = (a1, a2, Â· Â· Â· , adM ) the single row, so that A = aT ,
with a âˆˆ RdM . Then, A B(Ï„ ) AT = aT B(Ï„ ) a = aT EÎ¸T
[
Î²(Ï„ ) Î²T (Ï„ )
]
a =
EÎ¸T
[
aT Î²(Ï„ ) Î²T (Ï„ ) a
]
= EÎ¸T
[(
aT Î²(Ï„ )
)2]
= EÎ¸T
[(âˆ‘dM
i=1 ai Ï€(Î¸i)
)2]
=âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai Ï€(Î¸i)âˆ¥âˆ¥âˆ¥2
L2
. Observe that aT B(Ï„ ) a is a non-negative scalar, i.e.
aT B(Ï„ ) a âˆˆ R+, and since we assumed that q âˆˆ CA then aT B(Ï„ ) a 6= 0,
as a matter of fact aT B(Ï„ ) a =
âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai Ï€(Î¸i)âˆ¥âˆ¥âˆ¥2
L2
> 0. On the other hand,
G(Ï„ ) AT = G(Ï„ ) a =
âˆ‘dM
i=1 ai h(Î¸i). Then, (7.1) takes the form
BW â‰¥
(âˆ‘dM
i=1 ai h(Î¸i)
)(âˆ‘dM
i=1 ai h(Î¸i)
)T
âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai Ï€(Î¸i)âˆ¥âˆ¥âˆ¥2
L2
Hence,
Tr [BW ] â‰¥
Tr
[(âˆ‘dM
i=1 ai h(Î¸i)
)(âˆ‘dM
i=1 ai h(Î¸i)
)T]
âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai Ï€(Î¸i)âˆ¥âˆ¥âˆ¥2
L2
Call KH = (Tr [BW ])
1/2
. Since, Tr
[(âˆ‘dM
i=1 ai h(Î¸i)
)(âˆ‘dM
i=1 ai h(Î¸i)
)T]
=âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai h(Î¸i)âˆ¥âˆ¥âˆ¥2RdP then, KH â‰¥ âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai h(Î¸i)âˆ¥âˆ¥âˆ¥RdP / âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai Ï€(Î¸i)âˆ¥âˆ¥âˆ¥L2 .
Hence
âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai h(Î¸i)âˆ¥âˆ¥âˆ¥RdP â‰¤ KH âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai Ï€(Î¸i)âˆ¥âˆ¥âˆ¥L2 , âˆ€dM âˆˆ N, âˆ€ai âˆˆ R,
1 â‰¤ i â‰¤ dM , âˆ€Î¸i âˆˆ Î˜, 1 â‰¤ i â‰¤ dM , such that
âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai Ï€(Î¸i)âˆ¥âˆ¥âˆ¥
L2
6= 0.
If
âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai Ï€(Î¸i)âˆ¥âˆ¥âˆ¥
L2
= 0, then
âˆ‘dM
i=1 ai Ï€(Î¸i) = 0 w.p. 1. Take an arbi-
trary uâˆ— âˆˆ B0, then, see Observation 2.3, â€–uâˆ—â€–L2 6= 0. Call Î¸
âˆ— = Ï€âˆ’1(uâˆ—).
Then
âˆ‘dM
i=1 ai Ï€(Î¸i) + (1/n) u
âˆ— = (1/n) uâˆ— w.p. 1, for all n âˆˆ N, so thatâˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai Ï€(Î¸i) + (1/n) uâˆ—âˆ¥âˆ¥âˆ¥
L2
= â€–(1/n) uâˆ—â€–L2 6= 0, for all n âˆˆ N. Hence
the previously obtained inequality applies,
âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 aih(Î¸i) + (1/n)h(Î¸âˆ—)âˆ¥âˆ¥âˆ¥RdP â‰¤
KH
âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 aiÏ€(Î¸i) + (1/n)uâˆ—âˆ¥âˆ¥âˆ¥
L2
= KH (1/n) â€–uâˆ—â€–L2 , so thatâˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai h(Î¸i) + (1/n) h(Î¸âˆ—)âˆ¥âˆ¥âˆ¥RdP â†’ 0, as n â†’ +âˆ. Sinceâˆ¥âˆ¥âˆ¥âˆ‘dMi=1 aih(Î¸i)âˆ¥âˆ¥âˆ¥RdP = âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 aih(Î¸i) + (1/n)h(Î¸âˆ—)âˆ’ (1/n) h(Î¸âˆ—)âˆ¥âˆ¥âˆ¥RdP â‰¤âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai h(Î¸i) + (1/n) h(Î¸âˆ—)âˆ¥âˆ¥âˆ¥RdP + (1/n) âˆ¥âˆ¥h(Î¸âˆ—)âˆ¥âˆ¥RdP then, taking the limit
nâ†’ +âˆ, it results
âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai h(Î¸i)âˆ¥âˆ¥âˆ¥RdP = 0.
B. Cernuschi-FrÌÄ±as 23
Hence
âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai h(Î¸i)âˆ¥âˆ¥âˆ¥RdP â‰¤ KH âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai Ï€(Î¸i)âˆ¥âˆ¥âˆ¥L2 , âˆ€dM âˆˆ N, âˆ€ai âˆˆ R,
1 â‰¤ i â‰¤ dM , âˆ€Î¸i âˆˆ Î˜, 1 â‰¤ i â‰¤ dM , such that
âˆ¥âˆ¥âˆ¥âˆ‘dMi=1 ai Ï€(Î¸i)âˆ¥âˆ¥âˆ¥
L2
â‰¥ 0, as required
by Definition 4.1, so that the HRBB condition holds.
Theorem 7.1. The HRBB condition holds, see Definition 4.1, if and only if
the collection WA is bounded, see Definition 3.2.
Proof. If HRBB holds, see Theorem 6.1, then there exists ÏˆÌ‚c âˆˆ Ug such that
ÏˆÌ‚c is Barankin-efficient, and then CovÎ¸T
(
ÏˆÌ‚c
)
â‰¥ W , âˆ€ W âˆˆ WA, so that WA is
bounded. The converse follows as a consequence of Lemma 7.1.
Lemma 7.2. If there exists a finite covariance unbiased estimator Ïˆ(X ) for
g(Î¸), âˆ€Î¸ âˆˆ Î˜ for Problem 2.3, then WA is bounded above, see Definition 3.2.
Proof. Since a finite covariance unbiased estimator Ïˆ exists, then (3.1) shows
that WA is bounded.
7.2 Other equivalent LMI bounds
One of the key ideas in Barankinâ€™s paper is the use of the free coefficients aiâ€™s,
see [2] p. 480, that here take the form of the matrices Aâ€™s. As discussed in [2],
and here below, the matrices A are not required for the determination of the
optimal matrix bound, but, they are most useful when one needs to compare the
Barankin bound with other bounds, such as Cramer-Rao, Bhattacharyya, etc.
For the scalar case see [2] Corollaries 5â€“1 p. 487 and 6â€“1 p. 488. For the vector
Cramer-Rao bound, compare the results here with e.g. [21] and references there.
Definition 7.1. Define the pair d = (dM , Ï„ ), where dM âˆˆ N, and Ï„ âˆˆ Î˜dM .
Define CB as the collection of all the pairs d with Det
(
B(Ï„ )
)
6= 0, with B(Ï„ ) as
in Definition 3.1, so that
CB =
{
d : âˆ€ dM âˆˆ N,âˆ€ Ï„ âˆˆ Î˜dM , with Det
(
B(Ï„ )
)
6= 0
}
Define WB as the collection of matrices
V (d) = G(Ï„ )
(
B(Ï„ )
)âˆ’1
GT (Ï„ ) âˆ€d âˆˆ CB
with G(Ï„ ) as in Definition 3.1. Equivalently WB =
{
V (d) : d âˆˆ CB
}
.
Define the function g(Î¸) as B0-compatible if whenever
âˆ‘dM
i=1 ai Ï€(Î¸i) = 0
w.p. 1, we have
âˆ‘dM
i=1 ai h(Î¸i) = 0, with dM âˆˆ N, ai âˆˆ R, Î¸i âˆˆ Î˜, for 1 â‰¤ i â‰¤ dM .
Note that if g(Î¸) is not B0-compatible then no unbiased estimator exists for
g(Î¸), âˆ€Î¸ âˆˆ Î˜, for Problems 2.1, 2.2 or 2.3. If g(Î¸) is B0-compatible, then for
Ï„ âˆˆ Î˜dM and a âˆˆ RdM , if aT Î²(Ï„ ) = 0, then G(Ï„ ) a = 0, and for A âˆˆ RdAÃ—dM , if
A Î²(Ï„ ) = 0, then G(Ï„ )AT = 0. Hence, if Ï„ 1 âˆˆ Î˜dA and we have A Î²(Ï„ ) = Î²(Ï„ 1),
then G(Ï„ )AT = G(Ï„ 1).
B. Cernuschi-FrÌÄ±as 24
Theorem 7.2. The collection WA, see Definition 3.2, is bounded above, if and
only if the collection WB is bounded above and g(Î¸) is B0-compatible.
Proof. Assume WA is bounded. Then, there exists a s.n.n.d. matrix B1 âˆˆ
RdPÃ—dP , such that B1 â‰¥ W (q), âˆ€q âˆˆ CA. Take an arbitrary dâ€² âˆˆ CB , with
dâ€² =
(
dâ€²M , Ï„
â€²), so that Det(B(Ï„ â€²)) 6= 0. Define qâ€² = (dâ€²M , dâ€²M , Idâ€²M , Ï„ â€²), where
Idâ€²M is the identity matrix of dimensions d
â€²
M Ã— dâ€²M . Since Det
(
Idâ€²MB(Ï„
â€²)ITdâ€²M
)
=
Det
(
B(Ï„ â€²)
)
6= 0, then qâ€² âˆˆ CA, and we have W (qâ€²) = V (dâ€²), so that B1 â‰¥ V (dâ€²),
âˆ€dâ€² âˆˆ CB , and then WB is bounded. Since WA is bounded, then the HRBB con-
dition holds, see Lemma 7.1, and then (4.2) shows that g(Î¸) is B0-compatible.
Conversely, assume WB is bounded. Then, there exists a s.n.n.d. matrix
B2 âˆˆ RdPÃ—dP , such that B2 â‰¥ V (d), âˆ€d âˆˆ CB . Take an arbitrary qâ€² âˆˆ CA, with
qâ€² =
(
dâ€²M , d
â€²
A, A
â€², Ï„ â€²
)
, with Aâ€² âˆˆ Rdâ€²AÃ—dâ€²M , so that Det
(
Aâ€² B(Ï„ â€²) (Aâ€²)
T ) 6= 0. As
in the proof of the last part of Theorem 3.1, obtain d?Ï„ âˆˆ N, 1 â‰¤ d?Ï„ â‰¤ dâ€²M ,
A? âˆˆ Rdâ€²MÃ—d?Ï„ , and Ï„ ? âˆˆ Î˜d?Ï„ , by elimination of the components of the vector
Î²(Ï„ â€²) which are linear combinations w.p. 1 of previous components, so that
Î²(Ï„ â€²) = A? Î²(Ï„ ?), with Det
(
B(Ï„ ?)
)
= Det
(
EÎ¸T
[
Î²(Ï„ ?) Î²T (Ï„ ?)
])
6= 0.
Then, B(Ï„ â€²) = EÎ¸T
[
Î²(Ï„ â€²) Î²T (Ï„ â€²)
]
= A? EÎ¸T
[
Î²(Ï„ ?) Î²T (Ï„ ?)
]
(A?)
T
=
A? B(Ï„ ?) (A?)
T
. Since g(Î¸) is B0-compatible, then G(Ï„
â€²) = G(Ï„ ?) (A?)
T
.
Hence,
W (qâ€²) = G(Ï„ â€²) (Aâ€²)
T (
Aâ€² B(Ï„ â€²) (Aâ€²)
T
)
)âˆ’1
Aâ€² G(Ï„ â€²)
= G(Ï„ ?) (A?)
T
(Aâ€²)
T (
Aâ€²A?B(Ï„ ?) (A?)
T
(Aâ€²)
T )âˆ’1
Aâ€² A? GT (Ï„ ?)
Define d? =
(
d?Ï„ , Ï„
?
)
so that V (d?) = G(Ï„ ?)Bâˆ’1(Ï„ ?)GT (Ï„ ?). Then, the Ap-
pendix Lemma A.4 shows that V (d?) â‰¥ W (qâ€²), so that B2 â‰¥ V (d?) â‰¥ W (qâ€²).
Hence B2 â‰¥W (qâ€²), for all qâ€² âˆˆ CA, so that WA is bounded.
For an arbitrary symmetric matrix W call Î»M (W ) âˆˆ R its greatest eigen-
value. The operator norm â€–Aâ€–op of a matrix A âˆˆ RNÃ—M is its greatest singular
value, [4] p. 12, i.e. the non-negative square root of the greatest eigenvalue
of the matrix ATA, so that â€–Aâ€–op =
(
Î»M (A
TA)
)1/2
. For s.n.n.d. matrices
singular values and eigenvalues coincide, [23] p. 19, so that if W âˆˆ W , then
â€–Wâ€–op = Î»M (W ). Define a k-identity matrix as a matrix of the form K IM
where K âˆˆ R and IM is the identity matrix of dimensions M Ã—M . Then, we
have
Lemma 7.3. If X is a s.n.n.d. matrix, X âˆˆ RMÃ—M , then, for K âˆˆ R, we have
K IM â‰¥ X if and only if K â‰¥ Î»M (X).
Proof. Since X is symmetric, then it is diagonalizable, so that there exist an
orthogonal matrix Q âˆˆ RMÃ—M , and a diagonal matrix Î› âˆˆ RMÃ—M , such
that X = Q Î› QT . Then, since Î»M (X) IM â‰¥ Î›, we have Î»M (X) IM =
Q Î»M (X) IM Q
T â‰¥ Q Î› QT = X. Hence, if K â‰¥ Î»M (X), then K IM â‰¥
Î»M (X) IM â‰¥ X. Conversely if K IM â‰¥ X, then K IM â‰¥ Q Î› QT , so that
B. Cernuschi-FrÌÄ±as 25
QTK IMQ â‰¥ Î›, and since QTK IM Q = K IM , then K IM â‰¥ Î›. Hence
K â‰¥ Î»M (X).
Lemma 7.4. A non-empty collection W of s.n.n.d. matrices W âˆˆ RMÃ—M is
upper bounded if and only if there exists KW âˆˆ R, such that KW IM â‰¥ W ,
âˆ€W âˆˆ W .
Proof. If there exists KW âˆˆ R, such that KW IM â‰¥ W , âˆ€W âˆˆ W , then by
definition KW IM is a matrix bound for W , and then W is bounded. Conversely,
assume W is bounded. Then there exists BW âˆˆ RMÃ—M s.n.n.d., such that BW â‰¥
W , âˆ€W âˆˆ W . Since BW is symmetric, then there exists the real maximum
eigenvalue Î»M (BW ) âˆˆ R. Take KW âˆˆ R such that KW â‰¥ Î»M (BW ). Then, from
Lemma 7.3, KW IM â‰¥ BW â‰¥W , âˆ€W âˆˆ W .
7.3 Main Theorem
Collecting all the previous results, we have
Theorem 7.3 (Main Theorem). The following statements for Problem 2.3 are
equivalent, meaning that if any one of them is true, then they are all true:
1. A finite covariance vector unbiased estimator exists, i.e. Ug is not empty.
2. A Barankin-efficient vector unbiased estimator exists.
3. The HRBB condition holds.
4. The collection WA is bounded.
5. There exists K âˆˆ R+ such that K IdP â‰¥W , âˆ€W âˆˆ WA.
6. The collection WB is bounded, and g(Î¸) is B0-compatible.
7. There exists K âˆˆ R+ such that K IdP â‰¥ W , âˆ€W âˆˆ WB, and g(Î¸) is
B0-compatible.
Proof. 3) â‡’ 2) â‡’ 1) follows from Theorem 6.1, 3) â‡” 4) from Theorem 7.1, 1)
â‡’ 4) from Lemma 7.2, 4) â‡” 5) and 6) â‡” 7) from Lemma 7.4, finally 4) â‡” 6)
follows from Theorem 7.2.
Corollary 7.3.1. As a corollary, the collection Ug is empty iff WA is not
bounded, i.e. for each k âˆˆ N there exists W ?k âˆˆ WA such that â€–W ?k â€–F â‰¥
â€–W ?k â€–op = Î»M (W
?
k ) > k, so that limkâ†’+âˆ â€–W ?k â€–F = +âˆ. Note that Ug is
empty either because there are no unbiased estimators for g(Î¸), âˆ€Î¸ âˆˆ Î˜, or if
there exist, they donâ€™t have finite finite covariance matrix at Î¸T , see Definition
3.2.
B. Cernuschi-FrÌÄ±as 26
A APPENDIX
Lemma A.1. Let H be an arbitrary Hilbert space. Let the elements ui âˆˆ H ,
for 1 â‰¤ i â‰¤ dL < +âˆ, be orthonormal so that â€–uiâ€–L2 = 1, for 1 â‰¤ i â‰¤ dL, and
ã€ˆui, ujã€‰H = 0, for i 6= j, 1 â‰¤ i, j â‰¤ dL, and then ã€ˆui, ujã€‰H = Î´i,j. Assume that
for each ui, for 1 â‰¤ i â‰¤ dL, there exist sequences
(
si(m)
)
mâˆˆN, with si(m) âˆˆH ,
âˆ€m âˆˆ N, for 1 â‰¤ i â‰¤ dL, such that lim
mâ†’âˆ
â€–ui âˆ’ si(m)â€–H = 0, for 1 â‰¤ i â‰¤ dL.
Define S(m) âˆˆ RdLÃ—dL , as a matrix with i-th, j-th element
[
S(m)
]
i,j
=
ã€ˆsi(m), sj(m)ã€‰H , for 1 â‰¤ i, j â‰¤ dL, âˆ€m âˆˆ N. Then:
1. â€–S(m)âˆ’ IdLâ€–F â†’ 0, as m â†’ âˆ, where IdL is the identity matrix of
dimensions dL Ã— dL.
2. Det(S(m))â†’ 1, and
âˆ¥âˆ¥Sâˆ’1(m)âˆ’ IdLâˆ¥âˆ¥F â†’ 0, as mâ†’âˆ.
3. lim
mâ†’âˆ
ã€ˆui âˆ’ si(m), sj(m)ã€‰H = 0, for all 1 â‰¤ i, j â‰¤ dL.
Proof. a) From the Cauchy-Schwarz inequality we obtain
âˆ£âˆ£ã€ˆui âˆ’ si(m), ujã€‰H âˆ£âˆ£ â‰¤
â€–ui âˆ’ si(m)â€–H , so that limmâ†’âˆ ã€ˆui âˆ’ si(m), ujã€‰H = 0, for all 1 â‰¤ i, j â‰¤ dL.
b) Also
âˆ£âˆ£ã€ˆui âˆ’ si(m), uj âˆ’ sj(m)ã€‰H âˆ£âˆ£ â‰¤ â€–ui âˆ’ si(m)â€–H â€–uj âˆ’ sj(m)â€–H , and
then lim
mâ†’âˆ
ã€ˆui âˆ’ si(m), uj âˆ’ sj(m)ã€‰H = 0, for all 1 â‰¤ i, j â‰¤ dL.
c) We have ã€ˆsi(m), sj(m)ã€‰H = ã€ˆsi(m)âˆ’ ui + ui, sj(m)âˆ’ uj + ujã€‰H =
ã€ˆsi(m)âˆ’ ui, sj(m)âˆ’ ujã€‰H + ã€ˆui, sj(m)âˆ’ ujã€‰H + ã€ˆsi(m)âˆ’ ui, ujã€‰H +
ã€ˆui, ujã€‰H . Taking the limit, and using a) and b), limmâ†’âˆ ã€ˆsi(m), sj(m)ã€‰H = Î´i,j ,
for all 1 â‰¤ i, j â‰¤ dL. Then S(m) â†’ IdL component by component, and then
in Frobenius norm. Since the determinant of a matrix is an algebraic sum of a
finite number of products of a finite number of elements of the matrix, see [5] p.
319, then Det (S(m)) â†’ Det(IdL) = 1, so that âˆƒM0 âˆˆ N, such that âˆ€m â‰¥ M0,
it will be Det (S(m)) â‰¥ 1/2, and then Det (S(m)) 6= 0. Similarly, since the
elements of the inverse of a matrix are the quotients of algebraic sums of a finite
number of products of a finite number of elements of the matrix divided the
determinant, see [5] p. 325, then Sâˆ’1(m) has a limit A0 component by compo-
nent, so that S(m) Sâˆ’1(m)â†’ IdL A0, but since S(m) Sâˆ’1(m) = IdL , âˆ€m âˆˆ N,
then A0 = IdL , and then S
âˆ’1(m)â†’ IdL component by component as mâ†’âˆ,
and then in Frobenius norm, or any other matrix norm, so that we have shown
items 1) and 2).
d) We have ã€ˆui âˆ’ si(m), sj(m)ã€‰H = ã€ˆui âˆ’ si(m), sj(m)âˆ’ uj + ujã€‰H =
ã€ˆui âˆ’ si(m), sj(m)âˆ’ ujã€‰H + ã€ˆui âˆ’ si(m), ujã€‰H . Applying a) and b) we ob-
tain lim
mâ†’âˆ
ã€ˆui âˆ’ si(m), sj(m)ã€‰H = 0, for all 1 â‰¤ i, j â‰¤ dL, so that we have
shown item 3).
Lemma A.2. Let
(
An
)
nâˆˆN be a sequence of s.n.n.d. matrices An âˆˆ R
NÃ—N ,
An â‰¥ 0, âˆ€n âˆˆ N , such that there exists A âˆˆ RNÃ—N for which An â†’ A c.b.c and
then in Frobenius norm. Then the matrix A is s.n.n.d.
B. Cernuschi-FrÌÄ±as 27
Proof. TakeÎ± âˆˆ RN arbitrary, sinceN is finite, then limnâ†’+âˆÎ±TAnÎ± = Î±TAÎ±.
But Î±TAnÎ± â‰¥ 0, âˆ€n âˆˆ N, so that limnâ†’+âˆÎ±TAnÎ± â‰¥ 0, and then Î±TAÎ± â‰¥ 0,
âˆ€Î± âˆˆ RN .
The following lemma is a LMI weighted form of the Cauchy-Schwarz inequal-
ity for matrices, [8] p. 1093. For convenience, a proof is given here.
Lemma A.3. Let M,N,P âˆˆ N. Let H âˆˆ RMÃ—M be an arbitrary real s.p.d.
matrix, and let X âˆˆ RNÃ—M and Y âˆˆ RPÃ—M , be otherwise arbitrary real matrices
such that Det(Y HY T ) 6= 0. Then
X H XT â‰¥ X H Y T
(
Y H Y T
)âˆ’1
Y H XT
with equality if and only if there exists Î› âˆˆ RNÃ—P , such that X = Î› Y , if and
only if
X = X H Y T
(
Y H Y T
)âˆ’1
Y
Proof. Let Î› âˆˆ RNÃ—P . Define T (Î›) = (X âˆ’ Î› Y ) H (X âˆ’ Î› Y )T , so that T (Î›)
is s.n.n.d., âˆ€Î› âˆˆ RNÃ—P . Define
R(Î›) =
(
Î›âˆ’X H Y T
(
Y H Y T
)âˆ’1)
Y H Y T
(
Î›âˆ’X H Y T
(
Y H Y T
)âˆ’1)T
and D = X H XT âˆ’ X H Y T
(
Y H Y T
)âˆ’1
Y H XT . Note that R(Î›) â‰¥
0, âˆ€Î› âˆˆ RNÃ—P . Then T (Î›) = R(Î›) + D â‰¥ 0, âˆ€Î› âˆˆ RNÃ—P . For Î›0 =
X H Y T
(
Y H Y T
)âˆ’1
, we have R(Î›0) = 0, so that T (Î›0) = D â‰¥ 0, and then
the LMI is obtained. If there is equality then D = 0, and then T (Î›) = R(Î›),
âˆ€Î› âˆˆ RNÃ—P . In particular for Î›0 we have R(Î›0) = 0, and then T (Î›0) = 0.
But, since H is s.p.d. then X = Î›0 Y = X H Y
T
(
Y H Y T
)âˆ’1
Y . As for
the converse, if there exists Î›1 such that X = Î›1 Y , then T (Î›1) = 0, since
R(Î›1) â‰¥ 0 by definition, and D â‰¥ 0 as previously shown, then, R(Î›1) = 0
and D = 0, because T (Î›1) = R(Î›1) + D. From D = 0 we obtain the equality
in the LMI inequality, and from R(Î›1) = 0, we obtain that Î›1 = Î›0, because
Det
(
Y H Y T
)
6= 0 and then Y H Y T is s.p.d. If X = X H Y T
(
Y H Y T
)âˆ’1
Y ,
multiply both sides on the right by H XT , and then the equality for the LMI
is obtained.
The following lemma, cf. [9] Lemma 2.4.1, may be interpreted as a LMI
generalization of the Rayleigh quotient, [12] p. 117.
Lemma A.4. Let M,N,P âˆˆ N. Let B âˆˆ RMÃ—M be an arbitrary real s.p.d.
matrix, and let G âˆˆ RNÃ—M and A âˆˆ RPÃ—M , be otherwise arbitrary real matrices
such that Det(ABAT ) 6= 0. Then
G Bâˆ’1 GT â‰¥ G AT
(
A B AT
)âˆ’1
A GT
with equality if and only if there exists Î›0 âˆˆ RNÃ—P , such that G = Î›0 A B, if
and only if
G = G AT
(
A B AT
)âˆ’1
A B
B. Cernuschi-FrÌÄ±as 28
Proof. Since B is s.p.d. then it has a unique s.p.d. square root B1/2, [16] p.
405, and we have Det(B) 6= 0 and Det(B1/2) 6= 0. The result follows from the
previous Lemma A.3 taking, X = G Bâˆ’1/2, Y = A B1/2, and H as the identity
matrix of dimensions M Ã—M .
References
[1] Stefan Banach, TheÌorie des opeÌrations lineÌaires, Druk M. Garasinski, War-
saw, 1932.
[2] E. W. Barankin, Locally best unbiased estimates, The Ann. of Mathematical
Statistics 20 (1949), no. 4, 477â€“501.
[3] Richard Beals, Advanced mathematical analysis; periodic functions and dis-
tributions, complex analysis, laplace transform and applications, Springer-
Verlag, 1973.
[4] Rajendra Bhatia, Positive definite matrices, Princeton University Press,
Princeton, N.J, 2007.
[5] Garrett Birkhoff and Saunders Mac Lane, A survey of modern algebra,
fourth ed., Macmillan, 1977.
[6] A. V. Bitsadze, Integral equations of first kind, World Scientific, Singapore
River Edge, NJ, 1995.
[7] B. Cernuschi-FrÌÄ±as, F. Gama, and D. Casaglia, Deepest minimum criterion
for biased affine estimation, IEEE Transactions on Signal Processing 62
(2014), no. 9, 2437â€“2449.
[8] John S. Chipman, On least squares with insufficient observations, Journal
of the American Statistical Association 59 (1964), no. 308, 1078â€“1111.
[9] John S. Chipman, Advanced econometric theory, Routledge, New York,
2011.
[10] Kai Lai Chung, A course in probability theory, second ed., Academic Press,
1974.
[11] John B. Conway, A course in functional analysis, Springer-Verlag, 1985.
[12] R. O. Duda and P. E. Hart, Pattern classification and scene analysis, first
ed., Wiley, New York, 1973.
[13] J. D. Gorman and A. O. Hero, Lower bounds for parametric estimation with
constraints, IEEE Transactions on Information Theory 36 (1990), no. 6,
1285â€“1301.
[14] Paul Richard. Halmos, Measure theory, Springer, 1974.
B. Cernuschi-FrÌÄ±as 29
[15] Edwin Hewitt and Karl Stromberg, Real and abstract analysis: a modern
treatment of the theory of functions of a real variable, Springer, 1975.
[16] R. A. Horn and C. R. Johnson, Matrix analysis, first ed., Cambridge Uni-
versity Press, Cambridge, UK, 1985.
[17] R. G. Laha and V. K. Rohatgi, Probability theory, Wiley, 1979.
[18] Lawrence Narici and Edward Beckenstein, The Hahn-Banach theorem: the
life and times, Topology and its Applications 77 (1997), no. 2, 193 â€“ 211.
[19] C. R. Rao, Linear statistical inference and its applications, second ed.,
Wiley, New York, 1973.
[20] Saburou Saitoh and Y. Sawano, Theory of reproducing kernels and appli-
cations, Springer, Singapore, 2016.
[21] P. Stoica and Boon Chong Ng, On the Cramer-Rao bound under parametric
constraints, IEEE Signal Processing Letters 5 (1998), no. 7, 177â€“179.
[22] H. L. Van Trees and K. L. Bell, Bayesian bounds for parameter estimation
and nonlinear filtering/tracking, first ed., Wiley, IEEE Press, Piscataway,
NJ, 2007.
[23] Xingzhi Zhan, Matrix inequalities, Springer, Berlin New York, 2002.
[24] Fuzhen Zhang, Matrix theory: Basic results and techniques, first ed.,
Springer-Verlag New York, New York, 1999.
[25] Fuzhen Zhang, The Schur complement and its applications, Springer, New
York, 2005.

