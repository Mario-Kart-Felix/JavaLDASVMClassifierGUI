Neural Sequence Model Training
via α-divergence Minimization
Sotetsu Koyamada 1 2 3 Yuta Kikuchi 4 Atsunori Kanemura 2 Shin-ichi Maeda 4 Shin Ishii 3 5
Abstract
We propose a new neural sequence model train-
ing method in which the objective function is
defined by α-divergence. We demonstrate that
the objective function generalizes the maximum-
likelihood (ML)-based and reinforcement learn-
ing (RL)-based objective functions as special
cases (i.e., ML corresponds to α → 0 and RL
to α→ 1). We also show that the gradient of the
objective function can be considered a mixture of
ML- and RL-based objective gradients. The ex-
perimental results of a machine translation task
show that minimizing the objective function with
α > 0 outperforms α→ 0, which corresponds to
ML-based methods.
1. Introduction
Neural sequence models have been applied successfully
to various types of machine learning tasks, such as neu-
ral machine translation (Cho et al., 2014; Sutskever et al.,
2014; Bahdanau et al., 2015), caption generation (Xu
et al., 2015; Chen & Lawrence Zitnick, 2015), conver-
sation task (Vinyals & Le, 2015), and speech recogni-
tion (Chorowski et al., 2014; 2015; Bahdanau et al., 2016).
As neural sequence models have a wide range of applica-
tions, developing more effective and sophisticated learning
algorithms can be beneficial.
Popular objective functions for training neural sequence
models include the maximum-likelihood (ML) and rein-
forcement learning (RL) objective functions. However,
both have limitations, i.e., training/testing discrepancy and
sample inefficiency, respectively. Bengio et al. (2015) in-
1Recruit Technologies Co., Ltd., Tokyo, Japan 2National In-
stitute of Advanced Industrial Science and Technology (AIST),
Tokyo, Japan 3Graduate School of Informatics, Kyoto University,
Kyoto, Japan 4Preferred Networks, Inc., Tokyo, Japan 5ATR Cog-
nitive Mechanisms Laboratories, Kyoto, Japan. Correspondence
to: Sotetsu Koyamada <sotetsu.koyamada@gmail.com>.
2017 ICML Workshop on Learning to Generate Natural Lan-
guage (LGNL 2017), Sydney, Australia. Copyright 2017 by the
author(s).
dicated that optimizing the ML objective is not equal to
optimizing the evaluation metric (e.g., BLEU (Papineni
et al., 2002) score in machine translation). In addition,
during training, the ground-truth tokens are used for the
prediction of the next token; however, during testing, no
ground-truth tokens are available and the tokens that are
predicted by the model are used instead. On the other
hand, although the RL-based approach does not suffer from
the training/testing discrepancy, it does suffer from sam-
ple inefficiency. Samples generated by the model do not
necessarily yield high evaluation scores (i.e., rewards) es-
pecially in the early stage of the training. Consequently,
RL-based methods are not self-contained, i.e., they require
pre-training via ML-based methods. As discussed in Sec-
tion 2, since these problems depend on their sampling dis-
tributions, it is difficult to resolve these problems simulta-
neously.
We propose α-divergence minimization training named α-
DiMT for a neural sequence model. We demonstrate that an
α-DiMT objective function generalizes ML- and RL- based
objective functions, i.e., α-DiMT can represent both func-
tions as its special cases (α→ 0 and α→ 1). We also show
that, for α ∈ (0, 1), the gradient of the α-DiMT objec-
tive becomes the weighted sum of the gradients of negative
log-likelihoods. Here the weights are obtained by the geo-
metric mean of the sampling distributions of the ML- and
RL-based objectives. We apply gradient descent methods
to optimize the proposed objective function, where the gra-
dient of the objective is estimated by means of importance
sampling. Consequently the optimization strategy avoids
on-policy RL sampling which suffers from sample ineffi-
ciency, and optimizes the objective function closer to the
desired RL-based objective.
Experimental results on a machine translation task indicate
that the proposed α-DiMT approach outperforms the ML
baseline and the reward augmented maximum-likelihood
method (RAML; Norouzi et al., 2016), upon which we
build the proposed method. We compare our results to
those reported by Bahdanau et al. (2017), who proposed an
on-policy RL-based method. We also confirm that α-DiMT
can provide comparable BLEU score without pre-training.
The contributions of this paper are summarized as follows.
ar
X
iv
:1
70
6.
10
03
1v
1 
 [
st
at
.M
L
] 
 3
0 
Ju
n 
20
17
Neural Sequence Model Training via α-divergence Minimization
• We consider the limitations and advantages of ML,
RL, and RAML objective functions with respect to
(i) objective score discrepancy, (ii) sampling distri-
bution discrepancy and (iii) sample inefficiency (Sec-
tion 2).
• We define the α-DiMT objective function using α-
divergence and demonstrate that it can be considered
a generalization of ML- and RL-based objective func-
tions (Section 4.1).
• We demonstrate that the gradient of the α-DiMT ob-
jective can be obtained by the weighted sum of the
gradients of negative log likelihoods and that the
weights are a mixture of the sampling distributions of
ML- and RL-based objective functions (Section 4.2).
• We propose an importance sampling-based optimiza-
tion method, which is very similar to the RAML opti-
mization, for the proposed α-DiMT objective function
(Section 4.3). Thus, there is nearly no implementation
cost if RAML has already been implemented.
• The results of machine translation experiments
demonstrate that the proposed α-DiMT outperforms
the ML-baseline and RAML (Section 6).
2. Comparing objective functions
At least three problems associated with learning neural se-
quence models exist, and the ML and RL approaches can-
not resolve all of these problems simultaneously. In this
section, we examine why these problems matter and how
they are addressed by current state-of-the-art methods.
Given a context (or input sequence) x ∈ X and a target se-
quence y = (y1, . . . , yT ) ∈ Y , an ML approach is typically
used to train a neural sequence model. ML minimizes the
negative log-likelihood objective function
L(θ) = −
∑
x∈X
∑
y∈Y
q(y|x) log pθ(y|x), (1)
where q(y|x) denotes the true sampling distribution. Here,
we assume that x is uniformly sampled from X and omit
the distribution of x from Eq. (1) for simplicity. For ex-
ample, in machine translation, if a corpus contains only a
single target sentence y∗ for each input sentence x, then
q(y|x) = δ(y, y∗|x) and the objective becomes L(θ) =
−
∑
x∈X log pθ(y
∗|x).
It is known that ML does not optimize the final perfor-
mance measure. For example, in the case of machine trans-
lation, the famous evaluation measures such as BLEU or
edit rate (Snover et al., 2006) differ from the negative like-
lihood function.
The optimization of the final performance measure can be
formulated as the minimization of the negative total ex-
pected rewards expressed as follows:
L∗(θ) = −
∑
x∈X
∑
y∈Y
pθ(y|x)r(y, y∗|x), (2)
where r(y, y∗|x) is a reward function associated with the
sequence prediction y, i.e. the BLEU score or the edit rate
in machine translation.
The above observations raise the following problems.
(i) Objective score discrepancy. The reward function
is not used when training the model; however, it is
the performance measure in the testing (evaluation)
phase.
(ii) Sampling distribution discrepancy. The model is
trained with samples from the true sampling distri-
bution q(y|x); however, it is evaluated using samples
generated from the learned distribution pθ(y|x).
On-policy RL is an approach to solve the above problems.
The objective function of on-policy RL is L∗ in Eq. (2),
which is a reward-based objective function; thus, there is
no objective score discrepancy, which resolves problem (i).
On-policy sampling from pθ(y|x) and taking the expec-
tation with pθ(y|x) in Eq. (2) also resolves problem (ii).
Ranzato et al. (2016) and Bahdanau et al. (2017) directly
optimized L∗ using policy gradient methods (Sutton et al.,
2000). A sequence prediction task that selects the next
token based on an action trajectory (y1, . . . , yt−1) can be
considered an RL problem. Here the next token selection
corresponds to the next action selection in RL. In addition,
the action trajectory and the context x correspond to the
current state in RL. To prevent the policy from becoming
overly greedy and deterministic, some studies have used
the following entropy-regularized version of the policy gra-
dient objective function (Mnih et al., 2016):
L∗(τ)(θ) :=
∑
x∈D
{
−τH(pθ(y|x))−
∑
y∈Y
pθ(y|x)r(y, y∗|x)
}
.
(3)
Note that limτ→0 L∗(τ) = L
∗ holds. We can ob-
tain the gradient of the objective as ∇θL∗(θ) =
−
∑
x∈X
∑
y∈Y pθ(y|x)∇θ log pθ(y|x)r(y, y∗|x) us-
ing the policy gradient theorem (Sutton et al., 2000).
On-policy RL can suffer from sample inefficiency; thus,
it may not generate samples with high rewards, particu-
larly in the early learning stage. By definition, on-policy
RL generates training samples from its model distribution.
This means that, if model pθ(y|x) has low prediction abil-
ity, only a few samples will exist with high rewards.
Neural Sequence Model Training via α-divergence Minimization
(iii) Sample inefficiency. The RL model can draw sam-
ples with low rewards, which results in the failure of
estimating the objective function around the peak.
Machine translation suffers from this problem because the
action (token) space is vast (typically >10, 000 dimen-
sions) and rewards are sparse, i.e., positive rewards are
observed only at the end of a sequence. Therefore, the
RL-based approach usually requires good initialization and
thus is not self-contained. Previous studies have employed
pre-training with ML before performing on-policy RL-
based sampling (Ranzato et al., 2016; Bahdanau et al.,
2017).
Despite various attempts, a fundamental technical barrier
exists. This barrier prevents solving the three problems us-
ing a single method. The barrier comes from a trade-off be-
tween (ii) sampling distribution discrepancy and (iii) sam-
ple inefficiency because these issues are related to the sam-
pling distribution.
Norouzi et al. (2016) proposed RAML, which solves prob-
lems (i) and (iii) simultaneously. RAML replaces the sam-
pling distribution of ML, i.e., q(y|x) in Eq. (1), with a
reward-based distribution q(τ)(y|x) ∝ exp {r(y, y∗|x)/τ}.
In other words, RAML incorporates the reward information
into the ML objective function. The RAML objective func-
tion is expressed as follows:
L(τ)(θ) := −
∑
x∈X
∑
y∈Y
q(τ)(y|x) log pθ(y|x). (4)
However, problem (ii) remains. As mentioned above, it
is difficult to solve problems (ii) and (iii) simultaneously;
thus, our approach is to control the trade-off of the sam-
pling distributions by considering their mixture.
3. α-divergence
The proposed method utilizes α-divergence D(α)A (p‖q),
which measures the asymmetric distance between two dis-
tributions p and q (Amari, 1985). A prominent feature
of α-divergence is that it can behave as DKL(p‖q) or
DKL(q‖p) depending on the value of α, i.e., D(1)A (p‖q) :=
limα→1D
(α)
A (p‖q) = DKL(p‖q) and D
(0)
A (p‖q) :=
limα→0D
(α)
A (p‖q) = DKL(q‖p). This fact follows from
the definition of α-divergence
D
(α)
A (p‖q) :=
1
α(1− α)
{
1−
∑
y∈Y
pα(y)q1−α(y)
}
(5)
= − 1
α
∑
y∈Y
p(y) log(α)
(
q(y)
p(y)
)
, (6)
where log(α)(·) is the generalized logarithm log(α)(x) :=
(1− α)−1(x1−α − 1).
4. α-DiMT
In this section, we describe the objective function of the
proposed method, i.e., α-DiMT, and how it is trained.
4.1. Objective function
We define the objective function of α-DiMT as the α-
divergence between pθ and q(τ):
L(α,τ)(θ) := τ
∑
x∈X
D
(α)
A (pθ‖q(τ)) (7)
= − τ
α
∑
x∈X
∑
y∈Y
pθ(y|x) log(α)
(
q(τ)(y|x)
pθ(y|x)
)
.
(8)
This α-divergence is equal to (up to constant) L∗(τ) in
Eq. (3) or L(τ) in Eq. (4) by employing α → 1 or α → 0
limits, respectively. Figure 1 illustrates how the α-DiMT
objective bridges the ML- and RL-based objective func-
tions.
lim
α→1
L(α,τ)(θ) = τ
∑
x∈X
DKL(pθ‖q(τ)) (9)
= L∗(τ)(θ) + constant, (10)
lim
α→0
L(α,τ)(θ) = τ
∑
x∈X
DKL(q(τ)‖pθ) (11)
= τL(τ)(θ) + constant. (12)
Although the objectives L∗(α,τ)(θ), L
∗
(τ)(θ), and L(τ)(θ)
have the same minimizer pθ(y|x) = q(τ)(y|x), empirical
solutions often differ.
4.2. Objective function gradient
The gradient of (8) used for gradient descent optimization
in the proposed α-DiMT can be obtained by a discussion
similar to that of the policy gradient theorem (Sutton et al.,
2000). The gradient of the α-DiMT objective function is
expressed as
∇θL(α,τ)(θ) = −
∑
x∈X
∑
y∈Y
p
(α,τ)
θ (y|x)∇θ log pθ(y|x),
(13)
where
p
(α,τ)
θ (y|x) =
τ
1− α
pαθ (y|x)q1−α(τ) (y|x) (14)
is a weight that mixes sampling distributions pθ and q(τ).
See Appendix A for the derivation of this gradient. This
gradient differs from that of ML or RAML only in the
weights. In addition, it converges to the gradient of RL
or RAML (up to constant) by taking α → 1 or α → 0
limits, respectively; i.e., limα→1∇θL(α,τ) = ∇θL∗(τ) and
limα→0∇θL(α,τ) = τ∇θL(τ).
Neural Sequence Model Training via α-divergence Minimization
α → 0 α → 10 < α < 1
τ = 0
τ > 0
Maximum likelihood Policy gradient
(w/o regularization)
Policy gradient
(w/ entropy regularization)
RAML
(Norouzi et al., 2016)R
eg
ul
ar
iz
at
io
n
Maximum likelihood 
based
Reinforcement learning 
based
Proposed method
τ → 0
α → 0 α → 1
τ → 0→ →
Figure 1. α-DiMT objective bridges ML- and RL-based objectives.
4.3. Optimization of objective function
Our optimization strategy is similar to that of RAML. First,
we estimate the gradient of the objective function by impor-
tance sampling and then use this estimate with the gradient
method. We sample target sentence y for each x from a
proposal distribution q0 and estimate the gradient by im-
portance sampling as follows:
∇θL(α,τ)(θ)
= −
∑
x∈X
∑
y∈Y
q0(y|x)
(
p̃
(α,τ)
θ (y|x)
q0(y|x)
)
∇ log pθ(y|x),
(15)
where p̃(α,τ)θ (y|x) =
1
Z p
α
θ (y|x)q
1−α
(τ) (y|x) is the nor-
malized distribution of p(α,τ)θ (y|x). This normalization
changes the magnitude of the gradient but not the direction.
Here, q0 is typically obtained by applying data augmenta-
tion to a corpus and calculating rewards for the generated
samples.
5. Related works
From the RL literature, reward-based neural sequence
model training can be separated into on-policy and off-
policy approaches, which differ in the sampling distribu-
tions. The proposed α-DiMT approach can be considered
an off-policy approach with importance sampling.
Recently, on-policy RL-based approaches for neural se-
quence predictions have been proposed. Ranzato et al.
(2016) proposed a method that uses the REINFORCE al-
gorithm (Williams, 1992). Based on Ranzato et al. (2016),
Bahdanau et al. (2017) proposed a method that estimates a
critic network and uses it to reduce the variance of the esti-
mated gradient. Bengio et al. (2015) proposed a method
that replaces some ground-truth tokens in an output se-
quence with generated tokens. Yu et al. (2017), Lamb et al.
(2016), and Wu et al. (2017) proposed methods based on
GAN (generative adversarial net) approaches (Goodfellow
et al., 2014). Note that on-policy RL-based approaches
can directly optimize the evaluation metric. Degris et al.
(2012) proposed off-policy gradient methods using impor-
tance sampling, and the proposed α-DiMT off-policy ap-
proach utilizes importance sampling to reduce the differ-
ence between the objective function and the evaluation
measure when α > 0.
As mentioned previously, the proposed α-DiMT can be
considered an off-policy RL-based approach in that the
sampling distribution differs from the model itself. Thus,
the proposed α-DiMT approach has the same advantages as
off-policy RL methods compared to on-policy RL methods,
i.e., computational efficiency during training and learning
stability. On-policy RL approaches must generate sam-
ples during training, and immediately utilize these sam-
ples. This property leads to high computational costs dur-
ing training and if the model falls into a poor local mini-
mum, it is difficult to recover from this failure. On the other
hand, by exploiting data augmentation, the proposed α-
DiMT can collect samples before training. Moreover, be-
cause the sampling distribution is a stationary distribution
independent of the model, one can expect that the learning
process of α-DiMT is more stable than that of on-policy RL
approaches. Several other methods that compute rewards
before training can be considered off-policy RL-based ap-
proaches, e.g., minimum risk training (MRT; Shen et al.,
2016, RANDOMER (Guu et al., 2017), and Google neural
machine translation (GNMT; Wu et al., 2016).
While the proposed approach is a mixture of ML- and RL-
based approaches, this attempt is not unique. The sampling
distribution of scheduled sampling (Bengio et al., 2015) is
also a mixture of ML- and RL-based sampling distribu-
tions. However, the sampling distributions of scheduled
sampling can differ even in the same sentence, whereas
ours are sampled from a stationary distribution. To bridge
the ML- and RL-based approaches, Guu et al. (2017) con-
sidered the weights of the gradients of the ML- and RL-
based approaches by directly comparing both gradients. In
contrast, the weights of the proposed α-DiMT approach
are obtained as the results of defining the α-divergence ob-
jective function. GNMT (Wu et al., 2016) considered a
Neural Sequence Model Training via α-divergence Minimization
Table 1. IWSLT’14 German–English machine translation per-
formance: The best BLEU scores for the development set and
corresponding BLEU scores for the test set are shown. Each
search algorithm is greedy or beam search (BS) (beam size 10).
AC+ML and RF-C+ML denote actor-critic + maximum likeli-
hood and REINFORCE-critic + maximum likelihood, respec-
tively. Both combine on-policy RL- and ML-based objective
functions (Bahdanau et al., 2017).
BLEU Dev (greedy) Test (greedy) Test (BS)
Results from Bahdanau et al. (2017)
ML n/a 25.82 27.56
AC + LL n/a 27.49 28.53
RF-C+LL n/a 27.7 28.3
Our results
ML 29.83 27.96 28.26
RAML 29.65 27.50 28.35
Ours (α = 0.3) 29.90 27.73 28.29
Ours (α = 0.5) 29.91 28.02 28.49
Ours (α = 0.7) 29.72 27.81 28.25
mixture of ML- and RL-based objective functions by the
weighted arithmetic sum of L and L∗. Comparing this
weighted mean objective function and α-DiMT’s objective
function could be an interesting research direction in future.
6. Numerical experiments
We evaluated the effectiveness of α-DiMT experimentally
using a neural machine translation task. We compared the
BLEU scores of the ML baseline, RAML, and the pro-
posed α-DiMT on the IWSLT’14 German–English cor-
pus (Cettolo et al., 2014). We trained the same attention-
based encoder-decoder model (Bahdanau et al., 2015; Lu-
ong et al., 2015) for each method. When sampling from
q(τ), we employed a data augmentation procedure similar
to that of Norouzi et al. (2016). As a result, we found that
α-DiMT with α > 0 without pre-training outperformed the
ML baseline and RAML.
To compare our experimental results to on-policy RL meth-
ods (Ranzato et al., 2016; Bahdanau et al., 2017), we
used the IWSLT’14 German–English corpus (Cettolo et al.,
2014) and an attention-based encoder-decoder model (Bah-
danau et al., 2015; Luong et al., 2015). The training data
comprised 150K German–English sentence pairs and ap-
proximately 7K development/test sentence pairs. We fol-
lowed the model parameters used by Bahdanau et al. (2017)
and used the same encoder-decoder model architecture for
all methods. Details about the models and parameters are
discussed at the end of this section.
We obtained augmented data in the same manner as the
RAML framework (Norouzi et al., 2016). For each target
sentence, some tokens were replaced by other tokens in the
vocabulary and we used the negative Hamming distance as
reward. We assumed that Hamming distance e for each
sentence is less than [m×0.2], wherem is the length of the
sentence and [a] denotes the maximum integer which is less
than or equal to a ∈ R. Moreover, the Hamming distance
for a sample is uniformly selected from 0 to [m×0.2]. One
can also use BLEU or another machine translation metric
for this reward. However, we assumed the different pro-
posal distribution q0 from that of RAML. We assumed the
simplified proposal distribution q0, which is a discrete uni-
form distribution over [0,m× 0.2]. This results in that hy-
perparameter τ used in this experiment is larger than that of
RAML; thus, τ was set to 3.0 for RAML and the proposed
α-DiMT.
The experimental results obtained with the IWSLT’14 cor-
pus are shown in Table 1. We calculated the BLEU scores
with multi-bleu.perl1 script for both the development and
test sets. As shown in Table 1, the best BLEU score on
the development set with greedy search prediction and the
corresponding BLEU scores on the test set with greedy and
beam search prediction are shown for each method. Here,
the beam width was set to 10 (Ranzato et al., 2016; Bah-
danau et al., 2017). We found that the proposed α-DiMT
with α = 0.5 outperformed the ML-baseline and RAML.
This implies that an objective function better than RAML
for neural sequence model training exists in α > 0. Note
that the proposed α-DiMT does not utilize pre-training; on
the other hand, an on-policy RL approach (Bahdanau et al.,
2017) needs good initialization by pre-training using ML.
Although the ML baseline performances differ between our
results and those of Bahdanau et al. (2017), we emphasize
that the proposed α-DiMT performance with α = 0.5 with-
out pre-training is comparable with the on-policy RL-based
methods.
Model details The model architecture and parameters
follow that of Ranzato et al. (2016) and Bahdanau et al.
(2017). Here, the encoder was a bidirectional LSTM
with 256 units. The decoder was also an LSTM with
the same number of units. The vocabulary sizes for the
source/target were 32 009 and 22 822, respectively. We
utilized stochastic gradient descent with a decaying learn-
ing rate. The learning rate decays from 1.0 to 0.05 with
dev-decay (Wilson et al., 2017), i.e., after training each
epoch, we monitored the greedy BLEU score for the de-
velopment set and reduced the learning rate by multiply-
ing it with δ = 0.5 only when the greedy BLEU score
for the development set did not update the best BLEU
score. The minibatch size was 128. In addition, if an
unknown token, i.e. a special token representing a word
1https://github.com/moses-smt/
mosesdecoder/blob/master/scripts/generic/
multi-bleu.perl
Neural Sequence Model Training via α-divergence Minimization
not in the vocabulary, was generated in the predicted sen-
tence, it was replaced by the token with the highest atten-
tion in the source sentence (Jean et al., 2015). We imple-
mented our models using a fork from the PyTorch2 version
of the OpenNMT toolkit (Klein et al., 2017). Our reference
implementation is available: https://github.com/
sotetsuk/alpha-dimt-icmlws.
7. Conclusion
In this study, we have proposed a new objective function
as α-divergence minimization for neural sequence model
training that unifies ML- and RL-based objective functions.
In addition, we proved that the gradient of the objective
function is the weighted sum of the gradients of negative
log-likelihoods, and that the weights are represented as a
mixture of the sampling distributions of the ML- and RL-
based objective functions. We demonstrated that the pro-
posed approach outperforms the ML baseline and RAML
in the IWSLT’14 machine translation task.
By extending the existing objectives via α-divergence, we
gained an additional freedom to control the trade-off be-
tween the training/testing discrepancy and sample ineffi-
ciency. We consider that the sample inefficiency disappears
as the learning of the neural sequence model progresses if
α is increased as the learning progresses. We will investi-
gate methods to tune the control parameter α in accordance
with the learning process in future.
Acknowledgments
AK was supported in part by NEDO, Japan, and JSPS
KAKENHI 26730130.
References
Amari, Shun-ichi. Differential-Geometrical Methods in Statistics.
Springer, 1985.
Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neu-
ral machine translation by jointly learning to align and trans-
late. In Proceedings of International Conference on Learning
Representations (ICLR), 2015.
Bahdanau, Dzmitry, Chorowski, Jan, Serdyuk, Dmitriy, Brakel,
Philemon, and Bengio, Yoshua. End-to-end attention-based
large vocabulary speech recognition. In Proceedings of the
IEEE International Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP), 2016.
Bahdanau, Dzmitry, Brakel, Philemon, Xu, Kelvin, Goyal,
Anirudh, Lowe, Ryan, Pineau, Joelle, Courville, Aaron, and
Bengio, Yoshua. An actor-critic algorithm for sequence predic-
tion. In Proceedings of International Conference on Learning
Representations (ICLR), 2017.
2http://pytorch.org
Bengio, Samy, Vinyals, Oriol, Jaitly, Navdeep, and Shazeer,
Noam. Scheduled sampling for sequence prediction with re-
current neural networks. In Advances in Neural Information
Processing Systems (NIPS), 2015.
Cettolo, Mauro, Niehues, Jan, Stüker, Sebastian, Bentivogli,
Luisa, and Federico, Marcello. Report on the 11th IWSLT
evaluation campaign, IWSLT 2014. In Proceedings of Inter-
national Workshop on Spoken Language Translation (IWSLT),
2014.
Chen, Xinlei and Lawrence Zitnick, C. Mind’s eye: A recurrent
visual representation for image caption generation. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2015.
Cho, Kyunghyun, Van Merriënboer, Bart, Gulcehre, Caglar, Bah-
danau, Dzmitry, Bougares, Fethi, Schwenk, Holger, and Ben-
gio, Yoshua. Learning phrase representations using RNN
encoder-decoder for statistical machine translation. In Pro-
ceedings of Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), 2014.
Chorowski, Jan, Bahdanau, Dzmitry, Cho, Kyunghyun, and Ben-
gio, Yoshua. End-to-end continuous speech recognition us-
ing attention-based recurrent NN: First results. arXiv preprint
arXiv:1412.1602, 2014.
Chorowski, Jan K, Bahdanau, Dzmitry, Serdyuk, Dmitriy, Cho,
Kyunghyun, and Bengio, Yoshua. Attention-based models for
speech recognition. In Advances in Neural Information Pro-
cessing Systems (NIPS), 2015.
Degris, Thomas, White, Martha, and Sutton, Richard S. Off-
policy actor-critic. In Proceedings of International Conference
on Machine Learning (ICML), 2012.
Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing,
Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and
Bengio, Yoshua. Generative adversarial nets. In Advances in
Neural Information Processing Systems (NIPS), 2014.
Guu, Kelvin, Pasupat, Panupong, Liu, Evan Zheran, and Liang,
Percy. From language to programs: Bridging reinforcement
learning and maximum marginal likelihood. In Proceedings of
the Annual Meeting of Association for Computational Linguis-
tics (ACL), 2017.
Jean, Sébastien, Cho, Kyunghyun, Memisevic, Roland, and Ben-
gio, Yoshua. On using very large target vocabulary for neural
machine translation. In Proceedings of the Annual Meeting of
Association for Computational Linguistics (ACL), 2015.
Klein, Guillaume, Kim, Yoon, Deng, Yuntian, Senellart, Jean, and
Rush, Alexander M. OpenNMT: Open-source toolkit for neural
machine translation. arXiv preprint arXiv:1701.02810, 2017.
Lamb, Alex, Goyal, Anirudh, Zhang, Ying, Zhang, Saizheng,
Courville, Aaron, and Bengio, Yoshua. Professor forcing: A
new algorithm for training recurrent networks. In Advances in
Neural Information Processing Systems (NIPS), 2016.
Luong, Minh-Thang, Pham, Hieu, and Manning, Christopher D.
Effective approaches to attention-based neural machine trans-
lation. In Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP), 2015.
Neural Sequence Model Training via α-divergence Minimization
Mnih, Volodymyr, Badia, Adria Puigdomenech, Mirza, Mehdi,
Graves, Alex, Lillicrap, Timothy, Harley, Tim, Silver, David,
and Kavukcuoglu, Koray. Asynchronous methods for deep re-
inforcement learning. In Proceedings of International Confer-
ence on Machine Learning (ICML), 2016.
Norouzi, Mohammad, Bengio, Samy, Jaitly, Navdeep, Schuster,
Mike, Wu, Yonghui, Schuurmans, Dale, et al. Reward aug-
mented maximum likelihood for neural structured prediction.
In Advances In Neural Information Processing Systems (NIPS),
2016.
Papineni, Kishore, Roukos, Salim, Ward, Todd, and Zhu, Wei-
Jing. BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the Annual Meeting of Associa-
tion for Computational Linguistics (ACL), 2002.
Ranzato, Marc’Aurelio, Chopra, Sumit, Auli, Michael, and
Zaremba, Wojciech. Sequence level training with recurrent
neural networks. In Proceedings of International Conference
on Learning Representations (ICLR), 2016.
Shen, Shiqi, Cheng, Yong, He, Zhongjun, He, Wei, Wu, Hua, Sun,
Maosong, and Liu, Yang. Minimum risk training for neural
machine translation. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL), 2016.
Snover, Matthew, Dorr, Bonnie, Schwartz, Richard, Micciulla,
Linnea, and Makhoul, John. A study of translation edit rate
with targeted human annotation. In Proceedings of Associa-
tion for Machine Translation in the Americas (AMTA), 2006.
Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to
sequence learning with neural networks. In Advances in Neural
Information Processing Systems (NIPS), 2014.
Sutton, Richard S, McAllester, David A, Singh, Satinder P, and
Mansour, Yishay. Policy gradient methods for reinforcement
learning with function approximation. In Advances in Neural
Information Processing Systems (NIPS), 2000.
Vinyals, Oriol and Le, Quoc. A neural conversational model. In
Proceedings of International Conference on Machine Learning
(ICML), 2015.
Williams, Ronald J. Simple statistical gradient-following al-
gorithms for connectionist reinforcement learning. Machine
Learning, 8(3-4):229–256, 1992.
Wilson, Ashia C, Roelofs, Rebecca, Stern, Mitchell, Srebro,
Nathan, and Recht, Benjamin. The marginal value of adap-
tive gradient methods in machine learning. arXiv preprint
arXiv:1705.08292, 2017.
Wu, Lijun, Xia, Yingce, Zhao, Li, Tian, Fei, Qin, Tao, Lai, Jian-
huang, and Liu, Tie-Yan. Adversarial neural machine transla-
tion. arXiv preprint arXiv:1704.06933, 2017.
Wu, Yonghui, Schuster, Mike, Chen, Zhifeng, Le, Quoc V,
Norouzi, Mohammad, Macherey, Wolfgang, Krikun, Maxim,
Cao, Yuan, Gao, Qin, Macherey, Klaus, et al. Google’s neural
machine translation system: Bridging the gap between human
and machine translation. arXiv preprint arXiv:1609.08144,
2016.
Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun, Courville,
Aaron, Salakhudinov, Ruslan, Zemel, Rich, and Bengio,
Yoshua. Show, attend and tell: Neural image caption gener-
ation with visual attention. In Proceedings of International
Conference on Machine Learning (ICML), 2015.
Yu, Lantao, Zhang, Weinan, Wang, Jun, and Yu, Yong. Seq-
GAN: Sequence generative adversarial nets with policy gra-
dient. In Proceedings of AAAI Conference on Artificial Intelli-
gence (AAAI), 2017.
A. Gradient of α-DiMT objective
The gradient of α-DiMT can be obtained as follows:
∇θL∗(α,τ)(θ)
= ∇θ
{
−
∑
x∈X
τ
α(1− α)
{
1−
∑
y∈Y
pαθ (y|x)q1−α(τ) (y|x)
}}
(16)
= − τ
α(1− α)
∑
x∈X
∑
y∈Y
∇θpαθ (y|x)q1−α(τ) (y|x) (17)
= − τ
1− α
∑
x∈X
∑
y∈Y
pαθ (y|x)q1−α(τ) (y|x)∇θ log pθ(y|x)
(18)
= −
∑
x∈X
∑
y∈Y
p
(α,τ)
θ (y|x)∇θ log pθ(y|x), (19)
where
p
(α,τ)
θ (y|x) =
τ
1− α
pαθ (y|x)q1−α(τ) (y|x). (20)
In Eq. (18), we used the so-called log-trick: ∇θpθ(y|x) =
pθ(y|x)∇θ log pθ(y|x).

