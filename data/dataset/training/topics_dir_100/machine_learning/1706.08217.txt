An Effective Way to Improve YouTube-8M Classification Accuracy in Google
Cloud Platform
Zhenzhen Zhong
zhenzhenzhongise@gmail.com
Shujiao Huang
shujiao.h@gmail.com
Cheng Zhan
zhancheng0511@gmail.com
Licheng Zhang
xiyao.fei@gmail.com
Zhiwei Xiao
zhiwei.xiao@gmail.com
Chang-chun Wang
wither81@gmail.coms
Pei Yang
peiyangdailylife@gmail.com
Abstract
Large-scale datasets have played a significant role
in progress of neural network and deep learning areas.
YouTube-8M is such a benchmark dataset for general multi-
label video classification. It was created from over 7 million
YouTube videos (450,000 hours of video) and includes video
labels from a vocabulary of 4716 classes (3.4 labels/video
on average). It also comes with pre-extracted audio & vi-
sual features from every second of video (3.2 billion feature
vectors in total).
Google cloud recently released the datasets and orga-
nized ‚ÄòGoogle Cloud & YouTube-8M Video Understanding
Challenge‚Äô on Kaggle. Competitors are challenged to de-
velop classification algorithms that assign video-level la-
bels using the new and improved Youtube-8M V2 dataset.
Inspired by the competition, we started exploration of
audio understanding and classification using deep learning
algorithms and ensemble methods. We built several base-
line predictions according to the benchmark paper [4] and
public github tensorflow code. Furthermore, we improved
global prediction accuracy (GAP) from base level 77% to
80.7% through approaches of ensemble.
1. Introduction
There is an English idiom: ‚Äúa picture is worth a thou-
sand words‚Äù. Such theory has been standing in human so-
ciety for many years, as this is the way our brain functions.
With the development of neural network and deep learn-
ing, it can be applied to machine as well. In other words,
we human beings are able to teach or train computer to
recognize objects from pictures and even describe them in
our natural language. Thanks Google for organizing this
‚ÄòGoogle Cloud & YouTube-8M Video Understanding Chal-
lenge‚Äô, which gives us a wonderful opportunity to test new
ideas and implement them with the Google cloud platform.
In this paper, we first review the baseline algorithms, and
then introduce the innovative ensemble experiments.
2. Data
There are two types of data, video-level and frame-
level features. The data and detailed information
are well explained in YouTube-8M dataset webpage
(https://research.google.com/youtube8m/
download.html). Both of the video-level and frame-
level data are stored as tensorflow.Example protocol
buffers, which have been saved as ‚Äòtfrecord‚Äô files.
For each type of the data, it has been split in three sets:
train, validate and test. The numbers of observations in each
dataset are given in following table.
Train Validate Test
Number of Obs 4,906,660 1,401,82 700,640
Video-level data example proto is given in following text
format:
‚Ä¢ ‚Äúvideo id‚Äù: an id string;
‚Ä¢ ‚Äúlabels‚Äù: a list of integers;
Note: the feature for test set is missing, but given for
train and validation datasets.
‚Ä¢ ‚Äúmean rgb‚Äù: a 1024-dim float list;
‚Ä¢ ‚Äúmean audio‚Äù: a 128-dim float list.
A few examples of video-level data are given in follow-
ing table:
4321
ar
X
iv
:1
70
6.
08
21
7v
1 
 [
st
at
.M
L
] 
 2
6 
Ju
n 
20
17
video id (ID) labels (y) mean rbg (X1) mean audio (X2)
‚Äò-09K4OPZSSo‚Äô [66]
[0.11, -0.87,
-0.19, -0.22,
0.23, ¬∑ ¬∑ ¬∑ ]
[0.89, 1.31,
-0.13, 0.20,
-1.76, ¬∑ ¬∑ ¬∑ ]
‚Äò-0MDly IiNM‚Äô [37, 101, 29, 23]
[-0.99, 1.02,
-0.74, 0.09,
0.56, ¬∑ ¬∑ ¬∑ ]
[-0.66, -1.12,
0.61, -1.37,
-0.01, ¬∑ ¬∑ ¬∑ ]
...
Frame-level data has similar ‚Äòvideo id‚Äô and ‚Äòlabels‚Äô fea-
tures, but ‚Äòrgb‚Äô and ‚Äòaudio‚Äô are given in each frame:
‚Ä¢ ‚Äúvideo id‚Äù: e.g. ‚Äò-09K4OPZSSo‚Äô;
‚Ä¢ ‚Äúlabels‚Äù: e.g. [66];
‚Ä¢ Feature list ‚Äúrgb‚Äù: e.g. [[a 1024-dim float list], [a
1024-dim float list],...];
‚Ä¢ Feature list ‚Äúaudio‚Äù: e.g. [[a 128-dim float list],[a 12-
dim float list],...].
Note: each frame represents one second of the video,
which up to 300.
The data can be represented as (Xtrain, ytrain),
(Xval, yval) and (Xtest), where X is the information of
each video (features), and y is the corresponding labels. In
video-level data, X = (mean rgb,mean audio).
3. Baseline Approaches
[4] gave detailed introduction for some baseline ap-
proaches, including logistic regression and mixture of ex-
perts for video-level data, as well as frame-level logistic re-
gression, deep bag of frame and long short-term memory
models for frame-level data.
Given the video-level representations, we train indepen-
dent binary classifiers for each label using all the data. Ex-
ploiting the structure information between the various la-
bels is left for future work. A key challenge is to train these
classifiers at the scale of this dataset. Even with a compact
video-level representation for the 6M training videos, it is
unfeasible to train batch optimization classifiers, like SVM.
Instead, we use online learning algorithms, and use Ada-
grad to perform model updates on the weight vectors given
a small mini-batch of examples (each example is associated
with a binary ground-truth value).
3.1. Models from Video-level Features
The average values of rgb and audio presentation are ex-
tracted from each video for model training. We focus on
performance of logistic regression model and mixture of ex-
perts(MoE) model, which can be trained within 2 hours on
Google cloud platform.
3.1.1 Logistic Regression
Logistic regression computes the weighted entity similarly
to linear regression, and obtains the probability by output
logistic results [1]. Its cost function is called as log-loss:
Œª||We||22 +
N‚àë
i=1
L(yi,e, œÉ(WTe xi)), (1)
where œÉ(¬∑) is the standard logistic, œÉ(z) = 1/(1 +
exp(‚àíz)). The optimal weights can be found with gradient
descent algorithm.
3.1.2 Mixture of Experts (MoE)
Mixture of experts (MoE) was first proposed by Jacobs and
Jordan [3]. Given a set of training examples (xi, gi), i =
1...N for a binary classifier, where xi is the feature vector
and gi ‚àà [0, 1] is the ground-truth, let L(pi, gi) be the log-
loss between the predicted probability and the ground-truth:
L(p, g) = ‚àíg log p‚àí (1‚àí g) log(1‚àí p). (2)
Probability of each entity is calculated from a softmax dis-
tribution of a set of hidden states. Cost function over all the
datasets is log-loss.
Video-level models trained with RGB feature achieve
70% - 74% precision accuracy. Adding audio feature raises
the score to 78% precision accuracy. We also tested includ-
ing validation dataset as part of training datasets. With this
larger training set, prediction accuracy on tests set is 2.5%
higher than using original training sets only for model fit-
ting. In other words, we used 90% of YouTube-8M datasets
to obtain better model parameters.
3.2. Models from Frame-level Features
Videos are decoded at one frame-per-second to extract
frame-level presentations. In our experiments, most frame-
level models achieve similar performance as video-level
models.
3.2.1 Frame-level Logistic Model
The frame-level features in the training dataset are obtained
by randomly sampling 20 frames in each video. Frames
from the same video are all assigned the ground-truth of the
corresponding video. There are totally about 120 million
frames. So the frame-level features are:
(xi, y

i ),  = 1, ..., 4800, i = 1, ¬∑ ¬∑ ¬∑ , 120M
where xi ‚àà R1024 and yi ‚àà {0, 1}.
The logistic models are trained in ‚Äúone-vs-all‚Äù sense;
hence there are totally 4800 models. For inference on test
4322
data, we compute the probability of existence of label e in
each video ŒΩ as follows
pŒΩ(e|xŒΩ1:FŒΩ ) =
1
FŒΩ
FŒΩ‚àë
j=1
pŒΩ(e|xŒΩj ), j = 1, ¬∑ ¬∑ ¬∑ , FŒΩ , (3)
where FŒΩ is the number of frames in a video. That is, video-
level probabilities are obtained by simply averaging frame-
level probabilities.
3.2.2 Deep Bag of Frame (DBoF) Pooling
Deep bag of frame model is a convolutional neural net-
work. The main idea is to design two layers in the con-
volutional part. In the first layer, the up-projection layer,
the weights are still applied on frames, although all selected
frames share the same parameter. The second layer is pool-
ing the previous layer into video level. The approach enjoys
the computational benefits of CNN, while at the same time
the weights on the up-projection layer can still provide a
strong representation of input features on frame level.
For implementation and test, more features and input
data can slightly improve the results. For example, if we
combined training and validate data, the score will be im-
proved. When we add both features (RGB + audio), the
result is boosted by around 0.4%. The computing cost is
quite low compared to other frame level models. It took 36
hours using one single GPU for training data. The bench-
mark model is not well implemented for parallel computing
in the prediction stage. Using more GPUs doesn‚Äôt boost
training speed. We tested 4 GPU, and the total time is only
reduced by 10%.
3.2.3 Long Short-Term Memory (LSTM) Model
Trained from Frame-Level Features
Long short-term memory (LSTM) [5] is a recurrent neu-
ral network (RNN) architecture. In contrast to conventional
RNN, LSTM uses memory cells to store, modify, and access
internal state, allowing it to better discover long-range tem-
poral relationships. As shown in Figure 1 ([6]) the LSTM
cell stores a single floating point value and it maintained the
value unless it is added to by the input gate or diminished
by the forget gate. The emission of the memory value from
the LSTM cell is controlled by the output gate.
The hidden layerH of the LSTM is computed as follows
[6]:
it = œÉ(WxiXt +Whiht‚àí1 +Wcict‚àí1 + bi) (4)
ft = œÉ(WxfXt +Whfht‚àí1 +Wcfct‚àí1 + bf ) (5)
ct = ftct‚àí1 + it tanh(WxcXt +Whcht‚àí1 + bc)(6)
ot = œÉ(Wxoxt +Whoht‚àí1 +Wcoct + bo) (7)
ht = ot tanh(ct) (8)
Figure 1.
where x denote the input, W denote weight matrices (e.g.
the subscript hi is the hidden-input weight matrix), b terms
denote bias vectors, œÉ is the logistic sigmoid function, and
i, f, o, and c are respectively the input gate, forget gate, out-
put gate, and cell activation vectors.
In the current project, the LSTM model was build fol-
lowed a similar approach to [6]. Provided best performance
on the validation set, 2 stacked LSTM layers with 1024 hid-
den units and 60 unrolling iterations were used [4].
4. Ensemble Approaches
Several predictions on the base level have been gener-
ated. Most of the models perform reasonably well, and ag-
gregating the predictions even better results. It is known
as ensemble learning to combine the base level models and
train a second level to improve the prediction.
There are several approaches of ensemble learning, such
as blending, averaging, bagging, voting, etc. Where blend-
ing ([2]) is a powerful method for model ensemble. Aver-
aging, the simple solution, works well, and is also applied
in this case. Bagging (short for bootstrap aggregating) is
to train the same models on different random subsets of the
training sets, and aggregating the results. The improvement
from bagging could be limited. Majority voting is not ap-
propriate in this case, since the outputs here are the confi-
dence level probabilities instead of label ids.
4.1. Blending
A detailed introduction for popular kaggle ensem-
bling methods are given in https://mlwave.com/
kaggle-ensembling-guide. Define f1, f2, ¬∑ ¬∑ ¬∑ be
different classification models, like logistic and MoE in this
case.
The general idea for blending method is given as follows:
‚Ä¢ Create a small holdout set, like 10% of the train set.
‚Ä¢ Build the prediction model with rest of the train set.
‚Ä¢ Train the stacker model in this holdout set only.
4323
Training the blend model for the YouTube-8M data re-
quires large computational memories. Google cloud plat-
form provides sufficient memory and computing resources
for blending. We tried this blending method in video-level
logistic and MoE base models with validation set as the
holdout set.
‚Ä¢ Build the logistic and MoE model on video-level train
data, ytrain ‚àº Xtrain.
‚Ä¢ Do the inference and print the output predic-
tion on validation set (yÃÇlogisticval , yÃÇ
moe
val ) and test set
(yÃÇlogistictest , yÃÇ
moe
test ).
‚Ä¢ Predictions on dataset p (p = val or test) with model q
(q = logistic or MoE), yÃÇqp, gives top 20 predictions and
their probabilities, e.g.,
VideoId LabelConfidencePairs
100011194
1 0.991708 4 0.830637 1833 0.781667
2292 0.730538 297 0.718730 3547 0.465280
34 0.396639 1511 0.371649 2 0.351788
0 0.303522 92 0.169908 933 0.164513
198 0.145657 202 0.143494 658 0.106776
74 0.089043 167 0.088266 33 0.052943
332 0.049101 360 0.045714
100253546
77 0.996484 21 0.987201 142 0.971881
59 0.931193 112 0.817585 0 0.445608
8 0.112624 11 0.100307 17 0.025623
262 0.021074 1 0.020778 312 0.020060
75 0.017796 57 0.011925 60 0.005532
67 0.004512 69 0.004346 575 0.004044
3960 0.003965 710 0.003961
...
In order to apply second stage stacker model. We de-
fine stacking new feature Xq‚àóp to be a 4716-dimension
vector. Each dimension represents a label with cor-
responding probability. The stacking new feature has
default value 0, and the 20 estimated probabilities are
defined. For simplicity, if there are 3 estimated label
with probabilities, the new defined feature is given as
following:
(yÃÇqp)i = [1 0.99 4 0.83 5 0.78]
‚áí
(Xq‚àóp )i = (0.99, 0, 0, 0.83, 0.78, 0, ¬∑ ¬∑ ¬∑ , 0)
where i = 1, ¬∑ ¬∑ ¬∑ , 1401828 if p =val, i =
1, ¬∑ ¬∑ ¬∑ , 700640 if p =test. Write the new defined stack-
ing feature to be X‚àóval = (X
logistic‚àó
val , X
moe‚àó
val ) and
X‚àótest = (X
logistic‚àó
test , X
moe‚àó
test ).
‚Ä¢ Stacker model is hard to run in local machine with new
defined feature X‚àóval, since it costs too much memo-
ries. Hence, we save the new defined data as tfrecord
file, so that the model can be run in Google cloud. The
new defined data example proto is given as following:
a. ‚Äúvideo id‚Äù
b. ‚Äúlabels‚Äù
c. ‚Äúlogistic newfeature‚Äù: float array of length 4716
d. ‚Äúmoe newfeature‚Äù: float array of length 4716
‚Ä¢ Train the stacker model in the validation new feature
X‚àóval, yval ‚àº X‚àóval. In this case, we use only logistic
and MoE as stacker model, since these two are well-
defined in th package. Other models need to be defined
if desire to use.
‚Ä¢ Do the inference on the test set X‚àótest, hence final pre-
diction yÃÇ‚àótest.
Note since the new defined feature has length 4716 in-
stead of 1024, train.py and test.py in Google cloud script
need some small correction to apply this method. Defined
feature names and feature sizes need to match with new de-
fined feature name and corresponding length. By adding the
new features, the score has been improved from 0.760 to
0.775, see Table 1.
4.2. Averaging
The idea of averaging is to generate a smooth separation
between different predictors and reduce over-fitting. For
each video, if one label is predicted several times among
these models, the mean value of these confidence level val-
ues is used as the final prediction. On the other hand, if
a label id is rarely predicted among the base models, the
final prediction is calculated using the sum of these confi-
dence level values divided by the number of base models,
eventually lower the confidence level. On a computer with
8G physical memories, 30G virtual memory, the averaging
process for the whole test dataset, which includes 700640
YouTube video records, can be finished within 15 minutes,
which is quite efficient considering the size of files.
Several strategies are tested for the averaging:
Strategy A: 3 frame level models, 2 video level mod-
els, 2 blending models (video level logistic model blending
with MoE prediction, video level MoE model blending with
MoE prediction), the GAP score is 0.80396 after averaging,
higher than the score of any individual base predictions.
Strategy B: 3 frame level models, 2 video level models, 2
blending models (video level logistic model blending with
both logistic and MoE prediction, video level MoE model
blending with both logistic and MoE prediction). The base-
ment of the 2 blending predictions are the 2 video level
models, thus the 2 blending models are highly correlated
with the 2 video level models. The video level logistic
model is a weaker predictor than video level MoE predictor.
4324
Figure 2. Stacking Diverse Predictors Flow Chart
The usage of logistic prediction blended models increases
the weight of this weaker predictor. Therefore, the GAP
score is 0.80380 after averaging, slightly lower than Strat-
egy A.
Strategy C: the base models are the same as in strategy
A, except that the deep bag of frame pooling frame model
prediction is replaced by the one generated after fine-tuning
the hyper-parameters. Including the individual model with
better performance, the score after averaging is improved to
0.80424.
Strategy D: the base models in strategy D is the same
as strategy C, but the weight of the blending model (video
level MoE model blending with both logistic and MoE pre-
diction) is increased to 2. Among all the base models, the
GAP score of the MoE blending model is the highest. The
score rises to 0.80692 with the weighted average strategy.
Strategy E: similar to strategy D, the weight of the MoE
blending model is 2. The logistic model blending with lo-
gistic plus MoE prediction is replaced by the logistic model
blending only with MoE prediction, which reduces the lo-
gistic components compared to strategy D. This approach
yields the best GAP score, 0.80695.
Such method boosts the final result by 7-8%, quite re-
markable considering the simplicity nature. Table 1 shows
results of all base models, where the scores are calculate
using GAP metrics on Kaggle platform.
5. Conclusion
Utilizing the open resource of Youtube-8M train and
evaluation datasets, we trained baseline models in a fast-
track fashion. Video-level representations are trained with
a) logistic models; b) MoE model, while frame-level fea-
tures are trained with a) LSTM model; b) Dbof model; c)
frame-level logistic model. We also demonstrated the effi-
ciency of blending and averaging to improve the accuracy
Table 1. Predictions from Individual Base Models
Models Features Datasets for model fitting Score
Frame Level
LSTM Model rgb validate 0.7457
Frame Level
Deep Bag of
Pooling Model I
audio train+validate 0.77
Frame Level
Deep Bag of
Pooling Model II
audio train 0.767
Video Level
Logistic Model audio/rgb train+validate 0.76036
Video Level
Mixture of
Experts Model
audio/rgb train+validate 0.78453
Video Level
Logistic Model
Blending I
audio/rgb/logistic/moe validate 0.77518
Video Level
Logistic Model
Blending II
audio/rgb/moe validate 0.76873
Video Level
Mixture of
Experts Model
Blending
audio/rgb/logistic/moe validate 0.78617
of prediction. Blending plays a key role to raise the perfor-
mance of baseline. We are able to train the blender model
on Google cloud, where the computer resource can digest
thousands of features. The averaging solution, in addition,
aggregates the wisdom from all predictions with low com-
puter cost.
References
[1] Aurelien Geron, Hands on Machine Learning with Scikit
Learn and Tensorflow.
[2] A. Toscher and M. Jahrer, The bigchaos solution to the netflix
grand prize, 2009.
[3] Jordan, Michael I and Jacobs, Robert A, Hierarchical mix-
tures of experts and the EM algorithm, Neural computation,
6, 2, 181-214, 1994.
[4] Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok
and Natsev, Paul and Toderici, George and Varadarajan, Bal-
akrishnan and Vijayanarasimhan, Sudheendra, Youtube-8m:
A large-scale video classification benchmark, arXiv preprint
arXiv:1609.08675, 2016.
[5] Hochreiter, Sepp and Schmidhuber, JuÃàrgen, Long short-term
memory, Neural Computation, 9:8, 1735-1780, 2997.
[6] Yue-Hei Ng, Joe and Hausknecht, Matthew and Vijaya-
narasimhan, Sudheendra and Vinyals, Oriol and Monga, Rajat
and Toderici, George, Beyond short snippets: Deep networks
for video classification, Proceedings of the IEEE conference
on computer vision and pattern recognition, 4694‚Äì4702, 2015.
4325

