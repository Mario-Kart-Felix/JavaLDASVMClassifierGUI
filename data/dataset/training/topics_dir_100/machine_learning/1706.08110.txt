ar
X
iv
:1
70
6.
08
11
0v
1 
 [
st
at
.M
L
] 
 2
5 
Ju
n 
20
17
Noname manuscript No.
(will be inserted by the editor)
Matrix Hilbert Space
Yunfei Ye
Received: date / Accepted: date
Abstract Theoretical studies have proven that the Hilbert space has remarkable
performance in many fields of applications. Frames in tensor product of Hilbert
spaces were introduced to generalize the inner product to high-order tensors. How-
ever, these techniques require tensor decomposition where information could be
lost and it is a NP-hard problem to determine the rank of tensors. Here, we present
a new framework, namely matrix Hilbert space to perform a matrix inner product
space when data observations are represented as matrices. We put forward theo-
retical analysis, including several basic inequalities, orthogonality and orthonor-
mal basis. In addition, we extend the reproducing kernel Hilbert space (RKHS)
to reproducing kernel matrix Hilbert space (RKMHS) and propose an equivalent
condition of the space uses of the certain kernel function. A new family of kernels
is introduced in our framework and comparative experiments are performed on a
number of real-world datasets to support our contributions.
Keywords Hilbert space · matrix Hilbert space · reproducing kernel matrix
Hilbert space · kernel function
1 Introduction
The Hilbert space was named after David Hilbert for his fundamental work to gen-
eralize the concept of Euclidean space to an infinite dimensional one in the field
of functional analysis. With other related works, they have made great contribu-
tion in the development of quantum mechanics (Birkhoff and Von Neumann, 1936;
Sakurai et al, 1995; Ballentine, 2014), partial differential equations (Crandall et al,
1992; Gustafson, 2012; Gilbarg and Trudinger, 2015), Fourier analysis (Stein and Weiss,
2016), spectral theory (Birman and Solomjak, 2012), etc. Many methodologies
have been proposed in the literature, but they often refer to situations where
Yunfei Ye
Department of Mathematical Sciences, Shanghai Jiao Tong University
800 Dongchuan RD Shanghai, 200240 China
E-mail: tianshapojun@sjtu.edu.cn
2 Yunfei Ye
spaces are infinite vector spaces in assumption. Since it is more natural to rep-
resent real-world data as high-order tensors, tensor product has become useful in
approximating such variables. Khosravi and Asgari (Asgari and Khosravi, 2003)
introduced frames in tensor product of Hilbert spaces. It is an extension of tensor
product to construct a new Hilbert space of higher order tensors with several ex-
isting Hilbert spaces. Meanwhile, bases and frames in Hilbert C∗-modules with a
C∗-algebra were investigated (Lance, 1995). Tensor product of frames for Hilbert
modules produce frames for a new Hilbert module (Khosravi and Khosravi, 2007).
Based on this framework, a reproducing kernel Hilbert space (RKHS) of func-
tions was proposed and proven essential in a number of applications, such as
signal processing and detection, as well as statistical learning theory. The repro-
ducing kernel was systematically developed in the early 1950s by Nachman Aron-
szajn (Aronszajn, 1950) and Stefan Bergman. The notion of kernels in Hilbert
spaces wasn’t brought to the field of machine learning until 20th century (Wahba,
1990; Schölkopf et al, 1998; Vapnik and Vapnik, 1998; Boser et al, 1992). The ker-
nel trick expands theories and algorithms well developed for the linear cases to
nonlinear methods to detect the kind of dependencies that allow successful pre-
diction of properties of interest (Hofmann et al, 2008).
Most of standard kernels use tensor decomposition to reveal the underlying
structure of tensor data (Signoretto et al, 2011; He et al, 2014). Existing tensor-
based techniques consist of seeking representative low-dimensional subspaces or
sum of rank-1 factor. However, information could be lost in this procedure and
it is a NP-hard problem to determine the rank of tensors. Another matrix-based
approach (Gao et al, 2015) reformulates the Support Tensor Machine (STM) clas-
sifier where a matrix representation (Gao and Wu, 2012) was applied in the con-
struction of kernel function. Its improvement in the performance of classification
problems attributes to the matrix kernel function which describes the inner prod-
uct. Inspired by the above work, we study a new framework in this paper, namely
matrix Hilbert space to perform inner product when data observations are repre-
sented as matrices. We exploit matrix inner product to describe similarity among
matrix objects. This includes in particular the case of Hilbert space where the prop-
erties of the scalar inner product are generalized. In addition, we systematically
explain the matrix integral based on matrix polynomials (Sinap and Van Assche,
1994) upon our work. We begin by presenting the framework of matrix Hilbert
spaces followed by several inequalities and properties. We then define the orthog-
onality and orthogonal basis in a natural way to span the whole matrix space. To
this end we develop tools extending to our matrix Hilbert space the concept of
reproducing kernel matrix Hilbert space (RKMHS).
The remainder of the article is structured as follows. In Sect. 2 we present the
framework of matrix Hilbert spaces combined with basic inequalities and proper-
ties. In Sect. 3, extended definitions of orthogonality and orthogonal basis to our
space are described, including revised Reisz representation theorem and the exis-
tence of orthonormal basis for every matrix Hilbert space. In Sect. 4 we present
definitions of reproducing kernel and corresponding reproducing kernel matrix
Hilbert space. In Sect. 5 a new family of kernels is described based on the frame-
work of RKMHS and its performance on benchmark datasets. Finally, we present
concluding remarks in Sect. 6.
Matrix Hilbert Space 3
2 Matrix Hilbert Space
In this section, we present the framework of matrix Hilbert spaces which extends
the scalar inner product to a matrix form. One advantage is that it is a natural
generalization of Hilbert space of vectors; this is the case especially when the
dimension of the matrix inner product is one by one. Meanwhile, we reformulate
some fundamental inequalities such as Cauchy-Schwarz inequality in our new space
and obtain some good results.
In this study, scales are denoted by lowercase letters, e.g., s, vectors by boldface
lowercase letters, e.g., v, matrices by boldface capital letters, e.g., M and general
sets or spaces by gothic letters, e.g., B. We start with some basic notations defined
in the literature.
The Frobenius norm of a matrix X ∈ Rm×n is defined by
‖X‖F =
√√√√
m∑
i1=1
n∑
i2=1
x2i1,i2 ,
which is a generalization of the normal l2 norm for vectors.
The inner product of two same-sized matrices X,Y ∈ Rm×n is defined as the
sum of products of their entries, i.e.,
〈X,Y〉 =
m∑
i1=1
n∑
i2=1
xi1,i2yi1,i2 .
Now we present our framework of matrix inner product as follows.
Definition 1 (Matrix Inner Product) Let H be a complex linear space, the
matrix inner product with respect to a Hermitian matrix W ∈ Cn×n is a mapping
〈·, ·〉H : H×H → C
n×n satisfying the following properties
(1) 〈Y,X〉H = 〈X,Y〉
∗
H, where A
∗ = A
⊺
(2) 〈λX1 + µX2,Y〉H = λ〈X1,Y〉H + µ〈X2,Y〉H
(3) 〈〈X,X〉H,W〉 ≥ 0 , where the case of equality holds precisely when X is a
zero element, 〈·, ·〉 is the product of two matrices defined above.
Naturally, we can define the norm as ‖X‖H = (
〈〈X,X〉H,W〉
‖W‖F
)
1
2 and d(X,Y) =
‖X − Y‖H. Property (3) is a weak condition for the positive definiteness of
〈X,X〉H. Furthermore, if 〈〈X,X〉H,W〉 > 0 for all nonzero W as Wi,j = wiwj
(i, j = 1, . . . , n, wi ∈ C), then 〈X,X〉H is positive definite.
For example, 〈X,Y〉H = X
⊺Y with W = In×n is a simple case for the matrix
inner product of complex matrix space H = Cm×n. It simplifies to scalar inner
product when n = 1 and W = [1].
A matrical inner product on Rn×n[x] defined by the matrix integral can be
represented as
〈P (x),Q(x)〉H =
∫ b
a
P (x)W (x)Q(x)⊺dx (1)
where W (x) is a weight matrix function, p(x) and Q(x) are polynomials in a real
variable x whose coefficients are n×n matrices (Sinap and Van Assche, 1994). This
can be applied in our framework as a special case where properties in Definition 1
are satisfied.
4 Yunfei Ye
Definition 2 (Convergence and Limit) Let (H, 〈·, ·〉H) be a matrix inner prod-
uct space with respect toW. We say that {Xi ∈ H}
∞
i=1 converges toX with respect
to W, written limi→∞ Xi
W
= X, if
lim
i→∞
‖Xi −X‖H = 0.
Definition 3 (Cauchy Sequence) A sequence {Xi}
∞
i=1 of a matrix inner prod-
uct space (H, 〈·, ·〉H) with respect to W is called a Cauchy sequence, if for every
real number ǫ > 0, there is a positive integer N such that for all p, q > N ,
‖Xp −Xq‖H < ǫ.
Definition 4 (Complete Space and Closed Space) A matrix inner product
space (H, 〈·, ·〉H) with respect to W is called complete if every Cauchy sequence
in H converges in H. It is closed if H contains all its limit points.
Definition 5 A matrix Hilbert space is a complete matrix inner product space.
In particular, every matrix Hilbert space is a Banach space with respect to the
norm defined above.
Now we introduce some fundamental inequalities on the matrix Hilbert space.
Theorem 1 Let (H, 〈·, ·〉H) be a matrix inner product space with respect to W
then
(1) (Cauchy-Schwarz Inequality)
|
〈〈X,Y〉H,W〉
‖W‖F
| ≤ ‖X‖H‖Y‖H. (2)
(2) (Triangle Inequality)
‖X+Y‖H ≤ ‖X‖H + ‖Y‖H. (3)
Proof (1) By the nonnegativity of the matrix inner product, we have
〈〈X− λY,X− λY〉H,W〉 ≥ 0
⇒λ〈〈Y,X〉H,W〉+ λ〈〈X,Y〉H,W〉 ≤ 〈〈X,X〉H,W〉+ |λ|
2〈〈Y,Y〉H,W〉.
If 〈〈Y,X〉H,W〉 = re
iϕ, where r = |〈〈Y,X〉H,W〉| and ϕ = arg(〈〈Y,X〉H,W〉),
then we choose λ = ‖X‖H‖Y‖H e
−iϕ. It follows that
2
‖X‖H
‖Y‖H
|〈〈Y,X〉H,W〉| ≤ 2‖X‖
2
H‖W‖F .
Notice that 〈〈Y,X〉H,W〉 = 〈〈X,Y〉H,W〉 which proves the result.
(2) For any X ∈ H, ‖X‖H ≥ 0, we have
‖X+Y‖H ≤ ‖X‖H + ‖Y‖H
⇔‖X+Y‖2H ≤ (‖X‖H + ‖Y‖H)
2
⇔2Re(
〈〈X,Y〉H,W〉
‖W‖F
) ≤ 2‖X‖H‖Y‖H
which can derived from the Cauchy-Schwarz inequality.
Matrix Hilbert Space 5
Theorem 2 (Parallelogram Law) Let (H, 〈·, ·〉H) be a matrix inner product
space with respect to W then
‖X+Y‖2H + ‖X−Y‖
2
H = 2‖X‖
2
H + 2‖Y‖
2
H (4)
for all X,Y ∈ H.
Proof We have the following fundamental computations:
‖X+Y‖2H + ‖X−Y‖
2
H = ‖X‖
2
H + ‖Y‖
2
H + 2Re(
〈〈X,Y〉H,W〉
‖W‖F
) + ‖X‖2H
+ ‖Y‖2H − 2Re(
〈〈X,Y〉H,W〉
‖W‖F
) = 2‖X‖2H + 2‖Y‖
2
H.
In vector case, an inner product space can be derived from the norm if the
parallelogram law holds. However, we cannot repeat that process because the inner
product defined here is a matrix instead.
Definition 6 (Continuity) Let (H, 〈·, ·〉H) be a matrix inner product space with
respect to W. We define a mapping from H×H → G : (X1,X2) → f(X1,X2). We
say that the mapping is continuous, if for all (X,Y) ∈ H×H and {(Xi,Yi)
∞
i=1 ∈
H×H| limi→∞ Xi
W
= X, limi→∞ Yi
W
= Y}
lim
i→∞
〈f(Xi,Yi)− f(X,Y),W〉
‖W‖F
= 0.
Proposition 1 The matrix inner product 〈·, ·〉H with respect to W is a continuous
mapping from H×H → G.
Proof For all (X,Y), {(Xi,Yi)}
∞
i=1 ∈ H×H satisfying the following: limi→∞ Xi
W
=
X, limi→∞ Yi
W
= Y, we have
|
〈〈Xi,Yi〉H − 〈X,Y〉H,W〉
‖W‖F
| = |
〈〈Xi −X,Yi〉H + 〈X,Yi −Y〉H,W〉
‖W‖F
|
≤ |
〈〈Xi −X,Yi〉H,W〉
‖W‖F
|+ |
〈〈X,Yi −Y〉H,W〉
‖W‖F
|
≤ ‖Xi −X‖H‖Yi‖H + ‖X‖H‖Yi −Y‖H → 0.
The last inequality holds because of the Cauchy-Schwarz inequality.
3 Orthogonality and Orthogonal Basis
So far we have elaborated on the general framework of matrix Hilbert spaces; in
this section we introduce concepts of orthogonality and orthogonal basis due to the
structure of matrix inner product. Then we present the revised Riesz representation
theorem, which makes it possible to construct a conjugate isometric isomorphism
for each element in the dual space. We show that every matrix Hilbert space has
an orthonormal basis, which contributes to express matrices and linear subspaces
of a matrix Hilbert space in a geometric way.
6 Yunfei Ye
3.1 Orthogonality
The orthogonality of vectors allows us to span the whole vector space through
orthonormal basis and gives the detailed structure of it. Inspired by the significance
of this concept, we modify and apply it to our matrix Hilbert space.
Definition 7 Let (H, 〈·, ·〉H) be a matrix inner product space with respect to
W. For X,Y ∈ H, we say that X and Y are orthogonal, written X⊥Y, if
〈〈X,Y〉H,W〉 = 0. We say that subsets A and B are orthogonal, written A⊥B, if
X⊥Y for all X ∈ A and Y ∈ B. The orthogonal complement A⊥ of a subset A is
the set of elements orthogonal to A.
Definition 8 If M and N are orthogonal closed linear subspaces of a matrix
Hilbert space H, then we define the orthogonal direct sum, simply the direct sum,
M⊕N as
M⊕N = {M+N|M ∈ M,N ∈ N}.
Proposition 2 Let (H, 〈·, ·〉H) be a matrix inner product space with respect to W
and A is its subset. If X ∈ A ∩A⊥, then X is a zero element.
Proof X ∈ A⊥, so 〈〈X,Y〉H,W〉 = 0 for every Y ∈ A. In addition, X ∈ A, which
shows that 〈〈X,X〉H,W〉 = 0. By the definition of norm, we can derive that X is
a zero element.
Theorem 3 The orthogonal complement of a subset of a matrix Hilbert space is
a closed linear subspace.
Proof Let H be a matrix Hilbert space with respect to W and A is its subset. If
X,Y ∈ A⊥ and λ, µ ∈ C, then
〈〈λX+ µY,Z〉H,W〉 = λ〈〈X,Z〉H,W〉+ µ〈〈Y,Z〉H,W〉 = 0
for all Z ∈ A. Therefore, A⊥ is a linear subspace.
To show that A⊥ is closed, we need to prove that for every convergence se-
quence {Xi}
∞
i=1 ∈ A
⊥ with limi→∞ Xi
W
= X, X is also in A⊥. Remember H is
complete so X is in H. Let Y ∈ A, from Proposition 1 the matrix inner product
is continuous and therefore
〈〈X,Y〉H,W〉 = 〈〈 lim
i→∞
Xi,Y〉H,W〉 = lim
i→∞
〈〈Xi,Y〉H,W〉 = 0
which shows that X ∈ A⊥.
Now we introduce the definition of dual space and give the revised Reisz rep-
resentation theorem on our matrix Hilbert space.
Definition 9 (Dual Space) Let (H, 〈·, ·〉H) be a matrix inner product space
with respect to W. Its dual space H∗is a linear mapping from H → C, then for
any f ∈ H∗, its norm is defined as
‖f‖H∗ = sup
X 6=0
|f(X)|
‖X‖H
.
Matrix Hilbert Space 7
Theorem 4 (Revised Reisz Representation Theorem) Let H∗ be the dual
space of H. The mapping
X ∈ H
f
→
〈〈·,X〉H,W〉
‖W‖F
∈ H∗ (5)
is a conjugate linear isometric isomorphism.
Proof The mapping is conjugate linear by the property of matrix inner product.
For any given X ∈ H, we write the mapping as fX. Moreover, from Theorem 1,
we can obtain that
|fX(Y)| ≤ ‖X‖H‖Y‖H,
so ‖fX‖H∗ = ‖X‖H. Therefore, f is isometric.
To show that the mapping in injective, we suppose that for X,Y ∈ H, fX(Z) =
fY(Z) for all Z ∈ H. Moreover, fX(X−Y) = fY(X−Y), which implies
〈〈X−Y,X−Y〉H,W〉 = 0.
So X = Y from the axioms of matrix inner product.
To show that the mapping is surjective, let g ∈ H∗ which we assume without
loss of generality is non-zero. Otherwise, X is the zero element. We define a subset
of H as M = {X ∈ H|g(X) = 0}. Then choose Z ∈ M⊥\{0} such that ‖Z‖H = 1.
It exists because g is non-zero. For any X0 ∈ H, g(X0)Z − g(Z)X0 ∈ M and we
can obtain
〈〈g(X0)Z− g(Z)X0,Z〉H,W〉 = 0
⇒g(X0) =
〈〈X0, g(Z)Z〉H,W〉
‖W‖F
which shows that g = f
g(Z)Z
.
Then we extend the definition of operator to matrix form. It is a mapping
between two matrix Hilbert spaces. The equivalence relation between bounded
operator and continuous operator holds in our case.
Definition 10 (Operator) Let (H, 〈·, ·〉H) and (G, 〈·, ·〉G) be matrix Hilbert spaces
with respect to W. Any mapping from H to G is called an operator. An operator
f is called linear if
f(λX+ µY) = λf(X) + µf(Y)
for all X,Y ∈ H and λ, µ ∈ C.
Definition 11 Let (H, 〈·, ·〉H) and (G, 〈·, ·〉G) be matrix Hilbert spaces with re-
spect to W.
(1) (Bounded Operator) An operator f is called a bounded operator if there
exists some M > 0 such that for all X ∈ H,
‖f(X)‖G ≤ M‖X‖H.
Moreover, if f is bounded, the norm is f is defined as ‖f‖ = sup
X 6=0
‖f(X)‖G
‖X‖H
.
(2) (Continuous Operator) An operator f is called a continuous operator if for
every convergence sequence {Xi}
∞
i=1 ∈ H with limi→∞ Xi
W
= X,
lim
i→∞
‖f(X)− f(Xi)‖G = 0.
which implies f(X) = f(Xi).
8 Yunfei Ye
Proposition 3 Let (H, 〈·, ·〉H), (G, 〈·, ·〉G) be matrix Hilbert spaces with respect to
W and f is a linear operator. Then f is a linear bounded operator if and only if f
is a linear continuous operator.
Proof If f is a linear bounded operator, then for any X,Y ∈ H
‖f(X−Y)‖G ≤ M‖X−Y‖H.
With limi→∞ Xi
W
= X, for all ǫ > 0, ∃n ∈ N, then for all i > n, ‖Xi −X‖H <
ǫ
M
, so
‖f(Xi −X)‖G ≤ M‖Xi −X‖H < ǫ.
which shows that f is a linear continuous operator.
On the other hand, if f is a linear continuous operator, then it is continuous
at the zero point. So for all ǫ > 0,∃η > 0, for all X ∈ H satisfying ‖X‖H <
η, ‖f(X)‖G < ǫ. Then for all X ∈ H
‖f(X)‖G =
2‖X‖H
η
‖f(
ηX
2‖X‖H
)‖G <
2ǫ
η
‖X‖H
which shows that f is a linear bounded operator.
The following theorem expresses the existence of the adjoint operator for every
linear bounded operator in a matrix Hilbert space with respect to W.
Theorem 5 (Adjoins) Let (H, 〈·, ·〉H), (G, 〈·, ·〉H) be matrix Hilbert spaces with
respect to W and f : H → G be a bounded operator. Then there exists a unique
bounded operator f∗ : G → H such that
〈〈f(X),Y〉G ,W〉 = 〈〈X, f
∗(Y)〉H,W〉
for all X ∈ H,Y ∈ G.
Proof For each Y ∈ G, the mapping X → 〈〈f(X),Y〉G ,W〉‖W‖F is in H
∗. From Theorem
4, there exists a unique matrix Z ∈ H so that
〈〈f(X),Y〉G ,W〉
‖W‖F
=
〈〈X,Z〉H,W〉
‖W‖F
for all X ∈ H. Then we define mapping f∗(Y) = Z. Moreover, we need to prove
that f∗ is linear and bounded.
To show that f is linear, let Y1,Y2 ∈ G, then for any X ∈ H and λ, µ ∈ C
〈〈f(X), λY1 + µY2〉G ,W〉 = λ〈〈f(X),Y1〉G ,W〉+ µ〈〈f(X),Y2〉G ,W〉
= λ〈〈X, f∗(Y1)〉H,W〉+ µ〈〈X, f
∗(Y2)〉H,W〉
= 〈〈X, λf∗(Y1) + µf
∗(Y2)〉H,W〉
= 〈〈X, f∗(λY1 + µY2)〉H,W〉
and by the uniqueness of f∗(λY1 +µY2), f
∗(λY1 +µY2) = λf
∗(Y1)+µf
∗(Y2).
Matrix Hilbert Space 9
In addition, the following arguments show the relation between the norms of
f and f∗,
‖f∗‖ = sup
‖Y‖G=1
‖f∗(Y)‖H = sup
‖Y‖G=1
sup
‖X‖H=1
|
〈〈X, f∗(Y)〉H,W〉
‖W‖F
|
= sup
‖X‖H=1
sup
‖Y‖G=1
|
〈〈f(X),Y〉G ,W〉
‖W‖F
| = sup
‖X‖H=1
‖f(X)‖G
= ‖f‖
which shows that f and f∗ have the same norm. Hence, f∗ is a bounded norm.
Then we present one of the fundamental geometrical properties of matrix
Hilbert space. It helps to find the distance between a point and a subspace.
Theorem 6 (Projection) Let M be a closed linear subspace of a matrix Hilbert
space H with respect to W,
(1) For each X ∈ H, there exists a unique point Y ∈ M such that
‖X−Y‖H = inf
Z∈M
‖X− Z‖H.
(2) The point Y ∈ M is the unique element in M with the property that
X−Y⊥M.
Proof (1) Let d be the distance of X from M as d = infZ∈M ‖X − Z‖H. First,
we prove the existence of Y that reaches this infimum. From the definition of d,
there is a sequence {Yi}
∞
i=1 such that
lim
i→∞
‖X−Yi‖H = d.
Thus, for all ǫ > 0, ∃n ∈ N, for all i > n such that
‖X−Yi‖H < d+ ǫ.
We show that {Yi}i=1 is a Cauchy sequence. From the Parallelogram Law, we
have
‖Ym −Yn‖
2
H = 2‖X−Ym‖
2
H + 2‖X−Yn‖
2
H − ‖2X−Ym −Yn‖
2
H
= 2‖X−Ym‖
2
H + 2‖X−Yn‖
2
H − 4‖X−
Ym
2
−
Yn
2
‖2H
< 4(d+ ǫ)2 − 4d2 = 8dǫ+ 4ǫ2
Therefore, {Yi}
∞
i=1 is Cauchy. Since the space is complete, there exists Y ∈ H
such that limi→∞ Yi
W
= Y. Since M is closed, Y is also in M. From Proposition
1, the norm is continuous, so ‖X−Y‖H = limi→∞ ‖X−Yi‖H = d.
Next, we show the uniqueness of Y. If there exists Y,Y′ ∈ M such that
‖X−Y‖H = ‖X−Y
′‖H = d, the Parallelogram Law implies that
‖Y−Y′‖2H = 2‖X−Y‖
2
H + 2‖X−Y
′‖2H − ‖2X−Y−Y
′‖2H
= 2‖X−Y‖2H + 2‖X−Y
′‖2H − 4‖X−
Y
2
−
Y′
2
‖2H
≤ 4d2 − 4d2 = 0
10 Yunfei Ye
Therefore, ‖Y−Y′‖H = 0 so Y = Y
′.
(2) Since Y minimize the distance between X and M, then for every λ ∈ C
and Z ∈ M we have
‖X−Y‖2H ≤ ‖X−Y− λZ‖
2
H
⇒〈〈X−Y,X−Y〉H,W〉 ≤ 〈〈X−Y− λZ,X−Y− λZ〉H,W〉
⇒2Re(λ
〈〈Z,X−Y〉H,W〉
‖W‖F
) ≤ |λ|2‖Z‖2
If 〈〈Z,X−Y〉H,W〉‖W‖F = re
iϕ, where r = | 〈〈Z,X−Y〉H,W〉‖W‖F | and ϕ = arg(
〈〈Z,X−Y〉H,W〉
‖W‖F
),
then we choose λ = ǫe−iϕ. It follows that
2|
〈〈Z,X−Y〉H,W〉
‖W‖F
| ≤ ǫ‖Z‖2H.
Taking the limit ǫ → 0+, we find that 〈Z,X−Y〉H = 0 for every Z ∈ M. Therefore,
X−Y⊥M.
Finally, we show that Y is the unique element such that X −Y⊥M. If there
exists another Y′ ∈ M such that X − Y′⊥M, then Y − Y′ ∈ M ∩ M⊥ which
means Y = Y′ from Proposition 2.
If M is a closed linear subspace, Theorem 6 also indicates that any X ∈ H can
be uniquely represented as X = Y + Z, where Y ∈ M and Z ∈ M⊥. Then we
have the following corollary.
Corollary 1 Let M be a closed linear subspace of a matrix Hilbert space H with
respect to W, then H = M⊕M⊥.
3.2 Orthonormal Basis
Since we have given the definition of orthogonality, it is natural to extend the idea
to orthonormal basis. Meanwhile, we present some equivalent conditions on it.
Finally, we prove the existence of the orthonormal basis for every matrix Hilbert
space.
We first propose the definition as follows.
Definition 12 {Ei}
n
i=1 is an orthonormal basis of the finite dimensional matrix
Hilbert space H with respect to W if:
(1)
〈〈Ei,Ej〉H,W〉
‖W‖F
= δij , for 1 ≤ i, j ≤ n
(2) For all X ∈ H we have the following equality
X =
n∑
i=1
〈〈X,Ei〉H,W〉
‖W‖F
Ei. (6)
Then we have to dealt with the problem of how to construct the orthonor-
mal basis of H with respect to W. The Gram-Schmidt process is a method for
orthonormalising a set of vectors in an inner product space. Here, we give our
revised method to matrix Hilbert space.
Matrix Hilbert Space 11
Theorem 7 Given a finite, linearly independent set S = {V1, . . . ,Vn}, we can
generate an orthogonal set S′ = {U1, . . . ,Un} that spans the same n-dimensional
subspace of H with respect to W as S as follows:
U1 =
V1
‖V1‖H
Ui+1 = ci+1(Vi+1 −
i∑
k=1
〈〈Vi+1,Uk〉H,W〉
‖W‖F
Uk)
for all i ≥ 2 and ci+1 is a real number to ensure that ‖Ui+1‖H = 1.
Proof From the construction of S′, it is trivial to see that its elements are linearly
independent. We use mathematical induction to prove that each element in S′i =
{U1, . . . ,Ui}, 1 ≤ i ≤ n is orthogonal.
Let i = 2, it is true because
〈〈U1,U2〉H,W〉 = 〈〈U1, c2(V2 −
〈〈V2,U1〉H,W〉
‖W‖F
U1)〉H,W〉
= c2〈〈U1,V2〉H,W〉 − c2〈〈U1,V2〉H,W〉 = 0
Assume it is true for i = k,
〈〈Um,Un〉H,W〉 = 0
for all m,n ≤ k and m 6= n.
Then for i = k + 1,
〈〈Um,Uk+1〉H,W〉 = 〈〈Um, ck+1(Vk+1 −
k∑
j=1
〈〈Vk+1,Uj〉H,W〉
‖W‖F
Uj)〉H,W〉
= ck+1〈〈Um,Vk+1〉H,W〉 − ck+1〈〈Um,Vk+1〉H,W〉 = 0
for all m ≤ k. Therefore, each element in S′k+1 = {U1, . . . ,Uk+1}, 1 ≤ i ≤ n is
orthogonal.
Since both the basis and the inductive step have been performed, by mathe-
matical induction, the statement holds for all natural number n.
The final part of this subsection is to prove the existence of an orthonormal
basis for every matrix Hilbert space. Meanwhile, we point out the fact that each
element in a matrix Hilbert space can be represented as the linear combination of
orthonormal basis at most countable.
Definition 13 Let {Xα ∈ H|α ∈ I} be the subset of a matrix Hilbert space H
with respect to W, where the index set I can be either countable or uncountable.
For each finite subset J ⊆ I, we define the sum SJ as SJ =
∑
α∈J Xα. The
unordered sum of set {Xα ∈ H|α ∈ I} converges toX ∈ H, writtenX =
∑
α∈I Xα
if for every ǫ > 0 there exists a finite subset J ǫ ⊆ I such that ‖SJ −X‖H < ǫ for
all finite subsets J ǫ ⊆ J ⊆ I.
Proposition 4 A convergent unordered sum of a matrix Hilbert space H with
respect to W has only countably many nonzero terms.
12 Yunfei Ye
Proof If an unordered sum
∑
α∈I Xα converges to X, then for each n ∈ N, there
exists a finite subset Jn ⊆ I such that for all finite subsets Jn ⊆ J ⊆ I, we have
‖SJ −X‖H <
1
n
. We know that J ′ = ∪n∈NJn is countable. For each α ∈ I but
α /∈ J ′, it follows that
‖Xα‖H = ‖SJ ′∪{α} − SJ ′‖H ≤ ‖SJ ′∪{α} −X‖H + ‖SJ ′ −X‖H
<
1
n
+
1
n
=
2
n
for all n ∈ N. Therefore, Xα is a zero element and there is only countably many
nonzero terms.
Definition 14 An unordered sum
∑
α∈I Xα is called Cauchy if for every ǫ > 0
there exists a finite subset J ǫ ⊆ I such that ‖SJ ‖H < ǫ for all finite subsets
J ⊆ I\J ǫ.
Proposition 5 An unordered sum in a matrix Hilbert space converges if and only
if it is Cauchy.
Proof Suppose an unordered sum
∑
α∈I Xα converges to X ∈ H, then by the
definition of convergence for every ǫ > 0 there exists a finite subset J ǫ ⊆ I such
that ‖SJ −X‖H <
ǫ
2 for all finite subsets J
ǫ ⊆ J ⊆ I. Then for all K ⊆ I\J ǫ,
‖SK‖H = ‖SJ ǫ∪K − SJ ǫ‖H ≤ ‖SJ ǫ∪K −X‖H + ‖SJ ǫ −X‖H <
ǫ
2
+
ǫ
2
= ǫ
which shows that the unordered sum is Cauchy.
If the unordered sum is Cauchy, then by its definition for every n ∈ N there
exists a finite subset Jn ⊆ I such that ‖SK‖H <
1
n
for all finite subsets K ⊆ I\Jn.
Considering a sequence {S∪ni=1Ji}
∞
n=1, it follows that ‖S∪mi=1Ji − S∪ni=1Ji‖H <
1
n
for all m ≥ n. Therefore, it is Cauchy. Since the matrix Hilbert space is complete,
it converges to a point X ∈ H. For every ǫ > 0, we put J ǫ = ∪ni=1Ji such that
2
n
< ǫ, we have
‖SJ −X‖H ≤ ‖SJ − S∪n
i=1
Ji‖H + ‖S∪ni=1Ji −X‖H <
2
n
< ǫ
for all finite subsets J ǫ ⊆ J ⊆ I. Therefore, the unordered sum
∑
α∈I Xα con-
verges to X.
Definition 15 We define the closed linear span [U ] of the subset U = {Uα}α∈I
of a matrix Hilbert space H with respect to W by
[U ] = {
∑
α∈I
cαUα|cα ∈ C,
∑
α∈I
cαUα converges}.
Proposition 6 Suppose U = {Uα}α∈I be an orthonormal set in H with respect
to W, then
[U ] = {
∑
α∈I
cαUα|cα ∈ C,
∑
α∈I
|cα|
2 < ∞}.
Matrix Hilbert Space 13
Proof We just show that
∑
α∈I cαUα converges if and only if
∑
α∈I |cα|
2 < ∞.
From Proposition 5, we know that the convergence of an unordered sum is equiv-
alent to be a Cauchy sequence. For any finite subset J ⊆ I
‖
∑
α∈J
cαUα‖
2 =
〈〈
∑
α∈J cαUα,
∑
α∈J cαUα〉H,W〉
‖W‖F
=
∑
α∈J
|cα|
2
It follows that the Cauchy criterion is satisfied for
∑
α∈I cαUα if and only if it
is satisfied for
∑
α∈I |cα|
2. Therefore, one of the sum converges if and only if the
other does. In addition, for the continuity of norm, we have
‖
∑
α∈I
cαUα‖
2 =
∑
α∈I
|cα|
2.
Theorem 8 (Bessel’s Inequality) Let U = {Uα}α∈I be an orthonormal set in
a matrix Hilbert space H with respect to W. Then for every X ∈ H, we have
(1)
∑
α∈I
| 〈〈X,Uα〉H,W〉‖W‖F |
2 ≤ ‖X‖2H
(2) XU =
∑
α∈I
〈〈X,Uα〉H,W〉
‖W‖F
Uα is a convergent sum
(3) X−XU ∈ U
⊥.
Proof (1) For any finite subset J ⊆ I, we have the following equality
‖X−
∑
α∈J
〈〈X,Uα〉H,W〉
‖W‖F
Uα‖
2
H
=
〈〈X−
∑
α∈J
〈〈X,Uα〉H,W〉
‖W‖F
Uα,X−
∑
α∈J
〈〈X,Uα〉H,W〉
‖W‖F
Uα〉H,W〉
‖W‖F
=‖X‖2H −
∑
α∈J
〈〈X,Uα〉H,W〉
‖W‖F
〈〈Uα,X〉H,W〉
‖W‖F
−
∑
α∈J
|
〈〈X,Uα〉H,W〉
‖W‖F
|2
+
∑
α∈J
|
〈〈X,Uα〉H,W〉
‖W‖F
|2
=‖X‖2H −
∑
α∈J
|
〈〈X,Uα〉H,W〉
‖W‖F
|2 ≥ 0
showing that
∑
α∈J |
〈〈X,Uα〉H,W〉
‖W‖F
|2 ≤ ‖X‖2H. Taking the supremum of it we have
the equality above.
(2) From Proposition 6, we know that XU =
∑
α∈I
〈〈X,Uα〉H,W〉
‖W‖F
Uα converges if
and only if
∑
α∈I |
〈〈X,Uα〉H,W〉
‖W‖F
|2 < ∞ which can be simply achieved by (1).
(3) Considering everyUα0 ∈ U , we just need to show that 〈〈X−XU ,Uα0〉H,W〉 =
0. It can be calculated by
〈〈X−XU ,Uα0〉H,W〉 = 〈〈X−
∑
α∈I
〈〈X,Uα〉H,W〉
‖W‖F
Uα,Uα0〉H,W〉
= 〈〈X,Uα0〉H,W〉 −
∑
α∈I
〈〈X,Uα〉H,W〉
〈〈Uα,Uα0〉H,W〉
‖W‖F
= 0.
14 Yunfei Ye
Therefore, X−XU ∈ U
⊥.
Now we extend our work of orthonormal set to orthonormal basis. The following
are equivalent conditions for an orthonormal set to be a basis.
Theorem 9 Let U = {Uα}α∈I be an orthonormal set in a matrix Hilbert space
H with respect to W, then the following conditions are equivalent:
(1) 〈〈X,Uα〉H,W〉 = 0 for all α ∈ I implies X is a zero element
(2) X =
∑
α∈I
〈〈X,Uα〉H,W〉
‖W‖F
Uα for all X ∈ H
(3) ‖X‖2H =
∑
α∈I
| 〈〈X,Uα〉H,W〉‖W‖F |
2 for all X ∈ H
(4) [U ] = H
(5) U is the maximal orthonormal set.
Proof We prove that (1) implies (2), (2) implies (3), (3) implies (4), (4) implies (5)
and (5) implies (1). (1) ⇒ (2) Let XU =
∑
α∈I
〈〈X,Uα〉H,W〉
‖W‖F
Uα be a convergent
sum. From Theorem 8, it follows that X − XU ∈ U
⊥. From the definition of
orthogonal, 〈〈X−XU ,Uα〉H,W〉 = 0 for all α ∈ I which implies that X = XU .
(2) ⇒ (3) It is shown in the proof of Proposition 6. (3) ⇒ (4) (3) implies that
U⊥ = {0} which means that [U ]⊥ = {0}. Hence, [U ] = H. (4)⇒ (5) Since [U ] = H,
then for X ∈ H, we have X =
∑
α∈I cαUα. If X ∈ U
⊥ then cα = 0 for every α.
Hence, X = 0 which implies that no new element can be added to U . Finally, (5)
is just a reformulation of (1) so (5) ⇒ (1).
In addition, combined with Proposition 4, the above Theorem also implies that
for all X ∈ H, it can be decomposed as the linear combination of U if U is the
maximal orthonormal set.
Here, we use Zorn’s lemma (Moore, 2012) to show that every matrix Hilbert
space H with respect to W has an orthonormal basis. It can be stated as:
Lemma 1 (Zorn’s Lemma) Suppose a partially ordered set P has the property
that every totally ordered subset has an upper bound in P . Then the set P contains
at least one maximal element.
We conclude this section with a theorem based on above work. The following
theorem indicates that every matrix Hilbert space can be expressed as the spanning
of an orthonormal basis.
Theorem 10 Every matrix Hilbert space H with respect to W has an orthonormal
basis.
Proof We introduce a partial ordering ≤ on orthonormal sets of H by inclusion,
which means U ≤ V if and only if U ⊆ V . If {Uα|α ∈ I} is a totally ordered subset of
orthonormal sets, then Uα ≤ Uβ or Uβ ≤ Uα for all α, β ∈ I. Notice that ∪α∈IUα is
an orthonormal set and it is an upper bound for the totally ordered subset. Zorn’s
lemma implies that the orthonormal sets contains at least one maximal element
and this element satisfies (5) in Theorem 9. Therefore, it is an orthonormal basis.
Matrix Hilbert Space 15
4 Reproducing Kernel Matrix Hilbert Space
The Moore-Aronszajn theorem (Aronszajn, 1950) made the connection between
kernel functions and Hilbert spaces. In the 20th century, Boser et al (Boser et al,
1992) were the first to use kernels to construct a nonlinear estimation algorithm
in the filed of machine learning. Over the last decades, researchers applied the
technique of the kernel trick to nonlinear analysis problems rather than explic-
itly compute the high-dimensional coordinates in feature space. In this section, we
extend the methodology of reproducing kernel Hilbert space (RKHS) to our repro-
ducing kernel matrix Hilbert space (RKMHS). We develop the relation between
reproducing kernel and matrix Hilbert space. We begin with some definitions of
the RKMHS.
Definition 16 (Evaluation functional) An evaluation functional over the ma-
trix Hilbert space H with respect to W of complex-valued functions on a domain
X (f ∈ H and f(X) ∈ C), is a linear functional that evaluates each function in the
space at the point X as
LX(f) = f(X)
for all f ∈ H,X ∈ X .
The functional is bounded if for every X ∈ X there exists a M > 0 such that
|LX(f)| ≤ M‖f‖H.
Definition 17 (Reproducing Kernel Matrix Hilbert Space) A RKMHS is
a matrix Hilbert space H with respect to W of complex-valued functions on a
domain X (f ∈ H and f(X) ∈ C), with the property that for every X ∈ X the
evaluation functional is a bounded linear functional.
The evaluation functional has a connection to the matrix inner product of f
and a function KX ∈ H using Theorem 4. A more intuitive definition can be
obtained by the following statement.
Definition 18 (Reproducing Kernel and Reproducing Kernel Matrix
Hilbert Space) If H is a matrix Hilbert space with respect to W, then for
each Y ∈ X there exists, by the Theorem 4 a function KY of H with the property
for every f ∈ H,
f(Y) =
〈〈f,KY〉H,W〉
‖W‖F
Since KX is itself a function in H, we have that for each X ∈ X
KX(Y) =
〈〈KX, KY〉H,W〉
‖W‖F
The reproducing kernel of H is a function K : X × X → Cn×n defined by
K(X,Y) = 〈KX,KY〉H.
So We call H a RKMHS.
16 Yunfei Ye
We can easily derive that for all Xi,Xj ∈ X , αi, αj ∈ C,m ∈ N
m∑
i,j=1
αiαj
〈K(Xi,Xj),W〉
‖W‖F
=
m∑
i,j=1
αiαj
〈〈KXi ,KXj 〉H,W〉
‖W‖F
=
〈〈
m∑
i=1
αiKXi ,
m∑
j=1
αjKXj 〉H,W〉
‖W‖F
≥ 0.
And if ∃Xi,Xj ∈ X , αi, αj ∈ C,m ∈ N,
m∑
i,j=1
αiαj
〈K(Xi,Xj),W〉
‖W‖F
= 0,
then
∑m
i=1 αiKXi is zero.
More generally, we use mapping to obtain the definition of kernel.
Definition 19 (Kernel) A function
K : X × X → Cn×n, (X,X′) 7→ K(X,X′)
satisfying for all X,X′ ∈ X
K(X,X′) = 〈Φ(X), Φ(X′)〉H (7)
is called a kernel where the feature map Φ maps into some matrix Hilbert space
H, or the feature space with respect to W.
Now we define a positive semidefinite kernel with respect to W.
Definition 20 (Positive Semidefinite Kernel with respect to W) A func-
tion K : X × X → Cn×n satisfying
K(X,X′) = K(X′,X)∗ (8)
m∑
i,j=1
αiαj
〈K(Xi,Xj),W〉
‖W‖F
≥ 0 (9)
for all Xi,Xj ∈ X , αi, αj ∈ C,m ∈ N is called a positive semidefinite kernel with
respect to W.
Now we show that the class of kernels that can be written in the form (7)
coincides with the class of positive semidefinite kernels with respect to W.
Theorem 11 To every positive semidefinite function K with respect to W on
X × X , there corresponds a unique RKMHS HK of complex-valued functions on
X and vice versa.
Matrix Hilbert Space 17
Proof Suppose HK is a RKMHS with respect to W, then the reproducing kernel
K(X,Y) = 〈KX,KY〉H, the properties of (8) and (9) follow from the definition
of matrix Hilbert space.
We then suggest how to construct HK given K. Let KX(Y) =
〈K(X,Y),W〉
‖W‖F
denotes the function of Y obtained by fixing X. Consider H be the completion of
all the linear combinations in span{KX|X ∈ X}
f(·) =
m∑
i=1
αiKXi(·). (10)
Here, Xi ∈ X , αi ∈ C,m ∈ N are arbitrary.
Next, we define the matrix inner product between f and another function
g(·) =
∑m′
j=1 βjKX′j (·) as
〈f, g〉H =
m∑
i=1
m′∑
j=1
αiβjK(Xi,X
′
j). (11)
Note that 〈f, g〉H =
∑m′
j=1 βj
∑m
i=1 αiK(Xi,X
′
j) =
∑m
i=1 αi
∑m′
j=1 βjK(Xi,X
′
j)
which shows that 〈·, ·〉 is bilinear. 〈f, g〉H = 〈g, f〉
∗
H, as K(Xi,X
′
j) = K(X
′
j ,Xi)
∗.
Moreover, for any function f , written as (10), we have
〈〈f, f〉H,W〉 =
m∑
i,j=1
αiαj〈K(Xi,Xj),W〉 ≥ 0. (12)
To prove that KX is the reproducing kernel of H, we can easily derive that
〈〈f,KY〉H,W〉
‖W‖F
=
〈
∑m
i=1 αiK(Xi,Y),W〉
‖W‖F
= f(Y), 〈KX,KY〉H = K(X,Y),
which proves the reproducing property.
For the last step in proving that H is a RKMHS, due to (12) and the Cauchy-
Schwarz inequality, we have
|f(Y)| = |
〈〈f,KY〉H,W〉
‖W‖F
| ≤ ‖f‖H|‖KY‖H
for all Y ∈ X . By this inequality, ‖f‖H = 0 implies f = 0, which is the last
property that was left to prove in order to establish that 〈·, ·〉H is a matrix inner
product.
To prove uniqueness, let G be another RKMHS for which K is also a repro-
ducing kernel. For all X,Y ∈ X , we have
〈KX,KY〉H = K(X,Y) = 〈KX,KY〉G
By linearity, 〈·, ·〉H = 〈·, ·〉G on H. Then H ⊆ G because G is complete and
contains H.
Now we need to prove that every element in G is in H. Given f ∈ G, since H is
a closed subset of G, we can rewrite f = fH + fH⊥ where fH ∈ H and fH⊥ ∈ H
⊥.
For all X ∈ X , since K is a reproducing kernel of G, we have
f(X) =
〈〈f,KX〉G ,W〉
‖W‖F
=
〈〈fH + f
⊥
H ,KX〉G ,W〉
‖W‖F
=
〈〈fH,KX〉G ,W〉
‖W‖F
= fH(X)
which shows that fH⊥ = 0 in G and concludes the proof.
18 Yunfei Ye
In such case we show that the class of kernels that can be written in the form
(7) coincides with the class of positive semidefinite kernels with respect to W as
Φ(X) = KX(·).
Now we introduce some closure properties of the set of positive semidefinite
kernels with respect to W.
Proposition 7 Below, K1, . . . ,Km are arbitrary positive semidefinite kernels with
respect to W on X × X , where X is a nonempty set:
(1) For all λ1, λ2 ≥ 0, λ1K1+λ2K2 is positive semidefinite with respect to W.
(2) If K(X,X′) = limm→∞ Km(X,X
′) for all X,X′ ∈ X , then K is positive
semidefinite with respect to W.
(3) The products K(·, ·) = 〈K2(·, ·),W〉K1(·, ·) and K(·, ·) = 〈K1(·, ·),W〉K2(·, ·)
are positive semidefinite with respect to W.
The proof is trivial. Some possible choices of K include
Linear kernel : K(X,Y) = X⊺Y,
Polynomial kernel : K(X,Y) = (X⊺Y+ αIn×n)
◦β
Gaussian kernel : K(X,Y) = [exp(−γ‖X(:, i)−Y(:, j)‖2)]n×n
where α ≥ 0, β ∈ N, γ > 0,X,Y ∈ H = Cm×n with respect to W = In×n. X(:, i)
is the i-th column of X and ◦ is the Hadamard product (Horn, 1990).
The above kernel design is by no means complete. Any construction satisfying
the properties of positive semidefinite can be applied in practice.
From kernels, functions can be expressed in terms of kernel expansions. The
Representer Theorem (Kimeldorf and Wahba, 1971) shows that solutions of a large
class of optimization problems can be expressed as kernel expansions over the
sample points. In our RKMHS, we have the similar expression as follows.
Theorem 12 (Revised Representer Theorem) Let HK be a RKMHS with
respect to W ∈ Cn×n of complex-valued functions on a domain X , where K(·, ·)
is its reproducing kernel. Given training samples {(Xi, yi) ∈ X ×C}
m
i=1, a strictly
monotonic increasing real-valued function g : [0,∞) → R, and an arbitrary loss
function L : (X × C2)m → R ∪∞. Then for all f∗ ∈ HK satisfying
f∗ = argminf∈HK{L((X1, y1, f(X1)), . . . , (Xm, ym, f(Xm))) + g(‖f‖HK )} (13)
admits a representation of the form
f∗(·) =
m∑
i=1
αi
〈K(Xi, ·),W〉
‖W‖F
.
Proof Let KX =
〈K(X,·),W〉
‖W‖F
and H0 be the completion of all linear combinations
in span{KX|X ∈ X}
f(·) =
m∑
i=1
αiKXi(·). (14)
Matrix Hilbert Space 19
Here, Xi ∈ X , αi ∈ C,m ∈ N are arbitrary. Given f ∈ HK , since H0 is a closed
subset of HK , we can rewrite f = fH0 +fH⊥
0
where fH0 ∈ H0 and fH⊥
0
∈ H⊥0 . The
reproducing property shows that applying f to any training point Xj produces
f(Xj) =
〈〈f,KXj 〉HK ,W〉
‖W‖F
=
〈〈fH0 ,KXj 〉HK ,W〉
‖W‖F
which is independent of fH⊥
0
. Consequently, the value of L in (13) is independent
of fH⊥
0
. On the other hand, due the property of g and fH⊥
0
, we can obtain that
g(‖f‖HK) = g(‖fH0 + fH⊥
0
‖HK )
= g(
√
‖fH0‖
2
HK
+ ‖fH⊥
0
‖2HK )
≥ g(‖fH0‖HK ).
Therefore, setting fH⊥
0
to zero strictly decreasing the value of (13) and we have
the conclusion that the minimizer f∗ must be of the form
f∗(·) =
m∑
i=1
αiKXi(·) =
m∑
i=1
αi
〈K(Xi, ·),W〉
‖W‖F
.
5 Experiments
In this section, we demonstrate the use of our matrix Hilbert Space in practice.
We point out a connection to the Support Tensor Machine (STM) in the field of
machine learning. We propose a family of matrix kernels to estimate the similarity
of matrix data. Note that by doing so we essentially recover the matrix-based
approach (Gao et al, 2015) and construct the kernel by the methodology of our
space. We use the real world data to evaluate the performance of different kernels
(DuSK (He et al, 2014), factor kernel (Signoretto et al, 2011), linear, Gaussian-
RBF, matrix kernel (Gao et al, 2014) and ours) on SVM or STM classifier, since
they have been proven successful in various applications.
All experiments were conducted on a computer with Intel(R) Core(TM) i5 (3.30
GHZ) processor with 16.0 GB RAM memory. The algorithms were implemented
in Matlab.
5.1 Algorithms
Given a set of samples {(yi,Xi)}
N
i=1 for binary classification problem, where Xi ∈
R
m×n are the input matrix data and yi ∈ {−1,+1} are the corresponding class
labels. Linear STM aims to find the separating hyperplane f(X) = 〈W,X〉+b = 0,
it can be evaluated by considering the following question (Hao et al, 2013):
min
W,b,ξ
1
2
‖W‖2F + C
N∑
i=1
ξi
s.t. yi(〈W,Xi〉+ b) ≥ 1− ξi, 1 ≤ i ≤ N
ξ ≥ 0,
(15)
20 Yunfei Ye
where W ∈ Rm×n, ξ = [ξ1, · · · , ξN ]
T is the vector of all slack variables of training
examples. By using the technique of the singular value decomposition (SVD) for
matrix W and mapping X to the feature space Φ : X → Φ(X), we can reformulate
the optimization problem as (Gao et al, 2015):
min
u,v,b,ξ
1
2
r∑
k=1
(uTk uk)(v
T
k vk) + C
N∑
i=1
ξi
s.t. yi(
r∑
k=1
uTk Φ(Xi)vk + b) ≥ 1− ξi, 1 ≤ i ≤ N
ξ ≥ 0.
(16)
In the literature, the numerical solution of the optimization problem depends
on the calculation of 〈Φ(X), Φ(X′)〉, which is the reproducing kernel of our matrix
Hilbert space. Therefore, we define our reproducing kernel in the following way.
Using SVD, a matrix X ∈ Rm×n can be decomposed in block-partitioned form
as
X = [UX, ŨX]
[
SX 0
0 0
]
[VX, ṼX]
T . (17)
Notice that the number of column C in UX and VX is equal to min{m,n}.
Let WX = [U
T
X,V
T
X]
T . UX can be divided by columns as [UX,1, . . . ,UX,C ], so
as VX and WX.
Definition 21 (New matrix kernel) For X,Y ∈ H, let Φ : Rm×n → H be a
mapping where Φ(X) = [Φ(WX,1), . . . , Φ(WX,C)]. We then define the reproducing
kernel KM as
KM (X,Y) = Φ
T (X)Φ(Y)
= [ΦT (WX,i)Φ(WY,j)]C×C.
(18)
Theorem 13 Generally speaking, if ΦT (xi)Φ(yj) = k(xi, yj) is any valid kernel
form of vector space (e.g. Gaussian RBF kernel, polynomial kernel) in Definition
21, then the kernel function KM is a positive semidefinite kernel with respect to
W = IC×C.
The proof is trivial. According to Theorem 11, we just need to show that the
kernel function satisfies conditions in Definition 20 so we leave it to readers.
5.2 Datasets
Next, we compare the performance of our kernels with those introduced in the lit-
erature: DuSK, linear, Gaussian-RBF, factor kernel and matrix kernel on real data
sets. We consider the following benchmark datasets to perform a series of compar-
ative experiments on both binary and multiple classification problems. We use the
MNIST database (LeCun et al, 1998) of handwritten digits established by Yann
LeCun, etc. from http://yann.lecun.com/exdb/mnist/, the Yale Face database
(Belhumeur et al, 1997) from http://vision.ucsd.edu/content/yale-face-database
and the FingerDB database from http://bias.csr.unibo.it/fvc2000/databases.asp.
Matrix Hilbert Space 21
(a)
(b)
(c)
Fig. 1 Examples for matrix datasets. a MNIST samples. b Yale face samples. c FingerDB
samples
To better visualize the experimental data, we randomly choose a small subset for
each database, as shown in Fig. 1.
The MNIST database of handwritten digits has a training set of 60,000 exam-
ples, and a test set of 10,000 examples. Each image was centered in a 28×28 image
by computing the center of mass of the pixels, and translating the image so as to
position this point at the center of the 28 × 28 field. For each digit, we compare
it with other digits to deal with the binary classification problem. We randomly
choose 100 and 200 examples respectively as the training set while 400 arbitrary
examples are used as test set. In addition, half of them are of one digit while the
remaining are of other digits.
The Yale Face database contains 165 grayscale images in GIF format of 15
individuals with 243×320 pixels. There are 11 images per subject, one per different
facial expression or configuration. In the experiment, we randomly pick up 6 images
of each individual as the training set and other images remained for testing for
multiple classification.
The FingerDB database contains 80 finger images of 10 individuals with 300×
300 pixels, and each individual has 8 finger images. In this experiment, 4 randomly
selected images of each individual were gathered as training set and the rest images
retained as test set for the multiple classification. This random selection experi-
ment was repeated 10 times for all data sets to obtain the average and standard
deviation of performance measures.
22 Yunfei Ye
Table 1 Prediction performance of different kernels on experimental datasets in terms of
accuracy and macro-averaged F-measure
Kernel Accuracy (%) F-measure
Yale Face FingerDB Yale Face FingerDB
SVMlinear 86.9 ± 3.7 72.0 ± 5.8 0.871 ± 0.036 0.700 ± 0.064
SVMRBF 85.6 ± 3.6 73.0 ± 4.0 0.859 ± 0.035 0.712 ± 0.045
Factor Kernel 76.0 ± 5.2 64.0 ± 2.5 0.764 ± 0.052 0.599 ± 0.043
DuSKRBF 80.8 ± 3.2 73.5 ± 2.5 0.808 ± 0.033 0.732 ± 0.033
Matrix Kernel (RBF) 84.0 ± 4.4 72.5 ± 2.7 0.844 ± 0.040 0.719 ± 0.029
New Matrix Kernel (poly) 89.3 ± 4.9 75.0 ± 3.5 0.892 ± 0.049 0.741 ± 0.036
One-to-one classification method is introduced when it comes to multiple clas-
sification problem. All data are standardized into [0, 1] through linear transforma-
tion. The input matrices are converted into vectors when it comes to the SVM prob-
lems. All kernels select the optimal trade-off parameter C ∈ {10−2, 10−1, . . . , 102},
kernel width parameter σ ∈ {10−4, 10−3, . . . , 104} and rank r ∈ {1, 2, · · · , 10}. All
the learning machines use the same training and test set. We first randomly sample
25% of whole data from each dataset for the purpose of parameter selection. In ma-
trix kernel and new matrix kernel, Gaussian RBF kernel k(x,y) = exp(−σ‖x−y‖2)
and polynomial kernel k(x,y) = (xTy+σ)2 are used respectively as the vector ker-
nel functions. Gaussian RBF kernel is used in DuSK which denoted as DuSKRBF.
In addition, linear and Gaussian-RBF kernel are introduced on SVM classifier
which denoted as SVMlinear and SVMRBF respectively.
To evaluate the performance of the different kernels, we introduce two per-
formance measures. We report the accuracy which counts on the proportion of
correct predictions, the F1 score as the harmonic mean of precision and recall
F1 = 2 ·
Pre×Rec
Pre+Rec . Precision is the fraction of retrieved instances that are relevant,
while recall is the fraction of relevant instances that are retrieved. In multiple clas-
sification problems, macro-averaged F-measure (Yang and Liu, 1999) is adopted
as the average of F1 score for each category.
5.3 Discussions
Fig. 2 summarizes the results of SVMRBF, factor kernel, DuSKRBF, matrix kernel
and new matrix kernel in terms of accuracy and F1 score on MNIST dataset. The
linear kernel-based SVM is not included because Gaussian-RBF kernel-based SVM
proved to be better in the literature (Liu et al, 2003). Similar patterns of curves are
detected in accuracy and F1 score among all the kernel methods. We can observe
that our new matrix kernel performs well in general, just beaten by SVMRBF and
matrix kernel in some special cases. We are interested in accuracy and F1 score
in kernel comparison experiments and one way to understand this is to realize
that matrices are calculated to construct our new kernel which occupies much
more space and time. On the other hand, the matrix kernel is competitive against
SVMRBF, factor kernel and DuSKRBF, performs worse than our new matrix kernel.
In addition, the observations demonstrate the size of training set has positive effect
on the performance in most cases.
Matrix Hilbert Space 23
1 2 3 4 5 6 7 8 9 10
Different Pair
70
75
80
85
90
95
100
C
la
ss
ifi
ca
tio
n 
A
cc
ur
ac
y 
(%
)
SVMRBF
Factor kernel
DuSKRBF
Matrix kernel
New matrix kernel
(a) 100 train
1 2 3 4 5 6 7 8 9 10
Different Pair
70
75
80
85
90
95
100
C
la
ss
ifi
ca
tio
n 
A
cc
ur
ac
y 
(%
)
SVMRBF
Factor kernel
DuSKRBF
Matrix kernel
New matrix kernel
(b) 200 train
1 2 3 4 5 6 7 8 9 10
Different Pair
0.7
0.75
0.8
0.85
0.9
0.95
1
F
1 
S
co
re
SVMRBF
Factor kernel
DuSKRBF
Matrix kernel
New matrix kernel
(c) 100 train
1 2 3 4 5 6 7 8 9 10
Different Pair
0.7
0.75
0.8
0.85
0.9
0.95
1
F
1 
S
co
re
SVMRBF
Factor kernel
DuSKRBF
Matrix kernel
New matrix kernel
(d) 200 train
Fig. 2 Average accuracy and F1 score compared by different kernels for binary classification
problem on MNIST dataset.
Table 1 reports the performance of several types of kernels with respect to
accuracy and macro-averaged F-measure for multiple classification problems. We
can observe that both of accuracy and macro-averaged F-measure show the similar
trend. Our new matrix kernel clearly benefits from its complexity and on both
datasets it outperforms SVMlinear, SVMRBF, DuSKRBF and factor kernel in almost
significant manner. Factor kernel performs significantly worse in all domains. On
the other hand, the matrix kernel performs slightly worse than our new matrix
kernel and the performance of it is quite different on different data sets. This may
due to the construction of the kernel where only the columns of matrix are under
consideration, whereas information in the rows or inside the structure is lost.
So far we have compared all experimental results. The results of classification
accuracy and F-measure for DuSKRBF, factor kernel, SVM, the matrix kernel
and our new matrix kernel demonstrate that our new matrix kernel is significantly
effective on both binary and multiple classification problems. Notice that we apply
polynomial kernel in the construction of the new matrix kernel, and other types
of vector kernel can also be adopted.
24 Yunfei Ye
6 Concluding Remarks
In this paper we have established a mathematical framework of matrix Hilbert
spaces. Intuitively, we introduce a matrix inner product to generalize the scalar
inner product, which is especially useful in the presence of structured data. The
theoretical analysis of mapping and orthogonality highlights the existence of con-
jugate isometric isomorphism and orthogonal basis for each matrix Hilbert space.
The reproducing kernel and the reproducing kernel matrix Hilbert space in our
framework allow one to construct various structure of kernels in nonlinear cases.
In our experiments, kernel induced by our framework has favourable predictive
performance on both binary and multiple classification problems.
Matrix Hilbert space provides several interesting avenues for future work. For
calculating the new matrix kernel developing computational methods could im-
prove efficiency. Since the dimension of matrix inner product has a negative effect
on the usage of space and time, we could pick up an appropriate one to balance
the trade-off between efficiency and accuracy. While experiments have been con-
ducted on the classification problems, our results indicate that RKMHS is directly
applicable to regression, clustering, and ranking, among other tasks.
Acknowledgements The work is supported by National Natural Science Foundations of
China under Grant 11531001 and National Program on Key Basic Research Project under
Grant 2015CB856004. We are grateful to Dong Han and Lynn Kuo for our discussions.
References
Aronszajn N (1950) Theory of reproducing kernels. Transactions of the American
mathematical society 68(3):337–404
Asgari M, Khosravi A (2003) Frames and bases in tensor product of hilbert spaces,
intern. Math Journal 4(527-537)
Ballentine LE (2014) Quantummechanics: a modern development.World Scientific
Publishing Co Inc
Belhumeur PN, Hespanha JP, Kriegman DJ (1997) Eigenfaces vs. fisherfaces:
Recognition using class specific linear projection. Pattern Analysis and Machine
Intelligence, IEEE Transactions on 19(7):711–720
Birkhoff G, Von Neumann J (1936) The logic of quantum mechanics. Annals of
mathematics pp 823–843
Birman MS, Solomjak MZ (2012) Spectral theory of self-adjoint operators in
Hilbert space, vol 5. Springer Science & Business Media
Boser BE, Guyon IM, Vapnik VN (1992) A training algorithm for optimal mar-
gin classifiers. In: Proceedings of the fifth annual workshop on Computational
learning theory, ACM, pp 144–152
Crandall MG, Ishii H, Lions PL (1992) Users guide to viscosity solutions of sec-
ond order partial differential equations. Bulletin of the American Mathematical
Society 27(1):1–67
Gao C, Wu XJ (2012) Kernel support tensor regression. Procedia Engineering
29(4):3986–3990
Matrix Hilbert Space 25
Gao X, Fan L, Xu H (2015) Multiple rank multi-linear kernel support vector ma-
chine for matrix data classification. International Journal of Machine Learning
and Cybernetics pp 1–11
Gao XZ, Fan L, Xu H (2014) Nls-tstm: A novel and fast nonlinear image classifi-
cation method. Wseas Transactions on Mathematics 13:626–635
Gilbarg D, Trudinger NS (2015) Elliptic partial differential equations of second
order. springer
Gustafson KE (2012) Introduction to partial differential equations and Hilbert
space methods. Courier Corporation
Hao Z, He L, Chen B, Yang X (2013) A linear support higher-order tensor machine
for classification. IEEE Transactions on Image Processing 22(7):2911–2920
He L, Kong X, Yu PS, Yang X, Ragin AB, Hao Z (2014) Dusk: A dual structure-
preserving kernel for supervised tensor learning with applications to neuroim-
ages. In: Proceedings of the 2014 SIAM International Conference on Data Min-
ing, SIAM, pp 127–135
Hofmann T, Schölkopf B, Smola AJ (2008) Kernel methods in machine learning.
The annals of statistics pp 1171–1220
Horn RA (1990) The hadamard product. In: Proc. Symp. Appl. Math, vol 40, pp
87–169
Khosravi A, Khosravi B (2007) Frames and bases in tensor products of hilbert
spaces and hilbert c*-modules. Proceedings Mathematical Sciences 117(1):1–12
Kimeldorf G, Wahba G (1971) Some results on tchebycheffian spline functions.
Journal of mathematical analysis and applications 33(1):82–95
Lance EC (1995) Hilbert C*-modules: a toolkit for operator algebraists, vol 210.
Cambridge University Press
LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based learning applied
to document recognition. Proceedings of the IEEE 86(11):2278–2324
Liu CL, Nakashima K, Sako H, Fujisawa H (2003) Handwritten digit recognition:
benchmarking of state-of-the-art techniques. Pattern recognition 36(10):2271–
2285
Moore GH (2012) Zermelo’s axiom of choice: Its origins, development, and influ-
ence. Courier Corporation
Sakurai JJ, Tuan SF, Commins ED (1995) Modern quantum mechanics, revised
edition
Schölkopf B, Smola A, Müller KR (1998) Nonlinear component analysis as a kernel
eigenvalue problem. Neural computation 10(5):1299–1319
Signoretto M, De Lathauwer L, Suykens JA (2011) A kernel-based framework to
tensorial data analysis. Neural networks 24(8):861–874
Sinap A, Van Assche W (1994) Polynomial interpolation and gaussian quadrature
for matrix-valued functions. Linear algebra and its applications 207:71–114
Stein EM, Weiss G (2016) Introduction to Fourier analysis on Euclidean spaces
(PMS-32), vol 32. Princeton university press
Vapnik VN, Vapnik V (1998) Statistical learning theory, vol 1. Wiley New York
Wahba G (1990) Spline models for observational data. SIAM
Yang Y, Liu X (1999) A re-examination of text categorization methods. In: Pro-
ceedings of the 22nd annual international ACM SIGIR conference on Research
and development in information retrieval, ACM, pp 42–49

