Hypothesis Testing For Densities and
High-Dimensional Multinomials: Sharp Local Minimax Rates
Sivaraman Balakrishnanâ€  Larry Wassermanâ€ 
Department of Statisticsâ€ 
Carnegie Mellon University
Pittsburgh, PA 15213
{siva,larry}@stat.cmu.edu
July 3, 2017
Abstract
We consider the goodness-of-fit testing problem of distinguishing whether the data are
drawn from a specified distribution, versus a composite alternative separated from the null
in the total variation metric. In the discrete case, we consider goodness-of-fit testing when
the null distribution has a possibly growing or unbounded number of categories. In the
continuous case, we consider testing a Lipschitz density, with possibly unbounded support,
in the low-smoothness regime where the Lipschitz parameter is not assumed to be constant.
In contrast to existing results, we show that the minimax rate and critical testing radius
in these settings depend strongly, and in a precise way, on the null distribution being
tested and this motivates the study of the (local) minimax rate as a function of the null
distribution. For multinomials the local minimax rate was recently studied in the work of
Valiant and Valiant [30]. We re-visit and extend their results and develop two modifications
to the Ï‡2-test whose performance we characterize. For testing Lipschitz densities, we show
that the usual binning tests are inadequate in the low-smoothness regime and we design a
spatially adaptive partitioning scheme that forms the basis for our locally minimax optimal
tests. Furthermore, we provide the first local minimax lower bounds for this problem
which yield a sharp characterization of the dependence of the critical radius on the null
hypothesis being tested. In the low-smoothness regime we also provide adaptive tests, that
adapt to the unknown smoothness parameter. We illustrate our results with a variety of
simulations that demonstrate the practical utility of our proposed tests.
1 Introduction
Hypothesis testing is one of the pillars of modern mathematical statistics with a vast array of
scientific applications. There is a well-developed theory of hypothesis testing starting with the
work of Neyman and Pearson [22], and their framework plays a central role in the theory and
practice of statistics. In this paper we re-visit the classical goodness-of-fit testing problem of
distinguishing the hypotheses:
H0 : Z1, . . . , Zn âˆ¼ P0 versus H1 : Z1, . . . , Zn âˆ¼ P âˆˆ A (1)
for some set of distributions A. This fundamental problem has been widely studied (see for
instance [19] and references therein).
A natural choice of the composite alternative, one that has a clear probabilistic inter-
pretation, excludes a total variation neighborhood around the null, i.e. we take A = {P :
1
ar
X
iv
:1
70
6.
10
00
3v
1 
 [
m
at
h.
ST
] 
 3
0 
Ju
n 
20
17
TV(P, P0) â‰¥ /2}. This is equivalent to A = {P : â€–P âˆ’ P0â€–1 â‰¥ }, and we use this representa-
tion in the rest of this paper. However, there exist no consistent tests that can distinguish
an arbitrary distribution P0 from alternatives separated in `1; see [2, 17]. Hence, we impose
structural restrictions on P0 and A. We focus on two cases:
1. Multinomial testing: When the null and alternate distributions are multinomials.
2. Lipschitz testing: When the null and alternate distributions have Lipschitz densities.
The problem of goodness-of-fit testing for multinomials has a rich history in statistics and
popular approaches are based on the Ï‡2-test [24] or the likelihood ratio test [5, 22, 32]; see,
for instance, [9, 11, 21, 23, 25] and references therein. Motivated by connections to property
testing [26], there is also a recent literature developing in computer science; see [3, 10, 13, 30].
Testing Lipschitz densities is one of the basic non-parametric hypothesis testing problems and
tests are often based on the Kolmogorov-Smirnov or CrameÌr-von Mises statistics [7, 27, 31].
This problem was originally studied from the minimax perspective in the work of Ingster
[14, 15]. See [1, 12, 14] for further references.
In the goodness-of-fit testing problem in (1), previous results use the (global) critical radius
as a benchmark. Roughly, this global critical radius is a measure of the minimal separation
between the null and alternate hypotheses that ensures distinguishability, as the null hypothesis
is varied over a large class of distributions (for instance over the class of distributions with
Lipschitz densities or over the class of all multinomials on d categories). Remarkably, as shown
in the work of Valiant and Valiant [30] for the case of multinomials and as we show in this paper
for the case of Lipschitz densities, there is considerable heterogeneity in the critical radius
as a function of the null distribution P0. In other words, even within the class of Lipschitz
densities, testing certain null hypotheses can be much easier than testing others. Consequently,
the local minimax rate which describes the critical radius for each individual null distribution
provides a much more nuanced picture. In this paper, we provide (near) matching upper and
lower bounds on the critical radii for Lipschitz testing as a function of the null distribution,
i.e. we precisely upper and lower bound the critical radius for each individual Lipschitz null
hypothesis. Our upper bounds are based on Ï‡2-type tests, performed on a carefully chosen
spatially adaptive binning, and highlight the fact that the standard prescriptions of choosing
bins with a fixed width [28] can yield sub-optimal tests.
The distinction between local and global perspectives is reminiscent of similar effects
that arise in some estimation problems, for instance in shape-constrained inference [4], in
constrained least-squares problems [6] and in classical Fisher Information-CrameÌr-Rao bounds
[18].
The remainder of this paper is organized as follows. In Section 2 we provide some
background on the minimax perspective on hypothesis testing, and formally describe the
local and global minimax rates. We provide a detailed discussion of the problem of study
and finally provide an overview of our main results. In Section 3 we review the results of
[30] and present a new globally-minimax test for testing multinomials, as well as a (nearly)
locally-minimax test. In Section 4 we consider the problem of testing a Lipschitz density
against a total variation neighbourhood. We present the body of our main technical result in
Section 4.3 and defer technical aspects of this proof to the Appendix. In each of Section 3
and 4 we present simulation results that demonstrate the superiority of the tests we propose
and their potential practical applicability. In the Appendix, we also present several other
results including a brief study of limiting distributions of the test statistics under the null, as
well as tests that are adaptive to various parameters.
2
2 Background and Problem Setup
We begin with some basic background on hypothesis testing, the testing risk and minimax
rates, before providing a detailed treatment of some related work.
2.1 Hypothesis testing and minimax rates
Our focus in this paper is on the one sample goodness-of-fit testing problem. We observe
samples Z1, . . . , Zn âˆˆ X , where X âŠ‚ Rd, which are independent and identically distributed
with distribution P . In this context, for a fixed distribution P0, we want to test the hypotheses:
H0 : P = P0 versus
H1 : â€–P âˆ’ P0â€–1 â‰¥ n.
(2)
Throughout this paper we use P0 to denote the null distribution and P to denote an arbi-
trary alternate distribution. Throughout the paper, we use the total variation distance (or
equivalently the `1 distance) between two distributions P and Q, defined by
TV(P,Q) = sup
A
|P (A)âˆ’Q(A)| (3)
where the supremum is over all measurable sets. If P and Q have densities p and q with
respect to a common dominating measure Î½, then
TV(P,Q) =
1
2
âˆ«
|pâˆ’ q|dÎ½ = 1
2
â€–pâˆ’ qâ€–1 â‰¡
1
2
â€–P âˆ’Qâ€–1. (4)
We consider the total variation distance because it has a clear probabilistic meaning and
because it is invariant under one-to-one transformations [8]. The `2 metric is often easier to
work with but in the context of distribution testing its interpretation is less intuitive. Of
course, other metrics (for instance Hellinger, Ï‡2 or Kullback-Leibler) can be used as well but
we focus on TV (or `1) throughout this paper. It is well-understood [2, 17] that without
further restrictions there are no uniformly consistent tests for distinguishing these hypotheses.
Consequently, we focus on two restricted variants of this problem:
1. Multinomial testing: In the multinomial testing problem, the domain of the distributions
is X = {1, . . . , d} and the distributions P0 and P are equivalently characterized by
vectors p0, p âˆˆ Rd. Formally, we define,
M =
{
p : p âˆˆ Rd,
dâˆ‘
i=1
pi = 1, pi â‰¥ 0 âˆ€ i âˆˆ {1, . . . , d}
}
,
and consider the multinomial testing problem of distinguishing:
H0 : P = P0, P0 âˆˆM versus H1 : â€–P âˆ’ P0â€–1 â‰¥ n, P âˆˆM. (5)
In contrast to classical â€œfixed-cellsâ€ asymptotic theory [25], we focus on high-dimensional
multinomials where d can grow with, and potentially exceed the sample size n.
2. Lipschitz testing: In the Lipschitz density testing problem the set X âŠ‚ Rd, and we
restrict our attention to distributions with Lipschitz densities, i.e. letting p0 and p denote
3
the densities of P0 and P with respect to the Lebesgue measure, we consider the set of
densities:
L(Ln) =
{
p :
âˆ«
X
p(x)dx = 1, p(x) â‰¥ 0 âˆ€ x, |p(x)âˆ’ p(y)| â‰¤ Lnâ€–xâˆ’ yâ€–2 âˆ€ x, y âˆˆ Rd
}
,
and consider the Lipschitz testing problem of distinguishing:
H0 : P = P0, P0 âˆˆ L(Ln) versus H1 : â€–P âˆ’ P0â€–1 â‰¥ n, P âˆˆ L(Ln). (6)
We emphasize, that unlike prior work [1, 12, 15] we do not require p0 to be uniform.
We also do not restrict the domain of the densities and we consider the low-smoothness
regime where the Lipschitz parameter Ln is allowed to grow with the sample size.
Hypothesis testing and risk. Returning to the setting described in (2), we define a test Ï†
as a Borel measurable map, Ï† : X n 7â†’ {0, 1}. For a fixed null distribution P0, we define the set
of level Î± tests:
Î¦n,Î± =
{
Ï† : Pn0 (Ï† = 1) â‰¤ Î±
}
. (7)
The worst-case risk (type II error) of a test Ï† over a restricted class C which contains P0 is
Rn(Ï†;P0, n, C) = sup
{
EP [1âˆ’ Ï†] : â€–P âˆ’ P0â€–1 â‰¥ n, P âˆˆ C
}
.
The local minimax risk is1:
Rn(P0, n, C) = inf
Ï†âˆˆÎ¦n,Î±
Rn(Ï†;P0, n, C). (8)
It is common to study the minimax risk via a coarse lens by studying instead the critical
radius or the minimax separation. The critical radius is the smallest value n for which a
hypothesis test has non-trivial power to distinguish P0 from the set of alternatives. Formally,
we define the local critical radius as:
n(P0, C) = inf
{
 : Rn(P0, n, C) â‰¤ 1/2
}
. (9)
The constant 1/2 is arbitrary; we could use any number in (0, 1âˆ’ Î±).
The local minimax risk and critical radius depend on the null distribution P0. A more
common quantity of interest is the global minimax risk
Rn(n, C) = sup
P0âˆˆC
Rn(P0, n, C). (10)
The corresponding global critical radius is
n(C) = inf
{
n : Rn(n, C) â‰¤ 1/2
}
. (11)
In typical non-parametric problems, the local minimax risk and the global minimax risk
match up to constants and this has led researchers in past work to focus on the global minimax
risk. We show that for the distribution testing problems we consider, the local critical radius
1Although our proofs are explicit in their dependence on Î±, we suppress this dependence in our notation
and in our main results treating Î± as a fixed strictly positive universal constant.
4
in (9) can vary considerably as a function of the null distribution P0. As a result, the global
critical radius, provides only a partial understanding of the intrinsic difficulty of this family of
hypothesis testing problems. In this paper, we focus on producing tight bounds on the local
minimax separation. These bounds yield as a simple corollary, sharp bounds on the global
minimax separation, but are in general considerably more refined.
Poissonization: In constructing upper bounds on the minimax riskâ€”we work under a
simplifying assumption that the sample size is random: n0 âˆ¼ Poisson(n). This assumption is
standard in the literature [1, 30], and simplifies several calculations. When the sample size is
chosen to be distributed as Poisson(n), it is straightforward to verify that for any fixed set
A,B âŠ‚ X with A âˆ©B = âˆ…, under P the number of samples falling in A and B are distributed
independently as Poisson(nP (A)) and Poisson(nP (B)) respectively.
In the Poissonized setting, we consider the averaged minimax risk, where we additionally
average the risk in (8) over the random sample size. The Poisson distribution is tightly
concentrated around its mean and this additional averaging only affects constant factors in
the minimax risk and we ignore this averaging in the rest of the paper.
2.2 Overview of our results
With the basic framework in place we now provide a high-level overview of the main results of
this paper. In the context of testing multinomials, the results of [30] characterize the local
and global minimax rates. We provide the following additional results:
â€¢ In Theorem 2 we characterize a simple and practical globally minimax test. In Theorem 4
building on the results of [10] we provide a simple (near) locally minimax test.
In the context of testing Lipschitz densities we make advances over classical results [12, 14]
by eliminating several unnecessary assumptions (uniform null, bounded support, fixed Lipschitz
parameter). We provide the first characterization of the local minimax rate for this problem.
In studying the Lipschitz testing problem in its full generality we find that the critical testing
radius can exhibit a wide range of possible behaviours, based roughly on the tail behaviour of
the null hypothesis.
â€¢ In Theorem 5 we provide a characterization of the local minimax rate for Lipschitz density
testing. In Section 4.1, we consider a variety of concrete examples that demonstrate the
rich scaling behaviour exhibited by the critical radius in this problem.
â€¢ Our upper and lower bounds are based on a novel spatially adaptive partitioning scheme.
We describe this scheme and derive some of its useful properties in Section 4.2.
In the Supplementary Material we provide the technical details of the proofs. We briefly
consider the limiting behaviour of our test statistics under the null in Appendix A. Our results
show that the critical radius is determined by a certain functional of the null hypothesis. In
Appendix D we study certain important properties of this functional pertaining to its stability.
Finally, we study tests which are adaptive to various parameters in Appendix F.
3 Testing high-dimensional multinomials
Given a sample Z1, . . . , Zn âˆ¼ P define the counts X = (X1, . . . , Xd) where Xj =
âˆ‘n
i=1 I(Zi =
j). The local minimax critical radii for the multinomial problem have been found in Valiant
and Valiant [30]. We begin by summarizing these results.
5
Without loss of generality we assume that the entries of the null multinomial p0 are sorted
so that p0(1) â‰¥ p0(2) â‰¥ . . . â‰¥ p0(d). For any 0 â‰¤ Ïƒ â‰¤ 1 we denote Ïƒ-tail of the multinomial by:
QÏƒ(p0) =
ï£±ï£²ï£³i :
dâˆ‘
j=i
p0(j) â‰¤ Ïƒ
ï£¼ï£½ï£¾ . (12)
The Ïƒ-bulk is defined to be
BÏƒ(p0) = {i > 1 : i /âˆˆ QÏƒ(p0)}. (13)
Note that i = 1 is excluded from the Ïƒ-bulk. The minimax rate depends on the functional:
VÏƒ(p0) =
ï£«ï£­ âˆ‘
iâˆˆBÏƒ(p0)
p0(i)
2/3
ï£¶ï£¸3/2 . (14)
For a given multinomial p0, our goal is to upper and lower bound the local critical radius
n(p0,M) in (9). We define, `n and un to be the solutions to the equations 2:
`n(p0) = max
{
1
n
,
âˆš
V`n(p0)(p0)
n
}
, un(p0) = max
{
1
n
,
âˆš
Vun(p0)/16(p0)
n
}
. (15)
With these definitions in place, we are now ready to state the result of [30]. We use
c1, c2, C1, C2 > 0 to denote positive universal constants.
Theorem 1 ([30]). The local critical radius n(p0,M) for multinomial testing is upper and
lower bounded as:
c1`n(p0) â‰¤ n(p0,M) â‰¤ C1un(p0). (16)
Furthermore, the global critical radius n(M) is bounded as:
c2d
1/4
âˆš
n
â‰¤ n(M) â‰¤
C2d
1/4
âˆš
n
.
Remarks:
â€¢ The local critical radius is roughly determined by the (truncated) 2/3-rd norm of the
multinomial p0. This norm is maximized when p0 is uniform and is small when p0 is
sparse, and at a high-level captures the â€œeffective sparsityâ€ of p0.
â€¢ The global critical radius can shrink to zero even when d n. When d 
âˆš
n almost all
categories of the multinomial are unobserved but it is still possible to reliably distinguish
any p0 from an `1-neighborhood. This phenomenon is noted for instance in the work of
[23]. We also note the work of Barron [2] that shows that when d = Ï‰(n), no test can
have power that approaches 1 at an exponential rate.
2These equations always have a unique solution since the right hand side monotonically decreases to 0 as
the left hand side monotonically increases from 0 to 1.
6
â€¢ The local critical radius can be much smaller than the global minimax radius. If the
multinomial p0 is nearly (or exactly) s-sparse then the critical radius is upper and lower
bounded up to constants by s1/4/
âˆš
n. Furthermore, these results also show that it is
possible to design consistent tests for sufficiently structured null hypotheses: in cases
when
âˆš
d n, and even in cases when d is infinite.
â€¢ Except for certain pathological multinomials, the upper and lower critical radii match up
to constants. We revisit this issue in Appendix D, in the context of Lipschitz densities,
where we present examples where the solutions to critical equations similar to (15) are
stable and examples where they are unstable.
In the remainder of this section we consider a variety of tests, including the test presented in
[30] and several alternatives. The test of [30] is a composite test that requires knowledge of n
and the analysis of their test is quite intricate. We present an alternative, simple test that is
globally minimax, and then present an alternative composite test that is locally minimax but
simpler to analyze. Finally, we present a few illustrative simulations.
3.1 The truncated Ï‡2 test
We begin with a simple globally minimax test. From a practical standpoint, the most popular
test for multinomials is Pearsonâ€™s Ï‡2 test. However, in the high-dimensional regime where the
dimension of the multinomial d is not treated as fixed the Ï‡2 test can have bad power due to
the fact that the variance of the Ï‡2 statistic is dominated by small entries of the multinomial
(see [20, 30]).
A natural thought then is to truncate the normalization factors of the Ï‡2 statistic in order
to limit the contribution to the variance from each cell of the multinomial. Recalling that
(X1, . . . , Xd) denote the observed counts, we propose the test statistic:
Ttrunc =
dâˆ‘
i=1
(Xi âˆ’ np0(i))2 âˆ’Xi
max{1/d, p0(i)}
:=
dâˆ‘
i=1
(Xi âˆ’ np0(i))2 âˆ’Xi
Î¸i
(17)
and the corresponding test,
Ï†trunc = I
ï£«ï£­Ttrunc > n
âˆšâˆšâˆšâˆš 2
Î±
dâˆ‘
i=1
p0(i)2
Î¸2i
ï£¶ï£¸ . (18)
This test statistic truncates the usual normalization factor for the Ï‡2 test for any entry which
falls below 1/d, and thus ensures that very small entries in p0 do not have a large effect on the
variance of the statistic. We emphasize the simplicity and practicality of this test. We have
the following result which bounds the power and size of the truncated Ï‡2 test. We use C > 0
to denote a positive universal constant.
Theorem 2. Consider the testing problem in (5). The truncated Ï‡2 test has size at most Î±,
i.e. P0(Ï†trunc = 1) â‰¤ Î±. Furthermore, there is a universal constant C > 0 such that if for any
0 < Î¶ â‰¤ 1 we have that,
2n â‰¥
C
âˆš
d
n
[
1âˆš
Î±
+
1
Î¶
]
, (19)
then the Type II error of the test is bounded by Î¶, i.e. P (Ï†trunc = 0) â‰¤ Î¶.
7
Remarks:
â€¢ A straightforward consequence of this result together with the result in Theorem 1 is
that the truncated Ï‡2 test is globally minimax optimal.
â€¢ The classical Ï‡2 and likelihood ratio tests are not generally consistent (and thus not
globally minimax optimal) in the high-dimensional regime (see also, Figure 2).
â€¢ At a high-level the proof follows by verifying that when the alternate hypothesis is true,
under the condition on the critical radius in (19), the test statistic is larger than the
threshold in (18). To verify this, we lower bound the mean and upper bound the variance
of the test statistic under the alternate and then use standard concentration results. We
defer the details to the Supplementary Material (Appendix B).
3.2 The 2/3-rd + tail test
The truncated Ï‡2 test described in the previous section, although globally minimax, is not
locally adaptive. The test from [30], achieves the local minimax upper bound in Theorem 1.
We refer to this as the 2/3-rd + tail test. We use a slightly modified version of their test when
testing Lipschitz goodness-of-fit in Section 4, and provide a description here.
The test is a composite two-stage test, and has a tuning parameter Ïƒ. Recalling the
definitions of BÏƒ(p0) andQÏƒ(p0) (see (12)), we define two test statistics T1, T2 and corresponding
test thresholds t1, t2:
T1(Ïƒ) =
âˆ‘
jâˆˆQÏƒ(p0)
(Xj âˆ’ np0(j)), t1(Î±, Ïƒ) =
âˆš
nP0(QÏƒ(p0))
Î±
,
T2(Ïƒ) =
âˆ‘
jâˆˆBÏƒ(p0)
(Xj âˆ’ np0(j))2 âˆ’Xj
p
2/3
j
, t2(Î±, Ïƒ) =
âˆšâˆ‘
jâˆˆBÏƒ 2n
2p0(j)2/3
Î±
.
We define two tests:
1. The tail test: Ï†tail(Ïƒ, Î±) = I(T1(Ïƒ) > t1(Î±, Ïƒ)).
2. The 2/3-test: Ï†2/3(Ïƒ, Î±) = I(T2(Ïƒ) > t2(Î±, Ïƒ)).
The composite test Ï†V (Ïƒ, Î±) is then given as:
Ï†V (Ïƒ, Î±) = max{Ï†tail(Ïƒ, Î±/2), Ï†2/3(Ïƒ, Î±/2)}. (20)
With these definitions in place, the following result is essentially from the work of [30]. We
use C > 0 to denote a positive universal constant.
Theorem 3. Consider the testing problem in (5). The composite test Ï†V (Ïƒ, Î±) has size at
most Î±, i.e. P0(Ï†V = 1) â‰¤ Î±. Furthermore, if we choose Ïƒ = n(p0,M)/8, and un(p0) as
in (15), then for any 0 < Î¶ â‰¤ 1, if
n(p0,M) â‰¥ Cun(p0) max{1/Î±, 1/Î¶}, (21)
then the Type II error of the test is bounded by Î¶, i.e. P (Ï†V = 0) â‰¤ Î¶.
Remarks:
8
â€¢ The test Ï†V is also motivated by deficiencies of the Ï‡2 test. In particular, the test
includes two main modifications to the Ï‡2 test which limit the contribution of the small
entries of p0: some of the small entries of p0 are dealt with via a separate tail test and
further the normalization of the Ï‡2 test is changed from p0(i) to p0(i)
2/3.
â€¢ This result provides the upper bound of Theorem 1. It requires that the tuning parameter
Ïƒ is chosen as n(p0,M)/8. In the Supplementary Material (Appendix F) we discuss
adaptive choices for Ïƒ.
â€¢ The proof essentially follows from the paper of [30], but we maintain an explicit bound
on the power and size of the test, which we use in later sections. We provide the details
in Appendix B.
While the 2/3-rd norm test is locally minimax optimal its analysis is quite challenging. In the
next section, we build on results from a recent paper of Diakonikolas and Kane [10] to provide
an alternative (nearly) locally minimax test with a simpler analysis.
3.3 The Max Test
An important insight, one that is seen for instance in Figure 1, is that many simple tests are
optimal when p0 is uniform and that careful modifications to the Ï‡
2 test are required only
when p0 is far from uniform. This suggests the following strategy: partition the multinomial
into nearly uniform groups, apply a simple test within each group and combine the tests with
an appropriate Bonferroni correction. We refer to this as the max test. Such a strategy was
used by Diakonikolas and Kane [10], but their test is quite complicated and involves many
constants. Furthermore, it is not clear how to ensure that their test controls the Type I error
at level Î±. Motivated by their approach, we present a simple test that controls the type I error
as required and is (nearly) locally minimax.
As with the test in the previous section, the test has to be combined with the tail test.
The test is defined to be
Ï†max(Ïƒ, Î±) = max{Ï†tail(Ïƒ, Î±/2), Ï†M (Ïƒ, Î±/2)},
where Ï†M is defined as follows. We partition BÏƒ(p0) into sets Sj for j â‰¥ 1, where
Sj =
{
t :
p0(2)
2j
< p0(t) â‰¤
p0(2)
2jâˆ’1
}
.
We can bound the total number of sets Sj by noting that for any i âˆˆ BÏƒ(p0), we have that
p0(i) â‰¥ Ïƒ/d, so that the number of sets k is bounded by dlog2(d/Ïƒ)e. Within each set we use
a modified `2 statistic. Let
Tj =
âˆ‘
tâˆˆSj
[(Xt âˆ’ np0(t))2 âˆ’Xt] (22)
for j â‰¥ 1. Unlike the traditional `2 statistic, each term in this statistic is centered around Xt.
As observed in [30], this results in the statistic having smaller variance in the n d regime.
Let
Ï†M (Ïƒ, Î±) =
âˆ¨
j
I(Tj > tj), (23)
9
where
tj =
âˆšâˆšâˆšâˆš2kn2 [âˆ‘tâˆˆSj p0(t)2]
Î±
. (24)
Theorem 4. Consider the testing problem in (5). Suppose we choose Ïƒ = n(p0,M)/8, then
the composite test Ï†max(Ïƒ, Î±) has size at most Î±, i.e. P0(Ï†max = 1) â‰¤ Î±. Furthermore, there is
a universal constant C > 0, such that for un(p0) as in (15), if for any 0 < Î¶ â‰¤ 1 we have that,
n(p0,M) â‰¥ Ck2un(p0) max
{âˆš
k
Î±
,
1
Î¶
}
, (25)
then the Type II error of the test is bounded by Î¶, i.e. P (Ï†max = 0) â‰¤ Î¶.
Remarks:
â€¢ Comparing the critical radii in Equations (25) and (16), and noting that k â‰¤ dlog2(8d/n)e,
we conclude that the max test is locally minimax optimal, up to a logarithmic factor.
â€¢ In contrast to the analysis of the 2/3-rd + tail test in [30], the analysis of the max
test involves mostly elementary calculations. We provide the details in Appendix B.
As emphasized in the work of [10], the reduction of testing problems to simpler testing
problems (in this case, testing uniformity) is a more broadly useful idea. Our upper
bound for the Lipschitz testing problem (in Section 4) proceeds by reducing it to a
multinomial testing problem through a spatially adaptive binning scheme.
3.4 Simulations
In this section, we report some simulation results that demonstrate the practicality of the
proposed tests. We focus on two simulation scenarios and compare the globally-minimax
truncated Ï‡2 test, and the 2/3rd + tail test [30] with the classical Ï‡2-test and the likelihood
ratio test. The Ï‡2 statistic is,
TÏ‡2 =
dâˆ‘
i=1
(Xi âˆ’ np0(i))2 âˆ’ np0(i)
np0(i)
,
and the likelihood ratio test statistic is
TLRT = 2
dâˆ‘
i=1
Xi log
(
Xi
np0(i)
)
.
In Appendix G, we consider a few additional simulations as well as a comparison with statistics
based on the `1 and `2 distances.
In each setting described below, we set the Î± level threshold via simulation (by sampling
from the null 1000 times) and we calculate the power under particular alternatives by averaging
over a 1000 trials.
10
â„“1 Distance
0 0.2 0.4 0.6 0.8 1
P
o
w
er
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Uniform Null, Dense Alternate
Chi-sq.
Truncated chi-sq
LRT
2/3rd and tail
â„“1 Distance
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
P
o
w
er
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Uniform Null, Sparse Alternate
Chi-sq.
Truncated chi-sq
LRT
2/3rd and tail
Figure 1: A comparison between the truncated Ï‡2 test, the 2/3rd + tail test [30], the Ï‡2-test
and the likelihood ratio test. The null is chosen to be uniform, and the alternate is either
a dense or sparse perturbation of the null. The power of the tests are plotted against the
`1 distance between the null and alternate. Each point in the graph is an average over 1000
trials. Despite the high-dimensionality (i.e. n = 200, d = 2000) the tests have high-power, and
perform comparably.
1. Figure 1 considers a high-dimensional setting where n = 200, d = 2000, the null distribu-
tion is uniform, and the alternate is either dense (perturbing each coordinate by a scaled
Rademacher) or sparse (perturbing only two coordinates).
In each case we observe that all the tests perform comparably indicating that a variety of
tests are optimal around the uniform distribution, a fact that we exploit in designing the
max test. The test from [30] performs slightly worse than others due to the Bonferroni
correction from applying a two-stage test.
2. Figure 2 considers a power-law null where p0(i) âˆ 1/i. Again we take n = 200, d = 2000,
and compare against a dense and sparse alternative. In this setting, we choose the sparse
alternative to only perturb the first two coordinates of the distribution.
We observe two notable effects. First, we see that when the alternate is dense, the
truncated Ï‡2 test, although consistent in the high-dimensional regime, is outperformed
by the other tests highlighting the need to study the local-minimax properties of tests.
Perhaps more surprising is that in the setting where the alternate is sparse, the classical
Ï‡2 and likelihood ratio tests can fail dramatically.
The locally minimax test is remarkably robust across simulation settings. However, it requires
that we specify n, a drawback shared by the max test. In Appendix F we provide adaptive
alternatives that adapt to unknown n.
4 Testing Lipschitz Densities
In this section, we focus our attention on the Lipschitz testing problem (6). As is standard
in non-parametric problems, throughout this section, we treat the dimension d as a fixed
11
â„“1 Distance
0 0.1 0.2 0.3 0.4 0.5 0.6
P
o
w
er
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Power Law Null, Dense Alternate
Chi-sq.
Truncated chi-sq
LRT
2/3rd and tail
â„“1 Distance
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
P
o
w
er
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Power Law Null, Sparse Alternate
Chi-sq.
Truncated chi-sq
LRT
2/3rd and tail
Figure 2: A comparison between the truncated Ï‡2 test, the 2/3rd + tail test [30], the Ï‡2-test
and the likelihood ratio test. The null is chosen to be a power law, and the alternate is either
a dense or sparse perturbation of the null. The power of the tests are plotted against the `1
distance between the null and alternate. Each point in the graph is an average over 1000 trials.
The truncated Ï‡2 test despite being globally minimax optimal, can perform poorly for any
particular fixed null. The Ï‡2 and likelihood ratio tests can fail to be consistent even when n
is quite large, and n
âˆš
d.
(universal) constant. Our emphasis is on understanding the local critical radius while making
minimal assumptions. In contrast to past work, we do not assume that the null is uniform or
even that its support is compact. We would like to be able to detect more subtle deviations
from the null as the sample size gets large and hence we do not assume that the Lipschitz
parameter Ln is fixed as n grows.
The classical method, due to Ingster [15, 16] to constructing lower and upper bounds on
the critical radius, is based on binning the domain of the density. In particular, upper bounds
were obtained by considering Ï‡2 tests applied to the multinomial that results from binning
the null distribution. Ingster focused on the case when the null distribution P0 was taken to
be uniform on [0, 1], noting that the testing problem for a general null distribution could be
â€œreducedâ€ to testing uniformity by modifying the observations via the quantile transformation
corresponding to the null distribution P0 (see also [12]). We emphasize that such a reduction
alters the smoothness class tailoring it to the null distribution P0. The quantile transformation
forces the deviations from the null distribution to be more smooth in regions where P0 is small
and less smooth where P0 is large, i.e. we need to re-interpret smoothness of the alternative
density p as an assumption about the function p(Fâˆ’10 (t)), where F
âˆ’1
0 is the quantile function of
the null distribution P0. We find this assumption to be unnatural and instead aim to directly
test the hypotheses in (6).
We begin with some high-level intuition for our upper and lower bounds.
â€¢ Upper bounding the critical radius: The strategy of binning domain of p0, and then
testing the resulting multinomial against an appropriate `1 neighborhood using a locally
minimax test is natural even when p0 is not uniform. However, there is considerable
flexibility in how precisely to bin the domain of p0. Essentially, the only constraint in
the choice of bin-widths is that the approximation error (of approximating the density
by its piecewise constant, histogram approximation) is sufficiently well-controlled. When
the null is not uniform the choice of fixed bin-widths is arbitrary and as we will see,
12
sub-optimal. A bulk of the technical effort in constructing our optimal tests is then in
determining the optimal inhomogenous, spatially adaptive partition of the domain in
order to apply a multinomial test.
â€¢ Lower bounding the critical radius: At a high-level the construction of Ingster
is similar to standard lower bounds in non-parametric problems. Roughly, we create a
collection of possible alternate densities, by evenly partitioning the domain of p0, and
then perturbing each cell of the partition by adding or subtracting a small (sufficiently
smooth) bump. We then analyze the optimal likelihood ratio test for the (simple versus
simple) testing problem of distinguishing p0 from a uniform mixture of the set of possible
alternate densities. We observe that when p0 is not uniform once again creating a
fixed bin-width partition is not optimal. The optimal choice of bin-widths is to choose
larger bin-widths when p0 is large and smaller bin-widths when p0 is small. Intuitively,
this choice allows us to perturb the null distribution p0 more when the density is large,
without violating the constraint that the alternate distribution remain sufficiently smooth.
Once again, we create an inhomogenous, spatially adaptive partition of the domain, and
then use this partition to construct the optimal perturbation of the null.
Define,
Î³ :=
2
3 + d
, (26)
and for any 0 â‰¤ Ïƒ â‰¤ 1 denote the collection of sets of probability mass at least 1âˆ’ Ïƒ as BÏƒ, i.e.
BÏƒ := {B : P0(B) â‰¥ 1âˆ’ Ïƒ}. Define the functional,
TÏƒ(p0) := inf
BâˆˆBÏƒ
(âˆ«
B
pÎ³0(x)dx
)1/Î³
. (27)
We refer to this as the truncated T -functional3. The functional TÏƒ(p0) is the analog of the
functional VÏƒ(p0) in (14), and roughly characterizes the local critical radius. We return to
study this functional in light of several examples, in Section 4.1 (and Appendix D).
In constructing lower bounds we will assume that the null density lies in the interior of the
Lipschitz ball, i.e. we assume that for some constant 0 â‰¤ cint < 1, we have that, p0 âˆˆ L(cintLn).
This assumption avoids certain technical issues that arise in creating perturbations of the null
density when it lies on the boundary of the Lipschitz ball.
Finally, we define for two universal constants C â‰¥ c > 0 (that are explicit in our proofs)
the upper and lower critical radii:
vn(p0) =
(Ld/2n TCvn(p0)(p0)
n
) 2
4+d
, wn(p0) =
(Ld/2n Tcwn(p0)(p0)
n
) 2
4+d
. (28)
With these preliminaries in place we now state our main result on testing Lipschitz densities.
We let c, C > 0 denote two positive universal constants (different from the ones above).
Theorem 5. The local critical radius n(p0,L(Ln)) for testing Lipschitz densities is upper
bounded as:
n(p0,L(Ln)) â‰¤ Cwn(p0). (29)
3Although the set B that achieves the minimum in the definition of TÏƒ(p0) need not be unique, the functional
itself is well-defined.
13
Furthermore, if for some constant 0 â‰¤ cint < 1 we have that, p0 âˆˆ L(cintLn), then the critical
radius is lower bounded as
cvn(p0) â‰¤ n(p0,L(Ln)). (30)
Remarks:
â€¢ A natural question of interest is to understand the worst-case rate for the critical radius,
or equivalently to understand the largest that the T -functional can be. Since the T -
functional can be infinite if the support is unrestricted, we restrict our attention to
Lipschitz densities with a bounded support S. In this case, letting Âµ(S) denote the
Lebesgue measure of S and using HoÌˆlderâ€™s inequality (see Appendix D) we have that for
any Ïƒ > 0,
TÏƒ(p0) â‰¤ (1âˆ’ Ïƒ)Âµ(S)
1âˆ’Î³
Î³ . (31)
Up to constants involving Î³, Ïƒ this is attained when p0 is uniform on the set S. In other
words, the critical radius is maximal for testing the uniform density against a Lipschitz,
`1 neighborhood. In this case, we simply recover a generalization of the result of [15] for
testing when p0 is uniform on [0, 1].
â€¢ The main discrepancy between the upper and lower bounds is in the truncation level, i.e.
the upper and lower bounds depend on the functional TÏƒ(p0) for different values of the
parameter Ïƒ. This is identical to the situation in Theorem 1 for testing multinomials. In
most non-pathological examples this functional is stable with respect to constant factor
discrepancies in the truncation level and consequently our upper and lower bounds are
typically tight (see the examples in Section 4.1). In the Supplementary Material (see
Appendix D) we formally study the stability of the T -functional. We provide general
bounds and relate the stability of the T -functional to the stability of the level-sets of p0.
The remainder of this section is organized as follows: we first consider various examples,
calculate the T -functional and develop the consequences of Theorem 5 for these examples.
We then turn our attention to our adaptive binning, describing both a recursive partitioning
algorithm for constructing it as well as developing some of its useful properties. Finally,
we provide the body of our proof of Theorem 5 and defer more technical aspects to the
Supplementary Material. We conclude with a few illustrative simulations.
4.1 Examples
The result in Theorem 5 provides a general characterization of the critical radius for testing any
density p0, against a Lipschitz, `1 neighborhood. In this section we consider several concrete
examples. Although our theorem is more generally applicable, for ease of exposition we focus
on the setting where d = 1, highlighting the variability of the T -functional and consequently
of the critical radius as the null density is changed. Our examples have straightforward
d-dimensional extensions.
When d = 1, we have that Î³ = 1/2 so the T -functional is simply:
TÏƒ(p0) = inf
BâˆˆBÏƒ
(âˆ«
B
âˆš
p0(x)dx
)2
,
where BÏƒ is as before. Our interest in general is in the setting where Ïƒ â†’ 0 (which happens as
nâ†’âˆž), so in some examples we will simply calculate T0(p0). In other examples however, the
truncation at level Ïƒ will play a crucial role and in those cases we will compute TÏƒ(p0).
14
Example 1 (Uniform null). Suppose that the null distribution p0 is Uniform[a, b] then,
T0(p0) = |bâˆ’ a|.
Example 2 (Gaussian null). Suppose that the null distribution p0 is a Gaussian, i.e. for some
Î½ > 0, Âµ âˆˆ R,
p0(x) =
1âˆš
2Ï€Î½
exp(âˆ’(xâˆ’ Âµ)2/(2Î½2)).
In this case, a simple calculation (see Appendix C) shows that,
T0(p0) = (8Ï€)
1/2Î½.
Example 3 (Beta null). Suppose that the null density is a Beta distribution:
p0(x) =
Î“(Î±+ Î²)
Î“(Î±)Î“(Î²)
xÎ±âˆ’1xÎ²âˆ’1 =
1
B(Î±, Î²)
xÎ±âˆ’1xÎ²âˆ’1,
where Î“ and B denote the gamma and beta functions respectively. It is easy to verify that,
T0(p0) =
(âˆ« 1
0
âˆš
p0(x)dx
)2
=
B2((Î±+ 1)/2, (Î² + 1)/2)
B(Î±, Î²)
.
To get some sense of the behaviour of this functional, we consider the case when Î± = Î² = tâ†’âˆž.
In this case, we show (see Appendix C) that for t â‰¥ 1,
Ï€2
4e4
tâˆ’1/2 â‰¤ T0(p0) â‰¤
e4
4
tâˆ’1/2.
In particular, we have that T0(p0)  tâˆ’1/2.
Remark:
â€¢ These examples illustrate that in the simplest settings when the density p0 is close to
uniform, the T -functional is roughly the effective support of p0. In each of these cases,
it is straightforward to verify that the truncation of the T -functional simply affects
constants so that the critical radius scales as:
n 
(âˆš
LnT0(p0)
n
)2/5
,
where T0(p0) in each case scales as roughly the size of the (1âˆ’ n)-support of the density
p0, i.e. as the Lebesgue measure of the smallest set that contains (1âˆ’ n) probability
mass. This motivates understanding the Lipschitz density with smallest effective support,
and we consider this next.
Example 4 (Spiky null). Suppose that the null hypothesis is:
p0(x) =
ï£±ï£´ï£´ï£²ï£´ï£´ï£³
Lnx 0 â‰¤ x â‰¤ 1âˆšLn
2âˆš
Ln
âˆ’ Lnx 1âˆšLn â‰¤ x â‰¤
2âˆš
Ln
0 otherwise,
then we have that T0(p0)  1âˆšLn .
15
Remark:
â€¢ For the spiky null distribution we obtain an extremely fast rate, i.e. we have that the
critical radius n  nâˆ’2/5, and is independent of the Lipschitz parameter Ln (although,
we note that the null p0 is more spiky as Ln increases). This is the fastest rate we
obtain for Lipschitz testing. In settings where the tail decay is slow, the truncation of
the T -functional can be crucial and the rates can be much slower. We consider these
examples next.
Example 5 (Cauchy distribution). The mean zero, Cauchy distribution with parameter Î± has
pdf:
p0(x) =
1
Ï€Î±
Î±2
x2 + Î±2
.
As we show (see Appendix C), the T -functional without truncation is infinite, i.e. T0(p0) =âˆž.
However, the truncated T -functional is finite. In the Supplementary Material we show that for
any 0 â‰¤ Ïƒ â‰¤ 0.5 (recall that our interest is in cases where Ïƒ â†’ 0),
4Î±
Ï€
[
ln2
(
1
Ïƒ
)]
â‰¤ TÏƒ(p0) â‰¤
4Î±
Ï€
[
ln2
(
2e
Ï€Ïƒ
)]
,
i.e. we have that TÏƒ(p0)  ln2(1/Ïƒ).
Remark:
â€¢ When the null distribution is Cauchy as above, we note that the rate for the critical
radius is no longer the typical n  nâˆ’2/5, even when the other problem specific
parameters (Ln and the Cauchy parameter Î±) are held fixed. We instead obtain a
slower n  (n/ log2 n)âˆ’2/5 rate. Our final example, shows that we can obtain an entire
spectrum of slower rates.
Example 6 (Pareto null). For a fixed x0 > 0 and for 0 < Î± < 1, suppose that the null
distribution is
p0(x) =
{
Î±xÎ±0
xÎ±+1
for x â‰¥ x0,
0 for x < x0.
This distribution for 0 < Î± < 1 has thicker tails than the Cauchy distribution. The T -functional
without truncation is infinite, i.e. T0(p0) =âˆž, and we can further show that (see Appendix C):
4Î±x0
(1âˆ’ Î±)2
(
Ïƒâˆ’
1âˆ’Î±
2Î± âˆ’ 1
)2
= TÏƒ(p0) â‰¤
4Î±x0
(1âˆ’ Î±)2
Ïƒâˆ’
1âˆ’Î±
Î± .
In the regime of interest when Ïƒ â†’ 0, we have that TÏƒ(p0) âˆ¼ Ïƒâˆ’
1âˆ’Î±
Î± .
Remark:
â€¢ We observe that once again, the critical radius no longer follows the typical rate:
n  nâˆ’2/5. Instead we obtain the rate, n  nâˆ’2Î±/(2+3Î±), and indeed have much slower
rates as Î±â†’ 0, indicating the difficulty of testing heavy-tailed distributions against a
Lipschitz, `1 neighborhood.
We conclude this section by emphasizing the value of the local minimax perspective and of
studying the goodness-of-fit problem beyond the uniform null. We are able to provide a sharp
characterization of the critical radius for a broad class of interesting examples, and we obtain
faster (than at uniform) rates when the null is spiky and non-standard rates in cases when the
null is heavy-tailed.
16
4.2 A recursive partitioning scheme
At the heart of our upper and lower bounds are spatially adaptive partitions of the domain of
p0. The partitions used in our upper and lower bounds are similar but not identical. In this
section, we describe an algorithm for producing the desired partitions and then briefly describe
some of the main properties of the partition that we leverage in our upper and lower bounds.
We begin by describing the desiderata for the partition from the perspective of the upper
bound. Our goal is to construct a test for the hypotheses in (6), and we do so by constructing
a partition (consisting of N + 1 cells) {A1, . . . , AN , Aâˆž} of Rd. Each cell Ai for i âˆˆ {1, . . . , N}
will be a cube, while the cell Aâˆž will be arbitrary but will have small total probability content.
We let,
K :=
Nâ‹ƒ
i=1
Ai. (32)
We form the multinomial corresponding to the partition {P0(A1), . . . , P0(AN ), P0(Aâˆž)}, where
P0(Ai) =
âˆ«
Ai
p0(x)dx. We then test this multinomial using the counts of the number of samples
falling in each cell of the partition.
Requirement 1: A basic requirement of the partition is that it must ensure that a density
p that is at least n far away in `1 distance from p0 should remain roughly n away from p0
when converted to a multinomial. Formally, for any p such that â€–pâˆ’ p0â€–1 â‰¥ n, p âˆˆ L(Ln) we
require that for some small constant c > 0,
Nâˆ‘
i=1
|P0(Ai)âˆ’ P (Ai)|+ |P0(Aâˆž)âˆ’ P (Aâˆž)| â‰¥ cn. (33)
Of course, there are several ways to ensure this condition is met. In particular, supposing
that we restrict attention to densities supported on [0, 1] then it suffices for instance to
choose roughly Ln/n even-width bins. This is precisely the partition considered in prior work
[1, 15, 16]. When we do not restrict attention to compactly supported, uniform densities an
even-width partition is no longer optimal and a careful optimization of the upper and lower
bounds with respect to the partition yields the optimal choice. The optimal partition has
bin-widths that are roughly taken proportional to pÎ³0(x), where the constant of proportionality
is chosen to ensure that the condition in (33) is satisfied. Precisely determining the constant
of proportionality turns out to be quite subtle so we defer a discussion of this to the end of
this section.
Requirement 2: A second requirement that arises in both our upper and lower bounds is
that the cells of our partition (except Aâˆž) are not chosen too wide. In particular, we must
choose the cells small enough to ensure that the density is roughly constant on each cell, i.e.
on each cell we need that for any i âˆˆ {1, . . . , N},
supxâˆˆAi p0(x)
infxâˆˆAi p0(x)
â‰¤ 3. (34)
Using the Lipschitz property of p0, this condition is satisfied if any point x is in a cell of
diameter at most p0(x)/(2Ln).
Taken together the first two requirements suggest that we need to create a partition such
that: for every point x âˆˆ K the diameter of the cell A containing the point x, should be
roughly,
diam(A) â‰ˆ min {Î¸1p0(x), Î¸2pÎ³0(x)} ,
17
where Î¸1 is to be chosen to be smaller than 1/(2Ln), and Î¸2 is chosen to ensure that Requirement
1 is satisfied.
Algorithm 1 constructively establishes the existence of a partition satisfying these require-
ments. The upper and lower bounds use this algorithm with slightly different parameters.
The key idea is to recursively partition cells that are too large by halving each side. This is
illustrated in Figure 3. The proof of correctness of the algorithm uses the smoothness of p0 in
an essential fashion. Indeed, were the density p0 not sufficiently smooth then such a partition
would likely not exist.
In order to ensure that the algorithm has a finite termination, we choose two parameters
a, b n (these are chosen sufficiently small to not affect subsequent results):
â€¢ We restrict our attention to the a-effective support of p0, i.e. we define Sa to be the
smallest cube centered at the mean of p0 such that, P0(Sa) â‰¥ 1 âˆ’ a. We begin with
Aâˆž = S
c
a.
â€¢ If the density in any cell is sufficiently small we do not split the cell further, i.e. for a
parameter b, if supxâˆˆA p0(x) â‰¤ b/vol(Sa) then we do not split it, rather we add it to Aâˆž.
By construction, such cells have total probability content at most b.
For each cube Ai for i âˆˆ {1, . . . , NÌƒ} we let xi denote its centroid, and we let NÌƒ denote the
number of cubes created by Algorithm 1.
Algorithm 1: Adaptive Partition
1. Input: Parameters Î¸1, Î¸2, a, b.
2. Set Aâˆž = âˆ… and A1 = Sa.
3. For each cube Ai do:
â€¢ If
sup
xâˆˆAi
p0(xi) â‰¤
b
vol(Sa)
, (35)
then remove Ai from the partition and let Aâˆž = Aâˆž âˆªAi.
â€¢ If
diam(Ai) â‰¤ min {Î¸1p0(xi), Î¸2pÎ³0(xi)} , (36)
then do nothing to Ai.
â€¢ If Ai fails to satisfy (35) or (36) then replace Ai by a set of 2d cubes that are obtained by
halving the original Ai along each of its axes.
4. If no cubes are split or removed, STOP. Else go to step 3.
5. Output: Partition P = {A1, . . . , ANÌƒ , Aâˆž}.
Requirement 3: The final major requirement is two-fold: (1) we require that the Î³-norm
of the density over the support of the partition should be upper bounded by the truncated
T -functional, and (2) that the density over the cells of the partition be sufficiently large.
This necessitates a further pruning of the partition, where we order cells by their probability
content and successively eliminate (adding them to Aâˆž) cells of low probability until we
18
200 400 600 800 1000
100
200
300
400
500
600
700
800
900
1000
Ã—10
-3
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
(a)
0 0.2 0.4 0.6 0.8 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
(b)
Figure 3: (a) A density p0 on [0, 1]
2 evaluated on a 1000Ã— 1000 grid. (b) The corresponding
spatially adaptive partition P produced by Algorithm 1. Cells of the partition are larger in
regions where the density p0 is higher.
have eliminated mass that is close to the desired truncation level. This is accomplished by
Algorithm 2.
Algorithm 2: Prune Partition
1. Input: Unpruned partition P = {A1, . . . , ANÌƒ , Aâˆž} and a target pruning level c. Without
loss of generality we assume P0(A1) â‰¥ P0(A2) â‰¥ . . . â‰¥ P0(ANÌƒ ).
2. For any j âˆˆ {1, . . . , NÌƒ} let Q(j) =
âˆ‘NÌƒ
i=j P0(Ai). Let j
âˆ— denote the smallest positive integer
such that, Q(jâˆ—) â‰¤ c.
3. If Q(jâˆ—) â‰¥ c/5:
â€¢ Set N = jâˆ— âˆ’ 1, and Aâˆž = Aâˆž âˆªAjâˆ— âˆª . . . âˆªANÌƒ .
4. If Q(jâˆ—) â‰¤ c/5:
â€¢ Set N = jâˆ— âˆ’ 1, Î± = min{c/(5P0(AN )), 1/5}, and Aâˆž = Aâˆž âˆªAjâˆ— âˆª . . . âˆªANÌƒ .
â€¢ AN is a cube, i.e. for some Î´ > 0, AN = [a1, a1 + Î´]Ã— Â· Â· Â· Ã— [ad, ad + Î´]. Let
D1 = [a1, (1âˆ’ Î±)(a1 + Î´)]Ã— Â· Â· Â· Ã— [ad, (1âˆ’ Î±)(ad + Î´)] and D2 = AN âˆ’D1. Set: AN = D1
and Aâˆž = Aâˆž âˆªD2.
5. Output: Pâ€  = {A1, . . . , AN , Aâˆž}.
It remains to specify a precise choice for the parameter Î¸2. We do so indirectly by defining
a function Âµ : R 7â†’ R that is closely related to the truncated T -functional. For x âˆˆ R we define
Âµ(x) as the smallest positive number that satisfies the equation:
n =
âˆ«
Rd
min
{
p0(y)
x
,
np0(y)
Î³
Âµ(x)
}
dy. (37)
If x < 1/n then we obtain a finite value for Âµ(x), otherwise we take Âµ(x) =âˆž. The following
19
result, relates Âµ to the truncated T -functional.
Lemma 1. For any 0 â‰¤ x < 1/n,
T Î³xn(p0) â‰¤ Âµ(x) â‰¤ 2T
Î³
xn/2
(p0). (38)
With the definition of Âµ in place, we now state our main result regarding the partitions
produced by Algorithms 1 and 2. We let P denote the unpruned partition obtained from
Algorithm 1 and Pâ€  denote the pruned partition obtained from Algorithm 2. For each cell Ai
we denote its centroid by xi. We have the following result summarizing some of the important
properties of P and Pâ€ .
Lemma 2. Suppose we choose, Î¸1 = 1/(2Ln), Î¸2 = n/(8LnÂµ(1/4)), a = b = n/1024, c =
n/512, then the partition Pâ€  satisfies the following properties:
1. [Diameter control] The partition has the property that,
1
5
min {Î¸1p0(xi), Î¸2pÎ³0(xi)} â‰¤ diam(Ai) â‰¤ min {Î¸1p0(xi), Î¸2p
Î³
0(xi)} . (39)
2. [Multiplicative control] The density is multiplicatively controlled on each cell, i.e. for
i âˆˆ {1, . . . , N} we have that,
supxâˆˆAi p0(x)
infxâˆˆAi p0(x)
â‰¤ 2. (40)
3. [Properties of Aâˆž] The cell Aâˆž has probability content roughly n, i.e.
n
2560
â‰¤ P (Aâˆž) â‰¤
n
256
. (41)
4. [`1 distance] The partition preserves the `1 distance, i.e. for any p such that â€–pâˆ’p0â€–1 â‰¥
n, p âˆˆ L(Ln),
Nâˆ‘
i=1
|P0(Ai)âˆ’ P (Ai)|+ |P0(Aâˆž)âˆ’ P (Aâˆž)| â‰¥
n
8
. (42)
5. [Truncated T -functional] Recalling the definition of K in (32), we have that,âˆ«
K
pÎ³0(x)dx â‰¤ T
Î³
n/5120
(p0). (43)
6. [Density Lower Bound] The density over K is lower bounded as:
inf
xâˆˆK
p0(x) â‰¥
(
n
5120Âµ(1/5120)
)1/(1âˆ’Î³)
. (44)
Furthermore, for any choice of the parameter Î¸2 the unpruned partition P of Algorithm 1
satisfies (39) with the constant 5 sharpened to 4, (40) and the upper bound in (41).
20
The proof of this result is technical and we defer it to Appendix E.
While we focused our discussions on the properties of the partition from the perspective of
establishing the upper bound in Theorem 5 it turns out that several of these properties are
crucial in proving the lower bound as well. The optimal adaptive partition creates larger cells
in regions where the density p0 is higher, and smaller cells where p0 is lower. This might seem
counter-intuitive from the perspective of the upper bound since we create many low-probability
cells which are likely to be empty in a small finite-sample, and indeed this construction is
in some sense opposite to the quantile transformation suggested by previous work [12, 15].
However, from the perspective of the lower bound this is completely natural. It is intuitive
that our perturbation be large in regions where the density is large since the likelihood ratio is
relatively stable in these regions, and hence these changes are more difficult to detect. The
requirement of smoothness constrains the amount by which we can we can perturb the density
on any given cell, i.e. for a large perturbation the corresponding cell should have a large
diameter leading to the conclusion that we must use larger cells in regions where p0 is higher.
In this section, we have focused on providing intuition for our adaptive partitioning scheme.
In the next section we provide the body of the proof of Theorem 5, and defer the remaining
technical aspects to the Supplementary Material.
4.3 Proof of Theorem 5
We consider the lower and upper bounds in turn.
4.3.1 Proof of Lower Bound
We note that the lower bound in (30) is trivial when n â‰¥ 1/C so throughout the proof we
focus on the case when n is smaller than a universal constant, i.e. when n â‰¤ 1C .
Preliminaries: We begin by briefly introducing the lower bound technique due to Ingster
(see for instance [14]). Let P be a set of distributions and let Î¦n be the set level Î± tests based
on n observations where 0 < Î± < 1 is fixed. We want to bound the minimax type II error
Î¶n(P) = inf
Ï†âˆˆÎ¦n
sup
PâˆˆP
Pn(Ï† = 0).
Define Q as Q(A) =
âˆ«
Pn(A)dÏ€(P ), where Ï€ is a prior distribution whose support is contained
in P. In particular, if Ï€ is uniform on a finite set P1, . . . , PN then
Q(A) =
1
N
âˆ‘
j
Pnj (A).
Given n observations we define the likelihood ratio
Wn(Z1, . . . , Zn) =
dQ
dPn0
=
âˆ«
p(Z1, . . . , Zn)
p0(Z1, . . . , Zn)
dÏ€(p) =
âˆ« âˆ
j
p(Zj)
p0(Zj)
dÏ€(p).
Lemma 3. Let 0 < Î¶ < 1âˆ’ Î±. If
E0[W 2n(Z1, . . . , Zn)] â‰¤ 1 + 4(1âˆ’ Î±âˆ’ Î¶)2 (45)
then Î¶n(P) â‰¥ Î¶.
21
Roughly, this result asserts that in order to produce a minimax lower bound on the Type
II error, it suffices to appropriately upper bound the second moment under the null of the
likelihood ratio. The proof is standard but presented in Appendix E for completeness. A
natural way to construct the prior Ï€ on the set of alternatives, is to partition the domain of p0
and then to locally perturb p0 by adding or subtracting sufficiently smooth â€œbumpsâ€. In the
setting where the partition has fixed-width cells this construction is standard [1, 15] and we
provide a generalization to allow for variable width partitions and to allow for non-uniform p0.
Formally, let Ïˆ be a smooth bounded function on the hypercube I = [âˆ’1/2, 1/2]d such thatâˆ«
I
Ïˆ(x)dx = 0 and
âˆ«
I
Ïˆ2(x)dx = 1.
Let P = {A1, . . . , AN , Aâˆž} be any partition that satisfies the condition in (40), and further
let {x1, . . . , xN} denote the centroids of the cells {A1, . . . , AN}. Suppose further, that each
cell Aj for j âˆˆ {1, . . . , N} is a cube with side-length cjhj for some constants cj â‰¤ 1, and
hj =
1âˆš
d
min{Î¸1p0(xj), Î¸2pÎ³0(xj)}.
Let Î· = (Î·1, Î·2, . . . , Î·N ) be a Rademacher sequence and define
pÎ· = p0 +
Nâˆ‘
j=1
ÏjÎ·jÏˆj (46)
where each Ïj â‰¥ 0 and
Ïˆj(t) =
1
c
d/2
j h
d/2
j
Ïˆ
(
tâˆ’ xj
cjhj
)
for t âˆˆ Aj . Hence,
âˆ«
Aj
Ïˆj(t) = 0 and
âˆ«
Aj
Ïˆ2j (t) = 1. Finally, let us denote:
Ï‰1 := max
{
â€–Ïˆâ€–âˆž,
8â€–Ïˆâ€²â€–âˆž
(1âˆ’ cint)
}
, and Ï‰2 := â€–Ïˆâ€–1.
With these definitions in place we state a result that gives a lower bound for a sequence of
perturbations Ïj that satisfy certain conditions.
Lemma 4. Let Î±, Î¶ and n be non-negative numbers with 1âˆ’Î±âˆ’Î¶ > 0. Let C0 = 1+4(1âˆ’Î±âˆ’Î¶)2.
Assume that for each j âˆˆ {1, . . . , N}, Ïj and hj satisfy:
(a) Ïj â‰¤
c
d/2
j
Ï‰1
Lnh
1+ d
2
j (47)
(b)
Nâˆ‘
j=1
Ïjc
d/2
j h
d/2
j â‰¥
n
Ï‰2
(48)
(c)
Nâˆ‘
j=1
Ï4j
p20(xj)
â‰¤ logC0
4n2
, (49)
then the Type II error of any test is at least Î¶.
22
Effectively, this lemma generalizes the result of Ingster [15] to allow for non-uniform p0
and further allows for variable width bins. The proof proceeds by verifying that under the
conditions of the lemma, pÎ· is sufficiently smooth, and separated from p0 by at least n in the
`1 metric. We let the prior be uniform on the the set of possible distributions pÎ· and directly
analyze the second moment of the likelihood ratio, and obtain the result by applying Lemma 3.
See Appendix E for the proof of this lemma. It is worth noting the condition in (47), which
ensures smoothness of pÎ·, allows for larger perturbations Ïj for bins where hj is large, which
is one of the key benefits of using variable bin-widths in the lower bound.
With this result in place, to produce the desired minimax lower bound it only remains to
specify the partition, select a sequence of perturbations Ïj and verify that the conditions of
Lemma 4 are satisfied.
Final Steps: We begin by specifying the partition. We define,
Î½ = min
{
Ï‰2
Ï‰14d+1
âˆš
d
, 1
}
.
For the lower bound we do not need to prune the partition, rather we simply apply Algorithm 1
with Î¸1 = 1/(2Ln), and Î¸2 = n/(LnÎ½Âµ(2/Î½)). We choose a = b = n/1024, and denote
the resulting partition P = {A1, . . . , AN , Aâˆž}. Using Lemma 2 we have that the partition
satisfies (39) with the constant 5 replaced by 4, (40) and the upper bound in (41). We now
choose a sequence {Ï1, . . . , ÏN}, and proceed to verify that the conditions of Lemma 4 are
satisfied. We choose,
Ïj =
c
d/2
j
Ï‰1
Lnh
1+ d
2
j ,
thus ensuring the condition in (47) is satisfied.
Verifying the condition in (48): Recall the definition of Âµ in (37),
n
Î½
=
âˆ«
Rd
min
{
p0(y)
2
,
np0(y)
Î³
Î½Âµ(2/Î½)
}
dy,
provided that n â‰¤ Î½/2 which is true by our assumption on the critical radius. Recalling the
definition of K in (32), we have that,âˆ«
K
min
{
p0(y)
2
,
np0(y)
Î³
Î½Âµ(2/Î½)
}
dy â‰¥ n
Î½
âˆ’ P (Aâˆž)
2
.
We define the function
h(y) :=
1âˆš
d
min
{
p0(y)
2Ln
,
np0(y)
Î³
LnÎ½Âµ(2/Î½)
}
,
and as a consequence of the property (40) we obtain that for any y âˆˆ Aj for j âˆˆ {1, . . . , N},
hj â‰¥
h(y)
2
.
This in turn yields that,
L
Nâˆ‘
j=1
hd+1j â‰¥
1
2
âˆš
d
âˆ«
K
min
{
p0(y)
2
,
np0(y)
Î³
Î½Âµ(2/Î½)
}
dy â‰¥ 1
2
âˆš
d
(
n
Î½
âˆ’ P (Aâˆž)
2
)
â‰¥ n
4
âˆš
dÎ½
,
23
where the final step uses the upper bound in (41). We then have that,
Nâˆ‘
j=1
Ïjc
d/2
j h
d/2
j =
Nâˆ‘
j=1
Lnc
d
jh
d+1
j
Ï‰1
â‰¥
Nâˆ‘
j=1
Lnh
d+1
j
4dÏ‰1
â‰¥ n
Ï‰2
,
which establishes the condition in (48).
Verifying the condition in (49): We note the inequality (which can be verified by simple
case analysis) that for a, b, u, v â‰¥ 0,
min{a, b} â‰¤ min{a
u
u+v b
v
u+v , b},
in particular for u = 1, v = 3 + d we obtain,
min{a, b} â‰¤ min{(ab3+d)
1
4+d , b}. (50)
Returning to the condition in (49) we have that,
Nâˆ‘
j=1
Ï4j
p0(xj)2
â‰¤ L
4
n
Ï‰41
Nâˆ‘
j=1
c2dj h
4+2d
j
p0(xj)2
â‰¤ L
4
n
Ï‰41
Nâˆ‘
j=1
hdjh
4+d
j
p0(xj)2
,
using the fact that cj â‰¤ 1. Using the chosen values for hj we obtain that,
Nâˆ‘
j=1
Ï4j
p0(xj)2
â‰¤ L
4
n
Ï‰41
âˆš
d
4+d
Nâˆ‘
j=1
hdj
p0(xj)2
min
{[
p0(xj)
2Ln
]4+d
,
[
np
Î³
0(xj)
LnÎ½Âµ(2/Î½)
]4+d}
(i)
â‰¤ L
4
n
Ï‰41
âˆš
d
4+d
Nâˆ‘
j=1
hdj
p0(xj)2
min
{
p0(xj)
33+dn
2Ln(LnÎ½Âµ(2/Î½))3+d
,
[
np
Î³
0(xj)
LnÎ½Âµ(2/Î½)
]4+d}
=
3+dn
LdnÂµ(2/Î½)
3+dÎ½4+dÏ‰41
âˆš
d
4+d
Nâˆ‘
j=1
hdj min
{
p0(xj)
2/Î½
,
np0(xj)
Î³
Âµ(2/Î½)
}
â‰¤ 2
3+d
n
LdnÂµ(2/Î½)
3+dÎ½4+dÏ‰41
âˆš
d
4+d
âˆ«
K
min
{
p0(x)
2/Î½
,
np0(x)
Î³
Âµ(2/Î½)
}
dx
â‰¤ 2
3+d
n
LdnÂµ(2/Î½)
3+dÎ½4+dÏ‰41
âˆš
d
4+d
âˆ«
Rd
min
{
p0(x)
2/Î½
,
np0(x)
Î³
Âµ(2/Î½)
}
dx
(ii)
â‰¤ 2
4+d
n
LdnÂµ(2/Î½)
3+dÎ½4+dÏ‰41
âˆš
d
4+d
.
where (i) follows from the inequality in (50), and (ii) uses (37). Using Lemma 1 we obtain,
Âµ(2/Î½) â‰¥ T Î³2n/Î½(p0),
provided that n â‰¤ Î½/2. This yields that,
Nâˆ‘
j=1
Ï4j
p0(xj)2
â‰¤ 2
4+d
n
LdnT
2
2n/Î½
(p0)Î½4+dÏ‰41
âˆš
d
4+d
,
24
and we require that this quantity is upper bounded by logC0
4n2
. For constants c1, c2 that depend
only on the dimension d it suffices to choose n as the solution to the equation:
n =
(
L
d/2
n Tc2n(p0) logC0
c1n
)2/(4+d)
and an application of Lemma 4 yields the lower bound of Theorem 5.
4.3.2 Proof of Upper Bound
In order to establish the upper bound, we construct an adaptive partition using Algorithms 1
and 2, and utilize the test analyzed in Theorem 3 from [30] to test the resulting multinomial. For
the upper bound we use the partition Pâ€  studied in Lemma 2, i.e. we take Î¸1 = 1/(2Ln), Î¸2 =
n/(8LnÂµ(1/4)), a = b = n/1024 and c = n/512. Using the property in (42), it suffices to
upper bound the V -functional in (14), for Ïƒ = n/128.
The following technical lemma shows that the truncated V -functional is upper bounded by
the V -functional over the partition excluding Aâˆž. For the partition Pâ€ , we have the associated
multinomial q := {P0(A1), . . . , P0(Aâˆž)}. With these definitions in place we have the following
result.
Lemma 5. For the multinomial q defined above, the truncated V -functional is upper bounded
as:
V
2/3
n/128
(q) â‰¤
Nâˆ‘
i=1
P0(Ai)
2/3 := Îº.
We prove this result in Appendix E. Roughly, this lemma asserts that our pruning is less
aggressive than the tail truncation of the multinomial test from the perspective of controlling
the 2/3-rd norm. With this claim in place it only remains to upper bound Îº. Using the
property in (40) we have that,
Îº â‰¤
Nâˆ‘
i=1
(2p0(xi)vol(Ai))
2/3 â‰¤ 22/3
Nâˆ‘
i=1
p0(xi)
2/3
h
d/3
i
hdi .
Using the condition in (44) verify that for all x âˆˆ K we have that
Î¸1p0(x) â‰¥
np
Î³
0(x)
10240LnÂµ(1/5120)
,
and this yields that for a constant c > 0 for each i âˆˆ {1, . . . , N},
hi â‰¥
cnp
Î³
0(xi)
LnÂµ(1/5120)
.
Using the property in (40) we then obtain that for a constant C > 0,
Îº â‰¤ C
(
LnÂµ(1/5120)
n
)d/3 âˆ«
K
pÎ³0(x)dx,
25
and using the property (43) and Lemma 1, we obtain that for constants c, C > 0 that,
Îº â‰¤ C
(
Ln
n
)d/3
T 2/3cn (p0).
With Lemma 5 we obtain that for the multinomial q,
Vn/128(q) â‰¤ C
3/2
(
Ln
n
)d/2
Tcn(p0),
which together with the upper bound of Theorem 1 yields the desired upper bound for
Theorem 5. We note that a direct application of Theorem 1 yields a bound on the critical
radius that is the maximum of two terms, one scaling as 1/n and the other being the desired
term in Theorem 5. In Lipschitz testing, the 1/n term is always dominated by the term
involving the truncated functional. This follows from the lower bound on the truncated
functional shown in (86).
4.4 Simulations
In this section, we report some simulation results on Lipschitz testing. We focus on the case
when d = 1. In Figure 4 we compare the following tests:
1. 2/3-rd + Tail Test: This is the locally minimax test studied in Theorem 5, where we use
our binning Algorithm followed by the locally minimax multinomial test from [30].
2. Chi-sq. Test: Here we use our binning Algorithm followed by the standard Ï‡2 test.
3. Kolmogorov-Smirnov (KS) Test: Since we focus on the case when d = 1, we also compare
to the standard KS test based on comparing the CDF of p0 to the empirical CDF.
4. Naive Binning: Finally, we compare to the approach of using fixed-width bins, together
with the Ï‡2 test. Following the prescription of Ingster [15] (for the case when p0 is
uniform) we choose the number of bins so that the `1-distance between the null and
alternate is approximately preserved, i.e. denoting the effective support to be S we
choose the bin-width as n/(LnÂµ(S)).
We focus on two simulation scenarios: when the null distribution is a standard Gaussian,
and when the null distribution has a heavier tail, i.e. is a Pareto distribution with parameter
Î± = 0.5. We create the alternate density by smoothly perturbing the null after binning, and
choose the perturbation weights as in our lower bound construction in order to construct a
near worst-case alternative.
We set the Î±-level threshold via simulation (by sampling from the null 1000 times) and we
calculate the power under particular alternatives by averaging over a 1000 trials. We observe
several notable effects. First, we see that the locally minimax test can significantly out perform
the KS test as well the test based on fixed bin-widths. The failure of the fixed bin-width
test is more apparent in the setting where the null is Pareto as the distribution has a large
effective support and the naive binning is far less parsimonious than the adaptive binning. On
the other hand, we also observe that at least in these simulations the Ï‡2 test and the locally
minimax test from [30] perform comparably when based on our adaptive binning indicating
the crucial role played by the binning procedure.
26
â„“1 Distance
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
P
o
w
er
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Gaussian Null, Dense Alternate
2/3-rd and tail
KS test
Chi-sq test
Naive Binning + Chi-sq
â„“1 Distance
0.1 0.15 0.2 0.25 0.3 0.35 0.4
P
o
w
er
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Generalized Pareto with parameter 0.5 null, Dense Alternate
2/3-rd and tail
KS test
Chi-sq test
Naive Binning + Chi-sq
Figure 4: A comparison between the KS test, multinomial tests on an adaptive binning and
multinomial tests on a fixed bin-width binning. In the figure on the left we choose the null
to be standard Gaussian and on the right we choose the null to be Pareto. The alternate is
chosen to be a dense near worst-case, smooth perturbation of the null. The power of the tests
are plotted against the `1 distance between the null and alternate. Each point in the graph is
an average over 1000 trials.
5 Discussion
In this paper, we studied the goodness-of-fit testing problem in the context of testing multino-
mials and Lipschitz densities. For testing multinomials, we built on prior works [10, 30] to
provide new globally and locally minimax tests. For testing Lipschitz densities we provide the
first results that give a characterization of the critical radius under mild conditions.
Our work highlights the heterogeneity of the critical radius in the goodness-of-fit testing
problem and the importance of understanding the local critical radius. In the multinomial
testing problem it is particularly noteworthy that classical tests can perform quite poorly in
the high-dimensional setting, and that simple modifications of these tests can lead to more
robust inference. In the density testing problem, carefully constructed spatially adaptive
partitions play a crucial role.
Our work motivates several open questions, and we conclude by highlighting a few of them.
First, in the context of density testing we focused on the case when the density is Lipschitz.
An important extension would be to consider higher-order smoothness. Surprisingly, Ingster
[16] shows that bin-based tests continue to be optimal for higher-order smoothness classes
when the null is uniform on [0, 1]. We conjecture that bin-based tests are no longer optimal
when the null is not uniform, and further that the local critical radius is roughly determined
by the solution to:
n(p0) 
[
L
d/2s
n Sn(p0)(p0)
n
]2s/(4s+d)
,
where the functional S is defined as in (27) with Î³ = 2s/(3s+ d), and Ln is the radius of the
HoÌˆlder ball. Second, it is possible to invert our locally minimax tests in order to construct
confidence intervals. We believe that these intervals might also have some local adaptive
properties that are worthy of further study. Finally, in the Appendix, we provide some basic
27
results on the limiting distributions of the multinomial test statistics under the null when the
null is uniform, and it would be interesting to consider the extension to settings where the
null is arbitrary.
6 Acknowledgements
This work was partially supported by the NSF grant DMS-1713003. The authors would
like to thank the participants of the Oberwolfach workshop on â€œStatistical Recovery of
Discrete, Geometric and Invariant Structuresâ€, for their generous feedback. Suggestions by
various participants including David Donoho, Richard Nickl, Markus Reiss, Vladimir Spokoiny,
Alexandre Tsybakov, Martin Wainwright, Yuting Wei and Harry Zhou have been incorporated
in various parts of this manuscript.
References
[1] Ery Arias-Castro, Bruno Pelletier, and Venkatesh Saligrama. Remember the curse of
dimensionality: The case of goodness-of-fit testing in arbitrary dimension, 2016.
[2] Andrew R. Barron. Uniformly powerful goodness of fit tests. Ann. Statist., 17(1):107â€“124,
03 1989.
[3] Tugkan Batu, Lance Fortnow, Eldar Fischer, Ravi Kumar, Ronitt Rubinfeld, and Patrick
White. Testing random variables for independence and identity. In 42nd Annual Symposium
on Foundations of Computer Science, FOCS 2001, 14-17 October 2001, Las Vegas, Nevada,
USA, pages 442â€“451, 2001.
[4] Tony Cai and Mark Low. A framework for estimation of convex functions. Statistica
Sinica, 2015.
[5] G. Casella and R.L. Berger. Statistical Inference. Duxbury advanced series in statistics
and decision sciences. Thomson Learning, 2002.
[6] Sourav Chatterjee. A new perspective on least squares under convex constraint. Ann.
Statist., 42(6):2340â€“2381, 12 2014. doi: 10.1214/14-AOS1254.
[7] Harald CrameÌr. On the composition of elementary errors. Scandinavian Actuarial Journal,
1928(1):13â€“74, 1928.
[8] L. Devroye and L. Gyorfi. Nonparametric Density Estimation: The L1 View. Wiley
Interscience Series in Discrete Mathematics. Wiley, 1985.
[9] Persi Diaconis and Frederick Mosteller. Methods for Studying Coincidences, pages 605â€“622.
Springer New York, New York, NY, 2006.
[10] Ilias Diakonikolas and Daniel M. Kane. A new approach for testing properties of discrete
distributions. 2016 IEEE 57th Annual Symposium on Foundations of Computer Science
(FOCS), 2016.
[11] Stephen E. Fienberg. The use of chi-squared statistics for categorical data problems.
Journal of the Royal Statistical Society. Series B (Methodological), 41(1):54â€“64, 1979.
28
[12] E. GineÌ and R. Nickl. Mathematical Foundations of Infinite-Dimensional Statistical
Models. Cambridge University Press, 2015.
[13] Oded Goldreich and Dana Ron. On testing expansion in bounded-degree graphs. In
Studies in Complexity and Cryptography., pages 68â€“75. Springer, 2011.
[14] Y. Ingster and I.A. Suslina. Nonparametric Goodness-of-Fit Testing Under Gaussian
Models. Lecture Notes in Statistics. Springer, 2003.
[15] Yu. I. Ingster. Minimax detection of a signal in `p metrics. Journal of Mathematical
Sciences, 68(4):503â€“515, 1994.
[16] Yuri Izmailovich Ingster. Adaptive chi-square tests. Zapiski Nauchnykh Seminarov POMI,
244:150â€“166, 1997.
[17] L. LeCam. Convergence of estimates under dimensionality restrictions. Ann. Statist., 1
(1):38â€“53, 01 1973.
[18] E.L. Lehmann and G. Casella. Theory of Point Estimation. Springer Verlag, 1998. ISBN
0387985026.
[19] E.L. Lehmann and J.P. Romano. Testing Statistical Hypotheses. Springer Texts in
Statistics. Springer New York, 2006.
[20] Paul Marriott, Radka Sabolova, Germain Van Bever, and Frank Critchley. Geometry
of Goodness-of-Fit Testing in High Dimensional Low Sample Size Modelling. Springer
International Publishing, 2015.
[21] Carl Morris. Central limit theorems for multinomial sums. Ann. Statist., 3(1):165â€“188, 01
1975.
[22] J. Neyman and E. S. Pearson. On the problem of the most efficient tests of statisti-
cal hypotheses. Philosophical Transactions of the Royal Society of London. Series A,
Containing Papers of a Mathematical or Physical Character, 231:289â€“337, 1933.
[23] Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled
discrete data. IEEE Trans. Information Theory, 54(10):4750â€“4755, 2008.
[24] Karl Pearson. On the criterion that a given system of deviations from the probable in the
case of a correlated system of variables is such that it can be reasonably supposed to have
arisen from random sampling. Philosophical Magazine Series 5, 50(302):157â€“175, 1900.
[25] Timothy R. C. Read and Noel A. C. Cressie. Goodness-of-fit statistics for discrete
multivariate data. Springer-Verlag Inc, 1988.
[26] Dana Ron. Property testing: A learning theory perspective. Foundations and Trends RÂ©
in Machine Learning, 1(3):307â€“402, 2008.
[27] N. V. Smirnov. On the Estimation of the Discrepancy Between Empirical Curves of
Distribution for Two Independent Samples. Bul. Math. de lâ€™Univ. de Moscou, 2:3â€“14,
1939.
[28] G.W. Snedecor and W.G. Cochran. Statistical methods. Iowa State University Press,
1980.
29
[29] V. G. Spokoiny. Adaptive hypothesis testing using wavelets. Ann. Statist., 24(6):2477â€“2498,
12 1996.
[30] Gregory Valiant and Paul Valiant. An automatic inequality prover and instance optimal
identity testing. 2014 IEEE 55th Annual Symposium on Foundations of Computer Science
(FOCS), pages 51â€“60, 2014.
[31] R. Von Mises. Wahrscheinlichkeit, Statistik und Wahrheit. Schriften zur wissenschaftlichen
Weltauffassung. J. Springer, 1928.
[32] S. S. Wilks. The large-sample distribution of the likelihood ratio for testing composite
hypotheses. Ann. Math. Statist., 9(1):60â€“62, 03 1938.
A Limiting behaviour of test statistics under the null
In this section, we consider the problem of finding the asymptotic distribution of the multinomial
test statistics under the null. Broadly, there is a dichotomy between classical asymptotics
where the null distribution is kept fixed and a high-dimensional asymptotic where the number
of cells is growing and the null distribution can vary with the number of cells. We present a
few simple results on the limiting behaviour of our test statistics when the null is uniform and
highlight some open problems. Although our techniques generalize in a straightforward way to
non-uniform null distributions, they do not necessarily yield tight results.
We focus on the family of test statistics that we use in our paper, that are weighted Ï‡2-type
statistics:
T (w) =
dâˆ‘
i=1
(Xi âˆ’ np0(i))2 âˆ’Xi
wi
, (51)
where each wi is a positive weight that is a fixed function of p0(i). This family includes the
2/3-rd statistic from [30], the truncated Ï‡2 statistic that we propose, and the usual Ï‡2 and `2
statistics. When the null is uniform, this family of test statistics reduces to simple re-scalings
of the `2 statistic in (22):
T`2 =
dâˆ‘
i=1
[
(Xi âˆ’ np0(i))2 âˆ’Xi
]
.
Our results are summarized in the following lemma.
Lemma 6. 1. Classical Asymptotics: For any fixed p0, the statistic T (w) under the null
converges in distribution to a weighted sum of Ï‡2 distributions, i.e. for Z1, . . . , Zd âˆ¼ Ï‡21,
T (w)
dâ†’
dâˆ‘
i=1
pi
wi
(Zi âˆ’ 1) . (52)
2. High-dimensional Asymptotics: Suppose p0 is uniform and dâ†’âˆž, then we have that,
â€¢ If n/
âˆš
dâ†’âˆž, then
T`2âˆš
Var0(T`2)
dâ†’ N(0, 1).
30
â€¢ If n/
âˆš
dâ†’ 0, then
T`2âˆš
Var0(T`2)
dâ†’ Î´0.
Remarks:
â€¢ The behaviour of the Ï‡2-type statistics under classical asymptotics is well understood
and we do not prove the claim in (52).
â€¢ Focusing on the high-dimensional setting, the asymptotic distribution of the test statistic
is Gaussian in the regime where the risk of the optimal test tends to 0 as nâ†’âˆž, and is
degenerate in the regime where there are no consistent tests. In the most interesting
regime when, n/
âˆš
d â†’ c, the optimal test can have non-trivial risk, and the limiting
distribution is neither Gaussian nor degenerate.
â€¢ More broadly, an important open question is to characterize the limiting distribution
of the test statistic, under both the null and the alternate in the high-dimensional
asymptotic.
Proof. The first part follows, by checking the Lyapunov conditions. We denote
Î¶i = (Xi âˆ’ np0(i))2 âˆ’Xi.
and can calculate the sum of the variances as:
s2d =
dâˆ‘
i=1
var(Î¶i) =
2n2
d
.
The Lyapunov condition then requires that,
lim
dâ†’âˆž
1
s4d
dâˆ‘
i=1
EÎ¶4i = 0.
A straightforward computation gives that,
EÎ¶4i = 8
n2
d2
+ 144
n3
d3
+ 60
n4
d4
,
so that the Lyapunov condition is satisfied provided that,
lim
dâ†’âˆž
d3
n6
â†’ 0,
which is indeed the case.
In order to verify the degenerate limit it suffices to show that when n/
âˆš
dâ†’ 0, then the
number of categories that have strictly larger than one occurrence converges to 0. When each
observed category is observed only once we have that the test statistic is deterministic, i.e.,
T`2 =
dâˆ‘
i=1
Î¶i = (dâˆ’ n)
n2
d2
+ n
(
n2
d2
âˆ’ 2n
d
)
.
31
When rescaled by the standard deviation we obtain that,
T`2âˆš
var0(T`2)
=
âˆš
d
2n2
[
(dâˆ’ n)n
2
d2
+ n
(
n2
d2
âˆ’ 2n
d
)]
â†’ 0.
Finally, we can bound the probability that any category is observed more than once as:
P (âˆƒ i,Xi â‰¥ 2) â‰¤
dâˆ‘
i=1
P (Xi â‰¥ 2)
â‰¤
dâˆ‘
i=1
exp(âˆ’Î»)
âˆžâˆ‘
k=2
(n
d
)k
â‰¤ Cn
2
d
â†’ 0.
Taken together these facts give the desired degenerate limit.
B Analysis of Multinomial Tests
B.1 Proof of Theorem 2
In this section we analyze the truncated Ï‡2 test. For convenience, throughout this proof we
we work with a scaled version of the statistic in (17), i.e. we let T := Ttrunc/d and abusing
notation slightly we redefine Î¸i appropriately, i.e. we take Î¸i = max{1, dp0(i)}.
We begin by controlling the size of the truncated Ï‡2 test. Fix any multinomial p on [d],
and suppose we denote âˆ†i = p0(i)âˆ’ p(i), then a straightforward computation shows that,
Ep[T ] = n2
dâˆ‘
i=1
âˆ†2i
Î¸i
, (53)
Varp[T ] =
dâˆ‘
i=1
1
Î¸2i
[
2n2p0(i)
2 + 2n2âˆ†2i âˆ’ 4n2âˆ†ip0(i) + 4n3âˆ†2i p0(i)âˆ’ 4n3âˆ†3i
]
. (54)
This yields that the null variance of T is given by:
Var0[T ] =
dâˆ‘
i=1
2n2p0(i)
2
Î¸2i
,
which together with Chebyshevâ€™s inequality yields the desired bound on the size. Turning our
attention to the power of the test we fix a multinomial p âˆˆ P1. Denote the Î± level threshold
of the test by
tÎ± = n
âˆšâˆšâˆšâˆš 2
Î±
dâˆ‘
i=1
p0(i)2
Î¸2i
.
32
We observe that, if we can verify the following two conditions:
tÎ± â‰¤
Ep[T ]
2
(55)
Ep[T ] â‰¥ 2
âˆš
Varp[T ]
Î¶
, (56)
then we obtain that P (Ï†trunc = 0) â‰¤ Î¶. To see this, observe that
P (Ï†trunc = 0) â‰¤ P1(T âˆ’ Ep[T ] < tÎ± âˆ’ Ep[T ])
â‰¤ P1((T âˆ’ Ep[T ])2 < (tÎ± âˆ’ Ep[T ])2)
â‰¤ Varp[T ]
(tÎ± âˆ’ Ep[T ])2
â‰¤ 4Varp[T ]
Ep[T ]2
â‰¤ Î¶.
Condition in Equation (55): This condition reduces to verifying the following,
2tÎ± â‰¤ n2
dâˆ‘
i=1
âˆ†2i
Î¸i
,
and as a result we focus on lower bounding the mean under the alternate. By Cauchy-Schwarz
we obtain that,
dâˆ‘
i=1
âˆ†2i
Î¸i
â‰¥ â€–âˆ†â€–
2
1âˆ‘d
i=1 Î¸i
â‰¥ 
2
nâˆ‘d
i=1{1 + dp0(i)}
â‰¥ 
2
n
2d
. (57)
We can further upper bound tÎ± as
tÎ± = n
âˆšâˆšâˆšâˆš 2
Î±
dâˆ‘
i=1
p0(i)2
Î¸2i
â‰¤ n
âˆš
2
dÎ±
,
using the fact that p0(i)/Î¸i â‰¤ 1d . This yields that Equation (55) is satisfied if:
2n
2d
â‰¥ 2
âˆš
2âˆš
dÎ±n
,
which is indeed the case.
Condition in Equation (56): We can upper bound the variance under the alternate as:
Varp[T ] â‰¤
dâˆ‘
t=1
1
Î¸2t
[
4n2p0(t)
2 + 4n2âˆ†2t + 4n
3âˆ†2t p0(t)âˆ’ 4n3âˆ†3t
]
=
dâˆ‘
t=1
4n2p0(t)
2
Î¸2tï¸¸ ï¸·ï¸· ï¸¸
U1
+
dâˆ‘
t=1
4n2âˆ†2t
Î¸2tï¸¸ ï¸·ï¸· ï¸¸
U2
+
dâˆ‘
t=1
4n3âˆ†2t p0(t)
Î¸2tï¸¸ ï¸·ï¸· ï¸¸
U3
+
dâˆ‘
t=1
âˆ’4n3âˆ†3t
Î¸2tï¸¸ ï¸·ï¸· ï¸¸
U4
.
Consequently, it suffices to verify that,
4âˆ‘
i=1
2
âˆš
Ui/Î¶
Ep[T ]
â‰¤ 1.
33
for i = {1, 2, 3, 4} and we do this by bounding each of these terms in turn. For the first term
we follow a similar argument to the one dealing with the first condition,
2
âˆš
U1/Î¶
Ep[T ]
â‰¤
8d
âˆšâˆ‘d
t=1
p0(t)2
Î¸2tâˆš
Î¶n2n
â‰¤ 8
âˆš
dâˆš
Î¶n2n
â‰¤ 1
4
.
For the second term,
2
âˆš
U2/Î¶
Ep[T ]
â‰¤
4
âˆš
1
Î¶
âˆ‘d
t=1
n2âˆ†2t
Î¸2t
Ep[T ]
â‰¤
4
âˆš
1
Î¶
âˆ‘d
t=1
n2âˆ†2t
Î¸t
Ep[T ]
=
4âˆš
Î¶Ep[T ]
.
Using Equation (57) we obtain that,
Ep[T ] â‰¥
n22n
2d
,
which in turn yields that,
2
âˆš
U2/Î¶
Ep[T ]
â‰¤ 8
âˆš
d
nn
âˆš
Î¶
â‰¤ 1
4
.
Turning our attention to the third term we obtain that,
2
âˆš
U3/Î¶
Ep[T ]
=
4
âˆš
1
Î¶
âˆ‘d
t=1
n3âˆ†2t p0(t)
Î¸2t
Ep[T ]
â‰¤
4
âˆš
n
dÎ¶
âˆ‘d
t=1
n2âˆ†2t
Î¸t
Ep[T ]
=
4
âˆš
n
dÎ¶âˆš
Ep[T ]
.
Using the lower bound on the mean we obtain that,
2
âˆš
U3/Î¶
Ep[T ]
â‰¤ 8
nn
âˆš
Î¶
â‰¤ 1
4
.
For the final term,
2
âˆš
U4/Î¶
Ep[T ]
â‰¤
4
âˆš
1
Î¶
âˆ‘d
t=1
n3|âˆ†3t |
Î¸2t
Ep[T ]
â‰¤
4
âˆš
n3
Î¶
âˆ‘d
t=1
|âˆ†3t |
Î¸2t
Ep[T ]
â‰¤
4
âˆš
1
Î¶
(âˆ‘d
i=1
n2âˆ†2i
Î¸
4/3
i
)3/2
Ep[T ]
,
where the last step uses the monotonicity of the `p norms. Observing that Î¸i â‰¥ 1, we have
2
âˆš
U4/Î¶
Ep[T ]
â‰¤
4
âˆš
1
Î¶
(âˆ‘d
i=1
n2âˆ†2i
Î¸i
)3/2
Ep[T ]
=
4
âˆš
1
Î¶
Ep[T ]1/4
â‰¤ 8d
1/4
âˆš
Î¶
1/2
n
âˆš
n
â‰¤ 1
4
.
This completes the proof.
34
B.2 Proof of Theorem 3
Recall the definition of BÏƒ in (13). We define:
âˆ†BÏƒ =
âˆ‘
iâˆˆBÏƒ
|p0(i)âˆ’ p(i)|, (58)
and
pmin,Ïƒ = min
iâˆˆBÏƒ
p0(i). (59)
Our main results concern the combined test Ï†V in (20). It is easy to verify that the size of
this test is at most Î± so it only remains to control its power. We first provide a general result
that allows for a range of possible values for the parameter Ïƒ.
Lemma 7. For any Ïƒ â‰¤ n8 , if
n â‰¥ 2 max
{
2
Î±
,
1
Î¶
}
max
{
1
Ïƒ
,
4096VÏƒ/2(p0)
2n
}
,
then the Type II error P (Ï†V = 0) â‰¤ Î¶.
Taking this lemma as given, it is straightforward to verify the result of Theorem 3. In
particular, if we take Ïƒ = n/8, then we recover the result of the theorem.
Proof of Lemma 7: As a preliminary, we state two technical results from [30]. The following
result is Lemma 6 in [30].
Lemma 8. For any c â‰¥ 1, suppose that n â‰¥ cmax
{
VÏƒ(p0)1/3
p
1/3
min,Ïƒâˆ†BÏƒ
, VÏƒ(p0)
âˆ†2BÏƒ
}
, then we have that
Varp(T2(Ïƒ)) â‰¤
16
c
[Ep(T2(Ïƒ))]2 .
The following result appears in the proof of Proposition 1 of [30].
Lemma 9. For any c â‰¥ 1, suppose that,
n â‰¥ 2c
VÏƒ/2(p0)
âˆ†2BÏƒ
,
then we have that,
n â‰¥ cmax
{
VÏƒ(p0)
1/3
p
1/3
min,Ïƒâˆ†BÏƒ
,
VÏƒ(p0)
âˆ†2BÏƒ
}
.
With these two results in place, we can now complete the proof. We divide the space of
alternatives into two sets:
S1 =
ï£±ï£²ï£³p : â€–pâˆ’ p0â€–1 â‰¥ n, âˆ‘
iâˆˆQÏƒ(p0)
|p0(i)âˆ’ p(i)| â‰¥ 3Ïƒ
ï£¼ï£½ï£¾
S2 =
ï£±ï£²ï£³p : â€–pâˆ’ p0â€–1 â‰¥ n, âˆ‘
iâˆˆQÏƒ(p0)
|p0(i)âˆ’ p(i)| < 3Ïƒ
ï£¼ï£½ï£¾ .
35
In order to show desired result it then suffices to show that when p âˆˆ S1, P (Ï†1 = 0) â‰¤ Î¶, and
that when p âˆˆ S2, P (Ï†2 = 0) â‰¤ Î¶. We verify each of these claims in turn.
When p âˆˆ S1: In this case, we have that P (QÏƒ(p0)) â‰¥ 2Ïƒ. Under the alternate we have that
T1(Ïƒ) âˆ¼ Poi(nP (QÏƒ(p0)))âˆ’ nP0(QÏƒ(p0)). This yields,
P (Ï†tail = 0) â‰¤ P (Poi(nP (QÏƒ(p0))) < ÏnP (QÏƒ(p0))) , (60)
where
Ï =
P0(QÏƒ(p0))
P (QÏƒ(p0))
+
1
P (QÏƒ(p0))
âˆš
P0(QÏƒ(p0))
nÎ±
.
Provided Ï â‰¤ 1 we obtain via Chebyshevâ€™s inequality that,
P (Ï†tail = 0) â‰¤
1
n(1âˆ’ Ï)2P1(QÏƒ(p0))
.
We further have that,
Ï â‰¤ 1
2
[
1 +
1âˆš
nÎ±Ïƒ
]
.
Under the conditions that,
n â‰¥ 4
Î±Ïƒ
,
we obtain that Ï â‰¤ 1/2, which yields that,
P (Ï†tail = 0) â‰¤
2
nÏƒ
â‰¤ Î¶,
where the final inequality uses the condition on n.
When p âˆˆ S2: In this case, we first observe that the bulk deviation must be sufficiently large.
Concretely, at most n/2 deviation can occur in the largest element and at most 3Ïƒ occurs in
the tail, i.e.
âˆ†BÏƒ â‰¥
n
2
âˆ’ 3Ïƒ â‰¥ n
8
.
Our next goal will be to upper bound the test threshold, t2(Î±/2, Ïƒ). In particular, we claim
that,
t2(Î±/2, Ïƒ) â‰¤
âˆš
2Var(T2(Ïƒ))
Î±
(61)
Taking this claim as given for now and supposing that our sample size can be written as
n = cmax
{
TÏƒ(p0)1/3
p
1/3
min,Ïƒâˆ†BÏƒ
, TÏƒ(p0)
âˆ†2BÏƒ
}
, for some c â‰¥ 1, we can use Lemma 8 and Chebyshevâ€™s inequality
to obtain that,
P (Ï†2/3 = 0) â‰¤
1
(
âˆš
c
16 âˆ’
âˆš
2
Î±)
2
,
36
provided that
âˆš
c
16 â‰¥
âˆš
2
Î± . Thus, it suffices to ensure that,
n â‰¥ 64 max
{
2
Î±
,
1
Î¶
}
max
{
TÏƒ(p0)
1/3
p
1/3
min,Ïƒâˆ†BÏƒ
,
TÏƒ(p0)
âˆ†2BÏƒ
}
,
to obtain that P (Ï†2/3 = 0) â‰¤ Î¶ as desired. Using Lemma 9, we have that this holds under the
condition on n. It remains to verify the claim in (61). In order to do so we just note that the
variance of the statistic is minimized at the null, i.e.
Var(T2(Ïƒ)) â‰¥
âˆ‘
iâˆˆBÏƒ
2n2p0(i)
2/3 =
Î±t22(Î±/2, Ïƒ)
2
.
as desired.
B.3 Proof of Theorem 4
Fix any multinomial p on [d], and suppose we denote âˆ†i = p0(i)âˆ’ p(i), then a straightforward
computation shows that,
Ep[Tj ] = n2
âˆ‘
tâˆˆSj
âˆ†2t , (62)
Varp[Tj ] =
âˆ‘
tâˆˆSj
[
2n2p0(t)
2 + 2n2âˆ†2t âˆ’ 4n2âˆ†tp0(t) + 4n3âˆ†2t p0(t)âˆ’ 4n3âˆ†3t
]
. (63)
This in turn yields that the null variance of Tj is simply Var0[Tj ] = 2n
2
âˆ‘
tâˆˆSj p(t)
2. By
Chebyshevâ€™s inequality we then obtain that:
P0(Tj > tj) â‰¤ Î±/k,
which together with the union bound yields,
P0(Ï†max = 1) â‰¤ Î±.
As in the proof of Theorem 3 we consider two cases: when p âˆˆ S1 and when p âˆˆ S2. Since the
composite test includes the tail test, the analysis of the case when p âˆˆ S1 is identical to before.
Now, we consider the case when p âˆˆ S2.
We have further partitioned the bulk of the distribution into at most k sets, so that at
least one of the sets Sj must witness a discrepancy of at least n/(8k), i.e. when p âˆˆ S2 we
have that,
sup
j
âˆ‘
iâˆˆSj
|p0(i)âˆ’ p(i)| â‰¥
n
8k
.
Let jâˆ— denote the set that witnesses this discrepancy. We focus the rest of the proof on this
fixed set Sjâˆ— and show that under the alternate Tjâˆ— > tjâˆ— with sufficiently high probability.
Suppose that for jâˆ— we can verify the following two conditions:
tjâˆ— â‰¤
Ep[Tjâˆ— ]
2
(64)
Ep[Tjâˆ— ] â‰¥ 2
âˆš
Varp[Tjâˆ— ]
Î¶
, (65)
37
then we obtain that P (Ï†max = 0) â‰¤ Î¶. To see this, observe that
P (Ï†max = 0) â‰¤ P (Tjâˆ— âˆ’ Ep[Tjâˆ— ] < tjâˆ— âˆ’ Ep[Tjâˆ— ])
â‰¤ P ((Tjâˆ— âˆ’ Ep[Tjâˆ— ])2 < (tjâˆ— âˆ’ Ep[Tjâˆ— ])2)
â‰¤ Varp[Tj
âˆ— ]
(tjâˆ— âˆ’ Ep[Tjâˆ— ])2
â‰¤ 4Varp[Tj
âˆ— ]
Ep[Tjâˆ— ]2
â‰¤ Î¶.
Consequently, we focus the rest of the proof on showing the above two conditions. We let djâˆ—
denote the size of Sjâˆ— .
Condition in Equation (64): Observe that,
âˆ‘
iâˆˆSjâˆ—
âˆ†2i â‰¥
(âˆ‘
iâˆˆSjâˆ— |âˆ†i|
)2
djâˆ—
â‰¥ 
2
n
64k2djâˆ—
. (66)
Using Equations (24) and (62), it suffices to check that,
n
âˆš
2k
âˆ‘
iâˆˆSjâˆ— p0(i)
2
Î±
â‰¤ n2
âˆ‘
iâˆˆSjâˆ—
âˆ†2j ,
and applying the lower bound in Equation (66) it suffices if,
2n
64k2djâˆ—
â‰¥ 1
n
âˆš
2k
âˆ‘
iâˆˆSjâˆ— p0(i)
2
Î±
.
Denote the maximum and minimum entry of the multinomial on Sjâˆ— as bjâˆ— and ajâˆ— respectively.
Noting that on each bin the multinomial is roughly uniform one can further observe that,
djâˆ—
âˆšâˆ‘
iâˆˆSjâˆ—
p0(i)2 â‰¤ d3/2jâˆ— bjâˆ— â‰¤ 2d
3/2
jâˆ— ajâˆ— â‰¤ 2Vn/8(p0).
This yields that the first condition is satisfied if,
2n â‰¥
256k5/2
n
Vn/8(p0)âˆš
Î±
,
which is indeed the case.
Condition in Equation (65): We proceed by upper bounding the variance under the
alternate. Using Equation (63) we have,
Varp[Tjâˆ— ] =
âˆ‘
tâˆˆSjâˆ—
[
2n2p0(t)
2 + 2n2âˆ†2t âˆ’ 4n2âˆ†tp0(t) + 4n3âˆ†2t p0(t)âˆ’ 4n3âˆ†3t
]
â‰¤
âˆ‘
tâˆˆSjâˆ—
[
4n2p0(t)
2 + 4n2âˆ†2t + 4n
3âˆ†2t p0(t)âˆ’ 4n3âˆ†3t
]
â‰¤ 4n2b2jâˆ—djâˆ—ï¸¸ ï¸·ï¸· ï¸¸
U1
+ 4n2
âˆ‘
tâˆˆSjâˆ—
âˆ†2tï¸¸ ï¸·ï¸· ï¸¸
U2
+ 4n3bjâˆ—
âˆ‘
tâˆˆSjâˆ—
âˆ†2tï¸¸ ï¸·ï¸· ï¸¸
U3
âˆ’4n3
âˆ‘
tâˆˆSjâˆ—
âˆ†3tï¸¸ ï¸·ï¸· ï¸¸
U4
.
38
In order to check the desired condition, it suffices to verify that
Ep[Tjâˆ— ] â‰¥ 8
âˆš
Ui
Î¶
,
for each i âˆˆ {1, 2, 3, 4}. We consider these tasks in sequence. For the first term we obtain that
it suffices if,
âˆ‘
iâˆˆSjâˆ—
âˆ†2i â‰¥
(
16bjâˆ—d
1/2
jâˆ—
n
âˆš
Î¶
)
,
and applying the lower bound in Equation (66), and from some straightforward algebra it is
sufficient to ensure that,
2n â‰¥
2048k2Vn/8(p0)
n
âˆš
Î¶
,
which is indeed the case. For the second term, some simple algebra yields that it suffices to
have that, âˆ‘
iâˆˆSjâˆ—
âˆ†2i â‰¥
(
144
n2Î¶
)
. (67)
In order to establish this, we need to appropriately lower bound n. Let pmin denote the smallest
entry in Bn/8(p0). For a sufficiently large universal constant C > 0, let us denote:
Î¸k,Î± := Ck
2
[âˆš
k
Î±
+
1
Î¶
]
.
Then using the lower bound on n we obtain,
n â‰¥
Î¸k,Î±Vn/16(p0)
2n
=
Î¸k,Î±Vn/16(p0)
1/3
[âˆ‘
iâˆˆBn/16(p0)
p0(i)
2/3
]
2n
.
Now denote B = Bn/16(p0)\Bn/8(p0), then we have that,
pmin +
âˆ‘
iâˆˆB
pi â‰¥ n/16,
so that,
âˆ‘
iâˆˆBn/16(p0)
p0(i)
2/3 â‰¥
âˆ‘
iâˆˆB
p0(i)
2/3 + p
2/3
min =
1
p
1/3
min
[âˆ‘
iâˆˆB
p0(i)
2/3p
2/3
min + pmin
]
â‰¥ n
16p
1/3
min
.
This gives the lower bound,
n â‰¥
Î¸k,Î±Vn/16(p0)
1/3
16np
1/3
min
â‰¥
Î¸k,Î±
(âˆ‘
tâˆˆSjâˆ— p0(t)
2/3
)1/2
16np
1/3
min
â‰¥
Î¸k,Î±
âˆš
djâˆ—
16n
.
39
Returning to the bound in Equation (67), and using the lower bound in Equation (66) we
obtain that it suffices to ensure that
n
8k
âˆš
djâˆ—
â‰¥
(
192n
Î¸k,Î±
âˆš
djâˆ—
âˆš
Î¶
)
,
which is indeed the case. Turning our attention to the term involving U3 we have, that by
some simple algebra it suffices to verify that,âˆ‘
iâˆˆSjâˆ—
âˆ†2i â‰¥
(
144bjâˆ—
nÎ¶
)
.
Using the lower bound in Equation (66) we obtain that it is sufficient to ensure,
2n
64k2djâˆ—
â‰¥
(
144bjâˆ—
nÎ¶
)
,
and with the observation that djâˆ—bjâˆ— â‰¤ 2d3/2jâˆ— ajâˆ— â‰¤ 2Vn/8(p0) we obtain,
2n â‰¥
(
18432k2Vn/8(p0)
nÎ¶
)
,
which is indeed the case. Finally, we turn our attention to the term involving U4. In this case
we have that it suffices to show that,
n1/2
âˆ‘
iâˆˆSjâˆ—
âˆ†2i â‰¥ 16
âˆšâˆ‘
iâˆˆSjâˆ— âˆ†
3
i
Î¶
,
by the monotonicity of the `p norm it suffices then to show that,
n1/2
âˆ‘
iâˆˆSjâˆ—
âˆ†2i â‰¥ 16
âˆšâˆšâˆšâˆš[âˆ‘iâˆˆSjâˆ— âˆ†2i ]3/2
Î¶
,
and after some simple algebra this yields that it suffices to have,âˆ‘
iâˆˆSjâˆ—
âˆ†2i â‰¥
(16)4
Î¶2n2
,
and this follows from an essentially identical argument to the one handling the term involving
U2. This completes the proof.
C Proofs for Examples of Lipschitz Testing
In this Section we provide proofs of the claims in Section 4.1. For convenience, we restate all
the claims in the following lemma.
Lemma 10. â€¢ Suppose that p0 is a standard one-dimensional Gaussian, with mean Âµ,
and variance Î½2, then we have that:
T0(p0) = (8Ï€)
1/2Î½. (68)
40
â€¢ Suppose that p0 is a Beta distribution with parameters Î±, Î². Then we have,
T0(p0) =
(âˆ« 1
0
âˆš
p0(x)dx
)2
=
B2((Î±+ 1)/2, (Î² + 1)/2)
B(Î±, Î²)
, (69)
where B : R2 7â†’ R is the Beta function. Furthermore, if we take Î± = Î² = t â‰¥ 1, then we
have that,
Ï€2
4e4
tâˆ’1/2 â‰¤ T0(p0) â‰¤
e4
4
tâˆ’1/2. (70)
â€¢ Suppose that p0 is Cauchy with parameter Î±, then we have that,
T0(p0) =âˆž. (71)
Furthermore, if 0 â‰¤ Ïƒ â‰¤ 0.5 then,
4Î±
Ï€
[
ln2
(
1
Ïƒ
)]
â‰¤ TÏƒ(p0) â‰¤
4Î±
Ï€
[
ln2
(
2e
Ï€Ïƒ
)]
. (72)
â€¢ Suppose that p0 has a Pareto distribution with parameter Î± then we have that,
T0(p0) =âˆž, (73)
while the truncated T -functional satisfies:
4Î±x0
(1âˆ’ Î±)2
(
Ïƒâˆ’
1âˆ’Î±
2Î± âˆ’ 1
)2
= TÏƒ(p0) â‰¤
4Î±x0
(1âˆ’ Î±)2
Ïƒâˆ’
1âˆ’Î±
Î± . (74)
Proof. Notice that Claims (71) and (73) follow by taking Ïƒ â†’ 0 in Claims (72) and (74)
respectively. We prove the remaining claims in turn.
Proof of Claim (68): Observe that,
T0(p0) =
1âˆš
2Ï€Î½
(âˆ« âˆž
âˆ’âˆž
exp(âˆ’(xâˆ’ Âµ)2/(4Î½2))dx
)2
=
1âˆš
2Ï€Î½
4Ï€Î½2
=
âˆš
8Ï€Î½.
Proof of Claim (69): The Beta density can be written as:
p0(x) =
Î“(Î±+ Î²)
Î“(Î±)Î“(Î²)
xÎ±âˆ’1xÎ²âˆ’1 =
1
B(Î±, Î²)
xÎ±âˆ’1xÎ²âˆ’1,
where Î“ : R 7â†’ R denotes the Gamma function. Some simple algebra yields that the T -
functional is simply:
T0(p0) =
âˆ« 1
0
âˆš
p0(x)dx =
B((Î±+ 1)/2, (Î² + 1)/2)âˆš
B(Î±, Î²)
. (75)
41
Proof of Claim (70): We now take Î± = Î² = t â‰¥ 1 in the above expression. To prove the
claim we use standard approximations to the Beta function derived using Stirlingâ€™s formula.
Recall, that by Stirlingâ€™s formula we have that:
âˆš
2Ï€n
(n
e
)n
â‰¤ n! â‰¤ e
âˆš
n
(n
e
)n
.
We begin by upper bounding the Beta function for integers Î±, Î² â‰¥ 0:
B(Î±, Î²) =
Î“(Î±)Î“(Î²)
Î“(Î±+ Î²)
=
(Î±âˆ’ 1)!(Î² âˆ’ 1)!
(Î±+ Î² âˆ’ 1)!
=
Î±!Î²!
(Î±+ Î²)!
Î±+ Î²
Î±Î²
â‰¤ e
2
âˆš
2Ï€
Î±+ Î²
Î±Î²
âˆš
Î±Î²Î±Î±Î²Î² exp(Î±+ Î²)âˆš
Î±+ Î²(Î±+ Î²)Î±+Î² exp(Î±+ Î²)
=
e2âˆš
2Ï€
âˆš
Î±+ Î²
Î±Î²
Î±Î±Î²Î²
(Î±+ Î²)Î±+Î²
.
Now, setting Î± = Î² = t â‰¥ 1, we obtain:
B(t, t) â‰¤ e
2
âˆš
Ï€
2âˆ’2tâˆš
t
.
We can similarly lower bound the Beta function as:
B(t, t) â‰¥ 2
âˆš
2Ï€
e
2âˆ’2tâˆš
t
.
We also need to bound the Beta function at certain non-integer values. In particular, we
observe that,
B(t+ 1, t+ 1) â‰¤ B(t+ 1/2, t+ 1/2) â‰¤ B(t, t),
so that we can similarly sandwich the Beta function at these non-integer values as:
2Ï€
4e
2âˆ’2tâˆš
t
. â‰¤ B(t+ 1/2, t+ 1/2) â‰¤ e
2
âˆš
Ï€
2âˆ’2tâˆš
t
.
With these bounds in place we can now upper and lower bound the T -functional in (75). We
can upper bound this expression by considering the cases when t is odd and t is even separately,
and taking the worse of these two bounds to obtain:
T (p0) â‰¤
e2
2
tâˆ’1/4.
Similarly, using the above results we can lower bound the T -functional as:
T (p0) â‰¥
Ï€
2e2
tâˆ’1/4,
and this yields the claim.
Proof of Claim (72): We are interested in the truncated T -functional. The set BÏƒ of
probability content 1âˆ’ Ïƒ, takes the form [âˆ’Î±, Î±], where
Î± = Î³ tan
(Ï€
2
(1âˆ’ Ïƒ)
)
= Î³ cot
(Ï€Ïƒ
2
)
.
42
Using the inequality that cot(x) â‰¤ 1x , we can upper bound Î± as:
Î± â‰¤ 2Î³
Ï€Ïƒ
.
Similarly, we can (numerically) lower bound Î± by noting that for 0 â‰¤ Ïƒ â‰¤ 0.5 we have that,
Î± â‰¥ Î³
4Ïƒ
.
With these bounds in place, we can now proceed to upper and lower bound the truncated T
functional. Concretely,
TÏƒ(p0) â‰¤
Î³
âˆš
Ï€Î³
âˆ« 2Î³
Ï€Ïƒ
âˆ’ 2Î³
Ï€Ïƒ
1âˆš
x2 + Î³2
dx â‰¤ 2Î³âˆš
Ï€Î³
[âˆ« Î³
0
1
Î³
dx+
âˆ« 2Î³
Ï€Ïƒ
Î³
1
x
dx
]
â‰¤ 2Î³âˆš
Ï€Î³
[
1 + ln
(
2
Ï€Ïƒ
)]
= 2
âˆš
Î³
Ï€
[
ln
(
2e
Ï€Ïƒ
)]
.
In a similar fashion, we can lower bound the functional as:
TÏƒ(p0) â‰¥ 2
âˆš
Î³
Ï€
[
ln
(
1
Ïƒ
)]
.
Taken together these bounds give the desired claim.
Proof of Claim (74): We treat x0 as a fixed constant. The CDF for the Pareto family of
distributions takes the simple form:
F (x) = 1âˆ’
(x0
x
)Î±
, for x â‰¥ x0,
we obtain that the set BÏƒ takes the form [x0, x0Ïƒ
âˆ’1/Î±]. So that the truncated functional is
simply:
TÏƒ(p0) =
âˆ« x0Ïƒâˆ’1/Î±
x0
âˆš
p0(x;x0, Î±)dx
=
2
âˆš
Î±x0
1âˆ’ Î±
(
Ïƒâˆ’
1âˆ’Î±
2Î± âˆ’ 1
)
,
which yields the desired claim.
D Properties of the T -functional
The rate for Lipschitz testing is largely dependent on the truncated T -functional of the null
hypothesis. In this section we establish several properties of the T -functional, and its stability
with respect to perturbations. There are two notions of stability of the truncated T -functional
that are of interest: its stability with respect to perturbation of the truncation parameter,
and its stability with respect to perturbations of the density p0. In particular, the truncation
stability determines the discrepancy between the upper and lower bounds in Theorem 5.
43
Our interest is in the difference between TÏƒ1(p0) and TÏƒ2(p0) (where without loss of
generality we take Ïƒ1 â‰¤ Ïƒ2). We show that if the support of the density is stable with respect
to the truncation parameter then so is the T -functional. Intuitively, the discrepancy can be
large only if the density has a long Ïƒ1-tail but a relatively small Ïƒ2-tail. Returning to the
definition of the T -functional in (27), we let BÏƒ1 and BÏƒ2 denote the sets that achieve the
infimum for TÏƒ1 and TÏƒ2 respectively. These are not typically well-defined for two reasons: the
set may not be unique and the infimum might not be attained. The second problem can be
easily dealt with by introducing a small amount of slack. To deal with the non-uniqueness we
simply choose the sets that have maximal overlap in Lebesgue measure, i.e. we define BÏƒ1 and
BÏƒ2 to be two sets that have maximal Lebesgue overlap such that,(âˆ«
BÏƒ1
âˆš
p0(x)dx
)2
â‰¥ TÏƒ1(p0)âˆ’ Î¾,(âˆ«
BÏƒ2
âˆš
p0(x)dx
)2
â‰¥ TÏƒ2(p0)âˆ’ Î¾,
for an arbitrary small Î¾ > 0. The quantity Î¾ may be taken as small as we like and has no effect
when chosen small enough so we ignore it in what follows. We define, S = BÏƒ1\BÏƒ2 which
measures the stability of the support with respect to changes in the truncation parameter, i.e.
if the Lebesgue measure Âµ(S) is small then the support is stable. With these definitions in
place we have the following lemma:
Lemma 11. For any two truncation levels Ïƒ1 â‰¤ Ïƒ2, we have that,
T Î³Ïƒ1(p0)âˆ’ T
Î³
Ïƒ2(p0) â‰¤ (Ïƒ1 âˆ’ Ïƒ2)
Î³Âµ(S)1âˆ’Î³ .
Remarks:
â€¢ Since Î³ < 1, this result asserts that if the support of the density is stable with respect to
the truncation parameter then so is the truncated T -functional. This is the case in all
the examples we considered in Section 4.1.
â€¢ If we restrict attention to compactly supported densities then we can upper bound Âµ(S)
by the Lebesgue measure of the support indicating that in these cases the truncated
T -functional is somewhat stable.
â€¢ On the other hand this result also gives insight into when the truncated functional is
not stable. In particular, it is straightforward to construct examples of densities p0
which have a very long Ïƒ1-tail but a light Ïƒ2-tail, in which case this discrepancy can be
arbitrarily large. Noting however that in our bounds the regime of interest is when the
truncation parameter is not fixed, i.e. when Ïƒ â†’ 0, in which case this discrepancy can
be large only for carefully constructed pathological densities.
Proof. The result follows using HoÌˆlderâ€™s inequality:
T Î³Ïƒ1(p0)âˆ’ T
Î³
Ïƒ2(p0) =
âˆ«
S
pÎ³0(x)dx
= Âµ(S)
âˆ«
S
pÎ³0(x)
Âµ(S)
dx
â‰¤ Âµ(S)
(âˆ«
S
p0(x)
Âµ(S)
dx
)Î³
= Âµ(S)1âˆ’Î³(Ïƒ1 âˆ’ Ïƒ2)Î³ .
44
In order to understand the stability of the T -functional with respect to perturbations of
p0 it is natural to consider a form of the modulus of continuity. We restrict our attention
to densities p0 which have support contained in a fixed set S, and denote these densities by
L(Ln, S), and only consider the case when d = 1 and hence Î³ = 1/2.
Focussing on the case when the truncation parameter is fixed (say to 0) we define:
s(p0, Ï„, S) = sup
p,p0âˆˆL(Ln,S),â€–pâˆ’p0â€–1â‰¤Ï„
|T Î³0 (p)âˆ’ T
Î³
0 (p0)|.
With these definitions in place, we have the following result:
Lemma 12. For any p0, the modulus of continuity of the T -functional is upper bounded as:
s(p0, Ï„, S) â‰¤
âˆš
Ï„Âµ(S).
Remark:
â€¢ This result guarantees that for densities that are close in `1, their corresponding T -
functionals are close, provided that we restrict attention to compactly supported densities.
â€¢ On the other hand, an inspection of the proof below reveals that if we eliminate the
restriction of compact support, then for any density p0, we can construct a density p
that is close in `1 but has an arbitrarily large discrepancy in the T -functional, i.e. the
T -functional can be highly unstable to perturbations of p0 if we allow densities with
arbitrary support.
Proof. Notice that,
T Î³0 (p)âˆ’ T
Î³
0 (p0) =
âˆ«
S
(
âˆš
p(x)âˆ’
âˆš
p0(x))dx
= Âµ(S)
âˆ«
S
(
âˆš
p(x)âˆ’
âˆš
p0(x))
1
Âµ(S)
dx
â‰¤ Âµ(S)
âˆšâˆ«
S
(
âˆš
p(x)âˆ’
âˆš
p0(x))2
1
Âµ(S)
dx
(i)
â‰¤
âˆš
Âµ(S)â€–pâˆ’ p0â€–1 â‰¤
âˆš
Ï„Âµ(S),
where (i) uses the fact that the Hellinger distance is upper bounded by the `1 distance.
D.1 Proof of Claim (31)
This claim is a straightforward consequence of HoÌˆlderâ€™s inequality. We have that,
TÏƒ(p0) = inf
BâˆˆBÏƒ
(âˆ«
B
pÎ³0(x)dx
)1/Î³
.
We restrict our attention to densities with support contained in a fixed set S. We let BÏƒ
denote an arbitrary set in BÏƒ that minimizes the above integral (dealing with non-uniqueness
45
as before). Then,
T Î³Ïƒ (p0) = Âµ(BÏƒ)
âˆ«
BÏƒ
pÎ³0(x)
Âµ(BÏƒ)
dx
(i)
â‰¤ Âµ(BÏƒ)
(âˆ«
BÏƒ
p0(x)
Âµ(BÏƒ)
dx
)Î³
= Âµ(BÏƒ)
1âˆ’Î³(1âˆ’ Ïƒ)Î³ â‰¤ Âµ(S)1âˆ’Î³(1âˆ’ Ïƒ)Î³
where (i) uses HoÌˆlderâ€™s inequality. This yields the claim. For the uniform distribution u on
the set S we have that for any set BÏƒ of mass 1âˆ’ Ïƒ,
T Î³Ïƒ (u) =
âˆ«
BÏƒ
1
ÂµÎ³(S)
dx
= Âµ(S)1âˆ’Î³(1âˆ’ Ïƒ),
which matches the result of (31) up to constant factors involving Ïƒ and Î³. In particular, our
interest is in the regime when Ïƒ â†’ 0, and Î³ is a constant, in which case the two quantities are
equal.
E Technical Results for Lipschitz Testing
In this section we provide the remaining technical proofs related the Theorem 5. We begin
with the preliminary Lemmas 3 and 4.
E.1 Preliminaries
E.1.1 Proof of Lemma 3
Let A be all sets A such that Pn0 (A) â‰¤ Î±. Now
Î¶n(P) â‰¥ inf
Ï†
Q(Ï† = 0) â‰¥ 1âˆ’ Î±âˆ’ sup
AâˆˆA
|Q(A)âˆ’ Pn0 (A)|
â‰¥ 1âˆ’ Î±âˆ’ sup
A
|Q(A)âˆ’ Pn0 (A)|
= 1âˆ’ Î±âˆ’ 1
2
â€–Qâˆ’ Pn0 â€–1.
Note that
â€–Qâˆ’ Pn0 â€–1 = E0|Wn(Z1, . . . , Zn)âˆ’ 1| â‰¤
âˆš
E0[W 2n(Z1, . . . , Zn)]âˆ’ 1.
The result then follows from (45).
E.1.2 Proof of Lemma 4
We divide the proof into several claims.
Claim 1: Each pÎ· is a density function. Note thatâˆ«
pÎ·(x)dx = 1.
46
Now we show it is non-negative. Let x âˆˆ Aj . Then
pÎ·(x) = p0(x) + ÏjÎ·jÏˆj(x) â‰¥ p0(x)âˆ’ ÏjÏˆj(x)
â‰¥ p0(x)âˆ’
Ïj
c
d/2
j h
d/2
j
â€–Ïˆâ€–âˆž.
Now, we observe that for each piece of our partition we have that,
p0(x) â‰¥
p0(xj)
2
â‰¥ LnÎ±Î²
âˆš
dhj
2
â‰¥ Ln
âˆš
dhj ,
where we use the fact that Î±Î² â‰¥ 2. We then obtain that it suffices to choose,
Ïj â‰¤
Lnc
d/2
j
â€–Ïˆâ€–âˆž
h
d/2+1
j ,
which is ensured by the condition in Equation (47).
Claim 2: Each pÎ· âˆˆ L(Ln). Let x, y be two points, and that x âˆˆ Aj , y âˆˆ Ak. We consider
two cases: when neither of j, k are âˆž, and when at least one of them is. Noting that we do
not perturb Aâˆž the second case follows from a similar argument to that of the first case. In
the first case, we have that:
|pÎ·(y)âˆ’ pÎ·(x)| â‰¤ |p0(x)âˆ’ p0(y)|+
âˆ£âˆ£âˆ£âˆ£âˆ£ ÏkÎ·kcd/2k hd/2k Ïˆ
(
y âˆ’ xk
ckhk
)
âˆ’ ÏjÎ·j
c
d/2
j h
d/2
j
Ïˆ
(
xâˆ’ xj
cjhj
)âˆ£âˆ£âˆ£âˆ£âˆ£
â‰¤ cintLnâ€–xâˆ’ yâ€–+
âˆ£âˆ£âˆ£âˆ£âˆ£ ÏkÎ·kcd/2k hd/2k Ïˆ
(
y âˆ’ xk
ckhk
)
âˆ’ ÏkÎ·k
c
d/2
k h
d/2
k
Ïˆ
(
xâˆ’ xk
ckhk
)âˆ£âˆ£âˆ£âˆ£âˆ£
+
âˆ£âˆ£âˆ£âˆ£âˆ£ ÏjÎ·jcd/2j hd/2j Ïˆ
(
y âˆ’ xj
cjhj
)
âˆ’ ÏjÎ·j
c
d/2
j h
d/2
j
Ïˆ
(
xâˆ’ xj
cjhj
)âˆ£âˆ£âˆ£âˆ£âˆ£
â‰¤ cintLnâ€–xâˆ’ yâ€–+
Ïkâ€–Ïˆâ€²â€–âˆžâ€–xâˆ’ yâ€–
c
d/2+1
k h
d/2+1
k
+
Ïjâ€–Ïˆâ€²â€–âˆžâ€–xâˆ’ yâ€–
c
d/2+1
j h
d/2+1
j
,
so that it suffices to ensure that for i âˆˆ {1, . . . , N},
Ïi â‰¤
(1âˆ’ cint)Lncd/2+1i h
d/2+1
i
2â€–Ïˆâ€²â€–âˆž
,
which is ensured by the condition in (47).
Claim 3.
âˆ«
|p0 âˆ’ pÎ·| â‰¥ . We haveâˆ«
|p0 âˆ’ pÎ·| =
âˆ‘
j
âˆ«
Aj
|p0 âˆ’ pÎ·| =
âˆ‘
j
âˆ«
Aj
|ÏjÎ·jÏˆj |
=
âˆ‘
j
Ïj
âˆ«
Aj
|Ïˆj | =
âˆ‘
j
Ïj
âˆ«
Aj
1
c
d/2
j h
d/2
j
âˆ£âˆ£âˆ£âˆ£Ïˆ(xâˆ’ xjcjhj
)âˆ£âˆ£âˆ£âˆ£
=
âˆ‘
j
Ïjc
d/2
j h
d/2
j
âˆ«
[âˆ’1/2,1/2]d
|Ïˆ| = c1
âˆ‘
j
Ïjc
d/2
j h
d/2
j â‰¥ ,
47
where we use the condition in (48). Taken together claims 1, 2 and 3 show that pÎ· âˆˆ L(Ln)
and that â€–pÎ· âˆ’ p0â€–1 â‰¥ n.
Claim 4: Likelihood ratio bound. For observations {Z1, . . . , Zn} the likelihood ratio is
given as
Wn(Z1, . . . , Zn) =
1
2N
âˆ‘
Î·âˆˆ{âˆ’1,1}N
âˆ
i
pÎ·(Zi)
p0(Zi)
and
W 2n(Z1, . . . , Zn) =
1
22N
âˆ‘
Î·âˆˆ{âˆ’1,1}N
âˆ‘
Î½âˆˆ{âˆ’1,1}N
âˆ
i
pÎ·(Zi)pÎ½(Zi)
p0(Zi)p0(Zi)
=
1
22N
âˆ‘
Î·âˆˆ{âˆ’1,1}N
âˆ‘
Î½âˆˆ{âˆ’1,1}N
âˆ
i
(
1 +
âˆ‘N
j=1 ÏjÎ·jÏˆj(Zi)
p0(Zi)
)(
1 +
âˆ‘N
j=1 ÏjÎ½jÏˆj(Zi)
p0(Zi)
)
.
Taking the expected value over Z1, . . . , Zn, and using the fact that the Ïˆjs have disjoint
support we obtain
E0[W
2
n(Z1, . . . , Zn)] =
1
22N
âˆ‘
Î·âˆˆ{âˆ’1,1}N
âˆ‘
Î½âˆˆ{âˆ’1,1}N
ï£«ï£­1 + Nâˆ‘
j=1
Ï2jÎ·jÎ½jaj
ï£¶ï£¸n
â‰¤ 1
22N
âˆ‘
Î·âˆˆ{âˆ’1,1}N
âˆ‘
Î½âˆˆ{âˆ’1,1}N
exp
ï£«ï£­nâˆ‘
j
Ï2jÎ·jÎ½jaj
ï£¶ï£¸
where
aj =
âˆ«
Aj
Ïˆ2j (z)
p0(z)
dz =
1
p0(zj)
âˆ«
Aj
Ïˆ2j (z)
p0(zj)
p0(z)
dz
â‰¤ 2
p0(zj)
.
Thus E0[W
2
n(Z1, . . . , Zn)] â‰¤ EÎ·,Î½enã€ˆÎ·,Î½ã€‰ where we use the weighted inner product defined as:
ã€ˆÎ·, Î½ã€‰ :=
âˆ‘
j
Ï2jÎ·jÎ½jaj .
Hence,
E0[W
2
n(Z1, . . . , Zn)] â‰¤ EÎ·,Î½enã€ˆÎ·,Î½ã€‰ =
âˆ
j
EenÎ·jÎ½j
=
âˆ
j
cosh(nÏ2jaj) â‰¤
âˆ
j
(1 + n2Ï4ja
2
j ) â‰¤
âˆ
j
exp(n2Ï4ja
2
j )
= exp
ï£±ï£²ï£³âˆ‘
j
n2Ï4ja
2
j
ï£¼ï£½ï£¾ â‰¤ exp
ï£±ï£²ï£³4n2âˆ‘
j
Ï4j
p20(xj)
ï£¼ï£½ï£¾ â‰¤ C0,
where the final inequality uses the condition in (49). From Lemma 3, it follows that the Type
II error of any test is at least Î´.
48
E.2 Further technical preliminaries
Our analysis of the pruning in Algorithm 2 uses various results that we provide in this section.
Lemma 13. Let P be a distribution with density p and let Î³ âˆˆ [0, 1). Let
A = {x : p(x) â‰¥ t}
for some t. Define Î¸ = P (A). Finally, let B = {B : P (B) â‰¥ Î¸}. Then, for every B âˆˆ B,âˆ«
A
pÎ³(x)dx â‰¤
âˆ«
B
pÎ³(x)dx.
Proof. Let
S1 = A
â‹‚
Bc, S2 = A
c
â‹‚
B.
Then âˆ«
A
pÎ³(x)dxâˆ’
âˆ«
B
pÎ³(x)dx =
âˆ«
S1
pÎ³(x)dxâˆ’
âˆ«
S2
pÎ³(x)dx.
So it suffices to show that
âˆ«
S1
pÎ³(x)dx â‰¤
âˆ«
S2
pÎ³(x)dx. Note that:
1. S1 and S2 are disjoint,
2. infyâˆˆS1 p(y) â‰¥ supyâˆˆS2 p(y) and
3.
âˆ«
S1
p(x)dx â‰¤
âˆ«
S2
p(x)dx.
where the last fact follows since
âˆ«
A p(x)dx â‰¤
âˆ«
B p(x)dx. Thus, letting g(x) = 1/p
1âˆ’Î³(x), we
have that
g(x) â‰¤ g(y)
for all x âˆˆ S1 and y âˆˆ S2. Soâˆ«
S1
pÎ³(x)dx =
âˆ«
S1
p(x)g(x)dx â‰¤ sup
xâˆˆS1
g(x)
âˆ«
xâˆˆS1
p(x)dx
â‰¤ sup
xâˆˆS1
g(x)
âˆ«
xâˆˆS2
p(x)dx â‰¤ inf
xâˆˆS2
g(x)
âˆ«
xâˆˆS2
p(x)dx
â‰¤
âˆ«
S2
p(x)g(x)dx =
âˆ«
S2
pÎ³(x)dx.
The following lemma concerns the optimal truncation of a piecewise constant function. Sup-
pose we have a piecewise constant positive function f , which is constant on the partition
{A1, . . . , AN}. Without loss of generality suppose that A1, . . . , AN are arranged in decreasing
order of the value of f on the cell Ai. The lemma follows from lemma 13.
Lemma 14. With the notation introduced above suppose that we construct a set A =
â‹ƒt
i=1Ai
and let Î¸ =
âˆ«
A f(x) then we have that for any Î³ â‰¤ 1âˆ«
A
fÎ³(x)dx â‰¤ inf
B,
âˆ«
B f(x)â‰¥Î¸
âˆ«
B
fÎ³(x)dx.
49
The following result is the discrete analogue of the one above. Suppose that we have a sequence
{p1, . . . , pd} of positive numbers sorted as p1 â‰¥ p2 â‰¥ . . . pd. By replacing Lebesgue measure in
Lemma 13 by the counting measure we get:
Lemma 15. Suppose we construct a set of indices A = {1, . . . , t} and let Î¸ =
âˆ‘t
i=1 pi, then
we have that,
tâˆ‘
i=1
p
2/3
i â‰¤ minJ
âˆ‘
jâˆˆJ ,
âˆ‘
kâˆˆJ pkâ‰¥Î¸
p
2/3
j .
E.3 Proof of Lemma 2
We divide the proof into two steps: the first step analyzes the output of Algorithm 1, and the
second step analyzes the pruning of Algorithm 2.
E.3.1 Analysis of Algorithm 1
We analyze Algorithm 1, with the paramters: Î¸1 = 1/(2Ln) and a, b = n/1024. We allow
Î¸2 > 0 to be arbitrary.
Before turning our attention to the main properties, we verify that the partition created by
Algorithm 1 is indeed finite. It is immediate to check that the partition Pâ€  = {A1, . . . , ANÌƒ , Aâˆž}
has the property that P (Aâˆž) â‰¤ a + b, which yields the upper bound of property (41). We
claim that no cell Ai has very small diameter. Recall that Algorithm 1 is run on Sa a set of
probability content 1âˆ’ a (centered around the mean of p0). Define,
pmin =
b
vol(Sa)
,
Suppose that,
diam(Ai) <
1
4
min
{
Î¸1pmin, Î¸2p
Î³
min
}
, (76)
then let us denote the parent cell of Ai by Ui and its centroid by yi. The parent cell Ui,
satisfies the condition that:
diam(Ui) <
1
2
min
{
Î¸1pmin, Î¸2p
Î³
min
}
.
Since this cell was split, we must have that neither stopping rule (35) or (36) was satisfied.
We claim that if the second stopping rule was not satisfied it must be the case that,
p0(yi) â‰¤
pmin
2
.
Indeed, if the second rule is not satisfied we obtain that:
min {Î¸1p0(yi), Î¸2pÎ³0(yi)} â‰¤
1
2
min
{
Î¸1pmin, Î¸2p
Î³
min
}
,
which via some simple case analysis of the minâ€™s, together with the fact that Î³ < 1 yields the
desired claim. Now using the Lipschitz property and the fact that Î¸1 = 1/(2Ln), we have that:
sup
xâˆˆUi
p0(x) â‰¤ p0(yi) + Lndiam(Ui) <
pmin
2
+
pmin
4
< pmin.
50
This means that the first stopping rule was in fact satisfied and we could not have split Ui.
This in turn means that every cell in our partition (excluding Aâˆž) has diameter at least:
diam(Ai) >
1
4
min
{
Î¸1pmin, Î¸2p
Î³
min
}
.
This yields that our produced partition is finite and in turn that algorithm terminates in a
finite number of steps.
Proof of Claim 39: Our final task is to show that the partition satisfies the condition that,
1
4
min {Î¸1p0(xi), Î¸2pÎ³0(xi)} â‰¤ diam(Ai) â‰¤ min {Î¸1p0(xi), Î¸2p
Î³
0(xi)} .
The upper bound is straightforward since it is enforced by our stopping rule. To observe that
the lower bound is always satisfied we note that if
diam(Ai) <
1
4
min {Î¸1p0(xi), Î¸2pÎ³0(xi)} ,
then denoting the parent cell of Ai to be Ui (with centroid yi) we obtain that,
diam(Ui) <
1
2
min {Î¸1p0(xi), Î¸2pÎ³0(xi)} .
Using this we obtain that,
p0(yi) â‰¥ p0(xi)âˆ’ Lndiam(Ui) â‰¥
3
4
p0(xi).
This yields that,
diam(Ui) <
1
2(3/4)Î³
min {Î¸1p0(yi), Î¸2pÎ³0(yi)} < min {Î¸1p0(yi), Î¸2p
Î³
0(yi)} ,
where in our final step we use the fact that Î³ < 1. This results in a contradiction since this
means that Ui satisfies our stopping rule and would not have been split.
Proof of Claim (40): This is a straightforward consequence of the previous property. In
particular, we have that diam(Ai) â‰¤ Î¸1p0(xi), with Î¸1 = 1/(2Ln) so that,
sup
xâˆˆAi
p0(x) â‰¤ p0(xi) + Ln
Î¸1p0(xi)
2
â‰¤ 5
4
p0(xi).
Similarly,
inf
xâˆˆAi
p0(x) â‰¤ p0(xi)âˆ’ Ln
Î¸1p0(xi)
2
â‰¤ 3
4
p0(xi),
which yields the desired claim.
E.3.2 Analysis of Algorithm 2
We now turn our attention to studying the properties of the pruned partition P = {A1, . . . , AN , Aâˆž}.
For this algorithm, we choose Î¸2 = n/(8LnÂµ(1)) and take c = n/512.
Proof of Claim (39): The pruning algorithm completely eliminates some cells, adding them
to Aâˆž. In the case when Q(jâˆ—) â‰¤ c/5 we change the diameter of the final cell AN , shrinking
it by a 1âˆ’ Î± factor. By definition Î± â‰¤ 1/5, and this yields Claim (39).
51
Proof of Claim (40): Since the pruning step either eliminates cells, adding them to Aâˆž, or
reduces their diameter this claim follows directly from the fact that this property holds for Pâ€ .
Proof of Claim (41): The pruning eliminates cells of total additional mass at most c so we
obtain that, P (Aâˆž) â‰¤ a + b + c â‰¤ n/256 verifying the upper bound in (41). To verify the
lower bound, we claim that the difference in the probability mass of the unpruned partition,
{A1, . . . , ANÌƒ} and the pruned partition {A1, . . . , AN} is at least c/5, i.e.
P0
( NÌƒâ‹ƒ
j=1
Aj
)
âˆ’ P0
( Nâ‹ƒ
j=1
Aj
)
â‰¥ c/5.
In the case when Q(jâˆ—) â‰¥ c/5 the claim is direct. When this is not the case then the cell AN
was too large, so that Q(jâˆ—) + P0(AN ) â‰¥ c, which implies that, P0(AN ) â‰¥ 4c/5. Let xN be
the center of AN . Using property (40) and the fact that (1âˆ’ Î±))d â‰¤ (1âˆ’ Î±) verify that,
P0(D1) â‰¤ 4(1âˆ’ Î±)P0(AN ).
Using the definition of Î± we obtain that P0(D1) â‰¥ c/5 as desired.
Proof of Claim (42): We claim that the partition satisfies the property that,
Ln
Nâˆ‘
i=1
diam(Ai)vol(Ai) â‰¤
n
4
. (77)
Taking this claim as given we verify the property (42). We divide the proof into two cases:
1. P (Aâˆž) â‰¥ n/4: In this case we obtain that,
Nâˆ‘
i=1
|P0(Ai)âˆ’ P (Ai)|+ |P0(Aâˆž)âˆ’ P (Aâˆž)| â‰¥ |P0(Aâˆž)âˆ’ P (Aâˆž)| â‰¥ n/8,
using the upper bound in property (41).
2. P (Aâˆž) â‰¤ n/4: In this case we observe that,âˆ«
Aâˆž
|p0(x)âˆ’ p(x)|dx â‰¤
âˆ«
Aâˆž
p0(x)dx+
âˆ«
Aâˆž
p(x)dx â‰¤ 3n
8
,
and this yields that, âˆ«
Rd\Aâˆž
|p0(x)âˆ’ p(x)|dx â‰¥ n(1âˆ’ 3/8).
Now denoting by pÌ„ the approximation of p by a density equal to the average of p on each
cell of the partition we have that,âˆ«
Rd\Aâˆž
|p0(x)âˆ’ p(x)|dx â‰¤
âˆ«
Rd\Aâˆž
|p0(x)âˆ’ pÌ„0(x)|dx+
âˆ«
Rd\Aâˆž
|p(x)âˆ’ pÌ„(x)|dx
+
Nâˆ‘
i=1
|p0(Ai)âˆ’ p(Ai)|dx.
52
For any Ln-Lipschitz density we have that,âˆ«
Rd\Aâˆž
|p(x)âˆ’ pÌ„(x)|dx â‰¤ Ln
Nâˆ‘
i=1
diam(Ai)vol(Ai) â‰¤
n
4
,
using claim (77). This yields that,
Nâˆ‘
i=1
|p0(Ai)âˆ’ p(Ai)|+ |p0(Aâˆž)âˆ’ p(Aâˆž)| â‰¥
Nâˆ‘
i=1
|p0(Ai)âˆ’ p(Ai)| â‰¥ n(1âˆ’ 7/8) = n/8,
as desired.
It remains to prove claim (77). Notice that,
Ln
Nâˆ‘
i=1
diam(Ai)vol(Ai) â‰¤
Nâˆ‘
i=1
min {Î¸1p0(xi), Î¸2pÎ³0(xi)} vol(Ai)
(i)
â‰¤ 2
âˆ«
Rd
{Î¸1p0(x), Î¸2pÎ³0(x)} dx
= 2
âˆ«
Rd
{
p0(x)
2Ln
,
np
Î³
0(x)
8LnÂµ(1/4)
}
dx
(ii)
=
n
4
,
where step (i) uses property (40) and (ii) uses the definition of Âµ in (37).
Proof of Claim (43): Recall that we have chosen c = n/512. In order to prove this claim
we need to use properties of the pruning step. Let us define pÌƒ0(x) as the piecewise constant
density formed by replacing p0(x) by its maximum value over the cell containing x, and 0
outside the support of {A1, . . . , ANÌƒ}. We note that,âˆ«
K
pÎ³0(x)dx â‰¤
âˆ«
K
pÌƒÎ³0(x)dx. (78)
Now, abusing notation slightly and ignoring the set Aâˆž we denote the original partition
as {A1, . . . , ANÌƒ}, which we take as sorted by the values in g0, and the pruned partition as
{A1, . . . , AN}, noting that we might potentially have split the last cell AN into two cells. We
let A =
â‹ƒNÌƒ
i=1Ai. Let us denote,
B =
{
B : B âŠ‚ A,
âˆ«
Bc
pÌƒ0(x)dx â‰¤
âˆ«
Kc
pÌƒ0(x)dx
}
.
Using Lemma 14 we obtain that,âˆ«
K
pÌƒÎ³0(x)dx â‰¤ inf
BâˆˆB
âˆ«
B
pÌƒÎ³0(x)dx. (79)
Noting, that âˆ«
Kc
pÌƒ0(x)dx â‰¥
âˆ«
Kc
p0(x)dx â‰¥
c
5
,
and defining,
C =
{
C : C âŠ‚ A,
âˆ«
Cc
pÌƒ0(x)dx â‰¤
c
5
}
.
53
we obtain that,
inf
BâˆˆB
âˆ«
B
pÌƒÎ³0(x)dx â‰¤ inf
CâˆˆC
âˆ«
C
pÌƒÎ³0(x)dx. (80)
Defining,
D =
{
D : D âŠ‚ A,
âˆ«
Dc
p0(x)dx â‰¤
c
10
}
,
we see that
D âŠ‚ C âŠ‚ B
so that
inf
CâˆˆC
âˆ«
C
pÌƒÎ³0(x)dx â‰¤ 2
Î³ inf
DâˆˆD
âˆ«
D
pÎ³0(x)dx â‰¤ 2
Î³T Î³c/10. (81)
Putting together Equations (78), (79), (80) and (81) we obtain the desired result.
Proof of Claim (44): In order to lower bound the density over the pruned partition we
will show that our pruning step is approximately a level set truncation. We have that for any
point x that is removed and any point y that is retained it must be the case that,
p0(x) â‰¤ 2p0(y).
where we used property (40). Let K denote the set of points retained by the pruning. The
above observation yields that, there exists some t â‰¥ 0 such that,
{p0 â‰¥ t} âŠ† K âŠ† {p0 â‰¥ t/2}.
We know that
âˆ«
K p0(x)dx â‰¤ 1âˆ’ c/10. Consider, the set
G(u) = {x : p0(x) â‰¥ u} .
Suppose that for some u we can show that,
P(K) â‰¤ P(G(u)),
then we can conclude that t â‰¥ u, and further that the density on K is at least u/2. It
thus only remains to find a value u such that P(G(u)) â‰¥ 1 âˆ’ c/10. Suppose we choose
u =
(
c
10Âµ(c/(10))
)1/(1âˆ’Î³)
, and recall that,
 =
âˆ«
min
{
p0(x)
c/(10)
,
pÎ³0(x)
Âµ(c/(10))
}
dx.
Over the set Gc the minimizer is always the first term above which yields,
 â‰¥
âˆ«
Gc
p0(x)
c/(10)
dx,
i.e. that P(Gc) â‰¤ c/10, as desired. This in turn yields the claim.
54
E.4 Proof of Lemma 5
To show this, it suffices to show that more mass is truncated from q than is truncated from p,
i.e. that
N+1âˆ‘
s+1
qi â‰¥ P (Aâˆž), (82)
and then we apply Lemma 15. To show (82) we proceed as follows. Note that P (Aâˆž) = qa for
some a. If s â‰¤ a then
âˆ‘N+1
s+1 qi â‰¥ P (Aâˆž) follows immediately. Now suppose that s > a. From
the definition of s we know that qs +
âˆ‘N+1
s+1 qi â‰¥ /128 so that
âˆ‘N+1
s+1 qi â‰¥ /128âˆ’ qs. Since
s > a, qs â‰¤ P (Aâˆž) â‰¤ /256 and so
âˆ‘N+1
s+1 qi â‰¥ /256 â‰¥ P (Aâˆž) so that
âˆ‘N+1
s+1 qi â‰¥ P (Aâˆž) as
required. Thus (82) holds.
F Adapting to Unknown Parameters
In this section, we consider ways to choose the parameter Ïƒ for the max test, and for the
test in [30], and then consider tests that are adaptive to the typically unknown smoothness
parameter Ln.
F.1 Choice of Ïƒ
The max test and the test from [30] require choosing the truncation parameter Ïƒ = n/8. In
typical settings, we do not assume that n is known. We consider the case of the test from [30]
though our ideas generalize to the max test in a straightforward way.
Perhaps the most natural way to choose the parameter Ïƒ is to solve the critical equation
and choose Ïƒ accordingly, i.e. we find ÏƒÌƒ that satisfies:
ÏƒÌƒ = max
{
1
n
,
âˆš
VÏƒÌƒ/16(p0)
n
}
, (83)
and then we choose the tuning parameter Ïƒ := C max{1/Î±, 1/Î¶}ÏƒÌƒ, for a sufficiently large
constant C > 0.
When the unknown n â‰¥ 8C max{1/Î±, 1/Î¶}ÏƒÌƒ, then it is clear that our choice guarantees
that the tuning parameter Ïƒ is chosen sufficiently small, i.e. Ïƒ â‰¤ n/8 as desired. It is also
clear that the test has size at most Î±. It remains to understand the Type II error. Inverting
the above relationship we see that,
n = max
{
1
ÏƒÌƒ
,
VÏƒÌƒ/16(p0)
ÏƒÌƒ2
}
.
Noting that Ïƒ/2 â‰¥ ÏƒÌƒ/16, and that C max{1/Î±, 1/Î¶} â‰¥ 1 we obtain that,
n â‰¥ C max{1/Î±, 1/Î¶}max
{
1
Ïƒ
,
VÏƒ/2(p0)
Ïƒ2
}
â‰¥ C max{1/Î±, 1/Î¶}max
{
1
Ïƒ
,
VÏƒ/2(p0)
2n
}
.
An application of Lemma 7 shows that the Type II error of the test is at most Î¶ as desired.
Thus, we see that this test provides the same result as the test in Theorem 3 without knowledge
of n.
Although adequate from a theoretical perspective, the previous choice of the parameter
depends on an unknown (albeit universal) constant. An alternative is to consider a range of
55
possible values for the parameter Ïƒ and appropriately adjust the threshold Î± via a Bonferroni
correction. One natural range is to consider scalings of the parameter ÏƒÌƒ in (83). More
generally, if we considered Î£ = {Ïƒ1, . . . , ÏƒK}, a natural goal would be to compare the risk of
the Bonferroni corrected test to the oracle test which minimizes the risk over the set Î£ of
possible tuning parameters. We leave a more detailed analysis of this test to future work.
F.2 Adapting to unknown Ln
Our tests for Lipschitz testing, in addition to assuming knowledge of n use knowledge of the
Lipschitz constant Ln in constructing the binning. The techniques from the previous section
can be used to construct tests without knowledge of n. Constructing tests which are adaptive
to unknown smoothness parameters is a problem which has received much attention in classical
works. We focus only on establishing upper bounds. Some lower bounds follow from standard
arguments and we highlight important open questions in the sequel.
In order to define precisely the notion of an adaptive test, we follow the prescription of
Spokoiny [29] (see also [12, 16]). As in (28) define a sequence of critical radii wn(p0, L) as the
solutions to the critical equations:
wn(p0, L) =
(
Ld/2Tcwn(p0,L)(p0)
n
)2/(4+d)
for a sufficiently small constant c > 0. We now define the adaptive upper critical radii as the
solutions to the critical equations:
wan(p0, L) =
(
Ld/2 log(n)Tcwan(p0,L)(p0)
n
)2/(4+d)
. (84)
We can upper bound the ratio:
wan(p0, L)
wn(p0, L)
â‰¤ (log(n))2/(4+d).
This ratio upper bounds the price for adaptivity. It will be necessary to distinguish the (known)
smoothness parameter of the null from the possibly unknown parameter Ln in (6). We will
denote the smoothness parameter of p0 by L0. We note that in the setting where Ln was
known, we assumed that both p0, p âˆˆ L(Ln) and this in turn requires that Ln â‰¥ L0.
We take Î±, Î¶ > 0 to be fixed constants. For a sufficiently large constant C > 0 we define
the class of densities:
L(Ln, wan) = {p : p âˆˆ L(Ln), â€–pâˆ’ p0â€–1 â‰¥ C max{1/Î±, 1/Î¶}wan(p0, Ln)}.
For some p0 âˆˆ L(L0), consider the hypothesis testing problem of distinguishing:
H0 : p = p0, p0 âˆˆ L(L0) versus H1 : p âˆˆ
â‹ƒ
Lnâ‰¥L0
L(Ln, wan). (85)
In order to precisely define our testing procedure we first show that there are natural upper
bounds on Ln. In particular, we claim that when Ln  n2/dL0 then the critical radius remains
lower bounded by a constant.
We have the following lemma. We let C`, c > 0 denote universal constants.
56
Lemma 16. If Ln â‰¥ C`n2/dL0, then
n(p0, Ln) â‰¥ c.
Thus we restrict our attention to the regime where Ln âˆˆ [L0, Cn2/dL0], for a sufficiently
large constant C > 0. A natural strategy is then to consider a discretization of the set of
possible values for Ln,
L = {L0, 2L0, . . . , 2log2(Cn
2/d)L0}.
Our adaptive test then simply performs the binning test described in Theorem 5 for each
choice of Ln âˆˆ L, with the threshold Î± reduced by a factor of dlog2(Cn2/d) + 1e. We refer to
this test as the adaptive Lipschitz test. We have the following result:
Theorem 6. Consider the testing problem in (85). The adaptive Lipschitz test has Type I
error at most Î±, and has Type II error at most Î¶.
Remarks:
â€¢ Comparing the non-adaptive critical radii in (28) and the adaptive critical radii in (84)
we see that we lose a factor of (log(n))2/(4+d). A natural question is whether such a loss
is necessary.
â€¢ Classical results [16] consider adapting to an unknown HoÌˆlder exponent s and show that
for testing uniformity (with deviations in the `2 metric) a loss of a factor (
âˆš
log log(n))2s/(4s+d)
is necessary and sufficient. In our setting, the loss is of a logarithmic factor instead of a
log log factor and this is a consequence of the fact that the high-dimensional multinomial
tests we build on [30] are not in general exponentially consistent (i.e. their power and
size do not tend to zero at an exponential rate). We hope to develop a more precise
understanding of this situation in future work.
Proof. The proof follows almost directly from our previous analysis of Theorem 5 so we only
provide a brief sketch. It is straightforward to check that the Bonferroni correction controls
the size of the adaptive Lipschitz test at Î±. Let jâˆ— denote the smallest integer such that,
2j
âˆ—
L0 â‰¥ Ln. In order to bound the Type II error, it is sufficient to show that under the
alternate, the test corresponding to the index jâˆ— rejects the null hypothesis with probability
at least 1 âˆ’ Î¶. Noting that the ratio 2jâˆ—L0/Ln â‰¤ 2 this follows directly from the proof of
Theorem 5.
F.2.1 Proof of Lemma 16
In order to establish this claim, it suffices to show that the lower bound on the critical radius
in (28) is at least a constant. By the monotonicity of the critical equation, it suffices to show
that for some small constant c > 0 we have that,
c â‰¤
(
L
d/2
n TCc(p0)
n
)2/(4+d)
,
where C > 0 is the universal constant in (28). We choose c < C/2 so we obtain that it suffices
to show,
c â‰¤
(
L
d/2
n T1/2(p0)
n
)2/(4+d)
.
57
We claim that for any p0 âˆˆ L(L0) there is a universal constant C1 > 0 such that,
T1/2(p0) â‰¥
C1
L
d/2
0
. (86)
Taking this claim as given for now we see that,(
L
d/2
n T1/2(p0)
n
)2/(4+d)
â‰¥
(
C1L
d/2
n
L
d/2
0 n
)2/(4+d)
â‰¥
(
C1
C
d/2
`
)2/(4+d)
â‰¥ c,
as desired.
Proof of Claim (86): As a preliminary we first produce an upper bound on any Lipschitz
density. We claim that, there exists a constant C > 0 depending only on the dimension such
that any L0-Lipschitz density p0 is upper bounded as â€–p0â€–âˆž â‰¤ CLd/(d+1)0 .
Without loss of generality let us suppose the density p0 is maximized at x = 0. The density
p0 is then lower bounded by the function,
g0(x) = (â€–p0â€–âˆž âˆ’ L0â€–xâ€–) I(â€–p0â€–âˆž âˆ’ L0â€–xâ€– â‰¥ 0).
The integral of this function is straightforward to compute, and since p0 must integrate to 1
we obtain that,
1 =
âˆ«
x
p0(x)dx â‰¥
âˆ«
x
g0(x)dx =
vd
d+ 1
â€–pâ€–d+1âˆž
Ld0
,
where vd denotes the volume of the d-dimensional unit ball. This in turn yields the upper
bound,
â€–pâ€–âˆž â‰¤
(
d+ 1
vd
)1/(d+1)
L
d/(d+1)
0 ,
as desired. With this result in place we can lower bound the truncated T -functional. In
particular, letting BÏƒ denote a set of probability content 1âˆ’ Ïƒ that (nearly) minimizes the
truncated T -functional we have that,
T Î³Ïƒ (p0) =
âˆ«
BÏƒ
pÎ³0(x)dx â‰¥
âˆ«
BÏƒ
p0(x)
â€–pâ€–1âˆ’Î³âˆž
dx â‰¥
(
vd
d+ 1
)1/(3+d) 1âˆ’ Ïƒ
L
d/(3+d)
0
,
which gives the bound,
TÏƒ(p0) â‰¥
(
vd
d+ 1
)1/2 (1âˆ’ Ïƒ)(3+d)/2
L
d/2
0
,
as desired. Taking Ïƒ = 1/2 yields the desired claim.
58
â„“1 Distance
0 0.2 0.4 0.6 0.8
P
o
w
er
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Uniform Null, Uniform Alternate
Chi-sq.
L2
L1
Truncated chi-sq
LRT
2/3rd and tail
â„“1 Distance
0 0.2 0.4 0.6 0.8
P
o
w
er
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Uniform Null, Sparse Alternate
Chi-sq.
L2
L1
Truncated chi-sq
LRT
2/3rd and tail
Figure 5: A comparison between the truncated Ï‡2 test, the 2/3rd + tail test [30], the Ï‡2-test,
the likelihood ratio test, the `1 test and the `2 test. The null is chosen to be uniform, and
the alternate is either a dense or sparse perturbation of the null. The power of the tests are
plotted against the `1 distance between the null and alternate. Each point in the graph is an
average over 1000 trials. Despite the high-dimensionality (i.e. n = 200, d = 2000) the tests
have high-power, and perform comparably.
G Additional Simulations
In this section we re-visit the simulations for multinomials. We add comparisons to tests based
on the `1 and `2 statistics which are given as
T`1 =
dâˆ‘
i=1
|Xi âˆ’ np0(i)|,
and
T`2 =
dâˆ‘
i=1
(Xi âˆ’ np0(i))2.
In addition to the alternatives that are created by dense and sparse perturbations of the null
we also consider two other perturbations: one where we perturb each coordinate of the null
by an amount proportional to the entry p0(i), and one where we perturb each coordinate
by p0(i)
2/3, in magnitude with a Rademacher sign. The latter perturbation is close to the
worst-case perturbation considered by [30] in their proof of local minimax lower bounds. We
take n = 200, d = 2000 and each point in the graph is an average over 1000 trials.
Once again we observe that the truncated Ï‡2 test we propose, and the 2/3rd + tail test
from [30] are remarkably robust. All tests are comparable when the null is uniform, while
distinctions are clearer for the power law null. The `2 test appears to have high-power against
sparse alternatives suggesting potential avenues for future investigation.
59
â„“1 Distance
0 0.1 0.2 0.3 0.4 0.5 0.6
P
o
w
er
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Power Law Null, Uniform Alternate
Chi-sq.
L2
L1
Truncated chi-sq
LRT
2/3rd and tail
â„“1 Distance
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
P
o
w
er
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Power Law Null, Sparse Alternate
Chi-sq.
L2
L1
Truncated chi-sq
LRT
2/3rd and tail
â„“1 Distance
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
P
ow
er
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Power Law Null, 2/3-rd Alternate
Chi-sq.
L2
L1
Truncated chi-sq
LRT
2/3rd and tail
â„“1 Distance
0 0.2 0.4 0.6 0.8
P
o
w
er
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Power Law Null, Alternate Prop. to Null
Chi-sq.
L2
L1
Truncated chi-sq
LRT
2/3rd and tail
Figure 6: A comparison between the truncated Ï‡2 test, the 2/3rd + tail test [30], the Ï‡2-test,
the likelihood ratio test, the `1 test and the `2 test. The null is chosen to be a power law
with p0(i) âˆ 1/i. We consider four possible alternates, the first uniformly perturbs the
coordinates, the second is a sparse perturbation only perturbing the first two coordinates, the
third perturbs each co-ordinate proportional to p0(i)
2/3 and the final setting perturbs each
coordinate proportional to p0(i). The power of the tests are plotted against the `1 distance
between the null and alternate. Each point in the graph is an average over 1000 trials.
60

