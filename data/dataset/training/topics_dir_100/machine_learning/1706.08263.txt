Efficient Manifold and Subspace Approximations with
Spherelets
Didong Li and David B Dunson
Data lying in a high-dimensional ambient space are commonly thought to have a much
lower intrinsic dimension. In particular, the data may be concentrated near a lower-dimensional
subspace or manifold. There is an immense literature focused on approximating the unknown
subspace, and in exploiting such approximations in clustering, data compression, and build-
ing of predictive models. Most of the literature relies on approximating subspaces using a
locally linear, and potentially multiscale, dictionary. In this article, we propose a simple and
general alternative, which instead uses pieces of spheres, or spherelets, to locally approxi-
mate the unknown subspace. Building on this idea, we develop a simple and computationally
efficient algorithm for subspace learning and clustering. Results relative to state-of-the-art
competitors show dramatic gains in ability to accurately approximate the subspace with
orders of magnitude fewer components. This leads to substantial gains in data compress-
ibility, few clusters and hence better interpretability, and much lower MSE based on small
to moderate sample sizes. Basic theory on approximation accuracy is presented, and the
methods are applied to multiple examples.
Key Words: Curvature; Dimensionality reduction; Geometric multiresolution analysis;
Manifold learning; Subspace learning.
1 Introduction
Given data set X = {xi}ni=1, we assume xi’s are sampled i.i.d from some probability measure
ρ in Rp. In many applications, ρ is supported on or near a lower dimensional subspace or
manifold M with dimension d p. In the majority of the literature, the lower-dimensional
subspace structure is assumed to be linear; for example, typical principal components anal-
ysis (PCA) and latent factor models assume a linear subspace. To more realistically charac-
terize a broad class of datasets and enable greater dimensionality reduction, a more flexible
assumption is to suppose the subspace is a potentially nonlinear d-dimensional manifold
embedded in the dimension p ambient space. “Manifold learning” is the problem of estimat-
ing a mapping from the p-dimensional ambient space of the observed data to the intrinsic
d-dimensional space.
A rich variety of algorithms are available for manifold learning and nonlinear PCA . The
most popular method is to approximate the manifold or subspace by a set of hyperplanes.
These approaches build on the rich literature on approximating manifolds by a set of tangent
planes; for example multiscale PCA uses such a representation to approximate the support
1
ar
X
iv
:1
70
6.
08
26
3v
1 
 [
st
at
.M
L
] 
 2
6 
Ju
n 
20
17
of the data by piecewise hyperplanes ([4-15]). A key disadvantage of such approaches is
the lack of statistical efficiency and parsimony of the representation, particularly when the
true manifold has large curvature. For example, if the manifold is a circle with very small
radius (very large curvature), a huge number of tangent lines will be needed to fit the circle
well. Such disadvantages also hold when M is not a single manifold but is a more complex
collection of manifolds.
Taking curvature into consideration, a reasonable local approximation for a manifold is
provided by a sphere. With such a locally curved basis, a pseudo distance between any two
sets of points is defined based on how well they can be fit by a space form. By using a
collection of spheres to build up a local approximation to a manifold, or to a collection of
disconnected manifolds, one obtains a generalization of common local linear approximations.
In particular, each sphere converges to a hyperplane in the limit as the radius→∞. However,
when the subspace is not approximately flat, our new sphere-based basis will perform much
better. The improvement will increase with the curvature of the unknown subspace being
approximated.
Our simulation results show significant gains in reduction in the number of components
needed to provide an approximation within a given error tolerance. We compare our results
with Geometric MultiResolution Analysis (GMRA) (Liao and Maggioni), which performs
better than most algorithms, according to [1][2][3]. Our spherelets approach has much lower
mean square error using dramatically fewer components. Substantial practical gains are
shown in multiple examples, including in cases in which the true subspace does not have
a manifold structure, but is more appropriately characterized by multiple separated mani-
folds. The proposed approach automatically accounts for such structure. Classification from
features having a lower-dimensional subspace structure is also addressed as a byproduct of
our algorithm.
The paper is organized as follows. The selection of local basis is discussed in section 2.
In section 3, the optimization problem of one single sphere is solved. In section 4, some
important concepts including spherelet and spherical distance are defined. We present our
manifold approximation algorithm in section 5 and then show numerical experiments in
Section 6. An improved version of the algorithm is shown in 7, followed by some other
numerical experiments.
2 Selection of local basis
Most existing algorithms approximate manifolds or subspaces by fitting data with hyper-
planes locally. A primary motivation for relying on hyperplanes instead of alternatives is
simplicity. However, as we have motivated in Section 1, hyperplanes have disadvantages
in terms of parsimony in approximating manifolds that have moderate to high curvature.
There is hence motivation to consider alternative local approximations to manifolds that are
not much more complex than hyperplanes but have non-zero curvature.
2
One can view local hyperplane approximations to the manifold as arising from a first
order Taylor series approximation. It is hence natural to consider second order Taylor series
expansions, which have a quadratic form. In particular, instead of a hyperplane embedded
in Rp, we have
M = {x ∈ Rp : x>Hx+ f>x+ c = 0},
where H ∈ Sym(p), f ∈ Rp and c ∈ R. These quadratic surfaces provide a generalization
of hyperplanes, which arise in the special case in which H = 0. However, the extra flex-
ibility of the quadratic form comes at the expense of a dramatic increase in the number
of parameters, which is p(p+1)2 + p + 1 = O(p
2) for each local piece relative to O(p) for
hyperplane dictionary functions. This makes the extension from local linear to quadratic
approximations impractical; a primary goal is reducing the number of parameters needed
in accurately approximating the overall manifold and using too many parameters per local
piece runs counter to this goal.
To reduce the complexity, we need to simplify the dictionary functions. With this goal
in mind, we start by taking a geometric perspective and viewing hyperplanes as complete,
simply connected manifolds with zero (sectional) curvature. One can then consider the
generalization to space forms defined as follows:
Definition 1. A complete, simply connected Riemannian manifold of constant sectional
curvature is called a space form. A p dimensional space form with curvature c is denoted by
Mp(c).
There is a famous classification theorem up to isometry.
Theorem 1. Let Mp(c) be a space form, then
Mp(c) ∼=

Sp( 1√
c
) c > 0
Rp c = 0
Hp(c) c < 0
, (1)
where Sp( 1√
c
) is p dimensional sphere with radius 1√
c
and Hp(c) is a p dimensional hyperbolic
space with curvature c.
Hence, hyperplanes are one of three types of space forms, and it is natural to consider
the other two types as alternative choices of dictionary functions. In particular, instead of
just hyperplanes, we could include spheres and/or hyperbolic functions. A p dimensional
sphere centered at x0 with radius r, denoted by S
p(x0, r), can be expressed as
‖x− x0‖2 = 〈x− x0, x− x0〉 = r2,
where 〈·, ·〉 is the usual Euclidean inner product in Rp+1 and the curvature is 1
r2
. We can
rewrite this equation in form (1) as:
x>Ix+ (−2x0)>x+ (x>0 x0 − r2) = 0.
3
Hyperbolic spaces have more diversity, with p+ 1 symmetric axes:
‖x− x0‖2i = 〈x− x0, x− x0〉i = −r2, xi ≥ 0, i = 1, · · · , p+ 1,
where 〈·, ·〉i is the Lorentz inner product defined as
〈x, y〉i = x1y1 + · · ·+ xi−1yi−1 − xiyi + xi+1yi+1 + · · ·+ xn+1yn+1.
Again, we rewrite the above equation in the same form as equation (1):
x>Lix+ (−2Lix0)>x+ (x>0 Lix0 + r2) = 0,
where Li = {(Li)kl = δkl(1−2δki)} is the diagonal matrix with all 1s on the diagonal except
the ith, which is −1.
Although we could potentially include spheres, hyperplanes and hyperbolic spaces simul-
taneously in our dictionary, we only include spheres for the following reasons:
1. Assume the computational complexity of fitting data X ∈ Rn×(p+1) by a sphere is
β(n, p), then the complexity of fitting a hyperbolic space with known symmetric axes is also
β(n, p). Since there are p+ 1 symmetric axes of p dimensional hyperbolic spaces, the total
complexity is (p+ 1)β(n, p).
2. The sphere is compact, while hyperbolic spaces and hyperplanes are noncompact.
Compactness is appealing from a computational stability perspective and in terms of guar-
anteeing locality of each dictionary element.
3. The cell complex structure of the sphere is ideal in this situation. Letting ei be the i
dimensional open ball, the classical cell complex structure of Sp is
Sp = Sp−1 ∪ ep1 ∪ e
p
2.
In other words, a lower dimensional sphere can be viewed as an “equator” of a higher
dimensional sphere. We will make use of this property in our algorithm later.
4. Hyperplanes can be approximated by spheres with large radius.
As a result, we choose spheres as our local basis. Since the manifold is approximated
with multiple local spheres, the basis is called spherelets.
3 Optimization
In this section, we focus on the problem of fitting data using a single sphere; this will
serve as a key building block in developing general spherelet fitting algorithms. Given data
X ∈ Rn×p, where each row xi ∈ Rp, our goal is to estimate a sphere centered at x with
radius r that fits the data as well as possible.
Let x̂i be the projection of xi onto the sphere centered at x with radius r. That is,
x̂i = x+
r
‖xi − x‖
(xi − x).
4
As a typical choice of loss function, we consider the mean square error (MSE):
min
x,r
1
n
n∑
i=1
‖x̂i − xi‖2.
The MSE can be re-expressed as follows:
1
n
n∑
i=1
∥∥∥∥x̂i − xi∥∥∥∥2 = 1n
n∑
i=1
‖x+ r
‖xi − x‖
(xi − x)− xi
∥∥∥∥2
=
1
n
n∑
i=1
∥∥∥∥(xi − x)( r||xi − x|| − 1
)∥∥∥∥2
=
1
n
n∑
i=1
(‖xi − x‖ − r)2
=
1
n
n∑
i=1
(di − r)2
where di = ‖xi−x‖. Note that di is determined by x only, and the loss function is minimized
by r = d̄ =
1
n
n∑
i=1
di when all di’s are known. As a result, we only need to find the optimal
x, and r will be obtained automatically. Then observe that
min
x,r
1
n
n∑
i=1
‖x̂i − xi‖2 = min
x,r
1
n
n∑
i=1
(di − r)2
= min
x
1
n
n∑
i=1
(di − d̄)2
= min
x
var(di) =: f(x)
That is, to minimize the mean square error is equivalent to minimize the variance of {di}.
This is not surprising since if all xi lie on some d dimensional sphere S
d ⊂ Rp, where d is the
intrinsic dimension of these data, then ‖xi−x‖ should be all close to r, that is, the variance
of di = ‖xi − x‖ should be small. However, it is difficult to solve this problem, since x can
not be factored out of di = 〈x− xi, x− xi〉
1
2 . As a result, we choose another loss function g
which is easier than f to handle:
g(x) : = var(d2i )
=
1
n
n∑
i=1
(d2i − d̄2)2
=
1
n
n∑
i=1
(‖xi − x‖2 −
1
n
n∑
j=1
‖xj − x‖2)2.
5
Lemma 1. The optimization problem of
min
x
g(x) = min
x
1
n
n∑
i=1
(
‖xi − x‖2 −
1
n
n∑
j=1
‖xj − x‖2
)2
is equivalent to the following quadratic optimization problem:
min
x
x>Hx+ f>x,
where H =
n∑
i=1
(x̄− xi)(x̄− xi)>, x̄ =
1
n
n∑
i=1
xi, f =
n∑
i=1
(li − l̄)(x̄− xi), li = ‖xi‖2,
l̄ =
1
n
n∑
i=1
li.
Proof. We begin with the building block of g,
||x− xi||2 = (x− xi)>(x− xi)
= x>x− 2x>xi + x>i xi,
and
n∑
i=1
||x− xi||2 =
n∑
i=1
(x>x− 2x>xi + x>i xi)
= nx>x− 2nx>x̄+
n∑
i=1
li.
Then,
||x− xi||2 −
∑n
i=1 ||x− xi||2
n
= x>x− 2x>xi + x>i xi −
1
n
(nx>x− 2nx>x̄+
n∑
i=1
li)
= −2x>xi + li + 2x>x̄− l̄.
Then
(||x− xi||2 −
∑n
i=1 ||x− xi||2
n
)2 = (−2x>xi + li + 2x>x̄− l̄)2
= (2x>(x̄− xi) + (li − l̄))2
= 4x>(x̄− xi)x>(x̄− xi) + 4(li − l̄)x>(x̄− xi) + (li − l̄)2
= 4x>(x̄− xi)(x̄− xi)>x+ 4(li − l̄)(x̄− xi)>x+ (li − l̄)2
6
Since li and l do not depend on x, it’s equivalent to minimize
x>
( n∑
i=1
(x̄− xi)(x̄− xi)>
)
x+
( n∑
i=1
(li − l̄)(x̄− xi)>
)
x
= x>Hx+ f>x,
where H =
n∑
i=1
(x̄− xi)(x̄− xi)> and f =
n∑
i=1
(li − l̄)(x̄− xi).
Note 1. In fact H is nothing but the sample covariance of X. It is not surprising that H
comes in since we want to minimize the variance of d2i .
This quadratic optimization problem is easy to solve:
Proposition 1. arg min
x
g(x) = −1
2
H−1f when H is invertible.
Proof. Trivial.
Note 2. The solution is unique if and only if H is invertible. When n < p and H is singular,
we can still solve the problem by applying pseudo-inverse of H, also denoted by H−1.
4 Spherelet and divergence
In this section, we define several core terms in our algorithm. First, we define the spherelet
s(X) corresponding to a set of points X ∈ Rn×p:
Definition 2. Let X ∈ Rn×p with rows xi ∈ Rp, then the sphere centered at c with radius r
is called the spherelet of X, denoted by s(X):
s(X) := S(c, r),
where c = −12H
−1f , r =
1
n
n∑
i=1
‖xi − c‖.
In some sense, the spherelet s(X) is the sphere providing the best fit to data set X.
Definition 3. Let X ∈ Rn×p with rows xi ∈ Rp and assume s(X) = S(c, r) is the spherelet
of X, then
(X) =
1
n
n∑
i=1
(‖xi − c‖ − r)2,
is called the spherical error of X.
7
Note 3. The smaller the spherical error (X), the more spherical X is. The following
proposition explains the intuition of the spherical error:
Proposition 2. All points xi lie on a sphere if and only if (X) = 0.
Proof. =⇒ Assume all xi lie on the sphere centered at c′ with radius r′. then ‖xi− c′‖ = r′,
∀i = 1, · · · , n, so 1
n
n∑
i=1
(‖xi − c′‖ − r′)2 = 0. Assume s(X) is centered at r with radius r,
then by the definition of spherelet,
(X) =
1
n
n∑
i=1
(‖xi − c‖ − r)2 ≤
1
n
n∑
i=1
(‖xi − c′‖ − r′)2 = 0.
⇐= Assume 0 = (X) = 1
n
n∑
i=1
(‖xi − c‖ − r)2, then ‖xi− c‖− r = 0, ∀i = 1, · · · , n, so all xi
lie on the sphere centered at c with radius r.
Then we define a divergence measuring the dissimilarity of two data sets X ∈ Rn×p and
Y ∈ Rm×p in terms of the distance between their corresponding spheres. If the datasets lie
on the same sphere, their divergence is zero.
Definition 4. Let Fpd be the collection of all sets of samples drawn by some probability
measure whose support is some d dimensional manifold embedded in Rp. Then each element
in Fpd can be written as an n × p matrix, each row represents a sample. The Riemannian
divergence dR : Fpd ×F
p
d → R≥0 is a divergence defined on F
p
d :
dR(X,Y ) = (Z)
where X,Y ∈ Fpd , Z = X ∪ Y ∈ R
(nX+nY )×p = [x1, · · · , xnX , y1, · · · , ynY ]>.
By proposition 2, X,Y lie on the same sphere if and only if (X,Y ) = 0. This Riemannian
divergence is semi-positive definite since dR(X,Y ) = (X ∪ Y ) ≥ 0 for any X,Y . However,
dR is not positive definite. Actually, when dR(X,Y ) = 0, we only know X,Y lie on the same
sphere, but X,Y are not necessarily the same. The spherelet divergence is also symmetric.
In order to measure the similarity between two sets of points, we also need to take the
Euclidean distance into consideration: if two set of points could be fitted by one single
sphere, and their Euclidean distance is also small, it is reasonable to regard them as one
group.
Definition 5. Let dE be the Euclidean distance between two sets of points, that is,
dE(X,Y ) = inf
i,j
‖xi − yj‖,
8
then the following divergence is called spherelet divergence.
dS : Fpd ×F
p
d → R≥ : (X,Y ) 7→ dR(X,Y ) +∞1dE(X,Y )>λ,
where λ > 0 is a tuning parameter, and can be optimized by cross validation. Details on
the choice of λ are in the next section.
Note 4. dR is called Riemannian divergence since it captures the similarity between X and
Y in the sense that dR(X,Y ) is small if X,Y almost belong to one single space form with
constant sectional curvature. dE is called Euclidean distance since it is the Euclidean distance
of two closest points in X and Y respectively. In fact, dE captures the (path) connectivity of
two data sets: if the supporting manifolds belong to one connected component, either their
Euclidean distance is small, or they could be connected by some other data set. We will see
the importance of this property later in our algorithm. As dR reflects geometric properties
and dE reflects topological properties, dS can pick up both types of properties.
5 Manifold approximation algorithm
Assume the data set X ∈ Rn×p ⊂ Fpd is sampled by some probability measure ρ supported
on a d dimensional manifold or subspace M . Our goal is to find the best piecewise spherical
manifold or subspace M̂ and the projection map p : Rp → M̂ such that the mean square
error
n∑
i=1
‖xi − p(xi)‖2 is minimized.
Our algorithm has the following key steps:
Step 1: Preprocess
To make computation and default tuning parameter choice easier, we normalize the data so
that xi ∈ [−1, 1]p.
Step 2: split
We split the data into clusters, with the data in each cluster fit well by a cluster-specific
sphere. Algorithm 1 provides the pseudo code for our initial clustering algorithm.
Step 3: merge
Step 2 can result in many clusters, and performance can be improved by adding a merge
step based on spherelet distance; refer to Algorithm 2.
Step 4: calculating spherelets
Based on Steps 1-3 we have a cluster label for each data point. Step 4 applies Algorithm 3
to calculate the explicit formula of the spherelet for each cluster.
9
Algorithm 1: Assign data to clusters having cluster-specific spherelets.
input : X, , λ
output: label
1 initialize label= 0n;
2 cluster=0 ;
3 while labeling not done do
4 cluster++;
5 flag= 1;
6 find all unlabeled data;
7 while flag==1 and labeling not done do
8 choose the nearest unlabeled point x of current cluster Xcls;
9 if dλ(x,Xcls) > λ then
10 flag=0;
11 end
12 else
13 find the spherelet s(Xcls ∪ x);
14 if spherical error (Xcls ∪ x) <  then
15 assign x to current cluster, label of x = cluster ;
16 end
17 else
18 flag=0;
19 end
20 end
21 end
22 end
10
Algorithm 2: merge clusters
input : X, label, , λ
output: label
1 ncls = current number of clusters;
2 flag= 1;
3 while flag== 1 and ncls> 1 do
4 Dij = dλ(Xi, Xj) is the spherelet distance between cluster i and j;
5 Find the closest two clusters ;
6 if min(D) <  then
7 merge the two corresponding clusters;
8 end
9 else
10 flag = 0;
11 end
12 end
Algorithm 3: calculate spherelets
input : X, label
output: centers, radii
1 ncls = number of clusters;
2 for i = 1:ncls do
3 currX= all points in cluster i;
4 s(currX) = S(c, r)= spherelet of currX;
5 centers[i, :] = c;
6 radii[i] = r;
7 end
11
Step 5: construct p
An estimate M̂ of the supportM is obtained by essentially pasting all the spherelets together.
Given any point x ∈ Rp, we want to find the projection of x onto M̂ . To find which cluster
an arbitrary x belongs to, we calculate
arg min
k
d(x, Sd(ck, rk)) = arg min
k
(‖x− ck‖ − rk)2.
Then the projection map is
x̂ = p(x) = ck +
rk
‖x− ck‖
(x− ck).
Step 6: calculating MSE on test data
Given Xtest = [x
t
1, · · · , xtm]> ∈ Rm×p, then the mean square error is
MSE =
1
m
m∑
i=1
‖xti − p(xti)‖2.
In order to choose the two tuning parameters,  and λ, we recommend using cross
validation (CV). In particular, the data are split into a training and test subsample. For
each possible choice of (, λ) on a grid, the spherelets algorithm in Steps 1-6 is applied, with
the spherelets estimated using the training data, and MSE calculated using the test data.
The (, λ) minimizing test MSE is chosen.
The code for the algorithm can be found here: https://github.com/david-dunson/spherelets
6 Examples
There is a rich literature on toy examples for manifold and subspace learning. We consider
some such examples here.
Olympic rings
We generate 2, 000 data uniformly from the subspace M : Olympic rings. We applied our
spherelets algorithm to these data obtaining tuning parameters of  = 10−5 and λ = 0.1
via cross validation, using 1, 000 samples for training and 1, 000 samples for test. Figure 1
shows the result. The top panel is the test data, the middle panel is the projection of the
test data onto M̂ , and the third panel color codes the clusters. It is easy to see that the
model fit is excellent in that the first two panels are very close. Indeed, the test MSE was
only 1.706 × 10−7. In addition, clustering does an excellent job in perfectly labelling the
different circles in the olympic rings. Of course, this is essentially an ideal case for spherelets
in that the different rings are exact circles and data lie exactly on M . We will first make
the problem more difficult by adding noise.
12
Figure 1: Olympic rings
13
Noised Olympic rings
We add Gaussian noise  ∼ N(0,Σ) where Σ =
[
0.01 0
0 0.01
]
to the Olympic ring data as
shown in the previous example, and re-run our spherelet algorithm exactly as before. In
this case, the optimal tuning parameter values from CV were  = 10−3 and λ = 0.1, and
the MSE in the test set was 9.49× 10−4. Figure 2 shows the results in the same format as
for Figure 1; the top panel shows that there is a moderate level of noise, but the rings are
still visibly evident. The algorithm is able to accurately pick this up and cluster the data
into five color-coded rings; there is some error in estimating the radius, which is as expected
given the level of measurement error.
Noised spiral
The first two examples consist of circles, which are constant curvature. Now we consider a
more complicated manifold: the noised spiral. The curvature is decreasing as the point is
going outward. We generate 1, 000 data uniformly from the subspace M : noised spiral. We
applied our spherelets algorithm to these data obtaining tuning parameters of  = 10−4 and
λ = 0.1 via cross validation, using 500 samples for training and 500 samples for test. Figure
3 shows the result. The first panel is the test data, the middle panel is the projection of the
test data onto M̂ , and the third panel color codes the clusters. We can see that the first
two panels are similar to each other and the test MSE was only 1.4× 10−4. From the third
panel, we can see five clusters with five different colors. The radius of the circle is decreasing
as the point is going outward, which coincides with the change of curvature.
7 Improvement
When p > n, or when p is very large, H tends to be singular and there will be an overfitting
problem. A simple example is when there are only two points, we can find infinitely many
circles so that the error is 0. To avoid this problem, we add another step in our algorithm
but we need more information about the intrinsic dimension. Assume an upper bound of
intrinsic dimension is known, that is, d ≤ K where K ∈ N is known. In the splitting
step (step 2), if the size of some cluster is less than or equal to K, this cluster is not that
informative, so we label all data in this cluster by −1. After merging step (step 3), we assign
each point with label −1 to the nearest cluster under the spherelet distance. In addition,
another merging step follows. The remaining steps are the same - refer to Algorithm 4.
14
Figure 2: Noised Olympic rings
15
Figure 3: Noised spiral
Algorithm 4: split (new version)
input : X, , λ
output: label
1 initialize label= 0n;
2 cluster=0 ;
3 while labeling not done do
4 cluster++;
5 flag= 1;
6 find all unlabeled data;
7 while flag==1 and labeling not done do
8 choose the nearest unlabeled point x of current cluster Xcls;
9 if dλ(x,Xcls) > λ then
10 flag=0;
11 end
12 else
13 find the spherelet s(Xcls ∪ x);
14 if spherical error (Xcls ∪ x) <  then
15 assign x to current cluster, label of x = cluster ;
16 end
17 else
18 flag=0;
19 end
20 end
21 end
22 if number of rows of Xcls < K then
23 label(Xcls) = −1;
24 end
25 end
16
Another change is in the function locquad:
Algorithm 5: locquad
input : X, , λ
output: label, centers, radii, MSE, ncls
1 normalize X;
2 label=split(Xtrain,, λ);
3 label=merge(Xtrain,label,,λ);
4 assign each point with label -1 to the nearest cluster based on dλ;
5 label=merge(Xtrain,label,,λ);
6 find centers and radii;
7 calculate MSE;
All other functions are the same as in section 5.
8 More Examples
In this section, we test the improved algorithm on some more complicated data sets.
Swiss roll
We generate 1, 000 data uniformly from the subspace M : Swiss roll. We applied our
spherelets algorithm to these data for different combinations of tuning parameters λ ∈
{0.05, 0.1, 0.15} and  ∈ {0.01, 0.001, 0.0001, 0.0000} using 500 samples for training and 500
samples for test. For each combination, we store the number of clusters and the correspond-
ing MSE. Figure 4 shows the result. The top panel shows the test set in clusters when the
MSE is the smallest among all MSEs. The second panel shows the relation between MSEs
and the number of clusters. The x-axis is log10(ncls) and the y-axis is log10(MSE). We
compare the curve obtained by our spherelet algorithm and the results obtained by adaptive
GMRA [1]. The code is provided by Mauro Maggioni.
The red line, obtained from the spherelet algorithm, has not only smaller y-intercept,
but also smaller slope, which means our algorithm works better at using the data to obtain
a low error approximation using relatively few components.
Dragon
Now we consider a more complicated dataset: the surface of a dragon with sample size
2,000. As in the previous example, we compare our result with the adaptive GMRA. Figure
5 shows the result, where the red line is still below the other two and has smaller slope.
The above figure shows that the MSE of our spherelet algorithm is much smaller than
that of GMRA.
17
Figure 4: Swiss roll
Figure 5: Dragon
18
Figure 6: Atmosphere boundary
Atmosphere boundary
Above the earth surface, where are two layers in the Troposphere: planetary boundary layer
(L0) and free atmosphere (L1). The boundary between L0 and L1 is changing with time and
location. The concentration of certain pollutants drop off suddenly around this boundary,
which provides an approach to estimate the altitude of this boundary. Given a data set with
the coordinates of the boundary surface[16], we run our algorithm. The sample size is 3648
and the tuning parameters are λ ∈ {0.05, 0.1, 0.15} and  ∈ {0.01, 0.001, 0.0001, 0.0000}.
Figure 6 shows the results. The first panel is the test data, the second one is the projected
data and the last one is the log10(ncls) vs log10(MSE) curve, as before. The MSE could be
as low as 104.5, which implies our algorithm works well for this real dataset.
References
[1] W. Liao and M. Mauro , Adaptive Geometric Multiscale Approximations for Intrinsically
Low-dimensional Data, arXiv:1611.011, 2016.
[2] G. Chen and M. Maggioni. Multiscale geometric and spectral analysis of plane arrange-
ments. In Conference on Computer Vision and Pattern Recognition, 2011.
19
[3] W. Liao, M. Maggioni, S. Vigogna, Learning Adaptive Multiscale Approximations to
Data and Functions near Low-Dimensional Sets, IEEE 2016.
[4] J.Wang, P. Neskovic and L. Cooper, A minimum sphere covering approach to pattern
classification,18th International Conference on Pattern Recognition, 2006.
[5] M. Scholz, F. Kaplan, C. Guy, J. Kopka, J. Selbig, Non-linear PCA: a missing data
approach, In Bioinformatics, Vol. 21, Number 20, pp. 3887–3895, Oxford University Press,
2005.
[6] N. Lawrence, Probabilistic Non-linear Principal Component Analysis with Gaussian Pro-
cess Latent Variable Models, Journal of Machine Learning Research 6(Nov): 1783–1816,
2005.
[7] P. Demartines and J. Hérault, Curvilinear Component Analysis: A Self-Organizing Neu-
ral Network for Nonlinear Mapping of Data Sets, IEEE Transactions on Neural Networks,
Vol. 8(1), 1997, pp. 148–154.
[8] C. Walder and B. Schölkopf, Diffeomorphic Dimensionality Reduction, Advances in Neu-
ral Information Processing Systems 22, 2009, pp. 1713–1720, MIT Press.
[9] B. Schölkopf, A. Smola, K.-R. Müller, Nonlinear Component Analysis as a Kernel Eigen-
value Problem. Neural Computation 10(5):1299-1319, 1998, MIT Press Cambridge, MA,
USA, doi:10.1162/089976698300017467.
[10] J. B. Tenenbaum, V. de Silva, J. C. Langford, A Global Geometric Framework for
Nonlinear Dimensionality Reduction, Science 290, (2000), 2319–2323.
[11] S. T. Roweis and L. K. Saul, Nonlinear Dimensionality Reduction by Locally Linear
Embedding, Science Vol 290, 22 December 2000, 2323–2326.
[12] M. Belkin and P. Niyogi, Laplacian Eigenmaps and Spectral Techniques for Embedding
and Clustering, Advances in Neural Information Processing Systems 14, 2001, p. 586–691,
MIT Press.
[13] S.Lafon, Diffusion Maps and Geometric Harmonics, PhD Thesis, Yale University, May
2004.
[14] D. Donoho and C. Grimes, ”Hessian eigenmaps: Locally linear embedding techniques
for high-dimensional data” Proc Natl Acad Sci U S A. 2003 May 13; 100(10): 5591–5596.
[15] Z. Zhang and J. Wang, ”MLLE: Modified Locally Linear Embedding Using Multiple
Weights” http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382.
[16] ”Shale Oil and Natural Gas Nexus” (SONGNEX) Campaign, https://esrl.noaa.gov/,
2015.
20

