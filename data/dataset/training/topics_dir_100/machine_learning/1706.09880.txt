ar
X
iv
:1
70
6.
09
88
0v
1 
 [
st
at
.M
L
] 
 2
9 
Ju
n 
20
17
A FIXED-POINT OF VIEW ON GRADIENT METHODS FOR BIG DATA
Alexander Jung
Department of Computer Science, Aalto University, Finland; firstname.lastname(at)aalto.fi
ABSTRACT
Using their interpretation as fixed-point iterations, we review
first order gradient methods for minimizing convex objective
functions. Due to their conceptual and algorithmic simplicity,
first order gradient methods are widely used in machine
learning methods involving massive datasets. In particular,
stochastic first order methods are considered the de-facto
standard for training deep neural networks. By studying these
methods within fixed-point theory provides us with powerful
tools to study the convergence properties of a wide range of
gradient methods. In particular, first order methods using
inexact or noisy gradients, such as in stochastic gradient
descent, can be studied using well-known results on inexact
fixed-point iterations. Moreover, as illustrated clearly in this
paper, the fixed-point picture allows an elegant derivation of
accelerations for basic gradient methods. In particular, we
show how gradient descent can be accelerated by an fixed-
point preserving transformation of an operator associated
with the objective function.
Index Terms— convex optimization, fixed point theory,
contraction mapping, gradient descent, heavy balls
I. INTRODUCTION
One of the main recent trends in the algorithmics of
signal processing and data mining for massive datasets is to
leverage the inferential strength of the vast amounts of data
by using relatively simple, but fast, optimization methods [3].
Most of these optimization methods are modifications of the
basic gradient descent (GD) method. Indeed, computation-
ally more heavy approaches such as interior point methods
are often infeasible for the given computational budget and
data volume to be processed [5].
Moreover, the rise of deep learning has brought a sig-
nificant boost for the interest in gradient methods. Indeed,
a major insight within the theory of deep learning is that
for typical high-dimensional models represented by deep
neural networks, most of the local minima of the cost
function (representing, e.g., an empirical loss or training
error) are reasonable close (in terms of objective value)
to the global optimum [6], [9]. These local minima can
be found efficiently by gradient methods such as stochastic
gradient descent (SGD), which might be considered as the
de-facto standard algorithmic primitive for training deep
neural networks [9].
We elaborate the interpretation of some basic gradient
methods such as GD and its variants as fixed-point iterations.
These fixed-point iterations are obtained for operators asso-
ciated with the convex objective function. Using the prism
of fixed-point theory unleashes some powerful tools, e.g.,
on the acceleration of fixed-point iterations [16] or inexact
fixed-point iterations [2], for the analysis and construction
of convex optimization methods. In particular, we show
how the convergence of GD can be understood from the
contraction properties of some operator which is associated
naturally with a differentiable objective function. Moreover,
we work out in some detail how plain GD can be accelerated
by modifying the operator underlying GD in a way that
preserves its fixed-points but decreases the contraction factor
which implies faster convergence by the contraction mapping
theorem.
Outline. We discuss the basic problem of minimizing
convex functions in Section II. In Section III, we introduce
one of the most widely used computational models for
convex optimization methods, i.e., the model of first order
methods. We then derive GD, which is a particular first order
method, as a fixed-point iteration in Section IV. In order to
asses the efficiency of GD, we present in Section V a lower
bound on the number of iterations required by any first order
method to reach a given sub-optimality. Using the insight
provided from the fixed-point interpretation we show how
to obtain an accelerated variant of GD in Section VI, which
turns out to be optimal in terms of convergence rate.
Notation. The set of natural numbers is denoted N :=
{1, 2, . . .}. Given a vector x = (x1, . . . , xn)T , we denote its
lth entry by xl. The transpose and trace of a square matrix A
are denoted AT and tr{A}, respectively. The euclidian norm
of a vector x is denoted ‖x‖ :=
√
xTx. The spectral norm of
a matrix M is denoted ‖M‖ := max
‖x‖=1
‖Mx‖. The spectral
decomposition of a positive semidefinite (psd) matrix Q ∈
R
n×n is Q = UΛUT with matrix U =
(
u(1), . . . ,u(n)
)
whose columns are the orthonormal eigenvectors u(i) ∈Rn
of Q and the diagonal matrix Λ containing the eigenvalues
λ1(Q) ≥ . . . ≥ λn(Q) ≥ 0 of Q on its diagonal. For a
square matrix M, we denote its spectral radius as ρ(M) :=
max{|λ|, λ is an eigenvalue}.
II. CONVEX FUNCTIONS
A function f(·) : Rn → R is convex if, for x,y ∈ Rn,
f((1− α)x+ αy) ≤ (1− α)f(x) + αf(y) (1)
holds for all α ∈ [0, 1] [5]. For a differentiable function f(x)
with gradient ∇f(x) a necessary and sufficient condition for
convexity is
f(y) ≥ f(x) +∇f(x)T (y − x), (2)
which has to hold for any x,y ∈ Rn.
Our main object of interest in this paper is the optimization
problem
x0 ∈ arg min
x∈Rn
f(x). (3)
Given a convex function f(x), we aim at finding a point x0
with lowest function value f(x0), i.e., f(x0) = minx f(x).
In order to motivate our interest in optimization problems
like (3), consider a machine learning problem based on
training data X := {(d(i), y(i))}Ni=1 consisting of N data
points with feature vector d(i) and output or label y(i). We
might approximate the label y(i) by a linear combination
xTd(i),
y(i) ≈ xTd(i). (4)
The choice for the weight vector x can be based on balancing
the empirical loss or training error, e.g., measured by the
mean squared error (1/N)
∑N
i=1(y
(i)−xTd(i))2 with some
regularization term, e.g., measured by the squared norm
‖x‖22 , i.e., the learning problem amounts to solving the
optimization problem
x0=arg min
x∈Rn
(1/N)
N∑
i=1
(y(i)−xTd(i))2+λ‖x‖22. (5)
The learning problem (5) is precisely of the form (3) with
the convex objective function
f(x) := (1/N)
N∑
i=1
(y(i)−xTd(i))2+λ‖x‖22. (6)
By choosing a large value for λ, we de-emphasize the
relevance of the training error and thus avoid overfitting.
However, choosing λ too large induces a bias if the true
underlying weight vectors x has a large norm [9]. One basic
strategy to find a suitable choice for λ is cross validation
[9].
Differentiable Convex Functions. It will be convenient
to associate a differentiable function f(·) with its gradient
operator
∇f : Rn → Rn,x 7→ ∇f(x). (7)
While the gradient operator ∇f is defined for any (even non-
convex) differentiable convex function, the gradient operator
of a convex function satisfies a strong structural property, i.e.,
it is a monotone operator [1].
Smooth and Strongly Convex Functions. If all second
order partial derivatives of the function f(x) exist and are
continuous, then f(x) is convex if and only if
∇2f(x)  0. (8)
In what follows we focus a particular class of twice differen-
tiable convex functions, i.e., with Hessian ∇2f(x) satisfying
L ≤ λl
(
∇2f(x)
)
≤ U (9)
with fixed and known positive constants U ≥ L > 0.
The set of all convex functions f(·) : Rn → R satisfying
(9) will be denoted SL,Un . As it turns out, the difficulty of
finding the minimum of some function f(·) ∈ SL,Un using
gradient methods is typically governed by the
condition number κ := U/L of function class SL,Un . (10)
. Thus, regarding the difficulty of optimizing the functions
in SL,Un , the absolute values of the bounds L and U in (9)
are not crucial, only their ratio U/L is.
One particular sub-class of functions f(·) ∈ SL,Un , which
is of paramount importance for the analysis of gradient
methods, are quadratic functions of the form
f(x) = (1/2)xTQ+qTx+c, (11)
with some vector q ∈ Rn and a psd matrix Q having eigen-
values λ(Q) ∈ [L,U ]. As can be verified easily, the gradient
and Hessian of such a quadratic function are obtained as
∇f(x) = Qx+ q and ∇2f(x) = Q, respectively.
It turns our that most of the results (see below) on gradient
methods for minimizing quadratic functions (11) with some
matrix Q having eigenvalues λ ∈ [L,U ] apply also with
minor modifications when expanding their scope from the
set of quadratic functions to the considerably larger set
SL,Un . This should not come as a surprise, since any smooth
function can be approximated locally around a point x0 by
a quadratic function which is obtained by a truncated Taylor
series [15]. In particular, we have [15]
f(y)−f(x)=f(x)+(y− x)T∇f(x)
+ (1/2)(y−x)T∇2f(z)(y−x), (12)
where z = ηx+ (1− η)y with some η ∈ [0, 1].
Let us now show that learning a linear regression model
(cf. (5)) amounts to minimizing a convex quadratic function
of the form (11). Indeed, using some elementary linear al-
gebraic manipulations, we can rewrite the objective function
in (6) as a quadratic (11) with the particular choices
QLR :=λI+
1
N
N∑
i=1
d(i)
(
d(i)
)T
, and qLR :=
2
N
N∑
i=1
y(i)d(i).
(13)
The eigenvalues of the matrix QLR obey [8]
λ ≤ λl
(
QLR
)
≤ λ+ λ1(DTD) (14)
with the data matrix D :=
(
d(1), . . . ,d(N)
)
∈ Rn×N .
Thus, learning a linear regression model via (5) amounts
to minimizing a convex function f(·) ∈ SL,Un with L = λ
and U = λ+ λ1(D
TD).
III. FIRST ORDER METHODS
Without a computational model taking into account a finite
amount of resources, the study of computational complexity
inherent to (3) becomes meaningless. Indeed, if we have
unbounded computational power at our disposal we could
build an “optimization device” which maps each function
f(·) ∈ SL,Un to its unique minimum x0. However, this
approach is infeasible since we cannot perfectly represent
such a mapping, let alone its domain SL,Un using real-
world hardware which can only handle finite sets instead
of continuous spaces like SL,Un .
As we have seen in the previous section, the regularized
linear regression model (5) amounts to minimizing a convex
quadratic function (11) with the choices (13). Even for this
special case, it is typically infeasible to have access to a
complete description of the objective function (11). Indeed,
in order to fully specify a quadratic function we need know
the matrix Q and vector q. For the linear regression model
this would require to compute QLR (cf. (13)) using the
training data X = {z(i)}Ni=1. Computing the matrix QLR
naively amounts to a number of arithmetic operations on the
order of N · n2. This might be prohibitive in a typical big
data application with N and n being on the order of billions
and moreover using distributed storage of the training data
X [7].
application layer
data layer
raw data z^{i} (e.g., stored at 
different servers all over the 
internet)
min
x
f(x) = (1/N)
N∑
i=1
ρ(z(i);x)
x
(0)
∇f(x(0)) x(1)
first order method x(k) ∈ span{x(0),∇f(x(0)), . . . ,∇f(x(k−1))}
∇f(x(1))
Fig. 1. Programming model underlying a FOM.
There has emerged a widely accepted computational
model for convex optimization which abstracts away the de-
tails of the computational and storage infrastructure. Within
this computational model, a method for solving (3) is never
provided with a complete description of the objective func-
tion but rather is can only access the function via an oracle
[5], [11].
We can think of an oracle model as an application
programming interface (API) which answers queries issued
by a convex optimization method (which is interpreted as
the application) (cf. Figure 1). There are different types of
oracles but one of the most popular type is a first order oracle
[11]. Given a query x ∈ Rn, a first order oracle returns the
function value f(x) and the gradient ∇f(x) at this particular
point.
A first order method (FOM) aims at solving (3) by sequen-
tially querying a first order oracle, at the current iterate x(k),
to obtain the function values f(x(k)) and gradient ∇f(x(k)).
Using the current and past information obtained from the
oracle, a FOM then constructs the new iterate x(k+1) such
that eventually limk→∞ x(k) = x0. For the sake of simplicity
and without essential loss in generality, we will only consider
FOM whose iterates x(k) satisfy [11]
x(k)∈span
{
x(0),∇f(x(0)), . . . ,∇f(x(k−1))
}
. (15)
IV. GRADIENT DESCENT
Let us now show how one of the most basic methods for
solving the problem (3), i.e., GD, can be obtained naturally
as fixed-point iterations involving the gradient operator ∇f
(cf. (7)).
Our point of departure is the necessary and sufficient
condition [4]
∇f(x0) = 0, (16)
for a vector x0 ∈ Rn to be optimal for the problem (3) with
a convex differentiable objective function f(·).
Lemma 1. We have ∇f(x) = 0 if and only if the vector
x ∈ Rn is a fixed point of the operator
T (α) : Rn → Rn : x 7→ x− α∇f(x), (17)
for an arbitrary but fixed non-zero α ∈ R \ {0}. Thus,
∇f(x) = 0 if and only if T (α)x = x. (18)
Proof. Consider a vector x such that ∇f(x) = 0. Then,
T (α)x (17)= x− α∇f(x) = x. (19)
Conversely, let x be a fixed point of T (α), i.e.,
T (α)x = x. (20)
Then,
∇f(x) = (1/α)(x− (x − α∇f(x)))
(17)
= (1/α)(x − T (α)x)
(20)
= 0. (21)
According to Lemma 1, the solution x0 of the optimiza-
tion problem (3) is obtained as the fixed point of the operator
T (α) (cf. (17)) with some non-zero α. As we will see
shortly, the freedom in choosing different values for α can
be exploited in order to compute the fixed points T (α) more
efficiently.
The most straightforward approach to finding the fixed-
points of an operator T (α) is provided by the iterations
x(k+1) = T (α)x(k). (22)
By tailoring a fundamental result of analysis (cf. [15, The-
orem 9.23]), we can characterize the convergence of the
sequence x(k) obtained from (22).
Lemma 2. Assume that for some q∈ [0, 1), we have
∥∥T (α)x− T (α)y
∥∥ ≤ q‖x− y‖, (23)
for any x,y ∈ Rn. Then, the operator T (α) has a unique
fixed point x0 and the iterates x
(k) (cf. (22)) satisfy
‖x(k) − x0‖ ≤ ‖x(0) − x0‖qk. (24)
Proof. Let us first verify that the operator T (α) cannot have
two different fixed points. Indeed, assume there would be
two different fixed points x, y such that
x = T (α)x, and y = T (α)y. (25)
This would imply, in turn,
q‖x− y‖
(23)
≥
∥∥T (α)x− T (α)y
∥∥
(25)
= ‖x− y‖. (26)
However, since q < 1, this inequality can only be satisfied if
‖x−y‖ = 0, i.e., we must have x = y. So far, we have show
that no two distinct fixed points can exist. The existence of
a unique fixed point x0 follows from [15, Theorem 9.23].
The estimate (35) can be obtained by induction and noting
‖x(k+1) − x0‖ (22)= ‖T (α)x(k) − x0‖
(a)
= ‖T (α)x(k) − T (α)x0‖
(23)
≤ q‖x(k) − x0‖. (27)
Here, step (a) is valid since x0 is a fixed point of T (α), i.e.,
x0 = T (α)x0.
In order to apply Lemma 2 to (22), we have to ensure
that the operator T (α) is a contraction, i.e., it satisfies (23)
with some contraction coefficient q ∈ [0, 1). For the operator
T (α) (cf. (17)) associated with the function f(·) ∈ SL,Un this
can be verified by standard results from vector analysis.
Lemma 3. Consider the operator T (α) : x 7→ x− α∇f(x)
with some convex function f(·) ∈ SL,Un . Then,
∥∥T (α)x− T (α)y
∥∥ ≤ q(α)‖x− y‖ (28)
with contraction factor q(α) = max{|1−Uα|, |1−Lα|}.
Proof. First,
T (α)x− T (α)y (17)= (x− y)− α(∇f(x) −∇f(y))
(a)
= (x − y)− α(x − y)T∇2f(z)
= (x− y)(I − α∇2f(z)) (29)
using z = ηx + (1 − η)y with some η ∈ [0, 1]. Here, we
used in step (a) the mean value theorem of vector calculus
[15, Theorem 5.10].
Combining (29) with the submultiplicativity of euclidean
and spectral norm [8, p. 55] yields
‖T (α)x− T (α)y‖ ≤ ‖x− y‖‖I− α∇2f(z)‖. (30)
The matrix M(α) := I−α∇2f(z) is symmetric with real-
valued eigenvalues [8]
λl
(
M(α)
)
∈ [1− Uα, 1− Lα]. (31)
Since also
‖M(α)‖ = max{|λl|}
(31)
≤ max{|1−Uα|, |1−Lα|}, (32)
we obtain from (30)
‖T (α)x−T (α)y‖
(32)
≤ ‖x−y‖max{|1−Uα|, |1−Lα|}. (33)
It will be handy to write out the straightforward combi-
nation of Lemma 2 and Lemma 3.
Lemma 4. Consider a convex function f(·) ∈ SL,Un with
the unique minimizer x0, i.e., f(x0) = minx f(x). We then
construct the operator T (α) : x 7→ x−α∇f(x) with a step
size α such that
q(α) := max{|1−Uα|, |1−Lα|} < 1. (34)
Then, starting from an arbitrary initial guess x(0), the
iterates x(k) (cf. (22)) satisfy
‖x(k) − x0‖ ≤ ‖x(0) − x0‖
[
q(α)
]k
. (35)
According to Lemma 4, and also illustrated in Figure 2,
starting from an arbitrary initial guess x(0), the sequence
generated by the fixed-point iteration (22) is guaranteed to
converge to the unique solution x0 of (3). What is more, this
convergence is quite fast, since the error ‖x(k)−x0‖ decays
exponentially according to (35). This exponential decrease
implies that after the number of additional iterations required
to have on more correct digit in x(k) is constant.
Let us now work out the iterations (22) more explicitly
by inserting the expression (17) for the operator T (α). We
then obtain the following equivalent representation of (22):
x(k+1) = x(k) − α∇f(x(k)). (36)
This iteration is nothing but plain vanilla GD using a fixed
step size α [9].
Since GD is precisely the fixed-point iteration (22), we can
rely on Lemma 4 for characterising the convergence (rate) of
GD. In particular, convergence of GD is ensured by chosing
the step size of GD (36) such that q(α) :=max{|1−Uα|, |1−
Lα|} < 1. Moreover, in order to make the convergence as
fast as possible we need to chose the step size α = α∗
which makes the contraction factor q(α) (cf. (34)) as small
as possible.
T (α)
x0
Fig. 2. Fixed-point iterations for a contractive mapping T (α)
with the unique fixed point x0.
q∗= κ−1κ+1
α∗= 2L+U
q(α)
1
1/U
α
|1−Uα|
|1−Lα|
Fig. 3. Dependence of contraction factor q(α) := max{|1−
αL|, |1−αU |} on step size α.
In Figure 3, we illustrate how the quantifies |1−αL| and
|1−αU | evolve as the step size α (cf. (36)) is varied. From
Figure 3 we can easily read off the optimal choice
α∗ =
2
L+ U
(37)
yielding the smallest possible contraction factor
q∗ = min
α∈[0,1]
q(α) =
U−L
U+L
(10)
=
κ−1
κ+1
. (38)
We have arrived at the following characterization of GD for
minimizing convex functions f(·) ∈ SL,Un .
Theorem 5. Consider the optimization problem (3) with
objective function f(·) ∈ SL,Un , where the parameters L and
U are fixed and known. Starting from an arbitrarily chosen
initial guess x(0), we construct a sequence by GD (36) using
the optimal step size (37). Then, we have
‖x(k)−x0‖≤
(
κ−1
κ+1
)k
‖x(0)−x0‖. (39)
In what follows, we will use the shorthand T := T (α∗)
for the gradient operator T (α) obtained for the optimal step
size α = α∗ (cf. (37)).
V. LOWER BOUNDS ON NUMBER OF
ITERATIONS
According to the previous section, solving (3) can be ac-
complished by the simple GD iterations (36). The particular
choice α∗ for the step size α in (36) ensures the convergence
rate
(
κ−1
κ+1
)k
with the condition number κ = U/L of the
function class SL,Un . While the convergence is quite fast,
i.e., the error decays exponentially with iteration number k,
we would of course like to know how efficient this method
is in general. The next result provides a fundamental lower
bound on the convergence rate of any first order method (cf.
(15)) for solving problems like (3).
Lemma 6. Consider a particular FOM, which for a given
convex function f(·) ∈ SL,Un generates iterates x(k) satis-
fying (15). For fixed L,U there is a sequence of functions
fn(·) ∈ SL,Un (indexed by dimension n) such that
‖x(k)−x0‖ ≥ ‖x(0)−x0‖
1−1/√κ
1+
√
κ
(√
κ−1√
κ+1
)k
− |δ(n)|
(40)
with limn→∞ |δ(n)| = 0.
Proof. Without loss of generality we consider FOM which
use the initial guess x(0) = 0. Let us now construct a
function fn(·) ∈ SL,Un which is particularly difficult to
optimize by a FOM (cf. (15)) such as GD (36). The function
is a quadratic
f̂(x) := (1/2)xT
(
(L/4)(κ− 1)Q̃+ LI
)
x+ q̃Tx (41)
with vector
q̃ :=
L(κ−1)
4
(1, 0, . . . , 0)T ∈ Rn (42)
and matrix
P := (L/4)(κ− 1)Q̃+ LI ∈ Rn×n. (43)
The matrix Q̃ is defined row-wise by successive circular
shifts of its first row (2,−1, 0, . . . , 0,−1)T ∈ Rn. Note that
the matrix P is a circulant matrix [10] with orthonormal
eigenvectors
{
u(l)
}n
l=1
given element-wise as
u
(l)
i = (1/
√
n) exp(j2π(i−1)(l−1)/n). (44)
The eigenvalues λl(P) of the circulant matrix P are obtained
as the discrete Fourier transform (DFT) coefficients of its
first row [10]
p=
L(κ−1)
4
(2,−1, 0, . . . , 0,−1)+L(1, 0, . . . , 0)T , (45)
i.e.,
λl(P) =
n∑
i=1
pi exp(−j2π(i−1)(l−1)/n) (46)
= (L/2)(κ−1)(1−cos(−2π(i−1)/n) + L.
Thus, λl(P) ∈ [L,U ] and, in turn, fn(·) ∈ SL,Un (cf. (9)).
Consider the sequence x(k) generated by the FOM, i.e.,
which satisfies (15), for the particular objective function
fn(x) (cf. (41)) using initial guess x0 = 0. It can be verified
easily that the kth iterate x(k) has only zero entries starting
from index k + 1, i.e.,
x
(k)
l = 0 for all l ∈ {k + 1, . . . , n}. (47)
This implies
‖x(k) − x0‖ ≥ |x0,k+1|. (48)
The main part of the proof is then to show that the minimizer
x0 for the particular function fn(·) cannot decay too fast,
i.e., we will derive a lower bound on |x0,k+1|.
Let us denote the DFT coefficients of the finite length
discrete time signal represented by the vector q̃ as
cl =
n∑
i=1
q̃i exp(−j2π(i− 1)l/n)
(42)
= (L/4)(κ− 1). (49)
Using the optimality condition (16), the minimizer for (41)
is
x0 = −P−1q̃. (50)
Inserting the spectral decomposition P =
n∑
l=1
λlu
(l)
(
u(l)
)H
[10, Theorem 3.1] of the psd matrix P into (50),
x0,k = −
(
P−1q̃
)
k
(44)
= −(1/n)
n∑
i=1
(ci/λi) exp(j2π(i−1)(k−1)/n)
(46),(49)
= − 1
n
n∑
i=1
exp(j2π(i−1)(k−1)/n)
2(1−cos(−2π(i−1)/n))+4/(κ−1). (51)
We will also need a lower bound on the norm ‖x0‖ of the
minimizer of fn(·). This bound can be obtained from (50)
and λl(P)∈ [L,U ], i.e., λl
(
P−1
)
∈ [1/U, 1/L],
‖x0‖ ≤ (1/L)‖q̃‖ (42)=
κ−1
4
. (52)
The last expression in (51) is a Riemann sum for the inte-
gral
1∫
θ=0
exp(−j2πθ)
2(1−exp(−j2πθ))+4/(κ−1)dθ. Indeed, by basic calculus
[15, Theorem 6.8]
x0,k = −
1∫
θ=0
exp(j2π(k−1)θ)
2(1−cos(2πθ))+4/(κ−1)dθ+δ(n) (53)
where the error δ(n) becomes arbitrarily small for suffi-
ciently large n, i.e., lim
n→∞
|δ(n)| = 0.
According to Lemma 9, we have
1∫
θ=0
exp(j2π(k−1)θ)
2(1−cos(2πθ))+4/(κ−1)dθ = −
κ−1
4
√
κ
(√
κ−1√
κ+1
)k
.
(54)
By inserting (54) into (53),
x0,k =
κ−1
4
√
κ
(√
κ−1√
κ+1
)k
+δ(n), (55)
which is valid for all k ∈ N. Inserting (55) into (48),
‖x(k)−x0‖ ≥ |x0,k+1|
≥ κ−1
4
√
κ
(√
κ−1√
κ+1
)(√
κ−1√
κ+1
)k
−|δ(n)|
(52)
≥ ‖x0‖
1−1/√κ
1+
√
κ
(√
κ−1√
κ+1
)k
−|δ(n)|
x
(0)=0
≥ ‖x(0)−x0‖
1−1/√κ
1+
√
κ
(√
κ−1√
κ+1
)k
−|δ(n)| (56)
There is a considerable gap between the upper bound
(39) on the error achieved by GD after k iterations and the
lower bound (40) which applies to any FOM which is run
for the same number iterations. In order to illustrate this
gap, we have plotted in Figure 4 the upper and lower bound
for the quite moderate condition number κ = 100. Thus,
there might exist FOM which are different from GD (39)
but which converge faster and come more close to the lower
bound (40). Indeed, in the next section we will detail how
to obtain a faster FOM by applying a fixed point preserving
transformation to the operator T underlying GD (cf. (22)).
This accelerated gradient method is known as the heavy balls
method [13] and effectively achieves the lower bound (40),
i.e., the method is optimal in terms of convergence rate.
VI. ACCELERATING GRADIENT DESCENT
In this section we show how to modify the basic GD
iterations (36) in order to obtain an accelerated FOM whose
k
1/2
50 100
1
κ=100
(
κ−1
κ+1
)k
, GD
(√κ−1√
κ+1
)k
Fig. 4. Upper bound (39) on convergence rate of GD and
lower bound (40) for FOMs minimizing functions f(·) ∈
SL,Un with condition number κ=U/L=100.
convergence rate essentially matches the lower bound (40)
for the function class SL,Un with condition number κ =
U/L > 1 (cf. (10)) and is therefore optimal among all
FOMs.
Our derivation of this accelerated FOM, which is inspired
heavily by [14], starts from an equivalent formulation of GD
as the fixed-point iteration
x̄(k) = T x̄(k−1) (57)
with the operator
T :R2n→R2n :
(
u
v
)
7→
(
u−α∗∇u
u
)
=
(
T u
u
)
, (58)
with the optimal GD step-size α∗ (37). As can be verified
easily, the fixed-point iteration (57) starting from an arbitrary
initial guess x̄(0) =
(
z(0)
y(0)
)
are related to GD x(k) with
initial guess z(0) as
x̄(k) =
(
x(k)
x(k−1)
)
(59)
for all iterations k ≥ 1.
Using the equivalence (59) we can straightforwardly con-
clude from Theorem 7, that for any initial guess x̄(0) the
iterations (57) converge to the fixed point
(
x0
x0
)
with x0
being the unique minimizer of f(·) ∈ SL,Un . Moreover,
the convergence speed of the fixed-point iterations (57)
is trivially the same as those of GD, i.e., proportional to(
κ−1
κ+1
)k
.
In what follows we will modify the operator T to obtain
a new operator M : R2n →R2n which has the same fixed
point as T but improved contraction behavior, i.e., the fixed
point iteration
x̃(k) = Mx̃(k−1), (60)
will converge faster than those obtained from T in (57). In
T
M
Fig. 5. Schematic illustration of the fixed-point iteration
using operator T (58) (equivalent to GD) and for the
modified operator M (61) (yielding HB method).
particular, this improved operator M is
M :R2n→R2n :
(
u
v
)
7→
(
u−α̃∇u+ β̃(u− v)
u
)
, (61)
with
α̃ :=
4
(
√
U+
√
L)2
, and β̃ :=
[√
U−
√
L√
U+
√
L
]2
. (62)
As can be verified easily, the fixed point (x0,x0) of T is
also a fixed point of M.
Before we analyze the convergence rate of the fixed-point
iteration (60), let us work out explicitly the FOM which is
represented by the fixed-point iteration (60). To this end, we
partition the kth iterate, for k ≥ 1, as
x̃(k) :=
(
x
(k)
HB
x
(k−1)
HB
)
. (63)
Inserting (63) into (60), we have for k ≥ 1
x
(k)
HB = x
(k−1)
HB −α̃∇f(x
(k−1)
HB )+β̃(x
(k−1)
HB −x
(k−2)
HB ) (64)
with the convention x
(−1)
HB := 0. The iteration (64) defines
a FOM which is also known as the heavy balls (HB)
method [13], for solving the optimization problem (3). By
contract to GD (36), the HB iteration (64) makes also use of
the penultimate iterate x
(k−2)
HB for the determination of the
current iterate x
(k)
HB.
We will now characterize the converge rate of HB via its
fixed-point equivalent (60). To this end, we first restrict our-
selves to the subclass of SL,Un given by quadratic functions
of the form (11).
Theorem 7. Consider the optimization problem (3) with
objective function f(·) ∈ SL,Un which is a quadratic (11).
Starting from an arbitrarily chosen initial guess x
(−1)
HB and
x
(0)
HB, we construct a sequence by the HB iteration (36). Then,
we have
‖x(k)HB−x0‖≤C(κ)k
(√
κ−1√
κ+1
)k
(‖x(0)HB−x0‖+‖x
(−1)
HB −x0‖).
(65)
with
C(κ) := 4c̃
√
κ+1√
κ−1 . (66)
Proof. By evaluating the operator M (cf. (61)) for a
quadratic function f(·) of the form (11), we can verify
Mx−My = R(x−y) (67)
with the matrix
R =
(
(1+β̃)I−α̃Q −β̃I
I 0
)
. (68)
This matrix R ∈ R2n×2n is a 2 × 2 block matrix whose
individual blocks can be diagonalized simultaneously via
the orthonormal eigenvectors U =
(
u(1), . . . ,u(n)
)
of the
psd matrix Q. Inserting the spectral decomposition Q =
Udiag{λi}ni=1UT into (68),
R = UPBPTUT , (69)
with some (orthonormal) permutation matrix P and a block
diagonal matrix
B :=


B(1) . . . 0
0
. . .
...
0 . . . B(n)

 , B(i) :=
(
1+β̃−α̃λi −β̃
1 0
)
.
(70)
Combining (69) with (67) and inserting into (60) yields
x̃(k)−x̃0=UPBkPTUT (x̃(0)−x̃0). (71)
Thus, in order to control the convergence rate of the itera-
tions (60), i.e., the decay of the error ‖x̃(k) − x̃0‖ we will
now derive an upper bound on the spectral norm of the block
diagonal matrix Bk (cf. (70)).
Due to the block diagonal structure (70), we can control
the norm of Bk via controlling the norm of the powers of
its diagonal blocks B(i) since
‖Bk‖ = max
i
∥∥(B(i)
)k∥∥. (72)
A pen and paper exercise reveals
ρ
(
B(i)
)
≤ β̃1/2 (62)=
√
U−
√
L√
U+
√
L
=
√
κ−1√
κ+1
, (73)
k
lo
g
‖x
(k
)
−
x
0
‖
50 100
κ=100
GD
lower bound (40)
HB
Fig. 6. Upper bound (65) on convergence rate of HB (solid),
upper bound (39) for error of GD (dashed) and lower bound
(40) (dotted) for FOMs for the function class SL,Un with
condition number κ=U/L=100.
According to Lemma 8,
(
B(i)
)k
=
(
λk1 d
0 λk2
)
, (74)
with |λ1|, |λ2| ≤ β̃1/2 and d ≤ k(2+2β̃+α̃)β̃(k−1)/2. Using
the shorthand c̃ := (2+2β̃+α̃), we can estimate the spectral
norm of Bk as
‖Bk‖ (72)= max
i
∥∥(B(i)
)k∥∥
(74)
≤
(√
κ−1√
κ+1
)k(
1+kc̃
√
κ+1√
κ−1
)
. (75)
Combining (75) with (71),
‖x̃(k)−x̃0‖≤
(√
κ−1√
κ+1
)k(
1+kc̃
√
κ+1√
κ−1
)
‖x̃(0)−x̃0‖
c̃≥1
≤ 2kc̃
√
κ+1√
κ−1
(√
κ−1√
κ+1
)k
‖x̃(0)−x̃0‖. (76)
Using (63), the error bound (76) can be translated into an
error bound on the heavy balls iterates, i.e.,
‖x(k)HB−x0‖≤
4kc̃
√
κ+1√
κ−1
(√
κ−1√
κ+1
)k
(‖x(0)HB−x0‖+‖x
(−1)
HB −x0‖). (77)
The upper bound (65) does not exactly match the lower
bound (40) which applies to any FOM which is run for
k iterations. However, the discrepancy is rather decent as
the linear factor k in (65) grows much slower than the
exponential
(√
κ−1√
κ+1
)k
in (65) decays. In Figure 6, we depict
the upper bound (65) on the error of HB along with the upper
bound (39) on the error of GD and the lower bound (40) on
the error of any FOM after k iterations.
VII. CONCLUSIONS
We have presented a fixed-point theory of some basic
gradient methods for minimizing convex functions. The
approach via fixed-point theory allows for a rather elegant
analysis of the convergence properties of these gradient
methods. In particular, their convergence rate is obtained as
the contraction factor for an operator associated with the
objective function. The fixed-point approach is also appeal-
ing since it lends rather naturally to acceleration of gradient
methods via fixed-point preserving transformations of the
underlying operator. We plan to further develop the fixed-
point theory of gradient methods in order to acommodate
stochastic variants of GD such as SGD. Furthermore, we
will bring the popular class of proximal methods into the
picture by replacing the gradient operator underlying GD
with the proximal operator. However, by contrast to FOM
such as GD, proximal methods use a different oracle model
(cf. Figure 1). In particular, proximal methods require an
oracle which can evaluate the proximal mapping efficiently
which is typically more expensive than gradient evaluations.
Nonetheless, the popularity of proximal methods is due to
the fact that for objective functions arising in many important
machine learning applications, the proximal mapping can be
evaluated efficiently.
VIII. TECHNICALITIES
We collect some elementary results from linear algebra
and analysis which are required for the proof of our main
results.
Lemma 8. Consider a matrix M =
(
a b
1 0
)
∈ R2×2 with
spectral radius ρ(M). Then, there is an orthonormal matrix
U∈R2×2 such that for k∈N
Mk = U
(
λk1 d
0 λk2
)
UT (78)
where λ1, λ2 are eigenvalues of M and |d| ≤ k(|a|+ |b|+
1)ρk−1(M).
Proof. Consider an eigenvalue λ1 of the matrix M with
normalized eigenvector u=(u1, u2)
T , i.e., Mu=λ1u with
‖u‖2 = 1. According to [8, Lemma 7.1.2], we can find a
normalized vector v=(v1, v2)
T , orthogonal to u, such that
M = (u,v)
(
λ1 d
0 λ2
)
(u,v)T , (79)
or equivalently
(
λ1 d
0 λ2
)
= (u,v)TM(u,v), (80)
with some eigenvalue λ2 of M. As can be read off (80),
d = u1(u2a + v2b) + v1u2 which implies (78) since
|u1|, |u2|, |v1|, |v2| ≤ 1. Based on (79), we can verify (78)
by induction.
Lemma 9. For any κ > 1 and k ∈ N,
1∫
θ=0
exp(j2π(k−1)θ)
2(1−cos(2πθ))+4/(κ−1)dθ=−
κ−1
4
√
κ
(√
κ−1√
κ+1
)k
. (81)
Proof. Let us introduce the shorthand z := exp(j2πθ) and
further develop the LHS of (81) as
1∫
θ=0
zk−1
2(1−(z−1+z)/2)+4/(κ−1)dθ
=
1∫
θ=0
zk−1
2(1−(z−1+z)/2)+4/(κ−1)dθ
=
1∫
θ=0
zk
2(z−(1+z2)/2)+4/(κ−1)dθ. (82)
The denominator of the integrand in (82) can be factored as
2(z−(1+z2)/2)+4/(κ−1) = (z−z1)(z−z2) (83)
with
z1 :=
√
κ+1√
κ−1 , and z2 :=
√
κ−1√
κ+1
. (84)
Thus, we have
1∫
θ=0
zk
2(z − (1 + z2)/2) + 4/(κ−1)dθ
=
1∫
θ=0
zk
(z − z1)(z − z2)
dθ
=
1∫
θ=0
zk(z1 − z2)−1
z − z1
− z
k(z1 − z2)−1
z − z2
dθ. (85)
Since |z2| < 1, we can develop the second term in (85) by
using the identity [12, Sec. 2.7]
1∫
θ=0
exp(j2πkθ)
exp(j2πθ)−αdθ=α
k−1 for k∈N, α∈R, |α|<1. (86)
Since |z1| > 1, we can develop the first term in (85) by
using the identity [12, Sec. 2.7]
1∫
θ=0
exp(j2πkθ)
exp(j2πθ)−αdθ=0 for k∈N, α∈R, |α|>1. (87)
Applying (86) and (87) to (85),
1∫
θ=0
zk
2(z−(1+z2)/2)+4/(κ−1)dθ=−
zk−12
z1−z2
. (88)
Inserting (88) into (82), we arrive at
1∫
θ=0
exp(j2π(k−1)θ)
2(1−cos(2πθ))+4/(κ−1)dθ = −
zk−12
z1−z2
. (89)
The proof is finished by noting the identity
1
z1−z2
(84)
=
√
κ+1√
κ−1 −
√
κ−1√
κ+1
=
4
√
κ
κ−1 . (90)
IX. REFERENCES
[1] H. H. Bauschke and P. L. Combettes. Convex Analysis
and Monotone Operator Theory in Hilbert Spaces.
Springer, 2010.
[2] P. Birken. Termination criteria for inexact fixed-point
schemes. Num. Lin. Alg. App., 22(4):702 – 716, Aug.
[3] L. Bottou and O. Bousquet. The tradeoffs of large
scale learning. In Advances in Neural Information
Processing Systems (NIPS), pages 161–168, 2008.
[4] S. Boyd and L. Vandenberghe. Convex Optimization.
Cambridge Univ. Press, Cambridge, UK, 2004.
[5] V. Cevher, S. Becker, and M. Schmidt. Convex
optimization for big data: Scalable, randomized, and
parallel algorithms for big data analytics. IEEE Signal
Processing Magazine, 31(5):32–43, Sept. 2014.
[6] A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous,
and Y. LeCun. The loss surfaces of multilayer net-
works. arxiv, 2015.
[7] G. B. Giannakis, K. Slavakis, and G. Mateos. Signal
processing for big data. In Tutorial at EUSIPCO 2014,
2014.
[8] G. H. Golub and C. F. Van Loan. Matrix Computations.
Johns Hopkins University Press, Baltimore, MD, 3rd
edition, 1996.
[9] I. Goodfellow, Y. Bengio, and A. Courville. Deep
Learning. MIT Press, 2016.
[10] R. Gray. Toeplitz and Circulant Matrices: A review,
volume 2 of Foundations and Trends in Communica-
tions and Information Theory. 2006.
[11] Y. Nesterov. Introductory lectures on convex opti-
mization, volume 87 of Applied Optimization. Kluwer
Academic Publishers, Boston, MA, 2004. A basic
course.
[12] A. V. Oppenheim, R. W. Schafer, and J. R. Buck.
Discrete-Time Signal Processing. Prentice Hall, En-
glewood Cliffs, NJ, 2nd edition, 1998.
[13] B. T. Polyak. Some methods of speeding up the con-
vergence of iteration methods. USSR Computational
Mathematics and Mathematical Physics, 4(5):1 – 17,
1964.
[14] B. Recht. Lyapunov analysis and the heavy ball
method. Lecture Notes, 2012.
[15] W. Rudin. Principles of Mathematical Analysis.
McGraw-Hill, New York, 3 edition, 1976.
[16] H. F. Walker and P. Ni. Anderson acceleration for fixed-
point iterations. SIAM J. Numer. Anal., 49(4):1715 –
1735, 2011.

