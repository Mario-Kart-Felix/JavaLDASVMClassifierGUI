iterative Random Forests to discover predictive and stable
high-order interactions
Sumanta Basu∗a, Karl Kumbier∗b, James B. Brown†c,d,b,e, and Bin Yu†b,f
a
Department of Biological Statistics and Computational Biology, Cornell University
bStatistics Department, University of California, Berkeley
cCentre for Computational Biology, School of Biosciences, University of Birmingham
dMolecular Ecosystems Biology Department, Lawrence Berkeley National Laboratory
ePreminon, LLC
fDepartment of Electrical Engineering and Computer Sciences, University of California, Berkeley
Abstract
Genomics has revolutionized biology, enabling the interrogation of whole transcriptomes, genome-
wide binding sites for proteins, and many other molecular processes. However, individual genomic assays
measure elements that operate in vivo as components of larger molecular machines that regulate gene
expression. Understanding these processes and the high-order interactions that govern them presents a
substantial statistical challenge. Building on Random Forests (RF), Random Intersection Trees (RIT),
and through extensive, biologically inspired simulations, we developed iterative Random Forests (iRF).
iRF leverages the Principle of Stability to train an interpretable ensemble of decisions trees and detect
stable, high-order interactions with same order of computational cost as RF. We demonstrate the util-
ity of iRF for high-order interaction discovery in two prediction problems: enhancer activity for the
early Drosophila embryo and alternative splicing of primary transcripts in human derived cell lines. In
Drosophila, iRF re-discovered the essential role of zelda (zld) in early zygotic enhancer activation, and
novel third-order interactions, e.g. between zld, giant (gt), and twist (twi). In human-derived cells, iRF
re-discovered that H3K36me3 plays a central role in chromatin-mediated splicing regulation, and iden-
tified novel 5th and 6th order interactions, indicative of multi-valent nucleosomes with specific roles in
splicing regulation. By decoupling the order of interactions from the computational cost of identification,
iRF opens new avenues of inquiry in genome biology, automating hypothesis generation for the discovery
of new molecular mechanisms from high-throughput, genome-wide datasets.
1 Introduction
Advances in next generation sequencing (NGS) technologies have enabled high throughput, genome-wide
measurements of protein-DNA and protein-RNA interactions. Databases generated by the Berkeley Drosophila
Transcriptional Network Project (BDTNP) and ENCODE consortium provide maps of transcription factor
(TF) binding events and regulatory chromatin marks for substantial fractions of the regulatory factors active
in several systems, including both the model organism Drosophila melanogaster and human-derived cell lines.
These databases contain a previously inconceivable amount of molecular information and have already shed
light into the complex architecture of functional regulation in bilaterian genomes (Fisher et al., 2012; Thomas
et al., 2011; Li et al., 2008; Breeze et al., 2016; Hoffman et al., 2012; ENCODE Project Consortium, 2012).
However, a central challenge lies in the fact that ChIP-seq, the principal tool used to measure DNA-protein
interactions, assays a single protein target at a time. In well studied regulatory systems, regulatory factors
such as TFs act in concert with other chromatin-associated and RNA-associated proteins, often through
∗SB and KK contributed equally to this work
†JB and BY contributed equally to this work
1
ar
X
iv
:1
70
6.
08
45
7v
1 
 [
st
at
.M
L
] 
 2
6 
Ju
n 
20
17
stereospecific interactions (Hoffman et al., 2012; Dong et al., 2012), and for a recent review see (Hota and
Bruneau, 2016). Discovering these types of interactions from high-throughput genomics datasets, such as
those generated by the ENCODE Consortium (ENCODE Project Consortium, 2012), will help us learn
principles that underlie gene regulation.
Popular statistical and machine learning methods for detecting interactions among features include deci-
sion trees and their ensembles: CART (Breiman et al., 1984), Random Forests (RF) (Breiman, 2001), Node
Harvest (Meinshausen, 2010), Forest Garotte (Meinshausen, 2009), and Rulefit3 (Friedman and Popescu,
2008), as well as methods more specific to gene-gene interactions with categorical features: logic regression
(Ruczinski et al., 2001), Multifactor Dimensionality Reductions (Ritchie et al., 2001), and Bayesian Epistasis
mapping (Zhang and Liu, 2007). With the exception of RF, the above tree-based procedures grow shallow
trees to prevent overfitting, excluding the possibility of detecting high-order interactions in a computation-
ally feasible manner without affecting predictive accuracy. RF is an attractive alternative, growing deep
decision trees that leverage high-order interactions while still providing state-of-the-art prediction accuracy.
However, interpreting these interactions from the resulting ensemble of decision trees remains a challenge
due, in part, to the instability of RF decision paths.
In this paper, we take a step towards overcoming these challenges and propose a novel, fast algorithm built
on RF to search for important, potentially local, high-order interactions. Aside from RF’s high predictive ac-
curacy, the decision tree base learner captures the underlying biology of local, combinatorial interactions (Li
et al., 2012). For instance, there is quantitative evidence in several model systems to suggest that a genomic
region can act as an enhancer only when activating transcription factors achieve sufficiently high DNA-
occupancy at their cognate binding sites, and silencing chromatin marks are sufficiently depleted (Knowles
and Biggin, 2013). Further, RF’s invariance to monotone transformations to a large extent mitigates nor-
malization issues, an important concern in the analysis of genomics data where signal to noise ratios vary
widely even between biological replicates (Landt et al., 2012; Li et al., 2011).
Guided by the Principle of Stability (Yu, 2013), our proposed method, iterative Random Forest (iRF),
sequentially grows feature-weighted RFs to perform soft dimension reduction of the feature space and stabilize
decision paths. We decode the fitted RFs by sampling decision rules using the recently proposed Random
Intersection Trees (RIT) algorithm (Shah and Meinshausen, 2014). This procedure extracts stable high-
order feature combinations prevalent in the RF decision tree ensemble. Moreover, the inherent structure of
decision trees enables us to detect local interactions, an important feature for biological data where a single
molecule often performs many roles in various cellular contexts. Using empirical and numerical examples, we
show that iRF has competitive predictive accuracy with RF, and extracts both known and compelling novel
interactions in two motivating biological problems in epigenomics and transcriptomics. An open source R
implementation of iRF is available at https://github.com/sumbose/iRF.
2 Our method: iterative Random Forests (iRF)
The iRF algorithm searches for high-order interactions among features in three steps. First, iterative feature
re-weighting adaptively regularizes the RF fitting procedure. Second, decision rules extracted from a feature-
weighted RF map from continuous or categorical to binary features. This mapping forms the foundation for
our generalization of Random Intersection Trees (RIT), a computationally efficient algorithm that searches
for high-order interactions in binary data (Shah and Meinshausen, 2014). Finally, a bagging step assesses
the stability of recovered interactions with respect to the bootstrap-perturbation of the data. We briefly
review feature-weighted RF and RIT before presenting our algorithm.
2.1 Preliminaries: Feature-weighted RF and RIT
To reduce the dimensionality of the feature space without removing marginally unimportant features that
may take part in high-order interactions, we use a feature-weighted version of RF. Specifically, for a set of
non-negative weights w = (w1, . . . , wp), let RF (w) denote a feature-weighted RF constructed with w. In
RF (w), instead of taking a uniform random sample of features at each split, one chooses the jth feature with
probability proportional to wj . Weighted tree ensembles have been proposed in (Amaratunga et al., 2008)
2
under the name “enriched random forests” and used for feature selection in genomic data analysis. Note
that with this notation, Breiman’s original RF amounts to RF (1/p, . . . , 1/p).
iRF builds upon a generalization of RIT, an algorithm that searches for high-order interactions among
binary features in a deterministic setting. RIT recovers s-order interactions in a dataset with p binary features
with high probability (relative to the randomness introduced by RIT) at a substantially lower computational
cost than O(ps), provided the interaction pattern is sufficiently prevalent in the data and individual features
are sparse. Here, we briefly describe the basic RIT algorithm. For a complete description of RIT, we refer
the readers to the original paper (Shah and Meinshausen, 2014).
Consider a binary classification problem with n observations and p features. Suppose we are given data
in the form (Zi, Ii), i = 1, . . . , n. Here, each Zi ∈ {0, 1} is a binary label and we view each feature-index
subset Ii ⊆ {1, 2, . . . , p} as an interaction. In the context of gene transcription, Ii can be thought of as
a collection of TFs and histone modifications with abnormally high or low enrichments near the ith gene’s
promoter region, and Zi can indicate whether gene i is transcribed or not. For a general binary classification
with p binary features, Ii can be viewed as the indices of “active” features associated with observation i.
With these notations, prevalence of an interaction S ⊆ {1, . . . , p} in the class C ∈ {0, 1} is defined as
Pn(S|Z = C) :=
∑n
i=1 1[S ⊆ Ii]∑n
i=1 1[Zi = C]
For given thresholds 0 ≤ θ0 < θ1 ≤ 1, RIT searches for interactions S, with injected randomness described
below, satisfying
Pn(S|Z = 1) ≥ θ1, Pn(S|Z = 0) ≤ θ0. (1)
For each class C ∈ {0, 1} and a pre-specified integer D, let i(j), j = 1, ..., D, be a randomly chosen index
from the set of observations {i : Zi = C}. To search for interactions S satisfying equation (1), RIT takes
D-fold intersections Ii(1) ∩Ii(2) ∩ . . .∩Ii(D) from such randomly selected observations in class C. To reduce
computational complexity, these interactions are performed in a tree-like fashion (SI Algorithm 1), where
each non-leaf node has nchild children. Any non-empty set that remains following the D-fold intersection
operation is said to be a survived interaction. This process is repeated M times for a given class C, resulting
in a collection of survived interactions S = ⋃Mm=1 Sm. Here, each Sm is itself a set of interactions that
survived the intersection process in tree m = 1, . . . ,M . The prevalence of these interactions across different
classes are subsequently compared using (1). The main intuition is that if an interaction S is highly prevalent
within a class, it will survive the D-fold intersection with high probability.
2.2 iterative Random Forests (iRF)
The iRF algorithm places interaction discovery in a supervised learning framework to identify class-specific,
active index sets required for RIT. This framing allows us to recover high-order interactions that are associ-
ated with accurate prediction in feature-weighted RF.
We consider the binary classification setting with training data D in the form {(xi, yi)}ni=1, with con-
tinuous or categorical features x = (x1, . . . , xp), and a binary label yi ∈ {0, 1}. Our goal is to find subsets
S ⊆ {1, . . . , p} of features, or interactions, that are both highly prevalent within a class C ∈ {0, 1}, and that
provide good differentiation between the two classes. To encourage generalizability of our results, we seek
interactions that are robust to small perturbations in the data. We achieve this by searching for interactions
in ensembles of decision trees fitted on bootstrap samples of D.
Before describing iRF, we present a generalized RIT that uses any RF, weighted or not, to generate active
index sets from continuous or categorical features. Our generalized RIT is independent of the other iRF
components in the sense that one could generate the input to RIT using any approach for selecting active
features. We remark on our particular choices in SI.
Generalized RIT (through an RF): For each tree t = 1, . . . , T in the output tree ensemble of an RF,
we collect all leaf nodes and index them by jt = 1, ..., J(t). Each feature-response pair (yi,xi) is represented
with respect to a tree t by (Zit , Iit), where Iit is the set of unique feature indices falling on the path of the
leaf node containing (yi,xi) in the t
th tree. Hence, each (yi,xi) produces T such index set and label pairs,
3
corresponding to the T trees. We aggregate these pairs across observations and trees as
R = {(Zit , Iit) : xi falls in leaf node it of tree t} (2)
and apply RIT on this transformed dataset R to obtain a set of interactions.
We now describe the three components of iRF. The complete workflow is presented in Algorithm (1) and
a depiction is shown in Figure 1.
1. Iteratively re-weighted RF: Given an iteration number K, iRF iteratively grows K weighted
Random Forests RF (w(k)), k = 1, . . . ,K on the data D. The first iteration of iRF (k = 1) starts with
w(1) := (1/p, . . . , 1/p), and stores the importance (mean decrease in Gini impurity) of the p features as
v(1) = (v
(1)
1 , . . . , v
(1)
p ). For iterations k = 2, . . . ,K, we set w(k) = v(k−1). That is, we grow a weighted RF
with weights set equal to the RF feature importance from the previous iteration.
2. Generalized RIT (through RF (w(K))): We apply the generalized RIT to the last feature-weighted
RF grown in iteration K. That is, for each observation point, the collection of decision rules or trees generated
in the process of fitting RF (w(K)) provides the mapping from continuous or categorical to binary features
required for RIT. This process produces a collection of interactions S.
3. Bagged Stability Scores: In addition to bootstrap sampling in the weighted RF, we use an “outer
layer” of bootstrapping to assess the stability of recovered interactions. We generate bootstrap samples of the
data D(b), b = 1, . . . , B, fit RF (w(K)) on each bootstrap sample D(b), and use our generalized RIT to identify
interactions S(b) in each bootstrap sample. We define the stability score of an interaction S ∈ ∪Bb=1S(b) as
sta(S) =
1
B
B∑
b=1
1{S ∈ S(b)},
representing the proportion of times (out of B bootstrap samples) an interaction appears as an output of
RIT. This averaging step is exactly the Bagging idea of Breimain (Breiman, 1996).
Algorithm 1: iterative Random Forest (iRF)
Input: D, C ∈ {0, 1}, B, K, w(1) ← (1/p, . . . , 1/p)
1 (1) for k ← 1 to K do
2 Fit RF (w(k)) on D
3 v(k) ← Gini Importance of RF (w(k))
4 w(k+1) ← v(k)
5 end
6 (2) for b← 1 to B do
7 Generate bootstrap samples D(b) from D
8 Fit RF (w(K)) on D(b)
9 R(b) ← interaction rule set as defined in (2)
10 S(b) ← RIT(R(b), C)
11 end
12 (3) for S ∈ ∪Bb=1S(b) do
13 sta(S) = (1/B)
∑B
b=1 1
[
S ∈ S(b)
]
14 end
Output: {S, sta(S)}S∈∪B
b=1
S(b)
Output: {RF (w(K)) on D}
2.3 Tuning parameter selection in iRF
iRF inherits the tuning parameters associated with its two base algorithms, RF and RIT. The predictive
performance of RF is known to be highly resistant to the choice of tuning parameters (Breiman, 2001), so
we use the default RF parameters in the R randomForest package. Specifically, we set the number of trees
ntree = 500, the number of variables sampled at each node mtry =
√
p, and grow trees to purity. For
the RIT algorithm, we use the basic version or Algorithm 1 of (Shah and Meinshausen, 2014), and grow
4
D
1
D
1
D
1
D(1)
1
D(B)
1
Figure 1: iRF workflow. Feature-weighted RF (blue boxes) are trained on full data D and pass Gini
importance as weights to the next iteration. In iteration K (red box), feature-weighted RF are grown using
w(K) on B bootstrap samples of the full data D(1), . . . ,D(B). Decision paths and predicted leaf node labels
are passed to RIT (green box) which computes prevalent interactions. Recovered interactions are scored for
stability across (outer-layer) bootstrap samples.
5
M = 100 intersection trees of depth D = 5 with nchild = 2. In addition to the tuning parameters of RF and
RIT, the iRF workflow introduces two additional tuning parameters — (i) number of iterations K, and (ii)
number of bootstrap samples B. For the numerical and real datasets analyzed in this paper, we found that
results are fairly stable for K = 3, 4, 5, and B ∈ (10, 100). A data-driven method for selecting the number of
iterations K can be formulated by assessing the estimation stability (Yu, 2013) of the learner for different
values of K, although we do not explore the idea in this paper.
3 Simulation Experiments
We developed and tested iRF through extensive simulation studies based on biologically inspired generative
models using both synthetic and real data (SI). In particular, we generated responses using Boolean rules
intended to reflect the stereospecific nature of interactions among biomolecules (Nelson et al., 2008). In total,
we considered 8 generative models built from AND, OR, and XOR rules, with number of observations and
features ranging from 100 to 5000 and 50 to 2500 respectively. We introduced noise into our models both by
randomly swapping response labels for up to 40% of observations and through RF-derived rules learned on
held out data.
We find that iRF (K = 5) tends to exhibit comparable predictive performance to RF (K = 1) as
measured by area under the ROC curve, but in some settings has up to a 13% improvement on average over
20 simulation replicates. Moreover, we find that iRF (K > 1) recovers the full data generating rule, up to
an order-8 interaction in our simulations, as the most stable interaction in many settings where RF (K = 1)
rarely recovers interactions of order > 2.
Our experiments suggest that iterative re-weighting encourages iRF to use a stable set of features on
decision paths (SI Figures 4, 7). Specifically, features that are identified as important in early iterations
tend to be selected earlier on decision paths in later iterations (SI Figure 5). This allows iRF to consistently
generate partitions of the feature space where marginally unimportant active features become conditionally
important, and thus more likely to be selected on decision paths. For a full description of simulations and
results, see SI.
4 Case Study I: Enhancer elements in Drosophila
Development and function in multicellular organisms rely on precisely regulated spatio-temporal gene expres-
sion. During embryogenesis, this regulation manifests itself through “patterned” gene expression, whereby
genes exhibit unique spatial expression patterns. Enhancers play a critical role in this process, coordinating
combinatorial TF binding, whose integrated activity leads to differentiated gene expression over time and
space (Levine, 2010).
In the early Drosophila embryo, a small cohort of ∼40 TFs drive patterned gene expression and organ
differentiation (for a review see (Rivera-Pomar and Jãckle, 1996)). Hence, Drosophila development offers a
well-studied, simplified model system in which to investigate the intricate relationship between TF binding
and enhancer activities. Extensive work in this system has resulted in genome-wide, quantitative maps
of DNA occupancy for 23 of the TFs involved in body patterning of the early (pre-gastrula blastoderm)
fly embryo (ChIP-seq and ChIP-chip data (MacArthur et al., 2009), http://bdtnp.lbl.gov/), as well as
genome-wide profiles (ChIP-seq) for 13 histone modifications, and DNase-seq data to provide information
on chromatin state and DNA accessibility (ENCODE Project Consortium, 2012).
To investigate the relationship between enhancers, TF binding, and chromatin state, we ran iRF on this
data to predict enhancer status for 7987 (3994 training, 3993 test) genomic sequences. See SI for descriptions
of data collection and preprocessing. We achieve a stable AUC on the held out test data of 0.82 in the first
two iRF iterations and 0.83 in the third (Figure 2A). iRF (K = 3) identified several third order interactions
between TFs that were not identified with iRF (K = 1), as well as many additional second order interactions
(Figure 2B), including the well-known interactions between giant (gt), kruppel (kr), and hunchback (hb):
gt− kr (stability score 1.0); gt−hb (stability score 0.57); hb− kr (stability score 0.5). We also re-discovered
the central role of the early regulatory factor zelda (zld), which had previously been shown to be essential
6
for establishing patterns of zygotic transcription during the maternal-zygotic transition (Liang et al., 2008;
Harrison et al., 2011), as well as the importance of H3K18ac in the early system (Li et al., 2014; Schulz
et al., 2015). It is interesting to note that zld binding in isolation rarely drives enhancer activity, but in the
presence of at least one other factor, particularly the anterior-posterior (AP) patterning factor gt, it is highly
likely to induce transcription. That is, we find compelling evidence to support that DNA occupation by zld
alone is not sufficient to convey enhancer activity in the early blastoderm. This generalizes the observation
of the dependence of bcd-induced transcription on zld binding to several of the AP factors (Xu et al., 2014),
and is broadly consistent with the idea that zld is potentiating, rather than an activating factor, requiring
additional inputs from other TFs (Foo et al., 2014).
More broadly, the response surface associated with stable TF-TF interactions reveals that the probability
a sequence will act as an enhancer in the early embryo admits an AND-like structure. That is, DNA
elements exhibiting high levels of occupancy by a single factor have around four-fold lower probability of
being active enhancers compared to sequences bound at high occupancy by pairs of the TFs iRF identifies
as interacting. Accounting for third-order interactions, e.g. kr-gt-zld (stability score 0.53), reveals stronger
and more complex non-linear dependence between these features (Figure 2C,D). The effect size of each of
these interactions can be assessed on completely held-out test-data, and indeed the response surfaced plotted
in Figure 2C,D are generated from the held-out test data, rather than the training set. While overlapping
patterns of TF binding have been previously reported (MacArthur et al., 2009), to the best of our knowledge
this is the first report of an AND-like response surface for enhancer activation. Third-order interactions have
been studied in only a handful of enhancer elements, most notably eve stripe 2 (for a review see (Levine,
2013)), and our results indicate that they are broadly important for the establishment of early zygotic
transcription, and therefore body patterning.
5 Case Study II: Alternative Splicing in Human
In eukaryotes, alternative splicing of primary mRNA transcripts is a highly regulated process in which
multiple distinct mRNAs are produced by the same gene. In the case of messenger RNAs (mRNAs), the
result of this process is the diversification of the proteome, and hence the library of functional molecules
in cells. The structure of the spliceosome, the ribonucleoprotein responsible for most splicing in eukaryotic
genomes, has been characterized (for a review see (Sperling, 2016)). Its activity is context-dependent and
driven by complex, cell type specific interactions with cohorts of RNA binding proteins (RBP) (So et al.,
2016; Stoiber et al., 2015), suggesting that high-order interactions play an important role in the regulation of
alternative splicing in metazoans. However, our understanding of this system derives from decades of study
in genetics, biochemistry, and structural biology. As with transcriptional regulation, learning interactions
directly from genomics data has the potential to accelerate our pace of discovery in the study of co- and
post-transcriptional gene regulation.
Studies, initially in model organisms, have revealed that the chromatin mark H3K36me3, the DNA
binding protein CTCF, and a few other factors all play splice-enhancing roles (Kolasinska-Zwierz et al.,
2009; Sims Iii and Reinberg, 2009; Kornblihtt, 2012). However, the extent to which chromatin state and
DNA binding factors interact en masse to modulate co-transcriptional splicing remains unknown. To identify
interactions that form the basis of chromatin mediated splicing, we used iRF to predict thresholded splicing
rates for 23823 exons (RNA-seq Percent-spliced-in (PSI) values (Pervouchine et al., 2016); 11911 train, 11912
test), from ChIPseq assays measuring enrichment of chromatin marks and TF binding events (253 ChIP
assays on 107 unique transcription factors and 11 histone modifications, https://www.encodeproject.org/).
Preprocessing methods are described in the SI.
In this prediction problem, we achieve an AUC on the held out test data of 0.72 in the first iterations
and 0.78 in the second and third (Figure 3A). We find interactions involving H3K36me3, as expected, a
number of novel interactions involving other chromatin marks, and post-translationally modified states of
RNA Pol II (Figure 3B). In particular, the impact of serine 2 phosphorylation of Pol II is highly dependent
on local chromatin state. Remarkably, iRF identified two interactions of order 6 surrounding H3K36me3 and
S2 phospho-Pol II (stability scores 0.47) along with two highly stable order 5 subsets of these interactions
(stability score 1.0). Figure 3C shows a superheat map (Barter and Yu, 2015) for one of these interactions,
7
A B
input_twi
H3K9ac_gt_twi
bcd_gt
H4K5ac_twi
H3K4me3_kr_twi
D_gt
H3K27me3_twi
H3K18ac_ZLD
H3_twi
H3K27ac_gt_twi
H3K4me1_twi
hb_kr
H3K4me3_gt_twi
H3K18ac_kr_twi
ZLD_gt_kr
H3K36me3_kr_twi
kr_med
gt_hb
H3K36me3_gt_twi
D_twi
ZLD_kr_twi
hb_twi
H3K18ac_gt_twi
ZLD_gt_twi
gt_kr_twi
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.4 0.5 0.6 0.7 0.8 0.9
Enhancer data interactions
stability score
●
●
2−order
3−order
C D
gt
0
zld
10
20
30
40
50
60
0.2 0.4 0.6
0.8
0.8
0.6
0.4
0.2
gt
zld
0
10
20
30
40
50
60
0.2 0.4 0.6
0.8
0.8
0.6
0.4
0.2
kr low kr high
zld zld
t gt quantile
Figure 2: [A]: Accuracy of iRF in predicting active enhancers from TF and histone modification data.
[B]: iRF interaction stability scores after 3 iterations. Interactions that are a strict subset of other recovered
interactions have been removed for cleaner visualization. iRF recovers known interactions among transcrip-
tion factors gt, kr and hb and recently discovered interacting roles of master regulator zld. [C]: Surface
maps demonstrating proportion of active enhancers by levels of zld, gt, and kr binding (held-out test data).
Structures of the response surface indicates an AND-type interaction. [D]: Superheat map showing quantiles
of zld, gt, and kr binding by enhancer status. The block of active enhancers with elevated levels of zld, gt,
and kr binding reflects the AND-type interaction in C for a subset of active enhancers.
8
between POL II, S2 phospho-Pol II, H3K36me3, H3K79me2, H3K9me1, and H4K20me1. A subset of highly
spliced exons are the only observations enriched for all these elements, indicating a potential AND-type rule
related to splicing events (Figure 3C). This observation is consistent with, and offers a quantitative model
for the previously reported predominance of co-transcriptional splicing in this cell line (Tilgner et al., 2012).
We note that the search space of interactions of order 6 is very large, (> 1011), and that this interaction
is discovered at precisely the same computational cost as main effects, an order-zero increase over RF.
Recovering such interactions without exponential speed penalties represents a substantial advantage over
previous methods and positions our approach uniquely for the discovery of complex, non-linear interactions.
6 Discussion
Systems governed by nonlinear interactions are ubiquitous in biology. We focused on developing a predictive
and stable method, iRF, for learning such interactions among features. It was motivated by the discovery
of bimolecular interactions, specifically in transcriptional and co- and post-transcriptional regulation. iRF
identified known and novel interactions in early zygotic enhancer activation in the Drosophila embryo, and
posits new high-order interactions in splicing regulation for a human-derived system.
Validation and assessment of complex interactions in biological systems is necessary and challenging, but
new wet-lab tools are becoming available for targeted genome and epigenome engineering. For instance, the
CRISPR system has been adjusted for targeted manipulation of post-translational modifications to histones
(Hilton et al., 2015). This may allow for tests to determine if modifications to distinct residues at multivalent
nucleosomes function in a non-additive fashion in splicing regulation. Several of the histone marks that
appear in the interactions we report, including H3K36me3 and H4K20me1, have been previously identified
(Hallmann et al., 1998) as essential for establishing splicing patterns in the early embryo. Our findings point
to direct interactions between these two distinct marks. This observation generates interesting questions:
What proteins, if any, mediate these dependencies? What is the role of Phospho-S2 Pol II in the interaction?
Proteomics on ChIP samples may help reveal the complete set of factors involved in these processes, and new
assays such as Co-ChIP may enable the mapping of multiple histone marks at single-nucleosome resolution
(Weiner et al., 2016).
We have offered evidence that iRF constitutes a useful tool for generating new hypotheses from the study
of high-throughput genomics data, but many challenges await. iRF currently handles data heterogeneity
only implicitly, and the order of detectable interaction depends directly on the depth of the tree, which
is on the order of log2(n). We are currently investigating local importance measures to explicitly identify
the relative importance of discovered interactions for specific observations. This strategy has the potential
to further localize feature selection and improve the interpretability of discovered rules. Additionally, iRF
does not distinguish between interaction forms, for instance additive versus non-additive. We are exploring
tests of rule structure to provide better insights into the precise form of rule-response relationships. Finally,
although we use iRF to generate surface maps naively from the data in this paper, our on-going work makes
better use of the information stored in the underlying ensemble to decode the density estimate implicit in
the smoothed feature splits.
To date, machine learning has been driven largely by the need for accurate prediction. Leveraging machine
learning algorithms for scientific insights into the mechanics that underlie natural and artificial systems will
require an understanding of why prediction is possible. The Stability Principle has been advocated in (Yu,
2013) as a second consideration, in addition to predictability as a goodness-of-fit metric, to work towards
understanding and interpretablity in science. Iterative and data-adaptive regularization procedures such as
iRF are based on prediction and stability and have the potential to be widely adaptable to diverse algorithmic
and computational architectures, improving interpretability and informativeness by increasing the stability
of learners.
9
A B
POLR2A_POLR2AphosphoS2_H3K36me3_H3K9me1_H3K9me3
POLR2A_H3K36me3_H3K9ac
H3K36me3_H3K4me1_H3K79me2_H3K9me1
POLR2AphosphoS2_H3K27ac_H3K36me3_H3K9me3
POLR2A_POLR2AphosphoS2_H3K36me3_H3K79me2_H3K9me3_H4K20me1
POLR2A_POLR2AphosphoS2_H3K36me3_H3K79me2_H3K9me1_H4K20me1
POLR2A_POLR2AphosphoS2_H3K36me3_H3K4me1
POLR2AphosphoS2_H3K36me3_H3K9me1_H3K9me3_H4K20me1
POLR2AphosphoS2_H3K36me3_H3K4me1_H4K20me1
POLR2AphosphoS2_H3K27ac_H3K36me3_H3K9me1
POLR2A_H3K36me3_H3K4me3_H3K79me2
POLR2A_H3K36me3_H3K9me1_H3K9me3_H4K20me1
H3K27ac_H3K36me3_H3K79me2_H3K9me3
POLR2A_POLR2AphosphoS2_H3K27ac_H3K36me3_H3K79me2
H3K36me3_H3K4me3_H4K20me1
H3K36me3_H3K4me1_H3K9me3
H3K27ac_H3K36me3_H3K79me2_H3K9me1
POLR2A_H3K36me3_H3K4me1_H3K79me2
POLR2A_H3K27ac_H3K36me3_H4K20me1
H3K36me3_H3K79me2_H3K9ac
POLR2AphosphoS2_H3K36me3_H3K4me3
POLR2AphosphoS2_H3K36me3_H3K79me2_H3K9me1_H3K9me3
H3K36me3_H3K79me2_H3K9me1_H3K9me3_H4K20me1
POLR2A_H3K36me3_H3K79me2_H3K9me1_H3K9me3
POLR2AphosphoS2_H3K36me3_H3K4me1_H3K79me2
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.5 0.6 0.7 0.8
Splicing data interactions
stability score
●
●
●
●
3−order
4−order
5−order
6−order
C
quantile
Figure 3: [A]: Accuracy of iRF in classifying included exons from excluded exons in held-out test data.
iRF shows 6% increase in AUROC over RF. [B]: stability scores for interactions among TF and histones
detected by iRF. Lower order interactions that are a strict subset of identified higher order interactions
have been removed for cleaner visualization. [C]: Superheat map of an order-6 interaction (stability score
0.47) recovered by iRF, indicating enrichment of each element in test set observations. The subset of Pol
II, S2 phospho-Pol II, H3K36me3, H3K79me2, and H4K20me1 was recovered as an order-5 interaction in all
bootstrap samples (stability score 1.0).
10
Acknowledgments
This research was supported in part by NHGRI grant U01HG007031, ARO grant W911NF-11-10114, the
ONR grant N00014-16-1-2664, the Center for Science and Information (CSoI) 4101-38048, the Department
of Energy Contract DE-AC02-05CH11231 the National Human Genome Research Institute (NHGRI) R00
HG006698, the Department of Energy contract no. (SBIR/STTR) Award DE-SC0017069, and the Depart-
ment of Energy contract no. DE-AC02-05CH11231. SB gratefully acknowledges the support of UC Berkeley
and LBNL, where he conducted most of his work on this paper as a postdoctoral scholar. We thank Peter
Bickel and Shamindra Shrotriya for helpful discussion and comments, the laboratory of Sue Celniker for
conducting experiments on enhancer elements, Taly Arbel for preparing Drosophila datasets, the ENCODE
Consortium for experiments in human-derived cell lines, and the laboratories of Roderic Guigo and Dmitri
Pourvechine for quantification of exon splicing rates.
11
Supporting Information Appendix
1 RIT algorithm (Shah and Meinshausen, 2014)
The basic version of the Random Intersection Tree (RIT) algorithm is presented below. For a complete
description, including analysis of computational complexity and theoretical guarantees, we refer readers to
the original paper (Shah and Meinshausen, 2014).
Algorithm 2: Random Intersection Trees (RIT) Shah and Meinshausen (2014)
Input: {(Zi, Ii);Zi ∈ {0, 1}, Ii ⊆ {1, . . . , p}}ni=1, C ∈ {0, 1}
Tuning Parameter: (D,M,nchild)
1 for tree m← 1 to M do
2 Let m be a tree of depth D, with each node j in levels 0, . . . , D − 1 having nchild children. Let J
denote the total number of nodes in the tree, and index the nodes such that for every parent
child pair, larger indices are assigned to the child than the parent. For each node j = 1, . . . , J , let
i(j) be a uniform sample from the set of class C observations {i : Zi = C}.
3 Set S1 = Ii(1).
4 for j = 2 to J do
5 Sj ← Ii(j) ∩ Spa(j)
6 end
7 return Sm = {Sj : depth(j) = d}
8 end
Output: S = ∪Mm=1Sm
2 Remarks on iRF
2.1 Iterative re-weighting
Our generalized RIT can be used with any Random Forest (RF) model, weighted or not. Iterative re-
weighting acts as a soft dimension reduction step, encouraging RF to select a stable set of features on
decision paths. This leads to improved recovery of high-order interactions in our numerical simulations and
in real data settings. Specifically, we find that for a fixed sample size and noise level in our simulations,
later iterations of iRF tend to assign larger stability scores to higher-order interactions. This finding also
holds for the enhancer and splicing data. Moreover, we find that in both our simulation and real data
settings, iteratively re-weighted RF achieve similar predictive accuracy compared with unweighted RF, and
can achieve better predictive accuracy when the underlying model is sparse.
2.2 Generalized RIT
The RIT algorithm could be generalized through any approach that selects active features from continuous
or categorical data. However, if the selection procedure is not carried out carefully, the interactions recovered
by RIT may not be meaningful. There are several reasons we use an RF-based approach. First, RFs are
empirically successful predictive models that provide a principled, data-driven approach to select active
features specific to each observation. Second, randomness inherent to tree ensembles offers a natural way to
generate multiple active index sets for each observation xi, making the representations more robust to small
data perturbations. Finally, our approach allows us to interpret (in a computational efficient manner given
by RIT) complex, high-order relationships that drive impressive predictive accuracy in RFs, granting new
insights into this widely used class of models.
2.3 Node sampling
In the generalized RIT step of iRF, we represent each observation i = 1, . . . , n by T rule-response pairs,
determined by the leaf nodes containing observation i in each tree t = 1, . . . , T of an RF. We accomplish
this by replicating each rule-response pair (Zjt , Ijt) in tree t based on the number of observations in the
1
corresponding leaf node. We view this as a natural representation of the observations in D, made more
robust to sampling perturbations through rules derived from bootstrap samples of D. Our representation is
equivalent to sampling rule response pairs (Zjt , Ijt) in RIT with probability proportional to the number of
observations that fall in the leaf node. However, one could sample or select a subset of leaf nodes based on
other properties such as homogeneity and/or predictive accuracy. We are exploring how different sampling
strategies impact recovered interactions in our ongoing work.
2.4 Bagging stability scores
iRF uses two layers of bootstrap sampling. The “inner” layer takes place when growing weighted RF. By
drawing a separate bootstrap sample from the input data before growing each tree, we can learn binary
representations of each observation xi that are more robust to small perturbations. The “outer” layer of
bootstrap sampling is used in the final iteration of iRF. Growing RF (w(K)) on different bootstrap samples
allows us to assess the stability, or uncertainty associated with the recovered interactions.
2.5 Regression and multiclass classification
We presented iRF in the binary classification setting, but our algorithm can be naturally extended to
multiclass or continuous responses. The requirement that responses are binary is only used for selecting a
subset of leaf nodes as input to our generalized RIT. In particular, for a given class C ∈ {0, 1}, iRF runs
RIT over decision paths whose corresponding leaf node predictions are equal to C. In the multiclass setting,
we select leaf nodes with predicted class or classes of interest as inputs to RIT. In the regression setting, we
consider leaf nodes whose predictions fall within a range of interest as inputs to generalized RIT. This range
could be determined in domain-specific manner or by grouping responses using some clustering method.
2.6 Grouped features and replicate assays
In many classification and regression problems with omics data, one faces the problem of drawing conclusion
at an aggregated level of the features at hand. The simplest example is the presence of multiple replicates
of a single assay, when there is neither a standard protocol to choose one assay over the other, nor a known
strategy to aggregate the assays after normalizing them individually. Similar situations arise when there are
multiple genes from a single pathway in the feature sets, and one is only interested in learning interactions
among the pathways and not the individual genes.
In linear regression based feature selection methods like Lasso, grouping information among features is
usually incorporated by devising suitable grouped penalties, which requires solving new optimization prob-
lems. The invariance property of RF to monotone transformations of features and the nature of intersection
operation provide iRF a simple and computationally efficient workaround to this issue. In particular, one
uses all the unnormalized assays in the tree growing procedure, and collapses the grouped features or repli-
cates into a “super feature” before taking random intersections. iRF then provides interaction information
among these super features, which could be used to achieve further dimension reduction of the interaction
search space.
2.7 Interaction validation through prediction
We view the task of identifying candidate, high-order interactions as a step towards automated hypothesis
generation for systems whose behavior is characterized by more than the sum of their parts. An important
step in this process will be validating interactions recovered by iRF in a domain-specific manner to determine
whether they represent relevant hypotheses. This is an interesting and challenging problem that requires
subject matter knowledge into the anticipated forms of interactions. For instance, biomolecules are believed
to interact in stereospecific groups (Nelson et al., 2008) that can be represented through Boolean-type rules.
Thus, tests of non-additivity may provide insight into which iRF-recovered interactions warrant further
examination in biological systems.
2
We do not consider domain specific evaluation here, but instead propose broadly applicable metrics of
assessment based on both stability and predictability. We have incorporated the Stability Principle (Yu,
2013) through both iterative re-weighting, which encourages iRF to use a consistent set of features along
decision paths, and through bagged stability scores, which provide a metric to asses how consistently decision
rules are used throughout an RF. Here, we propose two additional validation metrics based on notions of
predictability.
Conditional prediction: Our first metric evaluates a recovered interaction S based on the predictive
accuracy of an RF that predicts response values using only leaf nodes for which all features in S fall on
the decision path. Specifically, for each observation i = 1, . . . , n we evaluate the prediction for each tree
t = 1, . . . T with respect to an interaction S as
ŷi(t;S) =
{
Zit if s ∈ Iit ,∀s ∈ S
Pn(y = 1) else
where Zit is the prediction of the leaf node containing observation i in tree t, and Iit is the index set of
features falling on the decision path for this leaf node, and Pn(y = 1) is the empirical proportion of class 1
observations. We average these predictions across the tree ensemble to obtain the RF-level prediction with
respect to an interaction S
ŷi(S) =
1
T
T∑
t=1
ŷi(t;S). (3)
Predictions from (3) can be used to evaluate predictive accuracy using any metric of interest. For instance,
we calculate AUC using predictions ŷi(S) for each interaction S ∈ S recovered by iRF. Intuitively, our
conditional prediction draws information from only the decision rules that rely on S, making a best-case
random guess otherwise.
Permutation based importance: Our second metric is inspired by Breiman’s permutation-based
measure of variable importance. In the single variable case, Breiman proposed permuting each column of
the feature matrix individually and evaluating the decrease in prediction accuracy of an RF. The intuition
behind this measure of importance is that if an RF’s predictions are heavily influenced by a particular
variable, permuting that feature will lead to a drop in predictive accuracy by destroying the feature/response
relationship. The direct analogue in our setting would be to permute all variables in a recovered interaction
S and evaluate the resulting drop in predictive accuracy of iRF. However, this permutation scheme does
not capture the notion that we expect variables in an interaction to act collectively. That is, by permuting
a single feature, we destroy the interaction/response relationship for any interaction that the feature takes
part in. If S contains features that are components of multiple, distinct interactions, permuting each variable
in S would destroy multiple interaction/response relationships. To avoid this issue, we propose a slightly
different notion of permutation importance that asks the question: how accurate are predictions using only
the variables contained in S? We answer this question by permuting all variables in Sc and evaluating
predictive accuracy of the fitted RF.
Let XπSc ∈ Rn×p denote the feature matrix where all columns in Sc have been permuted. We evaluate
predictions ŷ(XπSc ) on permuted data, and use these predictions to assess accuracy with respect to a metric
of interest, such as the AUC, which we denote as AUC(ŷ(XπSc )). In general, one would expect higher-order
interactions to be scored as more important under this metric since they use a larger number of features to
make predictions. Thus, we report a normalized version
2 · (AUC(ŷ(XπSc ))− 0.5)
|S|
that maps AUC(ŷ(XπSc )) to the interval [0, 1], and scales by interaction order. Intuitively, this metric
captures the idea that if an interaction is important independently of any other features, making predictions
using only this interaction should lead to improved prediction over random guessing.
3
Validating enhancer and splicing interactions: Figure S1 presents interactions from both the
enhancer and splicing data, evaluated in terms of our predictive metrics. In the enhancer data, interactions
between collections of TFs gt, hb, kr, and twi are ranked highly, as was the case with stability scores
(Figure S1A,C). In the splicing data, interactions among POL II, S2 phospho-Pol II, H3K36me3, H3K79me2,
H3K9me1, and H4K20me1 consistently appear in highly ranked interactions, providing further validation
of the order-6 interaction recovered using the stability score metric (Figure S1B,D). Comparing predictive
importance measures against stability scores suggests that there is some degree of similarity, reflected by the
moderate Spearman correlations between each (Figure S1E,F). Importantly though, each metric appears to
capture a different notion of importance. Taking advantage of different views, as well as other domain-specific
measures of importance, will be critical to identifying relevant, testable hypotheses.
3 Data processing
3.1 Drosophila Enhancers
In total, 7987 genomic sequences have been evaluated for their enhancer activity in a gold-standard, stable-
integration transgenic assay. In this setting, a short genomic sequence (100-3000nt) is placed in a reporter
construct and integrated into a targeted site in the genome. The resulting fly line is amplified, embryos are
collected, stained (for the reporter), and imaged to determine: a) whether or not the genomic segment is
sufficient to drive transcription of the reporter construct, and b) where and when in the embryo expression
is driven (Frise et al., 2010; Hammonds et al., 2013; Kvon et al., 2014). Sequences that drive patterned
expression are subsequently labeled as active enhancers (positive), or non-enhancers (negative). Because
the sequences are genomic DNA, and because we have ChIP-seq and ChIP-chip data for features of interest
on all genomic sequences, we encounter a binary classification problem. Approximately 10% of genomic
sequences tested were positive enhancers. However, it is important to note that the tested sequences do not
represent a random sample from the genome — rather they were chosen based on prior biological knowledge
and may therefore exhibit a higher frequency of positive tests than one would expect from genomic sequences
in general.
For each ChIP assay, we computed the maximum value over sliding windows of lengths 200, 500 and
1000 nucleotides. The windowing approach is useful because element lengths vary from 400 nt to > 3000 nt,
and it is likely that for many elements only a small sub-sequence drives patterned expression. We used these
measurements to form a set of features for predicting the enhancer activity status (active or not) of each
genomic sequence using iRF. We randomly divided the dataset into balanced training and test sets of 3994
and 3993 observations respectively, with approximately equal portions of positive and negative elements,
and applied iRF with B = 30, K = 3. The tuning parameters in RF were set to default levels of the R
randomForest package, and 100 Random Intersection Trees of depth 5 with nchild = 2 were grown to capture
candidate interactions.
3.2 Alternative Splicing
The ENCODE consortium has collected extensive genome-wide data on both chromatin state and splicing in
the human-derived erythroleukemia cell line K562 (ENCODE Project Consortium, 2012). To identify critical
interactions that form the basis of chromatin mediated splicing, we used splicing rates (Percent-spliced-in,
PSI values, Pervouchine et al. (2016)) from ENCODE RNA-seq data, along with ChIP-seq assays measuring
enrichment of chromatin marks and transcription factor binding events (253 ChIP assays on 107 unique
transcription factors and 11 histone modifications, https://www.encodeproject.org/).
For each ChIP assay, we computed the maximum value over the genomic region corresponding to each
exon. This yielded a set of p = 270 features for our analysis. We took our response to be a thresholded
function of the PSI values for each exon. Only internal exons with high read count (at least 100 rpkm)
were used in downstream analysis. Exons with Percent-spliced-in index (PSI) above 70% were classified as
frequently included (Y = 1) and exons with PSI below 30% were classified as frequently excluded exons (Y
= 0). This led to a total of 24535 exons, among which 3143 were excluded. We randomly divided the dataset
4
into balanced training and test sets and applied iRF with B = 30 and K = 3. The tuning parameters in
random forest were set to default and 100 binary random intersection trees of depth 5 were grown to capture
candidate interactions.
4 Simulations experiments
We developed iRF through extensive simulation studies based on biologically inspired generative models
using both synthetic and real data. In particular, we generated responses using Boolean rules intended to
reflect the stereospecific nature of interactions among biomolecules (Nelson et al., 2008). In this section, we
examine interaction recovery and predictive accuracy of iRF in a variety of simulation settings.
For all simulations, we evaluated predictive accuracy in terms of area under the ROC curve (AUC) for
a held out test set of 500 observations. We use three metrics to evaluate interaction recovery that account
for the notion that subsets of the full data generating interactions are informative of the underlying model.
These metrics are intended to give a broad sense of the overall quality of interactions S recovered by iRF.
For responses generated through an interaction over an active set S∗, we consider interactions of any order
between only active variables {j : j ∈ S∗} to be true positives and interactions containing any non-active
variable {j : j /∈ S∗} to be a false positives. We note that under this strict definition, interactions that are
comprised of mostly, but not all, active features are considered false positives.
1. Interaction AUC: We consider the AUC generated by thresholding interactions recovered by iRF
at each unique stability score. This metric provides a general, rank-based measurement of the over-
all quality of iRF interaction stability scores, reflecting both true and false positive rates at various
threshold settings.
2. Recovery rate: We define an interaction as “recovered” if it is returned by our generalized RIT in
any of the B bootstrap samples (i.e. stability score > 0) to eliminate the need for threshold selection.
We also consider an interaction to be recovered if it is a subset of any interaction recovered under the
previous definition. For a given order s = 2, . . . , |S|, we calculate the proportion of the total
(|S|
s
)
true
positive order-s interactions recovered by iRF. This metric provides a way to distinguish models in
settings where all consistently recover low-order interactions but only some consistently recover high
order interactions.
3. False positive weight: Let S = S(T ) ∪ S(F ) represent the set of interactions recovered by iRF, with
S(T ) and S(F ) being the sets of true and false positive interactions repectively. For a given interaction
order s = 2, . . . , |S|, we calculate ∑
S∈S(F ):|S|=s sta(S)∑
S∈S:|S|=s sta(S)
.
This metric captures the degree to which iRF rates false positive interactions of a given order as stable
relative to all recovered interactions. As with our measurement of recovery rate, this metric includes
all recovered interactions (stability score > 0) to eliminate the need for threshold selection. It can be
thought of as the weighted analogue to false discovery proportion.
4.1 Simulation 1: Boolean rules
Our first set of simulations demonstrates the benefit of iterative re-weighting for a variety of Boolean-type
rules. We sampled features x = (x1, . . . , x50) from independent, standard Cauchy distributions to reflect
heavy-tailed data, and generated the binary response variable y from three rule settings (OR, AND, and
XOR) as
5
y = 1 [x1 > tOR |x2 > tOR |x3 > tOR |x4 > tOR] (4)
y =
4∏
i=1
1 [xi > tAND] (5)
y = 1
[
4∑
i=1
1(xi > tXOR) ≡ 1 (mod 2)
]
. (6)
We injected noise by swapping the labels for 20% of the responses selected at random. From a modeling
perspective, the rules (4), (5), and (6) give rise to non-additive main effects that can be represented as an
order-4 interaction between the active features x1, x2, x3, and x4.
For the AND and OR models, we set tOR = 3.2, tAND = −1 to ensure reasonable class balance, and
trained on samples of size n ∈ {100, 200, . . . , 500}. The XOR interaction is more difficult to recover due to
the lower marginal importance of active features relative to inactive features. To account for this, we set
tXOR = 1 to ensure that some active features were marginally important relative to inactive features, and
trained on larger samples of size n ∈ {200, 400, . . . , 1000}. We simulated from each model 20 times to asses
variability in both prediction accuracy and interaction recovery for iterations k ∈ {1, 2, . . . , 5} of iRF. The
tuning parameters of Random Forests were set to default levels for the R randomForest package (Liaw and
Wiener, 2002), M = 100 random intersection trees of depth 5 were grown with nchild = 2, and B = 20
bootstrap replicates were taken to determine the stability scores of recovered interactions.
Iterative re-weighting can be viewed as a form of regularization on the base RF learner, since it restricts the
form of functions RF is allowed to fit in a probabilistic manner. Figure S2A shows the prediction accuracy
of iRF in terms of test set AUC as we vary the degree of regularization through iteration number. iRF
achieves comparable or better predictive performance for increasing k, with the most dramatic improvement
in the XOR model. It is important to note that the rules (4), (5), and (6) are inherently sparse, and that
regularization induced by iterative re-weighting may hurt predictive performance in non-sparse settings.
Figure S2B shows the quality of interaction stability score rankings by iteration, demonstrating that iRF
(k > 1) tends to rank true interactions higher with respect to stability score than RF (k = 1), reflected
by the larger AUC for increasing k. Figure S2C breaks down recovery by interaction order, showing the
proportion of order-s interactions recovered across any bootstrap sample (stability score > 0), averaged
over 20 replicates. For each of the rules, RF (k = 1) never recovers the true order-4 interaction while iRF
k = 4, 5 always identifies it as the most stable order-4 interaction for large enough n. The improvement in
interaction recovery with iteration is accompanied by a modest increase in the stability scores of false positive
interactions (Figure S2D). We find that this increase is generally due to the fact that iRF recovers many
false interactions with low stability scores as opposed to few false interactions with high stability scores. As
a result, true positives can be easily distinguished due to their generally higher stability scores.
Our findings support the idea that iterative re-weighting is important for recovering high-order interac-
tions and does not hurt predictive performance when the underlying model is sparse. In particular, improved
interaction recovery with iteration indicates that iterative re-weighting stabilizes decision paths, leading to
more interpretable models. In settings where the data generating mechanism is not sparse, one may need to
consider trade-offs between interpretability and predictive accuracy, which can be controlled through itera-
tion number K. We note that a principled approach for selecting K can be formulated in terms of ESCV
(Lim and Yu, 2015).
4.2 Simulation 2: marginal importance
Section 4.1 demonstrates the advantage that iterative re-weighting can provide for recovering high-order
interactions. In the following set of simulations, we develop an intuition for how iRF constructs high-order
interactions, and under what conditions the algorithm fails. In particular, we find that iterative re-weighting
allows iRF to select marginally important active features earlier in decision paths. This leads to more
favorable partitions of the feature space, where active features that are marginally less important are more
likely to be selected.
6
We sampled features x = (x1, . . . , x100) from independent, standard Cauchy distributions, and generated
the binary response y as
y = 1
[ ∑
i∈SXOR
1(xi > tXOR) ≡ 1 (mod 2)
]
, (7)
SXOR = {1, . . . , 8}. We set tXOR = 2, which resulted in a mix of marginally important and unimportant
active features, allowing us to study how iRF builds up interactions. In the following simulations, we
investigate a variety of settings based off of the generative model in (7). For all simulations described in
this section, we generated n = 5, 000 training points and evaluated the fitted model on a test set of 500 held
out observations. RF parameters were set to their default values with the exception of ntree, which was
set to 200 for computational purposes. We ran iRF for k ∈ {1, . . . , 5} iterations with 10 bootstrap samples
and grew M = 100 random intersection trees of depth 5 with nchild = 2. Each simulation was replicated 10
times to evaluate performance stability.
4.2.1 Noise level
In the first simulation, we considered the effect of noise on interaction recovery to asses the underlying
difficulty of the problem and to develop an intuition for how iRF constructs high-order interactions. We gen-
erated responses using equation (7), and swapped labels for a proportion π ∈ {0.10, 0.15, 0.20} of randomly
selected responses. The range of π was selected to ensure comparable predictive accuracy in our simulations
and real data examples (enhancer: AUC = 0.83; splicing: AUC = 0.78). Figure S3 shows performance
in terms of predictive accuracy and interaction recovery. At each level of π, increasing k leads to superior
performance, though there is a substantial drop in both absolute performance and the rate of improvement
over iteration for increasing π.
The sharp transitions in interaction recovery, for instance at k = 2, π = 0.1 or k = 3, π = 0.15 (Figure
S3C), reinforce the idea that regularization is critical for recovering high-order interactions. Figure S4 shows
the distribution of iRF weights by iteration, which reflect the degree of regularization. iRF successfully
recovers the full XOR interaction in settings where there is clear separation between the distribution of
weights for active and inactive variables, indicating that a reasonable degree of regularization may be required
to recover high-order interactions. This separation develops over several iterations, and at a noticeably slower
rate for larger π, indicating that further iteration may be necessary in low signal-noise regimes.
Marginal importance and variable selection: iRF’s improvement with iteration suggests that the
algorithm leverages informative lower-order interactions to construct the full data generating rule through
adaptive regularization. That is, by re-weighting towards some active features, iRF is more likely to produce
partitions the feature space where remaining active variables are selected. To investigate this idea further,
we examined the relationship between marginal importance and the average depth at which features are
first selected across the forest. Here, we define a variable’s marginal importance as the best case decrease in
Gini impurity if it were selected as the first splitting feature. We note that this definition is different from
the standard measure of RF importance (mean decrease in Gini impurity), which captures an aggregate
measurement of marginal and conditional importance over an RF. We consider this particular definition
to examine whether iterative re-weighting leads to more “favorable” partitions of the feature space, where
marginally unimportant features are selected earlier on decision paths.
Figure S5 shows the relationship between marginal importance and depth at which features are first
selected for a single replicate of our simulation. On average over the tree ensemble, active features enter the
model earlier with further iteration, particularly in settings where iRF successfully recovers the full XOR
interaction. We note that this occurs for active features with both high and low marginal importance, though
more marginally important, active features enter the model earliest. This behavior supports the idea that
iRF constructs high-order interactions by identifying a core set of active features, and using these, partitions
the feature space in a way that marginally less important variables become conditionally important, and
thus more likely to be selected.
7
4.2.2 Mixture model
Our finding that iterative-reweighting helps iRF to build up interactions around marginally important fea-
tures, suggests that the algorithm may struggle to recover interactions when other features are considerably
more marginally important. To test this idea, we considered a mixture model of XOR and AND rules.
Responses were drawn from equation (7) with probability π ∈ {0.5, 0.75, 0.9}, and were generated as
y =
∏
i∈SAND
1 [xi > tAND] (8)
with probability 1−π. We introduced noise by swapping labels for 10% of the responses selected at random,
a setting where iRF easily recovers the full XOR rule, and set SAND = {9, 10, 11, 12}, tAND = −0.5 to ensure
that the XOR and AND interactions were dominant with respect to marginal importance for π = 0.9 and
π = 0.5 respectively.
Figure S6 shows performance in terms of predictive accuracy (A) and interaction recovery of XOR (B)
and AND (C) rules at each level of π. When one rule is clearly dominant (AND: π = 0.5; XOR: π = 0.9),
iRF fails recover the the other (Figure S6 B,C). The distribution of variable weights by iteration indicates
that iRF re-weights exclusively towards the dominant rule in these settings (Figure S7). This is driven by
the fact that the algorithm iteratively updates feature weights using a global measure of importance, without
distinguishing between features that are more important for certain observations and/or in specific regions
of the feature space. One could address this with local measures of feature importance, though we do not
explore the idea in this paper.
In the π = 0.75 setting, none of the interactions are clearly dominant, and as a result, iRF re-weights
towards both XOR and AND variables (Figure S7). By iteration k = 5, the distribution of weights for active
and inactive variables begins to separate, similar to the weight distribution for noisy settings (Figure S4).
As the weights differentiate between active and inactive variables, iRF recovers subsets of both the XOR
and AND interactions (Figure S6). While iRF may recover a larger proportion of each rule with further
iteration, we note that iRF does not explicitly distinguish between rule types, and would do so only when
different decision paths in an RF learn the distinct rules. Characterizing the specific form of interactions
recovered by iRF is an interesting question that we are exploring in our ongoing work.
4.2.3 Correlated features
In our next set of simulations, we examined the effect of correlated features on interaction recovery. Responses
were generated using equation (7), with features x = (x1, . . . , x100) drawn from a Cauchy distribution with
mean 0 and covariance Σ, and active set SXOR, |SXOR| = 8 uniformly sampled from {1, . . . , 100}. We
considered both a decaying covariance structure, Σij = ρ
|i−j|, and a block diagonal covariance structure,
with blocks of size 10, diagonal 1, and off diagonal elements in the same block set to ρ, ρ ∈ {0.25, 0.5, 0.75}.
Prediction accuracy and interaction recovery are fairly consistent for moderate values of ρ (Figures S8,
S9), while interaction recovery degrades for larger values of ρ, particularly in the block covariance setting
(Figure S9B,C). For instance when ρ = 0.75, iRF only recovers the full order-8 interaction at k = 5, and
simultaneously recovers many more false positive interactions. The drop in interaction recovery rate is much
greater for larger interactions due to the fact that for increasing ρ, inactive features are more frequently
selected in place of active features. These findings suggest both that iRF can recover meaningful interactions
in highly correlated data, but that these interactions may also contain an increasing proportion of false
positive features.
We note that the problem of distinguishing between many highly correlated features, as in the ρ = 0.75
block covariance setting, is difficult for any feature selection method. With a priori knowledge about the
relationship between variables, such as whether variables represent replicate assays or components of the
same pathway, one could group features as described in SI 2.6.
8
4.3 Simulation 3: big p
Our final set of synthetic data simulations tests the performance of iRF in settings where the number of
features is large relative to the number of observations. This regime characterizes many of the biological
problems where we anticipate iRF being useful. Specifically, we drew 500 independent, p-dimensional stan-
dard Cauchy variables, with p ∈ {500, 1000, 2500}. Responses were generated using the 4-d AND interaction
from (5), selected to reflect the form of interactions recovered in the splicing and enhancer data. We in-
jected noise into responses by swapping labels for proportions π ∈ {0.1, 0.2, 0.3, 0.4} of randomly selected
observations.
Figures S10, S11, and S12 show prediction accuracy and interaction recovery of iRF for p = 500, 1000,
and 2500 respectively. iRF behaves similarly with respect to each metric for increasing p and fixed π. In
particular, prediction accuracy improves and stabilizes with iteration, especially at lower levels of π (Figures
S10A, S11A, and S12A). For k = 1, iRF rarely recovers correct interactions and never recovers interactions of
order > 2, while later iterations recover many of the true interactions at lower noise levels (π = 0.1, 0.2, 0.3;
Figures S10C, S11C, and S12C). iRF recovers many false positive order-2 interactions at higher noise levels
(π = 0.3, 0.4; Figures S10D, S11D, and S12D). However, at π = 0.3, the stability scores of these interactions
tend to be low relative to true positives, as reflected by the interaction AUC (Figures S10B, S11B, and
S12B).
Overall, increasing p has only a small impact on prediction accuracy and interaction recovery for these
simulations when k > 1. This suggests that iterative re-weighting is effectively regularizing the RF fitting
procedure in this highly sparse setting. Based on the results from our previous simulations, we note that the
the effectiveness iterative re-weighting will be related to the form of interactions. In particular, iRF should
perform worse in settings where p >> n and interactions have few or no marginally important features since
adaptive regularization is less effective in such a regime.
4.4 Simulation 4: enhancer data
To test iRF’s ability to recover interactions in real data, we incorporated biologically inspired Boolean rules
into the Drosophila enhancer dataset. These simulations were motivated by our desire to assess iRF’s ability
to recover signals embedded in a noisy, nonsmooth and realistic response surface generated by correlated
features, as opposed to our earlier simulations with i.i.d. Cauchy features. Specifically, we used all TF
binding features from the enhancer data and embedded a 5-dimensional AND rule between krüppel, (kr),
hunchback (hb), Dichaete (D), twist (twi), and zelda (zld):
y = 1[xkr > 1.25 &xhb > 1.5 &xD > 1.25 &xtwi > 1 &xzld > 2].
The active TFs and thresholds were selected to ensure that the proportion of positive responses was com-
parable to the true data, and the form of the interaction was selected to match the form of interactions
recovered in both the enhancer and splicing data.
In this set of simulations, we considered two types of noise. For the first, we incorporated noise by
swapping labels for a randomly selected subset of 20% of the observations, which resulted in comparable
predictive accuracy to our enhancer and splicing data examples. Our second noise setting was based on an
RF/sample splitting procedure. Specifically, we divided the data into two disjoint groups of equal size. For
each group, we trained an RF and used it to predict the responses of observations in the held out group. This
process resulted in predicted class probabilities for each observation i = 1, . . . , n. To account for the highly
heterogeneous nature of the enhancer data, we repeated this procedure 20 times to obtain the maximum
predicted probability that yi = 1, denoted pi. For each observation, we sampled a Bernoulli noising variable
ỹi ∼ Bernoulli(max{2 ∗ pi, 1}), where the rescaling and max operations were performed to account for the
low proportion of active enhancers. We used these noising variables to generate a binary response for each
observation
yi = max{ỹi,1[xkr > 1.25 &xhb > 1.5 &xD > 1.25 &xtwi > 1 &xzld > 2]}
Intuitively, this model derives its noise from rules learned by an RF. That is, feature interactions that are
useful for classifying observations in the split data are built into the predicted class probabilities pi. This
9
results in an underlying noise model that is heterogeneous, composed of many “bumps” throughout the
feature space.
In each setting, we trained on balanced samples of size n ∈ {200, 400, . . . , 1000} and tested prediction
performance on all held out data. We repeated this process 20 times to asses the variability in interaction
recovery and prediction accuracy. The tuning parameters of Random Forests were set to default levels for
the R randomForest package, M = 100 random intersection trees of depth 5 were grown with nchild = 2,
and B = 20 bootstrap replicates were taken to determine the stability scores of recovered interactions.
Figure S13A shows that different iterations of iRF achieve comparable predictive accuracy in both noise
settings. Interestingly, the overall quality of recovered interactions, as measured by interaction AUC, is
better in the swapped label noise setting, despite substantially worse predictive accuracy (Figure S13A,B).
This is due to the fact that in the RF noise setting, many low signal interactions were injected into responses
based on their association with actual enhancer status. Iterations k > 1 are noticeably better at recovering
high-order interactions for a fixed sample size, although there are substantial proportions of false discoveries
amongst these higher order interactions (Figure S13B,C,D). We note that these false discoveries are typically
comprised of mostly active features, and therefore still partially informative of the underlying data generating
mechanism. In both noise settings, there is a drop in the quality of recovered interactions for the largest
values of k, particularly for larger sample sizes (Figure S13). This finding re-emphasizes the need to develop
a principled approach, such as ESCV (Lim and Yu, 2015), for selecting K.
5 Computational cost of detecting high-order interaction
We use the enhancer data from our case studies to demonstrate the computational advantage of iRF for
detecting high-order interactions from high-dimensional data. Rulefit3 serves as a benchmark, which has
competitive prediction accuracy to RF and also comes with a flexible framework for detecting nonlinear
interactions hierarchically, using the so-called “H-statistic” (Friedman and Popescu, 2008). We find that for
moderate to large dimensional datasets typically encountered in omics studies, the computational complexity
of seeking high-order interactions hierarchically (select marginally important features first, then look for
pairwise interaction among them, and so on) increases rapidly, while the computation time of iRF grows far
more slowly with dimension.
We fit iRF and rulefit on balanced training samples from the enhancer dataset (7705 samples, 80 features)
using subsets of p randomly selected features, where p ∈ {10, 20, . . . , 80}. We ran rulefit with default param-
eters, generating null interaction models with 10 bootstrap samples and looked for higher order interactions
among features whose H-statistics are at least one null standard deviation above their null average (following
Friedman and Popescu (2008)). The current implementation of rulefit only allows H-statistic calculation for
interactions of up to order 3, so we do not assess higher order interactions. We run iRF with B = 10 boot-
strap samples, and the same tuning parameter specifications as described in the case studies section. The
run time (in minutes) and the AUC for different values of p, averaged over 10 replications of the experiment
by randomly permuting the original features in enhancer data, are reported in Figure S14.
The plot on the left panel shows that the runtime for rulefit’s interaction detection increases exponentially
as p increases, while the increase is linear for iRF. The search space of rulefit is restricted to all possible
interactions of order 3, while iRF searches for arbitrarily high-order interactions, leveraging the deep structure
decision trees in RF. The linear vs. polynomial growth of computing time is not an optimization issue, it is
merely an consequence of the exponentially growing search space of high-order interactions.
6 Case Study III: Ames Housing Sales
The iRF algorithm was motivated primarily by biological problems, but we anticipate that it will be
a useful tool for interpreting feature interactions in other areas as well. In this section, we consider a
dataset containing 80 features on 2930 sales of residential property in Ames, Iowa between 2006 and 2010
(http://ww2.amstat.org/). The straightforward interpretation of features from this dataset provides an in-
teresting and intuitive test case for exploring interactions recovered by iRF. We note that for this regression
10
problem, we sample from all leaf nodes in our generalized RIT instead of sampling from a class specific subset
as in the enhancer and splicing data. As a result, recovered interactions represent features that collectively
determine home value, whether low or high.
Of the 80 features included in the Ames dataset, several were not recorded for a large number of housing
sales. We removed any feature for which more than 100 sales were missing data as well as observations with
any missing features following this step, leaving a total of 2826 sales and 70 features (including sale price).
In addition to features that were missing data, several contained redundant information, particularly those
related to area (sqft) in different parts of a home. We combined measurements of “livable” area into a single
variable that included first story, second story, and finished basement total area (sqft) and removed variables
containing different aggregate measurements of above and below ground area. After setting aside sale price
as the response of interest, we were left with a total of 64 features.
We randomly divided the data into training and test sets of size 2119 (75%) and 707 (25%) and ran
iRF with K = 3 and B = 20. We set RF parameters to the standard R randomForest values, and grew
M = 100 random intersections trees of depth 5 with nchild = 2. Each of the three iRF iterations K = 1, 2, 3
achieved comparable predictive performance, explaining 88.9%, 88.1%, and 88.0% of the variance in sale
prices respectively.
The interactions recovered by iRF are shown in Figure S15A. Unsurprisingly, overall quality is particularly
important for predicting sale price. This ordinal feature provides an aggregate rating of the overall material
and finish of a home that ranges from very poor (1) to very excellent (10). Most of the interactions identified
by iRF encode relationships between overall quality and other features of the house (e.g. livable square
footage, year built, lot area).
We investigated the relationship between overall quality and other features and found that homes rated
higher for overall quality exhibit different sale price behavior than homes rated lower for overall quality.
Figures S15B,C show response surfaces of two third order interactions recovered by iRF that demonstrate
these differences. The response surfaces for livable area (sqft), lot size (sqft), and sale price (Figure S15B)
indicates that there is a stronger association between lot size and sale price, relative to the association
between livable area and sale price, in lower quality homes than in higher quality homes. This finding would
suggest that for lower quality homes, land plays a bigger role in final sale price. The response surfaces for
livable area (sqft), year built, and sale price (Figure S15C) shows that the sale price of high quality homes
appears to be fairly independent of year built. In contrast, lower quality homes of comparable size have a
stronger association between sale price and year built.
11
A B
H3K18ac_kr
gt_hb_kr
wt_H3_kr
ZLD_bcd_twi
H3_ZLD
H3K4me3_gt_twi
gt_med
kr_med
bcd_gt
H3K18ac_twi
H3K27ac_ZLD
hb_kr_twi
wt_H3_gt
bcd_kr_twi
med_twi
H3K4me3_kr
ZLD_bcd_kr
H3K4me1_kr
H4K5ac_kr_twi
H3K18ac_H4K5ac
ZLD_gt_kr_twi
H3K9ac_kr
gt_prdBQ
H4K5ac_gt_twi
H4K5ac_ZLD
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.816 0.818 0.820 0.822 0.824
Enhancer data interactions
conditional prediction
●
●
●
2−order
3−order
4−order POLR2AphosphoS2_H3K27ac_H3K36me3_H3K4me1
IRF1_POLR2AphosphoS2_H3K36me3_H3K9me3
POLR2A_POLR2AphosphoS2_H3K36me3_H3K9ac_H4K20me1
POLR2A_POLR2AphosphoS2_H3K36me3_H3K4me1
POLR2A_H3K27ac_H3K36me3_H3K9me3_H4K20me1
CEBPB_H3K36me3
POLR2A_POLR2AphosphoS2_H3K36me3_H3K4me3_H3K79me2
POLR2A_H3K27ac_H3K36me3_H3K79me2_H3K9me1
POLR2A_H3K27ac_H3K36me3_H3K4me1
POLR2A_H3K27ac_H3K36me3_H3K79me2_H3K9me3
POLR2A_POLR2AphosphoS2_H3K36me3_H3K79me2_H3K9ac
POLR2A_POLR2AphosphoS2_H3K27ac_H3K36me3_H3K9me1
H3K36me3_H3K4me2_H3K79me2
IRF1_H3K36me3_H3K79me2_H3K9me3
POLR2A_POLR2AphosphoS2_H3K27ac_H3K36me3_H3K9me3
POLR2AphosphoS2_H3K27ac_H3K36me3_H3K4me3
POLR2A_POLR2AphosphoS2_H3K27ac_H3K36me3_H3K79me2_H4K20me1
POLR2A_H3K36me3_H3K4me2
POLR2A_H3K27ac_H3K36me3_H3K4me3
POLR2A_H3K27ac_H3K36me3_H3K9ac
IRF1_H3K36me3_H4K20me1
POLR2A_IRF1_H3K36me3_H3K79me2
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.760 0.765 0.770 0.775
Splicing data interactions
conditional prediction
●
●
●
●
●
2−order
3−order
4−order
5−order
6−order
C D
ZLD_bcd_twi
H3K18ac_H3K27me3_twi
H3K18ac_H3K4me3_twi
H3K36me3_bcd_gt_twi
H3K18ac_D_twi
H3K4me3_gt_kr
H3_gt_kr_twi
H3K4me3_med_twi
H3K36me3_gt_kr_twi
H3K4me1_gt_kr_twi
H3K9ac_gt_kr_twi
H3K18ac_bcd_kr_twi
H3K27ac_gt_kr
H3K18ac_gt_kr_twi
gt_kr_med_twi
gt_hb_kr_twi
H3_bcd_twi
D_gt_twi
dl3_gt_twi
H3K4me3_gt_twi
H3K27ac_gt_twi
bcd_med_twi
H3K27me3_kr_twi
H4K5ac_kr_twi
gt_prd_twi
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.13 0.14 0.15 0.16
Enhancer data interactions
permutation importance
●
●
3−order
4−order POLR2A_H3K27ac_H3K36me3_H3K9ac
POLR2A_H3K27me3_H3K36me3_H4K20me1
H3K36me3_H3K4me3_H3K79me2_H3K9me1_H4K20me1
POLR2A_H3K27ac_H3K36me3_H3K4me3
H3K27me3_H3K36me3_H3K9me3
POLR2AphosphoS2_H3K36me3_H3K4me3_H3K79me2_H3K9me1
POLR2A_H3K36me3_H3K4me3_H3K79me2_H3K9me1
IRF1_H3K36me3_H3K79me2_H3K9me3
IRF1_H3K36me3_H4K20me1
CEBPB_POLR2AphosphoS2_H3K36me3
POLR2AphosphoS2_H3K36me3_H3K4me3_H3K79me2_H4K20me1
H3K36me3_H3K79me2_H3K9ac_H3K9me1
POLR2AphosphoS2_SPI1_H3K36me3
H3K27ac_H3K27me3_H3K36me3_H3K79me2
H3K36me3_H3K4me2_H4K20me1
POLR2A_H3K36me3_H3K4me3_H3K79me2_H4K20me1
H3K27me3_H3K36me3_H3K4me3
POLR2A_POLR2AphosphoS2_H3K36me3_H3K4me3_H3K79me2
POLR2A_CEBPB_H3K36me3_H3K79me2
POLR2A_H3K27me3_H3K36me3_H3K79me2
POLR2A_IRF1_H3K36me3_H3K79me2
POLR2A_H3K36me3_H3K4me2
POLR2A_SPI1_H3K36me3
H3K36me3_H3K4me2_H3K79me2
MYC_H3K36me3
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.08 0.09 0.10 0.11
Splicing data interactions
permutation importance
●
●
●
●
2−order
3−order
4−order
5−order
E F
stability score
0.05 0.10 0.15 0.20 0.25
● ● ● ● ●● ●
●●
● ●●
● ●●
●
● ●● ●
●
●
●
● ●
●●
●● ●●
● ●●●
● ●
●●● ●
●● ●
● ● ●
●●
●● ●●
●●
● ● ●●●
● ●●●●
● ● ●● ●●
●● ●● ●● ●● ●
●●●● ● ●●● ●
●●● ●●●● ●● ● ●●●
●● ●● ●● ● ●●●●● ● ●●● ● ●●
● ● ●● ●● ●● ●●●● ●● ● ●● ●●● ●●● ●● ●●● ●● ● ●● ●●● ●
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
●● ●● ●●
●●
●●●
● ●
●
●●●
●
●
●
●●
●●
●●● ●
●●● ●
●●
●● ● ●
●●
●● ●
●
● ●●●
● ●
● ●●●
●● ●● ●
●●●● ● ●
● ●●●●● ●●
● ●●● ●● ● ●
●●● ● ●●● ● ● ●
● ●●●●● ● ●●● ● ●●● ●● ●●
●●● ●●● ●●● ●●●● ●●● ●●● ● ●● ●●● ● ●● ●●● ● ●
0.
05
0.
10
0.
15
0.
20
0.
25
Correlation: 0.48 permutation importance
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
● ●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.0 0.2 0.4 0.6 0.8 1.0
Correlation: 0.65 Correlation: 0.24
0.76 0.78 0.80 0.82
0.
76
0.
78
0.
80
0.
82
conditional prediction
stability score
0.04 0.08 0.12
●● ● ●●●● ●● ●● ● ●● ●●● ●●● ●●
●● ●●
● ●● ● ●
●●
●●● ●●
●●●
●●●
● ●●●●
●●
●●● ●
●● ● ●
●●
●
●
●●
●●●
●●
● ●● ●
●●●●
●●
●● ●●●
●●● ●● ●
● ●● ●● ●
● ●●● ●●
●● ●●●●●
●●● ●●●● ●
●● ● ●●● ●●●
● ●●●●●●● ● ●●
●● ●●● ●●●● ●●● ●●●●●
●●●● ●● ● ●●● ●●● ●●●●●● ●●● ●● ●● ● ●●●
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
●●● ● ●●● ● ●● ●●● ● ● ● ●● ● ● ● ●● ●●
● ● ●● ●
● ●● ●●
●●
●● ● ●●
●●●
●●●
● ● ●● ●●
●●
●● ●
● ●● ●
●●
●
●
●●
●●●
●● ● ●
● ●● ●
● ● ●●
● ●●
●●●● ●
● ● ●●●● ●
●● ● ●● ●
● ●● ● ●● ●
●● ● ●●●
●●● ●●●
●●● ●●● ●●● ● ●
● ● ●● ● ●● ● ●●●●
●●●●● ●●● ●●●● ● ●●●● ●●● ● ●
● ●● ● ●●● ●●● ● ● ● ●● ●●● ●●● ●● ●● ●● ●● ●●●● ● ●●● ●● ● ●● ● ● ●
0.
04
0.
08
0.
12
Correlation: 0.56 permutation importance
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
●
●
●
●
● ●
●
●
● ●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ●
●
●●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
● ●
●
●
●
0.0 0.2 0.4 0.6 0.8 1.0
Correlation: 0.43 Correlation: 0.64
0.70 0.72 0.74 0.76 0.78
0.
70
0.
72
0.
74
0.
76
0.
78
conditional prediction
Figure S1: Prediction-based validation metrics for enhancer and splicing data. [A-D] Top 25 interactions
with respect to prediction based importance metrics. Low-order interactions that are a strict subset of some
higher-order interactions have been removed for clearer visualization. [A] Conditional prediction importance,
enhancer data; [B] conditional prediction importance, splicing data; [C] permutation prediction importance,
enhancer data; [C] permutation prediction importance, splicing data. [E-F] Scatter plot matrices showing
the relationship between stability and prediction based importance metrics by interaction order. Spearman
correlation between importance metrics is given in the bottom left.[E] Enhancer data; [F] splicing data.
12
A B
●
● ●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
rule: and rule: or rule: xor
100 300 500 100 300 500 200 600 1000
0.5
0.6
0.7
0.8
0.9
1.0
n
A
U
C
iter
1
2
3
4
5
●
●
●
●
● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ● ● ●
●
●
●
●
●
●
●
●
● ● ●
rule: and rule: or rule: xor
100 300 500 100 300 500 200 600 1000
0.5
0.6
0.7
0.8
0.9
1.0
n
in
te
ra
ct
io
n_
A
U
C
iter
1
2
3
4
5
C D
●
● ●
●
● ●
●
● ●
●
● ●
●
●
●
●
●
●
●
● ●
●
● ●
●
●
●
n: 500
rule: and
n: 500
rule: or
n: 1000
rule: xor
n: 300
rule: and
n: 300
rule: or
n: 600
rule: xor
n: 100
rule: and
n: 100
rule: or
n: 200
rule: xor
2 3 4 2 3 4 2 3 4
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
interaction_order
re
co
ve
ry
_r
at
e
iter
● 1
2
3
4
5
●
● ●
●
● ●
●
●
●
●
● ●
●
●
●
●
●
●
●
● ●
●
● ●
●
●
●
n: 500
rule: and
n: 500
rule: or
n: 1000
rule: xor
n: 300
rule: and
n: 300
rule: or
n: 600
rule: xor
n: 100
rule: and
n: 100
rule: or
n: 200
rule: xor
2 3 4 2 3 4 2 3 4
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
interaction_order
fa
ls
e_
po
si
tiv
e_
w
ei
gh
t
iter
● 1
2
3
4
5
Figure S2: iRF performance for order-4 AND, OR, and XOR rules over 20 replicates. Plots show results
for models trained with ntrain ∈ {100, 300, 500} in the AND and OR models and ntrain ∈ {200, 600, 1000}
in the XOR model. [A] Prediction accuracy (AUC) is comparable or better for increasing k. [B] Interaction
AUC improves with increasing k, achieving optimal AUC of 1 in all models for large k and n. [C] Recovery
rate for interactions of all orders improves with increasing k. In particular, k = 1 fails to recover any order-4
interactions. [D] False positive weight increases in settings where iRF recovers high-order interactions as a
result of many false positives with low stability scores.
13
A B
●
●
●
noise: 0.1 noise: 0.15 noise: 0.2
5000 5000 5000
0.5
0.6
0.7
0.8
0.9
1.0
n
A
U
C
iter
1
2
3
4
5
●
●
●
●
●
●
●
noise: 0.1 noise: 0.15 noise: 0.2
5000 5000 5000
0.5
0.6
0.7
0.8
0.9
1.0
n
in
te
ra
ct
io
n_
A
U
C
iter
1
2
3
4
5
C D
●
●
●
● ● ● ●
●
●
● ● ● ● ●
●
● ● ● ● ● ●
noise: 0.1 noise: 0.15 noise: 0.2
2 3 4 5 6 7 8 2 3 4 5 6 7 8 2 3 4 5 6 7 8
0.00
0.25
0.50
0.75
1.00
interaction_order
re
co
ve
ry
_r
at
e
iter
● 1
2
3
4
5
●
●
●
● ● ● ●
●
●
●
● ● ● ●
●
● ● ● ● ● ●
noise: 0.1 noise: 0.15 noise: 0.2
2 3 4 5 6 7 8 2 3 4 5 6 7 8 2 3 4 5 6 7 8
0.00
0.25
0.50
0.75
1.00
interaction_order
fa
ls
e_
po
si
tiv
e_
w
ei
gh
t
iter
● 1
2
3
4
5
Figure S3: iRF performance for order-8 XOR rule over 10 replicates grouped by noise level. All models
were trained using ntrain = 5, 000 observations. [A] Prediction accuracy (AUC) improves for increasing
k and at a slower rate for increasing noise levels. [B] Interaction AUC improves with increasing k. [C]
Recovery rate for interactions of all orders improves with increasing k. In particular, k = 1 rarely recovers
any interactions of order > 2. [D] False positive weight increases in settings where iRF recovers high-order
interactions as a result of many false positives with low stability scores.
14
●
●
●●
●
●
●
●●●
●
●●●●●
●
●●●●●●
●●●●●●●
●●●●●●●●●●●●●●
●
●●
●
●●
●●●●
●●●●●●●●
●●●
●●
●
●
●
●
● ●● ●
●● ● ● ● ● ●●● ●
●
●
●
●
●
● ●●●● ●●
●
● ●
●
● ●
●
●
●
●
● ●
●
●
●
● ●
●
● ●
● ●●
●
● ● ● ●
●
●
●
●
● ●
●
●
● ●
●●
●
● ●●
●
●
● ● ●
●
●
●● ●
●
●
●
●
●
●●
●●●
●
●●●●●●●●●●●
●
●●●●
●●●●●●
●
●●●●●●●●●
●
●●
●
●●●●●●●●
●●●
●●●●●
●
●
●●
●
●●● ●
● ● ● ● ● ● ●●
●
●● ● ●
●
● ●● ● ●
●
● ● ●
● ●
●●
●
● ●
● ● ●
●
●
●
●
●●
●
●
●
●
●● ● ● ● ● ● ●●●
●
● ●● ●
● ●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●●
●●●●●
●
●●●●
●
●●
●●●●●●
●
●●●●●
●
●●●●●●●●
●●●●●●●
●
●●●●●●●
●
●●●●
●
●●
●
● ● ● ● ●●
● ●
●
●
●
●
●
● ●
●
● ● ●
●
●
●
●
●
●
●
●
●
●
● ● ●● ●
● ● ● ● ● ●●● ● ●●● ●
● ●● ●
● ●
●
●
●
●
●
●●
●
●
●
●●●●
●
●●●●●●
●
●●
●●●●●
●
●●●●●●●
●●●●
●
●
●
●●●●●●●
●
●●
●
●●●
●●●●
●
●●●●●●●●
●
●
●
●
●
●
●●
●
●
●●
●● ● ●●
●●
●
●
● ●
● ●
●●
●
●● ●●
●●
●
●
●
●
●
●
●
●
●
● ●
●
●
●●● ● ●
●●
●
●●●
●●
●
●
●
●
●
●
●
●●
●
●
● ●
●
●
●
●
● ●
●
●
●
●
●
● ● ●
●●
●
●
● ●
●
●
●
●
●
● ●
●
●
●
●
●
●
● ●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●●●
●●●
●
●●●●●
●
●●
●●●●
●
●●
●●●
●
●●
●●●●
●
●
●
●●●●●●●
●
●
●●
●
●●●●●●●●
●
●●
●●●
●●
●
●
●
●
●
●
●
●
●
●
● ●● ● ●●●●
●●
● ●
●
●●●
● ● ●
●
●
●
●
●
●
●
●
●
● ●●
● ● ● ●● ●
●
● ●
●●●
●
●
●
●
●●
●
●
●
●
●
●
● ●
● ●
●
●
●
●
●
●
●
● ●
● ●
●
●
●
●
●
● ● ●●
●
●
●
●
●
●
●
● ●
●●
●
● ●
●
●
●
●
●
●
●
●
● ●
●
●
● ●
●
● ●●
●
●
●
●
●
●
●
●●●●
●
●●●
●
●●●●
●
●
●
●●
●●●●
●●●
●●●
●●●●
●●●
●
●●
●●
●●
●
●●●
●
●●●●●●●
●
●●●
●●●
●
●●●
●
●
●
●
●
●● ●
●
●
● ●● ● ●
●
●
●● ●
●
●
●
●
●
●●
● ● ● ●
●
●●
●
●
●
●
●
●
●●
●
●
●
●●
● ●
●
● ● ●
●
●
●
●
● ●
●
●
●
●
●
● ●
● ●
● ● ●
●
●
●
●
● ● ● ●
●
●
●●●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●●●
●
●●●●●●●●●●●
●
●●
●●●●
●●●●
●
●
●
●●●●●●●
●
●●
●●●●
●
●●
●●●
●
●●●●●●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●●
●
●
●●
●
●●
●
●
●●●●
●
●●● ●
●
●● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●●
●
● ●●●
●●
●
●
●
● ●
●
●● ●●●
●
●
●
●
●
● ●
●
●
●
●
●
● ●●
●●
●
●●
●
●
●
●●
●
●
● ●
●
●
●
●
●
●
●
● ●
●
● ●●
●
●
●
● ●
●
● ●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●●●●●●●●●●
●●
●
●●●●●
●
●●
●●●●
●
●
●
●
●●●●●
●
●
●
●●
●
●●●●●●●●●
●
●●
●
●●
●●
●
●
●
●
●
●
●●
●
●● ●
●
●
●●●
●
●●● ● ●●●●
●
●
●
●●
●●
●● ● ●●● ●
●
●
●
●
●
●
●
●
●●
●
● ●
● ●
●
●● ● ●
●
●
●●
●
●
●
●
●
●
● ●
●
● ●● ● ●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●● ●
●
●
●
●
● ●● ●
●●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●●●
●
●●●
●
●
●
●
●
●●●●●
●
●●
●
●●
●●●
●
●
●
●●
●
●●
●
●●
●●
●
●●●
●
●●●●●●●●●
●
●●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●
●
●
●
●
●● ● ● ●
● ● ●
●
●● ●
●
● ●
●
●
●
●
●
●
●
●
●●
● ● ●
●
●●●●
●
●
●
●
●●
●
● ● ● ●
●
●
●
●
●●
●
●
● ●
●
●●
● ●
●
●
●
●
●●
●
●●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
● ● ●
●
●
● ●
●●
●
●
● ●
●
●
●●
● ●
● ● ● ●●
● ●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
● ●●
● ●
●
●
●
●
●
●
●
●
●●
●
●
●
●●●
●
●●●●●●●●●●●
●●●
●
●●●●●●
●
●
●
●●●●●●●
●
●●
●
●●
●
●●
●●●
●
●●●●●
●●
●
●
●
●●
●
●●
●●
●
●
●●●
●
●
●●●
●
●
●
●
●●
●
●
●
●●
●
●●●●●● ●●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
● ● ●●
●●
●
●
●
●
●●
●
●
●
●
● ●
●
●●
●
●
● ●
● ●
● ●
●
●
●
● ●
● ●
●
●●
●
●
●
●
●
● ● ● ●
● ●
●
●
●
● ● ●
●
●●●
● ●
● ●
● ●
●
●
●
●
●
●
●
● ●
●
● ●
●
●
●
●
●
●
●●
●
●●●●
●
●●●●●●●●●●
●●
●
●●
●
●●●●●
●●●●
●●
●
●
●●●●●
●
●
●●
●
●●
●
●●
●
●●●●●
●●●●●●
●●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●●
●
●●● ●
●
●●●●
●
●
● ●●
●
●
●●
●●●●
●●●●● ●
●●
●
●
●
●
●
●
●
●
●
●
● ● ●
●●●● ●● ●●
●
●
●●
●
●
●●
●
●●● ●● ● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●
●
●
● ●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
● ●
●
● ● ●
● ●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●●
●
●● ● ●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●●●●
●
●●●●●
●
●●
●●
●●●●●
●
●●
●
●●
●●●
●
●●
●●●
●
●
●
●
●
●●
●
●●
●
●●●●●●●●
●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●
●
●●
● ●● ●●
●
● ●●●
●
●●
●
●
●
● ●
●
●
●
●
●
●
●
●●
●
●
● ● ●
●
● ●
●
●
●
●
●
● ●
●
●● ●
●
●● ● ●
●
●
●
●
●
●
●● ●
●
●
● ●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
● ● ●
●
●
●
●
●
●
● ●
● ●
● ●●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
● ●
●
●
●
●
● ●
●
●
●
●●
●
●
●
●
●
● ●
●
noise: 0.2
iter: 2
noise: 0.2
iter: 3
noise: 0.2
iter: 4
noise: 0.2
iter: 5
noise: 0.15
iter: 2
noise: 0.15
iter: 3
noise: 0.15
iter: 4
noise: 0.15
iter: 5
noise: 0.1
iter: 2
noise: 0.1
iter: 3
noise: 0.1
iter: 4
noise: 0.1
iter: 5
0 25 50 75 100 0 25 50 75 100 0 25 50 75 100 0 25 50 75 100
0.000
0.005
0.010
0.015
0.020
0.025
0.000
0.005
0.010
0.015
0.020
0.025
0.000
0.005
0.010
0.015
0.020
0.025
var
iR
F
_w
ei
gh
t
active_var
●
●
0
1
Figure S4: iRF weights for active (blue) and inactive (red) variables for 10 replicates grouped by noise
level and iteration. Weight distributions show a clear separation between active and inactive variables in
settings where iRF recovers the full interaction.
iter: 5
noise: 0.1
iter: 5
noise: 0.15
iter: 5
noise: 0.2
iter: 1
noise: 0.1
iter: 1
noise: 0.15
iter: 1
noise: 0.2
0.006 0.009 0.012 0.006 0.009 0.012 0.006 0.009 0.012
0.0
2.5
5.0
7.5
10.0
12.5
0.0
2.5
5.0
7.5
10.0
12.5
importance
av
er
ag
e_
de
pt
h
active_var
0
1
Figure S5: Average entry depth for active (blue) and inactive (red) variables across the forest vs marginal
importance for a single replicate by iteration and noise level. In later iterations, active variables are selected
earlier on average than inactive variables with comparable marginal importance.
15
A
●
● ●
●
proportion_XOR: 0.5 proportion_XOR: 0.75 proportion_XOR: 0.9
5000 5000 5000
0.5
0.6
0.7
0.8
0.9
1.0
n
A
U
C
iter
1
2
3
4
5
B C
●
proportion_XOR: 0.5 proportion_XOR: 0.75 proportion_XOR: 0.9
5000 5000 5000
0.5
0.6
0.7
0.8
0.9
1.0
n
A
U
C
iter
1
2
3
4
5
●
● ● ●
●
● ●
● ●
●
proportion_XOR: 0.5 proportion_XOR: 0.75 proportion_XOR: 0.9
5000 5000 5000
0.5
0.6
0.7
0.8
0.9
1.0
n
A
U
C
iter
1
2
3
4
5
Figure S6: iRF performance for mixture model by mixture proportion over 10 replicates. All models were
trained using ntrain = 5, 000 observations. [A] Prediction accuracy (AUC) is generally poor as iRF tends to
learn rules that characterize a subset of the data. [B] Interaction AUC for the XOR rule suggest that iRF
fails to recover this marginally less important rule unless it is represented in a large proportion of the data
(π = 0.9). [C] Interaction AUC for the AND rule improves to an optimal AUC of 1 for π = 0.5 and k ≥ 3,
suggesting that iRF favors the more marginally important rule.
16
●●
●●●
●●●
●
●●●●●●●●●●
●●●
●●●●●●●●
●●●●
●
●●●●●●●●●●●●●●●●●●●●●●
●●●●●●●
●
●
●●
● ● ● ● ● ●●
● ●
●
●● ●●● ●
●
● ●
●
●
● ●
● ● ●● ●● ●
● ● ● ● ● ●
●
●
●
●●
●
●
●
●
●●● ● ●
●
●
●
● ●
●
●●
●
● ●
● ●
●
●
●●●●●●
●
●●●●●●●●●●●●●●●●●●●●●
●●●
●
●●●●●●
●●●●●●●●●●●●●●●●
●●●●
●
●●
● ● ●● ● ●● ●
●
●
●● ● ●●● ● ● ● ● ●
●
● ● ●
●
● ●
●
● ●● ● ● ●
●●
●
● ●●
●
●
● ●
●●
●
●
●
●
●
●●●●●●●●●
●●●●●
●●●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●
●
●
●●●
●
●
● ●
● ●●● ●
●
● ●
●
●● ● ●●
●
●
●
●
●
●
●
● ●
●
●
● ●
●
●●
● ●
●
●
●
●● ●
●●
●●●
●
●
●
●
●
●●
●
●●●●●●
●●●
●
●●●●●●
●
●●
●
●
●
●●●
●
●●●●
●●●●
●
●●●
●
●●●
●●●●
●
●●●●●●●●
●
●
●
●
●● ●
●
● ● ●
●
● ● ●
●
●
●
●●
●
●●● ●
●
●
●
●
●
●
●● ●
●
●
● ●
●
●
●●●
●
●
●●
●
● ● ●
● ●●●
● ●
●
●
●
●
● ●
● ●
●●
●
●
●
● ●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
● ●
● ● ●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●●●
●
●
●
●●●●
●●●●●●●●●●●●●●●●●
●●●
●
●●
●
●●
●
●
●●●●●●●●●●●●●●●●
●●●
●●●
●●
●●
●
●
●●
●●
●●●
●● ●
●
●
● ●
● ●
● ●
●
●●
●●
●●
●
●
● ●
●
●
●● ●● ●
●● ● ●
● ●●●
●
●
●
●
●
● ● ●
● ● ●●
●●
●
●
●
● ● ●
●
●
●
●
● ●
●
● ●●● ● ● ● ●
●
●●
● ●
●
●
● ● ●
●●
●
●
●●
●
●
●
●
●●
●
●●●
●
●●●●●
●●●
●
●●●●
●
●●●●●●●●
●●
●
●●●●
●●●●
●●●●●●●
●●●●
●●●●●●●●
●
●
●●●
● ●● ●
●
●● ●● ●
●
●
●●
●
●●
●
● ●●●●
●
●●●
● ●●
●
● ● ●
●● ● ●
●
● ●●
●
●
●●
●
●
● ●
●
●●●
●
●
● ● ● ●
●
●
● ● ●
●
●
●
●
● ● ●
●●●
● ●
● ● ● ● ● ●
● ●
●
● ●
●●
●
●●
●
●
●
●
●
●●●●
●
●●
●
●●●
●●●●
●
●
●●
●●●●
●
●●
●
●
●
●
●●
●
●
●
●●
●●●●●●
●
●●●
●
●●●
●●
●
●
●
●●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ● ●
●●●
● ●
●●● ●
●
●
●●
●
● ● ●● ●
●
●
●●
●
● ●● ●
●
●
●
● ●●
●●
●
●
●●
● ●
●
●
●
●
●
● ● ●
●●●
● ●
●
●
●
●
● ● ● ●
●
●●
●
● ● ●
●
●
●
●
●
●
●
●● ●
●
●
●
● ●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●●●
●
●
●●
●●●●
●●●●●
●
●
●
●●●●●
●●●●●
●●
●●
●
●●
●
●
●●
●
●
●●
●●●●●
●
●●
●●●●●
●
●●
●●●
●●●●
●
●
●●
●
●
●
●
●●
●
●●
●
●
●● ●
●
● ●
● ●
●
●●●●
●●
●●
●●
●
●
●
●●
●
●
●
●
● ●●
●
●
●●
●
● ● ● ●
●
●
●●
●
●
●
● ●
●
● ● ● ●
●
●
●
●
●
●
● ●
●
●
●
●
●
● ●
●
●●
●
● ● ●●
●
●
● ●
● ●● ●● ●
●
● ●
● ●
●
●● ●
● ● ● ● ●
● ●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●●
●
●●
●
●
●●
●
●●●●
●
●●●●●●●●
●
●
●
●
●●
●
●●●
●●●●●●●●●●
●●●
●●●●●●●
●
●●
●
●●
●
●
●●● ●●●
●●●
●
● ●●● ●● ●
●
●●●
●
●
●●●
●
● ●●●●●●
●
●●●
●
● ●
● ●●●●●
● ●● ●●●
●●
●
● ● ● ● ●●
●
●
●●●●●●
●
●
●
●
●●
●
●
●
●
● ● ● ●
●
●●
●●
● ● ●
●
● ● ●
● ●
●
●●
●
●
●
●
●●●●
●
●
●
●
● ●
●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●●
●
●
●
●
●
●●
●
●●●
●
●●●●
●
●●
●●●
●
●
●
●
●●
●●
●●●●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●●●
●
●●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●
● ●
●
●●●
●
● ●
●
●
●
●
●
●
● ●● ●
●
●
●
●
●
●
●
●
●● ●
●
●●
●●
●
●
● ●●
●
●●
●
●
●
●
●
●
●
●
●
●
● ●
● ●
●
●
●
●
●
●
●
●
● ●● ● ● ●
●
●
● ●
●
●
●
●
●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●●●●●
●
●
●
●
●
●●●
●●●●
●
●●●●●●●
●●●●●
●●
●
●●●
●
●●●
●
●●
●
●
●
●
●●
●●●●●●
●●●
●
●●●
●
●●
●
●
●●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●● ●●
●●●
●●
●●
●●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
● ●
●
●
●
●●
● ●● ● ●
●
●●
● ● ● ●●
●
●
●
●
●
●
●
●
●
●
● ● ●●
●
●
●
●
● ●
●
●
● ●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
● ●
●
●
● ●
●
●
● ●
●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
● ●● ●
●
●
● ● ●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●●
●
●
●
●
●
●
●●
●
●
●●●●●●●
●●●
●
●
●
●
●●●
●●●●
●●●●●●●
●
●●●
●●●
●●●●
●
●
●
●
●
●
●
●●●
●● ●●
●●●●● ●● ●●● ●●●● ●●●●
●●●
●
● ●●●●●●●
●
●
●
●
●●
● ●●●
●
● ● ●
●● ●●●●
●
●
●● ●
●
● ●●
●
●
●●
●
●
●
● ●
●● ● ● ●● ● ●
●
●
●
●●
●
●
●
● ● ● ● ● ●
●
●●●
●
●
●
●
● ●
● ●
● ●
●
●●
●
●●
●
●
●●
●
● ● ● ●
●●
●●
●
●
● ●
●
● ● ●
●
● ●
●
● ●● ●
●●
●●
●
●
●
●
●●
●
●
●
● ●
●
●
●
proportion_XOR: 0.9
iter: 2
proportion_XOR: 0.9
iter: 3
proportion_XOR: 0.9
iter: 4
proportion_XOR: 0.9
iter: 5
proportion_XOR: 0.75
iter: 2
proportion_XOR: 0.75
iter: 3
proportion_XOR: 0.75
iter: 4
proportion_XOR: 0.75
iter: 5
proportion_XOR: 0.5
iter: 2
proportion_XOR: 0.5
iter: 3
proportion_XOR: 0.5
iter: 4
proportion_XOR: 0.5
iter: 5
0 25 50 75 100 0 25 50 75 100 0 25 50 75 100 0 25 50 75 100
0.005
0.010
0.015
0.020
0.005
0.010
0.015
0.020
0.005
0.010
0.015
0.020
variable
iR
F
_w
ei
gh
t active_var
●
●
●
inactive
XOR
AND
Figure S7: iRF weights for active (blue, green) and inactive (red) variables for 10 replicates by mixture
proportion and iteration. Weight distributions show clear separation between active and inactive features in
settings where iRF recovers the full interaction.
17
A B
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
rho: 0 rho: 0.25 rho: 0.5 rho: 0.75
5000 5000 5000 5000
0.5
0.6
0.7
0.8
0.9
1.0
n
A
U
C
iter
1
2
3
4
5
●
●
rho: 0 rho: 0.25 rho: 0.5 rho: 0.75
5000 5000 5000 5000
0.5
0.6
0.7
0.8
0.9
1.0
n
in
te
ra
ct
io
n_
A
U
C
iter
1
2
3
4
5
C D
●
●
●
● ● ● ●
●
●
●
● ● ● ●
●
●
●
● ● ● ●
●
●
● ● ● ● ●
rho: 0 rho: 0.25 rho: 0.5 rho: 0.75
2 3 4 5 6 7 8 2 3 4 5 6 7 8 2 3 4 5 6 7 8 2 3 4 5 6 7 8
0.00
0.25
0.50
0.75
1.00
interaction_order
re
co
ve
ry
_r
at
e
iter
● 1
2
3
4
5
●
●
●
● ● ● ●
●
●
●
●
●
● ●
●
●
●
● ● ● ●
●
●
●
● ● ● ●
rho: 0 rho: 0.25 rho: 0.5 rho: 0.75
2 3 4 5 6 7 8 2 3 4 5 6 7 8 2 3 4 5 6 7 8 2 3 4 5 6 7 8
0.00
0.25
0.50
0.75
1.00
interaction_order
fa
ls
e_
po
si
tiv
e_
w
ei
gh
t
iter
● 1
2
3
4
5
Figure S8: iRF performance for order-8 XOR rule over 10 replicates by correlation level with decaying
covariance structure. All models were trained on ntrain = 5, 000 observations. [A] Prediction accuracy
(AUC) improves for increasing k. [B] Interaction AUC improves with increasing k, but is more variable
than uncorrelated settings. [C] Recovery rate for interactions of all orders improves with increasing k. In
particular, k = 1 rarely recovers any interactions of order > 2. For ρ = 0, iRF recovers the full order-8
interaction in 1 of the 10 replicates. [D] False positive weight increases in settings where iRF recovers
high-order interactions as a result of many false positives with low stability scores.
18
A B
●
●
●
●
●
●
●
●
rho: 0 rho: 0.25 rho: 0.5 rho: 0.75
5000 5000 5000 5000
0.5
0.6
0.7
0.8
0.9
1.0
n
A
U
C
iter
1
2
3
4
5
●
●
rho: 0 rho: 0.25 rho: 0.5 rho: 0.75
5000 5000 5000 5000
0.5
0.6
0.7
0.8
0.9
1.0
n
in
te
ra
ct
io
n_
A
U
C
iter
1
2
3
4
5
C D
●
●
●
● ● ● ●
●
●
●
●
● ● ●
●
●
●
● ● ● ●
●
●
● ● ● ● ●
rho: 0 rho: 0.25 rho: 0.5 rho: 0.75
2 3 4 5 6 7 8 2 3 4 5 6 7 8 2 3 4 5 6 7 8 2 3 4 5 6 7 8
0.00
0.25
0.50
0.75
1.00
interaction_order
re
co
ve
ry
_r
at
e
iter
● 1
2
3
4
5
●
●
●
●
● ● ●
●
●
●
●
●
● ●
●
●
●
● ● ● ●
●
●
● ● ● ● ●
rho: 0 rho: 0.25 rho: 0.5 rho: 0.75
2 3 4 5 6 7 8 2 3 4 5 6 7 8 2 3 4 5 6 7 8 2 3 4 5 6 7 8
0.00
0.25
0.50
0.75
1.00
interaction_order
fa
ls
e_
po
si
tiv
e_
w
ei
gh
t
iter
● 1
2
3
4
5
Figure S9: iRF performance for order-8 XOR rule over 10 replicates by correlation level with block covari-
ance. All models were trained on ntrain = 5, 000 observations. [A] Prediction accuracy (AUC) improves for
increasing k. [B] Interaction AUC improves with increasing k and drops for large values of ρ. Variabiliy is
comparable to the decaying covariance case and greater than in uncorrelated settings. [C] Recovery rate for
interactions of all orders improves with increasing k. In particular, k = 1 rarely recovers any interactions
of order > 2. [D] False positive weight increases in settings where iRF recovers high-order interactions as a
result of many false positives with low stability scores.
19
A B
● ●
●
●
●
●
●
●
●
●
●
●
noise: 0.3 noise: 0.4
noise: 0.1 noise: 0.2
500 500
500 500
0.5
0.6
0.7
0.8
0.9
1.0
0.5
0.6
0.7
0.8
0.9
1.0
n
A
U
C
iter
1
2
3
4
5
●
●
●
●
● ● ●
●
●
●
●
●
● ●
●
●
●
●
noise: 0.3 noise: 0.4
noise: 0.1 noise: 0.2
500 500
500 500
0.5
0.6
0.7
0.8
0.9
1.0
0.5
0.6
0.7
0.8
0.9
1.0
n
in
te
ra
ct
io
n_
A
U
C
iter
1
2
3
4
5
C D
●
● ●
●
● ●
●
● ●
● ● ●
noise: 0.3 noise: 0.4
noise: 0.1 noise: 0.2
2 3 4 2 3 4
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
interaction_order
re
co
ve
ry
_r
at
e
iter
● 1
2
3
4
5
●
● ●
●
● ●
●
● ●
●
● ●
noise: 0.3 noise: 0.4
noise: 0.1 noise: 0.2
2 3 4 2 3 4
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
interaction_order
fa
ls
e_
po
si
tiv
e_
w
ei
gh
t
iter
● 1
2
3
4
5
Figure S10: iRF performance for order-4 AND rule with p = 500 over 10 replicates. All models were trained
on ntrain = 500 observations. [A] Prediction accuracy (AUC) improves and stabilizes with increasing k. [B]
Interaction AUC improves with increasing k for π = 0.1, 0.2, 0.3 and indicates poor performance at all levels
of k for π = 0.4. [C] Recovery rate improves with increasing k but to a smaller degree for increasing π. [D]
False positive weight is low for interactions of order > 2 and drops with iteration for interactions of order 2.
20
A B
●
●
●
●
●
●
●
●
●
●
●
●
●
●
noise: 0.3 noise: 0.4
noise: 0.1 noise: 0.2
500 500
500 500
0.5
0.6
0.7
0.8
0.9
1.0
0.5
0.6
0.7
0.8
0.9
1.0
n
A
U
C
iter
1
2
3
4
5
●
●
● ● ●
●
●
●
●
●
● ●
●
●
noise: 0.3 noise: 0.4
noise: 0.1 noise: 0.2
500 500
500 500
0.5
0.6
0.7
0.8
0.9
1.0
0.5
0.6
0.7
0.8
0.9
1.0
n
in
te
ra
ct
io
n_
A
U
C
iter
1
2
3
4
5
C D
●
● ●
● ● ●
●
● ●
● ● ●
noise: 0.3 noise: 0.4
noise: 0.1 noise: 0.2
2 3 4 2 3 4
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
interaction_order
re
co
ve
ry
_r
at
e
iter
● 1
2
3
4
5
●
● ●
●
● ●
●
● ●
●
● ●
noise: 0.3 noise: 0.4
noise: 0.1 noise: 0.2
2 3 4 2 3 4
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
interaction_order
fa
ls
e_
po
si
tiv
e_
w
ei
gh
t
iter
● 1
2
3
4
5
Figure S11: iRF performance for order-4 AND rule with p = 500 over 10 replicates. All models were trained
on ntrain = 500 observations. [A] Prediction accuracy (AUC) improves and stabilizes with increasing k. [B]
Interaction AUC improves with increasing k for π = 0.1, 0.2, 0.3 and indicates poor performance at all levels
of k for π = 0.4. [C] Recovery rate improves with increasing k but to a smaller degree for increasing π. [D]
False positive weight is low for interactions of order > 2 and drops with iteration for interactions of order 2.
21
A B
●
●
●
●
●
●
●
●
●
noise: 0.3 noise: 0.4
noise: 0.1 noise: 0.2
500 500
500 500
0.5
0.6
0.7
0.8
0.9
1.0
0.5
0.6
0.7
0.8
0.9
1.0
n
A
U
C
iter
1
2
3
4
5
●
●
●
●
●
●
●
●
●
●
●
● ●
noise: 0.3 noise: 0.4
noise: 0.1 noise: 0.2
500 500
500 500
0.5
0.6
0.7
0.8
0.9
1.0
0.5
0.6
0.7
0.8
0.9
1.0
n
in
te
ra
ct
io
n_
A
U
C
iter
1
2
3
4
5
C D
●
● ●
● ● ●
● ● ●
● ● ●
noise: 0.3 noise: 0.4
noise: 0.1 noise: 0.2
2 3 4 2 3 4
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
interaction_order
re
co
ve
ry
_r
at
e
iter
● 1
2
3
4
5
●
● ●
●
● ●
●
● ●
●
● ●
noise: 0.3 noise: 0.4
noise: 0.1 noise: 0.2
2 3 4 2 3 4
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
interaction_order
fa
ls
e_
po
si
tiv
e_
w
ei
gh
t
iter
● 1
2
3
4
5
Figure S12: iRF performance for order-4 AND rule with p = 500 over 10 replicates. All models were trained
on ntrain = 500 observations. [A] Prediction accuracy (AUC) improves and stabilizes with increasing k. [B]
Interaction AUC improves with increasing k for π = 0.1, 0.2, 0.3 and indicates poor performance at all levels
of k for π = 0.4. [C] Recovery rate improves with increasing k but to a smaller degree for increasing π. [D]
False positive weight is low for interactions of order > 2. High false positive weights for order-2 interactions
when π = 0.3 are the result of many false positives with low stability scores.
22
A B
●
●● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
noise: RF noise: Swap
200 400 600 800 1000 200 400 600 800 1000
0.5
0.6
0.7
0.8
0.9
1.0
n
A
U
C
iter
1
2
3
4
5
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
● ●
●
●
noise: RF noise: Swap
200 400 600 800 1000 200 400 600 800 1000
0.5
0.6
0.7
0.8
0.9
1.0
n
in
te
ra
ct
io
n_
A
U
C
iter
1
2
3
4
5
C D
●
● ● ●
●
●
● ●
●
●
● ●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
noise: Swap
n: 200
noise: Swap
n: 400
noise: Swap
n: 600
noise: Swap
n: 800
noise: Swap
n: 1000
noise: RF
n: 200
noise: RF
n: 400
noise: RF
n: 600
noise: RF
n: 800
noise: RF
n: 1000
2 3 4 5 2 3 4 5 2 3 4 5 2 3 4 5 2 3 4 5
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
interaction_order
re
co
ve
ry
_r
at
e
iter
● 1
2
3
4
5
● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
● ●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
noise: Swap
n: 200
noise: Swap
n: 400
noise: Swap
n: 600
noise: Swap
n: 800
noise: Swap
n: 1000
noise: RF
n: 200
noise: RF
n: 400
noise: RF
n: 600
noise: RF
n: 800
noise: RF
n: 1000
2 3 4 5 2 3 4 5 2 3 4 5 2 3 4 5 2 3 4 5
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
interaction_order
fa
ls
e_
po
si
tiv
e_
w
ei
gh
t
iter
● 1
2
3
4
5
Figure S13: iRF performance for the enhancer data simulations by noise type. Models were trained
on ntrain ∈ {200, 400, . . . , 1000} observations. [A] Prediction accuracy (AUC) remains consistent with
increasing k in both noise models. [B] Interaction AUC improves after iteration k = 1 but drops for later
iterations in some settings, emphasizing the importance of tuning K. [C] Recovery rate improves with
increasing k. [D] False positive weight is high in both noise settings whenever iRF recovers interactions.
These false positives tend to be comprised of many active features and few inactive features and are therefore
partially informative of the data generating rule.
23
10 20 30 40 50 60 70 80
0
5
10
15
Runtime (Enhancer Data)
No. of Features (p)
R
un
tim
e 
(m
in
s.
)
iRF
Rulefit
10 20 30 40 50 60 70 80
76
78
80
82
84
AUC (Enhancer Data)
No. of Features (p)
A
U
R
O
C
 (
%
)
iRF
Rulefit
Figure S14: Runtime (left) of interaction detection and Area under ROC curve (right) of prediction by
Rulefit and iRF on subsets of the enhancer data with p ∈ {10, 20, . . . , 80} features and balanced training
and test sets, each of size n = 731. The results are averaged over 10 different permutations of the original
features in the enhancer dataset. The two algorithms provide similar classification accuracy in test data,
although computational cost of iRF grows much slower with p than that of Rulefit.
24
A
Lot.Area_Overall.Qual_Year.Built_Full.Bath_sqft.live
Lot.Area_Overall.Qual
Overall.Qual_Garage.Cars_sqft.live
Overall.Qual_Year.Remod.Add_Full.Bath_sqft.live
Overall.Qual_Year.Built_Year.Remod.Add_sqft.live
Overall.Qual_Full.Bath_Garage.Area_sqft.live
Lot.Area_Overall.Qual_Year.Built_Garage.Area_sqft.live
Overall.Qual_Year.Built_Bsmt.Unf.SF_sqft.live
Lot.Area_Overall.Qual_Year.Remod.Add_sqft.live
Overall.Qual_Year.Built_Full.Bath_sqft.live
Lot.Area_Overall.Qual_Full.Bath_sqft.live
Lot.Area_Overall.Qual_Bsmt.Unf.SF_sqft.live
Overall.Qual_Full.Bath_sqft.live
Overall.Qual_Bsmt.Unf.SF_Garage.Area_sqft.live
Overall.Qual_Year.Built_Garage.Area_sqft.live
Lot.Area_Overall.Qual_sqft.live
Lot.Area_Overall.Qual_Garage.Area_sqft.live
Lot.Area_Overall.Qual_Year.Built_sqft.live
Overall.Qual_sqft.live
Overall.Qual_Garage.Area_sqft.live
Overall.Qual_Bsmt.Unf.SF_sqft.live
Overall.Qual_Year.Remod.Add_sqft.live
Overall.Qual_Year.Built_sqft.live
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Housing data interactions
stability score
●
●
●
●
2−order
3−order
4−order
5−order
B C
Above average qualityAverage quality and below
Sa
le
 P
ric
e 
($
)
Sa
le
 P
ric
e 
($
)
Livable Sqft (quantile) Livable Sqft (quantile)
Lot Size (quantile) Lot Size (quantile)
Above average qualityAverage quality and below
Sa
le
 P
ric
e 
($
)
Sa
le
 P
ric
e 
($
)
Livable Sqft (quantile) Livable Sqft (quantile)
Year Built (quantile)Year Built (quantile)
Figure S15: Interactions recovered in Ames housing data. [A] Stability scores of interactions recovered by
iRF. Interactions with stability scores less than 0.25 have been removed for visualization. [B] Surface maps
showing order-3 interaction between livable square footage, lot size, and overall home quality. These maps
suggest that homes of below average quality show a stronger relationship between lot size and price and
a weaker relationship between livable square footage and price when compared to homes of above average
quality. [C] Surface map of order-3 interaction between livable square footage, year built, and overall home
quality. These maps suggest that homes of above average quality show a weaker relationship between year
built and price.
25
References
D. Amaratunga, J. Cabrera, and Y.-S. Lee. Enriched random forests. Bioinformatics, 24(18):2010–2014,
2008.
R. L. Barter and B. Yu. Superheat: Supervised heatmaps for visualizing complex data. arXiv preprint
arXiv:1512.01524, 2015.
C. E. Breeze, D. S. Paul, J. van Dongen, L. M. Butcher, J. C. Ambrose, J. E. Barrett, R. Lowe, V. K. Rakyan,
V. Iotchkova, M. Frontini, et al. eforge: a tool for identifying cell type-specific signal in epigenomic data.
Cell reports, 17(8):2137–2150, 2016.
L. Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.
L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.
L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen. Classification and regression trees. CRC press,
1984.
X. Dong, M. C. Greven, A. Kundaje, S. Djebali, J. B. Brown, C. Cheng, T. R. Gingeras, M. Gerstein,
R. Guigó, E. Birney, et al. Modeling gene expression using chromatin features in various cellular contexts.
Genome Biology, 13(9):1–17, 2012.
ENCODE Project Consortium. An integrated encyclopedia of dna elements in the human genome. Nature,
489(7414):57–74, 2012.
W. W. Fisher, J. J. Li, A. S. Hammonds, J. B. Brown, B. D. Pfeiffer, R. Weiszmann, S. MacArthur,
S. Thomas, J. A. Stamatoyannopoulos, M. B. Eisen, et al. Dna regions bound at low occupancy by
transcription factors do not drive patterned reporter gene expression in drosophila. Proceedings of the
National Academy of Sciences, 109(52):21330–21335, 2012.
S. M. Foo, Y. Sun, B. Lim, R. Ziukaite, K. O’Brien, C. Y. Nien, N. Kirov, S. Y. Shvartsman, and C. A.
Rushlow. Zelda potentiates morphogen activity by increasing chromatin accessibility. Curr. Biol., 24(12):
1341–1346, Jun 2014.
J. H. Friedman and B. E. Popescu. Predictive learning via rule ensembles. The Annals of Applied Statistics,
pages 916–954, 2008.
E. Frise, A. S. Hammonds, and S. E. Celniker. Systematic image-driven analysis of the spatial Drosophila
embryonic expression landscape. Mol. Syst. Biol., 6:345, 2010.
R. S. Hallmann, L. G. Schneeweiss, E. Correa, and J. Zamora. Fine needle aspiration biopsy of thymic
carcinoid tumor: a case with immunocytochemical correlation. Acta Cytol., 42(4):1042–1043, 1998.
A. S. Hammonds, C. A. Bristow, W. W. Fisher, R. Weiszmann, S. Wu, V. Hartenstein, M. Kellis, B. Yu,
E. Frise, and S. E. Celniker. Spatial expression of transcription factors in Drosophila embryonic organ
development. Genome Biol., 14(12):R140, Dec 2013.
M. M. Harrison, X.-Y. Li, T. Kaplan, M. R. Botchan, and M. B. Eisen. Zelda binding in the early drosophila
melanogaster embryo marks regions subsequently activated at the maternal-to-zygotic transition. PLoS
Genet, 7(10):e1002266, 2011.
I. B. Hilton, A. M. D’Ippolito, C. M. Vockley, P. I. Thakore, G. E. Crawford, T. E. Reddy, and C. A.
Gersbach. Epigenome editing by a crispr-cas9-based acetyltransferase activates genes from promoters and
enhancers. Nature biotechnology, 33(5):510–517, 2015.
M. M. Hoffman, J. Ernst, S. P. Wilder, A. Kundaje, R. S. Harris, M. Libbrecht, B. Giardine, P. M. Ellenbogen,
J. A. Bilmes, E. Birney, et al. Integrative annotation of chromatin elements from encode data. Nucleic
acids research, page gks1284, 2012.
26
S. K. Hota and B. G. Bruneau. Atp-dependent chromatin remodeling during mammalian development.
Development, 143(16):2882–2897, 2016.
D. W. Knowles and M. D. Biggin. Building quantitative, three-dimensional atlases of gene expression and
morphology at cellular resolution. Wiley Interdisciplinary Reviews: Developmental Biology, 2(6):767–779,
2013.
P. Kolasinska-Zwierz, T. Down, I. Latorre, T. Liu, X. S. Liu, and J. Ahringer. Differential chromatin marking
of introns and expressed exons by h3k36me3. Nature genetics, 41(3):376–381, 2009.
A. R. Kornblihtt. Ctcf: from insulators to alternative splicing regulation. Cell research, 22(3):450–452, 2012.
E. Z. Kvon, T. Kazmar, G. Stampfel, J. O. Yanez-Cuna, M. Pagani, K. Schernhuber, B. J. Dickson, and
A. Stark. Genome-scale functional characterization of Drosophila developmental enhancers in vivo. Nature,
512(7512):91–95, Aug 2014.
S. G. Landt, G. K. Marinov, A. Kundaje, P. Kheradpour, F. Pauli, S. Batzoglou, B. E. Bernstein, P. Bickel,
J. B. Brown, P. Cayting, et al. Chip-seq guidelines and practices of the encode and modencode consortia.
Genome research, 22(9):1813–1831, 2012.
M. Levine. Transcriptional enhancers in animal development and evolution. Current Biology, 20(17):R754–
R763, 2010.
M. Levine. Computing away the magic? eLife, 2:e01135, 2013.
G. Li, X. Ruan, R. K. Auerbach, K. S. Sandhu, M. Zheng, P. Wang, H. M. Poh, Y. Goh, J. Lim, J. Zhang,
et al. Extensive promoter-centered chromatin interactions provide a topological basis for transcription
regulation. Cell, 148(1):84–98, 2012.
Q. Li, J. B. Brown, H. Huang, and P. J. Bickel. Measuring reproducibility of high-throughput experiments.
The annals of applied statistics, pages 1752–1779, 2011.
X.-y. Li, S. MacArthur, R. Bourgon, D. Nix, D. A. Pollard, V. N. Iyer, A. Hechmer, L. Simirenko, M. Sta-
pleton, and C. L. L. Hendriks. Transcription factors bind thousands of active and inactive regions in the
drosophila blastoderm. PLoS Biol, 6(2):e27, 2008.
X.-Y. Li, M. M. Harrison, J. E. Villalta, T. Kaplan, and M. B. Eisen. Establishment of regions of genomic
activity during the drosophila maternal to zygotic transition. Elife, 3:e03737, 2014.
H.-L. Liang, C.-Y. Nien, H.-Y. Liu, M. M. Metzstein, N. Kirov, and C. Rushlow. The zinc-finger protein
zelda is a key activator of the early zygotic genome in drosophila. Nature, 456(7220):400–403, 2008.
A. Liaw and M. Wiener. Classification and regression by randomforest. R news, 2(3):18–22, 2002.
C. Lim and B. Yu. Estimation stability with cross validation (escv). Journal of Computational and Graphical
Statistics, (just-accepted), 2015.
S. MacArthur, X.-Y. Li, J. Li, J. B. Brown, H. C. Chu, L. Zeng, B. P. Grondona, A. Hechmer, L. Simirenko,
S. V. Keränen, et al. Developmental roles of 21 drosophila transcription factors are determined by quan-
titative differences in binding to an overlapping set of thousands of genomic regions. Genome biology, 10
(7):R80, 2009.
N. Meinshausen. Forest garrote. Electronic Journal of Statistics, 3:1288–1304, 2009.
N. Meinshausen. Node harvest. The Annals of Applied Statistics, pages 2049–2072, 2010.
D. L. Nelson, A. L. Lehninger, and M. M. Cox. Lehninger principles of biochemistry. Macmillan, 2008.
D. D. Pervouchine, A. Breschi, E. Palumbo, and R. Guigo. Ipsa: An integrative pipeline for splicing analysis.
preprint, 2016.
27
M. D. Ritchie, L. W. Hahn, N. Roodi, L. R. Bailey, W. D. Dupont, F. F. Parl, and J. H. Moore. Multifactor-
dimensionality reduction reveals high-order interactions among estrogen-metabolism genes in sporadic
breast cancer. The American Journal of Human Genetics, 69(1):138–147, 2001.
R. Rivera-Pomar and H. Jãckle. From gradients to stripes in drosophila embryogenesis: filling in the gaps.
Trends in Genetics, 12(11):478–483, 1996.
C. K. I. Ruczinski, M. L. LeBlanc, and L. Hsu. Sequence analysis using logic regression. Genetic epidemiology,
21(1):S626–S631, 2001.
K. N. Schulz, E. R. Bondra, A. Moshe, J. E. Villalta, J. D. Lieb, T. Kaplan, D. J. McKay, and M. M.
Harrison. Zelda is differentially required for chromatin accessibility, transcription factor binding, and gene
expression in the early drosophila embryo. Genome research, 25(11):1715–1726, 2015.
R. D. Shah and N. Meinshausen. Random intersection trees. The Journal of Machine Learning Research,
15(1):629–654, 2014.
R. J. Sims Iii and D. Reinberg. Processing the h3k36me3 signature. Nature genetics, 41(3):270–271, 2009.
B. R. So, L. Wan, Z. Zhang, P. Li, E. Babiash, J. Duan, I. Younis, and G. Dreyfuss. A u1 snrnp-specific
assembly pathway reveals the smn complex as a versatile hub for rnp exchange. Nature structural &
molecular biology, 2016.
R. Sperling. The nuts and bolts of the endogenous spliceosome. Wiley Interdisciplinary Reviews: RNA,
2016.
M. H. Stoiber, S. Olson, G. E. May, M. O. Duff, J. Manent, R. Obar, K. Guruharsha, P. J. Bickel,
S. Artavanis-Tsakonas, J. B. Brown, et al. Extensive cross-regulation of post-transcriptional regulatory
networks in drosophila. Genome research, 25(11):1692–1702, 2015.
S. Thomas, X.-Y. Li, P. J. Sabo, R. Sandstrom, R. E. Thurman, T. K. Canfield, E. Giste, W. Fisher,
A. Hammonds, S. E. Celniker, et al. Dynamic reprogramming of chromatin accessibility during drosophila
embryo development. Genome biology, 12(5):1, 2011.
H. Tilgner, D. G. Knowles, R. Johnson, C. A. Davis, S. Chakrabortty, S. Djebali, J. Curado, M. Snyder, T. R.
Gingeras, and R. Guigó. Deep sequencing of subcellular rna fractions shows splicing to be predominantly
co-transcriptional in the human genome but inefficient for lncrnas. Genome research, 22(9):1616–1625,
2012.
A. Weiner, D. Lara-Astiaso, V. Krupalnik, O. Gafni, E. David, D. R. Winter, J. H. Hanna, and I. Amit.
Co-chip enables genome-wide mapping of histone mark co-occurrence at single-molecule resolution. Nature
Biotechnology, 34(9):953–961, 2016.
Z. Xu, H. Chen, J. Ling, D. Yu, P. Struffi, and S. Small. Impacts of the ubiquitous factor Zelda on Bicoid-
dependent DNA binding and transcription in Drosophila. Genes Dev., 28(6):608–621, Mar 2014.
B. Yu. Stability. Bernoulli, 19(4):1484–1500, 2013.
Y. Zhang and J. S. Liu. Bayesian inference of epistatic interactions in case-control studies. Nature genetics,
39(9):1167–1173, 2007.
28

