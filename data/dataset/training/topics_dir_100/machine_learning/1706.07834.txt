COVER TREE COMPRESSED SENSING FOR FAST MR FINGERPRINT RECOVERY
Mohammad Golbabaee*, Zhouye Chen†, Yves Wiaux† and Mike E. Davies*
*Institute for digital communications, University of Edinburgh, EH9 3JL, UK
†Institute of sensors, signals and systems, Heriot-Watt University, EH14 4AS, UK
ABSTRACT
We adopt data structure in the form of cover trees and iteratively
apply approximate nearest neighbour (ANN) searches for fast com-
pressed sensing reconstruction of signals living on discrete smooth
manifolds. Levering on the recent stability results for the inexact
Iterative Projected Gradient (IPG) algorithm and by using the cover
tree’s ANN searches, we decrease the projection cost of the IPG al-
gorithm to be logarithmically growing with data population for low
dimensional smooth manifolds. We apply our results to quantitative
MRI compressed sensing and in particular within the Magnetic Res-
onance Fingerprinting (MRF) framework. For a similar (or some-
times better) reconstruction accuracy, we report 2-3 orders of mag-
nitude reduction in computations compared to the standard iterative
method which uses brute-force searches.
Index Terms— Data driven compressed sensing, iterative pro-
jected gradient, magnetic resonance fingerprinting, cover tree, ap-
proximate nearest neighbour search.
1. INTRODUCTION
Compressed sensing (CS) algorithms adopt efficient signal mod-
els to achieve accurate reconstruction given small number of non-
adaptive measurements [1, 2]. CS consists of a linear sampling
model:
y ≈ Ax0, (1)
where y is a noisy m-dimensional vector of measurements taken
from the ground truth signal x0 in a high dimension n m through
using a linear mapping A. The success of CS for stably invert-
ing such ill-posed systems relies on the fact that natural signals of-
ten follow low-dimensional models C with limited degrees of free-
dom compared to their original ambient dimension, and that cer-
tain random sampling schemes can compactly (in terms of the mea-
surements) preserve the key signal information (see e.g. [3] for an
overview on different CS models and their corresponding sample
complexities). Variational approach for CS reconstruction consists
of solving e.g. the following constrained least-square problem:
x̂ = argminx∈C ||y −Ax||
2
2. (2)
where the constraint takes into account the signal model.
First order methods based on iterative local gradient and projec-
tion (or also proximal) updates are popular for CS recovery due to
their scalability for big data problems [4] and their flexibility in han-
dling various complicated (convex/nonconvex and sometimes com-
binatorial) models [5]. Starting from an initial point e.g. xk=0 = 0
This work is partly funded by the EPSRC grant EP/M019802/1 and the
ERC C-SENSE project (ERC-ADG-2015-694888).
and for a given step size µ, the solution of IPG iteratively updates as
follows:
xk+1 = PC
(
xk − µAH(Axk − y)
)
, (3)
where the projection is defined as PC ∈ argminu∈C ||x − u|| and
throughout, ||.|| stands for the euclidean norm.
In this work we consider compressed sensing of signals living
on a low-dimensional smooth manifold. In this case, the model pro-
jection might not be computationally easy for certain complex man-
ifolds. A popular approach consists of collecting discrete samples of
the manifold in a dictionary [6, 7], i.e. a data driven CS approach,
and approximate the projection step by searching the nearest atom
in this dictionary. Depending on the manifold complexity and the
desired approximation accuracy, the size of this dictionary can be
very large in practice. In this case performing an exhaustive nearest
neighbour (NN) search could become computationally challenging.
We address this shortcoming by preprocessing the dictionary
and build a tree structure suitable for fast approximate nearest neigh-
bour (ANN) searches. Recently, it as been shown that provided
enough measurements and for moderate approximations the inexact
IPG achieves similar solution accuracy as for the exact algorithm [8].
In this regard, we use the cover tree structure [9, 10] which is pro-
posed for fast ANN searches on large datasets with low intrinsic di-
mensionality and with a search complexityO(log(d)) in time, where
d is the dataset population (in contrast with the linear complexity of
an exact exhaustive search O(d)).
We apply our results to accelerate the Magnetic Resonance
Fingerprint (MRF) recovery from under-sampled k-space measure-
ments; an emerging field of study in computational medical imag-
ing [11]. Since the MR fingerprints belong to the low-dimensional
manifold of Bloch dynamic equations, we show that using the cover
tree ANN search significantly shortcuts the computations compared
to an exact IPG with exhaustive searches as in [12].
2. PRELIMINARIES
The following embedding assumption plays a critical role in sta-
bility analysis of the (convex/nonconvex) IPG algorithm [13, 5, 8]:
Definition 1. A is Bi-Lipschitz w.r.t. C, if ∀x, x′ ∈ C there exists
constants 0 < α 6 β such that
α||x− x′||2 6 ||A(x− x′)||2 6 β||x− x′||2 (4)
Dealing with complicated signal models, the projection step can
bring a huge computational burden at each iteration, and thus a nat-
ural line of thought is to perform this step with cheaper (available)
approximations. The inexact IPG takes the following form:
xk = PεC
(
xk−1 − µAH(Axk − y)
)
. (5)
ar
X
iv
:1
70
6.
07
83
4v
1 
 [
st
at
.M
L
] 
 2
3 
Ju
n 
20
17
We consider the class of (1 + ε)-approximate projections: for any
ε > 0:
PεC(x) ∈
{
u ∈ C : ||u− x|| 6 (1 + ε)||PC(x)− x||
}
. (6)
The following result states that if the embedding holds (with a better
conditioning compared to the exact IPG case ε = 0) then the inexact
algorithm still achieves a solution accuracy comparable to the exact
IPG [8]:
Theorem 1. Assume (A, C) is bi-Lipschitz, x0 ∈ C and that√
ε+ ε2 6 δ
√
α/|||A||| and β < (2− 2δ + δ2)α
for ε > 0 and some constant δ ∈ [0, 1). Set the step size(
(2− 2δ + δ2)α
)−1
< µ 6 β−1. The sequence generated by
Algorithm (5) obeys the following bound:
||xk − x0|| 6 ρk||x0||+
κw
1− ρw
where
ρ =
√
1
µα
− 1 + δ, κw = 2
√
β
α
+
√
µδ, w = ||y −Ax0||.
Remark 1. Increasing ε slows down the rate ρ of linear convergence,
however the iterations are potentially cheaper. Too large approxima-
tions ε may result in divergence.
Remark 2. After a finite K = O(log(τ−1)) number of iterations
Algorithm (5) achieves the solution accuracy ||xK −x0|| = O(w) +
τ , for any τ > 0.
3. DATA DRIVEN CS IN PRODUCT SPACE
Data driven CS corresponds to cases where in the absence of an
algebraic (semi-algebraic) physical model one resorts to collecting
a large number of data samples in a dictionary and use it e.g. as a
point cloud model for CS reconstruction [3]. Data driven CS finds
numerous applications e.g. in hyperspectral imaging (HSI) [14],
mass spectroscopy (e.g. MALDI imaging) [15] and MRF recov-
ery [11, 12] just to name a few. For instance the USGS Hyperspectral
library1 contains the spectral signatures of thousands of substances.
This side-information is shown to enhance CS recovery and classifi-
cation [14]. Here the model is discrete and nonconvex:
C̃ :=
d⋃
i=1
{ψi} ∈ Cñ (7)
and it corresponds to atoms of a (possibly complex-valued) dictio-
nary Ψ customized for e.g. HSI, MALDI or MRF data.
A multichannel image such as HSI, MALDI or MRF can be rep-
resented by a ñ × J matrix X , where n = ñJ is the total number
of pixels, J is the spatial resolution and ñ is the number of channels.
Under the pixel purity assumption each spatial pixel corresponds to a
certain material with specific signature. Considering a notion of sig-
nal intensity in this model (e.g. due to variable illumination condi-
tions in HSI, or a non-uniform proton density in MRF), the columns
ofX belong to the cone associated with the signatures (7). Denoting
by Xj the jth column of X (i.e. a multi-dimensional spatial pixel),
we have
Xj ∈ cone(C̃), ∀j = 1, . . . , J, (8)
1http://speclab.cr.usgs.gov
where the cone is also a discrete set defined as
cone(C̃) := {x ∈ Cñ : x/γ ∈ C̃ for some γ > 0}.
Here, γ corresponds to the signal intensity (per spatial pixel).
The CS sampling and reconstruction follows (1) and (2) by set-
ting x∗ := Xvec (by Xvec we denote the vector-rearranged form of
the matrix X) and the only update is that now the solution lives in a
product space of the same model i.e.
C :=
J∏
j=1
cone(C̃) (9)
In a single dimensional setting i.e. C = cone(C̃), there will be
no need for an IPG algorithm to solve problem (2). If the embedding
holds with a α > 0 (i.e. an identifiability condition) then the atom
with a maximum coherence 〈Aψi/||Aψi||, y〉with the measurements
identifies the correct signature in a single iteration. However such a
direct approach in a multi-dimensional setting J  1 (e.g. when
the sampling model non-trivially combines columns ofX) generally
has an exponential O(dJ) complexity because of the combinatorial
nature of the product space constraints. A tractable scheme which
has been frequently considered for this case e.g. in [12] would be an
IPG type algorithm (3).
After the gradient update Zvec = Xkvec + µAH(y − AXkvec), the
projection onto the product model (9) decouples into separate cone
projections for each spatial pixel j. The cone projectionPcone(C̃)(Zj)
follows the steps below:
i∗ = argmini ||Zj − ψi/||ψi|||| (NN search) (10)
Xk+1j = Pcone(C̃)(Zj) = γjψi∗ (rescaling) (11)
where, γj = max
(
real(〈Zj , ψi∗〉)/||ψi∗ ||2, 0
)
is the per-pixel sig-
nal intensity.
As a result, the IPG algorithm breaks down the computations
into local gradient and projection updates for which an exact projec-
tion step e.g. by using a brute-force nearest neighbour (NN) search,
has complexity O(Jd) in time.
4. COVER TREE FOR FAST NEAREST NEIGHBOUR
SEARCH
With the data driven CS formalism and discretization of the
model the projection step of IPG reduces to a search for the near-
est atom in each of the product spaces, however in a potentially very
large d size dictionary. And thus search strategies with linear com-
plexity in d e.g. an exhaustive search, can be a serious bottleneck for
solving such problems. A very well-established approach to over-
come the complexity of an exhaustive NN search on a large dataset
consists of hierarchically partitioning the solution space and forming
a tree structure whose nodes representing those partitions, and then
using a branch-and-bound method on the resulting tree for a fast Ap-
proximate NN (ANN) search with o(d) complexity. In this regard,
we address the computational shortcoming of the projection step in
the exact IPG by preprocessing C̃ and form a cover tree structure
suitable for fast ANN searches [9, 10].
A cover tree is a levelled tree whose nodes (i.e. associated data
points) at different scales form covering nets for data points at mul-
tiple resolutions. Denote by Sl the set of nodes appearing at scale
l = 1, . . . , Lmax and by σ := maxψ∈C̃ ||ψroot−ψ|| the maximal cov-
erage by the root, then a cover tree structure must have the following
three properties:
Algorithm 1: (1 + ε)-ANN(cover tree T , query point p, cur-
rent estimate qc ∈ C̃), accuracy parameter ε
Q0 = {S0}, S0 the root of T , dmin = ||p− qc||
l = 0
while l < Lmax AND σ2−l+1(1 + ε−1) > dmin do
Q = {children(q) : q ∈ Ql}
q∗ = argminq∈Q ||p− q||, d = ||p− q∗||
if d < dmin then
dmin = d, qc = q
∗
end
Ql+1 = {q ∈ Q : ||p− q|| 6 dmin +maxdist(q)}
l = l + 1
end
return qc
1. Nesting: Sl ⊆ Sl+1, once a point p appears as a node in Sl,
then every lower level in the tree has that node.
2. Covering: every node q ∈ Sl+1 has a parent node p ∈ Sl,
where ||p − q|| 6 σ2−l. As a result, covering becomes finer
at higher scales in a dyadic fashion.
3. Separation: nodes belonging to the same scale are separated
by a minimal distance which dyadically shrinks at higher
scales i.e. ∀q, q′ ∈ Sl we have ||q − q′|| > σ2−l.
Each node p also keeps the maximum distance to its descendants
denoted by maxdist(q) := maxq′∈descendant(q) ||q − q′|| 6 σ2−l+1.
which will be useful for fast NN/ANN search.2 The explicit tree
representation requires the storage space O(d) [9].
Definition 2. Given a query p, q ∈ C̃ is a (1 + ε)-ANN of p for an
ε > 0 if it holds: ||p− q|| 6 (1 + ε) infu∈C̃ ||p− u||.
Algorithm 1 details the branch-and-bound procedure for (1+ε)-
ANN search on a given cover tree (proof of correctness is available
in [9]). In short, we iteratively traverse down the cover tree and at
each scale we populate the set of candidates Ql with nodes which
could be the ancestors of the solution and discard others (this re-
finement uses the triangular inequality and the lower bound on the
distance of the grandchildren of Q to p, based on maxdist(q)). At
the finest scale (before stoppage) we search the whole set of final
candidates and report an (1 + ε)-ANN point. Note that at each scale
we only compute distances for non self-parent nodes (we pass, with-
out computation, distance information of the self-parent children to
finer scales). The case ε = 0 refers to the exact tree NN search
where one has to continue Algorithm 1 until the finest level of the
tree. One should distinguish between this strategy and performing a
brute-force search. Although they both perform an exact NN search,
the complexity of Algorithm 1 is empirically shown to be way less
in practical datasets.
Although the cover tree construction is blind to the explicit
structure of data, several key growth properties such as the tree’s
explicit depth, the number of children per node, and importantly
the overall search complexity are characterized by the intrinsic
doubling dimension of the model [10]. Practical datasets are often
assumed to have small doubling dimensions e.g. when C̃ ⊆ M
samples a K-dimensional manifoldM with certain regularity, one
has dimD(C̃) 6 dimD(M) = O(K) [16]. The following theorem
bounds the complexity of (1 + ε)-ANN cover tree search [10, 9]:
2Note that any node q ∈ Sl due to the covering property satisfies
maxdist(q) 6 σ
(
2−l + 2−l−1 + 2−l−2 + . . .
)
< σ2−l+1 and thus,
one might avoid saving maxdist values and use this upper bound instead.
Theorem 2. Given a query which might not belong to C̃, the approx-
imate (1 + ε)-ANN search on a cover tree takes at most
2O(dimD(C̃)) log ∆ + (1/ε)O(dimD(C̃)) (12)
computations in time with O(#C̃) memory requirement, where ∆ is
the aspect ratio of C̃.
For most applications log(∆) = O(log(d)) [10] and thus for
datasets with low dimensional structures i.e. dimD = O(1) and
by using moderate approximations one achieves a search complexity
logarithmic in d, as opposed to the linear complexity of a brute-force
search.
Note that the complexity of an exact cover tree search could be
arbitrary high i.e. linear in d, at least in theory (unless the query
belongs to the dataset [9] which does not generally apply e.g. for
the intermediate steps of the IPG). However in our experiments we
empirically observe that the complexity of an exact cover tree NN is
still much lower than performing an exhaustive search.
4.1. Inexact data driven IPG algorithm
We use cover tree’s (1 + ε)-ANN approximate search to effi-
ciently implement the cone projection step of the data driven IPG
algorithm. We only replace the exact search step (10) with the fol-
lowing approximation:
ψi∗ = (1 + ε)-ANN
(
T , Zj||Zj ||
,
Xkj
γj
)
, ∀j = 1, . . . , J. (13)
We denote by T the cover tree structure built for the normalized dic-
tionary with atoms ψi/||ψi||. For the current estimate we use the
atom previously selected i.e. Xkj /γj for all j. We additionally nor-
malize the gradient update Zj for the search input. This step may
sound redundant however since all dictionary atoms are normalized
and live on the hypersphere, we empirically observed that this trick
leads to a better acceleration.
As a result by this update we obtain a (1 + ε)-approximate cone
projection denoted by Pεcone(C̃)(Zj) satisfying definition (6). There-
fore supported with an embedding assumption w.r.t. to the product
model (9) and according to Theorem 1 we can guarantee stable CS
recovery using this algorithm with a small computational effort.
5. APPLICATION IN QUANTITATIVE MRI
In quantitative MRI rather than simply forming a single MR im-
age with only contrast information (i.e. qualitative MRI), physicists
are interested in measuring the NMR properties of tissues namely,
the T1, T2 relaxation times, δf magnetic resonance and proton den-
sity (PD), and use these parameters to differentiate different biolog-
ical tissues [17]. The standard approach for parameter estimation is
to acquire a large sequence of images in different times from a sim-
ple excitation pulse (i.e. excitations in terms of rotating the magnetic
field) and then use an exponential model curve fitting to recover the
decay exponents T1, T2 for each voxel. This procedure runs sep-
arately to estimate each relaxation time. The long process of ac-
quiring multiple fully sampled images, brings serious limitation to
standard approaches in quantitative imaging to apply within a rea-
sonable time and with an acceptable signal-to-noise ratio (SNR) and
resolution.
5.1. Magnetic Resonance Fingerprinting
Recently, a novel parameter estimation process coined as the
Magnetic Resonance Fingerprinting (MRF) has been proposed to
address this shortcoming. For the acquisition, MRF applies a shorter
sequence of random excitations (i.e. Γ ∈ [0, π/2]ñ random flip an-
gels with typically ñ ≈ 300 − 1000 and the time interval TR ∼
10− 40 msec) for estimating all parameters T1, T2, δf , and proton
density at once. After each excitation pulse the response is recorded
through the measurements taken from a small portion of k-space. As
a result the acquisition time can significantly decreases to a couple
of minutes [11].
The exponential model as for the traditional acquisition se-
quences will no longer hold for the MRF model. Due to using ran-
dom excitations, the signatures casts a non-trivial implicit expression
readable by solving the Bloch dynamic equations B(Γ,Θ) ∈ Cñ
customized for the set of parameters Θ = {T1, T2, δf}. The MRF
problem is an example of data driven CS model with an algebraic
signal model (i.e. the Bloch equations) but a non-trivial projection.
For this problem one constructs off-line a dictionary of fingerprints
(i.e. magnetization responses) for all possible d = #{Θi}i param-
eters presented in normal tissues where the atoms ψi := B(Γ,Θi)
uniquely represent the underlying set of parameters Θi . This corre-
sponds to sampling a low-dimensional manifold associated with the
solutions of the Bloch dynamic equations [12], which for compli-
cated excitation patterns neither the response nor the projection has
an analytic solution.
5.2. MRF image model and parameter estimation
An MRF image can be represented by a matrix X ∈ Cñ×J .
Each columnXj represents a ñ-dimensional spatial pixel j and each
row Xl represents an image slice (with the spatial resolution equal
to J) corresponding to the measurements taken after the excitation
pulse Γl, ∀l = 1, . . . , ñ. By the pixel purity assumption, the image
follows the product space data driven model (7) and (8) where each
column/pixel j correspond to a single fingerprint ψ with a unique
parameter Θ. The cone(C̃) corresponds to the fingerprints dictionary
and ρ > 0 represents the proton density.3 The MRF acquisition
model is linear as follows:
Y l ≈ FΩl(X
l), ∀l = 1, . . . ñ, (14)
where F denotes the two-dimensional Fourier transform, and FΩl :
CJ → Cm/ñ subsamples the k-space coefficients of the image slice
Xl according a pattern/set Ωl. The k-space subsampling could cor-
respond to a spiral pattern [11] or a random horizontal/vertical line
subselection [12] e.g. in the Echo Planar Imaging (EPI). By vector-
ization of X,Y , we have a product space CS model similar to (1)
for a sampling protocol A acting spatially however with different
k-space sampling pattern per image slice.
The CS reconstruction proposed by [12] is based on a data driven
IPG algorithm (with the exact cone projection described in Sec-
tion 3) where one by identifying the correct fingerprint per pixel
Xj can recover the underlying parameters Θ stored in a look-up
table (the last rescaling step also recovers the proton density γj).
It has been shown in [12, Theorem 1] that if Ωl independently at
random subselects the rows (or columns) of the k-space, which cor-
responds to a random EPI sampling protocol, the resulting forward
3The proton density can be generally a complex number for which only
the rescaling step of the cone projection slightly changes and the rest of the
analysis similarly holds, for more details see [12].
(a) Flip angles (excitations) sequence
(b) Normalized MRF dictionary atoms (the real components)
Fig. 1: TrueFISP flip angles and the corresponding normalized
Bloch responses (fingerprints).
map A is bi-Lipschitz w.r.t. the product model C =
∏J
j=1 cone(C̃),
provided sufficient discriminations between the fingerprints.4 This
result guarantees stability of the parameter estimation using this al-
gorithm with an step size µ = n/m.
Since the Bloch manifold is parametrized by only three param-
eters in Θ and thus has a low intrinsic dimension, we use the cover
tree ANN searches to accelerate the projection step. We perform the
following procedure iteratively until convergence:
(Xl)k+1 = Pεcone(C̃)
(
(Xl)k − µFHΩl
(
FΩi((X
l)k)− Y l
))
The approximate cone projection follows the update in (13).
Provided with the same embedding result (for the random EPI
acquisition protocol) and according to Theorem 1 we can deduce the
linear convergence of this algorithm to the true solution for mildly-
chosen approximations ε (and the same step size µ = n/m) where
the projection step has now the complexity O(J log(d)). For large-
sized dictionaries the gap between the computation costs of the exact
and inexact IPG algorithm can be significantly large.
6. NUMERICAL EXPERIMENTS
We investigate application of our proposed scheme for acceler-
ating the MRF reconstruction. Parameter estimation consists of two
4One achieves a suitable discrimination between the MR fingerprints by
e.g. choosing the random excitation sequence long enough.
(a) Cover tree segments at scale 2 (b) Cover tree segments at scale 3 (c) Cover tree segments at scale 4 (d) Cover tree segments at scale 5
Fig. 2: A cover tree is formed on the Bloch response manifold composed of 14 scales: (a-d) data partitions are highlighted in different colours
and demonstrated for scales 2-5. Low-scale partitions divide into finer segments by traversing down the cover tree i.e. increasing the scale.
off-line steps: i) forming the fingerprints dictionary by solving the
Bloch dynamic equations for a wide range of parameters. ii) building
a cover tree structure on the resulting dictionary which enables using
fast ANN searches within the iterations of the inexact IPG algorithm.
We construct a cover tree on a dictionary composed of d =
48682 fingerprints which are sampled from the Bloch response of d
pairs of (T1, T2) relaxation times ranging between 100-5000 (ms)
and 20-1800 (ms), respectively.5 Fingerprints ψi ∈ C1024 are nor-
malized magnetization responses of the corresponding parameters
to a balanced steady-state free-precession (bSSFP or TrueFISP) se-
quence of 1024 slowly varying flip angles (excitations) ranging from
0◦ to 60◦, shown in Figure 1. From Figure 2 we can observe the
MRF low dimensional manifold across its first three principal com-
ponents. The hierarchical data partitions resulted by the cover tree
is also depicted for four scales (the tree has total 14 scales with the
finest coverage resolution∼ 10−4). The ground truth image is a syn-
thetic 256 × 256 brain phantom with six (T1, T2) segments previ-
ously used for evaluations in [12]. Figure 3 shows the corresponding
segments, T1, T2 and proton density maps. This phantom is syn-
thesized with the corresponding Bloch responses and proton density
according to model (8) to build the magnetization image X . The
k-space subsampling is based on a lattice-based non random EPI
protocol (i.e. uniform row subselection with a shifted pattern for
different image slices).
We apply the (1 + ε)-ANN tree search for different values of
ε = {0, 0.2, 0.4, 0.6, 0.8}. We compare the performances of the ex-
act and inexact data driven IPG on this synthetic phantom. We only
report the cost of projections i.e. the total number of pairwise dis-
tances calculated for performing the NN or ANN searches.6 Results
are depicted in Figure 4 and indicate that the inexact algorithm (for
a suitable parameter ε) achieves a similar level of accuracy as for the
exact IPG (with an exhaustive search) by saving 2-3 orders of mag-
nitude in computations. Using an exact tree search (i.e. ε = 0) also
saves computations by an order of magnitude compared to using a
brute-force search. Remarkably, for subsampling ratio 8 : 1 the solu-
tion accuracy of the approximate scheme is about an orders of mag-
nitude better than the exact IPG. Despite our theoretical results do
not cover such observation, we shall relate it to a common practical
knowledge that using relaxations, e.g. here approximations, gener-
ally improves the performance of nonconvex algorithms compared to
making hard decisions, and introduces a notion of robustness against
undesirable local minima in such settings.
5In our experiments we set the off-resonance frequency parameter to zero,
the repetition time TR = 37 (ms) and the echo time TE = TR/2.
6Since the MRF’s forward/adjoint sampling operator is based on the Fast
Fourier Transform, the gradient updates cost a tiny fraction of the search step
(particularly when dealing with large-size datasets) and thus we exclude them
from our evaluations.
(a) Brain phantom segments (b) T1 map
(c) T2 map (d) Proton density map
Fig. 3: Anatomical brain phantom. Segments correspond to Back-
ground, CSF, Grey Matter, White Matter, Muscle, Skin.
7. CONCLUSIONS
We proposed an inexact IPG algorithm for data driven CS prob-
lem with an application to fast MRF parameter estimation. The pro-
jection step is implemented approximately by using cover trees ANN
searches. Thanks to the IPG’s robustness against approximations, by
using this algorithm one achieves a comparable (and sometimes em-
pirically enhanced) solution precision to the exact algorithm how-
ever with significantly less amount of computations regarding itera-
tively searching through large datasets.
References
[1] D. L. Donoho, “Compressed sensing,” IEEE Transactions on
Information Theory, vol. 52, no. 4, pp. 1289–1306, Apr. 2006.
[2] E. J. Candes, J. Romberg, and T. Tao, “Robust uncertainty prin-
ciples: exact signal reconstruction from highly incomplete fre-
quency information,” IEEE Transactions on Information The-
ory, vol. 52, no. 2, pp. 489–509, Feb 2006.
[3] R. G. Baraniuk, V. Cevher, and M. B. Wakin, “Low-
dimensional models for dimensionality reduction and signal
recovery: A geometric perspective,” Proceedings of the IEEE,
vol. 98, no. 6, pp. 959–971, June 2010.
104 105 106 107 108
Total NN cost
10-6
10-5
10-4
10-3
N
or
m
al
iz
ed
 s
ol
ut
io
n 
M
S
E
Subsampling8:1
Exact tree NN
(1+ǫ)-ANN
Brute force
104 105 106 107 108
Total NN cost
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
A
ve
ra
ge
 T
1 
er
ro
r 
(m
s)
Subsampling8:1
Exact tree NN
(1+ǫ)-ANN
Brute force
104 105 106 107 108
Total NN cost
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
A
ve
ra
ge
 T
2 
er
ro
r 
(m
s)
Subsampling8:1
Exact tree NN
(1+ǫ)-ANN
Brute force
105 106 107 108 109
Total NN cost
10-4
10-3
10-2
10-1
N
or
m
al
iz
ed
 s
ol
ut
io
n 
M
S
E
Subsampling16:1
Exact tree NN
(1+ǫ)-ANN
Brute force
105 106 107 108 109
Total NN cost
0
50
100
150
200
250
300
A
ve
ra
ge
 T
1 
er
ro
r 
(m
s)
Subsampling16:1
Exact tree NN
(1+ǫ)-ANN
Brute force
105 106 107 108 109
Total NN cost
0
2
4
6
8
10
12
14
A
ve
ra
ge
 T
2 
er
ro
r 
(m
s)
Subsampling16:1
Exact tree NN
(1+ǫ)-ANN
Brute force
Fig. 4: Normalized solution MSE (i.e. ||x̂−x0||||x0|| ) and average T1, T2 errors (i.e.
1
J
∑J
j=1 |T̂1(j)− T1(j)| and
1
J
∑J
j=1 |T̂2(j)− T2(j)|) vs.
computations cost (projection step only), for two subsampling regimes.
[4] V. Cevher, S. Becker, and M. Schmidt, “Convex optimization
for big data: Scalable, randomized, and parallel algorithms for
big data analytics,” IEEE Signal Processing Magazine, vol. 31,
no. 5, pp. 32–43, Sept 2014.
[5] T. Blumensath, “Sampling and reconstructing signals from a
union of linear subspaces,” IEEE Transactions on Information
Theory, vol. 57, no. 7, pp. 4660–4671, July 2011.
[6] G. Tang, B. N. Bhaskar, and B. Recht, “Sparse recovery over
continuous dictionaries-just discretize,” in 2013 Asilomar Con-
ference on Signals, Systems and Computers, 2013, pp. 1043–
1047.
[7] M. Golbabaee, A. Alahi, and P. Vandergheynst, “Scoop:
A real-time sparsity driven people localization algorithm,”
Journal of Mathematical Imaging and Vision, vol. 48, no. 1,
pp. 160–175, Jan. 2014.
[8] M. Golbabaee and M. E. Davies, “Inexact gradient projec-
tion for fast data driven compressed sensing,” Technical report,
2017.
[9] A. Beygelzimer, S. Kakade, and J. Langford, “Cover trees for
nearest neighbor,” in Proceedings of the 23rd international
conference on Machine learning. ACM, 2006, pp. 97–104.
[10] R. Krauthgamer and J. R. Lee, “Navigating nets: Simple algo-
rithms for proximity search,” in Proceedings of the Fifteenth
Annual ACM-SIAM Symposium on Discrete Algorithms, ser.
SODA ’04, 2004.
[11] D. Ma, V. Gulani, N. Seiberlich, K. Liu, J. Sunshine, J. Durek,
and M. Griswold, “Magnetic resonance fingerprinting,” Na-
ture, vol. 495, no. 7440, pp. 187–192, 2013.
[12] M. Davies, G. Puy, P. Vandergheynst, and Y. Wiaux, “A com-
pressed sensing framework for magnetic resonance fingerprint-
ing,” SIAM Journal on Imaging Sciences, vol. 7, no. 4, pp.
2623–2656, 2014.
[13] R. G. Baraniuk and M. B. Wakin, “Random projections of
smooth manifolds,” Foundations of Computational Mathemat-
ics, vol. 9, no. 1, pp. 51–77, 2009.
[14] M. Golbabaee, S. Arberet, and P. Vandergheynst, “Compres-
sive source separation: Theory and methods for hyperspectral
imaging,” IEEE Transactions on Image Processing, vol. 22,
no. 12, pp. 5096–5110, Dec 2013.
[15] J. H. Kobarg, P. Maass, J. Oetjen, O. Tropp, E. Hirsch, C. Sagiv,
M. Golbabaee, and P. Vandergheynst, “Numerical experiments
with maldi imaging data,” Advances in Computational
Mathematics, vol. 40, no. 3, pp. 667–682, 2014.
[16] S. Dasgupta and Y. Freund, “Random projection trees and low
dimensional manifolds,” in Proceedings of the fortieth annual
ACM symposium on Theory of computing. ACM, 2008, pp.
537–546.
[17] P. Tofts, Quantitative MRI of the brain: measuring changes
caused by disease. John Wiley & Sons, 2005.

