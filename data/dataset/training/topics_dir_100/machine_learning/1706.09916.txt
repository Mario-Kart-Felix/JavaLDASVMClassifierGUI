ar
X
iv
:1
70
6.
09
91
6v
1 
 [
cs
.L
G
] 
 2
9 
Ju
n 
20
17
Graph Convolutional Networks for Molecules
Zhenpeng Zhou
Stanford University
zhenpeng@stanford.edu
Abstract
Representation learning for molecules is important for
molecular properties prediction, material design, drug
screening, etc. In this work a graph convolutional network
architecture for learning representations for molecules is
presented. An operation for convolving k-neighbourhood of
a specific node in graph is defined, which is corresponding
to kernel size of k in convolutional neural networks. Be-
sides, A module of adaptive filtering is defined to find the
sampling locations based on graph connections and node
features.
1. Introduction:
Representation learning for molecules is important for
molecular properties prediction, material design, drug
screening, etc. One of the most common representations
is based on “hand made” 2D molecule and atom descrip-
tors, such as extended connectivity fingerprints (ECFP).
[16] Without the capability of learning, they are not pow-
erful enough for many tasks.
Convolutional neural networks have been successful in
computer vision and natural language processing. Recently,
convolutional neural network has been extend to graphs in
addition to pictures, making it ideal for learning representa-
tions for molecules.[10, 14, 3, 4, 2] However, we still need
to define efficient operation for graph convolution with a
kernel size larger than one, which allow us to consider more
nodes that has connection with the central node besides the
nearest neighbors.
In this work, a graph convolutional network architecture
is used for molecule representation learning. The main con-
tributions in this work can be summarized as follows:
• An operation for convolving k-neighbourhood of a
specific node in graph, which is corresponding to ker-
nel size k in convolutional neural networks, is defined.
• A adaptive filtering module is introduced, in order to
find sampling locations based on graph connections
and node features.
• A new network structure, which concatenates features
from different filter sizes instead of summing them up,
is employed.
2. Related Works
Bruna et al. [3] introduced a model to generalize convo-
lutional neural networks to graph convolutions, which was
demonstrated on simple low dimensional datasets.
Henaff et al. [10] extended the idea form Bruna et al.
to large scale datasets like ImageNet Object Recognition,
text categorization, and bioinformations. They have also
considered the general setting where no prior information
in available.
Niepert et al. [2] proposed a approach of PATCHY-SAN,
which defines operations of node sequence selection, neigh-
borhood assembly, and graph normalization. Their neigh-
borhood assembly operation is still based on computations
that are not parallelizable on GPU.
Defferrard et al. [4] has employed spectral filter on
graphs, and graph coarsening operations. They defined
spectral filtering of graph signals based on Chebyshev poly-
nomials, where a fast k-neighborhood summation is de-
fined. However, that operation is not exactly graph convo-
lution with filter size of k, as some nodes are counted more
than once in Chebyshev polynomial formulations.
Kipf et al. [12] has defined first-order approximation for
graph convolutions. They defined their task as node classi-
fication within networks of significantly larger scale. They
introduced several simplifications to Bruna et al. and Def-
ferrard et al.
Duvenaud et al. [6] has developed neural fingerprints
for learning molecule fingerprints based on the process of
constructing ECFP but in a learnable way. They have intro-
duce the operation for convolving k-neighborhood of a spe-
cific node, but their operation is not parallelizable on GPU,
which makes their computation slow, “on the order of an
hour” in their words.
Kearnes et al. [11] has applied graph convolutions on
molecules. They have defined graph convolution start from
invairant-perserving operations. But they did not describe
convolution operations for neighbor with order 2 or higher.
1
Gómez-Bombarelli et al. [8] has designed variational au-
toencoder for molecules with smiles representations. With-
out using graph convolutions, their method is an extension
to molecule fingerprinting methods.
3. Technical Approach
3.1. Kth order graph convolution
Molecules can be modeled as graphs, with atoms as
nodes and bonds as edges. Therefore, molecules can be rep-
resented as connected and undirected graph G = {A,X},
where A ∈ Rn×n is the binary adjacency matrix defining
the connection between nodes, in which Aij is the weight
of the connection between node i and j, and X ∈ Rn×m
is the feature of the nodes, in which Xi·is the feature vec-
tor of the ith node. More specifically in terms of molecule,
the adjacency matrix is binary valued, i.e. A ∈ Zn×n, with
Aij = 1 if there is connection between atom i and j, and
Aij = 0 otherwise.
In convolutional neural network for pictures, the defini-
tion of convolutional layer on pixel j with kernel size k is
expressed as
Lconv(j) =
∑
i∈[j−k,j+k]
wixi + b
where wi is the filter weight, b is the filter bias, and k is
the kernel size.
Similarly, the Kth order graph convolution is defined as
weighted summation of nodes which are k-neighborhood of
the interest node. Therefore the graph convolutional layer
can be written as
Lgconv =
(
W ◦ Ãk
)
X + b
where
Ãk = minimum{Ak + I, 1}
is the elementwise minimum of Ak + I and 1, I is the
identity matrix, and k is the “filter size” of graph convolu-
tion, and W , b are filter weight and bias. This can be intu-
itively explained as follows: the element (i, j) of matrix Ak
(i.e. the matrix product of n copies of A) gives the number
of routes within length k from node i to node j. There-
fore, the element (i, j) of matrix Ãk will be one if (i, j) are
k-neighbours, and zero otherwise.
3.2. Adaptive filtering
In application of graph convolution on molecules, it
is desirable that we can concentrate on some part of the
molecule rather than others. For example, benzene rings
are more important than the alkyl chains, as benzene will
determine most properties of the molecule. However, nor-
mal graph convolutions are limited to fixed structures. Here
we introduce adaptive filtering, which will find the convo-
lution targets by learning from the graph connections and
node features.
Adaptive filtering selects the interest targets through a
gate learned from both the connection and features:
g = fadp
(
Ãk, X
)
The intuition of the adaptive filtering comes from the at-
tention mechanism [18], which choose the interest pixels
while generating the corresponding words in the output se-
quence. There can be different functions that can realize the
goal of adptive filtering, some functions are
fadp−gen(A,X) = softmax(AHX)
fadp−con(A,X) = σ ([A,X ]×H)
where [·, ·] means matrix concatenation, σ is sigmoid ac-
tivation function, and H is the parameter. The fadp−con
one was chosen because it shows better performance on real
tasks.
Then the gate is applied to the filter to select targets, with
W̃ = g ◦W
the graph convolution with adaptive filter can be further
modified as
Lgconv =
(
W̃ ◦ Ãk
)
X + b
4. Experiment
4.1. Node classification
We closely follow the experimental setup in the work of
Kipf et al. In the citation network datasets—Citeseer, Cora
and Pubmed—nodes are documents and edges are citation
links. The nodes are seperated into training set, cross val-
idation set, and test set with ratio of 7:1.5:1.5. The results
are summarized in Table 1.
Method Citeseer Cora Pubmed
GCN 0.776 0.889 0.839
HO GCN 0.788 0.901 0.851
Table 1. Node classification results
The hyperparameters are 0.7(dropout rate), 5e-8(L2 reg-
ularization), 128 (hidden units) for the datasets.
4.2. Performance on predictive models
Experimental setup SMILES [17] string encoding of
each molecule is converted into a graph using RDKit[13],
with hydrogen atoms treated implicitly. The initial atom
and bond features were chosen to be the same to those used
2
by Duvenaud et al.[4]: Initial atom features concatenated
a one-hot encoding of the atom’s element, its degree, the
number of attached hydrogen atoms, and the implicit va-
lence, and an aromaticity indicator. The bond features were
a concatenation of whether the bond type was single, dou-
ble, triple, or aromatic, whether the bond was conjugated,
and whether the bond was part of a ring.
Training and Architecture The following network
structure are used in comparison.
name structure
l1 gcn gcn {1,2,3}-ReLU-fc64-ReLU-fc16
l1 adp gcn adp gcn{1,2,3}-ReLU-fc64-ReLU-fc16
l2 gcn [gcn {1,2,3}-ReLU]*2-fc64-ReLU-fc16
where l1 gcn means layer 1 graph convolutional net-
work. l1 adp gcn means layer 1 graph convolutional
network with adaptive filtering, l2 gcn means layer 2
graph convolutional network, gcn {k} means graph convo-
lutional layer with kernel size k. fc means fully connected
layers. It is needed to mention that, the features generated
by graph convolution with different kernel sizes are con-
catenated instead of summed up. TensorFlow [1] was used
to implement the models.
Datasets The same datasets as described in Duvenaud et
al. are used:
• Solubility: The aqueous solubility of 1144 molecules
by [5].
• Drug efficacy: The half-maximal effective concentra-
tion (EC50) in vitro of 10,000 molecules against a
sulfide-resistant strain of P. falciparum, the parasite
that causes malaria, as measured by [7].
• Organic photovoltaic efficiency: The Harvard Clean
Energy Project [9] uses expensive DFT simulations to
estimate the photovoltaic efficiency of 30,000 organic
molecules.
We compared our results with neural fingerprints (NFP)
from Duvenaud et al. [6] and molecular graph convolution
(MGC) from Kearnes et al. [11]The results are shown in
Table 2.
4.3. Performance on generative models.
Training and Architecture Adversarial autoencoder is
applied with graph convolutional networks (gcn) and fully
connected networks (fc). The netowork structures are
name structure
gcn encoder gcn {1,2,3}-ReLU-fc64-ReLU-fc16
gcn decoder fc16-fc64-dconv-ReLU
fc encoder fc64-ReLU-fc32-ReLU-fc16
fc decoder fc16-ReLU-fc32-ReLU-fc64-ReLU
Model
Dataset
Solubility
Drug Photovoltaic
efficacy efficiency
NFP [6] 0.52 1.16 1.43
MGC [11] 0.46 1.07 1.10
l1 gcn 0.41 0.85 0.71
l1 adp gcn 0.30 0.67 0.54
l2 gcn 0.36 0.59 0.45
Table 2. Comparison of GCN to neural fingerprint (NFP) and
molecular graph convolution (MGC) models, with the metric of
RMSE.
where gcn {k} means graph convolutional layer with
kernel size k, fc means fully connected layers, and dconv is
defined as Ldconv = σ
(
AA⊤
)
. It is needed to mention that,
the features generated by graph convolution with different
kernel sizes are concatenated instead of summed up. The
dataset using is the organic photovoltaic efficiency dataset
mentioned above.
The implemenation of distribution classes are taken from
OpenAI git repository [15] with modifications.
The performance of graph convolutional modules and
fully connected modules are compared on a adversarial au-
toencoder. (Figure 1) The latent distribution with encoder
and decoder of graph convolutional network modules shows
a better gaussian shaped distribution, while the one with
fully connected network modules has a breach on the dis-
tribution.
5. Visualizations
5.1. Visualization of convolutional weight
The regression model was trained on the dataset of or-
ganic photovoltaic efficiency dataset mentioned above, and
the convolutional weight matrix was loaded and shown in
Figure 2. It is easy to notice that the weight are almost sym-
metrical, which is because the adjacency matrix A (or Ã) is
symmetrical. Besides, as the filter size increases, the weight
on the central nodes increases as well. This is because as
filter size increases, there are more nodes in the reception
field, the weights of the central nodes increases to balance
the effect of having more nodes.
5.2. Visualization of adaptive filter
The filters learnt from graph connections and node fea-
tures was visualized in Figure 3. The atoms highlighted
with red is the randomly selected central node for convolu-
tion, the blue color on the atoms indicate the filter weight,
with darker blue meaning larger weight. The filter is in-
tended to be a selection of nodes, therefore a sigmoid no-
linearity was added when adaptive filtering is defined, and
the filter weight is almost binarized. In terms of graph con-
volution applied on molecules, the atoms that the filters se-
3
Graph Convolutional networks with adversarial autoencoder Fully connected networks with adversarial autoencoder
Figure 1. The comparison between latent encoding distributions of graph convolutional networks and fully connected networks on adver-
sarial autoencoders.
Figure 2. Visualization of the convolutional weight W for different
filter size in the networks. Darker blue means higher weight.
lected are mostly atoms on aromatic rings, which justified
the assumption that aromatic rings are more important than
alkyl chains in organic photovoltaic efficiency. Another in-
teresting thing is that the filter learnt the orthe-para rule,
which states that on the benzene ring, the functional groups
on the opposite of and next to a specific atom has more ef-
fect on the property of that atom. (See the molecule on
Figure 3 row 2 column 1)
6. Conclusion
In conclusion, an efficient operation for convolving k-
neighborhoods of a specific node in graph is defined. Be-
sides, an adaptive filtering module to learn the convolv-
ing locations from graph connections and node features is
designed. With the two new modules and new network
design, the predictive performance outperforms the state-
of-the-art results. And Graph convolutions also show bet-
ter performance on generative models than fully connected
ones. Visualization of the convolution weights and filters
also helped in understanding graph convolutions.
References
References
[1] M Abadi, P Barham, J Chen, Z Chen, and A Davis.
TensorFlow: A system for large-scale machine learn-
ing. In Proceedings of the 12th . . . , 2016. 4.2
[2] Maria Florina Balcan and Kilian Q Weinberger, ed-
itors. Learning Convolutional Neural Networks for
Graphs, New York, New York, USA, June 2016.
PMLR. 1, 2
4
Figure 3. Visualization of the filter weights. The atoms highlighted with red is the randomly selected central node for convolution, the blue
color on the atoms indicate the filter weight, with darker blue meaning larger weight.
[3] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and
Yann LeCun. Spectral Networks and Locally Con-
nected Networks on Graphs. December 2013. 1, 2
[4] M Defferrard and X Bresson. Convolutional neural
networks on graphs with fast localized spectral filter-
ing. Advances in Neural, pages 3844–3852, 2016. 1,
2, 4.2
[5] John S Delaney. ESOL: Estimating Aqueous Sol-
ubility Directly from Molecular Structure. Jour-
nal of Chemical Information and Computer Sciences,
44(3):1000–1005, March 2004. 4.2
[6] David K Duvenaud, Dougal Maclaurin, Jorge Ipar-
raguirre, Rafael Bombarell, Timothy Hirzel, Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional
Networks on Graphs for Learning Molecular Finger-
prints. pages 2224–2232, 2015. 2, 4.2
[7] Francisco-Javier Gamo, Laura M Sanz, Jaume Vidal,
Cristina de Cozar, Emilio Alvarez, Jose-Luis Lavan-
dera, Dana E Vanderwall, Darren V S Green, Vinod
Kumar, Samiul Hasan, James R Brown, Catherine E
Peishoff, Lon R Cardon, and Jose F Garcia-Bustos.
Thousands of chemical starting points for antimalarial
lead identification. Nature, 465(7296):305–310, May
2010. 4.2
[8] Rafael Gómez-Bombarelli, David Duvenaud,
José Miguel Hernández-Lobato, Jorge Aguilera-
Iparraguirre, Timothy D Hirzel, Ryan P Adams, and
Alan Aspuru-Guzik. Automatic chemical design
using a data-driven continuous representation of
molecules. October 2016. 2
[9] Johannes Hachmann, Roberto Olivares-Amaya, Sule
Atahan-Evrenk, Carlos Amador-Bedolla, Roel S
Sánchez-Carrera, Aryeh Gold-Parker, Leslie Vogt,
Anna M Brockway, and Alan Aspuru-Guzik. The
Harvard Clean Energy Project: Large-Scale Com-
putational Screening and Design of Organic Photo-
voltaics on the World Community Grid. The Journal
of Physical Chemistry Letters, 2(17):2241–2251, Au-
gust 2011. 4.2
5
[10] Mikael Henaff, Joan Bruna, and Yann LeCun. Deep
Convolutional Networks on Graph-Structured Data.
June 2015. 1, 2
[11] Steven Kearnes, Kevin McCloskey, Marc Berndl, Vi-
jay Pande, and Patrick Riley. Molecular graph con-
volutions: moving beyond fingerprints. Journal of
Computer-Aided Molecular Design, 30(8):595–608,
2016. 2, 4.2
[12] Thomas N Kipf and Max Welling. Semi-Supervised
Classification with Graph Convolutional Networks.
September 2016. 2
[13] G Landrum. RDKit: Open-source Cheminformatics.
2014. 4.2
[14] Y Li, D Tarlow, M Brockschmidt, and R Zemel. Gated
graph sequence neural networks. arXiv.org, 2015. 1
[15] OpenAI. Infogan,
https://github.com/openai/infogan/blob/master/infogan/misc/distributions.py,
2016. 4.3
[16] David Rogers and Mathew Hahn. Extended-
Connectivity Fingerprints. Journal of Chemical In-
formation and Modeling, 50(5):742–754, 2010. 1
[17] D Weininger. SMILES, a chemical language and in-
formation system. 1. Introduction to methodology and
encoding rules. . . . of chemical information and com-
puter sciences, 1988. 4.2
[18] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. Show, Attend and Tell: Neu-
ral Image Caption Generation with Visual Attention.
In Francis Bach and David Blei, editors, Proceed-
ings of the 32nd International Conference on Machine
Learning, pages 2048–2057, Lille, France, July 2015.
PMLR. 3.2
6

