ar
X
iv
:1
70
6.
07
88
0v
1 
 [
st
at
.M
L
] 
 2
3 
Ju
n 
20
17
Collaborative Deep Learning in
Fixed Topology Networks
Zhanhong Jiang1, Aditya Balu1, Chinmay Hegde2, and Soumik Sarkar1
1Department of Mechanical Engineering, Iowa State University,
zhjiang, baditya, soumiks@iastate.edu
2Department of Electrical and Computer Engineering , Iowa State University,
chinmay@iastate.edu
June 27, 2017
Abstract
There is significant recent interest to parallelize deep learning algorithms in order to handle the
enormous growth in data and model sizes. While most advances focus on model parallelization and
engaging multiple computing agents via using a central parameter server, aspect of data parallelization
along with decentralized computation has not been explored sufficiently. In this context, this paper
presents a new consensus-based distributed SGD (CDSGD) (and its momentum variant, CDMSGD)
algorithm for collaborative deep learning over fixed topology networks that enables data parallelization
as well as decentralized computation. Such a framework can be extremely useful for learning agents
with access to only local/private data in a communication constrained environment. We analyze the
convergence properties of the proposed algorithm with strongly convex and nonconvex objective functions
with fixed and diminishing step sizes using concepts of Lyapunov function construction. We demonstrate
the efficacy of our algorithms in comparison with the baseline centralized SGD and the recently proposed
federated averaging algorithm (that also enables data parallelism) based on benchmark datasets such as
MNIST, CIFAR-10 and CIFAR-100.
1 Introduction
In this paper, we address the scalability of optimization algorithms for deep learning in a distributed setting.
Scaling up deep learning [1] is becoming increasingly crucial for large-scale applications where the sizes of both
the available data as well as the models are massive [2]. Among various algorithmic advances, many recent
attempts have been made to parallelize stochastic gradient descent (SGD) based learning schemes across
multiple computing agents. An early approach called Downpour SGD [3], developed within Google’s disbelief
software framework, primarily focuses on model parallelization (i.e., splitting the model across the agents).
A different approach known as elastic averaging SGD (EASGD) [4] attempts to improve perform multiple
SGDs in parallel; this method uses a central parameter server that helps in assimilating parameter updates
from the computing agents. However, none of the above approaches concretely address the issue of data
parallelization, which is an important issue for several learning scenarios: for example, data parallelization
enables privacy-preserving learning in scenarios such as distributed learning with a network of mobile and
Internet-of-Things (IoT) devices. A recent scheme called Federated Averaging SGD [5] attempts such a
data parallelization in the context of deep learning with significant success; however, they still use a central
parameter server.
In contrast, deep learning with decentralized computation can be achieved via gossip SGD algorithms [6,
7], where agents communicate probabilistically without the aid of a parameter server. However, decentralized
1
Table 1: Comparisons between different optimization approaches
Method f ∇f Step Size Con.Rate D.P. D.C. C.C.T.
SGD Str-con Lip. Con. O(γk) No No No
Downpour SGD [3] Nonconvex Lip. Con.&Ada. N/A Yes No No
EASGD [4] Str-con Lip. Con. O(γk) No No No
Gossip SGD [7]
Str-con Lip.&Bou. Con. O(γk)
No Yes No
Str-con Lip.&Bou. Dim. O( 1
k
)
FedAvg [5] Nonconvex Lip. Con. N/A Yes No No
CDSGD [This paper]
Str-con Lip.&Bou. Con. O(γk)
Yes Yes Yes
Str-con Lip.&Bou. Dim. O( 1
kǫ
)
Nonconvex Lip.&Bou. Con. N/A
Nonconvex Lip.&Bou. Dim. N/A
Con.Rate: convergence rate, Str-con: strongly convex. Lip.&Bou.: Lipschitz continuous and
bounded. Con.: constant and Con.&Ada.: constant&adagrad. Dim.: diminishing. γ ∈ (0, 1)
is a positive constant. ǫ ∈ (0.5, 1] is a positive constant. D.P.: data parallelism. D.C.:
decentralized computation. C.C.T.: constrained communication topology.
computation in the sense of gossip SGD is not feasible in many real life applications. For instance, consider
a large (wide-area) sensor network [8, 9] or multi-agent robotic network that aims to learn a model of the
environment in a collaborative manner [10, 11]. For such cases, it may be infeasible for arbitrary pairs of
agents to communicate on-demand; typically, agents are only able to communicate with their respective
neighbors in a communication network in a fixed (or evolving) topology.
Contribution: This paper introduces a new class of approaches for deep learning that enables both
data parallelization and decentralized computation. Specifically, we propose consensus-based distributed
SGD (CDSGD) and consensus-based distributed momentum SGD (CDMSGD) algorithms for collaborative
deep learning that, for the first time, satisfies all three requirements: data parallelization, decentralized
computation, and constrained communication over fixed topology networks. Moreover, while most existing
studies solely rely on empirical evidence from simulations, we present rigorous convergence analysis for
both (strongly) convex and non-convex objective functions, with both fixed and diminishing step sizes using
a Lyapunov function construction approach. Our analysis reveals several advantages of our method: we
match the best existing rates of convergence in the centralized setting, while simultaneously supporting data
parallelism as well as constrained communication topologies; to our knowledge, this is the first approach that
achieves all three desirable properties; see Table 1 for a detailed comparison.
Finally, we validate our algorithms’ performance on benchmark datasets, such as MNIST, CIFAR-10, and
CIFAR-100. Apart from centralized SGD as a baseline, we also compare performance with that of Federated
Averaging SGD as it also enables data parallelization. Empirical evidence (for a given number of agents and
other hyperparametric conditions) suggests that while our method is slightly slower, we can achieve higher
accuracy compared to the best available algorithm (Federated Averaging (FedAvg)).
Related work: Apart from the algorithms mentioned above, a few other related works exist, includ-
ing a distributed system called Adam for large deep neural network (DNN) models [12] and a distributed
methodology by Strom [13] for DNN training by controlling the rate of weight-update to reduce the amount
of communication. Natural Gradient Stochastic Gradient Descent (NG-SGD) based on model averaging [14]
and staleness-aware async-SGD [15] have also been developed for distributed deep learning. A method
called CentralVR [16] was proposed for reducing the variance and conducting parallel execution with linear
convergence rate. Moreover, a decentralized algorithm based on gossip protocol called the multi-step dual
accelerated (MSDA) [17] was developed for solving deterministically smooth and strongly convex distributed
optimization problems in networks with a provable optimal linear convergence rate. A new class of decen-
tralized primal-dual methods [18] was also proposed recently in order to improve inter-node communication
efficiency for distributed convex optimization problems. To minimize a finite sum of nonconvex functions over
a network, the authors in [19] proposed a zeroth-order distributed algorithm (ZENITH) that was globally
convergent with a sublinear rate. From the perspective of distributed optimization, the proposed algorithms
2
have similarities with the approaches of [20, 21]. However, we distinguish our work due to the collaborative
learning aspect with data parallelization and extension to the stochastic setting and nonconvex objective
functions. In [20] the authors only considered convex objective functions in a deterministic setting, while
the authors in [21] presented results for non-convex optimization problems in a deterministic setting.
The rest of the paper is organized as follows. While section 2 formulates the distributed, unconstrained
stochastic optimization problem, section 3 presents the CDSGD algorithm and the Lyapunov stochastic
gradient required for analysis presented in section 4. Validation experiments and performance comparison
results are described in section 5. The paper is summarized, concluded in section 6 along with future
research directions. Detailed proofs of analytical results, extensions (e.g., effect of diminishing step size) and
additional experiments are included in the supplementary section 7.
2 Formulation
We consider the standard (unconstrained) empirical risk minimization problem typically used in machine
learning problems (such as deep learning):
min f(x) :=
1
n
n
∑
i=1
f i(x), (1)
where x ∈ Rd denotes the parameter of interest and f : Rd → R is a given loss function, and f i is the
function value corresponding to a data point i. In this paper, we are interested in learning problems where
the computational agents exhibit data parallelism, i.e., they only have access to their own respective training
datasets. However, we assume that the agents can communicate over a static undirected graph G = (V , E),
where V is a vertex set (with nodes corresponding to agents) and E is an edge set. With N agents, we have
V = {1, 2, ..., N} and E ⊆ V×V . If (j, l) ∈ E , then Agent j can communicate with Agent l. The neighborhood
of agent j ∈ V is defined as: Nb(j) , {l ∈ V : (j, l) ∈ E or j = l}. Throughout this paper we assume that the
graph G is connected. Let Dj , j = 1, . . . , n denote the subset of the training data (comprising nj samples)
corresponding to the jth agents such that
∑N
j=1 nj = n. With this setup, we have the following simplification
of Eq. 1:
minf(x) :=
1
n
N
∑
j=1
∑
i∈Dj
f i(x) =
N
n
N
∑
j=1
∑
i∈Dj
f ij(x), (2)
where, fj(x) =
1
N
f(x) is the objective function specific to Agent j. This formulation enables us to state the
optimization problem in a distributed manner, where f(x) =
∑N
j=1 fj(x).
1 Furthermore, the problem (1)
can be reformulated as
min
N
n
1TF(x) :=
N
n
N
∑
j=1
∑
i∈Dj
f ij(x
j) (3a)
s.t. xj = xl ∀(j, l) ∈ E , (3b)
where x := (x1, x2, . . . , xN )T ∈ RN×d and F(x) can be written as
F(x) =
[
∑
i∈D1
f i1(x
1),
∑
i∈D2
f i2(x
2), . . . ,
∑
i∈DN
f iN (x
N )
]T
(4)
Note that with d > 1, the parameter set x as well as the gradient ∇F(x) correspond to matrix variables.
However, for simplicity in presenting our analysis, we set d = 1 in this paper, which corresponds to the case
where x and ∇F(x) are vectors.
1Note that in our formulation, we are assuming that every agent has the same local objective function while in general
distributed optimization problems they can be different.
3
We now introduce several key definitions and assumptions that characterize the objective functions and
the agent interaction matrix.
Definition 1. A function f : Rd → R is H-strongly convex, if for all x, y ∈ Rd, we have f(y) ≥ f(x) +
∇f(x)T (y − x) + H2 ‖y − x‖
2.
Definition 2. A function f : Rd → R is γ-smooth if for all x, y ∈ Rd, we have f(y) ≤ f(x) +∇f(x)T (y −
x) + γ2 ‖y − x‖
2.
As a consequence of Definition 2, we can conclude that ∇f is Lipschitz continuous, i.e., ‖∇f(y)−∇f(x)‖ ≤
γ‖y − x‖ [22].
Definition 3. A function c is said to be coercive if it satisfies: c(x) → ∞ when‖x‖ → ∞.
Assumption 1. The objective functions fj : R
d → R are assumed to satisfy the following conditions: a) Each
fj is γj-smooth; b) each fj is proper (not everywhere infinite) and coercive; and c) each fj is Lj-Lipschitz
continuous, i.e., |fj(y)− fj(x)| < Lj‖y − x‖ ∀x, y ∈ R
d.
As a consequence of Assumption 1, we can conclude that
∑N
j=1 fj(x
j) possesses Lipschitz continuous
gradient with parameter γm := maxjγj. Similarly, each fj is strongly convex with Hj such that
∑N
j=1 fj(x
j)
is strongly convex with Hm = minjHj .
Regarding the communication network, we use Π to denote the agent interaction matrix, where the
element πjl signifies the link weight between agents j and l.
Assumption 2. a) If (j, l) /∈ E, then πjl = 0; b) 1
TΠ = 1T ,Π1 = 1; c) null{I − Π} = span{1}; and d)
I  Π ≻ 0.
The main outcome of Assumption 2 is that the probability transition matrix is doubly stochastic and
that we have λ1(Π) = 1 > λ2(Π) ≥ · · · ≥ λN (Π) > 0, where λz(Π) denotes the z-th largest eigenvalue of Π.
3 Proposed Algorithm
3.1 Consensus Distributed SGD
For solving stochastic optimization problems, SGD and its variants have been commonly used to centralized
and distributed problem formulations. Therefore, the following algorithm is proposed based on SGD and
the concept of consensus to solve the problem laid out in Eq. 2,
xjk+1 =
∑
l∈Nb(j)
πjlx
l
k − αgj(x
j
k) (5)
where Nb(j) indicates the neighborhood of agent j, α is the step size, gj(x
j
k) is stochastic gradient of fj at
xjk. While the pseudo-code of CDSGD is shown below in Algorithm 1, momentum versions of CDSGD based
on Polyak momentum [23] and Nesterov momentum [24] are also presented in the supplementary section 7.
Note, mini-batch implementations of these algorithms are straightforward, hence, are not discussed here in
detail.
Algorithm 1: CDSGD
Input : m, α, N
1 Initialize: xj0, (j = 1, 2, . . . , N)
2 Distribute the training dataset to N agents.
3 for each agent do
4 for k = 0 : m do
5 Randomly shuffle the corresponding data subset Dj
6 wjk+1 =
∑
l∈Nb(j) πjlx
l
k
7 xjk+1 = w
j
k+1 − αgj(x
j
k)
8 end
9 end
4
3.2 Tools for convergence analysis
We now analyze the convergence properties of the iterates {xjk} generated by Algorithm 1. The following
section summarizes some key intermediate concepts required to establish our main results.
First, we construct an appropriate Lyapunov function that will enable us to establish convergence. Ob-
serve that the update law in Alg. 1 can be expressed as:
xk+1 = Πxk − αg(xk), (6)
where
g(xk) = [g1(x
1
k)g2(x
2
k)...gN (x
N
k )]
T
Denoting wk = Πxk, the update law can be re-written as xk+1 = wk − αg(xk). Moreover, xk+1 =
xk − xk +wk − αg(xk). Rearranging the last equality yields the following relation:
xk+1 = xk − α(g(xk) + α
−1(xk −wk)) = xk − α(g(xk) + α
−1(I −Π)xk) (7)
where the last term in Eq. 7 is the Stochastic Lyapunov Gradient. From Eq. 7, we observe that the “effective"
gradient step is given by g(xk) + α
−1(I − Π)xk. Rewriting ∇J
i(xk) = g(xk) + α
−1(I −Π)xk, the updates
of CDSGD can be expressed as:
xk+1 = xk − α∇J
i(xk). (8)
The above expression naturally motivates the following Lyapunov function candidate:
V (x, α) :=
N
n
1TF(x) +
1
2α
‖x‖2I−Π (9)
where ‖·‖I−Π denotes the norm with respect to the PSD matrix I−Π. Since
∑N
j=1 fj(x
j) has a γm-Lipschitz
continuous gradient, ∇V (x) also has a Lipschitz continuous gradient with parameter:
γ̂ := γm + α
−1λmax(I −Π) = γm + α
−1(1− λN (Π)).
Similarly, as
∑N
j=1 fj(x
j) is Hm-strongly convex, then V (x) is strongly convex with parameter:
Ĥ := Hm + (2α)
−1λmin(I −Π) = Hm + (2α)
−1(1− λ2(Π)).
Based on Definition 1, V has a unique minimizer, denoted by x∗ with V ∗ = V (x∗). Correspondingly,
using strong convexity of V , we can obtain the relation:
2Ĥ(V (x)− V ∗) ≤ ‖∇V (x)‖2 for all x ∈ RN . (10)
From strong convexity and the Lipschitz continuous property of ∇fj , the constants Hm and γm further
satisfy Hm ≤ γm and hence, Ĥ ≤ γ̂.
Next, we introduce two key lemmas that will help establish our main theoretical guarantees. Due to
space limitations, all proofs are deferred to the supplementary material in Section 7.
Lemma 1. Under Assumptions 1 and 2, the iterates of CDSGD satisfy ∀k ∈ N:
E[V (xk+1)]− V (xk) ≤ −α∇V (xk)
T
E[∇J i(xk)] +
γ̂
2
α2E[‖∇J i(xk)‖
2] (11)
At a high level, since E[∇J i(xk)] is the unbiased estimate of ∇V (xk), using the updates ∇J
i(xk) will
lead to sufficient decrease in the Lyapunov function. However, unbiasedness is not enough, and we also need
to control higher order moments of ∇J i(xk) to ensure convergence. Specifically, we consider the variance of
∇J i(xk):
V ar[∇J i(xk)] := E[‖∇J
i(xk)‖
2]− ‖E[∇J i(xk)]‖
2 (12)
To bound the variance of ∇J i(xk), we use a standard assumption presented in [25] in the context of (central-
ized) deep learning. Such an assumption aims at providing an upper bound for the “gradient noise" caused
by the randomness in the minibatch selection at each iteration.
5
Assumption 3. a) There exist scalars ζ2 ≥ ζ1 > 0 such that ∇V (xk)
T
E[∇J i(xk)] ≥ ζ1‖∇V (xk)‖
2
and ‖E[∇J i(xk)]‖ ≤ ζ2‖∇V (xk)‖ for all k ∈ N; b) There exist scalars Q ≥ 0 and QV ≥ 0 such that
V ar[∇J i(xk)] ≤ Q+QV ‖∇V (xk)‖
2 for all k ∈ N.
Remark 1. While Assumption 3(a) guarantees the sufficient descent of V in the direction of −∇J i(xk),
Assumption 3(b) states that the variance of ∇J i(xk) is bounded above by the second moment of ∇V (xk).
The constant Q can be considered to represent the second moment of the “gradient noise" in ∇J i(xk).
Therefore, the second moment of ∇J i(xk) can be bounded above as E[‖∇J
i(xk)‖
2] ≤ Q+Qm‖∇V (xk)‖
2,
where Qm := QV + ζ
2
2 ≥ ζ
2
1 > 0.
Lemma 2. Under Assumptions 1, 2, and 3, the iterates of CDSGD satisfy ∀k ∈ N:
E[V (xk+1)]− V (xk) ≤ −(ζ1 −
γ̂
2
αQm)α‖∇V (xk)‖
2 +
γ̂
2
α2Q . (13)
In Lemma 2, the first term is strictly negative if the step size satisfies the following necessary condition:
0 < α ≤
2ζ1
γ̂Qm
(14)
However, in latter analysis, when such a condition is substituted into the convergence analysis, it may
produce a larger upper bound. For obtaining a tight upper bound, we impose a sufficient condition for the
rest of analysis as follows:
0 < α ≤
ζ1
γ̂Qm
(15)
As γ̂ is a function of α, the above inequality can be rewritten as 0 < α ≤ ζ1−(1−λN (Π))Qm
γmQm
.
4 Main Results
We now present our main theoretical results establishing the convergence of CDSGD. First, we show that for
most generic loss functions (whether convex or not), CDSGD achieves consensus across different agents in
the graph, provided the step size (which is fixed across iterations) does not exceed a natural upper bound.
Proposition 1. (Consensus with fixed step size) Under Assumptions 1 and 2, the iterates of CDSGD
(Algorithm 1) satisfy ∀k ∈ N:
E[‖xjk − sk‖] ≤
αL
1− λ2(Π)
(16)
where α satisfies 0 < α ≤ ζ1−(1−λN (Π))Qm
γmQm
and L is an upper bound of E[‖g(xk)‖], ∀k ∈ N (defined properly
and discussed in Lemma 4 in the supplementary section 7) and sk =
1
N
∑N
j=1 x
j
k represents the average
parameter estimate.
The proof of this proposition can be adapted from [26, Lemma 1].
Next, we show that for strongly convex loss functions, CDSGD converges linearly to a neighborhood of
the global optimum.
Theorem 1. (Convergence of CDSGD with fixed step size, strongly convex case) Under Assumptions 1, 2
and 3, the iterates of CDSGD satisfy the following inequality ∀k ∈ N:
E[V (xk)− V
∗] ≤ (1− αĤζ1)
k−1(V (x1)− V
∗) +
α2γ̂Q
2
k−1
∑
l=0
(1− αĤζ1)
l
= (1− (αHm + 1− λ2(Π))ζ1)
k−1(V (x1)− V
∗)
+
(α2γm + α(1 − λN (Π)))Q
2
k−1
∑
l=0
(1− (αHm + 1− λ2(Π))ζ1)
l
(17)
when the step size satisfies 0 < α ≤ ζ1−(1−λN (Π))Qm
γmQm
.
6
A detailed proof is presented in the supplementary section 7. We observe from Theorem 1 that the
sequence of Lyapunov function values {V (xk)} converges linearly to a neighborhood of the optimal value,
i.e., limk→∞ E[V (xk) − V
∗] ≤ αγ̂Q
2Ĥζ1
= (αγm+1−λN (Π))Q2(Hm+α−1(1−λ2(Π))ζ1 . We also observe that the term on the right
hand side decreases with the spectral gap of the agent interaction matrix Π, i.e., 1− λ2(Π), which suggests
an interesting relation between convergence and topology of the graph. Moreover, we observe that the
upper bound is proportional to the step size parameter α, and smaller step sizes lead to smaller radii of
convergence. (However, choosing a very small step-size may negatively affect the convergence rate of the
algorithm). Finally, if the gradient in this context is not stochastic (i.e., the parameter Q = 0), then linear
convergence to the optimal value is achieved, which matches known rates of convergence with (centralized)
gradient descent under strong convexity and smoothness assumptions.
Remark 2. Since E[N
n
1TF(xk)] ≤ E[V (xk)] and
N
n
1TF(x∗) = V ∗, the sequence of objective function values
are themselves upper bounded as follows: E[N
n
1TF(xk) −
N
n
1TF(x∗)] ≤ E[V (xk) − V
∗]. Therefore, using
Theorem 1 we can establish analogous convergence rates in terms of the true objective function values
{N
n
1TF(xk)} as well.
The above convergence result for CDSGD is limited to the case when the objective functions are strongly
convex. However, most practical deep learning systems (such as convolutional neural network learning) in-
volve optimizing over highly non-convex objective functions, which are much harder to analyze. Nevertheless,
we show that even under such situations, CDSGD exhibits a (weaker) notion of convergence.
Theorem 2. (Convergence of CDSGD with fixed step size, nonconvex case) Under Assumptions 1, 2, and 3,
the iterates of CDSGD satisfy ∀m ∈ N:
E[
m
∑
k=1
‖∇V (xk)‖
2] ≤
γ̂mαQ
ζ1
+
2(V (x1)− Vinf)
ζ1α
=
(γmα+ 1− λN (Π))mQ
ζ1
+
2(V (x1)− Vinf)
ζ1α
.
(18)
when the step size satisfies 0 < α ≤ ζ1−(1−λN (Π))Qm
γmQm
.
Remark 3. Theorem 2 states that when in the absence of “gradient noise" (i.e., when Q = 0), the quantity
E[
∑m
k=1 ‖∇V (xk)‖
2] remains finite. Therefore, necessarily {‖∇V (xk)‖} → 0 and the estimates approach a
stationary point. On the other hand, if the gradient calculations are stochastic, then a similar claim cannot be
made. However, for this case we have the upper bound limm→∞ E[
1
m
∑m
k=1 ‖∇V (xk)‖
2] ≤ (γmα+1−λN (Π))Q
ζ1
.
This tells us that while we cannot guarantee convergence in terms of sequence of objective function values,
we can still assert that the average of the second moment of gradients is strictly bounded from above even
for the case of nonconvex objective functions.
Moreover, the upper bound cannot be solely controlled via the step-size parameter α (which is different
from what is implied in the strongly convex case by Theorem 1). In general, the upper bound becomes
tighter as λN (Π) increases; however, an increase in λN (Π) may result in a commensurate increase in λ2(Π),
leading to worse connectivity in the graph and adversely affecting consensus among agents. Again, our upper
bounds are reflective of interesting tradeoffs between consensus and convergence in the gradients, and their
dependence on graph topology.
The above results are for fixed step size α, and we can prove complementary results for CDSGD even for
the (more prevalent) case of diminishing step size αk. These are presented in the supplementary material
due to space constraints.
7
0 1000 2000 3000 4000 5000
Number of epochs
0.0
0.2
0.4
0.6
0.8
1.0
ac
cs
cifar10 experiment
SGD
CDSGD
(a)
0 200 400 600 800 1000
Number of epochs
0.0
0.2
0.4
0.6
0.8
1.0
ac
cs
cifar10 experiment
SGD
CDSGD
CDMSGD
Federated Averaging
(b)
Figure 1: Average training (solid lines) and validation (dash lines) accuracy for (a) comparison of CDSGD
with centralized SGD and (b) CDMSGD with Federated average method
5 Experimental Results
This section presents the experimental results using the benchmark image recognition dataset, CIFAR-10.
We use a deep convolutional nerual network (CNN) model (with 2 convolutional layers with 32 filters each
followed by a max pooling layer, then 2 more convolutional layers with 64 filters each followed by another max
pooling layer and a dense layer with 512 units, ReLU activation is used in convolutional layers) to validate
the proposed algorithm. We use a fully connected topology with 5 agents and uniform agent interaction
matrix except mentioned otherwise. A mini-batch size of 128 and a fixed step size of 0.01 are used in these
experiments. The experiments are performed using Keras and TensorFlow [27, 28] and the codes will be made
publicly available soon. While we included the training and validation accuracy plots for the different case
studies here, the corresponding training loss plots, results with other becnmark datasets such as MNIST and
CIFAR-100 and decaying as well as different fixed step sizes are presented in the supplementary section 7.
5.1 Performance comparison with benchmark methods
We begin with comparing the accuracy of CDSGD with that of the centralized SGD algorithm as shown
in Fig. 1(a). While the CDSGD convergence rate is significantly slower compared to SGD as expected, it
is observed that CDSGD can eventually achieve high accuracy, comparable with centralized SGD. However,
another interesting observation is that the generalization gap (the difference between training and validation
accuracy as defined in [29]) for the proposed CDSGD algorithm is significantly smaller than that of SGD
which is an useful property. We also compare both CDSGD and CDMSGD with the Federated averaging SGD
(FedAvg) algorithm which also performs data parallelization (see Fig. 1(b)). For the sake of comparison, we
use same number of agents and choose E = 1 and C = 1 as the hyperparameters in the FedAvg algorithm as
it is close to a fully connected topology scenario as considered in the CDSGD and CDMSGD experiments. As
CDSGD is significantly slow, we mainly compare the CDMSGD with FedAvg which have similar convergence
rates (CDMSGD being slightly slower). The main observation is that CDMSGD performs better than FedAvg
at the steady state and can achieve centralized SGD level performance. It is important to note that FedAvg
does not perform decentralized computation. Essentially it runs a brute force parameter averaging on a
central parameter server at every epoch (i.e., consensus at every epoch) and then broadcasts the updated
parameters to the agents. Hence, it tends to be slightly faster than CDMSGD which uses a truly decentralized
computation over a network.
5.2 Effect of network size and topology
In this section, we investigate the effects of network size and topology on the performance of the proposed
algorithms. Figure 2(a) shows the change in training performance as the number of agents grow from 2 to 8
and to 16. Although with increase in number of agents, the convergence rate slows down, all networks are
able to achieve similar accuracy levels. Finally, we investigate the impact of network sparsity (as quantified
8
0 200 400 600 800 1000
Number of epochs
0.0
0.2
0.4
0.6
0.8
1.0
ac
cs
cifar10 experiment
2 Agents
8 Agents
16 Agents
(a)
0 200 400 600 800 1000
Number of epochs
0.0
0.2
0.4
0.6
0.8
1.0
ac
cs
0.00
0.01
0.02
0.03
0.04
Va
ria
nc
e 
am
on
g 
ag
en
ts
 fo
r a
cc
scifar10 experiment
Fully Connected with λ2(Π) = 0
Sparse Topology with λ2(Π) = 0.54
Sparse Topology with λ2(Π) = 0.86
(b)
Figure 2: Average training (solid lines) and validation (dash lines) accuracy along with accuracy variance
over agents for CDMSGD algorithm with (a) varying network size and (b) varying network topology
by the second largest eigenvalue) on the learning performance. The primary observation is convergence
of average accuracy value happens faster for sparser networks (higher second largest eigenvalue). This is
similar to the trend observed for FedAvg algorithm while reducing the Client fraction (C) which makes the
(stochastic) agent interaction matrix sparser. However, from the plot of the variance of accuracy values over
agents (a smooth version using moving average filter), it can be observed that the level of consensus is more
stable for denser networks compared to that for sparser networks. This is also expected as discussed in
Proposition 1. Note, with the availability of a central parameter server (as in federated averaging), sparser
topology may be useful for a faster convergence, however, consensus (hence, topology density) is critical for
a collaborative learning paradigm with decentralized computation.
6 Conclusion and Future Work
This paper addresses the collaborative deep learning (and many other machine learning) problem in a com-
pletely distributed manner (i.e., with data parallelism and decentralized computation) over networks with
fixed topology. We establish a consensus based distributed SGD framework and proposed associated learn-
ing algorithms that can prove to be extremely useful in practice. Using a Lyapunov function construction
approach, we show that the proposed CDSGD algorithm can achieve linear convergence rate with sufficiently
small fixed step size and sublinear convergence rate with diminishing step size (see supplementary section 7
for details) for strongly convex and Lipschitz differentiable objective functions. Moreover, decaying gradients
can be observed for the nonconvex objective functions using CDSGD. Relevant experimental results using
benchmark datasets show that CDSGD can achieve centralized SGD level accuracy with sufficient training
epochs while maintaining a significantly low generalization error. The momentum variant of the proposed
algorithm, CDMSGD can outperform recently proposed FedAvg algorithm which also uses data parallelism
but does not perform a decentralized computation, i.e., uses a central parameter server. The effects of net-
work size and topology are also explored experimentally which conforms to the analytical understandings.
While current and future research is focusing on extensive testing and validation of the proposed framework
especially for large networks, a few technical research directions include: (i) collaborative learning with ex-
treme non-IID data; (ii) collaborative learning over directed time-varying graphs; and (iii) understanding
the dependencies between learning rate and consensus.
9
References
[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, 2015.
[2] Suyog Gupta, Wei Zhang, and Josh Milthorpe. Model accuracy and runtime tradeoff in distributed
deep learning. arXiv preprint arXiv:1509.04210, 2015.
[3] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior,
Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in neural
information processing systems, pages 1223–1231, 2012.
[4] Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd. In
Advances in Neural Information Processing Systems, pages 685–693, 2015.
[5] H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient
learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.
[6] Michael Blot, David Picard, Matthieu Cord, and Nicolas Thome. Gossip training for deep learning.
arXiv preprint arXiv:1611.09726, 2016.
[7] Peter H Jin, Qiaochu Yuan, Forrest Iandola, and Kurt Keutzer. How to scale distributed deep learning?
arXiv preprint arXiv:1611.04581, 2016.
[8] Kushal Mukherjee, Asok Ray, Thomas Wettergren, Shalabh Gupta, and Shashi Phoha. Real-time adap-
tation of decision thresholds in sensor networks for detection of moving targets. Automatica, 47(1):185
– 191, 2011.
[9] Chao Liu, Yongqiang Gong, Simon Laflamme, Brent Phares, and Soumik Sarkar. Bridge damage
detection using spatiotemporal patterns extracted from dense sensor network. Measurement Science
and Technology, 28(1):014011, 2017.
[10] H.-L. Choi and J. P. How. Continuous trajectory planning of mobile sensors for informative forecasting.
Automatica, 46(8):1266–1275, 2010.
[11] D. K. Jha, P. Chattopadhyay, S. Sarkar, and A. Ray. Path planning in gps-denied environments with
collective intelligence of distributed sensor networks. International Journal of Control, 89, 2016.
[12] Trishul M Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. Project adam:
Building an efficient and scalable deep learning training system. In OSDI, volume 14, pages 571–582,
2014.
[13] Nikko Strom. Scalable distributed dnn training using commodity gpu cloud computing. In INTER-
SPEECH, volume 7, page 10, 2015.
[14] Hang Su and Haoyu Chen. Experiments on parallel training of deep neural network using model
averaging. arXiv preprint arXiv:1507.01239, 2015.
[15] Wei Zhang, Suyog Gupta, Xiangru Lian, and Ji Liu. Staleness-aware async-sgd for distributed deep
learning. arXiv preprint arXiv:1511.05950, 2015.
[16] Soham De and Tom Goldstein. Efficient distributed sgd with variance reduction. In Data Mining
(ICDM), 2016 IEEE 16th International Conference on, pages 111–120. IEEE, 2016.
[17] Kevin Scaman, Francis Bach, Sébastien Bubeck, Yin Tat Lee, and Laurent Massoulié. Optimal
algorithms for smooth and strongly convex distributed optimization in networks. arXiv preprint
arXiv:1702.08704, 2017.
10
[18] Guanghui Lan, Soomin Lee, and Yi Zhou. Communication-efficient algorithms for decentralized and
stochastic optimization. arXiv preprint arXiv:1701.03961, 2017.
[19] Davood Hajinezhad, Mingyi Hong, and Alfredo Garcia. Zenith: A zeroth-order distributed algorithm
for multi-agent nonconvex optimization.
[20] Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization.
IEEE Transactions on Automatic Control, 54(1):48–61, 2009.
[21] Jinshan Zeng and Wotao Yin. On nonconvex decentralized gradient descent. arXiv preprint
arXiv:1608.05766, 2016.
[22] Angelia Nedić and Alex Olshevsky. Stochastic gradient-push for strongly convex functions on time-
varying directed graphs. IEEE Transactions on Automatic Control 61.12, pages 3936–3947, 2016.
[23] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 4(5):1–17, 1964.
[24] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
[25] Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning.
arXiv preprint arXiv:1606.04838, 2016.
[26] Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. arXiv
preprint arXiv:1310.7063, 2013.
[27] François Chollet. Keras. https://github.com/fchollet/keras, 2015.
[28] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning
on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
[29] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. CoRR, abs/1611.03530, 2016.
[30] Angelia Nedić and Alex Olshevsky. Distributed optimization over time-varying directed graphs. IEEE
Transactions on Automatic Control, 60(3):601–615, 2015.
[31] S. Ram, A. Nedic, and V. Veeravalli. A new class of distributed optimization algorithms: application
to regression of distributed data. Optimization Methods and Software, 27(1):71– 88, 2012.
11
7 Supplementary Materials for “Collaborative Deep Learning in
Fixed Topology Networks"
7.1 Additional analytical results and proofs
We begin with proofs of the lemmas and theorems that are presented in the main body of the paper without
proof. The statements of the lemmas and theorems are presented again for completeness.
Lemma 1: Let Assumptions 1 and 2 hold. The iterates of CDSGD (Algorithm 1) satisfy the following
inequality ∀k ∈ N:
E[V (xk+1)]− V (xk) ≤ −αk∇V (xk)
T
E[∇J i(xk)] +
γ̂
2
α2kE[‖∇J
i(xk)‖
2] (19)
Proof. By Assumption 1, the iterates generated by CDSGD satisfy
V (xk+1)− V (xk) ≤ ∇V (xk)
T (xk+1 − xk) +
1
2
γ̂‖xk+1 − xk‖
2
= −α∇V (xk)
T∇J i(xk) +
1
2
γ̂α2‖∇J i(xk)‖
2
(20)
Taking expectations on both sides, we can obtain
E[V (xk+1)− V (xk)] ≤ E[−α∇V (xk)
T∇J i(xk) +
1
2
γ̂α2‖∇J i(xk)‖
2] (21)
While V (xk) is deterministic, V (xk+1) is stochastic due to the random sampling aspect. Therefore, we have
E[V (xk+1)]− V (xk) ≤ −α∇V (xk)
T
E[∇J i(xk)] +
1
2
γ̂α2E[‖∇J i(xk)‖
2] (22)
which completes the proof.
Lemma 2: Let Assumptions 1, 2, and 3 hold. The iterates of CDSGD (Algorithm 1) satisfy the following
inequality ∀k ∈ N:
E[V (xk+1)]− V (xk) ≤ −(ζ1 −
γ̂
2
αQm)α‖∇V (xk)‖
2 +
γ̂
2
α2Q (23)
Proof. Recalling Lemma 1 and using Assumption 3 and Remark 1, we have
E[V (xk+1)]− V (xk) ≤ −ζ1α‖∇V (xk)‖
2 +
γ̂
2
α2E[‖∇J i(xk)‖
2]
≤ −ζ1α‖∇V (xk)‖
2 +
γ̂
2
α2(Q+Qm‖∇V (xk)‖
2)
= −(ζ1 −
γ̂
2
αQm)α‖∇V (xk)‖
2 +
γ̂
2
α2Q
(24)
which completes the proof.
In order to prove Propositon 1, several auxiliary technical lemmas are presented first.
Lemma 3. V has a lower bound denoted by Vinf over an open set which contains the iterates {xk} generated
by CDSGD (Algorithm 1).
Lemma 3 can be obtained as each fj is proper and coercive. Such a lemma is able to help characterize
the nonconvex case in which the global optimum may not be achieved.
12
Lemma 4. Let Assumption 1 holds. There exists some constant 0 < L < ∞ such that E[‖g(xk)‖] ≤ L.
The proof of Lemma 4 directly follows from the Assumption 1 c) and L = maxjLj.
Theorem 1(Convergence of CDSGD with fixed step size, strongly convex case): Let Assumptions 1, 2 and 3
hold. The iterates of CDSGD (Algorithm 1) satisfy the following inequality ∀k ∈ N, when the step size
satisfies
0 < α ≤
ζ1 − (1 − λN (Π))Qm
γmQm
E[V (xk)− V
∗] ≤ (1− αĤζ1)
k−1(V (x1)− V
∗) +
α2γ̂Q
2
k−1
∑
l=0
(1− αĤζ1)
l
= (1− (αHm + 1− λ2(Π))ζ1)
k−1(V (x1)− V
∗)
+
(α2γm + α(1 − λN (Π)))Q
2
k−1
∑
l=0
(1− (αHm + 1− λ2(Π))ζ1)
l
(25)
Proof. Recalling Lemma 2 and using Eq. 10 yield that
E[V (xk+1)]− V (xk) ≤ −(ζ1 −
γ̂
2
αQm)α‖∇V (xk)‖
2 +
γ̂
2
α2Q
≤ −
1
2
αζ1‖∇(xk)‖
2 +
α2γ̂Q
2
≤ −αζ1Ĥ(V (xk)− V
∗) +
α2γ̂Q
2
(26)
The second inequality follows from the relation: α ≤ ζ1
γ̂Qm
, which is implied by: α ≤ ζ1−(1−λN (Π))Qm
γmQm
. The
third inequality follows from the strong convexity. The expectation taken in the above inequalities is only
related to xk+1. Hence, recursively taking the expectation and subtracting V
∗ from both sides requires the
following inequality to hold
E[V (xk+1)− V
∗] ≤ (1− αĤζ1)E[V (xk)− V
∗] +
α2γ̂Q
2
(27)
As 0 < αĤζ1 ≤
Ĥζ2
1
γ̂Qm
≤
Ĥζ2
1
γ̂ζ2
1
= Ĥ
γ̂
≤ 1, the conclusion follows by applying Eq. 27 recursively through iteration
k ∈ N.
Theorem 2(Convergence of CDSGD with fixed step size, nonconvex case): Let Assumptions 1, 2, and 3
hold. The iterates of CDSGD (Algorithm 1) satisfy the following inequality ∀m ∈ N, when the step size
satisfies
0 < α ≤
ζ1 − (1 − λN (Π))Qm
γmQm
E[
m
∑
k=1
‖∇V (xk)‖
2] ≤
γ̂mαQ
ζ1
+
2(V (x1)− Vinf)
ζ1α
=
(γmα+ 1− λN (Π))mQ
ζ1
+
2(V (x1)− Vinf)
ζ1α
(28)
Proof. Recalling Lemma 2, and also taking the expectation lead to the following relation,
E[V (xk+1)]− E[V (xk)] ≤ −(ζ1 −
γ̂αQm
2
)αE[‖∇V (xk)‖
2] +
γ̂α2Q
2
(29)
13
As the step size satisfies that α ≤ ζ1
γ̂Qm
, it results in
E[V (xk+1)]− E[V (xk)] ≤ −
ζ1α
2
E[‖∇V (xk)‖
2] +
α2γ̂Q
2
(30)
Applying the above inequality from 1 to m and summing them up can give the following relation
Vinf − V (x1) ≤ E[V (xk+1)]− V (x1) ≤ −
ζ1α
2
m
∑
k=1
E[‖∇V (xk)‖
2] +
mα2γ̂Q
2
(31)
The last inequality follows from the Lemma 3. Rearrangement of the above inequality and substituting
γ̂ = γm + α
−1(1 − λN (Π)) into it yield the desired result.
7.2 Proof with Diminishing Step Size
From results presented in section 4, it can be concluded that when the step size is fixed, the function value
can only converge near the optimal value. However, in many deep learning models, noisy gradient is quite
common due to the random data sampling. Hence, such a situation requires the step size to be adaptive and
then with noise, the function value sequence is able to converge to the optimal value. Let {αk} be defined
as a diminishing step size sequence that satisfies the following properties:
αk > 0,
∞
∑
k=0
αk = ∞,
∞
∑
k=0
α2k < ∞
The implication of the above properties is that limk→∞ αk = 0. The next proposition states that when the
step size is diminishing, consensus can be achieved asymptotically, i.e., limk→∞ E[‖x
j
k − sk‖] = 0.
Proposition 2. (Consensus with diminishing step size) Let Assumptions 1 and 2 hold. The iterates of
CDSGD (Algorithm 1) satisfy the following inequality ∀k ∈ N, when αk is diminishing,
lim
k→∞
E[‖xjk − sk‖] = 0 (32)
The proof is adapted from the Lemma 1 in [26], Lemmas 5 and 6 in [30].
Recalling the algorithm CDSGD
xk+1 = wk − αkg(xk) = xk − αk(g(xk) +
1
αk
(xk −wk))
We define ∇Ĵ i(xk) = ∇J
i(xk, αk) = g(xk) +
1
αk
(xk −wk), and the following Lyapunov function
V̂ (x) = V (x, αk) :=
N
n
1TF(x) +
1
2αk
‖x‖2I−Π (33)
The general Lyapunov function is a function of the diminishing step size αk. However, the step size is
independent of the variable x such that it only affects the magnitude of ‖x‖2I−Π along with iterations. Note,
from Proposition 2, we have that each agent eventually reaches the consensus with diminishing step size.
Hence, the term 12αk ‖x‖
2
I−Π should not increase with increase in k as the step size αk → 0 for k → ∞.
To show that CDSGD with diminishing step size enables convergence to the optimal value, the necessary
lemmas and assumptions are directly used from the previous part of the paper with modified constants.
We next show that the Lyapunov function and stochastic Lyapunov gradient with the diminishing step
size are bounded. More formally, we aim to show that ‖∇Ĵ i(xk)‖ is bounded above for all k ∈ N. We
have, ‖∇Ĵ i(xk)‖ ≤ ‖g(xk)‖ +
1
αk
‖(I − Π)xk‖ and g(xk) is bounded. Therefore, we have to show that
1
αk
‖(I −Π)xk‖ is bounded for all k ∈ N.
14
Lemma 5. Let Assumptions 1 and 2 hold. The iterates of CDSGD (Algorithm 1) satisfy the following
inequality ∀k ∈ N, when the step size is diminishing and satisfies that
0 < α0 ≤
ζ̂1 − (1 − λN (Π))Q̂m
γmQ̂m
,
1
αk
E[‖(I −Π)xk‖] < ∞ (34)
and
lim
k→∞
E[‖(I −Π)xk‖] = 0. (35)
ζ̂1, Q̂m correspond to V̂ .
The proof of Lemma 5 requires another auxiliary technical lemma as follows.
Lemma 6. Let Assumptions 1 and 2 hold. The iterates of CDSGD (Algorithm 1) satisfy the following
inequality ∀k ∈ N, when the step size is diminishing and satisfies that
0 < α0 ≤
ζ̂1 − (1 − λN (Π))Q̂m
γmQ̂m
,
∞
∑
k=2
αkE[‖(I −Π)xk‖] < ∞. (36)
Proof. Recalling the CDSGD algorithm,
xk+1 = Πxk − αkg(xk) (37)
Applying the above equality from 1 to k − 1 yields that
xk = Π
k−1x1 −
k−1
∑
l=1
αlΠ
k−1−lg(xl) (38)
Setting x1 = 0 results in that xk = −
∑k−1
l=1 αlΠ
k−1−lg(xl). With this setup, we have
∞
∑
k=2
αk‖(I −Π)xk‖ ≤
∞
∑
k=2
αk‖I −Π‖‖xk‖
≤
∞
∑
k=2
αk‖
k−1
∑
l=1
αlΠ
k−1−lg(xl)‖
≤
∞
∑
k=2
αk
k−1
∑
l=1
αl‖Π
k−1−lg(xl)‖
≤
∞
∑
k=2
αk
k−1
∑
l=1
αl‖Π
k−1−l‖‖g(xl)‖
≤
∞
∑
k=2
αk
k−1
∑
l=1
αl‖Π‖
k−1−l‖g(xl)‖
(39)
With the step size being nonincreasing, taking expectation on both side leads to
E[
∞
∑
k=2
αk‖(I −Π)xk‖] ≤
∞
∑
k=2
k−1
∑
l=1
α2l ‖Π‖
k−1−l
E[‖g(xl)‖] (40)
15
As we discussed earlier, we consider that there exists a constant L that bounds E[‖g(xk)‖] from above for
k ∈ N. Thus, the following relation can be obtained
E[
∞
∑
k=2
αk‖(I −Π)xk‖] ≤
∞
∑
k=2
k−1
∑
l=1
α2l λ2(Π)
k−1−l
E[‖g(xl)‖]
≤ L
∞
∑
k=2
k−1
∑
l=1
α2l λ2(Π)
k−1−l
(41)
As
∑∞
k=1 α
2
k < ∞ and λ2(Π) < 1 then by Lemma 5 in [30], the desired result follows.
Proof of Lemma 5. We first define that hk =
‖(I−Π)xk‖
αk
. Hence, the result of Lemma 6 can be rewritten
as
∑∞
k=2 α
2
kE[hk] < ∞. By defining hm = sup{E[hk]}, we have hm
∑∞
k=2 α
2
k < ∞, which implies that
hm < ∞ as
∑∞
k=1 α
2
k < ∞. Hence, it is immediately seen that E[hk] < ∞. As k → ∞, αk → 0 and
1
αk
E[‖(I −Π)xk‖] < ∞, then limk→∞ E[‖(I −Π)xk‖] = 0, which completes the proof.
The implication of Lemma 5 is two folds: One can observe that V̂ (xk) and ∇Ĵ
i(xk) are finite even with
diminishing step size such that based on Definition 2 there exists a finite positive constant γ′ to allow the
smoothness of V̂ (xk) for all k ∈ N to hold true; another observation is that Assumption 3 still can be used
in the main results. It can also be concluded that V̂ is strongly convex with some constant 0 ≤ H ′ < ∞,
where H ′ corresponds to V̂ .
Theorem 3. (Convergence of CDSGD with diminishing step size, strongly convex case) Let Assumptions 1, 2
and 3 hold. The iterates of CDSGD (Algorithm 1) satisfy the following inequality ∀k ∈ N, when the step size
is diminishing and satisfies that
0 < α0 ≤
ζ̂1 − (1 − λN (Π))Q̂m
γmQ̂m
,
E[V̂ (xk)− V̂
∗] ≤ βk−2(V̂ (x1)− V̂
∗) +
γ′Q̂
2
k−2
∑
p=1
βk−p−2α2p
+
γ′Q̂α2k−1
2
(42)
where sup{1− αkH
′ζ̂1} ≤ β < 1, and γ
′, Q̂ correspond to V̂ .
Proof. As α1 ≤
ζ̂1
γ′Q̂m
, then it can be obtained that αkγ
′Q̂m ≤ α1γ
′Q̂m ≤ ζ̂1 for all k ∈ N. Recalling
Lemma 2 and Eq. 10, subtracting V̂ ∗ from both sides, and taking the expectation yield the following relation
E[V̂ (xk+1)− V̂
∗] ≤ (1 − αkH
′ζ̂1)E[V̂ (xk)− V̂
∗] +
γ′Q̂α2k
2
(43)
Applying the above inequality recursively can give the following relation
E[V̂ (xk+1)− V̂
∗] ≤ (1− αkH
′ζ̂1)(1 − αk−1H
′ζ̂1)E[V̂ (xk−1)− V̂
∗]
+ (1− αk−1H
′ζ̂1)
γ′Q̂α2k−1
2
+
γ′Q̂α2k
2
(44)
By induction, the following can be obtained
E[V̂ (xk+1)− V̂
∗] ≤
k
∏
q=1
(1− αqH
′ζ̂1)(V̂ (x1)− V̂
∗) +
γ′Q̂
2
k−1
∑
p=1
k
∏
r=p+1
(1− αrH
′ζ̂1)α
2
p
+
γ′Q̂α2k
2
(45)
16
As H ′ ≤ γ′, it can be derived that 0 < αkH
′ζ̂1 ≤ 1 for all k ∈ N. Therefore, 1 − αkH
′ζ̂1 ∈ [0, 1) such
that we can define a positive constant β satisfies that sup{1− αkH
′ζ̂1} ≤ β < 1. Hence, combining the last
inequalities together, we have
E[V̂ (xk+1)− V̂
∗] ≤ βk−1(V̂ (x1)− V̂
∗) +
γ′Q̂
2
k−1
∑
p=1
βk−p−1α2p
+
γ′Q̂α2k
2
(46)
which completes the proof by replacing k + 1 with k.
Remark 4. From Theorem 3, we can conclude that the function value sequence {V̂ (xk)} asymptotically
converges to the optimal value. (This holds regardless of whether the “gradient noise" parameter Q̂ is zero
or not.) In fact, we can establish the rate of convergence as follows: the first term on the right hand side
decreases exponentially if β. < 1, and the last term decreases as quickly as α2k. For the middle term, we
can use Lemma 3.1 of [31] that establishes bounds on the convolution of two scalar sequences. If we choose
t > 0 such that αk =
1
kǫ+t , where ǫ ∈ (0.5, 1], then the necessary growth conditions on αk are satisfied;
substituting this into Theorem 3 yields the stated convergence rate of O( 1
kǫ
). In practice, αk can be made
adaptive to Θ
kǫ+t for any constant Θ > 0.
Similarly, we also present the convergence results for the nonconvex objective functions.
Theorem 4. (Convergence of CDSGD with diminishing step size, nonconvex case) Let Assumptions 1, 2
and 3 hold. The iterates of CDSGD (Algorithm 1) satisfy the following inequality ∀m ∈ N, when the step
size is diminishing and satisfies that
0 < α0 ≤
ζ̂1 − (1 − λN (Π))Q̂m
γmQ̂m
,
E[
m
∑
k=1
αk‖∇V̂ (xk)‖
2] ≤
2(V̂ (x1)− V̂inf )
ζ̂1
+
γ′Q̂
ζ̂1
m
∑
k=1
α2k (47)
Proof. Assume that αkγ
′Q̂m ≤ ζ̂1 for all k ∈ N. Based on Eq. 29 we consider the diminishing step size and
Lyapunov function, then the following relation can be obtained
E[V̂ (xk+1)]− E[V̂ (xk)] ≤ −(ζ̂1 −
γ′αkQ̂m
2
)αkE[‖∇V̂ (xk)‖
2] +
γ′αkQ̂
2
(48)
Combining the condition for the step size yields the following inequality
E[V̂ (xk+1)]− E[V̂ (xk)] ≤ −
ζ̂1αk
2
E[‖∇V̂ (xk)‖
2] +
γ′αkQ̂
2
(49)
Applying the last inequality from 1 to m and summing them up,
V̂inf − E[V̂ (x1)] ≤ E[V̂ (xk+1)]− E[V̂ (x1)] ≤
−
ζ̂1
2
m
∑
k=1
αkE[‖∇V̂(xk)‖
2] +
γ′Q̂
2
m
∑
k=1
α2k
(50)
Dividing by ζ̂1/2 and rearranging the terms lead to the desired results.
Remark 5. Compared to Theorem 2, Theorem 4 has shown the decaying of gradient ‖∇V̂ (xk)‖ even with
noise when the step size is diminishing in the nonconvex case. This is because when k → ∞, the right hand
side of Eq. 47 remains finite such that ‖∇V̂ (xk)‖
2 approaches 0.
17
7.3 Additional pseudo-codes of the algorithms
Momentum methods have been regarded as effective methods to speed up the convergence in numerous op-
timization problems. While the Nesterov Momentum method has been extended widely to generate variants
with provable global convergence properties, the global convergence analysis of Polyak Momentum methods
is still quite challenging and an active research topic. Pseudo-codes of CDSGD combined with Polyak mo-
mentum and Nesterov momentum methods are presented below.
Algorithm 2: CDSGD with Polyak Momentum
Input :m, α, N , µ (momentum term)
1 Initialize:xj0, v
j
0
2 Distribute the training data set to N agents
3 For each agent:
4 for k = 0 : m do
5 Randomly shuffle the corresponding data subset;
6 wjk+1 =
∑
l∈Nb(j) πjlx
l
k
7 vjk+1 = µv
j
k − αkgj(x
j
k)
8 xjk+1 = w
j
k+1 + v
j
k+1
9 end
Algorithm 3: CDSGD with Nesterov Momentum
Input :m, α, N , µ
1 Initialize:xj0, v
j
0
2 Distribute the training data set to N agents
3 For each agent:
4 for k = 0 : m do
5 Randomly shuffle the corresponding data subset
6 wjk+1 =
∑
l∈Nb(j) πjlx
l
k
7 vjk+1 = µv
j
k − αkgj(x
j
k + µv
j
k)
8 xjk+1 = w
j
k+1 + v
j
k+1
9 end
7.4 Additional Experimental Results
We begin with a discussion on the training loss profiles for the CIFAR-10 results presented in the main body
of the paper.
7.4.1 Comparison of the loss for benchmark methods
Figure 3 (a) shows the loss (in log scale) with respect to the number of epochs for SGD and CDSGD
algorithms. The solid curve means training and the dash curve indicates validation. From the loss results,
it can be observed that SGD has the sublinear convergence rate for training and dominates among the
two methods during the training process. While for the validation, SGD performs poorly after around 70
epochs. However, CDSGD shows linear convergence rate (in log scale as discussed in the analysis) for both
training and validation. Though, it takes a lot of more time compared to SGD for convergence, it eventually
performs better than SGD in the validation data and the gap between the training and validation loss (i.e.,
the generalization gap [29]) is very less compared to that in SGD.
18
0 1000 2000 3000 4000 5000
Number of epochs
10−4
10−3
10−2
10−1
100
lo
ss
es
cifar10 experiment
SGD
CDSGD
(a)
0 200 400 600 800 1000
Number of epochs
10−4
10−3
10−2
10−1
100
lo
ss
es
cifar10 experiment
SGD
CDSGD
CDMSGD
Federated Averaging
(b)
Figure 3: Average training (solid lines) and validation (dash lines) loss for (a) CDSGD algorithm with SGD
algorithm and (b) CDMSGD with Federated averaging method
7.4.2 Results on CIFAR-100 dataset
For the experiments on the CIFAR-100 dataset, we use a CNN similar to that used for the CIFAR-10
dataset. While the results of CIFAR-100 also converges fast for SGD, CDMSGD and Federated Averaging
SGD (FedAvg) algorithms (CDMSGD being the slowest) as shown in Figure 4, it can be seen that eventually,
the loss converges better than the FedAvg algorithm. Similar to the observation made for the CIFAR-10
dataset, we observe that CDMSGD achieves significantly higher validation accuracy compared to FedAvg
while approaching similar accuracy level as that of (centralized) SGD. It can also be seen that as expected
CDSGD’s convergence is very slow compared to the others.
7.4.3 Results on MNIST dataset
For the experiments on the MNIST dataset, the model used for training is a Deep Neural Network with 20
Fully Connected layers consisting of 50 ReLU units each and the output layer with 10 units having softmax
activation. The model was trained using the catagorical cross-entropy loss. Figure 4(c & d) shows the loss
and accuracy obtained over the number of epochs. In this case, while the accuracy levels are significantly
higher as expected for the MNIST dataset, the trends remain consistent with the results obtained for the
other benchmark datasets of CIFAR-10 and CIFAR-100. Note, the generalization gap between the training
and validation data for all the methods are very less (least for CDMSGD).
7.4.4 Effect of the decaying step size
Based on the analysis presented in section 7.2, it is evident that decaying step size has a significant effect
on the accuracy as well as convergence. A performance comparison of SGD, Momentum SGD (MSGD)
and CDMSGD with a decaying stepsize is performed using the MNIST dataset. It can be seen that the
performance of the CDMSGD with decaying step size becomes slightly better than SGD with decaying step
size while (centralized) MSGD has the best performance. Although CDMSGD sometimes suffers from large
fluctuations, it demonstrates the least generalization gap among all the algorithms.
7.4.5 Effect of step size
The analysis presented in this paper shows that choice of step size is critical in terms of convergence as
well as accuracy. To explore this aspect experimentally, we compare the performance of CDMSGD for three
19
0 100 200 300 400 500 600 700 800
Number of epochs
10−2
10−1
100
101
lo
ss
es
cifar100 experiment
SGD
CDSGD
CDMSGD
Federated Averaging
(a)
0 100 200 300 400 500 600 700 800
Number of epochs
0.0
0.2
0.4
0.6
0.8
1.0
ac
cs
cifar100 experiment
SGD
CDSGD
CDMSGD
Federated Averaging
(b)
0 100 200 300 400 500 600 700 800
Number of epochs
10−3
10−2
10−1
100
lo
ss
es
MNIST experiment
SGD
CDSGD
CDMSGD
Federated Averaging
(c)
0 100 200 300 400 500 600 700 800
Number of epochs
0.2
0.4
0.6
0.8
1.0
ac
cs
MNIST experiment
SGD
CDSGD
CDMSGD
Federated Averaging
(d)
Figure 4: Average training (solid lines) and validation (dash lines) (a) loss and (b) accuracy for SGD,
CDSGD, CDMSGD and Federated averaging method for the CIFAR-100 dataset (c) loss and (d) accuracy
for SGD, CDSGD, CDMSGD and Federated averaging method for the MNIST dataset
20
0 200 400 600 800 1000
Number of epochs
10−2
10−1
100
lo
ss
es
Different Step Sizes
SGD
MSGD
CDMSGD
(a)
0 200 400 600 800 1000
Number of epochs
0.0
0.2
0.4
0.6
0.8
1.0
ac
cs
Different Step Sizes
SGD
MSGD
CDMSGD
(b)
0 100 200 300 400 500
Number of epochs
10−1
100
lo
ss
es
Different Step Sizes
1E− 1
1E− 2
1E− 3
(c)
0 100 200 300 400 500
Number of epochs
0.0
0.2
0.4
0.6
0.8
1.0
ac
cs
−0.100
−0.075
−0.050
−0.025
0.000
0.025
0.050
0.075
0.100
Va
ria
nc
e 
am
on
g 
ag
en
ts
 fo
r a
cc
sDifferent Step Sizes
1E− 1
1E− 2
1E− 3
(d)
Figure 5: Average training (solid lines) and validation (dash lines) (a) loss and (b) accuracy for SGD, MSGD
and CDMSGD method for the MNIST dataset for decaying step size. (c) loss and (d) accuracy for CDMSGD
for the MNIST data with different learning rates
21
different fixed step sizes using MNIST data. The results are presented in 5 (c) & (d), where the (fixed) step
size was varied from 0.1(1E − 1) to 0.01(1E − 2) and then to 0.001(1E − 3). While the fastest convergence
of the algorithm is observed with step size 0.1, the level of consensus (indicated by the variance among the
agents) is quite unstable. On the other hand, with very low step size 0.001, the level of consensus is quite
stable (moving average of variance remains 0). However, the convergence is extremely slow. This observation
conforms to the theoretical analysis described in the paper as well as justifies the choice of step size 0.01 in
the experiments presented above.
22

