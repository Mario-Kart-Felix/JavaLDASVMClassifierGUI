An Actor-Critic Contextual Bandit
Algorithm for Personalized Mobile Health
Interventions
Huitian Lei
Department of Statistics, University of Michigan
Ambuj Tewari
Department of Statistics, University of Michigan
Susan A. Murphy ∗
Department of Statistics,
Department of Psychiatry,
Institute of Social Research, University of Michigan
June 29, 2017
∗The authors gratefully acknowledge acknowledge funding from the National Institutes of Health grants,
R01HL125440, R01AA023187, P50DA039838, U54EB020404, NSF CAREER grant IIS-1452099 and Sloan
Research Fellowship
1
ar
X
iv
:1
70
6.
09
09
0v
1 
 [
st
at
.M
L
] 
 2
8 
Ju
n 
20
17
Abstract
Increasing technological sophistication and widespread use of smartphones and
wearable devices provide opportunities for innovative and highly personalized health
interventions. A Just-In-Time Adaptive Intervention (JITAI) uses real-time data col-
lection and communication capabilities of modern mobile devices to deliver interven-
tions in real-time that are adapted to the in-the-moment needs of the user. The lack
of methodological guidance in constructing data-based JITAIs remains a hurdle in
advancing JITAI research despite the increasing popularity of JITAIs among clinical
scientists. In this article, we make a first attempt to bridge this methodological gap by
formulating the task of tailoring interventions in real-time as a contextual bandit prob-
lem. Interpretability requirements in the domain of mobile health lead us to formulate
the problem differently from existing formulations intended for web applications such
as ad or news article placement. Under the assumption of linear reward function, we
choose the reward function (the “critic”) parameterization separately from a lower di-
mensional parameterization of stochastic policies (the “actor”). We provide an online
actor-critic algorithm that guides the construction and refinement of a JITAI. Asymp-
totic properties of the actor-critic algorithm are developed and backed up by numerical
experiments. Additional numerical experiments are conducted to test the robustness
of the algorithm when idealized assumptions used in the analysis of contextual bandit
algorithm are breached.
Keywords: mobile health, just-in-time adaptive interventions, contextual bandit problems,
bandit problems with covariates, actor-critic learning algorithms
2
1 Introduction
Equipped with sophisticated sensing, communication and computation capabilities, smart-
phones and mobile devices are being increasingly used to deliver Just-In-Time Adaptive
Interventions (JITAIs). JITAIs are mobile health interventions where treatment is delivered
in real time to individuals as they go about their daily lives. A key ingredient of a JITAI
is a policy, that is, a decision rule that inputs sensor and self-report information at any
given decision point and output a decision. The decision can be whether or not to provide
treatment or the type of treatment to be provided. The use of decision rules to adapt the
type and timing of treatment delivery to the individual makes JITAIs particularly promising
in facilitating long-term health behavior change, a pressing but notoriously hard problem
(Nahum-Shani et al. (2014)). Indeed JITAIs have received increasing popularity and have
been used to support health behavior change in a variety of domains including physical ac-
tivity (King et al. (2013); Consolvo et al. (2008)), eating disorders (Bauer et al. (2010)),
drug abuse (Scott and Dennis (2009)), alcohol use (Witkiewitz et al. (2014); Suffoletto et al.
(2012); Gustafson et al. (2011)), smoking cessation (Riley et al. (2011)), obesity and weight
management (Patrick et al. (2009)), and other chronic disorders.
Despite the growing popularity of JITAIs, there is a lack of guidance concerning how to
best learn a high-quality evidence-based JITAIs in an “online” setting. That is, learning
occurs in a sequential manner as a given user experiences the treatments and sensor/self-
report data, including health outcomes of interest, are collected. Ideally, the policy we
learn for a given user should take into account the specific way he or she responds to the
delivered treatments and is thus personalized to the user. However, most of the JITAIs
used in existing clinical trials are specified a priori and are based primarily on domain
expertise. The main contribution of this article to take a first step towards bridging the
gap between the enthusiasm for JITAIs in the mobile health field and the current lack of
statistical methodology to guide the online construction of a personalized policy for a user.
We model the learning of a user-specific optimal policy as a contextual bandit problem
(Woodroofe (1979); Langford and Zhang (2008); Li et al. (2010)). A contextual bandit
problem, also called a bandit problem with side-information, is a sequential decision making
3
problem where a learning algorithm, (i) chooses an action (e.g., treatment) at each time
point based on the context or side information, and (ii) receives an reward that reflects
the quality of the action under the current context. In mobile health settings, the context
can include summaries of the sensor and self-report data available at each time point. The
goal of the algorithm is to learn the optimal policy, that is, the policy that maximizes a
regularized average reward for a user. We propose an online “actor-critic” algorithm for
learning the optimal policy. Compared to offline learning, in online learning the contexts
and rewards arrive in a sequential fashion and the estimate of the optimal policy is updated
as data accumulates. The updated policy is used to choose the treatment action at the
subsequent time point. In our actor-critic algorithm, the critic estimates parameters in a
model for the conditional mean of the reward given context and action. The actor then
updates the estimated optimal policy based on the estimated reward model. Under idealized
assumptions, we derive asymptotic theory for the consistency and asymptotic normality of
the estimated optimal policy.
Our work is motivated by our collaboration on HeartSteps (Klasnja et al. (2015); Dempsey
et al. (2015)). In the HeartSteps project, the second and third of three studies will involve
the use of a online learning algorithm for constructing personalized policies; the algorithm
presented here represents our first step in developing the learning algorithm. The goal
of the HeartStepsproject is to reduce sedentary behavior and increase physical activity in
individuals who have experienced a cardiac event and been in cardiac rehab. The current
version of HeartSteps involves data collection both via a smartphone as well as wristband
sensor. A variety of sensor and self-report data is available at each time point, including
step count, GPS location, weather, time and user calendar busyness. The current version of
HeartSteps can deliver a treatment (an activity suggestion) at any of 5 time points per day
via an audible ping and a notification on the smartphone lock screen.
This article is organized as follows. In Section 2, we formulate online learning of a policy
for a given user as a contextual bandit problem and define what we mean by an optimal policy.
Due to the concern that deterministic policies may habituate users to treatments, thereby
causing them to ignore treatment, our definition of optimality is different from the ones found
4
in most existing contextual bandit papers. In Section 3, we present an actor-critic contextual
bandit algorithm for learning the optimal policy. In Section 4, we derive asymptotic theory
on the consistency and asymptotic normality of the estimated optimal policy. In Section 5,
we present a comprehensive simulation study to investigate the performance of the actor-
critic algorithm under various simulation settings including settings which violate the usual
assumptions underpinning contextual bandit algorithms.
2 Learning JITAIs via a Contextual Bandit Algorithm
We formulate the online learning of optimal policy for a given user as a stochastic contextual
bandit problem. A contextual bandit problem is specified by a quadruple (S, d,A, r), where
S is the context space, d is a probability distribution on the context space, A is the action
space and r is the reward space. At a decision point t, the online learning algorithm collects
the context St ∈ S, take an action At ∈ A after which a reward Rt ∈ r is revealed before
the next decision point. The online learning algorithm does not have access to the rewards
that would occur had other actions been taken. Prior to decision point t+ 1, the algorithm
has access to the sequence of tuples {(Sτ , Aτ , Rτ )}tτ=1. We make the following assumption.
Assumption 1. (i.i.d. contexts) Action At has a in-the-moment effect on the reward Rt
with expected reward function:
E (Rt|St = s, At = a) = r(s, a),
but At does not affect the distribution of Sτ for τ ≥ t+ 1. We further assume that contexts
St are i.i.d. with probability density function d(s).
This assumption matches the conceptual design of many JITAIs well. In fact, interven-
tion options in a JITAI are sometimes referred to as “Ecological Momentary Interventions”
(EMIs) or “micro-interventions”. Such a terminology emphasizes that the effects of many of
the treatments in this domain are expected to be short-lived in nature.
A (stochastic) policy is a mapping from the context space to (a probability distribution
over) the action space. In JITAIs, policies are used to specify (the probability of) an ac-
tion given a context. In this article, we focus on a binary action space A = {0, 1} and
5
a class of parametrized stochastic policies in which P (A = 1|S = s) is parameterized as
πθ(s, 1) =
eg(s)
T θ
1+eg(s)
T θ
. Here g(s) is a p-dimensional vector that contains candidate variables
that may be useful for decision making. Using the parametrized policy, the way each variable
in g(s) influences the choice of action is reflected by the sign and magnitude of the corre-
sponding component in θ. Confidence intervals for and hypothesis testing on the optimal θ
can answer scientific questions about the usefulness of a particular contextual variable for
decision making. For example, suppose the scientist includes a GPS location based variable
as a candidate variable in the policy, yet the confidence interval for the θ coefficient of this
variable turns out to contain 0. Then we might omit the sensing of this variable in future
because continuously sensing GPS location on smartphones drains the battery. Similarly,
self-reported measures on user’s emotional states induce user burden. Therefore, if the con-
fidence interval for the θ coefficients of these variables contains 0 we may reduce user burden
by omitting their collection.
2.1 The Regularized Average Reward
Empirical evidence and some behavioral science theories indicate that deterministic poli-
cies can lead to habituation and that treatment variety can increase user engagement and
retard habituation (Raynor and Epstein (2001); Epstein et al. (2009, 2011); Wilson et al.
(2005)).To maintain treatment variety, we consider stochastic policies. However it turns out
that standard definitions of optimality often lead to deterministic policies. For example, a
natural and intuitive definition of an optimal policy is a policy that maximizes the average
reward:
V ∗(θ) =
∫
s∈S
d(s)
∑
a∈A
r(s, a)πθ(s, a)ds,
where d(s) is the probability density function of context. The following lemma shows that,
in a simple setting where the context space is one-dimensional and finite, there always exists
a deterministic optimal policy. The proof of this lemma is provided in the supplementary
material section A.
Lemma 1. Suppose that the context space is discrete and finite, S = {s1, s2, ..., sK}. Among
6
the policies parameterized as πθ(s, 1) =
eθ0+θ1s
1+eθ0+θ1s
, there exists a policy that maximizes V ∗(θ)
for which P (πθ(S, 1) = 0 or 1) = 1.
One way to ensure treatment variety is to introduce a chance constraint (also called a
“probabilistic constraint”; see, e.g., Prékopa (1995)) that ensures, with high probability over
the context distribution, that the probability of taking each treatment action under any
policy we consider is bounded sufficiently away from 0. For binary actions the constraint
has the form:
P (p0 ≤ πθ(S, 1) ≤ 1− p0) ≥ 1− α (1)
where 0 < p0 < 0.5, 0 < α < 1 are constants controlling the amount of stochasticity. The
stochasticity constraint requires that, for at least (1 − α)100% of the contexts, there is at
least p0 probability to take either of the two available actions.
Maximizing the average reward V ∗(θ) subject to the stochasticity constraint (1) is a
chance constrained optimization problem, an active research area in recent years (Nemirovski
and Shapiro (2006); Campi and Garatti (2011)). Solving this chance constraint problem,
however, involves a major difficulty: constraint (1) is, in general, a non-convex constraint on
θ. Moreover, the left hand side of the chance constraint is an expectation of a non-smooth
indicator function. Both the non-convexity and the non-smoothness make the optimization
problem computationally intractable. We circumvent this difficulty by relaxing constraint
(1) to a convex alternative:
θTE[g(S)g(S)T ]θ = θT [
∫
s∈S
g(s)g(s)Td(s)ds]θ ≤
(
log(
p0
1− p0
)
)2
α, (2)
which is obtained by bounding the probability in (1) using Markov’s inequality and some
algebra. Since the quadratic constraint is derived using an upper bound on the original
probability, it is more stringent than the chance constraint and always guarantees at least
the desired amount of treatment variety.
Instead of solving the quadratic optimization problem that maximizes the average reward
V ∗(θ) subject to the quadratic constraint (2), we choose to maximize the corresponding La-
grangian function. Incorporating inequality constraints by using Lagrangian multipliers has
7
been widely used in reinforcement learning literature to solve constrained Markov decision
problem (Borkar (2005); Bhatnagar and Lakshmanan (2012)). Given a Lagrangian multiplier
λ, the following Lagrangian function:
J∗λ(θ) =
∫
s∈S
d(s)
∑
a∈A
r(s, a)πθ(s, a)ds− λ θTE[g(S)g(S)T ]θ (3)
is referred to as the regularized average reward in this article. For a fixed value of λ, we define
the optimal policy to be the policy that maximizes the regularized average reward, namely
θ∗λ = argmax J
∗
λ(θ). Under mild regularity conditions, we show that there is a one-to-one
correspondence between quadratic constrained optimization of the average reward (using the
constraint (2)) and the unconstrained optimization of the regularized average reward (3).
Details can be found in the supplementary material section B. There are two computational
advantages of maximizing the regularized average reward as opposed to solving a constrained
optimization. First, optimizing the regularized average reward function results in a unique
solution even when there is no treatment effect. When the expected reward does not depend
on the treatment action, i.e., E(R|S = s, A = a) = E(R|S = s), all policies in the feasible
set given by the constraint have the same average reward. The regularized average reward
function, in contrast, has a unique maximizer at θ = 0p×1, a purely random policy that
assigns 50% probability to both actions. Therefore, maximizing the regularized average
reward gives rise to a 0 estimand when there is no treatment effect. Second, even when
the uniqueness of optimal policy is not an issue, maximization of J∗λ(θ) has computational
advantages over maximization of V ∗(θ) under the constraint (2) because the subtraction
of the quadratic term λθTE[g(S)g(S)T ]θ introduces a degree of concavity to the surface of
J∗λ(θ), thus stabilizing the optimization.
3 The Online Actor-Critic Algorithm
In this section, we propose an online actor-critic algorithm for learning the policy parameter
θ∗λ. We consider a fixed λ in this section and will drop the subscript in θ
∗
λ from now on.
Recall that right before decision point t + 1, the observed “training data” consists of a
stream of triples {(Sτ , Aτ , Rτ )}tτ=1. The θ coefficients in the optimal policy can be estimated
8
by maximizing an empirical version of the aforementioned regularized average reward:
Jλ(θ) =
1
t
t∑
τ=1
∑
a
r(Sτ , a)πθ(Sτ , a)− λθT
(
1
t
t∑
τ=1
g(Sτ )g(Sτ )
T
)
θ. (4)
The proposed actor-critic algorithm has two parts: the critic estimates the expected reward,
r; the estimated expected reward is then plugged into (4), which is then maximized to
estimate the parameter, θ, in the optimal policy. The estimated optimal policy is used to
select an action at the next decision point.
We develop a critic algorithm based on a linear assumption for the expected reward
function.
Assumption 2. (Linear model assumption) Given context St = s and action At = a,
the reward is generated according to the model Rt = f(s, a)
Tµ∗ + t, where f(s, a) is a k-
dimensional reward feature. The error terms t are i.i.d. with mean 0 and variance σ
2. In
particular, we have r(s, a) = f(s, a)Tµ∗.
See Section 5 for simulation results concerning the robustness of the developed method
to breakdown of this linearity assumption as well as Section 6 for discussion. We further
assume that values of the reward, the reward parameter and the reward feature are bounded
as follows.
Assumption 3. (Bounded rewards and features) There exist constants which provide an a.s.
upper bound on the absolute value |R| of the reward as well as the norms |f(S,A)|2, |µ∗|2 of
the reward feature and reward parameter vector. Furthermore, we assume that we know the
constant K for which P [|R| ≤ K] = 1. Without further loss of generality we assume all of
these constants are 1 (including K = 1).
Bounded rewards along with bounded features are standard assumption in the bandit
literature (see for instance Agrawal and Goyal (2013)). In practice, a known bound on the
reward is usually available. For example, consider HeartSteps in which the reward is the
number of steps over 30 minutes following time t; there is a generally accepted upper limit
on the number of steps a human can take in 30 minutes.
9
A natural estimator of the reward parameter µ∗ is the L2 penalized least squares estima-
tor:
µ̂t =
(
ζI +
t∑
τ=1
f(Sτ , Aτ )f(Sτ , Aτ )
T
)−1 t∑
τ=1
f(Sτ , Aτ )Rτ (5)
where the ζ is the weight on the L2 penalty. This penalty ensures invertibility of the first
term on the right hand side when t is small (note that k, the dimension of the reward feature
f does not vary with time t so the penalty term is solely to ensure invertibility). Note that
even though we have assumed that the contexts Sτ are i.i.d., this does not meant that the
feature vectors, f(Sτ , Aτ ) are i.i.d. Indeed recall that the actions Aτ are drawn according
to the estimated optimal policy at decision point τ − 1, which depends on the entire history
at or before decision point τ − 1. This dependency presents challenges in analyzing the
actor-critic algorithm; see Section 4.
An additional challenge in analyzing the actor-critic algorithm is getting around an inher-
ent circular dependence: the boundedness of the actor estimates depends on the boundedness
of the estimated reward function, or equivalently the critic. The critic’s estimates, in turn,
depends on the actions selected by the actor. To deal with this challenge we use the known
bound from assumption 3, to construct a bounded estimator of r(s, a). The penalized least
squares estimator results in the estimator r̂t(S,A) = f(S,A)
T µ̂t; this estimator may not
be bounded a.s. even though by assumption 3, |r(S,A)| is bounded by 1 a.s. We enforce
boundedness on the estimator of r(s, a); in particular we replace r̂t(s, a) by
r̂t(s, a) =

−2 if f(s, a)T µ̂t < −2
f(s, a)T µ̂t if |f(s, a)T µ̂t| ≤ 2
2 if f(s, a)T µ̂t > 2
(6)
The estimated reward function, r̂t(s, a), is the output of the critic step. In Theorem 1 we
show that the above procedure will result in a consistent µ̂t. Thus for any  > 0, for large t,
with high probability, |r̂t(S,A)|2 will a.s. be less than 1+ . We point out that, when reward
is bounded by a positive constant K other than 1, one shall modify the above projection by
replacing 2 by K + 1.
10
Next the actor step maximizes the estimated Jλ(θ):
Ĵt(θ, µ̂t) =
1
t
t∑
τ=1
∑
a
r̂t(Sτ , a)πθ(Sτ , a)− λ θT
(
1
t
t∑
τ=1
g(Sτ )g(Sτ )
T
)
θ (7)
at decision point t to obtain θ̂t. The action at decision point t+1 is selected according to the
stochastic policy πθ̂t(St+1, a). The actor critic algorithm, which alternates between a critic
step and an actor step is depicted in Algorithm 1.
Algorithm 1: An online actor-critic algorithm with linear expected reward and
stochastic policies
Inputs: T , the total number of decision points; a k dimensional reward feature
f(s, a); a p dimensional policy feature g(s).
Critic initialization: B(0) = ζIk×k; A(0) = 0k×1.
Actor initialization: θ0 is initial policy parameter based on domain theory or
historical data.
Start from t = 0.
while t ≤ T do
At decision point t, observe context St.
Draw an action At according to probability distribution πθ̂t−1(St, A).
Observe an immediate reward Rt.
Critic update:
B(t) = B(t− 1) + f(St, At)f(St, At)T , A(t) = A(t− 1) + f(St, At)Rt,
µ̂t = B(t)
−1A(t). The estimated reward function is r̂t(s, a) from (6).
Actor update:
θ̂t = argmax
θ
1
t
t∑
τ=1
∑
a
r̂t(Sτ , a)πθ(Sτ , a)− λ θT
(
1
t
t∑
τ=1
g(Sτ )g(Sτ )
T
)
θ.
Go to decision point t+ 1.
end
11
4 Asymptotic Theory for the Actor-Critic Algorithm
In this section, we present consistency and the asymptotic normality results for the proposed
actor-critic algorithm. Proofs are provided in the supplementary material section C. In
addition to assumptions 1, 2 and 3, we make the following assumption that ensures the
identifiability of each component of the policy parameter.
Assumption 4. (Positive definiteness of policy features) The p× p matrix E(g(S)g(S)T ) =∫
s∈S d(s)g(s)g(s)
Tds is positive definite.
As the very first step towards establishing the asymptotic properties of the actor-critic
algorithm, we show that, for a fixed Lagrangian multiplier λ, the optimal policy parameter
that maximizes the regularized average reward (3) lies in a bounded set. Moreover, the
estimated optimal policy parameter is bounded with probability going to 1. Lemma 2 sets
the foundation for us building on which we can use existing asymptotic statistical theory.
Lemma 2. Assume that Assumption 3 and 4 hold. Given a fixed λ, the population optimal
policy parameter, θ∗, lies in a compact set. In addition, the sequence of estimated optimal
policy parameters, θ̂t, lies in a compact set almost surely as t→∞. Denote this compact set
by Cθ∗.
Following this lemma, we make additional assumptions to establish the asymptotic theory
of the actor-critic algorithm.
Assumption 5. (Positive definiteness of reward features) The k×k matrix Eθ(f(S,A)f(S,A)T ) =∫
s∈S d(s)
∑
a f(s, a)f(s, a)
Tπθ(s, a)ds is positive definite for all θ in the compact set Cθ∗ in
Lemma 2.
Assumption 6. (Uniform separateness of the global maximum) There exists a neighborhood
of µ∗, say B(µ∗) such that the following holds. J(θ, µ) as a function of θ has unique global
maximum denoted by θµ, for each µ ∈ B(µ∗). Moreover, for any δ > 0, there exists  > 0
and neighborhood of θµ, denoted by B(θµ, ), such that
J(θµ, µ)− max
θ/∈B(θµ,)
J(θ, µ) ≥ δ (8)
for all µ ∈ B(µ∗).
12
Under assumptions 1 through 6, the following theorems establish the consistency and
asymptotic normality of the critic and the actor.
Theorem 1. (Asymptotic properties of the critic) The k× 1 vector µ̂t converges to the true
reward parameter µ∗ in probability. In addition,
√
t(µ̂t − µ∗) converges in distribution to
a multivariate normal with mean 0k×1 and covariance matrix [Eθ∗(f(S,A)f(S,A)T )]−1σ2,
where Eθ(f(S,A)f(S,A)T ) =
∫
s
d(s)
∑
a f(s, a)f(s, a)
Tπθ(s, a)ds is the expected value of
f(S,A)f(S,A)T under the policy with parameter θ, and σ is the standard deviation of the er-
ror term in Assumption 2. The plug-in estimator of the asymptotic covariance is consistent.
Theorem 2. (Asymptotic properties of the actor) The p × 1 vector θ̂t converges to θ∗ in
probability. In addition,
√
t(θ̂t − θ∗) converges in distribution to multivariate normal with
mean 0p×1 and covariance matrix [Jθθ(µ
∗, θ∗]−1V ∗[Jθθ(µ
∗, θ∗)]−1, where
V ∗ = σ2Jθµ(µ
∗, θ∗)Eθ[f(S,A)f(S,A)T ]Jµθ(µ∗, θ∗) + E[jθ(µ∗, θ∗, S)jθ(µ∗, θ∗, S)T ]
. In the expression of asymptotic covariance matrix,
jθ(µ, θ, S) =
∂
∂θ
(∑
a
f(S, a)Tµ πθ(S, a)− λθT [g(S)g(S)T ]θ
)
,
and both Jθθ and Jθµ are the second order partial derivatives with respect to θ twice and with
respect θ and µ, respectively of J :
J(µ, θ) =
∫
s∈S
d(s)
∑
a∈A
f(s, a)Tµ πθ(s, a)ds− λθTE[g(S)g(S)T ]θ. (9)
A bound on the expected regret can be derived as a by-product of the square-root con-
vergence rate of θ̂t. The expected regret of an online algorithm up to time T is the difference
between the expected cumulative reward under the algorithm and that under the optimal
policy θ∗:
U(T ) = T
∫
s∈S
d(s)
∑
a
r(s, a)πθ∗(s, a)ds− E
[
T∑
t=1
Rt
]
(10)
13
where {Rt}Tt=1 is the sequence of rewards generated in the algorithm and the expectation
E is with respect to the distribution of {St, At ∼ πθ̂t−1 , Rt}
T
t=1. Straightforward calculation
shows that,
U(T ) =
T∑
t=1
∫
s∈S
d(s)
∑
a
r(s, a)[πθ∗(s, a)− E(πθ̂t−1(s, a))]ds
=
T∑
t=1
∫
s∈S
d(s)
∑
a
r(s, a)E[π′
θ̂t,s,a
(s, a)(θ∗ − θ̂t−1)]ds
where θ̂t,s,a is a random variable that lies on the line segment joining θ
∗ and θ̂t−1. The
boundedness of r(s, a) together with Theorem 2 imply the following corollary.
Corollary 1. The expected regret U(T ) of the actor-critic algorithm 1 is O(
√
T ).
Readers familiar with contextual bandit literature may wish to compare the above regret
bound with the regret bounds for LinUCB (Chu et al. (2011)) and for Thompson sampling
(Agrawal and Goyal (2013)). There are at least three differences between our framework
and those considered in LinUCB and Thompson sampling papers. First, we parameterize
explicitly both the policy class as well as the expected reward. These two papers parameter-
ize the expected reward which then implicitly implies a parameterized deterministic policy
class. As a result, our optimal policy is the policy that maximizes the regularized average
reward over our explicitly defined policy class; however their optimal policy is the policy
that maximizes the unregularized average reward. Second, we restrict our policy class to be
stochastic whereas their implicitly defined policy class is composed of deterministic policies.
Lastly, the setting considered here is more restrictive in that we assume contexts are i.i.d.
whereas these papers allows for arbitrary contexts as long as the conditional mean of the
reward in any context is linear in the context features.
5 Numerical Experiments
To assess the performance of the proposed actor-critic algorithm we conducted extensive
simulations across a variety of realistic scenarios. Firstly, we conduct simulations to eval-
uate the relevance of our asymptotic theory in finite T settings in which the contexts are
14
indeed i.i.d. As will be seen, the bias and mean squared error (MSE) in estimating opti-
mal policy decreases to 0 as sample size increases, and the bootstrap confidence interval
for the optimal policy parameter achieves nominal confidence level. Secondly, we note that
the i.i.d. assumption on the contexts is likely violated in real world applications in at least
two ways: the current context may be influenced by the context at previous decision points
and the current context may be influenced by past actions. Simulations in which the con-
texts follow an auto-regressive process show that the actor-critic algorithm is quite robust
to auto-correlation among contexts. We also create simulation settings where context is
influenced by previous actions through a burden effect of the treatments on the users. We
observe reasonable robustness of the bandit actor-critic algorithm when the burden effects
are small or moderate. Last but not least, we investigate how performance of the algorithm
may deteriorate when assumption 2 is violated, that is, the conditional mean of the reward
is non-linear.
Throughout we base the simulations on a generative model that is motivated by the
Heartsteps application for improving daily physical activity (Klasnja et al. (2015); Dempsey
et al. (2015)). A simplified description of HeartSteps follows. HeartSteps is a mobile health
smartphone application seeking to reduce users’ sedentary behavior and increase physical
activity such as walking. A commercial wristband sensor is usted to collect minute level steps
counts. Each evening self-report on the usefulness of the application as well as problems in
daily life are collected. At each of 3 decision points per day, sensor data is collected including
the user’s current location (home/work/other) and weather. At each decision point, the
algorithm on the smartphone application must decide whether to “push” a tailored physical
activity suggestion, i.e., At = 1, or remain silent, i.e., At = 0. Our generative model uses a
three dimensional context at decision point t: St = [St,1, St,2, St,3]. St,1 represents weather,
with St,1 = −∞ being extremely severe and unfriendly weather for any outdoor activities
and St,1 =∞ being the opposite. St,2 reflects the user’s recent habits in engaging in physical
activity. St,2 =∞ represents that the user has been maintaining positive daily physical habits
while St,2 = −∞ represents the opposite. St,3 is a composite measure of disengagement with
HeartSteps. St,3 = −∞ reflects an extreme state that the user is fully engaged, is adherent
15
and is reporting that the application is useful. On the other hand, St,3 = ∞ denotes the
opposite state of disengagement. Note that although we assumed bounded features in proving
theoretical properties of the algorithm, these feature vectors have unbounded range. Results
shown in later sections demonstrate robustness to the boundness assumption.
The goal of HeartSteps is to reduce users’ sedentary behavior. Here we reverse code the
reward and define the cost to be the sedentary time per hour between two decision points.
So the goal of the actor-critic algorithm is to minimize an average penalized cost as opposed
to maximizing an average penalized reward. The generative model for the cost is a linear
model: Ct = 10 − .4St,1 − .4St,2 − At × (0.2 + 0.2St,1 + 0.2St,2) + 0.4St,3 + ξt,0, where ξt,0
are i.i.d. N(0,1) errors. In this linear model, higher values of S1 and S2, good weather and
positive physical activity habits, are associated with less sedentary time while a higher value
of S3, disengagement, leads to increased sedentary time. The negative main effect of At
indicates that physical activity suggestion (At = 1) reduces sedentary behavior compared to
no suggestion At = 0. The negative interaction between At and St,1 and between At and St,2
reflects that physical activity suggestions are more effective when the weather condition is
activity friendly or when the user has acquired good physical activity habits.
The class of parametrized policies is πθ(S, 1) =
eθ0+
∑3
i=1 θiSi
1+eθ0+
∑3
i=1
θiSi
. The average cost under
policy πθ is:
C(θ) =
∫
s∈S
dθ(s)
∑
a
E(C|S = a,A = a)πθ(s, a)ds
where dθ(s) is the stationary distribution of context under policy πθ. When actions have
no impact on context distributions, the stationary distribution d(s) does not depend on the
policy parameter θ. In this case, the average cost reduces to: C(θ) =
∫
s∈S d(s)
∑
a E(C|S =
a,A = a)πθ(s, a)ds. This is true for the generative models we investigate in Section 5.1 and
Section 5.2. The generative model we investigate in Section 5.3 allows actions to impact the
context distribution at future decision points. In such a case, the stationary distribution of
context depends on the policy parameter θ. A quadratic constraint is enforced so that the
optimal policy is stochastic. In the quadratic inequality (2), we use α = 0.1 and p0 = 0.1
throughout the numerical experiment unless otherwise specified. We then minimize the
corresponding Lagrangian function.
16
The optimal policy θ∗ and the oracle λ∗. According to results in Section 2.1, we have
that for every pair of (p0, α) there exists a Lagrangian multiplier λ
∗ such that the optimal
solution to the regularized average cost function:
θ∗ = argmin
θ
C(θ) + λθT
∑
s
dθ([1, s1, s2, s3][1, s1, s2, s3]
T )θ (11)
satisfies the quadratic constraint with equality. Furthermore, as λ increases the stringency
of the quadratic constraint increases: an increased value of λ penalizes the quadratic term
θ∗T
∑
s dθ∗([1, s1, s2, s3][1, s1, s2, s3]
T )θ∗ more heavily. For a fixed pair of (p0, α), we perform
a line search to find the smallest λ, denoted as λ∗, such that the minimizer to the regularized
average cost, denoted as θ∗ satisfies the quadratic constraint. We recognize the difficulty in
solving the optimization problem due to the non-convexity of the regularized average cost
function. In our search for a global minimizer, we therefore use grid search, for a given λ,
to find a crude solution to the optimization problem. We then improve the accuracy of the
optimal solution using a more refined grid search provided by the pattern search function in
Matlab. The regularized average cost function is approximated by Monte Carlo samples. We
used 5000 Monte Carlo samples to approximate the regularized average cost for simulation
in Section 5.1 and Section 5.2 where the stationary distribution of contexts does not depend
on the policy. For the simulations in Section 5.3, where context distribution does depend on
the policy, we generate a trajectory of 100000 Monte Carlo samples and ignore the first 10%
of the samples to approximate the stationary distribution.
Estimating λ online. In practice, the decision maker has no access to the oracle La-
grangian multiplier λ∗. A natural remedy is to integrate the estimation of λ∗ with the online
actor-critic algorithm that estimates the policy parameters. An actor-critic algorithm with a
fixed Lagrangian multiplier solves the “primal” problem while the “dual” problem is solved
by searching for λ∗. Our integrated algorithm performs a line search to find the smallest
λ such that the estimated optimal policy satisfies the quadratic constraint. The stationary
distribution of the contexts is approximated by the empirical distribution. Estimating λ
can be very time consuming, therefore in our simulations, the algorithm performs the line
search over λ only every 10 decision points. Similar ideas with gradient based updates on λ
have appeared in reinforcement literature to find the optimal policies in constrained MDP
17
problems, see Borkar (2005); Bhatnagar and Lakshmanan (2012) for examples.
Bootstrap confidence intervals. In a number of trial simulations, we found that
the plug-in variance estimator derived from Theorem 2 tends to underestimate in small to
moderate sample size, a direct consequence of which is the anti-conservatism of the Wald
confidence interval. Details of the anti-conservatism are discussed in the supplementary
material section D. Our solution to the anti-conservative Wald confidence interval is the
percentile-t bootstrap confidence interval. Algorithm 2 shows how to generate a bootstrap
sample. Algorithm 2 is repeated for a total of B times to obtain a bootstrap sample of the
estimated optimal policy parameters, {θ̂bT}Bb=1 and plug-in variance estimates, {V̂ bT}Bb=1. We
create bootstrap percentile-t confidence intervals for θ∗i , the i-th component of the optimal
policy parameter. For each θ∗i , we use the empirical percentile of
{√
t(θ̂bT,i−θ̂T,i)√
V̂ bT
}B
b=1
, denoted
by pα to replace the normal distribution percentile in Wald confidence intervals. A (1−2α)%
confidence interval is [
θ̂T,i − pα
V̂i√
T
, θ̂T,i + pα
V̂i√
T
]
(12)
where θ̂T,i is the i-th component of θ̂T and V̂i is the plug-in variance estimate based on the
original sample.
Simulation details. The simulation results presented in the following sections are based
on 1000 independent simulated users. For each simulated user, we allow a burn-in period of
20 decision points. During the burn-in period, actions are chosen by fair coin flips. After
the burn-in period, the online actor-critic algorithm is implemented to learn the optimal
policy and obtain an end-of-study estimated optimal policy at the last decision point. In
these simulations we did not force r̂t(s, a) to be in the interval [−2, 2] as in Algorithm 1 or
Algorithm 2. We do not encounter any issues in convergence of the algorithm.
Both bias and MSE shown in all of the following tables are averaged over 1000 end-of-
study estimated optimal policies. For each simulated user the 95% bootstrapped confidence
intervals for θ∗ is based on 500 bootstrapped samples generated by Algorithm 2. With 95%
confidence, we expect that the empirical coverage rate of a confidence interval should be
within 0.936 and 0.964, if the true confidence level is 0.95.
18
Algorithm 2: Generating a bootstrap sample estimate θ̂bT , V̂
b
T
Inputs: The observed context history {St}Tt=1. A bootstrap sample of residuals
{bt}Tt=1. The estimated reward parameter µ̂T
Critic initialization: B(0) = ζIk×k, a k × k identity matrix. A(0) = 0k is a k × 1
column vector.
Actor initialization: θ̂b0 = θ̂0 is the best treatment policy based on domain theory or
historical data.
while t < T do
Context is St ;
Draw an action Abt according to policy πθ̂bt−1
;
Generate a bootstrap reward Rbt = f(St, A
b
t)
T µ̂T + 
b
t ;
Critic update:
B(t) = B(t− 1) + f(St, At)f(St, At)T , A(t) = A(t− 1) + f(St, At)Rbt ;
µ̂bt = A(t)
−1B(t). The bounded estimate to reward function is r̂bt (s, a). ;
Actor update:
θ̂bt = argmax
θ
1
t
t∑
τ=1
∑
a
r̂bt (Sτ , a)πθ(a|St)− λθT [
1
t
t∑
τ=1
g(Sτ , 1)
Tg(Sτ , 1)]θ
Go to decision point t+ 1 ;
end
Plugin µ̂bT and θ̂
b
T to the asymptotic variance formula to get a bootstrapped variance
estimate V̂ bT .
19
T (sample size)
Bias MSE
θ0 θ1 θ2 θ3 θ0 θ1 θ2 θ3
200 −0.081 −0.090 −0.089 0.010 0.054 0.052 0.052 0.055
500 −0.053 −0.037 −0.034 −0.002 0.027 0.024 0.021 0.029
Table 1: I.I.D. contexts: bias and MSE in estimating the optimal policy parameter.
Bias=E(θ̂T )− θ∗
.
T(sample size) θ0 θ1 θ2 θ3
200 0.962 0.942 0.938 0.945
500 0.96 0.948 0.968 0.941
Table 2: I.I.D. contexts: coverage rates of percentile-t bootstrap confidence intervals for the
optimal policy parameter.
5.1 I.I.D. Contexts
In this generative model, we choose the simplest setting where contexts at different deci-
sion points are i.i.d. We generate contexts {[St,1, St,2, St,3]}Tt=1 from a multivariate normal
distribution with mean 0 and identity covariance matrix. The population optimal policy
is θ∗ = [0.417778, 0.394811, 0.389474, 0.001068] at λ∗ = 0.046875. Table 1 lists the bias
and mean squared error (MSE) of the estimated optimal policy parameters. Both measures
shrink towards 0 as T , sample size per simulated user, increases from 200 to 500, which
is consistent with the convergence in estimated optimal policy parameter as established in
Theorem 2. Table 2 shows the empirical coverage rates of percentile-t bootstrap confidence
interval at sample sizes 200 and 500. At sample size 200, the empirical coverage rates are
between 0.936 and 0.964 for all θi’s. At sample size 500, however, the bootstrap confidence
interval for θ2 is a little conservative with an empirical coverage rate of 0.968.
20
5.2 AR(1) Context
In this section, we study the performance of the actor-critic algorithm when the dynamics
of the context is an auto-regressive stochastic process. We envision that in many health
applications, contexts at adjacent decision points are likely to be correlated. Using Heart-
Steps as an example, weather (S1) at two adjacent decisions points are likely to be similar.
So are users’ learning ability (S2) and disengagement level S3. One way to incorporate the
correlation among contexts at near-by decision points is through a first order auto-regression
process. We simulate the context according to
St,1 = 0.4St−1,1 + ξt,1,
St,2 = 0.4St−1,2 + ξt,2,
St,3 = ξt,3
Here we choose ξt,1 ∼ N(0, 1 − 0.42), ξt,2 ∼ N(0, 1 − 0.42) and ξt,3 ∼ N(0, 1) so that the
stationary distribution of St is multivariate normal with zero mean and identity covariance
matrix, same as the distribution of St in the previous section. The initial distribution of
St, t = 1 is a multivariate standard normal.
The oracle Lagrangian multiplier is λ∗ = 0.05 and the population optimal policy is
θ∗ = [0.417, 0.395, 0.394, 0], same as in the i.i.d. simulation. Bias and MSE of the estimated
policy parameters are shown in Table 3. Empirical coverage rate of the percentile t bootstrap
confidence interval is reported in Table 4. Both the bias and MSE diminish towards 0 as the
sample size increases from 200 to 500, a clear indication that convergence of the algorithm
is not affected by the auto-correlation in context. The bootstrap confidence interval for θ3
is anti-conservative at sample size 200, but recovers decent coverage at sample size 500.
5.3 Actions Cause Increased Burden
In this section, we study behavior of the actor-critic algorithm in the presence of an interven-
tion burden effect. Our generative model with a burden effect represents a scenario where
users disengage with the Heartsteps application, and hence the recommended intervention,
if the application provides physical activity suggestions at too high a frequency. When users
21
T (sample size)
Bias MSE
θ0 θ1 θ2 θ3 θ0 θ1 θ2 θ3
200 −0.093 −0.089 −0.076 0.006 0.058 0.053 0.047 0.057
500 −0.046 −0.032 −0.040 −0.005 0.025 0.022 0.024 0.028
Table 3: AR(1) contexts: bias and MSE in estimating the optimal policy parameter.
Bias=E(θ̂T )− θ∗.
T(sample size) θ0 θ1 θ2 θ3
200 0.963 0.952 0.957 0.927*
500 0.969 0.962 0.96 0.949
Table 4: AR(1) contexts: coverage rates of percentile-t bootstrap confidence intervals. Cov-
erage rates significantly lower than 0.95 are marked with asterisks (*).
experience intervention burden effects, they become frustrated and have a tendency of falling
back to their sedentary behavior. In our burden effect generative model, St,3 represents the
disengagement level whose value increases if there is a physical activity suggestion at the
previous decision point At−1 = 1. The positive main effect of St,3 in the cost model (13) be-
low reflects that higher disengagement level is associated with higher cost (higher sedentary
time). The initial distribution of St is the standard multivariate normal distribution. After
the first decision point, contexts are generated according to the following stochastic process:
St,1 = 0.4St−1,1 + ξt,1,
St,2 = 0.4St−1,2 + ξt,2,
St,3 = 0.4St−1,3 + 0.2St−1,3At−1 + 0.4At−1 + ξt,3
We simulate the cost, sedentary time per hour between two decision points, according to the
following linear model:
Ct = 10− .4St,1 − .4St,2 − At × (0.2 + 0.2St,1 + 0.2St,2) + τSt,3 + ξt,0. (13)
22
where parameter τ controls the “size” of the burden effect: the larger τ is, the more severe
the burden effect is. We study the performance of our algorithm in five different cases
corresponding to τ = 0, 0.2, 0.4, 0.6, 0.8. Different values of τ represent users who experience
different levels of burden effect. τ = 0 represents the type of users who experience no burden
effect while τ = 0.8 represents the type of users who experience a large burden effect.
Table 15 in the supplementary material section E lists the oracle λ∗ and the corresponding
optimal policy θ∗ at different levels of burden effect. Higher level of burden effects calls for
increased value of oracle λ∗ to keep the desired intervention variety. The negative sign of
θ∗3 at τ ≥ 0.2 indicates that the application should lower the probability of pushing an
activity suggestion when the disengagement level is high. The magnitude of θ∗3 rises with
the size of the burden effect, implying that as burden effect increases the application should
further lower the probability of pushing activity suggestions at high disengagement level. θ∗0
decreases to be negative when τ increases, which indicates that as the size of burden effect
grows, the application should lower the frequency of activity suggestions in general.
Table 5 and 6 list the bias, MSE and the empirical coverage rate of the percentile-t
bootstrap confidence interval at sample size 200. Table 7 and 8 list these three measures at
sample size 500. When there is no burden effect (τ = 0), St,3 has no influence on the cost and
is therefore considered as a “noise” variable. The optimal policy parameters are estimated
with low bias and MSE under the generative model with τ = 0 and the bootstrap confi-
dence intervals have decent coverage, both of which are clear indications that the algorithm
is robust to presence of noise variables that are affected by previous actions. As burden
effects levels go up, we observe an increased bias and MSE in the estimated optimal policy
parameters, θ0 and θ3 in particular. The empirical coverage rates of bootstrap confidence
intervals for θ0 and θ3 are below the nominal 95% level. There are two reasons to explain the
increased bias and MSE. The most important one is the near-sightedness of bandit actor-
critic algorithm. The bandit algorithm chooses the policy that maximizes the (immediate)
average cost while ignoring the negative consequence of a physical activity suggestion At = 1
on the disengagement level at the next decision point. The bandit algorithm therefore tends
to “over-treat” in general and in particular at high disengagement level, which is reflected
23
in an over-estimated θ0 and θ3. The second reason comes from the bias in estimating λ,
the Lagrangian multiplier. The oracle Lagrangian multiplier λ∗ is chosen so that the opti-
mal policy parameter satisfies the quadratic constraint while the online bandit actor-critic
algorithm estimates the Lagrangian multiplier so that the bandit-estimated optimal policy
satisfies the quadratic constraint. To separate the consequence of underestimated λ from the
consequence of the myopia of the bandit algorithm, we implement the bandit algorithm with
oracle λ∗. Results of these experiments are shown in the supplementary material section E.
We observe that, even with the use of oracle λ∗, the overestimation of θ0 and θ3 as well as
the anti-conservatism of the confidence intervals are still present.
Overall, the estimation of θ1 and θ2 shows robustness to the presence of burden effects. θ1
and θ2 are estimated with low bias and MSE under the presence of small to moderate burden
effects (τ = 0.2, 0.4). While we observe biases in estimating θ1 and θ2 under moderate to
large burden effects (τ = 0.6, 0.8), the magnitude of such bias increases slowly with the size
of the burden effect. Empirical coverage rates of the bootstrap confidence intervals for θ1
and θ2 are decent for τ = 0.2, 0.4 and only degrade slowly under 95% when τ = 0.6, 0.8.
τ
Bias MSE
θ0 θ1 θ2 θ3 θ0 θ1 θ2 θ3
0 −0.027 −0.036 −0.030 0.003 0.058 0.037 0.036 0.036
0.2 0.229 −0.093 −0.104 0.164 0.110 0.044 0.046 0.063
0.4 0.506 −0.063 −0.035 0.235 0.313 0.040 0.037 0.091
0.6 0.645 0.043 0.073 0.272 0.473 0.038 0.041 0.110
0.8 0.702 0.084 0.096 0.272 0.550 0.043 0.045 0.110
Table 5: Burden effect: bias and MSE in estimating the optimal policy parameter at sample
size 200. Bias=E(θ̂T )− θ∗.
24
τ θ0 θ1 θ2 θ3
0 0.963 0.963 0.955 0.942
0.2 0.853* 0.946 0.937 0.862*
0.4 0.565* 0.96 0.954 0.776*
0.6 0.39* 0.937 0.916* 0.739*
0.8 0.329* 0.908* 0.899* 0.739*
Table 6: Burden effect: coverage rates of percentile-t bootstrap confidence intervals for
the optimal policy parameter at sample size 200. λ is estimated online. Coverage rates
significantly lower than 0.95 are marked with asterisks (*).
τ
Bias MSE
θ0 θ1 θ2 θ3 θ0 θ1 θ2 θ3
0 0.006 0.010 0.017 −0.008 0.027 0.018 0.016 0.019
0.2 0.263 −0.048 −0.057 0.153 0.096 0.020 0.019 0.042
0.4 0.539 −0.018 0.012 0.224 0.318 0.018 0.016 0.069
0.6 0.678 0.088 0.120 0.261 0.487 0.026 0.030 0.087
0.8 0.735 0.129 0.143 0.261 0.568 0.035 0.037 0.087
Table 7: Burden effect: bias and MSE in estimating the optimal policy parameter at sample
size 500. Bias=E(θ̂t)− θ∗.
25
τ θ0 θ1 θ2 θ3
0 0.973 0.949 0.955 0.942
0.2 0.714* 0.95 0.962 0.788*
0.4 0.217* 0.951 0.961 0.635*
0.6 0.101* 0.886* 0.835* 0.545*
0.8 0.07* 0.806* 0.788* 0.546*
Table 8: Burden effect: coverage rates of percentile-t bootstrap confidence intervals for
the optimal policy parameter at sample size 200. λ is estimated online. Coverage rates
significantly lower than 0.95 are marked with asterisks (*).
Figure 1 and 2 assess the quality of the estimated optimal policies by comparing the
regularized average cost with the optimal regularized average cost. Figure 1 does the com-
parison at five levels of burden effect: τ = 0, 0.2, 0.4, 0.6, 0.8, at sample size 200. As the
burden effects level up, the overall long-run average cost goes up, which is simply an artifact
of the increasing main effect size of the disengagement level. Having a higher long-term
average cost, the estimated optimal policy by the contextual bandit algorithm is always in-
ferior then the optimal policy. The inferiority gap, as measure by the difference between the
median long-run average cost and the long-run average cost of the optimal policy increases
as τ increases. When sample size increases from 200 to 500, we observe less variation in
the long-run average cost of the estimated optimal policies. Nevertheless, the gap remains
stable. We also observe that the variance in the regularized average cost increases as the
burden effect level goes up.
26
0 0.2 0.4 0.6 0.8
Burden e,ect =
9.9
10
10.1
10.2
10.3
10.4
A
ve
ra
ge
 c
os
t
Figure 1: Burden effect: box plots of
regularized average cost at different levels
of the burden effect at sample size 200.
0 0.2 0.4 0.6 0.8
Burden e,ect =
9.9
10
10.1
10.2
10.3
10.4
A
ve
ra
ge
 c
os
t
Figure 2: Burden effect: box plots of regu-
larized average cost at different levels of the
burden effect at sample size 500.
Because our theoretical results assume i.i.d. contexts, we have no proof that the optimal
policy estimated by the bandit actor-critic algorithm will converge to the optimal policy.
Nevertheless, we observe convergence in the estimated policy as sample size T grows. We
conjecture that, when actions affect contexts distributions, the bandit algorithm converges
to the policy πθ∗∗ that satisfies the following equilibrium equation:
θ∗∗ = argmin
θ
∑
s
dθ∗∗(s)
∑
a
πθ(a|s)E(C|A = a, S = s)− λ∗∗θTEθ∗∗ [g(S)g(S)T ]θ (14)
where λ∗∗ is the smallest λ such that θ∗∗
∑
s
dθ∗∗(s)g(s)
Tg(s)θ∗∗ ≤ (log( p0
1− p0
))2α (15)
When actions do not influence contexts distributions, the equilibrium equation is the same
system of equations satisfied by the optimal policy. When previous actions have an impact
on context distribution at later decision points, the stationary distribution of context is a
function of policy. We call solution to equation 15, the myopic equilibrium policy. The myopic
equilibrium policy minimizes the regularized average cost under the stationary distribution
generated by itself. Such policy achieves an “equilibrium state” and there is no reason
for the actor-critic to change the current policy if a myopic equilibrium has been reached.
27
The conjecture is supported by our numerical results. Since myopic equilibrium policy
only depends on the context dynamics and the treatment effect E(C|A = 1, S = s) −
E(C|A = 0, S = s), it remains the same at different levels of the burden effect. The myopic
equilibrium policy is θ∗∗ = [0.392, 0.372, 0.371, 0]. The bias and MSE in estimating the
myopic equilibrium policy for τ = 0.4 is shown in table 9. The bias and MSE at other
levels of the burden effect are the same. These results support with our conjecture that
the estimated optimal policy by the bandit algorithm converges to the myopic equilibrium
policy.
Sample size (T)
Bias MSE
θ0 θ1 θ2 θ3 θ0 θ1 θ2 θ3
200 −0.078 −0.081 −0.075 0.004 0.063 0.042 0.041 0.036
500 −0.045 −0.035 −0.028 −0.007 0.029 0.019 0.017 0.019
Table 9: Burden effect: bias and MSE in estimating the myopic equilibrium policy for
τ = 0.4. Bias=E(θ̂t)− θ∗∗.
5.4 Expected Cost is a Nonlinear function of the Cost Feature
In this section, we investigate the performance of the online actor critic algorithm when
the expected cost is a nonlinear function of the cost feature used in the critic step. In
such scenarios, the linear actor critic algorithm finds the “best” policy in two steps: first it
projects the true cost function into the linear space spanned by the cost feature, then it finds
the policy that minimizes the regularized cost function under the projection. In contrast,
the true optimal policy is the policy that minimizes the regularized cost function without
the projection. In this simulation, we are interested to see how the extra step of projection
affects the estimation and inference of the optimal policy parameter.
Recall that the cost feature is f(St, At) = [1, St,1, St,2, St,3, At, AtSt,1, AtSt,2, AtSt,3]. In
particular consider the case where the interaction term between At and St,1 is a linear
28
combination of a linear cost function and a nonlinear one:
Ct = (1− α)[10− .4St,1 − .4St,2 − At × (0.2 + 0.2St,1 + 0.2St,2) + 0.4St,3 + ξt,0]
+ α[10− .4S2t,1 − .4St,2 − At × (0.2 + 0.2S2t,1 + 0.2St,2) + 0.4St,3 + ξt,0]
= 10− .4[(1− α)St,1 + αSt,1]− .4St,2 − At × (0.2 + 0.2[(1− α)St,1 + αSt,1] + 0.2St,2) + 0.4St,3 + ξt,0
The tuning parameter α ∈ [0, 1] controls the amount of nonlinearity: when α = 0, the ex-
pected cost is the linear cost function used in the previous sections. Nonlinearity increasingly
dominates the interaction between St,1 and At as α increases. The online actor critic algo-
rithm, unaware of the possible nonlinearity in the cost function, uses the same cost feature
and the same policy feature as in the previous sections. Recall the policy is parameterized
as πθ(S, 1) =
eθ0+
∑3
i=1 θiSi
1+eθ0+
∑3
i=1
θiSi
. Table 23 in the supplementary material section F provides the
optimal θ values. Table 10 and Table 11 show the bias and MSE of the linear actor critic
algorithm at different levels of nonlinearity at sample size 200 and 500. The bias for es-
timating θ∗i , i = 1, 2, 3 remains stable whereas the MSE inflates as the α increases. Both
the bias and MSE for estimating θ∗0 increase as the cost function moves away from a linear
structure. Table 12 and table 13 show the coverage rates of the confidence interval for θ∗.
The confidence interval coverages for θ∗i , i = 0.1, 2 deteriorate as the level of nonlinearity
increases. However, the confidence level for θ∗3 = 0, the coefficient for St,3 which is not a
useful tailoring variable, remains decent as the level of nonlinearity increases.
α
Bias MSE
θ0 θ1 θ2 θ3 θ0 θ1 θ2 θ3
0 −0.100 −0.074 −0.103 −0.007 0.063 0.052 0.051 0.057
0.2 −0.137 −0.012 −0.109 −0.013 0.064 0.065 0.050 0.053
0.4 −0.179 0.012 −0.109 −0.014 0.076 0.099 0.048 0.048
0.6 −0.211 0.020 −0.099 −0.017 0.083 0.142 0.043 0.042
Table 10: Nonlinear Cost: bias and MSE in estimating the optimal policy parameter at
sample size 200. Bias=E(θ̂T )− θ∗.
29
α
Bias MSE
θ0 θ1 θ2 θ3 θ0 θ1 θ2 θ3
0 −0.038 −0.036 −0.045 −0.002 0.025 0.022 0.022 0.027
0.2 −0.080 0.030 −0.067 −0.008 0.026 0.032 0.023 0.023
0.4 −0.109 0.065 −0.069 −0.010 0.031 0.064 0.023 0.020
0.6 −0.136 0.058 −0.068 −0.009 0.038 0.112 0.021 0.018
Table 11: Nonlinear Cost: bias and MSE in estimating the optimal policy parameter at
sample size 500. Bias=E(θ̂T )− θ∗.
α θ0 θ1 θ2 θ3
0 0.944 0.947 0.954 0.939
0.2 0.926* 0.879* 0.942 0.935*
0.4 0.892* 0.738* 0.922* 0.942
0.6 0.835* 0.588* 0.914* 0.942
Table 12: Nonlinear Cost: coverage rates of percentile-t bootstrap confidence intervals for
the optimal policy parameter at sample size 200. λ is estimated online. Coverage rates
significantly lower than 0.95 are marked with asterisks (*).
α θ0 θ1 θ2 θ3
0 0.971 0.961 0.966 0.958
0.2 0.931* 0.875* 0.936 0.956
0.4 0.885* 0.655* 0.924* 0.958
0.6 0.837* 0.471* 0.915* 0.961
Table 13: Nonlinear Cost: coverage rates of percentile-t bootstrap confidence intervals for
the optimal policy parameter at sample size 500. λ is estimated online. Coverage rates
significantly lower than 0.95 are marked with asterisks (*).
30
6 Conclusion and Discussion
In this article, we present a general framework to define optimal policies for use in JITAIs that
encourages intervention variety. We also gave an online actor-critic algorithm to learn the
optimal policy. Although the theoretical properties of the algorithm assume i.i.d. contexts,
the numerical experiments show robustness of the algorithm to violations of this assumption.
In particular, experiments show that performance of the algorithm, in term of bias, MSE and
confidence interval coverage, is not affected by auto-correlation among contexts. Experiments
also demonstrate some robustness of the algorithm when distribution of the context depends
on previous actions. Furthermore, we conjecture that, when actions influence the distribution
of context at later decision points, the contextual bandit algorithm converges to the myopic
equilibrium policy. Our numerical experiments back up this conjecture. Theoretical proof
of the conjecture, however, is an open question and requires future work.
There are a few areas for which the actor-critic algorithm could be improved and ex-
tended. First, the linear expected reward assumption might be a bit strong in some sce-
narios, especially when a low dimension reward feature is used. When the assumption is
deemed untenable, more sophisticated components should be added to the reward (cost)
feature. To this end, both the actor-critic algorithm and the asymptotic theory should be
extended to encompass the scenario where the dimension of the reward (cost) feature grows
with the sample size. If one intends to use linear reward (cost) model with a fixed dimen-
sion of reward feature, we highly recommend frequent validation of the linear model using
model diagnostic tools. Linear regression diagnostic tools can be used as the first line of
defense. However, more sophisticated model checking methods for online learning need to
be developed to make sure the reward (cost) model is adequate. Second, there is room
for improvement in optimization in the actor step. Optimizing the estimated regularized
average reward function is in general a non-convex optimization problem and could be time-
consuming. In the proposed algorithm, optimization at decision point t + 1 does not use
the estimated policy parameters at previous decision points. In other words, the optimiza-
tion is not incremental and may waste computing resources when the sample size gets large.
Careful design of online optimization methods that leverages previous estimates will likely
31
significantly improve the computational efficiency of the actor-critic algorithm and help in
its practical adoption in mobile health applications. Third, the algorithm presented in this
article learns a user’s the optimal policy based solely on his/her history. However, in order to
speed up the learning it is attractive idea, especially in the beginning of the learning period,
to pool data across multiple users. Methods and theories for learning based on multiple users
need to be developed.
References
Agrawal, S. and N. Goyal (2013). Thompson sampling for contextual bandits with linear
payoffs. In ICML (3), pp. 127–135.
Bauer, S., J. de Niet, R. Timman, and H. Kordy (2010). Enhancement of care through
self-monitoring and tailored feedback via text messaging and their use in the treatment of
childhood overweight. Patient education and counseling 79 (3), 315–319.
Bertsekas, D. P. (1999). Nonlinear programming.
Bhatnagar, S. and K. Lakshmanan (2012). An online actor–critic algorithm with function
approximation for constrained markov decision processes. Journal of Optimization Theory
and Applications 153 (3), 688–708.
Billingsley, P. (1961). The lindeberg-levy theorem for martingales. Proceedings of the Amer-
ican Mathematical Society 12 (5), 788–792.
Borkar, V. S. (2005). An actor-critic algorithm for constrained markov decision processes.
Systems & control letters 54 (3), 207–213.
Campi, M. C. and S. Garatti (2011). A sampling-and-discarding approach to chance-
constrained optimization: feasibility and optimality. Journal of Optimization Theory and
Applications 148 (2), 257–280.
32
Chu, W., L. Li, L. Reyzin, and R. E. Schapire (2011). Contextual bandits with linear
payoff functions. In International Conference on Artificial Intelligence and Statistics, pp.
208–214.
Consolvo, S., D. W. McDonald, T. Toscos, M. Y. Chen, J. Froehlich, B. Harrison, P. Klasnja,
A. LaMarca, L. LeGrand, R. Libby, et al. (2008). Activity sensing in the wild: a field
trial of ubifit garden. In Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems, pp. 1797–1806. ACM.
Dempsey, W., P. Liao, P. Klasnja, I. Nahum-Shani, and S. A. Murphy (2015). Randomised
trials for the fitbit generation. Significance 12 (6), 20–23.
Epstein, L. H., K. A. Carr, M. D. Cavanaugh, R. A. Paluch, and M. E. Bouton (2011).
Long-term habituation to food in obese and nonobese women. The American journal of
clinical nutrition 94 (2), 371–376.
Epstein, L. H., J. L. Temple, J. N. Roemmich, and M. E. Bouton (2009). Habituation as a
determinant of human food intake. Psychological review 116 (2), 384.
Fiacco, A. V. and Y. Ishizuka (1990). Sensitivity and stability analysis for nonlinear pro-
gramming. Annals of Operations Research 27 (1), 215–235.
Gustafson, D. H., B. R. Shaw, A. Isham, T. Baker, M. G. Boyle, and M. Levy (2011).
Explicating an evidence-based, theoretically informed, mobile technology-based system
to improve outcomes for people in recovery for alcohol dependence. Substance use &
misuse 46 (1), 96–111.
Keener, R. (2010). Theoretical Statistics: Topics for a Core Course. Springer Texts in
Statistics. Springer New York.
King, A. C., E. B. Hekler, L. A. Grieco, S. J. Winter, J. L. Sheats, M. P. Buman, B. Banerjee,
T. N. Robinson, and J. Cirimele (2013). Harnessing different motivational frames via
mobile phones to promote daily physical activity and reduce sedentary behavior in aging
adults. PloS one 8 (4), e62613.
33
Klasnja, P., E. B. Hekler, S. Shiffman, A. Boruvka, D. Almirall, A. Tewari, and S. A.
Murphy (2015). Microrandomized trials: An experimental design for developing just-in-
time adaptive interventions. Health Psychology 34 (Suppl), 1220–1228.
Langford, J. and T. Zhang (2008). The epoch-greedy algorithm for multi-armed bandits with
side information. In Advances in neural information processing systems, pp. 817–824.
Li, L., W. Chu, J. Langford, and R. E. Schapire (2010). A contextual-bandit approach
to personalized news article recommendation. In Proceedings of the 19th international
conference on World wide web, pp. 661–670. ACM.
Nahum-Shani, I., S. N. Smith, A. Tewari, K. Witkiewitz, L. M. Collins, B. Spring, and
S. Murphy (2014). Just in time adaptive interventions (jitais): An organizing framework
for ongoing health behavior support. Methodology Center technical report (14-126).
Nemirovski, A. and A. Shapiro (2006). Convex approximations of chance constrained pro-
grams. SIAM Journal on Optimization 17 (4), 969–996.
Patrick, K., F. Raab, M. Adams, L. Dillon, M. Zabinski, C. Rock, W. Griswold, and G. Nor-
man (2009). A text message-based intervention for weight loss: randomized controlled
trial. Journal of medical Internet research 11 (1), e1.
Prékopa, A. (1995). Stochastic programming. Springer Science & Business Media.
Raynor, H. A. and L. H. Epstein (2001). Dietary variety, energy regulation, and obesity.
Psychological bulletin 127 (3), 325.
Riley, W. T., D. E. Rivera, A. A. Atienza, W. Nilsen, S. M. Allison, and R. Mermelstein
(2011). Health behavior models in the age of mobile interventions: are our theories up to
the task? Translational behavioral medicine 1 (1), 53–71.
Scott, C. K. and M. L. Dennis (2009). Results from two randomized clinical trials evaluating
the impact of quarterly recovery management checkups with adult chronic substance users.
Addiction 104 (6), 959–971.
34
Suffoletto, B., C. Callaway, J. Kristan, K. Kraemer, and D. B. Clark (2012). Text-message-
based drinking assessments and brief interventions for young adults discharged from the
emergency department. Alcoholism: Clinical and Experimental Research 36 (3), 552–560.
Tropp, J. A. (2012). User-friendly tail bounds for sums of random matrices. Foundations of
computational mathematics 12 (4), 389–434.
Van der Vaart, A. W. (2000). Asymptotic statistics, Volume 3. Cambridge university press.
Wilson, T. D., D. B. Centerbar, D. A. Kermer, and D. T. Gilbert (2005). The pleasures
of uncertainty: prolonging positive moods in ways people do not anticipate. Journal of
personality and social psychology 88 (1), 5.
Witkiewitz, K., S. A. Desai, S. Bowen, B. C. Leigh, M. Kirouac, and M. E. Larimer (2014).
Development and evaluation of a mobile intervention for heavy drinking and smoking
among college students. Psychology of Addictive Behaviors 28 (3), 639.
Woodroofe, M. (1979). A one-armed bandit problem with a concomitant variable. Journal
of the American Statistical Association 74 (368), 799–806.
35
Supplementary Material
A Proof of Lemma 1
Proof. Without the loss of generality, we assume that 0 < s1 < s2 < ... < sK . Otherwise, if
some si’s are negative, we can transform all the contexts to be positive by adding to si’s a
constant greater than min1≤i≤K si. Denote this constant by M and the corresponding policy
parameter by θ̃. There is a one-to-one correspondence between the two policy classes:
θ̃0 = θ0 −Mθ1
θ̃1 = θ1
Therefore if the lemma holds when all contexts are positive the same conclusion hold in the
general setting. We use p(θ) to denote the probability the probability of choosing action
A = 1 for policy πθ at the K different values of context:
(
eθ0+θ1s1
1 + eθ0+θ1s1
,
eθ0+θ1s2
1 + eθ0+θ1s2
, ...,
eθ0+θ1sK
1 + eθ0+θ1sK
)
Notice that each entry in p(θ) is number between 0 and 1 with equality if the policy is
deterministic at certain context. A key step towards proving deterministic optimal policy is
to show the following closed convex hull equivalency:
conv({p(θ) : θ ∈ R2}) = conv({(ν1, ..., νK), νi ∈ {0, 1}, ν1 ≤ ... ≤ νK or ν1 ≥ ... ≥ νK})
We examine the limiting points of p(θ) when θ0 and θ1 tends to infinity. We consider the
case where θ0 6= 0 and let θ1 = pθ0 where p is a fixed value. It holds that
eθ0+θ1s
1 + eθ0+θ1s
=
eθ0(1+ps)
1 + eθ0(1+ps)
→

0 : ifθ0 → −∞, p > −1/s
0 : ifθ0 →∞, p < −1/s
1 : ifθ0 → −∞, p < −1/s
1 : ifθ0 →∞, p > −1/s
It follows that when θ0 → −∞ and p scans through the K + 1 intervals on R: (−∞,−1/s1],
(−1/s1,−1/s2], . ... (−1/sK ,∞), p(θ) approaches the following K + 1 limiting points:
A1
(1, 1, ..., 1)
(0, 1, ..., 1)
...
(0, 0, ..., 1)
(0, 0, ..., 0)
when θ0 →∞ and p scans through the K + 1 intervals, p(θ) approaches the following K + 1
limiting points
(0, 0, ..., 0)
(1, 0, ..., 0)
...
(1, 1, ..., 0)
(1, 1, ..., 1)
There are in total 2K limiting points: {(ν1, ..., νK), νi ∈ {0, 1}, ν1 ≤ ... ≤ νK or ν1 ≥ ... ≥
νK}. Each limiting point is a K dimensional vector with 0-1 entries in an either increasing
or decreasing order. Now we show that any p(θ), θ ∈ R2 is a convex combination of the
limiting points. Let p(θ) = [p1(θ), p2(θ), ..., pK(θ)]. In fact,
• If θ1 = 0, p(θ) = (1− p1(θ))(0, 0, ..., 0) + p1(θ)(1, 1, ..., 1)
• If θ1 > 0, we have 0 < p1(θ) < p2(θ) < ... < pK(θ) < 1 and
p(θ) = p1(θ)(1, 1, ..., 1) + (p2(θ)− p1(θ))(0, 1, ..., 1) + ...
+(pK(θ)− pK−1(θ))(0, 0, ..., 1) + (1− pK(θ)) ∗ (0, 0, ..., 0)
• If θ1 < 0, we have 1 > p1(θ) > p2(θ) > ... > pK(θ) > 0 and
p(θ) = (1− p1(θ)) ∗ (0, 0, ..., 0) + (p1(θ)− p2(θ))(1, 0, ..., 0) + ...
+(pK(θ)− pK−1(θ))(1, 1, ..., 0) + pK(θ)(1, 1, ..., 1)
A2
Returning to optimizing the average reward, we denote αi = P (S = si)(E(R|S = si, A =
1)− E(R|S = si, A = 0)).
max
θ
V ∗(θ) = max
θ
K∑
i=1
αipi(θ) (16)
= max
(p1,...,pK)∈{p(θ):θ∈R2}
K∑
i=1
αipi (17)
= max
(p1,...,pK)∈conv({p(θ):θ∈R2})
K∑
i=1
αipi (18)
= max
(p1,...,pK)∈conv({(ν1,...,νK),νi∈{0,1},ν1≤...≤νK or ν1≥...≥νK})
K∑
i=1
αipi (19)
. Equation from (17) to (18) is followed by the fact that the objective function is linear (and
thus convex) in pi’s. Equivalency from (18) to (19) is a direct product of the closed convex
hull equivalency. Theories in linear programming theory suggests that one of the maximal
points is attained at the vertices of the convex hull of the feasible set. Therefore we have
proved that one of the policy that maximizes V ∗(θ) is deterministic.
B One-to-one Correspondence between Constrained and
Unconstrained Optimization
The constrained optimization finds the policy that maximizes the average reward subject to
the quadratic constraint, i.e.,
max
θ
V ∗(θ), s. t. θTE[g(S)Tg(S)]θ ≤ (log( p0
1− p0
))2α (20)
The unconstrained optimization finds the policy that maximizes the regularized average
reward:
θ∗ = argmax
θ
J∗λ(θ) (21)
A natural question to ask, when transforming the constrained optimization problem (20)
to an unconstrained one (21), is whether a Lagrangian multiplier exists for each level of
A3
stringency of the quadratic constraint. While the correspondence between the constrained
optimization and the unconstrained one may not seem so obvious due to the lack of convex-
ity in V ∗(θ), we established the following lemma 3 given assumption 7 and assumption 4.
Assumption 7 assumes the uniqueness of the global maximum for all positive λ.
Assumption 7. For every 0 < λ <∞, the global maximum of the regularized average reward
is a singleton.
J∗λ(θ) =
∑
s∈S
d(s)
∑
a∈A
E(R|S = s, A = a)πθ(s, a)− λθTE[g(S)Tg(S)]θ
Lemma 3. If the maximizer of the average reward function V ∗(θ) is deterministic, i.e.
P (πθ(A = 1|S) = 1) > 0 or P (πθ(A = 0|S) = 1) > 0, under assumption 7 and 4, for
every K = (log( p0
1−p0 ))
2α > 0 there exist a λ > 0 such that the solution of the constrained
optimization problem 20 is the solution of the unconstrained optimization problem 21.
Proof. Let θ∗λ be one of the global maxima of the Lagrangian function: θ
∗
λ = argmaxθ J
∗
λ(θ).
Let βλ = θ
∗T
λ E[g(S)Tg(S)]θ∗λ. By proposition 3.3.4 in Bertsekas (1999), θ∗λ is a global maxi-
mum of constrained problem:
max
θ
V ∗(θ)
s.t. θTE[g(S)Tg(S)]θ ≤ βλ
In addition, the stringency of the quadratic constraint increases monotonically with the value
of the Lagrangian coefficient λ. Let 0 < λ1 < λ2 and with some abuse of notation, let θ1 and
θ2 be (one of) the global maximals of Lagrangian function J
∗
λ1
(θ) and J∗λ2(θ). It follows that
− V ∗(θ2) + λ2θT2 E[g(S)Tg(S)]θ2
≤− V ∗(θ1) + λ2θT1 E[g(S)Tg(S)]θ1
=− V ∗(θ1) + λ1θT1 E[g(S)Tg(S)]θ1 + (λ2 − λ1)θT1 E[g(S)Tg(S)]θ1
≤− V ∗(θ2) + λ1θT2 E[g(S)Tg(S)]θ2 + (λ2 − λ1)θT1 E[g(S)Tg(S)]θ1
A4
It follows that
θT1 E[g(S)Tg(S)]θ1 ≥ θT2 E[g(S)Tg(S)]θ2
. As λ approaches 0, the maximal of the regularized average reward approaches the maximal
of the average reward function, for which E(θTg(S))2 → ∞. As λ increases towards ∞,
maximal of the regularized average reward approaches the random policy with θ = 0. It’s
only left to show that θ∗Tλ E[g(S)Tg(S)]θ∗λ is a continuous function of λ. Under assumption
7, we can verify that conditions in Theorem 2.2 in Fiacco and Ishizuka (1990) holds. This
theorem implies that the solution set of the unconstrained optimization 21 is continuous in
λ, sufficient to conclude the continuity of θ∗Tλ E[g(S)Tg(S)]θ∗λ.
C Proof of Asymptotic Theory of the Actor Critic Al-
gorithm
C.1 Proof of Lemma 2
Proof. This lemma is proved by comparing the regularized average reward function J∗λ(θ) at
θ∗ and at 0p. The optimal regularized average reward is:
J∗λ(θ
∗) =
∑
s∈S
d(s)
∑
a∈A
f(s, a)Tµ∗πθ∗(A = a|S = s)− λθ∗TE[g(S)Tg(S)]θ∗
≤
∑
s,a
d(s)
|f(s, a)|22 + |µ∗|22
2
πθ∗(A = a|S = s)− λθ∗TE[g(S)Tg(S)]θ∗
≤ 1− λθ∗TE[g(S)Tg(S)]θ∗
While on the other hand the regularized average reward for the random policy θ = 0p is
J∗λ(0p) =
∑
s,a
d(s)f(s, a)Tµ∗/2 ≥ 0
By the optimality of policy θ∗, 1 − λθTE[g(S)Tg(S)]θ ≥ 0, which leads to the necessary
condition for the optimal policy parameter:
A5
θ∗TE[g(S)Tg(S)]θ∗ ≤ 1
λ
(22)
According to assumption 4, the above inequality defines a bounded ellipsoid for θ∗, which
concludes the first part of the lemma. To prove the second part of this lemma, we notice
that the estimated reward function r̂t(s, a) is by definition bounded. By comparing Ĵt(θ, µ̂t)
at θ = θ̂t and θ = 0p we have
θ̂Tt [
1
t
t∑
τ=1
g(Sτ )g(Sτ )
T ]θ̂t ≤
2
λ
(23)
It remains to show that the smallest eigenvalue of 1
t
∑t
τ=1 g(Sτ )g(Sτ )
T is bounded away
from 0 with probability going to 1. Using the matrix Chernoff inequality, theorem 1 in
Tropp (2012), for any 0 < δ < 1,
P{λmin(
1
t
t∑
τ=1
g(Sτ )g(Sτ )
T ) ≤ (1− δ)λmin(Eg(S)g(S)T )} ≤ p[
e−δ
(1− δ)1−δ
]
tλmin(Eg(S)g(S)
T )
Q
(24)
where Q is the bound on the maximal eigenvalue of g(S)g(S)T and p is the length of g(S).
Taking δ = 0.5, the right-hand side of the Chernoff inequality goes to 0 as t goes to ∞.
Therefore with probability going to 1, inequality 23 defines a compact set on Rp. We have
proved the second part of the lemma.
C.2 Proof of the consistency of the critic
Proof. Based on the expression of µ̂t, its L2 distance from µ∗ is
|µ̂t − µ∗|2 = C(t)B(t)−1B(t)−1C(t) + op(1) (25)
=
C(t)
t
(
B(t)
t
)−1(
B(t)
t
)−1
C(t)
t
+ op(1) (26)
A6
where
C(t) =
t∑
τ=1
f(Sτ , Aτ )τ
B(t) = ζIk×k +
t∑
τ=1
f(Sτ , Aτ )f(Sτ , Aτ )
T
The two steps in proving |µ̂t − µ∗|22 → 0 in probability are
1. The matrix B(t)
t
has eigenvalue bounded away from 0 with probability going to 1, and
2. C(t)
t
converges to 0d in probability.
To prove the first step, we construct a matrix-valued martingale difference sequence. Define
K(θ) = Eθ[f(S,A)f(S,A)T ] =
∑
s d(s)
∑
a f(s, a)f(s, a)
Tπθ(A = a|S = s)
Xi = f(Si, Ai)f(Si, Ai)
T − E(f(Si, Ai)f(Si, Ai)T |Fi)
= f(Si, Ai)f(Si, Ai)
T −
∫
s
d(s)
∑
a
f(s, a)f(s, a)Tπθi−1(s, a)ds
= f(si, ai)f(si, ai)
T −K(θ̂i−1)
where the filtration Fi = σ{θ̂j, j ≤ i − 1} is the sigma algebra expand by the estimated
optimal policy before decision point i. By assumption 3, the sequence of random matrices
{Xi} are uniformly bounded. Applying the matrix Azuma inequality in Tropp (2012), it
follows that
λmax(
B(t)
t
−
∑t
i=1K(θ̂i−1)
t
)→ 0 in probability
λmin(
B(t)
t
−
∑t
i=1K(θ̂i−1)
t
)→ 0 in probability
Let the operators λmin and λmax represent the smallest and the largest eigenvalue of a matrix.
λmin(
B(t)
t
) = λmin(
B(t)
t
−
∑t
i=1K(θ̂i−1)
t
+
∑t
i=1K(θ̂i−1)
t
)
≥ λmin(
B(t)
t
−
∑t
i=1K(θ̂i−1)
t
) + λmin(
∑t
i=1K(θ̂i−1)
t
)
A7
By assumption 5, the second term λmin(
∑t
i=1K(θ̂i−1)
t
) is bounded with probability going to
1. Hence we have shown that the minimal eigenvalue of B(t)
t
is bounded with probability
going to 1. Using the same proving techniques we can show that the maximal eigenvalue of
(B(t)
t
)−1 is bounded with probability going to 1.
The second step in proving consistency of the critic is standard. Using the same filtration
Fi, we construct vector-valued martingale difference sequence Yi = f(Si, Ai)i. The sequence
has bounded variance under assumption 3. The in-probability convergence of C(t)
t
to 0 follows
immediately by applying the vector-valued Azuma inequality.
C.3 Proof of the consistency of the actor
First we prove the following lemma, which will be utilized in proving both the consistency
and the asymptotic normality of the actor.
Lemma 4. Define θ̃t to be the estimated optimal policy parameter by plugging in the possible
unbounded estimated reward function:
θ̃t = argmax
θ
1
t
t∑
τ=1
∑
a
f(Sτ , a)
T µ̂tπθ(Sτ , a)− λ θT
(
1
t
t∑
τ=1
g(Sτ )g(Sτ )
T
)
θ.
The probability that θ̃t 6= θ̂t goes to 0 as t→∞.
Proof. The proof starts by noticing that θ̂t and θ̃t are equal if |f(s, a)T µ̂t| are bounded by 2
for all combinations of (Sτ , a), 1 ≤ τ ≤ t, a ∈ A.
P (θ̃t 6= θ̂t) ≤ P (∃1 ≤ τ ≤ t, a ∈ A, s.t.|f(Sτ , a)T µ̂t| > 2)
≤ P (∃1 ≤ τ ≤ t, a ∈ A, s.t. |f(Sτ , a)T (µ̂t − µ∗)|+ |f(Sτ , a)Tµ∗| > 2)
≤ P (∃1 ≤ τ ≤ t, a ∈ A, s.t. |f(Sτ , a)T (µ̂t − µ∗)| > 1)
≤ P (∃1 ≤ τ ≤ t, a ∈ A, s.t. |f(Sτ , a)|2|µ̂t − µ∗|2 > 1)
≤ P (|µ̂t − µ∗|2 > 1)
→ 0
A8
where the third inequality is based on |f(Sτ , a)Tµ∗| ≤ 1 a.s. from assumption 3. The fourth
inequality is based on Holder’s inequality. In the end we use the consistency of µ̂t.
Now we can prove the consistency of the actor by proving the consistency of θ̃t.
Proof. Proof of the theorem consists of two steps. As the first step, we claim that if a sequence
µt converges to µ
∗, θ̃t = argmaxθ J(θ, µt) converges to θ
∗. By Lemma 9.1 in Keener (2010),
J(θ, µ) is an absolute continuous function. We proof the claim by contradiction. Suppose the
claim does not hold, i.e. there exist  such that ‖θ̃t−θ∗‖2 ≥  for all t by taking a subsequence
if necessary. The optimality of θ̃t implies that the inequality J(θ̃t, µt) ≥ J(θ∗, µt) holds for
all t. Since θ̃t is bounded, it converges to an accumulation point θ̃ by taking a subsequence if
necessary. Let t→∞ we have J(θ̃, µ∗) ≥ J(θ∗, µ∗). On the other hand ‖θ∗∗−θ∗‖2 ≥ , which
contradicts with assumption 7. As the second step, we prove that the following M-estimator
converges uniformly in a neighborhood of µ∗, namely
θµt = argmax
θ
1
t
t∑
τ=1
∑
a∈A
f(Sτ , a)
Tµπθ(Sτ , a)− θT [
1
t
t∑
τ=1
g(Sτ )g(Sτ )
T ]θ
→ θµ = argmax
θ
J(θ, µ)
in probability, and uniformly over all µ in a neighborhood of µ∗. Arguments in the proof
are parallel to those in Theorem 9.4 in Keener (2010). The key is to observe that the class
of random functions {j(θ, µ, s) =
∑
a∈A f(s, a)
Tµπθ(s, a) − θTg(s)g(s)T θ : θ ∈ Rp, |µ|2 ≤ 1}
are Glivenko-Cantelli. We have now proved the consistency of θ̃t and thus the consistency
of θ̂t.
C.4 Proof of the asymptotic normality of the critic
Proof. Based on the formula of µ̂t,
µ̂t − µ∗ = (ζId +
t∑
i=1
f(Si, Ai)f(Si, Ai)
T )−1(
t∑
i=1
f(Si, Ai)i − µ∗)
= (
ζId +
∑t
i=1 f(Si, Ai)f(Si, Ai)
T
t
)−1
√
t
∑t
i=1 f(Si, Ai)i
t
+ op(1)
A9
Based on the consistency of θt, we have that
ζId+
∑t
i=1 f(Si,Ai)f(Si,Ai)
T
t
converges in proba-
bility to Eθ∗(f(S,A)f(S,A)T . Now it is the key to analyze the asymptotic distribution
of the martingale difference sequence {f(Si, Ai)i}ti=1. With respect to filtration Ft,j =
σ({Si, Ai, i}ji=1). Define M∗ = [Eθ∗(f(S,A)f(S,A)T )]−1/2 and a martingale difference se-
quence {ξt,i = M
∗f(si,ai)i√
t
}ti=1 which is adapted to the filtration Ft,j and satisfies E(ξt,i|Ft,i−1) =
0, To apply vector Lindberg-Levy central limit theorem for martingale difference sequences
(Billingsley (1961)), we check the two conditions in this theorem:
1. The conditional variance assumption.
Vt =
t∑
i=1
E(ξ2t,i|Ft,i−1)
=
1
t
t∑
i=1
M∗Eθi−1(f(s, a)f(s, a)T )M∗
converges in probability to Idσ
2 by consistency of θt.
2. The Lindeberg condition. For any given δ > 0,
t∑
i=1
E(ξ2t,iI(‖ξt,i‖2 > δ)|Ft,i−1)
=
1
t
t∑
i=1
E(M∗f(Si, Ai)f(Si, Ai)T 2iM∗I(‖M∗f(Si, Ai)i‖1 >
√
tδ)|Ft,i−1)
≤1
t
t∑
i=1
E(M∗f(Si, Ai)f(Si, Ai)T 2iM∗I(‖M∗f(Si, Ai)‖22i >
√
tδ)|Ft,i−1)
By assumption 3, f(S,A) are bounded almost surely, therefore the above expression
goes to 0 as t→ 0.
The Lindberg-Levy martingale central limit theorem concludes that
t∑
i=1
ξt,i → N(0d, Idσ2) in distribution
Therefore
√
t(µ̂t − µ∗)→ N(0d, [Eθ∗(f(S,A)f(S,A)T )]−1σ2) (27)
A10
C.5 Proof of the asymptotic normality of the actor
Again, our strategy is to derive the asymptotic normality of θ̃t and then use the fact that θ̂t
must have the same asymptotic distribution.
Proof. We first prove that
Gtjθ(µ̂t, θ̂t, S)−Gtjθ(µ∗, θ∗, S) = op(1) (28)
, where Gt =
√
t(Pt−P ), the empirical process induced by the “marginal” stochastic process
{Si}ti=1 formed by the history of contexts. The “full” stochastic process involves the sequence
of triples {Si, Ai, i}ti=1, the complete history of contexts, actions and reward errors. We
consider the class of functions F = {jθ(µ, θ, s) : ‖θ − θ∗‖2 ≤ δ, ‖µ − µ∗‖2 ≤ δ}, where
jθ(µ, θ, s) is the partial derivative with respect to θ of function:
j(µ, θ, s) =
∑
a
f(s, a)Tµπθ(s, a)− λθTg(s)g(s)T θ
The boundedness assumption on reward feature, policy feature and reward ensures that the
parametrized class of functions jθ(µ, θ, s) is P-Donsker in a neighborhood of (µ
∗, θ∗). In other
words F is P-Donsker, where P is the distribution of the marginal stochastic process formed
by contexts. We complete the first part of the proof by modiftying Lemma 19.24 in Van der
Vaart (2000). It may seem that the dependence of µ̂t and θ̃t on the full stochastic process
could introduce complexity but a closer inspection shows that the proof goes through. The
random function jθ(µ̂t, θ̃t, S) belongs to the P-Donsker class defined above and satisfies that
∑
s
d(s)(jθ(µ̂t, θ̃t, s)− jθ(µ∗, θ∗, s))2 → 0
in probability. This is a result of the consistency of both µ̂t and θ̃t, as well as applying the con-
tinuous mapping theorem. By Theorem 18.10(v) in Van der Vaart (2000), (Gt, jθ(µ̂t, θ̃t, s))→
(Gp, jθ(µ∗, θ∗, s)) in distribution, where Gp is the P-Brownian bridge. The key here is that
Theorem 18.10 only relies on the convergence of two stochastic processes, regardlessly of
A11
whether the stochastic processes consist of i.i.d. observations and whether or not the two pro-
cesses are dependent. By Lemma 18.15 in Van der Vaart (2000), almost all sample paths of Gp
are continuous on F . Define a mapping h : l(F)∞×F → R by h(z, f) = z(f)−z(jθ(µ∗, θ∗, s)),
which is continuous at almost every point of (Gp, jθ(µ∗, θ∗, s)). By the continuous mapping
theorem, we have
Gt(jθ(µ̂t, θ̃t, s)− jθ(µ∗, θ∗, s)) = h(Gt, jθ(µ̂t, θ̃t, s))→ h(Gp, jθ(µ∗, θ∗, s)) = 0
in distribution and thus in probability, therefore (28) holds. The second part of the proof
begins by noticing that θ̃t satisfies the estimating equation Ptjθ(µ̂t, θ̃t, s) = 0, so we have
Gtjθ(µ̂t, θ̃t, s) =
√
t(Pjθ(µ
∗, θ∗, s)− Pjθ(µ̂t, θ̃t, s))
=
√
t(Jθ(µ
∗, θ∗)− Jθ(µ̂t, θ̃t))
=
√
tJ∗θθ(θ
∗ − θ̃t) +
√
tJ∗θµ(µ
∗ − µ̂t) +
√
top(‖θ̃t − θ∗‖) + op(1)
Together with (28) the above implies
√
t(θ∗ − θ̃t) = (J∗θθ)−1J∗θµ
√
t(µ̂t − µ∗) +
√
top(‖θ̃t − θ∗‖) + (J∗θθ)−1Gtjθ(µ∗, θ∗, s) + op(1)
= Op(1) +
√
top(‖θ̃t − θ∗‖)
where J∗θθ and J
∗
θµ are Jθθ and Jθµ evaluated at (θ
∗, µ∗). The
√
t consistency of θ̃t follows
through. Now (29) has become
√
t(θ∗ − θ̃t) = (J∗θθ)−1J∗θµ
√
t(µ̂t − µ∗) + (J∗θθ)−1Gtjθ(µ∗, θ∗, S) + op(1) (29)
Since both the two non-vanishing terms on the righthand side are asymptotically normal
with zero mean,
√
t(θ∗ − θ̃t) is asymptotically normal. The only task left is to derive the
asymptotic variance. Plugging in the formula for µ̂t, we have
√
t(θ∗ − θ̃t) = (J∗θθ)−1
∑t
i=1 J
∗
θµB
∗f(Si, Ai)i + jθ(µ
∗, θ∗, Si)
t
+ op(1)
= (J∗θθ)
−1
t∑
i=1
ζt,i + op(1)
A12
where B∗ = (M∗)2 = [Eθ∗f(S,A)f(S,A)T ]−1. {ζi =
J∗θµB
∗f(Si,Ai)i+jθ(µ
∗,θ∗,Si)
t
}ti=1 is a martin-
gale difference sequence with asymptotic variance
t∑
i=1
E(ζ2t,i|Ft,i)
=
1
t
t∑
i=1
E(2i g∗θµB∗f(Si, Ai)f(Si, Ai)TB∗g∗µθ
+ jθ(µ
∗, θ∗, Si)jθ(µ
∗, θ∗, Si)
T − 2JθµB∗f(Si, Ai)jθ(µ∗, θ∗, Si)T i|Ft,i)
=
1
t
t∑
i=1
σ2J∗θµB
∗Eθi−1(f(S,A)f(S,A)T )B∗J∗µθ +
∑
s
d(s)jθ(µ
∗, θ∗, s)jθ(µ
∗, θ∗, s)T
which converges in probability to V ∗ = σ2J∗θµB
∗J∗µθ+
∑
s d(s)jθ(µ
∗, θ∗, s)jθ(µ
∗, θ∗, s)T . There-
fore the asymptotic variance of
√
t(θ∗ − θ̃t) is (J∗θθ)−1V ∗(J∗θθ)−1.
D Small Sample Variance estimation and Bootstrap
Confidence intervals
In this section, we discuss issues, challenges and solutions in creating confidence intervals
for the optimal policy parameter θ∗ when the sample size, the total number of decision
points, is small. We use a simple example to illustrate that the traditional plug-in variance
estimator is plagued with underestimation issue, the direct consequence of which is the
deflated confidence levels of the Wald-type confidence intervals for θ∗. We propose to use
bootstrap confidence intervals when the sample size is finite. We use simulation to evaluate
the bootstrap confidence intervals.
D.1 Plug-in Variance Estimation and Wald Confidence intervals
One of the most straightforward ways to estimate the asymptotic variance of θt is through
the plug-in variance estimation, the formulae of which is provided in Theorem 2. Once an
A13
estimated variance V̂i is obtained for
√
t(θ̂i− θ∗i ), a (1− 2α)% Wald type confidence interval
for θ∗i has the form: [θ̂i − zα V̂i√t , θ̂i + zα
V̂i√
t
]. Here θi is the i-th component in θ and zα is the
upper 100α percentile of a standard normal distribution. The plug-in variance estimator and
the associated Wald confidence intervals work well in many statistics problems. We shall
see that, however, the plug-in variance estimator of the estimated optimal policy parameters
suffers from underestimation issue in small to moderate sample sizes. In particular this
estimator is very sensitive to the plugged-in value of the estimated reward parameter and
policy parameter: a small deviation from the true parameters can result in an inflated
or deflated variance estimation. Deflated variance estimation produces anti-conservative
confidence intervals, a grossly undesirable property for confidence intervals. The following
simple example illustrates the problem.
Example 1. The context is binary with probability distribution P(S = 1) = P(S = −1) =
0.5. The reward is generated according to the following linear model: given context S ∈
{−1, 1} and action A ∈ {0, 1},
R = µ∗0 + µ
∗
1S + µ
∗
2A+ µ
∗
3SA+ 
where  follows a normal distribution with mean zero and standard deviation 9. The true
reward parameter is µ∗ = [1, 1, 1, 1]. Both µ∗ and the standard deviation of  are chosen to
approximate the realistic signal noise ratio in mobile health applications. We consider the
policy class πθ(A = 1|S = s) = e
θ0+θ1s
1+eθ0+θ1s
.
The differences between the plug-in estimated variance and its population counterpart are
that (1) the former uses the empirical distribution of context to replace the unknown popu-
lation distribution and (2) the unknown reward parameter and optimal policy parameter are
replaced by their estimates. We emphasize that it is the second difference that leads to the
underestimated variance in small sample size. To see this, we ignore the difference between
the empirical distribution and the population distribution of contexts, which is very small for
sample size T ≥ 50 under a Bernoulli context distribution with equal probability. Now the
plug-in variance estimator is a function of the estimated reward parameter µ̂t and the esti-
mated policy parameter θ̂t. Notice that θ̂t = [θ̂t,0, θ̂t,1] is a function of µ̂t = [µ̂t,0, µ̂t,1, µ̂t,2, µ̂t,3]
A14
and the empirical distribution of context. If we replace the empirical distribution in calcu-
lating θ̂t by its population counterpart, θ̂t is simply a function of µ̂t. In the rest part of the
example, we drop the subscript t in the estimated reward parameter and denote the estimate
of µ2 and µ3 by µ̂2 and µ̂3, respectively. Likewise, θ̂t,i is replaced by θ̂i for i = 0, 1.
Figure 3 is the surface plot showing how the plug-in variance estimation changes as func-
tion of the estimated reward parameter. The surface plot of the plug-in variance estima-
tion has a mountain-like pattern with two ridges along the two diagonals µ̂2 + µ̂3 = 0 and
µ̂2 − µ̂3 = 0. The height of the ridge increases as both µ̂2 and µ̂3 approaches the origin.
The peak of mountain is at the origin where µ̂2 = µ̂3 = 0. The true reward parameter
(µ∗2, µ
∗
3) = (1, 1) is close to the origin and lies right on the one of the ridges. There are four
“valleys” where the combinations of µ̂2 and µ̂3 gives a small plug-in variance.
Figure 3: Plug in variance estimation as a function of
µ̂2 and µ̂3, x axis represents µ̂t,2, y axis represents µ̂t,3 and z axis represents the plug-in
asymptotic variance of θ̂0 with λ = 0.1
Due to large areas of valley the plug-in variance estimation is biased down, a direct
consequence of which is the anti-conservatism of the Wald confidence intervals. We perform
a simulation study using the toy generative model described above. The simulation consists
of 1000 repetitions of running the online actor critic algorithm and recording the end-of-
A15
study statistics, including the plugin variance estimate, the Wald confidence intervals and the
theoretical Wald confidence intervals based on the true asymptotic variance. The first two
columns in table 14 show the bias of plug-in variance at different sample sizes. At all three
different sample sizes, the plug-in variance estimator underestimates the true asymptotic
variance, which is 293.03 for both policy parameters. Column 3 and column 4 show the
coverage rate of the Wald-type confidence interval (CI) using the plug-in estimated variance.
It is not surprising that the confidence intervals suffer from severe anti-conservatism, a
consequence of the heavily biased variance estimation. Column 5 and 6 show the coverage
rate of the Wald-type confidence interval based on the true asymptotic variance. Comparing
the coverage rates, it is clear that the anti-conservatism is due to the underestimated variance.
A16
sa
m
p
le
si
ze
b
ia
s
in
va
ri
an
ce
es
ti
m
at
io
n
co
ve
ra
ge
of
W
al
d
C
I
(%
)
co
ve
ra
ge
of
th
eo
re
ti
ca
l
W
al
d
C
I
(%
)
θ 0
θ 1
θ 0
θ 1
θ 0
θ 1
10
0
-1
81
.5
6
-1
81
.5
6
75
.5
74
.9
10
0.
0
10
0.
0
25
0
-1
31
.7
1
-1
31
.7
1
77
.9
77
.3
98
.5
98
.1
50
0
-1
08
.6
4
-1
08
.6
4
78
.8
79
.2
98
.9
98
.7
T
ab
le
14
:
U
n
d
er
es
ti
m
at
io
n
of
th
e
p
lu
g-
in
va
ri
an
ce
es
ti
m
at
or
an
d
th
e
W
al
d
co
n
fi
d
en
ce
in
te
rv
al
s.
T
h
eo
re
ti
ca
l
W
al
d
C
I
is
cr
ea
te
d
b
as
ed
on
th
e
tr
u
e
as
y
m
p
to
ti
c
va
ri
an
ce
.
A17
To detail how the confidence interval coverage is connected with the estimated reward
parameter (µ̂2, µ̂3), figure 4 and figure 5 present two scatter plots of µ̂2, µ̂3 for the 1000
simulated datasets at sample size 100 and 500. Different colors are used to mark the datasets
where the confidence intervals of both θ0 and θ1 cover the true parameter (blue), only one
of them cover the truth (green), neither of them covers the truth (fading yellow). The true
parameter are marked with a red asterisk. Indeed the yellow points and green points are in
the “valleys”. Some of the blue points are away from truth, but nevertheless they remain on
the ridge, which produces a high variance estimate. Comparing the two scatter plots, as the
sample size increases, the estimated reward parameter is less spread out. Nevertheless there
are still significantly many pair of µ̂2, µ̂3 that fall in the “valleys”, leading to a underestimated
variance and anti-conservative confidence intervals.
A18
7̂
2
-8
-6
-4
-2
0
2
4
6
8
7̂3
-8-6-4-2
02468
F
ig
u
re
4:
W
al
d
co
n
fi
d
en
ce
in
te
rv
al
co
ve
ra
ge
fo
r
10
00
si
m
u
-
la
te
d
d
at
as
et
s
as
a
fu
n
ct
io
n
of
µ̂
3
an
d
µ̂
2
at
sa
m
p
le
si
ze
10
0.
7̂
2
-8
-6
-4
-2
0
2
4
6
8
7̂3
-8-6-4-2
02468
F
ig
u
re
5:
W
al
d
co
n
fi
d
en
ce
in
te
rv
al
co
ve
ra
ge
in
10
00
si
m
u
la
te
d
d
at
as
et
s
as
a
fu
n
ct
io
n
of
µ̂
3
an
d
µ̂
2
at
sa
m
p
le
si
ze
50
0.
A19
Figure 6 shows the histogram for the normalized distance
ˆ√
T (θi−θ∗i )
V̂i
for i = 0, 1 where
T = 100. This is the distance between the estimated and the true optimal policy parameter
normalized by the estimated asymptotic variance. For the Wald confidence intervals to have
descent coverage, histogram of the normalized distances need to approximate a standard nor-
mal distribution. However, as figure 6 suggests, the histograms have heavier tails compared
to a standard normal due to the underestimated variance. The figure also suggests that the
percentile-t bootstrap confidence intervals can be a good remedy.
standardized distance for 3̂0
-10 -5 0 5 10
de
ns
ity
0
0.1
0.2
0.3
0.4
0.5
0.6
standardized distance for 3̂1
-10 -5 0 5 10
de
ns
ity
0
0.1
0.2
0.3
0.4
0.5
0.6
Figure 6: Histograms of the normalized distance
ˆ√
T (θi−θ∗i )
V̂i
for i = 0, 1 at sample size 100
A20
E Burden Effect: Actor Critic Algorithm Uses λ∗
ν λ∗ θ∗0 θ
∗
1 θ
∗
2 θ
∗
3
0.0 0.06 0.3410 0.3269 0.3264 0
0.2 0.05 0.0844 0.3844 0.4 -0.1609
0.4 0.06 -0.1922 0.3547 0.3312 -0.2313
0.6 0.08 -0.3312 0.2488 0.2234 -0.2687
0.8 0.1 -0.3883 0.2078 0.2 -0.2687
Table 15: Burden effect: the optimal policy and the oracle lambda.
τ θ∗0 θ
∗
1 θ
∗
2 θ
∗
3
0 −0.027 352 −0.035 565 −0.030 344 0.003 449
0.2 0.229 47 −0.092 877 −0.104 06 0.164 21
0.4 0.505 86 −0.063 199 −0.035 223 0.234 73
0.6 0.645 07 0.042 695 0.072 542 0.271 98
0.8 0.702 29 0.083 867 0.096 08 0.2718
Table 16: Burden effect: bias in estimating the optimal policy parameter at sample size 200.
The algorithm uses λ∗ instead of learning λ online. Bias=E(θ̂t)− θ∗.
A21
τ θ∗0 θ
∗
1 θ
∗
2 θ
∗
3
0 0.057 811 0.037 16 0.036 343 0.035 898
0.2 0.109 61 0.044 463 0.046 192 0.062 836
0.4 0.312 95 0.039 819 0.036 65 0.090 984
0.6 0.473 09 0.037 714 0.040 625 0.109 84
0.8 0.550 24 0.042 799 0.044 54 0.1097
Table 17: Burden effect: MSE in estimating the optimal policy parameter at sample size
200. The algorithm uses λ∗ instead of learning λ online.
ν θ0 θ1 θ2 θ3
0 0.963 0.963 0.955 0.942
0.2 0.853* 0.946 0.937 0.862*
0.4 0.565* 0.96 0.954 0.776*
0.6 0.39* 0.937 0.916* 0.739*
0.8 0.329* 0.908* 0.899* 0.739*
Table 18: Burden effect: coverage rates of percentile-t bootstrap confidence intervals for the
optimal policy parameter at sample size 200. The algorithm uses λ∗ instead of learning λ
online. Coverage rates significantly lower than 0.95 are marked with asterisks (*).
A22
τ θ∗0 θ
∗
1 θ
∗
2 θ
∗
3
0.0 −0.017 692 −0.013 808 −0.006 068 −0.008 696
0.2 0.288 29 −0.031 35 −0.039 795 0.148 92
0.4 0.515 53 −0.041 593 −0.010 872 0.222 59
0.6 0.591 24 0.005 305 0.037 288 0.261 54
0.8 0.606 07 0.006 205 0.020 356 0.262 63
Table 19: Burden effect: bias in estimating the optimal policy parameter at sample size 500.
The algorithm uses λ∗ instead of learning λ online. Bias=E(θ̂t)− θ∗.
τ θ∗0 θ
∗
1 θ
∗
2 θ
∗
3
0.0 0.029 022 0.016 576 0.015 445 0.016 196
0.2 0.120 73 0.022 334 0.021 348 0.042 485
0.4 0.294 46 0.018 117 0.015 525 0.065 667
0.6 0.366 97 0.011 343 0.011 526 0.078 681
0.8 0.378 72 0.008 136 0.007 766 0.076 209
Table 20: Burden effect: MSE in estimating the optimal policy parameter at sample size
500. The algorithm uses λ∗ instead of learning λ online.
A23
τ θ0 θ1 θ2 θ3
0.0 0.944 0.950 0.952 0.933*
0.2 0.689* 0.943 0.959 0.815*
0.4 0.159* 0.944 0.954 0.6*
0.6 0.006* 0.941 0.928* 0.295*
0.8 0* 0.94 0.944 0.144*
Table 21: Burden effect: coverage rates of percentile-t bootstrap confidence intervals for the
optimal policy parameter at sample size 500. The algorithm uses λ∗ instead of learning λ
online. Coverage rates significantly lower than 0.95 are marked with asterisks (*).
τ θ∗0 θ
∗
1 θ
∗
2 θ
∗
3
0 0.392 0.3723 0.3713 −0.0006
0.2 0.3921 0.3722 0.3713 −0.0006
0.4 0.392 0.3723 0.3713 −0.0006
0.6 0.392 0.3723 0.3713 −0.0006
0.8 0.392 0.3723 0.3713 −0.0006
Table 22: Burden effect: the myopic equilibrium policy.
F Nonlinear Reward: The Optimal Policy
α θ∗0 θ
∗
1 θ
∗
2 θ
∗
3
0 0.418 035 0.395 067 0.397 071 −0.001 615
0.2 0.496 240 0.296 973 0.385 421 0.000 480
0.4 0.564 503 0.202 857 0.365 239 0.001 684
0.6 0.811 000 0.542 000 0.888 000 0.925 000
Table 23: Nonlinear reward: the optimal policy.
A24

