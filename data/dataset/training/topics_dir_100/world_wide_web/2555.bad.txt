 
 
 
 
Administration of Web Versus Paper Surveys:   
Mode Effects and Response Rates 
 
by  
C. Michele Matz  
   
   
   
   
A Master's paper submitted to the faculty  
of the School of Information and Library Science  
of the University of North Carolina at Chapel Hill  
in partial fulfillment of the requirements  
for the degree of Master of Science in  
Library Science.  
  
  
Chapel Hill, North Carolina 
November, 1999 
 
 
 
Approved by: 
 
_________________________________________ 
Advisor 
 
 
 
C. Michele Matz.  Administration of Web Versus Paper Surveys:  Mode Effects and 
Response Rates.  A Master's paper for the M.S. in L.S. degree.  November, 1999. 86 
pages.  Advisor: Barbara M. Wildemuth.  
 
 
A survey of academic reference librarians in North Carolina provided data for an 
examination of differences in survey administration on paper and the World Wide Web.  
Research via the Internet is becoming more attractive for many researchers, but the 
effects of this medium upon research outcomes has been little explored.  This study 
examined in particular sampling and mode effects, and response rates of Web surveys.  
The study found no sampling bias or mode effects in tests of the respondents’ 
demographics and the content of responses.  Response rates to Web surveys are not as 
high as traditional survey methods, and while responses are gathered more quickly, the 
paper instrument was not far behind.  Email notices were more efficient for promoting 
the Web survey than paper notices.  Traditional postal surveys still hold some advantages 
over Web surveys.  Researchers must weigh the advantages in cost and speed to justify 
use of such instruments. 
 
Headings: 
Surveys—methodology 
Surveys—evaluation 
Surveys—Internet 
College and university libraries 
Surveys—reference services 
 Library schools - Theses - University of North Carolina at Chapel Hill 
 
 
 
Table of Contents 
 
 
I.  Introduction.................................................................................................................4 
 
II.  Literature Review ......................................................................................................5 
Sampling Effects .......................................................................................................6 
Mode Effects .............................................................................................................7 
Advantages of Electronic Surveys ........................................................................... 11 
Publicizing Electronic Surveys ................................................................................ 15 
Summary of Research Questions ............................................................................. 16 
 
III.  Methodology .......................................................................................................... 17 
Population.............................................................................................................. 18 
The Survey............................................................................................................. 19 
Procedures for Distributing the Surveys ................................................................. 20 
 
IV.  Results ................................................................................................................... 22 
Hypotheses 1.......................................................................................................... 23 
Hypotheses 2 and 3 ................................................................................................ 23 
Hypothesis 4 .......................................................................................................... 25 
Hypothesis 5 .......................................................................................................... 25 
Hypothesis 6 .......................................................................................................... 27 
 
V.  Discussion ............................................................................................................... 28 
Technical Difficulties .............................................................................................. 30 
 
VI.  Conclusion ............................................................................................................. 31 
 
Bibliography ................................................................................................................. 33 
 
Appendix A – Paper Survey ........................................................................................A-1 
Appendix B – Web Survey .......................................................................................... B-1 
Appendix C – Cover Letters ........................................................................................ C-1 
Appendix D – Selected Statistical Test Results ............................................................D-1 
Tests for Hypothesis 1 .....................................................................................D-1 
Tests for Hypotheses 2 and 3...........................................................................D-5 
Tests for Hypothesis 5 ...................................................................................D-13 
Other Comparisons ........................................................................................D-14 
Appendix E – Survey Content Summary ..................................................................... E-1 
 
 
 
I.  Introduction to the problem 
Social science researchers have noted that the medium in which a researcher 
gathers data may affect the data gathered (Babbie, 1998).  This effect may skew what 
audience is reached, the kinds of information gathered or whether the audience self-
selects in a particular way.  A number of studies have examined the benefits and 
liabilities of various data collection methods including personal interviews, telephone 
interviews, mail surveys, and electronic mail surveys; however, few studies have 
examined surveys administered on the World Wide Web.  While a growing number of 
surveys are being posted on Web sites and there is much speculation as to how this new 
medium will affect the results, little outcome data is available.   
Whereas surveys have been administered electronically—that is, on computers—
since the late 1960s, these were usually surveys that were given to traditional population 
samples.   Participants were invited to go to a particular place where they would sit at a 
dumb terminal and answer the survey questions as they were prompted by the screen.  A 
researcher might or might not be present to clarify questions.  Electronic mail offered a 
way to send the survey to the respondent to answer at his or her convenience.  A number 
of the earlier studies of email surveys were conducted within a particular organization:  a 
university or a company.  Very quickly, though, researchers began to see the potential of 
reaching a much broader audience via such avenues as listservs.  Here was a way to 
negate geographic boundaries and reach very large numbers of people.  Web surveys  
 
 
 
seem to offer many of the same benefits as email surveys, but with a much nicer graphic 
interface, and electronic forms (with form “objects”) that provide the means for a 
researcher to standardize the responses.  (For example, radio buttons will only permit one 
answer, while check boxes allow multiple responses.) 
Many studies have established the skew in demographics of Internet users (e.g., 
the GVU survey [GVU Center, 1998]).  The current study sought to examine directly this 
sampling effect.  Specifically, it controlled for sampling bias by surveying a population 
that, while Internet-savvy, is not circumscribed by participation in the Internet.  In 
addition, the responses received via a Web survey were compared with those received via 
a traditional paper survey.  Finally, two variations in promoting the Web survey were 
compared:  a letter mailed in the usual way and an email letter.   
Although Web surveys are only beginning to be examined in the literature, studies 
of electronic mail surveys have demonstrated some administrative benefits that should be 
equally applicable for Web surveys.   Email surveys have been shown to be particularly 
advantageous in terms of cost and speed.  Further questions for the study involved how 
quickly surveys were returned, and the comparative response rates elicited by the Web 
and paper instruments.   
 
II. Literature review  
Sample effects of Internet populations and mode effects of electronic surveys are 
major factors for researchers to consider when conducting a survey with an Internet 
population.  It is important to understand the implications of such effects in order to avoid 
their consequences, as this study attempts to do.  In particular, the sampling effects and 
mode effects that occurred with the use of electronic mail surveys will be discussed, as 
 
 
 
well as the ways in which such effects had an impact on study results.  In spite of the 
disadvantages associated with sampling and mode effects, there are distinct advantages to 
administering surveys electronically, both by email and over the Web.  These advantages 
will be described along with other characteristics of the two types of surveys.  
Implications for the current study will be considered. 
 
Sampling Effects 
A major factor affecting Web surveys is the overall population that uses the 
Internet has different characteristics than the general population.  A recent survey of 
Internet users found 67.5% of their respondents were men (Pitkow, 1996).  Other groups 
over-represented among Internet users are whites, the young, the rich, and the highly 
educated (GVU Center, 1998; Anderson and Gansneder, 1995).  If the sampling frame 
from which a researcher selects a study population is unrepresentative of the general 
population, that study will exhibit the skew of the sample.  Because Internet users do not 
constitute a representative sample of the population, researchers have been wary of the 
potential for a strong sampling bias (Shaw and Davis, 1996; Walsh, Kiesler, Sproull and 
Hesse, 1992).  This continues to be true, although some of these same studies 
demonstrate the speed with which Internet use is becoming mainstream (Pitkow, 1996; 
GVU Center, 1998). 
The skew in the demographics of the Internet population versus the general public 
clearly impacts what kind of research can be conducted via the Web, and the kinds of 
generalizations one can make from data collected in this manner.  And although more 
research is being conducted with groups that are clearly Internet-literate, further questions 
must be asked, such as whether members of such a group have equal access to the 
 
 
 
Internet and use it in comparable ways and at comparable levels.  When the answer to 
any of these questions is no, predicting how survey responses will consequently be 
distorted becomes very difficult.   
Mode Effects  
One of the earliest surveys of electronic research identified significant mode 
effects—differences in results caused by the medium in which the survey was 
administered.  Kiesler and Sproull (1986) queried students and faculty of a major 
academic institution who were known to have used email recently.  They found important 
differences between the responses to their email versus print surveys.   
Electronic respondents were more cooperative, returning a larger number of 
surveys in a shorter period of time than paper respondents.  Electronic respondents made 
fewer errors in responding to questions and refused to answer or skipped fewer questions 
than paper respondents.  And finally, the electronic responses were more "extreme," or 
further from a socially accepted norm.  They theorized that the lack of social context in 
the electronic medium, normally provided by such cues as a cover letter’s institutional 
letterhead, resulted in respondents feeling less inhibited  to respond freely. 
Kiesler and Sproull's results were further strengthened when they re-administered 
the survey four months after the initial instrument to volunteers from the original group 
of respondents.  They switched the medium in which the subjects received the survey to 
the one each group had not used in the first round.   Although the number of responses 
was smaller, the anticipated effects were the same.  They concluded that, although there 
was "considerable similarity of response between the paper and electronic survey", it was 
 
 
 
"not so much that the two may be considered interchangeable without further research." 
(411)   
Despite some disagreement in the literature, however, most other studies have not 
found significant mode effects in responses gathered electronically.  Erdman, Klein and 
Greist (1983) found little difference between computer-administered and paper survey 
reports of drug use/abuse.  Skinner and Allen (1983) found no significant difference 
between self-reported levels of alcohol, drug and tobacco use reported in face-to-face 
interviews versus those reported via computerized questionnaires.  And in direct conflict 
with Kiesler and Sproull's results, the computer responses in their study indicated slightly 
lower reported frequencies of alcohol and marijuana use.   
Helgeson and Ursic (1989) evaluated decision process equivalency of 
undergraduate business students via electronic and paper data collection in part by 
comparing how the substance of answers changed when the order of survey questions 
was changed.  They found no significant differences between the content of responses 
gathered electronically versus on paper; however, they found respondents' decision 
processes to be more stable in the electronic medium.  As the extremity of scale anchors 
changed, answers in the electronic medium remained more stable than those on paper.  
Booth-Kewley, Edwards and Rosenfeld (1992) surveyed male Navy recruits with 
Paulhus's Balanced Inventory of Desirable Responding (1984), varying the medium in 
which they responded (electronic and paper) and the level of anonymity of the 
respondents.  While they found a significant variance of response in relation to the level 
of anonymity, the effects of the survey medium were insignificant.   
 
 
 
Synodinos, Papacostas and Okimoto (1994) administered a survey to randomly- 
and self-selected airport users via computer terminal and to randomly-selected users via 
personal interview.  They found no significance differences in the responses between 
computer respondents and those personally interviewed, but, predictably, did find 
significant differences between self-selected and randomly-selected respondents.  In their 
survey about Usenet newsgroup users’ attitudes toward Internet commercialization, 
Mehta and Sivadas (1995) found no difference between their email and regular mail 
responses.   
Bachmann, Elfrink and Vazzana (1996) surveyed business school deans and 
division chairpersons and found no significant difference between responses to the email 
and regular mail instrument.  Bertot and McClure (1997) surveyed public libraries across 
the country via the Web about Internet use, and at the time of publication had found no 
response-rate bias on the basis of the population size of responding libraries’ legal service 
area or region (their study was ongoing).  They did receive a greater response from some 
geographic regions--34.8% from the Midwest and 28.2% from the West compared to 
19.6% from the Northeast and 17.4% from the South--but "it is unclear as to whether the 
percentages of electronic survey respondents by region and population of legal service 
area correspond to public library Internet connectivity in general by those strata." (174) 
A few studies did find differences between responses from electronic and paper 
instruments, but felt they were the result of other factors than the survey mode or that 
they were within acceptable limits.  Miller, Daly, Wood, Brooks and Roper (1996) found 
a difference in response content between their email and paper surveys of professional 
computer scientists, but they attributed it to the two versions of the survey reaching 
 
 
 
different audiences defined by job position and concluded that little self-selection bias 
(which can be interpreted as sampling frame bias) was evident in their study.  Morphew 
and Williams (1998) determined email surveys to have a "sizable risk of nonresponse 
bias due to low response rates" but that the risk "is on the order of that associated with 
postal surveys."  (p. 52)  
A few studies did note apparent mode effects but they may actually have been 
sampling frame effects, specifically differences in demographics and computer 
experience.  Shaw and Davis (1996) reported significant differences in responses between 
their electronic and paper groups in a survey of Modern Language Association members, 
but these corresponded to demographic differences between the respondents in the two 
groups, especially that electronic respondents were much more experienced with 
electronic technology.  These participants were more likely to have a computer at home, 
more likely to use email, and more likely to use online library catalogs.  Similarly, 
Anderson and Gansneder (1995) noted that comparisons of computer-monitored data 
between respondents and non-respondents indicated the former were more likely to use 
the computer and for longer periods of time.  Their survey collected mainly demographic 
and electronic experience data, so substantive differences could not be measured.  
Finally, Kaufman, Carlozzi, Boswell, Barnes, Wheeler-Scruggs and Levy (1997) found in 
their survey of gays, bisexuals and lesbians about therapist selection that respondents to 
their electronic survey were more open about their sexual orientation than respondents to 
the paper instrument.  The authors attributed the difference to the demographic 
differences between the two groups:  "The email sample was younger, more educated and 
had higher reported incomes than the traditional sample…." (295-6)  These demographic 
 
 
 
differences correspond to the general demographics of Internet users, although the paper 
and electronic groups in this study were both balanced between men and women.  In the 
other studies, the selection parameters for the samples were also questionable:  the 
assumptions made by the researchers may have involved faulty logic.  Shaw and Davis 
selected members of  a professional organization as being electronically savvy because 
the organization they supported chose to  support the development of an important 
electronic bibliographic database, Modern Languages Association International 
Bibliography.  Likewise, both Anderson and Gansneder (1995) and Kaufman et al. 
(1997) recruited participants for their electronic instruments from listservs, assuming a 
uniformity of sample that may not have actually existed.   
The population of interest to the current study was academic librarians in North 
Carolina and was not chosen from a group organized on the basis of Internet 
participation.  Thus, the first three hypotheses for this study were: 
v Hypothesis 1:  Respondents to the Web survey will not exhibit significantly 
different demographics from respondents to the paper survey. 
 
v Hypothesis 2:  Respondents to the Web survey will give responses that are not 
significantly different from responses to the paper survey. 
 
v Hypothesis 3:  Respondents will not provide significantly different 
answers to the Web survey whether they were notified of it by email or 
postal mail. 
 
Advantages of Electronic Surveys 
Electronic data gathering has significant advantages which are agreed upon in the 
literature.  (Most of the studies cited here used electronic mail to distribute their surveys.)  
First, studies using electronic surveys note the low cost of administering them (Roselle 
and Neufeld, 1998; Berge and Collins, 1996; Clayton, Applebee and Pascoe, 1996; Miller 
 
 
 
et al., 1996; Anderson and Gansneder, 1995; Kiesler and Sproull, 1986; Erdman et al, 
1983).  Few state their actual costs, but electronic surveys inevitably eliminate the need to 
copy surveys, as well as the cost of postage, usually the major expense in postal surveys. 
Second, response is very fast.  Several studies received the majority of their 
responses within one to two weeks of posting surveys (Roselle and Neufeld, 1998; 
Meehan and Burns, 1997; Berge and Collins,1996; Miller et al., 1996; Anderson and 
Gansneder, 1995; Mehta and Sivadas, 1995).  Berge and Collins (1996) received their 
first response within twenty minutes of releasing their survey.  Meehan and Burns (1997) 
received 39% of their responses within twenty-four hours.  Mehta and Sivadas (1995) 
received more than half of all their responses within two to three days.  Swoboda, 
Mühlberger, Weitkunat and Schneeweib (1997) received 90% of their 1,713 responses 
within 4 days.  Morphew and Williams (1998) argue that multiple follow-up mailings 
make electronic survey periods comparable to mail and telephone survey periods, but 
they seem to be the only objectors. 
Good response rates are less uniformly agreed upon as a benefit of electronic 
surveys.  Several email surveys have received response rates fully on par with traditional 
instruments.  Roselle and Neufeld (1998) studied the effectiveness of email followup 
messages for a traditional postal survey.  They received responses from 85.3% of the 
participants who received the email followup, compared to a 79.8% response rate from 
participants who received a postcard followup.  Their overall response rate was 83%.  
Anderson and Gansneder (1995) achieved a 68% response rate to their email survey, 
excluding from their calculation a number of people who, according to computer data, 
did not read their email during the survey period.  (Their response rate including those 
 
 
 
people was 58%.)  Walsh et al. (1992) received a 76% response rate to their email survey 
of 300 oceanographers.  In addition, they received responses from an additional 104 
individuals spontaneously asking to participate.  (The researchers analyzed this self-
selected group separately from their original stratified random sample.) 
Other studies note lower response to email surveys than paper instruments used 
for the same surveys, but only slightly lower, as demonstrated by Table 1.  
 
Table 1.  Comparison of Response Rates by Survey Medium 
 
Study 
Paper  
Response Rate 
(Percentage) 
Electronic 
Response Rate 
(Percentage) 
Bachmann et al. (1996) 66 53 
Shaw and Davis (1996) 41 37 
Kiesler and Sproull (1986) 75 67 
Sproull (1986) 87 73 
Miller et al. (1996) 30 *19 
*See paragraph below 
 
 
Only one survey reported an electronic response rate that was  small enough as to 
be almost unusable.  Miller et al. (1996) experienced significantly different response rates 
to their electronic versus their postal mail survey.  The postal mail survey returned a 30% 
response rate.  Although they could not conclusively state the size of the audience the 
electronic survey reached because they distributed the survey to a newsgroup, the authors 
based their estimated response rate on the average monthly postings per week of the 
newsgroup.  Even using this very rough estimate of the number of recipients of the 
survey, the response rate for the email survey is 19%--and it could possibly be even lower 
than that if the number of recipients was underestimated.   
Three other studies reported low response to their electronic surveys, but had not 
conducted more traditional surveys with which to compare them.  Meehan and Burns 
 
 
 
(1997) reported electronic returns of approximately 23.6% from a survey of secondary 
school teachers and administrators.  Smith (1997) reported a virtually unusable response 
rate to her electronic survey, but attributed this to technical difficulties respondents 
encountered with her instrument--some browser programs were unable to properly 
process respondents' completed surveys.  Swoboda et al. (1997) received a 20% response 
rate to their survey about problems facing the world (political, social, etc.).  It could be 
argued that in this case the low response is partially due to the target audience not being 
highly invested in the results of the questionnaire.  Their survey was sent to 200 
randomly selected newsgroups focused on a variety of subjects, so the individuals it 
reached were not as concerned about participating as if they had been, say, international 
affairs analysts or environmentalists.   
The audience targeted by the current study is impacted daily in their professional 
work by the program which is the subject of the survey.  Consequently, one would expect 
to have a high response rate regardless of the survey medium.  The overall advantages 
reported for e-mailed surveys should also be present for a Web survey, so the next two 
hypotheses of the study were:  
v Hypothesis 4: The response rate of a Web-based survey will be no different 
from that of a paper survey. 
 
v Hypothesis 5: At least 50 percent of the total number of responses to the Web 
survey will be returned in one week.  
 
 
 
Publicizing Electronic Surveys 
 
Researchers are often interested in special populations for their research, and seek 
efficient ways to contact large numbers of a particular group rapidly.  Listservs and 
electronic bulletin boards represent “large populations [which] are well-defined in terms 
of a particular phenomenon." (113, Miller et al., 1996)  As demonstrated in the previous 
section, studies are beginning to demonstrate the efficiency of electronic media for 
reaching particular audiences, especially ones that are geographically diverse.  The main 
difference for surveys posted on the Internet from those distributed by electronic mail is 
targeting. 
Web surveys must be publicized.  No audience will automatically see it without 
some promotional effort on the part of the researcher.  Listservs and links on organization 
Web pages can be effective ways to advertise a survey; however, both strategies share the 
sampling bias of the Internet as noted earlier.  In addition, they may be more or less 
effective depending on many factors such as user traffic, subject of the survey, and so 
forth.  And these methods are not at all precise in targeting.  A researcher cannot be 
completely sure of what population(s) they actually reach in these ways.  Finally, they 
also make it impossible to calculate precise response rates.  Membership figures for a 
listserv vary widely over any given period of time as individuals choose to withdraw 
from or join the list.  Web page traffic may fluctuate widely depending on how often an 
organization's members seek updates or information.  Also, an email sent to one listserv 
may be reposted to other groups.  In both cases, the researcher cannot know the overall 
number of individuals who saw the advertisement, but only the number of people who 
respond (Berge and Collins, 1996; Miller et al., 1996; Walsh et al., 1992).  
 
 
 
A way to circumvent this problem with en mass advertising is to use targeted 
email (Anderson and Gansneder, 1995; Shaw and Davis, 1995; Kiesler and Sproull, 
1986).  Using personal email addresses is usually as specific as postal mailing.  Many 
professional directories now include members' personal email addresses.  Researchers 
can randomly choose participants from the directory in the same way that they might 
select a sample of mailing addresses for a mail survey.  It is advisable to type each 
individual's address into a separate message to avoid compromising other participants' 
privacy, rather than send one message to all participants.  Functions such as “copy” and 
“paste” make this process fairly rapid.  Anderson and Gansneder (1995) also note that 
addressing emails individually personalizes the appeal for response. 
Apart from the time involved, one would expect this method of advertising to be 
highly efficient and have the added benefit of enabling the researcher to calculate an 
exact response rate.  Mailed notices are a more traditional means of providing 
preliminary notice of a survey, but respondents may be discouraged from responding by 
needing to take the extra step of going to the Web to answer the survey.  The final 
hypothesis was: 
v Hypothesis 6:  Of the two methods of publicizing a Web survey, postal 
mail and email, email is the more efficient one.  More responses will 
arrive more quickly from the group notified by email. 
 
 
Summary of Research Questions 
Due to the skew of user demographics, Internet populations are likely to provide a 
poor sample for research surveys.  However, the sole fact of administering a survey via 
the World Wide Web does not necessarily introduce such bias.  Electronic surveys, 
whether distributed by email or the Web, are distinctly faster and less costly than 
 
 
 
traditional postal surveys, but must be carefully publicized to produce response rates 
equivalent to more traditional methods.  These observations have resulted in the 
following hypotheses for the current study: 
v Hypothesis 1:  Respondents to the Web survey will not exhibit significantly 
different demographics from respondents to the paper survey. 
 
v Hypothesis 2:  Respondents to the Web survey will give responses that are not 
significantly different from responses to the paper survey. 
 
v Hypothesis 3:  Respondents will not provide significantly different 
answers to the Web survey whether they were notified of it by email or 
postal mail. 
 
v Hypothesis 4:  The response rate of a Web-based survey will be no different 
from that of a paper survey. 
 
v Hypothesis 5: At least 50 percent of the total number of responses to the 
Websed survey will be returned in one week.  
 
v Hypothesis 6:  Of the two methods of publicizing a Web survey, postal 
mail and email, email is the more efficient one.  More responses will 
arrive more quickly from the group notified by email. 
 
III.  Methodology 
 
 Academic librarians in North Carolina were surveyed about their attitudes toward 
NC LIVE, a state-wide digital library initiative.  After selecting a stratified random 
sample of academic reference librarians in North Carolina for this study, the sample was 
divided into three groups:  two to receive notice of the electronic survey, one to receive 
the paper survey.   A software program was chosen to process the Web survey responses 
and forward the results by email to the researcher.  Once the survey was administered, 
returned surveys were tracked for date of receipt.  Data entry was completed in SPSS 9.0.  
One-way analysis of variance and chi-square tests were utilized to analyze differences 
between survey groups.  Bonferroni post-hoc analyses were used to further analyze 
 
 
 
statistically significant ANOVAs.  Spearman correlations were used to test relationships 
among ordinal data, and Pearson correlations were used to test relationships among 
interval data.   
 
Population 
The population for this study was academic reference librarians in North Carolina.  
A stratified random sample of 400 academic librarians was drawn from a combination of 
sources.  The membership lists of the academic library sections of the North Carolina 
Library Association and the American Library Association provided 275 individuals for 
the survey, and another 125 were researched from institutions' Web page staff directories.  
The stratification method seems unlikely to have produced significant bias since email 
addresses were relatively easy to locate for all three strata in the sample.  Difficulty 
seemed to arise with specific institutions rather than any given classification of institution 
(e.g., community college versus university).  Representatives were included from across 
the state in all types of academic libraries, from large university libraries to community 
colleges and private colleges.  Large numbers of individuals from the larger staffs in 
university libraries were offset by the greater number of community and small colleges.1    
The total group of 400 was randomly divided into four groups:  one received the 
paper survey, one received the paper announcement of the Web survey and a third 
received the email announcement of the Web survey.  The fourth group was randomly 
divided into three further groups to provide substitutions for the first three groups.  Such 
substitutions occurred when an individual's title made it clear he or she was unlikely to 
work on the reference desk, or if their mailing address or email address was incomplete, 
                                               
1 The exact numbers of representatives from universities,  community colleges, etc., were not calculated 
 
 
 
erroneous or unavailable.  Overall, 37 substitutions occurred, 22 of them in the group 
receiving the paper survey and most often because of inappropriate position title.  
For the group receiving the email notice, it was necessary to research individuals' 
work email addresses because no listserv exclusively serves academic librarians in North 
Carolina, and the membership lists mentioned above included only regular postal mail 
addresses.  This research entailed approximately fifteen hours of work.  Actually 
emailing the survey notices required approximately one and a half hours, both for the first 
and second notices. 
 
The Survey 
This study sought to survey a population experienced with electronic resources,2 
by electronic and paper surveys, about their attitudes toward NC LIVE, a new program to 
provide North Carolina academic and public libraries with collective access to a wide 
variety of electronic databases.3  "NC LIVE is a statewide electronic library project of the 
libraries of North Carolina designed to strengthen the delivery of information statewide to 
enhance education, economic development, and the overall quality of life." (State of 
North Carolina, 1998)  This program provides access to over 3,500 general magazines, 
journals and newspapers via approximately forty licensed databases, including several 
full text vendors such as ProQuest and EBSCOhost.  The resources included cover a wide 
range of disciplines, from religion to politics, from psychology to recreation, and include 
general reference resources as well as subject-specific ones.   
                                                                                                                                            
because those affiliations were not always apparent from the associations' mailing lists. 
2 "Electronic resources" includes computers, electronic mail, electronic databases and other software. 
3 Summary results of this survey are presented in Appendix E, since they are not the primary focus of this 
study. 
 
 
 
The State Library, with its partners, introduced North Carolina Libraries for 
Virtual Education (NC LIVE) in the spring of 1998.  Partners include public libraries and 
community college, private college and university libraries around the state.  Although 
larger libraries already had access to some of the resources the partners decided to offer 
through the program, the cost benefit of consortially negotiated licenses made it attractive 
to join.  A structured introductory program provided optional training to librarians across 
the state before and during the NC LIVE premier, and continues to provide workshops as 
needed.  The paper survey is presented as Appendix A, and the printed version of the 
Web survey as Appendix B.  The cover letter for the paper survey as well as the paper 
and email announcements of the Web survey appear in Appendix C.   
 
Procedures for Distributing the Surveys 
The paper survey and mail notice of the Web survey were sent several days ahead 
of the email notice of the Web survey in an effort to ensure that all instruments arrived at 
approximately the same time.  Recipients of the paper survey were invited to complete 
the questionnaire and return it within one month, and received a follow up notice two 
weeks after the original mailing.  Recipients of the paper and email notices of the Web 
survey were invited to complete the questionnaire within two weeks, and received a 
follow up notice after one week. 
The Web survey was as nearly a duplicate of the paper instrument as possible.  It 
was created using a combination of FrontPage 98, an HTML editor program, and direct 
HTML programming.  The form for the Web survey was created with Gform, a program 
which relays a respondent’s answers to the server on which the survey is mounted.  The 
server, in turn, encodes the information as an email message to the address specified by 
 
 
 
the researcher, including no information about the respondent.  This ensured responses 
would be, not just confidential, but anonymous.  Browsers do collect information about 
users as they respond to Web surveys, including their IP address and host domain (the 
specific address of the computer they use and the general address of the host, such as 
“.unc.edu”).  It would be possible to collect this information and identify respondents’ 
institutions if they use their work computer to respond, but it would be nearly impossible 
to discover the individual user (Dixon, 1999).  Anonymity is complete, although this does 
present problems for any subsequent follow up.   (If participants had been invited to 
include their email address voluntarily along with their responses, follow up would have 
been possible.)  
Gform assisted in differentiation and coding, as well as anonymity.  The two Web 
survey groups  (paper notice and email notice) were directed to two separate but identical 
Web pages.  The program enabled the researcher to insert identifying subject lines in the 
server’s email indicating from which Web page the response had been submitted, clearly 
delineating the responses of the two groups.  In addition, Gform will convey to the server 
whatever information a programmer associates with each answer, enabling coding to be 
assigned to each answer at the time the Web form is constructed.  Although more 
advanced software is available which can deliver response information directly into a 
database file, working with Gform on this more basic level helped speed the manual data 
entry without the high cost of such software. 
One significant difference between the electronic and paper versions of the survey 
concerned information about the respondent’s library’s Carnegie classification.  In 
preliminary testing, many respondents were unsure of their library’s category.  In an 
 
 
 
effort to boost response to this item, the Web survey linked to a Web site presenting a list 
of institutions in each category.  This had significant unforeseen ramifications due to 
inadequate pre-testing.  Shortly after the email announcement survey was sent, a 
respondent notified the researcher that following this link cleared all previously marked 
responses on the Web form.  Since the classification question occurred at the end of the 
questionnaire, answers to virtually all questions were lost and it was annoying to have to 
do the entire survey again.  A warning was immediately inserted about the problem on 
both survey Web pages, as well as a suggestion to open a separate browser window to 
follow the link; but several people had already responded without noticing and several 
responded subsequently with blank forms.  The consequences of this problem are 
discussed further in the Results section. 
Another problem which manifested itself in the data analysis concerned the 
question about respondents’ primary work responsibilities.  The original question invited 
respondents to note whether their primary work was technical, public service or 
managerial.  In the Web survey the options were controlled by radio buttons, enabling a 
respondent to select only one answer.  In the paper survey this preference for one answer 
wasn’t expressed (e.g, “Select only one”), so a number of respondents marked more than 
one response.  Handling of this problem will be discussed further in the Results section. 
 
IV.  Results 
A total of 130 people responded to the survey overall, a response rate of 43.33%.  
53 respondents had received the paper survey; 33 had received the mail announcement of 
the Web survey; and 44 had received the email announcement of the Web survey.  Of the 
53 paper responses, 51 were usable; of the 33 mail announcement responses, 27 were 
 
 
 
usable; and of the 44 email announcement responses, 39 were usable.  This results in an 
overall usable return rate of 39.33%.  On the basis of these 118 usable responses, the 
usable paper survey response rate was 43.22%, the mail announcement rate 22.88%, and 
the email announcement rate 33.05%. 
v Hypothesis 1:  Respondents to the Web survey will not exhibit significantly 
different demographics from respondents to the paper survey. 
 
Hypothesis 1 was not rejected.  Tests of the three survey groups on variables 
relating to respondents' demographic characteristics found no significant differences (at a 
.05 level of significance) between respondents to the paper versus the Web survey.   
Demographic characteristics tested include age, sex, library education and amount of 
time respondents have worked in libraries among other things.  Details of the tests are 
presented in Appendix 4. 
v Hypothesis 2:  Respondents to the Web survey will give responses that are not 
significantly different from responses to the paper survey; and 
 
v Hypothesis 3:  Respondents will not respond significantly differently to 
the Web survey whether they were notified of it by email or postal mail. 
 
Hypothesis 2 was not rejected.  The three survey groups were compared on the 
basis of variables relating to respondents' opinions about the NC LIVE program and those 
relating to respondents’ computer experience.  Opinion variables tested include five 
positive and four negative aspects of the NC LIVE program.  Computer experience 
variables tested include questions about respondents’ frequency of use of various kinds of 
electronic resources and home access to computers.   The tests found no significant 
differences (at a .05 level of significance) between respondents to the paper versus the 
Web survey, except for one variable.  An ANOVA demonstrated a significant 
relationship between the mean demand for computers prior to the start of the NC LIVE 
 
 
 
program for responses from the three survey groups (F=37.769 with 2 df, p=0.000).  The 
Bonferroni post-hoc analysis indicated a significant relationship only between the paper 
survey and the Web survey group notified by email (see Appendix D for more details).  
Details of the tests are presented in Appendix D.    
On the basis of these same tests of variables relating to respondents' opinions 
about the NC LIVE program and those relating to respondents’ computer experience, 
Hypothesis 3 was not rejected.   The tests found no significant differences (at a .05 level 
of significance) among respondents to the Web survey, whether they were notified of it 
by mail or email.   
One variable that at first appeared to show a difference between the Web and 
paper instruments was found to be not significant upon further analysis.  In the data entry 
stage, with the goal of capturing as much information as possible, a fourth category was 
noted for the question about primary work, “combination,” to make note of those paper 
surveys where the respondent had marked more than one category.  In the preliminary 
data analysis stage, the fact that this fourth category was artificially weighted toward the 
paper survey was forgotten.  An ANOVA seemed to indicate that those who marked 
“managerial” as their primary work were more likely to answer the paper survey.  When 
the mistake was realized, the responses in this category were reclassified into the work 
category with the largest number of responses, forcing a somewhat artificial designation.  
So, for example, if  someone marked both “technical” and “manager
response was reclassified in the “managerial” set because there were more responses in 
that group than in the “technical” set.    In re-running the significance tests, no 
 
 
 
relationship was indicated between respondents’ primary work category and their 
likelihood of answering via the Web or on paper. 
v Hypothesis 4:  The response rate of a Web survey will be no different from that of 
a paper survey. 
 
Hypothesis 4 was rejected.  The Web survey in this study did not have the same 
response rate as the paper survey.  The paper survey achieved a response rate ten 
percentage points higher than the Web survey group notified by email and more than 
twenty percentage points higher than the Web survey group notified by mail.  Certainly, 
to achieve a comparable response rate to a paper instrument, it is critical that the 
electronic survey be free of technical problems.  The effects of the problem link from the 
Carnegie classification item in the Web survey were significant, invalidating  15.15% of 
the total response to the Web survey.   Yet even if all responses to the Web survey had 
been valid, the Web survey’s response rate would not have matched that of the paper 
survey.  Consideration of the literature review seems to confirm this as a general trend for 
electronic surveys in comparison with paper surveys, whether the electronic survey is 
administered via email or the Web.  Electronic surveys often seem to generate lower 
response rates than paper surveys, although the degree of difference between the two 
rates may vary according to how well each survey is promoted, what followup and 
motivational procedures are employed, and the general responsiveness of the population 
surveyed.   
 
v Hypothesis 5: At least 50 percent of the total number of responses to the Web 
survey will be returned in one week.  
 
Hypothesis 5 was rejected.  Fifty percent of responses to the Web survey were not 
received within one week.  Only 27.5% of the total number of responses to the Web 
 
 
 
survey was received within the first week.  An ANOVA showed a clear relationship 
between the date the returned survey was received and which survey the respondent 
completed (F=37.769 with 2 df, p=0.000).  Furthermore, a Bonferroni post-hoc analysis 
indicated a strong difference between the mean of the paper survey and those of both the 
Web survey groups (p=.000 for each comparison); however, there was no significant 
difference between the two Web survey groups (see Appendix D for more details). 
Certainly, responses from the email announcement group were returned the fastest  
of the three groups.  The first response was returned within an hour and a half of sending 
the announcement.  Of the 52 responses received in the first seven days, 27 (65.85%) 
were from this group.  Fourteen were from the Web survey/mail notice group, and eleven 
from the paper survey group (see Figure 1, next page).   
The electronic returns did not have quite as large a lead over the paper returns as 
expected, however:  while 30% of responses received from the Web survey notified by 
email group arrived by the seventh day from mailing, only 14% of the total from the Web  
survey notified by mail group were received by that time, compared to 21.57% of the 
total responses received from the paper survey group.   (Only usable responses were 
counted for these calculations.)  This may be in part because the paper surveys were 
mailed earlier in hopes that the paper and electronic instruments would be received at the 
same time.   
 
 
 
 
 
Figure 1.  Date Received (by Survey Group)
0
5
10
15
20
25
4/
20
4/
21
4/
22
4/
23
4/
24
4/
26
4/
27
4/
28
4/
29
4/
30 5/
4
5/
5
5/
6
5/
7
5/
8
5/
11
5/
13
5/
15
5/
19
Date
N
um
be
r 
of
 S
ur
ve
ys
 R
ec
ei
ve
d
Paper survey Web Survey/Notified by Email
Web Survey/Notified by Mail
 
 
v Hypothesis 6:  Of the two methods of publicizing a Web survey, postal 
mail and email, email is the more efficient one.  
 
Hypothesis 6 was not rejected.  The email notice of the Web survey was more 
efficient in eliciting responses than the mailed notice.  An ANOVA demonstrated a 
significant relationship between the mean date received for responses from the three 
survey groups (F=37.769 with 2 df, p=0.000).  However, the Bonferroni post-hoc 
 
 
 
analysis indicated significant relationships only between the paper survey and each of the 
Web groups, not between the two Web groups (see Appendix D for more details).  
  
V.  Discussion 
Comparisons of the demographics of the respondents to the Web and paper 
surveys found no significant differences between the demographic make up of the three 
survey groups.   How old a person was, nor their sex, nor what degree of education they 
had attained influenced which survey they answered.  Details of the tests are available in 
Appendix 4. 
Comparisons of the responses to the Web and paper surveys found no significant 
differences between the content of responses from the three survey groups, except for one 
variable.  Since no other variables showed significant differences between the responses 
to the paper and Web surveys, and since there was no difference between the mean 
demand for computers prior to the start of the NC LIVE program between the paper 
survey and the Web survey group notified by mail, this result is puzzling.  Overall, 
attitude toward the NC LIVE program was not affected by the medium in which 
respondents answered, nor was there significant difference between the groups in terms 
of experience with electronic resources.  Details of these tests and some further 
discussion are available in Appendix D. 
The overall response rate was higher for the paper survey than for either of the 
Web survey groups (43.22% for the paper survey versus 33.05% for the Web survey 
notified by email and 22.88% for the Web survey notified by regular mail).  When only 
the most thorough method will do and a high response rate is critical, paper and pencil 
 
 
 
still hold the lead over electronic means as a survey method.  Considerations for future 
researchers will include time constraints, cost, and the motivation of participants to 
respond.  Significantly more responses were received to the paper survey than to the Web 
survey.  Paper surveys remain a more productive medium for response, even among an 
electronically proficient community. 
Response from the Web survey groups was faster overall than from the paper 
survey group, but not by as wide a margin as originally expected.  Counting only usable 
responses, 30% of responses received from the Web survey notified by email group 
arrived by day seven, but only 14% from the Web survey notified by mail group, 
compared to 21.57% of the total responses received from the paper survey group which 
arrived by day seven.  Promotion of the Web survey was clearly accomplished more 
efficiently via email than mail.  Even before discounting unusable responses, the email 
group filling out the Web survey was more likely to respond than the mail group.   It 
seems likely that the better early response to the Web survey is partly due to its earlier 
deadline.  The cover letter/email made clear that responses were requested by May 1,  
and respondents clearly made an effort to comply with that.  It would be interesting to 
compare long-term response with identical deadlines. 
The most significant difference between the two instruments from a data 
standpoint was the flexibility the paper instrument provided respondents in how they 
answered.  Respondents felt free to make comments about questions they didn’t 
understand or felt were ambiguous.  They often provided different answers to the same 
question to illustrate the different ways of interpreting it.  The Web survey forced 
respondents to answer in particular ways, with no easy means for providing comments.  
 
 
 
Confusion on the part of the researcher over the significance of respondents’ primary 
work category arose because respondents to the paper survey were able to mark more 
than one answer to the question, whereas the web survey respondents were permitted 
only one choice by the radio button answer selector.  Choice of radio buttons or check 
boxes gives researchers a greater amount of control over how they want respondents to 
answer questions, but it also results in receipt of less information overall from the 
respondents.  It might have been very important for the study to know that some 
academic librarians have combinations of different kinds of work in their jobs.  If only 
the Web survey had been administered, this fact would not have come to light at all. 
For this population, the use of Web surveys seems a reasonable alternative to 
postal surveys or telephone interviews, depending on the research question.  Clearly, the 
response rate in this study is rather low.  Dillman (1978) notes that a response rate such 
as that achieved for the Web survey group notified by email—33%—leaves the majority 
of the population unsurveyed.  If the survey in some way discouraged from responding 
people who all felt the same way, the majority opinion would remain unrepresented by 
these results.  Given that no significant difference was found between survey groups on 
the basis of the demographic variables (see Appendix D), it is unlikely that such glaring 
bias would exist among the non-respondents.  It is, however, a possibility.  Further 
research into what motivations can be used effectively with electronic surveys would be 
useful. 
 
Technical Difficulties 
Perhaps the greatest caveat this study offers for future Web surveys is to caution 
that survey authors find expert consultation on the technical aspects of the survey and 
 
 
 
conduct fully as thorough a pre-test on an electronic survey as on a paper instrument, 
even if they include exactly the same items.  Smith (1997) provided the only comparison 
of email and Web surveys found in this study, but she encountered serious technical 
difficulties in publishing her Web survey to the extent that she was unable to gather 
sufficient data to make a meaningful comparison between the two modes.  Her survey 
format was incompatible with at least two types of Internet browsers, preventing 
respondents from submitting their completed surveys.  Due to inadequate pre-testing of 
the electronic instrument for the current study, a significant number of responses to the 
Web survey were lost.  Researchers should be very sure they have anticipated and 
diagnosed as many technical issues as possible.  A few other possible problems, among 
many, include the way in which the questions are displayed by different browsers, 
whether different browsers can interpret the form protocol used, and problems with early 
generation computers interpreting advanced applications, such as Javascript and frames.  
 
VI.  Conclusion 
The advantages the Web survey holds in administration time and cost almost 
force researchers to consider it as a serious alternative to more traditional survey media.  
Miller et al. (1996) suggested the efficiency of electronic data collection method 
justified its use, particularly for exploratory research and for populations that have no 
sampling frame.  However, I would disagree.  The greater freedom paper instruments 
offer respondents should be an important consideration, particularly for exploratory 
research.  Of course researchers must weigh many factors in choosing an appropriate 
survey medium, but when cost and time constraints outweigh other considerations, the 
 
 
 
Web may be an adequate medium if sufficient comment areas are supplied within the 
survey form and the sampling frame for the population is not biased. 
 
 
 
Bibliography 
Anderson, Susan E., and Gansneder, Bruce M. (1995).  Using electronic mail surveys and 
computer-monitored data for studying computer-mediated communication 
systems.  Social Science Computer Review, 13(1), 33-47. 
 
Babbie, Earl.  (1998).  The Practice of Social Research.  Belmont, CA:  Wadsworth 
Publishing Company. 
 
Bachmann, Duane, Elfrink, John and Vazzana, Gary (1996).  Tracking the progress of e-
mail versus snail-mail:  gap narrows on response rates, but applications still 
limited.  Marketing Research, 8(2), 31-35. 
 
Berge, Zane L., and Collins, Mauri P. (1996).  IPCT journal readership survey.  Journal 
of the American Society for Information Science, 47(9), 701-710. 
 
Bertot, John C., and McClure, Charles R. (1996).  Electronic surveys:  methodological 
implications for using the World Wide Web to collect survey data.  ASIS 
Proceedings, 33, 173-185. 
 
Booth-Kewley, Stephanie, Edwards, Jack E., and Rosenfeld Paul (1992).  Impression 
management, social desirability, and computer administration of attitude 
questionnaires:  Does the computer make a difference?  Journal of Applied 
Psychology, 77(4), 562-566. 
 
Clayton, Peter, Applebee, Andrelyn and Pascoe, Celina (1996).  Email surveys:  old 
problems with a new delivery medium.  LASIE:  Information Bulletin of the 
Library Automated Systems Information Exchange, 27, 30-39. 
 
Dillman, Don A.  (1978).  Mail and Telephone Surveys:  The Total Design Method.  New 
York:  John Wiley and Sons. 
 
Dixon, Philip.  Personal correspondence, October 13 and November 3, 1999. 
 
Erdman, Harold, Klein, Marjorie H., and Greist, John H. (1983).  The reliability of a 
computer interview for drug use/abuse information.  Behavior Research Methods 
and Instrumentation, 15(1), 66-68. 
 
Gazel, Ricardo C., and Schwer, R. Keith (1998).  Interview mode choice by survey 
respondents:  a methodological analysis.  Social Science Computer Review, 16(2), 
185-191. 
 
 
 
GVU Center.  (1998).  GVU's 10th WWW User Survey.  College of Computing, Georgia 
Institute of Technology.  [Online, November 1, 1999].  Available:  
http://www.gvu.gatech.edu/user_surveys/survey-1998-10/. 
 
Helgeson, James G. and Ursic, Michael L. (1989).  The decision process equivalency of 
electronic versus pencil and paper data collection methods.  Social Science 
Computer Review, 7(3), 296-310. 
 
Kaufman, Judith S., Carlozzi, Alfred F., Boswell, Donald L., Barnes, Laura L.B., 
Wheeler-Scruggs, Kathy and Levy, Patricia A. (1997).  Factors influencing 
therapist selection among gays, lesbians and bisexuals.  Counseling Psychology 
Quarterly, 10(3), 287-297. 
 
Kiesler, Sara, and Sproull, Lee (1986).  Response effects in the electronic survey.  Public 
Opinion Quarterly, 50(3), 402-413. 
 
Meehan, Merrill L. and Burns, Rebecca C. (1997, March).  E-mail survey of a listserv 
discussion group:  lessons learned from surveying an electronic network of 
learners.  Paper presented at the annual meeting of the American Educational 
Research Association, Chicago, IL. 
 
Mehta, Raj and Sivadas, Eugene (1995).  Comparing response rates and response content 
in mail versus electronic mail surveys.  Journal of the Market Research Society, 
37(4), 429-439. 
 
Miller, James, Daly, John, Wood, Murray, Brooks, Andrew, and Roper, Marc (1996).  
Electronic bulletin board distributed questionnaires for exploratory research.  
Journal of Information Science, 22(2), 107-115. 
 
Morphew, Christopher C. and Williams, Andrew N. (1998).  Using electronic mail to 
assess undergraduates' experiences:  assessing frame and mode effects.  Journal 
of Computing in Higher Education, 10(1), 38-55. 
 
Pitkow, James E.  (1996).  Emerging trends in the WWW user population.  
Communications of the ACM, 39(6) 106-108. 
 
Roselle, Ann and Neufeld, Steven (1998).  The utility of electronic mail follow-ups for 
library research.  Library and Information Science Research, 20(2), 153-161. 
 
Shaw, Debora, and Davis, Charles H. (1996). The Modern Language Association:  
electronic and paper surveys of computer-based tool use.  Journal of the 
American Society for Information Science, 47(12), 932-940. 
 
Skinner, Harvey A.,  and Allen, Barbara A. (1983).  Does the computer make a 
difference?  Computerized versus face-to-face versus self-report assessment of 
 
 
 
alcohol, drug and tobacco use.  Journal of Consulting and Clinical Psychology, 
51(2), 267-275. 
 
Smith, Christine B. (1997).  Casting the `Net:  surveying an Internet population.  Journal 
of Computer Mediated Communication [Online, July 26, 1999], 3(1).  Available:  
http://jcmc.mscc.huji.ac.il/vol3/issue1/smith.html. 
 
Sproull, Lee (1986).  Using electronic mail for data collection in organizational research.  
Academy of Management Journal, 29(1), 159-169. 
 
State of North Carolina.  NC LIVE Interim Memorandum of Understanding.  [Online, 
November 17, 1998].  Available:  http://www.nclive.org/backgrnd/mou.pdf. 
 
Swoboda, Walter J., Mühlberger, Nikolai, Weitkunat, Rolf, and Schneeweib, Sebastian 
(1997).  Internet surveys by direct mailing.  Social Science Computer Review, 
15(3), 242-255. 
 
Synodinos, Nicolaos E., Papacostas, C.S., and Okimoto, Glenn M. (1994).  Computer 
administered versus paper-and-pencil surveys and the effect of sample selection.  
Behavior Research Methods, Instruments and Computers, 26(4), 395-401. 
 
Thach, Liz (1995).  Using electronic mail to conduct survey research.  Educational 
Technology, 35(2), 27-31. 
 
Walsh, John P., Kiesler, Sara, Sproull, Lee S., and Hesse, Bradford W. (1992).  Self-
selected and randomly selected respondents in a computer network survey.  
Public Opinion Quarterly, 56(2), 241-244. 
 
 

