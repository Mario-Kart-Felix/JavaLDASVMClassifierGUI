 
Advantages and Disadvantages of Internet Research Surveys: 
Evidence from the Literature 
 
Ronald D. Fricker, Jr. and 
 Matthias Schonlau 
RAND 
 
E-mail and Web surveys have been the subject of much hyperbole about their capabilities 
as well as some criticism about their limitations.  In this report we examine what is and 
is not known about the use of the Internet for surveying.  Specifically, we consider 
evidence found in the literature regarding response rates, timeliness, data quality and 
cost.  In light of this evidence, we evaluate popular claims that Internet-based surveys 
can be conducted faster, better, cheaper, and/or easier than surveys conducted via 
conventional modes.  We find that the reality of cost and speed often does not live up to 
the hype. Nonetheless, it is possible to implement Internet-based surveys in ways that are 
effective and cost-efficient.  We conclude that the Internet will continue to grow in 
importance for conducting certain types of research surveys.   
INTRODUCTION 
With the advent of the World Wide Web (Web or WWW) and electronic mail (e-
mail), the Internet has opened up new vistas in surveying.  Rather than mailing a paper 
survey, a respondent can now be given a hyperlink to a Web site containing the survey.  
Or, in an e-mail survey, a questionnaire is sent to a respondent via e-mail, possibly as an 
attachment.  As either an alternative or an adjunct to conventional survey modes (e.g., the 
telephone, mail, and face-to-face interviewing) Internet-based surveys offer unique new 
capabilities.  For example, a Web survey can relatively simply incorporate multi-media 
graphics and sound into the survey instrument.  Similarly, other features that were once 
restricted to more expensive interviewer-assisted modes, such as automatic branching 
and real-time randomization of survey questions and/or answers, can be incorporated into 
self-administered Web (and some e-mail) surveys.  However, not unlike when phone and 
mail surveys were first introduced, concerns exist about whether these Internet-based 
surveys are scientifically valid and how they are best conducted. 
In the late 1980s and early 1990s, prior to the widespread availability of the Web, 
e-mail was first explored as a survey mode.  As with the Web, e-mail offers the 
possibility of nearly instantaneous transmission of surveys to recipients while avoiding 
any postal costs.   Early e-mail were primarily ASCII text-based, with rudimentary 
formatting at best, which tended to limit their length and scope.  The only significant 
advantage they offered over paper was a potential decrease in delivery and response 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
1 
 
times, though some also hypothesized that the novelty of the new medium might enhance 
response rates (Parker, 1992; Zhang, 2000).  
The Web started to become widely available in the early to mid-1990s and quickly 
supplanted e-mail as the Internet survey medium of choice because it was easy to 
implement, it provided an improved interface with the respondent, and it offered the 
possibility of multimedia and interactive surveys containing audio and video.  For 
convenience samples, the Web also offered a way around the necessity of having to know 
respondents’ e-mail addresses.  As a result, “quick polls” and other types of 
entertainment surveys have become increasingly popular and widespread on the Web. 
Internet-based surveys are now in vogue—those conducted via the Web in 
particular—because of three assumptions: (a) Internet-based surveys are much cheaper to 
conduct; (b) Internet-based surveys are faster; and, (c) when combined with other survey 
modes, Internet-based surveys yield higher response rates than conventional survey 
modes by themselves.  Yet, does the evidence in the literature confirm these 
assumptions?  Are Internet-based surveys faster, better, cheaper, and/or easier than 
surveys conducted via conventional modes?  What can we conclude about the strengths 
and current limitations of Internet-based surveying from the facts in the literature?   
  In this report we synthesize the literature about the use of the Internet (e-mail and 
the Web) in the survey process.  Other accounts of the literature include Schonlau, 
Fricker and Elliott (2002), Couper (2000), Dillman (2000), and Tuten et al. (2002).  In 
addition, an extensive source of Web survey literature can be found on the Web at 
www.websm.org. 
LITERATURE SUMMARY FOR INTERNET-BASED SURVEYS 
In this section we summarize key characteristics of Internet-based surveys—that 
is, surveys using the Web and e-mail as a response mode—as documented in the 
literature.  We employed a professional librarian to conduct a thorough literature search 
in the Social Science Database and the Conference Paper Index database. The Social 
Science Database indexes more than 1,500 of the most important worldwide social 
sciences journals since 1972.  Additional articles relevant to the social sciences are also 
incorporated from over 2,400 journals in the natural, physical, and biomedical sciences.  
The Conference Paper Index provides access to records of the more than 100,000 
scientific and technical papers (since 1973) presented at over 1,000 major regional, 
national, and international meetings each year. 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
2 
 
The literature search yielded 57 papers that were substantively interesting and 
informative.  Here we report on a subset of those articles of direct relevance to this 
discussion.  (Appendix B of Schonlau et al., 2002,  lists 52 papers and we have 
augmented the list here with an additional five that have appeared since Schonlau et al. 
was published.)  We consider the following key characteristics of surveys: (1) response 
rate, (2) timeliness, (3) data quality, and (4) cost.  We compare what has been published 
in the literature about Internet-based surveys to a natural conventional survey alternative: 
mail.  While no survey mode is going to be optimal in all of these areas, we chose mail 
because both mail and Internet-based surveys are self-administered, mail surveys tend to 
be the least expensive of the conventional modes, and virtually all of the comparisons 
made in the literature are to mail surveys.    
Response Rates 
A standard way to summarize survey performance is by comparing response rates 
among various survey modes.  By “survey mode” (sometimes called response mode) we 
mean the mode by which the survey itself is conducted: Web, e-mail, mail, etc.  In this 
section, we compare response rates for studies classified into one of three categories: (1) 
Surveys employing probability sampling or conducting a census that used the Web as the 
only response mode; (2) Surveys in which respondents were allowed to choose one of 
several response modes, including at least one Internet-based response mode; and, (3) 
Surveys in which respondents were assigned one of several response modes, including at 
least one Internet-based response mode.  
We begin with results for studies that used the Web as the primary or only response 
mode with either censuses or probability samples (Table 1).  The table is ordered by year 
and it shows that Web-only research surveys have currently only achieved fairly modest 
response rates, at least as documented in the literature.   
Field Methods, Vol. 14 No. 4, 2002 347-367. 
3 
 
Table 1.  Response Rates for Web-only Surveys Using Probability Samples or Censuses                                     
 
Survey 
Sample 
Size 
Response 
Rate 
Population 
Couper et al. (2001) 1,602 42%d University of Michigan Students 
Asch (2001)a 14,150 8% College-bound high school 
  and college students 
Everingham (2001) 1,298 44% RAND employees 
Jones and Pitt (1999) 200 19% University staff 
Dillman et al. (1998)b 9,522 41% Purchasers of  
  computer products 
Dillman et al. (1998)c 2,466 38% Purchasers of  
  computer products 
a Most respondents were contacted via their parents, which reduced the response rate. A mail 
response mode was added late in the survey protocol.
b A relatively plain Web survey design was used in this experimental arm. 
    c A relatively fancy Web survey design was used in this experimental arm. 
    d Another 5.6 percent of partially completed surveys were also received. 
 
In fact, the results in Table 1 may overstate response rate performance for research 
surveys of broader populations because Dillman’s results are based on participants who 
were initially contacted by phone and had agreed to participate in a Web survey and 
Everingham’s sample was of a closed population of employees at one company.  Jones 
and Pitt (1999) sampled staff at “10 universities whose staff directories were available on 
the WWW”  and Couper et al. (2001) surveyed 1,602 University of Michigan students.  
In all of these cases, the potential survey participants were likely to be more 
homogeneous and more disposed to respond compared to a random sample of the general 
population.  In addition, because university populations often tend to have greater access 
to the Internet, and today’s college students can be expected to be more computer- and 
Internet-savvy.  
In Table 2 we summarize the studies published in the literature that allowed the 
respondent to choose to respond either via the Web or through the mail, ordered in terms 
of the fraction that responded via the Web.  Since for many populations the fraction of 
respondents that can or will answer via the Web may not be sufficiently large, and mail 
emerges as the most relevant second mode for a dual mode survey, these studies are 
important. 
 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
4 
 
Table 2.  Studies Allowing Respondents to Choose a Web or Mail Response Mode 
 
 
Total 
Sample 
% Chose to 
Respond by … 
Overall 
Response 
 
Study Size Mail Web Rate Population 
Raziano et al. (2001)a 57 96% 4%b 77%
 U.S. Geriatric Chiefs 
Sedivi Gaul (2001) and Griffin et al. (2001) 
(American Community Survey [2000]) 9,596 
95% 5% 38% U.S. households 
Sedivi Gaul (2001) and Griffin et al. (2001) 
(Library Media Center Survey [1998]) 924 
95% 5% 38% Librarians 
Sedivi Gaul (2001) and Griffin et al. (2001) 
(Library Media Center Survey [1999]) 13,440 
81% 19% 63% Librarians 
Quigley et al. (2000) (DoD study) 21,805 77% 23% 42% U.S. military and spouses  
Quigley et al. (2000) (DoD study) 7,209 83% 27% 37% Civilians 
Raziano  et al. (2001)c 57 45% 52%b 58%
 U.S. Geriatric Chiefs 
Zhang (2000) 201 20% 80% 78% Researchers 
Schleyer and Forrest (2000) 405 16%d 84% 74% Dentists  
NOTE: The multiple Quigley et al. and Raziano et al. entries represent multiple arms of the same study. 
a This arm of the study used mail as the contact mode. 
b Includes e-mail. The authors do not distinguish between e-mail and Web as a response mode. 
c This arm of the study used e-mail as the contact mode. 
d The response mode in this case was either e-mail or fax. 
 
 
In Table 2 we see that for most of the studies respondents currently tend to choose 
mail when given a choice between Web and mail.  In fact, even when respondents are 
contracted electronically it is not axiomatic that they will prefer to respond electronically, 
as in Raziano et al. (2001) that did not find a statistically significant difference in 
response rates.  Zhang (2000) and Schleyer and Forrest (2000) are the only studies that 
contradict this conclusion and they tend to represent groups of respondents that are 
largely or entirely computer literate and comfortable with electronic communication.  In 
comparison, Quigley et al. (2000) and the American Community Survey (2000) study 
tend to represent general cross-sections of the U.S. public in terms of computer literacy 
and availability and for these studies the fraction that chose Web as the response mode 
was quite small. 
In Table 3 we present studies that compared response rates between groups 
assigned to one of either two or three response modes.  Here we see that Internet-based 
mode response rates generally do not achieve response rates equal to mail surveys.  (The 
table is first ordered from lowest to highest e-mail response rate and then by Web 
response rate.)  Further, Sheehan (2001) concludes that e-mail response rates are 
declining over time (though the reason for the decline is unknown). 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
5 
 
 
Table 3.  Studies With Multiple Study Arms: Comparing Response Rates for E-mail, Web and Mail 
Response Modes 
 
Study 
Total 
Sample Size 
Response Rate 
Web      E-mail       Mail 
 
Population 
Tse et al. (1995) 400 -- 6% 27% University staff 
Tse (1998) 500 -- 7% 52% University staff 
Schuldt and Totten (1994) 418 -- 19% 57% MIS and marketing faculty 
Kittleson (1995) 153 -- 28% 78% Health educators 
Mehta and Sivadas (1995) 262 -- 40% 45% BBS newsgroup users 
Couper et al. (1999) 8,000 -- 43% 71% Federal employees 
Schaefer and Dillman (1998) 904 -- 53%a 58% WSU faculty 
Parker (1992) 140 -- 68% 38% AT&T employees 
Jones and Pitt (1999) 200 19% 34% 72% University staff 
Vehovar et al. (2001)c 1,800 32% -- 54% Businesses in Slovenia 
Pealer et al. (2001)b 600 58%
 -- 62% Undergraduates at the University 
of Florida 
McCabe et al. (2002) 5,000 63% -- 40% University of Michigan Students 
-- Indicates not applicable; the indicated response mode was not evaluated in the study. 
a An additional 5 percent that were returned by mail are not included in this number. 
b  In the 2nd follow-up of both study arms respondents were contacted by both mail and e-mail. 
c An additional phone study arm achieved a response rate of  63%, an additional contact mail / 
response fax study arm achieved a response rate of 43%. 
 
Parker (1992) is the only study of which we are aware in which e-mail achieved 
equal or higher response rates when compared to postal mail.  Parker conducted a survey 
of 140 expatriate AT&T employees on matters related corporate policies for expatriation 
and repatriation, reporting a 63 percent response rate via e-mail (63 returned out of 100 
sent by e-mail) compared to a 38 percent response rate for postal mail (14 returned out of 
40 sent by mail).  Interestingly, Parker (1992) also attributed the difference in response 
rates to the fact that, at the time, AT&T employees received a lot of corporate paper junk 
mail yet, over the internal e-mail system, they received little to no electronic junk mail.  
Hence, recipients of the paper survey were more likely to discount its importance 
compared to e-mail survey recipients.  With the spread of e-mail “spam,” this situation is 
likely to be reversed today. 
In an example more typical of the current state of affairs, and in one of the few 
studies to randomize respondents to mode, Couper et al. (1999) obtained an average e-
mail response rate of about 43 percent compared to almost 71 percent with mail in a 
survey of employees in five federal statistical agencies.  Couper et al. chose e-mail as the 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
6 
 
mode for the survey over the Web because e-mail was almost universally available in the 
five agencies while the Web was often not available.   
Turning to the Web, McCabe et al. (2002) conducted an experiment in which 5,000 
University of Michigan students were randomized to receive a survey about drug and 
alcohol use; 2,500 potential respondents received a mail survey and 2,500 were notified 
of an equivalent Web-based survey.  Respondents in both groups received a $10 gift 
certificate incentive.  In this study, McCabe et al. achieved a 63 percent Web response 
rate compared to 40% for mail.   In contrast, however, Pealer et al. did not find a 
statistically significant difference between Web and mail response rates for a survey of 
undergraduates at the University of Florida.   
The only other published study that achieved exceptional response rates with an 
Internet-based survey is Walsh et al. (1992) in which potential respondents were solicited 
by e-mail and offered the option to respond by e-mail or request a paper survey by postal 
mail.  While they did not conduct an equivalent postal mail only survey for comparison 
(and thus are not listed in Table 3), Walsh et al. achieved a 76 percent overall response 
rate of a randomly sample of subscribers (300 out of a total population of 1,100) to a 
scientific computer network for an e-mail survey.  In addition to providing non-
respondents with two follow-up reminders, a lottery prize of $250 was employed as an 
incentive.   
Walsh et al. found that 76 percent of the respondents replied by e-mail and the 
other 24 percent responded by postal mail.  They also received requests from an 
additional 104 subscribers (who were not chosen in the sample of 300) to participate in 
the survey.  For the self-selected 104, 96 percent responded by e-mail.  Not surprisingly, 
they also found a positive correlation between propensity to respond electronically and 
amount of network usage. 
In conclusion, there is little evidence in the literature that Internet-based surveys 
achieve higher response rates, as a general rule, than conventional surveys.  The few 
Internet-based surveys that have achieved higher response rates have tended to be either 
of university-based populations or small, specialized populations.  The majority of results 
reported in the literature show Internet-based surveys at best currently achieve response 
rates equal to conventional modes and often do worse.  The reasons for this difference are 
not yet clear and require more study.   
Yet, as we have seen, there are also a few examples of Web surveys outperforming 
mail for some specific populations.  Whether this was idiosyncratic of these few surveys, 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
7 
 
or it is an indication that methodology is developing to achieve higher response rates in 
the new medium is yet to be shown. 
It is important to note that, contrary to intuition, there is no evidence in the 
literature that concurrent fielding of a survey via a conventional mode and via an 
Internet-based mode results in any significant improvement in response rates.  This may 
be because, as Table 2 shows, except in specialized populations, when given a choice 
between mail and Web surveys, most individuals tend to respond to the mail survey.  In 
addition, there is no evidence that those who would normally refuse to complete a mail 
survey would choose to respond if the survey was Internet-based.  Of course, these 
results are specific to the current state of the art of Internet-based surveying, existing 
technology, and the current state of respondent attitudes toward surveys, both Internet-
based and conventional.  Future developments may significantly alter these findings and 
more research is certainly warranted in an attempt to improve the response rate 
performance of Internet-based surveys. 
Finally, we note that while research surveys based on probabilistic survey sampling 
methods are generally recognized as being necessary to conduct statistical inference to 
any population outside of the sample, convenience sampling can also be useful to some 
researchers for other purposes.  For example, early in the course of research, responses 
from a convenience sample might be useful in developing research hypotheses.  
Responses from convenience samples might also be useful for identifying issues, 
defining ranges of alternatives, or collecting other sorts of non-inferential data.  In fact, in 
certain types of qualitative research, convenience samples on the Web may be just as 
valid as other methods that use convenience samples.  
There are a number of studies in the literature that used convenience samples, for 
which response rate comparisons do not apply (and hence precluded their inclusion in 
Tables 1-3), often with respondents recruited through advertising of some form.  While 
response rates for these studies are meaningless, we present a few of the more interesting 
studies here to illustrate alternative ways that Web surveys can be used.  In a social 
science study of geographic mobility and other topics Witte et al. (2000) recruit a large 
number of respondents: 32,688.  Similarly, Vehovar et al. (1999) conducted a large-scale 
survey targeted at the Internet population of Slovenia, which corresponds to about 13 
percent of the total population of Slovenia.  In both cases, similarly sized traditional mail 
surveys would likely have been more complicated and very expensive to field.  Coomber 
(1997) conducted a survey about drug dealer practices, where his target population was 
illicit drug-dealers throughout the world.  Coomber solicited responses by e-mail and 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
8 
 
through advertising, and collected responses on the Web hoping his respondents would 
be encouraged to respond more honestly because of a perceived anonymity.  
Timeliness 
In today’s fast-paced world, survey timeliness is increasingly stressed.  The length 
of time it takes to field a survey is a function of the contact, response, and follow-up 
modes.  Decreasing the time in one or more of these parts of the survey process will tend 
to decrease the overall time in the field.  However, it is important to keep in mind that the 
relevant measure is not average response time but maximum response time (or perhaps 
some large percentile of the response time distribution) since survey analysis generally 
does not begin until all of the responses are in.   
Most studies tend to conclude, often with little or no empirical evidence, that 
Internet-based surveys are faster than surveys sent by postal mail.  This conclusion is 
usually based on the recognition that electronic mail and other forms of electronic 
communication can be instantaneously transmitted while postal mail takes more time.  
However, simply concluding that Internet-based surveys are faster than mail surveys 
naïvely ignores the reality that the total amount of time for survey fielding time is more 
than just the survey response time.  A complete comparison must take into account the 
mode of contact and how long that process will take and the mode of follow-up allowing 
for multiple follow-up contact periods.  For example, if e-mail addresses of respondents 
are unavailable and a probability sample is desired then respondents may have to be 
contacted by mail.  In this case a Web survey only saves time for the return delivery of 
the completed questionnaire, and not for the contact and follow-up, so that the resulting 
time savings may only be a fraction of the total survey fielding time.     
In the case of e-mail surveys, where the presumption is that the potential 
respondents e-mail addresses are known and can therefore be used not just for delivering 
the survey but also for pre-notification and non-response follow-up, the time savings can 
be substantial.  For example, one is often forced to allow for a week of delivery time in 
the postal mail.  With an advance letter and a single mail follow-up, this one week delay 
telescopes into over a month in survey fielding when two weeks must be budgeted for 
initial survey delivery and return time, plus an additional two weeks for a single follow-
up reminder delivery and response time.  By comparison, in an all-electronic process the 
same operation has the potential to be completed in a few days or less. 
Yet, even in an all-electronic environment it is not necessarily true that the 
Internet-based survey will be timelier.  For example, in a comparison of response speed 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
9 
 
between e-mail and mail, Tse et al. (1995) did not find a statistically significant 
difference in the time between sending and receipt of an e-mail survey to university 
faculty and staff and an equivalent survey sent by mail.    Furthermore, to achieve 
sufficiently high response rates, it may be necessary to leave an Internet-based survey in 
the field for an extended period of time.  For example, a prominent commercial Internet 
survey company, Knowledge Networks, has indicated that to achieve 70-80 percent 
response rates they must leave a survey in the field for about 10 days.  This period 
comprises one workweek with two weekends, because they find that most respondents 
complete their surveys on the weekend. 
However, there are cases in the literature that did show more timely response.  Tse 
(1998) found a statistically significant difference in the average initial response time for 
those that received an e-mail survey compared to those that received a paper survey in 
the campus mail (one day versus 2-1/2).  Further, in Tse’s experiment, most e-mail 
survey recipients either responded almost immediately (within one day) or they did not 
respond at all, which raises the question of the effectiveness of non-response follow-up in 
the electronic forum.  Schaefer and Dillman (1998) also document faster e-mail response 
rates: 76 percent of all responses were received in 4 days or less. Pealer et al. (2001) 
found a statistically significant difference in the average return time between their e-mail 
study arm (7.3 days) and their mail study arm (9.8 days).  However, the final e-mail 
survey was received after 24 days and the final mail survey after 25 days—a negligible 
difference in overall fielding time. 
In conclusion, while it is certainly reasonable to conclude prima facie that the 
delivery time of an Internet-based survey is faster than the delivery of a survey by mail, it 
does not necessarily follow that the increased delivery speed will translate into a 
significantly shorter survey fielding period.  Two points are relevant: (1) dramatic 
improvements are only possible with an all-electronic process, which is currently only 
possible for specialized populations; and, (2) even for populations in which all-electronic 
surveys are possible, the literature is not very informative as there is no information 
available about the length of fielding time required to achieve particular response rates. 
Quality 
When the primary purpose of a survey is to gather information about a population, 
the information is useless unless it is accurate and representative of the population.  
While survey error is commonly characterized in terms of the precision of statistical 
estimates, a good survey design seeks to reduce all types of errors, including coverage, 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
10 
 
sampling, non-response, and measurement errors.  (See Groves, 1989, for a detailed 
discussion of the “Total Survey Error” approach.)  Indeed, even when a survey is 
conducted as a census, the results still may be affected by many of these sources of error.   
Coverage error is the most widely recognized shortcoming of Internet-based 
surveys.  Today the general population coverage for Internet-based surveys still 
significantly lags behind the coverage achievable using conventional survey modes.  
However, there are some important caveats to keep in mind.  First, the coverage 
differential is rapidly closing and may become immaterial in the relatively near future 
(though this is far from a preordained conclusion).  Second, even though conventional 
modes have the ability to reach most of the population, it is becoming increasingly 
difficult to get people to respond (e.g., answering machines are routinely used to screen 
calls these days and, hence, screen out telephone surveyors and solicitors).  Third, while 
conventional modes have near universal coverage, there will always be special sub-
populations that have little or no coverage for any mode.  Fourth, in the case of Internet-
based surveys, access is only one consideration.  Even if the respondent in principle has 
Internet access (e.g. through a library), there are large portions of the population that are 
still computer illiterate and would have difficulty correctly responding to such a survey.  
Finally, access and computer literacy are necessary but not sufficient conditions for 
success: Respondents must also have compatible hardware and software.   
However, less than universal access to the Internet can be immaterial for some 
studies, such as studies that focus on closed populations with equal access or Internet 
users, for example.  In order to improve coverage, Dillman (2000) recommends a mixed-
mode strategy for contact, using both e-mail and postal mail for pre-notification.  
Similarly, using mixed response modes, such as Web and e-mail can be used to increase 
coverage.  However, as we previously mentioned, there is little evidence in the literature 
that concurrent mixed mode fielding increases response rates over what would have been 
achieved using a single, conventional mode. 
In addition to coverage, data quality is a function of a number of other dimensions, 
including: (1) unit and item nonresponse; (2) honesty of responses, particularly for 
questions of a sensitive nature; (3) completeness of responses, particularly for open-
ended questions; and, (4) quality of data transcription into an electronic format for 
analysis if required by the survey mode.  
All other things held constant (such as pre-notification and non-response follow-
up), unit and item non-response are generally smaller using interviewer-assisted modes 
(de Leeuw, 1992) compared to self-administered survey modes.  Face-to-face interviews 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
11 
 
have long been considered the gold standard of surveys and tend to result in the lowest 
unit and item non-response as well as minimizing respondent misinterpretation of 
questions and skip patterns.  However, it has been shown that interviewer-administered 
survey modes, particularly face-to-face, yield more socially desirable answers than self-
administered modes (de Leeuw, 1992, Kiesler et al., 1986, p. 409). This is particularly 
relevant for surveys of sensitive topics or for surveys that contain sensitive questions, 
such as questions about income or sexual practices, for example.  Mail and other self-
administered modes tend to be the least expensive but often have higher unit and item 
non-response rates.  On the other hand, they tend to elicit the most accurate responses to 
sensitive questions. 
Data quality is usually measured by the number of respondents with missing items 
or the percentage of missing items.  For open-ended questions, longer answers are 
usually considered more informative and of higher quality.  In those studies that 
compared e-mail versus mail, for closed-ended questions, it appears that e-mail surveys 
may incur a higher percentage of items missing than mail surveys.  As Table 4 shows, for 
studies in the literature that reported the percentage of missed items, the percentage for 
mail respondents was less than or equal to the percent for e-mail respondents.  
 
Table 4.  Average Percentage of Missed Items for E-mail and Postal Mail Surveys 
 
Study  
 
E-mail  
Postal 
Mail 
 
Population 
Peale et al (2001) 14.2 14.2 Undergraduates, University of Florida 
Bachman at al. (1996) 3.7 0.7 Business school deans and chairpersons 
Comley (1996)a 1.2 0.4 Names and addresses purchased from 
Internet magazine in the U.K. 
Paolo et al. (2000) 1.2 0.5 Fourth-year medical students 
Couper et al. (1999)b 0.8 0.8 Employees of five U.S. federal agencies 
Mehta and Sivadas (1995)c < 0.3 < 0.3 Active U.S. users of bulletin board system 
(BBS) news group 
a Based on three questions. 
b Based on 81 attitude questions. 
c Across five different study arms, one of which allowed for both mail and e-mail responses.  
 
At the respondent level, Paolo et al. (2000) also found that 27 percent of e-mail 
respondents did not respond to at least one question versus 9 percent for mail 
respondents.  Kiesler and Sproull (1986) found the opposite: in the e-mail (contact and 
response) study arm only 10 percent of respondents failed to complete or spoiled one 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
12 
 
item compared to 22 percent in the mail (contact and response) study arm.  Tse (1995, 
1998) found no difference in quality of responses.  
For open-ended questions, studies found that e-mail responses are either longer or 
of the same length as mail responses.  Comley (1996) found that in the two open-ended 
questions e-mail respondents gave longer answers.  One respondent even wrote a mini-
essay.  Mehta and Sivadas (1995) found that there was “hardly any difference between 
the average completed responses for both the open and close-ended questions” (Mehta 
and Sivadas, 1995, p. 436).  Kiesler and Sproull  (1986) found that the total number of 
words did not significantly differ for e-mail and mail respondents.  If one also takes into 
consideration that open-ended items for mail respondents are not always encoded for cost 
reasons, it appears that Internet-based survey modes may be better suited to open-ended 
questions. 
Other quality issues for Internet-based surveys resulting from some sort of 
sampling error are generally the same as for conventional surveys.  However, as the 
Internet becomes more ubiquitous, collecting much larger samples becomes more 
feasible.  Indeed, we have talked to some organizations recently that have electronic 
access to their entire population and are considering eliminating sampling and simply 
conducting censuses.  Often these census efforts result in much larger numbers of 
respondents than otherwise could have been gathered using traditional survey sampling 
techniques and those larger numbers give the appearance of greater statistical accuracy.  
However, such accuracy may be misleading if non-response biases are not accounted for 
and researchers need to carefully consider the trade-offs between smaller samples that 
allow for careful non-response follow-up and larger samples with less or no follow-up.  
The former may have larger standard errors but less bias while the latter may have much 
smaller standard errors but an unknown, and potentially very large, amount of bias.   
Finally, we note that Web surveys offer the ability to clearly improve on other 
forms of self-administered surveys in terms of data validation, skip pattern automation, 
and the elimination of transcription errors, all of which help to minimize measurement 
error.  Web surveys can be programmed to conduct input validation as a logical check of 
the respondent’s answers.  These types of checks improve data quality and subsequently 
save time in the preparation of the analysis file.  As with logic checks, Web surveys can 
also be programmed to manage the process of skipping questions.  This will eliminate 
errors and, from the respondent’s point of view, simplify the process of taking the survey.  
And, while all conventional surveys required some form of conversion into an electronic 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
13 
 
format for analysis, for Web surveys respondents’ answers are directly downloaded into a 
database, avoiding transcription errors.   
Cost 
Designing a survey fundamentally involves making trade-offs between the quality 
and quantity of data and cost.  For smaller research surveys that are not subsidized in any 
way, a major component of total survey cost is frequently the researchers’ time for survey 
design and subsequent data analysis.  However, these costs vary little by survey mode.  A 
major expense that does vary by mode is the labor cost of the personnel who actually 
execute the survey.  Depending on the size of the survey and the complexity of the 
design, either researcher labor costs, survey personnel labor costs, or a combination of 
the two will likely dominate the survey budget.  
Comparing the costs of doing a Web survey versus a mail survey or other some 
other mode in the literature is difficult because different authors define costs different 
ways.  Academics frequently only consider postage and reproduction costs and often fail 
to account for the cost of one or more of various types of labor, including: survey design 
and/or programming, coding, analysis, and other such items.  Estimates also vary 
depending on whether they are given on a per mail-out or per complete survey response 
basis and, unfortunately, most studies in the literature omit any discussion about costs 
altogether.  However, the question often reduces to how to price the time spent 
programming a Web survey and whether and how to price the time of the investigator or 
a survey coordinator. 
While lower costs are often touted as one of the benefits of Internet-based surveys, 
Couper et al. (1999) found no cost benefit in e-mail compared to postal mail surveys in 
their work.  In a large and comprehensive survey effort of different government agencies 
Couper et al. compared an all e-mail survey (contact, response, and follow-up) versus an 
all mail survey.  They found that evaluating and testing the e-mail software took over 150 
hours - almost 4 times as much as they budgeted.  For the mail survey, costs for printing 
and postage were $1.60 per reply; and data editing and entry cost about $1.81.  For the e-
mail survey, managing the e-mail cost $1.74 per completed case.  In addition, they 
handled over 900 toll-free calls of a mostly technical nature.  While the printing and 
mailing costs were eliminated for the e-mail survey, Couper et al. found that the costs of 
evaluating and testing the e-mail software, additional post-collection processing, and the 
costs of maintaining a toll-free phone line which was largely dedicated to responding to 
technical questions related to the e-mail surveys offset any savings.  (For example, while 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
14 
 
the e-mail survey was designed so that respondents would use the reply function of their 
e-mail program so the resulting replies could be automatically read into a database upon 
receipt.)  Further, almost 47 percent of the e-mail surveys required some type of clerical 
action to prepare them for automatic reading. 
On the other hand, Raziano et al. (2001) in a small study of 110 Geriatric Chiefs 
across the U.S., compute the cost per respondent for their mail study arm to be $7.70 and 
for their e-mail study arm $10.50.  The programming time to construct the e-mail survey 
is factored into this calculation.  However, the total programming time accounted for, two 
hours, may be unrealistic for a large or complicated survey operation.  Also, these 
estimates fail to reflect the fact that their postal arm response rate from the first mail-out 
exceeded the e-mail arm response rate after four contact attempts.  Hence, for a given 
desired response rate, the difference in costs would be less as fewer mailings would be 
required.  Similarly, Schleyer and Forrest (2000) in their study received responses over 
the Web, by mail, and by fax and found the total costs for the Web survey turned out to 
be 38 percent lower than for the equivalent mail survey. 
Asch (as reported in Schonlau et al., 2002) found that adding a Web response 
option to a mail survey to be economical when about 620 responses are obtained over the 
Web when the Web is first used as the primary survey mode and surveys are only mailed 
out to non-respondents.  Their calculations were based on the trade-off of the expected 
savings in postage, printing, and labor costs to prepare survey mailing packages and code 
the subsequent survey returns against the expected extra costs of programming, 
additional management effort, and maintaining a telephone help-line for the Web survey.  
This study did achieve a cost savings since it garnered over 1,000 Web responses.  
In two studies that essentially ignore personnel costs, Mehta and Sivadas (1995) 
and Jones and Pitt (1999) conclude, not surprisingly, that Internet-based surveys are less 
costly than mail surveys.  These conclusions simply stem from the fact that Internet-
based surveys do not incur postage and printing costs while mail surveys do.   
In conclusion, when only considering postage and printing costs, e-mail and Web 
surveys almost by definition are cheaper than mail surveys.  However, when the total 
costs of a survey are considered, including labor and other costs, Web surveys may or 
may not be cheaper depending on whether the additional expenses incurred with that 
mode, such as programmer costs, are offset by savings, such as postage and data entry 
costs.  When planning for and subsequently executing a Web survey, care must be taken 
that unanticipated technical problems are minimized or these problems can easily 
eliminate all potential cost benefits.   
Field Methods, Vol. 14 No. 4, 2002 347-367. 
15 
 
SUMMARIZING THE CURRENT PERFORMANCE OF INTERNET SURVEYS 
In the Introduction we said that Internet-based surveys are in vogue – those 
conducted via the Web in particular – primarily because of three assumptions: (a) Web 
surveys are much cheaper to conduct; (b) Web surveys are faster; and, (c) combined with 
other survey modes, Web surveys yield a higher response rate than the other survey 
modes by themselves.  That is, the usual naïve generalization about Internet-based 
surveys is that they can be conducted faster, better, cheaper, and easier than surveys 
conducted via conventional methods.  How do these claims stand up when compared to 
what has been published in the literature? 
Faster?  
Web surveys are thought to be much faster than conventional survey modes.  
While there is no question that the delivery time of an Internet-based survey is faster than 
a survey sent via the mail, there is little to no evidence in the literature to substantiate 
whether this increase subsequently results in a shorter overall fielding period.  We are 
aware of a couple of organizations that have implemented all-electronic survey processes 
by communicating with respondents via e-mail, but this is only currently possible for pre-
recruited panels or specialized subsets of the population.  If respondents must be 
contacted through mail or phone, which generally is the case if a probability sample is 
required by the research, then there may only be a marginal improvement in overall 
response times.   
Better? 
Response rates for Web surveys where no other survey mode is given have tended 
to range from moderate to poor.  The reasons for this are not clear.  It is possible that 
potential respondents simply do not respond as well to electronic solicitation or response.  
If true, this may improve as Internet-based communication methods continue to spread 
and become routine with all segments of the general population.  It is also possible that 
the execution of the Internet-based survey experiments have been less than optimal – 
something that will improve with surveyor experience.   
There are a few examples of Web surveys outperforming mail in some of the more 
recent comparisons between these two media.  Whether this was a unique result for these 
few surveys, or whether it is a leading indicator that the field is maturing and learning 
how to achieve higher response rates in the new medium is not known.  In either case, it 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
16 
 
is of concern that any improvements in these areas may be offset by over-saturation of 
the population with other forms of commercial surveys.  
Setting the question of response rate aside, Web surveys offer some advantages 
over conventional modes.  For example, if multi-media and/or interactive graphics are 
required then there are few conventional alternatives (and those alternatives, such as 
face-to-face interviewing, would likely be significantly more costly).  If a convenience 
sample will suffice for the research, then Web can be an excellent medium to use, 
particularly if the desired respondents are geographically diverse or hard to find/identify. 
A major issue for Web surveys is their ease of implementation facilitates naïve 
misuse.  The particular concern for this medium is the easy collection of large numbers of 
surveys can result in surveyors and survey data consumers confusing quantity with 
quality.  There is on-going research about the effects of surveying via the Internet, the 
Web in particular, on unit and item non-response and on the affect the medium has on 
survey responses.  Preliminary results have been reported at some conferences and 
symposia, but little has appeared in the literature as yet. 
Cheaper? 
The usual claim that Web surveys are much cheaper than mail surveys is not 
necessarily true.  Web and e-mail surveys can save on some or all mailing costs, but 
except for very large surveys these may be small costs in the overall survey effort.  Web 
surveys can also eliminate data entry costs; e-mail survey results may not because they 
often require additional manipulation before they can be downloaded into an analytical 
database.  However, savings in data entry may be partially or completely offset against 
higher programming costs and additional help desk staffing requirements.  The literature 
mostly neglects labor costs, which form the highest cost component for Web surveys.  
Nonetheless, adding a Web survey to a mail survey can be cost efficient if done carefully 
and properly.  
Easier? 
The implementation of Web surveys is technically more involved than mail or 
phone surveys.  Survey designers need to specify many issues related to the technical 
control of Web surveys (e.g. how to move back and forward between questions, input 
validation, passwords, for what questions answers are not optional) that are simpler or 
not required with conventional survey modes.  Web surveys also require more extensive 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
17 
 
pretesting to ensure both that the questions elicit the desired information and that the 
program works properly across numerous hardware and software configurations. 
The fielding process may or may not be made easier.  Internet-based surveys have 
the potential to eliminate some of the more labor-intensive fielding tasks, such as survey 
package preparation and mailing and the subsequent data entry.  Yet, if mixed modes are 
required to obtain sufficient population coverage and/or response rates, then these tasks 
cannot be completely eliminated and the fielding process may actually then become more 
complex since support for two or modes must be maintained and managed. 
What is the Future of Internet-based Surveying? 
The first Internet browser was introduced only about a decade ago and early use of 
the World Wide Web as a survey medium only started about five years ago.  The result is 
that significant research results about the use of this new survey medium have only 
recently begun to become available in the literature.  Hence, there is a great deal that is 
still not well known about Internet-based surveys.  While some predict that Web surveys 
will replace other survey modes, we expect Web surveys to develop into a distinct survey 
mode with advantages and disadvantages that will have to be weighed against the 
conventional alternatives. 
Little is known about Web instrument design and the effects of instrument design 
on how survey participants respond to a survey or a particular survey question, and what 
enhances response rates and response accuracy.  For example, at the 2001 American 
Association of Public Opinion Researchers conference, some anecdotal evidence was 
presented that respondents taking surveys on the Web had shorter attention spans, 
tending to browse the survey like they browse other Web sites.  If true, this would 
suggest that long surveys and/or surveys with complex questions may not perform as well 
on the Web as by mail.  While many of the design principles from paper-based surveys 
may translate to Internet-based surveys, much more research is required. 
To date, most Web surveys have been conducted on convenience samples or in 
organizations where a list of target populations readily exists.  However, Internet-based 
surveys with probability samples can be fielded by using the mail or telephone for 
respondent contact and the Web for response.  There is currently no equivalent to random 
digit dialing for e-mail.  Even though the fraction of the population having access to e-
mail will continue to grow, it is unlikely that one will ever be able to construct a random 
e-mail address in the same way a random telephone number is constructed.  However, 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
18 
 
large commercial e-mail lists may yet emerge that are of high enough quality to be useful 
in survey research. 
A major challenge for researchers will be to distinguish themselves and their 
survey from the plethora of commercial and entertainment surveys that exist and continue 
to multiply on the Web.  These other surveys will continue to proliferate because the 
financial and technical barriers are so low for Web surveys.  Thus, just as telephone 
survey response rates have continued to decline because of telemarketers, it is likely to 
become increasingly difficult to achieve superior response rates in the new medium.   
Nonetheless, Internet-based surveys are here to stay.  The challenge for researchers 
is to learn to use the new medium to their best advantage.   
REFERENCES 
Asch, B., (2001). RAND, Santa Monica, California.  Personal communication. 
Bachman, E., J. Elfrink, and G. Vazzana (1996). Tracking the Progress of E-Mail vs. 
Snail-Mail, Marketing Research, 8, 31-35. 
Bradley, N. (1999). Sampling for Internet Surveys.  An Examination of Respondent 
Selection for Internet Research, Journal of the Market Research Society, 41, 387-
395. 
Cochran, W.G. (1977).  Sampling Techniques, 3rd edition, John Wiley & Sons, New 
York, NY. 
Comley, P. (1996). Internet Surveys. The Use of the Internet as a Data Collection 
Method, ESOMAR/EMAC: Research Methodologies for “The New Marketing," 
Symposium ESOMAR Publication Services, vol. 204, 335-346. 
Coomber, R. (1997).  Using the Internet for Survey Research, Sociological Research 
Online, 2, 14-23. 
Couper, M. (2000).  Web Surveys, A Review of Issues and Approaches, Public Opinion 
Quarterly, 64, 464-494. 
Couper, M.P., J. Blair and T. Triplett (1999).  A Comparison of Mail and E-mail for a 
Survey of Employees in U.S. Statistical Agencies.  Journal of Official Statistics, 
15, 39-56. 
Couper, M.P., M.W. Traugott, M.J. Lamias (2001). Web Survey Design and 
Administration. Public Opinion Quarterly, 65, 230-253. 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
19 
 
de Leeuw, E.D. (1992).  Data Quality in Mail, Telephone, and face to Face Surveys, 
Ph.D. dissertation, University of Amsterdam, ISBN 90-801073-1-X. 
Dillman, D.A. (2000).  Mail and Internet Surveys, The Tailored Design Method, 2nd ed., 
John Wiley & Sons, New York, NY. 
Dillman, D.A., R.D. Tortora, J. Conradt and D. Bowerk (1998). Influence of Plain vs. 
Fancy Design on Response Rates for Web Surveys. Unpublished paper presented 
at the Annual Meeting of the American Statistical Association, Dallas, TX. 
Dillman, D.A. (1978).  Mail and Telephone Surveys, The Total Design Method, John 
Wiley & Sons, New York, NY. 
Everingham, S. (2001).  RAND, Santa Monica, California.  Personal communication. 
Fowler, Jr., F.J. (1993). Survey Research Methods, 2nd ed., Applied Social Science 
Research Methods Series, volume 1, SAGE Publications, Newbury Park, CA.  
Griffin, D.H., D.P. Fischer, and M.T. Morgan (2001).  Testing an Internet Response 
Option for the American Community Survey.  Paper presented at the American 
Association for Public Opinion Research, Montreal, Quebec, Canada. 
Groves, R. (1989). Survey Errors and Survey Costs, John Wiley & Sons, New York, NY.  
Hamilton, C.H. (2001).  Air Force Personnel Center, Randolph Air Force Base, personal 
communication. 
Henry, G.T. (1990).  Practical Sampling, Applied Social Research Methods Series, 
Volume 21, SAGE Publications, Newbury Park, CA. 
Jones, R. and N. Pitt (1999).  Health Surveys in the Workplace: Comparison of Postal, 
Email and World Wide Web Methods, Occupational Medicine, 49, 556-558. 
Kiesler, S. and L.S. Sproull (1986). Response Effects in the Electronic Survey, Public 
Opinion Quarterly, 50, 402-413. 
Kish, L. (1965).  Survey Sampling, John Wiley and Sons, New York, NY. 
Kittleson, M.J. (1995). An Assessment of the Response Rate Via the Postal Service and 
E-Mail, Health Values, 18, 27-29. 
McCabe, S.E., Boyd, C., Couper, M.P., Crawford, S., and H. d'Arcy (2002).  Mode 
Effects for Collecting Health Data from College Students: Internet and US Mail.  
Paper under review. 
Mehta, R. and E. Sivadas (1995). Comparing Response Rates and Response Content in 
Mail versus Electronic Mail Surveys, Journal of the Market Research Society, 37, 
429-439. 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
20 
 
Nichols, E., and B. Sedivi (1998).  Economic Data Collection via the Web: A Census 
Bureau Case Study Proceedings of the Section On Survey research Methods, 
American Statistical Association,366-371. 
Paolo, A.M., Bonaminio, G.A., Gibson, C., Partridge, T. and K. Kallail (2000). Response 
Rate Comparisons of e-mail and mail distributed student evaluations, Teaching and 
Learning in Medicine, 12,  81-84. 
Parker, L. (1992). Collecting Data the E-Mail Way, Training and Development, July, 52-
54. 
Pealer, L., R.M. Weiler, R.M. Pigg, D. Miller, and S.M. Dorman (2001). The Feasibility 
of a Web-Based Surveillance System to Collect Health Risk Behavior Data From 
College Students. Health Education & Behavior, 28, 547-559. 
Quigley, B., Riemer, R.A., Cruzen, D.E., and S. Rosen (2000). Internet Versus Paper 
Survey Administration: Preliminary Finding on Response Rates, 42nd Annual 
Conference of the International Military Testing Association, Edinburgh Scotland. 
Raziano, D.B., R. Jayadevappa, D. Valenzula, M. Weiner, and R. Lavizzo-Mourey 
(2001). E-mail Versus Conventional Postal Mail Survey of Geriatric Chiefs. The 
Gerontologist, 41, 799-804. 
Schaefer, D.R. and D.A. Dillman (1998). Development of a Standard E-mail 
Methodology: Results of an Experiment.  Public Opinion Quarterly, 62, 378-397. 
Schleyer, T.K.L. and J.L. Forrest (2000). Methods for the Design and Administration 
Web-Based Surveys, Journal of the American Medical Informatics Association, 7, 
416-425 
Schillewaert, N., F. Langerak and T. Duhamel (1998). Non-probability Sampling for 
WWW Surveys: A Comparison of Methods, Journal of the Market Research 
Society, 40, 307-322. 
Schonlau, M., Fricker, R.D., Jr., and M. Elliott. (2002).  Conducting Research Surveys 
via E-Mail and the Web, RAND: Santa Monica, MR-1480-RC. 
Schuldt, B.A. and J.W. Totten (1994). Electronic Mail vs. Mail Survey Response Rates, 
Marketing Research, 6, 36-44.  
Sedivi Gaul, B. (2001a).  Web Computerized Self-administered Questionnaires (CSAQ).  
Presentation to the 2001 Federal CASIC Workshops.  U.S. Census Bureau, 
Computer Assisted Survey Research Office. 
Sedivi Gaul, B. (2001b).  United States Census Bureau, Washington, D.C.  Personal 
Communication. 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
21 
 
Sheehan, K.B. (2001).  E-mail survey response rates: A review. Journal of Computer-
Mediated Communication, 6(2). Retrieved March 9, 2002, from 
http://www.ascusc.org/jcmc/vol6/issue2/sheehan.html. 
Tse, A.C.B., Tse, K.C., Yin, C.H., Ting, C.B., Yi, K.W., Yee, K.P., and W.C. Hong 
(1995).  Comparing Two Methods of Sending Out Questionnaires: E-mail versus 
Mail, Journal of the Market Research Society, 37, 441-446. 
Tse, A.C.B. (1998).  Comparing the Response Rate, Response Speed and Response 
Quality of Two Methods of Sending Questionnaires: E-mail versus Mail, Journal 
of the Market Research Society, 40, 353-361. 
Tuten, T.L., D.J. Urban, and M. Bosnjak (in press, 2002). “Internet Surveys and Data 
Quality: A Review” in: B. Batinic, U. Reips, M. Bosnjak, A.Werner, eds., Online 
Social Sciences, Hogrefe & Huber, Seattle, 7-27. 
Vehovar, V., K. Lozar Manfreda, and Z. Batagelj (1999). Web Surveys:  Can the 
Weighting Solve the Problem?  Proceedings of the Section on Survey Research 
Methods, American Statistical Association, Alexandria, VA, 962-967.  
Vehovar, V., K. Lozar Manfreda, and Z. Batagelj (2001). Sensitivity of e-commerce 
Measurement to the Survey Instrument. International Journal of Electronic 
Commerce, 6, 31-51. 
Walsh, J.P., S. Kiesler, L.S. Sproull, and B.W. Hesse (1992). Self-Selected and 
Randomly Selected Respondents in a Computer Network Survey, Public Opinion 
Quarterly, 56, 241-244. 
Witte, J.C., L.M. Amoroso, and P.E.N. Howard (2000).  Research Methodology – 
Method and Representation in Internet-based Survey Tools, Social Science 
Computer Review, 18, 179-195. 
Zhang, Y. (2000). Using the Internet for Survey Research:  A Case Study, Journal of 
Education for Library and Information Science, 5, 57-68.   
 
Ron Fricker is a statistician at RAND.  He has designed, managed, and analyzed many 
large surveys of national importance, including a survey of Persian Gulf War veterans 
about Gulf War Illnesses and, most recently, a survey on domestic terrorism preparedness 
in the United States.  Dr. Fricker holds Ph.D. in Statistics from Yale University.  In 
addition to his position at RAND, Dr. Fricker is the vice-chairman of the Committee on 
Statisticians in Defense and National Security of the American Statistical Association, an 
associate editor of Naval Research Logistics, and an adjunct assistant professor at 
University of Southern California. 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
22 
 
 
Matthias Schonlau, Ph.D., is an associate statistician with RAND and heads its 
statistical consulting service.  Dr. Schonlau has extensive experience with the design and 
analysis of surveys in areas such as health care, military manpower and terrorism. 
Prior to joining RAND, he held positions with the National Institute of Statistical 
Sciences and with AT&T Labs Research.  Dr. Schonlau has co-authored numerous 
articles as well as a recent RAND book “Conducting Internet Surveys via E-mail and the 
Web.”  In 2001, he and his team won second place in the data mining competition at the 
world's largest conference on data mining “KDD.”   
 
  
 
Acknowledgements.  The helpful and substantive comments of three anonymous 
reviewers and the editor significantly improved this work.  Our research was supported 
by RAND as part of its continuing program of independent research. 
Field Methods, Vol. 14 No. 4, 2002 347-367. 
23 

