Default Logic and Bounded Treewidth∗
Johannes K. Fichte†, Markus Hecher†, and Irene Schindler
TU Wien, Austria and Leibniz Universität Hannover, Hannover, Germany
lastname@dbai.tuwien.ac.at
Abstract
In this paper, we study Reiters propositional default logic when the treewidth of a certain graph representa-
tion (semi-incidence graph) of the input theory is bounded. We establish a dynamic programming algorithm
on tree decompositions that decides whether a theory has a consistent stable extension or can even be used
to enumerate all generating defaults that lead to stable extensions. We show that, for input theories whose
semi-incidence graph has bounded treewidth, our algorithm decides whether a theory has a stable extension in
linear time and enumerates all characteristic generating defaults with linear delay.
1 Introduction
Reiter’s default logic (DL) is one of the most fundamental formalisms to non-monotonic reasoning where
reasoners draw tentative conclusions that can be retracted based on further evidence [16, 13]. DL augments
classical logic by rules of default assumptions (default rules). Intuitively, a default rule expresses “in the
absence of contrary information, assume . . . ”. Formally, such rule is a triple P :JC of formulas P , J , and C
expressing “if prerequisite P can be deduced and justification J is never violated then assume conclusion C”.
For an initial set of facts, beliefs supported by default rules are called an extension of this set of facts. The
default rules must not lead to an inconsistency if conflicting rules are present, instead such rules should be
avoided. If the default rules can be “fired” consistently until a fixed point, the extension is a maximally
consistent view (consistent stable extension) with respect to the facts together with the default rules. In
DL stable extensions involve the construction of the deductive closure, which can be generated from the
conclusions C of the defaults and the initial facts by means of so-called generating defaults. If a generating
default also leads to a consistent stable extension, we call it a characteristic generating default. Even
though formulas in DL can be used for various classical logics, we consider only propositional formulas
in conjunctive normal form (cnf). Our problems of interest are deciding whether a default theory has a
consistent stable extension (Ext), counting the number of characteristic generating defaults (CompSE),
and enumerating all generating defaults (EnumSE). The computational complexity of various problems
arising in default logic has been subject of several studies. Gottlob [11] established that the problem Ext
is Σp2-complete and thus are of high intractability. Beyersdorff et al. considered the complexity of fragments
for default logic in the sense of Post’s lattice [1].
Parameterized algorithms [8] have attracted considerable interest in recent years and allow to tackle
hard problems by directly exploiting certain structural properties present in input instances (the parameter).
When conducting a complexity analysis the computational complexity of the problem is then considered
in dependency of the parameter. Problems where the complexity drops and where we have an algorithm
that solves the problem in polynomial time when the parameter is fixed are of particular interest. Such
problems are called fixed-parameter tractable and fixed-parameter linear, if the runtime bounds are linear in
∗This is the authors self-archived copy including detailed proofs.
†Also affiliated with the Institute of Computer Science and Computational Science at University of Potsdam, Germany.
1
ar
X
iv
:1
70
6.
09
39
3v
1 
 [
cs
.A
I]
  2
8 
Ju
n 
20
17
the program size. Recently, a parameter for DL has been gained from backdoors [10] and fixed-parameter
tractability results have been established for the problem Ext. Another parameter is treewidth, which has
been researched extensively [4] and widely applied in knowledge representation and reasoning [5, 9, 12, 17].
Treewidth is the parameter of our interest. A classical way to establish fixed-parameter tractability results
when the problem is parameterized by the width of a given TD is to encode the problem in monadic second
order logic and apply Courcelle’s theorem [6]. In that way, Meier et al. [14] established that the extension
existence problem Ext is fixed-parameter tractable when parameterized by treewidth of a certain graph
representation of the default theory (incidence graph). However, in many settings dynamic programming
on tree decompositions is more feasible when we are not just interested in the complexity result, but also
aim to implement and use the algorithm in a practical setting [2, 3, 5, 9].
Contributions. In this paper, we introduce dynamic programming algorithms on tree decompositions
to solve the problems Ext and EnumSE. Our algorithms are fixed-parameter linear and fixed-parameter
linear for a pre-computation step followed by linear delay for outputting the solutions (Output-FPT [7]),
respectively.
2 Formal Background
We assume familiarity with standard notions in computational complexity, the complexity classes P and
NP as well as the polynomial hierarchy. For more detailed information, we refer to other standard sources .
For parameterized (decision) problems we refer to work by Cygan et al. [8].
A literal is a propositional variable or its negation and a clause is a finite set of literals. A CNF formula
is a finite set of clauses. A (truth) assignment is a mapping θ : X → {0, 1} defined for a set X of variables.
For x ∈ X we put τ(¬x) = 1− τ(x). By A(X) we denote the set of all truth assignments θ : X → {0, 1}.
The truth assignment reduct of a CNF formula F with respect to θ ∈ A(X) is the CNF formula Fθ obtained
from F by first removing all clauses c that contain a literal set to 1 by θ, and second removing from the
remaining clauses all literals set to 0 by θ. θ satisfies F if Fθ = ∅, and F is satisfiable if it is satisfied by
some θ. Let F and G be CNF formulas and X = Vars(F ) ∪ Vars(G). We write F  G if and only if for
all assignments θ ∈ A(X) it holds that all assignments θ that satisfy F also satisfy G. Further, we define
the deductive closure of F as Th(F ) := {G ∈ cnf | F  G } and let for formula F and a family M of
sets of variables be Mod(M, F ) = {M |,M ∈ M,M  F )}. We denote with Sat the problem, given a
propositional formula F asking whether F is satisfiable. The problem Taut is defined over a given formula
F asking whether F tautological.
We define for CNF formulas P , J , and C a default rule δ as a triple P :JC ; P is called the prerequisite, J
is called the justification, and C is called the conclusion; we set α(d) := P , β(d) := J , and γ(d) := C. We
follow the definitions by Reiter [16]. A default theory 〈W,D〉 consists of a set W of propositional CNF
formulas (knowledge base) and a set of default rules. A default theory D is a set of default rules. Let
〈W,D〉 be a default theory and E be a set of formulas. Then, Γ(E) is the smallest set of formulas such
that: (i) W ⊆ Γ(E) (ii) Γ(E) = Th(Γ(E)), and (iii) for each α:βγ ∈ D with α ∈ Γ(E) and ¬β /∈ E, it holds
that γ ∈ Γ(E). E is a stable extension of 〈W,D〉, if E = Γ(E). An extension is inconsistent if it contains ∅,
otherwise it is called consistent. The definition of stable extensions allows inconsistent stable extensions.
However, inconsistent extensions only occur if the set W is already inconsistent where 〈W,D〉 is the theory
of interest [13, Corollary 3.60]. In consequence, (i) if W is consistent, then every stable extension of 〈W,D〉
is consistent, and (ii) if W is inconsistent, then 〈W,D〉 has a stable extension. For Case (ii) the stable
extension consists of all formulas. Thus, we consider only consistent stable extensions. For default theories
with consistent W , we can trivially transform every formula in W into a default rule. Hence, in this paper
we generally assume that W = ∅ and write a default theory simply as set of default rules. Moreover, we
refer by SE(D) to the set of all consistent stable extensions of D.
A definition of stable extensions beyond fixed point semantics, which has been introduced by Reiter [16]
as well, uses the principle of a stage construction.
2
ljust,d2 ljust,d1 a
b
d2lcon,d2
lpre,d2
d1
lpre,d1
lcon,d1
{a, d1, ljust,d1}t1
{a, d1, lpre,d1}t2
{a, b, d1, lcon,d1}t3
{a, d2, lpre,d2} t4
{a, d2, ljust,d2} t5
{a, b, d2, lcon,d2} t6
{a, b} t7
Figure 1 Graph G (left) and an LTD T (right) of G.
Proposition 1 (Stage construction, [16]). Let D be a default theory and E be a set of formulas. Then
define E0 := ∅ and Ei+1 := Th(Ei) ∪ { γ(d) | α(d) ∈ Ei,¬β(d) /∈ E, d ∈ D }. E is a stable extension of
〈W,D〉 if and only if E = ⋃i∈NEi. The set G = { d | α(d) ∈ E ∧ ¬β(d) /∈ E, d ∈ D } is called the set of
generating defaults. If E is a stable extension of D, then E = Th({ γ(d) | d ∈ G }). In that case, we call G
a characteristic generating default.
Example 1. Let the default theories D1 and D2 be given as
D1 :=
{
d1 =
{} : {a}
{a ∨ b} , d2 =
{} : {¬a}
{¬b}
}
,
D2 :=
{
d1 =
{c} : {a}
{a ∨ b} , d2 =
{c} : {¬a}
{¬b} , d3 =
{} : {c}
{c} , d4 =
{} : {¬c}
{¬c}
}
.
D1 has no stable extension, while D2 has only one stable extension E1 = {¬c} .
Given a default theory D we are interested in the following problems: The extension existence problem
(called Ext) asks whether D has a consistent stable extension. Ext is Σp2-complete [11]. The computation
problem (called CompSE) asks to output a characteristic generating default of D. The enumerating
problem asks to enumerate all characteristic generating defaults of D (called EnumSE).
Tree Decompositions. Let G = (V,E) be a graph, T = (N,F, n) a rooted tree, and χ : N → 2V a
function that maps to each node t ∈ T a set of vertices. We call the sets χ(·) bags and N the set of
nodes. Then, the pair T = (T, χ) is a tree decomposition (TD) of G if the following conditions hold: (i) for
every vertex v ∈ V there is a node t ∈ N with v ∈ χ(t); (ii) for every edge e ∈ E there is a node t ∈ N
with e ⊆ χ(t); and (iii) for any three nodes t1, t2, t3 ∈ N , if t2 lies on the unique path from t1 to t3, then
χ(t1) ∩ χ(t3) ⊆ χ(t2). We call max{|χ(t)| − 1 | t ∈ N} the width of the TD. The treewidth tw(G) of the
graph G is the minimum width over all possible TDs of G.
Note that each graph has a trivial TD (T, χ) consisting of the tree ({n}, ∅, n) and the mapping χ(n) = V .
It is well known that the treewidth of a tree is 1, and a graph containing a clique of size k has at least
treewidth k − 1. For arbitrary but fixed k we can compute a TD of a graph of width that equals its
treewidth in time 2O(k
3) · |V | [4]. Given a TD (T, χ) with T = (N, ·, ·), for a node t ∈ N we say that
type(t) is leaf if t has no children; join if t has children t′ and t′′ with t′ 6= t′′ and χ(t) = χ(t′) = χ(t′′);
int (“introduce”) if t has a single child t′, χ(t′) ⊆ χ(t) and |χ(t)| = |χ(t′)| + 1; rem (“removal”) if t has
a single child t′, χ(t) ⊆ χ(t′) and |χ(t′)| = |χ(t)| + 1. If every node t ∈ N has at most two children,
type(t) ∈ {leaf, join, int, rem}, and bags of leaf nodes and the root are empty, then the TD is called nice.
For every TD, we can compute a nice TD in linear time without increasing the width [4]. In our algorithms
we will traverse a TD bottom up, therefore, let post-order(T, t) be the sequence of nodes in post-order of
the induced subtree T ′ = (N ′, ·, t) of T rooted at t.
For the ease of presentation, we also require the notion of labelled tree decompositions (LTDs). Again,
let G = (V,E) be a graph and T = (T, χ) be a tree decomposition of G. Further, let L ⊆ V be a set of
labels. We call T a labelled tree decomposition, if (i) for each node t of T , we have |χ(t) ∩ L| = 1, and
(ii) whenever for some node t of T χ(t)∩L 6= ∅, then χ(t) ⊇ {v | (v, l) ∈ E, {l} = χ(t)∩L} as well. Assume
in the following, that we use labelled graphs and nice labelled TDs, unless mentioned otherwise.
3
Algorithm 1: Algorithm DPA(T ) for Dynamic Programming on TD T for DL, cf. [9].
In: Table algorithm A, nice TD T = (T, χ) with T = (N, ·, n) of G(D) according to A.
Out: Table: maps each TD node t ∈ T to some computed table τt.
1 for iterate t in post-order(T,n) do
2 Child-Tabs := {Tables[t′] | t′ is a child of t in T}
3 Tables[t] ← A(t, χ(t), Dt, at≤t,Child-Tabs)
Example 2. Figure 1 (left) depicts a graph G together with a LTD of width 3 of G. G has treewidth 3,
since there is a clique on the set {a, b, d1, lcon,d1} of vertices. Further, the LTD T in Figure 3 sketches main
parts of a nice LTD of G (obvious parts are left out).
Graph Representations of Default Theories. In order to use TDs for DL solving, we need dedicated
graph representations. These definitions adapt similar concepts from ASP [9]. For a default theory D, its
semi-incidence graph P (D) is the graph that has the atoms of D as vertices and an edge a b if there exists
a default d ∈ D and a, b ∈ at(d). The incidence graph I(G) is the bipartite graph, where the vertices are of
atoms of D and defaults d ∈ D, and there is an edge d a between a default d ∈ D and a corresponding
atom a ∈ at(d). In order to simplify the presentation of our algorithm, we mainly cover the semi-incidence
graph S(D) of D, where the vertices are atoms at(D), defaults of D and labels l(pre,d), l(just,d) and l(con,d)
for each default d ∈ D. For each default d ∈ D, we have edges l(pre,d) d, l(just,d) d and l(con,d) d, and a d
if atom a ∈ at(d) occurs in d. Moreover, there are edges a l(pre,d), a l(just,d), or a l(con,d) if atom a occurs
in the respective formula of d. Finally, we have an edge a b if either a, b ∈ at(α(d)), or a, b ∈ at(β(d)), or
a, b ∈ at(γ(d)). For sake of simplicity of the presentation, which we will see later, we require in addition
that in each bag of the decomposition two fresh vertices are present ε and bf, where ε represents an empty
default and bf represents a fail state.
Example 3. Recall default theory D1 of Example 1. We observe that graph G in the left part of Figure 1
is the semi-incidence graph of D1.
Sub-Theories. Let T = (T, χ) be a nice LTD of the semi-incidence graph S(D) of a default theory D.
Further, let T = (N, ·, n) and t ∈ N . The bag-defaults are defined as Dt := D ∩ χ(t). Moreover, the bag-
default parts αt, βt and γt contain the corresponding preconditions αt := α(D) ∩ χ(t), justifications βt :=
β(D) ∩ χ(t) and conclusions γt := γ(D) ∩ χ(t), respectively. Further, the set at≤t := {a | a ∈ at(D) ∩
χ(t′), t′ ∈ post-order(T, t)} is called atoms below t, the default theory below t is defined as D≤t := {d |
d ∈ Dt′ , t′ ∈ post-order(T, t)}, and the default theory strictly below t is D<t := D≤t \ Dt. It holds that
D≤n = D<n = D and at≤n = at(D). The former definitions naturally extend to the respective bag-default
parts, i.e., we also use α≤t, β≤t, and γ≤t.
Example 4. Intuitively, the LTD of Figure 1 enables us to evaluate D by analyzing sub-theories ({d1} and
{d2}) and combining results agreeing on a, b. Indeed, for the given LTD of Figure 1 (right), D≤t3 = {d1},
D≤t4 = {d2} and D = D≤t7 = D<t7 = D≤t3 ∪ D≤t6 . Moreover, at≤t7 = at(D), α≤t3 = {α(d1)} and
γ≤t7 = {γ(d1), γ(d2)}.
3 Computing Characterizing Generating Defaults
In this section, we present our dynamic programming (DP) algorithms to solve the problems Ext and
EnumSE. Our algorithms are inspired by earlier work [9] in answer set programming, but require significant
extensions due to much more evolved semantics in DL. Algorithm 1 (DPA) outlines the basis of our
algorithms and sketches the general dynamic programming scheme on TD for computing characterizing
generating defaults. Roughly, the algorithm splits the search space for a stage construction (see Proposition 1)
based on a given LTD and evaluates the input default theory D in parts. The results are stored in so-called
4
Algorithm 2: Table algorithm SINC(t, χt, Dt, at≤t,Child-Tabs).
In: Bag χt, bag-theory Dt and child tables Child-Tabs of node t. Out: Table τt.
1 cpyd(C, δ) := {〈ρ,MC,RC〉 | 〈ρ,MC,RC〉 ∈ C, ρ(d) 6= δ}
2 if type(t) = leaf then τt ← {〈∅, {∅}, {〈∅, ∅, ∅〉}, ∅〉} /* Abbreviations see Footnote 1. */
3 else if type(t) = int, d ∈ Dt is the introduced default, and τ ′ ∈ Child-Tabs then
4 subS,M(C) := {〈ρ+d7→δ,M∪MC,RC〉 | 〈ρ,MC,RC〉 ∈ C, δ ∈ S}
5 τt ← {〈Z+d ,M, sub{c},∅(P), sub{p,j,c},∅(C) ∪ sub{p,j},M(P)〉,
〈Z,M, sub{p,j},∅(P), sub{p,j},∅(C)〉 | 〈Z,M,P, C〉 ∈ τ
′}
6 else if type(t) = int, l(con,d) ∈ χt for d ∈ Dt is the introduced label, and τ ′ ∈ Child-Tabs then
7 Updc(C) := {〈ρ,Mod(γ(d),MC), {R | R ∈ RC, R = Mod(γ(d), R)}〉 | 〈ρ,MC,RC〉 ∈ C, ρ(d) = c}
8 CW(C) := {〈ρ,MC∼bf ∪Mod(γ(d),MCBF),RC〉 | 〈ρ,MC,RC〉 ∈ C, ρ(d) 6= γ}
9 τt ← {〈Z,Mod(γ(d),M),Updc(P), Updc(C) ∪ CW(C)〉 | 〈Z,M,P, C〉 ∈ τ ′, d ∈ Z} ∪
{〈Z,M,P, C〉 | 〈Z,M,P, C〉 ∈ τ ′, d 6∈ Z}
10 else if type(t) = int, l(pre,d) ∈ χt for d ∈ Dt is the introduced label, and τ ′ ∈ Child-Tabs then
11 RtσM := {R ∪ {M} | R ∈ R ∪ {∅ | ε 6∈ σ−1(α)},M ∈M}
12 Updp(C, f) := cpyd(C, α) ∪ {〈ρ+ε7→α,MC,RC tρ Mod(α(d), f(MC∼bf))〉 | 〈ρ,MC,RC〉 ∈ C, ρ(d) = p}
13 τt ← {〈Z,M,Updp(P, fM), Updp(C, fid)〉 | 〈Z,M,P, C〉 ∈ τ ′}
14 else if type(t) = int, l(just,d) ∈ χt for d ∈ Dt is the introduced label, and τ ′ ∈ Child-Tabs then
15 Updj(M, C) := cpyd(C, β) ∪ {〈ρ,MC ∪ [Mod(β(d),M)]
]
bf,RC)〉 | 〈ρ,MC,RC〉 ∈ C, ρ(d) = j}
16 τt ← {〈Z,M,Updj(M,P), Updj(M, C)〉 | 〈Z,M,P, C〉 ∈ τ
′}
17 else if type(t) = int, a ∈ χt is the introduced atom, and τ ′ ∈ Child-Tabs then
18 AGuess(C) := {〈ρ,MC ∪MC]a , {R′ | R′ ∈ R?a, R ∈ RC}〉 | 〈ρ,MC,RC〉 ∈ C}
19 τt ← {〈Z,M∪M]a ,AGuess(P), AGuess(C)〉 | 〈Z,M,P, C〉 ∈ τ ′}
20 else if type(t) = rem, d 6∈ Dt is the removed default, and τ ′ ∈ Child-Tabs then
21 SProj(C) := {〈ρ \ {d 7→ α, d 7→ β, d 7→ γ},MC,RC〉 | 〈ρ,MC,RC〉 ∈ C}
22 τt ← {〈Z−d ,M, SProj(P), SProj(C)〉 | 〈Z,M,P, C〉 ∈ τ
′}
23 else if type(t) = rem, a 6∈ χt is the removed atom or label, and τ ′ ∈ Child-Tabs then
24 AProj(C) := {〈ρ,MC∼a , {R∼a | R ∈ RC}〉 | 〈ρ,MC,RC〉 ∈ C}
25 τt ← {〈Z,M∼a ,AProj(P), AProj(C)〉 | 〈Z,M,P, C〉 ∈ τ ′}
26 else if type(t) = join and τ ′, τ ′′ ∈ Child-Tabs with τ ′ 6= τ ′′ then
27 M′ ./M′′ := {M ′ ∪M ′′ |M ′ ∈M′,M ′′ ∈M′′,M ′ ∩ χt =M ′′ ∩ χt}
28 MC′ ./M′,M′′ MC′′ := (MC′ ∩MC′′) ∪ [MC′BF ./ (MC′′BF ∪M′′)] ∪ [(MC′BF ∪M′) ./MC′′BF]
29 C′.̂/M′,M′′C′′ := {〈ρ,MC′ ./M′,M′′ MC′′,RC′ ∩RC′′〉 | 〈ρ,MC′,RC′〉 ∈ C′, 〈ρ,MC′′,RC′′〉 ∈ C′′}
30 τt ← {〈Z,M′ ∩M′′,P ′.̂/M′,M′′P ′′, (C′.̂/M′,M′′C′′) ∪ (P ′.̂/M′,M′′C′′) ∪ (C′.̂/M′,M′′P ′′)〉
| 〈Z,M′,P ′, C′〉 ∈ τ ′, 〈Z,M′′,P ′′, C′′〉 ∈ τ ′′}
Figure 2 Selected DP tables of SCONS for nice LTD T .
tables, that is, sets of all possible tuples of generating defaults, additional information to identify the
satisfiability of formulas, and counter-candidates that contradict to extend a generating default to a
characterizing default. A main part of the algorithm is a procedure that computes a table τt for node t
from the tables of the children of t (if exist), which we call table algorithm. We will below present the table
algorithm for DL as given in Algorithm 2 (SINC) in more detail. Given a default theory D. Algorithm DPSINC
visits every node t ∈ T in post-order, and then applies algorithm SINC to the sub-theory Dt to compute
new tables.
Consider an LTD T = (T, χ) and a node t of T . The Algorithm 2 (SINC) consists of two parts:
finding (i) consistent generating defaults of the default theory Dt and (ii) generating defaults Z, whose
extensions γ(Z) are subset minimal among consistent defaults (characterizing generating defaults). For SINC,
we store in τt tuples that are of the form 〈Z,M,P, C〉, where Z ⊆ γt and M⊆ 2X for X = χ(t) ∩ at(D).
The first three tuple positions (red and green) cover Part (i), which we call the witness part. The last
positions consist of a set of tuples C = 〈ρ,MC,RC〉 to handle Part (ii), which we call the counter-witness
5
part.
We call Z the witness generating default, since γ(Z), is, roughly speaking, part of a potential generating
default. In particular, Z witnesses the existence of an extension Z ′ ⊇ γ(Z) for a sub-theory S of D<t, i.e., the
default theory strictly below t. The set M of witness models contains models of F ′ := {C | C ∈ F, F ∈ ∆}
where ∆ ⊆ γ(D<t) ∪ Z ′ is a set of conclusions of the firing defaults of D≤t. For our assumed witness
generating default Z, we require a witness proof P. The set P consists of tuples of the form 〈σ,B,R〉,
where σ : Dt ∪ {ε} → {p, j, c}, B ⊆ 2X and R ⊆ 22
X
for X = χ(t) ∩ at(D). The function σ, which we
call witness states function, maps each default d ∈ Dt to a certain decision state. In particular, σ(d) = c
marks that d should “fire”, in other words, we assume that d is part of a characterizing generating default.
Assignment σ(d) = p or σ(d) = j marks that d does not fire, because of α or β, respectively. Note that
we require to remember whether any default in D<t has been set to p, resulting in σ(ε) = p in such a
case (using empty default ε). The set B, consists of backfire witness models, containing atom bf. Such a
model M ∈ B is not only a model of F ′, but also of β(d) for some default d ∈ σ−1(β) ∪D<t and proves
that d “backfires”, i.e., d should have fired. In the end, Z proves an extension of sub-theory S, where
such backfiring defaults are excluded. Hence, {d | d ∈ D<t, σ(d) = β,∃M ∈ B : M  Z ′, β(d)} ∩ S = ∅.
In the end, we want to have S = D<n for the root n. If there is such a tuple 〈σ, {{bf}}, ·〉 ∈ P, this
indicates a wrong assignment to σ′(d) = j for some default d and node in the tree decomposition below.
For assignment σ(d) = p to be correct, it is required that d does not fire, because α(d) is not a consequence
of Z ′. To be more concrete, we additionally have to find at least one model M such that M  Z ′,¬α(d).
Since such a model M is required for each d ∈ D<t \ {d | γ(d) ∈ Z ′}, which does not fire because of α, the
set R of required witness model sets keeps a set of such potential model sets R ∈ R containing at least one
such model M ∈ R for each such default d. Note that R contains and keeps every R of these potential sets,
as long as each M ∈ R also satisfies Z ′, and otherwise the whole set is excluded. This set R of sets allows
us to not distinguish at any TD node t between defaults d with σ(d) = α, and to therefore still maintain
the runtime guarantees. To be more concrete, we have that {d | d ∈ D<t, σ(d) = α,R = ∅} ∩ S = ∅, i.e.,
if at least one default d ∈ D<t did not fire because of α, we require to have at least one R ∈ R in order
to guarantee that all of these defaults are satisfied (we can not differentiate). Finally, we end up with
S = D<t \ [{d | d ∈ D<t, σ(d) = β,∃M ∈ B : M  Z ′, β(d)} ∪ {d | d ∈ D<t, σ(d) = α,R = ∅}]. To
conclude, there is a consistent default E of the default theory D if table tn for (empty) root n contains
〈∅, {∅},P〉, where P either contains 〈∅, ∅, ∅〉 or 〈{ε 7→ α}, ∅, {{∅}}〉. The main aim of C is to invalidate the
subset-minimality of γ(Z), and will be covered later.
In the following, we intuitively discuss important cases of Algorithm 2 for Part (i), which consists only
of the first three tuple positions (colored red and green) and ignore the remaining parts of the tuple. We
call the resulting algorithm SCONS. Let t again be a node of a given LTD and 〈Z,M,P, ·〉 a tuple of
table ∈ τ ′ for an LTD child node of t. Further, we also assume any tuple 〈σ,B,R〉 ∈ P . When a default d
is introduced (type(t) = int), Line 5 is responsible for making decisions concerning whether d fires, and the
reason why it does not fire, otherwise. Lines 6–16 cover nodes with type(t) = int using labels, “intermediate”
node types designed for the ease of presentation, and are explained as follows: In the case of l(con,d) is
the introduced label (Lines 6–9) and if σ(d) = c, we enforce that each M ∈ M is also a model of γ(d)
and only keep sets R in R, where each M ∈ R is a model of γ(d). Next, Lines 10–13 mark for defaults d,
which do not fire because of α (σ(d) = p), the empty default ε by setting σ(ε) = p. Additionally, it is
enforced for these defaults d that each set R ∈ R contains any non-backfire model of M, which is also a
model of ¬(α(d)). Finally, if σ(d) = β, Lines 14–16 increase B such that it grows by models of M that
are also models of β(d) and contain the atom bf. Next, we cover the case, where an atom a is introduced.
Lines 17–19 cover this node type, which increases existing witness set M, backfire witness sets B, and
required witness sets R by the case where a is set to true. In particular, we have to compute all potential
1SBF = {S : S ∈ S, bf ∈ S}, S−e := S \ {e}, S∼e := {S
−
e | S ∈ S}, S+e := S ∪ {e}, S]e := {S
+
e | S ∈ S}, ∅?e := {∅}
and S?e :=
⋃
S∈S,S′∈(S\S)?e
{S′ ∪ {S+e }, S′ ∪ {S}}. Further, identity function fid is defined by fid(x) 7→ x, and constant
function fS is given by f(x) 7→ S.
6
/0
t16T:
{a,b}
t15
{a,b,d2, lcon,d2}
t14
{a,d2, ljust,d2}t13
{a,d2, lpre,d2}t12
{a,d2}t11
/0t9
{a,b}
t8
{a,b,d1, lcon,d1}
t7
{a,b,d1} t6
{a,d1, lpre,d1} t5
{a,d1, ljust,d1} t4
{a,d1} t3
{a} t2
/0t1
〈Z14.i,M14.i,P14.i,C14.i〉 τ14
〈 /0,2{a,b},{〈{d2 7→ p,ε 7→ p}, /0, /0〉,
〈{d2 7→ j},{{bf},{b,bf}}, /0〉}, /0〉
〈{d2},{ /0,{a}},{〈{d2 7→ c}, /0, /0〉},{
〈{d2 7→ p,ε 7→ p},2{a,b}, /0〉,
〈{d2 7→ j},2{a,b} ∪{{bf}}, /0〉}〉
〈Z16.i,M16.i,P16.i,C16.i〉 τ16
〈 /0,{ /0},{〈{ε 7→ p}, /0, /0〉},
{〈 /0,{bf}, /0〉}, /0〉
〈 /0,{ /0},{〈 /0, /0, /0〉},{
〈{ε 7→ p}, /0, /0〉,〈 /0,{{bf}}, /0〉}〉
〈Z5.i,M5.i,P5.i,C5.i〉 τ5
〈 /0,{ /0,{a}},{〈{d1 7→ p,ε 7→ p}, /0, /0〉,
〈{d1 7→ j},{{a,bf}}, /0〉}, /0〉
〈{d1},{ /0,{a}},{〈{d1 7→ c}, /0, /0〉},{
〈{d1 7→ p,ε 7→ p},2{a}, /0〉,
〈{d1 7→ j},{ /0,{a},{a,bf}}, /0〉}〉
〈Z7.i,M7.i,P7.i,C7.i〉 τ7
〈 /0, 2{a,b},{〈{d1 7→ p,ε 7→ p}, /0, /0〉,
〈{d1 7→ j},{{a,bf},{a,b,bf}}, /0〉}, /0〉
〈{d1},2{a,b} \ /0,{〈{d1 7→ c}, /0, /0〉},{
〈{d1 7→ p,ε 7→ p},2{a,b}, /0〉,
〈{d1 7→ j},2{a,b} ∪{{a,bf},{a,b,bf}}, /0〉}〉
〈Z3.i,M3.i,P3.i,C3.i〉 τ3
〈 /0,{ /0,{a}},{〈{d1 7→ α}, /0, /0〉,
〈{d1 7→ j}, /0, /0〉}, /0〉
〈{d1},{ /0,{a}},{〈{d1 7→ c}, /0, /0〉},{
〈{d1 7→ p},2{a}, /0〉,
〈{d1 7→ j},2{a}, /0〉}〉
〈Z4.i,M4.i,P4.i,C4.i〉 τ4
〈 /0,{ /0,{a}},{〈{d1 7→ p}, /0, /0〉,
〈{d1 7→ j},{{a,bf}}, /0〉}, /0〉
〈{d1},{ /0,{a}},{〈{d1 7→ c}, /0, /0〉},{
〈{d1 7→ p},2{a}, /0〉,
〈{d1 7→ j},{ /0,{a},{a,bf}}, /0〉}〉
〈Z1.i,M1.i,P1.i,C1.i〉 τ1
〈 /0,{ /0},{〈 /0, /0, /0〉}, /0〉
Figure 3 Selected DP tables of SINC for nice LTD T .
sets R ∈ R of models, where a is either set to true or to false. When a default d gets removed in Lines 20–22,
we remove the mapping of d in σ to any of {p, j, c} since d is not considered anymore. The removal of an
atom a (Lines 23–25) is roughly speaking, a kind of projection to the current bag. We remove objects
anywhere within the tuples, which contain the atom a we do not consider anymore. We also use this case
for removing a label, since we only copy all the tuples there. Finally, the case where type(t) = join, also
talks about a second tuple u′′ of a different, second child table τ ′′. Intuitively, both tuples, representing
intermediate results of two different branches, have to agree on the witness extension, witness states, and
the witness models. Note that for a backfire model B to endure in B within P of τt, it suffices if B is a
backfire model in one branch B ∪ {bf} ∈ B′ and an ordinary model in the other branch (M ∈M′′).
Example 5. Consider default theory D from Example 1 and in Figure 3 (left) LTD T = (·, χ) of S(D)
and the tables τ1, . . . , τ16 illustrating computation results obtained during post-order traversal of T by
DPSCONS. Table τ1 = {〈∅, {∅}, {〈∅, ∅, ∅〉}〉} as type(t1) = leaf (see Line 2). Since type(t2) = int and a is
the introduced atom, we construct table τ2 from τ1 by {〈σ1.1,M2.1,R1.1〉}, where M2.1 contains M1.1.k
and M1.1.k ∪{a} for each M1.1.k (k ≤ 1) in τ1 (corresponding to a guess on a). Precisely, M2.1 := {∅, {a}}
(L 19). Then, t3 introduces default d1, resulting in two tuples. Either (see tuple u3.1), we do not let
default d1 fire. In this case, the reason for not firing can be α(d1) or β(d1) (see P3.1, L 5). Otherwise,
i.e., default d1 fires, we have Z3.2 = {d1} and P3.2 = {{d1 7→ c}, ∅, ∅〉}, since d1 fires. Node t4 introduces
label ljust,d1 and modifies P4.1. In particular, it chooses among M candidates, which might prove that d1
backfires (see Line 16). Obviously, since β(d1) = b, B4.1.2 = {{a, bf}}. In table τ5, we cover cases, where
default d1 does not fire because of its preconditions α(d1) = {}. In this case (see P5.1.1), we do not find any
model of {∅}, hence R5.1.1 = ∅. Whenever such a set 〈σ, ·, ∅〉 ∈ P occurs in some table, we could discard
the tuple in case of ε ∈ σ−1(α), since at least one choice concerning α was wrong. Table τ7 concerns about
the conclusion γ(d1) of a default, and, intuitively, updates every model occurring in the table, such that the
models satisfy γ(d1) in case it fires. The right branch of the LTD works similarly to the left branch, and in
the end, join node t16, intuitively, just combines witnesses agreeing on its bag content.
Next, we briefly discuss the handling of counter-witnesses, which completes Algorithm SINC. Observe
that the handling of counter-witnesses C is quite similar to the witness proofs P . The tuples 〈ρ,MC,RC〉 ∈ C
consist of a counter-witness state ρ : D≤t ∪ {ε} 7→ {p, j, c}, counter-witness models MC ⊆ 2X , and a set of
required counter-witness models RC ⊆ 22X for X = at≤t ∪ {bf}. Compared to the witness models, MC
potentially contains backfire counter-witness models per design, since here we do not require to enumerate
7
all the justifications, why a certain witness does not lead to a (subset-minimal) generating default. Instead,
the existence of a certain counter-witness tuple for a witness in a table τt proves that the corresponding
witness can not be extended to a generating default of D≤t.
In the following, we provide insights on the correctness of the algorithm DPSINC.
Theorem 1. Given a default theory D, the algorithm DPSINC correctly solves Ext.
The correctness proof of these algorithms need to investigate each node type separately. We have to
show that a tuple at a node t guarantees existence of a model for the program D≤t, proving soundness.
Conversely, one can show that each generating default set is indeed evaluated while traversing the TD,
which provides completeness. We employ this idea using the notions of (i) partial solutions consisting of
partial extensions and the notion of (ii) local partial solutions.
Definition 1. Let D be a default theory, T = (T, χ) be an LTD of the semi-incidence graph S(D) of D, where
T = (N, ·, ·), and t ∈ N be a node. Further, let ∅ (M⊆ 2at≤t∪{bf}, R ⊆ 2M∼bf , σ : (D≤t ∪ {ε})→ {p, j, c},
E ⊆ γ≤t, and Z := γ(σ−1(c)) with Z ⊆ E. The tuple (σ,M,R) is a partial extension under E for t if the
following conditions hold:
1. Z  D<t \ [{d ∈ D<t | σ(d) = j, ∃M ∈MBF : M  E, β(d)} ∪ {d ∈ D<t | σ(d) = p,R = ∅}],
2. σ(ε) = p⇐= σ−1(p) 6= ∅; σ−1(p) = ∅ =⇒ σ(ε) undefined,
3. M is the largest set such that:
(a) M  Z for every M ∈M,
(b) M  E, β(d) =⇒M+bf ∈MBF for every d ∈ D≤t s.t. σ(d) = j, β(d) ∈ β≤t,M ∈M,
(c) ∃d ∈ D≤t : σ(d) = j, β(d) ∈ β≤t,M  E, β(d) for every M ∈MBF, and
4. R is the largest set such that:
(a) |R| ≤ |σ−1(α)| − 1 for every R ∈ R,
(b) ∃d ∈ D≤t : σ(d) = p, α(d) ∈ α≤t,M  Z,¬α(d) for every M ∈ (
⋃
R∈RR), and
(c) ∃M ∈ R : M  Z,¬(α(d))⇐= σ(d) = p for every d ∈ D≤t s.t. α(d) ∈ α≤t, R ∈ R
Definition 2. Let D be a default theory, T = (T, χ) where T = (N, ·, n) be an LTD of S(D), and t ∈ N
be a node. A partial solution for t is a tuple (Z,M,P, C) where Z ⊆ γ≤t, and P is the largest set of tuples
such that each (σ,B,R) ∈ P is a partial extension under Z with BBF = B and Z = σ−1(c). Moreover, C is
the largest set of tuples such that for each (ρ,MC,RC) ∈ C, we have that (ρ,MC,RC) is a partial extension
under Z with ρ−1(c) ( σ−1(c). Finally, M⊆ 2at≤t is the largest set with M  γ(Z) for each M ∈M.
The following lemma establishes correspondence between stable extensions and partial solutions.
Lemma 1 (?). Let D be a default theory, T = (T, χ) be an LTD of the semi-incidence graph S(D), where
T = (·, ·, n), and χ(n) = ∅. Then, there exists a stable extension Z ′ for D if and only if there exists a
partial solution u = (Z,M,P, C) for root n with at least one tuple 〈σ,B,R〉 ∈ P where BBF = ∅, and either
σ−1(p) = ∅, or R 6= ∅, where C is of the following form: For each (ρ,MC,RC) ∈ C, either MCBF 6= ∅, or
RC = ∅ and ρ−1(p) 6= ∅.
Next, we require the notion of local partial solutions corresponding to the tuples obtained in Algorithm 2.
Definition 3. Let D be a default theory, T = (T, χ) an LTD of the semi-incidence graph S(D), where T =
(N, ·, n), and t ∈ N be a node. A tuple (σ,B,R) is a local partial solution part of partial solution (σ̂, B̂, R̂)
for t if
1Proofs for statements marked with “?” can be found in the appendix.
8
1. σ = σ̂ ∩ ([χ(t) ∪ {ε}]× {p, j, c}),
2. B = B̂t, where St := {S ∩ (χ(t) ∪ {bf}) | S ∈ S}, and
3. R = {Rt | R ∈ R̂}.
Definition 4. Let D be a default theory, T = (T, χ) an LTD of the semi-incidence graph S(D), where
T = (N, ·, n), and t ∈ N be a node. A tuple u = 〈Z,M,P, C〉 is a local partial solution for t if there
exists a partial solution û = (Ẑ,M̂, P̂, Ĉ) for t such that the following conditions hold: (1) Z = Ẑ ∩ 2Dt ,
(2) M = M̂t, (3) P is the smallest set containing local partial solution part (σ,B,R) for each (σ̂, B̂, R̂) ∈ P̂,
and (4) C is the smallest set with local partial solution part (ρ,MC,RC) ∈ C for each (ρ̂,M̂C, R̂C) ∈ Ĉ. We
denote by ût the local partial solution u for t given partial solution û.
The following proposition provides justification that it suffices to store local partial solutions instead of
partial solutions for a node t ∈ N .
Lemma 2. Let D be a default theory, T = (T, χ) an LTD of S(D), where T = (N, ·, n), and χ(n) = ∅. Then,
there exists a stable extension for D if and only if there exists a local partial solution of the form 〈∅, {∅},P, C〉
for the root n ∈ N with at least one tuple of the form 〈σ, ∅,R〉 ∈ P where either σ−1(α) = ∅, or R 6= ∅.
Moreover, for each 〈ρ,MC,RC〉 in C, either MCBF 6= ∅, or ρ−1(α) 6= ∅ and RC = ∅.
Proof. Since χ(n) = ∅, every partial solution for the root n is an extension of the local partial solution u
for the root n ∈ N according to Definition 4. By Lemma 1, we obtain that the lemma is true.
In the following, we abbreviate atoms occurring in bag χ(t) by att, i.e., att := χ(t) \Dt.
Proposition 2 (?,Soundness). Let D be a default theory, T = (T, χ) an LTD of the semi-incidence
graph S(D), where T = (N, ·, ·), and t ∈ N a node. Given a local partial solution u′ of child table τ ′
(or local partial solution u′ of table τ ′ and local partial solution u′′ of table τ ′′), each tuple u of table τt
constructed using table algorithm SINC is also a local partial solution.
Proposition 3 (?,Completeness). Let D be a default theory, T = (T, χ) where T = (N, ·, ·) be an LTD
of S(D) and t ∈ N be a node. Given a local partial solution u of table τt, either t is a leaf node, or there
exists a local partial solution u′ of child table τ ′ (or local partial solution u′ of table τ ′ and local partial
solution u′′ of table τ ′′) such that u can be constructed by u′ (or u′ and u′′, respectively) and using table
algorithm SINC.
Corollary 1 (Completeness for Enumeration). Let D be a default theory, T = (T, χ) where T = (N, ·, ·)
be an LTD of S(D) and t ∈ N be a node. Given a partial solution û and the corresponding local partial
solution u = ût for table τt, either t is a leaf node, or there exists a local partial solution u
′ of child table
τ ′ (or local partial solution u′ of table τ ′ and local partial solution u′′ of table τ ′′) such that u can be
constructed by u′ (or u′ and u′′, respectively) and using table algorithm SINC.
Proof. The result directly follows from the proof for completeness (see Proposition 3).
Now, we are in the situation to prove Theorem 1.
Proof of Theorem 1. We first show soundness. Let T = (T, χ) be the given LTD, where T = (N, ·, n). By
Lemma 2 we know that there is a stable extension D if and only if there exists a local partial solution for
the root n. Note that the tuple is by construction of the form 〈∅, {∅},P, C〉, where P 6= ∅ can contain a
combination of the following tuples 〈∅, ∅, ∅〉, 〈{ε 7→ α}, ∅, {{∅}}〉. For each 〈ρ,MC,RC〉 ∈ C eitherMCBF 6=
∅ or ρ−1(α) 6= ∅ and RC = ∅. In total, this results in 48 possible tuples, since C ⊆ 2C can contain any
combination (16 many) of C, where C = {〈∅, {∅, {bf}}, ∅〉, 〈{ε 7→ α}, {∅}, ∅〉, 〈{ε 7→ α}, {∅, {bf}}, {{∅}}〉,
〈{ε 7→ α}, {∅, {bf}}, ∅〉}.
9
Hence, we proceed by induction starting from the leaf nodes in order to end up with such a tuple at the
root node n. In fact, the tuple 〈∅, {∅}, {〈∅, ∅, ∅〉}, ∅〉 is trivially a partial solution for (empty) leaf nodes by
Definitions 1 and 2 and also a local partial solution of 〈∅, {∅}, {〈∅, ∅, ∅〉}, ∅〉 by Definition 4. We already
established the induction step in Proposition 2. Hence, when we reach the root n, when traversing the
TD in post-order by Algorithm DPSINC, we obtain only valid tuples inbetween and a tuple of the form
discussed above in the table of the root n witnesses an answer set.
Next, we establish completeness by induction starting from the root n. Let therefore, Ẑ be an arbitrary
stable extension of D. By Lemma 2, we know that for the root n there exists a local partial solution
of the discussed form 〈∅, {∅},P, C〉 for some partial solution 〈Ẑ ′,M̂, P̂, Ĉ〉 with γ(Ẑ ′) = Ẑ. We already
established the induction step in Proposition 3. Hence, we obtain some (corresponding) tuples for every
node t. Finally, stopping at the leaves n. In consequence, we have shown both soundness and completeness
resulting in the fact that Theorem 1 is true.
Theorem 1 states that we can decide the problem Cons by means of Algorithm DPSINC. The following
theorem states its that we obtain fourfold exponential runtime in the treewidth.
Theorem 2 (?). Given a default theory D, the algorithm DPSINC and runs in time O(42
22
k+2
· ‖S(D)‖),
where k := tw(S(D)) is the treewidth of the semi-incidence graph S(D).
Next, we establish that we can slightly extend DPSINC to enumerate characterizing generating defaults.
The algorithm on top of DPSINC is relatively straight forward and presented in Algorithm 3.
Theorem 3 (?). Given a default theory D, the algorithm DPSINC can be used as a preprocessing step to
construct tables from which we can correctly solve the problem EnumSE.
The correctness proof requires to extend the previous results to establish a one-to-one correspondence
when traversing the tree the TD and we can reconstruct each solution as well as we do not get duplicates.
Therefore, we require the following three results.
Observation 1. Let D be a default theory, T = (T, χ) where T = (N, ·, ·) be an LTD of S(D) and t ∈ N
be a node. Then, for each partial solution u = 〈Z,M,P, C〉 for t, M,P and C are functional dependent
from Z, i.e., for any partial solution u′ = 〈Z,M′,P ′, C′〉 for t, we have u = u′.
Lemma 3 (?). Let D be a default theory, T = (T, χ) with T = (N, ·, ·) be an LTD of S(D), and Z be a
characterizing generating default. Then, there is a unique set of tuples S, containing exactly one tuple per
node t ∈ N containing only local partial solutions of the unique partial solution for Z.
Proposition 4 (?). Let D be a default theory, T = (T, χ) with T = (N, ·, ·) be an LTD of S(D), and Z be
a characterizing generating default. Moreover, let S be the unique set of tuples, containing exactly one tuple
per node t ∈ N and containing only local partial solutions of the unique partial solution for Z. Given S,
and tables of Algorithm SINC, one can compute in time O(‖D‖) a characterizing generating default Z ′
with Z ′ 6= Z, assuming one can get for a specific tuple u for node t its corresponding ≺-ordered predecessor
tuple set origt(u) of tuples in the child node(s) of t in constant time.
4 Conclusion
In this paper, we established dynamic programming algorithms that operate on tree decompositions of the
semi-incidence graph of a given default theory. Our algorithms can be used to decide whether the default
theory has a stable extension or to enumerate all characterizing generating defaults. Our algorithms run in
fpt-time and linear delay, respectively.
We believe that our algorithms can be extended to tree decompositions of the incidence graph. However,
then we need additional states to handle the situation that prerequisite, justification, and conclusion may
10
not occur together in one bag, consequently such an algorithm will likely be very complex. An interesting
research question is whether we can improve our runtime bounds. Still it might be worth implementing
our algorithms to enumerate characterizing generating defaults for DL, as previous work showed that a
relatively bad worst-case runtime may anyways lead to practical useful results [5].
References
[1] Olaf Beyersdorff, Arne Meier, Michael Thomas, and Heribert Vollmer. The complexity of reasoning
for fragments of default logic. J. of Logic and Computation, 22(3):587–604, 2012.
[2] Bernhard Bliem, Günther Charwat, Markus Hecher, and Stefan Woltran. D-FLATˆ2: Subset min-
imization in dynamic programming on tree decompositions made easy. Fund. Inform., 147:27–34,
2016.
[3] Bernhard Bliem, Markus Hecher, and Stefan Woltran. On efficiently enumerating semi-stable extensions
via dynamic programming on tree decompositions. COMMA’16, 2016.
[4] H. Bodlaender and A. M. C. A. Koster. Combinatorial optimization on graphs of bounded treewidth.
The Computer Journal, 51(3):255–269, 2008.
[5] Günther Charwat and Stefan Woltran. Dynamic programming-based qbf solving. In Florian Lonsing
and Martina Seidl, editors, QBF’16, 2016.
[6] Bruno Courcelle. Graph rewriting: An algebraic and logic approach. In Jan van Leeuwen, editor,
Handbook of theoretical computer science, Vol. B, pages 193–242. Elsevier, 1990.
[7] Nadia Creignou, Arne Meier, Julian-Steffen Müller, Johannes Schmidt, and Heribert Vollmer. Paradigms
for parameterized enumeration. Th.Comp. Syst., 60(4):737–758, 2017.
[8] M. Cygan, F. V. Fomin, L. Kowalik, D. Lokshtanov, D. Marx, M. Pilipczuk, and S. Saurabh.
Parameterized Algorithms. Springer, 2015.
[9] Johannes K. Fichte, Markus Hecher, Michael Morak, and Stefan Woltran. Answer set solving with
bounded treewidth revisited. LPNMR’17, Springer, 2017.
[10] Johannes K. Fichte, Arne Meier, and Irina Schindler. Strong backdoors for default logic. SAT’16, 2016.
[11] Georg Gottlob. Complexity results for nonmonotonic logics. JLC., 2(3):397–425, 1992.
[12] M. Jakl, R. Pichler, and S. Woltran. Answer-set programming with bounded treewidth. In IJCAI’09,
2009.
[13] Victor W. Marek and Miros law Truszczyński. Nonmonotonic Logic: context-dependent reasoning.
Artificial Intelligence. 1993.
[14] Arne Meier, Irina Schindler, Johannes Schmidt, Michael Thomas, and Heribert Vollmer. On the
parameterized complexity of non-monotonic logics. Archive for Mathematical Logic, 54(5-6):685–710,
2015.
[15] Christos H. Papadimitriou. Computational Complexity. Addison-Wesley, 1994.
[16] Raymond Reiter. A logic for default reasoning. AIJ, 13:81–132, April 1980.
[17] Marko Samer and Stefan Szeider. Constraint satisfaction with bounded treewidth revisited. J. Comp.
and Sys. Sci,., 76(2):103–114, 2010.
11
A Omitted Proofs
Lemma 4. Let D be a default theory, T = (T, χ) be an LTD of the semi-incidence graph S(D), where
T = (·, ·, n), and χ(n) = ∅. Then, there exists a stable extension Z ′ for D if and only if there exists a
partial solution u = (Z,M,P, C) for root n with at least one tuple 〈σ,B,R〉 ∈ P where BBF = ∅, and either
σ−1(p) = ∅, or R 6= ∅, where C is of the following form: For each (ρ,MC,RC) ∈ C, either MCBF 6= ∅, or
RC = ∅ and ρ−1(p) 6= ∅.
Proof (Sketch). Given a stable extension Z ′ of D we construct u = (Z,M,P, C) where we generate every
potential function σ : D≤t ∪ {ε} → {p, j, c} with σ(d) := p (only if Z ′,¬(α(d)) 6 {∅}), or σ(d) := j (only
if Z ′, β(d)  {∅}) for any d ∈ D≤t. Finally, if σ(d) has not been set yet (and neither one of the two
conditions holds), let σ(d) := c. In case of σ−1(p) 6= ∅, we also set the empty default σ(ε) := p. Observe
that since Z ′ is a stable extension, σ(d) = c =⇒ γ(d) ∈ Z ′.
For each of this functions σ, we require 〈σ, ∅,R〉 ∈ P, where R := {R | R ⊆ 2at(D), |R| ≤ |σ−1(α)| −
1, for all d ∈ σ−1(α) \ {ε} : ∃M ∈ R : M  Z ′,¬α(d)}.
Moreover, we define set Z := σ−1(γ) and set M := Mod({C | C ∈ Z ′i, Z ′i ∈ Z ′}, 2at(D)), in order for u
to be a partial solution for n (see Definition 2). We construct C, consisting of partial solutions (ρ,MC,RC)
where we use every potential state function ρ with ρ−1(c) ( σ−1(c). For this, let E := γ(ρ−1(c)). For
the defaults d with ρ(d) 6= c, i.e., defaults d that do not fire because of α(d) or β(d), we also set their
state ρ(d) to α or β, respectively (analogous to above). Again, in case of ρ−1(p) 6= ∅, we also set the
empty default ρ(ε) := p, otherwise ρ(ε) is undefined. Finally, we define set MC := Mod({C | C ∈ Ei, Ei ∈
E}, 2at(D)) ∪ [⋃d:ρ(d)=j Mod({Z ′i | C ∈ Z ′i, Z ′i ∈ Z ′} ∪ β(d), 2at(D))+bf] and RC := {R | R ⊆ 2at(D), |R| ≤
|ρ−1(p)| − 1, for all d ∈ ρ−1(p) \ {ε} : ∃M ∈ R : M  E,¬(α(d))} according to Definition 1.
For the other direction, Definitions 1 and 2 guarantee that Z ′ is a stable extension if there exists such a
partial solution u. In consequence, the lemma holds.
Proposition 5 (Soundness). Let D be a default theory, T = (T, χ) an LTD of the semi-incidence
graph S(D), where T = (N, ·, ·), and t ∈ N a node. Given a local partial solution u′ of child table τ ′
(or local partial solution u′ of table τ ′ and local partial solution u′′ of table τ ′′), each tuple u of table τt
constructed using table algorithm SINC is also a local partial solution.
Proof. Let u′ be a local partial solution for t′ ∈ N and u a tuple for node t ∈ N such that u was derived
from u′ using table algorithm SINC. Hence, node t′ is the only child of t and t is either removal or introduce
node.
Assume that t is a removal node and d ∈ Dt′ \Dt for some default d. Observe that for u = 〈Z,M,P, C〉
and u′ = 〈Z ′,M,P ′, C′〉, models M and model sets R are equal, i.e., 〈·,M,R〉 ∈ P ⇐⇒ 〈·,M,R〉 ∈ P ′
and 〈·,M,R〉 ∈ C ⇐⇒ 〈·,M,R〉 ∈ C′. Since u′ is a local partial solution, there exists a partial solution û′
of t′, satisfying the conditions of Definition 4. Then, û′ is also a partial solution for node t, since it satisfies
all conditions of Definitions 1 and 2. Finally, note that u = (û′)t since the projection of û′ to the bag χ(t)
is u itself. In consequence, the tuple u is a local partial solution.
For a ∈ att′ \ att as well as for introduce nodes, we can analogously check the proposition.
Next, assume that t is a join node. Therefore, let u′ and u′′ be local partial solutions for t′, t′′ ∈ N ,
respectively, and u be a tuple for node t ∈ N such that u can be derived using both u′ and u′′ in
accordance with the SINC algorithm. Since u′ and u′′ are local partial solutions, there exists partial solution
û′ = (Ẑ ′,M̂′, P̂ ′, Ĉ′) for node t′ and partial solution û′′ = (Ẑ ′′,M̂′′, P̂ ′′, Ĉ′′) for node t′′. Using these two
partial solutions, we can construct û = (Ẑ ′ ∪ Ẑ ′′,M̂′ ./ M̂′′, P̂ ′ .̇/ P̂ ′′, (Ĉ′ .̇/ Ĉ′′) ∪ (P̂ ′ .̇/ Ĉ′′) ∪ (Ĉ′ .̇/ P̂ ′′))
12
where for ./ (·, ·) we refer to Algorithm 2. We define .̇/(·, ·) in accordance with Algorithm 2 as follows:
compatf (R̂) := true⇐⇒ |{M̂ | M̂ ∈ R̂, M̂ ∩ χ(t) = f(M̂) ∩ χ(t)}| = |R̂|
R̂′ .̇/ R̂′′ :=
⋃
f :R̂′→R̂′′,f surjective, compatf (R̂′)
{
⋃
M̂ ′∈R̂′
{M̂ ′ ∪ f(M̂ ′)}}
R̂′ .̇/ R̂′′ :=
⋃
R̂′∈R̂′,R̂′′∈R̂′′
(R̂′ .̇/ R̂′′) ∪ (R̂′′ .̇/ R̂′)
MC′ .̇/M′,M′′MC′′ :=(MC′ ./MC′′) ∪ [MC′BF ./ (MC′′BF ∪M′′)]∪
[(MC′BF ∪M′) ./MC′′BF]
Ĉ′ .̇/ Ĉ′′ :={(ρ̂′ ∪ ρ̂′′,M̂C′ ./M̂′,M̂′′ M̂C′′, R̂C′ .̇/ ˆRC′′) | (ρ̂′,M̂C′, R̂C′) ∈ Ĉ′,
(ρ̂′′,M̂C′′, ˆRC′′) ∈ Ĉ′′, ρ̂′−1(π) ∩ χ(t) = ρ̂′′−1(π) ∩ χ(t)
forall π ∈ {p, j, c}}.
Then, we check all conditions of Definitions 1 and 2 in order to verify that û is a partial solution for t.
Moreover, the projection ût of û to the bag χ(t) is exactly u by construction and hence, u = ût is a local
partial solution.
Since one can provide similar arguments for each node type, we established soundness in terms of the
statement of the proposition.
Proposition 6 (Completeness). Let D be a default theory, T = (T, χ) where T = (N, ·, ·) be an LTD of
S(D) and t ∈ N be a node. Given a local partial solution u of table τt, either t is a leaf node, or there
exists a local partial solution u′ of child table τ ′ (or local partial solution u′ of table τ ′ and local partial
solution u′′ of table τ ′′) such that u can be constructed by u′ (or u′ and u′′, respectively) and using table
algorithm SINC.
Proof. Let t ∈ N be a removal node and d ∈ Dt′ \Dt with child node t′ ∈ N . We show that there exists a
tuple u′ in table τt′ for node t
′ such that u can be constructed using u′ by SINC (Algorithm 2). Since u is
a local partial solution, there exists a partial solution û = (Ẑ,M̂, P̂, Ĉ) for node t, satisfying the conditions
of Definition 4. It is easy to see that û is also a partial solution for t′ and we define u′ := ût
′
, which is the
projection of û onto the bag of t′. Apparently, the tuple u′ is a local partial solution for node t′ according
to Definition 4. Then, u can be derived using SINC algorithm and u′. By similar arguments, we establish
the proposition for a ∈ att′ \ att and the remaining node types. Hence, the propositions sustains.
Theorem 2. Given a default theory D, the algorithm DPSINC and runs in time O(42
22
k+2
· ‖S(D)‖), where
k := tw(S(D)) is the treewidth of the semi-incidence graph S(D).
First, we give a proposition on worst-case space requirements in tables for the nodes of our algorithm.
Proposition 7. Given a default theory D, an LTD T = (T, χ) with T = (N, ·, ·) of the semi-incidence
graph S(D), and a node t ∈ N . Then, there are at most 2k+1 · 22k+1 · 4(3k+1·22
k+1
·22
2k+1
) tuples in τt using
algorithm DPSINC for width k of T .
Proof. (Sketch) Let D be the given default theory, T = (T, χ) an LTD of the semi-incidence graph S(D),
where T = (N, ·, ·), and t ∈ N a node of the TD. Then, by definition of a decomposition of the semi-
incidence graph for each node t ∈ N , we have |χ(t)| − 1 ≤ k. In consequence, we can have at most
2k+1 many witness defaults and 22
k+1
many witnesses models. Each set P may contain a set of witness
proof tuples of the form 〈σ,B,R〉, with at most 3k+1 many witness state σ mappings, 2k+1 many backfire
13
Algorithm 3: Algorithm NGDS(T ) for computing next characteristic generating default.
In: TD T = (T, ·) with T = (N, ·, n), solution tuples S, total ordering ≺ of orig(· · · ).
Out: Next solution tuples using ≺.
1 for iterate t in post-order(T,n) do
2 Child-Tabs := {Tables[t′] | t′ is a child of t in T}
3 t̂ := parent of t
4 S[t] ← direct successor s′  S[t] in origt̂(S[t̂])
5 if S[t] defined then
6 for iterate t′ in Child-Tabs do
7 for iterate t′′ in pre-order(T,t’) do
8 t̂′′ := parent of t′′
9 S[t′′] ← ≺-smallest element in origt̂′′(S[t̂′′])
10 return S
11 return undefined
witness models B, and 222
k+1
many required witnesses model sets. In the end, we need to distinguish
2k+1 · 22k+1 · 2(3k+1·2k+1·22
2k+1
) different witnesses of a tuple in the table τt for node t. For each witness,
we can have at most 2(3
k+1·22
k+1
·22
2k+1
) many counter-witnesses per witness default, witness models, and
required witness model sets. Thus, there are at most 2k+1 · 22k+1 · 4(3k+1·22
k+1
·22
2k+1
) tuples in table τt for
node t. In consequence, we established the proposition.
Proof of Theorem 2. Let D be a default theory, S(D) = (V, ·) its semi-incidence graph, and k be the
treewidth of S(D). Then, we can compute in time 2O(k
3) · |V | an LTD of width at most k [4]. We take such
a TD and compute in linear time a nice TD [4]Let T = (T, χ) be such a nice TD with T = (N, ·, ·). Since the
number of nodes in N is linear in the graph size and since for every node t ∈ N the table τt is bounded by
2k+1 · 22k+1 · 4(3k+1·22
k+1
·22
2k+1
) according to Proposition 7, we obtain a running time of O(422
2k+2
· ‖S(D)‖).
Consequently, the theorem sustains.
Theorem 3. Given a default theory D, the algorithm DPSINC can be used as a preprocessing step to
construct tables from which we can correctly solve the problem EnumSE; more precisely, by first running
Algorithm DPSINC and then Algorithm NGDSINC(T ) on the resulting tables of Algorithm DPSINC until
NGDSINC(T ) returns “undefined”.
Observation 2. Let D be a default theory, T = (T, χ) where T = (N, ·, ·) be an LTD of S(D) and t ∈ N
be a node. Then, for each partial solution u = 〈Z,M,P, C〉 for t, M,P and C are functional dependent
from Z, i.e., for any partial solution u′ = 〈Z,M′,P ′, C′〉 for t, we have u = u′.
Proof. The claim immediately follows from Definition 2.
Lemma 5. Let D be a default theory, T = (T, χ) with T = (N, ·, ·) be an LTD of S(D), and Z be a
characterizing generating default. Then, there is a unique set of tuples S, containing exactly one tuple per
node t ∈ N containing only local partial solutions of the unique partial solution for Z.
Proof. By Observation 1, given Z, we can construct one unique partial solution û = 〈Z,M,P, C〉 for n.
We then define the set S by S :=
⋃
t∈N{ût}. Assume that there is a different set S′ 6= S containing also
exactly one tuple per node t ∈ N . Then there is at least one node t ∈ N , for which the corresponding
tuples u ∈ S, u′ ∈ S′ differ (u 6= u′), since û is unique and the computation ût is defined in a deterministic,
functional way (see Definition 4). Hence, either ût 6= u or ût 6= u′, leading to the claim.
14
Proposition 8. Let D be a default theory, T = (T, χ) with T = (N, ·, ·) be an LTD of S(D), and Z be a
characterizing generating default. Moreover, let S be the unique set of tuples, containing exactly one tuple
per node t ∈ N and containing only local partial solutions of the unique partial solution for Z. Given S,
and tables of Algorithm SINC, one can compute in time O(‖D‖) a characterizing generating default Z ′
with Z ′ 6= Z, assuming one can get for a specific tuple u for node t its corresponding ≺-ordered predecessor
tuple set origt(u) of tuples in the child node(s) of t in constant time.
Proof. Note that with Z, it is easy to determine, which element of S belongs to which node t in T , hence,
we can construct a mapping S : N → S. With S, we can easily apply algorithm NGD, which is given
in Algorithm 3, in order to construct a different solution S ′ in a systematic way with linear time delay,
since T is nice.
Proof of Theorem 3 (Sketch). First, we construct an LTD T = (T, χ) with T = (N, ·) for graph S(D). Then
we run our algorithm DPSINC and get tables for each TD node. In order to enumerate all the characterizing
generating defaults, we investigate each of these tuple, which lead to a valid characterizing generating default
(see proof of Theorem 1). For each of these tuples (if exist), we construct a first solution S (if exist) (as done
in Lines 6 to 9 of Algorithm 3) using origt(·, ·), and total order ≺. Thereby, we keep track of which tuple
in S belongs to which node, resulting in the mapping S (see proof of Proposition 4). Note that origt(·, ·)
and ≺ can easily be provided by remembering for each tuple an ordered set of predecessor tuple sets during
construction (using table algorithm SINC). Now, we call algorithm NGDS(T ) multiple times, by outputting
and passing the result again as argument, until the return value is undefined, enumerating solutions in a
systematic way. Using correctness results (by Theorem 1), and completeness result for enumeration by
Corollary 1, we obtain only valid solution sets, which directly represent characterizing generating defaults
and, in particular, we do not miss a single one. Observe, that we do not get duplicates (see Lemma 3).
15

