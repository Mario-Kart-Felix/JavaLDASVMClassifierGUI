Probabilistic Active Learning of Functions
in Structural Causal Models
Paul K. Rubenstein∗, Ilya Tolstikhin, Philipp Hennig, Bernhard Schölkopf
Max Planck Institute for Intelligent Systems, Tübingen
∗Machine Learning Group, University of Cambridge
Abstract
We consider the problem of learning the functions computing children from parents
in a Structural Causal Model once the underlying causal graph has been identified.
This is in some sense the second step after causal discovery. Taking a probabilistic
approach to estimating these functions, we derive a natural myopic active learning
scheme that identifies the intervention which is optimally informative about all of
the unknown functions jointly, given previously observed data. We test the derived
algorithms on simple examples, to demonstrate that they produce a structured
exploration policy that significantly improves on unstructured base-lines.
1 Introduction
Large parts of the literature on causality are concerned with learning the causal graph of a system of
random variables [Spirtes et al., 2000, Tong and Koller, 2001, Eberhardt, 2010, Hyttinen et al., 2013,
Mooij et al., 2016]. Also known as causal discovery or causal inference, this problem is motivated by
realistic problems in science: a biologist may wish to discover genes responsible for regulating other
genes in a cell; a public health researcher may wish to know whether certain habits in a population
(e. g. smoking) influence certain health outcomes (e. g. probability of developing lung cancer).
The starting point of this paper is to consider what should be done after the causal graph of a system
of variables has already been identified (be it by causal discovery methods, or from prior knowledge).
That is, it is known which variables are functions of which other variables, but the precise functional
relationships are still unknown. Thus, although we understand the causal relationships in a coarse
sense, we will not be able to accurately predict the result of an intervention to the system, possibly
with implications for decision making.
For instance, suppose that in a cell, Gene A up-regulates Gene B and Gene B down-regulates Gene C.
We know that reducing expression of Gene A will lead to a decrease in expression of Gene B which,
in turn, will lead to an increase in expression of Gene C. However, without more precise knowledge
of the relationships between genes, we will be unable to quantitatively predict the effect of applying a
drug that reduces expression of Gene A by 20%. Similarly, if our goal is to reduce overall levels of
lung cancer in the population, then knowing only that smoking causes cancer is insufficient to know
what the best public health policy should be: would it be better if we could persuade 50% of smokers
to stop smoking completely, or persuade every smoker to reduce their consumption by 50%?
For a passive agent supplied with data generated by the system, learning the functional relationships
between parents and children reduces simply to separate regression problems—one for each unknown
function—once the causal graph is known. Many knowledge acquisition problems, however, can be
phrased as a sequential decision making process in which the data that is received at the next point in
time is affected by a decision made based on the data that has already been observed. A biologist
does not blindly perform a series of costly and time-consuming experiments, only looking at the
generated data once the last is over; the data from each experiment would be analysed before the next
is performed, thus informing which experiment would be best to perform next.
ar
X
iv
:1
70
6.
10
23
4v
1 
 [
st
at
.M
L
] 
 3
0 
Ju
n 
20
17
X1 X2 X3
f2 f3
(a) The observational setting
X1 X2 X3
f2 f3
(b) do(X1 = x1)
X1 X2 X3
f3
(c) do(X2 = x2)
Figure 1: Even in the simple setting of three variables whose graph is a chain, there is a non-trivial
trade-off between the information one expects to gain by performing different interventions (see text).
Below, we formalise this problem using the language of Structural Causal Models (SCMs), also
known as Structural Equation Models [Pearl, 2009, Bollen, 2014]. An SCM, in essence, consists
of functions connecting child variables with their causal parents, and is equipped with a notion of
intervention in which a variable (or subset of variables) is externally forced to take a particular value
(or values). We use these interventions as an idealised mathematical representation of performing
an experiment. We take a Bayesian approach to estimating the functional relationships between
parents and children, which naturally gives rise to an active learning algorithm to decide on the next
‘experiment’ to perform.
While our approach works with causal graphs that are arbitrary DAGs, the non-triviality of this
problem is apparent even in the simple case of three variables whose causal graph is a chain (Figure 1).
Our goal in this situation is, in a sense to be made precise later, to learn the functions f2 and f3. At
each point in time, we must decide whether to perform one of the possible interventions or to passively
observe the system, with each different action having some cost. If we make a passive observation,
we will learn something about both f2 and f3, though only in areas where the distributions over X1
and X2 put probability mass. In the small sample setting, we are very unlikely to learn anything
about the functions in areas of low probability of their inputs. If we intervene on X1 and choose what
value to set it to, we can decide precisely where we want to learn about f2 and we will also learn
about f3 in some region, although we would be uncertain about where. If we intervene on X2, we
can learn a precise aspect of f3, but will learn nothing about f2. How should we decide which action
to pick, given what we already know about f2 and f3?
The problem considered in this paper and our approach to solving it have a close connection to ideas
in Bayesian optimization [Jones et al., 1998, Osborne et al., 2009, Shahriari et al., 2016]. In Bayesian
optimization, the goal is to find the extremum of a function that is expensive to evaluate, possibly
exploiting known structure to speed up the search. Due to this expense, the information obtained
from each evaluation should be used efficiently. In contrast, however, in our setting we are not simply
interested in finding the extremum of an unknown function, but rather to learn the entire function (or
set of functions) in some sense. Tong and Koller [2000] considered a similar setting, but only treated
discrete variables.
Below, we begin with a formal definition of Structural Causal Models (Section 2). Section 3 states
the precise problem we are tackling. Section 4 provides a Bayesian formulation for inference in this
setting, which is then complemented by an active learning scheme to guide exploration (Section 5).
Section 6 provides empirical evaluations on synthetic toy examples.
2 Structural Causal Models
We now formally define Structural Causal Models (SCMs), the learning of which we will study in the
remainder. For notational simplicity, our definition deviates from the wider literature by including the
set of interventions modelled by the SCM.
Definition 1. Suppose that X1, . . . , XN , E1, . . . , EN are variables with each Xn and En taking
value in Xn = R and En = R respectively. We write X = (Xn)n=1,...,N for the vector of variables,
and X =
∏
n=1,...,N Xn for their domain, similarly for E and E . A Structural Causal Model (SCM)
M = (S, I,PE) is a tuple consisting of the following three quantities:
• S is a set of structural equations Xn = fn(Xpa(n), En), where pa(n) ⊂ {1, . . . , N} and
Xpa(n) is the vector of variables (Xm)m∈pa(n). We call the Xpa(n) the causal parents of Xn.
• I is a set of interventions. An intervention is a mathematical operation that replaces a
subset of the equations in S with equations setting the variables to specific constants (e. g.
Xn = 3). We write the intervention i ∈ I that intervenes on the subset of variables Xvar(i),
var(i) ⊆ {1, . . . , n}, setting them to values xval(i), as do(Xvar(i) = xval(i)) (we will often
abuse notation by writing do(i) instead). We write Sdo(i) for the resulting equations.
2
• PE is a distribution over the exogenous (aka. noise, unexplained) variables E taking value
in E = RN . This distribution is fixed and does not change due to interventions.
We will only consider acyclic1 SCMs. In this case, given any fixed value e of the variablesE, there is a
unique value x ∈ X such that each structural equation in S is satisfied;2 thus PE induces a distribution
over X via these unique solutions. We refer to this as the observational distribution of the SCM, and
write it as Pdo(∅)X (where ∅ signifies the ‘empty’ intervention). Under each intervention, the resulting
intervened structural equations Sdo(i) are also acyclic. It follows by the same reasoning as above that
the SCM implies a distribution Pdo(i)X over X . Therefore, once all of the parameters of the model
M have been fixed, it implies a set of distributions indexed by intervention: {Pdo(i)X : i ∈ I}. We
assume that the empty intervention ∅ is an element of I.
An important note is that once the graphical structure has been fixed, the parameters of the model
M are the functions fn and the distribution PE over the variables E. We will make the non-trivial
assumption that the structural equations are non-linear with additive Gaussian noise, meaning that
each equation is of the formXn = fn(Xpa(n))+En, where eachEn is a zero-mean Gaussian random
variable. This setting comes with results on the identifiability of the causal graph [Peters et al., 2011].
Example 1. Consider the following SCM M = (S, I,PE) represented by Figure 1, where
S =
X1 = 3 + E1,X2 = X21 + E2,X3 = 2X2 + sin(X2) + E3
 , I = {∅} ∪ {do(Xi = xi) | xi ∈ R, i = 1, 2},En ∼ N (0, σ2n), n = 1, 2, 3.
The observational distribution ofM factorises as Pdo(∅)X1X2X3 = P
do(∅)
X1
Pdo(∅)X2|X1P
do(∅)
X3|X2 . Under, for
instance, the intervention do(X2 = 0), the equation X2 = X21 + E2 in S is replaced by X2 = 0.
Under the distribution Pdo(X2=0)X1X2X3 , X1 and X3 are independent and X2 is degenerate.
3 Problem setup
Suppose there exists an SCMM = (S, I,PE) with S = {Xn = fn(Xpa(n)) + En : n = 1, . . . , N}
and PE ∼ N (0,Λ) where Λ is diagonal. We assume that the graphical structure is known, but the
functions fn themselves are not. For simplicity, we assume that Λ is known.3 We are given data D
drawn from the observational and a variety of interventional distributions ofM; each element of D
is a tuple (i, x), where x is an independent draw from Pdo(i)X . We are interested in two separate tasks.
Problem 1: Estimating M. Using D, learn functions f̂n such that the estimated model M̂ =
(Ŝ, I,PE) with Ŝ = {Xn = f̂n(Xpa(n)) + En : n = 1, . . . , N} is ‘close’ to the true modelM in
some sense. Part of this problem is to define a sensible notion of closeness between SCMs.
Problem 2: Active learning. We can select an intervention i ∈ I at some cost c(i) and observe
a single draw from Pdo(i)X . Which i should be selected to ensure the next estimation M̂ after
incorporating the new datum is as close toM as possible?
The rest of this section is devoted to defining a risk functional to provide a notion of closeness
between M̂ andM. There may be no single best way to define this notion of closeness, as desirable
properties may be dependent on particular use case.4 Consider, for instance, the following scenarios
in which the ultimate goal is to:
• Approximate each function fn so that a practitioner can visually interpret the relationship
between parents and children (e.g., identifying genes as ‘excitatory’ or ‘inhibitory’, or
identifying a threshold dosage at which a drug under medical trial is considered toxic.).
• Predict the result of an intervention that cannot be practically carried out, or is potentially
dangerous to do so (e.g., raising interest rates, or giving a patient a drug).
1That is, the directed graph with nodes {1, . . . , N} with edges n→ m if and only if n ∈ pa(m) is a DAG.
2That is, for any e ∈ E , there is a unique x ∈ X such that xn = fn(xpa(n), en) for each n. This can be seen
to be true by explicitly writing each xn as a function of the en by substituting the equations into one another.
3This assumption could be relaxed to a more Bayesian approach involving a prior over the covariance matrix,
which would reduce to running the algorithm derived in the following, but averaging its results over the posterior.
4The same is true in the case of causal graph learning. See e. g. de Jongh and Druzdzel [2009].
3
• Better control a system in a variety of environments or conditions (e.g., learning the dynamics
of a complex vehicle to be employed in variable conditions)
We will suppose that for each function fn with domain Xpa(n) =
∏
m∈pa(n) Xm we are supplied with
a probability measure Πn over Xpa(n) specifying the importance of learning the pointwise value of fn
at each input xpa(n) ∈ Xpa(n). That is, Πn puts large amounts of mass in areas that we should learn
fn precisely, small amounts of mass in areas that we should learn fn only approximately, and zero
mass in areas for which we do not care about learning fn at all. For the (estimated) function f̂n, we
use the risk functional Ln below. The weighted sum of these according to the importance of each
function gives rise to the total risk L, where f = (fn)n=1,...,N and f̂ = (f̂n)n=1,...,N are vectors of
the functions fn and f̂n, respectively.
Ln(f̂n||fn) =
∫
Xpa(n)
(
fn(x)− f̂n(x)
)2
dΠn(x), L(f̂ ||f) =
N∑
n=1
αnLn(f̂n||fn), αn ≥ 0.
We will assume for simplicity that αn = 1 for each n. Ln is also known as the Mean Integrated
Squared Error [Tsybakov, 2009] and in the case that Πn = Pdo(∅)Xpa(n) , this coincides with a typical
objective that would be minimised in a classical non-parametric statistical learning setting [Györfi
et al., 2006]. It is worth considering other possible risk functionals that could be used, since the
presence of the measures Πn is arguably somewhat arbitrary in the L that we consider. When learning
the parameters of a statistical model, a commonly used objective with many separate justifications is
to minimise the KL divergence KL[PX ||P̂X ] between the true data distribution PX and that implied
by the learned model, P̂X . SCMs do not imply a single distribution over the variables X, but rather
a family of distributions, one for each intervention: {Pdo(i)X : i ∈ I}. One may therefore wish to
consider a separate loss for each interventional distribution and, for example, uniformly bound these
losses over a subset of interventions I ′ ⊆ I of interest.
LiKL(f̂ ||f) = KL
[
Pdo(i)X ||P̂
do(i)
X
]
, LI
′
KL(f̂ ||f) = sup
i∈I′
LiKL(f̂ ||f).
Alternatively, one could replace the KL divergence with a different divergence measure or metric on
distributions. For instance, one could use the Maximum Mean Discrepancy (MMD) corresponding to
a characteristic kernel l [Sriperumbudur et al., 2008]
LiMMDl(f̂ ||f) = MMDl
[
Pdo(i)X ||P̂
do(i)
X
]
, LI
′
MMDl
(f̂ ||f) = sup
i∈I′
LiMMDl(f̂ ||f).
Though we do not analyse or derive active learning schemes for these risk functionals, they will be
used in Section 6 to evaluate our algorithm. We leave their consideration for future work.
4 A probabilistic approach to learning f
By taking a Bayesian approach to learning the vector of unknown functions f , Problem 1 can be
reduced to a series of independent regression problems between input and output domains Xpa(n)
and Xn for each n. A common choice of prior when learning functions is a Gaussian Process (GP)
[Rasmussen and Williams, 2005]. For each function fn, we will assume a zero mean GP prior with
kernel kn over the domain Xpa(n)5
fn ∼ GP(0, kn).
Recall that we are given a dataset D consisting of elements (i, x) where x ∼ Pdo(i)X . Let Dn be
the collection of marginal observations of (Xpa(n), Xn) drawn from any distribution in which Xn
is not intervened upon.6 Since by assumption Xn ∼ fn(Xpa(n)) + En where the distribution of
En ∼ N (0, σ2n) is known, each element (xpa(n), xn) of Dn represents an evaluation of fn at the
input point xpa(n) corrupted by Gaussian noise of known variance. Performing GP regression using
5No specific assumptions will be made on the choice of kn, which can be freely chosen to incorporate prior
knowledge about the functions (for instance, typical length scale of variation and magnitude). IfXn is parentless,
fn is an unknown constant for which we assume a 1-dimensional Gaussian prior with zero mean and variance
kn.
6That is, for any distribution Pdo(i)X for which Xn 6∈ var(i).
4
Dn as the data gives the posterior distribution over fn. By properties of Gaussians, this is also a GP
with distribution
fn|Dn ∼ GP(µfn|Dn , kfn|Dn),
where µfn|Dn and kfn|Dn(x, y) can be explicitly written in terms of kn and the data Dn (see
Appendix for details). The above procedure can be applied for each fn independently, giving a
posterior distribution over the vector of functions f .
Which f̂ should be chosen, given the posterior over f? Problem 1 demands that a single choice
f̂ be made when making the estimation M̂ ofM. For a fixed f̂ , the total risk L(f̂ ||f) is a random
variable (the randomness coming from the uncertain belief over f ). The expectation of this random
variable can be calculated and expressed in terms of the posterior covariance and mean functions of
each fn.
Lemma 1.
Ef |D
[
L(f̂ ||f)
]
=
N∑
n=1
αn
∫
Xpa(n)
(
f̂n(x)− µfn|Dn(x)
)2
+ kn|Dn(x, x)dΠn(x)
See Appendix for proof. This immediately implies the following result.
Lemma 2. Let µf |D be the tuple of functions (µfn|Dn)n=1,...,N . Then
µf |D = arg min
f̂
Ef |D
[
L(f̂ ||f)
]
That is, choosing f̂n to be the posterior mean of fn for each n minimises the expected total risk.
The uncertain distribution over f directly yields an estimate of the total risk once the optimal
f̂ = µf |D is chosen which, once the prior has been fixed, is purely a function of the data D. Denote
by R(D) = Ef |D
[
L(µf |D||f)
]
this expected total risk. Choosing the intervention i for which
R(D ∪ {(i, x)}) is expected to be smallest after making the new observation x from Pdo(i)X forms the
basis of the proposed active learning algorithm.
5 Active learning
In this section a myopic active learning algorithm is derived based on the GP belief of the functions fn
and the expected total riskR(D) described above. At each step in time, we select an intervention i at
cost c(i) and observe a single draw from the distribution Pdo(i)X . The goal is to select the intervention
i ∈ I which will reduce the expected total risk as much as possible, taking into account the cost c(i).
This problem is non-trivial for two main reasons.
1. The true distributions Pdo(i)X are unknown and therefore it is not possible to calculate the
true expected reduction in expected total risk given a proposed intervention.
2. There is a potentially large set of interventions that must be searched over.
Consider the first issue above. How will the expected total risk change if the intervention i is chosen
and a single new observation is drawn from Pdo(i)X ? If the new observation is x ∈ X , the new expected
total risk will beR(D ∪ {(i, x)}). Define the value of the intervention i to be the expected reduction
ofR after performing the intervention i, divided by the cost of i:
V (i|D) =
R(D)− E
x∼Pdo(i)X
R (D ∪ {(i, x)})
c(i)
(?)
The goal is to find the intervention with the largest value, but since Pdo(i)X is unknown it is not possible
to calculate the right-hand term in the numerator of (?). It is possible, however, to estimate this by
replacing Pdo(i)X in the expectation with the belief of the distribution based on the uncertain estimates
of each fn. In this next two parts of this section, two different methods are proposed that estimate
the expected total risk after performing each intervention. The first uses sampling, and requires a
brute-force search over the set of interventions. This may be appropriate when the set of possible
interventions is small enough that this is feasible. The second uses a form of dynamic programming,
enabling a search over a larger set of interventions more efficiently. The derived algorithm, however,
makes specific assumptions on the graph ofM and the set of interventions.
5
5.1 Sampling Algorithm 1 Sampling to estimate expected risk after
intervention
1: Input: Previously observed dataD, GP kernels kn for prior on fn,
number of samples T , proposed intervention i.
2: for t = 1, . . . , T do
3: Draw xt from predictive distribution P̂do(i)X
4: st = R(D ∪ {(i, xt)}): expected loss given new xt
5: end for
6: return 1T
∑T
t=1 st: estimated expected loss after intervention i.
Write P̃do(i)X for the belief of P
do(i)
X , taking
into account the full uncertainty over f .7
Since the belief over each fn at each input
xpa(n) ∈ Xpa(n) is Gaussian and the noise vari-
ables are additive and Gaussian, it is possible
to efficiently sample from P̃do(i)X . It is illus-
trated here how to draw from the estimated observational distribution P̃do(∅)X for notational conve-
nience, but the procedure for any other P̃do(i)X is essentially the same.
For any parentless variable, the structural equation is Xn = fn + En where fn ∼ N (µn|Dn , kn|Dn)
and En ∼ N (0, σ2n), and therefore Xn ∼ N (µn|Dn , kn|Dn + σ2n).
For any variable with parents, the structural equation is Xn = fn(Xpa(n)) + En where
fn ∼ GP(µn|Dn , kn|Dn) and En ∼ N (0, σ2n). Therefore the conditional distribution of a variable
given its parents is Xn|Xpa(n) ∼ N (µn|Dn(Xpa(n)), kn|Dn(Xpa(n), Xpa(n)) + σ2n). Observe that the
joint distribution P̃do(∅)X factorises as
P̃do(∅)X =
∏
n:pa(n)=∅
P̃do(∅)Xn
∏
n:pa(n) 6=∅
P̃do(∅)Xn|Xpa(n) . (∗)
Since drawing from each of the above factors amounts to drawing from a Gaussian distribution, it is
possible to efficiently sample from the entire joint distribution. By replacing Pdo(i)X with P̃
do(i)
X in
Equation (?) above, we arrive at an estimate of the expected total risk which can be estimated using
samples drawn from P̃do(i)X :
E
x∼P̃do(i)X
R(D ∪ {(i, x)}) ≈ 1
T
T∑
t=1
R(D ∪ {(i, xt)})
where each xt ∼ P̃do(i)X (see Algorithm 1). Finding the optimal intervention hence reduces to
computing the above quantity for each i ∈ I from which it is possible to estimate each V (i|D).
5.2 Dynamic programming Algorithm 2 Dynamic programming to estimate expected
risk after interventions (chain, interventions on all Xn≤m,
some m )
1: Input: Previously observed dataD, GP kernels kn, discretisations X̂n
of each Xn.
2: Pre-compute Un vectors and discrete approximations to conditional
probability distributions:
3: for n = 1, . . . , N do
4: If n = 1: P 1x1 ∝ P (x1) for x1 ∈ X̂1, else:
5: Pnxn−1,xn ∝ P (xn|xn−1) for xn−1 ∈ X̂n−1, xn ∈ X̂n
6: U currn = Rn(Dn)
7: Un(xn−1) for xn−1 ∈ X̂n−1
8: end for
9: Calculate new expected risk for all interventions:
10: for n = 1, . . . , N do
11: V = 0
12: for m = N − 1, . . . , n + 1 do
13: V = Pm(V + Um+1)
14: end for
15: Expected risk after intervention on variables Xm, m ≤ n:
16: ERn = V + U old1 + . . . + U
old
n
17: end for
18: return Vectors ERn giving estimated expected risks after all interven-
tions do(Xn = xn, Xn−1 = . . .)
If the set of interventions under considera-
tion exhibits structure that coincides with
that of the causal graph appropriately, it
is possible to estimate the value of many
interventions simultaneously. A specific
example is provided here of how this can
be done in the case that the causal graph
is a chain X1 → . . . → XN , and any in-
tervention intervenes on one variable and
everything upstream of it.8 A similar ex-
ample for chains in which all interventions
intervene on exactly one variable is pro-
vided in the Appendix.
The crux of this approach is the fact
that the posterior covariance function of
a Gaussian Process is only a function
of the inputs of the conditioning data,
not of the outputs. That is, writing
7In contrast to P̂do(i)X , which is the estimated distribution once a particular choice for f̂ is made.
8That is, any intervention is of the form do(Xn = xn, n ≤ m) for some m. This is equivalent to the case
that all interventions act on a single variable, but only variables downstream of this are observable.
6
X0 X1 X2 X3 X4
(a)
X0
X1
X2
X3
X4
(b)
Figure 2: The causal graphs of the SCMs (a)M1 and (b)M2 used in the experiments (Section 6).
Rn(Dn) = Efn|Dn [Ln(µfn|Dn , fn)] =
∫
Xpa(n)
kn|Dn(x, x)dΠn(x) for the contribution to the ex-
pected total risk due to estimating function fn, and writing Dn = {(xspa(n), x
s
n) : s = 1 . . . , |Dn|},
it follows thatRn is only actually a function of the xspa(n) (or, for parentless variables, just the size
of the dataset |Dn|). Consider the intervention i = do(Xm = xm, Xm−1 = . . .) that sets Xm = xm
and all variables upstream of Xm to arbitrary values. When a new observation x ∼ Pdo(i)X is made,
this only provides new information about the functions fn for n > m, since i intervenes on Xn
for n ≤ m. Let U currn = Rn(Dn) for n ≤ m be the current contributions to the expected total
risk. Define, for n > m, the following shorthand for the contribution to the new expected total risk
function made by fn if a new observation of fn at the input point xn−1 is made:
Un(xn−1) = Rn(Dn ∪ {(xn−1, xn)}) for any value xn, n > m.
It follows that the estimated expected total risk after performing the intervention i decomposes thus:
E
x∼P̃do(i)X
[R(D ∪ {(i, x)})] =
m∑
n=1
U currn + Ex∼P̃do(i)X
[
N∑
n=m+1
Un(xn−1)
]
. (†)
By exploiting a factorisation of P̃do(i)X similar to (∗) and discretely approximating the continuous
domains Xn, it is possible to reduce evaluating the right hand side of (†) to a series of matrix
multiplications and additions (see Appendix for details). This series of operations can be vectorised
to allow calculation of (†) for many xm simultaneously. Moreover, many of the initial calculations
can be cached and used to speed up calculation over different values of m. See Algorithm 2.
6 Experiments
Figure 3 shows the results of running the proposed methods on two synthetic example SCMsM1
andM2, for which the graphs are given by Figures 2a and 2b respectively. In each case, structural
equations consisting of sines and cosines of the parent variables were fixed, giving two sets of
structural equations S1 and S2. All noise variables in both SCMs were fixed to have variance
0.1. The interventions I1 for the chain SCMM1 were chosen to be all interventions of the form
do(Xm = xm, Xm−1 = . . .) with xm ∈ [−6, 6], such thatM1 satisfies the conditions set out in the
description of the dynamic programming algorithm. The interventions I2 for the non-chain SCM
M2 were defined to be all interventions on single variables do(Xn = xn) with xn ∈ [−6, 6]. The
total risk function for each experiment was chosen by setting Πn to be the uniform distribution on
the domain [−6, 6]| pa(n)| and αn = 1 for each n. All GP kernels used were Radial Basis Function
kernels with bandwidth parameter 1. All costs c(i) were assumed to be equal.
In each experiment, learning was initiated with no data. For learningM1, the proposed algorithms
were compared against the strategies of only drawing from the observational distribution ofM1 and
of selecting an intervention uniformly at random. Since the dynamic programming algorithm could
not be used for learningM2, we could only test the sampling algorithm. Uniform discretisations
I ′1 and I ′2 of the intervention sets were made of total size 250 each. These were used as the sets of
interventions to search over using the sampling algorithm.
The following quantities were used to evaluate the performance of the algorithms at each point in
time: the true total risk incurred by choosing f̂ to be the vector of GP posterior means; the maximum
and median of the set of KL divergences KL[Pdo(i)X ||P̂
do(i)
X ] calculated for each intervention i ∈ I ′∗
in the discretised intervention sets; the maximum and median of the set of MMDs (corresponding
to a Radial Basis Function kernel l with bandwidth 1) MMDl[Pdo(i)X ||P̂
do(i)
X ] calculated for each
intervention i ∈ I ′∗ in the discretised intervention sets. See Figure 3 for results.
7
observe only
random intervention
Algorithm 1Algorithm
2
0 20 40
0
5
10
Iterations
Tr
ue
lo
ss
maxi∈I′1mediani∈I′1
0 20 40
0
2
4
6
Iterations
lo
g
(K
L
[P
d
o
(
i
)
X
||P̂
d
o
(
i
)
X
])
maxi∈I′1mediani∈I′1
0 20 40
−4
−2
0
Iterations
lo
g
(M
M
D
[P
d
o
(
i
)
X
||P̂
d
o
(
i
)
X
])
observe only
random intervention
Algorithm 1
0 20 40
3
4
5
Iterations
Tr
ue
lo
ss
maxi∈I′2mediani∈I′2
0 20 40
1
2
3
4
Iterations
lo
g
(K
L
[P
d
o
(
i
)
X
||P̂
d
o
(
i
)
X
])
maxi∈I′2
mediani∈I′2
0 20 40
−4
−2
0
Iterations
lo
g
(M
M
D
[P
d
o
(
i
)
X
||P̂
d
o
(
i
)
X
])
Figure 3: Actively choosing informative interventions speeds up learning. The proposed methods
(Algorithm 1, Algorithm 2) outperform only observing and randomly intervening by each metric
of evaluation. Top and bottom row show results of experiments on learningM1 andM2 respectively.
Experimental details described in Section 6. Each experiment was performed many times; faded lines
represent results from single trials, bold lines represent averages of these single trials.
7 Discussion and future directions
There are many ways in which the work presented here could be incrementally furthered: it may
be possible to find efficient ways to search over the set of interventions subject to less restrictive
assumptions than those made for Algorithm 2; different distributions over the noise variables could
be considered, in which case an approximation may need to be made when regressing to find the
posterior distribution over each fn; one could try to relax the additive noise assumption altogether.
Other natural extensions include estimating the value of an intervention based on reasoning multiple
steps into the future, or considering the implications of a constrained budget.
Although the proposed algorithms seem to perform well on the synthetic toy examples considered,
it remains to be seen whether this method, suitably extended, would similarly perform well on a
convincing real-world problem. It is hard to find suitable real-world problems where convincing
ground truth (for the functional relationships) exists, which is why we believe that it is sensible to
assay such methods on synthetic data where performance can be accurately measured.
A perhaps more fundamental issue that was raised and not tackled is the fact that it is not clear how
best one should even define what it means to learn an SCM, or the parameters thereof. We proposed
supi∈I′ L
i
KL(f̂ ||f) and supi∈I′ LiMMDl(f̂ ||f) for a suitable set of interventions I
′ as potentially
more principled objectives to minimise than the total risk functional considered here. Interestingly,
the derived algorithms do reduce these quantities in the experiments considered, though this was in
no way an explicit objective. Future directions of research include trying to understand whether these,
or other objectives, give rise to tractable methods for parameter estimation in causal models and for
selecting interventions in active settings, and under what assumptions mathematical guarantees can
be made.
8
References
K. A. Bollen. Structural equations with latent variables. John Wiley & Sons, 2014.
M. de Jongh and M. J. Druzdzel. A comparison of structural distance measures for causal Bayesian network
models. Recent Advances in Intelligent Information Systems, Challenging Problems of Science, Computer
Science series, pages 443–456, 2009.
F. Eberhardt. Causal discovery as a game. In NIPS Causality: Objectives and Assessment, pages 87–96, 2010.
L. Györfi, M. Kohler, A. Krzyzak, and H. Walk. A distribution-free theory of nonparametric regression. Springer
Science & Business Media, 2006.
A. Hyttinen, F. Eberhardt, and P. O. Hoyer. Experiment selection for causal discovery. Journal of Machine
Learning Research, 14(1):3041–3071, 2013.
D. R. Jones, M. Schonlau, and W. J. Welch. Efficient global optimization of expensive black-box functions.
Journal of Global optimization, 13(4):455–492, 1998.
J. Mooij, J. Peters, D. Janzing, J. Zscheischler, and B. Schölkopf. Distinguishing cause from effect using
observational data: methods and benchmarks. Journal of Machine Learning Research, 17(32):1–102, 2016.
URL http://jmlr.org/papers/volume17/14-518/14-518.pdf.
M. A. Osborne, R. Garnett, and S. J. Roberts. Gaussian processes for global optimization. In in LION. Citeseer,
2009.
J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York, NY, 2nd edition,
2009.
J. Peters, J. Mooij, D. Janzing, and B. Schölkopf. Identifiability of causal graphs using functional models. In
F. Cozman and A. Pfeffer, editors, 27th Conference on Uncertainty in Artificial Intelligence (UAI 2011), pages
589–598, Corvallis, OR, USA, 2011. AUAI Press.
C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation and
Machine Learning). MIT Press, 2005.
B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the human out of the loop: A review
of Bayesian optimization. Proceedings of the IEEE, 104(1):148–175, 2016.
P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT Press, Cambridge, MA, 2nd
edition, 2000.
B. Sriperumbudur, A. Gretton, K. Fukumizu, G. Lanckriet, and B. Schölkopf. Injective Hilbert Space Embeddings
of Probability Measures. In R. Servedio and T. Zhang, editors, Proceedings of the 21st Annual Conference on
Learning Theory (COLT 2008), pages 111–122, Madison, WI, USA, 2008. Omnipress.
S. Tong and D. Koller. Active learning for parameter estimation in bayesian networks. In T. K. Leen, T. G.
Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 647–653.
2000.
S. Tong and D. Koller. Active learning for structure in Bayesian networks. In International joint conference on
artificial intelligence, volume 17, pages 863–869. Lawrence Erlbaum Associates LDT, 2001.
A. B. Tsybakov. Introduction to nonparametric estimation. Revised and extended from the 2004 French original.
Translated by Vladimir Zaiats, 2009.
9
A Appendix
A.1 Posterior distribution of fn given Dn
The prior over fn is a Gaussian Process with zero mean and covariance function kn
fn ∼ GP(0, kn).
Let Dn = {(xspa(n), xsn) : s = 1 . . . , |Dn|} and let K be the matrix with entries
Kst = ki(x
s
pa(n), x
t
pa(n)).
Suppose that the Gaussian noise variable En has variance σ2n. Then the posterior mean function µfn|Dn and
posterior covariance function kfn|Dn can be written in closed form as
µfn|Dn(x) =
|Dn|∑
s,t=1
kn(x, x
s
pa(n))(K + σ
2
n)
−1
st x
t
n,
kfn|Dn(x, y) = kn(x, y)−
|Dn|∑
s,t=1
kn(x, x
s
pa(n))(K + σ
2
nI)
−1
st kn(y, x
t
pa(n)).
A.2 Proof of Lemma 1
Proof. The expected total risk can be written in full form as
Ef |D
[
L(f̂ ||f)
]
= Ef |D
[
N∑
n=1
αn
∫
Xpa(n)
(
f̂n(x)− fn(x)
)2
dΠn(x)
]
=
N∑
n=1
αn
∫
Xpa(n)
Ef |D
[(
f̂n(x)− fn(x)
)2]
dΠn(x)
=
N∑
n=1
αn
∫
Xpa(n)
Efn|Dn
[(
f̂n(x)− fn(x)
)2]
dΠn(x).
Under the posterior distribution given Dn, we can decompose fn as the sum of its posterior mean and a zero
mean Gaussian Process:
fn|Dn = µfn|Dn + gn, gn ∼ GP(0, kfn|Dn).
We can therefore rewrite the expected total risk as
Ef |D
[
L(f̂ ||f)
]
=
N∑
n=1
αn
∫
Xpa(n)
Egn|Dn
[(
f̂n(x)− µfn|Dn(x)− gn(x)
)2]
dΠn(x)
=
N∑
n=1
αn
∫
Xpa(n)
Egn|Dn
[(
f̂n(x)− µfn|Dn(x)
)2
−
(
f̂n(x)− µfn|Dn(x)
)
gn(x) + gn(x)
2
]
dΠn(x)
=
N∑
n=1
αn
∫
Xpa(n)
(
f̂n(x)− µfn|Dn(x)
)2
−
(
f̂n(x)− µfn|Dn(x)
)
Egn|Dn [gn(x)] + Egn|Dn
[
gn(x)
2] dΠn(x)
=
N∑
n=1
αn
∫
Xpa(n)
(
f̂n(x)− µfn|Dn(x)
)2
+ kfn|Dn(x, x)dΠn(x).
A.3 Derivation of Algorithm 2
Under the intervention i = do(Xm = x∗m, Xm−1 = . . .), the distribution P̃
do(i)
X factorises as:
P̃do(i)X =
∏
n≤m
δXn=x∗n
∏
n≥m+1
P̃Xn|Xn−1 ,
10
where
Xn|Xn−1 ∼ N
(
fn(Xn−1), σ
2
n
)
∼ N
(
µfn|Dn(Xn−1), kfn|Dn(Xn−1, Xn−1) + σ
2
n
)
.
The quantity we are trying to calculate can hence be written
E
x∼P̃do(i)
X
[R(D ∪ {(i, x)})] =
m∑
n=1
U currn + Ex∼P̃do(i)
X
[
N∑
n=m+1
Un(xn−1)
]
=
m∑
n=1
U currn +
∫
X
N∑
n=m+1
Un(xn−1) dP̃do(i)X (x)
=
m∑
n=1
U currn +
N−1∑
n=m
∫
Xn
Un+1(xn) dP̃do(i)Xn (xn).
Now, since P̃do(i)Xm = δXm=x∗m , observe that we can write∫
Xm
Um+1(xm) dP̃do(i)Xm (xm) = Um+1(x
∗
m).
For any n > m, we have∫
Xn
Un+1(xn) dP̃do(i)Xn (xn)
=
∫
Xm
. . .
∫
Xn
Un+1(xn) dP̃do(i)Xn|Xn−1(xn|xn−1) . . . dP̃
do(i)
Xm+1|Xm
(xm+1|xm)dP̃do(i)Xm (xm).
Hence if we define the following quantities recursively
VN−1(xN−2) =
∫
XN−1
UN (xN−1) dP̃do(i)XN−1|XN−2(xN−1|xN−2),
Vn(xn−1) =
∫
Xn
Vn+1(xn) + Un+1(xn) dP̃do(i)Xn|Xn−1(xn|xn−1) n = N − 2, . . .m+ 1,
it follows that
Vm+1(x
∗
m) =
N−1∑
n=m
∫
Xn
Un+1(xn) dP̃do(i)Xn (xn).
We can approximate calculation of Vm+1(x∗m) for many x∗m simultaneously by discretising each Xn
into a set of points x1n, x2n, . . . , xDnn . Define Pn to be the matrix with normalised rows such that
Pnij ∝ p̃
do(i)
Xn|Xn−1
(xjn|xin−1), where p̃do(i) is the density of P̃do(i) with respect to the Lebesgue measure
on X . Define un to be the vector with entries uni = Un(xin−1). Then, if we recursively define
vN−1 = PN−1uN ,
vn = Pn(vn+1 + un+1) n = N − 2, . . .m+ 1,
it follows that vm+1i ≈ Vm+1(x
i
m).
A.4 Another Dynamic Programming Algorithm: chain, single variable interventions.
Similar reasoning to the above can be used to derive a dynamic programming scheme to calculate the estimated
total risk after a proposed intervention i = do(Xm = x∗m) (i. e. intervening on a single variable) for many x∗m
simultaneously. This is summarised by Algorithm 3.
Under this intervention, the joint distribution factorises as
P̃do(i)X = P̃X1
m−1∏
n=2
P̃Xn|Xn−1δXm=x∗m
N∏
n=m+1
P̃Xn|Xn−1 .
11
When we intervene on a single variable Xm, we learn something new about all functions except fm. We can
write the expected new total risk, the quantity we want to evaluate, as
E
x∼P̃do(i)
X
[R(D ∪ {(i, x)})] = U1 + Ex∼P̃do(i)
X
[
m−1∑
n=2
Un(xn−1)
]
+ U currm + Ex∼P̃do(i)
X
[
N∑
n=m+1
Un(xn−1)
]
.
Each of the expectations above can be calculated recursively in a similar fashion to the strategy employed above.
If we define
VN−1(xN−2) =
∫
XN−1
UN (xN−1) dP̃do(i)XN−1|XN−2(xN−1|xN−2),
Vn(xn−1) =
∫
Xn
Vn+1(xn) + Un+1(xn) dP̃do(i)Xn|Xn−1(xn|xn−1), n = N − 2, . . .m+ 1,
it follows that
Vm+1(x
∗
m) = Ex∼P̃do(i)
X
[
N∑
n=m+1
Un(xn−1)
]
.
Similarly, defining
Vm−2(xm−3) =
∫
Xm−2
Um−1(xm−2) dP̃do(i)Xm−2|Xm−3(xm−2|xm−3),
Vn(xn−1) =
∫
Xn
Vn+1(xn) + Un+1(xn) dP̃do(i)Xn|Xn−1(xn|xn−1), n = m− 3, . . . 2,
V1 =
∫
X1
V2(x1) + U2(x1) dP̃do(i)X1 (x1),
it follows that
V1 = Ex∼P̃do(i)
X
[
m−1∑
n=2
Un(xn−1)
]
.
As before, we can approximate calculation of Vm+1(x∗m) for many x∗m simultaneously by discretising each
Xn into a set of points x1n, x2n, . . . , xDnn . Define Pn to be the matrix with normalised rows such that
Pnij ∝ p̃
do(i)
Xn|Xn−1
(xjn|xin−1) for n > 1, where p̃do(i) is the density of P̃do(i) with respect to the Lebesgue
measure on X . Define P 1 to be the normalised vector with P 1i ∝ p̃X1(xi1). Define un to be the vector with
entries uni = Un(x
i
n−1). Then, if we recursively define
vN−1 = PN−1uN ,
vn = Pn(vn+1 + un+1) n = N − 2, . . .m+ 1,
it follows that vm+1i ≈ Vm+1(x
i
m). We must also estimate V1. If we define
vm−2 = Pm−2um−2,
vn = Pn(vn+1 + un+1) n = m− 3, . . . 2,
v1 = P 1
ᵀ
(v2 + u2)
then it follows that v1 ≈ V1.
12
Algorithm 3 Dynamic programming to estimate expected risk after interventions (chain, interventions on
single variable Xm)
1: Input: Previously observed dataD, GP kernels kn, discretisations X̂n of each Xn.
2: Pre-compute Un vectors and discrete approximations to conditional probability distributions:
3: for n = 1, . . . , N do
4: Pnxn−1,xn ∝ P (xn|xn−1) for xn−1 ∈ X̂n−1, xn ∈ X̂n
5: U currn = Rn(Dn)
6: Un(xn−1) for xn−1 ∈ X̂n−1
7: end for
8: Calculate expected loss for all interventions on each variable in turn:
9: for m = 1, . . . , N do
10: V = 0
11: for n = N − 1, . . . ,m + 1 do
12: V = Pn(V + Un)
13: end for
14: V ′ = 0
15: for n = m− 1, . . . , 2 do
16: V ′ = Pn(V ′ + Un)
17: end for
18: Expected risk after intervention on variable m: ERm = V + V ′ + U1 + U currm
19: end for
20: return Vectors ERn giving estimated expected risks after interventions for all interventions on Xn.
13

