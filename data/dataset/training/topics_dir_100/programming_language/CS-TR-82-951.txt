DCCCIII bcr 1982 Report  No. STAN-CS-82-951
Five Paradigm Shifts in Programming  Language
Design and Their Realization in Viron,
a Dataflow Programming  Environment
bY
vaughl Pratt
Department of Computer Science
Stanford University
Stanford, CA 94305
Five Paradigm Shifts in Programming Language Oesign
and their Realization in Viron,
a Oataf low Programming Environment
Vaughan Pratt
Stanford University
Abstract
We describe five paradigm shifts in programming language
design, some old and some  relatively new, namely Effect to
Entity, Serial to Parallel. Partition Types to Predicate Types.
Computable to Dcfinabic, and Syntactic Consistency to Semantic
Consistency. WC argue for the adoption of each.  WC exhibit a
programming language,  Viron, that capitalizes on these shifts.
‘This research was supported by NSF gmnt number MCS82-05451.
1. Background
This is a companion paper to [P&32].  These two papers
started out as one paper, but when  a clear division emerged
between the language proper and its theoretical foundations it
was decided to publish the foundations as a separate and scif-
contained thcorcticai paper, thereby removing much foundational
clutter from the language paper that was not essential on a first
reading.
It is possible to read and understand the present paper with
only a iimitcd understanding of the foundations. ‘Ihe
foundations come  into play when  imp!cmcnting  the system,
writing the manual, convincing oncscif  of the consistency of the
ideas in the prcscnt  paper, or cstnblishing  the soundness and/or
compictcncss  of proof systems for proving the correctness of
programs in this language.  ‘ihc grcatcr  the accuracy with which
each of thcsc  tasks arc to be pcrformcd. the more ciosciy must
one examine our foundations.a
In related work on the ianguagc side, our most visible
intciiectuai creditors  are IIcwitt (Actors) [I Iew], Iloarc (CSP)
[IIoa], and Dennis  (VAL) [AI)]. On the foundational side, they
arc Kahn and McQucen  (Streams) [Knh,Khl]  and I3rock  and
Ackerman (Scenarios) [f3A]. ‘I’wo notable contributors to the
foundations of concurrency arc Petri [Pet] and Miincr [Mill,
whose influential work we mention here only to iiiustratc that
there is not yet one central  generally accepted model of
concurrent proccsscg
‘i‘his language is the programming language for the Viron
computing en\ ironmcnt, a gral)tiics-itltcnsivc user inlcrfacc being
devcioped  at Stanford by the author that takes as its starting point
the philosophy of the Star environment [I,ip] but that goes
considerably beyond  it in its attention both to semantic
foundations and realistic graphics. WC will call both the interface
and its programming language Viron, relying on context to
determine which is meant.
This paper deals only with the semantics of Viron. For
prcscntation  purposes we will adopt an abstract syntax in the
spirit of McCarthy’s Miisp. We believe however  that such an
abstract syntax is too spartan for a comfortable and efficient user
interface. Other  papers will dcscribc syntaxes appropriate for
given applications, including a three-dimensional graphical syntax
wcii suited to the Viron interface, as well as a textual syntax that
amounts to a formal approximation to Ilngiish.
As to novelty, the paradigm shifts vary considerably in their
individual no\city,  with the effect-to-entity and serial-to-parallel
paradigm shifts being  the least novel.  lhc emphasis  of this paper
is however not on the paradigms taken alone, but on their
harmonious combination in a single programming language.
2. Sumrhary of the Paradigm Shifts
WC give here a one-paragraph summary of each shift.
From EjJect  to /:‘ntity. I arge objects arc made as mobile as
small, so that they can bc casiiy created, destroyed, and moved
around as entities instead of being  opcratcd on pieccmcai as a
static collection of small objects.
From Serial to I’LJNII~~I. Constructs intended for parallel
computation arc also used to subsume  both serial and data
constructs, achieving a simplification of the control structures of
the language as well as providing a framework for organizing the
data structures.
From Par&ion  TJ~PCS  fo Predicate 7jpes.  The partition view
of types partitions the univcrsc into blocks constituting the atomic
types, so that every  object  has a dclinitc type, namely the block
containing it. Ihc predicate view considers a type to be just
another prcdicatc, wilh no a priori notion of /hr type of an object.
pi-orn Comprrlrrblc  fo I~r~nablr.  fTffcctivcricss is a trnditionai
prcrc<luisItc  for the admission of constructs  to programming
languages. Weakening Lhis prcrcquisitc  to mcrc definability
expands the language’s exprcssivcncss thereby  improving
communication between programmer and computer.
From Swactic  Consistency lo Semantic Consistency.
Consistency is traditionally cnforccd  by syntactic restriction, c.g.
via type structure. A recently dcveiopcd alternative popularized
by Ilana Scott takes the dual approach of resolving
inconsistencies via semantic expansion. as done for modcis  of the
untyped  h cAcuius.  WC argue that  the Iattcr approach simplifies
language structure.
We now expand on each of these paradigm shifts.
3. From Effect to Entity
Traditionally computation has been viewed, both by
programmers and by mathematicians, as being performed  for its
effect. The classical basic instruction is the state-modifying
assignment statement.  The classical model of computation is an
automaton (finite-state, pushdown, Turing machine, etc.) with a
state or configuration transition function that describes how each
state or configuration leads to its successor.
This view of computation is gradually being superseded by a
more entity or object oriented view. The computing universe  is
regarded as being populated with entities. The dynamics of the
universe is no longer described in terms of the state-to-state
transitions of automata but rather in terms of the site-to-site
transitions of entities, for which two of the more popular settings
are recursive function evaluation and dataflow architecture.
There are several forces  acting to bring about this paradigm
shift. I-irst, mathematics tends to emphasize entities over effects:
consider the classical mathematical model, the algebraic structure,
consisting of a set together with operations and relations.
Second, there is the increasing prevalence of parallelism, as
nets connect computers  into inhcrcntly  parallel configurations, as
cheap  microprocessors start showing up several to a system, and
ZLS  silicon compilers that translate algorithms into VLSI designs
become a reality. ‘Ihc scqucncc-of-states model, well suited to
serial computation, rapidly becomes tinworkable  in the presence
of parallelism.
Third, there seems to be a psychological advantage to being
able to cxternalile concepts, that is, to treat them all conceptually
as nouns instead of verbs, relations, etc. We speculate that this
advantage comes from the simplicity of the “typelessness”
resulting from complete  externalization. We will raise the issue of
typelessness  again in more detail in a later section.
4. From Serial to Parallel
4.1. Which is the Basic Concept?
One issue here is which is the more central concept, serial or
parallel computation? ‘Ihcrc is an analogous question for
determinism versus nondctcrminism. For both questions there
are waJs of formaliLin2  the question to make the answer come out
either way. For the latter question the predominant view of
automata thcsc days, which is to represent them in terms of
constraints on their  state transitions, favors nondctcrminism. with
determinism being  merely a special case of nondeterminism.
Thus instead of dividing automata into two classes, the
dctcrminislic  and the nondctcrministic, WC treat determinism  as a
property a~nondctcrministic  automaton might or might not have,
the lingui4;ilc  awkwnrdtlcss of the prefix  “mm”  notwithstanding.
‘I his lmnt of view has been firmly supported  by the  csscntially
uni~cnally  accepted formal dclinition of “automaton” for the
past two decades.
Our own intuition about serial versus parallel is that serial is a
special cast of parallel, rather  than vice versa. Ifowevcr unlike
the situation with dctcrminism  vs. nondctcrminism, there  has not
been  a similarly universally acccptcd  formal definition of the
notions “serial” and “l~~111cl.” This makes it much harder to
rcsolvc  the question by appeal to a defmition.
One can set how hard it is to rclatc the two notions formally
b y  considcrrng h o w  they rclatc in current programming
languages. I ct us consider  C (under Unix), Ada, and Ifoarc’s
CSP, which are among the better  known languages offering
parallelism. In each of these languages the basic computational
paradigms are serial. Parallelism is introduced by modclling the
computing universe as a set of serial computers communicating
with each other. (In the cast of C under Unix concurrency and
inter-process communication, like I/O, is supplied by Unix kernel
calls, and is not part of the C language proper. However it is a
fine example of a language in current use that in practice does
offer concurrency.)
These examples strongly suggest that the relation between the
two notions is that serial computation is a necessary prerequisite
to defining a notion of parallel computation.
This view of parallelism obviously does not reflect what
actually happens inside a machine. Within any “serial” computer
one can observe parallelism  at many levels in its implementation:
in the many electrons that flow through a wire (to go to an
absurdly low level). in the many wires making up a bus, and in
the many microprocessors that can be found in today’s large
mainframes, to name just some examples. For whatever reason
the programming language of a mainframe is serial, it is not
because the hardware itself is serial.
What WC would like is a model of computation that not only
reflected this ubiquity of parallelism but that at the same time
subsumed the notion of serial computation, making it merely a
special case of parallel computation. The chief advantage of this
would be in simplifying both our theoretical models of
computation and our programming languages. A program would
then  be serial more as an accident  than through not using parallel
constructs, just as determinism  arises more by accident than by
avoidance of nondcterministic constructs.
Ilcwitt [IIcw] has advocated just this simplification of
programming languages, in the form of his mcssagc-passing
Actors theory. Although some  of the details differ (actors have no
output), the underlying rationale  appears to bc similar.
4.2. Need for a Formal Model
Comparing the situation once  again with (non)dctcrminism,
thcrc is still one missing ingredient, namely a formal semantics
that converts the prcccdcnco of parallel over serial from a matter
of taste to a mathematical definition. The candidates for a model
of parallel computation that wc  take at all seriouc;ly  arc Petri nets
Ipct], Milner’s Calculus of Concurrent Systems (C’CS) [Mill, the
Kahn-MacQuecn model of determinate  processes [KM], the
fjrock-Ackerman  Scenarios model [l3A], and our own model of
processes [Pra82].  Among thcsc models the greatest unity, and
the longest history, can bc found among [KM], [f]A],  and [Pra82],
which togcthcr  constitute  a monotonically improving sequence  of
models (in that order). WC consider  the resulting model to supply
exactly the missing ingredient.
The Kahn-MacQuecn  model defines an n-port process to be
an n-ary relation on the set of all histories (sequences of data).
This model was developed only for dctcrminntc  proccsscs, and
was suspected by its authors of not being directly usable for
nondctcrminism. This suspicion was formally confimlcd  by an
cnlightcning  countcrcxample  due to Rrock and Ackerman,  w h o
also proposed  the ~xccss;~r~  modllication  to the nmdcl to extend
it to nondctcrminism [],A]. ‘Ihe  modilication  was to introduce
inter-hislory temporal  prcccdencc  information. ‘lhc Drock-
Ackerman model was adopted and further extended by the
present author [f’ra82]  to cater for process composition in a
satisfactorily formal way, and to support an algebraic  view of
process composition analogous to the algcbtaic view of serial-
program composition mandated by the structured-programming
movement.
f\csidcs the grcatcr  economy of subsuming the serial with the
parallel, thcrc is also the issue  of irrctcvant  scrinli/ation  forced in
a scnal  language. Thus x:-a; y: = b is a pair of assignments
2
whose order must be given even though it is clearly not needed.
This issue can bc met  piecemeal by adding yet more constructs to
the language, e.g. parallel assignment. IIowever  starting with an
inherently parallel language from the beginning solves this and
related problems just as effectively and more generally.
The Viron programming language is noteworthy in having no
explicitly serial constructs.
4.3. Data Structures
Auxiliary to the vertical integration of processes into Viron’s
control structures is its incorporation into the data structures as
well. The denotational semantics of processes given in [Pra82]
imbues them with the status of object, permitting processes to be
thought of as data with the same “charter of rights” [Pop] or
“mobility” pra79]  as integers.
Taking this development one step further, we have chosen to
make processes not the organic molcculcs of our language but
rather  the elcmcntary  particles. That is, every datum, whether of
the complexity normally associated with processes, or as simple as
a character or an integer, is defined to be a process. In this we are
again following Hewitt [IIew], in whose development
“everything” is an actor.
The main conceptual obstacle to thinking of an atom as a
process is that atoms seem too simple  to be thought of in this way.
However essentially the same argument was made for millennia
excluding zero as a legitimatenumber. Yet today zero is almost
universally acknowledged to bc, though less than 1, no less a
number  than 1.
Of course one  might come up with an unconvincing behavior
for numbers viewed as processes. IIewitt embeds  the knowledge
that 3 + 2 = 5 and 3x2 = 6 in the actor that is the number 3, which
. makes 3 a much more complex process than seems intuitively
nccessaly. The Viron idea of a number, and more generally of
any atom. as a process is that. although the atom does output
something in response to each input, the output is independent of
the value  of the input and consists of the atom itself.
The main reason for this choice is to fit in with our
extensional view of processes, in which two processes with the
same behavior must be the same  process, If an atom was
unrcsponsivc  all atoms would collapse to the same  atom. A useM
fringe benefit of this convention is that, following our
straightforward definition of addition, the addition of a number n
to an array of numbers results in the addition of n to each  of the
elements of the array, as will be seen  in the account below of
Viron.
4.4. The Process Compiler
One might well ask why can’t the notion of process be
excluded from the programming language proper and made a
pa” of the subroutine library, on the principle that the
programming language only need supply a basis from which to
extend via the subroutine library. All process-oriented  notions in
Unix arc supplied  in this way, for cxa~~~plc.  (It should bc rcalir.cd
that C was dcvcloped  by the dcvclopcrs of Unix as part of the
Unix effort: thus this expulsion of the notion of process to the
library was a consciously made decision in this case, not an
accident resulting from an inherited  language.)
A plausible motivation for putting the notion in the language
is that parallelism  is not definable in purely serial terms,  much as
one might argue that nondeterminism is not dcfinablc  using only
deterministic concepts. f Iowevcr this argument assumes that the
library is a true language extension in the sense  that all its
functions could have  been written in the language.  ‘I’his is
actually not the case in the Unix cxamplc. which requires  non-C
assembly code in its system calls in order  to access  the kcrncl,
3
which is the source of parallelism in Unix. Thus it is possible to
introduce parallelism into the language via the library cvcn  if
parallelism  is not definable using just the basic language, given
that the library is permitted to step outside the basic language.
Our actual motivation is that we want to expose parallelism to
thc’optimizing  compiler. The state of the art of parallelism forces
it to be an interpreted concept, due to its having a purely
operational definition, one which admits only literal
interpretation of parallel constructs by an interpretive machine.
If a more abstract definition of parallelism is given, it becomes
possible for an optimizing compiler to choose from a variety of
equivalent implementations in compiling a given parallel
construct. The dclinition should be maximally abstract: only
necessary detail should be retained in the definition.
5. From Partition to Predicate Types
Another paradigm shift has to do with the nature of types.
The  partition view of types considers “type” to bc a function from
the universe onto a partition of that univcrsc; each individual is
mapped to the block of that partition containing that individual.
Thus type(3) = integer, type(3.14) = real, type([3,1,4])  = list,
typc(cos) = real->real, and so on.
In contrast to the partition view, the predicate view of types
abandons the attempt to keep types disjoint, ‘and permits each
individual to bc of many types. For example 3 may
simultaneously bc of type real, integer, positive integer, integer
mod 4, mod 5, mod 6, etc. You yourself may simultaneously be a
human, a teacher, an American, a Democrat, a Prcsbqtcrian,  a
non-smoker, and so on. There is no such thing in the physical
world as TIIE type of an object, although any given context may
suggest a particular prcdicatc  as being the most appropriate
predicate to be called the type of that object in that contcx t.
The partition view can admittedly be made to work in the
simple environments that come with today’s programming
languages. However  as the environment gets  richer the partition
view becomes progressively more intractable. Imagine a
programming language in which for cvcry  pair i.j of integers  with
i<j there is a type i..j of integers in the interval from i to j. A pure
partition view of types would require that the integer 3 not be one
individual but many, one for each interval containing 3. This may
seem  laughable, yet it is a logical extension  of the more readily
accepted idea that the real 3.0 is distinct from the intcgcr  3. (It is
noteworthy  that Pascal adopts a prcdicatc-like approach to its
treatment of the range subtype, while  remaining partition-
oriented elsewhere, thereby avoiding this problem in its more
extreme forms.)
The predicate approach to types simplifies this by having only
one individual recognizable as 3, common to all intervals
containing 3. This individual can even be identified with the
individual 3.0 if one wishes to make the integers  a subset of the
reals, a simplifying view of the integer-real relationship which has
much to recommend it.
‘Ihe predicate view has a certain amount of support from
modern mathematical  logic. There  has been much study of
logical thcorics  incorporating various notions of type. lndccd
l~usscll’s approach to coIltrolling  the logical paradoxes of’f:regc’s
theory was to introduce a t)‘pe hierarchy  of the partition kind.
However  this approach was eventually  supcrsedcd  by the typeless
theories of Zcrmclo-f;raenkel  and f<crnays-Gocdcl. Admittedly
the Wernays-Gocdcl theory did go so far as to postulate a two-type
hierarchy of sets and classes, but it is noteworthy that the
“typcless” (but not prcdicatclcss)  Zcrmclo-Fracnkcl thcoty is the
one that today is taken (modulo details) as the formal definition
of set theory, which in turn is acccptcd  by many mathematicians
as supplying the formal basis for all of mathematics. While
~crmclo-f;racnkcl  set theory may from time to time bc subjcctcd
to attacks, it is rarely if cvcr because  of its typclessncss.
The entity oriented approach that WC wish to explore will be Those fragments may grow in size and number as the supply
characterized by the “typclcssness” of the predicate approach, in of algorithms improves:  all that noneffectiveness does here is to
that all entities will belong to a single domain. Thus our approach prevent a complete irnplemcntation  of Viron. The programmer
will have the flavor of Lisp’s typelessness, though with what we should accept such incompleteness with the same good grace that
feel is a sounder rationale than has been advanced by the Lisp the mathematician accepts it for his logical tools, which inevitably
community to date for typelessness. must be incomplete.
6. From Computable to Definable
It is unthinkable today to propose a noneffective  model for a
computing cnvironmcnt. 110~ would you implement it? It is
unimplementable by definition. Ncverthclcss WC feel that this
insistence on effectiveness produces inarticulate programmers.
WC propose to include noneffective  concepts  in our models to
simultaneously  enhance the expressive power of and simplify the
language.
‘To begin with, consider the set of even  integers and the set of
primes.  These are objects that are very natural to be able to refer
to in a program: certainly in natural language they are referred to
all the time.  IIaving  these objects in one’s domain is only
noncffectivc  if one  insists on a traditional representation of sets as
bit vectors  or linked lists of clcmcnts. If those two objects were all
there were in the domain one bit would serve to represent each.
Ilowcvcr suppose we close this tiny domain under Boolean
operations. WC now want to manipulate Roolcan combinations of
thcsc two sets. Can this be done cffcctivcly’? Yes: equality .
bctwecn expressions is decidable since  it reduces trivially to the
decision problem for two-variable propositional calculus, with the
two sets playing the role of the two variables. Four bits suffice
(exercise: and arc necessary)  to represent the sixteen possible
Boolean  combinations of these two sets.
Now let us go a little further and add a unary operation to the
language that adds one to cbery clcmcnt  in a set. Suddenly WC
can cuprcss  infinitely many distinct subsets of the integers, even
without the evens.  Ncvcrthcless can WC still compute in. this
language? In particular can we always decide whether two
expressions denote  the same  set? Maybe, maybe not (let us know
if you lind out), but clearly we cannot continue to add such
“reasonable” constructs to the language for long without arriving
at a non-cffcctivc domain, one in which not even  equality is
decidable.
R. I’opplcstonc  ran into this predicament when drawing up
his “charter of nghts” for data [Pop], whcrc his notion of datum
went  beyond just integers and boolcans. 1 ic wanted cvcry datum,
including objects such as arrays and functions, to bc assignable to
a variable, passable as a parameter, and returnable  as the value of
a procedure.
-1Iowcvcr he did not require that it be possible to tell whether
two data wcrc equal.  More generally, hc did not require that
procedures behave  the same with different representations of the
same  data. Why? Because  equality is undecidable for functions,
inrcr alia.
W C cons ide r  Popplcstone’s  charter of rights to be
substanda;d. Under  Ihal charter data is not abstract. A
prograrnn;ing  language should assign abstractness  higher  priority
than cffcctivcncss.  ‘l‘h~s  is a logical cx(cnsion of the programming
rule. “Make  it work bcforc you make  it fast.” ‘l’hc  cxtcnsion  is to
treat (he programming language as being primarily a descriptive
tool, and only secondarily as a medium for achieving performance
or even effectiveness.
Our approach to implementing a noneffective domain is to
implcmcnt  succinctly specified decidable language fragments.
‘1%~  key hcrc is the cxistcncc of easily rccogni7cd decidable
fragments  of undccidablc  languages. We have developed this
idea 111  [t’ra80]  and [Pra81]  for the cast of program vcrilication.
‘lhc idea is not specific to vcrilication howcvcr, and can bc
applied just as readily to cxccution in a noncffcctive  domain. *
Making the break not only with performance but with
effectiveness removes a source of worry from the programmer
much as having an undo key reassures the user of a word
processor. ‘I’hc programmer  can get on with the job without the
distraction of whether a given way of saying something will run
fast, or even will run at all.
There is a feeling in some programming circles that the
burden  of performance should be placed on the compiler. This is
possible up to a point, although no compiler can assume the full
burden, since  there are always new algorithms to bc discovered.
Our position is that exactly this situation holds for effectiveness as
well as for performance. A compiler can deal with some  of the
issues of finding an effective way to execute a program, but no
one compiler can discover every such cffectivc way on its own, it
must sometimes  depend on the programmer. Just as the
impossibility of the perfect optimizer dots not imply the
usclcssncss of optimiLcrs,  so dots the impossibility of the perfect
automatic programmer not imply the uselessness of compilers
that can find effective methods in many cases.
7. From Syntactic to Semantic Consistency
Effectiveness is only one of the inhibitors of articulate
expression. The current approaches to controlling inconsistency
constitute another. Russell’s  theory of types was designed to’
avoid the inconsistencies Iiusscll  and others found in Frege’s
logical theories. ‘Ihc introduction of a hierarchy of types into the
X-calculus serves a similar end.
In contrast to these syntically cautious approaches are the
syntactically casual languages of Schocnfmkcl [Sch] (cornbinatory
logic) and Church [Chu] (the untyped X calculus). I Icre
paradovcs  of the traditional kind may bc obtained at the drop of a
hat: for example cithcr  language may cxprcss the seemingly
nonsensical concept of a fixed point of the integer  successor
function. Yet the languages arc more “user-friendly”  than ones
which introduce typing restrictions aimed at preventing such
paradoxes. Arc such languages mcrcly syntactic curiosities devoid
of rcfcrcntial  significance,  or can they be considered  to actually
dcnotc, dcspitc  the inconsistencies? Surely they could not denote,
or they would not bc inconsistent.
Dana Scott has worked out the details of an approach to
making semantic sense  of paradoxical and hence ostcnsivcly
mcaninglcss  languages, which is to computation as complex
numbers arc to electrial impedance. The idea is to augment an
otherwise normal domain with fuzzy or information-lacking
elements. Fuzziness is represented with a partial ordering of the
domain in which x dominating y indicates that x has more
information than y, which can bc rephrased without using the
word “information” by saying that y might on closer examination
turn out to bc x.
A very simple cxamplc of the shift from syntactic to semantic
consistency is provided by Boolean  circuits. A simple syntactic
constraint on a circuit that guarantees prcdictablc  static behavior
is that it bc acyclic. ‘Ihis condition may bc rclaxcd with caution to
yield more interesting behaviors. IIowever  if in the interests of
simplicity all conditions on circuits are dropped. we can then
connect the output of an invcrtcr (a dcvicc rcalijing the unary
13oolcan  operation of conil~lcnlcnlation)  to its input. This
prolidcs  a simple physical model of the logical paradox implicit
in the equation x = -x.
Classically a paradox means an inconsistency, which in turn
means there is no model of the paradox - the universe should
disappear when we feed the invcrtcr’s output back to its input!
This  actually dots happen, at least in the sense that the universe
of pure truth values no longer provides an adequate account of
the circuit behavior. With the feedback loop the inverter
functions like an amplifier. with negative feedback, with its
common input and output stabilizing at a voltage somewhere
between logical 0 and 1. ‘I’his intermediate voltage is not a part of
the O-l 13oolcan  universe, but it is a part of a more detailed model
that admits invalid or uninformative data in addition to the
regular data. Thus if we postulate three values, 0, *, and 1, with 0
and 1 considered maximally informative and * uninformative,
and take the response of an invcrter to the inputs 0, *, 1 to be
respectively 1, *, 0, then  we may solve x = -x with x = *.
The key feature of this simple example is that we have moved
from a syntactic to a semantic solution to the problem of paradox.
Instead of relying on the absence of cycles or some other syntactic
constraint to prevent  paradoxes, Scott’s approach is instead to
expand the universe to account for and hcncc dispose of
paradoxes.
Scott’s approach was motivated by just the sort of “user-
friendly” syntactic sloppiness that actually arises in real
programming languages. such % the ability in Algol 60 to pass as
a parameter to the function f any function including f itself. More
recently Saul Kripkc [Kri] has made a very similar proposal to the
philosophical community with a paradox-explaining theory of
truth that has been received with remarkable  enthusiasm by the
philosophical community. Kripkc’s thcoty  of truth is founded on
the cxistcncc of fixpoints of monotone  functionals  in a complete
partial order, just as with Scott’s theory.
It should be observed that the Scott-Strache) school of
mathematical semantics that dcvcloped  at Oxford has made two
distinct contributions to programming semantics: the notion of
dcnotational semantics as a homomorphism from expressions to
. values, and the notion of the information order  as a basis for a
fixpoint-of-monotone-functional semantics for resolving
paradoxes. Yet littlc attempt is made by computer scientists to
distinguish lhcsc two contributions,  and the term “dcnotational
semantics” is frcqucntly  applied to both of them  as a single
package, with the implication that the latter is a vital component
of the former. In fact one can carry out a very comprchcnsive
program of semantics  without any rcfcrcncc to an information
ordering. ‘l‘his is done for cxamplc in such progr,am logic schools
as algorithmic logic, dynamic logic, and temporal logic, whcrc the
semantics is of a homomorphic character  but with no dcpcndcnce
on ordcrcd  domains. 11 is also done in [Pra82], the foundations on
which the semantics of Viron arc built.
When  paradoxes cmcrge however  in response to lax syntax,
Scott’s information order bccomcs  a key ingredient  of a successful- semantics.
In the commonest account of Scott’s theory (not Scott’s own
account howcvcr). based on complctc  partial orders (cpo’s, partial
orders in which every directed set has a sup), the maxlmal
elements of the cpo can bc considered the “normal” or “ideal”
elcmcnts. the objects WC consider to normally populate the
uni4crsc. ‘I‘hc other  clcmcnts arc approximations (0 the ideal
clcments.  in the same  scnsc  as intervals  with rational endpoints on
the real lint arc :~l~l~roxin~alions  to reals. In rhc cl’0  a c c o u n t ,
unlike in Scott’s account. thcrc NC no ovcrspccificd  clcmcnts
containing more information  than the idcal clcmcnts.
The simple expressions of the language, c.g.  the numerals,
arithmetic cxprcssions over numerals,  etc., arc considered  to
denote ideal elements.  Ilowcvcr some of the more complex
expressions will only dcnotc approximations. In particular the
paradox i al expressions are guaranteed to denote
approximations: no matter how closely you inspect a paradoxical
elcmcnt  you cannot tell what idcal clcmcnt it should denote. By
withholding information in this way, the model prevents  you
from arriving al a contradiction. I;or cxamplc an expression
denoting a fiucd point of the successor fun&on  will dcnotc  an
approximation to integers, usUally one that approximates  all
5
integers (integer “bottom”).
The main advantage of Scott’s approach is the way it can
simplify the language, which no longer needs to be sensitive to
inconsistcncics. On the other hand it dots complicate  the model.
Yet even here there is an advantage, for the model can be used to
permit  the relocation of the implementability boundary from
syntax to semantics, a novel concept for programming languages
but one that we believe can be used to good effect. Let us see
how this works.
Normally a system designer chooses an implementable
language, and as new needs arise augments the language with
additional implcmcntable  constructs. With Scottish models it is
possible to fix an absurdly over-expressive  yet simple language
once and for all, and to augment not the language but the
interpretation of the language, by increasing the information
available to the language intcrprcter about the interpretations of
expressions in the language. (Intcrprctation I, mapping
expressions to domain elements, is considered  an augmentation of
interpretation J when I dominates J, i.e. I(e) dominates J(c) for all
expressions e in the language.)
As a trivial example, one could start out with a semantic
function that mapped numerals to integers, and all other
expressions  to the bottom clcmcnt of the domain. Although the
language might have addition, that function would in effect start
out ;tF the cvcrywhcrc undefined function. Then  one could add
some set of computable arithmetic functions by raising from
bottom to integers the Interpretations of all expressions
containing only those functions and numerals, at the same time
providing the necessary implcmcntation  of this increase. At some
point one might raise the interpretations of “set of evens” and
“set of primes” to the appropriate sets, also ideal clcmcnts. As
algorithms for evaluating various linguistic fragments of set
theory came to light one could implement  them and so raise the
interpretations of corresponding expressions. (If dcsircd one
might also add heuristics for noncffcctivc fragments, thcrcby
further raising SOI~C  inlcrprctations, though by ill-characterized
amounts for an ill-characterized subset of the language.)
The advantage of putting “language subsetting” in the
semantics instead of in the syntax is that it decouples language
development from implementability considerations. ‘Ihis in turn
makes it possible  to make the full language available immediately
for dcvelopmcnt  of algorithms without waiting for full
implcmcntation  support for those algorithms. ‘lhcsc would
sometimes bc noncffcctivc algorithms when they rcfcrrcd to as-
yet undcfincd  functions, but they still would scrvc the useful
purpose of specifying problems that could then bc rewritten
manually in an cffcctivc sublanguagc.
A language as powerful as this can be built up until it
subsumes any given requirements language. From this point of
view implementation  reduces to translation within the language to
achieve a raising of the intcrprclation (meaning) of the translated
expression. The  raising happens  bccausc, for example, some
noncffectivc  function or concept (e.g. quantilication)  is translated
to a more cffcctivc form. ‘Ihe definition of corrcctncss  of an
implementation  is that it dominate the expression it was
translated from (whcrc the ordering bctwccn expressions is just
that induced by the ordering on the inlcrprc(alions  of those
csprcssions,  i.c. for cxprcssions  c and f, c<f when f(c)<l(r)).
If one views an automatic programmer  as a fLinction  mapping
expressions to expressions in this language then  the automatic
programmer  is correct just when it is monotonic.
This one-language view of the relation between  requirements
and implementation  is appealingly simple.  Yet it fits naturally
into the real world of requirements and implctncr~lations,  which
typically form a hierarchy in which implcmcntations  turn into
rcquircmcnts as one programs from top to bottom. ‘he
homogcncity  of our rcquircmcnts  and ilnplcnicntalions  siml)lifies
this dual view of rcquirclncnts/progr~tns  by expressing them  all
. in a common language.
8. Lisp as a Benchmark
Lisp is a good benchmark against which to measure progress
in language design. Despite its age (approaching the quarter
century mark) it still ranks as one of the primary sources of
insight into the principles of programming language design.
Lisp, at least pure Lisp, emphasizes entity over effect. Lisp
treats its complex data, lists and (to an extent)  functions, as
objects to bc moved around the computing cnvironmcnt  with the
same  mobility as integers, putting demands on the storage
management algorithms beyond what suffices for a domain of say
integers. Furthermore L,isp emphasizes the homogeneity or
typelessness of the predicate approach to typing.
However pure Lisp does not gracefillly handle the process-
oriented notions of state. memory, coroulinc,  or concurrency,
concepts that are at best feebly captured in a domain of
recursively defined functions and functionals  on a basis of lists
and atonts. It is usual to think of these as only recently being
demanded, but we arc of the opinion that their need has always
been present, and that only the lack of the necessary concepts  has
prevented the Lisp designers and users from recognizing these
needs as process-oriented needs long ago. WC bclicve that the
impurities of Lisp - PROG. SETQ,  GOTO, RPLACA, RPLACD,
etc.  - arose in response to such needs, and met them by reverting
from the entity paradigm to the effect paradigm, where it was
already understood intuitively how to implement process oriented
notions. The price for lhis step backwards was the loss of
mathematical meaning for the concepts  of I ,isp, to the extent that
being effect-oriented leads to clumsier definitions than being
entity-oriented.
The similarity between pure Lisp and Viron is that both arc
entity oriented. The difference  is that Viron entities are
spccilically intended to model the notions of state, memory,
coroutines, and concurrency.
9. Foundations
As stated in the introduction, this is the second paper of a
series whose  first paper pra82] described the mathematical
foundations for a notion of process. We repeat  here the bare
definitions.
There are two views of processes, internal and external. The
internal view is the more detailed  one, and dcpcnds  on the notion
of a net of processes,  without regard for what actual data flows
bctwccn them. A11 proccsscs have two countable  sets of input and
output ports. 11.12,1,  ,,.. and O,,O,,O,  ,..,., all but finitely many of
which will normally go unused. (‘I his arrangement avoids the
enG,umbrance  of a syntactic classification of processes according to
their  port structure.) ‘l’he net consists of zero or more disjoint
communication links each connecting one output port to one
input port; each port is connected to at most one link. A net can
be studied in its own right, or as a means of implementing a
process. in which cast certain of its processes are associated with
ports of the implcmcnted process.
In [Pra82]  each port-associated process was assumed to USC
only one of its own 1301  IS. One minor improvcntcnt  WC make hcrc
lo that model IS to collccl ~111  lhc port-associalcd proccsscs of a net
into a single  process. called lhc c.utcr’ior’pl‘oc’c~ss  of the net. Port I.
of this process  corresponds to port 0. of the implcmcnted process!
in the sense that data sent  by the nt!t to 1. will appear as output
from port 0. of the process implcmcntcd b$ this net. Dually data
arriving at dart 1. of rhc  implemented processes cntcrs  the net of
the implementat?on  of that process via port Oj of the net’s exterior
process.
To ask how the exterior  process of net N is implemented is to
ask what network  the process implcmenlcd  by N is embedded in.
‘Ibis viewpoint rcflcrls  a certain symmetry bctwcen the cxtcrior
and interior of processes that sharpens the role of the process as
network interface.
A link is to be thought of not in the information theoretic
sense of a channel having capacity, or affecting its messages, but
rather merely as an arbitrary boundary between  two processes. A
datum flowing between  two processes must at some time cross
that boundary: this is called an net event. It either happens or
dots not happen: there is no probability, distortion, delay, or
queuing associated with the event. lmperfcctions  in the net must
always be associated with processes. A transmission link that
accumulates, permutes or distorts messages must be modclled as a
process in our nets. The question of whether  a link has a finite
queue,  an infinite queue, or no queue, is translated to the
question as to what buffering mechanisms a process provides at
each of its input and output ports. ‘Ihis in turn is captured
abstractly in the “reliability”  of a process - finite buffers will
reveal themselves  through the possibility of intpcrfcct behavior.
Formally. a net event is a link-datum pair, interpreted as the
traversing of that link by that datum. A net trace is a partially
ordered multiset  of net events, interpreted as a possible
computation, with the order specifying which events necessarily
preceded  which other events in time. Necessary temporal
precedence is a primitive notion in this theory.  A net behavior is a
set of net traces. Ihcsc three notions, net cvcnt, net trace, and net
behavior, constitute the internal view of a process.
In the external view, a process event is a port-datum pair, a
process trace is a partially ordered multisct of process events, and
a process behavior is a set of process traces. (The tight
correspondcncc  bctwcen the internal and external views of a
process should bc noted.)
‘Ihcrc  are two connections to be made between the internal
and external views of a process. Network traces need to be
consistent with the behavior of the constituent  processes of the
net, achieved  by requiring that the restriction of each net trace to
any constituent (i.e. non-exterior) process of the net be a process
trace of that process. And the process behavior imp!cmentcd bjl a
net is obtained as the restriction of the net behavior to the exterior
process, with 1 and 0 interchanged. In both cases “rcstricLion”
involves a renaming of links to ports, selection of the relevant
even&.  and corresponding restriction of the partial order; details
are in [Pra82].
We adopt an extensional view of processes, identifying them
with their  process behavior, just as one idcntillcs a function with
its graph (set of ordered pairs). ‘Ihus  WC ntay abbreviate  “process
behavior implemented by a net” to “process implcmcntcd by a
net.”
A network1  of n processes numbered  1 through n defines an
n-ary operation  mapping each n-tuplc  of processes to the process
implemented  by that network having those n processes as
constituents. The net-dcfinabk  operations arc those operations on
proccsscs definable in this way. A net a&bra  is any set of
processes closed under the net-definable operations.
10. The Programming Language Viron
The goal of Viron is to bc maximally usefill with a minimum
of machinery.
10.1. At the interface
In the word-object  dichotomy, the concept of “language”
seems  to belong as much to the word as to the object  it names. In
this paper however WC shall play down the syntactic part of Viron,
leaving that to other  papers. and focus instead on Viron’s domain
of discourse.
lThcrc is a distinction made in ~Pr;r82]  between
have since decided to consider  only simple nets.
simple general nets. We
In the intcrcsts  of brevity and readability, and in keeping with
the introductory nature of this paper, the description of Viron will
remain at an informal level. A more rigorous treatment of the
language would entail the use of a formal description language. It
is our intent to use Viron to describe itself formally, just as an
informal description of ZF set theory may be formalized in the
language of ZF. (One reason for not using ZF instead of Viron is
that they have quite different inconsistency-avoidance
mechanisms. Viron evades inconsistency by being noncommittal,
cautiously raising its definitions as far as its algorithms permit,
whereas %I; sets itself up with fingers crossed as a fixed target that
either is or is not consistent.)
The Viron universe is simply a set of processes, ranging in
complexity from simple atoms through functional objects such as
application and composition to large and/or complex systems.
The Viron user  interacts with processes: he manipulates them,
watches them, talks to them, listens to them, and discourses on
them (with an occasional break for coffee). No -one of these
activities is intended to be the dominant one, nor is this list of
what one can do with processes intended to be complete.
Abstract programming languages generally start out with one
or another basic combining primitive. One popular such
primitive is application; the domain of discourse of such a
language is called a combinafory  algebra, and the language itself is
characterized as being npplicafise.  All other combining operators,
or combintlfors,  are provided as elements of the combinatoty
algebra. Church’s h-calculus [Chu] provides a familiar example
of a combinatory algebra;  the set of proofs of propositional
calculus, with modus ponens as the analogue of application,
provides another.
The informal interface between Viron and its user takes the
place of application in an applicative language. The precise
definition of the processes themselves  makes it possible to
provide a formal definition of any given mode of user interaction
. on demand. Manipulation of processes may be formalized in
terms of whatever combinators are supplied by the manipulation
language - composition when  processes can be assembled into a
net, application when date can be input to processes, etc.
Watching a process execute can be described formally in terms of
viewing a trace. Talking to a process is the same as inputting data
to a process, while listening to one is the convcrsc  - output from a
process is sent to the user. Discourse on p~occsses  charactcrizcs  a
user-Viron talk-listen loop since  all transactions are themselves
processes.
The fact that all data and computing agents arc proccsscs
need not bc pointed out to the beginner, who will encounter
numbers,  lists, functions, and so on well before  the general  notion
of a process  makes its appearance. I Iowcver since this paper is for
a more sophisticated audicncc  WC can afford to make the basica
process representation explicit.
The least likely candidates for representation as processes are
atomic data such as integers and characters. Somewhat more
plausible are functions, which amount to memorylcss processes.
We have chosen to represent n-ary functions as processes that
send one datum to output I when  one datum has been consumed
at each of the first n inputs, the output being the dcsircd function
of ihc consumed inputs.
10.2. Basic Data
Having ensured  that functions arc processes, to make an atom
a process it suffices to make it a function, which we do by
defining the atom b to be the constant function b satisfying b(x)
= b for all x. (Type circularity is no problem  hcrc since  we are
not using a conventional type hierarchy of ftlnctions  and
functionals.)
Atoms: WC take as the atoms of Viron the set % of integers.
(It is tempting to have other  atoms such as characters, but the
7
notion of “set of characters” is not sufficiently
its inclusion in Viron as a primitive.)
universal to justify
Arithmetic functions: the rational finctions (addition,
subtraction, multiplication, division) are provided. Division is a
partial function in the sense that it absorbs its two arguments
without response when the divisor is zero.
The arithmetic functions are defined not only on integers but
on all functions. (Recall that an integer is an atom and hence a
function.) The sum of two functions is coordinatewise addition
(on the intersection of the domains of the two functions), and
similarly for the other functions. It can be seen  that this is
consistent with the normal  behavior of addition on integers; thus
2+ 3 = 5 either for the usual reason or because as functions the
two constant functions whose values arc respectively 2 and 3 sum
to the constant function whose value is 5. Taking this further, it
can be seen that the sum of a list and an integer is the result of
adding that integer  to the elements of the list, since the integer
can be viewed as a constant function, with a domain that is a
superset  of the domain of the list (an initial segment of the
positive integers). This is a happy circumstance, as it coincides
with what programmers generally mean by the sum of a list and
an integer, e.g. as in APL.
n-dimensional Array: a function
rectangular subspace of Z”.
with domain a contiguous
List: a l-dimensional array starting at 1. (This agrees with the
definition  of “list” on p.43 of Maciane  and Birkhoff mai],  and
makes no attempt to relate lists to pointers. Making the pointer
implementation of lists visible to the user, despite  its obvious
advantages in terms of control, makes the list concept unduly
complicated.)
Filter:
function.
a restriction of the identity function to a partial
Set: a one-input two-output process whose two outputs in
effect implcmcnt  two filters with complementary  domains, i.e. a
steering mechanism. As such a set is not a function only in that it
has two ac;ynchronous  outputs (as opposed to one output
synchronously yielding a pair).
Predicate: a set.
Record: a function with domain a finite set of symbols.
Memory cell: a process that when  sent any value on its second
input outputs the most recent value seen  on its first. (Cells are
defined more formally in [Pra82].) It is a nontrivial example of a
process that  is not a function.
One may link n processes into a net with the help of various
n-ary functions for that purpose. I:or example thcrc  is a binary
function Sequcnce(a,b),  which yields a process implemented  by a
net that connects its input 1 to a’s input 1, a’s output 1 to b’s input
1, and b’s output 1 to output 1 of Sequcnce(a,b).  (This is made
more formal using the definition of process composition in
[Pra82].)
The quaternary function Fork(a.b,c,d)  yields a process
implcmcntcd by a net that connects its input 1 to a’s input 1, a’s
output I to b’s input 1, a’s output 2 to c’s input 1, b’s output 1 to
d’s input 1, c’s output 1 to d’s input 2, and d’s output 1 to output 1
of I :ork(a.b,c,d).
‘Ihc ternary lilnction  I,oop(a,b,c)  connects its input 1 to that
of a, a’s output 1 to b’s input 1, b’s output 1 to c’s input I, b’s
output 2 to output 1 of Loop(a,b,c),  and c’s output 1 to a’s input
2.
The process Merge passes all data received on inpu&  1 and 2
straight through to output 1, merging them subject to no
particular rule.
The above process-combining operations form a useful,
though surely incomplete, basis for parallel programming.
Ilowcvcr  they can be seen  to easily  subsume  the conventional
serial constructs as well. if WC consider “flow of control” in a
serial machine  to mean the flow of the cntirc  state of the machine
as a single giant datum through a net. Thus “begin a; b end” may
be written as Sequence(a,b).  “if p then a else b” as
Fork(p.a,b,Merge),  and “while p do a” as Loop(Mcrge,p,a),
where the predicate p is as defined above (a set, i.e. a pair of
filters).
There is no explicit notion of type declaration in Viron.
IIowever one can always insert a filter into a data stream to
achieve the effect of a declaration, which it does by blocking any
object not of that type. If type error reporting is desired, this may
be accomplished by using a set instead of a filter and routing the
false output to a suitable error handler at run time. Compile time
type error reporting amounts to testing at compile time whether it
is possible for any errors to reach the error handler. (As usual
with compile time computation, such a test may need to be
conservative, sometimes predicting errors when none can happen,
but never overlooking a possible error.)
Recursion is introduced into our model at the semantic level
via the notion of least fixed point. (It is noteworthy that in our
semantics the notion of minimality, whether of fixed points or
anything else, is not used in the definition of Loop and hence of
“while.“) Operationally, this becomes the usual substitution of
the process definition for the recursive use (invocation) of that
process.
The notion of a passing a parameter to a function corresponds
in Viron to the notion of inputting data to a process. In this sense
only call by value is provided. Call by reference and call by name
arc avoided as being  too unpredictable: it is difficult to prove a
program correct when nearby prograps  hold pointers to objects
of that program. Call by need should be treated as an
implementation issue. The effects of these parameter-passing
disciplines are best handled by passing objects of higher type by
value.
11. Impact of the Paradigm Shifts on Viron
We now give a more detailed discussion of the impact of the
paradigm shifts on the structure of the language. Much of the
impact should already bc apparent given the discussion of the
paradigm shifts and the nature of the language. Thus this section
is just a short Viron-specific supplement to the main discussion of
the shifts.
11 .l. From Effect to Entity
Viron is in one sense an applicative language. Every
communication path is brought out into the open, instead of
bcifig hidden by references to shared variables. Applicative
languages arc normally inherently entity oriented. IIowever in
another sense  Viron is effect-oriented, in that data entering  a
process can have an effect on that process. Yet the typical effect
is to alter the set and/or arrangement of entities existing inside
the process.
Thus Yiron is at once entity oriented, like an applicative
language. and effect  oriented. like an impcrativc  language.
I3y making cvcry concept an object, and by having processes
that can take proccsscs ‘as their input, we get the effect of a
language of higher type. This provides a mathematically
attractive way of getting expressive power that in other  languages
either cannot be attained or is strained for with a family of
esoteric parameter-passing mechanisms.
11.2. From Partition to Predicate Types
The role of types in conventional programming languages is
on the one hand to make clearer  to the reader what the program
does, and on the other to tell the compiler what data
representation and type of operations to use in the translation. In
Viron, filters, which arise naturally in Viron as simple process
objects, are used to achieve both of these ends. This takes much
of the mystery out of types, and at the same time provides a more
flexible approach to types in that any Viron-definable predicate
may be used as a type.
11.3. From Serial to Parallel
The net-definable operations provide all the needed control
structures. This makes Viron an easy language to teach - once the
notion of a net  is in place, a variety of control structures, whether
serial or parallel, can bc introduced simply by exhibiting the
appropriate net.
In Viron every datum, regardless of its complexity, is a
process. Thus adoption of parallelism over serialism permits a
uniform treatment of data, whether atomic, structured, or active.
11.4. From Effectiveness to Definability
One result of replacing effectiveness by delinability is that it
makes sense to think of Viron as a requirements language as well
as a programming language. In this respect an implementation of
Viron can be vicwcd  a5 either a compiler/interpreter of Viron or
an inference engine. ‘Ihc  boundary between execution and
inference is not a sharp one, and we feel is best characterized in
terms of how much optimization  is performed. Code motion,
where an operation that the program shows as executing n times
is actually only executed  once thanks to the optimizer, is clearly
an execution-related notion. Induction, where An operation that
is shown as executing over all natural numbers is reduced to one
step, is clearly part of infercncing. Yet the difference between
these two “optimizations”  is really only quantitative, if we accept
inlinity as a quantity. In between,  WC have in logic the notion of
arguing by cases, which is indistinguishable from a program set
up to deal with each of those cases.
Another conscqucnce  of dccmphasizing  effectiveness is that it
changes the status of lazy evaluation.  Normally lazy evaluation is
thought of as part of the operational or interpretive semantics of
the language, giving it the extra power needed  to compute with
infinite objects without going into an infinite loop trying to
generate the object  all at once. In Viron lazy evaluation
disappears as a -language  concept, resurfacing if at all as an
implementation concept. The Viron user is not meant to be
aware (other than via performance) of whether  lazy evaluation or
some other method is used to deal with such infinite objects as the
set of all primes. It should be possible to interchange such
methods and have no effect on the semantics of any Viron
program.
11.5. From Syntactic to Semantic Consistency
In Viron it is unnecessary to restrict how expressions may be
built up and where data may bc sent. One consequence of this is
that a single least fixed point operator is possible in Viron, rather
than a fixed point operator at each type as would be required in a
more traditionally cautious language. Viron is to the untyped X-
calculus as a cautious language would be to the typed X-calculus.
12. Conclusion
WC have proposed several changes to the way in which we
view our programming languages, only some of which are
prcscnlly  advocated by others. These changes are not all obvious
ones  IO make. Nevcrthcless WC belicvc that they are all changes
for the better. We believe our arguments defending them  to be
sound. Ihus  the changes certainly should not be rejected without
first disposing of our arguments.
13. Bibliography
[AD] Ackerman, W.B. and J.B. Dennis, A Value-Oriented
Algorithmic Language, MIT LCS TR-218, June  13, 1979.
[DA] Rrock, J.L>.  and W.B. Ackerman, Scenarios: A Model of
Non-Determinate Computation. In Lecture Notes in Computer
Science, 107: Formalization  of Programming Concepts, J. Diaz
and I. Ramos, Eds.. Springer-Vcrlag, New York, 1981.
[Chu] Church, A., The Calculi of Lambda-conversion.
Princeton University Press, 1941.
[I lew] flcwitt,  C. and 1I.G. Baker, Laws for Communicating
Parallel Proccsscs,  IFIP 77, 987-992, North-I Iolland, Amsterdam,
1977.
[Iloa] lloare,  C.A.R., Communicating Sequential Processes,
CACM, 21. 8, 666-672, August, 1978,
[Kah] Kahn, G., The Semantics of a Simple Language for
Parallel Programming, IFIP 74, North-IIolland,  Amsterdam,
1974.
[KM] Kahn, G. and D.J3.  MacQueen,  Coroutines and
Networks of Parallel Processes,  II:IP 77, 993-998, North-I Iolland,
Amsterdam, 1977.
[Kri] Kripke. S., Outline of a Theory of Truth, J. of Phil., 690-
716,1975.
[Lip] Star Graphics: An Object-Oricntcd Implementation,
SIGGRAf’LI-82 Conference Proceedings, 115-124,  ACM, July
1982.
[Mac] Maclane,  S., and G. Birkhoff, Algebra, Macmillan,
NY, 1967.
[Mill klilncr, R . , A Calculus of Communicating Systems,
Spnngcr-Vcrlag  I-ccturc  Notes in Computer  Science, 92, 1980.
[Mor] Morris, J.11..  ‘l‘lpes arc Not Sets; 1st Annual ACM
S! mposium on f’rinciplcs of f’rogramming Languages, Boston,
MA, October  1973.
[f’et] f’ctri, CA.,  Introduclion  to Gcncral  Net T h e o r y ,
Springer-Vcrlag Lecture Xotcs  in Computer Scicncc, 1981.
[Pop] f’opplestonc,  R., The Design Philosophy of POP-II,
Machine Inlelligcncc 3, f’dinburgh  University f+-ess,  1968.
pra79] Pratt. V.R.. A Mathematician’s View of I.isp, Byte
. Magazine, August 1979,  p. 162
[Pra80]  Pratt, V.R.. On Specifying Vcrificrs, 7th Annual ACM
Symposium on Principles of f’rogramming 1 anguagcs, Jan. 1980.
[Pra8I]  Pratt, V.R.. f’rogram  I ogic Without Binding is
I)ccidablc.  8rh  AIII~KII  AC’M  S y m p o s i u m  on l’rinciplcs  o f
f’rogramming I anguagcs.  Jan. 1981.
[Pra82]  Pratt. V.R.. On the Composition of f’roccsses,  9th
Annual ACM Symposium on f’rinciplcs of f’rogramming
Languages,  Albuquerque. NM, Jan. 1982.
[Sch]  Schocnfinkcl, M, Ueber die I3austeine  der
Mathcmatischcn  Logik.  Malh. Ann. 92, 305-316. 1924. English
lranslalion  in From Frege  to (;oc&I, IIarvard University Press,
1967.


