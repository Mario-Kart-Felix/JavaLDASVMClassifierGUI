Query Processing in Main Memory Database Management Systems 
Tobm J L.&man 
Mchael J Carey 
Computer Sctenccs Department 
Untversity of Wisconsm 
Ma&son, WI 53706 
ABSTRACT 
Most pmv~ous work m the ama of mam memory database sys- 
tems has focused on the problem of developing query processmg 
techmoues that work well wnh a very large buffer pool In thus 
paper, -we address query processmg issues for memoryresrdent rela- 
uonal databases, an envuonment wrth a very dtfferent set of costs 
and pnonues We present an arclutectum for a main memory 
DBMS, discussing the ways m whtch a memory resident database 
differs from a disk-based database We then address the problem of 
processmg relauonal quenes 1n bus archttecture, cons1denng altema- 
t1ve algonthms for seleetron, pmpecuon, and Join opemnons and 
studying then performance We show that a new mdex structure, the 
T Tree, works well for selectton and JOM processing in memory 
restdent databases We also show that bashmg methods work well 
for processmg pro~cttons and JOUIS. and that an old Join method, 
sort-merge, shll has a place m mam memory 
1 Introductron 
Today, medmm to hrgh-end computer systems typmally have 
memory capacmes m the range of 16 to 128 megabytes, and 1t 1s pro- 
Jetted that chp densmes will contmue their current trend of doubling 
every year for the foreseeable future [Rs86] As a result, It IS 
expected that mam memory sizes of a ggabyte or mote will be feasi- 
ble and perhaps even fanly common wuhm the next decade Some 
researchers b&eve that many apphcaaons wuh memory requue- 
ments whtch currently exceed those of today’s technology wtll thus 
become memory restdent apphcauons 1n the not-too-dntant future 
[GLV83], and the database systems area 1s certam to be affected 1n 
some way by these trends Pmv~ous studies of how large amounts of 
memory will affect the design of database management systems have 
focused almost entuely on how to make use of a large buffer pool 
[DK084, DeG85, ElB84, Sha86] 
With memory stzes growing as they am, 1t IS qtute 11kely that 
databases, at least for some apphcauons, will eventually fit enurely 
This research was parttally supported by so IBM FellowshIp, an IBM Faculty 
Develosment Award, and Nattonal Scrwce Foundauon Grant Number DCR- 
84028l-8 
PermIssIon to copy wtthout fee all or part of this matenal IS granted 
provtded that the copies are not made or dtstrtbuted for dnect 
commerctal advantage, the ACM copynght nottce and the tttle of the 
pubheatron and its date appear, and nottce IS grven that copymg 1s by 
permtsston of the Associatron for Computrng Machmery To copy 
otherwtse, or to repubhsh, reqmres a fee and/or spectfic permtsston 
0 1986 ACM 0-89791-191-l/86/0500/0239 $00 75 
m mam memory For those apphcauons whose storage reqmrements 
contmue to exceed memory capacmes, them may sttll be often- 
referenced relattons that wtll fit m memory, m which case rt may pay 
to pamuon the database into memory resident and disk resident por- 
ttons and then use memory-specific techmques for the memory 
restdent pomon (much hke IMS Fastpath and IMS [Dat81]) In 
add1uon to tradruonal database appllcattons, there are a number of 
emerging appl1cattons for wluch mam memory sizes will almost cer- 
tainly be sufficient - appllcat1on.s that wish to be able to store and 
access mlaaonal data mostly because the relauonal model and 1ts 
assoctated operattons provide an attracuve abstracaon for their 
needs Horwuz and Teltelbaum have proposed using relaaonal 
storage for program mformanon 1n language-based editors, as adding 
relations and relaaonal operanons to attnbute grammars provides a 
mce mechamsm for speafy1ng and building such systems [HoT85] 
Linton has also proposed the use of a database system as the basis for 
constructmg program development envlromnents but841 
Snodgrass has shown that the relauonal model provides a good basis 
for the development of performance momtonng tools and their inter- 
faces [Sno84] Finally, Warren (and others) have addressed the rela- 
ttonsh1p between Prolog and mlahonal database systems fJVar81], 
and having efficient algonthms for telauonal operaaons 1n mam 
memory could be useful for processing quenes m future logic pro- 
gramming language implementauons 
Mouvated by these considerations, we are addressing the ques- 
uon of how to manage large memory res&nr relauonal databases 
Whereas tradmonal database algonthms are usually designed to 
muum1ze disk traffic, a main memory database system must employ 
algonthms that am dnven by other cost factors such as the number of 
data compansons and the amount of data movement. We are study- 
mg these issues, evaluatmg both old and new algonthms to deter- 
mme which ones make the best use of both CPU cycles and memory 
(Note that whde memory can be expected to be large, 1t wdl never be 
free ) We have focused mostly on query processing issues to date, 
but we also plan to examme concurrency control and recovery issues 
III our research - main memory databases ~111 still be mulu-user 
systems, and many apphcauons will require their data to be stored 
safely on disk as well as 1n mam memory 
The remainder of thts paper IS orgamzed as follows Secuon 2 
describes our main memory DBMS archnecture, pointing out ways 
m which the orgamzauon of mam memory databases can profitably 
doffer from drsk-based databases Sections 3 presents our work on 
algonthms for tmplementmg selechon, projectton, and JoIn opera- 
uons Both algonthms and performance results are given for each of 
these operauons Fmally, Secuon 4 presents our conclusions and 
discusses their impact on query opumizatton 
239 
2 Mam Memory DBMS Arch&cture 
In ths sectlon, we present the design of a mam memory data- 
base management system (MM-DBMS) that we are bmldmg as part 
of a research proJect at the Umverslty of W~s~~~n-Mad~son The 
key aspects of the design are the stmcture of relanons, m&es, and 
temporary hsts (for holding query results and temporary relanons) 
Ideas for approachmg the problems of concurrency control and 
recovery are in the development stages The design 1s presented 
under the assumption that the. entire. database resides m mam 
memory, ignonng (for now) the case of a pamuoned database 
2 1 Relations 
Every relanon in the MM-DBMS ~11 be broken up mto partl- 
tions, a partmon is a umt of recovery that IS larger than a typical &Sk 
page, probably on the order of one or two Qsk tracks In order to 
allow more freedom of design of these pamuons, the relanons ~111 
not be allowed to be traversed dltecdy, so all access to a relation 1s 
through an index (Note that tins reqmres all telauons to have at 
least one mdex ) Although physical contiguity 1s not a major perfor- 
mance issue m mam memory (indeed, the tuples of a relation could 
be scattered across all of memory), keepmg the tuples grouped 
together m a pamnon aids m space management and recovery, as 
well as being more efficient in a multi-level cache environment (In a 
single-level cache, cache block Sizes are typically smaller than the 
size of a tuple, but m a muln-level cache where there are several 
cache block sizes, the larger sized cache blocks could hold most or 
all of a pamtion ) 
The tuples m a pamuon will be referred to directly by memory 
addresses, so tuples must not change locauons once they have been 
entered mto the database For a vanable-length field, the tuple itself 
will contain a pointer to the field m the ptitron’s heap space, so 
tuple growth will not cause tuples to move ’ Since tuples in memory 
can be randomly accessed with no loss m performance, tt 1s possible 
for the MM-DBMS to use pointers where it would otherwise be 
necessary to copy data in a disk-based DBMS For example, If 
foreign keys (attnbutes that reference tuples m other relations) are 
ldentlfied m the manner proposed by Date [DatW, the MM-DBMS 
can substitute a tuple pointer field for the foreign key field (Tins 
field could hold a single pointer value m the case of a one to one 
relatlonshlp, or It could hold a list of pomters If the relationship 1s 
one to many ) When the foreign key field’s value 1s referenced, the 
MM-DBMS can simply follow the pointer to the foreign relation 
tuple to obtam the desired value %s will be more space efficient, 
as pointers will usually be as small as or smaller than data values 
(especially when the values are stnngs) Tlus wdl also enhance 
remeval performance by allowing the use of precomputed Jams 
Consider the following example 
Employee Relauon (Name, Id, Age, DeptId) 
Department Relation (Name, Id) 
Query 1 Retneve the Employee name, Employee age, and Depart- 
ment name for all employees over age 65 
Most convenuonal DBMSs lack precomputed Jams and would 
require a Join operation to answer tis query Even with precom- 
puted Joms, a conventional DBMS would need to have the Depart- 
ment tuples clustered with the Employee tuples or It could pay the 
pnce of a disk access for every Department tuple remeved In the 
MM-DBMS, usmg precomputed Joins is much easier Assummg 
that the Emp Depud field has been ldentlfied as a foreign key that 
references Department tuples, the MM-DBMS will subsmute a 
Department tuple pointer m tts place, The MM-DBMS can then sim- 
ply perform the selection on the Employee relanon, followmg the 
Department pointer of each result tuple 
Assuming that the Department Relahon does not have pointers 
to the Employee Relation, remevmg data m the other drecaon 
would sull requne a Jam operation, but the Join’s comparison can be 
done on pointers rather than on data Usmg the relanow from the 
example above, consider the followmg query 
Query 2 Retneve the names of all employees who work m the Toy 
or Shoe Departments 
To process tis query, a selecuon wdl be done on the Department 
relauon to remeve the “Shoe” and “Toy” Department tuples, and the 
result will then be homed with the Employee relanon For the Join, 
compansons will be performed using the tuple pointers for the 
selecuon’s result and the Department tuple pointers m the Employee 
relation Wlnle this would be equivalent m cost to Jmmng on 
Dept-Id m tis example, it could lead to a slgmficant cost savmgs If 
the Join columns were smng values instead 
2 2 Indices 
Since relauons are memory resident, it 1s not necessary for a 
main memory index to store actual attnbute values Instead, pomters 
to tuples can be stored m then place, and these potntem can be used 
to extract the atmbute values when needed This has several advan- 
tages First, a single tuple pointer provides the index with access to 
both the atmbute value of a tuple and the tuple itself, reducing the 
Size of the index Second, dus eliminates the complexity of dealing 
with long fields, vanable length fields, compression techmques, and 
calculatmg storage reqmrements for the index Third, movmg 
pointers will tend to be cheaper than moving the (usually longer) 
attnbute values when updates necessitate mdex operaaons Finally, 
Since a single tuple pomter provides access to any field in the tuple, 
muIn-atmbute m&ces will need less m the way of special mechan- 
isms Figure 1 shows an example of two m&es built for the 
Employee relauon (The m&es are shown as sorted tables for sim- 
PllW) 
The MM-DBMS design has two types of dynanuc mdex struc- 
tures, each serving a &fferent purpose The T Tree mC851, a rela- 
uvely new index structure designed for use m mam memory, is used 
as the general purpose index for ordered data It 1s able to grow and 
shnnk gracefully, be scanned m either Qrection, use storage 
efficiently, and handle duplicates with httle extra work Modified 
Linear Hashmg, a vanant of Linear Hashmg [Lit801 that has been 
modified for use m mam memory mC85], is used for unordered 
data Several other index structures were constructed to aid in the 
exammatlon of Join and project methods shown later m tis paper 
The array index structure [AHK85] was used to store ordered data It 
1s easy to build and scan, but it 1s useful only as a read-only index 
because It does not handle updates well Chained Bucket Hashmg 
[AHU74] was used as the temporary index structure for unordered 
data, as It has excellent performance for static data (Ongmally, 
Chained Bucket Hashmg was going to be used for static structures in 
the MM-DBMS, but it has Since been replaced by Mo&ied Lmear 
Hashmg, because It was discovered that the two have slrmlar perfar- 
mance when the number of elements remants stabc ) 
2 3 Temporary hsts 
The MM-DBMS uses a temporary hst stmctum far srormg 
mtermedmte result nlahons A temm hst is a hst of tuple 
pomters plus an awxx@d result descnptor The pomters point to 
the source relauon(s) from which the temporary relanon was formed, 
240 
and the result descnptor 1dentlfies the fields that are contamed 1n the 
relation that the temporary hst represents The descnptor takes the 
place of projection - no wdtb reduction 1s ever done, so there 1s ht- 
tle motivation for computing pmJect1ons before the last step of query 
processmg unless a sigmficant number of duphcates can be ehm- 
1nated Unlike regular relauons, a temporary hst can be traversed 
Qrectly, however, 1t 1s also possible to have an index on a temporary 
hst 
As an example, 1f the Employee and Department relations of 
Figure 1 were Joined on the Department Id fields, then each result 
tuple 1n the temporary hst would hold a pour of tuple pomters (one 
poumng to an Employee tuple and one pomting to a Department 
tuple), and the result descnptor would hst the fields 1n each relation 
that appear 1n the result Rgure 1 also shows the result hst for such 
an equijom on Department Id (Query 1) 
EmDlOVee Relatton r Employee 1 
DeDartment 
124 102 
110 124 L-L-I 105 110 137 137 
1 287 1 Pamt 1 455 1 1 (102,201)1 ’ - 
I 
Flgure 1 - Relation and Index Design 
2 4 Concurrency Control and Recovery 
The MM-DBMS 1s intended to provide very high performance 
for the apphcahons that 1t 1s capable of serving, many of which urlll 
reqmre their data to be stored safely on disk as well as 1n memory 
Thus, the MM-DBMS must have a fast recovery mechamsm The 
system 1s intended for multiple users, so 1t must also provide con- 
currency control While we have not yet fimshed the design of these 
subsystems, we wish to point out some of the major issues that are 
gmd1ng their design 
One proposed solution to the recovery problem 1s to use 
battery-backup RAM modules &eR85], but thus does not protect 
memory from the possibility of a me&a failure - a malfunctionmg 
CPU or a memory frulure could destroy a several gigabyte database 
Thus, disks will stdl be needed to provide a stable storage medium 
for the database Given the size of memory, appl1cauons that depend 
on the DBMS ~11 probably not be able to afford to wat for the 
entire database to be reloaded and brought up to date from the log 
Thus, we are developing an approach that will allow normal process- 
1ng to contmue 1mme&ately, although at a slower pace until the 
workmg sets of the current transacuons are read into mam memory 
Our approach to recovery 1n the MM-DBMS 1s based on an 
active log device. Dunng normal operation, the log device reads the 
updates of comrmtted transactions from the stable log buffer and 
updates the disk copy of the database The log device holds a change 
accumulation log, so 1t does not need to update the disk version of 
the database every ume a partmon 1s modified The MM-DBMS 
wntes all log 1nformauon directly into a stable log buffer before the 
actual update 1s done to the database, as 1s done 1n IMS FASTPATH 
[IBM791 If the transaction aborts, then the log entry 1s removed and 
no undo 1s needed If the transacuon comrmts, then the updates are 
propagated to the database After a crash, the MM-DBMS can 
continue processmg as soon as the workmg sets of the current tran- 
sacDons are present in main memory The process of readmg in a 
workmg set works as follows Each pamtlon that pamclpates 1n the 
working set 1s read from the &sk copy of the database The log dev- 
1ce 1s checked for any updates to that pamuon that have not yet been 
propagated to the disk copy Any updates that exist are merged with 
the parution on the fly and the updated pamuon 1s placed 1n 
memory Once the workmg set has been read in, the MM-DBMS 
should be able to run at close to 1ts normal rate while the remainder 
of the database 1s read 1n by a background process A related propo- 
sal for mam memory database recovery has been developed in paral- 
lel with ours [&86], since both schemes are 1n theu development 
stages, however, 1t would be premature to compare them here 
c 
cl- CPU 
L 
Log Device 
I I 
Flgure 2 - Recovery Components 
Concurrency control costs are different for a memory resident 
database Transactions will be much shorter 1n the absence of &sk 
accesses In tis environment, 1t will be reasonable to lock large 
items, as locks will be held for only a short time Complete senall- 
zat1on would even be possible 1f all transactions could be guaranteed 
to be reasonably short, but transachon interleaving 1s necessary for 
fairness 1f some transactions will be long We expect to set locks at 
the partition level, a frurly coarSe level of granulanty, as tuple-level 
locking would be prolubrtlvely expensive here (A lock table 1s basi- 
tally a hashed relation, so the cost of lockmg a tuple would be com- 
parable to the cost of accessing 1t - thus doubling the cost of tuple 
accesses 1f tuple-level lockmg 1s used ) Recall that the Size of a par- 
tmon 1s expected to be on the order of one or several disk tracks 
(since tlus 1s the unit of recovery) Partmon-level lockmg may lead 
to problems with certam types of transactions that are mherently 
long (e g , conversational transachons) We will address these issues 
1n future work 
3 Query Processmg m Mam Memory DBMS 
The direct addressatnhty of data 1n a memory resident database 
has a profound impact on query processmg With the nouon of clus- 
tenng removed, the methods for selection, Join and projection 
acquire new cost formulas Old and new algonthms for these query 
processing operations were tested to determme which algonthms 
perform best 1n a main memory environment 
3 1 The Test Environment 
All of the tests reported here were run on a PDP VAX 1 l/750 
running with two megabytes of real memory (as opposed to virtual 
memory) ’ Each of the algontbms was Implemented 1n the C pro- 
gramming language, and every effort was made to ensure that the 
quality of the implementauons was umform acmss the algonthms 
The validity of the execution umes reported here was venfied by 
241 
recording and exanumng the number of compansons, the amount of 
data movement, the number of hash funcoon calls, and other nuscel- 
laneous operations to ensure that the algonthms were doing what 
they were supposed to (1 e , neither more nor less) These counters 
were compiled out of the code when the final performance tests were 
run, so the execution hmes presented here reflect the mnmng times 
of the actual operaaons ~rlth very little ume spent m overhead (e g , 
dnver) routmes Tumng was done usmg a rouhne sundar to the 
‘getrusage’ facility of Umx ’ 
3 2 Selection 
This section summarizes the results from a study of index 
mechamsms for mam memory databases [Lec85] The index stmc- 
tures tested were AVL Trees [AHU74], B Trees [Com7913, arrays 
[AHK85], Chamed Bucket hashmg [Knu73], Extendible Hashmg 
[FNP79], Linear Hashmg &t80], Modified Linear Hashmg [LeC85], 
and one new method, the T Tree [LeC85] (Modified Linear Hashmg 
uses the basic pnnclples of Lmear Hashmg, but uses very small 
nodes m the directory, single-item overflow buckets, and average 
overtlow cham length as the cntena to control duectoly growth ) All 
of these mdex structures, except for the T Tree, are well-known, and 
tbelr algontbms are described m the hterature Thus, we descnbe 
only the T Tree here 
3 2 1 The T Tree Index Structure 
The T Tree 1s a new balanced tree structure that evolved from 
AVL and B Trees, both of which have certam posmve quahhes for 
use m mam memory The AVL Tree was designed as an internal 
memory data structure It uses a bmary tree search, which 1s fast 
sum the binary search 1s mtrm~c to the tree structure (1 e , no anth- 
mettc calculations are needed) Updates always affect a leaf node, 
and may result m an unbalanced tree, so the tree IS kept balanced by 
rotation operations The AVL Tree has one major hsadvantage - 
its poor storage uuhza0on Each tree node holds only one data item, 
so there are two pointers and some control mformation for every data 
item The B Tree IS also good for memory use - its storage unhza- 
uon IS better since there are many data items per pomter4, searchmg 
1s fairly fast since a small number of nodes are searched with a 
binary search, and updatmg 1s fast smce data movement usually 
involves only one node 
The T Tree is a binary tree with many elements per node (Rg- 
ure 3) Figure 4 shows a node of a T Tree, called a T Node Since 
the T Tree IS a binary tree, It retams the mtnnslc binary search nature 
of the AVL Tree, and, because a T node contmns many elements, the 
T Tree has the good update and storage charactenshcs of the B Tree 
Data movement 1s required for msemon and deletion, but it 1s usu- 
ally needed only withm a single node Rebalancing IS done using 
rotations smular to those of the AVL Tree, but it 1s done much less 
often than m an AVL ‘Fiee due to the possibility of mtra-node data 
movement 
To md in our dlscuss:on of T Trees, we begin by mtroducmg 
some helpful termmology There are three different types of T- 
nodes, as shown in Figure 4 A T-node that has two subtrees 1s 
2Unvr IS a trademark of AT&T Bell Laboratones 
’ We refer to the onglaal B Tree, not the commonly used B+ Tree Tests re- 
ported m [L&85] showed that the B+ Tree uses more storage than the B Tree and 
does not perform any better m mam memory 
’ A B Tree mternal node contams (N + 1) node pomters for every N data 
Items while a B Tree leaf node coota~ns only data items Since leaf nodes greatly 
outnumber mlemal nodes for typlcal values of N, there are many data items per node 
poulter 
Figure 3 - A T Tree 
+ Half-Leaf Nodes 
data, data, data, “’ dat 
I I 
Figure 4 - T Nodes 
called an rnrernal node A T-node that has one NIL chdd pointer and 
one non-NIL cluld pointer 1s called a half-leaf A node that has two 
NIL cluld pointers 1s called a leaf For a node N and a value X, If X 
lies between the mlmmum element of N and the maximum element 
of N (mcluslve), then we say that node N bounds the value X Since 
the data m a T-node IS kept m sorted or&r, its leftmost element IS the 
smallest element m the node and its nghrmost element 1s the largest. 
For each internal node A, there 1s a cormspondmg leaf (or half-leaf) 
that holds the data value that 1s the predecessor to the mimmum 
value m A, and there 1s also a leaf (or half-leaf) that holds the succes- 
sor to the maximum value m A The predecessor value 1s called the 
greatest lower bound of the internal node A, and the successor value 
IS called the least upper bound 
Associated with a T 1 ree is a nummum count and a maxlmum 
count Internal nodes no&s keep their occupancy (I e the number of 
data items m the node) m dus range The mlmmum and maximum 
counts will usually differ by Just a small amount, on the order of one 
or two items, whch turns out to be enough to slgmficandy reduce the 
need for tree rotations With a mix of mserts and deletes, dns little 
bit of extra mom reduces the amount of data passed down to leaves 
due to msert overtlows, and It also reduces the amount of data bor- 
rowed from leaves due to delete underllows Thus, havmg flexlblhty 
m the occupancy of internal nodes allows storage uuhxanon and 
insert/delete time to be traded off to some extent Leaf nodes and 
half-leaf nodes have an occupancy ranging from zero to the max- 
Imum count 
Searchmg m a T Tree 1s sumlar to seamhmg m a binary tree 
The mam difference. 1s that compansons are made wltb the mimmum 
and maximum values of the node rather than a single value as m a 
bmary tree no& The search conslsta of a bmary tree search to iind 
the node that bounds the search value and then a bmary search of the 
node to find the value, if such a node ts found 
To insert mto a T Tree, one first searches for a node that 
bounds the msert value If such a node 1s found, the item 1s inserted 
there If the insert causes an overtlow, the mimmum elementS of that 
shlovmg the muummn element mqures less total data movement tbao movmg 
the maxmum element Smularly, when a node underflow because of a deletion, 
borrowlog the greatest lower bound from a leaf node rqmres less work than kc- 
rowmg the least upper bound These details are explamed m [L.eC851 
242 
node IS transferred to a leaf node, becoming the new greatest lower 
bound for the node it used to occupy If no boundmg node can be 
found, then the leaf node where the search ended 1s the node where 
the insert value goes If the leaf node 1s full, a new leaf 1s added and 
the tree IS rebalanced 
To delete from a T Tree, one first searches for the node that 
bounds the delete value Then, one searches the node for the delete 
value If a boundmg node IS not found, or the delete value wlthm the 
bounding node IS not found, the delete returns unsuccessful Other- 
wse, the item IS removed from the node If deletmg from the node 
causes an underllow, then the greatest lower bound for tlus node 1s 
borrowed from a leaf If dns causes a leaf node to become empty, 
the leaf node 1s deleted and the tree 1s rebalanced If there 1s no leaf 
to borrow from, then the node (which must be a leaf) 1s allowed to 
underflow 
3 2 2 The Index Tests 
Each index structure (arrays, AVL Trees, B Trees, Chained 
Bucket Hashmg, Extedble Hashmg, Lmear Hashmg, Modified 
Linear Hashmg, and T Trees) was tested for all aspects of mdex use 
creation, search, scan, range quenes (hash structures excluded), 
query nuxes (mtenmxed searches, mserts and deletes), and deletion 
Each test used m&x structures filled ~rlth 30,000 umque elements 
(except for create, which inserted 30,000 elements) The m&ces 
were configured to run as umque mdlces - no duphcates were per- 
mltted The index structures were constructed in a *mam memory” 
style, that is, the Indices held only tuple pomters instead of actual 
key values or whole tuples We summanze the results of three of the 
tests from [LeCSS] searchmg, a query mrx of searches and updates, 
and storage cost measurements In order to compare the perfor- 
mance of the mdex structures m the same graphs, the number of van- 
able parameters of the vanous st.mctures was reduced to one - node 
size In the case of Mtified Lmear Hashmg, single-item nodes were 
used, so the “Node Sue” axis m the graphs refers to the average 
overflow bucket cham length Those structures without vanable 
node sizes simply have straight hnes for their execution umes The 
graphs represent the hashmg algontbms with dashed lines and the 
order-presexvmg structures with solid lines 
Search 
Graph 1 shows the search times of each algonthm for vanous 
node sizes The array uses a pure bmary search The overhead of the 
anthmeac calculauon and movement of pointers 1s nonceable when 
compared to the “hanlwu&” binary search of a binary tree In con- 
trast, the AVL Tree needs no anthmetrc calculahons, as It Just does 
one compare and then follows a pomter The T Tree does the major- 
ity of lta search m a manner slmllar to that of the AVL Tree, then, 
when It locates the correct node, It sHrltches to a binary search of that 
node Thus, the search cost of the T Tree search 1s shghdy more 
than the AVL Tme. search cost, as some time 1s lost m binary search- 
mg the final node The B Tree search ume IS the worst of the four 
order-preservmg structures, because It reqmres several bmary 
searches, one for each no& m the search path 
The hashmg schemes have a fixed cost for the hash funcaon 
computation plus the cost of a linear search of the node and any asso- 
ciated overflow buckets For the smallest node sizes, all four hash- 
mg methods are basically equivalent The differences he in the 
search fimes as the nodes get larger Linear Hashmg and Extendible 
Hashmg are Just about the same, as they both search mulaple-Item 
nodes Mo&fied Linear Hashmg searches a linked hst of single-Item 
nodes, so each data reference reqmres traversing a pointer Tlus 
overhead 1s noticeable when the cham becomes long (Recall that 
Modltied LlFear Hash 
Array 
T Tree 
Seconds 
AVL Tree 
Chamed Bucket Hash 
o! ’ ’ ’ 1 8 1 ’ ’ 0 ’ 
0 10 20 30 40 50 60 70 80 90 100 
Node Size 
Graph 1 - Index Search 
“Node Size” 1s really average cham length for Mo&fied Linear Hash- 
ing 1 
Query MIX 
The query mix test IS most Important, as It shows the index 
structures m a normal workmg envlromnent Tests wem performed 
for three query nuxes using different percentages of mterspersed 
searches, inserts and deletes 
1) 80% searches, 10% msew, 10% deletes 
2) 60% searches, 20% mserta, 20% deletes 
3) 40% searches, 30% inserts, 30% deletes 
The query mix of 60 percent searches, 20 percent mserta and 20 per- 
cent deletes (Graph 2) was representative of the three query mix 
graphs The T Tree performs better than the AVL Tree and the B 
Tree here because of its better combmed search / update capability 
The AVL tree is faster than the B Ttee because It 1s able to search 
faster than the B Tree, but the execution times are smular because of 
the B Tree’s better update capability For the smallest node sizes, 
Modified Linear Hashmg, Extendible Hashmg, and Chamed Bucket 
Hashing are all basically equivalent They have similar search cost, 
and when the need to resize the directory 1s not present, they all have 
the same update cost Lmear Hashmg, on the other hand, was much 
slower because, trying to mamtam a pamcular storage uuhzatlon 
(number of data bytes used I total number of data bytes aviulable), It 
did a slgmficant amount of data reorgamzmon even though the 
number of elements was relatively constant As for the array index, 
ita performance was two orders of magmmde worse than that of the 
other index structures because of the large amount of data movement 
reqmred to keep the amy m sorted order (Every update reqmres 
moving half of the army, on the average ) 
Storage Cost 
Space considerations preclude the mcluslon of the storage 
results graph, but we summanze them here The array uses the 
mlmmum amount of storage, so we discuss the storage costs of the 
other algonthms as a raao of their storage cost to the array storage 
243 
947 
& Array 
501 I I 
I i 
45 - \ Lmear Hash : 
: 
: / 
/’ 
,/ Modified Lmear Hash 
1 i ; 30 !I 
Extendible Ha+,. 
,//--’ 
/’ 
/’ \ B Tree 
/’ L AVL Tree 
,‘- / 
/ 
c T Tree 
5~-------------------------------- 
Chamed Bucket Hash 
0- I I , I I , t I 
0 10 20 30 40 50 60 70 80 90 100 
Node Size 
Graph 2 - Query MIX of 60% Searches 
cost Fust, we consider the fixed values the AVL Tree storage fac- 
tor was 3 because of the two node pomters it needs for each data 
item, and Chamed Bucket Hashmg had a storage factor of 2 3 
because it had one pointer for each data Item and part of the table 
remained unused (the hash funcnon was not perfecrly umform) 
Modified Lmear Hashmg was slmdar to Chamed Bucket Hasbmg for 
an average hash chain length of 2, but, for larger hash chams, the 
number of empty slots m the table decreased and eventually the table 
became completely full Finally, Linear Hashmg, B Trees, Extendl- 
ble Hashmg and T Trees all had nearly equal storage factors of 15 
for medmm to large size nodes Extendible Hashmg tended to use 
tbe largest amount of storage for small nodes sizes (2,4 and 6) This 
was because a small node size mcreased the probablbty that some 
nodes would get more values than others, causing tbe directory to 
double repeatedly and thus use large amounts of storage As its node 
size was mcreased, the probabllny of dus happemng became lower 
3 2 3 Index Study Results 
Table 1 summarizes the results of our study of mam memory 
index structures We use a four level ratmg scale (poor, far, good, 
great) to show the performance of the index structures m the three 
categones An important dung to nonce about the hash-based 
indices is that, whtle Extendble Hashmg and Mtified Linear Hash- 
mg had very good performance for small nodes, they also had hgh 
storage costs for small nodes (However, the storage uhhzatlon for 
Modified Linear Hashmg can probably be Improved by using 
multiple-item nodes, thereby reducing the pointer to data Item rauo, 
the version of Modified Linear Hasbmg tested here used smgle-Item 
nodes, so there was 4 bytes of pointer overhead for each data item ) 
As for the other two hash-based methods Chained Bucket Hashmg 
had good search and update performance, but it also bad fairly high 
storage costs, and it 1s only a stauc stn~cture, and finally, Linear 
Hashing is Just too slow to use m mam memory Among the hash- 
based methods tested, Modified Linear Hashmg pmvlded the best 
overall performance 
Lookmg at the order-preserving mdex structures, AVL Trees 
have good search execution times and fair update execution rimes,, 
but they have tigh storage costs Arrays have reasonable search 
umes and low storage costs, but any update actlvlty at all causes it to 
have execunon rimes orders of mugnrrude higher than the other 
mdex structures AVL Trees and arrays do not have suffiaently 
good performance I storage charactenshcs for conslderaaon as mam 
memory indices T Trees and B Trees do not have the storage prob- 
lems of dynarmc haslung methods, they have low storage costs for 
those node sizes that lead to good performance The T Tree seems to 
be the best of choice for an order-preservmg mdex structure, as it 
performs umformly well in all of the tests 
Table 1 -Index Study Results 
3 3 Jorn 
Previous Jam studies mvolvmg large memones have been 
based on the large buffer pool assumphon [Sha86], [DK084], 
[DeG851 (Others have studled hash moms as well m a normal dtsk 
environment [Bab791, [VaG84], [Bra84], but tbelr results are less 
applicable here ) Three mam Jam methods were tested m [DeG85] 
Nested Loops wtth a hashed index, Sort Merge [BlE77], and three 
hasbmg methods, Simple Hash, Hybnd Hash and GRACE Hash 
[DK084] The msulta showed that when both Elations fit m 
memory, the three hash algonthms became equvalent, and the 
nested loops Jam with a bash index was found to perform Just as well 
as the other hash algonthms (and outperformed Sort Merge) They 
also studied the use of semqom pmcessmg with ht vectors to reduce 
the number of disk accesses involved m the Join, but dus senqom 
pass 1s redundant when the relattons are memory resident The 
variety of Join relation compostaons (e g , sizes, Join selechvitles, 
Jam column value dtstnbunons) used m their study was small, and 
may not completely reflect all posslblhties (performance-wise) 
In dus study, we examme the performance of a number of can- 
didate Join methods for the MM-DBMS We use a wide selection of 
relation composlhons so as to evaluate the algonthms under a wde 
vanety of possible condmons 
3 3 1 Relation GeneratIon 
In or&r to support our intent to test a variety of relanon com- 
poslhons, we constructed our test relahons so that we could vary 
several parameters The vanable parameters were 
(1) The relauon cardmahty (IRj) 
(2) Tbe number of Jam column duplicate values (as a percentage of 
IRI) and then &stnbuhon 
(3) The semjorn selecnvlty (the number of values m the larger 
relauon tbat paruclpate in the Join, expressed as a percentage of 
the larger telauon) 
244 
In order to get a vanable semlJom selecnvlty, the smaller rela- 
tlon was built with a specified number of values from the larger rela- 
tion To get a vanable number of duphcates, a specified number of 
umque values were generated (either from a random number genera- 
tor or from the larger relauon), and then the number of occurrences 
of each of these values was determmed usmg a random sampling 
procedure based on a truncated normal dlstnbutlon with a vanable 
standard deviation Graph 3 shows the three duphcate dlstnbuhons 
used for the tests - a skewed &smbutlon (where the standard devla- 
hon was 0 1), a moderately skewed dlstnbuhon (the 0 4 curve m the 
graph), and a near-umform dlsmbution (the 0 8 curve m the graph) 
100 
90 
80 
70 
60 
SO 
Percent 
Tuples 
40 
30 
20 
10 
0 
i 
; 
-I I 
-i 
0 10 20 30 40 50 60 70 80 90 100 
Percent Values 
Graph 3 - Dlstrlhutlon of Duphcate Values 
The results for the 0 4 and 0 8 cases were slmdar, so results are given 
here only for the two extreme cases 
3.3 2 The Jom Algorithms 
For memory resident databases, all of the hash-based algo- 
nthms tested m [DeG85] were found to perform equally well 
Therefore, the hash-based nested loops algonthm IS the only hash- 
based algonthm that we examme here For our tests, we lmple- 
mented and measured the performance of a total of five Join algo- 
nthms Nested Loops, a simple mam-memory version of a nested 
loops Jam with no index, Hash Jom and Tree Jom, two vanants of 
the nested loops Jom that use indices, and Sort Merge and Tree 
Merge, two vanants of the sort-merge Jom method of [BlE77] We 
bnefly descnbe each of these methods m turn Recall that relauons 
are always accessed vta an index, unless otherwise specified, an 
array index was used to scan the relations m our tests 
The pure Nested Loops Join IS an O(N*) algontbm It uses one 
relation as the outer, scamung each of its tuples once For each outer 
tuple, it then scans the entire inner relaaon lookmg for tuples with a 
matchmg Jom column value The Hash Jom and Tree Jom algo- 
nthms are similar, but they each use an index to limit the number of 
tuples that have to be scanned m the inner relation The Hash Join 
bmlds a Cham Bucket Hash index on the Join column of the mner 
relation, and then it uses tis index to find matchmg tuples durmg the 
JoIn The Tree Jom uses an exlstmg T Tree index on the mner rela- 
hon to find matchmg tuples We do not include the posslblhty of 
building a T Tree on the mner relation for the Join because It turns 
out to be a viable alternative only if the T tree already exists as a reg- 
ular index - if the cost to build the tree is included, a Tree Jom ~111 
aZwuys cost more than a Hash Jom, as a T tree costs more to bmld 
and a hash table IS faster for single value remeval DC851 On the 
other hand, we always include the cost of bmldmg a hash table, 
because we feel tbat a hash table mdex 1s less likely to emst than a T 
Tree index The cost of creating a hash table with 30,000 elements 1s 
about 5 seconds m our envlmnment OCSS] 
The merge Jam algonthm [BlE77] was implemented using two 
index struchzes, an array mdex and a T Tree mdex For the Son 
Merge algorithm tested here, array indexes were built on both rela- 
tions and then sorted The sort was done using qmcksort wtth an 
msemon sort for subarrays of ten elements or less 6 For the Tree 
Merge tests, we built T Tree mdlces on the Jam columns of each 
relation, and then performed a merge Join usmg these indices How- 
ever, we do not report the T Tree constmctton ames m our tests - tt 
turns out that the T Merge algorithm 1s only a viable altemanve if 
the indices already exist Prehmmary tests showed that the arrays 
can be built and sorted m 60 percent of the time to bmld the trees, 
and also that the array can be scanned m about 60 percent of the time 
It takes to scan a tree 
3.3 3 Jom Tests 
The Jom algonthms wem each tested with a variety of relation 
composltrons m order to determine their relative performance Six 
tests were performed m all, and they are summarized below In our 
descnpuon of the tests, IRl( denotes the outer relation and IR21 
denotes the mner relation 
(1) Vary Cardtnuhty Vary the sizes of the relations with (Rll = 
jR21,0% duphcates, and a semlJom selechvlty of 100% 
(2) Vary Inner Curdmnultty Vary the size of R2 (IR2( = l-100% of 
IRll) with [Rll = 30,000, 0% duplicates, and a semJom selec- 
t1v1ty of 100% 
(3) Vury Outer Cardmltty Vary the size of Rl (IRll = l-100% of 
IR21) with IR21 = 30,000, 0% duplicates, and a senuJom selec- 
t1v1ty of 100% 
(4) Vary Dupltcate Percentage (skewed) Vary the duplicate per- 
centage of both relations from O-100% with IRll = IR21 = 
20,000, a semiJom selecnvlty of 1008, and a skewed duplicate 
dlsmbution 
(5) Vary Duphcate Percentage (uniform) Vary the duplicate per- 
centage of both mlauons from O-100% with IRll = IR21 = 
20,000, a semlJom selecuvlty of lOO%, and a umform duplicate 
dlsmbuuon 
(6) Vary Semyorn Selectrvrty Vary the semiJoin selecmrty from 
l-100% with jRl[ = IR21 = 30,000 and a duplicate percentage of 
50% with a umform duplicate dlsmbunon 
6 We ran a test to detenmne the optunal subarray sue for swtchmg from 
qucksort to’msertmn sort, the ophmal subarray size was 10 
245 
-- 
3 3 4 Jom Test Results 
we present the results of each of the JOHI tests m dns secuon 
The results for the Nested Loops algorithm ~11 be presented 
separately at the end of the secaon, as its performance was typIcally 
two orders of magmtude worse than that of the other Jam methods 
Test 1 - Vary Cardmality 
Graph 4 shows the performance of the Jam methods for rela- 
tlons with equal cardmahues The relations are Jomed on keys (1 e , 
no duplicates) w1t.h a semlJom selectlvlty of 100% (1 e , all tuples 
pamclpate m the Join) If both m&ces are av;ulable, then a Tree 
Merge gives the best performance It does the least amount of work, 
as the T Tree indices are assumed to exist, and scanmng them m 
order 1lmlt.s the number of compansons required to perform the Join 
The number of compansons done 1s approximately ([Rll + IR21 * 2), 
as each element in Rl 1s referenced once and each element m R2 IS 
referenced twice me presence of duplicates would increase the 
number of hmes the elements m R2 are referenced) If it 1s not that 
case that both mdlces are avadable, it 1s best to do a Hash Join It 
turns out that, m tlus case, it is actually faster to build and use a hash 
table on the mner relanon than to simply use an exishng T Tree 
index A Hash table has a fixed cost, independent of the index size, 
to look up a value The number of compansons done in a Hash Join 
1s approximately (IRlj + (IRll * k)) where k IS the fixed lookup cost, 
whereas the number of compansons m a Tree Jom 1s roughly (IR 1 I + 
(IRl) * Log@2]))) The value of k 1s much smaller than 
Logz(lR21))) but larger than 2 Finally, the Sort Merge algorithm has 
the worst performance of the algonthms in dus test, as the cost of 
bulldmg and sorting the arrays for use m the merge phase 1s too high 
(WI * Lw$lW) + WI * L4$lW)) + WI + WIN 
JOIN TEST 1 ((RI! = IR2() 
Hash Jom 
- Tree Jam 
------ Sort Merge 
/’ 
1’ 
/ 1’ /’ 
Seconds 
10 - 
/’ 
--- Tree Merge /’ 
/’ 
1’ 
/’ II/-; 
/’ 
/’ 
1’ , 
/’ 
0 7500 15000 22500 30000 
Number Of Tuples 
Graph 4 - Vary Cardmahty 
Test 2 - Vary Inner Cardmallty 
Graph 5 shows the performance of the JOHI methods as R2’s 
cardmahty 1s vaned from l-10036 of the cardmahty of Rl In tis 
test, Rl’s cardmahty IS fixed at 30,000, the Join columns were agam 
keys (1 e , no duplicates), and the senuJom selectlmty was agam 
100% The results obtamed here are snmlar to those of Test 1, with 
Tree Merge performmg the best if T Tree m&ces exist on both Join 
columns, and Hash Jom performing the best othemse In dus test, 
each of the the mdex JOIIIS were basically doing (RI\ searches of an 
index of (Increasing) cardmahty IR21 
20- JOIN TEST 2 (Vary IR21) 
Hash Jom 
- Tree Join 
15- 
------ Sort Merge 
--- 
lo- 
Seconds 
1’ 
,/- 
/ 
I 
/’ /’ /’ , 
I 
, 
5 
, 
-- __--- 
/-- 
_--- --/ -- 
0- 
0 25% 50% 75% 100% 
1R2) Percentage of JR11 
Graph 5 - Vary Inner Cardmahty 
Test 3 -Vary Outer Cardmahty 
The parameters of Test 3 were identical to those of Test 2 
except that jRl( was vaned instead of (R2( The results of dus test are 
shown m Graph 6 The Tree Merge, Hash Join, and Sort Merge 
algonthms perform much the same as they did m Test 2 In dus 
case, however, the Tree Join outperforms the others for small values 
of (Rll, beatmg even the Tree Merge algorithm for the smallest (RI1 
values Tlus IS mtumve, as this algorithm behaves like a simple 
selecuon when [Rll contams few tuples Once IR21 mcreases to 
about 60% of IRlI, the Hash Jom algorithm becomes the better 
method agam because the speed of the hash lookup overcomes the 
lmtlal cost of bmldmg the hash table, both of which combmed are 
cheaper than the cost of many T Tree searches for large values of 
lRl/ Note if a hash table index already existed for R2, then the 
Hash Jom would be faster than the Tree Jom (recall that bmldmg the 
hash table takes about 5 seconds) 
246 
20 JOIN TEST 3 Wary lR11) JOIN TEST 4 (Vary Duplicates - Skewed Dust ) 
10000 
Hash Jom 
- Tree Jom 
Hash Join 
- Tree Jom 
------ Sort Merge 
--- Tree Merge 
10 
Seconds 
Seconds 
10 
1 
0 25% 50% 75% 
IRll Percentage of IR21 
Graph 6 - Vary Outer Cardmahty 
100% 
Test 4 -Vary Duphcate Percentage (skewed) 
For test 4, IRll and /R21 were fixed at 20,000, the sern1Jom 
selecttvity was kept at lOO%, and the duphcate percentage for both 
relahons was vaned from 1 to 100% The results of dus test are 
shown m Graph 7 The duplicate chsmbuuon was skewed, so there 
were many duplicates for some values and few or none for others 
(The duplicate percentages of the two relauons were different m this 
test - a result of the relauon construction procedure In order to 
actieve 100 percent semqom selecnvlty, the values for R2 were 
chosen from Rl, wluch already contamed a non-urnform dlsmbuhon 
of duphcates Therefore, number of duplicates m R2 1s greater than 
that of Rl The duplicate percentages m Graph 7 refer to Rl ) Once 
the number of duplicates becomes sigmficant, the number of match- 
mg tuples (and hence result tuples) becomes large, resultmg m many 
more tuples being scanned The Sort Merge method 1s the most 
efficient of the algonthms for scanmng large numbers of tuples - 
once the skewed duplicate percentage leaches about 80 percent, the 
cost of bmldmg and sortmg the arrays IS overcome by the efficiency 
of scannmg the relations via the arrays, so It beats even Tree Merge 
m tlus case Although the number of compansons 1s the same, as 
both Tree Merge and Sort Merge use the same Merge Jam algorithm,, 
the array index can be scanned faster than the T Tree index because 
the army index holds a hst of contiguous elements whereas the T 
Tree holds nodes of con@uous elements Joined by pomters Test 
results from [L&851 show that the array can be scanned m about 2/3 
the time It takes to scan a T Tree The Index Jam methods are less 
efficient for processmg large numbers of elements for each Jcnn 
value, so they begin to lose to Sort Merge when the skewed duphcate 
percentage reaches about 40 percent 
Test 5 -Vary Duplicate Percentage (muform) 
Test 5 is ldenucal to Test 4 except that the chsmbuaon of 
duphcates was umform The results of Test 5 are shown m Graph 8 
0 25% 50% 75% 100% 
Duplicate Percentage 
Graph 7 - Vary Duplicate Percentage (skewed) 
IN TEST 5 (Vary Duphcates - Uniform Dust ) 
Hash Jom 
- Tree Jom 
------ Sort Merge 
--- Tree Merge 
100 I I 
1000 
Seconds 
10 / 
1 
0 25% 50% 75% 100% 
Duplicate Percentage 
Graph 8 - Vary Duplicate Percentage (uniform) 
(Note that the duplicate percentages of Rl and R2 are the same here, 
because R2 was created with a umform &smbunon of Rl values ) 
Here, the Tree Merge algonthm remamed the best method unttl the 
duphcate percentage exceeded about 97 percent because the output 
247 
of the loin was much lower for most duplicate percentages When 
the duplicate percentages were low (O-60 percent), the Jam algo- 
nthms had behavior similar to that of earlier tests Once the duph- 
cate percentage became tigh enough to cause a lugh output Jam (at 
about 97 percent), Sort Merge agam became the fastest Join method 
Test 6 -Vary SenuJom Selectlvlty 
In the previous tests, the senuJom selecnvlty was held constant 
at 100% In Test 6, however, it was vaned, and the results of tis 
test are shown in Graph 9 For @IIS test, /Rlj = JR21 = 30,000 ele- 
ments, the duplicate percentage was fixed at 50% m each relauon 
with a umform dlsmbuhon (so there were roughly two occurrences 
of each JOUI column value in each relation), and the senuJom selec- 
tivity was vaned from l-100% The Tree Jam was affected the most 
by the mcrease in matching values, a bnef descnpaon of the search 
procedure ~111 explam why When the T Tree IS searched for a set of 
tuples with a single value, the search stops at any tuple with that 
value, and the tree 1s then scanned m both dnec~ons from that poa- 
non (smce the list of tuples for a gven value 1s logically connguous 
m the tree) If the lmtlal search does not find any tuples matchmg 
the search value, then the scan phase 1s bypassed and the search 
returns unsuccessful When the percentage of matchmg values IS 
low then, most of the searches are unsuccessful and the total cost IS 
much lower than when the maJonty of searches are successful A 
slmllar case can be made for the Hash Jam m that unsuccessful 
searches sometimes reqmre less work than successful ones - an 
unsuccessful search may scan an empty hash chain instead of a full 
one The increase m the Tree Merge execunon time m Graph 9 was 
due mostly to the extra data compansons and the extra overhead of 
recording the mcreasmg number of matchmg tuples Sort Merge 1s 
less affected by the increase m matchmg tuples because the sortmg 
time overshadows the time required to perform the actual merge Jam 
20 
15 
10 
Seconds 
JOIN TEST 6 (Vary SemlJom Selectwty) 
fl 
Hash Jom 
Tree Jom 
------ Sort Merge 
_--- Tree Merge 
_--- 
--- 
_--- 
_ c -- 
_---- 
1 
0 25% 50% 75% 
Percent Matchmg Values 
Graph 9 - Vary Semqom Selectwlty 
100% 
3 3 5 Jom Test Result Summary 
If the proper pair of tree indices IS present, the Tree Merge Jam 
method was found to perform the best m almost all of the sltuatlons 
tested It turned out never to be advantageous to budd the T Tree 
Indices for thus Jam method, however, as it would then be slower 
then the other three methods In sltuauons where one of the two 
relations 1s missing a Jam column index, the Hash Jom method was 
found to be the best choice There are only two exceptions to these 
statements 
(1) If an mdex exists on the larger relation and the smaller relation 
1s less than half the size of the larger relation, then a Tree Jam 
(T Tree index Join) was found to execute faster than a Hash 
Join In tis slmaaon, the tuples m the smaller relation can be 
looked up m the tree mdex faster than a hash table can be bmlt 
and scanned This would also be true for a hash mdex if It 
already existed 
(2) When the semiJom selecnvlty and the duplicate percentage are 
both tigh, the Sort Merge Jam method should be used, pamcu- 
larly if the duplicate dlsmbutlon IS highly skewed A Tree 
Merge Join IS also sansfactoly is this case, but the required 
mdlces may not be present If the indices must be bmlt, then 
the Tree Merge Join will be more costly than the Hash Jom for 
duplicate percentages less then 60 m the skewed case and 80 m 
the umform case 
It should be mentioned that only eqmJoms were tested Non- 
equljoins other than “not equals” can make use of ordenng of the 
data, so the Tree Jom should be used for such (<, 5, >, 2) Jams 
As mennoned earher, we also tested the nested loops Join 
method Due to the fact that its performance was usually several 
orders of magnitude worse than the other Join methods, we were 
unable to present them on the same graphs Graph 10 shows the cost 
of nested loops Jam for a pomon of Test 1, with IRl( = jR21 vaned 
from 1,000 to 20,000 It is clear that, unless one plans to generate 
full cross products on a regular basis, nested loops Join should slm- 
ply never be considered as a practical Join method for a mam 
memory DBMS 
The precomputed Jam described m Section 2 1 was not tested 
along with the other Join methods Intumvely, It would beat each of 
the Join methods m every case, because the Jolmng tuples have 
already been poured Thus, the tuple pointers for the result relation 
can simply be extracted from a single relation 
3 4 ProJectIon 
In our discussion of the MM-DBMS m Section 2, we explained 
that much of the work of the proJectlon phase of a query 1s imphcltly 
done by speclfymg the attnbutes in the form of result descnptors 
Thus, the only step requmng any slgmficant processing is the final 
operation of removing duplicates For duphcate ehmmatlon, we 
tested two candidate methods Sort Scan [BBD83] and Hashmg 
[DK084] Agam, we implemented both methods and compared their 
performance 
In these tests, the composltlon of the relation to be proJected 
was vaned m ways similar to the those of the Join tests - both the 
relation cardmahty and its duplicate percentage were vaned Since 
prehmmary tests showed that the dlsmbunon of duphcates had no 
effect on the results, we do not vary the dlsmbuuon in the tests 
presented here 
248 
10000 NESTED LOOPS JOIN 
100 
Seconds 
10 
1 7 
0 10000 20000 30000 
Number of Tuples (IF11 = JR2)) 
Graph 10 - Nested Loops Join 
Graph 11 shows the performance of the two duphcate ehmma- 
non algonthms for relations of various sizes For dus test, no duph- 
cates were actually introduced m the relation, so the startmg size of 
the relation and lta final stze were the same The msemon overhead 
m the hash table IS linear for all values of JR1 (since the hash table 
size was always chosen to be (R1/2), whde the cost for sortmg goes as 
O(lRI log IRI) As the number of tuples becomes large, this sorting 
cost dommates the performance of the Sort Scan method In addl- 
tlon, these tests were performed using single column relations - the 
number of compansons IS much higher m the sort process, and thus 
cost would only be exacerbated If more columns paruclpated m the 
proJection Thus, the Hashmg method IS the clear winner m dus test 
Graph 12 shows the results for a relation with 30,000 elements 
but a varying number of duplicates As the number of duplicates 
Increases, the hash table stores fewer elements (since the duplicates 
are discarded as they are encountered) The Hashmg method IS thus 
able to run faster than It would with all the elements (since it has 
shorter chams of elements to process for each hash value) Sortmg, 
on the other hand, realizes no such advantage, as it must shll sort the 
entire hst before ehmmatmg tuples durmg the scan phase The large 
number of duplicates does affect the sort to some degree, however, 
because the msertlon sort has less work to do when there are many 
duplicates - with many equal values, the subarray m quicksort IS 
often already sorted by the time it IS passed to the msemon sort 
4 Conclusions and Future Work 
In thus paper, we have addressed query processmg Issues and 
slgonthms for a mam memory database management system We 
sketched an archnecture for such a system, the MM-DBMS archtec- 
ture, pomtmg out the major dtfferences between disk-based data- 
bases and memory resident databases We then addressed the prob- 
lem of processmg relational quenes m the MM-DBMS architecture, 
studying algonthms for the selection, Join, and proJechon operations 
0 10000 20000 30000 
Number of Tuples 
Graph 11 - Vary Cardmahty 
PROJECT TEST 2 (Vary Duphcate Percentage) 
8 
7 
6 
5 
4 
Seconds 
3 Hash 
0 25% 50% 75% 100% 
Duphcate Percentage 
Graph 12 - Vary Duphcate Percentage 
249 
- - -  1 
A number of candldate algonthms were implemented for each opera- 
uon, and their performance was expenmentally compared We 
found that, for selection, the T Tree provides excellent overall per- 
formance for quenes on ordered data, and that Modified Linear 
Hashing 1s the best mdex structure (of those exammed) for unordered 
data For JOHIS, when a precomputed JOIII does not exist, we found 
that a T Tree based merge Join offers good performance if both 
mdlces exist, and that hashmg tends to offer the best performance 
otherwlse A mam memory vanant of the sort merge algorithm was 
found to perform well for high output JOHIS Fmally, it was shown 
that hashmg 1s the dominant algorithm for processmg proJections m 
mam memory 
In light of these results, query optimization m MM-DBMS 
should be simpler than m convenuonal database systems, as the cost 
formulas are less complicated [SAC791 The issue of clustenng and 
proJection for size reduction has been removed from conslderauon, 
thereby slmphfymg the choice of algonthms (ProJectlon may be 
needed to reduce the number of duplicate entnes m a temporary 
result, but it is never needed to reduce the szze of the result tuples, 
because tuples are never copied, only pointed to ) There are three 
possible access paths for selection (hash lookup, tree lookup, or 
sequential scan through an unrelated index), three mam Jom methods 
(precomputed Join, Tree Merge Join, and Hash Join) and one method 
for ehmmatmg duphcates (Hash) Moreover, the choice of w2llch 
algorithm IS slmphfied because there 1s a more defimte ordenng of 
preference a hash lookup (exact match only) IS always faster than a 
tree lookup whch 1s always faster than a sequenhal scan, a precom- 
puted Jam 1s always faster than the other Jam methods, and a Tree 
Merge Jam IS nearly always preferred when the T Tree m&ces 
already exist 
5 References 
[AHU74] A Aho, J Hopcroft and J Ullman, The Design and 
Analysrs of Computer Algorrthms, Addison-Wesley 
Pubhshmg Company, 1974 
[AHK85] 
[Bab79] 
[BBD83] 
[BlE77] 
[Bra841 
[Corn791 
[Dat81] 
[Dat85] 
[DK084] 
A Ammann, M Hanrahan and R Knshnamurthy, 
Design of a Memory Resident DBMS, Proc IEEE 
COMPCON, San Francisco, February 1985 
E Babb, Implementing a Relational Database by Means 
of Specialized Hardware, ACM Transactions on 
Database Systems 4,l (March 1979), l-29 
D Bltton, H Boral, D DeWltt and W Wllkmson, 
Parallel Algonthms for the execution of Relational 
Database Operatrons, ACM Transacnons on Database 
Systems 8,3 (September 1983), 324 
M Blasgen and K Eswaran, Storage and Access m 
Relational Databases, IBM Systems Journal 16,4 (1977) 
K Bratbeesengen, Hashing Methods and Relational 
Algebra Operahons, Proc of 10th Znt Conf on Very 
Large Data Bases, Singapore, August 1984,323 
D Comer, The Ublqmtous B-Tree, Computrng Surveys 
11,2 (June 1979) 
C J Date, An Introductron to Database Systems, 
Addison-Wesley, 1981 (3rd Ed ) 
C J Date, An Introductton to Database Systems, 
Addison-Wesley, 1985 (4th Ed ) 
D Dewitt, R Katz, F Olken, L Shapiro, M 
Stonebraker and D Wood, Implementahon Techmques 
for Mam Memory Database Systems, Proc ACM 
SIGMOD Conf, June 1984, l-8 
[DeC85] 
[ElC86] 
[ElB84] 
[FNP791 
[FE861 
[CiLV83] 
[HoT85] 
[IBM791 
[Knu73] 
[LeC851 
IWW 
&in841 
&t80] 
[SAC791 
[Sha86] 
[Sno84] 
[VaG84] 
mar8 11 
D DeWltt and R Gerber, Multiprocessor Hash-Based 
Jom Algonthms, Proc of Ilth Int Conf on Very Lurge 
Data Bases, Stockholm, Sweden, August 1985 
M Elch, MMDB Recovery, Southern Methodist Umv 
Dept of Computer Sciences Tech Rep # 86-CSE-11, 
March 1986 
K Elhardt and R Bayer, A Database Cache for &gh 
Performance and Fast Restart m Database Systems, 
ACM Transacttons on Database Systems 9,4 (December 
1984), 503-526 
R Fagm, J Nlevergelt, N Pippenger and H Strong, 
Extenhble Hashmg - A Fast Access Method for 
Dynanuc Rles, ACM Transactions on Database Systems 
4,3 (Sept 1979) 
M Fishem, Techology ‘86 Solid State, IEEE Spectrum 
23,l (January 1986) 
H Garcia-Mohna, R J Lipton and J Valdes, A Massive 
Memory Machme, Prmceton Umv EECS Dept Tech 
Rep # 315, July 1983 
S Horwltz and T Teltelbaum, Relations and Attnbutes 
A Symblotlc Basis for Edmng Environments, Proc of 
the ACM SIGPLAN Notrces Conf on Language Issues m 
Programmrng Envrronments, Seattle, WA, June 1985 
IBM, IMS Verwn I Release I5 Fast Path Feature 
Descnptron and Destgn Gut&, IBM World Trade 
Systems Centers (G320-5775), 1979 
D Knuth, Sortzng and Searchmng, Addison-Wesley, 
1973 
T Lehman and M Carey, A Study of Index Structures 
for Mam Memory Database Management Systems, UW 
CS Tech Rep # 605, July 1985 (A revised version has 
been submttted for pubhcauon) 
M Leland and W Roome, The S&on Database 
Machme, Proc 4th Int Workhop on Database 
Machtnes, Grand Bahama Island, March 1985 
M Lmton, Implementmg Relational Views of Programs, 
Proc of the ACM Software Eng NoteslSlGPLAN 
Nohces Sofhvare Eng Symp on PraChCal Sofhvare 
Development Envwonments, Pmsburgh, PA, Apnl 1984 
W Lltwm, Linear Hashmg A New Tool For File and 
Table Addressing, Proc of 6th Int Conf on Very Large 
Data Bases, Montreal, Canada, October 1980 
P Sehnger, M Astrahan, D Chamberhn, R Lone and 
T Price, Access Path Selection m a Relational DBMS, 
Proc ACM SIGMOD Conf, June 1979 
L D Shapiro, Jom Processing m Database Systems with 
Large Mam Memones, ACM Transachons on Database 
Systems, 1986 (to appear) 
R Snodgrass, Momtonng m a Software Development 
Environment A Relauonal Approach, Proc of the ACM 
Software Eng NoteslSIGPLAN Nottces Sofrware Eng 
Symp on Pracncal Sojlware Development 
Envaronments, Pmsburgh, PA, Apnl1984 
P Valdunez and G Gardann, Jom and SemiJoin 
Algonthms for a Mmluprocessor Database Machme 
Transacuons on Database Systems, ACM Transachons 
on Database Systems 9,1 (March 1984), 133 
D H D Warren, Efficient Processing of Interactive 
Relational Database Quenes Expressed m Loge, Proc 
of 7th Int Conf on Very Large Data Bases, Cannes, 
Fance, September, 198 1 
250 

