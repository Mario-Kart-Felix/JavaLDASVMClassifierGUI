Local Differential Privacy for Physical Sensor Data and Sparse Recovery
Anna Gilbert 1 Audra McMillan 1
Abstract
In this work we explore the utility of locally dif-
ferentially private thermal sensor data. We de-
sign a locally differentially private recovery algo-
rithm for the 1-dimensional, discrete heat source
location problem and analyse its performance in
terms of the Earth Mover Distance error. Our
work indicates that it is possible to produce lo-
cally private sensor measurements that both keep
the exact locations of the heat sources private and
permit recovery of the “general geographic vicin-
ity” of the sources. We also discuss the relation-
ship between the property of an inverse problem
being ill-conditioned and the amount of noise
needed to maintain privacy.
1. Introduction
In recent years, wireless technology has allowed the power
of lightweight (thermal, light, motion, etc.) sensors to be
explored. This data offers important benefits to society.
For example, thermal sensor data now plays an important
role in controlling HVAC systems and minimising energy
consumption in smart buildings (Lin et al., 2002; Beltran
et al., 2013). Simultaneously, we have begun to understand
the extent to which our privacy is compromised by allow-
ing this increased level of data collection. In particular,
allowing sensors into the home has resulted in consider-
able privacy concerns. The field of privacy-preserving data
analytics has developed to help alleviate these privacy con-
cerns (Dwork & Roth, 2014). A particular notion of privacy
called differential privacy has emerged as a gold standard
for privacy.
In this work we explore the utility of locally differentially
private thermal sensor data. Our aim is to release use-
ful thermal data while keeping the exact locations of heat
sources private. For example, one might consider these
heat sources to be people, whose locations we aim to keep
private. We consider an idealised, discrete setting of the
heat equation. Our work indicates that it is possible to pro-
duce locally private sensor measurements that both keep
the exact locations of the heat sources private and permit
recovery of the “general vicinity” of the sources. That is,
the locally private data can be used to recover an estimate,
f̂ , that is close to the true source locations, f0, in the Earth
Mover Distance (EMD).
The “local” part refers to the fact that the measurements
are made private before they are sent to and collated by
the data analyst. This is desirable for sensor measurements
since the data analyst (e.g. landlord, building manager, util-
ity company) is often the person the consumer would like
to be protected against. In addition to removing the need
to trust the data analyst, local differential privacy is attrac-
tive from an implementation perspective. It is often the
case with wireless sensors that the data must be commu-
nicated via some untrusted channel (Walters et al., 2007;
FTC, 2015). Usually this step would involve encrypting the
data, incurring significant computational and communica-
tion overhead. However, if the data is made private prior
to being sent, then an argument can then be made that it no
longer needs to be encrypted. In addition, the dataset that
is produced from this process can act like a synthetic data
set for robust statistics.
The heat kernel is an example of a severely ill-conditioned
inverse problem (Weber, 1981). That is, it is well-known
that adding noise to thermal sensor data results in poor re-
covery error in classical norms like the `1 and `2 norms.
Intuitively, this should mean that we do not have to per-
turb the sensor data very much to achieve privacy. It
turns out that while partially true, the definition of dif-
ferential privacy is too strong for this to be true for gen-
eral ill-conditioned inverse problems. We discuss the con-
nections between the property of being ill-conditioned and
how much noise we need to add to make measurements
private.
1.1. Our Contribution
In this work we formulate a notion of local privacy for
physical measurements and demonstrate that it can be used
to produce data that allows a data analyst to recover ap-
proximately where a heat source is but prevents them from
determining precisely where the heat source is. The prob-
lem of interest is stated precisely in Problem 2. We design
a locally differentially private recovery algorithm for the
1-dimensional, discrete heat source location problem us-
ing the Gaussian mechanism and `1 constrained minimisa-
tion. Under some assumptions stated in Corollary 9, Table
ar
X
iv
:1
70
6.
05
91
6v
1 
 [
cs
.C
R
] 
 3
1 
M
ay
 2
01
7
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
Table 1. Asymptotic upper bounds for private recovery
VARIABLE EMD
(
f0
‖f0‖1
, f̂‖f̂‖1
)
n O(1 + 1√
n
)
m O(
√
m)
t O( 1
t
+
√
t+ t2e
−A2
4µt )
1 shows the asymptotics of our theoretical upper bound on
EMD error of the private recovery algorithm, where f0 is
the true source vector, f̂ is our recovered estimate, n is
the size of the discrete universe, m is the number of sen-
sors, t is the time lapse before the measurement is taken,
µ is the diffusion constant and A is a measure of separa-
tion between the sources. The full bound can be found in
Corollary 9.
Our proof of Corollary 9 travels via Theorem 8, which is an
upper bound on the EMD error for recovery of heat sources
from noisy sensor data. Our proof of Theorem 8 is a gen-
eralization of the work of (Li et al., 2014) to more than one
heat source. We then provide a lower bound for the EMD
error of recovery in Theorem 10. Our lower bound asymp-
totically matches our upper bound in its dependence on the
standard deviation of the noise. It also matches experimen-
tal results in its dependence on the number of sensors.
Finally, we explore the relationship between the condi-
tion number of a matrix, M , and the amount of noise we
need to add to the measurements to maintain privacy. That
is, if we have a vector x ∈ Rn and measurement vec-
tor y = Mx, how much noise do we need to add to y to
keep x private? We find that the amount of noise is greater
than 1/(
√
nκ2(M)), where κ2(M) is the condition num-
ber. That is, if a problem is well-conditioned then we nec-
essarily need to add a significant amount of noise to main-
tain privacy. The converse however is not generally true.
It is possible to have a ill-conditioned matrix such that we
still need to add a considerable amount of noise to maintain
privacy.
2. Background and Problem Formulation
2.1. The sparse source recovery problem
We consider the 1-dimensional heat equation on an un-
bounded domain. Let u(x, t) : R × [0,∞) → R be the
temperature at location x at time t, f(x) be the initial tem-
perature (with bounded support) and µ be the diffusion con-
stant. Consider the Cauchy problem for the heat equation
∂u
∂t
= µO2u
where u obeys the boundary conditions
u(x, 0) = f(x) ∀x ∈ R and
lim
|x|→∞
u(x, t) = 0 ∀t ∈ [0,∞).
Let g(x, t) = 1√
4πµt
e
−|x|2
4µt then from standard potential
theory (Guenther & Lee, 1996) we know we can write the
solution as
u(x, t) = (g ∗ f)(x, t) =
∫
R
g(x− y, t)f(y)dy.
We will let T = µt for the remainder the paper. Now, we
consider the discretization of this problem. Let n > 0 and
suppose the support of f is contained in the discrete set
{ 1n , · · · , 1}. The function f can be represented by a vector
f0 ∈ Rn where f(x) =
∑n
i=1(f0)iδ(
i
n − x) so
u(x, t) =
n∑
i=1
(f0)ig
(
i
n
− x, t
)
. (1)
Let m > 0 and suppose we take m measurements at loca-
tions 1m , · · · , 1 so yi = u(
i
m , t) is the measurement of the
sensor at location im . Since Equation 1 is linear we have
y = Af0 where Aij = g
(
i
n
− j
m
, t
)
.
An obvious question then is: given the diffusion constant,
time t and the (possibly noisy) measurements y = Af0,
how well can we reconstruct f0? If the measurements are
not noisy then the answer is we can recover f0 exactly if we
have enough measurements (Candes et al., 2006). However
this problem is severely ill-posed (Weber, 1981), that is,
small errors in the measurement data can lead to very large
errors in the recovered source vector.
The ill-posedness of the heat source inversion problem is
essentially derived from the fact that as heat dissipates, the
measurement vectors for different source vectors become
increasingly close. Thus, we only need to add a small
amount of noise to make it difficult to determine between
them. If the sources were originally far apart then this ef-
fect is less pronounced. Intuitively, if we know there are
not very many heat sources, then we should be able to re-
cover the general vicinity of the sources, even after we add
a small amount of noise. With this in mind, we impose
the following two changes to the problem to make it more
tractable:
• Assume the source vector is sparse, that is ‖f0‖0 = k
where k is small.
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
• Measure the error in terms of the Earth-Mover dis-
tance (EMD).
The EMD, which we will define rigorously later, gives a
measure of how “geographically similar” two distributions
are. In the context of the heat source location problem,
EMD(f0, f̂) being small means that even though we may
not be able to pinpoint exactly where the heat sources are,
we can say approximately where they are.
Problem 1. Suppose that ‖f0‖0 = k and σ > 0. Given
measurements ỹ = Af0 +N(0, σ2Im) can we produce an
estimate f̂ such that EMD(f0, f̂) is small?
2.2. Differential Privacy
Differential privacy has emerged over the past decade as
the leading definition of privacy for privacy-preserving data
analysis. We give a very brief introduction to differential
privacy in this section, a more in-depth introduction can be
found in (Dwork & Roth, 2014). A database is a vector D
in Dn for some data universe D. We call two databases D,
D′ adjacent or “neighbouring” if ‖D −D′‖0 = 1.
Definition 1 ((, δ)-Differential Privacy). (Dwork et al.,
2006) A randomised algorithmA is (, δ)-differentially pri-
vate if for all adjacent databases D, D′ and events E,
P(A(D) ∈ E) ≤ eP(A(D′) ∈ E) + δ.
To understand this definition suppose the database D con-
tains some sensitive information about Charlie and the data
analyst, Lucy, produces some statistic A(D) about the
database D via a differentially private algorithm. Then
Lucy can give Charlie the following guarantee: an adver-
sary given access to the output A(D) can not determine
whether it was sampled from the probability distribution
generated by the algorithm when the database is D or the
distribution generated by D′, where D has Charlie’s true
data and D′ has Charlie’s data replaced with an arbitrary
element of D.
The definition of differential privacy is particularly attrac-
tive for machine learning because of two main properties.
The first is that the guarantee holds regardless of what side-
information an adversary has. The second is that Lucy can
do adaptive data analysis and retain her privacy guarantee
to Charlie. These properties combine to give differentially
private algorithms the ability to be used as building blocks
in designing algorithms.
A locally differentially private algorithm is a private algo-
rithm in which Charlie creates a differentially private ver-
sion of his data before sending it to Lucy. While each indi-
vidual data point is not disclosive, in the aggregate, locally
differentially private data analysis can still produce useful
statistical insights (Ren et al., 2016; Kairouz et al., 2016).
2.3. Locally differentially private heat source location
Let us return to the sparse source recovery problem. As-
sume µ and t are fixed and known. We would like to design
a locally differentially private algorithm to solve Problem
1. Firstly, we need to clarify exactly what “data” we would
like to keep private. We are going to consider the coordi-
nates of f0 to be our data, that is the locations of the heat
sources are what we would like to keep private. We can not
hope to keep the existence of a heat source private and also
recover an estimation to f0 that is close in the EMD. There-
fore, we are going narrow our definition of “neighbouring”
databases.
Definition 2. Two source vectors f0 and f ′0 are neighbours
if there exists i ∈ [n] such that (f0)j = (f ′0)j for all j 6=
i, i+ 1 and (f0)i = (f ′0)i+1 and (f0)i+1 = (f
′
0)i.
That is, two source vectors are neighbours if the differ in
location of a single source by one unit. For example, the
following two source vectors are neighbours (where n =
5).
0 15
2
5
3
5
4
5 1 0
1
5
2
5
3
5
4
5 1
We have access to the source location vectors through the
sensor measurements so the “local” part of our problem is
that we will require each sensor to compute a differentially
private version of its measurement before it sends its data
to a central node. We then wish to use this locally differen-
tially private data to recover an estimate to the source vec-
tor that is close in the EMD. The structure of the problem
is outlined in the following diagram:
f0
A
ym
y1
A
A
ỹm
ỹ1
R
f̂
Problem 2. Design algorithms A and R such that:
1. (Privacy) For all neighbouring source vectors f0 and
f ′0, sensor locations s and Borel measurable sets E
we have
P(A(uf0(s, t)) ∈ E) ≤ eP(A(uf ′0(s, t)) ∈ E) + δ.
2. (Utility) EMD(f0, f̂) is small.
2.4. Related work
An in-depth survey on differential privacy and its links to
machine learning and signal processing can be found in
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
(Sarwate & Chaudhuri, 2013). The Gaussian mechanism
was folklore originally observed in (Dwork et al., 2007)
and a proof can now be found in (Dwork & Roth, 2014).
The body of literature on general and local differential pri-
vacy is vast and so we only mention here work that is di-
rectly related. There is growing body of literature of differ-
entially private sensor data, for example (Liu et al., 2012;
Li et al., 2015; Wang et al., 2016; Jelasity & Birman, 2014;
Eibl & Engel, 2016). Much of this work is concerned with
differentially private release of aggregate statistics derived
from sensor data and the difficulty in maintaining privacy
over a period of time (called the continual monitoring prob-
lem).
Connections between privacy and signal recovery have
been explored previously in the literature. In (2007) Dwork
et al. considered the recovery problem with noisy measure-
ments where the matrix M has i.i.d. standard Gaussian en-
tries. Let x ∈ Rn, y = Mx ∈ Rm where m = Ω(n),
ρ < 0.239.... Suppose y′ is a perturbed version of y such
that a ρ fraction of the measurements are perturbed arbi-
trary and the remaining measurements are correct to within
an error of α. Then (Dwork et al., 2007) concludes that
w.h.p. the constrained `1-minimisation, min ‖y − y′‖1 s.t.
Mx = y, recovers an estimate, x̂, s.t. ‖x − x̂‖1 ≤ O(α).
This is a negative result for privacy. In particular, when
α = 0 it says that providing reasonably accurate answers
to a 0.761 fraction of randomly generated weighted sub-
set sum queries is blatantly non-private. Newer results of
Bun et al. (Bun et al., 2014) can be interpreted in a similar
light where M is a binary matrix. Compressed sensing has
also been used in the privacy literature as a way to reduce
the amount of noise needed to maintain privacy (Li et al.,
2011; Roozgard et al., 2016)
There are also several connections between sparse signal
recovery and inverse problems (Farmer et al., 2013; Burger
et al., 2010; Haber, 2008; Landa et al., 2011). The par-
ticular case of the recovery of sparse heat sources was de-
veloped in (Li et al., 2014). Li et al. use `1 minimisation
to identify and to approximately locate heat sources from
thermal measurements in a physical region. They provide
a theoretical upper bound on the EMD error of their recov-
ery algorithm for a single source. They also develop an
adaptive sampling scheme.
3. Private Algorithm A
In this section we discuss the design of the (, δ)-
differentially private algorithm A from Problem 2. Since
we are considering the application of lightweight sensors,
the algorithm is simply going to be that each sensor will lo-
cally add Gaussian noise to their measurement before send-
ing to the central node. The question then is, how much
noise should we add to maintain privacy?
The following lemma is a standard result from the differ-
ential privacy literature that will help us answer the above
question. It says essentially that the standard deviation
of the noise added to a statistic should be proportional
to how much the statistic can vary between neighbour-
ing data sets. Let f : Dn → Rn be a function and let
42f = maxD,D′neighbours ‖f(D) − f(D′)‖2 (called the `2
sensitivity of f ).
Lemma 3 (The Gaussian Mechanism). (Dwork & Roth,
2014) Let  > 0, δ > 0 and σ = 2 ln(1.25/δ)42f then
A(D) ∼ f(D) +N(0, σ2In)
is an (, δ)-differentially private algorithm.
Let Ai be the ith column of A.
Proposition 4. With the definition of neighbours presented
in Definition 2 and restricting to f0 ∈ [0, 1]n we have
A(uf0(s, t)) ∼ uf0(s, t) +
2 log(1.25/δ)42(A)

N(0, 1)
is a (, δ)-differentially private algorithm where
42(A) = max
i∈[n]
‖Ai −Ai+1‖2 = O
( √
m
nT 1.5
)
Proof. Suppose f0 and f ′0 are neighbours where a =
(f0)i = (f
′
0)i+1 and b = (f0)i+1 = (f
′
0)i. Then
‖Af0 − Af ′0‖2 = |a − b|‖Ai − Ai+1‖2 and so 42(A) =
maxi∈[n] ‖Ai − Ai+1‖2. Then the fact that the algorithm
is (, δ)-differentially private follows from Lemma 3. The
proof of the upper bound on 42(A) in Proposition 4 is a
straightforward calculation and can be found in Appendix
A.
This result is a little misleading in the parameter n since our
notion of neighbours depends on n. That is, two neighbours
are at EMD at most 1n . Computational experiments found
in Appendix A indicate that this analysis is asymptotically
tight in m, n and T .
3.1. Preserving Privacy for Ill-Conditioned Inverse
Problems
We digress for a moment to discuss the intuition for general
inverse problems. As mentioned earlier, the heat source lo-
cation problem is ill-conditioned, that is, it behaves poorly
under addition of noise to the sensor measurements. In-
tuitively, this should mean we only need to add a small
amount of noise to mask the original data. In this discus-
sion we will see that this statement is partially true. There is
however, a fundamental difference between the notion of a
problem being ill-conditioned (as defined by the condition
number) and being easily kept private.
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
Let M be a m × n matrix and consider the general in-
verse problem of recovering x0 from the measurement vec-
tor Mx0. The condition number, κ2(M), is a measure of
how ill-conditioned this inverse problem is. It is defined as
κ2(M) := max
e,b
‖A−1b‖2
‖A−1e‖2
‖e‖2
‖b‖2
=
σmax(M)
σmin(M)
.
The larger the condition number the more ill-conditioned
the problem is (Belsley et al., 1980). The following ma-
trix illustrates the difference between how ill-conditioned a
matrix is and how much noise we need to add to maintain
privacy. Suppose
M =
(
1 0
0 ρ
)
where ρ < 1 is small. While this problem is ill-
conditioned, κ2(M) = 1/ρ is large, we still need to add
considerable noise to the first coordinate of Mx0 to main-
tain privacy. The difference is, intuitively, that the problem
being ill-conditioned means that there exists a coordinate
that can be hidden well by adding a small amount of noise,
while for privacy we need that all coordinates are hidden.
The converse is however true. That is, if the amount of
noise we need to add,42(M), is small then the problem is
necessarily ill-conditioned.
Lemma 5. Let M be a matrix such that ‖M‖2 = 1 and
suppose the domain is [0, 1]n, then
42(M) ≥
1
κ2(M)
√
n
.
Proof.
1
κ2(M)
= min
rankE<min{m,n}
‖M − E‖2
≤ ‖M − [M1 M1 M2 · · ·Mn−1]‖2
≤
√
nmax
i
‖Mi −Mi+1‖2 =
√
n42(M).
Note that our definitions of neighbours was not particularly
important for Lemma 5. Indeed, the same statement would
hold for any more general notion of neighbours such as
‖x0 − x′0‖2 ≤ 1 or ‖x0 − x′0‖0 ≤ 1.
4. Noisy Recovery
In this section we present and analyse an algorithm R to
solve Problem 1. We will then use this algorithm to ex-
plore how well we can recover the heat source locations
from the locally private data. As has been studied exten-
sively in the compressed sensing literature, one can solve
Algorithm 1 R: Constrained `1 minimisation recovery al-
gorithm
Input: A, σ > 0, ỹ
Output: f̂ ∈ [0, 1]n
f̂ = arg minf∈[0,1]n ‖f‖1 s.t. ‖Af − ỹ‖2 ≤ σ
√
m
`0 problems (the problem of finding a sparse vector) by re-
ducing to the `1 convex relaxation if the matrix A satisfies
the restricted isometry property (RIP) (Donoho, 2006). The
heat kernel matrixA does not satisfy the RIP but we still re-
duce to the `1 convex relaxation to promote sparsity while
allowing the problem to be computationally tractable. This
use of constrained `1-minimisation to recover heat sources
was introduced in Li et al. (2014). Li et al. also discuss the
use of Bregman iteration to solve the `1 minimisation prob-
lem as a sequence of unconstrained problems. We will not
touch on specific algorithms for finding the minimiser in
this work. Suppose ỹ ∼ Af0 +N(0, σ2Im). The recovery
algorithm R is outlined in Algorithm 1.
The constraint ‖Af − ỹ‖2 ≤ σ
√
m is derived from the
following lemma.
Lemma 6. (Hsu et al., 2012) Let ν ∼ N(0, σ2Im) then for
all t > 0,
P[‖ν‖22 > σ2(m+ 2
√
mt+ 2t)] ≤ e−t.
So for large m and small ρ, we have ‖ν‖2 ≤ (1 + ρ)σ
√
m
with high probability.
Lemma 6 implies that for large enough m, w.h.p. we have
f0 is a feasible point for the constrained optimisation in Al-
gorithm 1. Figure 1 demonstrates the typical behaviour of
the algorithmR. As can be seen in the figure, this algorithm
returns an estimate f̂ that is indeed close to f0 in the EMD
but not necessarily close in more traditional norms like the
`1 and `2 norms. This phenomenon was noticed by Li et
al. who proved that if f0 consists of a single source then
EMD(f0, f̂) is small where f̂ = R(ỹ) (Li et al., 2014).
4.1. Earth Mover Distance error for constrained
`1-minimisation
As mentioned earlier, the error norm we are concerned with
is the Earth Mover Distance (EMD). The EMD can be de-
fined between any two probability distributions on a finite
discrete metric space (Ω, d(·, ·)). It can be understood as
follows: imagine each probability distribution as represent-
ing the depth of dirt on the space then the EMD between the
two distributions is the amount of “effort” it takes to trans-
form one distribution into the other. The amount of effort
depends on both how much dirt is being moved and how
far it is being moved.
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 1. The red line represents f0 and the blue (dashed) line rep-
resents f̂ = R(ỹ) (both normalised to have unit `1 norm) where
n = 100, m = 50, T = 0.5 and σ = 0.5.
Definition 7. (Rubner et al., 2000) Let
P = {(x1, p1), · · · , (xn, pn)} and Q =
{(x1, q1), · · · , (xn, qn)} be two probability distribu-
tions on the space {x1, · · · , xn}. Now, let
f∗ = arg min
f∈[0,1]n×n
n∑
i=1
n∑
j=1
fijd(xi, xj)
s.t. fij ≥ 0 ∀i, j ∈ [m]
n∑
j=1
fij ≤ pi ∀i ∈ [m]
n∑
i=1
fij ≤ qi ∀i ∈ [n]
n∑
i=1
n∑
i=1
fij = 1
then EMD(P,Q) =
∑n
i=1
∑n
j=1 f
∗
ijd(xi, xj).
We can interpret fij as the amount of dirt that is moved
from xi in Q to xj in P . We can now state our recovery
bound.
Theorem 8. Suppose that f0 is a source vector, ŷ = R(ỹ)
and assume the following:
1. m
√
T/2 > 1
2.
√
2T < 1
3. |xi − xj | >
√
2T + 2A for some A > 0
4.
√
T (2σ + k√
2π
e
−A2
4T ) ≤ c < 1
Table 2. Asymptotic upper bounds for noisy recovery from Theo-
rem 8
VARIABLE EMD
(
f0
‖f0‖1
, f̂‖f̂‖1
)
n O(1)
m O(1)
T O(
√
T + T 2)
σ O(σ)
for some constant c, then w.h.p.
EMD
(
f0
‖f0‖1
,
f̂
‖f̂‖1
)
≤
O
[
1
1− c
[
1
k
e
−1
2m2T T 0.75
√
2σ +
k√
2π
e
−A2
4T
+
(
k
√
T +
T 2
k
)
e
−1
4m2T
(
2σ +
k√
2π
e
−A2
4T
)]]
First, let us parse the assumptions of Theorem 8. A picture
of the set-up for k = 2 is provided below:
x1 x2
√
2T 2A
√
2T
Assumption 1 states that m needs to be large enough that
for each possible source, there are sensors near it. Assump-
tion 2 says that you need to take the measurements reason-
ably quickly. The exact bound of 1 comes from the fact
that we are looking at the interval [0, 1] so we can interpret
this assumption as saying that you should take the measure-
ments before the heat mass leaves the interval. Assumption
3 says that the sources need to be sufficiently far apart. We
will discuss later how the algorithm fails if the sources are
too close. The final assumption, Assumption 4, says that
if we increase the noise or the number of sources, then we
need to take the measurements sooner to compensate.
Table 2 describes the asymptotics of Theorem 8 if all vari-
ables except the variable denoted in the left column are
held constant. In Section 5 we discuss the tightness of this
bound. The result is a generalisation of a result of Li et al.
(2014) to source vectors with more than one source. Our
proof is a generalisation of Li et al.’s proof. The details can
be found in Appendix B.
Finally, we can state our EMD error bound for recovery
of heat source locations from locally differentially private
thermal sensor data.
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
Corollary 9. Suppose Assumptions (1) - (3) from Theorem
8 hold and the following two Assumptions replace Assump-
tion (4),
1.
√
m
nT = o(1)
2. k
√
T ≤
√
π.
Then there exists a locally differentially private algo-
rithm A and recovery algorithm R such that if f̂ =
R(A(y1), · · · ,A(ym)) then w.h.p.
EMD
(
f0
‖f0‖1
,
f̂
‖f̂‖1
)
≤ O
[
1
k
e
−1
2m2T T 0.75
√
2
√
m log(1/δ)
nT 1.5
+
k√
2π
e
−A2
4T
+
(
k
√
T +
T 2
k
)
e
−1
4m2T
(
2
√
m log(1/δ)
nT 1.5
+
k√
2π
e
−A2
4T
)]
Assumptions 1 and 2 ensure that Assumption 4 of Theo-
rem 8 hold. The asymptotics of this bound were contained
in Table 1 in Section 1.1. It is interesting to note that, un-
like in the constant σ case, the error increases as T → 0.
This is because as T → 0 the inverse problem becomes
less ill-conditioned so we need to add more noise. As the
heat dissipates we don’t need to add as much noise to the
measurements to keep the source locations private, but the
unperturbed measurements become less useful so the EMD
begins to increase again.
4.2. Lower bound on Estimation Error
The following theorem gives a lower bound on the estima-
tion error of the noisy recovery problem.
Theorem 10. We have
inf
f̂
sup
f0
E[EMD(f0, f̂)] = Ω
(
min
{
1
2
,
T 1.5σ√
m
})
.
where inf f̂ is the infimum over all estimators f̂ : R
m →
[0, 1]n, supf0 is the supremum over all source vectors in
[0, 1]n and ỹ is sampled from y +N(0, σ2Im).
Note that this lower bound matches our upper bound
asymptotically in σ and is slightly loose in T . It varies
by a factor of
√
m from our theoretical upper bound but
matches our experimental results in m. The proof of The-
orem 10 is an application of Fano’s lemma. We first up-
per bound the KL-divergence between the distributions on
measurements generated by two source vectors f0 and f ′0
in terms of EMD(f0, f ′0). We then find a small class T of
source vectors such that for every pair in T we can upper
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
(a) f0 has unit peaks at 0.41
and 0.59.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
(b) f0 has unit peaks 0.76 and
0.24.
Figure 2. Each graph shows the result of the recovery algorithm
R on noisy data with n = 100, m = 50, T = 0.05 and σ = 0.1.
In each case, the red line is f0, the blue (dashed) line is f̂ , k = 2
and the heat sources have unit weight.
bound the KL-divergence and lower bound the EMD. The
details can be found in Appendix C.
Recall, from Section 2.1, that the measurement function u
is a linear combination of Gaussian functions, or an unnor-
malised Gaussian Mixture Model (GMM). Thus our mea-
surements, yi are evaluations of a GMM. It has been noted
in the GMM literature that it is difficult to determine be-
tween the sum of two overlapping normal distributions and
the normal distribution centred between them (Dasgupta,
1999). If two sources are close enough together then the
algorithm will often output an estimate that has the mass
centred between the true source locations. In Figure 2 we
can see the difference between the behaviour of the algo-
rithm when the sources are close together (Figure 2a) com-
pared to when they are further apart (Figure 2b).
A consequence of Theorem 10 is that that if two
peaks are too close together, roughly at a distance of
O
(
min{ 12 ,
T 1.5σ√
m
)
, then it is impossible for an estimator to
differentiate between the true source vector and the source
vector that has a single peak located in the middle.
We have not included the parameter k in the asymptotic
tables because our theoretical bound does not adequately
explain the experimental results we will present in Section
5. It appears from empirical results that the EMD error de-
cays as we increase k. The authors have not yet found an
analytical explanation for this phenomenon however, we
postulate that is it caused by the algorithm failing to dif-
ferentiate between close peaks. If the source locations are
chosen randomly then as we increase k, the sources will
become close enough that this effect kicks in. So, suppose
that f̂ always has the mass centred between the true source
locations. Then as k increases, the EMD error between f0
and f̂ will decrease.
5. Experimental Results
In this section we present experimental results. The
source vectors are n dimensional vectors where the possi-
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
0 0.2 0.4 0.6 0.8 1 1.2
0
-0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
EM
D
(a) Dependence on n
0 20 40 60 80 100 120
m
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
EM
D
(b) Dependence on m
0 0.02 0.04 0.06 0.08 0.1 0.12
t
-0.005
0
0.005
0.01
0.015
0.02
0.025
EM
D
(c) Dependence on t
0 20 40 60 80 100 120
k
0
0.05
0.1
0.15
0.2
0.25
EM
D
(d) Dependence on k
Figure 3. Empirical results for the EMD error of experiments run-
ning Algorithm R on measurement data with added gaussian
noise with variance σ2. Unless specified otherwise, n = 100,
m = 50, T = 0.5, σ = 0.1 and k = 1. In (3c), σ = 0.2.
ble source locations are { 1n , · · · , 1}. The sensors are placed
at locations { 1m , · · · , 1}. The diffusion constant, µ, is equal
to 1/2 for all experiments. In all graphs, all but one of the
variables n, m, t, σ,  or δ are kept constant. The EMD be-
tween two vectors is computed by first scaling both vectors
to have unit `1 norm, then computing the EMD between
the normalised vectors. In experiments where k = 1, the
source is always at 0.5. If k > 1 then k source locations
are chosen uniformly at random. All sources have unit in-
tensity.
All code was run in MATLAB R2016a. In all cases where
confidence bars are present, the bars are 95% confidence in-
tervals computed after performing 10 trials with fixed pa-
rameters. The sensitivity 42(A) = maxi ‖Ai − Ai+1‖2
was computed exactly for each trial.
Figure 3 shows the results of simulations of Algorithm 1
on noisy thermal measurements. These simulations sug-
gest that Theorem 8 is asymptotically tight in n and σ. It
is more difficult to determine whether the empirical results
suggest tightness is T , although it certainly does not con-
tradict it. The result for m is inconclusive, but it appears
that the bound is loose by a factor of
√
m. As discussed
earlier, the bound is not tight in the number of sources, k.
Figure 4 shows the results of simulations of our locally dif-
ferentially private recovery algorithm. The results suggest
that Theorem 9 is asymptotically tight in n, while it is loose
by a factor of approximately
√
m in m. It is difficult to de-
termine the asymptotic nature in T from the empirical re-
sult however the shape is consistent with the upper bound
-50 0 50 100 150 200 250
n
-0.2
0
0.2
0.4
0.6
0.8
1
EM
D
(a) Dependence on n
0 50 100 150 200
m
-0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
EM
D
(b) Dependence on m
0 0.2 0.4 0.6 0.8 1 1.2
t
-0.2
0
0.2
0.4
0.6
0.8
EM
D
(c) Dependence on t
0 0.2 0.4 0.6 0.8 1 1.2
0
-0.2
0
0.2
0.4
0.6
0.8
1
EM
D
(d) Dependence on 
Figure 4. Empirical results for the EMD error of experiments run-
ning Algorithm 1 on the locally differentially private thermal mea-
surements. Unless specified otherwise, n = 100, m = 50,
t = 0.1, δ = 0.1 and  = 1.
given. In particular both empirical and theoretical results
imply that the EMD error increases both as T → 0 and
T →∞.
6. Future Work
As discussed earlier, the heat source location problem is
related to the problem of estimating GMMs. A GMM is
a probability distribution that is a weighted sum of gaus-
sian distributions. The problem here is to reconstruct the
probability distribution from a dataset sampled i.i.d. from
the distribution. This problem is well-studied and there ex-
ist algorithms that w.h.p. recover accurate estimates of the
means (Dasgupta, 1999; Kalai et al., 2012). It would be
interesting in the future to explore how these algorithms
(for example, Expectation-Maximisation) compare to `1-
minimisation for noisy recovery.
In this paper we discussed the relationship between ill-
conditioned inverse problems and the amount of noise we
need to achieve privacy. We found that while the condition
number and the sensitivity are related, it is possible to have
an ill-conditioned problem that still requires a lot of noise
to achieve privacy. We would be interesting in exploring
this relationship further. For example, what happens if we
relax our definition of privacy to only requiring a fraction
of the source locations to be kept private? Also, how does
the distribution of the singular values affect the amount of
noise needed for privacy?
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
References
Belsley, David A., Kuh, Edwin, and Welsch, Roy E. Re-
gression Diagnostics: Identifying influential data and
sources of collinearity. John Wiley & Sons, New York,
Chichester, 1980.
Beltran, Alex, Erickson, Varick L., and Cerpa, Alberto E.
Thermosense: Occupancy thermal based sensing for
hvac control. In Proceedings of the 5th ACM Workshop
on Embedded Systems For Energy-Efficient Buildings,
BuildSys’13, pp. 11:1–11:8, New York, NY, USA, 2013.
ACM.
Bun, Mark, Ullman, Jonathan, and Vadhan, Salil. Finger-
printing codes and the price of approximate differential
privacy. In Proceedings of the Forty-sixth Annual ACM
Symposium on Theory of Computing, STOC ’14, pp. 1–
10, New York, NY, USA, 2014. ACM.
Burger, Martin, Landa, Yanina, Tanushev, Nicolay M., and
Tsai, Richard. Discovering a Point Source in Unknown
Environments, pp. 663–678. Springer Berlin Heidelberg,
Berlin, Heidelberg, 2010.
Candes, E. J., Romberg, J., and Tao, T. Robust uncertainty
principles: exact signal reconstruction from highly in-
complete frequency information. IEEE Transactions on
Information Theory, 52(2):489–509, Feb 2006.
Dasgupta, S. Learning mixtures of gaussians. In 40th An-
nual Symposium on Foundations of Computer Science
(Cat. No.99CB37039), pp. 634–644, 1999.
Donoho, D. L. Compressed sensing. IEEE Trans. Inf.
Theor., 52(4):1289–1306, April 2006.
Dwork, Cynthia and Roth, Aaron. The algorithmic founda-
tions of differential privacy. Found. Trends Theor. Com-
put. Sci., 9:211–407, August 2014.
Dwork, Cynthia, McSherry, Frank, Nissim, Kobbi, and
Smith, Adam. Calibrating noise to sensitivity in private
data analysis. In Proceedings of the Third Conference on
Theory of Cryptography, TCC’06, pp. 265–284, Berlin,
Heidelberg, 2006. Springer-Verlag.
Dwork, Cynthia, McSherry, Frank, and Talwar, Kunal. The
price of privacy and the limits of lp decoding. In Pro-
ceedings of the Thirty-ninth Annual ACM Symposium on
Theory of Computing, STOC ’07, pp. 85–94, New York,
NY, USA, 2007. ACM.
Eibl, Günther and Engel, Dominik. Differential privacy for
real smart metering data. Computer Science - Research
and Development, pp. 1–10, 2016.
Farmer, Brittan, Hall, Cassandra, and Esedolu, Selim.
Source identification from line integral measurements
and simple atmospheric models. Inverse Problems and
Imaging, 7(2):471–490, 2013.
FTC. Ftc staff report: Internet of Things: Privacy & Se-
curity in a Connected World. Technical report, Federal
Trade Commission, January 2015.
Guenther, R.B. and Lee, J.W. Partial Differential Equa-
tions of Mathematical Physics and Integral Equations.
Dover books on mathematics. Dover Publications, 1996.
Haber, E. Numerical methods for optimal experimental de-
sign of large-scale ill-posed problems. Inverse Problems,
24, 2008.
Hsu, Daniel, Kakade, Sham, and Zhang, Tong. A tail
inequality for quadratic forms of subgaussian random
vectors. Electronic Communications in Probability, 17,
2012.
Jelasity, Márk and Birman, Kenneth P. Distributional dif-
ferential privacy for large-scale smart metering. In Pro-
ceedings of the 2Nd ACM Workshop on Information Hid-
ing and Multimedia Security, pp. 141–146, New York,
NY, USA, 2014. ACM.
Kairouz, Peter, Oh, Sewoong, and Viswanath, Pramod.
Extremal mechanisms for local differential privacy. J.
Mach. Learn. Res., 17(1):492–542, January 2016.
Kalai, Adam Tauman, Moitra, Ankur, and Valiant, Gre-
gory. Disentangling gaussians. Commun. ACM, 55(2):
113–120, February 2012.
Landa, Y., Tanushev, N., and Tsai, R. Discovery of point
sources in the Helmholtz equation posed in unknown do-
mains with obstacles. Comm. in Math. Sci., 9:903–928,
2011.
Li, C., Zhou, P., and Jiang, T. Differential privacy and dis-
tributed online learning for wireless big data. In 2015
International Conference on Wireless Communications
Signal Processing (WCSP), pp. 1–5, Oct 2015.
Li, Yang D., Zhang, Zhenjie, Winslett, Marianne, and
Yang, Yin. Compressive mechanism: Utilizing sparse
representation in differential privacy. In Proceedings of
the 10th Annual ACM Workshop on Privacy in the Elec-
tronic Society, WPES ’11, pp. 177–182, New York, NY,
USA, 2011. ACM.
Li, Yingying, Osher, Stanley, and Tsai, Richard. Heat
source identification based on L1 constrained minimiza-
tion. Inverse Problems and Imaging, 1(1), 2014.
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
Lin, Craig, Federspiel, Clifford C., and Auslander,
David M. Multi-sensor single-actuator control of hvac
systems. In Proc. Int. Conf. Enhanced Building Opera-
tions, Richardson, TX, October 2002.
Liu, He, Saroiu, Stefan, Wolman, Alec, and Raj, Himan-
shu. Software abstractions for trusted sensors. In Pro-
ceedings of the 10th International Conference on Mobile
Systems, Applications, and Services, MobiSys ’12, pp.
365–378, New York, NY, USA, 2012. ACM.
Ren, Xuebin, Yu, Chia-Mu, Yu, Weiren, Yang, Shusen,
McCann, Julie A., and Yu, Phlip S. LoPub: High-
Dimensional Crowdsourced Data Publication with Lo-
cal Differential Privacy. arXiv:1612.04350, December
2016.
Roozgard, Aminmohammad, Barzigar, Nafise, Verma,
Pramode, and Cheng, Samuel. Genomic data privacy
protection using compressed sensing. Trans. Data Pri-
vacy, 9(1):1–13, April 2016.
Rubner, Yossi, Tomasi, Carlo, and Guibas, Leonidas J. The
earth mover’s distance as a metric for image retrieval. In-
ternational Journal of Computer Vision, 40(2):99–121,
2000.
Sarwate, A. D. and Chaudhuri, K. Signal processing and
machine learning with differential privacy: Algorithms
and challenges for continuous data. IEEE Signal Pro-
cessing Magazine, 30(5):86–94, Sept 2013.
Walters, J.P., Liang, Z., Shi, W., and Chaudhary, V. Wire-
less Sensor Network Security: A Survey, pp. 367. CRC
Press: Boca Raton, FL, USA, 2007.
Wang, L., Zhang, D., Yang, D., Lim, B. Y., and Ma, X.
Differential location privacy for sparse mobile crowd-
sensing. In 2016 IEEE 16th International Conference
on Data Mining (ICDM), pp. 1257–1262, Dec 2016.
Weber, Charles F. Analysis and solution of the ill-posed in-
verse heat conduction problem. International Journal of
Heat and Mass Transfer, 24(11):1783–1792, November
1981.
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
0 100 200 300 400 500 600 700 800 900 1000
m
0
0.005
0.01
0.015
(4
2
(A
))
2
(a) Dependence on m
0 100 200 300 400 500 600 700 800 900 1000
n
0
5
10
15
20
25
1=
(4
2
(A
))
(b) Dependence on n
0 100 200 300 400 500 600 700 800 900 1000
t
0
5
10
15
20
25
30
35
40
45
1=
(4
2
(A
))
2=
3
(c) Dependence on t
Figure 5. Empirical results of computation of 42(A). Unless specified otherwise, m = 500 and t = 0.1. In (5a), n = 500 and in (5c),
n = 1000.
A. Proof of upper bound in Proposition 4
The following is a proof of the asymptotic upper bound in Proposition 4.
Proof of upper bound in Proposition 4. For all i ∈ [n] we have
‖Ai −Ai+1‖22 =
1
4πT
m∑
j=1
(
e
−( i
n
− j
m
)2
4T − e
−( i+1
n
− j
m
)2
4T
)2
=
1
4πT
m∑
j=1
e
−( i
n
− j
m
)2
2T
(
1− e
( i
n
− j
m
)2−( i+1
n
− j
m
)2
4T
)2
≤ 1
4πT
max
i∈[n]
max
j∈[m]
(
1− e
( i
n
− j
m
)2−( i+1
n
− j
m
)2
4T
)2 m∑
j=1
e
−( i
n
− j
m
)2
2T
Now,
∑m
j=1 e
−( i
n
− j
m
)2
2T ≤ m and
max
i∈[n]
max
j∈[m]
(
1− e
( i
n
− j
m
)2−( i+1
n
− j
m
)2
4T
)2
≤ max{(1− e
−3
4nT )2, (1− e 24nT )2} = O
(
1
n2T 2
)
.
Therefore,
‖Ai −Ai+1‖2 = O
( √
m
nT 1.5
)
.
Figure 5 shows calculations of42(A) with varying parameters. The vertical axes are scaled to emphasis the asymptotics.
These calculations suggest that the analysis in Proposition 4 is asymptotically tight in m, n and T .
B. Proof of Theorem 8
The following lemma is from (Li et al., 2014). Since T = µt is fixed we will let g(x) = g(x, t).
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
Lemma 11. (Li et al., 2014) Suppose s1 < x < s2 and |s1 − s2| ≤
√
2T and consider the function W (z) = −g′(s2 −
x)g(z − s1)− g′(x− s1)g(s2 − z). Then W (z) has a single maximum at x and
W (x)−W (z)
> W (x)−W (s2 −
√
2T ) for z ≤ s2 −
√
2T
≥ C1‖z − x‖22 for z ∈ [s2 −
√
2T , s1 +
√
2T ]
> W (x)−W (s1 +
√
2T ) for z ≥ s1 +
√
2T
where C1 = infz∈[s2−
√
2T ,s1+
√
2T ][−W ′′(z)/2] > 0.
The following two lemmas are necessary for our proof of Theorem 8. For all i ∈ [k] and j ∈ [p], letWij(z) = −g′(sij+p−
xi)g(z− sij )− g′(xi− sij )g(sij+p − z). Let p = m
√
T/2. We will often replace the distance between sij and sij+p with√
T/2 since it is asymptotically equal to the true distance p/m = bm
√
T/c/m in m.
Lemma 12. Using the assumptions of Theorem 8 we have
p∑
j=1
inf
z∈[sij+p−
√
2T ,sij+
√
2T ]
[−W ′′ij(z)/2] ≥ Ω
(
m
√
T/8 + 1
T 2.5
)
Proof. Note first that sij+p−x1 =
√
T/2− (xi−sij ) and sij+p−z =
√
T/2− (z−sij ) for any z ∈ [sij+p−
√
2T , sij +√
2T ]. Let z ∈ [sij+p −
√
2T , sij +
√
2T ] then
−W ′′ij(z) = g′(sij+p − xi)g′′(z − sij ) + g′(xi − sij )g′′(sij+p − z)
=
1
16πT 3
[
(sij+p − xi)
(
1−
(z − sij )2
4T
)
e
−(sij+p−xi)
2−(z−sij )
2
4T
+ (xi − sij )
(
1−
(sij+p − z)2
4T
)
e
−(xi−sij )
2−(sij+p−z)
2
4T
]
≥ 1
2T
√
4πT
1
2T
√
4πT
e
−5
8
[
(sij+p − xi)
(
1−
(z − sij )2
4T
)
+ (xi − sij )
(
1−
(sij+p − z)2
4T
)]
≥ e
−5
8
16πT 3
min{(sij+p − xi), (xi − sij )}
(
2−
(z − sij )2
4T
−
(sij+p − z)2
4T
)
≥ e
−5
8
16πT 3
min{(sij+p − xi), (xi − sij )}
3
4
Therefore,
p∑
j=1
inf
z∈[sij+p−
√
2T ,sij+
√
2T ]
[−W ′′ij(z)/2] ≥
3e
−5
8
64πT 3
p∑
j=1
min{(sij+p − xi), (xi − sij )}
=
3e
−5
8
64πT 3
2
p/2∑
i=1
i
m
=
3e
−5
8
64πT 3
2
m
√
T/8(m
√
T/8 + 1)
2m
Lemma 13. Using the assumptions of Theorem 8 we have
min
i∈[k]
min
l:l/n6∈Si
p∑
j=1
(Wij(xi)−Wij(l/n))
= Ω
(
m
√
T/2(m
√
T/2 + 1)2
6m2
e
−1
8
3
256π
√
4πT 3.5
e
−1
4T
)
.
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
Proof. From Lemma 11 we know for all l s.t. l/n 6∈ Si we have
Wij(xi)−Wij(l/n) ≥ min{Wij(xi)−Wij(sij+p −
√
2T ),Wij(xi)−Wij(sij +
√
2T )}.
Let’s start with
Wij(xi)−Wij(sij+p −
√
2T )
= −g′(sij+p − xi)
(
g(xi − sij )− g(sij+p −
√
2T − sij )
)
− g′(xi − sij )
(
g(sij+p − xi)− g(
√
2T )
)
.
Now, g(z) is concave down for z ∈ [−
√
2T ,
√
2T ] and m-strongly concave on the interval [−
√
T/2,
√
T/2] with m =
−3
8
√
4πT 1.5
e
−1
4T so
g(x)− g(z)
{
≥ −g′(x)(y − x) for x, z ∈ [−
√
2T ,
√
2T ]
≥ −g′(x)(y − x) + 3
8
√
4πT 1.5
e
−1
4T (y − x)2 for x, z ∈ [−
√
T/2,
√
T/2]
Thus, since |sij+p − sij | = p/m ∼
√
T/2 and |xi − sij | ≤
√
T/2 and |sij+p − xi| ≤
√
T/2 we have
Wij(xi)−Wij(sij+p −
√
2T )
≥ −g′(sij+p − xi)
(
−g′(xi − sij )(sij+p − xi −
√
2T ) +
√
3
8
√
4πT 1.5
e
−1
4T (sij+p − xi −
√
2T )2
)
− g′(xi − sij )
(
−g′(sij+p − xi)(
√
2T − sij+p − xi)
)
= g′(sij+p − xi)g′(xi − sij )
3
8
√
4πT 1.5
e
−1
4T (sij+p − xi −
√
2T )2
≥ (sij+p − xi)(xi − sij )e
−(sij+p−xi)
2−(xi−sij )
2
4T
3
256π
√
4πT 3.5
e
−1
4T
≥ (sij+p − xi)(xi − sij )e
−1
8
3
256π
√
4πT 3.5
e
−1
4T
where the last inequality follow since 0 ≤ xi − sij =
√
T/2− (sij+p − xi) ≤
√
T/2. Now,
p∑
j=1
(sij+p − xi)(xi − sij ) ≥
p∑
j=1
j
m
(
√
T/2− j
m
)
=
p(p+ 1)(3m
√
T/2 + 2p+ 1
6m2
∼
m
√
T/2(m
√
T/2 + 1)2
6m2
We can now turn to our proof of the upper bound on the EMD error of Algorithm 1.
Proof of Theorem 8. Let Si = (xi −
√
T/2, xi +
√
T/2) ∩ [0, 1] for i ∈ [k]. Then we have that the Si’s are disjoint and
each interval Si contains
√
2Tm sensors. Let p = b
√
T/2mc and let si1 < · · · < sip be the locations of the sensors in Si
to the left of xi and sip+1 > · · · > si2p be the locations to the right. By Condition 3 we know that for any pair l ∈ Ti and
scj where i 6= c we have |l/n− scj | ≥ A.
For all i ∈ [k] and j ∈ [p], let Wij(z) = −g′(sij+p − xi)g(z − sij ) − g′(xi − sij )g(sij+p − z). Let Ti be the set of all
l ∈ [n] such that l/n ∈ (xi − |xi−xi−1|2 ) ∩ [0, 1] then
g(xi − sij )−
∑
l∈Ti
f̂lg(l/n− sij ) ≤ yij − ŷij +
‖x̂‖1√
4πT
e
−A2
4T .
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
Therefore,
k∑
i=1
p∑
j=1
∣∣∣∣∣Wij(xi)−∑
l∈Ti
f̂lWij(l/n)
∣∣∣∣∣
=
k∑
i=1
p∑
j=1
[
−g′(sj+p − xi)
∣∣∣∣∣g(xi − sij )−∑
l∈Ti
f̂lg(l/n− sij )
∣∣∣∣∣− g′(xi − sij )
∣∣∣∣∣g(sij+p − xi)−∑
l∈Ti
f̂lg(sij − l/n)
∣∣∣∣∣
]
≤ C2
[
‖y − ŷ‖1 +
2p‖x̂‖1√
4πT
e
−A2
4T
]
≤ C2
[√
m‖y − ŷ‖2 +
2p‖x̂‖1√
4πT
e
−A2
4T
]
≤ C2
[
2σm+
mk√
2π
e
−A2
4T
]
= B
where C2 = maxi∈[k] maxj∈[2p][−g′(|sij − xi|)] and the last inequality holds with high probability from Lemma 6.
Conversely, by Lemma 11, we have
k∑
i=1
p∑
j=1
∣∣∣∣∣Wij(xi)−∑
l∈Ti
f̂lWij(l/n)
∣∣∣∣∣
≥
p∑
j=1
[
k∑
i=1
(
1−
∑
l∈Ti
f̂l
)
Wij(xi) +
k∑
i=1
∑
l∈Ti
f̂l(Wij(xi)−Wij(l/n))
]
≥
p∑
j=1
k∑
i=1
(
1−
∑
l∈Ti
f̂l
)
Wij(xi) +
p∑
j=1
k∑
i=1
∑
l:l/n∈Si
f̂lC
j
1(xi − l/n)2 +
k∑
i=1
∑
l:l/n6∈Si
C3f̂l
≥
p∑
j=1
k∑
i=1
(
1−
∑
l∈Ti
f̂l
)
Wij(xi) + C5
k∑
i=1
∑
l:l/n∈Si
f̂l(xi − l/n)2 +
k∑
i=1
∑
l:l/n6∈Si
C3f̂l
where C3 = mini∈[k] minl:l/n6∈Si
∑p
j=1(Wij(xi)−Wij(l/n)) ≥ 0.
Now, by the uniformity of the sensor locations, Wj = Wij (xi) = Wi′j (xi′) so
∑p
j=1
∑k
i=1
(
1−
∑
l∈Ti f̂l
)
Wij(xi) =∑p
j=1
∑k
i=1
(
1−
∑
l∈Ti f̂l
)
Wj ≥
∑p
j=1Wj(k−‖f̂‖1) ≥ 0. Similarly the other two terms are both positive. Therefore,∑k
i=1
∑
l:l/n6∈Si f̂l ≤ B/C3 or equivalently,
k∑
i=1
∑
l:l/n∈Si
f̂l ≥ ‖f̂‖1 −B/C3.
This implies that most of the weight of the estimate f̂ is contained in the intervals S1, · · · , Sk. Also,
B ≥
p∑
j=1
|Wij(xi)−
∑
i∈Ti
f̂lWij(l/n)|
≥
p∑
j=1
Wij(xi)−
∑
i∈Ti
f̂lWij(xi)
≥
 p∑
j=1
Wij(xi)
(1−∑
l∈Ti
f̂l
)
= C4
(
1−
∑
l∈Ti
f̂l
)
Therefore,
∑
l∈Ti f̂l ≥ 1−B/C4 and∑
l:l/n∈Si
f̂l ≤
∑
l∈Ti
f̂l = ‖f̂‖1 −
∑
a 6=i
∑
l∈Ta
f̂l ≤ ‖f̂‖1 − (k − 1)(1−B/C4) ≤ 1 + (k − 1)B/C4.
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
This implies that the weight of estimate f̂ contained in the interval Si is not too much larger than the true weight of 1.
Also, ∑
l:l/n∈Si f̂l
‖f̂‖1
≤ 1 + (k − 1)B/C4
k(1−B/C4)
=
1
k
+
B/C4
1−B/C4
In order to upper bound the EMD( f0k ,
f̂
‖f̂‖1
) we need a flow, we are going to assign weight min{
∑
l:l/n∈Si
f̂l
‖f̂‖1
, 1k} to travel
to xi from within Si. The remaining unassigned weight is at most k
B/C4
1−B/C4 +
B/C3
k(1−B/C4) and this weight can travel at
most 1 unit in any flow. Therefore,
EMD
(
f0
k
,
f̂
‖f̂‖1
)
≤
k∑
i=1
∑
l:l/n∈Si
f̂l
‖f̂‖1
|xi − l/n|+ k
B/C4
1−B/C4
+
B/C3
k(1−B/C4)
≤ 1
k(1−B/C4)
√
B
C5
+ k
B/C4
1−B/C4
+
B/C3
k(1−B/C4)
(2)
Now, we need bounds on C1, C2, C3 and C4. Firstly, recall C1 = infz∈[s2−
√
2T ,s1+
√
2T ][−W ′′(z)/2] > 0. The sensors
sij and sij+p are at a distance of p/m and recall that we only chose the sensors such that |xi − sij | ≥ 1/m. Thus, any
z ∈ [s2 −
√
2T , s1 +
√
2T ] we have either |z − sij | ≤
√
T/8 or |z − sij+p | ≤
√
T/8 so
−W ′′(z) = g′(sij+p − xi)g′′(z − sij ) + g′(xi − sij )g′′(sij+p − z) ≥
1
16πT 3
(
1
m
(
1− 1
2T
T
8
)
e−
1
4T
T
8
)
Therefore, C1 ≥ 17e
−1
32
512
1
m
1
T 3 . Next,
C2 = max
i∈[k]
max
j∈[2p]
−g′(|xi − sj |) ≤
1
2
√
4πT
e
−1
4m2T .
By Lemma 13 we have, C3 = mini∈[k] minl:l/n6∈Si
∑p
j=1(Wij(xi) − Wij(l/n)) =
Ω
(
m
√
T/2(m
√
T/2+1)2
6m2 e
−1
8
3
256π
√
4πT 3.5
e
−1
4T
)
. Finally,
C4 =
p∑
j=1
Wij(xi)
=
1
8πT 2
p∑
j=1
(sij+p − xi)e
−(sij+p−xi)
2−(xi−sij )
2
4T + (xi − sij )e
−(sij+p−xi)
2−(xi−sij )
2
4T
≥ 1
8πT 2
e
−1
8
p∑
j=1
(sij+p − sij )
≥ Ω
(
me
−1
8
16πT
)
.
Lemma 12 gives C5 = Ω
(
m
√
T/8+1
T 2.5
)
. Note that by assumption (4) we have B/C4 < c so 11−B/C4 ≤
1
1−c . Putting all
our bounds into (2) we gain the final result.
C. Proof of Theorem 10
The following generalisation of the upper bound in Proposition 4 will aid in our proof of Theorem 10.
Lemma 14. Suppose ‖f0‖1 = ‖f ′0‖1 = 1 then
‖Af0 −Af ′0‖2 = O
(√
m
T 1.5
EMD(f0, f ′0)
)
Local Differential Privacy for Physical Sensor Data and Sparse Recovery
Proof of Lemma 14. Firstly, consider the single peak vectors ei and ej . Then noting that Aei = Ai, we have from Propo-
sition 4 that
‖Aei −Aej‖2 ≤
j−i−1∑
l=0
‖Aei+l −Aei+1+1‖2 ≤ O
(
|i− j|
√
m
nT 1.5
)
Now, let fij be the optimal flow from f0 to f ′0 as described in Definition 7 so f0 =
∑
i,j fijei and f
′
0 =
∑
ij fijej . Then
‖Af0 −Af ′0‖2 ≤
∑
ij
fij‖Aei −Aej‖2
≤ O
∑
ij
fij
∣∣∣∣ in − jn
∣∣∣∣ √mT 1.5

= O
(√
m
T 1.5
EMD(f0, f ′0)
)
Suppose p, q are probability distributions on the same space. Then the Kullback-Leibler (KL) divergence of p and q is
defined by D(p||q) =
∫
(log dpdq )dp. For a collection T of probability distributions, the KL diameter is defined by
dKL(T ) = sup
p,q∈T
D(p||q).
If (Ω, d) is a metric space,  > 0 and T ⊂ Ω, then we define the -packing number of T to be the largest number of disjoint
balls of radius  that can fit in T , denoted byM(, T, d). The following version of Fano’s lemma is found in (?).
Lemma 15 (Fano’s Inequality.). Let (Ω, d) be a metric space and {Pθ | θ ∈ Ω} be a collection of probability measures.
For any totally bounded T ⊂ Ω and  > 0,
inf
θ̂
sup
θ∈Ω
Pθ
(
d2(θ̂(X), θ) ≥ 
2
4
)
≥ 1− dKL(T ) + 1
logM(, T, d)
(3)
where the infimum is over all estimators.
Proof of Theorem 10. For any source vector f0, let Pf0 be the probability distribution induced on Rm by the process
Af0 + N(0, σ
2Im). Then the inverse problem becomes estimating which distribution Pf0 the perturbed measurement
vector is sampled from. Let f0 and f ′0 be two source vectors. Then
D(Pf0 ||Pf ′0) =
m∑
i=1
((Af0)i − (Af ′0)i)2
2σ2
=
1
2σ2
‖Af0 −Af ′0‖22
≤ C m
T 3σ2
(EMD(f0, f ′0))
2
for some constant C, where we use the fact that the KL-divergence is additive over independent random variables, along
with Lemma 14. Now, let a = min{ 12 ,
T 1.5σ√
2C
√
m
}. Let T be the set consisting of the following source vectors: e1/2,
(1/2)e1/2−a/2 + (1/2)e1/2+a/2, (1/4)e1/2−a + (1/2)e1/2 + (1/4)e1/2+a, (1/2)e1/2 + (1/2)e1/2+a, which are all at an
EMD a from each other. Then dKL(T ) + 1 ≤ 3/2 and logM(a, T,EMD) = 2. Thus, by Lemma 15,
inf
f̂
sup
f0
E[EMD(f0, f̂)] ≥
3
4
a = Ω
(
min
{
1
2
,
T 1.5σ√
m
})
.

