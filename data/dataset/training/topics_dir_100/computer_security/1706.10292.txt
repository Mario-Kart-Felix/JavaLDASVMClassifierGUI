Onions in the Crosshairs
When The Man really is out to get you
Aaron D. Jaggard
U.S. Naval Research Laboratory
aaron.jaggard@nrl.navy.mil
Paul Syverson
U.S. Naval Research Laboratory
paul.syverson@nrl.navy.mil
ABSTRACT
We introduce and investigate targeting adversaries who selectively
aack users of Tor or other secure-communication networks. We
argue that aacks by such adversaries are more realistic and more
signicant threats to those most relying on Tor’s protection than
are aacks in prior analyses of Tor security. Previous research and
Tor design decisions have focused on protecting against adversaries
who are equally interested in any user of the network. Our adver-
saries selectively target users—e.g., those who visit a particular
website or chat on a particular private channel—and essentially
disregard Tor users other than these. We present a model of such
adversaries and investigate three example cases where particular
users might be targeted: a cabal conducting meetings using MTor, a
published Tor multicast protocol; a cabal meeting on a private IRC
channel; and users visiting a particular .onion website. In general
for our adversaries, compromise is much faster and provides more
feedback and possibilities for adaptation than do aacks examined
in prior work. We also discuss selection of websites for targeting
of their users based on the distribution across users of site activity.
We describe adversaries both aempting to learn the size of a cabal
meeting online or of a set of suciently active visitors to a targeted
site and aempting to identify guards of each targeted user. We
compare the threat of targeting adversaries versus previously con-
sidered adversaries, and we briey sketch possible countermeasures
for resisting targeting adversaries.
KEYWORDS
adversary models, Tor, targeted aacks
1 INTRODUCTION
Tor is a network for trac security of Internet communications [7]
with millions of users [31]. Most Tor users are unlikely to be of
specic interest to an adversary; they are primarily protected by
Tor against opportunistic local eavesdroppers and local censors
or against hostile destinations. Deanonymizing adversaries are
generally modeled as aempting to aack as many users as possible
rather than targeting particular users or groups.
For many Tor users this is perhaps appropriate, but Tor is explic-
itly intended to protect human rights workers, law enforcement,
military, journalists, and others [30] who may face large, well-
nanced, and determined adversaries. More to the point, some of
these adversaries adversaries will hoover up whatever they can, but
they may also be more interested in specic individuals or groups
of Tor users, possibly based on oine or out-of-band reasons. An
,
. .
DOI:
adversary whose interest is directed primarily or more fervently
at particular users may employ dierent strategies. And if Tor’s
design decisions are motivated by analyses of what hoovering ad-
versaries can do, those most in need of Tor’s protections may be
the least well served.
We regard the adversaries in this paper as the next step in an
ongoing evolution of most appropriate and important onion routing
adversaries, away from abstracting reality till it matches models
and towards beer matching models to reality. Our focus in this
work is on targeting adversaries. ese need not dier at all from
previously studied adversaries in terms of their capabilities or re-
source endowment, though they might. ey dier primarily in
their goals and strategies. We will set out various types of targeting
adversaries presently; however, we mention an example here to
give the basic idea. A targeting (or “selective”) adversary, Sam, who
has compromised a particular user of interest, Alice, and observed
her connecting to Bob, an interesting and unusual .onion website
(essentially websites reachable only over Tor) may wish to target
other users of that site. Sam might be particularly interested to
learn which are the most active site users or how popular the site
is in general.
Background: We sketch here a basic background on Tor to provide
context for this work. For further descriptions, see the Tor design
paper [7], or related documentation at the Tor website [33]. Tor
clients randomly select sequences of three out of roughly 10,000
relays [32] forming the current Tor network, and create a crypto-
graphic circuit through these to connect to Internet services. Since
only the rst relay in the circuit sees the IP address of the client and
only the last (exit) relay sees the IP address of the destination, this
technique separates identication from routing. In response to a
vulnerability analysis [25], Tor began having clients each choose a
small set of entry guards from which to persistently choose the rst
relay in any circuit. e appropriate size and rotation criteria for the
set of guards is the focus of ongoing work, including that presented
below. Tor currently has roughly two million users connecting
in this way [31]. For simplicity of this rst analysis of targeting
adversaries, we ignore the 150,000 users currently connecting to
Tor via bridges, a mechanism to access Tor in environments that
censor users connecting to the Tor network at all. Nonetheless,
much of our analysis will apply there as well.
Tor also facilitates onion services, which are services at Inter-
net sites on the .onion top-level domain that is reserved by IETF
standard [2]. Onionsites create circuits into the Tor network to
Introduction Points at which they are then reachable. us, though
not part of the Tor network itself, onionsites are only reachable via
Tor. Connections to them, including for address lookup, do not exit
the Tor network described above.
ar
X
iv
:1
70
6.
10
29
2v
1 
 [
cs
.C
R
] 
 3
0 
Ju
n 
20
17
, , Aaron D. Jaggard and Paul Syverson
Result Highlights: MTor is a published protocol for multicast
over Tor [20]. Its security goals are to “prevent an adversary from
(1) discerning the sender of the message and (2) enumerating group
members.” We show in Sec. 3 that a targeting adversary with capabil-
ities within those assumed by MTor’s authors can enumerate group
members and can identify the guard relay of a message sender.
In Sec. 4, we describe how a targeting adversary will, within a few
weeks of aack initiation, have a high probability of locating the
leader of a cabal that meets several times per day on a private IRC
channel. In contrast, to achieve the same expectation of cabal-leader
location by an adversary with roughly the same resources using
previous strategies would require several months [18]. We also
show that targeting adversaries receive feedback on intermediate
goals such as cabal size and activity, allowing them to decide, adapt,
or refocus subsequent aacks at a similarly faster rate than previous
adversaries. ose results are discussed in Sec. 6, where we will also
briey describe possible counters to the kinds of targeted aacks
we introduce in this paper.
Tor has recently taken steps to make it dicult for adversaries
to predict the onionsites for which relays they own would function
as directory [9] or recognize which onionsite is being requested
when receiving a directory request [21]. is was in part to counter
published aacks allowing an adversary to monitor interest in
onionsites by monitoring the rate of directory requests [3]. Using
aacks similar to those that we describe against MTor and IRC
cabals, we show in Sec. 5 that a moderately resourced adversary
can assess not just the level of site activity but the distribution of
client interaction with a targeted onionsite and will identify the
guards of more active clients, potentially for additional targeted
aacks.
Aer noting some relevant prior work next, we present in Sec. 2
a brief description of the targeting adversaries used in the worked
examples just mentioned, along with the general strategy they all
follow. A more general and abstract description of various types of
targeting adversaries, their goals, and their properties is reserved
for Apps. A and B.
Related Work: We will primarily discuss related work at points
in the text where that work is relevant. We here briey mention
a few highlights of prior general work on Tor (or more generally,
onion routing) adversary models and security analysis.
Analysis of onion routing security has generally focused on end-
to-end correlation. To be practical, onion routing networks are
generally low-latency. us, an adversary able to observe both
ends of a connection can correlate paerns of communication and
correlate connection source and destination with lile error re-
gardless of what happens between the ends. Given the fraction
f of Tor relays that are compromised or observed, this provides
roughly f 2 probability of any one onion-routing circuit being com-
promised [29]. Various end-to-end correlating adversaries and this
metric of security against them are the basis for the bulk of Tor
security research and design. Hintz, however, was the rst to ob-
serve that if an adversary can recognize a destination from the
usual paern of trac that results from connecting to it, then it is
sucient to observe the client end to recognize its destination in a
ngerprinting aack [11]. Such recognition is a central feature of
aacks for all three of our examples.
Feamster and Dingledine were the rst to examine an adversary
occupying the network links between relays rather than at the
relays themselves [8]. Vulnerability to link adversaries is signicant
enough that any useful Tor security design must take them into
account. Nonetheless, we will show that a targeting relay adversary
is sucient to carry out eective aacks.
Prior to the last half decade, research has primarily looked at
the risk of correlation at a network snapshot. Johnson et al. consid-
ered security of using the Tor network over time, examining such
questions as the time until a user with a given type of behavior is
likely to experience its rst correlated connection, and given such
behavior, the fraction of connections that will be so compromised
over a period of use [18]. Since they consider IRC use as one of
their classes of behavior, we will compare the aacks we devise on
an IRC cabal to those they examined.
Not all work prior to Johnson et al. ignored observation over
time. Predecessor and intersection aacks examine repeated con-
nections or message transmissions to see who is the only one or
the most common one who could have been sending when a given
destination is receiving. Crowds was a published system design
for anonymous web browsing that was created with these aacks
in mind [28]. Wright et al. analyzed these aacks for many trac
security systems including pre-Tor onion routing [36]. Most inter-
section aacks and analyses looked for any association of a sender
and receiver. As such they were not targeted. However, the rst
such aacks conducted on a deployed, publicly-used system were
used to show that an adversary could repeatedly connect to and
thereby nd the IP address of a specic hidden onion service. ese
were a basis for introducing guard relays to Tor [25]. Nonetheless,
end-to-end correlation still remains the primary concern of most
Tor analyses and defenses.
2 TARGETING ADVERSARIES
We expect selective targeting Tor adversary description and analysis
to be amenable to rigorous formal reasoning. We also anticipate
future analyses of other targeting adversaries than in our examples,
e.g., an adversary that aempts for a given targeted user to build a
prole of that user’s selected destinations and activity at them over
a given period. To this end, we set out in Apps. A and B an abstract
model of both system elements and actions, as well as dierent
categories of targeting adversaries and their goals. Here we simply
sketch the basic adversary properties and strategy that should apply
to all of our worked examples. In the rst two of these examples, the
adversary is interested in a cabal of users communicating through
Tor with either a multicast system or IRC; in our third main example,
he is interested not in a cabal per se, but in the set of users who
frequently visit a targeted onionsite.
e general approach that all of our examples follow is to have
an adversary that initially deploys all of its relay capacity at middle
relays in the network. We assume that communication within a
targeted cabal or with a targeted onionsite is recognizable by its
trac paerns at middle relays. e basis of that assumption varies
with example and is stated in each section. e initial strategy of the
adversary is then to aempt to nd a guard for each of the targeted
clients by observing which guards transmit or receive recognizable
target trac. is may be a nal strategy if the adversary’s only
Onions in the Crosshairs , ,
goal is to learn the size of a cabal and/or monitor the distribution of
its activity. But, it may be just a stepping stone, e.g., to inform the
decision whether to aempt to “bridge” guards—i.e., to transition
beyond knowing that a guard is being used by one or more clients
of interest to identifying client IP addresses. e adversary may
be selective in this decision as well; rather than targeting all cabal
members it might, e.g., aempt to bridge only for those that send
the most or prompt the most responses when they send. Note
that “bridging” typically implies geing across a network obstacle,
rather than compromising it directly [4]. In our seing this could
be done via the guard ISP, compromised ASes between a guard and
client, etc. For convenience, we will subsume under “bridging” both
bridging in that sense and compromise of a guard by physical access
to its hardware, threatening its operator, exploiting conguration
errors, using zero-day exploits on its soware, etc.
Another adversary goal is to assign condence to its assessment
of cabal size. is can involve evaluation of condence in the
correctness of the fraction of cabal users who have had a guard
identied. e adversary will also need to evaluate the expected
degree of guard collision amongst cabal members.
3 EXAMPLE: MULTICAST CABALS
MTor is a design for multicast group communication over Tor
recently introduced by Lin et al. [20].
3.1 MTor overview
Each client that joins a given MTor multicast group creates a circuit
to a Tor relay that serves as the group’s current multicast root
(MR). We do not describe MTor’s selection or rotation of MR and
skip many other details as well. Communication for the group
travels up this circuit and then propagates down from any node
with untraversed subtrees to the group members at the leaves.
is creates the overhead savings in Tor circuit construction of a
tree (at largest a star topology) versus pairwise connections to all
group members. As we will see, for moderately sized cabals on the
current Tor network, a star topology or a tree that is almost a star
is reasonable to expect.
Since the MR is a Tor relay, MTor’s design also incurs the per-
formance and overhead advantages of not having trac exit the
network. Limited network exit capacity is oen a dominating factor
for Tor performance and is one of the motivations for Facebook’s of-
fering an onionsite rather than merely encouraging connections to
their registered domain via Tor [23]. And, MTor circuits are client–
guard–middle–MR vs. client–guard–middle–exit-server for unicast,
thus saving one hop of path overhead for each group member.
MTor also modies normal Tor behavior for the potential perfor-
mance gain from message deduplication that is typical of multicast.
Tor normally creates cryptographic circuits by tunneling a Die-
Hellman protocol to establish a session key known only to the client
and to the next onion router in the circuit being built. MTor uses
group keys so that if a client aempts to build a circuit through
a relay that is already part of the same multicast tree, the relay
will recognize the session group identier (GID) sent by the client
and join the new circuit to the existing group circuit rather than
continue building as per the client request. To further manage tree
size and improve the advantages of multicast, MTor also allows the
restriction of middle relay selection for MTor communication to
a designated subset of relays and/or to relays having a minimum
bandwidth.
3.2 MTor adversary
A targeted and compromised Alice belonging to a cabal that meets
only via MTor reveals to the adversary all cabal communications,
as well as long-term group keys and identiers. A targeted but
uncompromised Alice with a compromised guard connecting to an
MTor cabal could make the cabal a target by association. (When
the targeted group is open, the adversary can join it too.)
3.2.1 Adversary goals. Lin et al. consider adversary goals of
looking at all pairs of users and trying to link each pair as part
of a multicast group (by their guards seeing the same GID) and
of identifying a user as participating in a multicast group (by a
guard seeing the GID—experiments consider only a single multicast
group) [20]. While these may be useful for some purposes, our
targeting adversary has goals of identifying all members of a mul-
ticast group of interest, estimating the cabal’s size, or identifying
MTor groups to which a targeted user might belong. Indeed, a
targeted user communicating over MTor is a natural subject of all
the adversary goals identied in App. B.3.
3.2.2 Adversary endowment and capabilities. For simplicity, and
like Lin et al., we will consider only a relay adversary. On the
other hand, it will be useful for our adversary to compromise relays
other than guards. An adversary that owns middle relays can both
estimate the cabal size and identify guards to target for compromise
or bridging so as to identify the clients behind them. Even the MR
can estimate cabal size.
e guard of an MTor group member can see all session GIDs
for the group, and may then wish to identify, e.g., others in that
group. We will assume, however, that trac paerns for any cabal
member in a multicast session will be adequately linkable so that
the GID will not be needed for this adversary to associate other
clients with the cabal. (is assumption is also made by MTor’s
authors.) In general, we consider an adversary capable of active
aacks, including disrupting group communications or generating
its own group trac if a member. For simplicity, however, our
initial analysis assumes a passive adversary. Since MTor sessions
are always identiable by a participating adversary relay and our
analysis will parametrize over the number of sessions, this is not
as signicant a limitation on the adversary as is usually the case
for Tor communications.
3.3 MTor cabal analysis
Note that while the GID is not that signicant to a targeting adver-
sary who can observe trac paerns, the multicast tree structure
is. To illustrate with an unrealistic example, if the middle-relay set
were restricted to a singleton, then in sessions where the adversary
has compromised this relay he has thereby identied all the guards
used by any cabal members in that session. Lin et al. do not give
criteria for middle-relay-set restriction. If gameable an adversary
might be able to improve its expected inclusion in this set dispro-
portionate to its actual relay and bandwidth resources. On the other
, , Aaron D. Jaggard and Paul Syverson
hand, a restriction of middle-relay-set size can obscure cabal size
estimates by a an adversarial MR.
3.3.1 Learning a guard of every cabal member. For our initial
analysis, we consider an adversary who controls a fraction B of the
middle-relay bandwidth and seeks to identify at least one guard of
each member of a cabal. (is might be used to adaptively target
guards for future aack or as part of an estimation of cabal size.
If the adversary has a joined or compromised cabal member, he
can associate a guard with every cabal member who ever sends.
) We also consider the eects of the number c of cabal members
and the number m of cabal multicast sessions observed. If the
instance of MTor restricts the set of usable middle relays to obtain
the associated deduplication benet, we take B to be the fraction of
MTor-available middle-relay bandwidth that is controlled by the
adversary. Here, we also consider a probability T that an adversary
might allow for his failure.
We assume for simplicity a static network that does not change
for the period of our analysis. In each multicast session, a new
random MR is chosen and the circuits constituting the multicast
tree are also reformed. We also assume that all members of the
cabal participate in all multicast sessions (meetings) and that cabal
composition does not change.
If a cabal member constructs a circuit that uses a middle relay
controlled by the adversary, then the adversary learns the client’s
guard for that circuit. We take this as the only way that the adver-
sary learns guards.
e probability that a given cabal member never uses a com-
promised middle relay in any of m sessions is (1 − B)m . Given the
simplifying assumption that such compromise is independent for
all clients, the probability that a guard of every one of the c cabal
members is identied at least once over the course ofm meetings
is thus (1 − (1 − B)m )c . We can then gauge adversary success by
bounding the probability that the adversary fails to carry out this
compromise, giving us
1 −
(
1 − (1 − B)m
)c
< T . (1)
We now explore the parts of the (c,m,B,T ) space that satisfy this
inequality. If the number of meetings
m > log1−B
[
1 − (1 −T )1/c
]
, (2)
where B ∈ (0, 1), then, with probability at least 1−T , the adversary
learns at least one guard of each of the c cabal members. e le
column of Fig. 1 plots the right-hand side of (2) as a function of B for
c = 5, 20, and 50 (top to boom subplots) andT = 0.1, 0.5, and 0.9 as
indicated on the curves within each subplot. us, for cabal sizes up
to 50 an adversary holding 10% of middle-relay bandwidth has a 90%
chance of having identied a guard for each cabal member aer no
more than 60 meetings. On the other hand, aer 10 meetings, even
for c = 5 the adversary must hold a more ambitious middle-relay
bandwidth of nearly 40%. Note that the more relays the adversary
introduces the more gradually he must introduce them and the
more the relays should be in multiple locations if he is to avoid
suspicion.
Again using (1), if the number of cabal members
c < log1−(1−B)m [1 −T ] , (3)
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of bandwidth with c = 5
0
20
40
60
80
100
M
in
. 
m
e
e
ti
n
g
s
T = 0.1
T = 0.5
T = 0.9
0.0 0.2 0.4 0.6 0.8 1.0
Failure probability with B = 0.05
0
1
2
3
4
5
M
a
x
. 
ca
b
a
l 
si
ze
m = 1
m = 4
m = 8
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of bandwidth with c = 20
0
20
40
60
80
100
M
in
. 
m
e
e
ti
n
g
s
0.0 0.2 0.4 0.6 0.8 1.0
Failure probability with B = 0.1
0
1
2
3
4
5
M
a
x
. 
ca
b
a
l 
si
ze
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of bandwidth with c = 50
0
20
40
60
80
100
M
in
. 
m
e
e
ti
n
g
s
0.0 0.2 0.4 0.6 0.8 1.0
Failure probability with B = 0.2
0
1
2
3
4
5
M
a
x
. 
ca
b
a
l 
si
ze
Figure 1: Le column: eminimumnumber ofmeetingsm,
as a function of middle-relay bandwidth B controlled by the
adversary, required for the adversary to learn at least one
guard of each cabal member for cabal sizes c = 5, 20, and
50 (top to bottom). Dierent curves in each subplot corre-
spond to dierent failure probabilities for the adversary as
indicated. Right column: e maximum cabal size c, as a
function of the adversary’s failure thresholdT , required for
the adversary to learn at least one guard of each cabal mem-
ber when the adversary controls B = 0.05, 0.1, and 0.2 of the
middle-relay bandwidth (top to bottom). Dierent curves in
each subplot correspond to dierent numbers of cabal meet-
ings as indicated.
where B ∈ (0, 1) andm is a positive integer, then the adversary will
learn at least one guard of each of the c cabal members. e right
column of Fig. 1 plots the right-hand-side of (3) as a function of T
for B = 0.05, 0.1, and 0.2 (top to boom subplots) andm = 1, 4, and
8 as indicated on the curves within each subplot.
From the cabal’s perspective, it should keep its set of members
minimal with respect to the qualities needed to accomplish its goals.
However, the cabal might reasonably ask how many meetings it
should hold and what the eects of additional meetings are on its
security. For a given cabal size c and a subset of i cabal members,
the probability that a guard of exactly those i members is exposed
aerm meetings is (1 − (1 − B)m )i (1 − B)m(c−i). Considering the
dierent ways to choose the subset of cabal members, the expected
number of cabal members with an identied guard aerm meetings
is
∑c
i=0 i
(c
i
)
(1 − (1 − B)m )i (1 − B)m(c−i). Applying known results
about the mean of the binomial distribution, the expected number
of cabal members with no guard identied is c(1−B)m . e fraction
of the cabal that has identied guards is thus independent of c and
equal to 1 − (1 − B)m . Figure 2 shows the expected fraction of
cabal members with identied guards aerm meetings for dierent
fractions B of middle-relay bandwidth controlled by the adversary.
Perhaps unsurprisingly, failure probability appears to be highly
Onions in the Crosshairs , ,
5 10 15 20 25
Number of meetings m
0.0
0.2
0.4
0.6
0.8
1.0
E
x
p
e
ct
e
d
 f
ra
ct
io
n
 o
f 
ca
b
a
l 
co
m
p
ro
m
is
e
d
B = 0.2
B = 0.15
B = 0.1
B = 0.05
Figure 2: e expected fraction of the cabal members for
which the adversary identies at least one guard as a func-
tion of the number of meetings m. Dierent curves show
dierent fractions B of middle-relay bandwidth controlled
by the adversary.
responsive to either the number of meetings observed or the fraction
doing the observing.
3.3.2 Estimating cabal size as multicast root. An adversary who
controls the MR can estimate cabal size from the number of distinct
GID circuits connecting to the MR. e MR is selected uniformly at
random from all relays that have the fast and stable ags and meet
the minimum bandwidth parameter of the multicast group. We
assume that all adversary relays meet these requirements. Given a
xed fraction of middle-relay adversary bandwidth, the adversary
maximizes his chances of being selected for MR if he distributes that
bandwidth across as many relays as possible, subject to inclusion
requirements. He can accomplish this with a uniform distribution
amongst this maximal set of relays, which will also simplify any
combination of calculations based on adversary’s fraction of relays
with those based on adversary’s fraction of bandwidth.
As we have noted, increased deduplication decreases the infor-
mation an MR gets about cabal size from the number of observed
circuits. Collisions can occur in either the selection of guard or of
middle relay; as there are far fewer guards than middles in the cur-
rent Tor network, guard selection is by far the dominating source
of collision. For the following calculations we assume that guard
bandwidth is also uniformly distributed.
With д guards in the network, for a cabal of size c , the expected
number of guards selected is д(1 − (1 − 1/д)c ). As of this writing,
the Tor network has about 2500 guards, and Tor selects middles from
about 5000 relays. If we assume that the expected number of guards
are selected, and middles are selected uniformly, then the expected
number of middles is given by the the same formula as above where
c is replaced by h, the expected number of guards selected, and
guards are replaced by middles, viz: m(1 − (1 − 1/m)h ). us the
expected number of middles selected, hence observed by the MR,
is the same as cabal size for cabals up to c = 41, o by less than
one for cabals up to c = 58 and within ve percent for cabals up to
c = 182.
A signicantly skewed guard distribution would impact the like-
lihood of collisions, again diminishing information about cabal size
available to the MR, but it may also help the adversary in narrowing
the set of most useful guards to target for bridging. Even for a selec-
tion among 800 guards and 2000 relays, the above calculation gives
an expected number of middles the same as cabal size up to c = 24
and within ve percent up to c = 60. Whether the distribution
is skewed or not, an adversary owning a cabal member and the
MR can recognize all colliding circuits for cabal members who ever
send during a meeting, which can be used to improve the cabal size
estimate.
If all relays selectable as middles by Tor meet the group criteria
for serving as MR, then the probability of an adversary being in a
position to make the above observation is governed by the fraction
of middle relays he owns. us, a B = 0.2 adversary will be more
likely than not to have been chosen as MR aer only four multicast
sessions and will have a nearly 90% chance of being chosen aer
10 sessions.
3.3.3 Estimating cabal size from middle relays. Estimates based
on relays serving as middles (second hops) in MTor sessions are
also possible. Compared to results from compromising a multicast
root, these will be much less likely to have information about the
circuits of all cabal members but much more likely to provide some
information about cabal size every session. In addition, middle
relays will identify guards of cabal members with every observed
cabal connection.
Setting and assumptions: Here, we look at some very basic nu-
merical simulations of the information learned by the adversary
from middle relays in the MTor usage scenario. ese make as-
sumptions that parallel those made in our analysis above, but they
allow us to study the eects of MTor deduplication more easily.
In particular, we assume a static network and that cabal members
choose guards and middle relays uniformly at random from sets of
2500 and 5000 such nodes, respectively.
Approach: In each of 10,000 trials, we identify some middle relays
as compromised; each is compromised independently with proba-
bility B, and the compromise status remains unchanged throughout
m cabal meetings. We then choose a set of guards (the size of which
depends on the simulation) for each cabal member; these sets are
unchanged throughout them meetings.
For each meeting, each guard selects a client from its set. For each
guard selected, we then select a middle relay to use to connect to the
MR. (is simulates the choice of a middle relay made by the rst
client using that guard in that meeting. Any other clients who use
that guard for that meeting will use the same middle relay.) If that
middle relay is compromised and the adversary has not aempted
to bridge the guard before (i.e., that guard has not been used in
conjunction with a compromised middle relay in this trial), then
we bridge the guard with probability pb . Across all meetings in the
trial, we keep lists of the guards that have been bridged and that the
adversary has tried but failed to bridge. Aer determining which
guards are newly compromised in each meeting, we determine the
clients that have been identied; this set is the union of the clients
that were compromised before the current meeting and all of those
that, in the current meeting, used a guard that has been successfully
bridged during any meeting up to this point (including the current
meeting).
, , Aaron D. Jaggard and Paul Syverson
0 10 20 30 40 50
Number of meetings
0
5
10
15
20
25
M
e
a
n
 n
u
m
b
e
r 
o
f 
id
e
n
ti
fi
e
d
 c
a
b
a
l 
m
e
m
b
e
rs B = 0.5
B = 0.2
B = 0.15
B = 0.1
B = 0.05
0 10 20 30 40 50
Number of meetings
0
5
10
15
20
25
M
e
a
n
 n
u
m
b
e
r 
o
f 
id
e
n
ti
fi
e
d
 c
a
b
a
l 
m
e
m
b
e
rs
Figure 3: Mean (over 10,000 trials) number of cabal mem-
bers (out of 25) identied as a function of the number of ca-
bal meetings for the one (le) and three (right) guards per
client when the adversary controls B = 0.05, 0.1, 0.15, 0.2, and
0.5 of the middle-relay bandwidth (dierent curves) and the
probability of bridging a guard is pb = 0.5.
As noted in Sec. 2, a guard might be bridged in many ways,
and we expect an adversary to try them in parallel and/or succes-
sively. He will likely start with the least costly, least likely to raise
suspicion, and most likely to succeed, perhaps with varied empha-
sis depending on seing. We use pb to capture the cumulative
probability of success of these dierent approaches.
Results: Figure 3 shows the mean number of identied cabal mem-
bers, out of 25 and averaged over 10,000 random trials, for dierent
values of B as a function of the number of meetings when each
client has one (le) and three (right) guards. is assumes a bridg-
ing probability pb = 0.5. Figure 4 is similar and shows the eects
of the value of the bridging probability on these computations in
the three-guard-per-client case. e dierent curves in this plot
correspond to dierent values of pb ; this assumes B = 0.2.
Both B and pb can have a signicant impact on the adversary’s
success. Figure 3 illustrates the benet to the adversary of having
additional guards that he may aempt to bridge (for a xed pb ).
As we also discuss below, the adversary might be able to increase
pb against long-lived guards by continuing to devote resources to
bridging them if initial aempts fail. Figure 4 highlights the benets
of increasing pb by this or other means.
4 EXAMPLE: TARGETING IRC CABALS
We now consider the following scenario: A cabal of interest to the
adversary communicates via a private IRC channel. All of the cabal
members access the IRC server via Tor, and each creates a new
Tor circuit for each cabal meeting. In doing so, we assume a client
chooses a guard uniformly at random from her set of guards and
then chooses a middle relay uniformly at random from all middle
relays. e adversary compromises the middle relays independently
with probability B, corresponding to the fraction of middle-relay
bandwidth that he controls. We assume that the network is static.
4.1 Identifying guards and cabal members
With respect to learning guards of cabal members, the analysis of
Sec. 3.3.1 applies in this seing as well. e assumption made there
that probabilities for clients never using a compromised middle
relay are independent is even more realistic here.
0 10 20 30 40 50
Number of meetings
0
5
10
15
20
25
M
e
a
n
 n
u
m
. 
id
e
n
ti
fi
e
d
 c
a
b
a
l 
m
e
m
b
e
rs pb = 0.95
pb = 0.9
pb = 0.75
pb = 0.5
pb = 0.25
pb = 0.1
pb = 0.05
Figure 4: Mean (over 10,000 trials) number of cabalmembers
(out of 25) identied as a function of the number of cabal
meetings in the three-guard case when the adversary con-
trols B = 0.2 of the middle-relay bandwidth and the proba-
bility of bridging a guard is pb = 0.05, 0.1, 0.25, 0.5, 0.75, 0.9,
and 0.95.
We also consider the adversary’s success in identifying and lo-
cating particular cabal members. If the adversary already has a
cabal membership, by the properties of IRC he has a list of channel
pseudonyms for all cabal members, even those aending meetings
silently, and has a complete pseudonmymous record of all com-
munications. We again assume the adversary owns some fraction
B of the middle-relay bandwidth. If the cabal leader Alice uses
a middle relay controlled by the adversary Sam, he observes the
trac paern through that relay and the matching messages that
appears in the channel and thus learns the guard used by Alice for
that circuit. Once the adversary knows this guard, he is able to
bridge the guard and identify Alice’s IP address with probability
pb . Other than through this combination of events, we assume the
adversary is not able to identify Alice’s IP address. Even if Sam
only owns the ISP of some targeted, uncompromised cabal member,
he still passively observes everything there, including the trac
paern for a cabal’s IRC channel; as noted above, we assume that
this or other information allows the adversary to identify cabal
trac at middle relays.
We make the simplifying assumption that bridging is actually
with respect to client–guard pairs rather than individual guards.
us, if clients c1 and c2 use the same guard д for circuits that go
through compromised middle nodes (which may or may not be the
same for the two clients), then the adversary bridges д and learns
c1 with probability pb and, independently, bridges д and learns c2
with probability pb . Bridging that arises from compromising the
guard itself would not be independent for these two clients, while
bridging that arises from compromising client ISPs or some part of
the network between the clients and д might be. We thus think this
assumption is reasonable, although others could be made as well.
Figure 5 plots the probabilities, for various cases, that the ad-
versary is able to identify the cabal leader. e computations are
discussed in App. C. e top row assumes one guard per client; the
boom row assumes three guards per client. e le column (pair
of subplots) shows plots this probability as a function of B for m
meetings (dierent curves form = 1, 5, 10, and 20). e right col-
umn (pair of subplots) shows this as a function ofm with dierent
Onions in the Crosshairs , ,
curves for B = 0.05, 0.1, 0.15, and 0.2. Each pair of subplots shows
bridging probability pb of 0.5 (le) and 0.95 (right).
0.0 0.2 0.4 0.6 0.8 1.0
Middle-relay BW B
0.0
0.2
0.4
0.6
0.8
1.0
P
ro
b
a
b
ili
ty
 o
f 
lo
ca
ti
n
g
 c
lie
n
t
m = 20
m = 10
m = 5
m = 1
0.0 0.2 0.4 0.6 0.8 1.0
Middle-relay BW B
0.0
0.2
0.4
0.6
0.8
1.0
10 20 30 40 50
Num. meetings m
0.0
0.2
0.4
0.6
0.8
1.0
B = 0.2
B = 0.15
B = 0.1
B = 0.05
10 20 30 40 50
Num. meetings m
0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.2 0.4 0.6 0.8 1.0
Middle-relay BW B
0.0
0.2
0.4
0.6
0.8
1.0
P
ro
b
a
b
ili
ty
 o
f 
lo
ca
ti
n
g
 c
lie
n
t
0.0 0.2 0.4 0.6 0.8 1.0
Middle-relay BW B
0.0
0.2
0.4
0.6
0.8
1.0
10 20 30 40 50
Num. meetings m
0.0
0.2
0.4
0.6
0.8
1.0
10 20 30 40 50
Num. meetings m
0.0
0.2
0.4
0.6
0.8
1.0
Figure 5: Probability that the attacker is able to identify the
cabal leader for one (top row) and three (bottom row) guards
per cleint when the attacker controls a fraction B of the
middle-relay capacity over the course of m meetings. is
is shown as a function of B in the le column (with dier-
ent curves for dierent values ofm) and as a functionm in
the right column (with dierent curves for dierent values
of B). e bridging probability pb is, in each quadrant, 0.5
(le subplot) and 0.95 (right subplot).
Considering Fig. 5, the aacker is likely to be able to identify the
cabal leader if he makes a substantial but not unrealistic investment
in middle-relay bandwidth (B = 0.2), even if the cabal has a modest
number of meetings (10 or more). e le subplot in the top-le
pair shows the adversary’s chances bounded by pb = 0.5 because
there is only one guard to bridge. However, we expect that, if there
is a single long-lived guard, the adversary’s chance of bridging the
guard would go up over time, and the success probabilities would
become closer to those shown in the right subplot in this pair.
Separately, we note that (ignoring the fact that the aacker
cannot expect to kill every circuit) killing one circuit per client
per meeting would compress them axis in Fig. 2 (which, as noted,
applies to the IRC case as well) by a factor of 2. If the aacker were
willing and able to kill k total circuits per client, it could shi these
curves to the le by k . Considering Fig. 2, we see that the greatest
benet to the aacker comes in the rst circuit that he kills.
4.2 Estimating cabal size
We turn now to estimating the total size of the cabal (when the
adversary does not already own a member). In particular, if an IRC
cabal of size c hasm meetings, what estimate of c will be made by
an adversary who controls a fraction B of middle-relay bandwidth?
How is this estimate likely to be distributed, and how much of an
error is he likely to make?
To answer these questions, we numerically compute the maximum-
likelihood estimator (MLE) of the cabal size based on an m-tuple
®x ∈ {0, 1, . . . , c}m of observations that the adversary might make
of the number of cabal members using compromised middle relays
during each ofm meetings. We then compute the distribution on
possible observations to determine, for each estimated value, the
probability that the adversary will make observations that give rise
to that estimate.
In doing this, we assume that the adversary considers only the
number of circuits that he sees during each meeting window and
not the guards used for these circuits. In the case that each client
uses only one guard, if the adversary sees one circuit in each of three
meetings and these circuits all use dierent guards, then he knows
that he has observed three dierent clients and not just one client
multiple times. As a result, he should probably increase his estimate
of the cabal size compared to his estimate when simply using a
count of the circuits he observes. e approach we take already
requires signicant computational resources, and accounting for
guard identities makes it even more complex to the point that we
expect it would be infeasible (while also not adding signicantly to
the adversary’s accuracy). We do note that this issue may have an
eect. (We compute the probabilities of guard collisions and show
these in Tab. 1 in App. C.2.)
Figure 6 shows the error in the expected value of the MLE—i.e.,
the expected MLE minus the actual cabal size c—as a function of c
for dierent values of B (dierent subplots) andm (dierent curves).
5 10 15 20
Actual cabal size with B = 0.05
2.0
1.5
1.0
0.5
0.0
M
LE
 E
rr
o
r
m = 1
m = 2
m = 5
5 10 15 20
Actual cabal size with B = 0.2
2.0
1.5
1.0
0.5
0.0
M
LE
 E
rr
o
r
5 10 15 20
Actual cabal size with B = 0.1
2.0
1.5
1.0
0.5
0.0
M
LE
 E
rr
o
r
5 10 15 20
Actual cabal size with B = 0.5
2.0
1.5
1.0
0.5
0.0
M
LE
 E
rr
o
r
5 10 15 20
Actual cabal size with B = 0.15
2.0
1.5
1.0
0.5
0.0
M
LE
 E
rr
o
r
5 10 15 20
Actual cabal size with B = 0.9
2.0
1.5
1.0
0.5
0.0
M
LE
 E
rr
o
r
Figure 6: Expected value of the MLE for the number c of
cabal members minus c as a function of c for number of
meetingsm = 1, 2, and 5 (dierent curves). e le column
has the adversary-controlled fraction of middle-relay band-
width B = 0.05, 0.1, and 0.15 (top to bottom); the right column
has B = 0.2, 0.5, and 0.9 (top to bottom).
, , Aaron D. Jaggard and Paul Syverson
1 5 10 15 20
Actual cabal size
50
40
30
20
10
0
M
LE
 V
a
lu
e
1 5 10 15 20
Actual cabal size
50
40
30
20
10
0
M
LE
 V
a
lu
e
1 5 10 15 20
Actual cabal size
50
40
30
20
10
0
M
LE
 V
a
lu
e
1 5 10 15 20
Actual cabal size
50
40
30
20
10
0
M
LE
 V
a
lu
e
1 5 10 15 20
Actual cabal size
50
40
30
20
10
0
M
LE
 V
a
lu
e
1 5 10 15 20
Actual cabal size
50
40
30
20
10
0
M
LE
 V
a
lu
e
1 5 10 15 20
Actual cabal size
50
40
30
20
10
0
M
LE
 V
a
lu
e
1 5 10 15 20
Actual cabal size
50
40
30
20
10
0
M
LE
 V
a
lu
e
1 5 10 15 20
Actual cabal size
50
40
30
20
10
0
M
LE
 V
a
lu
e
1 5 10 15 20
Actual cabal size
50
40
30
20
10
0
M
LE
 V
a
lu
e
1 5 10 15 20
Actual cabal size
50
40
30
20
10
0
M
LE
 V
a
lu
e
1 5 10 15 20
Actual cabal size
50
40
30
20
10
0
M
LE
 V
a
lu
e
Figure 7: e eect of increasing the number of meetingsm
and changing the adversary-controlled middle-relay band-
width B. From top to bottom, m = 1, 2, and 5. From le to
right, B = 0.05, 0.1, 0.2, and 0.5. Within each subplot, the ac-
tual cabal size c increases from 1 to 20 along the horizontal
axis, and possible MLE values increase from 0 to 50 up the
vertical axis. For each c and MLE value, the plot indicates
the probability (darker values are larger) of the adversary
observing a tuple for which it would compute the indicated
value as its MLE for c.
Figure 7 presents a matrix of plots of distributions of the MLE
value. Each subplot shows, for each value c on the horizontal axis,
the distribution of MLE values, with larger probabilities correspond-
ing to darker shading. In the matrix of plots, B increases from le
to right (0.05, 0.1, 0.15, 0.2, 0.5, 0.75, and 0.9), andm increases from
top to boom (1, 2, and 5).
From these distributions we may compute, for each parameter
vector, the probability that the adversary will compute an MLE of
cabal size that has error at least a certain size. Figure 8 presents a
matrix of subplots that show these probabilities. In each subplot
we show, as a function of the true cabal size c , the probability that
the adversary’s MLE computation diers from c by at least 1, 5, and
10 (dierent curves within the subplot). is is repeated across the
matrix for dierent values ofm (1, 2, and 5, le to right) and B (0.05,
0.1, 0.15, 0.2, 0.5, 0.75, and 0.9, top to boom).
From Figs. 6–8, we see that MLE is not an unbiased estimator of
cabal size but that, in expectation, it gives a fairly close estimate.
While the distributions may be spread out and discontinuous, by
the time the cabal hasm = 5 meetings and the adversary controls
B = 0.2 of the middle-relay bandwidth, the MLE distribution is
starting to converge and the probability of a substantial error is
fairly small.
5 EXAMPLE: PICKING RIPE ONIONS
e set of users of a particular site may be similar to a cabal com-
municating via multicast or IRC. While they may not be holding
simultaneous meetings or even see themselves as a group, an ad-
versary may target them because they are users of that site. A
site may be interesting for exogenous reasons, e.g., because of a
public mention of it or its presence in the browser bookmarks of a
previously compromised target. e adversary may want to count
the users visiting a site that he has targeted, but site popularity
may also be a criterion for targeting a site. For example, he might
target onionsites that show a sudden spike in popularity.
Our analysis here essentially applies to Tor users visiting many
ordinary Internet sites, but we focus on onionsites, particularly
hidden web services. ese were designed to hide many features
typically visible for ordinary websites. ey have also had recent
design changes specically intended to make it harder for an adver-
sary to discover a site’s .onion address, popularity [21] or network
location [19].
Beyond this inherent interest, such sites are plausible candi-
dates for targeting of their users. Onionsites oer protections for
their users that are not yet common on the Internet, much like
HTTPS-protected websites when those were relatively rare. Like
HTTPS-protected sites of that era, there has been a prominent per-
ception that most current (thus still early-adoption) onionsites are
intentionally set up to thwart targeted aack. Even if that percep-
tion of onionspace is initially inaccurate, the perception can itself
drive aention of aackers, thus becoming somewhat self-fullling.
As noted, learning about site popularity may be an adversary
goal for a targeted site or may be a criterion for deciding to target
a site. Previous work [3, 26] has measured popularity of onionsites
by requests to the onion address directory. Besides the directory
system design changes that make this approach much less feasible,
it can also be a misleading and inaccurate measure of onionsite
popularity in several ways [22]. We can, however, use variants of
the techniques described in previous sections to measure onionsite
popularity (as well as the popularity of other Tor-accessed Internet
sites).
Directory requests and even site connections can be unreliable
indicators of human interest in onionsites because of crawlers and
automation such as link prefetching. us, just knowing the number
of connections or distinct clients connecting to a site is not very
useful to a targeting adversary. Information about the distribution
Onions in the Crosshairs , ,
5 10 15 20
Actual cabal size with B = 0.05 and m = 1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P
ro
b
a
b
ili
ty
 o
f 
sp
e
ci
fi
e
d
 a
b
so
lu
te
 e
rr
o
r
error >= 1
error >= 2
error >= 5
error >= 10
5 10 15 20
Actual cabal size with B = 0.1 and m = 1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P
ro
b
a
b
ili
ty
 o
f 
sp
e
ci
fi
e
d
 a
b
so
lu
te
 e
rr
o
r
error >= 1
error >= 2
error >= 5
error >= 10
5 10 15 20
Actual cabal size with B = 0.2 and m = 1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P
ro
b
a
b
ili
ty
 o
f 
sp
e
ci
fi
e
d
 a
b
so
lu
te
 e
rr
o
r
error >= 1
error >= 2
error >= 5
error >= 10
5 10 15 20
Actual cabal size with B = 0.5 and m = 1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P
ro
b
a
b
ili
ty
 o
f 
sp
e
ci
fi
e
d
 a
b
so
lu
te
 e
rr
o
r
error >= 1
error >= 2
error >= 5
error >= 10
5 10 15 20
Actual cabal size with B = 0.05 and m = 2
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P
ro
b
a
b
ili
ty
 o
f 
sp
e
ci
fi
e
d
 a
b
so
lu
te
 e
rr
o
r
error >= 1
error >= 2
error >= 5
error >= 10
5 10 15 20
Actual cabal size with B = 0.1 and m = 2
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P
ro
b
a
b
ili
ty
 o
f 
sp
e
ci
fi
e
d
 a
b
so
lu
te
 e
rr
o
r
error >= 1
error >= 2
error >= 5
error >= 10
5 10 15 20
Actual cabal size with B = 0.2 and m = 2
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P
ro
b
a
b
ili
ty
 o
f 
sp
e
ci
fi
e
d
 a
b
so
lu
te
 e
rr
o
r
error >= 1
error >= 2
error >= 5
error >= 10
5 10 15 20
Actual cabal size with B = 0.5 and m = 2
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P
ro
b
a
b
ili
ty
 o
f 
sp
e
ci
fi
e
d
 a
b
so
lu
te
 e
rr
o
r
error >= 1
error >= 2
error >= 5
error >= 10
5 10 15 20
Actual cabal size with B = 0.05 and m = 5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P
ro
b
a
b
ili
ty
 o
f 
sp
e
ci
fi
e
d
 a
b
so
lu
te
 e
rr
o
r
error >= 1
error >= 2
error >= 5
error >= 10
5 10 15 20
Actual cabal size with B = 0.1 and m = 5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P
ro
b
a
b
ili
ty
 o
f 
sp
e
ci
fi
e
d
 a
b
so
lu
te
 e
rr
o
r
error >= 1
error >= 2
error >= 5
error >= 10
5 10 15 20
Actual cabal size with B = 0.2 and m = 5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P
ro
b
a
b
ili
ty
 o
f 
sp
e
ci
fi
e
d
 a
b
so
lu
te
 e
rr
o
r
error >= 1
error >= 2
error >= 5
error >= 10
5 10 15 20
Actual cabal size with B = 0.5 and m = 5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P
ro
b
a
b
ili
ty
 o
f 
sp
e
ci
fi
e
d
 a
b
so
lu
te
 e
rr
o
r
error >= 1
error >= 2
error >= 5
error >= 10
Figure 8: Probability MLE error is at least a specied value (1, 2, 5, and 10; dierent curves in each subplot) as a function of
true cabal size c. Number of meetingsm is 1, 2, or 5 (top to bottom); the fraction B of middle-relay bandwidth controlled by the
adversary is 0.05, 0.1, 0.2, and 0.5 (le to right).
of connections among users would be much more useful both for
understanding a site’s popularity and for selecting its users for
targeting.
Unlike the cabal-meeting case, because visits are not synchro-
nized the targeting-adversary technique of starting with middle
relays might seem problematic for individuating onionsite visitors.
But, as with our analyses above, distinct guards are a fairly accurate
indicator of distinct clients up to a moderate-size set of clients, all
the more when each client uses a single guard per destination (or
overall).
Even if site users do share guards, as long as guard overlap is
infrequent enough, this will still give a targeting adversary a much
beer idea about interest in the site than could be obtained via
previously published techniques. “Infrequent enough” implies that
the rough picture of popularity painted by targeted-site connections
per guard per unit time presents a ballpark estimate of site-user
activity distribution.
To use our middle-relay techniques, we must assume that desti-
nations of potential interest can be recognized by an adversary in
the middle of circuits. We do so based on trac paerns plus possi-
bly other factors like latency, which has been known since at least
2007 [12] to leak some identifying information for Tor communica-
tion. Destination ngerprinting of route-protected communications
predates Tor [11]. How eective it is has been the subject of much
research [34].
ough the level of success for ngerprinting Tor destinations in
general may remain uncertain, onionsites constitute a much smaller
set. And current onionsite protocols are suciently dierent from
applications connected over vanilla Tor circuit protocols that sepa-
rating these is currently very easy and reliable for relays carrying
their streams. ere are about 60,000 unique onion addresses in the
directory system, but of these typically only a few thousand at any
time are reachable and return a connection to a web server. If these
numbers are roughly representative (even if we add in onionsites
not listed in the directory system and those that can only be reached
if proper authentication is shown to the Introduction Point), then
ngerprintability of all persistent onionsites becomes a reasonable
assumption. Note that our techniques are not aected by whether
or not a website is listed in the onionsite directory system or re-
quires authentication to be reachable. e most direct technique
for a middle relay targeting adversary is then to simply count the
number of connections going to a particular onionsite from each
guard. is will already give a rough picture of the distribution of
client activity as well as which guards are most worth targeting for
further adversary interest.
5.1 Recounting Onions From Our Past
Given our assumptions about numbers and ngerprintability of
.onion websites, and rough numbers and distribution of their users
and implications for individuating clients by guards, there are addi-
tional estimation techniques at our disposal.
We can estimate the number of clients visiting a site n or more
times using capture-recapture techniques. ese were originally
used for species population estimates in biology where it would
be impossible to observe the entire population, e.g., estimating the
, , Aaron D. Jaggard and Paul Syverson
number of sh in a given lake. ey are now used in many seings,
including computer networks [1].
e Lincoln–Petersen (LP) estimate is the simplest and quickest
for our adversary to perform, provided we add a few assumptions
to those made above. To calculate the number of clients making n
or more connections to a target site per unit time, we must assume
(1) that all clients (targeted or otherwise) visit a target site with
the same frequency during dierent sampling intervals of the same
length, and (2) that connections (visits) and observation intervals
are such that no connection is counted in more than one interval.
To avoid problems with division by 0, we use the Chapman version
of the LP estimator, given by N̂ = (c1+1)(c2+1)m2+1 − 1, where c1 is the
number of guards in interval t1 to be ‘captured’ and ‘marked’ as
carrying circuits for n or more visits in that interval, c2 is the total
number of guards observed to ‘visit the target’ n or more times in
t2, and m2 is the number of guards ‘marked’ in t1 and observed
visiting the target at least n times in t2.
Note that, if multiple clients used the same guard simultaneously
to connect to an onionsite through one or more compromised mid-
dles, the adversary might reasonably conclude that the guard is in
fact serving multiple visitors to the onionsite. is does not follow
with certainty, and we do not model such reasoning here.
In Fig. 9 we show the results of numerical experiments with
the Chapman estimator. In these experiments, we assume 2,500
guards and 5,000 middle relays; each of the laer is compromised
with probability B. We run each experiment 10,000 times. For
each client, we pick a guard set. We assume there are two types of
clients: “regular” clients who visit the targeted site twice during
each examination window and “interesting” clients who visit the
targeted site 10 times during each examination window. Each
experimental run species the number of each type of client; for
each client, we have it repeatedly pick (twice or 10 times, depending
on its type) a guard from its guard set and a middle relay. We track
the number of times that each guard is used with any compromised
relay; this could be multiple compromised relays, and the same
guard could be used by multiple clients. We specify a threshold
value; guards that are seen by compromised middle relays at least
this many times are considered marked and are remembered by
the adversary. is process is repeated again to model the second
examination window.
e subplots of Fig. 9 show violin plots that focus on varying
dierent parameters; except for the parameter being varied in a
particular subplot, these use B = 0.25, a threshold of 3, a client mix
of 25 targeted clients (who each visit the destination 10 times per
period) and 225 “regular” clients (who each visit the destination 2
times per period), and 1 guard per client. e subplots examine the
eects of varying B (top le; results for B = 0.05, 0.1, 0.15, 0.2, 0.25,
0.3, 0.35, 0.4, 0.45, 0.5, and 0.55), the threshold (top right; results
for thresholds of 1, 2, 3, 5, 7, and 10), the client mix (boom le;
results for regular/targeted clients combinations of 25/25, 225/25,
475/25, 2475/25, and 50/50), and the number of guards per client
(boom right; results for 1, 2, 3, 5, and 10 guards per client).
Considering Fig. 9, we see that the adversary’s estimate increases
in accuracy with B, as expected, but much of the gain comes in
ensuring that the adversary controls one h to one third of the
middle-relay bandwidth. We also see that the best threshold seems
0 1 2 3 4 5 6
10*B
0
20
40
60
80
100
120
0 2 4 6 8 10 12
Threshold
0
50
100
150
200
250
300
25
/2
5
22
5/
25
47
5/
25
24
75
/2
5
50
/5
0
Mix of regular/targeted clients
0
100
200
300
400
500
600
700
0 2 4 6 8 10 12
Guards per client
0
20
40
60
80
100
120
140
160
Figure 9: Violin plots showing Chapman estimates for
10,000 trials each of: dierent values of B (top le), dierent
threshold values (top right), dierent combinations of regu-
lar/targeted client types (bottom le), and dierent numbers
of guards per client (bottom right). When not being explic-
itly varied as noted, the parameters are: B = 0.25, threshold
of 3, 25 targeted clients who visit 10 times per period, 225
regular clients who visit 2 times per period, and 1 guard per
client.
to be 3 for this combination of parameters; this also appears to be
the case for other parameter combinations that we explored (with
the regular clients visiting the destination twice and the targeted
clients visiting 10 times). As might be expected, increasing the
number of guards generally decreases the adversary’s accuracy.
Finally, the adversary’s estimate is reasonable for small enough
numbers of targeted users; however, when these users are just 1%
of the total user population, the estimate is o by a large amount.
More sophisticated and accurate estimators exist. For example,
the Schnabel estimator generalizes Lincoln–Petersen to incorpo-
rate repeated samplings with marking of not-previously-marked
members of the population. Other estimators may allow relaxation
of assumptions such as constant ratios from one sample to the next.
And, unlike most applications, the set of unmarked guards has a
structure that can inform our estimates. For example, given a guard
observed to have carried circuits connecting to a targeted site 9
times in a day, we can use the expectation that the associated client
actually visited 10 or more times that day to adjust our estimates,
and similarly for smaller numbers of target visits per guard per
day.
6 DISCUSSION
In this paper, we have considered adversaries with dierent goals
and strategies, but oen with the same endowment and capabilities
as adversaries in previous work. Another important dierence only
touched on above is how long an adversary may have (or be willing
Onions in the Crosshairs , ,
to use) some of his resources. is can aect both aack success
and decisions about which aacks to aempt.
We now briey describe some of the temporally dependent fea-
tures of an adversary’s endowment and strategy, although we leave
detailed analysis of this for future work. We then describe possible
countermeasures to a targeting adversary, particularly one with
temporal limitations on his endowment.
6.1 Time Is on My Side
Temporal aspects of adversary endowment have been considered for
a long time [24], and have also been applied to onion routing before
Tor [29]. And we have noted that intersection aacks are inherently
across time. Nonetheless, these adversaries have a uniform bound
on resources; what varies is either the distribution of these [24] or of
network resources and status [18]. And generally, these adversaries
will still be equally happy to aack any user, or possibly any user
with the same behavior on the network.
In this section we discuss a Tor adversary that has, or is willing
to deploy, dierent amounts and types of resources at dierent
times, typically based on some particular target. Limitations on
endowment could derive from funding or statutory start date or
expiration. (We assume that the adversary has enough understand-
ing of his funding status to plan.) More generally, given a level
of required resources, approving or commiing to any course of
action is typically easier if the anticipated duration of needed re-
sources is relatively clear, whether for oneself or when authorizing
others, and whether psychologically or in policy-based decisions;
the shorter that duration is, typically the greater the willingness
to commit resources. Similarly, the presence of intermediate goals,
which if achieved would themselves be of interest or would help
determine whether and how to commit to further action, can make
commitment to an action more acceptable hence more likely.
Johnson et al. [18] set out as a behavioral user class an IRC user,
who 27 times a day creates the same single IRC sessions to the same
server. We now compare their analysis of IRC users’ security against
an end-to-end correlating relay adversary to the adversary in Sec. 4
targeting a cabal that meets on a private IRC channel. For this user
type, they considered variously endowed adversaries and looked in
detail at a relay adversary allocated 100 MiB/s, approximately 4%
of the total network bandwidth at the time of their analysis, and on
the order of the largest identied families of relays under a single
operator or organization. e time until an IRC user experienced
the rst end-to-end correlation failure (both guard and exit of a
circuit compromised) was analyzed under an optimal distribution
of adversary bandwidth to guards and exits (roughly four to one).
In our scenario, a relay is assumed to be able to always identify a
cabal connection passing through it. So we should assume that the
Johnson et al. adversary is able to devote all relays to the guard
position. We thus very roughly estimate a 20% reduction in median
time to compromise compared to what Johnson et al. reported.
For a cabal size of 10 or 20, and a roughly comparable fraction of
bandwdith allocated to middle relays, our targeting adversary will
have a good idea of cabal size and will identify the guards of nearly
all cabal members in under 4 days (100 meetings). According to
the analysis by Johnson et al., the just-described contributed-guard
adversary will require about 10 times as long to get a much rougher
idea of cabal size, having identied guards for roughly half the
cabal. To get approximately the same likelihood as the targeting
adversary of having identied guards for almost all of the cabal
will take 40-50 times as long (under a week vs. 150-200 days). is
is not just about size: in about a week of IRC usage the targeting
adversary will have a good sense of cabal size, cabal guards, and
client send-receive activity per cabal guard (which may indicate
cabal leaders and will indicate which members send the most).
e contributed-guard adversary is also no more likely at any
time to identify a cabal leader than any other member. is also
holds for the targeting adversary with respect to identifying a
leader’s guard. But it will still typically take less than a week at
the stated rate of meetings. For a leader recognized by message
paerns, a determined targeting adversary can bring all resources
and aacks to bear to signicantly increase his chance of bridging
a leader’s guard. Some aacks may take weeks to know if they
have succeeded, but others will take only hours or even minutes.
A leader’s guard might be resistant to all aempts at compromise
or bypass. But such relays can then be subject to persistent DoS or
other resource depletion aacks [14], giving the adversary another
chance with a new leader guard.
One guard vs. three or more guards in general involves many
trade-os, some of which we have identied. Our analysis of leader
identication in Sec. 4 clearly favors using single guards to thwart
our targeting aacker. But vs. the contributed-guard adversary
(which was a primary basis for Tor’s decision to move to single
guards [6]), this is much more signicant in terms of speed of aack
and adversary feedback in process—despite a correlating adversary
automatically identifying any client for which he owns the guard.
ough the correlating adversary remains important and should
not simply be abandoned, this again shows that even when they
agree about what is more secure, the targeting adversary is oen
the dominant basis for the assessment.
Johnson et al. also assume a network at steady state, aer ad-
versary relays have obtained the guard ag. Middle relays can see
some usage in less than a day aer announcing themselves and
reach steady state in about a week. Relays generally take over a
week to obtain the guard ag and about ten weeks to reach steady
state for guard usage [5]. For an adversary mounting an aack from
scratch, the above time comparison thus overstates signicantly in
favor of the contributed-guard adversary.
6.2 Possible countermeasures
e primary purpose of this paper is to describe a class of adver-
sary that has been overlooked but we believe is as pertinent or
more pertinent for Tor than the one generally receiving the most
aention. Before concluding, however, we wanted to at least sketch
some possible ways to improve resilience against such selective
adversaries on the Tor network. In the interest of space (and time)
we have limited the scope of our analysis to adversaries at Tor re-
lays, only minimally considering an adversary on the network links
between them and/or between clients or destinations and the Tor
network. Any countermeasure we describe here will likely need to
be signicantly redesigned to be eective once those resources are
added to the adversary arsenal. So there is lile point to providing
more than a sketch here.
, , Aaron D. Jaggard and Paul Syverson
Layered Guards: e same paper that introduced Tor guards also
introduced the idea of layered guards [25]. e notion of layered
guards has been revisited periodically, most recently in a Tor Pro-
posal on slowing guard discovery for onion services [19]. Using
only one or a small set of relays for each client-guard’s middle
could make it hard to identify guards just as guards make it hard to
identify clients. But, if single persistent middles are chosen, then
randomly selected exits could possibly enumerate and monitor the
behavior of associated users for many destinations and, worse, will
always know for which destination without ngerprinting. In the
case of ngerprinted onion services, cabal/user enumeration will
be possible using (easily spun up) middles chosen as rendezvous
points. In general, the number and rotation of guards and their
second-layer guards can complicate determination of cabal size as
well as guard discovery.
Randomized selection of guard set size and duration: As our
analysis shows, single, persistent guards generally provide much
more enumeration information about size of a cabal or set of targeted-
site users and more information about targeted-site user behavior
than does a set of guards with more overlap between clients’ sets.
And long persistence means that bridging an identifed guard is not
needed for monitoring a portion of targeted client’s behavior, and
that, if a bridging is aempted, it will pay o for a long period of
monitoring targeted-client behavior and IP address(es). And that
bridging need not be quickly successful to be useful. In addition
to enlarging and randomizing size of a client’s guard set, selecting
guards for less persistent or predictable periods would also counter
targeting aacks, pseudonymous proling, and condence in the
expected value of bridging a guard. Other related strategies may be
worth investigating, such as a client limiting and controlling the
use of the same guard for visits to the same sensitive site. Obviously
there is a tension between the increased risk of correlation aack
from using more guards for shorter periods and targeted aacks
on cabals or clients of targeted sites.
Trust: One way to simultaneously reduce vulnerability from both
targeted middle-relay aacks and untargeted correlation aacks is
to incorporate trust into route selection. e paper that introduced
guards for Tor observed that guards could be “chosen at random
or chosen based on trust” [25]. Subsequent work noted that trust
could be based on many of the criteria just mentioned as useful for
determining which bridging strategies might be eective against
which guards and introduced a mathematical characterization of
trust as the complement to the probability of compromise [16]. e
downhill algorithm [17] explored combining layered guards with
trust: A the rst relay in a circuit is chosen from a small set of
those most highly trusted. Later relays in a circuit are selected
from ever larger sets that include relays further “down the hill” of
assigned trust. is would add delay to enumeration aacks as
well as reducing their eectiveness. It could also be combined with
varying periods of guard rotation in various ways. An unpublished
version of the downhill algorithm had a slowly rotating rst-hop
relay set and ever faster rotating relay sets for each subsequent
hop [27]. For trust-based protections to be eective in practice,
trust of all elements in the network path (ASes, IXPs, submarine
cables, etc.) must be considered [13].
Standardized onion service trac templates: Our targeting
aacks on onion services are dependent on the eectiveness of
ngerprint-based individuation of them. Making trac nger-
prints of many onion services similar to each other could reduce
the eectivness of those aacks. Providing simple bundles or tem-
plates for users wanting to set up onionsites would be useful for
many reasons, incorporating data management and communication
protocols to create default standardized trac ngerprints among
them. To reduce the likelihood that a single draconian standard
will discourage site operators from using these defaults, a small
number of templates might be available depending on basic site
conguration choices. Sites using standardized templates can also
communicate this to clients either in their directory information
or upon rst contact. Trac ngerpint normalization can then
be enhanced by cooperation between clients and sites. To further
facilitate adaptation to individual sites, onion protocols could use
componentized chunks of ngerprint-normalized communication,
possibly split over multiple circuits. Again the trade-os against
correlation vulnerability would need to be considered, but we hope
we have shown that ngerprinting by interior elements of the net-
work is as realistic or more realistic and serious a threat to Tor’s
most sensitive users.
7 CONCLUSIONS AND FUTUREWORK
We have introduced targeting adversaries, who focus on particular
system users or groups of users, and shown that the dierence
in goals and strategy of these overlooked adversaries can lead to
aacks on Tor users at least as devastating and relevant as any Tor
aacks set out in previous research. While we have shown the
capabilities of a targeting adversary in realistic scenarios involv-
ing a published multicast system, IRC, and onionsites, we cannot
hope to quantify in this introductory treatment the possible eects
for all Tor users, for all possible client locations and network des-
tinations. We anticipate extensive future research into targeting
adversaries, including: abstract characterization and formal treat-
ment of targeting adversaries, expansion of contexts in applied
targeting adversary models (such as network link adversaries), and
analysis of security against combinations of targeted and hoover-
ing adversaries, particularly when large and well-resourced. And
we expect all of this to have an impact on future guard and path
selection algorithms.
Onion services are also an active area of discussion and redevel-
opment [21, 35]. We expect that an interesting and useful direction
for future research will be the analysis of the eects of dierent re-
design proposals on security in the context of targeting adversaries.
is will require a substantial extension to TorPS [15], which does
not currently support modeling of onion services.
ACKNOWLEDGMENTS
We thank Giulia Fanti, Nick Hopper, Rob Jansen, George Kadianakis,
Ma Traudt, and Ryan Wails for their comments on earlier dras
of this paper.
REFERENCES
[1] Nicola Acceura, Giovanni Neglia, and Luigi Alfredo Grieco. e capture-
recapture approach for population estimation in computer networks. Computer
Networks, 89:107 – 122, 2015.
[2] J. Appelbaum and A. Mue. e “.onion” special-use domain name. hps:
//tools.ietf.org/html/rfc7686, 2015.
Onions in the Crosshairs , ,
[3] Alex Biryukov, Ivan Pustogarov, and Ralf-Philipp Weinmann. Trawling for Tor
hidden services: Detection, measurement, deanonymization. In Proceedings of
the 2013 IEEE Symposium on Security and Privacy, SP ’13, 2013.
[4] George Danezis and Paul Syverson. Bridging and ngerprinting: Epistemic
aacks on route selection. In Proceedings of the 8th International Symposium on
Privacy Enhancing Technologies, PETS ’08, pages 151–166, 2008.
[5] Roger Dingledine. e lifecycle of a new relay. hps://blog.torproject.org/blog/
lifecycle-of-a-new-relay, September 2013.
[6] Roger Dingledine, Nicholas Hopper, George Kadianakis, and Nick Mathewson.
One fast guard for life (or 9 months). 7th Workshop on Hot Topics in Privacy
Enhancing Technologies (HotPETs 2014), July 2014.
[7] Roger Dingledine, Nick Mathewson, and Paul Syverson. Tor: e Second-
Generation Onion Router. In Proceedings of the 13th USENIX Security Symposium,
August 2004.
[8] Nick Feamster and Roger Dingledine. Location diversity in anonymity networks.
In Sabrina De Capitani di Vimercati and Paul Syverson, editors, WPES’04: Pro-
ceedings of the 2004 ACM Workshop on Privacy in the Electronic Society, pages
66–76, Washington, DC, USA, October 2004. ACM Press.
[9] David Goulet and George Kadianakis. Random number generation during Tor vot-
ing (Tor proposal #250). hps://gitweb.torproject.org/torspec.git/tree/proposals/
250-commit-reveal-consensus.txt, August 2015.
[10] Jamie Hayes and George Danezis. Guard sets for onion routing. Proceedings on
Privacy Enhancing Technologies, 2015(2):65–80, June 2015.
[11] Andrew Hintz. Fingerprinting websites using trac analysis. In Roger Dingledine
and Paul Syverson, editors, Privacy Enhancing Technologies: Second International
Workshop, PET 2002, pages 171–178, San Francisco, CA, USA, April 2002. Springer-
Verlag, LNCS 2482.
[12] Nicholas Hopper, Eugene Y. Vasserman, and Eric Chan-Tin. How much
anonymity does network latency leak? In Sabrina De Capitani di Vimercati,
Paul Syverson, and David Evans, editors, CCS’07: Proceedings of the 14th ACM
Conference on Computer and Communications Security, pages 82–91. ACM Press,
2007.
[13] Aaron D. Jaggard, Aaron Johnson, Sarah Cortes, Paul Syverson, and Joan Feigen-
baum. 20,000 in league under the sea: Anonymous communication, trust, MLATs,
and undersea cables. Proceedings on Privacy Enhancing Technologies, 2015(1):4–24,
April 2015.
[14] Rob Jansen, Florian Tschorsch, Aaron Johnson, and Björn Scheuermann. e
sniper aack: Anonymously deanonymizing and disabling the Tor network. In
Proceedings of the Network and Distributed Security Symposium - NDSS ’14. IEEE,
February 2014.
[15] Aaron Johnson. Tor Path Simulator code. hps://github.com/torps/torps.
[16] Aaron Johnson and Paul Syverson. More anonymous onion routing through
trust. In 22nd IEEE Computer Security Foundations Symposium, CSF 2009, pages
3–12, Port Jeerson, New York, July 2009. IEEE Computer Society.
[17] Aaron Johnson, Paul Syverson, Roger Dingledine, and Nick Mathewson. Trust-
based anonymous communication: Adversary models and routing algorithms.
In Proceedings of the 18th ACM Conference on Computer and Communications
Security, CCS ’11, pages 175–186, New York, NY, USA, 2011. ACM.
[18] Aaron Johnson, Chris Wacek, Rob Jansen, Micah Sherr, and Paul Syverson. Users
get routed: Trac correlation on Tor by realistic adversaries. In Proceedings of
the 2013 ACM SIGSAC Conference on Computer & Communications Security, CCS
’13, pages 337–348, New York, NY, USA, 2013. ACM.
[19] George Kadianakis and Mike Perry. Defending against guard discovery aacks
using vanguards (Tor proposal #247). hps://gitweb.torproject.org/torspec.git/
tree/proposals/247-hs-guard-discovery.txt, July 2015.
[20] Dong Lin, Micah Sherr, and Boon au Loo. Scalable and anonymous group
communication with MTor. Proceedings on Privacy Enhancing Technologies,
2016(2):22–39, June 2016.
[21] Nick Mathewson. Next-generation hidden services in Tor, (Tor proposal
#224). hps://gitweb.torproject.org/torspec.git/tree/proposals/224-rend-spec-ng.
txt, November 2013.
[22] Nick Mathewson. Some thoughts on hidden services. hps://blog.torproject.org/
blog/some-thoughts-hidden-services, December 2014.
[23] Alec Mue. RFC 7686 and all that . . . . hps://www.facebook.com/notes/
alec-mue/rfc-7686-and-all-that/10153809113970962, 2015.
[24] Rafail Ostrovsky and Moti Yung. How to withstand mobile virus aacks. In
Proceedings of the Tenth ACM Symposium on Principles of Distributed Computing
(PODC ’91), pages 51–59. ACM Press, 1991.
[25] Lasse Øverlier and Paul Syverson. Locating hidden servers. In 2006 IEEE Sympo-
sium on Security and Privacy (S& P 2006), Proceedings, pages 100–114. IEEE CS,
May 2006.
[26] Gareth Owen and Nick Savage. e Tor dark net. Paper Series: No. 20 20, Global
Commission on Internet Governance, September 2015.
[27] Personal communication.
[28] Michael Reiter and Aviel Rubin. Crowds: Anonymity for web transactions. ACM
Transactions on Information and System Security (TISSEC), 1(1):66–92, 1998.
[29] Paul Syverson, Gene Tsudik, Michael Reed, and Carl Landwehr. Towards an
analysis of onion routing security. In InternationalWorkshop on Designing Privacy
Enhancing Technologies: Design Issues in Anonymity and Unobservability, pages
96–114, New York, NY, USA, 2001. Springer-Verlag New York, Inc.
[30] Who uses Tor. hps://www.torproject.org/about/torusers.html. Accessed Febru-
ary 2017.
[31] Tor metrics portal. hps://metrics.torproject.org/.
[32] Tor network size. hps://metrics.torproject.org/networksize.html.
[33] e Tor Project. hps://www.torproject.org/.
[34] Tao Wang and Ian Goldberg. On realistically aacking Tor with website nger-
printing. Proceedings on Privacy Enhancing Technologies, 2016(4):21–36, October
2016.
[35] Tim Wilson-Brown, John Brooks, Aaron Johnson, Rob Jansen, George Kadi-
anakis, Paul Syverson, and Roger Dingledine. Rendezvous single onion ser-
vices, (Tor proposal 260. hps://gitweb.torproject.org/torspec.git/tree/proposals/
260-rend-single-onion.txt, 2015.
[36] Mahew Wright, Micah Adler, Brian Neil Levine, and Clay Shields. Defending
anonymous communication against passive logging aacks. In Proceedings, 2003
IEEE Symposium on Security and Privacy, pages 28–43. IEEE Computer Society,
May 2003.
A MODEL
e model we present is meant to t within general models of
network actors. Jaggard et al. [13] describe a more general model
and associated formal language. We focus here in seing out a
model specic to Tor and that is suitable for describing targeting
adversaries. Adversaries are modeled in App. B.
A.1 Entities
Principals P is the set of all principals. We think of these as
the individual humans using the system.
• ese may be mobile or using multiple Tor client in-
stances
– For now, we assume that a change in location
by a principal implies a change in the client
instance being used.
• e principals are the ultimate targets of aacks. A
natural simplifying step is to focus on aacking Tor
clients instead of the principals using the clients. Do-
ing so omits the consideration of aacking a principal
who uses Tor in multiple locations because, in our
current model, each location would have a dierent
client.
• Principals might use multiple identities, e.g., via the
New Identity buon in the Tor Browser
Identities I is the set of all identities. An identity is any col-
lection of actions or properties that might be tied together
under a pseudonym from some perspective. But unlike the
usual usage of ‘pseudonym’, we consider the possibility
that a single identity might include the behavior of multiple
principals. In practice, we generally make the simplifying
assumption that relevant perspectives assign one princi-
pal per identity. Clicking the New Identity buon in Tor
Browser results in a new identity from the perspective of
a destination seeing prior and posterior connections from
that client.
Clients CL is the set of Tor clients.
• To simplify things, we assume that each client is used
by a single principal. As noted in the discussion of
principals, a single principal may use multiple Tor
clients.
Relays R denotes the set of all Tor relays.
, , Aaron D. Jaggard and Paul Syverson
Guards G denotes the set of all Tor guards. G ⊆ R. We use
G for a set of guards used by an entity. We will assume
that each client chooses a single set G of guards from G
according to a distributionγ , which may be time dependent.
More complex guard selection strategies are also possible.
[10]
Middles M denotes the set of all Tor middle relays.M ⊆ R
Exits E is the set of Tor exit nodes. E ⊆ R.
Circuits CIR is the set of Tor circuits.
Destinations D is the set of destinations.
Links L is the set of network links between relays, bridges,
clients, and destinations. We will consider only links be-
tween clients and the Tor network or destinations and the
Tor network in this paper, and only abstractly. In practice
it can be further decomposed into ASes, ISPs, submarine
cables, etc. [13]
Bridges We will not analyze the use of Tor bridges in this
paper, and include this entry only for readers who may be
wondering about the omission.
Remark A.1 (Conventions). We typically use lowercase, italicized
leers for atomic individuals (e.g., a guard д), uppercase, italicized
leers for distinguished sets of atomic individuals (e.g., a principal’s
guard set G), and uppercase, caligraphic leers for the set of all
such possibilities (e.g., the set G of all guards).
A.2 Dynamics, history, and activity
Roughly, we expect a principal to choose a client cl (possibly mul-
tiple clients over time). Each client will have a distribution δ on
destinations (this may be inherited from the underlying principal).
Given δ , the client will choose a distribution γ on 2G ; this may be
time dependent. (We expect that γ will capture the various guard-
selection procedures of interest. Its support may thus be limited to
all sets of size k , etc..) e guard set may change over time; indeed,
the eect of temporal change is a question upon which this work
will shed light).
Once the client has chosen her guard set, she selects/receives
destinations one at a time. We assume that the destination d is
drawn from D according to a distribution δ that depends on either
the principal or the client, depending on which type is the ultimate
target of aack.1 For each destination, we assume that the client
selects an existing circuit or builds a new one; we assume that
the circuit-selection and -construction decisions are made in an
online way, before the next destination is selected/received. Once
the circuit (which we denote ci) is chosen/constructed, the client
and the destination exchange trac over the circuit. is trac
may depend on the circuit and destination d , and we assume it is
timestamped. As needed, we denote this trac by tr(cl, ci,d, t).
We capture this ow as shown in Fig. 10, with right arrows denot-
ing the sequence of events and le arrows indicating randomized
choices.
1δ could be enriched to model the fact that destinations might not be chosen indepen-
dently of each other. One destination may induce trac to/from another destination
automatically, or the trac may prompt the user to explore other destinations on her
own, e.g., going to the Wikipedia page for a person mentioned in a news article that
she reads. δ might then produce a set of destinations, each of which is fed into the
rest of the process modeled here.
We think of the history and current activity of the principal/client
as the primary things that an adversary might use in an aack. As
an initial step, we take the history and current activity to comprise
instances of:
• Circuit construction, including timestamps (and perhaps
some portion of the circuit)
• Tor trac, including some projection of the information in
tr(cl, ci,d, t) (the projection may be dierent for dierent
instances of trac)
B ADVERSARIES AND THEIR GOALS
B.1 Adversaries
Adversaries will vary in the type of resources they control or ob-
serve. ey may observe or operate Tor relays, or they may observe
all trac at an ISP or AS. ey may observe all trac arriving at
a specic destination. In addition they may have dierent endow-
ments of each type of resource, and they may vary in their goals.
We will leave goals until aer seing out the targets to which they
may direct those goals.
An adversary will be dened as a tuple of resources of the types
dened in the model, where each entry will reect the endowment,
either as an absolute number or as a fraction. Elements in the
tuple are not automatically independent or disjoint. For example,
one element may reect the number of compromised guards and
another the number of compromised middles. How to handle such
will depend on the needs of the analysis being done.
B.2 Targets and the proles they generate
For convenience, we let T be the set of possible adversary targets.2
As we consider possible aacks on targets, we will think of the
adversary as having “proles” of his targets. We leave the formal
denition of these to specic scenarios; broadly, we think of a
prole as comprising some combination of history and auxiliary
information such as identity, anity, etc..3 We consider I ⊂ T .
is is a proper subset because adversaries may be targeting a
collection of identities and may be specically targeting a collection
of principals, e.g., all the people who visit a particular website on a
given day or receive email from a particular identity.
We might consider proles generated by targets, e.g., P = P(T ).
While again deferring formal denitions, this intuitively means that
all of the history in P was generated by T , and all of the auxiliary
information in P is about T . Of course, for a collection T ′ of
targets, we can generalize this to saying that a prole P concerns
this collection, wrien P = P(T ′), if
P ⊆
⋃
T ∈T′
P(T ). (5)
Similarly, we will write A for ongoing network activity that the
adversary might observe. Connecting this to specic targets raises
the same issues raised by proles, so we will write A = A(T ) to
indicate that activity is generated by a particular target T .
2It is natural to let this be the set of all principals or the set of all clients, but we do
not want to restate this inclusiveness at every instance of discussing targets, nor do
we wish to rule out other collections of targets.
3While we defer the formal denition of a prole, we note that it must admit a notion
of containment so that we can meaningfully write P ⊆ P′.
Onions in the Crosshairs , ,
p −→ cl −→ δ −→ γ −→ (G
γ
←− 2G) −→ (d δ←− D) −→ ci −→ tr(cl, ci,d, t)
(4)
Figure 10: Activity ow in our model.
B.3 Adversary goals
We now set out some of the abstract goals a targeting adversary
may have.
Prole matching e adversary has a pseudonymous pro-
le and is presented with new network activity; is this
activity part of the prole (or how likely is it to be so)? I.e.,
given P(T ) and A(T ′), does T = T ′?
Prole extension e adversary has a user prole (not nec-
essarily pseudonymous); can the adversary extend this to
include additional information (perhaps of a specied type).
I.e., given P(T ), construct P ′(T ) ⊇ P(T ).
Knowledge gathering A client is identied as being of in-
terest; what else can the adversary learn about the client?
I.e., given T , construct P(T ) (maybe also with some auxil-
iary information Aux(T ) related to the target).
Group identication Which people (or pseudonymous pro-
les) are behaving similarly, e.g., using the same services?
(If “behaving similarly” is dened by some predicate, how is
the ease of answering this question related to the specicity
of the predicate?) As one example, given a target T and a
predicate P (maybe satised by T ), describe {T ′ |P(T ′)} (or
a nontrivial subset of this set).
Group enumeration Whether or not the individual targets
are known, it may be of interest to determine the exact
or approximate size of a group target, |{T ′ |P(T ′)}|. is is
one of the goals considered in the body of this paper.
An example of a group target is a cabal with some properties in
common, e.g., meet together on Tuesdays at noon, all were on the
same high school sports team, etc.
As in earlier research, a primary goal is to associate source and
destination of communication. On the other hand, for a targeting
adversary if the source or destination here is a target, other sources
(respectively destinations) may be ignored as uninteresting. Other
goals also become salient for a targeting adversary. For example,
group enumeration may be useful in itself as a measure of a group
target’s signicance. Other goals of interest may be similarly about
numbers rather than about source and destination correlation per
se: the fraction of a source target’s connections devoted to dierent
categories of activity, website downloads vs. chaing or bandwidth
to route-unprotected sites vs. onionsites, etc.
C COMPUTATIONS
C.1 Identifying an IRC cabal member
C.1.1 One-guard case. If the cabal leader is using one guard,
then with probability B the leader will choose a compromised mid-
dle relay during the rst meeting, allowing the aacker to learn
the leader’s guard. With probability pb , the aacker will learn
the leader’s address. Alternatively, the leader does not use a com-
promised middle relay for the rst meeting (which happens with
probability 1−B, or with probability (1−B)i for the rst i meetings)
but then uses a compromised middle relay (with probability B) for
the second (or (i + 1)st) meeting. Once the compromised middle
relay is used, then the aacker learns the leader’s address with
probability pb . We note that it only maers when the leader rst
uses a compromised middle relay—the aacker only has one chance
to “bridge” the leader’s guard; if she fails the rst time, then we
assume that she is not able to successfully bridge that guard on
a later occasion that the leader uses a compromised middle relay.
us, we have that the probability of the adversary successfully
bridging the cabal leader’s single guard is
Bpb + (1 − B)Bpb + · · · + (1 − B)m−1Bpb
=
[
1 − (1 − B)m
]
pb . (6)
C.1.2 Three-guard case. We turn now to the case where each
client uses three guards. Our approach parallels the one-guard
case, but the computations becomes more complex. We describe a
general approach that works for any number of guards. e sepa-
rates out the adversary’s successful identication of the client into
dierent cases, depending on how many guards the adversary un-
successfully bridges before successfully bridging one of the client’s
guards. Together, the probability of success in these cases (up to
the total number of guards used by the client) gives the adversary’s
total probability of success.
In particular, we let Fnj (i), with i > 0, be the probability that the
adversary makes her jth observation of a previously unseen guard i
meetings aer previously observing a new guard, that the leader is
using n guards total (and choosing from these uniformly at random
for each meeting), and that the adversary fails to bridge this guard.
is involves the leader doing something other than both choosing
a fresh guard (i.e., one the adversary has not previously aempted
to bridge) and a compromised middle relay for i j −1 meetings (each
time with probability 1 − n−j+1n B), then choosing a fresh relay and
a compromised middle guard (with probability n−j+1n B), and then
the adversary failing to bridge that guard (with probability 1 − pb ).
is gives
Fnj (i) =
(
1 − n − j + 1
n
B
)i−1 n − j + 1
n
B(1 − pb ). (7)
We also let Snj (m) denote the probability that the adversary suc-
cessfully bridges the jth guard she observes and that she has m
meetings in which to do so (aer trying and failing to bridge the
(j − 1)st guard. is involves the leader doing something other than
both choosing a fresh guard and a compromised middle relay for k
meetings (each time with probability 1 − n−j+1n B), 0 ≤ k ≤ m − 1,
and then choosing a fresh guard and a compromised middle relay
, , Aaron D. Jaggard and Paul Syverson
(with probability n−j+1n B) and having the adversary bridge that
guard (with probability pb ). is gives
Snj (m) =
[
1 −
(
1 − n − j + 1
n
B
)m ]
pb . (8)
With this notation, we can concisely capture the probability of
success for any number of guards д as
д∑
k=1
∑
0=i0<i1< · · ·<ik−1<m
S
д
k (m − ik−1)
k−1∏
j=1
F
д
j (i j − i j−1). (9)
Here, k represents the guard that is successfully bridged, and i1,
. . . , ik−1 are the meeting numbers at which the previous guards are
identied but unsuccessfully bridged.
In the three-guard case, we obtain the following probability of
success for identifying the leader of the cabal:
S31(m) +
∑
1≤i1<m
F 31 (i1)S
3
2(m − i1)+∑
1≤i1<i2<m
F 31 (i1)F
3
2 (i2 − i1)S
3
3(m − i2). (10)
C.2 Guard collisions
We show in Tab. 1 the probabilities of multiple clients sharing a
guard for one and three guards per client and various cabal sizes.
is assumes 2,500 guards.
# Clients→ 3 5 10 20 25
1 guard/client 0.1% 0.4% 1.8% 7.3% 11.3%
3 guards/client 1.1% 3.5% 15.0% 49.8% 66.4%
Table 1: Guard-collision probabilities for 2,500 guards, one
or three distinct guards per client, and various numbers of
clients.
D MLE COMPUTATIONS
Here, we present more detail about the process of computing the
MLE of cabal size.
First, we construct the function L(®x |θ ) capturing the likelihood
that the adversary will make a sequence of observations ®x if the
actual size of the cabal is θ . is is 0 if any of the observations
is greater than θ . Otherwise, it is a maer of choosing, in the ith
meeting, xi cabal members to observe, observing each of them with
probability B, and not observing each of the θ − xi cabal members
with probability 1−B. is is done independently for each meeting,
giving us
L(®x |θ ) =
∏
i
(
θ
xi
)
Bxi (1 − B)θ−xi , xi ≤ θ . (11)
We consider actual cabal sizes c ranging from 1 to 20. For each
c and each possible observation vector ®x that could be observed
for that cabal size (i.e., iterating over {0, . . . , c}m ), we compute
the value θ (®x) that maximizes the likelihood function for ®x by
numerically testing values of θ up to 100. (If multiple values of θ
maximize L(θ | ®x), we choose the smallest as θ (®x).) We also compute
the probability, given the actual cabal size c , that ®x is observed by
the adversary.

