A Language Modeling Approach to Information Retrieval 
Jay M. Ponte and W. Bruce Croft 
Computer Science Department 
University of Massachusetts, Amherst 
{ponte, croft}&s.umass.edu 
Abstract Models of document indexing and docu- 
ment retrieval have been extensively studied. The in- 
tegration of these two classes of models has been the 
goal of several researchers but it is a very difficult prob- 
lem. We argue that much of the reason for this is the 
lack of an adequate indexing model. This suggests that 
perhaps a better indexing model would help solve the 
problem. However, we feel that making unwarranted 
parametric assumptions will not lead to better retrieval 
performance. Furthermore, making prior assumptions 
about the similarity of documents is not warranted ei- 
ther. Instead, we propose an approach to retrieval based 
on probabilistic language modeling. We estimate models 
for each document individually. Our approach to model- 
ing is non-parametric and integrates document indexing 
and document retrieval into a single model. One advan- 
tage of our approach is that collection statistics which 
are used heuristically in many other retrieval models are 
an integral part of our model. We have implemented 
our model and tested it empirically. Our approach sig- 
nificantly outperforms standard tf.idf weighting on two 
different collections and query sets. 
1 Introduction 
Over the past three decades, probabilistic models of doc- 
ument retrieval have been studied extensively. In gen- 
eral, these approaches can be characterized as methods 
of estimating the probability of relevance of documents 
to user queries. One component of a probabilistic re- 
trieval model is the indexing model, i.e., a model of the 
assignment of indexing terms to documents. We argue 
that the current indexing models have not led to im- 
proved retrieval results. We believe this is due to two 
unwarranted assumptions made by these models. We 
have taken a different approach based on non-parametric 
estimation that allows us to relax these assumptions. We 
have implemented our approach and empirical results on 
two different collections and query sets are significantly 
better than the standard tf.idf method of retrieval. Now 
we take a brief look at some existing models of document 
indexing. 
We begin our discussion of indexing models with the 
2-Poisson model, due to Bookstein and Swanson [l] and 
Permission to make digital/hard copy of all or part of this work 
for personal or classroom use is granted without fee provided that 
copies are not made or distributed for profit or commercial ad- 
vantage, the copyright notice, the title of the publication and its 
date appear, and notice is given that copying is by permission of 
ACM, Inc. To copy otherwise, to republish, to post on servers or 
to redistribute to lists, requires prior specific permission and/or 
fee. SIGIR’93, Melbourne, Australia @ 1998 ACM l-58113-015-5 
8/98 $5.00. 
also to Harter [7]. By analogy to manual indexing, the 
task was to assign a subset of words contained in a doc- 
ument (the ‘specialty words’) as indexing terms. The 
probability model was intended to indicate the useful in- 
dexing terms by means of the differences in their rate 
of occurrence in documents ‘elite’ for a given term, i.e., 
a document that would satisfy a user posing that sin- 
gle term as a query, vs. those without the property of 
eliteness. 
The success of the 2-Poisson model has been some- 
what limited but it should be noted that Robertson’s tf, 
which has been quite successful, was intended to behave 
similarly to the 2-Poisson model [12]. 
Other researchers have proposed a mixture model of 
more than two Poisson distributions in order to better 
fit the observed data. Margulis proposed the n-Poisson 
model and tested the idea empirically [lo]. The conclu- 
sion of this study was that a mixture of n-Poisson distri- 
butions provides a very close fit to the data. In a certain 
sense, this is not surprising. For large values of n one 
can fit a very complex distribution arbitrarily closely by 
a mixture of n parametric models if one has enough data 
to estimate the parameters [18]. However, what is some- 
what surprising is the closeness of fit for relatively small 
values of n reported by Margulis [lo]. 
Nevertheless, the n-Poisson model has not brought 
about increased retrieval effectiveness in spite of the close 
fit to the data. In any event, the semantics of the under- 
lying distributions are less obvious in the n-Poisson case 
as compared to the 2-Poisson case where they model the 
concept of eliteness. 
Apart from the adequacy of of the available index- 
ing models, estimating the parameters of these models 
is a difficult problem. Researchers have looked at this 
problem from a variety of perspectives and we will dis- 
cuss several of these of these approaches in section 2. In 
addition, as previously mentioned, many of the current 
indexing models make assumptions about the data that 
we feel are unwarranted. 
l The parametric assumption. 
l Documents are members of pre-defined classes. 
In our approach we relax these two assumptions. 
Rather than making parametric assumptions, as is done 
in the 2-Poisson model it is assumed that terms follow a 
mixture of two Poisson distributions, as Silverman said, 
“the data will be allowed to speak for themselves [16].” 
We feel that it is unnecessary to construct a parametric 
model of the data when we have the actual data. Instead, 
we rely on non-parametric methods. 
Regarding the second assumption, the 2-Poisson model 
was originally based on the idea of ‘eliteness’ [7]. It was 
assumed that a document elite for a given term would 
275 
satisfy a user if the user posed that single term as a query. 
Since that time, the prevailing view has come to be that 
multiple term queries are more realistic. In general, this 
requires a combinatorial explosion of elite sets for all pos- 
sible subsets of terms in the collection. We take the view 
that each query needs to be looked at individually and 
that documents will not necessarily fall cleanly into elite 
and non-elite sets. 
In order to relax these assumptions and to avoid the 
difficulties imposed by separate indexing and retrieval 
models, we have developed an approach to retrieval based 
on probabilistic language modeling. Our approach pro- 
vides a conceptually simple but explanatory model of re- 
trieval. 
At this time, we should make clear what we mean 
by the word ‘model.’ In our view, the word ‘model’ is 
used in information retrieval in two senses. The first 
sense denotes an abstraction of the retrieval task itself. 
The best example of this is the vector space model which 
allows one to talk about the task of retrieval apart from 
implementation details such as storage media, and data 
structures [15]. A second sense of the word ‘model’ is 
the probabilistic sense where it refers to an explanatory 
model of the data. This was intention behind the 2- 
Poisson model. 
We add a third sense of the word when we refer to lan- 
guage modeling. The phrase ‘language model’ is used by 
the speech recognition community to refer to a probabil- 
ity distribution that captures the statistical regularities 
of the generation of language [21]. In the context of the 
retrieval task, we can treat the generation of queries as 
a random process. Generally speaking, language mod- 
els for speech attempt to predict the probability of the 
next word in an ordered sequence. For the purposes of 
document retrieval, one can model occurrences at the 
document level without regard to sequential effects and 
will be the approach taken here. It is also possible to 
model local predictive effects for features such as phrases 
but that will be left for future work. Regarding query 
generation as a random process, it is not the case that 
queries really are generated randomly, but it is the case 
that retrieval systems are not endowed with knowledge 
of the generation process. Instead, we will treat language 
generation as a random process modeled by a probability 
distribution and focus on the estimation of probabilities 
as a means of achieving effective retrieval. 
Our approach to retrieval is to infer a language model 
for each document and to estimate the probability of gen- 
erating the query according to each of these models. We 
then rank the documents according to these probabili- 
ties. This means that our data model and our discrim- 
inant function for retrieval are one and the same. The 
intuition behind our approach is that, in our view, users 
have a reasonable idea of terms that are likely to occur in 
documents of interest and will choose query terms that 
distinguish these documents from others in the collection, 
an intuition discussed in more detail in section 5. By fo- 
cusing on the query generation probability as opposed to 
the probability of relevance, our model does not require 
us to make a set of inferences for indexing and a separate 
set of inferences for retrieval. 
Most retrieval systems use term frequency, document 
frequency and document length statistics. Typically 
these are used to compute a tf.idf score with docu- 
ment length normalization. An example of this is the 
INQUERY ranking formula shown in section 4.3. 
In our approach, collection statistics such as term fro 
quency, document length and document frequency are 
integral parts of the language model and are not used 
heuristically as in many other approaches. For this rea- 
son, we do not use the standard tf and idf scores. In 
addition, length normalization is implicit in the calcula- 
tion of the probabilities and does not have to be done in 
an ad hoc manner. 
The remainder of the paper is organized as follows. 
In section 2 we review some existing retrieval models. 
Section 3 describes a language modeling approach that 
closely parallels the standard approach to IR. Section 4 
shows the effectiveness of this model empirically. Finally 
we offer concluding remarks and describe future direc- 
tions of this work. 
2 Previous Work 
As mentioned previously. the standard probabilistic in- 
dexing model is the Z-Poisson model. One of the assump- 
tions of the model was that a subset of terms occurring in 
a document would be useful for indexing. According to 
Harter (71, such words can be identified by their distribu- 
tion and thereby assigned as indexing terms. Documents 
were assumed to be of approximately equal length, a rea- 
sonable assumption for the data used in the initial studies 
[7]. This model is somewhat similar to ours if one views 
the probability of term assignment as analogous to the 
term generation probability. The two main differences 
are that we do not make distributional assumptions and 
we do not not distinguish a subset of specialty words or 
assume a preexisting classification of documents into elite 
and non-elite sets. 
Two well known probabilistic approaches to retrieval 
are the Robertson and Sparck Jones model [14] and the 
Croft and Harper model [3]. Both of these models esti- 
mate the probability of relevance of each document to the 
query. Our approach differs in that we do not focus on 
relevance except to the extent that the process of query 
production is correlated with it. 
An additional probabilistic model is that of Fuhr [4]. 
A notable feature of the Fuhr model is the integration of 
indexing and retrieval models. The main difference be- 
tween this approach and ours is that in the Fuhr model 
the collection statistics are used in a heuristic fashion in 
order to estimate the probabilities of assigning concepts 
to documents. In our approach, we are able to avoid US- 
ing heuristic methods since we are not inferring concepts 
from terms. 
Another recent probabilistic approach is the INQUERY 
inference network model by Turtle and Croft [19]. Simi- 
lar to the Fuhr model, Turtle and Croft integrate index- 
ing and retrieval by making inferences of concepts from 
features. Features include words, phrases and more com- 
plex structured features. Evidence from multiple feature 
sets and multiple queries can be combined by means of 
a Bayesian network in order to infer the probability that 
the information need of the user has been met. This 
distinction between information need and query is a no- 
table feature of this model. As previously noted, in our 
approach, we have shifted our emphasis from probability 
of relevance to probability of query production. We as- 
sume these are correlated but do not currently attempt 
to model that correlation explicitly. We will discuss this 
point further in section 5. 
In section 3 we will discuss our probability estimation 
procedure. One statistic that we will be using is the av- 
erage probability of term occurrence. A similar statistic 
was used by Kwok [9] for a different purpose. Kwok used 
276 
the unnormalized average tf to estimate the importance 
of a term with respect to the query. In our approach we 
use the average of tf normalized by document length in 
the estimation of the generation probability. 
Wong and Yao [20] proposed a model in which they 
represented documents according to a probability distri- 
bution. They then developed two separate approaches 
to retrieval, one based on utility theory and the other 
based on information theory. Regarding the probability 
distribution, Wong and Yao use a maximum likelihood 
estimator for term probabilities. In our approach, we 
use a more robust estimator. Wong and Yao’s utility 
and information theoretic retrieval models are somewhat 
analogous to other approaches to retrieval in that they 
have an indexing model apart from their retrieval model. 
Terms are associated with documents according to the 
maximum likelihood probability estimate and the dis- 
criminant is a utility theoretic or information theoretic 
function of this estimate. In our approach we have been 
able to avoid this extra complexity and perform retrieval 
according to a single probabilistic model. 
The most similar approach to the one we have taken is 
that of Kalt [8]. In this model, documents are assumed 
to be generated by a stochastic process; a multinomial 
model. The task Kalt investigated was text classification. 
Each document was treated as a sample from a language 
model representing the class of that document. In this 
model, tf and document length are both integral parts 
of the model rather than being heuristics as they are in 
many other models. The discriminant function is taken 
to be the maximum likelihood estimator of the query 
given the document’s language model. Note that the 
‘query’ in this case was inferred from the training set in 
the context of the classification task. 
While our approach is conceptually somewhat differ- 
ent, it is clearly related to the Kalt approach and shares 
the desirable property that the collection statistics are in- 
tegral parts of the model, In our initial study we have de- 
liberately not made any distributional assumptions. An- 
other major difference from the Kalt approach is that 
we do not rely on the maximum likelihood estimator but 
instead use a more robust estimator. Next, we do not 
assume that documents were necessarily drawn from I; 
language models representing the k classes of interest. 
Instead, we make a weaker assumption that we can get 
estimates of each document’s language model individu- 
ally without making inferences about the class member- 
ship of documents. We then use these models to compute 
the query generation probability. We now describe the 
development of our approach. 
3 Model Description 
As mentioned, we infer a language model for each doc- 
ument and rank according to our estimate of producing 
the query according to that model. We would like to es- 
timate $(Q]Md), the probability of the query given the 
language model of document d. 
The maximum likelihood estimate of the probability 
of term t under the term distribution for document d is: 
where tf~~,~) is the raw term frequency of term t in 
document d and d&j is the total number of tokens in doc- 
ument d. We assume that given a particular language 
model that the query terms occur independently. This 
gives rise to the ranking formula fl,,, &,l(t, d) for each 
document. There are several problems with this estima- 
tor. The most obvious practical problem is that we do 
not wish to assign a probability of zero to a document 
that is missing one or more of the query terms. In addi- 
tion to this practical consideration, from a probabilistic 
perspective, it is a somewhat radical assumption to infer 
that p(i!]Md) = 0. I.e., the fact that we have not seen it 
does not make it impossible. Instead, we make the a~- 
sumption that a non-occurring term is possible, but no 
more likely than what would be expected by chance in the 
collection. I.e., d, where cft is the raw count of term t 
in the collection%d cs is the raw collection size or the 
total number of tokens in the collection. This provides 
us with a more reasonable distribution and circumvents 
the practical problem. It should be noted that in homo- 
geneous databases, one may need to use a more careful 
estimate of the collection probability since, in some cases 
the absence of a very frequently occurring word (i.e. a 
word with the characteristics of a stopword) could con- 
ceivably contribute more to the score of a document in 
which it does not occur. This is not a problem in the 
collections we have studied as they are heterogeneous in 
nature and stopwords have been removed, but we plan 
to address this issue in the future to be sure that our 
approach will be immune to these pathological cases. 
The other problem with this estimator may be less 
obvious. If we could get an arbitrary sized sample of 
data from Md we could be reasonably confident in the 
maximum likelihood estimator. However we only have a 
document sized sample from that distribution. To cir- 
cumvent this problem we are going to need an estimate 
from a larger amount of data. That estimate is: 
where df, is the document frequency of t. In other 
words, the mean probability oft in documents containing 
it. This is a robust statistic in the sense that we have a 
lot more data from which to estimate it but it too has 
a problem. We cannot and are not assuming that every 
document containing t is drawn from the same language 
model and so there is some risk in using the mean to 
estimate fJ(i?]Md) furthermore if we used the mean by 
itself, there would be no distinction between documents 
with different term frequencies. 
In order to benefit from the robustness of this estima- 
tor and to minimize the risk we will model the risk for a 
term t in a document d using the geometric distribution 
[22] as follows: 
where 7, is the the mean term frequency of term t in 
documents where t occurs, i.e., pavg(t) x d&i. Another 
way to say this is that 7, is the term count one would get 
if the term occurred at the average rate. The intuition 
behind this formula is that as the tf gets further away 
from the normalized mean, the mean probability becomes 
riskier to use as an estimate. For a somewhat related use 
of the geometric distribution see [5]. 
Now we will use this risk function as a mixing param- 
eter in our calculation of fi(Q]Md), our estimate of the 
probability of producing the query for a given document 
model as follows: 
277 
Let, 
fi(tlMd) = 
p,,(t, d)(‘.‘-‘-) x p_,(t)%- 
5!L 
if tf(t,d) > 0 
otherwise 
c.5 
Then, 
XII 1.0 - $(tlkfd) 
tBQ 
In this formula, the first term is the probability of 
producing the terms in the query and the second term is 
the probability of not producing other terms. Notice the 
risk function, &,d and the background probability 2 
mentioned earlier. We compute this function for each 
candidate document and rank accordingly. Now we de- 
scribe our empirical results. 
4 Empirical Results 
4.1 Data 
We performed recall/precision experiments on two data 
sets. The first set was TREC topics 202-250 on TREC 
disks 2 and 3, the TREC 4 ad hoc task and the second was 
TREC topics 51-100 on TREC disk 3 using the concept 
fields. We chose these query sets because they are quite 
different from each other. The 51-100 concept fields are 
essentially lists of good terms while topics 202-250 are 
‘natural language’ queries consisting of one sentence each. 
4.2 Implementation 
We have implemented a research prototype retrieval en- 
gine known as Labrador to test our approach. This en- 
gine was originally implemented as a high throughput 
retrieval system in the context of our previous work on 
topic segmentation [13]. For these experiments, the sys- 
tem does tokenization, stopping and stemming in the 
usual way. We have implemented both standard tf.idf 
weighting as well our language modeling approach. 
4.3 Recall/Precision Experiments 
Table 1 shows the results for TREC topics 202-250 on 
TREC disks 2 and 3. In the figure we see the eleven point 
recall/precision results as well as the non-interpolated av- 
erage precision and precision figures for the top N doc- 
uments for several values of N. The first two columns 
compare the baseline result to our new approach. The 
baseline result was obtained using the INQUERY ranking 
formula which uses Robertson’s tf score and a standard 
idf score and is defined as follows: 
Ifbelt,d = 
tft.d 
tft.d + o.5 + 1.5 .,;:,4,, 
idf, = log( F)/ log(N + 1) 
t 
The third column reports the percent change. Col- 
umn four is of the form I/D where I is the count of 
-s-r 
&et.: 
Prec. 
0.00 
0.10 
0.20 
0.30 
0.40 
0.50 
0.60 
0.70 
0.80 
0.90 
1.00 
Avg: 
Prec. 
5 
10 
15 
20 
30 
100 
200 
500 
1000 
RPr 
tf.idf LM %chg 1 I/D 1 Sign 1 Wk. 
6501 6501 
3201 3364 +5.09 [ 36143 1 O.OOOO* 1 0.0002* 
0.7439 
0.4521 
0.3514 
0.2761 
0.2093 
0.1558 
0.1024 
0.0451 
0.0160 
0.0033 
0.0028 
0.1868 
0.4939 
0.4449 
0.3932 
0.3643 
0.3313 
0.2157 
0.1655 
0.1004 
0.0653 
0.2473 
0.7590 
0.4910 
0.4045 
0.3342 
0.2572 
0.2061 
0.1405 
0.0760 
0.0432 
0.0063 
0.0050 
0.2233 
+2.0 
+8.6 
+15.1 
+21.0 
+22.9 
+32.3 
+37.1 
+68.7 
+169.6 
+89.3 
+76.9 
+19.55 
0.4435 +12.8 
0.4051 +11.2 
0.3707 +11.9 
0.2500 +15.9 
0.1903 +15.0 
0.1119 +11.4 
0.0687 +5.1 
0.2876 1 +16.32 
10/22 
24/42 
27/44 
28/43 
25139 
24/35 
22/27 
13/15 
9/10 
213 
213 
32/49 
Tipi 
22130 
19126 
22134 
28141 
32/42 
35/44 
36/44 
36/43 
34/43 
0.7383 0.5709 
0.2204 0.0761 
0.0871 0.0081~ 
0.0330* 0.0054* 
0.0541 0.0158* 
0.0205* O.OOlS* 
O.OOOS* 0.0027* 
0.0037* 0.0062* 
0.0107* 0.0035* 
0.6682 0.4106 
O.OOSl* 0.0154* 
0.0145* 0.0038* 
0.0607 0.0218* 
0.0138* 0.0070* 
0.0005* 0.0003* 
0.0001* o.oooo* 
o.oooo* o.oooo* 
o.oooo* 0.0002* 
0.0001* o.oooo* 
Table 1: Comparison of tf.idf to the language modeling 
approach on TREC queries 202-250 on TREC disks 2 and 
3. 
queries for which performance improved using the new 
method and D is count of queries for which performance 
was different. Column five reports significance values ac- 
cording to the sign test and column six does likewise ac- 
cording to the Wilcoxon test. The entries in these two 
columns marked with a star indicate a statistically sig- 
nificant difference at the 0.05 level. Note that these are 
one sided tests. 
Notice that on the eleven point recall/precision sec- 
tion, the language modeling approach achieves better 
precision at all levels of recall, significantly at several lev- 
els. Also notice that there is a significant improvement in 
recall, uninterpolated average precision and R-precision, 
the precision after R documents where R is equal to the 
number of relevant documents for each query. On the sec- 
ond part of the figure there is again an improvement at 
all levels of recall, most of them statistically significant. 
The results for TREC queries 51-100 on TREC disk 3 
are shown in table 2. Once again we see improvement in 
precision at all levels of recall on the eleven point chart 
and we also see improvement in precision for each level 
of recall by document count. Several levels show a sig- 
nificant improvement. 
4.4 Improving the Basic Model 
We now try to improve our probability estimates since, 
according to our model, this should yield better retrieval 
performance. A simple improvement of the estimate de- 
veloped in section 3 is to smooth the estimates of the av- 
erage probability for terms with low document frequency. 
The estimate of the average probability of these terms is 
based on a small amount of data and so could be sensitive 
to outliers. 
In order to correct for this, we binned the low fre- 
quency data by document frequency and used the binned 
estimate for the average. We used a cutoff of u’f = 100 
for low frequency terms, though it turned out that the 
cutoff is not critical. 
278 
tf.idf 1 LM %chg 1 I/D 1 Sign 1 Wk. 
10485 1 10485 
5818 1 6105 +4.93 [ 32142 1 0.0005* 1 0.0003* 
Rel: 
Rret.: 
Prec. 
0.00 
0.10 
0.20 
0.30 
0.40 
0.50 
0.60 
0.70 
0.80 
0.90 
1.00 
Avg: 
Prec. 
5 
10 
15 
20 
30 
100 
200 
500 
1000 
RPr 
+7.3 
+2.9 
+4.9 
+8.2 
+8.4 
+16.2 
+15.2 
+21.5 
+3.7 
-14.9 
-11.9 
+8.74 
+12.0 
+3.5 
+2.4 
+4.7 
+7.0 
+6.5 
+6.8 
+4.7 
$4.9 
+6.24 
24/45 
28/47 
25145 
26140 
20/30 
14/22 
8/13 
l/4 
l/2 
32/50 
0.7383 
0.1456 
0.3830 
0.1215 
0.2757 
0.0403* 
0.0494* 
0.1431 
0.2905 
0.3125 
0.7500 
0.0325* 
0.2961 
0.1017 
0.1405 
0.0277* 
0.0286* 
0.0007* 
0.0025* 
0.0288* 
0.2108 
undef 
undef 
0.0015* 
iFpi 0.0392* 0.0125* 
14/30 0.7077 0.1938 
14/28 0.5747 0.3002 
16/34 0.6962 0.1260 
20/32 0.1077 0.0095* 
29145 0.0362* 0.0076* 
29144 0.0244* 0.0009* 
30142 0.0040* 0.0011* 
32142 0.0005* 0.0003* 
30143 0.0069* 0.0052* 
LM LM2 %chg 1 I/D I Sign I Wik. 
6501 6501 
3364 3350 -0.42 I 16133 I 0.5000 I 0.4432 
I 
I 
Rel: 
Rret : 
Prec. 
0.00 
0.10 
0.20 
0.30 
0.40 
0.50 
0.60 
0.70 
0.80 
0.90 
1.00 
Avg: 
Prec. 
5 
10 
15 
20 
30 
100 
200 
500 
1000 
RPr 
0.7590 0.7717 +1.7 
0.4910 0.5115 +4.2 
0.4045 0.4137 +2.3 
0.3342 0.3539 +5.9 
0.2572 0.2709 +5.3 
0.2061 0.2164 +5.0 
0.1405 0.1405 -0.0 
0.0760 0.0724 -4.8 
0.0432 0.0450 +4.1 
0.0063 0.0065 +4.6 
0.0050 0.0040 -19.1 
0.2233 0.2318 +3.81 
11/17 
25141 
23/42 
26142 
23137 
23133 
15124 
4114 
5/9 
213 
2/3 
34/49 
0.1662 
0.1055 
0.3220 
0.0821 
0.0939 
0.0175* 
0.9242 
0.0898 
0.5000 
0.5000 
0.8750 
0.0047* 
0.1137 
0.0194* 
0.2100 
0.0275* 
0.0420* 
0.0222* 
0.8197 
0.0886 
undef 
undef 
undef 
0.0055* 
0.5020 0.5469 +8.9 
0.4898 0.5082 f3.7 
0.4435 0.4571 +3.1 
0.4051 0.4235 +4.5 
0.3707 0.3755 +1.3 
0.2500 0.2655 +6.2 
0.1903 0.1932 +1.5 
0.1119 0.1128 +0.8 
0.0687 0.0684 -0.4 
0.2876 0.2928 +1.79 
151/17 
12122 
14/23 
18/25 
16134 
28/39 
18130 
21137 
16133 
19134 
1: able 3: Comparison 01 E the I original language model- 
ing approach to the new language modeling approach on 
TREC queries 202-250 on TREC disks 2 and 3. 
0.7274 
0.4861 
0.3898 
0.3352 
0.2826 
0.2163 
0.1561 
0.0913 
0.0510 
0.0179 
0.0005 
0.2286 
0 
0:5080 
0.4933 
0.4670 
0.4293 
0.3344 
0.2670 
0.1797 
0.1164 
0.2836 
0.7805 
0.5002 
0.4088 
0.3626 
0.3064 
0.2512 
0.1798 
0.1109 
0.0529 
0.0152 
0.0004 
0.2486 
0.5960 
0.5260 
0.5053 
0.4890 
0.4593 
0.3562 
0.2852 
0.1881 
0.1221 
0.3013 
Table 2: Comparison of tf.idf to the language modeling 
approach on TREC queries 51-100 on TREC disk 3. 
This new estimate of the average is incorporated into 
our ranking formula as before and rerun on TREC queries 
202-250 against TREC disks 2 and 3 and the results are 
shown in table 3. 
The results show a statistically significant improve- 
ment in precision at several levels of recall. The aver- 
age precision is also improved. Running the new model 
on our second query set, TREC queries 51-100 against 
TREC disk 3, we get the result shown in table 4. 
Again we see significant improvements, albeit modest 
ones, at several levels of recall and on average. Our con- 
jecture is that that smaller improvement on this query set 
is due to the longer average query length as compared to 
the other query set. It appears that for low frequency 
terms, the effects on the average due to outliers is just 
as likely to overestimate as it is to underestimate and so 
these effects cancel each other out with more terms in the 
query. However, this is only a conjecture, the verification 
of which we leave for future work. 
5 Conclusions and Future Work 
We have presented a novel way of looking at the problem 
of text retrieval based on probabilistic language modeling 
that is both conceptually simple and explanatory. 
We feel that our model will provide effective retrieval 
and can be improved to the extent that the following 
conditions can be met: 
1. Our language models are accurate representations 
of the data. 
2. Users understand our approach to retrieval. 
3. Users have a some sense of term distribution. 
We feel that condition one has been met reasonably 
well by the approach we have taken in this study. How- 
ever, we also feel that our models can and should be im- 
proved. Our current language models do not incorporate 
any knowledge of the language generation process. It is 
D 
Rel: 
Rret.: 
Prec. 
0.00 
0.10 
0.20 
0.30 
0.40 
0.50 
0.60 
0.70 
0.80 
0.90 
1.00 
Avg: 
Prec. 
5 
10 
15 
20 
30 
100 
200 
500 
1000 
RPr 
LM LM2 %chg I I/D 1 Sign I Wk. 
10485 10485 
6105 6107 +0.03 I 3/5 1 0.5000 1 undef 
+o.o 
+0.7 
+0.1 
+0.2 
+0.4 
-0.3 
-1.2 
+0.3 
+0.1 
+0.9 
+0.4 
i-O.08 
3Js 
16/20 
16/25 
13/18 
16/24 
11/25 
10/20 
9112 
5/8 
l/2 
l/l 
28139 
+0.7 
+o.o 
+0.8 
+0.6 
+0.4 
+0.2 
+0.2 
+0.1 
+o.o 
-0.08 
-v 
O/O 
3/3 
4/5 
518 
517 
8111 
619 
315 
8/12 
I 
I 
0.5000 
0.0059* 
0.1148 
0.0481* 
0.0758 
0.3450 
0.5881 
0.0730 
0.3633 
0.7500 
0.5000 
0.0047* 
0.5000 
1.0000 
0.1250 
0.1875 
0.3633 
0.2266 
0.1133 
0.2539 
0.5000 
0.9270 
0.7807 
0.5038 
0.4093 
0.3634 
0.3077 
0.2505 
0.1777 
0.1113 
0.0530 
9.0154 
1.0004 
1.2488 
0.6000 
0.5260 
0.5093 
0.4920 
0.4613 
0.3568 
0.2859 
0.1884 
0.1221 
0.3011 
0.7805 
0.5002 
0.4088 
0.3626 
0.3064 
0.2512 
0.1798 
0.1109 
0.0529 
0.0152 
0.0004 
0.2486 
0.5960 
0.5260 
0.5053 
0.4890 
0.4593 
0.3562 
0.2852 
0.1881 
0.1221 
0.3013 
Table 4: Comparison of the original language model- 
ing approach to the new language modeling approach on 
TREC queries 51-100 on TREC disk 3. 
279 
possible that additional knowledge added to the models 
will yield better estimates. 
Regarding point two, we feel that our model is sim- 
ple enough to be explained to users at an intuitive level 
and that the understanding of it will facilitate the for- 
mation of better queries. It is not that users will need 
or want to know the details of the model but it is more 
the case that if users have a general understanding of 
how the system works, they will be able to use it more 
effectively. Users are typically instructed to pose natu- 
ral language descriptions of their information needs as 
queries. A user that understands our model will tend to 
think in terms of which words will help the system distin- 
guish the documents of interest from everything else. We 
feel that if we can get users to think in this manner they 
will be able to formulate queries that will better express 
their information needs in manner useful to the retrieval 
system. 
Regarding point three, in order for users to identify 
useful words, we feel that they would benefit from a sense 
of how the words are distributed in the collection. Again, 
it is not the case that users need or want to know the 
term distribution in detail, but a sense of which terms 
are more likely to be useful would be beneficial. We can 
imagine a variety of both textual and graphical tools to 
help users get a better sense of the distribution of terms. 
This especially important to win over expert users who 
often prefer boolean retrieval because they like the sense 
of control over the search [2]. 
Regarding the results of our study, performance on 
two different query sets was better than that obtained 
by tf.idf weighting. However, the improvement in per- 
formance is not the main point. More significant is that 
a different approach to retrieval has been shown to be ef- 
fective. The ability to think about retrieval in a new way 
can lead to insights that would be less obvious in other 
approaches. Of course, the converse is is also true, and so 
rather than viewing our approach as a competing model, 
we view it as a one of a number of tools for investigating 
retrieval. 
Our second set of experiments showed that using 
simple smoothing yields results significantly better than 
baseline on both query sets. This is an example of an 
insight gained from our approach that is not an obvious 
consequence of other approaches. It is also possible that 
a more elaborate smoothing technique or perhaps other 
techniques such as data transformation would improve 
results further. We plan to investigate these matters in 
the future. 
We also need to address the estimate of default prob- 
ability. As mentioned, our current estimator could in 
some strange cases assign a higher probability to a non- 
occurring query term. This could only happen in cases 
of very commonly occuring terms, i.e., terms which are 
not likely to be useful, however, we feel we should ad- 
dress this problem in order to insure the robustness of 
our model in such cases. Our approach will be to apply a 
smoothing based estimator that will guarantee a default 
probability estimate that cannot exceed the lowest esti- 
mate that we assign to a document in which a given term 
occurs. 
Finally, the generative language model seems to be an 
intuitive way to think about query expansion techniques 
such as relevance feedback or local feedback. We intend 
to derive these techniques from our model rather than 
attempting to explain existing techniques. 
6 Acknowledgments 
The authors would like to thank Warren GreiII for his 
comments on several aspects of this work and for numer- 
ous useful comments on an early draft. Thanks to Ron 
Papka for his remarks on an early draft of this paper. 
Also, thanks to Tom Kalt for several useful discussions. 
This material is based on work supported in part by 
the National Science Foundation, Library of Congress 
and Department of Commerce under cooperative agree- 
ment number EEC-9209623. This material is also based 
on work supported in part by United States Patent 
and Trademark Office and Defense Advanced Research 
Projects Agency/IT0 under ARPA order number D468, 
issued by ESC/AXS contract number F19628-95-C-0235. 
Any opinions, findings and conclusions or recommenda- 
tions expressed in this material are the authors and do 
not necessarily reflect those of the sponsors. 
References 
PI 
PI 
[31 
141 
[51 
PI 
(71 
PI 
PI 
PO1 
WI 
Bookstein, A. and D. Swanson. “Probabilistic mod- 
els for automatic indexing” Journal for the Amer- 
ican Society for Information Science. v.25 no.5 pp. 
312-318, 1976. 
Byrd, D. Personal Communication. 1998. 
Croft, W. B. and D. J. Harper. “Using probabilis- 
tic models of document retrieval without relevance 
information.” Journal of Documentation, 35, 1979 
(pp. 285-295). 
Fuhr, N. “Models for Retrieval with Probabilistic 
Indexing” Information Processing and Management. 
v. 25, no. 1 1989. 
Ghosh, M. J., T. Hwang and K. W. Tsui. ‘Construc- 
tion of Improved Estimators in Multiparameter Es- 
timation for Discrete Exponential Families.” Annals 
of Statistics, v. 11, 351-367. 
Greenwood, M. and G. U. Yule. “An Inquiry into the 
Nature of Frequency Distribution Representative of 
Multiple Happenings with Particular Reference to 
the Occurrence of Multiple Attacks of Disease or of 
Repeated Accidents.” Journal of the Royal Statisti- 
cal Society. v. 83, pp. 255-279, 1920. 
Harter, S. P. “A Probabilistic Approach to Auto- 
matic Keyword Indexing” Journal of the American 
Society for Information Science, July-August, 1975. 
Kalt, T. “A New Probabilistic Model of Text Clas- 
sification and Retrieval”, CIIR Tech. Report No. 78, 
1996. 
Kwok, K. L. “A New Method of Weighting Query 
Terms for Ad-Hoc Retrieval”, In proceedings of 
ACM SIGIR 1996, pp 187-195. 
Margulis, E. L. “Modeling documents with multiple 
Poisson distributions.” Information Processing and 
Management v. 29 no. 2 pp. 215-227, 1993. 
Parzen, E. “On estimation of a probability density 
function and mode.” Annals of Mathematical Statis- 
tics, vol. 33, 1962. 
280 
[12] Robertson, S. E. and S. Walker. Some simple effec- 
tive approximations to the 2-Poisson model for prob- 
abilistic weighted retrieval. In proceedings of ACM 
SIGIR 1994. pp. 232-241. 
[13] Ponte, J. M. and W. B. Croft. “Text Segmentation 
by Topic,” in Proceedings of the First European 
Conference on Research and Advanced Technology 
for Digital Libraries, 1997. 
[14] Robertson, S. E. and K. Sparck Jones. “Relevance 
Weighting Of Search Terms,” Journal of the Amer- 
ican Society for Information Science, vol. 27, 1977. 
[15] Salton, G. Automatic Tezt Processing. Addison 
Wesley, 1989. 
[16] Silverman, B. W. Density Estimation for Statistics 
and Data Analysis Chapman and Hall, 1986. 
[17] Terrell, G. R. and D. W. Scott. “Oversmoothed 
Nonparametric Density Estimators” Journal of the 
American Statistical Association. Vol. 3, Number 
389, 1985. 
[18] Titterington, D. M., U. E. Makov and A. F. M 
Smith. Statistical Analysis of Finite Mixture Dis- 
tributions John Wiley and Sons, 1985. 
[19] Turtle H. and W. B. Croft. “Efficient Probabilistic 
Inference for Text Retrieval,” Proceedings of RIAO 
3, 1991. 
[20] Wong, S. K. M. and Y. Y. Yao. “A Probability Dis- 
tribution Model for Information Retrieval” Informa- 
tion Processing and Management, v. 25 no. 1 pp. 
39-53, 1989. 
[21] Yamron, J. “Topic Detection and Tracking Segmen- 
tation Task” In proceedings of The Topic Detection 
and Tracking Workshop, Oct. 1997. 
(221 Zelen, M. and N. Severo. “Probability Func- 
tions” Handbook of Mathematical Functions. M. 
Abramowitz and I. A. Stegun ed. National Bureau of 
Standards Applied Mathematics Series No. 55. 1964. 
281 

