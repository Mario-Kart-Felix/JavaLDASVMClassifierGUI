Image classification using local tensor singular
value decompositions
Elizabeth Newman
Department of Mathematics
Tufts University
Medford, Massachusetts 02155
Email: e.newman@tufts.edu
Misha Kilmer
Department of Mathematics
Tufts University
Medford, Massachusetts 02155
Email: misha.kilmer@tufts.edu
Lior Horesh
IBM TJ Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY
Email: lhoresh@us.ibm.com
Abstract—From linear classifiers to neural networks, image
classification has been a widely explored topic in mathematics,
and many algorithms have proven to be effective classifiers.
However, the most accurate classifiers typically have significantly
high storage costs, or require complicated procedures that may
be computationally expensive. We present a novel (nonlinear)
classification approach using truncation of local tensor singular
value decompositions (tSVD) that robustly offers accurate results,
while maintaining manageable storage costs. Our approach takes
advantage of the optimality of the representation under the tensor
algebra described to determine to which class an image belongs.
We extend our approach to a method that can determine specific
pairwise match scores, which could be useful in, for example,
object recognition problems where pose/position are different. We
demonstrate the promise of our new techniques on the MNIST
data set.
I. INTRODUCTION
Image classification is a well-explored problem in which an
image is identified as belonging to one of a known number
of classes. Researchers seek to extract particular features
from which to determine patterns comprising an image. Algo-
rithms to determine these essential features include statistical
methods such as centroid-based clustering, connectivity/graph-
based clustering, distribution-based clustering, and density-
based clustering [13], [14], [15], as well as learning algorithms
(linear discriminant analysis, support vector machines, neural
networks) [5].
Our approach differs significantly from techniques in the
literature in that it uses local tensor singular value decompo-
sitions (tSVD) to form the feature space of an image. Tensor
approaches are gaining increasing popularity for tasks such as
image recognition and dictionary learning and reconstruction
[3], [9], [7], [10]. These are favored over matrix-vector-based
approaches as it has been demonstrated that a tensor-based
approach enables retention of the original image structural
correlations that are lost by image vectorization. Tensor ap-
proaches for image classification appear to be in their infancy,
although some approaches based on the tensor HOSVD [11]
have been explored in the literature [6].
Here, we are motivated by the work in [3] which em-
ploys optimal low tubal-rank tensor factorizations through
use of the t-product [1] and by the work in [2] describing
tensor orthogonal projections. We present a new approach
for classification based on the tensor SVD from [1], called
the tSVD, which is elegant for its straightforward mathe-
matical interpretation and implementation, and which has the
advantage that it can be easily parallelized for great com-
putational advantage. State-of-the-art matrix decompositions
are asymptotically challenged in dealing with the demand to
process ever-growing datasets of larger and more complex
objects [16], so the importance of this dimension of this study
cannot be overstated. Our method is in direct contrast to deep
neural network based approaches which require many layers
of complexity and for which theoretical interpretation is not
readily available [17]. Our approach is also different from
the tensor approach in [6] because truncating the tSVD has
optimality properties that truncating the HOSVD does not
enjoy. We conclude this study with a demonstration on the
MNIST [4] dataset.
A. Notation and Preliminaries
In this paper, a tensor is a third-order tensor, or three-
dimensional array of data, denoted by a capital script letter.
As depicted in Figure 1, A is an ` ×m × n tensor. Frontal
slices A(k) for k = 1, . . . , n are `×m matrices. Lateral slices
~Aj for j = 1, . . . ,m are ` × n matrices oriented along the
third dimension. Tubes aij for i = 1, . . . , ` and j = 1, . . . ,m
are n × 1 column vectors oriented along the third dimension
[2].
(a) Tensor A. (b) Frontal
slices A(k).
(c) Lateral
slices ~Aj .
(d) Tubes aij .
Fig. 1. Representations of third-order tensors.
To paraphrase the definition by Kilmer et al. [2], the range
of a tensor A is the t-linear span of the lateral slices of A:
R(A) = { ~A1 ∗ c1 + · · ·+ ~Am ∗ cm | ci ∈ R1×1×n}. (1)
ar
X
iv
:1
70
6.
09
69
3v
1 
 [
st
at
.M
L
] 
 2
9 
Ju
n 
20
17
Because the lateral slices of ~A form the range, we store our
images as lateral slices. Furthermore, A is real-valued because
images are real-value.
To multiply a pair of tensors, we need to understand the
t-product, which requires the following tensor reshaping ma-
chinery. Given A ∈ R`×m×n, the unfold function reshapes
A into an `n × m block-column vector (ie. the first block-
column of (2)), while fold folds it back up again. The bcirc
function forms an `n × mn block-circulant matrix from the
frontal slices of A:
bcirc(A) =

A(1) A(n) . . . A(2)
A(2) A(1) . . . A(3)
...
...
. . .
...
A(n) A(n−1) . . . A(1)
 . (2)
Now the t-product is defined as follows ([1]):
Definition 1 (t-product): Given A ∈ R`×p×n and B ∈
Rp×m×n, the t-product is the `×m× n product
A ∗ B = fold(bcirc(A) · unfold(B)). (3)
Under the t-product (Definition 1), we need the following
from [1].
Definition 2: The tensor transpose AT ∈ Rp×`×n takes the
transpose of the frontal slices of A and reverses the order of
slices 2 through n.
Definition 3: The identity tensor J is an m×m×n tensor
where J (1) an m × m identity matrix and all other frontal
slices are zero.
Definition 4: An orthogonal tensor Q is an m × m × n
tensor such that QT ∗ Q = Q ∗ QT = J .
Analogous to the columns of an orthogonal matrix, the lateral
slices of Q are orthonormal [2].
Definition 5: A tensor is f-diagonal if each frontal slice is
a diagonal matrix.
II. TENSOR SINGULAR VALUE DECOMPOSITION
Let A be an `×m×n tensor. As defined in [1], the tensor
singular value decomposition (tSVD) of A is the following:
A = U ∗ S ∗ VT , (4)
where for p = min(`,m), U is an ` × p × n tensor with
orthonormal lateral slices, V is a m × p × n tensor with
orthonormal lateral slices, and S is p× p× n f-diagonal.
The algorithm for computing the tSVD is given in [1].
Importantly, as noted in that paper, the bulk of the com-
putations are performed on matrices, which are independent
and can thus be done in parallel. Furthermore, synonymously
to matrix computation strategies, randomized variants of the
tSVD algorithm have recently been proposed [12] which can
be favored when the tensor is particularly large.
A. Range and Tubal-Rank of Tensors
As proven in Kilmer et al. [2], the range of A determined
via t-linear combinations of the lateral slices of U , for appro-
priate tensor coefficients ci:
R(A) = {~U1 ∗ c1 + · · ·+ ~Up ∗ cp | ci ∈ R1×1×n}. (5)
The lateral slices of U form an orthonormal basis for the range
of A. More details related to the definition and the rest of the
linear-algebraic framework can be found in [2].
The definition of the range of a tensor leads to the notion of
projection. Given a lateral slice ~B ∈ Rm×1×n, the orthogonal
projection into the range of A is defined as U ∗ UT ∗ ~B.
We require the following theorem to understand tubal-rank
of tensors:
Theorem 1 ( [1]): For k ≤ min(`,m), define
Ak =
k∑
i=1
~Ui ∗ sii ∗ ~VTi .
where ~Ui and ~Vi are the ith lateral slices of U and V ,
respectively, and sii is the (i, i)-tube of S. Then Ak =
argminÃ∈M ||A − Ã||F where M = {C = X ∗ Y | X ∈
R`×k×n,Y ∈ Rk×m×n}.
From Theorem 1, we say Ak is a tensor of tubal-rank-k.
The definition of tubal rank is from [2]. It follows from the
above that Ak is best tubal-rank-k approximation to A.
B. The Algorithm
Suppose we have a set of training images and each image
in the set belongs to one of N different classes. First, we form
a third-order tensor1 for each class A1,A2, . . . ,AN where Ai
contains all the training images belonging to class i, stored as
lateral slices. We assume all the training images are `×n and
that there are mi images in class i; i.e., Ai is an `×mi × n
tensor. Note that the mi need not be the same. We then form
a tubal-rank-k local tSVD (Theorem 1) for each tensor:
Ai ≈ Ui ∗ Si ∗ VTi for i = 1, . . . , N, (6)
where Ui is an `×k×n tensor. Here, k  mi. Now, instead of
storing all the training images, we need only store an `×k×n
tensor for each class. The training basis is thus an optimal
basis in the sense of Theorem 1. The tensor operator Ui ∗ UTi
is an orthogonal projection tensor [2] onto the space which is
the t-linear combination of the lateral slices of the Ui tensor.
Likewise, (I − Ui ∗ UTi ) projects orthogonally to this space.
Next, suppose a test image belongs to one of the N classes
and we want to determine the class to which it belongs. We
re-orient this image as a lateral slice ~B and use our local tSVD
bases to compute the norms of the tensor coefficients of the
image projected orthogonally to the current training set:
arg min
i=1,...,N
|| ~B − Ui ∗ UTi ∗ ~B||F , for i = 1, . . . , N. (7)
1 We note that extensions of the t-product and corresponding decomposi-
tions are possible for higher order tensor representations (e.g. for color image
training data), as well [18], [19].
If ~B is a member of the class i, we expect (7) to be small. We
determine the class to which ~B belongs by which projection
is the closest to the original image in the Frobenius norm.
III. EXPERIMENTS AND RESULTS
To test our local tSVD classifier, we use the public MNIST
dataset of handwritten digits as a benchmark [4]. The MNIST
dataset contains of 60, 000 training images and 10, 000 test
images. Each image is a 28 × 28 grayscale image consisting
of a single hand-written digit (i.e., 0 through 9). We organize
the training images by digit resulting in 10 different classes
with the distribution of digits displayed in Figure 2.
Fig. 2. Table of MNIST digit distribution.
Digit 0 1 2 3 4
# training 5923 6742 5958 6131 5842
# test 980 1135 1032 1010 982
Digit 5 6 7 8 9
# training 5421 5918 6265 5851 5949
# test 892 958 1028 974 1009
We store each class of training images as a tensor with
the images stored as lateral slices (e.g., the tensor containing
images of the digit 0 is of size 28 × 5923 × 28). Using (6),
we independently form a local tSVD basis for each class
U0,U1, . . . ,U9 where Ui is the basis for the digit i and of
size 28×k×28 for some truncation k. For simplicity, we use
the same truncation for all bases 2.
A. Numerical Results: Classification
Our first objective is to use these local tSVD bases to
determine the digit in each test image. Suppose ~Pj is the
28× 1× 28 lateral slice of the jth test image. We determine
how similar ~Pj is to each digit using the following metric (7):
arg min
i=0,...,9
||~Pj − Ui ∗ UTi ∗ ~Pj ||F . (8)
To measure the accuracy of our classification, we compute
the recognition rate for the entire test data as follows:
r =
# of correctly classified test images
# of test images
. (9)
For various truncation values k, we obtain the following
recognition rates:
Fig. 3. Classification accuracy for various truncation values.
Truncation k = 3 k = 4 k = 5 k = 10
r (%) 87.99 88.51 87.14 75.31
From Figure 3, we notice that smaller truncation values
yield greater classification accuracy. This indicates that the
magnitude of the tubes of singular values in S (i.e., ||sii||F )
2 Note that the tSVD offers flexibility in prescription of the truncation level
per basis [3].
decays rapidly for the early truncation values, as demonstrated
in Figure 4.
Fig. 4. Magnitude decay of norm of singular value tubes for digits 0-4.
Notice in Figure 4 the magnitude of the tubes of S decays
rapidly for the first few indices i and decays more slowly
starting at the index i = 5. This implies we can optimize
our storage costs by truncating at about k = 5 without losing
significant classification accuracy.
In addition to the overall classification accuracy, we can
measure the accuracy of classifying each digit as
ri =
# of correctly classified test images of digit i
# of test images of digit i
. (10)
We show the per-digit accuracy results for k = 4 below:
Fig. 5. Classification accuracy per digit for truncation k = 4.
Digit Most Freq. 2nd Most ri (%)
0 0 1 91.12
1 1 4 96.56
2 2 0 83.92
3 3 8 82.77
4 4 1 96.13
5 5 8 79.48
6 6 1 93.32
7 7 9 90.95
8 8 5 82.14
9 9 4 87.02
In Figure 5, the “Most Freq.” column indicates the class to
which the images of each digit were most frequently classified.
The “2nd Most” column indicates the second class to which
the images of each digit were most frequently classified.
We illustrate some of the mis-classifications that occur in
Figure 6 for truncation k = 4.
(a) Incorrectly classified as 9. (b) Incorrectly classified as 2.
Fig. 6. Examples of incorrect classification of images that should be 7.
We notice that Figure 6a and 6b do have qualitative sim-
ilarities to 9 and 2, respectively. We can likely improve for
ambiguous digits by adding additional features for each class
and/or employing slightly different metrics.
B. Numerical Results: Identification
Our second objective is to use our local tSVD feature
vectors to determine if a pair of test images contain the same
digit. To solve this problem, we consider each comparison (8)
to be a feature for a particular image ~Pj instead of minimizing
over the number of classes. More specifically, we construct a
1× 10 vector of features for each of our 10,000 test images.
We measure the similarity between two images by comput-
ing the cosine between the feature vectors. Though other sim-
ilarity metrics are possible, given what all the (non-negative)
entries in the feature vector represent, this seemed appropriate
for proof of concept.
We compute the similarity for all (i, j)-pairs of test images
to form a similarity score matrix S of size 10000 × 10000
where S is symmetric.
Fig. 7. Similarity score matrix for truncation k = 4.
In Figure 7, we display only the similarity scores between
0.98 and 1 and we notice that blocks along the diagonal
contain the highest similarity scores, as desired given the
ordering of the test data. This illustrates that the cosine metric
does enable us to determine if two images contain the same
digit.
Fig. 8. ROC curve for various truncation values k.
Using a receiver operating characteristic (ROC) curve in
Figure 8, we visualize the effectiveness of our local tSVD
classifier. Notice the curve for truncation k = 10 is signifi-
cantly lower, indicating smaller truncation values (indicative
of less storage) yield better accuracy for the MNIST dataset.
IV. CONCLUSIONS AND FUTURE WORK
We have developed a new local truncated tSVD approach
to image classification based on provable optimality condi-
tions which is elegant in its straightforward mathematical
approach to the problem. Beyond the innate computational
and storage efficiency advantages of the proposed approach, it
has demonstrated effective performance in classifying MNIST
data. The primary purpose of this short paper was a proof
of concept of a new method. In the future, we will compare
our approach to current state-of-the-art approaches in terms of
storage, computation time and qualitative classification results
for larger and different datasets (e.g. subjects from a dataset of
facial images). Additionally, we seek an automated strategy for
determining optimal truncation value k or a varied truncation
scheme denoted tSVDII as in [3]. We will also explore
whether the alternative tensor-tensor products from [8] and
their corresponding truncated tSVDs will allow us to obtain
more illustrative features, and whether new double-sided tSVD
techniques [20] that are insensitive to tensor orientation are
useful here as well.
ACKNOWLEDGMENT
This research is partially based upon work supported by
the National Science Foundation under NSF 1319653 and by
the Office of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity (IARPA),
via IARPAs 2014-14071600011. The views and conclusions
contained herein are those of the authors and should not be
interpreted as necessarily representing the official policies or
endorsements, either expressed or implied, of ODNI, IARPA,
or the U.S. Government. The U.S. Government is authorized
to reproduce and distribute reprints for Governmental purpose
notwithstanding any copyright annotation thereon.
REFERENCES
[1] M. E. Kilmer and C. D. Martin. Factorization strategies for third-order
tensors. Linear Algebra and its Applications, 435, 2011, 641-658.
[2] M. E. Kilmer, K. Braman, N. Hao, and R. C. Hoover. Third- order tensors
as operators on matrices: A theoretical and computational framework with
applications in imaging. SIAM J. Matrix Anal. Appl., 2015, 148-172.
[3] N. Hao, M. E. Kilmer, K. Braman, and R. C. Hoover. Facial recognition
using tensor-tensor decompositions. SIAM J. Imag. Sci., 2013, 437-463.
[4] Y. Lecun, C. Cortes, and C. Curges. The MNIST Database.
http://yann.lecun.com/exdb/mnist/.
[5] S. Theodoridis and K. Koutroumbas. Pattern Recognition, 3rd Edition.
Academic Press, 2006.
[6] B. Savas and L. Elden. Handwritten digit classification using higher order
singular value decomposition, Pattern Recognition, 40, 2007, 993-1003.
[7] M. A. O. Vasilescu and D. Terzopoulos, Multilinear subspace analysis of
image ensembles. In Proceedings of the 2003 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition, 2003, 9399.
[8] E. Kernfeld, M. E. Kilmer, S. Aeron. Tensor-tensor products with invert-
ible linear transforms. Linear Algebra Appl., 485, 2015, 545-570.
[9] E. Kernfeld, N. Majumder, S. Aeron, M. Kilmer. Multilinear sub-
space clustering. IEEE Statistical Signal Processing, 2016, DOI:
10.1109/SSP.2016.7551817
[10] S. Soltani, M. E. Kilmer, P. C. Hansen. A tensor-based dictionary
learning approach to tomographic image reconstruction. BIT Numer
Math, 2016, 1425-1454.
[11] L. De Lathauwer, B. De Moor, J. Vandewalle. A multilinear singular
value decomposition. SIAM J. Matrix Anal. Appl. 21, 2000, 12531278.
[12] J. Zhang, A. Saibab, M. E. Kilmer, S. Aeron. A randomized tensor
singular value decomposition. arXiv: 1609.07086. Submitted.
[13] Rokach, Lior, and Oded Maimon. Clustering methods. Data mining and
knowledge discovery handbook. Springer US, 2005. 321-352.
[14] Lloyd, S. (1982). Least squares quantization in PCM. IEEE Transactions
on Information Theory. 28 (2): 129137. doi:10.1109/TIT.1982.1056489.
[15] M. Ester, H-P. Kriegel, J. Sander, X. Xu. A density-based algorithm for
discovering clusters in large spatial databases with noise. In E. Simoudis,
J. Han, U. M. Fayyad, (KDD-96). AAAI Press. 226-231.
[16] C. Boutsidis, D. P. Woodruff, P. Zhong. Optimal Principal Component
Analysis in Distributed and Streaming Models, STOC2016, 2016.
[17] S. Shalev-Shwartz, O. Shamir, S. Shammah, Failures of Gradient-Based
Deep Learning, arXiv:1703.07950, 2017.
[18] N. Hao, L. Horesh, M. E. Kilmer, Nonnegative tensor decomposition.
In Compressed Sensing and Sparse Filtering, Springer Series on Signals
and Communication Technology, Avisha Y. Carmi, Lyudmila Mihaylova,
Simon J. Godsill, Eds., 2014.
[19] C.D. Martin, R. Shafer, B. LaRue, An order-p tensor factorization with
applications in imaging, SIAM J. Sci. Comput. 35, 2013, A474A490.
[20] Jiani Zhang. Ph.D. Thesis, Department of Mathematics, Tufts University,
2017.

