Bolt: Accelerated Data Mining with Fast Vector Compression
Davis W. Blalock
Computer Science and Articial
Intelligence Laboratory
Massachuses Institute of Technology
dblalock@mit.edu
John V. Guag
Computer Science and Articial
Intelligence Laboratory
Massachuses Institute of Technology
guag@mit.edu
ABSTRACT
Vectors of data are at the heart of machine learning and data mining.
Recently, vector quantization methods have shown great promise
in reducing both the time and space costs of operating on vectors.
We introduce a vector quantization algorithm that can compress
vectors over 12× faster than existing techniques while also accel-
erating approximate vector operations such as distance and dot
product computations by up to 10×. Because it can encode over
2GB of vectors per second, it makes vector quantization cheap
enough to employ in many more circumstances. For example, us-
ing our technique to compute approximate dot products in a nested
loop can multiply matrices faster than a state-of-the-art BLAS im-
plementation, even when our algorithm must rst compress the
matrices.
In addition to showing the above speedups, we demonstrate that
our approach can accelerate nearest neighbor search and maximum
inner product search by over 100× compared to oating point opera-
tions and up to 10× compared to other vector quantization methods.
Our approximate Euclidean distance and dot product computations
are not only faster than those of related algorithms with slower
encodings, but also faster than Hamming distance computations,
which have direct hardware support on the tested platforms. We
also assess the errors of our algorithm’s approximate distances and
dot products, and nd that it is competitive with existing, slower
vector quantization algorithms.
CCS CONCEPTS
•Mathematics of computing →Probabilistic algorithms; Di-
mensionality reduction; Mathematical soware;
KEYWORDS
Vector antization, Scalability, Compression, Nearest Neighbor
Search
ACM Reference format:
Davis W. Blalock and John V. Guag. 2017. Bolt: Accelerated Data Mining
with Fast Vector Compression. In Proceedings of KDD’17, August 13–17, 2017,
Halifax, NS, Canada., , 9 pages.
DOI: 10.1145/3097983.3098195
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for prot or commercial advantage and that copies bear this notice and the full citation
on the rst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permied. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specic permission
and/or a fee. Request permissions from permissions@acm.org.
KDD’17, August 13–17, 2017, Halifax, NS, Canada.
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4887-4/17/08. . . $15.00
DOI: 10.1145/3097983.3098195
1 INTRODUCTION
As datasets grow larger, so too do the costs of mining them. ese
costs include not only the space to store the dataset, but also the
compute time to operate on it. is time cost can be decomposed
into:
Costtime = Costread + Costwrite (1)
where Costread is the time cost of operations that read the data,
and Costwrite is the time cost of creating, updating, or deleting
data.
For datasets of vectors, for which many of the read operations are
scalar reductions such as Euclidean distance and dot product com-
putations, vector quantization methods enable signicant savings
in both space usage and Costread. By replacing each vector with a
learned approximation, these methods both save space and enable
fast approximate distance and similarity computations. With as
lile as 8B per vector, these techniques can oen preserve distances
and dot products with extremely high accuracy [7, 14, 30, 31, 41].
However, computing the approximation for a given vector can be
time-consuming, adding greatly to Costwrite. e state-of-the-art
method of [30], for example, requires up to 4ms to encode a single
128-dimensional vector. is makes it practical only if there are
few writes per second. Other techniques are faster, but as we show
experimentally, there is signicant room for improvement.
We describe a vector quantization algorithm, Bolt, that greatly
reduces both the time to encode vectors (Costwrite) and the time
to compute scalar reductions over them (Costread). is not only
reduces the overhead of quantization, but also increases its benets,
making it worthwhile for many more datasets. Our key ideas are
to 1) learn an approximation for the lookup tables used to compute
scalar reductions; and 2) use much smaller quantization codebooks
than similar techniques. Together, these changes facilitate nding
optimal vector encodings and allow scans over codes to be done in
a computationally vectorized manner.
Our contributions consist of:
1. A vector quantization algorithm that encodes vectors sig-
nicantly faster than existing algorithms for a given level
of compression.
2. A fast means of computing approximate similarities and
distances using quantized vectors. Possible similarities
and distances include dot products, cosine similarities, and
distances in Lp spaces, such as the Euclidean distance.
1.1 Problem Statement
Let q ∈ RJ be a query vector and let X = {x1, . . . , xN }, xi ∈ RJ be
a collection of database vectors. Further let d : RJ × RJ → R be a
ar
X
iv
:1
70
6.
10
28
3v
1 
 [
cs
.P
F]
  3
0 
Ju
n 
20
17
distance or similarity function that can be wrien as:
d(q, x) = f
( J∑
j=1
δ (qj ,x j )
)
(2)
where f : R→ R, δ : R × R→ R. is includes both distances
in Lp spaces and dot products as special cases. In the former case,
δ (qj ,x j ) = |qj −x j |p and f (r ) = r (1/p); in the laer case, δ (qj ,x j ) =
qjx j and f (r ) = r . For brevity, we will henceforth refer to d as a
distance function and its output as a distance, though our remarks
apply to all functions of the above form unless otherwise noted.
Our task is to construct three functions д : RJ → G, h : RJ →
H , and d̂ : G ×H → R such that for a given approximation loss
L,
L = Eq,x[(d(q, x) − d̂(д(q),h(x)))2] (3)
the computation time T ,
T = Tд +Th +Td (4)
is minimized, where Tд is the time to encode received queries q
using д,1 Th is the time to encode the database X using h, andTd is
the time to compute the approximate distances between the encoded
queries and encoded database vectors. e relative contributions
of each of these terms depends on how frequently X changes, so
many of our experiments characterize each separately.
ere is a tradeo between the value of the loss L and the time
T , so multiple operating points are possible. In the extreme cases,
L can be xed at 0 by seing д and h to identity functions and
seing d̂ = d . Similarly, T can be set to 0 by ignoring the data
and estimating d as a constant. e primary contribution of this
work is therefore the introduction of д, h and d̂ functions that are
signicantly faster to compute than those of existing work for a
wide range of operating points.
1.2 Assumptions
Like other vector quantization work [7, 14, 21, 30, 41], we assume
that there is an initial oine phase during which the functions д
and h may be learned. is phase contains a training dataset for X
and q. Following this oine phase, there is an online phase wherein
we are given database vectors x that must be encoded and query
vectors q for which we must compute the distances to all of the
database vectors received so far. Once a query is received, these
distances must be computed with as lile latency as possible. e
vectors of X may be given all at once, or one at a time; they may also
be modied or deleted, necessitating re-encoding or removal. is is
in contrast to most existing work, which assumes that x vectors are
all added at once before any queries are received [7, 14, 21, 30, 41],
and therefore that encoding speed is less of a concern.
In practice, one might require the distances between q and only
some of the database vectors X (in particular, the k closest vectors).
is can be achieved using an indexing structure, such as an In-
verted Multi-Index [5, 8] or Locality-Sensitive Hashing hash tables
[2, 12], that allow inspection of only a fraction of X. Such indexing
is complementary to our work in that our approach could be used
to accelerate the computation of distances to the subset of X that
1We cast the creation of query-specic lookup tables as encoding q rather than creating
a new d̂ (the typical interpretation in recent literature).
is inspected. Consequently, we assume that the task is to compute
the distances to all vectors, noting that, in a production seing, “all
vectors” for a given query might be a subset of a full database.
Finally, we assume that both X and q are relatively dense. Bolt
can be applied to sparse data, but does not leverage the sparsity.
Consequently, it is advisable to embed sparse vectors into a dense,
lower-dimensional space before using Bolt.
2 RELATEDWORK
Accelerating vector operations through compression has been the
subject of a great deal of research in the computer vision, informa-
tion retrieval, and machine learning communities, among others.
Our review will necessarily be incomplete, so we refer the reader
to [37, 38] for detailed surveys.
Many existing approaches in the computer vision and informa-
tion retrieval literature fall into one of two categories [38]: binary
embedding and vector quantization. Binary embedding techniques
seek to map vectors in RJ to B-dimensional Hamming space, typ-
ically with B < J . e appeal of binary embedding is that a B-
element vector in Hamming space can be stored in B bits, aording
excellent compression. Moreover, the popcount instruction present
on virtually all desktop, smart phone, and server processors can
be used to compute Hamming distances between 8 byte vectors
in as lile as three cycles. is fast distance computation comes
at the price of reduced representational accuracy for a given code
length [14, 38]. He et al. [14] showed that the popular binary em-
bedding technique of [15] is a more constrained version of their
vector quantization algorithm, and that the objective function of
another state-of-the art binary embedding [24] can be understood
as maximizing only one of two sucient conditions for optimal
encoding of Gaussian data.
Vector quantization approaches yield lower errors than binary
embedding for a given code length, but entail slower encoding
and distance computations. e simplest and most popular vector
quantization method is k-means, which can be seen as encoding
a vector as the centroid to which it is closest. A generalization of
k-means, Product antization (PQ) [21], splits the vector into M
disjoint subvectors and runs k-means on each. e resulting code is
the concatenation of the codes for each subspace. Numerous gener-
alizations of PQ have been published, including Cartesian k-means
[34], Optimized Product antization [14], Generalized Residual
Vector antization [27], Additive antization [6], Composite
antization [41], Optimized Tree antization [7], Stacked an-
tizers [31], and Local Search antization [30]. e idea behind
most of these generalizations is to either rotate the vectors or relax
the constraint that the subvectors be disjoint. Collectively, these
techniques that rely on using the concatenation of multiple codes
to describe a vector are known as Multi-Codebook antization
(MCQ) methods.
An interesting hybrid of binary embedding and vector quanti-
zation is the recent Polysemous Coding of Douze et al. [13]. is
encoding uses product quantization codebooks optimized to also
function as binary codes, allowing the use of Hamming distances
as a fast approximation that can be rened for promising nearest
neighbor candidates.
e most similar vector quantization-related algorithm to our
own is that of [3], which also vectorizes PQ distance computations.
However, their method requires hundreds of thousands or millions
of encodings to be sorted lexicographically and stored contigu-
ously ahead of time, as well as scanned through serially. is is
not possible when the data is rapidly changing or when using an
indexing structure, which would split the data into far smaller par-
titions. eir approach also requires a second renement pass of
non-vectorized PQ distance computations, making their reported
speedups signicantly lower than our own.
In the machine learning community, accelerating vector oper-
ations has been done primarily through (non-binary) embedding,
structured matrices, and model compression. Embedding yields ac-
celeration by reducing the dimensionality of data while preserving
the relevant structure of a dataset overall. ere are strong theo-
retical guarantees regarding the level of reduction aainable for
a given level of distortion in pairwise distances [1, 11, 25], as well
as strong empirical results [22, 36]. However, because embedding
per se only entails reducing the number of oating-point numbers
stored, without reducing the size of each, it is not usually com-
petitive with vector quantization methods. It is possible to embed
data before applying vector quantization, so the two techniques are
complementary.
An alternative to embedding that reduces the cost of storing
and multiplying by matrices is the use of structured matrices. is
consists of repeatedly applying a linear transform, such as permu-
tation [39], the Fast Fourier Transform [40], the Discrete Cosine
Transform [32], or the Fast Hadamard Transform [2, 9], possibly
with learned elementwise weights, instead of performing a matrix
multiply. ese methods have strong theoretical grounding [9]
and sometimes outperform non-structured matrices [39]. ey are
orthogonal to our work in that they bypass the need for a dense
matrix entirely, while our approach can accelerate operations for
which a dense matrix is used.
Another vector-quantization-like technique common in machine
learning is model compression. is typically consists of some
combination of 1) restricting the representation of variables, such
as neural network weights, to fewer bits [20]; 2) reusing weights
[10]; 3) pruning weights in a model aer training [19, 29]; and 4)
training a small model to approximate the outputs of a larger model
[35]. is has been a subject of intense research for neural networks
in recent years, so we do not believe that our approach could yield
smaller neural networks than the current state of the art. Instead,
our focus is on accelerating operations on weights and data that
would otherwise not have been compressed.
3 METHOD
As mentioned in the problem statement, our goal is to construct
a distance function d̂ and two encoding functions д and h such
that d̂(д(q),h(x)) ≈ d(q, x) for some “true” distance function d . To
explain how we do this, we rst begin with a review of Product
antization [21], and then describe how our method diers.
3.1 Background: Productantization
Perhaps the simplest form of vector quantization is the k-means
algorithm, which quantizes a vector to its closest centroid among a
xed codebook of possibilities. As an encoding function, it trans-
forms a vector into a dloд2(K)e-bit code indicating which centroid
is closest, where K is the codebook size (i.e., number of centroids).
Using this encoding, the distance between a query and a database
vector can be approximated as the distance between the query and
its associated centroid.
Product antization (PQ) is a generalization ofk-means wherein
the vector is split into disjoint subvectors and the full vector is en-
coded as the concatenation of the codes for the subvectors. en, the
full distance is approximated as the sum of the distances between
the subvectors of q and the chosen centroids for each corresponding
subvector of x.
Formally, PQ approximates the function d as follows. First, recall
that, by assumption, d can be wrien as:
d(q, x) = f
( J∑
j=1
δ (qj ,x j )
)
where f : R → R, δ : R × R → R. Now, suppose one partitions
the indices j into M disjoint subsets {p1, . . . ,pM }. Typically, each
subset is a sequence of J/M consecutive indices. e argument to
f can then be wrien as:
M∑
m=1
∑
j ∈pm
δ (qj ,x j ) =
M∑
m=1
δ
(
q(m), x(m)
)
(5)
where q(m) and x(m) are the subvectors formed by gathering the
elements of q and x at the indices j ∈ pm , andδ sums the δ functions
applied to each dimension. Product quantization replaces each x(m)
with one vector c(m)i from a codebook set Cm of possibilities. at
is:
M∑
m=1
δ
(
q(m), x(m)
)
≈
M∑
m=1
δ
(
q(m), c(m)i
)
(6)
is allows one to store only the index of the codebook vector
chosen (i.e., i), instead of the elements of the original vector x(m).
More formally, let C = {C1, . . . ,CM } be a set ofM codebooks where
each codebook Cm is itself a set of K vectors {c(m)1 , . . . , c
(m)
K }; we
will refer to these vectors as centroids. Given this set of codebooks,
the PQ encoding function h(x) is:
h(x) = [i1 ; . . . ; iM ], im = arg min
i
d
(
c(m)i , x
(m)
)
(7)
at is, h(x) is a vector such that h(x)m is the index of the centroid
within codebookm to which x(m) is closest.
Using these codebooks also enables construction of a fast query
encoding д and distance approximation d̂ . Specically, let the query
encoding space G be RK×M and dene D = д(q) as:
Dim , δ
(
q(m), c(m)i
)
(8)
en we can rewrite the approximate distance on the right hand
side of 6 as:
M∑
m=1
Dim , i = h(x)m (9)
In other words, the distance can be reduced to a sum of precomputed
distances between q(m) and the codebook vectors c(m)i used to
approximate x. Each of theM columns of D represents the distances
between q(m) and the K centroids in codebook CM . Computation
of the distance proceeds by iterating through the columns, looking
up the distance in row h(x)m , and adding it to a running total. By
reintroducing f , one can now dene:
d̂(д(q),h(x)) , f
( M∑
m=1
Dim , i = h(x)m
)
(10)
If M  D and K  |X|, then computation of d̂ is much faster
than computation of d given the д(q) matrix D and data encodings
H = {h(x), x ∈ X}.
e total computational cost of product quantization is Θ(K J ) to
encode each x, Θ(K J ) to encode each query q, and Θ(M) to compute
the approximate distance between an encoded q and encoded x.
Because queries must be encoded before distance computations
can be performed, this means that the cost of computing the dis-
tances to the N database vectors X when a query is received is
Θ(K J )+Θ(NM). Lastly, since codebooks are learned using k-means
clustering, the time to learn the codebook vectors is O(KN JT ),
whereT is the number of k-means iterations. In all works of which
we are aware, K is set to 256 so that each element of h(x) can be
encoded as one byte.
In certain cases, product quantization is nearly an optimal en-
coding scheme. Specically, under the assumptions that:
1. x ∼ MVN (µ,Σ), and therefore x(m) ∼ MVN (µm ,Σm ),
2. ∀m |Σm | = |Σ|1/m ,
PQ achieves the information-theoretic lower bound on code length
for a given quantization error [14]. is means that PQ encod-
ing is optimal if x is drawn from a multivariate Gaussian and the
subspaces pm are independent and have covariance matrices with
equal determinants.
In practice, however, most datasets are not Gaussian and their
subspaces are neither independent nor described by similar covari-
ances. Consequently, many works have generalized PQ to capture
relationships across subspaces or decrease the dependencies be-
tween them [6, 7, 14, 30, 34].
In summary, PQ consists of three components:
1. Encoding each x in the database using h(x). is transforms
x to a list of M 8-bit integers, representing the indices of the
closest centroids in the M codebooks.
2. Encoding a query q when it is received usingд(q). is returns
a K ×M matrix D where themth column is the distances to
each centroid in codebook Cm .
3. Scanning the database. Once a query is computed, the approx-
imate distance to each x is computed using (10) by looking up
and summing the appropriate entries from each column of D.
3.2 Bolt
Bolt is similar to product quantization but diers in two key ways:
1. It uses much smaller codebooks.
2. It approximates the distance matrix D.
Change (1) directly increases the speeds of the encoding func-
tions д and h. is is because it reduces the number of k-means
centroids for which the distances to a given subvector x(m) or q(m)
must be computed. More specically, by using K = 16 centroids
(motivated below) instead of 256, we reduce the computation by
a factor of 256/16 = 16. is is the source of Bolt’s fast encod-
ing. Using fewer centroids also reduces the k-means training time,
although this is not our focus.
Change (2), approximating the query distance matrix D, allows
us to reduce the size of D. is approximation is separate from ap-
proximating the overall distance—in other algorithms, the entries of
D are the exact distances between each q(m) and the corresponding
centroids Cm . In Bolt, the entries of D are learned 8-bit quantiza-
tions of these exact distances.
Together, changes (1) and (2) allow hardware vectorization of the
lookups in D. Instead of looking up the entry in a given column of
D for one x (a standard load from memory), we can leverage vector
instructions to instead perform V lookups for V consecutive h(x),
h(xi ), . . . ,h(xi+V ), where V = 16, 32, or 64 depending on the plat-
form. Under the mild assumption that encodings can be stored in
blocks of at leastV elements, this aords roughly aV -fold speedup
in the computation of distances. e ability to perform such vec-
torized lookups is present on nearly all modern desktops, laptops,
servers, tablets, and CUDA-enabled GPUs.2 Consequently, while
the performance gain comes from fairly low-level hardware func-
tionality, Bolt is not tied to any particular architecture, processor,
or platform.
Mathematically, the challenge in the above approach is quantiz-
ing D. e distances in this matrix vary tremendously as a function
of dataset, query vector, and even codebook. Naively truncating the
oating-point values to integers in the range [0, 255], for example,
would yield almost entirely 0s for datasets with entries  1 and
almost entirely 255s for datasets with entries  255. is can of
course be counteracted to some extent by globally shiing and
scaling the dataset, but such global changes do not account for
query-specic and codebook-specic variation.
Consequently, we propose to learn a quantization function at
training time. e basic approach is to learn the distribution of dis-
tances within a given column of D (the distances to centroids within
one codebook) across many queries sampled from the training set
and nd upper and lower cutos such that the expected squared
error between the quantized and original distances is minimized.
Formally, for a given column m of D (henceforth, one lookup
table), let Q be the distribution of query subvectors q(m), X be the
distribution of database subvectors x(m), and Y be the distribution
of distances within that table. I.e.:
p(Y = y) ,
∫
Q,X
p(q(m), x(m))I {δ
(
q(m), x(m)
)
= y} (11)
We seek to learn a table-specic quantization function βm : R→
{0, . . . , 255} that minimizes the quantization error. For computa-
tional eciency, we constrain βm (y) to be of the form:
βm (y) = max(0,min(255, bay − bc)) (12)
for some constants a and b. Formally, we seek values for a and b
that minimize:
EY [(ŷ − y)2] (13)
where ŷ , (βm (y) + b)/a is termed the reconstruction of y. Y can
be an arbitrary distribution (though we assume it has nite mean
2e relevant instructions are vpshufb on x86, vtbl on ARM, vperm on PowerPC, and
shfl on CUDA.
and variance) and the value of βm (y) is constrained to a nite set
of integers, so there is not an obvious solution to this problem.
We propose to set b = F−1(α), a = 255/(F−1(1−α) −b) for some
suitable α , where F−1 is the inverse CDF ofY , estimated empirically.
at is, we set a and b such that the α and 1 − α quantiles of Y are
mapped to 0 and 255. Because both F−1(α) and the loss function
are cheap to compute, we can nd a good α at training time with a
simple grid search. In our experiments, we search over the values
{0, .001, .002, .005, .01, .02, .05, .1}. In practice, the chosen α tends
to be among the smaller values, consistent with the observation
that loss from extreme values of y is more costly than reduced
granularity in representing typical values of y.
To quantize multiple lookup tables, we learn a b value for each
table and set a based on the CDF of the aggregated distances Y
across all tables. We cannot learn table-specica values because this
would amount to weighting distances from each table dierently.
e b values can be table-specic because they sum to one overall
bias, which is known at the end of training time and can be corrected
for.
In summary, Bolt is an extension of product quantization with
1) fast encoding speed stemming from small codebooks, and 2) fast
distance computations stemming from adaptively quantized lookup
tables and ecient use of hardware.
3.3 eoretical Guarantees
Due to space constraints, we state the following without proof.
Supporting materials, including proofs and additional bounds, can
be found on Bolt’s website (see Section 4). roughout the following,
let bmin , F−1(α), bmax , F−1(1 − α), ∆ , bmax−bmin256 , and
σY ,
√
Var[Y ]. Furthermore, let the tails of Y be drawn from
any Laplace, Exponential, Gaussian, or subgaussian distribution,
where the tails are dened to include the intervals (−∞,bmin ] and
[bmax ,∞).
Lemma 3.1. bmin ≤ y ≤ bmax =⇒ |y − ŷ | < ∆.
Lemma 3.2. For all ε > ∆, p(|y − ŷ | > ε) <
1
σY
(
e−(bmax−E[Y ])/σY + e−(E[Y ]−bmin )/σY
)
e−ε/σY (14)
We now bound the overall errors in dot products and Euclidean
distances. First, regardless of the distributions of q and x, the
following hold:
Lemma 3.3. |q>x − q>x̂| < ‖q‖ · ‖x − x̂‖
Lemma 3.4. |‖q − x‖ − ‖q − x̂‖| < ‖x − x̂‖
Using these bounds, it is possible to obtain tighter, probabilistic
bounds using Hoeding’s inequality.
Denition 3.5 (Reconstruction). Let C be the set of codebooks
used to encode x. en the vector obtained by replacing each x(m)
with its nearest centroid in codebook Cm is the reconstruction of x,
denoted x̂.
Lemma 3.6. Let r(m) , x(m) − x̂(m), and assume that the values
of ‖r(m)‖ are independent for allm. en:
p(|q>x − q>x̂| ≥ ε) ≤ 2 exp
(
−ε2
2
∑M
m=1(‖q(m)‖ · ‖r(m)‖))2
)
(15)
Lemma 3.7. Let r(m) , x(m) − x̂(m), and assume that the values
of ‖q(m) − x(m)‖2 − ‖q(m) − x̂(m)‖2 are independent for allm. en:
p(|‖q − x‖2 − ‖q − x̂‖2 | > ε) ≤ 2 exp
(
−ε2
2
∑M
m=1‖r(m)‖
4
)
(16)
4 EXPERIMENTAL RESULTS
To assess Bolt’s eectiveness, we implemented both it and com-
parison algorithms in C++ and Python. All of our code and raw
results are publicly available on the Bolt website.3 is website also
contains experiments on additional datasets, as well as thorough
documentation of both our code and experimental setups. All ex-
periments use a single thread on a 2013 Macbook Pro with a 2.6GHz
Intel Core i7-4960HQ processor.
e goals of our experiments are to show that 1) Bolt is ex-
tremely fast at encoding vectors and computing scalar reductions,
both compared to similar algorithms and in absolute terms; and
2) Bolt achieves this speed at lile cost in accuracy compared to
similar algorithms. To do the former, we record its throughput
in encoding and computing reductions. To do the laer, we mea-
sure its accuracy in retrieving nearest neighbors, as well as the
correlations between the reduced values it returns and the true
values. Because they are by far the most benchmarked scalar reduc-
tions in related work and are widely used in practice, we test Bolt
only on the Euclidean distance and dot product. Because of space
constraints, we do not compare Bolt’s distance table quantization
method to possible alternatives, instead simply demonstrating that
it yields no discernible loss of accuracy compared to exact distance
tables.
For all experiments, we assess Bolt and the comparison methods
using the commonly-employed encoding sizes of 8B, 16B, and 32B to
characterize the relationships between space, speed, and accuracy.
All reported timings and throughputs are the best of 5 runs, av-
eraged over 10 trials (i.e., the code is executed 50 times). We use the
best in each trial, rather than average, since this is standard practice
in performance benchmarking. Because there are no conditional
branches in either Bolt or the comparison algorithms (when imple-
mented eciently), all running times depend only on the sizes of
the database and queries, not their distributions; consequently, we
report timing results on random data.
4.1 Datasets
For assessing accuracy, we use several datasets widely used to
benchmark Multi-Codebook antization (MCQ) algorithms:
• Si1M [21] — 1 million 128-dimensional SIFT [28] descriptors
of images. Si1M vectors tend to have high correlations among
many dimensions, and are therefore highly compressible. is
dataset has a predened query/train database/test database split,
consisting of 10,000 query vectors, 100,000 training vectors, and
1 million database vectors.
• Convnet1M [31] — 1 million 128-dimensional Convnet descrip-
tors of images. ese vectors have some amount of correlation,
but less than Si1M. It has a query/train/test split matching that
of Si1M.
• LabelMe22k [33] — 22,000 512-dimensional GIST descriptors
of images. Like Si, it has a great deal of correlation between
3hps://github.com/dblalock/bolt
many dimensions. It only has a train/test split, so we follow
[30, 41] and use the 2,000-vector test set as the queries and the
20,000 vector training set as both the training and test database.
• MNIST [26] — 60,000 28x28-pixel greyscale images, aened
to 784-dimensional vectors. is dataset is sparse and has high
correlations between various dimensions. Again following [30]
and [41], we split it the same way as the LabelMe dataset.
For all datasets, we use a portion of the training database as
queries when learning Bolt’s lookup table quantization.
4.2 Comparison Algorithms
Our comparison algorithms include MCQ methods that have high
encoding speeds ( 1ms / vector on a CPU). If encoding speed is
not a design consideration or is dominated by a need for maximal
compression, methods such as GRVQ [27] or LSQ [30] are more
appropriate than Bolt.4
Our primary baselines are Product antization (PQ) [21] and
Optimized Product antization (OPQ) [14], since they oer the
fastest encoding times. ere are several algorithms that extend
these basic approaches by adding indexing methods [8, 23], or more
sophisticated training-time optimizations [4, 13, 18], but since these
extensions are compatible with our own work, we do not compare
to them. We compare only to versions of PQ and OPQ that use 8
bits per codebook, since this is the seing used in all related work
of which we are aware; we do not compare to using 4 bits, as in
Bolt, since this both reduces their accuracy and increases their com-
putation time. Note that, because the number of bits per codebook
is xed in all methods, varying the encoded representation size
means varying the number of codebooks.
We do not to compare to binary embedding methods in terms
of accuracy as they are known to yield much lower accuracy for
a given code length than MCQ methods [14, 38] and, as we show,
are also slower in computing distances than Bolt.
We have done our best to optimize the implementations of the
comparison algorithms, and nd that we obtain running times supe-
rior to those described in previous works. For example, [31] reports
encoding roughly 190,000 128-dimensional vectors per second with
PQ, while our implementation encodes nearly 300,000.
As a nal comparison, we include a modied version of Bolt, Bolt
Noantize, in our accuracy experiments. is version does not
quantize the distance lookup tables. It is not a useful algorithm since
it sacrices Bolt’s high speed, but it allows us to assess whether
our codebook quantization reduces accuracy.
4.3 Encoding Speed
Before a vector quantization method can compute approximate
distances, it must rst encode the data. We measured how many
vectors each algorithm can encode per second as a function of the
vectors’ length. As shown in Figure 1.le, Bolt can encode data
vectors over 10× faster than PQ, the fastest comparison. Encoding
5 million 128-dimensional vectors of 4B oats per second (top le
plot) translates to an encoding speed of 2.5GB/s. For perspective,
this encoding rate is sucient to encode the entire Si1M dataset
of 1 million vectors in 200ms, and the Si1B dataset of 1 billion
4Although Bolt might still be desirable for its high query speed even if encoding speed
is not a consideration.
vectors in 200s. is rate is also much higher than that of high-speed
(but general-purpose) compression algorithms such as Snappy [17],
which reports an encoding speed of 250MB/s.
Similarly, Bolt can compute the distance matrix constituting a
query’s encoding at over 6 million queries/s (top right plot), while
PQ obtains less than 350, 000 queries/s. Both of these numbers
are suciently high that encoding the query is unlikely to be a
boleneck in computing distances to it.
Figure 1: Bolt encodes both data and query vectors signi-
cantly faster than similar algorithms.
4.4 ery Speed
Much of the appeal of MCQ methods is that they allow fast compu-
tation of approximate distances and similarities directly on com-
pressed data. We assessed various algorithms’ speeds in computing
Euclidean distances from a set of queries to each vector in a com-
pressed dataset. We do not present results for other distances and
similarities since they only aect the computation of queries’ dis-
tance matrices and therefore have speeds nearly identical to those
shown here. In all experiments, the number of compressed data
vectors N is xed at 100, 000 and their dimensionality is xed at
256.
We compare Bolt not only to other MCQ methods, but also to
other methods of computing distances that might serve as reason-
able alternatives to using MCQ at all. ese methods include:
• Binary Embedding. As mentioned in Section 2, the current fastest
method of obtaining approximate distances over compressed vec-
tors is to embed them into Hamming space and use the popcount
instruction to quickly compute Hamming distances between
them.
• Matrix Multiplies. Given the norms of query and database vec-
tors, Euclidean distances can be computed using matrix-vector
multiplies. When queries arrive quickly relative to the latency
with which they must be answered, multiple queries can be
batched into a matrix. Performing one matrix multiply is many
times faster than performing individual matrix-vector multiplies.
We compare to batch sizes of 1, 256, and 1024.
Bolt computes Euclidean distances up to ten times faster than
any other MCQ algorithm and signicantly faster than binary em-
bedding methods can compute Hamming distances (Figure 2). Its
speedup over matrix multiplies depends on the batch size and num-
ber of bytes used in MCQ encoding. When it is not possible to
batch multiple queries (Matmul 1), Bolt 8B is roughly 250× faster,
Bolt 16B is 140× faster, and Bolt 32B is 60× faster (see website for
exact timings). When hundreds of queries can be batched (Matmul
256, Matmul 1024), these numbers are reduced to roughly 13×, 7×,
and 3×.
Figure 2: Bolt can compute the distances/similarities be-
tween a query and the vectors of a compressed database up
to 10× faster than other MCQ algorithms. It is also faster
than binary embedding methods, which use the hardware
popcount instruction, and matrix-vector multiplies using
batches of 1, 256, or 1024 vectors.
Because matrix multiplies are so ubiquitous in data mining, ma-
chine learning, and many other elds, we compare Bolt to matrix
multiplication in more detail. In Figure 3, we prole the time that
Bolt and a state-of-the-art BLAS implementation [16] take to do
matrix multiplies of various sizes. Bolt computes matrix multiplies
by treating each row of the rst matrix as a query, treating the
second matrix as the database, and iteratively computing the in-
ner products between each query and all database vectors. is
nested-loop implementation is not optimal, but Bolt is still able to
outperform BLAS.
In Figure 3.top, we multiply two square matrices of varying sizes,
which is the optimal scenario for most matrix multiply algorithms.
For small matrices, the cost of encoding one matrix as the database
is too high for Bolt to be faster. For larger matrices, this cost is
amortized over many more queries, and Bolt becomes faster. When
the database matrix is already encoded, Bolt is faster for almost
all matrix sizes, even using 32B encodings. Note, though, that
this comparison ceases to be fair for especially large matrices (e.g.
4096 × 4096) since encoding so many dimensions accurately would
almost certainly require more than 32B.
In Figure 3.boom, we multiply a 100,000 × 256 matrix by a
256 × n matrix. Bolt uses the rows of the former matrix as the
database and the columns of the laer as the queries. Again, Bolt is
slower for small matrices when it must rst encode the database,
but always faster for larger ones or when it does not need to encode
the database. Because only the number of queries is changing and
not the dimensionality of each vector, longer encodings would not
be necessary for the larger matrices.
Figure 3: Using a naive nested loop implementation, Bolt
can compute (approximate) matrix products faster than op-
timizedmatrixmultiply routines. Except for smallmatrices,
Bolt is faster even when it must encode the matrices from
scratch as a rst step.
4.5 Nearest Neighbor Accuracy
e most common assessment of MCQ algorithms’ accuracy is their
Recall@R. is is dened as the fraction of the queries q for which
the true nearest neighbor in Euclidean space is among the top R
points with smallest approximate distances to q. is is a proxy for
how many points would likely have to be reranked in a retrieval
context when using an approximate distance measure to generate a
set of candidates. As shown in Figure 4, Bolt yields slightly lower ac-
curacy for a given encoding length than other (much slower) MCQ
methods. e nearly identical curves for Bolt and Bolt No antize
suggest that our proposed lookup table quantization introduces
lile or no error.
Figure 4: Compared to other MCQ algorithms, Bolt is
slightly less accurate in retrieving the nearest neighbor for
a given encoding length.
e dierences across datasets can be explained by their varying
dimensionalities and the extent to which correlated dimensions
tend to be in the same subspaces. In the Si1M dataset, adjacent
dimensions are highly correlated, but they are also correlated with
other dimensions slightly farther away. is rst characteristic
allows all algorithms to perform well, but the second allows PQ
and OPQ to perform even beer thanks to their smaller numbers
of larger codebooks. Having fewer codebooks means that the sub-
spaces associated with each are larger (i.e., more dimensions are
quantized together), allowing mutual information between them
to be exploited. Bolt, with its larger number of smaller codebooks,
must quantize more sets of dimensions independently, which does
not allow it to exploit this mutual inforation. Much the same phe-
nomena explain the results on MNIST.
For the LabelMe dataset, the correlations between dimensions
tend to be even more diuse, with small correlations spanning
dimensions belonging to many subspaces. is is less problematic
for OPQ, which learns a rotation such that correlated dimensions
tend to be placed in the same subspaces. PQ and Bolt, which lack
the ability to rotate the data, have no such option, and so are unable
to encode the data as eectively.
Finally, for the Convnet1M dataset, most of the correlated di-
mensions tend to be immediately adjacent to one another, allowing
all methods to perform roughly equally.
4.6 Accuracy in Preserving Distances and Dot
Products
e Recall@R experiment characterizes how well each algorithm
preserves distances to highly similar points, but not whether dis-
tances in general tend to be preserved. To assess this, we computed
the correlations between the true dot products and approximate
dot products for Bolt and the comparison algorithms. Results for
Euclidean distances are similar, so we omit them. As Figure 5 illus-
trates, Bolt is again slightly less accurate than other MCQ methods.
In absolute terms, however, it consistently exhibits correlations
with the true dot products above .9, and oen near 1.0. is sug-
gests that its approximations could reliably be used instead of exact
computations when slight errors are permissible.
Figure 5: Bolt dot products are highly correlated with true
dot products, though slightly less so than those from other
MCQ algorithms.
For example, if one could tolerate a correlation of .9, one could
use Bolt 8B instead of dense vectors of 4B oats and achieve dra-
matic speedups, as well as compression ratios of 64× for SIFT1M
and Convnet1M, 256× for LabelMe, and 392× for MNIST. If one
required correlations of .95 or more, one could use Bolt 32B and
achieve slightly smaller speedups with compression ratios of 16×,
64×, and 98×.
5 SUMMARY
We describe Bolt, a vector quantization algorithm that rapidly com-
presses large collections of vectors and enables fast computation
of approximate Euclidean distances and dot products directly on
the compressed representations. Bolt both compresses data and
computes distances and dot products up to 10× faster than existing
algorithms, making it advantageous both in read-heavy and write-
heavy scenarios. Its approximate computations can be over 100×
faster than the exact computations on the original oating-point
numbers, while maintaining correlations with the true values of
over .95. Moreover, at this level of correlation, Bolt can achieve
10-200× compression or more. ese aributes make Bolt ideal
as a subroutine in algorithms that are amenable to approximate
computations, such as nearest neighbor or maximum inner product
searches.
It is our hope that Bolt will be used in many production systems
to greatly reduce storage and computation costs for large, real-
valued datasets.
REFERENCES
[1] Nir Ailon and Bernard Chazelle. 2009. e Fast Johnson-Lindenstrauss Transform
and Approximate Nearest Neighbors. SIAM Journal on Computing (SICOMP) 39,
1 (2009), 302–322. DOI:hp://dx.doi.org/10.1137/060673096
[2] Alexandr Andoni, Piotr Indyk, ijs Laarhoven, Ilya Razenshteyn, and Ludwig
Schmidt. 2015. Practical and optimal LSH for angular distance. In Advances in
Neural Information Processing Systems. 1225–1233.
[3] Fabien André, Anne-Marie Kermarrec, and Nicolas Le Scouarnec. 2015. Cache
locality is not enough: high-performance nearest neighbor search with product
quantization fast scan. Proceedings of the VLDB Endowment 9, 4 (2015), 288–299.
[4] Artem Babenko, Relja Arandjelović, and Victor Lempitsky. 2016. Pairwise an-
tization. arXiv preprint arXiv:1606.01550 (2016).
[5] Artem Babenko and Victor Lempitsky. 2012. e inverted multi-index. In Com-
puter Vision and Paern Recognition (CVPR), 2012 IEEE Conference on. IEEE,
3069–3076.
[6] Artem Babenko and Victor Lempitsky. 2014. Additive quantization for extreme
vector compression. In Proceedings of the IEEE Conference on Computer Vision
and Paern Recognition. 931–938.
[7] Artem Babenko and Victor Lempitsky. 2015. Tree antization for Large-Scale
Similarity Search and Classication. CVPR (2015), 1–9. papers3://publication/
uuid/F4762974-BB97-4208-B035-508945A90EFC
[8] Artem Babenko and Victor Lempitsky. 2016. Ecient indexing of billion-scale
datasets of deep descriptors. In Proceedings of the IEEE Conference on Computer
Vision and Paern Recognition. 2055–2063.
[9] Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Francois Fa-
gan, Cedric Gouy-Pailler, Anne Morvan, Nouri Sakr, Tamas Sarlos, and Jamal
Atif. 2016. Structured adaptive and random spinners for fast machine learning
computations. arXiv preprint arXiv:1610.06209 (2016).
[10] Wenlin Chen, James T Wilson, Stephen Tyree, Kilian Q Weinberger, and Yixin
Chen. 2015. Compressing Neural Networks with the Hashing Trick.. In ICML.
2285–2294.
[11] Sanjoy Dasgupta and Anupam Gupta. 2003. An elementary proof of a theorem of
Johnson and Lindenstrauss. Random Structures & Algorithms 22, 1 (2003), 60–65.
[12] M. Datar, N. Immorlica, Piotr Indyk, and V.S. Mirrokni. 2004. Locality-Sensitive
Hashing Scheme Based on p-Stable Distributions. Proceedings of the Twentieth
Annual Symposium on Computational Geometry (2004), 253–262. DOI:hp://dx.
doi.org/10.1145/997817.997857 arXiv:arXiv:1011.1669v3
[13] Mahijs Douze, Hervé Jégou, and Florent Perronnin. 2016. Polysemous codes.
In European Conference on Computer Vision. Springer, 785–801.
[14] Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2014. Optimized product
quantization. IEEE transactions on paern analysis and machine intelligence 36, 4
(2014), 744–755.
[15] Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. 2013.
Iterative quantization: A procrustean approach to learning binary codes for
large-scale image retrieval. IEEE Transactions on Paern Analysis and Machine
Intelligence 35, 12 (2013), 2916–2929.
[16] Gael Guennebaud, Benoit Jacob, and others. 2010. Eigen v3.
hp://eigen.tuxfamily.org. (2010).
[17] SH Gunderson. 2015. Snappy: A fast compressor/decompressor. code. google.
com/p/snappy (2015).
[18] Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and David Simcha. 2016. an-
tization based fast inner product search. In Proceedings of the 19th International
Conference on Articial Intelligence and Statistics. 482–490.
[19] Song Han, Je Pool, John Tran, and William Dally. 2015. Learning both weights
and connections for ecient neural network. In Advances in Neural Information
Processing Systems. 1135–1143.
[20] Lu Hou, anming Yao, and James T Kwok. 2016. Loss-aware Binarization of
Deep Networks. arXiv preprint arXiv:1611.01600 (2016).
[21] Herve Jegou, Mahijs Douze, and Cordelia Schmid. 2011. Product quantization
for nearest neighbor search. IEEE transactions on paern analysis and machine
intelligence 33, 1 (2011), 117–128.
[22] Jianqiu Ji, Jianmin Li, Shuicheng Yan, Bo Zhang, and Qi Tian. 2012. Super-bit
locality-sensitive hashing. In Advances in Neural Information Processing Systems.
108–116.
[23] Yannis Kalantidis and Yannis Avrithis. 2014. Locally optimized product quan-
tization for approximate nearest neighbor search. In Proceedings of the IEEE
Conference on Computer Vision and Paern Recognition. 2321–2328.
[24] Weihao Kong and Wu-Jun Li. 2012. Isotropic hashing. In Advances in Neural
Information Processing Systems. 1646–1654.
[25] Kasper Green Larsen and Jelani Nelson. 2014. e Johnson-Lindenstrauss lemma
is optimal for linear dimensionality reduction. arXiv preprint arXiv:1411.2404
(2014).
[26] Yann LeCun, Corinna Cortes, and Christopher JC Burges. 1998. e MNIST
database of handwrien digits. (1998).
[27] Shicong Liu, Junru Shao, and Hongtao Lu. 2016. Generalized Residual Vector
antization for Large Scale Data. Proceedings - IEEE International Conference on
Multimedia and Expo 2016-Augus (2016). DOI:hp://dx.doi.org/10.1109/ICME.
2016.7552944 arXiv:1609.05345
[28] David G Lowe. 1999. Object recognition from local scale-invariant features. In
Computer vision, 1999. e proceedings of the seventh IEEE international conference
on, Vol. 2. Ieee, 1150–1157.
[29] Zelda Mariet and Suvrit Sra. 2015. Diversity networks. arXiv preprint
arXiv:1511.05077 (2015).
[30] Julieta Martinez, Joris Clement, Holger H Hoos, and James J Lile. 2016. Revisit-
ing additive quantization. In European Conference on Computer Vision. Springer,
137–153.
[31] Julieta Martinez, Holger H Hoos, and James J Lile. 2014. Stacked quantizers for
compositional vector compression. arXiv preprint arXiv:1411.2173 (2014).
[32] Marcin Moczulski, Misha Denil, Jeremy Appleyard, and Nando de Freitas. 2016.
ACDC: A Structured Ecient Linear Layer. ICLR 2 (2016), 1–11. arXiv:1511.05946
hp://arxiv.org/abs/1511.05946
[33] Mohammad Norouzi and David J. Fleet. 2011. Minimal loss hashing for compact
binary codes. In Proceedings of the 28th international conference on machine
learning (ICML-11). 353–360.
[34] Mohammad Norouzi and David J Fleet. 2013. Cartesian k-means. In Proceedings
of the IEEE Conference on Computer Vision and Paern Recognition. 3017–3024.
[35] Zhiyuan Tang, Dong Wang, and Zhiyong Zhang. 2016. Recurrent neural network
training with dark knowledge transfer. In Acoustics, Speech and Signal Processing
(ICASSP), 2016 IEEE International Conference on. IEEE, 5900–5904.
[36] Michail Vlachos, Nikolaos M. Freris, and Anastasios Kyrillidis. 2015. Compres-
sive mining: Fast and optimal data mining in the compressed domain. VLDB
Journal 24, 1 (2015), 1–24. DOI:hp://dx.doi.org/10.1007/s00778-014-0360-3
arXiv:1405.5873
[37] Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. 2016. Learning to hash
for indexing big dataa survey. Proc. IEEE 104, 1 (2016), 34–57.
[38] Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji. 2014. Hashing
for similarity search: A survey. arXiv preprint arXiv:1408.2927 (2014).
[39] Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola,
Le Song, and Ziyu Wang. 2014. Deep Fried Convnets. ICCV (2014). DOI:
hp://dx.doi.org/10.1007/s13398-014-0173-7.2 arXiv:1412.7149
[40] Felix X Yu, Ananda eertha Suresh, Krzysztof M Choromanski, Daniel N
Holtmann-Rice, and Sanjiv Kumar. 2016. Orthogonal Random Features. In
Advances in Neural Information Processing Systems 29, D. D. Lee, M. Sugiyama,
U. V. Luxburg, I. Guyon, and R. Garne (Eds.). Curran Associates, Inc., 1975–1983.
hp://papers.nips.cc/paper/6246-orthogonal-random-features.pdf
[41] Ting Zhang, Chao Du, and Jingdong Wang. 2014. Composite antization for
Approximate Nearest Neighbor Search. Proceedings of the 31st International
Conference on Machine Learning (ICML-14) 32 (2014), 838–846.

