Neural Sequence Prediction by Coaching
Wenhu Chen,1 Guanlin Li,2 Shujie Liu,3 Zhirui Zhang,4 Mu Li,3 Ming Zhou3
University of California, Santa Barbara1
Harbin Institute of Technology2
Microsoft Research Asia3
University of Science and Technology of China4
Abstract
In sequence prediction task, there mainly exist two types of
learning algorithms. One genre based on maximum likeli-
hood estimation (MLE) supervises the training in every time
step via a surrogate loss function but leads to train-test dis-
crepancy and hard-to-train problem. The other genre based on
Reinforcement Learning gives the model freedom to generate
samples and guides the training by feeding back task-level re-
ward but suffers from high variance and instability problem.
In general, these two frameworks both have their own trade-
offs and complement each other. In this paper, we introduce
a hybrid-coach framework to combine these two existing al-
gorithms so that the supervision from MLE can stabilize RL
training while RL can incorporate task-level loss into MLE
training. To augment more easy-to-learn samples for MLE
training and denser reward for RL training, the coach is en-
couraged to be close to both the data and model distribution.
During training, the coach acts as a proxy target and pulls
the model distribution to the data distribution, as the model
makes progress it correspondingly upgrades itself to become
harsher and finally anneal to the ultimate target distribution.
With the coordination of the coach system, our hybrid-coach
framework can efficiently resolve RLâ€™s reward sparsity prob-
lem and MLEâ€™s data sparsity problem. We apply our algo-
rithm on two Machine Translation Task and show significant
improvements over the state-of-art systems.
1 Introduction
Sequence prediction has been widely used in tasks which
involve a group of mutually dependent outputs. Recently
more and more sophisticated exploration in this area has
been made to solve real-life problems like machine trans-
lation (Bahdanau, Cho, and Bengio 2014; Ma et al. 2017;
Norouzi et al. 2016), syntactic parsing (McDonald, Cram-
mer, and Pereira 2005), spelling correction (Bahdanau, Cho,
and Bengio 2014), image captioning (Xu et al. 2015) and
speech recognition (Chorowski et al. 2015). Combined with
recent development in deep neural network techniques, i.e.
LSTM (Hochreiter and Schmidhuber 1997), GRU (Chung et
al. 2014), CNN (Krizhevsky, Sutskever, and Hinton 2012),
sequence Prediction techniques have achieved state-of-the-
art performance on these tasks.
Recent frameworks for sequence predictions are mainly
based on the following two learning algorithm, namely MLE
and RL:
â€¢ Maximum Likelihood Estimation (MLE): MLE aims
to maximize the likelihood of pairwise training data
(X,Y âˆ—). It seeks for the optimal parameter to maximize
the expected conditional probability pÎ¸(Y âˆ—|X):
Î¸âˆ— = argmax
Î¸
E
(X,Y âˆ—)âˆˆD
[pÎ¸(Y
âˆ—|X)] (1)
â€¢ Reinforcement Learning (RL): RL proposes to maximize
the expected task-level reward R(Y, Y âˆ—), the optimal pa-
rameter is defined as:
Î¸âˆ— = argmax
Î¸
E
(X,Y âˆ—)âˆˆD
[ E
Y âˆˆpÎ¸(Y |X)
[R(Y, Y âˆ—)]] (2)
Though these two objectives differ in their definitions, they
both aim to achieve the same goal â€“ learn a model pÎ¸(Y |X)
to approximate real data distribution pdata(Y |X). Many
earlier research works are mainly based on MLE train-
ing, such as sequence-to-sequence model and attention-
based RNN (Bahdanau, Cho, and Bengio 2014; Cho et
al. 2014), these works advocate to factorize sequence-level
likelihood pÎ¸(Y âˆ—|X) as a product of stepwise likelihood
pÎ¸(yt|y1:tâˆ’1, X) and then use it as the surrogate objective
function during training. Though such canonical MLE algo-
rithm achieves really strong performance in many tasks, it
suffers from the following two drawbacks:
â€¢ Train-Test Discrepancy: the input label used to predict
next output token is always correct during training, how-
ever, at test, the input label is actually predicted from the
previous step. Besides, a surrogate loss is often used as
training objective, while task level evaluation metric is
used during testing, these two metrics donâ€™t always cor-
respond well with each other.
â€¢ Data Sparsity: MLE training only maximizes the likeli-
hood of pairwise data (X,Y ) existing in the training set,
thus ignoring all potentially admissible candidates which
are easier to be learned by the model. As a result, at test
time, a small acceptable perturbation at the early stage of
outputs will lead the model to an unseen state which the
model fails to understand, thereupon the quality of fol-
lowing outputs will largely deteriorate.
Recently, reinforcement learning (Sutton and Barto 1998)
has gained its popularity in the supervised learning setting
like sequence prediction or structured prediction tasks, many
ar
X
iv
:1
70
6.
09
15
2v
1 
 [
cs
.A
I]
  2
8 
Ju
n 
20
17
Figure 1: Both MLE and RL have their own drawbacks,
RL suffers from high-variance problem during optimization
and lacks stepwise supervision; MLE suffers the data spar-
sity problem and exposes to train-test discrepancy problem.
Building a hybrid MLE-RL system can help these two al-
gorithms complement each other and partially resolve the
above-mentioned problems. With coach-based framework,
MLEâ€™s data sparsity and RLâ€™s reward sparsity problem can
be further alleviated.
researchers have successfully applied it into tasks like ma-
chine translation, spelling correction, abstractive summa-
rization. Different from MLE, reinforcement learning ex-
poses the model to its own generated words and guides the
model directly with task-level rewards, these two strategies
have successfully alleviated the problems caused by train-
test discrepancy. However, Reinforcement Learning also
faces the following two significant challenges:
â€¢ Reward Sparsity: In sequence prediction, the task-level
reward defined as similarity score between output with
one human-labeled reference is normally sparse, which
means that only very few actions drawn from modelâ€™s
stochastic policy are able to receive positive feedback.
Such sparse reward can normally lead to very inefficient
exploration and discourage potentially valuable explo-
rations.
â€¢ Poor Supervision: In Monte-Carlo Policy Gradient algo-
rithm (Williams 1992), the provided supervision is based
on reward score estimated via Monte-Carlo Sampling.
However, since the action space is often extremely large,
such approximation can be extremely inaccurate and non-
stationary, which consequently makes RLâ€™s supervision
signal very poor and biased. Such poor supervision prob-
lem can lead to high variance and vibrations during train-
ing.
In this paper, we advocate to combine RL and MLE as a
hybrid system, which alleviates RLâ€™s poor supervision prob-
lem and MLEâ€™s train-test discrepancy problem. However,
since the RLâ€™s reward sparsity and MLEâ€™s data sparsity are
yet to be resolved, we further introduce the concept of coach
as an adaptive proxy of data distribution to resolve resolve
reward and data sparsity problem. Formally, we first define
oracle âˆª
(X,Y âˆ—)âˆˆD
q(Y |Y âˆ—) as a surrogate for data distribution,
Figure 2: Four iterative updates of coach and learner, blue
curve represents learnerâ€™s conditional distribution, green
curve represents coach distribution, and red curve repre-
sents the oracle, horizontal axis refers to the potential search
space.
then incorporate a coach pÎ·(Y |Y âˆ—, X) to help the learner
pÎ¸(Y |X) gradually approach the above-defined oracle via
Jensen-Shannon Divergence, please note that we switch the
term of learner and model in the following section, they
both refer to the term pÎ¸(Y |X). As depicted in Figure 1,
the benefit of our proposed hybrid-coaching system are two-
fold. On one hand, it combines RL and MLE algorithm as a
hybrid system, but also assists to stabilize the training pro-
cess. More specifically, our proposed coach acts as a bridge
to combine RL and MLE into one framework, the system-
wise integrity can help alleviate MLEâ€™s train-test discrep-
ancy and RLâ€™s poor supervision problem. On the other hand,
since the coach lies in between coach and oracle, it can
alleviate MLEâ€™s data sparsity problem by providing more
easy-to-learn admissible candidates and RLâ€™s reward spar-
sity problem by giving more abundant rewards to encourage
exploration. As learner makes progress, coach will be ac-
cordingly upgraded and gradually anneal to the oracle. In
order to give a more intuitive view of the iterative optimiza-
tion process, here we draw a schematic digram in Figure 2,
which displays four episodes of update. In the first update,
coach pÎ·(Y |Y âˆ—, X) is trained to put mass between learner
pÎ¸(Y |X) and the oracle âˆª
(X,Y âˆ—)âˆˆD
q(Y |Y âˆ—) via minimizing
the KL-divergence with both learner (flat) and oracle(many
modes). In the second iteration, the learner tries to approxi-
mate the coach distribution (smooth modes). As the learner
makes progress, the coach becomes harsher and closer to or-
acle(sharp modes). Finally, the learner learns to reach its real
target â€“ oracle, shown in the fourth iteration.
Our main contributions are as follow:
â€¢ We leverage a coach model to combine the advantages of
MLE and RL algorithm as a hybrid system.
â€¢ We propose the concept of coach to alleviate hard-to-train
and reward sparsity problems in RL and MLE algorithm.
â€¢ We show that our framework is able to efficiently improve
the state-of-art sequence-to-sequence system on neural
machine translation tasks.
2 Related Literature
Exposure bias and train-test loss discrepancy are two ma-
jor issues in the training of sequence prediction models.
Many research works (Bahdanau, Cho, and Bengio 2014;
Shen et al. 2015; Ranzato et al. 2015; Norouzi et al. 2016;
Lamb et al. 2016) have attempted to tackle these issues
by adding reward-weighted samples drawn from model dis-
tribution into the training data via a reinforcement learn-
ing (Sutton and Barto 1998) framework. By exposing the
model to its own distribution, these methods achieved sig-
nificant improvements. (Bahdanau et al. 2016; Ranzato et
al. 2015; Shen et al. 2015) try to optimize the model as
a stochastic policy to maximize the expected task-level re-
ward. To estimate the policy gradient, all the samples in
the output space should be enumerated, however, due to its
infeasibility in extremely large output space(like machine
translation, speech recognition, etc.), a common practical
approach is using an N-best subset to do the approximation,
as is done in (Shen et al. 2015), which is also adopted in our
paper. Besides, Actor-critic network (Bahdanau et al. 2016)
is also proposed to incorporate task level loss into train-
ing, in which, actor-network is the policy network, and the
critic-network functions like a value network in reinforce-
ment learning (Sutton and Barto 1998). Combining these
two networks together forces the training phrase to be much
closer to the test phrase, and improves performances on sev-
eral tasks.
2.1 Reward Augmented Maximum Likelihood
Another genre to integrate task-level loss into training is re-
ward augmented maximum likelihood (RAML) (Norouzi et
al. 2016), in which an exponentiated payoff distribution de-
fined with the task level rewards is used to connect ML and
RL. By minimizing the KL distance of model distribution
and payoff distribution, optimal regularized expected reward
is achieved. To speed up the training process, instead of
model distribution, payoff distribution is used for sampling
with the help of edit distance trick. Based on RAML, (Ma
et al. 2017) introduces a novel softmax Q-Distribution to in-
terpret RAML and reveals RAMLâ€™s relation with Bayes de-
cision rule. Their experiments on NER, dependency tree and
machine translation all shows notable improvements over
the MLE baseline. Similar with RAML, our paper also in-
terprets sequence prediction as a distribution matching prob-
lem, our proposed oracle resembles the RAMLâ€™s definition
of reward exponentiated payoff distribution. However, our
paper differs from RAML in three perspective:
â€¢ Though we define our oracle in a similar manner as
RAML, our approach doesnâ€™t require heuristic sampling
from this distribution, we only incorporate it as the long-
term optimization goal.
â€¢ Instead of using KL-divergence, we propose to use
Jensen-Shannon Divergence to build a hybrid system.
â€¢ Most importantly, we incorporate a parametric coach dis-
tribution to stabilize the optimization process, which con-
sequently alleviates some well-known problems in current
RL/MLE frameworks.
2.2 Coaching in Imitation Learning
Imitation Learning has also been applied to sequence predic-
tion (Vlachos 2013), many algorithms like (DaumeÌ, Lang-
ford, and Marcu 2009) and (Ross, Gordon, and Bagnell
2011) try to accomplish particular task via expert demon-
strations and achieve state-of-art performance in various
NLP tasks (Berant and Liang 2015; Lampouras and Vlachos
2016). Unfortunately, IL has been known for its difficulty in
optimizing learner towards oracle when their policies have
substantial difference or the oracle are inexpressible by the
model. So as to resolve such hard-to-imitate problem, (He,
Eisner, and Daume 2012) adaptively learns a hope action
based on the interpolation of learnerâ€™s policy and environ-
ment loss function. Our framework is inspired by their pro-
posed concept â€“ coach, we apply a similar strategy to im-
prove current sequence prediction framework â€“ no harsh pe-
nalization from start, encourage the learner to gradually cor-
rect its errors.
2.3 Hybrid System
Hybrid system in sequence prediction is also a very efficient
strategy under the condition when two systems can compen-
sate for each otherâ€™s disadvantages. (Ranzato et al. 2015;
Yang et al. 2017) have proposed to combine two algorithm
into one framework. (Yang et al. 2017) leverages MLE into
Generative Adversarial Network and verifies its capability
in avoiding high variance during training. (Ranzato et al.
2015) also incorporates MLE into sequence-level training
by maximizing the likelihood of front steps while maximiz-
ing the expected reward for the rest. This hybrid strategy
also proves to be useful in stabilizing the training proce-
dure while resolving exposure bias problem. Different from
their work which aim to combine two training methods in a
heuristic way, our proposed coach is introduced as a bridge
in our hybrid system, RL and MLE are combined to alleviate
their counterpartâ€™s disadvantage.
3 Background
3.1 Sequence-to-Sequence Learning
We consider the problem of learning to produce an output
sequence Y = (y1, y2, . . . , yT ), yt âˆˆ A given an input X .
Training and Test use two input-target pairs (X,Y âˆ—), the
generated sequence on test is evaluated with task-specific
score R(Y, Y âˆ—). The task of sequence-to-sequence learning
is to train a model which can output sequence Y as close
(minimize R(Y, Y*)) to the ground truth sequence Y âˆ— as
possible.Encoder-Decoder framework with recurrent neural
network has been widely used for this task (Bahdanau, Cho,
and Bengio 2014; Cho et al. 2014),In this framework, an en-
coder is used to encode the input sequence to be a sequence
of source hidden states, based on which, attention mecha-
nism is leveraged to compute a context vector, which is a
weighted average of the hidden states, the context vector to-
gether with previous target hidden states and previous labels
are fed into the decoder RNN to predict the next state and
its label. In our approach, attention-based RNN (Bahdanau,
Cho, and Bengio 2014) is adopted for both learner and coach
model, and it is formulated as:
yt âˆ¼ g(stâˆ’1, ctâˆ’1) (3)
st = f(stâˆ’1, ctâˆ’1, e(yt)) (4)
Î±t = Î²(st, (h1, . . . , hL)) (5)
ct =
Lâˆ‘
j=1
Î±t,jhj (6)
As suggested in (Xu et al. 2015; Williams 1992; Bahdanau et
al. 2016), sequence-to-sequence learning can be formulated
as a reinforcement learning problem as in (Xu et al. 2015;
Bahdanau et al. 2016). Similarly, we also define the condi-
tional RNN output probability layer as our stochastic pol-
icy policy = pÎ¸(yt|y1:tâˆ’1, x), the reinforcement state as
the input plus by far generated label sequence state =
(X, y1:tâˆ’1). Besides, we define the task score (e.g. n-gram
match, BLEU, etc) as the return return = R(Y, Y âˆ—), based
on which we can also define the stepwise reward rt =
R(y1:t, Y
âˆ—)âˆ’R(y1:tâˆ’1, Y âˆ—).
3.2 Distribution Matching as MLE/RL problem
Here, we first introduce the concept of oracle â€“ the union of
kernel set within dataset D as following:
âˆª
(X,Y âˆ—)âˆˆD
q(Y |Y âˆ—)
= âˆª
(X,Y âˆ—)âˆˆD
K(Y, Y âˆ—)âˆ‘
Y âˆˆY K(Y, Y
âˆ—)
(7)
where K(Y, Y âˆ—) represents the similarity score between
the Y and Y âˆ—, i.e. the task-level reward. As have been
proved in (Norouzi et al. 2016), minimizing the modelâ€™s KL-
divergence with the oracle is equivalent to maximizing its
expected log-scaled reward logK(Y, Y âˆ—) under the regular-
ization of learnerâ€™s entropy.
E
(X,Y âˆ—)âˆˆD
KL(pÎ¸(Y |X)||q(Y |Y âˆ—))
= E
(X,Y âˆ—)âˆˆD
[ E
Y âˆˆpÎ¸(Y |X)
[âˆ’ log q(Y |Y âˆ—)]âˆ’H(pÎ¸(Y |X))]
+ constant
E
(X,Y âˆ—)âˆˆD
KL(q(Y |Y âˆ—)||pÎ¸(Y |X))
= E
(X,Y âˆ—)âˆˆD
[ E
Y âˆˆq(Y,Y âˆ—)
[âˆ’ log pÎ¸(Y |X)]] + constant
(8)
Therefore, minimizing the KL-divergence between learner
and our kernel function is in nature aligned with our goal
to maximize the similarity of system output Y with respect
to ground truth Y âˆ—. By minimizing the KL divergence of
oracle distribution and model distribution, three goals can
be achieved:
â€¢ maximizing the outputâ€™s similarity score K(Y, Y âˆ—) with
respect to ground truth Y âˆ— with Reinforcement Learning.
â€¢ minimizing the variance of modelâ€™s output distribution via
entropy regularization.
â€¢ maximizing the likelihood (in modelâ€™s view) of reward
augmented candidates sampled from q(Y |Y âˆ—).
Though the variance control smooths the output distribution,
it introduces additional noise and harms the model perfor-
mance. Hence we discard the entropy regularization term H
and have KL(pÎ¸(Y |X)||q(Y |Y âˆ—)) minimization problem
equivalent of a reward log q(Y |Y âˆ—) maximization problem.
Further, we apply Jensen-Shannon Divergence to combine
the bidirectional KL-divergence, the system can thus be seen
as a hybrid RL-MLE system. We explicitly express the rela-
tionships between KL-divergence/Shannon-Divergence and
RL/MLE as following:
âˆ‡Î¸KL(pÎ¸(Y |X)||q(Y |Y âˆ—))
â‰ˆ E
Yâˆ¼pÎ¸(Y |X)
[âˆ’ log q(Y |Y âˆ—)âˆ‡Î¸ log pÎ¸(Y |X)] = âˆ‡RL(Î¸)
âˆ‡Î¸KL(q(Y |Y âˆ—)||pÎ¸(Y |X))
= E
Yâˆ¼q(Y |Y âˆ—)
[âˆ’âˆ‡Î¸ log pÎ¸(Y |X)] = âˆ‡MLE(Î¸)
âˆ‡Î¸JSD(pÎ¸(Y |X)||q(Y |Y âˆ—))
â‰ˆ1
2
[âˆ‡RL(Î¸) +âˆ‡MLE(Î¸)]
(9)
4 Model
4.1 Framework Overview
We design a multi-agent interactive architecture, which con-
sists of three essential components, an oracle distribution
R(Y |Y âˆ—) representing a probability space concentrated in
ground truth Y âˆ—, a learner model distribution pÎ¸(Y |X) rep-
resenting the conditional probability of Y given input X , a
coach pÎ·(Y |Y âˆ—, X) representing the probability of Y given
both ground truth Y âˆ— and input, itâ€™s an intermediate distri-
bution lying in between oracle and learner. In order to give
a big picture of our learning framework, we draw a dia-
gram in Figure 3. The system is composed of three systems,
namely a constant oracle q(Y |Y âˆ—), two trainable agents
pÎ·(Y |Y âˆ—, X) and pÎ¸(Y |X). The training proceeds in a it-
erative manner, when learner is fixed, coach is trained to
lie between oracle and learner, when coach is fixed, learner
learns to approach coach as its short-term target.
Figure 3: Iterative Training â€“ the framework first update
coach by minimizing coachâ€™s KL divergence with oracle and
learner, then update learner by minimizing learnerâ€™s Jensen-
Shannon Divergence with coach.
4.2 Oracle
We design the similarity score as the exponent of task-level
reward â€“ K(Y, Y âˆ—) = R(Y, Y âˆ—), we prove that maximiz-
ing learnerâ€™s task-level reward under entropy regularization
is equivalent to minimizing the KL-divergence with oracle.
Formally, we re-write the kernel distribution as:
q(Y |Y âˆ—) = exp(R(Y, Y âˆ—))âˆ‘
Y â€²âˆˆY exp(R(Y
â€², Y âˆ—))
(10)
Oracle is defined as the union of kernel distributions
grounded on datasetD. We follow the strategy in (Bahdanau
et al. 2016) to decompose R(Y, Y âˆ—) into step-wise delta re-
wards r(yt|y1:tâˆ’1, Y âˆ—):
r(yt|y1:tâˆ’1, Y âˆ—) = R(y1:t|Y âˆ—)âˆ’R(y1:tâˆ’1|Y âˆ—) (11)
Note that the capital symbols represent sequence-level in-
formation (Y is the target sequence, and R is the sequence
level reward), while the lowercase letter denotes stepwise
information(yt is the symbol at time t, and ryt)is the reward
of yt).
4.3 Coach
We design our coach model in the same fashion (sequence-
to-sequence) as the learner, it takes Y âˆ— as input and ini-
tializes its first hidden state with input X . At each it-
eration the coach strives to minimize an interpolation
of coach-learner divergence KL(pÎ¸(Y |X)||pÎ·(Y |Y âˆ—, X))
or KL(pÎ·(Y |Y âˆ—, X)||pÎ¸(Y |X)), and coach-oracle diver-
gence KL(pÎ·(Y |Y âˆ—, X)||q(Y |Y âˆ—)). Formally, we define
the coachâ€™s hybrid objective function Lp(pÎ·(Y |Y âˆ—, X)) as
following:
Lp(pÎ·) =Î²KL(pÎ·(Y |Y âˆ—, X)||q(Y |Y âˆ—))+
(1âˆ’ Î²)KL(pÎ¸(Y |X)||pÎ·(Y |Y âˆ—, X))
(12)
The first term KL(pÎ·(Y |Y âˆ—, X)||q(Y |Y âˆ—)) denotes the di-
vergence between coach and oracle, according to Equa-
tion 9, we can directly approximate it with RL, this term
restricts the coach to lie close to the oracle distribution.
KL(pÎ·(Y |Y âˆ—, X)||q(Y |Y âˆ—))
â‰ˆ
Tâˆ‘
t=1
E
Y1:tâˆ’1âˆ¼pÎ·(Y |Y âˆ—,X)
[
âˆ‘
ytâˆˆA
pÎ·(yt|y1:tâˆ’1, Y âˆ—, X)(âˆ’r(yt|y1:tâˆ’1, Y âˆ—))]
+ constant
(13)
The second term KL(pÎ¸(Y |X)||pÎ·(Y |Y âˆ—, X)) refers to the
constraint that coach shouldnâ€™t be too far away from the
learnerâ€™s search ability. We train the coach to lie in between
learner and oracle, so that coach can act as a short-term tar-
get proxy to provide easy-to-learn supervision and abundant
rewards. Formally, we rewrite this term as following:
KL(pÎ¸(Y |X)||pÎ·(Y |Y âˆ—, X))
=
Tâˆ‘
t=1
E
Y1:tâˆ’1âˆ¼pÎ¸(Y |X)
[
âˆ‘
ytâˆˆA
pÎ¸(yt|y1:tâˆ’1, X) log pÎ·(yt|y1:tâˆ’1, Y âˆ—, X)
+ constant
(14)
Based on the above-mentioned propositions, we could
rewrite coachâ€™s gradient as a weighted-sum of RL and MLE:
âˆ‡Î·Lp(pÎ·)
â‰ˆÎ² E
Yâˆ¼pÎ·
[
Tâˆ‘
t=1
(âˆ’r(yt|y1:tâˆ’1, Y âˆ—))âˆ‡log pÎ·(yt|y1:tâˆ’1, Y âˆ—, X)]+
(1âˆ’ Î²) E
Yâˆ¼pÎ¸
[
Tâˆ‘
t=1
(âˆ’âˆ‡log pÎ·(yt|y1:tâˆ’1, Y âˆ—, X))]
â‰ˆÎ²âˆ‡RL(Î·) + (1âˆ’ Î²)âˆ‡MLE(Î·)
(15)
4.4 Learner
In order to finally approach oracleâ€™s distribution, we smooth
the optimization process by splitting the long-term goal into
a series of short-term targets â€“ approximate the coachâ€™s dis-
tribution. Formally, We define the learnerâ€™s optimization tar-
get Lg as its Jensen-Shannon divergence with coach:
Lg(pÎ¸) = JSD(pÎ¸(Y |X)||pÎ·(Y |Y âˆ—, X)) (16)
Accordingly, the first term of Jensen-Shannon Divergence
is KL(pÎ¸(Y |X)||pÎ·(Y |Y âˆ—, X)), which can be rewritten as
follows.
JSD(pÎ¸(Y |X)||pÎ·(Y |Y âˆ—, X))
=
1
2
[KL(pÎ¸(Y |X)||pÎ·(yt|y1:tâˆ’1, Y âˆ—, X))
+KL(pÎ·(yt|y1:tâˆ’1, Y âˆ—, X)||pÎ¸(Y |X))]
â‰ˆ1
2
Tâˆ‘
t=1
E
Y1:tâˆ’1âˆ¼pÎ¸(Y |X)
[
âˆ‘
ytâˆˆA
âˆ’pÎ¸(yt|y1:tâˆ’1, X)logpÎ·(yt|y1:tâˆ’1, Y âˆ—, X)]+
1
2
Tâˆ‘
t=1
E
y1:tâˆ’1âˆ¼pÎ·(Y |Y âˆ—,X)
[
âˆ‘
ytâˆˆA
âˆ’pÎ·(yt|y1:tâˆ’1, Y âˆ—, X) log pÎ¸(yt|y1:tâˆ’1, X)]
+ constant
(17)
The gradient of âˆ‡Î¸KL(pÎ¸(Y |X)||pÎ·(Y |Y âˆ—, X)) can thus
be written as:
âˆ‡Î¸KL(pÎ¸(Y |X)||pÎ·(Y |Y âˆ—, X))
â‰ˆ E
Yâˆ¼pÎ¸
[
Tâˆ‘
t=1
âˆ‡log pÎ¸(yt|y1:tâˆ’1, X)(âˆ’ log pÎ·(yt|y1:tâˆ’1, Y âˆ—, X))]
(18)
However, since the RL reward log pÎ·(yt|y1:tâˆ’1, Y âˆ—, X)
is unbounded, directly applying this policy gradient
into our framework will cause huge variance during
training. Therefore, we replace pÎ·(Y |Y âˆ—, X) with a
soft-coach exp pÎ·(Y |Y
âˆ—,X)âˆ‘
Y exp pÎ·(Y |Y âˆ—,X)
, which bounds the reward
pÎ·(yt|y1:tâˆ’1, Y âˆ—, X) between 0 and 1. Accordingly, we
rewrite the whole JSD gradient term as following:
âˆ‡Î¸Lg(pÎ¸)
â‰ˆ1
2
E
Yâˆ¼pÎ¸
[
Tâˆ‘
t=1
âˆ’pÎ·(yt|y1:tâˆ’1, Y âˆ—, X)âˆ‡log pÎ¸(yt|y1:tâˆ’1, X)]+
1
2
E
Yâˆ¼pÎ·
[
Tâˆ‘
t=1
âˆ’âˆ‡log pÎ¸(yt|y1:tâˆ’1, X)]
â‰ˆ1
2
[âˆ‡RL(Î¸) +âˆ‡MLE(Î¸)]
(19)
The gradient term consists of two components, the first
term correspond to Monte-Carlo Policy Gradient (Williams
1992), while the second term refers to RAML (Norouzi et al.
2016) gradient. The integrity of two RL and MLE not only
directly supervises via the termâˆ‡MLE(Î¸) but also gives the
model freedom to explore via the term âˆ‡RL(Î¸). Moreover,
the leverage of coach into training can provide abundant re-
wards log pÎ·(yt|y1:tâˆ’1, Y âˆ—, X) to encourage more efficient
searching and demonstrate more easy-to-learn candidates
Y âˆ¼ pÎ·(Y |Y âˆ—, X) for MLE training to imitate.
4.5 Coach-guided Training
We show our iterative training algorithm in Algorithm 1,
the algorithm takes two stages, we first pre-train both the
learner and coach model to a decent performance, then we
start the iterative training stage by alternating coach update
and learner update.
Algorithm 1 Maximizing Task-Level Reward via Coaching
procedure PRE-TRAINING
Initialize pÎ¸(Y |X) and pÎ·(Y |Y âˆ—, X) with random
weights Î¸ and Î·
Pre-train the pÎ¸(Y |X) to predict Y âˆ— given X
Use pre-trained pÎ¸(Y |X) to generate YÌ‚ given X
Pre-train the pÎ·(Y |Y âˆ—, X) to predict YÌ‚ and Y âˆ—
end procedure
procedure ITERATIVE-TRAINING
while Not Converged do
Receive a random example (X,Y âˆ—)
Generate a sequence YÌ‚ from pÎ¸(Y |X)
Generate a sequence YÌƒ from pÎ·(Y |Y âˆ—, X)
if Learner-step then
Update learner Î¸ with Equation 19
else if Coach-step then
Update coach Î· with Equation 15
end if
end while
end procedure
5 Experiment
5.1 Machine Translation
Implementation In our experiment, we design a stepwise
n-gram matching reward function as Equation 20, which
computes n-gram matching score based on by far generated
labels y1:tâˆ’1 and ground truth Y âˆ—. Specifically, We compute
the 1-4 gram matches and use their interpolation to approxi-
mate the task-level reward.
r(yt|y1:tâˆ’1, Y âˆ—) =
ï£±ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£³
1.0;N(y1:t, ytâˆ’3:t) <= N(Y
âˆ—, ytâˆ’3:t)
0.6;N(y1:t, ytâˆ’2:t) <= N(Y
âˆ—, ytâˆ’2:t)
0.3;N(y1:t, ytâˆ’1:t) <= N(Y
âˆ—, ytâˆ’1:t)
0.1;N(y1:t, yt) <= N(Y
âˆ—, yt)
0.0; otherwise
(20)
where N(Y, YÌƒ ) represents the occurrence of phrase YÌƒ in Y ,
N(y1:t, ytâˆ’3:t) <= N(Y
âˆ—, ytâˆ’3:t) means the sub-sequence
{ytâˆ’3, ytâˆ’2, ytâˆ’1, yt} matches the ground truthâ€™s 4-gram,
thus it receives the maximum reward 1.0, the reward for 3-
gram, 2-gram and 1-gram decrease progressively. Based on
the defined n-gram match reward r, we are able to estab-
lish an oracle distribution q(Y |Y âˆ—) as its exponentiated nor-
malization. In order to compute Monte-Carlo Policy Gra-
dient (Williams 1992), we only adopt a n-best list. In our
experiment, we test with k-best samples from beam search
algorithm and verify our model is insensitive to k value. We
speculate that itâ€™s due to the fact that beam search algorithm
Table 1: Comparison of our coaching framework with the
existing work on IWSLT-2014 German-English Machine
Translation Task, all methods require pre-training using ML
baseline except Softmax-Q.
Methods Baseline Model
MIXER (Ranzato et al. 2015) 20.10 21.81
BSO (Wiseman and Rush 2016) 24.03 26.36
A-C (Bahdanau et al. 2016) 27.56 28.53
Softmax-Q (Ma et al. 2017) 27.66 28.77
Our Work (Î² = 0.33)
28.80
29.48
Our Work (Î² = 0.50) 29.50
Our Work (Î² = 0.66) 29.48
Table 2: Results on IWSLT-2014 English-German Machine
Translation Task, since no research paper has reported the
result, we compare with our own baseline system.
Methods Baseline Model
Our Work (Î² = 0.33)
24.28
24.91
Our Work (Î² = 0.50) 24.86
Our Work (Î² = 0.66) 24.70
produces too-similar samples, thus more samples doesnâ€™t
lead to a more accurate approximation. Therefore, we stick
to top-1 approximation in our following experiments since it
accelerates our training process.
Dataset To validate our coaching framework on sequence
prediction learning task, we select machine translation as
our benchmark to perform experiments. Following (Ran-
zato et al. 2015; Bahdanau et al. 2016), we select the
German-English and English-German machine translation
track of IWSLT 2014 evaluation campaign. The corpus con-
tains sentence-aligned subtitles of TED and TEDx talks,
we use Moses toolkit (Koehn et al. 2007) and remove sen-
tences longer than 50 words as well as casing. The training
dataset contains 153K sentences while the validation con-
tains 6,969 sentences pairs, the test set comprises dev2010,
dev2012, tst2010, tst2011 ad tst2012, the total amount are
6,750 sentences. Note that we donâ€™t cut off sentences longer
than 50 words in test/validation set. The evaluation metric
is BLEU (Papineni et al. 2002) computed via the multi-
bleu.perl script.
System Setting We design a unified GRU-based
RNN (Chung et al. 2014) for both learner and coach
model. For the sake of comparability with the already
existed work, we use a similar system setting with 512 RNN
hidden units and 256 as embedding size. We use bidirec-
tional encoder and initialize both its own decoder states and
coachâ€™s hidden state with the learnerâ€™s last hidden state, in
this way we ensure the input sequence is also encoded by
coach RNN model. Besides, we also follow the attention
mechanism proposed in (Bahdanau, Cho, and Bengio 2014)
to help the model gain better performance. During training,
we use ADADELTA (Zeiler 2012) with  = 10âˆ’6 and
Figure 4: Learning curve on IWSLT English-German Dev
set, coach and learner cooperate with each other and improve
simultaneously.
Ï = 0.95 to separately optimize the learner and coachâ€™s
parameters. During decoding, a beam size of 8 is used
to approximate the full search space. Experimentally, we
implement the system based on our in-house deep learning
library, the whole training pipeline normally takes around
24 hours on a GeForce GTX 1080 device.
Results The experimental results for IWSLT2014
German-English and English-German Translation Task
are summarized in Table 1 and Table 2, we compare our
results with baseline systems as well as other state-of-art
competing algorithms. We can see that our approach can
still bring improvement over a very strong baseline system
and outperforms the other systems by a notable margin.
Therefore, we empirically verify that the abundant rewards
and easy-to-learn supervisions from coach can encourage
learner to find a better solution. Here we also draw the
learning curve of our framework in Figure 4 with different
hyper-parameter, we can clearly see the interaction between
these two agents, as learner makes progress, coach corre-
spondingly updates its parameters towards oracle, finally
the two models both converge to their local optimum.
6 Conclusion
We present a learning algorithm for sequence prediction task
that combines current popular MLE and RL algorithms as a
hybrid system and helps alleviate MLEâ€™s data sparsity and
RLâ€™s reward sparsity problem. Our whole system is based
on an interactive multi-agent framework, we have verified
that such novel architecture can efficient improve over the
baseline system and outperforms competing algorithms. We
believe the concept of coaching can be applicable to a wide
range of probabilistic matching tasks. In the future, we in-
tend to explore coachâ€™s application in more scenarios.
References
[Bahdanau et al. 2016] Bahdanau, D.; Brakel, P.; Xu, K.;
Goyal, A.; Lowe, R.; Pineau, J.; Courville, A.; and Bengio,
Y. 2016. An actor-critic algorithm for sequence prediction.
arXiv preprint arXiv:1607.07086.
[Bahdanau, Cho, and Bengio 2014] Bahdanau, D.; Cho, K.;
and Bengio, Y. 2014. Neural machine translation by
jointly learning to align and translate. arXiv preprint
arXiv:1409.0473.
[Berant and Liang 2015] Berant, J., and Liang, P. 2015. Im-
itation learning of agenda-based semantic parsers. Trans-
actions of the Association for Computational Linguistics
3:545â€“558.
[Cho et al. 2014] Cho, K.; Van MerrieÌˆnboer, B.; Gulcehre,
C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio,
Y. 2014. Learning phrase representations using rnn encoder-
decoder for statistical machine translation. arXiv preprint
arXiv:1406.1078.
[Chorowski et al. 2015] Chorowski, J. K.; Bahdanau, D.;
Serdyuk, D.; Cho, K.; and Bengio, Y. 2015. Attention-based
models for speech recognition. In Advances in Neural Infor-
mation Processing Systems, 577â€“585.
[Chung et al. 2014] Chung, J.; Gulcehre, C.; Cho, K.; and
Bengio, Y. 2014. Empirical evaluation of gated recur-
rent neural networks on sequence modeling. arXiv preprint
arXiv:1412.3555.
[DaumeÌ, Langford, and Marcu 2009] DaumeÌ, H.; Langford,
J.; and Marcu, D. 2009. Search-based structured prediction.
Machine learning 75(3):297â€“325.
[He, Eisner, and Daume 2012] He, H.; Eisner, J.; and
Daume, H. 2012. Imitation learning by coaching. In
Advances in Neural Information Processing Systems,
3149â€“3157.
[Hochreiter and Schmidhuber 1997] Hochreiter, S., and
Schmidhuber, J. 1997. Long short-term memory. Neural
computation 9(8):1735â€“1780.
[Koehn et al. 2007] Koehn, P.; Hoang, H.; Birch, A.;
Callison-Burch, C.; Federico, M.; Bertoldi, N.; Cowan, B.;
Shen, W.; Moran, C.; Zens, R.; et al. 2007. Moses: Open
source toolkit for statistical machine translation. In Proceed-
ings of the 45th annual meeting of the ACL on interactive
poster and demonstration sessions, 177â€“180. Association
for Computational Linguistics.
[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky, A.;
Sutskever, I.; and Hinton, G. E. 2012. Imagenet classifica-
tion with deep convolutional neural networks. In Advances
in neural information processing systems, 1097â€“1105.
[Lamb et al. 2016] Lamb, A. M.; GOYAL, A. G. A. P.;
Zhang, Y.; Zhang, S.; Courville, A. C.; and Bengio, Y. 2016.
Professor forcing: A new algorithm for training recurrent
networks. In Advances In Neural Information Processing
Systems, 4601â€“4609.
[Lampouras and Vlachos 2016] Lampouras, G., and Vla-
chos, A. 2016. Imitation learning for language generation
from unaligned data. In Proceedings of COLING 2016, the
26th International Conference on Computational Linguis-
tics: Technical Papers, 1101â€“1112. The COLING 2016 Or-
ganizing Committee.
[Ma et al. 2017] Ma, X.; Yin, P.; Liu, J.; Neubig, G.; and
Hovy, E. 2017. Softmax q-distribution estimation for struc-
tured prediction: A theoretical interpretation for raml. arXiv
preprint arXiv:1705.07136.
[McDonald, Crammer, and Pereira 2005] McDonald, R.;
Crammer, K.; and Pereira, F. 2005. Online large-margin
training of dependency parsers. In Proceedings of the 43rd
annual meeting on association for computational linguis-
tics, 91â€“98. Association for Computational Linguistics.
[Norouzi et al. 2016] Norouzi, M.; Bengio, S.; Jaitly, N.;
Schuster, M.; Wu, Y.; Schuurmans, D.; et al. 2016. Re-
ward augmented maximum likelihood for neural structured
prediction. In Advances In Neural Information Processing
Systems, 1723â€“1731.
[Papineni et al. 2002] Papineni, K.; Roukos, S.; Ward, T.;
and Zhu, W.-J. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th an-
nual meeting on association for computational linguistics,
311â€“318. Association for Computational Linguistics.
[Ranzato et al. 2015] Ranzato, M.; Chopra, S.; Auli, M.; and
Zaremba, W. 2015. Sequence level training with recurrent
neural networks. arXiv preprint arXiv:1511.06732.
[Ross, Gordon, and Bagnell 2011] Ross, S.; Gordon, G. J.;
and Bagnell, D. 2011. A reduction of imitation learning
and structured prediction to no-regret online learning. In
AISTATS, volume 1, 6.
[Shen et al. 2015] Shen, S.; Cheng, Y.; He, Z.; He, W.;
Wu, H.; Sun, M.; and Liu, Y. 2015. Minimum risk
training for neural machine translation. arXiv preprint
arXiv:1512.02433.
[Sutton and Barto 1998] Sutton, R. S., and Barto, A. G.
1998. Reinforcement learning: An introduction, volume 1.
MIT press Cambridge.
[Vlachos 2013] Vlachos, A. 2013. An investigation of imi-
tation learning algorithms for structured prediction. In Eu-
ropean Workshop on Reinforcement Learning, 143â€“154.
[Williams 1992] Williams, R. J. 1992. Simple statistical
gradient-following algorithms for connectionist reinforce-
ment learning. Machine learning 8(3-4):229â€“256.
[Wiseman and Rush 2016] Wiseman, S., and Rush, A. M.
2016. Sequence-to-sequence learning as beam-search op-
timization. arXiv preprint arXiv:1606.02960.
[Xu et al. 2015] Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville,
A.; Salakhudinov, R.; Zemel, R.; and Bengio, Y. 2015.
Show, attend and tell: Neural image caption generation with
visual attention. In International Conference on Machine
Learning, 2048â€“2057.
[Yang et al. 2017] Yang, Z.; Chen, W.; Wang, F.; and Xu, B.
2017. Improving neural machine translation with condi-
tional sequence generative adversarial nets. arXiv preprint
arXiv:1703.04887.
[Zeiler 2012] Zeiler, M. D. 2012. Adadelta: an adaptive
learning rate method. arXiv preprint arXiv:1212.5701.

