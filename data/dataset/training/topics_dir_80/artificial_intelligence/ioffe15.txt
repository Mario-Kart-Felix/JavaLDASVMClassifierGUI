Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift
Sergey Ioffe SIOFFE@GOOGLE.COM
Christian Szegedy SZEGEDY@GOOGLE.COM
Google, 1600 Amphitheatre Pkwy, Mountain View, CA 94043
Abstract
Training Deep Neural Networks is complicated
by the fact that the distribution of each layer’s
inputs changes during training, as the parame-
ters of the previous layers change. This slows
down the training by requiring lower learning
rates and careful parameter initialization, and
makes it notoriously hard to train models with
saturating nonlinearities. We refer to this phe-
nomenon as internal covariate shift, and ad-
dress the problem by normalizing layer inputs.
Our method draws its strength from making nor-
malization a part of the model architecture and
performing the normalization for each training
mini-batch. Batch Normalization allows us to
use much higher learning rates and be less care-
ful about initialization, and in some cases elim-
inates the need for Dropout. Applied to a state-
of-the-art image classification model, Batch Nor-
malization achieves the same accuracy with 14
times fewer training steps, and beats the original
model by a significant margin. Using an ensem-
ble of batch-normalized networks, we improve
upon the best published result on ImageNet clas-
sification: reaching 4.82% top-5 test error, ex-
ceeding the accuracy of human raters.
1. Introduction
Deep learning has dramatically advanced the state of the art
in vision, speech, and many other areas. Stochastic gradient
descent (SGD) has proved to be an effective way of train-
ing deep networks, and SGD variants such as momentum
(Sutskever et al., 2013) and Adagrad (Duchi et al., 2011)
have been used to achieve state of the art performance.
SGD optimizes the parameters Θ of the network, so as to
Proceedings of the 32nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-
right 2015 by the author(s).
minimize the loss
Θ = arg min
Θ
1
N
N∑
i=1
`(xi,Θ)
where x1...N is the training data set. With SGD, the train-
ing proceeds in steps, at each step considering a mini-
batch x1...m of size m. Using mini-batches of examples,
as opposed to one example at a time, is helpful in sev-
eral ways. First, the gradient of the loss over a mini-batch
1
m
∑m
i=1
∂`(xi,Θ)
∂Θ is an estimate of the gradient over the
training set, whose quality improves as the batch size in-
creases. Second, computation over a mini-batch can be
more efficient than m computations for individual exam-
ples on modern computing platforms.
While stochastic gradient is simple and effective, it requires
careful tuning of the model hyper-parameters, specifically
the learning rate and the initial parameter values. The train-
ing is complicated by the fact that the inputs to each layer
are affected by the parameters of all preceding layers – so
that small changes to the network parameters amplify as
the network becomes deeper.
The change in the distributions of layers’ inputs presents
a problem because the layers need to continuously adapt
to the new distribution. When the input distribution to a
learning system changes, it is said to experience covari-
ate shift (Shimodaira, 2000). This is typically handled via
domain adaptation (Jiang, 2008). However, the notion of
covariate shift can be extended beyond the learning system
as a whole, to apply to its parts, such as a sub-network or a
layer. Consider a network computing
` = F2(F1(u,Θ1),Θ2)
where F1 and F2 are arbitrary transformations, and the
parameters Θ1,Θ2 are to be learned so as to minimize
the loss `. Learning Θ2 can be viewed as if the inputs
x = F1(u,Θ1) are fed into the sub-network
` = F2(x,Θ2).
Batch Normalization
For example, a gradient descent step
Θ2 ← Θ2 −
α
m
m∑
i=1
∂F2(xi,Θ2)
∂Θ2
(for mini-batch sizem and learning rate α) is exactly equiv-
alent to that for a stand-alone network F2 with input x.
Therefore, the input distribution properties that aid the net-
work generalization – such as having the same distribution
between the training and test data – apply to training the
sub-network as well. As such it is advantageous for the
distribution of x to remain fixed over time. Then, Θ2 does
not have to readjust to compensate for the change in the
distribution of x.
Fixed distribution of inputs to a sub-network would have
positive consequences for the layers outside the sub-
network, as well. Consider a layer with a sigmoid acti-
vation function z = g(Wu + b) where u is the layer in-
put, the weight matrix W and bias vector b are the layer
parameters to be learned, and g(x) = 11+exp(−x) . As |x|
increases, g′(x) tends to zero. This means that for all di-
mensions of x = Wu + b except those with small absolute
values, the gradient flowing down to u will vanish and the
model will train slowly. However, since x is affected by
W, b and the parameters of all the layers below, changes to
those parameters during training will likely move many di-
mensions of x into the saturated regime of the nonlinearity
and slow down the convergence. This effect is amplified
as the network depth increases. In practice, the saturation
problem and the resulting vanishing gradients are usually
addressed by using Rectified Linear Units (Nair & Hinton,
2010) ReLU(x) = max(x, 0), careful initialization (Ben-
gio & Glorot, 2010; Saxe et al., 2013), and small learning
rates. If, however, we could ensure that the distribution
of nonlinearity inputs remains more stable as the network
trains, then the optimizer would be less likely to get stuck
in the saturated regime, and the training would accelerate.
We refer to the change in the distributions of internal nodes
of a deep network, in the course of training, as Internal Co-
variate Shift. Eliminating it offers a promise of faster train-
ing. We propose a new mechanism, which we call Batch
Normalization, that takes a step towards reducing internal
covariate shift, and in doing so dramatically accelerates the
training of deep neural nets. It accomplishes this via a nor-
malization step that fixes the means and variances of layer
inputs. Batch Normalization also has a beneficial effect
on the gradient flow through the network, by reducing the
dependence of gradients on the scale of the parameters or
of their initial values. This allows us to use much higher
learning rates without the risk of divergence. Furthermore,
batch normalization regularizes the model and reduces the
need for Dropout (Srivastava et al., 2014). Finally, Batch
Normalization makes it possible to use saturating nonlin-
earities by preventing the network from getting stuck in the
saturated modes.
In Sec. 4.2, we apply Batch Normalization to the best-
performing ImageNet classification network, and show that
we can match its performance using only 7% of the training
steps, and can further exceed its accuracy by a substantial
margin. Using an ensemble of such networks trained with
Batch Normalization, we achieve the top-5 error rate that
improves upon the best known results on ImageNet classi-
fication.
2. Towards Reducing Internal Covariate Shift
We define Internal Covariate Shift as the change in the dis-
tribution of network activations due to the change in net-
work parameters during training. To improve the training,
we seek to reduce the internal covariate shift. By fixing
the distribution of the layer inputs x as the training pro-
gresses, we expect to improve the training speed. It has
been long known (LeCun et al., 1998b; Wiesler & Ney,
2011) that the network training converges faster if its in-
puts are whitened – i.e., linearly transformed to have zero
means and unit variances, and decorrelated. As each layer
observes the inputs produced by the layers below, it would
be advantageous to achieve the same whitening of the in-
puts of each layer. By whitening the inputs to each layer,
we would take a step towards achieving the fixed distri-
butions of inputs that would remove the ill effects of the
internal covariate shift.
We could consider whitening activations at every training
step or at some interval, either by modifying the network
directly or by changing the parameters of the optimiza-
tion algorithm to depend on the network activation values
(Wiesler et al., 2014; Raiko et al., 2012; Povey et al., 2014;
Desjardins & Kavukcuoglu). However, if these modifica-
tions are interspersed with the optimization steps, then the
gradient descent step may attempt to update the parame-
ters in a way that requires the normalization to be updated,
which reduces the effect of the gradient step. For example,
consider a layer with the input u that adds the learned bias
b, and normalizes the result by subtracting the mean of the
activation computed over the training data: x̂ = x − E[x]
where x = u + b, X = {x1...N} is the set of values of x
over the training set, and E[x] = 1N
∑N
i=1 xi. If a gradi-
ent descent step ignores the dependence of E[x] on b, then
it will update b ← b + ∆b, where ∆b ∝ −∂`/∂x̂. Then
u+ (b+ ∆b)−E[u+ (b+ ∆b)] = u+ b−E[u+ b]. Thus,
the combination of the update to b and subsequent change
in normalization led to no change in the output of the layer
nor, consequently, the loss. As the training continues, b
will grow indefinitely while the loss remains fixed. This
problem can get worse if the normalization not only cen-
ters but also scales the activations. We have observed this
Batch Normalization
empirically in initial experiments, where the model blows
up when the normalization parameters are computed out-
side the gradient descent step.
The issue with the above approach is that the gradient de-
scent optimization does not take into account the fact that
the normalization takes place. To address this issue, we
would like to ensure that, for any parameter values, the net-
work always produces activations with the desired distri-
bution. Doing so would allow the gradient of the loss with
respect to the model parameters to account for the normal-
ization, and for its dependence on the model parameters Θ.
Let again x be a layer input, treated as a vector, and X be
the set of these inputs over the training data set. The nor-
malization can then be written as a transformation
x̂ = Norm(x,X )
which depends not only on the given training example x
but on all examples X – each of which depends on Θ
if x is generated by another layer. For backpropagation,
we would need to compute the Jacobians ∂Norm(x,X )∂x and
∂Norm(x,X )
∂X ; ignoring the latter term would lead to the ex-
plosion described above. Within this framework, whiten-
ing the layer inputs is expensive, as it requires computing
the covariance matrix Cov[x] = Ex∈X [xxT ] − E[x]E[x]T
and its inverse square root, to produce the whitened acti-
vations Cov[x]−1/2(x− E[x]), as well as the derivatives of
these transforms for backpropagation. This motivates us to
seek an alternative that performs input normalization in a
way that is differentiable and does not require the analysis
of the entire training set after every parameter update.
Some of the previous approaches (e.g. (Lyu & Simoncelli,
2008)) use statistics computed over a single training exam-
ple, or, in the case of image networks, over different feature
maps at a given location. However, this changes the repre-
sentation ability of a network by discarding the absolute
scale of activations. We want to a preserve the information
in the network, by normalizing the activations in a training
example relative to the statistics of the entire training data.
3. Normalization via Mini-Batch Statistics
Since the full whitening of each layer’s inputs is costly, we
make two necessary simplifications. The first is that instead
of whitening the features in layer inputs and outputs jointly,
we will normalize each scalar feature independently, by
making it have zero mean and unit variance. For a layer
with d-dimensional input x = (x(1) . . . x(d)), we will nor-
malize each dimension
x̂(k) =
x(k) − E[x(k)]√
Var[x(k)]
where the expectation and variance are computed over the
training data set. As shown in (LeCun et al., 1998b), such
normalization speeds up convergence, even when the fea-
tures are not decorrelated.
Note that simply normalizing each input of a layer may
change what the layer can represent. For instance, nor-
malizing the inputs of a sigmoid would constrain them to
the linear regime of the nonlinearity. To address this, we
make sure that the transformation inserted in the network
can represent the identity transform. To accomplish this,
we introduce, for each activation x(k), a pair of parameters
γ(k), β(k), which scale and shift the normalized value:
y(k) = γ(k)x̂(k) + β(k).
These parameters are learned along with the original model
parameters, and restore the representation power of the net-
work. Indeed, by setting γ(k) =
√
Var[x(k)] and β(k) =
E[x(k)], we could recover the original activations, if that
were the optimal thing to do.
In the batch setting where each training step is based on
the entire training set, we would use the whole set to nor-
malize activations. However, this is impractical when us-
ing stochastic optimization. Therefore, we make the sec-
ond simplification: since we use mini-batches in stochas-
tic gradient training, each mini-batch produces estimates
of the mean and variance of each activation. This way,
the statistics used for normalization can fully participate in
the gradient backpropagation. Note that the use of mini-
batches is enabled by computation of per-dimension vari-
ances rather than joint covariances; in the joint case, reg-
ularization would be required since the mini-batch size is
likely to be smaller than the number of activations being
whitened, resulting in singular covariance matrices.
Consider a mini-batch B of sizem. Since the normalization
is applied to each activation independently, let us focus on
a particular activation x(k) and omit k for clarity. We have
m values of this activation in the mini-batch,
B = {x1...m}.
Let the normalized values be x̂1...m, and their linear trans-
formations be y1...m. We refer to the transform
BNγ,β : x1...m → y1...m
as the Batch Normalizing Transform. We present the BN
Transform in Algorithm 1. In the algorithm,  is a constant
added to the mini-batch variance for numerical stability.
The BN transform can be added to a network to manip-
ulate any activation. In the notation y = BNγ,β(x), we
indicate that the parameters γ and β are to be learned, but
it should be noted that the BN transform does not inde-
pendently process the activation in each training example.
Rather, BNγ,β(x) depends both on the training example
and the other examples in the mini-batch. The scaled and
Batch Normalization
Input: Values of x over a mini-batch: B = {x1...m};
Parameters to be learned: γ, β
Output: {yi = BNγ,β(xi)}
µB ←
1
m
m∑
i=1
xi // mini-batch mean
σ2B ←
1
m
m∑
i=1
(xi − µB)2 // mini-batch variance
x̂i ←
xi − µB√
σ2B + 
// normalize
yi ← γx̂i + β ≡ BNγ,β(xi) // scale and shift
Algorithm 1: Batch Normalizing Transform, applied to
activation x over a mini-batch.
shifted values y are passed to other network layers. The
normalized activations x̂ are internal to our transformation,
but their presence is crucial. The distributions of values
of any x̂ has the expected value of 0 and the variance of
1, as long as the elements of each mini-batch are sampled
from the same distribution, and if we neglect . This can be
seen by observing that
∑m
i=1 x̂i = 0 and
1
m
∑m
i=1 x̂
2
i = 1,
and taking expectations. Each normalized activation x̂(k)
can be viewed as an input to a sub-network composed of
the linear transform y(k) = γ(k)x̂(k) + β(k), followed by
the other processing done by the original network. These
sub-network inputs all have fixed means and variances, and
although the joint distribution of these normalized x̂(k) can
change over the course of training, we expect that the intro-
duction of normalized inputs accelerates the training of the
sub-network and, consequently, the network as a whole.
During training we need to backpropagate the gradient of
loss ` through this transformation, as well as compute the
gradients with respect to the parameters of the BN trans-
form. We use chain rule, as follows:
∂`
∂x̂i
= ∂`∂yi · γ
∂`
∂σ2B
=
∑m
i=1
∂`
∂x̂i
· (xi − µB) · −12 (σ
2
B + )
−3/2
∂`
∂µB
=
∑m
i=1
∂`
∂x̂i
· −1√
σ2B+
∂`
∂xi
= ∂`∂x̂i ·
1√
σ2B+
+ ∂`
∂σ2B
· 2(xi−µB)m +
∂`
∂µB
· 1m
∂`
∂γ =
∑m
i=1
∂`
∂yi
· x̂i
∂`
∂β =
∑m
i=1
∂`
∂yi
Thus, BN transform is a differentiable transformation that
introduces normalized activations into the network. This
ensures that as the model is training, layers can continue
learning on input distributions that exhibit less internal co-
variate shift, thus accelerating the training. Furthermore,
the learned affine transform applied to these normalized ac-
tivations allows the BN transform to represent the identity
transformation and preserves the network capacity.
3.1. Training and Inference with Batch-Normalized
Networks
To Batch-Normalize a network, we specify a subset of ac-
tivations and insert the BN transform for each of them, ac-
cording to Alg. 1. Any layer that previously received x
as the input, now receives BN(x). A model employing
Batch Normalization can be trained using batch gradient
descent, or Stochastic Gradient Descent with a mini-batch
size m > 1, or with any of its variants such as Adagrad
(Duchi et al., 2011). The normalization of activations that
depends on the mini-batch allows efficient training, but is
neither necessary nor desirable during inference; we want
the output to depend only on the input, deterministically.
For this, once the network has been trained, we use the
normalization
x̂ =
x− E[x]√
Var[x] + 
using the population, rather than mini-batch, statistics. Ne-
glecting , these normalized activations have the same
mean 0 and variance 1 as during training. We use the unbi-
ased variance estimate Var[x] = mm−1 · EB[σ
2
B], where the
expectation is over training mini-batches of size m and σ2B
are their sample variances. Using moving averages instead,
we can track the accuracy of a model as it trains. Since the
means and variances are fixed during inference, the nor-
malization is simply a linear transform applied to each ac-
tivation. It may further be composed with the scaling by
γ and shift by β, to yield a single linear transform that re-
places BN(x). Algorithm 2 summarizes the procedure for
training batch-normalized networks.
3.2. Batch-Normalized Convolutional Networks
Batch Normalization can be applied to any set of activa-
tions in the network. Here, we focus on transforms that
consist of an affine transformation followed by an element-
wise nonlinearity:
z = g(Wu + b)
where W and b are learned parameters of the model, and
g(·) is the nonlinearity such as sigmoid or ReLU. This
formulation covers both fully-connected and convolutional
layers. We add the BN transform immediately before the
nonlinearity, by normalizing x = Wu + b. We could have
also normalized the layer inputs u, but since u is likely
the output of another nonlinearity, the shape of its distri-
bution is likely to change during training, and constraining
its first and second moments would not eliminate the co-
variate shift. In contrast, Wu + b is more likely to have
a symmetric, non-sparse distribution, that is “more Gaus-
Batch Normalization
Input: Network N with trainable parameters Θ;
subset of activations {x(k)}Kk=1
Output: Batch-normalized network for inference, NinfBN
1: NtrBN ← N // Training BN network
2: for k = 1 . . .K do
3: Add transformation y(k) = BNγ(k),β(k)(x(k)) to
NtrBN (Alg. 1)
4: Modify each layer in NtrBN with input x
(k) to take
y(k) instead
5: end for
6: Train NtrBN to optimize the parameters
Θ ∪ {γ(k), β(k)}Kk=1
7: NinfBN ← NtrBN // Inference BN network with frozen
// parameters
8: for k = 1 . . .K do
9: // For clarity, x ≡ x(k), γ ≡ γ(k), µB ≡ µ(k)B , etc.
10: Process multiple training mini-batches B, each of
size m, and average over them:
E[x]← EB[µB]
Var[x]← mm−1 EB[σ
2
B]
11: In NinfBN, replace the transform y = BNγ,β(x) with
y = γ√
Var[x]+
· x+
(
β − γ E[x]√
Var[x]+
)
12: end for
Algorithm 2: Training a Batch-Normalized Network
sian” (Hyvärinen & Oja, 2000); normalizing it is likely to
produce activations with a stable distribution.
Note that, since we normalize Wu + b, the bias b can be
ignored since its effect will be canceled by the subsequent
mean subtraction (the role of the bias is subsumed by β in
Alg. 1). Thus, z = g(Wu + b) is replaced with
z = g(BN(Wu))
where the BN transform is applied independently to each
dimension of x = Wu, with a separate pair of learned pa-
rameters γ(k), β(k) per dimension.
For convolutional layers, we additionally want the normal-
ization to obey the convolutional property – so that differ-
ent elements of the same feature map, at different locations,
are normalized in the same way. To achieve this, we jointly
normalize all the activations in a mini-batch, over all lo-
cations. In Alg. 1, we let B be the set of all values in a
feature map across both the elements of a mini-batch and
spatial locations – so for a mini-batch of sizem and feature
maps of size p× q, we use the effective mini-batch of size
m′ = |B| = m ·p q. We learn a pair of parameters γ(k) and
β(k) per feature map, rather than per activation. Alg. 2 is
modified similarly, so that during inference the BN trans-
form applies the same linear transformation to each activa-
tion in a given feature map.
3.3. Batch Normalization enables higher learning rates
In traditional deep networks, too high a learning rate may
result in the gradients that explode or vanish, as well as get-
ting stuck in poor local minima. Batch Normalization helps
address these issues. By normalizing activations through-
out the network, it prevents small changes in layer parame-
ters from amplifying as the data propagates through a deep
network. For example, this enables the sigmoid nonlin-
earities to more easily stay in their non-saturated regimes,
which is crucial for training deep sigmoid networks but has
traditionally been hard to accomplish.
Batch Normalization also makes training more resilient to
the parameter scale. Normally, large learning rates may in-
crease the scale of layer parameters, which then amplify
the gradient during backpropagation and lead to the model
explosion. However, with Batch Normalization, backprop-
agation through a layer is unaffected by the scale of its pa-
rameters. Indeed, for a scalar a,
BN(Wu) = BN((aW )u)
and thus ∂BN((aW )u)∂u =
∂BN(Wu)
∂u , so the scale does not af-
fect the layer Jacobian nor, consequently, the gradient prop-
agation. Moreover, ∂BN((aW )u)∂(aW ) =
1
a ·
∂BN(Wu)
∂W , so larger
weights lead to smaller gradients, and Batch Normalization
will stabilize the parameter growth.
We further conjecture that Batch Normalization may lead
the layer Jacobians to have singular values close to 1, which
is known to be beneficial for training (Saxe et al., 2013).
Consider two consecutive layers with normalized inputs,
and the transformation between these normalized vectors:
ẑ = F (x̂). If we assume that x̂ and ẑ are Gaussian and un-
correlated, and that F (x̂) ≈ J x̂ is a linear transformation
for the given model parameters, then both x̂ and ẑ have unit
covariances, and I = Cov[̂z] = JCov[x̂]JT = JJT . Thus,
J is orthogonal, which preserves the gradient magnitudes
during backpropagation. Although the above assumptions
are not true in reality, we expect Batch Normalization to
help make gradient propagation better behaved. This re-
mains an area of further study.
4. Experiments
4.1. Activations over time
To verify the effects of internal covariate shift on train-
ing, and the ability of Batch Normalization to combat it,
we considered the problem of predicting the digit class on
the MNIST dataset (LeCun et al., 1998a). We used a very
simple network, with a 28x28 binary image as input, and
3 fully-connected hidden layers with 100 activations each.
Each hidden layer computes y = g(Wu + b) with sigmoid
nonlinearity, and the weights W initialized to small ran-
dom Gaussian values. The last hidden layer is followed
Batch Normalization
10K 20K 30K 40K 50K
0.7
0.8
0.9
1
 
 
Without BN
With BN
−2
0
2
−2
0
2
(a) (b) Without BN (c) With BN
Figure 1. (a) The test accuracy of the MNIST network trained with
and without Batch Normalization, vs. the number of training
steps. Batch Normalization helps the network train faster and
achieve higher accuracy. (b, c) The evolution of input distribu-
tions to a typical sigmoid, over the course of training, shown as
{15, 50, 85}th percentiles. Batch Normalization makes the distri-
bution more stable and reduces the internal covariate shift.
by a fully-connected layer with 10 activations (one per
class) and cross-entropy loss. We trained the network for
50000 steps, with 60 examples per mini-batch. We added
Batch Normalization to each hidden layer of the network,
as in Sec. 3.1. We were interested in the comparison be-
tween the baseline and batch-normalized networks, rather
than achieving the state of the art performance on MNIST
(which the described architecture does not).
Figure 1(a) shows the fraction of correct predictions by the
two networks on held-out test data, as training progresses.
The batch-normalized network enjoys the higher test accu-
racy. To investigate why, we studied inputs to the sigmoid,
in the original network N and batch-normalized network
NtrBN (Alg. 2) over the course of training. In Fig. 1(b,c) we
show, for one typical activation from the last hidden layer
of each network, how its distribution evolves. The distribu-
tions in the original network change significantly over time,
both in their mean and the variance, which complicates the
training of the subsequent layers. In contrast, the distri-
butions in the batch-normalized network are much more
stable as training progresses, which aids the training.
4.2. ImageNet classification
We applied Batch Normalization to a new variant of the In-
ception network (Szegedy et al., 2014), trained on the Im-
ageNet classification task (Russakovsky et al., 2014). The
network has a large number of convolutional and pooling
layers, with a softmax layer to predict the image class, out
of 1000 possibilities. Convolutional layers use ReLU as the
nonlinearity. The main difference to the network described
in (Szegedy et al., 2014) is that the 5×5 convolutional lay-
ers are replaced by two consecutive layers of 3×3 convolu-
tions with up to 128 filters. The network contains 13.6 ·106
parameters, and, other than the top softmax layer, has no
fully-connected layers. We refer to this model as Incep-
tion in the rest of the text. The training was performed on
a large-scale, distributed architecture (Dean et al., 2012),
using 5 concurrent steps on each of 10 model replicas, us-
ing asynchronous SGD with momentum (Sutskever et al.,
2013), with the mini-batch size of 32. All networks are
evaluated as training progresses by computing the valida-
tion accuracy @1, i.e. the probability of predicting the cor-
rect label out of 1000 possibilities, on a held-out set, using
a single crop per image.
In our experiments, we evaluated several modifications of
Inception with Batch Normalization. In all cases, Batch
Normalization was applied to the input of each nonlinear-
ity, in a convolutional way, as described in section 3.2,
while keeping the rest of the architecture constant.
4.2.1. ACCELERATING BN NETWORKS
Simply adding Batch Normalization to a network does not
take full advantage of our method. To do so, we applied the
following modifications:
Increase learning rate. In a batch-normalized model, we
have been able to achieve a training speedup from higher
learning rates, with no ill side effects (Sec. 3.3).
Remove Dropout. We have found that removing Dropout
from BN-Inception allows the network to achieve higher
validation accuracy. We conjecture that Batch Normal-
ization provides similar regularization benefits as Dropout,
since the activations observed for a training example are
affected by the random selection of examples in the same
mini-batch.
Shuffle training examples more thoroughly. We enabled
within-shard shuffling of the training data, which prevents
the same examples from always appearing in a mini-batch
together. This led to about 1% improvement in the valida-
tion accuracy, which is consistent with the view of Batch
Normalization as a regularizer: the randomization inherent
in our method should be most beneficial when it affects an
example differently each time it is seen.
Reduce the L2 weight regularization. While in Inception
an L2 loss on the model parameters controls overfitting, in
modified BN-Inception the weight of this loss is reduced
by a factor of 5. We find that this improves the accuracy on
the held-out validation data.
Accelerate the learning rate decay. In training Inception,
learning rate was decayed exponentially. Because our net-
work trains faster than Inception, we lower the learning rate
6 times faster.
Remove Local Response Normalization While Inception
and other networks (Srivastava et al., 2014) benefit from it,
we found that with Batch Normalization it is not necessary.
Reduce the photometric distortions. Because batch-
normalized networks train faster and observe each train-
ing example fewer times, we let the trainer focus on more
“real” images by distorting them less.
Batch Normalization
5M 10M 15M 20M 25M 30M
0.4
0.5
0.6
0.7
0.8
Inception
BN−Baseline
BN−x5
BN−x30
BN−x5−Sigmoid
Steps to match Inception
Figure 2. Single crop validation accuracy of Inception and its
batch-normalized variants, vs. the number of training steps.
Model Steps to 72.2% Max accuracy
Inception 31.0 · 106 72.2%
BN-Baseline 13.3 · 106 72.7%
BN-x5 2.1 · 106 73.0%
BN-x30 2.7 · 106 74.8%
BN-x5-Sigmoid 69.8%
Figure 3. For Inception and the batch-normalized variants,
the number of training steps required to reach the maximum
accuracy of Inception (72.2%), and the maximum accuracy
achieved by the network.
4.2.2. SINGLE-NETWORK CLASSIFICATION
We evaluated the following networks, all trained on the
LSVRC2012 training data, and tested on the validation
data:
Inception: the network described at the beginning of Sec-
tion 4.2, trained with the initial learning rate of 0.0015.
BN-Baseline: Same as Inception with Batch Normalization
before each nonlinearity.
BN-x5: Inception with Batch Normalization and the mod-
ifications in Sec. 4.2.1. The initial learning rate was in-
creased by a factor of 5, to 0.0075. The same learning rate
increase with original Inception caused the model parame-
ters to reach machine infinity.
BN-x30: Like BN-x5, but with the initial learning rate
0.045 (30 times that of Inception).
BN-x5-Sigmoid: Like BN-x5, but with sigmoid nonlinear-
ity g(t) = 11+exp(−x) instead of ReLU. We also attempted
to train the original Inception with sigmoid, but the model
remained at the accuracy equivalent to chance.
In Figure 2, we show the validation accuracy of the net-
works, as a function of the number of training steps. Incep-
tion reached the accuracy of 72.2% after 31 · 106 training
steps. The Figure 3 shows, for each network, the number of
training steps required to reach the same 72.2% accuracy,
as well as the maximum validation accuracy reached by the
network and the number of steps to reach it.
By only using Batch Normalization (BN-Baseline), we
match the accuracy of Inception in less than half the num-
ber of training steps. By applying the modifications in
Sec. 4.2.1, we significantly increase the training speed of
the network. BN-x5 needs 14 times fewer steps than Incep-
tion to reach the 72.2% accuracy. Interestingly, increasing
the learning rate further (BN-x30) causes the model to train
somewhat slower initially, but allows it to reach a higher
final accuracy. This phenomenon is counterintuitive and
should be investigated further. BN-x30 reaches 74.8% af-
ter 6 · 106 steps, i.e. 5 times fewer steps than required by
Inception to reach 72.2%.
We also verified that the reduction in internal covari-
ate shift allows deep networks with Batch Normalization
to be trained when sigmoid is used as the nonlinearity,
despite the well-known difficulty of training such net-
works. Indeed, BN-x5-Sigmoid achieves the accuracy of
69.8%. Without Batch Normalization, Inception with sig-
moid never achieves better than 1/1000 accuracy.
4.2.3. ENSEMBLE CLASSIFICATION
The current reported best results on the ImageNet Large
Scale Visual Recognition Competition are reached by the
Deep Image ensemble of traditional models (Wu et al.,
2015) and the ensemble model of (He et al., 2015). The
latter reports the top-5 error of 4.94%, as evaluated by the
ILSVRC test server. Here we report a test error of 4.82%
on test server. This improves upon the previous best re-
sult, and exceeds the estimated accuracy of human raters
according to (Russakovsky et al., 2014).
For our ensemble, we used 6 networks. Each was based
on BN-x30, modified via some of the following: increased
initial weights in the convolutional layers; using Dropout
(with the Dropout probability of 5% or 10%, vs. 40% for
the original Inception); and using non-convolutional Batch
Normalization with last hidden layers of the model. Each
network achieved its maximum accuracy after about 6 ·106
training steps. The ensemble prediction was based on the
arithmetic average of class probabilities predicted by the
constituent networks. The details of ensemble and multi-
crop inference are similar to (Szegedy et al., 2014).
We demonstrate in Fig. 4 that batch normalization allows
us to set new state-of-the-art on the ImageNet classification
challenge benchmarks.
Batch Normalization
Model Resolution Crops Models Top-1 error Top-5 error
GoogLeNet ensemble 224 144 7 - 6.67%
Deep Image low-res 256 - 1 - 7.96%
Deep Image high-res 512 - 1 24.88 7.42%
Deep Image ensemble up to 512 - - - 5.98%
MSRA multicrop up to 480 - - - 5.71%
MSRA ensemble up to 480 - - - 4.94%*
BN-Inception single crop 224 1 1 25.2% 7.82%
BN-Inception multicrop 224 144 1 21.99% 5.82%
BN-Inception ensemble 224 144 6 20.1% 4.82%*
Figure 4. Batch-Normalized Inception comparison with previous state of the art on the provided validation set comprising 50000 images.
*Ensemble results are test server evaluation results on the test set. The BN-Inception ensemble has reached 4.9% top-5 error on the
50000 images of the validation set. All other reported results are on the validation set.
5. Conclusion
We have presented a novel mechanism for dramatically ac-
celerating the training of deep networks. It is based on the
premise that covariate shift, which is known to complicate
the training of machine learning systems, also applies to
sub-networks and layers, and removing it from internal ac-
tivations of the network may aid in training. Our proposed
method draws its power from normalizing activations, and
from incorporating this normalization in the network archi-
tecture itself. This ensures that the normalization is appro-
priately handled by any optimization method that is being
used to train the network. To enable stochastic optimiza-
tion methods commonly used in deep network training, we
perform the normalization for each mini-batch, and back-
propagate the gradients through the normalization param-
eters. Batch Normalization adds only two extra parame-
ters per activation, and in doing so preserves the represen-
tation ability of the network. We presented an algorithm
for constructing, training, and performing inference with
batch-normalized networks. The resulting networks can be
trained with saturating nonlinearities, are more tolerant to
increased training rates, and often do not require Dropout
for regularization.
Merely adding Batch Normalization to a state-of-the-art
image classification model yields a substantial speedup in
training. By further increasing the learning rates, remov-
ing Dropout, and applying other modifications afforded by
Batch Normalization, we reach the previous state of the
art with only a small fraction of training steps – and then
beat the state of the art in single-network image classifica-
tion. Furthermore, by combining multiple models trained
with Batch Normalization, we perform better than the best
known system on ImageNet, by a significant margin.
Our method bears similarity to the standardization layer of
(Gülçehre & Bengio, 2013), though the two address dif-
ferent goals. Batch Normalization seeks a stable distribu-
tion of activation values throughout training, and normal-
izes the inputs of a nonlinearity since that is where match-
ing the moments is more likely to stabilize the distribution.
On the contrary, the standardization layer is applied to the
output of the nonlinearity, which results in sparser acti-
vations. We have not observed the nonlinearity inputs to
be sparse, neither with nor without Batch Normalization.
Other notable differences of Batch Normalization include
the learned scale and shift that allow the BN transform
to represent identity, handling of convolutional layers, and
deterministic inference that does not depend on the mini-
batch.
In this work, we have not explored the full range of possi-
bilities that Batch Normalization potentially enables. Our
future work includes applications of our method to Recur-
rent Neural Networks (Pascanu et al., 2013), where the in-
ternal covariate shift and the vanishing or exploding gradi-
ents may be especially severe, and which would allow us
to more thoroughly test the hypothesis that normalization
improves gradient propagation (Sec. 3.3). More study is
needed of the regularization properties of Batch Normal-
ization, which we believe to be responsible for the im-
provements we have observed when Dropout is removed
from BN-Inception. We plan to investigate whether Batch
Normalization can help with domain adaptation, in its tra-
ditional sense – i.e. whether the normalization performed
by the network would allow it to more easily generalize to
new data distributions, perhaps with just a recomputation
of the population means and variances (Alg. 2). Finally,
we believe that further theoretical analysis of the algorithm
would allow still more improvements and applications.
Acknowledgments
We thank Vincent Vanhoucke and Jay Yagnik for help and
discussions, and the reviewers for insightful comments.
Batch Normalization
References
Bengio, Yoshua and Glorot, Xavier. Understanding the dif-
ficulty of training deep feedforward neural networks. In
Proceedings of AISTATS 2010, volume 9, pp. 249–256,
May 2010.
Dean, Jeffrey, Corrado, Greg S., Monga, Rajat, Chen, Kai,
Devin, Matthieu, Le, Quoc V., Mao, Mark Z., Ranzato,
Marc’Aurelio, Senior, Andrew, Tucker, Paul, Yang, Ke,
and Ng, Andrew Y. Large scale distributed deep net-
works. In NIPS, 2012.
Desjardins, Guillaume and Kavukcuoglu, Koray. Natural
neural networks. (unpublished).
Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. J. Mach. Learn. Res., 12:2121–2159, July
2011. ISSN 1532-4435.
Gülçehre, Çaglar and Bengio, Yoshua. Knowledge matters:
Importance of prior information for optimization. CoRR,
abs/1301.4083, 2013.
He, K., Zhang, X., Ren, S., and Sun, J. Delving Deep
into Rectifiers: Surpassing Human-Level Performance
on ImageNet Classification. ArXiv e-prints, February
2015.
Hyvärinen, A. and Oja, E. Independent component analy-
sis: Algorithms and applications. Neural Netw., 13(4-5):
411–430, May 2000.
Jiang, Jing. A literature survey on domain adaptation of
statistical classifiers, 2008.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
based learning applied to document recognition. Pro-
ceedings of the IEEE, 86(11):2278–2324, November
1998a.
LeCun, Y., Bottou, L., Orr, G., and Muller, K. Efficient
backprop. In Orr, G. and K., Muller (eds.), Neural Net-
works: Tricks of the trade. Springer, 1998b.
Lyu, S and Simoncelli, E P. Nonlinear image representation
using divisive normalization. In Proc. Computer Vision
and Pattern Recognition, pp. 1–8. IEEE Computer Soci-
ety, Jun 23-28 2008. doi: 10.1109/CVPR.2008.4587821.
Nair, Vinod and Hinton, Geoffrey E. Rectified linear units
improve restricted boltzmann machines. In ICML, pp.
807–814. Omnipress, 2010.
Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.
On the difficulty of training recurrent neural networks.
In Proceedings of the 30th International Conference on
Machine Learning, ICML 2013, Atlanta, GA, USA, 16-
21 June 2013, pp. 1310–1318, 2013.
Povey, Daniel, Zhang, Xiaohui, and Khudanpur, San-
jeev. Parallel training of deep neural networks with
natural gradient and parameter averaging. CoRR,
abs/1410.7455, 2014.
Raiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep
learning made easier by linear transformations in per-
ceptrons. In International Conference on Artificial Intel-
ligence and Statistics (AISTATS), pp. 924–932, 2012.
Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan,
Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpa-
thy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg,
Alexander C., and Fei-Fei, Li. ImageNet Large Scale
Visual Recognition Challenge, 2014.
Saxe, Andrew M., McClelland, James L., and Ganguli,
Surya. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. CoRR,
abs/1312.6120, 2013.
Shimodaira, Hidetoshi. Improving predictive inference un-
der covariate shift by weighting the log-likelihood func-
tion. Journal of Statistical Planning and Inference, 90
(2):227–244, October 2000.
Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A
simple way to prevent neural networks from overfitting.
J. Mach. Learn. Res., 15(1):1929–1958, January 2014.
Sutskever, Ilya, Martens, James, Dahl, George E., and Hin-
ton, Geoffrey E. On the importance of initialization and
momentum in deep learning. In ICML (3), volume 28 of
JMLR Proceedings, pp. 1139–1147. JMLR.org, 2013.
Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,
Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du-
mitru, Vanhoucke, Vincent, and Rabinovich, Andrew.
Going deeper with convolutions. CoRR, abs/1409.4842,
2014.
Wiesler, Simon and Ney, Hermann. A convergence analysis
of log-linear training. In Shawe-Taylor, J., Zemel, R.S.,
Bartlett, P., Pereira, F.C.N., and Weinberger, K.Q. (eds.),
Advances in Neural Information Processing Systems 24,
pp. 657–665, Granada, Spain, December 2011.
Wiesler, Simon, Richard, Alexander, Schlüter, Ralf, and
Ney, Hermann. Mean-normalized stochastic gradient for
large-scale deep learning. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing, pp.
180–184, Florence, Italy, May 2014.
Wu, Ren, Yan, Shengen, Shan, Yi, Dang, Qingqing, and
Sun, Gang. Deep image: Scaling up image recognition,
2015.

