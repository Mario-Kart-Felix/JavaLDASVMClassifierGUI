Combining Labeled and Unlabeled Data with Co-Training* 
Avrim Blum 
School of Computer Science 
Carnegie Mellon University 
Pittsburgh, PA 15213-3891 
avrim+Qcs.cmu.edu 
Abstract 
We consider the problem of using a large unla- 
beled sample to boost performance of a learn- 
ing algorit,hrn when only a small set of labeled 
examples is available. In particular, we con- 
sider a problem setting motivated by the task 
of learning to classify web pages, in which the 
description of each example can be partitioned 
into two distinct views. For example, the de- 
scription of a web page can be partitioned into 
the words occurring on that page, and the words 
occurring in hyperlinks t,hat point to that page. 
We assume that either view of the example 
would be sufficient for learning if we had enough 
labeled data, but our goal is to use both views 
together to allow inexpensive unlabeled data 
to augment, a much smaller set of labeled ex- 
amples. Specifically, the presence of two dis- 
tinct views of each example suggests strategies 
in which two learning algorithms are trained 
separately on each view, and then each algo- 
rithm’s predictions on new unlabeled exam- 
ples are used to enlarge the training set of the 
other. Our goal in this paper is to provide a 
PAC-style analysis for this setting, and, more 
broadly, a PAC-style framework for the general 
problem of learning from both labeled and un- 
labeled data. We also provide empirical results 
on real web-page data indicating that this use 
of unlabeled examples can lead to significant 
improvement of hypotheses in practice. 
*This research was supported in part by the DARPA 
HPKB program under contract F30602-97-1-0215 and by 
NSF National Young investigator grant CCR-9357793. 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. TO copy 
otherwise, to republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
COLT 98 Madison WI USA 
Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 
92 
Tom Mitchell 
School of Computer Science 
Carnegie Mellon University 
Pittsburgh, PA 15213-3891 
mitchell+@cs.cmu.edu 
1 INTRODUCTION 
In many machine learning settings, unlabeled examples 
are significantly easier to come by than labeled ones 
[4, 151. One example of this is web-page classification. 
Suppose that we want a program to electronically visit 
some web site and download all the web pages of interest’ 
to us, such as all the CS faculty member pages, or all 
the course home pages at some university [l]. To train 
such a system to automatically classify web pages, one 
would typically rely on hand labeled web pages. These 
labeled examples are fairly expensive to obtain because 
they require human effort. In contrast, the web has 
hundreds of millions of unlabeled web pages that can be 
inexpensively gathered using a web crawler. Therefore, 
we would like our learning algorithm to be able to take 
as much advantage of the unlabeled data as possible. 
This web-page learning problem has an interesting 
feature. Each example in this domain can naturally be 
described using several different “kinds” of information. 
One kind of information about a web page is the text 
appearing on the document itself. A second kind of 
information is the anchor text attached to hyperlinks 
pointing to this page, from other pages on the web. 
The two problem characteristics mentioned above 
(availability of both labeled and unlabeled data, and 
the availability of two different “kinds” of information 
about examples) suggest the following learning strat- 
egy. Using an initial small set of labeled examples, find 
weak predictors based on each kind of information; for 
instance, we might find that the phrase “research inter- 
ests” on a web page is a weak indicator that the page is 
a faculty home page, and we might find that the phrase 
“my advisor” on a link is an indicator that the page 
being pointed to is a faculty page. Then, attempt to 
bootstrap from these weak predictors using ,unlabeled 
data. For instance, we could search for pages pointed 
to with links having the phrase “my advisor” and use 
them as “probably positive” examples to further train a 
learning algorithm based on the words on the text page, 
and vice-versa. We call this type of bootstrapping co- 
trazning, and it has a close connection to bootstrapping 
from incomplete data in the Expectation-Maximization 
setting; see, for instance, [5, 131. The question this raises 
is: is there any reason to believe co-training will help? 
Our goal is to address this question by developing a 
PAC’-st,yle theoretical framework t,o better underst,and 
the issues involved in this approach. We also give some 
preliminary prnpirical results on classifying univrrsit,y 
web pages (se<‘ Section 6) that are encouraging in this 
context 
More broadly. the general question of how unlabeled 
examples (‘at1 I_)P used to augment labeled data seems a 
slippery one from the point of view of standard PAC as- 
sumptions. We address this issue by proposing a notion 
of “compatibility” between a data distribut,ion and a 
target funct,ion (Se&on 2) and discuss how this relates 
to other approaches to combining labeled and unlabeled 
data (Section 3). 
2 A FORMAL FRAMEWORK 
We define the co-training tnodel as follows. We have an 
instance space 4Y = X1 x X2, where X1 and X2 corre- 
spond to two different “views” of an example. That is, 
each example .c is given as a pair (~1, ~2). We assume 
that each view in itself is sufficient for correct classifi- 
cation. Specifically, let ‘D be a distribution over X, and 
let C’1 and c’:! be concept classes defined over X1 and 
X2, respectively. What we assume is that all labels on 
examples wit,h non-zero probability under D are consis- 
tent with some target function fl E (31, and are also 
consistent with some target function f2 E Cz. In other 
words, if f denotes the combined target concept over the 
entire example, then for any example .Z = (21, ~2) ob- 
served with label P, we have f(x) = fl(~l) = fi(zz) = a. 
This means in particular that ‘D assigns probability zero 
to any example (~1, ~2) such that Al # fp(zz). 
Why might we expect unlabeled data to be useful for 
amplifying a small labeled sample in this context? We 
can think of t,his question through the lens of the stan- 
dard PAC supervised learning setting as follows. For 
a given distribution 2, over X, we can talk of a target 
function f = (Jl, fz) E Cl x Cg as being “compatible” 
with ‘D if it satisfies the condition that 2) assigns prob- 
ability zero to t#he set of examples (21, ~2) such that 
fl(~l) # fz(~z). That is, the pair (fi, f2) is compatible 
with 23 if fl, fi) and 2) are legal together in our frame- 
work. Notice that even if CL and C’z are large concept 
classes with high complexity in, say, the VC-dimension 
measure, for a given distribution D the set, of compati- 
ble target concepts might be much simpler and smaller. 
Thus, one might hope to be able to use unlabeled ex- 
amples to gain a better sense of which target concepts 
are compatible, yielding information that could reduce 
the number of labeled examples needed by a learning 
algorithm. In general, we might hope to have a t,rade- 
off between the number of unlabeled examples and the 
number of labeled examples needed. 
To illustrate this idea, suppose that X1 = X2 = 
(0, l}” and CI = C2 = “conjunctions over (0, l},,” 
Say that it is known that the first coordinate is rele- 
vant t,o the target concept fl (i.e., if the first coordinate 
of ~1 is 0, then fl(zl) = 0 since fi is a conjunction). 
Then, any unlabeled example (~1, ~2) such t,hat the first 
coordinate of ~1 is zero can be used to produce a (la- 
beled) negative example x2 of fz. Of course, if 2) is an 
Figure 1: Graphs Gz, and Gs. Edges represent examples 
with non-zero probability under 2). Solid edges represent 
examples observed in some finite sample S. Notice that 
given our assumptions, even without seeing any labels the 
learning algorithm can deduce that any two examples be- 
longing to the same connected component in Gs must 
have the same classification. 
“unhelpful” distribution, such as one that has nonzero 
probability only on pairs where x1 = x2, then this may 
give no useful information about f2. However, if x1 and 
x2 are not so tightly correla,ted, then perhaps it does. 
For instance, suppose D is such that 22 is conditionally 
independent of ~1 given the classification. In that case, 
given that x1 has its first component set to 0, x2 is now 
a random negative example of f2, which could be quite 
useful. We explore a generalization of this idea in Sec- 
tion 5, where we show that any weak hypothesis can 
be boosted from unlabeled data if 2, has such a condi- 
tional independence property and if the target class is 
learnable with random classification noise. 
In terms of other PAC-style models, we can think of 
our setting as somewhat in between the uniform distri- 
bution model, in which the distribution is particularly 
neutral, and teacher models [6, 8] in which examples are 
being supplied by a helpful oracle. 
2.1 A BIPARTITE GRAPH 
REPRESENTATION 
One way to look at the co-training problem is to view 
the distribution D as a weighted bipartite graph, which 
we write as GD(X~, X2), or just Gv if X1 and X2 are 
clear from context. The left-hand side of G= has one 
node for each point in X1 and the right-hand side has 
one node for each point in X2. There is an edge (21, x2) 
if and only if the example (~1~22) has non-zero prob- 
ability under 23. We give this edge a weight equal to 
its probability. For convenience, remove any vertex of 
degree 0, corresponding to those views having zero prob- 
ability. See Figure 1. 
In this representation, the “compatible” concepts in 
C are exactly those corresponding to a partition of this 
graph with no cross-edges. One could also reasonably 
define the extent to which a partition is not compati- 
ble as the weight of the cut it induces in G. In other 
93 
words, the tltagrcc, of compatibility of a target funct,ion 
f’ = (f, ~ ,f?) wit,11 3 distribution 23 could be defined as 
a IlllInt1~~r 0 5 p < 1 wtlerc~ y = 1 - Pl,[(~,~.Z,) : 
,f,(.rr) # f’:!(.Q)]. 1 11 n 1s rlit[lt‘r, we assume full cornpat,- 
ibility (p = 1). 
C;iveri n set, of urilnbel~d examples S, we can sim- 
larly defill~~ a graph (:,s as t,he bipartite graph having 
one edge (.r 1 1 ~2) for ~a& (J 1 1 ~2) E S’. Notice, that 
givr,n our assul,lpt iolls, arly t,wo examptcs belonging t,o 
t,hc same corrnec-f,~d component in S’ must’ have the same 
c-lassificat,ioll. For inst,ancc,, t,wo web pages wit,11 t,he ex- 
act, samf~ coritcnt. (the salnc represent ation in the Xl 
vi(w) would corrc3pontl to t#wo cclges wit,ti the same left 
(,ntlpoint and would t tic:rc>forr: be required to have the 
sanle label. 
3 A HIGH LEVEL VIEW AND 
RELATION TO OTHER 
APPROACHES 
In it#s most, general form, what we are proposing to add 
tjo f,he l’A(~’ model is a notion of compatibility between 
a concept and a data distribution. If we then postulate 
t,hat# t,he t,arget concept must be compatible with the dis- 
tribution giver], this allows unlabeled data to reduce the 
class C to the smaller set C’ of functions in C that are 
also compatible with what is known about V. (We can 
think of this as intersect,ing C with a concept class CD 
associated with ‘L, which is part#ially known through the 
unlabeled data observed.) For the co-training scenario, 
t,he specific not,ion of compatibility given in the previous 
section is especially natural; however, one could imag- 
ine postulating other forms of compatibility in other 
set,tings. 
We now discuss relations between our setting and 
ot.her methods that have been used for combining la- 
beled and unlabeled data. 
One st,andard approach to learning with missing val- 
ues (e.g., such as when some of the labels are unknown) 
is the EM algorithm j3]. The EM algorithm is typically 
analyzed under the assumption t,hat the data is gener- 
at,ed according to some simple known parametric model. 
For instance, a cotnrnon assumption is that the posit#ive 
examples are generated according to an n-dimensional 
Gaussian 2?+ centered around the point B+, and nega- 
tive examples are generated according to Gaussian D- 
cerltered around the point B-, where B+ and 8- are 
unknown to the learning algorithm. Examples are gen- 
erated by choosing either a positive point from ;I)+ or 
a negative point from K. each with probability l/2. 
In t.his case, the Bayes-optimal hypothesis is the lin- 
ear separator defined by the hyperplane bisecting and 
o&ogonal t,o the line segment O+tl-. 
This parametric model is less rigid than our “PAC 
with compatibility” sett’ing in the sense that it incor- 
porates noise: even the Bayes-optimal hypothesis is not 
a perfect classifier. On the other hand, it is signifi- 
cantly more restrictive in that, the underlying probabil- 
ity distribution is effectively forced to commit to the 
target conc.ept. If we consider the class C of all lin- 
ear separators, then really only two concepts in C are 
“compatible” with the underlying distribution on un- 
labeled examples: namely. the Bayes-optimal one and 
its negation. In other words, if we knew the underly- 
ing distjributjion, thrn there are only two possible target 
concepts left. Given this view, it is not surprising that 
unlabeled data can be so helpful under this set of as- 
sumptions. Our proposal of a compatibility function 
het#ween a concept and a probability distribution is an 
attempt to more broadly consider dist,ributions that do 
not complet,ety commit to a target function and yet are 
not completely uncommit,ted either. 
A second approach to using unlabeled data, given 
by Yarowsky [IS] in the context of the “word sense dis- 
ambiguation” problem is much closer in spirit to co- 
training, and can Le nicely viewed in our model. The 
problem Yarowsky considers is the following. Many 
words have several quite different, dictionary definitions. 
For instance, “plant” can mean a type of Iife form or 
a factory. Given a text document and an instance of 
the word “plant” in it, the goal of the algorithm is to 
determine which meaning is intended. Yarowsky [15] 
makes use of unlabeled data via the following observa- 
tion: within any fixed document, it is highly likely that 
all instances of a word like “plant” have the same in- 
tended meaning, whichever meaning that happens to be. 
He then uses this observation, together with a learning 
algorithm that learns to make predictions based on local 
context,, to achieve good results with only a few labeled 
examples and many unlabeled ones. 
We can think of Yarowsky’s approach in the context 
of co-training as follows. Each example (an instance of 
the word “plant”) is described using two distinct rep- 
resentations. The first representation is the unique-ID 
of the document that the word is in. The second rep- 
resentation is the local context surrounding the word. 
(For instance, in the bipartite graph view, each node 
on the left represents a document, and its degree is the 
number of instances of “plant” in that document; each 
node on the right represents a different local context.) 
The assumptions that any two instances of “plant” in 
the same document have the same label, and that local 
context is also sufficient for determining a word’s mean- 
ing, are equivalent to our assumption that all examples 
in the same connected component must have the same 
classification. 
4 ROTE LEARNING 
In order to get a feeling for the co-training model, we 
consider in this section the simple problem of rote learn- 
ing. In particular, we consider the case that Cl = 2x1 
and CT2 = 2x’, so all partitions consistent with Z? are 
possible, and we have a learning algorithm that simply 
outputs “I don’t know” on any example whose label it 
cannot deduce from its training data and the compat- 
ibility assumpt,ion. Let (X11 = IX21 = N, and imagine 
that N is a “medium-size” number in the sense that 
gathering O(N) unlabeled examples is feasible but la- 
94 
beling them all is not,.’ In t,his case. given just a sin- 
glc view (i.e., just, t,he .‘il port,ion), WV might need to 
SPP Q(N) I t 1 I a )e PC examples in order t,o cover a substan- 
tial fraction of ‘l?. Spccific~all?;, t,he probability that the 
(m + 1)st example has not, yet been seen is 
C Pr,D[zl]( 1 - Prp[cl])‘“. 
1.IEXl 
If, for instance. each example has t,he same probability 
under D, our rote-learner will need a(,%‘) labeled exam- 
ples in order to achieve low error. 
On the other hand, t,he two views we have of each 
example allow a potentially much smaller number of la- 
beled examples to be used if we have a large unlabeled 
sample. For instance, suppose at one extreme that our 
unlabeled sample contains every edge in the graph GD 
(every example with nonzero probability). In this case, 
our rote-learnr,r will bc c,onfidentJ about, t#he label of a 
new example exactly when it has previously seen a la- 
beled example in the same connected component of Gn. 
Thus, if the connected components in GD are cl, cz, ., 
and have probability mass PI, c),, .) respectively, then 
the probability that given m labeled examples, the la- 
bel of an (777 + 1 )st example cannot be deduced by the 
algorithm is just 
we expect the components in G,s t,o converge to those of 
Gr, as we see more unlabeled examples, based on prop- 
erties of the distribution D. For a given connected com- 
ponent H of GD, let, NH be t,he value of t,he minimum 
cut. of H (t,he minimum, over all cuts of H, of the sum 
of the weights on the edges in the cut,). In other words, 
NIf is the probability t,hat a random example will cross 
this specific minimum cut. Clearly, for our sample S to 
contain a spanning tree of M, and therefore to include 
all of H as one component8, it must, have at least, one 
edge in t,hat minimum cut. ‘I’hus, the expect,ed number 
of unlabeled samples needed for this to occur is at least 
l/a~. Of course, there arc many cuts in H and to have 
a spanning tree one must include at least one edge from 
every cut. Nonetheless, Karger [9] shows that this is 
nearly sufficient as well. Specifically, Theorem 2.1 of [9] 
shows that O((logN)/cuH) unlabeled samples are suffi- 
cient to ensure that a spanning tree is found with high 
probability.” So, if (1 = minH{aH}, then O((logN)/rr) 
unlabeled samples are sufficient to ensure that the num- 
ber of comiected components in our sample is equal to 
the number in D, minimizing the number of labeled ex- 
amples needed. 
c p,(l - Pj)‘” (1) 
C,EGD 
For instance, if the graph Gz, has only k connected 
components, then we can achieve error 6 with at most 
O(k/6) examples. 
More generally, we can use the two views to achieve 
a tradeoff bet,ween the nurnber of labeled and unlabeled 
examples needed. If we consider the graph Gs (the 
graph with one edge for each observed example), we 
can see that as we observe more unlabeled examples, 
the nurnber of connected components will drop as com- 
ponents rnerge together, until finally t#hey are the same 
as the components of GD. Fllrtherrnore, for a given set 
S, if we now select a random subset of m of them to la- 
bel, the probability that, the label of a random (m+ l)st 
example chosen from the remaining portion of S cannot 
be deduced by the algorithm is 
L (,A$, ’ C,EG’S 
where sj is the number of edges in component cj of S. 
If m << ISJ, the above formula is approximately 
in analogy to Equation 1. 
In fact, we can use recent results in the study of ran- 
dom graph processes [9] to describe quantitatively how 
‘T o make this more plausible in the context of web pages, 
think of ~1 as not the document itself but rather some small 
set of attributes of the document. 
For instance, suppose N/2 points in X1 are posi- 
tive and N/2 are negative, and similarly for X2, and 
the distribution 2, is uniform subject to placing zero 
probability on illega.1 examples. In this case, each legal 
example has probability p = 2/N2. To reduce the ob- 
served graph to two connected components we do not 
need Tao see all O(N’) edges, however. All we need are 
two spanning trees. The minimum cut for each compo- 
nent has value pN/2, so by Karger’s result, O(N log N) 
unlabeled examples suffice. (This simple case can be 
analyzed easily from first principles as well.) 
More generally, we can bound the number of con- 
nected components we expect to see (and thus the num- 
ber of labeled examples needed to produce a perfect hy- 
pothesis if we imagine the algorithm is allowed to select 
which unlabeled examples will be labeled) in terms of 
the number of unlabeled examples m, as follows. For a 
given N < 1, consider a greedy process in which any 
cut of value less that cy in G;n has all its edges re- 
moved, and this process is then repeated until no con- 
nected component has such a cut. Let NCC(CY) be the 
number of connected components remaining. If we let 
cy = clog(N)/m,, where c is the constant from Karger’s 
theorem, and if m, is large enough so that there are 
no singleton components (components having no edges) 
remaining after the above process, then NCC(CV) is an 
2Tllis theorem is in a model in which each edge e in- 
dependently appears in the observed graph with probability 
mp,, where pe is the weight of edge e and m is the ex- 
pected number of edges chosen. (Specifically, Karger is con- 
cerned with the network reliability problem in which each 
edge goes “down” independently with some known probabil- 
ity and you want to know the probability that connectivity 
is maintained.) However, it is not hard to convert this to the 
setting we are concerned with, in which a fixed m samples 
are drawn, each independently from the distribution defined 
by the pe’s. In fact, Karger in [lo] handles this conversion 
formally. 
95 
upper bound on the expected number of labeled exam- 
ples needed to cover all of DD. On the other hand. if 
we let u = 1/(2m,), then $Ncc;(cu) is a lower bound 
since the above greedy proce>s must have made at) most 
Ncc - 1 cuts, and for each one t.he expected number of 
edges crossing t,he cut is at most l/2. 
5 LEARNING IN LARGE INPUT 
SPACES 
In the previous section we saw how c-o-t,raining could 
provide a tradeoff‘ between the number of labeled and 
unlabeled examples needed in a setting where IX/ is 
relat,ively small and the algorithm is performing rote- 
learning. We now move to the more difficult case where 
1x1 is large (e.g., X1 = X2 = {O,l}“) and our goal is to 
he polynomial in the description length of the examples 
and the target concept. 
What we show is that given a conditronal indepen- 
dence assumption on the distribut,ion D, if the target 
class is learnable from random classification noise in the 
standard PAC model, then any initial weak predictor 
can be boosted to arbit,rarily high accuracy using unlu- 
belr’d examples only by co-training. 
Specifically, we say that target functions fl, fz and 
dist,ribution % together satisfy the condztzonal indepen- 
dence assumption if, for any fixed (k1,21) E X of non- 
zero probability, 
and similarly, 
In other words, ~1 and x2 are conditionally independent 
given the label. For instance, we are assuming that the 
words on a page P and the words on hyperlinks pointing 
to P are conditionally independent given the classifica- 
tion of P. This seems to be a somewhat plausible start- 
ing point given that the page itself is constructed by a 
different user than the one who made the lmk. On the 
other hand, Theorem 1 below can be viewed as showing 
why this is not really so plausible after all.” 
In order to state the theorem, we define a “weakly- 
useful predictor” h of a function f to be a function such 
that 
“Using our bipartite graph view from Section 2.1, it is 
easy to see that for this distribution ID, the only “compati- 
ble” target functions are the pair (fl, fi), its negation, and 
the all-positive and all-negative functions (assuming ‘D does 
not give probability zero to any example). Theorem 1 can be 
interpreted as showing how, given access t,o ‘D and a slight 
bias towards (fl , f2 ), the unlabeled data can be used in poly- 
nomial time to discover this fact. 
1. PrD h(z 
[ 1 = 11 
2 E, and 
2. Pi-p f(~ 
[ 
,) = Ilh(r) = l] > PrD[j(x) = l] + t, 
for some t > l/poly(71). For example, seeing the word 
“handouts” on a web page would be a weakly-useful pre- 
dictor that the page is a course homepage if (1) “hand- 
outs” appears on a non-negligible fraction of pages, and 
(2) the probability a given page is a course homepage 
given that “handouts” appears is non-negligibly higher 
than t,he probability without that word. If f is unbi- 
ased in the sense that PrD(f(x) = 1) = Pr,(f(x) = 
0) = l/2, then this is the same as t,he usual notion of 
a weak predictor, namely PrD(h(z) = f(x)) 2 l/2 + t. 
Otherwise, it is equivalent (assuming that f is not over- 
whelmingly often 0 or 1) to the statement, that h is 
a weak predictor over the distribution “normalized” to 
make f appear unbiased. 
Theorem 1 If C2 is learnable in the PAC model wath 
clussificatzon noise, and if the conditzonal andependence 
assumption is satzsjied, then (Cl, Cz) is learnable in the 
Co-truining model from unlabeled datu only, given an 
inztial weakly-useful predictor h(zl). 
Thus, for instance, the conditional independence as- 
sumption implies that any concept class learnable in the 
Statistical Query model [l I] is learnable from unlabeled 
data and an initial weakly-useful predictor. 
Before proving the theorem, it will be convenient to 
define a variation on the standard classification noise 
model where the noise rate on positive examples may 
be different from the noise rate on negative examples. 
Specifically, let (CV, /3) classification noise be a setting 
in which true positive examples are incorrectly labeled 
(independently) with probability LY, and true negative 
examples are incorrectly labeled (independently) with 
probability j?. In this case we have the following simple 
lemma: 
Lemma 1 If concept class C is learnable in the classi- 
fication noise model, then it is also learnable with (a, /3) 
classificataon noise so long us 00 + p < 1 (running tzme 
is polynomial in l/(1 - (Y - /3)). 
Proof. First, suppose a and p are known to the learning 
algorithm. Without loss of generality, assume cy < ,0. 
To learn C with (cy, p) noise, simply flip each positive 
label to a negative label independently with probability 
(,8 - a)/(@ + (1 - a)). This results in standard clas- 
sification noise with noise rate Y = /3/(,0 + (1 - a)) = 
P/(2/3 + t), where E = 1 - (a + /3). 
If u and p are not known, this can be dealt with in 
the usual way. For instance, given a data set S of m 
examples of which m+ are labeled positive, we can cre- 
ate m + 1 hypotheses, where hypothesis i (0 5 i 5 m+) 
is produced by flipping the labels on i random positive 
examples in S and running the classification noise al- 
gorithm, and hypothesis j (m+ < j < m) is produced 
by flipping the labels on j random negative examples in 
S and t#hen running the algorithm. These hypotheses 
96 
can then be evaluated on a separate test srt. We expect, 
at, least one hypothesis to he good since t,he protretlurc~ 
when CY and /3 are known can be viewed as a proha.bilit,> 
dist,rihution over these ?n + 1 experiments. I 
The (a, $) classification noise model can be thought 
of as a kind of constant-partition classification noise [a]. 
However, the resu1t.s in [2] require that each noise rate 
be less than l/2. We will need t,he stronger &atement 
presented here, namely that it, suffices t,o a.ssume only 
t#hat the sum of cy and ,!Y is less than 1. 
Proo,f of Theorem 1. Let f(r) he the target concept 
and p = Pr,o(f(z) = 1) be t,hc probability that a ran- 
dom example from D is positive. Let Q = PrD(f(X) = 
l(h(x1) = 1) and let c = Pr,~(h(zl) = 1). So, 
IJrzJ[h(xl) = IIf = I] 
= Pr,~[f(x) = Ilh(zl) = l]Pr2)[h(Z1) = 11 
Pm[f(x) = 11 
_ qc 
I’ 
(2) 
and 
P+(Q) = l/f(z) = o] = *. (3) 
By the conditional independence assumption, for a ran- 
dom example z = (x~,zz), h(zl) is independent of XZ, 
given f(x). Thus, if we use h(zl) as a noisy label of 
x2, t,hen this is equivalent to (CX, p)-classification noise, 
where LY = 1 - ~c/p and ,!? = (1 - q)c/(l - p) using 
equations (2) and (3). The sum of the two noise rates 
satisfies 
ct+p = 1-E-t (1 - cl)c 
P 
___ = I-+& 
1 - JJ 
By t,he assumption that h is a weakly-useful predictor, 
we have c 2 t and q - p 2 t. Therefore, this quantity 
is at most 1 - t’/(p( 1 - p)), which is at most 1 - 4t2. 
Applying Lemma 1, we have the theorem. I 
6 EXPERIMENTS 
In order to test the idea of co-training, we applied it to 
the problem of learning to classify web pages. This par- 
ticular experiment was motivated by a larger research 
effort [l] to apply machine learning to the problem of 
extracting information from the world wide web. 
The data for this experiment4 consists of 1051 web 
pages collected from Computer Science department web 
sites at four universities: Cornell, University of Wash- 
ington, University of Wisconsin, and University of Texas. 
These pages have been hand labeled into a number of 
categories. For our experiments we considered the cat- 
egory “course home page” as the target function; thus, 
course home pages are the positive examples and all 
4’rhis data is available at http://www.cs.cmu.edu/afs/cs/ 
projectftheo-ll/www/wwkb/ 
other pages are negative examples. In this dataset, 22% 
of the web pages were course pages. 
For each example web page x:, WC consider& CL to 
he the bag (multi-set,) of words appearing on the wrb 
page, and 1~2 to be the bag of words underlined in all 
links pointing into the web page from other pages in 
the database. Classifiers were trained separately for SI 
and for x2, using t#he naive Bayes algorithm. We will 
refer to these as the page-based and the hyperlink-based 
classifiers, respectively. This naive Bayes algorithm has 
been empirically observed to be successful for a variety 
of text-categorization tasks [la]. 
The co-training algorit,hm we used is described in 
Table I. Given a set L of labeled examples and a set 
CJ of unlabeled examples, the algorithm first creates a 
smaller pool lf’ containing u unlabeled examples. It 
then iterates the following procedure. First, use I, to 
t,rain two distinct classifiers: hl and ha. hl is a naive 
Bayes classifier based only on the x1 portion of the in- 
stance, and ha is a naive Bayes classifier based only on 
the x2 portion. Second, allow each of these two clas- 
sifiers to examine the unlabeled set U’ and select the 
p examples it most confidently labels as positive, and 
the n examples it most confidently labels negative. We 
used p = 1 and n = 3, to match the ratio of positive to 
negative examples in the underlying data distribution. 
Each example selected in this way is added to L, along 
with the label assigned by the classifier that selected it. 
Finally, the pool U’ is replenished by drawing 2p + 2n 
examples from I/ at random. In earlier implementations 
of Co-training, we allowed hl and h2 to select examples 
directly from the larger set U, but have obtained bet- 
ter results when using a smaller pool U’, presumably 
because this forces hl and h2 to select examples that 
are more representative of the underlying distribution 
V that generated Ii. 
Experiments were conducted to determine whether 
this co-training algorithm could successfully use the un- 
labeled data to outperform standard supervised training 
of naive Bayes classifiers. In each experiment, 263 (25%) 
of the 1051 web pages were first selected at random as 
a test set. The remaining data was used to generate a 
labeled set L containing 3 positive and 9 negative ex- 
amples drawn at random. The remaining examples that 
were not drawn for L were used as the unlabeled pool 
U. Five such experiments were conducted using differ- 
ent training/test splits, with Co-training parameters set 
top = 1, n = 3, k = 30 and u = 75. 
To compare Co-training to supervised training, we 
trained naive Bayes classifiers that used only the 12 la- 
beled training examples in L. We trained a hyperlink- 
based classifier and a page-based classifier, just as for 
co-training. In addition, we defined a third combined 
classifier, based on the outputs from the page-based 
and hyperlink-based classifier. In keeping with the naive 
Bayes assumption of conditional independence, this com- 
bined classifier computes the probability P(cj 1~) of class 
cj given the instance 2 = (~1, ~2) by multiplying the 
probabilities output by the page-based and hyperlink- 
97 
Given: 
l a set, L of labeled training examples 
l a set U of unlabeled examples 
I 
Create a pool U’ of examples by choosing u examples at random from IJ 
Loop for k iterations: 
Use L to train a classifier hl that considers only t,he ~1 portion of z 
Use I, to train a classifier hz that considers only the x2 portion of 1c 
Allow hl to label p positive and n negative examples from U’ 
Allow h2 to label p positive and n negative examples from U’ 
Add these self-labeled examples to I, 
Randomly choose 21, + 2n examples from TJ to replenish 11’ 
Table 1: The Co-Training algorithm. In the experiments reported here both hl and h2 were trained using a naive Bayes 
algorithm, and algorithm parameters were set to p = 1, n = 3, k = 30 and u = 75. 
Page-based classifier Hyperlink-based classifier Combined classifier 
Supervised training 12.9 12.4 11.1 
Co-training 6.2 11..6 5.0 
Table 2: Error rate in percent for classifying web pages as course home pages. The top row shows errors when training 
on only the labeled examples. Bottom row shows errors when co-training, using both labeled and unlabeled examples. 
based classifiers: 
The results of these experiments are summarized in 
Table 2. Numbers shown here are the test set error rates 
averaged over the five random train/test splits. The 
first row of the table shows the test set accuracies for 
the three classifiers formed by supervised learning; the 
second row shows accuracies for the classifiers formed by 
co-training. Note that for this data the default hypoth- 
esis that always predicts “negative” achieves an error 
rate of 22%. Figure 2 gives a plot of error versus num- 
ber of iterations for one of the five runs. 
Notice that for all three types of classifiers (hyperlink- 
based, page-based, and combined), the co-trained clas- 
sifier outperforms the classifier formed by supervised 
training. In fact, the page-based and combined classi- 
fiers achieve error rates that are half the error achieved 
by supervised training. The hyperlink-based classifier is 
helped less by co-training. This may be due to the fact 
that hyperlinks contain fewer words and are less capable 
of expressing an accurate approximation to the target 
function. 
This experiment involves just one data set and one 
target function. Further experiments are needed to de- 
termine the general behavior of the co-training algo- 
rithm, and to determine what exactly is responsible for 
the pattern of behavior observed. However, these re- 
sults do indicate that co-training can provide a useful 
way of taking advantage of unlabeled data. 
7 CONCLUSIONS AND OPEN 
QUESTIONS 
We have described a model in which unlabeled data can 
be used to augment labeled data, based on having two 
views (~1, ~2) of an example that are redundant but not 
completely correlated. Our theoretical model is clearly 
an over-simplification of real-world target functions and 
distributions. In particular, even for the optimal pair of 
functions fl , f2 E Cl x Cz we would expect to occasion- 
ally see inconsistent examples (i.e., examples (X1,22) 
such that $1(x1) # fz(~z)). Nonetheless, it provides a 
way of looking at the notion of the “friendliness” of a 
distribution (in terms of the components and minimum 
cuts) and at how unlabeled examples can potentially 
be used to prune away “incompatible” target concepts 
to reduce the number of labeled examples needed to 
learn. It is an open question to what extent the consis- 
tency constraints in the model and the mutual indepen- 
dence assumption of Section 5 can be relaxed and still 
allow provable results on the utility of co-training from 
unlabeled data. The preliminary experimental results 
presented suggest that this method of using unlabeled 
data has a potential for significant benefits in practice, 
though further studies are clearly needed. 
98 
10 15 20 25 30 35 40 
Co-Training Iterations 
Figure 2: Error versus number of iterations for one run of co-training experiment 
We conjecture that there are many practical learn- 
ing problems t,hat fit or approximately fit the co-training 
model. For example, consider the problem of learning 
to classify segments of television broadcasts [7, 141. We 
might he interested, say, in learning to identify televised 
segments containing the US President. Here Xl could 
be t,he set of possible video images, X2 the set of pos- 
siblc audio signals, and X their cross product. Given 
a small sample of labeled segments, we might learn a 
weakly predictive recognizer hl that spots full-frontal 
images of the president,‘s face, and a recognizer hz that 
spot,s his voice when no background noise is present. 
We could then use co-training applied to the large vol- 
ume of unlabeled television broadcasts, to improve the 
accuracy of both classifiers. Similar problems exist in 
many perception learning tasks involving multiple sen- 
sors. For example, consider a mobile robot that must 
learn to recognize open doorways based on a collection 
of vision (Xl). sonar (X2), and laser range (X3) sen- 
sors. The important structure in, the above problems 
is that each instance 2 can be partitioned into subcom- 
ponents xi, where the xi are not perfectly correlated, 
where each xi can in principle be used on its own to 
make the classification, and where a large volume of 
unlabeled irlstances can easily be collected. 
References 
[l] M. Craven, D. Freitag, A. McCallum, ‘I?. Mitchell, 
K. Nigam, and C.Y. Quek. Learning to extract 
symbolic knowledge from the world wide web. 
Technical report, Carnegie Mellon University, Jan- 
uary 1997. 
[2] S. E. Decatur. PAC learning with constant- 
partition classification noise and applications to de- 
cision tree induction. In Proceedings of the Four- 
teenth International Conference on Machine Learn- 
ang, pages 83~-91, July 1997. 
[3] A.P. Dempster, N.M. Laird, and D.B. Rubin. Max- 
imum likelihood from incomplete data via the EM 
algorithm. Jo,urnal of the Royal Statistacal Socaety 
R, 39:1-38, 1!)77. 
[4] Richard 0. Duda and Peter E. Hart. Pattern Clas- 
sificataon and Scene Analysis. Wiley, 1973. 
[5] Z. Ghahramani and M. I. Jordan. Supervised learn- 
ing from incomplete data via an EM approach. In 
Advances zn Neural Information Processing Sys- 
tems (NIPS 6). Morgan Kauffman, 1994. 
[S] S. A. Goldman and M. J. Kearns. On the complex- 
ity of teaching. Journal of Computer and System 
Sczences, 50( 1):20-31, February 1995. 
[7] A.G. Haupt mann and M.J. Witbrock. Informedia: 
News-on-demand - multimedia information acqui- 
sition and ret,rieval. In M. Maybury, editor, Zntel- 
lzgent Multim.edia Information Retrieval, 1997. 
[8] .J. Jackson a.nd A. Tomkins. A computational 
model of teaching. In Proceedings of the Fifth An- 
nual Workshop on Computational Learning The- 
99 
nry, pages 31%326. Morgan Kaufmann, 1992. 
[9] D. R. Karger. Random sampling in cut, flow, 
and network design problems. In Proceedings of 
the Tu~ent!J-Lkth .-1nnucd ACM ,‘;‘,ymposium on the 
Theory of (‘omp(utzng, pages 648-657, May 1994. 
[lo] D. R. Karger. R itn~ 01x1 sampling in cut, flow, and I 
network design problems. .Journal version draft, 
1997. 
[ll] M. Kearns. Efficient, noise-tolerant learning from 
statistical queries. Irl Proceedings of the Twenty- 
Fifth Annual ;Ic‘M Symposium on Theory of Com- 
puting, pages 392.-40 1. 1993. 
[12] D. D. Lewis and M. Ringuet,te. A comparison of 
two learning algorithms for text categorization. In 
Third Annuul ,s’ym~oszum on ljocument Analyszs 
and Infomatzon Rctrreuul, pages 81-93, 1994. 
[13] Joel Ratsaby and Santosh S. Venkatesh. Learning 
from a mixture of labeled and unlabeled examples 
with parametric side information. In Proceedzngs 
of the 8th Annual Conference on Computational 
Learning Theory, pages 412-417. ACM Press, New 
York, NY, 1995. 
[14] M.J. Witbrock and A.G. Hauptmann. Improving 
acoustic models by watching television. Technical 
R,eport CMU-CS-98-110, Carnegie Mellon Univer- 
sity, March 19 1998. 
[15] D. Yarowsky. Unsupervised word sense disam- 
biguation rivaling supervised methods. In Proceed- 
ings of the .33rd Ann& Meetzng of the Assocza- 
tion for Computational Linguistics, pages 189-196, 
1995. 
100 

