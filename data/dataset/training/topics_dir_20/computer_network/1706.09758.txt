USING SECOND-ORDER HIDDEN MARKOV MODEL TO IMPROVE 
SPEAKER IDENTIFICATION RECOGNITION PERFORMANCE UNDER 
NEUTRAL CONDITION 
 
Ismail Shahin 
 
Electrical/Electronics and Computer Engineering Department 
University of Sharjah, P. O. Box  27272, Sharjah, United Arab Emirates 
E-mail: ismail@sharjah.ac.ae 
 
 
ABSTRACT 
 
In this paper, second-order hidden Markov 
model (HMM2) has been used and implemented 
to improve the recognition performance of text-
dependent speaker identification systems under 
neutral talking condition. Our results show that 
HMM2 improves the recognition performance 
under neutral talking condition compared to the 
first-order hidden Markov model (HMM1). The 
recognition performance has been improved by 
9%. 
 
 
1. INTRODUCTION 
 
The use of hidden Markov model (HMM) for 
speech and speaker recognition has become 
popular in the last two decades. HMM has 
become one of the most successful and broadly 
used modeling technique for speech and speaker 
recognition [1]. 
HMM uses Markov chain to model the 
changing statistical characteristics that exist in 
the actual observations of speech signals. The 
Markov process is a double stochastic process 
where there is an unobservable Markov chain 
defined by a state transition matrix, and where 
each state of the Markov chain is associated 
with either a discrete output probability 
distribution (discrete HMM) or a continuous 
output probability density function (continuous 
HMM) [2]. The theory of Markov chains has 
been applied to speech and speaker recognition 
[3]. 
HMM is a powerful technique in 
optimizing the parameters that are used in 
modeling the speech signals. This optimization 
decreases the computational complexity in the 
decoding procedure and improves the 
recognition accuracy [2]. 
In the last two decades, the majority of 
the work performed in the field of speech and 
speaker recognition on hidden Markov model 
(HMM) has been done using HMM1 [2-7]. 
In this paper, HMM2 is being used and 
implemented to train and test text-dependent 
speaker identification systems under neutral 
talking condition. 
 
2. SECOND-ORDER HIDDEN 
MARKOV MODEL 
 
In HMM1, the underlying state sequence is a 
first-order Markov chain where the stochastic 
process is specified by a 2-D matrix of a priori 
transition probabilities (aij) between states si and 
sj, while in HMM2 the underlying state 
sequence is a second-order Markov chain where 
the stochastic process is specified by a 3-D 
matrix (aijk) [8,9]: 
 
 
with the constraints: 
 
  (1)sq,sqsqProba i2tj1tktijk  
1j,iN1a
N
1k
ijk 

where N is the number of states in the model, 
and qt is the actual state at time t. 
The probability of the state sequence, 
T21 q,...,q,qQ   is defined as: 
 
where i  is the probability of state si at time t = 
1, aij is the probability of the transition from 
state si to state sj at time t = 2, and T is the 
utterance length. 
Each state si is associated with a mixture 
of Gaussian distributions: 
 
where the vector Ot is the input vector at time t. 
Given a sequence of observed vectors, 
,O,...,O,OO T21  the joint state-output 
probability is defined as: 
 
 
 
3.  EXTENDED VITERBI AND BAUM- 
WELCH ALGORITHMS 
 
The most likely state sequence can be found by 
using the probability of the partial alignment 
ending at transition (sj,sk) at times (t-1,t): 
 
 
Recursive computation of the forward 
function t (j,k) is given by: 
 
 
 
The forward function t (j,k) defines the 
probability of the partial observation sequence, 
O1, O2,…, Ot, and the transition (sj,sk) between 
time t-1 and t: 


t (j,k) can be computed from the two transitions 
(si,sj) and (sj,sk) between states si and sk: 
 
The backward function t (i,j) can be 
expressed as: 
 
 
where, t (i, j) is defined as the probability of the 
partial observation sequence from t+1 to T, 
given the model  and the transition (si,sj) 
between times t-1 and t. 
        
  
4. SPEECH DATA BASE 
 
In this research, our speech data base consists of 
twenty different speakers (ten adult males and 
ten adult females) uttering the same utterance 
(10 different isolated-word utterances) nine 
times under neutral talking condition. 
Our speech data base is captured by a speech 
acquisition board using a 10-bit A/D converter 
and sampled at a sampling rate of 8 KHz. The 
speech data base is then preprocessed in order to 
be treated and analyzed easily. 
 
 
 

 



M
1m
im
M
1m
imimtimti
(3)1cwith
,,μ;ONcΔ)(Ob
(4))(Oba
.)(Oba)(ObΠλ)O(Q,Prob
T
3t
tqqqq
2qqq1qq
tt1t2t
22111




(2)aaΠ(Q)Prob
T
3t
qqqqqq t1t2t211 



 
(5)1k,jN2,tT
λO,...,O,O,sq,sq,...,qProb
Δk)(j,δ
t21ktj1t1
t


 
(6)1kj,N3,tT
)(Ob.a.j)(i,δmaxk)(j,δ tkijk1t1iNt

 
 
(7)1k,jN2,tT
λsq,sq,O,...,OProbΔk)(j,α ktj1tt1t


)(81kj,N2,t1T
)(Ob.a.j)(i,αk)(j,α
N
1i
1tkijkt1t




 
(9)1ji,N2,t1T
λ,sq,sqO,...,OProbΔj)(i,β jti1tT1tt


5. RESULTS 
 
In this research, linear predictive coding (LPC) 
cepstral feature analysis is used to form the 
observation vector for HMM1 and HMM2 (left-
to-right model in both HMM1 and HMM2). 
Cepstral based features have been used 
extensively in speech recognition applications 
because they have been shown to outperform 
linear predictive coefficients. Cepstral based 
features attempt to incorporate the nonlinear 
filtering characteristics of the human auditory 
system in the measurement of spectral band 
energies. 
The number of states, N, is equal to 5, 
the number of mixtures, M, is equal to 5 per 
state, with continuous mixture observation 
density is selected in HMM1 and HMM2. 
Our results show that using HMM2 in 
the training and testing phases of text-dependent 
speaker identification systems under neutral 
talking condition improves the recognition 
performance compared to that using HMM1. 
The recognition performance has been 
improved by 9%. 
Table 1 compares the results of the 
recognition performance of text-dependent 
speaker identification systems under neutral 
talking condition when HMM2 is used with that 
when HMM1 is used. 
 
Table 1. Recognition performance under neutral 
talking condition using HMM1 and HMM2 
 
 
6. DISCUSSION AND CONCLUSIONS 
 
In this work, we showed that HMM2 yields high 
speaker identification recognition performance 
under neutral talking condition. Using HMM2 in 
the training and testing phases of text-dependent 
speaker identification systems under neutral 
talking condition improves the recognition 
performance compared to that using HMM1. 
The recognition performance has been improved 
by 9%. 
Despite the success of using HMM1 under 
neutral talking condition, HMM2 is more 
appropriate model than HMM1 for text-
dependent speaker identification systems 
because the underlying state sequence in HMM2 
is a second-order Markov chain where the 
stochastic process is specified by a 3-D matrix, 
while in HMM1 the underlying state sequence is 
a first-order Markov chain where the stochastic 
process is specified by a 2-D matrix. 
A naïve implementation of the recursion for 
the computation of  and  in HMM2 requires 
on the order of N
3
T operations, compared with 
N
2
T operations in HMM1. Therefore, more 
memory space is required in HMM2 than that in 
HMM1. 
 
7. REFERENCES 
 
[1] B. H. Juang and L. R. Rabiner, "Hidden 
Markov models for speech recognition," 
Technometrics, vol. 33, no. 3, pp. 251-272, 
August 1991. 
 
[2] X. D. Huang, Y. Ariki, and M. A. Jack, 
Hidden Markov Models for Speech Recognition, 
Edinburgh University Press, Great Britain, 
1990. 
  
[3] J. Dai, "Isolated word recognition using 
Markov chain models," IEEE Trans. on Speech 
and Audio Processing, vol. 3, no. 6, pp. 458-
463, November 1995. 
 
[4] B. H. Juang and L. R. Rabiner, "Mixture 
autoregressive hidden Markov models for 
speech signals," IEEE Trans. on ASSP, vol. 33, 
no. 6, pp. 1404-1412, December 1985. 
 
[5] L. R. Rabiner, S. E. Levinson, and M. M. 
Sondhi, "On the application of vector 
quantization and hidden Markov models to 
speaker-independent, isolated word 
recognition," The Bell System Technical 
   Model Recognition performance 
   HMM1                90% 
   HMM2                98% 
Journal, vol. 62, no. 4, pp. 1075-1105, April 
1983. 
 
[6] I. Shahin and N. Botros, "Text-dependent 
speaker identification using hidden Markov 
model with stress compensation technique," 
IEEE SOUTHEASTCON '98 Proceedings, 
Orlando, FL, April 1998, pp. 61-64. 
 
[7] L. R. Rabiner, "A Tutorial on hidden 
Markov models and selected applications in 
speech recognition," Proceedings of IEEE, vol. 
77, no. 2, pp. 257-286, February 1989. 
 
[8] J. F. Mari, J. P. Haton, and A. Kriouile, 
"Automatic word recognition based on second-
order hidden Markov models," IEEE Trans. on 
Speech and Audio Processing, vol. 5, no. 1, pp. 
22-25, January 1997. 
 
[9] J. F. Mari, F. D. Fohr, and J. C. Janqua, "A 
second-order HMM for high performance word 
and phoneme-based continuous speech 
recognition," Proceedings IEEE International 
Conference on Acoustics Speech and Signal 
Processing, Atlanta, USA, May 1996, vol. 1, 
pp. 435-438. 

