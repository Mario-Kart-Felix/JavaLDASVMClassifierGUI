Fast Algorithms for Learning
Latent Variables in Graphical Models
Mohammadreza Soltani
Iowa State University
msoltani@iastate.edu
Chinmay Hegde
Iowa State University
chinmay@iastate.edu
ABSTRACT
We study the problem of learning latent variables in Gaussian graph-
ical models. Existing methods for this problem assume that the
precision matrix of the observed variables is the superposition of a
sparse and a low-rank component. In this paper, we focus on the
estimation of the low-rank component, which encodes the effect of
marginalization over the latent variables. We introduce fast, proper
learning algorithms for this problem. In contrast with existing ap-
proaches, our algorithms are manifestly non-convex. We support
their efficacy via a rigorous theoretical analysis, and show that our
algorithms match the best possible in terms of sample complexity,
while achieving computational speed-ups over existing methods.
We complement our theory with several numerical experiments.
1 INTRODUCTION
1.1 Setup
Gaussian graphical models are a popular tool for modeling the in-
teraction of a collection of Gaussian random variables. In Gaussian
graphical models, nodes represent random variables and edges
model conditional (in)dependence among the variables [1]. Over
the last decade, significant efforts have been directed towards al-
gorithms for learning sparse graphical models. Mathematically, let
Σ∗ denote the positive definite covariance matrix of p Gaussian
random variables, and let Θ∗ = (Σ∗)−1 be the corresponding preci-
sion matrix. Then, Θ∗i j = 0 implies that the i
th and jth variables are
conditionally independent given all other variables and the edge
(i, j) does not exist in the underlying graph. The basic modeling
assumption is that Θ∗ is sparse, i.e., such graphs possess only a few
edges. Such models have been fruitfully used in several applications
including astrophysics [2], scene recognition [3–5], and genomic
analysis [6].
Numerous algorithms for sparse graphical model learning – both
statistically as well as computationally efficient – have been pro-
posed in the machine learning literature [7–11]. Unfortunately,
sparsity is a simplistic first-order model and is not amenable to
modeling more complex interactions. For instance, in certain sce-
narios, only some of the random variables are directly observed,
and there could be relevant latent interactions to which we do not
directly have access.
The existence of latent variables poses a significant challenge
in graphical model learning since they can confound an otherwise
sparse graphical model with a dense one. This scenario is illustrated
in Figure 1. Here, nodes with solid circles denote the observed
variables, and solid black edges are the “true" edges in the graphical
model. One can see that the “true" graph is rather sparse. However,
if there is even a single unobserved (hidden) variable denoted by the
node with the broken red circle, then it will induce dense, apparent
interactions between nodes that are otherwise disconnected; these
are denoted by the dotted black lines.
A flexible and elegant method to learn latent variables in graphi-
cal models was proposed by [12]. At its core, the method imposes a
superposition structure in the observed precision matrix as the sum
of sparse and low-rank matrices, i.e., Θ∗ = S∗ + L∗. Here, Θ∗, S∗,L∗
are p×p matrices where p is the number of variables. The matrix S∗
specifies the conditional observed precision matrix given the latent
variables, while L∗ encodes the effect of marginalization over the
latent variables. The rank of L∗, r , is equal to the number of latent
variables and we assume that r is much smaller than p.
To learn such a superposition model, [12] propose a regularized
maximum-likelihood estimation framework, with ℓ1-norm and nu-
clear norm penalties as regularizers; these correspond to convex
relaxations of the sparsity and rank constraints, respectively. Us-
ing this framework, they prove that such graphical models can be
learned with merely n = O(pr ) random samples. However, this
statistical guarantee comes at a steep computational price; the
framework involves solving a semidefinite program (SDP) with p2
variables and is computationally very challenging. Several subse-
quent works [13, 14] have attempted to provide faster algorithms,
but all known (provable) methods involve at least cubic worst-case
running time.
1.2 Our contributions
In this paper, we provide a new class of fast algorithms for learning
latent variables in Gaussian graphical models. Our algorithms are
(i) provably statistically efficient: they achieve the optimal sample
complexity of latent variable learning; (ii) provably computationally
efficient: they are linearly convergent, and their per-iteration time
is close to the best possible.
We clarify the above claims using some notation. Suppose that
we observe samples X1,X2, . . . ,Xn
i .i .d∼ N(0, Σ) where each Xi ∈
Rp . Let C = 1n
∑n
i=1 XiX
T
i denote the sample covariance matrix,
and Θ∗ = (Σ∗)−1 denote the true precision matrix; as above, we
assume that Θ∗ = S∗ + L∗. We will exclusively function in the high-
dimensional regime where n ≪ p2. Our sole focus is on learning
the low-rank part from samples; i.e., we pre-suppose that the sparse
part, S∗ is a known positive definite matrix, while the low-rank
part, L∗ is unknown (with rank r ≪ p). This models the situation
where the“true" edges between the observed variables are known
a priori, but there exists latent interaction among these variables
that needs to be discovered.
We estimate L∗ in the high-dimensional regime where n ≪ p2 by
attempting to solve a non-convex optimization problem, following
the formulation of [15]. We label this as LVM, short for Latent
ar
X
iv
:1
70
6.
08
93
6v
1 
 [
st
at
.M
L
] 
 2
7 
Ju
n 
20
17
p − 1
p
1
2
3
4
5
h
Figure 1: Illustration of effects of latent variable in graphi-
cal model learning. Solid edges represent “true" conditional
dependence, while dotted edges represent apparent depen-
dence due to the presence of the latent variable h.
Variable Gaussian Graphical Modeling:
min
L
F (L) = − log det(S∗ + L) + ⟨S∗ + L,C⟩
s.t. rank(L) ≤ r , L ⪰ 0.
(1)
Above, ⟨., .⟩ denotes the standard Frobenius inner product in
matrix space, ⪰ 0 denotes membership in the positive semi-definite
(psd) cone, and the objective function F (L) denotes the negative log
likelihood of the samples. Problem (1) is highly non-convex due to
the rank constraint, rank(L) ≤ r . Moreover, the log-det loss function
is highly nonlinear and challenging to handle. As mentioned earlier,
most known (provable) methods solve a convex relaxation of this
problem, but suffer from high computational costs.
In contrast, we solve (1) without convex relaxation. Specifically,
we propose two non-convex algorithms for solving (1). Our first
algorithm, that we call LVM with Exact Projections, or EP-LVM,
performs (non-convex) projected gradient descent on the objective
function F (L), together with the low-rank and psd constraints. This
algorithm yields sample-optimal results, but its running time can be
cubic, Ω(p3), in the number of variables. Our second algorithm, that
we call LVM with Approximate Projections, or AP-LVM, performs
a variant of projected gradient descent with (deliberately) inaccu-
rate projections onto the constraints. Interestingly, this algorithm
also yields sample-optimal results, and its running time is nearly
quadratic, Õ(p2), in the dimension p for a fixed number of latent
variables. To the best of our knowledge, this is the fastest universal1
algorithm for solving (1).
Both our proposed algorithms enjoy the following benefits:
Sample efficiency. For both algorithms, the sample complexity
(i.e., number of samples in order to achieve a desired estimation
error ϑ ) for learning a rank-r latent variable model in p variables
scales as n = O(pr ), and this matches those of the best available
methods.
1The running time of some other existing algorithms achieve similar scaling in p ,
but also depend adversely on matrix properties such as condition number and/or the
minimum singular value.
Linear convergence.We provide rigorous analysis to show that
both our proposed algorithms enjoy global linear convergence with
no specific initialization step.
Proper learning. Our algorithms are examples of proper learn-
ing methods, in the sense that their output is a rank-r estimate of
the true latent variable model. In contrast, methods based on con-
vex relaxation often fail to do this and return a high-rank estimate,
thus potentially having a negative effect on interpretability of the
discovered latent variables.
1.3 Techniques
Our first algorithm is a variant of the singular value projection
approach of [19], with an extra psd projection step. Our second
algorithm is a variant of approximate subspace-IHT [20] and uses a
careful combination of approximate singular value decomposition
techniques. While our proposed methods are structurally similar to
these previously proposed methods, their analysis is considerably
different; we elaborate on this below.
Our technique for establishing linear convergence of our first
algorithm (EP-LVM) is based on bounding the restricted strong
convexity/smoothness (RSC/RSS) constants [21] of the objective
function F (L) in (1). The key observation is that F (L) is globally
strongly convex, and when restricted to any compact psd cone,
it also satisfies strong smoothness. The above analysis technique
is fairly standard [17]. However, unlike in previously considered
scenarios, the RSS/RSC constants of F (L) are harder to bound. These
may vary across iterations, and depend on several properties of the
true precision matrix Θ∗. Therefore, additional effort is required to
establish global linear convergence.
As a byproduct of this analysis, we show that with n = O(pr )
independent samples, EP-LVM returns an estimate up to constant
error. Moreover, we show that EP-LVM provides the best empirical
performance (in terms of estimation error) among all considered
methods.
However, since EP-LVM performs an exact eigenvalue decom-
position (EVD) per iteration, its overall running time can be slow
since it incurs cubic per-iteration running time. Our second algo-
rithm (AP-LVM) resolves this issue. The basic idea is to replace
exact EVDswith approximate low-rank projections in each iteration.
However, it is known [22] that a straightforward replacement of all
EVDs with approximate low-rank projections within non-convex
projected gradient descent is not a successful strategy.
In order to guarantee convergence, we use a careful combination
of tail and head approximate low-rank projections in each itera-
tion [20]. This enables us to improve the overall running time to
Õ(p2r ). Moreover, the statistical accuracy matches that of EP-LVM
in theory (up to constants) as well as in practice (negligible loss in
performance). We show that both EP-LVM and AP-LVM provide
better empirical performance than convex methods.
Table 1 provides a summary of the theoretical properties of our
methods, and contrasts them with other existing methods for latent
variable modeling.
2 RELATION TO PRIORWORK
Learning graphical models in high dimensional settings have been
of special interest, and most existing works assume some sort
2
Table 1: Summary of our contributions, and comparison with existing methods. Here, γ =
√
σr
σr+1 − 1 represents the spectral
gap parameter in intermediate iterations. The overall running time of the ADMM approach is marked as poly(p) since the
precise rate of convergence is unknown.
Algorithm Reference Running Time Spectral dependency Output rank
SDP [12] poly(p) Yes ≫ r
ADMM [13] poly(p) Yes ≫ r
QUIC & DIRTY [16] Õ (p3) Yes ≫ r
SVP [17] Õ (p3) No ≫ r
Factorized [18] Õ (p2r/γ ) Yes r
EP-LVM This paper Õ
(
p3
)
No r
AP-LVM This paper Õ (p2r ) No r
of low-dimensional structure on the covariance or precision ma-
trix [23–26]. Among all the structured models, sparse graphical
model learning has received the most attention. The typical ap-
proach for learning sparse graphical models is to obtain a regular-
ized maximum likelihood (ML) estimate given the observations. The
work of [27] establishes the statistical efficiency of the regularized
ML estimate. Parallel to such statistical analysis is the development
of efficient computational techniques for solving the regularized
ML estimate [7, 8, 28].
To capture latent interactions between variables, more complex
structures (beyond mere sparsity) are necessary. [12] propose a
superposition of sparse and low-rank structure in the precision
matrix. To solve this latent variable problem, they introduce an
extra nuclear norm term to the regularized ML objective function
as a convex surrogate of the rank. However, they propose using
a generic semi-definite programming (SDP) solver, which is very
cumbersome for even moderate size problem. Subsequently, the
authors in [13] have proposed Alternating Direction Method of
Multipliers (ADMM) which scales to relatively large size problems.
The work of [14, 16] also consider general superposition structures
in precision matrix estimation. However, the running time of these
methods is still cubic in the number of variables (p2).
Problem (1) is an instance of the more general problem of low-
rank matrix recovery. Broadly, three classes of approaches for low-
rank matrix recovery exist. The first (and most popular) class is
based on convex relaxation [13, 14, 29]; while their statistical prop-
erties are well-established, such methods often suffer from high
computational costs.
Methods in the second class of approaches are fundamentally
non-convex, and are based on the approach of [30]. In these algo-
rithms, the psd rank-r matrix L is factorized as L = UUT , where
U ∈ Rp×r . Using this idea removes the difficulties caused by the
non-convex rank constraint; however, the objective function is not
convex anymore and proving convergence remains tricky. Nev-
ertheless, under certain conditions, such methods succeed and
have recently gained in popularity in the machine learning litera-
ture [18, 31–35]. In general, these methods need a careful spectral
initialization that usually involves a full singular value decompo-
sition, and their convergence depends heavily on the condition
number of the input as well as other spectral properties of the true
low-rank component.
The third class of methods are also non-convex. Unlike the sec-
ond class, they do not factorize the optimization variable, L, but
instead use low-rank projections within the classical gradient de-
scent framework. This approach was introduced by [19] for matrix
recovery from linear measurements, and was later modified for
general M-estimation problems with well-behaved objective func-
tions [17]. In principle, the approach of [17] can be used to solve (1)
(using a similar analysis of the RSS/RSC constants as in this paper.)
However, it is an improper learning algorithm, and the rank of the
estimate of L∗ is several times larger than the target rank. More-
over, each iteration is computationally expensive, since it involves
computing an SVD in each iteration.
Our contributions in this paper fall under the third category. We
first propose an iterative PSD projection algorithm similar to that
of [19] but for the specific problem stated in (1). We then accelerate
this algorithm (with no loss in statistical performance) using the
approximate low-rank projections method of [20].
3 PRELIMINARIES
Throughout this paper, the minimum and maximum eigenvalues of
the sparse matrix S∗ will be denoted by Sp and S1 respectively. We
use ∥A∥2 and ∥A∥F for spectral norm and Frobenius norm of a ma-
trixA, respectively. We denoteAr as the best rank-r approximation
(in Frobenius norm) of a given matrix A. In addition, λ1(A), λp (A)
denote the maximum and minimum eigenvalues of A ∈ Rp×p re-
spectively. For any subspace U ⊂ Rp×p , we denote PU as the
orthogonal projection operator ontoU .
Our analysis will rely upon on the following definition [17, 21,
36].
Definition 3.1. A function f satisfies the Restricted Strong Con-
vexity (RSC) and Restricted Strong Smoothness (RSS) conditions if
for all L1,L2 ∈ Rp×p such that rank(L1) ≤ r , rank(L2) ≤ r :
mr
2
∥L2 − L1∥2F ≤ f (L2) − f (L1) − ⟨∇f (L1),L2 − L1⟩
≤ Mr
2
∥L2 − L1∥2F , (2)
wheremr andMr are called the RSC and RSS constants respectively.
3
ALGORITHM 1: EP-LVM
Input: Matrices S∗ and C , rank r , step size η.
Output: Estimates L̂, Θ̂ = S∗ + L̂.
Initialization: L0 ← 0, t ← 0;
repeat
Lt+1 = P+r
(
Lt − η∇F (Lt )
)
;
t ← t + 1;
until t ≤ T ;
We denote Ur as the set of all rank-r matrix subspaces, i.e.,
subspaces of Rp×p that are spanned by any r atoms of the form
uvT where u,v ∈ Rp are unit-norm vectors.
We will also employ the idea of head and tail projection intro-
duced by [22], and instantiated in the context of low-rank approxi-
mation by [20].
Definition 3.2 (Approximate tail projection). Let cT > 1 be a
constant. Then T : Rp×p → Ur is a cT -approximate tail projection
algorithm if for all L ∈p×p , T returns a subspaceW = T(L) that
satisfies: ∥L − PW L∥F ≤ cT ∥L − Lr ∥F .
Definition 3.3 (Approximate head projection). Let 0 < cH < 1 be
a constant. ThenH : Rp×p → Ur is a cH-approximate head pro-
jection if for all L ∈p×p , the returned subspace V = H(L) satisfies:
∥PV L∥F ≥ cH ∥Lr ∥F .
4 ALGORITHMS AND ANALYSIS
First, we present our projected gradient-descent algorithm to solve 1.
This algorithm provides the best sample complexity (both theoreti-
cal and empirical) among all existing approaches. Our algorithm,
that we call LVM with exact projections (EP-LVM), is described in
pseudocode form as Alg 1.
In Alg (1), the exact projection step, P+r (·) denotes projection
onto the space of rank-r psd matrices. This is implemented through
performing an exact eigenvalue decomposition (EVD) of the argu-
ment and selecting the nonnegative eigenvalues and corresponding
eigenvectors [37]. The gradient of the objective function F (L) in (1)
can be calculated as:
∇F (L) = −(S∗ + L)−1 +C = −Θ−1 +C . (3)
Since L is a low-rank matrix with rank r , it can be factorized as
L = UUT for some U ∈ Rp×r . Hence, to calculate efficiently the
inverse in (3), we utilize the low-rank structure of L by applying
the Woodbury matrix identity:
(S∗ + L)−1 = S∗−1 − S∗−1U
(
I +UT S∗−1U
)−1
UT S∗−1.
We now provide our first main theoretical result, supporting the
statistical and computational efficiency of EP-LVM. In particular,
we derive an upper bound on the estimation error of the low-rank
matrix at each iteration (Please see [38] for all the proofs).
Theorem 4.1 (Linear convergence of EP-LVM). Assume that
the objective function F (L) satisfies the RSC/RSS conditions with con-
stant M3r andm3r . Let Jt denote the subspace formed by the span
of the column spaces of the matrices Lt ,Lt+1, and L∗. In addition,
assume that 1 ≤ M3rm3r ≤
2√
3
. Choose step size as 0.5M3r ≤ η ≤
1.5
m3r .
Then, EP-LVM outputs a sequence of estimates Lt such that:
∥Lt+1 − L∗∥F ≤ ρ∥Lt − L∗∥F + 2η∥PJt ∇F (L
∗)∥F , (4)
where ρ = 2
√
1 +M23rη
2 − 2m3rη < 1.
We will show below that the second term on the right hand
side of this inequality is upper-bounded by an arbitrarily small
constant with sufficient number of samples. Also, the first term
decreases exponentially with iteration count. Overall, after T =
O
(
log1/ρ
(
∥L∗ ∥F
ϑ
))
iterations, we obtain an upper-bound of O(ϑ )
on the total estimation error, indicating linear convergence.
Next, we provide bounds on the RSS/RSC constants of F (L),
justifying the assumptions made in Theorem 4.1.
Theorem 4.2 (Bounding RSC/RSS constants.). Let the number
of samples scaled as n = O
(
1
δ 2
(
η
1−ρ
)2
rp
)
for some small constant
δ > 0 and ρ defined above. Also, assume that
Sp ≤ S1 ≤
√
2
√
3
Sp −
(
1 +
√
r
)
∥L∗∥2 − δ .
Then, the loss function F (L) satisfies RSC/RSS conditions with con-
stantsm3r andM3r that satisfy the assumptions of Theorem 4 in each
iteration.
The above theorem states that convergence of our method is
guaranteed when the eigenvalues of S∗ are roughly of the same
magnitude, and large when compared to the spectral norm of L∗.
We believe that this is merely a sufficient condition arising from
our proof technique, and our numerical evidence shows that the
algorithm succeeds for more general S∗ and L∗.
Time complexity. Each iteration of EP-LVM needs a full EVD,
which requires cubic running time. Since the total number of itera-
tions is logarithmic, the overall running time scales as Õ(p3).
For large p, the cubic running time of EP-LVM can be very
challenging. To alleviate this issue, one can instead attempt to
replace the full EVD in each iteration with an ε-approximate low-
rank psd projection; it is known that such projections can computed
in O(p2 logp) time [39]. However, a naïve replacement of the EVD
with an ε-approximate low-rank projection method does not lead
to algorithms with rigorous convergence guarantees.2
Instead, we use a combination of approximate tail and head
projections, as suggested in [20]. The high level idea is that the
use of two inaccurate low-rank projections instead of one, if done
carefully, will balance out the errors and will result in provable
convergence. The full algorithm, that we call LVM with approximate
projections (AP-LVM), is described in pseudocode form as Alg. 2.
Note that we do not impose a psd projection within every iter-
ation. If an application requires a psd matrix as the output (i.e., if
proper learning is desired), then we can simply post-process the
final estimate LT by retaining the nonnegative eigenvalues (and
corresponding eigenvectors) through an exact EVD. We note that
this EVD can be done only once, and is applied to the final output
of Alg 2. This is itself a rank-r matrix, therefore leaving the overall
asymptotic running time unchanged.
2Indeed, algorithms with only “tail-approximate" projections can be shown to get
stuck at a solution arbitrarily far from the true estimate, even with a large number of
samples; see Section 2 of [22].
4
ALGORITHM 2: AP-LVM
Input: Matrices S∗ and C , rank r , step size η.
Output: Estimates L̂, Θ̂ = S∗ + L̂.
Initialization: L0 ← 0, t ← 0;
repeat
Lt+1 = T
(
Lt − ηH
(
∇F (Lt )
) )
;
t ← t + 1;
until t ≤ T ;
The choice of approximate low-rank projections is flexible, as
long as the approximate tail and head projection guarantees are
satisfied. We note that tail-approximate low-rank projection algo-
rithms are widespread in the literature [40–42]; however, head-
approximate projection algorithms (or at least, algorithms with
head guarantees) are less common.
We focus on the randomized Block Krylov SVD method (BK-
SVD) method of [43]. BK-SVD generates a rank-r subspace approx-
imating the top right r singular vectors of a given input matrix.
Moreover, for constant approximation factors, its running time is
Õ(p2r ), independent of any spectral properties of the input matrix.
Formally, let A ∈ Rn×n be a given matrix and let Ar denote its best
rank-r approximation. Then BK-SVD generates a matrix B = ZZTA
which is the projection of A onto the column space of matrix Z
with orthonormal vectors z1, z2, . . . , zr . Moreover, with probability
99/100, we have:
∥A − B∥F ≤ cT ∥A −Ar ∥F . (5)
where cT > 1 is the tail projection constant. Equation (5) is equiva-
lent to the tail approximation guarantee according to Definition 3.2.
In addition to (5), [43] also provide the so-called per vector ap-
proximation guarantee for BK-SVD with probability 99/100:
|uTi AA
Tui − ziAAT zi | ≤ (1 − cH)σ 2r+1,
where ui are the right eigenvectors of A and cH < 1 is the head
projection constant. [20] show that the above property implies the
head approximation guarantee:
∥B∥F ≥ cH ∥Ar ∥F .
Therefore, in AP-LVM we invoke the BK-SVD method for both
head and tail projections3. Using BK-SVD as the approximate low-
rank projection method of choice, we now provide our second
main theoretical result supporting the statistical and computational
efficiency of AP-LVM.
Theorem 4.3 (Linear convergence). Assume that the objective
function F (L) satisfies the RSC/RSS conditions with constants M2r
and m2r . In addition, assume that 1 ≤
M22r
m22r
≤ 11−ρ20
where ρ0 =
1
1+cT −
√
1 − η20 and η0 =
(
cHm −
√
1 +M22r − 2m2r
)
. Choose step
size as
1−ρ20
M2r ≤ η ≤
1+ρ20
m2r . LetVt be the subspace returned by the head
3We note that since the BK-SVD algorithm is randomized while our definitions of
tail and head guarantees are deterministic. Fortunately, the running time of BK-SVD
depends only logarithmically on the failure probability of the algorithm, and therefore
a union bound argument over the iterations of AP-LVM is required to precisely prove
algorithmic correctness.
approximate projectionH(·) applied to the gradient. Then, for any
t > 0, AP-LVM outputs a sequence of estimates Lt that satisfy:
∥Lt+1 − L∗∥F ≤ ρ1∥Lt − L∗∥F + ρ2∥PVt ∇F (L
∗)∥F , (6)
where ρ1 =
(√
1 +M22rη
2 − 2m2rη +
√
1 − η20
)
(1 + cT ) and ρ2 =(
η0√
1−η20
+ 1
)
(1 + cT ).
A similar calculation as before shows that AP-LVM converges
after T = O
(
log
(
∥L∗ ∥F
ϑ
))
iterations. AP-LVM is structurally simi-
lar to the approximate subspace-IHT algorithm of [20]. However,
their proofs are specific to least-squares loss functions. On the
other hand, the loss function F (L) for recovering latent variables
is complicated4 and in general the RSS/RSC constants can vary
across iterations. Therefore, considerable effort is needed to prove
algorithm convergence. First, we provide conditions under which
the assumption of RSC/RSS in Theorem 4.3 are satisfied.
Theorem 4.4 (Bounding RSC/RSS constants). Let n scaled as
n = O
(
1
δ ′2
(
ρ2
1−ρ1
)2
rp
)
for some small constant δ ′ > 0, with ρ1 and
ρ2 as defined in theorem 4.3. Also, assume that,
∥L∗∥2 ≤
1
1 +
√
r
(
Sp
1 +
√
1 − ρ20
(7)
−
S1
√
1 − ρ20
1 +
√
1 − ρ20
− c3ρ2
1 − ρ1
√
rp
n
)
.
Finally, assume that:
Sp ≤ S1 ≤
1√
1 − ρ20
(Sp − a′) −
(
1 +
√
r
)
∥L∗∥2 − δ ′
where 0 < a′ ≤
(
1 +
√
r
)
∥L∗∥2 + δ ′ for some δ ′ > 0. Then, the loss
function F (L) satisfies RSC/RSS conditions with constantsm2r and
M2r that satisfy the assumptions of Theorem 4.3 in each iteration.
Theorem 4.4 specifies a family of true precision matrices Θ∗ =
S∗ + L∗ that can be provably estimated using our approach with an
optimal number of samples. Note that since we do not perform psd
projection within AP-LVM, it is possible that some of the eigenval-
ues of Lt are negative. Next, we show that with high probability,
the absolute value of the minimum eigenvalue of Lt is small.
Theorem 4.5. Under the assumptions in Theorem 4.4 on L∗, if
we use AP-LVM to generate a rank r matrix Lt for all t = 1, . . . ,T ,
then with high probability the minimum eigenvalue of Lt satisfies:
λp (Lt ) ≥ −a′ where 0 < a′ ≤
(
1 +
√
r
)
∥L∗∥2 + c3ρ21−ρ1
√
rp
n .
Time complexity. Each iteration of AP-LVM needs a head and
a tail projection on the rank 2r and rank r matrices respectively.
According to [43], these operations takes k ′ = O
(
p2r logp√
ε
)
for er-
ror ε . Since the total number of iterations is once again logarithmic,
the overall running time scales as Õ(p2r ).
4Indeed, F (L) is notwell-defined everywhere, e.g. at matrices L that have large negative
eigenvalues.
5
The above analysis shows that our proposed algorithms for dis-
covering latent variables are linearly convergent.We now show that
they converge to the true underlying low-rank matrix. The quality
of the estimates in Theorems 4.1 and 4.3 is upper-bounded by the
gradient terms ∥PJt ∇F (L∗)∥F and ∥PVt ∇F (L∗)∥F in (4) and (6), re-
spectively, within each iteration. The following theorem bounds
these gradient terms in terms of the number of observed samples.
Theorem 4.6. Under the assumptions of Theorem 4.1, for any fixed
t we have:
∥PJt ∇F (L
∗)∥F ≤ c2
√
rp
n
, (8)
Similarly, under the assumptions of Theorem 4.3,
∥PVt ∇F (L
∗)∥F ≤ c3
√
rp
n
, (9)
Both hold with probability at least 1− 2 exp(−p) where c2, c3 > 0 are
absolute constants.
Sample complexity. Plugging in the upper bounds in (8) and (9)
into Theorems 4.2 and 4.4, the sample complexity of both algorithms
scales as n = O(pr ) to achieve constant estimation error. This
matches the number of degrees of freedom of a p × p matrix with
rank r .
5 EXPERIMENTS
We provide a range of numerical experiments supporting our pro-
posed algorithms and comparing with existing convex approaches.
Our comparisons is with the regularized maximum likelihood ap-
proach of [12], which we solve using CVX [44]. The second al-
gorithm that we have used is a modification of the ADMM-type
method proposed by [13]. We assume that our algorithms are pro-
vided with the rank parameter r , and have manually tuned step-
sizes/regularization parameters of all algorithms to achieve best
possible performance.
Synthetic data. we use a diagonal matrix with positive values
for the (known) sparse part, S∗. For a given number of observed
variables p, we set r = 5% as the number of latent variables. We
then follow the method proposed in [13] for generating the sparse
and low-rank components S∗ and L∗. For simplicity, we impose
the sparse component to be psd by forcing it to be diagonal. All
reported results on synthetic data are the average of 5 indepen-
dent Monte-Carlo trials. Our observations comprise n samples,
x1,x2, . . . ,xn
i .i .d∼ N(0, (S∗ + L∗)−1). In our experiments, we used
a full SVD as projection step in EP-LVM. (Due to numerical stability,
we use SVD rather than EVD.) For AP-LVM, we compare two ver-
sions: AP-LVM(1) denotes the use of BK-SVD for the approximate
tail and head projections, while AP-LVM(2) denotes the use of the
more well-known (but spectrum-dependent) Lanczos method for
these projections.
In the first experiment, we set p = 100, n = 400p, and r = 5.
Table 2 lists several metrics that we use for algorithm comparison.
An algorithm terminates if it satisfies one of two conditions: the
evaluated objective function in the estimated L in each iteration falls
below the true negative log likelihood (NLL) (i.e., F (L∗)), or the total
number of iterations exceeds 600. From Table 2, we see that both
EP-LVM, AP-LVM(1) and AP-LVM(2) produce better estimates of L
compared to ADMM and CVX, with AP-LVM(1) and AP-LVM(2)
having the edge in running time and EP-LVM having the edge
in accuracy. Note that the convex methods strictly produce an
estimate of rank larger than 5 (indicating that they are improper
learning methods). As anticipated, the total running time with
CVX is much larger than other algorithms. Finally, the estimated
objective function for our proposed algorithms is very close to the
optimal (true) objective function compared to ADMM and CVX.
We increase the dimension to p = 1000 and reported the same
metrics in Table 3 similar to Table 2. Since CVX cannot solve the
problem with size p = 1000, we did not report its results. Again,
we get the same conclusions as Table 2; however, the improvement
obtained by AP-LVM in terms of running time is considerably
magnified.
In addition, Tables 4 and 5 shows the same experiment discussed
in Tables 2 and 3 but for small number of samples, n = 50p.
In Figures 2 (a) and (b), we graphically compare four algorithms
in terms of the relative error of the estimated L in Frobenius norm
versus the “oversampling" ratio n/p. In this experiment, we fixed
p = 100 in (a) and p = 1000 in (b) and vary n. We observe that
EP-LVM, AP-LVM(1), and AP-LVM(2) estimate the low-rank matrix
even for the regime where n is very small, whereas both ADMM
and CVX does not produce very meaningful results.
Real data.We evaluate our methods through the Rosetta gene
expression data set [45]. This data set includes 301 samples with
6316 variables. We run the ADMM algorithm by [13] with p = 1000
variables and obtained an estimate of the sparse component S∗.
Then we used S∗ as the input for EP-LVM, AP-LVM(1) and AP-
LVM(2). The target rank for all three algorithms is set to be the same
as that returned by ADMM. In Figure 2 plot (c), we illustrate the
NLL for these three algorithms versus wall-clock time (in seconds)
over 50 iterations. We observe that all three algorithms demonstrate
linear convergence, as predicted in the theory. Among the three
algorithms, AP-LVM(1) obtains the quickest rate of decrease of the
objective function.
REFERENCES
[1] M. Wainwright and M. Jordan. Graphical models, exponential families, and
variational inference. Foundations and Trends® in Machine Learning, 1(1–2):1–
305, 2008.
[2] N. Padmanabhan, M. White, H. Zhou, and R. O’Connell. Estimating sparse
precisionmatrices. Monthly Notices of the Royal Astronomical Society, 460(2):1567–
1576, 2016.
[3] N. Souly and M. Shah. Scene labeling using sparse precision matrix. In IEEE
Conf. Comp. Vision and Pattern Recog, pages 3650–3658, 2016.
[4] S. Minaee, A. Abdolrashidi, and Y. Wang. Iris recognition using scattering trans-
form and textural features. In IEEE Sig. Proc. and Sig. Proc. Education Workshop
(SP/SPE), pages 37–42. IEEE, 2015.
[5] S. Minaee and Y. Wang. Fingerprint recognition using translation invariant
scattering network. In IEEE Sig. Proc. Med. Bio. Symposium (SPMB), pages 1–6.
IEEE, 2015.
[6] J. Yin and H.e Li. Adjusting for high-dimensional covariates in sparse precision
matrix estimation by âĎŞ1-penalization. Journal of multivariate analysis, 116:365–
381, 2013.
[7] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation
with the graphical lasso. Biostatistics, 9(3):432–441, 2008.
[8] R. Mazumder and T. Hastie. The graphical lasso: New insights and alternatives.
Electronic journal of statistics, 6:2125, 2012.
[9] O. Banerjee, L. Ghaoui, and A. d’Aspremont. Model selection through sparse
maximum likelihood estimation for multivariate gaussian or binary data. J.
Machine Learning Research, 9(Mar):485–516, 2008.
[10] C. Hsieh, I. Dhillon, P. Ravikumar, and M. Sustik. Sparse inverse covariance
matrix estimation using quadratic approximation. In Adv. Neural Inf. Proc. Sys.
(NIPS), pages 2330–2338, 2011.
6
25 50 100 200 400
n/p
0
0.5
1
1.5
2
2.5
3
R
el
a
ti
v
e
F
ro
b
en
iu
s
E
rr
o
r EP-LVMAP-LVM(1)
AP-LVM(2)
ADMM
SDPCVX
25 50 100 200 400
n/p
0
0.5
1
1.5
2
2.5
R
el
a
ti
v
e
F
ro
b
en
iu
s
E
rr
o
r EP-LVMAP-LVM(1)
AP-LVM(2)
ADMM
0 20 40 60 80
Time (sec)
-2400
-2350
-2300
-2250
-2200
-2150
N
eg
a
ti
v
e
L
o
g
-L
ik
el
ih
o
o
d
EP-LVM
AP-LVM(1)
AP-LVM(2)
(a) (b) (c)
Figure 2: Comparison of algorithms both in synthetic and real data. (a) relative error of L in Frobenius norm with p = 100. (a)
relative error of L in Frobenius norm with p = 1000. (c) NLL versus time in Rosetta data set with p = 1000.
Table 2: Comparison of different algorithms for p = 100 and n = 400p. NLL stands for negative log-likelihood.
Alg Estimated NLL Estimated NLL w/reg. True NLL True NLL w/reg.
EP-LVM −8.884646e + 01 − −8.884365e + 01 −
AP-LVM(1) −8.883135e + 01 − −8.884365e + 01 −
AP-LVM(2) −8.884646e + 01 − −8.884365e + 01 −
ADMM −9.374270e + 01 −9.372003e + 01 − −8.639705e + 01
CVX −8.891208e + 01 −8.889070e + 01 − −8.883386e + 01
Alg Relative error Per-iteration time Total time Output rank
EP-LVM 3.341525e − 01 4.982088e − 03 2.386314e + 00 5
AP-LVM(1) 4.381772e − 01 5.626742e − 03 1.886186e + 00 5
AP-LVM(2) 3.341525e − 01 1.017454e − 02 4.710893e + 00 5
ADMM 4.746382e − 01 9.093788e − 03 5.069543e + 00 49
CVX 5.281450e − 01 - 8.505811e + 02 100
Table 3: Comparison of different algorithms for p = 1000 and n = 400p.
Alg Estimated NLL Estimated NLL w/reg. True NLL True NLL w/reg.
EP-LVM −2.640322e + 03 − −2.640204e + 03 −
AP-LVM(1) −2.640186e + 03 − −2.640204e + 03 −
AP-LVM(2) −2.640322e + 03 − −2.640204e + 03 −
ADMM −2.640565e + 03 −2.640000e + 03 − −2.522379e + 03
Alg Relative error Per-iteration time Total time Output rank
EP-LVM 3.065917e − 01 2.557906e − 01 1.534744e + 02 50
AP-LVM(1) 4.048012e − 01 9.880854e − 02 5.928513e + 01 50
AP-LVM(2) 3.065917e − 01 3.759073e − 01 2.255444e + 02 50
ADMM 3.962763e − 01 5.990084e − 01 3.397271e + 02 350
[11] B. Wang, C. Lin, X. Fan, N. Jiang, and D. Farina. Hierarchical bayes based adaptive
sparsity in gaussian mixture model. Pattern Recognition Letters, 49:238–247, 2014.
[12] V. Chandrasekaran, P. Parrilo, and A. Willsky. Latent variable graphical model
selection via convex optimization. The Annals of Statistics, 40(4):1935–1967, 2012.
[13] S. Ma, L. Xue, and H. Zou. Alternating direction methods for latent variable
gaussian graphical model selection. Neural computation, 25(8):2172–2198, 2013.
[14] C. Hsieh, I. Dhillon, P. Ravikumar, S. Becker, and P. Olsen. Quic & dirty: A
quadratic approximation approach for dirty statistical models. In Adv. Neural Inf.
Proc. Sys. (NIPS), pages 2006–2014, 2014.
[15] L. Han, Y. Zhang, and T. Zhang. Fast component pursuit for large-scale inverse
covariance estimation. In Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pages 1585–1594. ACM,
2016.
[16] E. Yang and P. Ravikumar. Dirty statistical models. In Adv. Neural Inf. Proc. Sys.
(NIPS), pages 611–619, 2013.
[17] P. Jain, A. Tewari, and P. Kar. On iterative hard thresholding methods for high-
dimensional m-estimation. In Adv. Neural Inf. Proc. Sys. (NIPS), pages 685–693,
2014.
[18] S. Bhojanapalli, A. Kyrillidis, and S. Sanghavi. Dropping convexity for faster
semi-definite optimization. In 29th Annual Conference on Learning Theory, pages
530–582, 2016.
[19] P. Jain, R. Meka, and I. Dhillon. Guaranteed rank minimization via singular value
projection. In Adv. Neural Inf. Proc. Sys. (NIPS, pages 937–945, 2010.
[20] C. Hegde, I. Indyk, and S. Ludwig. Fast recovery from a union of subspaces. In
Adv. Neural Inf. Proc. Sys. (NIPS), 2016.
[21] S. Negahban, B. Yu, M. Wainwright, and P. Ravikumar. A unified framework for
high-dimensional analysis ofm-estimators with decomposable regularizers. In
Adv. Neural Inf. Proc. Sys. (NIPS), 2011.
[22] C. Hegde, P. Indyk, and L. Schmidt. Approximation algorithms for model-based
compressive sensing. IEEE Trans. Inform. Theory, 61(9):5129–5147, 2015.
7
Table 4: Comparison of different algorithms for p = 100 and n = 50p. NLL stands for negative log-likelihood.
Alg Estimated NLL Estimated NLL w/reg. True NLL True NLL w/reg.
EP-LVM −8.889947e + 01 − −8.887721e + 01 −
AP-LVM(1) −8.884089e + 01 − −8.887721e + 01 −
AP-LVM(2) −8.889947e + 01 − −8.887721e + 01 −
ADMM −8.732370e + 01 −8.727177e + 01 − −8.643062e + 01
CVX −8.946498e + 01 −8.941170e + 01 − −8.886743e + 01
Alg Relative error Per-iteration time Total time Output rank
EP-LVM 8.020263e − 01 4.652518e − 03 2.791511e + 00 5
AP-LVM(1) 8.269273e − 01 6.010683e − 03 3.387369e + 00 5
AP-LVM(2) 8.020263e − 01 9.625240e − 03 5.775144e + 00 3
ADMM 1.776615e + 00 1.281858e − 02 7.691148e + 00 52
CVX 1.779234e + 00 − 8.416155e + 02 100
Table 5: Comparison of different algorithms for p = 1000 and n = 50p.
Alg Estimated NLL Estimated NLL w/reg. True NLL True NLL w/reg.
EP-LVM −2.640797e + 03 − −2.640199e + 03 −
AP-LVM(1) −2.640143e + 03 − −2.640199e + 03 −
AP-LVM(2) −2.640797e + 03 − −2.640199e + 03 −
ADMM −2.645466e + 03 −2.643407e + 03 − −2.522374e + 03
Alg Relative error Per-iteration time Total time Output rank
EP-LVGGM 8.789615e − 01 2.510983e − 01 1.506590e + 02 50
AP-LVGGM(1) 8.609531e − 01 9.864577e − 02 5.918746e + 01 50
AP-LVGGM(2) 8.789615e − 01 3.756105e − 01 2.253663e + 02 50
ADMM 1.540313e + 00 6.669523e − 01 3.793097e + 02 462
[23] T. Cai, Z. Ren, and H. Zhou. Estimating structured high-dimensional covariance
and precision matrices: Optimal rates and adaptive estimation. Electronic Journal
of Statistics, 10(1):1–59, 2016.
[24] M. Rahmani and M. Bastani. Robust and rapid converging adaptive beamform-
ing via a subspace method for the signal-plus-interferences covariance matrix
estimation. IET Sig. Proc., 8(5):507–520, 2014.
[25] M. Rahmani and G. Atia. High dimensional low rank plus sparse matrix decom-
position. IEEE Trans. Sig. Proc., 65(8):2004–2019, 2017.
[26] M. Rahmani and G. Atia. A subspace learning approach for high dimensional
matrix decomposition with efficient column/row sampling. In Proc. Int. Conf.
Machine Learning, pages 1206–1214, 2016.
[27] P. Ravikumar, M. Wainwright, G. Raskutti, and B. Yu. High-dimensional co-
variance estimation by minimizing ℓ1-penalized log-determinant divergence.
Electronic Journal of Statistics, 5:935–980, 2011.
[28] B. Rolfs, B. Rajaratnam, D. Guillot, I. Wong, and A. Maleki. Iterative thresholding
algorithm for sparse inverse covariance estimation. In Adv. Neural Inf. Proc. Sys.
(NIPS), pages 1574–1582, 2012.
[29] Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex
optimization. Communications of the ACM, 55(6):111–119, 2012.
[30] S. Burer and R. Monteiro. A nonlinear programming algorithm for solving
semidefinite programs via low-rank factorization. Mathematical Programming,
95(2):329–357, 2003.
[31] S. Tu, R. Boczar, M. Simchowitz, M. Soltanolkotabi, and B. Recht. Low-rank
solutions of linear matrix equations via procrustes flow. In Proceedings of The
33rd International Conference on Machine Learning, pages 964–973, 2016.
[32] D. Park, A. Kyrillidis, C. Caramanis, and S. Sanghavi. Non-square matrix sensing
without spurious local minima via the burer-monteiro approach. stat, 1050:12,
2016.
[33] Y. Chen and M. Wainwright. Fast low-rank estimation by projected gradi-
ent descent: General statistical and algorithmic guarantees. arXiv preprint
arXiv:1509.03025, 2015.
[34] Q. Zheng and J. Lafferty. A convergent gradient descent algorithm for rank
minimization and semidefinite programming from random linear measurements.
In Adv. Neural Inf. Proc. Sys. (NIPS), pages 109–117, 2015.
[35] D. Hajinezhad, T. Chang, X. Wang, Q. Shi, and M. Hong. Nonnegative matrix
factorization using admm: Algorithm and convergence analysis. In Proc. IEEE
Int. Conf. Acoust., Speech, and Signal Processing (ICASSP), pages 4742–4746, 2016.
[36] X. Yuan, P. Li, and T. Zhang. Gradient hard thresholding pursuit for sparsity-
constrained optimization. In Proc. Int. Conf. Machine Learning, pages 127–135,
2014.
[37] D. Henrion and J. Malick. Projection methods in conic optimization. In Handbook
on Semidefinite, Conic and Polynomial Optimization, pages 565–600. Springer,
2012.
[38] M. Soltani and C. Hegde. Fast algorithms for learning latent variables in graphical
models. Technical report, http://www.ece.iastate.edu/ msoltani/, 2017.
[39] K. Clarkson and D. Woodruff. Low-rank psd approximation in input-sparsity
time. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on
Discrete Algorithms, pages 2061–2072. SIAM, 2017.
[40] Kenneth L Clarkson and David P Woodruff. Low rank approximation and regres-
sion in input sparsity time. In Proc. ACM Symp. Theory of Comput., pages 81–90.
ACM, 2013.
[41] Michael W Mahoney and Petros Drineas. Cur matrix decompositions for im-
proved data analysis. Proc. Natl. Acad. Sci., 106(3):697–702, 2009.
[42] Vladimir Rokhlin, Arthur Szlam, and Mark Tygert. A randomized algorithm for
principal component analysis. SIAM Journal on Matrix Analysis and Applications,
31(3):1100–1124, 2009.
[43] C. Musco and C. Musco. Randomized block krylov methods for stronger and
faster approximate singular value decomposition. In Adv. Neural Inf. Proc. Sys.
(NIPS), pages 1396–1404, 2015.
[44] M. Grant, S. Boyd, and Y. Ye. Cvx: Matlab software for disciplined convex
programming, 2008.
[45] T. Hughes, M. Marton, A. Jones, C. Roberts, R. Stoughton, C. Armour, H. Ben-
nett, E. Coffey, H. Dai, Y. He, et al. Functional discovery via a compendium of
expression profiles. Cell, 102(1):109–126, 2000.
[46] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press,
2004.
[47] V. Chandrasekaran, S. Sanghavi, P. Parrilo, and A. S. Willsky. Sparse and low-
rank matrix decompositions. In Proc. Allerton Conf. on Comm., Contr., and Comp.,
pages 962–967, 2009.
8
6 APPENDIX
We provide full proofs of all theorems discussed in this paper.
Below, the expression C + D for two sets C and D refers to the Minkowski sum of two sets, defined as C + D = {c + d | c ∈ C, d ∈ D} for
given sets C and D. Also,M(Ur ) denotes the set of vectors associated with Ur , the set of all rank-r matrix subspaces. Furthermore σi (A)
denotes the ith largest singular value of matrix A. We need the following equivalent definitions of restricted strongly convex and restricted
strong smoothness conditions.
Definition 6.1. A function f satisfies the Restricted Strong Convexity (RSC) and Restricted Strong Smoothness (RSS) conditions if one of
the following equivalent definitions is satisfied for all L1,L2,L ∈ Rp×p such that rank(L1) ≤ r , rank(L2) ≤ r , rank(L) ≤ r :
mr
2
∥L2 − L1∥2F ≤ f (L2) − f (L1) − ⟨∇f (L1),L2 − L1⟩ ≤
Mr
2
∥L2 − L1∥2F , (10)
∥L2 − L1∥2F ≤ ⟨PU (∇f (L2) − ∇f (L1)) ,L2 − L1⟩ ≤ Mr ∥L2 − L1∥
2
F , (11)
mr ≤ ∥PU ∇2 f (L)∥2 ≤ Mr , (12)
mr ∥L2 − L1∥F ≤ ∥PU (∇f (L2) − ∇f (L1)) ∥F ≤ Mr ∥L2 − L1∥F , (13)
whereU is the span of the union of column spaces of the matrices L1 and L2. Here,mr andMr are the RSC and RSS constants, respectively.
The key observation is that the objective function in (1) is globally strongly convex, and when restricted to any compact psd cone, it also
satisfies the smoothness condition. As a result, it satisfies RSC/RSS conditions.
Proof of Theorem 4.1. Let V t ,V t+1, and V ∗ denote the bases for the column space of Lt ,Lt+1, and L∗, respectively. By definition of set
J in the theorem, V t ∪V t+1 ∪V ∗ ⊆ Jt := J . Define b = Lt − ηPJ∇F (Lt ). We have:
∥Lt+1 − L∗∥F ≤ ∥Lt+1 − b∥F + ∥b − L∗∥F (14)
e1≤ 2∥b − L∗∥F
≤ 2∥Lt − L∗ − ηPJ∇F (Lt )∥F
e2≤ 2∥Lt − L∗ − ηPJ
(
∇F (Lt ) − ∇F (L∗)
)
∥F + 2η∥PJ∇F (L∗)∥F
e3≤ 2
√
1 +M23rη
2 − 2m3rη∥Lt − L∗∥F + 2η∥PJ∇F (L∗)∥F (15)
where e1 holds since Lt+1 is generated by projecting onto the set of matrices with rank r and retaining only the positive eigenvalues; and by
definition of J , Lt+1 also has the minimum Euclidean distance to b over all matrices with rank r . Moreover, e2 holds by applying triangle
inequality and e3 is obtained by combining by lower bound in (11) and upper bound in (13), i.e.,
∥Lt − L∗ − η′
(
∇J F (Lt ) − ∇J F (L∗)
)
∥22 ≤ (1 + η
′2M23r − 2η
′m3r )∥Lt − L∗∥22 .
For (14) to imply convergence, we require that
√
1 +M23rη
2 − 2m3rη < 1. By solving this quadratic inequality with respect to η, we obtain
the conditions 1 ≤ M3rm3r ≤
2√
3
, and 0.5M3r ≤ η ≤
1.5
m3r . If we initialize at L
0 = 0, then we obtain ϑ accuracy after T = O
(
log
(
∥L∗ ∥F
ϑ
))
. □
Proof of Theorem 4.3. Define b ′ = Lt −ηH∇F (Lt ). Let Y ∈ U2r andV := Vt = H(∇F (Lt )). Also, by the definition of the tail projection,
we have Lt ∈ M(Ur ). Hence, we have:
∥Lt+1 − L∗∥F = ∥L∗ − T(b ′)∥F
≤ ∥L∗ − b ′∥F + ∥b ′ − T(b ′)∥F
e1≤ (1 + cT )∥b ′ − L∗∥F
= (1 + cT )∥Lt − L∗ − ηH
(
∇F (Lt
)
)∥F
e2
= (1 + cT )∥Lt − L∗ − ηPV ∇F (Lt )∥F
e3≤ (1 + cT )∥PV (Lt − L∗) + PV ⊥ (Lt − L∗) − ηPV ∇F (Lt )∥F
≤ (1 + cT )∥PV (Lt − L∗) − ηPV ∇F (Lt )∥F + (1 + cT )∥PV ⊥ (Lt − L∗)∥F
e4≤ (1 + cT )∥Lt − L∗ − ηPV+Y
(
∇F (Lt ) − ∇F (L∗)
)
∥F
+ (1 + cT )∥PV ⊥ (Lt − L∗)∥F + (1 + cT )∥PV ∇F (L∗)∥F (16)
9
In the above inequalities, e1 is due to the triangle inequality and the definition of approximate tail projection, e2 is obtained by the definition
of approximate head projection, e3 holds by decomposing of the residual Lt − L∗ in the subspace V and V⊥, and finally e4 is due to the
triangle inequality and the fact that Lt − L∗ ∈ M(U2r ) and V ⊆ V + Y .
As we can see in (16), we have three terms that we need to bound. For the first term we have:
(1 + cT )∥Lt − L∗ − ηPV+Y
(
∇F (Lt ) − ∇F (L∗)
)
∥F ≤ (1 + cT )
√
1 +M22rη
2 − 2m2rη∥Lt − L∗∥F , (17)
where the above inequality holds due to the RSC/RSS assumption on the objective function, F (L) similar to e3 in (14). The third term in (16)
is bounded by the argument given in (9) (see section 4). To bound the second term, ∥PV ⊥ (Lt − L∗)∥F , we follow the proof technique in [20].
First we have:
∥PV ∇F (Lt )∥F ≥ cH ∥PY∇F (Lt )∥F
e1≥ cH ∥PY
(
∇F (Lt ) − ∇F (L∗)
)
∥F − cH ∥PY∇F (L∗)∥F
e2≥ cHm∥Lt − L∗∥F − cH ∥PY∇F (L∗)∥F , (18)
where e1 is followed by adding and subtracting ∥PY∇F (L∗)∥F , and then invoking the triangle inequality. Also, e2 holds due to the lower
bound in the definition of (14) in the RSC/RSS conditions. In addition, we can bound ∥PV ∇F (Lt )∥F from the above as follows:
∥PV ∇F (Lt )∥F
e1≤ ∥PV
(
∇F (Lt ) − ∇F (L∗)
)
− PV (Lt − L∗)∥F + ∥PV (Lt − L∗)∥F + ∥PV ∇F (L∗)∥F
e2≤ ∥PV+Y
(
∇F (Lt ) − ∇F (L∗)
)
− PV+Y (Lt − L∗)∥F + ∥PV (Lt − L∗)∥F + ∥PV ∇F (L∗)∥F
e2≤ ∥Lt − L∗ − PV+Y
(
∇F (Lt ) − ∇F (L∗)
)
∥F + ∥PV (Lt − L∗)∥F + ∥PV ∇F (L∗)∥F
e4≤
√
1 +M22r − 2m2r ∥L
t − L∗∥F + ∥PV (Lt − L∗)∥F + ∥PV ∇F (L∗)∥F , (19)
where e1 holds by adding and subtracting PV ∇F (L∗) and PV (Lt − L∗) and using triangle inequality. e2 is followed by the fact thatV ⊆ V +Y
which implies that projecting onto the extended subspaceV +Y instead ofV cannot decrease the norm. Also, e3 holds since Lt −L∗ ∈ M(U2r ).
Finally e4 holds by using RSC/RSS assumption on the objective function, F (L) similar to e3 in (14). As a result we have from (18) and (19):
∥PV (Lt − L∗)∥F ≥
(
cHm −
√
1 +M22r − 2m2r
)
∥Lt − L∗∥F − (1 + cH)∥PV ∇F (L∗)∥F (20)
Now we can bound the second term in (16), (1 + cT )∥PV ⊥ (Lt − L∗)∥F since from the Pythagoras theorem, we have ∥PV ⊥ (Lt − L∗)∥2F =
∥Lt − L∗∥2F − ∥PV (L
t − L∗)∥2F . To do this, we invoke Claim (14) in [20] which gives:
(1 + cT )∥PV ⊥ (Lt − L∗)∥F ≤ (1 + cT )
√
1 − η20∥L
t − L∗∥F +
η0(1 + cT )√
1 − η20
∥PV ∇F (L∗)∥F (21)
where η0 =
(
cHm −
√
1 +M22r − 2m2r
)
. We obtained the claimed bound in the theorem by combining upper bounds in (17) and (21):
∥Lt+1 − L∗∥F ≤ ρ1∥Lt − L∗∥F + ρ2∥PV ∇F (L∗)∥F , (22)
where ρ1 =
(√
1 +M22rη
2 − 2m2rη +
√
1 − η20
)
(1 + cT ) and ρ2 =
(
η0√
1−η20
+ 1
)
(1 + cT ). Now to have meaningful bound in (22), we need
to have ρ1 < 1 or M22rη
2 − 2m2rη + 1 − 0.25
(
1
1+cT −
√
1 − η20
)2
< 0 which implies 1 ≤ M
2
2r
m22r
≤ 11−ρ20
where ρ0 = 11+cT −
√
1 − η20. Now by
induction and zero initialization, we obtain the ϑ accuracy after T = O
(
log
(
∥L∗ ∥F
ϑ
))
. □
We still need to show that why the assumptions on the RSC/RSS constants of F (L) is satisfied at each iteration of EP-LVM and AP-LVM.
To do this, we prove Theorems 4.2 and 4.4. Our strategy is to establish upper and lower bounds on the spectrum of the sequence of estimates
Lt independent of t . We use the following lemma.
Lemma 6.2. [36, 46] The Hessian of the objective function F (L) is given by ∇2F (L) = Θ−1 ⊗ Θ−1 where ⊗ denotes the Kronecker product and
Θ = S∗ + L. In addition if αI ⪯ Θ ⪯ βI for some α and β , then 1β 2 I ⪯ ∇
2F (L) ⪯ 1α 2 I .
Lemma 6.3 (Weyl type ineqality). For any two matrices A,B ∈ Rp×p , we have:
max
1≤i≤p
|σi (A + B) − σi (A)| ≤ ∥B∥2.
If we establish an universal upper bound and lower bound on λ1(Θt ) and λp (Θt ) ∀t = 1 . . .T , then we can bound the RSC constant as
m ≥ 1λ1(Θt )2 and the RSS-constant asM ≤
1
λp (Θt )2 using Lemma 6.2 and the definition of RSS/RSC.
10
Proof of Theorem 4.2. Recall that by Theorem 4.1, we have ∥Lt − L∗∥F ≤ ρ∥Lt−1 − L∗∥F + 2η∥PJ∇F (L∗)∥F , where ρ < 1 is defined as
ρ = 2
√
1 +M23rη
2 − 2m3rη for 1 ≤ M3rm3r ≤
2√
3
. By Theorem 4.6, the second term on the right hand side can be bounded by O(
√
rp
n ) with
high probability. Therefore, recursively applying this inequality to Lt (and initializing with zero), we obtain:
∥Lt − L∗∥F ≤ ρt ∥L∗∥F +
2η
1 − ρ c2
√
rp
n
. (23)
Since ρ < 1, then ρt < 1. On the other hand ∥L∗∥F ≤
√
r ∥L∗∥2. Hence, ρt ∥L∗∥F ≤
√
r ∥L∗∥2. Also, by the Weyl inequality, we have:
∥Lt ∥2 − ∥L∗∥2 ≤ ∥Lt − L∗∥2 ≤ ∥Lt − L∗∥F . (24)
Combining (23) and (25) and using the fact that λ1(Lt+1) ≤ σ1(Lt+1),
λ1(Lt ) ≤ ∥L∗∥2 + ∥Lt − L∗∥F
≤ ∥L∗∥2 +
√
r ∥L∗∥2 +
2η
1 − ρ c2
√
rp
n
Hence for all t ,
λ1(Θt ) = S1 + λ1(Lt ) ≤ S1 +
(
1 +
√
r
)
∥L∗∥2 +
2c2η
1 − ρ
√
rp
n
. (25)
For the lower bound, we trivially have for all t :
λp (Θt ) = λp (S∗ + Lt ) ≥ Sp . (26)
If we select n = O
(
1
δ 2
(
η
1−ρ
)2
rp
)
for some small constant δ > 0, then (25) becomes:
λ1(Θt ) ≤ S1 +
(
1 +
√
r
)
∥L∗∥2 + δ .
As mentioned above, we setm3r ≥ 1λ21(Θt )
andM3r ≤ 1λ2p (Θt ) which implies
M3r
m3r ≤
λ21(Θt )
λ2p (Θt )
. In order to satisfy the assumption on the RSC/RSS
in theorem 4.1, i.e., M3rm3r ≤
2√
3
, we need to establish a regime such that λ
2
1(Θt )
λ2p (Θt )
≤ 2√
3
. As a result, to satisfy the inequality λ
2
1(Θt )
λ2p (Θt )
≤ 2√
3
, we
need to have the following condition:
Sp ≤ S1 ≤
√
2
√
3
Sp −
(
1 +
√
r
)
∥L∗∥2 − δ . (27)
□
Proof of Theorem 4.4. The proof is similar to the proof of theorem 4.2. Recall that by theorem 4.3, we have
∥Lt − L∗∥F ≤ ρ1∥Lt−1 − L∗∥F + ρ2∥PVt ∇F (L
∗)∥F ,
where ρ1 =
(√
1 +M22rη
2 − 2m2rη +
√
1 − η20
)
(1 + cT ), ρ2 =
(
η0√
1−η20
+ 1
)
(1 + cT ), and the set Vt is as defined in Theorem 4.3. Again, by
Theorem 4.6, the second term on the right hand side is bounded by O(
√
rp/n) with high probability. As above, recursively applying this
inequality to Lt and using zero initialization, we obtain:
∥Lt − L∗∥F ≤ ρt1∥L
∗∥F +
ρ2
1 − ρ1
c3
√
rp
n
.
Since ρ1 < 1, then ρt1 < 1. Now similar to the exact algorithm, ∥L
∗∥F ≤
√
r ∥L∗ |2 and ρt1∥L
∗∥F ≤
√
r ∥L∗∥2. , Hence with high probability,
λ1(Lt ) ≤ ∥L∗∥2 + ∥Lt − L∗∥F
≤ ∥L∗∥2 +
√
r ∥L∗∥2 +
ρ2
1 − ρ1
√
rp
n
e1≤
(
1 +
√
r
)
∥L∗∥2 +
c3ρ2
1 − ρ1
√
rp
n
, (28)
where e1 holds due to (9). Hence, for all t :
λ1(Θt ) = S1 + λ1(Lt ) ≤ S1 +
(
1 +
√
r
)
∥L∗∥2 +
c3ρ2
1 − ρ1
√
rp
n
, (29)
11
Also, we trivially have:
λp (Θt ) = λp (S∗ + Lt ) ≥ Sp − a′, ∀t . (30)
By selecting n = O
(
1
δ ′2
(
ρ2
1−ρ1
)2
rp
)
for some small constant δ ′ > 0, we can write (29) as follows:
λ1(Θt ) ≤ S1 +
(
1 +
√
r
)
∥L∗∥2 + δ ′,
In order to satisfy the assumptions in Theorem 4.3, i.e., M
2
2r
m22r
≤ 11−ρ20
where ρ0 = 11+cT −
√
1 − η20 and η0 =
(
cHm −
√
1 +M22r − 2m2r
)
, we
need to guarantee that λ1(Θ
t )
λp (Θt ) ≤
1√
1−ρ20
. As a result, to satisfy the inequality λ1(Θ
t )
λp (Θt ) ≤
1√
1−ρ20
, we need to have the following condition on S1
and Sp :
Sp ≤ S1 ≤
1√
1 − ρ20
(Sp − a′) −
(
1 +
√
r
)
∥L∗∥2 − δ ′. (31)
□
Proof of Theorem 4.5. Recall from (28) that with very high probability, ∥Lt ∥2≤
(
1 +
√
r
)
∥L∗∥2 + c3ρ21−ρ1
√
rp
n . Also, we always have:
λp (Lt ) ≥ −∥Lt ∥2. As a result:
λp (Lt ) ≥ −
(
1 +
√
r
)
∥L∗∥2 −
c3ρ2
1 − ρ1
√
rp
n
. (32)
Now if the inequality
(
1 +
√
r
)
∥L∗∥2 + c3ρ21−ρ1
√
rp
n < Sp is satisfied, then we can select 0 < a
′ ≤
(
1 +
√
r
)
∥L∗∥2 + c3ρ21−ρ1
√
rp
n . The former
inequality is satisfied by the assumption of Theorem 4.4 on ∥L∗∥2, i.e.,
∥L∗∥2 ≤
1
1 +
√
r
©­­«
Sp
1 +
√
1 − ρ20
−
S1
√
1 − ρ20
1 +
√
1 − ρ20
− c3ρ2
1 − ρ1
√
rp
n
ª®®¬ .
□
Proof of Theorem 4.6. The proof of this theorem is a direct application of the Lemma 5.4 in [47] and we restate it for completeness:
Lemma 6.4. Let C denote the sample covariance matrix, then with probability at least 1 − 2 exp(−p) we have ∥C − (S∗ + L∗)−1∥2 ≤ c1
√
p
n
where c1 > 0 is a constant.
By noting that ∇F (L∗) = C − (S∗ + L∗)−1 and rank(Jt ) ≤ 3r , we can bound the term on the right hand side in Theorem 4.1 as:
∥PJt ∇F (L
∗)∥F ≤
√
3r ∥∇F (L∗)∥2 ≤ c2
√
rp
n
.
The proof for upper-bounding ∥PVt ∇F (L∗)∥F in Theorem 4.3 follows analogously. □
12

