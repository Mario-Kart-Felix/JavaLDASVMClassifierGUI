40 years of computer architecture research
at University of Wisconsin-Madison
This poster highlights the professors, their PhD graduates, and some of the great works from 
the computer architecture program in the University of Wisconsin-Madison Computer 
Sciences department. Wisconsin computer architects have led the way through innovations 
in out-of-order processing, multi-CPU processing, transactional memory, branch predictors, 
caches, and many others. This poster is a window into the depth and breadth of this program.
Poster designed by Jason Power (powerjg@cs.wisc.edu, pages.cs.wisc.edu/~powerjg)
Comments? Questions? Want to point 
out an error? Is your picture missing?
Email: powerjg@cs.wisc.edu
A COROLLARY FOR MULTICORE CHIP COST
Augmenting Amdahl’s law with a corollary for multicore hardware makes it relevant to future 
generations of chips with multiple processor cores. Obtaining optimal multicore performance 
will require further research in both extracting more parallelism and making sequential cores 
faster.
33
C O V E R  F E A T U R E
Dynamically Specialized Datapaths for Energy Efficient Computing
Venkatraman Govindaraju Chen-Han Ho Karthikeyan Sankaralingam
Vertical Research Group
University of Wisconsin-Madison
{venkatra,chen-han,karu}@cs.wisc.edu
Abstract
Due to limits in technology scaling, energy efficiency of logic
devices is decreasing in successive generations. To provide con-
tinued performance improvements without increasing power, re-
gardless of the sequential or parallel nature of the application,
microarchitectural energy efficiency must improve. We propose
Dynamically Specialized Datapaths to improve the energy effi-
ciency of general purpose programmable processors. The key in-
sights of this work are the following. First, applications execute in
phases and these phases can be determined by creating a path-tree
of basic-blocks rooted at the inner-most loop. Second, special-
ized datapaths corresponding to these path-trees, which we refer to
as DySER blocks, can be constructed by interconnecting a set of
heterogeneous computation units with a circuit-switched network.
These blocks can be easily integrated with a processor pipeline.
A synthesized RTL implementation using an industry 55nm
technology library shows a 64-functional-unit DySER block occu-
pies approximately the same area as a 64 KB single-ported SRAM
and can execute at 2 GHz. We extend the GCC compiler to iden-
tify path-trees and code-mapping to DySER and evaluate the PAR-
SEC, SPEC and Parboil benchmarks suites. Our results show that
in most cases two DySER blocks can achieve the same perfor-
mance (within 5%) as having a specialized hardware module for
each path-tree. A 64-FU DySER block can cover 12% to 100%
of the dynamically executed instruction stream. When integrated
with a dual-issue out-of-order processor, two DySER blocks pro-
vide geometric mean speedup of 2.1X (1.15X to 10X), and geo-
metric mean energy reduction of 40% (up to 70%), and 60% en-
ergy reduction if no performance improvement is required.
1 Introduction
Materials and device-driven technology challenges are
ushering an era of non-classical scaling. While the num-
ber of devices is expected to double every generation, the
power efficiency of devices is growing slowly. The main
reason behind this trend is that classical voltage scaling has
effectively ended and capacitance of transistors is reducing
slowly from one generation to another [1]. While the num-
ber of transistors increases sixteen-fold from now through
2020, capacitance only reduces by 3X. Using a conserva-
tive model of 5% reduction in voltage per generation and
no reduction in gate capacitance in future technology nodes,
power increases by 3.4X in 10 years. Architectural tech-
niques are required to improve performance while being en-
ergy efficient as highlighted by others as well [18, 16].
While there is consensus that hardware specializa-
tion can energy-efficiently improve performance, the pro-
grammability and implementation challenges are daunting.
This paper explores hardware specialization using a co-
designed hardware-compiler approach that avoids disrup-
tive hardware or software changes. The idea is to dy-
namically specialize hardware to match application phases.
We call the execution model, Dynamically Specialized
Execution (DySE). The compiler slices applications into
phases and maps them to the hardware substrate – Dynami-
cally Specialized Execution Resource (DySER) blocks.
A DySER block is integrated like a functional unit into
a processor’s pipeline. It is a heterogeneous array of com-
putation units interconnected with a circuit-switched mesh
network, without any storage or other overhead resources.
The key insight is to leave only the computation units on
the commonly executed hardware path, eliminating per-
instruction overheads such as fetch, decode, and register
access. Circuit-switching in the network provides energy
efficiency. By providing support for network flow-control,
we can pipeline multiple DySER invocations and tolerate
loads/stores cache misses. As part of the compiler, we de-
velop a novel path-profiling technique that develops trees of
paths ranging hundreds of instructions that capture the most
commonly executed code. These trees are then mapped to
DySER blocks creating specialized datapaths at run-time.
Thus, DySER integrates a very general purpose and
flexible accelerator into a processor pipeline and with its
co-designed compiler, the same hardware can target “any”
application and diverse domains through dynamic special-
ization. This specialization can help improve energy ef-
ficiency. Judiciously exploiting technology and architec-
ture capability, DySER overcomes the complexity, domain-
specialization, program-scope and language restrictions,
and scalability restrictions of previous efforts.
We have implemented the DySER module in Verilog
and synthesized it on a 55nm technology library and we
have built a compiler-pass in GCC to identify path-trees
and create mappings. Our results show: i) A 64-functional-
unit DySER datapath occupies the same area as a 64KB
single-ported SRAM and can cover 12% to 100% of ap-
plications’ dynamically executed instruction stream. ii) In
most cases two DySER blocks can achieve the same per-
formance (within 5%) as having a specialized block for
each path-tree. iii) When coupled to a single-issue proces-
sor, two DySER blocks provide geometric mean speedup
of 2.1X (1.1X to 10X), and geometric mean energy reduc-
tion of 40% (up to 70%) and geometric-mean energy-delay
product reduction of 2.7X. iv) With a dual-issue and 4-issue
out-of-order (OOO) machine we see similar performance
improvements.
The remainder of this paper is organized as follows.
 
503
Relax: An Architectural Framework for Software Recovery
of Hardware Faults
Marc de Kruijf, Shuou Nomura, Karthikeyan Sankaralingam
Vertical Research Group
University of Wisconsin – Madison
{dekruijf, nomura, karu}@cs.wisc.edu
ABSTRACT
As technology scales ever further, device unreliability is cre-
ating excessive complexity for hardware to maintain the illu-
sion of perfect operation. In this paper, we consider whether
exposing hardware fault information to software and allow-
ing software to control fault recovery simplifies hardware
design and helps technology scaling.
The combination of emerging applications and emerging
many-core architectures makes software recovery a viable al-
ternative to hardware-based fault recovery. Emerging appli-
cations tend to have few I/O and memory side-effects, which
limits the amount of information that needs checkpointing,
and they allow discarding individual sub-computations with
small qualitative impact. Software recovery can harness
these properties in ways that hardware recovery cannot.
We describe Relax, an architectural framework for soft-
ware recovery of hardware faults. Relax includes three core
components: (1) an ISA extension that allows software to
mark regions of code for software recovery, (2) a hardware or-
ganization that simplifies reliability considerations and pro-
vides energy efficiency with hardware recovery support re-
moved, and (3) software support for compilers and program-
mers to utilize the Relax ISA. Applying Relax to counter the
effects of process variation, our results show a 20% energy
efficiency improvement for PARSEC applications with only
minimal source code changes and simpler hardware.
Categories and Subject Descriptors
C.0 [Computer Systems Organization]: General—Hard-
ware/Software Interfaces; C.4 [Computer Systems Orga-
nization]: Performance of Systems—Fault Tolerance
General Terms
Design, Performance, Reliability
1. INTRODUCTION
As CMOS technology scales, individual transistor compo-
nents will soon consist of only a handful of atoms. At these
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
ISCA’10, June 19–23, 2010, Saint-Malo, France.
Copyright 2010 ACM 978-1-4503-0053-7/10/06 ...$10.00.
sizes, transistors are extremely difficult to control in terms
of their individual power and performance characteristics,
their susceptibility to soft errors caused by particle strikes,
the rate at which their performance degrades over time, and
their manufacturability – concerns commonly referred to as
variability, soft errors, wear-out, and yield, respectively. Al-
ready, the illusion that hardware is perfect is becoming hard
to maintain at the VLSI circuit design, CAD, and manufac-
turing layers. Moreover, opportunities for energy efficiency
are lost due to the conservative voltage and frequency as-
sumptions necessary to overcome unpredictability.
This trend towards increasingly unreliable hardware has
led to an abundance of work on hardware fault detection [21,
25, 27, 33, 36] and recovery [3, 8, 32, 38]. Additionally, re-
searchers have explored architectural pruning [26] and tim-
ing speculation [12, 14, 15] as ways to mitigate chip design
and manufacturing constraints. However, in all cases these
proposals have focused on conventional applications running
on conventional architectures, with a typical separation of
hardware and software concerns.
In this paper, we observe two complementary trends in
emerging applications and architectures that favor a new
overall architectural vision: hardware faults recovered in
software. Below, we explain these trends, articulate the chal-
lenges in designing an architecture with software recovery,
and finally describe our proposed framework, Relax.
Emerging applications – applications that continue to drive
increases in chip performance – include computer vision,
data mining, search, media processing, and data-intensive
scientific applications. Many of these applications have two
distinct characteristics that make them interesting from a
reliability perspective. First, and a key observation unique
to this work, is that many have few memory side-effects at
the core of their computation. In particular, state-modifying
I/O operations are rare and memory operations are primar-
ily loads, because the compute regions of these applications
perform reductions over large amounts of data. Second, for
many emerging applications, a perfect answer is not attain-
able due to the inherent computational complexity of the
problem and/or noisy input data. Therefore, they employ
approximation techniques to maximize the qualitative “use-
fulness” of their output. This suggests that these applica-
tions might be error tolerant, which has been observed in
prior work as well [6, 11, 22, 23, 42]. In this paper, we
specifically explore the phenomenon that the application can
discard computations in the event of an error.
The concurrent architecture trend is that massively multi-
core architectures are emerging to meet the computational

Complexity-Effective Superscalar Processors
Subbarao Palacharla Norman P. Jouppi J. E. Smith
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706, USA
subbarao@cs.wisc.edu
Western Research Laboratory
Digital Equipment Corporation
Palo Alto, CA 94301, USA
jouppi@pa.dec.com
Dept. of Electrical and Computer Engg.
University of Wisconsin-Madison
Madison, WI 53706, USA
jes@ece.wisc.edu
Abstract
The performance tradeoff between hardware complexity and
clock speed is studied. First, a generic superscalar pipeline is de-
fined. Then the specific areas of register renaming, instruction win-
dow wakeup and selection logic, and operand bypassing are ana-
lyzed. Each is modeled and Spice simulated for feature sizes of
, , and . Performance results and trends are
expressed in terms of issue width and window size. Our analysis in-
dicates that window wakeup and selection logic as well as operand
bypass logic are likely to be the most critical in the future.
A microarchitecture that simplifies wakeup and selection logic
is proposed and discussed. This implementation puts chains of de-
pendent instructions into queues, and issues instructions from mul-
tiple queues in parallel. Simulation shows little slowdown as com-
pared with a completely flexible issue window when performance is
measured in clock cycles. Furthermore, because only instructions at
queue heads need to be awakened and selected, issue logic is simpli-
fied and the clock cycle is faster – consequently overall performance
is improved. By grouping dependent instructions together, the pro-
posed microarchitecture will help minimize performance degrada-
tion due to slow bypasses in future wide-issue machines.
1 Introduction
For many years, a major point of contention among microproces-
sor designers has revolved around complex implementations that at-
tempt to maximize the number of instructions issued per clock cycle,
and much simpler implementations that have a very fast clock cy-
cle. These two camps are often referred to as “brainiacs” and “speed
demons” – taken from an editorial in Microprocessor Report [7]. Of
course the tradeoff is not a simple one, and through innovation and
good engineering, it may be possible to achieve most, if not all, of
the benefits of complex issue schemes, while still allowing a very
fast clock in the implementation; that is, to develop microarchitec-
tures we refer to as complexity-effective. One of two primary ob-
jectives of this paper is to propose such a complexity-effective mi-
croarchitecture. The proposed microarchitecture achieves high per-
formance, as measured by instructions per cycle (IPC), yet it permits
a design with a very high clock frequency.
Supporting the claim of high IPC with a fast clock leads to the
second primary objective of this paper. It is commonplace to mea-
sure the effectiveness (i.e. IPC) of a new microarchitecture, typ-
ically by using trace driven simulation. Such simulations count
clock cycles and can provide IPC in a fairly straightforward man-
ner. However, the complexity (or simplicity) of a microarchitecture
is much more difficult to determine – to be very accurate, it requires
a full implementation in a specific technology. What is very much
needed are fairly straightforward measures of complexity that can
be used by microarchitects at a fairly early stage of the design pro-
cess. Such methods would allow the determination of complexity-
effectiveness. It is the second objective of this paper to take a step
in the direction of characterizing complexity and complexity trends.
Before proceeding, it must be emphasized that while complexity
can be variously quantified in terms such as number of transistors,
die area, and power dissipated, in this paper complexity is measured
as the delay of the critical path through a piece of logic, and the
longest critical path through any of the pipeline stages determines
the clock cycle.
The two primary objectives given above are covered in reverse
order – first sources of pipeline complexity are analyzed, then a
new complexity-effective microarchitecture is proposed and eval-
uated. In the next section we describe those portions of a microar-
chitecture that tend to have complexity that grows with increasing
instruction-level parallelism. Of these, we focus on instruction dis-
patch and issue logic, and data bypass logic. We analyze potential
critical paths in these structures and develop models for quantifying
their delays. We study the variation of these delays with microarchi-
tectural parameters of window size (the number of waiting instruc-
tions from which ready instructions are selected for issue) and the is-
sue width (the number of instructions that can be issued in a cycle).
We also study the impact of the technology trend towards smaller
feature sizes. The complexity analysis shows that logic associated
with the issue window and data bypasses are likely to be key lim-
iters of clock speed since smaller feature sizes cause wire delays to
dominate overall delay [20, 3].
Taking sources of complexity into account, we propose and eval-
uate a new microarchitecture. This microarchitecture is called
dependence-based because it focuses on grouping dependent in-
structions rather than independent ones, as is often the case in super-
scalar implementations. The dependence-based microarchitecture
simplifies issue window logic while exploiting similar levels of par-
allelism to that achieved by current superscalar microarchitectures
using more complex logic.
The rest of the paper is organized as follows. Section 2 describes
the sources of complexity in a baseline microarchitecture. Section
3 describes the methodology we use to study the critical pipeline
1
Weak Ordering - A New Definition†
Sarita V. Adve
Mark D. Hill
Computer Sciences Department
University of Wisconsin
Madison, Wisconsin 53706
ABSTRACT
A memory model for a shared memory, multipro-
cessor commonly and often implicitly assumed by pro-
grammers is that of sequential consistency. This model
guarantees that all memory accesses will appear to exe-
cute atomically and in program order. An alternative
model, weak ordering, offers greater performance
potential. Weak ordering was first defined by Dubois,
Scheurich and Briggs in terms of a set of rules for
hardware that have to be made visible to software.
The central hypothesis of this work is that pro-
grammers prefer to reason about sequentially consistent
memory, rather than having to think about weaker
memory, or even write buffers. Following this
hypothesis, we re-define weak ordering as a contract
between software and hardware. By this contract,
software agrees to some formally specified constraints,
and hardware agrees to appear sequentially consistent to
at least the software that obeys those constraints. We
illustrate the power of the new definition with a set of
software constraints that forbid data races and an imple-
mentation for cache-coherent systems that is not
allowed by the old definition.
Key words: shared-memory multiprocessor,
sequential consistency, weak ordering.
1. Introduction
This paper is concerned with the programmer’s
model of memory for a shared memory, MIMD mul-
tiprocessor, and its implications on hardware design and
performance. A memory model commonly (and often
† The material presented here is based on research supported in
part by the National Science Foundation’s Presidential Young
Investigator and Computer and Computation Research Programs
under grants MIPS-8957278 and CCR-8902536, A. T. & T. Bell
Laboratories, Digital Equipment Corporation, Texas Instru-
ments, Cray Research and the graduate school at the University
of Wisconsin-Madison.
implicitly) assumed by programmers is that of sequen-
tial consistency, formally defined by Lamport [Lam79]
as follows:
[Hardware is sequentially consistent if] the
result of any execution is the same as if the
operations of all the processors were exe-
cuted in some sequential order, and the
operations of each individual processor
appear in this sequence in the order
specified by its program.
Application of the definition requires a specific interpre-
tation of the terms operations and result. We assume
that operations refer to memory operations or accesses
(e.g., reads and writes) and result refers to the union of
the values returned by all the read operations in the exe-
cution and the final state of memory. With these
assumptions, the above definition translates into the fol-
lowing two conditions: (1) all memory accesses appear
to execute atomically in some total order, and (2) all
memory accesses of each processor appear to execute in
an order specified by its program (program order).
Uniprocessor systems offer the model of sequen-
tial consistency almost naturally and without much
compromise in performance. In multiprocessor systems
on the other hand, the conditions for ensuring sequential
consistency are not usually as obvious, and almost
always involve serious performance trade-offs. For four
configurations of shared memory systems (bus-based
systems and systems with general interconnection net-
works, both with and without caches), Figure 1 shows
that as potential for parallelism is increased, sequential
consistency imposes greater constraints on hardware,
thereby limiting performance. The use of many perfor-
mance enhancing features of uniprocessors, such as
write buffers, instruction execution overlap, out-of-
order memory accesses and lockup-free caches [Kro81]
is heavily restricted.
The problem of maintaining sequential con-
sistency manifests itself when two or more processors
interact through memory operations on common vari-
- 2 -
Tempest and Typhoon: User-Level Shared Memory
Steven K. Reinhardt, James R. Larus, and David A. Wood
Computer Sciences Department
University of Wisconsin–Madison
1210 West Dayton Street
Madison, WI 53706
wwt@cs.wisc.edu
Abstract
Future parallel computers must efficiently execute not
only hand-coded applications but also programs written in
high-level, parallel programming languages. Today’s
machines limit these programs to a single communication
paradigm, either message-passing or shared-memory,
which results in uneven performance. This paper addresses
this problem by defining an interface, Tempest, that
exposes low-level communication and memory-system
mechanisms so programmers and compilers can customize
policies for a given application. Typhoon is a proposed
hardware platform that implements these mechanisms with
a fully-programmable, user-level processor in the network
interface. We demonstrate the utility of Tempest with two
examples. First, the Stache protocol uses Tempest’s fine-
grain access control mechanisms to manage part of a pro-
cessor’s local memory as a large, fully-associative cache
for remote data. We simulated Typhoon on the Wisconsin
Wind Tunnel and found that Stache running on Typhoon
performs comparably (±30%) to an all-hardware DirNNB
cache-coherence protocol for five shared-memory pro-
grams. Second, we illustrate how programmers or compil-
ers can use Tempest’s flexibility to exploit an application’s
sharing patterns with a custom protocol. For the EM3D
application, the custom protocol improves performance up
to 35% over the all-hardware protocol.
1  Introduction
Consensus is emerging on two aspects of massively-
parallel supercomputing. At the application level, these
systems increasingly will be programmed in high-level
parallel languages—such as HPF [17]—that support a
shared address space in which processes uniformly refer-
ence data. At the lowest level, the machines are converging
on workstation-like nodes connected by a point-to-point
network. Unfortunately, no consensus has emerged on the
communication model—shared memory or message pass-
ing—for parallel languages.
Current parallel machines take an all-or-nothing
approach to providing a shared address space. Message-
passing machines, such as the Thinking Machines CM-5
[44] and Intel Paragon [20], have no hardware support, so
compilers for these machines synthesize a shared address
space by generating code that copies values between pro-
cessors in messages. In the best case, this approach per-
forms well and efficiently uses a machine’s memory and
communications network. Unfortunately, the approach
relies on static program analysis and performance degrades
dramatically when a compiler (or programmer) cannot
fully analyze a program.
On the other hand, shared-memory machines, such as
the Kendall Square KSR-1 [21] and Stanford DASH [27],
implement cache-coherent shared-memory policies and
mechanisms entirely in hardware. Although these
machines share a common hardware base with message-
passing machines (workstation-like nodes and point-to-
point message passing), compilers for shared-memory
machines have been constrained to use memory loads and
stores for communication, even when static analysis could
identify better approaches [24].
This paper describes Tempest and Typhoon. Tempest is
an interface that permits programmers and compilers to use
hardware communication facilities directly and to modify
the semantics and performance of shared-memory opera-
tions. It enables an application’s user-level code to support
shared memory and message passing efficiently, along with
hybrid combinations of the two. Typhoon is a proposed
hardware implementation of this interface.
At one extreme, programs with coarse-grain, static com-
munication can send messages. Tempest does not impose
shared-memory overhead on these message-passing pro-
grams. At the other extreme, programs with unanalyzable,
This work is supported in part by NSF PYI/NYI Awards CCR-9157366 and
CCR-9357779, NSF Grants CCR-9101035 and MIP-9225097, a Univ. of
Wisconsin Graduate School Grant, a Wisconsin Alumni Research Founda-
tion Fellowship, an AT&T Ph.D. Fellowship, and donations from Digital
Equipment Corporation, Thinking Machines Corporation, and Xerox Cor-
poration. Our Thinking Machines CM-5 was purchased through NSF Insti-
tutional Infrastructure Grant No. CDA-9024618 with matching funding
from the Univ. of Wisconsin Graduate School.
Appears in: Proceedings of the 21st Annual International Symposium on Computer Architecture, April 1994.
Trace Cache: a Low Latency Approach to High Bandwidth Instruction Fetching
Eric Rotenberg
Computer Science Dept.
Univ. of Wisconsin - Madison
ericro@cs.wisc.edu
Steve Bennett
Intel Corporation
sbennett@ichips.intel.com
James E. Smith
Dept. of Elec. and Comp. Engr.
Univ. of Wisconsin - Madison
jes@ece.wisc.edu
Abstract
As the issue width of superscalar processors is increased,
instruction fetch bandwidth requirements will also increase.
It will become necessary to fetch multiple basic blocks per
cycle. Conventional instruction caches hinder this effort be-
cause long instruction sequences are not always in contigu-
ous cache locations.
We propose supplementing the conventional instruction
cache with a trace cache. This structure caches traces of the
dynamic instruction stream, so instructions that are other-
wise noncontiguous appear contiguous. For the Instruction
Benchmark Suite (IBS) and SPEC92 integer benchmarks,
a 4 kilobyte trace cache improves performance on average
by 28% over conventional sequential fetching. Further, it
is shown that the trace cache’s efficient, low latency ap-
proach enables it to outperform more complex mechanisms
that work solely out of the instruction cache.
1. Introduction
High performance superscalar processor organizations
divide naturally into an instruction fetch mechanism and
an instruction execution mechanism (Figure 1). The fetch
and execution mechanisms are separated by instruction is-
sue buffer(s), for example queues, reservation stations, etc.
Conceptually, the instruction fetch mechanism acts as a
“producer” which fetches, decodes, and places instructions
into the buffer. The instructionexecution engine is the “con-
sumer” which removes instructions from the buffer and ex-
ecutes them, subject to data dependence and resource con-
straints. Control dependences (branches and jumps) provide
a feedback mechanism between the producer and consumer.
Processors having this organization employ aggressive
techniques to exploit instruction-level parallelism. Wide
dispatch and issue paths place an upper bound on peak in-
struction throughput. Large issue buffers are used to main-
tain a window of instructions necessary for detecting paral-
lelism, and a large pool of physical registers provides desti-
nations for all the in-flight instructions issued from the win-
Instruction
Fetch &
Decode
Instruction
branch outcomes/jump addresses
Buffer(s)
Execution
Instruction
Figure 1. Decoupled fetch/execute engines.
dow. To enable concurrent execution of instructions, the
execution engine is composed of many parallel functional
units. The fetch engine speculates past multiple branches in
order to supply a continuous instruction stream to the win-
dow.
The trend in superscalar design is to increase the scale
of these techniques: wider dispatch/issue, larger windows,
more physical registers, more functional units, and deeper
speculation. To maintain this trend, it is important to bal-
ance all parts of the processor – any bottlenecks diminish the
benefit of aggressive ILP techniques.
In this paper, we are concerned with instruction fetch
bandwidth becoming a performance bottleneck. Instruc-
tion fetch performance depends on a number of factors. In-
struction cache hit rate and branch prediction accuracy have
long been recognized as important problems in fetch perfor-
mance and are well-researched areas. In this paper, we are
interested in additional factors that are only now emerging
as processor issue rates exceed four instructions per cycle:
branch throughput – If only one conditional branch is
predicted per cycle, then the window can grow at the
rate of only one basic block per cycle. Predicting mul-
tiple branches per cycle allows the overall instruction
throughput to be correspondingly higher.
noncontiguous instruction alignment – Because of
branches and jumps, instructions to be fetched during
any given cycle may not be in contiguous cache loca-
tions. Hence, there must be adequate paths and logic
available to fetch and align noncontiguous basic blocks
and pass them up the pipeline. That is, it is not enough
Dark Silicon and the End of Multicore Scaling
Hadi Esmaeilzadeh† Emily Blem‡ Renée St. Amant§ Karthikeyan Sankaralingam‡ Doug Burger⋄
†University of Washington ‡University of Wisconsin-Madison
§The University of Texas at Austin ⋄Microsoft Research
hadianeh@cs.washington.edu blem@cs.wisc.edu stamant@cs.utexas.edu karu@cs.wisc.edu dburger@microsoft.com
ABSTRACT
Since 2005, processor designers have increased core counts to ex-
ploit Moore’s Law scaling, rather than focusing on single-core per-
formance. The failure of Dennard scaling, to which the shift to mul-
ticore parts is partially a response, may soon limit multicore scaling
just as single-core scaling has been curtailed. This paper models
multicore scaling limits by combining device scaling, single-core
scaling, and multicore scaling to measure the speedup potential for
a set of parallel workloads for the next five technology generations.
For device scaling, we use both the ITRS projections and a set
of more conservative device scaling parameters. To model single-
core scaling, we combine measurements from over 150 processors
to derive Pareto-optimal frontiers for area/performance and pow-
er/performance. Finally, to model multicore scaling, we build a de-
tailed performance model of upper-bound performance and lower-
bound core power. The multicore designs we study include single-
threaded CPU-like and massively threaded GPU-like multicore chip
organizations with symmetric, asymmetric, dynamic, and composed
topologies. The study shows that regardless of chip organization
and topology, multicore scaling is power limited to a degree not
widely appreciated by the computing community. Even at 22 nm
(just one year from now), 21% of a fixed-size chip must be powered
off, and at 8 nm, this number grows to more than 50%. Through
2024, only 7.9× average speedup is possible across commonly used
parallel workloads, leaving a nearly 24-fold gap from a target of
doubled performance per generation.
Categories and Subject Descriptors: C.0 [Computer Systems Or-
ganization] General — Modeling of computer architecture; C.0
[Computer Systems Organization] General — System architectures
General Terms: Design, Measurement, Performance
Keywords: Dark Silicon, Modeling, Power, Technology Scaling,
Multicore
1. INTRODUCTION
Moore’s Law [23] (the doubling of transistors on chip every 18
months) has been a fundamental driver of computing. For the past
three decades, through device, circuit, microarchitecture, architec-
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
ISCA’11, June 4–8, 2011, San Jose, California, USA.
Copyright 2011 ACM 978-1-4503-0472-6/11/06 ...$10.00.
ture, and compiler advances, Moore’s Law, coupled with Dennard
scaling [11], has resulted in commensurate exponential performance
increases. The recent shift to multicore designs has aimed to in-
crease the number of cores along with transistor count increases,
and continue the proportional scaling of performance. As a re-
sult, architecture researchers have started focusing on 100-core and
1000-core chips and related research topics and called for changes
to the undergraduate curriculum to solve the parallel programming
challenge for multicore designs at these scales.
With the failure of Dennard scaling–and thus slowed supply volt-
age scaling–core count scaling may be in jeopardy, which would
leave the community with no clear scaling path to exploit contin-
ued transistor count increases. Since future designs will be power
limited, higher core counts must provide performance gains despite
the worsening energy and speed scaling of transistors, and given
the available parallelism in applications. By studying these charac-
teristics together, it is possible to predict for how many additional
technology generations multicore scaling will provide a clear ben-
efit. Since the energy efficiency of devices is not scaling along with
integration capacity, and since few applications (even from emerg-
ing domains such as recognition, mining, and synthesis [5]) have
parallelism levels that can efficiently use a 100-core or 1000-core
chip, it is critical to understand how good multicore performance
will be in the long term. In 2024, will processors have 32 times the
performance of processors from 2008, exploiting five generations
of core doubling?
Such a study must consider devices, core microarchitectures,
chip organizations, and benchmark characteristics, applying area
and power limits at each technology node. This paper consid-
ers all those factors together, projecting upper-bound performance
achievable through multicore scaling, and measuring the effects of
non-ideal device scaling, including the percentage of “dark silicon”
(transistor under-utilization) on future multicore chips. Additional
projections include best core organization, best chip-level topology,
and optimal number of cores.
We consider technology scaling projections, single-core design
scaling, multicore design choices, actual application behavior, and
microarchitectural features together. Previous studies have also
analyzed these features in various combinations, but not all to-
gether [8, 9, 10, 14, 15, 20, 22, 27, 28]. This study builds and
combines three models to project performance and fraction of “dark
silicon” on fixed-size and fixed-power chips as listed below:
• Device scaling model (DevM): area, frequency, and power
requirements at future technology nodes through 2024.
• Core scaling model (CorM): power/performance and area/
performance single core Pareto frontiers derived from a large
set of diverse microprocessor designs.
• Multicore scaling model (CmpM): area, power and perfor-
ABSTRACT
Process variability from a range of sources is growing as
technology scales below 65nm, resulting in increasingly nonuniform
transistor delay and leakage power both within a die and across
dies. As a result, the negative impact of process variations on the
maximum operating frequency and the total power consumption of a
processor is expected to worsen. Meanwhile, manufacturers have
integrated more cores in a single die, substantially improving the
throughput of a processor running highly-parallel applications.
However, many existing applications do not have high enough
parallelism to exploit multiple cores in a die. In this paper, first, we
analyze the throughput impact of applying per-core power gating
and dynamic voltage and frequency scaling to power- and thermal-
constrained multicore processors. To optimize the throughput of the
multicore processors running applications with limited parallelism,
we exploit power- and thermal-headroom resulted from power-
gated idle cores, allowing active cores to increase operating
frequency through supply voltage scaling. Our analysis using a
32nm predictive technology model shows that optimizing the
number of active cores and operating frequency within power,
thermal, and supply voltage scaling limits improves the throughput
of a 16-core processor by ~16%. Furthermore, we extend our
throughput analysis and optimization to consider the impact of
within-die process variations leading to core-to-core frequency (and
leakage power) variations in a multicore processor. Our analysis
shows that exploiting core-to-core frequency variations improves
the throughput of a 16-core processor by ~75%.
Categories and Subject Descriptors
C.4 [Performance of Systems]: Design studies
General Terms
Design, Performance
Keywords
DVFS, power gating, multicore processor
1. INTRODUCTION
Today we are at an inflection point in the computing landscape as
we enter the multicore era. Recently, manufacturers have announced
multicore processors because technology scaling has allowed more
devices in a single die. Moreover, manufacturer roadmap promises
to repeatedly double the number of cores per die to continue
Moore’s Law. With technology scaling, however, manufactured dies
exhibit a large spread of maximum operating frequency (Fmax) and
leakage power due to process variations that can be classified into
two categories: die-to-die (D2D) and within-die (WID). D2D
variations affect all transistors on a die equally, while WID
variations induce different electrical characteristics across a die.
This leads to increasingly nonuniform transistor delay and leakage
both within a die and across dies. Furthermore, it is expected that
individual cores are becoming small enough that spatially correlated
WID process variations manifest themselves as core-to-core (C2C)
Fmax and leakage power variations in future multicore processors. 
K. Bowman et al. and J. Tschanz et al. evaluated the impact of
D2D and WID variations on the Fmax distribution of single core
processors [1][2]. K. Bowman, et al. also presented an analytical
throughput model for globally-clocked multicore processors and the
sensitivity of several processor designs’ throughputs to process
variations [3]. E. Humenay et al. modeled the impact of WID
variations on C2C Fmax variations in multicore processors and
analyzed the benefit of adaptive body-biasing (ABB) and adaptive
voltage scaling (AVS) [4]. S. Herbert et al. presented an analytical
model for the throughput of frequency-island (FI) multicore
processors, and quantified the performance benefit of the FI design
style across a range of multicore processor designs [5]. R. Rao et al.
provided analytical thermal and power models, and analyzed the
impacts of a thermal constraint on multicore processor performance
[6]. J. Donald et al. presented power-performance trade-off analysis
under the presence of process variations, and provided a method to
predict an optimal cut-off point for turning-off extra cores in
multicore processors [7]. M. Hill et al. showed that an optimal core
size for a given die size providing the highest throughput depends
on the level of parallelism in applications [8]. Finally, Woo et al.
analyzed energy-efficient core size for many-core processors
considering parallelism in applications [9].
As an alternative to optimize the throughput of multicore
processors for a wide range of parallelism, we can use per-core
power-gating (PCPG) and dynamic voltage and frequency scaling
(DVFS); both of them are very commonly used in many commercial
multicore processors to improve the energy and power efficiency of
multicore processors. A multicore processor with many, smaller
cores provides much higher throughput than one with a few, larger
cores for applications with high parallelism. However, for
applications with limited parallelism, a multicore processor with
many, smaller cores suffers from significantly lower throughput
than one with a fewer, larger cores due to lower single-thread
performance of the smaller cores. To improve the single-thread
performance of the smaller cores (thus the overall throughput of the
multicore processor), Fmax can be increased through supply voltage
(VDD) scaling. In recent multicore processors, however, increasing
VDD (and Fmax) is often limited by power and thermal constraints
rather than a maximum VDD (VDD,max) constraint. Note that
manufactured dies with the same multicore processor design can
have different power and thermal constraints depending on a target
market segment: server, desktop, and mobile; the power and thermal
constraints are often determined by the capacity of supply voltage
regulators and cooling solutions in computing platforms customized
for each market segment.    
When a multicore processor with many, smaller cores runs
applications with limited parallelism, extra power- and thermal-
headroom can be provided for the multicore processor after idle
cores are disabled by PCPG. This allows one or more active cores to
increase VDD and Fmax (thus single-thread performance) within
power and thermal constraints. Furthermore, WID C2C Fmax
variations can be also exploited to improve the single-thread
performance of a multicore processor with many, smaller cores
through Fmax increase; smaller cores exhibit a more relative Fmax
spread among the cores in a die [5]. When all cores are active in a
globally-clocked multicore processor, Fmax is limited by the slowest
core in a die. However, we need only a few number of active cores
due to limited parallelism in applications, allowing a multicore
processor to pick faster cores for running applications. Then the
slowest core among the chosen faster cores determines the
processor’s Fmax that is much faster than the original Fmax
determined by the slowest core in a die. However, note that Fmax
increase of faster cores can be more severely limited by power and
Optimizing Throughput of Power- and Thermal-Constrained 
Multicore Processors Using DVFS and Per-Core Power-Gating
Jungseob Lee and Nam Sung Kim
Department of Electrical and Computer Engineering, University of Wisconsin - Madison, WI, U.S.A.
{jslee9, nskim3} at wisc dot edu
 
On the Value Locality of Store Instructions
Kevin M. Lepak and Mikko H. Lipasti
Electrical and Computer Engineering
University of Wisconsin
1415 Engineering Drive
Madison, WI 53706
{lepak,mikko}@ece.wisc.edu
Abstract
Value locality, a recently discovered program attribute
that describes the likelihood of the recurrence of previ-
ously-seen program values, has been studied enthusias-
tically in the recent published literature. Much of the
energy has focused on refining the initial efforts at pre-
dicting load instruction outcomes, with the balance of
the effort examining the value locality of either all reg-
ister-writing instructions, or a focused subset of them.
Surprisingly, there has been very little published char-
acterization of or effort to exploit the value locality of
data words stored to memory by computer programs.
This paper presents such a characterization, proposes
both memory-centric (based on message passing) and
producer-centric (based on program structure) predic-
tion mechanisms for stored data values, introduces the
concept of silent stores and new definitions of multipro-
cessor false sharing based on these observations, and
suggests new techniques for aligning cache coherence
protocols and microarchitectural store handling tech-
niques to exploit the value locality of stores.We find that
realistic implementations of these techniques can signif-
icantly reduce multiprocessor data bus traffic and are
more effective at reducing address bus traffic than the
addition of Exclusive state to a MSI coherence protocol.
We also show that squashing of silent stores can provide
uniprocessor speedups greater than the addition of
store-to-load forwarding.
1 .0 Introduction
A flurry of recent publications have examined the program
attribute of value locality. Value locality describes the likelihood
of recurrence of previously-seen program values within computer
storage locations. Most of this work has focused on exploiting
this property to accelerate the processing of instructions within a
superscalar processor, with the goal of exposing greater instruc-
tion-level parallelism and improving instruction throughput. In
fact, value locality makes it possible to exceed the classical data-
flow limit, which is defined as the program performance obtained
when machine instructions execute as soon as their operands are
available. Indeed, value locality allows instructions to execute
before their operands are available by enabling the prediction of
the operand values before they are computed. Value prediction
has been proposed and examined for the purpose of reducing
average memory latency by predicting load instruction outcomes
[10], improving throughput of all register-writing instructions by
predicting the outcomes of all such instructions [11], as well as
focusing prediction on only those computations that help resolve
mispredicted branches [7] or occur on some other critical path
[2]. All of these proposed uses of value prediction share the com-
mon goal of accelerating the processing of instructions within a
superscalar processor.
While important and interesting in its own right, this approach to
exploiting value locality is in some ways misguided, as it focuses
on process rather than outcome. In other words, it places empha-
sis on efficient and rapid processing of instructions, which is of
course the means of modern computing, rather than on timely and
correct generation of the result of the computation, which is after
all the real end or goal of computing. To better understand the
distinction, we revisit the useful abstraction of the finite state
machine (FSM) model of instruction set processing [17], as illus-
trated in Figure 1].
When reduced to its most basic form, a modern microprocessor
can be viewed as nothing more than a simple Moore finite state
machine. The microprocessor has some initial state S, consumes
a sequence of inputs X by sequencing through a set of finite states
determined by the next state function g(S,X), and generates a
sequence of outputs Y defined by the output function f(S). The
inputs X are embodied as bit patterns retrieved by instruction
fetches and loads from memory and/or I/O devices, and the out-
FIGURE 1. Finite State Machine Model of a Processor.
S
X Y
g(S,X) f(S)
S
X Y
g(S,X) f(S)
S
X Y
g(S,X) f(S)
S
X Y
g(S,X) f(S)
S
X Y
g(S,X) f(S)
N
N = g(S,X)
Y = f(S)
Moore FSM Model
Memory & I/O
& Interconnect
Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advan-
tage and that copies bear this notice and the full citation on the first
page. To copy otherwise, to republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. 
ISCA 00 Vancouver, British Columbia Canada 
Copyright (c) 2000 ACM 1-58113-287-5/00/06-182 $5.00
182





		





	



	









     
	



	










   	 


		­




 

 

	






   
 
  
 




	

 
   ­
 

    



 
  
 
 



	


   

	  

  

 
 
 
 

	  

  
 	




	



     
 


         
­


      
  

        
  

 



 


 


     





          



 
  
      
  
     
  
     

  





 
       
       
  
         

 
 
     



   
        
  


 


 
     
 
 
 


 


	







 

  
 

 
 

       
    

  
 	 	

 

  ­  
 






    
  






 
   	 	­ 
 
     

 
     
 

    
  	­  





   
    


	­
         

­





 
IEEE 
Managing Wire Delay in Large Chip-Multiprocessor Caches
Bradford M. Beckmann and David A. Wood
Computer Sciences Department
University of Wisconsin—Madison
{beckmann, david}@cs.wisc.edu
Abstract
In response to increasing (relative) wire delay, archi-
tects have proposed various technologies to manage the
impact of slow wires on large uniprocessor L2 caches.
Block migration (e.g., D-NUCA [27] and NuRapid [12])
reduces average hit latency by migrating frequently used
blocks towards the lower-latency banks. Transmission Line
Caches (TLC) [6] use on-chip transmission lines to provide
low latency to all banks. Traditional stride-based hardware
prefetching strives to tolerate, rather than reduce, latency.
Chip multiprocessors (CMPs) present additional chal-
lenges. First, CMPs often share the on-chip L2 cache,
requiring multiple ports to provide sufficient bandwidth.
Second, multiple threads mean multiple working sets, which
compete for limited on-chip storage. Third, sharing code
and data interferes with block migration, since one proces-
sor’s low-latency bank is another processor’s high-latency
bank.
In this paper, we develop L2 cache designs for CMPs
that incorporate these three latency management tech-
niques. We use detailed full-system simulation to analyze
the performance trade-offs for both commercial and scien-
tific workloads. First, we demonstrate that block migration
is less effective for CMPs because 40-60% of L2 cache hits
in commercial workloads are satisfied in the central banks,
which are equally far from all processors. Second, we
observe that although transmission lines provide low
latency, contention for their restricted bandwidth limits
their performance. Third, we show stride-based prefetching
between L1 and L2 caches alone improves performance by
at least as much as the other two techniques. Finally, we
present a hybrid design—combining all three techniques—
that improves performance by an additional 2% to 19%
over prefetching alone.
1  Introduction
Many factors—both technological and marketing—are
driving the semiconductor industry to implement multiple
processors per chip. Small-scale chip multiprocessors
(CMPs), with two processors per chip, are already commer-
cially available [24, 30, 44]. Larger-scale CMPs seem likely
to follow as transistor densities increase [5, 18, 45, 28]. Due
to the benefits of sharing, current and future CMPs are
likely to have a shared, unified L2 cache [25, 37].
Wire delay plays an increasingly significant role in
cache design. Design partitioning, along with the integra-
tion of more metal layers, allows wire dimensions to
decrease slower than transistor dimensions, thus keeping
wire delay controllable for short distances [20, 42]. For
instance as technology improves, designers split caches into
multiple banks, controlling the wire delay within a bank.
However, wire delay between banks is a growing perfor-
mance bottleneck. For example, transmitting data 1 cm
requires only 2-3 cycles in current (2004) technology, but
will necessitate over 12 cycles in 2010 technology assum-
ing a cycle time of 12 fanout-of-three delays [16]. Thus, L2
caches are likely to have hit latencies in the tens of cycles.
Increasing wire delay makes it difficult to provide uni-
form access latencies to all L2 cache banks. One alternative
is Non-Uniform Cache Architecture (NUCA) designs [27],
which allow nearer cache banks to have lower access laten-
cies than further banks. However, supporting multiple pro-
cessors (e.g., 8) places additional demands on NUCA cache
designs. First, simple geometry dictates that eight regular-
shaped processors must be physically distributed across the
2-dimensional die. A cache bank that is physically close to
one processor cannot be physically close to all the others.
Second, an 8-way CMP requires eight times the sustained
cache bandwidth. These two factors strongly suggest a
physically distributed, multi-port NUCA cache design.
This paper examines three techniques—previously
evaluated only for uniprocessors—for managing L2 cache
latency in an eight-processor CMP. First, we consider using
hardware-directed stride-based prefetching [9, 13, 23] to
tolerate the variable latency in a NUCA cache design.
While current systems perform hardware-directed strided
prefetching [19, 21, 43], its effectiveness is workload
dependent [10, 22, 46, 49]. Second, we consider cache
block migration [12, 27], a recently proposed technique for
NUCA caches that moves frequently accessed blocks to
cache banks closer to the requesting processor. While block
migration works well for uniprocessors, adapting it to
This work was supported by the National Science Foundation
(CDA-9623632, EIA-9971256, EIA-0205286, and CCR-
0324878), a Wisconsin Romnes Fellowship (Wood), and dona-
tions from Intel Corp. and Sun Microsystems, Inc. Dr. Wood has a
significant financial interest in Sun Microsystems, Inc.
The gem5 Simulator
Nathan Binkert1, Bradford Beckmann2,
Gabriel Black3, Steven K. Reinhardt2, Ali Saidi4, Arkaprava Basu5, Joel Hestness6,
Derek R. Hower5, Tushar Krishna7, Somayeh Sardashti5, Rathijit Sen5, Korey Sewell8,
Muhammad Shoaib5, Nilay Vaish5, Mark D. Hill5, and David A. Wood5
http://gem5.org
Abstract
The gem5 simulation infrastructure is the merger of the
best aspects of the M5 [4] and GEMS [9] simulators.
M5 provides a highly configurable simulation framework,
multiple ISAs, and diverse CPU models. GEMS comple-
ments these features with a detailed and flexible mem-
ory system, including support for multiple cache coher-
ence protocols and interconnect models. Currently, gem5
supports most commercial ISAs (ARM, ALPHA, MIPS,
Power, SPARC, and x86), including booting Linux on
three of them (ARM, ALPHA, and x86).
The project is the result of the combined efforts of many
academic and industrial institutions, including AMD,
ARM, HP, MIPS, Princeton, MIT, and the Universities
of Michigan, Texas, and Wisconsin. Over the past ten
years, M5 and GEMS have been used in hundreds of pub-
lications and have been downloaded tens of thousands
of times. The high level of collaboration on the gem5
project, combined with the previous success of the com-
ponent parts and a liberal BSD-like license, make gem5 a
valuable full-system simulation tool.
1 Introduction
Computer architecture researchers commonly use soft-
ware simulation to prototype and evaluate their ideas.
As the computer industry continues to advance, the range
of designs being considered increases. On one hand, the
1Hewlett-Packard Labs, Palo Alto, Cal.
2Advanced Micro Devices, Inc., Bellevue, Wash.
3Google, Inc., Mountain View, Cal.
4ARM, Inc., Austin, Tex.
5University of Wisconsin, Madison, Wisc.
6University of Texas, Austin, Tex.
7Massachusetts Institute of Technology, Cambridge, Mass.
8University of Michigan, Ann Arbor, Mich.
emergence of multicore systems and deeper cache hier-
archies has presented architects with several new dimen-
sions of exploration. On the other hand, researchers need
a flexible simulation framework that can evaluate a wide
diversity of designs and support rich OS facilities includ-
ing IO and networking.
Computer architecture researchers also need a simula-
tion framework that allows them to collaborate with their
colleagues in both industry and academia. However, a
simulator’s licensing terms and code quality can inhibit
that collaboration. Some open source software licenses
can be too restrictive, especially in an industrial setting,
because they require publishing any simulator enhance-
ments. Furthermore, poor code quality and the lack of
modularity can make it difficult for new users to under-
stand and modify the code.
The gem5 simulator overcomes these limitations by
providing a flexible, modular simulation system that is
capable of evaluating a broad range of systems and is
widely available to all researchers. This infrastructure
provides flexibility by offering a diverse set of CPU mod-
els, system execution modes, and memory system models.
A commitment to modularity and clean interfaces allows
researchers to focus on a particular aspect of the code
without understanding the entire code base. The BSD-
based license makes the code available to all researchers
without awkward legal restrictions.
This paper provides a brief overview of gem5’s goals,
philosophy, capabilities, and future work along with
pointers to sources of additional information.
2 Overall Goals
The overarching goal of the gem5 simulator is to be a
community tool focused on architectural modeling. Three
key aspects of this goal are flexible modeling to appeal to
a broad range of users, wide availability and utility to
To appear in Proceedings of the 24th International Symposium on Computer Architecture (ISCA), June, 1997
Abstract
This paper introduces the concept of dynamic instruction reuse.
Empirical observations suggest that many instructions, and
groups of instructions, having the same inputs, are executed
dynamically. Such instructions do not have to be executed repeat-
edly — their results can be obtained from a buffer where they were
saved previously. This paper presents three hardware schemes for
exploiting the phenomenon of dynamic instruction reuse, and eval-
uates their effectiveness using execution-driven simulation. We
find that in some cases over 50% of the instructions can be reused.
The speedups so obtained, though less striking than the percent-
age of instructions reused, are still quite significant.
1  Introduction
There are three parameters that influence the execution time
of a program. Microarchitecture has concentrated on two of
them: (i) the number of instructions executed per clock cycle,
i.e., the IPC, and (ii) the clock cycle time. The third parameter:
(iii) the total number of instructions, has been considered the
domain of software. In this paper we address the following ques-
tion: “Can we develop microarchitectural techniques to reduce
the number of instructions that have to be executed dynamically,
and what are the potential benefits of such techniques?’’
Just as caches reduce the number of memory accesses made
dynamically if a memory location is going to be accessed repeat-
edly, the number of instructions executed dynamically can be
reduced if an instruction is going to produce the same value
repeatedly. We have observed many instructions, and groups of
instructions, having the same inputs (consequently producing the
same output) when executed dynamically. This observation can
be exploited to reduce the number of instructions executed
dynamically as follows: by buffering the previous result of the
instruction, future dynamic instances of the same static instruc-
tion can use the result by establishing that the input operands in
both cases are the same. We call this dynamic instruction reuse.
Dynamic instruction reuse can benefit performance in two
main ways. First, by not having to pass through all the phases of
execution (e.g., issue, execute, result bypass) dynamically, utili-
zation of machine resources could be reduced, alleviating
resource conflicts. Second, and more important, the outcome of
an instruction can be known much earlier, allowing instructions
that are dependent upon the outcome to proceed sooner. As we
shall see, the results of chains of dependent instructions could all
be generated in a single cycle, short-circuiting dependence
chains, and reducing the lengths of critical paths of execution.
This paper is concerned with exploiting the phenomenon of
dynamic instruction reuse. Towards this end, we develop
microarchitectural mechanisms that allow the outcome of an
instruction to be known earlier than it would had the instruction
have to pass through all the phases of its execution. We describe
the concept of dynamic instruction reuse in section 2, and
present scenarios to illustrate why it occurs and why might it be
a useful phenomenon to exploit. In section 3, we present three
different schemes for instruction reuse. Each scheme employs a
reuse buffer, a buffer of previous outcomes of instruction execu-
tion. In section 4 we show how a reuse buffer can be incorpo-
rated into a generic superscalar processor. In section 5 we
provide a quantitative evaluation, and in section 6 we discuss
related work. Finally section 7 presents some concluding
remarks.
2  Scenarios for Dynamic Instruction Reuse
Before developing mechanisms to allow dynamic instruction
reuse, we need to understand why this phenomenon occurs.
What causes instructions to be executed with the same input
operands? Why are such instructions in the program in the first
place? Are such instructions needed? Why might it be better to
obtain the outcome of an instruction from a buffer rather than
recompute it? In order to answer these questions, we look at a
couple of scenarios
The first scenario involves speculative execution in a dynam-
ically scheduled processor. As illustrated in Figure 1, when a
branch instruction is encountered, its outcome is predicted, and
instructions from the predicted basic block (block A) are exe-
cuted speculatively. In addition to executing instructions from
path
correct
path
Dynamic
instruction
stream
branch
(C)
(A)
(B)
Squashed
predicted
Figure 1. Scenario where execution on the (mis)predicted path
converges with the execution on the correct path. In such cases
certain instructions from part (C) need not be re-executed when
encountered on the correct path.
Dynamic Instruction Reuse
Avinash Sodani and Gurindar S. Sohi
Computer Sciences Department
University of Wisconsin-Madison
1210 West Dayton Street
Madison, WI 53706 USA
{sodani,sohi}@cs.wisc.edu
Permission to make digital/hard copy of part or all of this work for personal or
classroom use if granted without fee provided that copies are not made or dis-
tributed for profit or commercial advantage, the copyright notice, the title of
the publication and its date appear, and notice is given that copying is by per-
mission of ACM, Inc. To copy otherwise, to publish, to post on servers, or to
redistribute to lists, requires prior specific permission and/or fee.
© 1997 ACM.
Memory Bandwidth Limitations of Future Microprocessors
Doug Burger, James R. Goodman, and Alain Kägi
Computer Sciences Department
University of Wisconsin-Madison
1210 W st Dayton Street
Madison, Wisconsin 53706 USA
galileo@cs.wisc.edu  -  http://www.cs.wisc.edu/~galileo
This work is supported in part by NSF Grant CCR-9207971, an unre-
stricted grant from the Intel Research Council, an unrestricted grant from
the Apple Computer Advanced Technology Group, and equipment dona-
tions from Sun Microsystems.
This paper appears in the 23rd International Symposium on Computer Architecture, May, 1996. Reprinted by permission of ACM
Copyright 1996 (c) by Association for Computing Machinery (ACM). Per-
mission to copy and distribute this document is hereby granted provided
that this notice is retained on all copies and that copies are not altered.
Abstract
This paper makes the case that pin bandwidth will be a critical
consideration for future microprocessors. We show that many of
the techniques used to tolerate growing memory lat ncies do so at
the expense of increased bandwidth requirements. Using a decom-
position of execution time, we show that for modern processors
that employ aggressive memory latency tolerance techniq es,
wasted cycles due to insufficient bandwidth generally exc ed those
due to raw memory latencies. Given the importance of maximizing
memory bandwidth, we calculate effective pin bandwidth, then
estimate optimal effective pin bandwidth. We measure these quan-
tities by determining the amount by which both caches and mini-
mal-traffic caches filter accesses to the lower levels of the memory
hierarchy. We see that there is a gap that can exceed two orders of
magnitude between the total memory traffic generated by caches
and the minimal-traffic caches—implying that the potential exists
to increase effective pin bandwidth substantially. We decompose
this traffic gap into four factors, and show they contribute quite
differently to traffic reduction for different benchmarks. We con-
clude that, in the short term, pin bandwidth limitations will make
more complex on-chip caches cost-effective. For example, flexible
caches may allow individual applications to choose from a range
of caching policies. In the long term, we predict that off-chip
accesses will be so expensive that all system memory will reside on
one or more processor chips.
1  Introduction
The growing inability of memory systems to keep up with pro-
cessor requests has significant ramifications for the design of
microprocessors in the next decade. Technological trends have
produced a large and growing gap between CPU speeds and
DRAM speeds. The number of instructions that the processor can
issue during an access to main memory is already large. Extrapo-
lating current trends suggests that soon a processor may be able to
issue hundreds or even thousands of instructions while it fetches a
single datum into on-chip memory.
Much research has focused on reducing or tolerating these
large memory access latencies. Researchers have proposed many
techniques for reducing the frequency and impact of cache misses.
These include lockup-free caches [28, 40], cache-conscious load
scheduling [1], hardware and software prefetching [6, 7, 13, 14,
26, 32], stream buffers [24, 33], speculative loads and execution
[11, 35], and multithreading [30, 38].
It is our hypothesis that the increasing use and success of
latency-tolerance techniques will expose memory bandwidth, not
raw access latencies, as a more fundamental impediment to higher
performance. Increased latency due to bandwidth constraints will
emerge for four reasons:
1. Continuing progress in processor design will increase the
issue rate of instructions. These advances include both archi-
tectural innovation (wider issue, speculative execution, etc.)
and circuit advances (faster, denser logic).
2. To the extent that latency-tolerance techniques are successful,
they will speed up the retirement rate of instructions, thus
requiring more memory operands per unit of time.
3. Many of the latency-tolerance techniques increase the abso-
lute amount of memory traffic by fetching more data than are
needed. They also create contention in the memory system.
4. Packaging and testing costs, along with power and cooling
considerations, will increasingly affect costs—resulting in
slower, or more costly, increases in off-chip bandwidth than
in on-chip processing and memory.
The factors enumerated above will render memory band-
width—particularly pin bandwidth—a more critical and expensive
resource than it is today. Given the complex interactions between
memory latency and bandwidth, however, it is difficult to deter-
mine whether memory-related processor stalls are due to raw
memory latency or increased latency from insufficient bandwidth.
Current metrics (such as average memory access time) do not
address this issue. This paper therefore separates execution time
into three categories: processing time (which includes idle time
caused by lack of instruction-level parallelism [ILP]), memory
latency stall time, and memory bandwidth stall time.
Assuming that a growing percentage of lost cycles are due to
insufficient pin bandwidth, the performance of future systems will
increasingly be determined by (i) the rate at which the external
memory system can supply operands, and (ii) how effectively on-
chip memory can retain operands for reuse. By retaining operands,
on-chip memory (caches, registers, and other structures) can
increase effective pin bandwidth. By measuring the extent to
which on-chip memory shields the pins from processor requests,
we can determine how much computational power a given pack-
age can support.
The miss rate provides a good estimate of traffic reduction for
simple caches. Since many techniques can trade increased traffic
for decreased latency (i.e., more cache hits), miss rate is not the
best measure of traffic reduction for more complex memory hierar-
chies. The use of traffic ratios [18, 20]—the ratio of traffic below a
cache to the traffic above it—provides a more accurate measure of
how on-chip memories change effective off-chip bandwidth.
0018-9162/97/$10.00 © 1997 IEEE46
B lli n-Tr nsi tor
Archit ctur s
T
he circumstances in which computer architects
will find themselves in the next 15 years are truly
daunting. By the end of this period, micro-
processors will have more than a billion logic tran-
sistors on a single chip. Tiny transistors and wires will
have feature sizes less than a tenth of a micron. The
time required to send a signal along an on-chip wire
will become proportionately much greater than that
needed for a transistor to switch. Off-chip communi-
cation will become relatively slower. Minimizing
power dissipation and the resultant heat will be para-
mount, despite reduced voltage levels. Although these
predictions are not controversial, their implications
for microprocessor architectures certainly are.
THE DEBATE
During an informal discussion at the 1996
International Symposium on Computer Architecture
(ISCA-23), several architects had a boisterous discus-
sion (read argument) over the direction that future
architectures will take. Our community generally
understands the evolving possibilities and the underly-
ing constraints. The rate of progress is so great, how-
ever, that radical models easily dismissed only a few
years ago are now feasible, and there is little agreement
on which models are likely to achieve dominance. We
organized this special issue because our discussion at
ISCA was sufficiently controversial and interesting to
appeal to a wide audience. Our goals for this issue are
to explore both the trends that will affect future archi-
tectures and the space of these architectures.
ADDRESSING THE DEBATE
The articles in this issue fall into two categories. The
first category contains three articles, which appear in
Cybersquare. Each describes one trend that will affect
future microprocessor architectures. In the second cat-
egory, each article makes the case for a different bil-
lion-transistor architecture. Although these articles
represent the state of the art and the authors’ best
guesses, the future is notoriously hard to predict in our
breakneck-paced field. Technology trends are gener-
ally easier to predict than their effects, but trend esti-
mates can be wildly inaccurate. Intel’s 1989 prediction
for 1996 processors underestimated performance by
a factor of four.1 Forecasting the effects of technology
is even harder, as illustrated by several well-known
quotes:
• “Everything that can be invented has been
invented.” US Commissioner of Patents, 1899.
• “I think there is a world market for about five com-
puters.” Thomas J. Watson Sr., IBM founder, 1943.
• “There is no reason for any individuals to have a
computer in their home.” Ken Olsen, CEO of
Digital Equipment Corp., 1977.
• “The current rate of progress can’t continue much
longer. We’re facing fundamental problems that
we didn’t have to deal with before.” Various com-
puter technologists, 1955-1997.
Given the wide scope of these articles and the cred-
ibility of the authors, however, it is certain that many
of the ideas discussed in this issue will be incorporated
into the first billion-transistor processor.
FUTURE TRENDS
Before we discuss the possible alternatives for micro-
processors’ evolution, it is important to understand
the driving factors, five of which we discuss next.
Hardware trends and physical limits
In its 1994 road map,2 the Semiconductor Industry
Association predicted the course of semiconductor
technology over the next 15 years. The SIA predicted
that by 2010, industry would be manufacturing 800-
million-transistor processors with thousands of pins,
a 1,000-bit bus, and clock speeds over 2 GHz. Such
chips would produce a predicted maximum of 180 W
(allowing the computer of 2010 to serve also as a bar-
becue grill or space heater).
The most important physical trend, however, is the
fact that on-chip wires are becoming much slower rel-
ative to logic gates as the on-chip devices shrink. It will
soon be impossible to maintain one global clock over
the entire chip. Sending signals across a billion-
transistor processor may require as many as 20 cycles.
In the first of the short trend articles, Doug Matzke of
Texas Instruments describes these effects in detail.
System software
Much of the performance gain in recent years has
come from exploitation of parallelism, as processors
overlap multiple instructions (pipelining) and simul-
taneously execute multiple instructions (superscalar
execution). It is probable that future processors will
harvest significantly more parallelism; the question is
Advances in semiconductor manufacturing will permit an unprecedented
number of tra sistors n a single processor die. But what architecture will
make the best us  of thes  riches?
Doug Burger
James R.
Goodman
University of
Wisconsi -
Madis n
G
u
e
s
t 
E
d
it
o
rs
’
In
tr
o
d
u
c
ti
o
n
.
Weaving Relations for Cache Performance
Anastassia Ailamaki ‡ David J. DeWitt Mark D. Hill Marios Skounakis
Carnegie Mellon University Univ. of Wisconsin-Madison Univ. of Wisconsin-Madison Univ. of Wisconsin-Madison
natassa@cs.cmu.edu dewitt@cs.wisc.edu markhill@cs.wisc.edu marios@cs.wisc.edu
Abstract
Relational database systems have traditionally optimzed for
I/O performance and organized records sequentially on disk
pages using the N-ary Storage Model (NSM) (a.k.a., slotted
pages). Recent research, however, indicates that cache utilization
and performance is becoming increasingly important on modern
platforms. In this paper, we first demonstrate that in-page data
placement is the key to high cache performance and that NSM
exhibits low cache utilization on modern platforms. Next, we pro-
pose a new data organization model called PAX (Partition
Attributes Across), that significantly improves cache perfor-
mance by grouping together all values of each attribute within
each page. Because PAX only affects layout inside the pages, it
incurs no storage penalty and does not affect I/O behavior.
According to our experimental results, when compared to NSM
(a) PAX exhibits superior cache and memory bandwidth utiliza-
tion, saving at least 75% of NSM’s stall time due to data cache
accesses, (b) range selection queries and updates on memory-
resident relations execute 17-25% faster, and (c) TPC-H queries
involving I/O execute 11-48% faster.
1 Introduction
The communication between the CPU and the secondary
storage (I/O) has been traditionally recognized as the
major database performance bottleneck. To optimize data
transfer to and from mass storage, relational DBMSs have
long organized records in slotted disk pages using the N-
ary Storage Model (NSM). NSM stores records contigu-
ously starting from the beginning of each disk page, and
uses an offset (slot) table at the end of the page to locate
the beginning of each record [27].
Unfortunately, most queries use only a fraction of
each record. To minimize unnecessary I/O, the Decompo-
sition Storage Model (DSM) was proposed in 1985 [10].
DSM partitions an n-attribute relation vertically into n
sub-relations, each of which is accessed only when the
corresponding attribute is needed. Queries that involve
multiple attributes from a relation, however, must spend
tremendous additional time to join the participating sub-
relations together. Except for Sybase-IQ [33], today’s rela-
tional DBMSs use NSM for general-purpose data place-
ment [20][29][32].
Recent research has demonstrated that modern data-
base workloads, such as decision support systems and spa-
tial applications, are often bound by delays related to the
processor and the memory subsystem rather than I/O
[20][5][26]. When running commercial database systems
on a modern processor, data requests that miss in the cache
hierarchy (i.e., requests for data that are not found in any
of the caches and are transferred from main memory) are a
key memory bottleneck [1]. In addition, only a fraction of
the data transferred to the cache is useful to the query: the
item that the query processing algorithm requests and the
transfer unit between the memory and the processor are
typically not the same size. Loading the cache with useless
data (a) wastes bandwidth, (b) pollutes the cache, and (c)
possibly forces replacement of information that may be
needed in the future, incurring even more delays. The
challenge is to repair NSM’s cache behavior without com-
promising its advantages over DSM.
This paper introduces and evaluates Partition
Attributes Across (PAX), a new layout for data records
that combines the best of the two worlds and exhibits per-
formance superior to both placement schemes by eliminat-
ing unnecessary accesses to main memory. For a given
relation, PAX stores the same data on each page as NSM.
Within each page, however, PAX groups all the values of a
particular attribute together on a minipage. During a
sequential scan (e.g., to apply a predicate on a fraction of
the record), PAX fully utilizes the cache resources,
because on each miss a number of a single attribute’s val-
ues are loaded into the cache together. At the same time,
all parts of the record are on the same page. To reconstruct
a record one needs to perform a mini-join among
minipages, which incurs minimal cost because it does not
have to look beyond the page.
We evaluated PAX against NSM and DSM using (a)
predicate selection queries on numeric data and (b) a vari-
ety of queries on TPC-H datasets on top of the Shore stor-
age manager [7]. We vary query parameters including
selectivity, projectivity, number of predicates, distance
between the projected attribute and the attribute in the
predicate, and degree of the relation. The experimental
results show that, when compared to NSM, PAX (a) incurs
50-75% fewer second-level cache misses due to data
‡ Work done while author was at the University of Wisconsin-Madison.
Permission to copy without fee all or part of this material is granted pro-
vided that the copies are not made or distributed for direct commercial
advantage, the VLDB copyright notice and the title of the publication and
its date appear, and notice is given that copying is by permission of the
Very Large Data Base Endowment. To copy otherwise, or to republish,
requires a fee and/or special permission from the Endowment
Proceedings of the 27th VLDB Conference,
Roma, Italy, 2001
1
Appears in the proceedings of the
12th Annual International Symposium on High Performance Computer Architecture (HPCA-12)
Austin, TX February 11-15, 2006
LogTM: Log-based Transactional Memory
Kevin E. Moore, Jayara Bobba, Michelle J. Moravan, Mark D. Hill & David A. Wood
De artment of Computer Sciences, University of Wisconsin–Madison
{kmoore, bobba, moravan, markhill, david}@cs.wisc.edu
http://www.cs.wisc.edu/multifacet
Abstract
Transactional memory (TM) simplifies parallel rogram-
ming by guaranteeing that transactions appear to execute
atomically and in isolation. Implementing these properties
includes providing data version management for the simul-
taneous storage of both new (visible if the transaction com-
mits) and old (retained if the transaction aborts) values.
Most (hardware) TM systems leave old values “in place”
(the target memory address) and buffer new values else-
where until commit. This makes aborts fast, but penalizes
(the much more frequent) commits.
In this paper, we present a new implementation of trans-
actional memory, Log-based Transactional Memory
(LogTM), that makes commits fast by storing old values to a
per-thread log in cacheable virtual memory and storing new
values in place. LogTM makes two additional contributions.
First, LogTM exte ds a MOESI directory protocol to enable
both fast conflict detection on evicted blocks and fast com-
mit (using lazy cleanup). Second, LogTM handles aborts in
(library) software with little performance penalty. Evalua-
tions running micro- and SPLASH-2 benchmarks on a 32-
way multiprocessor support our decision to optimize for
commit by showing that only 1-2% of transactions abort.
1. Introduction
The promise of plentiful thread support from chip multi-
processors is re-energizing interest in transactional memory
(TM) [14] systems, implemented in software only [12, 13,
18, 27] or, our focus, in hardware (with some software sup-
port) [2, 11, 14, 25]. TM systems must provide transaction
atomicity (all or nothing) and isolation (the partially-com-
plete state of a transaction is hidden from other transactions)
[9]. Providing these properties requires data version man-
agement and conflict detection, whose implementations dis-
tinguish alternative TM proposals.
Version management handles the simultaneous storage of
both new data (to be visible if the transaction commits) and
old data (retained if the transaction aborts). At most one of
these values can be stored “in place” (the target memory
address), while the other value must be stored “on the side”
(e.g., in speculative hardware). On a st re, a TM system can
use eager version management and put the new value in
place, or use lazy version management to (temporarily)
leave the old value in place.
Conflict detection signals an overlap between the write
set (data written) of one transaction and the write set or read
set (data re d) of othe con urrent tra sactions. Conflict
detection is called eager if it detects offending loads or
stores immediately and lazy if it defers detection until later
(e.g., when transactions commit).
The taxonomy in Table 1 illustrates which TM proposals
use lazy versus eager version management and conflict
detection.
TCC. Hammond et al.’s Transactional Memory Coherence
and Consistency (TCC) [11] uses both lazy version manage-
ment and lazy conflict detection, similar to the few database
management systems (DBMSs) that use optimistic concur-
rency control (OCC) [16]. TCC buffers stores at the proces-
sor’s L1 cach and overwrites the L2 cache and memory
only on commit. TCC detects conflicts with a pending trans-
action only when other transactions commit (not when data
is first stored).
LTM. Ananian et al.’s Large Transactional Memory (LTM)
[2] u es lazy version management and eager conflict detec-
tion. LTM keeps the old value in main memory and stores
the new value in cache, coercing the coherence protocol to
store two different values at the same address. Repeated
transactions which modify the same block, however, require
a writeback of the block once per transaction. On cache
overflows, LTM spills the new values to an in-memory hash
table. In contrast to TCC, LTM uses eager conflict detection
invoked when conflicting loads or stores seek to execute.
LTM conflict detection is complicated by the cache overflow
case. When the controller detects a potential conflict with an
overflowed block, it must walk the uncacheable in-memory
hash table before responding (and possibly aborting).
VTM. Rajwar et al.’s Virtual Transactional Memory (VTM)
[25] also combines lazy version management with eager
conflict detection. Memory always holds old values. On
cache overflows, VTM writes the new (and a second copy of
Analyzing the Impact of Joint Optimization of Cell Size,
Redundancy, and ECC on Low-Voltage
SRAM Array Total Area
Nam Sung Kim, Stark C. Draper, Shi-Ting Zhou, Sumeet Katariya,
Hamid Reza Ghasemi, and Taejoon Park
Abstract—The increasing power consumption of processors has made
power reduction a first-order priority in processor design. Voltage scaling
is one of the most powerful power-reduction techniques introduced to date,
but is limited to some minimum voltage . Below on-chip
SRAM cells cannot all operate reliably due to increased process variability
with technology scaling. The use of larger SRAM cells, which are less sen-
sitive to process variability, allows a reduction in . However, since
the large-scale memory tructures su as last-level caches (LLCs) often de-
termine the of processors, these structures cannot afford to use
large SRAM cells due to the resulting increase in die area. In this paper we
first propose a joint optimization of LLC cell size, the number of redun-
dant cells, and the strength of error-correction coding (ECC) to minimize
total SRAM area while meeting yield and targets. The joint use
of redundant cells and ECC enables the use of smaller cell sizes while main-
taining design targets. Smaller cell sizes more than make up for the extra
cells required by redundancy and ECC. In 32-nm technology our joint ap-
proach yields a 27% reduction in total SRAM area (including the extra
cells) when targeting 90% yield and 600 mV . Second, we demon-
strate that the ECC used to repair defective cells can be combined with a
simple architectural technique, which can also fix particle-induced soft er-
rors, without increasing ECC strength or processor runtime.
Index Terms—Error correction coding (ECC), low-voltage SRAM, re-
dundancy, voltage scaling.
I. INTRODUCTION
As technology scaling allows us to integrate more transistors,
reducing overall power consumption has become a critical design
priority. Although voltage scaling is one of the most effective tech-
niques for minimizing power consumption, processors do not operate
reliably at low supply voltage. This is because random uncorrelated
process variations, which increases with technology scaling, result
in mismatches between the transistors in SRAM cells. This make
the cells unstable at low voltages and increase the cell failure proba-
bility; a failure mode addressed by imposing some minimum operating
voltage, . Processors typically contain very large-scale SRAM
structures, such as on-chip caches, and the needed by these
structures often determines the of the whole processor.
To lower one can increase the size of the constituent transis-
tors in 6-transistor cells, which reduces the amount of random process
variations (i.e., mismatches) [1], or one can adopt 8- or 10-transistor
cells [2], [3]. However, these approaches substantially increase the area
of large on-chip caches. On the other hand, one can use redundant
columns of cells or error-correction coding (ECC). A small number
of extra (redundant) columns of cells are made available to replace a
column of cells that contains a cell (or cells) with manufacturing defects
(e.g., stuck-at-faults) [4]. Following manufacturing, a test for defective
Manuscript received February 06, 2011; revised June 17, 2011; accepted
August 12, 2011. Date of publication November 15, 2011; date of current
version August 02, 2012. This work was supported in part by an NSF Grant
(CCF-1016262), by a Multidisciplinary Research Award from the University
of Wisconsin-Madison Graduate School, and by NSF CAREER Awards
(CCF-0953603 and CCF-0844539).
The authors are with the University of Wisconsin-Madison, WI 53706 USA
(e-mail: nam.sung.kim@gmail.com).
Digital Object Identifier 10.1109/TVLSI.2011.2173220
cells is performed. The SRAM array is then reconfigured to access a re-
dundant column instead of the column containing the defective cell(s).
Although ECC was originally adopted to protect memory from par-
ticle-induced soft errors [5], it can also be used to correct errors arised
from low-voltage operation.
SRAM arrays composed of smaller cells, which exhibit higher
failure probability than larger ones, require more redundancy and
stronger ECC to achieve a target . Although the overall cell
area allocated for dat bits of this solution is small, the total SRAM
area can be very large due to substantial are that must be allocated for
the redundant and ECC bits. On the other hand, increased transistor
size (i.e., larger cell siz ) reduces the amount of random process
variation, and results in a su stantially lower cell failure probability.
This greatly lowers the number of redundant or ECC bits required
to achieve a target. However, using large cells results in a
significant increase in total SRAM array area. In one way or another
we must pay for a low through increased area. However,
balancing cell size with the degree of fault tolerance (i.e., redundancy
and/or ECC) will allow us to minimize the total area of SRAM arrays.
Prior studies of reduction through the use of redundancy,
ECC, or other fault-tolerance techniques, did not take advantage of
the trade-off between SRAM cell size and failure probability. The
approach we take in this paper is to combine the proven design
techniques discussed above in a novel hybrid manner that delivers a
quantitative boost in effectiveness when compared to the application of
any single technique. First, for SRAM cells, we explore the change in
single-cell failure probability as the size of the constituent transistors
is varied. Second, we apply a necessary amount of fault tolerance
needed, through redundancy and/or ECC to achieve a target
for the given cell failure probability, which is determined by the cell
size. As we show, a joint optimization that includes transistor sizing
can quite substantially impact the overall failure probability of large
on-chip caches. Finally, we demonstrate that the ECC used to repair
defective cells can be combined with a simple architectural technique
to fix particle-induced soft errors without increasing ECC strength or
processor runtime.
The remainder of the paper is organized as follows. In Section II, we
discuss the relationship between failure probability and size of SRAM
cells. In Section III, we jointly explore the size of SRAM cells, the
degree of redundancy, and the strength of the ECCs used to minimize
the total SRAM area. In Section IV, we describe how to use a mask
error correction technique in combination with an ECC that has both
error correction and detection capabilities, to correct errors from both
particle strikes and defects more efficiently. Section V concludes the
study.
II. FAILURE PROBABILITY VERSUS SRAM CELL SIZE
We estimate the failure probability of SRAM cells in our study using
the method described in [6]. The degree of transistor mismatch is quan-
tified by the standard deviation of each transistor’s threshold voltage
, i.e., , and as is discussed in [1], increases as tran-
sistor size decreases. The for an nMOS (pMOS) transistor with
width equal to the minimum length in a high-performance 32-nm pre-
dictive technology model is 24 mV (29.2 mV) [7]. For each statistical
sample, we apply random values to individual transistors in an
SRAM cell and check read and write failures for a 128 by 256 SRAM
array using SPICE.
Our base-line cell (C1) has the minimum width for all
6 transistors. To analyze the relative area of 6 different SRAM cells
m technology
design rule and Cadence Virtuoso layout editor since the design rule
1063-8210/$26.00 © 2011 IEEE
Abstract
The use of the Java programming language for imple-
menting server-side application logic is increasing in popu-
larity, yet there is very little known about the architectural
requirements of this emerging commercial workload. We
present a detailed characterization of the Transaction Pro-
cessing Council’s TPC-W web benchmark, implemented in
Java. The TPC-W benchmark is designed to exercise the
web server and transaction processing system of a typical
e-commerce web site. We have implemented TPC-W as a
collection of Java servlets, and present an architectural
study detailing the memory system and branch predictor
behavior of the workload. We also evaluate the effective-
ness of a coarse-grained multithreaded processor at
increasing system throughput using TPC-W and other com-
mercial workloads. We measure system throughput
improvements from 8% to 41% for a two context processor,
and 12% to 60% for a four context uniprocessor over a sin-
gle-threaded uniprocessor, despite decreased branch pre-
diction accuracy and cache hit rates.
1. Introduction
In the last few years, the world-wide web has evolved
from a global repository for static information into a
dynamic environment that provides mechanisms for con-
necting to and interacting with an ever-increasing number
of on-line databases and other sources of dynamic content.
In the pursuit of global market share and mindshare, com-
panies ranging from traditional “brick-and-mortar” retailers
to online-only startup companies are implementing web
sites that allow a high level of dynamic interaction with
their inventory, purchasing, and order databases. This tran-
sition to online electronic commerce is placing new
demands on both the software and hardware infrastructure
used to implement these complex systems. On the software
side, new implementation techniques like increasing layers
of middleware, business object frameworks, and new pro-
gramming languages such as Java are required to simplify
the task of the application programmer and reduce develop-
ment time. On the hardware side, the overhead of such soft-
ware techniques and the unique demands created by the
confluence of web connectivity, complex middleware and
application logic, as well as the high availability expected
of traditional on-line transaction processing database sys-
tems have the potential to dramatically change the behavior
of the program code being executed on the large server
computer systems that serve as the hardware platforms for
these web sites.
Historically, the Transaction Processing Performance
Council (TPC), a consortium of system and database ven-
dors, has specified standard benchmarks (e.g. TPC-A, TPC-
B, TPC-C, TPC-D, TPC-H, TPC-R) for evaluating the per-
formance of both transaction processing and decision sup-
port database systems [27]. These benchmarks have been
very useful for gauging the absolute performance and
price/performance of combined software/hardware systems.
A significant body of prior work has studied the architec-
tural requirements of such workloads [13,15,17,19,20, 22].
At the same time, the Systems Performance Evaluation
Cooperative (SPEC), a similar consortium, has developed
standard benchmarks for evaluating both static and
dynamic web content serving (SPECweb96 and
SPECweb99, respectively) [26]. While useful in their own
right, none of these benchmarks reflect the demands placed
on systems that must perform all of these functions in con-
cert. As a response to this shortcoming, the TPC has devel-
oped a new benchmark for e-commerce called TPC-W that
is modeled after an on-line bookstore, and includes many of
the elements present in such a website, including complex
application logic, a significant web serving component
including both static and dynamic web pages, and direct
transaction processing and decision support connectivity to
an online relational database containing product inventory,
customer, and order tracking information. We have imple-
mented most of the requirements specified in the TPC-W
specification and have published some of our early findings
[4]. An overview of the TPC-W specification is presented in
Section 2, and the details of our implementation are
An Architectural Evaluation of Java TPC-W
Harold W. Cain, Ravi Rajwar
Computer Sciences Department
University of Wisconsin
Madison, WI 53706
{cain,rajwar}@cs.wisc.edu
 Morris Marden, Mikko H. Lipasti
Dept. of Electrical and Computer Engineering
University of Wisconsin
Madison, WI 53706
{marden,mikko}@ece.wisc.edu
Abstract—State-of-the-art graphic processing units (GPUs) can
offer very high computational throughput for highly parallel
applications using hundreds of integrated cores. In general, the
peak throughput of a GPU is proportional to the product of the
number of cores and their frequency. However, the product is
often limited by a power constraint. Although the throughput
can be increased with more cores for some applications, it
cannot for others because parallelism of applications and/or
bandwidth of on-chip interconnects/caches and off-chip
memory are limited. In this paper, first, we demonstrate that
adjusting the number of operating cores and the
voltage/frequency of cores and/or on-chip interconnects/caches
for different applications can improve the throughput of GPUs
under a power constraint. Second, we show that dynamically
scaling the number of operating cores and the
voltages/frequencies of both cores and on-chip
interconnects/caches at runtime can improve the throughput of
application even further. Our experimental results show that a
GPU adopting our runtime dynamic voltage/frequency and core
scaling technique can provide up to 38% (and nearly 20% on
average) higher throughput than the baseline GPU under the
same power constraint.
Keywords—GPU; dynamic voltage, frequency, and core scaling;
power constraint; throughput.
I. INTRODUCTION
The throughput of processors has been improved by
increasing the number of cores for highly parallel
applications. Exploiting such high parallelism at the thread
and application levels, many-core processors have become a
major design trend in the computing industry [1]. 
Originally, GPUs were developed for computer graphics,
which requires high computational throughput capability, but
state-of-the-art GPUs now also support the execution of
general-purpose applications. The Compute Unified Device
Architecture (CUDA™) from NVIDIA has been developed
to support such applications on GPUs. It has been widely
used due to its convenient programming interface based on
standard industry programming languages, such as C++.
With hundreds of small cores, an NVIDIA GeForce 8800 can
provide 10-197× higher throughput than a 2.66GHz Core™ 2
Duo processor for data-intensive, highly-parallel applications
So far, the major improvements in GPU throughput have
been achieved in two ways: (i) integrating more hardware
resources (i.e., cores, on-chip interconnects/caches1, and off-
chip memory channels) and (ii) op rating them faster [2]-[4]
with some software-level optimizations to exploit the parallel
ar itecture of GPUs [5]-[7]. However, GPUs also consume
a larg  am unt of power (e.g., a G Force® 8800 consumes
280W [2]). This fundamentally limits further improvement in
GPU throughput by just integrating more hardware resources
and operating them at higher frequencies.
The throughput of some applications scales linearly with
the number of available cores and/or their operating
frequency. However, for example, assume that only 75% of
the cores can run simultaneously at a given frequency due to
a power constraint. In such a case, reducing the
voltage/frequency of the cores allows the GPU to utilize all
the available cores under the power constraint. This can
improve the throughput of the applications since the
throughput is approximately proportional to the product of
the frequency and the number of operating cores. The
product tends to increase with more cores operating at lower
voltage/frequency under a power constraint. This is because
the power reduction from lower voltage/frequency outweighs
the increase of power consumption due to more operating
cores. 
On the other hand, the throughput of other applications
does not scale with the number of operating cores and/or
their frequency because parallelism of applications and/or
bandwidth of on-chip interconnects/caches and off-chip
memory are limited [13]; increasing the bandwidth of on-
chip interconnects/caches and off-chip memory channels is
also limited by a power constraint. For example, if the
number of threads in an application is not sufficiently large
compared to the number of operating cores, then the
throughput does not increase with more cores. In such a case,
higher voltage/frequency with fewer cores (e.g., half of all
available cores) can lead to higher throughput for a given
power constraint. 
Furthermore, if applications have a high rate of inter-
core communications and/or memory accesses, then the
throughput may not increase with either more cores or higher
core frequency. This is because the throughput is limited by
1 On-chip interconnects and L2 caches share the same voltage/frequency
domain in the GPU architecture we investigate.
Improving Throughput of Power-Constrained GPUs Using Dynamic 
Voltage/Frequ ncy and Core Sc ling
Jungseob Lee¹, Vijay Sathisha¹, Michael Schulte², Katherine Compton¹, Nam Sung Kim¹
¹Department of Electrical and Computer Engineering, University of Wisconsin, Madison, WI, U.S.A.
²Advanced Micro Devices, Austin, TX, U.S.A.
e-mail: {jslee9, sathish}@wisc.edu, michael.schulte@amd.com, nskim3@wisc.edu




The Case for GPGPU Spatial Multitasking
Jacob T. Adriaens, Katherine Compton, Nam Sung Kim
Department of Electrical and Computer Engineering
University of Wisconsin - Madison
jtadriaens@wisc.edu, kati@engr.wisc.edu, nskim3@wisc.edu
Michael J. Schulte
AMD Research
Michael.Schulte@amd.com
Abstract
The set-top and portable device market continues to
grow, as does the demand for more performance under in-
creasing cost, power, and thermal constraints. The integra-
tion of Graphics Processing Units (GPUs) into these de-
vices and the emergence of general-purpose computations
on graphics hardware enable a new set of highly paral-
lel applications. In this paper, we propose and make the
case for a GPU multitasking technique called spatial mul-
titasking. Traditional GPU multitasking techniques, such
as cooperative and preemptive multitasking, partition GPU
time among applications, while spatial multitasking allows
GPU resources to be partitioned among multiple applica-
tions simultaneously. We demonstrate the potential benefits
of spatial multitasking with an analysis and characteriza-
tion of General-Purpose GPU (GPGPU) applications. We
find that many GPGPU applications fail to utilize available
GPU resources fully, which suggests the potential for sig-
nificant performance benefits using spatial multitasking in-
stead of, or in combination with, preemptive or cooperative
multitasking. We then implement spatial multitasking and
compare it to cooperative multitasking using simulation.
We evaluate several heuristics for partitioning GPU stream
multiprocessors (SMs) among applications and find spatial
multitasking shows an average speedup of up to 1.19 over
cooperative multitasking when two applications are sharing
the GPU. Speedups are even higher when more than two ap-
plications are sharing the GPU.
1. Introduction
Set-top and portable devices are becoming increasingly
popular and powerful. Due to the cost, power, and thermal
constraints placed on these devices, often they are designed
with a low-power general-purpose CPU and several hetero-
geneous processors, each specialized for a subset of the
device’s tasks. These heterogeneous systems increasingly
include programmable Graphics Processing Units (GPUs).
The iPhone 4, for example, contains a programmable
GPU in addition to a general-purpose CPU and several
Application-Specific Instruction Processors (ASIPs). Trans-
forming the GPU from a graphics and compute offload de-
vice to a general-purpose data-parallel processor has the po-
tential to enable entirely new classes of applications that
were previously unavailable on mobile devices due to per-
formance and power constraints.
GPGPU computations are motivated by GPUs’ tremen-
dous computational capabilities and high memory band-
width for data-parallel workloads [22]. A range of appli-
cations, from scientific computing to multimedia, are well-
suited to this form of parallelism and achieve large speedups
on a GPU. For example, Yang et al. achieve up to a 38x
speedup compared to a high-performance CPU when using
a GPU for real-time motion estimation [31].
Unfortunately, GPUs have very primitive support for
multitasking, a key feature of modern computing systems.
Multitasking provides concurrent execution of multiple ap-
plications on a single device. Advanced multitasking is
critical for preserving user responsiveness and satisfying
quality-of-service (QoS) requirements. NVIDIA’s Fermi
supports co-executing multiple tasks from the same appli-
cation on a single GPU [19]. However, even Fermi does
not allow multiple different GPGPU applications to access
GPU resources simultaneously. Other applications needing
the GPU must wait until the application occupying the GPU
voluntarily yields control. Having the application voluntar-
ily yield control of the GPU is a form of cooperative mul-
titasking. In contrast, on the CPU, the operating system
(OS) typically uses preemptive multitasking—suspending
and later resuming applications to time-share the CPU with-
out the applications’ intervention or control. Both coop-
erative and preemptive multitasking are forms of temporal
multitasking. Finally, multi-core CPUs support spatial mul-
titasking, which allows multiple applications to execute si-
multaneously on different cores.
Until GPUs better support multitasking, they will con-
tinue to remain second-class computational citizens. As
future technologies move the GPU onto the same chip as
978-1-4673-0826-7/12
Appears in the proceedings of the Tenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS),
Oct. 6-Oct. 9, 2002, San Jose, California.
Transactional Lock-Free Execution of Lock-Based Programs
Ravi Rajwar and James R. Goodman
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706 USA
{rajwar, goodman}@cs.wisc.edu
Abstract
This paper is motivated by the difficulty in writing correct
high-performance programs. Writing shared-memory multi-
threaded programs imposes a complex trade-off between pro-
gramming ease and performance, largely due to subtleties in
coordinating access to shared data. To ensure correctness pro-
grammers often rely on conservative locking at the expense of
performance. The resulting serialization of threads is a perfor-
mance bottleneck. Locks also interact poorly with thread schedul-
ing and faults, resulting in poor system performance.
We seek to improve multithreaded programming trade-offs by
providing architectural support for optimistic lock-free execution.
In a lock-free execution, shared objects are never locked when
accessed by various threads. We propose Transactional Lock
Removal (TLR) and show how a program that uses lock-based
synchronization can be executed by the hardware in a lock-free
manner, even in the presence of conflicts, without programmer
support or software changes. TLR uses timestamps for conflict
resolution, modest hardware, and features already present in
many modern computer systems.
TLR’s benefits include improved programmability, stability,
and performance. Programmers can obtain benefits of lock-free
data structures, such as non-blocking behavior and wait-freedom,
while using lock-protected critical sections for writing programs.
1  Introduction
Programming complexity is a significant problem in writing
shared-memory multithreaded applications. Although threads
simplify the conceptual design of programs, care and expertise
are required to ensure correct interaction among threads. Errors in
reasoning about appropriate synchronization among threads while
accessing shared data objects result in incorrect program execu-
tion, and may be extremely subtle.
Transactions serve as an intuitive model for coordinating
access to shared data. A transaction [7] comprises a series of read
and write operations that provide the following properties: fail-
ure-atomicity, consistency, and durability. Failure-atomicity states
a transaction must either execute to completion, or in the presence
of failures, must appear not to have executed at all. Consistency
requires the transaction to follow a protocol that provides threads
with a consistent view of the data object. Serializability is an intu-
itive and popular consistency criterion for transactions. Serializ-
ability requires the result of executions of concurrent transactions
to be as if there were some global order in which these transac-
tions had executed serially [7]. Durability states that once a trans-
action is committed, it cannot be undone.
While the concept of transactions is simple and convenient
for programmers to reason with [10], processors today provide
only restricted support for such transactions in their instruction
sets. Examples are the atomic read-modify-write operations on a
single word. The restricted size for these operations and limita-
tions placed on their use render them ineffective in providing
functionality of general transactions.
A lack of general transaction support in processors has led to
programmers often relying on critical sections to achieve some of
the functionality of transactions. Critical sections are software
constructs that enforce mutually exclusive access among threads
to shared objects and thus trivially satisfy serializability. Failure-
atomicity is difficult to achieve with critical sections because it
requires support for logging modifications performed in the criti-
cal section and then making these modifications visible instanta-
neously using an atomic operation. Critical sections therefore do
not provide failure-atomicity. Critical sections are most com-
monly implemented using a software construct known as a lock.
A lock is associated with a shared object and determines whether
the shared object is currently available. Nearly all architectures
support instructions for implementing lock operations. Locks
have become the synchronization mechanism of choice for pro-
grammers and are extensively used in various software such as
operating systems, database servers, and web servers.
Motivation. Two key limitations of lock-based critical sections
motivate the work in this paper: 1) Complex trade-off between
programmability and performance, and 2) Problems of stability
(i.e., behavior in the presence of unexpected conditions) of the
application.
Performance/programmability limitation of locks. The com-
plex trade-off between programmability and performance exists
because programmers have to reason about data sharing during
code development using static information rather than dynamic
run-time information. Programmers often use conservative syn-
chronization to easily write correct code. While such use may
guarantee correctness, provides stable software, and leads to
faster code development, it also inhibits parallelism because
threads are unnecessarily serialized. Fine-grain locks (e.g., one
lock per node in a data structure) may help performance but make
code difficult to write and error prone. Coarse-grain locks (e.g.,
one lock per data structure) help in writing correct code and
reducing errors but hurt performance. Additionally, locks can
contribute to significant overhead, serialize execution, and
degrade overall system performance [16]. Exploiting dynamic
concurrency is also often a non-trivial task [13].
Permission to make digital or hard copies of part or all of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advan-
tage and that copies bear this notice and the full citation on the first
page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To
copy otherwise, to republish, to post on servers or to redistribute to
lists, requires prior specific permission and/or a fee.
ASPLOS X 10/02 San Jose, CA, USA
© 2002 ACM 1-58113-574-2/02/0010...$5.00
C
M
& Morgan   Claypool Publishers&
SYNTHESIS LECTURES ON
COMPUTER ARCHITECTURE
Mark D. Hill, Series Editor
A Primer on Memory
Consistency and
Cache Coherence
Daniel J. Sorin
Mark D. Hill
David A. Wood
GPUWattch
†
: Enabling Energy Optimizations in GPGPUs
Jingwen Leng1, Tayler Hetherington2, Ahmed ElTantawy2, Syed Gilani3,
Nam Sung Kim3, Tor M. Aamodt2, Vijay Janapa Reddi1
1 The University of Texas at Austin, 2 University of British Columbia, 3 University of Wisconsin-Madison
ABSTRACT
General-purpose GPUs (GPGPUs) are becoming prevalent
in mainstream computing, and performance per watt has
emerged as a more crucial evaluation metric than peak per-
formance. As such, GPU architects require robust tools that
will enable them to quickly explore new ways to optimize
GPGPUs for energy efficiency. We propose a new GPGPU
power model that is configurable, capable of cycle-level cal-
culations, and carefully validated against real hardware mea-
surements. To achieve configurability, we use a bottom-up
methodology and abstract parameters from the microarchi-
tectural components as the model’s inputs. We developed a
rigorous suite of 80 microbenchmarks that we use to bound
any modeling uncertainties and inaccuracies. The power
model is comprehensively validated against measurements of
two commercially available GPUs, and the measured error is
within 9.9% and 13.4% for the two target GPUs (GTX 480
and Quadro FX5600). The model also accurately tracks
the power consumption trend over time. We integrated the
power model with the cycle-level simulator GPGPU-Sim and
demonstrate the energy savings by utilizing dynamic voltage
and frequency scaling (DVFS) and clock gating. Traditional
DVFS reduces GPU energy consumption by 14.4% by lever-
aging within-kernel runtime variations. More finer-grained
SM cluster-level DVFS improves the energy savings from
6.6% to 13.6% for those benchmarks that show clustered ex-
ecution behavior. We also show that clock gating inactive
lanes during divergence reduces dynamic power by 11.2%.
Categories and Subject Descriptors
C.1.4 [Processor Architectures]: Parallel Architectures;
C.4 [Performance of Systems]: Modeling techniques
General Terms
Experimentation, Measurement, Power, Performance
Keywords
Energy, CUDA, GPU architecture, Power estimation
†
GPUWattch is named after the original CPU power modeling framework,
Wattch [6], which enabled widespread architecture-level power analysis and
optimizations. However, our approach is independent of Wattch.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
ISCA ’13 Tel-Aviv, Israel
Copyright 2013 ACM 978-1-4503-2079-5/13/06 ...$15.00.
1. INTRODUCTION
From datacenters to power-constrained mobile devices,
performance per watt has emerged as an indispensable met-
ric for evaluating the efficiency of a GPU architecture [9,
18]. Although GPU performance models, such as GPGPU-
Sim [4], Multi2Sim [34] and MacSim [1], have enabled per-
formance oriented research on branch divergence [11,12,27],
memory bandwidth pressure [5, 32], and so forth, similar
efforts to investigate and optimize GPU energy-efficiency
problems have been difficult owing to the lack of a suit-
able power modeling infrastructure. Researchers using these
tools may inadvertently be optimizing for performance while
penalizing performance per watt. To avoid such pitfalls and
develop energy-efficient GPU architectures, we require a ro-
bust power model.
A robust power model must satisfy three requirements
to be useful for computer architecture research. It must
be (1) configurable, (2) cycle level, and (3) strongly vali-
dated against existing processor architectures using a rigor-
ous methodology. As shown in Table 1, Wattch [6] and Mc-
PAT [22] are robust CPU power models that satisfy all three
requirements, and as such have enabled new research areas
in energy-efficient CPU design. No such power model exists
for GPU architecture research. Hong and Kim [13] were the
first to propose an integrated power and performance model
for GPUs. However, their power model is not configurable
to different architectural parameters. Moreover, it is inca-
pable of providing cycle-level power estimates to evaluate
fine-grained power saving techniques such as clock gating.
In this paper, we introduce GPUWattch, a new power
model that addresses all of the aforementioned requirements.
We follow the rigorous process shown in Figure 1(a) to de-
velop the robust power model. We use a bottom-up method-
ology to build the initial model. Then we compare our sim-
ulated power with the measured hardware power to identify
any modeling inaccuracies. We resolve these inaccuracies us-
ing a special suite of 80 microbenchmarks that are designed
to create a system of linear equations that correspond to the
total power consumption. By solving for the unknowns in
the system, we progressively eliminate the inaccuracies. We
Work GPU Configurable? Cycle-level? Validated?
Wattch/McPAT [6,22] No Yes Yes Yes
Hong and Kim [13] Yes No No Yes
GPUWattch Yes Yes Yes Yes
Table 1: Robust power modeling requirements for a GPU.
Efficient Synchronization: Let Them Eat QOLB1
Alain Kägi, Doug Burger, and James R. Goodman
Computer Sciences Department
University of Wisconsin-Madison
1210 West Dayton Street
Madison, Wisconsin 53706 USA
galileo@cs.wisc.edu - http://www.cs.wisc.edu/~galileo
This work is supported in part by NSF Grant CCR-9509589 and by the
Intel Research Council.
1. Pronounced “Colby.”
This paper appears in the 24th International Symposium on Computer Architecture, June, 1997. Reprinted by permission of ACM
Minor editorial changes: May 7, 1997.
1
Copyright 1997 (c) by Association for Computing Machinery (ACM). Per-
mission to copy and distribute this document is hereby granted provided
that this notice is retained on all copies and that copies are not altererd.
Abstract
Efficient synchronization primitives are essential for achieving
high performance in fine-grain, shared-memory parallel pro-
grams. One function of synchronization primitives is to enable
exclusive access to shared data and critical sections of code. This
paper makes three contributions. (1) We enumerate the five sources
of overhead that locking synchronization primitives can incur. (2)
We describe four mechanisms (local spinning, queue-based lock-
ing, collocation, and synchronized prefetch) that reduce these syn-
chronization overheads. (3) With detailed simulations, we show the
extent to which these four mechanisms can improve the perfor-
mance of shared-memory programs. We evaluate the space of these
mechanisms using seventeen synchronization constructs, which are
formed from six base types of locks (TEST&SET, TEST&TEST&SET,
MCS, LH, M, and QOLB). We show that large performance gains
(speedups of more than 1.5 for three of five benchmarks) can be
achieved if at least three optimizing mechanisms are used simulta-
neously. We find that QOLB, which incorporates all four mecha-
nisms, outperforms all other primitives (including reactive
synchronization) in all cases. Finally, we demonstrate the superior
performance of a low-cost implementation of QOLB, which runs on
an unmodified cluster of commodity workstations.
1  Introduction
Shared-memory multiprocessors are rapidly becoming the
machines of choice for solving large, fine-grained scientific pro-
grams. Multiple factors support this trend. The advent of afford-
able desktop symmetric multiprocessors (SMPs) will increase the
application base. The successful development of shared-memory
multiprocessing standards [43] reduce the time to market by
decreasing design time and by letting manufacturers use commod-
ity parts. Both the Convex Exemplar [7] and the Sequent STING
[27] relied on these standards. The emergence of low-cost, fine-
grain software implementations of shared-memory, such as SHASTA
[38] or T0 [35] further reduce the cost of supporting the shared-
memory model. Finally, successful research prototypes such as the
Stanford DASH [25] have shown that this class of machines can
obtain excellent speedups for a wide range of programs that use
fine-grained communication.
Traditional message-passing programming models force the
programmer to embed implicit synchronization with each commu-
nication of data. Such a requirement restricts the parallelization
strategy—dynamic task distribution becomes extremely difficult,
for example. The shared-memory programming model, conversely,
uses cache coherence protocols to keep shared data consistent. The
programmer judiciously employs explicit synchronization to pro-
vide mutual exclusion for data and code, as well as synchronizing
processors between phases of computation.
The two major classes of explicit synchronization operations in
shared-memory multiprocessors are barriers and locks. Although
barriers are important to efficient shared-memory programs, they
are well-understood, and many efficient implementations have
been proposed and/or built [15, 20, 23, 32, 44]. In this study, we
focus on providing more efficient mutual exclusion through better
locks.
Locks provide individual processors with exclusive access to
shared data and a critical section of code. This exclusive access is
particularly well-suited to the fine-grained nature of many shared-
memory parallel programs. Fine-grained programs ideally associ-
ate as little data or code as possible with a critical section, mini-
mizing serialized processing, thus maximizing available
parallelism. Since access to critical sections is by definition serial-
ized among processors, large overheads when accessing a con-
tested critical section degrade both parallel performance and
potential scalability. To maximize both the performance of fine-
grain parallel applications that use locking, and the potential to
scale to larger numbers of processors, we must minimize the
delays associated with the transfer of exclusively accessed
resources.
The act of transferring control of a critical section is a complex
one, that may involve multiple remote transactions. Complex pro-
tocols have been proposed that perform this transfer efficiently,
allowing reasonable performance when there is high contention for
a lock. The complexity of these protocols causes unnecessary
delays when accessing a lock that is not held. Conversely, simple
locking schemes that can access a free lock quickly may perform
poorly in the presence of contention. This fundamental trade-off
has resulted in proposals of numerous primitives in the literature
[3, 13, 16, 26, 28, 30, 37].
This paper contains a detailed and thorough evaluation of a
range of locking primitives. To understand where the opportunities
for optimization lie, we first decompose the time associated with a
complete locking period into three phases: Transfer, Load/Com-
pute, and Release. Together, these phases form a synchronization
period, which determines the global throughput of synchronization
operations and thus determines scalability for codes that rely
heavily on locks. We then describe four mechanisms that locks
may incorporate to reduce the time spent in the three phases: local
Implementing Signatures for Transactional Memory
Daniel Sa chez, Luke Yen, Mark D. Hill, Karthik yan Sankara ingam
Department of Computer Sciences, University of Wisc nsin-Madison
http://www.cs.wisc.edu/multifacet/logtm
{daniel, lyen, markhill, karu}@cs.wisc.edu
Abstract
Transactional Memory (TM) systems must track the
read and write sets—items read and written during a
transaction—to detect conflicts among concurrent trans-
actions. Several TMs use signatures, which summarize
unbounded read/write sets in bounded hardware at a per-
formance cost of false positives (conflicts detected when
none exists).
This paper examines different organizations to achieve
hardware-efficient and accurate TM signatures. First, we
find that implementing each signature with a single k-hash-
function Bloom filter (True Bloom signature) is inefficient,
as it requires multi-ported SRAMs. Instead, we advocate
using k single-hash-function Bloom filters in parallel (Par-
allel Bloom signature), using area-efficient single-ported
SRAMs. Our formal analysis shows that both organiza-
tions perform equally well in theory and our simulation-
based evaluation shows this to hold approximately in prac-
tice. We also show that by choosing high-quality hash func-
tions we can achieve signature designs noticeably more ac-
curate than the previously proposed implementations. Fi-
nally, we adapt Pagh and Rodler’s cuckoo hashing to im-
plement Cuckoo-Bloom signatures. While this representa-
tion does not support set intersection, it mitigates false pos-
itives for the common case of small read/write sets and per-
forms like a Bloom filter for large sets.
1. Introduction
Transactional memory (TM) [13, 15] systems ease multi-
threaded programming by guaranteeing that some dynamic
code sequences, called transactions, execute atomically and
in isolation. To achieve high performance, TMs execute
multiple transactions concurrently and commit only those
that do not conflict. A conflict occurs when two concurrent
transactions perform an access to the same memory address
and at least one of the accesses is a write. A TM system
must implement mechanisms to detect these events (con-
flict detection).
An important aspect of conflict detection is recording
the addresses that a transaction reads (read set) and writes
(write set) at some granularity (e.g., memory block or
word). One promising approach is to use signatures, data
structures that can represent an unbounded number of el-
ements approximately in a bounded amount of state. Led
by Bulk [7], several systems including LogTM-SE [31],
BulkSC [8], and SigTM [17], have implemented read/write
sets with per-thread hardware signatures built with Bloom
filters [2]. These systems track the addresses read/written
in a transaction by inserting them into the read/write sig-
natures, and clear both signatures as the transaction com-
mits or aborts. Depending on the system, signatures must
also support testing whether an address is represented in it
or intersecting two signatures. A test or intersection oper-
ation may signal a conflict when none existed (a false pos-
itive), but may not miss a conflict (a false negative). False
positives cause unnecessary conflicts that may degrade per-
formance, but they do not violate transaction atomicity.
This paper seeks to improve the performance and to re-
duce the cost of hardware signature implementations. The
three main functional requirements of signature implemen-
tations are: (a) they should minimize gratuitous aliases for
small read/write sets, (b) they should gracefully degrade as
read/write sets become large, and (c) performance should be
robust to changes in workload and system size. Hardware
signature implementations should be cost-efficient (e.g., by
efficiently using state and cheaply implementing state and
logic). This paper explores the design space of signatures
with formal analysis, area analysis, and experimental per-
formance evaluation of signatures. Our contributions in-
clude:
• We show that true Bloom signatures, implemented with
a single Bloom filter of k hash functions and m state bits,
are area inefficient when implemented as k-ported mem-
ories.
• Rather we advocate parallel Bloom signatures that use
k parallel Bloom filters, each with one hash function
and m/k state bits, and only require single-ported mem-
ories. We show with probabilistic analysis that paral-
40th IEEE/ACM International Symposium on Microarchitecture
1072-4451/07 $25.00 © 2007 IEEE
DOI 10.1109/MICRO.2007.24
123
1
Appears in Computer Architecture News (CAN), September 2005
Abstract
The Wisconsin Multifacet Project has created a sim-
ulation toolset to characterize and evaluate the per-
formance of multiprocessor hardware systems 
commonly used as database and web servers. We 
leverage an existing full-system functional simula-
tion infrastructure (Simics [14]) as the basis around 
which to build a set of timing simulator modules for 
modeling the timing of the memory system and 
microprocessors. This simulator infrastructure 
enables us to run architectural experiments using a 
suite of scaled-down commercial workloads [3]. To 
enable other researchers to more easily perform such 
research, we have released these timing simulator 
modules as the Multifacet General Execution-driven 
Multiprocessor Simulator (GEMS) Toolset, release 
1.0, under GNU GPL [9].
1  Introduction
Simulation is one of the most important tech-
niques used by computer architects to evaluate 
their innovations. Not only does the target machine 
need to be simulated with sufficient detail, but it 
also must be driven with a realistic workload. For 
example, SimpleScalar [4] has been widely used in 
the architectural research community to evaluate 
new ideas. However it and other similar simulators 
run only user-mode, single-threaded workloads 
such as the SPEC CPU benchmarks [24]. Many 
designers are interested in multiprocessor systems 
that run more complicated, multithreaded work-
loads such as databases, web servers, and parallel 
scientific codes. These workloads depend upon
many operating system services (e.g., I/O, synchro-
nization, thread scheduling and igration). Fur-
thermore, as the single-chip microprocessor 
evolves to a chip-multiprocessor, the ability to sim-
ulate these machines running realistic multi-
threaded workloads is paramount to continue 
innovation in architecture research. 
Simul tion Challenges. Creating a timing simula-
tor for evaluati  multiprocessor systems with 
workloads that require operating system support is 
difficult. First, creating even a functional simulator, 
which provides no modeling of timing, is a sub-
stantial task. Providing sufficient functional fidelity 
to boot an unmodified operating system requires 
implementing supervisor instructions, and inter-
facing with functional models of many I/O devices. 
Such simulators are called full-system simulators 
[14, 20]. Second, creating a detailed timing simula-
tor that executes only user-level code is a substan-
tial undertaking, although the wide availability of 
such tools reduces redundant effort. Finally, creat-
ing a simulation toolset that supports both full-sys-
tem and timing simulation is substantially more 
complicated than either endeavor alone. 
Our Approach: Decoupled Functionality and 
Timing Simulation. To address these simulation 
challenges, we designed a modular simulation 
infrastructure (GEMS) that decouples simulation 
functionality and timing. To expedite simulator 
development, we used Simics [14], a full-system 
functional simulator, as a foundation on which var-
ious timing simulation modules can be dynami-
cally loaded. By decoupling functionality and 
timing simulation in GEMS, we leverage both the 
efficiency and the robustness of a functional simu-
lator. Using modular design provides the flexibility 
Multifacet’s General Execution-driven
Multiprocessor Simulator (GEMS) Toolset
Milo M. K. Martin1, Daniel J. Sorin2, Bradford M. Beckmann3, Michael R. Marty3, Min Xu4,
Alaa R. Alameldeen3, Kevin E. Moore3, Mark D. Hill3,4, and David A. Wood3,4
http://www.cs.wisc.edu/gems/
1Computer and Information 
Sciences Dept.
Univ. of Pennsylvania
2Electrical and Computer 
Engineering Dept.
Duke Univ.
3Computer Sciences Dept.
Univ. of Wisconsin-Madison
4Electrical and Computer 
Engineering Dept.
Univ. of Wiscon in-Madison
Virtual Circuit Tree Multicasting: A Case for On-Chip Hardware Multicast
Support
‡Natalie Enright Jerger, ⋆Li-Shiuan Peh, and ‡Mikko Lipasti
‡Electrical and Computer Engineering Department, University of Wisconsin-Madison
⋆Department of Electrical Engineering, Princeton University
Abstract
Current state-of-the-art on-chip etworks provide efficiency,
high th oughput, and low lat ncy for one-to-one (unicast)
traffic. The presence of one-to-many ( ulticast) or one-to-all
(broadcast) traffic can significantly degrade the performance
of these designs, since they rely on multiple unicasts to pro-
vide one-to-many communication. This results in a burst of
packets from a single source and is a very inefficient way
of performing multicast and broadcast communication. This
inefficiency is compounded by h proliferation of architectures
and coherence protocols that require multicast and broadcast
communication. In this paper, we characterize a wide array of
on-chip communication scenarios that benefit from hardware
multicast support. We propose Virtual Circuit Tree Multicasting
(VCTM) and present a detailed multicast router design that
improves network performance by up to 90% while reducing
network activity (hence power) by up to 53%. Our VCTM router
is flexible enough to improve interconnect performance for a
broad spectrum of multicasting scenarios, and achieves these
benefits with straightforward and inexpensive extensions to a
state-of-the-art packet-switched router.
1. Introduction
Future many-core architectures with dozens to hundreds of
nodes will require scalable and efficient on-chip communication
solutions [15]. This has motivated substantial research into
network-on-chip designs. Recent proposals [12], [14], [20],
[21], [30] have successfully driven down interconnect delay
to approach that of pure wire delay. However, one of the
implicit assumptions in the evaluation of these proposals is
that the vast majority of traffic is of a one-to-one (unicast)
nature. Unfortunately, current router architectures are extremely
inefficient at handling multicast and broadcast traffic.
In this work, we leverage several popular research inno-
vations to demonstrate that the assumption of predominantly
unicast traffic is not a valid one for on-chip networks and
motivate the design of our multicast router, Virtual Circuit
Tree Multicasting (VCTM). The inability of current router
architectures to efficiently handle multicast communication can
also have performance ramifications for unicast communica-
tions. Unicast communications occurring at the same time as a
multicast communication are likely to be delayed by the burst
of communication.
This research was supported in part by the National Science Foundation
under grants CCR-0133437, CCF-0429854, CCF-0702272, CNS-0509402, the
MARCO Gigascale Systems Research Center, an IBM PhD Fellowship, as well
as grants and equipment donations from IBM and Intel. The authors would like
to thank Niket Agarwal and Noel Eisley for their assistance with the traces
used in this study. Additionally, we thank the anonymous reviewers for their
thoughtful comments and constructive suggestions.
12
14
16
18
20
o
n
n
ec
t 
L
at
en
cy
MC (10%)
MC (5%)
MC (1%)
6
8
10
0% 10% 20% 30% 40% 50%
In
te
rc
o
Network Load (% of Link Capacity)
No MC
Figure 1. Performance of multicasts on packet-switched
interconnect
Figure 1 shows the performance of a state-of-the-art packet-
switched router in a 4x4 mesh in the presence of uniform
random traffic. This router performs very well when all injected
packets are intended for a single destination (No MC). When
we start injecting packets in the same cycle, at the same source
destined for multiple nodes, we see significant throughput
degradation; MC 1% converts 1% of injected packets into a
multicast destined for a random number of destinations (<=15).
If 1% of injected packets are multicasts the saturation point
drops from 40% capacity to 25% capacity. Saturation is defined
to be when the latency is double the zero-load latency. The
network saturates at 20% and 5% for 5% and 10% multicasts
respectively. These multicast packets are broken down into
multiple unicasts by the network interface controllers as the
packet-switched routers are not designed to handle multiple
destinations for one packet. More details about the packet-
switched router under evaluation are discussed in Section 3.
Figure 2 shows a multicast originating from node X intended
for nodes A, B, C and D. The network interface controller
creates four identical copies of the message (1A-1D). With
deterministic dimension-ordered routing, all 4 messages want
to traverse the same link in the same cycle. Messages 1B and
1D successfully arbitrate for the output in subsequent cycles.
However, the messages now intended for nodes A and C are
blocked waiting for B and D to gain access to a busy channel.
Messages intended for B and D will again compete for the same
output port.
There are several problems in this scenario. The first issue is
stalled messages 1A and 1C; this problem could be addressed
with more virtual channels and buffers at each router. The
second problem is competition for the same bandwidth; this can
be alleviated with wider links. Both of these solutions are costly
in terms of area and power and exhibit poor scalability. The
International Symposium on Computer Architecture
1063-6897/08 $25.00 © 2008 IEEE
DOI 10.1109/ISCA.2008.12
229
Multiscalar Processors
Gurindar S. Sohi Scott E. Breach T.N. Vijaykumar
sohi@cs.wisc.edu breach@cs.wisc.edu vijay@cs.wisc.edu
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706
Abstract
Multiscalar processors use a new, aggressive imple-
mentation paradigm for extracting large quantities of instruc-
tion level parallelism from ordinary high level language pro-
grams. A single program is divided into a collection of tasks
by a combination of software and hardware. The tasks are
distributed to a number of parallel processing units which
reside within a processor complex. Each of these units
fetches and executes instructions belonging to its assigned
task. The appearance of a single logical register file is main-
tained with a copy in each parallel processing unit. Register
results are dynamically routed among the many parallel pro-
cessing units with the help of compiler-generated masks.
Memory accesses may occur speculatively without
knowledge of preceding loads or stores. Addresses are
disambiguated dynamically, many in parallel, and processing
waits only for true data dependences.
This paper presents the philosophy of the multiscalar
paradigm, the structure of multiscalar programs, and the
hardware architecture of a multiscalar processor. The paper
also discusses performance issues in the multiscalar model,
and compares the multiscalar paradigm with other para-
digms. Experimental results evaluating the performance of a
sample of multiscalar organizations are also presented.
1. Introduction
The basic paradigm of sequencing through a program,
i.e., the fetch-execute cycle using a program counter, has
been with us for about 50 years. A consequence of this
sequencing paradigm is that programs are written with the
tacit assumption that instructions will be executed in the
same order as they appear in the program. To achieve high
performance, however, modern processors attempt to execute
multiple instructions simultaneously, and in some cases in a
different order than the original program sequence. This
reordering may be done in the compiler, in the hardware at
execution time, or both. Superscalar and VLIW processors
belong to this class of architectures that exploit instruction
level parallelism (ILP).
ILP processors and compilers typically convert the
total ordering of instructions as they appear in the original
program into a partial ordering determined by dependences
on data and control. Control dependences (which appear as
conditional branches) present a major obstacle to highly
parallel execution because these dependences must be
resolved before all subsequent instructions are known to be
valid.
Focusing on control dependences, one can represent a
static program as a control flow graph (CFG), where basic
blocks are nodes, and arcs represent flow of control from one
basic block to another. Dynamic program execution can be
viewed as walking through the program CFG, generating a
dynamic sequence of basic blocks which have to be executed
for a particular run of the program.
To achieve high performance, an ILP processor must
attempt to walk through the CFG with a high level of paral-
lelism. Branch prediction with speculative execution is one
commonly-used technique for raising the level of parallelism
that can be achieved during the walk. The primary constraint
on any parallel walk, however, is that it must preserve the
sequential semantics assumed in the program.
In the multiscalar model of execution, the CFG is par-
titioned into portions called tasks. A multiscalar processor
walks through the CFG speculatively, taking task-sized steps,
without pausing to inspect any of the instructions within a
task. A task is assigned to one of a collection of processing
units for execution by passing the initial program counter of
the task to the processing unit. Multiple tasks then execute in
parallel on the processing units, resulting in an aggregate
execution rate of multiple instructions per cycle.
At this level, the concept sounds simple, however, the
key to making it work is the proper resolution of inter-task
data dependences. In particular, data that is passed between
instructions via registers and memory must be routed
correctly by the hardware. Furthermore, it is in this area of
inter-task data communication that the multiscalar approach
differs significantly from more traditional multiprocessing
methods.
This paper describes the multiscalar approach to
exploiting fine-grain parallelism (or instruction-level paral-
lelism or ILP). Section 2 provides an overview of the multis-
calar paradigm. A breakdown of the distribution of the avail-
able processing unit cycles in multiscalar execution follows
in Section 3. In Section 4, we compare multiscalar with
other ILP paradigms. A performance evaluation of potential
configurations of a multiscalar processor is given in Section
1985 1990 1995 2000 2005 2010
Chen-han Ho
Advised by Karu 
Sankaralingam
Sung Jin Kim
Advised by Karu 
Sankaralingam
Tony Nowatzki
Advised by Karu 
Sankaralingam
Newsha Ardalani
Advised by Karu 
Sankaralingam
Vijay 
Thiruvengadem
Advised by Karu 
Sankaralingam
Vinay Gangadhar
Advised by Karu 
Sankaralingam
Gagan Gupta
Advised by Guri 
Sohi
Hongil Yoon
Advised by Guri 
Sohi
Srinath Sridharan
Advised by Guri 
Sohi
Jayneel Gandhi
Advised by Mark 
Hill and Mike 
Swift
Jason Power
Advised by Mark 
Hill and David 
Wood
Lena Olson
Advised by Mark 
Hill
Chris Feilbach
Advised by Mark 
Hill
Swapnil Haria
Advised by Mark 
Hill
Rathijit Sen
Advised by David 
Wood
Somayeh 
Sardashti
Advised by David 
Wood
Muhammad 
Shoaib bin Atlaf
Advised by David 
Wood
Nilay Vaish
Advised by David 
Wood
Marc Orr
Advised by David 
Wood
Joel Hestness
Advised by David 
Wood and Steve 
Keckler
Hamid Ghasemi
Advised by Nam 
Kim
David Palframan
Advised by Nam 
Kim and Mikko 
Lipasti
Hao Wang
Advised by Nam 
Kim
Amin Farmahini
Advised by Nam 
Kim
Zhenhong Liu
Advised by Nam 
Kim
Ahmed Abulila
Advised by Nam 
Kim
Hadi 
Asgharimoghaddam
Advised by Nam 
Kim
Michael Mishkin
Advised by Nam 
Kim
Paula Aguilera
Advised by Nam 
Kim
Mohammad Alian
Advised by Nam 
Kim
Anmol Mohanty
Advised by Nam 
Kim
Deahoon Kim 
(postdoc)
Advised by Nam 
Kim
Sean Franey
Advised by Mikko 
Lipasti
Arslan Zulfiqar
Advised by Mikko 
Lipasti
Dibakar Gope
Advised by Mikko 
Lipasti
Current Wisconsin 
Architecture Students
Richard E. Kessler
Analysis of Multi-Megabyte 
Secondary CPU Cache 
Memories
Advised by Mark D. Hill
First employment at Cray 
Research
Sarita V. Adve
Designing Memory Consistency 
Models for Shared-Memory 
Multiprocessors
Advised by Mark D. Hill
First employment at Rice 
University
Madhusudhan (Madhu) Talluri
Use of Superpages and 
Subblocking in the Address 
Translation Hierarchy
Advised by Mark D. Hill
First employment at Sun 
Microsystems
Ioan is (Yannis) Schoi as
Fine Grain D stribute  S ared
Memory Systems on Clusters 
of Workstations
Advised by M rk D. Hill
First employment at Int l 
Corporat on
Shubhendu (Shubu) Mukherjee
Design and Evaluation of 
Network Interfaces for System 
Area Networks
Advised by Mark D. Hill
First employment at Digital 
Equipment Corporation
Trishul Chilimbi
Cache-Conscious Data 
Structures–Design and 
Implementation
Advised by Mark D. Hill and 
James R. Larus
First employment at 
Microsoft Corporation
Eric Schnarr
Applying Programming 
Language Implementation 
Techniques to Processor 
Simulation
Advised by Mark D. Hill and 
James R. Larus
First employment at 
Hypercosm Corporation
Anastasia (N tass ) Ailamaki
Architecture-Conscious 
Database Systems
Advised by Ma k D. Hi l and 
David DeWitt
First employment at 
Carnegie-Mellon U iversity
Milo Martin
Token Coherence
Advised by Mark D. Hill
First employment at 
University of Pennsylvania
Min Xu
Race Recording for 
Multithreaded Deterministic 
Replay using Multiprocessor 
Hardware
Advised by Mark D. Hill and 
Rastislav Bodik
First employment at VMware
Brian Fields
Using Criticality to Attack 
Performance Bottlenecks
(Completed at UC-Berkeley)
Advised by Mark D. Hill and 
Rastislav Bodik
First employment at NVIDIA
Michael R. Marty
Cache Coherence Techniques 
or Mul icor  Processors
Advised by Mark D. Hill
Fir t em loyment at Google
Luke Ye
Signatures in Transactional 
Memory Systems
Advised by Mark D. Hill
First employment at AMD
Jayaram Bobba
Hardware Support for 
Efficient Transactional and 
Supervised Memory Systems
Advised by Mark D. Hill
First employment at Intel
De ek R. Hower
Acoherent Shared Memory
Advised by Mark D. Hill
First employ ent t AMD 
Postdoc
Arkaprava (Arka) Basu
Revisiting Virtual Memory
dvised by Mark D. Hill and 
Michael Swift
First employment at AMD
Alvin Lebeck
Tools and Techniques for 
Memory System Design and 
Analysis
Advised by David A. Wood
First employment at Duke 
University
Steve Reinha dt
Mechanisms for Distributed 
Shared Memory
A vised by David A. Wood
First employment at 
University of Michigan
Babak Falsafi
Fine-Grain Protocol Execution 
Mechanisms & Scheduling 
Policies on SMP Clusters
Advised by David A. Wood
First employment at Purdue 
University
Daniel Sorin
Using Lightweight Checkpoint/
Recovery to Improve the 
Availability and Designability of 
Shared Memory Multiprocessors
Advised by David A. Wood
First employment at Duke 
University
Alaa Alameldeen
Using Compression to 
Improve Chip Multiprocessor 
Performance
Advised by David A. Wood
First employment at Intel 
Microprocessor Research Lab
Brad Beckmann
Managing Wire Delay in Chip 
Multiprocessor Caches
Advised by David A. Wood
First employment at Microsoft 
Windows Server Performance
Kevin Moore
Log-based Transactional 
Memory
Advised by David A. Wood
First employment at Sun 
Microsystems Labs
Dan Gibson
Scalable Cores in Chip 
Multiprocessors
Advised by David A. Wood
First employment at Google 
Labs (Madison)
Yasuk  Wat n be
On Power Proportional 
Processors
Advised by David A. Wood
First employment at AMD 
Research
Mark Friedman
An Architectural 
Characterization of Prolog 
Execution
Advised by Gurindar S. Sohi
First employment at Trinity 
University
Manoj Franklin
The Multiscalar Architecture
Advised by Gurindar S. Sohi
First employment at Clemson 
University
Dio isios Pnevmatikat s
Incorporating Guarded 
Execution int  Existing
Instruction Sets
Advised by Gurindar S. Sohi
Fir t employment at FORTH-
ICS
T dd Austin
H rdware and Software 
Mechanisms for Reducing 
Load Latency
Advised by Gurindar S. Sohi
First employme t at Intel 
T. N. Vijaykumar
Compiling for the Multiscalar 
Architecture
Advised by Gurindar S. Sohi
Fir t employment at Purdue 
University
Andreas Moshovos
Memory Dependence 
Prediction
Advised by Gurindar S. Sohi
First employment at 
Northwestern University
Avinash Sodani
Dynamic Instruction Reuse
Advised by Gurindar S. Sohi
First employment at Intel
Amir Roth
Pre-Execution via Speculative 
Data-Driven Multithreading
Advised by Gurindar S. Sohi
First employment at 
University of Pennsylvania
Craig Zilles
Master/Slave Speculative 
Parallelization and 
Approximate Code
Advised by Gurindar S. Soh
First employment at UIUC
Jichuan Chang
Cooperative Caching for Chip 
Multiprocessors
Advised by Gurindar S. Sohi
First employment at HP Labs
Koushik Chakraborty
Over-provisioned Multicore 
Systems
Advised by Gurindar S. Sohi
First employment at Utah 
State University
Philip Wells
Dynamic Heterogen it : 
Vi tu liz tion for the 
Multi e Era
Advised by Guri dar S. Sohi
First employ e t at G gle
Matthew Allen
Data-Driven Decomposition 
of Sequential Programs for 
Determinate Parallel Execution
Advised by Gurindar S. Sohi
First employment at Amazon
Marc de Kruijf
Compiler Construction of 
Idempotent Regions and 
Applications in Architecture 
Design
Advised by Karu 
Sankaralingam
First employment at Google
Emily Blem
Unified Models to Study 
Multicore Scaling Limits and 
Accelerator-based Solutions
Advised by Karu 
Sankaralingam
First employment at Google
Venkatra an Govindaraju
E ergy Efficient Computing 
thr ugh Compiler Assist d 
Dynamic Specialization
Advised by Karu 
Sankaralingam
First employment at Oracle
Kevin Lepak
Exploring, Defining, and 
Exploiting Recent Store Value 
Locality
Advised by Mikko H. Lipasti
First employment at 
Advanced Micro Devices
Ilhyun Kim
Macro-op Scheduling and 
Execution
Advised by Mikko H. Lipasti
First employment at Intel
Jason F. Cantin
Co rse-grain coherence 
tracking
Advised by Mikko H. Lipasti
First employment at IBM
Harold (Trey) Cain
Detecting and Exploiting 
Causal Relationships in 
Hardware Shared-Memory 
Multiprocessors
Advised by Mikko H. Lipasti
First employment at 
Qualcomm
Gordon Bell
Latency- and Error-Tolerant 
Redundant Execution
Advised by Mikko H. Lipasti
First employment at IBM
Lixi  Su
Streamlined atomic execution 
for Java
Advised by Mikko H. Lipasti
First employment at 
Qualcomm
Natalie Enright Jerger
Chip Multiprocessor 
Coherence and Interconnect 
System Design
Advised by Mikko H. Lipasti
First employment at University 
of Toronto ECE Dept
Eric Hill
Understanding and mitigating 
the effects of soft errors in logic
Advised by Mikko H. Lipasti
First employment at Intel
Erika Gunadi
CRIB: Consolidated Rename, 
Issu , and Bypass
Advised by Mikko H. Lipa ti
Fi st employment at Stealth 
Startup
Dana Vantrease
Optical Tokens in Many-core 
Processors
Advised by Mikko H. Lipasti
First employment at 
Qualcomm
Atif Hashimi
Cortical Columns: A Non Von 
Neumann Computational 
Abstraction
Advised by Mikko H. Lipasti
First employment at 
Thalchemy
Andrew Nere
Computing with Hierarchical 
Attractors of Spiking Neurons
Advised by Mikko H. Lipasti
First employment at 
Thalchemy
Jungseob Lee
Optimizing Throughput 
and Power Consumption of 
Graphics Processing Units 
Advised by Nam Sung Kim
First employment at Intel
Men-Chow Chiang
Memory System Design for 
Bus Based Multiprocessors
Advised by Gurindar S. Sohi
First employment at IBM 
Sriram Vajapeyam
Instruction Level 
Characterization of the Cray 
Y-MP Processor
Advised by Gurindar S. Sohi
First employment at Cray 
Research
Scott Breach
Design and Evaluation of a 
Multiscalar Processor
Advised by Gurindar S. Sohi
First employment at Compaq 
J. Adam Butts
Optimizing Inter-Instruction 
Value Communication through 
Degree of Use Prediction
Advised by Gurindar S. Sohi
First employment at IBM 
Research
S isan hosh Balakr sh an
Progra  Demultipl xing: 
Dat -flow Based Spec lative 
P rallelization of Methods in 
Sequential Program
Advised by Gurindar S. Sohi
Fir t employment at Intel
Steve Kunkel
Pipelined processing of fine-
grain parallelism
Advised by James E. Smith
First employment at IBM
Subbarao Palacharla
Complexity-effective 
superscalar processors
Advised by James E. Smith
First employment at Intel
Quinn Able Jacobson
High-performance frontends 
for trace processors
Advised by James E. Smith
First employment at Sun 
Microsystems
Eric Rotenberg
Trace Processors: Exploiting 
Hierarchy and Speculation
Advised by James E. Smith
First employment at NC State 
University
Timothy Hume Heil
Relational profiling in 
multithreaded virtual 
machines
Advised by James E. Smith
First employment at IBM
S. Su ra ya Sa try
Techniques for Transparent 
Program Specialization In 
Dynamic Optimizers
A vised by James E. S ith 
an  Ras Bodik
First employment at Environment 
Support Group, Bangalore
Ashutosh Sham Dhodapkar
Autonomic management of 
adaptive microarchitectures
Advised by James E. Smith
First employment at AMD
Ho-Seop Kim
A co-designed virtual 
machine for instruction-level 
distributed processing
Advised by James E. Smith
First employment at Intel
Shiliang Hu
Efficient binary translation in 
co-designed virtual machines
Advised by James E. Smith
First employment at Intel
Tejas Karkhanis
Automated design of 
application-specific 
superscalar processors
Advised by James E. Smith
First employment at AMD
Nidhi Aggarwal
Achieving high availability 
with commodity hardware 
and software
Advised by James E. Smith
First employment at 
McKinsey & Company
Kyle J. Nesbit
Virtual Private Machines: 
A resource abstraction for 
multicore computer systems
Advised by James E. Smith
First employment at Google
Steve Scott
Toward the design of large-
scale, shared-memory 
multiprocessors
Advised by James R. 
Goodman
First employment at Cray 
Research
Ross Evan Johnson
Extending the scalable coherent 
interface for large-scale shared-
memory multiprocessors
Advised by James R. 
Goodman
First employment at IBM
Stefanos Kaxiras
Identification and optimization 
of sharing patterns for scalable 
shared-memory multiprocessors
Advised by James R. 
Goodman
First employment a  Bell La s
Doug Burg r
Hardware techniques to 
improve the performanc  of the 
processor/memory interface
Advised by James R. 
Good an
First employment at 
University f Texas at Austin
Alain Kagi
Mechanisms for efficient 
shared-memory, lock-based 
synchronization
Advised by James R. 
Goodman
First employment at Intel
Ravi Rajwar
Speculation-based techniques fo  
transactional lock-fre  exe ution 
of lock-based programs
Advis d by James R. 
Goodma
First employment at Intel
Mitch Hayenga
Power-Efficient Loop 
Execution Techniques
Advised by Mikko H. Lipasti
First employment at ARM 
Inc.
Daniel W. Chang
3D Stacked Memories for 
Digital Signal Processors
Advised by Nam Sung Kim 
and Michael Schulte
First employment at Rose-
Hulman
Syed G la
Compute efficie t embedded 
processors
Advis d by Na  Sun  Kim 
a d Michael Schulte
First employment at AMD
A hishek Sinkar
Improving Performance, Power 
Efficiency, Yield, and Reliability 
Using Programmable Power-
g i g Techniques
Advised by Nam Sung Kim
First employment at Oracle
Shlomo Weiss
Very high performance scalar 
processing
Advised by James E. Smith
First employment at 
University of Maryland
Craig Sheppard Holt
Diagnosis and self-diagnosis 
of digital systems
Advised by James E. Smith
First employment at Tufts 
University
Matthew K. Farrens
The design and analysis of a 
high-performance single-chip 
processor
Advised by Andrew Pleszkun
First employment at University 
of California, Davis
Koujuch Liou
Design of pipelined memory 
systems for decoupled 
architecures
Advised by James R. 
Goodman
Jian-Tu Hsieh
Performance evaluation of the 
pipe computer architecture
Advised by James R. 
Goodman
Honesty Young
Evaluation of a decoupled 
computer architecture and the 
design of a vector extension
Advised by James R. 
Goodman
First employment at IBM 
Wei-Chung Hsu
Register allocation and code 
scheduling for load/store 
architectures
Advised by James R. 
Goodman
First employment at Cray 
Mark D. Hill
PhD University of California, Berkeley
David A. Wood
PhD University of California, Berkeley Mikko H. Lipasti
P D Carnegie Mellon University
Nam Sung Kim
PhD from University of Michigan
Gurindar S. Sohi
PhD University of Illinois at 
Urbana-Champaign
Karthikeyan (Karu) 
Sankaralingam
PhD The University of Texas at Austin
James E. Smith
PhD University of Illinois at 
Urbana-Champaign
James R. Goodman
PhD University of California, Berkeley
Karthikeyan Sankaralingam
Young Computer Architect 
Award 
for
 outstanding contributions in the field of computer 
architecture in both research and education
Gurindar (Guri) Sohi
Eckert-Mauchly Award
for
pioneering widely used micro-architectural 
techniques for instruction-level parallelism
Jam s R. Go dman
E k r -M u y Award
for
pioneering contributions to the architecture of
s a ed-memory multiprocessors
James E. Smith
Eckert-Mauchly Award
for
fundamental contributions to high performance 
micro-architecture, including saturating counters 
for branch prediction, reorder buffers for precise 
exceptions, decoupled access/execute architectures, 
and vector supercomputer organization memory, 
and interconnects.
Gene M. Amdahl
Eckert-Mauchly Award
for
outstanding innovations in computer architecture, 
including pipelining, instruction look- ahead and 
cache memory
Doug Burger
Maurice Wilkes Award
for
contributions to spatially distributed processor and 
memory systems architecture
Sarita Adve
Maurice Wilkes Award
for
formalization of memory consistency models, 
especially data-race free models, and their influence 
on both hardware and high-level languages
Todd Austin
Maurice Wilkes Award
for
innovative contributions in Computer Architecture 
including the SimpleScalar Toolkit and the DIVA 
and Razor architectures
Andreas Moshovos
Maurice Wilkes Award
for
foundational contributions to the area of memory 
dependence prediction
Gurindar S. Sohi
Maurice Wilkes Award
for
seminal contributions in the areas of high issue rate 
processors and instruction level parallelism
Steve Scott
Maurice Wilkes Award
for
For seminal contributions to the architecture and 
design of high-performance computer systems and 
interconnection networks
R vi Rajwar
Maurice Wilk  Awa
f r
contributions to the desi n a  comm rc aliz tion of 
hardware transact l memory
Shubu Mukherjee
Maurice Wilkes Award
for
outstanding contributions to modeling and design of 
soft-error tolerant microarchitectures
Mark D. Hill
Distinguished Service Award 
for
 leadership in improving the architecture 
community’s review process, organizing computer 
architecture research information on-line, and 
providing outstanding service on the SIGARCH 
Executive Committee
Gurindar S. Sohi
National Academy of Engineering, 
Computer Science
for
contributions to the design of high-performance, 
superscalar computer architectures

