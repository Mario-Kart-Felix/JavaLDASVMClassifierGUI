Tight Load Balancing via Randomized Local Search∗
Petra Berenbrink† and Peter Kling‡
Simon Fraser University, Burnaby, Canada
Christopher Liaw§ and Abbas Mehrabian¶
University of British Columbia, Vancouver, Canada
July 3, 2017
Abstract
We consider the following balls-into-bins process with n bins and m balls: Each
ball is equipped with a mutually independent exponential clock of rate 1. Whenever
a ball’s clock rings, the ball samples a random bin and moves there if the number of
balls in the sampled bin is smaller than in its current bin.
This simple process models a typical load balancing problem where users (balls)
seek a selfish improvement of their assignment to resources (bins). From a game
theoretic perspective, this is a randomized approach to the well-known KP-model [16],
while it is known as Randomized Local Search (RLS) in load balancing literature [12,
11]. Up to now, the best bound on the expected time to reach perfect balance was
O
(
(lnn)2 + ln(n) · n2/m
)
due to [11]. We improve this to an asymptotically
tight O
(
ln(n) + n2/m
)
. Our analysis is based on the crucial observation that
performing destructive moves (reversals of RLS moves) cannot decrease the balancing
time. This allows us to simplify problem instances and to ignore “inconvenient moves”
in the analysis.
Keywords: Balls-into-bins; load balancing; randomized local search; coupling
∗A preliminary version of this paper appeared in proceedings of 2017 IEEE International Parallel and
Distributed Processing Symposium (IPDPS’17).
†Now affiliated with University of Hamburg, Germany. Email: petra@sfu.ca
‡Now affiliated with University of Hamburg, Germany. Email: pkling@sfu.ca
§Supported by an NSERC graduate scholarship. Email: cvliaw@cs.ubc.ca
¶Affiliated with both UBC and SFU when this work was done. Supported by a PIMS Postdoctoral
Fellowship and an NSERC Postdoctoral Fellowship. Email: abbasmehrabian@gmail.com.
1
ar
X
iv
:1
70
6.
09
99
7v
1 
 [
cs
.D
C
] 
 3
0 
Ju
n 
20
17
1 Introduction
We consider a system of n identical resources and m identical users. Each user must be
assigned to exactly one resource. A user on resource i experiences a load equal to the
number of users on resource i. Users can migrate between resources, and the goal is to
find a fast and simple migration strategy that reaches a perfectly balanced state in which
the load experienced by the users differs by at most 1.
Such load balancing (or reallocation) problems are well-studied and have a multitude of
applications, ranging from scheduling in peer-to-peer systems [20] and channel allocation
in wireless networks [19] to numerical applications such as computation of dynamics [7].
In the past decade, scalability and robustness concerns caused a shift from relatively
complex centralized protocols to simple, often randomized distributed protocols. Indeed,
consider applications such as multicore computers, routing in and between data centres,
or load distribution in peer-to-peer networks. Due to the sheer size of these systems,
maintainability issues, and increasing user demand, we have to push for distributed
protocols that are easy to implement and that do not rely on global knowledge or user
coordination.
1.1 Protocol and Results in a Nutshell
We analyze the following natural and simple load balancing process: Each of the m users
is activated by an independent exponential clock of rate 1. Upon activation, a user chooses
one of the n resources uniformly at random and compares his currently experienced load
with the load it would experience at the new resource. He migrates to the new resource if
and only if doing so does not result in a worse load. From a game theoretic perspective,
this is a simple randomized approach to the well-known KP-model with unit weights and
capacities [16]. In load balancing, this strategy is known as Randomized Local Search
(RLS) [12, 11]. (Here, “local” refers to the closeness of two consecutive solutions in the
solution space, since they differ by the placement of at most one ball.)
Our main result (Theorem 1) is that RLS reaches perfect balance in expected time
O(ln(n) + n2/m), and with high probability in time O(ln(n) + ln(n) · n2/m). These bounds
are asymptotically tight and improve the previously best bounds [11] by a logarithmic factor.
At the heart of our improvement lies the simple but tremendously useful Destructive
Majorization Lemma (DML, see Section 4). The DML formalizes the intuition that
performing destructive moves (the reverse of a move permitted by RLS) cannot result in a
2
speedup. This allows us to reverse unwanted protocol moves during the analysis. Moreover,
in any analysis phase we can simplify the given configuration to a worst-case instance for
that phase, reducing the number of cases to consider.
We continue with a survey of related literature in Section 2. Formal problem and
protocol definitions are given in Section 3. Section 4 states our main result and introduces
the above mentioned destructive moves argument. In Section 5 we gather some auxiliary
results. The analysis of RLS can be found in Section 6. The paper closes with a short
conclusion in Section 7
2 Related Work
The following literature survey on load balancing adapts the balls-into-bins terminology
and the notation n for the number of bins (resources) and m for the number of balls
(users). (Note that n and m may be swapped in some other papers.) Balls-into-bins games
come in a vast number of variations and have a long tradition to model load balancing and
similar problems [15]. Many of the most recent results consider the effect of the “power
of 2 choices” [17] and processes that are in some form “self-stabilizing” [8, 3, 2]. We refer
to [3, 2] for a recent and comprehensive overview of these variants. Here, we focus on
three classes of more closely related balls-into-bins processes:
1. Local Search: With respect to our work, the most relevant type of protocols are
local search protocols, where balls are sequentially activated and relocated with the goal to
achieve a perfectly balanced situation. The term “local” is with respect to the solution
space, since one step of the protocol changes the current solution by the placement of at
most one ball. Protocols in this class are typically quite simple in that movement decisions
depend only on the involved bins. Most closely related to our work are [12, 11]. They
study exactly the same process (RLS). [12] claims an O(n2) upper bound on the expected
time to reach perfect balance. This is improved by [11] to O
(
(ln(n))2 + ln(n) · n2/m
)
,
which is O
(
(ln(n))2
)
for the important case m  n. In the present work, we provide a
tight upper bound of expected time Θ(ln(n) + n2/m) (i.e., Θ(lnn) for m n).
[9] study another local search protocol. Here, initially each ball picks two alternative
bins and is placed arbitrarily in one of them. Now, in each step of the protocol a pair of
bins (b1, b2) is chosen uniformly at random. If there is a ball in b1 with alternative bin b2,
then this ball is placed in the least loaded bin among b1 and b2. One of the major results
in [9] is that if the balls are initially placed via the power of 2 choices, then perfect balance
3
is reached in nO(1) steps (the hidden constant is ≥ 4). In the same situation, RLS needs
only O(n2) activations (see Phase 2 in Section 6). Moreover, RLS can be started from an
arbitrary load situation.
2. Selfish Load Balancing: Another strain of related work has a game theoretical
background and is published under the theme of selfish load balancing (see [21] for
a comprehensive survey). A key difference to local search protocols is that balls act
simultaneously. This introduces a problem: moves that, in isolation, improve the load of a
ball might become bad if many balls perform this move. So, while one can compare the
balancing times to local search protocols, such a direct comparison should be taken with a
grain of salt. (In one selfish load balancing time step all m balls are activated. Similarly,
in one time unit of RLS m balls are activated in expectation.) In particular, the results
below suggest that the time to perfect balance in selfish load balancing has an inherent
dependency on m, while there is no such dependency for local search protocols.
[10] consider selfish load balancing protocols with global knowledge (e.g., the average
load). This allows them to reach perfect balance in expected O(ln lnm+ lnn) steps. [4]
consider a protocol without global knowledge. Here, balls move to a randomly sampled bin
with a probability depending on the load difference. They bound the expected balancing
time by O(ln lnm+ n4). In a follow-up work, [5] suggested another protocol with expected
balancing time O(lnm+ n · lnn). In comparison with [10], these results indicate that
avoiding global knowledge in selfish load balancing might increase the dependency on n
substantially.
3. Threshold Load Balancing: A third series of articles evolves around the idea of
threshold load balancing : each ball has a threshold and moves with a certain probability to
a random bin whenever its experienced load is above that threshold (note that [10] also
falls into this category). As in selfish load balancing, balls act simultaneously (resulting
in a similar, seemingly inherent dependency on m). An interesting observation is that
the RLS protocol can be seen as a (sequential) threshold protocol with an adaptive, local
threshold (the sampled bin’s load).
[1] introduced the idea of threshold load balancing and gave a protocol that balances up
to a constant multiplicative factor in time O(lnm) and up to an additive constant in time
O(n2 · lnm). [13, 14] extended these protocols to general graphs. Recent improvements
by [6] show that, on general graphs, one can balance up to a constant multiplicative factor
in time O(τmix · lnm), τmix being the graph’s mixing time.
4
3 Model and Notation
We describe our load balancing problem in terms of balls and bins. There are n bins (re-
sources/processors) and m balls (users/tasks). We use the shorthands [n] := { 1, 2, . . . , n }
and [m] := { 1, 2, . . . ,m } for the set of bins and balls, respectively. A configuration
` = (`i)i∈[n] ∈ Nn0 is an n-dimensional vector with
∑
i∈[n] `i = m. Its i-th component
`i ∈ N0 denotes the number of balls in bin i (its load).
We seek a simple distributed load balancing protocol (to be executed by each ball)
such that all bins end up with almost the same load. To define this formally, let ∅ := m/n
denote the average load of the system. The discrepancy of configuration ` is disc(`) :=
maxi∈[n]|`i−∅|. We say a configuration ` is x-balanced if disc(`) ≤ x and perfectly balanced
if disc(`) < 1.
Protocol Description. Let us formally describe the Randomized Local Search (RLS)
protocol. Each ball is equipped with an exponential clock of rate 1, and the clocks are
mutually independent. A ball is activated whenever its clock rings. Consider a configuration
` and assume ball j ∈ [m] in bin i ∈ [n] is activated. Then, it chooses a destination bin
i′ ∈ [n] uniformly at random and moves from i to i′ if and only if `i ≥ `i′ + 1. Let us
remark that the protocol studied by [12, 11] is slightly different: they allow movement
from i to i′ if `i > `i′ + 1. However, since the bins and the balls are identical, the two
protocols have precisely the same balancing time.
Randomized Local Search (RLS)
1 {code executed by ball j in bin i when j is activated}
2 sample random bin i′
3 if `i ≥ `i′ + 1:
4 move to bin i′
This describes a continuous time stochastic process. Starting from an initial configura-
tion ` we write `(t) for the configuration at time t. Note that ` depends on the random
ball activations as well as the random destination bin choices of each ball. Notice that
RLS has the desirable properties that the discrepancy never increases, the minimum load
never decreases, and the maximum load never increases.
5
Additional Notation. We use N := { 1, 2, . . . } for the (positive) natural numbers and
N0 := { 0, 1, 2 . . . } to include zero in the natural numbers. For any k ∈ N we define
Hk :=
∑k
i=1 1/i = ln(k) + O(1) as the k-th harmonic number. Given two random variables
A and B, we write B  A if A stochastically dominates B (i.e., if Pr(A ≥ x) ≥ Pr(B ≥ x)
for all x ∈ R), and we write A d= B if A and B are equal in distribution. We say an event
E holds with high probability (w.h.p.) if Pr(E) ≥ 1−n−Ω(1). Bin(n, p) denotes a binomial
random variable with parameters n and p, and Exp(λ) denotes an exponential random
variable with parameter λ.
4 Results and Proof Outline
Our main result is the following theorem.
Theorem 1. Consider a system of n identical bins and m identical balls in an arbitrary
initial configuration. Let T be the time when RLS reaches a perfectly balanced configuration.
We have E[T ] = O(ln(n) + n2/m) and w.h.p. T = O(ln(n) + ln(n) · n2/m).
As observed in [11], our bounds are asymptotically tight. Indeed, assume that initially
all balls are in the same bin. To reach a perfectly balanced configuration, we need to
activate at least m − ∅ balls. The expected time to do so is at least ∑mk=∅+1 1/k =
Hm − H∅ = Ω(lnn), yielding the first term in the lower bound. For the second term,
suppose ∅ is an integer, and consider a configuration in which exactly one bin has load
∅ + 1, one other bin has load ∅− 1, and every other bin has load ∅. The time to balance
perfectly is exactly the time until one of the balls in the overloaded bin is activated and
samples the underloaded bin. The latter is an exponential random variable with parameter
(∅ + 1) · 1/n. Thus, the expected time to reach perfect balance is n/(∅ + 1) = Ω(n2/m).
Requiring a high probability result gives an additional lnn factor.
Destructive Moves. Before we continue, we state an auxiliary lemma which is used
throughout our analysis. In a nutshell, it states that reversing a ball movement of RLS
cannot improve the time to reach perfect balance. This intuitively simple observation
turns out to be extremely useful. Basically, we will be able to reduce arbitrary initial
configurations to “well shaped” configurations (decreasing the number of cases to consider)
and to ignore certain (at the moment unwanted) moves of the protocol.
To formalize this idea, consider a configuration `. We call a movement of a ball from
bin i to bin j destructive if `i ≤ `j + 1. Note that a movement is destructive if and only if
6
RLS
both
destructive
5 10 15
bin ID
lo
a
d
Figure 1: Illustration of RLS moves versus destructive moves.
it is the reversal of a valid protocol move. Also note that, if `i = `j + 1, then a move from
i to j is both a valid protocol move and a destructive move (see Figure 1). Such a move is
called a neutral move.
Lemma 2 (Destructive Majorization Lemma). For any t ≥ 0, consider the load vector
`(t) resulting from protocol RLS at time t. Let ˜̀(t) denote the load vector resulting from
RLS at time t under the presence of an adversary who performs an arbitrary number of
destructive moves after each ball movement. Then disc(`(t))  disc(˜̀(t)).
We remark that the adversary is allowed to have full knowledge of the protocol and its
random choices (even future).
Proof. For k ∈ N0, consider the random process P (k) that executes our protocol starting
in the initial configuration `(0) under the presence of the first k adversarial (destructive)
moves (ignoring any further adversarial moves). Let `(k)(t) denote the configuration at
time t under process P (k). Note that `(0)(t) d= `(t) and `(∞)(t) d= ˜̀(t) for all t ≥ 0. We
show that for all k ∈ N0 and t ≥ 0 we have disc
(
`(k)(t)
)
 disc
(
`(k+1)(t)
)
. The lemma
follows from this via the transitivity of stochastic domination.
Call a configuration `′ close to a configuration ` if `′ is constructed from ` by at most
one destructive move. Two immediate observations are:
7
(i) Either `′ = ` or there are two bins iL 6= iR with `iR ≤ `iL + 1 such that `′iL = `iL + 1,
`′iR = `iR − 1, and `′i = `i for all i 6∈ { iL, iR } (i.e., `′ is constructed from ` by a
destructive move from iR to iL. (If you think of the bins ordered non-increasingly,
the destructive move goes from Right (iR) to Left (iL).)
(ii) We have disc(`) ≤ disc(`′).
Fix a k ∈ N0 and consider the processes P (k) and P (k+1). Initially, we have `(k)(0) =
`(k+1)(0) and, thus, `(k+1)(0) is close to `(k)(0). To show the majorization disc
(
`(k)(t)
)

disc
(
`(k+1)(t)
)
, it is sufficient (by the above observations) to define a coupling between
P (k) and P (k+1) that maintains `(k+1)(t) being close to `(k)(t) for all t ≥ 0. So fix t ∈ N
and assume `(k+1)(t− 1) is close to `(k)(t− 1). To simplify notation, let ` := `(k)(t− 1)
and `′ := `(k+1)(t− 1).
We claim that without loss of generality, we may let both ` and `′ be sorted non-
increasingly, such that `1 ≥ `2 ≥ · · · ≥ `n and `′1 ≥ `′2 ≥ · · · ≥ `′n. The reason is that RLS is
ignorant of the bin order, so we can assume it sorts configurations non-increasingly before
each step. Also, sorting both ` and `′ maintains `′ being close to `: Assume ` is sorted and
construct `′ from ` by a destructive move from i′R to i′L. This implies i′L ≤ i′R. Let ~̀′ be
the sorted version of `′, iR := max { i ∈ [n] | `i = `i′R }, and iL := min { i ∈ [n] | `i = `i′L }.
Then ~̀′ is constructed from ` by a destructive move from iR to iL and iL ≤ iR.
If `′ = ` we use the identity coupling. Otherwise, `′ is constructed from ` by a
destructive move from a bin iR to a bin iL with iL < iR. Without loss of generality, let
m be the ball in which ` and `′ differ and assume all other balls j ∈ [m− 1] are in the
same bin in configuration ` and `′. We couple the random choices of P (k+1) to the random
choices of P (k) as follows: Assume P (k) activates ball j ∈ [m] who is in source bin iS in `
and chooses destination bin iD ∈ [n] (the iD-th fullest bin in `). Then P (k+1) activates j
and chooses destination bin iD.
See Figure 2 for an illustration of the coupling and the following case discrimination. It
remains to show that the resulting configuration `(k+1)(t) of P (k+1) is close to the resulting
configuration `(k)(t) of P (k). This is immediate if ` = `′ (since then we use the identity
coupling). Otherwise, `′ is constructed from ` by a destructive move from a bin iR to
a bin iL with iL < iR. We distinguish the following cases depending on the source and
destination bins of process P (k):
(1) iS, iD 6∈ { iL, iR }
8
3 iD? 3 iD? 3 iD?
iL iR
bin ID
lo
a
d Configuration ` = `
(k)(t− 1)
vs
3 iD? 3 iD? 3 iD?
iL iR
bin ID
lo
a
d Configuration `
′ = `(k+1)(t− 1)
Figure 2: Illustration of the coupling used for Lemma 2. The red ball is m. Thus, `′
results from ` by a destructive move from iR = 7 to iL = 4. One of the balls (e.g., the blue
or red ball) is activated in both processes and tries to move from its source to a random
destination. If the red ball is activated, the source is iS = 7 on the left and 4 on the right.
If the blue ball is activated, the source is iS = 7 both on the left and right. The destination
iD is always the same on both sides. If the red ball is activated, the red intervals indicate
the three subcases of the first part of Case (2) from the proof. If the blue ball is activated,
the blue intervals indicate the three subcases of the second part of the same case (we drew
the intervals below the right figure for space reasons). Similar pictures can be drawn for
the remaining cases.
9
The processes behave identical and `(k+1)(t) still results from `(k)(t) by a destructive
move from iR to iL.
(2) iS = iR
The activated ball might be m. If that is the case, P (k) activates a ball in bin iR,
while P (k+1) activates a ball in bin iL. We distinguish three subcases depending on
the destination bin: If iD ≤ iL, both moves fail and nothing changes. If iL < iD ≤ iR,
only the move in P (k+1) succeeds and either the configurations become identical (if
iD = iR) or `(k+1)(t) results from `(k)(t) by a destructive move from iR to iD. If
iD > iR, both moves succeed and the configurations become identical.
If the activated ball is not m, both processes activate a ball in iR. We distinguish
three subcases depending on the destination bin: If iD ∈ { i ∈ [n] | `i > `iR − 1 },
both moves fail and nothing changes. If iD ∈ { i ∈ [n] | `i = `iR − 1 }, only the move
in P (k) succeeds (a neutral move) and `(k+1)(t) results from `(k)(t) by a destructive
move from iD to iL. If iD ∈ { i ∈ [n] | `i < `iR − 1 }, both moves succeed and `(k+1)(t)
still results from `(k)(t) by a destructive move from iR to iL.
(3) iS = iL
The activated ball cannot be m, so both processes activate a ball in bin iL. We distin-
guish three subcases depending on the destination bin: If iD ∈ { i ∈ [n] | `′i > `′iL − 1 },
both moves fail and nothing changes. If iD ∈ { i ∈ [n] | `′i = `′iL − 1 }, only the move
in P (k+1) succeeds (a neutral move) and `(k+1)(t) results from `(k)(t) by a destruc-
tive move from iR to iD. If iD ∈ { i ∈ [n] | `′i < `′iL − 1 }, both moves succeed and
`(k+1)(t) still results from `(k)(t) by a destructive move from iR to iL.
(4) iS 6∈ { iL, iR } , iD = iL
The activated ball cannot be m, so both processes activate a ball in the same bin iS.
We distinguish three subcases depending on the source bin: If iS ∈ { i ∈ [n] | `′i < `′iL },
both moves fail and nothing changes. If iS ∈ { i ∈ [n] | `′i = `′iL }, only the move in
P (k) succeeds (a neutral move) and `(k+1)(t) results from `(k)(t) by a destructive
move from iR to iS. If iS ∈ { i ∈ [n] | `′i > `′iL }, both moves succeed and `(k+1)(t)
still results from `(k)(t) by a destructive move from iR to iL.
(5) iS 6∈ { iL, iR } , iD = iR
The activated ball cannot be m, so both processes activate a ball in the same
bin iS. We distinguish three subcases depending on the source bin: If iS ∈
10
{ i ∈ [n] | `i < `iR }, both moves fail and nothing changes. If iS ∈ { i ∈ [n] | `i = `iR },
only the move in P (k+1) succeeds (a neutral move) and `(k+1)(t) results from `(k)(t)
by a destructive move from iS to iL. If iS ∈ { i ∈ [n] | `i > `iR }, both moves succeed
and `(k+1)(t) still results from `(k)(t) by a destructive move from iR to iL.
In all cases, `(k+1)(t) is close to `(k)(t). This completes the proof.
5 Auxiliary Results
Lemma 3 (Chernoff Bound, see [18, Theorem 4.4]). Consider the binomial distribution
Bin(n, p) with parameters n ∈ N and p ∈ [0, 1]. Then, for any ε ∈ [0, 3/2] and R ≥ 6np
we have
Pr(|Bin(n, p)− np| > ε · np) < 2e− ε
2·np
3 and (1)
Pr(Bin(n, p) ≥ R) ≤ 2−R. (2)
Lemma 4 (Concentration: Sum of Independent Exponentials). Let X be a sum of
independent exponential random variables, each having parameter ≥ λ. Then for any δ
Pr(X ≥ E[X] + δ) ≤ eλ2 Var[X]/4−λδ/2. (3)
Proof. Let X =
∑k
i=1Xi, with Xi
d
= Exp(λi) being independent exponential random
variables with parameter λi ≥ λ. Define s := λ/2 and µ := E[X] =
∑k
i=1
1
λi
. By Markov’s
inequality, we have
Pr(X ≥ µ+ δ) = Pr
(
esX ≥ esµ+sδ
)
≤ E
[
esX
]
esµ+sδ
= e−sµ−sδ ·
k∏
i=1
λi
λi − s
.
Note that s/λi ≤ 1/2. Thus, using the inequality 1/(1 − x) ≤ ex+x2 which holds for all
x ∈ [0, 1/2], we have λi
λi−s ≤ e
s/λi+s
2/λ2i . With this, we conclude
Pr(X ≥ µ+ δ) ≤ e−sµ−sδ ·
k∏
i=1
es/λi+s
2/λ2i ≤ e−sµ−sδ+sµ+s2 Var[X] = eλ2 Var[X]/4−λδ/2.
Lemma 5 (Concentration: Sum of Independent geometric random variables). Let Y1, . . . , Yk
be independent geometric random variables with parameter p ∈ [0, 1). Define L :=
− ln(1 − p), and let c1, . . . , ck,M, S, V be positive constants satisfying M := maxi ci,
S ≥∑i ci, and V ≥∑i c2i . Then for any t we have
Pr
(∑
i
ciYi ≥ t
)
≤ exp
(
V
4M2
+
S + SL− tL
2M
)
. (4)
11
Proof. For each i, define Zi := Yi − 1. Let X1, X2, . . . , Xk be independent exponential
random variables such that Xi has parameter L/ci. Then,
Pr(ciZi ≥ t) = Pr(Zi ≥ t/ci) = (1− p)dt/cie ≤ (1− p)t/ci = e−Lt/ci = Pr(Xi ≥ t)
holds for all i and t. Therefore,
Pr
(∑
i
ciYi ≥ t
)
≤ Pr
(∑
i
ciZi ≥ t− S
)
≤ Pr
(∑
i
Xi ≥ t− S
)
.
We will use Lemma 4 to bound the right hand side here. Note that all Xi have parameters
≥ L/M =: λ. Moreover, E[∑iXi] ≤ S/L and Var[∑iXi] ≤ V/L2. Therefore, Lemma 4
gives that
Pr
(∑
i
Xi ≥ t− S
)
≤ exp
(
λ2 Var[
∑
iXi]
4
− λ(t− S − E[
∑
iXi])
2
)
≤ exp
(
V
4M2
+
S + SL− tL
2M
)
,
as required.
Lemma 6. Let d1 ≤ d2 and suppose that, for any initial d2-balanced configuration,
the expected time to reach a d1-balanced configuration is t. Then, for any d2-balanced
configuration, the time to reach a d1-balanced configuration is at most 2t log2 n with high
probability.
Proof. The crucial observation is that since `(0) is d2-balanced, so is `(t) for any t ≥ 0. We
partition the time interval [0, 2t log2 n) into log2 n epochs [0, 2t), [2t, 4t), and so on. We say
the i-th epoch [2(i− 1)t, 2it) is successful if disc(`(2it)) ≤ d1. Since we deterministically
have disc(`(2(i− 1)t)) ≤ d2, regardless of the history up to time 2(i− 1)t, by Markov’s
inequality the probability that the i-th epoch is successful is at least 1/2. Hence, the
probability that none of the log2 n epochs are successful is bounded by (1/2)
log2 n = 1/n,
as required.
Lemma 7. Let d1 ≤ d2 and suppose that, for any initial d2-balanced configuration, the
time to reach a d1-balanced configuration is at most t with probability at least p. Let Y be
12
a geometric random variable with parameter p. Then, for any d2-balanced configuration,
the time to reach a d1-balanced configuration is stochastically dominated by tY , and so has
expected value at most t/p.
Proof. The crucial observation is that since `(0) is d2-balanced, so is `(t) for any t ≥ 0.
We partition the time interval [0,∞) into epochs [0, t), [t, 2t), and so on. We say the
i-th epoch [(i − 1)t, it) is successful if disc(`(it)) ≤ d1. Since we deterministically have
disc(`((i− 1)t)) ≤ d2, regardless of the history up to time (i− 1)t, the probability that
the i-th epoch is successful is at least p. So, the index of the first successful epoch is
dominated by a geometric random variable with parameter p, which has expected value
1/p.
6 Analysis of RLS
In this section we analyze the RLS protocol. For simplicity we would like to assume that
m ≥ 2n and that n divides m. The following two lemmas justify these assumptions.
Lemma 8. Suppose m ≤ n and let T be the time until perfect balance. Then E[T ] ≤ O(n)
and T ≤ O(n ln(n)) w.h.p.
Proof. By Lemma 2, we may assume that all balls start in the first bin, and that we may
wait for each of the m balls to move to m distinct empty bins (and ignore any other move).
Note that this is possible since m ≤ n. If there are 2 ≤ r ≤ m balls left in the first bin,
then there are at least r− 1 empty bins, hence the time it takes for one of the r balls to be
activated and choose an empty bin is an exponential with rate r × (r − 1)/n. Therefore,
the total expected time for the m balls to choose m distinct empty bins is at most
m∑
r=2
n/r(r − 1) <
∞∑
r=1
2n/r2 = O(n). (5)
This shows E[T ] = O(n). Lemma 6 then implies T ≤ O(n ln(n)) w.h.p.
Note that the above lemma implies Theorem 1 when m ≤ n. The next lemma implies
that, in order to prove Theorem 1, it is sufficient to consider the case when n divides m.
Lemma 9. Suppose m ≥ n and write m = kn+ r for some k ∈ N and r ∈ { 0, . . . , n− 1 }.
Let T be the time until perfect balance. Furthermore, suppose that for any configuration
with n bins and kn balls, RLS balances in expected time at most f(n, kn) and in time at
most g(n, kn) w.h.p. Then E[T ] ≤ O(ln(n)) + f(n, kn) and T ≤ O(ln(n)) + g(n, kn) w.h.p.
13
Proof. By Lemma 2, we may assume that all balls start in the first bin. We first wait
for r balls in the first bin to move to r distinct empty bins (and ignore any other move).
Once each of these balls finds a bin, we no longer allow it to move. After these r balls
are moved to distinct bins, we run the RLS protocol assuming it had only kn balls (these
assumptions can only slow down the protocol by Lemma 2). To complete the proof, we
need only show that the running time of the initial phase is O(lnn) in expectation and
with high probability. Denote this running time by T ′.
Note that T ′ =
∑r
i=1 Ti, where Ti is the time for the ith ball to activate and choose an
empty bin. Observe that Ti is an exponential random variable with parameter (kn+ r −
i+ 1)(n− i)/n > n− i. Thus,
E[T ′] <
r∑
i=1
1
n− i ≤ O(ln(n)), and (6)
Var[T ′] <
r∑
i=1
1
(n− i)2
≤ O(1). (7)
By concentration of sums of exponential random variables (see Lemma 4), we have
T ′ = O(lnn) w.h.p.
In light of the previous two lemmas, in the following we assume m ≥ n and that n
divides m. Our analysis of RLS proceeds via the following three phases:
Phase 1: In Section 6.1 we show that, from any initial configuration, w.h.p. it takes time
O(lnn) to become O(lnn)-balanced.
Phase 2: In Section 6.2 we show that, from any O(lnn)-balanced configuration, it takes
expected time O(n/∅) to become 1-balanced.
Phase 3: In Section 6.3 we show that, from any 1-balanced configuration, it takes expected
time O(n/∅) to become perfectly balanced.
Standard arguments imply that Phase 1 takes expected time O(lnn), and that Phases
2 and 3 take time O(lnn · n/∅) w.h.p. (see Lemma 6 and Lemma 7 in the appendix).
Since ∅ = m/n, these imply the total time to reach perfect balance is in expectation
O(ln(n) + n2/m) and w.h.p. O(ln(n) + ln(n) · n2/m).
14
6.1 Phase 1: Reaching an O(lnn)-balanced Configuration
We first consider how long it takes to go from an arbitrary initial configuration to a
configuration ` with disc(`) = O(lnn). We distinguish between two cases, depending on
whether ∅ is large or small.
Phase 1 for small ∅. The easier case, for ∅ ≤ 16 · lnn, is covered by the following
lemma.
Lemma 10. Assume ∅ ≤ 16 · lnn and consider an arbitrary initial configuration ` = `(0).
Let T := inf { t | disc(`(t)) ≤ 96 · lnn }. Then, w.h.p., T = O(lnn).
Proof. By the assumption we have ∅− 96 · lnn ≤ 0. Thus, `i(t) ≥ 0 ≥ ∅− 96 · lnn for all
bins i ∈ [n] and times t ≥ 0. Consequently, T is the first time t such that `i(t) ≤ ∅+96·lnn
for all i. By Lemma 2, we can assume that, initially, all balls are in the same bin (by
performing up to m− 1 destructive movements). So assume (w.l.o.g.) that all balls start
in bin 1. Let us first bound the time T ′ until m−∅ balls move from bin 1 to one of the
other n− 1 bins. Applying Lemma 2 once more, we ignore movements of balls to bin 1,
movements between any of the remaining n− 1 bins, and assume that all movements from
bin 1 to any of the remaining n− 1 bins are successful. If Ti denotes the time in which
the load of bin 1 decreases from i to i− 1, we have T ′ = ∑mi=∅+1 Ti. The different Ti are
independent exponential random variables with parameter i · (n− 1)/n. This yields
E[T ′] =
m∑
i=∅+1
(
i · n− 1
n
)−1
≤ 2 · lnn, and (8)
Var[T ′] =
m∑
i=∅+1
(
i · n− 1
n
)−2
= O(1/∅). (9)
By concentration of sums of independent exponential random variables (Lemma 4 in
Appendix 5), we have, w.h.p., T ′ = O(lnn).
To complete the proof, it suffices to show that w.h.p. T ≤ T ′. Whenever one of
these m − ∅ balls is activated, we may assume it chooses one other bin uniformly at
random and moves there (without checking the load). While this might violate our original
protocol (balls could move to a bin with a higher load), such a violation would be due
to a destructive movement. By Lemma 2, this merely slows the process down. Hence, at
time T ′, the number of balls in every other bin is Bin(m−∅, 1/(n− 1)), which has mean
(m − ∅)/(n − 1) = ∅. Using a Chernoff bound (Lemma 3) with the union bound, the
maximum of n such binomials is not more than 96 · lnn w.h.p.
15
Phase 1 for large ∅. We now turn to the more interesting case where ∅ > 16 · lnn
and consider two subphases. First, Lemma 11 shows that we reach a ∅/2-balanced
configuration in time O(lnn). The proof is basically identical to the proof of Lemma 10,
the only difference being that we use a different Chernoff bound at the end. Afterward,
Lemma 12 shows that it takes an additional O(lnn) time to become O(lnn)-balanced.
Lemma 11. Assume ∅ > 16 · lnn and consider an arbitrary initial configuration ` = `(0).
Let T := inf { t | disc(`(t)) ≤ ∅/2 }. Then, w.h.p., T = O(lnn).
Proof Sketch. The proof is basically identical to the proof of Lemma 10, the only difference
being that we use a Chernoff bound for large expected values (Inequality (1)) at the end.
As in the previous proof, by Lemma 2 (destructive movements) we can assume all balls
to be in bin 1 at time 0, and define T ′ as the time until m−∅ balls move to one of the
other n− 1 bins. The same calculations yield (w.h.p.) T ′ ≤ O(lnn). Once more, Lemma 2
allows us to majorize the ball distribution in each of these n− 1 remaining bins at time T ′
by the binomial distribution Bin(m−∅, 1/(n− 1)) with mean ∅. Applying a Chernoff
bound (this time the variant for large expected values, i.e., Equation (1)) and a union
bound yields that, w.h.p., the load of all bins is within [∅−2
√
∅ lnn,∅+2
√
∅ lnn]. Since
∅ > 16 lnn, this means w.h.p. disc(`(T ′)) ≤ 2
√
∅ lnn ≤ ∅/2.
Lemma 12. Assume m > n and consider an initial configuration ` = `(0) with disc(`) ≤
∅/2. Let T := inf { t | disc(`(t)) ≤ 8 lnn }. Then, w.h.p., T ≤ O(lnn).
For proving Lemma 12, we will apply the following lemma iteratively.
Lemma 13. Consider an initial configuration ` = `(0) with disc(`) ≤ x for some
x ≥ 4 · lnn. Let Tx := inf { t | disc(`(t)) ≤ 2
√
x · lnn }. Then, with probability ≥ 1− n−1
we have Tx ≤ ln
(∅+x
∅−x
)
. Moreover, Tx is dominated by Y · ln
(∅+x
∅−x
)
, where Y is a geometric
random variable with parameter 1− n−1.
Proof. First note that the second statement (domination by Y · ln
(∅+x
∅−x
)
) follows from the
first one via Lemma 7. We now prove the first statement. Assume for simplicity that
∅± x are integers. Let p := 2x/(x + ∅) and t := ln(∅ + x)− ln(∅− x). Note that the
probability of activation of each ball during the interval [0, t] is 1− exp(−t) = p. Using
Lemma 2 we make the following simplifying assumptions (see also Figure 3):
1. At time 0, we move some balls from the n/2 lightest bins (the light bins) to the n/2
heaviest bins (the heavy bins) in such a way that all light bins have exactly ∅− x
16
16
∅-x
∅
∅+x
bin ID
lo
a
d ignore
ignore
ignore
16
bin ID
lo
a
d
Figure 3: Using Lemma 2, we reorder the balls such that the distance to the average is
exactly x = 2 in the first and last eight bins. We also allow only moves from heavy to
light bins; all other moves are ignored.
balls and all heavy bins have exactly ∅ + x balls. All these moves are destructive,
thus we can assume (by Lemma 2) that we start in the resulting configuration. Bins
labeled as light/heavy in the beginning keep this label (regardless of how their loads
changes) during the time interval [0, t].
2. During the time interval [0, t], we ignore activations of balls in light bins (as we could
reverse them via Lemma 2).
3. During the time interval [0, t], we ignore movements between any two heavy bins (as
we could reverse them via Lemma 2).
4. During the time interval [0, t], if a ball in a heavy bin i is activated and tries to move
to a light bin i′, it does so unconditionally (i.e., even if `i < `i′ + 1; in that case it is
a destructive move which we may allow via Lemma 2).
First, consider a heavy bin. During the time interval [0, t], each of its balls is activated
with probability p, and moves to a light bin with probability 1/2. So this bin loses
Bin(∅ + x, p/2) balls. This binomial has expected value x ≥ 4 lnn. Thus, by Chernoff,
with probability ≥ 1− n−2 its value is in [x− 2
√
x lnn, x+ 2
√
x lnn]. This bin had ∅ + x
balls initially, so with probability ≥ 1 − n−2, it will have between ∅ − 2
√
x · lnn and
∅ + 2
√
x · lnn balls at time t.
Next, consider a light bin. There are (∅ + x)(n/2) balls it can potentially receive
during the time interval [0, t]. It receives each one with probability p/n, so the number of
balls it receives is Bin((∅ + x)(n/2), p/n). This binomial has expected value x ≥ 4 lnn.
Thus, by Chernoff, with probability ≥ 1− n−2 its value is in [x− 2
√
x lnn, x+ 2
√
x lnn].
This bin had ∅ − x balls initially, so with probability ≥ 1 − n−2, it will have between
17
∅− 2
√
x · lnn and ∅ + 2
√
x · lnn balls at time t. Applying the union bound over all bins
completes the lemma’s proof.
We now present the proof of Lemma 12.
Proof of Lemma 12. Note that, whenever x ≤ ∅/2, we have ln(∅ + x) − ln(∅ − x) =
ln
(
1 + 2x∅−x
)
≤ 2x∅−x ≤ 4x/∅. Define t0 := 0, x0 := ∅/2, and xk :=
√
4xk−1 · lnn for
k > 0. By induction, xk ≤ 4 ln(n) · x1/2
k
0 . Let r = log2 log2 ∅. Since x0 ≤ ∅, we have
xr ≤ 4 ln(n) ·∅1/ log2 ∅ = 8 · lnn. Let Y1, Y2, . . . be independent geometric random variables
with parameter 1− n−1. Applying Lemma 13 iteratively, the time to reach an xr-balanced
configuration is stochastically dominated by
Zr :=
r−1∑
i=0
Yi · 4xi/∅ ≤
r−1∑
i=0
ci · Yi, (10)
where ci := 16 · ln(n) ·x1/2
i
0 /∅. Straightforward calculations yield max ci = O(lnn),
∑
ci =
O(lnn), and
∑
c2i = O
(
ln2 n
)
(see below for the detailed calculations). By concentration
of sums of geometric random variables (see Lemma 5), we find that Zr = O(lnn) w.h.p.,
completing the proof.
Detailed calculations: it only remains to show that max ci = O(lnn),
∑
ci = O(lnn),
and
∑
c2i = O
(
ln2 n
)
. First, we have max ci = c0 = 8 · lnn.
Second, note that for any y ≥ 1 we have ∑r−1i=0 y1/2i ≤ 2y + 4r. Indeed, let k be the
smallest integer such that y1/2k < 4. Then,
r−1∑
i=0
y1/2
i
=
k∑
i=0
y1/2
i
+
r−1∑
i=k+1
y1/2
i ≤
k∑
i=0
y/2i + 4r < 2y + 4r.
Using this, we bound
r−1∑
i=0
ci =
16 · ln(n)
∅
·
r−1∑
i=0
x
1/2i
0 ≤
16 · ln(n)
∅
· (2x0 + 4r)
=
16 · ln(n)
∅
· (∅ + 4 log log∅) ≤ 16 · ln(n) · 2
18
Finally, using a similar analysis we get
r−1∑
i=0
c2i =
(
16 · ln(n)
∅
)2
·
r−1∑
i=0
x
2/2i
0
≤
(
16 · ln(n)
∅
)2
· (2x20 + 4r)
=
(
16 · ln(n)
∅
)2
·
(
∅2
2
+ 4 · log log∅
)
≤
(
16 · ln(n)
∅
)2
·∅2 = 256 · (lnn)2.
6.2 Phase 2: Reaching a 1-balanced Configuration
Lemma 14. Consider an initial configuration ` = `(0) with disc(`) = O(lnn). Let
T := inf { t | disc(`(t)) ≤ 1 }. Then E[T ] = O(n/∅).
Before we prove Lemma 14, let us introduce the notion of overloaded bins/balls: A bin
i is overloaded if `i > ∅. The quantity
∑
i max { 0, `i(t)−∅ } is called the number of
overloaded balls. If we enumerate the balls in each bin arbitrarily using natural numbers,
this is simply the number of balls whose number is greater than ∅. For instance, in
Figure 3 (left), the number of overloaded balls is 6. Note that this is also the number of
“holes” (i.e.,
∑
i max { 0,∅− `i(t) }).
We split this phase into two subphases. First, we show that it takes O
(
(lnn)2/∅
)
time
to reduce the number of overloaded balls to n (Lemma 15). Afterward (Lemma 16), we
prove that if both the discrepancy is logarithmic and the number of overloaded balls is
small, a 1-balanced configuration is reached in time O(n/∅). Together, these immediately
imply Lemma 14.
Lemma 15. Suppose disc(`(0)) = O(lnn) and let T := inf { t > 0
∣∣∑
i max { 0, `i(t)−∅ } ≤ n }.
Then E[T ] = O
(
(lnn)2/∅
)
.
Proof. Fix a time t > 0 and let A denote the number of overloaded balls. Let h and k be the
number of bins with load > ∅ and < ∅, respectively. Observe that ∅−O(lnn) ≤ `min ≤
`max ≤ ∅ + O(lnn) implies min {h, k } = Ω(A/ lnn) and, thus, h · k = Ω
(
A2/(lnn)2
)
.
We wait for some ball in some overloaded bin to choose an underloaded bin and move
there. Using Lemma 2, we ignore any other move. There are h overloaded bins and at
least h · ∅ balls in them. The probability that such a ball, when activated, chooses an
19
underloaded bin is k/n. Hence, the expected time for such a move to happen is at most
1
h·∅ · nk = O
(n·(lnn)2
A2·∅
)
. When such a move happens, the value of A decreases by 1. Hence,
total expected time to reduce A to n is bounded by
∞∑
A=n
O
(
n · (lnn)2
A2 ·∅
)
= O
(
n · (lnn)2
∅
∞∑
A=n
A−2
)
= O
(
n · (lnn)2
∅
∫ ∞
n−1
x−2 dx
)
= O
(
(lnn)2/∅
)
.
Lemma 16. Assume that disc(`(0)) = O(lnn) and that the number of overloaded balls is
at most n. Let T := inf { t | disc(`(t)) ≤ 1 }. Then E[T ] = O(n/∅).
Proof. Let A denote the number of overloaded balls. Suppose that h bins have load > ∅,
r bins have load = ∅, and k bins have load < ∅. Note that h+ r + k = n. We use the
quantity 3A− k − h as a potential function and prove the following claim.
Claim. if A > min {h, k }, then the expected time to decrease 3A− k− h by at least 1 is
≤ 3/∅.
To see that this implies the lemma, note that we always have A ≥ max {h, k }; moreover,
if A = min {h, k } then ∅− 1 ≤ `min ≤ `max ≤ ∅ + 1, which means the discrepancy is 1.
Since 3A − k − h is always between 0 and 3n and never increases over time, the claim
implies that it takes expected time O(n/∅) to achieve discrepancy 1.
We prove the claim by considering three cases.
Case 1: r ≥ n/3 and A > h. We wait for some ball in a bin with load > ∅ + 1 to choose
a bin with load ∅ and move there (ignoring any other move via Lemma 2). Since A > h,
there is at least one bin with load > ∅ + 1 and, hence, ≥ ∅ such balls. The probability
that such a ball, when activated, chooses a bin with load ∅ is r/n. Hence the expected
time for such a move to happen is at most 1∅ · nr ≤ 3/∅. When such a move happens, A
and k do not change but h increases by 1, so the potential decreases by 1, as required.
Case 2: r ≥ n/3 and A > k. We wait for some ball in a bin with load ∅ to choose a bin
with load < ∅− 1 and move there (ignoring any other move via Lemma 2). Since A > k,
there is at least one bin with load < ∅− 1 and, hence, the expected time for such a move
to happen is at most 1
r·∅ · n1 ≤ 3/∅. When such a move happens, A and h do not change
but k increases by 1, so the potential decreases by 1, as required.
Case 3: r < n/3. Note that h+ r + k = n and, since we are not yet balanced, h, k ≥ 1.
So h+ k > 2n/3 gives h · k ≥ h+ k − 1 > 2n/3− 1 > n/3. We wait for some ball in an
20
overloaded bin to choose an underloaded bin and move there (ignoring any other move via
Lemma 2). There are h overloaded bins and, hence, > h ·∅ such balls. The probability
that such a ball, when activated, choose an underloaded bin is k/n. Hence the expected
time for such a move to happen is at most 1
h·∅ · nk ≤ 3/∅. When such a move happens,
the value of A decreases by 1, while the values of k, h can decrease by at most 1. So, the
potential decreases by at least 1, as required.
6.3 Phase 3: Reaching Perfect Balance
Lemma 17. Consider an initial configuration ` = `(0) with disc(`) ≤ 1. Let T :=
inf { t | disc(`(t)) < 1 }. Then E[T ] = O(n/∅).
Proof. Note that ∅−1 ≤ `min ≤ `max ≤ ∅+1. By Lemma 2, we may ignore any movement
of balls from bins with load exactly m/n. Suppose there are A bins of load > ∅, and so
there are also A bins of load < ∅. If the configuration is not already balanced, then A ≥ 1
and there are > ∅ ·A balls that, when activated, find a bin with load < ∅ with probability
A/n. The expected time for the first such move to happen is at most 1
A·∅ · 1A/n = n∅·A2 .
Since A decreases one by one until balancing out (and at that point we would have A ≥ 1),
the expected total time to balance out is at most
∑n
A=1
n
∅·A2 ≤ O(n/∅), as required.
7 Conclusion
We analyzed the randomized local search protocol that was first introduced by [12] and
showed that the protocol achieves perfect balance in expected time O(ln(n) + n2/m).
Moreover, there is a matching lower bound so our analysis is tight. We now present a few
possible future directions.
The first direction is to extend the analysis to the setting where the bins may have
different speeds, and the load of a bin is defined as its number of balls divided by its speed.
One can consider a similar protocol to RLS: a ball chooses a random bin on activation, and
moves there if and only if doing so improves its load. A second direction is to study the
protocol when the balls may have different weights. In particular, can we obtain similar
balancing times in the weighted case as in the non-weighted case? The third direction is
to analyze the protocol in network topologies other than the complete graph.
21
References
[1] Heiner Ackermann, Simon Fischer, Martin Hoefer, and Marcel Schöngens. Distributed
algorithms for qos load balancing. Distributed Computing, 23(5):321–330, 2011.
[2] Luca Becchetti, Andrea E. F. Clementi, Emanuele Natale, Francesco Pasquale, and
Gustavo Posta. Self-stabilizing repeated balls-into-bins. In Proceedings of the 27th
Symposium on Parallelism in Algorithms and Architectures (SPAA), pages 332–339,
2015.
[3] P. Berenbrink, T. Friedetzky, P. Kling, F. Mallmann-Trenn, L. Nagel, and C. Wastell.
Self-stabilizing balls & bins in batches: The power of leaky bins [extended abstract].
In Proceedings of the 2016 ACM Symposium on Principles of Distributed Computing,
PODC ’16, pages 83–92, New York, NY, USA, 2016. ACM. available at http:
//arxiv.org/abs/1603.02188.
[4] Petra Berenbrink, Tom Friedetzky, Leslie Ann Goldberg, Paul W. Goldberg, Zengjian
Hu, and Russell Martin. Distributed selfish load balancing. SIAM Journal on
Computing, 37(4):1163–1181, 2007.
[5] Petra Berenbrink, Tom Friedetzky, Iman Hajirasouliha, and Zengjian Hu. Conver-
gence to equilibria in distributed, selfish reallocation processes with weighted tasks.
Algorithmica, 62(3):767–786, 2012.
[6] Petra Berenbrink, Tom Friedetzky, Frederik Mallmann-Trenn, S. Meshkinfamfard,
and Chris Wastell. Threshold load balancing with weighted tasks. In Proceedings of
the 29th IEEE Parallel & Distributed Processing Symposium (IPDPS), pages 550–558,
2015.
[7] J. E. Boillat, F. Brugé, and P. G. Kropf. A dynamic load-balancing algorithm for
molecular dynamics simulation on multi-processor systems. Journal of Computational
Physics, 96(1):1–14, 1991.
[8] Artur Czumaj. Recovery time of dynamic allocation processes. In Proceedings of the
10th Symposium on Parallel Algorithms and Architectures (SPAA), pages 202–211.
ACM, 1998.
[9] Artur Czumaj, Chris Riley, and Christian Scheideler. Perfectly balanced allocation.
In Proceedings of the 6th International Workshop on Approximation Algorithms
22
for Combinatorial Optimization Problems, pages 240–251, Berlin, Heidelberg, 2003.
Springer Berlin Heidelberg.
[10] Eyal Even-Dar and Yishay Mansour. Fast convergence of selfish rerouting. In
Proceedings of the 16th ACM-SIAM Symposium on Discrete Algorithms (SODA),
pages 772–781. ACM, 2005.
[11] Ayalvadi Ganesh, Sarah Lilienthal, D. Manjunath, Alexandre Proutiere, and Florian
Simatos. Load balancing via random local search in closed and open systems. Queueing
Systems, 71(3):321–345, 2012.
[12] Paul W. Goldberg. Bounds for the convergence rate of randomized local search in a
multiplayer load-balancing game. In Proceedings of the 23rd Annual ACM Symposium
on Principles of Distributed Computing (PODC), pages 131–140. ACM, 2004.
[13] Martin Hoefer and Thomas Sauerwald. Brief announcement: Threshold load balancing
in networks. In Proceedings of the 2013 ACM Symposium on Principles of Distributed
Computing (PODC), pages 54–56. ACM, 2013.
[14] Martin Hoefer and Thomas Sauerwald. Threshold load balancing in networks. CoRR,
abs/1306.1402, 2013.
[15] Norman Lloyd Johnson and Samuel Kotz. Urn Models and Their Application: An
Approach to Modern Discrete Probability Theory. Wiley Series in Probability and
Mathematical Statistics. Wiley, New York, 1977.
[16] Elias Koutsoupias and Christos Papadimitriou. Worst-case equilibria. In Proceedings
of the 16th Annual Conference on Theoretical Aspects of Computer Science (STACS),
pages 404–413. Springer-Verlag, 1999.
[17] Michael Mitzenmacher. The power of two choices in randomized load balancing. IEEE
Transactions on Parallel and Distributed Systems, 12(10):1094–1104, October 2001.
[18] Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized
Algorithms and Probabilistic Analysis. Cambridge University Press, 2005.
[19] M. Petrova, N. Olano, and P. Mahonen. Balls and bins distributed load balancing
algorithm for channel allocation. In Proceedings of the 7th International Conference
on Wireless On-demand Network Systems and Services (WONS), pages 25–30, 2010.
23
[20] Sonesh Surana, Brighten Godfrey, Karthik Lakshminarayanan, Richard Karp, and
Ion Stoica. Load balancing in dynamic structured peer-to-peer systems. Performance
Evaluation, 63(3):217–240, 2006.
[21] Berthold Vöcking. Selfish load balancing. In Noam Nisan, Tim Roughgarden,
Eva Tardos, and Vijay V. Vazirani, editors, Algorithmic Game Theory. Cambridge
University Press, 2007.
24

