New Fairness Metrics for Recommendation
that Embrace Differences
Sirui Yao
Department of Computer Science
Virginia Tech
Blacksburg, Virginia 24061
ysirui@vt.edu
Bert Huang
Department of Computer Science
Virginia Tech
Blacksburg, Virginia 24061
bhuang@vt.edu
ABSTRACT
We study fairness in collaborative-filtering recommender
systems, which are sensitive to discrimination that ex-
ists in historical data. Biased data can lead collaborative
filtering methods to make unfair predictions against mi-
nority groups of users. We identify the insufficiency of
existing fairness metrics and propose four new metrics
that address different forms of unfairness. These fair-
ness metrics can be optimized by adding fairness terms
to the learning objective. Experiments on synthetic and
real data show that our new metrics can better mea-
sure fairness than the baseline, and that the fairness
objectives effectively help reduce unfairness.
ACM Reference format:
Sirui Yao and Bert Huang. 2017. New Fairness Metrics for
Recommendation that Embrace Differences. In Proceedings
of Workshop on Fairness, Accountability, and Transparency
in Machine Learning, Halifax, Nova Scotia, 2017 (FAT/ML),
5 pages.
https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
This paper introduces new measures of unfairness in
algorithmic recommendation and demonstrates how
to optimize these metrics to reduce different forms of
unfairness. Since recommender systems make predic-
tions based on observed data, they can easily inherit
bias that may already exist. To address this issue, we
first describe a process that leads to unfairness in rec-
ommender systems and identify the insufficiency of
demographic parity for this setting. We then propose
four new unfairnessmetrics that address different forms
of unfairness. To improve model fairness, we provide
FAT/ML, 2017, Halifax, Nova Scotia
2017. ACM ISBN 123-4567-24-567/08/06. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn
five fairness objectives that can be optimized as regu-
larizers.
We focus on a frequently practiced approach for rec-
ommendation called collaborative filtering. With this
approach, predictions are made based on co-occurrence
statistics, and most methods assume that the missing
ratings are missing at random. Unfortunately, research
has shown that sampled ratings have markedly differ-
ent properties from the users’ true preferences [15, 16],
which is a potential source of unfairness.
We consider a running example of unfair recommen-
dation in education [4, 18, 20], in which the under-
representation of women in science, technology, en-
gineering, and mathematics (STEM) topics [1, 6, 19]
causes the learned model to underestimate women’s
preferences and be biased towards men. We find this
setting a serious motivation to advance understanding
of unfairness—and methods to reduce unfairness—in
recommendation.
Related Work. Various studies have considered al-
gorithmic fairness in problems such as classification
[14, 17, 21]. Removing sensitive features (e.g., gender,
race, or age) is often insufficient for fairness. Features
are often correlated, so other unprotected attributes can
be related to the sensitive features [11, 22]. Moreover,
in problems such as collaborative filtering, algorithms
do not directly consider measured features and instead
infer latent user attributes from their behavior.
Another frequently practiced strategy for encourag-
ing fairness is to enforce demographic parity, which is
to ensure that the overall proportion of members in the
protected group receiving positive (negative) classifica-
tions are identical to the proportion of the population as
a whole [22]. Based on this non-parity unfairness con-
cept, Kamishima et al. [10, 11] try to solve the unfairness
ar
X
iv
:1
70
6.
09
83
8v
1 
 [
cs
.C
Y
] 
 2
9 
Ju
n 
20
17
FAT/ML, 2017, Halifax, Nova Scotia Sirui Yao and Bert Huang
issue in recommender systems by adding a regulariza-
tion term that enforces demographic parity. However,
demographic parity is only appropriate when prefer-
ences are unrelated to the sensitive features. In recom-
mendation, user preferences are indeed influenced by
sensitive features such as gender, race and age [3, 5].
To address the issues of demographic parity, Hardt
et al. [7] measure unfairness with the true positive rate
and true negative rate. They propose that, in a binary
setting, given a decision Ŷ ∈ {0, 1}, a protected attribute
A ∈ {0, 1} and the true label Y ∈ {0, 1}, the constraints
are [7] Pr{Ŷ = 1|A = 0,Y = y} = Pr{Ŷ = 1|A = 1,Y =
y},y ∈ {0, 1}. This idea encourages equal opportunity
and no longer relies on the assumption of demographic
parity, that the target variable is independent of sen-
sitive features. Similarly, Calders et al. [2] propose to
impose constrains on the residuals of linear regression
models, which requires not only the mean prediction
but also themean residuals to be the same across groups.
These ideas form the basis of the unfairness metrics we
propose for recommendation.
2 FAIRNESS OBJECTIVES FOR
COLLABORATIVE FILTERING
This section introduces fairness objectives for collabora-
tive filtering. We begin by reviewing the matrix factor-
ization method. We then describe the various fairness
objectives we consider, providing formal definitions
and discussion of their motivations.
2.1 Matrix Factorization
We consider the task of collaborative filtering using
matrix factorization [13]. We have a set of users indexed
from 1 to m and a set of items indexed from 1 to n.
For the ith user, let дi be a variable indicating which
group the ith user belongs to. For the jth item, let hj
indicate the item group that it belongs to. Let ri j be the
preference score of the ith user for the jth item.
The matrix-factorization formulation assumes that
each rating can be represented as ri j ≈ p⊤i q j + ui +vj ,
where pi is a d-dimensional vector representing the
ith user, q j is a d-dimensional vector representing the
jth item, and ui and vj are scalar bias terms for the
user and item, respectively. The matrix-factorization
learning algorithm seeks to learn these parameters from
observed ratings X , typically by minimizing a regular-
ized, squared reconstruction error:
J (P ,Q,u,v) = λ
2
(
| |P | |2F + | |Q | |2F
)
+
1
|X |
∑
(i, j)∈X
(
yi j − ri j
)2
,
where u andv are the vectors of bias terms, and | | · | |F
represents the Frobenius norm.
2.2 Fairness Metrics
We consider a binary group feature distinguishing dis-
advantaged and advantaged groups. In the STEM ex-
ample, the disadvantaged group may be women and
non-binary gender identities, and the advantaged group
may be men.
The first metric is value unfairness, which measures
inconsistency in signed estimation error across the user
types, computed as
Uval =
1
n
n∑
j=1
(Eд [y]j − Eд [r ]j ) − (Eд̃ [y]j − Eд̃ [r ]j ) ,
where Eд [y]j is the average predicted score for the jth
item from disadvantaged users, Eд̃ [y]j is the average
predicted score for advantaged users, and Eд [r ]j and
Eд̃ [r ]j are the average ratings for the disadvantaged
and advantaged users, respectively.
Value unfairness occurs when one class of user is
consistently given higher or lower predictions than
their true preferences.
The second metric is absolute unfairness, which mea-
sures inconsistency in absolute estimation error across
user types, computed as
Uabs =
1
n
n∑
j=1
Eд [y]j − Eд [r ]j  − Eд̃ [y]j − Eд̃ [r ]j  .
Absolute unfairness is unsigned, so it captures the qual-
ity of prediction for each user type.
The third metric is underestimation unfairness, which
measures inconsistency in how much the predictions
underestimate the true ratings:
Uunder =
1
n
n∑
j=1
H (Eд [r ]j − Eд [y]j ) − H (Eд̃ [r ]j − Eд̃ [y]j ) ,
where H (x) is the hinge function, i.e., x if x ≥ 0 and
0 otherwise. Underestimation unfairness is important
in settings where missing recommendations are more
critical than extra recommendations.
Conversely, the fourth new metric is overestimation
unfairness, which measures inconsistency in how much
New Fairness Metrics for Recommendation that Embrace Differences FAT/ML, 2017, Halifax, Nova Scotia
the predictions overestimate the true ratings:
Uover =
1
n
n∑
j=1
H (Eд [y]j − Eд [r ]j ) − H (Eд̃ [y]j − Eд̃ [r ]j ) .
Finally, a non-parity unfairness measure based on the
regularization term introduced by Kamishima et al. [11]
can be computed as the absolute difference between the
overall average ratings of disadvantaged users and that
of advantaged usersUpar =
Eд [y] − Eд̃ [y].
To optimize the metric(s), we solve for a local mini-
mum of minP ,Q ,u,v J (P ,Q,u,v) + αU .
3 EXPERIMENTS
We run experiments on simulated course-recommendation
data and real movie rating data [8].
3.1 Synthetic data
In our synthetic experiments, we consider four user
groups д ∈ {W,WS,M,MS} and three item groups h ∈
{Fem, STEM,Masc}. The user groups represent women
who do not enjoy STEM topics (W), women who do
enjoy STEM topics (WS), men who do not enjoy STEM
topics (M), and men who do (MS). The item groups
represent courses that tend to appeal to women (Fem),
STEM courses, and courses that tend to appeal to men
(Masc). We generate simulated course-recommendation
data with two stochastic block models [9]. Our rating
block model determines the probability that a user in a
user group likes an item in an item group
L =

Fem STEM Masc
W 0.8 0.2 0.2
WS 0.8 0.8 0.2
MS 0.2 0.8 0.8
M 0.2 0.2 0.8

.
We use two observation block models that determine
the probability a user in a user group rates an item in an
item group: one with uniform observation probability
for all groupsOuni = [0.4]4×3 and one with unbalanced
observation probabilities inspired by real-world biases
O bias =

Fem STEM Masc
W 0.3 0.4 0.2
WS 0.6 0.2 0.1
MS 0.1 0.3 0.5
M 0.05 0.5 0.35

.
We define two different user group distributions: one
in which each of the four groups is exactly a quarter
of the population, and an imbalanced setting where 0.4
of the population is in W, 0.1 in WS, 0.4 in MS, and 0.1
Figure 1: Average unfairness scores for standard matrix fac-
torization on synthetic data generated from different under-
representation schemes.
in M. This heavy imbalance is inspired by some of the
severe gender imbalance in certain STEM areas today.
Unfairness from different types of underrepresentation.
Using standard matrix factorization, we measure the
various unfairness metrics under the different sampling
settings. We average over five random trials and plot
the average score in Fig. 1. In each trial, we gener-
ated ratings by 400 users and 300 items with the block
models. We label the settings as follows: uniform user
groups and uniform observation probabilities (U), uni-
form groups and biased observation probabilities (O), bi-
ased user group populations and uniform observations
(P), and biased populations and observations (P+O).
The statistics suggest that each underrepresentation
type contributes to various forms of unfairness. For all
metrics except parity, there is a strict order of unfair-
ness, where uniform data is themost fair and biasing the
populations and observations causes the most unfair-
ness. Because of the observation bias, there is actually
non-parity in the labeled ratings, so a high non-parity
score does not necessarily indicate an unfair situation.
FAT/ML, 2017, Halifax, Nova Scotia Sirui Yao and Bert Huang
Table 1: Average error and unfairness metrics for synthetic data using different fairness objectives. The best scores and those
that are statistically indistinguishable from the best are printed in bold. Each row represents a different unfairness penalty,
and each column is the measured metric on the expected value of unseen ratings.
Fairness Obj. Error Value Absolute Under Over Non-Parity
None 0.252 ± 3e-02 0.037 ± 4e-03 0.049 ± 9e-03 0.019 ± 1e-03 0.024 ± 3e-03 0.221 ± 5e-03
Value 0.238 ± 7e-03 0.000 ± 9e-06 0.049 ± 5e-03 0.012 ± 1e-03 0.012 ± 1e-03 0.218 ± 2e-03
Absolute 0.249 ± 2e-02 0.051 ± 3e-03 0.006 ± 3e-04 0.015 ± 9e-04 0.014 ± 7e-04 0.206 ± 8e-03
Underestimation 0.285 ± 1e-02 0.022 ± 1e-03 0.035 ± 5e-03 0.002 ± 5e-05 0.027 ± 3e-03 0.211 ± 3e-03
Overestimation 0.241 ± 3e-02 0.022 ± 1e-03 0.035 ± 3e-03 0.027 ± 2e-03 0.002 ± 2e-04 0.223 ± 2e-03
Non-Parity 0.282 ± 3e-02 0.077 ± 7e-03 0.045 ± 4e-03 0.028 ± 4e-03 0.032 ± 5e-03 0.044 ± 9e-04
Over+Under 0.242 ± 2e-02 0.002 ± 1e-04 0.014 ± 1e-03 0.004 ± 3e-04 0.004 ± 4e-04 0.212 ± 5e-03
Table 2: Average error and unfairness metrics for movie-rating data using different fairness objectives.
Fairness Obj. Error Value Absolute Under Over Non-Parity
None 0.843 ± 1e-02 0.063 ± 3e-03 0.023 ± 8e-04 0.026 ± 2e-03 0.017 ± 9e-04 0.037 ± 2e-03
Value 0.850 ± 4e-03 0.006 ± 3e-03 0.030 ± 5e-03 0.011 ± 3e-03 0.007 ± 6e-04 0.039 ± 2e-03
Absolute 0.847 ± 2e-03 0.051 ± 2e-03 0.004 ± 1e-03 0.014 ± 1e-03 0.013 ± 5e-04 0.035 ± 2e-03
Underestimation 0.845 ± 8e-03 0.026 ± 3e-03 0.020 ± 3e-03 0.008 ± 3e-03 0.015 ± 3e-04 0.038 ± 1e-03
Overestimation 0.840 ± 9e-03 0.036 ± 3e-03 0.027 ± 3e-03 0.029 ± 3e-03 0.002 ± 9e-05 0.037 ± 3e-03
Non-Parity 0.838 ± 3e-03 0.065 ± 3e-03 0.025 ± 4e-03 0.029 ± 3e-03 0.016 ± 6e-04 0.006 ± 2e-04
Over+Under 0.857 ± 9e-03 0.008 ± 3e-03 0.006 ± 3e-03 0.005 ± 3e-03 0.002 ± 3e-05 0.036 ± 1e-03
These tests verify that unfairness can occur with im-
balanced populations or observations even when the
measured ratings accurately represent user preferences.
Optimization of unfairness metrics. We optimize fair-
ness objectives under the most imbalanced setting: the
user populations are imbalanced, and the sampling rate
is skewed. We optimize for 500 iterations of Adam [12].
The results are listed in Table 1. The learning algo-
rithm successfully minimizes the unfairness penalties,
generalizing to unseen, held-out user-item pairs. And
reducing any unfairness metric does not lead to a sig-
nificant increase in reconstruction error. The combined
objective “Over+Under” leads to scores that are close
to the minimum of each metric except parity.
3.2 Real data
We use the Movielens Million Dataset [8], which con-
tains ratings in [1,5] by 6,040 users and 3,883 movies.
Wemanually selected five genres (action, crime,musical,
romance, and sci-fi) that each have different forms of
gender imbalance and only consider movies that list
these genres. Thenwe filtered the users to only consider
those who rated at least 50 of the selected movies.
After filtering by genre and rating frequency, we have
1,704 users and 1,343 movies in the dataset.
We run three trials in which we randomly split the
ratings into training and testing sets,
the average scores are listed in Table 2.
As in the synthetic setting, the results show that op-
timizing each unfairness metric leads to the best per-
formance on that metric without a significant change
in the reconstruction error.
4 CONCLUSION
In this paper, we discussed various types of unfairness
that can occur in collaborative filtering. We demon-
strate that these forms of unfairness can occur even
when the observed rating data accurately reflects the
users’ preferences. We propose four fairness metrics
and demonstrate that augmenting matrix factorization
objectives with these metrics as penalty functions en-
ables their minimization. Our experiments on synthetic
and real data show that minimization of these unfair-
ness metrics is possible with no significant increase in
reconstruction error.
However, no single objective was the best for all un-
fairness metrics, so it remains necessary for practition-
ers to consider precisely which form of unfairness is
most important in their application and optimize that
specific objective.
New Fairness Metrics for Recommendation that Embrace Differences FAT/ML, 2017, Halifax, Nova Scotia
FutureWork. While ourwork here focused on improv-
ing fairness among user groups, we did not address fair
treatment of different item groups. The model could
be biased towards certain items, e.g., performing better
for some items than others. Achieving fairness for both
user and items may be important when considering
that the items may also suffer from discrimination or
bias, e.g., when courses are taught by instructors with
different demographics.
Moreover, our fairness metrics assume that users rate
items according to their true preferences. This assump-
tion is likely violated in real data, since ratings can also
be influenced by environmental factors. E.g., in educa-
tion, a student’s rating for a course also depends on
whether the course has an inclusive and welcoming
learning environment. However, addressing this type
of bias may require additional information or external
interventions beyond the provided rating data.
Finally, we are investigating methods to reduce un-
fairness by directly modeling the two-stage sampling
process we used in Section 3.1. Explicitly modeling the
rating and observation probabilities as separate vari-
ables may enable a principled, probabilistic approach
to address these forms of data imbalance.
REFERENCES
[1] David N Beede, Tiffany A Julian, David Langdon, George
McKittrick, Beethika Khan, and Mark E Doms. Women in
STEM: A gender gap to innovation. U.S. Department of Com-
merce, Economics and Statistics Administration, 2011.
[2] Toon Calders, Asim Karim, Faisal Kamiran, Wasif Ali, and
Xiangliang Zhang. Controlling attribute effect in linear re-
gression. In Data Mining (ICDM), 2013 IEEE 13th International
Conference on, pages 71–80. IEEE, 2013.
[3] Olivia Chausson. Who watches what?: assessing the impact
of gender and personality on film preferences. Paper published
online on the MyPersonality project website http://mypersonality.
org/wiki/doku. php, 2010.
[4] Maria-Iuliana Dascalu, Constanta-Nicoleta Bodea, Mon-
ica Nastasia Mihailescu, Elena Alice Tanase, and Patricia Or-
doñez de Pablos. Educational recommender systems and their
application in lifelong learning. Behaviour & Information Tech-
nology, 35(4):290–297, 2016.
[5] Thomas N. Daymont and Paul J. Andrisani. Job preferences,
college major, and the gender gap in earnings. Journal of
Human Resources, pages 408–428, 1984.
[6] Amanda L. Griffith. Persistence of women and minorities in
STEM field majors: Is it the school that matters? Economics of
Education Review, 29(6):911–922, 2010.
[7] Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportu-
nity in supervised learning. In Advances in Neural Information
Processing Systems, pages 3315–3323, 2016.
[8] F Maxwell Harper and Joseph A Konstan. The Movielens
datasets: History and context. ACM Transactions on Interactive
Intelligent Systems (TiiS), 5(4):19, 2016.
[9] Paul W. Holland and Samuel Leinhardt. Local structure in
social networks. Sociological Methodology, 7:1–45, 1976.
[10] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun
Sakuma. Enhancement of the neutrality in recommendation.
In Decisions@ RecSys, pages 8–14, 2012.
[11] Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma.
Fairness-aware learning through regularization approach.
In 11th International Conference on Data Mining Workshops
(ICDMW), pages 643–650. IEEE, 2011.
[12] Diederik Kingma and Jimmy Ba. Adam: A method for stochas-
tic optimization. arXiv preprint arXiv:1412.6980, 2014.
[13] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factor-
ization techniques for recommender systems. Computer, 42(8),
2009.
[14] Kristian Lum and James Johndrow. A statistical framework
for fair predictive algorithms. arXiv preprint arXiv:1610.08077,
2016.
[15] Benjamin Marlin, Richard S Zemel, Sam Roweis, and Malcolm
Slaney. Collaborative filtering and the missing at random
assumption. arXiv preprint arXiv:1206.5267, 2012.
[16] Benjamin M. Marlin and Richard S. Zemel. Collaborative
prediction and ranking with non-random missing data. In
Proceedings of the Third ACM Conference on Recommender
Systems, pages 5–12. ACM, 2009.
[17] Dino Pedreshi, Salvatore Ruggieri, and Franco Turini.
Discrimination-aware data mining. In Proceedings of the 14th
ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 560–568. ACM, 2008.
[18] Cesar Vialardi Sacin, Javier Bravo Agapito, Leila Shafti, and
Alvaro Ortigosa. Recommendation in higher education using
data mining techniques. In Educational Data Mining, 2009.
[19] Emma Smith. Women into science and engineering? Gen-
dered participation in higher education STEM subjects. British
Educational Research Journal, 37(6):993–1014, 2011.
[20] Nguyen Thai-Nghe, Lucas Drumond, Artus Krohn-
Grimberghe, and Lars Schmidt-Thieme. Recommender
system for predicting student performance. Procedia
Computer Science, 1(2):2811–2819, 2010.
[21] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Ro-
driguez, and Krishna P. Gummadi. Fairness constraints: Mech-
anisms for fair classification. arXiv preprint arXiv:1507.05259,
2017.
[22] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia
Dwork. Learning fair representations. In Proceedings of the
30th International Conference on Machine Learning, pages 325–
333, 2013.

