ar
X
iv
:1
70
6.
09
79
5v
1 
 [
st
at
.M
L
] 
 2
9 
Ju
n 
20
17
Feature uncertainty bounding schemes
for large robust nonlinear SVM classifiers
Nicolas Couellan∗ Sophie Jan†
Abstract
We consider the binary classification problem when data are large and
subject to unknown but bounded uncertainties. We address the problem
by formulating the nonlinear support vector machine training problem
with robust optimization. To do so, we analyze and propose two bounding
schemes for uncertainties associated to random approximate features in
low dimensional spaces. The proposed techniques are based on Random
Fourier Features and the Nyström methods. The resulting formulations
can be solved with efficient stochastic approximation techniques such as
stochastic (sub)-gradient, stochastic proximal gradient techniques or their
variants.
1 Introduction
When designing classification models, the common idea is to consider that train-
ing samples are certain and not subject to noise. In reality, this assumption may
be far from being verified. There are several sources of possible noise. For ex-
ample, data may be noisy by nature. In biology, for instance, measurements
on living cells under the miscroscope are unprecise due to the movement of the
studied organism. Furthermore, any phenomenon that occurs at a smaller scale
than the given microscope resolution is measured with errors. Uncertainty is
sometimes also introduced as a way to handle missing data. If some of the sam-
ple attributes are not known but can be estimated, they can be replaced by their
estimation, assuming some level of uncertainty. In the work that is presented
here, we are interested in constructing optimization-based classification models
that can account for uncertainty in the data and provide some robustness to
noise. To do so, some knowledge on the noise is required. Most of the time,
assuming that we know the probability distribution of the noisy perturbations
is unrealistic. However, depending on the context, one may have knowledge
on the mean and variance or simply the magnitude of the perturbations. The
∗Institut de Mathématiques de Toulouse, Université Paul Sabatier, 118 route de Narbonne
31062 Toulouse Cedex 9, France - nicolas.couellan@math.univ-toulouse.fr
†Institut de Mathématiques de Toulouse, Université Paul Sabatier, 118 route de Narbonne
31062 Toulouse Cedex 9, France
1
quantity and the nature of information available will be critical for the design
of models that are immune to noise.
In binary classification problems, the idea is to compute a decision rule based
on empirical data in order to predict the class on new observations. If em-
pirical data is uncertain and no robustness is incorporated in the model, the
decision rule will also lead to uncertain decision. In the context of Support
Vector Machine (SVM) classification [22], there are several ways to construct
robust counterparts of the classification problem, see for example [2]. In these
techniques, one considers that data points can be anywhere within an uncer-
tainty set centered at a nominal point. The uncertainty set may have various
shapes and dimension depending on the application and the level of noise. In
the chance constrained SVM method, instead of ensuring that as many possible
points are on the right side of the separating hyperplane, the idea is to ensure
that a large probability of them will be correctly classified [3, 2]. Alternatively,
one may want to ensure that no matter where the data points are in the uncer-
tainty set, the separating hyperplane will ensure that it will always be on the
correct side. This is known as the worst case robust SVM approach [19, 24].
In both methods, one can show that the resulting classification problem can be
formulated as a Second Order Cone Program (SOCP).
Classification problems are usually very large optimization problems due to the
great number of samples available. Furthermore, so far, to the best of our
knowledge, robust SVM formulations are only dealing with linear classification.
If uncertainties are large, additional non-separability is introduced in the data
that may limit the predictive performance of robust SVM models.
To propose solutions to these challenges, we present several new models and
SVM classification approaches. We first describe the concept of robust Support
Vector Machine (SVM) classification and its computation through stochastic
optimization techniques when the number of samples is large. We further explain
the concept of safe SVM we have introduced to overcome the conservatism issues
in classical robust optimization techniques. We then develop new extensions to
robust nonlinear classification models where we propose computational schemes
to bound uncertainties in the feature space.
2 Notations
We define the following notations:
• Σ1/2 the matrix in Rn×n associated to the positive definite matrix Σ such
that Σ1/2(Σ1/2)⊤ = Σ ;
• Σ−1/2 the matrix (Σ1/2)−1 ;
• Σ⊤/2 the matrix (Σ1/2)⊤ ;
• N
(
µ, ν2
)
the normal distribution with mean µ and variance ν2.
2
3 Robust SVM classification models: prelimi-
naries
We first recall the standard SVM methodology [22, 14, 8] to find the maximum
margin separating hyperplane between two classes of data points.
Let xi ∈ Rn be a collection of input training vectors for i = 1, ..., L and
yi ∈ {−1, 1} be their corresponding labels. The SVM classification problem
to separate the training vectors into the −1 and 1 classes can be formulated as
follows:
min
w,b,ξ
λ
2 ‖w‖2 +
L
∑
i=1
ξi (1)
subject to yi(〈w, xi〉+ b) + ξi ≥ 1 i = 1, . . . , L
ξi ≥ 0 i = 1, . . . , L
where ξi are slack variables added to the constraints and in the objective as a
penalty term, λ is a problem specific constant controlling the trade-off between
margin (generalization) and classification, and 〈w, x〉 + b = 0 defines the sepa-
rating hyperplane with w the normal vector to the separating hyperplane and
b its relative position to the origin.
Consider now instead a set of noisy training vectors {x̃i ∈ Rn, i = 1, . . . , L}
where x̃i = xi + ∆xi for all i in {1, ..., L} and ∆xi is a random perturbation,
Problem (1) becomes:
min
w,b,ξ
λ
2 ‖w‖2 +
L
∑
i=1
ξi (2)
subject to yi(〈w, xi +∆xi〉+ b) + ξi ≥ 1, ∀i = 1, . . . , L
ξi ≥ 0, ∀i = 1, . . . , L.
Observe that the first constraint involves the random variable ∆xi and Prob-
lem (2) cannot be solved as such. Extra knowledge on the perturbations is
needed to transform (2) into a deterministic and numerically solvable problem.
If the uncertainty set is bounded and the bound is known, one can consider that
‖Σ−1/2i ∆xi‖p ≤ γi, with γi > 0, ∀i ∈ {1, . . . , L} and p ≥ 1.
Various choices of Σi and p will lead to various types of uncertainties such as
for example box-shaped uncertainty (‖ x̃i − xi ‖∞≤ γi), spherical uncertainty
(‖ x̃i − xi ‖2≤ γi) or ellipsoidal uncertainty ((x̃i − xi)⊤Σ−1i (x̃i − xi) ≤ γ2i for
some positive definite matrix Σi).
To design a robust model, one has to satisfy the linear inequality constraint in
Problem (2) for every realizations of ∆xi. This can be done by ensuring the
constraint in the worst case scenario for ∆xi, leading to the following robust
3
counterpart optimization problem:
min
w,b,ξ
λ
2 ‖w‖2 +
L
∑
i=1
ξi (3)
subject to min
‖Σ
−1/2
i ∆xi‖p≤γi
yi(〈w, xi +∆xi〉+ b) + ξi ≥ 1, ∀i = 1, . . . , L,
ξi ≥ 0, ∀i = 1, . . . , L.
Denoting ‖.‖q the dual norm of ‖.‖p, we have
min
‖Σ
−1/2
i ∆xi‖p≤γi
yi〈w,∆xi〉 = −γi
∥
∥
∥Σ
⊤/2
i w
∥
∥
∥
q
.
Therefore, Problem (3) can be rewritten as
min
w,b,ξ
λ
2 ‖w‖2 +
L
∑
i=1
ξi (4)
subject to yi(〈w, xi〉+ b) + ξi − γi
∥
∥
∥Σ
⊤/2
i w
∥
∥
∥
q
≥ 1, ∀i = 1, . . . , L,
ξi ≥ 0, ∀i = 1, . . . , L.
This problem is a Second Order Cone Program (SOCP). There has been previous
investigation on this formulation where the problem was solved using a primal-
dual IPM [1] and solver packages such as [17, 12]. The complexity of primal-
dual IPM is O(s3) operations per iteration and the number of iterations is
O(
√
s × log(1/ǫ)) where ǫ is the required precision and s the number of cones
in the problem. In Problem (4), the number of cones is L + 1 (L is the size
of the dataset). Therefore these methods do not scale well as the size of the
dataset increases. However, for small and medium size problems, they have
given satisfying results [20, 19]. For large problems, we propose alternative
techniques in the next section.
Chance constrained approach
Observe that if one has additional knowledge on the bound of the perturbations
(ex: mean and variance of the noise distribution), one may want to consider the
chance constrained approach rather than the worst case methodology above.
Doing so, we can replace in Problem (2) the constraint yi(〈w, xi+∆xi〉+b)+ξi ≥
1 by
P∆xi [yi(〈w, xi +∆xi〉+ b) + ξi ≥ 1] ≥ 1− ǫi, ∀i ∈ {1, ..., L},
where ǫi is a given positive confidence level. In other words, instead of ensuring
the constraint for all realizations of ∆xi, we would like to ensure its probability
of being satisfied to be greater than some threshold value 1− ǫi. Indeed, it was
shown in [16] that if ∆xi follows a normal distribution N (0,Σi), the chance
4
constraint problem can be formulated exactly as Problem (4) if p is taken as 2
and ǫi such that γi =
√
ǫi/(1− ǫi). The proof makes use of the multivariate
Chebyshev inequality to replace the probability of ensuring the constraint by
the Chebyshev bound. The two approaches are therefore related when p = 2.
In the work that is described next, we have chosen to exploit the worst case
model as less assumptions are taken on the uncertainty set since p can take
any positive value greater than 1 and we do not require explicit knowledge of
variance of the perturbations.
Large scale robust framework
We now consider the case where the dataset is very large and SOCP techniques
cannot be used in practice. We reformulate the problem (4) as an equivalent
unconstrained optimization problem:
min
w,b
f(w, b) =
λ
2
‖w‖2 +
L
∑
i=1
ℓΣi(yi, 〈w, xi〉+ b), (5)
where ℓΣi is the robust loss function defined by
ℓΣi(yi, 〈w, xi〉+ b) = max
{
0, 1− yi(〈w, xi〉+ b) + γi‖Σ⊤/2i w‖q
}
. (6)
There are many techniques available to solve problem (5). When L is very
large, stochastic approximation techniques may be considered as they have very
low per-iteration complexity. For example, stochastic proximal methods have
proven to be very effective for this type of large decomposable problems [23].
The idea is to split the objective f into two components G(w, b) = g(w, b)+h(w)
where
g(w, b) =
L
∑
l=1
gl(w, b), gl(w, b) = ℓΣl(yl, 〈w, xl〉+ b), h(w) =
λ
2
‖w‖2,
to perform a stochastic forward step on the decomposable component g by taking
a gradient approximation using only one random term gl of the decomposable
sum, and finally to carry out a backward proximal step using h.
Controlling the robustness of the model
Chance constraints and worst case robust formulations are based on a priori in-
formation on the noise bound or its variance. If these estimations are erroneous
and over-estimated for example, the models may be too conservative. Addition-
ally, worst case situations may be too pessimistics and may introduce additional
non separability in the problem resulting in lower generalization performance
of the model. This is the reason why we have proposed an alternative adaptive
robust model that will have the advantage of being less conservative without
taking any additional assumption on the probability distribution [7]. The idea
5
is to consider an adjustable subset of the support of the probability distribution
of the uncertainties. Assuming an ellipsoidal uncertainty set, a reduced uncer-
tainty set (reduced ellipsoid) is computed so as to minimize a generalization
error. The model is referred to as safe SVM rather than robust SVM. Mathe-
matically, this is achieved by introducing a variable σ defining a Σσ matrix of a
reduced ellipsoid where σ is the vector of lengths of the ellipsoid along its axes.
The resulting model can be cast as a bi-level program as follows:
min
σ∈Rn
N
∑
j=1
ℓ(yvj , 〈w∗, xvj 〉+ b∗)
s.t. σmin ≤ σt ≤ σmax ∀t = 1, . . . , n
(w∗, b∗) = argmin
w,b
λ
2
‖w‖2 +
L
∑
i=1
ℓΣσt (yi, 〈w, xi〉+ b)
(7)
where (xvj , y
v
j ), for j = 1, . . . , N , are the sample vectors and their labels from
the validation fold. The upper and lower bounds σmax and σmin are parameters
that control the minimum and maximum amount of uncertainty we would like
to take into account in the model. They can be taken as rough bounds of the
perturbations. The function ℓ is the standard SVM hinge loss function that will
estimate the generalization error for one test sample and ℓΣσ is the robust loss
function as defined in (6). The matrix Σσ is defined as (diag(σ))
2
.
In [7], we have solved Problem (7) with a stochastic bi-level gradient algo-
rithm [6]. Using a robust error measure as an indicator of performance of the
robustness of the model, we have shown that the proposed model achieves a
better robust error on several public datasets when compared to the SOCP for-
mulation or formulation (5). Using gaussian noise, we have also shown that the
volume of the reduced ellipsoid computed by the technique was indeed increas-
ing with variance of the noise, confirming that the model was auto adjusting its
robustness to the noise in the data.
4 Nonlinear robust SVM classification models
There are situations where the model (1) will not be able to compute a suf-
ficiently good linear separation between the classes if the data are highly non
linearly separable. This phenomenon is actually even more present when noise
is taken into account since robust models introduce additional non separability
in the data. For this reason, it is interesting to investigate nonlinear robust
formulations. Nonlinear SVM models are based on the use of kernel functions:
min
α,b,ξ
λ
2α
⊤Kα+
L
∑
i=1
ξi (8)
subject to yi
(
L
∑
i=1
αjk(xi, xj) + b
)
+ ξi ≥ 1, ∀i = 1, . . . , L,
ξi ≥ 0, ∀i = 1, . . . , L,
6
where K is a kernel matrix whose elements are defined as
kij = 〈φ(xi), φ(xj)〉 = k(xi, xj)
and k is a kernel function such as a polynomial, a Gaussian Radial Basis Func-
tion (RBF), or other specific choices of kernel functions [15].
One difficulty we are facing when designing robust counterparts formulations in
the RKHS space is the understanding of how the uncertainty set is transformed
through the implicit mapping chosen via the use of a specific kernel. The kernel
function and its associated matrix K are known but its underlying mapping φ is
unknown. It is therefore difficult to seek bounds of the image of the uncertainty
set in the original space through φ. To address this issue, we investigate two
methods : the Random Fourier Features (RFF) and the Nyström method. In
both methods, the idea is to approximate the feature mapping φ by an explicit
function φ̃ from Rn to RD (n ≤ D ≤ L) that will allow bounding of the image
of uncertainties in the feature space. Specifically, if we can show that for all i in
{1, . . . , L} we can bound the uncertainty ∆φi = φ̃(xi +∆xi)− φ̃(xi) as follows:
‖Ri∆φi‖p̄ ≤ Γi, (9)
for some Lp̄ norm in R
D and where Γi is a constant, Ri is some matrix, both
depending on the choice of feature map approximation, we are able to formulate
the nonlinear robust SVM problem as follows
min
ζ∈RD,b∈R
λ
2
‖ζ‖2 +
L
∑
i=1
max
{
0, 1− yi(〈ζ, φ̃(xi)〉+ b) + Γi‖R⊤i ζ‖q̄
}
, (10)
where q̄ is such that 1p̄ +
1
q̄ = 1. The construction of this problem is similar
to the construction of problem (4) in the linear case considering separation of
approximate features φ̃(xi) instead of data points xi and the use of a robust
loss function in the feature space as in (6).
As far as we know, the model (10) is the first model that is specifically designed
to perform nonlinear classification on large and uncertain datasets. The de-
composable structure of the objective allows, as for the linear case, the use
of stochastic approximation methods such as the stochastic sub-gradient or
stochastic proximal methods.
In the next 2 sections, we concentrate on deriving upper bounds of the form
of (9) by the RFF and the Nyström methods.
4.1 Bounding feature uncertainties via Random Fourier
Features
We propose first to make use of the so-called Random Fourier Features (RFF)
to approximate the inner product 〈φ(xi), φ(xj)〉 by φ̃(xi)⊤φ̃(xj) in a low dimen-
sional Euclidian space RD, meaning that:
k(x, y) = 〈φ(x), φ(y)〉 ≈ φ̃(x)⊤φ̃(y), ∀x, y ∈ Rn.
7
The explicit knowledge of the randomized feature map φ̃ helps in bounding the
uncertainties in the randomized feature space. Additionally, as we expect D to
be fairly low in practice, the technique should be able to handle large datasets
as opposed to exact kernel methods that require the storage of a large and dense
kernel matrix.
Mathematically, the technique relies on the Bochner theorem [13] which states
that any shift invariant kernel k(x, y) = k(x−y) is the inverse Fourier transform
of a proper probability distribution P . If we define a complex-valued mapping
φ̂ : Rn → C by φ̂ω(x) = ejω
⊤x and if we draw ω from P , the following holds:
k(x− y) =
∫
Rn
P (ω)ejω
⊤(x−y)dω = E
[
φ̂ω(x)φ̂ω(y)
∗
]
where ∗ denotes the complex conjugate. Furthermore, if instead we define
φ̃ω,ν(x) =
√
2
[
cos
(
ω⊤x+ ν
)]
,
where ω is drawn from P and ν from the uniform distribution in [0, 2π], we
obtain an approximate real-valued random Fourier feature mapping that satisfies
E
[
φ̃ω,ν(x)φ̃ω,ν(y)
]
= k(x, y).
The method for constructing RFF to approximate a Gaussian kernel could there-
fore be summarized as follows:
1. Compute the Fourier transform P of the kernel k by taking:
P (ω) =
1
2π
∫
e−jω
⊤δk(δ)dδ;
2. Draw D i.i.d. samples ω1, . . . , ωD in R
n from P ;
3. Draw D i.i.d. samples ν1, . . . , νD in R from the uniform distribution
on [0, 2π];
4. The RFF φ̃ω,ν(x) is given by
φ̃ω,ν(x) =
√
2
D
[
cos(ω⊤1 x+ ν1), . . . , cos(ω
⊤
Dx+ νD)
]⊤
.
In [18], it has been shown that, when k is Gaussian, the following choice of
features leads to lower variance and should be preferred in practice:
φ̃ω(x) =
√
2
D
[
cos(ω⊤1 x), sin(ω
⊤
1 x), . . . , cos(ω
⊤
D/2x), sin(ω
⊤
D/2x)
]⊤
.
In Figure 1, a toy example is given. On the left side, one can see 4 nominal data
points surrounded by their noisy observations. The 2 classes of points cannot
be separated linearly. We construct random Fourier features with D = 2 on the
8
Figure 1: Random Fourier projections of uncertainties in a low dimensional
feature space
−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5
−0.06
−0.04
−0.02
0.00
0.02
0.04
0.06
In the initial space
−1.0 −0.5 0.0 0.5 1.0
−1.0
−0.5
0.0
0.5
1.0
In the RFF space
right side. One can observe that the noisy observations are now projected on
the circle and linear separation is possible.
We are now interested in the analysis of the image of uncertainties when mapped
in RD through φ̃ω. With the explicit knowledge of φ̃ω and if we define ∆φω,i =
φ̃ω(xi +∆xi)− φ̃ω(xi) as the induced uncertainty around each random feature
in the randomized feature space RD, we establish the following result:
Proposition 4.1 For all i in {1, . . . , L}, let Ri be the block-diagonal matrix
where the D/2 blocks of Ri are rotation matrices of angle −ω⊤j xi for j in
{1, . . . , D/2}. We have
‖Ri∆φω,i‖p̄ ≤ Γi,
where Γi are constants depending on γi, Σ
1/2
i and the D/2 vectors (ωi).
The proof of this result is as follows:
Proof: Let cij = cos
(
ω⊤j xi
)
and sij = sin
(
ω⊤j xi
)
. Using trigonometric rela-
tions, the j-th block of ∆φω,i can be written
(∆φω,i)j =
√
2
D
(
cos
(
ω⊤j (xi +∆xi)
)
− cos
(
ω⊤j xi
)
,
sin
(
ω⊤j (xi +∆xi)
)
− sin
(
ω⊤j xi
)
)⊤
=
√
2
D
(
cij cos
(
ω⊤j ∆xi
)
− sij sin
(
ω⊤j ∆xi
)
− cij ,
sij cos
(
ω⊤j ∆xi
)
+ cij sin
(
ω⊤j ∆xi
)
− sij ,
)⊤
=
√
2
D
Pij


cos
(
ω⊤j ∆xi
)
− 1
sin
(
ω⊤j ∆xi
)


9
where
Pij =
(
cij −sij
sij cij
)
is the rotation matrix of angle ω⊤j xi. Consequently, we obtain that
P⊤ij (∆φω,i)j =
√
2
D


cos
(
ω⊤j ∆xi
)
− 1
sin
(
ω⊤j ∆xi
)

 .
Using the following bounding schemes
| cos(θ)− 1| ≤ θ
2
2
, | sin(θ)| ≤ |θ|,
we have
∣
∣
∣cos
(
ω⊤j ∆xi
)
− 1
∣
∣
∣ ≤
(
ω⊤j ∆xi
)2
2
∣
∣
∣sin
(
ω⊤j ∆xi
)∣
∣
∣ ≤
∣
∣ω⊤j ∆xi
∣
∣ .
(11)
Using Hölder inequality, we can write
∣
∣ω⊤j ∆xi
∣
∣ =
∣
∣
∣ω⊤j Σ
1/2
i Σ
−1/2
i ∆xi
∣
∣
∣
≤
∥
∥
∥
ω⊤j Σ
1/2
i
∥
∥
∥
q
∥
∥
∥
Σ
−1/2
i ∆xi
∥
∥
∥
p
≤
∥
∥
∥Σ
⊤/2
i ωj
∥
∥
∥
q
γi,
(12)
and
∣
∣
∣cos
(
ω⊤j ∆xi
)
− 1
∣
∣
∣ ≤ min





2,
(
γi
∥
∥
∥Σ
⊤/2
i ωj
∥
∥
∥
q
)2
2





∣
∣
∣
sin
(
ω⊤j ∆xi
)∣
∣
∣
≤ min
(
1, γi
∥
∥
∥
Σ
⊤/2
i ωj
∥
∥
∥
q
)
.
Introducing the notations
Ri = diag(P
⊤
i,1, ..., P
⊤
i,D/2),
αi,j := min





2,
(
γi
∥
∥
∥
Σ
⊤/2
i ωj
∥
∥
∥
q
)2
2





and
βi,j := min
(
1, γi
∥
∥
∥Σ
⊤/2
i ωj
∥
∥
∥
q
)
,
10
we finally obtain the expected bound
‖Ri∆Φω,i‖p̄ ≤ Γi
where
Γi :=





















√
2
D
D/2
∑
j=1
(αi,j + βi,j) if p̄ = 1;
√
√
√
√
4
D
D/2
∑
j=1
αi,j if p̄ = 2;
√
2
D
max
(
max
j=1,...,D/2
αi,j , max
j=1,...,D/2
βi,j
)
if p̄ = ∞.

4.2 Bounding feature uncertainties via the Nyströmmethod
We now investigate another method. Note that in the following, we will use the
notation ‖.‖ for the euclidean norm ‖.‖2. When working with large datasets,
the computation and storage of the entire kernel matrix K whose size is L2
becomes difficult. For this reason, the Nyström method has been proposed to
compute a low rank approximation of the kernel matrix [9, 21, 10, 11, 5, 4]. We
can summarize the method as follows:
1. Choose an integer m between the number of attributes n and the size L
of the dataset.
2. Choose m vectors (x̂i)i=1,...,m among the L training vectors.
3. Build the associated kernel matrix K̂ ∈ Rm×m defined by
K̂ij := k(x̂i, x̂j).
4. Perform an eigenvalue decomposition of this matrix K̂ to obtain
• its rank r,
• a m × r matrix U (r)σ := (uσ,1, ..., uσ,r) such that
(
U
(r)
σ
)⊤
U
(r)
σ is the
identity matrix,
• and a diagonal matrix Λ(r)σ := diag(λσ,1, ..., λσ,r)
such that K̂U
(r)
σ = U
(r)
σ Λ
(r)
σ .
5. The Nyström φ̃σ(x) can be chosen as follows :
φ̃σ(x) =
(
Λ(r)σ
)−1/2 (
U (r)σ
)⊤
k̂σ(x),
with
k̂σ(x) = (kσ(x, x̂1), kσ(x, x̂2), ..., kσ(x, x̂m))
⊤
.
11
As before, we are interested by the analysis of the uncertainties when they
are mapped to the approximate feature space induced by the Nyström method
through φ̃σ. We define ∆φσ,i = φ̃σ(xi +∆xi)− φ̃σ(xi) as the uncertainty in the
feature space Rr and show the following result:
Proposition 4.2 For all i in {1, . . . , L}, let Ri =
(
Λ
(r)
σ
)1/2
, we have
‖Ri∆φσ,i‖2 ≤ Γi,
where Γi are constants depending on γi, Σ
1/2
i , σ and the m vectors (x̂i)i=1,...,m.
Proof: Using the expression of the feature mapping constructed via the Nyström
method, we have
(
Λ(r)σ
)1/2 (
φ̃σ(xi +∆xi)− φ̃σ(xi)
)
=
(
U (r)σ
)⊤ (
k̂σ(xi +∆xi)− k̂σ(xi)
)
=





u⊤σ,1
(
k̂σ(xi +∆xi)− k̂σ(xi)
)
...
u⊤σ,r
(
k̂σ(xi +∆xi)− k̂σ(xi)
)





.
Consequently, we can write
∥
∥
∥
∥
(
Λ(r)σ
)1/2 (
φ̃σ(xi +∆xi)− φ̃σ(xi)
)
∥
∥
∥
∥
2
2
=
r
∑
j=1
(
u⊤σ,j
(
k̂σ(xi +∆xi)− k̂σ(xi)
))2
≤
r
∑
j=1
∥
∥
∥
k̂σ(xi +∆xi)− k̂σ(xi)
∥
∥
∥
2
2
= r
∥
∥
∥k̂σ(xi +∆xi)− k̂σ(xi)
∥
∥
∥
2
2
.
We are now interested in computing an upper bound of the left hand side of the
above expression. Since
k̂σ(xi +∆xi)− k̂σ(xi) =



kσ(xi +∆xi, x̂1)− kσ(xi, x̂1)
...
kσ(xi +∆xi, x̂m)− kσ(xi, x̂m)



we get
∥
∥
∥k̂σ(xi +∆xi)− k̂σ(xi)
∥
∥
∥
2
2
=
m
∑
j=1
(kσ(xi +∆xi, x̂j)− kσ(xi, x̂j))2 .
If the uncertainties are defined with the choice of Euclidian norm (meaning
p = q = 2), if we also define the kernel kσ with the Euclidian norm (meaning
kσ(x, z) := e
−
‖x− z‖22
2σ2 ), and observing that
12
kσ(xi +∆xi, x̂j)− kσ(xi, x̂j) = e
−
‖xi +∆xi − x̂j‖2
2σ2 − e
−
‖xi − x̂j‖2
2σ2
= e
−
‖xi − x̂j‖2
2σ2



e
−
2∆x⊤i (xi − x̂j) + ‖∆xi‖2
2σ2 − 1



,
we can obtain
(kσ(xi +∆xi, x̂j)− kσ(xi, x̂j))2 =



e
−
‖xi − x̂j‖2
2σ2



2


e
−
2∆x⊤i (xi − x̂j) + ‖∆xi‖2
2σ2



2
−2



e
−
‖xi − x̂j‖2
2σ2



2
e
−
2∆x⊤i (xi − x̂j) + ‖∆xi‖2
2σ2
+



e
−
‖xi − x̂j‖2
2σ2



2
=



e
−
‖xi − x̂j‖2
2σ2



2


e
−
2∆x⊤i (xi − x̂j)
2σ2



2


e
−
‖∆xi‖2
2σ2



2
−2



e
−
‖xi − x̂j‖2
2σ2



2
e
−
2∆x⊤i (xi − x̂j)
2σ2 e
−
‖∆xi‖2
2σ2
+



e
−
‖xi − x̂j‖2
2σ2



2
.
We have easily



e
−
‖∆xi‖2
2σ2



2
≤ 1. We are now aiming at bounding from below
the terms e
−
‖∆xi‖2
2σ2 and e
−
2∆x⊤i (xi − x̂j)
2σ2 . Using ‖Σ−1/2i ∆xi‖2 ≤ γi, we can
write
0 ≤ ‖∆xi‖ = ‖Σ1/2i Σ
−1/2
i ∆xi‖ ≤ ‖Σ
1/2
i ‖‖Σ
−1/2
i ∆xi‖ ≤ γi‖Σ
1/2
i ‖.
which consequently leads to, −γ2i ‖Σ
1/2
i ‖2 ≤ −‖∆xi‖2 ≤ 0 and
e
−
γ2i ‖Σ
1/2
i ‖2
2σ2 ≤ e
−
‖∆xi‖2
2σ2 ≤ 1.
13
Now, writing ∆x⊤i (xi−x̂j) = (xi−x̂j)⊤Σ1/2i Σ
−1/2
i ∆xi, we have
∣
∣∆x⊤i (xi − x̂j)
∣
∣ ≤
γi‖Σ⊤/2i (xi − x̂j)‖ and thus
e
−
2γi‖Σ⊤/2i (xi − x̂j)‖
2σ2 ≤ e
−
2∆x⊤i (xi − x̂j)
2σ2 ≤ e
2γi‖Σ⊤/2i (xi − x̂j)‖
2σ2 .
Using these bounds, we can write
(kσ(xi +∆xi, x̂j)− kσ(xi, x̂j))2 ≤



e
−
‖xi − x̂j‖2
2σ2



2


e
2γi‖Σ⊤/2i (xi − x̂j)‖
2σ2



2
−2



e
−
‖xi − x̂j‖2
2σ2



2
e
−
2γi‖Σ⊤/2i (xi − x̂j)‖
2σ2 e
−
γ2i ‖Σ
1/2
i ‖2
2σ2
+



e
−
‖xi − x̂j‖2
2σ2



2
and come back to
∥
∥
∥k̂σ(xi +∆xi)− k̂σ(xi)
∥
∥
∥
2
2
≤
m
∑
j=1



e
−
‖xi − x̂j‖2
2σ2



2




1 +



e
2γi‖Σ⊤/2i (xi − x̂j)‖
2σ2



2



− 2e
−
γ2i ‖Σ
1/2
i ‖2
2σ2
m
∑
j=1



e
−
‖xi − x̂j‖2
2σ2



2
e
−
2γi‖Σ⊤/2i (xi − x̂j)‖
2σ2
or
∥
∥
∥k̂σ(xi +∆xi)− k̂σ(xi)
∥
∥
∥
2
2
≤
m
∑
j=1
k2ij
(
1
τ2ij
+ 1
)
− 2ρi
m
∑
j=1
k2ijτij
where we have denoted
kij = e
−
‖xi − x̂j‖2
2σ2 , τij = e
−
2γi‖Σ⊤/2i (xi − x̂j)‖
2σ2 , ρi = e
−
γ2i ‖Σ1/2i ‖2
2σ2 .
Finally, we have obtained
∥
∥
∥
∥
(
Λ(r)σ
)1/2 (
φ̃σ(xi +∆xi)− φ̃σ(xi)
)
∥
∥
∥
∥
2
2
≤ Γ2i
14
with
Γi =
√
√
√
√
√r


m
∑
j=1
k2ij
(
1
τ2ij
+ 1
)
− 2ρi
m
∑
j=1
k2ijτij

.

5 A word on the tightness of bounding schemes
We would like here to analyze under which conditions the bounds we have
proposed are tight. Ensuring some tightness will avoid over estimating the
uncertainties in the approximate feature space that could make class separation
more difficult.
Consider the bounding scheme from Section 4.1 and where k is chosen as the
Gaussian kernel, meaning that kσ(x, z) := e
−
‖x− z‖2
2σ2 . The method relies on
the inequalities (11) that ensure that
∣
∣
∣cos
(
ω⊤j ∆xi
)
− 1
∣
∣
∣ ≤
(
ω⊤j ∆xi
)2
2∣
∣
∣sin
(
ω⊤j ∆xi
)∣
∣
∣ ≤ ω⊤j ∆xi.
These bounds are rather tight if the angle
∣
∣ω⊤j ∆xi
∣
∣ is smaller than some given
θmax. To ensure (with high probability) that the angle will remain below θmax
we state and proof the following result:
Proposition 5.1 Assume that Σ
1/2
i = Σ
1/2 where Σ1/2 is a constant diagonal
matrix. When q = 2, if
σ ≥ 3γi‖Σ
1/2‖F
θmax
,
with high probability (greater than 0.997), we have
∣
∣ω⊤j ∆xi
∣
∣ ≤ θmax.
Proof: Remember that ωj are distributed according to the Fourier transform of
the kernel k, therefore we have ωj ∼ N
(
0, 1σ2
)
and we know that ωj ∈
[
− 3σ , 3σ
]
with high probability (greater than 0.997). Additionally, we have shown in (12)
that for all j in {1, . . . , D/2},
∣
∣ω⊤j ∆xi
∣
∣ ≤
∥
∥
∥
Σ
⊤/2
i ωj
∥
∥
∥
q
γi. (13)
When q = 2 and Σ
1/2
i = Σ
1/2 = diag(s), we have
∥
∥
∥
Σ
⊤/2
i ωj
∥
∥
∥
2
2
=
n
∑
k=1
(skωjk)
2 ≤ 9‖Σ
1/2‖2F
σ2
,
15
and therefore, when we take
9γ2i ‖Σ1/2‖2F
σ2
≤ θ2max, (14)
using (13) we have, with high probability, that
∣
∣ω⊤j ∆xi
∣
∣ ≤ θmax. The inequality
(14) can also be written as σ ≥ 3γi‖Σ
1/2‖F
θmax
. 
The result in Proposition 5.1 actually says that there is a trade-off between
bounding the uncertainties in the approximate feature space and separating
correctly the uncertain features. For the latter, we would like to achieve rel-
atively small (but not too small) values of σ. On the other hand to ensure
tightness of our bounding scheme, the RFF method requires that σ should be
taken sufficiently large. Therefore, the right trade-off for choosing the σ value
is depending on data. The condition in Proposition 5.1 should then be incor-
porated in the definition of the σ-grid that one usually defines when designing
a cross-validation procedure in practice.
References
[1] F. Alizadeh and D. Goldfarb. Second-order cone programming. Mathemat-
ical Programming, 95(1):3–51, 2003.
[2] A. Ben-Tal, L. El Ghaoui, and A.S. Nemirovski. Robust Optimization.
Princeton Series in Applied Mathematics. Princeton University Press, Oc-
tober 2009.
[3] Aharon Ben-Tal, Sahely Bhadra, Chiranjib Bhattacharyya, and J. Saketha
Nath. Chance constrained uncertain classification via robust optimization.
Mathematical Programming, 127(1):145–173, 2011.
[4] Lo-Bin Chang, Zhidong Bai, Su-Yun Huang, and Chii-Ruey Hwang.
Asymptotic error bounds for kernel-based Nyström low-rank approxima-
tion matrices. J. Multivariate Anal., 120:102–119, 2013.
[5] Anna Choromanska, Tony Jebara, Hyungtae Kim, Mahesh Mohan, and
Claire Monteleoni. Fast spectral clustering via the Nyström method. In
Algorithmic learning theory, volume 8139 of Lecture Notes in Comput. Sci.,
pages 367–381. Springer, Heidelberg, 2013.
[6] Nicolas Couellan and Wenjuan Wang. On the convergence of stochastic
bi-level gradient methods. Eprint - Optimization Online, 2016.
[7] Nicolas Couellan and Wenjuan Wang. Uncertainty-safe large scale support
vector machines. Comput. Statist. Data Anal., 109:215–230, 2017.
[8] Nello Cristianini and John Shawe-Taylor. An introduction to support vec-
tor machines and other kernel-based learning methods. Repr. Cambridge:
Cambridge University Press, repr. edition, 2001.
16
[9] Alex Gittens and Michael W. Mahoney. Revisiting the Nyström method
for improved large-scale machine learning. J. Mach. Learn. Res., 17:1–65,
2016.
[10] Darren Homrighausen and Daniel J. McDonald. On the Nyström and
column-sampling methods for the approximate principal components anal-
ysis of large datasets. J. Comput. Graph. Statist., 25(2):344–362, 2016.
[11] Mu Li, Wei Bi, James T. Kwok, and Bao-Liang Lu. Large-scale Nyström
kernel matrix approximation using randomized SVD. IEEE Trans. Neural
Netw. Learn. Syst., 26(1):152–164, 2015.
[12] MOSEK-ApS. The MOSEK optimization toolbox for MATLAB manual.
Version 7.1 (Revision 28)., 2015.
[13] Ali Rahimi and Ben Recht. Random features for large-scale kernel ma-
chines. In Neural Information Processing Systems, 2007.
[14] Bernhard Scholkopf and Alexander J. Smola. Learning with Kernels: Sup-
port Vector Machines, Regularization, Optimization, and Beyond. MIT
Press, Cambridge, MA, USA, 2001.
[15] John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Anal-
ysis. Cambridge University Press, New York, NY, USA, 2004.
[16] P.K. Shiwaswamy, C. Bhattacharyya, and A. Smola. Second order cone
programming approaches for handling missing and uncertain data. Journal
of Machine Learning Research, 7:1283–1314, 2006.
[17] J.F. Sturm. Using sedumi 1.02, a matlab toolbox for optimization over
symmetric cones. Optimization Methods and Software, 1112:625653, 1999.
[18] Dougal J Sutherland and Jeff Schneider. On the error of random fourier
features. arXiv preprint arXiv:1506.02785, 2015.
[19] T. Trafalis and R. Gilbert. Robust support vector machines for classi-
fication and computational issues. Optimization Methods and Software,
22(1):187–198, 2007.
[20] T. B. Trafalis and R. C. Gilbert. Robust classification and regression us-
ing support vector machines. European Journal of Operational Research,
173(3):893–909, 2006.
[21] Aleksandar Trokicić. Approximate spectral learning using Nyström
method. Facta Univ. Ser. Math. Inform., 31(2):569–578, 2016.
[22] Vladimir Naoumovitch Vapnik. Statistical learning theory. Adaptive and
learning systems for signal processing, communications, and control. Wiley,
New York, 1998.
17
[23] Silvia Villa, Lorenzo Rosasco, and Bang Cong Vu. Learning with stochastic
proximal gradient. InWorkshop on Optimization for for Machine Learning.
At the conference on Advances in Neural Information Processing Systems
(NIPS), 2014.
[24] P. Xanthopoulos, P. Pardalos, and T.B. Trafalis. Robust Data Mining.
SpringerBriefs in Optimization. Springer, 2012.
18

