The Data GridTowards an Architecture for the Distributed Managementand Analysis of Large Scientific DatasetsAnn Chervenak Ian Foster Carl Kesselman Charles Salisbury Steven Tuecke Information Sciences Institute, University of Southern California Mathematics and Computer Science Division, Argonne National Laboratory Department of Computer Science, The University of Chicago1 IntroductionIn an increasing number of scientific disciplines, large data collections are emerging as importantcommunity resources. In domains as diverse as global climate change, high energy physics, andcomputational genomics, the volume of interesting data is already measured in terabytes and will soontotal petabytes. The communities of researchers that need to access and analyze this data oftenusing sophisticated and computationally expensive techniques are often large and are almost alwaysgeographically distributed, as are the computing and storage resources that these communities relyupon to store and analyze their data 17.This combination of large dataset size, geographic distribution of users and resources, and computationally intensive analysis results in complex and stringent performance demands that are notsatisfied by any existing data management infrastructure. A large scientific collaboration may generate many queries, each involving access toor supercomputerclass computations ongigabytes orterabytes of data. Efficient and reliable execution of these queries may require careful managementof terabyte caches, gigabits data transfer over wide area networks, coscheduling of data transfersand supercomputer computation, accurate performance estimations to guide the selection of datasetreplicas, and other advanced techniques that collectively maximize use of scarce storage, networking,and computing resources.The literature offers numerous point solutions that address these issues e.g., see 17, 14, 19, 3.But no integrating architecture exists that allows us to identify requirements and components commonto different systems and hence apply different technologies in a coordinated fashion to a range of dataintensive petabytescale application domains.Motivated by these considerations, we have launched a collaborative effort to design and producesuch an integrating architecture. We call this architecture the data grid, to emphasize its role as aspecialization and extension of the Grid that has emerged recently as an integrating infrastructurefor distributed computation 10, 20, 15. Our goal in this effort is to define the requirements that adata grid must satisfy and the components and APIs that will be required in its implementation. Wehope that the definition of such an architecture will accelerate progress on petascale dataintensivecomputing by enabling the integration of currently disjoint approaches, encouraging the deploymentof basic enabling technologies, and revealing technology gaps that require further research and development. In addition, we plan to construct a reference implementation for this architecture so as toenable largescale experimentation.1This work complements other activities in dataintensive computing. Work on highspeed diskcaches 21 and on tertiary storage and cache management 5, 19 provides basic building blocks.Work within the digital library community is developing relevant metadata standards and metadatadriven retrieval mechanisms 16, 6, 1 but has focused less on highspeed movement of large dataobjects, a particular focus of our work. The Storage Resource Broker SRB 2 shows how diversestorage systems can be integrated under uniform metadatadriven access mechanisms it provides avaluable building block for our architecture but should also benefit from the basic services describedhere. The High Performance Storage System HPSS 24 addresses enterpriselevel concerns e.g., itassumes that all accesses occur within the same DCE cell our work addresses new issues associatedwith wide area access from multiple administrative domains.In this paper, we first review the principles that we are following in developing a design for adata grid architecture. Then, we describe two basic services that we believe are fundamental to thedesign of a data grid, namely, storage systems and metadata management. Next, we explain howthese services can be used to develop various higherlevel services for replica management and replicaselection. We conclude by describing our initial implementation of data grid functionality.2 Data Grid DesignThe following four principles drive the design of our data grid architecture. These principles derivefrom the fact that data grid applications must frequently operate in wide area, multiinstitutional,heterogeneous environments, in which we cannot typically assume spatial or temporal uniformity ofbehavior or policy.Mechanism neutrality. The data grid architecture is designed to be as independent as possible ofthe lowlevel mechanisms used to store data, store metadata, transfer data, and so forth. This goalis achieved by defining data access, thirdparty data mover, catalog access, and other interfaces thatencapsulate peculiarities of specific storage systems, catalogs, data transfer algorithms, and the like.Policy neutrality. The data grid architecture is structured so that, as far as possible, designdecisions with significant performance implications are exposed to the user, rather than encapsulatedin black box implementations. Thus, while data movement and replica cataloging are provided asbasic operations, replication policies are implemented via higherlevel procedures, for which defaultsare provided but that can easily be substituted with applicationspecific code.Compatibility with Grid infrastructure. We attempt to overcome the difficulties of wide area, multiinstitutional operation by exploiting underlying Grid infrastructure 10, 20, 15 e.g., Globus 9 thatprovides basic services such as authentication, resource management, and information. To this end, westructure the data grid architecture so that more specialized data grid tools are compatible with lowerlevel Grid mechanisms. This approach also simplifies the implementation of strategies that integrate,for example, storage and computation.Uniformity of information infrastructure. As in the underlying Grid, uniform and convenientaccess to information about resource structure and state is emphasized as a means of enabling runtimeadaptation to system conditions. In practice, this means that we use the same data model and interfaceto access the data grids metadata, replica, and instance catalogs as are used in the underlying Gridinformation infrastructure.These four principles lead us to develop a layered architecture Figure 1, in which the lowest layersprovide highperformance access to an orthogonal set of basic mechanisms, but do not enforce specificusage policies. For example, we define highspeed data movement functions with rich error interfacesas a lowlevel mechanism, but do not encode within these functions how to respond to storage systemfailure. Rather, such policies are implemented in higher layers of the architecture, which build on the2Storage SystemDPSS HPSSMetadata RepositoryLDAP MCATResource ManagementLSF DIFFSERV NWSReplica SelectionReplica ManagementGeneric Grid ServicesData Grid Specific ServicesCore ServicesHigh LevelComponentsSecurity InstrumentationKerberos NetLoggerService ServiceFigure 1 Major components and structure of the data grid architecturemechanisms provided by the basic components.This approach is motivated by the observation that achieving high performance in specific applications often requires that an implementation exploit domainspecific or applicationspecific knowledge.In data grids, as in other Grid systems, this focus on simple, policyindependent mechanisms willencourage and enable broad deployment without limiting the range of applications that can be implemented. By limiting application specific behaviors to the upper layers of the architecture, we canpromote reuse of the basic mechanisms while delivering highperformance and specialized capabilitiesto the end user and application.3 Core Data Grid ServicesWe now turn our attention to the basic services required in a data grid architecture. We focus inparticular on two services that we view as fundamental data access and metadata access. The dataaccess service provides mechanisms for accessing, managing, and initiating thirdparty transfers ofdata stored in storage systems. The metadata access service provides mechanisms for accessing andmanaging information about data stored in storage systems. This explicit distinction between storageand metadata is worth discussing briefly. In some circumstances, for example when data is being storedin a database system, there are advantages to combining metadata and storage into the same abstraction. However, we believe that keeping these concepts separate at the architectural level enhancesflexibility in storage system implementation while having minimal impact on the implementation ofbehaviors that combine metadata access with storage access.3.1 Storage Systems and the Grid Storage APIIn a Grid environment, data may be stored in different locations and on different devices with differentcharacteristics. As we discussed above, mechanism neutrality implies that applications should notneed to be aware of the specific lowlevel mechanisms required to access data at a particular location.Instead, applications should be presented with a uniform view of data and with uniform mechanismsfor accessing that data. These requirements are met by the storage system abstraction and our gridstorage API. Together, these define our data access service.33.1.1 Data Abstraction Storage SystemsWe introduce as a basic data grid component what we call a storage system, which we define as anentity that can be manipulated with a set of functions for creating, destroying, reading, writing, andmanipulating the attributes of named sequences of bytes called file instances.Notice that our definition of a storage system is a logical one a storage system can be implementedby any storage technology that can support the required access functions. Implementations that targetUnix file systems, HTTP servers, hierarchical storage systems such as HPSS, and network caches suchas the Distributed Parallel Storage System DPSS are certainly envisioned. In fact, a storage systemneed not map directly to a single lowlevel storage device. For example, a distributed file system thatmanages files distributed over multiple storage devices or even sites can serve as a storage system, ascan an SRB system that serves requests by mapping to multiple storage systems of different types.Our definition of a file instance is also logical rather than physical. A storage system holds data,which may actually be stored in a file system, database, or other system we do not care about howdata is stored but specify simply that the basic unit that we deal with is a named sequences ofuninterpreted bytes. The use of the term file instance for this basic unit is not intended to implythat the data must live in a conventional file system. For example, a data grid implementation mightuse a system such as SRB to access data stored within a database management system.A storage system will associate with each of the file instances that it contains a set of properties,including a name and attributes such as its size and access restrictions. The name assigned to a fileinstance by a particular storage system is arbitrary and has meaning only to that storage system. Inmany storage systems, a name will be a hierarchical directory path. In other systems such as SRB,it may be a set of application metadata that the storage system maps internally to a physical fileinstance.3.1.2 Grid Storage APIThe behavior of a storage system as seen by a data grid user is defined by the data grid storage API,which defines a variety of operations on storage systems and file instances. Our understanding of thefunctionality required in this API is still evolving, but it certainly should include support for remoterequests to read andor write named file instances and to determine file instance attributes such assize. In addition, to support optimized implementation of replica management services discussedbelow we require a third party transfer operation used to transfer the entire contents of a file instancefrom one storage system to another.While the basic storage system functions just listed are relatively simple, various data grid considerations can increase the complexity of an implementation. For example, storage system accessfunctions must be integrated with the security environment of each site to which remote access is required 12. Robust performance within higherlevel functions requires reservation capabilities withinstorage systems and network interfaces 11. Applications should be able to provide storage systemswith hints concerning access patterns, network performance, and so forth that the storage systemcan use to optimize its behavior. Similarly, storage systems should be capable of characterizing andmonitoring their own performance this information, when made available to storage system clients,allows them to optimize their behavior. Finally, data movement functions must be able to detect andreport errors. While it may be possible to recover from some errors within the storage system, othererrors may need to reported back to the remote application that initiated the movement.43.2 The Metadata ServiceThe second set of basic machinery that we require is concerned with the management of informationabout the data grid itself, including information about file instances, the contents of file instances,and the various storage systems contained in the data grid. We refer to this information as metadata.The metadata service provides a means for publishing and accessing this metadata.Various types of metadata can be distinguished. It has become common practice to associate withscientific datasets metadata that describes the contents and structure of that data. The metadatamay describe the information content represented by the file, the circumstances under which the datawas obtained, andor other information useful to applications that process the data. We refer to thisas application metadata. Such metadata can be viewed as defining the logical structure or semanticsthat should apply to the uninterpreted bytes that make up a file instance or a set of file instances. Asecond type of metadata is used to describe the fabric of the data grid itself for example, details aboutstorage systems, such as their capacity and usage policy, as well as information about file instancesstored within a given storage system.The metadata service provides a uniform means for naming, publishing, and accessing these different types of metadata. Each type of metadata has its own characteristics in terms of frequency andmechanism of update and its logical relationship to other grid components and data items. Interestingdata management applications are likely to use several kinds of metadata. Although we have referredto several different sources of metadata, we propose that a single interface be used for accessing alltypes of metadata.The difficulty of specifying a general structure for all metadata is apparent when one considersthe variety of approaches used to describe application metadata. Some applications build a metadatarepository from a specified list of file instances based on data stored in a selfdescribing format e.g.,NetCDF, HDF. High energy physics applications are successfully using a specialized indexing structure. The Digital Library community is developing sets of metadata for different fields e.g., citedli3.Other user communities are pursuing the use of eXtended Markup Language XML 4 to representapplication metadata.The situation is further complicated when one considers the additional requirements imposed bylargescale data grid environments. Besides providing a means of integrating the different approaches tometadata storage and representation, the service must operate efficiently in a distributed environment.It must be scalable, supporting metadata about large number of entities being contributed by largenumbers of information sources located in large numbers of organizations. The service must be robustin the face of failure, and organizations should be able to assert local control over their information.Analysis of these requirements leads us to conclude that the metadata service must be structured asa hierarchical and distributed system. This approach allows us to achieve scalability, avoid any singlepoint of failure, and facilitate local control over data. Distribution does complicate efficient retrieval,but this difficulty can be overcome by having data organization exploit the hierarchical nature of themetadata service.This analysis leads us to propose that the metadata service be treated as a distributed directoryservice, such as that provided by the Lightweight Directory Access Protocol LDAP 23. Such systemssupport a hierarchal naming structure and rich data models and are designed to enable distribution.Mechanisms defined by LDAP include a means for naming objects, a data model based on namedcollections of attributes, and a protocol for performing attributebased searching and writing of dataelements. We have had extensive experience in using distributed directory services to represent generalGrid metadata 8, and we believe that they will be well suited to the metadata requirements of datagrids as well.The directory hierarchy associated with LDAP provides a structure for organizing, replicating,5and distributing catalog information. However, the directory service does not specify how the data isstored or where it is stored. Queries may be referred between servers, and the LDAP protocol can beplaced in front of a wide range of alternative information and metadata services. This capability canprovide a mechanism for the data grid to support a wide variety of approaches to providing applicationmetadata, while retaining a consistent overall approach to accessing that metadata.3.3 Other Basic ServicesThe data grid architecture also assumes the existence of a number of other basic services, includingthe following An authorization and authentication infrastructure that supports multiinstitutional operation.The public keybased Grid Security Infrastructure GSI 12 meets our requirements. Resource reservation and coallocation mechanisms for both storage systems and other resourcessuch as networks, to support the endtoend performance guarantees required for predictabletransfers e.g., 11. Performance measurements and estimation techniques for key resources involved in data gridoperation, including storage systems, networks, and computers e.g., the Network Weather Service 25. Instrumentation services that enable the endtoend instrumentation of storage transfers andother operations e.g., NetLogger 22, Pablo 18, and Paradyn 13.4 HigherLevel Data Grid ComponentsA potentially unlimited number of components can exist in the upper layer of the data grid architecture.Consequently, we will limit our discussion to two representative components replica management andreplica selection.4.1 Replica and Cache ManagementA replica manager is a data grid service whose functionality can be defined in terms of that providedby the storage system and metadata repository services. The role of a replica manager is to createor delete copies of file instances, or replicas, within specified storage systems. Typically, a replicais created because the new storage location offers better performance or availability for accesses to orfrom a particular location. In this section, we use the terms replica and file instance interchangeably.A replica might be deleted because storage space is required for another purpose.In this discussion, we assume that replicated files are read only we are not concerned with issuesof file update and coherency. Thus, replicas are primarily useful for access to published data sets.While this read only model is sufficient for many uses of scientific data sets, we intend to investigatesupport for modifying the contents of file instances in the future.For convenience, we group all of the replicas of a file instance along with any associated metadatainto a single entry in the metadata repository. We call this entry a logical file, since it representsthe logical structure i.e. metadata associated with the referenced file instances. In most situations,the file instances contained in a logical file will be byteforbyte copies of one another, but this is notrequired.A logical file exists in the metadata repository, since it describes attributes of a set of file instances,and its position in the repository provides a logical file with a globally unique name. To facilitate data6Replica CatalogReplica CatalogLogical FileLogical File MetadataFile Instance PointerFile Instance PointerLogical FileLogical File MetadataFile Instance PointerFile Instance PointerStorage SystemFile InstanceFile InstanceStorage System MetadataFile Instance MetadataFile Instance MetadataStorage SystemFile InstanceFile InstanceFile Instance MetadataFile Instance MetadataStorage System MetadataFigure 2 The structure of a replica catalog. Boxes shaded in gray represent storage system entities,all other boxes represent entity in the metadata repositorydiscovery, related logical files are grouped into collections, called replica catalogs, that are stored atwellknown locations in the metadata repository. The relationship between file instances, logical files,and replica catalogs is shown in Figure 2.A data grid may and indeed typically will contain multiple replica catalogs. For example, acommunity of researchers interested in a particular research topic might maintain a replica catalogfor a collection of data sets of mutual interest. Replica catalogs can thus provide the functionality oflogical collections, grouping logical files on related topics. It is possible to create hierarchies of replicacatalogs to impose a directorylike structure on related logical collections.A replica manager can perform access control on entire catalogs as well as on individual logicalfiles. By combining the functionality provided by the storage system and metadata repository, thereplica manager also can perform a number of basic operations, including creation and deletion ofreplicas, logical files, and replica catalogs.Note that the existence of a replica manager does not determine when or where replicas are created,or which replicas are to be used by an application, nor does it even require that every file instancebe entered into a replica catalog. In keeping policy out of the definition of the replica manager,we maximize the types of situations in which the replica manager will be useful. For example, a fileinstance that is not entered into the catalog may be considered to be in a local cache and available forlocal use only. Designing this as a policy rather than coupling file movement with catalog registrationin a single atomic operation explicitly acknowledges that there may be good, userdefined reasons forsatisfying application needs by using files that are not registered in a replica catalog.4.2 Replica Creation and Replica SelectionThe second representative highlevel service provided in the upper level of the data grid is replicaselection. Replica selection is interesting because it does not build on top of the core services, but ratherrelies on the functions provided by the replica management component described in the precedingsection. Replica selection is the process of choosing a replica that will provide an application with data7access characteristics that optimize a desired performance criterion, such as absolute performance i.e.speed, cost, or security. The selected file instance may be local or accessed remotely. Alternativelythe selection process may initiate the creation of a new replica whose performance will be superior tothe existing ones.Where replicas are to be selected based on access time, Grid information services can provideinformation about network performance, and perhaps the ability to reserve network bandwidth, whilethe metadata repository can provide information about the size of the file. Based on this, the selectorcan rank all of the existing replicas to determine which one will yield the fastest data access time.Alternatively, the selector can consult the same information sources to determine whether there is astorage system that would result in better performance if a replica was created on it.A more general selection service may consider access to subsets of a file instance. Scientific experiments often produce large files containing data for many variables, time steps, or events, and someapplication processing may require only a subset of this data. In this case, the selection function mayprovide an application with a file instance that contains only the needed subset of the data foundin the original file instance. This can obviously reduce the amount of data that must be accessed ormoved.This type of replica management has been implemented in other datamanagement systems. Forexample, STACS is often capable of satisfying requests from High Energy Physics applications byextracting a subset of data from a file instance. It does this using a complex indexing scheme thatrepresents application metadata for the events contained within the file. Other mechanisms for providing similar function may be built on application metadata obtainable from selfdescribing file formatssuch as NetCDF or HDF.Providing this capability requires the ability to invoke filtering or extraction programs that understand the structure of the file and produce the required subset of data. This subset becomes afile instance with its own metadata and physical characteristics, which are provided to the replicamanager. Replication policies determine whether this subset is recognized as a new logical file withan entry in the metadata repository and a file instance recorded in the replica catalog, or whetherthe file should be known only locally, to the selection manager.Data selection with subsetting may exploit Gridenabled servers, whose capabilities involve common operations such as reformatting data, extracting a subset, converting data for storage in a differenttype of system, or transferring data directly to another storage system in the Grid. The utility of thisapproach has been demonstrated as part of the Active Data Repository 7. The subsetting functioncould also exploit the more general capabilities of a computational Grid such as that provided byGlobus. This offers the ability to support arbitrary extraction and processing operations on files aspart of a data management activity.5 Status of the Data Grid ImplementationWe have made progress on several fronts in our effort to identify the basic lowlevel services for a datagrid architecture. In particular, we have defined a Grid Storage API, a standard interface to storagesystems that includes create, delete, open, close, read and write operations on file instances. Thisinterface also supports storage to storage transfers. We have implemented the interface for severalstorage systems, including local file access, HTTP servers, and DPSS network disk caches.We also have defined simple replica management and metadata services. These services use theMDS information infrastructure to store attribute information about file instances, storage systems,logical files, and replica catalogs. Using these attributes, we can query the information system tofind the replicas associated with a logical file, estimate their performance, and select among replicas8according to particular performance metrics.This work represents the first steps in our effort to create an integrating architecture for dataintensive petabytescale application domains. Performance studies are of the Grid Storage API areunder way, and we plan to further explore basic services such as instrumentation.AcknowledgmentsWe gratefully acknowledge helpful discussions with Steve Fitzgerald, Bill Johnston, Reagan Moore,Richard Mount, Harvey Newman, Arie Shoshani, Brian Tierney and other participants in the DOENext Generation Internet Earth System Grid and Particle Physics Data Grid projects. Thiswork was supported in part by the Mathematical, Information, and Computational Sciences Divisionsubprogram of the Office of Computational and Technology Research, U.S. Department of Energy,under Contract W31109Eng38.References1 M. Baldonado, C. Chang, L. Gravano, and A. Paepcke. The Stanford digital library metadataarchitecture. Intl J. Digital Libraries, 12108121, 1997.2 Chaitanya Baru, Reagan Moore, Arcot Rajasekar, and Michael Wan. The SDSC storage resourcebroker. In Proceedings of CASCON98 Conference. 1998.3 M. Beck and T. Moore. The Internet2 distributed storage infrastructure project An architecturefor internet content channels. Computer Networking and ISDN Systems, 30222321412148,1998.4 Tim Bray, Jean Paoli, and C. M. SperbergMcQueen. The extensible markup languagexml 1.0. W3C recomendation, World Wide Web Consortium, February 1998. Seehttpwww.w3.orgTR1998RECxml19980210.5 L.T. Chen, R. Drach, M. Keating, S. Louis, D. Rotem, and A. Shoshani. Efficient organizationand access of multidimensional datasets on tertiary storage systems. Information Systems SpecialIssue on Scientific Databases, 20215583, 1995.6 S. Cousins, H. GarciaMolina, S. Hassan, S. Ketchpel, M. Roscheisen, and T. Winograd. Towardsinteroperability in digital libraries. IEEE Computer, 295, 1996.7 Renato Ferreira, Tahsin Kurc, Michael Beynon, Chialin Chang, Alan Sussman, and Joel Saltz.Objectrelational queries into multidimensional databases with the active data repository. International Journal of Supercomputer Applications, 1999.8 S. Fitzgerald, I. Foster, C. Kesselman, G. von Laszewski, W. Smith, and S. Tuecke. A directoryservice for configuring highperformance distributed computations. In Proc. 6th IEEE Symp. onHigh Performance Distributed Computing, pages 365375. IEEE Computer Society Press, 1997.9 I. Foster and C. Kesselman. Globus A metacomputing infrastructure toolkit. InternationalJournal of Supercomputer Applications, 112115128, 1997.10 I. Foster and C. Kesselman, editors. The Grid Blueprint for a Future Computing Infrastructure.Morgan Kaufmann Publishers, 1999.911 I. Foster, C. Kesselman, C. Lee, R. Lindell, K. Nahrstedt, and A. Roy. A distributed resourcemanagement architecture that supports advance reservations and coallocation. In Proceedings ofthe International Workshop on Quality of Service, pages 2736, 1999.12 I. Foster, C. Kesselman, G. Tsudik, and S. Tuecke. A security architecture for computationalgrids. In ACM Conference on Computers and Security, pages 8391. ACM Press, 1998.13 Jeffrey Hollingsworth and Bart Miller. Instrumentation and measurement. In 10, pages 339365.14 William Johnston. Realtime widely distributed instrumentation systems. In 10, pages 75103.15 William E. Johnston, Dennis Gannon, and Bill Nitzberg. Grids as production computing environments The engineering aspects of NASAs Information Power Grid. In Proc. 8th IEEE Symp.on High Performance Distributed Computing. IEEE Computer Society Press, 1999.16 M. Lesk. Practical Digital Libraries Books, Bytes, and Bucks. Morgan Kaufmann Publishers,1997.17 Reagan Moore, Chaitanya Baru, Richard Marciano, Arcot Rajasekar, and Michael Wan. Dataintensive computing. In 10, pages 105129.18 Daniel Reed and Randy Ribler. Performance analysis and visualization. In 10, pages 367393.19 A. Shoshani, L. M. Bernardo, H. Nordberg, D. Rotem, and A. Sim. Storage management forhigh energy physics applications. In Computing in High Energy Physics 1998 CHEP 98. 1998.httpwww.lbl.gov ariepapersprocCHEP98.ps.20 R. Stevens, P. Woodward, T. DeFanti, and C. Catlett. From the IWAY to the National Technology Grid. Communications of the ACM, 40115061, 1997.21 B. Tierney, W. Johnston, L. Chen, H. Herzog, G. Hoo, G. Jin, and J. Lee. Distributed paralleldata storage systems A scalable approach to high speed image servers. In Proc. ACM Multimedia94. ACM Press, 1994.22 B. Tierney, W. Johnston, B. Crowley, G. Hoo, C. Brooks, and D. Gunter. The NetLoggermethodology for high performance distributed systems performance analysis. In Proc. 7th IEEESymp. on High Performance Distributed Computing. IEEE Computer Society Press, 1998.23 M. Wahl, T. Howes, and S. Kille. Lightweight directory access protocol v3. RFC 2251, InternetEngineering Task Force, 1997.24 Richard W. Watson and Robert A. Coyne. The parallel IO architecture of the HighPerformanceStorage System HPSS. In IEEE MSS Symposium, 1995.25 R. Wolski. Forecasting network performance to support dynamic scheduling using the networkweather service. In Proc. 6th IEEE Symp. on High Performance Distributed Computing, Portland,Oregon, 1997. IEEE Press.10
