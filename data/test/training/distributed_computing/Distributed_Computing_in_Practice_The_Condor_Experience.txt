Distributed Computingin PracticeThe Condor ExperienceDouglas Thain, Todd Tannenbaum, and Miron LivnyComputer Sciences Department, University of WisconsinMadison1210 West Dayton Street, Madison WI 53706SUMMARYSince 1984, the Condor project has enabled ordinary users to do extraordinarycomputing. Today, the project continues to explore the social and technical problemsof cooperative computing on scales ranging from the desktop to the worldwidecomputational grid. In this chapter, we provide the history and philosophy of the Condorproject and describe how it has interacted with other projects and evolved along with thefield of distributed computing. We outline the core components of the Condor systemand describe how the technology of computing must correspond to social structures.Throughout, we reflect on the lessons of experience and chart the course traveled byresearch ideas as they grow into production systems.key words Condor, grid, history, community, planning, scheduling, split execution1. IntroductionReady access to large amounts of computing power has been a persistent goal of computerscientists for decades. Since the 1960s, visions of computing utilities as pervasive and assimple as the telephone have driven users and system designers. 54 It was recognized inthe 1970s that such power could be achieved inexpensively with collections of small devicesrather than expensive single supercomputers. Interest in schemes for managing distributedprocessors 68, 21, 18 became so popular that there was even once a minor controversy overthe meaning of the word distributed. 25As this early work made it clear that distributed computing was feasible, researchers beganto take notice that distributed computing would be difficult. When messages may be lost,corrupted, or delayed, robust algorithms must be used in order to build a coherent if notcontrollable system. 40, 39, 19, 53 Such lessons were not lost on the system designers ofthe early 1980s. Production systems such as Locus 77 and Grapevine 16 wrestled with thefundamental tension between consistency, availability, and performance in distributed systems.DISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 1In this environment, the Condor project was born. At the University of Wisconsin, MironLivny combined his doctoral thesis on cooperative processing 47 with the powerful CrystalMulticomputer 24 designed by Dewitt, Finkel, and Solomon and the novel Remote Unix 46software designed by Michael Litzkow. The result was Condor, a new system for distributedcomputing. In contrast to the dominant centralized control model of the day, Condor wasunique in its insistence that every participant in the system remain free to contribute as muchor as little as it cared to.The Condor system soon became a staple of the production computing environment at theUniversity of Wisconsin, partially because of its concern for protecting individual interests. 44A production setting can be both a curse and a blessing The Condor project learned hardlessons as it gained real users. It was soon discovered that inconvenienced machine ownerswould quickly withdraw from the community. This led to a longstanding Condor motto Leavethe owner in control, regardless of the cost. A fixed schema for representing users and machineswas in constant change and so eventually led to the development of a schemafree resourceallocation language called ClassAds. 59, 60, 58 It has been observed that most complexsystems struggle through an adolescence of five to seven years. 42 Condor was no exception.Scientific interests began to recognize that coupled commodity machines were significantlyless expensive than supercomputers of equivalent power 66. A wide variety of powerful batchexecution systems such as LoadLeveler 22 a descendant of Condor, LSF 79, Maui 35,NQE 34, and PBS 33 spread throughout academia and business. Several high profiledistributed computing efforts such as SETIHome and Napster raised the public consciousnessabout the power of distributed computing, generating not a little moral and legal controversyalong the way 9, 67. A vision called grid computing began to build the case for resourcesharing across organizational boundaries 30.Throughout this period, the Condor project immersed itself in the problems of productionusers. As new programming environments such as PVM 56, MPI 78, and Java 74 becamepopular, the project added system support and contributed to standards development. Asscientists grouped themselves into international computing efforts such as the Grid PhysicsNetwork 3 and the Particle Physics Data Grid PPDG 6, the Condor project took partfrom initial design to enduser support. As new protocols such as GRAM 23, GSI 28, andGridFTP 8 developed, the project applied them to production systems and suggested changesbased on the experience. Through the years, the Condor project adapted computing structuresto fit changing human communities.Many previous publications about Condor have described in fine detail the features of thesystem. In this chapter, we will lay out a broad history of the Condor project and its designphilosophy. We will describe how this philosophy has led to an organic growth of computingcommunities and discuss the planning and scheduling techniques needed in such an uncontrolledsystem. Next, we will describe how our insistence on dividing responsibility has led to a uniquemodel of cooperative computing called split execution. In recent years, the project has addeda new focus on dataintensive computing. We will outline this new research area and describeout recent contributions. Security has been an increasing user concern over the years. We willdescribe how Condor interacts with a variety of security systems. Finally, we will conclude bydescribing how real users have put Condor to work.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls2 D. THAIN, T. TANNENBAUM, AND M. LIVNY2. The Philosophy of FlexibilityThe Condor design philosophy can be summarized with one word flexibility.As distributed systems scale to ever larger sizes, they become more and more difficultto control or even to describe. International distributed systems are heterogeneous in everyway they are composed of many types and brands of hardware they run various operatingsystems and applications they are connected by unreliable networks they change configurationconstantly as old components become obsolete and new components are powered on. Mostimportantly, they have many owners, each with private policies and requirements that controltheir participation in the community.Flexibility is the key to surviving in such a hostile environment. Five admonitions outlinethe philosophy of flexibility.Let communities grow naturally. People have a natural desire to work togetheron common problems. Given tools of sufficient power, people will organize the computingstructures that they need. However, human relationships are complex. People invest theirtime and resources into many communities with varying degrees. Trust is rarely completeor symmetric. Communities and contracts are never formalized with the same level ofprecision as computer code. Relationships and requirements change over time. Thus, we aimto build structures that permit but do not require cooperation. We believe that relationships,obligations, and schemata will develop according to user necessity.Leave the owner in control, whatever the cost. To attract the maximum number ofparticipants in a community, the barriers to participation must be low. Users will not donatetheir property to the common good unless they maintain some control over how it is used.Therefore, we must be careful to provide tools for the owner of a resource to set policies andeven instantly retract a resource for private use.Plan without being picky. Progress requires optimism. In a community of sufficientsize, there will always be idle resources available to do work. But, there will also always beresources that are slow, misconfigured, disconnected, or broken. An overdependence on thecorrect operation of any remote device is a recipe for disaster. As we design software, wemust spend more time contemplating the consequences of failure than the potential benefitsof success. When failures come our way, we must be prepared to retry or reassign work as thesituation permits.Lend and borrow. The Condor project has developed a large body of expertise indistributed resource management. Countless other practitioners in the field are experts inrelated fields such as networking, databases, programming languages, and security. The Condorproject aims to give the research community the benefits of our expertise while accepting andintegrating knowledge and software from other sources. Our field has developed over manydecades, known by many overlapping names such as operating systems, distributed computing,metacomputing, peertopeer computing, and grid computing. Each of these emphasizes aparticular aspect of the discipline, but are united by fundamental concepts. If we fail tounderstand and apply previous research, we will at best rediscover wellcharted shores. Atworst, we will wreck ourselves on wellcharted rocks.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 33. The Condor SoftwareResearch in distributed computing requires immersion in the real world. To this end, theCondor project maintains, distributes, and supports a variety of computing systems that aredeployed by commercial and academic interests world wide. These products are the provinggrounds for ideas generated in the academic research environment. The project is best knownfor two products The Condor highthroughput computing system, and the CondorG agentfor grid computing.3.0.1. The Condor High Throughput Computing SystemCondor is a highthroughput distributed batch computing system. Like other batch systems,Condor provides a job management mechanism, scheduling policy, priority scheme, resourcemonitoring, and resource management. 69, 70 Users submit their jobs to Condor, and Condorsubsequently chooses when and where to run them based upon a policy, monitors their progress,and ultimately informs the user upon completion.While similar to other conventional batch systems, Condors novel architecture allows it toperform well in environments where other batch systems are weak highthroughput computingand opportunistic computing. The goal of a highthroughput computing environment 12 is toprovide large amounts of fault tolerant computational power over prolonged periods of time byeffectively utilizing all resources available to the network. The goal of opportunistic computingis the ability to use resources whenever they are available, without requiring one hundredpercent availability. The two goals are naturally coupled. Highthroughput computing is mosteasily achieved through opportunistic means.This requires several unique and powerful tools ClassAds. The ClassAd language in Condor provides an extremely flexible andexpressive framework for matching resource requests e.g. jobs with resource offers e.g.machines. ClassAds allow Condor to adopt to nearly any allocation policy, and to adopta planning approach when incorporating grid resources. We will discuss this approachfurther in a section below. Job Checkpoint and Migration. With certain types of jobs, Condor can transparentlyrecord a checkpoint and subsequently resume the application from the checkpoint file. Aperiodic checkpoint provides a form of fault tolerance and safeguards the accumulatedcomputation time of a job. A checkpoint also permits a job to migrate from onemachine to another machine, enabling Condor to perform lowpenalty preemptiveresumescheduling. 38 Remote System Calls. When running jobs on remote machines, Condor can oftenpreserve the local execution environment via remote system calls. Remote system callsis one of Condors mobile sandbox mechanisms for redirecting all of a jobs IO relatedsystem calls back to the machine which submitted the job. Therefore users do not need tomake data files available on remote workstations before Condor executes their programsthere, even in the absence of a shared filesystem.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls4 D. THAIN, T. TANNENBAUM, AND M. LIVNYCondor CondorGProcessing, Storage, Communication, ...Application, Problem Solver, ...FabricGridUserCondor Globus ToolkitFigure 1. Condor in the GridWith these tools, Condor can do more than effectively manage dedicated compute clusters.69, 70 Condor can also scavenge and manage wasted CPU power from otherwise idle desktopworkstations across an entire organization with minimal effort. For example, Condor can beconfigured to run jobs on desktop workstations only when the keyboard and CPU are idle. Ifa job is running on a workstation when the user returns and hits a key, Condor can migratethe job to a different workstation and resume the job right where it left off.Moreover, these same mechanisms enable preemptiveresume scheduling on dedicatedcompute cluster resources. This allows Condor to cleanly support prioritybased schedulingon clusters. When any node in a dedicated cluster is not scheduled to run a job, Condor canutilize that node in an opportunistic manner  but when a schedule reservation requires thatnode again in the future, Condor can preempt any opportunistic computing job which mayhave been placed there in the meantime. 78 The end result Condor is used to seamlesslycombine all of an organizations computational power into one resource.3.0.2. CondorG An Agent for Grid ComputingCondorG 31 represents the marriage of technologies from the Condor and Globus projects.From Globus 29 comes the use of protocols for secure interdomain communications andstandardized access to a variety of remote batch systems. From Condor comes the userconcerns of job submission, job allocation, error recovery, and creation of a friendly executionenvironment. The result is a tool that binds resources spread across many systems into apersonal highthroughput computing system.Condor technology can exist at both the front and back ends of a grid, as depicted inFigure 1. CondorG can be used as the reliable submission and job management service forone or more sites, the Condor High Throughput Computing system can be used as the fabricCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 5ProblemSolverAgent ResourceShadowJobSandboxDAGManMasterWorkerstarterstartdscheddshadowMatchmakercentral managerUserFigure 2. The Condor Kernel This figure shows the major processes in a Condor system. Thecommon generic name for each process is given in large print. In parentheses are the technical Condorspecific names used in some publications.management service a grid generator for one or more sites and the Globus Toolkit can beused as the bridge between them. In fact, Figure 1 can serve as a simplified diagram for manyemerging grids, such as the USCMS Testbed Grid 2 and the European Union Data Grid. 14. An Architectural History of CondorOver the course of the Condor project, the fundamental structure of the system has remainedconstant while its power and functionality has steadily grown. The core components, knownas the kernel, are shown in Figure 2. In this section, we will examine how a wide variety ofcomputing communities may be constructed with small variations to the kernel.Briefly, the kernel works as follows The user submits jobs to an agent. The agent isresponsible for remembering jobs in persistent storage while finding resources willing torun them. Agents and resources advertise themselves to a matchmaker, which is responsiblefor introducing potentially compatible agents and resources. Once introduced, an agent isresponsible for contacting a resource and verifying that the match is still valid. To actuallyexecute a job, each side must start a new process. At the agent, a shadow is responsible forproviding all of the details necessary to execute a job. At the resource, a sandbox is responsiblefor creating a safe execution environment for the job and protecting the resource from anymischief.Let us begin by examining how agents, resources, and matchmakers come together to formCondor pools. Later in this chapter, we will return to examine the other components of thekernel.The initial conception of Condor is shown in Figure 3. Agents and resources independentlyreport information about themselves to a wellknown matchmaker, which then makes the sameinformation available to the community. A single machine typically runs both an agent and aCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls6 D. THAIN, T. TANNENBAUM, AND M. LIVNYRRRA1312MCondorPoolFigure 3. A Condor Pool ca. 1988 An agent A executes a job on a resource R with the help ofa matchmaker M. Step 1 The agent and the resource advertise themselves to the matchmaker. Step2 The matchmaker informs the two parties that they are potentially compatible. Step 3 The agentcontacts the resource and executes a job.resource server and is capable of submitting and executing jobs. However, agents and resourcesare logically distinct. A single machine may run either or both, reflecting the needs of its owner.Each of the three parties  agents, resources, and matchmakers  are independent andindividually responsible for enforcing their owners policies. The agent enforces the submittingusers policies on what resources are trusted and suitable for running jobs. For example, a usermay wish to use machines running the Linux operating system, preferring the use of fasterCPUs. The resource enforces the machine owners policies on what users are to be trustedand serviced. For example, a machine owner might be willing to serve any user, but givepreference to members of the Computer Science department, while rejecting a user knownto be untrustworthy. The matchmaker is responsible for enforcing communal policies. Forexample, a matchmaker might allow any user to access a pool, but limit nonmembers ofthe Computer Science department to consuming ten machines at a time. Each participant isautonomous, but the community as a single entity is defined by the common selection of amatchmaker.As the Condor software developed, pools began to sprout up around the world. In the originaldesign, it was very easy to accomplish resource sharing in the context of one community. Aparticipant merely had to get in touch with a single matchmaker to consume or provideresources. However, a user could only participate in one community that defined by amatchmaker. Users began to express their need to share across organizational boundaries.This observation led to the development of gateway flocking in 1994. 26 At that time, therewere several hundred workstations at Wisconsin, while tens of workstations were scatteredacross several organizations in Europe. Combining all of the machines into one Condor poolwas not a possibility because each organization wished to retain existing community policiesenforced by established matchmakers. Even at the University of Wisconsin, researchers wereunable to share resources between the separate engineering and computer science pools.The concept of gateway flocking is shown in Figure 4. Here, the structure of two existingpools is preserved, while two gateway nodes pass information about participants between theCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 7RRRGRRACondorPool ARGPool BCondor21M M3134Figure 4. Gateway Flocking ca. 1994 An agent A is shown executing a job on a resource Rvia a gateway G. Step 1 The agent and resource advertise themselves locally. Step 2 The gatewayforwards the agents unsatisfied request to Condor Pool B. Step 3 The matchmaker informs the twoparties that they are potentially compatible. Step 4 The agent contacts the resource and executes a jobvia the gateway.DelftWarsawMadison200 3310 30Amsterdam33Geneva 104DubnaBerlinFigure 5. Condor World Map ca. 1994 This is a map of the worldwide Condor flock in 1994. Eachdot indicates a complete Condor pool. Numbers indicate the size of each Condor pool. Lines indicateflocking via gateways. Arrows indicate the direction that jobs may flow.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls8 D. THAIN, T. TANNENBAUM, AND M. LIVNYRRRRRACondorPool ARPool BCondorM1 13 324MFigure 6. Direct Flocking ca. 1998 An agent A is shown executing a job on a resource R viadirect flocking. Step 1 The agent and the resource advertise themselves locally. Step 2 The agent isunsatisfied, so it also advertises itself to Condor Pool B. Step 3 The matchmaker M informs thetwo parties that they are potentially compatible. Step 4 The agent contacts the resource and executesa job.two pools. If a gateway detects idle agents or resources in its home pool, it passes them to itspeer, which advertises them in the remote pool, subject to the admission controls of the remotematchmaker. Gateway flocking is not necessarily bidirectional. A gateway may be configuredwith entirely different policies for advertising and accepting remote participants.Gateway flocking was deployed for several years in the 1990s to share computing resourcesbetween the United States and several European institutions. Figure 5 shows the state of theworldwide Condor flock in 1994.The primary advantage of gateway flocking is that it is completely transparent toparticipants. If the owners of each pool agree on policies for sharing load, then crosspoolmatches will be made without any modification by users. A very large system may be grownincrementally with administration only required between adjacent pools.There are also significant limitations to gateway flocking. Because each pool is representedby a single gateway machine, the accounting of use by individual remote users is essentiallyimpossible. Most importantly, gateway flocking only allows sharing at the organizationallevel  it does not permit an individual user to join multiple communities. This becamea significant limitation as distributed computing became a larger and larger part of dailyproduction work in scientific and commercial circles. Individual users might be members ofmultiple communities and yet not have the power or need to establish a formal relationshipbetween both communities.This problem was solved by direct flocking, shown in Figure 6. Here, an agent may simplyreport itself to multiple matchmakers. Jobs need not be assigned to any individual community,but may execute in either as resources become available. An agent may still use eithercommunity according to its policy while all participants maintain autonomy as before.Both forms of flocking have their uses, and may even be applied at the same time. Gatewayflocking requires agreement at the organizational level, but provides immediate and transparentCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 9Q QR R R R RARForeign Batch System Foreign Batch System1122Figure 7. CondorG ca. 2000 An agent A is shown executing two jobs through foreign batch queuesQ. Step 1 The agent transfers jobs directly to remote queues. Step 2 The jobs wait for idle resourcesR, and then execute on them.benefit to all users. Direct flocking only requires agreement between one individual and anotherorganization, but accordingly only benefits the user who takes the initiative.This is a reasonable tradeoff found in everyday life. Consider an agreement between twoairlines to crossbook each others flights. This may require years of negotiation, pages ofcontracts, and complex compensation schemes to satisfy executives at a high level. But, onceput in place, customer have immediate access to twice as many flights with no inconvenience.Conversely, an individual may take the initiative to seek service from two competing airlinesindividually. This places an additional burden on the customer to seek and use multipleservices, but requires no herculean administrative agreement.Although gateway flocking was of great use before the development of direct flocking, it didnot survive the evolution of Condor. In addition to the necessary administrative complexity,it was also technically complex. The gateway participated in every interaction in the Condorkernel. It had to appear as both an agent and a resource, communicate with the matchmaker,and provide tunneling for the interaction between shadows and sandboxes. Any change tothe protocol between any two components required a change to the gateway. Direct flocking,although less powerful, was much simpler to build and much easier for users to understandand deploy.About 1998, a vision of a worldwide computational grid began to grow. 30 A significantearly piece in the grid computing vision was a uniform interface for batch execution. The GlobusProject 29 designed the Grid Resource Access and Management GRAM protocol 23 tofill this need. GRAM provides an abstraction for remote process queuing and execution withseveral powerful features such as strong security and file transfer. The Globus Project providesa server that speaks GRAM and converts its commands into a form understood by a varietyof batch systems.To take advantage of GRAM, a user still needs a system that can remember what jobshave been submitted, where they are, and what they are doing. If jobs should fail, the systemCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls10 D. THAIN, T. TANNENBAUM, AND M. LIVNYStep Oneas batch jobs in foreign systems.Step Twoadhoc personal Condor pool.User runs jobs on.Step Threepersonal Condor pool.Q QR R R R RARForeign Batch System Foreign Batch SystemQ QR R R R RARPersonal Condor PoolQ QR R R R RRPersonal Condor PoolMMMAGRAM GRAMUser submits the Condor serversSubmitted servers form anFigure 8. CondorG and Gliding In ca. 2001 A CondorG agent A executes jobs on resourcesR by gliding in through remote batch queues Q. Step 1 A CondorG agent submits the Condorservers to two foreign batch queues via GRAM. Step 2 The servers form a personal Condor pool withthe users personal matchmaker M. Step 3 The agent executes jobs as in Figure 3.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 11must analyze the failure and resubmit the job if necessary. To track large numbers of jobs,users need queuing, prioritization, logging, and accounting. To provide this service, the Condorproject adapted a standard Condor agent to speak GRAM, yielding a system called CondorG,shown in Figure 7. This required some small changes to GRAM such as adding durability andtwophase commit to prevent the loss or repetition of jobs. 32GRAM expands the reach of a user to any sort of batch system, whether it runs Condoror another batch system. For example, the solution of the NUG30 43 quadratic assignmentproblem relied on the ability of CondorG to mediate access to over a thousand hosts spreadacross tens of batch systems on several continents. We will describe NUG30 in greater detailbelow.The are also some disadvantages to GRAM. Primarily, it couples resource allocation and jobexecution. Unlike direct flocking in Figure 6, the agent must direct a particular job, with itsexecutable image and all, to a particular queue without knowing the availability of resourcesbehind that queue. This forces the agent to either oversubscribe itself by submitting jobsto multiple queues at once or undersubscribe itself by submitting jobs to potentially longqueues. Another disadvantage is that CondorG does not support all of the varied features ofeach batch system underlying GRAM. Of course, this is a necessity if GRAM included all thebells and whistles of every underlying system, it would be so complex as to be unusable.This problem is solved with a technique called gliding in, shown in Figure 8. To takeadvantage of both the powerful reach of GRAM and the full Condor machinery, a personalCondor pool may be carved out of remote resources. This requires three steps. In the firststep, a CondorG agent is used to submit the standard Condor servers as jobs to remote batchsystems. From the remote systems perspective, the Condor servers are ordinary jobs withno special privileges. In the second step, the servers begin executing and contact a personalmatchmaker started by the user. These remote resources along with the users CondorG agentand matchmaker from a personal Condor pool. In step three, the user may submit normal jobsto the CondorG agent, which are then matched to and executed on remote resources with thefull capabilities of Condor.To this point, we have defined communities in terms of such concepts as responsibility,ownership, and control. However, communities may also be defined as a function of moretangible properties such as location, accessibility, and performance. Resources may groupthemselves together to express that they are nearby in measurable properties such as networklatency or system throughput. We call these groupings IO communities.IO communities were expressed in early computational grids such as the Distributed BatchController DBC 20. The DBC was designed in 1996 for processing data from the NASAGoddard Space Flight Center. Two communities were included in the original design oneat the University of Wisconsin, and the other in the District of Columbia. A high levelscheduler at Goddard would divide a set of data files among available communities. Eachcommunity was then responsible for transferring the input data, performing computation, andtransferring the output back. Although the highlevel scheduler directed the general progressof the computation, each community retained local control by employing Condor to manageits resources.Another example of an IO community is the execution domain. This concept was developedto improve the efficiency of data transfers across a widearea network. An execution domain is aCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls12 D. THAIN, T. TANNENBAUM, AND M. LIVNYR RR RPavia25 CR RR R18PadovaCR RR RTrieste11R RR R51MilanoR RR R18TorinoC1RRomaNapoli2R RR RR RBari4CR RR R11 CLAquilaR RR RBologna51MCFigure 9. INFN Condor Pool ca. 2002 This is a map of a single Condor pool spread across Italy.All resources R across the country share the same matchmaker M in Bologna. Dotted lines indicateexecution domains where resources share a checkpoint server C. Numbers indicate resources at eachsite. Resources not assigned to a domain use the checkpoint server in Bologna.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 13unmappablePool Size101005001000Condor Pools on 25 Mar 2004956 pools  37790 hostsFigure 10. Condor World Map ca. 2004 Each dot indicates an independent Condor pool. The areacovered by each dot is proportional to the number of machines in the pool. The location of each poolis determined by the toplevel country domain name of the matchmaker, if present, or otherwise frompublic WHOIS records. Each dot is scattered by a small random factor, thus some appear to fall in thesea. A small number of pools that could not be mapped are plotted in the South Pacific.collection of resources that identify themselves with a checkpoint server that is close enough toprovide good IO performance. An agent may then make informed placement and migrationdecisions by taking into account the rough physical information provided by an executiondomain. For example, an agent might strictly require that a job remain in the executiondomain that it was submitted from. Or, it might permit a job to migrate out of its domainafter a suitable waiting period. Examples of such policies expressed in the ClassAd languagemay be found in 13.Figure 9 shows a deployed example of execution domains. The Istituto Nazionale de FisicaNucleare INFN Condor pool consists of a large set of workstations spread across Italy.Although these resources are physically distributed, they are all part of a national organization,and thus share a common matchmaker in Bologna which enforces institutional policies. Toencourage local access to data, six execution domains are defined within the pool, indicated bydotted lines. Each domain is internally connected by a fast network and shares a checkpointserver. Machines not specifically assigned to an execution domain default to the checkpointserver in Bologna.Today, Condor is deployed around the world in pools ranging from a handful of CPUs tothousands of CPUs. Each matchmaker periodically sends a message home to the University ofWisconsin by email of UDP, where we maintain a global catalog of Condor pools. Figure 10plots this data on a map, showing the size and distribution of Condor around the world. Ofcourse, this map is incomplete some Condor pools are deployed behind firewalls, and someusers voluntarily disable the periodic messages. However, it can be seen that Condor is wellused today.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls14 D. THAIN, T. TANNENBAUM, AND M. LIVNY5. Planning and SchedulingIn preparing for battle I have always found thatplans are useless, but planning is indispensable. Dwight D. Eisenhower 1890  1969The central purpose of distributed computing is to enable a community of users to performwork on a pool of shared resources. Because the number of jobs to be done nearly alwaysoutnumbers the available resources, somebody must decide how to allocate resources to jobs.Historically, this has been known as scheduling. A large amount of research in scheduling wasmotivated by the proliferation of massively parallel processor machines in the early 1990s andthe desire to use these very expensive resources as efficiently as possible. Many of the resourcemanagement systems we have mentioned contain powerful scheduling components in theirarchitecture.Yet, grid computing cannot be served by a centralized scheduling algorithm. By definition,a grid has multiple owners. Two supercomputers purchased by separate organizations withdistinct funds will never share a single scheduling algorithm. The owners of these resourceswill rightfully retain ultimate control over their own machines and may change schedulingpolicies according to local decisions. Therefore, we draw a distinction based on the ownership.Grid computing requires both planning and schedulingPlanning is the acquisition of resources by users. Users are typically interested in increasingpersonal metrics such as response time, turnaround time, and throughput of their own jobswithin reasonable costs. For example, an airline customer performs planning when she examinesall available flights from Madison to Melbourne in an attempt to arrive before Friday for lessthan 1500. Planning is usually concerned with the matters of what and where.Scheduling is the management of a resource by its owner. Resource owners are typicallyinterested in increasing system metrics such as efficiency, utilization, and throughput withoutlosing the customers they intend to serve. For example, an airline performs scheduling whenits sets the routes and times that its planes travel. It has an interest in keeping planes full andprices high without losing customers to its competitors. Scheduling is usually concerned withthe matters of who and when.Of course, there is feedback between planning and scheduling. Customers change theirplans when they discover a scheduled flight is frequently late. Airlines change their schedulesaccording to the number of customers that actually purchase tickets and board the plane.But both parties retain their independence. A customer may purchase more tickets than sheactually uses. An airline may change its schedules knowing full well it will lose some customers.Each side must weigh the social and financial consequences against the benefits.The challenges faced by planning and scheduling in a grid computing environment are verysimilar to the challenges faced by cyclescavenging from desktop workstations. The insistencethat each desktop workstation is the sole property of one individual who is in completecontrol, characterized by the success of the personal computer, results in distributed ownership.Personal preferences and the fact that desktop workstations are often purchased, upgraded,and configured in a haphazard manner results in heterogeneous resources. Workstation ownersCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 15Advertisement 1Notification 3Claiming 4Advertisement 1Notification 3Agent ResourceMatchmaking Algorithm 2MatchmakerFigure 11. Matchmakingpowering their machines on and off whenever they desire creates a dynamic resource pool, andowners performing interactive work on their own machines creates external influences.Condor uses matchmaking to bridge the gap between planning and scheduling. Matchmakingcreates opportunities for planners and schedulers to work together while still respectingtheir essential independence. Although Condor has traditionally focused on producing robustplanners rather than complex schedulers, the matchmaking framework allows both parties toimplement sophisticated algorithms.Matchmaking requires four steps, shown in Figure 11. In the first step, agents and resourcesadvertise their characteristics and requirements in classified advertisements ClassAds, namedafter brief advertisements for goods and services found in the morning newspaper. In thesecond step, a matchmaker scans the known ClassAds and creates pairs that satisfy eachothers constraints and preferences. In the third step, the matchmaker informs both parties ofthe match. The responsibility of the matchmaker then ceases with respect to the match. Inthe final step, claiming, the matched agent and resource establish contact, possibly negotiatefurther terms, and then cooperate to execute a job. The clean separation of the claiming stepallows the resource and agent to independently verify the match. 48A ClassAd is a set of uniquely named expressions, using a semistructured data model so thatno specific schema is required by the matchmaker. Each named expression is called an attribute.Each attribute has an name and an value. The first version of the ClassAd language allowedsimple values such as integers and strings, as well as expressions comprised of arithmetic andlogical operators. After gaining experience with ClassAds, we created a second version of thelanguage that permitted richer values and operators permitting complex data structures suchas records, lists, and sets.Because ClassAds are schemafree, participants in the system may attempt to refer toattributes that do not exist. For example, an job may prefer machines with the attributeOwner  Fred, yet some machines may fail to define the attribute Owner. To solvethis, ClassAds use threevalued logic which allows expressions to evaluated to either true,Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls16 D. THAIN, T. TANNENBAUM, AND M. LIVNYJob ClassAd Machine ClassAdMyType  JobTargetType  MachineRequirements other.ArchINTEL other.OpSysLINUX other.Disk  my.DiskUsageRank  Memory  10000  KFlopsCmd  hometannenbabinsimexeDepartment  CompSciOwner  tannenbaDiskUsage  6000MyType  MachineTargetType  JobMachine  nostos.cs.wisc.eduRequirements LoadAvg  0.300000 KeyboardIdle  15  60Rank  other.Departmentself.DepartmentArch  INTELOpSys  LINUXDisk  3076076Figure 12. Two Sample ClassAds from Condor.false, or undefined. This explicit support for missing information allows users to build robustrequirements even without a fixed schema.The Condor matchmaker assigns significance to two special attributes Requirements andRank. Requirements indicates a constraint and Rank measures the desirability of a match. Thematchmaking algorithm requires that for two ClassAds to match, both of their correspondingRequirements must evaluate to true. The Rank attribute should evaluate to an arbitraryfloating point number. Rank is used to choose among compatible matches Among providerads matching a given customer ad, the matchmaker chooses the one with the highest Rankvalue noninteger values are treated as zero, breaking ties according to the providers Rankvalue.ClassAds for a job and a machine are shown in Figure 12. The Requirements state that thejob must be matched with an Intel Linux machine which has enough free disk space more than6 megabytes. Out of any machines which meet these requirements, the job prefers a machinewith lots of memory, followed by good floating point performance. Meanwhile, the machine adRequirements states that this machine is not willing to match with any job unless its loadaverage is low and the keyboard has been idle for more than 15 minutes. In other words, it isonly willing to run jobs when it would otherwise sit idle. When it is willing to run a job, theRank expression states it prefers to run jobs submitted by users from its own department.5.1. Combinations of Planning and SchedulingAs we mentioned above, planning and scheduling are related yet independent. Both planningand scheduling can be combined within on system.CondorG, for instance, can perform planning around a schedule. Remote site schedulerscontrol the resources, and once CondorG submits a job into a remote queue, when it willactually run is at the mercy of the remote scheduler see Figure 7. But if the remote schedulerpublishes information about its timetable or workload priorities via a ClassAd to the CondorGCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 17matchmaker, CondorG could make better choices by planning where it should submit jobsif authorized at multiple sites, when it should submit them, andor what types of jobs tosubmit. This approach is used by the PPDG. 6 As more information is published, CondorGcan perform better planning. But even in a complete absence of information from the remotescheduler, CondorG could still perform planning, although the plan may start to resembleshooting in the dark. For example, one such plan could be to submit the job once to eachsite willing to take it, wait and see where it completes first, and then upon completion, deletethe job from the remaining sites.Another combination is scheduling within a plan. Consider as an analogy a large companywhich purchases, in advance, eight seats on a train each week for a year. The company doesnot control the train schedule, so they must plan how to utilize the buses. However, afterpurchasing the tickets, the company is free to decide which employees to send to the trainstation each week. In this manner, Condor performs scheduling within a plan when schedulingparallel jobs on compute clusters. 78 When the matchmaking framework offers a match to anagent and the subsequent claiming protocol is successful, the agent considers itself the ownerof that resource until told otherwise. The agent then creates a schedule for running tasks uponthe resources which it has claimed via planning.5.2. Matchmaking in PracticeMatchmaking emerged over several versions of the Condor software. The initial system used afixed structure for representing both resources and jobs. As the needs of the users developed,these structures went through three major revisions, each introducing more complexity in anattempt to retain backwards compatibility with the old. This finally led to the realizationthat no fixed schema would serve for all time and resulted in the development of a Clike language known as control expressions 17 in 1992. By 1995, the expressions had beengeneralized into classified advertisements or ClassAds. 55 This first implementation is stillused heavily in Condor at the time of this writing. However, it is slowly being replaced by anew implementation 59, 60, 58 which incorporated lessons from language theory and databasesystems.A standalone open source software package for manipulating ClassAds is available in bothJava and C. 71 This package enables the matchmaking framework to be used in otherdistributed computing projects. 76, 27 Several research extensions to matchmaking havebeen built. Gang matching 60, 58 permits the coallocation of more than once resource,such as a license and a machine. Collections provide persistent storage for large numbers ofClassAds with database features such as transactions and indexing. Set matching 10 permitsthe selection and claiming of large numbers of resource using a very compact expressionrepresentation. Named references 72 permit one ClassAd to refer to another and facilitatethe construction of the IO communities mentioned above.In practice, we have found matchmaking with ClassAds to be very powerful. Most resourcemanagement systems allow customers to set provide requirements and preferences on theresources they wish. But the matchmaking frameworks ability to allow resources to imposeconstraints on the customers they wish to service is unique and necessary for preservingdistributed ownership. The clean separation between matchmaking and claiming allows theCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls18 D. THAIN, T. TANNENBAUM, AND M. LIVNYmatchmaker to be blissfully ignorant about the actual mechanics of allocation, permittingit to be a general service which does not have to change when new types of resources orcustomers are added. Because stale information may lead to a bad match, a resource is free torefuse a claim even after it has been matched. Matchmaking is capable of representing wildlydivergent resources, ranging from electron microscopes to storage arrays because resources arefree to describe themselves without a schema. Even with similar resources, organizations trackdifferent data, so no schema promulgated by the Condor software would be sufficient. Finally,the matchmaker is stateless and thus can scale to very large systems without complex failurerecovery.6. Problem SolversSo far, we have delved down into the details of Condor that the user relies on, but may neversee. Let us now move up in the Condor kernel and discuss the environment in which a useractually works.A problem solver is a higherlevel structure built on top of the Condor agent. Two problemsolvers are provided with Condor masterworker and the directed acyclic graph manager. Eachprovides a unique programming model for managing large numbers of jobs. Other problemsolvers are possible and may be built using the public interfaces of the agent.A problem solver relies on a Condor agent in two important ways. A problem solver usesthe agent as a service for reliably executing jobs. It need not worry about the many ways thata job may fail in a distributed system, because the agent assumes all responsibility for hidingand retrying such errors. Thus, a problem solver need only concern itself with the applicationspecific details of ordering and task selection. The agent is also responsible for making theproblem solver itself reliable. To accomplish this, the problem solver is presented as a normalCondor job which simply executes at the submission site. Once started, the problem solvermay then turn around and submit subjobs back to the agent.From the perspective of a user or a problem solver, a Condor agent is identical to a CondorGagent. Thus, any of the structures we describe below may be applied to an ordinary Condorpool or to a widearea grid computing scenario.6.1. MasterWorkerMasterWorker MW is a system for solving a problem of indeterminate size on a large andunreliable workforce. The MW model is wellsuited for problems such as parameter searcheswhere large portions of the problem space may be examined independently, yet the progressof the program is guided by intermediate results.The MW model is shown in Figure 13. One master process directs the computation withthe assistance of as many remote workers as the computing environment can provide. Themaster itself contains three components a work list, a tracking module, and a steering module.The work list is simply a record of all outstanding work the master wishes to be done. Thetracking module accounts for remote worker processes and assigns them uncompleted work.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 19         Worker ProcessesMaster ProcessWork ListSteeringTrackingFigure 13. Structure of a MasterWorker ProgramThe steering module directs the computation by examining results, modifying the work list,and communicating with Condor to obtain a sufficient number of worker processes.Of course, workers are inherently unreliable they disappear when machines crash and theyreappear as new resources become available. If a worker should disappear while holding a workunit, the tracking module simply returns it to the work list. The tracking module may eventake additional steps to replicate or reassign work for greater reliability or simply to speed thecompletion of the last remaining work units.MW is packaged as source code for several C classes. The user must extend the classesto perform the necessary applicationspecific worker processing and master assignment, butall of the necessary communication details are transparent to the user.MW is the result of several generations of software development. James Pruyne first proposedthat applications ought to have an explicit interface for finding resource and placing jobsin a batch system. 55, To facilitate this, such an interface was contributed to the PVMprogramming environment. 57 The first user of this interface was the Work DistributorWoDi, pronounced Woody, which provided a simple interface to a work list processed by alarge number of workers. The WoDi interface was a very highlevel abstraction that presentedno fundamental dependencies on PVM. It was quickly realized that the same functionalitycould be built entirely without PVM. Thus, MW was born. 43 MW provides an interfacesimilar to WoDi, but has several interchangeable implementations. Today, MW can operateby communicating through PVM, through a shared file system, over sockets, or using thestandard universe described below.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls20 D. THAIN, T. TANNENBAUM, AND M. LIVNYABCD Ein.plout.plJOB A a.condorJOB B b.condorJOB C c.condorJOB D d.condorJOB E e.condorPARENT A CHILD B CPARENT C CHILD D ESCRIPT PRE C in.plSCRIPT POST C out.plRETRY C 3Figure 14. A Directed Acyclic Graph6.2. DAGManThe Directed Acyclic Graph Manager DAGMan is a service for executing multiple jobs withdependencies in a declarative form. DAGMan might be thought of as a distributed, faulttolerant version of the traditional tool make. Like its ancestor, it accepts a declaration thatlists the work to be done with constraints on the order. Unlike make, it does not depend onthe file system to record a DAGs progress. Indications of completion may be scattered acrossa distributed system, so DAGMan keeps private logs, allowing it to resume a DAG where itleft off, even in the face of crashes and other failures.Figure 14 demonstrates the language accepted by DAGMan. A JOB statement associatesan abstract name A with a file a.condor that describes a complete Condor job. APARENTCHILD statement describes the relationship between two or more jobs. In this script,jobs B and C are may not run until A has completed, while jobs D and E may not run until Chas completed. Jobs that are independent of each other may run in any order and possiblysimultaneously.In this script, job C is associated with a PRE and a POST program. These commands indicateprograms to be run before and after a job executes. PRE and POST programs are not submitted asCondor jobs, but are run by DAGMan on the submitting machine. PRE programs are generallyused to prepare the execution environment by transferring or uncompressing files, while POSTprograms are generally used to tear down the environment or to evaluate the output of thejob.DAGMan presents an excellent opportunity to study the problem of multilevel errorprocessing. In a complex system that ranges from the highlevel view of DAGs all the wayCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 21down to the minutiae of remote procedure calls, it is essential to tease out the source of anerror to avoid unnecessarily burdening the user with error messages.Jobs may fail because of the nature of the distributed system. Network outages and reclaimedresources may cause Condor to lose contact with a running job. Such failures are not indicationsthat the job itself has failed, but rather that the system has failed. Such situations are detectedand retried by the agent in its responsibility to execute jobs reliably. DAGMan is never awareof such failures.Jobs may also fail of their own accord. A job may produce an ordinary error result if theuser forgets to provide a necessary argument or input file. In this case, DAGMan is aware thatthe job has completed and sees a program result indicating an error. It responds by writingout a rescue DAG and exiting with an error code. The rescue DAG is a new DAG listing theelements of the original DAG left unexecuted. To remedy the situation, the user may examinethe rescue DAG, fix any mistakes in submission, and resubmit it as a normal DAG.Some environmental errors go undetected by the distributed system. For example, acorrupted executable or a dismounted file system should be detected by the distributed systemand retried at the level of the agent. However, if the job was executed via CondorG througha foreign batch system, such detail beyond job failed may not be available, and the job willappear to have failed of its own accord. For these reasons, DAGMan allows the user to specifythat a failed job be retried, using the RETRY command shown in Figure 14.Some errors may be reported in unusual ways. Some applications, upon detecting a corruptenvironment, do not set an appropriate exit code, but simply produce a message on the outputstream and exit with an indication of success. To remedy this, the user may provide a POSTscript that examines the programs output for a valid format. If not found, the POST scriptmay return failure, indicating that the job has failed and triggering a RETRY or the productionof a rescue DAG.7. Split ExecutionThis far, we have explored the techniques needed merely to get a job to an appropriateexecution site. However, that only solves part of the problem. Once placed, a job may find itselfin a hostile environment it may be without the files it needs, it may be behind a firewall, or itmay not even have the necessary user credentials to access its data. Worse yet, few resourcessites are uniform in their hostility. One site may have a users files yet not recognize the user,while another site may have just the opposite situation.No single party can solve this problem because noone has all the information and toolsnecessary to reproduce the users home environment. Only the execution machine knows whatfile systems, networks, and databases may be accessed and how they must be reached. Onlythe submission machine knows at runtime what precise resources the job must actually bedirected to. Nobody knows in advance what names the job may find its resources under, asthis is a function of location, time, and user preference.Cooperation is needed. We call this cooperation split execution. It is accomplished by twodistinct components the shadow and the sandbox. These were mentioned in Figure 2 above.Here we will examine them in detail.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls22 D. THAIN, T. TANNENBAUM, AND M. LIVNYThe shadow represents the user to the system. It is responsible for deciding exactly what thejob must do as it runs. The shadow provides absolutely everything needed to specify the jobat runtime the executable, the arguments, the environment, the input files, and so on. Noneof this is made known outside of the agent until the actual moment of execution. This allowsthe agent to defer placement decisions until the last possible moment. If the agent submitsrequests for resources to several matchmakers, it may award the highest priority job to thefirst resource that becomes available, without breaking any previous commitments.The sandbox is responsible for giving the job a safe place to work. It must ask the shadowfor the jobs details and then create an appropriate environment. The sandbox really has twodistinct components the sand and the box. The sand must make the job feel at home byproviding everything that it needs to run correctly. The box must protect the resource fromany harm that a malicious job might cause. The box has already received much attention41, 61, 15, 63, so we will focus here on describing the sand.Condor provides several universes that create a specific job environment. A universe isdefined by a matched sandbox and shadow, so the development of a new universe necessarilyrequires the deployment of new software modules at both sides. The matchmaking frameworkdescribed above can be used to select resources equipped with the appropriate universe. Here,we will describe the oldest and the newest universes in Condor the standard universe and theJava universe.7.1. The Standard UniverseThe standard universe was the only universe supplied by the earliest versions of Condor andis a descendant of the Remote Unix 46 facility.The goal of the standard universe is to faithfully reproduce the users home Unix environmentfor a single process running at a remote site. The standard universe provides emulation forthe vast majority of standard system calls including file IO, signal routing, and resourcemanagement. Process creation and interprocess communication are not supported. Usersrequiring such features are advised to consider the MPI and PVM universes or the MWproblem solver, all described above.The standard universe also provides checkpointing. This is the ability to take a snapshot ofa running process and place it in stable storage. The snapshot may then be moved to anothersite and the entire process reconstructed and then resumed right from where it left off. Thismay be done to migrate a process from one machine to another, or it may be used to recoverfailed processes and improve throughput in the face of failures.Figure 15 shows all of the components necessary to create the standard universe. At theexecution site, the sandbox is responsible for creating a safe and usable execution environment.It prepares the machine by creating a temporary directory for the job, and then fetches allof the jobs details  the executable, environment, arguments, and so on  and places themin the execute directory. It then invokes the job and is responsible for monitoring its health,protecting it from interference, and destroying it if necessary.At the submission site, the shadow is responsible for representing the user. It provides all ofthe job details for the sandbox and makes all of the necessary policy decisions about the jobCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 23IO ServerShadowLocalSystemCallsForkSandboxC LibraryCondorThe JobJob SetupSecure RPCSecure RPCJobs  IOHomeFileSystemFigure 15. The Standard Universeas it runs. In addition, it provides an IO service accessible over a secure RPC channel. Thisprovides remote access to the users home storage device.To communicate with the shadow, the users job must be relinked with a special libraryprovided by Condor. This library has the same interface as the standard C library, so nochanges to the users code are necessary. The library converts all of the jobs standard systemcalls into secure remote procedure calls back to the shadow. It is also capable of convertingIO operations into a variety of remote access protocols such as HTTP and NeST 14, Inaddition, it may apply a number of other transformations, such as buffering, compression, andspeculative IO.It is vital to note that the shadow remains in control of the entire operation. Althoughboth the sandbox and the Condor library are equipped with powerful mechanisms, neither isauthorized to make decisions without the shadows consent. This maximizes the flexibility ofthe user to make runtime decisions about exactly what runs where and when.An example of this principle is the twophase open. Neither the sandbox nor the library ispermitted to simply open a file by name. Instead, they must first issue a request to map alogical file name the applications argument to open into a physical file name. The physicalfile name is similar to a URL and describes the actual file name to be used, the method bywhich to access it, and any transformations to be applied.Figure 16 demonstrates twophase open. Here the application requests a file named alpha.The library asks the shadow how the file should be accessed. The shadow responds that theCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls24 D. THAIN, T. TANNENBAUM, AND M. LIVNYIO ServerShadowThe JobC LibraryCondor3 compressremotedatanewalpha.gz2 Where is file alpha 4 Open datanewalpha.gz5 Success1 Open alpha6 SuccessFigure 16. TwoPhase Open Using the ShadowThe JobStorageApplianceNeSTIO ServerShadowC LibraryCondor3 nestnest.wisc.edubeta2 Where is file beta  4 Open beta5 Success6 Success1 Open betaFigure 17. TwoPhase Open Using a NestCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 25IO ServerShadowThe JobIO LibraryWrapperIO ProxySandboxLocalSystemCallsForkLocalRPCChirpJVMJob Setup and IOSecure RPCFileSystemHomeFigure 18. The Java Universefile is available using remote procedure calls, but is compressed and under a different name.the library then issues an open to access the file.Another example is given in Figure 17. Here the application requests a file named beta.The library asks the shadow how the file should be accessed. The shadow responds that thefile is available using the NeST protocol on a server named nest.wisc.edu. The library thencontacts that server and indicates success to the users job.The mechanics of checkpointing and remote system calls in Condor are described in greatdetail by Litzkow et al 64, 45.7.2. The Java UniverseA universe for Java programs was added to Condor in late 2001. This was due to a growingcommunity of scientific users that wished to perform simulations and other work in Java.Although such programs might run slower than native code, such losses were offset by fasterdevelopment times and access to larger numbers of machines. By targeting applications to theJava Virtual Machine JVM, users could avoid dealing with the timeconsuming details ofspecific computing systems.Previously, users had run Java programs in Condor by submitting an entire JVM binary as astandard universe job. Although this worked, it was inefficient on two counts the JVM binarycould only run on one type of CPU, which defied the whole point of a universal instructionCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls26 D. THAIN, T. TANNENBAUM, AND M. LIVNYset and the repeated transfer of the JVM and the standard libraries was a waste of resourceson static data.A new Java universe was developed which would raise the level of abstraction to create acomplete Java environment rather than a Unix environment. The components of the new Javauniverse are shown in Figure 18. The responsibilities of each component are the same as otheruniverses, but the functionality changes to accommodate the unique features of Java.The sandbox is responsible for creating a safe and comfortable execution environment. Itmust ask the shadow for all of the jobs details, just as in the standard universe. However, thelocation of the JVM is provided by the local administrator, as this may change from machineto machine. In addition, a Java program consists of a variety of runtime components, includingclass files, archive files, and standard libraries. The sandbox must place all of these componentsin a private execution directory along with the users credentials and start the JVM accordingto the local details.The IO mechanism is somewhat more complicated in the Java universe. The job is linkedagainst a Java IO library that presents remote IO in terms of standard interfaces such asInputStream and OutputStream. This library does not communicate directly with any storagedevice, but instead calls an IO proxy managed by the sandbox. This unencrypted connectionis secure by making use of the loopback network interface and presenting a shared secret. Thesandbox then executes the jobs IO requests along the secure RPC channel to the shadow,using all of the same security mechanisms and techniques as in the standard universe.Initially, we chose this IO mechanism so as to avoid reimplementing all of the IO andsecurity features in Java and suffering the attendant maintenance work. However, there areseveral advantages of the IO proxy over the more direct route used by the standard universe.The proxy allows the sandbox to pass through obstacles that the job does not know about.For example, if a firewall lies between the execution site and the jobs storage, the sandboxmay use its knowledge of the firewall to authenticate and pass through. Likewise, the user mayprovide credentials for the sandbox to use on behalf of the job without rewriting the job tomake use of them.The Java universe is sensitive to a wider variety of errors than most distributed computingenvironments. In addition to all of the usual failures that plague remote execution, the Javaenvironment is notoriously sensitive to installation problems, and many jobs and sites areunable to find runtime components, whether they are shared libraries, Java classes, or theJVM itself. Unfortunately, many of these environmental errors are presented to the job itselfas ordinary exceptions, rather than expressed to the sandbox as an environmental failure. Tocombat this problem, a small Java wrapper program is used to execute the users job indirectlyand analyze the meaning of any errors in the execution. A complete discussion of this problemand its solution may be found in 74.8. DataIntensive ComputingCondor was initially conceived as a manager of computing resources, with some incidentalfacilities for accessing small amounts of data. However, the data needs of batch computingusers have grown dramatically in the last two decades. Many forms of science are increasinglyCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 27NestNestNestMakerMatchStorkDAGManParrot JobDiskRoutersFigure 19. DataIntensive Computing Toolscentered around large data sets such as image libraries, biological genomes, or simulationresults. In the last several years, the Condor project has built a variety of tools for managingand accessing data within a batch system. These tools, shown in Figure 19 are naturallyanalogous to the Condor kernel described earlier. Because these tools are relative newcomersto the project, we will give a highlevel overview of their purpose and role, and direct thereader to more detailed publications.The basic resource to be managed in a dataintensive system is storage. A resource managercalled Nest supervises a storage device, periodically advertising itself to a matchmaker so thatan external user may discover and harness it. Once discovered, Nestmanaged storage maybe allocated for a given amount of time. A user may then access this allocated space using avariety of standard protocols such as FTP and HTTP. 14The transfer of data to a remote Nest may be thought of as a batch job much like a programto be run a transfer must be named, queued, scheduled, logged, and perhaps retried if itfails. A user with many transfers to carry out as part of a batch workload may queue suchtransfers with a Stork. Like the agent process in standard Condor, a Stork negotiates witha matchmaker, communicates with remote storage devices, and generally makes large datatransfers reliable. A Stork speaks many standard protocols such as FTP, HTTP, and Nest,and thus may arrange transfers between many types of devices. The choice of a protocol isdynamically chosen to maximize throughput. 37Very large data transfers over the wide area are often adversely affected by fluctuating localnetwork congestion. To smooth out large transfers, a series of DiskRouters can serve as innetwork buffering space for transfers between storage devices. When arranging a transfer, aStork may arrange for one or more DiskRouters to smooth out local bandwidth fluctuationsand thus achieve higher endtoend bandwidth. 36In this heterogeneous environment, a jobs data may be stored in a wide variety of devices,each with their own peculiar naming scheme and access protocol. Parrot connects ordinaryprograms to these unusual devices. Parrot uses the debugger interface to trap a programsCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls28 D. THAIN, T. TANNENBAUM, AND M. LIVNYsystem calls and convert them into operations on remote storage devices, which simply appearas entries in the filesystem. In this way, ordinary programs and scripts can be deployed intothis system without any changes. As with the other components, a variety of protocols aresupported. 75Of course, the Condor data components must work in concert with the computationcomponents. Coordination can be achieved at a high level by using the DAGMan problemsolver. In the same manner that DAGMan can dispatch ready jobs to a Condor agent, itcan also dispatch data placement requests to a Stork. In this manner, an entire DAG can beconstructed which stages data to a remote site, runs a series of jobs, retrieves the output, andcleans up the space.The project is only just beginning to develop experience with deploying and using these tools.Management of data is a large and multifaceted problem that we will continue to explore inthe future.9. SecurityCondor shares many of the same security challenges as other distributed computingenvironments, and the notion of splitexecution adds several more. The security mechanismsin Condor strive to provide flexible secure communication, protect resource owners from errantor malicious jobs, and protect Condor users from errant or malicious resources.9.1. Secure CommunicationSecure communication in Condor is handled by CEDAR, a messagebased communicationlibrary developed by the Condor Project. This library allows clients and servers to negotiateand use a variety of security protocols.This ability is critical because Condor is deployed atmany different sites, each with their own local security mechanism and policy. CEDAR permitsauthentication via Kerberos 65, GSI 28, 3DES 52, Blowfish 62, and Microsofts SSPI 49.For sites that are secured by an external device such as a firewall, CEDAR also permits simpleauthentication via trusted hosts or networks. CEDAR bears a similarity to SASL 51, butalso supports connectionbased and connectionless datagram communications, as well as theability to negotiate data integrity and privacy algorithms separately from the authenticationprotocol.Although currently Condors secure communication model is based upon a secure connectionestablished via CEDAR, we are currently investigating messagebased security models viadigital signatures of ClassAd attributes. With messagebased security, authenticity is attachedto the payload itself, instead of being attached to the communication channel. A clearadvantage of messagebased security is that any information exchanged continues to be secureafter the network channel has been torn down, permitting data transferred to be storedpersistently to disk or transferred via multiple connections and still remain authentic.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 299.2. Secure ExecutionIn general, Condor assumes that a fair amount of trust exists between machine owners andusers that wish to run jobs. These two parties typically have some relationship, whether theyare employees of the same organization or have agreed to collaborate on a specific project.Condor allows owners and users to specify what parties are trusted, but in general, a user musthave some external reason to believe that a machine will execute the computations that havebeen requested. That said, Condor provides mechanisms that protect both machine ownersand users from runaway, crashing, and in some cases, malicious parties. Machine owners areprotected by the sandbox at the execution site, while users are protected by the shadow at thesubmission site.The sandbox at the execution site prevents a job from accessing resources that it shouldnot. The sandbox has evolved over several years of Condor, in reaction to tensions betweenusability and security. Early versions of Condor restricted running jobs to a limited portionof the filesystem using the Unix chroot feature. This technique was abandoned when dynamiclinking became prevalent, because it made it quite difficult for users to compose jobs that couldexecute correctly without library support. Today, jobs are run in an unrestricted filesystem,but are given a restricted login account. Filesystem access control lists can then be used torestrict access by remote jobs.The selection of a restricted account is a tricky problem in itself. In Unix, one may use thestandard nobody account to run a job with few privileges. However, if a multiCPU machinewere to run multiple jobs at once, they would be able to interfere with each other by virtueof having the same nobody user ID. A malicious Condor user could submit jobs that couldhijack another users jobs. This exploit, and ones similar to it, are described by Miller, etal. 50 To prevent this, Condor now dynamically allocates user IDs for each running job. OneUnix machines, this requires the administrator to manually set aside certain IDs. On Windowsmachines, Condor can automatically allocate users on the fly.Many Condor installations operate in a cluster or workgroup environment where a commonuser database is shared among a large number of machines. In these environments, users maywish to have their jobs run with normal credentials so that they may access existing distributedfilesystems and other resources. To allow this behavior, Condor has the notion of a user IDdomain a set of machines known to share the same user database. When running a job,Condor checks to see if the submitting and executing machine are members of the same userID domain. If so, the job runs with the submitters user ID. If not, the job is sandboxed asabove.As we develop new mechanisms to permit splitexecution without requiring a relink 75, 73,we hope to return to the days of only running jobs via a dynamic nobody account. Having aunique account specific to the launch of one job is beneficial for both for ease of administrationand for cleanup. Cleanup of the processes left behind for a given job is easy with the dynamicnobody model  just kill all processes owned by the nobody user. Conversely, cleanup isproblematic when running the job as a real user. Condor cannot just go and kill all processesowned by a given user, because they may not have all been launched by Condor. Sadly, Unixprovides no facility for reliably cleaning up a process tree. The setsid feature is only a voluntaryCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls30 D. THAIN, T. TANNENBAUM, AND M. LIVNYgrouping. Note that starting with Windows 2000, Microsoft leaped ahead of Unix in this regardby providing a reliable way to track all the descendants of a process.10. Case StudiesToday, Condor is used by organizations around the world. Three brief case studies presentedbelow provide a glimpse of the many ways that Condor is deployed in industry and the academy.10.1. C.O.R.E. Digital PicturesC.O.R.E. Digital Pictures is a highly successful Torontobased computer animation studio, cofounded in 1994 by William Shatner and four talented animators. Photorealistic animation,especially for cuttingedge film special effects, is a compute intensive process. Each frame cantake up to an hour, and one second of animation can require 30 or more frames.Today, Condor manages a pool at C.O.R.E. consisting of hundreds of Linux and SiliconGraphics machines. The Linux machines are all dualCPU and mostly reside on the desktopsof the animators. By taking advantage of Condor ClassAds and native support for multiprocessor machines, one CPU is dedicated to running Condor jobs while the second CPU onlyruns jobs when the machine is not being used interactively by its owner.Each animator has a Condor agent on the desktop. On a busy day, C.O.R.E. animatorssubmit over 15,000 jobs to Condor. C.O.R.E. developers created a session metascheduler thatinterfaces with Condor in a manner similar to the DAGMan service previously described. Whenan animator hits the render button, a new session is created and the custom metascheduleris submitted as a job into Condor. The metascheduler translates this session into a series ofrendering jobs which it subsequently submits to Condor.C.O.R.E. makes considerable use of the schemafree properties of ClassAds by insertingcustom attributes into the job ClassAd. These attributes allow Condor to make planningdecisions based upon realtime input from production managers, who can tag a project, ora shot, or individual animator with a priority. When jobs are preempted due to changingpriorities, Condor will preempt jobs in such a way that minimizes the loss of forward progressas defined by C.O.R.E.s policy expressions.Condor has been used by C.O.R.E. for many major productions such as XMen, Blade II,Nutty Professor II, and The Time Machine.10.2. Micron Technology, Inc.Micron Technology, Inc., is a worldwide provider of semiconductors. Significant computationalanalysis is required to tightly control all steps of the engineering process, enabling Micronto achieve short cycle times, high yields, low production costs, and die sizes that are someof the smallest in the industry. Before Condor, Micron had to purchase dedicated computeresources to meet peak demand for engineering analysis tasks. Condors ability to consolidateidle compute resources across the enterprise offered Micron the opportunity to meet itsengineering needs without incurring the cost associated with traditional, dedicated computeCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 31Table I. NUG30 Computation Statistics. Part A lists how many CPUs were utilizedat different locations on the grid during the seven day NUG30 run. Part B lists otherinteresting statistics about the run.Part ANumber Architecture Location1024 SGIIrix NCSA414 IntelLinux Argonne246 IntelLinux U. of Wisconsin190 IntelLinux Georgia Tech146 IntelSolaris U. of Wisconsin133 SunSolaris U. of Wisconsin96 SGIIrix Argonne94 IntelSolaris Georgia Tech54 IntelLinux Italy INFN45 SGIIrix NCSA25 IntelLinux U. of New Mexico16 IntelLinux NCSA12 SunSolaris Northwestern U.10 SunSolaris Columbia U.5 IntelLinux Columbia U.Part BTotal number of CPUsutilized2510Average number of simultaneous CPUs652.7Maximum number of simultaneous CPUs1009Running wall clock timesec597,872Total CPU time consumed sec346,640,860Number of times a machine joined the computation19,063Equivalent CPU timesec on an HP C3000workstation218,823,577resources. So far, Micron has setup two primary Condor pools that contain a mixture of desktopmachines and dedicated compute servers. Condor manages the processing of tens of thousandsof engineering analysis jobs per week. Micron engineers report that the analysis jobs run fasterand require less maintenance.10.3. NUG30 Optimization ChallengeIn the summer of year 2000, four mathematicians from Argonne National Laboratory,University of Iowa, and Northwestern University used CondorG and several other technologiesdiscussed in this document to be the first to solve a problem known as NUG30. 11 NUG30is a quadratic assignment problem that was first proposed in 1968 as one of the most difficultcombinatorial optimization challenges, but remained unsolved for 32 years because of itscomplexity.In order to solve NUG30, the mathematicians started with a sequential solver based upona branchandbound tree search technique. Although the sophistication level of the solverwas enough to drastically reduce the amount of compute time it would take to determinea solution, the amount of time was still considerable. To combat this computation hurdle, aparallel implementation of the solver was developed which fit the masterworker model. Theactual computation itself was managed by Condors MasterWorker MW problem solvingenvironment. MW submitted work to CondorG, which provided compute resources fromaround the world by both direct flocking to other Condor pools and by gliding in to othercompute resources accessible via the Globus GRAM protocol. Remote System Calls, part ofCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls32 D. THAIN, T. TANNENBAUM, AND M. LIVNYCondors standard universe, was used as the IO service between the master and the workers.Checkpointing was performed every 15 minutes for fault tolerance. All of these technologieswere introduced earlier in this paper.The end result a solution to NUG30 was discovered utilizing CondorG in a computationalrun of less than one week. During this week, over 95,000 CPU hours were used to solve theover 540 billion linear assignment problems necessary to crack NUG30. CondorG allowed themathematicians to harness over 2500 CPUs at ten different sites eight Condor pools, onecompute cluster managed by PBS, and one supercomputer managed by LSF spanning eightdifferent institutions. Additional statistics about the NUG30 run are presented in Table I.11. ConclusionThrough its lifetime, the Condor software has grown in power and flexibility. As other systemssuch as Kerberos, PVM, and Java have reached maturity and widespread deployment, Condorhas adjusted to accommodate the needs of users and administrators without sacrificing itsessential design. In fact, the Condor kernel shown in Figure 2 has not changed at all since1988. Why is thisWe believe the key to lasting system design is to outline structures first in terms ofresponsibility rather than expected functionality. This may lead to interactions which, at firstblush, seem complex. Consider, for example, the four steps to matchmaking shown in Figure 11or the six steps to accessing a file shown in Figures 16 and 17. Every step is necessary fordischarging a components responsibility. The apparent complexity preserves the independenceof each component. We may update one with more complex policies and mechanisms withoutharming another.The Condor project will also continue to grow. The project is home to a variety of systemsresearch ventures in addition to the flagship Condor software, such as the ClassAd 58resource management language, the Hawkeye 4 cluster management system, the NeST storageappliance 14. and the Public Key Infrastructure Lab. 7 In these and other ventures, theproject seeks to gain the hard but valuable experience of nurturing research concepts intoproduction software. To this end, the project is a key player in collaborations such as theNational Middleware Initiative 5 that aim to harden and disseminate research systems asstable tools for end users. The project will continue to train students, solve hard problems,and accept and integrate good solutions from others.We look forward to the challenges aheadACKNOWLEDGEMENTSWe would like to acknowledge all of the people who have contributed to the development of theCondor system over the years. They are too many to list here, but include faculty and staff graduatesand undergraduates visitors and residents. However, we must particularly recognize the first corearchitect of Condor, Mike Litzkow, whose guidance through example and advice has deeply influencedthe Condor software and philosophy.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 33We are also grateful to Brooklin Gore and Doug Warner at Micron Technology, and to Mark Visserat C.O.R.E. Digital Pictures for their Condor enthusiasm and for sharing their experiences with us.Jamie Frey, Mike Litzkow, and Alain Roy provided sound advice as this chapter was written.The Condor project is supported by a wide variety of funding sources. In addition, Douglas Thainis supported in part by a Cisco distinguished graduate fellowship and a Lawrence Landweber NCRfellowship in distributed systems.REFERENCES1. European Union DataGrid Project. httpwww.eudatagrid.org.2. The Compact Muon Solenoid Collaboration. httpuscms.fnal.gov, August 2002.3. The Grid Physics Network GriPhyN. httpwww.griphyn.org, August 2002.4. HawkEye. httpwww.cs.wisc.educondorhawkeye, August 2002.5. NSF Middleware Initiative NMI. httpwww.nsfmiddleware.org, August 2002.6. Particle Physics Data Grid PPDG. httpwww.ppdg.net, August 2002.7. Public Key Infrastructure Lab PKILab. httpwww.cs.wisc.edupkilab, August 2002.8. William Allcock, Ann Chervenak, Ian Foster, Carl Kesselman, and Steve Tuecke. Protocols and servicesfor distributed dataintensive science. In Proceedings of Advanced Computing and Analysis Techniquesin Physics Research ACAT, pages 161163, 2000.9. D. Anderson, S. Bowyer, J. Cobb, D. Gedye, W.T. Sullivan, and D. Werthimer. Astronomical andbiochemical origins and the search for life in the universe. In Proceedings of the 5th InternationalConference on Bioastronomy. Editrice Compositori, Bologna, Italy, 1997.10. D. Angulo, I. Foster, C. Liu, , and L. Yang. Design and evaluation of a resource selection frameworkfor grid applications. In Proceedings of the 11th IEEE Symposium on High Performance DistributedComputing HPDC11, Edinburgh, Scotland, July 2002.11. K. Anstreicher, N. Brixius, J.P. Goux, and J. Linderoth. Solving large quadratic assignment problemson computational grids. In Mathematical Programming, 2000.12. Jim Basney and Miron Livny. Deploying a high throughput computing cluster. In Rajkumar Buyya,editor, High Performance Cluster Computing Architectures and Systems, Volume 1. Prentice Hall PTR,1999.13. Jim Basney, Miron Livny, and Paolo Mazzanti. Utilizing widely distributed computational resourcesefficiently with execution domains. Computer Physics Communications, 2001.14. John Bent, Venkateshwaran Venkataramani, Nick LeRoy, Alain Roy, Joseph Stanley, Andrea ArpaciDusseau, Remzi ArpaciDusseau, and Miron Livny. Flexibility, manageability, and performance in a gridstorage appliance. In Proceedings of the Eleventh IEEE Symposium on High Performance DistributedComputing, Edinburgh, Scotland, July 2002.15. B.N. Bershad, S. Savage, P.Pardyak, E.G. Sirer, M. Fiuchynski, D. Becker, S. Eggers, and C. Chambers.Extensibility, safety, and performance in the SPIN operating system. In Proceedings of the 15th ACMSymposium on Operating Systems Principles, pages 251266, December 1995.16. Andrew D. Birrell, Roy Levin, Roger M. Needham, and Michael D. Schroeder. Grapevine An exercise indistributed computing. Communications of the ACM, 254260274, April 1982.17. Alan Bricker, Mike Litzkow, and Miron Livny. Condor technical summary. Technical Report 1069,University of Wisconsin, Computer Sciences Department, January 1992.18. R.M. Bryant and R.A. Finkle. A stable distributed scheduling algorithm. In Proceedings of the SecondInternational Conference on Distributed Computing Systems, pages 314323, Paris, France, April 1981.19. K. Chandy and L. Lamport. Distributed snapshots Determining global states of distributed systems.ACM Transactions on Computer Systems, 316375, 1985.20. Chungmin Chen, Kenneth Salem, and Miron Livny. The DBC Processing scientific data over the internet.In 16th International Conference on Distributed Computing Systems, May 1996.21. Y.C. Chow and W.H. Kohler. Dynamic load balancing in homogeneous twoprocessor distributed systems.In Proceedings of the International Symposium on Computer Performance, Modeling, Measurement andEvaluation, pages 3952, Yorktown Heights, New York, August 1977.22. I.B.M. Corporation. IBM Load Leveler Users Guide. I.B.M. Corporation, September 1993.23. K. Czajkowski, I. Foster, N. Karonis, C. Kesselman, S. Martin, W. Smith, and S. Tuecke. A resourcemanagement architecture for metacomputing systems. In Proceedings of the IPPSSPDP Workshop onJob Scheduling Strategies for Parallel Processing, pages 6282, 1988.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls34 D. THAIN, T. TANNENBAUM, AND M. LIVNY24. David DeWitt, Raphael Finkel, and Marvin Solomon. The CRYSTAL multicomputer Design andimplementation experience. IEEE Transactions on Software Engineering, September 1984.25. P.H. Enslow. What is a distributed data processing system Computer, 1111321, January 1978.26. D.H.J. Epema, M. Livny, R. van Dantzig, X. Evers, and J. Pruyne. A worldwide flock of Condors Loadsharing among workstation clusters. Future Generation Computer Systems, 125365, 1996.27. C. Anglano et al. Integrating Grid tools to build a Computing Resource Broker Activities of DataGridWP1. In Proceedings of the Conference on Computing in High Energy Physics 2001 CHEP01, Beijing,September 2001.28. I. Foster, C. Kesselman, G. Tsudik, and S. Tuecke. A security architecture for computational grids. InProceedings of the 5th ACM Conference on Computer and Communications Security Conference, pages8392, 1998.29. Ian Foster and Carl Kesselman. Globus A metacomputing intrastructure toolkit. International Journalof Supercomputer Applications, 112115128, 1997.30. Ian Foster and Carl Kesselman, editors. The Grid Blueprint for a New Computing Infrastructure. MorganKaufmann, 1998.31. James Frey, Todd Tannenbaum, Ian Foster, Miron Livny, and Steve Tuecke. CondorG A computationmanagement agent for multiinstitutional grids. In Proceedings of the Tenth IEEE Symposium on HighPerformance Distributed Computing HPDC, pages 79, San Francisco, California, August 2001.32. James Frey, Todd Tannenbaum, Ian Foster, Miron Livny, and Steve Tuecke. CondorG A computationmanagement agent for multiinstitutional grids. Cluster Computing, 5237246, 2002.33. R. Henderson and D. Tweten. Portable batch system External reference specification. Technical report,NASA, Ames Research Center, 1996.34. Cray Inc. Introducing NQE. Technical Report 2153 2.97, Cray Inc., Seattle, WA, February 1997.35. D. Jackson, Q. Snell, and M. Clement. Core algorithms of the maui scheduler. In Proceedings of the 7thWorkshop on Job Scheduling Strategies for Parallel Processing, 2001.36. Tevfik Kosar, George Kola, and Miron Livny. A framework for selfoptimising, faulttolerant, highperformance bulk data transfers in a heterogeneous grid environment. In Proceedings of the 2ndInternational Symposium on Parallel and Distributed Computing, Ljubljana, Slovenia, October 2003.37. Tevfik Kosar and Miron Livny. Stork Making data placement a first class citizen in the grid. In Porceedingsof the 24th IEEE International Conference on Distributed Computing Systems, Tokyo, Japan, March 2004.38. Phillip E. Krueger. Distributed scheduling for a changing environment. Technical Report UWCSTR780,University of Wisconsin  Madison Computer Sciences Department, June 1988.39. L. Lamport, R. Shostak, and M. Pease. The byzantine generals problem. ACM Transactions onProgramming Languages and Systems, 43382402, July 1982.40. Leslie Lamport. Time, clocks, and the ordering of events in a distributed system. Communications ofthe ACM, 721558565, July 1978.41. Butler Lampson. Protection. In Proceedings of the 5th Princeton Symposium on Information Sciencesand Systems, Princeton University, March 1971.42. Hugh C. Lauer. Observations on the development of an operating system. In Proceedings of the 8thSymposium on Operating Systems Principles SOSP, pages 3036, 1981.43. Jeff Linderoth, Sanjeev Kulkarni, JeanPierre Goux, and Michael Yoder. An enabling framework formasterworker applications on the computational grid. In Proceedings of the Ninth IEEE Symposiumon High Performance Distributed Computing HPDC9, pages 4350, Pittsburgh, Pennsylvania, August2000.44. Michael Litzkow and Miron Livny. Experience with the Condor distributed batch system. In Proceedingsof the IEEE Workshop on Experimental Distributed Systems, October 1990.45. Michael Litzkow, Todd Tannenbaum, Jim Basney, and Miron Livny. Checkpoint and migration of UNIXprocesses in the Condor distributed processing system. Technical Report UWCSTR1346, University ofWisconsin  Madison Computer Sciences Department, April 1997.46. Michael J. Litzkow. Remote UNIX  Turning idle workstations into cycle servers. In Proceedings ofUSENIX, pages 381384, Summer 1987.47. Miron Livny. The Study of Load Balancing Algorithms for Decentralized Distributed Processing Systems.PhD thesis, Weizmann Institute of Science, 1983.48. Miron Livny and Rajesh Raman. Highthroughput resource management. In Ian Foster and CarlKesselman, editors, The Grid Blueprint for a New Computing Infrastructure. Morgan Kaufmann, 1998.49. Microsoft Corp. The security support provider interface white paper.50. Barton P. Miller, Mihai Christodorescu, Robert Iverson, Tevfik Kosar, Alexander Mirgordskii, andFlorentina Popovici. Playing inside the black box Using dynamic instrumentation to create securityCopyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.clsDISTRIBUTED COMPUTING IN PRACTICE THE CONDOR EXPERIENCE 35holes. Parallel Processing Letters, 1123, JuneSeptember 2001.51. J. Myers. RFC2222 Simple Authentication and Security Layer SASL. Network Working Group Requestfor Comments, October 1997.52. National Bureau of Standards. Data Encryption Standard. U. S. Department of Commerce, Washington,DC, USA, January 1977.53. R. M. Needham. Systems aspects of the cambridge ring. In Proceedings of the seventh symposium onOperating systems principles, pages 8285, 1979.54. E.I. Organick. The MULTICS system An examination of its structure. The MIT Press, Cambridge,Massachusetts and London, England, 1972.55. James C. Pruyne. Resource Management Services for Parallel Applications. PhD thesis, University ofWisconsin, 1996.56. Jim Pruyne and Miron Livny. Providing resource management services to parallel applications. InProceedings of the Second Workshop on Environments and Tools for Parallel Scientific Computing, May1994.57. Jim Pruyne and Miron Livny. Interfacing Condor and PVM to harness the cycles of workstation clusters.Future Generation Computer Systems, 126786, May 1996.58. Rajesh Raman. Matchmaking Frameworks for Distributed Resource Management. PhD thesis, Universityof Wisconsin, October 2000.59. Rajesh Raman, Miron Livny, and Marvin Solomon. Matchmaking Distributed resource managementfor high throughput computing. In Proceedings of the Seventh IEEE International Symposium on HighPerformance Distributed Computing HPDC7, July 1998.60. Rajesh Raman, Miron Livny, and Marvin Solomon. Resource management through multilateralmatchmaking. In Proceedings of the Ninth IEEE Symposium on High Performance Distributed ComputingHPDC9, pages 290291, Pittsburgh, PA, August 2000.61. J.H. Saltzer and M.D. Schroeder. The protection of information in computer systems. Proceedings of theIEEE, 63912781308, September 1975.62. Bruce Schneier. The Blowfish encryption algorithm. Dr. Dobbs Journal of Software Tools, 19438, 40,98, 99, April 1994.63. M.I. Seltzer, Y. Endo, C. Small, and K. A. Smith. Dealing with disaster Surviving misbehavedkernel extensions. In Proceedings of the 2nd USENIX Symposium on Operating Systems Design andImplementation OSDI, pages 213227, October 1996.64. Marvin Solomon and Michael Litzkow. Supporting checkpointing and process migration outside the UNIXkernel. In Proceedings of USENIX, pages 283290, Winter 1992.65. J.G. Steiner, C. Neuman, and J. I. Schiller. Kerberos An authentication service for open network systems.In Proceedings of USENIX, pages 191200, Winter 1988.66. T. Sterling, D. Savarese, D. J. Becker, J. E. Dorband, U. A. Ranawake, and C. V. Packer. BEOWULFA parallel workstation for scientific computation. In Proceedings of the 24th International Conference onParallel Processing, pages 1114, Oconomowoc, Wisconsin, 1995.67. Richard Stern. Micro law Napster A walking copyright infringement IEEE Micro, 20645,NovemberDecember 2000.68. H.S. Stone. Multiprocessor scheduling with the aid of network flow algorithms. IEEE Trans of SoftwareEngineering, SE319593, January 1977.69. Todd Tannenbaum, Derek Wright, Karen Miller, and Miron Livny. Condor  A distributed job scheduler.In Thomas Sterling, editor, Beowulf Cluster Computing with Linux. MIT Press, October 2001.70. Todd Tannenbaum, Derek Wright, Karen Miller, and Miron Livny. Condor  A distributed job scheduler.In Thomas Sterling, editor, Beowulf Cluster Computing with Windows. MIT Press, October 2001.71. Condor Team. Condor Manual. Available from httpwww.cs.wisc.educondormanual, 2001.72. Douglas Thain, John Bent, Andrea ArpaciDusseau, Remzi ArpaciDusseau, and Miron Livny. Gatheringat the well Creating communities for grid IO. In Proceedings of Supercomputing 2001, Denver, Colorado,November 2001.73. Douglas Thain and Miron Livny. Bypass A tool for building split execution systems. In Proceedings of theNinth IEEE Symposium on High Performance Distributed Computing HPDC9, pages 7985, Pittsburg,PA, August 2000.74. Douglas Thain and Miron Livny. Error scope on a computational grid Theory and practice. In Proceedingsof the 11th IEEE Symposium on High Performance Distributed Computing HPDC, July 2002.75. Douglas Thain and Miron Livny. Parrot Transparent userlevel middleware for dataintensive computing.In Proceedings of the Workshop on Adaptive Grid Middleware, New Orleans, September 2003.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls36 D. THAIN, T. TANNENBAUM, AND M. LIVNY76. Sudharshan Vazhkudai, Steven Tuecke, and Ian Foster. Replica selection in the globus data grid. IEEEInternational Symposium on Cluster Computing and the Grid CCGrid, pages 106113, May 2001.77. Bruch Walker, Gerald Popek, Robert English, Charles Kline, and Greg Thiel. The LOCUS distributedoperating system. In Proceedings of the 9th Symposium on Operating Systems Principles SOSP, pages4970, November 1983.78. Derek Wright. Cheap cycles from the desktop to the dedicated cluster combining opportunisitc anddedicated scheduling with Condor. In Conference on Linux Clusters The HPC Revolution, ChampaignUrbana, Illinois, June 2001.79. S. Zhou. LSF Load sharing in largescale heterogenous distributed systems. In Proc. Workshop onCluster Computing, 1992.Copyright c 2004 John Wiley  Sons, Ltd. Concurrency Pract. Exper. 2004 0020Prepared using cpeauth.cls
