OpenMP for Clusters  Lei Huang, Barbara Chapman, Ricky Kendall Dept. of Computer Science , University of Houston  Texas Scalable Computing Laboratory, Ames Laboratory, Iowa leihuang, chapmancs.uh.edu, rickykameslab.gov Abstract. This paper presents a sourcetosource translation strategy from OpenMP to Global Arrays in order to extend OpenMP to distributed memory systems. This translation provides a simple approach for programmers to write parallel programs using a highlevel API that will run on both shared memory and distributed memory systems. Our benchmark experiments show scalability and lead us to believe that this approach is more promising than the use of software DSM systems.  1 Introduction Parallel computer architectures in broad use include Shared Memory Systems SMSs, Distributed Memory systems DMSs, and Distributed Shared Memory systems DSMs. Clusters and DMSs are increasingly popular because of their good priceperformance ratio. However, most programs written for them are SPMD Single Program Multiple Data programs using the explicit parallel interface MPI MessagePassing Interface for communication and synchronization of processes. But MPI requires advanced programming skills, is error prone, and is too complex for some classes of users. Message passing programming models often lead to multiple versions or a complex set of parameters that users must set to get truly portable performance.   Global Arrays GA 10 was designed to simplify the programming methodology on distributed memory systems. It provides a portable interface via which processes in an SPMDstyle parallel program dont need the explicit cooperation of other processes. The most innovative idea of GA is that it provides an asynchronous onesided, sharedmemory programming environment for distributed memory systems. In contrast to other popular approaches, it does so by providing a library of routines that enable the user to specify and manage access to shared data structures in a program. GA reduces the effort required to write parallel program for clusters since they can                                                             This work was partially supported by the DOE under contract DEFC0301ER25502 and by the Los Alamos National Laboratory Computer Science Institute LACSI through LANL contract number 038919923. This work was performed, in part, under the auspices of the U.S. Department of Energy USDOE under contract W7405ENG82 at Ames Laboratory, operated by Iowa State University of Science and Technology and funded by the MICS Division of the Office in Advanced Scientific Computing Research at USDOE. assume a virtual shared memory. Part of the task of the user is to explicitly define the physical data locality for the virtual shared memory and the appropriate access patterns of the parallel algorithm.  OpenMP 5 has emerged as a popular parallel programming interface for medium scale high performance applications on SMSs. Strong points are its ability to support incremental parallelization, portability, and ease of use. The OpenMP programmer inserts parallel directives and does not need to change the control flow of the corresponding sequential program, which dramatically reduces the effort of adapting a previously sequential program to parallel form. Although OpenMP is attractive for parallel programming in general, it cannot be used directly on a DMS.  In this paper, we show how Global Arrays may be used to implement OpenMP on clusters. We explain why we believe that this is worthwhile in the next section below, outline the translation from OpenMP to GA in Section 3 and give some benchmarks to show the potential performance of applications translated in this manner on a variety of current platforms. We conclude by briefly discussing related work and future plans. 2 Motivation Compared with MPI programming, GA simplifies parallel programming on DMSs by providing users with a conceptual layer of virtual shared memory. Programmers can write their parallel program for clusters as if they have shared memory access, specifying the layout of shared data at a higher level. However, it does not change the parallel programming model dramatically since programmers still need to write SPMD style parallel code and deal with the complexity of distributed arrays by identifying the specific data movement required for the parallel algorithm.  The GA programming model forces the programmer to determine the needed locality for each phase of the computation.  By tuning the algorithm to maximize locality, portable high performance is easily obtained.  Furthermore, since GA is a librarybased approach, the programming model works with most popular language environments currently bindings are available for FORTRAN, C, C and Python. OpenMP provides an efficient and simple parallel programming methodology for SMSs. Given its broad acceptance in the community and the need for a simpler programming model for clusters, we believe that OpenMP should also be adapted to run on clusters, whether via extensions to the standard or improvements in compiler and runtime system technology 8. However, the traditional approach to doing so requires use of a software DSM to manage shared data in a program. Such systems potentially exchange large amounts of superfluous data at synchronization points in the code, since they transfer pages even when just one element on a page has been updated thus their ability to provide good performance is unclear. OpenMP programs map computation to threads and hence indirectly specify the data needed by a thread. This attribute makes it possible to translate OpenMP programs into GA programs. If the user has taken data locality into account when writing OpenMP code, the benefits will be realized in the corresponding GA code. The translation can give a user the advantages of both programming models straightforward programming and cluster execution.  3 Translation from OpenMP to GA Global Arrays programs do not require explicit cooperative communication between processes. From a programmers point of view, they are coding for NUMA nonuniform memory architecture shared memory systems. It is possible to automatically translate OpenMP programs into GA because each has the concept of shared data.  A careful study of OpenMP directives and GA routines showed that almost all OpenMP directives can be translated into GA or MPI library calls at source level. We may use these together if needed, since GA was designed to work in concert with the message passing environment.  Most of OpenMP library routines and environment variables can be also be translated to GA routines. one of Those that dynamically setchange the number of threads, such as OMPSETDYNAMIC, OMPSETNUMTHREADS, may not be translated.are an exception  The general approach to translating OpenMP into GA is to declare all shared variables in the OpenMP program to be global arrays in GA. Before shared data is used in an OpenMP construct, it must be fetched into a local copy, also achieved via calls to GA routines the modified data must then be written back to its global location after the computation finishes. GA synchronization routines will replace OpenMP synchronizations. OpenMP synchronization ensures that all computation in the parallel construct has completed GA synchronization will do the same but will also guarantee that the requisite data movement has completed to properly update the GA data structures.                             a     b Fig. 1.  OpenMP Parallel Regiona and translated GA program b  Call MPIINIT Call gainitialize create global arrays for shared variables        call gacreate.. calculate shared variables read region and get the local copy     call gaget..   perform computation  calculate shared variables modified region  and put the data back     call gaput.. call gaterminate call MPI FINALIZErc OMP PARALLEL SHARED     OMP END PARALLEL  The translated GA program cf. Fig. 1 first calls MPIINIT and then GAINITIALIZE to initialize memory for distributed array data.  only need call MPIINIT and GAINITIALIZE once  GA program. Variables specified in an OpenMP private clause can be simply declared as local variables, since all such variables are private to each process in a GA program by default. The translation will turn shared variables into distributed global arrays in GA code by inserting a call to the GACREATE routine. GA permits the creation of regular and irregular distributed global arrays. If needed, ghost cells are available. The GA program will make calls to GAGET to fetch the distributed global data into a local copy. After local computations have been performed using this copy, modified data will be transferred to its global location by calling GAPUT or GAACCUMULATE. GATERMINATE and MPIFINALIZE routines are called to terminate the parallel region.  OpenMPs FIRSTPRIVATE and COPYIN clauses are implemented via the GA broadcast routine GABRDCST. The reduction clause is translated by calling GAs reduction routine GADGOP. GA library calls GANODEID and GANNODES are used to get process ID and number of processes, respectively. OpenMP provides routines to dynamically change the number of executing threads at runtime. We do not attempt to translate these since this would amount to redistributing data and GA is based upon the premise that this is not necessary.                      a     b Fig. 2.  OpenMP DO construct a and translated GA program b  In order to implement OpenMP loop worksharing directives, the translated GA program calculates the new lower and upper loop bounds in order to assign work to each CPU based on the specified schedule e.g. Fig 2 shows the default static block OMP DO do ibegin, end, step          enddo OMP END DO  Calculate the local lower and upper bound of   iteration set size  end  beginstep  1nproc ifsize  nproc .NE. endbeginstep 1 then        size  size  1 endif newlow  begin threadidsizestep newupper  newlow  size1step ifthreadid .EQ. nproc1 then       newupper  end endif  calculate shared variables read region  call gaget do inewlow, newupper, step      enddo  calculate shared variables modified region call gaput  schedule. Each GA process fetches a partial copy of global data based on the array region read in the local code. Several index translation strategies are possible. A simple one will declare the size of each local portion of an array to be that of the original shared array this avoids the need to transform array subscript expressions 14.   For DYNAMIC and GUIDED schedules, the iteration set and therefore also the shared data, must be computed dynamically. In order to do so, we must use GA locking routines to ensure exclusive access to code assigning a piece of work and updating the lower bound of the remaining iteration set the latter must be shared and visible to every process. However, due to the expense of data transfer in distributed memory systems, DYNAMIC and GUIDED schedules may not be as efficient as static schedules, and may not provide the intended benefits.   Fig. 3.  Jacobi OpenMP program fragment  The OpenMP SECTION, SINGLE and MASTER directives can be translated into GA by inserting conditionals to ensure that only the specified processes perform the required computation.  GA locks and Mutex library calls are used to translate the OpenMP CRITICAL and ATOMIC directives. OpenMP FLUSH is implemented by using GA put and get routines to update shared variables.  This could be implemented with the GAFENCE operations if more explicit control is necessary. The GASYNC library call is used to replace OpenMP BARRIER as well as implicit barriers at the end of OpenMP constructs. The only directive that cannot be efficiently translated OMP PARALLEL SHARED a,b,sum        OMP DO        do j  2, SIZE      do i  2, SIZE         ai, j  bi  1, j  bi  1, j  bi, j  1  bi, j  1  4      enddo  enddo OMP END DO  OMP DO  do j  2, SIZE      do i  2, SIZE          bi, j  ai, j       enddo  enddo OMP END DO  OMP DO REDUCTIONsum    do j  1,SIZE1       do i  1,SIZE1           sum  sum  bi,j       end do     end do OMP END DO  OMP END PARALLEL into equivalent GA routines is OpenMPs ORDERED. We use MPI library calls, MPISend and MPIRecv, to guarantee the execution order of processes if necessary.  Fig. 3 shows a fragment of a simple Jacobi OpenMP program with three shared variables a, b, sum. To translate this, we need to analyze the access pattern for each shared array in order to minimize interprocess communication in the resulting code. There is no need to declare a global array for the scalar shared variable sum, since it can be handled by the GA reduction function.                                Fig. 4. Global Array version of Jacobi program We create two global arrays ga and gb Fig. 4 and distribute them in the j dimension following OpenMP program semantics, which leads to this usage pattern. For each OpenMP DO construct, we compute the new bounds for the chunk to be executed by each thread, and the region of each shared array that is read by a thread. call MPIINIT call gainitialize myid  ganodeid nproc  gannodes  create Global Arrays for shared variables OKgacreateMTDBL, SIZE1, SIZE1, A, SIZE1, SIZE1nproc, ga  OKgacreateMTDBL, SIZE1, SIZE1, B, SIZE1,SIZE1nproc, gb compute new low bound and upper bound for each thread psize  SIZE  2  1nproc ifpsize  nproc .NE. SIZE21 then        psize  psize  1 endif newlow  2  myidpsize newupper  newlow  psize1 ifmyid .EQ. nproc1 then       newupper  SIZE endif compute the array read region for each thread jlo  newlow  1 jhi  newupper  1 get array local read region call gagetgb, 1, SIZE1, jlo, jhi, b1,jlo, ld call gasync do j  newlow, newupper   do i  2, SIZE         ai, j  bi  1, j  bi  1, j  bi, j  1 bi, j  1  4  enddo enddo  compute array write region for each thread jlo  newlow jhi  newupper   put array local data back global arrays call gaputga, 2, SIZE, jlo, jhi, a2, jlo, ld call gasync .. call gaterminate call MPIFINALIZErc Then we get the local data from the corresponding global array using GAGET, prior to executing the loop. After the computation has completed, we compute the modified array region and put the locally written data back into its global storage.  4 Benchmarks We have translated small OpenMP programs nto GA and tested their performance and scalability.  The first three experiments shown here use the Jacobi code and a 11521152 matrix the last set of timings uses this code and a 23042304 matrix. Fig. 5 gives the performance of the Jacobi OpenMP and GA programs on an Itanium 2 cluster with 24 900MHz 2CPU nodes at the University of Houston each has 4 GB memory. The Scali interconnect has a system bus bandwidth of 6.4GBs and a memory Bandwidth of 12.8GBs. The Intel Fortran 7.1 compiler was used with the switches O2 i8 cm w90 w95 align LINUX64. 0102030405060708090Time Seconds1 2 4 8 16 32 40No. of ProcessorsJacobi BenchmarkOpenMPGASpeedup0510152025301 2 4 8 16 32 40No. of ProcessorsSpeedupOpenMPGA Fig. 5. Jacobi program performance on an Itanium 2 cluster The results in Fig. 6 were achieved using an SGI Origin2000 DSM system from NCSA, a hypercube with 128 195MHz MIPS R10000 processors, in multiuser mode. The OpenMP performance drops when the Jacobi program uses more than 32 processors as a result of the structure of the interconnect. 01002003004005006001 2 4 8 16 32 40 48 64No.  of  P r ocessor sJ a c obi  Be nc hma r kOpenMPGA Sp eed up01020304050601 2 4 8 16 32 40 48 64No.  of  P r ocessor sOpenMPGA Fig. 6.  Jacobi OpenMP and GA performance on SGI 2000  Fig. 7 shows performance of this code on a 44 SUN cluster 1 4way ULtraSPARCII 400 MHz E450 and 3 4way 450MHz E420s with Gigabit Ethernet connectivity. The compiler is Suns Forte Developer release 7.           Fig. 7.  Jacobi OpenMP and GA performance on a SUN cluster The last experiment performed was on NERSCs IBM SP RS6000, a distributed memory machine with 6,080 375 MHz POWER 3 CPUs with 16GB to 64 GB memory for each node. They are connected to an IBM Colony highspeed switch via two GX Bus Colony network adapters. We increased the matrix size to 23042304 in order to utilize more processors. We have not included results for more than 56 processors since there was not enough computation remaining to keep additional processes busy. 050100150200250300350Time Seconds1 4 8 16 24 32 48 56No. of ProcessorsJacobi BenchmarkOpenMPGAJacobiSpeedup0102030405060701 4 8 16 24 32 48 56No. of ProcessorsSpeedupOpenMPGA Fig. 8.  Jacobi OpenMP and GA programs performance in NERSC IBM SP cluster 5 Related Work OpenMP is not immediately implementable on distributed memory systems. Given its potential as a high level programming model for large applications, this is a serious matter and it has clearly been recognized as such by the community. There have been a variety of efforts that attempt to overcome this. Jacobi  Speedup01234561 2 4 8 12 16No.  of  P r oc e ssor sOpenMPGA0501001502002503003501 2 4 8 12 16N o. o f  ProcessorsJacob i B enchmarkOpenMPGASome of these are based upon efforts to provide support for data locality in ccNUMA systems, where mechanisms for userlevel page allocation11 and migration, and data distribution directives have been developed by SGI 13, 4 and Compaq 3. Data distribution directives can be added to OpenMP 7. However, this will necessitate a number of additional language changes that do not seem natural in a shared memory model. Moreover, OpenMP code already contains an implicit data distribution, since work is explicitly assigned to threads, or CPUs. The drawback of the OpenMP approach is that the user is encouraged to ignore locality when assigning work, not that there is no assignment of data and work. A number of efforts have attempted to provide OpenMP on clusters by using it together with a software distributed shared memory software DSM environment 1,12 ,2. Although this is a promising approach, and work will continue to improve results, it does come with high overheads. In particular, such environments generally move data at the page level and may not be able to restrict data transfers to those objects that truly require it. There are many ways in which this might be improved, including prefetching and forwarding of data, general OpenMP optimizations such as eliminating barriers, and using techniques of automatic data distribution to help carefully place pages of data. An additional approach is to perform an aggressive, possibly global, privatization of data. These issues are discussed in a number of papers, some of which explicitly consider software DSM needs 2, 9,15,6. The approach that is closest to our own is an attempt to translate OpenMP directly to a combination of software DSM and MPI 8. This work attempts to translate to MPI where this is straightforward, and to a software DSM API elsewhere.  The purpose of this hybrid approach is that it tries to avoid the software DSM overheads as far as possible. While this has similar potential to our own work, GA is a simpler interface and enables a more convenient implementation strategy. Because it has a straightforward strategy for allocating data, it can also handle irregular array accesses, which is the main reason for retaining a software DSM in the above work. GA data has a global home but it is copied to and from it to perform the computation in regions of code this is not unlike the OpenMP strategy of focusing on the allocation of work. For both models, this works best if the regions are suitably large. If the user is potentially exposed to the end result of the translation, we feel that they should be shielded as far as possible from the difficulties of distributed memory programming via MPI. GA is ideal in this respect as it retains the concept of shared data. 6 Conclusions and Future Work This paper presents a basic compiletime strategy for translating OpenMP programs into GA programs. Our experiments show good scalability of translated GA program in distributed memory systems, even with relatively slow interconnects. We do not currently attempt to translate into combined OpenMPGA as might be appropriate for exploiting shared memory on nodes under OpenMP. We intend to explore this issue and begin an implementation that will enable us to handle largescale applications.  References 1. C. Amza, A. Cox et al. Treadmarks Shared memory computing on networks of workstations. IEEE Computer, 2921828, 1996 2. A. Basumallik, SJ. Min and R. Eigenmann Towards OpenMP execution on software distributed shared memory systems. Proc. WOMPEI02, LNCS 2327, Springer Verlag, 2002 3. J. Bircsak, P. Craig, R. Crowell, Z. Cvetanovic, J. Harris, C. A. Nelson, C. D. Offner Extending OpenMP For NUMA Machines, Proceedings of Supercomputing 2000, Dallas, Texas, November 2000  4. Chandra, R., Chen, D.K., Cox, R., Maydan, D. E., Nedeljkovic, N., and Anderson, J. M. Data Distribution Support on Distributed Shared Memory Multiprocessors. Proceedings of the ACM SIGPLAN 97 Conference on Programming Language Design and Implementation, Las Vegas, NV, June 1997 5. Chapman, B., Bregier, F., Patil, A. and Prabhakar, A. Achieving High Performance under OpenMP on ccNUMA and Software Distributed Share Memory Systems. Currency and Computation Practice and Experience. Vol. 14, 2002 117 6. B. Chapman. F. Bregier, A. Patil and A. Prabhakar Achieving Performance under OpenMP  on ccNUMA and Software Distributed Shared Memory Systems. Special Issue of Concurrency Practice and Experience,  14117, 2002 7. B. Chapman and P. Mehrotra OpenMP and HPF Integrating Two Paradigms. Proc. Europar 98, LNCS 1470, Springer Verlag, 65658, 1998 8. R. Eigenmann, J. Hoeflinger, R.H. Kuhn, D. Padua et al. Is OpenMP for grids Proc. Workshop on NextGeneration Systems, IPDPS02, 2002 9. Z. Liu, B. Chapman, Y. Wen, L. Huang and O. Hernandez Analyses and Optimizations for the Translation of OpenMP Codes into SPMD Style. Proc. WOMPAT 03, LNCS 2716, 2641, Springer Verlag, 2003 10. J. Nieplocha, RJ Harrison, and RJ Littlefield Global Arrays A nonuniform memory access programming model for highperformance computers. The Journal of Supercomputing, 10197220, 1996 11. Nikolopoulos, D. S., Papatheodorou, T. S., Polychronopoulos, C. D., Labarta, J., Ayguad, E. Is Data Distribution Necessary in OpenMP Proceedings of Supercomputing 2000, Dallas, Texas, November 2000  12. M. Sato, H. Harada and Y. Ishikawa OpenMP compiler for a software distributed shared memory system SCASH. Proc. WOMPAT 2000, San Diego, 2000 13. Silicon Graphics Inc. MIPSpro 7 FORTRAN 90 Commands and Directives Reference Manual, Chapter 5 Parallel Processing on Origin Series Systems. Documentation number 0073696003. http techpubs.sgi.com 14. Mario Soukup A SourcetoSource OpenMP Compiler, Master Thesis, Department of Electrical and Computer Engineering, University of Toronto 15. T.H. Weng and B. Chapman Asynchronous Execution of OpenMP Code. Proc. ICCS 03, LNCS 2660, 667676, Springer Verlag, 2003
