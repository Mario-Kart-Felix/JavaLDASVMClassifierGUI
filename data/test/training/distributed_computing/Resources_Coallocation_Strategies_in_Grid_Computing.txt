Resources Co-allocation Strategies in
Grid Computing
Sid Ahmed MAKHLOUF
Department of Computer Science
University of Oran, Algeria
Email: sidahmedmakhloufgmailcom
Belabbas YAGOUBI
Department of Computer Science
University of Oran, Algeria
Email: byagoubigmailcom
Abstract—Computational grids have the potential for solving
large-scale scientific problems using heterogeneous and geograph
ically distributed resources. However, a number of the major
technical hurdles must overcome before this potential can be
realized. One problem that is critical to effective utilization
of computational grids and gives a certain Quality of Service
(QoS) for grid users is the efficient co-allocation of jobs. Due
to the lack of centralized control and the dynamic nature
of resource availability, any successful co-allocation mechanism
should be highly distributed and robust to the changes in the Grid
environment. Moreover, it is desirable to have a coallocation
mechanism that does not rely on the availability of coherent
global information. This work addresses those problems by
describing and evaluating two resources co-allocation Strategies
The proposed Strategies have been verified through an extension
of GridSim simulation toolkit and the simulation results confirm
that our Strategies allow us to achieve the most of our goals
Keywords: Grid Computing, Grid Scheduling, Resources Allo
cation, Resources Co-allocation, Multi Agents Systems, Advance
Reservation, Reinforcement Learning
I. INTRODUCTION AND RELATED WORK
Grid Computing is concerned with coordinated resource
sharing and problem solving in dynamic, multiinstitutional
virtual organizations [8]. The coordination between multiple
administrative domain results heterogeneity in Grid Environ
ment. Resources owned by various administrative organiza
tions are shared under locally defined policies that specify
what is shared, who is allowed to access what, and under
what conditions
To take full advantage of the promising capabilities of
grid computing, good resources co-allocation schemas must
be developed. Compared to the resources allocation in tradi
tional distributed systems, resources co-allocation in the grid
must consider the characteristics of users and the nature of
resources, making the design of a co-allocation system very
complicated [1]. Sometimes, the needs of a single job may
exceed the capacity available in each of the subsystems making
up a Grid, and so co-allocation [5] (i.e. the simultaneous access
to resources of possibly multiple types in multiple locations
managed by different resource managers or locals scheduler
may be required
The advance reservation is an effective technique to ensure
quality of service. It was incorporated in several grids. It
allows applications to obtain simultaneous access to adequate
resources and ensure their availability at the required time
Several studies have been proposed using the advance reser
vation [11], [18]. However, advance reservation have many
negative effects on resource utilization rate and jobs scheduling
in the Grid systems. Several studies [7], [16] show that the
advance reservation reduces the resources utilization rate and
excessive reservation requests often result in a high rate of
rejections from the resource providers. These negative effects
influence the Grid economy [2], where resource providers wish
to increase the utilization rate of their resources to obtain
maximal profits
Agents are known to be an appropriate paradigm for
modelling complex, open and distributed systems. In [9] the
authors discuss the benefits obtained through the application
of multi-agent systems for Grid computing and show that
multi-agent systems are well suited to describe the grids
because the distributed nature of autonomous agents users
and resource providers) reflects the nature of federated grid
Therefore, many systems applying multi-agent paradigm for
Grid computing have been proposed. In [10], [19] the authors
show that the multi-agent systems can effectively solve the
problems of load balancing and resource allocation in grid
computing. In [4], authors introduce an ontology based on the
paradigm of an intelligent agents for resource allocation in the
grid, the proposed approach uses the ontological argument to
select an appropriate resource provider
This paper presents agent-based resources coallocation
in Grid computing. A new co-allocation model had been
introduced where a set of co-allocators agents receive jobs
from multiple Grid users and use some techniques to schedule
the job to one or more resources agent (see section III). The
objectives of introducing these co-allocation strategies are as
follows: (i) improve users benefit by minimizing their jobs
execution time and waiting time; (ii) improve resources benefit
by maximizing theirs utilization rate. We assume that the
co-allocation model is non-preemptive, and all the jobs are
independent
The remaining part of this paper is organized as follows. Our
Grid Co-allocation Architecture and Motivations are presented
in Section II. In Section III, the algorithmic description of our
co-allocation strategies is presented. An experimental setup
along with the comparative results is explained in Section
IV. Conclusion and future research direction are proposed in
Section V
II. SYSTEM ARCHITECTURE AND MOTIVATIONS
A. Architecture
We model the Grid as a heterogeneous system consisting
of three different types of agents [15] : co-allocator agents
resources agents and the regulator agent (see Figures a
(a) Co-allocation system architecture
(b) Resources provider’s slots representation in the time
(c) Reinforcement learning mecanisme
Fig. 1. System Model
Resources Agents: is a set of agents that offer resources
to serve computational needs of Grid users. Each resource
agent is characterized by the type and amount of the resources
under her disposal, support advance reservation and represents
one or more resources providers. Each resource provider may
differ from the rest of the others ones with respect to number
of processors, speed of processing, local scheduling. The
introduction of this type of agent is was necessity because the
most of the providers use local resources management that
does not support the advance reservation
Co-allocator Agents (or Brokers): is a set of selfish
agents who try to increase their profits. They are interested
to reduce at least the time-out and the response time of the
applications that they receive
Regulator Agent: it intervenes to solve conflicts between
the two or more co-allocators agents
B. Motivations
The lack of precise information on the resources status in
large scale is one of the challenges of resources coallocation
in grid computing. The co-allocation systems integrated into
the grids do not support a co-allocation in real time as they
often retrieve systems information asynchronously. This sug
gests that the mechanisms for resources co-allocation should
not depend on the availability of global information
Conventionally the advance reservation is defined as a
process of allocating resources for use at a specific time
in the future [12]. The two main attributes of a reservation
request is the reservation start time and the reservation duration
time (Figures 1(b)). As the availability and performance of
resources are unpredictable in the grid systems on a large
scale, the estimation of these two attributes are difficult
Therefore, the advance reservation has many disadvantages on
the resources sharing and scheduling applications in grids
In many problems, it is desirable that a user of the Grid can
reach a goal without knowing the rules to achieve the goal in
question [17]. The Reinforcement Learning is a method to
learn dynamically these rules by interacting with its environ
ment. The interaction between the user and its environment is
permanent (Figures 1(c)). The user chooses to make action and
the environment changes according to the action, so, the user
will be confronted to choose new actions. The user does not
have the complete perception of his environment, this restric
tion imposes an additional difficulty on decision-making. A
particularity appropriate to the reinforcement learning is the
use of a reward after each action. The reward is an integer
sent by the environment to the user for each action carried
out. The goal of the user is to increase the total sum of the
received rewards. The reinforcement learning allows a system
to adapt itself to the environment changes, such as change of
the resources capacities, resources insufficiency, or arrived of
the users in the system
The MAS (Multi Agents System) approach is well suited
for describing the Grid, because the distributed, autonomous
nature of agents (Grid users and resources) reflects the feder
ated nature of the Grid. Introducing resources offers allows
the multi agents system to adapt to changes, such as the
changing resource capacities, resource failure, or introduction
new agents into the system
Motivated by these facts, we propose two distributed re
sources co-allocation strategies, the first strategy use resources
offers and advance reservation planning (OAR) and the second
use the reinforcement learning mechanism (RLA). The objec
tive of OAR and RLA is to minimize the user’s jobs response
time, minimize the waiting time of the users and increase the
resource utilization rate
III. CO-ALLOCATION MODEL
Proposal co-allocation system supports Bag of Tasks appli
cations (BoT) where they are mainly for intensive calculation
A broker (co-allocator agent) can decompose a BoT applica
tion into k sub-applications (fragments of application), where
every sub-application is sent to a resources agent
J
kPL
(W,E) : The k − th application submitted to a resources
agent. It needs P processors and has a size of L MI Million
Instructions). W and E are respectively the waiting and the
execution time of the application. At the submission timeW
0 and E =
Oc : Offer from the resource agent c, it consists of a list of
slots. A slot S(s,n)(i,c,D) is windows for a resource agent c that
represent it’s start time availability s. Each slot i consist of a
number available processors n at time s and duration time D
to execute a job. D is calculated by the resources agent c by
using the resources providers processing speed and L witch is
the size of the received application. We can represent an offer
from resource agent c as Oc

S
sn
(i,c,D)|i = 1, Oc

. Offers
are to execute part or the entire job
A. Resources Agents
The resource agent : it’s goal is to increase its resources
utilisation rate by giving the same offer to several coallocators
agents. The resource agent uses the job information provided
by the co-allocator agent. In order to generate an offer Oc for
a job J (k,P,L)(W,E) , the resource agent c
1) Calculate the execution duration time D of the job
2) Find all available free slots in the advance reservation
list
3) Create a list of free slots Oc for each job J
kPL
(W,E)
Each slot i include it’s start time s , the number of the
available processors n, the duration D to execute the job
and the resource agent identifier c
4) Return the offer Oc to the co-allocator agent
B. Co-allocator Agents
We developed the co-allocators agents to support the strate
gies of the advance reservations planning and the reinforce
ment learning to measure their impacts on the behaviour of
the grid
1) Advance reservations planning strategy: In this strategy
each co-allocator agent is responsible for the composition
of offers from different resource agents to meet the needs
of users in terms of resources. The composition determines
the workload that each co-allocator agents will sent for each
resource agent. The aim of the offers composition is to
minimize the execution and waiting time of jobs user. The
offer composition is described in the algorithm
After collecting the offers from all the N resource agents
the co-allocator agent
1) Creates a OffersList
N
c
Oc with all the proposed
offers, each offer Oc from resource agent c contain a
set of free slots Oc

S
sn
(i,c,D)|i = 1, Oc


Algorithm 1 Co-allocation algorithm integrated in each broker
that use Advance reservations planning strategy
Require: J (k,P,L)(W,E) , N
1: OffersList ←
2: for c = 1 ; c ≤ N ; c ← c+ 1 do
3: Oc ← queryFreeSlot c
4: OffersList ← OffersList Oc
5: end for
6: SortedOffersList ← sortOffersListFinishTime
7: BestOffersList ← ϕ; NumPE ←
8: while NumPE < P do
9: S
sn
(i,c,D) ← min SortedOffersListFinishTime
10: BestOffers ← BestOffers

S
sn
icD

11: NumPE ← NumPE + n
12: SortedOffersList ← SortedOffersList
S
sn
icD

13: end while
14: while BestOffersList ̸= ϕ do
15: S
sn
(i,c,D) ← get BestOffersList
16: BestOffersList ← BestOfferList

S
sn
icD

17: if sendAR

s, n,D, J
kPL
(W,E) , c

= failed then
18: requestRegAgt

s, n,D, J
kPL
(W,E) , ThisAgentID

19: end if
20: end while
2) Sorts the OffersList in ascending order by finish time
with FinishTime = StartTime + DurationTime
3) Create a BestOffersList ⊆ OffersList that contains
the first best slots in the OffersList according to the
job requirement P
4) For each slot S(s,n)(i,c,D) in the BestOffersList with
i = 1, |BestOffersList|, send an advance reservation
request AR⟨s, n,D, J (k,P,L)(W,E) , c⟩ to the resource agent c
that include slot’s start time s, slot’s available processors
n , the job execution duration time D proposed by the
resource agent c and the job request J (k,P,L)(W,E) . If the
co-allocator agent receives an error message then that
means the slot had been taken by another coallocator
agent. This behavior is normal since a resource agent
can propose the same offer to several coallocators
agents to increase its resource utilization rate. In this
case, the co-allocator agent asks the regulator agent to
find him a new emergency reservation for the subjob
J
knL
(W,E)
2) Reinforcement learning strategy: The brokers co
allocators agents) have no preliminary knowledge on the
capacities of the resources agents, they use the reinforcement
learning mechanism to estimate their capacities by using their
experiences gained in the past. So to do this, they attribute
a score for each resources agent. This score represents an
evaluation of the performances of the resources agent. He
indicates how this last one behaves in past. After the execution
of each application, the broker updates the score of the
corresponding resources agent
When the broker a successfully receives the execution result
of the application J (k,P,L)(W,E) , it updates the score R
a
c of the
resources agent c that have finished the execution in question
so to do this
1) It calculates the response time Tc = W + E for the
resources agent c
2) It calculates the average response time Twc
Twc Tc
k of
the k applications submitted to the resources agent c
3) It calculates the reward rc = |Twc − Tc| of the resources
agent c
4) It updates the score Rac ← Rac + rc of the resources
agent c
As indicated below, every broker a uses his experience to
distribute an application between the N resources agents with
1 ≤ a ≤ M Where M is the number of brokers in the system
For each resource agent c the broker a keeps a score Rac to
indicate him his efficiency in past and to calculate him the
size of the fragment of the application which he is going to
run. The process of co-allocation integrated into each broker is
described in the algorithm 2 for each new received application
1) The broker a calculates RT which is the sum of the
scores of all the resources agents
2) The broker a calculates Fc which is the size of the
fragment of the application that each resource agent will
take. More the resource agent has an important score
more the size of the fragment which he is going to
receive is important
3) The broker a sends each fragment of the application
J
kFcL
(w,e) to the resource agent corresponding by calling
the function sendFragment (). If the query fail then it
means that the broker in question overestimate the ca
pacities of the resources agent. In this case, the broker in
question asks to the regulator agent to find him another
reservation by calling the function requestRegAgt
Algorithm 2 Co-allocation algorithm integrated in each broker
that use reinforcement learning strategy
Require: J (k,P,L)(W,E) , N

Ra1 , R
a
2 , ..., R
a
c , ..., R
a
N−1, R
a
N

1: W ← 0; E ← 0; RT ←
2: for c ← 1 ; c ≤ N ; c ← c+ 1 do
3: RT ← RT Rac
4: end for
5: for c ← 1 ; c ≤ N ; c ← c+ 1 do
6: Fc ← 1RT ∗ R
a
c ∗ P
7: end for
8: for c ← 1 ; c ≤ N ; c ← c+ 1 do
9: if sendFragment

c, J
kFcL
WE

= failed then
10: requestRegAgt

J
kFcL
WE

11: end if
12: end for
This policy of co-allocation distribute an application on all
the resources agents of the system
C. Regulator Agent
The regulator agent is responsible for requesting emergency
offers from the different resource agents to meet a subjobs
requirement that was sended by the brokers. The goal of the
regulator agent is to find the best offer so that the system
remains always stable in terms of execution time and waiting
time after the appearance of a conflict
After collecting the offers from all theN resource agents for
the sub-job J (k,n,L)(W,E) that require n processors and has length
of L MI (Million Instructions), the regulator agent
1) Creates a reservation list RvList
N
c
Rv
sD
c with
all the proposed reservations, each reservation RvsDc
from resource agent c has a start time s and duration
time D
2) Take the best advance reservation BestRv(s,D)c
RvList that has the earliest finish time with
FinishT ime = sD
3) Cancel the rest of the reservations in the RvList
4) For the reservation BestRv(s,D)c send a commit message
CAR⟨s,D, J (k,n,L)(W,E) , c⟩ to the resource agent c that
include reservation’s start time s, the job execution
duration time D proposed by the resource agent c and
the sub-job request J (k,n,L)(W,E)
IV. EXPERIMENT STUDY
A. Experimental Setup
We have evaluated our co-allocation policies by means of
simulations to observe its effect in a long-term usage. We have
used the event-driven simulator named GridSim [3], which we
have extended to support jobs on multi-site environments and
MAS (Multi Agent System). We have used real traces from
supercomputers available at the Parallel Workloads Archive
In our simulation, various entities connected by a network and
every two entities’ connectivity had an exclusive bandwidth
We analyse the global behaviour of the system using OAR
strategy that use resources offers and advance reservation
planning and compare it with the strategy RLA that makes
use of a agent knowledge of current resource usages. We have
modelled an environment composed of seven resources agents
each resources agent manage one cluster with it own local
scheduler, one regulator agent and seven co-allocators agents
that receives jobs, Table I illustrate our grid environment. We
have used the trace file of the San Diego Supercomputer Center
Blue Horizon. More details on the trace file can be found at
the Parallel Workloads Archive. We have simulated 503 days
of this traces that is equivalent to 117000 jobs
We performed the experiments on a PC with 2 Cores
Processors, 2.20GHz and 2GB RAM using Linux 64 bits. We
have assessed four metrics
httpwwwcshujiacillabsparallelworkload
TABLE I
COMPUTATIONAL GRID ENVIRONMENT
Resource Provider Processors Numbers Resource Speed MIPS
Cluster 1 240
Cluster 2 210
Cluster 3 180
Cluster 4 150
Cluster 5 120
Cluster 6 90
Cluster 7 60
• Makespan: total time to execute all the submitted jobs
Makespen
MaxJobs
i
(Fi − Ei) with Ei and Fi are
respectively the execution start time and the execution
finish time of the job i
• Waiting Time: average time difference between the
submit time and execution start time of all the jobs
WaitingT ime
MaxJobs
i
EiSi
MaxJobs with Si and Ei are
respectively the submission time and the execution start
time of the job i
• System Utilization: average resource utilization rate ω
N
c
c
N with N the numbers of resources providers, ωc is
the system utilization rate of resource’s provider c
• Number of the regulator agent interventions: We count
the number of times C
n
i
Ci where the coallocator
agents call the regulator agent with n is the number of
the co-allocator agents and Ci is the number of calls to
the regulator agent of the co-allocator agent i
B. Results analysts
1) User performance: From the Figures 2(a) we can see
that when the number of jobs increases theMakespan increases
and by comparing the two curves of the Figures 2(a), we see
that we have obtained a gain in Makespan in large scale by
using our OAR algorithm compared to the RLA algorithm
This is due to the Demands/Offers strategy that allows the
distribution of the job’s execution time on the most available
resources, unlike the RLA algorithm that try to estimates the
best resources that can satisfy the job’s requirements
From the Figures 2(b) we can see that when the number
of jobs increases the average waiting time decreases and by
comparing the two curves of the Figures 2(b), we see that in
the large scale the RLA algorithm reduce the average waiting
time of the jobs compared to the OAR algorithm. This is
due to the reinforcement learning mechanism that allows to
the co-allocator agents to have a global vision of the system
via the response time of the returned jobs, in this algorithm
the co-allocators agents try to send the jobs to the resources
agents that have the best score in terms of response time
unlike the OAR algorithm that uses the advance reservation
mechanism to ensure the availability of the resources in the
future
(a) Total Makespan to execute all the jobs
(b) average waiting time of all the submitted the jobs
Fig. 2. User performance
2) System performance: From the Figures 3(a) we can see
that the utilization rate of the resources remains stable in time
and by comparing the two curves of the Figures 3(a), we see
that the OAR strategy give us a maximization of resources
utilization rate compared to the RLA algorithm. This is due
to the Demands/Offers policy that allows to a resource agent
to maximize the use of its resources by giving the same offer
to several co-allocator agents, unlike the RLA algorithm that
use a reinforcement learning techniques to schedule jobs, these
techniques do not allow to the resources agents to maximize
the use of their resources
By comparing the two curves of the Figures 3(b), we see that
the RLA algorithm remains stable in time and the OAR increase
in time, this is due to the reinforcement learning techniques
that allow to the co-allocator agents to know the global stat
of the system after few time, and by consequence, the co
allocator agents do not need the help of the regulator agent
unlike the OAR algorithm that use the offers concurrence to
schedule the jobs and by consequence, there is a risk to have
a offers conflicts
(a) Resources utilization rate
(b) Number of the regulator agent interventions
Fig. 3. System performance
V. CONCLUSION AND FUTURE WORK
In this paper, we introduced a MAS based coallocation
policy for composing resource offers from multiple resources
providers to co-allocate a grid user’s jobs. so, we introduced
3 types of agents: Co-allocator agents, resources agents and
regulator agent. We extend the GridSim Toolkit to carry out
the simulation of our co-allocation policy and compared our
results with an extension of a distributed resource allocation
that use reinforcement learning agents (RLA). We draw the
conclusion that regarding to the execution time and utilization
rate, our policy gives us good performances compared to the
RLA policy, but regarding to the waiting time and the number
of conflicts in the system, the RLA policy gives us good
performances compared to our policy. We remark that the two
policy (OAR and RLA) have advantages and disadvantages
so as future work we think to develop a new hybrid model
that includes the two models together to profit from their
advantages
REFERENCES
[1] H. Blanco, J. Lrida, F. Cores, and F. Guirado, “Multiple job co
allocation strategy for heterogeneous multi-cluster systems based on
linear programming,” The Journal of Supercomputing, pp. 1–9,
[2] R. Buyya, D. Abramson, and S. Venugopal, “The Grid Economy,” in
Proceedings of the IEEE. IEEE Press, New Jersey, USA, 2005, vol.
no. 3, pp.
[3] R. Buyya and M. M. Murshed, “Gridsim: a toolkit for the modeling and
simulation of distributed resource management and scheduling for grid
computing,” Concurrency and Computation: Practice and Experience
vol. 14, no. 13-15, pp. 1175–1220,
[4] K. C. Cho, C. H. Noh, and J. S. Lee, “Ontology-based intelligent
agent for grid resource management,” in Computational Collective
Intelligence. Semantic Web, Social Networks and Multiagent Systems
First International Conference, ICCCI 2009, Wroclaw, Poland, October
5-7, 2009. Proceedings, ser. Lecture Notes in Computer Science, vol
5796. Springer, 2009, pp.
[5] K. Czajkowski, I. Foster, and C. Kesselman, “Resource coallocation
in computational grids,” in HPDC ’99: Proceedings of the 8th IEEE
International Symposium on High Performance Distributed Computing
IEEE Computer Society, 1999, p.
[6] I. Foster, C. Kesselman, C. Lee, R. Lindell, K. Nahrstedt, and A. Roy
“A distributed resource management architecture that supports advance
reservations and co-allocation,” in Proceedings of the International
Workshop on Quality of Service, 1999, pp.
[7] I. Foster, A. Roy, V. Sander, and F. J. Gmbh, “A quality of service archi
tecture that combines resource reservation and application adaptation
Tech. Rep.,
[8] I. T. Foster, “The anatomy of the grid: Enabling scalable virtual
organizations,” in Proc. First IEEE International Symposium on Cluster
Computing and the Grid (1st CCGRID’01). IEEE Computer Society
(Los Alamitos, CA), 2001, pp.
[9] I. T. Foster, N. R. Jennings, and C. Kesselman, “Brain meets brawn
Why grid and agents need each other,” in AAMAS. IEEE Computer
Society, 2004, pp.
[10] M. Meriem and Y. Belabbas, “Dynamic dependent tasks assignment
for grid computing,” in Algorithms and Architectures for Parallel Pro
cessing, ser. Lecture Notes in Computer Science. Springer Berlin
Heidelberg, 2010, vol. 6082, pp.
[11] C. Qu, “A grid advance reservation framework for co-allocation and co
reservation across heterogeneous local resource management systems
in Parallel Processing and Applied Mathematics, 7th International
Conference (7th PPAM’07), ser. Lecture Notes in Computer Science
(LNCS), vol. 4967. Springer-Verlag (New York), 2008, pp.
[12] A. Roy and V. Sander, “Advance reservation api,” Global Grid Forum
(GGF),
[13] M. Sid Ahmed and Y. Belabbas, “Co-allocation des ressources dans
les grilles de calcul utilisant le renforcement de l’apprentissage,” in Les
premières journées doctoriales en informatique (ReSyD2010). BEJAIA
University,
[14] ——, “Co-allocation in grid computing using resources offers and
advance reservation planning,” in International Symposium on Modelling
and Implementation of Complex Systems Constantine MISC
UMC University, 2010, pp.
[15] ——, “Distributed resources co-allocation in grid computing,” in Inter
national Conference on Machine and Web Intelligence ICMWI
USTHB University, 2010, pp.
[16] Q. Snell, M. Clement, D. Jackson, and C. Gregory, “The performance
impact of advance reservation meta-scheduling,” Lecture Notes in Com
puter Science, vol. 1911, pp. 137–153,
[17] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction
The MIT Press,
[18] A. Takefusa, H. Nakada, T. Kudoh, and Y. Tanaka, “An advance
reservation-based co-allocation algorithm for distributed computers and
network bandwidth on qos-guaranteed grids,” in Job Scheduling Strate
gies for Parallel Processing, ser. Lecture Notes in Computer Science
Springer Berlin / Heidelberg, 2010, vol. 6253, pp.
[19] B. Yahaya, R. Latip, M. Othman, and A. Abdullah, “Dynamic load
balancing policy in grid computing with multi-agent system integration
in Software Engineering and Computer Systems, ser. Communications in
Computer and Information Science. Springer Berlin Heidelberg,
vol. 179, pp.

