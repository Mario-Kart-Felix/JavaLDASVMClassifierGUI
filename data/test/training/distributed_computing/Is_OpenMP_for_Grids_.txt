Is OpenMP for Grids Rudolf Eigenmann, Jay Hoeflinger, Robert H. Kuhn, David Padua,Ayon Basumallik, SeungJai Min, Jiajing ZhuPurdue UniversityKAI SoftwareIntel Americas, Inc.University of Illinois at UrbanaChampaignAbstractThis paper presents an overview of an ongoing NSFsponsored project for the study of runtime systems and compilers to support the development of efficient OpenMP parallel programs for distributed memory systems. The firstpart of the paper discusses a prototype compiler, now under development, that will accept OpenMP and will targetTreadMarks, a Software Distributed Shared Memory System SDSM, and MessagePassing Interface MPI libraryroutines. A second part of the paper presents ideas forOpenMP extensions that enable the programmer to overridethe compiler whenever automatic methods fail to generatehighquality code.1. IntroductionOne of the main objectives of parallel software research has been the development of a standard programming methodology for the development of efficient programs for all classes of parallel machines. Such standardization would reduce the effort needed to train programmers, facilitate the porting of programs, and, in general,would reduce the burden of adopting parallel computing.An important contribution in this direction was the development of High Performance Fortran HPF 4 which extended Fortran 90 with directives to specify parallel execution, data distribution, and data alignment. Although practically all research on HPF compilers assumed that the targetwas distributed memory machines, it is possible, and probably much easier, to target HPF to shared memory systems.Unfortunately, HPF has not received widespread acceptanceand parallel programming seems to have converged to twoapproaches that differ significantly from each other.By far the most popular way of programming parallelmachines today, especially clusters and distributed memory machines in general, is to write SPMD Single Program Multiple Data programs and use MessagePassingInterface MPI library routines 6 for communication andsynchronization. The second approach, which dominateswhen the target machine is a Symmetric MultiprocessorSMP with a few processors, is to use thread libraries orOpenMP 3 to write parallel programs assuming a sharedmemory model. OpenMP resembles HPF because of its reliance on directives. However, the OpenMP standard differsfrom HPF in that it deals almost exclusively with controlflow and synchronization and has practically no mechanismto control data placement and alignment.Of the two approaches, the former is seen as a low levelprogramming model to the point that MPI has been calledthe assembly language of parallel programming. Clearly,messagepassing programming has the advantage that itgives the programmer direct and explicit control of the communication between threads. However, the complexity ofsubscripts that arise when arrays are distributed manuallyand the difficulty of changing distributions and, in general, modifying messagepassing parallel program makesthe messagepassing programming model inconvenient andcostly.Given this state of affairs, a natural course of action isto try to unify the two approaches into a universal parallel programming paradigm that can be used for all classesof parallel systems available today from small SMPs towidearea distributed computer grids. OpenMP, with theappropriate extensions, seems a natural candidate for sucha unifying role. Both OpenMP and thread libraries bringthe programming advantages of the shared memory model,but OpenMP has the additional advantage of enforcing anested structure in the parallel program. This last consideration gives OpenMP an advantage over thread libraries.Although OpenMP was developed to program shared memory machines, we believe it is possible to use OpenMP togenerate efficient programs for distributed memory clustersand computer grids. Clearly, to achieve this goal the appropriate runtime systems, OpenMP extensions, and compilertechniques must be developed.Although the replacement of MPI with OpenMP willprobably happen first for conventional distributed memorymachines, we believe that grids are also an important target for this project. In fact, grids have matured from justa collaborative environment to a resource where users areactually thinking of running large parallel applications. Thestateoftheart tools available to parallel programmers toharness this resource appears to be MPI variants such asMPICHG. However, programming at a higher abstractionlevel is perhaps even more important in the grid scenariodue to the complexity involved in the programming of unrelated systems.A possible approach to implement OpenMP is to use aSoftware Distributed Shared Memory SDSM system sucha TreadMarks 1 to create a shared memory view on topof the target system. By following this approach the implementation of OpenMP on distributed memory systemsbecomes equivalent in difficulty to implementing OpenMPon an SMP machine. The drawback is that the overheadtypical of SDSMs can affect speedup significantly.A way to reduce the overhead is to translate OpenMPprograms so that the SDSM system is used only to handlethe communications due to arrays with access patterns thatare irregular or unknown to the compiler, while communications due to regular access patterns is handled by messagepassing libraries such as MPI. This can be achieved by applying compiler techniques similar to those developed byHPF. This approach does not suffer from the same overhead problems as the SDSM approach in the case of regularmemory access patterns and can be easily combined withthe SDSM approach to handle irregular accesses.The development of an effective compiler methodologyto map OpenMP codes onto a runtime environment containing message passing routines, a SDSM system, and otherforms of runtime support could be a major challenge. Onereason is that not much is known about the effectiveness oftodays compiler technology in generating messagepassingcode from shared memory programs. Much testing andevaluation of existing compiler techniques for message generation is still needed and probably these techniques willhave to be extended to achieve the desired result. Also,the combination of several approaches introduces importantquestions that need to be studied. For example, besidesusing SDSM support it is also possible to use inspectorexecutor techniques to handle irregular access. It is not clearhow these two ways to deal with irregular accesses comparein terms of implementation cost, efficiency, and portability.Any effective compiler system should allow users directcontrol of the code generated, especially when the compilertechnology is not mature. To this end, we propose to extendOpenMP to allow users direct control of messagepassingcode generation, computation distribution, and data alignment. This is important because the compiler, especially theearlier versions of it, is not expected to adequately handle allconceivable situations. Providing direct control through extensions to OpenMP will make it possible for the programmers to take advantage of the compiler in order to avoid thecomplexities of messagepassing programming in a significant portion of the program. And in those, hopefully small,sections of the code where the compiler fails, the programmer will be able to intervene and produce efficient code.Under NSF support we are developing a compiler thatwill accept an extended form of OpenMP and will generate parallel code that will take advantage of the most appropriate runtime support available. So far, we have onlyconsidered software runtime support, but we plan to lookat hardware support in the future. As discussed below, it isreasonable to expect that successful software mechanismswill evolve at least partially into hardware form.This paper describes our OpenMP compiler. The firstversion of this compiler will accept standard OpenMP programs. We have already developed practically all components of this first version of the compiler and only their integration in a complete system remains. A second version ofthe compiler will include a number of additional optimizations along the lines of those discussed in Section 2.6, andwill accept OpenMP extensions as discussed in Section 3.Although this paper does not give a complete answerto the question posed in the title, it presents a clear planto find such an answer. We believe that, in order to beable to answer YES, a complete program development andexecution system has to be developed, and it has to beshown that it enables the development of efficient parallel code for distributed memory systems. The rest of thispaper presents some components of such a program developmentexecution system. Section 2 discusses the hybridcommunication model and compiler techniques to take advantage of this model, which will be incorporated in the firstversion of our compiler. Additional compiler optimizationsare discussed in Section 2.6, and Section 3 presents ideasabout OpenMP extensions.2 The OpenMP Compiler2.1 The Hybrid Communication ModelBased on the Polaris system 2, we are developing acompiler that accepts conventional OpenMP and generatesa program that follows a hybrid communication model. Ituses a synthesis of the private memory assumed by MPI2programs today and shared memory models. The newmodel inherits the strong points from both existing models.By targeting this model, we believe that the compiler cangenerate efficient parallel code easily and automatically, onregular, irregular and mixed access pattern programs. Weassume an environment that supports both message passingand a hardware or software shared memory mechanism.In our hybrid communication model, we include boththe private memory and the shared memory model. Sincethe most efficient mechanism for communicating data differs, depending on how the data is used within the code, weapply two different communication mechanisms to handledata with different communication features. The user datais classified by its usage patterns. We divide data into threeclasses private data, distributed data, and shared data.Private data consists of variables that are only accessedby a single thread. No communication is needed for thisdata. Some of the private data will be that identified inOpenMP private clauses and other will be identified by ourOpenMP compiler as an optimization.Distributed data consists of variables, with simple usage patterns, which might need to be communicated between threads. Communication of these variables is handled by the message passing mechanism. Since they have asimple usage pattern, the precise message passing code canbe easily generated by the compiler automatically.Shared data consists of variables with statically unknown or irregular access patterns. The consistency ofshared variables across threads is handled by a shared memory mechanism. Shared data is data for which precise message passing code cannot be generated, due to lack of precise access information.In this hybrid model, explicit message passing andthe shared memory system are independent and equallevel mechanisms. They work separately on different dataclasses. The current target of our compiler is a systemwhere shared memory is implemented in software via aSDSM system such as TreadMarks. Distributed data is located outside the shared memory space of the SDSM system, eliminating the overhead that would be otherwise necessary to manage it within the SDSM system.2.2 Phases of the OpenMP CompilerThe framework of the prototype compiler algorithm toimplement the hybrid model consists of three phases. Wegive a brief outline of the algorithm framework here, followed by a more detailed description in the next subsections.Phase 1. Data classification and distribution In thisphase, the compiler classifies unprivatizable user data as either distributed data or shared data. Distributed data is divided among the threads, while shared data is apportionedaccording to the rules for the SDSM being used TreadMarks replicates its shared data on each thread. The communication of distributed data will be handled by the explicit message passing mechanism, while the shared datawill be handled via the SDSM mechanism.The compiler analyzes data access patterns. Data havingsimpleenough access patterns is considered as distributeddata. All other data in the program is considered as shareddata, except for privatizable data identified in the privateclauses of the OpenMP source program.Phase 2. Computation Division In this phase, thecompiler divides the iteration space and assigns iterationsto each thread. The code is organized in the SPMD model.This phase includes two subphases computation divisionfor distributed data and computation division for shareddata. The iterations of parallel loops that contain distributeddata are divided among the threads, based on, althoughnot restricted to, the ownercomputes rule. For the parallel loops that contain only shared data, the compiler assignsiterations to threads evenly.Phase 3. Communication generation Function callsfor communications are generated and inserted in thisphase. Like Phase 2, this phase consists of 2 subphasesfor message passing communication and SDSM communication. The explicit message passing calls are generatedat first for communicating the distributed data, and thenSDSM calls for synchronizing the shared data are generated.Notice that the framework is particularly designed forthe hybrid communication model. It does not have an independent path for either message passing or SDSM code.The generation of message passing and SDSM primitives isperformed jointly in a single pass.2.3 Data classification and distributionThe source OpenMP program identifies private data andparallel loops. In Phase 1, we classify the remaining userdata, which is involved in parallel loops, as distributed andshared data.The algorithms used in our framework are quite simple,especially when comparing them to the algorithms that researchers have developed to automatically distribute and access data in distributed systems. This is one of the importantadvantages of our system. By applying an appropriate communication mechanism to each type of data regular or irregular, we hope to generate efficient code, even by simplealgorithms.The algorithm is designed to operate in an incrementalway. In the basic algorithm, only the data with very simpleaccess patterns will be considered as distributed data, butthe algorithm can be made more sophisticated and open tofurther optimization. New techniques can be added incre3mentally to push more data into the distributed data set, toachieve better performance.The algorithm analyzes the access pattern for every dimension of each reference with respect to the loop index.If the section of the array accessed by each iteration can bedescribed at compile time using a simple notation, such astriplets, we say that the access is regular. Otherwise, we saythat the access pattern is irregular. Then each dimension isclassified as distributable or nondistributable depending onthe type of loop that controls access to the dimension. Also,the type of distribution is determined by the iteration spaceof the loops controlling access to the dimension. More details about this algorithm can be found in 12.2.4 Computation divisionWe discuss first how to translate parallel loops that onlycontain shared and private data. The common translationmethod for loopparallel languages employs a microtaskingscheme. In this scheme, execution of the program startswith the master thread which during initialization createshelper threads that sleep until they are needed. When aparallel construct is encountered, the master wakes up thehelpers and informs them about the parallel code to be executed and the environment to be setup for this execution.Such microtasking schemes typically are used in sharedmemory environments, where communication latencies arelow and where fully shared address spaces are available.In contrast to shared memory environments, distributedarchitectures exhibit significantly higher communication latencies and they do not support fullyshared address spaces.In the TreadMarks SDSM system used in our work, sharedmemory sections can be allocated onrequest. However,all threadlocal address spaces are private  not visible toother threads. These properties question the benefit of a microtasking scheme because 1 the helper wakeup call performed by the master thread would take significantly longerand 2 the data environment communicated to the helperscould only include addresses from the explicitly allocatedshared address space. Therefore, in our work we have chosen an SPMD execution scheme, as is more common in applications for distributed memory systems. The SPMD execution scheme is also necessary for program sections thataccess distributed data. In an SPMD scheme all threads begin execution of the program in parallel. Sections that needto be executed by one thread only must be marked as suchexplicitly and executed conditionally on the thread identification. In general, sequential program sections are executedredundantly by all threads, whereas in parallel regions thethreads share the work.For parallel loops that only contain references to shareddata, we divide the iteration space evenly for each thread.Our SPMD translation of OpenMP parallel regions splitsthe worksharing constructs, modifying lower and upperbounds of the loops according to the iteration space assigned for each participating thread. Currently, we supportstatic scheduling only. All parallel constructs containing atleast one access to a shared variable are placed between apair of barrier synchronizations. At these barriers, a coherent state of the shared memory is also maintained. During the parallel regions, the SDSM system supports releaseconsistency. The dual function of the barrier as synchronization and maintaining coherence is important to note. Tomaintain coherence, threads might need to execute expensive system calls, which increase in number with the amountof shared data written in the parallel region.Parallel loops that contain distributed data or both distributed data and shared data are scheduled according to thelayout of the distributed data, decided in Phase 1, and iterations are assigned to the thread that owns most of the lefthand side elements. Currently, each iteration is executed asa whole by a single thread, but this may be changed in thefuture.In the cases where there are different access patterns inthe same loop, we use an evaluation strategy to pick up themost frequently visited pattern as the owner pattern of theloop.The translation of serial program sections is nontrivial.Since, in the SPMD scheme that we have adopted, all serialprogram sections will be executed by all threads, we need toidentify serial program regions where this is not appropriate. If variables updated in the sequential region are shared,then they should not be redundantly updated by all threads.Instead, they are marked to be executed by the master only.Furthermore, after such a masteronly section, a barrier synchronization is inserted. This ensures that other threads thatsubsequently read from these shared variables are properlydelayed until the new values are correctly seen.2.5 Communication GenerationIn this phase, function calls for messagepassing operations are generated and inserted. We compute the regionof distributed data that needs to be transferred. Messagepassing calls are inserted to perform the communication fordistributed data, in a copyincopyout manner. Data that isread within a parallel loop nest is fetched from its ownerbefore the loop. Data that is written within the loop must bewritten back to its owner afterward.Figure 1 presents the algorithm for message passing callgeneration. For each reference  to array A in the sourceprogram, the compiler must determine a sendreceive pairfor communicating the data accessed but not owned by theinvolved threads. This is done by intersecting two internalrepresentations, one describing the memory accesses madeby  on threadand the other describing the section of A4foreach reference  of distributed array Aaccess  computeLMAD  foreach i and j, i  jif  is readmode  supersetelsemode  preciseoverlap intersectdistribute, access , modeifoverlap  NULLsuccess generate messageoverlap, i, jifsuccess push backAendforendforFigure 1. MessagePassing Communicationreal A100omp distribute ACYCLIComp parallel do schedulestaticdo i  1, 100, 3... Ai ...enddoFigure 2. Loop accessing array Aowned by thread. For our internal representation we usethe LMAD Linear Memory Access Descriptor 7 notationwhich is more general than the conventional triplet notation.LMADs are usually able to capture precisely the accesses toa multidimensional array made by a sequence of multiplynested loops, even when the subscript expressions are notaffine.To compute the intersection between the region of an array accessed in one thread and the region owned by anotherthread, we use the LMAD intersection algorithm which isdescribed in 5. This intersection is stored into the variableoverlap. For example, consider an array, A, that has aCYCLIC distribution across four threads and is accessed inthe loop in Figure 2. The region stored in thread P0 can berepresented by the triplet A11004 and that stored inthread P2 by the triplet A31004. The region accessedin thread P0 can be represented by the triplet A1253.The intersection between the region accessed thread P0 andthe region stored in thread P2 is A72512The intersect function requires a mode parameter thatindicates what to do if the function cannot calculate the precise intersection. If the access is a read reference, it wouldnot affect the correctness of the code if we fetch more datathan needed from the other thread, so superset is specifiedfor mode. If the access is a write reference, the precisewriteback is required. In that case, precise is specified.region   ifmy id  isendAoffset, span1, data type, jifmy id  jreceiveAoffset, span1, data type, ia Transformation 1region   ,..MPI type vectorspanstride1, 1, stride, data type, NewTifmy id  isendAoffset, 1, NewT, jifmy id  jreceiveAoffset, 1, NewT, ib Transformation 2Figure 3. Using LMADs to generate messagepassing code. Only simple LMADs, equivalent to triplets, are used hereThen, if the intersection operation cannot be done precisely,the intersection function will report a failure. For example, consider a complicated reference like ABi, j1, distributed as A, BLOCK. If the access is a read,then we can conservatively fetch the whole column of A,so, the superset mode can be used. But if the access isa write, then the writeback must be precise. If the intersection procedure fails, due to being in precise mode, thearray must be handled by the SDSM system and thereforethe algorithm calls the push back function to turn the array back to the shared data class.A nonempty overlap result from the intersection operation indicates that communication is necessary. In the routine generate message, we try to convert the overlapregion to the proper message passing calls by transformations similar to those in Figure 3. The regions in Figure 3are the nonempty results from intersecting the distributionregions on thread with the regions accessed by thread .In Figure 3, represents the intersection between thedata stored in thread and data read by thread0. Thesegment of code below the lines describing the regionsare to be inserted before the loop reading the array so thatit is executed by thread i and j. Figure 3 gives two kinds oftransformations from LMAD represented as triplets herefor simplicity to the equivalent MPI message passing calls.Figure 3 a shows the transformation from a dense region,which has the stride of 1, to the equivalent MPI calls. Thethread where the data is stored sends the overlap data to the5thread that is going to access it. The beginning address ofthe message buffer is the offset of the overlap region. Thelength of the message buffer is span1.For the regions with , as shown in Figure 3 b, we build an MPI user data type to transfer thedata. The MPI type vector can be used to build the user datatype with the stride. It has the following interface 6    , . 0123The transformation in Figure 3 b can be extended tomultidimensional regions.For read references, the message passing calls are generated before the loop, to get the data before it is needed. Forwrite references, the message passing calls are generatedafter the loop, to write the data back to its owner. The message passing codes in Figure 3 are for read references. Thecommunication codes for write references can be generatedby simply switching the sender and the receiver.If some LMADs are too complicated to be converted intoproper message passing calls, the generate messagefunction will return a failure status. If a failure happens,the push back function will push the array back to theshared data class, letting the SDSM mechanism handle itscommunication.2.6 Additional Optimization to Improve OpenMPPerformance on SDSM SystemsThe methods presented in this paper mark the startingpoint of a project that seeks answers to the following questions4 Can OpenMP applications be executed efficiently ondistributed systems and computer grids, and underwhat parameters4 What are the compiler optimizations necessary to improve the efficiency of OpenMP applications runningon distributed systems and computer gridsIn this section we discuss further the second question. Wedescribe transformations that can optimize the performanceof OpenMP programs on a SDSM system. Many of thesetechniques have been discussed in other contexts. It is theircombined implementation in an OpenMPSDSM compilerthat we expect to have a significant impact. The realizationof such a compiler is one objective of our ongoing project.Data Prefetch Prefetch is a fundamental technique forovercoming memory access latencies. Ideally, a data item produced by thread 5 and consumed by 6 is sent to 6as soon as it is produced by 5 . Prefetch actions are initiated by the receiver  6 , in that the compiler moves theload of  upward in the instruction stream. Closelyrelatedto prefetch is data forwarding, which is producerinitiated.Forwarding has the advantage that it is a oneway communication  5 sends  to 6  whereas prefetching is a twowaycommunication  6 requests from 5 that  be sent. Animportant issue in prefetchforwarding is to determine theoptimal prefetch point. Choosing a prefetch point later thanthe earliest possible can be advantageous, as it reduces theneed for prefetched data to be cached by the recipient and itenables the combination of several prefetch operations intoa block transfer. Prefetch techniques have been studied previously albeit not in the context of OpenMP applications forSDSM systems. We expect that prefetch will significantlylower the cost of OpenMP END PARALLEL region constructs, as it reduces the need for coherence actions at thatpoint.Barrier elimination Two types of barrier eliminationswill become important. It is well known that the usualbarrier that separates two consecutive parallel loops canbe eliminated if permitted by data dependences 8. Similarly, within parallel regions containing consecutive parallelloops, it may be possible to eliminate the barrier separatingthe individual loops. As barrier costs are high in SDSM systems, this optimization can improve program performancesignificantly.In the OpenMPtoSDSM transformations described inSection 2.4, barriers are also introduced in serial programsections, after write operations to shared data. The barriercan be eliminated in some situations, such as, if the shareddata written are not read subsequently within the same serialsection.Data privatization The advantage of private data anddistributed data in a SDSM system is evident as they arenot subject to costly coherence actions. An extreme of aSDSM program can be viewed as a program that has onlyprivate and distributed, and no shared, data. The necessarydata exchanges between threads are performed by messageoperations which can be implemented as copy operationsin SMPs and NUMA machines. Many points are possibleinbetween this extreme and a program that has all OpenMPshared data placed in shared SDSM address space. For example, shared data with readonly accesses in certain program sections can be made private with copyin duringthese sections. Similarly, shared data that are exclusivelyaccessed by the same thread can be privatized during sucha program phase.We conducted an experiment where we manually appliedprivatization to readonly data in the equake benchmark.The program reads data in a serial region from a file. Inthe original code, since all the threads use the data, a sharedattribute is given. However, we have modified the programso that the input data, which are readonly after the initialization, become private to all threads. The results show thateven this simple optimization reduces the execution timessubstantially.6Page placement Some SDSM systems place memorypages on fixed home threads, pages may migrate betweenthreads, or pages are replicated this is the case in TreadMarks. Fixed page placement leads to high overhead ifthe chosen home is not the thread with the most frequentaccesses to this page. Migrating pages can incur high overhead if the pages end up changing their home frequently.Replicated pages can lead to high coherence traffic for keeping all page contents current. In all cases, the compiler canhelp direct the page management mechanism. It can estimate the number of accesses made to a page by all threadsand choose the best scheme. By considering the type of accesses it can decide in which program phases page replication is beneficial and when single ownership is appropriate.Furthermore, the compiler can choose a page size that accommodates the desired data structures. We expect that thecompilers ability to consider both large and future programphases will give it a significant advantage over the SDSMpage placement mechanism, which considers the programsrecent past only. As a result, such compiler mechanismswill take over substantial control from the SDSM system.Automatic data distribution The method described inSection 2 is still quite primitive. Its effectiveness needs tobe evaluated and the method extended, probably using someof the many methods discussed in the literature.Data and loop reorganization Data and loop transformation techniques for locality enhancement have also beenresearched extensively. We will study many of the proposedtechniques. We expect that the transformation leading todata affinity will be among the most effective. That is, programs in which successive loops cause individual threads toaccess the same data will be most successful.Adaptive optimization A big impediment for all compiler optimizations is the fact that input data is not knownat compiletime. Consequently, compilers must make conservative assumptions leading to suboptimal program performance. The potential performance degradation is especially high if the compilers decision making chooses between transformation variants whose performance differssubstantially. This is the case in distributed architecturesand SDSM systems, which exhibit a large number of parameters that need to be tuned.In ongoing work, we will build on a prototype of adynamically adaptive compilation system 11, 10, calledADAPT. The ADAPT system can dynamically compile program variants, substitute them at runtime, and evaluate themin the executing application.3 Extensions to OpenMPThe discussed compiler techniques can frequently generate parallel programs matching the performance of manually written messagepassing programs. However, we expect that in many other cases, compileroptimized performance will be limited. For example, with current technology the compiler may be unable to determine the best possible distribution of an array or the exact region of a distributed array that must be copied between threads beforeand after a parallel loop or parallel section.We believe that programmer should be able to use thesame programming paradigm when taking advantage ofcompileroptimized performance and when tuning this performance. This is important because the programmer maybe unable to determine whether the compiler will succeeduntil the program has been written. With stateofthe artprogramming support, programmers face the possibility ofhaving to start from scratch in this situation. Thereforethey may prefer to program entirely in a messagepassingparadigm, which is guaranteed to succeed, although harderto use than a shared memory paradigm.The solution we propose is to extend OpenMP so thatprogrammers would be able to override the compiler whenever necessary. The extended directives will be read by thecompiler and the information they contain will be used internally to guide code generation instead of the informationgathered by the compiler analysis algorithms. We envisionextensions to specify data distribution, computation distribution, and communication operations. One important advantage of specifying communication and distribution usingdirectives is that, in this way, the compiler will be able tocheck for consistency. Communication errors are typicallynot detected by todays compilers, which treat MPI messagepassing operations as calls to library routines and do not doany semantic analysis of the actual parameters.For data distribution, the notation used by HPF dataalignment should usually suffice for arrays accessed following a regular pattern throughout the program. We expect irregularly accessed array to be handled by the SDSM mechanism.A simple way to specify the location where a certaincomputation will be performed is to introduce a new clauseto specify where each iteration of a parallel do or eachsection within a sections directive will execute. Theclause schedulek could be used to specify that thesection will be executed by thread k, which could be any expression involving program variables. In the case of a parallel loop, a clause of the form schedulefp, where pis the index of the parallel loop, specify that iteration p willbe executed by thread number fp. The expression on therighthand side of the clause could include the home function. Given an element ai,j..., of a distributed array,the value of homeai,j, ... is the thread wherethe element has been assigned by a distribution directive.The first version of our OpenMP extensions will only allow explicit communications outside parallel regions. Thus,distributed data accessed in the parallel region could only7Qomp get num threadsomp parallel do distributedB, sharedA,omp schedulehomeBi,omp copy1,BUU,1do i  2,NAi  Bi  Bi1enddoFigure 4. example of the copy clausebe communicated before or after the region. In the simplest case, the data will be communicated just before or justafter the region, but performing the communication at otherpoints of the program to enable prefetching will also be possible. All the communication associated with a parallel region will be specified by a copy clause within the directivethat marks the beginning of the region.An example of how the copy directive may look like isshown in Figure 4. It is assumed in the Figure that vectorB is BLOCK distributed and that the intrinsic function UUreturns the number of the last element of B allocated on thecurrent thread by the BLOCK distribution. The first parameter of the copy clause specifies that data is to be transferred from thread p to thread p1, the second parameteris the beginning of the array region to be transferred, andthe third parameter is the length of the region. The copyclause makes sure that element Bi1 is available for thefirst iteration i executed by all threads except for thread 0.It will be possible to associate a label with the copyclause and, in a separate directive, specify that the copy operation is to be performed at a certain point of the programbefore execution of the parallel loop starts.Clearly, the extended OpenMP version of the loop is significantly more complex than originally and, for that reason,more errorprone for the programmer to write. This is theprice that one has to pay when the compiler is not powerfulenough to do the analysis and perform the appropriate transformations. Enhancing the compilers abilities and thus reducing this price is an important goal of the work presentedin this paper.4 ConclusionsWe have presented a number of compiler techniques thatcan be used to translate OpenMP programs for executionon distributed memory systems. Many of these techniques,which target the hybrid communication model, have already been implemented. Results obtained in preliminaryexperiments indicate that using the hybrid communicationmodel often produces significant performance gains. A newOpenMP prototype compiler targeted at distributed memorymachines is now under development.With this compiler andwith small extensions of OpenMP we expect that the question posed in the title can be answered affirmatively.5 AcknowledgmentsThe work reported in this paper was supported by NSFcontract EIA 0103582 Purdue and EIA 0103610 IllinoisReferences1 C. Amza, A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, W. Yu, and W. Zwaenepoel. TreadMarks SharedMemory Computing on Networks of Workstations. IEEEComputer, 2921828, February 1996.2 W. Blume, R. Doallo, R. Eigenmann, J. Grout, J. Hoeflinger, T. Lawrence, J. Lee, D. Padua, Y. Paek, W. Pottenger, L. Rauchwerger, and P. Tu. Parallel Programmingwith Polaris. IEEE Computer, 29127882, December1996.3 R. Chandra, L. Dagum, D. Kohr, D. Maydan, J. McDonald,and R. Menon. Parallel Programmig in OpenMP. MorganKaufmann Publshers, 2001.4 H. P. F. Forum. High Performance Fortran Language Specification, Version 1.0 . Scientific Programming, 21  2,1993.5 J. Hoeflinger and Y. Paek. A Comparative Analysis of Dependence Testing Mechanisms. In Thirteenth Workshop onLanguages and Compilers for Parallel Computing, August2000.6 Message Passing Interface Forum. MPI A MessagePassing Interface Standard , 1995.7 Y. Paek, J. Hoeflinger, and D. Padua. Simplication of ArrayAccess Patterns for Compiler Optimizations. Proceedings ofthe SIGPLAN Conference on Programming Language Design and Implementation, June 1998.8 C. Tseng. Compiler optimizations for eliminating barrier synchronization. In Proc. of the 5th ACM Symposium on Principles and Practice of Parallel ProgrammingPPOPP95, July 1995.9 M. Voss and R. Eigenmann. Dynamically adaptive parallel programs. In International Symposium on High Performance Computing, pages 109120, Kyoto, Japan, May1999.10 M. J. Voss and R. Eigenmann. A framework for remote dynamic program optimization. In Proc. of the ACM SIGPLANWorkshop on Dynamic and Adaptive Compilation and Optimization Dynamo00, held in conjunction with POPL00,Jan. 1999.11 M. J. Voss and R. Eigenmann. Highlevel adaptive program optimization with adapt. In Proc. of the ACM Symposium on Principles and Practice of Parallel ProgrammingPPOPP01, pages 93  102. ACM Press, June 2001.12 J. Zhu, J. Hoeflinger, and D. Padua. Compiling for a Hybrid Programming Model Using the LMAD Representation. In Fourteenth Workshop on Languages and Compilers forParallel Computing, August 2001.8
