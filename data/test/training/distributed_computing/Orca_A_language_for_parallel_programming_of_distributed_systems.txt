ORCA A LANGUAGE FOR PARALLELPROGRAMMING OF DISTRIBUTED SYSTEMSHenri E. Bal M. Frans KaashoekAndrew S. TanenbaumDept. of Mathematics and Computer ScienceVrije UniversiteitAmsterdam, The NetherlandsABSTRACTOrca is a language for implementing parallel applications on loosely coupled distributed systems. Unlike most languages for distributed programming, it allows processes ondifferent machines to share data. Such data are encapsulated in dataobjects, which areinstances of userdefined abstract data types. The implementation of Orca takes care of thephysical distribution of objects among the local memories of the processors. In particular, animplementation may replicate andor migrate objects in order to decrease access times toobjects and increase parallelism.This paper gives a detailed description of the Orca language design and motivates thedesign choices. Orca is intended for applications programmers rather than systems programmers. This is reflected in its design goals to provide a simple, easy to use language that istypesecure and provides clean semantics.The paper discusses three example parallel applications in Orca, one of which isdescribed in detail. It also describes one of the existing implementations, which is based onreliable broadcasting. Performance measurements of this system are given for three parallelapplications. The measurements show that significant speedups can be obtained for all threeapplications. Finally, the paper compares Orca with several related languages and systems.1. INTRODUCTIONAs communication in loosely coupled distributed computing systems gets faster, such systems become more and more attractive for running parallel applications. In the Amoeba system, for example, the cost of sending a short message between Sun workstations over an Ethernet is 1.1 milliseconds 1. Although this is still slower than communication in most multicomputers e.g., hypercubes and transputer grids, it is fast enough for many coarsegrainedparallel applications. In return, distributed systems are easy to build from offtheshelf A preliminary version of this paper appeared in the proceedings of the First UsenixSERC Workshop on Experiences withBuilding Distributed and Multiprocessor Systems, Ft. Lauderdale, Oct. 1989. This research was supported in part by the Netherlands organization for scientific research N.W.O. under grant 1253010. 2 components, by interconnecting multiple workstations or microprocessors through a localarea network LAN. In addition, such systems can easily be expanded to far larger numbersof processors than sharedmemory multiprocessors.In our research, we are studying the implementation of parallel applications on distributed systems. We started out by implementing several coarsegrained parallel applicationson top of the Amoeba system, using an existing sequential language extended with messagepassing for interprocess communication 2. We felt that, for parallel applications, both theuse of message passing and a sequential base language have many disadvantages, makingthem complicated for applications programmers to use.Since then, we have developed a new language for distributed programming, calledOrca 3, 4, 5. Orca is intended for distributed applications programming rather than systemsprogramming, and is therefore designed to be a simple, expressive, and efficient languagewith clean semantics. Below, we will briefly discuss the most important novelties in thelanguage design.Processes in Orca can communicate through shared data, even if the processors onwhich they run do not have physical shared memory. The main novelty of our approach isthe way access to shared data is expressed. Unlike shared physical memory or distributedshared memory 6, shared data in Orca are accessed through userdefined highlevel operations, which, as we will see, has many important implications.Supporting shared data on a distributed system imposes some challenging implementation problems. We have worked on several implementations of Orca, one of which we willdescribe in the paper. This system uses a reliable broadcast protocol. Both the protocol andthe integration with the rest of the system are new research results.Unlike the majority of other languages for distributed programming, Orca is not anextension to an existing sequential language. Instead, its sequential and distributed constructsespecially data structures have been designed together, in such a way that they integratewell. The language design addresses issues that are dealt with by few other languages. Mostdistributed languages simply add primitives for parallelism and communication to a sequential base language, but ignore problems due to poor integration with sequential constructs. Atypical example is passing a pointer in a message, which is usually not detected and maycause great havoc. Orca provides a solution to this problem, and keeps the semantics of thelanguage clean. At the same time, the Orca constructs are designed to have semantics closeto conventional languages, thus making it easy for programmers to learn Orca.An important goal in the design of Orca was to keep the language as simple as possible.Many interesting parallel applications exist outside the area of computer science, so thelanguage must be suitable for generalapplications programmers. Orca lacks lowlevelfeatures that would only be useful for systems programming. In addition, Orca reduces complexity by avoiding language features aimed solely at increasing efficiency, especially if thesame effect can be achieved through an optimizing compiler. Language designers frequentlyhave to choose between adding language features or adding compiler optimizations. In general, we prefer the latter option. We will discuss several examples of this design principle inthe paper. Finally, the principle of orthogonality 7 is used with care, but it is not a designgoal by itself. 3 Another issue we have taken into account is that of debugging. As debugging of distributed programs is difficult, one needs all the help one can get, so we have paid considerableattention to debugging. Most important, Orca is a typesecure language. The languagedesign allows the implementation to detect many errors during compiletime. In addition, thelanguage run time system does extensive error checking.The paper gives an overview of Orca, a distributed implementation of Orca, and its performance. It is structured as follows. In Section 2, we will describe the Orca language andmotivate our design choices. In Section 3, we will present an example application written inOrca. In Section 4, we will discuss one implementation of Orca, based on reliable broadcast.We will also describe how to implement this broadcast primitive on top of LANs that onlysupport unreliable broadcast. We will briefly compare this system with another implementation of Orca that uses Remote Procedure Call 8 rather than broadcasting. In Section 5, wewill give performance measurements for several applications. In Section 6, we will compareour approach with those of related languages and systems. Finally, in Section 7 we willpresent our conclusions.2. ORCAOrca is a procedural, strongly typed language. Its sequential statements and expressions arefairly conventional and are roughly comparable although not identical to those of Modula2. The data structuring facilities of Orca, however, are substantially different from thoseused in Modula2. Orca supports records, unions, dynamic arrays, sets, bags, and generalgraphs. Pointers have intentionally been omitted to provide security. Also, the languagelacks global variables, although such variables can be simulated by passing them around asreference parameters.The rest of this section is structured as follows. We will first motivate our choice forshared data over message passing. Next, we will look at processes, which are used forexpressing parallelism. Subsequently, we will describe Orcas communication model, whichis based on shared dataobjects. Synchronization of operations on shared objects is discussednext, followed by a discussion of hierarchically used objects. Finally, we will look at Orcasdata structures.2.1. Distributed Shared MemoryMost languages for distributed programming are based on message passing 9. This choiceseems obvious, since the underlying hardware already supports message passing. Still, thereare many cases in which message passing is not the appropriate programming model. Message passing is a form of communication between two parties, which interact explicitly bysending and receiving messages. Message passing is less suitable, however, if severalprocesses need to communicate indirectly, by sharing global state information.There are many examples of such applications. For example, in parallel branchandbound algorithms the current best solution the bound is stored in a global variable accessedby all processes. This is not to say the algorithms actually need physical shared memorythey merely need logically shared data. Such algorithms are much harder to implement efficiently using message passing than using shared data.The literature contains numerous other examples of distributed applications and 4 algorithms that would greatly benefit from support for shared data, even if no physical sharedmemory is available. Applications described in the literature include a distributed speechrecognition system 10 linear equation solving, threedimensional partial differential equations, and splitmerge sort 11 computer chess 12 distributed system services e.g., nameservice, time service, global scheduling, and replicated files 13.So, the difficulty in providing logically shared data makes message passing a poormatch for many applications. Several researchers have therefore worked on communicationmodels based on logically shared data rather than message passing. With these models, theprogrammer can use shared data, although the underlying hardware does not provide physicalshared memory. A memory model that looks to the user as a shared memory but is implemented on disjoint machines is referred to as Distributed Shared Memory DSM.Many different forms of DSM exist. Lis Shared Virtual Memory SVM 6 is perhapsthe bestknown example. It simulates physical sharedmemory on a distributed system. TheSVM distributes the pages of the memory space over the local memories. Readonly pagesmay also be replicated. SVM provides a clean, simple model, but unfortunately there aremany problems in implementing it efficiently.A few existing programming languages also fall into the DSM class. Linda 14 supports a globally shared Tuple Space, which processes can access using a form of associativeaddressing. On distributed systems, Tuple Space can be replicated or partitioned, much aspages in SVM are. The operations allowed on Tuple Space are lowlevel and builtin, which,as we will argue later, complicates programming and makes an efficient distributed implementation hard.The Emerald language 15 is related to the DSM class, in that it provides a shared namespace for objects, together with a locationtransparent invocation mechanism. Emerald doesnot use any of the replication techniques that are typical of DSM systems, however.The most important issue addressed by Orca is how data can be shared among distributed processes in an efficient way. In languages for multiprocessors, shared data structuresare stored in the shared memory and accessed in basically the same way as local variables,namely through simple load and store instructions. If a process is going to change part of ashared data structure and it does not want other processes to interfere, it locks that part. Allthese operations loads, stores, locks on shared data structures involve little overhead,because access to shared memory is hardly more expensive than access to local memory.In a distributed system, on the other hand, the time needed to access data very muchdepends on the location of the data. Accessing data on remote processors is orders of magnitude more expensive than accessing local data. It is therefore infeasible to apply the multiprocessor model of programming to distributed systems. The operations used in this modelare far too lowlevel and will have tremendous overhead on distributed systems.The key idea in Orca is to access shared data structures through higher level operations.Instead of using lowlevel instructions for reading, writing, and locking shared data, we letprogrammers define composite operations for manipulating shared data structures. Shareddata structures in our model are encapsulated in socalled dataobjects1 that are manipulatedthrough a set of userdefined operations. Dataobjects are best thought of as instances 1 We will sometimes use the term object as a shorthand notation. Note, however, that this term isused in many other languages and systems, with various different meanings. 5 variables of abstract data types. The programmer specifies an abstract data type by defining operations that can be applied to instances dataobjects of that type. The actual datacontained in the object and the executable code for the operations are hidden in the implementation of the abstract data type.2.2. ProcessesParallelism in Orca is explicit, because compilers currently are not effective at generatingparallelism automatically. Implicit parallelism may be suitable for vector machines, but,with the current stateoftheart in compiler technology, it is not effective for distributed systems.Parallelism is expressed in Orca through explicit creation of sequential processes.Processes are conceptually similar to procedures, except that procedure invocations are serialand process invocations are parallel.Initially, an Orca program consists of a single process, but new processes can be createdexplicitly through the fork statementfork nameactualparameters  on cpunumber This statement creates a new, anonymous, child process. Optionally, the new process can beassigned to a given processor. Processors are numbered sequentially the fork statement maycontain an onpart with an expression that specifies the processor on which to run the childprocess. If the onpart is absent, the child process is created on the same processor as itsparent. The system does not move processes around on its own initiative, since this isundesirable for many parallel applications.A process can take parameters, as specified in its definition. Two kinds are allowedinput and shared. A process may take any kind of data structure as value input parameter.In this case, the process gets a copy of the actual parameter. The parent can also pass any ofits dataobjects as a shared parameter to the child. In this case, the dataobject will be sharedbetween the parent and the child. The parent and child can communicate through this sharedobject, by executing the operations defined by the objects type, as will be explained later.For example, if a process child is declared asprocess childId integer X shared AnObjectType begin ... enda new child process can be created as followsMyObj AnObjectType  declare an object.... create a new child process, passing the constant 12 as value parameter and the object MyObj as shared parameter.fork child12, MyObjThe children can pass shared objects to their children, and so on. In this way, theobjects get distributed among some of the descendants of the process that created them. Ifany of these processes performs an operation on the object, they all observe the same effectas if the object were in shared memory, protected by a lock variable. 6 2.3. Shared dataobjects and abstract data typesA shared dataobject is a variable of an abstract data type object type. An abstract data typedefinition in Orca consists of two parts a specification part and an implementation part. Thespecification part defines the operations applicable to objects of the given type. As a simpleexample, the specification part of an object type encapsulating an integer is shown in Figure1.object specification IntObjectoperation Value integer  return current valueoperation Assignval integer  assign new valueoperation Addval integer  add val to current valueoperation Minval integer  set value to minimum of current value and valend  Fig. 1. Specification part of an object type IntObject.The implementation part contains the data used to represent objects of this type, thecode to initialize the data of new instances of the type, and the code implementing the operations. Part of the implementation of type IntObject is shown in Figure 2.object specification IntObjectx integer  internal dataoperation Value integerbeginreturn x  return the current valueendoperation Assignv integerbeginx  v  assign a new valueend...beginx  0  initialize objects to zeroendFig. 2. Implementation part of an object type IntObject.An operation implementation is similar to a procedure. An operation can only access itsown local variables and parameters and the local internal data of the object it is applied to.Once an object type has been defined, instances objects of the type can be created bydeclaring variables of the type. When an object is created, memory for the local variables ofthe object is allocated and the initialization code is executed. From then on, operations canbe applied to the object. The Orca syntax to declare an object and apply an operation to it isillustrated below 7 X IntObjecttmp integerXAssign3  assign 3 to XXAdd1  increment Xtmp  XValue  read current value of XOrca supports a single abstract data type mechanism, which can be used for encapsulating shared and nonshared data. In other words, the mechanism can also be used for regularsequential abstract data types. Even stronger, the same abstract type can be used for creating shared as well as local objects. Neither object declarations nor objecttype declarationsspecify whether objects will be shared. This information is derived from the usage ofobjects only objects that are ever passed as shared parameter in a fork statement are shared.All other objects are local and are treated as normal variables of an abstract data type.Most other languages use different mechanisms for these two purposes. Argus 16, forexample, uses clusters for local data and guardians for shared data clusters and guardians arecompletely different. SR 17 provides a single mechanism resources, but the overhead ofoperations on resources is far too high to be useful for sequential abstract data types 18.The fact that shared data are accessed through userdefined operations is an importantdistinction between our model and other models. Shared virtual memory, for example, simulates physical shared memory, so shared data are accessed through lowlevel read and writeoperations. Lindas Tuple Space model also uses a fixed number of builtin operations toadd, read, and delete shared tuples. Having users define their own operations has manyadvantages, both for the ease of programming and for the implementation, as we will discussshortly.Although dataobjects logically are shared among processes, their implementation doesnot need physical shared memory. In worst case, an operation on a remote object can beimplemented using message passing. The general idea, however, is for the implementation totake care of the physical distribution of dataobjects among processors. As we will see inSection 4, one way to achieve this goal is to replicate shared dataobjects. By replicatingobjects, access control to shared objects is decentralized, which decreases access costs andincreases parallelism. This is a major difference with, say, monitors 19, which centralizecontrol to shared data.2.4. SynchronizationAn abstract data type in Orca can be used for creating shared as well as local objects. Forobjects that are shared among multiple processes, the issue of synchronization arises. Twotypes of synchronization exist mutual exclusion synchronization and condition synchronization 20. We will look at them in turn.Mutual exclusion synchronizationMutual exclusion in our model is done implicitly, by executing all operations on objects indivisibly. Conceptually, each operation locks the entire object it is applied to, does the work,and releases the lock only when it is finished. To be more precise, the model guarantees serializability 21 of operation invocations if two operations are applied simultaneously to thesame dataobject, then the result is as if one of them is executed before the other the order of 8 invocation, however, is nondeterministic.An implementation of the model need not actually execute all operations one by one.To increase the degree of parallelism, it may execute multiple operations on the same objectsimultaneously, as long as the effect is the same as for serialized execution. For example,operations that only read but do not change the data stored in an object can easily be executed in parallel.Since users can define their own operations on objects, it is up to the user to decidewhich pieces of code should be executed indivisibly. For example, an abstract data typeencapsulating an integer variable may have an operation to increment the integer. Thisoperation will be done indivisibly. If, on the other hand, the integer is incremented throughseparate read and write operations i.e., first read the current value, then write the incremented value back, the increment will be done as two separate actions, and will thus not beindivisible. This rule for defining which actions are indivisible and which are not is botheasy to understand and flexible single operations are indivisible sequences of operations arenot. The model does not provide mutual exclusion at a granularity lower than the objectlevel. Other languages e.g., Sloop 22 give programmers more accurate control overmutual exclusion synchronization.Our model does not support indivisible operations on a collection of objects. Operationson multiple objects require a distributed locking protocol, which is complicated to implementefficiently. Moreover, this generality is seldom needed by parallel applications. We preferto keep our basic model simple and implement more complicated actions on top of it. Operations in our model therefore apply to single objects and are always executed indivisibly.However, the model is sufficiently powerful to allow users to construct locks for multioperation sequences on different objects, so arbitrary actions can be performed indivisibly.Condition synchronizationThe second form of synchronization is condition synchronization, which allows processes towait block until a certain condition becomes true. In our model, condition synchronizationis integrated with operation invocations by allowing operations to block. Processes synchronize implicitly through operations on shared objects. A blocking operation consists ofone or more guarded commandsoperation opformalparameters ResultTypebeginguard condition1 do statements1 od...guard conditionn do statementsn odendThe conditions are Boolean expressions, called guards. To simplify the presentation, we willinitially assume that guards are sideeffect free. The problem of side effects will be considered later, when discussing hierarchically used objects.The operation initially blocks until at least one of the guards evaluates to true. Next,one true guard is selected nondeterministically, and its sequence of statements is executed.The Boolean expressions may depend on the parameters and local data of the operationand on the data of the object. If a guard fails, it can later become true, after the state of the 9 object has been changed. It may thus be necessary to evaluate the guards several times.We have chosen this form of condition synchronization because it is highly simple andfits well into the model. An alternative approach that we considered and rejected is to use aseparate synchronization primitive, independent of the mechanism for shared objects. Toillustrate the difference between these two alternatives, we will first look at a specific example.Consider a shared Queue object with operations to add elements to the tail and retrieveelements from the headoperation Addx item  add to tailoperation Get item  get from headA process trying to fetch an element from an empty queue should not be allowed to continue.In other words, the number of Get operations applied to a queue should not exceed thenumber of Add operations. This is an example of a synchronization constraint on the order inwhich operations are executed. There are at least two conceivable ways for expressing suchconstraints in our model1. Processes trying to execute Get should first check the status of the queue andblock while the queue is empty. Doing a Get on an empty queue results in anerror.2. The Get operation itself blocks while the queue is empty. Processes executing a Get on an empty queue therefore block automatically.In both cases, a new primitive is needed for blocking processes. In the first case this primitive is to be used directly by user processes in the second case only operations on objects useit. Also, the first approach calls for an extra operation on queues that checks if a given queueis empty. For both approaches, unblocking the process and removing the head element fromthe queue should be done in one indivisible action, to avoid race conditions.The first approach has one major drawback the users of an object are responsible forsatisfying synchronization constraints. This is in contrast with the general idea of abstractdata types to hide implementation details of objects from users. The second approach ismuch cleaner, as the implementer of the object takes care of synchronization and hides itfrom the users. We therefore use the second approach and do condition synchronizationinside the operations. The model allows operations to block processes can only block byexecuting operations that block.An important issue in the design of the synchronization mechanism is how to provideblocking operations while still guaranteeing the indivisibility of operation invocations. If anoperation may block at any point during its execution, operations can no longer be serialized.Our solution is to allow operations only to block initially, before modifying the object. Anoperation may wait until a certain condition becomes true, but once it has started executing, itcannot block again. 10 2.5. Hierarchical objectsAbstract data types are useful for extending a language with new types. This method forbuilding new types is hierarchical existing abstract data types can be used to build new ones.The internal data of an object can therefore themselves be objects. Note that hierarchicalobjects are not derived from the constituent objects by extending them as can be done inobjectoriented languages. The old and new objects have a use relation, not an inheritance relation.This nesting of objects causes a difficult design problem, as we will explain below.Suppose we have an existing object type OldType, specified as followsobject specification OldTypeoperation OldOperation1 booleanoperation OldOperation2endWe may use this object type in the implementation of another type we omit the specificationof this typeobject implementation NewTypeNestedObject OldType  a nested objectoperation NewOperationbeginguard NestedObjectOldOperation1 do...NestedObjectOldOperation2odendendObjects of the new type contain an object, NestedObject, of type OldType. The latter objectis called a nested object, because it is part of another object. Note that instances of NewTypeare still single objects whose operations are executed indivisibly. The nested object is invisible outside its enclosing object, just like any other internal data.The implementer of NewType can be seen as a user of OldType. So, the implementer ofNewType does not know how OldType is implemented. This lack of information about theimplementation of the operations on OldType causes two problems.The first problem is illustrated by the use of OldOperation1 in the guard of NewOperation. We need to know whether the guard expressions have side effects, as they may have tobe evaluated several times. Unfortunately, we do not know whether the invocation of OldOperation1 has any side effects. If the operation modifies NestedObject, it does have sideeffects. We can only tell so, however, by looking at the implementation of this operation,which goes against the idea of abstract data types.The second problem is more subtle. Suppose a process declares an object NewObject oftype NewType and shares it with some of its child processes. If one of the processes invokesNewOperation on NewObject, the implementation of this object will invoke OldOperation2on the nested object. The problem is that the latter operation may very well block. If so, weviolate the rule that operations are only allowed to block initially. In this situation, there aretwo equally unattractive options1. Suspend the process invoking NewOperation, but allow other processes to 11 access the object. This means, however, that the operation will no longer beindivisible.2. Block the calling process, but do not allow any other processes to access theobject. This implies that the process will be suspended forever, because noother process will be able to modify NestedObject.One could solve this problem by disallowing blocking operations on nested objects, but againthis requires looking at the implementation of an operation to see how it may be used.Cooper and Hamilton have observed similar conflicts between parallel programmingand data abstraction in the context of monitors 23. They propose extending operationspecifications with information about their implementation, such as whether or not the operation suspends or has any side effects. We feel it is not very elegant to make such concessions, however. The specification of an abstract data type should not reveal informationabout the implementation.We solve these two problems by refining the execution model of operations. Conceptually, an operation is executed as follows. The operation repeatedly tries to evaluate its guardsand then tries to execute the statements of a successful guard. Before evaluating a guard,however, the operation conceptually creates a copy of the entire object, including anynested or deeply nested objects. This copy is used during the evaluation of the guard andexecution of the statements. The operation commits to a certain alternative, as soon as both1. The guard succeeds evaluates to true, and2. The corresponding statements can be executed without invoking any blocking operations on nested objects.As soon as a guard fails or the statements invoke a blocking operation, the copy of the entireobject is thrown away and another alternative is tried. So, an operation does not commit untilit has finished executing a successful guard and its corresponding statements, without invoking any blocking operations on nested objects. If all alternatives of an operation fail, theoperation and the process invoking it blocks until the object is modified by another process.If an operation commits to a certain alternative, the object is assigned the current value of thecopy i.e., the value after evaluating the selected guard and statements.This scheme solves both of the above problems. An operation on a nested object usedinside a guard e.g., OldOperation1 in the code above may have side effects these sideeffects will not be made permanent until the guard is actually committed to. An operation ona nested object may also block. As long as all guards of that operation fail, however, thealternative containing the invocation will never be committed to. The operation has noeffects until it commits to a certain alternative. Before commitment, it may try some alternatives, but their effects are thrown away. If the operation commits to an alternative, both theguards and statements of the alternative are executed without blocking. Therefore, operationinvocations are still executed indivisibly.The key issue is how to implement this execution model efficiently. It is quite expensive to copy objects before trying each alternative. In nearly all cases, however, the compilerwill be able to optimize away the need for copying objects. Many object types will not haveany nested objects, so they do not suffer from the problems described above. Also, an optimizing compiler can check if an operation used in a guard or body is sideeffect free and 12 nonblocking. To do so, it needs to access the implementation code of nested objects. This isnot any different from other global optimizations e.g., inline substitution, which basicallyneed to access the entire source program. Also, the same mechanism can be used to test forcircularities in nested object definitions.Our solution therefore preserves abstraction from the programmers point of view, butsometimes requires global optimizations to be efficient. The current Orca compiler performsthese optimizations. This approach keeps the language simple and relies on optimizationtechniques for achieving efficiency.2.6. Data structuresIn most procedural languages, data structures like graphs, trees, and lists are built out ofdynamically allocated and deallocated blocks of memory, linked together through pointers.For distributed programming, this approach has many disadvantages. The main difficulty ishow to transmit a complex data structure containing pointers to a remote machine. Pointers,if implemented as addresses, are only meaningful within a single machine, so they need special treatment before being transmitted. Even more important, most languages do not consider such graphs to be firstclass objects, so it is hard to determine what has to be transmitted.In addition to these problems, giving the programmer explicit control over allocationand deallocation of memory usually violates type security. A programmer can deallocatememory and then use it again, leading to obscure bugs.In Orca, these problems are solved through the introduction of a graph data type. Agraph in Orca consists of zero or more nodes, each having a number of fields, similar to thefields of a record. Also, the graph itself may contain global fields, which are used to storeinformation about the entire graph e.g., the root of a tree or the head and tail of a list. Individual nodes within a graph are identified by values of a nodename type. A variable or fieldof a nodename type is initialized to NIL, which indicates it does not name any node yet. Asan example, a binarytree type may be defined as followstype node  nodename of BinTreetype BinTree graph  global fieldroot node  name of the root of the treenodes  fields of each nodedata integerLeftSon,RightSon node  names of left and right sonsendThis program fragment declares a graph type BinTree. Each node of such a graph contains adata field and fields identifying the left and right sons of the node. Furthermore, the graphhas one global field, identifying the root node of the tree.A tree data structure is created by declaring a variable of this type. Initially, the tree isempty, but nodes can be added and deleted dynamically as follows 13 t BinTreen noden  addnodet  add a node to t, store its name in ndeletenodet, n  delete the node with given name from tThe construct addnode adds a new node to a graph and returns a unique name for it,chosen by the run time system. The run time system also automatically allocates memory forthe new node. In this sense, addnode is similar to the standard procedure new in Pascal 24.As a crucial difference between the two primitives, however, the addnode construct specifiesthe data structure for which the new block of memory is intended. Unlike in Pascal, the runtime system of Orca can keep track of the nodes that belong to a certain graph. This information is used whenever a copy of the graph has to be created, for example when it is passed asa value parameter to a procedure or remote process. Also, the information is used to deletethe entire graph at the end of the procedure in which it is declared.The global fields of a graph and the fields of its nodes are accessed through designatorsthat are similar to those for records and arrayst.root  n  access the global field of ttn.data  12  access data field of node ntn.LeftSon  addnodet  create left son of nn  tn.LeftSon  store name of left son in nNote that the designator for the field of a node specifies the name of the node as well as thegraph itself. This notation differs from the one in Pascal, where nodes are identified bypointers only. The notation of Orca may be somewhat more cumbersome, but it has theadvantage that it is always clear which data structure is accessed. Also, it makes it possibleto represent a nodename as an index into a graph, rather than as a machine address.Nodenames can therefore be transmitted to remote machines without losing their meaning.Graphs in Orca are typesecure. If a certain node is deleted from a graph and one of itsfields is subsequently accessed, a runtime error occurs, as illustrated by the following pieceof coden  addnodetdeletenodet, ntn.data  12  causes a runtime errorThe run time system checks whether the graph t contains a node with the given name. Furthermore, each invocation of addnodet returns a different name, so the same nodename willnot be reused for denoting a different node. Whenever a node has been deleted from agraph, any future references to the node will cause a runtime error.The data structuring mechanism of Orca has some properties of arrays and some properties of pointerbased data structures. The mechanism supports dynamic allocation of memorythrough the addnode primitive. Graphs, like arrays, are firstclass entities in Orca. Thisdesign has several advantages they can easily be passed to remote processes assignment isdefined for graph variables functions may return a value of a graph type and graphs areautomatically deallocated at the end of their enclosing procedure. The latter feature reducesthe need for automatic garbage collection of nodes. Nodenames in Orca have the safetyadvantages of both pointers and array indices. Like pointers, they cannot be manipulatedthrough arithmetic operations like array indices, any illegal usage of a nodename will be 14 detected at run time.The graph type of Orca also has some disadvantages, compared to pointers. Withpointers, for example, any two data structures can be hooked together through a singleassignment statement. With graphs, this is more difficult. If the programmer anticipates thejoin, the data structures can be built using a single graph. If separate graphs are used, onewill have to be copied into the other.Another disadvantage is the runtime overhead of graphs. A graph is represented as atable with pointers to the actual nodes, so the nodes are accessed indirectly through thistable 3. Also, there is a cost in making graphs typesecure, since each node access has to bevalidated. We are currently working on decreasing these costs through global optimizations.3. AN EXAMPLE OBJECT TYPE AND APPLICATIONIn this section, we will give an example of an object type definition in Orca and of a parallelapplication that uses this object type. The object defines a generic job queue type, withoperations to add and delete jobs. It is used in several parallel programs based on the replicated workers paradigm. With this paradigm, a master process repeatedly generates jobs tobe executed by workers. Communication between the master and workers takes placethrough the job queue. One such application, parallel branchandbound, will be discussed.3.1. An example object typeThe specification of the object type GenericJobQueue is shown in Figure 3. The formalparameter T represents the type of the elements jobs of the queue.generic type Tobject specification GenericJobQueueoperation AddJobjob T  add a job to the tail of the queueoperation NoMoreJobs  invoked when no more jobs will be addedoperation GetJobjob out T boolean Fetch a job from the head of the queue. This operation fails if the queue is empty and NoMoreJobs has been invoked.end generic  Fig. 3. Specification part of the object type definition GenericJobQueue.Three different operations are defined on job queues. AddJob adds a new job to the tailof the queue. The operation NoMoreJobs is to be called when no more jobs will be added tothe queue i.e., when the master has generated all the jobs. Finally, the operation GetJobtries to fetch a job from the head of the queue. If the queue is not empty, GetJob removes thefirst job from the queue and returns it through the out parameter job the operation itselfreturns true in this case. If the queue is empty and the operation NoMoreJobs has beenapplied to the queue, the operation fails and returns false. If none of these twoconditionsqueue not empty or NoMoreJobs invokedholds, the operation blocks until oneof them becomes true.The implementation part is shown in Figure 4. Objects of this type contain two variables a Boolean variable done and a variable Q of type queue. The latter type is defined as a 15 graph with two global fields, identifying the first and last element of the queue. Each element contains the nodename of the next element in the queue and data of formal type T.The implementation of AddJob uses straightforward list manipulation. The GetJoboperation is more interesting. It contains two guards, reflecting the two conditions describedabove.3.2. An example parallel application in OrcaWe will now look at one example application in Orca the traveling salesman problem TSP.A salesman is given an initial city in which to start, and a list of cities to visit. Each citymust be visited once and only once. The objective is to find the shortest path that visits allthe cities. The problem is solved using a parallel branchandbound algorithm.The algorithm we have implemented in Orca uses one manager process to generate initial paths for the salesman, starting at the initial city but visiting only part of the other cities.A number of worker processes further expand these initial paths, using the nearestcityfirst heuristic. A worker systematically generates all paths starting with a given initial pathand checks if they are better than the current shortest full path. The length of the current bestpath is stored in a dataobject of type IntObject see Figure 1. This object is shared amongall worker processes. The manager and worker processes communicate through a shared jobqueue, as shown in Figure 5.The Orca code for the master and worker processes is shown in Figure 6. The masterprocess creates and initializes the shared object minimum, and forks one worker process oneach processor except its own one. Subsequently, it generates the jobs by calling a functionGenerateJobs not shown here and then forks a worker process on its own processor. In thisway, job generation executes in parallel with most of the worker processes. The final workerprocess is not created until all jobs have been generated, so job generation will not be sloweddown by a competing process on the same processor.Each worker process repeatedly fetches a job from the job queue and executes it by calling the function tsp. The tsp function generates all routes that start with a given initial route.If the initial route passed as parameter is longer than the current best route, tsp returnsimmediately, because such a partial route cannot lead to an optimal solution. If the routepassed as parameter is a full route visiting all cities, a new best route has been found, so thevalue of minimum should be updated. It is possible, however, that two or more workerprocesses simultaneously detect a route that is better than the current best route. Therefore,the value of minimum is updated through the indivisible operation Min, which checks if thenew value presented is actually less than the current value of the object.If the job queue is empty and no more jobs will be generated, the operation GetJob willreturn false and the workers will terminate.4. A DISTRIBUTED IMPLEMENTATION OF ORCAAlthough Orca is a language for programming distributed systems, its communication modelis based on shared data. The implementation of the language therefore should hide the physical distribution of the hardware and simulate shared data in an efficient way. We haveseveral implementations of the language 3. The implementation described in this paper is 16 genericobject implementation GenericJobQueuetype ItemName  nodename of queuetype queue graph  a queue is represented as a linear listfirst, last ItemName  firstlast element of queuenodesnext ItemName  next element in queuedata T  data contained by this elementenddone boolean  set to true if NoMoreJobs has been invoked.Q queue  the queue itselfoperation AddJobjob Tp ItemNamebegin  add a job to the tail of the queuep  addnodeQ  add a new node to Q, return its name in pQp.data  job  fill in data field of the new node next field is NILif Q.first  NIL then  Is it the first nodeQ.first  p  yes assign it to global data fieldelseQQ.last.next  p  no set predecessors next fieldfiQ.last  p  Assign to last global data fieldendoperation NoMoreJobsbegin  Invoked to indicate that no more jobs will be addeddone  trueendoperation GetJobjob out T booleanp ItemNamebegin  Try to fetch a job from the queueguard Q.first  NIL do  A job is availablep  Q.first  Remove it from the queueQ.first  Qp.nextif Q.first  NIL then Q.last  NIL fijob  Qp.data  assign to output parameterdeletenodeQ,p  delete the node from the queuereturn true  succeeded in fetching a jobodguard done and Q.first  NIL doreturn false  All jobs have been doneodendbegin  Initialization code for JobQueues  executed on object creation.done  false  initialize done to falseend generic  Fig. 4. Implementation part of the object type definition GenericJobQueue. 17 Manager WorkerWorkerWorkerjob job...JobQueue MinimumFig. 5. Structure of the Orca implementation of TSP. The Manager and Workersare processes. The JobQueue is a dataobject shared among all these processes.Minimum is a dataobject of type IntObject it is read and written by all workers.based on replication and reliable broadcasting. We will briefly discuss a second implementation in Section 4.4.Replication of data is used in several faulttolerant systems e.g., ISIS 25 to increasethe availability of data in the presence of processor failures. Orca, in contrast, is not intendedfor faulttolerant applications. In our implementation, replication is used to decrease theaccess costs to shared data.Briefly stated, each processor keeps a local copy of each shared dataobject. This copycan be accessed by all processes running on that processor see Figure 7. Operations that donot change the object called read operations use this copy directly, without any messagesbeing sent. Operations that do change the object called write operations broadcast the newvalues or the operations to all the other processors, so they are updated simultaneously.The implementation is best thought of as a three layer software system, as shown below compiled application programs run time system reliable broadcasting The top layer is concerned with applications, which are written in Orca and compiled tomachine code by the Orca compiler. The executable code contains calls to the Orca run timesystem for example, to create and manipulate processes and objects.The middle layer is the run time system RTS. It implements the primitives called bythe upper layer. For example, if an application performs an operation on a shared dataobject, it is up to the RTS to ensure that the system behaves as if the object was placed inshared memory. To achieve this, the RTS of each processor maintains copies of sharedobjects, which are updated using reliable broadcasting.The bottom layer is concerned with implementing the reliable broadcasting, so that the 18 type PathType  arrayinteger of integertype JobType recordlen integer  length of partial routepath PathType the partial route itselfendtype DistTab  ...  distances tableobject TspQueue  new GenericJobQueueJobType Instantiation of the GenericJobQueue typeprocess masterminimum IntObject  length of current best path shared objectq TspQueue  the job queue shared objecti integerdistance DistTab  table with distances between citiesbeginminimumassignMAXinteger  initialize minimum to infinityfor i in 1.. NCPUS  1 do fork one worker per processor, except current processorfork workerminimum, q, distance oniodGenerateJobsq, distance  main thread generates the jobsqNoMoreJobs  all jobs have been generated nowfork workerminimum, q, distance on0 jobs have been generated fork a worker on this cpu tooendprocess workerminimum shared IntObject  length of current best pathq shared TspQueue  job queuedistance DistTab  distances between citiesjob JobTypebeginwhile qGetJobjob do  while there are jobs to dotspjob.len, job.path, minimum, distance do sequential tspodend  Fig. 6. Orca code for the master and worker processes of TSP.RTS does not have to worry about what happens if a broadcast message is lost. As far as theRTS is concerned, broadcast is error free. It is the job of the bottom layer to make it work.Below, we will describe the protocols and algorithms in each layer. This section isstructured top down we first discuss the applications layer, then the RTS layer, and finallythe reliable broadcast layer. 19 process1processncopyofXCPU 1process1processncopyofXCPU 2n e t w o r kFig. 7. Replication of dataobjects in a distributed system4.1. Top layer Orca application programsApplication programs are translated by the Orca compiler into executable code for the targetsystem.2 The code produced by the compiler contains calls to RTS routines that manageprocesses, shared dataobjects, and complex data structures e.g., dynamic arrays, sets, andgraphs. In this paper, we will only discuss how operation invocations are compiled.As described above, it is very important to distinguish between read and write operations on objects. The compiler therefore analyses the implementation code of each operationand checks whether the operation modifies the object to which it is applied.3 In mostlanguages, this optimization would be difficult to implement. Consider, for example, a Pascal statement containing an indirect assignment through a pointer variablep.f  0It is hard to determine which data structure is affected by this statement. Orca does not havethis problem, since the name of the data structure is given by the programmer. The Orcaequivalent of the Pascal code given above would look likeGn.f  0which explicitly specifies the name of the data structure that will be modified. So, in Orcathe compiler can determine which operations modify the objects data structures and whichdo not.The compiler stores its information in an operation descriptor. This descriptor alsospecifies the sizes and modes input or output of the parameters of the operation. If an Orcaprogram applies an operation on a given object, the compiler generates a call to the RTSprimitive INVOKE. This routine is called as followsINVOKEobject, operationdescriptor, parameters ...The first argument identifies the object to which the operation is applied. It is a network 2 We assume the target system does not contain multiple types of CPUs. Although a heterogeneousimplementation of Orca is conceivable, we do not address this issue here.3 The actual implementation is somewhat more complicated, since an operation may have multipleguards alternatives, some of which may be readonly. 20 wide name for the object. The second argument is the operation descriptor. The remainingarguments of INVOKE are the parameters of the operation. The implementation of this primitive is discussed below.4.2. Middle layer The Orca run time systemThe middle layer implements the Orca run time system. As mentioned above, its primary jobis to manage shared dataobjects. In particular, it implements the INVOKE primitivedescribed above. For efficiency, the RTS replicates objects so it can apply operations to localcopies of objects whenever possible.There are many different design choices to be made related to replication, such as whereto replicate objects, how to synchronize write operations to replicated objects, and whether toupdate or invalidate copies after a write operation. We have looked at many alternative strategies 26. The RTS described in this paper uses full replication of objects, updates replicasby applying write operations to all replicas, and implements mutual exclusion synchronization through a distributed update protocol.The full replication scheme was chosen for its simplicity and good performance formany applications. An alternative is to let the RTS decide dynamically where to store replicas. This strategy is employed in another implementation of Orca 26.We have chosen to use an update scheme rather than an invalidation scheme for tworeasons. First, in many applications objects contain large amounts of data e.g., a 100K bitvector. Invalidating a copy of such an object is wasteful, since the next time the object isreplicated its entire value must be transmitted. Second, in many cases updating a copy willtake no more CPU time and network bandwidth than sending invalidation messages.The presence of multiple copies of the same logical data introduces the socalled inconsistency problem. If the data are modified, all copies must be modified. If this updating isnot done as one indivisible action, different processors will temporarily have different valuesfor the same logical data, which is unacceptable.The semantics of shared dataobjects in our model define that simultaneous operationson the same object must conceptually be serialized. The exact order in which they are to beexecuted is not defined, however. If, for example, a read operation and a write operation areapplied to the same object simultaneously, the read operation may observe either the valuebefore or after the write, but not an intermediate value. However, all processes having accessto the object must see the events happen in the same order.The RTS described here solves the inconsistency problem by using a distributed updateprotocol that guarantees that all processes observe changes to shared objects in the sameorder. One way to achieve this would be to lock all copies of an object prior to changing theobject. Unfortunately, distributed locking is quite expensive and complicated. Our updateprotocol does not use locking. The key to avoid locking is the use of an indivisible, reliablebroadcast primitive, which has the following properties Each message is sent reliably from one source to all destinations. If two processors simultaneously broadcast two messages say m1 and m2, then eitherall destinations first receive m1, or they all receive m2 first. Mixed forms some get m1first, some get m2 first are excluded by the software protocols. 21 This primitive is implemented by the bottom layer of our system, as will be described in Section 4.3, Here, we simply assume the indivisible, reliable broadcast exists.The RTS uses an objectmanager for each processor. The objectmanager is a lightweight process thread that takes care of updating the local copies of all objects stored on itsprocessor. Objects and replicas are stored in an address space shared by the objectmanager and user processes. User processes can read local copies directly, without intervention by the objectmanagers. Write operations on shared objects, on the other hand, aremarshalled and then broadcast to all the objectmanagers in the system. A user process thatbroadcasts a write operation suspends until the message has been handled by its local objectmanager. This is illustrated in Figure 8.INVOKEobj, op, parametersif op.ReadOnly then  check if its a read operationset readlock on local copy of objcall op.codeobj, parameters  do operation locallyunlock local copy of objelsebroadcast GlobalOperationobj, op, parameters to all managersblock current processfiFig. 8. Implementation of the INVOKE run time system primitive. This routine iscalled by user processes.Each objectmanager maintains a queue of messages that have arrived but that have notyet been handled. As all processors receive all messages in the same order, the queues of allmanagers are the same, except that some managers may be ahead of others in handling themessages at the head of the queue.The objectmanager of each processor handles the messages of its queue in strict FIFOorder. A message may be handled as soon as it appears at the head of the queue. To handle amessage GlobalOperationobj, op, parameters the message is removed from the queue,unmarshalled, the local copy of the object is locked, the operation is applied to the localcopy, and finally the copy is unlocked. If the message was sent by a process on the same processor, the manager unblocks that process see Figure 9.receive GlobalOperationobj, op, parameters from W set writelock on local copy of objcall op.codeobj, parameters  apply operation to local copyunlock local copy of objif W is a local process thenunblockWfiFig. 9. The code to be executed by the objectmanagers for handling GlobalOperation messages.Write operations are executed by all objectmanagers in the same order. If a read 22 operation is executed concurrently with a write operation, the read may either be executedbefore or after the write, but not during it. Note that this is in agreement with the serialization principle described above.4.3. Bottom layer Reliable broadcastIn this section we describe a simple protocol that allows a group of nodes on an unreliablebroadcast network to broadcast messages reliably. The protocol guarantees that all of thereceivers in the group receive all broadcast messages and that all receivers accept the messages in the same order. The main purpose of this section is to show that a protocol with therequired semantics is feasible, without going into too much detail about the protocol itself.With current microprocessors and LANs, lost or damaged packets and processor crashesoccur infrequently. Nevertheless, the probability of an error is not zero, so they must be dealtwith. For this reason, our approach to achieving reliable broadcast is to make the normalcase highly efficient, even at the expense of making errorrecovery more complex, sinceerror recovery will not be done often.The basic reliable broadcast protocol works as follows. When the RTS wants to broadcast a message, M , it hands the message to its kernel. The kernel then encapsulates M in anordinary pointtopoint message and sends it to a special kernel called the sequencer . Thesequencers node contains the same hardware and kernel as all the others. The only difference is that a flag in the kernel tells it to process messages differently. If the sequencershould crash, the protocol provides for the election of a new sequencer on a different node.The sequencer determines the ordering of all broadcast messages by assigning asequence number to each message. When the sequencer receives the pointtopoint messagecontaining M , it allocates the next sequence number, s and broadcasts a packet containing Mand s . Thus all broadcasts are issued from the same node, by the sequencer. Assuming thatno packets are lost, it is easy to see that if two RTSs simultaneously want to broadcast, one ofthem will reach the sequencer first and its message will be broadcast to all the other nodesfirst. Only when that broadcast has been completed will the other broadcast be started. Thesequencer provides a global ordering in time. In this way, we can easily guarantee the atomicity of broadcasting.Although most modern networks are highly reliable, they are not perfect, so the protocolmust deal with errors. Suppose some node misses a broadcast packet, either due to a communication failure or lack of buffer space when the packet arrived. When the followingbroadcast packet eventually arrives, the kernel will immediately notice a gap in the sequencenumbers. It was expecting s next, and it got s  1, so it knows it has missed one.The kernel then sends a special pointtopoint message to the sequencer asking it forcopies of the missing message or messages, if several have been missed. To be able toreply to such requests, the sequencer stores old broadcast messages in its history buffer . Themissing messages are sent directly to the process requesting them.As a practical matter, the sequencer has a finite amount of space in its history buffer, soit cannot store broadcast messages forever. However, if it could somehow discover that allmachines have received broadcasts up to and including k , it could then purge the first kbroadcast messages from the history buffer. 23 The protocol has several ways of letting the sequencer discover this information. Forone thing, each pointtopoint message to the sequencer e.g., a broadcast request, contains,in a header field, the sequence number of the last broadcast received by the sender of themessage. In this way, the sequencer can maintain a table, indexed by node number, showingthat node i has received all broadcast messages 0 up to Ti , and perhaps more. At anymoment, the sequencer can compute the lowest value in this table, and safely discard allbroadcast messages up to and including that value. For example, if the values of this tableare 8, 7, 9, 8, 6, and 8, the sequencer knows that everyone has received broadcasts 0 through6, so they can be deleted from the history buffer.If a node does not need to do any broadcasting for a while, the sequencer will not havean uptodate idea of which broadcasts it has received. To provide this information, nodesthat have been quiet for a certain interval, t, can just send the sequencer a special packetacknowledging all received broadcasts. The sequencer can explicitly ask for this informationif it runs out of history space,Besides the protocol described above Method 1, we have designed and implementedanother protocol Method 2 that does not send messages to the sequencer first. Instead, thekernel of the sender immediately broadcasts the message. Each receiving kernel stores themessage and the sequencer broadcasts a short acknowledgement message for it. These acknowledgements again carry sequence numbers, which define the ordering of the original messages. If a kernel receives an acknowledgement with the right i.e., next in line sequencenumber, it delivers the original message to the application.Both protocols guarantee the same semantics, but have different performances underdifferent circumstances. With Method 1, each message is sent over the network twice onceto the sequencer and once from the sequencer to the other kernels. Method 2 uses lessbandwidth than Method 1 the message appears only once on the network, but generatesmore interrupts, because it uses two broadcast messages one from the sender to the otherkernels and one short message from the sequencer to all kernels. For the implementation ofthe Orca run time system we use Method 1, because the messages generated by the run timesystem are short and because Method 1 steals less computing cycles from the Orca application to handle interrupts.In philosophy, the protocol described above somewhat resembles the one described byChang and Maxemchuk 27, but they differ in some major aspects. With our protocol, messages can be delivered to the user as soon as one special node has acknowledged the message. In addition, fewer control messages are needed in the normal case no lost messages.Our protocol therefore is highly efficient, since, during normal operation, only two packetsare needed assuming that a message fits in a single packet, one pointtopoint packet fromthe sender to the sequencer and one broadcast packet from the sequencer to everyone. Acomparison between our protocol and other well known protocols e.g., those of Birman andJoseph 28, GarciaMolina and Spauster 29, and several others is given in 30. 24 4.4. Comparison with an RPCbased ProtocolAbove, we have described one implementation of Orca, based on full replication of objectsand on a distributed update protocol using indivisible broadcasting. Below, we will comparethis implementation with another one based on partial replication and Remote Procedure CallRPC.Updating replicas with RPC is more complicated than with indivisible broadcast. Theproblem is that all replicas must be updated in a consistent way. To assure consistency, theRPC system uses a twophase update protocol. During the first phase, all copies are updatedand locked. After all updates have been acknowledged, the second phase begins, duringwhich all copies are unlocked.This protocol is much more expensive than the one based on broadcasting. The time foran update to complete depends on the number of copies. It therefore makes sense to use apartial replication strategy, and only replicate objects where they are needed. The RPC system maintains statistics about the number of read and write operations issued by each processor for each object. Based on this information, it decides dynamically where to store theobject and where to keep copies. The system can dynamically migrate the object or createand delete copies.The statistics impose some overhead on the operations, but in general the savings incommunication time are well worth this overhead. Still, in most cases, the RPC system hasmore communication costs than the broadcast system. For the TSP program, for example, itis far more efficient to update the global bound variable through a single broadcast messagethan through multiple RPCs.The RPC system is more efficient if the readwrite ratio of an object is low. In this case,the broadcast system will needlessly replicate the object, but the RPC system will observethis behavior and decide dynamically not to replicate the object.5. PERFORMANCE OF EXAMPLE APPLICATIONSIn this section we will take a brief look at the performance of some example Orca programs.The main goal of this section is to show that, at least for some realistic applications, goodspeedups can be obtained with our approach.The prototype distributed implementation we use is based on the layered approachdescribed in the previous section. The prototype runs on top of the Amoeba system, whichhas been extended with the broadcast protocol described earlier.The implementation runs on a distributed system, containing 16 MC68030 CPUs running at 16 Mhz connected to each other through an 10 Mbits Ethernet 31. The implementation uses Ethernet multicast communication to broadcast a message to a group of processors. All processors are on one Ethernet and are connected to it by Lance chip interfaces.The performance of the broadcast protocol on the Ethernet system is described in 30.The time needed for multicasting a short message reliably to two processors is 2.6 msec.With 16 receivers, a multicast takes 2.7 msec.4 This high performance is due to the fact that 4 In an earlier implementation of the protocol 32 the delay was 1.4 msec. The difference is entirelydue to a new routing protocol on which the group communication protocol is implemented. TheAmoeba kernel can now deal with different kinds of networks and route messages dynamically overmultiple networks. 25 our protocol is optimized for the common case i.e., no lost messages. During the experiments described below, the number of lost messages was found to be zero.We have used the implementation for developing several parallel applications written inOrca. Some of these are small, but others are larger. The largest application we currentlyhave is a parallel chess program, consisting of about 2500 lines of code. In addition to TSP,smaller applications include matrix multiplication, prime number generation, and sorting.Below, we will give performance measurements of three sample programs running on theEthernet implementation.5.1. Parallel Traveling Salesman ProblemThe first application, the Traveling Salesman Problem TSP, was described in Section 3.2.The program uses two shared objects a job queue and an IntObject containing the length ofthe current best path see Figure 5. It should be clear that reading of the current best pathlength will be done very often, but since this is a local operation, there is no communicationoverhead. Updating the best path happens much less often, but still only requires one broadcast message.Although updates of the best path happen infrequently, it is important to broadcast anyimprovements immediately. If a worker uses an old i.e., inferior value of the best path, itwill investigate paths that could have been pruned if the new value had been known. In otherwords, the worker will search more nodes than necessary. This search overhead may easilybecome a dominating factor and cause a severe performance degradation.The performance of the traveling salesman program for a randomly generated graphwith 12 cities is given in Figure 10. The implementation achieves a speedup close to linear.With 16 CPUs it is 14.44 times faster than with 1 CPU.5.2. Parallel allpairs shortest paths problemThe second application we describe here is the Allpairs Shortest Paths problem ASP. Inthis problem it is desired to find the length of the shortest path from any node i to any othernode j in a given graph. The parallel algorithm we use is similar to the one given in 33,which is a parallel version of Floyds algorithm. The distances between the nodes arerepresented in a matrix. Each processor computes part of the result matrix. The algorithmrequires a nontrivial amount of communication and synchronization among the processors.The performance of the program for a graph with 300 nodes is given in Figure 11.The parallel algorithm performs 300 iterations after each iteration, an array of 300 integers issent from one processor to all other processors. In spite of this high communication overhead, the implementation still has a good performance. With 16 CPUs, it achieves a speedupof 15.88. One of the main reasons for this good performance is the use of broadcast messages for transferring the array to all processors.5.3. Successive OverrelaxationBoth TSP and ASP benefit from the use of broadcasting. We will now consider an application that only needs pointtopoint message passing. The application is successive overrelaxation SOR, which is an iterative method for solving discretized Laplace equations on a grid. 26 SpeedupNumber of processors123456789101112131415161 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . . . . . . . Perfect speedup   Speedup for OrcaFig. 10. Measured speedup for the Orca implementation of the Traveling SalesmanProblem.SpeedupNumber of processors123456789101112131415161 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . . . . . . . Perfect speedup   Speedup for OrcaFig. 11. Measured speedup for the Orca implementation of the Allpairs ShortestPaths problem. 27 During each iteration, the algorithm considers all nonboundary points of the grid. For eachpoint, SOR first computes the average value of its four neighbors and then updates the pointusing this value.We have parallelized SOR by partitioning the grid into regions and assigning theseregions to different processors. The partitioning of the grid is such that, at the beginning ofan iteration, each processor needs to exchange values with only two other processors. Theparallel algorithm therefore only needs pointtopoint message passing. With our currentprototype implementation of Orca, however, all communication is based on broadcasting.The message passing is simulated in Orca by having the sender and receiver share a bufferobject. Since shared objects are updated through broadcasting, all processors will receive theupdate message. So, SOR is a worstcase example for our system.SpeedupNumber of processors123456789101112131415161 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . . . . . . . Perfect speedup   Speedup for OrcaFig. 12. Measured speedup for the Orca implementation of Successive Overrelaxation.The measured speedup for SOR is shown in Figure 12. Despite the high communicationoverhead, the program still achieves a reasonable speedup. The speedup on 16 CPUs is 11.4.6. RELATED WORKIn this section, we will compare our language with several related languages and systems. Inparticular, we will look at objects as used in parallel objectbased languages, Lindas TupleSpace, and Shared Virtual Memory. 28 ObjectsObjects are used in many objectbased languages for parallel or distributed programming,such as Emerald 15, Amber 34, and ALPS 35. Objects in such languages typically havetwo parts1. Encapsulated data.2. A manager process that controls access to the data.The data are accessed by sending a message to the manager process, asking it to perform acertain operation on the data. As such objects contain a process as well as data, they are saidto be active.Although, in some sense, parallel objectbased languages allow processes objects toshare data also objects, their semantics are closer to message passing than to shared variables. Access to the shared data is under full control of the manager process. In ALPS, forexample, all operations on an object go through its manager process, which determines theorder in which the operations are to be executed. Therefore, the only way to implement themodel is to store an object on one specific processor, together with its manager process, andto translate all operations on the object into remote procedure calls to the manager process.Our model does not have such centralized control. Objects in Orca are purely passivethey contain data, but no manager process. Access control to shared dataobjects is distributed it is basically determined by only two rules1. Operations must be executed indivisibly.2. Operations are blocked while their guards are false.Therefore, the model can be implemented by replicating dataobjects on multiple processors,as we discussed in Section 4. Read operations can be applied to the local copy, without anymessage passing being involved. Moreover, processes located on different processors canapply read operations simultaneously, without losing any parallelism.Lindas Tuple SpaceLinda 14 is one of the first languages to recognize the disadvantages of central managerprocesses for guarding shared data. Linda supports socalled distributed data structures,which can be accessed simultaneously by multiple processes. In contrast, objectbasedlanguages typically serialize access to shared data structures. Linda uses the Tuple Spacemodel for implementing distributed data structures.In general, distributed data structures in Linda are built out of multiple tuples. Differenttuples can be accessed independently from each other, so processes can manipulate differenttuples of the same data structure simultaneously. In principle, multiple read operations ofthe same tuple can also be executed simultaneously. Tuples are conceptually modified bytaking them out of Tuple Space first, so modifications of a given tuple are executed strictlysequentially.Although the idea of distributed data structures is appealing, we think the support givenby the Tuple Space for implementing such data structures has important disadvantages. Fordistributed data structures built out of single tuples, mutual exclusion synchronization is doneautomatically. Operations on complex data structures built out of multiple tuples, however, 29 have to be synchronized explicitly by the programmer. In essence, Tuple Space supports afixed number of builtin operations that are executed indivisibly, but its support for buildingmore complex indivisible operations is too lowlevel 36.In Orca, on the other hand, programmers can define operations of arbitrary complexityon shared data structures all these operations are executed indivisibly, so mutual exclusionsynchronization is always done automatically by the run time system. This means it is thejob of the implementation the compiler and run time system to see which operations can beexecuted in parallel and which have to be executed sequentially. As discussed above, oneway of doing this is by distinguishing between read and write operations and executing readsin parallel on local copies more advanced implementations are also feasible.Shared Virtual MemoryShared Virtual Memory SVM 6 simulates physical shared memory on a distributed system. It partitions the global address space into fixedsized pages, just as with virtualmemory. Each processor contains some portion of the pages. If a process tries to access apage that it does not have, it gets a pagefault, and the operating system will then fetch thepage from wherever it is located. Readonly pages may be shared among multiple processors. Writable pages must reside on a single machine. They cannot be shared. If a processorneeds to modify a page, it will first have to invalidate all copies of the page on other processors.There are many important differences between the implementation of our model andSVM. SVM is at least partly implemented inside the operating system, so it can use theMMU registers. In Orca, everything except for the broadcast protocol is implemented insoftware outside the operating system. This difference gives SVM a potential performanceadvantage.Still, our model has important advantages over SVM. First, shared dataobjects areaccessed through welldefined, highlevel operations, whereas SVM is accessed throughlowlevel read and write instructions. Consequently, we have a choice between invalidatingobjects after a write operation or updating them by applying the operation to all copies or,alternatively, sending the new value. With SVM, there is no such choice only invalidatingpages is viable 6. In many cases, however, invalidating copies will be far less efficient thanupdating them.Several researchers have tried to solve this performance problem by relaxing the consistency constraints of the memory e.g., 37, 38. Although these weakly consistentmemory models may have better performance, we fear that they also ruin the ease of programming for which DSM was designed in the first place. Since Orca is intended to simplifyapplications programming, Orca programmers should not have to worry about consistency.In the future, we may investigate whether a compiler is able to relax the consistency transparently, much as is done in the Munin system 39.A second important difference between Orca and SVM is the granularity of the shareddata. In SVM, the granularity is the pagesize, which is fixed e.g. 4K. In Orca, the granularity is the object, which is determined by the user. So, with SVM, if only a single bit of apage is modified, the whole page has to be invalidated. This property leads to the wellknown problem of false sharing. Suppose a process P repeatedly writes a variable X and 30 process Q repeatedly writes Y. If X and Y happen to be on the same page, this page will continuously be moved between P and Q, resulting in thrashing. If X and Y are on differentpages, thrashing will not occur. Since SVM is transparent, however, the programmer has nocontrol over the allocation of variables to pages. In Orca, this problem does not occur, sinceX and Y would be separate objects and would be treated independently.A more detailed comparison between our work and Shared Virtual Memory is givenin 40.7. CONCLUSIONWe have described a new model and language for parallel programming of distributed systems. In contrast with most other models for distributed programming, our model allowsprocesses on different machines to share data. The key idea in our model is to encapsulateshared data in dataobjects and to access these objects through userdefined operations. Theadvantages of this approach for the programmer and the implementer are summarized below.Since operations on objects are always executed indivisibly, mutual exclusion synchronization is done automatically, which simplifies programming. Condition synchronization isintegrated into the model by allowing operations to suspend. The mechanism for suspendingoperations is easy to use and is only visible to the implementer of the operations and not totheir users.The implementation of our model takes care of the physical distribution of shared dataamong processors. In particular, the implementation replicates shared data, so each processcan directly read the local copy on its own processor. After a write operation, all replicas areupdated by broadcasting the operation. This update strategy is only possible because shareddata are accessed through userdefined operations. SVM, for example, cannot efficientlyupdate replicas after a write operation, since a logical write operation may require manymachine instructions, each modifying memory. Updating the memory by broadcasting themachine instructions would be highly inefficient, as the communication overhead per instruction would be enormous.We have also defined a language, Orca, based on shared dataobjects. The design ofOrca avoids problems found in many other distributed languages, such as pointers and globalvariables. A major goal in the design was to keep the language simple. In particular, wehave given several examples of simplifying the language design by having the compiler docertain optimizations.We have studied one distributed implementation of Orca. This implementation runs ona collection of processors connected through a broadcast network. We have not looked atimplementations of Orca on other systems, such as hypercubes. Such an implementationwould be feasible, however, since the Orca language itself does not depend on the networktopology. To port Orca to other architectures, a new run time system probably with a newreplication strategy would be needed, but the language and its application programs wouldnot have to be changed.Our approach is best suited for moderategrained parallel applications in whichprocesses share data that are read frequently and modified infrequently. A good example isthe TSP program, which uses a shared object that is read very frequently and is changed only 31 a few times. This program shows an excellent performance. The applications also benefitfrom the efficient broadcast protocol used in our implementation. The usefulness of broadcasting was demonstrated by the ASP program.In conclusion, we think that Orca is a useful language for writing parallel programs fordistributed systems. Also, we have shown that the language is efficient for a range of applications.ACKNOWLEDGEMENTSWe would like to thank Wim van Leersum for implementing the Orca compiler and ErikBaalbergen, Fred Douglis, Arnold Geels, and the anonymous referees for giving useful comments on the paper.REFERENCES1. A.S. Tanenbaum, R. van Renesse, H. van Staveren, G.J. Sharp, S.J. Mullender, A.J. Jansen, and G. van Rossum, Experiences with the Amoeba Distributed Operating System, Comm. ACM 332, pp. 4663 Dec. 1990.2. H.E. Bal, R. van Renesse, and A.S. Tanenbaum, Implementing Distributed AlgorithmsUsing Remote Procedure Calls, Proc. AFIPS Nat. Computer Conf., Chicago, Ill. 56,pp. 499506, AFIPS Press June 1987.3. H.E. Bal, Programming Distributed Systems, Silicon Press, Summit, NJ 1990.4. H.E. Bal and A.S. Tanenbaum, Distributed Programming with Shared Data, Proc.IEEE CS 1988 Int. Conf. on Computer Languages, Miami, Fl., pp. 8291 Oct. 1988.5. H.E. Bal, M.F. Kaashoek, and A.S. Tanenbaum, Experience with Distributed Programming in Orca, Proceedings IEEE CS 1990 International Conference on ComputerLanguages, New Orleans, LA, pp. 7989 March 1990.6. K. Li and P. Hudak, Memory Coherence in Shared Virtual Memory Systems, Proc.5th Ann. ACM Symp. on Princ. of Distr. Computing, Calgary, Canada, pp. 229239Aug. 1986.7. C. Ghezzi and M. Jazayeri, Programming Language Concepts, John Wiley, New York,NY 1982.8. A.D. Birrell and B.J. Nelson, Implementing Remote Procedure Calls, ACM Trans.Comp. Syst. 21, pp. 3959 Feb. 1984.9. H.E. Bal, J.G. Steiner, and A.S. Tanenbaum, Programming Languages for DistributedComputing Systems, ACM Computing Surveys 213 Sept. 1989.10. R. Bisiani and A. Forin, Architectural Support for Multilanguage Parallel Programming on Heterogenous Systems, Proc. 2nd Int. Conf. on Architectural Support forProgramming Languages and Operating Systems, Palo Alto, Calif., pp. 2130 Oct.1987.11. K. Li, IVY A Shared Virtual Memory System for Parallel Computing, Proc. 1988Int. Conf. Parallel Processing Vol. II, St. Charles, Ill., pp. 94101 Aug. 1988.12. E.W. Felten and S.W. Otto, A Highly Parallel Chess Program, Proc. of the Int. Conf. 32 on Fifth Generation Computer Systems 1988, Tokyo, pp. 10011009 Nov. 1988.13. D.R. Cheriton, Preliminary Thoughts on Problemoriented Shared Memory A Decentralized Approach to Distributed Systems, ACM Operating Systems Review 194,pp. 2633 Oct. 1985.14. S. Ahuja, N. Carriero, and D. Gelernter, Linda and Friends, IEEE Computer 198,pp. 2634 Aug. 1986.15. E. Jul, H. Levy, N. Hutchinson, and A. Black, FineGrained Mobility in the EmeraldSystem, ACM Trans. Comp. Syst. 61, pp. 109133 Feb. 1988.16. B. Liskov, Distributed Programming in Argus, Commun. ACM 313, pp. 300312March 1988.17. G.R. Andrews, R.A. Olsson, M. Coffin, I. Elshoff, K. Nilsen, T. Purdin, and G. Townsend, An Overview of the SR Language and Implementation, ACM Trans. Program.Lang. Syst. 101, pp. 5186 Jan. 1988.18. H.E. Bal, An Evaluation of the SR Language Design, report IR219, Vrije Universiteit, Amsterdam August 1990.19. C.A.R. Hoare, Monitors An Operating System Structuring Concept, Commun. ACM1710, pp. 549557 Oct. 1974.20. G.R. Andrews and F.B. Schneider, Concepts and Notations for Concurrent Programming, ACM Computing Surveys 151, pp. 343 March 1983.21. K.P. Eswaran, J.N. Gray, R.A. Lorie, and I.L. Traiger, The Notions of Consistencyand Predicate Locks in a Database System, Commun. ACM 1911, pp. 624633 Nov.1976.22. S.E. Lucco, Parallel Programming in a Virtual Object Space, SIGPLAN NoticesProc. ObjectOriented Programming Systems, Languages and Applications 1987,Orlando, FL 2212, pp. 2634 Dec. 1987.23. R.C.B. Cooper and K.G. Hamilton, Preserving Abstraction in Concurrent Programming, IEEE Trans. Softw. Eng. SE142, pp. 258263 Feb. 1988.24. N. Wirth, The Programming Language Pascal, Acta Informatica 11, pp. 35631971.25. T.A. Joseph and K.P. Birman, Low Cost Management of Replicated Data in FaultTolerant Distributed Systems, ACM Trans. Comp. Syst. 41 Feb. 1987.26. H.E. Bal, M.F. Kaashoek, A.S. Tanenbaum, and J. Jansen, Replication Techniques forSpeeding up Parallel Applications on Distributed Systems, Report IR202, VrijeUniversiteit, Amsterdam, The Netherlands Oct. 1989.27. J. Chang and N.F. Maxemchuk, Reliable Broadcast Protocols, ACM Trans. Comp.Syst. 23, pp. 251273 Aug. 1984.28. K.P. Birman and T.A. Joseph, Reliable Communication in the Presence of Failures,ACM Trans. Comp. Syst. 51, pp. 4776 Feb. 1987.29. H. GarciaMolina and A. Spauster, Message Ordering in a Multicast Environment,Proc. 9th Int. Conf. on Distr. Comp. Syst., Newport Beach, CA, pp. 354361 June1989. 33 30. M.F. Kaashoek and A.S. Tanenbaum, Group Communication in the Amoeba Distributed Operating System, 11th Intl Conf. on Distributed Computing Systems, Arlington, Texas, pp. 222230 2024 May 1991.31. R.M. Metcalfe and D.R. Boggs, Ethernet Distributed Packet Switching for LocalComputer Networks, Commun. ACM 197, pp. 395404 July 1976.32. M.F. Kaashoek, A.S. Tanenbaum, S. Flynn Hummel, and H.E. Bal, An Efficient Reliable Broadcast Protocol, ACM Operating Systems Review 234, pp. 520 Oct. 1989.33. J.F. Jenq and S. Sahni, All Pairs Shortest Paths on a Hypercube Multiprocessor,Proc. of the 1987 Int. Conf. on Parallel Processing, St. Charles, Ill., pp. 713716 Aug.1987.34. J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy, and R.J. Littlefield, The AmberSystem Parallel Programming on a Network of Multiprocessors, Proc. of the 12thACM Symp. on Operating System Principles, Litchfield Park, AZ, pp. 147158 Dec.1989.35. P. Vishnubhotia, Synchronization and Scheduling in ALPS Objects, Proc. 8th Int.Conf. on Distributed Computing Systems, San Jose, CA, pp. 256264 June 1988.36. M.F. Kaashoek, H.E. Bal, and A.S. Tanenbaum, Experience with the Distributed DataStructure Paradigm in Linda, Workshop on Experiences with Building Distributed andMultiprocessor Systems, Ft. Lauderdale, FL. Oct. 1989a.37. R.G. Minnich and D.J. Farber, Reducing Host Load, Network Load, and Latency in aDistributed Shared Memory, Proc. 10th Int. Conf. on Distributed Computing Systems,Paris, pp. 468475 May 1990.38. P.W. Hutto and M. Ahamad, Slow Memory Weakening Consistency to Enhance Concurrency in Distributed Shared Memories, Proceedings 10th International Conferenceon Distributed Computing Systems, Paris, pp. 302309 May 1990.39. J.K. Bennet, J.B. Carter, and W. Zwaenepoel, Munin Distributed Shared MemoryBased on TypeSpecific Memory Coherence, Proceedings 2nd Symposium on Principles and Practice of Parallel Programming, Seattle, WA March 1990.40. W.G. Levelt, M.F. Kaashoek, H.E. Bal, and A.S. Tanenbaum, A Comparison of TwoParadigms for Distributed Shared Memory, IR221, Vrije Universiteit, Amsterdam,The Netherlands August 1990.
