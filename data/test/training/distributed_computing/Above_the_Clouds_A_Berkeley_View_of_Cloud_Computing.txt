Above the Clouds A Berkeley View of CloudComputingMichael ArmbrustArmando FoxRean GriffithAnthony D. JosephRandy H. KatzAndrew KonwinskiGunho LeeDavid A. PattersonAriel RabkinIon StoicaMatei ZahariaElectrical Engineering and Computer SciencesUniversity of California at BerkeleyTechnical Report No. UCBEECS200928httpwww.eecs.berkeley.eduPubsTechRpts2009EECS200928.htmlFebruary 10, 2009Copyright  2009, by the authors.All rights reserved. Permission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page. To copy otherwise, torepublish, to post on servers or to redistribute to lists, requires prior specificpermission. Acknowledgement The RAD Labs existence is due to the generous support of the foundingmembers Google, Microsoft, and Sun Microsystems and of the affiliatemembers Amazon Web Services, Cisco Systems, Facebook, HewlettPackard, IBM, NEC, Network Appliance, Oracle, Siemens, and VMware bymatching funds from the State of Californias MICRO program grants 06152, 07010, 06148, 07012, 06146, 07009, 06147, 07013, 06149, 06150, and 07008 and the University of California IndustryUniversityCooperative Research Program UC Discovery grant COM0710240 andby the National Science Foundation grant CNS0509559.Above the Clouds A Berkeley View of Cloud ComputingMichael Armbrust, Armando Fox, Rean Griffith, Anthony D. Joseph, Randy Katz,Andy Konwinski, Gunho Lee, David Patterson, Ariel Rabkin, Ion Stoica, and Matei ZahariaComments should be addressed to abovethecloudscs.berkeley.eduUC Berkeley Reliable Adaptive Distributed Systems Laboratory httpradlab.cs.berkeley.eduFebruary 10, 2009KEYWORDS Cloud Computing, Utility Computing, Internet Datacenters, Distributed System Economics1 Executive SummaryCloud Computing, the longheld dream of computing as a utility, has the potential to transform a large part of theIT industry, making software even more attractive as a service and shaping the way IT hardware is designed andpurchased. Developers with innovative ideas for new Internet services no longer require the large capital outlaysin hardware to deploy their service or the human expense to operate it. They need not be concerned about overprovisioning for a service whose popularity does not meet their predictions, thus wasting costly resources, or underprovisioning for one that becomes wildly popular, thus missing potential customers and revenue. Moreover, companieswith large batchoriented tasks can get results as quickly as their programs can scale, since using 1000 servers for onehour costs no more than using one server for 1000 hours. This elasticity of resources, without paying a premium forlarge scale, is unprecedented in the history of IT.Cloud Computing refers to both the applications delivered as services over the Internet and the hardware andsystems software in the datacenters that provide those services. The services themselves have long been referred to asSoftware as a Service SaaS. The datacenter hardware and software is what we will call a Cloud. When a Cloud ismade available in a payasyougo manner to the general public, we call it a Public Cloud the service being sold isUtility Computing. We use the term Private Cloud to refer to internal datacenters of a business or other organization,not made available to the general public. Thus, Cloud Computing is the sum of SaaS and Utility Computing, but doesnot include Private Clouds. People can be users or providers of SaaS, or users or providers of Utility Computing. Wefocus on SaaS Providers Cloud Users and Cloud Providers, which have received less attention than SaaS Users.From a hardware point of view, three aspects are new in Cloud Computing.1. The illusion of infinite computing resources available on demand, thereby eliminating the need for Cloud Computing users to plan far ahead for provisioning.2. The elimination of an upfront commitment by Cloud users, thereby allowing companies to start small andincrease hardware resources only when there is an increase in their needs.3. The ability to pay for use of computing resources on a shortterm basis as needed e.g., processors by the hourand storage by the day and release them as needed, thereby rewarding conservation by letting machines andstorage go when they are no longer useful.We argue that the construction and operation of extremely largescale, commoditycomputer datacenters at lowcost locations was the key necessary enabler of Cloud Computing, for they uncovered the factors of 5 to 7 decreasein cost of electricity, network bandwidth, operations, software, and hardware available at these very large economiesThe RAD Labs existence is due to the generous support of the founding members Google, Microsoft, and Sun Microsystems and of the affiliatemembers Amazon Web Services, Cisco Systems, Facebook, HewlettPackard, IBM, NEC, Network Appliance, Oracle, Siemens, and VMware bymatching funds from the State of Californias MICRO program grants 06152, 07010, 06148, 07012, 06146, 07009, 06147, 07013, 06149,06150, and 07008 and the University of California IndustryUniversity Cooperative Research Program UC Discovery grant COM0710240 andby the National Science Foundation grant CNS0509559.1of scale. These factors, combined with statistical multiplexing to increase utilization compared a private cloud, meantthat cloud computing could offer services below the costs of a mediumsized datacenter and yet still make a goodprofit.Any application needs a model of computation, a model of storage, and a model of communication. The statisticalmultiplexing necessary to achieve elasticity and the illusion of infinite capacity requires each of these resources tobe virtualized to hide the implementation of how they are multiplexed and shared. Our view is that different utilitycomputing offerings will be distinguished based on the level of abstraction presented to the programmer and the levelof management of the resources.Amazon EC2 is at one end of the spectrum. An EC2 instance looks much like physical hardware, and users cancontrol nearly the entire software stack, from the kernel upwards. This low level makes it inherently difficult forAmazon to offer automatic scalability and failover, because the semantics associated with replication and other statemanagement issues are highly applicationdependent. At the other extreme of the spectrum are application domainspecific platforms such as Google AppEngine. AppEngine is targeted exclusively at traditional web applications,enforcing an application structure of clean separation between a stateless computation tier and a stateful storage tier.AppEngines impressive automatic scaling and highavailability mechanisms, and the proprietary MegaStore datastorage available to AppEngine applications, all rely on these constraints. Applications for Microsofts Azure arewritten using the .NET libraries, and compiled to the Common Language Runtime, a languageindependent managedenvironment. Thus, Azure is intermediate between application frameworks like AppEngine and hardware virtualmachines like EC2.When is Utility Computing preferable to running a Private Cloud A first case is when demand for a service varieswith time. Provisioning a data center for the peak load it must sustain a few days per month leads to underutilizationat other times, for example. Instead, Cloud Computing lets an organization pay by the hour for computing resources,potentially leading to cost savings even if the hourly rate to rent a machine from a cloud provider is higher than therate to own one. A second case is when demand is unknown in advance. For example, a web startup will need tosupport a spike in demand when it becomes popular, followed potentially by a reduction once some of the visitors turnaway. Finally, organizations that perform batch analytics can use the cost associativity of cloud computing to finishcomputations faster using 1000 EC2 machines for 1 hour costs the same as using 1 machine for 1000 hours. For thefirst case of a web business with varying demand over time and revenue proportional to user hours, we have capturedthe tradeoff in the equation below.UserHourscloud  revenue Costcloud  UserHoursdatacenter  revenueCostdatacenterUtilization 1The lefthand side multiplies the net revenue per userhour by the number of userhours, giving the expected profitfrom using Cloud Computing. The righthand side performs the same calculation for a fixedcapacity datacenterby factoring in the average utilization, including nonpeak workloads, of the datacenter. Whichever side is greaterrepresents the opportunity for higher profit.Table 1 below previews our ranked list of critical obstacles to growth of Cloud Computing in Section 7. The firstthree concern adoption, the next five affect growth, and the last two are policy and business obstacles. Each obstacle ispaired with an opportunity, ranging from product development to research projects, which can overcome that obstacle.We predict Cloud Computing will grow, so developers should take it into account. All levels should aim at horizontal scalability of virtual machines over the efficiency on a single VM. In addition1. Applications Software needs to both scale down rapidly as well as scale up, which is a new requirement. Suchsoftware also needs a payforuse licensing model to match needs of Cloud Computing.2. Infrastructure Software needs to be aware that it is no longer running on bare metal but on VMs. Moreover, itneeds to have billing built in from the beginning.3. Hardware Systems should be designed at the scale of a container at least a dozen racks, which will be isthe minimum purchase size. Cost of operation will match performance and cost of purchase in importance,rewarding energy proportionality such as by putting idle portions of the memory, disk, and network into lowpower mode. Processors should work well with VMs, flash memory should be added to the memory hierarchy,and LAN switches and WAN routers must improve in bandwidth and cost.2 Cloud Computing An Old Idea Whose Time Has Finally ComeCloud Computing is a new term for a longheld dream of computing as a utility 35, which has recently emerged asa commercial reality. Cloud Computing is likely to have the same impact on software that foundries have had on the2Table 1 Quick Preview of Top 10 Obstacles to and Opportunities for Growth of Cloud Computing.Obstacle Opportunity1 Availability of Service Use Multiple Cloud Providers Use Elasticity to Prevent DDOS2 Data LockIn Standardize APIs Compatible SW to enable Surge Computing3 Data Confidentiality and Auditability Deploy Encryption, VLANs, Firewalls Geographical Data Storage4 Data Transfer Bottlenecks FedExing Disks Data BackupArchival Higher BW Switches5 Performance Unpredictability Improved VM Support Flash Memory Gang Schedule VMs6 Scalable Storage Invent Scalable Store7 Bugs in Large Distributed Systems Invent Debugger that relies on Distributed VMs8 Scaling Quickly Invent AutoScaler that relies on ML Snapshots for Conservation9 Reputation Fate Sharing Offer reputationguarding services like those for email10 Software Licensing Payforuse licenses Bulk use saleshardware industry. At one time, leading hardware companies required a captive semiconductor fabrication facility,and companies had to be large enough to afford to build and operate it economically. However, processing equipmentdoubled in price every technology generation. A semiconductor fabrication line costs over 3B today, so only a handfulof major merchant companies with very high chip volumes, such as Intel and Samsung, can still justify owning andoperating their own fabrication lines. This motivated the rise of semiconductor foundries that build chips for others,such as Taiwan Semiconductor Manufacturing Company TSMC. Foundries enable fabless semiconductor chipcompanies whose value is in innovative chip design A company such as nVidia can now be successful in the chipbusiness without the capital, operational expenses, and risks associated with owning a stateoftheart fabricationline. Conversely, companies with fabrication lines can timemultiplex their use among the products of many fablesscompanies, to lower the risk of not having enough successful products to amortize operational costs. Similarly, theadvantages of the economy of scale and statistical multiplexing may ultimately lead to a handful of Cloud Computingproviders who can amortize the cost of their large datacenters over the products of many datacenterless companies.Cloud Computing has been talked about 10, blogged about 13, 25, written about 15, 37, 38 and been featuredin the title of workshops, conferences, and even magazines. Nevertheless, confusion remains about exactly what it isand when its useful, causing Oracles CEO to vent his frustrationThe interesting thing about Cloud Computing is that weve redefined Cloud Computing to include everything that we already do. . . . I dont understand what we would do differently in the light of CloudComputing other than change the wording of some of our ads.Larry Ellison, quoted in the Wall Street Journal, September 26, 2008These remarks are echoed more mildly by HewlettPackards Vice President of European Software SalesA lot of people are jumping on the cloud bandwagon, but I have not heard two people say the same thingabout it. There are multiple definitions out there of the cloud.Andy Isherwood, quoted in ZDnet News, December 11, 2008Richard Stallman, known for his advocacy of free software, thinks Cloud Computing is a trap for usersifapplications and data are managed in the cloud, users might become dependent on proprietary systems whose costswill escalate or whose terms of service might be changed unilaterally and adverselyIts stupidity. Its worse than stupidity its a marketing hype campaign. Somebody is saying this isinevitable  and whenever you hear somebody saying that, its very likely to be a set of businessescampaigning to make it true.Richard Stallman, quoted in The Guardian, September 29, 2008Our goal in this paper to clarify terms, provide simple formulas to quantify comparisons between of cloud andconventional Computing, and identify the top technical and nontechnical obstacles and opportunities of Cloud Computing. Our view is shaped in part by working since 2005 in the UC Berkeley RAD Lab and in part as users of AmazonWeb Services since January 2008 in conducting our research and our teaching. The RAD Labs research agenda is toinvent technology that leverages machine learning to help automate the operation of datacenters for scalable Internetservices. We spent six months brainstorming about Cloud Computing, leading to this paper that tries to answer thefollowing questions3 What is Cloud Computing, and how is it different from previous paradigm shifts such as Software as a ServiceSaaS Why is Cloud Computing poised to take off now, whereas previous attempts have foundered What does it take to become a Cloud Computing provider, and why would a company consider becoming one What new opportunities are either enabled by or potential drivers of Cloud Computing How might we classify current Cloud Computing offerings across a spectrum, and how do the technical andbusiness challenges differ depending on where in the spectrum a particular offering lies What, if any, are the new economic models enabled by Cloud Computing, and how can a service operator decidewhether to move to the cloud or stay in a private datacenter What are the top 10 obstacles to the success of Cloud Computingand the corresponding top 10 opportunitiesavailable for overcoming the obstacles What changes should be made to the design of future applications software, infrastructure software, and hardware to match the needs and opportunities of Cloud Computing3 What is Cloud ComputingCloud Computing refers to both the applications delivered as services over the Internet and the hardware and systemssoftware in the datacenters that provide those services. The services themselves have long been referred to as Softwareas a Service SaaS, so we use that term. The datacenter hardware and software is what we will call a Cloud.When a Cloud is made available in a payasyougo manner to the public, we call it a Public Cloud the servicebeing sold is Utility Computing. Current examples of public Utility Computing include Amazon Web Services, GoogleAppEngine, and Microsoft Azure. We use the term Private Cloud to refer to internal datacenters of a business orother organization that are not made available to the public. Thus, Cloud Computing is the sum of SaaS and UtilityComputing, but does not normally include Private Clouds. Well generally use Cloud Computing, replacing it withone of the other terms only when clarity demands it. Figure 1 shows the roles of the people as users or providers ofthese layers of Cloud Computing, and well use those terms to help make our arguments clear.The advantages of SaaS to both end users and service providers are well understood. Service providers enjoygreatly simplified software installation and maintenance and centralized control over versioning end users can accessthe service anytime, anywhere, share data and collaborate more easily, and keep their data stored safely in theinfrastructure. Cloud Computing does not change these arguments, but it does give more application providers thechoice of deploying their product as SaaS without provisioning a datacenter just as the emergence of semiconductorfoundries gave chip companies the opportunity to design and sell chips without owning a fab, Cloud Computing allowsdeploying SaaSand scaling on demandwithout building or provisioning a datacenter. Analogously to how SaaSallows the user to offload some problems to the SaaS provider, the SaaS provider can now offload some of his problemsto the Cloud Computing provider. From now on, we will focus on issues related to the potential SaaS Provider CloudUser and to the Cloud Providers, which have received less attention.We will eschew terminology such as X as a service XaaS values of X we have seen in print include Infrastructure, Hardware, and Platform, but we were unable to agree even among ourselves what the precise differences amongthem might be.1 We are using Endnotes instead of footnotes. Go to page 20 at the end of paper to read the notes,which have more details. Instead, we present a simple classification of Utility Computing services in Section 5 thatfocuses on the tradeoffs among programmer convenience, flexibility, and portability, from both the cloud providersand the cloud users point of view.From a hardware point of view, three aspects are new in Cloud Computing 421. The illusion of infinite computing resources available on demand, thereby eliminating the need for Cloud Computing users to plan far ahead for provisioning2. The elimination of an upfront commitment by Cloud users, thereby allowing companies to start small andincrease hardware resources only when there is an increase in their needs and3. The ability to pay for use of computing resources on a shortterm basis as needed e.g., processors by the hourand storage by the day and release them as needed, thereby rewarding conservation by letting machines andstorage go when they are no longer useful.4Figure 1 Users and Providers of Cloud Computing. The benefits of SaaS to both SaaS users and SaaS providers arewell documented, so we focus on Cloud Computings effects on Cloud Providers and SaaS ProvidersCloud users. Thetop level can be recursive, in that SaaS providers can also be a SaaS users. For example, a mashup provider of rentalmaps might be a user of the Craigslist and Google maps services.We will argue that all three are important to the technical and economic changes made possible by Cloud Computing. Indeed, past efforts at utility computing failed, and we note that in each case one or two of these three criticalcharacteristics were missing. For example, Intel Computing Services in 20002001 required negotiating a contract andlongerterm use than per hour.As a successful example, Elastic Compute Cloud EC2 from Amazon Web Services AWS sells 1.0GHz x86ISA slices for 10 cents per hour, and a new slice, or instance, can be added in 2 to 5 minutes. Amazons ScalableStorage Service S3 charges 0.12 to 0.15 per gigabytemonth, with additional bandwidth charges of 0.10 to 0.15per gigabyte to move data in to and out of AWS over the Internet. Amazons bet is that by statistically multiplexingmultiple instances onto a single physical box, that box can be simultaneously rented to many customers who will notin general interfere with each others usage see Section 7.While the attraction to Cloud Computing users SaaS providers is clear, who would become a Cloud Computingprovider, and why To begin with, realizing the economies of scale afforded by statistical multiplexing and bulkpurchasing requires the construction of extremely large datacenters.Building, provisioning, and launching such a facility is a hundredmilliondollar undertaking. However, because ofthe phenomenal growth of Web services through the early 2000s, many large Internet companies, including Amazon,eBay, Google, Microsoft and others, were already doing so. Equally important, these companies also had to developscalable software infrastructure such as MapReduce, the Google File System, BigTable, and Dynamo 16, 20, 14, 17and the operational expertise to armor their datacenters against potential physical and electronic attacks.Therefore, a necessary but not sufficient condition for a company to become a Cloud Computing provider is thatit must have existing investments not only in very large datacenters, but also in largescale software infrastructureand operational expertise required to run them. Given these conditions, a variety of factors might influence thesecompanies to become Cloud Computing providers1. Make a lot of money. Although 10 cents per serverhour seems low, Table 2 summarizes James Hamiltonsestimates 23 that very large datacenters tens of thousands of computers can purchase hardware, networkbandwidth, and power for 15 to 17 the prices offered to a mediumsized hundreds or thousands of computersdatacenter. Further, the fixed costs of software development and deployment can be amortized over many moremachines. Others estimate the price advantage as a factor of 3 to 5 37, 10. Thus, a sufficiently large companycould leverage these economies of scale to offer a service well below the costs of a mediumsized company andstill make a tidy profit.2. Leverage existing investment. Adding Cloud Computing services on top of existing infrastructure provides anew revenue stream at ideally low incremental cost, helping to amortize the large investments of datacenters.Indeed, according to Werner Vogels, Amazons CTO, many Amazon Web Services technologies were initiallydeveloped for Amazons internal operations 42.3. Defend a franchise. As conventional server and enterprise applications embrace Cloud Computing, vendorswith an established franchise in those applications would be motivated to provide a cloud option of their own.For example, Microsoft Azure provides an immediate path for migrating existing customers of Microsoft enterprise applications to a cloud environment.5Table 2 Economies of scale in 2006 for mediumsized datacenter 1000 servers vs. very large datacenter 50,000servers. 24Technology Cost in Mediumsized DC Cost in Very Large DC RatioNetwork 95 per Mbitsecmonth 13 per Mbitsecmonth 7.1Storage 2.20 per GByte  month 0.40 per GByte  month 5.7Administration 140 Servers  Administrator 1000 Servers  Administrator 7.1Table 3 Price of kilowatthours of electricity by region 7.Price per KWH Where Possible Reasons Why3.6 Idaho Hydroelectric power not sent long distance10.0 California Electricity transmitted long distance over the gridlimited transmission lines in Bay Area no coalfired electricity allowed in California.18.0 Hawaii Must ship fuel to generate electricity4. Attack an incumbent. A company with the requisite datacenter and software resources might want to establish abeachhead in this space before a single 800 pound gorilla emerges. Google AppEngine provides an alternativepath to cloud deployment whose appeal lies in its automation of many of the scalability and load balancingfeatures that developers might otherwise have to build for themselves.5. Leverage customer relationships. IT service organizations such as IBM Global Services have extensive customer relationships through their service offerings. Providing a branded Cloud Computing offering gives thosecustomers an anxietyfree migration path that preserves both parties investments in the customer relationship.6. Become a platform. Facebooks initiative to enable plugin applications is a great fit for cloud computing, aswe will see, and indeed one infrastructure provider for Facebook plugin applications is Joyent, a cloud provider.Yet Facebooks motivation was to make their socialnetworking application a new development platform.Several Cloud Computing and conventional computing datacenters are being built in seemingly surprising locations, such as Quincy, Washington Google, Microsoft, Yahoo, and others and San Antonio, Texas Microsoft, USNational Security Agency, others. The motivation behind choosing these locales is that the costs for electricity, cooling, labor, property purchase costs, and taxes are geographically variable, and of these costs, electricity and coolingalone can account for a third of the costs of the datacenter. Table 3 shows the cost of electricity in different locales 10.Physics tells us its easier to ship photons than electrons that is, its cheaper to ship data over fiber optic cables thanto ship electricity over highvoltage transmission lines.4 Clouds in a Perfect Storm Why Now, Not ThenAlthough we argue that the construction and operation of extremely large scale commoditycomputer datacenters wasthe key necessary enabler of Cloud Computing, additional technology trends and new business models also playeda key role in making it a reality this time around. Once Cloud Computing was off the ground, new applicationopportunities and usage models were discovered that would not have made sense previously.4.1 New Technology Trends and Business ModelsAccompanying the emergence of Web 2.0 was a shift from hightouch, highmargin, highcommitment provisioningof service lowtouch, lowmargin, lowcommitment selfservice. For example, in Web 1.0, accepting credit cardpayments from strangers required a contractual arrangement with a payment processing service such as VeriSign orAuthorize.net the arrangement was part of a larger business relationship, making it onerous for an individual or a verysmall business to accept credit cards online. With the emergence of PayPal, however, any individual can accept creditcard payments with no contract, no longterm commitment, and only modest payasyougo transaction fees. The levelof touch customer support and relationship management provided by these services is minimal to nonexistent, but6the fact that the services are now within reach of individuals seems to make this less important. Similarly, individualsWeb pages can now use Google AdSense to realize revenue from ads, rather than setting up a relationship with anad placement company, such DoubleClick now acquired by Google. Those ads can provide the business model forWed 2.0 apps as well. Individuals can distribute Web content using Amazon CloudFront rather than establishing arelationship with a content distribution network such as Akamai.Amazon Web Services capitalized on this insight in 2006 by providing payasyougo computing with no contractall customers need is a credit card. A second innovation was selling hardwarelevel virtual machines cycles, allowingcustomers to choose their own software stack without disrupting each other while sharing the same hardware andthereby lowering costs further.4.2 New Application OpportunitiesWhile we have yet to see fundamentally new types of applications enabled by Cloud Computing, we believe thatseveral important classes of existing applications will become even more compelling with Cloud Computing andcontribute further to its momentum. When Jim Gray examined technological trends in 2003 21, he concluded thateconomic necessity mandates putting the data near the application, since the cost of widearea networking has fallenmore slowly and remains relatively higher than all other IT hardware costs. Although hardware costs have changedsince Grays analysis, his idea of this breakeven point has not. Although we defer a more thorough discussion ofCloud Computing economics to Section 6, we use Grays insight in examining what kinds of applications representparticularly good opportunities and drivers for Cloud Computing.Mobile interactive applications. Tim OReilly believes that the future belongs to services that respond in realtime to information provided either by their users or by nonhuman sensors. 38 Such services will be attracted tothe cloud not only because they must be highly available, but also because these services generally rely on large datasets that are most conveniently hosted in large datacenters. This is especially the case for services that combine two ormore data sources or other services, e.g., mashups. While not all mobile devices enjoy connectivity to the cloud 100of the time, the challenge of disconnected operation has been addressed successfully in specific application domains,2 so we do not see this as a significant obstacle to the appeal of mobile applications.Parallel batch processing. Although thus far we have concentrated on using Cloud Computing for interactiveSaaS, Cloud Computing presents a unique opportunity for batchprocessing and analytics jobs that analyze terabytesof data and can take hours to finish. If there is enough data parallelism in the application, users can take advantageof the clouds new cost associativity using hundreds of computers for a short time costs the same as using a fewcomputers for a long time. For example, Peter Harkins, a Senior Engineer at The Washington Post, used 200 EC2instances 1,407 server hours to convert 17,481 pages of Hillary Clintons travel documents into a form more friendlyto use on the WWW within nine hours after they were released 3. Programming abstractions such as GooglesMapReduce 16 and its opensource counterpart Hadoop 11 allow programmers to express such tasks while hidingthe operational complexity of choreographing parallel execution across hundreds of Cloud Computing servers. Indeed,Cloudera 1 is pursuing commercial opportunities in this space. Again, using Grays insight, the costbenefit analysismust weigh the cost of moving large datasets into the cloud against the benefit of potential speedup in the data analysis.When we return to economic models later, we speculate that part of Amazons motivation to host large public datasetsfor free 8 may be to mitigate the cost side of this analysis and thereby attract users to purchase Cloud Computingcycles near this data.The rise of analytics. A special case of computeintensive batch processing is business analytics. While the largedatabase industry was originally dominated by transaction processing, that demand is leveling off. A growing shareof computing resources is now spent on understanding customers, supply chains, buying habits, ranking, and so on.Hence, while online transaction volumes will continue to grow slowly, decision support is growing rapidly, shiftingthe resource balance in database processing from transactions to business analytics.Extension of computeintensive desktop applications. The latest versions of the mathematics software packagesMatlab and Mathematica are capable of using Cloud Computing to perform expensive evaluations. Other desktopapplications might similarly benet from seamless extension into the cloud. Again, a reasonable test is comparing thecost of computing in the Cloud plus the cost of moving data in and out of the Cloud to the time savings from usingthe Cloud. Symbolic mathematics involves a great deal of computing per unit of data, making it a domain worthinvestigating. An interesting alternative model might be to keep the data in the cloud and rely on having sufficientbandwidth to enable suitable visualization and a responsive GUI back to the human user. Offline image rendering or 3Danimation might be a similar example given a compact description of the objects in a 3D scene and the characteristicsof the lighting sources, rendering the image is an embarrassingly parallel task with a high computationtobytes ratio.Earthbound applications. Some applications that would otherwise be good candidates for the clouds elasticityand parallelism may be thwarted by data movement costs, the fundamental latency limits of getting into and out of thecloud, or both. For example, while the analytics associated with making longterm financial decisions are appropriate7for the Cloud, stock trading that requires microsecond precision is not. Until the cost and possibly latency of widearea data transfer decrease see Section 7, such applications may be less obvious candidates for the cloud.5 Classes of Utility ComputingAny application needs a model of computation, a model of storage and, assuming the application is even triviallydistributed, a model of communication. The statistical multiplexing necessary to achieve elasticity and the illusionof infinite capacity requires resources to be virtualized, so that the implementation of how they are multiplexed andshared can be hidden from the programmer. Our view is that different utility computing offerings will be distinguishedbased on the level of abstraction presented to the programmer and the level of management of the resources.Amazon EC2 is at one end of the spectrum. An EC2 instance looks much like physical hardware, and userscan control nearly the entire software stack, from the kernel upwards. The API exposed is thin a few dozenAPI calls to request and configure the virtualized hardware. There is no a priori limit on the kinds of applicationsthat can be hosted the low level of virtualizationraw CPU cycles, blockdevice storage, IPlevel connectivityallow developers to code whatever they want. On the other hand, this makes it inherently difficult for Amazon tooffer automatic scalability and failover, because the semantics associated with replication and other state managementissues are highly applicationdependent.AWS does offer a number of higherlevel managed services, including several different managed storage servicesfor use in conjunction with EC2, such as SimpleDB. However, these offerings have higher latency and nonstandardAPIs, and our understanding is that they are not as widely used as other parts of AWS.At the other extreme of the spectrum are application domainspecific platforms such as Google AppEngine andForce.com, the SalesForce business software development platform. AppEngine is targeted exclusively at traditionalweb applications, enforcing an application structure of clean separation between a stateless computation tier and astateful storage tier. Furthermore, AppEngine applications are expected to be requestreply based, and as such theyare severely rationed in how much CPU time they can use in servicing a particular request. AppEngines impressiveautomatic scaling and highavailability mechanisms, and the proprietary MegaStore based on BigTable data storageavailable to AppEngine applications, all rely on these constraints. Thus, AppEngine is not suitable for generalpurposecomputing. Similarly, Force.com is designed to support business applications that run against the salesforce.comdatabase, and nothing else.Microsofts Azure is an intermediate point on this spectrum of flexibility vs. programmer convenience. Azureapplications are written using the .NET libraries, and compiled to the Common Language Runtime, a languageindependent managed environment. The system supports generalpurpose computing, rather than a single categoryof application. Users get a choice of language, but cannot control the underlying operating system or runtime. Thelibraries provide a degree of automatic network configuration and failoverscalability, but require the developer todeclaratively specify some application properties in order to do so. Thus, Azure is intermediate between completeapplication frameworks like AppEngine on the one hand, and hardware virtual machines like EC2 on the other.Table 4 summarizes how these three classes virtualize computation, storage, and networking. The scattershotofferings of scalable storage suggest that scalable storage with an API comparable in richness to SQL remains an openresearch problem see Section 7. Amazon has begun offering Oracle databases hosted on AWS, but the economicsand licensing model of this product makes it a less natural fit for Cloud Computing.Will one model beat out the others in the Cloud Computing space We can draw an analogy with programminglanguages and frameworks. Lowlevel languages such as C and assembly language allow fine control and closecommunication with the bare metal, but if the developer is writing a Web application, the mechanics of managingsockets, dispatching requests, and so on are cumbersome and tedious to code, even with good libraries. On the otherhand, highlevel frameworks such as Ruby on Rails make these mechanics invisible to the programmer, but are onlyuseful if the application readily fits the requestreply structure and the abstractions provided by Rails any deviationrequires diving into the framework at best, and may be awkward to code. No reasonable Ruby developer would argueagainst the superiority of C for certain tasks, and vice versa. Correspondingly, we believe different tasks will result indemand for different classes of utility computing.Continuing the language analogy, just as highlevel languages can be implemented in lowerlevel ones, highlymanaged cloud platforms can be hosted on top of lessmanaged ones. For example, AppEngine could be hosted ontop of Azure or EC2 Azure could be hosted on top of EC2. Of course, AppEngine and Azure each offer proprietaryfeatures AppEngines scaling, failover and MegaStore data storage or large, complex APIs Azures .NET librariesthat have no free implementation, so any attempt to clone AppEngine or Azure would require reimplementing thosefeatures or APIsa formidable challenge.8Table 4 Examples of Cloud Computing vendors and how each provides virtualized resources computation, storage,networking and ensures scalability and high availability of the resources.Amazon Web Services Microsoft Azure Google AppEngineComputationmodel VM x86 Instruction Set ArchitectureISA via Xen VM Computation elasticity allowsscalability, but developer must buildthe machinery, or third party VARsuch as RightScale must provide it Microsoft Common Language Runtime CLR VMcommon intermediate formexecuted in managed environment Machines are provisioned based on declarativedescriptions e.g. whichroles can be replicatedautomatic load balancing Predefined applicationstructure and frameworkprogrammerprovided handlers written in Python,all persistent state stored inMegaStore outside Pythoncode Automatic scaling up anddown of computation andstorage network and serverfailover all consistent with3tier Web app structureStorage model  Range of models from block storeEBS to augmented keyblob storeSimpleDB Automatic scaling varies from noscaling or sharing EBS to fully automatic SimpleDB, S3, dependingon which model used Consistency guarantees varywidely depending on which modelused APIs vary from standardizedEBS to proprietary SQL Data Services restricted view of SQL Server Azure storage serviceMegaStoreBigTableNetworkingmodel Declarative specification of IPlevel topology internal placementdetails concealed Security Groups enable restrictingwhich nodes may communicate Availability zones provide abstraction of independent networkfailure Elastic IP addresses provide persistently routable network name Automatic based on programmers declarative descriptions of app components roles Fixed topology to accommodate 3tier Web appstructure Scaling up and down isautomatic and programmerinvisible96 Cloud Computing EconomicsIn this section we make some observations about Cloud Computing economic models In deciding whether hosting a service in the cloud makes sense over the long term, we argue that the finegrained economic models enabled by Cloud Computing make tradeoff decisions more fluid, and in particularthe elasticity offered by clouds serves to transfer risk. As well, although hardware resource costs continue to decline, they do so at variable rates for example, computing and storage costs are falling faster than WAN costs. Cloud Computing can track these changesandpotentially pass them through to the customermore effectively than building ones own datacenter, resultingin a closer match of expenditure to actual resource usage. In making the decision about whether to move an existing service to the cloud, one must additionally examine theexpected average and peak resource utilization, especially if the application may have highly variable spikes inresource demand the practical limits on realworld utilization of purchased equipment and various operationalcosts that vary depending on the type of cloud environment being considered.6.1 Elasticity Shifting the RiskAlthough the economic appeal of Cloud Computing is often described as converting capital expenses to operatingexpenses CapEx to OpEx, we believe the phrase pay as you go more directly captures the economic benefit tothe buyer. Hours purchased via Cloud Computing can be distributed nonuniformly in time e.g., use 100 serverhourstoday and no serverhours tomorrow, and still pay only for what you use in the networking community, this way ofselling bandwidth is already known as usagebased pricing. 3 In addition, the absence of upfront capital expenseallows capital to be redirected to core business investment.Therefore, even though Amazons payasyougo pricing for example could be more expensive than buying anddepreciating a comparable server over the same period, we argue that the cost is outweighed by the extremely importantCloud Computing economic benefits of elasticity and transference of risk, especially the risks of overprovisioningunderutilization and underprovisioning saturation.We start with elasticity. The key observation is that Cloud Computings ability to add or remove resources at a finegrain one server at a time with EC2 and with a lead time of minutes rather than weeks allows matching resourcesto workload much more closely. Real world estimates of server utilization in datacenters range from 5 to 2037, 38. This may sound shockingly low, but it is consistent with the observation that for many services the peakworkload exceeds the average by factors of 2 to 10. Few users deliberately provision for less than the expected peak,and therefore they must provision for the peak and allow the resources to remain idle at nonpeak times. The morepronounced the variation, the more the waste. A simple example demonstrates how elasticity allows reducing thiswaste and can therefore more than compensate for the potentially higher cost per serverhour of payingasyougo vs.buying.Example Elasticity. Assume our service has a predictable daily demand where the peak requires 500servers at noon but the trough requires only 100 servers at midnight, as shown in Figure 2a. As long asthe average utilization over a whole day is 300 servers, the actual utilization over the whole day shadedarea under the curve is 300 24  7200 serverhours but since we must provision to the peak of 500servers, we pay for 500 24  12000 serverhours, a factor of 1.7 more than what is needed. Therefore,as long as the payasyougo cost per serverhour over 3 years4 is less than 1.7 times the cost of buying theserver, we can save money using utility computing.In fact, the above example underestimates the benefits of elasticity, because in addition to simple diurnal patterns,most nontrivial services also experience seasonal or other periodic demand variation e.g., ecommerce peaks in December and photo sharing sites peak after holidays as well as some unexpected demand bursts due to external eventse.g., news events. Since it can take weeks to acquire and rack new equipment, the only way to handle such spikesis to provision for them in advance. We already saw that even if service operators predict the spike sizes correctly,capacity is wasted, and if they overestimate the spike they provision for, its even worse.They may also underestimate the spike Figure 2b, however, accidentally turning away excess users. Whilethe monetary effects of overprovisioning are easily measured, those of underprovisioning are harder to measure yetpotentially equally serious not only do rejected users generate zero revenue, they may never come back due to poorservice. Figure 2c aims to capture this behavior users will desert an underprovisioned service until the peak user10a Provisioning for peak load b Underprovisioning 1c Underprovisioning 2Figure 2 a Even if peak load can be correctly anticipated, without elasticity we waste resources shaded area duringnonpeak times. b Underprovisioning case 1 potential revenue from users not served shaded area is sacrificed. cUnderprovisioning case 2 some users desert the site permanently after experiencing poor service this attrition andpossible negative press result in a permanent loss of a portion of the revenue stream.load equals the datacenters usable capacity, at which point users again receive acceptable service, but with fewerpotential users.Example Transferring risks. Suppose but 10 of users who receive poor service due to underprovisioning are permanently lost opportunities, i.e. users who would have remained regular visitors witha better experience. The site is initially provisioned to handle an expected peak of 400,000 users 1000users per server  400 servers, but unexpected positive press drives 500,000 users in the first hour. Ofthe 100,000 who are turned away or receive bad service, by our assumption 10,000 of them are permanently lost, leaving an active user base of 390,000. The next hour sees 250,000 new unique users. Thefirst 10,000 do fine, but the site is still over capacity by 240,000 users. This results in 24,000 additionaldefections, leaving 376,000 permanent users. If this pattern continues, after lg 500000 or 19 hours, thenumber of new users will approach zero and the site will be at capacity in steady state. Clearly, the serviceoperator has collected less than 400,000 users worth of steady revenue during those 19 hours, however,again illustrating the underutilization argument to say nothing of the bad reputation from the disgruntledusers.Do such scenarios really occur in practice When Animoto 4 made its service available via Facebook, it experienced a demand surge that resulted in growing from 50 servers to 3500 servers in three days. Even if the averageutilization of each server was low, no one could have foreseen that resource needs would suddenly double every 12hours for 3 days. After the peak subsided, traffic fell to a level that was well below the peak. So in this real worldexample, scaleup elasticity was not a cost optimization but an operational requirement, and scaledown elasticityallowed the steadystate expenditure to more closely match the steadystate workload.Elasticity is valuable to established companies as well as startups. For example, Target, the nations second largestretailer, uses AWS for the Target.com website. While other retailers had severe performance problems and intermittentunavailability on Black Friday November 28, Targets and Amazons sites were just slower by about 50. 5Similarly, Salesforce.com hosts customers ranging from 2 seat to 40,000 seat customers.Even lessdramatic cases suffice to illustrate this key benefit of Cloud Computing the risk of misestimatingworkload is shifted from the service operator to the cloud vendor. The cloud vendor may charge a premium reflectedas a higher use cost per serverhour compared to the 3year purchase cost for assuming this risk. We propose thefollowing simple equation that generalizes all of the above cases. We assume the Cloud Computing vendor employs11usagebased pricing, in which customers pay proportionally to the amount of time and the amount of resources theyuse. While some argue for more sophisticated pricing models for infrastructure services 28, 6, 40, we believe usagebased pricing will persist because it is simpler and more transparent, as demonstrated by its wide use by real utilitiessuch as electricity and gas companies. Similarly, we assume that the customers revenue is directly proportional to thetotal number of userhours. This assumption is consistent with the adsupported revenue model in which the numberof ads served is roughly proportional to the total visit time spent by end users on the service.UserHourscloud  revenue Costcloud  UserHoursdatacenter  revenueCostdatacenterUtilization 2The lefthand side multiplies the net revenue per userhour revenue realized per userhour minus cost of payingCloud Computing per userhour by the number of userhours, giving the expected profit from using Cloud Computing. The righthand side performs the same calculation for a fixedcapacity datacenter by factoring in the averageutilization, including nonpeak workloads. Whichever side is greater represents the opportunity for higher profit.Apparently, if Utilization  1.0 the datacenter equipment is 100 utilized, the two sides of the equation lookthe same. However, basic queueing theory tells us that as utilization approaches 1.0, system response time approachesinfinity. In practice, the usable capacity of a datacenter without compromising service is typically 0.6 to 0.8.6Whereas a datacenter must necessarily overprovision to account for this overhead, the cloud vendor can simplyfactor it into Costcloud. This overhead explains why we use the phrase payasyougo rather than rent or lease forutility computing. The latter phrases include this unusable overhead, while the former doesnt. Hence, even if youlease a 100 Mbitssecond Internet link, you can likely use only 60 to 80 Mbitssecond in practice.The equation makes clear that the common element in all of our examples is the ability to control the cost per userhour of operating the service. In Example 1, the cost per userhour without elasticity was high because of resourcessitting idlehigher costs but same number of userhours. The same thing happens when overestimation of demandresults in provisioning for workload that doesnt materialize. In Example 2, the cost per userhour increased as a resultof underestimating a spike and having to turn users away Since some fraction of those users never return, the fixedcosts stay the same but are now amortized over fewer userhours. This illustrates fundamental limitations of the buymodel in the face of any nontrivial burstiness in the workload.Finally, there are two additional benefits to the Cloud Computing user that result from being able to change theirresource usage on the scale of hours rather than years. First, unexpectedly scaling down disposing of temporarilyunderutilized equipmentfor example, due to a business slowdown, or ironically due to improved software efficiencynormally carries a financial penalty. With 3year depreciation, a 2,100 server decommissioned after 1 year of operation represents a penalty of 1,400. Cloud Computing eliminates this penalty.Second, technology trends suggest that over the useful lifetime of some purchased equipment, hardware costswill fall and new hardware and software technologies will become available. Cloud providers, who already enjoyeconomyofscale buying power as described in Section 3, can potentially pass on some of these savings to theircustomers. Indeed, heavy users of AWS saw storage costs fall 20 and networking costs fall 50 over the last 2.5years, and the addition of nine new services or features to AWS over less than one year. 7 If new technologies orpricing plans become available to a cloud vendor, existing applications and customers can potentially benefit fromthem immediately, without incurring a capital expense. In less than two years, Amazon Web Services increased thenumber of different types of compute servers instances from one to five, and in less than one year they added sevennew infrastructure services and two new operational support options. 86.2 Comparing Costs Should I Move to the CloudWhereas the previous section tried to quantify the economic value of specific Cloud Computing benefits such aselasticity, this section tackles an equally important but larger question Is it more economical to move my existingdatacenterhosted service to the cloud, or to keep it in a datacenterTable 5 updates Grays 2003 cost data 21 to 2008, allowing us to track the rate of change of key technologies forCloud Computing for the last 5 years. Note that, as expected, widearea networking costs have improved the least in 5years, by less than a factor of 3. While computing costs have improved the most in 5 years, the ability to use the extracomputing power is based on the assumption that programs can utilize all the cores on both sockets in the computer.This assumption is likely more true for Utility Computing, with many Virtual Machines serving thousands to millionsof customers, than it is for programs inside the datacenter of a single company.To facilitate calculations, Gray calculated what 1 bought in 2003. Table 5 shows his numbers vs. 2008 andcompares to EC2S3 charges. At first glance, it appears that a given dollar will go further if used to purchase hardwarein 2008 than to pay for use of that same hardware. However, this simple analysis glosses over several important factors.Pay separately per resource. Most applications do not make equal use of computation, storage, and networkbandwidth some are CPUbound, others networkbound, and so on, and may saturate one resource while underutiliz12Table 5 We update Grays costs of computing resources from 2003 to 2008, normalize to what 1 could buy in 2003vs. 2008, and compare to the cost of paying per use of 1 worth of resources on AWS at 2008 prices.WAN bandwidthmo. CPU hours all cores disk storageItem in 2003 1 Mbps WAN link 2 GHz CPU, 2 GB DRAM 200 GB disk, 50 Mbstransfer rateCost in 2003 100mo. 2000 2001 buys in 2003 1 GB 8 CPU hours 1 GBItem in 2008 100 Mbps WAN link 2 GHz, 2 sockets, 4coressocket, 4 GB DRAM1 TB disk, 115 MBs sustained transferCost in 2008 3600mo. 1000 1001 buys in 2008 2.7 GB 128 CPU hours 10 GBcostperformanceimprovement2.7x 16x 10xCost to rent 1 0.270.40 2.56 1.201.50worth on AWS in20080.100.15GB  3 GB 128 2 VMs0.10each0.120.15GBmonth 10 GBing others. Payasyougo Cloud Computing can charge the application separately for each type of resource, reducingthe waste of underutilization. While the exact savings depends on the application, suppose the CPU is only 50utilized while the network is at capacity then in a datacenter you are effectively paying for double the number ofCPU cycles actually being used. So rather than saying it costs 2.56 to rent only 1 worth of CPU, it would be moreaccurate to say it costs 2.56 to rent 2 worth of CPU. As a side note, AWSs prices for widearea networking areactually more competitive than what a mediumsized company would pay for the same bandwidth.Power, cooling and physical plant costs. The costs of power, cooling, and the amortized cost of the building aremissing from our simple analyses so far. Hamilton estimates that the costs of CPU, storage and bandwidth roughlydouble when those costs are amortized over the buildings lifetime 23, 26. Using this estimate, buying 128 hoursof CPU in 2008 really costs 2 rather than 1, compared to 2.56 on EC2. Similarly, 10 GB of disk space costs 2rather than 1, compared to 1.201.50 per month on S3. Lastly, S3 actually replicates the data at least 3 times fordurability and performance, ensure durability, and will replicate it further for performance is there is high demand forthe data. That means the costs are 6.00 when purchasing vs. 1.20 to 1.50 per month on S3.Operations costs. Today, hardware operations costs are very lowrebooting servers is easy e.g., IP addressablepower strips, separate out of band controllers, and so on and minimally trained staff can replace broken componentsat the rack or server level. On one hand, since Utility Computing uses virtual machines instead of physical machines,from the cloud users point of view these tasks are shifted to the cloud provider. On the other hand, depending on thelevel of virtualization, much of the software management costs may remainupgrades, applying patches, and so on.Returning to the managed vs. unmanaged discussion of Section 5, we believe these costs will be lower for managedenvironments e.g. Microsoft Azure, Google AppEngine, Force.com than for hardwarelevel utility computing e.g.Amazon EC2, but it seems hard to quantify these benefits in a way that many would agree with.With the above caveats in mind, here is a simple example of deciding whether to move a service into the cloud.Example Moving to cloud. Suppose a biology lab creates 500 GB of new data for every wet lab experiment. A computer the speed of one EC2 instance takes 2 hours per GB to process the new data. The lab hasthe equivalent 20 instances locally, so the time to evaluate the experiment is 500 220 or 50 hours. Theycould process it in a single hour on 1000 instances at AWS. The cost to process one experiment would bejust 1000 0.10 or 100 in computation and another 500 0.10 or 50 in network transfer fees. So far,so good. They measure the transfer rate from the lab to AWS at 20 Mbitssecond. 19 The transfer time is500GB  1000MBGB  8bitsByte20Mbitssec  4, 000, 00020  200, 000 seconds or morethan 55 hours. Thus, it takes 50 hours locally vs. 55  1 or 56 hours on AWS, so they dont move to thecloud. The next section offers an opportunity on how to overcome the transfer delay obstacle.A related issue is the software complexity and costs of partial or full migrating data from a legacy enterpriseapplication into the Cloud. While migration is a onetime task, the amount of effort can be significant and it needs to beconsidered as a factor in deciding to use Cloud Computing. This task is already spawning new business opportunitiesfor companies that provide data integration across public and private Clouds.13Table 6 Top 10 Obstacles to and Opportunities for Adoption and Growth of Cloud Computing.Obstacle Opportunity1 Availability of Service Use Multiple Cloud Providers to provide Business ContinuityUse Elasticity to Defend Against DDOS attacks2 Data LockIn Standardize APIsMake compatible software available to enable Surge Computing3 Data Confidentiality and Auditability Deploy Encryption, VLANs, and FirewallsAccommodate National Laws via Geographical Data Storage4 Data Transfer Bottlenecks FedExing Disks Data BackupArchivalLower WAN Router Costs Higher Bandwidth LAN Switches5 Performance Unpredictability Improved Virtual Machine Support Flash MemoryGang Scheduling VMs for HPC apps6 Scalable Storage Invent Scalable Store7 Bugs in LargeScale Distributed Systems Invent Debugger that relies on Distributed VMs8 Scaling Quickly Invent AutoScaler that relies on Machine LearningSnapshots to encourage Cloud Computing Conservationism9 Reputation Fate Sharing Offer reputationguarding services like those for email10 Software Licensing Payforuse licenses Bulk use sales7 Top 10 Obstacles and Opportunities for Cloud ComputingIn this section, we offer a ranked list of obstacles to the growth of Cloud Computing. Each obstacle is paired withan opportunityour thoughts on how to overcome the obstacle, ranging from straightforward product developmentto major research projects. Table 6 summarizes our top ten obstacles and opportunities. The first three are technicalobstacles to the adoption of Cloud Computing, the next five are technical obstacles to the growth of Cloud Computingonce it has been adopted, and the last two are policy and business obstacles to the adoption of Cloud Computing.Number 1 Obstacle Availability of a ServiceOrganizations worry about whether Utility Computing services will have adequate availability, and this makes somewary of Cloud Computing. Ironically, existing SaaS products have set a high standard in this regard. Google Searchis effectively the dial tone of the Internet if people went to Google for search and it wasnt available, they wouldthink the Internet was down. Users expect similar availability from new services, which is hard to do. Table 7 showsrecorded outages for Amazon Simple Storage Service S3, AppEngine and Gmail in 2008, and explanations for theoutages. Note that despite the negative publicity due to these outages, few enterprise IT infrastructures are as good.Table 7 Outages in AWS, AppEngine, and GmailService and Outage Duration DateS3 outage authentication service overload leading to unavailability 39 2 hours 21508S3 outage Single bit error leading to gossip protocol blowup. 41 68 hours 72008AppEngine partial outage programming error 43 5 hours 61708Gmail site unavailable due to outage in contacts system 29 1.5 hours 81108Just as large Internet service providers use multiple network providers so that failure by a single company willnot take them off the air, we believe the only plausible solution to very high availability is multiple Cloud Computingproviders. The highavailability computing community has long followed the mantra no single source of failure,yet the management of a Cloud Computing service by a single company is in fact a single point of failure. Evenif the company has multiple datacenters in different geographic regions using different network providers, it mayhave common software infrastructure and accounting systems, or the company may even go out of business. Largecustomers will be reluctant to migrate to Cloud Computing without a businesscontinuity strategy for such situations.We believe the best chance for independent software stacks is for them to be provided by different companies, as ithas been difficult for one company to justify creating and maintain two stacks in the name of software dependability.Another availability obstacle is Distributed Denial of Service DDoS attacks. Criminals threaten to cut off theincomes of SaaS providers by making their service unavailable, extorting 10,000 to 50,000 payments to prevent thelaunch of a DDoS attack. Such attacks typically use large botnets that rent bots on the black market for 0.03 per14bot simulated bogus user per week 36. Utility Computing offers SaaS providers the opportunity to defend againstDDoS attacks by using quick scaleup. Suppose an EC2 instance can handle 500 bots, and an attack is launched thatgenerates an extra 1 GBsecond of bogus network bandwidth and 500,000 bots. At 0.03 per bot, such an attackwould cost the attacker 15,000 invested up front. At AWSs current prices, the attack would cost the victim an extra360 per hour in network bandwidth and an extra 100 per hour 1,000 instances of computation. The attack wouldtherefore have to last 32 hours in order to cost the potential victim more than it would the blackmailer. A botnet attackthis long may be difficult to sustain, since the longer an attack lasts the easier it is to uncover and defend against, andthe attacking bots could not be immediately reused for other attacks on the same provider. As with elasticity, CloudComputing shifts the attack target from the SaaS provider to the Utility Computing provider, who can more readilyabsorb it and as we argued in Section 3 is also likely to have already DDoS protection as a core competency.Number 2 Obstacle Data LockInSoftware stacks have improved interoperability among platforms, but the APIs for Cloud Computing itself are stillessentially proprietary, or at least have not been the subject of active standardization. Thus, customers cannot easilyextract their data and programs from one site to run on another. Concern about the difficult of extracting data from thecloud is preventing some organizations from adopting Cloud Computing. Customer lockin may be attractive to CloudComputing providers, but Cloud Computing users are vulnerable to price increases as Stallman warned, to reliabilityproblems, or even to providers going out of business.For example, an online storage service called The Linkup shut down on August 8, 2008 after losing access as muchas 45 of customer data 12. The Linkup, in turn, had relied on the online storage service Nirvanix to store customerdata, and now there is finger pointing between the two organizations as to why customer data was lost. Meanwhile,The Linkups 20,000 users were told the service was no longer available and were urged to try out another storage site.The obvious solution is to standardize the APIs so that a SaaS developer could deploy services and data acrossmultiple Cloud Computing providers so that the failure of a single company would not take all copies of customer datawith it. The obvious fear is that this would lead to a racetothebottom of cloud pricing and flatten the profits ofCloud Computing providers. We offer two arguments to allay this fear.First, the quality of a service matters as well as the price, so customers will not necessarily jump to the lowest costservice. Some Internet Service Providers today cost a factor of ten more than others because they are more dependableand offer extra services to improve usability.Second, in addition to mitigating data lockin concerns, standardization of APIs enables a new usage model inwhich the same software infrastructure can be used in a Private Cloud and in a Public Cloud. 9 Such an option couldenable Surge Computing, in which the public Cloud is used to capture the extra tasks that cannot be easily run in thedatacenter or private cloud due to temporarily heavy workloads. 10Number 3 Obstacle Data Confidentiality and AuditabilityMy sensitive corporate data will never be in the cloud. Anecdotally we have heard this repeated multiple times.Current cloud offerings are essentially public rather than private networks, exposing the system to more attacks.There are also requirements for auditability, in the sense of SarbanesOxley and Health and Human Services HealthInsurance Portability and Accountability Act HIPAA regulations that must be provided for corporate data to bemoved to the cloud.We believe that there are no fundamental obstacles to making a cloudcomputing environment as secure as thevast majority of inhouse IT environments, and that many of the obstacles can be overcome immediately with wellunderstood technologies such as encrypted storage, Virtual Local Area Networks, and network middleboxes e.g.firewalls, packet filters. For example, encrypting data before placing it in a Cloud may be even more secure thanunencrypted data in a local data center this approach was successfully used by TC3, a healthcare company with accessto sensitive patient records and healthcare claims, when moving their HIPAAcompliant application to AWS 2.Similarly, auditability could be added as an additional layer beyond the reach of the virtualized guest OS orvirtualized application environment, providing facilities arguably more secure than those built into the applicationsthemselves and centralizing the software responsibilities related to confidentiality and auditability into a single logicallayer. Such a new feature reinforces the Cloud Computing perspective of changing our focus from specific hardwareto the virtualized capabilities being provided.A related concern is that many nations have laws requiring SaaS providers to keep customer data and copyrightedmaterial within national boundaries. Similarly, some businesses may not like the ability of a country to get access totheir data via the court system for example, a European customer might be concerned about using SaaS in the UnitedStates given the USA PATRIOT Act.15Cloud Computing gives SaaS providers and SaaS users greater freedom to place their storage. For example,Amazon provides S3 services located physically in the United States and in Europe, allowing providers to keep data inwhichever they choose. With AWS regions, a simple configuration change avoids the need to find and negotiate witha hosting provider overseas.Number 4 Obstacle Data Transfer BottlenecksApplications continue to become more dataintensive. If we assume applications may be pulled apart across theboundaries of clouds, this may complicate data placement and transport. At 100 to 150 per terabyte transferred,these costs can quickly add up, making data transfer costs an important issue. Cloud users and cloud providers have tothink about the implications of placement and traffic at every level of the system if they want to minimize costs. Thiskind of reasoning can be seen in Amazons development of their new Cloudfront service.One opportunity to overcome the high cost of Internet transfers is to ship disks. Jim Gray found that the cheapestway to send a lot of data is to physically send disks or even whole computers via overnight delivery services 22.Although there are no guarantees from the manufacturers of disks or computers that you can reliably ship data thatway, he experienced only one failure in about 400 attempts and even this could be mitigated by shipping extra diskswith redundant data in a RAIDlike manner.To quantify the argument, assume that we want to ship 10 TB from U.C. Berkeley to Amazon in Seattle, Washington. Garfinkel measured bandwidth to S3 from three sites and found an average write bandwidth of 5 to 18Mbitssecond. 19 Suppose we get 20 Mbitsec over a WAN link. It would take10  1012 Bytes  20 106 bitssecond  8 10132 107 seconds  4,000,000 seconds,which is more than 45 days. Amazon would also charge you 1000 in network transfer fees when it received the data.If we instead sent ten 1 TB disks via overnight shipping, it would take less than a day to transfer 10 TB and thecost would be roughly 400, an effective bandwidth of about 1500 Mbitsec.11 Thus, Netflix for Cloud Computingcould halve costs of bulk transfers into the cloud but more importantly reduce latency by a factor of 45.Returning to the biology lab example from Section 6, it would take about 1 hour to write a disk, 16 hours to FedExa disk, about 1 hour to read 500 GB, and then 1 hour to process it. Thus, the time to process the experiment would be20 hours instead of 50, and the cost is would be around 200 per experiment, so they decide to move to the cloud afterall. As disk capacity and costpergigabyte are growing much faster than network costperformance10X vs. lessthan 3X in the last 5 years according to Table 5the FedEx disk option for large data transfers will get more attractiveeach year.A second opportunity is to find other reasons to make it attractive to keep data in the cloud, for once data is in thecloud for any reason it may no longer be a bottleneck and may enable new services that could drive the purchase ofCloud Computing cycles. Amazon recently began hosting large public datasets e.g. US Census data for free on S3since there is no charge to transfer data between S3 and EC2, these datasets might attract EC2 cycles. As anotherexample, consider offsite archival and backup services. Since companies like Amazon, Google, and Microsoft likelysend much more data than they receive, the cost of ingress bandwidth could be much less. Therefore, for example, ifweekly full backups are moved by shipping physical disks and compressed daily incremental backups are sent overthe network, Cloud Computing might be able to offer an affordable offpremise backup service. Once archived data isin the cloud, new services become possible that could result in selling more Cloud Computing cycles, such as creatingsearchable indices of all your archival data or performing image recognition on all your archived photos to group themaccording to who appears in each photo.12A third, more radical opportunity is to try to reduce the cost of WAN bandwidth more quickly. One estimate isthat twothirds of the cost of WAN bandwidth is the cost of the highend routers, whereas only onethird is the fibercost 27. Researchers are exploring simpler routers built from commodity components with centralized control as alowcost alternative to the highend distributed routers 33. If such technology were deployed by WAN providers, wecould see WAN costs dropping more quickly than they have historically.In addition to WAN bandwidth being a bottleneck, intracloud networking technology may be a performancebottleneck as well. Today inside the datacenter, typically 2080 processing nodes within a rack are connected viaa topofrack switch to a second level aggregation switch. These in turn are connected via routers to storage areanetworks and widearea connectivity, such as the Internet or interdatacenter WANs. Inexpensive 1 Gigabit Ethernet 1GbE is universally deployed at the lower levels of aggregation. This bandwidth can represent a performancebottleneck for internode processing patterns that burst packets across the interconnect, such as the shuffle step thatoccurs between Map and Reduce producing. Another set of batch applications that need higher bandwidth is highperformance computing applications lack of bandwidth is one reason few scientists using Cloud Computing.10 Gigabit Ethernet is typically used for the aggregation links in cloud networks, but is currently too expensiveto deploy for individual servers about 1000 for a 10 GbE server connection today, vs. 100 for a 1GbE connection. However, as the cost per 10 GbE server connections is expected to drop to less than 200 in 2010, it will gain16RateMBsbin1000 1200 140001020304050607080ofTotalCountofRateMBsHistogramofStreamMemoryBenchmarkPerformanceofTotalCountofRateMBsforeachRateMBsbin.Thedataisf ilteredonmodelandRateMBs.Themodelf ilterkeepsDualCoreAMDOpterontmProcessor2218HE.TheRateMBsf ilterrangesfrom1144.905151367to1600.ratembsec5 15 25 35 45 55 65 750510152025ofTotalCountofRatembsecHistogramofSequentialDiskWritePerformancembsecFigure 3 a Memory benchmark performance on 75 Virtual Machines running the STREAM benchmark on left andb Disk performance writing 1 GB files on 75 Virtual Machines on right.widespread deployment inside the cloud since it has the highly desirable effect of reducing data transfer latencies andnetwork contention. This in turn enables more cores and virtual machines per physical server node by scaling up thenetwork. Also in 2010, 40 GbE and 100 GbE will appear for the higher aggregation layers 10.Number 5 Obstacle Performance UnpredictabilityOur experience is that multiple Virtual Machines can share CPUs and main memory surprisingly well in Cloud Computing, but that IO sharing is more problematic. Figure 3a shows the average memory bandwidth for 75 EC2instances running the STREAM memory benchmark 32. The mean bandwidth is 1355 MBytes per second, with astandard deviation of just 52 MBytessec, less than 4 of the mean. Figure 3b shows the average disk bandwidthfor 75 EC2 instances each writing 1 GB files to local disk. The mean disk write bandwidth is nearly 55 MBytes persecond with a standard deviation of a little over 9 MBytessec, more than 16 of the mean. This demonstrates theproblem of IO interference between virtual machines.One opportunity is to improve architectures and operating systems to efficiently virtualize interrupts and IO channels. Technologies such as PCIexpress are difficult to virtualize, but they are critical to the cloud. One reason to behopeful is that IBM mainframes and operating systems largely overcame these problems in the 1980s, so we havesuccessful examples from which to learn.Another possibility is that flash memory will decrease IO interference. Flash is semiconductor memory thatpreserves information when powered off like mechanical hard disks, but since it has no moving parts, it is much fasterto access microseconds vs. milliseconds and uses less energy. Flash memory can sustain many more IOs per secondper gigabyte of storage than disks, so multiple virtual machines with conflicting random IO workloads could coexistbetter on the same physical computer without the interference we see with mechanical disks. The lack of interferencethat we see with semiconductor main memory in Figure 3a might extend to semiconductor storage as well, therebyincreasing the number of applications that can run well on VMs and thus share a single computer. This advance couldlower costs to Cloud Computing providers, and eventually to Cloud Computing consumers.Another unpredictability obstacle concerns the scheduling of virtual machines for some classes of batch processingprograms, specifically for high performance computing. Given that highperformance computing is used to justifyGovernment purchases of 100M supercomputer centers with 10,000 to 1,000,000 processors, there certainly aremany tasks with parallelism that can benefit from elastic computing. Cost associativity means that there is no costpenalty for using 20 times as much computing for 120th the time. Potential applications that could benefit includethose with very high potential financial returnsfinancial analysis, petroleum exploration, movie animationandcould easily justify paying a modest premium for a 20x speedup. One estimate is that a third of todays server marketis highperformance computing 10.The obstacle to attracting HPC is not the use of clusters most parallel computing today is done in large clustersusing the messagepassing interface MPI. The problem is that many HPC applications need to ensure that all thethreads of a program are running simultaneously, and todays virtual machines and operating systems do not provide17a programmervisible way to ensure this. Thus, the opportunity to overcome this obstacle is to offer something likegang scheduling for Cloud Computing.13Number 6 Obstacle Scalable StorageEarly in this paper, we identified three properties whose combination gives Cloud Computing its appeal shorttermusage which implies scaling down as well as up when resources are no longer needed, no upfront cost, and infinitecapacity ondemand. While its straightforward what this means when applied to computation, its less obvious howto apply it to persistent storage.As Table 4 shows, there have been many attempts to answer this question, varying in the richness of the query andstorage APIs, the performance guarantees offered, and the complexity of data structures that are directly supportedby the storage system e.g., schemaless blobs vs. columnoriented storage.14 The opportunity, which is still an openresearch problem, is to create a storage system would not only meet these needs but combine them with the cloudadvantages of scaling arbitrarily up and down ondemand, as well as meeting programmer expectations in regard toresource management for scalability, data durability, and high availability.Number 7 Obstacle Bugs in LargeScale Distributed SystemsOne of the difficult challenges in Cloud Computing is removing errors in these very large scale distributed systems. Acommon occurrence is that these bugs cannot be reproduced in smaller configurations, so the debugging must occur atscale in the production datacenters.One opportunity may be the reliance on virtual machines in Cloud Computing. Many traditional SaaS providersdeveloped their infrastructure without using VMs, either because they preceded the recent popularity of VMs orbecause they felt they could not afford the performance hit of VMs. Since VMs are de rigueur in Utility Computing,that level of virtualization may make it possible to capture valuable information in ways that are implausible withoutVMs.Number 8 Obstacle Scaling QuicklyPayasyougo certainly applies to storage and to network bandwidth, both of which count bytes used. Computationis slightly different, depending on the virtualization level. Google AppEngine automatically scales in response toload increases and decreases, and users are charged by the cycles used. AWS charges by the hour for the number ofinstances you occupy, even if your machine is idle.The opportunity is then to automatically scale quickly up and down in response to load in order to save money,but without violating service level agreements. Indeed, one RAD Lab focus is the pervasive and aggressive use ofstatistical machine learning as a diagnostic and predictive tool that would allow dynamic scaling, automatic reactionto performance and correctness problems, and generally automatic management of many aspects of these systems.Another reason for scaling is to conserve resources as well as money. Since an idle computer uses about twothirdsof the power of a busy computer, careful use of resources could reduce the impact of datacenters on the environment,which is currently receiving a great deal of negative attention. Cloud Computing providers already perform carefuland low overhead accounting of resource consumption. By imposing perhour and perbyte costs, utility computingencourages programmers to pay attention to efficiency i.e., releasing and acquiring resources only when necessary,and allows more direct measurement of operational and development inefficiencies.Being aware of costs is the first step to conservation, but the hassles of configuration make it tempting to leavemachines idle overnight so that nothing has to be done to get started when developers return to work the next day. Afast and easytouse snapshotrestart tool might further encourage conservation of computing resources.Number 9 Obstacle Reputation Fate SharingReputations do not virtualize well. One customers bad behavior can affect the reputation of the cloud as a whole. Forinstance, blacklisting of EC2 IP addresses 31 by spamprevention services may limit which applications can be effectively hosted. An opportunity would be to create reputationguarding services similar to the trusted email servicescurrently offered for a fee to services hosted on smaller ISPs, which experience a microcosm of this problem.Another legal issue is the question of transfer of legal liabilityCloud Computing providers would want legalliability to remain with the customer and not be transferred to them i.e., the company sending the spam should beheld liable, not Amazon.18Number 10 Obstacle Software LicensingCurrent software licenses commonly restrict the computers on which the software can run. Users pay for the softwareand then pay an annual maintenance fee. Indeed, SAP announced that it would increase its annual maintenance fee toat least 22 of the purchase price of the software, which is comparable to Oracles pricing 38. Hence, many cloudcomputing providers originally relied on open source software in part because the licensing model for commercialsoftware is not a good match to Utility Computing.The primary opportunity is either for open source to remain popular or simply for commercial software companiesto change their licensing structure to better fit Cloud Computing. For example, Microsoft and Amazon now offerpayasyougo software licensing for Windows Server and Windows SQL Server on EC2. An EC2 instance runningMicrosoft Windows costs 0.15 per hour instead of the traditional 0.10 per hour of the open source version.15A related obstacle is encouraging sales forces of software companies to sell products into Cloud Computing. Payasyougo seems incompatible with the quarterly sales tracking used to measure effectiveness, which is based ononetime purchases. The opportunity for cloud providers is simply to offer prepaid plans for bulk use that can be soldat discount. For example, Oracle sales people might sell 100,000 instance hours using Oracle that can be used overthe next two years at a cost less than is the customer were to purchase 100,000 hours on their own. They could thenmeet their quarterly quotas and make their commissions from cloud sales as well as from traditional software sales,potentially converting this customerfacing part of a company from naysayers into advocates of cloud computing.8 Conclusion and Questions about the Clouds of TomorrowThe long dreamed vision of computing as a utility is finally emerging. The elasticity of a utility matches the need ofbusinesses providing services directly to customers over the Internet, as workloads can grow and shrink far fasterthan 20 years ago. It used to take years to grow a business to several million customers  now it can happen in months.From the cloud providers view, the construction of very large datacenters at low cost sites using commoditycomputing, storage, and networking uncovered the possibility of selling those resources on a payasyougo modelbelow the costs of many mediumsized datacenters, while making a profit by statistically multiplexing among a largegroup of customers. From the cloud users view, it would be as startling for a new software startup to build its owndatacenter as it would for a hardware startup to build its own fabrication line. In addition to startups, many otherestablished organizations take advantage of the elasticity of Cloud Computing regularly, including newspapers like theWashington Post, movie companies like Pixar, and universities like ours. Our lab has benefited substantially from theability to complete research by conference deadlines and adjust resources over the semester to accommodate coursedeadlines. As Cloud Computing users, we were relieved of dealing with the twin dangers of overprovisioning andunderprovisioning our internal datacenters.Some question whether companies accustomed to highmargin businesses, such as ad revenue from search enginesand traditional packaged software, can compete in Cloud Computing. First, the question presumes that Cloud Computing is a small margin business based on its low cost. Given the typical utilization of mediumsized datacenters, thepotential factors of 5 to 7 in economies of scale, and the further savings in selection of cloud datacenter locations, theapparently low costs offered to cloud users may still be highly profitable to cloud providers. Second, these companiesmay already have the datacenter, networking, and software infrastructure in place for their mainline businesses, soCloud Computing represents the opportunity for more income at little extra cost.Although Cloud Computing providers may run afoul of the obstacles summarized in Table 6, we believe that overthe long run providers will successfully navigate these challenges and set an example for others to follow, perhaps bysuccessfully exploiting the opportunities that correspond to those obstacles.Hence, developers would be wise to design their next generation of systems to be deployed into Cloud Computing. In general, the emphasis should be horizontal scalability to hundreds or thousands of virtual machines over theefficiency of the system on a single virtual machine. There are specific implications as well Applications Software of the future will likely have a piece that runs on clients and a piece that runs in theCloud. The cloud piece needs to both scale down rapidly as well as scale up, which is a new requirement forsoftware systems. The client piece needs to be useful when disconnected from the Cloud, which is not the casefor many Web 2.0 applications today. Such software also needs a payforuse licensing model to match needsof Cloud Computing. Infrastructure Software of the future needs to be cognizant that it is no longer running on bare metal but onvirtual machines. Moreover, it needs to have billing built in from the beginning, as it is very difficult to retrofitan accounting system.19 Hardware Systems of the future need to be designed at the scale of a container at least a dozen racks ratherthan at the scale of a single 1U box or single rack, as that is the minimum level at which it will be purchased. Costof operation will match performance and cost of purchase in importance in the acquisition decision. Hence, theyneed to strive for energy proportionality 9 by making it possible to put into low power mode the idle portions ofthe memory, storage, and networking, which already happens inside a microprocessor today. Hardware shouldalso be designed assuming that the lowest level software will be virtual machines rather than a single nativeoperating system, and it will need to facilitate flash as a new level of the memory hierarchy between DRAM anddisk. Finally, we need improvements in bandwidth and costs for both datacenter switches and WAN routers.While we are optimistic about the future of Cloud Computing, we would love to look into a crystal ball to see howpopular it is and what it will look like in five yearsChange In Technology and Prices Over Time What will billing units be like for the higherlevel virtualizationclouds What will Table 5, tracking the relative prices of different resources, look like Clearly, the number ofcores per chip will increase over time, doubling every two to four years. Flash memory has the potential of addinganother relatively fast layer to the classic memory hierarchy what will be its billing unit Will technology or businessinnovations accelerate network bandwidth pricing, which is currently the most slowlyimproving technologyVirtualization Level Will Cloud Computing be dominated by lowlevel hardware virtual machines like AmazonEC2, intermediate language offerings like Microsoft Azure, or highlevel frameworks like Google AppEngine Orwill we have many virtualization levels that match different applications Will valueadded services by independentcompanies like RightScale, Heroku, or EngineYard survive in Utility Computing, or will the successful services beentirely coopted by the Cloud providers If they do consolidate to a single virtualization layer, will multiple companies embrace a common standard Will this lead to a race to the bottom in pricing so that its unattractive to become aCloud Computing provider, or will they differentiate in services or quality to maintain marginsAcknowledgmentsWe all work in the RAD Lab. Its existence is due to the generous support of the founding members Google, Microsoft, and Sun Microsystems and to the affiliate members Amazon Web Services, Cisco Systems, Facebook, HewlettPackard, IBM, NEC, Network Appliance, Oracle, Siemens, and VMware by matching funds from the State of Californias MICRO program grants 06152, 07010, 06148, 07012, 06146, 07009, 06147, 07013, 06149, 06150,and 07008 and the University of California IndustryUniversity Cooperative Research Program UC Discovery grantCOM0710240 and by the National Science Foundation grant CNS0509559.We would also like to thank the following people for feedback that improved the alpha draft of this report LuizBarroso, Andy Bechtolsheim, John Cheung, David Cheriton, Mike Franklin, James Hamilton, Jeff Hammerbacher,Marvin Theimer, Hal Varian, and Peter Vosshall. For the beta draft, wed like to thank the following for their comments Andy Bechtolsheim, Jim Blakely, Paul Hofmann, Kim Keeton, Jim Larus, John Ousterhout, Steve Whittaker,and Feng Zhao.Notes1The related term grid computing, from the High Performance Computing community, suggests protocols to offer shared computation andstorage over long distances, but those protocols did not lead to a software environment that grew beyond its community. Another phrase found inCloud Computing papers is multitenant, which simply means multiple customers from different companies are using SaaS, so customers and theirdata need to be protected from each other.2The challenge of disconnected operation is not new to cloud computing extensive research has examined the problems of disconnected operation, with roots in the Coda filesystem 30 and the Bayou database 18. We simply point out that satisfactory applicationlevel and protocollevelsolutions have been developed and adopted in many domains, including IMAP email, CalDAV calendars, versioncontrol systems such as CVS andSubversion, and recently, Google Gears for JavaScript inbrowser applications that can run disconnected. We are confident that similar approacheswill develop as demanded by mobile applications that wish to use the cloud.3Usagebased pricing is different from renting. Renting a resource involves paying a negotiated cost to have the resource over some time period,whether or not you use the resource. Payasyougo involves metering usage and charging based on actual use, independently of the time periodover which the usage occurs. Amazon AWS rounds up their billing to the nearest serverhour or gigabytemonth, but the associated dollar amountsare small enough pennies to make AWS a true payasyougo service.4The most common financial models used in the US allow a capital expense to be depreciated deducted from tax obligations linearly over a3year period, so we use this figure as an estimate of equipment lifetime in our cost comparisons.5According to statistics collected by Keynote Systems Inc. on Black Friday 2008 November 28th, Target and Amazons ecommerce siteswere slower on Friday  a transaction that took 25 seconds last week required about 40 seconds Friday morning 5.62nd edition of HennessyPatterson had these rules of thumb for storage systems IO bus  75 Disk bus SCSI  40 when attach multiple disks per bus20 Disk arm seeking  60 Disk IO per second or MBs  80 peakHence, 60 to 80 is a safe upper bound.7Table 8 shows changes in prices for AWS storage and networking over 2.5 years.Table 8 Changes in price of AWS S3 storage and networking over time.Storage Cost of Data Stored per GBMonthDate  50 TB 50100 TB 100500 TB  500 TB31306 0.15 0.15 0.15 0.1510908 0.15 0.14 0.13 0.12 Original Price 100 93 87 80Networking Cost per GB of WideArea Networking TrafficDate In Out  10 TB Out 1050 TB Out 50150 TB Out 150 TB31306 0.20 0.20 0.20 0.20 0.20103107 0.10 0.18 0.16 0.13 0.135108 0.10 0.17 0.13 0.11 0.10 Original Price 50 85 65 55 508 Table 9 shows the new services and support options AWS added during 2008, and the date of each introduction. Table 10 shows the differenttypes of AWS compute instances and the date each type was introduced.Table 9 New AWS Services.Date New Service3Dec08 Public Data Sets on AWS Now Available18Nov08 Announcing Amazon CloudFront Content Distribution Network23Oct08 Amazon EC2 Running Windows Server Now Available23Oct08 Amazon EC2 Exits Beta and Now Offers a Service Level Agreement22Sep08 Oracle Products Licensed for Amazon Web Services20Aug08 Amazon Elastic Block Store Now Available5May08 OpenSolaris and MySQL Enterprise on Amazon EC216Apr08 Announcing AWS Premium Support26Mar08 Announcing Elastic IP Addresses and Availability Zones for Amazon EC2Table 10 Diversity of EC2 instances over time.Date Type CostHourComputeUnitsDRAMGBDisk GB Compute GB DRAM GB Disk82406 Small 0.10 1 1.7 160 10 17.0 1600102207 Large 0.40 4 7.5 850 10 18.8 2130102207 Extra Large 0.80 8 15.0 1690 10 18.8 211052908 HighCPU Medium 0.20 5 1.7 350 25 8.5 175052908 HighCPU Extra Large 0.80 20 7.0 1690 25 8.8 21109While such standardization can occur for the full spectrum of utility computing, the ability of the leading cloud providers to distribute softwareto match standardized APIs varies. Microsoft is in the software distribution business, so it would seem to be a small step for Azure to publish all theAPIs and offer software to run in the datacenter. Interestingly for AWS and Google AppEngine, the best examples of standardizing APIs come fromopen sources efforts from outside these companies. Hadoop and Hypertable are efforts to recreate the Google infrastructure 11, and Eucalyptusrecreates important aspects of the EC2 API 34.10Indeed, harking back to Section 2, surge chip fabrication is one of the common uses of chiples fabrication companies like TSMC.11A 1TB 3.5 disk weighs 1.4 pounds. If we assume that packaging material adds about 20 to the weight, the shipping weight of 10 disks is 17pounds. FedEx charges about 100 to deliver such a package by 1030 AM the next day and about 50 to deliver it in 2 days. Similar to Netflix,Amazon might let you have one disk boat on loan to use when you need it. Thus, the roundtrip shipping cost for Amazon to ship you a set ofdisks and for you to ship it back is 150, assuming 2day delivery from Amazon and overnight delivery to send it to Amazon. It would then takeAmazon about 2.4 hours to dump the disk contents into their datacenter a 1 TB disk can transfer at 115 Mbytessec. If each disk contains wholefiles e.g. a Linux ext3 or Windows NTFS filesystem, all disks could be read or written in parallel. While its hard to put a cost of internal datacenter LAN bandwidth, it is surely at least 100x less expensive than WAN bandwidth. Lets assume the labor costs to unpack disks, load them sothat they can be read, repackage them, and so on is 20 per disk.The total latency is then less than a day 2.4 hours to write, 1418 hours for overnight shipping, 2.4 hours to read at a cost about 400 50 toreceive from Amazon, 100 to send to Amazon, 200 for labor costs, and 40 charge for internal Amazon LAN bandwidth and labor charges.Rather than ship disks, another option would be to ship a whole disk array including sheet metal, fans, power suppliers, and network interfaces.The extra components would increase the shipping weight, but it would simplify connection of storage to the Cloud and to the local device andreduce labor. Note that you would want a lot more network bandwidth than is typically provided in conventional disk arrays, since you dont wantto stretch the time load or unload the data.12 The relatively new company Data Domain uses specialized compression algorithms tailored to incremental backups, they can reduce the sizeof these backups by a factor of 20. Note that compression can also reduce the cost to utility computing providers of their standard storage products.Lossless compression can reduce data demands by factors for two to three for many data types, and much higher for some. The Cloud Computing21provider likely has spare computation cycles at many times that could be used to compress data that has not been used recently. Thus, the actualstorage costs could be two to three times less than customers believe they are storing. Although customers could do compression as well, they haveto pay the computing cycles to compress and decompress the data, and do not have the luxury of free computation.A second advantage that customers cannot have is to dedupe files across multiple customers. This approach to storage calculates a signaturefor each file and then only stores a single copy of it on disk. Examples of files that could be identical across many customers include binaries forpopular programs and popular images and other media.13For example, to simplify parallel programming its common to have phases where all the threads compute and then all the threads communicate.Computation and communication phases are separated by a barrier synchronization, where every thread must wait until the last thread is finishingcomputing or communicating. If some threads of the gang are not running, that slows down these phases until they all have run. Although wecould ask highperformance computing programmers to rewrite their programs using more relaxed synchronization, such as that found in GooglesMap Reduce, a shorter term option would just be for the Cloud Computing provider to offer simultaneous gang scheduling of virtual machines as aUtility Computing option.14Among Amazons earliest offering was S3, a primarykeyonly store for large binary objects. While S3 manages its own replication, failuremasking and provisioning, the programmatic API is that of a keyvalue store i.e., a hash table, the response time is not adequate for interactiveclientserver applications, and the data stored in S3 is opaque from the storage systems point of view i.e., one cannot query or manage data basedon any property other than its arbitrary primary key. Amazons Elastic Block Store service allows customers to create a file system on a virtualizedblock device, but resource management and longterm redundancy are left to the programmer of each application this represents an impedancemismatch with application developers, who now routinely rely on storage systems that perform additional resource management and provide anAPI that exposes the structure of the stored data. Amazon S3 and Google BigTable do this automatically, but their programmatic APIs do not exposemuch of the structure of the stored data, in contrast to relational databases such as Amazon SimpleDB or Microsoft SQL Data Services.15The AWS announcement of Oracle product licensing only applies to users who are already Oracle customers on their local computers.References1 Cloudera, Hadoop training and support online. Available from httpwww.cloudera.com.2 TC3 Health Case Study Amazon Web Services online. Available from httpaws.amazon.comsolutionscasestudiestc3health.3 Washington Post Case Study Amazon Web Services online. Available from httpaws.amazon.comsolutionscasestudieswashingtonpost.4 Amazon.com CEO Jeff Bezos on Animoto online. April 2008. Available from httpblog.animoto.com20080421amazonceojeffbezosonanimoto.5 Black Friday traffic takes down Sears.com. Associated Press November 2008.6 ABRAMSON, D., BUYYA, R., AND GIDDY, J. A computational economy for grid computing and its implementation in theNimrodG resource broker. Future Generation Computer Systems 18, 8 2002, 10611074.7 ADMINISTRATION, E. I. State Electricity Prices, 2006 online. Available from httpwww.eia.doe.govneicrankingsstateelectricityprice.htm.8 AMAZON AWS. Public Data Sets on AWS online. 2008. Available from httpaws.amazon.compublicdatasets.9 BARROSO, L. A., AND HOLZLE, U. The Case for EnergyProportional Computing. IEEE Computer 40, 12 December2007.10 BECHTOLSHEIM, A. Cloud Computing and Cloud Networking. talk at UC Berkeley, December 2008.11 BIALECKI, A., CAFARELLA, M., CUTTING, D., AND OMALLEY, O. Hadoop a framework for running applications onlarge clusters built of commodity hardware. Wiki at httplucene. apache. orghadoop.12 BRODKIN, J. Loss of customer data spurs closure of online storage service The Linkup. Network World August 2008.13 CARR, N. Rough Type online. 2008. Available from httpwww.roughtype.com.14 CHANG, F., DEAN, J., GHEMAWAT, S., HSIEH, W., WALLACH, D., BURROWS, M., CHANDRA, T., FIKES, A., ANDGRUBER, R. Bigtable A distributed storage system for structured data. In Proceedings of the 7th USENIX Symposium onOperating Systems Design and Implementation OSDI06 2006.15 CHENG, D. PaaSonomics A CIOs Guide to using PlatformasaService to Lower Costs of Application Initiatives WhileImproving the Business Value of IT. Tech. rep., LongJump, 2008.16 DEAN, J., AND GHEMAWAT, S. Mapreduce simplified data processing on large clusters. In OSDI04 Proceedings ofthe 6th conference on Symposium on Opearting Systems Design  Implementation Berkeley, CA, USA, 2004, USENIXAssociation, pp. 1010.17 DECANDIA, G., HASTORUN, D., JAMPANI, M., KAKULAPATI, G., LAKSHMAN, A., PILCHIN, A., SIVASUBRAMANIAN,S., VOSSHALL, P., AND VOGELS, W. Dynamo Amazons highly available keyvalue store. In Proceedings of twentyfirstACM SIGOPS symposium on Operating systems principles 2007, ACM Press New York, NY, USA, pp. 205220.18 DEMERS, A. J., PETERSEN, K., SPREITZER, M. J., TERRY, D. B., THEIMER, M. M., AND WELCH, B. B. The bayouarchitecture Support for data sharing among mobile users. In Proceedings IEEE Workshop on Mobile Computing Systems Applications Santa Cruz, California, AugustSeptember 1994, pp. 27.2219 GARFINKEL, S. An Evaluation of Amazons Grid Computing Services EC2, S3 and SQS . Tech. Rep. TR0807, HarvardUniversity, August 2007.20 GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.T. The google file system. In SOSP 03 Proceedings ofthe nineteenth ACM symposium on Operating systems principles New York, NY, USA, 2003, ACM, pp. 2943.Available from httpportal.acm.orgftgateway.cfmid945450typepdfcollPortaldlGUIDECFID19219697CFTOKEN50259492.21 GRAY, J. Distributed Computing Economics. Queue 6, 3 2008, 6368. Available from httpportal.acm.orgftgateway.cfmid1394131typedigital20editioncollPortaldlGUIDECFID19219697CFTOKEN50259492.22 GRAY, J., AND PATTERSON, D. A conversation with Jim Gray. ACM Queue 1, 4 2003, 817.23 HAMILTON, J. Cost of Power in LargeScale Data Centers online. November 2008. Available from httpperspectives.mvdirona.com20081128CostOfPowerInLargeScaleDataCenters.aspx.24 HAMILTON, J. InternetScale Service Efficiency. In LargeScale Distributed Systems and Middleware LADIS WorkshopSeptember 2008.25 HAMILTON, J. Perspectives online. 2008. Available from httpperspectives.mvdirona.com.26 HAMILTON, J. Cooperative Expendable MicroSlice Servers CEMSLow Cost, Low Power Servers for InternetScaleServices. In Conference on Innovative Data Systems Research CIDR 09 January 2009.27 HOLZLE, U. Private communication, January 2009.28 HOSANAGAR, K., KRISHNAN, R., SMITH, M., AND CHUANG, J. Optimal pricing of content delivery network CDNservices. In The 37th Annual Hawaii International Conference onSystem Sciences 2004, pp. 205214.29 JACKSON, T. We feel your pain, and were sorry online. August 2008. Available from httpgmailblog.blogspot.com200808wefeelyourpainandweresorry.html.30 KISTLER, J. J., AND SATYANARAYANAN, M. Disconnected operation in the coda file system. In Thirteenth ACM Symposiumon Operating Systems Principles Asilomar Conference Center, Pacific Grove, U.S., 1991, vol. 25, ACM Press, pp. 213225.31 KREBS, B. Amazon Hey Spammers, Get Off My Cloud Washington Post July 2008.32 MCCALPIN, J. Memory bandwidth and machine balance in current high performance computers. IEEE Technical Committeeon Computer Architecture Newsletter 1995, 1925.33 MCKEOWN, N., ANDERSON, T., BALAKRISHNAN, H., PARULKAR, G., PETERSON, L., REXFORD, J., SHENKER, S., ,AND TURNER, J. OpenFlow Enabling innovation in campus networks. ACM SIGCOMM Computer Communication Review38, 2 April 2008.34 NURMI, D., WOLSKI, R., GRZEGORCZYK, C., OBERTELLI, G., SOMAN, S., YOUSEFF, L., AND ZAGORODNOV, D.Eucalyptus A Technical Report on an Elastic Utility Computing Archietcture Linking Your Programs to Useful Systems .Tech. Rep. 200810, University of California, Santa Barbara, October 2008.35 PARKHILL, D. The Challenge of the Computer Utility. AddisonWesley Educational Publishers Inc., US, 1966.36 PAXSON, V. private communication, December 2008.37 RANGAN, K. The Cloud Wars 100 billion at stake. Tech. rep., Merrill Lynch, May 2008.38 SIEGELE, L. Let It Rise A Special Report on Corporate IT. The Economist October 2008.39 STERN, A. Update From Amazon Regarding Fridays S3 Downtime. CenterNetworks February 2008. Available fromhttpwww.centernetworks.comamazons3downtimeupdate.40 STUER, G., VANMECHELEN, K., AND BROECKHOVE, J. A commodity market algorithm for pricing substitutable Gridresources. Future Generation Computer Systems 23, 5 2007, 688701.41 THE AMAZON S3 TEAM. Amazon S3 Availability Event July 20, 2008 online. July 2008. Available from httpstatus.aws.amazon.coms320080720.html.42 VOGELS, W. A Head in the CloudsThe Power of Infrastructure as a Service. In First workshop on Cloud Computing andin Applications CCA 08 October 2008.43 WILSON, S. AppEngine Outage. CIO Weblog June 2008. Available from httpwww.cioweblog.com50226711appengineoutage.php.23
