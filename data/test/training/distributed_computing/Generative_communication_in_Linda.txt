Generative Communication in Linda DAVID GELERNTER Yale University Generative communication is the basis of a new distributed programming langauge that is intended for systems programming in distributed settings generally and on integrated network computers in particular. It differs from previous interprocess communication models in specifying that messages be added in tuplestructured form to the computation environment, where they exist as named, independent entities until some process chooses to receive them. Generative communication results in a number of distinguishing properties in the new language, Linda, that is built around it. Linda is fully distributed in space and distributed in time it allows distributed sharing, continuation passing, and structured naming. We discuss these properties and their implications, then give a series of examples. Linda presents novel implementation problems that we discuss in Part II. We are particularly concerned with implementation of the dynamic global name space that the generative communication model requires. Categories and Subject Descriptors C.2.1 ComputerCommunication Networks Network Ar chitecture and Designnetwork communications C.2.4 ComputerCommunication Networks Distributed Systemsnetwork operating systems D.3.3 Programming Languages Language Constructsconcurrentprogramming structures D.4.4 Operating Systems Communication Man agementmessage sending. General Terms Languages Additional Key Words and Phrases Distributed programming languages 1. INTRODUCTION In his introduction to the distributed programming language SR, Andrews writes that there are only three basic kinds of mechanisms and three corresponding models of concurrent programming monitors shared variables, message passing, and remote op erations l, p. 4051. In the following we introduce generative communication, which, we argue, is sufficiently different from all three to constitute a fourth model. Our new technique is closest to message passing, but the differences between the two are as significant as the similarities. How should communication be incorporated in a programming language is our central question. The question has grown in interest as hardware has become cheaper and networks connecting large numbers of inexpensive processors cor This material is based upon work supported by the NSF under grant MCS8303905. Authors address Dept. of Computer Science, Yale University, New Haven, CT 06520. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission d the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee andor specific permission. 0 1985 ACM 016409258501000080. 00.75 ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985, Pages 80112. Generative Communication in Linda l 81 respondingly more important. Such networks fall into two main classes local area nets connecting autonomous personal workstations in network computers, still purely experimental, large numbers of processors work simultaneously on a single problem. Both sorts of network call for the construction of programs that run on many processors at once. On local area nets, these multimachine programs are mainly network operatingsystem and utility routines such as file and mail servers. On network computers, they include the operating system and any applications that are amenable to parallel solutions. Applications that have been studied in this context include numerical algorithms, simulation and sorting 5 parallel artificial intelligence programs are of rapidly growing interest. See, for example, Fehling and Erman 8. Network operating systems and parallel applications may be written in con ventional languages augmented with systems calls for interprocess communica tion. There is broad agreement, however, that distributed programming in a distributed languagein a language, that is, to which communication is intrin sicis in principle more satisfactory. What should a distributed language look like In particular, how should it deal with interprocess communication The concurrent systems languages that were based on the work of Dijkstra, Hoare, and BrinchHansen for example BrinchHansens Concurrent Pascal 2 and Wirths Modula 27 have generally been taken as starting points. Programs in both concurrent and distributed languages may consist of many concurrent processes, but the two language categories differ in the types of interprocess communication they may incorporate. The multiple processes of a concurrent language program are multiplexed on one machine, not distributed over many accordingly, they need not execute in whollydisjoint address spaces, and they may use shared variables or a generalization of conventional parameter passing such as the monitor call to communicate. The usual assumption in distributed languages, on the other hand, is that concurrent processes sometimes execute on separate machines and always in disjoint address spaces. They may therefore communicate with each other only by sending and receiving messages that are carried between address spaces by a runtime communication system. Generative communication is the basis of a new distributed programming language called Linda, developed originally for the SBN network computer ll. But many distributed or distributable programming languages have already been proposed CSP 17, DP 3, Plits 9, ECLU 23, Starmod 4, Ada 6, and SR l are examples we discuss below. What need for another one We argue first that, irrespective of the strengths and weaknesses of the many other proposals, Lindas unusual features make the language suggestive and interesting in its own right. Where most distributed languages are partially distributed in space and nondistributed in time, Linda is fully distributed in space and distributed in time as well, as we discuss. Second, Linda appears in many cases to be both simpler and more expressive than existing proposals within the application domains of interest. Linda attempts the same combination of simplicity and power in the distributed domain that the systems language C 20 achieves in the sequential one. The Linda design encompasses a model of computation with implications beyond distribution and communication as such. In this paper we are concerned, however, with communication exclusively. We introduce other aspects of the ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 82 l D. Gelernter Linda design only when communication issues make this necessary, and then only in brief outline. Part I deals with generative communication in Linda. Part II briefly discusses the implementation of generative communication on networks, particularly on network computers. A final point We attempt in the following to place Linda in context by comparing it to other distributed languages, as noted. But there are a very large number of designs that are relevant to this work in one way or another, and not all can be discussed here. We have chosen to concentrate on those other proposals that are closest in conception to ours, because the differences in these cases are least obvious. Other reports discuss Linda in relationship to Shapiros Concurrent Prolog 14 and to several AI languages 13. We also omit any discussion of formal semantics here some semantics issues are discussed elsewhere  131. 2. PART I GENERATIVE COMMUNICATION A Linda program is a collection of ordered tuples. Some tuples incorporate executing or executable code others are collections of passive data values. We are interested only in passive tuples here. The abstract computation environment called tuple space is the basis of Lindas model of communication. An executing Linda program is regarded as occupying an environment called tuple space or TS. However many concurrent processes make up a distributed program, all are encompassed within one TS. Consider two communicating processes A and B. To send data to B, A generates tuples and adds them to TS B withdraws them. Figures 1 and 2. This communication model is said to be generative because, until it is explicitly withdrawn, the tuple generated by A has an independent existence in TS. A tuple in TS is equally accessible to all processes within TS, but is bound to none. The remainder of this paper concerns the three operations defined over TS out  adds a tuple to TS, in  withdraws one, and read  reads one without withdrawing it. The following sections first define out , in , and read in their simplest forms, then generalize them by introducing structured names. Next, we discuss the basic properties inherent in our definitions that distinguish Linda and set it apart from other distributed programming languages. Finally, we present a series of examples. 2.1 Simple out, in , and read0 2.1.1 Preliminary The Type name. As a typespecifier, name refers to a character string acting as an identifier. Namevalued variables and constants are attached as identification tags to tuples, as we discuss. 2.1.2 Definitions out , in , and read . The out  statement appears as outN, P,, . . . , P,, where Pp, . . . , Pj is a list of parameters each of which may be either an actual or a formal, and N is an actual parameter of type name. Assume for the time being that all of the Pi are actual parameters. Execution of the out  statement results in insertion of the tuple N, Pp, . . . , Pj into TS the executing process continues immediately. ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. Generative Communication in Linda 83 TS A kEi tuple ccl Fig. 1. To send to B, A generates a tuple. . . Fig. 2. . . . and B withdraws it. The in  statement appears as inN, P2, . . . , P,, where Pp, . . . , Pj is a list of parameters each of which may be either an actual or a formal, and N is an actual parameter of type name. Assume for the time being that all of the Pi are formal parameters. When the in  statement is executed, if some typeconsonant tuple whose first component is N exists in TS, the tuple is withdrawn from TS, the values of its actuals are assinged to the in  statements formals, and the expression executing the in statement continues. If no matching tuple is available in TS, in  suspends until one is available and then proceeds as above. The read  statement, readN, Pz, . . . , P,. is identical to the in statement except that, when a matching tuple is found, assignment of actuals to formals is made as before but the tuple remains in TS. Nondeterminism is inherent in these definitions. Many in, . . . statements may execute simultaneously and suspend in several portions of a distributed program. Execution of out, . . . un d er such circumstances makes a single copy of the output tuple available. Some suspended in  statement will receive it, but which is logically nondetermined. The same holds for an iniV, . . . or readlr, . . .  statement that executes subsequent to many out s some stored tuple will be matched, but which one is nondetermined. The definitions require that tuples be inserted into and withdrawn from TS atomically. If two in  statements contend for one tuple, one gets it and the other ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 84 l D. Gelernter does not they cannot split it. Note that if a read0 statement and an in statement contend for one tuple, the two statements are served, as always, in arbitrary order. If the read statement is served first, in may subsequently withdraw the same tuple. If in  is served first, it removes the tuple and read  will block until another is available. Tuple names are global to a given programs TS. A tuple added to TS may be removed by an in  statement occurring anywhere else in the program. 2.2 Structured Naming Let t refer to any tuple added to TS by an outt statement. Then t designates the parameter list of any in  or read  statement that may legally receive tuple t. All of the hst F,, . . . . Fk cited by an in  or read statement need not be formals. Any or all components on the list may in fact be actuals. The actual parameters appearing in a t tuple including the initial namevalued actual collectively constitute a structured name. Thus the statement inP, iinteger, jboolean requests a tuple with name P. But it is also possible to write inP, 2, jboolean in this case a tuple with structured name P, 2 is requested. The statements inP, iinteger, FALSE and inP, 2, FALSE are also possible. In the first case, the structured name is P,,FALSE and in the latter P,2,FALSE. All actual components of a t list must be matched identi cally by the corresponding components of a t tuple in order for matching to occur. Structured naming is similar in principle to the select operation in relational database systems, and may be said to make TS contentaddressable. It also resembles in a rudimentary but significant way the patternmatching features that are part of some AI and logic languages, particularly Prolog 26. The in  statement and the Prolog assertion both deal with tuples whose unspecified components the formals may be inferred from some arbitrary specified subset the values. Just as components of t may be actuals, any component of a t tuple, except for the initial namevalued actual, may be a formal. Thus outP, 2, FALSE represents the simplest form of out  statement, but outP, iinteger, FALSE is also possible. The tuple P,iinteger,FALSE may be received by any ino statement that specifies first component P, last component FALSE, and middle component some actual of type integer. A tuple may in general be received ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. Generative Communication in Linda 85 by any in  statement with an identical actual where the tuple has an actual, and a typeconsonant actual where the tuple has a formal. Formals never match formals. The use of formals in out  statements is referred to as inverse structured naming. Note. A formal declared in an in statements parameter list is defined throughout the immediately enclosing lexical block, exactly as if it were declared as a local to that block. Variables declared conventionally rather than in parameter lists may be used as formals by in statements wherever they are lexically visible. In this context they must be prefixed by varotherwise they will be interpreted as actuals and part of a structured name. Thus, in the sequence iinteger .  . inP, var i, the integer variable i serves as an instatement formal. In the sequence iinteger .  . inP, i, is value is one component of the structured name P, i. Finally, note that all formals declared within in statements must include explicit type designators. The statement inP, i, jinteger requests a tuple with structured name P, i it does not declare two formals, i and j, of type integer. A formal declared in a t tuple is defined only within that t tuple. In the present context, formals in t tuples serve only as place holders, and their names are unimportant. 2.3 Distinguishing Properties Generative communication as defined by in , read , and out  has two funda mental characteristics that give rise to a series of distinguishing properties. The first fundamental characteristic is communication orthogonality. It is ordinarily the case that execution of an arbitrary distributed programming languages receive statement r presupposes no knowledge about which process is to execute the matching sendstatements it is in fact usually the case, on analogy with procedures or subroutines in sequential languages, that r may receive input from any number of sending processes as a procedure may be invoked from any number of points within a sequential program. Most send statements, however, name the receiver explicitly in some fashionon analogy with a procedure invocation statement, which invariably names the invoked procedure. In Linda, on the other hand, communication is orthogonal in the sense that, just as the receiver has no prior knowledge about the sender, the sender has none about the receiver. Communication orthogonality has two important consequences space uncoupling also referred to as distributed naming and timeuncoupling. A third property, distributed sharing, is a consequence of the first two. pl. Space uncoupling distributed naming. Distributed naming refers to the fact that a tuple in TS tagged P may be input by any number of addressspace disjoint processes. In particular, j processes executing on j distinct network nodes may all accept tuples tagged with one name. Distributed naming means that Linda is fully distributed in space. Distributed programming languages ordinarily as noted allow a receive statement to accept data originating in any one of ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 86 l D. Gelernter a b Fig. 3. Distributed naming. a Tuple t input by R may have originated in any one of many address spaces b tuple t output by S may be received in any one of many address spaces. many distinct address spaces Figure 3a. Linda allows a sender to direct data to any one of many distinct address spaces as well Figure 3b. p2. Time uncoupling. A tuple added to TS by out  remains in TS until it is removed by in . If it is never removed by in  it will, in the abstract, remain in TS forever. In practice, tuples added to TS by a given distributed program will be removed once all that programs processes have terminated, unless the pro grammer indicates explicitly to the contrary. Despite these practical considera tions, Linda allows programs to be distributed in time insofar as process A in which some out  statement appears may run to completion before process B, in which the corresponding in  appears, is loaded. Thus, while distributed program ming languages ordinarily allow communication between processes that are space disjoint Figure 4a, Linda allows communication between timedisjoint processes as well Figure 4b. A third effect results from space and timedistribution p3. Distributed sharing. Linda allows j addressspacedisjoint processes to share some variable u by depositing it in TS. The TSoperator definitions ensure that u will be maintained atomically. It is not necessary as in other languages that a shared variable be implemented by a process or module. We refer to the second of generative communications two fundamental prop erties as free naming. For expository purposes we introduced type name first ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. Generative Communication in Linda l 87 space 4 time 1 6 Fig. 4. Distribution in time. a Spacedisjoint processes A and B communicate via TS b time disjoint processes C and DC terminates before D startslikewise communicate via TS. and the idea of structured naming subsequently. In fact, the existence of type name is a logical consequence of structured naming. Other distributed program ming languages distinguish between the name specified by an in  statement and its list of parameters. In Linda, this distinction is eliminated the name specified by an in statement is by definition the sequence of actual parameters that appear in the in  statements parameter list. Actuals, however, are simply values that appear in a parameter list. It follows that, if an in  statement specifies only a single actualby definition that single actual will appear at the beginning of the listthen that single actual must itself be a value. Values have types, and we say that the type of a value serving as an in statements single actual is name. These facts about Linda are collectively designated free naming, and they result in two further properties. p4. Support for continuation passing. If there are values of type name, there must be variables of type name as well. Hence, names may be passed as tuple ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 88 l D. Gelernter components, be assigned to formals, and be stored in arbitrary data structures just as values of any other type may. We use continuation passing to refer specifically to one processs sending data to another, blocking in expectation of a reply, and being continued ultimately by a third or some nth process. Linda supports continuation passing in a simple and flexible way by allowing name type values to be tuple components and to be stored in arbitrary data structures in the same way as values of any other type. 5. Structured naming. Structured names allow in  and read  statements to restrict, and out  statements to widen, their focus. They also allow one name to generate a family of others. The examples illustrate each of these properties together with the basic program structures that Linda induces. It is incumbent upon the designers of a language that, like Linda, maximizes power and flexibility in a broad solution space over simplicity and convenience in a more narrowly constrained one to provide for macro and libraryroutine specification where simplicity and con venience are wanted. A macro definition facility is introduced and discussed in the course of the examples. Finally, the examples refer to the several other distributed or distributable programming language proposals mentioned above for purposes of comparison. The following sections present preliminary definitions, then a group of exam ples using simple in and out without structured names, a group using structured names as well, and a final group using in and out , structured names, and read . 2.3.1 Preparatory to the Examples An Outline of Expression Structure in Linda. In order to give examples, we need first to consider briefly the structure of Linda expressions. Linda expressions consist of simple statements and composed statements. The term expression is appropriate for a collection of statements because statements have values, but this does not concern us here. Simple statements include assignments, procedure calls, and control statements. These are again unimpor tant in the present context they are mainly identical to simple statements in C. Composed statements on the other hand are germane because they determine program structure. Linda has four composed statements sequencestatements, andstatements, orstatements, and starstatements. Leteachofsi,..., aj be either a simple statement or a composed statement enclosed in square brackets. A sequencestatement takes the form s1 sz . . .  s, and specifies that sl, . . . , aj are executed sequentially or, in Lindas terminology, have sequential lifetimes. The andstatement s, Ed s2  . . . Ed s, specifies that sl, . . . , Sj have concurrent lifetimes. For each pair of andstatement constituents, if both are executable statements, they execute concurrently if one is a statement and the other is a variable declaration, the declared variable exists for as long as the statement executes and is accessible to all constituents of the andstatement within which it is declared. Thus the andstatement provides both ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. Generative Communication in Linda 89 for variable declaration and for concurrency. The andstatement terminates when all of its constituents have either terminated or quiesced as defined below. Variables are always quiescent. Variables may be declared only within andstatements. The special notation s, . . . inP, iinteger . . . i S is simply an abbreivation for iinteger  s, . . . inP, var i . . . s,. These rules are unconventional, but since their justification and implications lie beyond the scope of this paper, the reader is asked to suspend judgment. The andstatement is the basis of Lindas socalled symmetric structure, and it allows for a richer collection of assignmentstatement forms and of bindingenvironment specifications than Algol languages ordinarily provide. These issues are discussed in  131, and at length in 15. The orstatement Sl I I0ISj S2 specifies that sl, . . . , sj have mutually exclusive lifetimes. A single enabled constituent of the orstatement is chosen for execution the choice is made nondeterministically at runtime. If no constituent is enabled, execution suspends until one is. For present purposes, an orstatement constituent is not enabled only if it consists of or begins with an in  statement and no matching tuple for that in  statement is available in TS. The starstatement s executes s 0 or more times. If s begins with an in statement this is the intended case, s executes once for each matching tuple available in TS when no matching tuples are available, the star statement quiesces. Ordinarily a quiescent star statement terminates immediately, but it is prevented from terminating if it is one andstatement component and other components of the same andstate ment are still active. A quiescent starstatement is reawakened by the appearance of a new matching tuple in TS it terminates when all other components of the encompassing andstatement have quiesced or terminated. Starstatements of the form int s, where s is any statement, are referred to as in blocks, and are so common that it is convenient to introduce the following shorthand notation for them int  s. 2.4 Examples Using Simple in and out0 2.4.1 BUSC Communication Structures. out  and in  may be used to implement messagepassing in the obvious way. Remote procedures are implemented by combining out  and in  remote procedure call out  P , me , outactuak in me, informals ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 90 l D. Gelernter remote procedure inP, whoname, informals  body of procedure P out who, outa I The distinguished name me is replaced by the compiler with any name that uniquely designates the process within which it appears, where process is a sequentiallyexecuting code block and a distributed program may encompass many processes. The structure referred to here as a remote procedure may be equivalently referred to as an active procedurea procedurelike block that is executed by a thread of control associated with itself, not with the caller. An active procedure is neither reentrant nor recursively callable. In Linda it is ordinarily one com ponent of an andstatement whose other components are the processes that make use of it. It disappears, as per the definitions of andstatements and star statements, when the encompassing andstatement terminates. Note that Linda resembles Plits, ECLU, SR, and the language fragment described by Kahn  191 in providing an asynchronous output operation. 2.4.2 Semaphores and Communication Orthogonality. A singleelement tuple is functionally equivalent to a semaphore. insem is functionally equivalent to the semaphore operation Psem, outsem to Vsem. A semaphore sem is initial ized to value n by n repetitions of outem. The semaphorelike nature of in  and out  is one result of communication orthogonality in Linda. The space and timeuncoupling properties mean that singleton sem tuples exist independently of the processes that generate them, and outlive those processes if necessary. Distributed naming means that the semaphores defined by in and out are in fact distributed semaphores as discussed, for example, by Schneider 24that is, semaphores defined over arbitrarilymany disjoint address spaces. It is an expression of distributed sharing that processes may share semaphores directly via TS no executable code block need be programmed to implement P  and V  requests. That semaphores are primitive is also indicative of Lindas simplicity and flexibility. Languages that abstract at a level higher than semaphores rely, at least conceptually, on high level constructs such as tasks or processes to implement these lowlevel objects. Linda differs from all other surveyed systems except CSP in making commu nication orthogonal. Linda and CSP are, however, orthogonal in opposite senses. In CSP, communicating processes must name each other explicitly data is transferred synchronously from sender to receiver with no systemprovided buffering. Thus communication in CSP is spacecoupled because of the naming rules and timecoupled because of synchronous transfer. 2.4.3 Distributed Naming. Consider a maximallysimple idlenode allocation procedure for a network computer. Idle nodes might execute inidle, jjobdescription execute job j . To fork a new asynchronous task k, processes execute outidle, k. ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. Generative Communication in Linda l 91 Tuples tagged idle may be input by any network node because of distributed naming. Note that if the out  statement executes when there is no idle node in the network, hence none executing its inidle, . . . . statement, the idle, k tuple remains buffered in TS until some node becomes idle and picks it up. This is a result of timeuncoupling. Another example In Shoch and Hupps Ethernet worm programs 25 users sometimes need to communicate with a worm segment. Any segment will do, but the worms whereabouts are unknown until runtime. In Linda, worm segments would execute inwormsegment, r request execute request r, and users with request k would execute outwormsegment, k. None of the other languages under discussion supports distributed naming. ECLU allows one name to be referenced by many processes, but only if all are confined within one Guardian and hence within one physical node. 2.4.4. Synchronized Message Exchange CallingRoutine Structures and Macro Definitions. CSP provides as noted synchronizing messageexchange operators. The receive operation suspends until a matching send is executed and the send suspends until a matching receive. To implement these operations it is not sufficient for the sender to transmit as soon as it reaches the send statement, then suspend execution until the receiver forwards an acknowledgment. No message buffering is to be provided the sender therefore cannot transmit until the receiver has suspended at its receive statement. Presenting an implementation of these operations in Linda is a good occasion for the introduction of a macro facility. Programs that use synchronous com munication specify or include the macro defintions def SYNCHSENDstuple inget outs ingot def SYNCHRECVstuple outget ins outgot. To communitate synchronously, process A executes SYNCHSEND  B , mes  sage andB executes SYNCHRECVB, m  messagetype   Note that this im plementation requires that the singleton tuple get be buffered in TS, but does not require that the message text be buffered. These macro definitions are satisfactory only if they are used by a single pair of processes within a given TSotherwise the names get and got will be ambiguous. More general definitions use structured names, as discussed below. In conventional languages, called routines procedures, functions, subroutines, and so on may be arbitrarily elaborate, but calling routines the protocol followed by one process in passing data to, and if necessary retrieving results from, another ordinarily consist of a single invocation statement. Linda allows the programming of calling routines that may in general be as elaborate as the called routine itself. SYNCHSEND  is one example of a calling routine others follow. 2.4.5 Resource Servers The ContinuationPassing Property the Filter and the ActiveMonitor Structures. Continuation passing and communication orthogo ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 92 l D. Gelernter nality allow resource servers to be programmed using the familiar feederdriver paradigm instead of the input guards that are common in distributed program ming languages. In the latter case, request queues are maintained in the kernel and are not directly accessible to the server. In the former, they are constructed and maintained entirely by the server. We include two examples whose solutions are included in the Preliminary Ada Rationale X3. The Ada solutions represent an inputguard programming style that is usefully compared to Lindas feeder driver style. Disk head scheduler. The Ada example assumes that client processes will communicate with the disk head scheduler by passing two parameters, one of type track and the other of type data. We make the same assumption. The logical structure of the Linda disk server is shown in Figure 5. The user passes its name to the filter, blocks under this name, and continues once its request has been filled by the server. Requests accepted by the filter are passed on to the server directly if the server is free otherwise they accumulate in the filter, and the filter passes them onward in whatever order is appropriate to the problem. The existence of the filter is transparent to the user. The filter structure may be programmed in Linda as a result of the continuationpassing property. Users call the disk by executing the calling routine DISKSERVICEtrack, data, where def DISKSERVICEt track ddata outdiskrequest, me, t, d inme. The server itself takes the form DISKFILTERDISK where DISK is def DISK ask the filter for an io request outgetdiskrequest receive the request indiskdriver, whoname, itrack, ddata perform the physical ijo transfer tell the user to continue outwho 1 DISK is an active procedure It repeatedly asks FILTER for an io request, performs it, and notifies the user that service is complete. DISKFILTER is an active monitor, a structure that in general takes the form shared variables rllr21 . . . jrn, where the ri are routines that access the shared variables in mutual exclusion. An active monitor resembles a Hoare monitor only the active monitors routines are executed by the thread of control associated with the monitor itself, not with the user, or an Ada accept block. ACM Transactions on Programming Languagesandsystems, Vol. 7,No. l,January1985. Generative Communication in Linda l 93 Fig. 5. A filterserver paradigm. DISKFILTER takes the form where def DECLARATIONS diskidleboolean  true  declarations required for building and maintaining the io requests queue queue frames will have three fields, of type name, track, and data def DISKENQUEUE indiskrequest, whoname, itrack, ddata accept disk io request if diskidle send the request on directly outdiskdriver, who, i, d diskidle  false else insertwho, i, d in the io request queue using any desired ordering discipline 1 def DISKDEQUEUE accept disks request for the next io request ingetdiskrequest whoname itrack ddata local variables  if request queue is empty disk will be answered eventually by DISKENQUEUE diskidle  true else who, i, d  head entry on request queue outdiskdriver, who, i, d II The disk head scheduler is considerably more complex in Ada. The user calls a procedure which calls an entry on its behalf the entry registers the users request and then the user suspends on another entry call, this time to one of a family of entries. Meanwhile, a background task repeatedly calls another entry to determine which request to serve next, and ultimately continues the user by accepting the entry call on which it is suspended. The elevator algorithm used to service io requests is intrinsic to the tasking and communication structure of the Ada example. It does not, because it need not, figure in the Linda example. The multiresource controller problem as stated in the Ada Rationale has users requesting a subset of resources from among a set designated A through K. Users request a resource group by passing to the server a parameter group of type resourceset, and return a group by passing a parameter of the same type. This example is simpler than the disk head scheduler. It requires an active monitor only, with no additional driver process. ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 94 l D. Gelernter The Linda solution takes the form The declarations include, as in the Ada example, a function try that returns TRUE if the resourceset passed is free and can be allocated to a requestor, and a variable used of type resourceset that records which resources are unavail able. Resource sets are bit vectors, and we assume Adas bitwise logical opera tions. The request queue is built of frames containing a field who of type name and a field asked of type resource set. For MRCENQUEUE and MRCDEQUEUE, we use def MRCENQUEUE inMRCRequest, whoname, askedresourceset if tryasked assign this group to the requestor outwho used  used or asked else append who, asked to the request queue 1 def MRCJEQUEUE inMRCRelease, releasedresourceset used  used and not released for each frame q in the request queue if tryq.asked award the requested group to the blocked requestor outq.who used  used or q.asked remove q from the request queue 1 . To request resource set s, users execute REQUESTRESOURCESs and to release s, RELEASERESOURCESs, where def REQUESTRESOURCESsresourceset outMRCREQUEST, me, s inme def RELEASERESOURCESsresourceset outMRCRELEASE, s. Note that the Linda example depends once again upon continuation passing. The Rationales Ada solution is, again, considerably more complex logically. The request queue consists of all nonaccepted calls to a single entry. To access the queue, the server accepts each waiting entry call and, if the request it represents cannot be satisfied, instructs the requestor to reenter the queue via another entry call. If the entire queue is to be inspected, the entire queue must be cycled into and then back out of the servers address space requestbyrequest. Note that maintaining the queue in some useful order or keeping pointers into the queue is impossible. Discussion. This Linda example strongly suggests CSP, and it is easy to imagine that a CSP solution would be nearly the same as the Linda version. This ACM Transactions on Programming Languages and Systems, Vol. 7, NO. 1, January 1985. Generative Communication in Linda 95 PROTECTION FILTER QUEUING FILTER Fig. 6. A protection filter in place. is not the case, because CSP lacks Lindas freenaming property and therefore cannot support continuations in a general way. The Linda solution depends on the requestor passing its name to MRCENQUEUE, which passes it via the queue to MRCDEQUEUE. In CSP, names cannot be passed as parameters the only way the requestor can identify itself to MRCENQUEUE is by passing either an index into a process array or some other parameter that allows the server to pick it out of a staticallyencoded list of possibilities. Note however that extensions to CSP that introduce ports and allow port names to be passed as parameterssee Kieburtz and Silberschatz 21, for exampledo allow this kind of solution. Of the systems surveyed here, CSP, DP, Ada, and Starmod do not allow general continuation passing. ECLU evidently does not. SR and Plits do although we argue elsewhere la that their formalisms are considerably more complex than Lindas. We conclude this section by considering briefly the question of access to tuple names. A single distributed program encompasses many processes, and they all share a single name space and a single TS. But when two different distributed programssay, a user program and the operating systemrun on the same network, each defines its own name space, and constituent processes of the first program ordinarily have no access to tuples generated by the second. By prefixing the internal representation of tuple names with program ids, the kernel protects each programs set of tuples from reference, augmentation, or deletion by any other program. There are clearly times, however, when a user program must have access to names in the systems or in some other users name space, and so it must be possible to import external names or to receive them as parameters. Programs may, however, specify, and the kernel must enforce, limitations on the uses to which exported names may be put. For example, the system might export the name diskrequest for use in out  statements only the kernel then traps any indiskrequest, . . .  or readdiskrequest, . . .  statements that occur outside the system. The system is free to protect itself further by installing protection filters in front of servers. The name disk request might for example be exported, but the statement in  di Skr e  quest, . . .  might appear in a protection filter that checks the identity of the requestor against an access list local to the protection filter. If the requestor has access rights, the public name diskrequest is stripped off, and the request tuple is forwarded under a private nonexported name to the queueing filter. Otherwise, an exception is generated or bad status is returned Figure 6. 2.5 Examples with Structured Names 2.5.1 Conversational Continuity Structured Naming. Conversational continu ity as described by Liskov 23 refers to the following systemsprogramming problem 1 There are many identical instances of some server. 2 A user may start a conversation with any instance, but having chosen one, should converse ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 19. 96 l D. Gelernter with that one exclusively for the duration of the conversation. 3 The multiplicity of servers, the process of selecting one, and the obligation to stick with it should be transparent to users. A Linda solution servers execute initially inserver, Uname, start from any user, accept a start request for the duration of the conversation inserver, U, rrequest from user U, accept any request except for start users execute initially outserver, me, start for the duration of the conversation outserver, me, request. 2.5.2 Node Allocation Inverse Structured Naming. As a refinement to the simple idlenode allocation scheme discussed above, taskgenerators may need to direct tasks to specific nodes in some cases and to any idle node in others. Network node i executes inidle, i, jjobdescriptionj execute job j . To direct task K specifically to node i outidle, i, k to direct task k to any idle node outidle, ninteger, k. 2.5.3 Reminder Messages Time Uncoupling and Inverse Structured Naming. Consider the following routine def REMINDER now  current date and time intick  now increment the current time inmsg, now, sstring  print string s 1 When REMINDER is startedwe assume that it is one component of an and statement, some of whose other components need to use itnow is initialized to the current date and time. We assume that both are encoded as a single integer, but only for maximum simplicity of the presentation. Date and time could as well have been represented as separate fields and updated appropriately. REMINDER now waits for ticks. Whenever the singleton tuple tick appears in TS, it is withdrawn and the value of now is incremented. After each incrementation, REMINDER looks for every tuple in TS whose first component is msg and whose second component is the current date and time. Each such tuple is withdrawn, and its third component is printed. ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. Generative Communication in Linda l 97 The statement outtick is executed in response to or, is the logical equivalent of a hardware clockinterrupt. REMINDER is a representative interrupthan dler. Correct realtime response depends, of course, upon scheduling policies implemented in the kernel. It will be useful to allow hardwaredetermined priorities to be associated with in  statements as, e.g., they are associated with device modules in the Concurrent language Modula 27, but we will not discuss this here. REMINDER is the basis of a remindermessage system that works as follows. If I want to be reminded of s on dateandtime t, I execute outmsg, t, s. Thus, for example, outmsg, atJune 5, 250PM, Faculty meeting in ten minutes. Beat it. at  is a function that translates a dateandtime specifying string into an equivalent internal representation as a single integer. at  needs to be able to read the current time in order to do this we discuss how to read the current time in Section 2.6.2. On June 5 at 250 PM, REMINDER will fish this tuple out of TS and print the specified message. Note that TS acts in this case as a content addressable file system. We discuss some implications in Section 2.7. Suppose now that dateandtime is represented internally not as a single integer but as a series of fieldsyear, month, date, hour, minute, set, ms. In this case, outmsg, atJune 5, 250PM, Faculty meeting in ten minutes. Beat it.  is equivalent to outmsg, 84, 6, 5, 2, 50, 0, 0, Faculty meeting in ten minutes. Beat it. Note that we allow a function to return a tuple. REMINDER will withdraw and print this message only if the user is logged on at 250 on June 5. Suppose instead that the message should appear some time on June 5, rather than at 250 precisely. In this case we can write outmsg, anytimeonJune 5, Faculty meeting at 300 today. and anytimeon  can produce the following translation outmsg, 83, 6, 5, iint, jint, kint, lint, Faculty meeting at 300 today. ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 98 l D. Gelernter Plits includes a specialized form of restrictedinput statement. SR includes a restrictedinput statement that is more general than Lindas in statement actuals in allowing arbitrary boolean expressions to be enforced over input parameters, but less general in that, since SR does not allow distributed naming, the restrictedinput statement cannot be used to steer parameters to certain nodes and away from others. None of the other languages surveyed here allows the equivalent of inverse structured naming. A larger example of the use of timeuncoupling, as well as of continuation passing and structured names, is given in the Appendix. 2.6 Examples Using read0 2.6.1 Global Variables Distributed Sharing. A checkersplaying program, imple mented on the Arachne network computer, contains multiple processes that search the tree of future game states in parallel. Given parallel processes i, j, and k, processes j and k are to be stopped if and when they are known to be exploring the consequences of a lessgood move than i has already found. Process i must notify j and k when this is so. If Arachne allowed shared variables between processes, the value of the best move found so far could be put in a global location. However, lacking global memory, we arrived at a different solution. lo Linda allows the simple solution to be programmed directly. To read the value of bestmove readbestmove, mmove. To change the value of bestmove inbestmove, oldmovemove outbestmove, newmove. The code actually required by a distributed gameplaying program would be considerably more complex than this. 2.6.2 Delay Statement Structured Names and Distributed Sharing. A global clock might be stored in TS in the form of a clock, timevalue tuple. Assume that time values are integers and that each tick increments the clock by one. To read the clock, processes execute readclock, nowinteger. To write the clock, the clock process executes inclock, nowinteger outclock, nowl. Processes might find a delay statement useful, where DELAYd delays the executing process for at least d time units. A conventional implementation of a delay service might send delay requests to a central server that stores them on a wakeuptimeordered queue. This solution is simple enough to implement, but there are simpler solutions in which the request queue is distributed throughout ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. Generative Communication in Linda l 99 TS. DELAY is the following calling routine def DELAYdintegerdelay for at least d ticks  readclock, nowinteger outdelayuntil, nowd, me inme At each clock tick, the wakeup server increments the clock and awakens all processes delayed until now intick  update the clock inclock, nowinteger outclock, nowl awaken processes delayed until time  now11 indelayuntil, nowl, whoname  outwho Under some circumstances, the following is even simpler def DELAYdinteger 1 readclock, nowinteger readclock, nowd I The first read statement consults the clock and assigns the current time to now. The second specifies a structured name that will match the clock tuple and cause the read  statement to continue when the time reaches now d. At every tick the wakeup server simply increments the clock. This solution will work correctly, though, only if the clock period is long relative to the amount of time it takes to execute an in  followed by an out . Otherwise the clock process may remove tuple  c 1 oc k , t from TS before all read  c 1 oc k , t statements have had a chance to see itin which case those read statements that have not seen it are stuck forever. 2.7 Time Uncoupling, Tuple Persistence, and File Systems Spacedistributed computing is an innovation, and its investigation has barely begun. Timedistributed programming languages in Lindas sense are likewise unfamiliar, but timedistributed computing itself is nothing new. Communication between timedisjoint programs has long been provided for by file systems. There is also a linguistic model for timedistributed communication in any routine that retains state between timedisjoint invocationscoroutines, generators, proce dures with Algol own variables, monitor routines with access to permanent monitor variables, and so on. In Linda, extension of a linguistic model into fully spacedistributed computing results in its extension into timedistributed computing as well the linguistic aegis now covers certain filing functions that are traditionally extralinguistic. Generative communication logically encompasses a tuplestructured, content addressable file system. But the extent to which it is desirable or practical to incorporate filing functions in Linda is not yet clear. At present, we assume that when all processes of a distributed program have completed, whatever tuples generated by that program remain in TS will be deleted, with two exceptions ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 100 l D. Gelernter tuples whose first names are in the systems name space and tuples whose first names appear on a save list specified by the programmer. 2.8 Summary and Conclusions We noted at the start that generative communication results in a number of properties that distinguish Linda from other languages. We summarize and reiterate here. Distributed naming. None of the other surveyed languages supports distributed naming. If multiple, identical, spacedisjoint servers exist in a network, client processes must refer to each under a different name, or send requests to a concentrator process that sends them onward to the distributed servers. Note that the latter scheme imposes centralization in a distributed environment on linguistic grounds. Time uncoupling. None of the other surveyed languages supports timeuncou pling in Lindas sense. Communication between timedisjoint processes is sup ported in other systems by the file system, not by interprocess communication mechanisms. Distributed sharing. None of the other surveyed systems supports distributed sharing. Variables shared between disjoint processes are implemented by proc esses or modules. Continuation passing. Linda, Plits, and SR all support continuation passing. Continuation passing style is to be contrasted in the systems domain to input guard style. Input guard languages admit to the servers address space only those requests that may be served immediately. Blocked requests, and consequently blocked processes and blockedprocess queues, are the domain of the kernel and are inaccessible to the programmer. Note that SR resembles the noncontinuation languages in providing input guards, and relying on them in presented examples, for resourceserver implementation. Plits strategy for programming request queue problems is not clear. Structured names. None of the other surveyed systems supports structured naming in Lindas sense although Plits and SR both include restrictedinput statements. Lindas weaknesses stem from its strengths programmers are required to implement structures such as calling routines and queuemanaging filters pro vided for them in other languages. Properties such as distributed naming and sharing make Linda inherently less secure than other languages care must be taken to prevent unauthorized access to stored tuples. Finally, Linda presents difficult and novel implementation problems, particularly in a distributed envi ronment. One central problem dominates the implementation question How is TS to be implemented in the absence of shared memory We address this question in Part II. Several important topics have been omitted in this survey, and, in conclusion, we mention some of them briefly. Modularization is important in the construction of large distributed programs, and Linda supports it the necessary mechanisms are discussed in 15. Typechecking is also particularly important in building large systems. The definitions above do not require that a given name be used ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. Generative Communication in Linda  101 with only one type of tuple the two statements out  P , FALSE  and out  P , 10  may legitimately appear in the same program. This flexibility is sometimes useful, but it makes runtime typechecking impossible. A process that executes inQ, i  integer  when it meant to say in Q, i  char is in danger of hanging forever. It would, however, be easy enough to allow user programs that so desire to run in a typechecked mode, in which the kernel signals an exception whenever a tuple and a blocked in  statement specify the same first name but do not match because of type incompability. Dynamic process creation is the one capability of the generative communication operators themselves that we have not discussed. Briefly, any sideeffectfree expression may appear in a generated tuple, and tuples are added to TS in unevaluated form. As soon as a tuple enters TS, evaluation of all its unevaluated components begins simultaneously. Thus the statement out  P , f  x   generates a new process its name is P and it executes f  x . If fx returns an integer, the result tuple may be removed from TS by the statement in  P , iinteger. IfinOexecuteswhile fx is still under evaluation, in blocks until evaluation is complete. Dynamic process creation is a powerful tool that entails difficult implementation problems, discussed in  131. 3. PART II A VIRTUAL LINDA MACHINE ON A NETWORK COMPUTER We are concerned here with aspects of the implementation of Linda on networks, particularly on network computers. Our only initial assumption about the target machine is that all of its nodes are essentially equal in power. Study of the Linda implementation problem leads in topdown fashion, however, to a class of interconnection topologies and communication architectures that are well suited to a Lindasupporting network computer. The techniques described below were designed for and partially implemented on the SBN network computer. We do not, however, discuss this implementation, but the principles behind it. The following is a general outline a full treatment appears elsewhere  161. 3.1 A Virtual Linda Machine We propose to think of out , in , and read  as the primitive instructions of a virtual Linda machine VLM that is implemented by the hardware, the network communication kernel, and the compiler in cooperation. The communication kernel is the program resident on each network node that implements internode communication. Implementing relatively highlevel computation operators as virtualmachine primitives is an established technique, but implementing com munication operators in analogous fashion is not, and requires some introduction. Communication operators are usually regarded as tools provided by the system. But we intend to write operating systems in Linda. An implementation of Lindas communication operators must therefore underlie the systemmust be provided, that is, by the hardware and the communication kernel. In constructing a distributed VLM, the central problem is, as noted, how to implement TS in the absence of shared memory. This question resolves into two subproblems how to find tuples the major issue and where to keep them. ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 102 l D. Gelemter 3.2 How to Find Tuples The Dynamic Global Name Space A name space that is global to all modules of a distributed Linda program makes it impossible to resolve globalname references statically before runtime. Suppose a distributed program is running on j network nodes and outt is executed on one of them at time q. Recall that ift is an in  or read  statements parameter list, t is any tuple that can be legally received by that in  or read  statement. At time q, processes are suspended at in statements on some possibly empty subset S of the j nodes occupied by the program, but it is impossible to determine the identity of S at compile or loadtime. Some method must therefore be provided for matching t tuples to int statements at runtime, or, in other words, for implementing a dynamic global name space. Providing such a name space has an important additional benefit as well it makes it unnecessary, so far as name resolution goes, to assign modules of a distributed program statically to network nodes. Allowing the network computers operating system maximal freedom to locate and relocate distributed jobs within the network is in fact an important goal in itself. Static mapping of modules to nodes requires that distributed programs be relinked each time they are run and rules out the possibility of new modules being generated dynamically by runn.ing programs, making it impossible for the system to reposition modules in the network in response to hardware failure, changing resource requirements, or a changing network environment. Enforcement of a static mapping policy thus requires ipso facto the surrender of a significant portion of a network computers potential power and flexibilitya loss roughly comparable to what might be engendered by a conventional systems requiring object modules to reside at fixed, absolute addresses. Our VLMs first requirement, then, is to provide a dynamic global name space. Runtime resolution of name references may be handled in either of two general ways. Namenode mapping information may be concentrated in central directory nodes, or it may be distributed networkwide, for example by means of a network wide broadcast. The maintenance of a central directory might lead to congestion and vulnera bility it is furthermore inappropriate as a kernel function. Ideally, the kernel is a small, simple program basically identical on every node. Distinguishing certain kernels as directory servers, and requiring all others to submit service requests to these, involves the kernel in an undesirable degree of complexity and special ization. If the kernel is to implement the global name spaceand it must, because Linda is to be supported directly by the kernelthen distributed storage of namenode mapping information is an attractive approach. The combined weight of these design decisions is to make the mapping of logical to physical addresses a function of the virtual machine itself. Concep tually, program modules and the names they define may be mapped to nodes and moved between nodes as freely as pages may be moved among page frames in a paged memory system. To support this flexibility, the paged machine translates virtual to physical addresses in the hardware on a perreference basis. The analogous function in the VLM is the translation by the communicltion kernel of tuple names to network addresses on a perreference basis. Against the ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. Generative Communication in Linda l 103 potential benefits of this flexible architecture must be balanced the cost and complexity of implementing internode communication and runtime resolution of name references. The power of the VLM will be realized only if the communica tion kernel can be implemented efficiently. We address specifically the issue of efficient name resolution in the remainder of this section. We define elsewhere 16 a method of network state storage called uniformly distributed, where network state is the collection of all data descriptive of any particular node that may be of interest to the network generally. The fact that a tuple with name M is currently required by an in statement on node i is an instance of a state datum. Informally, a network state storage scheme is uniformly distributed if no node stores any more of the current state than any other node. Uniformly distributed state storage schemes fall into a discrete spectrum. At one end of the spectrum, state data is broadcast to every node, and each node maintains its own complete copy of the state of the entire system. At the other end, state information remains local to the node on which it originates. In this case, to discover the current state, a given node must broadcast a query to the entire network nodes respond to this broadcast query with information extracted from their local states. If state data is read much more frequently than it is written, then clearly the first scheme is more efficient in terms of total communication and processor bandwidth expended by all reads and writes. In this scheme, writing is expensive a broadcast is required, but reading is cheap. If writes are much more frequent than reads, then the second scheme will be more efficient. If, on the other hand, reads and writes occur with roughly equal frequency, then a third scheme, intermediate between the first two, is more efficient than either. Roughly equal has a precise definition On an Nnode net, roughly equal means equal within a factor of N2  1, as discussed in the general treatment 16. In the intermediate scheme, assuming an Nnode network, writing the network state consists of broadcasting state data to a prearranged set of N1j2 nodes, called the write set. Reading the state consists of broadcasting a query to a different set of N12 nodes, called the read set. If the read set is chosen correctly, its nodes will store among themselves the entire network state. This condition is assured if each nodes read set has a nonnull intersection with each other nodes write set. Consider now the problem of implementing in  and out . By definition, the operation writestatet,j will add to the network state the datum t is available on node jin other words, that int has been executed on node j and that some instance of t is accordingly desired. The operation readstate will consult the network state and return the identity of some node on which int has been executed and on which an instance of t is accordingly needed. If there is no such node, the readstate request will remain outstanding until there is such a node and will then return that nodes identity. Note that readstate and writestate are internal to the communication kernel. They are not part of Linda. Abstractly, then, node is kernel may implement an outt operation as follows j  readstate send t to node j. ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 104 l D. Gelernter int as executed on node j is implemented writestatet, j await receipt of t Note that at this level the implementation of in  and out  is not atomic. As a result, two instances of t may be sent to the same node j, in which case the second instance must be reinserted into the global buffer. Let  tstate refer to all data in write sets pertaining to the whereabouts of ttuples i.e., of int statements in the network. If in  and out  are executed with roughly equal frequencythis seems to be a reasonable assumption, al though there are complicating factors, which we discuss belowthen the net works tstate will be read and written with roughly equal frequency. Given that the tstate is to be stored in uniformlydistributed fashion, writestatet, i and readstate are accordingly best implemented using the intermediate Nj2node broadcast technique. Our basic tools for implementing in  and out  are readstate and write state . readstate  and writestate  may be implemented using any state storage scheme on any type of network, large or small. The Nj2node broadcast technique in particular suggests, however, one class of network topologies. Networkcom puter designers are generally free to specify an interconnection topology design ers of larger networks are not. The remainder of the discussion therefore applies specifically to network computers. We describe the class of topology we have in mind as the WD wraparound hypercubes, where wraparound refers to each dimension of the networks being connected either as a ring or by a broadcast bus, so that all nodes interface to the network in the same way there are no edge nodes. Linda was developed, as noted, for the SBN network computer, which used a twodimensional represent ative of this class. SBN was a linked torusa square endaround grid in which each row and each column loops back on itself each node has four nearest neighbors. In the following, we describe the Linda implementation designed for SBN. It is applicable to any square grid or, with appropriate simple generaliza tions, to any evendimension WD wraparound hypercube. For each node i, is write set is its row and is read set is its column. When inP. . . executes on node j, js kernel executes writestateP, j by sending each of the N112 nodes in its row a message reading a tuple labelled P is needed on node j. This process is referred to as constructing a Pinthread. Note that we deal at this point only with simple names. Structured names are discussed below. When outP. . . executes on node i, is kernel sends to each of the N12 nodes in its column a message reading a tuple labelled P is available on node 2 this process is called constructing a Poutthread. When a Pinthread either inter cepts or is intercepted by a Poutthread on node k, a notification message is generated on node k and sent to node i instructing i to send tuple P, . . . to node j. When and if the tuple is delivered, accepted, and acknowledged, both Pin thread and Poutthread are removed. Some other P, . . . tuple may, on the other hand, arrive at node j before node is does. If so, node is tuple is not accepted and is Poutthread remains in place. Figures 7 and 8. ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. Generative Communication in Linda l 105 u Pthread dI I outP... Fig. 7. out P . . . executes in P . . . has already executed a Poutthread looks for a Pinthread. Fig. 8. inP . . . executes, but out P . . . has already executed, so a Pinthread looks for a Poutthread. Note that an outthread registers data in all nodes of a read set just as an in thread does in a write set. But the data recorded in an outthread are treated as a persistent state query, not as part of the network state. We have described an implementation technique for ino and out that is optimal if in and out are equally frequent within a factor of IV12  1. The technique requires that an inthread be constructed each time in  executes and an outthread be constructed each time out executes. The overhead of con ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 106 l D. Gelernter strutting in and outthreads may, however, be substantially reduced in the following ways. First, once node i has determined that tuple P. . . is needed on node j, it associates P with 7 in a nameaddress cache and sends all subsequent P. . . tuples directly to j, bypassing outthread construction until j rejects one such tuple. This scheme is analogous to the runtime conversion of logical to physical addresses in a virtual memory system. A related factor decreases inthread construction. As discussed in Part I, active procedures are a basic structure in Linda programs. Active procedures are headed by in  state ments to which execution returns repeatedly. Inthreads associated with these instatements need not be repeatedly torn down and reconstructed, they may be left in place for as long as the associated procedure block remains invocable. read  statements are implemented precisely as in  statements are, so far as inthreads go. But regarding the associated outthreads there are two possibilities. Suppose readt executes on node j and outt has been executed on node i. Execution of read might cause a copy of t to be transported to j, leaving the original undisturbed on i. On the other hand, the original might itself be transported to j, in which case subsequent readts will find it therenot, in other words, on its nodeoforigin i, but on its nodeoflastreference j. In the former case, the outthread remains undisturbed with its origin on i. In the latter, is outthread is torn down and j must construct a new one with itself as origin. Clearly, the second scheme involves greater threadconstruction over head than the first, but it has what might be the desirable property of distributing tuples to the network sites where they are needed, rather than leaving them forever at their creation sites. A node that stores a collection of tuples that are referenced from many points in the network is a potential bottleneck. Note that the first scheme has the effect of unbalancing queries and updates over the tstate by increasing inthread construction updates relative to outthread construction queries. If this scheme is to be implemented and a significant imbalance is anticipated, the size of read and write sets may have to be adjusted as described in 16 in order to maintain good performance with uniform distri bution. Threads as described above perform only simple name matching, ensuring that a tuple is delivered to an in0 statement whose first component matches the tuples. But that in  statement may in fact specify a structured name, and if the arriving tuple fails to match all fields of the structured name identically, it will be rejected and must be retransmitted to another candidate in statement. A more efficient implementation requires the propagation not just of names but of structured names in inthreads. A structured name is propagated as a list of fields that a tuple must match in order to be acceptable to a given in  statement outthreads continue to propagate simple names only. Notification messages generated when in and outthreads intersect contain a copy of the structured name list associated with the in thread, and no tuple that fails to match a structured name is transmitted. 3.3 Buffering We close with a brief discussion of the second of the two questions raised at the start of Part II Where should tuples in TS be stored ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. Generative Communication in Linda l 107 The VLM is required to simulate Lindas infinite global buffer. System buffer space must expand as needed, buffer space must be provided for undelivered tuples. These requirements are satisfied by making the allocation of buffer space largely the function of compilergenerated code. Allocation of buffers for arriving tuples is in principle a simple extension of the allocation of activation frames in the implementation of sequential languages. When inP. . . executes, a buffer for the actuals is allocated on the execution stack of the process executing in , and is referred to as buffer P. An arriving tuple is stored in buffer P after match and delivery have occurred. Outbound tuples are stored in perprocess heaps maintained by compilergenerated code until they are removed from TS. Header information required by the communi cation systemdealing with the length of the tuple, its logical address, and other basic control characteristicsis prefixed to the buffered outgoing tuple by compilergenerated code. If a process terminates with undelivered tuples, responsibility for the buffers in which they are stored devolves on the kernel, and these buffers are not deallocated until their contents have been delivered. When all processes of a given distributed user job have terminated, however, that jobs undelivered tuples may be erased, except for tuples whose first names appear on the save list or are imported from the systems name space, as mentioned earlier. 3.4 Hardware Failures The semantics of Linda in the presence of hardware failures and the implemen tation of TS in a robust way are important issues that are now under study. We include a few introductory comments only. First, reliable message delivery is not per se at issue in Linda, because Linda does not undertake to deliver messages. outt works correctly if and only if subsequent to execution t has been inserted in TS. out  fails only if the node on which it is executed crashes before t can be handed safely to the kernel on the same node. t may subsequently be lost or destroyed, but this represents a failure of TS, not of out  no bad status or failure exception can or should be reported to the process that executed out . Similarly, in fails only if an instance of t exists in TS but cannot be found. If an instance of t used to exist but has been lost, it is once again TS and not in  that has failed. What are the chances, then, that a t inserted by out  will be safely delivered to an in The system will not guarantee delivery, but should promise highly probable delivery. A system in which reliable delivery is critical may implement in Linda one of the handshake protocols that allow reliable communication over unreliable channels. We do not anticipate that many Linda programs will do this. In a usable implementation of TS, failures will be rare enough not to have to be taken into account in the course of normal program developmentin the same sense in which system failures, file crashes, and so on take place but do not impede normal program development on a useful conventional installation. Maintaining TS reliably is therefore the central issue, and it is made up of two subproblems remembering where tuples are and remembering the tuples them selves. Note that the N1 broadcast scheme for remembering tuple whereabouts is in general, because of its substantial redundancy, the potential basis of a highly ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 108 l D. Gelernter reliable system. A node in this scheme stores data on the whereabouts of t and t tuples its t data are identical to the t data stored by the nodes above and below it in the grid, its t data to the t data stored on the nodes to either side. A node that fails and is rebooted can therefore restore the correct tuple whereabouts state by appropriate queries to neighbors while a node is down, out and inthreads must be routed around it. In general, an entire row or column must fail for tuplewhereabouts data to be lost. The N broadcast scheme is redundant with respect to tuplewhereabouts data but not, as we have described it, to the values of the tuples themselves. Each is stored, until an in  statement seeks it, on the node where its associated out  was executed. If that node fails, the tuple is lost. We are currently investigating the ramifications of a scheme that would be equally redundant with respect to tuples and tuple whereabouts each tuple added to TS by out  would be broadcast to every node in the executing nodes read set i.e., to every node in the executing nodes column on a grid. To assure the atomic character of out  and in , all int requests would still be directed to the node where outt executed, only this tuplekeeper node is authorized to satisfy int requests, and once such,a request is satisfied, t is deleted from every node in the read set. But if the tuple keeper tails, any other node in its read set may become the tuple keeper in its stead. Every node in the read set must fail before a tuple is lost. This scheme makes heavy space and communicationbandwidth demands on the network, and presents some complex implementation problems. On the other hand, it is highly desirable with respect to reliability, and has two other major advantages as well. First, it increases the efficiency of read  N copies of each tuple are available in the network for reading purposes. Second, it increases the efficiency of implementing structured names an in with a structured name can check with any node in theread set to determine whether a t matches it need not go to the unique tuple keeper to find out. 4. CONCLUSIONS We have discussed generative communication and one technique for implement ing it on network computers. The technique we developed has attractive prop erties relative to other possibilitiesbut we have not demonstrated, needless to say, that is is acceptably efficient in absolute terms. Generative communication might place unacceptable burdens on network communication systems. No evi dence to the contrary will be available until implementation projects now in progress are more advanced. We do nonetheless find Linda well worth pursuing. After all, thinking up good uses for evermoreplentiful basic resourcestwo of which are shorthaul com munication bandwidth and communicationprocessor poweris contemporary system designs fundamental problem. We are willing to bet that it is a mistake, at least in the long term, to allow our thinking to be cramped by preconceptions about what is and is not implementable. If we are mistaken in our optimism, then at least we will have examined some interesting problems in a fresh light. We insist only and others will dispute the claim that languages do not become interesting because we can implement them. Implementation problems become interesting when new languages seem worthy of study. ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. Generative Communication in Linda l 109 A threadaddressing, packetswitched communication kernel to support Linda was implemented in part on the 4node prototype SBNnetwork computer at Stony Brook. But SBNs unsophisticated hardware proved unsatisfactory, and the project is quiescent. Linda now runs on an Apollo workstation at Yale this uniprocessor implementation will be used to test the languages utility and expressivity and to study the runtime behavior of distributed programs. In a joint project involving the Yale Linda group and researchers at ATT Bell Labs, Linda is being implemented on a network computer designed and built at Bell. The results of this project will tell us a great deal about whether generative communication can be made to work efficiently. APPENDIX 4.1. Deadlock Detection in TwoPhase Locking Time Uncoupling, Continuation Passing, and Structured Names In the following we discuss an example that is short and fairly simple, but not trivial. Following it will require brief contemplation of twophase locking, which is not the subject of this report. We include it nevertheless, to illustrate the way in which Linda encourages simple solutions that are much harder to express and, correspondingly, less likely to occur in other languages. Consider a set of database transaction processes T, U, V. . .  and a set of data locks A, B, C . . . . T ransaction processes set and clear locks according to the twophase locking protocol 7. An access graph for this system is a directed bipartite graph VI, Vz, E. Nodes in VI correspond to transaction processes and nodes in V2 to locks see Kohler 22. Let t E VI and 1 E V2. Et, 1 an edge from t to I means that transaction t is currently requesting lock 1. E1, t means that lock 1 is currently assigned to transaction t. A deadlock exists in the system iff there is a cycle in the access graph. The Linda solution detects deadlock by turning the set of transaction processes and lock processes into a lifesize as it were transaction graph. Each process acts as a node in the graph, and processes exchange tokens in order to detect cycles. The solution depends upon the fact that at any given time i a transaction node may have more than one incoming edge but only one outgoing edge because a transaction may own several locks but have no more than one unfilled lock request outstanding, and ii a lock node may likewise have more than one incoming edge but only one outgoing edge because many transactions may be requesting a lock, but only one transaction owns it. The solution works generally as follows transactions execute in from all currentlyowned locks outto unique currentlyrequested lock. locks execute in from all currentlyrequesting transactions ou tt0 unique currentowner transaction. A transaction T requests a lock by sending its name to the manager of the lock it is requesting. It then waits at an in statement, expecting some name n in return. If it receives n and rz  ok, then T now owns the lock and may continue. ok is a reserved name that indicates awarding of a lock no transaction may be named ok. If, on the other hand, it receives n and n  T, there is a deadlock in the system Ts name has travelled a cyclic path through the graph and ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 110 l D. Gelernter returned to its node of origin. At this point some corrective action must be taken, but we are not concerned with what it is. If T receives n and n is neither T nor ok, T forwards n to the manager of the unique lock it is currently requesting and reexecutes the in  statement. Transaction S will execute to acquire lock B if not LOCKS, B   there exists a deadlock to free lock B UNLOCKB. LOCK  is a calling routine that returns a value, just as a called routine might., def LOCKTname, Aname nname  outget, A, T enter a request in Ts name for lock A doneboolean  false  do await word from A or inT, var n donenokornT if not done outforward, A, n while not done 1  valn Tthe value of LOCK is the value of this comparison I def UNLOCKAnameoutfree, A The lock manager for lock A executes  LOCKDECL where def LOCKDECL freeboolean  true  0wnername b declarations for a request queue frames of type name I def LOCRENQUEUELname inget, L, requestorname accept a lock request nname  if free award lock to the requestor owner  requestor free  false n  ok prepare to notify the new owner e 1 s e add requestor to the request queue pass the blocked requestors name to the current owner n  requestor outowner, n I def LOCKFORWARDLname  inforward, L, nname outowner, n ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. Generative Communication in Linda l 111 def LOCKDEQUEUELname  infree, L headname   if the request queue is empty free  true else head  head of the request queue outhead, ok notify the new owner owner  head delete request queue head 1 1 1 This example illustrates several properties. Time uncoupling. LOCKFORWARD may execute outowner, n without ascertaining whether an inowner, . . .  statement will ever be executed. Continuation passing. Continuation passing is extended in this example through a chain of length bounded by the number of nodes in the transaction graph. Structured naming. Structured names allow a family of nameshere, get,A, forward,A, and free,Ato be generated from one. None of the other languages surveyed here allows the generation of multiple related names in Lindas sense. ACKNOWLEDGMENTS Thanks to Professor Arthur Bernstein, who directed the thesis in which Linda was first described, and to the referees particularly the indefatigable A for useful, lucid comments. Mauricio Arango was central to the Linda implementa tion project at Stony Brook. Without his enthusiasm and expertise, the work described here could never have been accomplished. Nick Carrier0 plays a similarly pivotal role in the implementation work at Yale. Thanks also to Suresh Jagganathan and Todd Morgan for their work at Stony Brook and to Brian Weston for his at Yale. So much for technical assistance it was Jane Backuss metatechnical contribution that was decisive. REFERENCES 1. ANDREWS, G. Synchronizing resources. ACM Trans. Program. Lung. Syst. 3,4 Oct. 1982, 405. 2. BRINCHHANSEN, P. The programming language Concurrent Pascal. IEEE Trans. Softw. Eng. SEl, 2 June 1975, 199207. 3. BRINCHHANSEN, P. Distributed processes A concurrent programming concept. Commun. ACM 21, 11 Nov. 1978, 934. 4. COOK, R. Moda language for distributed programming. In Proceedings 1st International Conference on Distributed Computing Systems, Oct. 1979, 233241. 5. DEMINET, J. Experience with multiprocessor algorithms. IEEE Trans. Comput. C31, 4 Apr. 1982, 278287. 6. U.S. Dept. of Defense. Reference Manual for the Ada Programming Language. July 1982. 7. ESWARAN, K., GRAY, J., LORIE, R., AND TRAIGER, L. The notions of consistency and predicate locks in a database system. Commun. ACM 19, 11 Nov. 1976, 624. 8. FEHLING, M., AND ERMAN, L. Report on the 3rd Annual Workshop on Distributed Artificial Intelligence. ACM SZGART Newsl. 84 Apr. 1983, 312. 9. FELDMAN, J. Highlevel programming for distributed computing. Commun. ACM 22, 6 June 1979, 353. ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985. 112  D. Gelernter 19. FINKEL, R., AND SOLOMON, M. The Arachne distributed operating system. Tech. Rep. 439, Univ. of Wisconsin at Madison, Computer Science Dept., July 1981. 11. GELERNTER, D., AND BERNSTEIN, A. Distributed communication via global buffer. In Proceed ings ACM Symposium on Principles of Distributed Computing, Aug. 1X2, 1018. 12. GELERNTER, D. An integrated microcomputer network for experiments in distributed program ming. Ph.D. dissertation, SUNY at Stony Brook, Dept. of Computer Science, Oct. 1982. 13. GELERNTER, D. Three reorthogonalizations in a distributed programming language. Tech. Rep., Yale Univ., Dept. Computer Science, Aug. 1983. 14. GELERNTER, D. A note on systems programming in Concurrent Prolog. In Proceedings 1984 International Symposium on Z.ogic Programming, Feb. 1984. 15. GELERNTER, D. Symmetric programming languages. Tech. Rep., Yale Univ., Dept. Computer Science, July 1984. 16. GELNTER, D. Global name spaces on network computers. In Proceedings 1984 International Conference on Parallel Processing, Aug. 1984. 17. HOARE, C.A.R. Communicating sequential processes. Commun. ACM 21 Aug. 1978, 666677. 18. ICHBIAH, J.D., et al. Rationale for the design of the Ada programming language. SZGPLAN Not. 14,6, Part B June 1979. 19. KAHN, G. The semantics of a simple language for parallel processing. In Proceedings ZFZP Congress 1974. p. 471. 20. KERNIGHAN, B., AND RITCHIE, D. The C Programming Language. PrenticeHall, Englewood Cliffs, N.J., 1978. 21. KIEBURTZ, R., AND SILBERSHATZ, A. Comments on communicating sequential processes. ACM Trans. Program. Lang. Syst. 1, 2 Oct. 1979, 218225. 22. KOHLER, W. Overview of synchronization and recovery problems in distributed databases. In Proceedings Fall COMPCON 1980.433441. 23. LISKOV, B. Primitives for distributed computing. In Proceedings 7th Symposium on Operating System Principles, Dec. 1979, 353. 24. SCHNEIDER, F. Synchronization in distributed programs. ACM Trans. Program. Lang. Syst. 4, 2 Apr. 1982, 125148. 25. SHOCH, J., AND HUPP, J. The worm programearly experience with a distributed computa tion. Commun. ACM 25,3 Mar. 1982, 172180. 26. WARREN, D., AND PEREIRA, L. Prolog The language and its implementation compared with Lisp. In Proceedings ACM Symposium on Artificial Intelligence and Programming Languages, Aug. 1977, 109. 27. WIRTH, N. Modula A language for modular multiprogramming. Softw. Pratt. Exper. 7 1977, 335. Received April 1983 revised July 1984 accepted July 1984 ACM Transactions on Programming Languages and Systems, Vol. 7, No. 1, January 1985.
