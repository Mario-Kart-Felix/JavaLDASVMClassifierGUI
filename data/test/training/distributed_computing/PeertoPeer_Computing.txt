PeertoPeer Computing  Dejan S. Milojicic, Vana Kalogeraki, Rajan Lukose, Kiran Nagaraja1, Jim Pruyne, Bruno Richard,  Sami Rollins 2 ,Zhichen Xu  HP Laboratories Palo Alto HPL200257 March 8th , 2002  Email dejan, vana, lukose, pruyne, zhichen  exch.hpl.hp.com, brunorichard  hp.com, knagaraj  cs. rutgers.edu, srollins  cs.ucsb.edu   peertopeer, decentralization, self organization, anonymity, cost  of ownership  The term peertopeer P2P refers to a class of systems and applications that employ distributed resources to perform a critical function in a decentralized manner. With the pervasive deployment of computers, P2P is increasingly receiving attention in research, product development, and investment circles. This interest ranges from enthusiasm, through hype, to disbelief in its potential. Some of the benefits of a P2P approach include improving scalability by avoiding dependency on centralized points eliminating the need for costly infrastructure by enabling direct communication among clients and enabling resource aggregation.  This survey reviews the field of P2P systems and applications by summarizing the key concepts and giving an overview of the most important systems. Design and implementation issues of P2P systems are analyzed in general, and then revisited for each of the case studies described in Section 6. This survey will help people understand the potential benefits of P2P in the research community and industry. For people unfamiliar with the field it provides a general overview, as well as detailed case studies. It is also intended for users, developers, and information technologies maintaining systems, in particular comparison of P2P solutions with alternative architectures and models.    Internal Accession Date Only    Approved for External Publication  1  Rutgers University, NJ, 08901 2   University of California at Santa Barbara, CA, 93106   Copyright HewlettPackard Company 2002 March 8, 2002 1052 am1 INTRODUCTIONPeertoPeer P2P is a very controversial topic. Manyexperts believe that there is not much new in P2P.There is a lot of confusion what really constitutes P2PFor example, is distributed computing really P2P ornot In addition, this field is relatively young, raisingthe question of whether a survey paper is needed. Webelieve that the confusion does warrant a thoroughanalysis. We have included in the paper the systemsthat the P2P community considers to be P2P e.g., SETIhome, as well as those systems that have someP2P aspects e.g., .NET my services, even if there is adivided opinion on the nature of P2P. The goals of thepaper are threefold 1 to understand what P2P is and itis not, as well as what is new, 2 to offer a thoroughanalysis of and examples of P2P computing, and 3 toanalyze the potential of P2P computing.The term peertopeer refers to a class of systems andapplications that employ distributed resources to performa critical function in a decentralized manner. The resources encompass computing power, data storage andcontent, network bandwidth, and presence computers,human, and other resources. The critical function can bedistributed computing, datacontent sharing, communication and collaboration, or platform services. Decentralization may apply to algorithms, data, and metadata, orto all of them. This does not preclude retaining centralization in some parts of the systems and applications if itmeets their requirements. Typical P2P systems reside onthe edge of the Internet or in adhoc networks. P2P enables valuable externalities, by aggregating resourcesthrough lowcost interoperability, the whole is madegreater than the sum of its partsDEJAN S. MILOJICIC1, VANA KALOGERAKI1, RAJAN LUKOSE1, KIRAN NAGARAJA2,JIM PRUYNE1, BRUNO RICHARD1, SAMI ROLLINS3, and ZHICHEN XU1dejan, vana, lukose, pruyne, zhichenexch.hpl.hp.com, brunorichardhp.com,knagarajcs.rutgers.edu, srollinscs.ucsb.eduPeertoPeer Computing1HP Labs, 2Rutgers University, 3University of California at Santa BarbaraAbstractThe term peertopeer P2P refers to a class of systems and applications that employ distributed resources toperform a critical function in a decentralized manner. With the pervasive deployment of computers, P2P is increasingly receiving attention in research, product development, and investment circles. This interest ranges fromenthusiasm, through hype, to disbelief in its potential. Some of the benefits of a P2P approach include improvingscalability by avoiding dependency on centralized points eliminating the need for costly infrastructure by enabling direct communication among clients and enabling resource aggregation.This survey reviews the field of P2P systems and applications by summarizing the key concepts and giving anoverview of the most important systems. Design and implementation issues of P2P systems are analyzed in general, and then revisited for each of the case studies described in Section 6. This survey will help people understandthe potential benefits of P2P in the research community and industry. For people unfamiliar with the field it provides a general overview, as well as detailed case studies. It is also intended for users, developers, and informationtechnologies maintaining systems, in particular comparison of P2P solutions with alternative architectures andmodels.Categories and Subject Descriptors C.2.4 ComputerCommunication Networks Distributed Systems  network operating systems D.1.3 Programming Techniques Concurrent Programming  distributed programming D.4.7 OperatingSystems Organization and Design  distributed systems E.1 Data Data Structures  distributed data structures F.1.2Theory of Computation Modes of Computation  parallelism and concurrency H.3.4 Information Systems Systemsand Software  Distributed systems.General Terms design, experimentationAdditional Key Words and Phrases peertopeer, decentralization, selforganization, anonymity, cost of ownership.2 lower cost of ownership and cost sharing, by usingexisting infrastructure and by eliminating and distributing the maintenance costs anonymityprivacy, by incorporating these requirements in the design and algorithms of P2P systems andapplications, and by allowing peers a greater degree ofautonomous control over their data and resourcesHowever, P2P also raises some security concerns for users and accountability concerns for IT. In general it isstill a technology in development where it is hard to distinguish useful from hype and new from old. In the restof the paper we evaluate these observations in general aswell as for specific P2P systems and applications.P2P gained visibility with Napsters support for musicsharing on the Web Napster 2001 and its law suit withthe music companies over digital rights management.However, it is increasingly becoming an important technique in various areas, such as distributed and collaborative computing both on the Web and in adhoc networks.P2P has received the attention of both industry and academia. Some big industrial efforts include the P2PWorking Group, led by many industrial partners such asIntel, HP, Sony, and a number of startup companies andJXTA, an opensource effort led by Sun. There are anumber academic events dedicated to P2P, such as theInternational Workshop on P2P Computing, Global andP2P Computing on Large Scale Distributed Systems, International Conference on P2P Computing, and OReillyP2P and Web Services Conference. There are already anumber of books published Oram 2000, Barkai 2001,Miller, 2001, Moore and Hebeler 2001, Fatah and Fatah2002, and a number of theses and projects in progress atuniversities, such as Chord Stoica et al 2001, OceanStore Kubiatowitz et al. 2000, PAST Druschel andRowstron 2001, CAN Ratnasamy 2001, and FreeNetClark 1999.There are several of the definitions of P2P that are beingused by the P2P community. The Intel P2P workinggroup defines it as the sharing of computer resourcesand services by direct exchange between systemsp2pwg, 2001. Alex Weytsel of Aberdeen defines P2Pas the use of devices on the internet periphery in a nonclient capacity Veytsel, 2001. Ross Lee Graham defines P2P through three key requirements a they havean operational computer of server quality b they havean addressing system independent of DNS and c theyare able to cope with variable connectivity Graham2001. Clay Shirky of OReilly and Associate uses thefollowing definition P2P is a class of applications thattakes advantage of resources  storage, cycles, content,human presence  available at the edges of the Internet.Because accessing these decentralized resources meansoperating in an environment of unstable connectivity andunpredictable IP addresses, P2P nodes must operate outside the DNS system and have significant or total autonomy from central servers Shirky 2001. Finally,Kindberg defines P2P systems as those with independentlifetimes Kindberg 2002.1. INTRODUCTIONPaper Organization and Intended Audience2. OVERVIEW2.1. Goals2.2. Terminology2.3. P2P Taxonomies3. COMPONENTS AND ALGORITHMS3.1. Infrastructure Components3.2. Algorithms4. CHARACTERISTICS4.1. Decentralization4.2. Scalability4.3. Anonymity4.4. SelfOrganization4.5. Cost of Ownership4.6. AdHoc Connectivity4.7. Performance4.8. Security4.9. Transparency and Usability4.10.Fault Resilience4.11.Interoperability4.12.Summary5. P2P SYSTEMS5.1. Historical5.2. Distributed Computing5.3. File Sharing5.4. Collaboration5.5. Platforms6. CASE STUDIES6.1. Avaki6.2. SETIhome6.3. Groove6.4. Magi6.5. FreeNet6.6. Gnutella6.7. JXTA6.8. .NET My Services6.9. Summary7. LESSONS LEARNED7.1. Strengths and Weaknesses7.2. NonTechnical Challenges7.3. Implications for Users, Developers, and IT8. SUMMARY AND FUTURE WORK8.1. Final Thoughts on What P2P Is8.2. Why We Think P2P is Important8.3. P2P in the Future8.4. SummaryACKNOWLEDGMENTSREFERENCESAPPENDIX A P2P VS. ALTERNATIVES3In our view, P2P is about sharing giving to and obtaining from the peer community. A peer gives some resources and obtains other resources in return. In the caseof Napster, it was about offering music to the rest of thecommunity and getting other music in return. It could bedonating resources for a good cause, such as searchingfor extraterrestrial life or combating cancer, where thebenefit is obtaining the satisfaction of helping others.P2P is also a way of implementing systems based on thenotion of increasing the decentralization of systems, applications, or simply algorithms. It is based on the principles that the world will be connected and widelydistributed and that it will not be possible or desirable toleverage everything off of centralized, administrativelymanaged infrastructures. P2P is a way to leverage vastamounts of computing power, storage, and connectivityfrom personal computers distributed around the world.Assuming that peer is defined as like each other, aP2P system then is one in which autonomous peers depend on other autonomous peers. Peers are autonomouswhen they are not wholly controlled by each other or bythe same authority, e.g., the same user. Peers depend oneach other for getting information, computing resources,forwarding requests, etc. which are essential for the functioning of the system as a whole and for the benefit of allpeers. As a result of the autonomy of peers, they cannotnecessarily trust each other and rely completely on thebehavior of other peers, so issues of scale and redundancy become much more important than in traditional centralized or distributed systems.Conceptually, P2P computing is an alternative to thecentralized and clientserver models of computing,where there is typically a single or small cluster of servers and many clients see Figure 1. In its purest form, theP2P model has no concept of server rather all participants are peers. This concept is not necessarily new.Many earlier distributed systems followed a similarmodel, such as UUCP Nowitz 1978 and switched networks Tanenbaum 1981. The term P2P is also not new.In one of its simplest forms, it refers to the communication among the peers. For example, in telephony userstalk to each other directly once the connection is established, in a computer networks the computers communicate P2P, and in games, such as Doom, players alsointeract directly. However, a comparison between clientserver and P2P computing is significantly more complexand intertwined along many dimensions. Figure 2 is anattempt to compare some aspects of these two models.The P2P model is quite broad and it could be evaluatedfrom different perspectives. Figure 3 categorizes thescope of P2P development and deployment. In terms ofdevelopment, platforms such as JXTA provide an infrastructure to support P2P applications. Additionally, developers are beginning to explore the benefit ofimplementing various horizontal technologies such asdistributed computing, collaborative, and content sharing software using the P2P model rather than more traditional models such as clientserver. Applications such asfile sharing and messaging software are being deployedin a number of different vertical markets. Section 2.3provides a more thorough evaluation of P2P markets andSection 5 describes the horizontal technologies in moredetail.Figure 1 Simplified, HighLevel View of PeertoPeer versus Centralized ClientServer Approach.peers clientsserverFigure 2 PeertoPeer versus ClientServer. There is noclear border between a clientserver and a P2P model. Bothmodels can be built on a spectrum of levels of characteristicse.g., manageability, configurability, functionality e.g., lookup versus discovery, organizations e.g., hierarchy versusmesh, components e.g., DNS, and protocols e.g., IP, etc.Furthermore, one model can be built on top of the other orparts of the components can be realized in one or the othermodel. Finally, both models can execute on different types ofplatforms Internet, intranet, etc. and both can serve as an underlying base for traditional and new applications. Therefore,it should not be a surprise that there is so much confusion aboutwhat P2P is and what it is not. It is extremely intertwined withexisting technologies Morgan 2002.clientserver peertopeerdistributed systemsmanaged..................................selforganizedconfigured.......................................................adhoclookup..................................................................discoverhierarchy.......................................................................meshstatic...........................................................................mobileIPcentric....................................................also nonIPDNSbased...............................custom namingclustersWANs gridsadhoc NWeServiceseBusinessWeb appsRPC........................................asyncCORBARMIGnutellaNapsterInternet intranetdistr. apps.NETJXTAserver dependencies...........................independent lifetime4The following three lists, which are summarized inTable 1. are an attempt to define the nature of P2P, whatis new in P2P, and what is not new in P2P.P2P is concerned with The historical evolution of computing in general andthe Internet in particular computing at the edge of theInternet. e.g., SETIhome and other distributed computing systems Sociological aspects of sharing of content e.g., Napster and other filecontent sharing systems Technological improvements to networks and communication advances e.g., wireless networks, handhelddevices enabling better collaboration and communication P2P software architectures e.g., JXTA or .NET Deployed P2P algorithms e.g., Gnutella, FreeNet.New aspects of P2P include Technology requirements scale of deployed computers adhoc connectivity security e.g., crossing the firewall Architectural requirements availability in particular of the scale of the futurenumber of computers scalability of future systems worldwide as well as ofembedded systems privacy and anonymity beyond the Web Economy requirements cost of ownership pervasive use home versus business  expectationsabout quality and guarantees versus best effortAspects of P2P that are not new include Concept and applications e.g., telephony, networks,servers Decentralization for scalability and availability e.g.,distributed systems in general, replication Distributed state management e.g., work of Barak andmany others Disconnected operations in general e.g., Coda andmany others Distributed scheduling algorithms e.g., on clustersand grids Scalability WWW and Web services in particular Adhoc networks eBusiness Algorithms many P2P and distributed algorithms already existPaper Organization and Intended AudienceThe paper is organized as follows. Section 2 providesbackground on P2P. Section 4 describes characteristicsof P2P solutions. Section 3 presents P2P components andalgorithms. Section 5 classifies P2P systems into fivecategories and describes a few representatives for eachcategory. In Section 6, we present more detailed casestudies. Section 7 presents lessons learned from analyzFigure 3 P2P Solutions. P2P can be classified into interoperable P2P platforms, applications of the P2P technology, andvertical P2P applications.JXTA, .NET Servicesdistributed collaboration and content sharingfinancial biotech.... entertainmentmessagingrich mediasharingonlinestoragefilesharingplatformshorizontaltechnologiesexampleapplicationsmarkets enterprisecomputing communicationexampleprocessmgmtgamessimulationmarketcalculationdemogr.analysisetc. etc. etc.genomeproteinfoldingetc.sequencewhiteboardsinstantindustriescommunic.PerspectiveComparisonWhat is New What isNot NewWellKnownExamples Enabler Enabling AlternativesPaperRoadmapHistoricalEvolutionaryComputingcomputing on theedge of the Internetscalability,availability,security,connectivitydistributedschedulingconcept,applications,distributionSETIhomeubiquitous computingcommunicationuse vast available computerpower at home and officeclassic NW, clusters, GridsSections5.2, 6.1, 6.2CulturalSociologicalContent sharingServicesdirect sharing privacy, anonymitydecentralization Napsterbroad Internetconnectivityusertouser exchange,minimal broker engagementclientserver, B2BWeb ServicesSections5.3, 6.5, 6.6CommunicationCollaborationdealing with disconnectionadhoc NW, disconnected operationAOL Chat, Grooveparasitic NWnew NWs wireless,broadbandimproved communicationand collaborationLotus Notes,NetMeetingSections5.4, 6.3, 6.4Architectural cost of ownership P2P concept andapplications JXTA, .NETincreased componentdecentralizationlarger scale,better accessibilityCORBA, RMI,other middlewareSections5.5, 6.7, 6.8AlgorithmsProgramming Modelparticularalgorithmsdistributed statealgor. in generalGnutella decentralized stateimproved scalability, availability, and anonymityclientserver Section 5.3Table 1. Various P2P Perspectives Illustrating What is and What is Not New in P2P.5ing the P2P area. Finally, in the last section, we summarize the paper and describe opportunities for furtherresearch. In Appendix A, we compare P2P systems withalternative solutions.This paper is intended for people new to the field of P2Pas well as for experts. It is intended for users, developers,and IT personnel. This first section provided a brief overview of the field. Readers interested in learning about thefield in general should read Sections 2 through 5. Expertscan benefit from the detailed case studies in Section 6.Perspectives of users, developers, and IT personnel areaddressed in Section 7.3. We assume that the readershave a general knowledge of computing systems.2 OVERVIEWP2P is frequently confused with other terms, such as traditional distributed computing Coulouris et al. 2001,grid computing Foster and Kesselman 1999, and adhoc networking Perkins 2001. To better define P2P,this section introduces P2P goals, terminology, and taxonomies.2.1 GoalsAs with any computing system, the goal of P2P systemsis to support applications that satisfy the needs of users.Selecting a P2P approach is often driven by one or moreof the following goals. Cost sharingreduction. Centralized systems thatserve many clients typically bear the majority of thecost of the system. When that main cost becomes toolarge, a P2P architecture can help spread the cost overall the peers. For example, in the filesharing space, theNapster system enabled the cost sharing of file storage,and was able to maintain the index required for sharing. In the end, the legal costs of maintaining the indexbecame too large, and so more radical P2P systemssuch as Gnutella were able to share the costs even further. Much of the cost sharing is realized by the utilization and aggregation of otherwise unused resourcese.g. SETIhome, which results both in net marginalcost reductions and a lower cost for the most costlysystem component. Because peers tend to be autonomous, it is important for costs to be shared reasonablyequitably. Improved scalabilityreliability. With the lack ofstrong central authority for autonomous peers, improving system scalability and reliability is an importantgoal. As a result, algorithmic innovation in the area ofresource discovery and search has been a clear area ofresearch, resulting in new algorithms for existing systems REFS, and the development of new P2P platforms such as CAN Ratnasamy 2001, Chord Stoica2001, and PAST Rowstron 2001. These developments will be discussed in more detail in Section 3. Resource aggregation and interoperability. A decentralized approach lends itself naturally to aggregation of resources. Each node in the P2P system bringswith it certain resources such as compute power orstorage space. Applications that benefit from hugeamounts of these resources, such as computeintensivesimulations or distributed file systems, naturally leantoward a P2P structure to aggregate these resources tosolve the larger problem. Distributed computing systems, such as SETIHome, distributed.net, and Endeavours are obvious examples of this approach. Byaggregating compute resources at thousands of nodes,they are able to perform computationally intensivefunctions. File sharing systems, such as Napster, Gnutella, and so forth, also aggregate resources. In thesecases, it is both disk space to store the communityscollection of data and bandwidth to move the data thatis aggregated. Interoperability is also an important requirement for the aggregation of diverse resources. Increased autonomy. In many cases, users of a distributed system are unwilling to rely on any centralizedservice provider. Instead, they prefer that all data andwork on their behalf be performed locally. P2P systems support this level of autonomy simply becausethey require that the local node do work on behalf ofits user. The principle example of this is the variousfile sharing systems such as Napster, Gnutella, andFreeNet. In each case, users are able to get files thatwould not be available at any central server because oflicensing restrictions. However, individuals autonomously running their own servers have been able toshare the files because they are more difficult to findthan a server operator would be. Anonymityprivacy. Related to autonomy is the notion of anonymity and privacy. A user may not wantanyone or any service provider to know about his orher involvement in the system. With a central server, itis difficult to ensure anonymity because the server willtypically be able to identify the client, at least by Internet address. By employing a P2P structure in which activities are performed locally, users can avoid havingto provide any information about themselves to anyone else. FreeNet is a prime example of how anonymity can be built into a P2P application. It uses aforwarding scheme for messages to ensure that theoriginal requestor of a service cannot be tracked. It increases anonymity by using probabilistic algorithmsso that origins cannot be easily tracked by analyzingnetwork traffic.6 Dynamism. P2P systems assume that the computingenvironment is highly dynamic. That is, resources,such as compute nodes, will be entering and leavingthe system continuously. When an application is intended to support a highly dynamic environment, theP2P approach is a natural fit. In communication applications, such as Instant Messaging, socalled buddylists are used to inform users when persons withwhom they wish to communicate become available.Without this support, users would be required to pollfor chat partners by sending periodic messages tothem. Likewise, distributed computing applicationssuch as distributed.net and SETIHome must adapt tochanging participants. They, therefore, must reissuecomputation jobs to other participants to ensure thatwork is not lost if earlier participants drop out of thenetwork while they were performing a computationstep. Enabling adhoc communication and collaboration. Related to dynamism is the notion of supportingadhoc environments. By ad hoc, we mean environments where members come and go based perhaps ontheir current physical location or their current interests. Again, P2P fits these applications because it naturally takes into account changes in the group ofparticipants. P2P systems typically do not rely on established infrastructure  they build their own, e.g.,logical overlay in CAN and PAST.2.2 TerminologyThe following terminology is used in the taxonomies inSection 2.3 and in the comparisons between P2P and itsalternatives in Section 7.1 and Appendix A. Centralized systems represent singleunit solutions, including single and multiprocessor machines, as wellas highend machines, such as supercomputers andmainframes. Distributed systems are those in which components located at networked computers communicate and coordinate their actions only by passing messagesCouloris, et al. 2001. Client is informally defined as an entity node, program, module, etc. that initiates requests but is notable to serve requests. If the client also serves the request, then it plays the role of a server. Server is informally defined as an entity that serves requests from other entities, but does not initiate requests. If the server does initiate requests, then it playsthe role of a client. Typically, there are one or a fewservers versus many clients. ClientServer model represents the execution of entities with the roles of clients and servers. Any entity ina system can play both roles but for a different purpose, i.e. server and client functionality residing onseparate nodes. Similarly an entity can be a server forone kind of request and client for others. Peer is informally defined as an entity with capabilities similar to other entities in the system. P2P model enables peers to share their resources information, processing, presence, etc. with at most alimited interaction with a centralized server. The peersmay have to handle a limited connectivity wireless,unreliable modem links, etc., support possibly independent naming, and be able to share the role of theserver Oram, 2000. It is equivalent to having all entities being client and servers for the same purpose.Other terms frequently associated with P2P include Distributed computing, which is defined as a computer system in which several interconnected computersshare the computing tasks assigned to the systemIEEE 1990. Such systems include computing clusters, Grids see below, and global computing systemsgathering computing resources from individual PCsover the Internet. We will use the term DistributedComputing to refer to systems that have inherent P2Pproperties. Grid computing, which is defined as coordinated resource sharing and problem solving in large, multiinstitutional virtual organization. Foster andKesselman 1999. More specifically, a Grid is an infrastructure for globally sharing computeintensive resources such as supercomputers or computationalclusters. As far as transparency is concerned, Gridcomputing is orthogonal to P2P distributed computingsystems. Adhoc communication, which is defined as a systemthat enables communication to take place without anypreexisting infrastructure in place, except for the communicating computers. These computers form an adhoc network. This network and associated computerstake care of communication, naming, and security.P2P systems can be used on top of an adhoc communication infrastructure.There are many examples of distributed systems, at various scales, such as the Internet, widearea networks, intranets, localarea networks, etc. Distributed systemcomponents can be organized in a P2P model or in a clientserver model. We believe that other models, such asthreetier and publishsubscribe, can be mapped onto clientserver. Typical client examples include Web browsers e.g., Netscape Communicator or Internet Explorer,7file system clients, DNS clients, CORBA clients, etc.Server examples include name servers DNS Albitz andLiu 2001, Mockapetris 1989, LDAP Howes and Smith2001, etc., distributed file servers NFS Sandberg etal. 1985, AFS Howard et al. 1988, CORBA Object Request Brokers OMG 1996, HTTP Server, authentication server, etc. Clientserver model examples includeCORBA, RMI Wolrath et al 1996, and other middleware Bernstein 1996 Britton 2000. Peer examples include computers in a network that serve a similar role.There are numerous examples of the P2P model throughout the paper.2.3 P2P TaxonomiesA taxonomy of computer systems from the P2P perspective is presented in Figure 4. All computer systems canbe classified into centralized and distributed. Distributedsystems can be further classified into the clientservermodel and the P2P model. The clientserver model canbe flat where all clients only communicate with a singleserver possibly replicated for improved reliability, or itcan be hierarchical for improved scalability. In a hierarchal model, the servers of one level are acting as clientsto higher level servers. Examples of a flat model includetraditional middleware solutions, such as object requestbrokers and distributed objects. Examples of a hierarchical model include DNS server and mounted file systems.The P2P model can either be pure or it can be hybrid. Ina pure model, there does not exist a centralized server.Examples of a pure P2P model include Gnutella andFreenet. In a hybrid model, a server is approached first toobtain metainformation, such as the identity of the peeron which some information is stored, or to verify securitycredentials see Figure 5 a. From then on, the P2Pcommunication is performed see Figure 5 b. Examples of a hybrid model include Napster, Groove, Aimster, Magi, Softwax, and iMesh. There are alsointermediate solutions where with SuperPeers, such asKaZaa. SuperPeers contain some of the information thatothers may not have. Other peers typically lookup information at SuperPeers if they cannot find it otherwise.Figure 6 presents a coarse taxonomy of P2P systems. Weclassify P2P systems into distributed computing e.g.,SETIhome, Avaki, Entropia, file sharing e.g., Napster, Gnutella, Freenet, Publius, Free Haven, collaboration e.g., Magi, Groove, Jabber, and platforms e.g.,JXTA and .NET My Services. Section 6 contains a description of eight case studies of P2P systems accordingto the taxonomy presented in Figure 6.In Figure 7, we present a classification of various P2Psystems according to the taxonomy presented inFigure 6. This figure demonstrates that certain P2P systems emphasize different aspects along the taxonomy dimensions computing, storage, communication,whereas the platforms support all of these dimensions.Distributed SystemsPeertoPeerClientServerHybridPureHierarchicalFlatFigure 4 A Taxonomy of Computer Systems.Centralized Systemsmainframes, SMPs, workstationsComputer SystemsFigure 5 Hybrid PeertoPeer Model. 1 Initial communication with a server, e.g., to obtain the locationidentity of a peer,followed by 2 direct communication with a peer.peersserverpeersserver1 2a bP2P SystemsFigure 6 A Taxonomy of P2P Systems.distributedcomputingfilesharingcollaboration platformsFigure 7 A Classification of P2P Systems Based on the Taxonomy in Figure 6. Systems in bolditalic are case studies described in detail in Section 6.distributedcomputingfilesharingcollaborationplatformsSETIhome, United DevicesEntropia, DataSynapseAvaki.NETJXTAJabberAIMster MagiGrooveGnutella, FreenetMojo Nation,NapsterFree Haven, PubliusPointeraOnSystemsCenterspanCybikoPorivo TechnologiesGlobuscommunication and8Application Taxonomy. Three main classes of P2P applications have emerged parallelizable, content and filemanagement, and collaborative see Figure 8. Parallelizable. Parallelizable P2P applications split alarge task into smaller subpieces that can execute inparallel over a number of independent peer nodes.Most implementations of this model have focused oncomputeintensive applications. The general idea behind these applications is that idle cycles from anycomputer connected to the Internet can be leverage tosolve difficult problems that require extreme amountsof computation. Most often, the same task is performed on each peer using different sets of parameters.Examples of implementations include searching forextraterrestrial life SETIHome, 2001, code breaking, portfolio pricing, risk hedge calculation, marketand credit evaluation, and demographic analysis. Section 5.2 presents systems supporting this class of application. Componentized applications have not yet beenwidely recognized as P2P. However, we envision applications that can be built out of finergrain components that execute over many nodes in parallel. Incontrast to computeintensive applications that run thesame task on many peers, componentized applicationsrun different components on each peer. Examples include Workflow, JavaBeans, or Web Services in general. Content and file management. Content and file management P2P applications focus on storing information on and retrieving information from various peersin the network. The model that popularized this classof application is the content exchange model. Applications like Napster Napster 2001 and Gnutella Gnutella 2001 allow peers to search for and downloadfiles, primarily music files, that other peers have madeavailable. For the most part, current implementationshave not focused much on providing reliability andrely on the user to make intelligent choices about thelocation from which to fetch files and to retry whendownloads fail. They focus on using otherwise unusedstorage space as a server for users. These applicationsmust ensure reliability by using more traditional database techniques such as replication. A number of research projects have explored the foundations of P2Pfile systems Ratnasamy et al 2001, Bolosky et al2000, Kubiatowicz et al 2000, Rowstron and Druschel2001, Gribble et al 2001, Stoica et al 2001. Finally,filtering and mining applications such as OpenCOLAOpenCOLA 2001 and JXTA Search Waterhouse etal. 2002 are beginning to emerge. Instead of focusingon sharing information, these applications focus oncollaborative filtering techniques that build searchableindices over a peer network. A technology such asJXTA Search can be used in conjunction with an application like Gnutella to allow more uptodate searchesover a large, distributed body of information. Collaborative. Collaborative P2P applications allowusers to collaborate, in real time, without relying on acentral server to collect and relay information. Instantmessaging is one subclass of this class of application.Services such as Yahoo, AOL, and Jabber instantmessaging have become popular among a wide varietyof computer users Strom 2001. Similarly, shared applications that allow people e.g., business colleaguesto interact while viewing and editing the same information simultaneously, yet possibly thousands ofmiles apart, are also emerging. Examples includeBuzzpad www.buzzpad.com and distributed PowerPoint Rice and Mahon 2000. Games are a final typeof collaborative P2P application. P2P games are hosted on all peer computers and updates are distributed toall peers without requiring a central server. Examplegames include NetZ 1.0 by Quazalwww.quazal.com, Scour Exchange by CenterSpan,Descent www.planetdescent.com, and Cybiko.P2P Target Environments. The target environments forP2P consist of the Internet, intranets, and adhoc networks. P2P systems connected to Internet support connections in the spectrum from dialup lines to broadbandDSL. The underlying architecture can rely on personalhome computers, corporate desktops, or personal mobilecomputers laptops and handhelds.The most frequent environment is personal home computers connected to the Internet. The early P2P systemsin this environment were primarily used for content sharing. Examples include Napster, Gnutella, and Aimster.Distributed computing in a P2P fashion also started ondesktop computers on the Internet such as SETIhome,but it also gained acceptance in Intranets, such as inDataSynapse Intel 2001, DataSynapse 2001.Adhoc networks of handhelds are only recently becoming available e.g., Endeavors Technologies Magi dedicated for collaborative computing, but similar platformsare expected to follow with a wider deployment of handheld computers and wireless networks.P2P applicationscollaborativeparallelizableinstant gamescontent and fileexchange systems messagingsharedfiltering,miningcontent filecompute compoFigure 8 A Taxonomy of P2P Applications.intensive appsnentizedmanagement9Future environments may include variations of existingdeployment scenarios, such as using corporate desktopsfor content sharing, e.g., for distributed Internet data centers, or handheld computers for resource aggregationcontent sharing or distributed computing. Furthermore,Internet2 httpwww.internet2.edu may offer moreenabling support for P2P systems and applications.P2P Markets. There are three main markets for P2Pconsumer, enterprise, and public. Consumer space encompasses the use of P2P for personal use, such as musicand content sharing, instant messaging, email, andgames. Application examples include music sharing andcommunication. Examples of companies in this space include Napster and Gnutella. Enterprise space P2P applications include biotech, financial, traditional ITsolutions, and B2B. Example companies include DataSynapse, Information Architects, and WorldStreet. Finally, the public class of applications includes the following types of applications information sharing, digitalrights management, and entertainment. Centerspan,AIM, and Scour deliver music and video on broadbandusing P2P technology.According to Philips, markets can be classified based onuser activities into those performed work, play, andrest Nethisinghe 200. We extended this perspectiveto map user activities to markets consumer, enterprise,and public see Table 2. The insight is that P2P permeates a number of user activities and markets.3 COMPONENTS AND ALGORITHMSThis section introduces components of P2P systems andalgorithms used in P2P.3.1 Infrastructure ComponentsFigure 10 illustrates an abstract P2P architecture. In thissection, we discuss the functions of each component andlook at some of the tradeoffs involved in making component implementation decisions.Communication. The P2P model covers a wide spectrum of communication paradigms. At one end of thespectrum are desktop machines mostly connected viastable, highspeed links over the Internet Saroiu et al.2002. At the other end of the spectrum, are small wireless devices such as PDAs or even sensorbased devicesthat are connected in an adhoc manner via a wirelessmedium. The fundamental challenge of communicationin a P2P community is overcoming the problems associated with the dynamic nature of peers. Either intentionally e.g., because a user turns off her computer orunintentionally e.g., due to a, possibly dialup, networklink failing peer groups frequently change. Maintainingapplicationlevel connectivity in such an environment isone of the biggest challenges facing P2P developers.Group Management. Peer group management includesdiscovery of other peers in the community and locationand routing between those peers. Discovery of peers canbe highly centralized such as in Napster Napster 2001,highly distributed such as in Gnutella Gnutella 2001, orsomewhere in between. A number of factors influencethe design of discovery algorithms. For example, mobile,wireless devices can discover other peers based uponP2P marketspublicconsumercontententerprisebiotech financial B2Bcontent commuentertaFigure 9 A Taxonomy of P2P Markets. There are also othermarkets that can map to some of the presented ones, such asmilitary, education, scientific, etc.deliverydigitalrights mgmtexchange nication inmenttype ofactivityscopeconsumer enterprise publicwork collaboration,communicationdistributed computing,storage, communication, collaborationcommunication,digital rights mgmtplay games HRsponsored events digital mediadigital experiencerest music sharing content consumption instant messagingTable 2. P2P Markets versus P2P Applications.Figure 10 An Informal P2P System Architecture. The orderof components does not strictly follow layering.toolsresourceaggregationschedulingdiscoveryreliabilitymetadata messaging managementcommunicationservicesapplicationslocating androutingsecurityCommunicationGroup ManagementRobustnessClassspecificApplicationspecificLayerLayerLayerLayerLayer10their range of communication Roman et al. 2001. Protocols built for desktop machines often use other approaches such as centralized directories.Location and routing algorithms generally try to optimize the path of a message traveling from one peer to another. While deployed systems such as Napster andGnutella try to optimize factors such as underlying network latency, the research in this space takes a more systematic approach as discussed in Section 3.2.Robustness. There are three main components that areessential to maintaining robust P2P systems security, resource aggregation, and reliability. Security is one of thebiggest challenges for P2P infrastructures. A benefit ofP2P is that it allows nodes to function as both a client anda server. However, transforming a standard client deviceinto a server poses a number of risks to the system. Onlytrusted or authenticated sources should have access to information and services provided by a given node. Unfortunately, the security requirement either requirespotentially cumbersome intervention from the user, orinteraction with a trusted third party. Centralizing thetask of security is often the only solution even though itvoids the P2P benefit of a distributed infrastructure. Fora more detailed discussion of P2P security concerns seeSection 4.8.The P2P model provides the basis for interacting peers toaggregate resources available on their systems. Classifying the architecture of a P2P resource aggregation component is difficult because there are a wide variety ofresources that may be aggregated across peers. On theone hand, resources include files or other content residing on a computer. A wide variety of file sharing systemshave addressed the problem of aggregating this type ofresource. On the other hand, resources can be defined interms of the resources available on a given peer devicesuch as CPU processing power, bandwidth, energy, anddisk space.Reliability in P2P systems is a hard problem. The inherently distributed nature of peer networks makes it difficult to guarantee reliable behavior. The most commonsolution to reliability across P2P systems is to take advantage of redundancy. For example, in case of computeintensive applications upon a detection of a failure thetask can be restarted on other available machines. Alternatively, the same task can be initially assigned to multiple peers. In file sharing applications, data can bereplicated across many peers. Finally, in messaging applications, lost messages can be resent or can be sentalong multiple paths simultaneously.ClassSpecific. While the components discussed so farare applicable to any P2P architecture, applicationspecific components abstract functionality from each classof P2P application. Scheduling applies to parallelizableor computeintensive applications. Computeintensivetasks are broken into pieces that must be scheduledacross the peer community. Metadata applies to contentand file management applications. Metadata describesthe content stored across nodes of the peer communityand may be consulted to determine the location of desired information. Messaging applies to collaborative applications. Messages sent between peers enablecommunication. Management supports managing theunderlying P2P infrastructure.ApplicationSpecific. Tools, applications, and servicesimplement applicationspecific functionality, which correspond to certain P2P applications running on top of theunderlying P2P infrastructure. It corresponds to specificcases of distributed scheduling e.g.scientific, financial,biotechnology, etc., content and file sharing e.g., musicMP3 file exchange, or specific applications running ontop of collaborative and communication systems, such ascalendaring, notes, messaging, and chatting.3.2 AlgorithmsThis section overviews three common P2P algorithmsand then compares their implementations in a few P2Psystems.Centralized directory model. This model was madepopular by Napster. The peers of the community connectto a central directory where they publish informationabout the content they offer for sharing see Figure 11.Upon request from a peer, the central index will matchthe request with the best peer in its directory that matchesthe request. The best peer could be the one that is cheapest, fastest, or the most available, depending on the userneeds. Then a file exchange will occur directly betweenthe two peers. This model requires some managed infraFigure 11 Central Index Algorithm.index15423searchdownload11structure the directory server, which hosts informationabout all participants in the community. This can causethe model to show some scalability limits, because it requires bigger servers when the number of requests increase, and larger storage when the number of usersincrease. However, Napster experience showed that except for legal issues  the model was very strong andefficient.Flooded requests model. The flooding model is different from the central index one. This is a pure P2P modelin which no advertisement of shared resources occurs.Instead, each request from a peer is flooded broadcastto directly connected peers, which themselves flood theirpeers etc., until the request is answered or a maximumnumber of flooding steps typically 5 to 9 occur seeFigure 12. This model, which is used by Gnutella, requires a lot of network bandwidth, and hence does notprove to be very scalable, but it is efficient in limitedcommunities such as a company network. To circumventthis problem, some companies have been developingsuperpeer client software, that concentrates lots of therequests. This leads to much lower network bandwidthrequirement, at the expense of high CPU consumption.Caching of recent search requests is also used to improvescalability.Document routing model. The document routing model, used by FreeNet, is the most recent approach. Eachpeer from the network is assigned a random ID and eachpeer also knows a given number of peers see Figure 13.When a document is published shared on such a system, an ID is assigned to the document based on a hashof the documents contents and its name. Each peer willthen route the document towards the peer with the IDthat is most similar to the document ID. This process isrepeated until the nearest peer ID is the current peers ID.Each routing operation also ensures that a local copy ofthe document is kept. When a peer requests the documentfrom the P2P system, the request will go to the peer withthe ID most similar to the document ID. This process isrepeated until a copy of the document is found. Then thedocument is transferred back to the request originator,while each peer participating the routing will keep a localcopy.Although the document routing model is very efficientfor large, global communities, it has the problem that thedocument IDs must be known before posting a requestfor a given document. Hence it is more difficult to implement a search than in the flooded requests model. Also,network partitioning can lead to an islanding problem,where the community splits into independent subcommunities, that dont have links to each other.Four main algorithms have implemented the documentrouting model Chord, CAN, Tapestry, and Pastry. Thegoals of each algorithm are similar. The primary goalsare to reduce the number of P2P hops that must be takento locate a document of interest and to reduce the amountof routing state that must be kept at each peer. Each of thefour algorithms either guarantee logarithmic bounds withrespect to the size of the peer community, or argue thatlogarithmic bounds can be achieved with high probability. The differences in each approach are minimal, however each is more suitable for slightly differentenvironments. In Chord, each peer keeps track ofother peers where N is the total number of peers in thecommunity. When peer joins and leaves occur the highly optimized version of the algorithm will only need tonotify other peers of the change. In CAN, eachpeer keeps track of only a small number of other peerspossibly less than . Only this set of peers is affected during insertion and deletion, making CAN more suitable for dynamic communities. However, the tradeoff inthis case lies in the fact that the smaller the routing tableof a CAN peer, the longer the length of searches. Tapestry and Pastry are very similar. The primary benefit ofthese algorithms over the other two is that they activelytry to reduce the latency of each P2P hop in addition toreducing the number of hops taken during a search.Figure 12 Flooded Requests Algorithm.15423searchdownload6Figure 13 Document Routing Algorithm.15423Id 0012006Id 001500Id 000010Id 000200Id 000024FileIdhdata0008NlogNlogNlog12Comparison of algorithms. The Chord algorithm models the identifier space as a unidimensional, circularidentifier space. Peers are assigned IDs based on a hashon the IP address of the peer. When a peer joins the network, it contacts a gateway peer and routes toward itssuccessor. The routing table at each peer n contains entries for other peers where the ith peer succeeds nby at least . To route to another peer, the routing table at each hop is consulted and the message is forwardedtoward the desired peer. When the successor of the newpeer is found, the new peer takes responsibility for the setof documents that have identifiers less than or equal to itsidentifier and establishes its routing table. It then updatesthe routing state of all other peers in the network that areaffected by the insertion. To increase the robustness ofthe algorithm, each document can be stored at somenumber of successive peers. Therefore, if a single peerfails, the network can be repaired and the document canbe found at another peer.CAN models the identifier space as multidimensional.Each peer keeps track of its neighbors in each dimension.When a new peer joins the network, it randomly choosesa point in the identifier space and contacts the peer currently responsible for that point. The contacted peersplits the entire space for which it is responsible into twopieces and transfers responsibility of half to the newpeer. The new peer also contacts all of the neighbors toupdate their routing entries. To increase the robustness ofthis algorithm, the entire identifier space can be replicated to create two or more realities. In each reality, eachpeer is responsible for a different set of information.Therefore, if a document cannot be found in one reality,a peer can use the routing information for a second reality to find the desired information.Tapestry and Pastry are very similar and are based on theidea of a Plaxton mesh. Identifiers are assigned based ona hash on the IP address of each peer. When a peer joinsthe network, it contacts a gateway peer and routes towardthe peer in the network with the ID that most closelymatches the its own ID. Routing state for the new peer isbuilt by copying the routing state of the peers along thepath toward the new peers location. For a given peer n,its routing table will contain i levels where the ith levelcontains references to b nodes where b is the base of theidentifier that have identifiers that match n in the last ipositions. Routing is based on a longest suffix protocolthat selects the next hop to be the peer that has a suffixthat matches the desired location in the greatest numberof positions. Robustness in this protocol relies on the factthat at each hop, multiple nodes, and hence multiplepaths, may be traversed.4 CHARACTERISTICSThis section addresses issues in P2P technology decentralization, scalability, anonymity, selforganization,cost of ownership, adhoc connectivity, performance, security, transparency, usability, faultresilience, and interoperability. These issues have a major impact on theeffectiveness and deployment of P2P systems and applications. We compare them in the summary, Section 4.12.4.1 DecentralizationP2P models question the wisdom of storing and processing data only on centralized servers and accessing thecontent via requestresponse protocols. In traditional clientserver models, the information is concentrated incentrally located servers and distributed through networks to client computers that act primarily as user interface devices. Such centralized systems are ideal for someapplications and tasks. For example, access rights and security are more easily managed in centralized systems.However, the topology of the centralized systems ineviP2P SystemAlgorithm Comparison CriteriaModel Parameters Hops tolocate data Routing statePeer joinsand leaves ReliabilityNapstercentralized metadata indexlocation inquiry from central serverdownload directly from peersnone constant constant constantCentral server returns multiple download locations, client can retryGnutellaBroadcast request to as many peersas possible, download directlynone no guaranteeconstantapprox 37constant receive multiple replies from peers withavailable data, requester can retryChord unidimensional, circular ID space N  number of peers in network replicate data on multiple consecutivepeers, app retries on failureCAN multidimensional ID space N  number of peers in networkd  number of dimensionsmultiple peers responsible for each dataitem, app retries on failureTapestry Plaxtonstyle global mesh N  number of peers in networkb  base of the chosen identifierreplicate data across multiple peers,keep track of multiple paths to each peerPastry Plaxtonstyle global mesh N  number of peers in networkb  base of the chosen identifierreplicate data across multiple peers,keep track of multiple paths to each peerTable 3. Comparison of Different P2P Location Algorithms.Nlog Nlog Nlog 2d N1 d 2 d 2 dNblog Nblog NlogNblog b Nblog b NlogNlog2i 113tably yields inefficiencies, bottlenecks, and wasted resources. Furthermore, although hardware performanceand cost have improved, centralized repositories are expensive to set up and hard to maintain. They require human intelligence to build, and to keep the informationthey contain relevant and current.One of the more powerful ideas of decentralization is theemphasis on the users ownership and control of data andresources. In a fully decentralized system, every peer isan equal participant. This makes the implementation ofthe P2P models difficult in practice because there is nocentralized server with a global view of all the peers inthe network or the files they provide. This is the reasonwhy many P2P file systems are built as hybrid approaches as in the case of Napster, where there is a centralizeddirectory of the files but the nodes download files directly from their peers.In fully decentralized file systems, such as Freenet andGnutella, just finding the network becomes difficult. InGnutella, for example, new nodes must know the addressof another Gnutella node or use a host list with known IPaddresses of other peers. The node joins the network ofpeers by establishing a connection with at least one peercurrently in the network. Then, it can begin discoveringother peers and cache their IP addresses locally.One way to categorize the autonomy of a P2P system isthrough the pure P2P versus hybrid P2P distinction.A more precise decomposition may be as presented inFigure 14. This categorization has a direct effect on theselforganization and scalability of a system, as the purest systems are loosely coupled to any infrastructure.4.2 ScalabilityAn immediate benefit of decentralization is improvedscalability. Scalability is limited by factors such as theamount of centralized operations e.g, synchronizationand coordination that needs to be performed, the amountof state that needs to be maintained, the inherent parallelism an application exhibits, and the programming modelthat is used to represent the computation.Napster attacked the scalability problem by having thepeers directly download music files from the peers thatpossess the requested document. As a result, Napster wasable to scale up to over 6 million users at the peak of itsservice. In contrast, SETIhome SETIhome 2001focuses on a task that is embarrassingly parallel. It harnesses the computer power that is available over the Internet to analyze data collected from its telescopes withthe goal of searching for extraterrestrial life forms. SETIhome has close to 3.5 million users so far. Systemslike Avaki address scalability by providing a distributedobject model.Achieving good scalability should not be at the expenseof other desirable features, such as determinism and performance guarantees. To address this problem, hybridP2P systems, such as Napster, intentionally keep someamount of the operations and files centralized.Early P2P systems such Gnutella Gnutella 2001 andFreenet Clark 2001 are adhoc in nature. A peer has toblindly send its requests to many other peers, causingthe rest of the peers to search for the requested document.This can cause the time to retrieve a document to be unbounded. In addition, searching may fail even when anobject exists, making the behavior of the system nondeterministic.Recent P2P systems, represented by CAN, Chord,Oceanstore, and PAST, dictate a consistent mapping between an object key and hosting node. Therefore, an object can always be retrieved as long as the hosting nodescan be reached. Nodes in these systems compose anoverlay network. Each node only maintains informationabout a small number of other nodes in the system. Thislimits the amount of state that needs to be maintained,and hence increases scalability. The logical topology ofthe overlay provides some guarantees on the lookup cost.These systems are designed to scale to billions of users,millions of servers and over 1014 files.In the future, as the bandwidth and computation powercontinue to grow, platforms will be able to take advantage of this power, which should become interesting tomore applications. The net effect is that these architectures will enable more automated scaling, as much resources can be provided, the applications could scale.Figure 14 Examples of Levels of Decentralization in Various P2P Systems.decentralizationdegree ofpurehybridall nodes the samededicated serversdedicated serversuperpeersmastersthrough masterFreenetKazaaNapsterSETIhomeGnutellaDirect ConnectAvakifilesharingdistrib.comp.collabor.communic.platformsJXTA.NET144.3 AnonymityAn important goal of anonymity is to allow people to usesystems without concern for legal or other ramifications.A further goal is to guarantee that censorship of digitalcontent is not possible. The authors of the Free HavenDingledine 2000 have identified the following forms ofanonymity Author A documents author or creator cannot beidentified Publisher The person who published the document tothe system cannot be identified Reader People who read or otherwise consume datacannot be identified Server Servers containing a document cannot be identified based on the document Document Servers do not know what documents theyare storing Query A server cannot tell what document it is usingto respond to a users queryRegardless of the forms of anonymity, enforcing themtypically involves enforcing three different kinds of anonymity between a communicating pair sender anonymity, which hides the senders identity receiver anonymity,which hides a receivers identity and mutual anonymity,which hides the identities of the sender and receiver arehidden from each other and other peers Pfitzmann1987.Besides the kinds of anonymity, it is also very importantto understand the degree of anonymity a certain technique can achieve. Reiter and Rubin Reiter 1998 presented a spectrum of anonymity degrees that coverabsolute privacy, beyond suspicion, probable innocence,and provably exposed. For example, beyond suspicionmeans that even though an attacker can see evidence of asent message., the sender appears no more likely to bethe originator of that message than any other potentialsender in the system.There are six popular techniques, each suitable for enforcing different kinds of anonymity and with differentkinds of constraints. We summarize them below.Multicasting. Multicasting or broadcasting can beused to enforce receiver anonymity Pfitzmann 1987. Amulticast group is formed for parties who wish to keepanonymous. An entity that is interested in obtaining adocument subscribes to the multicast group. The partythat possesses the document sends the document to thegroup. The identity of the requestor is effectively hiddenfrom both the sender and other members of the group,and the requestors anonymity is beyond suspicion. Thistechnique can take advantage of the underlying networkthat supports multicast e.g., Ethernet or token ring.Spoofing the senders address. For connectionless protocols such as UDP, the anonymity of the sender of amessage can be enforced by spoofing the senders IP address. This however, requires changing the protocol. Inaddition, this is not always feasible, because most ISPsnow filer packets originating from invalid IP addresses.Identity Spoofing Besides changing the originatorsaddress, anonymity can also be ensured by changing theidentity of a communicating party. For example, inFreenet Clark 2001, a peer passing a file to a requestor,either out of its own cache or from an upstream peer, canclaim to be the owner of the content. The responder ispossibly innocent, from an attackerss point view, because there is a nontrivial probability that the real responder is someone else.Covert paths. Instead of communicating directly, twoparties communicate through some middle nodes. Mostexisting techniques ensure only sender anonymity. Aparty that wishes to hide its identity prepares a covertpath with the other party as the end of the path. Examplesinclude Mix Chaum 1981, Onion Syverson 1997, Anonymizing Proxy Gabber 1997, Crowds Reiter 1998and Herdes Shields 2000. The covert paths can usestoreforward or persistent connection. By varying thelength of the covert paths and changing the selectedpaths with different frequency, different degrees of anonymity can be achieved.Intractable aliases. LPWA Gabber 1999 is a proxyserver that generates consistent untraceable aliases forclients from the servers. The client can open an accountand be recognized upon returning to the opened account,while hiding the true identity of the client from the server. Techniques of this kind ensure sender anonymity andrely on a trusted proxy server. The degree of anonymitythat can be achieved falls in between absolute privacyand beyond suspicion.Nonvoluntary placement. An interesting new approach is anonymity via nonvoluntary placement of adocument on a hosting node. Here, a publisher forces adocument onto a hosting node using, for example, consistent hashing. Because the placement is nonvoluntary,the host cannot be held accountable for owning the document.15We now summarize the forms of anonymity some of thepopular P2P systems support and the techniques theyemploy see Table 4.Gnutella Gnutella 2001 and Freenet Clark 2001 provide a certain degree of anonymity in the way peers requestsend a document. In Gnutella, a request isbroadcast and rebroadcast until it reaches a peer with thecontent. In Freenet, a request is sent and forwarded to apeer that is most likely to have the content. The reply issent back along the same path.APFS Scarlata 2001 addresses the mutual anonymityproblem assuming that trusted centralized support doesnot exist. Peers may inform a untrusted coordinatorabout their willingness to be index servers. Both the initiator and the responder need to prepare their own covertpaths.Free Haven Dingledine 2000 and Publius Waldman2000 are designed to defend against censorship. Theystrengthen the documents anonymity by further splittingthe files as they are stored at each server. In this way, nosingle server even contains all of the data needed to perform an attack on the encrypted file contents. The anonymity among a pair of communicating partiespublisherserver, readerserver is enforced via covertpaths. Both Free Haven and Publius build the covertpaths using an anonymous remailer system. Publiuscould be extended to support reader anonymity by usingits remailer for publishing, but it does not currently doso.PAST Rowstron 2001, CAN Ratnasamy 2001 andChord Stoica 2001 represent a new class of P2P systemthat provides a reliable infrastructure. One commonproperty among these systems is that object placementcan be entirely nonvoluntary. As a result, when an object is placed on a node, that node cannot be held accountable for owning that object. The embedded routingmechanisms in these systems can also easily be adaptedto covert path for mutual anonymity.4.4 SelfOrganizationIn cybernetics, selforganization is defined as a processwhere the organization constraint, redundancy of a system spontaneously increases, i.e., without this increasebeing controlled by the environment or an encompassingor otherwise external system Heylighen 1997.In P2P systems, selforganization is needed because ofscalability, fault resilience, intermittent connection of resources, and the cost of ownership. P2P systems canscale unpredictably in terms of the number of systems,number of users, and the load. It is very hard to predictany one of them, requiring frequent reconfiguration ofcentralized system. The significant level of scale resultsin an increased probability of failures, which requiresselfmaintenance and selfrepair of the systems. Similarreasoning applies to intermittent disconnection it is hardfor any predefined configuration to remain intact over along period of time. Adaptation is required to handle thechanges caused by peers connecting and disconnectingfrom the P2P systems. Finally, because it would be costly to have dedicated equipment andor people for managing such a fluctuating environment, the management isdistributed among the peers.There are a number of academic systems and productsthat address selforganization. In OceanStore, selforganization is applied to location and routing infrastructureKubiatowicz et al 2000, Rhea et al. 2001, Zhao et al.2001. Because of intermittent peer availability, as wellas variances in network latency and bandwidth, the infrastructure is continuously adapting its routing and location support.ProjectTypes and Techniques of AnonymityPublisher Reader Server DocumentGnutellamulticasting,covert pathsNA NA NAFreenetcovert path,identity spoofingcovert pathsnonvoluntaryplacementencryptionAPFS covert paths covert paths NA NAFreeHavencovert paths remailer covert paths broadcastencryptionsplit filesinto sharesPublius covert paths remailerNA nonvoluntaryplacementencryption split keyPAST NA NAnonvoluntaryplacementencryptionTable 4. Types of Anonymity and Techniques to Enforce Them16In Pastry, selforganization is handled through protocolsfor node arrivals and departures based on a faulttolerantoverlay network Druschel and Rowstron 2001, Rowstron and Druschel 2001. Client requests are guaranteedto be routed in less than steps on average. Also,file replicas are distributed and storage is randomizes forload balancing.The FastTrack product attributes quicker search anddownload to selforganizing distributed networks. Inthese networks, more powerful computers automaticallybecome SuperNodes and act as search hubs. Any clientcan become a SuperNode if it meets processing, and networking criteria bandwidth and latency. The distributed networks replace any centralized service FastTrack2001.SearchLing uses selforganization to adapt its networkaccording to the type of search, resulting in reduced network traffic and less unreachable informationSearchLing 2001.4.5 Cost of OwnershipOne of the premises of P2P computing is shared ownership. Shared ownership reduces the cost of owning thesystems and the content, and the cost of maintainingthem. This is applicable to all classes of P2P systems. Itis probably most obvious in distributed computing. Forexample, SETIhome is faster than the fastest supercomputer in the world, yet at only a fraction of its cost 1 Anderson 2000.The whole concept of Napster music sharing was basedon each member contributing to the pool of music files.Similar assumptions for peers are used in other file systems, such as OceanStore.In P2P collaboration and communication systems, and inplatforms, elimination of centralized computers for storing information also provides reduced ownership andmaintenance costs. A similar approach is taken in wireless communication in the United States. A socalledParasitic grid wireless movement, enables sharing ofthe existing homeinstalled 802.11b bandwidth amongthe users Schwarz 2001. These networks compete withthe companies installing wireless infrastructure at thefraction of the cost.4.6 AdHoc ConnectivityThe adhoc nature of connectivity has a strong effect onall classes of P2P systems. In distributed computing, theparallelized applications cannot be executed on all systems all of the time some of the systems will be availableall of the time, some will be available part of the time,and some will be not be available at all. P2P systems andapplications in distributed computing need to be aware ofthis adhoc nature and be able to handle systems joiningand withdrawing from the pool of available P2P systems.While in traditional distributed systems, this was an exceptional event, in P2P systems it is considered usual.In content sharing P2P systems and applications, usersexpect to be able to access content intermittently, subjectto the connectivity of the content providers. In systemswith higher guarantees, such as servicelevel agreements, the adhoc nature is reduced by redundant serviceproviders, but the parts of the providers may still be unavailable.In collaborative P2P systems and applications, the adhoc nature of connectivity is even more evident. Collaborative users are increasingly expected to use mobile devices, making them more connected to Internet andavailable for collaboration. To handle this situation, collaborative systems support transparent delay of communication to disconnected systems. This can beaccomplished by having proxies delegated on networksto receive messages, or by having other sorts of relays onthe sending system or somewhere in the network thatwill temporarily hold communication for an unavailablesystem.Furthermore, not everything will be connected to the Internet. Even under these circumstance, adhoc groups ofpeople should be able to form adhoc networks in orderto collaborate. The supporting adhoc networking infrastructures, such as 802.11b, Bluetooth, and infrared,have only a limited radius of accessibility. Therefore,both P2P systems and applications need to be designed totolerate sudden disconnection and adhoc additions togroups of peers.4.7 PerformancePerformance is a significant concern in P2P systems.P2P systems aim to improve performance by aggregatingdistributed storage capacity e.g., Napster, Gnutella andcomputing cycles e.g., SETIHome of devices spreadacross a network. Because of the decentralized nature ofthese models, performance is influenced by three typesof resources processing, storage, and networking. Inparticular, networking delays can be significant in widearea networks. Bandwidth is a major factor when a largenumber of messages are propagated in the network andlarge amounts of files are being transferred among manypeers. This limits the scalability of the system. Performance in this context does not put emphasis in the millisecond level, but rather tries to answer questions of howN16log17long it takes to retrieve a file or how much bandwidthwill a query consume.In centrally coordinated systems e.g., Napster, SetiHome coordination between peers is controlled andmediated by a central server, although the peers also maylater contact each other directly. This makes these systems vulnerable to the problems facing centralized servers. To overcome the limitations of a centralizedcoordinator, different hybrid P2P architectures Yang,2001 have been proposed to distribute the functionalityof the coordinator in multiple indexing servers that cooperate with each other to satisfy user requests. DNS is another example of a hierarchical P2P system thatimproves performance by defining a tree of coordinators,with each coordinator responsible for a peer group. Communication between peers in different groups is achievedthrough a higher level coordinator.In decentralized coordinated systems such as Gnutellaand Freenet, there is no central coordinator communication is handled individually by each peer. Typically, theyuse message forwarding mechanisms search for information and data. The problem with such systems is that theyend up sending a large number of messages over manyhops from one peer to another. Each hop contributes toan increase in the bandwidth on the communication linksand to the time required to get results for the queries. Thebandwidth for a search query is proportional to the number of messages sent, which in turn is proportional to thenumber of peers that must process the request beforefinding the data.There are three key approaches to optimize performancereplication, caching, and intelligent routing.Replication. Replication puts copies of objectsfilescloser to the requesting peers, thus minimizing the connection distance between the peers requesting and providing the objects. Changes to data objects have to bepropagated to all the object replicas. Oceanstore uses anupdate propagation scheme based on conflict resolutionthat supports a wide range of consistency semantics. Thegeographic distribution of the peers helps to reduce congestion on both the peers and the network. In combination with intelligent routing, replication helps tominimize the distance delay by sending requests to closely located peers. Replication also helps to cope with thedisappearance of peers. Because peers tend to be usermachines rather than dedicated servers, there is no guarantee that the peers wont be disconnected from the network at random.Caching. Caching reduces the path length required tofetch a fileobject and therefore the number of messagesexchanged between the peers. Reducing such transmissions is important because the communication latencybetween the peers is a serious performance bottleneckfacing P2P systems. In Freenet for example, when a fileis found and propagated to the requesting node, the file iscached locally in all the nodes in the return path. Moreefficient caching strategies can be used to cache largeamounts of data infrequently. The goal of caching is tominimize peer access latencies, to maximize querythroughput and to balance the workload in the system.The object replicas can be used for load balancing and latency reduction.Intelligent routing and network organization. To fullyrealize the potential of P2P networks, it is important tounderstand and explore the social interactions betweenthe peers. The most pioneering work in studying the social connections among people is the smallworld phenomenon initiated by Milgram Milgram 1967. Thegoal of his experiment was to find short chains of acquaintances linking pairs of people in the United Stateswho did not know one another. Using booklets of postcards he discovered that Americans in the 1960s were, onaverage, about six acquaintances away from each other.Adamic et al. have explored the powerlaw distributionof the P2P networks, and have introduced local searchstrategies that use highdegree nodes and have costs thatscale sublinearly with the size of the network Adamicet al. 2001. Ramanathan et al Ramanathan, 2001 determine good peers based on interests, and dynamicallymanipulate the connections between peers to guaranteethat peers with a high degree of similar interests are connected closely. Establishing a good set of peers reducesthe number of messages broadcast in the network and thenumber of peers that process a request before a result isfound. A number of academic systems Oceanstore,Pastry, see Section 2.6 improve performance by proactively moving the data in the network. The advantage ofthese approaches is that peers decide whom to contactand when to adddrop a connection based on local information only.4.8 SecurityP2P systems share most of their security needs with common distributed systems trust chains between peers andshared objects, session key exchange schemes, encryption, digital digests, and signatures. Extensive researchhas been done in these areas, and we will not discuss itfurther in the present document. New security requirements appeared with P2P systems.18 Multikey encryption. File sharing systems such asPublius intend to protect a shared object, as well as theanonymity of its author, publishing peer and hostingpeer. The security scheme chosen by Publius developers is based on a Public key, Multiple private keysasymmetric encryption mechanism derived from R.Shamirs shared secrets encryption method Shamir1979. Byzantine attacks by malicious authenticatedusers have typically been an issue for such schemes.Recent improvements see Castro and Liskov 2001for an example have greatly reduced the costs inherent to Byzantine agreements and opened the way tosolid systems used by large numbers of users. Sandboxing. Distributed computing P2P systems require execution of some code on peer machines. It iscrucial to protect the peer machines from potentiallymalicious code and protect the code from a maliciouspeer machine. Protecting a peer machine typically involves enforcing 1 safety properties such that the external code will not crash the host box, or will onlyaccess the host data in a typesafe way, and 2 enforcing security properties to prevent sensitive data frombeing leaked to malicious parties. Techniques to enforce the first include sandboxing, safe languagese.g., java, virtual machines e.g., Internet CPOSIX virtual machine, real mode Linux derivatives,which run a virtual machine on top of the actual OS,VMware, proofcarrying code and certifying compilers Necula 1997, 1998 and program verificationtechniques applied to verifying the safety properties ofmachine code Xu 2000, 2001. Techniques to checkthe latter include information flow theory Denning1976, and model checking Ball 2001. Digital Rights Management. P2P file sharing makesfile copying easy. It is necessary to be able to protectthe authors from having their intellectual property stolen. One way to handle this problem is to add a signature in the file that makes it recognizable the signatureremains attached to the file contents although the filecontents do not seem affected. This technique, referenced as watermarking or steganography Katzenbeisser 1999, has been experimented with by RIAA toprotect audio files such as MP3s, hiding the Copyrightinformation in the file in inaudible ways. Reputation and Accountability. We already spokeabout trust, which is the way we will mathematicallyensure that a communiquee is actually the entity itclaims it is. In P2P systems, reputation is built on topof trust, and requires ways to measure how good oruseful a peer is. For instance, if a given user shareslots of interesting files, its reputation should be high.Freeloader is a common term for a user who downloads files from P2P systems without offering files toothers. A freeloader usually has a low reputation. Toprevent this kind of noncooperative behavior, someaccountability mechanisms need to be devised. Current systems often rely on crossratings, but because itis based on a community of authenticated but untrusted users, it is difficult to produce a solid system. Firewalls. P2P applications inherently require directconnections between peers. However, in corporate environments internal networks get isolated from the external network the Internet, leaving reduced accessrights to applications. For instance, most firewallsblock inbound TCP connections Peertopeer Working Group. 2001. This means that a machine within aFirewall will not be accessible from a machine external to the network. Worse, home users frequently useIP Masquerading or Network Address TranslationNAT technology to share an internet connection between several machines, which leads to the same inaccessibility problem. However, as outbound accessthrough port 80 HTTP is often allowed by firewalls,some mechanisms have been devised that enable connections between hidden machines behind a firewallor NAT, inaccessible from the Internet and Internetmachines. This is quite limiting however, as it requiresconnection to be initiated from the hidden machine.When both peers who want to communicate reside behind different firewalls, the problem becomes harder.It requires a central reflector or relay server on the Internet, which provides a connection between the hidden peers.4.9 Transparency and UsabilityIn distributed systems, transparency was traditionally associated with the ability to transparently connect distributed systems into a seamlessly local system. The primaryform of transparency was location transparency, but other forms included transparency of access, concurrency,replication, failure, mobility, scaling, etc.Coulouris1990. Over time, some of the transparencieswere further qualified, such as transparency for failure,by requiring distributed applications to be aware of failures Waldo et al 1997, and addressing transparency onthe Internet and Web see next paragraph.From its beginning, the Internet paid particular attentionto transparency at the protocol level TCPIP, so calledendtoend address transparency Carpenter 2000. Theendtoend argument Saltzer et al. 1984 claims that certain functions in a communication between two entitiescan be performed only with the knowledge and statemaintained at these entities at the application levelhence, endtoend all the way from application throughthe communication stack, up to the other application.19This implies that application communication state is notmaintained in the network, but rather at the communication end points.This also implies that any point in the network knows thename and address of the other communication point, anassumption that is not true any more. IPv4s lack of thedomain names and IP numbers, as well as the introduction of intranets and mobile users, resulted in IP numbersthat are valid only during a single session. Examples include SLIP and PPP, VPNs, use of firewalls, DHCP, private addresses, network address translators NATs, splitDNS, load sharing optimizations, etc Carpenter 2000.This had significant implications for P2P and was alsoone of the reasons for the introduction of P2P. Because itwas no longer possible to rely on DNS to provide an accurate name, P2P systems came up with different namingand discovery schemes see Section 3.1 as well as end ofSection 5.3.Web naming did not necessarily offer full naming transparency. URLs are widely used instead of URNs, whichwere supposed to enable naming transparency BernersLee et al. 1998. Beside namingaddressing transparency in P2P there is also a requirement for administrationtransparency. Users are typically nonexperts and theydo not or cannot administer their software and devices.The P2P software should not require any significant setup or configuration of either networks or devices in orderto be able to run. Also, selfupdating software is a desirable feature. In addition, P2P systems should be networkand device transparent independent. They should workon the Internet, intranets, and private networks, usinghighspeed or dialup links. They should also be devicetransparent, which means they should work on a varietyof devices, such as handheld personal digital assistantsPDAs, desktops, cell phones, and tablets.Another form of transparency is related to security andmobility. Automatic and transparent authentication ofusers and delegation to user proxies can significantlysimplify users actions. Supporting mobile users and disconnection in particular, can enable users to work independently of whether and how they are connected to theInternet or intranets.A user can use P2P applications in the following manners as a user of services, typically through Web interfacese.g., content sharing, information gathering wrapped around nonP2P applications, typically on aP2P platform e.g., Groove, .NET as locally installed P2P software e.g., distributedcomputing screensavers and Napster4.10 Fault ResilienceOne of the primary design goals of a P2P system is toavoid a central point of failure. Although most P2P systems pure P2P already do this, they nevertheless arefaced with failures commonly associated with systemsspanning multiple hosts and networks disconnectionsunreachability, partitions, and node failures. These failures may be more pronounced in some networks e.g.,wireless than others e.g., wired enterprise networks. Itwould be desirable to continue active collaborationamong the still connected peers in the presence of suchfailures. An example would be an application, such asgenomehome GenomeHOME 2001 executing apartitioned computation among connected peers. Wouldit be possible to continue the computation if one of thepeers were to disappear because of a network link failure If the disconnected peer were to reappear, could thecompleted results generated during the standalonephase be integrated into the ongoing computationQuestions similar to these would have to be addressed byP2P systems aiming to provide more than just best effort Internet service.In the past, clientserver disconnection has been studiedfor distributed file systems that consider mobile clientse.g., Coda Satyanarayanan 1990, and a common solution is to have applicationspecific resolvers to handleany inconsistency on reconnection. Some current P2Psystems e.g., Groove Groove 2001 handle this by providing special nodes, called relays, that store any updatesor communication temporarily until the destination inthis case another peer reappears on the network. Otherse.g., Magi Magi 2001 queue messages at the source,until the presence of the destination peer is detected.Another problem related to disconnection is nonavailability of resources. This may occur either because theresource is unreachable because of a network failure orbecause the peer hosting the resource has crashedgoneoffline. While the former may be resolved by routingaround the failure and is already supported by the Internet, the latter requires more careful consideration. Replication of crucial resources helps alleviate the problem.P2P networks such as Napster and Gnutella representsystems having both a passive and an uncontrolled replication mechanism based solely on the files popularity.Depending on the application running over these networks, it may be necessary to provide certain persistenceguarantees. This requires a more active and reliable replication policy.Anonymous publishing systems such as Freenet Clarket al. 2000 and Publius Waldman 2000 ensure avail20ability by controlled replication. Oceanstore Kubiatowicz 2000 maintains a twolayered hierarchy of replicasand through monitoring of administrative domainsavoids sending replicas to locations with highly correlated probability of failures. However, because a resourcein the P2P system could be more than a just a file  suchas a proxy to the Internet, shared storage space, or sharedcomputing power  the concepts of replicated file systems have to be extended to additional types of resources. Grid computing solutions e.g. Legion provideresilience against node failures by restarting computations on different nodes.A challenging aspect of P2P systems is that the systemmaintenance responsibility is completely distributed andneeds to be addressed by each peer to ensure availability.This is quite different from clientserver systems, whereavailability is a serverside responsibility.4.11 InteroperabilityAlthough many P2P systems already exist there is still nosupport to enable these P2P systems to interoperate.Some of the requirements for interoperability include How do systems determine that they can ineroperate How do systems communicate, e.g., what protocolshould be used, such as sockets, messages, or HTTP How do systems exchange requests and data, and execute tasks at the higher level, e.g., do they exchangefiles or search for data How do systems determine if they are compatible atthe higher protocol levels, e.g., can one system rely onanother to properly search for a piece of information How do systems advertise and maintain the same levelof security, QoS, and reliabilityIn the past, there were different ways to approach interoperability, such as standards IEEE, e.g., IEEE standards for ethernet, token ring, and wireless commonspecifications, e.g., Object Management Group OMG,2001 common source code, e.g., OSF DCE Rosenberry, 1992 opensource e.g., Linux and de factostandards e.g., Windows or Java.In the P2P space, some efforts have been made towardsimproved interoperability, even though interoperabilityis still not supported. The P2P Working Group p2pwg,2001 is an attempt to gather the community of P2P developers together and establish common ground by writing reports and white papers that would enable commonunderstanding among P2P developers. The P2P WorkingGroup gathers developers from both adhoc communication systems and grid systems. The Grid Forum is a similar effort in the grid computing space. Both effortsrepresent an approach similar to OMG, in defining specifications and possibly reference implementations.The JXTA effort JXTA, 2001 approaches interoperability as an opensource effort, by attempting to imposea de facto standard. A number of developers are invitedto contribute to the common source tree with differentpieces of functionality. Only a minimal underlying architecture is supported as a base, enabling other systems tocontribute parts that may be compatible with their ownimplementations. A number of existing P2P systemshave already been ported to the JXTA base. JXTA is described in more detail in Section 6.7.4.12 SummaryDecentralization is a key feature of P2P systems. It affects how developers design systems and applications,by influencing algorithms, data structures, security, scalability, and availability assumptions. It affects how userscan be connected to a system and the people with whomthey can interact. For example, in games, users perceivethat other players are remote, and that they can also bedisconnected. This implies that they should devise strategies in a decentralized fashion. Distributed P2P applications are written assuming decentralization, andcollaborative applications have to handle group management without central naming, authorization, and data repositories.The adhoc nature of P2P systems also affects the wayapplications and systems are conceived. The fact that anysystem or user can disappear at time drives the design ofthese systems as well as user perceptions and expectations. In addition to the classical security issues of traditional distributed systems, P2P is distinguished by theimportance of anonymity in certain applications andmarkets. Scalability, performance, fault resilience, andinteroperability have similar importance for P2P as theyhave in traditional distributed systems.Distributed computing applications are primarily concerned with scalability which is derived from decentralization and performance. Fault resilience is tied in withthe adhoc nature of connectivity  distributed application designers and users need to account for the possibility of a system going down at any time and being able toresume from a previous checkpoint. P2P systems that usecritical or confidential data are concerned with security.Content sharing applications and systems are primarilyconcerned with the availability of data. Enterprise systems are concerned with security and performance, andpublic and consumer systems are concerned with transparency ease of use and anonymity. Collaborative and21communication applications are concerned with connectivity adhoc nature, security, and anonymity, and withinteroperability between systems.5 P2P SYSTEMSThis section describes in more detail the four categoriespresented in Figure 6 distributed computing, file sharing, collaborative systems, and P2P platforms. In addition, we also present historical P2P systems that predatedthe recent notion of P2P systems. While this section presents the P2P systems categories in general, Section 6presents each of the categories in more detail with twocase studies. This section and Section 6 are used as thebasis for a comparison of systems in Appendix A, andspecifically for Table 6.5.1 HistoricalIn most cases, early distributed applications were P2P.When most users were from a technical or academicbackground, and were using either timesharing systemsor engineering workstations, P2P applications seemedthe most natural approach. It was the late80s and early90s when clientserver architectures became more prevalent because they provided the fastest and most costeffective means of supporting large numbers of nontechnical users. It also allowed the use of less expensiveand less functional computers such as desktop PCs.While most early distributed applications can be considered P2P, email systems built on the Simple Mail Transfer Protocol SMTP and Usenet News were probably themost widely used. In each case, local servers that received a message built connections with peer servers todeliver messages into a users mail file or into a spool filecontaining messages for the newsgroup. The File Transfer Protocol FTP was the precursor to todays file sharing P2P systems. While FTP is a clientserverapplication, it was very common for individuals to runFTP servers on their workstations to provide files to theirpeers. Eventually, an indexing system, Archie, was developed to provide a central search mechanism for fileson FTP servers. This structure with central search anddistributed files is exactly replicated in the Napster P2Psystem.Prior to the establishment of a continuously connectednetwork such as the Internet, decentralized dialup networks were widely used. The most notable examples include UUNet and Fidonet. These networks werecomposed of a collection of machines that made periodicdialup connections to one another. On a typical connection, messages again, typically email or discussiongroup entries were transferred bidirectionally. Often, amessage would be routed through multiple dialup hopsto reach its destination. This multihop message routingapproach can be seen in current P2P systems such asGnutella.In our modern era dominated by PCs on workplaceLANs or home dialup connections, the first widen use ofP2P seems to have been in instant messaging systemssuch as AOL Instant Messenger. These are typically hybrid P2P solutions with discovery and brokering handledby a central server followed by direct communication between the peer messaging systems on the PCs. The current phase of interest and activity in P2P was driven bythe introduction of Napster Napster 2001 in 1999. Itcame at a time when computers and their network connections were nearing the level found previously in technical and academic environments, and recreated earlierapproaches with an interface more suitable to a nontechnical audience.5.2 Distributed ComputingDistributed computing is very successfully used by P2Psystems. The idea of using spare computing resourceshas been addressed for some time. The Beowulf projectfrom NASA Becker et al. 1995 was a major milestonethat showed that high performance can be obtained byusing a number of standard machines. Other efforts suchas MOSIX Barak and Litman 1985, Barak and Wheeler1989 and Condor Litzkow et al 1988, Litzkow and Solomon 1992 also addressed distributed computing in acommunity of machines, focusing on the delegation ormigration of computing tasks from machine to machine.Grid computing is another concept that was first explored in the 1995 IWAY experiment I. Foster, inwhich highspeed networks were used to connect highend resources at 17 sites across North America. Out ofthis activity grew a number of Grid research projects thatdeveloped the core technologies for production Gridsin various communities and scientific disciplines. Gridtechnology efforts are now focused around the GlobalGrid forum httpwww.globalgridforum.org and theGlobus project httpwww.globus.org. A computinggrid can be seen and used as a single, transparent computer. A user logs in, starts jobs, moves files, and receives results in a standard way.Derivatives of Grid Computing based on collaboration ofstandard Internetconnected PCs began to appear in thelate 90s. In this document, we will use DistributedComputing terminology to describe them.22Distributed Computing achieves processing scalabilityby aggregating the resources of large number of individual Internet PCs. Typically, distributed computing requires applications that are run in a proprietary way by acentral controller. Such applications are usually targetingmassive multiparameters systems, with long runningjobs months or years using P2P foundations. One of thefirst widely visible distributed computing events occurred in January 1999, where distributed.net, with thehelp of several tens of thousands of Internet computers,broke the RSA challenge DESIII in less than 24 hoursusing a distributed computing approach. This made people realize how much power can be available from idleInternet PCs.More recent projects have been raising interest frommany users within the Internet community. For example,SETIhome SETIhome 2001 now has a consolidated power of about 25 Tflops Thousands of Billions offloating point operation per second, collected from morethan three million registered user machines.PeertoPeer A common misconception is that distributed computing systems such as SETIhome are notP2P systems The argument is that central server is required for controlling the offered PC resources, the PCsdo not operate as servers, and no communication occursbetween peers PCs. However, a very significant part ofthe system is executed on the PCs, with high autonomy.Hence, we consider distributed computing systems to beP2P systems.How it works. The computational problem to be solvedis split into small independent parts. The processing ofeach of the parts using a forkandjoin mechanism isdone on an individual PC and the results are collected ona central server. This central server is responsible for distributing job items to PCs on the Internet. Each of theregistered PCs is equipped with client software. The client software takes advantage of inactivity periods oftencharacterized by screensaver activation times in the PCto perform some computation requested by the server.After the computation is finished, the result is sent backto the server, and a new job is allocated to the client.Current usage. One of the major limitations of DesktopComputing is that it requires jobs that can be split into independent small parts that do not require crosspeer communication. Internet latencies do not allow demandingcommunication patterns such as those found in typicalcluster computing. Hence, it is not possible to executesupercomputinglike processing such as linear algebraproblems matrix computation. Current applicationsconsist of Single Process Multiple Data SPMD problems and multiprogramming problems where a given jobhas to be run on many different input data sets. Thismostly includes simulation and model validation tasks.Because specific applications have to be developed usingvery specific constraints, paradigms, and environments,their development cost is prohibitive to most users, andthe scope remains limited to highly visible research domains, such as the human genome project, alien seeking,cancer research, and weather model validation.Application area examples  Financial and Biotechnology. Financial and biotechnology applications aresuitable for distributed computing. Financial institutions,such as banks and credit companies, are executing complex simulations for market calculations. Applicationsinclude portfolio pricing, risk hedge calculation, marketand credit evaluation, counterparty netting, margin calculations, cash flow, and demographic analysis Intel2001. In the past, financial applications were typicallyrun during the night. As they become more realtime innature, these requirements will grow even further. So faronly big institutions have been able to afford the computing power to automate these simulations, typically usingmainframe computers or very powerful workstations. Byrelying on P2P commodity platforms, smaller banks canalso benefit from these applications. Furthermore, astechnology favors farms of desktop computers, they become not only more cost effective, but also a more powerful platform. As an example, Intel and DataSynapseclaim speedups from 15 hours to 30 minutes in the caseFigure 15 Distributed computing over the Web23of interest rate swap modeling when moving to a P2P solution DataSynapse 2001. The biggest challenge in using P2P for financial markets is the intrinsic requirementfor security. Because most of these applications are executing behind the firewall, security requirements aresomewhat relaxed. In particular, parts of the applicationscan even be executed outside of the firewall.In the biotechnology sector, the need for advanced computing techniques is being driven by the availability ofcolossal amounts of data. For instance, genomic researchhas close to three billion sequences in the human genomedatabase. Applying statistical inference techniques todata of this magnitude requires unprecedented computational power. Traditionally, scientists have used highperformance clustering HPC and supercomputing solutions, and have been forced to employ approximatingtechniques in order to complete studies in an acceptableamount of time. By harnessing idle computing cycles9598 unused from general purpose machines onthe network, and grouping multisite resources, gridcomputing makes more computing power available to researchers. Grid solutions partition the problem spaceamong the aggregated resources to speed up completiontimes. Companies such as Platform Computing LSFPlatform Computing 2001, Entropia Entropia, AvakiAvaki 2001 and Grid Computing Bioinformatics GCB2001 offer complete HPC and grid solutions to biological research organizations and pharmaceutical researchand development. Genomics and proteomics projectssuch as Genomehome and Foldinghome managedby groups at Stanford make use of the idle cycles of registered clients to compute parts of the complex genomesequencing and protein folding problems.In search for a business model. Distributed computingrequires an expensive infrastructure to host the Web service, and this does not even account for development andmaintenance costs. Attempts have been made by severalcompanies such as Porivo to pay the user for use of hismachine by the system. If your machine computes a lotof jobs, you can get some money for having shared yourmachine. Other models are based on auctions, where thecheapest available CPU is allocated for a given job. Another model used by Entropia addresses vertical markets,such as Genomics, which is based on huge database lookups. However, none of the existing business models haveyet proven successful. The only model that currentlyseems to work is the dedicated community model, suchas astronomy researchers sharing their machines in afriendly way.5.3 File SharingContent storage and exchange is one of the areas whereP2P technology has been most successful. Multimediacontent, for instance, inherently requires large files. Napster and Gnutella have been used by Internet users to circumvent bandwidth limitations that make large filetransfers unacceptable with classic mechanisms. Distributed storage systems based on P2P technologies are taking advantage of the existing infrastructure to offer thefollowing features. File exchange areas. Systems such as Freenet providethe user with a potentially unlimited storage area bytaking advantage of redundancy. A given file is storedon some nodes in the P2P community, but it is madeavailable to any of the peers. A peer requesting a givenfile just has to know a reference to a file, and is able toretrieve the file from the community by submitting thefile reference. Systems such as Freenet, Gnutella, andKazaa fall in this category. Highly available safe storage. The duplication and redundancy policies in some projects offer virtual storage places where critical files get replicated multipletimes, which helps ensuring their availability. OceanStore Kubiatowicz et al. 2000 and Chord Dabek etal. 2000 are examples of such systems. Anonymity. Some P2P systems such as Publius Waldman, Rubin and Cranor references mathematicallyensure that published documents preserve anonymityfor their authors and publishers, while allowing peopleto access the documents. Manageability. P2P systems enable easy and fast retrieval of the data by distributing the data to caches located at the edges of the network. The location of thedata is not known by the retriever, perhaps not even after the data is retrieved. Freenet, for example, storesthe data in many locations in the path between the provider and the retriever, so the whole notion of hostinga file becomes meaningless. Files move freely amongthe peers and are allowed to disappear even if they arebeing downloaded. This has some important implications. For example, the question is who is accountablefor the files see Section 4.5. Also, how can we ensurethat the entire piece of data is being downloaded andcope with the unreliability of the peers see Section4.10.Technical issues. The major technical issues in file sharing systems are mostly the network bandwidth consumption, security, and search capabilities. Three mainmodels exist today, as discussed in Section 3.2 Centralized directory model Napster, the flooded request model Gnutella, and the document routing model24FreeNet. All P2P file sharing systems can be categorized into one of these three families, although variationsdo exist, such as extensions of the models with leaderelection mechanisms automatic or manual electionssuch as in KaZaA, which allow for better scalability ofthe systems and less stress on the network. In addition,current P2P systems e.g., Napster, Gnutella, Freenethave mainly focused on the exchange and sharing ofsmall objects such as files and music clips. However,we expect that in future P2P systems the content will beof any form, including audio, video, software, and documents. To do that, we will need intelligent decisions suchas from where the content be retrieved and over whichnetwork path should the content travel. XDegrees XDegrees 2001, for example, ensures that information is efficiently routed to users and that multiple documentreplicas are synchronized across many peers. They provide an eXtensible Resource Name System XRNSbased on the notion of the URL that creates a locationindependent name space. They place frequently accessedinformation at optimal locations in the network and thenselect the best route to retrieve that information based onsource availability, network topology, and responsetime.Application area example. Napster is first P2P filesharing application that jump started the P2P area. Napster was originally developed to defeat the copying problem and to enable the sharing of music files over theInternet. Napster uses the centralized directory modelsee Section 2.6 to maintain a list of music files, wherethe files are added and removed as individual users connect and disconnect from the system. Users submitsearch requests based on keywords such as title, artist, etc. Although Napsters search mechanism is centralized, the file sharing mechanism is decentralized theactual transfer of files is done directly between the peers.Napsters centralized directory model inevitably yieldsscalability limitations see Section 3.2. For example,your available bandwidth can be tremendously reducedby users downloading songs from your machine. Yet,centralization simplifies the problem of obtaining anamespace and enables the realization of security mechanisms see Section 3.8.Napster has been quite popular. It has had more than forty million client downloads and has led to numerousvariants of filesharing applications such as Gnutellaand Pointera. OpenNap httpopennap.sourceforge.net is an opensource Napster server that extendsthe Napster protocol to allow sharing of any media typeand add the ability to link Napster servers together. Tosolve the problem with copyright violations, Napster relaunched a new Napster membership service to start anew chapter in the music file business. Although it stilloffered the core functions searching, finding, sharing,and discovering digital music through the Napster community it also offered to artists the opportunity to register as rights holders and get paid for sharing their musicon Napster. Napster set the rules for how their music filesare used.Morpheus is a fullfeatured P2P filesharing system introduced by MusicCity www.comusiccity.com thattries to overcome some of the limitations of Napster. Itincludes a powerful search engine that can search for alltypes of media including music, videos, documents, andreference files. The results are grouped together so thesame file is displayed only once. Its SmartStream mechanism automatically resumes broken content streams byfinding another source for the same content and monitoring the network until the whole content stream is downloaded. Morpheus increases the download speed of largefiles through the simultaneous transfer of content frommultiple sources FastStream mechanism. Its encryption mechanisms protect privacy and transmissions, andprevent unauthorized intrusions. Also, it allows contentproviders to deploy thirdparty digital rights management technology to protect the copyrights of their digitalcontent that is distributed through the network.Kazaa is another example of a P2P file sharing systemthat uses SuperNodes as local search hubs. These arepowerful nodes on fast connections that are generatedautomatically. Peers connect to their local SuperNode toupload information about the files they share, and to perform searches in the network. Kazaa uses an intelligentdownload system to improve download speed and reliability. The system automatically finds and downloadsfiles from the fastest connections, failed transfers are automatically resumed, and files are even downloadedfrom several sources simultaneously to speed up thedownload. When files are imported, the system automatically extracts metadata from the contents of the filessuch as ID3 tags for mp3 files. This makes for muchfaster and more accurate searches.Kazaa also uses a technique called MD5 hashing to make sure the contents ofmultisourced files are identical.5.4 CollaborationCollaborative P2P applications aim to allow applicationlevel collaboration between users. The inherently adhocnature of P2P technology makes it a good fit for userlevel collaborative applications. These applications rangefrom instant messaging and chat, to online games, toshared applications that can be used in business, educational, and home environments. Unfortunately, a number25of technical challenges remain to be solved before pureP2P collaborative implementations become viable.Overview. Collaborative applications are generallyeventbased. Peers form a group and begin a given task.The group may include only two peers collaborating directly, or may be a larger group. When a change occursat one peer e.g., that peer initiates sending a new chatmessage, an event is generated and sent to the rest of thegroup. At the application layer, each peers interface isupdated accordingly.Technical Challenges. There are a number of technicalchallenges that make implementation of this type of system difficult. Like other classes of P2P systems, locationof other peers is a challenge for collaborative systems.Many systems, such as Magi, rely on centralized directories that list all peers who are online. To form a newgroup, peers consult the directory and select the peersthey wish to involve. Other systems, like MicrosoftsNetMeeting, can require that peers identify one anotherby IP address. This is much too restrictive, especially inenvironments where groups are large.Fault tolerance is another challenge for collaborativesystems. In shared applications, messages often must bedelivered reliably to ensure that all peers have the sameview of the information. In some cases, message ordering may be important. While many wellknown groupcommunication techniques address these challenges in anonP2P environment, most P2P applications do not require such strict guarantees. The primary solution employed in P2P applications is to queue messages thathave been sent and not delivered i.e., because a givenpeer is down or offline. The messages can then be delivered to the offline peer when it comes back online.Realtime constraints are perhaps the most challengingaspect of collaborative implementations. Users are theultimate end points in a collaborative environment. Assuch, any delay can be immediately perceived by the user. Unfortunately, the bottleneck in this case is not theP2P technology, but the underlying network. Whilemany collaborative applications may work well in a localarea systems, widearea latencies limit P2P applications just as they limit clientserver applications.Consider a gaming environment. The game DOOM is asocalled First Person Shooter FPS game in which multiple players can collaborate or compete in a virtual environment. DOOM uses a P2P structure in which eachplayers machine sends updates of the state of the environment such as the players movement to each of theother machines. Only when all updates have been received does the game update the view. This was marginally viable in localarea, smallscale games, but did notscale to widearea games. Long latencies and unevencomputing power at the various players machines madethis lockstep architecture unusable. All FPS games sinceDOOM have used a more standard clientserver architecture for communication.5.5 PlatformsOperating systems are becoming decreasingly relevantas environments for applications. Middleware solutions,such as Java Virtual Machines, or Web browsers andservers are the dominant environment that is of interestto users as well as to developers of applications. In thatregard, it is likely that future systems will increasinglydepend on some other sort of platform that will be a common denominator for users and services connected to theWeb or in an adhoc network. Examples of such environments include AOL and Yahoo, and .NET is striving toward a similar goal.As described in Section 3.1, platforms, even more sothan other P2P systems, have support for primary P2Pcomponents naming, discovery, communication, security, and resource aggregation. They have an OS dependency even though it is minimal. Most P2P systems areeither running on an opensource OS Linux or they arebased on Windows.There are a number of candidates competing for futureP2P platform. .NET is the most ambitious one, going beyond P2P to encompass all service support on the clientand server side. JXTA is another attempt, taking a bottom up and strong interoperability approach. Most othersystems also have some level of platform support, suchas Groove covering enterprise domain and Magi, covering handheld devices domain. Section 6.7 andSection 6.8 contain detailed descriptions of JXTA and.NET respectively.6 CASE STUDIESIn this section, we compare eight case studies of P2P systems. We selected systems in four different categories,representing the spectrum of different P2P system categories, as well as public domain and proprietary systemssee Table 5.6.1 AvakiAvaki provides a single virtual computer view of a heterogeneous network of computing resources. It is a classic example of metacomputing applied to networksranging from corporate compute and data grids to globalapplication grids of Internet scale.26History. Avaki began as Legion Grimshaw et al. 1994,a research project initiated by Andrew Grimshaw at theUniversity of Virginia in 1993. The vision was to achievea unified view of the computing resources scatteredaround the nation as a single nationwide virtual computer. Passing through various stages of development, andfollowing the first showcase of the Legion technology in1997 at the Super Computing conference, the researchproject emerged as a commercial venture called AppliedMetaComputing, in 1998. In mid 2001, it was relaunched as Avaki Corporation, which currently focuseson providing enterpriselevel distributed computing solutions.Goals. The eventual goal is to knit together the nationscomputing resources into a seamless parallel executionenvironment, allowing faster completion of applications.Current goals include robust security, performance management, and failure detection and recovery features toradically simplify grid administration. Avaki is marketedas a middleware platform for enterpriselevel computing.They are also working with other developing standardsin the field of distributed, pervasive, and P2P computing,such as JXTA, to make Avaki an interoperable platform.Design. The core of Avaki is based on the objectoriented paradigm. Every entity in the system is an individually addressable object with a set of interfaces forinteraction. This approach enables uniformity in systemdesign through inheritance, and containment through encapsulation. Legion was an extension to Mentat Grimshaw 1994, an objectoriented parallel processingsystem. The ability of Mentat to deliver high performance on platforms with very different communicationcharacteristics was the key reason it was chosen to be extended to produce Legion. High performance, exploitation of heterogeneity, scalability, and masking ofcomplexity were key design criteria in the developmentof Avaki.The middleware is structured as a layered virtual machine as shown in Figure 16. The stack has three maincomponents. Core Services. These service provide some of the basicfunctionality required to map the system to a networkof computing resources. The metadata and the directory services enable users and applications to efficiently locate files or computing resources. The protocoladaptors enable interoperability with various networking standards including JXTA  a developing openstandard from Sun. System Management Services. These services allowthe Avaki system to monitor and control the efficiencyof the metasystem. The policy management serviceallows administrators to manage access control for local resources. Application Services. Built over the basic services,these services enable the construction of applicationssuch as collaborative and highperformance computing.Scalability. Avaki is built as a layered virtual machinewith scalable, distributed control. It allows for multipleadministration domains, allowing system administratorscomplete control over local resources and their accesspolicies. To make the file system scalable and to not impose a single file system across all machines, Legiontakes a federated file system approach and builds a unified file system over existing ones.Fault resilience. The scale of the Avaki network enableshigh redundancy of hardware components, but at thesame time detecting and recovering from faults in thislarge system is a challenge. Legion decides to tradeoffP2P system Developer TechnologyAvaki Avakidistributed computingSETIhome public domainGroove Groove NetworkscollaborationMagi Endeavors TechnologiesFreeNet public domaincontent distributionGnutella public domainJXTA public domainplatform.NET MicrosoftTable 5. Case Studies for Eight P2P systemsFigure 16 Avaki Architecture.Job Scheduling, DistributionTCPIP .NETJXTARPCDistributed File SystemMonitoring, Load Balancing Policy managementMetering, Accounting Failover  RecoveryMetadata, Discovery Event NotificationDistributed Directory ServiceSecurityCommunication Protocol AdaptersSystemApplicationCore ServicesManagementServicesServices27extensive fault tolerance support for higher performance.The scope of the support is limited to failstop faults ofhardware components and chooses to stay away fromsoftware or Byzantine faults. However, in the event ofinternal failures during program execution, the user isnotified and can decide on appropriate steps. When ahost goes down, Avaki dynamically reconfigures the gridand automatically migrates failed jobs to different locations. Applications are allowed to specify the level offault tolerance they require, the idea being that theyshould not pay for fault tolerance that is not required.Implementation and performance. Because Avaki isdesigned as a layered virtual machine, the applicationsincur an overhead when compared to native executions.However, Avaki gains significantly by the parallel execution of applications, and trades off the overhead forsignificantly reduced software complexity. Applicationssubmitted to the Legion system are executed in parallelacross a pool of available computing resources. Themain design goal is the reduction of completion time ofthe application. This obviously depends on how parallelizable the application is and the existing data dependencies. Applications that can tolerate latency and are coarsegrained stand to gain from the parallel execution. Resultsfrom executing applications such as CHARMM andcomplib over Legion show an order of magnitude speedup over executions at a single installation.Security. Avaki has builtin security, which eliminatesthe need for any other softwarebased security controls.Its security is lateral and decentralized allowing individual administration domains control over access to localresources. Authentication by the users occurs once during signin, and Avaki manages all further authenticationrequired to run tasks over resources spread across multiple administration domains.Lessons learned. Handling heterogeneity and automaticerror detection and recovery are key to the success ofsystems of this scale. Systems such as Avaki also have tohandle the possibility of a small percentage of the resultsbeing incorrect. One approach to handling this problemis to evaluate the same job at separate locations and verify the consistency of the results.Business model. Avaki is marketing its product to enterprises as a complete data and compute grid. However, itfaces tough competition from established HPC solutionvendors such as Platform Computing. Avaki is currentlybeing evaluated at various research labs.Applications. Apart from enterprise solutions, Avakican provide highend computing power for problems inthe pure science area. Applications that involve parameterspace studies can benefit from the highperformanceparallel execution characteristics of Avaki. Examples include biochemical simulations, complib  a protein andDNA sequence comparator and CHARMM  a pspacestudy of 3D structures.6.2 SETIhomeSETI Search for Extraterrestrial Intelligence is a collection of research projects aimed at discovering aliencivilizations. One of these projects, SETIhome, usesradio emissions received from space and collected by thegiant Arecibo telescope, it analyzes them using the processing power of millions of unused Internet PCs.History. SETIhome is a scientific research projectaimed at building a huge virtual computer based on theaggregation of the computer power offered from internetconnected computers during their idle periods. Theproject has been widely accepted and raised an incredibleraw processing power of several dozens of TFlops frommore than three million internetconnected computers.Goals. The goal is to process a search for extraterrestrialradio emissions from nearby developed intelligent populations based on data collected from the huge radio telescope at Arecibo. The ultimate objective ofSETIhome, realized within the framework of the SETIproject, is to track down and identify signals sent fromintelligent civilizations situated outside our solar system.Design. A major value of SETIhome is that the projectold enough so that the tools have reached a very high level of quality. The project uses two major components thedatabase server and the client. They are provided by several different platforms, such as Windows, Linux, Solaris, and HPUX see Figure 17. The database hasproven to be very scalable more than three million usersregistered and solid. Parts of the serverside code haveFigure 17 SETIhome Architecture. Adapted from Sullivan et al. 1997.radio telescopeSETIhomedata serverdata conversionand storage.25MB every 5days to each clientInternethomePCMacUNIXPCMacUNIXPCMacUNIX.............business school28been made available for analysis. The clientside software is available as a screen saver module althoughstandalone operation is available and is not linked toany thirdparty technology. This means that the clientsoftware has been developed specifically for each supported platform. Although the required development effort was costly, it made it easier to solve specificproblems such as handling security sandboxing of theexecution, encryption of information, digital digests.Fault resilience. Fault resilience is a major value of theclient operation. Because one computing batch may require as much as ten hours to complete, it was necessaryto ensure seamless recovery when the user logs in whichstops the SETIhome client, as well as when the machine is shut down. The SETIhome client resilience relies on a checkpointing mechanism, where the resumabledataset is saved on the hard disk every ten minutes. Thismeans that each time a SETIhome computation is interrupted by the user or a system failure, the computation will resume at the last saved dataset and proceedfrom there. This simple mechanism adds only a smallprocessing overhead and little complexity, and results ina program that is very reliable.Scalability. The SETIhome project strongly relies onits server to distribute jobs to each participating peer andto collect results after processing is done. The horizontalscalability number of users of the system is excellent,with millions of enthusiastic users already participating.A vertical scalability bottleneck of the architecture maybe that a single server is responsible for coordinating allof its operations. However, the huge number of usersshows that the system can handle this load.Lessons learned. The major value of the SETIhomeproject  apart the scientific results  was to prove thatthe technology can be applied to real situations. We canenvision how it could be used to better tap the processingpower from unused computers and identify the peak periods of used computers. SETIhome developers werehoping for approximately 100,000 participants, but theysurpassed this number within a week. So far there havebeen more than 3,000,000 contributors. The last lesson isrelated to security. Most of the problems were not causedby malicious users, but as a consequence of competitiveness. People raised the frequency of their CPUs, makingthem more exposed to failures Wortheimer 2002.Business model. However, it is still not clear how vendors will package this approach into products and services, or what business model they should follow.6.3 GrooveGroove is a collaborative P2P system. It is mainly targeted to Internet and intranet users, although it can also beused on mobile devices, such as PDAs, mobile phones,and tablets. It is intended to enable communication, content sharing, and tools for joint activities Leigh andBenyola 2001.History. Groove was founded in 1997 by Ray Ozzie, thedeveloper of Lotus Notes. The first version of the product was released in the beginning of 2001.Goals. The Grooves main goal, which was P2P by design, was to allow users to communicate directly withother users without relying on a server Suthar and Ozzie2000. Other important goals include security and privacy, and flexibility.Groove users are authenticated, data is secured both ondisk and on the wire, and data confidentiality and integrity are enabled by secure key technology. User data residing on the server is opaque and private data is notshared with third parties. Finally, Groove componentsare signed.Flexibility in Groove is related to both network and development. Groove supports the Internet, intranets, andprivate networks, and allows users to transparently handle disconnection. Groove also supports a number of reusable components.Applications. Groove is intended for communication,collaboration, and content sharing. communication voice over Internet, instant messaging, chat, threaded discussions content sharing shared files, images, and contacts collaboration group calendaring, group drawing andediting, and coWeb browsingXML Object StoreFigure 18 Groove Layers. Adapted from Groove ProductBackgrounder White Paper, httpwww.groove.netpdfbackgrounderproduct.pdf.XML Object RoutingSecurityTool Tool Tool Tool ToolShared Application SpaceStorage NetworkUser AgentTransceiverShared Application Space29Groove presents itself to users or to agents representingusers in the form of shared spaces See Figure 18.Shared spaces offer users benefits, such as spontaneity users act without an administrator, security  sharedspaces act as virtual private networks, context  spacesprovide and maintain the context for users, synchronization  all spaces of the same user are synchronized acrossall users devices, and granularity  parts of documentscan be exchanged Udell, et all 2000.Design. The Groove layer is inserted between the application logic and command execution layers seeFigure 19. This enables the command requests or data tobe stored locally or forwarded to the network, as appropriate. Commands are transformed into XML objects, recorded, and sequenced, which enables them to be playedback and relayed in case of disconnection. Security layers are executed before storing objects to disk or sendingthem out to the network, preventing unauthorized access.Groove supports peerbased authentication and endtoend encryption.Groove supports system and centralized servicesGroove Networks 2001. System services include security, such as automatic handling of publicprivatekey management, encryption, and authentication storage based on the XML object store, enabling disconnection from the network synchronization of multiple local copies of the userspaces peer connection to support transparent administration,such as IP addresses, bandwidth selection, and firewalltranslatorsCentralized services, which leverage centralized management of resources, include licence management which enables viewing all the devices using certain software as well as managing theuse of this software component management, to enable managing Groovecomponents on the fly without requiring user interaction for upgrades relays and transparent crossfirewall interaction tooptimize the communication bandwidth and offlinework, while still enabling direct P2P communication usage reporting to track how shared spaces are used,enabling administrators to optimize resource distribution while respecting user privacyFault Resilience. The only form of fault resilience support is through relay activities, where messages sent tofailed nodes are queued until the nodes are brought up.The same applies to transient failures in the network connectivity.Business model. Groove Networks plans to license itsinfrastructure platform to corporations and thirdpartyintegrators Rice and Mahon 2001. The competitors areprimarily other collaborative P2P systems, such as Endeavors Technologies, but also Microsoft, given the nature of the systems platform that both Groove andMicrosoft advocate. Some of the enablers for adoption ofGroove include elimination of the network administration costs, minimization of dependences on server infrastructure, and availability.6.4 MagiMagi is a P2P infrastructure platform for building secure,crossplatform, collaborative applications. It employesWebbased standards such as HTTP, WebDAV, andXML to enable communication among applicationswithin an enterprise network or over the Internet. MagiEnterprise, their end product, builds over the infrastructure to link office and project teams so they can sharefiles, do instant messaging, and chat.History. Magi evolved from a research project headedby Greg Bolcer at the University of California, Irvine.His team was funded by grants from DARPA, and at thetime, it was believed to be the largest, nonSun Javaproject in the country. Endeavors Technology wasfounded in 1998, and was later bought over by TadpoleTechnology. The first version of Magi, which is theirP2P infrastructure, was released in late 2000. This wasfollowed by their enterprise edition in 2001.User InterfaceApplication LogicCmds as XML ObjsCmd ExecutionData StorageData Modificationrecord, sequence,playbackStore and ForwardFanout, relay serviceXML ObjectRoutingtofrom othernodesFigure 19 Groove Application Structure. Adapted from httpwww.groove.netdeveloperspresentationsarchitecture.exe.security points30Goals. The goal of Magi is to enable information sharingand messaging on any device using popular Webbasedstandards. The architectural framework runs on variousplatforms including PDAs and Pocket PCs. It aims tomake development of XML and Javabased distributedapplications easier by providing event notification to anydevice running an instance of Magi.Design. The key components of the Magi framework include a microApache HTTP server which provides alink to every instance of Magi, a set of core services, anda generic extensible interface. The modular architectureof an Apache server allows easy reconfiguration and theaddition of interfaces only if they are required. The coreservices include Communication Service messaging interface betweenMagi services and the HTTP server Event Service local modules register with the eventservice to receive specific event notifications also enables communication among local modules Buddy Manager maintains a list of active buddies andtheir most recent communication endpoints Access Controller allows restricted access to resources on a Magi peerPlugable service modules such as MagiChat, MagiDAV, and instant messaging, could be built over the coreinfrastructure as shown in Figure 20.Each instance of Magi acts as both a client as well as aserver and hence can participate in P2P communicationwith other Magi servers. Magi supports the followingWeb protocols HTTP1.1 WebDAV for remote authoring of documents and SWAPWfXML for remote process monitoring and control. The modules supportingthese protocols can be loaded when required to achieveinteroperability with applications compliant with theseprotocols. These features reinforce Magis primary design goal to achieve interoperability with existing Webaccess standards.Scalability. Magi relies on a centralized Dynamic DNSas a directory for IP addresses of Magi instances. Because Magi instances can have dynamic IPs, when aMagi instance comes up, it reports its IP to the DDNS.This enables other Magi instances to find the current network endpoints of their buddies. This centralized repository might be a scalability issue with the growingnumber of users on the Magi network. More importantly,it is a single point of failure, which could cause nonavailability of aliasing information. For user authentication, Magi provides a centralized Certificate AuthorityCA, which is another scalability concern and singlepoint of failure.Fault Resilience. Queuing messages that cannot be delivered to buddies who are currently offline enables guaranteed delivery when the destination buddy eventuallycomes online. Buddy crashes are not detected, as there isno mechanism to check the liveliness of each buddy.However, the core Magi interface can be extended as desired to support any kind of fault resilience mechanismamong the buddies. For its centralized services such asdynamic DNS and CA, Magi currently does not provideany fallback schemes in the event of failure.Implementation. The entire infrastructure is in Java andthe Web interface is through servlets. This makes Magias fast as the underlying virtual machine. Because eachMagi instance is both able to receive and send messagesover HTTP, a minimal Web server must be running. Thismay not be the best solution for a Pocket PC or a handheld with limited memory resources. Magi uses Tomcat,which supports modular loading of essential components, coupled with independent serviceoriented designof the infrastructure to target constrained and embeddeddevices. Magis services are accessible through a Webbased interface and is the access mechanism in their implementations for the Compaq iPaq and the HP Jornadapocket PCs.Lessons learned. While most other P2P infrastructuresimplement proprietary interfaces and communicationstandards, Magi emphasizes the use of existing popularstandards. This makes Magi a highly interoperable platform, and its Webbased design makes its deployment ona range of devices easy.Applications. Magi was designed primarily with the paperbased workflow scenario in mind and is targeted atany type of collaborative environment. It supports filesharing and collaborative tools such as chat and messagFigure 20 Magi Architecture.PlugableCore Magi ServicesEvent ServiceMicro ApacheDynamic DNS InternetIntranetHTTP  DAVrequests and eventsextensionmodulesCertificate AuthorityCommunication HTTP ServerServicesMagi Instance HTTP endpointPeer 2Peer 131ing. As a platform, it can be used to embed collaborativeprocesses into enterprisewide applications and B2Bproducts. An SDK has also been released to ease the integration of Magi into applications.6.5 FreeNetFreenet is a P2P filesharing system based on an initialdesign by Ian Clarke Clark 1999, Clark et al. 2001. Theprimary mission of Freenet is to make use of the systemanonymous. That is, upon entering the system, a usershould be able to make requests for files without anyonebeing able to determine who is making these requests.Likewise, if a user stores a file in the system, it should beimpossible to determine who placed the file into the system. Finally, the operator of a freenet node should haveno knowledge of what data is stored on the local disk.These forms of anonymity make it possible to providestorage and use the system without concern of beingtracked down or potentially held liable.History. The Freenet system was conceptualized by IanClarke in 1999 while at the University of Edinburgh. Inhis introduction to Freenet, he cites the following quotationI worry about my child and the Internet all the time eventhough shes too young to have logged on yet. Hereswhat I worry about. I worry that 10 or 15 years from now,she will come to me and say Daddy, where were youwhen they took freedom of the press away from the Internet  Mike GodwinPublic development of the Open Source Freenet reference implementation began in early 2000.Goals. The principle goal of Freenet is to provide ananonymous method for storing and retrieving information. Freenet permits no compromise on these goals.However, within these bounds, Freenet also strives to beas reliable, easy to use, and responsive as possible.Design. One of the key design points of the Freenet system is to remain completely decentralized. Therefore,Freenet represents the purest form of P2P system.Freenets basic unit of storage is a file. Every node in theFreenet network maintains a set of files locally up to themaximum disk space allocated by the node operator.When all disk space is consumed, files are replaced in accordance with a least recently used LRU replacementstrategy.Each file in the Freenet system is identified by a key.These are typically generated using the hash SHA1SHA1 1997 function. A variety of mechanisms areused to generate the desired hashes, but typically a userstarts by providing a short text description of the file.This description is then hashed to generate a key pair.The public key becomes the file identifier. The privatekey is used to sign the file to provide some form of fileintegrity check. Other schemes for generating keys canbe used as well permitting users to create hierarchical filestructures or to generate disjoint name spaces. The fileskey is then made available to users of the system by outofband mechanisms such as a Web site. Because the keycan be computed from the description of the file, it iscommon to publish only the description and not the actual key.The only operations in the Freenet system are insertingand searching for files. In either case, it is essential tofind the proper location for the file. In general, Freenetnodes form a network in which they pass and forwardmessages to find the location of an existing file or theproper location to store a new file. The keys are used toassist in the routing of these messages. Freenet attemptsto cluster files with similar keys on a single node. Byclustering, Freenet is able to optimize searches by creating routing tables. When a file is successfully located bya search, the files key is inserted into a local routing table. When another search message is received, it is firstforwarded to the peer node with the most similar key inthe routing table. When a search is received by a nodethat contains the desired file, it returns the entire file as asuccessful result. This is done recursively until the file isreturned to the initial requester. As a side effect, the filebecomes replicated at each node in the search path. Inthis way, popular files become highly replicated.Inserts operate much the same as searches. First, a searchoperation is performed on the files key. If a file with thatkey is found, it is returned to the inserter as a form of keycollision indication. When no existing file is found, thefile is propagated forward along the search path. This accomplishes both the replication of the file that occursduring a search as well as preserving the integrity of therouting tables by placing the new file in a cluster of fileswith similar keys.New nodes announce their presence in the network byperforming a search operation. This search basically accomplishes the function of announcing the new node toother existing nodes. Prior to sending the message, thenode must first discover at least one other node in the network to which it can connect. Freenet explicitly does nothelp in this problem because doing so typically leads tocentralized solutions. Users are required to bootstrap32themselves into the network using outofband means forfinding at least one peer.Scalability. The scalability of Freenet has been studiedby its authors using extensive simulation studies Clarkeet. al. 2001. Their studies support the hypothetical notion that route lengths grow logarithmically with thenumber of users. In their experiments, they start with anetwork of 1000 nodes in a ring topology. As the experiment runs, random keys are inserted and removed fromthe network, and the path length to find a file reducesfrom approximately 500 to less than ten. This is consistent with the logarithmic curve expected.Implementation and performance. Freenet is availablein an Open Source reference implementation. The protocol is also well defined allowing others to create theirown implementations. One sideeffect of the focus onanonymity in Freenet is the difficulty in observing its behavior. Obscured identities and probabilistic choicesmake measurements difficult. For these reasons, no realworld performance studies seem to be available. Onlythe simulation results described above can be used toevaluate the system.Lessons learned. The key lesson of Freenet is both theimportance and difficulty of maintaining anonymity. Anonymity opens the door to freer discourse on the networkbecause users need not be concerned with the ramifications of their actions. Anonymity also runs contrary tomany intellectual property notions such as copyright.The Freenet team argues that preserving freedom andeliminating censorship are more important than intellectual property concerns.Business model. Freenet operates as a nonprofit OpenSource project. There are as yet no commercial venturesbuilding on top of Freenet. The Freenet project does solicit donations to help fund continued development, butdoes not appear to be dependent on these donations.Applications. Freenets only real application is as an information storage and retrieval system. The heavy use ofcryptography as well as anonymization of requests maylead to other related uses. For example, it may be possible to use Freenet as a form of distributed backup systemwith some guarantees that data can only be retrieved bythe owner of the data. However, Freenet only providesprobabilistic guarantees about the persistence of data, sothis likely would not make for a highconfidence backupsolution.6.6 GnutellaGnutella is a file sharing protocol. Applications that implement the Gnutella protocol allow users to search forand download files from other users connected to the Internet.History. Gnutella file sharing technology Gnutella,2001 was introduced in March of 2000 by two employees of AOLs Nullsoft division. Touted as an opensource program with functionality similar to that of Napster Napster, 2001, the Gnutella servant program wastaken offline the following day because of a possiblethreat to Warner Music and EMI. AOL was rumored tobe in the midst of merger talks with the record companiesat that time. However, the opensource program remained online long enough for eager hackers to discoverthe Gnutella protocol and produce a series of clones tocommunicate using the Gnutella communication protocol. Soon after, versions of the original Gnutella servantwere communicating with Gnutella clones to search andtrade files over the Gnutella Network.Goals. The goal of Gnutella is to provide a purely distributed file sharing solution. Users can run software that implements the Gnutella protocol to share files and searchfor new files. The decentralized nature of Gnutella provides a level of anonymity for users, but also introducesa degree of uncertainty.Design. Gnutella is not a system or a piece of software.Gnutella is the communication protocol used to searchfor and share files among users. A user must first knowthe IP address of another Gnutella node in the network.This can be discovered by going to a wellknown Website where a number of Gnutella users are posted. Whena user wishes to find a file, the user issues a query for thefile to the Gnutella users about which it knows. Those users may or may not respond with results, and will forward the query request to any other Gnutella nodes theyknow about. A query contains a TimeToLive TTLfield and will be forwarded until the TTL has beenreached.Scalability. While the Gnutella model has managed tosucceed thus far, in theory it does not scale well. Thenumber of queries and the number of potential responsesincreases exponentially with each hop. For example, ifeach node is connected to only two others and the TTLof a query is 7 the default for most Gnutella queries, thenumber of queries sent will be 128 and the number of responses may be substantially more depending on thepopularity of the item.Fault resilience. The Gnutella protocol itself does notprovide a fault tolerance mechanism. The hope is that33enough nodes will be connected to the network at a giventime such that a query will propagate far enough to finda result. However, the distributed nature of the protocoldoes not guarantee this behavior. In fact, studies haveshown Adar and Huberman, 2000, Saroiu et al. 2002that only a small fraction of Gnutella users actually remain online long enough to respond to queries from other users. More robust software has begun to alleviate thisproblem, however no guaranteed solution exists. Theonly real solution at this point is to rely on users to retrywhen their queries or downloads fail.Implementation and performance. Since the first Gnutella servant was posted by Nullsoft, a number of companies have implemented clone software and made effortsto overcome many of the limitations not addressed by theprotocol. Among the most popular are LimewireLimewire, 2001, ToadNode ToadNode, 2001, andBearShare BearShare, 2001. Kotzen claims that whilein late 2000 only 10 of download attempts were successful, by March of 2001 the number grew to over 25Kotzen, 2001. In addition, the number of Gnutella users has increased. However, Kotzen reports that a maximum of 11,000 users has been seen on the network at anyone time. While this represents an increase over previousfindings Clip 2 Marketing, 2000, it does not provideany proof of the scalability or performance of the Gnutella network for the targeted thousands or millions ofnodes.Business model. Gnutella is not a company or a piece ofsoftware, but rather an open protocol. As such, there is noGnutella business model. Many of the companies developing Gnutellacompatible software are opensource oroffer free download. Additionally, some companies arebuilding software for the enterprise that offer contentspecific content sharing.Lessons learned. The Gnutella model has raised a number of questions in the research and development community. Wellknown solutions to traditional computersystems problems cannot be applied in a straightforwardmanner to Gnutellas decentralized network. Researchers are busily trying to apply distributed computing anddatabase problems to adhoc, unorganized, decentralizednetworks of nodes. The benefit of a Gnutellastyle network is the use of an unbounded set of diverse resources.However, solutions often trade reliability for the potential of discovering an otherwise unavailable resource.Applications. The primary application for this technology has been the sharing of music files. While this continues to be the main motivating application for companiesdeveloping Gnutellacompatible software, Gnutella ismore a paradigm than a given application. A number ofcompanies are developing visions of using the Gnutellaprotocol for enterprise software including projectmanagementstyle applications as well as academic softwarefor classroombased communication.6.7 JXTAThe vision of the JXTA project is to provide an open, innovative collaboration platform that supports a widerange of distributed computing applications and enablesthem to run on any device with a digital heartbeat. JXTAprovides core functionality in multiple layers, includingbasic mechanisms and concepts, higher level servicesthat expand the capabilities of the core, and a wide rangeof applications that demonstrate the broad applicabilityof the platform.History. The JXTA project was unveiled by Sun onApril 25, 2001 JXTA 2001 and was intended to be aplatform on which to develop a wide range of distributedcomputing applications. Despite its recent introduction,JXTA has been quite popular. Statistics show that on theweek of November 3rd, 2001, JXTA had 122 posts andwas used by 6,809 users.Goals. The goal of JXTA is to provide a generalpurpose network programming and computing infrastructure. Its goals are Interoperability by enabling interconnected peers toeasily locate each other, participate in communitybased activities and offer services to each other seamlessly across different P2P systems and different communities Platform independence JXTA is designed to be independent from programming languages such as C orJava, system platforms such as Microsoft Windowsand UNIX operating systems, and networking platforms such as TCPIP or Bluetooth Ubiquity JXTA is designed to be implementable onevery device with a digital heartbeat, including appliances, desktop computers, and storage systemsDesign. It is important to note that the JXTA project isapproaching the P2P space from the lower level as theyare proposing an entirely new infrastructure with no direct relation to other, existing P2P systems e.g., Gnutella and Napster. For example, they have built their owndistributed search service, called JXTA search.Peer groups are the core of JXTAs infrastructure. A peergroup is essentially a partitioning of the world of peersfor communication, security, performance, logical locality and other reasons. A single participant can be inmultiple groups at one time. JXTA provides core proto34cols for peer discovery, peer group memberships, andpeer monitoring. JXTA uses asynchronous unidirectional communication channels, called pipes, for sendingand receiving messages. All data interchange in JXTA isin the form of XML formatted documents.Services. The JXTA project has defined core and optional services that run on the JXTA platform. Examples ofcore services include authentication, discovery, andmanagement. To address the need for security, they haveimplemented a cryptography toolkit that enables message privacy, ensures authentication and integrity, andpermits transactions between peers that cannot be repudiated. However, it is not clear at this point how they dealwith firewalls for example, how a peer outside a firewallcan discover peers inside the firewall or how two peersinside two firewalls can communicate. Another core service is the Content Manager Service CMS that allowsJXTA applications to share, retrieve, and transfer contentwithin a peer group. However, in the current version ofthe CMS, there is no support for automatic propagationof content requests each peer sends content requests directly to the other peers. Examples of optional servicesare naming, routing, indexing, and searching. For example, the JXTA distributed indexing service implements ageneralpurpose, fully distributed index service by usinga distributed inverted index to map keywords to a list ofpostings.Applications. The JXTA Shell was the first applicationdeveloped that gives a commandline interface to thecore JXTA services, such as peer and group discoveryand messaging. Since then, the JXTA project has developed more applications that allow interactive access tothe JXTA platform. For example, the InstantP2P application enhances the JXTA project with chat capabilities. Itincludes functionality that enables users to chat one onone, chat with a group, and share files. Another application is JuxtaProse, which is a discussion application thatallows creating, viewing, and replying to HTML documents over JXTAs content management system. Otherapplications that the JXTA community is currently designing include event notification and P2P email.Scalability. The JXTA project has a layered architecture, which consists of three building blocks JXTA corefor peer group management, JXTA services such assearching and indexing, and JXTA applications e.g.,JXTA Shell. This layered architecture enables JXTA toeasily incorporate new protocols and services to supporta growing number of users. However, it will certainlyface some scalability problems that have been currentlyleft unattended. One of these problems is global naming.JXTA attaches a unique ID generated by the users witheach peer in the group but does not solve the global naming problem because it does not guarantee uniquenessacross the entire community of millions of peers. Even ifa peer has a unique ID within its group there is no guarantee that its name is unique across multiple groups.Implementation and performance. The first version ofthe JXTA core has currently been released, running inthree different platforms the Solaris Operating System,Linux, and Microsoft Windows. A prototype of theJXTA Shell that gives a commandline interface to thecore services is also available. In addition to that, therehave been many independent efforts that build servicesand applications on top of JXTA.Business model. JXTA has been released as opensourcecode and has already attracted many developers to buildapplications e.g., event notification, file sharing, P2Pemail on top of it. Sun is also working with other P2Pcompanies that are committed to work with the JXTAopensource model and contribute their P2P technologyto it. JXTA requires some familiarity with the UNIX operating system, which is not required for typical Gnutellausers.6.8 .NET My Services.NET My Services and .NET in a more global form donot represent a P2P system in its traditional form, as doother P2P systems described in this paper. However,.NET My Services encompass a lot of P2P architectureand as such they are presented here. .NET also introduces a new language called C, development tools, aframework, a foundation for Web services, and the underlying operating systems. This paper addresses onlythe .NET My Services part of it shaded part ofFigure 22.SecurityPeerGroupsPeerPipesPeerMonitoringInfrastructureJXTA Indexing ContentJXTAApplicationsShellInstantP2PFigure 21 JXTA Architecture.JXTA CoreServices Searching Manager35History. Microsoft officially announced .NET for thefirst time in June 2000. Shortly thereafter, certain components were available, such as operating systems, partsof the .NET framework and enterprise servers, and passport. During 2001 and 2002, subsequent versions of operating systems, frameworks, and some key serviceshave been introduced.Goals. The goals of .NET My Services and .NET in thebroader context are to enable users to access Web services on the Internet from available devices, including desktops, laptops, and handhelds, using existing standards,such as XML Bray et al. 2000, UDDI Ariba et al.2000, SOAP Box et al. 2000, WSDL Christensen etal. 2001. The goal of .NET is to shift focus from applications, platforms and, devices to userfocused data.Therefore, security and privacy are one of the majorgoals of .NET.Design. The design of .NET is focused around decomponentization and decentralization of distributed services. For example, a user will locate the desired servicethrough the UDDI registry, then she will access servicethrough the Web Services provider location, and she willbe authenticated through a separate passport service seeFigure 23. The .NET Services programming model isbuilt around Web standards and standard protocols, suchas SOAP, UDDI, and WSDL. The .NET System programming model is built around a new common language runtime, which is intended to complement theJava runtime. Passport see Security and Privacy belowis an authentication method for .NET Services. OtherWeb services can communicate with .NET My Serviceswithout disclosing a users identity and personal information.Security and Privacy. Security and privacy is one of themain goals but also one of the main challenges for .NETMy Services. The support is provided through an onlineservice Passport that enables authentication of users.This is used for accessing Web pages see Figure 24 orfor services in general. Once authenticated, the user canroam across passportparticipating Web sites. Web sitesin turn are required to implement the single signin service. Passport was released in July 1999 Microsoft2001a and it had over 160 million accounts by July2001.Users of passport are required to create a passport account that contains the following information. Passport unique identifier puid. Created automatically as a part of account creation. User profile. Contains an email address or a phonenumber required, a firstlast name, and demographicinformation postal code, region, country, etc.. Credential. Consists of a standard credential emailphone and a password and a fourdigit key for strongcredential signin required by some Web sites. Wallet optional. Contains credit card numbers and apostal address for delivery of purchased goods.Of the passport information, only the puid is shared withparticipating Web sites, and wallet information is sharedin case of purchases. This protects the privacy of users,with passport playing the intermediary in user authentication. Passport offers the Kids Passport service, whereparents can restrict the information visiting Web sitescan collect about kids. Finally, passport has been optimized for phone access by means of adaptation to minimal screen requirements and different protocol supporte.g., WML for WAP browsers or cHTML for imode.Passport will be integrated with Windows XP by storingthem in the local credentials manager.Scalability. It is only fair to compare the scalability of.NET implementations with its earlier pre.NET equivalents. Operating systems will be enhanced with hooks forthe benefit of Web Service Foundations, and the OSserver editions will be required to scale even more giventhe increased number of clients with the introduction ofmobile users that may have unpredictable access patFigure 22 .NET Architecture..NET My Services and Applications.NET .NETEnterprise ServersFrameworkFoundation forWeb ServicesWindows OSVisual Studio.NETlibs, tools, APIs,C, clr commonExchange andSQL ServersexBackOffice orDNA 2000authentication,storage, mail,instant msgingexHailstormadapted to .NET, e.g., hooks for Foundation Servicesadapted toin support ofIDE.NET servicedevelopmentlanguage runtimeVB.NET, VCe.g., address, profile, appSetting, inbox, documents, calendar,favouriteWebSites, wallet, devices, services, lists, categories..Figure 23 User Interaction with .NET Components.UDDIRegistryWebServiceMicrosoftPassportregisterservicefindservice useserviceuser authentication36terns. The ultimate benefit of the .NET approach will bein the further decomponentizing of the solutions enabling finer granularity of concurrency between components and hence better component utilization. At thesame time, the side effect is improved reliability andavailability.Fault resilience. Similar to scalability, further decomponentization of .NET enables fewer single points of failure and more opportunities for failover among peercomponents.Implementation and performance. It is fairly early totalk about implementation details and performance. Sofar, most of .NET is envisioned to execute on Windowsplatforms. It is still uncertain whether there will be otherplatforms where .NET can run. Given the large base ofdevelopers it is likely that performance will be addressedas the deployment progresses.Business model. .NET is a significant move in Microsofts strategy. It represents a move from a licensebased to a subscriptionbased model. .NET also has significant impact on how other businesses will function ingeneral. It is one of the first times that Microsoft embraces the Internet model in its entirety. Microsoft plans torun .NET Services as a business, similarly to AOL or Yahoo. It will charge consumers and service providers forusing .NET My Services. In the longer term, Microsoftmay provide enterpriselevel .NET My Services. Oneimportant aspect of the business model is that Microsoftembraced opensource standards, such as XML, SOAP,WSDL, and UDDI, as well as its submission of the Cand Common Language Infrastructure to standards bodyEuropean Computer Manufacturers Association ECMA.Applications. The major advantage of .NET My Services compared to other solutions is the wealth of applications. Microsoft will port all of the Microsoft Officeapplications to .NET, including the development tools.Lessons learned. .NET and .NET Services are fairlynew to be able to derive any substantial lessons learned.Nevertheless, at least two lessons can already be derived.First, the passport model raises a lot of concerns aboutsecurity problems, but it has also attracted a huge numberof users so far. Second, it is the applications that attractusers .NET seems to follow the same model as with MSDOS, where it is really the applications, such as Exceland Word that are of interest to users, not the infrastructure itself.6.9 SummaryThis Section summarizes the case studies. Table 6 summarizes the general characteristics of all of the case studies. The entries are self explanatory. P2P systems aredeployed on a variety of systems and support differentlanguages and standard protocols. They are largely targeted for the Internet and some are targeted for mobilesettings.Table 7 summarizes the case studies from the perspective of the characteristics described in Section 4. Thereare various decentralization models, including pureFreeNet, Gnutella, JXTA, hybrid Groove, Magi, masterslave SETIhome, and mixed .NET. Few sysFigure 24 .NET Passport.UserBrowserPassportServerWebServer1.request pageauthentication3.authentication2.authenticationauthenticationreturn Web pageuserdatabaseJust a dummy text tofill in thisdummy WebpassportmgrWebPageP2P SystemSystem FeatureSystemCategory Alternative Solution PlatformLanguages andTools Distinctive Features Target NetworksAvakidistributedcomputingsingle installation HPC andsupercomputersLinux, Solaris,MS WindowsOO, paral. lang. obj wrap., legacyapp xcomp. Fortran, Ada, Cdist. admin ctrl, heterogeneity,secure access, high paral. execInternet, intranetSETIhome distributed objects all commonOS supportedclosed source large scale InternetGroovecollaborationWebbasedcollaborationMS WindowsJavaScript, VB, Perl, SOAP, C,XMLexec. playback, selfupdatingmultiidentitiesInternet, intranetMagi dist. file sys. centralized chat messagingWindows and Mac Java, XML, HTTP, WebDAV HTTP based, platform indep.Internet, adhocmobileFreeNet contentdistributionanonymity none. others centralized  single point of trustany with Java Java implementation and APIs Preservation of anonymity InternetGnutella central servers Windows, Linux Java, C protocol InternetJXTAplatformclientserverSolaris, Linux, MSWindowsJava, C, Perl opensource effort Internet.NET My Services Webbased Windows C, VC, JScript, VBScript, VB widespread MS apps baseInternet andmobileTable 6. Summary of Case Studies.37tems support a high degree of anonymity FreeNet. Anumber of them support some sort of selforganizationand most of them improve cost of ownership. Adhoc nature is predominantly supported through the ability of thepeers to join and leave. Performance is improved primarily in the distributed computing type of P2P systemsAvaki, SETIhome. Standard security mechanismsare supported, such as encryption, authentication, andauthorization, and some of them protect against denial ofservice attacks Freenet.Transparency is primarily supported for the benefit ofdisconnection and to hide communicationcomputationheterogeneity Groove, Avaki. Fault resilience is supported through checkpointrestart mechanisms Avaki,SETIhome, queued messages Groove, Magi, and ingeneral by the redundancy of components and the elimination of a single point of failure. Most systems interoperate by relying on standard communication stacks IPand on Web standards SOAP, XML, UDDI, WSDL, andWebDAV. There are a only a couple of exceptions of interoperability with other P2P systems, such as Avakiwith Suns Grid, and Magi with JXTA.Table 8 summarizes the business aspect of the case studies. The surveyed case studies belong both to proprietaryGroove, .NET and open source systems FreeNet, Gnutella, JXTA, or they are offered as either Avaki, Magi.They support applications in their respective domains,such as distributed computing, communicationcollaboration, and file sharing. P2P systems classified as platforms .NET My Services, JXT, and to a certain extentGroove and Magi support a wider base of applications.The case studies we selected have a closed base of customers with the exception of .NET My Services, whichrelies on its earlier customer base. Also, they are competitors within each class of P2P systems. These systemshave different funding model startups Magi, Freenet,P2P SystemSystem FeatureDecentralization Scalability AnonymitySelfOrganizationCost ofOwnership AdhocPerformance Security TransparencyFaultResilience InteroperabilityAvaki distributed,no central mgrscale to 1000s2.53k testedNArestructuresaround failurelowjoinleave compute resourcesspeedupsencryp, authentication, adm. domainslocation HWSW heterog.chckptrestartreliable msginteroperateswith Sun GridSETIhome masterslave millions medium low very lowjoinleave compute resourceshugespeedupsproprietary hightimedchckptIPGroove hybrid P2P NA poor high low joinleave ofcollaboratorsmediumsharedspacesauthnauthr, encryphighqueuedmessagesIPbasedMagi hybrid P2Paround a 100 corporate NWNA NA lowjoinleave ofbuddies NAcertificateauthoritycommunicateoffline buddiesqueuedmessagesJXTA, WebDAVFreeNet pure P2P theoret. scales logsizenetworkhigh high lowjoinleave ofpeersmediumf anonymity  preventing DoShighNo 1 point offailure, replic.lowGnutella pure P2P thousands low high low joinleave ofpeerslow not addressed mediumresumedownloadIPJXTA pure P2P also addressesembedded systemsNA NA lowjoinleave ofpeersNAcrypto algor.distr. trust modellow low low.NET My Servicesmixed worldscale NA medium lowjoinleave ofpeershigh passportbased high replicationSOAP, XML,UDDI, WSDLTable 7. Comparison of Characteristics of Case Studies.P2PSystemSystem FeatureRevenue Model SupportedApplicationsKnown Customers Competitors Funding Business ModelAvaki product andopensourcecomputation grid, sharedsecure data accessnone. evaluated at several lifesciences labsPlatform ComputingGlobusstartup NASETIhome Academic Closed Academic cancerhomefightaidshomegovernmentsell more computersads on screen saversGroove product purchasing, inventory,auctions, etc.NA Magi IPOcollaborative tool ofchoice next Lotus NotesMagi product opensourceShared file access, messaging, chatGlobal eTech, InterPro GlobalPart., Mongoose T., MediasoftGroove startup NAFreeNet opensource file sharing public NA startup NAGnutella opensource file sharing public NA public domain algorithm of choice forP2PJXTA opensourceproprietary extensionsfile sharing, messaging,event notific., emailMany P2P systems ported toJXTA.NETMy Servicespublic domain, supportedby Sun Microsystemsdefacto common P2Pplatform.NET .NET My Servicesproprietary  opensource standardsMIcrosoft Office andmorelarge base of MScustomersAOL, Sun J2EEJXTA,Ximian MONOMS internallydefacto pervasive platformTable 8. Business Comparison of Case Studies.38government funded SETIhome, public domain efforts Gnutella, JXTA, or privately owned companies.NET My Services.Finally, they have different business models, rangingfrom becoming an allencompassing pervasive platformof choice .NET My Services, or becoming a collaborative platform of choice Groove, Magi, to selling morecomputers and using ads on screen savers SETIhomeand becoming a P2P algorithm of choice.7 LESSONS LEARNEDPeertopeer is a relatively new technology that is not yetproven in the research community and in industry.Therefore, it may be too early to make a firm statementon some of the lessons learned. Nevertheless, we canmake some preliminary observations. Some of them areintuitive, while others are not obvious. We divide the observations into P2P strengths and weaknesses, P2P nontechnical challenges, and implications on users, developers, and IT.7.1 Strengths and WeaknessesA comparison of P2P with its alternatives, centralizedand clientserver systems, is summarized in Table 9. Amore detailed comparison of P2P with its alternatives inspecific classes of P2P systems is also provided in Appendix A. P2P systems are designed with the goals of decentralization, adhoc behavior, improved cost ofownership, and anonymity. Therefore, P2P is superior tocentralized and clientserver systems in these areas. P2Phas more decentralized control and data compared to alternatives it supports systems whose parts can come andgo and can communicate in an adhoc manner the costof ownership is distributed among the peers and thepeers can be anonymous. Compared to P2P, centralizedsystems are inherently centralized and clientserver systems have centralized points of control and data at theservers.It is harder to compare P2P with alternatives in terms ofscalability, performance, security, selforganization, andfaulttolerance. It is our speculation that because of higher decentralization P2P can better satisfy these requirements as well. P2P has the potential to be more scalablethan centralized and clientserver solutions. However,P2P systems are highly dependent on the underlying topology and the types of applications. For example, mainframe computers are still competitive for transactionbased computing because they are optimized for this typeof applications. On the other hand, P2P system are verysuitable for large number of computers on the Internet,for mobile wireless users, or for device sensor networks.A similar reasoning applies to performance and faultresilience. Individually, centralized systems have highlyoptimized performance and faulttolerance, followed byclientserver systems, and only then by P2P systems. Theopposite is true for aggregate systems where P2P hashigher aggregate performance and fault resilience byavoiding the need to interact with servers and havingfewer points of failure.It is unclear whether any of the systems has a clear advantage in selforganization and transparency. There hasbeen longstanding work in improving these characteristics in both centralized systems and clientserver systems. Selforganization is especially critical in InternetData Centers, and mainframe computers also implementsome forms of adaptation. Transparency has been the focus of clientserver systems, in particular location transparency. While also a goal of P2P, transparency has notbeen sufficiently developed in P2P systems. Many actions require user intervention, such as connecting to aspecific network of P2P systems, providing knowledgeabout the files and users, and understanding the failuremodel.Finally, we believe that in two regards P2P lags behindits alternatives. Inherently, P2P systems expose more security threats than centralized and clientserver models.Because of the nature of peertopeer interaction, sufficient guarantees or additional trust has to be establishedamong the peers. Interoperability is matter of investmentSystemAppRequirementsType of the SystemCentralized ClientServer PeertoPeerdecentralization low none high very highadhoc behavior no medium highcost of ownership very high high lowanonymity low none medium very highscalability low high highperformanceindividual highmediumindividual lowaggregate low aggregate highfault resilienceindividual highmediumindividual lowaggregate low aggregate highselforganization medium medium mediumtransparency low medium mediumsecurity very high high lowinteroperability standardized standardized in progressTable 9. Comparison of Solutions. Darker shading representsmore preferred characteristics or strengths.39and as development proceeds more interoperability mayevolve. At the time of writing, clientserver systems support the most interoperability.7.2 NonTechnical ChallengesIn addition to all of the technical motivations and difficulties with developing P2P systems, there are also nontechnical challenges to the success of P2P. In spite of excellent technology, if these challenges are not overcome,P2P will likely remain an approach used only in smallniches or by specific communities.The chief challenge of P2P systems is acceptance anduse. Because peers rely on one another to provide service, it is essential that numerous peers be available forthe service to be useful. By comparison, centralized orclientserver environments are potentially useful as longas the service provider keeps the service running. This isnot the case in P2P systems. If the peers abandon the system, there are no services available to anyone.P2P systems live and die on their network effects, whichdraw in more and more users. The value of network effects was informally specified by Metcalfes Law, whichstates that the utility of a system grows with the squareof the number of users. While we cannot truly quantifyutility to prove the law, it resonates clearly with the ideathat more users make a system more useful.Studies have also shown that all users are not alike, andthat some users can actually damage a system. Adar andHuberman2000 showed that in the Gnutella system,many users download files, but few actual provide files.Further, those with poor resources, such as bandwidth,can actually slow the network down by becoming bottlenecks for the system as a whole. This situation has beenlikened to the tragedy of the commons where poorly behaving users of a free, shared resource make the resourceunusable by all.Solutions to this problem revolve around building aneconomy around the use of the shared resource. One example of this is MojoNation MojoNation 2001. In MojoNation, users accumulate a form of currency calledmojo by providing service to others. In the MojoNation scheme, this currency would be redeemable for other services or potentially from realworld vendors in theform of gift certificates or related benefits. However, implementing a MojoNation type scheme requires accounting to be performed, and this can limit anonymity andother potential benefits of P2P systems.Related to acceptance and use is the danger of fragmentation of the user base. Typically, individuals are onlygoing to participate in one or a very few different P2Psystems because they simply dont have the resources tosupport multiple systems at the same time. Because ofthis, as each new system is introduced it fragments theuser base and can potentially damage the value of all P2Psystems. Consider the success of Napster. It was arguably not the most technically sound P2P system developed, but it was the first to gather a large user base.Therefore, until it was forcibly shutdown, it thrived because the network effects continued to draw more andmore users.Since Napsters demise, a number of music sharing systems, such as Gnutella and KaZaa have been developedand are both free and technically sound. However, neither has yet become the one truly dominant system, soneither has become as truly useful as Napster was in itsprime.Instant messaging faces a similar difficulty. While thereare numerous IM systems, each with a significant userbase, the overall utility of IM is limited because of fragmentation. Typically, to communicate with everyone auser wishes to reach, the user must maintain accountsand run clients from many IM systems at the same time.Interoperability, as discussed in Section 4.11, seems tobe the only solution to this problem.While P2P systems rely on scale for success, scale is alsoa significant challenge. In centralized systems, problemsrelated to scale are relatively well understood and solutions are pretty well known. In the worst case, bigger,faster computing platforms are an option for improvingscale. Decentralized P2P systems more often require algorithmic solutions to problems of scale. There is no central place to simply throw more computing resources.These distributed algorithms tend to be some of the mostdifficult to develop because they require decisions to bemade at each local peer, usually with little global knowledge.A final challenge involves the release of control. In centralized systems, the operator of the system has somecontrol of the system. For example, an operator can monitor each transaction and potentially determine who initiated the transaction and how long it took. In adecentralized system, no such monitoring is possible.This scares many providers of traditional services so theyresist P2P schemes. Certainly, the release of control canbe seen as part of the reason for the Recording IndustryAssociation of Americas RIAA lawsuit against Napster. Napster provides an alternative method for musicdistribution. While copyright, payment, and fairuse issues are certainly at the center of the case, embracing a40P2P approach to distribution, such as Napster, also implies a release of control of the distribution channel. Thismay explain why the RIAA and Napster have not beenable to reach an agreement.7.3 Implications for Users, Developers, and ITP2P is not a solution for every future system and application. As we have seen in the previous two sections, it hasboth strengths and weaknesses. In this section, we evaluate the implications that P2P has for users, developers,and IT department. In Table 10, we compare P2P with itsalternatives. P2P has the following implications for theusers of P2P systems and applications pervasiveness,complexity of use, state of the art, and trust and reputation.Pervasiveness is becoming more and more important.While it may not be possible to access traditional services at any point on Earth at any time, P2P offers opportunities in this regard by relying on peers to provideservices when there is no other infrastructure or means ofaccess available. For example, cellular connectivity maynot be available to the server, but a peer can offer the service locally.Traditional centralized solutions have complex supportcompared to clientserver solutions. P2P solutions arenot as simple or as wellunderstood as the clientservermodel, yet wide deployment and use of Napster, Gnutellabased solutions, and other startups offers potential inthis regard.Currently, the clientserver model represents the state ofthe art, but there is a lot of ongoing development in P2Presearch  second generation P2P systems CAN, Pastry,CHORDS, Tapestry, etc. opensource  JXTA proprietary systems  .NET and standards  P2PWG.From the user perspective, P2P is probably weakest inthe sense trust and reputation. Owners want to have trustin the service they are using, so they will use only reputable sites unless price is the main objective or the service is free. Centralized systems and the clientservermodel have traditionally built up trust and reputation,and this is a concern for users of P2P.Developers are also concerned with complexity, as wellas with the sustainability of solutions, with the availability of the tools, and the compatibility of the systems. Inthis comparison, the clientserver dominate over the other solutions.Clientserver systems are wellunderstood and documented for developers. P2P systems offer promise insimple cases, such as document exchange, but for morecomplex requirements, such as collaborative applications, they require more complex algorithms and understanding. It is a similar case of sustainability. In the longterm, centralized solutions may not be as sustainable aspeertopeer solutions, but the clientserver model willcontinue to be supported. Finally, the weakest aspect ofP2P is the lack of tools and compatibility across variousP2P systems.P2P has the following implications for IT accountability, being in control, manageability, and standards. Thefirst three are very closely tied. Accountability is emphasized in centralized systems where access is monitoredthrough logins, accounts, and the logging of activities.Accountability is more difficult to achieve in clientserver systems, because of interactions with multiple clients.It is weakest in P2P systems, because of equal rights andfunctionality among the peers. Similar reasoning appliesfor being in control. In centralized and clientserver systems, control is exercised at one or more welldefinedpoints, whereas it is harder to achieve in P2P systems,where control is entirely distributed.A similar situation exists for manageability. There are alot of tools for the management of clientserver systems,somewhat fewer for centralized systems, and fewest forP2P. Similar applies for standards. Most standards havebeen developed for clientserver systems and very fewfor P2P systems.Target CriteriaType of the SystemCentralized ClientServer P2PUserPervasiveness low medium highStateoftheart low high mediumComplexity high low mediumTrust Reputationhigh medium lowDeveloperComplexity high straightforward typical  noatypical  yesSustainability low high mediumTools mediumproprietaryhighstandardizedlowfew toolsCompatibility medium high lowITAccountability high medium lowBeing in control high fully medium lowManageability medium high lowStandards mediumproprietary highlowinexistentTable 10. Comparison of Solutions. Darker shading representspreferred characteristics.41While Table 9. compares the potential of P2P versus itsalternatives in terms of the characteristics, Table 10.summarizes the existing implications of P2P on users,developers, and IT. In summary, there is a lot of potentialfor P2P, but it has not yet been realized.8 SUMMARY AND FUTURE WORKIn this paper, we surveyed the field of P2P systems. Wedefined the field through terminology, architectures,goals, components, and challenges. We also introducedtaxonomies for P2P systems, applications, and markets.Based on this information, we summarized P2P systemcharacteristics. Then, we surveyed different P2P systemcategories, as well as P2P markets. Out of systems presented in Section 5, we selected eight case studies anddescribed them in more detail. We also compared thembased on the characteristics we introduced in Section 4.Based on this information, we derived some lessonsabout P2P applications and systems.In the rest of this section, we revisit what P2P is, we explain why we think that P2P is an important technology,and finally we present the outlook for the P2P future.8.1 Final Thoughts on What P2P IsOne of the most contentious aspects in writing this paperwas to define what P2P is and what it is not. Even aftercompleting this effort, we do not feel compelled to offera concise definition and a recipe of what P2P is and whatit is not. A simple answer is that P2P is many things tomany people and it is not possible to come up with a simplified answer. P2P is a mind set, a model, an implementation choice, and property of a system or anenvironment. A mind set. As a mind set, P2P is a system andor application that either 1 takes advantage of resources atthe edge of the system or 2 supports direct interactionamong its users. Such a system andor application remains P2P regardless of its model or implementation.Examples include SETIhome, which is consideredto have a clientserver model, but displays the firstmind set property, and Slashdot Slashdot 2002,which enables the second mind set property, but reallyhas a centralized implementation. A model. A system andor application supporting themodel presented in Figure 1 is P2P. In its purest form,P2P is represented by Gnutella, Freenet, and Groove.According to this, SETIhome does not have a P2Pmodel, whereas Napster has a hybrid model. An implementation choice. P2P systems and applications can be implemented in a P2P way, such as JXTAor Magi. However, a nonP2P application can also beimplemented in a P2P way. For example, applicationlayer multicast can have a P2P implementation, andparts of the ORBs or DNS servers are also implemented in a P2P way. A property of a system or an environment. .NET aswell as environments, such as small device sensor networks, may require P2P implementation solutionswhile not necessarily supporting a P2P application.P2P solutions may be required for scalability, performance, or simply because of the lack of any kind of infrastructure, making P2P the only way tocommunicate. This is similar to looking at P2P as animplementation choice, however in this case P2P is theforced implementation choice.8.2 Why We Think P2P is ImportantAs P2P becomes more mature, its future infrastructureswill improve. There will be increased interoperability,more connections to the Internet world, and more robust software and hardware. Nevertheless, some inherentproblems will remain. P2P will remain an important approach for the following reasons. Scalability will always be a problem at certain levelsnetwork, system, and application, especially withglobal connectivity, much of it wireless. It will be hardto predict and guarantee all servicelevel agreements.P2P can contribute to each area. Certain parts of the world will not be covered by sufficient connectivity, requiring adhoc, decentralizedgroups to be formed. P2P is a wellsuited alternativewhen there is a lack of infrastructure. Certain configurations of systems and applicationswill inherently be P2P and will lend themselves to P2Psolutions.8.3 P2P in the FutureThe authors of this paper believe that there are at leastthree ways in which P2P may have impact in the future P2P algorithms probably have the biggest chance ofmaking impact. As the world becomes increasinglydecentralized and connected, there will be a growingneed for P2P algorithms to overcome the scalability,anonymity, and connectivity problems. P2P applications are the next most likely to succeedin the future. Examples, such as Napster are a convincing proof of such a possibility. P2P platforms are the third possible scenario for P2P.Platforms such as JXTA may be widely adopted, in42which case many other P2P systems can also gain wideadoption.8.4 SummaryWe believe that P2P is an important technology that hasalready found its way into existing products and researchprojects. It will remain an important solution to certaininherent problems in distributed systems. It may not bethe only solution and may not be appropriate for all problems, but it will continue to be a strong alternative forscalability, anonymity, and fault resilience requirements.P2P algorithms, applications, and platforms have an opportunity for deployment in the future. From the marketperspective, cost of ownership may be the driving factorfor P2P. The strong presence of P2P products indicatesthat P2P is not only an interesting research technologybut also a promising product base.ACKNOWLEDGMENTSWe are indebted to Fabio Casati, Denis Chalon, IraGreenberg, Martin Griss, Tim Kindberg, Alan Karp, RajKumar, Alan Messer, Jeff Morgan, Chandrakant Patel,Todd Poynor, Steve Richardson, Stphanie Rich, SumitRoy, Jim Rowson, Patrick Scaglia, MingChien Shan,and Zheng Zhang for reviewing the paper. Their comments significantly improved the content and presentation.43REFERENCESABERER, K., PUNCEVA, M., HAUSWIRTH, M., SCHMIDT,R. 2002. Improving Data Access in P2P Systems.IEEE Internet Computing 615867. JanuaryFebruary.ADAMIC, L. The Small World Web. 2000. Technical Report, Xerox Palo Alto Research Center.ADAR, E. AND HUBERMAN, B. 2000. Free Riding onGnutella. First Monday, vol 5, no 10 October. seealso www.firstmonday.dkissuesissue510adar.AKAMAI 2001. www.akamai.com.ALBITZ, P, AND LIU. C. 1995. DNS and BIND. OReillyand Associates.ANDY O. 2001. PeerToPeer, Harnessing the Power ofDisruptive Technology. OReilly.AOL 2001. www.aol.com.APPLIED METACOMPUTING. 2000. Legion  An Integrated Architecture for Secure Resource Sharing. Applied Meta Computing PeertoPeer ArchitecturalProposal.ARIBA, IBM, AND MICROSOFT. 2000. UDDI TechnicalWhite Paper, available at www.uddi.org, September2000.AVAKI CORPORATION. 2001. Avaki 2.0 Concepts andArchitecture. White paper. www.avaki.compapersAVAKIconceptsarchitecture.pdf.BEARSHARE 2001. www.bearshare.com.BALL, T. AND RAJAMANI, S. K. 2001 Automaticallyvalidating temporal safety properties of interfaces. Inthe Proceedings of the International SPIN Workshopon Model Checking of Software, pp. 333344, May2001.BARAK, A. AND LITMAN, A. 1985. MOS a Multicomputer Distributed Operating System. Software  Practice and Experience, 158725737. August.BARAK, A. AND WHEELER, R. 1989. MOSIX An Integrated Multiprocessor UNIX. Proceedings of theWinter 1989 USENIX Conference, pages 101112.February.BECKER D.J., STERLING T., SAVARESE D., DORBANDJ.E., RANAWAK U.A., PACKER C.V. 1995. BeowulfA Parallel Workstation for Scientific Computation,Proceedings of the International Conference on Parallel Processing.BERNERSLEE T. FIELDING, R., MASINTER, L. 1998. Uniform Resource Identifiers URI Generic Syntax.IETF Request for Comments. August 1998BERNSTEIN, P. A. 1996. Middleware A Model for Distributed System Services. Communications of theACM, 3928698.BOLCER, G. ET AL. 2000. PeertoPeer Architectures andthe Magi Open Source Infrastructure. Endeavorstechnologies White Paper.BOLCER, G, 2001. Magi An Architecture for Mobile andDisconnected Workflow. White paper www.endeavors.comBOLOSKY, W., DOUCEUR, J., ELY, D., AND THEIMER, M.2000. Feasibility of a Serverless Distributed File System Deployed on an Existing Set of Desktop PCs.Proceedings of SIGMETRICS, Santa Clara, CA,USA, Jun, 2000.BOND, J. 2001. Business Uses of Peer to Peer P2PTechnologies. Netmarkets Europe White Paper.www.netmarketseurope.com.BOX, D. EHNEBUSKE, D., KAKIVAYA, G., LAYMAN, A.MENDELSOHN, N., NIELSEN, H.F., THATTE, S., WINER. D. 2000. Simple Object Access Protocol SOAP1.1. W3C Note 08 May 2000.BRAY, T., PAOLI, J., SPERBERGMCQUEEN, C. M., MALER, E. Editors. 2000. Extensible Markup LanguageXML 1.0 Second Edition W3C Recommendation6 October 2000.BRITTON, C. 2000. IT Architectures and MiddlewareStrategies for Building Large, Integrated Systems.Addison Wesley.BUYYA, R. 2001. Economic models for Management ofthe Resources in PeertoPeer and Grid Computing.Proceedings of the Commercial Applications forHighPerformance Computing Conference.CARPENTER. B. 2000. Internet Transparency. InternetSociety Request for Comments 2775. www.faqs.orgrfcsrfc2775.html.CASAVANT, T. L. AND KUHL, J. 1988. A Taxonomy ofScheduling in GeneralPurpose Distributed Computing Systems. IEEE Transactions on Software Engineering, SE142141152.CASTRO, M. AND LISKOV, B. 1999. Practical ByzantineFault Tolerance. Proc. Usenix Symp. Operating Systems Design and Implementation OSDI 99 UsenixAssoc., Berkeley, California.CHAPIN, S. J. 1996. Distributed and MultiprocessorScheduling. ACM Computing Surveys, 281233235.CHAUM, D. 1981. Untraceable Electronic Mail ReturnAddresses and Digital Pseudonyms, Communicationof the ACM 24, 2, Feb. 1981, pp. 8488.CHILDERS, L., DISZ, T., OLSON, R, PAPKA, M., STEVENS,R., AND UDESHI, T. 2000. Access Grid ImmersiveGrouptoGroup Collaborative Visualization. Pro44ceedings of the Fourth International Immersive Projection Technology Workshop. Jun, 2000. ppCHRISTENSEN, C.M. 1997. The Innovators Dilemma.Harvard Business School Press.CHRISTENSEN, E., CURBERA, F., MEREDITH, G., WEERAWARANA, S. 2001. Web Services Description Language WSDL 1.1, W3C Note 15 March 2001.www.w3.orgTRwsdl.CLARKE, I. 1999. A Distributed Decentralized Information Storage and Retrieval System, unpublished report, Division of Informatics, University ofEdinburgh. freenet.sourceforge.netfreenet.pdf.CLARKE, I., I., MILLER, S.G. WONG, T.W., SANDBERG,O., WILEY, B. 2002. PROTECTING FREE EXPRESSIONONLINE WITH Freenet. IEEE Internet Computing614049. JanuaryFebruary.CLARKE, I., SANDBERG, O, WILEY, B., HONG, T.W.2001. Freenet A Distributed Anonymous Information Storage and Retrieval System. Designing Privacy Enhancing Technologies International Workshopon Design Issues in Anonymity and Unobservability,LNCS 2009, ed. by H. Federrath. Springer NewYork.CLIP 2 MARKETING 2000. Gnutella To the BandwidthBarrier and Beyond. www.clip2.comgnutella.html.COULORIS, G., DOLLIMORE, J. 2001. Distributed Systems. Concepts and Design. Addison Wesley.DABEK, F., BRUNSKILL, E. KAASHOEK, F., KARGER, D.,MORRIS, R., STOICA, I., AND BALAKRISHNAN, H.2001. Building PeertoPeer Systems With Chord, aDistributed Lookup Service. Proceedings of the 8thWorkshop on Hot Topics in Operating Systems HotOSVIII, Schloss Elmau, Germany, May 2001.DATASYNAPSE 2001. The Next Generation of High Performance Computing For Financial Services. WhitePaper.DENNING. D. E. 1976. A Lattice Model of Secure Information Flow. Communications of the ACM195236243, May 1976.DRUSCHEL P. AND ROWSTRON, A. 2001. PAST ALargeScale, Persistent PeertoPeer Storage Utility,HotOS VIII, Schloss Elmau, Germany, May 2001.EDWARDS, W., MYNATT, T., PETERSEN, K., SPREITZER,M., TERRY, D., THEIMER, M. 1997. Designing andImplementing Asynchronous Collaborative Applications with Bayou. Proceedings of the Tenth ACMSymposium on User Interface Software and Technology UIST, Banff, Alberta, Canada, October, 1997.ENDEAVORS TECHNOLOGY 2001. www.endeavors.com.EXODUS 2001. www.exodus.comFASTTRACK. 2001. Product Description. www.fasttrack.nuindexint.html.FATTAH, H.M., FATTAH, H.M. 2002. P2P How PeertoPeer Technology Is Revolutionizing the Way We DoBusiness, Dearborn TradeFOSTER, I. 2000. Internet Computing and the emergingGrid. Nature, Dec. 7.FOSTER, I., KESSELMAN, C. 1999. The Grid Blueprintfor a New Computing Infrastructure. Morgan Kaufman Publishers, Inc. San Francisco, California.FOLDINGHOME 2001. foldingathome.stanford.edu.FREENET. 2001. The FreeNet home page, freenet.sourceforge.net. www.freenetproject.org.GABBER, E., GIBBONS, P., KRISTOL, D., MATIAS, Y.,AND MAYER, A. 1999. Consistent, yet anonymous,Web access with LPWA. Communications of theACM. 42, 2. February 1999. pp. 4247.GARTNER CONSULTING 2001. The Emergence of Distributed Content Management and PeertoPeer Content Networks Engagement. Report 010022501.GENOMEHOME. 2001. genomeathome.stanford.edu.GNUTELLA. 2001. The Gnutella home page, gnutella.wego.com.GOSCINSKI, A. 1991. Distributed Operating Systems.The Logical Design. Addison Wesley Publishing.GRAHAM, R.L. 2001. Traditional and NonTraditionalApplications PeertoPeer Networks. Lecture.www.ida.liu.seTDTS43tdts4310peertopeer.pdf.GRIBBLE, S., HALEVY, A., IVES, Z., RODRIG, M., ANDSUCIU, D. 2001. What Can PeertoPeer Do for Databases and Vice Versa Proceedings of the WebDBWorkshop on Databases and the Web, Santa Barbara, CA, USA.GRID COMPUTING BIOTECHNOLOGY. 2001. www.gridcomputing.net.GRID FORUM. 2001. www.gridforum.org.GRIMSHAW, A., WULF, W.A., AND THE LEGION TEAM.1997. The Legion Vision of a Worldwide VirtualComputer. Communications of the ACM, 4013945.GRIMSHAW, A., Easy to use Objectoriented Parallel Programming with mentat. IEEE Computer, pp3951,May 1993.GRIMSHAW, A. S., WULF, W. A., FRENCH, J. C., WEAVER, A. C., REYNOLDS JR., P. F. 1994. Legion TheNext Logical Step Toward a Nationwide VirtualComputer. UVa CS Technical Report CS9421,June 8, 1994.45GROOVE NETWORKS. 2000. Introduction to Groove.Groove Networks White Paper.GROOVE NETWORKS. 2000. Connection Age. GrooveNetworks White Paper.GROOVE NETWORKS. 2001. Groove Networks ProductBackgrounder. Groove Networks White Paper.www.groove.netpdfgrooveproductbackgrounder.pdf.GROOVE NETWORKS. 2001. Groove and .NET Brief.Groove Networks Brief. www.groove.netfeaturewebandpeergrooveanddotnet.pdf.HAC, A. 1989. Load Balancing in Distributed SystemsA Summary. Performance Evaluation Review,161725.HALL GAILEY, J. 2001. Introducing .NET My Services.MSDN Library note, msdn.microsoft.comlibrarydefault.aspurllibraryenusdndotnethtmlmyservintro.asp. September 2001.HEYLIGHEN, F. 1997. Principa Cybernetica Web.pespmc1.vub.ac.beSELFORG.html.HOUSTON, B. 2001. The GnutellaMandragore virus.www.exocortex.orggnutella.HOWARD, J.H., KAZAR, M.L., MENEES, S.G., NICHOLS,D.A., SATYANARAYANAN, M., SIDEBOTHAM, R.N.,WEST, M.J. 1988. Scale and Performance in a Distributed File System. ACM Transactions on Computer Systems, Feb. 1988, Vol. 6, No. 1, pp. 5181.HOWES, T., SMITH, M. 1997. LDAP. Programming DirectoryEnabled Applications with Lightweight Directory Access Protocol. MacMillan TechnicalPublishing, USA.ICQ 2001. web.icq.com.IEEE 1990. Institute of Electrical and Electronics Engineers. IEEE Standard Computer Dictionary A Compilation of IEEE Standard Computer Glossaries. NewYork, NY 1990.IEEE 2001. www.ieee.org.INTEL 2001. PeertoPeerEnabled Distributed Computing. Intel White Paper.CARPENTER. B. 2000. Internet Transparency. InternetSociety Request for Comments 2775. www.faqs.orgrfcsrfc2775.html.JXTA 2001.The JXTA home page www.jxta.org.KASREL, B. 2001. P2Ps Pervasive Future. The ForresterReport.KATZENBEISSER, S. 1999. Information hiding techniquesfor steganography and digital watermarking. ArtechHouse Books.KaZaA. www.kazaa.com. 2001.KINDBERG, T. 2002. Personal Communication.KOTZEN, M. 2001. Dramatic Improvements in the Gnutella Network Since Late 2000. www.limewire.comindex.jspnetimprovements.KUBIATOWICZ, J., BINDEL, D., CHEN, Y., CZERWINSKI,S., EATON, P., GEELS, D., GUMMADI, R., RHEA, R.,WEATHERSPOON, H., WEIMER, W., WELLS, C., ANDZHAO, B. 2000. OceanStore An Architecture forGlobalScale Persistent Storage. Proceedings of theNinth international Conference on ArchitecturalSupport for Programming Languages and OperatingSystems ASPLOS 2000, November 2000.LEIGH, P. AND BENYOLA, P., 2001. Future Developments in Peer Networking. Equity Research, WhitePaper, Raymond James  Associates, INC.LIMEWIRE 2001. www.limewire.com.LITZKOW, M. AND SOLOMON, M. 1992. SupportingCheckpointing and Process Migration outside theUNIX Kernel. Proceedings of the USENIX WinterConference, pages 283290.LITZKOW, M., LIVNY, M., AND MUTKA, M. June 1988.Condor  A Hunter of Idle Workstations. Proceedings of the 8th International Conference on Distributed Computing Systems, pages 104111.MICROSOFT. 2001. Microsoft .NET Passport TechnicalOverview. September 2001.MICROSOFT. 2001a. Microsoft Windows 2000 DataSheet. www.microsoft.comeducationIDWin2kDatasheet.MILGRAM, S. 1967. The Small World Problem. Psychology today. 1, 61.MILLER, M. 2001. Discovering P2P. Sybex, to appear.MOCKAPETRIS, P. 1989. DNS Encoding of NetworkNames and Other Types. Internet Request For Comment 1101 Network Information Center, SRI International, Menlo Park, California.MOJONATION. 2001. httpmojonation.net.MORE, M. 2001. P2P Getting Down to Business. ManyWorlds White Paper. www. manyworlds.com.MOORE, D., HEBELER, J. 2001. PeertoPeer BuildingSecure, Scalable, and Manageable Networks.McGraw Hill.MORGAN, J. 2002. Personal Communication.NAPSTER. 2001. The Napster home page, www.napster.com.NATARAJAN. A., ET AL, Studying Protein Folding on theGrid Experiences Using CHARMM on NPACI under Legion. High Performance Distributed Computing 10, August 79, 200146NECULA, G. 1997. ProofCarrying Code. Proceedings ofthe 24th Annual ACM Symposium on Principles ofProgramming Languages, pp 106119. Paris France,January 1997.NECULA, G., AND LEE, P. 1998. The Design and Implementation of a Certifying Compiler. Proceedings ofthe ACM SIGPLAN Conference on ProgrammingLanguage Design and Implementation, pp 106119.Montreal, Canada, pp. 333344, June 1998.NETMEETING BY MICROSOFT 2001. www.microsoft.comwindowsnetmeetingdefault.asp.NETHISINGHE, S., 2001. TV or the PC Consumer Perspective. Philips Presentation.NOWITZ, D. 1978. UUCP Implementation Description.UNIX Programmers Manual, Bell Laboratories, October, 1978.OBJECT MANAGEMENT GROUP OMG 2001.www.omg.org.OMG. 1996. Common Object Request Broker Architecture and Specification. Object Management GroupDocument Number 96.03.04.OPENCOLA 2001. www.opencola.comOZZIE, J. 2000. SYSTEMS INTEGRATION WITH GROOVE.Groove Networks Presentation. devzone.groove.netlibraryPresentationsGrooveSystemsIntegratoin.ppt.PERKINS, C., 2001. Ad Hoc Networking. Addison Wesley.PEERTOPEER WORKING GROUP. 2001.www.p2pwg.org.PEERTOPEER WORKING GROUP. 2001. BidirectionalPeertoPeer Communication with Interposing Firewalls and NATs. p2pwg White Paper, Revision0.091. May 23, 2001. httpwww.peertopeerwg.orgtechnat.PFITZMANN, A., WAIDNER, M. 1987. Networks withoutUser Observability. Computer Security 2, 6, pp.158166.PLATFORM COMPUTING. 2001. www.platform.com.RAMANATHAN, M. K., KALOGERAKI, V. PRUYNE, J.2001. Finding Good Peers in the PeertoPeer Networks. HPL Technical Report HPL2001271.RATNER, D. 1998. Roam A Scalable Replicated Systemfor Mobile and Distributed Computing. UCLA Technical Report UCLACSD970044.RATNASAMY, S., FRANCIS, P., HANDLEY, M., KARP, R.,SHENKER, S. 2001. A Scalable ContentAddressableNetwork. Proceedings of the SIGCOMM, pp 161172.REITER, M. K., RUBIN, A. D. 1998. Crowss Anonymityfor Web Transactions. ACM Transaction on Information and System Security 1, 1, November 1998, pp6692.RHEA, S., WELLS, C., EATON, P., GEELS, D., ZHAO, B.,WEATHERSPOON, H., AND KUBIATOWICZ, J. 2001.MaintenanceFree Global Data Storage. InternetComputing, vol. vol 5, no 4. pp 4049. September2001ROSENBERRY, W., KENNEY, D., AND FISHER, G. 1992.Understanding DCE. OReilly  Associates, Inc.ROMAN, G., HUANG, Q., HAZEMI, A. 2001. ConsistentGroup Membership in Ad Hoc Networks. Proceedings of ICSE, Toronto, Canada.ROWSTRON, A. AND DRUSCHEL, P. 2001. Storage Management and Caching in PAST, a LargeScale, Persistent, PeertoPeer Storage Utility. Proceedings ofSOSP, pp 188201.SALTZER, J.H., REED, D.P., CLARK, D.D. 1984. EndToEnd Arguments in System Design, ACM TOCS, Vol2, Number 4, November 1984, pp 277288.SANDBERG, R., D. GOLDBERG, S. KLEIMAN, D. WALSH,B. LYON. 1985. Design and Implementation of theSun Network Filesystem. USENIX Conference Proceedings, USENIX Association, Berkeley, CA, Summer 1985.SAROIU, S., GUMMADI, P., AND GRIBBLE, S. 2002. AMeasurement Study of PeertoPeer File SharingSystems. MMCN, San Jose, CA, USA January 2002.SATYANARAYANAN, M. 2001. Pervasive Computing Vision and Challenges. IEEE Personal Communications, August 2001.SATYANARAYANAN, M., KISTLER, J.J., KUMAR, P.,OKASAKI, M.E., SIEGEL, E.H., AND STEERE, D.C.1990. Coda A Highly Available File System for aDistributed Workstation Environment. IEEE Transactions on Computers, pages 447459,vol 39 1990.SCARLATA, V., LEVINE, B. N., SHIELDS, C. 2001. Responder Anonymity and Anonymous PeertoPeerFile Sharing. ICNP01.SCHWARTZ, E. 2001. Parasitic Grid Wireless MovementMay Threaten Telecom Profits. August 24, 2001. httpwww.infoworld.comarticleshnxml010824010824hnfreewireless.xml0827mnam.SHA1. 1997. American National Standards Institute,American National Standard X9.30.21997 PublicKey Cryptography for the Financial Services Industry  Part 2 The Secure Hash Algorithm SHA1.SHAMIR, A. 1979. How to share a secret. Communications of the ACM, vol. 22, n. 11, pp. 612613, Nov.1979.SHILEDS, C., LEVINE, B. N. 2000. A protocol for Anony47mous Communication Over the Internet. 7th ACMConference on Computer and Communication Security ACM CCS 2000, November 2000.SHIRKY, C. 2001. What is P2P... and what Isnt. An article published on OReilly Network.www.openp2p.comlptap2p20001124shirky1whatisp2p.html.SEARCHLING. 2000. SearchLing PeertoPeer Networking. White Paper. October 2000.SETIHOME 2001. setiathome.ssl.berkeley.edu.SINGH, M., 2001. Peering at PeertoPeer Computing.IEEE Internet Computing, 5145.SINGH, M., YU, B., VENKATRAMAN, M. 2001. CommunityBased Service Location. Communications of theACM, 4444954.SOFTWAX 2001. SoftWax Technologies. www.softwax.com.STOICA, I., MORRIS, R., KARGER, D., KAASHOEK, F.,AND BALAKRISHNAN, H. 2001. Chord A ScalablePeertopeer Lookup Service for Internet Applications. Proceedings of the SIGCOMM, pp 149160.STROM, D. 2001. Businesses Embrace Instant Messaging. January 2001 enterprise.cnet.comenterprise0953474403317.html.SULLIVAN III, W. T., WERTHIMER, D., BOWYER, S.,COBB, J., GEDYE, D., ANDERSON, D. 1997. A newmajor SETI project based on Project Serendip dataand 100,000 personal computers. Astronomical andBiochemical Origins and the Search for Life in theUniverse. Proc. of the Fifth Intl. Conf. on Bioastronomy, Colloq. No. 161, eds. C.B. Cosmovici, S. Bowyer, and D. Werthimer Publisher EditriceCompositori, Bologna, Italy.SUTHAR, P. AND OZZIE, J. 2000. The Groove PlatformArchitecture. Groove Networks Presentation. devzone.groove.netlibraryPresentationsGrooveApplicationArchitecture.ppt.SYVERSON, P. F., GOLDSCHLAG, D. M., REED M. G.1997. Anonymous Connections and Onion Routing,1997 IEEE Symposium on Security and Privacy. pp.4453.TANENBAUM, A. S. 1981. Computer Networks, PrenticeHall International.TANENBAUM, A. S. 1992. Modern Operating Systems.Prentice Hall International.TEXAR CORPORATION. 2001. PeertoPeer ComputingIssues and Opportunities in Information Sharing. AWhite Paper.TOADNODE 2001. www.toadnode.com.VARELA, F., H. MATURANA, AND R. URIBE, 1974. Autopoiesis The Organization of Living Systems, ItsCharacterization and a Model. Biosystems, Vol. 51974, pp. 187196. Amsterdam NorthHolland.VEYTSEL, A. 2001. There is no PtoP Market... ButThere is a Market for PtoP. Aberdeen Group Presentation at the P2PWG, May 2001.XDEGREES 2001. www.xdegrees.com.W3C Note 08 May 2000WALDMAN, M., RUBIN, A. AND CRANOR, L 2000. Publius A Robust, TamperEvident, CensorshipResistant Web Publishing System. Proceedings of theUSENIX Security Symposium. Denver, Colorado,USA. Aug, 2000.WALDO, J., WYANT, G., WOLLRATH, A., KENDALL, S.1997. A Note on Distributed Computing, MobileObject Systems, Lecture Notes in Computer Science,No. 1222, pp. 4964, SpringerVerlag, Berlin D.WANG, Y.T. AND MORRIS, R. J. T. 1985. Load Sharingin Distributed Systems. IEEE Transactions on Computers, C343204217.WATERHOUSE, S., DOOLIN, D.M., KAN G., FAYBISHENKO, Y. 2002. Distributed Search in P2P Networks.IEEE Internet Computing 616872. JanuaryFebruary.WATTS, D.J. The Dynamics of Networks between Orderand Randomness. Princeton University Press.WATTS D.J. AND S. STROGATZ, H. 1998. Collective dynamics of smallworld networks, Nature 393, 440442.WILLIAMS RICE, A. AND MAHON B. 2000. Peer Networking. Deutsche Banc Alex. Brown White Paper.WOLLRATH, A., ET AL. 1996. A Distributed Object Model for the Java System. Proceedings of the USENIX1996 Conf. on ObjectOriented TechnologiesCOOTS, pp. 219231.XU, Z., MILLER, B., AND REPS, T., 2000. Safety Checking of Machine Code. Proceedings of the SIGPLANConference on Programming Language Design andImplementation, pp 7082. Vancouver B.C., Canada.June 2000.XU, Z., REPS, T., AND MILLER, B., 2001. TypestateChecking of Machine Code. Proceedings of the 10thEuropean Symposium on Programming, pp 335351.Genova, Italy, April 2001. Lecture Notes in Computer Science 2028, G. Goos, J. Hartmanis and J. vanLeeuwen Eds.YAGA 2001. Yaga, The Digital Market Place. www.yaga.com.YAHOO 2001. www.yahoo.com.48YANG, B GARCIAMOLINA, H. 2001 Comparing Hybrid PeertoPeer Systems. The VLDB Journal, pp561570, September 2001.ZHAO, B., KUBIATOWICZ, J., JOSEPH, A. 2001. TapestryAn Infrastructure for FaultTolerant Widearea Location and Routing. Computer Science Division, University of California, Berkeley Technical Report No.UCBCSD011141, April 2001.ZHOU, S., ZHENG, X., WANG, J., AND DELISLE, P. 1994.Utopia A Load Sharing Facility for Large, Heterogeneous Distributed Computer Systems. SoftwarePractice and Experience, 23213051336, December.49APPENDIX A P2P VS. ALTERNATIVESP2P is not a solution to every problem in the future ofcomputing. Alternatives to P2P are traditional technologies, such as centralized systems and the clientservermodel. One potentially interesting observation is thatthese two alternatives to P2P are a hardware architectureand a programming model, which seems like a comparison of apples and oranges. This can be particularly awkward as we compare these solutions. However, P2P isindeed both an architecture how we design and implement systems and a programming model how we program algorithms and in general conceive the systems andapplications.Centralized systems possibly organized in small clusters are represented by mainframes and supercomputersfor execution of computeintensive applications andlarge batch jobs, and highend servers for datacontentsharing and for collaborative applications. Internet DataCenters are also a form of centralizedpointofcontroleven if distributed installation of a large number of machines serving one purpose  content sharing.The clientserver model has different implementations,including on top of clusters, such as Platform ComputingLSF, widearea middleware systems, such as CORBA,RMI, and Webbased solutions, such as a Web server offering services to clients.Some of the strengths of centralized and clientserversystems include wellunderstood programming models,guaranteed performance, strong security, and wellknown solutions for reliability. A lot of work has alreadybeen standardized and used for several decades. Thesesystems are managed from a central point of control enabling good insight into the status and straightforwardchanges to configuration and parameters. On the negative side, these systems have limits to scalability, and alot of them are proprietary and consequently hard tochange. Centralized systems are costly to own and maintain and hard to deploy on a wide scale, such as in pervasive computing.In Table 11, we summarize various types of P2P systemsthat will be compared in the rest of the section. We haveclassified P2P systems into those supporting distributedcomputing, data sharing, and collaboration, and into platforms. As alternatives, most of the P2P systems haveWebbased tools, and some have traditional distributedsystems. The key features of these systems are scalability by avoiding the bottlenecks of centralized solutionssystems are better able to scale, selforganization, causedby individual peers continuously going up and down, theadhoc nature of peers that establish communicationchannels for collaboration, and interoperability as required by platforms.In the following three tables, we compare each class ofP2P system with its alternatives in more detail. InP2P SolutionsComparison CriteriaExample Systems Alternatives Key FeatureTypical TargetArchitectureDistributed Computing Avaki, Entropia, DataSynapse, MojoNation, UnitedDevices, cancerhome, SETIhomegrids, clusters,supercomputersscalability, selforganizationpersonalcompanyowned computersData Sharing Napster, Gnutella, Aimster, Freenet, DFS, Webbased availability InternetCollaboration Magi, Groove Cybiko, desktop, Webbased adhoc nature Internet,adhoc networksPlatforms JXTA, .NET, OS, Web, traditionaldistributed systems, interoperability Internet, enterprisesTable 11. The P2P Types of Systems.SolutionsComparison CriteriaExample InfrastructureDecentralizationScalabilitymaximumAnonymity SelfOrganiz.Cost ofOwnershipAdHoc PerformanceSecurityThreatsTransparencyFaultResilienceInteroperabilitySupercomputersHighEnd ServersIBM Sysplex, Compaq, Sun, HPstandalonesmall LANsomewhat a few low low high lowhigh forsmallminimal lowelaborate atall levelslowClusters of PCs Condor, LSF Intranet,LANsstrong a few 100 low low moderate medium low highcheckpointrestartfailovermediumP2P Avaki,SETIhomeInternetintranetssignificantSETI500000 high highlow distributedhigh highrestart onfailure,tolerate disconnectionlowTable 12. Comparison of Distributed Computing Solutions.50Table 12, we compare P2P solutions for distributed computing with clusters of PCs, supercomputers and highend servers, and grids. The earlier systems, such as highend servers and supercomputers, run either standalone orin intranets. Examples are IBM mainframes clusters ofSysplex machines, standalone or clusters of highendUNIX servers. They have small scalability, elaboratefaulthandling, and extensive security, but high cost ofownership. They are very expensive to obtain and tomaintain.Clusters represent a transition to P2P solutions in thatthey scale more compared to highend systems, up to afew hundreds e.g., LSF claims to have scaled to over500 hundred machines, but they compromise in terms ofsecurity, fault handling, and cost of ownership. Similarlyto highend systems, they are deployed within intranetsor even LANs.Finally, P2P solutions are deployed on the Internet wherethey have increased scalability e.g., up to 500,000 in thecase of SETIhome, but they are very vulnerable to security attacks and have implemented only the coarsestfailure tolerance such as restarting failed tasks. However,their cost of ownership is very small. It can be as little asthe cost of an average PC.Figure 25 shows how scalability increases in the transition from highend solutions over clusters of computersto P2P solutions. On the same plot, the cost of ownershipdecreases. Simultaneously, the graphs of security threatsare growing and fault tolerance are decreasing, but theseplots are omitted from the figure because of the lack ofquantitative comparison numbers.In Table 13, we compare collaborative P2P solutionswith two other collaborative solutions desktopbasedcomputing in an intranet environment and Webbasedcollaborative solutions. Historically, there seems to be aprogressive evolution of connectivity from intranetsthrough the Internet to adhoc wireless connectivity.However, the security threats are increasing proportionally with the increase in connectivity.In Table 14, we compare the solutions for context sharing with their historical alternatives distributed file systems, and Webbased publishing. Similar to othercomparisons, the cost of ownership is reduced, but security attacks are increased for P2P. In addition, consistency is decreased as well as availability, but anonymity ishighend clusters P2Pscalabilitycost of ownership100k10k1k101k1MFigure 25 Comparison of Distributed Computing Solutions.timecharacteristicsSolutionsComparison Criteria for Collaborative Computing SolutionsExample InfrastructureConnectivityDecentralizationScalabilitymaximumAnonymitySelfOrganizCost ofOwnershipAdhoc PerformanceSecurityThreatsTransparencyFaultResilienceInteroperabilityDesktopBasedNetmeeting,Lotus NotesIntranet limited low low low low high low medium low low low mediumWebBasedSharePoint,instant messagingAOL, YahooInternet Internetwide low medium medium low medium low high high low high highP2P Groove, MagiInternet,adhoc,enterpriseWorldwide high medium medium high low medium low very high high medium lowTable 13. Comparison of Collaborative Computing Solutions.SolutionsComparison Criteria for Context SharingExample PurposeInfrastructure ConsistencyDecentralizationScalabilitymaximum AnonymitySelfOrganiz.Cost ofOwnershipAdhocPerformanceSecurityThreatsTransparencyFaultResilienceInteroperabilityDistributedFile SystemsNFS,AFS,DFSgeneralpurposefile sharingClusters,WANs,Intranetstrong medium high restricted low high low NA minimal high high lowWebBasedWebpagesinformationsharingInternetmostly readonlymedium highby obscuritymedium low low NA high mediumInternetlimitedhighP2PNapster,GnutellacontentsharingInternet,adhocweak high high guaranteed highverylowhigh NAveryhighlowconnectivitylimitedlowTable 14. Comparison of Solutions for Context Sharing.51improved. Traditional file systems provided guaranteesfor acceptable information consistency at individualnodes in a Distributed File Systems DFS, modulo NFSproblems. Web based systems are largely consideredreadonly, even though there is increasingly more readwrite content. This significantly simplifies consistencyrequirements. P2P systems cannot make significantguarantees for consistency.Traditional DFSs are designed to be highly available andusers are guaranteed access to the data by techniques,such as replication, caching, and failover. On the Web,caching also takes place, but network connectivity problems are the major concern for consistency, with obsolete caches as the next level of problems. In P2P systems,there is an assumption that there will be a best effort foravailability. The content may or may not be availablesubject to connectivity and peer availability.Finally, anonymity is restricted in a DFS. It is possible onthe Web, largely by obscuring the identity. P2P systemsbring it to the next level, by guaranteeing anonymity. Forexample, in systems like Freenet, Publius, or Free Haven, anonymity is guaranteed to the reader, writer, andpublisher of the content.In summary, we can compare P2P solutions with traditional clientserver solutions based on the model seeFigure 4. Similarly, as in previous comparisons, the P2Psolutions have improved scalability, fault resilience, anonymity, security, and selforganization adhoc nature.
