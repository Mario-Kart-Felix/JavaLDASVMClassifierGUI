The Shape of FailureTaliver Heath, Richard P. Martin, Thu D. Nguyenftaliver, rmartin, tdnguyengcs.rutgers.eduDepartment of Computer ScienceRutgers University, Piscataway, NJ 08854Appears in Proceedings of the First Workshop on Evaluating and Architecting System dependabilitY EASY, July 2001.1 IntroductionThe construction of highly available Internet services is aninexact science at best. Complete and partial outages dueto a myriad of failures are still common. Indeed, as Internet services become woven into our everyday life, thesefailures are becoming important enough to make frontpage news.Part of the difficulty in constructing highly available services is that a modern Internet service is mindboggling incomplexity. The application, database, operating system,and firmware contain thousands of interfaces and millionsof lines of code. Worse, unlike many other highly available systems, the major components are created by different organizations with potentially different goals andreliability parameters no single entity has authority overthe design and implementation of the entire system. Thus,ensuring traditional faulttolerant techniques are followedtoptobottom is a difficult, if not impossible, organizational task.Fortunately, the situation is not completely intractable. Afirst step toward building failureresistant Internet servicesis to decompose the system into highlevel componentsseparated by welldefined interfaces. Next, the fault behavior of each component can be characterized in isolation. Then, armed with an understanding of componentbehaviors, the designer of a service can architect the system to tolerate many possible faults. Toward this end,we focus on quantifying the behavior of the workstationnodes comprising the core of many services.Our quantification takes the form of exploring the question what is the statistical distribution of reboots forworkstations The TimeToreBoot TTB is an important metric for service designers because it aggregates thefailure and repair rates of many system components including the operating system, the workstation hardware,and the operator, into a single number that can be associated to a component with welldefined interfaces. Although a broad metric, we argue that TTB is sufficientlypowerful and accurate to guide important design decisionsabout how to architect highly available services.To explore the above question, we obtained the last logsof two clusters operating in very different environmentsa cluster of 20 workstations in an undergraduate laboratory and a cluster of 20 workstations housed in a machine room and accessible only remotely by faculty members and graduate students. Our analysis shows the TTBfor both clusters is best modeled as a wiebull distributionwith a shape parameter  1. This has important implications. First, the distribution is not memoryless, suggesting models using exponentially distributed times may notadequately describe node failure behaviors.Second, a shape parameter  1 implies that workstationsbecome more reliable with time. That is, we can modela workstation as a component that becomes increasinglyreliable the longer it has been operating. This has important ramifications for request distribution, data partitioning, and software rejuvenation.In the remainder of the paper, we first describe how weobtained our reboot data and the method we used to perform the analysis. Next, in the results section, we discusssome implications of our observations. We then presentsome recent related work. Finally, we conclude with possible reasons why we observed the weibull distributionsand offer avenues of future research.2 MethodolgyWe gathered and analyzed data from two clusters. Thefirst cluster consisted of 20 Sun Ultra1 workstations running Solaris 5.8. All the machines were physically housedin one undergraduate laboratory and used exclusively forjunior and senior computer science students. All machines were managed by a single system administrator.The second cluster consisted of 20 machines that werephysically housed in our machine room and were accessible only via remote login to faculty and some graduatestudents. 17 of these machines were Sun Ultra1 workstations and 3 were Sun Sparc20 workstations. Unlike thefirst cluster, the supervision of these machines was spreadacross several different people that did not necessarily coordinate maintenance schedules.11502002503003504004505005500 1000 2000 3000 4000 5000 6000 7000Number of Machines that failed by Time XTime X Minutes10020030040050060070080090010000 50000 100000 150000 200000 250000Number of Machines that failed by Time XTime X Minutesa bFigure 1 Number of failures as a function of time a Machine Room and b Undergrad Cluster failures plotted as if therewere 521 and 979 machines, respectively, all started simultaneously, and times of failures recorded and plotted.We used the last logs to experimentally observe a machines TTB history. The last logs stretched back between6 months and 2 years. The undergraduate lab computers, which we call the undergrad cluster, recorded 979 reboots. The faculty computers, which we call the machinecluster, recorded 521 reboots.The first step in our analysis is to compute the TTB fromthe restart information in the last logs. Note that there isa discrepancy between our observed TTB times and actual node uptimes, as the TTB includes down times. Wewere unable to remove this inaccuracy, however, becausemost machines did not record the crashhaltshutdowntime when they went down. However, we believe this inaccuracy would not impact our conclusions. We are currently investigating the relative size of difference betweenthe uptime and TTB.To examine the distribution of the data, we assume thatthe data is independent of reboots on other systems, orreboots on the same system. We will examine the validity of this assumption in the next section. We simulatedan experiment with the cluster data by assuming that allcomputers were started at the same time, and then plotting the number of failures as time passes. This plot canbe seen in Figure 1. Using these plots, we can determinethe distribution of the data.3 ResultsWe attempted to fit our data to several distributions, including exponential, Weibull, Pareto, and Rayleigh. Foreach possible distribution, we used the Maximum Likelihood Estimates to approximate the parameters definingthe best instance of the distribution for our data. Afterfinding the parameters for a given distribution, we thenused a quantilequantile plot to compare the closeness ofthe distribution to our data that is, how accurately thisparticular distribution models our observed data.The quantilequantile plot we used is described in 2.This method arranges plots of known distributions againstthe given data. When the theoretical distribution matchesthe measured data, the quantilequantile plot will give astraight line. Any variation away from the straight line indicates a deviation from the theoretical distribution. Wethen define the best fitting distribution as the one with thesmallest residual from the leastsquare fitting of a straightline.Applying the above method, we found that the Weibulldistribution is the best matching distribution for data fromboth clusters. The quantilequantile plots and the resultingfitted curves can be seen in Figure 2. The parameters forthe best fit distribution is given in the following tableScale a Shape bMachine room cluster 99 hours 0.33Undergrad cluster 219 hours 0.49Since it fits the Weibull distribution with a shape parameter less than 1, we can see that the longer that a machinestays up, the less likely it will be rebooted in the near future. The equation for the probability density function ofthe Weibull distribution isfx  1 exa bThis analysis assumes that reboots between different systems are independent. It may be the case however, thatevents are related. For example, on the undergrad cluster the administrator would periodically upgrade systemsoftware, causing all of the systems to undergo reboots202000004000006000008000001e061.2e060 50000 100000 150000 200000 250000 300000 350000 40000001000002000003000004000005000006000000 50000 100000 150000 200000 250000a bFigure 2 Fitting the Weibull Distribution to the TTB observations. Quantilequantile plots showing the best fit curvefor the a Machine Room Cluster and the b Undergrad Room Cluster.00.10.20.30.40.50.60.70.80.910 50000 100000 150000 200000 250000 300000 350000 400000Theoretical00.10.20.30.40.50.60.70.80.910 50000 100000 150000 200000 250000Theoreticala bFigure 3 Fitting Weibull Distributions to the TTB observations. The resulting fitted curves are shown superimposed onthe orginal data for a the machine room cluser and b the undergrad cluster.at approximately the same time. However, the machineroom systems did not have this problem, so we believethat while these periodic reboots may have altered the distribution parameters slightly, they did not effect the general class of the distribution. An investigation as to correlation of reboots across machines in beyond the scope ofthis work.4 Related WorkThis short investigation does not include an exhaustive listof related work. Rather, in this section we present recentwork most closely related to our own.Perhaps the closest work related to this study is a recentwork characterizing the behavior of of Microsoft Windows NT machines 3. That study produced a detailedstatemachine model of a workstation node, and thus requires fairly detailed knowledge of Windows NT. Anotherrecent work examined the failure of components of an online image service 1. That study focused more on thehardware components rather than the entire node.5 ConclusionWhen viewed as a complete system of hardware and software, we have found no indication that a system that hasbeen up for an extended period of time is more likelyto be rebooted than any other system. In fact, our resultsshow the opposite for Sun workstations running Solaris5.8 in two of our departmental environments, a machine3that has been up for a long period of time should be leftup.This initial result has many interesting implications. Forexample, the assumption behind many software and hardware rejuvenation techniques, that components decayover time, may be flawed, at least with respect to nodesviewed in their entirety including hardware, operatingsystem, and application software. It may be the case that,since our data encompass a myriad of event types that canlead to workstation failures, the effect of component decay over time may have been overshadowed by other effects. Regardless, our results show that component decaydoes not seem to be the primary cause of workstation reboots so rejuvenation techniques are unlikely to be sufficient by themselves to make workstations more reliable.A second implication is that the classic bathtub curve ofhigh infant mortality, stable operating plateau, and finallyincreasingly failures may not apply to workstation clusters. However, our results did not cover timescales on theorder of many years. It may be the case that if we extendedour observations to these timescales, that we would seeincreasing failure rates. However, given that the useful operating range of computer hardware is about 3years, andsoftware even less, such long time regimes may be irrelevant to the construction of highly available services. Ourresults point to an avenue of future research that wouldquantify if such a bathtub curve exists for workstations ingeneral, and what the actual shape is. We need to gatherdata on many different systems over a longer period toanswer this question, however.A third conclusion our initial results point to is that whena system starts, it may be far from a clean state. Indeed, configuration errors of different components maywell cause the need for a subsequent reboots, leading tothe idea that a machines infant mortality period neverfully goes away. This would also lend credibility as towhy when a system has been up for a while, it tends tostay up. We have no evidence aside from our initial dataof this claim being true, but it does warrant further investigation.Finally, another interesting observation stems from thefact that although the undergraduate students had close,unsupervised physical contact with these systems, thisfact did not change the shape or scale parameters of theobserved TTB much. One would expect that machineswhere people could bump power cords and spill sodas onshould have an different TTB distribution, but this was notthe case.Future research in this area could be aided by installingclusters with more detailed load recording information,since this was something that was not kept well and wouldhave added much insight into other dimensions of failureanalysis. Also, most Unix systems encourage periodic removal of the last logs. This may have been due to diskspace concerns of the past, but these concerns have decreased with decreasing disk costs.Unfortunately, in addition to inadequate logs, the practices of many IT shops thwart data collection in this area.For example, we have extensive reboot histories from another IT shop that includes MacIntosh, Windows 98 andSolaris machines. However, our analysis was limited bythe policy of powering down the desktop machines everynight. In addition, those systems were rebooted betweenusers to reduce the chance of password spoofing. Eventheir Solaris servers were rebooted every week as a matterof course. In order to better understand the failure characteristics of machines, some machines must be left upas controlled experiments. Of course, data from companies actually running clusterbased Internet services, asopposed to the more individualoriented workloads thatclusters in an academic environment typically support,would be invaluable.References1 ASAMI, S. Reducing the cost of system administration of a disk storage system built from commoditycomponents. Tech. Rep. CSD001100, University ofCalifornia, Berkeley, 2000.2 JAIN, R. The Art of Computer Systems PerformanceAnalysis. John Wiley  Sons, 1991.3 M. KALYANAKRISHNAM, Z. KALBARCZYK, R. I.Failure Data Analysis of LAN of Windows NT BasedComputers. In 18th Symposium on Reliable and Distributed Systems, SRDS 99 1999, pp. 178187.4
