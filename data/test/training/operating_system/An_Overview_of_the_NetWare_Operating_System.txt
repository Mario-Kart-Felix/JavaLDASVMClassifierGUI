An Overview of the NetWare Operating SystemDrew MajorGreg MinshallKyle PowellNovell, Inc.AbstractThe NetWare operating system is designed specifically to provide service to clients over a computer network.  Thisdesign has resulted in a system that differs in several respects from more generalpurpose operating systems.  Inaddition to highlighting the design decisions that have led to these differences, this paper provides an overview of theNetWare operating system, with a detailed description of its kernel and its softwarebased approach to fault tolerance.1. IntroductionThe NetWare operating system NetWare OS was originally designed in 198283 and has had a number of majorchanges over the intervening ten years, including converting the system from a Motorola 68000based system to onebased on the Intel 80x86 architecture.  The most recent rewrite of the NetWare OS, which occurred four years ago,resulted in an open system, in the sense of one in which independently developed programs could run. Majorenhancements have occurred over the past two years, including the addition of an X.500like directory system for theidentification, location, and authentication of users and services.  The philosophy has been to start as with as simplea design as possible and try to make it simpler as we gain experience and understand the problems better.The NetWare OS provides a reasonably complete runtime environment for programs ranging from multiprotocolrouters to file servers to database servers to utility programs, and so forth.Because of the design tradeoffs made in the NetWare OS and the constraints those tradeoffs impose on the structure ofprograms developed to run on top of it, the NetWare OS is not suited to all applications. A NetWare program hasavailable to it an interface as rich as that available to a program running on the Unix operating system RITC78,but runs in an environment that is as intolerant of programming mistakes as is the Unix kernel.The NetWare OS is designed specifically to provide good performance for a highly variable workload of servicerequests that are frequent, overlapping, mostly of shortduration, and received from multiple client systems on acomputer network.  In this capacity, the NetWare OS is an example of a network operating system.  Generalpurposeoperating systems, on the other hand, are designed to provide appropriate levels of service across a wide spectrum ofdiffering workloads.Just as a generalpurpose operating system provides primitives required to act as a network operating system, theNetWare OS provides primitives required to develop most programs capable of running on a generalpurposeoperating system.The differences between a generalpurpose operating system and an operating system like the NetWare OS lie mostlyin the handling of tradeoffs between ease of programming and fault tolerance, on the one hand, and execution speed,on the other hand.Generalpurpose operating systems normally have a preemptive scheduler.  This frees the programmer frominvolvement in the systems scheduler.  This also increases fault tolerance by preventing a misbehaving programfrom monopolizing the system.  The NetWare OS, on the other hand, is nonpreemptive by design.  This makes thesystem more responsive under load and simpler and, therefore, faster by reducing the number of concurrency issues,such as the locking of data structures, that programs need to manage.Generalpurpose operating systems normally use hardware facilities to keep one program from incorrectly accessingstorage locations of another program.  This increases the fault tolerance in a system by isolating failures to separateregions of the system.  The NetWare OS allows all programs to run without hardware protection.  This reduces thetime required to communicate between programs and between a program and the NetWare OS.Generalpurpose, multiuser operating systems normally enforce a strong boundary between the kernel and userlevelprograms.  Additionally, any given instance of a userlevel program is often identified with exactly one end user forthe purpose of access control.  Both of these attributes contribute to the fault tolerance and security of such a generalpurpose operating system.  In contrast, the NetWare OS makes no formal distinction between kernel and userlevelprograms.  This allows the functionality of the operating system to be easily extended by other programs.Moreover, the NetWare OS does not prevent any program from assuming the identity of any end user for accesscontrol purposes.  In an environment in which one program may provide service to 1,000 network clientssimultaneously, it is more efficient to allow the program to assume different users identities as it services differentrequests.The purpose of this paper is to describe the NetWare OS in greater detail.  Section 2 introduces some terms used inthe paper and provides an overview of the NetWare system.  Section 3 provides more detail on the kernel of theNetWare OS, including a detailed description of the scheduler, one of the keys to the performance of the system.Section 4 describes a software approach to fault tolerance known as server replication.  We conclude with a briefdiscussion of some future work.  For brevity, this paper does not describe the NetWare file system, networkprotocols and implementation, or other higher level services of the NetWare OS.2. Overview and TermsThis section defines certain terms and gives an overview of NetWare and the NetWare OS.2.1. General TermsAn operating system manages the basic resource sharing within a computer system.  In addition, an operatingsystem provides a suite of programming abstractions at levels somewhat higher than those provided by the hardwareon which it runs TANE92.The kernel of an operating system manages the sharing of hardware resources of the computer system, such as thecentral processing unit CPU and main storage.  It also provides for interrupt handling and dispatching, interprocesscommunication, as well as, possibly, the loading of programs into main storage.The schedulable entities of a system are known as threads.  A thread consists of a program counter, other hardwareregister contents, and other state information relevant to its execution environment.While processing, we can distinguish between different system states as follows.  Interrupt time is the state thatoccurs after the processor has accepted an interrupt either hardware or software and before the software has returnedfrom the interrupt to the normal flow of control.  Processing during interrupt time normally occurs on the stack thatwas active when the interrupt occurred, or on a special stack reserved for interrupt processing.  Process time is thenormal flow of processing in which the kernel, operating system, and application programs run.  Processing duringprocess time occurs on the stack of the currently running thread.  There is a third state that we will call softwareinterrupt time that may occur at the end of other kernel activity LEFF89.  During this time, the kernel may decideto perform certain activities not directly related to the original reason for entering the kernel.  During this time,processing occurs on a borrowed or reserved stack.In the NetWare OS, software interrupt time occurs only when the scheduler has been entered see section 3.3.In general, execution is allowed to block only during process time.An upcall CLAR85 is a mechanism where a typically lower layer of a system can signal an event to a typicallyhigher layer in the system by means of a procedure call.  In the NetWare OS, these upcalls are known as EventService Routines ESRs or callback routines.Assume that the main storage of a system is divided into memory objects e.g., bits or bytes.  Assume that thestate of execution of a thread at a given time is summarized in an execution context that determines the value of theprogram counter and other pieces of state.  One element of a threads execution context is the threads rights to accessthe different memory objects of the system.  These access rights are enforced by the hardware of the system,specifically the virtual memory system, storage keys, or other hardware specific components.  In general, theseaccess rights include read, execute, readwrite, and none.  In the NetWare OS, only readwrite and noneare actually used.Using these concepts, we are in a position to define protection domain.  Two memory objects are said to be in thesame protection domain if and only if for each execution context X in the system, X has the same access rights toboth memory objects.  Similarly, two execution contexts are said to be in the same protection domain if and only iffor each memory object X in the system, both execution contexts have the same access rights to X.In the NetWare OS there is a privileged protection domain known as the kernel domain.  An execution contextwhose protection domain is the kernel domain has readwrite access to all memory objects in the system.We say that a memory object is in the protection domain of a given execution context except the kernel protectiondomain if and only if the execution context has readwrite access to the memory object.  We say that a memoryobject is in the kernel protection domain if and only if it is not in any other protection domain in the system.  Notethat protection domains induce a partition on all the memory objects and on all the execution contexts in the system.We then speak informally of a protection domain as an equivalence class of execution contexts and the relatedequivalence class of memory objects.2.2. NetWare Overview and TermsA NetWare system also known as a NetWare network or NetWare LAN provides for the sharing of servicesover a network.   The two principle components of the system are clients, which request service, and servers, whicharbitrate the requests and provide the service.1  Typical services include file systems, files, printer queues, printers,mail systems, and databases.  The network can either be a single subnet, or a collection of subnets interconnected byrouters.In a NetWare system, clients run off the shelf operating systems such as DOS, Windows, OS2, Unix, and theMacintosh operating system.  Depending on the specific client and on the service being requested, it may not benecessary to add any additional software to the client in order to participate in a NetWare system.  Macintosh andUnix clients, for example, typically come with builtin networked file system client software and thus dont requireadditional software to obtain file service in a NetWare system.1In certain configurations, both the client and server may run on the same computer.Servers in a NetWare system run the NetWare operating system, an operating system that has been designedspecifically as a platform for running programs that provide data and communications services to client systems.The NetWare OS currently runs on Intel 80x86 systems there is ongoing work to port the operating system to someof the current Reduced Instruction Set Computer RISC platforms.A NetWare server consists of the NetWare OS kernel and a number of NetWare Loadable Modules NLMs.  AnNLM is similar to an a.out file in the Unix operating system RITC78 it consists of code, data, relocation, andsymbol information for a program.  An NLM is generated by a linkage editor, which uses as input one or moreobject files as well as a control file.  The object files are generated by assemblers andor compilers from source files.The relocation information allows an NLM to be loaded at any address in the system.  NLMs also contain unresolvedexternal references that are resolved at load time as described in section 3.1.  Finally, NLMs may contain directivesthat allow them to export procedures for use by other NLMs again, this is described in section 3.1.  In this finalsense, NLMs resemble shared libraries GING89.The performance requirements for NLM loading are less stringent than those for the loading of Unix a.out filesbecause, in general, an NLM tends to run for a longer period of time than a Unix process and, therefore, does notneed to be loaded and unloaded as frequently as a Unix a.out file.The NetWare OS does not have the concept of a process.  Instead, the basic units are the NLM, a protection domain,and a thread.  In part, we do not use the term process because it has such different meanings in different systems.Depending on the system, there may be no memory sharing between processes, or there may be more than oneprocess in the same memory space there may be exactly one thread in a process, or there may be many threads in aprocess there may be exactly one executable in a process, or there may be multiple executables in a process.Although the NetWare OS has no concept of a process, we use the term process time section 2.1 for whatotherwise might be called thread time.Conceptually, there is no memory sharing between protection domains.  In practice, the kernel domain has readwriteaccess to other protection domains.  Additionally, in the current version of the operating system shared memory isused in the implementation of procedure calls between protection domains described in section 3.1.The following figure illustrates the conceptual structure of a typical NetWare server.  The dashed lines indicate theboundaries between the various layers in the system these boundaries, like the lines that denote them, are not soliddivisions because one NLM may provide service at several levels.kernelprogram loadermemory managementschedulerinterrupt handlingdispatchingconsole IO, switchinglow level debuggerdisk drivers network interface drivers other driversfile system protocol stacksIPXTCPIPAppleTalkfile serversNetWareNFSMacintoshprint serversNetWareUnix lprMacintoshdatabase serversOracleSybaseGuptaetc.mail and messaging serversother utilitiesmonitoringinstallationconfigurationcommand interpretersgamesetc.other serversC language runtime librarydirectory servicesOutside of the kernel, each separate piece in the above figure consists of one or more NLMs.  Thus, NLMs can bedevice drivers, higher level parts of the operating system, server processes, or utilities.3. The NetWare KernelThe NetWare kernel includes a scheduler, a memory manager, a primitive file system with which to access otherpieces of the system during the process of booting up the system, and a loader that is used to load NLMs.  Incontemporary usage, the NetWare kernel might be known as a microkernel GIEN90.3.1. The LoaderCurrently, all NLMs share the same address space, though this address space can be divided up into variousprotection domains.2  By default, all NLMs are loaded into and run in the kernel domain.  At the time an NLM isloaded, however, the user can specify a protection domain into which to load the NLM.  All NLMs resident in agiven protection domain have shared memory access to each others memory.  The kernel domain has readwriteaccess to memory in all other protection domains.The loader maintains a symbol table in order to resolve external references when loading NLMs.  When the systemfirst loads, this symbol table contains one entry for each procedure exported by the NetWare kernel.At load time, any unresolved external references in an NLM will be bound using the symbol table.  Any symbol leftunbound after load time will cause the NLM load to fail.  There are runtime facilities that an NLM can use to test2In the current release of NetWare, only two protection domains are available.  This is a restriction that we hope to lift.the availability of a binding for a given symbol, to register for a notification in the event a given symbol changes itsbinding, and to determine the binding itself.In addition to importing procedures, an NLM can export procedures  that is, make them available to otherNLMs.  Procedures exported by an NLM will be added to the loaders symbol table when that NLM is loaded andremoved when the NLM is unloaded.When the NLM importing a procedure and the NLM, or NetWare kernel, exporting the procedure are running in thesame protection domain, a direct procedure call is made to invoke the procedure.When an NLM contains a call to a procedure in an NLM that is loaded in a different protection domain, an intramachine remote procedure call RPC BIRR84 is made to invoke the procedure.For example, assume that NLM A is loaded into a different protection domain than NLM B, but NLM Acontains a call to routine foo in NLM B see the following figure.In a traditional implementation of RPC, a calling stub user stub in BIRR84 linked into NLM A marshals theparameters of the call to foo, and then calls into the kernel to transfer the parameters and control to B.  A called stubserver stub in BIRR84 linked into NLM B unmarshals the parameters and invokes foo.  After foo returns, thecalled stub marshals any results and then calls into the kernel to return the results and control to the calling stub inA.  The calling stub unmarshals the results and returns them to the call point in NLM A that originally called foo.The NetWare implementation of RPC differs from the traditional RPC model in several respects.  The majordifference is that the calling and called stubs for exported procedures are automatically generated at load time by theNetWare kernel they are not linked into either NLM.  This isolates developers from knowledge of which procedurecalls are direct and which are remote, and allows for increased flexibility in configuring the system.  Second, thetransfer of parameters and results is done by the kernel, not the calling or called stubs.3  This process is driven bypseudocode which describes the types of all the parameters to a given procedure.  This gives the kernel the ability tomake certain optimizations based on the types of the parameters and on the location of the protection domainsinvolved.  Third, when a pointer to a callback routine is passed as a parameter in an RPC, the necessary calling stub,called stub, and kernel data structures are generated at run time.  This allows callback routines to be invoked acrossprotection domains.Loadtime creation of RPC stubs and kernel data structures is possible because exported routines are described in aninterface description language that allows a programmer to provide a full description of the parameters to eachexported routine.  These descriptions are similar to the function prototypes of ANSI C HARB91 but also allowfor the specification of other attributes of the parameters in particular, whether the parameter is an input parameter,an output parameter, or is both an input and an output parameter.  The NetWare interface description language issimilar in function to that of the Common Object Request Broker Architecture OMG91.3The current implementation of the NetWare kernel makes the calling stack shared between the two protection domains anduses it to transfer the parameters to the called protection domain.  Pointers in the parameters result in the kernel making theindicated memory shared between the calling and called protection domains.calling stubExportsNWopenNWclose...called stubNLM AImportscalled stubcalling stubNLM BExports ImportsfooNetWare kernelKernel protection domainOther protectiondomainKernel RPC codeKernel RPC codeIn this figure, NLM A is importing one routine from the kernel and another foo from NLM B.  NLM B is alsoimporting one routine from the kernel.The solid lines indicate direct procedure calls.  The dotted lines are cross domain procedure calls.  The calling and calledstubs are created at NLM load time by the NetWare OS loader.The choice of which NLMs should share protection domains involves a tradeoff between performance and faulttolerance.  The use of direct procedure calls gives the best performance, but exposes each NLM to failures occurringin the other NLM.  Loading communicating NLMs in separate protection domains gives good fault isolation butincurs the performance overhead associated with the RPC mechanism.  By basing the system on a procedure callinterface, with loadtime stub generation, the NetWare OS allows this tradeoff to be made comparatively late at loadtime.  In this sense, the NetWare OS is similar to Lipto as described in DRUS92.Exposing other interprocess communication mechanisms such as message passing or shared memory to theprogrammer forces a decision on the binding of program units to protection domains to be made at design time inorder to get good performance.  Messagepassing systems ACCE86 suffer performance problems when thecommunicating entities are in the same protection domain.  Shared memory systems LI89, on the other hand,suffer performance problems when the memory sharing is not provided by the hardware.  By basing a design oneither of these paradigms, future flexibility in system structure is reduced.  Basing the NetWare OS on the procedurecall paradigm allows the kernel to make use of direct procedure calls when the two communicating NLMs are in thesame protection domain, to make use of shared memory to carry out the RPC when that is available, and, in thefuture, to use message passing when the two NLMs are not in the same address space.3.2. The NLM Execution Environment Provided by the KernelWhen first loaded, an NLM is given one thread of control.  It can then create as many additional threads as it needs.Alternatively, the NLM can destroy its only thread, which leaves the NLM existing as a shared library GING89.Each NLM has access to a virtual screen and keyboard.  The kernel switches the physical system screen and keyboardin unison between the various virtual screens and keyboards in the system.In the Unix operating system, file descriptors 0, 1, and 2 refer to standard input, standard output, and standard erroroutput respectively RITC78.  The NetWare OS, on the other hand, does not assign any special meaning to thesefile descriptors.  However, the C runtime environment in NetWare does set up these descriptors as well as allow forIO redirection RITC78.The NetWare OS does not provide a separation between user space and kernel space.  All NLMs running on a serverhave the same security privileges though they may be loaded in different protection domains, and all NLMs haveaccess to the same set of importable procedures.  For example, the initialization code in an NLM may be writtenusing higher level programming abstractions, such as a standard C library PLAU92, while the more performancecritical portions of the NLM may use programming interfaces more tightly coupled to the NetWare OS.An NLM has associated with it certain resources such as main storage and open files.  There are mechanisms bywhich an NLM can transfer ownership of these resources to other NLMs.  By convention, an NLM is expected torelease all resources it owns before exiting, as part of a philosophy of having developers aware of which resourcesthey are using in order to minimize resource usage with longrunning NLMs.  However, if an NLM exits withoutreleasing all its resources, the resources it had owned are returned to the system, and the system console makes noteof the fact that the NLM had not released all its resources.As a thread executes, its program counter moves between NLMs.  This may involve moving between protectiondomains, as an NLM calls routines in a protection domain external to it.  Threads in the NetWare OS combine theSpring concepts of thread and shuttle HAMI93.The ability of threads in the NetWare OS to follow procedure calls into other NLMs and protection domains issimilar to facilities in other systems JOHN93 HAMI93 FORD94 FINL90.If a thread is blocked because of a blocking call to the file system, say, or awaiting network IO, it goes to sleepuntil it is unblocked.  When a thread is unblocked, it is put on the run list until scheduled for execution section 3.3discusses the scheduler in more detail.If an NLM running in a nonkernel protection domain terminates abnormally because, for example, of a memoryprotection check, its protection domain is quarantined.  Because of the nonpreemptive nature of the NetWare OS,exactly one thread is executing in the protection domain when it terminates, and that thread is suspended when thefailure occurs.4  Threads are not allowed to call into a quarantined domain if a thread attempts to call into aquarantined domain, this same mechanism is invoked to suspend the calling thread and quarantine its domain.Previous RPC calls from a quarantined domain are allowed to return to the quarantined domain.  Nonsuspendedthreads running in a quarantined domain are allowed to call out from the domain as well as return from the domain.An abnormal termination in the kernel protection domain is treated as a fatal error.Because the NetWare OS scheduler is nonpreemptive, it is up to each NLM to yield control of the CPU frequentlyin order that the system remain responsive.  Thus, if an NLM has sections that execute for long periods of timewithout blocking, it will be necessary for the NLM to occasionally call a yield function exported by the NetWarekernel and described in greater detail in section 3.3.  A programmer, in deciding how often to call yield, will attemptincrease the signal to noise ratio in the program, i.e., make sure that the cost of the yield call is amortized over asignificant amount of program processing.  This is especially true given that the vast majority of yield calls are,effectively, noops because there is no other thread ready to run.  The desire to process a long time between yieldsconflicts with the goal of having a very responsive system.  For this reason, it is important to reduce the cost ofdoing a null yield in order to encourage the programmer to execute them more frequently.  To this effect, in thecurrent NetWare OS, the cost of executing a yield is less than 20 machine instructions if there is nothing else in thequeue to run not including the cost of possibly crossing a protection domain boundary.  If there is other work to bedone, it takes less than 100 instructions before the new thread is running.In the case where the NLM calling a yield function is not in the kernel protection domain, an intramachine RPCmust be executed.  This significantly increases the cost of the yield call.  For example, in the currentimplementation on an Intel 80486 processor, such a call with no parameters adds approximately 50 machineinstructions of overhead for the call and the same number for the return.  Of each group of 50 machine instructions,however, 49 take approximately two machine cycles each the other instruction takes approximately 100 machinecycles by itself.In the future, it is possible to provide preemption in the NetWare kernel but to maintain a nonpreemptive executionmodel within a protection domain.  This means that a protection domain could be preempted by the kernel andanother protection domain scheduled to run.  However, any RPC calls into the preempted domain, or returns backinto the preempted domain from previous RPC calls to other protection domains, would need to be blocked until thepreempted domain has been rescheduled and itself issued a yield.  If this extension to the NetWare kernel were tooccur, then the yield calls in a given protection domain would be for sharing the CPU with other threads in thatprotection domain other protection domains would not be affected by the frequency of yield calls.  A sideeffect ofthis would be to increase the speed of a yield call, because it would not need to cross a protection boundary.Our experience is that the nonpreemptive nature of the NetWare OS is both a blessing and a curse.5  We end upwith a more deterministic system with fewer of the concurrency issues that are so difficult in many systems.  For4There are ways in which a thread can arrange to catch such a failure, making the failure look like a nonlocal return.Depending on the mechanism used, the protection domain in which the failure occurred may or may not be quarantined.5For example, the developers of some NLMs have assumed that file system accesses will block often enough to keep thedeveloper from having to explicitly yield the CPU.  Because of the fact that the NetWare file system performs so muchcaching, this assumption of NLMs quite often turns out to be false.  In order to allow these NLMs to run but not monopolizethe system, the NetWare file system will occasionally, but deterministically, perform a yield for an NLM, even if the filesystem operation did not need to block.example, multithreaded NLMs can avoid locking shared data structures if their use of these data structures does notexplicitly yield or call a service that might yield or block.Additionally, a nonpreemptive system, in our opinion, offers significant performance advantages over moretraditional preemptive systems.  As an example, in the NetWare file system, a read request on a file that has byterange locks set runs through the linked list that makes up the locking structure for the file.  In a preemptiveoperating system, this code section would have to be protected by disabling interrupts or by locking the linked list.However, in the NetWare OS, the file system code is written with the understanding that there will be no preemptionduring the processing of the read request.  Thus, the code path is both simpler and faster.  In a nonpreemptivesystem, the developer has more control of how often his or her program yields control of the system, and can usethis control to simplify the program.  Additionally, placing yield calls in a program which can be done late in theprogram development cycle has the added benefit of keeping the developer aware of how long the code paths actuallyare.  The developer effort and mindset necessary to program in this environment require a certain amount of educationfor the developer.  While committed to nonpreemption, we continue to explore new tools and other means to easethe burden on the NLM developer.3.3. The SchedulerThe NetWare OS was designed specifically for the purpose of providing service to networked clients.  Theperformance of servicing network requests is very sensitive to the scheduling policy of the server operating system.The NetWare scheduler is therefore tuned to give high performance in this specific environment.As mentioned above, the NetWare scheduler provides a nonpreemptive scheduling paradigm for the execution ofthreads.  A thread that has control of the machine will not have this control taken away from it unless one of thefollowing occurs it has called one of the yield calls see below it has blocked awaiting disk IO, for example it has returned to the scheduler for threads dispatched to service work objects  see belowIn particular, interrupts including timer interrupts do not cause a thread switch until, at least, the next yield call.In order to be both nonpreemptive and responsive, NLMs are explicitly required to offer to relinquish control of theCPU at regular intervals not to exceed 150 milliseconds.  They do this using one of three yield functionsThreadSwitch informs the scheduler that the thread is in a position to continue performing useful work ifno other thread is ready to run.  This is the normal form of yield for threads that are processing but want toallow other work to execute.ThreadSwitchWithDelay delays the thread until at least a specific number of yields from other threadshave been processed.  This is quite often used by a thread when it is blocked on a spin lock JONE80 inorder to give the thread holding the lock time to release the lock.ThreadSwitchLowPriority informs the scheduler that this thread has more work to do, but as a backgroundactivity.Having different yield calls allows the scheduler to determine how to treat the yielding thread, i.e., in whichscheduling class the thread belongs and the threads parameters in that class.  An alternative structure would have oneyield call and separate calls that a thread could make to set its scheduling class.  We are trying to keep threads aslightweight in terms of state as possible, so we prefer not to keep a perthread variable detailing the schedulingclass.  Additionally, if there were such a state variable per thread, each thread would need to manage this state as itmoves between different tasks in the system.  This is particularly problematic when coupled with the work objectsof the NetWare OS see below, in which a thread may be involved over time in many different parts of the system.Finally, if the scheduling class were part of a threads state, we would increase the number of branches in the queuinglogic, which may break the processors pipeline.  As it is, we know at the entry point to the specific yield routineon which queue we are going to place the thread, so the number of branches is reduced.There are six basic structures the scheduler uses in scheduling the CPUThe delayed work to do list is an ordered by pollcount  see below list of routines to call, together with aparameter for each routine.  Routines called from this list run at software interrupt time.The work to do list is a firstin, firstout FIFO list of routines to call, together with a parameter for eachroutine.  Routines called from this list run at process time.The run list consists of those threads that are ready to run at any given time.  This is a FIFO list.  Threadsrun from this list run at process time.The hardware poll list is a list of routines to be called to check on the status of different hardwarecomponents of the system.  In contrast to all the other lists mentioned here, entries on this list remain onthe list when their associated routines are called i.e., they must be explicitly removed.  Routines calledfrom this list run at software interrupt time.The low priority list consists of threads that are ready to run but have indicated that they should be run aftereverything else in the system has run.  This list is also serviced in FIFO order.  Threads run from this listrun at process time.Pollcount is a monotonically increasing sequence number that is incremented by one for every yield callmade.ThreadSwitch will cause the calling thread to be enqueued on the run list if control is taken away from the callingthread.ThreadSwitchLowPriority will cause the calling thread to be enqueued on the low priority list if control is takenaway from the calling thread.ThreadSwitchWithDelay will cause an entry to be created and enqueued on the delayed work to do list.6  This entrywill have its pollcount value set to the current pollcount incremented by the value of a system parameter the socalled handicap value, nominally set to 50.  This entry will point at a routine that takes as a parameter a threadhandle and enqueues that thread on the run queue.Note that with the exception of the delayed work to do list, all the lists used by the scheduler have a constantinsertion and removal time, helping make the scheduler efficient.  The delayed work to do list has a variable insertiontime, but the removal time is still constant.There are separate calls to place an entry on the work to do list and on the delayed work to do list.  Note that theentries on the work to do list run at process time while entries on the delayed work to do list run at softwareinterrupt time and thus have a more restricted execution environment for example, they cannot block.The scheduler basically imposes the following priority on processing in the system from highest to lowestinterrupts these run at interrupt timeentries on the delayed work to do list that have a pollcount less than or equal to the current value ofpollcount these run at software interrupt time6In fact, the entry for the work to do list is created on the stack, so no memory allocation actually occurs.entries in the work to do list these run at process timethreads on the run list7 these run at process timeentries on the hardware poll list these run at software interrupt timethreads on the low priority list these run at process timeThe reality is slightly more complex than the above list because, for example, the scheduler does not allow lowerpriority tasks to become starved because of higher priority activity.  For example, threads on the low priority list areguaranteed to be scheduled at least eighteen times per second  the frequency of the system clock.The delayed work to do list and the hardware poll list are mechanisms that allow us to trade off responsiveness forhigher system capacity by setting certain hardware devices such as network interface boards or serial line adaptersinto a mode in which they do not normally interrupt the system, but rather are serviced periodically by systemsoftware.  The hardware poll list is a regular, background, polling list.  The delayed work to do list can be used tospeed up polling a given hardware device when hardware or system loading makes this necessary.Entries on the work to do list described in more detail in the next section are usually invoked to begin processingon a newly received request.  Since requests frequently complete without yielding or blocking, calling these entriesbefore processing the run list has the effect of favoring new requests over older, slightly longer running requests.  If arequest blocks or yields the CPU during its processing, it enters the category of older, slightly longer running and,thus, will be scheduled behind newly arriving requests.The choice between preemption and nonpreemption in scheduling is basically a trade off and is based in part on thepredicted workload of the system.  The NetWare scheduler is optimized for a nonCPU intensive workload.  The codepath for a typical file system request, for example, is short enough that it contains no yield calls partly because thecode paths do not need to deal with concurrency issues.  Additionally, fast thread switch times on the order of 100machine instructions help extend the space of those workloads that perform well in the NetWare OS.  Certain CPUintensive workloads, with multiple threads contending for the CPU, might perform better in a preemptiveenvironment.3.3.1. Work ObjectsEarlier versions of the NetWare OS had separate pools of threads dedicated to each different service offered byNetWare e.g., DOS file service, Unix file service, Macintosh file service, DOS print service.  Each pool wascreated by a particular NLM in general.  As described in the preceding section, the scheduler looks for active threadsand schedules them to run.  A thread is activated, for example, when an incoming network message arrived on aconnection associated with the service provided by that thread.In this environment, each service was responsible for determining how many threads to create, heuristics for when tocreate new threads or terminate existing threads as the system load changed over time, etc.  Implementing thesefunctions, while not difficult, imposed an overhead on the programmer.  Additionally, since each thread consumes acertain amount of real memory for such objects as stack space or control blocks, this scheme made inefficient use ofmemory.In the most recent version of the NetWare kernel 4.0, a new paradigm for scheduling work has been introduced.This paradigm takes advantage of the fact that NetWare threads carry essentially no state while waiting for a request7Note that threads that have called ThreadSwitchWithDelay reside on the delayed work to do list but actually have apriority lower than all the threads on the run list.  This is because when the entry on the delayed work to do list is actuallyexecuted, it will have the effect of enqueuing the thread at the end of the run list.to service, and so are interchangeable.  Thus any thread is often as good as any other thread in performing a service,as long as the call to the service carries the correct parameters.As mentioned in the previous section, the work to do list consists of a list of tuples, known as work objects.Each work object contains the address of a procedure to call, and a parameter to pass to that procedure.  As anexample of how this works, consider the following figure  a received packet interrupts the system, causing theprotocol stack to demultiplex the packet to a given connection.  The protocol stack, in the example, builds a workobject consisting of the address of an ESR that has been registered for this connection and the address of theincoming packet.  The protocol stack then calls into the NetWare scheduler to enqueue the new work object on thework to do list, and then returns from the interrupt.Packet comes inDevice DriverSystem CodeProtocol StackWork to do listSchedulerEvent Service Routinecall returnInterruptInterrupt time Process TimeSystem threadAt process time during a yield call, or when blocking another thread or because the system was idle when theinterrupt occurred the scheduler examines the work to do list before looking in the run list.  In this case, the work todo list is not empty.  If the thread running in the scheduler is an otherwise idle system thread as is often the case, itwill call the ESR associated with the first work object directly, with no context switch.  Otherwise,  a thread will beallocated from a pool of threads managed by the NetWare kernel and will call the ESR.  In either case, the ESR willbe passed a parameter in the example, the address of the incoming packet.The ESR may call other routines, block, and so forth, until it has finished the processing associated with theincoming packet and has returned.  The ESR, or any other routine in the processing of the incoming request, may atits option install the extra state that a thread may have.8  When the ESR returns, its thread runs the kernelscheduling code, possibly picking up and servicing another entry from the work to do list.Note that in the above example the transition into the protocol stack could also have been called at process time byusing work objects.Moving away from perNLM threads has allowed the system to be run with a much smaller number of workerdaemon threads, resulting in more efficient use of memory and increased productivity for NLM programmers.3.4. PagingThe NetWare kernel does not support paging.  It is not clear if the primitives supplied by the kernel today would besufficient to provide paging via a socalled external pager YOUN87.  The page hardware of the underlying systemis, however, used to implement protection domains. Currently, the page hardware is also used to keep certain perthread and perprotection domain variables at constant virtual addresses.4. Server ReplicationThe NetWare OS has long supported fault tolerance approaches such as the mirroring of disk drives.  Recently,however, we have developed a new technology, known as System Fault Tolerance III SFT III that allows for themirroring of an entire server.  In this configuration, two identical server hardware platforms act as one logical serverto their clients.  At any time, one of the servers acts as the primary and actually replies to requests from the clients.The second server acts as the secondary by tracking the state of the primary and staying ready to perform a failoverin which it takes over from the primary should the primary fail.  No special hardware is involved in performing themirroring and the possible failover.4.1. ArchitectureIn order to accomplish this, we have restructured the system into a socalled IO engine and a mirrored serverengine.  The IO engine contains all code that actually deals with hardware connected to the system, as well ascontrolling the execution of the mirrored server engine.  The mirrored server engine, which actually can run anycombination of correctly written, nonhardwarespecific NLMs, is the piece that is mirrored between two differentsystems.The interface between the IO and mirrored server engines consists of an event queue of events passed from the IOengine to the mirrored server engine, and a request queue moving from the mirrored server engine to the IO engine.These two queues are the entire interface between the two engines.  The mirrored server engine contains a copy of theNetWare kernel complete with scheduler, and so forth, but no code that actually touches any hardware beyond basicCPU registers and the storage allocated to that engine.Additionally, the mirrored server engine contains a virtual network, with its own network numbers in the variousprotocol address spaces, and protocol stacks connected to this virtual network.  Note that the network numbers andnode numbers on this virtual network are exactly the same for the two mirrored server engines.The two systems to be mirrored are connected through a highspeed, pointtopoint data link running a set ofprotocols specifically designed to support server replication.  The link and the protocols run on the link are known as8This state is related to the C language runtime environment provided by the operating system.  Installation of this statetakes 24 machine instructions in our Intel 80x86based implementation.the mirrored server link MSL.  The link itself can be any highspeed data link technology, configured in a pointtopoint fashion.9  The two systems to be mirrored are also connected via the shared internetwork.Initially, only one of the systems runs, operating in an unmirrored mode.  When the second system comes up, thetwo systems connect over the MSL.  The mirrored server engine on the primary server is suspended momentarily,and its entire memory image is transferred to the secondary, where it is loaded into memory, building an exact copy.Once the memory image and associated state such as register contents exist on both machines, both mirrored serverengines are started up.  The basic architecture is illustrated in the following diagram.Mirrored Server EngineIO EngineHardwareEvents RequestsMirrored Server EngineIO EngineHardwareEvents RequestsMirrored Server LinkAt the moment of starting, the state in both mirrored server engines is identical.  Server mirroring works by keepingthe state of the two mirrored server engines identical over time, so that if the primary dies because of a hardwarefailure, say, the secondary engine will be able to take over with almost no action required on the part of theclients10.Four basic conditions have made this possible1 Networks are inherently unreliable, so any packets awaiting transmission at the primary at the time offailover will either be rerequested by the client or retransmitted by the mirrored server engine.2 The mirrored server engine is a nonpreemptive system, so the execution path inside the mirrored serverengine does not require interrupts for its correct functioning.3 We are able to virtualize time, so the actual wall clock time, which will be different at the twomirrored server engines at the same point of execution, is not seen by the mirrored server engines.  Thus,the two engines will see the same time at the same points of execution, and thus will operate in exactly thesame manner.4 The event queue is the only input the mirrored server engine has from the rest of the world.Note that hardware physically attached to a system that is down is unavailable.  This includes disk drives.  To makesure that we are able to continue file service during such an outage, we mirror the disk drives between the two9A 10 megabitsecond ethernet link between the two systems is minimally acceptable as an MSL, but for the initialsynchronization, as well as to allow for increased distances between the primary and secondary servers, a more specialized,higher speed link is desirable.10Some network routing protocols converge slowly in certain topologies.  To increase the speed at which the clients startcommunicating with the old secondarynew primary, a message is sent to the clients causing them to reacquire their route tothe mirrored server.servers.  There are other protocols for bringing the mirrored disk drives into synchronization, but those are beyondthe scope of this paper.4.2. DynamicsEvery event enqueued to the primary mirrored server engine is reliably transferred over the MSL to the secondary IOengine to put on the queue for the secondary mirrored server.  No other events are queued up for the secondarymirrored server.When the mirrored server engine has completed all processing possible on its current event and needs new input, itwill request a new event from the IO engine.  The two IO engines maintain sequence numbers on the requestscoming down from the mirrored server engines and cooperate to pass the identical events in response to the samerequest numbers.  Thus, the mirrored server engines see the same set of events, and so behave like any statemachines that start off in the same state and are given the same set of input stimuli  they make the same statetransitions.  In fact, the two mirrored server engines execute the exact same code path between events.  The twosystems are not in close lock step  one of the mirrored server engines will normally be ahead of the other engine.However, the two engines will be in exactly the same state at the point at which they each make a request of theirIO engine.The mirrored server engines notion of time is controlled by the incoming events from the IO engine.  The mirroredserver engine does not access the CPU timer directly.In the normal mode of operation, the secondary IO engine keeps all requests given to it from the secondary mirroredserver engine, but does not act on the majority of these requests.11  The secondary needs to keep track of outstandingrequests in order to deal with primary failures.  The primary IO engine executes all requests with the exception ofthose directed at the secondary IO engine.  Events generated in the IO engine as a result of servicing events, or ofexternal events, are collected by the primary IO engine and communicated to the secondary IO engine.As a diagnostic tool we have the capability to capture the request stream generated by the secondary mirrored serverengine and compare it with the request stream generated by the primary mirrored server engine.  This has been usefulin detecting bugs in programs referencing uninitialized data or data in the process of being updated by the IOengine.Additionally, because of our ability to capture the state of a system and capture the event and request streams for thatsystems subsequent behavior, we have the currently unrealized ability to allow customers in the field to record thisdata when diagnosing a bug and to forward it to their software vendors for replay and analysis.4.3. Fault RecoveryThe primary and secondary IO engines constantly communicate over the MSL.  If the secondary detects that theprimary has failed, it will take over for the primary.12  While up, the network routing protocol running on theprimary had been advertising reachability to the virtual network used by the mirrored server engines.  Now, thesecondary starts advertising this reachability.  As soon as the routing topology converges which depends on therouting protocol in use as well as on the topology between the connected client systems and the two servers, allconnections will continue running as if nothing happened.When the old primary comes back up, it synchronizes its state with the new primary and takes on the role ofsecondary.11If the request is, for example, for a disk IO operation on a disk physically attached to the secondary server, the secondaryIO engine will execute that request.12The secondary, before deciding that the primary is down, attempts to contact the primary across the internetwork.  Thisis done to detect the case where the MSL, and not the primary, has become inoperative.When applied to file server processes running in the mirrored server engine, this system bears a resemblance to theHarp file system LISK91.  To our knowledge, however, this is the first system in which a pure software approachto fault tolerance has been applied to such a wide range of services such as database, mail and messaging, printing,different file service protocols, etc..  Additionally, in SFT III, the knowledge that the system has been replicated ishidden from the specific service processes, being handled by the operating system instead.The server replication design is described in more detail in MAJO92.5. Future WorkAs mentioned in section 3.1, currently the system only supports two protection domains.  We are working onallowing an arbitrarily large number of protection domains.  Currently, portions of the storage allocated to a callingdomain are shared with the called domain, a protection exposure which needs to be addressed.The failure model for protection domains discussed in section 3.2 will evolve as we get more experience with it.There is some potential for making use of protection domains in order to run NetWare in a nonshared memory,multiprocessor environment.Currently, some code paths for interrupt time processing are fairly long.  We would like to use work objects toshorten the interrupt time component of these paths.  Among other benefits, we expect to get a more robust systemby reducing the amount of code that needs to deal with concurrency issues.Finally, engineering work on NetWare, the product, is ongoing for example, porting the system to various RISCbased architectures.6. AcknowledgmentsThe NetWare kernel and operating system are the result of many peoples efforts over the years  too many people tolist here.  We would like to thank Sam Leffler and the USENIX referees for helpful comments on this paper.Michael Marks provided valuable comments and feedback on earlier drafts of this paper.7. ReferencesACCE86 Accetta, M. J., W. Baron, R. V. Bolosky, D. B. Golub, R. F. Rashid, A. Tevanian, and M. W.Young, Mach A New Kernel Foundation for Unix Development, in Proceedings of the SummerUSENIX Conference.  July, 1986.CLAR85 Clark, David D., The Structuring of Systems Using Upcalls, in Proceedings of the 10thSymposium on Operating Systems Principles.  December, 1985.DRUS92 Druschel, Peter, Larry L. Peterson, and Norman C. Hutchinson, Beyond MicroKernel DesignDecoupling Modularity and Protection in Lipto, in Proceedings of the Twelfth InternationalConference on Distributed Computing Systems.  June, 1992.FINL90 Finlayson, Ross, Mark D. Hennecke, and Steven L. Goldberg, Vanguard A Protocol Suite andOS Kernel for Distributed ObjectOriented Environments, in Proceedings of the IEEE Workshopon Experimental Distributed Systems, October, 1990.FORD94 Ford, Bryan, and Jay Lepreau, Evolving Mach 3.0 to a Migrating Thread Model, in Proceedingsof the Winter USENIX Conference.  January, 1994.GIEN90 Gien, Michel, MicroKernel Design, in Unix Review, 8115863.  November, 1990.GING89 Gingell, Robert A., Shared Libraries, in Unix Review, 785666.  August, 1989.HAMI93 Hamilton, Graham, and Panos Kougiouris, The Spring Nucleus A Microkernel for Objects, inProceedings of the Summer USENIX Conference.  June, 1993.HARB91 Harbison, Samuel P., and Guy L. Steele Jr., C, A Reference Manual.  1991.JOHN93 Johnson, David, and Willy Zwaenepoel, The Peregrine HighPerformance RPC System,Software   Practice  Experience, 232201221.  February, 1993.JONE80 Jones, Anita K., and Peter Schwartz, Experience Using Multiprocessor Systems  A StatusReport, in ACM Computing Surveys, 122121165.  June, 1980.LEFF89 Leffler, Samuel J., Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman, TheDesign and Implementation of the 4.3BSD UNIX Operating System.  1989.LI89 Li, Kai, and Paul Hudak, Memory Coherence in Shared Virtual Memory Systems, in ACMTransactions on Computer Systems, 74321359.  November, 1989.LISK91 Liskov, Barbara, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and MichaelWilliams, Replication in the Harp File System,  in Proceedings of the 13th Symposium onOperating Systems Principles.  December, 1991.MAJO92 Major, Drew, Kyle Powell, and Dale Neibaur, Fault Tolerant Computer System, in UnitedStates Patent 5,157,663.  October, 1992.OMG91 Object Management Group, The Common Object Request Broker  Architecture and Specification.1991.RITC78 Ritchie, D. M., and K. Thompson, The UNIX Timesharing System, in Bell System TechnicalJournal, 57619051929.  JulyAugust, 1978.TANE92 Tanenbaum, Andrew S., Modern Operating Systems.  1992.YOUN87 Young, M., A. Tevanian, R. Rashid, D. Golub, J. Eppinger, J. Chew, W. Bolosky, D. Black, andR. Baron, The Duality of Memory and Communication in the Implementation of aMultiprocessor Operating System, in Proceedings of the Eleventh ACM Symposium onOperating Systems Principles.  November, 1987.Author InformationDrew Major is chief scientist and systems architect at Novell, working with the software development team.  Hewas instrumental in the design and implementation of the NetWare operating system, scheduler, file system, andserver replication.  He holds a B.S. degree in Computer Science from Brigham Young University.Greg Minshall is involved in the design of networking and operating systems at Novell.  He holds an A.B. degreein Pure Mathematics from the University of California at Berkeley.  His email address is minshallwc.novell.com.Kyle Powell is a senior systems architect at Novell, having been heavily involved in the design andimplementation of the client portion of NetWare, as well as in server replication and the operating system itself.  Heholds a B.S. degree in Computer Science from Brigham Young University.NetWare is a trademark of Novell, Inc. UNIX is a registered trademark of UNIX System Laboratories, Inc., a subsidiary ofNovell, Inc. Macintosh is a registered trademark of Apple Computer, Inc. all other product names mentioned herein are thetrademarks of their respective owners.
