Ecient SoftwareBased Fault IsolationRobert Wahbe Steven Lucco Thomas E. Anderson Susan L. GrahamComputer Science DivisionUniversity of CaliforniaBerkeley, CA 94720AbstractOne way to provide fault isolation among cooperatingsoftware modules is to place each in its own addressspace. However, for tightlycoupled modules, this solution incurs prohibitive context switch overhead. Inthis paper, we present a software approach to implementing fault isolation within a single address space.Our approach has two parts. First, we load the codeand data for a distrusted module into its own fault domain, a logically separate portion of the applicationsaddress space. Second, we modify the object code of adistrusted module to prevent it from writing or jumping to an address outside its fault domain. Both thesesoftware operations are portable and programming language independent.Our approach poses a tradeo relative to hardwarefault isolation substantially faster communication between fault domains, at a cost of slightly increasedexecution time for distrusted modules. We demonstrate that for frequently communicating modules, implementing fault isolation in software rather than hardware can substantially improve endtoend applicationperformance.This work was supported in part by the National Science Foundation CDA8722788, Defense Advanced ResearchProjects Agency DARPA under grant MDA97292J1028 andcontracts DABT6392C0026 and N0060093C2481, the Digital Equipment Corporation the Systems Research Center andthe External Research Program, and the ATT Foundation.Anderson was also supported by a National Science FoundationYoung Investigator Award. The content of the paper does notnecessarily reect the position or the policy of the Governmentand no ocial endorsement should be inferred.To appear in the Proceedings of the Symposium on Operating System Principles, 1993.1 IntroductionApplication programs often achieve extensibility byincorporating independently developed software modules. However, faults in extension code can render asoftware system unreliable, or even dangerous, sincesuch faults could corrupt permanent data. To increase the reliability of these applications, an operating system can provide services that prevent faults indistrusted modules from corrupting application data.Such fault isolation services also facilitate software development by helping to identify sources of system failure.For example, the postgres database manager includes an extensible type system Sto87. Using thisfacility, postgres queries can refer to generalpurposecode that denes constructors, destructors, and predicates for userdened data types such as geometricobjects. Without fault isolation, any query that usesextension code could interfere with an unrelated queryor corrupt the database.Similarly, recent operating system research has focused on making it easier for third party vendorsto enhance parts of the operating system. An example is microkernel design parts of the operating system are implemented as userlevel servers thatcan be easily modied or replaced. More generally, several systems have added extension code intothe operating system, for example, the BSD networkpacket lter MRA87, MJ93, applicationspecic virtual memory management HC92, and Active Messages vCGS92. Among industry systems, MicrosoftsObject Linking and Embedding system Cla92 canlink together independently developed software modules. Also, the Quark Xpress desktop publishing system Dys92 is structured to support incorporation ofEmail frwahbe, lucco, tea, grahamgcs.berkeley.edu1generalpurpose third party code. As with postgres,faults in extension modules can render any of thesesystems unreliable.One way to provide fault isolation among cooperating software modules is to place each in its own addressspace. Using Remote Procedure Call RPC BN84,modules in separate address spaces can call into eachother through a normal procedure call interface. Hardware page tables prevent the code in one address spacefrom corrupting the contents of another.Unfortunately, there is a high performance costto providing fault isolation through separate addressspaces. Transferring control across protection boundaries is expensive, and does not necessarily scalewith improvements in a processors integer performance ALBL91. A crossaddressspace RPC requiresat least a trap into the operating system kernel, copying each argument from the caller to the callee, saving and restoring registers, switching hardware address spaces on many machines, ushing the translation lookaside buer, and a trap back to user level.These operations must be repeated upon RPC return. The execution time overhead of an RPC, evenwith a highly optimized implementation, will oftenbe two to three orders of magnitude greater thanthe execution time overhead of a normal procedurecall BALL90, ALBL91.The goal of our work is to make fault isolation cheapenough that system developers can ignore its performance eect in choosing which modules to place inseparate fault domains. In many cases where fault isolation would be useful, crossdomain procedure callsare frequent yet involve only a moderate amount ofcomputation per call. In this situation it is impractical to isolate each logically separate module withinits own address space, because of the cost of crossinghardware protection boundaries.We propose a software approach to implementingfault isolation within a single address space. Our approach has two parts. First, we load the code and datafor a distrusted module into its own fault domain, alogically separate portion of the applications addressspace. A fault domain, in addition to comprising a contiguous region of memory within an address space, hasa unique identier which is used to control its access toprocess resources such as le descriptors. Second, wemodify the object code of a distrusted module to prevent it from writing or jumping to an address outsideits fault domain. Program modules isolated in separate softwareenforced fault domains can not modifyeach others data or execute each others code exceptthrough an explicit crossfaultdomain RPC interface.We have identied several programminglanguageindependent transformation strategies that can renderobject code unable to escape its own code and datasegments. In this paper, we concentrate on a simple transformation technique, called sandboxing, thatonly slightly increases the execution time of the modied object code. We also investigate techniques thatprovide more debugging information but which incurgreater execution time overhead.Our approach poses a tradeo relative to hardwarebased fault isolation. Because we eliminate the need tocross hardware boundaries, we can oer substantiallylowercost RPC between fault domains. A safe RPC inour prototype implementation takes roughly 1.1s on aDECstation 5000240 and roughly 0.8s on a DEC Alpha 400, more than an order of magnitude faster thanany existing RPC system. This reduction in RPC timecomes at a cost of slightly increased distrusted moduleexecution time. On a test suite including the the Cspec92 benchmarks, sandboxing incurs an average of4 execution time overhead on both the DECstationand the Alpha.Softwareenforced fault isolation may seem to becounterintuitive we are slowing down the commoncase normal execution to speed up the uncommoncase crossdomain communication. But for frequently communicating fault domains, our approachcan oer substantially better endtoend performance.To demonstrate this, we applied softwareenforcedfault isolation to the postgres database system running the Sequoia 2000 benchmark. The benchmarkmakes use of the postgres extensible data type system to dene geometric operators. For this benchmark, the software approach reduced fault isolationoverhead by more than a factor of three on a DECstation 5000240.A software approach also provides a tradeo between performance and level of distrust. If some modules in a program are trusted while others are distrusted as may be the case with extension code, onlythe distrusted modules incur any execution time overhead. Code in trusted domains can run at full speed.Similarly, it is possible to use our techniques to implement full security, preventing distrusted code fromeven reading data outside of its domain, at a cost ofhigher execution time overhead. We quantify this effect in Section 5.The remainder of the paper is organized as follows.Section 2 provides some examples of systems that require frequent communication between fault domains.Section 3 outlines how we modify object code to prevent it from generating illegal addresses. Section 4describes how we implement low latency crossfaultdomain RPC. Section 5 presents performance resultsfor our prototype, and nally Section 6 discusses somerelated work.2 BackgroundIn this section, we characterize in more detail thetype of application that can benet from softwareenforced fault isolation. We defer further descriptionof the postgres extensible type system until Section5, which gives performance measurements for this application.The operating systems community has focused considerable attention on supporting kernel extensibility. For example, the UNIX vnode interface is designed to make it easy to add a new le system intoUNIX Kle86. Unfortunately, it is too expensive toforward every le system operation to user level, sotypically new le system implementations are addeddirectly into the kernel. The Andrew le system islargely implemented at user level, but it maintains akernel cache for performance HKM88. Epochs tertiary storage le system Web93 is one example of operating system kernel code developed by a third partyvendor.Another example is userprogrammable high performance IO systems. If data is arriving on an IOchannel at a high enough rate, performance will bedegraded substantially if control has to be transferredto user level to manipulate the incoming data FP93.Similarly, Active Messages provide high performancemessage handling in distributedmemory multiprocessors vCGS92. Typically, the message handlers areapplicationspecic, but unless the network controllercan be accessed from user level Thi92, the messagehandlers must be compiled into the kernel for reasonable performance.A userlevel example is the Quark Xpress desktoppublishing system. One can purchase third party software that will extend this system to perform functions unforeseen by its original designers Dys92. Atthe same time, this extensibility has caused Quark anumber of problems. Because of the lack of ecientfault domains on the personal computers where QuarkXpress runs, extension modules can corrupt Quarksinternal data structures. Hence, bugs in third partycode can make the Quark system appear unreliable,because endusers do not distinguish among sources ofsystem failure.All these examples share two characteristics. First,using hardware fault isolation would result in a significant portion of the overall execution time being spentin operating system context switch code. Second, onlya small amount of code is distrusted most of the execution time is spent in trusted code. In this situation,software fault isolation is likely to be more ecientthan hardware fault isolation because it sharply reduces the time spent crossing fault domain boundaries,while only slightly increasing the time spent executingthe distrusted part of the application. Section 5 quanties this tradeo between domaincrossing overheadand application execution time overhead, and demonstrates that even if domaincrossing overhead represents a modest proportion of the total application execution time, softwareenforced fault isolation is costeective.3 SoftwareEnforced Fault IsolationIn this section, we outline several software encapsulation techniques for transforming a distrusted moduleso that it can not escape its fault domain. We rstdescribe a technique that allows users to pinpoint thelocation of faults within a software module. Next, weintroduce a technique, called sandboxing, that can isolate a distrusted module while only slightly increasingits execution time. Section 5 provides a performanceanalysis of this techinique. Finally, we present a software encapsulation technique that allows cooperatingfault domains to share memory. The remainder ofthis discussion assumes we are operating on a RISCloadstore architecture, although our techniques couldbe extended to handle CISCs. Section 4 describeshow we implement safe and ecient crossfaultdomainRPC.We divide an applications virtual address space intosegments, aligned so that all virtual addresses withina segment share a unique pattern of upper bits, calledthe segment identier. A fault domain consists of twosegments, one for a distrusted modules code, the otherfor its static data, heap and stack. The specic segment addresses are determined at load time.Software encapsulation transforms a distrustedmodules object code so that it can jump only to targets in its code segment, and write only to addresseswithin its data segment. Hence, all legal jump targets in the distrusted module have the same upper bitpattern segment identier similarly, all legal dataaddresses generated by the distrusted module sharethe same segment identier. Separate code and datasegments are necessary to prevent a module frommodifying its code segment1. It is possible for an addresswith the correct segment identier to be illegal, for instance if it refers to an unmapped page. This is caughtby the normal operating system page fault mechanism.3.1 Segment MatchingAn unsafe instruction is any instruction that jumps toor stores to an address that can not be statically ver1Our system supports dynamic linking through a specialinterface.ied to be within the correct segment. Most controltransfer instructions, such as programcounterrelativebranches, can be statically veried. Stores to staticvariables often use an immediate addressing mode andcan be statically veried. However, jumps through registers, most commonly used to implement procedurereturns, and stores that use a register to hold theirtarget address, can not be statically veried.A straightforward approach to preventing the use ofillegal addresses is to insert checking code before every unsafe instruction. The checking code determineswhether the unsafe instructions target address has thecorrect segment identier. If the check fails, the inserted code will trap to a system error routine outsidethe distrusted modules fault domain. We call thissoftware encapsulation technique segment matching.On typical RISC architectures, segment matchingrequires four instructions. Figure 1 lists a pseudocodefragment for segment matching. The rst instructionin this fragment moves the store target address intoa dedicated register. Dedicated registers are used onlyby inserted code and are never modied by code inthe distrusted module. They are necessary becausecode elsewhere in the distrusted module may arrangeto jump directly to the unsafe store instruction, bypassing the inserted check. Hence, we transform allunsafe store and jump instructions to use a dedicatedregister.All the software encapsulation techniques presentedin this paper require dedicated registers2. Segmentmatching requires four dedicated registers one to holdaddresses in the code segment, one to hold addressesin the data segment, one to hold the segment shiftamount, and one to hold the segment identier.Using dedicated registers may have an impact onthe execution time of the distrusted module. However,since most modern RISC architectures, including theMIPS and Alpha, have at least 32 registers, we canretarget the compiler to use a smaller register set withminimal performance impact. For example, Section 5shows that, on the DECstation 5000240, reducing byve registers the register set available to a C compilergcc did not have a signicant eect on the averageexecution time of the spec92 benchmarks.3.2 Address SandboxingThe segment matching technique has the advantagethat it can pinpoint the oending instruction. Thiscapability is useful during software development. Wecan reduce runtime overhead still further, at the costof providing no information about the source of faults.2For architectures with limited register sets, such as the80386 Int86, it is possible to encapsulate a module using no reserved registers by restricting control ow within a fault domain.dedicatedreg target addressMove target address into dedicated register.scratchreg dedicatedregshiftregRightshift address to get segment identier.scratchreg is not a dedicated register.shiftreg is a dedicated register.compare scratchreg and segmentregsegmentreg is a dedicated register.trap if not equalTrap if store address is outside of segment.store instruction uses dedicatedregFigure 1 Assembly pseudo code for segment matching.dedicatedreg targetregandmaskregUse dedicated register andmaskregto clear segment identier bits.dedicatedreg dedicatedregsegmentregUse dedicated register segmentregto set segment identier bits.store instruction uses dedicatedregFigure 2 Assembly pseudo code to sandbox addressin targetreg.Before each unsafe instruction we simply insert codethat sets the upper bits of the target address to thecorrect segment identier. We call this sandboxing theaddress. Sandboxing does not catch illegal addressesit merely prevents them from aecting any fault domain other than the one generating the address.Address sandboxing requires insertion of two arithmetic instructions before each unsafe store or jumpinstruction. The rst inserted instruction clears thesegment identier bits and stores the result in a dedicated register. The second instruction sets the segment identier to the correct value. Figure 2 lists thepseudocode to perform this operation. As with segment matching, we modify the unsafe store or jumpinstruction to use the dedicated register. Since we areusing a dedicated register, the distrusted module codecan not produce an illegal address even by jumpingto the second instruction in the sandboxing sequencesince the upper bits of the dedicated register will already contain the correct segment identier, this second instruction will have no eect. Section 3.6 presentsa simple algorithm that can verify that an object codemodule has been correctly sandboxed.Address sandboxing requires ve dedicated registers.One register is used to hold the segment mask, tworegisters are used to hold the code and data segmentSegmentGuard ZonesregregoffsetFigure 3 A segment with guard zones. The size ofthe guard zones covers the range of possible immediateosets in registerplusoset addressing modes.identiers, and two are used to hold the sandboxedcode and data addresses.3.3 OptimizationsThe overhead of software encapsulation can be reduced by using conventional compiler optimizations.Our current prototype applies loop invariant code motion and instruction scheduling optimizations ASU86,ACD74. In addition to these conventional techniques,we employ a number of optimizations specialized tosoftware encapsulation.We can reduce the overhead of software encapsulation mechanisms by avoiding arithmetic that computestarget addresses. For example, many RISC architectures include a registerplusoset instruction mode,where the oset is an immediate constant in some limited range. On the MIPS architecture such osets arelimited to the range 64K to 64K. Consider thestore instruction store value,offsetreg, whoseaddress offsetreg uses the registerplusoset addressing mode. Sandboxing this instruction requiresthree inserted instructions one to sum regoffsetinto the dedicated register, and two sandboxing instructions to set the segment identier of the dedicatedregister.Our prototype optimizes this case by sandboxingonly the register reg, rather than the actual target address regoffset, thereby saving an instruction. Tosupport this optimization, the prototype establishesguard zones at the top and bottom of each segment.To create the guard zones, virtual memory pages adjacent to the segment are unmapped see Figure 3.We also reduce runtime overhead by treating theMIPS stack pointer as a dedicated register. We avoidsandboxing the uses of the stack pointer by sandboxingthis register whenever it is set. Since uses of the stackpointer to form addresses are much more plentiful thanchanges to it, this optimization signicantly improvesperformance.Further, we can avoid sandboxing the stack pointerafter it is modied by a small constant oset as long asthe modied stack pointer is used as part of a load orstore address before the next control transfer instruction. If the modied stack pointer has moved into aguard zone, the load or store instruction using it willcause a hardware address fault. On the DEC Alphaprocessor, we apply these optimizations to both theframe pointer and the stack pointer.There are a number of further optimizations thatcould reduce sandboxing overhead. For example,the transformation tool could remove sandboxing sequences from loops, in cases where a store target address changes by only a small constant oset duringeach loop iteration. Our prototype does not yet implement these optimizations.3.4 Process ResourcesBecause multiple fault domains share the same virtualaddress space, the fault domain implementation mustprevent distrusted modules from corrupting resourcesthat are allocated on a peraddressspace basis. Forexample, if a fault domain is allowed to make systemcalls, it can close or delete les needed by other codeexecuting in the address space, potentially causing theapplication as a whole to crash.One solution is to modify the operating system toknow about fault domains. On a system call or pagefault, the kernel can use the program counter to determine the currently executing fault domain, and restrictresources accordingly.To keep our prototype portable, we implementedan alternative approach. In addition to placing eachdistrusted module in a separate fault domain, we require distrusted modules to access system resourcesonly through crossfaultdomain RPC. We reserve afault domain to hold trusted arbitration code that determines whether a particular system call performedby some other fault domain is safe. If a distrustedmodules object code performs a direct system call, wetransform this call into the appropriate RPC call. Inthe case of an extensible application, the trusted portion of the application can make system calls directlyand shares a fault domain with the arbitration code.3.5 Data SharingHardware fault isolation mechanisms can support datasharing among virtual address spaces by manipulating page table entries. Fault domains share an address space, and hence a set of page table entries,so they can not use a standard shared memory implementation. Readonly sharing is straightforwardsince our software encapsulation techniques do not alter load instructions, fault domains can read any memory mapped in the applications address space 3.If the object code in a particular distrusted module has been sandboxed, then it can share readwritememory with other fault domains through a techniquewe call lazy pointer swizzling. Lazy pointer swizzlingprovides a mechanism for fault domains to share arbitrarily many readwrite memory regions with no additional runtime overhead. To support this technique,we modify the hardware page tables to map the sharedmemory region into every address space segment thatneeds access the region is mapped at the same osetin each segment. In other words, we alias the sharedregion into multiple locations in the virtual addressspace, but each aliased location has exactly the samelow order address bits. As with hardware shared memory schemes, each shared region must have a dierentsegment oset.To avoid incorrect shared pointer comparisons insandboxed code, the shared memory creation interface must ensure that each shared object is given aunique address. As the distrusted object code accesses shared memory, the sandboxing code automatically translates shared addresses into the corresponding addresses within the fault domains data segment.This translation works exactly like hardware translation the low bits of the address remain the same, andthe high bits are set to the data segment identier.Under operating systems that do not allow virtualaddress aliasing, we can implement shared regions byintroducing a new software encapsulation techniqueshared segment matching. To implement sharing, weuse a dedicated register to hold a bitmap. The bitmapindicates which segments the fault domain can access.For each unsafe instruction checked, shared segmentmatching requires one more instruction than segmentmatching.3.6 Implementation and VericationWe have identied two strategies for implementingsoftware encapsulation. One approach uses a compilerto emit encapsulated object code for a distrusted module the integrity of this code is then veried when themodule is loaded into a fault domain. Alternatively,the system can encapsulate the distrusted module bydirectly modifying its object code at load time.3We have implemented versions of these techniques that perform general protection by encapsulating load instructions aswell as store and jump instructions. We discuss the performanceof these variants in Section 5.Our current prototype uses the rst approach. Wemodied a version of the gcc compiler to perform software encapsulation. Note that while our current implementation is language dependent, our techniques arelanguage independent.We built a verier for the MIPS instruction setthat works for both sandboxing and segment matching. The main challenge in verication is that, in thepresence of indirect jumps, execution may begin onany instruction in the code segment. To address thissituation, the verier uses a property of our softwareencapsulation techniques all unsafe stores and jumpsuse a dedicated register to form their target address.The verier divides the program into sequences of instructions called unsafe regions. An unsafe store region begins with any modication to a dedicated storeregister. An unsafe jump region begins with any modication to a dedicated jump register. If the rst instruction in a unsafe store or jump region is executed,all subsequent instructions are guaranteed to be executed. An unsafe store region ends when one of thefollowing hold the next instruction is a store whichuses a dedicated register to form its target address,the next instruction is a control transfer instruction,the next instruction is not guaranteed to be executed,or there are no more instructions in the code segment.A similar denition is used for unsafe jump regions.The verier analyzes each unsafe store or jump region to insure that any dedicated register modied inthe region is valid upon exit of the region. For example, a load to a dedicated register begins an unsaferegion. If the region appropriately sandboxes the dedicated register, the unsafe region is deemed safe. If anunsafe region can not be veried, the code is rejected.By incorporating software encapsulation into an existing compiler, we are able to take advantage of compiler infrastructure for code optimization. However,this approach has two disadvantages. First, most modied compilers will support only one programming language gcc supports C, C, and Pascal. Second, thecompiler and verier must be synchronized with respect to the particular encapsulation technique beingemployed.An alternative, called binary patching, alleviatesthese problems. When the fault domain is loaded, thesystem can encapsulate the module by directly modifying the object code. Unfortunately, practical and robust binary patching, resulting in ecient code, is notcurrently possible LB92. Tools which translate onebinary format to another have been built, but thesetools rely on compilerspecic idioms to distinguishcode from data and use processor emulation to handle unknown indirect jumpsSCK93. For softwareencapsulation, the main challenge is to transform thecode so that it uses a subset of the registers, leavFigure 4 Major components of a crossfaultdomainRPC.ing registers available for dedicated use. To solve thisproblem, we are working on a binary patching prototype that uses simple extensions to current object leformats. The extensions store control ow and registerusage information that is sucient to support softwareencapsulation.4 Low Latency Cross Fault Domain CommunicationThe purpose of this work is to reduce the cost of faultisolation for cooperating but distrustful software modules. In the last section, we presented one half of oursolution ecient software encapsulation. In this section, we describe the other half fast communicationacross fault domains.Figure 4 illustrates the major components of a crossfaultdomain RPC between a trusted and distrustedfault domain. This section concentrates on three aspects of fault domain crossing. First, we describea simple mechanism which allows a fault domain tosafely call a trusted stub routine outside its domainthat stub routine then safely calls into the destinationdomain. Second, we discuss how arguments are eciently passed among fault domains. Third, we detailhow registers and other machine state are managed oncrossfaultdomain RPCs to insure fault isolation. Theprotocol for exporting and naming procedures amongfault domains is independent of our techniques.The only way for control to escape a fault domainis via a jump table. Each jump table entry is a control transfer instruction whose target address is a legalentry point outside the domain. By using instructionswhose target address is an immediate encoded in theinstruction, the jump table does not rely on the use ofa dedicated register. Because the table is kept in thereadonly code segment, it can only be modied bya trusted module.For each pair of fault domains a customized call andreturn stub is created for each exported procedure.Currently, the stubs are generated by hand rather thanusing a stub generator JRT85. The stubs run unprotected outside of both the caller and callee domain.The stubs are responsible for copying crossdomainarguments between domains and managing machinestate.Because the stubs are trusted, we are able to copycall arguments directly to the target domain. Traditional RPC implementations across address spacestypically perform three copies to transfer data. Thearguments are marshalled into a message, the kernelcopies the message to the target address space, andnally the callee must demarshall the arguments. Byhaving the caller and callee communicate via a sharedbuer, LRPC also uses only a single copy to pass databetween domains BALL91.The stubs are also responsible for managingmachinestate. On each crossdomain call any registers that areboth used in the future by the caller and potentiallymodied by the callee must be protected. Only registers that are designated by architectural convention tobe preserved across procedure calls are saved. As anoptimization, if the callee domain contains no instructions that modify a preserved register we can avoidsaving it. Karger uses a trusted linker to perform thiskind of optimization between address spaces Kar89.In addition to saving and restoring registers, the stubsmust switch the execution stack, establish the correctregister context for the software encapsulation technique being used, and validate all dedicated registers.Our system must also be robust in the presence offatal errors, for example, an addressing violation, whileexecuting in a fault domain. Our current implementation uses the UNIX signal facility to catch these errorsit then terminates the outstanding call and noties thecallers fault domain. If the application uses the sameoperating system thread for all fault domains, theremust be a way to terminate a call that is taking toolong, for example, because of an innite loop. Trustedmodules may use a timer facility to interrupt execution periodically and determine if a call needs to beterminated.5 Performance ResultsTo evaluate the performance of softwareenforced faultdomains, we implemented and measured a prototypeof our system on a 40MHz DECstation 5000240 decmips and a 133Mhz Alpha 400 decalpha.We consider three questions. First, how much overhead does software encapsulation incur Second, howfast is a crossfaultdomain RPC Third, what is theperformance impact of using software enforced faultisolation on an enduser application We discuss eachof these questions in turn.5.1 Encapsulation OverheadWe measured the execution time overhead of sandboxing a wide range of C programs, including the Cspec92 benchmarks and several of the Splash benchmarks Ass91, SWG91. We treated each benchmarkas if it were a distrusted module, sandboxing all ofits code. Column 1 of Table 1 reports overhead onthe decmips, column 6 reports overhead on the decalpha. Columns 2 and 7 report the overhead of usingour technique to provide general protection by sandboxing load instructions as well as store and jumpinstructions4. As detailed in Section 3, sandboxingrequires 5 dedicated registers. Column 3 reports theoverhead of removing these registers from possible useby the compiler. All overheads are computed as theadditional execution time divided by the original programs execution time.On the decmips, we used the program measurement tools pixie and qpt to calculate the numberof additional instructions executed due to sandboxing Dig, BL92. Column 4 of Table 1 reports thisdata as a percentage of original program instructioncounts.The data in Table 1 appears to contain a number of anomalies. For some of the benchmark programs, for example, 056.ear on the decmips and026.compress on the decalpha, sandboxing reducedexecution time. In a number of cases the overhead issurprisingly low.To identify the source of these variations we developed an analytical model for execution overhead.The model predicts overhead based on the numberof additional instructions executed due to sandboxing sinstructions, and the number of saved oating point interlock cycles interlocks. Sandboxing increases the available instructionlevel parallelism, allowing the number of oatingpoint interlocks to besubstantially reduced. The integer pipeline does notprovide interlocking instead, delay slots are explicitlylled with nop instructions by the compiler or assembler. Hence, scheduling eects among integer instructions will be accurately reected by the count of instructions added sinstructions. The expected overhead is computed assinstructions interlockscyclespersecondoriginalexecutiontimeseconds4Loads in the libraries, such as the standard C library, werenot sandboxed.The model provides an eective way to separate knownsources of overhead from second order eects. Column 5 of Table 1 are the predicted overheads.As can be seen from Table 1, the model is, on average, eective at predicting sandboxing overhead. Thedierences between measured and expected overheadsare normally distributed with mean 0.7 and standarddeviation of 2.6. The dierence between the meansof the measured and expected overheads is not statistically signicant. This experiment demonstrates that,by combining instruction count overhead and oatingpoint interlock measurements, we can accurately predict average execution time overhead. If we assumethat the model is also accurate at predicting the overhead of individual benchmarks, we can conclude thatthere is a second order eect creating the observedanomalies in measured overhead.We can discount eective instruction cache size andvirtual memory paging as sources for the observed execution time variance. Because sandboxing adds instructions, the eective size of the instruction cache isreduced. While this might account for measured overheads higher than predicted, it does not account forthe opposite eect. Because all of our benchmarks arecompute bound, it is unlikely that the variations aredue to virtual memory paging.The decmips has a physically indexed, physicallytagged, direct mapped data cache. In our experimentssandboxing did not aect the size, contents, or startingvirtual address of the data segment. For both originaland sandboxed versions of the benchmark programs,successive runs showed insignicant variation. Thoughdicult to quantify, we do not believe that data cachealignment was an important source of variation in ourexperiments.We conjecture that the observed variations arecaused by instruction cache mapping conicts. Software encapsulation changes the mapping of instructions to cache lines, hence changing the number of instruction cache conicts. A number of researchers haveinvestigated minimizing instruction cache conicts toreduce execution time McF89, PH90, Sam88. Oneresearcher reported a 20 performance gain by simply changing the order in which the object les werelinked PH90. Samples and Hilnger report significantly improved instruction cache miss rates by rearranging only 3 to 8 of an applications basicblocks Sam88.Beyond this eect, there were statistically signicantdierences among programs. On average, programswhich contained a signicant percentage of oatingpoint operations incurred less overhead. On the decmips the mean overhead for oating point intensivebenchmarks is 2.5, compared to a mean of 5.6 forthe remaining benchmarks. All of our benchmarks aredecmips decalphaFault Protection Reserved Instruction Fault Fault ProtectionBenchmark Isolation Overhead Register Count Isolation Isolation OverheadOverhead Overhead Overhead Overhead Overheadpredicted052.alvinn FP 1.4 33.4 0.3 19.4 0.2 8.1 35.5bps FP 5.6 15.5 0.1 8.9 5.7 4.7 20.3cholesky FP 0.0 22.7 0.5 6.5 1.5 0.0 9.3026.compress INT 3.3 13.3 0.0 10.9 4.4 4.3 0.0056.ear FP 1.2 19.1 0.2 12.4 2.2 3.7 18.3023.eqntott INT 2.9 34.4 1.0 2.7 2.2 2.3 17.4008.espresso INT 12.4 27.0 1.6 11.8 10.5 13.3 33.6001.gcc1.35 INT 3.1 18.7 9.4 17.0 8.9 NA NA022.li INT 5.1 23.4 0.3 14.9 11.4 5.4 16.2locus INT 8.7 30.4 4.3 10.3 8.6 4.3 8.7mp3d FP 10.7 10.7 0.0 13.3 8.7 0.0 6.7psgrind INT 10.4 19.5 1.3 12.1 9.9 8.0 36.0qcd FP 0.5 27.0 2.0 8.8 1.2 0.8 12.1072.sc INT 5.6 11.2 7.0 8.0 3.8 NA NAtracker INT 0.8 10.5 0.4 3.9 2.1 10.9 19.9water FP 0.7 7.4 0.3 6.7 1.5 4.3 12.3Average 4.3 21.8 0.4 10.5 5.0 4.3 17.6Table 1 Sandboxing overheads for decmips and decalpha platforms. The benchmarks 001.gcc1.35 and072.sc are dependent on a pointer size of 32 bits and do not compile on the decalpha. The predicted faultisolation overhead for cholesky is negative due to conservative interlocking on the MIPS oatingpoint unit.compute intensive. Programs that perform signicantamounts of IO will incur less overhead.5.2 Fault Domain CrossingWe now turn to the cost of crossfaultdomain RPC.Our RPC mechanism spends most of its time savingand restoring registers. As detailed in Section 4, onlyregisters that are designated by the architecture to bepreserved across procedure calls need to be saved. Inaddition, if no instructions in the callee fault domainmodify a preserved register then it does not need to besaved. Table 2 reports the times for three versions ofa NULL crossfaultdomain RPC. Column 1 lists thecrossing times when all data registers are caller saved.Column 2 lists the crossing times when the preservedinteger registers are saved. Finally, the times listed inColumn 3 include saving all preserved oating pointregisters. In many cases crossing times could be furtherreduced by statically partitioning the registers betweendomains.For comparison, we measured two other callingmechanisms. First, we measured the time to perform aC procedure call that takes no arguments and returnsno value. Second, we sent a single byte between twoaddress spaces using the pipe abstraction provided bythe native operating system and measured the roundtrip time. These times are reported in the last twocolumns of Table 2. On these platforms, the costof crossaddressspace calls is roughly three orders ofmagnitude more expensive than local procedure calls.Operating systems with highly optimized RPC implementations have reduced the cost of crossaddressspace RPC to within roughly two orders of magnitude of local procedure calls. On Mach 3.0, crossaddressspace RPC on a 25Mhz DECstation 5000200is 314 times more expensive than a local procedurecall Ber93. The Spring operating system, running ona 40Mhz SPARCstation2, delivers crossaddressspaceRPC that is 73 times more expensive than a local leafprocedure call HK93. Software enforced fault isolation is able to reduce the relative cost of crossfaultdomain RPC by an order of magnitude over these systems.5.3 Using Fault Domains in postgresTo capture the eect of our system on applicationperformance, we added software enforced fault domains to the postgres database management system,and measured postgres running the Sequoia 2000benchmark SFGM93. The Sequoia 2000 benchmarkCross FaultDomain RPCPlatform Caller Save Save C PipesSave Integer IntegerFloat ProcedureRegisters Registers Registers Calldecmips 1.11s 1.81s 2.83s 0.10s 204.72sdecalpha 0.75s 1.35s 1.80s 0.06s 227.88sTable 2 Crossfaultdomain crossing times.Sequoia 2000 Untrusted SoftwareEnforced Number decmipspipeQuery Function Manager Fault Isolation CrossDomain OverheadOverhead Overhead Calls predictedQuery 6 1.4 1.7 60989 18.6Query 7 5.0 1.8 121986 38.6Query 8 9.0 2.7 121978 31.2Query 10 9.6 5.7 1427024 31.9Table 3 Fault isolation overhead for postgres running Sequoia 2000 benchmark.contains queries typical of those used by earth scientists in studying the climate. To support these kindsof nontraditional queries, postgres provides a userextensible type system. Currently, userdened typesare written in conventional programming languages,such as C, and dynamically loaded into the databasemanager. This has long been recognized to be a serioussafety problemSto88.Four of the eleven queries in the Sequoia 2000 benchmark make use of userdened polygon data types. Wemeasured these four queries using both unprotecteddynamic linking and softwareenforced fault isolation.Since the postgres code is trusted, we only sandboxed the dynamically loaded user code. For thisexperiment, our crossfaultdomain RPC mechanismsaved the preserved integer registers the variant corresponding to Column 2 in Table 2. In addition, weinstrumented the code to count the number of crossfaultdomain RPCs so that we could estimate the performance of fault isolation based on separate addressspaces.Table 3 presents the results. Untrusted userdenedfunctions in postgres use a separate calling mechanism from builtin functions. Column 1 lists the overhead of the untrusted function manager without software enforced fault domains. All reported overheads inTable 3 are relative to original postgres using the untrusted function manager. Column 2 reports the measured overhead of software enforced fault domains. Using the number of crossdomain calls listed in Column 3and the decmipspipe time reported in Table 2, Column 4 lists the estimated overhead using conventionalhardware address spaces.5.4 AnalysisFor the postgres experiment software encapsulationprovided substantial savings over using native operating system services and hardware address spaces. Ingeneral, the savings provided by our techniques overhardwarebased mechanisms is a function of the percentage of time spent in distrusted code td, the percentage of time spent crossing among fault domainstc, the overhead of encapsulation h, and the ratio,r, of our fault domain crossing time to the crossingtime of the competing hardwarebased RPC mechanism.savings  1 rtc  htdFigure 5 graphically depicts these tradeos. The Xaxis gives the percentage of time an application spendscrossing among fault domains. The Y axis reports therelative cost of software enforced faultdomain crossing over hardware address spaces. Assuming that theexecution time overhead of encapsulated code is 4.3,the shaded region illustrates when software enforcedfault isolation is the better performance alternative.Softwareenforced fault isolation becomes increasingly attractive as applications achieve higher degreesof fault isolation see Figure 5. For example, if an application spends 30 of its time crossing fault domains,our RPC mechanism need only perform 10 betterthan its competitor. Applications that currently spendas little as 10 of their time crossing require only a39 improvement in fault domain crossing time. Asreported in Section 5.2, our crossing time for the decmips is 1.10s and for the decalpha 0.75s. Hence,AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPercentage of Execution Time Spent CrossingCrossing Time Relative toExisting RPC01020304050607080901005 101520253035404550Figure 5 The shaded region represents when software enforced fault isolation provides the better performance alternative. The X axis represents percentage of time spent crossing among fault domainstc. The Y axis represents the relative RPC crossingspeed r. The curve represents the break even point1rtc  htd. In this graph, h  0043 encapsulationoverhead on the decmips and decalpha.for this latter example, a hardware address space crossing time of 1.80s on the decmips and 1.23s on thedecalpha would provide better performance thansoftware fault domains. As far as we know, no production or experimental system currently provides thislevel of performance.Further, Figure 5 assumes that the entire application was encapsulated. For many applications, such aspostgres, this assumption is conservative. Figure 6transforms the previous gure, assuming that 50 oftotal execution is spent in distrusted extension code.Figures 5 and 6 illustrate that software enforcedfault isolation is the best choice whenever crossingoverhead is a signicant proportion of an applications execution time. Figure 7 demonstrates thatoverhead due to software enforced fault isolation remains small regardless of application behavior. Figure 7 plots overhead as a function of crossing behaviorand crossing cost. Crossing times typical of vendorsupplied and highly optimized hardwarebased RPCmechanisms are shown. The graph illustrates the relative performance stability of the software solution.This stability allows system developers to ignore theperformance eect of fault isolation in choosing whichmodules to place in separate fault domains.6 Related WorkMany systems have considered ways of optimizingRPC performance vvST88, TA88, Bla90, SB90, HK93,BALL90, BALL91. Traditional RPC systems basedAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPercentage of Execution Time Spent CrossingCrossing Time Relative toExisting RPC01020304050607080901005 101520253035404550Figure 6 The shaded region represents when software enforced fault isolation provides the better performance alternative. The X axis represents percentage of time spent crossing among fault domainstc. The Y axis represents the relative RPC crossingspeed r. The curve represents the break even point1rtc  htd. In this graph, h  0043 encapsulationoverhead on the decmips and decalpha. CrossingsMillesecondDECstation 5000Ultrix 4.2 Context SwitchPercentage of Execution Time Spent CrossingHardware MinimumSoftwareFigure 7 Percentage of time spent in crossing codeversus number of fault domain crossings per millisecond on the decmips. The hardware minimum crossing number is taken from a crossarchitectural studyof context switch times ALBL91. The Ultrix 4.2 context switch time is as reported in the last column ofTable 2.on hardware fault isolation are ultimately limited bythe minimal hardware cost of taking two kernel trapsand two hardware context switches. LRPC was oneof the rst RPC systems to approach this limit, andour prototype uses a number of the techniques foundin LRPC and later systems the same thread runs inboth the caller and the callee domain, the stubs arekept as simple as possible, and the crossing code jumpsdirectly to the called procedure, avoiding a dispatchin the callee domain. Unlike these systems, softwarebased fault isolation avoids hardware context switches,substantially reducing crossing costs.Address space identier tags can be used to reducehardware context switch times. Tags allow more thanone address space to share the TLB otherwise theTLB must be ushed on each context switch. It wasestimated that 25 of the cost of an LRPC on theFirey which does not have tags was due to TLBmissesBALL90. Address space tags do not, however,reduce the cost of register management or system calls,operations which are not scaling with integer performanceALBL91. An important advantage of softwarebased fault isolation is that it does not rely on specialized architectural features such as address space tags.Restrictive programming languages can also be usedto provide fault isolation. Pilot requires all kernel,user, and library code to be written in Mesa, a stronglytyped language all code then shares a single addressspace RDH80. The main disadvantage of relying onstrong typing is that it severely restricts the choiceof programming languages, ruling out conventionallanguages like C, C, and assembly. Even withstronglytyped languages such as Ada and Modula3,programmers often nd they need to use loopholes inthe type system, undercutting fault isolation. In contrast, our techniques are language independent.Deutsch and Grant built a system that alloweduserdened measurement modules to be dynamicallyloaded into the operating system and executed directlyon the processor DG71. The module format was astylized native object code designed to make it easierto statically verify that the code did not violate protection boundaries.An interpreter can also provide failure isolation. Forexample, the BSD UNIX network packet lter utilitydenes a language which is interpreted by the operating system network driver. The interpreter insulatesthe operating system from possible faults in the customization code. Our approach allows code written inany programming language to be safely encapsulatedor rejected if it is not safe, and then executed at nearfull speed by the operating system.Anonymous RPC exploits 64bit address spaces toprovide low latency RPC and probabilistic fault isolation YBA93. Logically independent domains areplaced at random locations in the same hardware address space. Calls between domains are anonymous,that is, they do not reveal the location of the calleror the callee to either side. This provides probabilistic protection  it is unlikely that any domain willbe able to discover the location of any other domainby malicious or accidental memory probes. To preserve anonymity, a cross domain call must trap to protected code in the kernel however, no hardware context switch is needed.7 SummaryWe have described a softwarebased mechanism forportable, programming language independent faultisolation among cooperating software modules. Byproviding fault isolation within a single address space,this approach delivers crossfaultdomain communication that is more than an order of magnitude fasterthan any RPC mechanism to date.To prevent distrusted modules from escaping theirown fault domain, we use a software encapsulationtechnique, called sandboxing, that incurs about 4execution time overhead. Despite this overhead inexecuting distrusted code, softwarebased fault isolation will often yield the best overall application performance. Extensive kernel optimizations can reducethe overhead of hardwarebased RPC to within a factor of ten over our softwarebased alternative. Evenin this situation, softwarebased fault isolation will bethe better performance choice whenever the overheadof using hardwarebased RPC is greater than 5.8 AcknowledgementsWe thank Brian Bershad, Mike Burrows, John Hennessy, Peter Kessler, Butler Lampson, Ed Lazowska,Dave Patterson, John Ousterhout, Oliver Sharp,Richard Sites, Alan Smith and Mike Stonebraker fortheir helpful comments on the paper. Jim Larus provided us with the proling tool qpt. We also thankMike Olson and Paul Aoki for helping us with postgres.ReferencesACD74 T.L. Adam, K.M. Chandy, and J.R. Dickson.A comparison of list schedules for parallel processing systems. Communications of the ACM,1712685690, December 1974.ALBL91 Thomas Anderson, Henry Levy, Brian Bershad, and Edward Lazowska. The Interactionof Architecture and Operating System Design.In Proceedings of the 4th International Conference on Architectural Support for ProgrammingLanguages and Operating Systems, pages 108120, April 1991.Ass91 Administrator National Computer GraphicsAssociation. SPEC Newsletter, 34, December1991.ASU86 Alfred V. Aho, Ravi Sethi, and Jerey D. Ullman. Compilers, Principles, Techniques, andTools. AddisonWesley Publishing Company,1986.BALL90 Brian Bershad, Thomas Anderson, Edward Lazowska, and Henry Levy. Lightweight RemoteProcedure Call. ACM Transactions on Computer Systems, 81, February 1990.BALL91 Brian Bershad, Thomas Anderson, Edward Lazowska, and Henry Levy. UserLevel Interprocess Communication for SharedMemory Multiprocessors. ACM Transactions on ComputerSystems, 92, May 1991.Ber93 Brian Bershad, August 1993. Private Communication.BL92 Thomas Ball and James R. Larus. Optimallyproling and tracing. In Proceedings of theConference on Principles of Programming Languages, pages 5970, 1992.Bla90 David Black. Scheduling Support for Concurrency and Parallelism in the Mach OperatingSystem. IEEE Computer, 2353543, May1990.BN84 Andrew Birrell and Bruce Nelson. Implementing Remote Procedure Calls. ACM Transactions on Computer Systems, 213959, February 1984.Cla92 J.D. Clark. Window Programmer Guide ToOLEDDE. PrenticeHall, 1992.DG71 L. P. Deutsch and C. A. Grant. A exible measurement tool for software systems. In IFIPCongress, 1971.Dig Digital Equipment Corporation. Ultrix v4.2Pixie Manual Page.Dys92 Peter Dyson. Xtensions for Xpress ModularSoftware for Custom Systems. Seybold Reporton Desktop Publishing, 610121, June 1992.FP93 Kevin Fall and Joseph Pasquale. Exploiting inkernel data paths to improve IO throughputand CPU a vailability. In Proceedings of the1993 Winter USENIX Conference, pages 327333, January 1993.HC92 Keiran Harty andDavid Cheriton. Applicationcontrolled physical memory using external pagecache management. In Proceedings of the 5th InternationalConference on Architectural Support for Programming Languages and Operating Systems,October 1992.HK93 Graham Hamilton and Panos Kougiouris. TheSpring nucleus A microkernel for objects. InProceedings of the Summer USENIX Conference, pages 147159, June 1993.HKM88 J. Howard, M. Kazar, S. Menees, D. Nichols,M. Satyanarayanan, R. Sidebotham, andM. West. Scale and Performance in a Distributed File System. ACM Transactions onComputer Systems, 615182, February 1988.Int86 Intel Corporation, Santa Clara, California.Intel 80386 Programmers Reference Manual,1986.JRT85 Michael B. Jones, Richard F. Rashid, andMary R. Thompson. Matchmaker An interface specication language for distributedprocessing. In Proceedings of the 12th ACMSIGACTSIGPLAN Symposium on Principlesof Programming Languages, pages 225235,January 1985.Kar89 Paul A. Karger. Using Registers to OptimizeCrossDomain Call Performance. In Proceedings of the 3rd International Conference onArchitectural Support for Programming Languages and Operating Systems, pages 194204,April 36 1989.Kle86 Steven R. Kleiman. Vnodes An Architecturefor Multiple File System Types in SUN UNIX.In Proceedings of the 1986 Summer USENIXConference, pages 238247, 1986.LB92 James R. Larus and Thomas Ball. Rewriting executable les to measure program behavior. Technical Report 1083, University ofWisconsinMadison, March 1992.McF89 Scott McFarling. Program optimization forinstruction caches. In Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, pages 183191, April 1989.MJ93 Steven McCanne and Van Jacobsen. TheBSD Packet Filter A New Architecture forUserLevel Packet Capture. In Proceedings ofthe 1993 Winter USENIX Conference, January1993.MRA87 J. C. Mogul, R. F. Rashid, and M. J. Accetta. The packet lter An ecient mechanism for userlevel network code. In Proceedings of the Symposium on Operating SystemPrinciples, pages 3951, November 1987.PH90 Karl Pettis and Robert C. Hansen. Proleguided code positioning. In Proceedings ofthe Conference on Programming Language Design and Implementation, pages 1627, WhitePlains, New York, June 1990. Appeared asSIGPLAN NOTICES 256.RDH80 David D. Redell, Yogen K. Dalal, Thomas R.Horsley, Hugh C. Lauer, William C. Lynch,Paul R. McJones, Hal G. Murray, andStephen C. Purcell. Pilot An Operating System for a Personal Computer. Communicationsof the ACM, 2328192, February 1980.Sam88 A. Dain Samples. Code reorganization for instruction caches. Technical Report UCBCSD88447, University of California, Berkeley, October 1988.SB90 Michael Schroeder and Michael Burrows. Performance of Firey RPC. ACM Transactions on Computer Systems, 81117, February 1990.SCK93 Richard L. Sites, Anton Cherno, Matthew B.Kirk, Maurice P. Marks, and Scott G. Robinson. Binary translation. Communications ofthe ACM, 3626981, February 1993.SFGM93 M. Stonebraker, J. Frew, K. Gardels, andJ. Meridith. The Sequoia 2000 Benchmark.In Proceedings of the ACM SIGMOD International Conference on Management of Data,May 1993.Sto87 Michael Stonebraker. Extensibility in POSTGRES. IEEE Database Engineering, September 1987.Sto88 Michael Stonebraker. Inclusion of new types inrelational data base systems. In Michael Stonebraker, editor, Readings in Database Systems,pages 480487. Morgan Kaufmann Publishers,Inc., 1988.SWG91 J. P. Singh, W. Weber, and A. Gupta.Splash Stanford parallel applications forsharedmemory. Technical Report CSLTR91469, Stanford, 1991.TA88 ShinYuan Tzou and David P. Anderson. APerformance Evaluation of the DASHMessagePassing System. Technical Report UCBCSD88452, Computer Science Division, Universityof California, Berkeley, October 1988.Thi92 Thinking Machines Corporation. CM5 Network Interface Programmers Guide, 1992.vCGS92 T. von Eicken, D. Culler, S. Goldstein, andK. Schauser. Active Messages A Mechanismfor Integrated Communication and Computation. In Proceedings of the 19th Annual Symposium on Computer Architecture, 1992.vvST88 Robbert van Renesse, Hans van Staveren, andAndrew S. Tanenbaum. Performance of theWorlds Fastest Distributed Operating System.Operating Systems Review, 2242534, October 1988.Web93 Neil Webber. Operating System Support forPortable Filesystem Extensions. In Proceedings of the 1993 Winter USENIX Conference,January 1993.YBA93 Curtis Yarvin, Richard Bukowski, and ThomasAnderson. Anonymous RPC Low LatencyProtection in a 64Bit Address Space. In Proceedings of the Summer USENIX Conference,June 1993.
